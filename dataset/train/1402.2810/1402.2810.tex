\documentclass{llncs}
\usepackage{url}
\usepackage{amsmath,amssymb}
\usepackage{xspace}
\usepackage[noend]{algorithm}
\usepackage[noend]{algorithmic}
\usepackage{tikz}
\usepackage{pgfplots}

\newcommand{\algo}[1]{Algorithm \textsc{#1}}
\newcommand{\mr}{\textsc{MapReduce}\xspace}
\newcommand{\dmr}{\textsc{DS-MapReduce}\xspace}
\newcommand{\mrs}{\textsc{MapReduce}\xspace}
\newcommand{\algomr}{\textsc{Algorithm} \xspace}
\newcommand{\fcfs}{\textsc{FCFS}\xspace}
\newcommand{\mrf}{\textsc{MapReduce}\xspace}
\newcommand{\mrsr}{\textsc{MapReduce}\xspace}
\newcommand{\sr}{\textsc{SR}\xspace}
\newcommand{\srp}{\textsc{SRP}\xspace}

\pagestyle{plain}

\begin{document}

\title{Energy Efficient Scheduling of MapReduce Jobs
}

\author{Evripidis Bampis\inst{1}
\and Vincent Chau\inst{2}
\and Dimitrios Letsios\inst{1}
\and Giorgio Lucarelli\inst{1}
\and Ioannis Milis\inst{3}
\and Georgios Zois\inst{1,3}
}

\institute{Sorbonne Universit\'es, UPMC Univ Paris 06, UMR 7606, LIP6, F-75005, France.\\
\email{\{Evripidis.Bampis,Dimitrios.Letsios,Giorgio.Lucarelli, Georgios.Zois\}@lip6.fr}
\and IBISC, Universit\'{e} d'\'{E}vry, France.\\
\email{vincent.chau@ibisc.univ-evry.fr}
\and Dept. of Informatics, AUEB, Athens, Greece.\\
\email{milis@aueb.gr}
}

\maketitle

\begin{abstract}
MapReduce is emerged as a prominent programming model for data-intensive computation.
In this work, we study power-aware MapReduce scheduling in the speed scaling setting first introduced by Yao et al.~[FOCS 1995].
We focus on the minimization of the total weighted completion time of a set of MapReduce jobs under a given budget of energy.
Using a linear programming relaxation of our problem, we derive a polynomial time constant-factor approximation algorithm.
We also propose a convex programming formulation that we combine with standard list scheduling policies,
and we evaluate their performance using simulations.
\end{abstract}

\section{Introduction}

MapReduce has been established
as a standard programming model for parallel computing in data centers or computational grids and it is currently used
for several applications including search indexing, web analytics  or data mining.
However, data centers consume an enormous amount of energy and hence, energy efficiency has emerged as an important issue in the data-processing framework.
Several empirical works have been carried-out in order to study different mechanisms for the reduction of the energy consumption in the MapReduce setting
and especially for the Hadoop framework \cite{FellerRM13,FengLZY12,GoiriLNGTB12}.
The main  mechanisms for energy saving are the \emph{power-down} mechanism where in periods of low-utilization some servers are switched-off,
and the \emph{speed-scaling} mechanism (or DVFS for Dynamic Voltage Frequency Scaling) where the servers' speeds may be adjusted dynamically \cite{YaoDS95}.
Until lately, most work in the MapReduce framework  were focused on the  power-down mechanism, but recently,
Wirtz and Ge \cite{WirtzG11} showed that for some computation intensive MapReduce applications the use of intelligent speed-scaling
may lead to significant energy savings.
In this paper, we study power-aware MapReduce scheduling in the speed scaling setting from a theoretical point of view.


In a typical MapReduce framework, the execution of a MapReduce job creates a number of Map and Reduce tasks.
Each Map task processes a portion of the input data and outputs a number of key-value pairs.
All key-value pairs having the same key are then given to a Reduce task which processes the values associated with a key to generate the final result.
This means that each Reduce task cannot start before the completion of the last Map task of the same job.
In other words, there is a complete bipartite graph implying the precedences between Map and Reduce tasks of a job.
However, the Map tasks of a job can be executed in parallel and the same holds for its Reduce tasks.

In what follows we consider a set of MapReduce jobs that have to be  executed on a set of speed-scalable processors,
i.e., on processors that can adjust dynamically their speed \cite{YaoDS95}.
In the speed scaling setting, each task is associated with a work volume instead of a processing time
and the scheduler  has to decide not only the processor and the time interval where a task is executed,
but also its speed over time, taking into account the energy consumption.
High processor's speeds are in favor of performance  at the price of high energy consumption.
Each job consists of a set of Map tasks and a set of Reduce tasks, with every task having a positive work volume.
Each job is also associated with a positive weight representing its importance/priority, and a release date (or arrival time).
Like in~\cite{ChangKKLLM11,ChenKL12}, we consider that the Map and the Reduce tasks of each job are \emph{preassigned} to the processors and in this way we take into account data locality, i.e. the fact that each Map task has to be executed on the server where
its data are located. Given that  the preemption of tasks, i.e. the possibility of interrupting a task and resuming it later, may cause important overheads we do not allow it. This is also the case often in practice: Hadoop does not offer the possibility of preemption \cite{MoseleyDKS11}.
Our goal is to schedule all the tasks to the processors,
so as to minimize the total weighted completion time of jobs respecting a given budget of energy.

\paragraph{Related Work.}
Chang et al.~\cite{ChangKKLLM11} consider a set of MapReduce jobs with their Map and Reduce tasks
preassigned to processors and their goal is to minimize the total weighted completion time of jobs.
They proposed approximation algorithms of ratios 3 and 2 for arbitrary and common release dates, respectively.
However, they do not consider neither distinction nor dependencies between Map and Reduce tasks of a job.
Moreover, their model  falls  into a well-studied problem known as  {\em concurrent open-shop}
(or {\em order scheduling}) for which the same approximation results are known  (see ~\cite{MastrolilliQSSU10} and the references therein).
Extending on the above-mentioned model, Chen et al.~\cite{ChenKL12},
proposed a more realistic one which takes into account the dependencies among Map and Reduce tasks  and derived  an 8-approximation algorithm for the same objective.
Moreover, they managed to model  also the  transfer of the output of Map tasks to Reduce tasks and to derive  a  58-approximation algorithm for this generalization.
In a third model proposed by Moseley et al.~\cite{MoseleyDKS11}, the dependencies between Map and Reduce tasks of a job are also  taken into account while the assignment of tasks to processors is not given in advance.
The authors studied the preemptive variant for both the case of identical and unrelated processors. They proposed constant approximation ratios of 12 and 6, respectively.
For the unrelated processors case, they focused on the special case where each job has a single Map and a single Reduce task.
For the latter case on a single map and a single reduce processor they also proposed a QPTAS which becomes a PTAS for a fixed number of processing times of tasks. Recently, in~\cite{FotakisMZZ13} the authors proposed a -approximation algorithm for the unrelated processors case with multiple Map and Reduce tasks per job.

In the energy-aware setting, Angel et al.~\cite{AngelBK12}
proposed approximation algorithms for the problem of minimizing
the total weighted completion time on unrelated parallel
processors, under a model where the processing time and the energy
consumption of the jobs are speed dependent. Other works in this setting,
related to our problem, deal with single processor problems.
Megow et al. ~\cite{MegowV12} recently proposed a PTAS for the problem of minimizing the total weighted completion time on a single speed-scalable processor.

\paragraph{Our Results and Organization of the Paper}
We adopt the MapReduce model of~\cite{ChangKKLLM11} where the tasks are preassigned to processors
but extended with dependencies between Map and Reduce tasks
as in Chen et al.~\cite{ChenKL12,MoseleyDKS11} in the speed scaling setting \cite{YaoDS95}.
After a formal statement of our problem and notation, we present, in Section~\ref{se:lpa}, a polynomial time LP-based -energy -approximation algorithm which allows energy augmentation, i.e., it may use more energy than an optimal solution which always respects the energy budget, while using discretization of the possible speed values and list scheduling in the order of tasks' -points (see e.g. \cite{PhillipsSW97,HallSW96,Skutella06}).
As we show, there is a tradeoff between the approximation ratio and energy augmentation as a function of , where the schedule is converted to a constant-factor approximation for our problem.
In Section~\ref{se:cp}, we are interested in natural list scheduling policies such as \textsc{First Come First Serve} (\fcfs) and \textsc{Smith Rule} (\sr). However, in our context we need to determine the speeds of every task in order to respect the energy budget. For that, we propose a convex programming relaxation of our problem when an order of the jobs is prespecified.
This relaxation can be solved in polynomial time to arbitrary precision by the Ellipsoid algorithm \cite{NesterovNN94}.
Then we combine the solution of this relaxation with  \fcfs and \sr and  we compare experimentally their effectiveness.
Finally, we conclude in Section~\ref{se:con}.

\section{Problem Definition and Notation}

In the sequel we consider a set  of  MapReduce jobs to be executed
on a set  of  speed-scalable processors.
Each job is associated with a positive weight  and a release date 
and consists of a set of Map tasks and a set of Reduce tasks that are preassigned to the  processors.
We denote by  the set of all tasks of all jobs,
and by  and  the sets of all Map and Reduce tasks, respectively.
Each task  is associated a non-negative work volume .

We consider each job having at least one Map and one Reduce task and
that each job  has at most one task, either Map or Reduce, assigned to each processor.
Map or Reduce tasks can run simultaneously on different processors, while the following precedence constraints hold for each job:
every Reduce task can start its execution after the completion of all Map tasks of the same job.

For a given schedule we denote by   and  the completion times of each
job  and each  task , respectively.
Note that, due to the precedence constraints of Map and Reduce tasks, .
By  we denote the makespan of the schedule, i.e., the completion time of the job which finishes last.
Let also, , ,
,  and .

In this paper, we combine this abstract model for MapReduce scheduling with the speed scaling mechanism for energy saving \cite{YaoDS95}
(see also \cite{Albers11} for a recent review).
In this setting the power required by a processor running at time  with speed  
is equal to , for a constant  (typical values of  are between 2 and 3)
and its energy consumption  is power integrated over  time, i.e., .


Due to the convexity of the speed-to-power function,
a key property of our problem is that each task runs at a constant speed during its whole execution.
So, if a task  is executed at a speed , the time needed for
its execution (processing time) is equal to
 and its energy consumption is
.

Moreover, we are given an energy budget  and the goal is to schedule \emph{non-preemptively} all the tasks to the  processors,
so as to minimize the total weighted completion time of the schedule, i.e., , without exceeding the energy budget .
We refer to this problem as \mr problem.

As already mentioned above, the special case of the \mr problem where there are not dependencies between Map and Reduce tasks of each job
and each processor runs at a constant speed, reduces to the \emph{concurrent open-shop} problem which is known to be strongly -complete~\cite{Roemer06}.
It is also easy to adapt -completeness reductions like the one for the concurrent open-shop problem in the speed scaling setting,
and therefore, the \mr problem is also strongly -hard.

\section{A Linear Programming Approach}\label{se:lpa}

In this section we present a constant-factor approximation algorithm for the \mr problem. Our algorithm allows energy augmentation and derives a -energy -approximation schedule for the problem. As we show, there is a tradeoff where the schedule can be converted to a constant-factor approximate schedule for the \mr problem.

Our algorithm is based on a formulation of the problem as a linear programming relaxation.
Then, we transform the solution obtained by the linear program to a feasible schedule for the \mr problem using the technique of -points.

\subsection{Discretization of Speeds}

Before presenting the linear programming formulation,
our first step is to discretize the possible speed values by loosing a factor of  with respect to an optimal solution.
In order to do this, we need the following propositions that bound the length of an optimal schedule and the possible speed values.

\begin{proposition}\label{prop:T}
The makespan of any optimal schedule for the \mr problem is at most

\end{proposition}
\begin{proof}
Consider an optimal schedule for the \mr problem.
By definition, we have that .
Hence, it holds that .

In order to give an upper bound to ,
consider an instance of our problem where the weight  and the release date 
of each job  are rounded up to  and , respectively.
Moreover, assume that in this instance all tasks have work equal to .

Consider now an arbitrary order  of the jobs.
We create a feasible schedule  for the modified instance as follows.
All tasks run with the same speed ,
hence each task has a processing time .
Note that this speed allows us to execute all tasks without exceeding the energy budget.
As all tasks have the same processing time, we can consider the time horizon partitioned into time slots of length  starting from .
For each job , , we execute its Map tasks at time 
and its Reduce tasks at time .
Then, for the objective value  of this schedule it holds that

The objective value of schedule  is clearly an upper bound on the objective value
 of an optimal schedule for the initial instance and the proposition follows.
\qed
\end{proof}

\begin{proposition}\label{prop:speedbound}
For the speed  of any task  in the optimal schedule it holds that

\end{proposition}
\begin{proof}
The processing time  of a task  in an  optimal schedule cannot exceed the maximum
completion time, that is   and,  since by Proposition~\ref{prop:T} it holds that
,  the lower bound follows.

The energy consumption of any task cannot exceed the energy
budget, that is  and
the upper bound follows.
\qed
\end{proof}

Let  and  be
an upper and a lower bound, respectively, on the speed of any task.
Given these bounds, we discretize the interval  geometrically.
In other words, we assume that the processors can only run according to one of the following speeds:
,
where  is the smallest integer such that .
Note that  and hence the number of possible speeds is polynomial to the size of the instance and to .
We denote by  the set of all possible discrete speed values.
Let also .

\begin{lemma}\label{le:discretespeeds}
There is a feasible -approximate schedule for the \mr problem in which each task  runs at a speed .
\end{lemma}
\begin{proof}
Let an optimal schedule for our problem and consider the speed of each task 
rounded down to the closest  value.
As the speeds are decreased, the energy consumption of  does not exceed .
Moreover, the execution time of all tasks, and hence the completion time of every job
and the optimal objective value increase by a factor at most . \qed
\end{proof}

Henceforth we will consider the \mr problem in which each task  runs at a single speed .
We call this version of the problem \dmr.

\subsection{Linear Programming Relaxation}\label{se:lp}

In what follows we give an interval-indexed linear programming relaxation of the \dmr problem.
In order to do this, we discretize the time horizon of an optimal schedule as follows.
By Proposition~\ref{prop:T}, in any optimal schedule, all jobs are executed during the interval .
We partition  into the intervals
,
where  is a small constant,  is a constant that we will define later,
and  is the smallest integer such that .
Let  and , for .
Moreover, let , for , and
 be the length of the interval , i.e.,  and , .
Note that, the number of intervals is polynomial to the size of the instance and to , as .

Let  be the potential processing time for each task 
if it is executed entirely with speed .
For each ,  and ,
we introduce a variable  that corresponds to the portion of the interval  during which the task  is executed with speed .
In other words,  is the time that task  is executed within the interval  at speed ,
or equivalently,  is the fraction of the task  that is executed within  at speed .
Note that the number of  variables is polynomial to the size of the instance, to  and to .
Furthermore, for each task , we introduce a variable , which corresponds to the completion time of .
Finally, let , , be the variable that corresponds to the completion time of job .
(LP) is a linear programming relaxation of the \dmr problem.
\begin{figure}

\end{figure}

Our objective is to minimize the sum of weighted completion times of all jobs.
For each task , the corresponding constraint~(\ref{lp:p1}) ensures that  is entirely executed.
Constraints~(\ref{lp:p2}) enforce that the total amount of processing time that is executed within an interval  cannot exceed its length.
In~\cite{SchulzS02}, the authors proposed a lower bound for the completion time of a job.
This lower bound can be adapted to our problem and for the completion time
of a task  leads to a corresponding constraint~(\ref{lp:p3}).
Constraints~(\ref{lp:p4}) ensure that the completion time of each job is the maximum over the completion times of all its tasks.
Constraint~(\ref{lp:p5}) ensures that the given energy budget is not exceeded.
Note that the value  for each  is a fixed number.
Constraints~(\ref{lp:p6}) imply the precedence constraints between the Map and the Reduce tasks of the same job,
as they enforce that the fraction of a Map task that is executed up to each time point
should be at least the fraction of a Reduce task of the same job executed up to the same time point;
hence, each Map task completes before all Reduce tasks of the same job.
Constraints~(\ref{lp:p7}) do not allow tasks of a job to be executed before their release date.


In what follows, we denote an optimal solution to (LP) by .

\subsection{The Algorithm}

In this section we use (LP) to derive a feasible schedule for the \dmr problem.
Depending on the choice of some parameters, this schedule may exceed the energy budget.
As we show, there is a tradeoff where a constant factor approximation ratio can be derived.

Our algorithm is based on the idea of list scheduling in order of -points~\cite{HallSSW97}.
In general, an -point of a job is the first point in time
where an -fraction of the job has been completed, where  is a constant that depends on the analysis.
In this paper, we will define the -point  of a task  as
the minimum , , such that at least an -fraction of  is accomplished up to the interval  to (LP), i.e.,

Thus, once our algorithm has computed an optimal solution , ,  to (LP),
it calculates the corresponding -point, , for each task .
Then, combining the ideas of~\cite{ChenKL12} with the notion of -points~\cite{HallSSW97}, we create a feasible schedule as follows:
For each processor , we consider a priority list  of its tasks such that tasks with smaller -point have higher priority.
A crucial point in our analysis is that we consider that a task 
becomes \emph{available} for the algorithm after the time .
Moreover, if  then we need also all tasks  to be completed in order  to be considered as available.
For each task , we use a constant speed , where

is the processing time of  used by our algorithm,
and  is a constant that we define later and describes the tradeoff between the energy consumption and the weighted completion time of jobs.
At each time point where a processor  is available,
our algorithm selects the highest priority available task in  which has not been yet executed.
Note that our algorithm always create a feasible solution as we do not insist on selecting the highest priority task if this is not available.
\algomr gives a formal description of our algorithm.

\begin{algorithm}
\algomr
\begin{algorithmic}[1]
\STATE Compute an optimal solution  to .
\FOR {each task }
\STATE Compute the -point , the processing time  and the speed .
\ENDFOR
\FOR {each processor }
\STATE Compute the priority list .
\ENDFOR
\FOR {each time where a processor  becomes available}
\STATE Select the first available task, let , in  which has not been yet executed.
\STATE Schedule , non-preemptively, with processing time .\\Let  be the completion time of task .
\ENDFOR
\FOR {each job }
\STATE Compute its completion time .
\ENDFOR
\end{algorithmic}
\end{algorithm}

Note that the processing time of a task  to an optimal solution to (LP) is

Hence, the energy consumption 
for the execution of  to an optimal solution to (LP) may be smaller or bigger than
the energy consumption  for the execution of  by the algorithm.
In order to give the relation between these two quantities, we need the following technical lemma.

\begin{lemma} \label{le:technical}
Let  and  be positive values and .
Then, it holds that

\end{lemma}
\begin{proof}
The expression of the statement can be written equivalently as follows.

Note that the function  is convex for .
Thus, by the Jensen's inequality we have that

which is translated as

Therefore, in order to show inequality (\ref{Eq:Technical}), it suffices to show that

Thus, it suffices to prove that

An equivalent representation of the above expression is

The last inequality is always true, as

and hence the lemma follows.\qed
\end{proof}

\begin{lemma}\label{le:speed}
Let  and  be the energy consumption of the task 
to the optimal solution to (LP) and to the solution of \algomr, respectively.
It holds that

\end{lemma}
\begin{proof}
By the definition of  we have that

Since for each speed , , the above equality can be written as

Hence, by using Lemma~\ref{le:technical} we get

By the definition of -points we have that
, and thus

and the lemma follows.\qed
\end{proof}

The following lemma provides a lower bound to the completion time  of the task  given by the (LP).
\begin{lemma}
If , then
for each task  it holds that .
\label{le:Clowerbound}
\end{lemma}
\begin{proof}
Recall that  corresponds to the interval .
If we select , then there is no task with -point to the interval .
Hence, we can consider that the -point of each task 
corresponds to an interval of the form .

Starting from constraint~(\ref{lp:p3}) we have that

where the last inequality holds by constraint~(\ref{lp:p1}) and as by the definition of -point we know that
.
\qed
\end{proof}

The following theorem gives the approximation ratio of \algomr.

\begin{theorem}
\algomr is a -energy -approximation algorithm for the \dmr problem,
where ,  and .
\label{thm:main}
\end{theorem}
\begin{proof}
Consider the schedule  produced by \algomr and let  be any Map task.
Recall that  is the priority list of processor .
Let  be the list of tasks with priority higher than the priority of  in , including .
Then, for  it holds that

as  is always available after , as a Map task.
For the total processing time of jobs in  we have that

where the last inequality holds by applying constraint~(\ref{lp:p2}) of the (LP).
Thus, from inequality~(\ref{eq:up}) we have

for each Map task .

Consider now a job  and let  be a Reduce task of .
Moreover, let  be the Map task of  that completes last in ,
i.e., .
By definition,  becomes available at time .
Note that

where the first inequality holds by inequality~(\ref{eq:map}) and the second by the constraint~(\ref{lp:p6}) of (LP).

Let again  be the list of tasks with higher priority than  in , including .
If in the schedule  the processor  at time  executes a task ,
then for the completion time of  it holds that

because  is available after time  and it has higher priority than any task .
As before, we have that

Moreover, for the processing time of  it holds that

as  is executed at time  and hence it is available.
Then, by equation~(\ref{eq:red}) we have


As , using Lemma~\ref{le:Clowerbound} we get

and by using constraint (\ref{lp:p4}) of (LP)

Since the above inequality holds for each processor ,
it must also hold for  and thus

If we sum up all weighted completion times in  we yield

and as  is a lower bound to the objective value of an optimal solution
for the \dmr problem, the theorem follows. \qed
\end{proof}

Note that in the absence of precedence constraints between the tasks, the above analysis can be improved.
Indeed, we can consider that all tasks are Map tasks,
and hence an upper bound to their completion time into the schedule created by \algomr is given by Inequality~(\ref{eq:map}).
Then, the following corollary holds.

\begin{corollary} \label{cor:noprec}
\algomr is a -energy -approximation algorithm for the \dmr problem without precedence constraints,
where ,  and .
\end{corollary}

Moreover, in the absence of both precedence constraints between tasks and release dates of jobs, our analysis can be further improved.
As before, we can consider that all tasks are Map tasks.
In addition, we can drop the demand that a task  becomes available for the algorithm after the time .
Hence, Inequality~(\ref{eq:up}) is simplified to ,
as all tasks are released at time 0 and they are available at any time.
Then, the following corollary holds.

\begin{corollary} \label{cor:noprecreal}
\algomr is a -energy -approximation algorithm
for the \dmr problem without precedence constraints and release dates,
where ,  and .
\end{corollary}

By combining Lemma~\ref{le:discretespeeds}, Theorem~\ref{thm:main} and Corollaries~\ref{cor:noprec} and~\ref{cor:noprecreal},
and as we can select an  such that , the following theorem holds.

\begin{theorem}
There is a -energy -approximation algorithm for the \mr problem,
a -energy -approximation algorithm for the \mr problem without precedence constraints,
and a -energy -approximation algorithm for the \mr problem without precedence constraints and release dates,
where ,  and .
\end{theorem}

In Fig.\ref{fig:tradeoff} we depict a tradeoff between energy augmentation and approximation ratio for some practical values of .
Note that, by choosing , energy augmentation is not allowed and the schedule can be converted to a constant-factor approximate schedule.
In this case the following theorem holds.

\begin{theorem}
There is a -approximation algorithm for the \mr problem,
a -approximation algorithm for the \mr problem without precedence constraints,
and a -approximation algorithm for the \mr problem without precedence constraints and release dates,
where  and .
\end{theorem}

The ratios of the above theorem can be optimized by selecting the appropriate value of  for each .
Table~\ref{tbl:results} gives the achieved ratios for practical values of .

\begin{table}[htb]
\begin{center}
\begin{tabular}{c||c|c|c}
   & general & without precedence & without precedence \& without release dates \\
  \hline
  \hline
  2   & 37.52 & 9.44 & 6.75 \\
  2.2 & 34.89 & 8.84 & 6.29 \\
  2.4 & 33.01 & 8.41 & 5.97 \\
  2.6 & 31.59 & 8.09 & 5.72 \\
  2.8 & 30.50 & 7.84 & 5.53 \\
  3   & 29.62 & 7.64 & 5.38
\end{tabular}
\end{center}
\caption{Approximation ratios for the \mr problem for different values of .}
\label{tbl:results}
\end{table}

\section{A Convex Programming Approach}\label{se:cp}

We are interested in natural list scheduling policies such as \textsc{First Come First Serve} (\fcfs) and \textsc{Smith Rule} (\sr). However, in our context we need to determine the speeds of every task in order to respect the energy budget. For that, we propose a convex programming relaxation of our problem when an order of the jobs is prespecified.

\subsection{The Convex Program}

Let  be a given order of the jobs.
Consider now the restricted version of the \mr problem where
for each processor  the tasks are forced to be executed according to this order.
We shall refer to this problem as the \mrs problem.
Note that, the order is the same for all processors.
We write  if job  precedes job  in .
We propose a convex program that considers the order  as input and returns a solution that is
a lower bound to the optimal solution for the \mrs problem.

In order to formulate our problem as a convex program, let  be a variable that corresponds to the processing time of task .
Moreover, for each task , we introduce a variable  that determines the completion time of .
Finally, let , , be the variable that corresponds to the completion time of job .
Consider the following convex programming formulation of the \mrs problem.


The objective function of (CP) is to minimize the weighted completion time of all jobs.
Constraint~(\ref{cp:p1}) guarantees that the energy budget is not exceeded.
Constraints~(\ref{cp:p2}) and~(\ref{cp:p3}) give lower bounds on the completion time of each task ,
based on the release dates and the precedence constraints, respectively.
Note that, if we do not consider precedences between the tasks,
then (CP) will return the optimal value of the objective function, instead of a lower bound of it,
as constraints~(\ref{cp:p2}) describe in a complete way the completion times of the tasks.
However, this is not true for constraints~(\ref{cp:p3}) which are responsible for the precedence constraints.
Finally, constraints~(\ref{cp:p4}) ensure that the completion time of each job is the maximum over the completion times among all of its tasks.

As the optimal solution to (CP) does not necessarily describe a feasible schedule,
we need to apply an algorithm that uses the processing times found by (CP) and the order  so as to create a feasible schedule for the \mrs problem,
and hence for the \mr problem.
In fact, it suffices to apply, for example, the Lines~6-8 of \algomr, by considering the same order for all processors.


\begin{figure}[ht]
\centering
\begin{minipage}[b]{0.45\linewidth}
\begin{tikzpicture}[scale=0.6]
\begin{axis}[xlabel=approximation ratio,ylabel=energy augmentation (\%),ymin=0]
\addplot[smooth,thick] plot coordinates {
 (37.52,0)
 (33.32,10)
 (29.97,20)
 (27.25,30)
 (24.99,40)
 (23.10,50)
 (21.49,60)
 (20.10,70)
 (18.90,80)
 (17.84,90)
 (16.91,100)
};
\addplot[smooth,dashed,thick] plot coordinates {
 (32.25,0)
 (29.80,10)
 (27.75,20)
 (26.02,30)
 (24.53,40)
 (23.24,50)
 (22.11,60)
 (21.11,70)
 (20.22,80)
 (19.42,90)
 (18.69,100)
};
\addplot[smooth,dotted,thick] plot coordinates {
 (29.62,0)
 (27.91,10)
 (26.46,20)
 (25.20,30)
 (24.10,40)
 (23.13,50)
 (22.27,60)
 (21.49,70)
 (20.79,80)
 (20.15,90)
 (19.57,100)
};
\legend{\\\\\\}
\end{axis}
\end{tikzpicture}
\caption{Tradeoff between energy augmentation and approximation ratio when .}
\label{fig:tradeoff}
\end{minipage}
\quad
\begin{minipage}[b]{0.45\linewidth}
\begin{tikzpicture}[scale=0.6]
\begin{axis}[xlabel=number of jobs,ylabel=,ymin=0,legend pos=north west]]
\addplot[smooth,mark=x,thick] plot coordinates {
 (5,1919/1000)
 (10,12857/1000)
 (15,38352/1000)
 (20,85177/1000)
 (25,174398/1000)
};
\addplot[smooth,mark=x,dashed,thick] plot coordinates {
 (5,2321/1000)
 (10,16222/1000)
 (15,47570/1000)
 (20,100876/1000)
 (25,200904/1000)
};
\addplot[smooth,thick] plot coordinates {
 (5,1259/1000)
 (10,8594/1000)
 (15,26120/1000)
 (20,60228/1000)
 (25,119361/1000)
};
\addplot[smooth,dashed,thick] plot coordinates {
 (5,1001/1000)
 (10,6311/1000)
 (15,18943/1000)
 (20,41997/1000)
 (25,86365/1000)
};
\legend{\fcfs\\\sr\\CP(\fcfs)\\CP(\sr)\\}
\end{axis}
\end{tikzpicture}
\caption{Experimental comparison of the solutions of \fcfs and \sr (scaled down by a factor of ).}
\label{fig:exp}
\end{minipage}
\end{figure}

\subsection{Scheduling Policies}

In this section we propose different orders of jobs and we discuss how far is an optimal solution for the \mrs problem using these orders
with respect to the optimal solution for the \mr problem.
Two standard orders of jobs are the following.
\bigskip

\noindent\textsc{First Come First Serve} (\fcfs): for each pair of jobs , if  then  in .
\smallskip

\noindent\textsc{Smith Rule} (\sr): for each pair of jobs ,
if  then  in .
\bigskip


The following propositions present negative results concerning the approximation ratio that we can achieve if we use the \fcfs or the \sr order.

\begin{proposition}\label{prop:fcfs}
Let  and  be the optimal solutions for the \mr and the \mrf problems, respectively.
There is an instance for which it holds that .
\end{proposition}
\begin{proof}
Consider an instance consisting of  processors and  jobs, where .
The release date of each job  is , for a very small , and its weight .
Each job  consists of  tasks, one per processor.
Moreover, the task  is a Map task only if ; otherwise  is a Reduce task.
For each task , let .
For each task , let .
Let also  and .

Note that, if  then the processing time of each Reduce task can be considered to be very small
in both the optimal schedules for the \mr and the \mrf problems.
So, we can ignore the execution time and the energy consumption of the Reduce tasks.
We only consider the precedence constraints that they imply.

In an optimal solution for the \mr problem, the Map task of job  starts at time .
Due to the convexity and the fact that  for each ,
we can assume that all Map tasks will be executed with the same speed;
hence the processing time of each Map task is approximately equal to ,
as  and .
Thus, the completion time of each job is approximately equal to , and hence .

On the other hand, in an optimal solution for the \mrf problem the Map tasks are not executed in parallel,
as we are forced to respect the order and the precedence constraints.
Ignoring again the processing times of the Reduce tasks, we can assume that the Map task of job  starts at the completion time of job .
In order to find the speed  of each Map task 
into an optimal solution for the \mrf problem, we have to solve the following convex program.

The objective of this convex program corresponds to the objective of the \mrf problem for the given instance,
while the constraint ensures that the selected speeds respect the energy budget.
By applying the Karush-Kuhn-Tucker conditions to this program
we get that .
By replacing this to the objective we get


As  and , the proposition follows.\qed
\end{proof}

\begin{proposition}\label{prop:sr}
Let  and  be the optimal solutions for the \mr and the \mrsr problems, respectively.
There is an instance for which it holds that .
\end{proposition}
\begin{proof}
We consider a simplified instance which consists of only one processor and does not take into account Map and Reduce tasks and hence precedences.
In this instance the critical issue is the release dates.
For each job , , we have ,  and ,
while for the job  we have ,  and , where  is a big number.
Let  and .

In an optimal schedule for the \mr problem, the jobs  are scheduled consecutively starting from time 0,
while the job  is scheduled starting from time .
Let  and  be parts of the energy budget used for the execution of the jobs  and , respectively.
Clearly, it holds that .
Hence, following similar analysis as in Proposition~\ref{prop:fcfs} for the \mrf problem,
for the total weighted completion time of the jobs  it holds that

The processing time of job  is , and hence its completion time is .
Therefore, for the optimal solution for the \mr problem we have that

as this function is minimized for .

On the other hand, in an optimal schedule for the \mrsr problem,
the jobs are scheduled starting from  according to the \sr order, i.e., .
As we can choose an  such that , we can assume that all jobs have the same work to execute.
Then, following similar analysis as in Proposition~\ref{prop:fcfs} for the \mrf problem,
we have that .

As  can be arbitrary large, the proposition follows. \qed
\end{proof}

\subsection{Experimental Evaluation of Scheduling Policies}

In this section, our goal is to compare the \fcfs and \sr policies with respect to the quality of the solution that they produce.


Our simulations have been performed on a machine with a CPU Intel Xeon X5650 with 8 cores, running at 2.67GHz.
The operating system of the machine is a Linux Debian 6.0.
We used Matlab with cvx toolbox.
The solver used for the convex program is SeDuMi.
The instance of the problem consists of a matrix  that corresponds to the work of the tasks,
two vectors of size  that correspond to the weights and the release dates of jobs,
a precedence graph for the tasks of the same job,
the energy budget and the value of .

Similarly with \cite{ChenKL12}, the instance consists of  processors and up to  jobs.
Each job has 20 Map and 10 Reduce tasks, which are preassigned at random to a different processor.
The work of each Map task is selected uniformly at random in ,
while the work of each Reduce task  is equal to a random number in  plus
,
taking into account the fact that Reduce tasks have more work to execute than Map tasks.
The weight of each job is selected uniformly at random in .
For the release date of a job, we select with probability 1/2 every interval .
Then, the release date is equal to a random value in this interval.
The energy budget that we used is .
We have also set .
We set the desired accuracy of the returned solution of the convex program to be equal to .
For each number of jobs we have repeated the experiments with 10 different matrices.
The results we present below, concern the average of these 10 instances.

The benchmark as well as the code we used in our experiments are freely available at\\
\url{http://www.ibisc.univ-evry.fr/~vchau/research/mapreduce/}.


As mentioned before, the (CP) does not lead to a feasible solution for our problem.
In order to get such a solution we apply the following algorithm.
At each time  where a processor becomes available we select to schedule the task  of higher priority such that:
(i)  is already released at ,
(ii) if  is a Reduce task, then all Map tasks of the same job have been already completed at , and
(iii)  has not been yet executed.

As shown in Fig.~\ref{fig:exp} the heuristic based on \fcfs outperforms the heuristic based on \sr.
In fact, the first heuristic gives up to  better solutions that the second one for different values of .
Surprisingly, the situation is completely inverse if we consider the corresponding solutions of the convex programs.
More precisely, the convex programming relaxation using \sr leads to  smaller values of the objective function
with respect to the convex programming relaxation using \fcfs.

Moreover, we can observe that the ratio between the final solution of each heuristic with respect to the lower bound for the \mrs problem
given by the convex program is equal to 1.46 for \fcfs and 2.43 for \sr;
the variance is less than 0.1 in both cases.
However, as we already mentioned, this ratio cannot be considered as the approximation ratio for the \mr problem,
as its optimal solution can be significantly smaller than the optimal solution for the \mrs problem using the \fcfs and \sr orders.

\section{Conclusions}\label{se:con}

We presented a constant-approximation algorithm for the problem of scheduling a set of MapReduce jobs
in order to minimize their total weighted completion time under a given budget of energy.
Our algorithm uses an optimal solution to an LP relaxation in interval-indexed variables and converts it to a feasible
non-preemptive schedule of the \mr problem using the idea of list scheduling in order of a-points.
Moreover, we proposed a convex programming relaxation of the problem when a prespecified order of jobs is given.
Based on the solution of this convex programming relaxation, we explored the efficiency of standard scheduling policies,
by presenting counterexamples for them as well as by experimentally evaluating their performance.
It has to be noticed that our results can be extended also to the case where multiple Map or Reduce tasks of a job are executed on the same processor.
An interesting direction for future work concerns the online case of the problem.
Although, it can be proved that there is no an -competitive deterministic algorithm (see Theorem~13 in~\cite{BansalPS09}),
a possible way to overcome this  is to consider resource (energy) augmentation,
or to study the closely-related objective of a linear combination of the sum of weighted completion times of the jobs and of the total consumed energy.


\bibliographystyle{plain}
\begin{thebibliography}{10}

\bibitem{Albers11}
S.~Albers.
\newblock Algorithms for dynamic speed scaling.
\newblock In {\em Symposium on Theoretical Aspects of Computer Science
  (STACS)}, pages 1--11, 2011.

\bibitem{AngelBK12}
E.~Angel, E.~Bampis, and F.~Kacem.
\newblock Energy aware scheduling for unrelated parallel machines.
\newblock In {\em Green Computing Conference}, pages 533--540, 2012.

\bibitem{BansalPS09}
N.~Bansal, K.~Pruhs, and C.~Stein.
\newblock Speed scaling for weighted flow time.
\newblock {\em {SIAM} Journal on Computing}, 39(4):1294--1308, 2009.

\bibitem{ChangKKLLM11}
H.~Chang, M.~S. Kodialam, R.~R. Kompella, T.~V. Lakshman, M.~Lee, and
  S.~Mukherjee.
\newblock Scheduling in mapreduce-like systems for fast completion time.
\newblock In {\em {IEEE} Proceedings of the 30th International Conference on
  Computer Communications}, pages 3074--3082, 2011.

\bibitem{ChenKL12}
F.~Chen, M.~S. Kodialam, and T.~V. Lakshman.
\newblock Joint scheduling of processing and shuffle phases in mapreduce
  systems.
\newblock In {\em {IEEE} Proceedings of the 31st International Conference on
  Computer Communications}, pages 1143--1151, 2012.

\bibitem{FellerRM13}
E.~Feller, L.~Ramakrishnan, and C.~Morin.
\newblock On the performance and energy efficiency of {H}adoop deployment
  models.
\newblock In {\em BigData Conference}, pages 131--136, 2013.

\bibitem{FengLZY12}
B.~Feng, J.~Lu, Y.~Zhou, and N.~Yang.
\newblock Energy efficiency for {M}ap{R}educe workloads: {A}n in-depth study.
\newblock In {\em ADC}, pages 61--70, 2012.

\bibitem{FotakisMZZ13}
D.~Fotakis, I.~Milis, E.~Zampetakis, and G.~Zois.
\newblock Scheduling mapreduce jobs on unrelated processors.
\newblock {\em Technical Report, arxv.org}, abs/1312.4203, 2013.

\bibitem{GoiriLNGTB12}
I.~Goiri, K.~Le, T.~D. Nguyen, J.~Guitart, J.~Torres, and R.~Bianchini.
\newblock Green{H}adoop: leveraging green energy in data-processing frameworks.
\newblock In {\em EuroSys}, pages 57--70, 2012.

\bibitem{HallSSW97}
L.~A. Hall, A.S. Schulz, D.~B. Shmoys, and J.~Wein.
\newblock Scheduling to minimize average completion time: Off-line and on-line
  approximation algorithms.
\newblock {\em Mathematics of Operations Research}, 22:513--544, 1997.

\bibitem{HallSW96}
L.~A. Hall, D.~B. Shmoys, and J.~Wein.
\newblock Scheduling to minimize average completion time: Off-line and on-line
  algorithms.
\newblock In {\em Proceedings of the 7th {ACM-SIAM} Symposium on Discrete
  Algorithms}, pages 142--151, 1996.

\bibitem{MastrolilliQSSU10}
M.~Mastrolilli, M.~Queyranne, A.~S. Schulz, O.~Svensson, and N.~A. Uhan.
\newblock Minimizing the sum of weighted completion times in a concurrent open
  shop.
\newblock {\em Operations Research Letters}, 38(5):390--395, 2010.

\bibitem{MegowV12}
N.~Megow and J.~Verschae.
\newblock Dual techniques for scheduling on a machine with varying speed.
\newblock In {\em Proceedings of the 40th ICALP}, pages 745--756, 2013.

\bibitem{MoseleyDKS11}
B.~Moseley, A.~Dasgupta, R.~Kumar, and T.~Sarl{\'o}s.
\newblock On scheduling in map-reduce and flow-shops.
\newblock In {\em Proceedings of the 23rd {ACM} Symposium on Parallel
  Algorithms and Architectures (SPAA)}, pages 289--298, 2011.

\bibitem{NesterovNN94}
A.~Nemirovski, I.~Nesterov, and Y.~Nesterov.
\newblock {\em Interior Point Polynomial Algorithms in Convex Programming}.
\newblock Society for Industrial and Applied Mathematics, 1994.

\bibitem{PhillipsSW97}
C.~A. Phillips, C.~Stein, and J.~Wein.
\newblock Task scheduling in networks.
\newblock {\em {SIAM} Journal on Discrete Mathematics}, 10(4):573--598, 1997.

\bibitem{Roemer06}
T.A. Roemer.
\newblock A note on the complexity of the concurrent open shop problem.
\newblock {\em Journal of Scheduling}, 9:389--396, 2006.

\bibitem{SchulzS02}
A.~S. Schulz and M.~Skutella.
\newblock Scheduling unrelated machines by randomized rounding.
\newblock {\em SIAM J. Discrete Math.}, 15(4):450--469, 2002.

\bibitem{Skutella06}
Martin Skutella.
\newblock List scheduling in order of α-points on a single machine.
\newblock In Evripidis Bampis, Klaus Jansen, and Claire Kenyon, editors, {\em
  Efficient Approximation and Online Algorithms: Recent Progress on Classical
  Combinatorial Optimization Problems and New Applications}, volume 3484 of
  {\em Lecture Notes in Computer Science}, page 250–291. Springer, 2006.

\bibitem{WirtzG11}
T.~Wirtz and R.~Ge.
\newblock Improving {M}ap{R}educe energy efficiency for computation intensive
  workloads.
\newblock In {\em IGCC}, pages 1--8, 2011.

\bibitem{YaoDS95}
F.~F. Yao, A.~J. Demers, and S.~Shenker.
\newblock A scheduling model for reduced cpu energy.
\newblock In {\em Proceedings of the 36th Annual {IEEE} Symposium on
  Foundations of Computer Science}, pages 374--382. {IEEE}, 1995.

\end{thebibliography}



\end{document}
