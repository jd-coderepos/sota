\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{wacv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage[tbtags]{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage[T1]{fontenc}
\usepackage[dvipsnames, table]{xcolor}
\usepackage{dirtytalk}

\usepackage{bm}
\usepackage{physics}
\usepackage{nicefrac}
\usepackage{cite}

\newtheorem{remark}{Remark}

\usepackage{multirow}

\usepackage{algorithm}
\usepackage{algpseudocodex}

\usepackage{tikz}
\usetikzlibrary{arrows.meta}

\usepackage[normalem]{ulem}
\usepackage{tablefootnote}
\usepackage{authblk}


\def\wacvPaperID{19} 

\wacvalgorithmstrack   

\wacvfinalcopy 







\usepackage[pagebackref=true, breaklinks=true, colorlinks, citecolor=PineGreen, linkcolor=BrickRed, bookmarks=false]{hyperref}


\usepackage[capitalize]{cleveref}

\pagestyle{empty}

\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\newcommand{\gustavo}[1]{\textcolor{BrickRed}{[Gustavo]: #1}}
\newcommand{\cuong}[1]{\textcolor{BurntOrange}{[Cuong]: #1}}
\newcommand{\arpit}[1]{\textcolor{OliveGreen}{[Arpit]: #1}}
\newcommand{\rafa}[1]{\textcolor{WildStrawberry}{[Rafa]: #1}}





\begin{document}

\title{Instance-Dependent Noisy Label Learning via Graphical Modelling}
    
\author[1]{Arpit Garg\thanks{arpit.garg@aiml.team}}
    \author[1]{Cuong Nguyen}
    \author[1]{Rafael Felix}
    \author[2]{Thanh-Toan Do}
    \author[1]{Gustavo Carneiro}
    \affil[1]{Australian Institute for Machine Learning, University of Adelaide}
    \affil[2]{Department of Data Science and AI, Faculty of Information Technology, Monash University}
    
    \renewcommand\Affilfont{\normalsize}
    
    \maketitle
    \thispagestyle{empty}
    
\begin{abstract}
    Noisy labels are unavoidable yet troublesome in the ecosystem of deep learning because models can easily overfit them. There are many types of label noise, such as symmetric, asymmetric and instance-dependent noise (IDN), with IDN being the only type that depends on image information. Such dependence on image information makes IDN a critical type of label noise to study, given that labelling mistakes are caused in large part by insufficient or ambiguous information about the visual classes present in images. Aiming to provide an effective technique to address IDN, we present a new graphical modelling approach called InstanceGM, that combines  discriminative and generative models. The main contributions of InstanceGM are: i) the use of the continuous Bernoulli distribution to train the generative model, offering significant training advantages, and ii) the exploration of a state-of-the-art noisy-label discriminative classifier to generate clean labels from instance-dependent noisy-label samples. InstanceGM is competitive with current noisy-label learning approaches, particularly in IDN benchmarks using synthetic and real-world datasets, where our method shows better accuracy than the competitors in most experiments.
\end{abstract}
     
\section{Introduction}

    The latest developments in deep neural networks (DNNs) have shown outstanding results in a variety of applications ranging from computer vision~\cite{krizhevsky2012imagenet} to natural language processing~\cite{ragesh2021hetegcn} and medical image analysis~\cite{qian20223d}. Such success is strongly reliant on high-capacity models, which in turn, require a massive amount of correctly-annotated data for training~\cite{yao2020dual, litjens2017survey}. Annotating a large amount of data is, however, arduous, costly and time-consuming, and therefore is often done via crowd-sourcing~\cite{song2022learning} that generally produces low-quality annotations. Although that brings down the cost and scales up the process, the trade-off is the mislabelling of the data, resulting in a deterioration of deep models' performance~\cite{liu2020early, bae2022noisy} due to the \emph{memorisation effect}~\cite{neyshabur2017exploring, arpit2017closer, liu2020early, zhang2021learning_}. This has, therefore, motivated the research of novel learning algorithms to tackle the label noise problem where data might have been mislabelled.
    


Early work in label noise~\cite{han2018co} was carried out under the assumption that label noise was instance-independent (IIN), i.e., mislabelling occurred regardless of the information about the visual classes present in images.
    In IIN, we generally have a transition matrix that contains a pre-defined probability of flipping between pairs of labels (e.g., any image showing a \emph{cat} has a high priori probability of being mislabelled as a \emph{dog} and low a priori probability of being mislabelled as a \emph{car}).
This type of noise can also be divided into two sub-types: \emph{symmetric}, where a true label is flipped to another label with equal probability across all classes, and \emph{asymmetric}, where a true label is more likely to be mislabeled into one of some particular classes~\cite{han2018co}. 
    Nevertheless, the IIN assumption is impractical for many real-world datasets because we can intuitively argue that mislabellings mostly occur because of insufficient or ambiguous information  about the visual classes present in images. 
    As a result, recent studies have gradually shifted their focus toward the more realistic scenario of instance-dependent noise (IDN), where label noise depends on both the true class label and the image information~\cite{xia2020part}.
    

    Many methods have been introduced to handle not only IIN, but also IDN problems. Those include, but are not limited to, \emph{sample selection}~\cite{xia2021sample, li2020dividemix, zheltonozhskii2022contrast,kim2021fine, cordeiro2021propmix} that detects clean and noisy labels and applies semi-supervised learning methods on the processed data, \emph{robust losses}~\cite{patrini2017making, arazo2019unsupervised, liu2020peer} that can work well with either clean or noisy labels, and \emph{probabilistic approaches}~\cite{yao2021instance} that model the data generation process, including how a noisy label is created. 
    Despite some successes, most methods are often demonstrated in IIN settings with simulated symmetric and asymmetric noise. However, their performance is degraded when evaluated on IDN problems, which include real-world and synthetic datasets. Although there are a few studies focusing on the IDN setting~\cite{yao2021instance, cheng2022instance, xia2020part,zhu2021clusterability, jiang2021information}, their relatively inaccurate classification results suggest that the algorithms can be improved further.
    
    


In this paper, we propose a new method to tackle the IDN problem, called InstanceGM. Our method is designed based on a graphical model that considers the clean label  as a latent variable and introduces another latent variable  representing the image feature to model the generation of a label noise  and an image . 
InstanceGM integrates generative and discriminate models, where the generative model is based on a variational auto-encoder (VAE)~\cite{kingma2013auto}, except that we replace the conventional mean squared error (MSE) when modelling the likelihood of reconstructed images by a  \emph{continuous Bernoulli distribution}~\cite{loaiza2019continuous} that facilitates the training process since it avoids tuning additional hyper-parameters. 
For the discriminative model, to mitigate the problem of 
only using clean label data during the training process, which is a common issue present in the similar graphical model methods~\cite{yao2021instance}, we rely on  DivideMix~\cite{li2020dividemix} that uses both clean and noisy-label data for training by exploring semi-supervised learning via MixMatch~\cite{berthelot2019mixmatch}. DivideMix is shown to be a reasonably effective discriminative classifier for our InstanceGM.
In summary, the main contributions of the proposed method are:
\begin{itemize}
    \item InstanceGM follows a graphical modelling approach to generate both the image  and its noisy label  with the true label  and image feature  as latent variables. The modelling is associated with the continuous Bernoulli distribution to model the generation of instance  to facilitate the training, avoiding tuning of additional hyper-parameters (see \cref{remark:avoid_reweighting}).
\item For the discriminative classifier of InstanceGM, we replace the commonly used
    co-teaching, which is a dual model that relies only on training samples classified as clean, with DivideMix~\cite{li2020dividemix} that uses all training samples classified as clean and noisy. 
\item InstanceGM shows state-of-the-art results on a variety of IDN benchmarks, including simulated and real-world datasets, such as CIFAR10 and CIFAR100~\cite{krizhevsky2009learning}, Red Mini-ImageNet from Controlled Noisy Web Labels (CNWL)~\cite{xu2021faster}, ANIMAL-10N~\cite{song2019selfie} and CLOTHING-1M~\cite{xiao2015learning}.
\end{itemize}































%
     \section{Related work}
\label{sec:related_work}

As DNNs have been shown to easily fit randomly labelled training data~\cite{zhang2017understanding}, they can also overfit a noisy-label dataset, which eventually results in poor generalisation to a clean-label testing data~\cite{neyshabur2017exploring, arpit2017closer, liu2020early, zhang2021learning_}. 
Several studies have, therefore, been conducted to investigate supervised learning under the label noise setting, including robust loss function~\cite{ma2020normalized, wang2019imae}, sample selection~\cite{wang2018iterative, song2019selfie, song2021robust}, robust regularisation~\cite{jenni2018deep, wei2021open, menon2019can, goodfellow2014explaining} and robust architecture~\cite{xiao2015learning, cheng2020weakly, han2018masking, kong2021resolving}. Below, we review methods dealing with noisy labels, especially IDN, without the reliance on clean validation sets~\cite{veit2017learning, hendrycks2018using, ren2018learning}.



Let us start with methods designed to handle \say{any} type of label noise, including IDN and IIN.
An important technique for both of these label noise types is sample selection~\cite{wang2018iterative, song2019selfie, song2021robust}, which aims to select clean-label samples automatically for training. Although it is well-motivated and often effective, it suffers from the cumulative error caused by mistakes in the selection process, mainly when there are numerous unclear classes in the training data. Consequently, sample selection methods often rely on multiple clean-label sample classifiers to increase their robustness against such cumulative error~\cite{li2020dividemix}. 
In addition, semi-supervised learning (SSL)~\cite{cordeiro2021propmix, li2020dividemix, song2019selfie, zheltonozhskii2022contrast, kim2021fine, bernhardt2022active} have also been integrated with sample selection and multiple clean-label classifiers to enable the training from clean and noisy-label samples. 
In particular, SSL methods use clean and noisy samples by treating them as labelled and unlabelled data, respectively, with a MixMatch approach~\cite{berthelot2019mixmatch}. These methods above have been designed to handle \say{any} type of label noise, so they are usually assessed in synthetic IIN benchmarks and real-world IDN benchmarks.

Given that real-world datasets do not, in general, contain IIN, more recently proposed methods aim to address IDN benchmarks~\cite{cheng2022instance, xia2020part, Zhu_2021_CVPR, berthon2021confidence, yao2021instance, liu2021understanding}. 
In these benchmarks, the task of differentiating between hard clean-labelled samples and noisy-label samples pose a major challenge.
Such issue is noted by Song et al.~\cite{song2019does}, who state that the model performance in IDN can degrade significantly compared to other types of noises. 


One direct way of addressing IDN problems relies on a graphical model approach that has random variables representing the observed noisy label, the image, and the latent clean label.
This model also has a generative process to produce an image given the (clean and noisy) label information~\cite{lawrence2001estimating}. Another approach examines a graphical model using a discriminative process~\cite{raykar2010learning},  where the model attempts to explain the posterior probability of the observed noisy label by averaging the posterior probabilities of the clean class label. 
Yao et al.~\cite{yao2021instance} developed a new causal model to address IDN that also uses the same variables as the methods above plus a latent image feature variable, which relies on  generative models to produce the image from the clean label and image feature, and to produce the noisy label from the image feature and clean label.
That approach~\cite{yao2021instance}, however, did not produce competitive results compared with state of the art.
We argue that the model's poor performance is mostly due to the co-teaching~\cite{han2018co} that is trained with a small set of samples classified as clean, which can inadvertently contain noisy-label samples -- this is an issue that can cause a cumulative error, particularly in IDN problems.



Our work is motivated by the graphical model approaches mentioned above, that aim to address IDN problems. 
The main difference in our approach is the use of a more effective clean sample identifier that replaces  co-teaching~\cite{yao2021instance} by DivideMix~\cite{li2020dividemix}, which considers the whole training set, instead of only the samples classified as clean.
Moreover, we propose a more effective training of the image generative model based on the continuous Bernoulli distribution~\cite{loaiza2019continuous}. 



     \section{Methodology}

\label{sec:methodology}

    \begin{figure}[t]
        \centering
        \begin{tikzpicture}
            \pgfmathsetmacro{\yshift}{5}
\pgfmathsetmacro{\xshift}{6}
\pgfmathsetmacro{\minsize}{2.5}

\node[circle, draw=Black, minimum size=\minsize em] at (0, 0) (y) {};

\node[circle, draw=Black, fill=Gray, minimum size=\minsize em] at ([xshift=-\xshift em, yshift=0em]y) (x) {};

\node[circle, draw=Black, minimum size=\minsize em] at ([xshift=-\xshift em, yshift=0em]x) (z) {};

\node[circle, draw=Black, fill=Gray, minimum size=\minsize em] at ([xshift=\xshift em, yshift=0em]y) (yhat) {};





\draw[-Latex] (y) -- (x);
\draw[-Latex] (z) -- (x);
\draw[-Latex] (x.north) to [out=30,in=150](yhat.north);
\draw[-Latex] (y) -- (yhat);         \end{tikzpicture}
        \caption{The proposed graphical model of the generation process that produces the observable (shaded nodes) data  and noisy label  from hidden (non-shaded nodes) data representation  and clean label .}
        \label{fig:graphical_model}
    \end{figure}


    \subsection{Problem definition}
    \label{sec:background}
        We denote 
        as an observed random variable representing an image, 
        
        as a latent random variable corresponding to the clean label of ,
         as a latent random variable denoting an image feature representation for of , and  as the observed random variable for the noisy label.
        The training set is represented by , where 
        the image is represented by  (with  color channels and size  pixels) and the noisy label  denoted by a one-hot vector.
In the conventional supervised learning,  is used to train a model  (where  represents the probability simplex), parameterised by , that can predict the labels of testing images. 
The aim is to exploit the noisy data  from a training set to infer a model  that can accurately predict the clean labels  of data in a testing set.

    \subsection{Probabilistic noisy label modelling}
    \label{sec:probabilistic_modelling}
        We follow a similar approach presented in~\cite{yao2021instance} to model the process that generates samples with noisy labels via the graphical model shown in \cref{fig:graphical_model}, where the clean label  and image feature representation  are latent variables. Under this modelling assumption, a noisy-label sample  can be generated as follows:
        \begin{enumerate}
            \item sample a clean label from its prior: ,
            \item sample a representation from its prior: ,
            \item sample an input data from its continuous Bernoulli distribution: , \label{forward:generate_x}
            \item sample the corresponding noisy label from its categorical distribution: 
\end{enumerate}

        \begin{remark}\label{remark}
            Conventionally, the process of generating data  in step~\ref{forward:generate_x} above is often modelled as a Bernoulli distribution or multivariate normal distribution, corresponding to the binary cross-entropy (BCE) or MSE reconstruction losses, respectively. Such modelling, however, leads to a pervasive error~\cite{loaiza2019continuous} since the image pixels are in  instead of  (Bernoulli distribution)\footnote{Except for black and white images.} or  (multivariate normal distribution). We therefore adopt the continuous Bernoulli distribution~\cite{loaiza2019continuous} which has a support in  to correctly model this image generation process.
        \end{remark}
        
        Note that the parameters of the continuous Bernoulli and categorical distributions are conditioned on ,  and , and modelled as the outputs of two DNNs:
        
where  denotes the neural network,
        and , represent the network parameters. Following the convention in machine learning, we call  the \emph{decoder} and  the \emph{noisy label classifier}.

        To solve the label noise problem that has data generated from the process above, we need to infer the posterior . However, due to the complexity of the graphical model in \cref{fig:graphical_model}, exact inference for the posterior  is intractable, and therefore, the estimation must rely on an approximation. Motivated by~\cite{yao2021instance}, we employ variational inference to approximate the true posterior  by a variational \say{posterior} . Such posterior can be obtained by minimising the following Kullback-Leibler (KL) divergence:
        
        where the variational posterior  can be factorised following the product rule of probability. We assume that the posterior of the clean label  is independent from the noisy label , given the instance : . In addition, the variational posterior of feature representation is independent from the noisy label given the clean label and input data: . The variational posterior of interest can, therefore, be written as:
        

        \begin{figure*}[ht!]
            \centering
            \includegraphics[width=0.65\linewidth, keepaspectratio]{img/arch.pdf}
            \caption{The proposed InstanceGM trains the \textit{Classifiers} to output clean labels for  instance-dependent noisy-label samples. We first warmup our two classifiers \textit{(Classifier-\{11,12\})} using the classification loss, and then with classification loss we train the GMM to separate clean and noisy samples with the semi-supervised model MixMatch~\cite{berthelot2019mixmatch} from the DivideMix~\cite{li2020dividemix} stage. Additionally, another set of encoders \textit{(Encoder-\{1,2\})} are used to generate the latent image features as depicted in the graphical model from~\cref{fig:graphical_model}. Furthermore, for image reconstruction, the decoders \textit{(Decoder-\{1,2\})} are used by utilizing the continuous Bernoulli loss, and another set of classifiers \textit{(Classifier-\{21,22\})} helps to identify the original noisy labels using the standard cross-entropy loss. 
}            \label{fig:architecture}
\end{figure*}
        
        The objective function in \eqref{eq:minimise_KL_divergence} can then be expanded as:
        
        
        \begin{remark}
            The objective function  in \eqref{eq:objective} shares similarity with the loss in variational auto-encoder~\cite{kingma2013auto}. In particular, the first two terms in \eqref{eq:objective} are analogous to the reconstruction loss, while the remaining terms are analogous to the KL loss that regularises the deviation between the posterior  and its prior.
        \end{remark}
        
        To optimise the objective in \eqref{eq:objective}, both the posteriors  and  and priors  and  must be specified. We assume  to be a multivariate normal distribution with a diagonal covariance matrix and  to be a categorical distribution:
        
        where the parameters of these distributions are modelled as the outputs of two DNNs. Hereafter, we call the network that models  the \emph{clean label classifier}, and the  model , the \emph{encoder}. 
        
        For the priors, we follow the convention in generative models, especially VAE, to assume  as a standard normal distribution, while  is a uniform distribution.
        
        Given such assumptions, we can minimise the loss function  in \eqref{eq:objective} w.r.t. the parameters of the two classifiers, the encoder and decoder in \eqref{eq:probs_encoders} and \eqref{eq:probs_decoders}. The obtained clean label classifier that models  will then be used as the final classifier to evaluate data in the testing set.
        
        \begin{algorithm*}[t]
            \caption{Graphical model approach for learning with label noise}
            \label{algorithm:Proposed_Algo}
            \begin{algorithmic}[1]
                \Procedure{InstanceGM}{}
\LComment{: noisy dataset}
                    \LComment{: total number of epochs}
                    \LComment{: threshold to decide clean or noisy samples used in DivideMix}
                    \State  \Call{Warmup}{} \Comment{Warm-up training of 2 clean-label classifiers on noisy dataset}
                    \For{}
                        \State  \Call{Co-divide}{}
                        \LComment{Apply Gaussian mixture model on loss values and filter out clean and noisy with a threshold on the likelihood} \LComment{ are labelled sets (mostly clean)}
                        \LComment{ are unlabelled sets (mostly noisy)}
                        
                        \Statex
                        
                        \State  \Call{DivideMix Loss}{} \Comment{Calculate training loss in DivideMix}
                        \State  \Call{DivideMix Loss}{}
                        
                        \For{k = 1:2} \Comment{Calculate loss on each one of the 2 models}
                            \For{each }
                                \State Compute each instance loss:  \Call{Variational-free energy}{}
                                \LComment{ is the variational posterior}
                                \LComment{ denotes prior and data generation}
                            \EndFor
                            \State Compute average loss: 
                            \State Update model parameters by minimizing  \Comment{\cref{eq:final_loss_function}}
                        \EndFor
\EndFor
                    \State \Return  \Comment{clean-label classifier}
                \EndProcedure
                
                \Statex
                
                \Function{Variational-free energy}{} \Comment{Calculate loss in \cref{eq:objective}}
                    \State Sample  \Comment{Sample a clean label from its variational posterior}
                    \State Sample  \Comment{Sample a feature representation from its variational posterior}
                    \State Compute the 1st term in \cref{eq:objective}:  \Comment{image reconstruction loss}
                    \State Compute the 2nd term in \cref{eq:objective}:  \Comment{noisy-label cross-entropy loss}
                    \State Compute the remaining terms in \cref{eq:objective}
                    \State Compute  as the sum of the above terms as specified in \cref{eq:objective}
                    \State \Return 
                \EndFunction
            \end{algorithmic}
        \end{algorithm*}

        \begin{remark}
            Optimising the objective function in \eqref{eq:objective} often requires the definition of  hyper-parameters to weight the KL divergences ~\cite{graves2011practical}. However, such weighting mechanism depends on the estimation of the KL divergences weights that is usually achieved with a grid-search using a validation set, making solutions dependent on the dataset. 
            The reason for such weighting mechanism lies at the log-likelhoods used as reconstruction losses. For example,  is simply replaced by the corresponding loss functions, such as MSE, without taking the normalisation constants of those likelihood functions into account, resulting in an incorrect balance between reconstruction loss and regularisation. In this paper, we propose the use of the correct form of the log-likelihood, namely the continuous Bernoulli distribution for  and categorical distribution for , with their normalisation constants. Hence, we no longer need the weighting of the KL divergences, making our proposed method simpler to train.\footnote{More detailed information mentioned in~\cref{sec:motivation_cb}}
            \label{remark:avoid_reweighting}
        \end{remark}
    
    \subsection{Practical implementation}
    \label{sec:practical_implementation}

    In practice, the small loss hypothesis is often used to effectively identify the clean samples in a training set~\cite{han2018co, li2020dividemix}. However, naively implementing such hypothesis using a single model might accumulate error due to sample selection bias. One way to avoid such scenario is to train two models simultaneously where each model is updated using only the clean samples selected by the other model. In this paper, we integrate a similar approach into our modelling presented in \cref{sec:probabilistic_modelling} to solve the label noise problem. 
    In particular, we propose to train two models in parallel, resulting in four classifiers (two for the clean label classifier  and the other two for noisy labels , two encoders  and two decoders .

    In CausalNL~\cite{yao2021instance}, co-teaching is used as a way to integrate the small loss hypothesis to regularise the clean label classifiers. Co-teaching might, however, limit the capability of the modelling since it only uses samples classified as clean and ignores the other samples classified as noisy. In addition, co-teaching is initially designed for IIN problems, while our focus is on IDN problems. Hence, we propose to integrate DivideMix~\cite{li2020dividemix}, a method based on the small loss hypothesis as shown in \cref{fig:architecture}. This method starts with a warmup stage, and utilizes all training samples after classifying them as clean and noisy (co-divide) using a two-component Gaussian mixture model (GMM). The training samples are used by MixMatch~\cite{berthelot2019mixmatch} -- a semi-supervised classification technique that considers clean samples as labelled and noisy samples as unlabeled.
    DivideMix shows a reasonable efficacy for IDN problems, as shown in~\cref{tab:cifar}.


    \begin{table*}[htbp!]
        \caption{Test accuracy (\%) of different methods on CIFAR10 and CIFAR100~\cite{krizhevsky2009learning} under various IDN noise rates. Most results are extracted from~\cite{yao2021instance}, while results with \textsuperscript{*} are reported in their respective papers. Results taken from kMEIDTM~\cite{cheng2022instance} are presented with \textsuperscript{\textdagger}. 
}
        \label{tab:cifar}
        \centering
        \begin{tabular}{l c c c c c c c c c c}
        \toprule
        \multirow{2}{*}{\bfseries Model} & \multicolumn{5}{c}{\bfseries IDN - CIFAR10} & \multicolumn{5}{c}{\bfseries IDN - CIFAR100} \\ 
        \cmidrule(lr){2-6} \cmidrule(lr){7-11}
        & \textbf{0.20} & \textbf{0.30} & \textbf{0.40} & \textbf{0.45} & \textbf{0.50} & \textbf{0.20} & \textbf{0.30} & \textbf{0.40} & \textbf{0.45} & \textbf{0.50} \\
        \midrule
        CE~\cite{yao2021instance} & 75.81 & 69.15 & 62.45 & 51.72  & 39.42 & 30.42 & 24.15 & 21.45 & 15.23  & 14.42\\
        Mixup~\cite{zhang2017mixup} & 73.17 & 70.02 & 61.56 & 56.45 & 48.95 & 32.92 & 29.76 & 25.92 & 23.13 & 21.31\\
        Forward~\cite{patrini2017making} & 74.64 & 69.75 & 60.21 & 48.81 & 46.27 & 36.38 & 33.17 & 26.75 & 21.93 & 19.27\\
        T-Revision~\cite{xia2019anchor} & 76.15 & 70.36 & 64.09 & 52.42 & 49.02 & 37.24 & 36.54 & 27.23 & 25.53 & 22.54\\
        Reweight~\cite{liu2015classification} & 76.23 & 70.12 & 62.58 & 51.54 & 45.46 & 36.73 & 31.91 & 28.39 & 24.12 & 20.23\\
        PTD-R-V~\cite{xia2020part}\textsuperscript{*}  & 76.58 & 72.77 & 59.50 & \_ & 56.32 & 65.33\textsuperscript{\textdagger} & 64.56\textsuperscript{\textdagger} & 59.73\textsuperscript{\textdagger} & \_ & 56.80\textsuperscript{\textdagger} \\
        Decoupling~\cite{malach2017decoupling} & 78.71 & 75.17 & 61.73 & 58.61 & 50.43 & 36.53 & 30.93 & 27.85 & 23.81 & 19.59\\
        Co-teaching~\cite{han2018co} & 80.96 & 78.56 & 73.41 & 71.60 & 45.92 & 37.96 & 33.43 & 28.04 & 25.60 & 23.97 \\
        MentorNet~\cite{jiang2018mentornet} & 81.03 & 77.22 & 71.83 & 66.18 & 47.89 & 38.91 & 34.23 & 31.89 & 27.53 & 24.15\\
        CausalNL~\cite{yao2021instance} & 81.79 & 80.75 & 77.98 & 79.53 & 78.63 & 41.47 & 40.98 & 34.02 & 33.34 & 32.13\\
        HOC~\cite{zhu2021clusterability}\textsuperscript{*} & 90.03 & \_ & 85.49 & \_ & \_ & 68.82 & \_ & 62.29 & \_ & \_\\
        CAL~\cite{Zhu_2021_CVPR}\textsuperscript{*} & 92.01 & \_ & 84.96 & \_ & \_ & 69.11 & \_ & 63.17 & \_ & \_\\
        kMEIDTM~\cite{cheng2022instance}\textsuperscript{*} & 92.26 & 90.73 & 85.94 & \_ & 73.77 & 69.16 & 66.76 & 63.46 & \_ & 59.18\\
        DivideMix~\cite{li2020dividemix} & 94.80  & 94.60 & 94.53 & 94.08  & 93.04 & 77.07 & 76.33 & 70.80 & 57.78  & 58.61\\
        \midrule
        \rowcolor{Gray!25} \textbf{InstanceGM}  & \textbf{96.68} & \textbf{96.52} & \textbf{96.36} & \textbf{96.15}  & \textbf{95.90} & \textbf{79.69} & \textbf{79.21} & \textbf{78.47} & \textbf{77.49}  & \textbf{77.19}\\
        \bottomrule
        \end{tabular}
        \vspace{-1em}
    \end{table*} 

    \begin{remark}
        Other instance-dependent methods similar to DivideMix~\cite{li2020dividemix}, such as Contrast-to-Divide~\cite{zheltonozhskii2022contrast}, ELR+~\cite{liu2020early}, can also be integrated into our proposed framework. The reason that DivideMix is used is due to its remarkable performance, especially on the IDN setting, and its publicly available implementation.
    \end{remark}

    In general, the loss function for training the proposed model consists of two losses: one is the loss  from the graphical modelling in \eqref{eq:objective}, and the other is the loss to train DivideMix~\cite[Eq. (12)]{li2020dividemix}, denoted as . The whole loss is represented as:
    
    and the training procedure is summarised in \cref{algorithm:Proposed_Algo} and depicted in \cref{fig:architecture}.
    












    
    
    
    
    
    
     \section{Experiments}
\label{sec:Experimentation}

    In this section, we show the results of extensive experiments on two standard benchmark datasets with IDN, CIFAR10~\cite{krizhevsky2009learning} and CIFAR100~\cite{krizhevsky2009learning} at various noise rates\footnote{Performance degradation at high IDN is presented in~\cref{sec:additional_test}.}, and three real-world datasets, ANIMAL-10N~\cite{song2019selfie}, Red Mini-Imagenet from CNWL~\cite{jiang2020beyond} and CLOTHING-1M~\cite{xiao2015learning}. In \cref{subsec:dataset}, we explain all  datasets mentioned above. In \cref{subsec:implementation}, we discuss all models and their parameters. 
    We compare our approach with state-of-the-art models in IDN benchmarks and real-world datasets
    in \cref{subsec:baseline}. 
    


    \subsection{Datasets}\label{subsec:dataset}
     In both CIFAR10 and CIFAR100, there are  training images and  testing images with each images of size  pixels, where CIFAR10 consists of  classes, CIFAR100 has   classes and both datasets are class-balanced. As CIFAR10 and CIFAR100 datasets do not include label noise by default, we added IDN with noise rates in  following the setup proposed by Xia et al.~\cite{xia2020part}.
     
     Red Mini-Imagenet from CNWL~\cite{jiang2020beyond} is a real-world dataset where images and their corresponding labels are crawled from internet at various controllable label noise rates. This dataset is proposed to study real-world noise in controlled settings. In this work, we focus on Red Mini-ImageNet since it shows a realistic type of label noise. Red Mini-ImageNet has  classes, with each class containing  images sampled from the ImageNet dataset~\cite{imagenet15russakovsky}. The images are resized to  pixels from the original size of  to have a fair comparison with~\cite{cordeiro2021propmix,xu2021faster}. The noise rates vary from  to , but we use the rates , ,  and  to be consistent with the literature~\cite{xu2021faster, yao2021instance, cordeiro2021propmix}.
     
    ANIMAL-10N is another real-world dataset proposed by Song et al.~\cite{song2019selfie}, which contains  animals with  pairs having similar appearances (e.g., wolf and coyote, hamster and guinea pig, etc.). The estimated rate of label noise is . There are  training images   test images. No data augmentation is used, hence the setup is identical to the one proposed in~\cite{song2019selfie}. 
    
    CLOTHING-1M~\cite{xiao2015learning} is a real-world dataset that comprises  training apparel images taken from  categories of online shopping websites. The labels in this dataset are generated from surrounding texts, with an estimated noise of . Due to the inconsistency in image sizes, we follow the standard setup in the literature~\cite{han2019deep, li2020dividemix, cordeiro2021propmix} and resize the images to  pixels. This dataset additionally includes , and  manually validated clean training, validation, and testing data, respectively. During training, the clean training and validation sets are not used and only the clean testing set is used for assessment.
    


    
    \subsection{Implementation}\label{subsec:implementation}
    
    
All the methods are implemented in PyTorch~\cite{paszke2019pytorch}. For the baseline model DivideMix, all the default hyperparameters are considered as mentioned in original paper by Li et al.~\cite{li2020dividemix}.
    All hyperparameter values mentioned below are from CausalNL~\cite{yao2021instance} and DivideMix~\cite{li2020dividemix} unless otherwise specified.
    The size of the latent representation  is fixed at  for CIFAR10, CIFAR100 and Red Mini-Imagenet,  for ANIMAL-10N, and  for CLOTHING-1M. 
    For CIFAR10, CIFAR100 and Red Mini-Imagenet, we used non-pretrained PreaAct-ResNet-18 (PRN18)~\cite{he2016identity} as an encoder. 
    VGG-19 is used as an encoder for ANIMAL-10N, following SELFIE~\cite{song2019selfie} and PLC~\cite{zhang2021learning}. For CLOTHING-1M, we used ImageNet-pretrained ResNet-50. \textit{Clean data is not used for training}. 

\begin{table}[t]
        \centering
        \caption{
        Test accuracy (\%) for Red Mini-Imagenet (CNWL)~\cite{jiang2020beyond}. Other model results are as presented in FaMUS~\cite{xu2021faster} and PropMix~\cite{cordeiro2021propmix}. We presented our proposed results with our proposed InstanceGM and with inclusion of self-supervision~\cite{caron2021emerging} in proposed algorithm (InstanceGM-SS). 
        }
        \label{table:RedMini}
        \begin{tabular}{l p{3em} p{3em} p{3em} l}
            \toprule
            \multirow{2}{*}{\bfseries Method} & \multicolumn{4}{c}{\bfseries Noise rate} \\
            \cmidrule{2-5}
            & \bfseries 0.2 & \bfseries 0.4 & \bfseries 0.6 & \bfseries 0.8 \\
            \midrule
            CE~\cite{xu2021faster} & 47.36 & 42.70 & 37.30 & 29.76 \\
            MixUp~\cite{zhang2017mixup} & 49.10 & 46.40 & 40.58 & 33.58 \\
            DivideMix~\cite{li2020dividemix} & 50.96 & 46.72 & 43.14 & 34.50 \\
            MentorMix~\cite{jiang2020beyond} & 51.02 & 47.14 & 43.80 & 33.46 \\
            FaMUS~\cite{xu2021faster} & 51.42 & 48.06 & 45.10 & 35.50\\
            \rowcolor{Gray!25} \textbf{InstanceGM}  & \textbf{58.38} & \textbf{52.24} & \textbf{47.96} & \textbf{39.62} \\
            \midrule
            \midrule
            \multicolumn{5}{l}{\bfseries With self-supervised learning}\\
            \midrule
PropMix~\cite{cordeiro2021propmix} & \textbf{61.24} & 56.22 & 52.84 & 43.42\\
            \rowcolor{Gray!25} \textbf{InstanceGM-SS}\tablefootnote{Implementation details are present in~\cref{sec:self_supervision}}  & 60.89  & \textbf{56.37} & \textbf{53.21} & \textbf{44.03} \\ \bottomrule
        \end{tabular}
\end{table}

    The training of the model used stochastic gradient descent (SGD) for DivideMix stage with momentum of , batch size of  and an L2 regularisation whose parameter is . Additionally, Adam is used to train the VAE part of the model. The training runs for  epochs for CIFAR10, CIFAR100, Red Mini-Imagenet and ANIMAL-10N. The learning rate is  which is reduced to  at half of the number of training epochs. The WarmUp stage lasts for  epochs for CIFAR10,  for CIFAR100, ANIMAL-10N and Red Mini-Imagenet. For CLOTHING-1M, the WarmUp stage lasts  epoch with batch size of , and training runs for  epochs and learning rate of  decayed by a factor of  after the  epoch .





    For CIFAR10, CIFAR100~\cite{krizhevsky2009learning}, Red Mini-Imagenet~\cite{jiang2020beyond} and ANIMAL-10N~\cite{song2019selfie}, the encoder has a similar architecture as CausalNL~\cite{yao2021instance}, with  hidden convolutional layers and feature maps containing  and  features. In the decoding stage, we use  hidden layer transposed-convolutional network and the feature maps have  and  features. In Red Mini-Imagenet, we use a similar architecture as CIFAR100 with and without self-supervision~\cite{caron2021emerging}. 
    For CLOTHING-1M~\cite{xiao2015learning}, we use encoder networks with  convolutional layers, and the feature maps contain  and  features. The decoder networks have  transposed-convolutional layers and the feature maps have  and  features.
    
    \subsection{Comparison with Baselines and Measurements}\label{subsec:baseline}

    In this section, we compare our proposed InstanceGM on baseline IDN benchmark datasets in Section~\ref{subsubsec:benchmark}, and we also validate our proposed model on various real-world noisy datasets in Section~\ref{subsubsec:real_world}.
    
    \subsubsection{Instance-Dependent Noise Benchmark Datasets}\label{subsubsec:benchmark}
    
    The comparison between our InstanceGM and recently proposed approaches on CIFAR10 and CIFAR100 IDN benchmarks is shown in Table~\ref{tab:cifar}. 
    Note that the proposed approach achieves considerable improvements in both datasets at various IDN noise rates ranging from  to . 
    Given that CausalNL represents the main reference for our method, it is important to compare the performance of the two approaches.  For CIFAR10, our method is roughly  better in all noise rates, and for CIFAR100, our method is between  and  better.
    Compared to the current state-of-the-art methods in this benchmark (kMEIDTM~\cite{cheng2022instance} and DivideMix~\cite{li2020dividemix}), our method is around  better in CIFAR10 and between  to almost  better in CIFAR100.
    
    \begin{table}[t]
        \centering
        \caption{Test accuracy (\%) of different methods evaluated on ANIMAL-10N~\cite{song2019selfie} where only noisy data are used to train models. Other models' results are as presented in  Nested-CE~\cite{chen2021boosting}, and results with \textsuperscript{*} are reported in their respective papers}
        \label{table:Animal10N}
        \begin{tabular}{l c}
            \toprule
            \bfseries Method & \bfseries Test Accuracy (\%) \\
            \midrule
            CE~\cite{zhang2021learning} & 79.4 \\
            Nested-Dropout~\cite{chen2021boosting} & 81.3 \\
            CE+Dropout~\cite{chen2021boosting} & 81.3\\
            SELFIE~\cite{song2019selfie}\textsuperscript{*} & 81.8 \\
            PLC~\cite{zhang2021learning}\textsuperscript{*} & 83.4 \\
            Nested-CE~\cite{chen2021boosting} & 84.1 \\
            \midrule
            \rowcolor{Gray!25} \textbf{InstanceGM} & \textbf{84.6} \\
            \bottomrule
        \end{tabular}
\end{table}


    
    
    


    


    \subsubsection{Real-world Noisy Datasets}\label{subsubsec:real_world}
    In \cref{table:Animal10N,table:RedMini,table:clothing1M}, we present the results on ANIMAL-10N, Red Mini-Imagenet and CLOTHING-1M, respectively. In general the results show that InstanceGM outperforms or is competitive with the present state-of-the-art models for large-scale web-crawled datasets and small-scale human-annotated noisy datasets.
\cref{table:Animal10N} reports the classification accuracy on ANIMAL-10N.
We can observe that  InstanceGM achieves slightly better performance than all other baselines. 
    For the other real-world datasets Red Mini-Imagenet and CLOTHING-1M, InstanceGM is competitive, as shown in \cref{table:RedMini,table:clothing1M}, demonstrating its ability to handle real-world IDN problems. In particular, ~\cref{table:RedMini} shows the results on Red Mini-Imagenet using two set-ups: 1) without pre-training (top part of the table), and 2) with self-supervised (SS) pre-training (bottom part of the table).
    The SS pre-training is based on DINO~\cite{caron2021emerging} with the unlabelled Red Mini-Imagenet dataset, allowing a fair comparison with PropMix~\cite{cordeiro2021propmix}, which uses a similar SS pre-training.
Without SS pre-training, our InstanceGM is substantially superior to recently proposed approaches.
    With SS pre-training, results show that InstanceGM can improve its performance, allowing us to achieve state-of-the-art results on Red Mini-Imagenet.
    


    


    \begin{table}
        \centering
        \caption{Test accuracy (\%) for  competing methods on CLOTHING-1M~\cite{xiao2015learning}. The accuracy of the baseline models (CausalNL and DivideMix)  are in italics. 
        Results of other models are from their respective papers. In the experiments only noisy labels are use for training. Top results with  accuracy are highlighted in bold.}
        \label{table:clothing1M}
        \begin{tabular}{l c}
            \toprule
            \bfseries Method & \bfseries Test Accuracy (\%) \\
            \midrule
CausalNL~\cite{yao2021instance} & \textit{72.24} \\
            IF-F-V~\cite{jiang2021information} & 72.29\\
\textbf{DivideMix}~\cite{li2020dividemix} & \textit{\textbf{74.76}} \\
\textbf{Nested-CoTeaching}~\cite{chen2021boosting} & \textbf{74.90}\\
\midrule
            \rowcolor{Gray!25} \textbf{InstanceGM}  & \textbf{74.40} \\
            \bottomrule
        \end{tabular}
\end{table}
    
    
    


    
    
    \section{Ablation Study}
    
    We show the ablation study of our proposed method on CIFAR10~\cite{krizhevsky2009learning}, under IDN noise rate of  and ANIMAL-10N~\cite{song2019selfie}. On Table~\ref{tab:ablation_cifar}, the performance of CausalNL~\cite{yao2021instance} is relatively low, which can be explained by the small number of clean samples used by co-teaching~\cite{han2018co}, and  the use of MSE for image reconstruction loss\footnote{line 80 in https://github.com/a5507203/IDLN/blob/main/causalNL.py}.
We argue that replacing co-teaching~\cite{han2018co} by DivideMix~\cite{li2020dividemix} will improve classification accuracy because it allows the use of the whole training set. 
    To demonstrate that, we take CausalNL~\cite{yao2021instance} and replace its co-teaching by DivideMix, but keep the MSE reconstruction loss -- this model is named CausalNL + DivideMix (w/o continuous Bernoulli).
    Note that this allows a  accuracy improvement from CausalNL, but the use of MSE reconstruction loss can still limit classification accuracy.
    Hence, by replacing the MSE loss by the continuous Bernoulli loss for image reconstruction, we notice a further  accuracy improvement.  


    \begin{table}[t]
        \centering
        \caption{This ablation study shows the test accuracy  on CIFAR10 under IDN at noise rate . First, we show the result of CausalNL~\cite{yao2021instance}. Second, we show the result of CausalNL~\cite{yao2021instance} with Co-teaching~\cite{han2018co} replaced by DivideMix~\cite{li2020dividemix} (without Continuous Bernoulli reconstruction). Then at last we show the results of our proposed algorithm InstanceGM. 
}
        \label{tab:ablation_cifar}
        \begin{tabular}{l c}
            \toprule
            \bfseries Method &  \bfseries Test Accuracy (\%) \\
            \midrule
CausalNL~\cite{yao2021instance} & 78.63 \\
            CausalNL~\cite{yao2021instance} + DivideMix~\cite{li2020dividemix}  & \multirow{2}{*}{88.62} \\
            \quad * w/o continuous Bernoulli   & \\
            \rowcolor{Gray!25} \textbf{InstanceGM} & \textbf{95.90} \\
            \bottomrule
        \end{tabular}
    \end{table}
    
    For ANIMAL-10N~\cite{song2019selfie}, we test InstanceGM with various backbone networks (VGG~\cite{simonyan2014very}, ResNet~\cite{he2016deep}, and ConvNeXt~\cite{liu2022convnet}) and the results are displayed in~\cref{tab:ablation_animal}. 
Due to the architectural differences, ConvNeXt~\cite{liu2022convnet} performed best on our proposed algorithm, but for a fair comparison with the other models, we use the VGG backbone~\cite{simonyan2014very} results in~\cref{table:Animal10N}.
    
    
    
    
    \begin{table}[t]
        \centering
        \caption{This ablation study shows the test accuracy  on ANIMAL-10N using various architectures (without self-supervision), including ResNet~\cite{he2016deep}, VGG~\cite{simonyan2014very} and ConvNeXt~\cite{liu2022convnet} with InstanceGM. \cref{table:Animal10N}, reported the results of VGG~\cite{simonyan2014very} to provide a fair comparison with other methods.}
        \label{tab:ablation_animal}
        \begin{tabular}{l  c}
            \toprule
            \bfseries Method & \bfseries Test Accuracy (\%) \\
            \midrule
            InstanceGM with ResNet~\cite{he2016deep} & 82.2 \\
            InstanceGM with VGG~\cite{simonyan2014very} & 84.6\\
            InstanceGM with ConvNeXt~\cite{liu2022convnet} & 84.7\\
            \bottomrule
        \end{tabular}
\end{table}
     \section{Conclusion}
\label{sec:conclusion}



In this paper, we presented an instance-dependent noisy label learning algorithm method, called InstanceGM. 
InstanceGM explores generative and discriminative models~\cite{yao2021instance}, where for the generative model, we replace the usual MSE image reconstruction loss by the continuous Bernoulli reconstruction loss~\cite{loaiza2019continuous} that improves the training process, and for the discriminative model, we replace co-teaching by DivideMix~\cite{li2020dividemix} to enable the use of clean and noisy samples during training.
We performed extensive experiments on various IDN benchmarks, and our results on CIFAR10, CIFAR100, Red Mini-Imagenet, ANIMAL-10N outperform the results of state-of-the-art methods, particularly in high noise rates and are competitive for CLOTHING-1M. 
The ablation study clearly shows the importance of the new continuous Bernoulli reconstruction loss~\cite{loaiza2019continuous} and DivideMix~\cite{li2020dividemix}, with both improving classification accuracy from CausalNL~\cite{yao2021instance}.
%
     
    
    {\small
    \bibliographystyle{ieee_fullname}
    \bibliography{references}
    }
    
    \onecolumn
\appendix
\section*{Appendix}
The appendix is organised as follows:
\begin{itemize}
\item \cref{sec:self_supervision} presents a detailed description of InstanceGM-SS and experimental details of self-supervision. \cref{subsec:exp_self_dino} contains the experimental details for the self-supervision method DINO, and \cref{subsec:exp_self_instances} contains the experimental details of InstanceGM with self-supervision (InstaceGM-SS).
    \item \cref{sec:motivation_cb} shows the motivation behind the use of the continuous Bernoulli distribution.
\item \cref{sec:additional_test} shows the results on high IDN rates for CIFAR10 dataset. 


\end{itemize}









\section{Self-supervision and experimental details}\label{sec:self_supervision}


    In addition to training the proposed method from scratch, we also adapt self-supervised learning to pre-train the feature extractor part in the classifier , denoted as InstanceGM-SS in \cref{table:RedMini}. In particular, we employ DINO~\cite{caron2021emerging} to self-supervisedly learn a feature extractor using the unlabelled data from the training set of Red Mini-Imagenet (DINO is a self-supervision method that uses self-distillation). Such integration allows our proposed method to be fairly compared with other label noise learning approaches that rely on self-supervision, such as PropMix~\cite{cordeiro2021propmix}.

    \subsection{Experimental details of self-supervision DINO}\label{subsec:exp_self_dino}
        We trained the  self-supervised model on Red Mini-Imagenet for  epochs on  PreAct-ResNet-18 (PRN18). We used the same set of hyper-parameters as provided by DINO. The method follows the teacher-student setting where the weights of the teacher network are exponentially weighted averaged from the student network~\cite{he2020momentum}. It includes the teacher model temperature for warmup as  and  for training, and warmup teacher epochs as . The L2 weight decay regularisation is , and batch size is  per gpu. The initial learning rate is set to  and the minimum learning rate is set to , with the training warm up number of epochs set as . In addition,DINO needs various augmented views of the input image. It includes multi-crop strategy~\cite{caron2020unsupervised} with high-resolution global and low-resolution local views.
        The two versions of global crops views are considered with scale values of  and . Moreover, the six different local crops views are considered having scale values of  and , with teacher momentum as . 

    \subsection{Experimental details of InstanceGM-SS}\label{subsec:exp_self_instances}
        When we use the self-supervised trained classifier for InstanceGM-SS, we slightly change  the settings to train the Red Mini-Imagenet, and results could be find in~\cref{table:RedMini}. 
In particular, we use the self-supervised  PreAct-ResNet-18 as a classifier with the latent representation Z of size . We train the network for  epochs with the learning rate reduced by 10 after  epochs. The warmup stage is reduced to  epochs. Otherwise, the previous settings for Red Mini-ImageNet without self-supervision are kept the same.




\section{Motivation of using continuous Bernoulli distribution}\label{sec:motivation_cb}
    To explain the motivation behind the use of the continuous Bernoulli likelihood for image reconstruction, we refer to the variational inference technique. In particular, we denote  as an observable variable, e.g., input images, while  as a hidden (or latent) variable. For simplicity, we assume that both  and  are scalars. In variational inference, e.g. VAE, the objective is to maximise the evidence lower bound (ELBO) or minimise the variational-free energy w.r.t.  -- the parameter of the variational posterior :
    
    where  is the prior of  (for example, a standard Gaussian distribution ),   is a re-weighting factor. In theory, , and we use  here to explain common practice in VAE which is described in the following.

    The second term in \eqref{eq:vae_reconst} could be evaluated with a closed-form formula for some simple cases of  and  or approximated using Monte Carlo sampling. Thus, we would focus on the explanation of the first term -- often known as reconstruction loss. Depending on how  is modelled, we could have different reconstruction losses, as explained below.

    \subsection{Gaussian likelihood}
        If  is a Gaussian distribution: , the negative log-likelihood term in \eqref{eq:vae_reconst} can be written as:
        
        The correct form of the reconstruction loss in \eqref{eq:mse_full} contains two terms including a \say{weighted} MSE term. However, common practice simply replaces the whole  by , resulting in an incorrect formula. As a result, it requires to fine-tune  to some small value to balance the contributions of the first and second terms in \eqref{eq:vae_reconst}.

    \subsection{Bernoulli likelihood}
        If  is a Bernoulli distribution:  where  and , then the negative log-likelihood in \eqref{eq:vae_reconst} is: 
        
        resulting in the binary cross-entropy loss (BCE)~\cite{creswell2017denoising}.
        
        Simply implementing the reconstruction loss as BCE results in the pervasive error since the input  must be in \{0, 1\}~\cite{loaiza2019continuous}, which is applicable for black and white images only.



    \subsection{Continuous Bernoulli likelihood}
        For colour images, although one can model  as a Gaussian distribution shown in \eqref{eq:mse_full}, it might be a suboptimal choice since the support of the Gaussian distribution is un-bounded, while image data is bounded. Thus, we use the continuous Bernoulli distribution to model ~\cite{loaiza2019continuous} since the continuous Bernoulli distribution is supported in  with only one parameter:

        
        Note that one could also use the Beta distribution whose support space is also . The advantage of using the continuous Bernoulli distribution is the simplicity since we need only one parameter per pixel, while the Beta distribution requires double the number of parameters.




















\section{Experimental Results on CIFAR10 at High IDN Levels}\label{sec:additional_test}
We investigated the performance of InstanceGM on high IDN levels including  and . We provided the test classification accuracy on CIFAR10 on~\cref{table:high_cifar}. The competing model results are from~\cite{jiang2021information}. Our InstanceGM shows superior results even in such high noise rate problems. Note that all results at these high noise rate problems are not good but the performance degradation for InstanceGM is lower, compared to the other models.

\begin{table}[h]
        \centering
        \caption{
        Test accuracy (\%) for CIFAR10 at high IDN rates. All the mentioned results of other methods are as presented in the paper~\cite{jiang2021information}. 
}
        \label{table:high_cifar}
        \begin{tabular}{l p{3em} p{3em} p{3em} l}
            \toprule
            \multirow{2}{*}{\bfseries Method} & \multicolumn{3}{c}{\bfseries IDN - CIFAR10} \\
            \cmidrule{2-4}
            & \bfseries 0.7 & \bfseries 0.8 & \bfseries 0.9 \\
            \midrule
PTD-F-V~\cite{xia2020part} & 20.35 & 13.58 & 09.44\\
PTM-F-V~\cite{jiang2021information} & 18.95 & 13.89 & 10.57\\
IF-F-V~\cite{jiang2021information} & 21.09 & 16.72 & 10.86\\
            DivideMix~\cite{li2020dividemix} & 22.13 & 08.10 & 04.08\\
            \midrule
            \rowcolor{Gray!25} \textbf{InstanceGM}  & \textbf{47.23} & \textbf{29.30} & \textbf{11.01}\\ \bottomrule
        \end{tabular}
    \end{table}







%
 
\end{document}
