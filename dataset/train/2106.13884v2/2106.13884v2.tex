Our experiments are designed to quantify three capacities that should be characteristic of a Multi-Modal Few-Shot Learner: \emph{rapid adaptation} to new tasks, fast access to~\emph{general knowledge} and \emph{fast binding} of visual and linguistic elements. We train \Model on Conceptual Captions, a public dataset that consists of around three million image-caption pairs \cite{sharma2018conceptual}. We do early stopping on the validation set perplexity which usually reaches an optimum just after a single epoch with batch size 128. All experiments used the Adam optimizer with $\beta_1 = 0.9$ and $\beta_2 = 0.95$ and a constant learning rate of $3e\text{-}4$ unless otherwise noted. We operate on 224$\times$224 images at both train and test-time. Images which are not square are first padded with zeroes to square and then resized to 224$\times$224.

\subsection{Rapid Task Adaptation}
\label{sec:rapid_adaptation}

We first examine zero-shot and few-shot generalization from captioning to visual question-answering. This is a type of \emph{rapid adaptation} from captioning behaviour to question-answering behaviour with either simple prompting alone or few-shot learning, analogous to transfer from language modelling to open-domain question-answering \cite{roberts2020much} in the vision plus language domain. We evaluate on the VQAv2 \cite{goyal2017making} validation set.

\paragraph{Zero-shot transfer from captioning to VQA}

Captioning training can transfer moderately well to visual question-answering in the zero-shot setting with no training or in-context examples at all. The strength of the pre-trained language model is a double-edged sword. It powers the generalization abilities of \Model but also enables the model to perform surprisingly well without considering the visual input at all. To guard against this possibility we also train blind baselines, in which the image presented to the visual encoder is blacked out, but the convnet weights are still trained. This amounts to prefix tuning \cite{li2021prefix}. We outperform this blind baseline which also inherits the few-shot learning abilities of the language model.

In these experiments we also include two additional and important baselines: $\text{\Model}_{\,\text{finetuned}}$ in which the language model is instead finetuned starting from the pretrained weights and $\text{\Model}_{\,\text{scratch}}$, wherein the whole system is trained from scratch end-to-end. These baselines preferred a smaller learning rate of $1e\text{-}5$. Results in \autoref{tab:vqa_table} show that keeping the language model frozen generalizes substantially better to visual question-answering than finetuning. The model trained from scratch is not able to transfer at all from captioning to VQA; we interpret this to suggest that the tremendous generalization abilities of large language models are reliant upon large-scale training datasets in which the task of predicting the next token mimics the test setting (here question-answering) with non-negligible frequency.

\begin{table}
\begin{floatrow}
\hspace*{-0.25cm}
\capbtabbox{
\begin{tabular}{l||c|c|c|c}
\textbf{n-shot Acc.}     & \textbf{n=0} & \textbf{n=1} & \textbf{n=4} & $\tau$ \\ \toprule
\textbf{\Model} & 29.5 & 35.7 & 38.2 & \xmark \\
\textbf{\Model$_{\text{scratch}}$} & 0.0 & 0.0& 0.0 & \xmark \\
\textbf{\Model$_{\text{finetuned}}$} & 24.0& 28.2& 29.2 & \xmark \\
\textbf{\Model$_{\text{train-blind}}$} &26.2 &33.5 & 33.3 & \xmark \\
\midrule
\textbf{\Model$_{\text{VQA}}$} &48.4 & -- & -- & \cmark \\
\textbf{\Model$_{\text{VQA-blind}}$} & 39.1 &  -- &   -- & \cmark \\
\midrule
\textbf{Oscar \cite{oscar}}   &            73.8 &         -- &         -- & \cmark \\ \bottomrule
\end{tabular}
}{
\caption{Transfer from Conceptual Captions to VQAv2. The $\tau$ column indicates whether a model uses training data from the VQAv2 training set. The row denoted \Model$_{\text{train-blind}}$ is the blind baseline described in \autoref{sec:rapid_adaptation}. \Model$_{\text{VQA}}$ is a baseline which mixes in VQAv2 training data.}
\label{tab:vqa_table}
}
\capbtabbox{
\begin{tabular}{l||c|c|c|c}
\textbf{n-shot Acc.} & \textbf{n=0} & \textbf{n=1} & \textbf{n=4} & $\tau$ \\ \toprule
\textbf{\Model} & 5.9 & 9.7 & 12.6 & \xmark \\
\textbf{\Model$_{\text{400mLM}}$} & 4.0 & 5.9 & 6.6 & \xmark \\
\textbf{\Model$_{\text{finetuned}}$} & 4.2 & 4.1 & 4.6 & \xmark \\
\textbf{\Model$_{\text{train-blind}}$} & 3.3 & 7.2 & 0.0 & \xmark \\
 \midrule
\textbf{\Model$_{\text{VQA}}$} & 19.6 & -- & -- & \xmark \\
\textbf{\Model$_{\text{VQA-blind}}$} & 12.5 & -- & -- & \xmark \\
\midrule
\textbf{MAVEx \cite{wu2021multimodal}} & 39.4 & -- &-- & \cmark \\
\bottomrule
\end{tabular}
}{
\caption{Transfer from Conceptual Captions to OKVQA. The $\tau$ column indicates if a model uses training data from the OKVQA training set. \Model does not train on VQAv2 except in the baseline row, and it never trains on OKVQA.}
\label{tab:okvqa_table}
}
\end{floatrow}
\end{table} 

\paragraph{Improving performance with few-shot learning}

This zero-shot transfer to visual question-answering via prompting improves by presenting examples to the model in-context. We repeat the previous experiments with up to four examples of image-question-answer triples shown to the model as conditioning information in the continuous prompt sequence (using the interface in \autoref{fig:method-testing}).  We present these few-shot results compared to mixing in data from the VQAv2 training set -- for SGD training -- in \autoref{tab:vqa_table}. Of course, few-shot learning on four examples is outperformed by SGD on tens of thousands of examples, but few-shot performance clearly improves with more examples and goes a decent way toward closing the gap from zero-shot performance (29.5\%) to full SGD training performance (48.4\%). With just four examples the gap is closed almost halfway at 38.2\%.

There are two important takeaways from the results presented in this section. First, they show that training a visual encoder through a pretrained and frozen language model results in a system capable of strong out-of-distribution (zero-shot) generalization. Second, they confirm that the ability to rapidly adapt to new tasks given appropriate prompts is inherited from the pretrained language model and transfers directly to multimodal tasks.

\subsection{Encyclopedic Knowledge}\label{sec:encyc_knowledge}

Here we study the extent to which \Model can leverage the encyclopedic knowledge in the language model towards visual tasks. The Conceptual Captions dataset is \emph{hypernymed} meaning that e.g.\ proper names are replaced with a general word like {\it person}. This enables us to rigorously study the transfer of factual knowledge because all knowledge of named entities comes from language model pretraining.

Consequently, when we show the model an image of an airplane and ask ``who invented this?'' (\autoref{fig:headline}), the visual encoder has determined that the image contains an airplane, and the language model has used this to retrieve the factual knowledge that airplanes were invented by the Wright brothers, a fact which is referenced in the C4 training set through (text-only) articles about airplanes. This is a fascinating chain of deduction. A detailed analysis of this behaviour with more examples is included in the Appendix (e.g.\ \autoref{fig:knowledge_wine}, \autoref{fig:knowledge_emoji}, \autoref{fig:wine_really}).

We bolster this finding quantitatively by evaluating performance on OKVQA \cite{marino2019ok}, a visual question-answering dataset designed to require outside knowledge in order to answer correctly. The pretrained language model's command of factual knowledge is of course dependent upon its scale, so we examine the performance of \Model using pretrained language models of varying sizes: the base model with 7 billion parameters, and a much smaller 400 million parameter language model pretrained on the same dataset. \autoref{tab:okvqa_table} shows the results: task performance scales with model size. Again finetuning performs worse than leaving the model frozen in terms of generalization performance. We stress that \Model is never trained on OKVQA.

\subsection{Fast Concept Binding}
In the multi-modal setting, fast-binding refers to a model's ability to associate a word with a visual category in a few shots and immediately use that word in an appropriate way.

\paragraph{Open-Ended miniImageNet and Real-Name miniImageNet}

To quantify the fast-binding capacity of of \Model, we evaluate it on the minImageNet meta-learning task \cite{vinyals2016matching}. Note that there are important differences with how we attempt miniImageNet and how it is approached in previous work. First, unlike standard meta-learning, we do not train \Model on the (meta) task. Second, we evaluate \Model in an open-ended fashion, where it must successfully generate a correct category name (and then the EOS token) in order to be credited with a correct answer. Finally, although we use the same image classes as the miniImageNet test set, they are at higher resolution (224$\times$224) and with class labels replaced with nonsense words (`dax', `blicket' etc). This allows the system to express its answers with word-like tokens. We refer to this task as \emph{Open-Ended miniImageNet}, and it mimics closely the standard miniImagenet setting used elsewhere. To assess how much difficulty is added by binding visual categories to nonsense words versus simply adapting to an image recognition task \emph{per se}, we also consider a version --  Real-Name miniImagenet -- in which visual categories in both the support set and the answer retain their original names. See \autoref{fig:vocab_mi_fvqa}a for an illustration. 

On both versions of this evaluation, we experiment by exposing the model to different numbers of inner-shots, repeats and task induction. On two-way Open-Ended miniImagenet, we observe that when \Model is presented with a sequence of images and descriptions of new names for them, it is able to learn new names for the objects presented and then use these new names immediately with substantially above chance accuracy. Importantly, the ability of the model to use these new words improves with with more examples of the corresponding category. Notably, this upward trend is more pronounced when this supporting information involves different exemplars from the visual category (inner-shots) rather than repetitions of a single exemplar (repeats). The fast-binding capacities of the model can thus be improved with richer and more varied visual support or prompting.

On two-way Real-Name miniImagenet, we observe a similar trend but with higher absolute performance. This underlines the difficulty in Open-Ended miniImagenet introduced by having to assign novel words to categories that may otherwise be already known to the model, and because the real names may carry visual information leveraged from the captioning data the model was trained on. 

In \autoref{tab:mi5}, we show that the observed effects on Open-Ended miniImagenet do not transfer to the 5-way setting, where \Model is not significantly above chance. This shows that learning to bind five new names to five visual categories in a single forward pass is beyond the current capabilities of \Model. As before, however, we do observe an upward trend in the model's capacity to return the actual name for a visual category among the five possibilities as the number of inner-shots or repeats increases. Further work is required and we look forward to progress in this more challenging setting.

\begin{table}[h]
\centering
\begin{tabular}{l||l|lll|lll}
\textbf{Task Induction}                         & \xmark    & \cmark    & \cmark             & \cmark             & \cmark    & \cmark             & \cmark             \\
\textbf{Inner Shots}                            & 1    & 1    & 3             & 5             & 1    & 1             & 1             \\
\textbf{Repeats}                          & 0    & 0    & 0             & 0             & 1    & 3             & 5             \\ \midrule
\textbf{\Model}                 & 29.0 & 53.4 & 57.9 & 58.9 & 51.1 & 57.7 & 58.5 \\
\textbf{\Model (Real-Name)}     & 1.7  & 33.7 & 66   & 66   & 63   & 65   & 63.7          \\ \midrule
\textbf{\Model$_{\text{test-blind}}$}    & --   & 48.5 & 46.7          & 45.3          & --   & --            & --            \\
\textbf{\Model$_{\text{test-blind}}$ (Real-Name)} & --   & 1.0  & 12.6          & 33.0          & --   & --            & --            \\
\textbf{ANIL Baseline \cite{raghu2019rapid}}                                   & --   & 73.9 & 81.7          & 84.2          & --   & --            & -- \\ \bottomrule          
\end{tabular}
\vspace{0.cm}
\caption{Performance of \Model and baselines on Open-Ended miniImageNet 2-Way Tasks. Randomly picking between the two class labels (then emitting the EOS token) would yield 50\% accuracy. As the model has to generate the answer, and is not counted correct if it paraphrases, this is not the best blind baseline, which is why we include open-ended blind baselines that also generate.}
\label{tab:mi2}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{l||l|lll|lll}
\textbf{Task Induction}                                  & \xmark    & \cmark    & \cmark             & \cmark             & \cmark             & \cmark             & \cmark    \\
\textbf{Inner Shots}                                     & 1    & 1    & 3             & 5             & 1             & 1             & 1    \\
\textbf{Repeats}                                   & 0    & 0    & 0             & 0             & 1             & 3             & 5    \\ \midrule
\textbf{\Model}                          & 18.0 & 20.2 & 22.3 & 21.3 & 21.4 & 21.6 & 20.9 \\
\textbf{\Model (Real-Name)}              & 0.9  & 14.5 & 34.7 & 33.8 & 33.8 & 33.3 & 32.8 \\ \midrule
\textbf{\Model$_{\text{test-blind}}$}             & --   & 18.6 & 19.9          & 19.8          & --            & --            & --   \\
\textbf{\Model$_{\text{test-blind}}$ (Real-Name)} & --   & 4.6  & 22.6          & 20.8          & --            & --            & --   \\
\textbf{ANIL Baseline \cite{raghu2019rapid}}                                                     & --   & 45.5 & 57.7          & 62.6          & --            & --            & --  \\ 
\bottomrule
\end{tabular}
\caption{Performance of \Model and baselines on Open-Ended miniImageNet 5-Way Tasks. Randomly picking between the five class labels (then emitting the EOS token) would yield 20\% accuracy.}
\label{tab:mi5}
\vspace*{-0.1cm}
\end{table}

\paragraph{Fast-VQA and Real-Fast-VQA}

As transformers are trained to model text, their attention weights learn to associate -- or `bind'-- pairs of words across sentences. The experiments with miniImageNet show that this capacity can transfer directly to binding visual categories to their names, enabling the system to generate the name on demand. This raises the question of whether \Model can integrate a newly-acquired visual category (and its names) more fully into the model's language system, so that it can, for instance, describe or answer questions about that  category. 

To test this capacity, we constructed a new task -- \emph{Fast-VQA} -- out of two well-known datasets, ImageNet \cite{russakovsky2015imagenet} and Visual Genome \cite{krishna2017visual}. For each question, the model is presented with nonsense words (`dax' and `blicket') and $n$ images of the referents of those words (e.g.\ of a `cat' or a `dog') taken from ImageNet. It is then asked a question containing at least one of those two words, about a further image (taken from Visual Genome) in which \emph{both} of the referents appear (see \autoref{fig:vocab_mi_fvqa}b). As with miniImagenet, the words `dax' and `blicket' (and how they refer) should be new to \Model, but the corresponding visual categories may be known from the Conceptual Captions training data, albeit by different names.

To quantify how much harder the introduction of new words for known categories makes this task, we also created a variant (\emph{Real-Fast-VQA}) in which the original category names (`cat' or `dog') are used instead of `dax' and `blicket'. Real-Fast-VQA is a special case of VQA involving questions from Visual Genome, in which a model is reminded what the important entities in the question look like prior to answering the question. \emph{Real-Fast-VQA} does not require the same ability to bind categories to new words, but it does measure how well a model can exploit task-relevant multimodal guidance when attempting a new task in an otherwise zero-shot manner.

Fast-VQA and Real-Fast-VQA are very challenging tasks because they are attempted without task-specific training, and because the underlying questions come from Visual Genome (VQAv2 images do not come with the necessary meta-data to construct the task). Visual Genome questions are particularly challenging because only a single answer exists for each question. When scoring models, for simplicity we credit only an exact match with the output generated by the model, modulo the same post-processing applied for VQAv2. Because of the inherent difficulty of the task, we use strong baselines to verify strength of observed effects. The Fast-VQA and Real-Fast-VQA evaluation sets will be provided with the camera ready version of this manuscript, as a resource to stimulate further research on multimodal fast-binding, together with training data (not used in this work).

\begin{table}[h]
\centering
\begin{tabular}{l||llllllll}
\multicolumn{1}{l}{}                 & \multicolumn{4}{c}{\bf Fast-VQA}     & \multicolumn{4}{c}{\bf Real-Fast-VQA}        \\
\multicolumn{1}{l||}{\bf Inner Shots}       & 0   & 1   & 3   & \multicolumn{1}{l|}{5}   & 0   & 1   & 3    & \multicolumn{1}{l}{5}    \\ \toprule
\multicolumn{1}{l||}{\bf \Model}       & 1.6 & 2.8 & 7.0 & \multicolumn{1}{l|}{7.9} & 3.7 & 7.8 & 10.1 & \multicolumn{1}{l}{10.5} \\
\multicolumn{1}{l||}{\bf \Model$_{\text{train-blind}}$} & 0.7 & 0.3 & 1.3 & \multicolumn{1}{l|}{0.4} & 1.9 & 2.3 & 3.7  & \multicolumn{1}{l}{3.7} \\
\bottomrule
\end{tabular}
\caption{Performance of \Model versus an equivalent blind model on Fast and Real-Fast VQA.}
\label{tab:dax-vqa-results}
\end{table}

As shown in \autoref{tab:dax-vqa-results}, the fact that the model improves with more shots in both Fast-VQA and Real-Fast-VQA confirms that \Model has some capacity to integrate novel words into its general capacity to process and generate natural language in a multimodal context. It is notable that a prefix-tuned model with no access to images improves moderately at Real-Fast-VQA as more concepts are presented, showing that additional linguistic cues (just being reminded of the words involved and the linguistic form of the task) goes some way to preparing for the upcoming question. As exemplified in \autoref{fig:vocab_mi_fvqa}, inspection of the model output confirms that in many cases it is indeed the multimodal (and not just linguistic) support that enables \Model to improve performance as the number of shots increases.