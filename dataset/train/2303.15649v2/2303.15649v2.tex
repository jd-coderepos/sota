
\RequirePackage{fix-cm}

\RequirePackage{snapshot}

\documentclass[twocolumn]{svjour3}          \smartqed  \usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\newcommand{\minisection}[1]{\vspace{0.04in} \noindent {\bf #1}\ \ }
\usepackage{subcaption}
\captionsetup{compatibility=false}
\usepackage{multirow}
\usepackage{multicol}


\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{$\eta$}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}



\newcommand{\vdelta}   {\boldsymbol{\delta}}
\newcommand{\mDelta}{\boldsymbol{\Delta}}
\newcommand{\mH}{\mathbf{H}}
\newcommand{\vg}{\mathbf{g}}


\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 \usepackage[table]{xcolor}
\newcommand{\loss}{\mathcal{L}}
\newcommand{\kldiv}{\mathrm{D}_{\mathrm{KL}}}
\usepackage[export]{adjustbox}\usepackage{tabularx}\usepackage{booktabs}  \usepackage{subcaption} \usepackage{array}\usepackage[inline]{enumitem}
\usepackage{cuted}
\usepackage{makecell}
\usepackage{algorithm}
\usepackage[algo2e]{algorithm2e}

\newcommand{\LH}[1]{{\color{green}{\bf LH:} #1}}
\newcommand{\YX}[1]{{\color{orange}{\bf YX:} #1}}
\newcommand{\JW}[1]{{\color{magenta}{\bf JW:} #1}}
\newcommand{\SM}[1]{{\color{cyan}{\bf SM:} #1}}
\newcommand{\red}[1]{{\color{red} #1}}
\usepackage{soul, xcolor}
\setstcolor{red}




\usepackage{array}
\makeatletter
\newcommand*{\@rowstyle}{}
\newcommand*{\rowstyle}[1]{\gdef\@rowstyle{#1}\@rowstyle\ignorespaces }

\newcolumntype{=}{>{\gdef\@rowstyle{}}}

\newcolumntype{+}{>{\@rowstyle}}

\makeatother


\usepackage[round]{natbib}
\usepackage[final, colorlinks=true,allcolors=blue,breaklinks=true]{hyperref} 

\newcommand\crule[3][black]{\textcolor{#1}{\rule{#2}{#3}}}

\definecolor{f_trail}{RGB}{170,170,170}
\definecolor{f_grass}{RGB}{0,255,0}
\definecolor{f_vegetation}{RGB}{102,102,51}
\definecolor{f_sky}{RGB}{0,120,255}
\definecolor{f_obstacle}{RGB}{0,0,0}
\definecolor{u_Bed}{RGB}{0,0,255}
\definecolor{u_Books}{RGB}{233, 89, 48}
\definecolor{u_Ceiling}{RGB}{0, 218, 0}
\definecolor{u_Chair}{RGB}{149, 0, 240}
\definecolor{u_Floor}{RGB}{222, 241, 24}
\definecolor{u_Furniture}{RGB}{255, 206, 206}
\definecolor{u_object}{RGB}{0, 224, 229}
\definecolor{u_Picture}{RGB}{106, 136, 204}
\definecolor{u_Sofa}{RGB}{117, 29, 41}
\definecolor{u_Table}{RGB}{240, 35, 235}
\definecolor{u_Tv}{RGB}{0, 167, 156}
\definecolor{u_wall}{RGB}{250, 139, 0}
\definecolor{u_Window}{RGB}{225, 229, 195}



\begin{document}
\sloppy
\title{StyleDiffusion: Prompt-Embedding Inversion for Text-Based Editing}


\author{Senmao Li,
        Joost van de Weijer, Taihang Hu,
        Fahad Shahbaz Khan, \\
        Qibin Hou, Yaxing Wang*\thanks{*The corresponding author.} and Jian Yang
}



\institute{S. Li, T. Hu, Q. Hou, Y. Wang and J. Yang are CS, Nankai University, Tianjin, China. \email{senmaonk@mail.nankai.edu.cn, hutaihang00@gmail.com, \{houqb,yaxing, csjyang\}@nankai.edu.cn}. \\
J. van de Weijer is with the Computer Vision Center, Universitat Aut\`onoma de Barcelona, Barcelona 08193, Spain. \email{joost@cvc.uab.es}. \\
F. Shahbaz  Khan  is   Mohamed bin Zayed University of Artificial Intelligence (UAE),  and Linkoping University (Sweden).  \email{fahad.khan@liu.se}\\
}

\date{Received: date / Accepted: date}

\maketitle

\begin{abstract}
A significant research effort is focused on exploiting the amazing capacities of pretrained diffusion models for the editing of images.
They either finetune the model, or invert the image in the latent space of the pretrained model. However, they suffer from two problems: (1) Unsatisfying results for selected regions and unexpected changes in non-selected regions.
    (2) They require careful text prompt editing where the prompt should include all visual objects in the input image.
To address this, we propose two improvements: 
(1) Only optimizing the input of the value linear network in the cross-attention layers is sufficiently powerful to reconstruct a real image. (2) We propose attention regularization to preserve the object-like attention maps after reconstruction and editing, enabling us to obtain accurate style editing without invoking significant structural changes.
We further improve the editing technique that is used for the unconditional branch of classifier-free guidance as used by P2P~\citep{hertz2022prompt}. Extensive experimental prompt-editing results on a variety of images demonstrate qualitatively and quantitatively that our method has superior editing capabilities compared to existing and concurrent works. See our accompanying code in  Stylediffusion: \url{https://github.com/sen-mao/StyleDiffusion}.
\end{abstract}

\section{Introduction}
\label{intro}

\begin{figure*}[t]
    \centering
\includegraphics[width=\textwidth]{imgs/survey_compressed.pdf}\vspace{-2mm}
        \caption{Three different optimization methods for inverting real image(s). (Left) Some works invert the image(s) into a new textual embedding “$S^{*}$” by finetuning the pretrained diffusion model~\citep{Kawar2022ImagicTR, valevski2022unitune, ruiz2022dreambooth} or freezing the model~\citep{kumari2022multi} and applying a denoise loss $\mathcal{L}_{denoise}$. 
        Here, $w$ is the classifier-free guidance parameter. These methods require a few training images. 
        (Middle) Null-text inversion~\citep{mokady2022null} optimizes the null-text embedding with the reconstruction loss. (Right) we propose Stylediffusion which maps the real image to the input embedding of the \textit{value} of the cross-attention, which enables us to obtain accurate style editing without invoking significant structural changes.}
    \label{fig:survey}\vspace{-2mm}
\end{figure*}




Text-based deep generative models have achieved extensive adoption in the field of image synthesis. Notably, GANs~\citep{patashnik2021styleclip,gal2021stylegan,kang2023gigagan,Sauer2023ICML}, Diffusion models~\citep{saharia2022photorealistic,ramesh2022hierarchical,gafni2022make}, autoregressive models~\citep{yu2022scaling}, and their hybrid counterparts have been prominently utilized in this domain.

Diffusion models ~\citep{ramesh2022hierarchical,saharia2022photorealistic,rombach2021highresolution} have made remarkable progress due to their exceptional realism and diversity. It has seen rapid applications in other domains such as video generation ~\citep{khachatryan2023text2video,wu2022tune,zhou2022magicvideo}, 3D generation ~\citep{poole2022dreamfusion,lin2023magic3d,wang2023prolificdreamer} and speech synthesis ~\citep{jeong2021diff,huang2022fastdiff,koizumi2022specgrad}.  In this work, we focus on Stable Diffusion (SD) models for real image editing.
Researchers basically perform real image editing with two steps: \textit{projection} and \textit{manipulation}. The former aims to either adapt the weights of the model~\citep{song2020denoising,liu2023accelerating,kim2022diffusionclip,xiao2023fastcomposer} or project given
images into the latent code or embedding space of SD models~\citep{mokady2022null,gal2022image,avrahami2023break,han2023highly}.

The latter aims to edit the latent code or embedding to further manipulate real images~\citep{Kawar2022ImagicTR,meng2021sdedit,cao2023masactrl,zhang2023adding,couairon2022diffedit}.



\begin{figure*}
\centering
        \includegraphics[width=\linewidth]{imgs/abstract_compressed.pdf}    \vspace{-6mm}
        \captionof{figure}{Our method takes as an input a real image (leftmost column) and an associated caption.  Here we demonstrate more accurate reconstruction and editing capabilities compared to Null-text~\citep{mokady2022null}. We manipulate the inverted image using the editing technique P2P.}
        \label{fig:teaser}
\end{figure*}


\begin{figure*}[t]
    \centering
\includegraphics[width=\linewidth]{imgs/perfect_prompt_compressed.pdf}\vspace{-2mm}
        \caption{Null-text~\citep{mokady2022null} editing results of the real image with different prompts. A satisfactory result requires a carefully selected  prompt.}
    \label{fig:perfect_prompt}\vspace{-2mm}
\end{figure*}

\begin{figure}[t]
    \centering
\includegraphics[width=\columnwidth]{imgs/insight_k_v_exchange_compressed.pdf}\vspace{-2mm}
        \caption{(Top) from second to last columns,  the prompts corresponding to both $\mathbf{c}_1$ and $\mathbf{c}_2$  are [\textit{“Dog”}, \textit{“Dog”}, \textit{“Dog”}, \textit{“A man in glasses”}, \textit{“A dog holding flowers”}] and [\textit{“Cat”}, \textit{“Tiger”}, \textit{“Watercolor drawing of a cat”}, “\textit{A man in sunglasses”}, \textit{“A cat wearing sunglasses”}], respectively. (Middle) we swap the input embedding of the key and fix the one of the value. This result shows that the generated images of both the third and fourth rows are similar to the ones of both the first and second rows, respectively. (Bottom) we fix the key and swap the one of the value. This experiment indicates that the value determines the object style (what). }
    \label{fig:k_v_exchange}\vspace{-2mm}
\end{figure}


In the first \emph{projection} step, some works finetune the whole model~\citep{Kawar2022ImagicTR, valevski2022unitune, ruiz2022dreambooth,kim2022diffusionclip,gal2023encoder,xiao2023fastcomposer} or  partial weights of the pretrained model~\citep{kumari2022multi,xie2023difffit}.  Yet,  finetuning both the entire or part of the generative model with only a few real images suffers from both the cumbersome tuning of the model's weights and catastrophic forgetting~\citep{kumari2022multi,wu2018memory,xie2023difffit}.  
Other works on \emph{projection} attempt to  learn a new embedding vector which represents given real images (keeping the SD model frozen)~\citep{ho2021classifier,gal2022image,avrahami2023break,han2023highly,tewel2023key,zhang2023prospect,dong2023prompt}. They focus on optimizing conditional or unconditional inputs of the cross-attention layers of the classifier-free diffusion model~\citep{ho2021classifier}. 
Textual Inversion~\citep{gal2022image} uses the denoising loss  to optimize the  textual embedding of the conditional branch given a few content-similar images. 
Null-text optimization~\citep{mokady2022null} firstly inverts the real image into a series of timestep-related latent codes, then leverages a reconstruction loss to learn the null-text embedding of the unconditional branch (see Fig.~\ref{fig:survey} (middle)). However, these methods suffer from the following challenges:
 \begin{enumerate*}[label=(\Roman*)]
    \item They lead to unsatisfying results for selected regions, and unexpected changes in non-selected regions, both when reconstruction and editing (see Fig.~\ref{fig:teaser} (\textit{Null-text})).
     \item They require a user to provide an accurate text prompt that describes every visual object,  and the relationship between them in the input image (see Fig.~\ref{fig:perfect_prompt}). 
 \end{enumerate*}
Finally, Pix2pix-zero~\citep{parmar2023zero} requires the textual embedding directions (e.g, cat $\rightarrow$ dog in Fig.~\ref{fig:cat2dog} (up, the fourth column)) with thousands of sentences with GPT-3~\citep{brown2020language} before editing, which lacks scalability and flexibility.

To overcome the above-mentioned challenges, we analyze the role of the attention mechanism (and specifically the roles of keys, queries and values) in the diffusion process. This leads to the observation that the key dominates the output image structure (where)~\citep{hertz2022prompt},  whereas the value determines the object style (what).  
We perform an effective experiment to demonstrate that the value determines the object style (what). As shown in Fig.~\ref{fig:k_v_exchange} (top),  we generate two sets of images with prompt embeddings $\mathbf{c}_1$ and $\mathbf{c}_2$.

We use two different embeddings for the input of both key and value in the same attention layer. When swapping the input of the keys and fixing the one of the values, we observe that the content of generated images are similar,  see Fig.~\ref{fig:k_v_exchange} (middle). For example, The images of Fig.~\ref{fig:k_v_exchange} (middle) are similar to the ones of  Fig.~\ref{fig:k_v_exchange} (top).  When exchanging the input of the values and fixing the one of the keys, we find that the content swaps while preserving much of the structure,  see Fig.~\ref{fig:k_v_exchange} (bottom). For example, the images of the last row of Fig.~\ref{fig:k_v_exchange} (bottom) have similar semantic information  with the ones of the first row of Fig.~\ref{fig:k_v_exchange} (top).  It should be noted that their latent codes are all shared~\footnote{The first row of top, middle, and bottom shares a common latent code, and the second row also shares a common latent code.}, so the structure of the results in Fig.~\ref{fig:k_v_exchange} (middle) does not change significantly.
This experiment indicates that the value determines the object's style (what).  


Therefore, to improve the projection of a real image, we introduce $\textbf{Stylediffusion}$ which maps a real image to the input embedding for the value computation (we refer to this embedding as $\textbf{prompt-embedding}$),  which enables us to obtain accurate style editing without invoking significant structural changes. 
We propose to map the real image to the input of the \textit{value} linear layer in the cross-attention layers~\citep{bahdanau2014neural,NIPS2017_3f5ee243} providing freedom to edit effectively the real image in the manipulation step.

We take the given textual embedding as  the input of the \textit{key} linear layer, which is frozen (see Fig.~\ref{fig:survey} (right)). Using frozen embedding contributes to preserving the well-learned attention map from DDIM inversion, which guarantees the initial editability of the inverted image. 

We observe that the system often outputs unsatisfactory reconstruction results (greatly adjusting the input image structure) (see Fig.~\ref{fig:teaser} (first row, second columns) and Fig.~\ref{fig:attnloss} (first row, second columns)) due to locally less accurate attention maps (see Fig.~\ref{fig:attnloss} (first row, and third columns)). 

Hence, to further improve our method, we propose an attention regularization to obtain more precise reconstruction and editing capabilities.


In the second manipulation step, researchers propose a series of outstanding techniques~\citep{Kawar2022ImagicTR,meng2021sdedit,cao2023masactrl,zhang2023adding,couairon2022diffedit,mou2023dragondiffusion,jia2023taming,zhang2023continuous,qiu2023controlling,levin2023differential}. Among them, P2P~\citep{hertz2022prompt} is one of the most widely used image editing methods. 

However, P2P only operates on the conditional branch, and ignores the unconditional branch. This leads to less accurate editing capabilities for some cases, especially where the structural changes before and after editing 

are relatively large (e.g., “...tree...”$\rightarrow$“...house...” in Fig.~\ref{fig:p2plus}).  To address this problem, we need to reduce the dependence of the structure on the source prompt and provide more freedom to generate the structure following the target prompt. Since the unconditional branch allows us to edit out  concepts~\citep{armandpour2023re,Tumanyan_2023_CVPR}. Thus, we propose to further perform the self-attention map exchange in  the unconditional branch based on P2P (called \textit{P2Plus}), as well as in the conditional branch like P2P~\citep{hertz2022prompt}. This technique enables us to obtain more accurate editing capabilities (see Fig.~\ref{fig:p2plus} (the third column)). We build our method on SD models~\citep{rombach2021highresolution} and experiment on a variety of images and several ways of prompt editing.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{imgs/p2plus_compressed.pdf}
        \caption{
Comparison of both P2P and P2Plus. P2P fails when the desired editing changes require large structural changes (e.g., tent, tree, and bike). By reducing the dependence on the source prompt (by also involving the unconditional branch), P2Plus can better handle such cases.
        }
    \label{fig:p2plus}
\end{figure}

\section{Related work}

\subsection{Transfer learning for diffusion models}
A series of recent works investigated knowledge transfer on diffusion models~\citep{song2020denoising,liu2023accelerating,mokady2022null,gal2022image,avrahami2023break,kwon2022diffusion,kim2022diffusionclip,gal2023encoder,xiao2023fastcomposer,han2023highly,tewel2023key,kumari2023multi,zhang2023prospect,dong2023prompt} with one or a few images. 
Recent work~\citep{Kawar2022ImagicTR,meng2021sdedit,cao2023masactrl,zhang2023adding,couairon2022diffedit,mou2023dragondiffusion,jia2023taming,zhang2023continuous,qiu2023controlling,levin2023differential,chen2023training}  either finetune the pretrained model or invert the image in the latent space of the  pretrained model. 
Dreambooth~\citep{ruiz2022dreambooth} shows that training a diffusion model on a small data set (of $3\sim5$ images) largely benefits from a pre-trained diffusion model, preserving the textual editing capability. Similarly,  Imagic~\citep{Kawar2022ImagicTR} and UniTune~\citep{valevski2022unitune} rely on the weights of the interpolation or the classifier-free guidance at the inference stage, except when finetuning the diffusion model during training. Kumari et al.~\citep{kumari2022multi} study only updating part of the parameters of the pre-trained model, namely the key and value mapping from text to latent features in the cross-attention layers.  
However, updating the diffusion model unavoidably loses the text editing capability of the pre-trained diffusion model. In this paper, we focus on real image editing with a frozen text-guilded diffusion model.

\subsection{GAN inversion}
Image inversion aims to project a given real image into the latent space,  allowing users to further manipulate the image. There exist several approaches~\citep{bermano2022state,creswell2018inverting,goetschalckx2019ganalyze,jahanian2019steerability,lipton2017precise,  xia2021gan,yeh2017semantic,zhu2016generative}  which focus on image manipulation based on pre-trained GANs, following literately optimization of the latent representation to restructure the target image.  Given a target semantic attribute, they aim to manipulate the output image 
of a pretrained GAN. Several other methods~\citep{abdal2019image2stylegan,zhu2020domain} reverse a given image into the input latent space of a pretrained GAN (e.g., StyleGAN), 
and restructure the target image by optimization of the latent representation.  They mainly consist of  fixing the generator~\cite{abdal2019image2stylegan,abdal2020image2stylegan++, richardson2020encoding,tov2021designing} and updating the generator~\cite{alaluf2021hyperstyle,roich2021pivotal}.

\subsection{Diffusion model inversion}
Diffusion-based inversion can be performed naively by optimizing the latent representation.  \cite{dhariwal2021diffusion} show that a given real image can be reconstructed by DDIM sampling ~\citep{song2020denoising}. DDIM provides a good starting point to synthesize a given real image.  Several works~\citep{avrahami2022blendedlatent, avrahami2022blended,nichol2021glide} assume that the user provides a mask to restrict the region in which the changes are applied, achieving both meaningful editing and background preservation. P2P~\citep{hertz2022prompt} proposes a mask-free editing method. However, it leads to unexpected results when editing the real image~\citep{mokady2022null}.  Recent work investigates the text embedding of the conditional input~\citep{gal2022image},  or the null-text optimization of the unconditional input (i.e., Null-text inversion~\citep{ mokady2022null}).  Although having the editing capability by combining the new prompts,  
they suffer from the following challenges:
   \begin{enumerate*}[label=(\Roman*)]
    \item They lead to unsatisfying results for the selected regions, and unexpected changes in non-selected regions.    
    \item They require careful text prompt editing where the prompt should include all visual objects in the input image.    
 \end{enumerate*}

Concurrent work~\citep{parmar2023zero} proposes pix2pix-zero, also aiming to increase more accurate editing capabilities of the real image. However, it firstly needs to compute the textual embedding direction with thousand sentences in advance. 


 
\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{imgs/framework.pdf}
        \caption{Overview of the proposed method. (\textbf{I}) DDIM inversion: the diffusion process is performed to generate the latent representations: ${(\mathbf{\hat{z}}_t, \mathbf{\hat{a}}_t)} (t = 1,...,T)$, where $\mathbf{\hat{z}}_0 = \mathbf{z}_0$, which is the extracted feature of the input real image $\mathbf{x}$. $\mathbf{c}$ is the extracted textual embedding by a Clip-text Encoder with a given prompt $\mathbf{p}^{src}$.    (\textbf{II}) Stylediffusion: we take the input image $\mathbf{x}$ as input, and extract the prompt-embedding $\mathbf{\widetilde{c}}_{t} = M_{t}\left ( E(\mathbf{\mathbf{x}})\right )$, which is taken as the input value matrix $\mathbf{v}$ of the linear layer $\Psi_V$. The input of  the linear layer $\Psi_K$ is the given textual embedding $\mathbf{c}$.  We get both the latent code $\mathbf{\widetilde{z}_{t-1}}$ and the attention map $\mathbf{\widetilde{a}}_t$, which are aligned with both the latent code $\mathbf{\hat{z}_{t-1}}$ and the attention map $\mathbf{\hat{a}_{t}}$, respectively.  Note  $\mathbf{\widetilde{z}}_T = \mathbf{\hat{z}}_T$.     (\textbf{III}) StyleDiffusion editing. From T to $\tau_v$ timestep, the input of the linear network $\Psi_v$ comes from the learned textual embedding $\mathbf{\widetilde {c}_{t}}$ produced by the trained $M_t$. From $\tau_{v}-1$ to 1 the corresponding input comes from the prompt-embedding $\mathbf{c}^{tgt}$ of the target prompt. We use P2Plus to perform the attention exchange.}
    \label{fig:framework}
\end{figure*}




\section{Method}

\minisection{Problem setting.} 
DDIM inversion proposes an inversion scheme for unconditional diffusion models. However, this method fails when applied to text-guided diffusion models. This was observed by Mokady et al.~\citep{mokady2022null} and they propose Null-text inversion to address this problem. However, their methods has some drawbacks:  (1) unsatisfying results for selected regions and unexpected changes in non-selected regions. (2) they require careful text prompt editing where the prompt should include all visual objects in the input image.

Therefore, our goal is to obtain a more accurate editing capability based on an accurate reconstruction of the real image $\mathbf{x}$ guided by the source prompt $\mathbf{p}^{src}$.      
Our method, called StyleDiffusion, is based on  the observation that the \emph{keys} of the cross-attention layer dominate the output image structure (where),  whereas the \emph{values} determine the object style (what). After projecting faithfully the real image, we propose P2Plus, which is an improved version of P2P~\citep{hertz2022prompt}. 

Next, we  introduce SD models in Sec.~\ref{subsec:SD_models}, followed by the proposed StyleDiffusion in Sec.~\ref{subsec:stylediffusion} and P2Plus in Sec.~\ref{subsec:p2plus}. A general overview is provided in Fig.~\ref{fig:framework}.


\subsection{Preliminary: Diffusion Model}\label{subsec:SD_models}
Generally, diffusion models optimize a UNet-based denoiser  network $\epsilon_\theta$  to predict Gaussian noise $\epsilon$,  following the objective:
\begin{equation}
\min_\theta E_{\mathbf{z}_0,\epsilon \sim N(0,I),t\sim [1,T]} \left \| \epsilon-\epsilon_\theta(\mathbf{z}_t,t,\mathbf{c}) \right \|_{2}^{2}, 
\end{equation}
where $z_t$ is a noise sample according to timestep $t\sim [1,T]$, and $T$ is the number of the timesteps.  The encoded text embedding $\mathbf{c}$ is extracted by a Clip-text Encoder $\Gamma$ with given prompt $\mathbf{p}^{src}$: $\mathbf{c} = \Gamma(\mathbf{p}^{src})$. 
In this paper, we  build on SD models~\citep{rombach2021highresolution}.  These first trains both encoder  and decoder. Then the diffusion process is performed in the latent space. Here the encoder maps the image $\mathbf{x}$ into the latent representation  $\mathbf{z_0}$, and the decoder aims to reverse the  latent representation  $\mathbf{z_0}$ into the image.  The sampling process is given by:
\begin{equation}
\resizebox{0.9\hsize}{!}{$\mathbf{z}_{t-1} = \sqrt{\frac{\alpha_{t-1}}{\alpha_t}}\mathbf{z}_t 
+\sqrt{\alpha_{t-1}}
\left(\sqrt{\frac{1}{\alpha_{t-1}}-1}-\sqrt{\frac{1}{\alpha_t}-1}\right) \cdot \epsilon_\theta(\mathbf{z}_t,t,\mathbf{c}),
$}
\label{eq:ddim_sampling}
\end{equation}
 where  $\alpha_t$ is a scalar function.


\minisection{DDIM inversion.} For real-image editing with a pretrained diffusion model, a given real image is to be reconstructed by finding its initial noise. 
 We use the deterministic DDIM model to perform image inversion.  
 This process is given by:
\begin{equation}
\label{eq:ddim_inversion}
\resizebox{0.9\hsize}{!}{$\mathbf{z}_{t+1} = \sqrt{\frac{\alpha_{t+1}}{\alpha_t}}\mathbf{z}_t \\
+ \sqrt{\alpha_{t+1}}
\left(\sqrt{\frac{1}{\alpha_{t+1}}-1}-\sqrt{\frac{1}{\alpha_t}-1}\right)\cdot \epsilon_\theta(\mathbf{z}_t,t,\mathbf{c}).
$}
\end{equation}
 DDIM inversion synthesizes the latent noise that produces an approximation of the input image when fed to the diffusion process. While the reconstruction based on DDIM is not sufficiently accurate,  it still provides a good starting point for training, enabling us to efficiently achieve high-fidelity  inversion~\citep{hertz2022prompt}. We use  the intermediate results of DDIM inversion to train our model, similarly as~\citep{couairon2022diffedit,mokady2022null}.


\minisection{Cross-attention.} SD models achieve text-driven image generation by feeding a prompt into the cross-attention layer.  Given both the text embedding $\mathbf{c}$ and  the image feature representation $\mathbf{f}$,  
we are able to produce the key matrix $\mathbf{k}= \Psi_K(\mathbf{c})$,  the value matrix $\mathbf{v}= \Psi_V(\mathbf{c})$ and the query matrix $\mathbf{q}= \Psi_Q(\mathbf{f})$,  via the  linear networks:  $\Psi_K, \Psi_V,\Psi_Q$. The attention maps are then  computed with:
\begin{equation}
\begin{aligned}\label{eq:atten_softmax}
\mathbf{a}= \text{Softmax}(\frac{\mathbf{q}\mathbf{k}^{T}}{\sqrt{\mathbf{d}}}),
\end{aligned}
\end{equation}
where $\mathbf{d}$ is the projection dimension of the keys and queries.  Finally, the cross-attention output is $\mathbf{\hat{f}} = \mathbf{a}\mathbf{v}$, which is then taken as input in the following  convolution layers. 


Intuitively, P2P~\citep{hertz2022prompt} performs prompt-to-prompt image editing  with cross attention control. P2P is based on the idea that the attention maps largely control where the image is drawn, and the values decide what is drawn (mainly defining the style). 
Improving the accuracy of the attention maps leads to more powerful editing capabilities~\citep{mokady2022null}.
  We experimentally observe that DDIM inversion generates satisfying attention maps (e.g., Fig.~\ref{fig:attnloss} (second row, first column )), 
and provides a good starting point for the optimization. Next, we investigate the attention maps to guide the image inversion.

\subsection{StyleDiffusion}
\label{subsec:stylediffusion}

\begin{algorithm}[t]
\SetAlgoLined
\textbf{Require:} the features of the training image and the prompt embeddings: $\{\mathbf{z}_0,  \mathbf{c} \}$. $K_t=e^{-t}*K$, where $K=100$ which is the starting number of inner iterations.
$K_t$ is the number of training iteration for each timestep t.
The mapping network $\{M_t\} (t=1,...,T)$ with initialization parameters $\omega$.\\
\textbf{Temporary results:} With guidance scale $w=1$ for the classifier-free diffusion model, we use DDIM inversion to produce $\{ \mathbf{\hat{z}}_j, \mathbf{\hat{a}}_j\} (j=1,...,T)$.

\textbf{Output:} 
Mapping network $\{M_t\} (t=1,...,T)$.\\
 \vspace{1mm} \hrule \vspace{1mm}
 Set guidance scale $w=7.5$; \\
 Initializing $ \mathbf{\widetilde{z}}_T \leftarrow \mathbf{\hat{z}}_T$; \\
 \For{$t=T,T-1,\ldots,1$}{
    \For{$k=0,\ldots,K_t-1$}{
        $ {\mathbf{a}_{t}, \mathbf{z}_{t-1}} \leftarrow \mathbf{\widetilde{z}}_t$;(Eqs.~\ref{eq:atten_softmax} and \ref{eq:pred_noise})\\
        $\omega  \leftarrow  \omega  - \eta \nabla_{\omega }\mathcal{L}$ ;(Eq.~\ref{eq:full_loss})
 }

 Synthesizing $\mathbf{\widetilde{z}}_{t-1}$;(Eq.~\ref{eq:pred_noise})
 
 }
 \textbf{Return} Mapping network $\{M_t\} (t=1,...,T)$
\caption{Our algorithm}\label{alg:alg_ours}
\end{algorithm}



\minisection{Method overview.}
As illustrated in Fig.~\ref{fig:framework} (I),  given a pair of a real image $\mathbf{x}$ and a corresponding prompt $\mathbf{p}^{src}$ (i.e., "dog"),  we  perform DDIM inversion~\citep{dhariwal2021diffusion,song2020denoising} to synthesize a series of latent noises $\{\mathbf{\hat{z}}_t\}$ and attention maps $\{\mathbf{\hat{a}}_t\} (t = 1, ..., T)$, where $\mathbf{\hat{z}}_0 = \mathbf{z}_0$, which is the extracted  latent code of the input image $\mathbf{x}$ ~\footnote{Note  when generating the attention map $\mathbf{\hat{a}}_T$ in the last timestep $t=T$, we throw out the synthesized latent code $\mathbf{\hat{z}}_{T+1}$. }.
Fig.~\ref{fig:framework} (II)   shows that our method reconstructs the latent noise $\mathbf{\hat{z}}_t$ in the order of the diffusion process $T \rightarrow 0$, where $\mathbf{\widetilde{z}}_T=\mathbf{\hat{z}}_T$. Our framework consists of three networks: a frozen ClipImageEncoder $E$,   a learnable mapping network $M_{t}$ and a denoiser network $\epsilon_\theta$.  For a specific timestep $t$, the  ClipImageEncoder $E$ takes the input image $\mathbf{x}$ as an input. The output $E(\mathbf{x})$ is fed into the mapping network $M_{t}$, producing the prompt-embedding $\mathbf{\widetilde {c}_{t}} = M_{t}\left ( E(\mathbf{x}) \right )$, which is fed into the value network $\Psi_V$ of the cross-attention layers. The  input of the linear layer $\Psi_K$ is the given textual embedding $\mathbf{c}$. We generate both the latent code $\mathbf{\widetilde{z}_{t-1}}$ and the attention map $\mathbf{\widetilde{a}}_t$. 
Our full algorithm is presented in algorithm~\ref{alg:alg_ours}.

The full loss function consists of two losses:   \textit{reconstruction loss} and \textit{attention loss}, which   guarantee
that both  the denoised latent code $\mathbf{\widetilde{z}_{t-1}}$ and  the  corresponding attention map $\mathbf{\widetilde{a}_{t}}$  at inference time are close to the ones: $\mathbf{\hat{z}_{t-1}}$ and  $\mathbf{\hat{a}_{t}}$ from DDIM inversion, respectively. 

\minisection{\textit{Reconstruction Loss}.} 
Since the noise representations ($\{\mathbf{\hat{z}}_1, \cdots \mathbf{\hat{z}}_T\}$) provide an initial trajectory which is close to the real image, we train the network $M_{t}$
to generate the prompt embedding $\mathbf{\widetilde {c}_{t}} = M_{t}\left ( E(\mathbf{x}) \right )$, which is the input of the value network. We optimize the $M_{t}$ in such a matter, that the output latent code ($\mathbf{\widetilde{z}}_t$)  is close to the noise representations ($\mathbf{\hat{z}}_t$). The objective is: 

\begin{equation}\label{eq:recon}
\resizebox{0.5\hsize}{!}{$\mathcal{L}_{rec} = \min_{M_{t}}  \left \|\mathbf{\hat{z}}_{t-1}-\mathbf{\widetilde{z}}_{t-1}\right \|^2
 $,\vspace{-10mm}}
\end{equation}
\begin{equation}\label{eq:pred_noise}
\resizebox{0.9\hsize}{!}{$\mathbf{\widetilde{z}}_{t-1} = \sqrt{\frac{\alpha_{t-1}}{\alpha_t}}\widetilde{\mathbf{z}}_t+
 \sqrt{\alpha_{t-1}}
 \left(\sqrt{\frac{1}{\alpha_{t-1}}-1}-\sqrt{\frac{1}{\alpha_t}-1}\right) \cdot \epsilon_\theta(\widetilde{\mathbf{z}}_t,t, \mathbf{c}, \mathbf{{c}_{t}}).
 $}
\end{equation}


\minisection{\textit{Attention loss.}} It is known that a more accurate attention map is positively correlated to the editing capability~\citep{mokady2022null}. 
The attention map, which is synthesized during the DDIM inversion, provides a good starting point. Thus, we introduce attention regularization 

when optimizing the mapping network $M_{t}$ to further improve its quality. The objective is the following:
\begin{equation}\label{eq:atten}
 \resizebox{0.5\hsize}{!}{$\mathcal{L}_{att} = \min_{M_{t}}  \left \|\mathbf{\hat{a}}_{t}-\mathbf{\widetilde{a}}_{t}\right \|^2
 $,\vspace{-10mm}}
\end{equation}
where  $\mathbf{\hat{a}}_{t}$ and  $\mathbf{\widetilde{a}}_{t}$ can be obtained with Eqs.~\ref{eq:atten_softmax}. 

\minisection{\textit{Full objective.}}
The full objective function of our model is:
\begin{equation}
\label{eq:full_loss}
\begin{aligned}
\mathcal{L} =   \mathcal{L}_{rec} +\mathcal{L}_{att}.
\end{aligned}
\end{equation}
In conclusion, in this section, we have proposed an alternative solution to the inversion of text-guided diffusion models which aims to improve over existing solutions by providing more accurate editing capabilities, and without requiring careful prompt engineering. 


\subsection{P2Plus}
\label{subsec:p2plus}

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{imgs/p2plus_pipline_compressed.pdf}
        \caption{(Top) Given the same latent code $\mathbf{z}_{T}$ and the classifier-free guidance parameter $w$, we generate two images with two prompts. The arrows indicate the diffusion process from $T$ to $0$. Here \textcolor{green}{green} is used for the generation with the source prompt: 'A tent in forest', and  \textcolor{red}{red} for the generation with the target prompt: 'A tiger in forest'. (Middle) P2P  copies both the self-attention maps and the cross-attention maps of the conditional branch of the SD models from the source prompt into the corresponding self-attention and cross-attention maps of the conditional branch of the SD models with target prompt. This process is indicated with  \textcolor{yellow}{yellow} arrows.  (Bottom) P2Plus additionally replaces the self-attention map of the unconditional branch of SD models, in addition to the ones of the conditional branch. The dashed \textcolor{yellow}{yellow} arrows indicate this technique. }
    \label{fig:p2plus_pipline}
\end{figure}



\begin{figure*}[t]
\vspace{-2mm}
   \centering
\includegraphics[width=\textwidth]{imgs/uncondselfattn_tree_compressed.pdf}
       \caption{\emph{P2P} (the second column: w/o $\tau_u$) has not succeeded in replacing the tent with the tiger.
       Adding the injection parameter $\tau_u$ can help to edit successfully, especially if $\tau_u=0.5$.
       We also use the classifier-free guidance parameter $w=7.5$ like SD, when the weight $1-w$ of the unconditional branch is negative~\citep{armandpour2023re,Tumanyan_2023_CVPR}, which can gradually weaken the influence of the "tent" in the unconditional branch as $\tau_u$ increases from 0.2 to 1.0 (third to eighth columns).}
   \label{fig:uncondselfattn}
\end{figure*}


After having inverted the text-guided diffusion model, we can now perform prompt-based image editing (see Figs.~\ref{fig:teaser} and \ref{fig:cat2dog}). We will here outline our approach, which is an improvement on the popular P2P~\citep{hertz2022prompt}.

P2P performs the replacement of both the cross-attention map and the self attention map of the conditional branch,  aiming to maintain the structure of the source prompt, see Fig.~\ref{fig:p2plus_pipline} (middle). However, it  ignores the replacement in the unconditional branch. This leads to less accurate editing capabilities for some cases, especially when the structural changes before and after editing 
are relatively large (e.g., “...tent...”$\rightarrow$“...tiger...” ).  To address this problem, we need to reduce the dependence of the structure on the source prompt and provide more freedom to generate the structure following the target prompt. Thus, as shown in Fig.~\ref{fig:p2plus_pipline} (bottom) we propose to further perform the self-attention map replacement in  the unconditional branch based on P2P (called \textit{P2Plus}), as well as in the conditional branch like P2P~\citep{hertz2022prompt}.   This technique enables us to obtain more accurate editing capabilities.  Like P2P, we introduce   a timestep parameter $\tau_u$ that determines until which step the injection is applied. Fig.~\ref{fig:uncondselfattn} shows the results with different  $\tau_u$  values. 
 


\begin{figure*}
    \centering
    \includegraphics[width=0.94\linewidth]{imgs/compa_compressed.pdf}
    \vspace{-10pt}
    \caption{Comparisons with different baselines for real images.  
        Our method, achieves realistic editing of both style and structured objects, while preserving the structure of the input image (last column).  }
    \label{fig:cat2dog}
\end{figure*}




\section{Experimental setup}
\vspace{-2mm}
\minisection{Training details and datasets.} 
We use the pretrained Stable Diffusion model. The mapping network $M_t$ configuration is provided in Tab.~\ref{tab:network}. We set $T=30$. $\tau_v$ is a timestep parameter that determines which timestep is used by the output of the mapping network in the StyleDiffusion editing phase. Similarly, we can set the timestep $\tau_u$ (as in the conditional branch in P2P~\citep{hertz2022prompt}) to control the number of diffusion steps in which the injection of the unconditional branch is applied.
We use Adam~\cite{kingma2014adam} with a batch size of 1 and a learning rate of 0.0001. The exponential decay rates are $( \beta_{1},\beta_{2})$ $= (0, 0.999)$. We randomly initialize the weights of the mapping network following a Gaussian distribution centered at 0 with 0.01 standard deviation. We use one Quadro RTX 3090 GPUs (24 GB VRAM) to conduct all our experiments.  We randomly collect a real image dataset of 100 images and caption pairs (of $512 \times 512$ resolution) from Unsplash(\url{https://unsplash.com/}) and COCO~\citep{chen2015microsoft}.

\minisection{Evaluation metrics.}    \textit{Clipscore}~\citep{hessel2021clipscore} is a metric that evaluates the quality of a pair of a prompt and an edited image. To evaluate the preservation of the structure information after editing,  we use Structure Dist~\citep{tumanyan2022splicing} to compute the structural consistency of the edited image.  Furthermore, in this paper, we aim to modify the selected region, which corresponds to the target prompt, and preserve the non-selected region. Thus, we need to evaluate the change in the non-selected region after editing. To get automatically the non-selected region of the edited image, we use a binary method to generate the raw mask from the attention map. Then we reverse it to get the non-selected region mask. Using the non-selected region mask, we compute the non-selected region LPIPS~\citep{zhang2018unreasonable}  between a pair of real and edited images, named \textit{NS-LPIPS}. A lower score on NS-LPIPS means that the non-selected region is more similar to the input image. We also use both PSNR and SSIM to evaluate image reconstruction.

\minisection{Baselines.}
We compare against the following baselines.
\emph{Null-text}~\citep{mokady2022null}  inverts real images with corresponding captions into the text embedding of the unconditional part of the classifier-free diffusion model.  \emph{SDEdit}~\citep{meng2021sdedit} introduces a stochastic differential equation to generate realistic images through an iterative denoising process.  \emph{Pix2pix-zero}~\citep{parmar2023zero} edits the real image to find the potential direction from the source to the target words.   We compare with \textit{DDIM + word swap}~\citep{parmar2023zero} which performs DDIM sampling with an edited prompt generated by swapping the source word with the target. We use the official codes of the baselines to compare with StyleDiffusion. 
We also ablate variants of StyleDiffusion.

\begin{table}[t]
    \setlength{\tabcolsep}{1mm}
    \resizebox{\columnwidth}{!}{\centering
    \footnotesize
    \setlength{\tabcolsep}{1pt}
    \begin{tabular}{|c|c|c|c|c|c|}
    \hline
        Metric   & (Input channel, Output channel)	&(Kernal size,Stride)& Input  Dimension&  Output dimension \cr\cline{1-5}
      Conv0  & (197, 77)&(1, 1) &(197, 768)&(77, 768)\cr\cline{1-5}
      Conv1  & (77, 77)&(1, 1)&(77, 768)&	(77, 768)\cr\cline{1-5}
    BatN1  & -&-&(77, 768)&	(77, 768)\cr\cline{1-5}
      LeakyRelu1  & -&-	&(77, 768)&	(77, 768)\cr\cline{1-5}
      Conv2  & (77, 77)&(1, 1)&(77, 768)&(77, 768) \cr\cline{1-5}
    \hline 
    \end{tabular}  
    }
\caption{\small Mapping network $M_t$  configuration.
}\label{tab:network}
\end{table}

\begin{table}[t]
    \setlength{\tabcolsep}{1mm}
    \resizebox{\columnwidth}{!}{\centering
    \footnotesize
    \setlength{\tabcolsep}{10pt}
    \begin{tabular}{|c|c|c|c|c|}
    \hline

        Metric   & Structure-dist$\downarrow$	&NS-LPIPS$\downarrow$	&Clipscore$\uparrow$\cr\cline{1-4}

      *DDIM  & 0.092	&0.4131 &\textbf{81.9$\%$ }\cr\cline{1-4}
      SDEit  & 0.046	&0.2473&78.0$\%$ 	\cr\cline{1-4}
      Null-text  & 0.027	&0.1480	&75.2$\%$ \cr\cline{1-4}
      Ours  & \textbf{0.026}&\textbf{0.1165}&77.9$\%$ \cr\cline{1-4}
    \hline 
    \end{tabular}  
    }
\caption{\small Comparison with baselines on three metrics. NS-LPIPS: non-selected LPIPS.  *DDIM: DDIM inversion with word swap.  Although \textit{DDIM with word swap} achieves the best Clipscore, it not only changes the background, but also modifies the structure of the selected region, see also Fig.~\ref{fig:cat2dog} (last three rows, fourth column).
}\label{tab:scores}
\end{table}


\begin{table}[t]
    \setlength{\tabcolsep}{1mm}
    \resizebox{\columnwidth}{!}{\centering
    \footnotesize
    \setlength{\tabcolsep}{10pt}
    \begin{tabular}{|c|c|c|}
    \hline

               & \thead{Inference Time$\downarrow$} & \thead{PSNR$\uparrow$/SSIM$\uparrow$} \cr\cline{1-3}
    Null-text  & \textbf{0.28s}	& 31.314$\slash$0.730 \cr\cline{1-3}
    Ours  & 0.30s & \textbf{31.523$\slash$0.751} \cr\cline{1-3}
    \hline 
    \end{tabular} 
    }
\caption{We report the inference time of each timestep and PSNR/SSIM. We have better reconstruction quality with a small computational overhead. 
}\label{tab:infer_time}
\end{table}

\begin{figure}[t]
\centering
\includegraphics[width=0.9\columnwidth]{imgs/user_study.pdf}
        \caption{ We conduct a forced-choice user study and ask subjects to select the results according to \textit{'Which figure does best preserve the input image structure and matches target prompt style?'}.}
    \label{fig:user_study}
\end{figure}

\section{Experiments}

\minisection{Qualitative and quantitative results.}
Fig.~\ref{fig:cat2dog}  presents a comparison between the baselines and our method. \textit{SDEit}~\citep{meng2021sdedit} fails to generate high-quality images, such as dog or cat faces (second column).  
\emph{Pix2pix-zero}~\citep{parmar2023zero} synthesizes better results, but it also modifies the non-selected region, such as removing the plant when translating cat $\rightarrow$ dog.   The official implementation of \emph{pix2pix-zero}~\citep{parmar2023zero} provides the editing directions (e.g, cat $\leftrightarrow$ dog), and we directly use them. Note, \emph{pix2pix-zero}~\citep{parmar2023zero} firstly requires that the editing directions are calculated in advance, while our method does not require this.   Fig.~\ref{fig:cat2dog} (last three rows, fourth column) shows that \textit{DDIM with word swap} largely modifies both the background and the structure information of the foreground.  Our method successfully edits the target-specific object, resulting in a high-quality image, indicating that the proposed method has more accurate editing capabilities.


\begin{figure*}[t]
    \centering

\includegraphics[width=\textwidth]{imgs/oursresults_compressed.pdf} 
\caption{Examples of StyleDiffusion for editing with   attention injection or prompt refinement.}
\label{fig:oursresults}
\end{figure*}

We evaluate the performance of the proposed method on the collected dataset.  As reported in Tab.~\ref{tab:scores},  in terms of both Structure distance and NS-LPIPS the proposed method achieves the best score, which indicates that we have superior capabilities to preserve structure information. In terms of Clipscore,  we get a better score than Null-text (i.e., 77.9$\%$ vs 75.2$\%$), and a comparative result with SDEdit. \textit{DDIM with word swap} achieves the best Clipscore. We observe that \textit{DDIM with word swap} not only changes the background, but also modifies the structure of the selected-regions (see Fig.~\ref{fig:cat2dog} (last three rows, fourth column)). Note that we do not compare with pix2pix-zero~\citep{parmar2023zero} in Fig.~\ref{fig:cat2dog} (last three rows), since it first needs to compute the textual embedding directions with thousands of sentences with GPT-3~\citep{brown2020language}. We also evaluate the reconstruction quality and the inference time for each timestep. As reported in  Tab.~\ref{tab:infer_time}, we achieve the best PSNR/SSIM scores, with an acceptable time overhead.
  
Furthermore, we conduct a user study and ask subjects to select the results that best match the following statement: \textit{which figure preserves the input image structure and matches the target prompt style} (Figure~\ref{fig:user_study}). We apply quadruplet comparisons (forced choice) with 18 users (30 quadruplets/user). Experiments are performed on images from the collected dataset. Fig.~\ref{fig:user_study} shows that our method considerably outperforms the other methods.

Fig~\ref{fig:oursresults}  shows that we can manipulate the inverted image with attention injection and prompt refinement. For example, we translate  \textit{glasses}  into \textit{sunglasses} (Fig.~\ref{fig:oursresults} (first row)). Fig.~\ref{fig:oursresults} (last row)  we  add \textit{Chinese style} (new prompts) to the source prompt. These results indicate that our approach manages to invert real images with corresponding captions into the latent space, while maintaining its powerful editing capabilities.

We observe that StyleDiffusion (Fig.~\ref{fig:compa3} (last column)) allows for object structure modifications while preserving the identity within the range given by the input image cross-attention map, resembling the capabilities demonstrated by  Imagic~\citep{Kawar2022ImagicTR} (the third column)). In contrast, Null-text~\citep{mokady2022null} does not possess the capacity to accomplish such changes (Fig.~\ref{fig:compa3} (the second column)).

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{imgs/compa3_compressed.pdf}
        \caption{StyleDiffusion can achieve object structure changes within the range of the input image cross-attention map (e.g., “...dog”$\rightarrow$“...sitting dog”).}
    \label{fig:compa3}
    \vspace{-5mm}
\end{figure}


\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{imgs/ablation_uncondiotion_AttentionRegu_compressed.pdf}
        \caption{
        (Left) Using additionally the attention injection in unconditional branch improves the real image editing ability of \emph{Null-text}~\citep{mokady2022null}~(\emph{P2P}). (Right) Comparison of  variants of our method.}
    \label{fig:uncondselfattn_more}
\end{figure*}


\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{imgs/attnloss_compressed.pdf}\vspace{-2mm}
        \caption{The reconstruction effect of attention regularization. The cross-attention map of our reconstruction image (second row, fifth column) more closely matches the one of the input image (second row, first column). Meanwhile, $\mathcal{L}_{att}$ can improve the reconstruction quality of Null-text (second row, second column). In the last two columns, the \emph{cow} is replaced by an \emph{elephant} (source prompt: “cow”, target prompt: “elephant”). }
    \label{fig:attnloss}
\end{figure*}


\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{imgs/v_embed_opt_compressed.pdf}
        \caption{\textit{\textbf{Value-embedding optimization.}} We compare our method (right) to the one (middle), where we directly optimize the input textual embedding for the value linear layer while freezing the input of the key linear layer. As can be seen (middle), this approach leads to an inaccurate reconstruction, resulting in the dog's face not being completely reconstructed. }
    \label{fig:v_embed_opt}
\end{figure*}

\minisection{Ablation study.}  Here, we evaluate the effect of each independent contribution to our method and their combinations.

\minisection{\textit{Attention injection in the unconditional branch.}} Although P2P~\citep{hertz2022prompt} obtains satisfactory editing results with attention injection in the conditional branch, this method ignores attention injection in the unconditional branch (as proposed by our P2Plus in Sec.~\ref{subsec:p2plus}).
We experimentally observe that the self-attention maps in the unconditional branch play an important role in obtaining more accurate editing capabilities, especially when the object structure changes before and after editing of the real image are relatively large, e.g., translating \textit{bike} to \textit{motorcycle} in Fig.~\ref{fig:uncondselfattn_more} (left, the third row).  It also shows that the unconditional branch contains a lot of useful texture and structure information, allowing us to reduce the influence of the unwanted structure of the input image.

\minisection{\textit{Prompt-embedding in cross-attention layers.}} 
We evaluate variants of our method, namely
(1) learning the input prompt-embedding for the key linear layer and freezing the input of the value linear layer with the one provided by the user, and (2) learning the prompt-embedding for both key and value linear layers. As shown in Fig.~\ref{fig:uncondselfattn_more} (right), the two variants fail to edit the image according to the target prompt. Our method successfully modifies the real image with the target prompt, and produces realistic results.


\minisection{\textit{Attention regularization.}}  
We perform an ablation study of attention regularization. Fig.~\ref{fig:attnloss} shows that the system fails to reconstruct partial object information (e.g., the nose in Fig.~\ref{fig:attnloss} (first row, second column)), and learns a less accurate attention map (e.g., the nose attention map in Fig.~\ref{fig:attnloss} (first row, third column). Our method not only synthesizes high-quality images, but also learns a better attention map even compared to the one generated by DDIM inversion (Fig.~\ref{fig:attnloss} (second row, first column)).

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{imgs/npi_compressed.pdf}
        \caption{Both the optimization-free methods NPI and ProxNPI (the second and third columns, the sixth and seventh columns) show limitations in reconstructing and editing real images that contain complex structures and content.}
    \label{fig:npi}
    \vspace{-5mm}
\end{figure*}

\minisection{\textit{Value-embedding optimization.}}  
Fig.~\ref{fig:v_embed_opt} illustrates the reconstruction and editing results of value-embedding optimization, that is, similar to our method extracting the prompt-embedding from the input image but directly optimizing the input textual embedding. Value-embedding optimization fails to reconstruct the input image.  Null-text~\citep{mokady2022null} draws a similar conclusion that optimizing both the input textual embedding for the value and key linear layers results in lower editing accuracy.

\minisection{\textit{StyleDiffusion with SDEdit.}}
\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{imgs/stylediffusion+sdedit_compressed.pdf}
        \caption{\textit{\textbf{StyleDiffusion with SDEdit.}} From left to right: input image, SDEdit, applying SDEdit after StyleDiffusion inversion, and applying P2Plus after StyleDiffusion inversion. It is evident that StyleDiffusion+SDEdit significantly improves the fidelity of the input image compared to SDEdit alone.}
    \label{fig:stylediffusion_sdedit}
\end{figure}
After inverting a real image with StyleDiffusion, we leverage SDEdit to edit it.  Only using SDEdit, the results suffer from unwanted changes, such as the orientation of the dog (Fig.~\ref{fig:stylediffusion_sdedit} (first row, second column) and the texture detail of the leg of the dog (Fig.~\ref{fig:stylediffusion_sdedit} (second row, second column)).  While combining StyleDiffusion and SDEdit significantly enhances the fidelity to the input image (see Fig.~\ref{fig:stylediffusion_sdedit} (the third column)). This indicates our method exhibits robust performance when combining different editing techniques (e.g., SDEdit and P2Plus). 



\minisection{\textit{Comparison with optimization-free  methods.}}
Recently, some methods without optimization have been proposed~\citep{miyake2023negative,han2023improving}. 
Negative-prompt inversion (NPI) ~\citep{miyake2023negative} replaces the null-text embedding of the unconditional branch with the textural embedding in SD to implement reconstruction and editing.
Proximal Negative-Prompt Inversion (ProxNPI)~\citep{han2023improving} attempts to enhance NPI by introducing regularization terms using proximal function and reconstruction guidance based on the foundation of NPI.
While these methods do not require optimizing parameters to achieve the inversion of real images, similar to the method shown in Fig.~\ref{fig:survey}, they suffer from challenges when reconstructing and editing images containing intricate content and structure (see Fig.~\ref{fig:npi} (the second and third columns, the sixth and seventh columns)).
Due to the absence of an optimization process in these methods, it is not feasible to utilize attention loss to refine the attention maps like Null-text+$\mathcal{L}_{att}$ (Fig.~\ref{fig:attnloss}), consequently limiting the potential for enhancing reconstruction and editing quality.


\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{imgs/failure_compressed.pdf}
        \vspace{-2mm}
        \caption{Some examples of failure cases.
        \vspace{2mm}}
    \label{fig:failure_compressed}
\end{figure}


\minisection{\textit{StyleDiffusion for style transfer.}}
As a final illustration, we show that StyleDiffusion can be used to perform style transfer. 
As shown in  Fig.~\ref{fig:styletransfer}(left),  given a content image, we use DDIM inversion to generate a series of timestep-related latent codes. They are then progressively denoised using DDIM sampling. During this
process, we extract the spatial features from the decoder layers. These spatial features are injected into the corresponding layers of StyleDiffusion model. Note that we first optimize StyleDiffusion to reconstruct the style image, then use both the well-learned $M_t$ and the extracted content feature to perform the style transfer task.  Fig.~\ref{fig:styletransfer}(right) exhibits that we can successfully combine both content and style images, and perform style transfer.




\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{imgs/styletransfer_compressed.pdf}
        \vspace{-2mm}
        \caption{\textit{\textbf{StyleDiffusion for style transfer.}} (Left) our framework for style transfer. Given a content image, we use DDIM inversion to generate a series of timestep-related latent codes. They are then progressively denoised using DDIM sampling. During this
process, we extract the spatial features from the decoder layers. These  spatial features are injected into the corresponding layers of StyleDiffusion model. Note we first optimize StyleDiffusion to reconstruct the style image, then use both the learned-well $M_t$ and the extracted content feature to perform the style transfer task.  
This approach allows us to efficiently transfer the desired artistic style to the content image without the need for additional optimization on the content image. (Right) results of style transfer with StyleDiffusion.
        \vspace{2mm}}
    \label{fig:styletransfer}
\end{figure*}


\section{Conclusions and Limitations} 
We propose a new method for real image editing.  We invert the real image into the input of the value linear mapping network in the cross-attention layers, and freeze the input of the key linear  layer with the textual embedding provided by the user. This allows us to learn initial attention maps, and an approximate trajectory to reconstruct the real image.  We introduce a new attention regularization to preserve the attention maps after editing, enabling us to obtain more accurate editing capabilities. In addition, we propose attention injection in the unconditional branch of the classifier-free diffusion model (P2Plus), further improving the editing capabilities, especially when both source and target prompts have a large domain shift. 

While StyleDiffusion successfully modifies the real image, it still suffers from some limitations.  Our method fails to generate satisfying images when the object of the real image has a rare pose (Fig.~\ref{fig:failure_compressed} (left)), or both the source and the target prompts have a large semantic shift (Fig.~\ref{fig:failure_compressed} (right)).


\bibliographystyle{spbasic}      \bibliography{shortstrings,refs}

\end{document}
