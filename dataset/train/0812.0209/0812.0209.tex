\documentclass[10pt]{article}

\usepackage{fullpage}
\usepackage{citesort}
\usepackage{times}
\usepackage{epsfig}
\usepackage{color}
\usepackage{amssymb}
\usepackage{jsv1}

\newcommand{\eps}{\epsilon}



\title{Optimal Tracking of Distributed Heavy Hitters and Quantiles}

\author{Ke Yi \and Qin Zhang}

\date{Department of Computer Science and Engineering \\
Hong Kong University of Science and Technology \\
yike, qinzhangcse.ust.hk}

\begin{document}

\maketitle

\begin{abstract}
  We consider the the problem of tracking heavy hitters and quantiles in
  the distributed streaming model.  The heavy hitters and quantiles are two
  important statistics for characterizing a data distribution.  Let  be
  a multiset of elements, drawn from the universe .  For a
  given , the -heavy hitters are those elements of
   whose frequency in  is at least ; the -quantile of
   is an element  of  such that at most  elements of 
  are smaller than  and at most  elements of  are
  greater than .  Suppose the elements of  are received at  remote
  {\em sites} over time, and each of the sites has a two-way communication
  channel to a designated {\em coordinator}, whose goal is to track the set
  of -heavy hitters and the -quantile of  approximately at
  all times with minimum communication.  We give tracking algorithms with
  worst-case communication cost  for both problems,
  where  is the total number of items in , and  is the
  approximation error.  This substantially improves upon the previous known
  algorithms.  We also give matching lower bounds on the communication
  costs for both problems, showing that our algorithms are optimal.  We
  also consider a more general version of the problem where we
  simultaneously track the -quantiles for all .
\end{abstract}



\section{Introduction}
Data streams have been studied in both the database and theory communities
for more than a decade \cite{alon99,babcock02:_model_}.  In this model,
data items arrive in an online fashion, and the goal is to maintain some
function  over all the items that have already arrived using small
space.  A lot of 's have been considered under the streaming model.  The
theory community have studied various frequency moments
\cite{alon99,indyk05:_optim_,woodruff04:_optim_}, geometric problems
\cite{Indyk:04,suri04:_range,agarwal07:_space_}, and some graph problems
\cite{feigenbaum05:_graph_,bar-yossef02:_reduc}.  While the database
community have mostly focused on maintaining the frequent items (a.k.a.\
{\em heavy hitters})
\cite{cormode08:_findin,cormode03,karp03,metwally06,manku02:_approx} and
quantiles \cite{gilbert02:_how,cormode06:_space,greenwald01:_space}, two
very important statistics for characterizing a data distribution.  Since we
cannot afford to store all the items, we can only maintain an approximate
 (except for some trivial 's), and all the results in the streaming
model are expressed as a tradeoff between the approximation error 
and the space used by the algorithm.  After a long and somehow disorganized
line of research, the heavy hitter problem is now completely understood
with both space upper and lower bounds determined at ;
please see the recent paper by Cormode and Hadjieleftheriou
\cite{cormode08:_findin} for a comprehensive comparison of the existing
algorithms for this problem, both theoretically and empirically.  For
maintaining quantiles, the best upper bound is due to a sketch structure by
Greenwald and Khanna \cite{greenwald01:_space}, using space  where  is the number of items in the stream.  This is
conjectured to be optimal but not yet proved.

Recent years have witnessed an increasing popularity of another model more
general than the streaming model, where multiple streams are considered.
In this model, multiple streams are received at multiple distributed {\em
  sites}, and again we would like to continuously track some function 
over the union of all the items that have arrived across all the sites.
Here the most important measure of complexity is the total communication
cost incurred during the entire tracking period.  This model, which is
either referred to as the {\em distributed streaming model} or the {\em
  continuous communication model}, is a natural combination of the
classical communication model \cite{yao79} and the data stream model.
Recall that the communication model studies the problem of computing some
function  over distributed data using minimum communication.  The
data is predetermined and stored at a number of sites, which communicate
with a central coordinator, and the goal is to do a {\em one-time}
computation of the function .  Thus the distributed streaming model is
more general as we need to maintain  continuously over time as items
arrive in a distributed fashion.

The rising interest on the distributed streaming model is mainly due to its
many applications in distributed databases, wireless sensor networks, and
network monitoring.  As a result, it has attracted a lot of attention
lately in the database community, resulting in a flurry of research in this
area
\cite{Cormode:Muthukrishnan:Zhuang:07,cormode06:what,keralapura06,Cormode:Garofalakis:Muthukrishnan:Rastogi:05,cormode05:sketch,Olston:Jiang:Widom:03,Babcock:Olston:03,fuller07:_fids,deshpande04:_model,manjhi05:_findin,olston05:_effic,sharfman08:_shape}.
However, nearly all works in this area are heuristic and empirical
in nature, with a few exceptions to be mentioned shortly.  For many
fundamental problems in this model, our theoretical understandings are
still premature.  This is to be contrasted with the standard streaming
model, where theory and practice nicely blend, and in fact many of the most
practically efficient solutions are the direct products of our theoretical
findings.  In this paper, we take an important step towards an analytical
study of the distributed streaming model, by considering the worst-case
communication complexity of tracking heavy hitters and quantiles, arguably
two of the most fundamental problems on data streams.

\paragraph{The distributed streaming model.}
We now formally define the distributed streaming model, which is the same
as in most works in this area.  Let  be a sequence of
items, where each item is drawn from the universe .  The
sequence  is observed in order by  remote {\em sites}  collectively, i.e., item  is observed by exactly one of
the sites at time instance , where .  Let
 be the multiset of items that have arrived up until time  from
all sites.  Then the general goal is to continuously track  for
some function  at all times  with minimum total communication among
the sites.  Note that in the classical communication model, the goal is to
just compute ; in the data stream model, the goal is to
track  for all  but there is only one site (), and we are
interested in the space complexity of the tracking algorithm, not
communication.  Thus, the distributed streaming model is a natural
combination of the two, but is also significantly different from either.

We define the manner of communication more precisely as follows.  There is
a distinguished {\em coordinator} , who will maintain (an approximate)
 at all times.  There is a two-way communication channel between the
coordinator and each of the  sites, but there is no direct communication
between any two sites (but up to a factor of 2, this is not a restriction).
Suppose site  receives the item  at time .  Based on its
local status,  may choose to send a message to , which in turn may
trigger iterative communication with other sites.  We assume that
communication is instant.  When all communication finishes, all the sites
who have been involved may have new statuses, getting ready for the next
item  to arrive.  We will measure the communication cost in terms
of words, and assume that each word consists of  bits.  Finally we assume that  is sufficiently large
(compared with  and ); if  is too small, a naive solution
that transmits every arrival to the coordinator would be the best.

In this paper we will focus on the communication cost (or simply the {\em
  cost}).  Nevertheless, all the algorithms proposed in this paper can be
implemented both space- and time-efficiently.

\paragraph{Heavy hitters and quantiles.}
By taking different 's, we arrive at different continuous tracking
problems.  The notion of -approximation also differs for different
functions.  We adopt the following agreed definitions in the literature.
In the sequel, we abbreviate  as  when there is no confusion.

For any , let  be the number of occurrences of  in .
For some user specified , the set of {\em -heavy
  hitters} of  is , where  denotes the total number of items in .  If an
-approximation is allowed, then the returned set of heavy hitters
must contain  and cannot include any  such that
.  If , then
 may or may not be reported.  In the {\em heavy hitter tracking}
problem, the coordinator should always maintain an approximate
 at all times for a given .

For any , the {\em -quantile} of  is some  such that at most  items of  are smaller than  and at
most  items of  are greater than .  The quantiles are
also called {\em order statistics} in the statistics literature.  In
particular, the -quantile is also known as the {\em median} of
.  If an -approximation is allowed, we can return any
-quantile of  such that .
In the {\em -quantile tracking} problem, the coordinator needs to
keep an -approximate -quantile of  at all times for a given
.  We also consider a more general version of the problem, where we
would like to keep track of all the quantiles approximately.  More
precisely, here the ``function''  is a data structure from which an
-approximate -quantile for any  can be extracted.  Note
that such a structure is equivalent to an (approximate) equal-height
histogram, which characterizes the entire distribution.

In particular, from an {\em all-quantile} structure, we can easily obtain
the -approximate -heavy hitters for any , as observed
in \cite{Cormode:Garofalakis:Muthukrishnan:Rastogi:05}.  Therefore, the
all-quantile tracking problem is more general than either the
-heavy hitter tracking problem or the -quantile tracking
problem.  In the rest of the paper, we omit the word ``approximate'' when
referring to heavy hitters and quantiles when the context is clear.

\paragraph{Previous works.}
Traditionally, query answering in distributed databases follows a ``poll''
based approach, that is, the coordinator collects information from the
sites to answer a query posed by the user using minimum communication.
Such a paradigm falls into the realm of the classical multi-party
communication theory.  These queries are also referred to as {\em one-shot}
queries in the literature.  As long-standing queries that need to be
answered continuously become common in many modern applications such as
sensor network monitoring, network anomaly detection, publish-subscribe
systems, etc., periodically polling all the sites is neither efficient nor
effective (i.e., long latency).  Thus, the trend is moving towards a
``push'' based approach \cite{Jain:Hellerstein:Ratnasamy:Wetherall:04}, in
which the sites actively participate in the tracking process.  In this
framework, each site maintains some local conditions, and will not initiate
communication unless one of the conditions is triggered.  Such an approach
often leads to much reduced communication overhead compared with the
``poll'' based approach, since the system will react only when
``interesting'' things are happening.  This is the main motivation that has
led to the distributed streaming model described above.

Various 's have been considered under this framework.  The simplest case
 just counts the total number of items received so far across
all the sites.  This problem can be easily solved with  communication where each site simply reports to the coordinator
whenever its local count increases by a  factor
\cite{keralapura06}.  The other important single-valued statistics are the
frequency moments: .   is the number of
distinct items, and can be tracked with cost  \cite{graham08};  is the self-join size and can
be tracked with cost  \cite{graham08}.  Some heuristic approaches
based on predicting future arrivals of items have been proposed in
\cite{cormode06:what,cormode05:sketch}.

Single-valued statistics have very limited expressive power, so
multi-valued statistics are often necessary to better capture the
distribution of data.  The most important ones include the heavy hitters
and quantiles, and they have also been studied under the distributed
streaming framework.  Babcock and Olston \cite{Babcock:Olston:03} designed
some heuristics for the top- monitoring problem, where the goal is to
track the  most frequent items (whose frequency may not be larger than
).  Their techniques can be adapted to tracking the heavy hitters
\cite{fuller07:_fids}, but the approach remains heuristic in nature.
Manjhi et al.~\cite{manjhi05:_findin} also studied the heavy hitter
tracking problem, but their communication model and the goal are different:
They organize the sites in a tree structure and the goal is to minimize the
communication only at the root node.  The all-quantile tracking problem has
been studied by Cormode et
al.~\cite{Cormode:Garofalakis:Muthukrishnan:Rastogi:05}, who gave an
algorithm with cost .  As commented earlier, this
also implies a heavy hitter tracking algorithm with the same cost.  This
remains the best communication upper bound for both problems to date.  No
lower bound is known.

\paragraph{Our results.}
Our main results in this paper are the matching upper and lower bounds on
the communication cost for deterministic algorithms for both the heavy
hitter tracking problem and the quantile tracking problem.  Specifically,
we show that for any , both the -heavy hitters
(Section~\ref{sec:track-heavy-hitt}) and the -quantile
(Section~\ref{sec:track-median}) can be tracked with total communication
cost .  This improves upon the previous result of
\cite{Cormode:Garofalakis:Muthukrishnan:Rastogi:05} by a 
factor.  We also give matching lower bounds for both problems, showing that
our tracking protocols are optimal in terms of communication.  Note that in
the classical communication model, we can easily do a one-shot computation
of the -heavy hitters and the -quantile easily with cost
, as observed in
\cite{Cormode:Garofalakis:Muthukrishnan:Rastogi:05}.  Interestingly, our
results show that requiring the heavy hitters and quantiles to be tracked
at all times indeed increases the communication complexity, but only by a
 factor.  In Section~\ref{sec:tracking-quantiles}, we give
an algorithm that tracks all quantiles with cost .  Because this problem is more difficult than
the single-quantile problem, it has the same lower bound of  as the latter.  Thus, our all-quantile tracking algorithm is
also optimal up to a  factor.



\section{Tracking the Heavy Hitters}
\label{sec:track-heavy-hitt}



\subsection{The upper bound}
\label{sec:upper-bound}
\paragraph{The algorithm.}
Let  be the current size of . First, the coordinator 
always maintains , an -approximation of .  This can
be achieved by letting each site send its local count every time
it has increased by a certain amount (to be specified shortly).
Each site  maintains the exact frequency of each  at
site , denoted , at all times. The overall frequency
of  is . Of course, we cannot afford to
keep track of  exactly.  Instead, the coordinator  maintains
an underestimate  of , and sets  as an estimate of .   will send its
local increment of  to , hence updating ,
from time to time following certain rules to be specified shortly.
In addition, each site  maintains , an estimate of
, a counter , denoting the increment of
 since its last communication to  about , as well
as a counter  for each , denoting the
increment of  since its last communication to  about
.

We can assume that the system starts with  items;
before that we could simply send each item to the coordinator.  So
when the algorithm initiates, all the estimates are exact. We
initialize  and  for all 
to be 0. The protocols of tracking the -heavy hitters are as
follows.
\begin{enumerate}
\item {\em Each site :} When a new item of  arrives,
 and  are incremented by 1.
When  (resp.\ ) reaches , site  sends a message  (resp.\ ) to the
coordinator, and resets  (resp.\
) to 0.

\item {\em Coordinator:} When  has received a message  or , it
updates  to  or  to
, respectively. Once  has
received  signals in the forms of , it collects the local counts from each site to compute
the exact value of , sets , and then broadcasts 
to all sites.  Then each site  updates its  to .
After getting a new ,  also resets  to
0.
\end{enumerate}

Finally, at any time, the coordinator  declares an item  to be a
-heavy hitter if and only if



\paragraph{Correctness.}
To prove correctness we first establish the following invariants
maintained by the algorithm.



The second inequalities of both (\ref{eq:bound_C.m}) and
(\ref{eq:bound_C.m_x}) are obvious. The first inequality of
(\ref{eq:bound_C.m_x}) is valid since once a site  gets
 items of , it sends a message to the
coordinator and the coordinator updates  accordingly. Thus
the maximum error of  in the coordinator is at most
. The first inequality of (\ref{eq:bound_C.m}) follows
from a similar reason. Combining (\ref{eq:bound_C.m_x}) and
(\ref{eq:bound_C.m}), we have

which guarantees that the approximate ratio  is within
 of , thus classifying an item using (\ref{eq:hh})
will not generate any false positives or false negatives.

\paragraph{Analysis of communication complexity.}
We divide the whole tracking period into rounds. A round start
from the time when the coordinator finishes a broadcast of  to the
time when it initiates the next broadcast. Since the coordinator
initiates a broadcast after  is increased by a factor of
, the number of rounds is
bounded by



In each round, the number of messages in the form of  sent by all the sites is  by the definition of our
protocol. Since there are  rounds in total, the number of
messages in the form of  can be bounded by
. On the other hand, it is easy to see that total
number of messages of the form  is no more than
the total number of messages of the form .
Therefore, the total cost of the whole system is bounded by
.


\begin{theorem}
For any , there is a deterministic algorithm that
  continuously tracks the -heavy hitters and incurs a total
  communication cost of 
  .
\end{theorem}

\paragraph{Implementing with small space.}
In the algorithm described above, we have assumed that each site maintains
all of its local frequencies  exactly.  In fact, it is not
difficult to see that our algorithm still works if we replace these exact
frequencies with a heavy hitter sketch, such as the {\em space-saving}
sketch \cite{metwally06}, that maintains the local -approximate
frequencies for all items for some .  More precisely,
such a sketch gives us an approximate  for any  with
absolute error at most , where  denotes the current
number of items received at  so far.  We need to adjust some of the
constants above, but this does not affect our asymptotic results.  By using
such a sketch at each site, our tracking algorithm can be implemented in
 space per site and amortized  time per item.

\subsection{The lower bound} \label{sec:lower-bound-heavy-hitt}

To give a lower bound on the total communication cost that any
deterministic tracking algorithm must take, we first consider the number of
changes that the set of heavy hitters could experience, where a {\em
  change} is defined to be the transition of the frequency of an item from
above  to below , or the other way round.  Then we
show that to correctly detect each change, the system must exchange at
least a certain amount of messages. The following lemma could be
established by construction.

\begin{lemma}
\label{lem:numberOfOutputUpdates} For any , there is a
sequence of item arrivals such that the set of heavy hitters in
the whole tracking period will have 
changes.
\end{lemma}

\begin{proof}
  Set . We construct two groups of 
  items each:  and
  . Since we only care
  about the total number of changes of the set of heavy hitters during the
  whole tracking period, we temporarily treat the whole system as one big
  site and items come one by one. We will construct an input sequence under
  which the set of heavy hitters will undergo  changes.

We still divide the whole tracking period to several rounds, and
let  denote the total number of items when round  starts.
The following invariant will be maintained throughout the
construction:
\begin{enumerate}
\item[] Let .  When round  starts, all items  have frequency , and all items  have frequency .
\end{enumerate}
It can be verified that the total frequency of all items is indeed
. Note that from the start of round  to the end of round
, all the non-heavy hitters become heavy hitters, and all the
heavy hitters become non-heavy hitters. In what follows we only
care about the changes of the former type, which lower bounds
the number of changes. To maintain the
invariant for round , we construct item arrivals as follows.
Without loss of generality, suppose . Let . We first generate
  copies of , and then 
copies of , \dots, then  copies of , in sequence.
After these items we end round  and start round
. At this turning point, the total number of items is

Now the frequency of each item in the set  is

and the frequency of each item in  remains the
same, that is, .  Now we have
  restored the invariant and can start round .

Finally, we bound the number of rounds.  Since the total number of
items  increases by a  factor in each
round, the total number of rounds is
. Consequently, the
total number of changes in the set of heavy hitters (from
non-heavy hitters to heavy hitters) is .
\end{proof}

Now we go back to the distributed scenario and consider the cost of
communication for ``recognizing'' each change. Because we allow some
approximation when classifying heavy hitters and non-heavy hitters, the
valid time to report a change is actually a time interval, from
the time when its frequency just passes  to the time when
its frequency reaches .  As long as the tracking algorithm signals
the change within this interval, the algorithm is considered to be correct.
Consider the construction in the proof of
Lemma~\ref{lem:numberOfOutputUpdates}.  In round , the transition
interval from a non-heavy hitter to a heavy hitter for an item  must lie
inside the period in which the  copies of  arrive.  Below we
will show that in order for the coordinator to signal the change within
this period,  messages have to be exchanged in the worst case
using an adversary argument.

Before presenting the lower bound proof, let us be more precise about
the computation model.  Recall that in the introduction, the model
forbids a site to spontaneously initiate communication or change its
local status; actions can only be triggered as a result of the
arrival of an item at this site, or in response to the coordinator.  Note
that for deterministic algorithms this is not a restrictive
assumption. 
In our case, since we only care about the frequency of a particular
item  increasing from  to , we may assume that
each site  has a {\em triggering threshold} , meaning that
 will only initiate communication when the number of copies of
 received by  is .  When all the communication triggered
by the arrival of an item finishes, all the sites that have
participated are allowed to update their triggering thresholds, but
the rest of the sites must retain their old thresholds.  

\begin{lemma}
\label{lem:costEachUpdate} To correctly recognize a change in
the heavy hitters under the input constructed in the proof of
lemma~\ref{lem:numberOfOutputUpdates}, any deterministic algorithm has to
incur a communication cost of .
\end{lemma}

\begin{proof}
  We will construct an adversary who will send the  copies of
   to the sites in a way such that at least  sites must
  communicate with the coordinator.  Since we are dealing with
  deterministic algorithms, we may assume that the adversary knows the
  triggering thresholds  at any time.

Initially, we must have

Otherwise, the adversary can send  copies to  for all 
without triggering any communication, and make the algorithm miss the
change.  Therefore there must be some  such that .  The adversary first sends  copies of
 to .   will then communicate with the coordinator at least
once.  After the first  copies, the new triggering thresholds
must still satisfy (\ref{eq:lb}).  Similarly, there is some , and the adversary will send another  copies
of  to .  Such a process can be repeated for  times, triggering at least 
messages of communication.
\end{proof}

The following lower bound follows immediately from
Lemma~\ref{lem:numberOfOutputUpdates} and
Lemma~\ref{lem:costEachUpdate}, for the reason that the tracking
algorithm has to correctly and continuously maintain the whole set
of heavy hitters.

\begin{theorem}
\label{thm:lowerbound-hh}
Any deterministic algorithm that continuously tracks the -heavy
hitters has to incur a total communication cost of , for any .
\end{theorem}

\paragraph{Remark.}
Note that our lower bound above is actually lower bound on the number of
messages required.  Also recall that our algorithm in
Section~\ref{sec:upper-bound} sends  messages and
each message if of constant size.  Our lower bound implies that one cannot
hope to reduce the number of messages by making each of them longer.

 
 


\section{Tracking the Median}
\label{sec:track-median}

In this section we first present an algorithm to track any -quantile
for .  For ease of presentation we describe how to track
the median (the -quantile); the generalization to any -quantile
is straightforward. Then we give a matching lower bound.

\subsection{The upper bound}
For simplicity we assume that all the items in  are distinct; issues
with ties can be easily resolved by standard techniques such as symbolic
perturbation.  We divide the whole tracking period into  rounds;
whenever  doubles, we start a new round.  In the following we focus on
one round, and show that our median-tracking algorithm has a communication
cost of .

Let  be the cardinality of  at the beginning of a round.  Note that
 is fixed throughout a round and we always have . 
The main
idea of our algorithm is to maintain a 
dynamic set of disjoint intervals in the coordinator (by
maintaining a set of separating items), such that each interval
contains between  and  items.
We first show that if we have such a set of intervals, the median
can be tracked efficiently.  Afterward we discuss how to maintain
these intervals.

Let  denote the approximate median that is kept at the coordinator.  We
maintain two counters  and , counting the number
of items that have been received at all sites to the left and the right of
, respectively. These two counters are maintained as underestimates with
an absolute error at most , by asking each site to send
in an update whenever it has received  items to the left
or right of .  So the cost of maintaining them is .

Whenever , we
update  as follows.

\begin{enumerate}
\item  Compute  and  as the total number of items to the left and
  the right of .  W.l.o.g., suppose  and let .

\item Compute a new median  such that  where  is the rank of  in .  Update  to
  .  Note that  is at most  items away from the
  exact median.  We will describe how to compute such an  shortly.

\item Reset  and  to 0.
\end{enumerate}

For the correctness of the algorithm, we can show that our
tracking algorithm always maintains an approximate median that is
at most  items away
from the exact median.  The first term  is due to
the fact that whenever we update ,  is within an error of
at most  to the exact median.  The second term
 accounts for the error introduced by the
triggering condition  monitored in
the coordinator. Note that we keep both  and
 within an additive error of at most  and whenever , we initiate an update.  Therefore, the total error
introduced is at most .

Now we analyze the communication cost. Step 1 could be done by
exchanging  messages. For step 2, first note that  since by the reasoning above,  is still an
-approximate median.  Next, we can find  quickly with
the help of the set of intervals. We start by finding the first
separating item  of the intervals to the left of , and
then collect information from all sites to compute the number of
items in the interval , say .  If , we are done; otherwise we go on to pick the
second separating item  to the left of , and check if
, where  is the number of items
in the interval . It is easy to see that after at most
 such probes, we can find an item  such that the rank
difference between  and the exact median is no more than
. Note that the cost of each probe is  thus the
total cost of step 2 is . Finally, we update  at most  times within
a single round, since each update increases  by at least a
factor of . To sum up, the total cost of
the algorithm within a round is  provided that the
dynamic set of intervals are maintained.

\paragraph{Maintaining the set of intervals.}
When a new round starts, we initialize
the set of intervals as follows: Each site 
computes a set of intervals, each containing  items,
where  stands for the set of items  has received, and then sends
the set of intervals to the coordinator (by sending those separating
items).  Then the coordinator can compute the rank of any  with an
error of at most ,
therefore it can compute a set of intervals, each of which contains at
least  and at most  items. After the
coordinator has built the set of intervals, it broadcasts them to all the
 sites, and then computes the exact number of items in each interval.
  The cost of each rebuilding is .

During each round, each site  maintains a counter for each interval as
new items arrive. And whenever the local counter of items in some interval
 has increased by , it sends a message to
the coordinator and the coordinator updates the count for interval 
accordingly.  Whenever the count of some interval in the coordinator 
reaches , the coordinator splits the interval into two
intervals, each of which containing at least  and at most
 items.  To perform such a split, we can again call the
rebuilding algorithm above, except that the rebuilding is only applied to
the interval , so the cost is only .

The correctness of algorithm is obvious. The total communication
cost of interval splits is  in each round, since
there are at most  splits and each split incurs a
communication cost .

\begin{theorem}
  There is a deterministic algorithm that continuously tracks the
  -approximate median (and generally, any -quantile ) and incurs a total communication cost of .
\end{theorem}

\paragraph{Implementing with small space.}
Similar to our heavy hitter tracking algorithm, instead of maintaining the
intervals exactly at each site, we can again deploy a sketch that maintains
the approximate -quantiles for some  to maintain
these intervals approximately.  Suppose we use the Greenwald-Khanna sketch
\cite{greenwald01:_space}, then we can implement our -quantile
tracking algorithm with  space per site and
amortized  time per item.

\subsection{The lower bound}


The idea of the proof of the lower bound is similar as that for
the heavy hitters. We try to construct a sequence of input with
the following properties.
\begin{enumerate}
\item The median will change at least  times.

\item To correctly recognize each update, any deterministic
algorithm has to incur a communication cost of .
\end{enumerate}

Consider the following construction. The universe consists of only
two items  and . We divide the whole tracking period to
several rounds and let  be the number of items at the
beginning of round . We maintain the following invariant: When
round  starts, the frequency of item  is  and the frequency of item  is , where . This could be done by inserting
 copies of  during round 
and then start a new round. It is easy to see that there will be
at least  rounds and the median will change
at least once during each round, therefore the total number of
changes of the median is . For the second
property, we can invoke the same arguments as that for
Lemma~\ref{lem:costEachUpdate}.
Combining the two properties, we have the following.

\begin{theorem}
\label{thm:lowerbound-median}
  Any deterministic algorithm that continuously tracks the approximate
  median has to incur a total communication cost of
  .
\end{theorem}




 
\newcommand{\T}{\mathcal{T}}
\newcommand{\hmax}{h}

\section{Tracking All Quantiles}
\label{sec:tracking-quantiles}

In this section, we give a tracking algorithm so that the
coordinator  always tracks the -approximate
-quantiles for all  simultaneously.  We will solve
the
following equivalent problem: The coordinator is required to
maintain a data structure from which we can extract the rank
 for any  in  with an additive error at most
. We still assume that all items in  are distinct.

We divide the whole tracking period into  rounds.  In each round
 roughly doubles. We will show that the algorithm's cost in each round
is .  The algorithm restarts itself
at the beginning of each round, therefore the total communication of the
algorithm will be .

\paragraph{The data structure.}
Let  be the cardinality of  at the beginning of a round. The data
structure is a binary tree  with  leaves.  The root 
of  corresponds to the entire .  It stores a splitting element 
which is an approximate median of , i.e., it divides  into two parts,
either of which contains at least  and at most
 items, for some constant .
Then we recursively build 's left and right subtrees on these two parts
respectively, until there are no more than  items left.  It is
clear that  has  nodes in total, and has height at most
, though it is not necessarily balanced.  Each
node in  is naturally associated with an interval.  Let  be the
interval associated with .  Then  is the entire ; suppose 
and  are 's children, then  is divided into  and  by
.  Set .  Each node  of  is in
addition associated with , which is an underestimate of 
with an absolute error of at most , i.e., . Please see Figure~\ref{fig:tracking_quan} for an
illustration of the data structure.


\begin{figure}
\begin{center}
\includegraphics[width=12cm]{tracking_quan.eps}
\caption{The data structure that can be used to extract the rank of any
 with absolute error .} \label{fig:tracking_quan}
\end{center}
\end{figure}

If the coordinator has such a data structure, it is not difficult to see
that we can compute the rank of  with an absolute error of at most .  For a given , we first search down the binary tree and locate the
leaf  such that .  As we go along the root-to-leaf path,
whenever we follow a right child, we add up the  of its left sibling.
In the end we add up  such partial sums, each contributing an error
of at most , totaling .
Finally, since , the sum of all the 's for
the preceding intervals of  is off by at most  from the actual
rank of .

\paragraph{Initialization.}
At the beginning of each round, we initialize the data structure
similarly as in Section~\ref{sec:track-median}. Suppose the set of
items at  is . Each site  builds its own structure
, but with  as the error parameter, and ships to
.  This costs a communication of . Note that
 allows one to extract the rank of any  within 
with an error of . By querying each , the
coordinator can compute the rank of any  with an error of
, which is
enough for the coordinator to build its own .  In
particular, all the splitting elements can be chosen to be within
a distance of  to the real median. After
building , the coordinator broadcasts it to all the sites,
costing communication .  Now each site  knows how
 is subdivided into those  intervals represented by the
binary tree . Then for
each interval , it computes  and sends the count to
, so that the coordinator has all the exact partial sums 
to start with. It is easy to see that the total communication cost
for initializing the data structure is .

\paragraph{Maintaining the partial sums.}
As items arrive, each site  monitors all the intervals 
in . For each , every time the local count of items in
 at  has increased by , it sends an
updated local count to .  Thus in the worst case, each site is
holding  items that have not been reported,
leading to a total error of at most .  The cost of these
messages can be bounded as follows.  When  sends a new count
for some interval , we charge the cost to the 
new items that have arrived since the last message for ,  each.  Since each item contributes to the counts of
at most  intervals, it is charged  times, so the
total cost charged to one item is .
There are a total of  items in a single round, so the
overall cost is .

\paragraph{Maintaining the splitting elements.}
The maintenance algorithm above ensures that all the  are within the
desired error bound.  We still need to take care of all the splitting
elements, making sure that they do not deviate from the real medians too
much.  Specifically, when we build , for any  with children  and
, we ensure that

This property can be easily established during initialization, since  for any internal node  of , and we can
estimate  with an error of .  In the middle
of the round, we maintain the following condition:

Recall that  (resp.\ ) is an estimate of  (resp.\
) with an error of at most .  As long as
(\ref{eq:3}) holds, we have

Rearranging,

for . (Note that assuming  larger than any constant does not
affect our asymptotic results.)  Similarly, we also have .  Thus condition~(\ref{eq:3}) ensures that the
height of  is bounded by .

Whenever (\ref{eq:3}) is violated, we do a partial rebuilding of the
subtree rooted at  to restore this condition.  If multiple conditions
are violated at the same time, we rebuild at the highest such node.  To
rebuild the subtree rooted at , we apply our initialization algorithm,
but only for the range .  This incurs a cost of , since we are essentially building a new data structure
on  elements with error parameter .  After rebuilding, we have restored (\ref{eq:2}) for  and all its
descendants.

It remains to bound the cost of the partial rebuildings.  Similarly as
before, we can show that when (\ref{eq:3}) is violated, we must have

or

assuming .  Note that both  and  may
increase.  From (\ref{eq:2}) to (\ref{eq:4}),  must increase
by ; from (\ref{eq:2}) to
(\ref{eq:5}),  must increase by , which
implies that  must also increase by 
since .  This means that between two partial rebuildings
of ,  must have increased by a constant factor.  Thus, we
can charge the rebuilding cost of  to the  new
items that have arrived since the last rebuilding,   each.
Since each item is contained in the intervals of  nodes, it is
charged a cost of  in total.  Therefore, the total cost of
all the partial rebuildings in this round is .

\paragraph{Maintaining the leaves.}
Finally, we need to make sure that  for
each leaf  as required by the data structure.  During initialization, we
can easily ensure that .  During the round, the coordinator monitors , and
will split  by adding two new leaves below  whenever .  Since  has error at most , this
splitting condition will ensure that .
To split , we again call our initialization algorithm on the interval
, incurring a cost of .  Since
we create at most  leaves in this entire round, the total cost
for all the splittings is .

\smallskip Putting everything together, we obtain the following result.

\begin{theorem}
  There is a deterministic algorithm that continuously tracks the
  -quantiles for all  simultaneously and incurs a
  total communication cost of
  .
\end{theorem}

\paragraph{Implementing with small space.}
Similar as before, instead of maintaining the counts in the intervals
associated with  exactly at each site, we can again deploy a sketch
that maintains the approximate -quantiles for some
 to maintain these intervals approximately.  Suppose
we use the Greenwald-Khanna sketch \cite{greenwald01:_space}, then we can
implement our all-quantile tracking algorithm with  space per
site and amortized  time per item.


 
\section{Open Problems}
We have restricted ourselves to deterministic algorithms in the paper.  If
randomization is allowed, simple random sampling can be used to achieve a
cost of  for tracking both the
heavy hitters and the quantiles.  This observation has been well exploited
in maintaining the heavy hitters and quantiles for a single stream when
both insertions and deletions are present (see e.g.\
\cite{gilbert02:_how}).  This breaks the deterministic lower bound for
.  It is not known if randomization can still help for
smaller .  Deriving lower bounds for randomized algorithms is also an
interesting open problem.  Another possible direction is to design
algorithms to track the heavy hitters and quantiles within a sliding window
in the distributed streaming model.

\bibliographystyle{abbrv}
\begin{thebibliography}{10}

\bibitem{agarwal07:_space_}
P.~K. Agarwal and H.~Yu.
\newblock A space-optimal data-stream algorithm for coresets in the plane.
\newblock In {\em Proc. ACM Symposium on Computational Geometry}, 2007.

\bibitem{alon99}
N.~Alon, Y.~Matias, and M.~Szegedy.
\newblock The space complexity of approximating the frequency moments.
\newblock {\em Journal of Computer and System Sciences}, 58:137--147, 1999.
\newblock See also STOC'96.

\bibitem{babcock02:_model_}
B.~Babcock, S.~Babu, M.~Datar, R.~Motwani, and J.~Widom.
\newblock Models and issues in data stream systems.
\newblock In {\em Proc. ACM Symposium on Principles of Database Systems}, 2002.

\bibitem{Babcock:Olston:03}
B.~Babcock and C.~Olston.
\newblock Distributed top-k monitoring.
\newblock In {\em Proc. ACM SIGMOD International Conference on Management of
  Data}, 2003.

\bibitem{bar-yossef02:_reduc}
Z.~Bar-Yossef, R.~Kumar, and D.~Sivakumar.
\newblock Reductions in streaming algorithms, with an application to counting
  triangles in graphs.
\newblock In {\em Proc. ACM-SIAM Symposium on Discrete Algorithms}, 2002.

\bibitem{cormode05:sketch}
G.~Cormode and M.~Garofalakis.
\newblock Sketching streams through the net: {D}istributed approximate query
  tracking.
\newblock In {\em Proc. International Conference on Very Large Databases},
  2005.

\bibitem{Cormode:Garofalakis:Muthukrishnan:Rastogi:05}
G.~Cormode, M.~Garofalakis, S.~Muthukrishnan, and R.~Rastogi.
\newblock Holistic aggregates in a networked world: Distributed tracking of
  approximate quantiles.
\newblock In {\em Proc. ACM SIGMOD International Conference on Management of
  Data}, 2005.

\bibitem{cormode08:_findin}
G.~Cormode and M.~Hadjieleftheriou.
\newblock Finding frequent items in data streams.
\newblock In {\em Proc. International Conference on Very Large Databases},
  2008.

\bibitem{cormode06:_space}
G.~Cormode, F.~Korn, S.~Muthukrishnan, and D.~Srivastava.
\newblock Space- and time-efficient deterministic algorithms for biased
  quantiles over data streams.
\newblock In {\em Proc. ACM Symposium on Principles of Database Systems}, 2006.

\bibitem{cormode03}
G.~Cormode and S.~Muthukrishnan.
\newblock What's hot and what's not: tracking most frequent items dynamically.
\newblock In {\em Proc. ACM Symposium on Principles of Database Systems}, 2003.

\bibitem{graham08}
G.~Cormode, S.~Muthukrishnan, and K.~Yi.
\newblock Algorithms for distributed functional monitoring.
\newblock In {\em Proc. ACM-SIAM Symposium on Discrete Algorithms}, 2008.

\bibitem{cormode06:what}
G.~Cormode, S.~Muthukrishnan, and W.~Zhuang.
\newblock What's different: Distributed, continuous monitoring of
  duplicate-resilient aggregates on data streams.
\newblock In {\em Proc. IEEE International Conference on Data Engineering},
  pages 20--31, 2006.

\bibitem{Cormode:Muthukrishnan:Zhuang:07}
G.~Cormode, S.~Muthukrishnan, and W.~Zhuang.
\newblock Conquering the divide: Continuous clustering of distributed data
  streams.
\newblock In {\em Proc. IEEE International Conference on Data Engineering},
  2007.

\bibitem{deshpande04:_model}
A.~Deshpande, C.~Guestrin, S.~R. Madden, J.~M. Hellerstein, and
W.~Hong.
\newblock Model-driven data acquisition in sensor networks.
\newblock In {\em Proc. International Conference on Very Large Databases},
  2004.

\bibitem{feigenbaum05:_graph_}
J.~Feigenbaum, S.~Kannan, A.~McGregor, S.~Suri, and J.~Zhang.
\newblock Graph distances in the streaming model: The value of space.
\newblock In {\em Proc. ACM-SIAM Symposium on Discrete Algorithms}, 2005.

\bibitem{fuller07:_fids}
R.~Fuller and M.~Kantardzic.
\newblock {FIDS}: Monitoring frequent items over distributed data streams.
\newblock In {\em MLDM}, 2007.

\bibitem{gilbert02:_how}
A.~C. Gilbert, Y.~Kotidis, S.~Muthukrishnan, and M.~J. Strauss.
\newblock How to summarize the universe: Dynamic maintenance of quantiles.
\newblock In {\em Proc. International Conference on Very Large Databases},
  2002.

\bibitem{greenwald01:_space}
M.~Greenwald and S.~Khanna.
\newblock Space-efficient online computation of quantile summaries.
\newblock In {\em Proc. ACM SIGMOD International Conference on Management of
  Data}, 2001.

\bibitem{Indyk:04}
P.~Indyk.
\newblock Algorithms for dynamic geometric problems over data streams.
\newblock In {\em Proc. ACM Symposium on Theory of Computation}, 2004.

\bibitem{indyk05:_optim_}
P.~Indyk and D.~Woodruff.
\newblock Optimal approximations of the frequency moments of data streams.
\newblock In {\em Proc. ACM Symposium on Theory of Computation}, 2005.

\bibitem{Jain:Hellerstein:Ratnasamy:Wetherall:04}
A.~Jain, J.~Hellerstein, S.~Ratnasamy, and D.~Wetherall.
\newblock A wakeup call for internet monitoring systems: The case for
  distributed triggers.
\newblock In {\em Proceedings of the 3rd Workshop on Hot Topics in Networks
  ({H}otnets)}, 2004.

\bibitem{karp03}
R.~M. Karp, S.~Shenker, and C.~H. Papadimitriou.
\newblock A simple algorithm for finding frequent elements in streams and bags.
\newblock {\em ACM Transactions on Database Systems}, 2003.

\bibitem{keralapura06}
R.~Keralapura, G.~Cormode, and J.~Ramamirtham.
\newblock Communication-efficient distributed monitoring of thresholded counts.
\newblock In {\em Proc. ACM SIGMOD International Conference on Management of
  Data}, 2006.

\bibitem{manjhi05:_findin}
A.~Manjhi, V.~Shkapenyuk, K.~Dhamdhere, and C.~Olston.
\newblock Finding (recently) frequent items in distributed data streams.
\newblock In {\em Proc. IEEE International Conference on Data Engineering},
  2005.

\bibitem{manku02:_approx}
G.~Manku and R.~Motwani.
\newblock Approximate frequency counts over data streams.
\newblock In {\em Proc. International Conference on Very Large Databases},
  2002.

\bibitem{metwally06}
A.~Metwally, D.~Agrawal, and A.~E. Abbadi.
\newblock An integrated efficient solution for computing frequent and top-k
  elements in data streams.
\newblock {\em ACM Transactions on Database Systems}, 2006.

\bibitem{Olston:Jiang:Widom:03}
C.~Olston, J.~Jiang, and J.~Widom.
\newblock Adaptive filters for continuous queries over distributed data
  streams.
\newblock In {\em Proc. ACM SIGMOD International Conference on Management of
  Data}, 2003.

\bibitem{olston05:_effic}
C.~Olston and J.~Widom.
\newblock Efficient monitoring and querying of distributed, dynamic data via
  approximate replication.
\newblock {\em IEEE Data Engineering Bulletin}, 2005.

\bibitem{sharfman08:_shape}
I.~Sharfman, A.~Schuster, and D.~Keren.
\newblock Shape sensitive geometric monitoring.
\newblock In {\em Proc. ACM Symposium on Principles of Database Systems}, 2008.

\bibitem{suri04:_range}
S.~Suri, C.~Toth, and Y.~Zhou.
\newblock Range counting over multidimensional data streams.
\newblock In {\em Proc. ACM Symposium on Computational Geometry}, 2004.

\bibitem{woodruff04:_optim_}
D.~Woodruff.
\newblock Optimal space lower bounds for all frequency moments.
\newblock In {\em Proc. ACM-SIAM Symposium on Discrete Algorithms}, 2004.

\bibitem{yao79}
A.~C. Yao.
\newblock Some complexity questions related to distributive computing.
\newblock In {\em Proc. ACM Symposium on Theory of Computation}, 1979.

\end{thebibliography}


\end{document}
