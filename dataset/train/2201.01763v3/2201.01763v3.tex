\section{Experiments}
\label{sec:exp}


\subsection{Data and Experimental Setup}
\label{sec:exp-data-setup}
Our experiments are conducted on LRS3~\cite{afouras2018lrs3} with around 433 hours of audio-visual speech from over 5000 speakers, which is the largest publicly available labeled audio-visual speech recognition dataset. VoxCeleb2~
\cite{voxceleb2}, a large-scale audio-visual speech dataset that was initially proposed for the speaker recognition task is used for our self-supervised pre-training. VoxCeleb2 has around 2,442 hours of videos from over 6,000 speakers and contains utterances from multiple languages. We follow the preprocessing steps in~\cite{avhubert} to select the "English" portion, which amounts to 1,326 hours of videos.

We augment input samples using many noise categories. The total duration of noise in each category is shown in table~\ref{tab:noise-amount}. The noise audio clips in the categories of "\texttt{natural}", "\texttt{music}" and "\texttt{babble}" are sampled from MUSAN dataset~\cite{Snyder2015MUSANAM}, while the overlapping "\texttt{speech}" noise samples are drawn from LRS3. In creating "\texttt{speech}" and "\texttt{babble}" noise sets, we ensured there are no speaker overlap among different partitions.

\begin{table}[htp]
    \centering
    \caption{\label{tab:noise-amount}Total duration in hours of noise samples in different categories}
    \begin{tabular}{ccccc}
    \toprule
    Partition & natural & music & babble & speech \\
    \midrule
        train & 6 & 35 & 20 & 50 \\
        validation & 1 & 4 & 2 & 6\\
        test & 1 & 4 & 2 & 6\\
        \bottomrule
    \end{tabular}
\end{table}

We follow the protocol of~\cite{avhubert} to create two settings for finetuning the model; a low-resource setting using 30h of labeled videos and a mid-resource setting using 433h of labels. Unless otherwise specified, we use AV-HuBERT LARGE as the default model architecture for all our experiments. The model has 24 transformer blocks, where each block has 16 attention heads and 1024/4096 embedding/feedforward dimensions. We add a 9-layer randomly initialized transformer decoder with similar embedding/feedforward dimensions during finetuning.

During training, we first select one noise category and sample a noise audio clip from its training partition. We randomly mix the sampled noise at 0dB SNR with a probability of 0.25, following~\cite{Xu2020DiscriminativeMS}. At test time, we evaluate the model separately for each noise type. The testing noise clips are added at five SNR levels: . The performance on the original clean test set is also reported for comparison. 
By default, noise clips are added during both pre-training and finetuning. We follow the training pipeline in~\cite{avhubert}, where the model is trained for five iterations in total. To save the computation time, we always use the smaller BASE model architecutre~\cite{avhubert} in all iterations except the last one, where we use a LARGE model. Video samples are batched together not to exceed 1000 image frames per GPU. The model is pre-trained with 600K steps using 64 V100-GPUs and finetuned for 30K/100K steps respectively in 30h/433h setting.


\begin{table*}[htb]
\centering
\caption{WER (\%) of our models and prior work on the LRS3 dataset. ``Mode'' denotes whether a model uses audio-visual input (AV) or only audio as input (A). ``Hr'' denotes the amount of labeled audio-visual speech data used in each system.
}
\label{tab:main_avsr}
\resizebox{\linewidth}{!}{
\begin{tabular}{
    lll | 
    cccccc | 
    cccccc | 
    cccccc | 
    c}
\toprule
\multirow{3}{*}{Model} & 
\multirow{3}{*}{Mode} & 
\multirow{3}{*}{Hr} & 
\\
& & &
    \multicolumn{6}{c|}{Babble, SNR=} & 
    \multicolumn{6}{c|}{Speech, SNR=} & 
    \multicolumn{6}{c|}{Music+Natural, SNR=} & 
    Clean \\
& & &
    -10 & -5 & 0 & 5 & 10 & avg &
    -10 & -5 & 0 & 5 & 10 & avg &
    -10 & -5 & 0 & 5 & 10 & avg &
     \\
\midrule
Makino et al.~\cite{Makino2019rnnt} & AV & 31K &
    - & - & - & - & - & - &
    - & - & - & - & - & - & 
    - & - & - & - & - & - &
    4.5 \\
Ma et al.~\cite{ma2021conformer} & AV & 595 &
    - & - & - & - & - & - &
    - & - & - & - & - & - & 
    - & - & - & - & - & - &
    2.3 \\
Afouras et al.~\cite{afouras2018deepavsr} & AV & 1.4K &
    - & - & 42.5 & - & - & - &
    - & - & - & - & - & - & 
    - & - & - & - & - & - &
    7.2 \\
Xu et al.~\cite{Xu2020DiscriminativeMS} & AV & 433 &
    38.6 & 31.1 & 25.5 & 24.3 & 20.7 & 28.0 &
    - & - & - & - & - & - &
    - & - & - & - & - & - &
    6.8 \\
\midrule
AV-HuBERT & AV & 30 &
    35.1 & 18.4 & 8.3 & 4.9 & 3.9 & 14.1 & 
    11.5 & 6.8 & 5.0 & 4.2 & 3.9 & 6.3 & 
    12.0 & 7.0 & 4.8 & 4.1 & 3.7 & 6.3 & 
    3.3 \\
AV-HuBERT & AV & 433 &
    34.9 & 16.6 & 5.8 & 2.6 & 2.0 & 12.4 & 
    11.4 & 4.6 & 2.9 & 2.2 & 1.8 & 4.6 & 
    9.7 & 4.77 & 2.5 & 1.9 & 1.8 & 4.1 & 
    1.4 \\
\midrule
AV-HuBERT & A & 30 &
    99.6 & 69.3 & 21.9 & 9.0 & 5.6 & 41.1 & 
    77.3 & 51.2 & 32.0 & 19. & 10.8 & 38.2 & 
    47.9 & 21.5 & 9.2 & 5.9 & 4.8 & 17.8 & 
    3.8 \\
AV-HuBERT & A & 433 &
    97.5 & 62.3 & 15.7 & 5.1 & 2.6 & 36.6 & 
    81.7 & 56.2 & 37.3 & 19.0 & 8.3 & 40.5 & 
    38.7 & 15.1 & 5.7 & 3.1 & 2.3 & 13.0 &
    1.6 \\
\bottomrule
\end{tabular}
}
\end{table*}
 \subsection{Main Results}
Table~\ref{tab:main_avsr} compares the performance of our proposed noise-augmented AV-HuBERT approach under different settings versus existing supervised AVSR models. In the clean audio setting, our best model outperforms the best model from Ma et al.~\cite{ma2021conformer} by ~39.0\% (2.3\%1.4\%) while using fewer labeled data. To enable direct comparison with previous research work which focus primarily on \texttt{babble} noise, we follow \cite{Xu2020DiscriminativeMS} to synthesize \texttt{babble} noise by randomly mixing 30 audio clips from LRS3\footnote{\cite{afouras2018deepavsr} uses audios from LRS2~\cite{afouras2018deepavsr}, which has restricted access and we are unable to obtain it.} for evaluation.

As shown in the "\texttt{Babble}" column, with only 30 hours of labeled data, our model outperforms \cite{afouras2018deepavsr} by 80.4\% (42.5\%8.3\%) and \cite{Xu2020DiscriminativeMS} by 67.4\% (25.5\%8.3\%) at 0dB SNR. Compared to the former SOTA~\cite{Xu2020DiscriminativeMS}, we achieve 49.6\% lower WER (28.0\%  14.1\%) on average across different SNR ratios with 10 times fewer labels. When using all 433 hours labeled data for finetuning, the relative improvement is further increased to 55.7\% (28.0\%  12.4\%). Note that the \texttt{babble} noise we use for training our model is synthesized from MUSAN, which has potential domain mismatch from the \texttt{babble} noise synthesized from LRS3 used at test time; however, our approach significantly improves over prior work and estableshes the new SOTA.

When the noise type is extended beyond \texttt{babble} noise, our proposed audio-visual model consistently improves over its audio-only ASR counterpart with is more than  relative WER reduction. The reduction varies depending on noise type and SNR; hence, we analyze how the model performs in different noise conditions and how each component of our approach contributes to such improvement.


\subsection{Analysis}
To examine the impact of pre-training (no pre-training, pre-training with clean audio, or noise-augmented pre-training) and input modality (audio or audio-visual), we experimented with the six setups covering the cross product of these conditions. For setups with audio-only input during finetuning, we follow~\cite{avhubert} by replacing the visual features in the pre-trained AV-HuBERT model with a dummy zero vector at each frame. Performances of these six setups are shown in table~\ref{tab:avg-comparison}. Figure~\ref{fig:overall-comparison} shows a more detailed performance breakdown over SNR ratios and noise types.


\subsubsection{Effect of the visual modality}

\begin{figure}[htb]
\caption{\label{fig:overall-comparison}Comparison of models using different inputs and pre-training methods.}
\begin{tabular}{c}
  \includegraphics[width=.95\linewidth]{figure/433-large-noise-snr.png}\\ \includegraphics[width=.95\linewidth]{figure/433-large-noise-type.png}
\end{tabular}
\end{figure}

\begin{table}[htb]
    \centering
    \caption{Comparison among models with different pre-training configurations and input modalities. C: clean audio, N: noisy audio. The N-WER is averaged over 4 noise types and 5 SNRs.}\label{tab:avg-comparison}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{ccc|cc|cc}
        \toprule
        Model   & PT    & FT    & \multicolumn{2}{c|}{Audio-only} & \multicolumn{2}{c}{Audio-visual} \\
        Size    & Type  & Data & C-WER & N-WER & C-WER & N-WER \\
        \midrule
        (a). LARGE & None  & 30h & 20.6 & 59.2 & 20.8 & 42.9 \\
        (b). LARGE & Clean & 30h & 4.3  & 39.8 & 3.3  & 9.3 \\
        (c). LARGE & Noisy & 30h & 3.8  & 28.7 & 3.3  & 7.8 \\
        \midrule
        (d). LARGE & None  & 433h & 4.7 & 39.2 & 3.5 & 14.8 \\
        (e). LARGE & Clean & 433h & 1.5 & 29.1 & 1.4 & 6.9  \\
        (f). LARGE & Noisy & 433h & 1.6 & 25.8 & 1.4 & 5.8  \\
        \bottomrule
    \end{tabular}
    }
\end{table} We first examine the performance of AVSR models against audio-only ASR models under low-resource and mid-resource conditions by comparing the blue and yellow bars of the same shading pattern in each group in figure~\ref{fig:overall-comparison} and audio-only vs. audio-visual columns in Table~\ref{tab:avg-comparison}. AVSR consistently outperforms audio-only ASR under all settings regardless of the SNR and the type of noise, except for the setup where the model is trained on only 30 hours of labeled data from scratch.

As shown in Figure~\ref{fig:overall-comparison}, the benefit of incorporating the visual stream is more apparent in challenging scenarios, where the WER degradation relative to the clean condition is large. Specifically, these scenarios include low SNR conditions where the volume of the noise is higher and noisy environments with \texttt{speech} or \texttt{babble} noise where the interfering signal is similar to the target speech. Averaged over different pre-training configurations, the AVSR model achieves 53.0\% (42.6\%  20.0\%) and 70.8\% (31.3\%  9.2\%) relative WER reduction over audio-only ASR under noisy settings using 30 hours and 433 hours of labeled data, respectively. 

It is worth noting that our AVSR model enjoys its largest gain over the audio-only model under \texttt{speech} noise settings, where a secondary speech utterance is randomly mixed into the primary one. When using overlapping speech noise under the mid-resource setting, the WER went from 48.1\% to 7.7\% by AVSR on average across different pre-training configurations, while the WER is reduced from 25.8\% to 9.6\% in the other three noise categories. These results suggest that the paired visual stream provides an effective audio source separation, where the audio-only recognizer can not distinguish two audio tracks.


\subsubsection{Effect of pre-training}
\label{sec:pre-train-vs-scratch}
The performance of fine-tuning an AV-HuBERT model is compared against directly optimizing a model from scratch on labeled audio-visual speech in (AV, PT=Clean) vs. (AV, PT=None) bars of Figure~\ref{fig:overall-comparison} and (Clean vs. None) rows (i.e., (b) vs. (a) and (e) vs. (d)) in Table~\ref{tab:avg-comparison}. Note that the AV-HuBERT model pre-trained on clean audio (PT=clean) is identical to the one used in \cite{avhubert}.

On average, AV-HuBERT pre-training brings substantial relative improvements of 78.3\% (42.9\%9.3\%) and 53.4\% (14.8\%6.9\%) when using 30h and 433h of labeled data, respectively. The model achieves bigger gains in the low-resource setting, which confirms the impact of the self-supervised audio-visual representations learned by the AV-HuBERT model. 


\subsubsection{Effect of noise-augmented pre-training}
The impact of incorporating noise in pre-training is presented in (AV, PT=Noisy) vs. (AV, PT=Clean) bars from Figure~\ref{fig:overall-comparison} and (Noisy vs. Clean) rows in Table~\ref{tab:avg-comparison} (i.e., (b) vs. (c) and (e) vs. (f)).

Overall the noise-augmented pre-training improves the result in noisy settings. The WER is reduced by 16.1\% (9.3\%7.8\%) / 15.9\% (6.9\%5.8\%) on average in low-resource (30h) and mid-resource (433h) settings compared to pre-training on clean data. Compared to an audio-visual model trained from scratch, the noise-augmented pre-training approach reduces recognition error by 81.8\% (42.9\%7.8\%) / 60.8\% (14.8\%5.8\%) in low/mid-resource settings.

\begin{table}[htb]
    \centering
    \caption{Comparison among BASE models with different pre-training configurations and input modalities. The model is fine-tuned with 30h labeled data. C: clean audio, N: noisy audio. The N-WER is averaged over 4 noise types and 5 SNR ratios.}\label{tab:avg-base-comparison}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{ccc|cc|cc}
        \toprule
        Model   & PT    & FT    & \multicolumn{2}{c|}{Audio-only} & \multicolumn{2}{c}{Audio-visual} \\
        Size    & Type  & Type  & C-WER & N-WER & C-WER & N-WER \\
        \midrule
        (a). BASE & None  & Clean & 24.6 & 79.8 & 22.0 & 70.9 \\
        (b). BASE & Clean & Clean & 4.6  & 46.3 & 4.0  & 28.2 \\
        (c). BASE & Noisy & Clean & 4.4  & 33.8 & 4.1  & 12.5 \\
        \midrule
        (d). BASE & None  & Noisy & 16.9 & 55.4 & 17.2 & 39.5 \\
        (e). BASE & Clean & Noisy & 4.8  & 37.3 & 4.2  & 13.1 \\
        (f). BASE & Noisy & Noisy & 4.4  & 33.3 & 4.1  & 10.3 \\
        \bottomrule
    \end{tabular}
    }
\end{table}


 Concerning SNR, the WER is reduced the most in low SNR settings, i.e., high noise, as is shown in Figure~\ref{fig:overall-comparison}. This observation matches our hypothesis made in section~\ref{sec:pre-train-vs-scratch} about the domain discrepancy between pre-training and finetuning. Introducing noise during the pre-training stage bridges the domain gap and makes the model more resilient to noise at test time. One key takeaway from this work is that noise augmentation is needed during pre-training and finetuning phases to achieve the best AVSR performance in adverse acoustic conditions. Compared to training from scratch, the gain of noise-augmented pre-training peaks around 0dB SNR ratio, which matches the SNR used in training. 

Compared to ``\texttt{babble}'', ``\texttt{music}'' and ``\texttt{natural}'' noise types, noise-augmented AV-HuBERT pre-training is more effective in overlapping ``\texttt{speech}'' noise as shown in Figure~\ref{fig:overall-comparison}.  Consistent with previous findings when comparing audio-visual and audio-only recognizers trained from scratch, the visual modality provides a strong clue to ``choose'' the target speech track. AV-HuBERT effectively learns visual representation from paired audio-visual data, leading to significant gains in speech separation.

The gains from noise-augmented pre-training generalize across model architectures, as shown in table~\ref{tab:avg-base-comparison}. In addition, regardless of the finetuning strategy, our proposed pre-training approach is helpful, as is shown by row (b) vs. row (c) and row (e) vs. row (f) in N-WER in Table~\ref{tab:avg-base-comparison}.