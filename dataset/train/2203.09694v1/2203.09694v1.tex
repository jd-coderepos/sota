\vspace{-0.2cm}
\section{Experiment}
\vspace{-0.2cm}
We conduct experiments on different benchmarks, including Something-Something V1\&V2~\cite{goyal2017something,mahdisoltani2018effectiveness} and Kinetics-400 \cite{kay2017kinetics} for video recognition. The four video datasets cover a broad range of activities. Specifically, the \textbf{Something-Something V1\&V2} datasets show 174 fine-grained humans performing pre-defined activities and are more focused on modeling the temporal relationships. The \textbf{Kinetics-400} dataset covers 400 human action classes with less motion variations. Due to space limitation, we include the results on EGTEA Gaze+~\cite{li2018eye}, which is a dataset offering first-person videos, Diving48~\cite{li2018resound} with unambiguous dive sequence, and Basketball-8\&Soccer-10 \cite{hao2020compact} with group activities in the supplementary document. 



\subsection{Experimental Setup}
\vspace{-0.1cm}
We insert the GC module into four different 2D and 3D ResNets, including TSN, TSM, GST and TDN. Most experiments are based on the backbone of ResNet-50 pretrained on ImageNet~\cite{russakovsky2015imagenet}. Notably, we additionally add a ``BatchNorm'' layer after each convolutional/FC layer in the ECals for TSN, TSM and TDN. We implement GC-Nets in Pytorch and run them on servers with 42080Ti or 4/83090.

{\bf Training \& Inference}. The \textbf{training} protocol mainly follows the work \cite{wang2016temporal}. Specifically, we use uniform sampling for all datasets. The spatial short side of input frames is resized to 256 maintaining the aspect ratio and then cropped to 224224. Data augmentation also follows \cite{wang2016temporal}. Training configurations for GC-TSN/TSM/GST are set as follows: a batch-size of 8/10 per GPU, an initial learning rate of 0.01 for 50 epochs and decayed at epoch 20 and 40, the SGD optimizer. GC-TDN follows the training protocol of TDN~\cite{wang2021tdn}. The dropout ratio is set to 0.5.  During the \textbf{inference}, we uniformly sample 8 frames per video and use the 224224 center crop for performance report in the ablation study. In the final performance comparison, we sample multiple clips per video and take no more than three crops per clip. Specifically, the test protocols are: 2 clips  3 crops (224) for Something-Something V1\&V2, and 1 clip  1 center crop (224) for others. We will also specify the sampled frames in the tables.


\subsection{Ablation Study}
We present ablation study to investigate the effect of hyperparameters, including the channel partition ratio , channel position, calibrator variants and backbones, on Something-Something V1 dataset.

\begin{table}[tbp]
		\centering
		\scriptsize
\begin{tabular}{l|l|c|cc|c}
\hline
			Backbone &Calibrator &(, Channel) &Params &FLOPs &Top-1 (\%)\\
            \midrule[1pt]
            \multirow{18}*{TSN} &--- &--- &23.9M &32.9G &19.7 \\
                    \cline{2-6}
                    &SE3D &--- &26.4M &32.9G &27.8 (+8.1)  \\
                    &GE3D-G &--- &23.9M &32.9G &22.3 (+2.6)  \\
                    &GE3D-C &--- &25.2M &33.3G &44.2 (+24.5)  \\
                    &S3D-G &--- &25.1M &32.9G &28.0 (+8.3)  \\
                    &NLN &--- &31.2M &49.4G &30.3 (+10.6)  \\
                    \cline{2-6}
                    &\multirow{2}*{ECal-G} &(, ) &23.9M &32.9G &26.3 (+6.6) \\
                    & & (1, ) &23.9M &32.9G &27.3 (+7.6)  \\
                    \cline{2-6}
                    &\multirow{2}*{ECal-T} &(, ) &23.9M &32.9G &35.9 (+16.2)  \\
                    & &(1, ) &24.1M &32.9G &36.4 (+16.7) \\
                    \cline{2-6}
                    &\multirow{2}*{ECal-S} &(, ) &24.0M &32.9G &34.0 (+14.3)  \\
                    & &(1, ) &24.6M &33.0G &34.1 (+14.4)  \\
                    \cline{2-6}
                    &\multirow{2}*{ECal-L} &(, ) &23.9M &33.0G &44.8 (+25.1) \\
                    & &(1, ) &24.1M &33.2G &44.9 (+25.2) \\
                    \cline{2-6}
                    &\multirow{3}*{GC} &(, ) &24.2M &33.0G &47.1 (+27.4) \\
                    & &(1, ) &25.1M &33.3G &47.9 (+28.2) \\
                    & &(1, ), loop &25.1M &33.3G &\textbf{48.0 (+28.3)} \\
            \hline
			\multirow{8}*{TSM} &--- &--- &23.9M &32.9G &45.6 \\
			        \cline{2-6}
			        &SE3D &--- &26.4M &32.9G &46.7 (+1.1)  \\
                    &GE3D-G &--- &23.9M &32.9G &45.7 (+0.1)  \\
                    &GE3D-C &--- &25.2M &33.3G &47.0 (+1.4)  \\
                    &S3D-G &--- &25.1M &32.9G &46.8 (+1.2)  \\
                    &NLN &--- &31.2M &49.4G &47.2 (+1.6)  \\
                    \cline{2-6}
                    &\multirow{3}*{GC} &(, ) &24.2M &33.0G &48.7 (+3.1) \\
                    & &(1, ) &25.1M &33.3G &\textbf{48.9 (+3.3)} \\
                    & &(1, ), loop &25.1M &33.3G &\textbf{48.9 (+3.3)} \\
            \hline
            \multirow{4}*{GST} &--- &--- &21.0M &29.2G &44.4 \\
                    \cline{2-6}
                    &\multirow{3}*{GC} &(, ) &21.4M &29.3G &45.5 (+1.1) \\
                    & &(1, ) &22.3M &29.6G &45.6 (+1.2) \\
                    & &(1, ), loop &22.3M &29.6G &\textbf{46.7 (+2.3)} \\
            \hline
            \multirow{3}*{TDN} &--- &--- &26.1M &36.0G &52.3 \\
                    \cline{2-6}
&\multirow{2}*{GC} &(1, ) &27.4M &36.7G &\textbf{53.7 (+1.4)} \\
                    & &(1, ), loop &27.4M &36.7G &53.6 (+1.3) \\
            \hline
		\end{tabular}
		\caption{Performance changes using different backbones, calibrators, partition ratio  and channel position on Something-Something V1 dataset. ``Channel'' denotes the number of channels in each calibrated feature group. SE3D is the 3D variant of SE-Net \cite{hu2018squeeze} by replacing the 2D spatial average pooling with the 3D spatio-temporal average pooling. GE3D-G and GE3D-C are two 3D variants of GE-Net \cite{hu2018gather}, where GE3D-G adopts global average pooling and GE3D-C employs 3D depthwise convolution. Their architectures can be found in Appendix. NLN denotes the nonlocal~\cite{wang2018non} module.}
		\label{tab:res_ab}
		\vspace{-0.5cm}
		\end{table}

{\bf  and calibrators}. We first compare different ECals on TSN with . The four types of ECals are designed to calibrate video feature with different axial context concerns. And the channel partition ratio  is introduced to control the number of channels to be calibrated by ECals. As shown in Table \ref{tab:res_ab}, we observe that ECal variants, regardless of their types, consistently improve the recognition performance of the backbone TSN, indicating their effectiveness. Although varying the value of  from  to 1 will result in slight increase of model size and computational cost, the performance boost is noticeable (e.g., 26.3\%27.3\% for ECal-G and 35.9\%36.4\% for ECal-T). 

{\bf Channel position and backbones}. Secondly, we test both the standard and loop GC versions on the four backbones. Here, we also set . Table \ref{tab:res_ab} shows their results. Compared to the single calibrator, the GC module, which combines the four ECals in parallel, achieves much better performance on TSN. The GC-TSM, GC-GST and GC-TDN also gain significant performance improvement (45.6\%48.9\% for TSM, 44.4\%46.7\% for GST, 52.3\%53.7\% for TDN) to their original backbones. Consistently, the models with larger  outperform their counterparts with . Based on the above results, we fix  for the GC-Nets in this work. For the channel position, we observe different performance tendencies on the four backbones, i.e., the result of loop version is clearly better than the standard version on GST, and their performances are about the same on TSN, TSM and TDN. As analysed in Section \ref{channelposition}, this is because that feature channels in TSN, TSM and TDN are entangled together during the feature learning while the group convolution method GST separately models the spatial and temporal features. 

{\bf Comparison with other calibrators}. Thirdly, we integrate the 3D variants of SE-Net~\cite{hu2018squeeze} and GE-Net~\cite{hu2018gather}, i.e., SE3D and GE3D-G/C, S3D-G and NLN, into the TSN and TSM backbones. Their hyperparameters are set as the same to their original papers. The NLN-Nets follows the implementation of \cite{lin2019tsm}. From Table \ref{tab:res_ab}, we can find that our GC module far outstrips SE3D, GE3D-G and S3D-G which only consider the global context and the pairwise self-attention NLN when using TSN as backbone. Since GE3D-C uses three 3D depthwise convolution layers to model local spatio-temporal context, relatively good performance (44.2\% Top-1 accuracy) is attained on TSN but still lower than our GC (47.9\%). On TSM, our GC can outperform the five other calibrators by the margins of 1.7\%-3.2\%. Moreover, compared to the self-attention NLN module that results in 31\% extra parameters and 50\% extra FLOPs to the backbones, our GC only introduces as low as 5\% extra parameters and 1.2\% extra FLOPs.

\begin{figure}[]
\centering
\includegraphics[width=0.45\textwidth]{figures/bars.pdf}
\vspace{-0.4cm}
\caption{Per-category accuracy comparison for TSN, TSN-ECals and GC-TSN over several selected activity categories on Something-Something V1 dataset (validation set). In each subfigure, we show 5 action categories that are improved most by TSN-ECal and another 4 activity categories that are being degraded by TSN-ECal but improved by GC-TSN. See the supplementary material for the label details of these selected activities.}
\label{figure:bars}
\vspace{-0.6cm}
\end{figure}

\subsection{Example Demonstration}
\vspace{-0.1cm}
We show the per-category results of TSN-ECal variants and GC-TSN to understand the impact of axial contexts on different kinds of video activities in Figure \ref{figure:bars}. Specifically, ECal-G can boost the recognition of activities that need global contexts, e.g., ``\textit{label-106: Putting something in front of something}'' in Figure \ref{figure:bars}(a). ECal-S focuses on enhancing the feature spatial-wisely with the context aggregated along temporal dimension and thus can improve the performance for activities that require more spatial than temporal information, for example ``\textit{label-26: Lifting a surface with something on it but not enough for it to slide down}'' in \ref{figure:bars}(b). TSN-ECal-T and TSN-ECal-L, which naturally calibrate the feature globally and locally along temporal dimension respectively, can significantly improve the recognition performance for activities that require temporal reasoning, as shown in the first 5 categories in figures \ref{figure:bars}(c) and (d). However, those failure cases in the four subfigures (last 4 terms) provide evidence that one kind of axial contexts is not suitable for all activity categories. For example, TSN-ECal-G fails to model the activities that involve rich spatial-temporal interactions between objects, e.g., ``\textit{label-61: Pouring something into something until it overflows}'', and TSN-ECal-S under-performs on ``\textit{label-73: Pretending to put something into something}'' and ``\textit{label-81: Pretending to squeeze something}'' which require strong temporal reasoning. Encouragingly, the GC module that aggregates all the four ECals indeed alleviates these shortcoming of individual ECal, leading to  performance improvement for a variety of activities. 


\subsection{Comparison with the State-of-the-Arts}
We compare GC-Nets with state-of-the-art networks in this section. The result comparison follows the same protocol of using RGB frames as input and adopting ResNet50 unless otherwise specified.


\begin{table}[t]
		\centering
		\scriptsize
\begin{tabular}{l|c|c|c|cc}
			\hline
Method &Params &\#Frame &FLOPsClips &Top-1 &Top-5 \\
            \midrule[1pt]
CorrNet \cite{wang2020video} &---  &32 &115.0G10  &49.3 &---  \\
            TIN \cite{shao2020temporal} &24.6M2  &8+16 &101G1 &49.6 &78.3  \\
V4D \cite{zhang2020v4d} &---  &84 &167.6G30 &50.4 &---  \\
            SmallBig \cite{li2020smallbignet} &---  &8+16 &157G1 &50.4 &80.5  \\
            TANet~\cite{liu2020tam} &25.1M2  &8+16 &99.0G1 &50.6 &79.3  \\
STM \cite{jiang2019stm} &24.0M  &16 &33.3G30  &50.7 &80.4  \\
TEA \cite{li2020tea}   &---  &16 &70.0G30 &52.3 &81.9  \\
            TEINet \cite{liu2020teinet} &30.4M2  &8+16 &99.0G1 &52.5 &---  \\
RNL-TSM \cite{huang2021region} &35.5M2  &8+16 &123.5G2 &52.7 &---  \\
\Xcline{1-6}{0.7pt}
GST* \cite{luo2019grouped} &21.0M &16 &58.4G2  &46.2 &75.0 \\
            GST* \cite{luo2019grouped} &21.0M2 &8+16 &87.6G2  &48.6 &78.3 \\
TSM \cite{lin2019tsm} &23.9M  &16 &65.8G2  &48.4 &78.1  \\
            TSM \cite{lin2019tsm} &23.9M2  &8+16 &98.7G2   &50.3 &79.3 \\
TDN \cite{wang2021tdn} &26.1M  &16 &72.0G1 &53.9 &82.1  \\
            TDN \cite{wang2021tdn} &26.1M2  &8+16 &108.0G1 &55.1 &82.9  \\
            \Xcline{1-6}{0.7pt}
            \textbf{GC-GST} &22.3M  &8 &29.6G2 &48.8 &78.5  \\
\textbf{GC-GST} &22.3M  &16 &59.1G2  &50.4 &79.4  \\
\textbf{GC-GST} &22.3M2  &8+16 &88.7G2  &52.5 &81.3  \\
            \hline
\textbf{GC-TSN} &25.1M  &8 &33.3G2 &49.7 &78.2 \\


\textbf{GC-TSN} &25.1M  &16 &66.5G2  &51.3 &80.0  \\
\textbf{GC-TSN} &25.1M2  &8+16 &99.8G2  &53.7 &81.8 \\
            \hline


\textbf{GC-TSM} &25.1M  &8 &33.3G2 &51.1 &79.4  \\




\textbf{GC-TSM} &25.1M  &16 &66.5G2  &53.1 &81.2  \\


            \textbf{GC-TSM} &25.1M2  &8+16 &99.8G2  &55.0 &82.6  \\
            \textbf{GC-TSM} &25.1M2  &8+16 &99.8G6  &55.3 &82.7  \\
            \hline
            \textbf{GC-TDN} &27.4M  &8 &36.7G1 &53.7 &82.2  \\
            \textbf{GC-TDN} &27.4M  &16 &73.4G1 &55.0 &82.3  \\
            \textbf{GC-TDN} &27.4M2  &8+16 &110.1G1 &\textbf{56.4} &\textbf{84.0} \\
\hline
		\end{tabular}
		\vspace{-0.3cm}
		\caption{Comparison of performance on Something-Something V1 dataset. ``*'' indicates that the result is obtained by ourselves.}
		\label{tab:res_somev1}
		\vspace{-0.5cm}
\end{table}

\begin{table}[!t]
		\centering
		\scriptsize
\begin{tabular}{l|c|c|c|cc}
			\hline
Method &Params &\#Frame &FLOPsClips &Top-1 &Top-5 \\
            \midrule[1pt]
TIN \cite{shao2020temporal} &24.6M  &16 &67.0G1 &60.1 &86.4 \\
            RubiksNet \cite{fan2020rubiksnet} &8.5M  &8 &15.8G2  &61.7 &87.3  \\
            TSM+TPN \cite{yang2020temporal} &---  &8 &33.0G1 &62.0 &---  \\
            SlowFast \cite{feichtenhofer2019slowfast} &32.9M  &4+32 &65.7G6  &61.9 &87.0 \\
            SlowFast(R101) \cite{feichtenhofer2019slowfast} &53.3M  &8+32 &106G6  &63.1 &87.6 \\
            SmallBig \cite{li2020smallbignet} &---  &16 &114.0G6 &63.8 &88.9  \\
            STM \cite{jiang2019stm} &24.0M  &16 &33.3G30 &64.2 &89.8  \\
            TEA \cite{li2020tea}   &---  &16 &70.0G30 &65.1 &89.9  \\
TEINet \cite{liu2020teinet} &30.4M2  &8+16  &99.0G1 &65.5 &89.8  \\
            TANet~\cite{liu2020tam} &25.1M2  &8+16 &99.0G6 &66.0 &90.1  \\
\Xcline{1-6}{0.7pt}
            TimeSformer-HR~\cite{bertasius2021space} &121.4M  &16 &1703G3  &62.5 &--- \\
            ViViT-L~\cite{arnab2021vivit} &352.1M &32 &903G4  &65.4 &89.8 \\
            MViT-B~\cite{fan2021multiscale} &36.6M  &64 &455G3  &67.7 &90.9 \\
            Video-Swin-B~\cite{liu2021video} &88.8M  &16 &321G3  &\textbf{69.6} &\textbf{92.7} \\
\Xcline{1-6}{0.7pt}
            TSN \cite{zhou2018temporal} from \cite{lin2019tsm}  &23.9M  &8 &32.9G1 &30.0 &60.5 \\
            GST* \cite{luo2019grouped} &21.0M &8 &29.2G2  &59.8 &86.3 \\
            GST* \cite{luo2019grouped} &21.0M &16 &58.4G2  &61.7 &87.2 \\
            GST* \cite{luo2019grouped} &21.0M2 &8+16 &87.6G2  &63.1 &88.3 \\
            TSM \cite{lin2019tsm} &23.9M  &8 &32.9G2   &61.2 &87.1  \\
TSM \cite{lin2019tsm} &23.9M  &16 &65.8G2  &63.1 &88.2  \\
            TSM \cite{lin2019tsm} &23.9M2  &8+16 &98.7G2  &64.3 &89.0 \\
            TDN \cite{wang2021tdn} &26.1M  &8 &36.0G1 &64.0 &88.8  \\
            TDN \cite{wang2021tdn} &26.1M  &16 &72.0G1 &65.3 &89.5  \\
            TDN \cite{wang2021tdn} &26.1M2  &8+16 &108G1 &67.0 &90.3  \\
            \Xcline{1-6}{0.7pt}
\textbf{GC-GST} &22.3M  &8 &29.6G2 &61.9 &87.8  \\
\textbf{GC-GST} &22.3M  &16 &59.1G2  &63.3 &88.5  \\
\textbf{GC-GST} &22.3M2  &8+16  &88.7G2  &65.0 &89.5  \\
            \hline
\textbf{GC-TSN} &25.1M  &8  &33.3G2 &62.4 &87.9 \\


\textbf{GC-TSN} &25.1M  &16  &66.5G2  &64.8 &89.4  \\
\textbf{GC-TSN} &25.1M  &8+16  &99.8G2  &66.3 &90.3 \\
            \hline


\textbf{GC-TSM} &25.1M  &8  &33.3G2 &63.0 &88.4  \\


\textbf{GC-TSM} &25.1M  &16  &66.5G2  &64.9 &89.7  \\


            \textbf{GC-TSM} &25.1M2  &8+16 &99.8G2  &66.7 &90.6 \\
            \textbf{GC-TSM} &25.1M2  &8+16 &99.8G6  &67.5 &90.9 \\
            
\hline
            \textbf{GC-TDN} &27.4M  &8  &36.7G1 &64.9 &89.7  \\
            \textbf{GC-TDN} &27.4M  &16  &73.4G1 &65.9 &90.0  \\
            \textbf{GC-TDN} &27.4M2  &8+16  &110.1G1 &\textbf{67.8} &\textbf{91.2}  \\
            \hline
		\end{tabular}
\caption{Comparison of performance on Something-Something V2 dataset. ``*'' indicates that the result is obtained by ourselves.}
		\label{tab:res_somev2}
		\vspace{-0.5cm}
\end{table}


{\bf Something-Something V1 \& V2}. A comprehensive comparison between our GC-Nets and SOTAs on Something-Something V1\&V2  datasets are presented. Tables \ref{tab:res_somev1} and \ref{tab:res_somev2} list the comparison in terms of Top-1/5 accuracy, FLOPs and model complexity. GC-TDN achieves the highest Top-1 accuracies of 56.4\% and 67.8\% with (8+16) frames  1 clip on Something-Something V1\&V2, respectively, which outperform all the CNN-based SOTAs by large margins (1.3\%-36.7\% for V1 and 0.8\%-37.8\% for V2). Moreover, all GC-Nets, including GC-GST, GC-TSN, GC-TSM and GC-TDN, consistently outperform their backbone networks with significant performance gains, demonstrating the capacity of GC module in recognizing diverse activities and the strong versatility against various deep video networks. For example, GC-TSN boosts the original TSN model with an absolute improvements of 28.2\% (19.7\%47.9\%) on V1 and 32.4\% (30.0\%62.4\%) on V2 with the same 8-frame input. Equipping TSN, which is an image-based CNN, with GC empowers the modeling of temporal relationship between objects on V1\&V2. The GC module can also improve the more advanced TDN by 1.3\% on V1 and 0.8\% on V2 with the same 8+16 frames. This demonstrates that the axial contexts modeled by GC can work cooperatively with the temporal difference contexts used by TDN. Compared to the more sophisticated Transformer-based models like MViT and Video-Swin, the GC-TSM and GC-TDN obtain lower Top-1 accuracies. The performance is compensated by lower computational cost and model complexity. GC-TDN requires 110.1G FLOPs, which is about 11.4 times cheaper than MViT-B (1,365G FLOPs) and 7.7 times lower than Video-Swin-B (963G FLOPs).


{\bf Kinetcis-400}. We report the results of GC-TSN with 8 frames and GC-TSM/TD with 8 and 16 frames respectively, in Table \ref{tab:res_kinetic}. Firstly, the GC-TSN/TSM/TDN improve the Top1 accuracy upon TSN/TSM/TDN by 4.6\%/2.0\%/1.2\% under the same input, respectively. They are much significant in terms of the data scale of Kinetics-400 dataset. Secondly, GC-TDN, with 1630 clips as input, achieves the 79.6\% Top1 accuracy, which is the highest one among the competing methods. This result is much better than other models equipped with feature contextualization techniques, such as  Nonlocal-I3D, S3D-G and TEA, which further demonstrates the superior performance of the proposed GC module. 


\begin{table}[htbp]
		\centering
		\scriptsize
\begin{tabular}{l|c|c|c|cc}
			\hline
			Model &Params &\#Frame &FLOPsClips &Top1 &Top5 \\
            \midrule[1pt]
I3D (InceptionV1) \cite{carreira2017quo} & --- &64 &---  &72.1 &90.3 \\
            Nonlocal-I3D \cite{wang2018non} &35.3M  &32 &282G10  &74.9 &91.6 \\
            S3D-G (InceptionV1) \cite{xie2018rethinking} &--- &64 &71.4G30 &74.7 &\textbf{93.4} \\
TEA \cite{li2020tea} &--- & 16 &70G30 &76.1 &92.5 \\
            TEINet \cite{liu2020teinet} &30.8M & 16 &66G30 &76.2 &92.5 \\
            TANet \cite{liu2020tam} &25.6M &16 &86G12 &76.9 &92.9 \\
SmallBig \cite{li2020smallbignet} &--- &8 &57G30 &76.3  &92.5 \\
SlowFast(88) \cite{feichtenhofer2019slowfast} &32.9M &8+32 &65.7G30 &77.0 &92.6 \\
            X3D-L \cite{feichtenhofer2020x3d} &6.1M & 16 &24.8G30 &77.5 &92.9 \\
\hline
            TSN \cite{zhou2018temporal} &24.3M &8 &32.9G10clip  &70.6 &89.2 \\
TSM \cite{lin2019tsm} &24.3M  &16 &66.0G10 &74.7 &91.4 \\
            TDN \cite{wang2021tdn} &26.6M &8+16 &108.0G30 &78.4  &93.6 \\
\hline
\textbf{GC-TSN} &25.6M &8 &33.3G10 &75.2  &92.1 \\
            \textbf{GC-TSM} &25.6M &8 &33.3G10 &75.4  &91.9 \\
            \textbf{GC-TSM} &25.6M &16 &66.6G10 &76.7  &92.9 \\
            \textbf{GC-TSM} &25.6M &16 &66.6G30 &77.1  &92.9 \\
            \textbf{GC-TDN} &27.4M &8 &36.7G30 &77.3  &93.2 \\
            \textbf{GC-TDN} &27.4M &16 &73.4G30 &78.8  &93.8 \\
            \textbf{GC-TDN} &27.4M &8+16 &110.1G30 &\textbf{79.6}  &94.1 \\
\hline
\end{tabular}
		\caption{Comparison of performance on Kinetics-400 dataset.}
		\vspace{-0.3cm}
		\label{tab:res_kinetic}
\end{table}



