\documentclass[british,10pt,twocolumn,letterpaper]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{luainputenc}
\usepackage{array}
\usepackage{rotating}
\usepackage{bbding}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}

\makeatletter

\providecommand{\tabularnewline}{\\}
\newcommand{\lyxdot}{.}


\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amstext}

\usepackage{placeins}
\usepackage{color}
\usepackage{colortbl}
\usepackage{rotating}
\definecolor{lightgray}{gray}{0.8}
\definecolor{verylightgray}{gray}{0.9}



\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\cvprfinalcopy
\def\cvprPaperID{1802} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ifcvprfinal\pagestyle{empty}\fi

\@ifundefined{showcaptionsetup}{}{\PassOptionsToPackage{caption=false}{subfig}}
\usepackage{subfig}
\makeatother

\usepackage{babel}
\begin{document}
\makeatletter  
\renewcommand{\paragraph}{\@startsection{paragraph}{4}{\z@}{0.5ex \@plus 1ex \@minus .2ex}{-0.5em}{\normalfont \normalsize \bfseries}} 

\let\originalparagraph\paragraph 
\renewcommand{\paragraph}[2][.]{\originalparagraph{#2#1}}

\makeatother


\title{\vspace{-1em}
Exploiting Saliency for Object Segmentation from Image Level Labels}

\author{\vspace{0em}
\setlength\tabcolsep{1em}
\begin{tabular}{ccc} 
Seong Joon Oh\footnotemark[2] & Rodrigo Benenson\footnotemark[2] & Anna   Khoreva\footnotemark[2] \tabularnewline
\texttt{\small{}joon@mpi-inf.mpg.de} & \texttt{\small{}benenson@mpi-inf.mpg.de} & \texttt{\small{}khoreva@mpi-inf.mpg.de} \tabularnewline
\vspace{-0.8em} & & \tabularnewline
Zeynep Akata\footnotemark[2]\hspace{0.35em}\textsuperscript{,}\footnotemark[3] & Mario Fritz\footnotemark[2] & Bernt Schiele\footnotemark[2] \tabularnewline
\texttt{\small{}Z.Akata@uva.nl} & \texttt{\small{}mfritz@mpi-inf.mpg.de} & \texttt{\small{}schiele@mpi-inf.mpg.de} \tabularnewline
\end{tabular}
\\
\\
\renewcommand{\arraystretch}{0.9}
\begin{tabular}{ccc} 
\footnotemark[2]\enskip{}\normalsize{Max Planck Institute for Informatics}  & & \footnotemark[3]\enskip{}\normalsize{Amsterdam Machine Learning Lab} \tabularnewline
\normalsize{Saarland Informatics Campus} & & \normalsize{University of Amsterdam} \tabularnewline
\normalsize{Saarbr\"ucken, Germany} & & \normalsize{Amsterdam, the Netherlands} \tabularnewline
\end{tabular}
}
\maketitle






\begin{abstract}
There have been remarkable improvements in the semantic labelling
task in the recent years. However, the state of the art methods rely
on large-scale pixel-level annotations. This paper studies the problem
of training a pixel-wise semantic labeller network from image-level
annotations of the present object classes. Recently, it has been shown
that high quality seeds indicating discriminative object regions can
be obtained from image-level labels. Without additional information,
obtaining the full extent of the object is an inherently ill-posed
problem due to co-occurrences. We propose using a saliency model as
additional information and hereby exploit prior knowledge on the
object extent and image statistics. We show how to combine both information
sources in order to recover  of the fully supervised performance
\textendash{} which is the new state of the art in weakly supervised
training for pixel-wise semantic labelling. The code is available at \url{https://goo.gl/KygSeb}.


\end{abstract}

\section{\label{sec:Introduction}Introduction}


\noindent Semantic image labelling provides rich information about
scenes, but comes at the cost of requiring pixel-wise labelled training
data. The accuracy of convnet-based models correlates strongly with
the amount of available training data. Collection and annotation of
data have become a bottleneck for progress. This problem has raised
interest in exploring partially supervised data or different means
of supervision, which represents different tradeoffs between annotation
efforts and yields in terms of supervision signal for the learning
task. For tasks like semantic segmentation there is a need to investigate
the minimal supervision to reach the quality comparable to the fully
supervised case.

A reasonable starting point considers that all training images have
image-level labels to indicate the presence or absence of the classes
of interest. The weakly supervised learning problem can be seen as
a specific instance of learning from constraints \cite{Shcherbatyi2016Gcpr,Xu2015CvprWeakSegmentation}.
Instead of explicitly supervising the output, the available labels
provide a constraint on the desired output. If an image label is absent,
no pixel in the image should take that label; if an image label is
present at least in one pixel the image must take that label. However,
the objects of interest are rarely single pixel. Thus to enforce larger
output regions size, shape, or appearance priors are commonly employed
(either explicitly or implicitly). 

\begin{figure}
\begin{centering}
\includegraphics[width=0.9\columnwidth,height=0.4\columnwidth]{figures/teaser}
\par\end{centering}
\caption{\label{fig:teaser}We train a semantic labelling network with (a)
image-level labels and (b) saliency masks, to generate (c) a pixel-wise
labelling of object classes at test time.}
\end{figure}

Another reason for exploiting priors, is the fact that the task is
fundamentally ambiguous. Strongly co-occurring categories (such as
train and rails, sculls and oars, snow-bikes and snow) cannot be separated
without additional information. Because additional information is
needed to solve the task, previous work have explored different avenues,
including class-specific size priors \cite{Pathak2015Iccv}, crawling
additional images \cite{Pinheiro2015Cvpr,Wei2015ArXiv}, or requesting
corrections from a human judge \cite{Kolesnikov2016Bmvc,Saleh2016Eccv}.

Despite these efforts, the quality of the current best results on
the task seems to level out at  of the fully
supervised case. Therefore, we argue that additional information sources
have to be explored to complement the image level label supervision
\textendash{} in particular addressing the inherent ambiguities of
the task. In this work, we propose to exploit class-agnostic saliency
as a new ingredient to train for class-specific pixel labelling; and
show new state of the art results on Pascal VOC 2012 semantic labelling
with image label supervision.

We decompose the problem of object segmentation from image labels
into two separate ones: finding the object location (any point on
the object), and finding the object's extent. Finding the object extent
can be equivalently seen as finding the background area in an image. 

For object location we exploit the fact that image classifiers are
sensitive to the discriminative areas of an image. Thus training using
the image labels enables to find high confidence points over the objects
classes of interest (we call these ``object seeds''), as well as
high confidence regions for background. A classifier however will
struggle to delineate the fine details of an object instance, since
these might not be particularly discriminative.

For finding the object extent, we exploit the fact that a large portion
of photos aim at capturing a subject. Using class-agnostic object
saliency we can find the segment corresponding to some of the detected
object seeds. Albeit saliency is noisy, it provides information delineating
the object extent beyond what seeds can indicate. Our experiment show
that this is an effective source of additional information. Our saliency
model is itself trained from bounding box annotations only. At no
point of our pipeline accurate pixel-wise annotations are used.

In this paper we provide an analysis of the factors that influence
the seeds generation, explore the utility of saliency for the task,
and report best known results both when using image labels only and
image labels with additional data. In summary, our contributions are: 

\begin{itemize}
\item Propose an effective method for combining seed and saliency for weakly supervised semantic segmentation. Our method achieves the best performance among
the known works that utilise image level supervision with or without
additional external data.
\item Compare recent seed methods side by side, and analyse the
    importance of saliency towards the final quality.
\end{itemize}

\S \ref{sec:architecture} presents our overall architecture,
\S \ref{sec:goods-seeds} investigates suitable object seeds,
and \S \ref{sec:object-extent} describes how we use saliency
to guide the convnet training. Finally \S \ref{sec:Experiments}
discusses the experimental setup, and presents our key results.

\section{\label{sec:Related-work}Related work}

\noindent
The last years have seen a renewed interest on weakly supervised training.
For semantic labelling, different forms of supervision have been explored:
image labels \cite{Pathak2015Iclrw,Pathak2015Iccv,Papandreou2015Iccv,Pinheiro2015Cvpr,Wei2015ArXiv,kolesnikov2016seed},
points \cite{Bearman2015ArXiv}, scribbles \cite{Xu2015CvprWeakSegmentation,Lin2016CvprScribbleSup},
and bounding boxes \cite{Dai2015Iccv,Papandreou2015Iccv,Khoreva2016Arxiv}.
In this work we focus on image labels as the main form of supervision.

\paragraph{Object seeds}

Multiple works have considered using a trained classifier (from image
level labels) to find areas of the image that belong to a given class,
without necessarily enforcing to cover the full object extent (high
precision, low recall). Starting from simple strategies such as ``probing
classifier with different image areas occluded'' \cite{Zeiler2014Eccv},
or back-propagating the class score gradient on the image \cite{Simonyan2014Iclr};
significantly more involved strategies have been proposed, mainly
by modifying the back-propagation strategy \cite{Springenberg2015Iclrw,Zhang2016Eccv,Shimoda2016Eccv},
or by solving a per-image optimization problem \cite{Cao2015Iccv}.
All these strategies provide some degree of empirical success but
lack a clear theoretical justification, and tend to have rather noisy
outputs.\\
Another approach considers modifying the classifier training procedure
so as to have it generate object masks as by-product of a forward-pass.
This can be achieved by adding a global a max-pooling \cite{Pinheiro2015Cvpr}
or mean-pooling layer \cite{zhou2015cnnlocalization} in the last
stages of the classifier.\\
In this work we provide an empirical comparison of existing seeders,
and explore variants of the mean-pooling approach \cite{zhou2015cnnlocalization}
(\S \ref{sec:goods-seeds}).

\paragraph{Pixel labelling from image level supervision}

Initial work approached this problem by adapting multiple-instance
learning \cite{Pathak2015Iclrw} and expectation-maximization techniques
\cite{Papandreou2015Iccv}, to the semantic labelling case. Without
additional priors only poor results are obtained. Using superpixels
to inform about the object shape helps \cite{Pinheiro2015Cvpr,Xu2015CvprWeakSegmentation}
and so does using priors on the object size \cite{Pathak2015Iccv}.
\cite{kolesnikov2016seed} carefully uses CRFs to propagate the seeds
across the image during training, while \cite{Qi2016Eccv} exploits
segment proposals for this.\\
Most methods compared propose each a new procedure to train a semantic
labelling convnet. One exception is \cite{Shimoda2016Eccv} which
fuses at test time guided back-propagation \cite{Springenberg2015Iclrw}
at multiple convnet layers to generate class-wise heatmaps. They do
this over a convnet trained for classification. Being based on classifier,
their output masks only partially capture the object extents, as reflected
in the comparatively low performance (table \ref{tab:pascal-results-others}).\\
Recognizing the ill-posed nature of the problem, \cite{Kolesnikov2016Bmvc}
and \cite{Saleh2016Eccv} propose to collect user-feedback as additional
information to guide the training of a segmentation convnet.\\
The closest work to our approach is \cite{Wei2015ArXiv}, which also
uses saliency as a cue to improve weakly supervised semantic segmentation.
There are however a number of differences. First, they use a curriculum
learning to expose the segmentation convnet first with simple images,
and later with more complex ones. We do not need such curriculum,
yet reach better results. Second, they use a manually crafted class-agnostic
saliency method, while we use a deep learning based one (which provides
better cues). Third, their training procedure uses 
additional images of the classes of interest crawled from the web;
we do not use such class-specific external data. Fourth, we report
significantly better results, showing in better light the potential
of saliency as additional information to guide weakly supervised semantic
object labelling. \\
The seminal work \cite{Vezhnevets2011Iccv} proposed to use
``objectness'' map from bounding boxes to guide the semantic
segmentation. By using bounding boxes, these maps end up being
diffuse; in contrast, our saliency map has sharp object
boundaries, thus giving more precise guidance to the semantic
labeller. 


\paragraph{Detection boxes from image level supervision}

Detecting object boxes from image labels has similar challenges as
pixel labelling. The object location and extent need to be found.
State of the art techniques for this task \cite{Bilen2016Cvpr,Teh2016Bmvc,Kantorov2016Eccv}
learn to re-score detection proposals using two stream architectures
that once trained separate ``objectness'' scores from class scores.
These architecture echo with our approach, where the seeds provide
information about the class scores at each pixel (albeit with low
recall for foreground classes), and the saliency output provides a
per-pixel (class agnostic) ``objectness'' score.

\paragraph{Saliency}

Image saliency has multiple connotations, it can refer to a spatial
probability map of where a person might look first \cite{Yamada2010Accv},
a probability map of which object a person might look first \cite{Li2014CvprSaliency},
or a binary mask segmenting the one object a person is most likely
to look first \cite{Borji2015Tip,Shi2016Pami}. We employ the last
definition in this paper. Note that this notion is class-agnostic,
and refers more to the composition of the image, than the specific object
category.\\
Like most computer vision areas, hand-crafted methods \cite{Jiang2013Cvpr,Margolin2013Cvpr,Cheng2015Pami}
have now been surpassed by convnet based approaches \cite{Zhao2015Cvpr,Li2016Tip,Li2016Cvpr}
for object saliency. In this paper we use saliency as an ingredient:
improved saliency models would lead to improved results for our method.
We describe in \S \ref{sec:Saliency-details} our saliency model
design, trained itself in a weakly supervised fashion from bounding
boxes.

\paragraph{Semantic labelling}

Even when pixel-level annotations are provided (fully supervised case),
the task of semantic labelling is far from being solved. Multiple convnet
architectures have been proposed, including recurrent networks \cite{Pinheiro2014Icml},
encoder-decoders \cite{Noh2015Iccv,Badrinarayanan2015ArxivSegNet},
up-sampling layers \cite{Long2015Cvpr}, using skip layers \cite{Bansal2016Arxiv},
or dilated convolutions \cite{Chen2016ArxivDeeplabv2,Yu2016Iclr},
to name a few. Most of them build upon classification architectures
such as VGG \cite{Simonyan2015Iclr} or ResNet \cite{He2016Cvpr}.
For comparison with previous work, our experiments are based on the
popular DeepLab \cite{Chen2016ArxivDeeplabv2} architecture.

\section{\label{sec:architecture} Guided Segmentation architecture}

While previous work has explored sophisticated training losses
or involved pipelines, we focus on saliency as an effective
prior knowledge, and thus keep our architecture simple.

We approach the image-level supervised semantic segmentation problem
via a system with two modules (see figure \ref{fig:high-level-architecture}),
we name this architecture ``Guided Segmentation''. Given an image
and image-level labels, the ``guide labeller'' module combines cues
from a seeder (\S\ref{sec:goods-seeds}) and saliency (\S\ref{sec:object-extent})
sub-modules, producing a rough segmentation mask (the ``guide'').
Then a segmenter convnet is trained using the produced guide mask
as supervision. In this architecture the segmentation convnet is trained
in a fully-supervised procedure, using per pixel softmax
cross-entropy loss. 

In \S \ref{sec:goods-seeds} and \ref{sec:object-extent} we
explain how we build our guide labeller, by first generating seeds
(discriminative areas of objects of interest), and then extending
them to better cover the full object extents. 

\begin{figure}
\begin{centering}
\includegraphics[width=0.95\columnwidth]{figures/method\lyxdot v4}
\par\end{centering}
\caption{\label{fig:high-level-architecture}High level Guided Segmentation
architecture. }
\end{figure}


\section{\label{sec:goods-seeds}Finding goods seeds}

\noindent There has been a recent burst of techniques for localising
objects from a classifier. Some approaches rely on image gradients
from a trained classifier \cite{Simonyan2014Iclr,Springenberg2015Iclrw,Zhang2016Eccv},
while the others propose to train global average pooling (GAP) based
 classifiers \cite{zhou2015cnnlocalization}. Although the classifier
 based localisation approach has a theoretical limitation that the
 training objective (image classification) does not match final goal
 (object locations), they have proved to be effective in practice.
 
 In this section, we review the seeder techniques side by side and
 compare their empirical performances. We report empirical results
 on different GAP architectures \cite{zhou2015cnnlocalization,kolesnikov2016seed,Chen2016ArxivDeeplabv2}. 


\subsection{GAP}

\noindent GAP, or global average pooling layer, can be inserted in
the last or penultimate layer of a fully convolutional architecture,
which produces a dense prediction, to turn it into a classifier. The
resulting architecture is then trained with a classification loss,
and at test time the activation maps before the global average pooling
layer have been shown to contain localisation information \cite{zhou2015cnnlocaliz     ation}. 

In our analysis, we consider four different fully convolutional architectures
with a GAP layer: , ,
, and .
The architectural differences are summarised in table \ref{tab:GAP-architectures},
 and the full details are provided in the supplementary materials.
  \cite{zhou2015cnnlocalization}
 is essentially a fully convolutional version of VGG-16 \cite{Simonyan2015Iclr}.
 , inspired by \cite{kolesnikov2016seed},
 has  times higher output resolution than .
  is a state of the art semantic
 segmenter DeepLab with a GAP layer over the dense score output. The
 main difference between  and
  is the presence of dilated
 convolutions.  is a variant of
  where we use the region of
 interest pooling to replace the sliding window convolutions in the
 last layers of VGG-16.  is identical
 to , except for a slight structural
 variation.


\begin{figure}
\begin{centering}
\subfloat[Foreground categories]{\begin{centering}
\includegraphics[width=0.75\columnwidth]{figures/localisation_variants_precrec_fg}
\par\end{centering}
}
\par\end{centering}
\begin{centering}
\subfloat[Background category]{\begin{centering}
\includegraphics[width=0.75\columnwidth]{figures/localisation_variants_precrec_bg}
\par\end{centering}
}
\par\end{centering}
\caption{\label{fig:comparing-seeds-methods}Precision-recall curves for different seeds. Foregrounds curves show the average precision and recall of the 20 foreground classes.}
\end{figure}

\begin{table}
\begin{centering}
\begin{tabular}{ccccc}
  &   &   &   &  \tabularnewline
 & \cite{zhou2015cnnlocalization} & \cite{kolesnikov2016seed} &  & \cite{Chen2016ArxivDeeplabv2}\tabularnewline
\hline 
high res. & \textbf{\scriptsize{}\XSolidBrush{}} &  &  & \tabularnewline
dil. conv. & \textbf{\scriptsize{}\XSolidBrush{}} & \textbf{\scriptsize{}\XSolidBrush{}} & \textbf{\scriptsize{}\XSolidBrush{}} & \tabularnewline
ROI pool & \textbf{\scriptsize{}\XSolidBrush{}} & \textbf{\scriptsize{}\XSolidBrush{}} &  & \textbf{\scriptsize{}\XSolidBrush{}}\tabularnewline
\hline 
mP & 76.5 & 80.7 & 80.8 & 57.7\tabularnewline
mAP & 88.0 & 87.0 & 87.2 & 92.7\tabularnewline
\end{tabular}
\par\end{centering}

 \caption{\label{tab:GAP-architectures}Architectural comparisons among GAP
  variants together with classification (mAP) and localisation (mP;
  see text for details) performances. We compare the output resolution
  (high res.), use of the dilated convolutions (dil. conv.), and the
  region of interest pooling (ROI pool).}
\end{table}

\subsection{Empirical study}

 \noindent In this section, we empirically compare the seed methods
 side by side focusing on their utility for the final semantic segmentation
 task. Together with GAP methods discussed in the previous section,
 we consider the back-propagation family: Vanilla, Guided, and Excitation
 back-propagations \cite{Simonyan2014Iclr,Springenberg2015Iclrw,Zhang2016Eccv}.
 We include the centre mean shape baseline that always outputs the
 average mask shape; it works as a lower bound on the localisation
 performance.
 
 \paragraph{Evaluation}
 
 We evaluate each method on the \emph{val} set of the Pascal VOC 2012
 \cite{pascal-voc-2012} segmentation benchmark. We plot the foreground
 and background precision-recall curves in figure \ref{fig:comparing-seeds-methods}     .
 In the foreground case, we compute the mean precision and recall over
 the  Pascal categories.
 
 We define mean precision (mP) as a summary metric for localisation
 performance. It averages the foreground precision at  recall
 and the background precision at  recall; .
 Intuitively, for the foreground region we only need a small discriminative
 region, as saliency will fill in the extent; we thus care about precision
 at  recall. On the other hand, background
 has more diverse appearance and usually takes a larger region; we
 thus care about precision at  recall. Since
 we care about both, we take the average (as for the mAP metric). This
 metric has shown a good correlation with the final performance in
 our preliminary experiments.
 
 We measure the classification performance in the standard mean average
 precision (mAP) metric. 
 
 \paragraph{Implementation details}
 
 We train all four GAP network variants for multi-label image classification
 over the \emph{trainaug} set of Pascal VOC 2012. Full convnet training
 details are in the supplementary materials. At test time, we take
 the output per-class heatmaps before the GAP layer and normalise them
 by the maximal per-class scores. 
 
 For the back-propagation based methods, we obtain image (pseudo-)gradients
 from the VGG-16 \cite{Simonyan2015Iclr} classifier trained on the
 \emph{trainaug} set of Pascal VOC 2012 ( images in total).
 We take the maximal absolute gradient value across the RGB channels
 to generate a rough object mask (following \cite{Simonyan2014Iclr});
 it is successively smoothed first with vanilla Gaussian kernel and
 then with dense CRF \cite{Kraehenbuehl2011Nips}. 
 
 In both GAP and backprop variants, we mark pixels with all foreground
 class scores below  as background; other pixels are marked
 according to the argmax foreground class.
 
 \paragraph{Results}
 
 See figure \ref{fig:comparing-seeds-methods} for the precision-recall
 curves. GAP variants have overall greater precision than backprop
 variants at the same recall rate. We note that the Guided backprop
 gives highest precision at a very low recall regime (),
 but the recall is too low to be useful. Among the GAP methods, 
 and  give higher precisions over
 a wide range of recall.  shows
 a significantly lower quality than any other GAP variants.
 
 
 \paragraph{Network matters for GAP}

 Table \ref{tab:GAP-architectures} shows detailed architectural comparisons
 and classification/localisation performances of the GAP variants.
 We observe that the network with higher resolution output has better
 localisation performance (80.7 mP for 
 versus 76.5 mP for ). Dilated
 convolutions significantly hurt the GAP performance (87.0 mP for 
 versus 57.7 mP for ).  The
 architectural choice matters a lot for the localisation performance.
 This contrasts with the classification performances (mAP), which are
 stable across design choices. Intriguingly, 
 is in fact the best classifier and the worst seeder at the same time;
 better design choices for classifiers do not lead to better seeders. 
 
 We use  as the seeder module
 in the next sections. In \cite{kolesnikov2016seed}, foreground and
 background seeds are handled via different mechanisms; in our experiments
 we treat all the non-foreground region as background.

\section{\label{sec:object-extent}Finding the object extent}


Having generated a set of seeds indicating discriminative object areas,
the guide labeller needs to find the extent of the object instances
(\S\ref{sec:architecture}). 

Without any prior knowledge, it is very hard, if not impossible, to
learn the extent of objects only from images and image-level labels.
Image-level labels only convey information about commonly occurring
patterns that are present in images with positive tags and absent
in images with negative tags. The system is thus susceptible to strong
inter-class co-occurrences (e.g. train with rail), as well as systematic
part occlusions (e.g. feet).


\paragraph{\label{subsec:CRF-and-CRFLoss}CRF and CRFLoss}

A traditional approach to make labels match object boundaries is to
solve a CRF inference problem \cite{Lafferty2001ICML,Kraehenbuehl2011Nips}
over the image grid; where the pair-wise terms relate to the object boundaries.
CRF can be applied at three stages: (1) on the seeds (\texttt{crf-seed}),
(2) as a loss function during segmenter convnet training (\texttt{crf-loss})
\cite{kolesnikov2016seed}, and (3) as a post-processing at test time
(\texttt{crf-postproc}). We have experimented with multiple combinations
of those (see supplementary materials section \ref{sec:supp-crf}). 

Albeit some gains are observed, these are inconsistent. For example
 and 
provide near identical classification and seeding performance (see
table \ref{tab:GAP-architectures}), yet using the same CRF setup
provides  percent points in one, but only 
on the other. In comparison our saliency approach will provide 
and  for these two networks respectively (see below).


\subsection{\label{subsec:Saliency}Saliency}


\begin{figure*}
\begin{centering}
\subfloat[\label{fig:high-quality-saliency}High quality]{\begin{centering}
\includegraphics[width=0.15\textwidth,height=0.1\textwidth]{figures/sal\lyxdot quality\lyxdot high\lyxdot 2009_000730\lyxdot orig}\,\includegraphics[width=0.15\textwidth,height=0.1\textwidth]{figures/sal\lyxdot quality\lyxdot high\lyxdot 2009_000730\lyxdot overlay}
\par\end{centering}
}~\subfloat[\label{fig:Medium-quality-saliency}Medium quality]{\begin{centering}
\includegraphics[width=0.15\textwidth,height=0.1\textwidth]{figures/sal\lyxdot quality\lyxdot med\lyxdot 2007_001423\lyxdot orig}\,\includegraphics[width=0.15\textwidth,height=0.1\textwidth]{figures/sal\lyxdot quality\lyxdot med\lyxdot 2007_001423\lyxdot overlay}
\par\end{centering}
}~\subfloat[\label{fig:Low-quality-saliency}Low quality]{\begin{centering}
\includegraphics[width=0.15\textwidth,height=0.1\textwidth]{figures/sal\lyxdot quality\lyxdot low\lyxdot 2008_001283\lyxdot orig}\,\includegraphics[width=0.15\textwidth,height=0.1\textwidth]{figures/sal\lyxdot quality\lyxdot low\lyxdot 2008_001283\lyxdot overlay}
\par\end{centering}
}\vspace{-1em}
\par\end{centering}
\caption{\label{fig:saliency-examples}Example of our saliency map results
on Pascal VOC 2012 data.}
\end{figure*}


\noindent
We propose to use object saliency to extract information about the
object extent. We work under the assumption that a large portion of
the dataset are intentional photographies, which is the case for most
datasets crawled from the web such as Pascal \cite{pascal-voc-2012}
and Coco \cite{Lin2014EccvCoco}. If the image contains a single label
``dog'', chances are that the image is about a dog, and that the
salient object of the image is a dog. We use a convnet based saliency
estimator (detailed in \S \ref{sec:Saliency-details}) which
adds the benefit of translation invariance. If two locally salient
dogs appear in the image, both will be labelled as foreground.

When using saliency to guide semantic labelling at least two difficulties
need to be handled. For one, saliency per-se does not segment object
instances. In the example figure \ref{fig:high-quality-saliency},
the person-bike is well segmented, but person and bike are not separated.
Yet, the ideal Guide labeller (fig. \ref{fig:high-level-architecture})
should give different labels to these two objects. The second difficulty,
clearly visible in the examples of figure \ref{fig:saliency-examples},
is that the salient object might not belong to a category of interest
(shirt instead of person in figure \ref{fig:Medium-quality-saliency})
or that the method fails to identify any salient region at all (figure
\ref{fig:Low-quality-saliency}). 

We measure the saliency quality when compared to the ground truth
foreground on Pascal VOC 2012 validation\emph{ }set. Albeit our convnet
saliency model is better than hand-crafted methods \cite{Jiang2013Cvpr,zhang2015MBD},
in the end only about  of images have reasonably good ()
foreground saliency quality. Yet, as we will see in \S \ref{sec:Experiments},
this bit of information is already helpful for the weakly supervised
learning task. 


\begin{figure}
\begin{centering}
\begin{tabular}{cccc}
\begin{turn}{90}
\hspace*{-1em}\begin{tabular}{c}
Salient object\tabularnewline
{\small{}and boxes}\tabularnewline
\end{tabular}
\end{turn} & \includegraphics[height=0.2\columnwidth]{figures/sal\lyxdot ex\lyxdot 84_84025\lyxdot box}\hspace*{-0.5em} & \includegraphics[height=0.2\columnwidth]{figures/sal\lyxdot ex\lyxdot 92_92376\lyxdot box}\hspace*{-0.5em} & \includegraphics[height=0.2\columnwidth]{figures/sal\lyxdot ex\lyxdot 7_202_202613\lyxdot box}\hspace*{-0.5em}\tabularnewline
\begin{turn}{90}
\hspace*{-1em}\begin{tabular}{c}
Saliency \tabularnewline
model{\small{} result}\tabularnewline
\end{tabular}
\end{turn} & \includegraphics[height=0.2\columnwidth]{figures/sal\lyxdot ex\lyxdot 84_84025\lyxdot sal}\hspace*{-0.5em} & \includegraphics[height=0.2\columnwidth]{figures/sal\lyxdot ex\lyxdot 92_92376\lyxdot sal}\hspace*{-0.5em} & \includegraphics[height=0.2\columnwidth]{figures/sal\lyxdot ex\lyxdot 7_202_202613\lyxdot sal}\hspace*{-0.5em}\tabularnewline
\end{tabular}
\par\end{centering}
\caption{\label{fig:MSRA-saliency}Example of saliency results on its training
data. We use MSRA box annotations to train a weakly supervised saliency
model. Note that the MSRA subset employed does not contain Pascal
categories.}
\end{figure}

 Crucially, our saliency system is trained on images containing diverse
 objects (hundreds of categories), the object categories are treated
 as ``unknown'', and to ensure clean experiments we handicap the
 system by removing any instance of Pascal categories in the object
 saliency training set. Our saliency model captures a general notion
 of plausible foreground objects and background areas (more details
 in section \ref{sec:Saliency-details}).
 
 On every Pascal training image, we obtain a class-agnostic foreground/background
 binary mask from our saliency model, and high precision/low recall
 class-specific image labels from the seeds model (section \ref{sec:goods-seeds}).
 We want to combine them in such a way that seed signals are well propagated
 throughout the foreground saliency mask. We consider two baselines
 strategies to generate guide labels using saliency but no seeds (
 and ), and then discuss how we combine saliency
 with seeds (). 

\paragraph{ Random class assignment}

Given a saliency mask, we assign all foreground pixels to a class
randomly picked from the ground truth image labels. If a single ``dog''
label is present, then all foreground pixels are ``dog''. Two labels
are present (``dog, cat''), then all pixels are either dog or cat.

\paragraph{ Per-connected component classification}

Given a saliency mask, we split it in components, and assign a separate
label for each component. The per-component labels are given using
a full-image classifier trained using the image labels (classifier
details in \S \ref{par:G1-Classifier-details}). Given a connected
component mask  (with pixel values : foreground,
: background), we compute the classifier scores when feeding the
original image (), and when feeding an image with background zeroed
(). Region  will be labelled with
the ground truth class with the greatest positive score difference
before and after zeroing.

\paragraph{ Propagating seeds}

Here, instead of assigning the label per connected component 
using a classifier, we instead use the seed labels. We also treat
the seeds as a set of connected components (seed ). Depending
on how the seeds and the foreground regions intersect, we decide the
label for each pixel in the guide labeller output. 

Our fusion strategy uses five simple ideas. 1) We treat the seeds
as reliable small size point predictors of each object instance, but
that might leak outside of the object. 2) We assume the saliency might
trigger on objects that are not part of the classes of interest. 3)
A foreground connected component  should take the label
of the seed touching it, 4) If two (or more) seeds touch the same
foreground component, then we want to propagate all the seed labels
inside it. 5) When in doubt, mark as ignore. The details for the corner
cases are provided in the supplementary material section \ref{sec:supp-alg}.

Figure \ref{fig:combination-strategies-examples} provides example
results of the different guide strategies. For additional qualitative
examples of seeds, saliency foreground, and generated labels, see
figure \ref{fig:qualitative-examples}. With our guide strategies
, , and  at
hand, we now proceed to empirically evaluate them in \S \ref{sec:Experiments}.

\section{\label{sec:Experiments}Experiments}

\begin{figure*}
\begin{centering}
\hspace*{\fill}\subfloat[Image]{\begin{centering}
\includegraphics[width=0.24\columnwidth]{figures/guide_strategies/2010_003115\lyxdot 1orig}
\par\end{centering}
}\hspace*{\fill}\subfloat[Ground truth]{\begin{centering}
\includegraphics[width=0.24\columnwidth]{figures/guide_strategies/2010_003115\lyxdot 2gt}
\par\end{centering}
}\hspace*{\fill}\subfloat[Seed]{\begin{centering}
\includegraphics[width=0.24\columnwidth]{figures/guide_strategies/2010_003115\lyxdot 3gap}
\par\end{centering}
}\hspace*{\fill}\subfloat[Saliency]{\begin{centering}
\includegraphics[width=0.24\columnwidth]{figures/guide_strategies/2010_003115\lyxdot 4sal}
\par\end{centering}
}\hspace*{\fill}\subfloat[]{\begin{centering}
\includegraphics[width=0.24\columnwidth]{figures/guide_strategies/2010_003115\lyxdot 5gv0}
\par\end{centering}
}\hspace*{\fill}\subfloat[]{\begin{centering}
\includegraphics[width=0.24\columnwidth]{figures/guide_strategies/2010_003115\lyxdot 6gv1}
\par\end{centering}
}\hspace*{\fill}\subfloat[]{\begin{centering}
\includegraphics[width=0.24\columnwidth]{figures/guide_strategies/2010_003115\lyxdot 7gv2}
\par\end{centering}
}\hspace*{\fill}
\par\end{centering}
\caption{\label{fig:combination-strategies-examples}Guide labelling strategies
example results. The image, its labels (``bicycle, chair''), seeds,
and saliency map are their input. White overlay indicates ``ignore''
pixel label.}
\end{figure*}
\noindent
\S \ref{subsec:Experimental-setup} and \ref{subsec:Implementation-details}
provide the details of the evaluation and our implementation. \S
\ref{subsec:Ingredients-study} compares our different guide strategies
amongst each other, and \S \ref{subsec:main-results} compares
with previous work on weakly supervised semantic labelling from image-level
labels.

\paragraph{\label{subsec:Experimental-setup}Evaluation}

We evaluate our image-level supervised semantic segmentation system
on the PASCAL VOC 2012 segmentation benchmark \cite{pascal-voc-2012}.
We report all the intermediate results on the \textit{val}\emph{ }set
( images) and only report the final system result on the
\textit{test} set ( images). Evaluation metric is the standard mean
intersection-over-union (mIoU).


\subsection{\label{subsec:Implementation-details}Implementation details}

\noindent
For training seeder and Segmenter networks, we use the
ImageNet \cite{imagenet_cvpr09} pretrained models for initialisation
and fine-tune on the Pascal VOC 2012 \emph{trainaug} set (
images), an extension of the original \textit{train} set ( images)
\cite{pascal-voc-2012,Hariharan2011Iccv}. This is the same procedure
used by previous work on fully \cite{Chen2016ArxivDeeplabv2} and
weakly supervised learning \cite{kolesnikov2016seed}.

\paragraph{Seeder}

Results in tables \ref{tab:pascal-results-ours} and \ref{tab:pascal-results-others}
are obtained using  (see \S\ref{sec:goods-seeds}),
trained for image classification on the Pascal trainaug set. The test
time foreground threshold  is set to , following the
previous literature \cite{zhou2015cnnlocalization,kolesnikov2016seed}.

\paragraph{ \label{par:G1-Classifier-details}Classifier}

The guide labeller strategy  uses an image classifier
trained on Pascal \textit{trainaug} set. We use the VGG-16 architecture \cite{Simonyan2015Iclr}
with a multi-label loss. 

\paragraph{\label{sec:Saliency-details}Saliency}

Following \cite{Zhao2015Cvpr,Li2016Tip,Li2016Cvpr} we re-purpose
a semantic labelling network for the task of class-agnostic saliency.
 We train the DeepLab-v2 ResNet \cite{Chen2016ArxivDeeplabv2} over
  a subset of MSRA \cite{liu2011learning}, a saliency dataset with
  \emph{class agnostic} bounding box annotations. We constrain the training
  only to samples of \emph{non-Pascal} categories. Thus, the saliency
  model does not leverage class specific features when Pascal images
  are fed. Out of  MSRA images,  remain after filtering. 
  
MRSA provides bounding boxes (from multiple annotators) of the main
salient element of each image. To train the saliency model to output
pixel-wise masks, we follow \cite{Khoreva2016Arxiv}. We generate
segments from the MSRA boxes by applying grabcut over the average
box annotation, and use these as supervision for the DeepLab model.
The model is trained as a binary semantic labeller for foreground
and background regions. The trained model generates masks like the
ones shown in figure \ref{fig:MSRA-saliency}. Although having been
trained with images with single salient objects, due to its convolutional
nature the network can predict multiple salient regions in the Pascal
images (as shown in figure \ref{fig:qualitative-examples}).

At test time, the saliency model generates a heatmap of foreground
probabilities. We threshold at  of the maximal foreground
probability to generate the mask.


\paragraph{Segmenter}

For comparison with previous work we use the DeepLabv1-LargeFOV \cite{Chen2016ArxivDeeplabv2}
architecture as our segmenter convnet. The network is trained on Pascal
\textit{trainaug} set with  images, using the output of the guide
labeller (\S\ref{fig:high-level-architecture}), which uses only
the image itself and presence-absence tags of the  Pascal categories
as supervision. The network is trained for  iterations. \\
Following the standard DeepLab procedure, at test time we up-sample
the output to the original image resolution and apply the dense CRF
inference \cite{Kraehenbuehl2011Nips}. Unless stated otherwise, we
use the CRF parameters used for DeepLabv1-LargeFOV \cite{Chen2016ArxivDeeplabv2}.

Additional training details and hyper-parameters are given in the
supplementary materials section \ref{sec:supp-convnet-training}.


\subsection{\label{subsec:Ingredients-study}Ingredients study}

 \noindent Table \ref{tab:pascal-results-ours} compares different
 guide strategies , , ,
 and oracle versions of . The first row shows the
 result of training our segmenter using the seeds directly as guide
 labels. This leads to poor quality (). The ``Supervision''
column shows recall and precision for foreground and background of
the guide labels themselves (training data for the segmenter). We
can see that the seeds alone have low recall for the foreground ().
In comparison, using saliency only,  reaches significantly
better results, due to the higher foreground recall (), at
a comparable precision.

Adding a classifier on top of the saliency ()
provides only a negligible improvement (). This
can be attributed the fact that many Pascal images contain only a
single foreground class, and that the classifier might have difficulties
recognizing the masked objects. Interestingly, when using a similar
classifier to generate seeds instead of scoring the image ()
we gain  (percent points, ).
This shows that the details of how a classifier is used can make a
large difference. 

Table \ref{tab:pascal-results-ours} also reports a saliency oracle
case on top of . If we use the ground truth annotation
 to generate an ideal saliency mask, we see a significant improvement
 over  (). Thus, the quality
 of saliency is an important ingredient, and there is room for further
 gains.


\begin{table}
\begin{centering}
\setlength{\tabcolsep}{1pt} \par\end{centering}
\begin{centering}
\par\end{centering}
\begin{centering}
\hspace*{-1em}\begin{tabular}{ccc|cccc|c}
\multirow{2}{*}{Method} & \multirow{2}{*}{Seeds} & Sali- & \multicolumn{4}{c|}{Supervision} & \multicolumn{1}{c}{val. set}\tabularnewline
 &  & ency & \multicolumn{2}{c}{Fg {\small{}P/R}} & \multicolumn{2}{c|}{Bg {\small{}P/R}} & mIoU\tabularnewline
\hline 
\hline 
Seeds only &  & \textbf{\scriptsize{}\XSolidBrush{}} & 69 & 37 & 81 & 95 & 38.7\tabularnewline
 & \textbf{\scriptsize{}\XSolidBrush{}} &  & 65 & 52 & 65 & 52 & 45.8\tabularnewline
  & \textbf{\scriptsize{}\XSolidBrush{}} &  & 75 & 51 & 75 & 51 & 46.2\tabularnewline
 &  &  & 73 & 59 & 87 & 95 & 51.2\tabularnewline
\hline 
\multirow{1}{*}{\hspace*{-1em}Saliency oracle\hspace*{-2em}} &  &  & 89 & 91 & 100 & 99 & 56.9\tabularnewline
\end{tabular}
\par\end{centering}
\begin{centering}
\par\end{centering}
\caption{\label{tab:pascal-results-ours}Comparison of different guide labeller
variants. Pascal VOC 2012 validation set results, without CRF post-processing.
Fg/Bg P/R: are foreground/background precision and recall of the guide
labels. Discussion in \S\ref{subsec:Ingredients-study}.}
\end{table}


\subsection{\label{subsec:main-results}Results}

 \noindent Table \ref{tab:pascal-results-others} compares our results
 with previous related work. We group results by methods that only
 use ImageNet pre-training and image-level labels (I, P, E; see legend
 table \ref{tab:pascal-results-others}), and methods that use additional
 data or user-inputs. Here our  and 
 results include a CRF post-processing (\texttt{crf-postproc}). We
 also experimented with \texttt{crf-loss }but did not find a parameter
set that provided improved results.

We see that the guide strategies , which uses saliency
and random ground-truth label, reaches competitive performance compared
to methods using +P only. This shows that saliency by itself
is already a strong cue. Our guide strategy  (which
uses seeds and saliency) obtains the best reported results on this
task\footnote{\cite{Qi2016Eccv} also reports  validation set results, however
we do not consider these results comparable since they use the MCG
scores \cite{PontTuset2015ArxivMcg}, which are trained on the ground
truth Pascal segments.}. We even improve over other methods using saliency (\texttt{STC})
or using additional human annotations (\texttt{MicroAnno},\texttt{
CheckMask}). Compared to a fully supervised DeepLabv1 model, our results
reach  of the fully supervised quality. 

\begin{table}
\begin{centering}
\begingroup
\setlength{\tabcolsep}{0.15em} 
\begin{tabular}{cccccc}
 &  &  & val. set & \multicolumn{2}{c}{test set}\tabularnewline
 & Method & Data & mIoU & mIoU & \tabularnewline
\hline 
\hline 
\multirow{9}{*}{\begin{turn}{90}
{\small{}Image labels only}
\end{turn}} & \texttt{MIL-FCN}\,\cite{Pathak2015Iclrw} & +P & 25.0 & 25.6 & 36.5\tabularnewline
 & \texttt{CCNN}\,\cite{Pathak2015Iccv} & +P & 35.3 & 35.6 & 50.6\tabularnewline
 & \texttt{WSSL}\,\cite{Papandreou2015Iccv} & +P & 38.2 & 39.6 & 56.3\tabularnewline
 & \texttt{MIL+Seg}\,\cite{Pinheiro2015Cvpr} & + & 42.0 & 40.6 & 57.8\tabularnewline
 & \texttt{DCSM}\,\cite{Shimoda2016Eccv} & +P & 44.1 & 45.1 & 64.2\tabularnewline
 & \texttt{CheckMask}\,\cite{Saleh2016Eccv} & +P & 46.6 & - & -\tabularnewline
 & \texttt{SEC}\,\cite{kolesnikov2016seed} & +P & 50.7 & 51.7 & 73.5\tabularnewline
 & \texttt{AF-ss}\,\cite{Qi2016Eccv} & +P & 51.6 & - & -\tabularnewline
\cline{2-6} 
 & Seeds only & +P & 39.8 & - & -\tabularnewline
\hline 
\multirow{6}{*}{\begin{turn}{90}
{\small{}More information}
\end{turn}} & \texttt{CCNN}\,\cite{Pathak2015Iccv} & +P+Z & - & 45.1 & 64.2\tabularnewline
 & \texttt{STC}\,\cite{Wei2015ArXiv} & +P++ & 49.8 & 51.2 & 72.8\tabularnewline
 & \texttt{CheckMask}\,\cite{Saleh2016Eccv} & +P+ & 51.5 & - & -\tabularnewline
 & \texttt{MicroAnno}\,\cite{Kolesnikov2016Bmvc} & +P+ & 51.9 & 53.2 & 75.7\tabularnewline
\cline{2-6} 
 &  & +P+ & 48.8 & - & -\tabularnewline
 &  & +P+ & \textbf{55.7} & \textbf{56.7} & \textbf{80.6}\tabularnewline
\hline 
 & DeepLabv1 & + & 67.6 & 70.3 & 100\tabularnewline
\end{tabular}\endgroup
\par\end{centering}
\caption{\label{tab:pascal-results-others}Comparison of state of the art methods,
on Pascal VOC 2012 val.\emph{ }and test set. : fully
supervised percent. Ingredients: : ImageNet classification
pre-training, P: Pascal image level tags, : fully
supervised case (pixel wise labels), :  extra images
with image level tags, S: saliency, Z: per-class size prior, :
human-in-the-loop micro-annotations.}
\end{table}

\begin{figure}
\begin{centering}
\begin{tabular}{ccc}
\begin{turn}{90}
Input image
\end{turn} & \includegraphics[width=0.35\columnwidth]{figures/main_method_examples/2010_003736\lyxdot 1orig} & \includegraphics[width=0.47\columnwidth]{figures/main_method_examples/2008_006071\lyxdot 1orig}\tabularnewline
\begin{turn}{90}
Seeds\hspace*{-3em}
\end{turn} & \includegraphics[width=0.35\columnwidth]{figures/main_method_examples/2010_003736\lyxdot 3gap} & \includegraphics[width=0.47\columnwidth]{figures/main_method_examples/2008_006071\lyxdot 3gap}\tabularnewline
\begin{turn}{90}
Saliency\hspace*{-3em}
\end{turn} & \includegraphics[width=0.35\columnwidth]{figures/main_method_examples/2010_003736\lyxdot 4sal} & \includegraphics[width=0.47\columnwidth]{figures/main_method_examples/2008_006071\lyxdot 4sal}\tabularnewline
\begin{turn}{90}
\begin{tabular}{c}
\tabularnewline
{\small{}(Seeds+Saliency)}\tabularnewline
\end{tabular}
\end{turn} & \includegraphics[width=0.35\columnwidth]{figures/main_method_examples/2010_003736\lyxdot 5v6} & \includegraphics[width=0.47\columnwidth]{figures/main_method_examples/2008_006071\lyxdot 5v6}\tabularnewline
\begin{turn}{90}
\begin{tabular}{c}
Segmenter\tabularnewline
output\tabularnewline
\end{tabular}
\end{turn} & \includegraphics[width=0.35\columnwidth]{figures/main_method_examples/2010_003736\lyxdot 6deeplab} & \includegraphics[width=0.47\columnwidth]{figures/main_method_examples/2008_006071\lyxdot 6deeplab}\tabularnewline
\begin{turn}{90}
+CRF\hspace*{-3em}
\end{turn} & \includegraphics[width=0.35\columnwidth]{figures/main_method_examples/2010_003736\lyxdot 7pcrf} & \includegraphics[width=0.47\columnwidth]{figures/main_method_examples/2008_006071\lyxdot 7pcrf}\tabularnewline
\begin{turn}{90}
Ground truth
\end{turn} & \includegraphics[width=0.35\columnwidth]{figures/main_method_examples/2010_003736\lyxdot 2gt} & \includegraphics[width=0.47\columnwidth]{figures/main_method_examples/2008_006071\lyxdot 2gt}\tabularnewline
\end{tabular}
\par\end{centering}
\caption{\label{fig:qualitative-examples}Qualitative examples of the different
stages of our system. Additional examples in the supplementary
materials figures \ref{fig:qualitative-examples-1} and \ref{fig:qualitative-examples-2}.}
\end{figure}



\section{\label{sec:Conclusion}Conclusion}

\noindent
We have addressed the problem of training a semantic segmentation
convnet from image labels. Image labels alone can provide high quality
seeds, or discriminative object regions, but learning the full object
extents is a hard problem. We have shown that saliency is a viable
option for feeding the object extent information.

The proposed Guided Segmentation architecture (\S\ref{sec:architecture}),
where the ``guide labeller'' combines cues from the seeds and saliency,
can successfully train a segmentation convnet to achieve the state
of the art performance. Our weakly supervised results reach 
of the fully supervised case.

We expect that a deeper understanding of the seeder methods and improvements
on the saliency model can lead to further improvements. 

\subsection*{Acknowledgements}

\noindent
This research was supported by the German Research Foundation (DFG CRC
1223).




\bibliographystyle{ieee}
\bibliography{arxiv}


\newpage
\appendix



\part*{Supplementary Materials}

\section{\label{sec:supp-content}Content}

This document contains the following additional details:
\begin{itemize}
\item GAP

\begin{itemize}
\item Architecture details for , ,
, .
\item Training details.
\item Qualitative results.
\end{itemize}
\item CRF experiments and CRF parameters used.
\item More qualitative results of our saliency model.
\item Details of  guide labeller rules, and more qualitative
examples of , , and 
strategies.
\item Convnet training details for Seeder, Classifier, and Segmenter networks.
\item Additional qualitative examples like figure \ref{fig:qualitative-examples} in the main paper.
\end{itemize}


\section{\label{sec:supp-gap}GAP}

\subsection{\label{sec:supp-gap-net}Network details}

See table \ref{tab:GAP-networks} for the details of the GAP networks
used. For , we insert the GAP layer
after the final linear layer, instead of after the penultimate layer
as suggested by \cite{zhou2015cnnlocalization}. We note that the
resulting functions are identical (and hence the forward and backward
passes): GAP is a linear sum over the spatial dimensions, and the
final layer performs a linear combination over the channel dimensions,
so they can be swapped without changing the function. Also in practice,
we find that there is only negligible difference in performance between
the variants.


\subsection{\label{sec:supp-gap-train}Training GAP}

All GAP network variants are trained with stochastic gradient descent
(SGD) with minibatch size , momentum , weight decay ,
and base learning rate , decreased by the factor of 
at every  iterations. The training stops at  iterations.

\subsection{\label{sec:supp-gap-qual}Qualitative examples}

See figure \ref{fig:GAP-qualitative} for qualitative examples. We
observe that , ,
and  show qualitatively similar
results, while  has significantly
low quality with repeating patterns in the output. The output suggests
that the learned filters have repeating patterns modulo 
output pixels, which is the width of the dilated filters in DeepLab-LargeFOV
\cite{Chen2016ArxivDeeplabv2} on  features.

\section{\label{sec:supp-crf}CRF}

See table \ref{tab:CRF-loss-summary} for the segmenter performance
after applying different combinations of CRF units, \texttt{crf-seed},
\texttt{crf-loss}, and \texttt{crf-postproc}. Combination of \texttt{crf-loss}
and \texttt{crf-postproc} on the 
seed gives 50.4 mIoU, giving  mIoU boost over the vanilla seed.
However, we do not see such a gain when either the CRF parameters
or the seed type is changed. When CRF parameters are changed from
 to , both of which are reasonable choices (see \S\ref{sec:crf-parameters}),
we lose  mIoU. When the seed type is changed from 
to , we lose  mIoU. The 
mIoU boost thus seems fragile.

Our saliency-based model, on the other hand, gives a consistent 
mIoU gain over the best CRF combination, regardless of the seed type
used, showing superiority over CRF both in terms of performance and
stability. It is possible to combine \texttt{crf-loss} and saliency,
but our preliminary experiments show that it hurts the performance
of the saliency-only case. Thus, \texttt{crf-loss} is excluded from
our final model.

See table \ref{tab:CRF-loss} for all the combinations considered
in our experiments. 

\begin{table*}
\caption{\label{tab:CRF-loss-summary}Results of the CRF variants on Pascal
2012 validation. , : CRF parameters from \cite{kolesnikov2016seed}
and \cite{Chen2016ArxivDeeplabv2} respectively.}

\begin{centering}
\begin{tabular}{cccccc}
 & \multicolumn{3}{c}{} & \multicolumn{2}{c}{}\tabularnewline
\hline 
Seed method & \multicolumn{3}{c}{\texttt{crf}} & \multicolumn{2}{c}{val. set}\tabularnewline
 & \texttt{-seed} & \texttt{-loss} & \texttt{-postproc} & mIoU & \tabularnewline
\hline 
 & \textbf{\scriptsize{}\XSolidBrush{}} & \textbf{\scriptsize{}\XSolidBrush{}} & \textbf{\scriptsize{}\XSolidBrush{}} & 37.5 & \tabularnewline
 & \textbf{\scriptsize{}\XSolidBrush{}} &  &  & 50.4 & 0\tabularnewline
 & \textbf{\scriptsize{}\XSolidBrush{}} &  &  & 45.2 & \tabularnewline
 & \multicolumn{3}{c}{Saliency: } &  & \tabularnewline
\hline 
 & \textbf{\scriptsize{}\XSolidBrush{}} & \textbf{\scriptsize{}\XSolidBrush{}} & \textbf{\scriptsize{}\XSolidBrush{}} & 37.6 & \tabularnewline
 & \textbf{\scriptsize{}\XSolidBrush{}} &  &  & 45.0 & \tabularnewline
 & \multicolumn{3}{c}{Saliency: } &  & \tabularnewline
\hline 
 &  &  &  &  & \tabularnewline
\end{tabular}
\par\end{centering}

\end{table*}

\begin{table*}
\caption{\label{tab:CRF-loss}Extension of table \ref{tab:CRF-loss-summary}
showing all the combinations considered.}

\begin{centering}
\begin{tabular}{cccccc}
 & \multicolumn{3}{c}{} & \multicolumn{2}{c}{}\tabularnewline
\hline 
Seed method & \multicolumn{3}{c}{\texttt{crf}} & \multicolumn{2}{c}{val. set}\tabularnewline
 & \texttt{-seed} & \texttt{-loss} & \texttt{-postproc} & mIoU & \tabularnewline
\hline 
 & \textbf{\scriptsize{}\XSolidBrush{}} & \textbf{\scriptsize{}\XSolidBrush{}} & \textbf{\scriptsize{}\XSolidBrush{}} & 37.5 & \tabularnewline
 & \textbf{\scriptsize{}\XSolidBrush{}} &  &  & 50.4 & 0\tabularnewline
 & \textbf{\scriptsize{}\XSolidBrush{}} &  & \textbf{\scriptsize{}\XSolidBrush{}} & 46.4 & \tabularnewline
 & \textbf{\scriptsize{}\XSolidBrush{}} & \textbf{\scriptsize{}\XSolidBrush{}} &  & 45.5 & \tabularnewline
 & \textbf{\scriptsize{}\XSolidBrush{}} &  &  & 45.2 & \tabularnewline
 &  & \textbf{\scriptsize{}\XSolidBrush{}} & \textbf{\scriptsize{}\XSolidBrush{}} & 33.0 & \tabularnewline
 & \multicolumn{3}{c}{Saliency: } &  & \tabularnewline
\hline 
 & \textbf{\scriptsize{}\XSolidBrush{}} & \textbf{\scriptsize{}\XSolidBrush{}} & \textbf{\scriptsize{}\XSolidBrush{}} & 37.6 & \tabularnewline
 & \textbf{\scriptsize{}\XSolidBrush{}} &  &  & 45.0 & \tabularnewline
 & \textbf{\scriptsize{}\XSolidBrush{}} &  &  & 44.2 & \tabularnewline
 & \multicolumn{3}{c}{Saliency: } &  & \tabularnewline
\hline 
 &  &  &  &  & \tabularnewline
\end{tabular}
\par\end{centering}

\end{table*}

\subsection{\label{sec:supp-crf-param}CRF parameters\label{sec:crf-parameters}}

Throughout the paper, we use the CRF parameters from the DeepLab-LargeFOV
model \cite{Chen2016ArxivDeeplabv2}, unless stated otherwise. The
parameters are given by , , and
 for the appearance kernel, and  and
 for the smoothness kernel, following the notation
of equation 3 in \cite{Kraehenbuehl2011Nips}. For compatibility,
we always use the Potts model: .

For some experiments, we also use parameters from \cite{kolesnikov2016seed},
which uses , , and 
for the appearance kernel, and  and 
for the smoothness kernel.

\section{\label{sec:supp-saliency}Saliency}

See figure \ref{fig:MSRA-saliency-1} for more examples of the MSRA
training samples for our weakly supervised saliency model. Samples
corresponding to Pascal categories are excluded from the training.

See figure \ref{fig:saliency-qualitative-1} for qualitative examples
of our saliency model on the Pascal images. We observe that the saliency
model does fail in examples usually when the central salient object
is not Pascal category, or when the scene is cluttered.

\section{\label{sec:supp-alg} guide labeller algorithm}

We introduce details of the algorithm for  strategy
of combining seed and saliency signals (\S5.3 of the main paper).
As mentioned in the main paper, we follow five simple ideas:
\begin{enumerate}
\item We treat seeds as reliable small size point predictors of each object
instance.
\item We assume the saliency might trigger on objects that are not part
of the classes of interest.
\item If a seed touches a connected component it should take
the label of the seed.
\item If two (or more) seeds touch the same foreground component, then we
want to propagate all the seed labels inside it.
\item When in doubt, mark as ignore.
\end{enumerate}
The detailed procedure is given as follows.

We compute the set of connected components of the saliency foreground
mask with area   of the image size, ,
and similarly for the set of connected components of the seeds, .
For each , we assign a ground truth label on it depending
on how many foreground seed categories it intersects with:
\begin{itemize}
\item 0 category:  is then either a false positive from the
saliency (e.g. salient object that is not part of the classes of interest),
or a false negative from the seeds. We don't commit to any of those
cases by marking with ``ignore'' label.
\item 1 category:  is delineating the full extent of the instance
for the seed. Put the class label from the seed.
\item  categories:  is a combination of instances from
multiple classes. Use dense CRF inference inside , with
unaries set by the seed(s), to assign precise pixel-wise labels in
.
\end{itemize}
After assigning pixel-wise labels on each , we perform
the following operations regarding the seed connected components :
\begin{itemize}
\item When a seed  intersects with some , but is
not strictly covered by , we put ``ignore'' labels
on the seed region bleeding out of , assuming that the
saliency mask provides a better delineation of the object.
\item If a seed  touches two or more foreground regions, it
will propagate its label to all of them.
\item Whenever there is an isolated seed  not intersecting with
any , we treat it as a reliable foreground prediction
missed by saliency, and include it in the final guide labelling. 
\end{itemize}

See figure \ref{fig:guide-strategies} for the qualitative examples
of guide labelling strategies, , ,
and . Note that  produces much
more precise labelling with the access to rich localisation information
from the seeds. 

\section{\label{sec:supp-convnet-training}Convnet training details}

\paragraph{Saliency}

The network is -v2 ResNet, and follows the training
procedure for -v2 ResNet in \cite{Chen2016ArxivDeeplabv2}. 

\paragraph{Segmenter}

The network is -v1, and is trained with stochastic
gradient descent (SGD) with minibatch size , momentum ,
weight decay , and base learning rate , decreased
by the factor of  at every  iterations. The training
stops at  iterations. 

\paragraph{Classifiers}

All classifiers discussed in the paper are -16 trained
with stochastic gradient descent (SGD) with minibatch size ,
momentum , weight decay , and base learning
rate , decreased by the factor of  at every 
iterations. The training stops at  iterations. 

\section{\label{sec:supp-qualitative}Qualitative examples}

See figure \ref{fig:qualitative-examples-1} and \ref{fig:qualitative-examples-2}
for more qualitative examples of the seeds, saliency, 
guide labeller output, and Guided Segmentation trained results on
the training set. Seeds have high precision and low recall. The saliency
foreground mask gives a pixel-wise class-agnostic object extent information.
 guide labeller combines both sources to generate
an accurate class-wise guide labelling. The generated guide labelling
can still be noisy especially if the quality of the saliency mask
is low. However, the segmenter convnet averages out the noisy supervision
to produce more precise predictions. CRF post-processing further refines
the predictions.


\newpage





\begin{table*}
\caption{\label{tab:GAP-networks}Detailed architecture of the GAP networks.
Triplets denote (channel, height, width) for the input and output
data, and (output channel dim, kernel height, kernel width) for the
layer parameters. stride and width
of dilated convolution, with default values  for both, unless
otherwise stated.}

\begin{centering}
\begin{tabular}{cccccc}
 &  &  &  &  & \tabularnewline
Layers &  &  &   &   &  \tabularnewline
 & \cite{Simonyan2015Iclr} &  \cite{zhou2015cnnlocalization} & \cite{kolesnikov2016seed} &  & \cite{Chen2016ArxivDeeplabv2}\tabularnewline
\hline 
 (C, H, W) &  &  &  &  & \tabularnewline
\hline 
 &  & Same as & Same as & Same as & Same as\tabularnewline
 &  &  &  &  & \tabularnewline
\hline 
 &  & Same as & Same as & Same as & \tabularnewline
 &  &  &  &  & , \tabularnewline
\hline 
 &  & Same as & Same as & Same as & Same as\tabularnewline
 &  &  &  &  & \tabularnewline
\hline 
 &  & Same as & Same as & Same as & \tabularnewline
 &  &  &  &  & , \tabularnewline
\hline 
 &  & Same as & Same as & Same as & Same as\tabularnewline
 &  &  &  &  & \tabularnewline
\hline 
 &  & Same as & Same as & Same as & \tabularnewline
 &  &  &  &  & , \tabularnewline
\hline 
 &  & Same as & Same as & Same as & Same as\tabularnewline
 &  &  &  &  & \tabularnewline
\hline 
 &  & Same as & None & None & \tabularnewline
 &  &  &  &  & , \tabularnewline
\hline 
 &  & Same as & Same as & Same as & \tabularnewline
 &  &  &  &  & , \tabularnewline
\hline 
 &  & None & None & ROI-pool & None\tabularnewline
 &  &  &  &  windows & \tabularnewline
\hline 
 &  &  &  &  & \tabularnewline
 &  &  &  &  & , \tabularnewline
\hline 
 &  & None &  &  & \tabularnewline
 &  &  &  &  & \tabularnewline
\hline 
 & None &  &  & None (used after ) & \tabularnewline
\hline 
 &  & Same as & Same as & Same as & Same as\tabularnewline
 &  &  &  &  & \tabularnewline
\hline 
 heatmap &  &  &  &  & \tabularnewline
\hline 
 &  &  &  &  & \tabularnewline
\end{tabular}
\par\end{centering}
\end{table*}

\newpage

\begin{figure*}
\begin{centering}
\begin{tabular}{cccccc}


Image &  &   &   &   & Ground truth\tabularnewline
\includegraphics[width=0.3\columnwidth]{figures/seed-random-examples/2007_003872\lyxdot 1orig} & \includegraphics[width=0.3\columnwidth]{figures/seed-random-examples/2007_003872\lyxdot 2torralba} & \includegraphics[width=0.3\columnwidth]{figures/seed-random-examples/2007_003872\lyxdot 3lampertsmodel} & \includegraphics[width=0.3\columnwidth]{figures/seed-random-examples/2007_003872\lyxdot 5propgap-fov-} & \includegraphics[width=0.3\columnwidth]{figures/seed-random-examples/2007_003872\lyxdot 4deeplabgap} & \includegraphics[width=0.3\columnwidth]{figures/seed-random-examples/2007_003872\lyxdot 2gt}\tabularnewline
\includegraphics[width=0.3\columnwidth]{figures/seed-random-examples/2007_009458\lyxdot 1orig} & \includegraphics[width=0.3\columnwidth]{figures/seed-random-examples/2007_009458\lyxdot 2torralba} & \includegraphics[width=0.3\columnwidth]{figures/seed-random-examples/2007_009458\lyxdot 3lampertsmodel} & \includegraphics[width=0.3\columnwidth]{figures/seed-random-examples/2007_009458\lyxdot 5propgap-fov-} & \includegraphics[width=0.3\columnwidth]{figures/seed-random-examples/2007_009458\lyxdot 4deeplabgap} & \includegraphics[width=0.3\columnwidth]{figures/seed-random-examples/2007_009458\lyxdot 2gt}\tabularnewline
\includegraphics[width=0.3\columnwidth]{figures/seed-random-examples/2008_000391\lyxdot 1orig} & \includegraphics[width=0.3\columnwidth]{figures/seed-random-examples/2008_000391\lyxdot 2torralba} & \includegraphics[width=0.3\columnwidth]{figures/seed-random-examples/2008_000391\lyxdot 3lampertsmodel} & \includegraphics[width=0.3\columnwidth]{figures/seed-random-examples/2008_000391\lyxdot 5propgap-fov-} & \includegraphics[width=0.3\columnwidth]{figures/seed-random-examples/2008_000391\lyxdot 4deeplabgap} & \includegraphics[width=0.3\columnwidth]{figures/seed-random-examples/2008_000391\lyxdot 2gt}\tabularnewline
\includegraphics[width=0.3\columnwidth]{figures/seed-random-examples/2008_005691\lyxdot 1orig} & \includegraphics[width=0.3\columnwidth]{figures/seed-random-examples/2008_005691\lyxdot 2torralba} & \includegraphics[width=0.3\columnwidth]{figures/seed-random-examples/2008_005691\lyxdot 3lampertsmodel} & \includegraphics[width=0.3\columnwidth]{figures/seed-random-examples/2008_005691\lyxdot 5propgap-fov-} & \includegraphics[width=0.3\columnwidth]{figures/seed-random-examples/2008_005691\lyxdot 4deeplabgap} & \includegraphics[width=0.3\columnwidth]{figures/seed-random-examples/2008_005691\lyxdot 2gt}\tabularnewline
\includegraphics[width=0.3\columnwidth]{figures/seed-random-examples/2008_006722\lyxdot 1orig} & \includegraphics[width=0.3\columnwidth]{figures/seed-random-examples/2008_006722\lyxdot 2torralba} & \includegraphics[width=0.3\columnwidth]{figures/seed-random-examples/2008_006722\lyxdot 3lampertsmodel} & \includegraphics[width=0.3\columnwidth]{figures/seed-random-examples/2008_006722\lyxdot 5propgap-fov-} & \includegraphics[width=0.3\columnwidth]{figures/seed-random-examples/2008_006722\lyxdot 4deeplabgap} & \includegraphics[width=0.3\columnwidth]{figures/seed-random-examples/2008_006722\lyxdot 2gt}\tabularnewline
\includegraphics[width=0.3\columnwidth]{figures/seed-random-examples/2009_000136\lyxdot 1orig} & \includegraphics[width=0.3\columnwidth]{figures/seed-random-examples/2009_000136\lyxdot 2torralba} & \includegraphics[width=0.3\columnwidth]{figures/seed-random-examples/2009_000136\lyxdot 3lampertsmodel} & \includegraphics[width=0.3\columnwidth]{figures/seed-random-examples/2009_000136\lyxdot 5propgap-fov-} & \includegraphics[width=0.3\columnwidth]{figures/seed-random-examples/2009_000136\lyxdot 4deeplabgap} & \includegraphics[width=0.3\columnwidth]{figures/seed-random-examples/2009_000136\lyxdot 2gt}\tabularnewline
\includegraphics[width=0.3\columnwidth]{figures/seed-random-examples/2009_004789\lyxdot 1orig} & \includegraphics[width=0.3\columnwidth]{figures/seed-random-examples/2009_004789\lyxdot 2torralba} & \includegraphics[width=0.3\columnwidth]{figures/seed-random-examples/2009_004789\lyxdot 3lampertsmodel} & \includegraphics[width=0.3\columnwidth]{figures/seed-random-examples/2009_004789\lyxdot 5propgap-fov-} & \includegraphics[width=0.3\columnwidth]{figures/seed-random-examples/2009_004789\lyxdot 4deeplabgap} & \includegraphics[width=0.3\columnwidth]{figures/seed-random-examples/2009_004789\lyxdot 2gt}\tabularnewline
\includegraphics[width=0.3\columnwidth]{figures/seed-random-examples/2011_000338\lyxdot 1orig} & \includegraphics[width=0.3\columnwidth]{figures/seed-random-examples/2011_000338\lyxdot 2torralba} & \includegraphics[width=0.3\columnwidth]{figures/seed-random-examples/2011_000338\lyxdot 3lampertsmodel} & \includegraphics[width=0.3\columnwidth]{figures/seed-random-examples/2011_000338\lyxdot 5propgap-fov-} & \includegraphics[width=0.3\columnwidth]{figures/seed-random-examples/2011_000338\lyxdot 4deeplabgap} & \includegraphics[width=0.3\columnwidth]{figures/seed-random-examples/2011_000338\lyxdot 2gt}\tabularnewline
 &  &  &  &  & \tabularnewline
\end{tabular}
\par\end{centering}
\caption{\label{fig:GAP-qualitative}Qualitative examples of GAP output for
, ,
, and .
Note that all of them, except for ,
are qualitatively similar. For ,
we observe repeating patterns of certain stride. Examples are chosen
at random.}
\end{figure*}

\begin{figure*}
\begin{centering}
\begin{tabular}{ccc|ccc}
Salient objects with boxes & Saliency model result &  &  & Salient objects with boxes & Saliency model result\tabularnewline
 &  &  &  &  & \tabularnewline
 &  &  &  &  & \tabularnewline
\includegraphics[width=0.4\columnwidth]{figures/msra-random-examples/9_9737\lyxdot box} & \includegraphics[width=0.4\columnwidth]{figures/msra-random-examples/9_9737\lyxdot sal} &  &  & \includegraphics[width=0.4\columnwidth]{figures/msra-random-examples/126_126297\lyxdot box} & \includegraphics[width=0.4\columnwidth]{figures/msra-random-examples/126_126297\lyxdot sal}\tabularnewline
\includegraphics[width=0.4\columnwidth]{figures/msra-random-examples/208_208145\lyxdot box} & \includegraphics[width=0.4\columnwidth]{figures/msra-random-examples/208_208145\lyxdot sal} &  &  & \includegraphics[width=0.4\columnwidth]{figures/msra-random-examples/129_129283\lyxdot box} & \includegraphics[width=0.4\columnwidth]{figures/msra-random-examples/129_129283\lyxdot sal}\tabularnewline
\includegraphics[width=0.4\columnwidth]{figures/msra-random-examples/46_46674\lyxdot box} & \includegraphics[width=0.4\columnwidth]{figures/msra-random-examples/46_46674\lyxdot sal} &  &  & \includegraphics[width=0.4\columnwidth]{figures/msra-random-examples/130_130855\lyxdot box} & \includegraphics[width=0.4\columnwidth]{figures/msra-random-examples/130_130855\lyxdot sal}\tabularnewline
\includegraphics[width=0.4\columnwidth]{figures/msra-random-examples/45_45762\lyxdot box} & \includegraphics[width=0.4\columnwidth]{figures/msra-random-examples/45_45762\lyxdot sal} &  &  & \includegraphics[width=0.4\columnwidth]{figures/msra-random-examples/161_161904\lyxdot box} & \includegraphics[width=0.4\columnwidth]{figures/msra-random-examples/161_161904\lyxdot sal}\tabularnewline
\includegraphics[width=0.4\columnwidth]{figures/msra-random-examples/109_109870\lyxdot box} & \includegraphics[width=0.4\columnwidth]{figures/msra-random-examples/109_109870\lyxdot sal} &  &  & \includegraphics[width=0.4\columnwidth]{figures/msra-random-examples/185_185635\lyxdot box} & \includegraphics[width=0.4\columnwidth]{figures/msra-random-examples/185_185635\lyxdot sal}\tabularnewline
\includegraphics[width=0.4\columnwidth]{figures/msra-random-examples/114_114522\lyxdot box} & \includegraphics[width=0.4\columnwidth]{figures/msra-random-examples/114_114522\lyxdot sal} &  &  & \includegraphics[width=0.4\columnwidth]{figures/msra-random-examples/190_190869\lyxdot box} & \includegraphics[width=0.4\columnwidth]{figures/msra-random-examples/190_190869\lyxdot sal}\tabularnewline
 &  & \multicolumn{1}{c}{} &  &  & \tabularnewline
\end{tabular}
\par\end{centering}
\caption{\label{fig:MSRA-saliency-1}Extension of figure \ref{fig:MSRA-saliency} in the main paper.
Examples of saliency results on its training data. We use MSRA box
annotations to train a weakly supervised saliency model. Note that
the MSRA subset employed is not biased towards the Pascal categories.
Examples are chosen at random.}
\end{figure*}

\begin{figure*}
\begin{centering}
\begin{tabular}{ccc|ccc}
\includegraphics[width=0.4\columnwidth]{figures/sal-random-examples/2011_000618\lyxdot orig} & \includegraphics[width=0.4\columnwidth]{figures/sal-random-examples/2011_000618\lyxdot overlay} &  &  & \includegraphics[width=0.4\columnwidth]{figures/sal-random-examples/2008_005812\lyxdot orig} & \includegraphics[width=0.4\columnwidth]{figures/sal-random-examples/2008_005812\lyxdot overlay}\tabularnewline
\includegraphics[width=0.4\columnwidth]{figures/sal-random-examples/2008_000149\lyxdot orig} & \includegraphics[width=0.4\columnwidth]{figures/sal-random-examples/2008_000149\lyxdot overlay} &  &  & \includegraphics[width=0.4\columnwidth]{figures/sal-random-examples/2010_002623\lyxdot orig} & \includegraphics[width=0.4\columnwidth]{figures/sal-random-examples/2010_002623\lyxdot overlay}\tabularnewline
\includegraphics[width=0.4\columnwidth]{figures/sal-random-examples/2008_002239\lyxdot orig} & \includegraphics[width=0.4\columnwidth]{figures/sal-random-examples/2008_002239\lyxdot overlay} &  &  & \includegraphics[width=0.4\columnwidth]{figures/sal-random-examples/2010_003418\lyxdot orig} & \includegraphics[width=0.4\columnwidth]{figures/sal-random-examples/2010_003418\lyxdot overlay}\tabularnewline
\includegraphics[width=0.4\columnwidth]{figures/sal-random-examples/2010_004815\lyxdot orig} & \includegraphics[width=0.4\columnwidth]{figures/sal-random-examples/2010_004815\lyxdot overlay} &  &  & \includegraphics[width=0.4\columnwidth]{figures/sal-random-examples/2010_004056\lyxdot orig} & \includegraphics[width=0.4\columnwidth]{figures/sal-random-examples/2010_004056\lyxdot overlay}\tabularnewline
\includegraphics[width=0.4\columnwidth]{figures/sal-random-examples/2009_004867\lyxdot orig} & \includegraphics[width=0.4\columnwidth]{figures/sal-random-examples/2009_004867\lyxdot overlay} &  &  & \includegraphics[width=0.4\columnwidth]{figures/sal-random-examples/2010_001403\lyxdot orig} & \includegraphics[width=0.4\columnwidth]{figures/sal-random-examples/2010_001403\lyxdot overlay}\tabularnewline
\includegraphics[width=0.4\columnwidth]{figures/sal-random-examples/2007_006046\lyxdot orig} & \includegraphics[width=0.4\columnwidth]{figures/sal-random-examples/2007_006046\lyxdot overlay} &  &  & \includegraphics[width=0.4\columnwidth]{figures/sal-random-examples/2011_001060\lyxdot orig} & \includegraphics[width=0.4\columnwidth]{figures/sal-random-examples/2011_001060\lyxdot overlay}\tabularnewline
 &  & \multicolumn{1}{c}{} &  &  & \tabularnewline
\end{tabular}
\par\end{centering}
\caption{\label{fig:saliency-qualitative-1}Extension of figure \ref{fig:saliency-examples} in the main
paper. Example of saliency results on Pascal images. We note that
the saliency often fails when the central, salient objects are non-Pascal
or when the scene is cluttered. Examples are chosen at random.}
\end{figure*}

\begin{figure*}
\begin{centering}
\begin{tabular}{ccccccc}
Image & Seeds & Saliency &  &  &  & Ground truth\tabularnewline
\includegraphics[width=0.25\columnwidth]{figures/supp-guide-strategies/2008_003761\lyxdot 1orig} & \includegraphics[width=0.25\columnwidth]{figures/supp-guide-strategies/2008_003761\lyxdot 3gap} & \includegraphics[width=0.25\columnwidth]{figures/supp-guide-strategies/2008_003761\lyxdot 4sal} & \includegraphics[width=0.25\columnwidth]{figures/supp-guide-strategies/2008_003761\lyxdot 5gv0} & \includegraphics[width=0.25\columnwidth]{figures/supp-guide-strategies/2008_003761\lyxdot 6gv1} & \includegraphics[width=0.25\columnwidth]{figures/supp-guide-strategies/2008_003761\lyxdot 7gv2} & \includegraphics[width=0.25\columnwidth]{figures/supp-guide-strategies/2008_003761\lyxdot 2gt}\tabularnewline
\includegraphics[width=0.25\columnwidth]{figures/supp-guide-strategies/2008_005451\lyxdot 1orig} & \includegraphics[width=0.25\columnwidth]{figures/supp-guide-strategies/2008_005451\lyxdot 3gap} & \includegraphics[width=0.25\columnwidth]{figures/supp-guide-strategies/2008_005451\lyxdot 4sal} & \includegraphics[width=0.25\columnwidth]{figures/supp-guide-strategies/2008_005451\lyxdot 5gv0} & \includegraphics[width=0.25\columnwidth]{figures/supp-guide-strategies/2008_005451\lyxdot 6gv1} & \includegraphics[width=0.25\columnwidth]{figures/supp-guide-strategies/2008_005451\lyxdot 7gv2} & \includegraphics[width=0.25\columnwidth]{figures/supp-guide-strategies/2008_005451\lyxdot 2gt}\tabularnewline
\includegraphics[width=0.25\columnwidth]{figures/supp-guide-strategies/2008_007748\lyxdot 1orig} & \includegraphics[width=0.25\columnwidth]{figures/supp-guide-strategies/2008_007748\lyxdot 3gap} & \includegraphics[width=0.25\columnwidth]{figures/supp-guide-strategies/2008_007748\lyxdot 4sal} & \includegraphics[width=0.25\columnwidth]{figures/supp-guide-strategies/2008_007748\lyxdot 5gv0} & \includegraphics[width=0.25\columnwidth]{figures/supp-guide-strategies/2008_007748\lyxdot 6gv1} & \includegraphics[width=0.25\columnwidth]{figures/supp-guide-strategies/2008_007748\lyxdot 7gv2} & \includegraphics[width=0.25\columnwidth]{figures/supp-guide-strategies/2008_007748\lyxdot 2gt}\tabularnewline
\includegraphics[width=0.25\columnwidth]{figures/supp-guide-strategies/2009_001577\lyxdot 1orig} & \includegraphics[width=0.25\columnwidth]{figures/supp-guide-strategies/2009_001577\lyxdot 3gap} & \includegraphics[width=0.25\columnwidth]{figures/supp-guide-strategies/2009_001577\lyxdot 4sal} & \includegraphics[width=0.25\columnwidth]{figures/supp-guide-strategies/2009_001577\lyxdot 5gv0} & \includegraphics[width=0.25\columnwidth]{figures/supp-guide-strategies/2009_001577\lyxdot 6gv1} & \includegraphics[width=0.25\columnwidth]{figures/supp-guide-strategies/2009_001577\lyxdot 7gv2} & \includegraphics[width=0.25\columnwidth]{figures/supp-guide-strategies/2009_001577\lyxdot 2gt}\tabularnewline
\includegraphics[width=0.25\columnwidth]{figures/supp-guide-strategies/2009_003509\lyxdot 1orig} & \includegraphics[width=0.25\columnwidth]{figures/supp-guide-strategies/2009_003509\lyxdot 3gap} & \includegraphics[width=0.25\columnwidth]{figures/supp-guide-strategies/2009_003509\lyxdot 4sal} & \includegraphics[width=0.25\columnwidth]{figures/supp-guide-strategies/2009_003509\lyxdot 5gv0} & \includegraphics[width=0.25\columnwidth]{figures/supp-guide-strategies/2009_003509\lyxdot 6gv1} & \includegraphics[width=0.25\columnwidth]{figures/supp-guide-strategies/2009_003509\lyxdot 7gv2} & \includegraphics[width=0.25\columnwidth]{figures/supp-guide-strategies/2009_003509\lyxdot 2gt}\tabularnewline
\includegraphics[width=0.25\columnwidth]{figures/supp-guide-strategies/2010_005098\lyxdot 1orig} & \includegraphics[width=0.25\columnwidth]{figures/supp-guide-strategies/2010_005098\lyxdot 3gap} & \includegraphics[width=0.25\columnwidth]{figures/supp-guide-strategies/2010_005098\lyxdot 4sal} & \includegraphics[width=0.25\columnwidth]{figures/supp-guide-strategies/2010_005098\lyxdot 5gv0} & \includegraphics[width=0.25\columnwidth]{figures/supp-guide-strategies/2010_005098\lyxdot 6gv1} & \includegraphics[width=0.25\columnwidth]{figures/supp-guide-strategies/2010_005098\lyxdot 7gv2} & \includegraphics[width=0.25\columnwidth]{figures/supp-guide-strategies/2010_005098\lyxdot 2gt}\tabularnewline
\includegraphics[width=0.25\columnwidth]{figures/supp-guide-strategies/2010_006009\lyxdot 1orig} & \includegraphics[width=0.25\columnwidth]{figures/supp-guide-strategies/2010_006009\lyxdot 3gap} & \includegraphics[width=0.25\columnwidth]{figures/supp-guide-strategies/2010_006009\lyxdot 4sal} & \includegraphics[width=0.25\columnwidth]{figures/supp-guide-strategies/2010_006009\lyxdot 5gv0} & \includegraphics[width=0.25\columnwidth]{figures/supp-guide-strategies/2010_006009\lyxdot 6gv1} & \includegraphics[width=0.25\columnwidth]{figures/supp-guide-strategies/2010_006009\lyxdot 7gv2} & \includegraphics[width=0.25\columnwidth]{figures/supp-guide-strategies/2010_006009\lyxdot 2gt}\tabularnewline
\includegraphics[width=0.25\columnwidth]{figures/supp-guide-strategies/2011_000428\lyxdot 1orig} & \includegraphics[width=0.25\columnwidth]{figures/supp-guide-strategies/2011_000428\lyxdot 3gap} & \includegraphics[width=0.25\columnwidth]{figures/supp-guide-strategies/2011_000428\lyxdot 4sal} & \includegraphics[width=0.25\columnwidth]{figures/supp-guide-strategies/2011_000428\lyxdot 5gv0} & \includegraphics[width=0.25\columnwidth]{figures/supp-guide-strategies/2011_000428\lyxdot 6gv1} & \includegraphics[width=0.25\columnwidth]{figures/supp-guide-strategies/2011_000428\lyxdot 7gv2} & \includegraphics[width=0.25\columnwidth]{figures/supp-guide-strategies/2011_000428\lyxdot 2gt}\tabularnewline
 &  &  &  &  &  & \tabularnewline
\end{tabular}
\par\end{centering}
\caption{\label{fig:guide-strategies}Extension of figure \ref{fig:combination-strategies-examples} in the main paper.
Example results for three different guide labelling strategies, ,
, and . The image, its image labels,
seeds, and saliency map are their input. White labels indicate ``ignore''
regions. Note that  and  give
qualitatively similar results, while  produces much
more precise labelling by exploiting rich localisation information
from the seeds. Examples are chosen at random.}
\end{figure*}

\begin{figure*}
\begin{centering}
\begin{tabular}{ccccc}
\begin{turn}{90}
Input image
\end{turn} & \includegraphics[width=0.4\columnwidth]{figures/supp-final-examples/2009_000027\lyxdot 1orig} & \includegraphics[width=0.4\columnwidth]{figures/supp-final-examples/2008_003764\lyxdot 1orig} & \includegraphics[width=0.4\columnwidth]{figures/main_method_examples/2008_000833\lyxdot 1orig} & \includegraphics[width=0.4\columnwidth]{figures/main_method_examples/2011_002543\lyxdot 1orig}\tabularnewline
\begin{turn}{90}
Seeds\hspace*{-3em}
\end{turn} & \includegraphics[width=0.4\columnwidth]{figures/supp-final-examples/2009_000027\lyxdot 3gap} & \includegraphics[width=0.4\columnwidth]{figures/supp-final-examples/2008_003764\lyxdot 3gap} & \includegraphics[width=0.4\columnwidth]{figures/main_method_examples/2008_000833\lyxdot 3gap} & \includegraphics[width=0.4\columnwidth]{figures/main_method_examples/2011_002543\lyxdot 3gap}\tabularnewline
\begin{turn}{90}
Saliency\hspace*{-3em}
\end{turn} & \includegraphics[width=0.4\columnwidth]{figures/supp-final-examples/2009_000027\lyxdot 4sal} & \includegraphics[width=0.4\columnwidth]{figures/supp-final-examples/2008_003764\lyxdot 4sal} & \includegraphics[width=0.4\columnwidth]{figures/main_method_examples/2008_000833\lyxdot 4sal} & \includegraphics[width=0.4\columnwidth]{figures/main_method_examples/2011_002543\lyxdot 4sal}\tabularnewline
\begin{turn}{90}
\begin{tabular}{c}
\tabularnewline
{\small{}(Seeds+Saliency)}\tabularnewline
\end{tabular}
\end{turn} & \includegraphics[width=0.4\columnwidth]{figures/supp-final-examples/2009_000027\lyxdot 5v6} & \includegraphics[width=0.4\columnwidth]{figures/supp-final-examples/2008_003764\lyxdot 5v6} & \includegraphics[width=0.4\columnwidth]{figures/main_method_examples/2008_000833\lyxdot 5v6} & \includegraphics[width=0.4\columnwidth]{figures/main_method_examples/2011_002543\lyxdot 5v6}\tabularnewline
\begin{turn}{90}
\begin{tabular}{c}
Segmenter\tabularnewline
output\tabularnewline
\end{tabular}
\end{turn} & \includegraphics[width=0.4\columnwidth]{figures/supp-final-examples/2009_000027\lyxdot 6deeplab} & \includegraphics[width=0.4\columnwidth]{figures/supp-final-examples/2008_003764\lyxdot 6deeplab} & \includegraphics[width=0.4\columnwidth]{figures/main_method_examples/2008_000833\lyxdot 6deeplab} & \includegraphics[width=0.4\columnwidth]{figures/main_method_examples/2011_002543\lyxdot 6deeplab}\tabularnewline
\begin{turn}{90}
+CRF\hspace*{-3em}
\end{turn} & \includegraphics[width=0.4\columnwidth]{figures/supp-final-examples/2009_000027\lyxdot 7pcrf} & \includegraphics[width=0.4\columnwidth]{figures/supp-final-examples/2008_003764\lyxdot 7pcrf} & \includegraphics[width=0.4\columnwidth]{figures/main_method_examples/2008_000833\lyxdot 7pcrf} & \includegraphics[width=0.4\columnwidth]{figures/main_method_examples/2011_002543\lyxdot 7pcrf}\tabularnewline
\begin{turn}{90}
Ground truth
\end{turn} & \includegraphics[width=0.4\columnwidth]{figures/supp-final-examples/2009_000027\lyxdot 2gt} & \includegraphics[width=0.4\columnwidth]{figures/supp-final-examples/2008_003764\lyxdot 2gt} & \includegraphics[width=0.4\columnwidth]{figures/main_method_examples/2008_000833\lyxdot 2gt} & \includegraphics[width=0.4\columnwidth]{figures/main_method_examples/2011_002543\lyxdot 2gt}\tabularnewline
 &  &  &  & \tabularnewline
\end{tabular}
\par\end{centering}
\caption{\label{fig:qualitative-examples-1}Extension of figure \ref{fig:qualitative-examples} in the main
paper. Qualitative examples of the different stages of the Guided
Segmentation system on the training images. White labels are ``ignore''
regions. Seeds have high precision and low recall; combined with saliency
foreground mask using  guide labeller, object extents
are recovered. The generated guide labelling can still be noisy; however,
the segmenter convnet can average out the noise to produce more precise
predictions. CRF post-processing further refines the predictions.}
\end{figure*}

\begin{figure*}
\begin{centering}
\begin{tabular}{cccccc}
\begin{turn}{90}
Input image
\end{turn} & \includegraphics[height=0.35\columnwidth]{figures/supp-final-examples/2008_006104\lyxdot 1orig} & \includegraphics[height=0.35\columnwidth]{figures/supp-final-examples/2009_002362\lyxdot 1orig} & \includegraphics[height=0.35\columnwidth]{figures/supp-final-examples/2010_002269\lyxdot 1orig} & \includegraphics[height=0.35\columnwidth]{figures/supp-final-examples/2010_004594\lyxdot 1orig} & \includegraphics[height=0.35\columnwidth]{figures/supp-final-examples/2010_001674\lyxdot 1orig}\tabularnewline
\begin{turn}{90}
Seeds\hspace*{-3em}
\end{turn} & \includegraphics[height=0.35\columnwidth]{figures/supp-final-examples/2008_006104\lyxdot 3gap} & \includegraphics[height=0.35\columnwidth]{figures/supp-final-examples/2009_002362\lyxdot 3gap} & \includegraphics[height=0.35\columnwidth]{figures/supp-final-examples/2010_002269\lyxdot 3gap} & \includegraphics[height=0.35\columnwidth]{figures/supp-final-examples/2010_004594\lyxdot 3gap} & \includegraphics[height=0.35\columnwidth]{figures/supp-final-examples/2010_001674\lyxdot 3gap}\tabularnewline
\begin{turn}{90}
Saliency\hspace*{-3em}
\end{turn} & \includegraphics[height=0.35\columnwidth]{figures/supp-final-examples/2008_006104\lyxdot 4sal} & \includegraphics[height=0.35\columnwidth]{figures/supp-final-examples/2009_002362\lyxdot 4sal} & \includegraphics[height=0.35\columnwidth]{figures/supp-final-examples/2010_002269\lyxdot 4sal} & \includegraphics[height=0.35\columnwidth]{figures/supp-final-examples/2010_004594\lyxdot 4sal} & \includegraphics[height=0.35\columnwidth]{figures/supp-final-examples/2010_001674\lyxdot 4sal}\tabularnewline
\begin{turn}{90}
\begin{tabular}{c}
\tabularnewline
{\small{}(Seeds+Saliency)}\tabularnewline
\end{tabular}
\end{turn} & \includegraphics[height=0.35\columnwidth]{figures/supp-final-examples/2008_006104\lyxdot 5v6} & \includegraphics[height=0.35\columnwidth]{figures/supp-final-examples/2009_002362\lyxdot 5v6} & \includegraphics[height=0.35\columnwidth]{figures/supp-final-examples/2010_002269\lyxdot 5v6} & \includegraphics[height=0.35\columnwidth]{figures/supp-final-examples/2010_004594\lyxdot 5v6} & \includegraphics[height=0.35\columnwidth]{figures/supp-final-examples/2010_001674\lyxdot 5v6}\tabularnewline
\begin{turn}{90}
\begin{tabular}{c}
Segmenter\tabularnewline
output\tabularnewline
\end{tabular}
\end{turn} & \includegraphics[height=0.35\columnwidth]{figures/supp-final-examples/2008_006104\lyxdot 6deeplab} & \includegraphics[height=0.35\columnwidth]{figures/supp-final-examples/2009_002362\lyxdot 6deeplab} & \includegraphics[height=0.35\columnwidth]{figures/supp-final-examples/2010_002269\lyxdot 6deeplab} & \includegraphics[height=0.35\columnwidth]{figures/supp-final-examples/2010_004594\lyxdot 6deeplab} & \includegraphics[height=0.35\columnwidth]{figures/supp-final-examples/2010_001674\lyxdot 6deeplab}\tabularnewline
\begin{turn}{90}
+CRF\hspace*{-3em}
\end{turn} & \includegraphics[height=0.35\columnwidth]{figures/supp-final-examples/2008_006104\lyxdot 7pcrf} & \includegraphics[height=0.35\columnwidth]{figures/supp-final-examples/2009_002362\lyxdot 7pcrf} & \includegraphics[height=0.35\columnwidth]{figures/supp-final-examples/2010_002269\lyxdot 7pcrf} & \includegraphics[height=0.35\columnwidth]{figures/supp-final-examples/2010_004594\lyxdot 7pcrf} & \includegraphics[height=0.35\columnwidth]{figures/supp-final-examples/2010_001674\lyxdot 7pcrf}\tabularnewline
\begin{turn}{90}
Ground truth
\end{turn} & \includegraphics[height=0.35\columnwidth]{figures/supp-final-examples/2008_006104\lyxdot 2gt} & \includegraphics[height=0.35\columnwidth]{figures/supp-final-examples/2009_002362\lyxdot 2gt} & \includegraphics[height=0.35\columnwidth]{figures/supp-final-examples/2010_002269\lyxdot 2gt} & \includegraphics[height=0.35\columnwidth]{figures/supp-final-examples/2010_004594\lyxdot 2gt} & \includegraphics[height=0.35\columnwidth]{figures/supp-final-examples/2010_001674\lyxdot 2gt}\tabularnewline
 &  &  &  &  & \tabularnewline
\end{tabular}
\par\end{centering}
\caption{\label{fig:qualitative-examples-2}Extension of figure \ref{fig:qualitative-examples} in the main
paper. More qualitative examples of the different stages of the Guided
Segmentation system on the training images. White labels are ``ignore''
regions. Seeds have high precision and low recall; combined with saliency
foreground mask using  guide labeller, object extents
are recovered. The generated guide labelling can still be noisy; however,
the segmenter convnet can average out the noise to produce more precise
predictions. CRF post-processing further refines the predictions.}
\end{figure*}


\end{document}
