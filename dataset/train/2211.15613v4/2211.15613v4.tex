\pdfoutput=1


\documentclass[11pt,dvipsnames]{article}
\setcounter{secnumdepth}{6}

\usepackage{arabtex}
\usepackage{utf8}
\setcode{utf8}
\usepackage[]{ACL2023}

\usepackage{times}
\usepackage{latexsym}

\usepackage[T1]{fontenc}


\usepackage[utf8]{inputenc}
\usepackage{CJKutf8}


\usepackage{microtype}

\usepackage{inconsolata}


\usepackage{graphicx}

\newcommand*{\img}[1]{\raisebox{-.19\baselineskip}{\includegraphics[
        height=0.9\baselineskip,
        width=0.9\baselineskip,
        keepaspectratio,
        ]{#1}}}


\usepackage{times}
\usepackage{latexsym}
\usepackage{comment}
\usepackage{caption}
\usepackage[LGR,T1]{fontenc}


\usepackage{hyperref}
\usepackage{url}
\usepackage{multirow}
\usepackage{physics}
\usepackage{array,booktabs}
\usepackage{multirow}
\usepackage{adjustbox}
\usepackage{tabularx,booktabs}
\newcolumntype{Y}{>{\centering\arraybackslash}X}
\newcolumntype{P}{>{\raggedleft\arraybackslash}p{.5cm}}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{enumitem}
\usepackage{xcolor}\usepackage{colortbl,hhline}
\usepackage[utf8]{inputenc}
\usepackage{soul}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{placeins}
\usepackage[bottom]{footmisc}


\usepackage{inconsolata}
\newcommand{\ar}[1]{\textcolor{brown}{\bf\small [#1 --AR]}}


\usepackage[colorinlistoftodos]{todonotes}
\newcommand{\ycinline}[1]{\todo[color=blue!50,size=\tiny]{#1 --YC}}
\newcommand{\cj}[1]{\todo[color=teal!50,size=\tiny]{#1 --CJ}}
\newcommand{\cjinline}[1]{\textcolor{teal}{\bf\small [#1 --CJ]}}

\newcommand{\wx}[1]{\todo[color=magenta!50,size=\tiny]{#1 --WX}}



\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\definecolor{green1}{HTML}{D4EFDF}
\definecolor{green2}{HTML}{A9DFBF}
\definecolor{green3}{HTML}{7DCEA0}
\definecolor{green4}{HTML}{52BE80}
\definecolor{red1}{HTML}{FDEDEC}
\definecolor{red2}{HTML}{FADBD8}
\definecolor{red3}{HTML}{F5B7B1}
\definecolor{red4}{HTML}{F1948A}


\usepackage{tikz,graphics,color,fullpage,float,epsf,caption,subcaption}






\title{Frustratingly Easy Label Projection for Cross-lingual Transfer}




\author{Yang Chen, Chao Jiang,  Alan Ritter, Wei Xu \\
  Georgia Institute of Technology \\
 \texttt{\{yang.chen, chao.jiang, alan.ritter, wei.xu\}@cc.gatech.edu} 
\\}


\begin{document}
\maketitle
\begin{abstract}


Translating training data into many languages has emerged as a practical solution for improving cross-lingual transfer. For tasks that involve span-level annotations, such as information extraction or question answering, an additional label projection step is required to map annotated spans onto the translated texts. Recently, a few efforts have utilized a simple mark-then-translate method to jointly perform translation and projection  by inserting special markers around the labeled spans in the original sentence \cite{lewis2020mlqa, hu2020xtreme}. However, as far as we are aware, no empirical analysis has been conducted on how this approach compares to traditional annotation projection based on word alignment. In this paper, we present an extensive empirical study across 57 languages and three  tasks (QA, NER, and Event Extraction) to evaluate the effectiveness and limitations of both methods, filling an important gap in the literature.  Experimental results show that our optimized version of mark-then-translate, which we call EasyProject,  is easily applied to many languages and works surprisingly well, outperforming the more complex word alignment-based methods. We analyze several key factors that affect the end-task performance, and show  EasyProject works well because it can accurately preserve label span boundaries after translation.~\footnote{Our code and data is available at:~\url{https://github.com/edchengg/easyproject}}

\end{abstract}

\section{Introduction}

Zero-shot cross-lingual transfer, where models trained on a source language (e.g., English) are directly applied to other target languages, has the potential to extend NLP systems to many languages \cite{nooralahzadeh-etal-2020-zero, keung-etal-2020-dont,chen2021model, niu-etal-2022-onealigner, huang-etal-2022-multilingual-generative}. Yet, its performance still lags behind models that are directly fine-tuned on labeled data (if available) from the target language.
Recent work has shown that combining training data in a source language together with its automatic translation to the target language leads to consistent performance improvements \cite{xue2021mt5,hu2020xtreme}.
However, for NLP tasks that involve span-level annotations, an additional label projection step is needed to map the span annotations onto the translated texts (see Figure \ref{fig:EasyProject}).


 
Traditionally, this annotation projection step is performed based on word alignment after machine translation \cite{akbik2015generating, aminian-etal-2019-cross}. To avoid the use of complex word alignment models, several recent efforts \cite{lewis2020mlqa, hu2020xtreme} directly translated sentences with span annotations wrapped between special markers (e.g., {\tt <a>} and {\tt </a>}). However, due to limited analysis presented in prior work, it is unclear (1) how well this approach works across different language families,  (2) how robust  MT systems are in handling special markers, as inserting  markers inevitably degrades the translation quality,   and (3) how well marker-based projection works in comparison to traditional alignment-based methods.























\begin{figure*}[ht!]
    \centering
    \vspace{-5pt}
\includegraphics[width=0.97\textwidth]{fig/easyproject_v11.pdf}
    \vspace{-6.5pt}
    \caption{Two methods for translating and projecting English ACE event triggers and named entities to Chinese. (a) Pipeline method based on  word alignment: starting with machine translation of the English sentence to Chinese, followed by word-to-word alignment. Then, labeled spans are projected using heuristics. (b) Mark-then-translate: markers are inserted around the  annotated spans in the text. The modified sentence with markers inserted is then fed as input to an MT system, projecting the label span markers to the target sentence as a byproduct of translation.}
\label{fig:EasyProject}
\vspace{-15.5pt}
\end{figure*}




In this paper, we present the first systematic study of the mark-then-translate annotation projection technique,  which includes careful evaluation of the choice of markers, projection accuracy, impact on translation quality, robustness to different MT systems, as well as a comparison to traditional alignment-based method across 57 languages (including 18 from Africa)   on 5 datasets and 3 NLP tasks. We also propose an improved variant of marker-based projection, \textsc{EasyProject}, that consistently outperforms the alignment-based approach, while being incredibly easy to use to project a variety of annotations (QA, entities, relations, events) across many languages. The key is to use language-agnostic square bracket markers, combined with an efficient fine-tuning strategy to encourage the multilingual MT system to better preserve the  special markers during  translation.








Our main findings include (1) the marker-based method is surprisingly robust across different translation systems and languages, but the choice of markers matters (\ref{sec:choice-of-markers}); (2) EasyProject can project annotated spans more accurately and is better at preserving span boundaries than the  alignment-based methods, which is key to its success (\ref{sec:comparsion_to_alignment}); (3)  fine-tuning an MT system for only 200 steps is sufficient  to improve its robustness in handling special markers during translation (\ref{sec:easyproject_intro}); (4) the margin of improved cross-lingual transfer is related to the language/script family and amount of pre-training data included in the multilingual model (\ref{sec:pre-training-data-size}). 
We hope our work will inspire more research on robust models that better handle text markup for the purpose of generating span annotations.










\section{Background and Related Work}

\paragraph{Alignment-based Projection.}  Projecting annotations via word alignment typically consists of the following steps: machine translate the available training data into the target language; run word alignment tools on the original and translated sentences; and finally, apply heuristics to map the span-level annotations from the original to translated texts. 
Statistical word alignment tools such as  GIZA++~\citep{giza} and fast-align~\citep{dyer2013fast}  have been widely adopted for projecting part-of-speech tags~\citep{yarowsky-etal-2001-inducing,eskander2020unsupervised}, semantic roles~\citep{akbik-etal-2015-srl,aminian-etal-2017-transferring,daza2020xsrl,fei2020srl}, slot filling~\citep{xu2020end}, semantic parser~\citep{moradshahi-etal-2020-localizing,nicosia-etal-2021-translate-fill}, and NER labels~\citep{ni-etal-2017-weakly,stengel-eskin-etal-2019-discriminative}.
Recent progress on supervised neural aligners~\citep{jalili-sabet-etal-2020-simalign,nagata2020supervised,dou2021awesome,lan2021crfalign} and multilingual contextualized embeddings~\citep{devlin2019mbert,conneau2019xlmr} has further improved alignment accuracy.
However, this pipeline-based method suffers from error propagation, translation shift~\citep{akbik-etal-2015-srl}, and    non-contiguous alignments~\citep{zenkel-etal-2020-end}.
Our analysis in \S \ref{sec:comparsion_to_alignment} shows that the  alignment-based methods are  more error-prone when projecting span-level annotations, compared to the marker-based approaches.








\paragraph{Marker-based Projection.} A few efforts have used mark-then-translate label projection method to translate question answering datasets into other languages ~\citep{lee-etal-2018-semi,lewis2020mlqa, hu2020xtreme,bornea2020qa}. However, the focus of these papers was not the label projection task itself and there was no in-depth analysis on the effectiveness of the approach. For instance, \newcite{lewis2020mlqa} used quotation marks to translate the SQuAD training set into other languages but did not present empirical comparison to any other label projection methods. Similarly, \newcite{hu2020xtreme} used XML tags for the same purpose when creating the XTREME dataset, but this was only briefly mentioned in a few sentences in appendix. Besides QA, MulDA \citep{liu-2021-mulda,zhou2022conner} is a labeled sequence translation method that replaces entities with variable names for cross-lingual NER. However, no comparison with existing  projection methods was presented, as the main focus is generating synthetic labeled data  using language models.






















\section{Analysis of Marker-Based Projection}
\label{EasyProject}
The idea of marker-based label projection is straightforward -- wrap labeled spans with special marker tokens, then translate the modified sentence (see an example in Figure \ref{fig:EasyProject}b). The projected spans can be directly decoded from the translation if the markers are retrained. However, inserting markers inevitably degrades the  translation quality.  In this section, we analyze several questions left open by prior work \cite{lewis2020mlqa, hu2020xtreme}, including  (1) how well are the special markers being  preserved in translation, (2) the impact of different marker choices on  translation quality and the performance of cross-lingual transfer.
















\subsection{Experimental Setup}
\label{sec:experiment}

We conduct experiments on three NLP tasks and 57 languages with five multilingual datasets to comprehensively evaluate  the marker-based method. Most multilingual datasets are created  by either (1) directly collecting and annotating data in the target language, or (2) translating English data with human or machine translation and then projecting labels manually or automatically.  Four of our selected datasets  were created with the first method, as evaluation on the translated datasets may over-estimate performance on a target language, when in fact a model might only perform well on  {\em translationese} \citep{riley2020translationese}. 














\paragraph{Datasets.} Our experiments include NER via the WikiANN~\citep{pan2017wikiann,rahimi2019ner} and MasahkaNER 2.0 \cite{adelani2022masakhaner} datasets (\S \ref{sec:comparsion_to_alignment}), in addition to CoNLL-2002/2003 multilingual NER datasets \cite{sang2002conll,sang2003conll} for comparison with \citet{liu-2021-mulda} (\S \ref{sec:mulda}).  
For event extraction, we use the ACE05 corpus \cite{walker2006ace},  which consists of six sub-tasks: entity and relation extraction, event trigger/argument identification and classification.
For QA, we use TyDiQA-GoldP \cite{clark-etal-2020-tydi}, which contains challenging questions written in eight languages.
Data statistics are shown in Table \ref{table:stat} and \ref{table:stat_conll} in Appendix.



\renewcommand{\arraystretch}{1.25}
\begin{table}[t!]
\centering
\small
\resizebox{\linewidth}{!}{
\begin{tabular}{@{\hspace{0.05cm}}l@{\hspace{0.01cm}}c@{\hspace{0.05cm}}c@{\hspace{0.06cm}}c@{\hspace{0.08cm}}c@{\hspace{0.1cm}}}
\toprule
& \textbf{WikiANN} & \textbf{MasakhaNER} & \textbf{ACE05} & \textbf{TyDiQA} \\
\midrule
\# of Lang. & 39 & 20 (from Africa) & 2  & 8  \\ 
\# of Docs & -- & -- & 526/31/40 & 3,696/440/-- \\ 
\# of Sent. & 20k/10k/10k & 4.4k/638/1.2k  & 19k/901/676 & 17k/2,122/-- \\
Avg.  Length & --/8.0 & --/23.9   & 519.3/14.9 & 96.8/21.0 \\
Avg.  \# of Spans & 1.4 & 1.8 & 2.9 & 1.0 \\
\bottomrule
\end{tabular}
}
\vspace{-5pt}
\caption{The detailed statistics of train/dev/test sets for each dataset.  \textbf{Avg. Length} represents the average number of tokens in each article/sentence, and \textbf{Avg.  \# of Spans} denotes the average number of annotated spans in each sentence (in each article for TyDiQA).} 
\vspace{-15pt}
\label{table:stat}
\end{table}







\paragraph{IE and QA Models.} We use  XLM-RoBERTa ~\citep{conneau2019xlmr} as the backbone  model, except where noted.\footnote{We also experiment with mT5, mT5, and mT5~\citep{xue2021mt5} on WikiANN-NER (Table \ref{table:mt5}). }
For NER and QA, we fine-tune XLM-R with standard tagging and SQuAD-style span prediction layers. For event extraction, we use the OneIE framework~\citep{lin2020oneie}, a joint neural model for information extraction with global features. We report average F scores over three runs with different random seeds. More implementation details can be found in Appendix \ref{appendix:implementation_details_IE}.



\paragraph{MT Systems.} We experiment with two MT systems: (1) the Google Translation (GMT) API,\footnote{\url{https://cloud.google.com/translate}} and (2) an open-sourced multilingual  translation model NLLB (No Language Left Behind)~\citep{nllb2022} with 3.3 billion parameters, supporting the translation between any pair of 200 languages.\footnote{\url{https://github.com/facebookresearch/fairseq/tree/nllb}} 






\subsection{Choice of Markers}
\label{sec:choice-of-markers}
Ideally, a good span marker should minimize the impact on translation quality while having a high chance of being preserved during translation. However, prior works used quotation marks (`` '')~\citep{lee-etal-2018-semi,lewis2020mlqa} and XML/HTML tags (e.g., <a> or <PER>)~\citep{hu2020xtreme, ahmad2021gate} without much justification, which we address below.






\paragraph{Preserved in Translation.}  In a pilot study, we experimented with several markers, including XML tags, \texttt{[]}, \text{`` ''}, \texttt{()}, \texttt{<>}, and \text{\{\}}, etc. We found that both MT systems work reasonably well to retain square brackets (\texttt{[]}) and XML markers during the translation across many languages, while other markers that have language-specific formats are easily lost in translation.  For example, quotation marks (`` '') are often translated in a language-specific way, e.g., «» in Russian, and  are sometimes  lost entirely in Arabic and Finnish, leading to low projection rates: 53\% for Russian, 76\% for Arabic, and 79\% for Finnish based on TyDiQA dataset. The \textit{projection rate} is measured by the percentage of data in which the numbers and type of special  markers in the translations match with the source sentences. To improve the robustness of MT system in handling  markers, we found  further fine-tuning the MT system on synthetic data, where the special markers are inserted around  name entities in parallel sentences,  for only 200 steps is sufficient to boost the projection rate while maintaining translation quality (more details in \ref{sec:easyproject_intro}).




\renewcommand{\arraystretch}{1.1}
\begin{table}[b!]
\centering
\small
\vspace{-11pt}
\begin{tabularx}{\linewidth}{llrPPP}
\toprule
\multirow{2}{*}{\textbf{\textit{en}  Lang.}}  & \multirow{2}{*}{\textbf{Corpus}} & \multirow{2}{*}{\# \textbf{sent}} & \multicolumn{3}{c}{GMT - \textbf{BLEU}}\\
\cmidrule{4-6}
&& & Orig. & XML & \texttt{[]}\\
\midrule
Arabic (\textit{ar}) & TED18 & 1,997 & \textbf{20.7} &14.0& \underline{15.1}\\
German (\textit{de}) & TED18 & 1,997 & \textbf{44.5} &33.9 & \underline{41.9}\\
Spanish (\textit{es}) & TED18&1,997 & \textbf{45.9} &34.2 & \underline{35.4} \\
French (\textit{fr}) & TED18&1,997 & \textbf{37.6} &31.0  &\underline{31.9}\\
Hindi (\textit{hi}) & TED18&1,070 & \textbf{14.5} &12.8 & \underline{13.0}\\
Russian (\textit{ru}) & WMT&1,997 & \textbf{36.4} &28.5 & \underline{35.2}\\
Vietnamese (\textit{vi}) & TED18&1,997 & \textbf{32.8} & \underline{28.5} &27.0\\
Chinese (\textit{zh}) & WMT&1,997 & \textbf{40.6}	&33.4 & \underline{37.1}\\
\midrule
AVG &  & 1,881 & \textbf{34.1} &27.0 & \underline{29.6}\\
\bottomrule
\end{tabularx}
\vspace{-5pt}
\caption{Comparsion of translation  quality with different span markers, where the \textbf{best} and \underline{second best} are marked. Overall, square brackets (\texttt{[]}) have less negative impact   compared to XML tags. ``Orig.'' denotes the translation when no marker is inserted.}
\vspace{-5pt}
\label{table:bleu_corpus}
\end{table}



\paragraph{Impact on Translation Quality.} After narrowing down the choices to XML tags and square brackets, we further measure the impact of adding markers on the translation quality by adopting the evaluation setup used by \citet{fan2021beyond}. We compare translation quality, with and without markers inserted, from English to various target languages using  BLEU score. Table~\ref{table:bleu_corpus} presents the experimental results with Google Translation. Examples of errors are shown in Table \ref{tab:translation_error}.  We find that inserting special  markers indeed  degrades translation quality, but overall,  square brackets have less negative impact compared to XML tags.    We hypothesize this is because using \texttt{[]} introduces less number of extra subword tokens  in the encoding and decoding of the text during translation, compared to XML tags. More results on 55 languages using the NLLB  translation system and more details about the evaluation setup can be found in Appendix \ref{appendix:translation-quality-evaluation}. 













\begin{figure*}[t!]
    \centering
    \vspace{-5pt}
    \includegraphics[width=\textwidth]{fig/demo-bleu-f1.pdf}
    \vspace{-25pt}
    \caption{Comparison of translation quality (x-axis) and end-task performance (y-axis) for different label projection methods on the WikiANN dataset using NLLB translation system. EasyProject\img{fig/dot-easyproject.png}(\S\ref{sec:easyproject_intro}) outperforms alignment-based approach\img{fig/dot-awesomealign.png}on F scores for most languages, though inserting  markers degrades translation quality. The experimental setting is detailed in \S \ref{sec:comparsion_to_alignment}.}
    \label{fig:bleu_f1}
    \vspace{-15pt}
\end{figure*}





\begin{CJK*}{UTF8}{gbsn} 
\renewcommand{\arraystretch}{1.1}
\begin{table}[t!]
    \small
    \centering
\resizebox{\linewidth}{!}{
    \begin{tabular}{@{\hspace{0.025cm}}r@{\hspace{0.025cm}}l@{\hspace{0.025cm}}}
    \toprule





       \textbf{English} \textbf{\#1:}   &  The \underline{divorce} settlement called for \underline{Giuliani} to \underline{\smash{pay}} \underline{Hanover}\\
       &more than \\rightarrow_1>_1_1\S_\textit{en}_{\textit{en}}_{\text{large}}_{\text{XL}}_{\text{large}}\rightarrow_{\text{en}}_R_{\text{ft}}\Delta_{\text{XLM}_R}_{\text{ft}}\Delta_{\text{XLM}_R})_{\text{en}}_R_{\text{ft}}\Delta_{\text{XLM}_R})_{\text{ft}}\Delta_{\text{XLM}_R})_{\text{en}}_R_{\text{large}}\Delta\Delta_1_1_1_{\text{large}}_{\text{XL}}_1_{\text{XXL}}_1_1\Delta_1_{\text{en}}_{\text{large}}_1_1_1_1\S\S\S\S\S_{\textit{en}}_{ft}_{\text{base}}_{\text{large}}_{\text{XXL}}_{\text{large}}_1^{\dagger}^{\dagger}\dagger^{+}\times\times\times\times\times_1_1_{\textit{en}}_{\text{large}}_1_{\text{large}}_{100k}_{100k}_1_{100k}_1_1_{\textit{en}}_{\textit{100k}}_{\textit{100k}}_1^{\dagger}_{\text{base}}^{\dagger}_{\text{large}}_1\dagger_{\text{finetune}}_{\text{finetune}}\sim\sim_1_1_1_{\text{large}}_1\times\times_{\text{ft}}_{\text{ft}}_{\text{large}}_{\text{large}}_{\text{XL}}_{\text{XXL}}_1_{\text{large}}_{\text{XXL}}_{\text{large}}_{\text{large}}_{\text{XL}}_{\text{XXL}}$ models. Models are fine-tuned on a combination of English and EasyProject data with Google Translation.}
\label{table:mt5_detail}
\end{table*}
 


\end{document}
