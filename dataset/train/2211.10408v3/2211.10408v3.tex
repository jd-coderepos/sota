\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage[dvipsnames]{xcolor}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{subcaption}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\usepackage[accsupp]{axessibility}
\usepackage{bm}
\usepackage{booktabs}
\usepackage{amsfonts}       
\usepackage{array}
\usepackage{multirow}
\let\checkmark\undefined \usepackage{dingbat}
\usepackage{xspace}

\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}



\iccvfinalcopy
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}





\newcommand{\para}[1]{\noindent{\bf{#1}}}
\newcommand{\PAR}[1]{\noindent{\bf{#1.}}}
\newcommand{\vpara}[1]{\vspace{1mm} \noindent{\bf{#1}}}

\newcommand{\ul}[1]{\underline{#1}}
\newcommand{\croconew}{CroCo~v2\xspace}
\newcommand{\ours}{CroCo-Stereo\xspace}
\newcommand{\oursflow}{CroCo-Flow\xspace}

\begin{document}

\title{\croconew: Improved Cross-view Completion Pre-training \\ 
for Stereo Matching and Optical Flow \vspace{-0.1cm}}

\author{Philippe Weinzaepfel ~~~~~~~~~~~~ Thomas Lucas ~~~~~~~~~~~~ Vincent Leroy ~~~~~~~~~~~~ 
\0.04cm] Leonid Antsfeld ~~~~~~~~~~~~ Boris Chidlovskii ~~~~~~~~~~~~ J\'er\^ome Revaud \-0.05cm]
{\small \url{https://github.com/naver/croco} }
\vspace{-0.4cm}
}


\maketitle



\begin{abstract}
Despite impressive performance for high-level downstream tasks, self-supervised pre-training methods have not yet fully delivered on dense geometric vision tasks such as stereo matching or optical flow.
The application of self-supervised 
concepts, such as instance discrimination or masked image modeling, to geometric tasks is an active area of research.
In this work, we build on the recent cross-view completion framework,
a variation of masked image modeling that leverages a second view from the same scene which makes it well suited for binocular downstream tasks.
The applicability of this concept has so far been limited in at least two ways: (a) by the difficulty of collecting real-world image pairs -- in practice only synthetic data have been used -- 
and (b) by the lack of generalization of vanilla transformers to dense downstream tasks 
for which relative position is more meaningful than absolute position.
We explore three avenues of improvement. 
First, we introduce a method to collect suitable real-world image pairs at large scale.
Second, we experiment with relative positional embeddings and show that they enable vision transformers 
to perform substantially better.
 Third, we scale up vision transformer based cross-completion architectures, which is made possible by the use of large amounts of 
 data.
With these improvements, we show for the first time that state-of-the-art results on 
stereo matching and optical flow can be reached without using any classical task-specific techniques like 
correlation volume, iterative estimation, image warping or multi-scale reasoning, thus paving the way towards universal vision models.
\end{abstract}


\section{Introduction}
\label{sec:intro}

\begin{figure}
\includegraphics[width=\linewidth]{fig/croco_v2_teaser.pdf} \-0.45cm]
\caption{\textbf{Overview of the improvements in \croconew for cross-view completion pre-training:} (a) collecting and using real-world images, (b) using rotary positional embeddings which model \textit{relative} token positions, instead of absolute positions using the standard cosine embedding, (c) increasing network size both in the encoder and the decoder.}
\label{fig:intro}
\vspace{-0.15cm}

\end{figure}


We finetune our pre-trained model, referred to as \croconew, with this improved cross-view completion scheme on stereo matching and optical flow using a Dense Prediction Transformer (DPT)~\cite{dpt} head. 
Our models, termed \ours and \oursflow, are simple and generic: 
we rely on a plain ViT encoder, followed by a plain transformer decoder which 
directly predicts the output (disparity for stereo, or optical flow) through the DPT head.
We believe this is a meaningful step towards a universal vision model, \ie, that can solve numerous vision tasks with a common architecture. In contrast to state-of-the-art methods for stereo matching or optical flow, our architecture does not rely on task-specific designs such as cost volumes~\cite{flowformer,jie2018left,gcnet,stereonet,yin2019hierarchical}, image warping~\cite{brox2004high,pwcnetplus}, iterative refinement~\cite{crestereo,raftstereo,raft} or multi-level feature pyramids~\cite{leastereo,crestereo,pwcnetplus}. While task-specific structures and prior knowledge may yield more data-efficient approaches, they come at the cost of being tailored to a single task. Our proposed pre-training allows us to eschew these and still reaches state-of-the-art performance on various stereo matching and optical flow benchmarks such as KITTI 2015~\cite{kitti15}, ETH3D~\cite{eth3d}, Spring~\cite{spring} or MPI-Sintel~\cite{sintel}.


\begin{figure*}
\centering
\newcommand{\figpairwidth}{0.115\linewidth}

\resizebox{\linewidth}{!}{
\begin{tabular}{@{}l@{~ }p{\linewidth}@{}}
\rotatebox{90}{\small Habitat (synth.)} & 
\includegraphics[width=\figpairwidth]{fig/pairs/Habitat/00000067_00.jpg}
\includegraphics[width=\figpairwidth]{fig/pairs/Habitat/00000067_01.jpg} \hfill
\includegraphics[width=\figpairwidth]{fig/pairs/Habitat/00000044_00.jpg}
\includegraphics[width=\figpairwidth]{fig/pairs/Habitat/00000044_01.jpg} \hfill
\includegraphics[width=\figpairwidth]{fig/pairs/Habitat/00000004_00.jpg}
\includegraphics[width=\figpairwidth]{fig/pairs/Habitat/00000004_01.jpg} \hfill
\includegraphics[width=\figpairwidth]{fig/pairs/Habitat/00000029_00.jpg}
\includegraphics[width=\figpairwidth]{fig/pairs/Habitat/00000029_01.jpg} \\

\rotatebox{90}{\small ~~ARKitScenes} & 
\includegraphics[angle=-90,origin=c,width=\figpairwidth]{fig/pairs/ARKitV2/3a74ce387fdebd_1.jpg}
\includegraphics[angle=-90,origin=c,width=\figpairwidth]{fig/pairs/ARKitV2/3a74ce387fdebd_2.jpg} \hfill
\includegraphics[angle=0,origin=c,width=\figpairwidth]{fig/pairs/ARKitV2/12a0de6c003ca569_1.jpg}
\includegraphics[angle=0,origin=c,width=\figpairwidth]{fig/pairs/ARKitV2/12a0de6c003ca569_2.jpg} \hfill
\includegraphics[angle=0,origin=c,width=\figpairwidth]{fig/pairs/ARKitV2/2a8343f99d79fa05_1.jpg}
\includegraphics[angle=0,origin=c,width=\figpairwidth]{fig/pairs/ARKitV2/2a8343f99d79fa05_2.jpg} \hfill
\includegraphics[angle=-90,origin=c,width=\figpairwidth]{fig/pairs/ARKitV2/7c460e6b8565b6db_1.jpg}
\includegraphics[angle=-90,origin=c,width=\figpairwidth]{fig/pairs/ARKitV2/7c460e6b8565b6db_2.jpg} \-0.1cm]

\rotatebox{90}{\small ~3DStreetView} &
\includegraphics[width=\figpairwidth]{fig/pairs/3Dstreetview/100080be64f58146_1.jpg}
\includegraphics[width=\figpairwidth]{fig/pairs/3Dstreetview/100080be64f58146_2.jpg} \hfill
\includegraphics[width=\figpairwidth]{fig/pairs/3Dstreetview/100640a802215b37_1.jpg}
\includegraphics[width=\figpairwidth]{fig/pairs/3Dstreetview/100640a802215b37_2.jpg} \hfill
\includegraphics[width=\figpairwidth]{fig/pairs/3Dstreetview/11019ee69fdd2397_1.jpg}
\includegraphics[width=\figpairwidth]{fig/pairs/3Dstreetview/11019ee69fdd2397_2.jpg} 
\hfill
\includegraphics[width=\figpairwidth]{fig/pairs/3Dstreetview/6ca79e70ca9ba_1.jpg}
\includegraphics[width=\figpairwidth]{fig/pairs/3Dstreetview/6ca79e70ca9ba_2.jpg} \-0.05cm]
\end{tabular}
}
\caption{\textbf{Example of pre-training cropped image pairs} from Habitat which was the synthetic data used by CroCo~\cite{croco} on the top row, and from real-world datasets we use in this paper (from ARKitScenes, MegaDepth, 3DStreetView and IndoorVL) below.}
\label{fig:pairs}

\vspace{-0.3cm}

\end{figure*}


\section{Related work}
\label{sec:related}

\PAR{Self-supervised learning}
The success of instance discrimination~\cite{swav,dino,simclr,byol,moco} has drawn a lot of attention to self-supervised learning in computer vision~\cite{JingPAMI21SelfSupervisedVisualFeatureLearningDeepSurvey}.
In that paradigm, variants of an image are obtained by applying different data augmentations. Features extracted from the different variants are trained to be similar, while being pushed away from features obtained from other images.
Such self-supervised models are particularly well tailored to image-level tasks, such as image classification, and have led to state-of-the-art performance on various benchmarks.
Recent studies suggest that this success could be due to the object-centric~\cite{purushwalkam2020demystifying} and the balanced~\cite{assran2022hidden} nature of ImageNet~\cite{imagenet} that is used for pre-training.
Recently, inspired by BERT~\cite{BERT} in natural language processing, different masked modeling methods have been adapted to computer vision. 
MIM pre-training aims at reconstructing masked information from an input image either in the pixel space~\cite{sit,multimae,iGPT,splitmask,mae,SimMIM}, or in the feature space~\cite{msn,data2vec,maskfeat}, and sometimes after quantization~\cite{BEiT,iBOT}.
Recent works combine this framework in a teacher-student approach~\cite{lee2022exploring,MST} with improved masking strategy~\cite{CiM,AttMask,MST}.
Overall, MIM models perform well on classification tasks. They have obtained some success on denser tasks such as object detection~\cite{mae} or human pose estimation~\cite{vitpose}, and have been applied to robotic vision~\cite{maerobot} when pre-trained on related datasets.
More recently, CroCo~\cite{croco} introduces the pretext task of cross-view completion, where a second view of the same scene is added to MIM. This is well suited to geometric downstream tasks: to leverage the second view and improve reconstruction accuracy, the model has to implicitly 
be aware of the geometry of the scene. CroCo outperforms MIM pre-training on an array of geometric tasks. However, it relies on synthetic data only, which may be sub-optimal, and does not reach the performance of the best task-specific methods.


\PAR{Positional embeddings} 
Since a ViT treats its input as an orderless set of image patches or tokens, positional embeddings are a necessary tool to keep track of the position of each patch token from the original image.
They can be either learned~\cite{dino,vit} or handcrafted, such as the cosine positional embeddings from the original transformer~\cite{attn}.
Both learned and cosine embeddings are added explicitly to the signal and contain absolute positional information.
However, models for pixel-level dense computer vision tasks should be able to process various image resolutions and be robust to cropping. Thus, relative positional embeddings, \eg~\cite{ShawNAACL18RelPos}, that consider distances between tokens are preferable. 
For instance, Bello~\etal~\cite{BelloICCV19AttentionAugmentedCNN} 
achieve better object detection results
using relative self-attention.
Similarly, 
Swin Transformers~\cite{swin} and Swin V2~\cite{swinv2} observed improved performance using relative positional embeddings, while~\cite{craft} showed it to be crucial in the cross attention for optical flow. 
Recently, \cite{rope} introduced the Rotary Positional Embedding (RoPE): 
a transformation to each key and query features is applied according to their absolute position, in such a way that the pairwise similarity scores used in the attention computation only depend on the relative positions of the token pairs and on their feature similarity. 
RoPE thus models relative positions 
at any resolution. 


\para{Stereo matching and optical flow} can both be seen as a dense correspondence matching problem~\cite{unimatch}. However the priors about matching itself and the completion of unmatched regions differ. This explains why most models are dedicated to one specific task despite many similarities in the strategies~\cite{laga2020survey,zhaisurvey21}.
Dense matching is most often posed with correlation/cost volume estimation from which matches can be extracted~\cite{flownet,sceneflow}.
For stereo, this volume typically has three dimensions~\cite{jie2018left,stereonet,yin2019hierarchical}, the third dimension representing a discretization of the disparity level, or four dimensions~\cite{psmnet,gcnet,nie2019multi}. 
For optical flow, each pixel of the first image can be associated to any pixel of the second, resulting in a 4D correlation volume. The complexity of building, storing and leveraging such volume motivated numerous methods revolving around the ideas of coarse-to-fine~\cite{deqflow,raft,gmflownet}, warping~\cite{pwcnetplus}, sparse formulation~\cite{jiang21}, random search~\cite{dip}, dimension separation~\cite{sepflow}, tokenization~\cite{flowformer}. 
Interestingly, recent  works~\cite{craft,gmflow,unimatch} leverage cross-attention to facilitate inter-image information exchanges but still rely on a low-resolution correlation volume, followed by an iterative refinement similar to~\cite{raft}. 
Unimatch~\cite{unimatch} made an important step towards a unified architecture for flow and stereo, but still relies on task-dependent (a) cross-attention mechanisms, (b) correlation volume and (c) post-processing. 
We similarly use the same architecture for both tasks, but our standard transformer model without cost volume can be pre-trained with existing self-supervised approaches and directly finetuned as is.

Several works propose self-supervised methods for estimating depth using stereo pairs or videos~\cite{godard2017unsupervised,godard2019digging}, stereo with matching priors~\cite{zhou2017unsupervised}, or optical flow~\cite{sun2014quantitative,selflow,yu2016back} typically with an unsupervised reconstruction loss. The main difference between this paradigm and ours is that we aim to pre-train a task-agnostic model that can be finetuned to different tasks, while these approaches aim to remove supervision for a single task.


\section{Cross-view completion pre-training at scale}
\label{sec:method}

Our proposed pre-training method is based on the recently introduced cross-view completion (CroCo) framework~\cite{croco}.
It extends MIM 
to pairs of images.
Given two different images depicting a given scene, the two images are divided into sets of non-overlapping patches, denoted as tokens, and 90\% of the tokens from the first image are masked.
The remaining ones are fed to a ViT~\cite{vit} encoder to extract features for the first image. Similarly, tokens from the second image are fed to the same encoder with shared weights,
and a ViT decoder processes the two sets of features together to reconstruct the target.
Figure~\ref{fig:intro} provides an overview of the pre-training stage.
Compared to standard masked image modeling methods, this approach can leverage the information in the second view to resolve some of the ambiguities about the masked context.
To leverage this information, the model has to implicitly reason about the scene geometry and the spatial relationship between the two views, which primes it well for geometric tasks. 

\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{fig/pipeline.pdf} \
 IoU(I_1,I_2) = \frac{\vert \mathcal{P}(I_1) \cap \mathcal{P}(I_2) \vert}{\vert \mathcal{P}(I_1) \cup \mathcal{P}(I_2) \vert}.

    s(I_1,I_2) = IoU(I_1,I_2) \times 4 \cos(\alpha) \big(1-\cos(\alpha)\big),
-0.5cm]
\caption{\textbf{Architecture of \ours and \oursflow.} The two images (left and right views for stereo, two frames for flow) are split into patches and encoded with a series of transformer blocks with RoPE positional embeddings. The decoder consists in a series of transformer decoder blocks (self-attention among token features from the first image, cross-attention with the token features from the second image, and an MLP). Token features from different intermediate blocks are fed to the DPT module~\cite{dpt} to obtain the final prediction.}
\label{fig:stereo}

\vspace{-0.35cm}

\end{figure*}

\PAR{Overall statistics}
In total, we collected about 5.3 million real-world pairs of crops with the process described above, with respectively 1,070,414 pairs from ARKitScenes~\cite{arkit}, 2,014,789 pairs from MegaDepth~\cite{megadepth}, 655,464 from 3DStreetView~\cite{streetview}, and 1,593,689 pairs from IndoorVL~\cite{gangnam}. We added this to 1,821,391 synthetic pairs generated with the Habitat simulator~\cite{habitat}, following the approach of~\cite{croco}. 
Example pairs for each dataset are shown in Figure~\ref{fig:pairs}.
They cover various scenarios, from indoor rooms -- synthetic with Habitat or real with ARKitScenes -- to larger crowded indoor environment (IndoorVL), landmarks (MegaDepth) and outdoor streets (3DStreetView).

\subsection{Positional embeddings}
\label{sub:posembed}

We replace the cosine embeddings, which inject absolute positional information, by Rotary Positional Embedding (RoPE)~\cite{rope}.
RoPE efficiently injects information about the \emph{relative} positioning of feature pairs when computing attention.
Formally, let  and  represent a query and a key feature, at absolute positions  and  respectively.
The main idea of RoPE is to design an efficient function  that transforms a feature  according to its absolute position  such that the similarity between the transformed query and the transformed key  is a function of ,  and  only.
\cite{rope} showed that a simple transformation such as applying rotations on pairs of dimensions according to a series of rotation matrices at different frequencies satisfy this desirable property.
To deal with 2D signals 
such as images, we 
split the features into 2 parts, we apply the 1D positional embedding of the x-dimension on the first part, and the embedding of the y-dimension on the second part.

\subsection{Scaling up the model}
\label{sub:decoder}

The combination of information 
extracted from the two images only occurs in the decoder.
Following MAE~\cite{mae}, CroCo~\cite{croco} uses a small decoder of 8 blocks consisting of self-attention, cross-attention and an MLP, with 512 dimensions and 16 attention heads. As the decoder is crucial for binocular tasks such as stereo or flow, we scale up the decoder and follow the ViT-Base hyper-parameters with 12 blocks, 768-dimensional features and 12 heads. We also scale up the image encoder from ViT-Base to ViT-Large, \ie, increase the depth from 12 to 24, the feature dimension from 768 to 1024 and the number of heads from 12 to 16. 

\PAR{Pre-training detailed setting}
We pre-train the network for 100 epochs with the AdamW optimizer~\cite{adamw}, a weight decay of , a cosine learning rate schedule at a base learning rate of  with a linear warmup in the first 10 epochs, and a batch size of 512 spread on 8 GPUs.
During pre-training, we simply use random crops and color jittering as data augmentation.
We mask 90\% of the tokens from the first image.
Examples of cross-view completion obtained with our model are shown in Appendix~\ref{app:crocovis}.

\begin{table*}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{ccccl@{\hskip 0.2cm}rrrrp{0cm}rrrr}
     \toprule
pos. & \multirow{2}{*}{encoder} & \multirow{2}{*}{decoder} & pre-train & & \multicolumn{4}{c}{Stereo (bad@1.0px)} & & \multicolumn{4}{c}{Flow (EPE)}  \\
emb. &  &  & data & & \multicolumn{1}{c}{\small Md} & \multicolumn{1}{c}{\small ETH} & \multicolumn{1}{c}{\small SF(c)} & \multicolumn{1}{c}{\small SF(f)} & & \multicolumn{1}{c}{\small FT(c)} & \multicolumn{1}{c}{\small FT(f)} & \multicolumn{1}{c}{\small Si.(c)} & \multicolumn{1}{c}{\small Si.(f)}  \\
\cmidrule(l){1-4} \cmidrule(lr){6-9} \cmidrule(lr){11-14}
cosine & ViT-B & Small & 2M habitat             & {\small (CroCo~\cite{croco})}  & 26.3 &  1.82 &   6.7 &   7.0 &       &3.89 &  3.56 &  2.07 &  2.57 \\
RoPE   & ViT-B & Small & 2M habitat             &                 & 25.3 &  \ul{0.60} &   6.0 &   6.3 &       &3.73 &  3.37 &  2.13 &  2.77 \\
RoPE   & ViT-B & Small & 2M habitat + 5.3M real &                 & 20.7 &  0.82 &   5.8 &   6.1 &       &3.35 &  2.94 &  1.76 &  2.30 \\
RoPE   & ViT-B & Base  & 2M habitat + 5.3M real &                 & \ul{17.1} &  1.14 &   \ul{5.3} &   \ul{5.6} &       &\ul{3.10} &  \ul{2.73} &  \ul{1.51} &  \bf{1.99} \\
  RoPE & ViT-L & Base  & 2M habitat + 5.3M real & {\small (\textbf{\croconew})} & \bf{15.5} &  \bf{0.38} &   \bf{5.0} &   \bf{5.3} &       &\bf{2.85} &  \bf{2.45} &  \bf{1.43} &  \bf{1.99}    \\
\bottomrule
\end{tabular}
} 
\caption{\textbf{Ablative study} of each change to CroCo with the percentage of pixels with error above 1px  (bad@1.0) on validation sets from Middlebury (Md), ETH3D, SceneFlow (SF) in clean (c) and final (f) renderings for stereo, and with the endpoint error (EPE) on validation sets from FlyingThings (FT) and MPI-Sintel (Si.) in both clean (c) and final (f) renderings for optical flow. A \textit{Small} decoder has 8 decoder blocks with 16 attention heads on 512-dimensional features, while the \textit{Base} one has 12 blocks with 12 heads on 768-dimensional features.
}
\label{tab:changes}
\end{table*}

\begin{table*}
\resizebox{\linewidth}{!}{
\begin{tabular}{lcccccccccccccccc}
\toprule
                        & {\small Bicyc2}    & {\small Compu}     & {\small Austr}     & {\small AustrP}    & {\small Djemb}     & {\small DjembL}    & {\small Livgrm}    & {\small Plants}    & {\small Hoops} & {\small Stairs} & {\small Nkuba} & {\small Class} & {\small ClassE} & {\small Crusa}  & {\small CrusaP} & \bf{avg} \\  
 & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & & & & & \\
\midrule
LEAStereo~\cite{leastereo}    & 1.83 & 3.81 & 2.81 & 2.52 & 1.07 & 1.64 & 2.59 & 5.13 & 5.34 & 2.79 & 3.09 & 2.46 & 2.75 & 2.91 & 3.09 & 2.89 \\
AdaStereo~\cite{adastereo}    & 2.19 & 2.29 & 4.37 & 3.08 & 1.40 & 1.64 & 3.93 & 7.58 & 4.46 & 2.67 & 3.69 & 3.29 & 3.35 & 3.78 & 2.94 & 3.39 \\
HITNet~\cite{hitnet}          & 1.43 & 1.87 & 3.61 & 3.27 & 0.90 & 9.12 & 2.37 & 4.07 & 4.45 & 3.38 & 3.45 & 2.43 & 3.20 & 4.67 & 4.74 & 3.29 \\
RAFT-Stereo~\cite{raftstereo} & \ul{0.90} & 1.13 & 2.64 & \ul{2.22} & \bf{0.63} & 1.22 & 3.13 & 3.55 & 3.54 & \ul{1.89} & 4.36 & \bf{1.46} & 2.44 & 4.58 & 6.00 & 2.71 \\
CREStereo~\cite{crestereo}    & 1.38 & \bf{1.06} & 2.63 & 2.53 & \ul{0.64} & \bf{1.11} & \bf{1.42} & 5.31 & \ul{3.22} & 2.40 & 2.51 & \ul{1.92} & \ul{2.31} & \ul{1.78} & \ul{1.83} & \ul{2.10} \\
GMStereo~\cite{unimatch}      & 1.34 & \ul{1.32} & \ul{2.26} & 2.23 & 1.01 & 1.62 & \ul{1.84} & \ul{2.49} & \bf{3.19} & 2.18 & \ul{2.10} & 2.19 & \bf{2.08} & \bf{1.71} & \bf{1.75} & \bf{1.89} \\
\bf{\ours}             & \bf{0.84} & 1.45 & \bf{1.87} & \bf{1.83} & 0.69 & \ul{1.19} & 2.40 & \bf{2.28} & 8.31 & \bf{1.44} & \bf{1.96} & 3.99 & 4.61 & 2.48 & 2.81 & 2.36 \\
\bottomrule
\end{tabular}
}
\caption{\textbf{Evaluation on Middlebury} with the average error over all pixels for each sequence and the average (last column). Sequences are ordered according to their `nd' value, which is the official threshold of maximum disparity used to clip predictions before evaluation.}
\label{tab:middlebury2}

\vspace{-0.4cm}

\end{table*}


\section{Application to stereo matching and flow}
\label{sec:stereo}

We now present \ours and \oursflow, our ViT-based correlation-free architecture for stereo matching and optical flow respectively, pre-trained with cross-view completion.
This is much in contrast to current state-of-the-art methods which rely on task-specific design in the form of cost volumes~\cite{flowformer,jie2018left,gcnet,stereonet,adastereo,pwcnetplus,acvnet,yin2019hierarchical,dip}, image warping~\cite{brox2004high,pwcnetplus}, iterative refinement~\cite{crestereo,raftstereo} and multi-level feature pyramids~\cite{leastereo,crestereo,pwcnetplus,hitnet}. Both \ours and \oursflow share the same architecture.

\PAR{Architecture} When finetuning the model for stereo or flow, both images are fed to the encoder as during pre-training (but without masking), and the decoder processes the tokens of both images.
To output a pixel-wise prediction, we rely on DPT~\cite{dpt}, which adapts the standard up-convolutions and fusions from multiple layers 
used in fully-convolutional approaches for dense tasks, to vision transformers.
This allows to combine features from different blocks by reshaping them to different resolutions and fusing them with convolutional layers. 
In practice, we use the features from  blocks, regularly spread by an interval of a third of the decoder depth, starting from the last block, resulting in  block at the end of the encoder and  decoder blocks.

\PAR{Loss} 
We parameterize the output of the network with a Laplacian distribution~\cite{laplacian}: given an input pair , the model outputs a location parameter  and a scale parameter  per pixel location  and is trained to minimize the negative log-likelihood of the ground-truth target disparity, denoted , under the predicted distribution:

The scale parameter  can be interpreted as an uncertainty score for the prediction: large errors are penalized less when  is high, while good predictions are rewarded more if  is low. 
It is thus optimal for the network to adapt the scale parameter. 
The second term comes from the normalization term of the Laplacian density and avoids the degenerate solution of always predicting a low scale parameter.
Empirically, we find that using a probabilistic loss improves performance, see Appendix~\ref{appsub:loss} for the ablation, and is useful for tiling strategies during inference, because it provides a per-pixel confidence estimate, as detailed below.
A parameterization of  ensures its positiveness: for stereo matching we use , with  the sigmoid function and , and for optical flow  with , unless otherwise stated.

\PAR{Training} We train \ours using  crops from various stereo datasets: CREStereo~\cite{crestereo}, SceneFlow~\cite{sceneflow}, ETH3D~\cite{eth3d}, Booster~\cite{booster}, Middlebury (2005, 2006, 2014, 2021 and v3)~\cite{middlebury}. We train \oursflow using  crops from the TartanAir~\cite{tartan}, MPI-Sintel~\cite{sintel}, FlyingThings~\cite{sceneflow} and FlyingChairs~\cite{flownet} datasets.
We refer to Appendix~\ref{app:details} for more details on these datasets, the splits we use for the ablations, the data augmentation strategy, as well as training hyper-parameters.

\PAR{Inference} We use a tiling-based approach.
We sample overlapping tiles with the same size as the training crops in the first image.
For each tile, we create a pair by sampling a corresponding tile at the same position from the second image.
We then predict the disparity or flow between each pair of tiles.
Such tiling approach was used \eg in~\cite{flowformer}. To merge the predictions done at a given pixel, we use a weighted average with weights  with  for stereo matching and  for optical flow, where  is the uncertainty predicted by the model.



\section{Experiments}
\label{sec:xp}

\PAR{Ablations}
We perform our ablations on the validation pairs (see Appendix~\ref{app:details} for the splits we use) of Middlebury, ETH3D and SceneFlow for stereo matching, and FlyingThings and MPI-Sintel for optical flow.
Table~\ref{tab:changes} reports the impact of the changes in \croconew to improve CroCo~\cite{croco} (pre-training data, positional embedding, larger encoder and decoder).
We observe that they all lead to consistent improvements: replacing the cosine absolute positional embedding by RoPE, scaling up the decoder, using larger-scale pre-training data and a larger encoder. Altogether, this allows \eg to improve performance as measured by the bad@1.0px metric from  to  on Middlebury (stereo matching), or the EPE from  to  on MPI-Sintel in its clean rendering (optical flow).

To further benchmark \croconew, we evaluate the pre-training of the encoder only on monocular tasks following the protocol of~\cite{multimae}. For semantic segmentation on ADE20k~\cite{ade}, we obtain 44.7 mean Intersection over Union \vs 40.6 for CroCo~\cite{croco}, and for monocular depth estimation on NYU v2~\cite{nyuv2}, we obtain 93.2 delta-1 \vs 90.1 for~\cite{croco}.

We provide in Appendix~\ref{app:morexp} an ablation on the impact of pre-training (\ie, a comparison with a randomly initialized network for finetuning), an ablation on the masking ratio during pre-training as well as a comparison between the L1 loss and Laplacian loss during finetuning.

\PAR{\ours \vs the state of the art} 
We now evaluate \ours on the official leaderboards of Middlebury~\cite{middlebury}, KITTI 2015~\cite{kitti15}, ETH3D~\cite{eth3d} and Spring~\cite{spring}.

\begin{figure}
\newcommand{\exlength}{0.245\linewidth}
\resizebox{\linewidth}{!}{
\begin{tabular}{c@{ }c@{ }c@{ }c@{ }c}
{\small Left image} & {\small Ground truth} & 
{\small CREStereo~\cite{crestereo}} & {\small \bf{\ours}} \\
\includegraphics[width=\exlength]{fig/middlebury_test_examples/Australia_im0.jpg} &
\includegraphics[width=\exlength]{fig/middlebury_test_examples/Australia_GT.jpg} &
\includegraphics[width=\exlength]{fig/middlebury_test_examples/Australia_CREStereo.jpg} & 
\includegraphics[width=\exlength]{fig/middlebury_test_examples/Australia_CroCoStereo.jpg} \-0.1cm]
\includegraphics[width=\exlength]{fig/middlebury_test_examples/Hoops_im0.jpg} &
\includegraphics[width=\exlength]{fig/middlebury_test_examples/Hoops_GT.jpg} &
\includegraphics[width=\exlength]{fig/middlebury_test_examples/Hoops_CREStereo.jpg} &
\includegraphics[width=\exlength]{fig/middlebury_test_examples/Hoops_CroCoStereo.jpg} 
\end{tabular}
} \-0.05cm]
\includegraphics[width=\fexlength]{fig/sintel_test_examples/sintel_images_R2C1.png} &
\includegraphics[width=\fexlength]{fig/sintel_test_examples/sintel_gt_flow_R2C1.png} &
\includegraphics[width=\fexlength]{fig/sintel_test_examples/sintel_gmflowplus_flowR2C1.png} &
\includegraphics[width=\fexlength]{fig/sintel_test_examples/sintel_crocoflow_flow_R2C1.png}
\end{tabular}
}
\-0.05cm] }


\twocolumn[{\renewcommand\twocolumn[1][]{#1}\appendix
\begin{center}
    \captionsetup{type=figure}
\resizebox{\linewidth}{!}{
\begin{tabular}{@{}c@{ }c@{ }c@{ }c@{ }c@{}}
Reference image & Masked target & CroCo~\cite{croco} & \textbf{\croconew} & Target image \\
\completionexample{dolls311}
\completionexample{couchimp}
\completionexample{baby312}
\end{tabular}
}
\-0.4cm]
    \caption{\textbf{Cross-view reconstruction examples} (pre-training pretext task) on scenes unseen during pre-training for the original CroCo~\cite{croco} and with our improvements. The images come from the stereo benchmark of KITTI~\cite{kitti}.
    }
    \label{fig:reconstructions2}
\end{figure*}

\section{Cross-view completion examples}
\label{app:crocovis}

To qualitatively evaluate the impact of \croconew, \ie, of the improvements that we propose on top of the CroCo~\cite{croco} pre-training, we show several examples of cross-view completions on real-world scenes, coming either from Middlebury v3~\cite{middlebury} in Figure~\ref{fig:reconstructions1} or KITTI~\cite{kitti15} in Figure~\ref{fig:reconstructions2}.
Note that these methods, as MAE~\cite{mae}, regress pixel values that are normalized according to the mean and standard deviation inside each patch, we thus apply the inverse transform for display: this means that the overall color of each patch will be correct, as it comes from the ground-truth values.
While the most important measure of performance of these models is their transfer to downstream tasks, as explored in the main paper, a qualitative observation of the fact that our improved method is better at solving the pretext task is noteworthy.
We clearly observe that the reconstructions from the original CroCo~\cite{croco} tend to be quite blurry in many areas, which might come from the fact that it relies on a smaller model and was pre-trained only on synthetic data from indoor environments, while details are impressively preserved thanks to our improvements. 
In Figure~\ref{fig:reconstructions1}, note how the lines and the eyes are well reconstructed in the first row, or the roads on the maps of the third row, despite the high masking ratio that is applied to the masked image (90\%).
Similarly, the text is clearly readable on the first row of Figure~\ref{fig:reconstructions2}. Some predictions by our model have some blur (\eg left of the first and thirds rows of Figure~\ref{fig:reconstructions1}), which makes sense because these parts are not visible in the reference image.


\section{Further experimental results}
\label{app:morexp}


\begin{table*}
\centering
\begin{tabular}{llp{0cm}rrrrp{0cm}rrrr}
     \toprule
& \multirow{2}{*}{Network initialization} & & \multicolumn{4}{c}{Stereo (bad@1.0px)} & & \multicolumn{4}{c}{Flow (EPE)}  \\
& & & \multicolumn{1}{c}{\small Md} & \multicolumn{1}{c}{\small ETH} & \multicolumn{1}{c}{\small SF(c)} & \multicolumn{1}{c}{\small SF(f)} & & \multicolumn{1}{c}{\small FT(c)} & \multicolumn{1}{c}{\small FT(f)} & \multicolumn{1}{c}{\small Si.(c)} & \multicolumn{1}{c}{\small Si.(f)}  \\
\cmidrule(lr){1-2} \cmidrule(lr){4-7} \cmidrule(lr){9-12}
\multicolumn{12}{l}{\emph{RoPE positional embedding, ViT-L encoder, Base decoder, 2M Habitat + 5.3M real pre-training pairs}} \\
& \bf{\croconew pre-training} && \bf{15.5} &  \bf{0.38} &   \bf{5.0} &   \bf{5.3} &       &\bf{2.85} &  \bf{2.45} &  \bf{1.43} &  \bf{1.99} \\
& random init.       &&   43.4 &  1.06 &  11.0 &  11.2     &    &  10.53 & 10.57 &  4.84 &  5.49 \\
\cmidrule(lr){1-2} \cmidrule(lr){4-7} \cmidrule(lr){9-12} 
\multicolumn{12}{l}{\emph{cosine positional embedding, ViT-B encoder, Small decoder, 2M Habitat (synthetic only) pre-training pairs}} \\
& CroCo~\cite{croco} pre-training && \bf{26.3} &  1.82 &   \bf{6.7} &   \bf{7.0} &    &   \bf{3.89} &  \bf{3.56} &  \bf{2.07} &  \bf{2.57}     \\
& MAE~\cite{mae} (ImageNet) pre-training (encoder only)
&& 35.8 &  \bf{1.68} &   8.6 &   8.8 &    &   5.13 &  4.83 &  2.92 &  3.82          \\
& random init.   && 87.5 &  5.42 &  24.6 &  24.6 &    &  14.28 & 14.31 &  8.99 &  9.76     \\
\bottomrule
\end{tabular}
\vspace{-0.3cm}
\caption{\textbf{Impact of pre-training.} We compare the performance of our final model (first row) with improved cross-view completion pre-training to a randomly initialized version (second row). To compare to MAE~\cite{mae}, that is pre-trained on ImageNet~\cite{imagenet}, and which is based on cosine positional embeddings, we make the comparison with the original CroCo in the bottom rows.}
\label{tab:pretrain}
\end{table*}



\begin{table*}
\centering
\begin{tabular}{cp{0cm}rrrrp{0cm}rrrr}
     \toprule
Masking & & \multicolumn{4}{c}{Stereo (bad@1.0px)} & & \multicolumn{4}{c}{Flow (EPE)}  \\
ratio & & \multicolumn{1}{c}{\small Md} & \multicolumn{1}{c}{\small ETH} & \multicolumn{1}{c}{\small SF(c)} & \multicolumn{1}{c}{\small SF(f)} & & \multicolumn{1}{c}{\small FT(c)} & \multicolumn{1}{c}{\small FT(f)} & \multicolumn{1}{c}{\small Si.(c)} & \multicolumn{1}{c}{\small Si.(f)}  \\
\cmidrule(lr){1-1} \cmidrule(lr){3-6} \cmidrule(lr){8-11}
80\% & &   32.5 &  1.96 &   7.3 &   7.5 &    &   4.29 &  4.06 &  2.06 &  2.71 \\
85\% & &   59.2 &  1.15 &   8.7 &   9.0 &    &   3.48 &  3.08 &  1.99 &  2.41 \\
\bf{90\%} & &  \bf{20.7} &  \bf{0.82} &   \bf{5.8} &   \bf{6.1} &    &   \bf{3.35} &  \bf{2.94} &  \bf{1.76} &  \bf{2.30} \\
\bottomrule
\end{tabular}
\vspace{-0.3cm}
\caption{\textbf{Impact of the pre-training masking ratio} for a model with RoPE positional embeddings, a ViT-B encoder, a Small decoder, pre-trained on 2M Habitat + 5.3M real pairs.}
\label{tab:maskratio}
\end{table*}

\subsection{Impact of pre-training} 

In Table~\ref{tab:pretrain}, we measure the impact of the pre-training on the downstream performance when the model is finetuned for stereo matching or optical flow. The first two rows compare our model, using our improved cross-view completion pre-training \vs a random initialization. We observe a clear gain of performance, \eg on the FlyingThings flow test set in the final rendering with an EPE of 2.45 pixels with pre-training \vs 10.57 without it, or on the Middlebury v3 stereo validation set with a bad@1.0px of 15.5\% with pre-training \vs 43.4\% without it.

We are not aware of any other pre-training strategy, other than cross-view completion, that readily includes a binocular decoder or architecture. While it is still possible to initialize part of the layers using other pre-training strategies, this means that some important parts of the network are still being initialized at random. Nevertheless, to compare to other pre-training strategies, we consider MAE~\cite{mae} pre-trained on ImageNet~\cite{imagenet}, thus with a cosine positional embedding, a ViT-Base encoder, and with a Small decoder that is randomly initialized. We compare that to the original CroCo~\cite{croco} pre-trained on synthetic data only. We observe that CroCo pre-training obtains the lowest errors, significantly outperforming the MAE pre-training and the random initialization.

Interestingly, the performance of this smaller model is also significantly better than the large one without pre-training. This again highlights the importance of the pre-training with such generic architecture.

\vpara{Masking ratio.} CroCo~\cite{croco} finds that using a masking ratio of 90\% performs best for cross-view completion on their synthetic data. This is higher than the 75\% masking ratio of MAE~\cite{mae}, as the unmasked reference view of the same scene adds redundancy. 
A question is whether this masking ratio of 90\% that has been found optimal on synthetic data generalizes to real data.
Table~\ref{tab:maskratio} reports the performance on stereo and flow downstream tasks for a masking ratio of 80\%, 85\% and 90\%.
We find that a masking ratio of 90\% performs best also in the case of using real data.

\subsection{Smaller training data}

\begin{table}
\centering
\begin{tabular}{lcc}
\toprule
 \multirow{2}{*}{Method} & \multicolumn{2}{c}{MPI-Sintel()} \\
 & clean & final \\
\cmidrule(lr){1-1} \cmidrule{2-3}
LiteFlowNet2~\cite{liteflownet2} & 2.24 & 3.78 \\
FM-RAFT~\cite{fmraft} & 1.29 & 2.95 \\
FlowFormer~\cite{flowformer} & \bf{1.01} & \bf{2.40} \\ 
RAFT~\cite{raft} before refinement        & 4.04 & 5.45 \\
RAFT~\cite{raft} & 1.41 & 2.69 \\ 
GMFlow~\cite{unimatch} before refinement  & 1.31 & 2.96 \\
GMFlow~\cite{unimatch} & \ul{1.08} & \ul{2.48} \\
\textbf{\oursflow} & 1.28 & 2.58 \\
\bottomrule
\end{tabular} \-0.3cm]
\caption{\textbf{Runtime and number of parameters.} Runtime is measured for a single tile of size , on a NVIDIA A100 GPU. For the number of parameters we report in parenthesis the numbers for the encoder, the decoder and the DPT head separately.}
\label{tab:runtime}
\end{table}



\vpara{Runtime.} In Table~\ref{tab:runtime}, we report the runtime for different sizes of our model. On one single tile of the same size as training for stereo, \ie, , on a NVIDIA A100 GPU. Our method remains relatively fast on one tile, in the order of a few tens of milliseconds.


\pgfplotsset{compat=newest}
\usepgfplotslibrary{fillbetween}

\newcommand{\oxmedc}{CornflowerBlue}
\newcommand{\oxhardc}{TealBlue}
\newcommand{\oxf}{MidnightBlue}

\newcommand{\pamedc}{WildStrawberry}
\newcommand{\pahardc}{RedViolet}
\newcommand{\paris}{RedViolet}

\newcommand{\valc}{PineGreen}

\newcommand{\ourscolor}{Maroon}
\newcommand{\howcolor}{Black}

\newcommand{\oxfmark}{o}
\newcommand{\parmark}{triangle}
\newcommand{\valmark}{square}

\newcommand{\bone}{CornflowerBlue}
\newcommand{\btwo}{MidnightBlue}
\newcommand{\bthree}{TealBlue}
\newcommand{\bfour}{Blue}


\newcommand{\leg}[1]{\addlegendentry{#1}}
\newcommand{\legtext}[1]{\addlegendimage{empty legend} \addlegendentry{#1}}

\tikzset{every mark/.append style={solid}}
\pgfplotsset{
	grid=both, width=\linewidth, try min ticks=5,
    legend cell align=left, 
    legend style={fill opacity=0.8},
	ylabel near ticks,
    xlabel near ticks,
    every tick label/.append style={font=\footnotesize},
}

\pgfplotsset{
oxmed/.style={thick, color=\oxf, mark=o},
    oxhard/.style={thick, dashed, color=\oxf, mark=o},
    pamed/.style={thick, color=\paris, mark=star}, 
    pahard/.style={thick, dashed, color=\paris, mark=star},
    val/.style={thick, color=\valc, mark=x},
    oursoxf/.style={thick, color=\ourscolor, mark=\oxfmark},
    howoxf/.style={thick, dashed, color=\howcolor, mark=\oxfmark},
    ourspar/.style={thick, color=\ourscolor, mark=\parmark},
    howpar/.style={thick, dashed, color=\howcolor, mark=\parmark},
    oursval/.style={thick, color=\ourscolor, mark=\valmark},
    howval/.style={thick, dashed, color=\howcolor, mark=\valmark},
    numean/.style={thick, color=\ourscolor, mark=none},
    numin/.style={thick, color=gray, mark=none},
    wid/.style={thick, color=\ourscolor, mark=o, mark size=0.5pt},
    woid/.style={thick, densely dashdotted, color=RedOrange, mark=o, mark size=0.5pt},
    d2k1/.style={thick, color=CornflowerBlue, mark=\oxfmark},
    d2k2/.style={thick, color=MidnightBlue, mark=\oxfmark},
    d2k4/.style={thick, color=TealBlue, mark=\oxfmark},
    d2k8/.style={thick, color=Blue, mark=\oxfmark},
    d4c256/.style={thick, color=\ourscolor, mark=\oxfmark},
    d8c256/.style={thick, color=Green, mark=\oxfmark},
    b1c/.style={thick, color=\bone, mark=o},
    b2c/.style={thick, color=\btwo, mark=o},
    b3c/.style={thick, color=\bthree, mark=o},
    b4c/.style={thick, color=\bfour, mark=o},
    mae/.style={thick, color=Gray, mark=star},
    y1c/.style={thick, color=Goldenrod, mark=o},
    r1c/.style={thick, color=Maroon, mark=o},
    r2c/.style={thick, color=WildStrawberry, mark=o},
    r3c/.style={thick, color=RedViolet, mark=o},
    y1c/.style={thick, color=Goldenrod, mark=o},
    y2c/.style={thick, color=Apricot, mark=o},
    y3c/.style={thick, color=BurntOrange, mark=o},
    y3cp/.style={thick, color=BurntOrange, mark=o, mark size=8pt},
    yvert/.style={dashed, thick, color=BurntOrange, mark=o},
    yvertp/.style={dashed, thick, color=BurntOrange, mark=o, mark size=8pt},
    tt3/.style={thick, color=Maroon, mark=o},
    tt4/.style={thick, color=BurntOrange, mark=o},
    tt1/.style={thick, color=Maroon, mark=star},
    tt2/.style={thick, color=BurntOrange, mark=star},
    ttl1/.style={thick, color=Maroon},
    ttl2/.style={thick, color=BurntOrange},
    ttl3/.style={thick, mark=star,mark size=3pt},
    ttl4/.style={thick, mark=o},
    crcr/.style={thick, color=LimeGreen, mark=o},
crcrp/.style={thick, color=Goldenrod, mark=o,mark size=2.5pt},
    crcap/.style={thick, color=BurntOrange, mark=diamond,mark size=2.5pt},
    maep/.style={thick, color=Maroon, mark=star, mark size=2.5pt},
    crca/.style={thick, color=Gray, mark=diamond},
    crcb/.style={thick, color=Gray, mark=o, mark size=2.5pt},
    crcc/.style={thick, color=Gray, mark=+,mark size=2.5pt},
    crcd/.style={thick, color=Gray, mark=star,mark size=2.5pt},
    crce/.style={thick, color=LimeGreen, mark=+,mark size=2.5pt},
    gvert/.style={dashed, thick, color=Gray,mark size=2.5pt},
    fin/.style={thick, color=Maroon, mark=diamond,mark size=2.5pt},
} \newcommand{\plotMd}[0]{
\begin{tikzpicture}
\begin{axis}[width=1.1\linewidth,
  height=4cm, 
   xlabel={\normalsize Overlap ratio},
   xlabel style={yshift=4pt},
minor tick num=1,
  ticklabel style = {font=\normalsize},
legend pos=north east,
title={\normalsize Middlebury v3 (bad@1.0px)},
            title style={yshift=-5pt},
  ]











\pgfplotstableread{
overlap   	bad_md      bad_sfclean epe_sintelc  epe_sintelf nbtiles       time
0.0    		23.1  		5.89   		1.73         2.29              12    	0.66 
0.1    		23.1  		5.89   		1.73         2.29              12    	0.66 
0.2    		23.6  		5.89   		1.51         2.10              16        0.88 
0.3    		22.6  		5.89   		1.51         2.10              16    	0.88 
0.4    		19.1  		5.89   		1.51         2.10              20    	1.1  
0.5    		17.7  		5.79   		1.52         2.11              30    	1.65 
0.6    		17.2  		5.79   		1.52         2.06              42    	2.31 
0.7    		16.2  		5.78   		1.44         2.01              56    	3.08 
0.8    		16.2  		5.77   		1.44         2.00             120   	6.6  
0.9    		15.5  		5.78   		1.43         1.9              418   	22.99
}{\map}               
     \addplot[fin]      table[x=overlap,  y=bad_md] \map;
\end{axis}
\end{tikzpicture}}

\newcommand{\plotSf}[0]{
\begin{tikzpicture}
\begin{axis}[width=1.1\linewidth,
  height=4cm, 
   xlabel={\normalsize Overlap ratio},
   xlabel style={yshift=4pt},
minor tick num=1,
  ticklabel style = {font=\normalsize},
legend pos=north east,
title={\normalsize SceneFlow clean (bad@1.0)},
            title style={yshift=-5pt},
  ]











\pgfplotstableread{
overlap   	bad_md      bad_sfclean epe_sintelc  epe_sintelf nbtiles       time
0.0    		23.1  		5.89   		1.73         2.29              12    	0.66 
0.1    		23.1  		5.89   		1.73         2.29              12    	0.66 
0.2    		23.6  		5.89   		1.51         2.10              16        0.88 
0.3    		22.6  		5.89   		1.51         2.10              16    	0.88 
0.4    		19.1  		5.89   		1.51         2.10              20    	1.1  
0.5    		17.7  		5.79   		1.52         2.11              30    	1.65 
0.6    		17.2  		5.79   		1.52         2.06              42    	2.31 
0.7    		16.2  		5.78   		1.44         2.01              56    	3.08 
0.8    		16.2  		5.77   		1.44         2.00             120   	6.6  
0.9    		15.5  		5.78   		1.43         1.9              418   	22.99
}{\map}               
     \addplot[fin]      table[x=overlap,  y=bad_sfclean] \map;
\end{axis}
\end{tikzpicture}}

\newcommand{\plotSintelf}[0]{
\begin{tikzpicture}
\begin{axis}[width=1.1\linewidth,
  height=4cm, 
   xlabel={\normalsize Overlap ratio},
   xlabel style={yshift=4pt},
minor tick num=1,
  ticklabel style = {font=\normalsize},
legend pos=north east,
title={\normalsize MPI-Sintel final (EPE)},
            title style={yshift=-5pt},
  ]











\pgfplotstableread{
overlap   	bad_md      bad_sfclean epe_sintelc  epe_sintelf nbtiles       time
0.0    		23.1  		5.89   		1.73         2.29              12    	0.66 
0.1    		23.1  		5.89   		1.73         2.29              12    	0.66 
0.2    		23.6  		5.89   		1.51         2.10              16        0.88 
0.3    		22.6  		5.89   		1.51         2.10              16    	0.88 
0.4    		19.1  		5.89   		1.51         2.10              20    	1.1  
0.5    		17.7  		5.79   		1.52         2.11              30    	1.65 
0.6    		17.2  		5.79   		1.52         2.06              42    	2.31 
0.7    		16.2  		5.78   		1.44         2.01              56    	3.08 
0.8    		16.2  		5.77   		1.44         2.00             120   	6.6  
0.9    		15.5  		5.78   		1.43         1.9              418   	22.99
}{\map}               
     \addplot[fin]      table[x=overlap,  y=epe_sintelf] \map;
\end{axis}
\end{tikzpicture}}

\newcommand{\plotTiles}[0]{
\begin{tikzpicture}
\begin{axis}[width=1.1\linewidth,
  height=4cm, 
   xlabel={\normalsize Overlap ratio},
   xlabel style={yshift=4pt},
minor tick num=1,
  ticklabel style = {font=\normalsize},
legend pos=north east,
title={\normalsize Nb. tiles},
            title style={yshift=-5pt},
  ]











\pgfplotstableread{
overlap   	bad_md      bad_sfclean epe_sintelc  epe_sintelf nbtiles       time
0.0    		23.1  		5.89   		1.73         2.29              12    	0.66 
0.1    		23.1  		5.89   		1.73         2.29              12    	0.66 
0.2    		23.6  		5.89   		1.51         2.10              16        0.88 
0.3    		22.6  		5.89   		1.51         2.10              16    	0.88 
0.4    		19.1  		5.89   		1.51         2.10              20    	1.1  
0.5    		17.7  		5.79   		1.52         2.11              30    	1.65 
0.6    		17.2  		5.79   		1.52         2.06              42    	2.31 
0.7    		16.2  		5.78   		1.44         2.01              56    	3.08 
0.8    		16.2  		5.77   		1.44         2.00             120   	6.6  
0.9    		15.5  		5.78   		1.43         1.9              418   	22.99
}{\map}               
     \addplot[fin]      table[x=overlap,  y=nbtiles] \map;
\end{axis}
\end{tikzpicture}}

\newcommand{\plotTime}[0]{
\begin{tikzpicture}
\begin{axis}[width=1.1\linewidth,
  height=4cm, 
   xlabel={\normalsize Overlap ratio},
   xlabel style={yshift=4pt},
minor tick num=1,
  ticklabel style = {font=\normalsize},
legend pos=north east,
title={\normalsize Time },
            title style={yshift=-3pt},
  ]











\pgfplotstableread{
overlap   	bad_md      bad_sfclean epe_sintelc  epe_sintelf nbtiles       time
0.0    		23.1  		5.89   		1.73         2.29              12    	0.66 
0.1    		23.1  		5.89   		1.73         2.29              12    	0.66 
0.2    		23.6  		5.89   		1.51         2.10              16        0.88 
0.3    		22.6  		5.89   		1.51         2.10              16    	0.88 
0.4    		19.1  		5.89   		1.51         2.10              20    	1.1  
0.5    		17.7  		5.79   		1.52         2.11              30    	1.65 
0.6    		17.2  		5.79   		1.52         2.06              42    	2.31 
0.7    		16.2  		5.78   		1.44         2.01              56    	3.08 
0.8    		16.2  		5.77   		1.44         2.00             120   	6.6  
0.9    		15.5  		5.78   		1.43         1.9              418   	22.99
}{\map}               
     \addplot[fin]      table[x=overlap,  y=time] \map;
\end{axis}
\end{tikzpicture}} \begin{figure*}[!th]
\centering
\begin{subfigure}{.32\linewidth}
\plotMd
\end{subfigure}
\hfill
\begin{subfigure}{.32\linewidth}
\plotSintelf
\end{subfigure}
\hfill
\begin{subfigure}{.32\linewidth}
\plotTiles
\end{subfigure}
\-0.05cm] }
\resizebox{\linewidth}{!}{
\begin{tabular}{@{}c@{ }c@{ }c@{}}
First image & Prediction error & Uncertainty \\ 
\confexample{0003}
\confexample{0042}
\confexample{0075}
\confexample{0044}
\end{tabular}
}

\vspace{-0.3cm}

\caption{\textbf{Visualization of the uncertainty predicted by \ours} on a few examples from the SceneFlow test set. The first column shows the first image, the second column shows the error of the prediction clamped within the segment , the third column shows the logarithm of the predicted scale of the Laplacian distribution output by the model: green colors denote confident areas while blue colors denote uncertain areas.}
\label{fig:confidence}    
\end{figure}

For flow and stereo, we regress a Laplacian distribution: the location parameter corresponds to the disparity or flow prediction, while the scale parameter could be seen as a measure of uncertainty. We thus denote here by `uncertainty' the logarithm of the predicted scale of the Laplacian distribution that our downstream model outputs, \ie,  from Equation~\ref{eq:loss}.

\begin{figure*}
\centering
\includegraphics[width=0.4\linewidth]{fig/confidence/error_vs_confidence.pdf} 
\quad \quad 
\includegraphics[width=0.4\linewidth]{fig/confidence/error_vs_points_fraction.pdf} \-0.3cm]
\caption{\textbf{Overview of our stereo training data.} We indicate here the train/val split used for the ablations, as well as the number of training pairs. For ETH3D and Middlebury, we also consider multiple times each pair in each epoch.}
\label{tab:stereodata}
\end{table*}

\begin{table*}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{lrrl}
\toprule
 flow dataset & \# pairs & prob. & comment \\
 \midrule
FlyingChairs~\cite{flownet} & 22,232 & 12\% & - \\
FlyingThings~\cite{sceneflow} & 80,604 & 40\% & 40,302 pairs for both `clean' and `final' renderings \\
             &     &            & ~~we use the same 1,024 validation pairs from the test set as~\cite{unimatch} \\
MPI-Sintel~\cite{sintel}   & 943 & 10\% & sequences `temple\_2' and `temple\_3' (98 pairs) are kept apart for validation \\
TartanAir~\cite{tartan}    & 306,268 & 38\% & - \\
 \midrule
 total         &  410,047 & 100\% &  \\ 
\bottomrule
\end{tabular}
} \-0.3cm]
\caption{\textbf{Overview of our flow training data.} We indicate here the train/val split used for the ablations, as well as the number of remaining training pairs. During training, we set a number of images per epoch and randomly sample them among the available datasets with the percentages shown in the column `prob.'.}
\label{tab:flowdata}
\end{table*}

\section{Training details}
\label{app:details}

\vpara{\ours training.}
We train \ours for 32 epochs using batches of 6 pairs of  crops. 
We detail the training/validation pairs we use for our ablations in Table~\ref{tab:stereodata}.
We use the AdamW optimizer~\cite{adamw} with a weight decay of , a cosine learning rate schedule with a single warm-up epoch and a learning rate of . During training, we apply standard data augmentations: color jittering (asymmetrically with probably 0.2), random vertical flipping with probably , random scaling with probability  in the range  and stretching (resize different along the  and  axis) with probability  in the range , and slightly jitter the right image with probability . 
When submitting to the official leaderboards, we include the pairs that were kept apart from the training sets for validation into the training epochs. 



\vpara{\oursflow training.} We train \oursflow for  epochs of  pairs each, randomly sampled from all available data, using batches of  pairs of crops of size . 
We detail the training/validation pairs we use for our ablations in Table~\ref{tab:flowdata}. 
To better balance the datasets, we set the probability of choosing a random pair from these datasets, see Table~\ref{tab:stereodata}. 
We
use the AdamW optimizer, a weight decay of , a cosine learning rate schedule with linear warm-up over  epoch, and a base learning rate of . During training, we apply standard augmentations~\cite{unimatch}: random color jittering (asymmetrically with probably 0.2), random scaling with probably  with a scale sampled in  and stretching with probability  in the range . 

\end{document}