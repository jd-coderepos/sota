\begin{table*}
\centering
\parbox{0.55\columnwidth}{
\centering
\resizebox{0.55\columnwidth}{!}{ 

\begin{tabular}{cm{3.6em}m{3em}}
\hline
\textbf{Method/Dataset}         & \textbf{iNaturalist 2018} & \textbf{Places 365}                  \\
\hline
Omni-MAE~\cite{girdhar2022omnimae}                       & 78.1            & 59.4                           \\
Omnivore~\cite{girdhar2022omnivore}                        & 84.1            & 59.9                           \\
EfficientNet B8\cite{tan2019efficientnet}                 & 81.3            & 58.6                           \\
MAE\cite{he2022masked}                             & 86.8            &                                \\
MetaFormer~\cite{yu2022metaformer}                      & 87.5            & 60.7                           \\
InternImage\cite{wang2023internimage} & {\ul 92.6}   & {\ul 61.2} \\
\hline
OmniVec                         & \textbf{93.8}      & \textbf{63.5}  \\
\hline
\end{tabular}}
\caption{\textbf{iNaturalist-2018 and Places-365} top- accuracy. 
}
\label{tab:sota_image}
}
\hspace{0.5em}
\parbox{0.45\columnwidth}{
\centering
\resizebox{0.45\columnwidth}{!}{ 

\begin{tabular}{cc}
\hline
\textbf{Method/Dataset} & \textbf{Kinetics-400} \\
\hline
Omnivore~\cite{girdhar2022omnivore}                & 84.1                  \\
VATT~\cite{akbari2021vatt}                    & 82.1                  \\
Uniformerv2~\cite{li2022uniformerv2} & 90.0 \\
InternVideo\cite{wang2022internvideo}             & \textbf{91.1}         \\
TubeViT\cite{piergiovanni2023rethinking}                 & {\ul 90.9}                  \\
\hline
OmniVec                 & \textbf{91.1} \\\hline                
\end{tabular}}
\caption{\textbf{Kinetics-400} top- accuracy.
}
\label{tab:sota_kinects400}
}
\hspace{0.5em}
\parbox{0.45\columnwidth}{
\centering
\resizebox{0.45\columnwidth}{!}{ 

\begin{tabular}{cm{4em}}
\hline
\textbf{Method/Dataset} & \textbf{Moments in Time} \\
\hline
VATT~\cite{akbari2021vatt}                    & 41.1                     \\
Uniformer v2\cite{li2022uniformerv2}           & 47.8                     \\
CoCa\cite{yu2022coca}                    & 47.4                     \\
CoCa-finetuned\cite{yu2022coca}          & {\ul 49.0}               \\
\hline
OmniVec                 & \textbf{49.8} \\
\hline
\end{tabular}
}
\caption{\textbf{Moments in time} top- accuracy.
}
\label{tab:sota_MIT}
}
\hspace{0.5em}
\parbox{0.4\columnwidth}{
\resizebox{0.4\columnwidth}{!}{
    \begin{tabular}{cc}
        \hline
        \textbf{Method/Dataset} & \textbf{ESC50} \\
        \hline
        AST~\cite{gong2021ast}                     & 85.7           \\
        EAT-M\cite{gazneli2022end}                   & 96.3           \\
        HTS-AT\cite{chen2022hts}                  & 97.0           \\
        BEATs\cite{oreshkin2019n}                   & {\ul 98.1}     \\
        \hline
        OmniVec                 & \textbf{98.4} \\
        \hline
    \end{tabular}
}
\caption{\textbf{ESC50} top- accuracy. 
}
\label{tab:sota_audio}
}
\hspace{1em}
\parbox{0.45\columnwidth}{
\resizebox{0.45\columnwidth}{!}{

}
}
\end{table*}



\section{Experiments}
\label{sec:experiments}


\noindent\textbf{Masked pretraining.} We do masked pretraining using the modality mixing as described in Section \ref{sec:approach}. We use AudioSet (audio) \cite{gemmeke2017audio}, Something-Something v2 (SSv2)(video) \cite{goyal2017something}, English Wikipedia (text), ImageNet1K (image) \cite{deng2009imagenet}, SUN RGB-D (depth maps) \cite{song2015sun}, ModelNet40 (3D point cloud) \cite{wu20153d} for pretraining the network.  As we perform autoencoder based pre-training, we do not group the tasks, and instead uniformly sample data from each of the datasets and modalities. Further, we randomly select patches for masking. For image, video and audio, we randomly mask  of the patches. For point cloud, we mask  of the patches, and for text we mask  of the patches. Further, we keep  =  of the tokens as predicted tokens (unlike  in \cite{yang2021generalized}). We perform pretraining for  epochs.  

\noindent\textbf{Modality Encoder.} For modality specific encoders, we use the networks from Table \ref{tab:modality_encoder}. We use the same network configurations for these networks as in corresponding publications. We pretrain the model using masked pretraining as described in Section \ref{sec:approach}, followed by training on specific modalities as per task groups and modality mixing. For different tasks on a modality, we keep the modality encoder same, while changing the task heads with appropriate loss functions. We train modality encoders for  epochs with  consecutive epochs each for simple and dense task groups.  

\noindent\textbf{Datasets for training on multiple modalities and tasks.} After masked pre-training, we fine tune the network on multiple tasks across modalities. 
The datasets and their corresponding task groups and modality are given in Table~\ref{tab:multi_task_training}.

\noindent\textbf{Task Heads.} For classification tasks, we use standard classification head from ViT~\cite{dosovitskiy2020image} while use ~\cite{arnab2021vivit} for video classification and ~\cite{gong2021ast} for audio classification.  For image and point cloud segmentation tasks, we use the segmentation head from~\cite{ranftl2021vision}. For text summarization, we use a -layered transformer.

We provide more implementation details in the supplementary material.


\begin{table}[]
\resizebox{\columnwidth}{!}{ 
\begin{tabular}{cccc}
\hline
\textbf{Task}              & \textbf{Dataset}              & \textbf{Modality} & \textbf{Task Group} \\
\hline
Image Recognition          & iNaturalist-2018 \cite{van2018inaturalist}             & Image             & Simple              \\
Scene Recognition          & Places-365 \cite{zhou2017places}                 & Image             & Dense               \\
Video Action Recognition   & Kinetics-400 \cite{kay2017kinetics}                 & Video             & Simple              \\
Video Action Recognition   & Moments in Time \cite{monfort2019moments}              & Video             & Dense               \\
Audio Event Classification & ESC50 \cite{piczak2015esc}                         & Audio             & Simple              \\
Point Cloud Segmentation   & S3DIS \cite{armeni20163d}                        & Point Cloud       & Dense               \\
Text Summarization         & DialogueSUM \cite{chen2021dialogsum}                & Text              & Dense               \\
Point Cloud Classification & ModelNet40-C \cite{wu20153d}                 & Point Cloud       & Simple      \\       
\hline
\end{tabular}
}
\caption{\textbf{List of tasks and corresponding datasets for task group based training after masked pretraining}. We assign each task to a task group (simple, dense) based on complexity of the dataset and output.}
\label{tab:multi_task_training}
\end{table}

\subsection{Results}

\noindent\textbf{Comparison of pretrained OmniVec with similar methods.}
Table~\ref{tab:res_masked_pretraining} compares OmniVec model with masked pretraining to various similar methods. The table also indicates the modalities supported by various methods (Col.-Supp. Modalities), and that if the method supports sharing knowledge between modalities (Col.-Cross-Modal sharing). Further, it also details the learning objectives by these methods. The table reports results on six benchmark datasets on seven tasks as AudioSet supports two tasks (audio only, and audio with video). These datasets are used to perform masked pretraining on the OmniVec model as described in Section~\ref{sec:approach}. It can be observed that the proposed OmniVec model outperforms all the compared methods on all the datasets. It is important to note that, we do not fine tune on any of these datasets specifically while other methods, in general, fine tune the results, mostly using a linear layer with softmax classification. This demonstrates the robustness of the proposed model and its ability to learn generalized embeddings without task specific fine-tuning. 




\noindent\textbf{Comparison to state-of-the-art.} For comparison with state of the art methods, we performed masked pretraining of OmniVec followed by training on multiple modalities and task groups as described in Section \ref{sec:approach}. We discuss the comparison on each modality below.

\noindent\textbf{(i) Image}
Table~\ref{tab:sota_image} shows state of the art on image datasets. We compare with multi-modal methods (Omni-MAE, Omnivore) and specialized methods (MetaFormer, InternImage). We surpass the state of the art on iNaturalist with a top-1 accuracy of 93.8\%, compared to InternImage's 92.6\%. On Places-365, we beat all competitors, achieving 61.6\% accuracy versus InternImage's 61.2\%. Moreover, we best Omnivore by  \% on iNaturalist and  \% on Places-365. Our results either match or surpass modality-specific methods in image classification, and outperforming unified learning methods.


\noindent\textbf{(ii) Video} Table~\ref{tab:sota_kinects400} and Table~\ref{tab:sota_MIT} show comparison against state of the art methods on Kinetics-400 and Moments in Time datasets.We observe that we outperform all the competing methods on Moments in Time dataset while perform same as the state of the art method InterVideo  \ie  top-1 accuracy.    

\noindent\textbf{(iii) Audio} 
Table~\ref{tab:sota_audio} highlights our comparison with top-performing methods on the ESC50 dataset. OmniVec outperforms competing methods, achieving an accuracy of 98.4\%, significantly higher than the Audio Spectrogram Transformer (AST) at 85.7\%. While most compared methods utilize supervised pretraining on AudioSet, we adopt masked pretraining without accessing labels. This suggests OmniVec's proficiency in learning from related tasks across different modalities, emphasizing its effectiveness in cross-modal knowledge transfer.

\begin{table*}[]
\centering
\resizebox{\textwidth}{!}{ 
\begin{tabular}{lccccccccc}
\hline
\multicolumn{1}{c}{\textbf{Method}} &  
\multicolumn{1}{c}{\textbf{Task Grouping}} & \multicolumn{1}{c}{\textbf{Modality Mixing}} & \multicolumn{1}{l}{\textbf{AudioSet (A+V.)}} & \multicolumn{1}{l}{\textbf{AudioSet (A)}} & \multicolumn{1}{l}{\textbf{SSv2}} & \multicolumn{1}{l}{\textbf{GLUE}} & \multicolumn{1}{l}{\textbf{ImageNet1K}} & \multicolumn{1}{l}{\textbf{Sun RGBD}} & \multicolumn{1}{l}{\textbf{ModelNet40}} \\ \hline

OmniVec-1 (baseline) & \xmark & \xmark & 37.5
& 36.3  
& 62.6
& 57.5
& 70.2
& 59.8 
& 68.5 \\ 
OmniVec-2 & \cmark & \xmark & 42.6 
& 40.1  
& 73.5
& 69.5
& 79.8
& 66.4 
& 75.2 \\ 
OmniVec-3 & \xmark & \cmark & 39.2
& 39.4  
& 70.2
& 68.8
& 77.3
& 65.5 
& 72.2 \\ 
OmniVec-4 & \cmark & \cmark & \textbf{48.6} 
& \textbf{44.7}                                    & \textbf{80.1}                                    & \textbf{84.3}
& \textbf{88.6}
& \textbf{71.4} 
& \textbf{83.6}\\ 

\hline
\end{tabular}
}
\caption{\textbf{Impact of various training strategies on OmniVec.} We report results with and without each of task grouping and modality mixing. The results are reported with masked pretraining only. We observe that individually, both task grouping and modality mixing improve the results over the baseline method. However, there combination outperforms individual performance using these mechanisms.}
\label{tab:ablation}
\end{table*}



\begin{table*}
\centering
\parbox{0.55\columnwidth}{
\centering
\resizebox{0.5\columnwidth}{!}{
\begin{tabular}{cm{4em}}
\hline
\textbf{Method/Dataset} & \textbf{Model Net40C} \\
\hline
PointNet++\cite{qi2017pointnet++}              & 0.236                 \\
DGCN+PCM-R\cite{zhang2022pointcutmix}     & 0.173                 \\
PCT + RSMIx\cite{lee2021regularization}             & 0.173                 \\
PCT + PCM-R\cite{sun2022benchmarking}     & {\ul 0.163}           \\
\hline
OmniVec                 & \textbf{0.156}   \\  
\hline
\end{tabular}
}
\caption{\textbf{ModelNet40-C} Error Rate.
}
\label{tab:sota_mnc}
}
\hspace{1em}
\parbox{0.65\columnwidth}{
\centering
\resizebox{0.5\columnwidth}{!}{
\begin{tabular}{cc}
\hline
\textbf{Method/Dataset} & \textbf{S3DIS} \\
\hline
PointTransformer+CBL\cite{tang2022contrastive}    & 71.6           \\
StratifiedTransformer\cite{lai2022stratified}   & 72.0           \\
PTv2\cite{wu2022point}                    & { 72.6}     \\
Swin3D\cite{yang2023swin3d}                   & {\ul 74.5}     \\
\hline
OmniVec                 & \textbf{75.9}  \\
\hline
\end{tabular}
}
\caption{\textbf{Stanford Indoor Dataset} mIoU.
}
\label{tab:sota_s3dis}
}
\hspace{1em}
\parbox{0.75\columnwidth}{
\centering
\resizebox{0.6\columnwidth}{!}{
\begin{tabular}{ccccc}
\hline
\textbf{Method} & \textbf{R-1}   & \textbf{R-2}   & \textbf{R-L}   & \textbf{B-S}  \\
\hline
CODS\cite{wu2021controllable}            & 44.27          & 17.90          & 36.98          & 70.49         \\
SICK\cite{kim2022mind}            & {\ul 46.2}     & {\ul 20.39}    & \textbf{40.83} & {\ul 71.32}   \\
\hline
OmniVec         & \textbf{46.91} & \textbf{21.22} & {\ul 40.19}    & \textbf{71.91} \\
\hline
\end{tabular}
}
\caption{\textbf{DialogueSUM} text summarization ROGUE scores. 
}
\label{tab:sota_dialoguesum}
}

\end{table*}


\noindent\textbf{(iv) Point Cloud.} Table~\ref{tab:sota_mnc} and Table~\ref{tab:sota_s3dis} compare against state of the art methods on ModelNet40-C and S3DIS datasets respectively. On ModelNet40-C, we evaluate a classification task, while on S3DIS we evaluate semantic segmentation. On both the datasets, we outperform the competing method. This demonstrates that the proposed method is able to robust performance with the shared backbone network across tasks. 

\noindent\textbf{(v) Text} 
Table~\ref{tab:sota_dialoguesum} shows state of the art on DialogueSUM dataset for text summarization. OmniVec surpasses other methods in three out of four metrics and comes in second on the R-L metric. Despite utilizing significantly fewer datasets for text (only two) in comparison to visual tasks (ten datasets), OmniVec demonstrates strong performance. This suggests OmniVec's capacity to bridge the modality gap~\cite{liang2022mind} across distinct domains in the latent space, even when the data distribution is skewed.

\subsection{Ablations}

\noindent\textbf{Impact of task grouping and modality mixing.} Table~\ref{tab:ablation} shows the effect of task grouping and modality mixing. We evaluate four network variations: (i) OmniVec-1 without either of task grouping and modality mixing, (ii) OmniVec-2 with just task grouping, (iii) OmniVec-3 with only modality mixing, and (iv) OmniVec-4 combining both. OmniVec-1 uses masked pretraining on single datasets. OmniVec-2 groups tasks by modality, OmniVec-3 mixes modalities randomly, and OmniVec-4 follows the settings from Section \ref{sec:approach}. Comparatively, OmniVec-1 lags behind the others. Both OmniVec-2 and OmniVec-3 outperform OmniVec-1 by around 30\% to 45\%, showing their efficacy. However, OmniVec-4, which combines both approaches, performs better, emphasizing the benefits of integrating tasks and modalities.

\begin{table*}
\centering
\resizebox{\textwidth}{!}{ 
\begin{tabular}{ccccccc}
\hline
\textbf{Dataset} & \textbf{Modality} & \textbf{Task} & \textbf{Metric}  & \textbf{OmniVec (Pre.)} & \textbf{OmniVec (FT.)}  & \textbf{SOTA}        \\
\hline
UCF-101 &  Video & Action Recognition  &   3-Fold Accuracy  & {\ul 98.7} & \textbf{99.6}  &   \textbf{99.6} (VideoMAE V2-g\cite{wang2023videomae}) \\
HMDB51 & Video  & Action Recognition  &   3-Fold Accuracy  & {\ul 89.21} & \textbf{91.6}  &   88.1 (VideoMAE V2-g\cite{wang2023videomae}) \\
Oxford-IIIT Pets &  Image & Fine grained classification  &   Top-1 Accuracy  & \uline{97.4} &  \textbf{99.2} &   97.1 (EffNet-L2\cite{foret2020sharpness}) \\
ScanObjectNN   & 3D Point Cloud & Classification  &   Accuracy  & 92.1 & \textbf{96.1}  &   \uline{93.4} (PointGPT\cite{chen2023pointgpt}) \\
NYU V2 & RGBD  & Semantic Segmentation  &   Mean IoU  & \uline{58.6} &  \textbf{60.8} &   56.9 (CMN\cite{liu2022cmx}) \\
SamSum & Text  & Meeting Summarization  &   ROGUE(R-L)  & \uline{51.2} &   \textbf{54.6} &   50.88 (MoCa\cite{zhang2022momentum}) \\
KITTI & RGB & Depth Prediction  &   iRMSE  & - & \textbf{10.2}  &   \uline{10.4} (VA-DepthNet\cite{liu2023va}) \\
YouCook2 & Video+Text  & Zero Shot Text-to-Video Retrieval  &   Recall@10  & \uline{64.2} &  \textbf{70.8} &   63.1 (VideoCLIP\cite{xu2021videoclip}) \\
MSR-VTT & Video+Text  & Zero Shot Text-to-Video retrieval  &   Recall@10  & 78.6 & \uline{89.4}  &  80.0(Pre.)/\textbf{90.8}(FT)(SM\cite{zeng2022socratic}) \\
\hline
\end{tabular}
}
\caption{\textbf{Generalization performance of OmniVec} on \textit{unseen datasets} (Oxford-IIIT Pets, UCF-101, HMDB51, ScanObjectNN, NYUv2 Seg, SamSum), \textit{unseen tasks} (KITTI Depth Prediction) and \textit{cross-domain} generalization (YouCook2, MSR-VTT). Pre. indicates network with pretraining only, FT indicates network finetuned on training set of respective datasets. See supplementary for more detailed results.}
\label{tab:generalization}
\end{table*}


\noindent\textbf{Influence of size of the modality encoder.} 
We evaluated the impact of enlarging the base modality encoder to the scale of our suggested network, using modality-specific data. This change slightly improved performance. For example, on ImageNet1K, the top-1 accuracy went from 88.5\% with the base ViT~\cite{dosovitskiy2020image} to 89.1\% with the augmented ViT having a similar parameter count, while OmniVec achieved 92.4\%. These findings suggest that even with enhancements, the augmented base modality encoder still lags significantly behind OmniVec, highlighting OmniVec's advantage of leveraging information from multiple modalities.

\noindent\textbf{Fine-tuning with the same datasets after masked pretraining and comparison to state-of-the-art.} In Table~\ref{tab:sota}, we show the results of fine-tuning the OmniVec-4 model on each of the datasets that was used for masked pretraining. As during masked pretraining, we use the standard train sets for each of these datasets for fine-tuning. 

It can be observed from the results that OmniVec achieves better performance on each dataset than existing state of the art method. As we are using same backbone (OmniVec-4) for each of these datasets, it shows the robustness of the embeddings and the capacity of the network to adapt to different tasks and distribution of dataset.

\begin{table}[h]
\centering
\resizebox{\columnwidth}{!}{ 
\begin{tabular}{cccc}
\hline
\textbf{Dataset} & \textbf{Metric}  & \textbf{OmniVec} & \textbf{SOTA}        \\
\hline
AudioSet(A)      & mAP              &  54.8                & 53.3 (MAViL~\cite{huang2022mavil})         \\
AudioSet(A+V)    & mAP              &      55.2            & 51.2 (CAV-MAE~\cite{gong2022contrastive})          \\
SSv2             & Top-1 Acc   &    85.4              & 77.3 (MVD~\cite{wang2022masked})           \\
ImageNet1K       & Top-1 Acc   &      92.4            & 91.1 (BASIC-L~\cite{chen2023symbolic})       \\
Sun RGBD         & Top-1 Acc   &    74.6              & 67.2 (Omnivore~\cite{girdhar2022omnivore})      \\
ModelNet40       & Overall Acc &    96.6              & 95.4 (GeomGCNN~\cite{srivastava2021exploiting}) \\
\hline
\end{tabular}
}
\caption{\textbf{Comparison with state of the art} after fine tuning on respective training sets.}
\label{tab:sota}
\end{table}

\begin{figure*}[t]
    \centering
     \includegraphics[width=0.88\textwidth]{kitti_qual_supp.eps}
    \caption{Qualitative results on test set of KITTI Depth Prediction. Ground truth is not available. For an RGB input image (left), the outputs from VA-DepthNet~\cite{liu2023va}(middle) and OmniVec (right) are shown. See supplementary material for more qual.~results.}
    \label{fig:kitti_qual}
\end{figure*}

\subsection{Generalization Ability}



\noindent\textbf{Generalization on unseen datasets.} We evaluate the performance of the learned embeddings on unseen datasets. Specifically, we show results on the tasks of fine grained image classification (Oxford-IIIT Pets\cite{parkhi2012cats}), Video Classification (UCF-101\cite{soomro2012ucf101}, HMDB51\cite{kuehne2011hmdb}), 3D point cloud classification (ScanObjectNN\cite{uy2019revisiting}), 3D point cloud segmentation (NYUv2\cite{silberman2012indoor}) and text summarization (SamSum\cite{gliwa2019samsum}). Our findings, tabulated in Table \ref{tab:generalization} [rows 1-6], demonstrates that even without fine-tuning, OmniVec surpasses most state-of-the-art methods. Further, while the pretrained OmniVec slightly underperformed on ScanObjectNN (\%) compared to PointGPT's \%, when fine-tuned, OmniVec achieved \% accuracy, outperforming PointGPT. This shows OmniVec's generalizability on datasets where it is exposed to analogous tasks. 

\noindent\textbf{Generalization on unseen tasks - Monocular Depth Prediction on KITTI Depth Prediction Benchmark.} We fine tune the network for the task of depth prediction on KITTI Depth Prediction benchmark~\cite{Uhrig2017THREEDV}. Our network has not seen such image to image style transfer tasks. The results on KITTI depth prediction benchmark are shown in Table~\ref{tab:generalization} (row 7). We outperform the state of the method VA-DepthNet~\cite{liu2023va} \ie 10.44 iRMSE on VA-DepthNet \cf 10.2 for OmniVec. As can be observed from Figure~\ref{fig:kitti_qual}, the depth maps obtained by OmniVec are able to better capture the details near edges. 

\noindent\textbf{Cross-domain generalization.} Following prior work~\cite{akbari2021vatt}, we evaluate on the task of zero-shot text-to-video retrieval. 

The results are reported in Table \ref{tab:generalization}. On the YouCook2 dataset, our pretrained OmniVec surpasses the state of the art in zero-shot retrieval, achieving a Recall@10 of 64.2\% compared to VideoCLIP's 63.1\%. On MSR-VTT, when compared with SM~\cite{zeng2022socratic}, our fine-tuned OmniVec embeddings yield a Recall@10 of 89.4\% against SM's 90.8\%. With just pretraining, SM has a Recall@10 of 80\%, slightly above our 78.6\%. SM utilizes large-scale pretraining on internet scale data, while OmniVec uses much less data. Further, the second-best MSR-VTT method~\cite{chen2023vast} achieves only 73.9\% Recall@10 (see supplementary), which is behind our pretrained OmniVec.