\documentclass{article}
\usepackage{spconf,amsmath,graphicx}
\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         \usepackage[numbers]{natbib}
\usepackage{amsmath}
\usepackage{svg} \usepackage{graphicx}
\usepackage{bm}
\usepackage{color}
\usepackage{tabularray}
\def\x{{\mathbf x}}
\def\L{{\cal L}}

\title{Spiking-Diffusion: Vector Quantized Discrete Diffusion Model with Spiking Neural Networks}
\name{Mingxuan Liu
\qquad Jie Gan
\qquad Rui Wen 
\qquad Tao Li 
\qquad Yongli Chen 
\qquad Hong Chen 
\thanks{Mingxuan Liu, Jie Gan and Rui Wen are co-first authors.}
\thanks{This work is supported by the National Science and Technology Major Project from Minister of Science and Technology, China (Grant No.
2018AAA0103100), and  the Laboratory Open Fund  of Beijing Smartchip Microelectronics Technology Co., Ltd (No. SGTYHT/21-JS-223).(Corresponding author: Hong Chen.)}}

 \address{ Tsinghua University \\
       Beijing Smartchip Microelectronics Technology Co., Ltd}
\begin{document}
\topmargin=0mm
\maketitle
\begin{abstract}
Spiking neural networks (SNNs) have tremendous potential for energy-efficient neuromorphic chips due to their binary and event-driven architecture. SNNs have been primarily used in classification tasks, but limited exploration on image generation tasks. To fill the gap, we propose a Spiking-Diffusion model, which is based on the vector quantized discrete diffusion model. First, we develop a vector quantized variational autoencoder with SNNs (VQ-SVAE) to learn a discrete latent space for images. In VQ-SVAE, image features are encoded using both the spike firing rate and postsynaptic potential, and an adaptive spike generator is designed to restore embedding features in the form of spike trains. Next, we perform absorbing state diffusion in the discrete latent space and construct a spiking diffusion image decoder (SDID) with SNNs to denoise the image. Our work is the first to build the diffusion model entirely from SNN layers. Experimental results on MNIST, FMNIST, KMNIST, Letters, and Cifar10 demonstrate that Spiking-Diffusion outperforms the existing SNN-based generation model. We achieve FIDs of 37.50, 91.98, 59.23, 67.41, and 120.5 on the above datasets respectively, with reductions  of 58.60\%, 18.75\%, 64.51\%, 29.75\%, and 44.88\% in FIDs compared with the state-of-art work. Our code will be available at \url{https://github.com/Arktis2022/Spiking-Diffusion}.
\end{abstract}
\begin{keywords}
Spiking neural networks, Diffusion model, Image generation, Bionic Learning
\end{keywords}
\section{Introduction}
Spiking neural networks (SNNs) are the third generation neural networks with biological plausibility that encode and transmit information in form of spikes by mimicking the dynamics of neurons in the brain. Compared with Artificial neural networks (ANNs), the event-driven nature of SNNs allows for significant reduction in energy consumption when running on neuromorphic chips \cite{b6}.

SNNs with deep learning techniques have shown promising results on simple tasks such as image classification, image segmentation, and optical flow estimation\cite{snn}. However, the scope of their utility in complex tasks, particularly image generation, is still confined. Spiking-GAN \cite{b17} is the first fully SNN-based GAN that uses time-to-first-spike (TTFS) encoding to generate MNIST images through adversarial training. Despite its significant progress, the quality of the generated images still falls short of ANN-based GAN. Another model, fully spiking variational autoencoder (FSVAE) \cite{b22}, is a VAE model constructed entirely from SNN layers. This model produces comparable results to those of ANN-based VAEs, but its performance is still constrained by the inherent limitations of VAEs, namely the weak generative capacity due to the overly simplified decoder and compressed latent space \cite{b24}. Thus, more exploration and development are needed to harness the full potential of SNNs in image generation tasks. 

In this paper, we propose a vector quantized discrete diffusion model with spiking neural networks (called Spiking-Diffusion), which is the first diffusion model using only SNN layers. The pipeline of proposed Spiking-Diffusion contains two stages. First, an image is transformed into a discrete matrix through proposed vector quantized spiking variational autoencoder (VQ-SVAE). However, creating VQ-VAEs \cite{b3} in SNNs brings two challenges. The one is converting spike sequences into dense features suitable for storage in the codebook, because storing spike sequences directly within the codebook will consume too much memory. To overcome this challenge, we combine spike frequency rate (SFR) and postsynaptic potential (PSP) to model the spike sequences and introduce the learnable operator  to optimize the weights of PSP and SFR. The other challenge is losslessly converting embedded features to spiking input for SNN decoder, because Poisson coding incurs information loss. To address this challenge, we design an adaptive spike generator (ASG) before the decoder and train it by using the same dictionary learning algorithm as traditional VQ-VAE. In the second stage, we utilize a spiking diffusion image decoder (SDID) to fit the prior discrete latent codes. In the forward process, we select a Markov transition matrix with an absorbing state \cite{b30} to gradually add masks to the discrete matrix, and adopt an SNN denoising network SDID to recover the masks and achieve inverse process parameterization. Experiments on MNIST \cite{b20}, FMNIST \cite{b31}, KMNIST \cite{b32}, Letters \cite{b33}, and Cifar10 \cite{cifar10} show that Spiking-Diffusion outperforms the SOTA SNN-based generative model.

\begin{figure*}[htb]
\centering
\includegraphics[width=0.8\textwidth]{1.pdf}
\caption{The training process of Spiking-Diffusion consists of two stages. Step 1: Compress the images into discrete variables through VQ-SVAE. Step 2: The spiking diffusion image decoder (SDID) models the discrete latent space by reversing the forward diffusion process, which gradually adds masks in the discrete matrix through a fixed Markov chain. Step 3: During the test process, the SDID lifts the masks through an autoregressive process to obtain a discrete matrix with the target distribution.}
\label{fig1}
\end{figure*}

\section{Methodology}
\subsection{SNNs Learning Algorithms}
In this study, we adopt he SNNs learning algorithms in \cite{b41}. Spike neurons in SNNs emulates the behavior of biological neurons by generating discrete pulses, or spikes, in response to input stimuli. The LIF neuron model \cite{b42} is used, which is described by the following dynamic equation:



where  represents the membrane time constant,  denotes the synaptic input current at time step ,  represents the membrane potential of a neuron after charging but before firing a spike, and  denotes the spike generated by a LIF neuron when its membrane potential exceeds the discharge threshold .  is the Heaviside step function, which is equal to 1 when  and 0 otherwise.  stands for the membrane potential after a spike event, which is reset to the  using a hard reset \cite{b43}. If no spike is generated, it is equal to .

\subsection{VQ-SVAE}
Although diffusion models \cite{b5} can generate high-quality images, they need to predict the noise added in the forward step. However, SNNs are difficult to fit and process continuous analog signals due to their discrete spike encoding \cite{b26}.  In order to solve the problem, we propose a VQ-SVAE to learn a discrete latent representation of images.

As shown in Fig. \ref{fig1}, Given a 2D image , we first convert it into sequences  using direct input encoding \cite{coding}, and then the  is encoded as a spike sequence  after the SNN encoder. In order to prevent the codebook from consuming too much memory, we utilize spike firing rate (SFR) and postsynaptic potential (PSP) to model the spike sequence. SFR has been proven in neurobiology to have the ability to represent information. For example, auditory nerves use SFR to encode steady-state vowels \cite{b53}.

PSP simulates the response of postsynaptic neurons to the action potential (AP) sequence. Similarly, studies have shown that PSP is related to experience-dependent plasticity in the mammalian nervous system \cite{b54}. We use the following formula to update the PSP \cite{b55}: 

where  is the synaptic time constant and  is set to 0. We use a trainable operator  to allocate the weights of the PSP and SFR, so the features used for quantized encoding  can be obtained from the following formula:

After obtaining , we construct a codebook , with  and  . The quantized encoding of , denoted as a discrete matrix , can then be derived through nearest neighbor search. According to , each spatial feature  is mapped to its nearest codebook entry  by indexing in , thereby obtaining the embedding feature :


However, the embedding feature  incorporates the SFR and PSP features of spike sequences. Consequently, it cannot be Poisson encoded \cite{b55} as input to the SNN decoder without loss of information. To solve the problem, we use a SNN layer to construct a trainable adaptive spike generator (ASG) that learns to restore  to the original spike sequence . At this stage, all parameters in the network can be trained end-to-end with the following loss function:

In the formula,  denotes the stop gradient operation. Notably, compared with VQ-VAE \cite{b3}, VQ-SVAE imposes an additional constraint on training the ASG, which corresponds to the third term above. Moreover, the straight-through estimator propagates the gradients from the spike sequences  to  during backpropagation, rather than propagating the gradients of the embedded features . Since spike sequences exhibit sparsity, directly computing the mean squared error (MSE) loss between sequences is not the optimal distance metric, therefore, we set the third term in Eq. (9) to maximum mean discrepancy (MMD), whose kernel function is PSP.
\subsection{Absorbing State Diffusion}
The training dataset is encoded into discrete matrices  by the VQ-SVAE to train the spiking diffusion image decoder (SDID). For the forward diffusion process , we use a transition matrix with an absorbing state \cite{b30} to convert all the elements in the discrete matrix into a mask after a fixed number of  time steps. In detail, for each discrete random variable element  in  with  categories, denoting the one-hot version of  with the row vector , we can define the forward process as: 

where  is a categorical distribution parameterized by , and  is the transition matrix of the forward Markov chain. We adopt absorbing state diffusion which is compatible with SNNs. Its transition matrix   can be expressed as follows:


The Eq. (11) indicates that at each timestep, each regular token has probability  of being replaced by the [MASK] token, and probability  of remaining unchanged, but the [MASK] token remains unchanged. In our work, we set the [MASK] token as , as a result,  and each token possesses  states.  Therefore, the transition matrix with the absorbing state is non-zero only on the diagonal and the last column, so the sparsity of transition matrix will help to reduce the computational cost of the forward process.

The reverse process is defined as a Markov chain parameterized by , which gradually denoises from the data distribution 
 and is optimized by the evidence lower bound (ELBO). With  term the loss can be written as:


To reduce the randomness of training, SDID  is employed to predict  rather than learn  directly. As a result, variational bound reduces to:  


\begin{table}
\centering
\caption{Performance comparisons of Spiking-Diffusion(Ours) and FSVAE on SpikingJelly \cite{b41}.}
\label{tb1}
\begin{tblr}{
  cell{2}{1} = {r=2}{},
  cell{4}{1} = {r=2}{},
  cell{6}{1} = {r=2}{},
  cell{8}{1} = {r=2}{},
  cell{10}{1} = {r=2}{},
  cell{10}{3} = {},
  cell{10}{4} = {},
  cell{10}{5} = {},
  cell{10}{6} = {},
  cell{11}{3} = {},
  cell{11}{4} = {},
  cell{11}{5} = {},
  cell{11}{6} = {},
  hline{1-2,4,6,8,10,12} = {-}{},
}
Dataset & Model         & MSE  & SSIM  & KID  & FID  \\
MNIST   & FSVAE         & 0.023             & 0.219              & 0.054             & 90.57             \\
        & \textbf{Ours} & \textbf{0.006}    & \textbf{0.077}     & \textbf{0.018}    & \textbf{37.50}    \\
FMNIST  & FSVAE         & 0.023             & 0.378              & 0.070             & 113.2             \\
        & \textbf{Ours} & \textbf{0.011}    & \textbf{0.233}     & \textbf{0.055}    & \textbf{91.98}    \\
KMNIST  & FSVAE         & 0.054             & 0.452              & 0.272             & 166.9             \\
        & \textbf{Ours} & \textbf{0.014}    & \textbf{0.151}     & \textbf{0.068}    & \textbf{59.23}    \\
Letters & FSVAE         & 0.015             & 0.180              & 0.140             & 95.96             \\
        & \textbf{Ours} & \textbf{0.005}    & \textbf{0.078}     & \textbf{0.075}    & \textbf{67.41}    \\
Cifar10 & FSVAE         & 0.024             & 0.689              & 0.181             & 218.6             \\
        & \textbf{Ours}          & \textbf{0.009}    & \textbf{0.399}     & \textbf{0.121}    & \textbf{120.5}    
\end{tblr}
\end{table}
\begin{figure}[htb]
\centering
\includegraphics[width=0.45\textwidth]{3.png}
\caption{Generated images of FSVAE and our Spiking-Diffusion (From top to down: MNIST, FMNIST, KMNIST, and Letters).}
\label{fig2}
\end{figure}

\section{Experiments}
\subsection{Datasets}
We test our model on five datasets: The MNIST \cite{b20} consists of 60,000 handwritten digit images divided into 10 classes (digits 0 to 9); FMNIST \cite{b31} contains 60,000 images of 10 different classes of clothing; KMNIST \cite{b32} consists of 60,000 images of Japanese characters, with each image belonging to one of 10 classes; Letters \cite{b33} provides 145,000 images of 26 Latin letters; for Cifar10\cite{cifar10}, 50,000 images are used for training and 10,000 images for evaluation.
\subsection{Implementation details}
We make a comprehensive comparison between Spiking-Diffusion and FSVAE \cite{b22}, which is the state-of-the-art SNN-based generative model generating images with quality equal to or better than ANN-based models. We test the FSVAE and Spiking-Diffusion on all datasets with the same experimental setup. The LIF neuron model's surrogate gradient function is , and its derivative is , where  represents the slope parameter. For all neurons, , , and . The Optimizer is AdamW with  and weight decay 0.001. The training lasts for 100 epochs. In addition, the original FSVAE used the STBP-tdBN \cite{b35} framework to train SNNs, while we adopt SpikingJelly \cite{b41} in our work. 

\subsection{Evaluation Metrics and Results}
\textbf{Evaluation Metrics.} We evaluate the reconstruction ability of the model by comparing the input and output images using two metrics: mean squared error (MSE) loss and structural similarity (SSIM) loss. To assess the quality of sampled images, we use two commonly used scores: Kernel Inception Distance (KID) and Fr\'{e}chet inception distance (FID). 

\textbf{Results.} Table \ref{tb1} shows that our Spiking-Diffusion model outperforms FSVAE in reconstruction and generation with the SpikingJelly framework. Spiking-Diffusion yields lower MSE and SSIM losses of reconstruction quality, and lower FID and KID scores of generated image quality. Figure \ref{fig2} shows the examples of generated images on the four datasets, from which we find that with the same SNN encoder and decoder structures, the images generated by FSVAE are blurrier, while the images generated by our Spiking-Diffusion are clearer with obvious boundaries. The is because that only the Bernoulli distribution is used in FSVAE to approximate the true posterior distribution, which cannot fully capture the rich semantic information in the image. In contrast, the Spiking-Diffusion can approximate the complex posterior distribution through nearest neighbor search and generate images by seeing context.

\section{Conclusion}
In this work, we present Spiking-Diffusion, the first implementation of diffusion models in SNNs. Our approach is based on VQ-DDM, which has two stages. First, we train the VQ-SVAE model to achieve image reconstruction, by using both PSP and SFR to model spiking features and constructing a codebook for image discretization. Secondly, we construct a discrete diffusion model in the discrete feature domain of images, and using absorbing diffusion state in Spiking-Diffusion is proven to generate highe quality images. The experimental results demonstrate that Spiking-Diffusion currently outperforms other SNN-based generative models. Future work will focus on how to train larger scale SNN generative models.


\bibliographystyle{IEEEbib}
\bibliography{strings,refs}

\end{document}
