


\subsection{Additional Related Work}
\label{apdx:related}
{\bf Improving Expressiveness of GNNs:} Several works other than those mentioned in Sec.\ref{sec:intro} tackle expressive GNNs. \citet{murphy2019relational} achieve universality by summing permutation-sensitive functions across a combinatorial number of permutations, limiting feasibility. \citet{coloring} adds node indicators to make them distinguishable, but at the cost of an invariant model, while \citet{NEURIPS2020_a32d7eea} further addresses the invariance problem, but at the cost of quadratic time complexity. \citet{pna} generalizes MPNN's default sum aggregator, but is still limited by -WL.  \citet{dgn_icml21} generalizes spatial and spectral aggregation with -WL expressiveness, but using expensive eigendecomposition.  Recently, \cite{mpsn_icml21} introduce MPNNs over simplicial complexes that shares similar expressiveness as \method. \revise{\cite{ying2021transformers} studies transformer with above 1-WL expressiveness.} \cite{azizian2021expressive} surveys GNN expressiveness work.


\textbf{Connections to CNN and -WL:} 
\method has a similar convolutional structure as CNN, and in fact historically many spatial GNNs are inspired by CNN; see \citet{wu2020comprehensive} for a detailed survey. The non-Euclidean nature of graphs makes such generalizations non-trivial. MoNet \citep{monet} introduces pseudo-coordinates for nodes, while PatchySAN \citep{patchy} learns to order and truncate neighboring nodes for convolution purposes. However, both methods aim to mimic the formulation of the CNN without admitting the inherent difference between graphs and images. In contrast, \method, generalizes CNN to graphs with a base GNN kernel, similar to how a CNN kernel encodes image patches. \method also shares connections with two variants of -WL test algorithms: depth- -dim WL \citep{cai1992optimal, weisfeiler1976construction} and deep WL \citep{arvind2020weisfeiler}.  The former recursively applies 1-WL to all size- subgraphs, with slightly weaker expressiveness than -WL, and the latter reduces the number of such subgraphs for the -WL test.  Instead of working on all O size- subgraphs, we keep linear scalability by only applying 1-WL-equivalent MPNNs to O() rooted subgraphs. 


\subsection{Sampling by SubgraphDrop}
\label{apdx:subgraphdrop}

\begin{figure}[h]
    \centering
    \includegraphics{figs/GNN-AK-S.pdf}
    \vspace{-0.15in}
    \caption{\method-S with SubgraphDrop used in training. \method-S first extracts subgraphs and subsamples  subgraphs to cover each node at least  times with multiple strategies. The base GNN is applied to compute all intermediate node embeddings in selected subgraphs. Context encodings are scaled to match evaluation. Subgraph and centroid encodings initially only exist for  root nodes of selected subgraphs, and are propagated to estimate those of other nodes.}
    \label{fig:sampling}
\end{figure}

The descriptions of subgraph sampling strategies are as follows. Fig. \ref{fig:sampling} shows an overview of \textsf{\small{SubgraphDrop}}.
\begin{compactitem}[\textbullet]
    \item {\bf Random sampling} selects subgraphs randomly until every node is covered  times. \item {\bf Farthest sampling} selects subgraphs iteratively, starting at a random one and greedily selecting each subsequent one whose root node is farthest w.r.t. shortest path distance from those of already selected subgraphs, 
until every node is covered  times. \item {\bf Min-set-cover sampling}  initially selects a subgraph randomly, and follows the greedy minimum set cover algorithm to iteratively select the subgraph containing the maximum number of uncovered nodes, until every node is covered  times. \end{compactitem}



\subsection{Proof of Theorem \ref{thm:equivalence}}\label{apdx:proof-of-equi}
\begin{proof}
\citep{xu2018powerful} proved that with sufficient number of layers and all injective functions, MPNN is as powerful as 1-WL. Then Eq.\ref{eq:gnn-ak} outputs different vectors for two graphs iff Subgraph-1-WL encodes different labels with . With \text{POOL} in Eq.\ref{eq:gnn-ak} also to be injective, \mtd{MPNN} outputs different vectors iff Subgraph-1-WL outputs different fingerprints for two graphs. Then \mtd{MPNN} is as powerful as Subgraph-1-WL.
\end{proof}

\subsection{Proof of Theorem \ref{thm:above-1WL}}\label{apdx:proof-of-above-1WL}
\begin{proof}
We first prove that if two graphs are identified as isomorphic by Subgraph-1-WL, they are also determined as isomorphic by 1-WL. Then we present a pair of non-isomorphic graphs that can be distinguished by Subgraph-1-WL but not by 1-WL. Together these two imply that Subgraph-1-WL is strictly more powerful than 1-WL.  Comparing with 2-WL can be concluded from the fact that 1-WL and 2-WL are equivalent in expressiveness \citep{maron2019provably}. In the proof we use 1-hop egonet subgraphs for Subgraph-1-WL.

Assume graphs  and  have the same number of nodes (otherwise easily determined as non-isomorphic) and are two non-isomorphic graphs 
but Subgraph-1-WL determines them as isomorphic. Then for any iteration , set  is the same as set . Then there existing an ordering of nodes  and  with , such that for any node order , . This implies that structure  and  are not distinguishable by 1-WL. Hence  and  are hashed to the same label otherwise the 1-WL that includes the hashed result of  and  can also distinguish  and . Then for any iteration  and any node with order ,  implies that 1-WL fails in distinguishing  and . In fact if we replace the 1-WL hashing function in Subgraph-1-WL to a stronger version , this directly implies the above statement. 

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figs/4-regular-graphs.pdf}
    \caption{Two 4-regular graphs that cannot be distinguished by 1-WL. Colored edges are the difference between two graphs. Two 1-hop egonets are visualized while all other rooted egonets are ignored as they are same across graph  and graph . }\label{fig:regular-graphs}
    \label{fig:my_label}
\end{figure}

In Figure \ref{fig:regular-graphs}, two 4-regular graphs are presented that cannot be distinguished by 1-WL but can be distinguished by Subgraph-1-WL. We visualize the 1-hop egonets that are structurally different among graphs  and . It's easy to see that 's egonet  and 's egonet  can be distinguished by 1-WL, as degree distribution is not the same. Hence,  and  can be distinguished by Subgraph-1-WL. 
\end{proof}

\subsection{Proof of Theorem \ref{thm:not-worse-3WL}}\label{apdx:proof-3WL}
\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figs/3-wl-failed.pdf}
    \caption{Two non-isomorphic strongly regular graphs that cannot be distinguished by 3-WL. }
    \label{fig:3-wl-failed}
\end{figure}
\begin{proof}
We prove by showing a pair of 3-WL failed non-isomorphic graphs can be distinguished by Subgraph-1-WL \revise{(see definition of ``no less powerful'' in \cite{zhengdao2020can}: we call A is no/not less powerful than B if there exists a pair of non-isomorphic graphs that cannot be distinguished by B but can be distinguished by A.)}, assuming  is 3-WL discriminative. Figure \ref{fig:3-wl-failed} shows two strongly regular graphs that can not be distinguished by 3-WL (any strongly regular graphs are not distinguishable by 3-WL \citep{arvind2020weisfeiler}), along with their 1-hop egonet rooted subgraphs. 
Notice that all 1-hop egonet rooted subgraphs in  (also in ) are the same, resulting that Subgraph-1-WL can successfully distinguish  and  if  can distinguish the showed two 1-hop egonets. 
\revise{Now we prove that 3-WL can distinguish these two \emph{subgraphs}. 3-WL constructs a coloring of 3-tuples of all vertices in a graph, and uses the histogram of colors of all -tuples as fingerprint of the graph. Then, different 3-tuples correspond to different colors or bins in the histogram. As a triangle is a unique type of 3-tuple, at iteration 0 of 3-WL, the histogram of all 3-tuples counts the number of triangles in the graph. Notice that 's subgraph has  triangles, and 's subgraph contains 6 triangles. This implies that even at iteration 0, 3-WL can distinguish these two subgraphs.}
Hence, when  is 3-WL expressive, Subgraph-1-WL can distinguish  and . Therefore there exists a pair of 3-WL failed non-isomorphic graphs that can be distinguished by Subgraph-1-WL. 
\end{proof}

\subsection{Proof of Theorem \ref{thm:upper-bound}} \label{apdx:proof-upperbound}
\begin{proof}
For any , \cite{cai1992optimal} provided a scheme to construct a pair of -WL-failed non-isomorphic graphs based on a base graph  with separator size  (more specifically,  can be chosen as a degree-3 regular graph with separator size , see Corollary 6.5 in \cite{cai1992optimal}). 

\revise{
We describe the proposed scheme of constructing the two graphs  and  briefly, for details see Section 6 in \cite{cai1992optimal}. At high level, this scheme first constructs  by replacing every node (with degree ) in  by a carefully designed graph , and then rewires two edges in  to its non-isomorphic counter graph . Specifically, the small graph  is defined as follows.

Hence each vertex  with degree  in  is replaced by a graph  of size , including a ``middle'' section  of size  and  pairs of vertices  representing ``end-points'' of each edge incident with  in the original graph . Let , then two small graphs  and  in  are connected by the following. Let ,  be the pair of vertices associated with  in  and  respectively, then  is connected to , and  is connected to . After constructing ,  is modified from  by rewiring a single edge  in the original graph . Specifically, now in ,  is connected to  and  is connected to . \cite{cai1992optimal} proved that  and  are non-isomorphic, and when base graph  is a degree-3 regular graph with separator size ,  and  are not distinguishable by -WL. See a visualization in Figure \ref{fig:CFI}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{figs/CFI.png}
    \caption{A pair of CFI graphs ( and ) zoomed at the (rewired) edge  of a base graph. The base graph is a degree-3 regular graph with separator size  for -WL-failed case. }
    \label{fig:CFI}
\end{figure}

We provide several useful lemmas related to graph  in the following, which are then used in proving Theorem \ref{thm:upper-bound}.

\begin{lemma}[\cite{cai1992optimal}]\label{lm:autom}
 has exactly  automorphisms, and each automorphism is determined by interchanging  and  for each  in some subset  of even cardinality. 
\end{lemma}
Lemma \ref{lm:autom} is directly taken from  Lemma 6.1 in \cite{cai1992optimal}.

\begin{lemma}\label{lm:diameter}
The shortest path distance between  and  in  for any  is exactly 4, and diameter of  is 4. 
\end{lemma}
\begin{proof}
Based on the construction shown in Eq.\ref{eq:cfi}, for any ,  and  are not directly connected, and any middle nodes share no connection. This implies the shortest path between  and  is larger than 3. Let  be one middle node connected with . We construct a new set , with  and , then  connects with  based on Eq.\ref{eq:cfi}. Additionally,  connects to both  and , which implies the shortest path distance between  and  . Hence, the shortest path distance between  and  is exactly 4.  To show the diameter of  is 4, it's easy to observe that distance() only decreases when replacing  with other nodes in  (or reverse, replacing  with other nodes in ).
\end{proof}

\begin{lemma}\label{lm:ego}
For any , -egonets of  and  in  are isomorphic.
\end{lemma}
\begin{proof}
When ,  based on Lemma \ref{lm:diameter}. Now for , we analyze the -hop egonets of  and  sequentially. First,  and  are two depth-1 trees with equal number of leaves (middle nodes in ), and of course are isomorphic. Similarly,  and  are two depth-2 trees with equal number of depth-1 nodes and equal number of leaves, which are also isomorphic. Last,  and  are the graphs by removing  from  and  from , respectively. It's trivial to observe that they are also isomorphic. Combining all cases proves the Lemma.
\end{proof}

To prove the main theorem,  we visualize the two graphs A and B zoomed at edge  of base graph in Figure \ref{fig:CFI} to help understanding. To prove that Subgraph-1-WL with -egonet (we use  to prevent notation conflict) cannot distinguish  and , we mainly analyze the subgraph around  for both graph  and . For other subgraphs with different root nodes within  shortest-path-distance around nodes , they follow exactly the same argument. For all subgraphs with other root nodes, it's trivial to observe that there is no difference among two subgraphs in  and . 


We introduce some notation first. Let  and  be the two -egonet subgraphs around  in  and  respectively. Let  be the ``left'' part graph (visualized in Figure \ref{fig:CFI}, the left side of , include ) of , and  be the ``right'' part graph (visualized in Figure \ref{fig:CFI}, the right side of , excluding ) of . Then  is reconstructed by connecting  and  with a single edge .  and  are defined in the same way, such that  is reconstructed by connecting  in . 

When , we show that there exists an isomorphism that maps  to . We construct this isomorphism by constructing an isomorphism between  and  and an isomorphism between  and  first. Trivially, the current node ordering in  and  already defines an isomorphism among them, that is, mapping any  () in  to  () in . To find an isomorphism between  and , one can easily observe that for ,  can be 1-to-1 mapped (with  in  being mapped to ) to  and  can be 1-to-1 mapped (with  in  being mapped to ) to  for some . When ,  is mapped to  with additional 2 nodes and  is mapped to  with additional 2 nodes. Then applying Lemma \ref{lm:ego}, when  we can construct an isomorphism between  and  with  in  being mapped to  in . We remark that when , the two additional nodes outside  and  does not affecting applying Lemma \ref{lm:ego}. Last, the combination of the isomorphism between  and  and the isomorphism between  and  is actually a new isomorphism between  and , as the edge  is mapped to edge . Thus, when ,  and  are isomorphic and  no matter what  function is used. Applying the same argument to all other pairs of subgraphs in  and  implies that the histogram encodings of  and  are the same. Hence Subgraph-1-WL with  cannot distinguish the two graphs  and  that are -WL-failed for any .
}
\end{proof}

\revise{
We remark that the theorem doesn't imply that Subgraph-1-WL with -egonet can distinguish  and . This should depend on the base graph. We give an informal sketch. Suppose that for ,  the left part  and right part  are only connected through , that is, removing  from  resulting in   and   being disconnected. Then we can apply Lemma \ref{lm:autom} multiple times with mapping some  in A to  in B and  in A to  in B, until this kind of switches reach the boundary of , resulting in an isomorphism between   and . We refer the reader to Lemma 6.2 in \cite{cai1992optimal}, which also uses this line of argument.
}

\subsection{Proof of Proposition \ref{conj:suffi}} \label{apdx:sufficient-condition}
\revise{
\begin{proof}
The proof is by contradiction. Let  and  be two non-isomorphic graphs and  (if graph sizes are different then  Subgraph-1-WL can trivially distinguish them). Now suppose that Prop. \ref{conj:suffi} is incorrect, i.e. Subgraph-1-WL cannot distinguish  and  even provided that the two conditions (1) and (2) are satisfied. Then, for any iteration of Subgraph-1-WL,  and  would have the \textit{same} histogram of subgraph colors. Now we focus on iteration 0. Formally, let color  and  for a node order  ( maps index  to a node in ). According to Condition (1): for any node reordering  and ,  that  and  are non-isomorphic, then we know that  that  and  are non-isomorphic, hence  since by Condition (2)  can distinguish these two subgraphs. However as two graphs have the same histogram of subgraph colors, there must be a  such that  and . Then we can create a new node order  by swapping  and , resulting  and . This process can be repeated until having a new node order  such that , . As  is discriminative enough according to Condition (2), this implies ,  and  are isomorphic, which contradicts with Condition (1). Thus, Prop. \ref{conj:suffi} must be true.
\end{proof}
}

\subsection{Proof of Proposition \ref{conj:plus}}\label{apdx:plus}
\revise{
\begin{proof}
When a pair of non-isomorphic graphs  and  cannot be distinguished by \methodplus, for any layer , the histogram of  in  and  in Eq. \ref{eq:gnn-ak-plus-realization} should be the same. Which implies that for any layer , the histogram of  in  and  in Eq. \ref{eq:gnn-ak-realization} should be the same. Then \method cannot distinguish  and . So for any pair of non-isomorphic graphs, \method cannot distinguish them if \methodplus cannot distinguish them. Thus \methodplus is more powerful than \method. 
\end{proof}
}






\subsection{Dataset Description \& Statistics}

\begin{table}[h!]
\fontsize{8.7}{9.2}\selectfont
 \setlength{\tabcolsep}{1pt}
    \caption{Dataset statistics.}\label{tab:data-semantics}
    \centering
    \begin{tabular}{llcccc}
    \toprule
    Dataset & Task Semantic & \# Cls./Tasks & \# Graphs & Ave. \# Nodes & Ave. \# Edges  \\
    \midrule  
    EXP  & Distinguish 1-WL failed graphs & 2  & 1200 & 44.4 & 110.2\\
    SR25 & Distinguish 3-WL failed graphs & 15 & 15   & 25   & 300\\
    \midrule
    CountingSub. & Regress num. of substructures   & 4& 1500 / 1000 / 2500& 18.8 & 62.6\\
    GraphProp.   & Regress global graph properties & 3& 5120 / 640 / 1280 & 19.5 & 101.1 \\
    \midrule
    ZINC-12K  & Regress molecular property    & 1  & 10000 / 1000 / 1000& 23.1 & 49.8\\
    CIFAR10   & 10-class classification          & 10 & 45000 / 5000 / 10000& 117.6 & 1129.8 \\
    PATTERN   & Recognize certain subgraphs   & 2  & 10000 / 2000 / 2000& 118.9 & 6079.8\\
    \midrule
    MolHIV    & 1-way binary classification   & 1  & 32901 / 4113 / 4113& 25.5 & 54.1\\
    MolPCBA   & 128-way binary classification & 128 & 350343 / 43793 / 43793 & 25.6 & 55.4 \\
    \midrule  
    MUTAG     & Recognize mutagenic compounds & 2 & 188 & 	17.93 & 19.79\\
    PTC-MR    & Classify chemical compounds & 2 & 344 & 14.29 & 14.69 \\
    PROTEINS  & Classify Enzyme \& Non-enzyme & 2 & 1113 & 39.06 & 72.82\\
    NCI1      & Classify molecular & 2 & 4110 & 29.87 & 32.30 \\
    IMDB-B    & Classify movie & 2 & 1000 & 19.77 & 96.53\\
    RDT-B  & Classify reddit thread & 2 & 2000 & 429.63 & 497.75 \\
    \bottomrule
    \end{tabular}
\end{table}

\subsection{Experimental Setup Details}
\label{apdx:experimental-setup}
To reduce the search space, we search hyperparameters in a two-phase approach:  First, we search common ones (hidden size from [64, 128], number of layers  from [2,4,5,6], (sub)graph pooling from [SUM, MEAN] for each dataset using GIN based on validation performance, and fix it for any other GNN and \methodall. While hyperparameters may not be optimal for other GNN models, the evaluation is fair as there is no benefit for \methodall. Next, we search \methodall exclusive ones (encoding types) over validation set using \mtdall{GIN} and keep them fixed for other \mtdall{GNN}.  
We use a -layer base model for \mtdall{GNN}, with exceptions that we tune  number of layers of base model (while keeping total number of layers fixed) for \mtd{GNN}  in simulation datasets (presented in Table \ref{tab:simulation}). 
We use -hop egonets for \mtdall{GNN}, with exceptions that CIFAR10 uses -hop egonet due to memory constraint; PATTERN and RDT-B use random walk based subgraph with walk length=10 and repeat times=5 as their graphs are dense. For \methodall-S,   is set as default. We use farthest sampling for molecular datasets ZINC, MolHIV, and MolPCBA; to speed up further, random sampling is used for CIFAR10 whose graphs are -NN graphs; min-set-cover sampling is used for PATTERN to adapt random walk based subgraph.  
We use Batch Normalization and ReLU activation in all models. For optimization we use Adam with learning rate 0.001 and weight decay 0. All experiments are repeated 3 times to calculate mean and standard derivation. All experiments are conducted on RTX-A6000 GPUs. 






\subsection{Ablation Study}
\label{apdx:ablation}






We present ablation results on various structural components of \method.  
Table \ref{tab:subgraphs-size} shows the performance of \mtd{GIN} for varying size egonets with . We find that larger subgraphs tend to yield improvement, although runtime-performance trade-off may vary by dataset. Notably, simply 1-egonets are enough for CIFAR10 to uplift performance of the base GIN considerably. 
















\begin{table}[!htb]
\fontsize{8}{9.2}\selectfont
\begin{minipage}{.4\linewidth}
      \caption{Effect of various -egonet size.}\label{tab:subgraphs-size}
\centering
        \begin{tabular}{l|cc}
        \toprule
          of \textsf{GIN-AK} & ZINC-12K & CIFAR10 \\
        \midrule  
        GIN & 0.163  0.004 & 59.82  0.33\\
        \midrule 
            & 0.147  0.006 & 71.37  0.28\\
             & 0.120  0.005 & 72.19  0.13\\
         & 0.080  0.001 & OOM\\
        \bottomrule
        \end{tabular}
    \end{minipage}
    
    \begin{minipage}{.6\linewidth}
      \caption{Effect of various encodings }\label{tab:encoding}
        \vspace{-0.1in}
        \centering
        \begin{tabular}{l|cc}
        \toprule
        Ablation of \textsf{GIN-AK} & ZINC-12K & CIFAR10 \\
        \midrule  
        Full & 0.080  0.001 & 72.19  0.13\\
        \midrule 
        w/o Subgraph encoding    & 0.086  0.001 & 67.76  0.29\\
        w/o Centroid encoding    & 0.084  0.003 & 72.20  0.96\\
        w/o Context encoding     & 0.088  0.003 & 69.25  0.30\\
        w/o Distance-to-Centroid & 0.085  0.001 & 71.91  0.22\\
        \bottomrule
        \end{tabular}
    \end{minipage} 
\end{table}

Next Table \ref{tab:encoding} illustrates the added benefit of various node encodings. Compared to the full design, eliminating any of the subgraph, centroid, or context encodings (Eq.s (\ref{eq:subgraph-emb-0})--(\ref{eq:context-emb}))  yields notably inferior results. Encoding without distance awareness is also subpar.  These justify the design choices in our framework and verify the practical benefits of our design. 


\revise{
As \methodplus is not directly explained by Subgraph-1-WL (though D2C is supported by Subgraph-1-WL, by strengthening ), we conduct additional ablation studies over \methodplus with different combinations of removing context encoding and D2C, shown in Table \ref{tab:ctx_d2c}.  Note that all experiments in Table \ref{tab:ctx_d2c} are using a 1-layer base GIN model. We summarize the observations as follows. 

\begin{compactitem}[\textbullet]
\item For real-world datasets (ZINC-12K, CIFAR10), We observe that the largest performance improvement over base model comes from wrapping base GNN with Eq.\ref{eq:gnn-ak} (the performance of \textsf{GIN-AK}), while adding context encoding and D2C monotonically improves performance of \methodplus.

\item For substructure counting and graph property regression tasks, we observe D2C significantly increases the performance of \textsf{GIN-AK w/o Ctx} (supported by the theory of Subgraph-1-WL where the  is required to be injective). Specifically, the cost-free D2C feature enhances the expressiveness of the base 1-layer GIN model (similar to the benefit of distance encoding shown in \cite{li2020distance}), resulting in a more expressive \methodplus, which lies between Subgraph-1-WL and Subgraph-1-WL. We leave the exact theoretical analysis of D2C's expressiveness benefit in future work. Notice that \textsf{GIN-AK} improves the performance over the base model but not in a large margin, we next show this is due to the insufficiency of the number of layers of base GIN.  
\end{compactitem}
\begin{table}[h]
\setlength{\tabcolsep}{1pt}
\vspace{-0.05in}
\fontsize{7.5}{9.2}\selectfont
    \caption{Study \method without context encoding (Ctx) and without distance-to-centroid (D2C). Base model is \textbf{1-layer} GIN for all methods.}\label{tab:ctx_d2c}
    \vspace{-0.1in}
    \centering
    \begin{tabular}{l| c c| cc| cccc | ccc}
    \toprule
     \multirow{2}{*}{Method} &\multirow{2}{*}[-1mm]{\makecell{ZINC-12K \\ (MAE)}} & \multirow{2}{*}[-1mm]{\makecell{CIFAR10 \\ (ACC)}} & \multirow{2}{*}[-1mm]{\makecell{EXP \\ (ACC)}} & \multirow{2}{*}[-1mm]{\makecell{SR25 \\ (ACC)}} & \multicolumn{4}{c|}{Counting Substructures (MAE)} & \multicolumn{3}{c}{Graph Properties ((MAE))} \\
     \cmidrule(l{2pt}r{2pt}){6-9} \cmidrule(l{2pt}r{2pt}){10-12}
                             &      &   & &   & Triangle & Tailed Tri. & Star & 4-Cycle          &  IsConnected & Diameter & Radius \\      
    \midrule
     GIN      & 0.163 & 59.82 &  50\%& 6.67\%& 0.3569 & 0.2373 & 0.0224 & 0.2185 & -1.9239 & -3.3079 & -4.7584 \\
    \midrule
    GIN-AK & 0.094 & 67.51 &  100\%& 6.67\%& 0.2311 & 0.1805 & 0.0207 & 0.1911 & -1.9574 & -3.6925& -5.0574\\
    \midrule
    GIN-AK w/o Ctx      & 0.088 & 69.25 &  100\%& 6.67\%& 0.0130 & 0.0108 & 0.0177 & 0.0131 & -2.7083 & -3.9257 &	-5.2784 \\
    GIN-AK  w/o D2C      & 0.085 & 71.91 &  100\%& 6.67\%& 0.1746 & 0.1449 & 0.0193 & 0.1467 & -2.0521 & -3.6980& -5.0984 \\
    \midrule
    \textsf{GIN-AK}  & 0.080 & 72.19 & 100\%& 6.67\%& 0.0123 & 0.0112 & 0.0150 & 0.0126 & -2.7513 & -3.9687 & -5.1846 \\
    \bottomrule         
    \end{tabular}
    \vspace{-0.1in}
\end{table}

\begin{table}[h]
\setlength{\tabcolsep}{2pt}
\vspace{-0.05in}
\fontsize{7.7}{9.2}\selectfont
    \caption{Study the effect of base modelâ€™s number of layers while keeping total number of layers in \method fixed. Different effect is observed for \method and \method without D2C. }\label{tab:d2c_inner}
    \vspace{-0.1in}
    \centering
    \begin{tabular}{l|c c| cccc | ccc}
    \toprule
     \multirow{2}{*}{Method} & \multirow{2}{*}[-1mm]{\makecell{GIN-AK's\\ \#Layers}} & \multirow{2}{*}[-1mm]{\makecell{Base GIN's\\ \#Layers}} & \multicolumn{4}{c|}{Counting Substructures (MAE)} & \multicolumn{3}{c}{Graph Properties ((MAE))}\\
     \cmidrule(l{2pt}r{2pt}){4-7} \cmidrule(l{2pt}r{2pt}){8-10} 
         & & & Triangle & Tailed Tri. & Star & 4-Cycle          &  IsConnected & Diameter & Radius \\    
     \midrule
     GIN & 0 & 6 & 0.3569 & 0.2373 & 0.0224 & 0.2185 & -1.9239 & -3.3079 & -4.7584\\
     \midrule 
    \multirow{2}{*}{GIN-AK} 
    & 6 & 1& 0.2311 & 0.1805 & 0.0207 & 0.1911 & -1.9574 & -3.6925& -5.0574\\
    & 3 & 2& 0.1556 & 0.1275 & 0.0172 & 0.1419 & -1.9134 & -3.7573 &-5.0100\\
    & 2 & 3& 0.1064 & 0.0819 & 0.0168 & 0.1071 & -1.9259 & -3.7243 &-4.9257\\
    & 1 & 6& 0.0934 & 0.0751 & 0.0216 & 0.0726 & -1.9916 & -3.6555 &-4.9249 \\
     \midrule 
     \midrule 
     \multirow{4}{*}{GIN-AK w/o D2C} & 6 & 1 & 0.1746 & 0.1449 & 0.0193 & 0.1467 & -2.0521 & -3.6980& -5.0984\\
       & 3 & 2 & 0.1244 & 0.1052 & 0.0219 & 0.1121 & -2.1538 & -3.7305 & -4.9250 \\
       & 2 & 3 & 0.1021 & 0.0830 & \BFSERIES 0.0162 & 0.0986 & \BFSERIES -2.2268 & \BFSERIES -3.7585 & \BFSERIES-5.1044\ \\
       & 1 & 6 & \BFSERIES 0.0885 & \BFSERIES 0.0696 & 0.0174 & \BFSERIES 0.0668 & -2.0541 & -3.6834 & -4.8428 \\
      \midrule 
     \multirow{4}{*}{GIN-AK with D2C} & 6 & 1 & 0.0123 & 0.0112 & 0.0150 & 0.0126 & \BFSERIES -2.7513 & \BFSERIES -3.9687 & \BFSERIES -5.1846\\
       & 3 & 2 & 0.0116 & 0.0100 & 0.0168 & 0.0122 & -2.6827 & -3.8407 & -5.1034\\
       & 2 & 3 & 0.0119 & 0.0102 & 0.0146 & 0.0127 & -2.6197 & -3.8745 & -5.1177\\
       & 1 & 6 & 0.0131 & 0.0123 & 0.0174 & 0.0162 & -2.5938 & -3.7978 & -5.0492\\
     \bottomrule
    \end{tabular}
    \vspace{-0.1in}
\end{table}

According to Prop. \ref{conj:suffi}, the base model must be discriminative enough such that , for Subgraph-1-WL with -egonet to enjoy expressiveness benefits. In addition to using D2C to increase the expressiveness of base model, another way is to practically increase the number of layers of the base model (akin to increasing the number of iterations of 1-WL, as in the definition of Subgraph-1-WL). We study the effect of base model's number of layers in \methodplus, with and without D2C in Table \ref{tab:d2c_inner}. \begin{compactitem}[\textbullet]
\item Firstly, \methodplus with D2C is insensitive to the depth of base model, with 1-layer base model being enough to achieve great performance in counting substructures and the best performance in regressing graph properties. We hypothesize that D2C-enhanced 1-layer GIN base model is discriminative enough for subgraphs in the dataset, and without expressiveness bottleneck of base model, increasing \methodplus's depth benefits expressiveness, akin to increasing iterations of Subgraph-1-WL. Besides, unlike counting substructure that needs local information within subgraphs, regressing graph properties needs the graph's global information which can only be accessed with increasing \methodplus's (outer) depth.


\item Secondly, \methodall without D2C suffers from a trade-off between the base model's expressiveness (depth of base model) and the number of \methodall layers (outer depth), which is clearly observed in regressing graph properties. We hypothesize that without D2C the 1-layer GIN base model is not discriminative enough for subgraphs in the dataset, and with this bottleneck of base model, \methodall cannot benefit from increasing the outer depth. Hence the number of layers of the base model are important for the expressiveness of \methodall when D2C is not used.
\end{compactitem}
}


\clearpage
\subsection{Additional Results on TU Datasets}\label{apdx:tu}
We also report additional results on several smaller datasets from TUDataset \citep{Morris+2020}, with their statistics reported in last block of Table \ref{tab:data-semantics}. The training setting and evaluation procedure follows \cite{xu2018powerful} exactly, where we perform 10-fold cross-validation and report the average and standard deviation of validation accuracy across the 10 folds within the cross-validation. We take results of existing baselines directly from \cite{bodnar2021b} with their method labeled as CIN, for references to all baselines see \cite{bodnar2021b}. The result is shown in Table \ref{tab:tud_datasets}.



\begin{table}[!h]
\fontsize{9}{9.2}\selectfont
\centering
    \caption{Results on TU Datasets. First section contains methods of graph kernels, second section has GNNs, and third is the method in \cite{bodnar2021b}. The top two are highlighted by \first{First}, \second{Second}, \third{Third}.}
    \label{tab:tud_datasets}
    \begin{tabular}{l|llllll}
        \toprule
        Dataset & MUTAG &PTC &PROTEINS &NCI1 & IMDB-B & RDT-B \\
        \midrule       
RWK  & 79.22.1 &  55.90.3 & 59.60.1 & 3 days & N/A & N/A \\
        
        GK () &81.41.7 & 55.70.5 &71.40.31 & 62.50.3 &N/A &N/A \\
        
        PK & 76.02.7&  59.52.4 & 73.70.7 &  82.50.5 & N/A & N/A \\

        WL kernel &
        90.45.7 & 
        59.94.3 & 
        75.03.1 & 
        \first{86.01.8} & 
        73.83.9 &
        81.03.1 \\
        \midrule
         
        DCNN & N/A&  N/A &61.31.6 &56.61.0  & 49.11.4 &N/A \\

        DGCNN & 85.81.8 & 58.62.5 &  75.50.9 & 74.40.5 &70.00.9  &N/A \\
        
        IGN &83.913.0 & 58.56.9 &76.65.5 &74.32.7  &  72.05.5 &N/A \\
 
        GIN & 89.45.6 & 64.67.0& 76.22.8 &82.71.7  & 75.15.1 & 
        \third{92.42.5} \\

        PPGNs  &9
        0.68.7 &
        66.26.6 & 
        \first{77.24.7} & 
        83.21.1 & 
        73.05.8 & N/A \\

        Natural GN  &
        89.41.6 &
        66.81.7 &
        71.71.0 &
        82.41.3 &
        73.52.0 &
        N/A\\

        GSN &
        \second{92.2  7.5} &
        \second{68.2  7.2} & 
        76.6  5.0 & 
        83.5  2.0 &
        \first{77.8  3.3} & 
        N/A \\
        
        SIN & 
        N/A  &
        N/A & 
        76.4  3.3 & 
        82.7  2.1 &
        \second{75.6  3.2} & 
        92.2  1.0 \\

        \midrule
        
        CIN & 
        \first{92.7  6.1} &
        \first{68.2  5.6} &
        \third{77.0  4.3} &
        \third{83.6  1.4} &
        \third{75.6  3.7} & 
        \second{92.4  2.1} \\
        \midrule 
        \mtdp{GIN} &
        \third{91.3  7.0} &	
        \third{67.7  8.8} &	
        \second{77.1  5.7} &	
        \second{85.0  2.0} &	
        75.0  4.2 &	
        \first{94.8  0.8}\\
\bottomrule
\end{tabular}
\end{table}

We mark that the performance of \mtdp{GIN} over IMDB-B is not improved because each graph in the dataset is an egonet, hence all nodes have the same rooted subgraph -- the whole graph. The performance of MUTAG and PTC is very unstable, given these datasets are too small: 188 and 344, respectively, and the evaluation method is based on average 10 validation curves over 10 folds.