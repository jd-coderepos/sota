\section{Deferred Proofs} 

\subsection{GSN is permutation equivariant}\label{proof:perm_equiv}

\begin{proof}
Let  the adjacency matrix of the graph,  the input vertex features,  the input edge features and  the edge features at dimension . Let  the functions generating the vertex and edge structural identifiers respectively. 

It is obvious that subgraph isomorphism is invariant to the ordering of the vertices, i.e. we will always obtain the same matching between subgraphs  and graphs  in the subgraph collection. Thus, each vertex  (edge ) in the graph will be assigned the same structural identifiers  () regarless of the vertex ordering, and  and  are permutation equivariant, i.e., for any permutation matrix  it holds that 

where the permutation is applied at each slice  of the tensor .



Let  a GSN layer. We will show that  is permutation equivariant.  We need to show that if  the output of the GSN layer, then  for any permutation matrix . It is easy to see that GSN-v can can be expressed as a traditional MPNN  (similar to Eq. (5) of the main paper) by replacing the vertex features  with the concatenation of the input vertex features and the vertex structural identifiers, i.e. . Thus, 


where in the last step we used the permutation equivariant property of MPNNs. Similarly, we can show that a GSN-e layer is permutation equivariant, since it can be expressed as a traditional MPNN layer by replacing the edge features with the concatenation of the original edge features and the edge structural identifiers .

Overall, a GSN network is permutation equivariant as composition of permutation equivariant functions, or permutation invariant when composed with a permutation invariant layer at the end, i.e. the READOUT function.

\end{proof}

\subsection{Proof of Theorem 3.1: {\modelname\ is at least as powerful as the 1-WL test}}\label{sec:proof_1}

\begin{proof}

To show that GSN it is at least as expressive as the 1-WL test, we will repurpose the proof of Theorem 3 in \cite{xu2018how}  and demand the injectivity of the update function (w.r.t. both the hidden state  and the message ), and the injectivity of the aggregation w.r.t. the multiset of the hidden states of the neighbours . It suffices then to show that if injectivity is preserved then \modelname s are at least as powerful as the 1-WL.



We will show the above statement for vertex-labelled graphs, since traditionally the 1-WL test does not take into account edge labels.\footnote{if one considers a simple 1-WL extension that concatenates edge labels to neighbour colours, then the same proof applies.} We can rephrase the statement as follows: \textit{If GSN deems two graphs ,  as isomorphic, then also 1-WL deems them isomorphic}. Given that the graph-level representation is extracted by a readout function that receives the multiset of the vertex colours in its input (i.e. the graph-level representation is the vertex colour histogram at some iteration ), then it suffices to show that if for the two graphs the multiset of the vertex colours that GSN infers is the same, then also 1-WL will infer the same multiset for the two graphs. 

Consider the case where the two multisets that GSN extracts are the same:  i.e. . Then both multisets contain the same distinct colour/hidden representations with the exact same multiplicity. Thus, it further suffices to show that
if two vertices  (that may belong to the same or to different graphs) have the same GSN hidden representations  at any iteration , then they will also have the same colours , extracted by 1-WL. Intuitively, this means that GSN creates a partition of the vertices of each graph that is at least as fine-grained as the one created by 1-WL. We prove by induction (similarly to \cite{xu2018how}) that GSN model class contains a model where this holds (w.l.o.g. we show that for GSN-v; same proof applies to GSN-e).

For  the statement holds since the initial vertex features are the same for both GSN and 1-WL, i.e. . Suppose the statement holds for , i.e. . Then we show that it also holds for . Every vertex hidden representation at step  is updated as follows: . Assuming that the update function  is injective, we have the following: if , then:
\begin{itemize}[wide, labelwidth=!, labelindent=0pt]
\item , which from the induction hypothesis implies that  
\item , where the message function is defined as in Eq. (5) of the main paper.
Additionally here we require  to be injective w.r.t. the multiset of the hidden representations of the neighbours:\footnote{Lemma 5 from \cite{xu2018how} states that such a function always exists assuming that the elements of the multiset originate from a countable domain}



From the induction hypothesis we know that   implies that  for any  , thus .
\end{itemize}
Concluding, given the update rule of 1-WL: , it holds that 

\end{proof}

\subsection{Proof of Corollary 3.2}\label{sec:proof_universality}
\begin{proof}
In order to prove the universality of GSN, we will show that when the substructure collection contains all graphs of size , then there exists a parametrisation of GSN that can infer the isomorphism classes of all vertex-deleted subgraphs of the graph  (the \textit{deck} of ). 

The reconstruction conjectures states that two graphs with at least three vertices are isomorphic if and only if they have the same deck. Thus, the deck is sufficient to distinguish all non-isomorphic graphs. The deck can be defined as follows:

Let  the set of all possible graphs of size . The vertex-deleted subgraphs of  are by definition all the induced subgraphs of  with size , which we denote as

Then, the deck  can be defined as a vector of size , where at the -th dimension

where  the indicator function. 
The structural feature   for each substructure  and orbit  are computed as follows:

where  if , otherwise it is the bijective mapping function. The deck can be inferred as follows:

where we used that  = 1, since each vertex can be mapped to a single orbit only. Thus, the deck can be inferred by a simple GSN-v parametrisation (a linear layer with depth equal to  that performs orbit-wise summation and division by the constant  for each vertex separately, followed by a sum readout). Since GSN-v can be inferred by GSN-e (Theorem 3.3), then GSN-e is also universal.
\end{proof}


\subsection{Proof of Theorem 3.3}\label{proof_GSNv_GSNe}


\begin{proof}
Without loss of generality we will show Theorem 3.3 for the case of a single substructure . In order to show that GSN-e can express GSN-v, we will first prove the following: \textit{the vertex identifier of a vertex  can be inferred by the edge identifiers of its incident edges}.

To simplify notation we define the following: For an orbit , denote the \textit{orbit neighbourhood} as the multiset of the orbits of the neighbours of  and the \textit{orbit degree} as the degree of any vertex in   :


For brevity we will use the following notation: vertex orbits are indexed as follows  and edge orbits  with . Structural features are denoted accordingly:  and 






Let's assume that there exists only one matched subgraph  and the bijection between  and  is denoted as . Then, for an abitrary vertex  and a vertex orbit , one of the following holds:
\begin{itemize}
    \item . Then  and ,
    \item  and . Then,  and . Note that here the directionality of the edge is important, otherwise we cannot determine the value of  unless we also know the orbit of .
    \item  and . Then,  and since  has exactly  neighbours in , then  has exactly  neighbours in  with vertex orbits . In other words
    
\end{itemize}

Thus, by induction, for  matched subgraphs  with  and , it holds that  and . Then it follows that:




The rest of the proof is straightforward: we will assume a GSN-v using substructure counts of the graph , with  layers and width  defined as in the main paper (Eq. (5))
Then, there exists a GSN-e with  layers, where the first layer has width  and implements the following function: , where:

Note that since  is a universal multiset function approximator, then there exist parameters of  with which the above function can be computed. The next  layers of GSN-e can implement a traditional MPNN where now the input vertex features are  (which is exactly the formulation of GSN-v) and this concludes the proof. 

\end{proof}




\section{Comparison with higher-order Weisfeiler-Leman tests}\label{k-wl-comparison}

\subsection{The WL hierarchy}\label{k-wl}

Following the terminology introduced in \cite{maron2019provably}, we describe the so-called {\em Folklore WL family (-FWL)}. Note that, in the majority of papers on GNN expressivity \cite{morris2019weisfeiler, maron2019provably, chen2020can} another family of WL tests is discussed, under the terminology \textit{-WL} with expressive power equal to -FWL. In contrast, in most graph theory papers on graph isomorphism \cite{DBLP:journals/combinatorica/CaiFI92, furer2017combinatorial, DBLP:conf/fct/ArvindFKV19} the -WL term is used to describe the algorithms referred to as -FWL in GNN papers. Here, we follow the -FWL convention to align with the work mostly related to ours.

The -FWL operates on -tuples of vertices  to which an initial colour  is assigned based on their \textit{isomorphism types} (see section \ref{sr_graphs_wl}), which can loosely be thought of as a generalisation of isomorphism that also preserves the ordering of the vertices in the tuple. Then, at each iteration the colour is refined as follows:

where . 

The multiset  can be perceived as a form of generalised neighbourhood. Observe that all possible tuples in the graph store information necessary for the updates, thus each -tuple receives information {\em from the entire graph}, contrary to the {\em local} nature of the 1-WL test. 


\subsection{Why does 2-FWL fail on strongly regular graphs?}\label{sr_graphs_wl}

\begin{proof}

We first formally define what an isomorphism type is and what are the properties of the SR family:

\begin{definition}[Isomorphism Types] 
Two k-tuples , 
 will have the same isomorphism type iff:

\begin{itemize}
    \item  
    \item  , where  means that the vertices are adjacent.
\end{itemize}
\end{definition}

Note that this is a stronger condition than isomorphism, since the mapping between the vertices of the two tuples needs to preserve order. In case the graph is employed with edge and vertex features, these need to be preserved as well (see \cite{chen2020can}) for the extended case). 

\begin{definition}[Strongly regular graph] 
A {\em SR(,,,)-graph} is a regular graph with  vertices and degree , where every two adjacent vertices have always  mutual neighbours, while every two non-adjacent vertices have always  mutual neighbours. 
\end{definition}


Now we can proceed to the details of the proof. For the 2-FWL test, when working with simple undirected graphs without self-loops, we have the following 2-tuple isomorphism types:
\begin{itemize}
    \item : \textit{vertex type}. Mapped to the colour 
    \item  and :\textit{ non-edge type}. Mapped to the colour 
    \item  and : \textit{edge type}. Mapped to the colour 
\end{itemize}
 
 For each 2-tuple , a generalised ``neighbour'' is the following tuple: , where  is an arbitrary vertex in the graph. 

Now, let us consider a strongly regular graph SR(,,,). We have the following cases:

\begin{itemize}
    \item generalised neighbour of a  \textit{vertex type} tuple: . The corresponding neighbour colour tuples are:
    \begin{itemize}
        \item   if  ,
        \item  if  ,
        \item  if .
    \end{itemize}
    The update of the 2-FWL is: n-1-dd same for all \textit{vertex type} 2-tuples.
    
    \item generalised neighbour of a \textit{non-edge type} tuple: . The corresponding neighbour colour tuples are:
    \begin{itemize}
        \item  if ,  
        \item  if , 
        \item  if  and , 
        \item  if  and , 
        \item  if  and , 
        \item  if  and .
    \end{itemize}
     The update of the 2-FWL is:
     
     d-\mud-\mun-2 - (2d - \mu)\mu
same for all \textit{non-edge type} 2-tuples.

    
    \item generalised neighbour of an \textit{edge type} tuple:  
    \begin{itemize}
        \item  if , 
        \item  if , 
        \item  if  and , 
        \item  if  and , 
        \item  if  and , 
        \item  if  and .
    \end{itemize}
    The update of the 2-FWL is:
     
     d-\lambdad-\lambdan-2-(2d - \lambda)\lambda
same for all \textit{edge type} 2-tuples.
\end{itemize}

From the analysis above, it is clear that all 2-tuples in the graph of the same initial type are assigned the same colour in the 1st iteration of 2-FWL. In other words, the vertices cannot be further partitioned, so the algorithm terminates. Therefore, if two SR graphs have the same parameters ,,, then 2-FWL will yield the same colour distribution and thus the graphs will be deemed isomorphic.

\end{proof}

\section{Experimental Settings - Additional Details}\label{experiments_supp}

In this section, we provide additional implementation details of our experiments. All experiments were performed on a server equipped with 8 Tesla V100 16 GB GPUs, except for the Collab dataset where a Tesla V100 GPU with 32 GB RAM was used due to larger memory requirements.
Experimental tracking and hyperparameter optimisation were done via the Weights \& Biases platform (wandb) \cite{wandb}. Our implementation is based on native PyTorch sparse operations \cite{paszke2019pytorch} in order to ensure complete reproducibility of the results. PyTorch Geometric \cite{DBLP:journals/corr/abs-1903-02428} was used for additional operations (such as preprocessing and data loading).

In each one of the different experiments we aim to show that \textbf{structural identifiers can be used off-the-shelf and independently of the architecture.} At the same time we aim to suppress the effect of other confounding factors in the model performance, thus wherever possible we build our model on top of a baseline architecture. For more details, please see the relevant subsections. Interestingly, we observed that in most of the cases it was sufficient to replace only the first layer of the baseline architecture with a GSN layer, in order to obtain a boost in performance.

Throughout the experimental evaluation the structural identifiers  and  are one-hot encoded, by taking into account the unique count values present in the dataset. Other more sophisticated methods can be used, e.g. transformation to continuous features via a normalisation scheme or binning. However, we found that the number of unique values in our datasets were usually relatively small (which is a good indication of recurrent structural roles) and thus such methods were not necessary.



\subsection{Synthetic Experiment}

For the Strongly Regular graphs dataset (available from \url{http://users.cecs.anu.edu.au/~bdm/data/graphs.html}) we use all the available families of graphs with size of at most 35 vertices: 

\begin{itemize}
    \item SR(16,6,2,2): 2 graphs
    \item SR(25,12,5,6): 15 graphs
    \item SR(26,10,3,4): 10 graphs
    \item SR(28,12,6,4): 4 graphs
    \item SR(29,14,6,7): 41 graphs
    \item SR(35,16,6,8): 3854 graphs
    \item SR(35,18,9,9): 227 graphs
\end{itemize}

The total number of non-isomorphic pairs of the same size is . We used a simple 2-layer architecture with width 64. The message aggregation was performed as in the general formulation of Eq. (5) of the main paper, where the update and the message functions are MLPs. The prediction is inferred by applying a sum readout function in the last layer and then passing the output through a MLP. Regarding the substructures, we use \textit{graphlet} counting, as certain \textit{motifs} (e.g. cycles of length up to 7) are known to be unable to distinguish strongly regular graphs (since they can be counted by the 2-FWL \cite{furer2017combinatorial, DBLP:conf/fct/ArvindFKV19}).

Given the adversities that strongly regular graphs pose in graph isomorphism testing, it would be interesting to see how this method can perform in other categories of hard instances, such as the classical  \textit{CFI} counter-examples for k-WL proposed in \cite{DBLP:journals/combinatorica/CaiFI92}, and explore further its expressive power and combinatorial properties. We leave this direction to future work.


\subsection{TUD Graph Classification Benchmarks}

For this family of experiments, due to the usually small size of the datasets, we choose a parameter-efficient architecture, in order to reduce the risk of overfitting. In particular, we follow the simple GIN architecture \cite{xu2018how} and we concatenate structural identifiers to vertex or edge features depending on the variant. Then for GSN-v, the hidden representation is updated as follows:

and for GSN-e:

where  is a dummy variable (also one-hot encoded) used to distinguish self-loops from edges. Empirically, we did not find training the  parameter used in GIN to make a difference. 
 


        
         
         
         
         
         
        
       
        


                
        
        

         
         
         
         
         


        

        
        
        

        
        
        
        
        
        

We implement an architecture similar to GIN \cite{xu2018how}, i.e. \textit{message passing layers}: 4 , \textit{jumping knowledge} from all the layers \cite{DBLP:conf/icml/XuLTSKJ18} (including the input), \textit{transformation of each intermediate graph-level representation}: linear layer, \textit{readout}: sum for biological and mean  for social networks. Vertex features are one-hot encodings of the categorical vertex labels. Similarly to the baseline, the hyperparameters search space is the following: \textit{batch size} in \{32, 128\} (except for Collab where only 32 was searched due to GPU memory limits), \textit{dropout} in \{0,0.5\}, \textit{network width} in \{16,32\} for biological networks, 64 for social networks, \textit{learning rate} in \{0.01, 0.001\}, \textit{decay rate} in \{0.5,0.9\} and \textit{decay steps} in \{10,50\} (number of epochs after which the learning rate is reduced by multiplying with the decay rate). For social networks, since they are not attributed graphs, we also experimented with using the degree as a vertex feature, but in most cases the structural identifiers were sufficient. 


Model selection is done in two stages. First, we choose a substructure that we perceive as promising based on indications from the specific domain: \textit{triangles} for social networks and Proteins, and \textit{6-cycles (motifs)} for molecules. Under this setting we tune model hyperparameters for a GSN-e model. Then, we extend our search to the parameters related to the substructure collection: i.e. \textit{the maximum size } and \textit{motifs vs graphlets}. In all the molecular datasets we search cycles with , except for NCI1, where we also consider larger sizes due to the presence of large rings in the dataset (\textit{macrocycles} \cite{liu2017surveying}).  For social networks, we searched cliques with .  
In Table \ref{tab:tud_datasets_hyperparams} we report the hyperparameters chosen by our model selection procedure, including the best performing substructures.



The seven datasets\footnote{more details on the description of the datasets and the corresponding tasks can be found at \cite{xu2018how}.} we chose are the intersection of the datasets used by the authors of our main baselines: the Graph Isomorphism Network (GIN) \cite{xu2018how}, a simple, yet powerful GNN with expressive power equal to the 1-WL test, and the Provably Powerful Graph Network (PPGN) \cite{maron2019provably}, a polynomial alternative to the Invariant Graph Network \cite{DBLP:conf/iclr/MaronBSL19}, that increases its expressive power to match the 2-FWL.
We also compare our results to other GNNs as well as Graph Kernel approaches. Our main baseline from the GK family is the Graph Neural Tangent Kernel (GNTK) \cite{DBLP:conf/nips/DuHSPWX19}, which is a kernel obtained from a GNN of infinite width. This operates in the Neural Tangent Kernel regime \cite{DBLP:conf/nips/JacotHG18, DBLP:conf/icml/Allen-ZhuLS19, DBLP:conf/icml/DuLL0Z19}.



\begin{table}[t]
    \centering
    \caption{Chosen hyperparameters for each of the two GSN variants for each dataset.}
    \label{tab:tud_datasets_hyperparams}
     \resizebox{\textwidth}{!}{
     {\small
    \begin{tabular}{p{0.8cm}p{3cm}|llll|lll}
        & Dataset & MUTAG & PTC & Proteins & NCI1 & Collab & IMDB-B & IMDB-M \\
        
      \hline
        
        \parbox[t]{1mm}{\multirow{9}{*}{GSN-e}} &  batch size & 32 & 128	& 32 & 32 & 32 & 32&  32\\
         
        & width  & 
         32 & 16 &  32 & 32 & 64 & 64 & 64\\
         
     & decay rate  & 0.9 & 0.5 & 0.5 & 0.9 & 0.5 &  0.5&  0.5 \\
     
        & decay steps & 50 & 50 & 10 & 10 & 50 &  10 & 10 \\
         
         & dropout & 0.5 & 0 & 0.5 & 0 & 0 & 0 & 0\\
         
        & lr &  &  &  & & &  & \\
        
        & degree  & No & No & No & No & No & No & Yes\\

         
        & substructure type & 
          graphlets &  motifs & same & graphlets & same & same & same\\

        
        & substrucure family & cycles & cycles & cliques &cycles& clique & clique & cliques\\
        
        & k & 6	& 6 & 4 & 15 & 3 & 5 & 5\\
        
        
        \hline 
         \parbox[t]{1mm}{\multirow{9}{*}{GSN-v}} &  batch size & 32 & 128	& 32 & 32 & 32 & 32&  32\\
         
        & width  & 
         32 & 16 &  32 & 32 & 64 & 64 & 64\\
         
     & decay rate  & 0.9 & 0.5 &0.5 & 0.9 & 0.5 &  0.5&  0.5 \\
     
        & decay steps & 50 & 50 & 10 & 10 & 50 &  10 & 10 \\
         
         & dropout & 0.5 & 0 & 0.5 & 0 & 0 & 0 & 0\\
         
        & lr &  &  &  & & &  & \\
        
        & degree  & No & No & No & No & No & Yes & Yes\\

        
         
        & substructure type & graphlets & graphlets & same& same & same & same & same\\
        

        & substrucure family & cycles & cycles & cliques &cycles& cliques & clique & cliques\\
       

        
        & k  &12 & 10& 4 & 3 & 3 & 4 & 3\\
        \bottomrule
    \end{tabular}}}
\end{table}


\subsection{Graph Regression on ZINC}\label{zinc_appendix}

\noindent\textbf{Experimental Details: }The ZINC dataset includes 12k molecular graphs of which 10k form the training set and the remaining 2k are equally split between validation and test (splits obtained from \url{https://github.com/graphdeeplearning/benchmarking-gnns}). Molecule sizes range from 9 to 37 vertices/atoms. Vertex features encode the type of atoms and edge features the chemical bonds between them. Again, here vertex and edge features are one-hot encoded. 

Our MPNN baseline model updates vertex representations as follows:  ,     . Our instantiation of GSN is a simple extension where structural identifiers are also given as input to the message MLP. 

Following the same rationale as before, the network configuration is minimally modified w.r.t. the baselines provided in \cite{dwivedi2020benchmarking}, while here no hyperparameter tuning is done and we use the default ones provided by the authors. In particular, the parameters are the following: \textit{message passing layers}: 4, \textit{transformation of the output of the last layer}: MLP, \textit{readout}: sum, \textit{batch size}: 128, \textit{dropout}: 0.0, \textit{network width}: 128, \textit{learning rate}: 0.001. The learning rate is reduced by 0.5 (decay rate) after 5 epochs (\textit{decay rate
patience}) without improvement in the validation loss. Training is stopped when the learning rate reaches the \textit{minimum learning rate} value of .  Validation and test metrics are inferred using the model at the last training epoch. 

We select our best performing substructure related parameters based on the performance in the validation set in the last epoch. We search cycles with  , \textit{graphlets vs motifs}, and \textit{GSN-v vs GSN-e}. The chosen hyperparameters for GSN are:  \textit{GSN-e, cycle graphlets of 10 vertices} and for GSN\textit{-EF}: \textit{GSN-v, cycle motifs of 8 vertices}. Once the model is chosen, we repeat the experiment 10 times with different seeds and report the mean and standard deviation of the test MAE in the last epoch. 

        





\noindent\textbf{Disambiguation Scores:} In Table \ref{tab:uniqueness}, we provide the disambiguation scores  as defined in section 5.5.1 of the main paper for different types of substructures. These are computed based on vertex structural identifiers (GSN-v).

\begin{table}[h]
         \centering
            \caption{Disambiguation scores  on \textit{ZINC} for different substructure families and their maximum size . Size  refers to the use of the original vertex features only.}
          \begin{tabular}{r | l | l | l}
            k & Cycles & Paths & Trees \\
            \hline
            0 & 0.196 & 0.196 & 0.196\\
            3 & 0.199 & 0.540 & 0.540\\
            4 & 0.200 & 0.746 & 0.762\\
            5 & 0.256 & 0.866 & 0.875\\
            6 & 0.327 & 0.895 & 0.897\\
            7 & 0.330 & 0.900 & 0.900\\
            8 & 0.330 & 0.901 & 0.901\\
            9 & 0.330 & 0.901 & 0.901\\
            10 & 0.330 & 0.901 & 0.901\\
          \end{tabular}\label{tab:uniqueness}
\end{table}




\subsection{Graph Classification on \texttt{ogbg-molhiv}}\label{app: molhiv}

The \texttt{ogbg-molhiv} dataset contains  41K graphs, with 25.5 vertices and 27.5 edges on average. As most molecular graphs, the average degree is small (2.2) and they exhibit a tree-like structure (average clustering coefficient 0.002). The average diameter is 12 (more details in \cite{DBLP:ogb}). Below we describe how we extend the two base architectures:

\medskip
\noindent\textbf{GIN+VN. } We follow the design choices of the authors of \cite{DBLP:ogb} and extend their architectures to include structural identifiers. Initial vertex and edge features are multi-hot encodings passed through linear layers that project them in the same embedding space, i.e. , .
The baseline model is a modification of GIN that allows for edge features: for each neighbour, the hidden representation is added to an embedding of its associated edge feature. Then the result is passed through a ReLU non-linearity which produces the neighbour's message. Formally, the aggregation is as follows:


In order to allow global information to be broadcasted to the vertices, a \textit{virtual node} takes part in the message passing. The virtual node representation, denoted as , is initialised as a zero vector  and then Message Passing becomes:
  

We modify this model, as follows: first the substructure counts are embedded into the same embedding space as the rest of the features. Then, for GSN-v, they are added to the corresponding vertex embeddings:  , or for GSN-e, they are added to the edge embeddings . 

\medskip
\noindent\textbf{DGN + substructures. } 
We use the directional average operator as defined in \cite{beaini2020directional}:

 where  are weighting average coefficients. In our case, each orbit  induces a separate set of averaging coefficients. For example, for GSN-e 
 , where  denotes edge-wise substructure counts (the index of the orbit  was dropped to simplify notation). Similarly, for GSN-v, .
Subsequently, the vertex representation is updated as follows:  . Observe that this model is simpler than the aforementioned,  in terms of both its parameter count and its expressive power. Since the MOLHIV dataset poses a significant challenge w.r.t. generalisation (the data splits reflect different molecular distributions), architectures biased towards simpler solutions usually perform better, sincey the mitigate the risk of overfitting.


In both cases we use the the same hyperparameters as the ones provided by the authors, and only select the substructure related parameters based on the highest validation ROC-AUC (choosing the best scoring epoch as in \cite{DBLP:ogb}). We search cycles with , \textit{graphlets vs motifs}, and \textit{GSN-v vs GSN-e}. The chosen hyperparameters are: \textit{GSN-e, cycle graphlets of 6 vertices}.
We repeat the experiment 10 times with different seeds and report the mean and standard deviation of the train, validation and test ROC-AUC, again by choosing the best scoring epoch w.r.t the validation set. 





        

        
        
        






\subsection{Structural Features \& Message Passing}\label{deepset_appendix}

The baseline architecture treats the input vertex and edge features, along with the structural identifiers, as a \textit{set}. In particular, we consider each graph as a set of independent edges  endowed with the features of the endpoint vertices , the structural identifiers  and the edge features , and we implement a DeepSets universal set function approximator \cite{zaheer2017deep} to learn a prediction function:
{\small

}
with  the edge set of the graph and  MLPs. This baseline is naturally extended to the case where we consider edge structural identifiers by replacing  with . For fairness of evaluation, we follow the exact same parameter tuning procedure as the one we followed for our GSN models for each benchmark, i.e. for the TUD datasets we first tune network and optimisation hyperaparameters (network width was set to be either equal to the ones we tuned for GSN, or such that the absolute number of learnable parameters was equal to those used by GSN; depth of the MLPs was set to 2) and subsequently we choose the substructure related parameters based on the evaluation protocol of \cite{xu2018how}. For ZINC and ogbg-molhiv we perform only substructure selection, based on the performance on the validation set. Using the same widths as in GSN leads to smaller baseline models w.r.t the absolute number of parameters, and we interestingly observed this to lead to particularly strong performance in some cases, especially Proteins and MUTAG, where our DeepSets implementation attains state-of-art results. This finding motivated us to explore `smaller' GSNs (with either reduced layer width or a single message passing layer). These GSN variants exhibited a similar trend, i.e. to perform better than their `larger' counterparts over these two datasets. We hypothesise this phenomenon to be mostly due to the small size of these datasets, which encourages overfitting when using architectures with larger capacity. In Table 4 in the main paper, we report the result for the best performing architectures, along with the number of learnable parameters.


