\documentclass[journal]{IEEEtran}




\ifCLASSINFOpdf
\usepackage[pdftex]{graphicx}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{caption}
\usepackage{xcolor} 
\usepackage{color} 
\usepackage{verbatim} 
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\else
\fi

\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
\title{Deep Video Super-Resolution using HR Optical Flow Estimation}
	


	\author{\IEEEauthorblockN{Longguang Wang,
			Yulan Guo,
			Li Liu,
			Zaiping Lin, 
			Xinpu Deng, and
			Wei An}
\thanks{
Longguang Wang, Yulan Guo, Zaiping Lin, Xinpu Deng and Wei An are with the College of Electronic Science and Technology, National University of Defense Technology, Changsha, 410073, China (e-mail: wanglongguang15@nudt.edu.cn; yulan.guo@nudt.edu.cn;  linzaiping@nudt.edu.cn; dengxinpu@nudt.edu.cn; anwei@nudt.edu.cn). Yulan Guo is also with the School of Electronics and Communication Engineering, Sun Yat-sen University, Guangzhou, 510275, China.
			Li Liu is with the College of System Engineering, National University of Defense Technology, Changsha, 410073, China (e-mail: 
			liuli\_nudt@nudt.edu.cn).\par
			Corresponding author: Yulan Guo (email: yulan.guo@nudt.edu.cn).}}
	
	
	
\markboth{Journal of \LaTeX\ Class Files,~Vol.~XX, No.~XX, JANUARY~2019}{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Transactions on Magnetics Journals}
	
	
	
\maketitle
	
	
\IEEEdisplaynontitleabstractindextext{
		\begin{abstract}
			Video super-resolution (SR) aims at generating a sequence of high-resolution (HR) frames with plausible and temporally consistent details from their low-resolution (LR) counterparts. The key challenge for video SR lies in the effective exploitation of temporal dependency between consecutive frames. Existing deep learning based methods commonly estimate optical flows between LR frames to provide temporal dependency. However, the resolution conflict between LR optical flows and HR outputs hinders the recovery of fine details. 
			In this paper, we propose an end-to-end video SR network to super-resolve both optical flows and images. Optical flow SR from LR frames provides accurate temporal dependency and ultimately improves video SR performance. Specifically, we first propose an optical flow reconstruction network (OFRnet) to infer HR optical flows in a coarse-to-fine manner. Then, motion compensation is performed using HR optical flows to encode temporal dependency. Finally, compensated LR inputs are fed to a super-resolution network (SRnet) to generate SR results. Extensive experiments have been conducted to demonstrate the effectiveness of HR optical flows for SR performance improvement. Comparative results on the Vid4 and DAVIS-10 datasets show that our network achieves the state-of-the-art performance.
		\end{abstract}
		
\begin{IEEEkeywords}
			Video Super-Resolution, Optical Flow Estimation, Temporal Consistency, Scale-Recurrent Architecture.
		\end{IEEEkeywords}
	}


	
	
\IEEEpeerreviewmaketitle
	
	
	
	\section{Introduction}
\IEEEPARstart{S}{uper}-resolution (SR) aims at generating high-resolution (HR) images from their low-resolution (LR) counterparts. As a typical low-level computer vision problem, SR has been investigated for decades \cite{2001-AComputationallyEfficientSuperresolutionImageReconstructionAlgorithm-Nguyen-573-583,2007-ImageUpsamplingViaImposedEdgeStatistics-Fattal-95-95,2011-ImageandVideoUpscalingfromLocalSelfExamples-Freedman-1-11}. Recently, converting LR videos into HR ones, namely video SR, is under great demand due to the prevalence of high-definition displays. Compared to a single image, adjacent frames in a video clip provide additional information for SR. Therefore, exploiting temporal dependency between consecutive frames plays an important role in video SR.
	
	To exploit temporal dependency between consecutive frames, traditional video SR (or multi-image SR) methods detect recurrent patches across images using patch similarities \cite{2009-GeneralizingtheNonlocalMeanstoSuperResolutionReconstruction-Protter-36-51,2009-SuperResolutionwithoutExplicitSubpixelMotionEstimation-Takeda-1958-1975}. However, these methods can only employ pixel-level dependency and their computational cost is high. To employ sub-pixel dependency, several methods have been proposed to use sub-pixel motion information through optical flow estimation   \cite{2007-OpticalFlowBasedSuperResolution:aProbabilisticApproach-Fransens-106-115,2014-OnBayesianAdaptiveVideoSuperResolution-Liu-346-360,2015-HandlingMotionBlurinMultiFrameSuperResolution-Ma-5224-5232}. These methods formulate the video SR task as an optimization problem and estimate HR images, optical flows and blur kernels alternately. Since a large number of iterations are required to reach convergence, these methods also suffer from high computational costs. 
	
	Motivated by the success of deep learning in single image SR \cite{2014-LearningaDeepConvolutionalNetworkforImageSuperResolution-Dong-184-199,2016-DeeplyRecursiveConvolutionalNetworkforImageSuperResolution-Kim-1637-1645,2017-DeepLaplacianPyramidNetworksforFastandAccurateSuperResolution-Lai-5835-5843}, numerous deep learning based video SR methods have been proposed recently \cite{2015-VideoSuperResolutionViaDeepDraftEnsembleLearning-Liao-531-539,2016-VideoSuperResolutionwithConvolutionalNeuralNetworks-Kappeler-109-122,2017-RealTimeVideoSuperResolutionwithSpatioTemporalNetworksandMotionCompensation-Caballero-2848-2857}. These methods first estimate optical flows from LR frames for motion compensation, and then learn a direct mapping from compensated LR frames to the HR output. Motion compensation encodes temporal dependency in compensated LR frames and facilitates these methods to exploit temporal information from consecutive frames. However, the accuracy of temporal dependency provided by LR optical flows is still low for video SR \cite{2013-SimultaneousSuperResolutionofDepthandImagesUsingaSingleCamera-Lee-281-288}, especially for scenarios with large upscaling factors. 
	
	Since video SR aims at generating high-quality videos with plausible and temporally consistent details, both temporal details and spatial details are important for video SR.  Although existing deep learning based video SR methods \cite{2015-VideoSuperResolutionViaDeepDraftEnsembleLearning-Liao-531-539,2016-VideoSuperResolutionwithConvolutionalNeuralNetworks-Kappeler-109-122,2017-RealTimeVideoSuperResolutionwithSpatioTemporalNetworksandMotionCompensation-Caballero-2848-2857} can successfully hallucinate spatial details from consecutive LR frames, the restoration of temporal details is still under investigated. To address this limitation, we use a convolutional neural network (CNN) to recover HR temporal details in LR frames for video SR. 
	
	In this paper, we propose an end-to-end network to Super-resolve Optical Flows for Video SR (namely, SOF-VSR). Our SOF-VSR network can recover temporal details through optical flow SR, which improves both the accuracy and consistency of video SR. Specifically, we first propose an optical flow reconstruction net (OFRnet) to reconstruct HR optical flows in a coarse-to-fine manner. Different from previous methods \cite{2015-VideoSuperResolutionViaDeepDraftEnsembleLearning-Liao-531-539,2017-RealTimeVideoSuperResolutionwithSpatioTemporalNetworksandMotionCompensation-Caballero-2848-2857,2017-DetailRevealingDeepVideoSuperResolution-Tao-4482-4490} that use optical flows to align LR frames, our OFRnet learns to infer HR optical flows to align latent HR frames. These HR optical flows are then used to perform motion compensation on LR frames. Meanwhile, a space-to-depth transformation is used to bridge the resolution gap between HR optical flows and LR frames.  Finally, these compensated LR frames are fed to a super-resolution net (SRnet) to generate an HR frame. Ablation study is performed to test the effectiveness of HR optical flows for SR performance improvement. Comparative results show that our SOF-VSR network achieves the state-of-the-art performance on the Vid4 and DAVIS-10 datasets.
	
	
	The major contributions of our work can be summarized as follows: 
	
	\begin{itemize}
		\item
		We incorporate the SR of both optical flows and images into a unified SOF-VSR network. The SR of optical flows contributes to the SR of images. Consequently, better performance can be achieved by our SOF-VSR network. 
		\item
		We propose an OFRnet to infer HR optical flows from LR frames in a coarse-to-fine manner. It is demonstrated that OFRnet can recover accurate temporal details for SR performance improvement.
		\item
		Our SOF-VSR network achieves the state-of-the-art performance as compared to recent video SR methods.
	\end{itemize}
	
	This work is an extension of our previous conference version \cite{2018-LearningforVideoSuperResolutionthroughHROpticalFlowEstimation-LongguangWang--} with four notable improvements. \textbf{First}, we introduce a more lightweight and compact architecture for SOF-VSR in this paper. Specifically, techniques including channel split, channel shuffle and depth-wise convolution \cite{Ma2018a} are employed to update our building blocks, and the OFRnet is rebuilt using a scale-recurrent network. Our lightweight SOF-VSR network achieves comparable performance to the original one \cite{2018-LearningforVideoSuperResolutionthroughHROpticalFlowEstimation-LongguangWang--}  with parameters being reduced by over 30\%. \textbf{Second}, we have included additional analyses on the design of our network, including ablation studies on HR optical flows, scale-recurrent architecture and building block. \textbf{Third}, we have conducted additional experiments on different upscaling factors and performed additional evaluation on computational complexity. \textbf{Fourth}, additional experiments have been provided to further test the video SR performance through a face recognition task.
	
	
	The rest of this paper is organized as follows. In Section II, we briefly review the related works. In Section III, we describe the proposed network in details. In Section IV, experimental results are presented. Finally, we conclude this paper in Section V. 
	
	\section{Related Work}
	In this section, we briefly review several methods that are closely related to our work.
	
	\subsection{Single Image SR}
	Interpolation-based approaches (\emph{e.g.}, bilinear, bicubic and Lanczos \cite{1990-FiltersforCommonResamplingTasks-Turkowski-147-165}) are initially used to increase the size of a single image. However, these methods cannot recover high-frequency details \cite{2003-SuperResolutionImageReconstruction:aTechnicalOverview-Park-21-36}. Later, numerous reconstruction-based approaches have been proposed for single image SR \cite{2005-ImageupSamplingUsingTotalVariationRegularizationwithaNewObservationModel-Aly-1647-1659,2007-ImageUpsamplingViaImposedEdgeStatistics-Fattal-95-95,2008-ImageSuperResolutionUsingGradientProfilePrior-Sun--}. These methods formulate the single image SR task as an optimization problem and introduce different regularization techniques to reconstruct HR images. However, these methods require a large number of iterations and thus suffer from a very high computational cost. To learn a direct mapping between LR and HR images, exemplars are collected from the input image \cite{2009-SuperResolutionfromaSingleImage-Glasner-349-356,2011-ImageandVideoUpscalingfromLocalSelfExamples-Freedman-1-11} and external datasets \cite{2012-CoupledDictionaryTrainingforImageSuperResolution-Yang-3467-3478,2013-AnchoredNeighborhoodRegressionforFastExampleBasedSuperResolution-Timofte-1920-1927}. These exemplar-based methods usually use machine learning approaches (\emph{e.g.}, Markov random field) to achieve promising performance \cite{2014-SingleImageSuperResolution:aBenchmark-Yang-372-386}. For comprehensive reviews on traditional single image SR methods, we refer the readers to \cite{2003-SuperResolutionImageReconstruction:aTechnicalOverview-Park-21-36,2014-SingleImageSuperResolution:aBenchmark-Yang-372-386}.
	
Recently, deep learning has been extensively investigated for SR. Dong \emph{et al.} \cite{2014-LearningaDeepConvolutionalNetworkforImageSuperResolution-Dong-184-199} proposed the pioneering work to use deep learning for single image SR. They used a three-layer CNN (namely, SRCNN) to approximate the non-linear mapping from an LR image to its corresponding HR image. Kim \emph{et al.} \cite{2016-AccurateImageSuperResolutionUsingVeryDeepConvolutionalNetworks-Kim-1646-1654} proposed a very deep super-resolution network (i.e., VDSR) with 20 convolutional layers. The deep architecture of VDSR improves the approximating capacity of CNN to achieve better performance. To achieve a compromise between model size and SR performance, Tai \emph{et al.} \cite{2017-ImageSuperResolutionViaDeepRecursiveResidualNetwork-Tai-2790-2798} developed a deep recursive residual network (DRRN) to deepen the network without obvious increase in model parameters. Shi \emph{et al.} \cite{2016-RealTimeSingleImageandVideoSuperResolutionUsinganEfficientSubPixelConvolutionalNeuralNetwork-Shi-1874-1883} proposed an efficient sub-pixel convolutional neural network (ESPCN) to increase the resolution of an LR image at the end of the network. Its computational complexity is significantly reduced. More recently, Zhang \emph{et al.} \cite{2018-ResidualDenseNetworkforImageSuperResolution-Zhang--} proposed a residual dense network (RDN) to facilitate effective feature learning using a contiguous memory mechanism.
	
	\subsection{Video SR}
	\subsubsection{Traditional Video SR}
	Since the seminal work proposed by Tsai and Huang \cite{1984-MultiframeImageRestorationandRegistration-Tsai--}, significant progresses have been achieved in multi-image SR and  video SR. Early methods
	\cite{1996-ExtractionofHighResolutionFramesfromVideoSequences-Schultz-996-1011,1997-JointMAPRegistrationandHighResolutionImageEstimationUsingaSequenceofUndersampledImages-Hardie-1621-1633} focus on videos with only affine transforms exist between adjacent frames, which is usually not the real case. To handle complex motion patterns in video clips, Protter \emph{et al.} \cite{2009-GeneralizingtheNonlocalMeanstoSuperResolutionReconstruction-Protter-36-51} generalized the non-local means framework for video SR. They performed adaptive fusion of multiple frames using patch-wise spatio-temporal similarities. Takeda \emph{et al.} \cite{2009-SuperResolutionwithoutExplicitSubpixelMotionEstimation-Takeda-1958-1975} further
	introduced a 3D kernel regression to exploit patch-wise spatio-temporal neighborhood
	relationship. However, HR images produced by these two methods are usually over-smoothed. To exploit pixel-wise correspondences, optical flow estimation was used in \cite{2007-OpticalFlowBasedSuperResolution:aProbabilisticApproach-Fransens-106-115,2014-OnBayesianAdaptiveVideoSuperResolution-Liu-346-360,2015-HandlingMotionBlurinMultiFrameSuperResolution-Ma-5224-5232}. These methods formulate the video SR task as an optimization problem and use iterative frameworks to estimate HR images, optical flows and blur kernels alternately. However, these methods are time-consuming.
	
	\subsubsection{Deep Video SR with Separated Motion Compensation}
	Inspired by the success of SRCNN in single image SR, deep learning has been investigated for video SR. Kappelar \emph{et al.} \cite{2016-VideoSuperResolutionwithConvolutionalNeuralNetworks-Kappeler-109-122} proposed a two-step framework to perform video SR. Specifically, optical flow estimation is first performed for motion compensation. Then, the compensated frames are concatenated and fed to a CNN to reconstruct an HR frame. Following the same two-step framework as \cite{2016-VideoSuperResolutionwithConvolutionalNeuralNetworks-Kappeler-109-122}, Liao \emph{et al.} \cite{2015-VideoSuperResolutionViaDeepDraftEnsembleLearning-Liao-531-539} estimated multiple optical flows using different parameter settings. These optical flows are then used for motion compensation to generate an ensemble of SR-drafts. Finally, a CNN is employed to recover high-frequency details from the ensemble. 
	The two-step framework separates motion estimation and compensation from the CNN network. Therefore, it is difficult for these methods to obtain an overall optimal solution.
	
	\begin{figure*}[ht]
		\centering
		\includegraphics[width=0.9\linewidth]{./Figs/fig1}
		\caption{An overview of our SOF-VSR network. Our network is fully convolutional and can be trained in an end-to-end manner.}
		\label{fig1}
	\end{figure*}
	
	\subsubsection{Deep Video SR with Integrated Motion Compensation} 
	Recently, Caballero \emph{et al.} \cite{2017-RealTimeVideoSuperResolutionwithSpatioTemporalNetworksandMotionCompensation-Caballero-2848-2857} proposed the first end-to-end CNN (namely, VESPCN) for video SR to integrate both motion estimation and compensation. Their VESPCN network comprises a motion estimation module and a spatio-temporal ESPCN module \cite{2016-RealTimeSingleImageandVideoSuperResolutionUsinganEfficientSubPixelConvolutionalNeuralNetwork-Shi-1874-1883}. Since then, end-to-end framework with integrated motion compensation
	dominates the research of video SR. 
	Tao \emph{et al.} \cite{2017-DetailRevealingDeepVideoSuperResolution-Tao-4482-4490}
	used the motion estimation module in VESPCN and then designed a new layer to achieve both sub-pixel motion compensation (SPMC) and resolution enhancement. They also proposed an encode-decoder network with LSTM to learn temporal contexts.
	Liu \emph{et al.} \cite{2017-RobustVideoSuperResolutionwithLearnedTemporalDynamics-Liu--} customized ESPCN \cite{2016-RealTimeSingleImageandVideoSuperResolutionUsinganEfficientSubPixelConvolutionalNeuralNetwork-Shi-1874-1883} to
	simultaneously reconstruct HR frames using different numbers of LR frames. A temporal adaptive network (namely, TDVSR) is then introduced to aggregate multiple HR estimates with learned dynamic weights. 
	Sajjadi \emph{et al.} \cite{2018-FrameRecurrentVideoSuperResolution-Sajjadi-6626-6634} proposed a frame-recurrent architecture (namely, FRVSR) to use previously inferred HR estimates for the SR of subsequent frames. This recurrent architecture can assimilate previous inferred HR frames without increasing computational costs.
	
	\subsubsection{Deep Video SR without Explicit Motion Compensation} 
	Huang \emph{et al.} \cite{2017-VideoSuperResolutionandViaBidirectionalandRecurrentConvolutionalandNetworks-Huang-1-1} proposed a bidirectional recurrent CNN to avoid explicit motion estimation and compensation. This recurrent-like architecture can capture long-term contextual information within temporal sequences. However, this method fails to handle large displacements and other complicated motions. Jo \emph{et al.} \cite{2018-DeepVideoSuperResolutionNetworkUsingDynamicUpsamplingFilterswithoutExplicitMotionCompensation-Jo--} introduced a CNN to generate dynamic upsampling filters for video SR. These dynamic upsampling filters are computed using local spatio-temporal neighborhood to avoid explicit motion compensation.
	
	Since temporal dependency between consecutive frames is important for video SR, existing deep learning based video SR methods focus on explicit or implicit exploitation of temporal dependency. However, these methods model temporal dependency in LR space, their limited accuracy in dependency hinders the restoration of fine details. Different from previous works, we propose an end-to-end video SR network to recover both temporal details and spatial details. Specifically, we first super-resolve optical flows to recover temporal details. These HR optical flows provide accurate temporal dependency and contribute to the restoration of spatial details. It is demonstrated that optical flow SR facilitates our network to achieve the state-of-the-art performance.
	
	\section{Methodology}
	In this section, we introduce our SOF-VSR network in details. We first give an overview of our SOF-VSR network, and then describe the OFRnet, the motion compensation module and the SRnet of our network. Finally, we present the loss function for the training of our network.
	
	\subsection{Overview}
	Given  consecutive LR frames () of a video clip as the input of SOF-VSR, our task is to super-resolve the central frame. Here, . Following \cite{2017-RobustVideoSuperResolutionwithLearnedTemporalDynamics-Liu--}, we convert input LR frames into YCbCr color space and only process the luminance channel. Input LR frames are first fed to OFRnet to infer HR optical flows. Specifically, our OFRnet takes  the central LR frame  and one neighboring frame  as input to generate an HR optical flow . Then, a space-to-depth
	transformation \cite{2018-FrameRecurrentVideoSuperResolution-Sajjadi-6626-6634}
	is employed to shuffle the HR optical flows into LR grids, resulting in LR flow cubes. Next, motion compensation is performed to generate a draft cube using these flow cubes. Finally, the draft cube is fed to SRnet to infer the HR frame. The overview of
	our network is shown in Fig. \ref{fig1}. For simplicity, we only show the architecture with .
	
	\subsection{Optical Flow Reconstruction Net (OFRnet)}
	
	\begin{figure*}[ht]
		\centering
		\includegraphics[width=0.99\linewidth]{./Figs/fig2}
		\caption{\textcolor{black}{The architecture of our OFRnet. Our OFRnet works in a coarse-to-fine manner. At each level, the output of its previous level is used to generate a residual optical flow.}}
		\label{fig2}
	\end{figure*}
	
	It has already been demonstrated by deep learning based SR methods (\emph{e.g.}, SRCNN \cite{2014-LearningaDeepConvolutionalNetworkforImageSuperResolution-Dong-184-199}, VDSR \cite{2016-AccurateImageSuperResolutionUsingVeryDeepConvolutionalNetworks-Kim-1646-1654} and RDN \cite{2018-ResidualDenseNetworkforImageSuperResolution-Zhang--})  that CNN is able to learn the non-linear mapping between LR and HR images. Recent CNN-based optical flow estimation methods (\emph{e.g.}, FlowNet \cite{2015-FlowNet:LearningOpticalFlowwithConvolutionalNetworks-Dosovitskiy-2758-2766}, PWCNet \cite{2017-PWCNet:CNNsforOpticalFlowUsingPyramidWarpingandCostVolume-Sun--} and LiteFlowNet \cite{2018-LiteFlowNet:aLightweightConvolutionalNeuralNetworkforOpticalFlowEstimation-Hui--}) have also shown the potential for motion estimation. Therefore, we incorporate these two tasks into a unified OFRnet to infer HR optical flows directly from LR images. Specifically, our OFRnet takes a pair of LR frames  and 
	as inputs, and reconstruct an optical flow  between their corresponding
	HR frames  and :
	
	
	where  represents the HR optical flow and  denotes
	the set of parameters.
	
	Multi-scale mechanism has been demonstrated to be effective in optical flow estimation \cite{2017-OpticalFlowEstimationUsingaSpatialPyramidNetwork-Ranjan-2720-2729,2018-LiteFlowNet:aLightweightConvolutionalNeuralNetworkforOpticalFlowEstimation-Hui--}, stereo matching \cite{2017-EndtoEndLearningofGeometryandContextforDeepStereoRegression-Kendall-66-75,2018-PyramidStereoMatchingNetwork-Chang--} and many other vision tasks \cite{2018-ScaleRecurrentNetworkforDeepImageDeblurring-Tao--}. To reduce model size and training difficulty, a scale-recurrent architecture with shared parameters across scales is used in SRN-DeblurNet \cite{2018-ScaleRecurrentNetworkforDeepImageDeblurring-Tao--}. Inspired by this, we introduce a scale-recurrent network for optical flow reconstruction, as illustrated in Fig. \ref{fig2}. For the first two levels, we use a recurrent module to estimate optical flows for inputs with different scales. For level 3, we first use the recurrent structure to generate deep representations, and then introduce an SR module to recover HR optical flows from the LR feature representations. The scale-recurrent architecture enables  OFRnet to handle complex motion patterns (especially large displacements) while being lightweight and compact.
	
	\textbf{Level 1:} The pair of input LR images  and  are first downsampled by a factor of 2 to produce  and . Meanwhile, an initial flow map  with all elements of 0 is generated. The initial flow map  is concatenated with  and  and then fed to a feature extraction layer with 320 kernels of size . Then, three efficient residual blocks are used to generate deep features. Channel split, channel shuffle and depth-wise convolution techniques  \cite{Ma2018a} are used in these residual blocks to improve the efficiency. Next, these features are fed to a flow estimation layer with 2 kernels of size  to generate optical flow   at this level. All convolutional layers are followed by a leaky rectified linear unit (ReLU) except the middle layer in each residual block and the last flow estimation layer.
	
	\textbf{Level 2:} Once the optical flow 
	is obtained from level 1, it is upscaled by a factor
	of 2 using bilinear interpolation. Note that, the magnitude of optical flow is also doubled with the resolution. The upscaled flow  is then used to warp , resulting in . Next, ,  and  are concatenated and fed
	to the recurrent module (which is the same as the one used in level 1) to generate optical flow   at this level.
	
	\textbf{Level 3:} Since the output optical flow  of level 2 has the same size as the LR input , level 3 works as an SR module to reconstruct HR optical flows. Similar to level 2, ,  and  are first concatenated and fed to the recurrent module (which is the same as the one used in levels 1 and 2) to extract features. These features are then fed to three additional  residual blocks to generate deep representations. Next, the resulting feature representations are fed to a sub-pixel layer \cite{2016-RealTimeSingleImageandVideoSuperResolutionUsinganEfficientSubPixelConvolutionalNeuralNetwork-Shi-1874-1883}  for resolution enhancement. Finally, a flow estimation layer is used to generate the final HR optical flow .
	
	Although numerous networks for SR \cite{2016-RealTimeSingleImageandVideoSuperResolutionUsinganEfficientSubPixelConvolutionalNeuralNetwork-Shi-1874-1883,2017-DeepLaplacianPyramidNetworksforFastandAccurateSuperResolution-Lai-5835-5843,2018-DeepBackProjectionNetworksforSuperResolution-Haris--} and optical flow estimation \cite{2015-FlowNet:LearningOpticalFlowwithConvolutionalNetworks-Dosovitskiy-2758-2766,2017-PWCNet:CNNsforOpticalFlowUsingPyramidWarpingandCostVolume-Sun--,2018-LiteFlowNet:aLightweightConvolutionalNeuralNetworkforOpticalFlowEstimation-Hui--} can be found in literature, our OFRnet is, to the best of our knowledge, the first unified network to integrate these two tasks. Specifically, our OFRnet learns to infer HR optical flows between latent HR images from LR inputs. Though some existing video SR methods can also obtain optical flows of full resolution by performing interpolation on LR inputs \cite{2017-EndtoEndLearningofVideoSuperResolutionwithMotionCompensation-Makansi-203-214} or LR optical flows \cite{2018-FrameRecurrentVideoSuperResolution-Sajjadi-6626-6634}, their flow estimation is still performed in LR space since interpolation does not introduce additional information for SR \cite{2016-RealTimeSingleImageandVideoSuperResolutionUsinganEfficientSubPixelConvolutionalNeuralNetwork-Shi-1874-1883}. Note that, inferring HR optical flows from LR images is quite challenging, our OFRnet has demonstrated the potential of CNN to address this challenge. It is further demonstrated in Sec. \ref{sec3.3} that our SOF-VSR network is benefited from HR optical flows in terms of both accuracy and consistency.
	
	\subsection{Motion Compensation Module}
	
	\begin{figure}[bt]
		\centering
		\includegraphics[width=0.9\linewidth]{./Figs/fig3}
		\caption{An illustration of space-to-depth transformation. The space-to-depth transformation folds an HR optical flow in LR space to generate an LR flow cube.}
		\label{fig3}
	\end{figure}
	
	Once HR optical flows are produced by OFRnet, space-to-depth transformation is used to bridge the resolution gap between HR optical flows and
	LR frames. As shown in Fig. \ref{fig3}, regular LR grids are extracted
	from the HR flow and placed into the channel dimension to derive
	a flow cube with the same resolution as LR frames:
	
	where  and  represent the size of the LR frame,  is the upscaling factor. Note that, the magnitude of optical flow is divided by a scalar  during the transformation to match the spatial resolution of LR frames.
	
	Then, slices are extracted from the LR flow
	cube to warp the LR frame , resulting in multiple warped drafts:
	
	where  denotes the warping operation using bilinear interpolation and  represents
	the concatenation of multiple warped drafts. Note that, although motion compensation is performed on LR frames, accurate temporal dependency can be encoded in compensated frames since HR optical flows are employed.
	
	\subsection{Super-Resolution Net (SRnet)}
	
	Our SOF-VSR takes  consecutive LR frames () as inputs to super-resolve the central frame. After motion compensation, multiple drafts are produced for each neighboring frame. As shown in Fig. \ref{fig1}, all the drafts are concatenated with the central LR frame and fed to SRnet to infer the HR frame:
	
	where  is the SR result of the central frame and  is the set of parameters.  represents the concatenation of all drafts after motion compensation, namely, draft cube.
	
	
	\textcolor{black}{As shown in Fig. \ref{fig4}, the draft cube is first passed to a feature extraction layer with 320 kernels of size  for feature extraction. The output features are then fed to 8 efficient residual  blocks to generate deep features. Once features are generated by these residual blocks, they are fed to a sub-pixel layer for resolution enhancement. Finally, a  convolutional layer is used to generate the HR frame. Since our SOF-VSR network only works on the luminance channel, the number of kernels in the last layer is set to 1.}
	
	\subsection{Loss Function}
	
	\begin{figure}[bt]
		\centering
		\includegraphics[width=0.95\linewidth]{./Figs/fig4}
		\caption{The architecture of our SRnet.}
		\label{fig4}
	\end{figure}
	
	We design two loss terms  and  for SRnet and OFRnet, respectively. For the training of SRnet, we use the mean square error (MSE) loss:
	
	
	
	For the
	training of OFRnet, intermediate supervision is used at each level
	of the pyramid:
	
	where
	
	,  and  are L1 regularization terms to constrain the smoothness of the optical flows at different scales.
	We empirically set  and  to make our OFRnet focus on the last level. We also set  as the regularization coefficient.
	
	
	Finally, the total loss for joint training is defined as ,
	where  is empirically set to 0.01 to balance these two loss terms.
	
	\section{Experiments}
In this section, we first introduce the datasets and implementation details. Next, ablation study is performed on the Vid4 dataset to test our network. Our SOF-VSR is then compared to the state-of-the-art methods on the Vid4 and DAVIS-10 datasets. Finally, face recognition task is used to further demonstrate the effectiveness of our network for high-level vision tasks.
	

	\begin{table*}[htp]
		\caption{Comparative results achieved by our network and its variants on the Vid4 dataset for  SR. FLOPs is computed based on HR frames with a  resolution of 720p (1280720).}
		\label{tab1}
		\begin{center}
			\small
			\setlength{\tabcolsep}{0.5mm}{
				\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|c|}
					\hline 
					& \multirow{2}{*}{PSNR()}  & \multirow{2}{*}{SSIM()} & \multirow{2}{*}{\tabincell{c}{T-MOVIE()\\times10^{-3}540\times960imresizeT32\times326+s\beta_{1}=0.9\beta_{2}=0.9991\!\times\!10^{-3}C^{L}4\times4\times4\times)} & PSNR
					\tabularnewline
					\hline 
					\emph{Calendar} &  4.76 & 26.51 & \textbf{4.56}  &  \textbf{26.89}
					\tabularnewline
					\emph{City} 	& 3.09  & 30.49 & \textbf{3.04}  &  \textbf{30.62}
					\tabularnewline
					\emph{Foliage}  & 3.00  & 30.49 & \textbf{2.71}  &  \textbf{31.37}
					\tabularnewline
					\emph{Walk} 	& 2.99  & 30.56 & \textbf{2.74}  &  \textbf{31.31}
					\tabularnewline
					\hline
					Average  & 3.46  & 29.51  & \textbf{3.26}  & \textbf{30.05}
					\tabularnewline
					\hline
			\end{tabular}}
		\end{center}
	\end{table}
	
	\begin{figure*}[t]
		\centering
		\includegraphics[width=0.82\linewidth]{./Figs/fig6_V2}
		\caption{\textcolor{black}{Visual comparison of optical flow estimation results achieved on the Middlebury dataset for  SR. The super-resolved optical flows recover finer correspondences with more clear edges and fewer artifacts \textcolor{black}{than the upsampled optical flows}.}}
		\label{fig6}
	\end{figure*}
	\begin{figure}[t]
		\centering
		\includegraphics[width=1\linewidth]{./Figs/fig7}
		\caption{\textcolor{black}{Visual comparison of error maps (difference between the warped image and the reference image) achieved on the Vid4 dataset for  SR. The results generated with super-resolved optical flows achieve higher accuracy.}}
		\label{fig7}
	\end{figure}
	
	We further compare the super-resolved optical flows and upsampled optical flows to the groundtruth on the Sintel \cite{2012-ANaturalisticOpenSourceMovieforOpticalFlowEvaluation-Butler-611-625}, Middlebury \cite{2011-ADatabaseandEvaluationMethodologyforOpticalFlow-Baker-1-31}, KITTI 2012 \cite{2012-AreWeReadyforAutonomousDriving?theKITTIVisionBenchmarkSuite-Geiger-3354-3361} and KITTI 2015 \cite{2015-ObjectSceneFlowforAutonomousVehicles-Menze-3061-3070} datasets. \textcolor{black}{We also include two dedicated optical flow estimation methods for comparison, including FlowNet \cite{2015-FlowNet:LearningOpticalFlowwithConvolutionalNetworks-Dosovitskiy-2758-2766} and SpyNet \cite{2017-OpticalFlowEstimationUsingaSpatialPyramidNetwork-Ranjan-2720-2729}. Note that, the optical flows estimated from LR frames are upsampled for fair evaluation.}
	We use the average end-point error (EPE) for quantitative comparison, and present the results in Table \ref{tab2}. 
	
	It can be observed that super-resolved optical flows significantly outperform upsampled ones, with EPE results being reduced by over 0.3. 
	\textcolor{black}{Note that, FlowNet (30.58M) and SpyNet (1.14M) are trained on a much larger dataset (\emph{i.e.} the Flying Chairs dataset with 22872 image pairs) in a supervised manner. Therefore, they achieve better performance than our OFRnet (0.41M) in terms of EPE.}
	Since  groundtruth optical flows are unavailable for the Vid4 dataset, we warped frames using the generated flows and then calculated root mean square error (RMSE) for quantitative evaluation. From Table \ref{tab3} we can also see that images warped using super-resolved optical flows have lower RMSE values (3.26 \emph{vs.} 3.46) and higher PSNR values (30.05 \emph{vs.} 29.51).
		
	
	\textcolor{black}{Visual comparison of optical flow estimation results achieved on the Sintel and Middlebury datasets is shown in Figs. \ref{fig5} and \ref{fig6}, respectively. It can be observed that upsampled optical flows produce distorted and blurred edges (\emph{e.g.}, the hand in Fig. \ref{fig5} and the bush in Fig. \ref{fig6})  with notable artifacts. In contrast, more clear edges can be observed in super-resolved optical flows, with finer details being recovered. \textcolor{black}{Moreover, our OFRnet produces visually comparable flow estimation results to SpyNet. This has clearly demonstrated the effectiveness of our OFRnet in recovering temporal details.}
	Error maps achieved on two scenes of the Vid4 dataset are further shown in Fig. \ref{fig7}. It can be observed that super-resolved optical flows produce fewer erroneous pixels, i.e, finer temporal details are recovered. }
	
	\textcolor{black}{In summary, the superior performance achieved on the Sintel, Middlebury, KITTI 2012, KITTI 2015 and Vid4 datasets demonstrates that finer temporal details can be recovered in super-resolved optical flows than upsampled ones. \textcolor{black}{Note that, the task of our work is not to design a superior optical flow estimation network. Instead, we focus on the design of a lightweight sub-network, which is sufficiently effective to provide fine temporal details for the improvement of overall video SR performance.}}
	
	\textcolor{black}{
	\subsubsection{SISR before Optical Flow Estimation.} 
	To obtain HR optical flows from LR inputs, an alternative is to  perform single image super-resolution (SISR) on separated LR frames first and then estimate HR optical flows from these SR results. To test the performance of this option, we designed a variant to perform SISR before optical flow estimation. Specifically, input LR frames were first super-resolved separately before being fed to the OFRnet for HR optical flow estimation. Note that, the sub-pixel convolution in level 3 of the OFRnet was replaced with a normal convolution. Next, SISR results were compensated and passed to the SRnet for fusion.}
	\textcolor{black}{It can be observed from Table \ref{tab1} that this variant does not introduce significant performance improvement against our SOF-VSR in terms of PSNR and SSIM. Meanwhile, this variant requires much higher computational cost than our SOF-VSR, with FLOPs being increased from 108.90G to 1.12T. Since SISR is first used to enhance the resolution of LR inputs, optical flow estimation and fusion of multiple frames are performed on HR images. Therefore, the computational complexity is significantly increased. In contrast, our SOF-VSR directly infers HR optical flows  from LR inputs and fuses multiple frames in LR space. Therefore, our network has a much lower computational cost and is more suitable for applications on mobile computing devices.} 
		
	\textcolor{black}{		
	\subsubsection{Scale-recurrent vs. Scale-cascaded Architectures} 
	Since the task of each level in our OFRnet is similar, we employ a scale-recurrent architecture in our OFRnet to reduce model size. To demonstrate its effectiveness, we replaced the scale-recurrent architecture with a scale-cascaded one by using independent networks at 3 levels. Results achieved on the Vid4 dataset are presented in Table \ref{tab1}.}
	
	\textcolor{black}{
	Our SOF-VSR achieves comparable performance to the scale-cascaded architecture with the overall model size being reduced from 1.33M to 1.00M. Since the tasks of different levels are similar, the scale-cascaded architecture contains redundant parameters. In contrast, using a scale-recurrent structure, our SOF-VSR is more  lightweight and compact while achieving comparable performance.}
	
	
	\begin{table*}[ht]
		\caption{Comparative results achieved on the Vid4 dataset. Note that, the first and last two frames are not used in our evaluation. FLOPs is computed based on HR frames with a resolution of 720p (1280720). Results marked with * are directly copied from the corresponding papers. Best results are shown in boldface.}
		\label{tab4}
		\begin{center}
			\normalsize
			\setlength{\tabcolsep}{2mm}{
				\begin{tabular}{|c|c|l|c|c|c|c|c|c|c|}
					\hline 
					Model & Scale & Method  & Frames & PSNR() & SSIM() & \tabincell{c}{T-MOVIE()\\times10^{-3}\times2\times3\times4\times4imresizes^{\textup{th}}\sigma\!=\!1.6\sigma\!=\!1.52\times3\times4\times4\times4\times\times\uparrow\uparrow\downarrow)} & \textcolor{black}{Params.} & \textcolor{black}{FLOPs}
					\tabularnewline
					\hline
					\hline
					\multirow{18}{*}{BI} & \multirow{5}{*}{} & Bicubic  & 1 & 36.43 & 0.958 & 4.63 & 0.70 & - & -
					\tabularnewline				
					& & DRCN \cite{2016-DeeplyRecursiveConvolutionalNetworkforImageSuperResolution-Kim-1637-1645} & 1 & 40.62 & 0.979 & 1.09 & 0.13 &1.8M & 9,788.7G
					\tabularnewline
					& & LapSRN \cite{2017-DeepLaplacianPyramidNetworksforFastandAccurateSuperResolution-Lai-5835-5843}  & 1 & 40.30 & 0.978 & 1.05 & 0.12 & 0.8M & 29.9G
					\tabularnewline
					& & CARN \cite{2018-FastAccurateandLightweightSuperResolutionwithCascadingResidualNetwork-Ahn--}& 1 & 40.99 & 0.981 & \textbf{0.85} & 0.11 & 1.6M & 222.8G
					\tabularnewline
					& & VSRnet \cite{2016-VideoSuperResolutionwithConvolutionalNeuralNetworks-Kappeler-109-122} & 5 & 39.00 & 0.972 & 1.31 & 0.20  &  266K & 242.7G
					\tabularnewline
					& & SOF-VSR  & 3 &\textbf{41.38} & \textbf{0.983} & 0.92 & \textbf{0.09} & 0.9M & 342.8G
					\tabularnewline
					\cline{2-10}
					& \multirow{5}{*}{} & Bicubic & 1 & 32.94 & 0.912 & 13.55 & 2.63  & - & -
					\tabularnewline
					& & DRCN \cite{2016-DeeplyRecursiveConvolutionalNetworkforImageSuperResolution-Kim-1637-1645}  & 1 & 36.08 & 0.947 & 5.26 & 0.92 & 1.8M & 9,788.7G
					\tabularnewline
					& & CARN \cite{2018-FastAccurateandLightweightSuperResolutionwithCascadingResidualNetwork-Ahn--}  & 1 & 36.70 & 0.952 & 4.44 & 0.79 & 1.6M & 118.8G
					\tabularnewline
					& & VSRnet \cite{2016-VideoSuperResolutionwithConvolutionalNeuralNetworks-Kappeler-109-122}  & 5 & 34.94 & 0.936 & 6.11 & 1.20 &266K & 242.7G
					\tabularnewline					
					& & SOF-VSR  & 3 & \textbf{36.80} & \textbf{0.955} & \textbf{4.36} & \textbf{0.68}  & 1.1M & 205.0G
					\tabularnewline
					\cline{2-10}
					& \multirow{5}{*}{} & Bicubic  & 1 & 30.97 & 0.870 & 22.73 & 4.75 & - & -
					\tabularnewline
					& & DRCN \cite{2016-DeeplyRecursiveConvolutionalNetworkforImageSuperResolution-Kim-1637-1645}  & 1 & 33.49 & 0.911 & 13.51 & 2.48 & 1.8M & 9,788.7G
					\tabularnewline
					& & LapSRN \cite{2017-DeepLaplacianPyramidNetworksforFastandAccurateSuperResolution-Lai-5835-5843}  & 1 & 33.54 & 0.911 & 12.83 & 2.43 & 0.8M & 149.4G
					\tabularnewline
					& & CARN \cite{2018-FastAccurateandLightweightSuperResolutionwithCascadingResidualNetwork-Ahn--} & 1 & 34.12 & 0.921 & \textbf{11.41} & 2.05 & 1.6M & 90.9G
					\tabularnewline
					& & VSRnet \cite{2016-VideoSuperResolutionwithConvolutionalNeuralNetworks-Kappeler-109-122}  & 5 & 32.63 & 0.897 & 14.63 & 2.85 & 266K & 242.7G
					\tabularnewline
					& & SOF-VSR \cite{2018-LearningforVideoSuperResolutionthroughHROpticalFlowEstimation-LongguangWang--}  & 3 & \textbf{34.32}* & 0.925* & 11.77* & 1.96* & 1.5M  & 105.2G
					\tabularnewline
					& & SOF-VSR  & 3 & 34.28 & \textbf{0.926} & 11.72 & \textbf{1.94} & 1.0M & 112.5G
					\tabularnewline
					\hline
					\hline
					\multirow{3}{*}{BD} & \multirow{3}{*}{}& SPMC \cite{2017-DetailRevealingDeepVideoSuperResolution-Tao-4482-4490}  & 3 & 33.02 & 0.911 & 14.06 & 1.96  &1.7M & 160.8G 
					\tabularnewline
					& & SOF-VSR-BD \cite{2018-LearningforVideoSuperResolutionthroughHROpticalFlowEstimation-LongguangWang--}  & 3 & 34.27* & 0.925* & 10.93* & 1.90* & 1.5M & 105.2G
					\tabularnewline
					& & SOF-VSR-BD  & 3 & \textbf{34.28} & \textbf{0.927} & \textbf{10.91} & \textbf{1.87}  & 1.0M & 112.5G
					\tabularnewline
					\hline
			\end{tabular}}
		\end{center}
	\end{table*}
	
	\begin{figure*}[t]
		\centering
		\includegraphics[width=1\linewidth]{./Figs/fig10}
		\caption{Visual comparison of  SR results on \emph{Boxing} and \emph{Demolition}. Bicubic, DRCN \cite{2016-DeeplyRecursiveConvolutionalNetworkforImageSuperResolution-Kim-1637-1645}, LapSRN \cite{2017-DeepLaplacianPyramidNetworksforFastandAccurateSuperResolution-Lai-5835-5843}, CARN \cite{2018-FastAccurateandLightweightSuperResolutionwithCascadingResidualNetwork-Ahn--}, VSRnet \cite{2016-VideoSuperResolutionwithConvolutionalNeuralNetworks-Kappeler-109-122}, and SOF-VSR are based on the BI degradation model, while SPMC \cite{2017-DetailRevealingDeepVideoSuperResolution-Tao-4482-4490} and SOF-VSR-BD are based on the BD degradation model.}
		\label{fig10}
	\end{figure*}
	
	We further show the trade-off between accuracy and consistency of different methods in Fig. \ref{fig8}. It can be observed that our SOF-VSR and SOF-VSR-BD networks achieve better PSNR and T-MOVIE performance on the Vid4 dataset, while being lightweight and compact.
	
	\textbf{Qualitative Evaluation.}
	Several qualitative results on two scenarios of the Vid4 dataset are shown in Fig. \ref{fig9}. We can see from the zoom-in regions that our SOF-VSR and SOF-VSR-BD networks recover finer details, such as the word ``MAREE" and the stripes of the building. Moreover, it can be observed from the temporal profiles that the word ``MAREE" can hardly be recognized in the SR results achieved by Bicubic, DRCN, LapSRN, CARN, VSRnet and TDVSR. Although finer results are produced by SPMC, the word is still distorted and blurred. In contrast, smooth and clear patterns with fewer artifacts can be observed in the temporal profiles of our results. In summary, our network produces temporally more consistent results and better perceptual quality.
	
	
	\subsubsection{Evaluation on the DAVIS-10 Dataset} \indent
	
	\textbf{Quantitative Evaluation.} 
	Quantitative results achieved on the DAVIS-10 dataset are shown in Table \ref{tab5}. For the BI degradation model, our SOF-VSR network achieves the state-of-the-art performance in terms of PSNR and SSIM. Although suffering from a slight PSNR performance drop as compared to the conference version for  SR, our network  achieves better performance in terms of other metrics with much fewer parameters (1.0M \emph{vs.} 1.5M). In terms of T-MOVIE, our network achieves comparable or better performance than other approaches. In summary, our SOF-VSR network produces SR results with the best overall video quality in terms of MOVIE. 
	
	For the BD degradation model, our SOF-VSR-BD network outperforms SPMC in terms of all metrics. Specifically, the PSNR/T-MOVIE values achieved by our network are better than SPMC by \textcolor{black}{1.26}/3.15. That is, better accuracy and consistency performance is achieved by our network. Since the DAVIS-10 dataset comprises scenes with fast moving objects, complex motion patterns (especially large displacements) lead to performance deterioration of existing video SR methods. In contrast, more accurate temporal dependency is provided by HR optical flows in our network. Therefore, complex motion patterns can be handled more robustly and better performance can be achieved.
	
	\textbf{Qualitative Evaluation.} Qualitative comparison on two scenarios of the DAVIS-10 dataset is shown in Fig. \ref{fig10}. Compared to other methods, our SOF-VSR and SOF-VSR-BD networks recover more accurate details and achieve better visual quality, such as the pattern on the shorts and the word ``PEUA". Specifically, the patterns on the shorts recovered by Bicubic and VSRnet are obviously blurred. Although finer details can be recovered by DRCN, LapSRN and SPMC, their resulting patterns are still distorted. In contrast, our networks produce more clear details with fewer artifacts.
	
	
	\subsection{High-Level Vision Tasks}

	\textcolor{black}{
	Rich details in a video clip are beneficial to high-level vision tasks such as face recognition and digit recognition \cite{2016-IsImageSuperResolutionHelpfulforOtherVisionTasks?-Dai-1-9}. Here, we further compare our network to LapSRN, CARN, and SPMC by integrating a video SR module into the face recognition task.}
	
	\textcolor{black}{
	\textbf{Data preparation.} Following  \cite{2018-LearningTemporalDynamicsforVideoSuperResolution:aDeepLearningApproach-Liu--}, we form a subset of the YouTube Face dataset \cite{2011-FaceRecognitioninUnconstrainedVideoswithMatchedBackgroundSimilarity-Wolf-529-534} by choosing 167 subject classes that contain more than three video sequences. For each class, we randomly select one video for test and the rest for training. We first cropped face regions and resized them to the size of 6060 to generate the HR data. Then, these HR data were downsampled to 1515 to form the LR data. For each test video, we splitted it into clips of 50 frames. In total, we have about 600 clips.}
	
	\textcolor{black}{
	\textbf{Classifier.} We used a customized AlexNet in \cite{2012-ImagenetClassificationwithDeepConvolutionalNeuralNetworks-Krizhevsky-1097-1105} as the classifier, whose architecture details are shown in Table \ref{tab6}. The classification network takes a 6060 facial image as input and predicts the class of the subject.} 
	
	\begin{table}[t]
		\caption{\textcolor{black}{Network architecture of the classifier used for face recognition.}}
		\label{tab6}
		\begin{center}
			\normalsize
			\setlength{\tabcolsep}{2mm}{
				\begin{tabular}{|l|c|c|c|}
					\hline  
					Layer Settings & Output Size
					\tabularnewline
					\hline
					Input & 
					\tabularnewline
					99 Conv, stride 1, padding 0, ReLU & 
					\tabularnewline
					55 Conv, stride 1, padding 0, ReLU & 
					\tabularnewline
					44 Conv, stride 1, padding 0, ReLU & 
					\tabularnewline 
					Max pooling, kernel 2, stride 2, padding 0 & 
					\tabularnewline
					33 Conv, stride 1, padding 0, ReLU & 
					\tabularnewline
					Max pooling, kernel 2, stride 2, padding 0 & 
					\tabularnewline
					Fully connected & 167
					\tabularnewline
					\hline
			\end{tabular}}
		\end{center}
	\end{table}
	
	\begin{table}[t]
		\caption{\textcolor{black}{Face recognition performance achieved by different SR methods on a subset of the YouTube Face dataset under  SR scenario. Best results are shown in boldface.}}
		\label{tab7}
		\begin{center}
			\normalsize
			\setlength{\tabcolsep}{1.5mm}{
				\begin{tabular}{|l|c|c|c|c|c|}
					\hline 
					Model & Model  & Top-1 Accuracy & Top-5 Accuracy  
					\tabularnewline
					\hline
					Original    & -  & 62.7\%  & 75.1\%     
					\tabularnewline
					\hline
					Bicubic    &  \multirow{4}{*}{BI} & 41.1\%  & 58.8\%     
					\tabularnewline
					LapSRN \cite{2017-DeepLaplacianPyramidNetworksforFastandAccurateSuperResolution-Lai-5835-5843}
					&   & 55.9\% & 71.4\%	   
					\tabularnewline
					CARN \cite{2018-FastAccurateandLightweightSuperResolutionwithCascadingResidualNetwork-Ahn--} 	
					& & 56.9\% & 72.4\%   
					\tabularnewline
					SOF-VSR &   & \textbf{58.1\%}  & \textbf{74.1\%}     
					\tabularnewline
					\hline
					SPMC \cite{2017-DetailRevealingDeepVideoSuperResolution-Tao-4482-4490}       
					&  \multirow{2}{*}{BD}  &  55.8\% & 71.4\%     
					\tabularnewline
					SOF-VSR-BD  
					&   & \textbf{57.6\%} & \textbf{72.9\%}     
					\tabularnewline
					\hline
			\end{tabular}}
		\end{center}
	\end{table}

	\textcolor{black}{
	\textbf{Implementation details.} During test, we first super-resolved each LR test clip using a specific SR method and then fed the SR results to the classifier. Note that, we did not fine-tune these SR methods on the Youtube Face dataset. The prediction probabilities were aggregated over all frames in each video clip. The top-1 and top-5 accuracy metrics were used for quantitative evaluation and the comparative results are shown in Table \ref{tab7}.}
	
	\textcolor{black}{
	It can be observed that our SOF-VSR network achieves the highest top-1 and top-5 accuracy on both BI and BD degradation models. Specifically, our network outperforms CARN by 1.2\%/1.7\% in terms of top-1/top-5 accuracy. That is because, our SOF-VSR network can recover richer details such that better face recognition performance can be achieved.}
	
	
	\section{Conclusion}
	In this paper, we have proposed an end-to-end deep network for video SR. Our OFRnet first super-resolves optical flows to provide accurate temporal dependency. Motion compensation is then performed based on HR optical flows. Finally, SRnet is used to infer SR results from these compensated LR frames. Extensive experimental results show that our SOF-VSR network can recover accurate temporal details for the improvement of both SR accuracy and consistency. Comparison to existing video SR methods has also demonstrated the state-of-the-art performance of our SOF-VSR network. 
	
	
	
	
	
	
	
\section*{Acknowledgment}
	This work was partially supported by the National Natural Science Foundation of China (No. 61972435, 61602499 and No. 61605242), and Fundamental Research Funds for the Central Universities (No. 18lgzd06).
	
	
\ifCLASSOPTIONcaptionsoff
	\newpage
	\fi
	
	
	
	
	
	


	


	






	
	
	\bibliographystyle{IEEEtran}
	\bibliography{IEEEabrv,super-resolution,other-CV-fields,neural-network,image-deblur}
	
	\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip]{Figs/LongguangWang}}]
		{Longguang Wang} received the B.E. degree in electric engineering from Shandong University (SDU), Jinan, China, in 2015, and the M.E. degree in information and communication engineering from National University of Defense Technology (NUDT), Changsha, China, in 2017. He is currently pursuing the Ph.D. degree with the College of Electronic Science and Technology, NUDT. His current research interests include low-level vision and deep learning.
	\end{IEEEbiography}
	\vspace{-5ex}
	\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip]{Figs/Yulan}}]
	{Yulan Guo} received the B.Eng. and Ph.D. degrees from National University of Defense Technology (NUDT) in 2008 and 2015, respectively. He was a visiting Ph.D. student with the University of Western Australia from 2011 to 2014. He worked as a postdoctorial research fellow with the Institute of Computing Technology, Chinese Academy of Sciences from 2016 to 2018. He has authored over 80 articles in journals and conferences, such as the IEEE TPAMI and IJCV. His current research interests focus on 3D vision, particularly on 3D feature learning, 3D modeling, 3D object recognition, and 3D biometrics. Dr. Guo received the CAAI Outstanding Doctoral Dissertation Award in 2016. He served as an associate editor for IET Computer Vision and IET Image Processing, a guest editor for IEEE TPAMI, a PC member for several conferences (e.g., CVPR and ICCV), a reviewer for over 30 journals, and an organizer for a tutorial in CVPR 2016 and a workshop in CVPR 2019.
\end{IEEEbiography}
	\vspace{-5ex}
	\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip]{Figs/liliu}}]
		{Li Liu} received the BSc degree in communication engineering, the MSc degree in photogrammetry and remote sensing and the Ph.D. degree in information and communication engineering from the National University of Defense Technology (NUDT), Changsha, China, in 2003, 2005 and 2012, respectively. She joined the faculty at NUDT in 2012, where she is currently an Associate Professor with the College of System Engineering. During her PhD study, she spent more than two years as a Visiting Student at the University of Waterloo, Canada, from 2008 to 2010. From 2015 to 2016, she spent ten months visiting the Multimedia Laboratory at the Chinese University of Hong Kong. From 2016 to 2018, she is working at the Machine Vision Group at the University of Oulu, Finland. She was a cochair of International Workshops at ACCV2014, CVPR2016, ICCV2017 and ECCV2018. She was a guest editor of special issues for IEEE TPAMI and IJCV. Her research interests include facial behavior analysis, texture analysis, image classification, object detection and recognition.
	\end{IEEEbiography}
	\vspace{-5ex}
	\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Figs/ZaipingLin}}]
		{Zaiping Lin} received the B.Eng. and Ph.D. degrees from the National University of Defense Technology (NUDT) in 2007 and 2012, respectively. He is currently an Assistant Professor with the College of Electronic Science and Technology, NUDT. His current research interests include infrared image processing and signal processing.
	\end{IEEEbiography}
	\vspace{-5ex}
	\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Figs/XinpuDeng}}]
		{Xinpu Deng} received the B.Eng. and Ph.D. degrees from the National University of Defense Technology (NUDT).  He is currently an Associate Professor with the College of Electronic
		Science and Technology, NUDT. His current research
		interests include signal processing and remote sensing.
	\end{IEEEbiography}
	\vspace{-5ex}
	\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip]{Figs/WeiAn}}]
		{Wei An} received the Ph.D. degree from the National University of Defense Technology (NUDT), Changsha, China, in 1999. She was a Senior Visiting Scholar with the University of Southampton, Southampton, U.K., in 2016. She is currently a Professor with the College of Electronic Science and Technology, NUDT. She has authored or co-authored over 100 journal and conference publications. Her current research interests include signal processing and image processing.
	\end{IEEEbiography}
\end{document}
