


The proposed method consists of two steps. The first step is frame reproduction, which obtains disentangled features from a consideration of semantics in the frames. The second step is video prediction using the disentangled features obtained during frame reproduction.



\vspace*{-10pt}
\subsection{Frame Reproduction} 
\label{featureextractors}
Let  denote the  frame in video . The frame reproduction is to reproduce  from the known frames , and . Following the `Reproducibility' assertion presented in the Introduction, we reproduce  from the content of  and the motion of , with the aim of obtaining disentangled motion and content features by considering semantics. We describe our network architecture in Section~\ref{networkarchi} and our training procedure in Section~\ref{trainingprocedure}.
\vspace*{-10pt}
\subsubsection{Network Architecture}\label{networkarchi}
The structure of MSnet is presented in Figure~\ref{figure1}. One encoder extracts content features and another extracts motion features. From these features, a generator reproduces . 
\begin{wrapfigure}{r}{5.5cm}
  \centering
\includegraphics[width=0.88\linewidth]{figure1_v8.pdf}
  
  \caption{\label{figure1} Architecture of MSnet, showing multi-scale motion-guided connections.}
\end{wrapfigure}
Specifically, The content encoder  obtains the content information  from two successive frames  and . The motion encoder  extracts motion information  from frames  and , which do not have to be adjacent. The generator  reproduces the last frame of the input . The motivation for these settings is shown in the appendix.




\textbf{Motion-guided Connection: }The generator  is connected to the content encoder  by block-wise motion-guided connections, which play a similar role to the skip connections in UNet~\cite{ronneberger2015u}, but each motion-guided connection performs an additional convolution operation guided by motion feature. This reduces ghosting in the reproduced frame: a standard skip connection tends to preserve information about previous frames  and  (but not ), which causes a ghost of  and  to remain in the reproduced . We concatenate the features of each convolutional block and a bi-linearly upscaled motion feature , and then pass the concatenated features into a  convolutional layer to adjust the number of channels, and add residual connections. These motion-guided connections use motion information to modify the spatial information from previous frames, so that it can be effectively transferred to the target frames.



\textbf{Discriminators for adversarial training: }We apply adversarial learning to train MSnet, with three discriminators. For realistic and sharp results, we use a frame discriminator. We use two additional discriminators to disentangle motion and content features. More details of these are given in Section~\ref{trainingprocedure}.


\subsubsection{Training Procedure}\label{trainingprocedure}

Based on the intuitive assertions presented in the Introduction, we define the following objective terms: To train the encoders, we use
\vspace{-3pt}

where  and  are hyperparameters. To train the discriminators, we use

We optimize  and  alternately. The loss terms in  and  are described below. In what follows,  denotes the reproduced frame .

\textbf{Reconstruction and time-reversal Losses: }
Based on the `Reproducibility' and `Time-reversibility' assertions presented in the Introduction, we define  and  as follows:

\vspace{-7pt}

where  represents the temporal distance between the target frame and the reference frame.


\textbf{Frame adversarial loss: }
DRnet \cite{denton2017unsupervised} uses mean squared error loss alone, which tends to produce blurry results in image reproduction \cite{mathieu2015deep}. We thus introduce an extra frame adversarial loss, using a technique similar to that employed in the pix2pix network \cite{isola2017image}.
The frame discriminator  is trained to determine whether its input is a real pair of frames or not, and  is trained by  which is expressed as follows: 

The adversarial loss  expresses the extent to which synthetic frames produced by the generator  manage to deceive the discriminator. The generator  is trained by  to synthesize realistic frames with the aim of deceiving the frame discriminator, and  is expressed as follows:




\textbf{Disentangling adversarial loss: }
The notion of `Separability of features' described in the Introduction is realized by the content discriminator  and motion discriminator .
The content discriminator is trained to determine whether two motion features come from the same video, which requires it to discover the content information in these features.
Thus, to deceive the content discriminator, the motion encoder must generate motion features that contain as little content information as possible.
We train the content discriminator to discover content information in motion features using the loss , and the loss  is used to train the motion encoder in such a way that the motion discriminator cannot make a decision, which means that the entropy becomes maximized. Note that Eq~\ref{equation6} can be simplified to  if we set , and the function has the minimum value when . 
The result is that the motion encoder obtains a pure motion feature. These two losses are formulated as follows:
\begin{figure}[t]
    \centering
    \includegraphics[width=0.88\linewidth]{figure3_v5.pdf}
    \caption{The frame prediction network. Given  frames, the network predicts upcoming  frames.} 
\vspace*{-10pt}
\label{fig_2}
\end{figure}
\small{

}
where  and  are different frame numbers, and  and  are different videos.




In a similar way, the motion discriminator is trained to determine whether two content features are from sequential or non-sequential frames, which requires it to discover the motion information from the content feature.
The content encoder can deceive the motion discriminator if it generates content features that do not contain motion information. 
We train the motion discriminator to discover motion information in the content feature by the loss , and the content encoder is trained to deceive the motion discriminator by the loss ; so that the content encoder can obtain a pure content feature. These two losses are formulated as follows:


where  and  are sequential frames, and  and  are non-sequential frames.

\subsection{Video Frame Prediction}
\label{futureframepred}
We apply the motion and content encoders trained during frame reproduction to video prediction. MSnet is given  frames  and trained to predict the following  frames , using the network illustrated in Figure~\ref{fig_2}. The motion encoder extracts motion features from the pairs , , and the content encoder extracts content features from . Note that the first frame in each pair is always  during motion extraction. A convolutional LSTM network (cLSTM) \cite{xingjian2015convolutional} takes the motion features  extracted from each given pairs of frames and predicts the motion features of the subsequent frames  until the  frame. 

For subsequent unknown frames, the predicted motion features are fed back into the cLSTM and the motion features of the next upcoming frames are predicted. By repeating this step, we can predict the motion features of the following  frames.

The cLSTM is trained using the following objective function:
\small{
}
Finally, the generator produces  from the  predicted motion features , together with the content features . By repeating this step, we can generate the required number of upcoming frames.
