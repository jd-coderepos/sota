
\documentclass{article}

\PassOptionsToPackage{numbers, compress}{natbib}



\usepackage[final]{neurips_2022}


\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         \usepackage{graphicx}
\usepackage{amsmath}

\usepackage{mathtools}
\usepackage{xspace,xfrac}
\usepackage{subfigure}
\usepackage{multirow}
\usepackage{color}
\usepackage{comment}
\usepackage[capitalise]{cleveref}

\usepackage{caption}
\captionsetup{labelfont=bf,tableposition=top}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    breaklinks=true,
    urlcolor=blue,
    linkcolor=blue,
    bookmarksopen=false,
    filecolor=black,
    citecolor=blue,
    linkbordercolor=blue
}

\usepackage{bibspacing}

\makeatletter
\renewcommand{\paragraph}{\@startsection{paragraph}{4}{\z@}{.5ex \@plus 1ex \@minus .2ex}{-1em}{\normalfont\normalsize\bfseries}}
\makeatother

\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}

\def\eg{\emph{e.g}\onedot}
\def\ie{\emph{i.e}\onedot} 
\def\cf{\emph{c.f}\onedot}
\def\etc{\emph{etc}\onedot} \def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot} 
\def\dof{d.o.f\onedot}
\def\etal{\emph{et al}\onedot}
\makeatother
\DeclareMathOperator{\sign}{sgn}

\def\BTheta{{{\boldsymbol \theta}}} 
\def\R{{\mathbb R}}
\def\atm{ATM} 
\def\tpn{QBD}
\def\seg{SegViT}
\def\SP{~~}



\title{SegViT: Semantic Segmentation with Plain Vision Transformers
}




\author{
   Bowen Zhang,
\SP 
 Zhi Tian\thanks{The first two authors contributed equally.
CS is the corresponding author, e-mail: \tt chunhua@me.com
},
\SP  
 Quan Tang,
\SP
\textbf{Xiangxiang Chu},
\
    Q  =  \phi_{q} (\mathcal{G}) \in \R^{N \times C},\ 
    K = \phi_{k} (\mathcal{F}_{i}) \in \R^{L \times C},\ 
    V = \phi_{v} (\mathcal{F}_{i}) \in \R^{L \times C},
    \label{eq:2}
 
\begin{split}
       S(Q, K)& = \frac{QK^T}{\sqrt{d_{k}}}\in \R^{N \times L}, \\
        Attention(\mathcal{G}, \mathcal{F}_{i}) & = 
        {\tt Softmax}(S(Q, K))V \in \R^{N \times C},
\end{split}

        Mask(\mathcal{G}, \mathcal{F}_{i}) = {\tt Sigmoid}(S(Q, K)) \in \R^{N \times L}

    \begin{aligned}
    \mathcal{L}_{overall}=\mathcal{L}_{cls} +\mathcal{L}_{mask} & =\mathcal{L}_{cls} +
    \lambda_{focal}\mathcal{L}_{IoU} + \lambda_{dice}\mathcal{L}_{dice} \\
    \end{aligned}


In each group, the output tokens are supervised by the classification loss () which is mentioned above and the masks are summed orderly and supervised by the mask loss () which is a linear combination of a focal loss \cite{lin2017focal} and a dice loss \cite{diceloss} multiplied by hyper-parameters  and  respectively as in DETR \cite{detr}. The loss of all three groups are then summed together.
We have further experiments to show that this design is beneficial and efficient.



\paragraph{The \emph{Shrunk} structure.}
Plain transformer backbones such as ViT is known to have larger computational cost than their counterparts with similar performance. 
We propose a \emph{Shrunk} structure using query-based down-sampling (QD) and up-sampling (QU). 
Since the shape of the output of the attention module is determined by the shape of the query, 
we can apply down-sampling before the query transformation to realize the QD or insert new query tokens during the cross attention to realize the QU. By changing the resolution with the number of query tokens, the spatial size is changed according to the cross attention, providing more flexibility to preserve (recover) important regions.
To be more specific, 
in the QD layer, we use the nearest sampling to reduce the number of the query tokens while keep the size of the key and value tokens. When passing through a transformer layer, the values are weighted and summed by the attention map between query tokens and the key tokens. This is non-linear downsampling that will pay more attention to the important regions.  In the QU layer, we employ a transformer decoder structure~\cite{2017attention} and initialize new learnable tokens as queries based on the desired output resolution.  

As shown 
in \cref{fig:struct}, we design the 
\seg\ structure with one single layer as the baseline (a).  
We first try 
a naive approach (b), which is to apply the QD once at the  depth of the backbone (\eg, the  layer of a backbone with  layers) to down-sample 
the resolution of the layer output from  to 
so as 
to reduce the overall computational cost.
The performance drops as expected since the QD process involves information lose.

To compensate for the information loss in the naive `shrunk' version, we further apply 
 two QU layers in parallel with the backbone.
 This is our proposed \emph{Shrunk} version (c).
 The first QU layer takes in features with  resolution from the low level of the backbone. Its output is then used as the query to make cross attention with the down-sampled features  with  resolution from the last layer of the backbone.
The shape of the output of this QU structure is of  resolution.

Directly reducing the number of the query tokens inevitably 
harms  the final performance.
However, with our designed QU layer and the ATM module, the \emph{Shrunk} structure is able to reduce  of overall computational cost while still being competitive in performance.

















\section{Experiments}
\subsection{Datasets}
\paragraph{ADE20K \cite{ade20k}}  
is a challenging scene parsing dataset which contains  images  as the training set and  images as the validation set with 150 semantic classes. 

\textbf{COCO-Stuff-10K \cite{cocostuff}}
is a scene parsing benchmark with  training images and  test images. Even though the dataset contains  categories, not all categories exist in the test split. We follow the implementation of mmsegmentation \cite{mmseg} with  categories to conduct the experiments.

\textbf{PASCAL-Context \cite{pascal_context}}
is a dataset with  images in training set and  images in the validation set. There are  semantic classes in total, including a class representing `background'. 


\subsection{Implementation details}

\textbf{Transformer backbone.} 
We use the naive ViT~\cite{vit} as the backbone. In particular, we use its `Base' variation for most ablation studies and provide results on the `Large' variation. Since there can be a huge difference with different pre-trained weights, as suggested by Segmenter~\cite{strudel2021segmenter}, we use the weights provided by Augreg~\cite{augreg} following the counterparts~\cite{strudel2021segmenter, lin2022structtoken} for a fair comparison. The weights are obtained by training on ImageNet-21k with strong data augmentation and regularization. For a simple reference, we report that for pre-trained weights provided by ViT~\cite{vit} and Augreg~\cite{augreg}, the mIoU scores using the same training recipe on ADE20K dataset are  and , respectively. 
\textbf{Training settings.}
We use MMSegmentation \cite{mmseg} and follow the commonly used training settings. 
During training, we applied data augmentation sequentially via random horizontal flipping, random resize with the ration between  and  and random cropping ( for all except that we use  for PASCAL-Context and  for ViT-large on ADE20K).
The batch size is  for all datasets with a total iteration of ,  and  for ADE20k, COCO-Stuff-10k and PASCAL-Context respectively. 
\textbf{Evaluation metric.} We use the mean Intersection over Union (mIoU) as the metric to evaluate the performance. `ss' means single-scale testing and `ms' test time augmentation with multi-scaled  inputs. All reported mIoU scores are in a percentage format. All reported computational costs in GFLOPs are measured using the fvcore~\footnote{\url{https://github.com/facebookresearch/fvcore}} library.




\subsection{Comparisons with the State-of-the-art Methods}
\paragraph{Results on ADE20K.}
\cref{tab:ade20k} reports the comparison with the state-of-the-art methods on ADE20K validation set using ViT backbone. 
The \seg\ uses
the \atm\ module with multi-layer inputs from the original ViT backbone, while the \emph{Shrunk} is the one that conducts QD to the ViT backbone and  saves  of the computational cost without sacrificing too much performance. Our method achieves  in terms of mIoU with the ViT-Large backbone. It is  better than the recent StructToken \cite{lin2022structtoken} using the same backbone. Besides, our \emph{Shrunk} version can also achieve a similar performance  with computational cost  GFLOPs which is much less than the ViT-Large backbone alone ( GFLOPs).

\begin{table}[t]
    \centering
        \caption{Experiment results on the ADE20K \texttt{val.}\  split.  `ms' means that mIoU is calculated using multi-scale inference. `' means the models use the backbone weights pre-trained by AugReg \cite{augreg}. `*' represents the model is reproduced under the same settings as the official repo. The GFLOPs is measured at single-scale inference with the given crop size. 
    }
    \vspace{0.5em}
    \begin{tabular}{rlcccc}
        \toprule
        Method & Backbone & Crop Size & GFLOPs & mIoU (ss) & mIoU (ms) \\
        \midrule
        UperNet* \cite{Upernet} & ViT-Base &  & >250 & 46.6 & 47.5 \\
        DPT* \cite{DPT} &ViT-Base&  & 219.8 &47.2 & 47.9 \\
        SETR-MLA* \cite{setr} &ViT-Base&  & 113.5 &48.2& 49.3 \\
        Segmenter* \cite{strudel2021segmenter}&ViT-Base & & 129.6 &49.0 & 50.0 \\
        StructToken \cite{lin2022structtoken}  & ViT-Base  & & >150 & 50.9 & 51.8\\
        \midrule
        \seg\ (Ours) &ViT-Base & & 120.9 & \textbf{51.3} & \textbf{53.0} \\
        \midrule
        \midrule
        
        DPT* \cite{DPT} & ViT-Large\textsuperscript{\textdagger} &  & 479.0 & 49.2& 49.5 \\
        UperNet* \cite{Upernet} & ViT-Large\textsuperscript{\textdagger}&  & >700  & 48.6& 50.0 \\
        SETR-MLA \cite{setr} & ViT-Large &  & 368.6 & 48.6&50.3\\
        MCIBI \cite{MCIBI}   & ViT-Large &  & >400 & - &50.8\\
        Segmenter \cite{strudel2021segmenter} & ViT-Large\textsuperscript{\textdagger}&  & 671.8 & 51.8 & 53.6\\
        StructToken \cite{lin2022structtoken}  & ViT-Large\textsuperscript{\textdagger} &  & >700 & 52.8 & 54.2\\
        \midrule
        \seg\ (\emph{Shrunk}, ours)  & ViT-Large\textsuperscript{\textdagger}  & & 373.5
        & 53.9 & 55.1 \\
       \seg\ (ours) & ViT-Large\textsuperscript{\textdagger}&  & 637.9 &\textbf{54.6} & \textbf{55.2}\\
        \bottomrule
    \end{tabular}

    \label{tab:ade20k}
\end{table}

\paragraph{Results on COCO-Stuff-10K.}
\cref{tab:cocostuff} shows the result on the COCO-Stuff-10K dataset. Our method achieves  which is higher than the previous state-to-the-art StrucToken by  with less computational cost. Our \emph{Shrunk} version achieves  with  GFLOPs, which is similar to the computational cost of a dilated ResNet-101 backbone but with much higher performance.

\begin{table}[h]
\centering
\caption{Experiment results on the COCO-Stuff-10K \texttt{test.}\  split. Following 
  published methods, 
  we report the results with multi-scale inference (denoted by `ms'). The GFLOPs is measured at single scale inference with a crop size of . 
}
\vspace{0.5em}
\begin{tabular}{rlcc}
\toprule
Method                     & Backbone & GFLOPs   & mIoU (ms) \\
\midrule
DANet        \cite{danet}              & Dilated-ResNet-101  & 289.3 &39.7      \\
MaskFormer    \cite{maskformer}             & ResNet-101-fpn  & 81.7 &  39.8      \\
EMANet      \cite{emanet}               & Dilated-ResNet-101 & 247.4  & 39.9      \\
SpyGR        \cite{spygr}              & ResNet-101-fpn & >80 & 39.9      \\
OCRNet       \cite{ocrnet}              & HRNetV2-W48 &167.9 & 40.5      \\
GINet       \cite{wu2020ginet}               & JPU-ResNet-101 & >200 & 40.6      \\
RecoNet     \cite{reconet}               & Dilated-ResNet-101  & >200 & 41.5      \\
ISNet     \cite{jin2021isnet}                 & Dilated-ResNeSt-101 & 228.3  &42.1      \\

MCIBI   \cite{MCIBI}                   & ViT-Large  & >380     & 44.9      \\
StructToken \cite{lin2022structtoken}               & ViT-Large  & >400     & 49.1      \\
\midrule
\seg\ (\emph{Shrunk}, ours) & ViT-Large   & 224.8    &  49.4      \\
\seg\ (ours) & ViT-Large   & 383.9   & \textbf{50.3} \\
\bottomrule
\end{tabular}

\label{tab:cocostuff}
\end{table}

\paragraph{Results on PASCAL-Context.}
\cref{tab:pascal} shows the results on the PASCAL-Context dataset. We follow HRNet~\cite{hrnet} to evaluate our method and report the results  under  classes (without background) and  classes (with background). 
\seg\ reaches mIoU  and  respectively for those two metrics that outperform the state-of-the-art methods using the ViT backbones with less computational cost. 


\begin{table}[h]
\centering
\caption{Expperiment results on the PASCAL-Context \texttt{val.}\  split. Following 
  published methods, 
  we report the results with multi-scale inference (denoted by `ms'). 
   mIoU: mIoU averaged over  classes (without background). mIoU: mIoU averaged over  classes ( classes plus background). Both metrics were used in the literature; and we report for the  classes. 
  The GFLOPs is measured at single scale inference with a crop size of .  
  }
  \vspace{0.5em}
\label{tab:pascal}
\begin{tabular}{rlc cc}
\toprule
Method     & Backbone & GFLOPs  &  mIoU  (ms) & mIoU  (ms)  \\
\midrule
    RefineNet   \cite{ lin2017refinenet}& ResNet-152 & -& - & 47.3\\
    UNet++  \cite{zhou2018unet++}  & ResNet-101 & -& 47.7 &-\\
    PSPNet   \cite{pspnet}&Dilated-ResNet-101 &  157.0 & 47.8 &- \\
    Ding \textit{et al.}  \cite{ding2018context} & ResNet-101 & -&51.6&-\\
    EncNet  \cite{Encnet}   &Dilated-ResNet-101& 192.1 &52.6 &- \\
    HRNet \cite{hrnet}   & HRNetV2-W48& 82.7 & 54.0 & 48.3 \\
    NRD  \cite{nrd} & ResNet-101 & 42.9 & 54.1 & 49.0 \\
    GFFNet \cite{li2020gated} & {\color{black}{Dilated-ResNet-101}} & {\color{black}{-}}& {\color{black}{54.3}} &{\color{black}{-}}\\
    

    
   EfficientFCN \cite{liu2020efficientfcn}  & {\color{black}{ResNet-101}} & {\color{black}{52.8}}& {\color{black}{55.3}} &{\color{black}{-}}\\
    
    {\color{black}{OCRNet  \cite{ocrnet}}} & {\color{black}{HRNetV2-W48}} & {\color{black}{143.9}}& {\color{black}{56.2}} &{\color{black}{-}}\\
    
SETR-MLA  \cite{setr}  & ViT-Large  & 318.5 & -     & 55.8      \\

Segmenter \cite{strudel2021segmenter}  & ViT-Large    & 346.2  & - &59.0     \\
\midrule
\seg\ (\emph{Shrunk}, ours) & ViT-Large  & 186.9 & 63.7  & 57.4 \\
\seg\ (ours) & ViT-Large & 321.6 & \textbf{65.3} & \textbf{59.3} \\
\bottomrule
\end{tabular}
\end{table}


\subsection{Ablation Study}
In this section, we conduct the ablation study to show the effectiveness of our proposed methods. 

\paragraph{Effect of the \atm\ module.}
\cref{tab:mask_pred} shows the effect of the \atm\ module. We set the SETR-naive as the baseline, which  uses two  convolutions to get per-pixel classifications directly from the last layer of the ViT-Base transformer output. 
We can see that by applying the \atm\ module and supervise with a regular cross-entropy loss, \atm\ is capable of providing  of performance boost. However, it is more beneficial to decouple the classification and mask prediction process and use the mask and classification supervision separately (3.1\% increase).

\paragraph{Ablation of using different layers as input for \seg.}
\cref{tab:atm_layer} shows the performance boost that multiple layers input can provide. We can see that the performance boost of feature maps from additional lower layers is obvious (+). 
We then involved more layers of features and see further performance gains.
We empirically choose to use three layers for its best performance.






    









    










\begin{table}[h]
\parbox{.49\linewidth}{
    \centering
        \caption{
        Comparison between our proposed \atm\ module with other methods. `CE loss' indicates the cross-entropy loss that is commonly used in semantic segmentation. The experiments are carried out on the ViT-Base backbone using ADE20K dataset. 
        }
        \vspace{0.5em}
    \label{tab:mask_pred}
    \begin{tabular}{rll }
    \toprule
    Decoder    & Loss & mIoU (ss) \\
    \midrule
     SETR & CE loss &   46.5\\
     \atm  &  CE loss&  47.0 (+0.5)\\
     \atm  &    loss&  \textbf{49.6 (+3.1)} \\

    \bottomrule
    \end{tabular}
}
\hfill
\parbox{.47\linewidth}{
    \centering
        \caption{Ablation results of using different layer inputs to the \seg\ structure on ADE20K dataset using ViT-Base as the backbone. Involving multi-layer features can bring obvious performance gain.}
        \vspace{.5em}
            \label{tab:atm_layer}
    \begin{tabular}{lrl}
    \toprule
        &Used layers & mIoU (ss) \\
        \midrule
        Single & {[12]} &  49.6  \\
        Cascade &{[6, 12] }&  50.9 (+1.3)\\
        Cascade &{[6, 8, 12] }& \textbf{51.3 (+1.7)} \\
        Cascade &{[3, 6, 9, 12] }& 51.2 (+1.6) \\
    \bottomrule
    \end{tabular}
}

\end{table}


\paragraph{Ablation for the ATM Decoder.}

We conduct experiments to show the effectiveness of the proposed ATM decoder

\paragraph{\seg\ on hierarchical backbones.}
Shown in \cref{tab:hierarchi},
the \seg\ structure is also able to apply to hierarchical backbones. We choose the most competitive methods Maskformer \cite{maskformer} and Mask2former \cite{cheng2021mask2former} for comparison. Results indicate that even though our method is not designed for hierarchical backbones, we can still achieve competitive performance while being efficient in terms of computational cost.

\paragraph{Ablation for the QD module.}
The motivation to use QD is to make use of the pre-train weights of the backbone. As in \cref{tab:downsample}, if we use a stride  convolution with learnable parameters to down-sample the query, it will destroy the pre-train weights and dramatically decrease the performance. If the down-sampling is applied to both Q and (K, V), there will be an inevitable loss in information during the down-sampling process which is reflected in the weaker performance.
 We found that applying  nearest down-sampling on query only for the QD module is the better option.


\paragraph{Ablation of the components in \emph{Shrunk} structure.} 
Shown in \cref{tab:tpn ablation}, we studied the effect of each component (QD and QU) in the \emph{Shrunk} structure. 
The results presented matches the structures illustrated in \cref{fig:struct}. When QD is applied, the performance decreases by  from the `Single' ATM head. However, by applying QU, the performance is recovered. QD learns a non-linear down-sampling by the attention mechanism between key and query. One query will attend to several keys. QU is used to preserve the resolution and at the same time provide low-level feature information. We can see that by using QD and QU 
jointly, the performance can be retained and the computational cost is reduced. 
\atm\ module can also be used as the decoder to form our \emph{Shrunk} structure to further boost  performance. 

\begin{table}[]
\parbox{.49\linewidth}{
    \centering
        \caption{The experiments use the Swin-Tiny \cite{liu2021swin} backbone and are carried out on the ADE20K dataset. The GFLOPs are measured at single scale inference with a crop size of . QD: query-based down-saumping. QU: query-based upsampling. 
        }
    \vspace{0.5em}
     \label{tab:hierarchi}
    \begin{tabular}{rcc}
    \toprule
        Method & mIoU (ss) & GFLOPs \\
        \midrule
        Maskformer \cite{maskformer} & 46.7 & 57.3\\
        Mask2former \cite{cheng2021mask2former} & \textbf{47.7} & 73.7 \\
        \seg\ (Ours) & 47.1 & \textbf{48.0}\\
        \bottomrule
    \end{tabular}
}
\hfill
\parbox{.47\linewidth}{
    \centering
        \caption{Ablation of the QD module in terms of the targets and methods to down-sample. The experiments are carried out on the ViT-Large backbone of ADE20K dataset. }
        \vspace{.5em}
            \label{tab:downsample}
    \begin{tabular}{lcc}
    \toprule
    Applied to & Methods &  mIoU (ss)  \\
    \midrule
        Q & Conv & 44.5  \\
        Q, K, V & Nearest &  52.6  \\
        Q & Nearest & \textbf{53.9} \\
    \bottomrule
    \end{tabular}
}
\end{table}


\begin{table}[h]
    \centering
    \vspace{-1em}
        \caption{Ablation results of \emph{Shrunk} version on the ADE20K dataset. The GFLOPs are measured at single scale inference with a crop size of  on ViT-Base backbone.
        }
            \label{tab:tpn ablation}
    \begin{tabular}{lccllc}
    \toprule
        Structure & QD & QU & Head & mIoU (ss) & GFLOPs \\
        \midrule
        Single &  & & SETR & 46.5 & 107.3 \\
        Single  &  & & \atm & 49.6 (+3.1) & 115.8 \\
        Naive \emph{Shrunk} & \checkmark & & \atm & 46.9 (+0.4) & 74.1 \\
        \emph{Shrunk} & \checkmark &\checkmark & \atm & \textbf{50.0 (+3.5)} & 97.1\\
    \bottomrule
    \end{tabular}



\end{table}








\section{Conclusion}
We proposed an effective structure using plain ViT transformer backbones termed \seg\ for the semantic segmentation task.
For the first time, we utilize spatial information in attention maps for semantic segmentation. To implement this idea, we proposed an Attention-to-mask (\atm) module that can derive mask predictions during the attention calculation process. We show on a number of semantic segmentation benchmarks that our method is efficient and achieves state-of-the-art performance. We also proposed a \emph{Shrunk} structure which is applied to the backbone and capable of reducing  of the computational cost while still maintaining competitive performance.  
We believe both structures can be strong paradigms, especially for semantic segmentation using ViT backbones. 
Last but not the least, our method still has some limitations. One of the limitations is that the  large amount of GPU memory consumed by the global attention mechanism might not be supported by some devices, which might restrict the applicability of our structures. 


\textbf{Acknowledgments} 
C. Shen's participation was in part supported by a major grant from Zhejiang Provincial Government.  Y. Liu's participation was in part supported by a start-up funding of  University of Adelaide.  This research was in part supported by Meituan.

\small
\bibliographystyle{ieeetr}
\bibliography{main}






\appendix


\section{Appendix}















\paragraph{}
In this section, we show more evaluation results to demonstrate the performance of the proposed SegViT structure.

\begin{figure}[t]
\raisebox{-1\height}{\vspace{0pt}\includegraphics[width=0.6\textwidth]{figures/supp/supp.pdf}}\hfill \begin{minipage}[t]{0.35\textwidth}
\caption{
Accuracy vs.\ computational cost on the ADE20K \texttt{val.}\ split. All the performances and computational costs are measured at single scale inference.  Our proposed SegViT structure can achieve a better trade-off and the best performance among methods using ViT backbone. 
}
 \label{fig:supp}
\end{minipage}
\end{figure}

\subsection{Illustration of the accuracy vs.\ computational cost}
As shown in \cref{fig:supp}, we achieve the best performance with a better trade-off between computational cost and accuracy compared to other methods that use ViT backbone. Also, for our SegViT \emph{Shrunk} version, we dramatically reduced the computation while still retaining competitive performance. 

\subsection{More Visualization Results}

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/supp/ade.pdf}
    \caption{
    Competitive segmentation results on the ADE20K \texttt{val.} split.}
    \label{fig:ade}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/supp/coco.pdf}
    \caption{Competitive segmentation results on the COCO-stuff-10K dataset.}
    \label{fig:coco}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/supp/pc.pdf}
    \caption{Competitive segmentation results on the PASCAL-Context  dataset with  classes.}
    \label{fig:pc}
\end{figure}

\paragraph{Competitive segmentation results on three datasets.}
\cref{fig:ade}, \cref{fig:coco} and \cref{fig:pc} illustrates the model performance on ADE20K, COCO-Stuff-10K and PASCAL-Context datasets respectively using single-scale inference. We can see that in various indoor and outdoor scenes, SegViT can produce satisfactory segmentation results. 








\subsection{More Ablation Study Results}
Different decoder methods have their corresponding feature merge types and loss types. We compare the difference on a plain ViT base backbone shown in Table~\ref{tab:struct compare}. For hierarchical backbones, such as Swin, since the resolution of the feature maps of each stage is reduced, to get feature maps with large resolution and rich semantic information, FPN is necessary. However, in Table~\ref{tab:struct compare}, FPN structure can not work well with plain vision transformers. 
For non-hierarchical backbones, such as ViT, the resolution is maintained and the last layer feature map contains the richest semantic information.  Thus, our proposed method that uses the tokens to merge features of different levels got better performance. By simply changing the FPN structure with out ATM based Token merge, we improve the performance from 46.7\% to 50.6\%.
For the loss type, pixel level loss indicates the regular cross-entropy loss applied to the feature map. The dot product loss indicates the loss used in \cite{detr} and \cite{maskformer}. 
Attention mask loss means the mask supervision is directly applied to the similarity map generated by the ATM during attention calculation. We can see that the loss supervised on the attention mask further improve the result with 0.6\%. 


\begin{table}[]
\centering
\setlength{\tabcolsep}{1.5pt}
\footnotesize
\caption{Ablation results of different decoder methods with their corresponding feature merge types and loss types. ViT-Base is employed as the backbone for all the variants.}
\vspace{0.5em}
\label{tab:struct compare}

\begin{tabular}{rcccccc}
\toprule
\multicolumn{1}{c}{} & \multicolumn{2}{c}{Multi-level Features} & \multicolumn{3}{c}{Loss Types} & \multicolumn{1}{c}{} \\ 
\cmidrule(r){2-3}
\cmidrule(l){4-6}
\multicolumn{1}{c}{Decoder} & FPN & \multicolumn{1}{c}{Token Merge} & Pixel level & Dot product & Attention Mask & \multicolumn{1}{c}{mIoU (ss)} \\ 
\midrule
SETR-MLA \cite{setr}& \checkmark & \multicolumn{1}{c}{} & \checkmark &  &  & 48.2 \\
Segmenter \cite{strudel2021segmenter} &  & \multicolumn{1}{c}{} & \checkmark &  &  & 49.0 \\
MaskFormer \cite{maskformer} & \checkmark & \multicolumn{1}{c}{} &  & \checkmark &  & 46.7 \\
Ours-Variant 1 &  & \multicolumn{1}{c}{} &  &  & \checkmark & 49.6 \\
Ours-Variant 2&  & \multicolumn{1}{c}{\checkmark} &  & \checkmark &  & 50.6 \\
Ours &  & \multicolumn{1}{c}{\checkmark} &  &  & \checkmark & 51.2 \\ 
\bottomrule
\end{tabular}

\end{table}



 























\end{document}