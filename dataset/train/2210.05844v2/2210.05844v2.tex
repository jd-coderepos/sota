
\documentclass{article}

\PassOptionsToPackage{numbers, compress}{natbib}



\usepackage[final]{neurips_2022}


\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         \usepackage{graphicx}
\usepackage{amsmath}

\usepackage{mathtools}
\usepackage{xspace,xfrac}
\usepackage{subfigure}
\usepackage{multirow}
\usepackage{color}
\usepackage{comment}
\usepackage[capitalise]{cleveref}

\usepackage{caption}
\captionsetup{labelfont=bf,tableposition=top}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    breaklinks=true,
    urlcolor=blue,
    linkcolor=blue,
    bookmarksopen=false,
    filecolor=black,
    citecolor=blue,
    linkbordercolor=blue
}

\usepackage{bibspacing}

\makeatletter
\renewcommand{\paragraph}{\@startsection{paragraph}{4}{\z@}{.5ex \@plus 1ex \@minus .2ex}{-1em}{\normalfont\normalsize\bfseries}}
\makeatother

\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}

\def\eg{\emph{e.g}\onedot}
\def\ie{\emph{i.e}\onedot} 
\def\cf{\emph{c.f}\onedot}
\def\etc{\emph{etc}\onedot} \def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot} 
\def\dof{d.o.f\onedot}
\def\etal{\emph{et al}\onedot}
\makeatother
\DeclareMathOperator{\sign}{sgn}

\def\BTheta{{{\boldsymbol \theta}}} 
\def\R{{\mathbb R}}
\def\atm{ATM} 
\def\tpn{QBD}
\def\seg{SegViT}
\def\SP{~~}



\title{SegViT: Semantic Segmentation with Plain Vision Transformers
}




\author{
   Bowen Zhang$^{1*}$,
\SP 
 Zhi Tian$^{2}$\thanks{The first two authors contributed equally.
CS is the corresponding author, e-mail: \tt chunhua@me.com
},
\SP  
 Quan Tang$^4$,
\SP
\textbf{Xiangxiang Chu}$^2$,
\\[0.051cm]
\textbf{Xiaolin Wei}$^2$,
\SP 
\textbf{Chunhua Shen}$^3$,
\SP
  \textbf{Yifan Liu}$^{1}$  
 \\ [0.285cm]
 $^1$ The University of Adelaide, Australia  ~ ~ ~ ~   $^2$ Meituan Inc.
 \\$^3$ Zhejiang University, China ~ ~ ~ ~   $^4$ South China University of Technology, China
}


\begin{document}


\maketitle


\begin{abstract}


We explore the capability of plain
Vision Transformers (ViTs)
for semantic segmentation and propose the \seg. Previous ViT-based segmentation networks usually learn a pixel-level representation from the output of the ViT. Differently, we make use of the fundamental component---attention mechanism, to generate masks for semantic segmentation. Specifically, we propose the Attention-to-Mask (\atm) module, in which the similarity maps between a set of learnable class tokens and the spatial feature maps are transferred to the segmentation masks. 
Experiments show that our proposed \seg\ using the \atm\ module  
outperforms its counterparts using the plain ViT backbone on the ADE20K dataset and achieves new state-of-the-art performance on COCO-Stuff-10K and PASCAL-Context datasets. 
Furthermore, to reduce the computational cost of the ViT backbone, we propose query-based down-sampling (QD) and query-based up-sampling (QU) to build a \emph{Shrunk} structure.
With the proposed  \emph{Shrunk} structure, the model can save up to $40\%$ computations while maintaining competitive performance. The code is available through the following link: \url{https://github.com/zbwxp/SegVit}.










 





  
  
  
\end{abstract}


\section{Introduction}
Semantic segmentation is a dense prediction task in computer vision that requires pixel-level classification of an input image. 
Fully Convolutional Networks (FCN)~\cite{fcn} are widely used in recent state-of-the-art methods.
This paradigm includes a deep convolutional neural network as the encoder/backbone and a segmentation-oriented decoder to provide dense predictions. A 1$\times$1 convolutional layer is usually applied to a representative feature map to obtain the pixel level predictions. To achieve higher performance, previous works~\cite{chen2017deeplabv2, ocrnet,fpn} focus on enriching the context information or fusing multi-scale information. However, the correlations among spatial locations are hard to model explicitly in FCNs due to the limited receptive field.





Recently, Vision Transformers (ViT) \cite{vit},
which make use of the spatial attention mechanism are introduced to the field of computer vision.
Unlike typical convolution-based backbones, the ViT has a plain and non-hierarchical architecture that keeps the resolution of the feature maps all the way through.
The lack of the down-sampling process (excluding tokenizing the image) brings differences to the architecture to do the semantic segmentation task using ViT backbone. 
Various semantic segmentation methods~\cite{setr,DPT,strudel2021segmenter} based on ViT backbones have achieved promising performance due to the powerful representation learned from the pre-trained backbones. However, the potential of the attention mechanism is not fully explored.

Different from previous per-pixel classification paradigm~\cite{setr,DPT,strudel2021segmenter}, we consider learning a meaningful  class token and then finding local patches with higher similarity to it. To achieve this goal, we propose the Attention-to-Mask (\atm) module. More specifically, we employ a transformer block that takes the learnable class tokens as queries and transfers the spatial feature maps as keys and values. A dot-product operator calculates the similarity maps between queries and keys. We encourage regions belonging to the same category to generate larger similarity values for the corresponding category (\ie a specific class token). \cref{fig:attn_vs_mask} visualizes the similarity maps between the features and the `Table' and `Chair' tokens. By simply applying a {\tt Sigmoid} operation, we can transfer the similarity maps to the masks. Meanwhile, following the design of a typical transformer block, a {\tt Softmax} operation is also applied to the similarity maps to get the cross attention maps.
The `Table' and `Chair' tokens are then updated as in any regular transformer decoders, by a weighted sum of the values with the cross attention maps as the weights.
Since the mask is a byproduct of the regular attentive calculations, negligible computation is involved during the operation.

Building upon this efficient \atm~module, we propose a new semantic segmentation paradigm with the plain ViT structure, dubbed~\seg.
In the paradigm, several \atm~modules are employed on different layers, and we get the final segmentation mask by adding the outputs from different layers together. \seg\ outperforms its ViT-based counterparts with less computational cost. However, compared with previous encoder-decoder structures that use hierarchical networks as encoders, ViT backbones as encoders are generally heavier. 
To further reduce the computational cost, we employ a \emph{Shrunk} structure consisting of query-based down-sampling (QD) and query-based up-sampling (QU). The QD can be inserted into the ViT backbone to reduce the resolution by half and QU is used parallel to the backbone to recover the resolution. The \emph{Shrunk} structure together with the \atm~module as the decoder can reduce up to $40\%$ computations while maintaining competitive performance.







We summarize our main contributions as follows:

\begin{itemize}
    \item We propose an Attention-to-Mask (\atm) decoder module that is effective and efficient for semantic segmentation. For the first time, we utilize the spatial information in attention maps to generate mask      predictions for each category, which can work as a new paradigm for semantic segmentation.
    
    \item We managed to apply our \atm\ decoder module to the plain, non-hierarchical ViT backbones in a cascade manner and designed a structure namely \seg\ that achieves  mIoU $55.2\%$  on the competitive ADE20K dataset which is the best and lightest among methods that use ViT backbones. We also benchmark our method on the PASCAL-Context dataset ($65.3\%$ mIoU) and COCO-Stuff-10K dataset ($50.3\%$ mIoU) and achieve new state-of-the-art performance.
    
    \item  We further explore the architecture of ViT backbones and work out a \emph{Shrunk} structure
    to apply to the backbone to reduce the
    overall computational cost while still maintaining competitive performance. This alleviates the disadvantage of ViT backbones that are usually more computationally intensive compared to their hierarchical counterparts. Our \emph{Shrunk} version of \seg\ on the ADE20K dataset reaches mIoU  $55.1\%$ with the computational cost of 373.5 GFLOPs which is about $40\%$ off compared to the original \seg\ (637.9 GFLOPs).
\end{itemize}


\begin{figure}[t]
    \centering
    \includegraphics[width=0.8
    \textwidth]{figures/fig1.pdf}
    \caption{\textbf{The overall concept of 
    our Attention-to-Mask decoder.} 
    In a typical attentive process, the dot-product is first 
    calculated 
    between queries and keys to measure the similarity (as illustrated on the left).
    If the similarity map is applied with ${\tt Softmax}$ operation on the spatial dimension, the output is the typical attention map (multiple heads are summed together).  
    However, if the same similarity map is applied with a per-pixel operation ${\tt Sigmoid}$, it
    produces a mask that indicates the area with certain similarity. 
    Based on the assumption that the tokens within the same category have higher similarity, we can train a token vector to have high similarity within tokens of the specific category and low similarity elsewhere. 
    In the meantime, this process does not violate the attention mechanism.
    Thus, it can process alongside the original transformer layers.
    }
    \label{fig:attn_vs_mask}
\end{figure}





\section{Related Work}
\paragraph{Semantic segmentation.}
Semantic segmentation which requires pixel-level classification on an input image is a fundamental task in computer vision. Fully Convolutional Networks (FCN) used to be the dominant approach to this task. 
Initial per-pixel approaches such as \cite{farabet2012learning, long2015fully} attribute the class label to each pixel based on the per-pixel probability. 
To enlarge the receptive field, several approaches \cite{pspnet, dv3} have proposed dilated convolutions or apply spatial pyramid pooling to capture contextual information of multiple scales. 
With the introduction of attention mechanisms, \cite{dranet, cpnet, setr}  replace the feature merge conducted by convolutions and pooling with attention to better capture long-range dependencies. 

Recent works \cite{maskformer, strudel2021segmenter, knet} decouple the per-pixel classification process. They reconstruct the structure by using a fixed number of learnable tokens and use them as weights for the transformation to apply on feature maps. 
Binary matching rather than cross-entropy is used to allow overlaps between feature maps and learnable tokens are used to dynamically generate classification probabilities. 
This paradigm enables the classification process to be conducted globally and alleviates the burden for the decoder to do per-pixel classification, which as a result, is more precise and the performance is generally better.  
However, for those methods, the feature map is still calculated in a static manner, usually requiring feature merge modules such as FPN~\cite{fpn}. 

\paragraph{Transformers for vision.}
Attention-based transformer backbones have become powerful alternatives to standard convolution based networks for image classification tasks. 
The original ViT \cite{vit} is a plain, non-hierarchical architecture. 
Various hierarchical transformers such as \cite{pvt, liu2021swin, chu2021twins, xie2021segformer, p2t} have been presented afterwards. 
These methods inherit some designs from convolution based networks such as hierarchical structures, pooling and down-sampling with convolutions.
As a result, they can be used as a straightforward replacement for convolutional based networks and applied with previous decoder heads for tasks such as semantic segmentation. 



\paragraph{Plain-backbone decoders.}
High-resolution feature maps generated by backbones are important for dense prediction tasks such as semantic segmentation. 
Typical hierarchical transformers use feature merge techniques such as FPN \cite{fpn} or dilated backbones to generate high-resolution feature maps. 
However, for plain, non-hierarchical transformer backbones, the resolution remains the same for all layers. SETR \cite{setr} proposed a simple strategy to treat transformer outputs in a sequence-to-sequence perspective to solve segmentation tasks. 
Segmenter \cite{strudel2021segmenter} 
joints random initialized class embeddings and the transformer patch embeddings together and applies several  self-attention layers to the joint token sequence 
 to obtain updated class embeddings and patch embeddings semantic prediction.
In our study, we consider learning a class token and then finding local patches with higher similarities with the help of the attention map, making the inference process more direct and efficient. 



\section{Method}
\begin{figure}
    \centering
    
    \includegraphics[width=0.8\textwidth]{figures/atm_arch.pdf}
    \caption{\textbf{The overall \seg\ structure with the \atm\ module.}
    The Attention-to-Mask (\atm) module  inherits the typical transformer decoder structure. It takes in  randomly initialized class embeddings as queries and the feature maps from the ViT backbone to generate keys and values. The outputs of the \atm\ module are used as the input queries for the next layer.
    The \atm\ module is carried out sequentially with inputs from different layers of the backbone as keys and values in a cascade manner. 
    A linear transform is then applied to the output of the 
    \atm\ module to produce the class predictions for each token. The mask for the corresponding class is transferred from the similarities between queries and keys in the \atm\ module.
    }
    \label{fig:atm_decoder}
\end{figure}

\subsection{Encoder}
Given an input image $I \in \R^{H \times W \times 3}$, a plain vision transformer backbone reshapes it into a sequence of tokens $\mathcal{F}_{0} \in \R^{L \times C}$ where $L = HW/P^2$, $P$ is the patch size and $C$ is the number of channels. 
Learnable position embeddings of the same size of $\mathcal{F}_{0}$ are added to capture the positional information. Then, the token sequence $\mathcal{F}_{0}$ is applied with $m$ transformer layers to get the output.
We define the output tokens for each layer as $[\mathcal{F}_{1}, \mathcal{F}_{2}, \dots , \mathcal{F}_{m}] \in \R^{L \times C}$.
Typically, a transformer layer consists of a multi-head self-attention 
block followed by a point-wise multilayer perceptron 
block with layer norm 
in between and then a residual connection is added afterward. 
The transformer layers are stacked repetitively  several times. 
For a plain vision transformer like ViT, there are no other modules involved and for each layer, the number of the tokens is not changed.

\subsection{Decoder}
\paragraph{Mask-to-Attention (\atm).}
Cross attention can be described as the mapping between two sequences of tokens. We define two token sequences as $\mathcal{G} \in \R^{N \times C}$ with the length $N$ equals to the number of classes and  $\mathcal{F}_{i} \in \R^{L \times C}$.
First, linear transformations are applied to each of them to form query (Q), key (K) and values (V), as presented by Eq.~\eqref{eq:2}.
\begin{equation}
    Q  =  \phi_{q} (\mathcal{G}) \in \R^{N \times C},\ 
    K = \phi_{k} (\mathcal{F}_{i}) \in \R^{L \times C},\ 
    V = \phi_{v} (\mathcal{F}_{i}) \in \R^{L \times C},
    \label{eq:2}
\end{equation}
The similarity map is calculated between the query and the key. 
Following the scaled dot-product attention mechanism, the similarity map and attention map are calculated by: 
\begin{equation} 
\begin{split}
       S(Q, K)& = \frac{QK^T}{\sqrt{d_{k}}}\in \R^{N \times L}, \\
        Attention(\mathcal{G}, \mathcal{F}_{i}) & = 
        {\tt Softmax}(S(Q, K))V \in \R^{N \times C},
\end{split}
\end{equation}
where $\sqrt{d_{k}}$ is a scaling factor with $d_{k}$ equals to the dimension of the keys. 
The shape of the similarity map $S(Q, K)$ is determined by the length of 
the two token sequences $N$ and $L$.
The attention mechanism is then to update  $\mathcal{G}$ by a weighted sum of $V$, where the weight assigned to the summation is the similarity map applied with $\tt Softmax$ along the dimension $L$.

Dot-product attention uses the {\tt Softmax} function to exclusively concentrate the attention on the token that has the most similarity. However, we suppose that the tokens other than ones that yield maximum similarities are also meaningful. Based on this intuition, we design a lightweight module that generates semantic predictions more directly. To be more specific, we assign $\mathcal{G}$ as the class embeddings for
the segmentation task and $\mathcal{F}_{i}$ as the output of layer $i$ of the ViT backbone.
We pair a semantic mask to each token in $\mathcal{G}$ to represent the semantic prediction for each class.
The calculation for the mask is:
\begin{equation}
        Mask(\mathcal{G}, \mathcal{F}_{i}) = {\tt Sigmoid}(S(Q, K)) \in \R^{N \times L}
\end{equation}
The shape of the masks is $N\times L$, which can be further reshaped to $N\times H/P \times W/P $. 
The structure of the \atm\ mechanism is illustrated in the right part in \cref{fig:atm_decoder}. Masks are the middle output of the cross attention. The final output tokens from the ATM module are used for classification. We apply a linear transformation followed by a $\tt Softmax$ activation to the output class tokens to get class probability predictions. Note that we follow~\cite{maskformer} to add a `no object' category (\O) in case the image doesn't contain certain classes. During inference, the output is produced by the dot-product between the class probability and the mask groups. 


\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/single_vs_shrink.pdf}
    \caption{\textbf{The structure comparison between \seg\ with a single layer and the \emph{Shrunk} version.}
    (a) illustrates the \seg\ structure with \atm\ module used once with the last layer of the ViT backbone as the input to generate predictions.
    (b) uses the query-based down-sampling (QD) module to implement a naive way to
    shrink the resolution of the features of the backbone from $\nicefrac{1}{16}$ to $\nicefrac{1}{32}$ and thus reduces the overall computational cost. 
    (c) is the proposed \emph(shrunk) version which applies the additional query-based up-sampling module.
    The \emph{Shrunk} version can save up to 40\% of computational cost when using the ViT-Large backbone without much sacrifice to the performance.
    }
    \label{fig:struct}
\end{figure}


Plain backbones such as ViT does not have multiple stages with features of different scale. Thus, structures such as FPN to merge features with multiple scales are not applicable. 
However, features other than the last layer contain rich low-level semantic information and are beneficial to the performance. 
We designed a structure that can make use of the feature maps from different layers of ViT to compact with our \atm\ decoder namely \seg.
In this study, we also found a way to compact the computational cost for the ViT backbone without sacrificing performance.
This proposed \emph{Shrunk} version of \seg\ uses query-based down-sampling (QD) module together with a query-based up-sampling (QU) module to compress the ViT backbone and bring an overall reduction to the computational cost. 

\paragraph{The \seg\ structure.}
As illustrated in \cref{fig:atm_decoder},
an \atm\ decoder takes in $N$ tokens as the class embeddings and another sequence of tokens as the base to calculate keys and values for the \atm\ module to generate masks. 
The output of the \atm\  is $N$ updated tokens and $N$ masks corresponding to each class token. 
We use random initialized learnable tokens as the class embeddings and the output of the last layer of the ViT backbone as the base first.
To make use of multi-layer information, the output of the first \atm\ decoder is then used as the class embeddings for
the next \atm\ decoder with the output of another layer of the ViT backbone as the base.
This process is repeated another time so that we can get three groups of tokens and masks. 
Formally, the loss function of each layer  can be formulated as,
\begin{equation}
    \begin{aligned}
    \mathcal{L}_{overall}=\mathcal{L}_{cls} +\mathcal{L}_{mask} & =\mathcal{L}_{cls} +
    \lambda_{focal}\mathcal{L}_{IoU} + \lambda_{dice}\mathcal{L}_{dice} \\
    \end{aligned}
\end{equation}

In each group, the output tokens are supervised by the classification loss ($\mathcal{L}_{cls}$) which is mentioned above and the masks are summed orderly and supervised by the mask loss ($\mathcal{L}_{mask}$) which is a linear combination of a focal loss \cite{lin2017focal} and a dice loss \cite{diceloss} multiplied by hyper-parameters $\lambda_{focal}$ and $\lambda_{dice}$ respectively as in DETR \cite{detr}. The loss of all three groups are then summed together.
We have further experiments to show that this design is beneficial and efficient.



\paragraph{The \emph{Shrunk} structure.}
Plain transformer backbones such as ViT is known to have larger computational cost than their counterparts with similar performance. 
We propose a \emph{Shrunk} structure using query-based down-sampling (QD) and up-sampling (QU). 
Since the shape of the output of the attention module is determined by the shape of the query, 
we can apply down-sampling before the query transformation to realize the QD or insert new query tokens during the cross attention to realize the QU. By changing the resolution with the number of query tokens, the spatial size is changed according to the cross attention, providing more flexibility to preserve (recover) important regions.
To be more specific, 
in the QD layer, we use the nearest sampling to reduce the number of the query tokens while keep the size of the key and value tokens. When passing through a transformer layer, the values are weighted and summed by the attention map between query tokens and the key tokens. This is non-linear downsampling that will pay more attention to the important regions.  In the QU layer, we employ a transformer decoder structure~\cite{2017attention} and initialize new learnable tokens as queries based on the desired output resolution.  

As shown 
in \cref{fig:struct}, we design the 
\seg\ structure with one single layer as the baseline (a).  
We first try 
a naive approach (b), which is to apply the QD once at the $\nicefrac{1}{3}$ depth of the backbone (\eg, the $8\rm th$ layer of a backbone with $24$ layers) to down-sample 
the resolution of the layer output from $\nicefrac{1}{16}$ to $\nicefrac{1}{32}$
so as 
to reduce the overall computational cost.
The performance drops as expected since the QD process involves information lose.

To compensate for the information loss in the naive `shrunk' version, we further apply 
 two QU layers in parallel with the backbone.
 This is our proposed \emph{Shrunk} version (c).
 The first QU layer takes in features with $\nicefrac{1}{16}$ resolution from the low level of the backbone. Its output is then used as the query to make cross attention with the down-sampled features  with $\nicefrac{1}{32}$ resolution from the last layer of the backbone.
The shape of the output of this QU structure is of $\nicefrac{1}{16}$ resolution.

Directly reducing the number of the query tokens inevitably 
harms  the final performance.
However, with our designed QU layer and the ATM module, the \emph{Shrunk} structure is able to reduce $40\%$ of overall computational cost while still being competitive in performance.

















\section{Experiments}
\subsection{Datasets}
\paragraph{ADE20K \cite{ade20k}}  
is a challenging scene parsing dataset which contains $20,210$ images  as the training set and $2,000$ images as the validation set with 150 semantic classes. 

\textbf{COCO-Stuff-10K \cite{cocostuff}}
is a scene parsing benchmark with $9,000$ training images and $1,000$ test images. Even though the dataset contains $182$ categories, not all categories exist in the test split. We follow the implementation of mmsegmentation \cite{mmseg} with $171$ categories to conduct the experiments.

\textbf{PASCAL-Context \cite{pascal_context}}
is a dataset with $4,996$ images in training set and $5,104$ images in the validation set. There are $60$ semantic classes in total, including a class representing `background'. 


\subsection{Implementation details}

\textbf{Transformer backbone.} 
We use the naive ViT~\cite{vit} as the backbone. In particular, we use its `Base' variation for most ablation studies and provide results on the `Large' variation. Since there can be a huge difference with different pre-trained weights, as suggested by Segmenter~\cite{strudel2021segmenter}, we use the weights provided by Augreg~\cite{augreg} following the counterparts~\cite{strudel2021segmenter, lin2022structtoken} for a fair comparison. The weights are obtained by training on ImageNet-21k with strong data augmentation and regularization. For a simple reference, we report that for pre-trained weights provided by ViT~\cite{vit} and Augreg~\cite{augreg}, the mIoU scores using the same training recipe on ADE20K dataset are $51.7\%$ and $54.6\%$, respectively. 
\textbf{Training settings.}
We use MMSegmentation \cite{mmseg} and follow the commonly used training settings. 
During training, we applied data augmentation sequentially via random horizontal flipping, random resize with the ration between $0.5$ and $2.0$ and random cropping ($512\times 512$ for all except that we use $480 \times 480$ for PASCAL-Context and $640\times 640$ for ViT-large on ADE20K).
The batch size is $16$ for all datasets with a total iteration of $160k$, $80k$ and $80k$ for ADE20k, COCO-Stuff-10k and PASCAL-Context respectively. 
\textbf{Evaluation metric.} We use the mean Intersection over Union (mIoU) as the metric to evaluate the performance. `ss' means single-scale testing and `ms' test time augmentation with multi-scaled $(0.5, 0.75, 1.0, 1.25, 1.5, 1.75)$ inputs. All reported mIoU scores are in a percentage format. All reported computational costs in GFLOPs are measured using the fvcore~\footnote{\url{https://github.com/facebookresearch/fvcore}} library.




\subsection{Comparisons with the State-of-the-art Methods}
\paragraph{Results on ADE20K.}
\cref{tab:ade20k} reports the comparison with the state-of-the-art methods on ADE20K validation set using ViT backbone. 
The \seg\ uses
the \atm\ module with multi-layer inputs from the original ViT backbone, while the \emph{Shrunk} is the one that conducts QD to the ViT backbone and  saves $40\%$ of the computational cost without sacrificing too much performance. Our method achieves $55.2\%$ in terms of mIoU with the ViT-Large backbone. It is $1.0\%$ better than the recent StructToken \cite{lin2022structtoken} using the same backbone. Besides, our \emph{Shrunk} version can also achieve a similar performance $55.1\%$ with computational cost $373.5$ GFLOPs which is much less than the ViT-Large backbone alone ($612.3$ GFLOPs).

\begin{table}[t]
    \centering
        \caption{Experiment results on the ADE20K \texttt{val.}\  split.  `ms' means that mIoU is calculated using multi-scale inference. `$\dagger$' means the models use the backbone weights pre-trained by AugReg \cite{augreg}. `*' represents the model is reproduced under the same settings as the official repo. The GFLOPs is measured at single-scale inference with the given crop size. 
    }
    \vspace{0.5em}
    \begin{tabular}{rlcccc}
        \toprule
        Method & Backbone & Crop Size & GFLOPs & mIoU (ss) & mIoU (ms) \\
        \midrule
        UperNet* \cite{Upernet} & ViT-Base & $512 \times 512$ & >250 & 46.6 & 47.5 \\
        DPT* \cite{DPT} &ViT-Base& $512 \times 512$ & 219.8 &47.2 & 47.9 \\
        SETR-MLA* \cite{setr} &ViT-Base& $512 \times 512$ & 113.5 &48.2& 49.3 \\
        Segmenter* \cite{strudel2021segmenter}&ViT-Base & $512 \times 512$& 129.6 &49.0 & 50.0 \\
        StructToken \cite{lin2022structtoken}  & ViT-Base  & $512 \times 512$& >150 & 50.9 & 51.8\\
        \midrule
        \seg\ (Ours) &ViT-Base & $512 \times 512$& 120.9 & \textbf{51.3} & \textbf{53.0} \\
        \midrule
        \midrule
        
        DPT* \cite{DPT} & ViT-Large\textsuperscript{\textdagger} & $640 \times 640$ & 479.0 & 49.2& 49.5 \\
        UperNet* \cite{Upernet} & ViT-Large\textsuperscript{\textdagger}& $640 \times 640$ & >700  & 48.6& 50.0 \\
        SETR-MLA \cite{setr} & ViT-Large & $512 \times 512$ & 368.6 & 48.6&50.3\\
        MCIBI \cite{MCIBI}   & ViT-Large & $512 \times 512$ & >400 & - &50.8\\
        Segmenter \cite{strudel2021segmenter} & ViT-Large\textsuperscript{\textdagger}& $640 \times 640$ & 671.8 & 51.8 & 53.6\\
        StructToken \cite{lin2022structtoken}  & ViT-Large\textsuperscript{\textdagger} & $640 \times 640$ & >700 & 52.8 & 54.2\\
        \midrule
        \seg\ (\emph{Shrunk}, ours)  & ViT-Large\textsuperscript{\textdagger}  & $640 \times 640$& 373.5
        & 53.9 & 55.1 \\
       \seg\ (ours) & ViT-Large\textsuperscript{\textdagger}& $640 \times 640$ & 637.9 &\textbf{54.6} & \textbf{55.2}\\
        \bottomrule
    \end{tabular}

    \label{tab:ade20k}
\end{table}

\paragraph{Results on COCO-Stuff-10K.}
\cref{tab:cocostuff} shows the result on the COCO-Stuff-10K dataset. Our method achieves $50.3\%$ which is higher than the previous state-to-the-art StrucToken by $1.2\%$ with less computational cost. Our \emph{Shrunk} version achieves $49.4\%$ with $224.8 $ GFLOPs, which is similar to the computational cost of a dilated ResNet-101 backbone but with much higher performance.

\begin{table}[h]
\centering
\caption{Experiment results on the COCO-Stuff-10K \texttt{test.}\  split. Following 
  published methods, 
  we report the results with multi-scale inference (denoted by `ms'). The GFLOPs is measured at single scale inference with a crop size of $512 \times 512$. 
}
\vspace{0.5em}
\begin{tabular}{rlcc}
\toprule
Method                     & Backbone & GFLOPs   & mIoU (ms) \\
\midrule
DANet        \cite{danet}              & Dilated-ResNet-101  & 289.3 &39.7      \\
MaskFormer    \cite{maskformer}             & ResNet-101-fpn  & 81.7 &  39.8      \\
EMANet      \cite{emanet}               & Dilated-ResNet-101 & 247.4  & 39.9      \\
SpyGR        \cite{spygr}              & ResNet-101-fpn & >80 & 39.9      \\
OCRNet       \cite{ocrnet}              & HRNetV2-W48 &167.9 & 40.5      \\
GINet       \cite{wu2020ginet}               & JPU-ResNet-101 & >200 & 40.6      \\
RecoNet     \cite{reconet}               & Dilated-ResNet-101  & >200 & 41.5      \\
ISNet     \cite{jin2021isnet}                 & Dilated-ResNeSt-101 & 228.3  &42.1      \\

MCIBI   \cite{MCIBI}                   & ViT-Large  & >380     & 44.9      \\
StructToken \cite{lin2022structtoken}               & ViT-Large  & >400     & 49.1      \\
\midrule
\seg\ (\emph{Shrunk}, ours) & ViT-Large   & 224.8    &  49.4      \\
\seg\ (ours) & ViT-Large   & 383.9   & \textbf{50.3} \\
\bottomrule
\end{tabular}

\label{tab:cocostuff}
\end{table}

\paragraph{Results on PASCAL-Context.}
\cref{tab:pascal} shows the results on the PASCAL-Context dataset. We follow HRNet~\cite{hrnet} to evaluate our method and report the results  under $59$ classes (without background) and $60$ classes (with background). 
\seg\ reaches mIoU $65.3\%$ and $59.3\%$ respectively for those two metrics that outperform the state-of-the-art methods using the ViT backbones with less computational cost. 


\begin{table}[h]
\centering
\caption{Expperiment results on the PASCAL-Context \texttt{val.}\  split. Following 
  published methods, 
  we report the results with multi-scale inference (denoted by `ms'). 
   mIoU$_{59}$: mIoU averaged over $59$ classes (without background). mIoU$_{60}$: mIoU averaged over $60$ classes ($59$ classes plus background). Both metrics were used in the literature; and we report for the $60$ classes. 
  The GFLOPs is measured at single scale inference with a crop size of $480 \times 480$.  
  }
  \vspace{0.5em}
\label{tab:pascal}
\begin{tabular}{rlc cc}
\toprule
Method     & Backbone & GFLOPs  &  mIoU$_{59}$  (ms) & mIoU$_{60}$  (ms)  \\
\midrule
    RefineNet   \cite{ lin2017refinenet}& ResNet-152 & -& - & 47.3\\
    UNet++  \cite{zhou2018unet++}  & ResNet-101 & -& 47.7 &-\\
    PSPNet   \cite{pspnet}&Dilated-ResNet-101 &  157.0 & 47.8 &- \\
    Ding \textit{et al.}  \cite{ding2018context} & ResNet-101 & -&51.6&-\\
    EncNet  \cite{Encnet}   &Dilated-ResNet-101& 192.1 &52.6 &- \\
    HRNet \cite{hrnet}   & HRNetV2-W48& 82.7 & 54.0 & 48.3 \\
    NRD  \cite{nrd} & ResNet-101 & 42.9 & 54.1 & 49.0 \\
    GFFNet \cite{li2020gated} & {\color{black}{Dilated-ResNet-101}} & {\color{black}{-}}& {\color{black}{54.3}} &{\color{black}{-}}\\
    

    
   EfficientFCN \cite{liu2020efficientfcn}  & {\color{black}{ResNet-101}} & {\color{black}{52.8}}& {\color{black}{55.3}} &{\color{black}{-}}\\
    
    {\color{black}{OCRNet  \cite{ocrnet}}} & {\color{black}{HRNetV2-W48}} & {\color{black}{143.9}}& {\color{black}{56.2}} &{\color{black}{-}}\\
    
SETR-MLA  \cite{setr}  & ViT-Large  & 318.5 & -     & 55.8      \\

Segmenter \cite{strudel2021segmenter}  & ViT-Large    & 346.2  & - &59.0     \\
\midrule
\seg\ (\emph{Shrunk}, ours) & ViT-Large  & 186.9 & 63.7  & 57.4 \\
\seg\ (ours) & ViT-Large & 321.6 & \textbf{65.3} & \textbf{59.3} \\
\bottomrule
\end{tabular}
\end{table}


\subsection{Ablation Study}
In this section, we conduct the ablation study to show the effectiveness of our proposed methods. 

\paragraph{Effect of the \atm\ module.}
\cref{tab:mask_pred} shows the effect of the \atm\ module. We set the SETR-naive as the baseline, which  uses two $1\times 1$ convolutions to get per-pixel classifications directly from the last layer of the ViT-Base transformer output. 
We can see that by applying the \atm\ module and supervise with a regular cross-entropy loss, \atm\ is capable of providing $0.5\%$ of performance boost. However, it is more beneficial to decouple the classification and mask prediction process and use the mask and classification supervision separately (3.1\% increase).

\paragraph{Ablation of using different layers as input for \seg.}
\cref{tab:atm_layer} shows the performance boost that multiple layers input can provide. We can see that the performance boost of feature maps from additional lower layers is obvious (+$1.3\%$). 
We then involved more layers of features and see further performance gains.
We empirically choose to use three layers for its best performance.






    









    










\begin{table}[h]
\parbox{.49\linewidth}{
    \centering
        \caption{
        Comparison between our proposed \atm\ module with other methods. `CE loss' indicates the cross-entropy loss that is commonly used in semantic segmentation. The experiments are carried out on the ViT-Base backbone using ADE20K dataset. 
        }
        \vspace{0.5em}
    \label{tab:mask_pred}
    \begin{tabular}{rll }
    \toprule
    Decoder    & Loss & mIoU (ss) \\
    \midrule
     SETR & CE loss &   46.5\\
     \atm  &  CE loss&  47.0 (+0.5)\\
     \atm  &   $\mathcal{L}_{mask}$ loss&  \textbf{49.6 (+3.1)} \\

    \bottomrule
    \end{tabular}
}
\hfill
\parbox{.47\linewidth}{
    \centering
        \caption{Ablation results of using different layer inputs to the \seg\ structure on ADE20K dataset using ViT-Base as the backbone. Involving multi-layer features can bring obvious performance gain.}
        \vspace{.5em}
            \label{tab:atm_layer}
    \begin{tabular}{lrl}
    \toprule
        &Used layers & mIoU (ss) \\
        \midrule
        Single & {[12]} &  49.6  \\
        Cascade &{[6, 12] }&  50.9 (+1.3)\\
        Cascade &{[6, 8, 12] }& \textbf{51.3 (+1.7)} \\
        Cascade &{[3, 6, 9, 12] }& 51.2 (+1.6) \\
    \bottomrule
    \end{tabular}
}

\end{table}


\paragraph{Ablation for the ATM Decoder.}

We conduct experiments to show the effectiveness of the proposed ATM decoder

\paragraph{\seg\ on hierarchical backbones.}
Shown in \cref{tab:hierarchi},
the \seg\ structure is also able to apply to hierarchical backbones. We choose the most competitive methods Maskformer \cite{maskformer} and Mask2former \cite{cheng2021mask2former} for comparison. Results indicate that even though our method is not designed for hierarchical backbones, we can still achieve competitive performance while being efficient in terms of computational cost.

\paragraph{Ablation for the QD module.}
The motivation to use QD is to make use of the pre-train weights of the backbone. As in \cref{tab:downsample}, if we use a stride $2$ convolution with learnable parameters to down-sample the query, it will destroy the pre-train weights and dramatically decrease the performance. If the down-sampling is applied to both Q and (K, V), there will be an inevitable loss in information during the down-sampling process which is reflected in the weaker performance.
 We found that applying $2 \times 2$ nearest down-sampling on query only for the QD module is the better option.


\paragraph{Ablation of the components in \emph{Shrunk} structure.} 
Shown in \cref{tab:tpn ablation}, we studied the effect of each component (QD and QU) in the \emph{Shrunk} structure. 
The results presented matches the structures illustrated in \cref{fig:struct}. When QD is applied, the performance decreases by $2.7\%$ from the `Single' ATM head. However, by applying QU, the performance is recovered. QD learns a non-linear down-sampling by the attention mechanism between key and query. One query will attend to several keys. QU is used to preserve the resolution and at the same time provide low-level feature information. We can see that by using QD and QU 
jointly, the performance can be retained and the computational cost is reduced. 
\atm\ module can also be used as the decoder to form our \emph{Shrunk} structure to further boost  performance. 

\begin{table}[]
\parbox{.49\linewidth}{
    \centering
        \caption{The experiments use the Swin-Tiny \cite{liu2021swin} backbone and are carried out on the ADE20K dataset. The GFLOPs are measured at single scale inference with a crop size of $512 \times 512$. QD: query-based down-saumping. QU: query-based upsampling. 
        }
    \vspace{0.5em}
     \label{tab:hierarchi}
    \begin{tabular}{rcc}
    \toprule
        Method & mIoU (ss) & GFLOPs \\
        \midrule
        Maskformer \cite{maskformer} & 46.7 & 57.3\\
        Mask2former \cite{cheng2021mask2former} & \textbf{47.7} & 73.7 \\
        \seg\ (Ours) & 47.1 & \textbf{48.0}\\
        \bottomrule
    \end{tabular}
}
\hfill
\parbox{.47\linewidth}{
    \centering
        \caption{Ablation of the QD module in terms of the targets and methods to down-sample. The experiments are carried out on the ViT-Large backbone of ADE20K dataset. }
        \vspace{.5em}
            \label{tab:downsample}
    \begin{tabular}{lcc}
    \toprule
    Applied to & Methods &  mIoU (ss)  \\
    \midrule
        Q & Conv & 44.5  \\
        Q, K, V & Nearest &  52.6  \\
        Q & Nearest & \textbf{53.9} \\
    \bottomrule
    \end{tabular}
}
\end{table}


\begin{table}[h]
    \centering
    \vspace{-1em}
        \caption{Ablation results of \emph{Shrunk} version on the ADE20K dataset. The GFLOPs are measured at single scale inference with a crop size of $512 \times 512$ on ViT-Base backbone.
        }
            \label{tab:tpn ablation}
    \begin{tabular}{lccllc}
    \toprule
        Structure & QD & QU & Head & mIoU (ss) & GFLOPs \\
        \midrule
        Single &  & & SETR & 46.5 & 107.3 \\
        Single  &  & & \atm & 49.6 (+3.1) & 115.8 \\
        Naive \emph{Shrunk} & \checkmark & & \atm & 46.9 (+0.4) & 74.1 \\
        \emph{Shrunk} & \checkmark &\checkmark & \atm & \textbf{50.0 (+3.5)} & 97.1\\
    \bottomrule
    \end{tabular}



\end{table}








\section{Conclusion}
We proposed an effective structure using plain ViT transformer backbones termed \seg\ for the semantic segmentation task.
For the first time, we utilize spatial information in attention maps for semantic segmentation. To implement this idea, we proposed an Attention-to-mask (\atm) module that can derive mask predictions during the attention calculation process. We show on a number of semantic segmentation benchmarks that our method is efficient and achieves state-of-the-art performance. We also proposed a \emph{Shrunk} structure which is applied to the backbone and capable of reducing $40\%$ of the computational cost while still maintaining competitive performance.  
We believe both structures can be strong paradigms, especially for semantic segmentation using ViT backbones. 
Last but not the least, our method still has some limitations. One of the limitations is that the  large amount of GPU memory consumed by the global attention mechanism might not be supported by some devices, which might restrict the applicability of our structures. 


\textbf{Acknowledgments} 
C. Shen's participation was in part supported by a major grant from Zhejiang Provincial Government.  Y. Liu's participation was in part supported by a start-up funding of  University of Adelaide.  This research was in part supported by Meituan.

\small
\bibliographystyle{ieeetr}
\bibliography{main}






\appendix


\section{Appendix}















\paragraph{}
In this section, we show more evaluation results to demonstrate the performance of the proposed SegViT structure.

\begin{figure}[t]
\raisebox{-1\height}{\vspace{0pt}\includegraphics[width=0.6\textwidth]{figures/supp/supp.pdf}}\hfill \begin{minipage}[t]{0.35\textwidth}
\caption{
Accuracy vs.\ computational cost on the ADE20K \texttt{val.}\ split. All the performances and computational costs are measured at single scale inference.  Our proposed SegViT structure can achieve a better trade-off and the best performance among methods using ViT backbone. 
}
 \label{fig:supp}
\end{minipage}
\end{figure}

\subsection{Illustration of the accuracy vs.\ computational cost}
As shown in \cref{fig:supp}, we achieve the best performance with a better trade-off between computational cost and accuracy compared to other methods that use ViT backbone. Also, for our SegViT \emph{Shrunk} version, we dramatically reduced the computation while still retaining competitive performance. 

\subsection{More Visualization Results}

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/supp/ade.pdf}
    \caption{
    Competitive segmentation results on the ADE20K \texttt{val.} split.}
    \label{fig:ade}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/supp/coco.pdf}
    \caption{Competitive segmentation results on the COCO-stuff-10K dataset.}
    \label{fig:coco}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/supp/pc.pdf}
    \caption{Competitive segmentation results on the PASCAL-Context  dataset with $60$ classes.}
    \label{fig:pc}
\end{figure}

\paragraph{Competitive segmentation results on three datasets.}
\cref{fig:ade}, \cref{fig:coco} and \cref{fig:pc} illustrates the model performance on ADE20K, COCO-Stuff-10K and PASCAL-Context datasets respectively using single-scale inference. We can see that in various indoor and outdoor scenes, SegViT can produce satisfactory segmentation results. 








\subsection{More Ablation Study Results}
Different decoder methods have their corresponding feature merge types and loss types. We compare the difference on a plain ViT base backbone shown in Table~\ref{tab:struct compare}. For hierarchical backbones, such as Swin, since the resolution of the feature maps of each stage is reduced, to get feature maps with large resolution and rich semantic information, FPN is necessary. However, in Table~\ref{tab:struct compare}, FPN structure can not work well with plain vision transformers. 
For non-hierarchical backbones, such as ViT, the resolution is maintained and the last layer feature map contains the richest semantic information.  Thus, our proposed method that uses the tokens to merge features of different levels got better performance. By simply changing the FPN structure with out ATM based Token merge, we improve the performance from 46.7\% to 50.6\%.
For the loss type, pixel level loss indicates the regular cross-entropy loss applied to the feature map. The dot product loss indicates the loss used in \cite{detr} and \cite{maskformer}. 
Attention mask loss means the mask supervision is directly applied to the similarity map generated by the ATM during attention calculation. We can see that the loss supervised on the attention mask further improve the result with 0.6\%. 


\begin{table}[]
\centering
\setlength{\tabcolsep}{1.5pt}
\footnotesize
\caption{Ablation results of different decoder methods with their corresponding feature merge types and loss types. ViT-Base is employed as the backbone for all the variants.}
\vspace{0.5em}
\label{tab:struct compare}

\begin{tabular}{rcccccc}
\toprule
\multicolumn{1}{c}{} & \multicolumn{2}{c}{Multi-level Features} & \multicolumn{3}{c}{Loss Types} & \multicolumn{1}{c}{} \\ 
\cmidrule(r){2-3}
\cmidrule(l){4-6}
\multicolumn{1}{c}{Decoder} & FPN & \multicolumn{1}{c}{Token Merge} & Pixel level & Dot product & Attention Mask & \multicolumn{1}{c}{mIoU (ss)} \\ 
\midrule
SETR-MLA \cite{setr}& \checkmark & \multicolumn{1}{c}{} & \checkmark &  &  & 48.2 \\
Segmenter \cite{strudel2021segmenter} &  & \multicolumn{1}{c}{} & \checkmark &  &  & 49.0 \\
MaskFormer \cite{maskformer} & \checkmark & \multicolumn{1}{c}{} &  & \checkmark &  & 46.7 \\
Ours-Variant 1 &  & \multicolumn{1}{c}{} &  &  & \checkmark & 49.6 \\
Ours-Variant 2&  & \multicolumn{1}{c}{\checkmark} &  & \checkmark &  & 50.6 \\
Ours &  & \multicolumn{1}{c}{\checkmark} &  &  & \checkmark & 51.2 \\ 
\bottomrule
\end{tabular}

\end{table}



 























\end{document}