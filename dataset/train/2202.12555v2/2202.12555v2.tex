\documentclass{article}
\usepackage{spconf,amsmath,graphicx}
\usepackage{tabularx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{pifont}
\usepackage{graphicx}
\usepackage{pgfplots}
\usepackage{hyperref}
\pgfplotsset{compat=1.16}
\usepgfplotslibrary{statistics}
\usepackage{fancyhdr}



\renewcommand{\headrulewidth}{0pt}









\def\x{{\mathbf x}}
\def\L{{\cal L}}
\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}

\newcommand\netname{6DRepNet}
\title{6D Rotation representation for unconstrained head pose estimation}
\name{Thorsten Hempel, Ahmed A. Abdelrahman, Ayoub Al-Hamadi\thanks{This research was funded by the Federal Ministry of Education and Research of Germany (BMBF) project~no.~03ZZ0448L, no.~13N16336, by the German Research Foundation (DFG) project AL 638/15, and the OVGU innovation fund.}}

\address{Faculty of Electrical Engineering and Information Technology, Neuro-Information Technology\\
Otto von Guericke University, Magdeburg, Germany}
\begin{document}
\maketitle
\begin{abstract}
In this paper, we present a method for unconstrained end-to-end head pose estimation. We address the problem of ambiguous rotation labels by introducing the rotation matrix formalism for our ground truth data and propose a continuous 6D rotation matrix representation for efficient and robust direct regression. This way, our method can learn the full rotation appearance which exceeds the capabilities of previous approaches that restrict the pose prediction to a narrow-angle for satisfactory results. In addition, we propose a geodesic distance-based loss to penalize our network with respect to the \textit{SO}(3) manifold geometry. Experiments on the public AFLW2000 and BIWI datasets demonstrate that our proposed method significantly outperforms other state-of-the-art methods by up to 20\%.
We open-source our training and testing code along with our trained models: \url{https://github.com/thohemp/6DRepNet}.
\end{abstract}
\begin{keywords}
head pose estimation, orientation regression, rotation matrix, geodesic loss
\end{keywords}
\section{Introduction}
\label{sec:intro}
Head pose estimation from a single image is a key task in facial analysis that is used for a wide range of applications such as driver assistance~\cite{4357803}, augmented reality~\cite{5443483}, and human-robot interaction~\cite{app11125366}. Current methods are commonly divided into landmark-based and landmark-free approaches. Landmark-based methods~\cite{8297015} detect facial landmarks in an initial step and, subsequently, recover the 3D head pose by establishing correspondence between these landmarks and a 3D head model. While this approach can lead to very accurate results, it is highly dependent on the correct prediction of the landmark positions. Hence, inferior landmark localization caused by occlusion and extreme rotation can consequently impair an accurate head pose estimation. 

Landmark-free approaches overcome this problem by directly estimating the head pose. These methods commonly facilitate deep neural networks to formulate the orientation prediction as an appearance-based task. HopeNet~\cite{Ruiz2018FineGrainedHP} presented a multi-loss approach by binning the target angle range to combine a cross-entropy and a mean squared error loss function for Euler angle prediction. Similarly, QuatNet~\cite{8444061} adapts the cross-entropy paradigm but splits classification and regression into separate network branches. One branch is used for classifying Euler angles and the second one for regressing the pose in quaternion representation. Similarly, HPE~\cite{Huang2020ImprovingHP} treats classification and regression separately and then averages the outputs as a pose regression subtask. WHENet~\cite{Zhou2020WHENetRF} keeps the single branch strategy but increases the number of bins to extend the predictable yaw range together with an EfficientNet backbone. Whereas, FSA-Net~\cite{Yang_2019_CVPR} proposes a network with a stage-wise regression and feature aggregation scheme for predicting Euler angles. TriNet~\cite{Cao_2021_WACV} adapts this method, but estimates the three unit vectors of the rotation matrix instead of Euler angles and incorporates an additional orthogonality loss to stabilize the predictions. In another approach, FDN~\cite{Zhang2020FDNFD} proposes a feature decoupling method to explicitly learn discriminative features of different head orientations.

It has become a common convention to split up the continuous rotation variables into bins for classification to stabilize the predictions~\cite{Ruiz2018FineGrainedHP,8444061,Huang2020ImprovingHP,Zhou2020WHENetRF,Zhang2020FDNFD}. However, this is problematic as pruning segments of angles into bins will consequently lead to a loss of information. Additionally, most of the current methods use the Euler angle or quaternion representation to train their networks. However, Zhou et al.~\cite{Zhou2019OnTC} demonstrated that any representation of rotation with four or fewer dimensions is discontinuous and therefore not ideal to use in a learning task for neural networks. 
\\
\\
\textbf{Contributions:} We propose a landmark-free head pose estimation method that uses the rotation matrix representation for regressing accurate head orientations. The nine-parameter matrix enables full pose regression without suffering ambiguity problems. Additionally, it allows simplifying the network by detaching unnecessary performance stabilizing measures used by other methods, \textit{e.g.} discretization of the rotation variables into a classification problem. This simplicity makes our network easily to be transferred into other rotation-related problems. Instead of predicting the entire nine-parameter rotation matrix, we efficiently regress a compressed 6D form that is transformed into the rotation matrix in a subsequent task. 

Further, we propose to use the geodesic loss instead of the commonly used mean squared error loss. This way, we can use the distance angle to penalize our network in the training process that encapsulates the Special Orthogonal Group \textit{SO}(3) manifold geometry. Fig \ref{fig:method} shows an overview of your proposed method. Each component will be explained in detail in the following sections. Inspired by the 6D representation that is used in our approach, we call our network \netname.

Our training code, testing code, and trained CNN models are made publicly available to facilitate research experimentation and practical application development.


\section{Method}
A key aspect for tackling direct orientation prediction is the use of an appropriate rotation representation. A popular and convenient representation is the Euler angle. However, this representation is not optimal as it suffers from the \textit{gimbal lock} in which case there are infinitive rotation parameterizations for the same visual head pose appearance. As a consequence, neural networks have difficulties to learn the accurate pose. Whereas, on the contrary, the quaternion representation doesn't suffer from the gimbal lock, it still has an ambiguity caused by its antipodal symmetry. Especially when learning the full range of head orientations, this can lead to decreased estimation performance. A more favorable rotation representation is the rotation matrix, which is a continuous representation with a distinct parameterization for each rotation. In \textit{SO}(3) the matrix representation  is sized  with orthogonality constraint , where  is the transposed matrix and  the identity matrix. One could now try to regress the rotation matrix directly, but this would require finding all nine parameters and enforcing the orthogonality constraint, either via the Gram-Schmidt process or by finding the nearest optimal solution using SVD. Instead, we follow the approach by Zhou et al.~\cite{Zhou2019OnTC} and perform the Gram-Schmidt mapping inside the representation itself by simply dropping the last column vector of the rotation matrix. This reduces the  matrix into a six parameter rotation representation that has been reported to introduce smaller errors for direct regression~\cite{Zhou2019OnTC}. 

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{method.png}
    \caption{Overview of the proposed method.}
    \label{fig:method}
\end{figure}




The predicted 6D representation matrix can then mapped back into \textit{SO}(3).



Hereby, the remaining column vector is simply determined by the cross product that ensures that the orthogonality constraint is satisfied for the resulting  matrix.


As a result, our network only has to predict 6 parameters that are mapped into a  rotation matrix in a subsequent transformation which at the same time also satisfies the orthogonality constraint.

A commonly used loss function for head pose related tasks is the \textit{l2}-norm. However, using the Frobenius norm for measuring distances between two matrices would break with the \textit{SO}(3) manifold geometry. Instead, the shortest path between two 3D rotations is geometrically interpreted as the geodesic distance. Let  and   \textit{SO}(3) be the estimated and the ground truth rotation matrices, respectively, then the geodesic distance between both rotation matrices is defined as:



In the following, we will use this metric as the loss function for our neural network to compute accurate distance information between the predicted and the ground truth orientation.

\begin{table*}[t]
\centering
\begin{tabular} { l c@{\hskip .3in} c c c c @{\hskip .5in} c c c c }
\hline
& & \multicolumn{4}{c}{\textbf{AFLW2000}} {\hskip .5in} & \multicolumn{4}{c}{\textbf{BIWI}}\\
\hline
  & Full Range& Yaw & Pitch & Roll & MAE & Yaw & Pitch & Roll & MAE \\
  \hline
 HopeNet () ~\cite{Ruiz2018FineGrainedHP}& \xmark  & 6.47 & 6.56 & 5.44 & 6.16  & 5.17& 6.98& 3.39 & 5.18\\ HopeNet () ~\cite{Ruiz2018FineGrainedHP}& \xmark & 6.92 & 6.64 & 5.67 & 6.41 & 4.81& 6.61& 3.27 & 4.90\\ FSA-Net~\cite{Yang_2019_CVPR}& \xmark  & 4.50 & 6.08 & 4.64 & 5.07 & 4.27 & 4.96 & 2.76 & 4.00\\ 
 HPE~\cite{Huang2020ImprovingHP} & \xmark & 4.80 & 6.18 & 4.87 & 5.28 & 3.12 & 5.18 & 4.57 & 4.29 \\
 QuatNet~\cite{8444061}& \xmark  & 3.97 & 5.62  & 3.92 & 4.50 & \textbf{2.94} & 5.49 & 4.01 & 4.15  \\
 WHENet-V~\cite{Zhou2020WHENetRF}& \xmark &4.44& 5.75& 4.31& 4.83 & 3.60 & \textbf{4.10} & 2.73 & 3.48\\
 WHENet~\cite{Zhou2020WHENetRF} & \cmark \xmark & 5.11 & 6.24 & 4.92 & 5.42 & 3.99 & 4.39 & 3.06 & 3.81 \\
 TriNet~\cite{Cao_2021_WACV}& \cmark  & 4.04 & 5.77 & 4.20 & 4.67 & 4.11 & 4.76 & 3.05 & 3.97 \\
 FDN~\cite{Zhang2020FDNFD} & \xmark & 3.78 & 5.61 & 3.88 & 4.42 & 4.52 & 4.70 & \textbf{2.56} & 3.93\\
 \hline\-2ex] 
\netname & \textbf{2.69} & \textbf{2.92} & \textbf{2.36} & \textbf{2.66}\\
\hline
\end{tabularx}
\caption{Comparisons with the state-of-the-art methods on the
BIWI dataset. 70\% of the BIWI dataset is used for training and the remaining 30\% for testing.}
\label{table2}
\end{table}
\begin{table}[t]
\begin{tabularx}{\linewidth} 
{    l  @{\hskip .5in}  c   c  c  }
\hline

& \textbf{AFLW2000} &\textbf{BIWI} & \textbf{70/30 BIWI}\\
\hline
  & MAE  & MAE & MAE \\
  \hline
 &  4.13 & 3.70 & 2.84 \\
Geodesic & 3.97 & 3.47 & 2.66\\

\hline
\end{tabularx}
\caption{Comparison of the MAE between  and geodesic loss.}
\label{table3}
\end{table}
\\
\textbf{Experiment 3:} Most current methods use the -norm for calculating the loss in the training procedure. We argue that the Geodesic distance is a better distance metric to measure the prediction accuracy for head pose orientation. To prove this, we conduct another experiment where we repeat our previous tests, but this time train our network with the   distance loss. Table \ref{table3} shows these results compared to our models trained with geodesic distance loss. It states that the networks with geodesic loss penalty performed slightly better than those that were trained with  -norm. 
\\
\\
\textbf{Experiment 4:} In a final experiment, we analyze the impact of the chosen backbone on the results. ResNet50 is a popular standard network that is also used by HopeNet and TriNet, so we use it as the comparison backbone. Table \ref{table4} shows that our method with RepVGG backbone is capable to perform about 7\% better on all test scenarios against the  loss. However, with the aid of the ResNet50 backbone, our method would still achieve state-of-the-art results on the AFLW2000 dataset.

\begin{table}[]
\begin{tabularx}{\linewidth} 
{    l  @{\hskip .2in}  c   c  c  }
\hline
& \textbf{AFLW2000} &\textbf{BIWI} & \textbf{70/30 BIWI}\\
\hline
& MAE  & MAE & MAE \\
  \hline
ResNet50 & 4.26 & 3.76 & 3.09\\
RepVGG-B1g2 &  3.97 & 3.47 & 2.66 \\


\hline
\end{tabularx}
\caption{Comparison of the MAE between the different backbones.}
\label{table4}
\end{table}

\section{Conclusion}
In this paper, we presented a method for unconstrained end-to-end head pose estimation from single images. We follow the argument that the rotation matrix is more suitable for orientation learning tasks and propose a continuous 6D rotation matrix representation for efficient direct regression. Additionally, we introduce the geodesic loss instead of the commonly used MSE for the robust training.
Also different from previous approaches, our method can regress full rotations and does not utilize an angle restricting binning principle. Nonetheless, our method outperforms other state-of-the-art methods on multiple datasets by up to 20\%. In additional experiments, we analyze the impact of the backbone and loss function on our results.
In the future, we aim to draw on our method's full potential by training it on a dataset that provides full rotation samples.
To this end, a potential dataset has been introduced by WHENet~\cite{Zhou2020WHENetRF} that presented a method for recovering full head pose annotations for the CMU Panoptic dataset~\cite{jooiccv2015}.












\bibliographystyle{IEEEbib}
\bibliography{bib}

\end{document}
