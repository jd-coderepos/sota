\section{Experiments}

\subsection{Datasets}
\textbf{CUHK-PEDES}~\cite{li2017person} is made up of 40,206 images and 80,412 language descriptions for 13,003 identities where each image include two sentences. The training set consists of 34,054 images and 68,108 language descriptions for 11,003 identities. Both validation set and test set have 1,000 identities where 3,078 images are in the validation set and 3,074 images are in the test set. The language description comprises 23 words on average.

\textbf{ICFG-PEDES}~\cite{ssan} is constructed from the MSMT17~\cite{MSMT17} database which is made up of 54,522 images and 54,522 language descriptions for 4,102 identities where each image include only one sentences.
The training set consists of 34,674 images for 3,102 identities. The test set has 1,000 identities with 19,848 images.
The language description comprises 37 words on average.

\renewcommand{\multirowsetup}{\centering}
\begin{table*}[t]
\renewcommand\arraystretch{1.06}
    \caption{ \small{The component analysis of \ourmodel. Local means adding  convolution layer to extract local textual feature. SGTL and VGKT are our proposed Semantic-Group Textual Learning Module and Vision-guided Knowledge Transfer Module, respectively.}}\label{tab:ablation}
    \vspace{-2mm}
\footnotesize
    \begin{center}
    \setlength{\tabcolsep}{3.96mm}{\begin{tabular}{lccc|ccc|ccc}
    \hline
     & & & &\multicolumn{3}{c|}{CUHK-PEDES} & \multicolumn{3}{c}{ICFG-PEDES} \\
    Index   & Local & SGTL & VGKT   & Rank-1    & Rank-5 & Rank-10    & Rank-1 & Rank-5 & Rank-10  \\
    \hline
    \hline
    1  &\xmarkg &  \xmarkg&  \xmarkg     &  65.26  &  82.81  &   88.84  & 56.69 &72.96&79.45\\
    2& \cmark & \xmarkg &      \xmarkg      & 65.58 & 83.05  &  89.17 & 56.90&72.58&  78.91 \\
    3& \xmarkg& \cmark     &\xmarkg  & 66.48  & 83.64  &  89.52 & 58.85& 74.41& 80.40 \\
4& \cmark &  \xmarkg    & \cmark     &  66.76  & 83.56  &  89.32 &   59.13        &     74.66      & 80.73\\
    5~(\textbf{\ourmodel})& \xmarkg &  \cmark    & \cmark     &  67.52  & 84.37  &  90.26 &   60.34        &     76.01      & 82.01\\
    \hline
    \end{tabular}}
\end{center}


\end{table*}
\subsection{Implementation}

\textbf{Experimental Details:}
We utilize the image and text encoder with CLIP~\cite{clip}, and choose ResNet-50\cite{he2016resnet} as the image encoder for all the experiments. CLIP-ViT visual backbone is applied for fair SOTA performance comparison.
The maximum length of input sentences is set to 77.
All person images are resized to  unless otherwise specified. 
Following conventions in the ReID community, the stride of the last block in the ResNet is set to 1 to increase the resolution of the final feature map.
The training images are augmented with random horizontal flipping, padding, random cropping, and random erasing \cite{random_erase3}. 
The batch size is set to 96 with 4 images and sentences pairs per identity. 
We train the network for 50 epochs employing the Adam optimizer with the initial learning rate of 0.00035, which is decayed by a factor of 0.1 after the 40th epoch. 
Temperature  and the transformer layer  are set to 4 and 2, respectively. 
All the experiments are performed with one RTX TITAN using the PyTorch toolbox.
\footnote{http://pytorch.org}. 

\textbf{Evaluation Protocols.} Following previous work~\cite{textreid}, we evaluate all methods with Rank-K(K=1,5,10) and some with the mean Average Precision (mAP). 
Rank-K indicates the percentage that, given a text/image as query, more than one correct image/text are retrieved among the top-k candidate list.
mAP is adopted for a comprehensive evaluation that focuses more on the order of the total retrieval results.



\subsection{Ablation Study}

\paragraph{Component Analysis}


We perform detailed component ablation studies to evaluate the effectiveness of our proposed method in \tablename~\ref{tab:ablation}. The experiments are performed on both CUHK-PEDES and ICFG-PEDES. 
We adopt the method clarified in Sec.~\ref{sec:strong-baseline} as our baseline with global feature only (index 1). 
First of all, when adding the proposed SGTL into our baseline (index 3), a performance gain of 1.22\% and 2.16\% in terms of Rank-1 is observed over the baseline on CUHK-PEDES and ICFG-PEDES, respectively. The performance gain is owning to the benefit of our effective local textual learning design.
The superior result demonstrates that  similar semantic patterns are grouped
together to represent semantic-group local textual features, which is important in text-based person search requiring fine-grained information.
On the other hand, to verify the superiority of our method, we use Local which adds  convolution layer to extract local textual features instead of SGTL (index 2). A performance gain is small with 0.32\% and 0.21\% in terms of Rank-1,
which proves that the gain of SGTL is not from increasing the number of parameters or introducing additional local visual features.


Then, by introducing VGKT as auxiliary supervision (index 5), we further get a significant improvement over the SGTL, \eg, 1.04\% and 1.49\% performance gain in terms of Rank-1 on CUHK-PEDES and ICFG-PEDES, respectively. 
The proposed VGKT forces the knowledge to adapt from vision-guided textual features to semantic-group textual features. With the help of VGKT,
SGTL generation is no longer only depending on itself, but taking its paired visual features into account through knowledge distillation loss in VGKT. As such, semantic-group textual features are capable of aligning local visual features.
To further illustrate its effectiveness, We add VGKT built upon Local (index 4).
There is also a significant performance gain beyond Local, which verifies that our VGKT is a versatile module.








\paragraph{Ablation Study of Baseline Configuration}
We study the effects of various designs of the baseline configuration. 
The results of different variations of the training settings are listed in \tablename~\ref{tab:ablation-baseline}. 
The results are improved consistently with the help of opening text encoder, adding drop path, and building shared layer with visual and textual embedding.
Because text-based person search is a fine-grained task that tackles in the same category. If we freeze the text encoder like other methods~\cite{textreid}, the differences in text embedding are very limited. Therefore, we finetune the text encoder in the TBPS dataset with a small learning rate of 3e-6 to capture fine-grained information. When freezing the text encoder, the performance decreases by 14.39\% Rank-1 and 10.2\% Rank-1 on CUHK-PEDES and ICFG-PEDES, respectively.
Introducing stochastic depth~\cite{stoc_depth} can boost the Rank-1 performance by about 2.61\% and 2.41\% on CUHK-PEDES and ICFG-PEDES, respectively. It is because the transformer design in text encoder has no regularization components, it is easily overfitting in downstream tasks without large datasets.
Constructing share layer between visual and textual modality can reduce the modality gap~\cite{zheng2020dual,ssan}, which provides 0.25\% Rank-1 improvement on ICFG-PEDES compared with the not share layer.

\begin{table}[t]
\renewcommand\arraystretch{1.06}
	\centering
		\caption{\small{Effects of different baseline network configuration on CUHK-PEDES and ICFG-PEDES datasets. The abbreviations O, D, S denote opening text encoder, stochastic depth~\cite{stoc_depth} and share layer between visual and textual modality, respectively.}}
    \setlength{\tabcolsep}{3.36mm}{\begin{tabular}{ccc|cc|cc}
    \hline
    			 &  &  & \multicolumn{2}{c|}{CUHK-PEDES} & \multicolumn{2}{c}{ICFG-PEDES} \\  
			O & D & S & Rank-1 & Rank-5 &Rank-1  & Rank-5 \\ \hline  \hline 
		    \xmarkg  & \xmarkg  & \xmarkg &   48.26 &  70.31  &   39.04 &   57.82  \\
			\cmark  & \xmarkg  & \xmarkg  &  62.65  &  80.40  &   53.93 &   70.83  \\
			\cmark  & \cmark  & \xmarkg   &  65.06  &  81.43  &   56.34 &   72.12  \\
			\cmark  & \cmark  & \cmark    &   65.26  &  82.81  &   56.59 &   72.96\\
	\hline
	\end{tabular}}

	\label{tab:ablation-baseline}
\end{table}
\subsection{Ablation Study of SGTL}

\paragraph{Evaluation of the Number of }
We conduct several experiments to investigate the influence of the different number of local visual and textual features in~\figurename~\ref{fig:hyper-parameter} (a). As can be seen, with  increase, the performance first improves and then becomes flat. It strikes the peak when  arrives 4, the trend is similar to the past methods like PCB~\cite{sun2018beyond} and TransReID~\cite{transreid}. While overly large region numbers consume more computing resources and slow down the inference speed. Thus, we choose 4 to achieve a good balance between accuracy and efficiency. 

\begin{table}[t]
\renewcommand\arraystretch{1.06}
	\centering
	\caption{\small{Comparison with other designs for SGTL on CUHK-PEDES and ICFG-PEDES datasets. TQ and CG denote text query and channel group, respectively.}}
    \setlength{\tabcolsep}{2.76mm}{\begin{tabular}{ccc|cc|cc}
    \hline
    	\multirow{2}{*}{Index}& 		\multirow{2}{*}{TQ}& \multirow{2}{*}{CG} & \multicolumn{2}{c|}{CUHK-PEDES} & \multicolumn{2}{c}{ICFG-PEDES} \\  
		&	  &  & Rank-1 & Rank-5 &Rank-1  & Rank-5 \\ \hline  \hline 
		1 &    \xmarkg  & \xmarkg &  65.73 &  83.13  &   57.15  &   72.96  \\
		2&	 \cmark  & \xmarkg  &  66.01  &  83.21  &   57.71  &   73.43  \\
		3&	 \xmarkg  & \cmark   &  66.29  & 83.26  & 57.66   &  73.72   \\
		4&	 \cmark  & \cmark    &   66.48 & 83.64  &   58.85  &  74.41\\
	\hline
	\end{tabular}}
	\label{tab:SGTL-variations}
\end{table}
\paragraph{Comparison with Other Designs for SGTL}

To verify the superiority of our proposed SGTL, we conduct experiments on other variations for SGTL in \tablename~\ref{tab:SGTL-variations}.
For a fair comparison, all the methods are performed with the same experimental configuration apart from the local textual learning design.
on the one hand,
We use DETR~\cite{detr} decoder branch as variants that contains self-and cross-attention block design without considering channel group operation. We set the text query as \textit{querys}, the whole textual features  as the \textit{keys} and \textit{values}. Through transformer learning, text queries aim to attend to different local discriminative cues from the entire textual features.
As the experimental results shown in \tablename~\ref{tab:SGTL-variations} (index 2), vanilla DETR transformer performs inferior to our proposed SGTL (index 4), where performance drops of -0.47\% on CUHK-PEDES and -1.14\% on ICFG-PEDES in terms of Rank-1 accuracy are observed, respectively. This confirms that simply applying Transformers for the text-based person search is not effective, because they neglect that feature channels
of textual feature can be grouped according to semantic
distribution and corresponds to a certain type of visual pattern.
On the other hand, text query plays an important role in SGTL learning. We conduct experiments to replace text query which add conditional global textual features information with word query. The results show that simply using text query instead of word query has attained improvement by 0.28 \% on CUHK-PEDES and 0.56\% on ICFG-PEDES without CG and 0.19 \% on CUHK-PEDES and 1.19\% on ICFG-PEDES based on CG. Therefore, text query is important and essential for SGTL, especially for difficult datasets like ICFG-PEDES.




\subsection{Ablation Study of VGKT}

\paragraph{Comparison with Other Designs for VGKT}
\begin{table}[t]
\renewcommand\arraystretch{1.06}
	\centering
		\caption{\small{Comparison with other designs for VGKT on CUHK-PEDES and ICFG-PEDES datasets. FT, ST and CPT denote feature transfer, similarity transfer and class probability transfer, respectively.}}
    \begin{tabular}{cccc|cc|cc}
    \hline
    \multirow{2}{*}{Index}&	\multirow{2}{*}{FT}& 		\multirow{2}{*}{ST}& \multirow{2}{*}{CPT} & \multicolumn{2}{c|}{CUHK-PEDES} & \multicolumn{2}{c}{ICFG-PEDES} \\  
&		&	  &  & Rank-1 & Rank-5 &Rank-1  & Rank-5 \\ \hline  \hline 
1&		\xmarkg &    \xmarkg  & \xmarkg &  66.48& 83.64  & 58.85 &74.41  \\
2&		\cmark &    \xmarkg  & \xmarkg &    66.19  & 83.21 & 58.55  & 73.89   \\
3&		\xmarkg&	 \cmark  & \xmarkg  &   66.95 & 83.98  &  59.89   &  75.52   \\
4&		\xmarkg&	 \xmarkg  & \cmark   &  66.79  &  83.99 & 59.01   &  75.13   \\
5&		\xmarkg&	 \cmark  & \cmark    &   67.52 & 84.37  &   60.34   &  76.01\\
	\hline
	\end{tabular}

	\label{tab:VGKT-variations}
\end{table}

\begin{table*}[t]
\begin{center}
\renewcommand\arraystretch{1.06}
\footnotesize
\caption{Comparisons with state-of-the-art methods on the CUHK-PEDES. Best results are labeled in \textbf{bold}.}
\setlength{\tabcolsep}{2.9mm}
\begin{tabular}{r|c|c|c|c|c|c|c|c|c|c}
\hline
\multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Reference}} & \multirow{2}{*}{\textbf{Arch.}} & \multicolumn{4}{c|}{\textbf{Text to Image}}             & \multicolumn{4}{c}{\textbf{Image to Text}}             \\ \cline{4-11} 
                                 &        &                          & \textbf{Rank-1} & \textbf{Rank-5} & \textbf{Rank-10} & \textbf{mAP} & \textbf{Rank-1} & \textbf{Rank-5} & \textbf{Rank-10} & \textbf{mAP} \\ \hline \hline

GNA-RNN \cite{li2017person}     &CVPR’17                     & VGG16                          & 19.05           & -               & 53.64            & -            & -               & -               & -                & -            \\ 


Dual Path \cite{zheng2020dual}    &TOMM’20                     & ResNet50                           & 44.40           & 66.26           & 75.07            & -            & -               & -               & -                & -            \\ 


CMPM/C \cite{zhang2018cmpc} &ECCV’2018             & MobileNet                              & 49.37           & 71.69           & 79.27            & -        & 60.96           & 84.42           & 90.83            & -        \\ 
MIA \cite{niu2020mia}       &TIP’20                       & ResNet50                                & 53.10           & 75.00           & 82.90            & -            & -               & -               & -                & -            \\ 
PMA \cite{jing2020pose}    &AAAI’20                            & ResNet50                              & 54.12           & 75.45           & 82.97            & -            & -               & -               & -                & -            \\ 


TIMAM \cite{sarafianos2019adversarial}   &ICCV’19                         & ResNet101                               & 54.51           & 77.56           & 84.78            & -        & 67.40           & 88.65           & 93.91            & -        \\ 


CMKA \cite{chen2021cmka}          &TIP’21                   & ResNet50                                & 54.69           & 73.65           & 81.86            & -            & -               & -               & -                & -            \\ 
ViTAA \cite{wang2020vitaa}        &ECCV’20                   & ResNet50                               & 54.92           & 75.18           & 82.90            & 51.60        & 65.71           & 88.68           & 93.75            & 45.75        \\ 
CMAAM \cite{aggarwal2020cmaam}     &WACV’20                        & ResNet50                               & 56.68           & 77.18           & 84.86            & -            & -               & -               & -                & -            \\ 
HGAN \cite{zheng2020hierarchical}  &MM’20                         & ResNet50                                & 59.00           & 79.49           & 86.62            & -        & 71.16      & 90.05           & 95.06            & -        \\ 
NAFS (G) \cite{gao2021contextual}    &arXiv’21                           & ResNet50                            & 59.36           & 79.13          & 86.00            & 54.07            & 71.89               & 90.99               & 95.28                & 50.16            \\ 
MGEL \cite{wang2021mgel}       &IJCAI’21                       & ResNet50                          & 60.27           & 80.01          & 86.74            & -            & 71.87               & 91.38               & 95.42                & -            \\ 
AXM-Net \cite{farooq2021axm}     &AAAI’22                     & ResNet50                              & 61.90           & 79.41           & 85.75            & 57.38       & -               & -               & -                & -            \\ 
TIPCB \cite{chen2021tipcb}    &Neuro’22                      & ResNet50                            &63.63           & 82.82           & 89.01            & 56.78       & 73.55               & 92.26               & 96.03                & 51.78            \\ 
SSAN \cite{ssan}          &arXiv’21                 & ResNet50                                    & 61.37           & 80.15           & 86.73            & -       & -         & -          & -          & -      \\ 
TextReID~\cite{textreid}    &BMVC’21                          & ResNet50                                     & 61.65           & 80.98           & 86.78            & 58.29        & 75.96          & 93.40           & 96.55            & 55.05        \\ 
LapsCore~\cite{lapscore}     &ICCV’21               &      ResNet50   &                                   63.40&    -        & 87.80          &   -  &     -      &      -   &        -    &  -
\\ 
ACSA~\cite{ACSA}          &TMM’22                   &    Swin-Tiny                               &   63.56& 81.40& 87.70   &   -  &       -    &     -    &       -     & -
\\ 
ISANet~\cite{ISANet}          &arXiv’22                   &    ResNet50                               &   63.92& 82.15&         87.69    &   -  &       -    &     -    &       -     & -
\\ 
SRCF~\cite{SRCF}          &ECCV’22                   &    ResNet50                               &   64.04& 82.99& 88.81    &   -  &       -    &     -    &       -     & -
\\ 
LBUL~\cite{LBUL}          &MM’22                   &    ResNet50                              &   64.04 &82.66 &87.22    &   -  &       -    &     -    &       -     & -
\\ 


LGUR~\cite{lgur}          &MM’22                   &    ResNet50                               &   64.21         &            81.94&         87.93    &   -  &       -    &     -    &       -     & -
\\ 

CAIBC~\cite{CAIBC}          &MM’22                   &    ResNet50                              &   64.43 &82.87& 88.37    &   -  &       -    &     -    &       -     & -
\\ 
CA~\cite{C2A2}          &MM’22                   &    ResNet50                               &   64.82& 83.54 &89.77    &   -  &       -    &     -    &       -     & -
\\ 
IVT~\cite{IVT}          &ECCVW’22                   &    ViT-Base                               &   65.59         &            83.11&         89.21    &   -  &       -    &     -    &       -     & -
\\ 
CFine~\cite{CFine}          &arXiv’22                   &    ViT-Base                               &   69.57 & 85.93  &91.15   &   -  &       -    &     -    &       -     & -
\\ 



\hline


\textbf{\ourmodel}~(Ours)       &                      &    ResNet50                               &         67.52   &      84.37      &   90.26        &  63.67   &         82.49  &     95.64   &     97.78       & 61.42 \\
\textbf{\ourmodel}~(Ours)       &                      &    ViT-Base                               &         \textbf{71.38}& \textbf{86.75}  &\textbf{91.86}  & \textbf{67.91}  & \textbf{84.92}  &  \textbf{96.35} & \textbf{98.24} &\textbf{63.83}
\\ \hline
\end{tabular}
\label{tab:cuhk}
\end{center}
\end{table*}

\begin{figure*}[t]
	\centering
	\includegraphics[width = 0.9\textwidth]{Figs/hyper-parameter.pdf}
	\caption{\small{Effects of three hyper-parameters, group number ,  weight , and  weight  on CUHK-PEDES and ICFG-PEDES datasets in terms of Rank-1 accuracy. 
}}
	\label{fig:hyper-parameter}
\end{figure*}

We further evaluate our model with our proposed VGKT. As shown in \tablename~\ref{tab:VGKT-variations}, when we take feature transfer to constrain numerical value between feature  and , the performance drops by -0.29\% on CUHK-PEDES and -0.3\% on ICFG-PEDES in terms of Rank-1 accuracy, respectively (index 2). This confirms that simply narrowing the distance between two features like 
conventional knowledge distillation is useless and might lead to discriminative information loss. 
Compared with this straightforward approach, we use the dark knowledge of the relationship between samples to have better generalization ability.
By introducing similarity transfer, Rank-1 accuracy is significantly improved by 0.47\% on CUHK-PEDES and 1.04\% on ICFG-PEDES. This suggests that pairwise similarity between visual features and textual features can act as a strong cue for knowledge transfer because the similarity score between cross-modal features is critical for the final performance (index 3).
Then, we evaluate class probability transfer. It can bring in 0.31\% and 0.16 improvement in terms of Rank-1 accuracy on CUHK-PEDES and ICFG-PEDES, respectively (index 4). 
Finally, we apply for both similarity transfer and class probability transfer for the SGTL generation, and the performance has been further improved (index 5).
Therefore, similarity transfer and class probability transfer are complementary to each other, which is also consistent with the loss function including similarity-based contrastive loss and class probability-based instance loss.

\paragraph{Evaluation of the Hyper-parameter  and }
We evaluate the inference of the hyper-parameter  and  in the final loss function Eq.\ref{eq:loss_function}. As shown in \figurename~\ref{fig:hyper-parameter} (b-c), our hyper-parameter are relatively stable and there is no big fluctuation. We set  to 4.0 and  to 0.25. 






\subsection{Comparison with State-of-the-Art Methods}

\begin{figure*}[t]
	\centering
	\includegraphics[width = 0.9\textwidth]{Figs/tsne.pdf}
	\caption{\small{t-SNE visualization on different kinds of the local visual features and local textual feature distribution. The circle denotes the local textual feature. The cross denotes the local visual feature. Different colors represent different classes. 
}}\label{fig:tsne}
\end{figure*}
\paragraph{Performance Comparison on CUHK-PEDES}
We compare \ourmodel~with state-of-the-art methods on CUHK-PEDES with text-to-image and image-to-text settings in \tablename~\ref{tab:cuhk} to show our superiority.
Our \ourmodel~outperforms the previous SOTA methods by a large margin with 2.70\%/1.81\% Rank-1 and 6.53\%/8.96\% Rank-1 for text-image and image-text using ResNet50/ViT, respectively.
Specifically, \ourmodel~(ResNet50) outperform TextReID~\cite{textreid} by 5.87\% Rank-1 and 6.53\% Rank-1 for text-image and image-text, respectively. It is worth noting that TextReID and \ourmodel~both utilize CLIP to extract visual and textual features. While TextReID does not make full use of CLIP and add redundant GRU to extract textual feature which leads to a performance gap. Furthermore, we apply the local branch to mine fine-grained information which is critical for retrieval tasks and neglected by TextReID. 

\begin{table}[t]
\renewcommand\arraystretch{1.06}
\centering
\footnotesize
\caption{\small{Comparisons with state-of-the-art methods on the ICFG-PEDES. Best results are labeled in \textbf{bold}.}}
\setlength{\tabcolsep}{1.5mm}
\begin{tabular}{r|c|c|ccc}
\hline
Method  &  Reference & Arch.  & Rank-1 &Rank-5 & Rank-10 \\ \hline \hline
Dual Path \cite{zheng2020dual}     &  TOMM’20 &ResNet50   & 38.99 & 59.44 &68.41      \\ 
CMPM/C \cite{zhang2018cmpc} & ECCV’18    &MobileNet       & 43.51 & 65.44 & 74.26               \\ 
MIA \cite{niu2020mia}   &    TIP’20   &ResNet50        & 46.49 & 67.14 & 75.18                   \\ 
SCAN~\cite{lee2018stacked}   &  ECCV’18  &ResNet50          & 50.05 & 69.65 & 77.21       \\ 
ViTAA \cite{wang2020vitaa}& ECCV’20  &ResNet50    &  50.98 & 68.79 & 75.78     \\ 
SSAN~\cite{ssan}  &  arXiv’21   &ResNet50     & 54.23 & 72.63 & 79.53          \\ 

IVT~\cite{IVT}     &ECCVW’22   &ViT-Base             &   56.04 & 73.60&  80.22 \\ 
SRCF~\cite{SRCF}          &ECCV’22 &ResNet50  &57.18 &75.01 &81.49
\\ 
LGUR~\cite{lgur}   &      MM’22     &ResNet50         &   57.42   &      74.97 &         81.45 \\ 


ISANet~\cite{ISANet}           &arXiv’22 &ResNet50 &57.73 &75.42&81.72\\
CFine~\cite{CFine}          &arXiv’22 &ViT-Base  &60.83 &76.55&82.42\\
\hline
\textbf{\ourmodel}~(Ours)      &                  &  ResNet50 & 60.34       &     76.01     & 82.01 \\
\textbf{\ourmodel}~(Ours)      &                  &  ViT-Base&  \textbf{63.05}        &     \textbf{78.43}       & \textbf{84.36} \\
\hline
\end{tabular}
\label{tab:icfg}
\end{table}




\begin{table}[t]
\renewcommand\arraystretch{1.06}
\begin{center}
\caption{\small{Comparison with state-of-the-art methods on Domain Generalization task. Best results are labeled in \textbf{bold}.\label{tab:dg}}}
\setlength{\tabcolsep}{3.96mm}{\begin{tabular}{c|r|ccc}
\hline
&Method  & Rank-1 &Rank-5 & Rank-10 \\ \hline \hline
\multirow{6}{*}{\rotatebox{90}{C  I}} & Dual Path \cite{zheng2020dual}           & 15.41& 29.80 &38.19      \\ 
&MIA \cite{niu2020mia}                 & 19.35  &36.78 & 46.42                  \\ 
&SCAN~\cite{lee2018stacked}               &21.27 &39.26 &48.83       \\ 

&SSAN~\cite{ssan}            & 24.72& 43.43 &53.01         \\ 

&LGUR~\cite{lgur}                     &   34.25& 52.58& 60.85 \\ \hline
&\textbf{\ourmodel}~(Ours)                       &      \textbf{35.85}      &     \textbf{55.04}       &\textbf{63.61}
\\ \hline
\multirow{6}{*}{\rotatebox{90}{I  C}} & Dual Path \cite{zheng2020dual}           & 7.63 &17.14& 23.52      \\ 
&MIA \cite{niu2020mia}                 & 10.93 &23.77 &32.39                  \\ 
&SCAN~\cite{lee2018stacked}               & 13.63 &28.61 &37.05       \\ 

&SSAN~\cite{ssan}            & 16.68& 33.84 &43.00         \\ 

&LGUR~\cite{lgur}                     &   25.44 &44.48& 54.39 \\ \hline
&\textbf{\ourmodel}~(Ours)                        &   \textbf{27.17}  &   \textbf{47.77}  & \textbf{57.27}
\\ \hline
\end{tabular}}
\end{center}

\end{table}


\paragraph{Performance Comparison on ICFG-PEDES}
We evaluate the performance of \ourmodel~with state-of-the-art methods on the other dataset named ICFG-PEDES. As shown in~\tablename~\ref{tab:icfg}, 
due to ICFG-PEDES being a new database, we directly compare the existing methods on text to image task. 
our method still achieves the best performance with 63.05\% Rank-1 accuracy and outperforms the previous SOTA method CFine~\cite{CFine} by 2.22\% in terms of Rank-1 accuracy.

\paragraph{Performance Comparison on the Domain Generalization Task}
As shown in~\tablename~\ref{tab:dg}, to further validate the superiority of our method, we conduct experiments on domain generalization task following~\cite{lgur}. We simply test the performance of model that is pretrained on the source domain on the target domain. Our \ourmodel~achieves the best performance among all the other methods. Specifically, we outperform LGUR~\cite{lgur} by 1.6\%, 1.73\% in terms of Rank-1 accuracy on C  I and I  C, respectively.
This experiment reveals that our local visual and textual alignment have good generalization capability.


\begin{table}[t!]
\renewcommand\arraystretch{1.06}
\centering
\caption{\small{Computational cost comparison among state-of-the-art methods on the CUHK-PEDES database. ``CAM'' represents the cross-modal attention mechanism.}}
\setlength{\tabcolsep}{3.36mm}{\begin{tabular}{c|r|cccc}
    \hline
       CAM & \multicolumn{1}{c|}{Methods}   & FLOPs & Inference & Rank-1 \\
    \hline
    \hline
     \cmark & MIA \cite{niu2020mia}  & 6.95G &13.1ms  &53.10\\
       \cmark & SCAN \cite{lee2018stacked}  &7.02G &13.7ms &55.86\\
       \cmark & NAFS \cite{gao2021contextual}  & 8.87G &15.2ms &59.94\\
       \xmarkg & Dual Path \cite{zheng2020dual}  & 2.92G &2.3ms & 44.40\\
       \xmarkg & CMPM/C \cite{zhang2018cmpc}  &3.11G &3.1ms &49.37\\
       \xmarkg & SSAN \cite{ssan}  & 6.94G &6.0ms &61.37\\
        \xmarkg & \ourmodel~(Ours) & 6.79G &5.3ms & 67.10 \\
    \hline
\end{tabular}}
\label{tab:costtime}
\end{table}


\subsection{Computational Complexity}

As mentioned in the introduction, we aim to design an efficient and effective pipeline for text-based person search and get rid of the pair-comparison paradigm. Thus, we have undertaken a series of experiments to assess the inference speed of each text-to-image query and the model complexity of various methods, employing a consistent and equitable experimental setup. Specifically, a batch size of 64 is employed, and all experiments are executed on a single Nvidia V100 GPU using the PyTorch toolkit. 
As shown in \tablename~\ref{tab:costtime}, the proposed \ourmodel~is faster than SSAN~\cite{ssan} by 0.13x on the CUHK-PEDES database. More notably, when compared to approaches reliant on the pair-comparison paradigm, such as MIA~\cite{niu2020mia}, SCAN~\cite{lee2018stacked}, and NAFS~\cite{gao2021contextual}, the proposed \ourmodel~ demonstrates substantially faster inference times and lower FLOPs. This achievement can be attributed to our utilization of knowledge distillation, a key factor in shedding the pair-comparison paradigm and attaining impressive results.
Collectively, the outcomes of these comparative analyses provide evidence that the proposed \ourmodel~stands as an effective and efficient pipeline for text-based person search.



\begin{figure*}[t]
	\centering
	\includegraphics[width = 0.976\textwidth]{Figs/attention_visualize.pdf}
	\caption{\small{Visualization of attention maps for four text queries within the SGTL cross-attention block. Each attention map represents the relationship between the -th text query and the original text description, aiming to estimate its ability to grasp specific fixed-strip body parts. All sentences are standardized to a length of 77 words, with zero padding applied to sentences shorter than 77 words. 
    It is worth noting that the language information corresponding to zero padding is denoted as ``None''. For ease of comprehension, we have highlighted the 10-th word in orange and the 20-th word in green.
Additionally, words with significant attention weight are denoted with a yellow background color.
 Different colors represent different attention scores. Please refer to the colormap on the right for the correspondence between colors and attention values.
}}\label{fig:attention_visualize}
\end{figure*}

\subsection{Quantitative Result}

\paragraph{Visualization of Feature Distribution}


We utilize t-SNE to visualize the features distribution with and without VGKT in \figurename~\ref{fig:tsne} to verify our superiority of VGKT.
Note that we use paired local textual and visual features (here we use 1-th feature for simplicity) and take features from 15 classes on CUHK-PEDES test set. Local textual features are semantic-group textual features and vision-guided semantic-group textual features.
As shown in \figurename~\ref{fig:tsne}(a), semantic-group textual features from SGTL and corresponding local visual feature (such as the brown circles and brown cross) are far away from each other. 
In addition, inter-class distance is not large enough. The reason behind this is that SGTL is conducted alone from the language itself and has no interaction with the image. In addition, there is misalignment between semantic-group textual features and local visual features.
 
In \figurename~\ref{fig:tsne}(b), we visualize vision-guided textual features distribution which is perfectly aligned with local visual features and has high intra-class compactness and inter-class separability characteristics.
The phenomenon demonstrates that vision-guided textual features are discriminative and can serve as teacher distribution. In addition, vision-guided textual features are well aligned with local visual features.

In \figurename~\ref{fig:tsne}(c), 
since we transfer knowledge from vision-guided textual features to semantic-group textual features, semantic-group textual features are well aligned with local visual features. 
Consequently, the local textual features and local visual features of the same identity (such as the brown circles and brown cross) are pulled closer. Besides, the inter-class distance are larger than that in \figurename~\ref{fig:tsne}(a). The comparisons indicate that VGKT can effectively transfer these valuable knowledge from vision-guided textual features to semantic-group textual features.






\paragraph{Visualization of Attention Map}

To further verify the merits of our proposed method, we visualize the attention maps within the cross-attention block of SGTL, both with and without VGKT in  \figurename~\ref{fig:attention_visualize}. The left four rows are the fixed-stripe body parts and the right four rows are the attention maps representing the responses of the four text queries (TQ 1 to TQ4) with the whole language expression. 
As shown in \figurename~\ref{fig:attention_visualize}~(a) and (c), the four text queries attended areas of language are disorganized and have no correspondence with the local visual feature on the left.
The reason behind this is that SGTL is conducted alone from the language itself and has no interaction with the image.
In contrast, in \figurename~\ref{fig:attention_visualize}~(b) and (d), 
since we transfer knowledge from vision-guided textual features to semantic-group textual features, semantic-group textual features grasp the distribution of local visual features. 
Consequently, the -th text query aligns with its corresponding local visual feature of the -th stripe, such as the 1-th text query has high attention scores in the ``pink headphones around his neck'' corresponding to the 1-th local visual feature.  This phenomenon demonstrates that the proposed VGKT module can effectively transfer this crucial knowledge from vision-guided textual features to semantic-group textual features. As such, semantic-group textual features are well-aligned with local visual features.