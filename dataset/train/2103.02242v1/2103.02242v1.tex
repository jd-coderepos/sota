\renewcommand{\arraystretch}{1.5}
\newcommand{\ycbC}{0.7}
\begin{table*}[tp]
    \centering
    \fontsize{7.2}{7.5}\selectfont
    \begin{tabular}{l|C{\ycbC cm}|C{\ycbC cm}|C{\ycbC cm}|C{\ycbC cm}|C{\ycbC cm}|C{\ycbC cm}|C{\ycbC cm}|C{\ycbC cm}|C{\ycbC cm}|C{\ycbC cm}|C{\ycbC cm}|C{\ycbC cm}}
    \hline
                                  & \multicolumn{2}{c|}{PoseCNN \cite{xiang2017posecnn}}  & \multicolumn{2}{c|}{PointFusion \cite{xu2018pointfusion}}  & \multicolumn{2}{c|}{DCF \cite{liang2018DCFdeepcontiguous}} & \multicolumn{2}{c|}{DF (per-pixel) \cite{wang2019densefusion}}   & \multicolumn{2}{c|}{PVN3D \cite{he2020pvn3d}} & \multicolumn{2}{c}{Our FFB6D} \cr\hline
        Object                         & ADDS        & ADD(S)        & ADDS          & ADD(S)          & ADDs      & ADD(S)      & ADDS           & ADD(S)           & ADDS          & ADD(S)        & ADDS          & ADD(S)        \cr\hline
        002 master chef can            & 83.9        & 50.2          & 90.9          & -               & 90.9      & 74.6        & 95.3           & 70.7             & 96.0          & 80.5          & \textbf{96.3} & \textbf{80.6} \\
        003 cracker box                & 76.9        & 53.1          & 80.5          & -               & 87.1      & 79.3        & 92.5           & 86.9             & 96.1          & \textbf{94.8} & \textbf{96.3} & 94.6          \\
        004 sugar box                  & 84.2        & 68.4          & 90.4          & -               & 94.3      & 84.2        & 95.1           & 90.8             & 97.4          & 96.3          & \textbf{97.6} & \textbf{96.6} \\
        005 tomato soup can            & 81.0        & 66.2          & 91.9          & -               & 90.5      & 79.8        & 93.8           & 84.7             & \textbf{96.2} & 88.5          & 95.6          & \textbf{89.6} \\
        006 mustard bottle             & 90.4        & 81.0          & 88.5          & -               & 90.6      & 83.5        & 95.8           & 90.9             & 97.5          & 96.2          & \textbf{97.8} & \textbf{97.0} \\
        007 tuna fish can              & 88.0        & 70.7          & 93.8          & -               & 91.7      & 73.8        & 95.7           & 79.6             & 96.0          & \textbf{89.3} & \textbf{96.8} & 88.9          \\
        008 pudding box                & 79.1        & 62.7          & 87.5          & -               & 89.3      & 84.1        & 94.3           & 89.3             & \textbf{97.1} & \textbf{95.7} & \textbf{97.1} & 94.6          \\
        009 gelatin box                & 87.2        & 75.2          & 95.0          & -               & 92.9      & 89.5        & 97.2           & 95.8             & 97.7          & 96.1          & \textbf{98.1} & \textbf{96.9} \\
        010 potted meat can            & 78.5        & 59.5          & 86.4          & -               & 83.2      & 74.6        & 89.3           & 79.6             & 93.3          & \textbf{88.6} & \textbf{94.7} & 88.1          \\
        011 banana                     & 86.0        & 72.3          & 84.7          & -               & 84.8      & 71.0        & 90.0           & 76.7             & 96.6          & 93.7          & \textbf{97.2} & \textbf{94.9} \\
        019 pitcher base               & 77.0        & 53.3          & 85.5          & -               & 89.5      & 80.3        & 93.6           & 87.1             & 97.4          & 96.5          & \textbf{97.6} & \textbf{96.9} \\
        021 bleach cleanser            & 71.6        & 50.3          & 81.0          & -               & 88.4      & 79.8        & 94.4           & 87.5             & 96.0          & 93.2          & \textbf{96.8} & \textbf{94.8} \\
        \textbf{024 bowl}              & 69.6        & 69.6          & 75.7          & 75.7            & 80.3      & 80.3        & 86.0           & 86.0             & 90.2          & 90.2          & \textbf{96.3} & \textbf{96.3} \\
        025 mug                        & 78.2        & 58.5          & 94.2          & -               & 90.7      & 76.6        & 95.3           & 83.8             & \textbf{97.6} & \textbf{95.4} & 97.3          & 94.2          \\
        035 power drill                & 72.7        & 55.3          & 71.5          & -               & 87.4      & 78.4        & 92.1           & 83.7             & 96.7          & 95.1          & \textbf{97.2} & \textbf{95.9} \\
        \textbf{036 wood block}        & 64.3        & 64.3          & 68.1          & 68.1            & 84.2      & 84.2        & 89.5           & 89.5             & 90.4          & 90.4          & \textbf{92.6} & \textbf{92.6} \\
        037 scissors                   & 56.9        & 35.8          & 76.7          & -               & 84.2      & 70.3        & 90.1           & 77.4             & 96.7          & 92.7          & \textbf{97.7} & \textbf{95.7} \\
        040 large marker               & 71.7        & 58.3          & 87.9          & -               & 89.5      & 81.0        & 95.1           & 89.1             & \textbf{96.7} & \textbf{91.8} & 96.6          & 89.1          \\
        \textbf{051 large clamp}       & 50.2        & 50.2          & 65.9          & 65.9            & 63.6      & 63.6        & 71.5           & 71.5             & 93.6          & 93.6          & \textbf{96.8} & \textbf{96.8} \\
        \textbf{052 extra large clamp} & 44.1        & 44.1          & 60.4          & 60.4            & 64.4      & 64.4        & 70.2           & 70.2             & 88.4          & 88.4          & \textbf{96.0} & \textbf{96.0} \\
        \textbf{061 foam brick}        & 88.0        & 88.0          & 91.8          & 91.8            & 83.1      & 83.1        & 92.2           & 92.2             & 96.8          & 96.8          & \textbf{97.3} & \textbf{97.3} \cr\hline
        ALL                            & 75.8        & 59.9          & 83.9          & -               & 85.7      & 77.9        & 91.2           & 82.9             & 95.5          & 91.8          & \textbf{96.6} & \textbf{92.7} \cr\hline
    \end{tabular}
    \caption{Quantitative evaluation of 6D Pose without iterative refinement on the YCB-Video Dataset. The ADD-S \cite{xiang2017posecnn} and ADD(S) \cite{hinterstoisser2012model} AUC are reported. Symmetric objects are in bold. DF (per-pixel) means DenseFusion (per-pixel).
}
    \label{tab:YCB_PFM}
\end{table*}

\newcommand{\kpC}{1.0}
\begin{table}[tp]
    \centering
    \fontsize{6.9}{6.8}\selectfont
    \begin{tabular}{L{0.9 cm}|C{\kpC cm}|C{0.9 cm}|C{\kpC cm}|C{1.1 cm}|C{\kpC cm}}
        \hline
                & PCNN+ICP & DF(iter.) & MoreFusion & PVN3D+ICP & FFB6D+ICP     \cr\hline
        ADD-S   & 93.0     & 93.2      & 95.7       & 96.1      & \textbf{97.0} \\
        ADD(S)  & 85.4     & 86.1      & 91.0       & 92.3      & \textbf{93.1} \\
        \hline  
    \end{tabular}
    \caption{Quantitative evaluation of 6D Pose with iterative refinement on the YCB-Video Dataset (ADD-S \cite{xiang2017posecnn} and ADD(S) AUC \cite{hinterstoisser2012model}). Baselines: PoseCNN+ICP \cite{xiang2017posecnn}, DF(iter.) \cite{wang2019densefusion}, MoreFusion \cite{wada2020morefusion}, PVN3D+ICP \cite{he2020pvn3d}.}
    \label{tab:YCB_PF_ICP}
\end{table}

\section{Experiments}
\subsection{Benchmark Datasets}
We evaluate our method on three benchmark datasets. 

\textbf{YCB-Video} \cite{calli2015ycb} contains 92 RGBD videos that capture scenes of 21 selected YCB objects. We followed previous works \cite{xiang2017posecnn,wang2019densefusion,he2020pvn3d} to split the training and testing set. Synthetic images were also taken for training as in \cite{xiang2017posecnn} and the hole completion algorithm \cite{ku2018defense} is applied for hole filling for depth images as in \cite{he2020pvn3d}. 

\textbf{LineMOD} \cite{hinterstoisser2011multimodal} is a dataset with 13 videos of 13 low-textured objects. The texture-less objects, cluttered scenes, and varying lighting make this dataset challenge. We split the training and testing set following previous works \cite{xiang2017posecnn,peng2019pvnet} and generate synthesis images for training following \cite{peng2019pvnet,he2020pvn3d}.

\textbf{Occlusion LINEMOD} \cite{OcclusionLMbrachmann2014learning} was selected and annotated from the LineMOD datasets. Each scene in this dataset consists of multi annotated objects, which are heavily occluded. The heavily occluded objects make this dataset challenge.


\begin{figure}
  \centering
  \includegraphics[scale=0.55]{images/occlusion_auc_less_0.02.png}
  \caption{
    Performance of different approaches under increasing levels of occlusion on the YCB-Video dataset.
  }
  \label{fig:occlussion_auc}
\end{figure}

\subsection{Evaluation Metrics}
We use the average distance metrics ADD and ADD-S for evaluation. For asymmetric objects, the ADD metric calculate the point-pair average distance between objects vertexes transformed by the predicted and the ground truth pose, defined as follows:

where  denotes a vertex in object , ,  the predicted pose and  the ground truth.
For symmetric objects, the ADD-S based on the closest point distance is applied:

In the YCB-Video dataset, we follows previous methods \cite{xiang2017posecnn,wang2019densefusion,he2020pvn3d} and report the area under the accuracy-threshold curve obtained by varying the distance threshold (ADD-S and ADD(S) AUC). In the LineMOD and Occlusion LineMOD datasets, we report the accuracy of distance less than 10\% of the objects diameter (ADD-0.1d) as in \cite{hinterstoisser2012model,peng2019pvnet}.

\subsection{Training and Implementation}
\textbf{Network architecture.} We apply ImageNet \cite{deng2009imagenet} pre-trained ResNet34 \cite{resnet} as encoder of RGB images, followed by a PSPNet \cite{zhao2017pyramid} as decoder.  For point cloud feature extraction, we randomly sample 12288 points from depth images following \cite{he2020pvn3d} and applied RandLA-Net \cite{hu2020randla} for representation learning. In each encoding and decoding layers of the two networks, max pooling and shared MLPs are applied to build bidirectional fusion modules. After the process of the full flow bidirectional fusion network, each point has a feature  of  dimension. These dense RGBD features are then fed into the instance semantic segmentation and the keypoint offset learning modules consist of shared MLPs.

\textbf{Optimization regularization.} The semantic segmentation branch is supervised by Focal Loss \cite{lin2017focal}. The center point voting and 3D keypoints voting modules are optimized by L1 loss as in \cite{he2020pvn3d}. To jointly optimize the three tasks, a multi-task loss with the weighted sum of them is applied following \cite{he2020pvn3d}.

\textbf{SIFT-FPS keypoint selection algorithm.} We put the target object at the center of a sphere and sample viewpoints of the camera on the sphere equidistantly. RGBD images with camera poses are obtained by render engines. We then detect 2D keypoints from RGB images with SIFT. These 2D keypoints are lifted to 3D and transformed back to the object coordinates system. Finally, an FPS algorithm is applied to select  target keypoints out of them.

\renewcommand{\arraystretch}{1.3}
\begin{table*}[tp]
    \centering
    \fontsize{7.0}{6.8}\selectfont
    \begin{tabular}{l|C{1.1cm}|C{1.1cm}|C{1.1cm}|C{1.1cm}|C{1.1cm}|C{1.1cm}|C{1.1cm}|C{1.1cm}|C{1.1cm} }
        \hline
                & \multicolumn{4}{c|}{RGB}               & \multicolumn{5}{c}{RGB-D}                    \cr\hline                                               
                & PoseCNN DeepIM \cite{xiang2017posecnn,li2018deepim} & PVNet\cite{peng2019pvnet} & CDPN\cite{li2019cdpn} & DPOD\cite{Zakharov2019dpod}  & Point- Fusion\cite{xu2018pointfusion} & Dense- Fusion\cite{wang2019densefusion} & G2L-Net\cite{chen2020g2l} & PVN3D\cite{he2020pvn3d} & Our FFB6D          \cr\hline

MEAN            & 88.6           & 86.3  & 89.9 & 95.2  & 73.7        & 94.3                   & 98.7           & 99.4                      & \textbf{99.7} 
        \cr\hline  
    \end{tabular}
    \caption{Quantitative evaluation of 6D pose on the LineMOD dataset (ADD-0.1d \cite{hinterstoisser2012model} metrics).}
    \label{tab:LM_PFM}
\end{table*}

\newcommand{\OlC}{0.95}
\begin{table}[tp]
    \centering
    \fontsize{6.9}{6.8}\selectfont
    \begin{tabular}{l|C{\OlC cm}|C{\OlC cm}|C{\OlC cm}|C{\OlC cm}|C{\OlC cm} }
        \hline
        Method   & PoseCNN \cite{xiang2017posecnn} & Oberweger \cite{oberweger2018making} & Hu et al. \cite{hu2019segmentation}         & Pix2Pose \cite{park2019pix2pose} & PVNet \cite{peng2019pvnet}          \cr\hline
        ADD-0.1d & 24.9    & 27.0      & 27.0       & 32.0     & 40.8  \cr\hline
        Method   & DPOD \cite{Zakharov2019dpod}    & Hu et al.\cite{hu2020single} & HybridPose \cite{song2020hybridpose} & PVN3D \cite{he2020pvn3d}    & Our FFB6D \cr\hline
        ADD-0.1d & 47.3    & 43.3      & 47.5       & 63.2     & \textbf{66.2}  \\
        \hline  
    \end{tabular}
    \caption{Quantitative evaluation of 6D pose (ADD-0.1d) on the Occlusion-LineMOD dataset.}
    \label{tab:OCC_LM_PFM}
\end{table}

\subsection{Evaluation on Three Benchmark Datasets.}
We evaluate the proposed models on the YCB-Video, the LineMOD, and the Occlusion LineMOD datasets.

\textbf{Evaluation on the YCB-Video dataset.} Table \ref{tab:YCB_PFM} shows the quantitative evaluation results of the proposed FFB6D on the YCB-Video dataset. We compare it with other single view methods without iterative refinement. FFB6D advances state-of-the-art results by 1.1\% on the ADD-S metric and 1.0\% on the ADD(S) metric. Equipped with extra iterative refinement, our approach also achieves the best performance, demonstrated in Table \ref{tab:YCB_PF_ICP}. Note that the proposed FFB6D without any iterative refinement even outperforms state-of-the-arts that require time-consuming post-refinement procedures. Qualitative results are reported in the supplementary material. 

\textbf{Robustness towards occlusion.} We follow \cite{wang2019densefusion,he2020pvn3d} to report the ADD-S less than 2cm accuracy under the growth of occlusion level on the YCB-Video dataset. As is shown in Figure \ref{fig:occlussion_auc}, previous methods degrade as the occlusion increase. In contrast, FFB6D didn't suffer from a drop in performance. We think our full flow bidirectional fusion mechanism makes full use of the texture and geometry information in the captured data and enables our approach to locate 3D keypoints even in highly occluded scenes. 

\textbf{Evaluation on the LineMOD dataset \& Occlusion LineMOD dataset.} The proposed FFB6D outperforms the state-of-the-art on the LineMOD dataset, presented in Table \ref{tab:LM_PFM}. We also evaluate FFB6D on the Occlusion LineMOD dataset, shown in Table \ref{tab:OCC_LM_PFM}. In the table, our FFB6D without iterative refinement advances state-of-the-art by 4.7\%, further confirming its robustness towards occlusion.


\begin{figure*}
  \centering
     \includegraphics[scale=0.55]{images/effect_bifusion.pdf}
     \caption{
         Effect of full flow bidirectional fusion, compared to PVN3D \cite{he2020pvn3d} with DenseFusion architecture.
}
     \label{fig:eff_BiF}
\end{figure*}


\subsection{Ablation Study}
In this subsection, we present extensive ablation studies on our design choices and discuss their effect. 

\newcommand{\fdeC}{1.15}
\begin{table}[tp]
  \centering
  \fontsize{6.9}{6.8}\selectfont
  \begin{tabular}{C{\fdeC cm}|C{\fdeC cm}|C{\fdeC cm}|C{\fdeC cm}|C{\fdeC cm} }
    \hline
    \multicolumn{3}{c|}{Fusion Stage} & \multicolumn{2}{c}{Pose Result} \cr\hline
    FE     & FD    & DF    & ADD-S    & ADD(S) \cr\hline
          &     &     & 91.9     & 87.5 \cr\hline
      &     &     & 96.2     & 92.2 \cr\hline
          &  &     & 93.0     & 89.2 \cr\hline
          &     &  & 94.0     & 90.8 \cr\hline
      &  &     & \textbf{96.6} & \textbf{92.7} \cr\hline
      &  &  & 96.4     & 92.6  
    \cr\hline 
  \end{tabular}
  \caption{Effect of fusion stages on the YCB-Video dataset. FE: fusion during encoding; FD: fusion during decoding; DF: Dense Fusion on the two final feature maps.}
  \label{tab:EffFuseEcDc}
\end{table}

\newcommand{\fprC}{1.15}
\begin{table}[tp]
  \centering
  \fontsize{6.9}{6.8}\selectfont
  \begin{tabular}{C{\fprC cm}|C{\fprC cm}|C{\fprC cm}|C{\fprC cm} }
    \hline
    \multicolumn{2}{c|}{Fusion Direction} & \multicolumn{2}{c}{Pose Result} \cr\hline
    P2R    & R2P   & ADD-S    & ADD(S)  \cr\hline
          &     & 91.9     & 87.5   \cr\hline
      &     & 95.8     & 91.1   \cr\hline
          &  & 94.5     & 90.6   \cr\hline
      &  & \textbf{96.6} & \textbf{92.7}
    \cr\hline
  \end{tabular}
  \caption{Effect of fusion direction on the YCB-Video dataset. P2R means fusion from point cloud embeddings to RGB embeddings, and R2P means fusion from RGB embeddings to point cloud embeddings.}
  \label{tab:EffFuseRGBPoint}
\end{table}

\textbf{Effect of full flow bidirectional fusion.}
To validate that building fusion modules between the two modality networks in full flow help, we ablate fusion stages in Table \ref{tab:EffFuseEcDc}. Compared to the mechanism without fusion, adding fusion modules either on the encoding stage, decoding stage, or on the final feature maps, can all boost the performance. Among the three stages, fusion on the encoding stages obtained the highest improvement. We think that's because the extracted local texture and geometric information are shared through the fusion bridge on the early encoding stage, and more global features are shared when the network goes deeper. Also, adding fusion modules in full flow of the network, saying that on both the encoding and decoding stages, obtains the highest performance. While adding extra DenseFusion behind full flow fusion obtains no performance gain as the two embeddings have been fully fused.

We also ablate the fusion direction in Table \ref{tab:EffFuseRGBPoint} to validate the help of bidirectional fusion. Compared with no fusion, both the fusion from RGB to point cloud and the inverse way facilitate better representation learning. Combining the two obtains the best results. On the one hand, from the view of PCN, we think the rich textures information obtained from high-resolution RGB images helps semantic recognition. In addition, the high-resolution RGB features provide rich information for blind regions of depth sensors caused by reflective surfaces. It serves as a completion to point cloud and improve pose accuracy, as shown in Figure \ref{fig:eff_BiF}(a). On the one hand, geometry information extracted from point cloud helps the RGB branch by distinguishing foreground objects from the background that are in similar colors. Moreover, the shape size information extracted from point clouds helps divide objects with a similar appearance but in a different size, as is shown in Figure \ref{fig:eff_BiF}(b).


\textbf{Effect of representation learning frameworks.} We explore the effect of different representation learning frameworks for the two modalities of data in this part. The result is presented in Table \ref{tab:EffBackB}. We find that neither concatenating the XYZ map as extra information to CNN (CNN-RD) nor adding RGB values as extra inputs to the PCN (PCN-RD) achieves satisfactory performance. Using two CNNs (CNN-R+CNN-D) or two PCNs (PCN-R+PCN-D) with full flow bidirectional fusion modules get better but are still far from satisfactory. In contrast, applying CNN on the RGB image and PCN on the point cloud (CNN-R+PCN-D) gets the best performance. We think that the grid-like image data is discrete, on which the regular convolution kernel fits better than continuous PCN. While the geometric information residing in the depth map is defined in a continuous vector space, and thus PCNs can learn better representation.

\newcommand{\bbC}{1.7}
\begin{table}[tp]
  \centering
  \fontsize{6.9}{6.8}\selectfont
  \begin{tabular}{l|C{\bbC cm}|C{\bbC cm}|C{\bbC cm} }
    \hline
           & CNN-RD    & PCN-RD    & CNN-R+CNN-D \cr\hline
    ADD-S  & 91.0        & 90.9        & 92.1          \\
    ADD(S) & 85.5        & 81.2        & 87.2          \cr\hline
           & PCN-R+PCN-D & CNN-R+3DC-D & CNN-R+PCN-D \cr\hline
    ADD-S  & 91.8        & 94.9        & \textbf{96.6} \\
    ADD(S) & 84.3        & 90.1        & \textbf{92.7}
    \cr\hline 
  \end{tabular}
  \caption{Effect of representation learning framework on the two modalities of data. CNN: 2D Convolution Neural Network; PCN: point cloud network; 3DC: 3D ConvNet; R: RGB images; D: XYZ maps for CNN, point clouds for PCN and voxelized point clouds for 3D ConvNet.}
  \label{tab:EffBackB}
\end{table}


\textbf{Effect of 3D keypoints selection algorithm.} In Table \ref{tab:EffKPSA}, we study the effect of different keypoint selection algorithms. Compared with FPS that only considers the mutual distance between keypoints, our SIFT-FPS algorithm taking both object texture and geometry information into account is easier to locate. Therefore, the predicted keypoint error is smaller and the estimated poses are more accurate. 

\newcommand{\cmpkpC}{0.62}
\begin{table}[tp]
  \centering
  \fontsize{6.9}{6.8}\selectfont
  \begin{tabular}{l|C{\cmpkpC cm}|C{\cmpkpC cm}|C{\cmpkpC cm}|C{\cmpkpC cm}|C{\cmpkpC cm}|C{\cmpkpC cm} }
    \hline
             & FPS4 & S-F4 & FPS8 & S-F8          & FPS12          & S-F12 \cr\hline
KP err. (cm) & 1.3   & 1.2   & 1.4   & \textbf{1.2}   & 1.6             & 1.3   \\
ADD-S        & 95.9  & 96.4  & 96.1  & \textbf{96.6}  & 96.0            & 96.5  \\
ADD(S)       & 92.0  & 92.4  & 92.3  & \textbf{92.7}  & 92.0            & 92.6 
    \cr\hline 
  \end{tabular}
  \caption{Effect of keypoint selection algorithm. S-F means the proposed SIFT-FPS algorithm.}
  \label{tab:EffKPSA}
\end{table}


\textbf{Effect of the downsample strategy of the assisting XYZ map.} The size of RGB feature maps are shrunk by stridden convolution kernels. To maintain the corresponding XYZ maps, we first scale it down with the same size of mean kernels and got 96.3 ADD-S AUC. However, simply resize the XYZ map with the nearest interpolation got 96.6. We find the average operation produces noise points on the boundary and decrease the performance. 

\textbf{Model parameters and time efficiency.}
In Table \ref{tab:ParamEff}, we report the parameters and run-time breakdown of FFB6D. Compared to PVN3D \cite{he2020pvn3d}, which obtained the fused RGBD feature by dense fusion modules \cite{wang2019densefusion} in the final layers, our full flow bidirectional fusion network achieve better performance with fewer parameters and is 2.5 times faster.

\newcommand{\ptC}{1.15}
\begin{table}[tp]
  \centering
  \fontsize{6.9}{6.8}\selectfont
  \begin{tabular}{l|C{\ptC cm}|C{\ptC cm}|C{\ptC cm}|C{\ptC cm} }
    \hline
    \multicolumn{1}{l|}{}         & \multicolumn{1}{c|}{}              & \multicolumn{3}{c}{Run-time (ms/frame)}                   \\ \cline{3-5}
    \multicolumn{1}{l|}{\multirow{-2}{*}{}} & \multicolumn{1}{c|}{\multirow{-2}{*}{Parameters}} & \multicolumn{1}{c|}{NF} & \multicolumn{1}{c|}{PE} & \multicolumn{1}{c}{All} \\ \hline
    PVN3D\cite{he2020pvn3d}         & 39.2M           & 170                 & 20                 & 190        \\
    Our FFB6D                & 33.8M           & 57                 & 18                 & 75 \cr
    \hline 
  \end{tabular}
  \caption{Model parameters and run-time breakdown on the LineMOD dataset. NF: Network Forward; PE: Pose Estimation. Our FFB6D with fewer parameters is 2.5x faster.
}
  \label{tab:ParamEff}
\end{table}





