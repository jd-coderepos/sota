\section{Experiments}\label{section:results}

We perform quantitative and qualitative evaluations of Hide-and-Seek for object localization in images and action localization in videos.  We also perform ablative studies to compare the different design choices of our algorithm.


\vspace{-10pt}
\paragraph{Datasets and evaluation metrics.} We use ILSVRC 2016~\cite{Russakovsky-IJCV2015} to evaluate object localization accuracy. For training, we use 1.2 million images with their class labels (1000 categories). We compare our approach with the baselines on the validation data. We use three evaluation metrics to measure performance: 1) Top-1 localization accuracy (\emph{Top-1 Loc}): fraction of images for which the predicted class with the highest probability is the same as the ground-truth class \emph{and} the predicted bounding box for that class has more than  IoU with the ground-truth box. 2) Localization accuracy with known ground-truth class (\emph{GT-known Loc}): fraction of images for which the predicted bounding box for the ground-truth class has more than  IoU with the ground-truth box.  As our approach is primarily designed to improve localization accuracy, we use this criterion to measure localization accuracy independent of classification performance. 3) We also use classification accuracy (\emph{Top-1 Clas}) to measure the impact of Hide-and-Seek on image classification performance.

For action localization, we use THUMOS 2014 validation data~\cite{jiang-14}, which consists of 1010 untrimmed videos belonging to 101 action classes. We train over all untrimmed videos for the classification task and then evaluate localization on the 20 classes that have temporal annotations. Each video can contain multiple instances of a class. For evaluation we compute mean average precision (mAP), and consider a prediction to be correct if it has IoU  with ground-truth. We vary  to be 0.1, 0.2, 0.3, 0.4, and 0.5. As we are focusing on localization ability of the network, we assume we know the ground-truth class label of the video.


\vspace{-10pt}
\paragraph{Implementation details.} To learn the object localizer, we use the same modified AlexNet and GoogLeNet networks introduced in~\cite{zhou-cvpr2016} (AlexNet-GAP and  GoogLeNet-GAP).  AlexNet-GAP is identical to AlexNet until pool5 (with stride 1) after which two new conv layers are added.  Similarly for GoogLeNet-GAP, layers after inception-4e are removed and a single conv layer is added.  For both AlexNet-GAP and GoogLeNet-GAP, the output of the last conv layer goes to a global average pooling (GAP) layer, followed by a softmax layer for classification.  Each added conv layer has 512 and 1024 kernels of size , stride 1, and pad 1 for AlexNet-GAP and  GoogLeNet-GAP, respectively. 

We train the networks from scratch for 55 and 40 epochs for AlexNet-GAP and GoogLeNet-GAP, respectively, with a batch size of 128 and initial learning rate of 0.01. We gradually decrease the learning rate to 0.0001.  We add batch normalization~\cite{bn} after every conv layer to help convergence of GoogLeNet-GAP. For simplicity, unlike the original AlexNet architecture~\cite{krizhevsky-nips2012}, we do not group the conv filters together (it produces statistically the same \emph{Top-1 Loc} accuracy as the grouped version for both AlexNet-GAP but has better image classification performance). The network remains exactly the same with (during training) and without (during testing) hidden image patches. To obtain the binary fg/bg map,  and  of the max value of the CAM is chosen as the threshold for AlexNet-GAP and GoogLeNet-GAP, respectively; the thresholds were chosen by observing a few qualitative results on training data.  During testing, we average 10 crops (4 corners plus center, and same with horizontal flip) to obtain class probabilities and localization maps.  We find similar localization/classification performance when fine-tuning pre-trained networks.

For action localization, we compute C3D~\cite{tran-iccv2015} fc7 features using a model pre-trained on Sports 1 million~\cite{karpathy-CVPR14}. We compute 10 feats/sec (each feature is computed over 16 frames) and uniformly sample 2000 features from the video. We then divide the video into 20 equal-length segments each consisting of  features.  During training, we hide each segment with .  For action classification, we feed C3D features as input to a CNN with two conv layers followed by a global max pooling and softmax classification layer. Each conv layer has 500 kernels of size , stride 1. For any hidden frame, we assign it the dataset mean C3D feature. For thresholding,  of the max value of the CAM is chosen. All continuous segments after thresholding are considered predictions.

\begin{figure*}[t!]
\centering
    \includegraphics[width=0.94\textwidth]{figs/drop_patch_quali.pdf}
    \caption{Qualitative object localization results.  We compare our approach with AlexNet-GAP~\cite{zhou-cvpr2016} on the ILVRC validation data. For each image, we show the bounding box and CAM obtained by AlexNet-GAP (left) and our method (right).  Our Hide-and-Seek approach localizes multiple relevant parts of an object whereas AlexNet-GAP mainly focuses only on the most discriminative parts.}
    \vspace*{-0.12in}
\label{fig:qualresults}
\end{figure*}






\footnotetext[2]{\cite{zhou-cvpr2016} does not provide GT-known loc, so we compute on our own GAP implementations, which achieve similar Top-1 Loc accuracy.}

\begin{table}[t!]
\begin{center}
    \footnotesize
    \begin{tabular}{| c | c | c| c|}
    	
    \hline    	
    Methods & GT-known Loc &  Top-1 Loc  & Top-1 Clas   \\
    \hline
    
    AlexNet-GAP~\cite{zhou-cvpr2016}  &  54.90\footnotemark[2] & 36.25 & \textbf{60.23}\\
    AlexNet-HaS-16     & 57.86 & 36.77 & 57.97  \\
    AlexNet-HaS-32            & \textbf{58.75} & 37.33 & 57.94 \\
    AlexNet-HaS-44            & 58.55 & 37.54 & 58.10 \\
    AlexNet-HaS-56            & 58.43 & 37.34 & 58.13  \\
    AlexNet-HaS-Mixed       & 58.68 & \textbf{37.65} & 58.68 \\
   

            \hline
     GoogLeNet-GAP~\cite{zhou-cvpr2016}  & 58.41\footnotemark[2] & 43.60 & \textbf{71.95}  \\
     GoogLeNet-HaS-16            & 59.83 & 44.62 & 70.49 \\
                GoogLeNet-HaS-32            &  \textbf{60.29} & \textbf{45.21} & 70.70 \\
                GoogLeNet-HaS-44          & 60.11 & 44.75 & 70.34  \\
                GoogLeNet-HaS-56            & 59.93 & 44.78 & 70.37  \\
                \hline

    \end{tabular}
    \caption{Localization accuracy on ILSVRC validation data with different patch sizes for hiding.  Our Hide-and-Seek always performs better than AlexNet-GAP~\cite{zhou-cvpr2016}, which sees the full image.}
    \label{table:patch_size_results}
\end{center}
\vspace*{-0.15in}
\end{table}

\subsection{Object localization quantitative results}

We first analyze object localization accuracy on the ILSVRC validation data. Table~\ref{table:patch_size_results} shows the results using the \emph{Top-1 Loc} and \emph{GT-known Loc} evaluation metrics.  AlexNet-GAP~\cite{zhou-cvpr2016} is our baseline in which the network has seen the full image during training without any hidden patches. Alex-HaS-N is our approach, in which patches of size  are hidden with 0.5 probability during training.

\vspace{-10pt}
\paragraph{Which patch size  should we choose?} We explored four different patch sizes , and each performs significantly better than AlexNet-GAP for both \emph{GT-known Loc} and \emph{Top-1 Loc}. Our GoogLeNet-HaS-N models also outperfors GoogLeNet-GAP for all patch sizes.  These results clearly show that hiding patches during training leads to better localization. Although our approach can lose some classification accuracy (\emph{Top-1 Clas}) since it has never seen a complete image and thus may not have learned to relate certain parts, the huge boost in localization performance (which can be seen by comparing the \emph{GT-known Loc} accuracies) makes up for any potential loss in classification.


\begin{table}[t!]
\begin{center}
    \footnotesize
    \begin{tabular}{| c | c | c| c|}
    \hline    	
    Methods & GT-known Loc &  Top-1 Loc    \\
    \hline
    Backprop on AlexNet~\cite{simonyan-iclr2014} & - & 34.83 \\
    AlexNet-GAP~\cite{zhou-cvpr2016}  &  54.90 & 36.25\\
    
    Ours         & \textbf{58.68} & \textbf{37.65}  \\
    \hline
    AlexNet-GAP-ensemble   & 56.91 & 38.58 \\
    Ours-ensemble       & \textbf{60.14} & \textbf{40.40}   \\
            \hline
     Backprop on GoogLeNet~\cite{simonyan-iclr2014} & - & 38.69 \\
         GoogLeNet-GAP~\cite{zhou-cvpr2016} & 58.41 & 43.60 \\
         Ours         &  \textbf{60.29} & \textbf{45.21} \\
     \hline
    \end{tabular}
    \caption{Localization accuracy on ILSVRC val data compared to state-of-the-art.  Our method outperforms all previous methods.}
    \label{table:main_results}
\end{center}
\vspace*{-0.15in}
\end{table}



We also train a network (AlexNet-HaS-Mixed) with mixed patch sizes. During training, for each image in every epoch, the patch size  to hide is chosen randomly from 16, 32, 44 and 56 as well as no hiding (full image).  Since different sized patches are hidden, the network can learn complementary information about different parts of an object (e.g. small/large patches are more suitable to hide smaller/larger parts). Indeed, we achieve the best results for \emph{Top-1 Loc} using AlexNet-HaS-Mixed. 

\vspace{-10pt}
\paragraph{Comparison to state-of-the-art.}  Next, we choose our best model for AlexNet and GoogLeNet, and compare it with state-of-the-art methods on ILSVRC validation data; see Table~\ref{table:main_results}. Our method performs 3.78\% and 1.40\% points better than AlexNet-GAP~\cite{zhou-cvpr2016} on \emph{GT-known Loc} and \emph{Top-1 Loc}, respectively. For GoogLeNet, our model gets a boost of 1.88\% and 1.61\% points compared to GoogLeNet-GAP for \emph{GT-known Loc} and \emph{Top-1 Loc} accuracy, respectively.  Importantly, these gains are obtained simply by changing the input image without changing the network architecture.

\vspace{-10pt}
\paragraph{Ensemble model.}  Since each patch size provides complementary information (as seen in the previous section), we also create an ensemble model of different patch sizes (Ours-ensemble).  To produce the final localization for an image, we average the CAMs obtained using AlexNet-HaS-16, 32, 44, and 56, while for classification, we average the classification probabilities of all four models as well as the probability obtained using AlexNet-GAP.  This ensemble model gives a boost of 5.24 \% and 4.15\% over AlexNet-GAP for \emph{GT-known Loc} and \emph{Top-1 Loc}, respectively.  For a more fair comparison, we also combine the results of five independent AlexNet-GAPs to create an ensemble baseline.  Ours-ensemble outperforms this strong baseline (AlexNet-GAP-ensemble) by 3.23\% and 1.82\% for \emph{GT-known Loc} and \emph{Top-1 Loc}, respectively.








\subsection{Object localization qualitative results}


In Fig.~\ref{fig:qualresults}, we visualize the class activation map (CAM) and bounding box obtained by our AlexNet-HaS approach versus those obtained with AlexNet-GAP.  In each image pair, the first image shows the predicted (green) and ground-truth (red) bounding box. The second image shows the CAM, i.e., where the network is focusing for that class.  Our approach localizes more relevant parts of an object compared to AlexNet-GAP and is not confined to only the most discriminative parts. For example, in the first, second, and fifth rows AlexNet-GAP only focuses on the face of the animals, whereas our method also localizes parts of the body.  Similarly, in the third and last rows AlexNet-GAP misses the tail for the snake and squirrel while ours gets the tail. 

\subsection{Further Analysis of Hide-and-Seek}


\paragraph{Comparison with dropout.}  Dropout~\cite{srivastava-jmlr2014} has been extensively used to reduce overfitting in deep network.  Although it is not designed to improve localization, the dropping of units is related to our hiding of patches. We therefore conduct an experiment in which 50\% dropout is applied at the image layer. We noticed that the due to the large dropout rate at the pixel-level, the learned filters develop a bias toward a dropped-out version of the images and produces significantly inferior classification and localization performance (AlexNet-dropout-trainonly). If we also do dropout during testing (AlexNet-dropout-traintest) then performance improves but is still much lower compared to our approach (Table~\ref{table:dropout_results}).   Since dropout drops pixels (and RGB channels) randomly, information from the most relevant parts of an object will still be seen by the network with high probability, which makes it likely to focus on only the most discriminative parts.




\begin{table}[t!]
            \begin{center}
                \footnotesize
                \begin{tabular}{| c | c | c|}
                \hline    	
                Methods & GT-known Loc &  Top-1 Loc \\
                \hline

                Ours      &     \textbf{58.68} & \textbf{37.65}  \\
                AlexNet-dropout-trainonly            & 42.17 & 7.65  \\
                AlexNet-dropout-traintest           &  53.48  & 31.68  \\

                \hline
                 \end{tabular}
                        \caption{Our approach outperforms Dropout~\cite{srivastava-jmlr2014} for localization.}
                        \label{table:dropout_results}
                        \end{center}
                        \vspace*{-0.15in}
                        \end{table}






\begin{table}[t!]
        \begin{center}
            \footnotesize
            \begin{tabular}{| c | c | c|}
            \hline    	
            Methods & GT-known Loc &  Top-1 Loc \\
            \hline
            AlexNet-GAP            & 54.90 & 36.25 \\
            AlexNet-Avg-HaS            & 58.43 & 37.34   \\
            AlexNet-GMP            & 50.40 & 32.52  \\
            AlexNet-Max-HaS            & \textbf{59.27} &  \textbf{37.57}  \\

            \hline
             \end{tabular}
                    \caption{Global average pooling (GAP) vs.~global max pooling (GMP).  Unlike~\cite{zhou-cvpr2016}, for Hide-and-Seek GMP still performs well for localization. For this experiment, we use patch size 56.}
                    \label{table:max_results}
                    \end{center}
                    \vspace*{-0.15in}
                    \end{table}



\vspace{-10pt}
\paragraph{Do we need global average pooling?}  \cite{zhou-cvpr2016} showed that GAP is better than global max pooling (GMP) for object localization, since average pooling encourages the network to focus on all the discriminative parts.  For max pooling, only the most discriminative parts need to contribute. But is global max pooling hopeless for localization?


With our Hide-and-Seek, even with max pooling, the network is forced to focus on a different discriminative parts.   In Table~\ref{table:max_results}, we see that max pooling (AlexNet-GMP) is inferior to average poling (AlexNet-GAP) for the baselines. But with Hide-and-Seek, max pooling (AlexNet-Max-HaS) localization accuracy increases by a big margin and even slightly outperforms average pooling (AlexNet-Avg-HaS). The slight improvement is likely due to max pooling being more robust to noise.




\begin{table}[t!]
                \begin{center}
                    \footnotesize
                    \begin{tabular}{| c | c | c|}
                    \hline    	
                    Methods & GT-known Loc &  Top-1 Loc \\
                    \hline

                    AlexNet-GAP            & 54.90 & 36.25 \\
                    AlexNet-HaS-conv1-5            & 57.36 & 36.91  \\
                    AlexNet-HaS-conv1-11            &  \textbf{58.33}  & \textbf{37.38}  \\

                    \hline
                     \end{tabular}
                            \caption{Applying Hide-and-Seek to the first conv layer. The improvement over~\cite{zhou-cvpr2016} shows the generality of the idea.}
                            \label{table:conv_results}
                            \end{center}
                            \vspace*{-0.15in}
                            \end{table}


\vspace{-10pt}
\paragraph{Hide-and-Seek in convolutional layers.} We next apply our idea to convolutional layers.  We divide the convolutional feature maps into a grid and hide each patch (and all of its corresponding channels) with 0.5 probability.  We hide patches of size 5 (AlexNet-HaS-conv1-5) and 11 (AlexNet-HaS-conv1-11) in the conv1 feature map (which has size ).  Table~\ref{table:conv_results} shows that this leads to a big boost in performance compared to the baseline AlexNet-GAP. This shows that our idea of randomly hiding patches can be generalized to the convolutional layers. 





\begin{table}[t!]
              \begin{center}
                  \footnotesize
                  \begin{tabular}{| c | c | c|}
                  \hline    	
                  Methods & GT-known Loc &  Top-1 Loc\\
                  \hline

                  AlexNet-HaS-25\%            & 57.49 & 37.77  \\
                  AlexNet-HaS-33\%          & 58.12 & 38.05  \\
                  AlexNet-HaS-50\%            & 58.43 & 37.34  \\
                  AlexNet-HaS-66\%            &  58.52  & 35.72  \\
                  AlexNet-HaS-75\%             &  58.28  & 34.21  \\

                  \hline
                   \end{tabular}
                          \caption{Varying the hiding probability. Higher probabilities lead to decrease in \emph{Top-1 Loc} whereas lower probability leads to smaller \emph{GT-known Loc}. For this experiment, we use patch size 56.}
                          \label{table:drop_percent_results}
                          \end{center}
                          \vspace*{-0.13in}
                          \end{table}


\vspace{-10pt}
\paragraph{Probability of hiding.} In all of the previous experiments, we hid patches with 50\% probability. In Table~\ref{table:drop_percent_results}, we measure the \emph{GT-known Loc} and \emph{Top-1 Loc} when we use different hiding probabilities.  If we increase the probability then \emph{GT-known Loc} remains almost the same while \emph{Top-1 Loc} decreases a lot. This happens because the network sees fewer pixels when the hiding probability is high; as a result, classification accuracy reduces and \emph{Top-1 Loc} drops.  If we decrease the probability then \emph{GT-known Loc} decreases but our \emph{Top-1 Loc} improves.  In this case, the network sees more pixels so its classification improves but since less parts are hidden, it will focus more on only the discriminative parts decreasing its localization ability.


\begin{table}[t!]
              \begin{center}
                  \footnotesize
                  \begin{tabular}{| c | c | c| c| c| c|}
                  \hline    	
	                Methods     & IOU thresh = 0.1 & 0.2 & 0.3 & 0.4 & 0.5 \\
	                \hline
					Video-full & 34.23 &   25.68 &   17.72   & 11.00 &   6.11\\
					Video-HaS & \textbf{36.44}   & \textbf{27.84} &   \textbf{19.49} &   \textbf{12.66}  &  \textbf{6.84}\\
					 \hline
					                   \end{tabular}

      \caption{Action localization accuracy on THUMOS validation data.   Across all 5 IoU thresholds, our Video-HaS outperforms the full video baseline (Video-full).}
                          \label{table:frame_hide}
                          \end{center}
                          \vspace*{-0.2in}
                          \end{table}
                          					
\subsection{Action localization results}

Finally, we evaluate action localization accuracy.  We compare our approach (Video-HaS), which randomly hides frame segments while learning action classification, with a baseline that sees the full video (Video-full).  Table~\ref{table:frame_hide} shows the result on THUMOS validation data.  Video-HaS consistently outperforms Video-full for action localization task, which shows that hiding frames forces our network to focus on more relevant frames, which ultimately leads to better action localization.  \emph{We show qualitative results in the supp.}

