\section{Experiments}






We test our model on two fine-grained video datasets, FineAction \cite{liu2021fineaction} and FineGym \cite{shao2020finegym}. In this section, we first introduce datasets and evaluation metrics. We also present implementation details. Then we discuss the main experiment results, followed by ablation studies on each of our components. In addition, we analyze the proposed visual concept learning with some qualitative results. 

\subsection{Datasets and Evaluation Metrics}

\textbf{FineAction}~\cite{liu2021fineaction} combines three existing datasets, YouTube8M~\cite{abu2016youtube}, Kinetics400~\cite{carreira2017quo}, FCVID~\cite{jiangfcvid}, and adds more videos crawled from the Internet. It has 8,440 videos with 57,752 action segments in the training set and 4,174 videos with 24,236 action segments in the validation set. The dataset contains a wide range of video contents including sports, household activities, personal care, etc. FineAction contains a three-level coarse-to-fine label hierarchy in its annotations. The three label levels contain 4, 14, 106 categories respectively and temporal annotations are available for every action label. The results reported by \cite{liu2021fineaction} is trained on the training set and tested on the validation set, so we follow the same setup. In addition to the 106-category fine-grained labels, we use the middle-level labels (14 classes) as the coarse-level labels in HAAN model.

For fair comparison in this dataset, we use the same evaluation metrics as in the FineAction dataset paper~\cite{liu2021fineaction}. Specifically, mean average precision (mAP) at temporal IoU thresholds from 0.5 to 0.95 with an interval of 0.05 is reported, as well as mAP@{0.5, 0.75, 0.95}.

\textbf{FineGym} \cite{shao2020finegym} is a dataset of gymnastics videos with three levels of annotations: ,  and . An  is a program routine of a player performing a whole set of actions. Within each , actions are temporally annotated. Each action has two levels of labels,  (coarse) and  (fine). Two types of action detection can be done in FineGym:  detection within a video, and  detection within an . The latter one is a fine-grained action detection task, thus we use it to conduct our experiments. Our experiments use FineGym-99, which has 14 coarse labels and 99 fine labels. 


We find that the data split introduced in the original FineGym paper \cite{shao2020finegym} is not suitable for action detection experiments, because under that data split, actions in the training set and the validation set can come from the same video. As a result, 18\% of the validation videos are actually seen by the model during training. To avoid the data leakage which can lead to unrealistically high results during testing, we propose a new train-val data split in FineGym using an iterative sampling approach with corresponding ratio control.

The training-to-validation sample ratio in the original split of FineGym is 75 to 25. In the iterative sampling approach, our goal is to also select 75\% of the total samples as training samples, and at the same time maintain the 75-to-25 ratio in each action class as much as possible. Since each video in FineGym might contain more than one action class, we follow the greedy search idea to generate the new data split. We first add a minimum number of videos to both training and validation sets so that both of them contain all action classes. 
Then we find the action class with the least per-class sample ratio in the training set, and sample a video containing that class into the training set. 
We repeat this process until each action class in the training set contains at least 75\% samples in that class. We run the sampling algorithm for 100 times, and pick the resulting data split whose sample distribution is the closest to our criterion with the approximate 75-to-25 ratios in both overall and per-class sample distribution. The new data split contains 3,775 videos with 26,866 action segments in the training set, and 1,193 videos with 7,975 action segments in the validation set. The training-to-validation sample ratios of each action class range from 75\% to 81\%.


Average mAP at temporal IoU thresholds from 0.1 to 0.5 with an interval of 0.05 is reported in this data split, as well as mAP@{0.1, 0.2, 0.3, 0.4, 0.5}.















\begin{table}[t]
  \setlength{\tabcolsep}{6pt}
  \centering
  \caption{Results on FineAction dataset. The avg.mAP refers to the average of mAPs at temporal IoU thresholds ranging from 0.5 to 0.95 with an interval of 0.05. The fully-supervised BMN model~\cite{lin2019bmn}'s results are from \cite{liu2021fineaction}. We run previous weakly-supervised models \cite{paul2018w,pardo2021refineloc,lee2021weakly,ma2021weakly,narayan2021d2} using publicly available code. Ablation studies of our HAAN model with different losses enabled are also presented}
  \begin{tabular}{@{}l|l|ccc|c@{}}
    \hline
    \multicolumn{2}{@{}l|}{\multirow{2}{*}{Methods}} & \multicolumn{3}{c|}{mAP@  } & \multirow{2}{*}{avg.mAP} \\
    \multicolumn{2}{@{}l|}{} & 0.5 & 0.75 & 0.95 &\\
    \hline
    \multirow{5}{*}{Prior} & BMN (fully-supervised) (2019)
    \cite{lin2019bmn} & 14.44 & 8.92 & 3.12 & 9.25\\
    \cline{2-6}
     & W-TALC (2018) \cite{paul2018w}  & 6.18 & 3.15& 0.83 & 3.45\\
     & RefineLoc (2021) \cite{pardo2021refineloc} & 5.93 &2.55 & 0.96& 3.02\\
     & WTAL-UM (2021) \cite{lee2021weakly}  & 6.65 &3.23 &0.95 &3.64 \\
     & ASL (2021) \cite{ma2021weakly}  & 6.79 & 2.68 & 0.81 & 3.30\\
     & D2-Net (2021) \cite{narayan2021d2} & 6.75 & 3.02 & 0.82 & 3.35\\
    \hline
    \multirow{4}{*}{Ours} &  & 5.74 & 2.29 & 0.39 & 2.72 \\
    & + & 6.40 & 2.73 & 0.61 & 3.18 \\
    & ++ & 6.57 & 3.27 & 0.83 & 3.57 \\
    & +++ & \textbf{7.05} & \textbf{3.95} & \textbf{1.14} & \textbf{4.10} \\
    \hline
  \end{tabular}
  
  \label{tab:results:fineaction}
\end{table}

\begin{table}[t]
\setlength{\tabcolsep}{3pt}
  \centering
  \caption{Results on FineGym dataset. The fully-supervised SSN model's results are reported on the new data split. We also run previous weakly-supervised models~\cite{paul2018w,pardo2021refineloc,lee2021weakly,ma2021weakly,narayan2021d2} using publicly available code. Ablation results of our HAAN model with different losses enabled are also presented}
  \begin{tabular}{@{}l|l|ccccc|c@{}}
    \hline
    \multicolumn{2}{@{}l|}{\multirow{2}{*}{Methods}} & \multicolumn{5}{c|}{mAP@  } & \multirow{2}{*}{avg.mAP} \\
    \multicolumn{2}{@{}l|}{} & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 &\\
    \hline
    \multirow{5}{*}{Prior} & SSN (fully-supervised) (2017) \cite{zhao2017temporal} & 20.23 & 19.29&17.12 &14.38  & 11.45&16.61\\
    \cline{2-8}
    & W-TALC (2018) \cite{paul2018w} & 8.85 & 7.32 & 6.24 &4.95 &3.15 &6.03\\
    & RefineLoc (2021) \cite{pardo2021refineloc} & 6.67 &4.63 &4.15 &3.86 & 3.72& 4.54\\
    & WTAL-UM (2021) \cite{lee2021weakly} & 9.45 &8.63 &5.10 &4.34 &3.05 & 6.11\\
    & ASL (2021) \cite{ma2021weakly} & 9.33 & 7.92 & 5.45& 3.67&2.24 & 5.74\\
    & D2-Net (2021) \cite{narayan2021d2} & 9.46 & 8.67 & 5.21 & 4.22 & 2.65 & 6.04\\
    \hline
    \multirow{4}{*}{Ours} &  &7.78 & 7.23& 5.92& 4.12& 2.98 & 5.61\\
    & + & 8.56& 7.50& 5.83& 4.60& 3.45 & 6.09 \\
    & ++ & 9.36  & 8.28 & 6.99 & 5.25 & 3.91 & 6.95 \\
    & +++ & \textbf{10.79} & \textbf{9.62} & \textbf{7.65} & \textbf{6.16} & \textbf{4.16} & \textbf{7.67} \\
    \hline
  \end{tabular}
  
  \label{tab:results:finegym}
\end{table}



\begin{table}[t]
  \setlength{\tabcolsep}{6pt}
  \centering
  \caption{mAP@0.5 results for different clustering methods on FineAction dataset using the HAAN model version with + supervision}
  \begin{tabular}{c|cccc}
    \hline
     \multirow{2}{*}{Methods}& \multicolumn{4}{c}{Number of Clusters} \\
     & 250 & 500 & 750 & 1000 \\
    \hline
     GMM~\cite{reynolds2009gaussian} & 6.15 & 6.14 & 6.20 & 6.36\\
     Birch~\cite{zhang1996birch} & 6.25 & 6.31 & 6.19 & 6.31\\
     K-means \cite{macqueen1967classification} & 6.31 & 6.40 & 6.25 & 6.26\\
    \hline
  \end{tabular}
  
  \label{tab:results:clustering}
\end{table}


\begin{table}[t]
  \setlength{\tabcolsep}{6pt}
  \centering
  \caption{Results of different distance calculations on FineAction dataset using the HAAN model version with ++ supervision}
  \begin{tabular}{c|ccc|c}
    \hline
   Method & mAP@0.5 & mAP@0.75 & mAP@0.95 & avg.mAP\\
    \hline
     Euclidean & 5.92 & 3.03 & 0.81 & 3.26\\
     cosine & 6.57 & 3.27 & 0.83 & 3.57\\
    \hline
  \end{tabular}
  
  \label{tab:results:mse}
\end{table}



\subsection{Implementation Details}
Our model is implemented in PyTorch. We use two fully connected layers on top of the two-stream Inception3D (I3D) backbone~\cite{carreira2017quo} with RGB and Optical Flow inputs to form the feature encoder , following the standard practice in weakly-supervised temporal action detection, e.g.~\cite{nguyen2018weakly}. We use one fully connected layer for MIL classifier , and two fully connected layers for self-supervised visual concept classifier . The number of visual concepts  is set as , and each class retains top  relevant visual concepts. Fine-to-coarse composition function  is max pooling for FineAction and mean pooling for FineGym. We use Adam optimizer with learning rate . The weights for each loss are: . The  in \cref{eq:detect} is  for FineAction and  for FineGym.


\subsection{Main Results}
We compare our HAAN model to previous weakly-supervised temporal action detection models~\cite{paul2018w,pardo2021refineloc,ma2021weakly,lee2021weakly,narayan2021d2}, and results on the two datasets are shown in~\cref{tab:results:fineaction,tab:results:finegym}. Results of fully-supervised models (BMN or SSN) are also listed as reference. In FineGym dataset, we re-run the fully-supervised algorithm SSN \cite{zhao2017temporal} on the new data split, and the new results shown in~\cref{tab:results:finegym} are worse than the results reported in FineGym paper~\cite{shao2020finegym} as expected, because the original data split has train-test video data leakage problem.
Overall, fully-supervised models also struggle on both datasets with low performance, demonstrating the difficulty of temporal action detection on fine-grained datasets.




Our HAAN model outperforms the best weakly-supervised detection model WTAL-UM \cite{lee2021weakly} by 12\% on FineAction and 25\% on FineGym in average mAP relatively. We also find that more advanced weakly models ASL \cite{ma2021weakly} and RefineLoc \cite{pardo2021refineloc} are not superior to the older weakly model W-TALC \cite{paul2018w} in fine-grained videos. This again indicates the big difference between fine-grained action datasets and general action datasets. Models that perform well on THUMOS-14 and ActivityNet-1.2 may not perform well on fine-grained video datasets.

Results in \cref{tab:results:fineaction} are reported on the validation set of FineAction. We also submit our HAAN model's predictions on the withheld test set to the FineAction competition leaderboard\footnote{\url{https://competitions.codalab.org/competitions/32363}}, and get the average mAP of 4.48 on the test set with hidden labels, which is close to the average mAP of 4.10 on the validation set.


For ablation study, we also include four different versions of our model with different losses enabled in \cref{tab:results:fineaction,tab:results:finegym}. It shows that each component in our HAAN model has a considerable contribution to the detection performance.


\subsection{Visual Concept Learning Analysis}
\label{sec:vc-learning}
We use K-means clustering to obtain visual concepts, and then build connections with fine-grained action classes by minimizing the cosine distance between visual concepts and the MIL classifier's weights. Here we show the ablation results of different clustering methods and distance calculations in visual concept learning. 


For the clustering method to generate pseudo labels, we explore Gaussian Mixture Model (GMM)~\cite{reynolds2009gaussian} and Birch~\cite{zhang1996birch} in addition to K-means, and test on different numbers of clusters. 
The HAAN model version with + supervision is used in this ablation to better reflect the effect of different clustering methods. The ablated mAP@0.5 results on FineAction dataset are shown in~\cref{tab:results:clustering}. Among all the clustering methods, K-means with 500 clusters achieves the best result.




To build connection between visual concepts and fine-grained action classes, distance metric is used, e.g. in the mapping function  (\cref{eq:distance}) between visual concepts and fine-grained actions, and the auxiliary loss function for fine-grained action representation learning based on visual concepts (\cref{eq:l_concept}). We experiment with Euclidean distance and cosine similarity in the HAAN model version with ++ supervision, and ablation results are shown in \cref{tab:results:mse}. Cosine similarity outperforms Euclidean distance generally in our model.






\subsection{Qualitative Results}




\begin{figure}[t]
  \centering
   \includegraphics[width=0.4\linewidth]{figures/visualization-1.jpg}
   \caption{Visualization of relevant visual concepts for each fine-grained action class in the \textit{Uneven Bars} event from FineGym dataset}
   \label{fig:concepts-actions}
\end{figure}

\begin{figure*}[t]
  \centering
   \includegraphics[width=0.95\linewidth]{figures/visualization-3.jpg}
   \caption{Three example visual concepts learned in our model representing atomic actions}
   \label{fig:concepts-example}
\end{figure*}


\begin{figure*}[t]
  \centering
   \includegraphics[width=0.95\linewidth]{figures/visualization-2.png}
   \caption{Two example action detections from our HAAN model. Visual concepts A and B correspond to those visualized in \cref{fig:concepts-example}}
   \label{fig:detection}
\end{figure*}


Our model assumes that fine-grained actions share common visual concepts (a.k.a. atomic actions) and vary in certain visual concepts. \cref{fig:concepts-actions} shows the visual concepts that each fine-grained action class is most related to in the \textit{Uneven Bars} event from FineGym dataset. The appearance pattern of visual concepts indeed demonstrates that there exists visual concepts that are shared by actions, and every action can be represented by a unique combination of visual concepts.






To better understand what the visual concepts are, we plot some examples of visual concepts in \cref{fig:concepts-example}, and the specific atomic action label below each visual concept (Visual concept A, B, C) is generated from our observation for illustration purpose. Each visual concept represents an atomic action. Visual concept A represents atomic action \textit{stalder backward to handstand} where the athlete starts from handstand phase, circles around the bar with legs wide apart, and moves backward to handstand. Visual concept B represents atomic action \textit{0.5 turn} on handstand. Visual concept C represents \textit{clear pike circle backward to handstand} with two legs together. Visual concepts A and C differ in whether the athlete's legs are wide apart or together in the circle.
Also, visual concept B can occur following visual concept A to form action \textit{stalder backward with 0.5/1 turn to handstand}, or following visual concept C to form action \textit{clear pike circle backward with 0.5/1 turn to handstand}.





Our model also learns to compose visual concepts into fine-grained actions. As shown in \cref{fig:detection}, our model successfully detects the sequence of visual concept A followed by two visual concepts B as fine-grained action~\textit{stalder backward with 1 turn to handstand}, and the sequence of only visual concepts A as fine-grained action~\textit{stalder backward to handstand}.




