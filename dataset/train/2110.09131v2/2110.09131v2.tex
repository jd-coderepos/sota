\begin{thebibliography}{42}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et al(2015)]{tensorflow2015-whitepaper}
M.~Abadi {\emph{et al}}.
\newblock {TensorFlow}: Large-scale machine learning on heterogeneous systems,
  2015.
\newblock URL \url{https://www.tensorflow.org/}.
\newblock Software available from tensorflow.org.

\bibitem[Bertsekas(2011)]{Bertsekas2011}
D.P. Bertsekas.
\newblock Incremental proximal methods for large scale convex optimization.
\newblock \emph{Math. Program.}, 129\penalty0 (2):\penalty0 163--195, 2011.

\bibitem[Bottou(2009)]{bottou2009curiously}
L.~Bottou.
\newblock Curiously fast convergence of some stochastic gradient descent
  algorithms.
\newblock 2009.

\bibitem[Bottou(2012)]{bottou2012stochastic}
L.~Bottou.
\newblock Stochastic gradient descent tricks.
\newblock In \emph{Neural networks: Tricks of the trade}, pages 421--436.
  Springer, 2012.

\bibitem[Bottou et~al.(2018)Bottou, Curtis, and Nocedal]{Bottou2018}
L.~Bottou, F.~E. Curtis, and J.~Nocedal.
\newblock {O}ptimization {M}ethods for {L}arge-{S}cale {M}achine {L}earning.
\newblock \emph{SIAM Rev.}, 60\penalty0 (2):\penalty0 223--311, 2018.

\bibitem[Chang and Lin(2011)]{LIBSVM}
Chih-Chung Chang and Chih-Jen Lin.
\newblock {LIBSVM}: A library for support vector machines.
\newblock \emph{ACM Transactions on Intelligent Systems and Technology},
  2:\penalty0 27:1--27:27, 2011.
\newblock Software available at \url{http://www.csie.ntu.edu.tw/~cjlin/libsvm}.

\bibitem[Chen et~al.(2018)Chen, Liu, Sun, and Hong]{chen2018convergence}
X.~Chen, S.~Liu, R.~Sun, and M.~Hong.
\newblock On the convergence of a class of adam-type algorithms for non-convex
  optimization.
\newblock \emph{ICLR}, 2018.

\bibitem[Defazio et~al.(2014)Defazio, Bach, and Lacoste-Julien]{SAGA}
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien.
\newblock Saga: A fast incremental gradient method with support for
  non-strongly convex composite objectives.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1646--1654, 2014.

\bibitem[Dozat(2016)]{dozat2016incorporating}
T.~Dozat.
\newblock Incorporating nesterov momentum into {ADAM}.
\newblock \emph{ICLRWorkshop}, 1:\penalty0 2013â€“--2016, 2016.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{AdaGrad}
John Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 12:\penalty0 2121--2159,
  2011.

\bibitem[Ghadimi and Lan(2013)]{ghadimi2013stochastic}
S.~Ghadimi and G.~Lan.
\newblock Stochastic first-and zeroth-order methods for nonconvex stochastic
  programming.
\newblock \emph{SIAM J. Optim.}, 23\penalty0 (4):\penalty0 2341--2368, 2013.

\bibitem[G{\"u}rb{\"u}zbalaban et~al.(2015)G{\"u}rb{\"u}zbalaban, Ozdaglar, and
  Parrilo]{gurbuzbalaban2015random}
Mert G{\"u}rb{\"u}zbalaban, Asu Ozdaglar, and Pablo Parrilo.
\newblock Why random reshuffling beats stochastic gradient descent.
\newblock \emph{arXiv preprint arXiv:1510.08560}, 2015.

\bibitem[HaoChen and Sra(2018)]{haochen2018random}
Jeffery~Z HaoChen and Suvrit Sra.
\newblock Random shuffling beats sgd after finite epochs.
\newblock \emph{arXiv preprint arXiv:1806.10077}, 2018.

\bibitem[Johnson and Zhang(2013)]{SVRG}
Rie Johnson and Tong Zhang.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In \emph{NIPS}, pages 315--323, 2013.

\bibitem[Kingma and Ba(2014)]{Kingma2014}
D.~P. Kingma and J.~Ba.
\newblock {ADAM}: {A} {M}ethod for {S}tochastic {O}ptimization.
\newblock \emph{Proceedings of the 3rd International Conference on Learning
  Representations (ICLR)}, abs/1412.6980, 2014.

\bibitem[Krizhevsky and Hinton(2009)]{CIFAR10}
Alex Krizhevsky and Geoffrey Hinton.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, Citeseer, 2009.

\bibitem[Le~Roux et~al.(2012)Le~Roux, Schmidt, and Bach]{SAG}
Nicolas Le~Roux, Mark Schmidt, and Francis Bach.
\newblock A stochastic gradient method with an exponential convergence rate for
  finite training sets.
\newblock In \emph{NIPS}, pages 2663--2671, 2012.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and Haffner]{MNIST}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[Li et~al.(2020)Li, Zhuang, and Orabona]{li2020exponential}
L.~Li, Z.~Zhuang, and F.~Orabona.
\newblock Exponential step sizes for non-convex optimization.
\newblock \emph{arXiv preprint arXiv:2002.05273}, 2020.

\bibitem[Li et~al.(2019)Li, Zhu, So, and Lee]{li2019incremental}
X.~Li, Z.~Zhu, A.~So, and J.~D. Lee.
\newblock Incremental methods for weakly convex optimization.
\newblock \emph{arXiv preprint arXiv:1907.11687}, 2019.

\bibitem[Loshchilov and Hutter(2017)]{loshchilov10sgdr}
I.~Loshchilov and F.~Hutter.
\newblock {SGDR}: {S}tochastic gradient descent with warm restarts.
\newblock 10:\penalty0 1--16, 2017.

\bibitem[Loshchilov and Hutter(2016)]{loshchilov2016sgdr}
Ilya Loshchilov and Frank Hutter.
\newblock Sgdr: Stochastic gradient descent with warm restarts, 2016.

\bibitem[Meng et~al.(2019)Meng, Chen, Wang, Ma, and Liu]{meng2019convergence}
Q.~Meng, W.~Chen, Y.~Wang, Z.-M. Ma, and T.-Y. Liu.
\newblock Convergence analysis of distributed stochastic gradient descent with
  shuffling.
\newblock \emph{Neurocomputing}, 337:\penalty0 46--57, 2019.

\bibitem[Mishchenko et~al.(2020)Mishchenko, Khaled, and
  Richt{\'a}rik]{mishchenko2020random}
K.~Mishchenko, A.~Khaled, and P.~Richt{\'a}rik.
\newblock Random reshuffling: {S}imple analysis with vast improvements.
\newblock \emph{arXiv preprint arXiv:2006.05988}, 2020.

\bibitem[Nagaraj et~al.(2019)Nagaraj, Jain, and Netrapalli]{nagaraj2019sgd}
Dheeraj Nagaraj, Prateek Jain, and Praneeth Netrapalli.
\newblock Sgd without replacement: Sharper rates for general smooth convex
  functions.
\newblock In \emph{International Conference on Machine Learning}, pages
  4703--4711, 2019.

\bibitem[Nedi{\'c} and Bertsekas(2001)]{nedic2001convergence}
A.~Nedi{\'c} and D.~Bertsekas.
\newblock Convergence rate of incremental subgradient algorithms.
\newblock In \emph{Stochastic optimization: algorithms and applications}, pages
  223--264. Springer, 2001.

\bibitem[Nedic and Bertsekas(2001)]{nedic2001incremental}
A.~Nedic and D.~P. Bertsekas.
\newblock Incremental subgradient methods for nondifferentiable optimization.
\newblock \emph{SIAM J. on Optim.}, 12\penalty0 (1):\penalty0 109--138, 2001.

\bibitem[Nesterov(1983)]{Nesterov1983}
Y.~Nesterov.
\newblock A method for unconstrained convex minimization problem with the rate
  of convergence .
\newblock \emph{Doklady AN SSSR}, 269:\penalty0 543--547, 1983.
\newblock Translated as Soviet Math. Dokl.

\bibitem[Nesterov(2004)]{Nesterov2004}
Y.~Nesterov.
\newblock \emph{{I}ntroductory lectures on convex optimization: {A} basic
  course}, volume~87 of \emph{Applied Optimization}.
\newblock Kluwer Academic Publishers, 2004.

\bibitem[Nguyen et~al.(2017)Nguyen, Liu, Scheinberg, and
  Tak{\'a}{\v{c}}]{Nguyen2017sarah}
L.~M. Nguyen, J.~Liu, K.~Scheinberg, and M.~Tak{\'a}{\v{c}}.
\newblock {SARAH}: {A} novel method for machine learning problems using
  stochastic recursive gradient.
\newblock \emph{ICML}, 2017.

\bibitem[Nguyen et~al.(2020)Nguyen, Tran-Dinh, Phan, Nguyen, and van
  Dijk]{nguyen2020unified}
L.~M. Nguyen, Q.~Tran-Dinh, D.~T. Phan, P.~H. Nguyen, and M.~van Dijk.
\newblock A unified convergence analysis for shuffling-type gradient methods.
\newblock \emph{arXiv preprint arXiv:2002.08246}, 2020.

\bibitem[Nguyen et~al.(2018)Nguyen, Nguyen, van Dijk, Richtarik, Scheinberg,
  and Takac]{Nguyen2018_sgdhogwild}
Lam Nguyen, Phuong~Ha Nguyen, Marten van Dijk, Peter Richtarik, Katya
  Scheinberg, and Martin Takac.
\newblock {SGD} and {H}ogwild! convergence without the bounded gradients
  assumption.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning-Volume 80}, pages 3747--3755, 2018.
  
\bibitem[Nguyen et~al.(2019{\natexlab{a}})Nguyen, Nguyen, Richt{{\'a}}rik,
  Scheinberg, Tak{{\'a}}{\v{c}}, and van Dijk]{nguyen2019newaspect}
Lam~M. Nguyen, Phuong~Ha Nguyen, Peter Richt{{\'a}}rik, Katya Scheinberg,
  Martin Tak{{\'a}}{\v{c}}, and Marten van Dijk.
\newblock New convergence aspects of stochastic gradient algorithms.
\newblock \emph{Journal of Machine Learning Research}, 20\penalty0
  (176):\penalty0 1--49, 2019{\natexlab{a}}.

\bibitem[Paszke et al (2019)]{pytorch}
A.~Paszke {\emph{et al}}.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In \emph{Advances in Neural Information Processing Systems 32}, pages
  8024--8035. Curran Associates, Inc., 2019.

\bibitem[Pham et~al.(2020)Pham, Nguyen, Phan, and Tran-Dinh]{Pham2019}
N.~H. Pham, L.~M. Nguyen, D.~T. Phan, and Q.~Tran-Dinh.
\newblock {ProxSARAH}: {A}n efficient algorithmic framework for stochastic
  composite nonconvex optimization.
\newblock \emph{J. Mach. Learn. Res.}, 2020.

\bibitem[Polyak(1964)]{polyak1964some}
B.~T. Polyak.
\newblock Some methods of speeding up the convergence of iteration methods.
\newblock \emph{USSR Computational Mathematics and Mathematical Physics},
  4\penalty0 (5):\penalty0 1--17, 1964.

\bibitem[Rajput et~al.(2020)Rajput, Gupta, and
  Papailiopoulos]{rajput2020closing}
S.~Rajput, A.~Gupta, and D.~Papailiopoulos.
\newblock Closing the convergence gap of {SGD} without replacement.
\newblock \emph{arXiv preprint arXiv:2002.10400}, 2020.

\bibitem[Reddi et~al.(2016)Reddi, Hefny, Sra, P{\'{o}}czos, and
  Smola]{Reddi2016a}
Sashank~J. Reddi, Ahmed Hefny, Suvrit Sra, Barnab{\'{a}}s P{\'{o}}czos, and
  Alexander~J. Smola.
\newblock Stochastic variance reduction for nonconvex optimization.
\newblock In \emph{ICML}, pages 314--323, 2016.

\bibitem[Robbins and Monro(1951)]{RM1951}
Herbert Robbins and Sutton Monro.
\newblock A stochastic approximation method.
\newblock \emph{The Annals of Mathematical Statistics}, 22\penalty0
  (3):\penalty0 400--407, 1951.

\bibitem[Ruder(2017)]{ruder2017overview}
Sebastian Ruder.
\newblock An overview of gradient descent optimization algorithms, 2017.

\bibitem[Safran and Shamir(2018)]{Safran2019HowGoodSGDShuffling}
I.~Safran and O.~Shamir.
\newblock How good is {SGD} with random shuffling?
\newblock \emph{arXiv preprint arXiv:1908.00045}, 2018.

\bibitem[Shamir(2016)]{shamir2016without}
O.~Shamir.
\newblock Without-replacement sampling for stochastic gradient methods.
\newblock In \emph{Advances in neural information processing systems}, pages
  46--54, 2016.

\bibitem[Smith(2017)]{smith2017cyclical}
L.~N. Smith.
\newblock Cyclical learning rates for training neural networks.
\newblock In \emph{2017 IEEE Winter Conference on Applications of Computer
  Vision (WACV)}, pages 464--472. IEEE, 2017.

\bibitem[Smith(2015)]{smith2015cyclical}
Leslie~N. Smith.
\newblock Cyclical learning rates for training neural networks, 2015.

\bibitem[Tran-Dinh et~al.(2019)Tran-Dinh, Pham, Phan, and
  Nguyen]{Tran-Dinh2019a}
Q.~Tran-Dinh, N.~H. Pham, D.~T. Phan, and L.~M. Nguyen.
\newblock A hybrid stochastic optimization framework for stochastic composite
  nonconvex optimization.
\newblock \emph{Preprint: UNC-STOR 07.10.2019}, 2019.

\bibitem[Wang et~al.(2020)Wang, Nguyen, Bertozzi, Baraniuk, and
  Osher]{wang2020scheduled}
B.~Wang, T.~M. Nguyen, A.~L. Bertozzi, R.~G. Baraniuk, and S.~J. Osher.
\newblock Scheduled restart momentum for accelerated stochastic gradient
  descent.
\newblock \emph{arXiv preprint arXiv:2002.10583}, 2020.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{xiao2017fashion}
Han Xiao, Kashif Rasul, and Roland Vollgraf.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine
  learning algorithms.
\newblock \emph{arXiv preprint arXiv:1708.07747}, 2017.

\bibitem[Ying et~al.(2017)Ying, Yuan, and Sayed]{ying2017convergence}
B.~Ying, K.~Yuan, and A.~H. Sayed.
\newblock Convergence of variance-reduced stochastic learning under random
  reshuffling.
\newblock \emph{arXiv preprint arXiv:1708.01383}, 2\penalty0 (3):\penalty0 6,
  2017.

\end{thebibliography}
