\documentclass[11pt]{article}
\usepackage{verbatim}

\newtheorem{define}{Definition}
\usepackage{amsmath}

\oddsidemargin=0.15in
\evensidemargin=0.15in
\topmargin=-.5in
\textheight=9in
\textwidth=6.25in
\parindent=3ex
\setlength{\parskip}{3pt}

\usepackage[noend]{algorithm2e}
\incmargin{1em}
\restylealgo{unboxed}
\linesnumbered
\dontprintsemicolon
\SetArgSty{rm}
\SetTitleSty{textsc}{11pt}
\SetKwComment{Comment}{ // }{}

\usepackage{amssymb}
\newcommand{\textbi}[1]{\textbf{\textit{#1}}}
\newcommand{\la}{\leftarrow}
\newcommand{\ra}{\rightarrow}
\newcommand{\NL}{\textrm{NL}}
\renewcommand{\L}{\textrm{L}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Obj}{\textrm{Obj}}
\newcommand{\eps}{\varepsilon}
\newcommand{\La}{\Leftarrow}
\newcommand{\diam}{\textrm{\rm diam}}
\newcommand{\st}{\textrm{ s.t. }}
\newcommand{\tO}{\tilde{O}}
\renewcommand{\bar}[1]{\overline{#1}}

\newcommand{\mnote}[1]{}

\title{Sublinear Time Algorithms for Earth Mover's Distance}
\author{Khanh Do Ba\\MIT, CSAIL\\doba@mit.edu \and 
Huy L. Nguyen\\MIT\\hlnguyen@mit.edu \and
Huy N. Nguyen\\MIT, CSAIL\\huy2n@mit.edu \and
Ronitt Rubinfeld\\MIT, CSAIL\\ronitt@csail.mit.edu}

\begin{document}


\hbadness=10000
\vbadness=10000

\setlength{\oddsidemargin}{.25in}
\setlength{\evensidemargin}{.25in}
\setlength{\textwidth}{6in}
\setlength{\topmargin}{-0.4in}
\setlength{\textheight}{8.5in}

\newcommand{\handout}[5]{
\noindent
   \begin{center}
   \framebox{
      \vbox{
    \hbox to 5.78in { {\bf 6.896 Sublinear Time Algorithms}
     	 \hfill #2 }
       \vspace{4mm}
       \hbox to 5.78in { {\Large \hfill #5  \hfill} }
       \vspace{2mm}
       \hbox to 5.78in { {\it #3 \hfill #4} }
      }
   }
   \end{center}
   \vspace*{4mm}
}

\newcommand{\lecture}[4]{\handout{#1}{#2}{Lecturer:
#3}{Scribe: #4}{Lecture #1}}


\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{assumption}[theorem]{Assumption}

\newcommand{\qed}{\rule{7pt}{7pt}}
\newcommand{\dis}{\mathop{\mbox{\rm d}}\nolimits}
\newcommand{\per}{\mathop{\mbox{\rm per}}\nolimits}
\newcommand{\area}{\mathop{\mbox{\rm area}}\nolimits}
\newcommand{\cw}{\mathop{\rm cw}\nolimits}
\newcommand{\ccw}{\mathop{\rm ccw}\nolimits}
\newcommand{\DIST}{\mathop{\mbox{\rm DIST}}\nolimits}
\newcommand{\OP}{\mathop{\mbox{\it OP}}\nolimits}
\newcommand{\OPprime}{\mathop{\mbox{\it OP}^{\,\prime}}\nolimits}
\newcommand{\ihat}{\hat{\imath}}
\newcommand{\jhat}{\hat{\jmath}}
\newcommand{\abs}[1]{\mathify{\left| #1 \right|}}

\newenvironment{proof}{\noindent{\bf Proof}\hspace*{1em}}{\qed\bigskip}
\newenvironment{proof-sketch}{\noindent{\bf Sketch of Proof}\hspace*{1em}}{\qed\bigskip}
\newenvironment{proof-idea}{\noindent{\bf Proof Idea}\hspace*{1em}}{\qed\bigskip}
\newenvironment{proof-of-lemma}[1]{\noindent{\bf Proof of Lemma #1}\hspace*{1em}}{\qed\bigskip}
\newenvironment{proof-attempt}{\noindent{\bf Proof Attempt}\hspace*{1em}}{\qed\bigskip}
\newenvironment{proofof}[1]{\noindent{\bf Proof}
of #1:\hspace*{1em}}{\qed\bigskip}
\newenvironment{remark}{\noindent{\bf Remark}\hspace*{1em}}{\bigskip}



\newcommand{\FOR}{{\bf for}}
\newcommand{\TO}{{\bf to}}
\newcommand{\DO}{{\bf do}}
\newcommand{\WHILE}{{\bf while}}
\newcommand{\AND}{{\bf and}}
\newcommand{\IF}{{\bf if}}
\newcommand{\THEN}{{\bf then}}
\newcommand{\ELSE}{{\bf else}}



\makeatletter
\def\fnum@figure{{\bf Figure \thefigure}}
\def\fnum@table{{\bf Table \thetable}}
\long\def\@mycaption#1[#2]#3{\addcontentsline{\csname
  ext@#1\endcsname}{#1}{\protect\numberline{\csname 
  the#1\endcsname}{\ignorespaces #2}}\par
  \begingroup
    \@parboxrestore
    \small
    \@makecaption{\csname fnum@#1\endcsname}{\ignorespaces #3}\par
  \endgroup}
\def\mycaption{\refstepcounter\@captype \@dblarg{\@mycaption\@captype}}
\makeatother

\newcommand{\figcaption}[1]{\mycaption[]{#1}}
\newcommand{\tabcaption}[1]{\mycaption[]{#1}}
\newcommand{\head}[1]{\chapter[Lecture \##1]{}}
\newcommand{\mathify}[1]{\ifmmode{#1}\else\mbox{}\fi}
\newcommand{\bigO}O
\newcommand{\set}[1]{\mathify{\left\{ #1 \right\}}}
\def\half{\frac{1}{2}}



\newcommand{\enc}{{\sf Enc}}
\newcommand{\dec}{{\sf Dec}}
\newcommand{\E}{{\rm Exp}}
\newcommand{\Var}{{\rm Var}}
\newcommand{\Z}{{\mathbb Z}}
\newcommand{\F}{{\mathbb F}}
\newcommand{\integers}{{\mathbb Z}^{\geq 0}}
\newcommand{\R}{{\mathbb R}}
\newcommand{\Q}{{\cal Q}}
\newcommand{\eqdef}{{\stackrel{\rm def}{=}}}
\newcommand{\from}{{\leftarrow}}
\newcommand{\vol}{{\rm Vol}}
\newcommand{\poly}{{\rm poly}}
\newcommand{\ip}[1]{{\langle #1 \rangle}}
\newcommand{\wt}{{\rm wt}}
\renewcommand{\vec}[1]{{\mathbf #1}}
\newcommand{\mspan}{{\rm span}}
\newcommand{\rs}{{\rm RS}}
\newcommand{\RM}{{\rm RM}}
\newcommand{\Had}{{\rm Had}}
\newcommand{\calc}{{\cal C}}
\renewcommand{\binom}[2]{{#1 \choose #2}}

\newcommand{\fig}[4]{
        \begin{figure}
        \setlength{\epsfysize}{#2}
        \vspace{3mm}
        \centerline{\epsfbox{#4}}
        \caption{#3} \label{#1}
        \end{figure}
        }

\newcommand{\ord}{{\rm ord}}

\providecommand{\norm}[1]{\lVert #1 \rVert}
\newcommand{\embed}{{\rm Embed}}
\newcommand{\qembed}{\mbox{-Embed}}
\newcommand{\calh}{{\cal H}}
\newcommand{\lp}{{\rm LP}}
 \maketitle

\begin{abstract}
	We study the problem of estimating the Earth Mover's Distance (EMD) between
	probability distributions when given access only to samples. We give closeness
	testers and additive-error estimators over domains in , with
	sample complexities independent of domain size -- permitting the testability
	even of continuous distributions over infinite domains. Instead, our algorithms depend on
	other parameters, such as the diameter of the domain space, which may be
	significantly smaller. We also prove lower bounds showing the dependencies
	on these parameters to be essentially optimal. Additionally, we consider whether
	natural classes of distributions exist for which there are
	algorithms with better dependence on the dimension, and show that for
	highly clusterable data, this is indeed the case. Lastly, we consider a variant
	of the EMD, defined over tree metrics instead of the usual  metric, and
	give optimal algorithms.
\end{abstract}
\section{Introduction}

In traditional algorithmic settings, algorithms requiring linear time and/or space
are generally considered to be highly efficient; sometimes even
polynomial time and space requirements are acceptable. However, today this is often no
longer the case. With data being generated at rates of terabytes a second,
sublinear time algorithms have become crucial in many applications. In the increasingly important
area of massive data algorithmics, a number of models have been proposed and
studied to address this. One of these arises when the data can be naturally viewed
as a probability distribution (e.g., over IP addresses, or items sold by an online
retailer, etc.) that allows i.i.d. samples to be drawn from it. This is the model
on which this paper focuses.

Perhaps the most fundamental problem in this model is that of testing whether two
distributions are close. For instance, if an online retailer such as Amazon.com
wishes to detect changes in consumer habits, one way of doing so might be to see if
the distribution of sales over all offered items this week, say, is significantly
different from last week's distribution. This problem has been studied extensively, mostly
under the  and  distances, and algorithms sublinear in time and
sample complexity exist to distinguish
whether two distributions are identical or -far from each other~\cite{L1tester,GR,valiant}.
However, under the  distance, for instance, the sample complexity, though sublinear, can be no
smaller than  (where  is the domain size), which may be
prohibitively large~\cite{L1tester,valiant}.

Fortunately, in many situations there is a natural metric on the underlying domain,
under which nearby points should be treated as ``less different'' than faraway
points. This motivates a metric known as Earth Mover's Distance (EMD), first
introduced in the vision community as a measure of (dis)similarity between images
that more accurately reflects human perception than more traditional 
~\cite{peleg}. It has since proven to be important in computer graphics and vision
~\cite{rubner2,rubner1,rubner3,cohen,ruzon1,ruzon2,rubner4}, and
has natural applications to other areas of computer science. As a result, its
computational aspects have recently drawn attention from the algorithms community
as well~\cite{alex,indyk,charikar,indyk2}. However, previous work has generally focused
on the more classical model of approximating the EMD when given the input distributions
explicitly; that is, when the exact probability of any domain element can be queried.
As far as we know, no work has been done on estimating and closeness
testing of EMD when given access only to i.i.d. samples of the distributions.

In this model, it is easy to see that we cannot hope to compute a multiplicative approximation,
even with arbitrarily many samples, since that would require us to distinguish between
arbitrarily close distributions and identical ones. However, if we settle for additive error, we
show in this paper that, in contrast to the  distance, we can estimate EMD using a number
of samples {\em independent}
of the domain size. Instead, our sample complexities depend only on the {\em
diameter} of the domain space, which in many cases can be significantly smaller.
The consequence is that this allows us to effectively deal with distributions over
extremely large domains, and even, under a natural generalization of EMD,
continuous distributions over infinite domains.

Specifically, if  and  are distributions over  (where  is a
constant), we can
\begin{itemize}
	\item estimate  to within an additive error of  with
		 samples,
	\item distinguish whether  or  with
		 samples, and
	\item if  is known, distinguish whether  or  with
		 samples.
\end{itemize}
We also give lower bounds that imply these results to be essentially optimal
(up to small  factors). In the case of  or , upper
and lower bounds for both testers become .

Additionally, we consider assumptions on the data that might make the problem
easier, and give an improved algorithm in the case our input distributions is highly
clusterable. Finally, it is natural to consider the EMD over domains endowed with a
metric other than  distance. We give an optimal (upto polylogarithmic factors)
algorithm for estimating EMD over tree metrics.


\section{Preliminaries}

\subsection{Earth Mover's Distance}

We start with the following definition.

\begin{definition}
	A {\em supply-demand network} is a directed bipartite graph 
	consisting of supply vertices  and demand vertices , with supply
	distribution  on  and demand distribution  on , and edge set
	,  with associated weights . A {\em satisfying flow} for  is a
	mapping  such that for each  and each ,
	
	The cost of satisfying flow  is given by
		
\end{definition}

We define the Earth Mover's Distance (EMD) as follows.

\begin{definition}\label{def:flow}
	Let  and  be probability distributions on a finite metric space .
	Then let  be the supply-demand network given by supply vertices
	 and demand vertices , with supply distribution
	 and demand distribution , and
	edge weights . Define  to be the minimum
	cost of all satisfying flows for .
\end{definition}

It is straightforward to verify that the EMD as defined above is a metric on all
probability distributions on . Note, moreover, that to upperbound the EMD it
suffices to exhibit any satisfying flow. 

Since the magnitude of the EMD depends on the distances in the underlying metric
space , we must assume that  is of bounded diameter. In particular, we will in
this paper focus mostly on the case where , for some ,
endowed with the  metric.

Finally, we define a closeness tester for the EMD in the usual way as follows.

\begin{definition}
  Let ,  be two distributions on (finite) metric space . An
  {\em EMD-closeness tester} is an algorithm which takes as input samples from  and ,
  together with a real number , and guarantees that
  \begin{itemize}
    \item[(1)] if , then it accepts with probability at least , and
    \item[(2)] if , then it rejects with probability at least .
  \end{itemize}
\end{definition}

Note that it is also possible to define EMD for any two probability measures  and  in an infinite (continuous) metric space , , by using Wasserstein metric:   where  denotes the collection of all measures on  with marginals  and  on the first and second factors respectively. By using -net,  can be discretized into a finite metric  in such a way that the  distance of any two probability measures only increases or decreases at most . Therefore, an -closeness tester with additive error  for  is also a valid -closeness tester for  with additive error .

\subsection{Properties of EMD}

Let us get a firmer handle on the EMD by relating it to  distance, via the
following lemmas.

\begin{lemma}\label{lem:L1/2}
        If  and  are distributions on , there exists a minimum cost
        satisfying flow  from  to  (as defined in Def. \ref{def:flow}) such that the total
        amount sent by  across edges with non-zero cost is exactly .
\end{lemma}

\begin{proof}
        Let  and , so that
         and . Observe that the total amount sent from  to
         is 1, and the maximum possible amount sent across edges with zero cost is , which
        leaves at least  to be sent across non-zero cost edges.
        
        On the other hand, suppose that for some , an optimal flow through the edge
         is less than .
        Then there exist points  and  such that  sends at least  from 
        to  and from  to . We can replace this partial flow, which costs
        , with one sending  from  to 
        and from  to , which costs . Doing so, we will not
        increase the total cost (by triangle inequality), nor will we affect the rest of the
        flow. We can therefore repeated the procedure until we obtain an optimal flow that
        saturates every zero-edge.
\end{proof}

\begin{corollary}\label{cor:EMDvsL1}
        If  and  are distributions on , where  has minimum
        distance  and diameter , then
        
\end{corollary}

\begin{lemma}\label{lem:buckets}
        Let  and  be distributions on  with diameter , and
         be a partition of  wherein  for
        every . Let  and  be distributions on 
        induced by  and , resp. Then .
\end{lemma}

\begin{proof}
        Let us define distribution  by moving some of the probability weight of 
        {\em between} 's (taking from and depositing anywhere within the respective
        's) in such a way that  induces  on . This is effectively a flow
        from  to  where all distances are bounded by , so by Lemma
        \ref{lem:L1/2} it can be done at cost at most .
        It follows that .
        
        Then, having equalized the probability weights of each , let us move the
        probability weight of  {\em within} each  to precisely match . This
        might require moving everything (i.e., 1), but the distance anything is moved is
        at most , so  and the lemma follows by triangle
        inequality.
\end{proof}

\subsection{Some tools from previous work}

Here, for completeness, we state some results from previous work that we will make use
of later. First, we define some useful distributions described in \cite{indyk}
for testing closeness between distributions over subsets of .

\begin{definition}
Given distribution  over  and a positive integer , let  be a grid with side length  over  centered at the origin. Define the {\em -coarsening of }, denoted , to be the distribution over the grid cells of  such that, for each grid cell c of , .
\end{definition}

The 's can be thought of as coarse approximations of 
where all points in the same grid cell are considered to be the same point.
We then have the following lemma from \cite{indyk} relating the EMD of two distributions
to a weighted sum of the  distances of their -coarsenings.
\begin{lemma}
\label{lemma:bindiv}
For any two distributions  and  over ,

\end{lemma}

Having established the various relationships between the EMD and the  distance,
we will make use of the result from \cite{L1tester} below for testing closeness of distributions
in  as a subroutine.

\begin{theorem}\label{complexL1tester}
  Given access to samples from two distributions  and  over , where ,
  there exists an algorithm that takes 
  samples and (1) accepts with probability at least  if 
  and (2) rejects with probability at least  if .
\end{theorem}

Alternatively, via a simple Chernoff bound analysis similar to \cite{L1indep},
we can show that a whole distribution can be approximated efficiently, giving
us another closeness tester for .

\begin{lemma}Given access to samples from a distribution  over , where , and
  parameters , there exists an algorithm
  that takes  samples and outputs a
  distribution  over  such that with probability at least 
  
  for every .
\end{lemma}

As a result, we can simply estimate  and  with  and ,
respectively, and compute their  distance to get the following alternative
tester, which gives us a different trade-off between  and  that will
become useful later.

\begin{theorem}\label{naiveL1tester}\mnote{combine w Th. \ref{complexL1tester}?}
  Given access to samples from two distributions  and  over M, where ,
  there exists an algorithm that takes 
  samples and (1) accepts with probability at least  if 
  and (2) rejects with probability at least  if .
\end{theorem}

\section{Closeness tester}\label{section:41}

In this section, we consider the EMD-closeness testing problem when the
domain is . The main idea behind the algorithm is to
embed EMD into the  metric and use an -closeness tester (Theorems
\ref{complexL1tester} and \ref{naiveL1tester}) to test the
resulting distributions. Recall from the preliminaries,  and  are
the -coarsening approximations of  and . We have the following algorithm,
where the subroutine \FuncSty{-Closeness-Tester} is an
-closeness tester on distributions  and  with distance parameter 
and failure probability .

\FuncSty{EMD-Closeness-Tester}\\
\begin{algorithm}[H]

  \For { \KwTo } {
    \If {\FuncSty{-Closeness-Tester},  rejects} {
      \KwSty{reject}
    }
  }
  \KwSty{accept}
\end{algorithm}


Note that our subroutine takes advantage of whichever tester (Theorem \ref{complexL1tester}
or \ref{naiveL1tester}) requires fewer samples. Specifically, when  is small,
the domains of the -coarsenings of  and  are small and  is the bottleneck,
so we use Theorem \ref{naiveL1tester}. On the other hand, if  is large, the sizes of
these domains become the bottleneck and we use Theorem \ref{complexL1tester}. This gives us
the following theorem.

\begin{theorem}
  The above is an EMD-closeness tester for distributions over  that
  takes  samples when  and
   samples when .
\end{theorem}
\begin{proof}
  If , then  for all , so by the union bound, the probability that
  the algorithm rejects is at most . 

  If, on the other hand, , then by Lemma {\ref{lemma:bindiv}},
  
  It follows by the pigeonhole principle that there exists an index  such that
  
  Hence, for that index , the -closeness tester in Step 2 will reject (with
  probability ).
  
  Now let us analyze the number of samples the algorithm needs. In the  iteration of
  the main loop,  and  has a domain with  elements, and we
  need to run an -closeness tester with a distance parameter of
  . Consider the following
  two cases:
  \begin{itemize}
    \item : Using the algorithm of Theorem \ref{complexL1tester}, we get a sample
    complexity of
    
    This quantity is maximized when , which gives us a total
    complexity of .
    
    \item : Using the algorithm of Theorem \ref{naiveL1tester}, we get a sample
    complexity of
    
    This quantity is maximized when , giving us a total complexity of
    .
  \end{itemize}
\end{proof}

If one of the distributions is explicitly known, we can use the corresponding -closeness
tester with sample complexity  from \cite{L1indep} to
similarly get the following theorem.

\begin{theorem}
	There exists an EMD-closeness tester for distributions over ,
	where one is explicitly known, that takes  samples
	when .
\end{theorem}

\section{Additive-error estimation}

We have seen that in -closeness testing, sometimes it is to our advantage to
simply estimate each probability value, rather than use the more sophisticated
algorithm of \cite{L1tester}. This seemingly naive approach has another advantage:
it gives an actual numeric estimate of the distances, instead of just an
accept/reject answer. Here, we use this approach to obtain an additive approximation
of the EMD of two unknown distributions over  as follows.

\FuncSty{EMD-Approx}\\
\begin{algorithm}[H]
	Let  be the grid on  with side length ,
		and let  and  be the distributions induced by  and  on , with
		weights in each cell concentrated at the center\;
	Take  samples from  and , and let 
		and  be the resulting empirical distributions\;
	\Return{}
\end{algorithm}

\begin{theorem}
	\FuncSty{EMD-Approx} takes  samples from  and
	 and, with probability , outputs an -additive approximation of .
\end{theorem}

\begin{proof}
Note that a sample from  or  gives us a sample from  or , respectively, so it remains to prove correctness.
Observe that , so  has  subsets. By the Chernoff bound, with the  samples from , we can guarantee for each  that
 with probability at most . By the union bound, with probability at least , all subsets of  will be approximated to within an additive . In that case,


We then have, by Corollary \ref{cor:EMDvsL1},
. Further, since each cell has radius ,
we have , giving us by the triangle inequality, .
Similarly, , so again by triangle inequality, we get

completing our proof.
\end{proof}

\section{Lower bounds}

We can show that our tester is optimal for the 1-dimensional domain by a simple argument:

\begin{theorem}
 	Let  be an EMD-closeness tester of distributions over any domain .
 	Then  requires  samples.
\end{theorem}

\begin{proof}
Consider two distributions  and  over the domain , where  is the distribution that puts the weight of  at  and  and  is the distribution that puts the weight of  at  and the weight of  at . Clearly , and it is a classic result that distinguishing  from  requires  samples.
\end{proof}

Clearly this also implies the same lower bound for 2-dimensional domains, making our
algorithm optimal in those cases. Next we prove that our -dimensional tester is
also essentially optimal in its dependence on .

\begin{theorem}
	There is no EMD-closeness tester that works on any  that
	takes  samples.
\end{theorem}

\begin{proof}
	Suppose  is an EMD-closeness tester that requires only 
	samples. Then consider the following -closeness tester for :

	\FuncSty{-Tester}\\
	\begin{algorithm}[H]
		Let  be a grid on  with side length \;
		Let  be an arbitrary injection from  into the lattice points of \;
		Let  and  be distributions on the lattice points of  induced by
			 on  and , resp.\;
		\Return{}\;
	\end{algorithm}

	Correctness is easy to see: if , then clearly  as well and the tester
	accepts; alternatively, if , then by Corollary \ref{cor:EMDvsL1}
	and the observation that ,
	
	so the tester rejects, as required.

	Now, to take a sample from  (or ), we simply take a sample
	 from  (or ) and return . Hence, the sample complexity of this
	tester is
	
	But this contradicts the lower bound for -closeness testing from
	\cite{L1tester,valiant}, completing our proof.
\end{proof}

If one of the distributions is known, we can similarly
use the weaker  lower bound for uniformity testing to obtain the
following theorem.

\begin{theorem}
	There is no EMD-closeness tester that works on any , where
	one of the input distributions is explicitly known, that takes 
	samples.
\end{theorem}



\section{Clusterable distributions}

Since the general technique of our algorithms is to forcibly divide the input
distributions into several small ``clusters,'' it is natural to consider what
improvements are possible when the distributions are naturally clusterable.
We obtain the following substantial improvement from the exponential dependence
on the dimension  that we had in the general case.

\begin{theorem}\label{thm:knowncenters}
	If the combined support of distributions  and  can be partitioned into
	 clusters of diameter , and we are given the  centers, then
	there exists an EMD-closeness tester for  and  that requires
	only  samples.
\end{theorem}

\begin{proof}
Let us denote the set of centers by . Consider the
distributions  and  on  induced by  and , respectively,
by assigning each point to its nearest center. If ,
by Lemma \ref{lem:buckets}, . We can,
of course, obtain samples from  and  by sampling from  and , respectively, and
returning the nearest center. Our problem thus reduces to -testing for
-closeness over  points, which requires
 samples using the -tester from
\cite{L1tester}.
\end{proof}

If we do not assume knowledge of the cluster centers, then we are still able to
obtain the following only slightly weaker result.

\begin{theorem}\label{thm:unknowncenters}
	If the combined support of  and  can be partitioned into  clusters
	of diameter , then even without knowledge of the centers there exists
	an EMD-closeness tester for  and  that requires only
	 samples.
\end{theorem}

To prove this, we need the following result by Alon et al. that was implicit in
Algorithm 1 from \cite{cluster}.

\begin{lemma}(Algorithm 1 from \cite{cluster})
	There exists an algorithm which, given distribution , returns 
	representative points if  is -clusterable, or rejects with
	probability  if  is -far from -clusterable, and
	which requires only  samples from . Moreover, if the
	 points are returned, they are with probability  the centers of a
	-clustering of all but a -weight of .
\end{lemma}

\begin{proof}\emph{(of Theorem \ref{thm:unknowncenters})\ }
By the lemma, if our distributions are -clusterable, using
 samples we obtain a -clustering of all but an
-fraction of the support of  and , with
centers . Note that the unclustered probability mass
contributes at most  to the EMD. The theorem then follows from an identical
argument as that of Theorem \ref{thm:knowncenters} (since we now know the centers).
\end{proof}

Note that in general, if we assume -clusterability, this implies
-clusterability (by packing  balls), where knowledge
of the super-cluster centers also implies knowledge of the sub-cluster centers.
Similarly, in the unknown centers case, -clusterability implies
-clusterability. Unfortunately, in both cases we
reintroduce exponential dependence on , so clusterability only really helps
when the cluster diameters are as assumed above.

\section{EMD over tree-metrics}

So far we have considered only underlying -spaces (or, almost equivalently,
). We will now see what can be done for EMD over tree-metrics, and prove
the following result.

\begin{theorem}
	If  and  are distributions over the nodes of a tree , with
	edge weight function , then there exists an -additive-error
	estimator for  that requires only 
	samples, where ,  is the number of nodes of , and  is defined with
	respect to the tree metric of . Moreover, up to polylog factors,
	this is optimal.
\end{theorem}

\begin{proof}
First, let us consider an unweighted tree  over  points (i.e., where
every edge has unit weight), with distributions  and  on the
vertices. Observe that the minimum cost flow between  and  on  is
simply the flow that sends through each edge  just enough to balance 
and  on each subtree on either side of . In other words, if  is
an arbitrary one of the two trees comprising ,

Then, with  samples we can, for every , estimate
 and  to within . This
gives us an -additive estimator for .

Generalizing to the case of a weighted tree, where edge  has weight
, we have

It then suffices to estimate each  and  term to within
. Thus,  samples
suffice, where .

Note that what we we get is
not only a closeness tester but also an additive-error estimator.
In fact, even if we only want a tester, this is the best we can do: in the case where  is a line graph (with diameter ), the standard biased-coin lower bound implies we need  samples.
\end{proof}



\bibliographystyle{plain}
\begin{thebibliography}{X}

\bibitem{cluster}
N.~Alon, S.~Dar, M.~Parnas and D.~Ron.
\newblock Testing of clustering.
\newblock {\em SIAM J. Discrete Math}, 16:393--417, 2003.

\bibitem{alex}
A.~Andoni, P.~Indyk and R.~Krauthgamer.
\newblock Earth Mover Distance over high-dimensional spaces.
\newblock {\em Proc. SODA}, 343--352, 2008.

\bibitem{L1indep}
T.~Batu, E.~Fischer, L.~Fortnow, R.~Kumar, R.~Rubinfeld and P.~White.
\newblock Testing random variables for independence and identity.
\newblock {\em Proc. FOCS}, 442--451, 2001.

\bibitem{L1tester}
T.~Batu, L.~Fortnow, R.~Rubinfeld, W.~D.~Smith and P.~White.
\newblock Testing that distributions are close.
\newblock {\em Proc. FOCS}, 259--269, 2000.

\bibitem{charikar}
M.~S.~Charikar.
\newblock Similarity estimation techniques from rounding algorithms.
\newblock {\em Proc. STOC}, 380--388, 2002.

\bibitem{cohen}
S.~Cohen and L.~Guibas.
\newblock The Earth Mover's Distance under transformation sets.
\newblock {\em Proc. ICCV}, 1076--1083, 1999.

\bibitem{GR}
O.~Goldreich and D.~Ron.
\newblock On Testing Expansion in Bounded-Degree Graphs.
\newblock {\em ECCC}, 7-20, 2000.

\bibitem{indyk2}
P.~Indyk.
\newblock A near linear time constant factor approximation for Euclidean Bichromatic Matching (Cost).
\newblock {\em Proc. SODA}, 39--42, 2007.

\bibitem{indyk}
P.~Indyk and N.~Thaper.
\newblock Fast image retrieval via embeddings.
\newblock {\em 3rd International Workshop on Statistical and Computational Theories of
                                        Vision}, 2003.

\bibitem{levina}
E.~Levina and P.~Bickel.
\newblock The Earth Mover's Distance is the Mallows Distance: some insights from
                                        statistics.
\newblock {\em Proc. ICCV}, 251--256, 2001.

\bibitem{peleg}
S.~Peleg, M.~Werman and H.~Rom.
\newblock A unified approach to the change of resolution: space and gray-level.
\newblock {\em Trans. Pattern Analysis and Machine Intelligence}, 11: 739--742.

\bibitem{rubner1}
Y.~Rubner and C.~Tomasi.
\newblock Texture metrics.
\newblock {\em Proc. ICSMC}, 4601--4607, 1998.

\bibitem{rubner2}
Y.~Rubner, C.~Tomasi and L.~J.~Guibas.
\newblock The Earth Mover's Distance, multi-dimensional scaling, and color-based
                                        image retrieval.
\newblock {\em Proc. ARPA Image Understanding Workshop}, 661--668, 1997.

\bibitem{rubner3}
Y.~Rubner, C.~Tomasi and L.~J.~Guibas.
\newblock A metric for distributions with applications to image databases.
\newblock {\em Proc. IEEE ICCV}, 59--66, 1998.

\bibitem{rubner4}
Y.~Rubner, C.~Tomasi and L.~J.~Guibas.
\newblock The Earth Mover's Distance as a metric for image retrieval.
\newblock {\em International Journal of Computer Vision}, 40(2): 99--121, 2000.

\bibitem{ruzon1}
M.~Ruzon and C.~Tomasi.
\newblock Color edge detection with the compass operator.
\newblock {\em Proc. CVPR}, 2:160--166, 1999.

\bibitem{ruzon2}
M.~Ruzon and C.~Tomasi.
\newblock Corner detection in textured color images.
\newblock {\em Proc. ICCV}, 2:1039--1045, 1999.

\bibitem{valiant}
P.~Valiant.
\newblock Testing Symmetric Properties of Distributions
\newblock {\em ECCC}, 2007.

\end{thebibliography}

\end{document}
