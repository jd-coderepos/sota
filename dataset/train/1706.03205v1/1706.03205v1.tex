\documentclass[sigconf]{acmart}

\usepackage{booktabs} \usepackage{multirow}
\usepackage{color}
\usepackage{subfigure} \usepackage{array}
\usepackage{breqn}
\usepackage{{inputenc}}
\usepackage{balance}
\usepackage{multirow,tabularx}
\usepackage{balance}
\usepackage{longtable}
\usepackage{booktabs,longtable}
\usepackage{hhline}
\usepackage[flushleft]{threeparttable}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{thmtools}
\usepackage{enumitem}
\usepackage{verbatim}
\usepackage{amsfonts}
\usepackage{hyperref}
\hypersetup{colorlinks=false,linkcolor=blue,urlcolor=blue,citecolor=red}
\epstopdfsetup{update}

\newcommand\norm[1]{\left\lVert#1\right\rVert}
\newcommand{\Lapl}{\mathbf{\mathop{\mathcal{L}}}}
\newcommand{\Trans}[1]{{#1}^{\top}}
\newcommand{\Trace}[1]{tr\left({#1}\right)}
\newcommand{\Bracs}[1]{\left({#1}\right)}
\newcommand{\Mat}[1]{\mathbf{#1}}
\newcommand{\MatS}[3]{\mathbf{#1}^{#2}_{#3}}
\newcommand{\Space}[1]{\mathbb{#1}}
\newcommand{\Set}[1]{\mathcal{#1}}
\newcommand{\vectornorm}[1]{\left|\left|#1\right|\right|}
\newcommand{\bpi}{\boldsymbol{\pi}}
\newcommand{\BlockMat}[2]{\left[\begin{matrix}#1\\#2\end{matrix}\right]}
\newcommand{\BlockMatSquare}[4]{\left[\begin{matrix}#1 & #2\\#3 & #4\end{matrix}\right]}

\newcommand{\ie}{\emph{i.e., }}
\newcommand{\eg}{\emph{e.g., }}
\newcommand{\etal}{\emph{et al.}}
\newcommand{\st}{\emph{s.t. }}
\newcommand{\etc}{\emph{etc.}}
\newcommand{\wrt}{\emph{w.r.t. }}
\newcommand{\cf}{\emph{cf. }}
\newcommand{\aka}{\emph{aka. }}

\hyphenpenalty=10000
\tolerance=5000

\copyrightyear{2017}
\acmYear{2017}
\setcopyright{acmcopyright}
\acmConference{SIGIR’17}{}{August 7--11, 2017, Shinjuku, Tokyo, Japan}
\acmPrice{15.00}
\acmDOI{http://dx.doi.org/10.1145/3077136.3080771}
\acmISBN{978-1-4503-5022-8/17/08}

\settopmatter{printacmref=false, printfolios=false}
\fancyhead{}

\begin{document}
\title{Item Silk Road: Recommending Items from Information Domains to Social Users}

\author{Xiang Wang}
\affiliation{\institution{National University of Singapore}
}
\email{xiangwang@u.nus.edu}

\author{Xiangnan He}
\authornote{Xiangnan He is the corresponding author.}
\affiliation{\institution{National University of Singapore}
}
\email{xiangnanhe@gmail.com}

\author{Liqiang Nie}
\affiliation{\institution{ShanDong University}
}
\email{nieliqiang@gmail.com}

\author{Tat-Seng Chua}
\affiliation{\institution{National University of Singapore}
}
\email{dcscts@nus.edu.sg}


\begin{abstract}
Online platforms can be divided into information-oriented and social-oriented domains. The former refers to forums or E-commerce sites that emphasize user-item interactions, like Trip.com and Amazon; whereas the latter refers to social networking services (SNSs) that have rich user-user connections, such as Facebook and Twitter.
Despite their heterogeneity, these two domains can be bridged by a few overlapping users, dubbed as \emph{bridge users}.
In this work, we address the problem of \textit{cross-domain social recommendation}, \emph{i.e.}, recommending relevant items of information domains to potential users of social networks.
To our knowledge, this is a new problem that has rarely been studied before.

Existing cross-domain recommender systems are unsuitable for this task since they have either focused on homogeneous information domains or assumed that users are fully overlapped. Towards this end, we present a novel \emph{Neural Social Collaborative Ranking} (NSCR) approach, which seamlessly sews up the user-item interactions in information domains and user-user connections in SNSs.
In the information domain part, the attributes of users and items are leveraged to strengthen the embedding learning of users and items. In the SNS part, the embeddings of bridge users are propagated to learn the embeddings of other non-bridge users. Extensive experiments on two real-world datasets demonstrate the effectiveness and rationality of our NSCR method.
\end{abstract} \vspace{-5pt}

\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10002951.10003260.10003261.10003270</concept_id>
<concept_desc>Information systems~Social recommendation</concept_desc> <concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10002951.10003317.10003338</concept_id>
<concept_desc>Information systems~Retrieval models and ranking</concept_desc> <concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10002951.10003317.10003347.10003350</concept_id>
<concept_desc>Information systems~Recommender systems</concept_desc> <concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Information systems~Social recommendation}
\ccsdesc[500]{Information systems~Retrieval models and ranking}
\ccsdesc[500]{Information systems~Recommender systems}
\vspace{-5pt}

\keywords{Cross-domain Recommendation, Deep Collaborative Filtering, Neural Network, Deep Learning} \vspace{-5pt}
\maketitle

\section{Introduction}
Nowadays online platforms play a pivotal role in our daily life and encourage people to share experiences, exchange thoughts, and enjoy online services.
Regardless of applications, we can roughly divide the existing platforms into information-oriented and social-oriented domains.
The former typically refers to forums or E-Commerce sites that have thorough knowledge on items, such as point-of-interests in Trip.com, movies in IMDb, and products in Amazon. These sites have ample user-item interactions available in the form of users' reviews, ratings, along with various kinds of implicit feedback like views and clicks~\cite{iCD}.
On the other hand, the social-oriented domains are mainly social network sites, which emphasize the social connections among users~\cite{SNE}.

When adopting an item, besides consulting the information sites, a user usually gathers more detailed information from her experienced friends. This refers to \emph{word-of-mouth marketing}, which is widely recognized as the most effective strategy for producing recommendation.
As reported by Cognizant\footnote{\url{https://www.cognizant.com}.}, more than  of travelers rely on social networks to seek advice from friends for travel.
However, most existing SNSs, like Facebook and Twitter, are designed mainly for users to rebuild their real-world connections, rather than for seeking options regarding items.
Though some item cues implying users' preference can be found in SNSs, they typically contain item names only with limited details.
The sparse and weak user-item interactions greatly hinder the ability of SNSs to offer item recommendation services.


Fortunately, some users may be simultaneously involved in both SNSs and information-domain sites, who can act as a bridge to propagate user-item interactions across domains.
For example, it is not unusual for a user to share her travel experiences in Trip.com; and if the user also holds a Facebook account, we can recommend her friends in Facebook with her liked items from Trip.com.
In social circles, these bridge users are like the \emph{silk road} to route relevant items from information domains to  (non-bridge) users of social networks.
As such, we formulate the task of \textit{cross-domain social recommendation}, which aims to recommend relevant items of information domains to the users of social domains.
Apparently, this task is related to the recently emerging topic --- cross-domain recommendation~\cite{DBLP:journals/tkde/JiangCCW0Y15}.
However, we argue that existing efforts have either focused on homogeneous domains (\textit{i.e.,} multiple sites of the information domain)~\cite{DBLP:conf/www/ElkahkySH15}, or unrealistically assumed that the users are fully overlapped~\cite{DBLP:journals/tkde/JiangCCW0Y15,wang2017unifying}. Our task to address is particularly challenging due to the following two practical considerations.


\begin{itemize}[leftmargin=*]
\item Insufficient bridge users. To gain a deep insight, we analyzed the overlapped users between Trip.com and Facebook/Twitter, finding that only  of  Facebook users and  of  Twitter users have public accounts in Trip.com. It is highly challenging to leverage history of such limited number of bridge users to provide quality recommendation for non-bridge users.\item Rich attributes. The users and items of an information domain are usually associated with rich attributes. For instance, Trip.com enables users to indicate their travel preference explicitly, and associates travel spots (\textit{i.e.,} items) with specific travel modes, among other information. However, little attention has been paid to leverage these attributes to boost the performance of cross-domain recommendation.
\end{itemize} \vspace{-5pt}

\noindent In this work, we propose a novel solution named \emph{Neural Social Collaborative Ranking} (NSCR) for the new task of cross-domain social recommendation. It is developed based on the recent advance of neural collaborative filtering (NCF)~\cite{heneural}, which is further extended to model the cross-domain social relations by combining with the graph regularization technique~\cite{DBLP:conf/cikm/HeCKC15}. We entail two key technical components of our NSCR as follows. \begin{itemize}[leftmargin=*]
\item For the modelling of information domain, we build an attribute-aware recommender based on the NCF framework. To fully exploit the interactions among a user, an item, and their attributes, we enhance NCF by plugging a \textit{pairwise pooling} operation above the embedding vectors of user (item) ID and attributes. In contrast to the default average pooling used by NCF~\cite{heneural} and other recent neural recommenders~\cite{DBLP:conf/recsys/CovingtonAS16}, our use of pairwise pooling better captures feature interactions in the low level~\cite{he2017neural,DBLP:conf/uai/RendleFGS09}, greatly facilitating the following deep layers to learn higher-order interactions among users, items and attributes.
\item For the modelling of social domain, it is natural to guide the embedding learning of social users by using the embeddings of bridge users. As the embeddings of bridge users are optimized to predict user--item interactions (\eg ratings and purchases), propagating their embeddings to social users helps to bridge the heterogeneity gap between information domain and social domain. To implement such propagation effect, we employ the \emph{smoothness} constraint (\ie\emph{graph Laplacian}) on the social network, which enforces close friends to have similar embedding so as to reflect their similar preferences.
\end{itemize}



\noindent To sum up, the key contributions of this work are three-fold:
\begin{enumerate}[leftmargin=*]
  \item To our knowledge, we are the first to introduce the task of cross-domain social recommendation, which recommends relevant items of information domains to target users of social domains.
  \item We propose a novel solution that unifies the strengths of deep neural networks in modelling attributed user-item interactions and graph Laplacian in modelling user-user social relations.
\item We construct two real-world benchmark datasets for exploring the new task of cross-domain social recommendation and extensively evaluate our proposed solution.
\end{enumerate}

\section{Preliminary}
We first formulate the task of cross-domain social recommendation, and then shortly recapitulate the matrix factorization model, highlighting its limitations for addressing the task.
\subsection{Problem Formulation}
\begin{figure}
	\centering
\includegraphics[width=0.46\textwidth]{Chart/overall.pdf}\\
	\vspace{-5pt}
	\caption{Illustration of the cross-domain social recommendation task.}\label{fig:framework}
	\vspace{-1em}
\end{figure}

Figure~\ref{fig:framework} illustrates the task of cross-domain social recommendation.
In the information domain, we have the interaction data between users and items.
Let  and  denote a user and the whole user set of the information domain, respectively; similarly, we use  and  to denote an item and the whole item set, respectively.
The edges between users and items denote their interactions, , which can be real-valued explicit ratings or binary 0/1 implicit feedback.
Traditional collaborative filtering algorithms can then be performed on the user-item interaction data.

In addition to the ID that distinguishes a user or an item, most information-domain sites also associate them with abundant side information, which can help to capture users' preferences and item properties better.
For example, in Trip.com, the user may choose the travel tastes of \emph{\{luxury travel, art lover\}} in her profile; while, the item \emph{Marina Bay Sands} is tagged most with travel modes \emph{\{luxury travel, family travel, nightlife\}}.
We term these associated information as \textit{attributes}, most of which are discrete categorical variables for the web domain~\cite{he2017neural}.
Formally, we denote  and  as an attribute and the whole attribute set, respectively; for a user  and an item , we can then construct the associated attribute set as  and , respectively.

In the social domain, we have social connections between users, such as the undirected friendship or directed follower/followee relations.
We denote a social user as , all users of the social domain as , and all social connections as .
We define the bridge users as the overlapping users between the information domain and social domain. These bridge users can be expressed as .
In a social network, a user's behaviours and preferences can be propagated along the social connections to influence her friends. As such, these bridge users play a pivotal role in addressing the cross-domain social recommendation problem, which is formally defined as:
\begin{description}
	\item[\textbf{Input}:] An information domain with ; a social domain with ; and  is nonempty.
	\item[\textbf{Output}:] A personalized ranking function for each user  of the social domain , which maps each item of the information domain to a real number.
\end{description}
It is noted that there indeed exist sparse and weak user-item interactions in SNSs as aforementioned. However, we simplify this scenario of cross-domain social recommendation by only emphasizing the social connections in SNSs and leaving the exploration of weak interactions as the future work.






\subsection{Factorization Model}
Collaborative filtering (CF) is the key technique for personalized recommendation systems. It exploits user-item interactions by assuming that similar users would have similar preference on items.
Model-based CF approaches~\cite{iCD,DCF} achieve this goal by describing the interaction data with an underlying model, for which the holistic goal is to build:

where  denotes the underlying model with parameters , and  denotes the predicted score for a user-item interaction .
Matrix factorization (MF) is one of the simplest yet effective models for the recommendation task, which characterizes a user or an item with a latent vector, modelling a user-item interaction as the inner product of their latent vectors:

where  and  are model parameters denoting the latent vector (\aka representation) for user  and item , respectively.

\begin{figure}
	\centering
\includegraphics[width=0.46\textwidth]{Chart/mf.pdf}
	\vspace{-5pt}
	\caption{MF as a shallow neural network model.}\label{fig:mf_limit}
	\vspace{-1em}
\end{figure}

Despite its effectiveness, we note that MF's expressiveness can be limited by the use of the inner product operation to model a user-item interaction. To illustrate this, we present a neural network view of the MF model. As shown in Figure~\ref{fig:mf_limit},  we feed the one-hot representation of user/item ID into the architecture, and project them with a fully connected embedding layer. By feeding the user/item embedding vectors into the element-wise product layer, we obtain a hidden vector .
If we directly project  into the output score, we can exactly recover the MF model.
As such, MF can be deemed as a shallow neural network with one hidden layer only. Based on this connection, we argue that there are two key limitations of MF-based approaches for cross-domain social recommendation:
\begin{itemize}[leftmargin=*]	
	\item First, MF only considers the simple two-way interaction between a user and an item, by assuming that their cross latent factors (\ie  and ) are independent of each other. However, such an independence assumption can be insufficient to model real-world data,
which usually have complex and non-linear underlying structures~\cite{he2017neural,SNE}.
	\item The case can be even worse if we take the attributes into account. A typical way to extend MF with side attributes is SVDfeature, \ie by summing attribute embedding vectors with user/item embedding vector. As a result, the rich correlations among users, items, and attributes are unintentionally ignored.
\end{itemize}

\noindent Our proposed NSCR solution addresses the above limitations of MF by 1) using a deep learning scheme to capture the higher-order correlations between user and item latent factors, and 2) devising a pairwise pooling operation to efficiently model the pair-wise correlations among users, items, and attributes.


\section{Our NSCR Solution}

The goal of cross-domain social recommendation is to select relevant items from the information domain for social users. Under the paradigm of embedding-based methods (\aka representation learning), the key for addressing the task is on how to project items (of the information domain) and users (of the social domain) into the same embedding space.
A generic solution is the factorization machine~(FM)~\cite{DBLP:conf/uai/RendleFGS09,DBLP:journals/tist/Rendle12}, which merges the data from the two domains by an early fusion; that is, constructing the predictive model by incorporating social users as the input features.
While the solution sounds reasonable conceptually, the problem is that the training instances which can incorporate social users are only applicable to the bridge users, which can be very few for real-world applications. As such, the generic recommender solution FM can suffer severely from the problem of insufficient bridge users.

To address the challenge of insufficient bridge users, we propose a new framework that separates the embedding learning process of each domain.
By enforcing the two learning processes to share the same embeddings for bridge users, we can ensure that items and social users are in the same embedding space.
Formally, we devise the optimization framework as:

where  (or ) denotes the objective function of the information domain (or social domain) learning with parameters  (or ), and most importantly,  are nonempty denoting the shared embeddings of bridge users.

By separating the learning process for two domains, we allow the design of each component to be more flexible. Specially, we can apply any collaborative filtering solution for  to learn from user-item interactions, and utilize any semi-supervised learning technique for  to propagate the embeddings of bridge users to non-bridge users.
In the remainder of this section, we first present our novel neural collaborative ranking solution for , followed by the design of social learning component . Lastly, we discuss how to optimize the joint objective function.








\subsection{Learning of Information Domain}
\label{ss:learning_information}
To estimate the parameters for a CF model from user-item interaction data, two types of objective functions --- point-wise~\cite{iCD,heneural} and pair-wise~\cite{DBLP:conf/uai/RendleFGS09,chen2017acf,RankALS} --- are most commonly used.
The point-wise objective functions aim to minimize the loss between the predicted score and its target value.
Here, to tailor our solution for both implicit feedback and the personalized ranking task, we adopt the pair-wise ranking objective functions.

Formally, we denote an observed user-item interaction as , otherwise . Instead of forcing the prediction score  to be close to , ranking-ware objective functions concern the relative order between the pairs of observed and unobserved interactions:

where  and ;  denotes the set of training triplets, each of which comprises of a user , an item  of observed interactions (\ie ), and an item  of unobserved interactions (\ie ). An ideal model should rank all  item pairs correctly for every user. To implement the ranking hypotheses, we adopt the regression-based loss~\cite{RankALS}:

\noindent Note that other pair-wise ranking functions can also be applied, such as the bayesian personalized ranking~(BPR)~\cite{chen2017acf,DBLP:conf/uai/RendleFGS09} and contrastive max-margin loss~\cite{DBLP:conf/nips/SocherCMN13}. In this work, we use the regression-based ranking loss as a demonstration for our NSCR, and leave the exploration of other choices as the future work.

\subsubsection{\textbf{Attribute-aware Deep CF Model}}\label{sec:representation-learning}
Having established the optimization function for learning from information domain, we now present our attribute-aware deep collaborative filtering model to estimate a user-item interaction . Figure~\ref{fig:neural-collaborative-ranking} illustrates its architecture, which is a multi-layered feed-forward neural network. We elaborate its design layer by layer.



\textbf{Input Layer.} The input to the model is a user , an item , and their associated attributes  and . We transform them into barbarized sparse vectors with one-hot encoding, where only the non-zero binary features are recorded.


\textbf{Embedding Layer.}~The embedding layer maps each non-zero feature into a dense vector representation. As we have four types of features here, we differentiate them with different symbols: , , , and  denote the -dimensional embedding vector for user , item , user attribute , and item attribute , respectively.

\textbf{Pooling Layer.}~The output of the embedding layer is a set of embedding vectors to describe user  and item , respectively. As different users (items) may have different number of attributes, the size of the embedding vector set may vary for different inputs. To train a neural network of fixed structure, it is essential to convert the set of variable-length vectors to a fixed-length vector, \ie the pooling operation.

The most commonly used pooling operations in neural network modelling are average pooling and max pooling. However, we argue that such simple operations are insufficient to capture the interaction between users/items and attributes.
For example, the average pooling assumes a user and her attributes are linearly independent, which fails to encode any correlation between them in the embedding space.
To tackle the problem, we consider to model the pairwise correlation between a user and her attributes, and all nested correlations among her attributes:



where  denotes the element-wise product of two vectors. We term it as \emph{pairwise pooling}, which is originally inspired from the design of factorization machines~\cite{DBLP:conf/icdm/Rendle10,he2017neural}. By applying pairwise pooling on the item counterpart, we can similarly model the pair-wise correlation between an item and its attributes:



It is worth pointing out that although pairwise pooling models the correlation between each pair of features, it can be efficiently computed in linear time --- the same time complexity with average/max pooling. To show the linear time complexity of evaluating pairwise pooling, we reformulate Eqn.\eqref{equ:user-bilinear-pooling} as,

which can be computed in  time. This is a very appealing property, meaning that the benefit of pairwise pooling in modelling all pair-wise correlations does not involve any additional cost, as compared to the average pooling that does not model any correlation between input features.


\begin{figure}
	\centering
\includegraphics[width=0.46\textwidth]{Chart/neural-collaborative-ranking.pdf}\\
	\vspace{-5pt}
	\caption{Illustration of our Attributed-aware Deep CF model for estimating an user-item interaction.}\label{fig:neural-collaborative-ranking}
	\vspace{-1em}
\end{figure}

\textbf{Hidden Layers:}~Above the pairwise pooling is a stack of full connected layers, which enable us to capture the nonlinear and higher-order correlations among users, items, and attributes.
Inspired by the neural network view of matrix factorization (\cf Figure~\ref{fig:mf_limit}), we first merge user representation  and item representation  with an element-wise product, which models the two-way interaction between  and . We then place a multi-layer perceptron (MLP) above the element-wise product. Formally, the hidden layers are defined as:

where , , , and  denote the weight matrix, bias vector, activation function, and output vector of the -th hidden layers, respectively. As for the activation function in each hidden layer, we opt for Rectifier (ReLU) unit, which is more biologically plausible and proven to be non-saturated.
Regarding the structure of hidden layers, common choices include the tower~\cite{heneural,DBLP:conf/recsys/CovingtonAS16}, constant, and diamond, among others. In this work, we simply set all hidden layers have the same size, leaving the further tuning of the deep structure as the future work.

\textbf{Prediction Layer:}~At last, the output vector of the last hidden layer  is transformed to the prediction score:

where  represents the weight vector of the prediction layer.

Note that we have recently proposed a neural factorization machine (NFM) model~\cite{he2017neural}, which similarly uses a pairwise pooling operation to model the interaction among features. We point out that the main architecture difference is in our separated treatment of the user and item channel, where each channel can essentially be seen as an application of NFM on the user/item ID and attributes.

















\subsection{Learning of Social Domain}

With the above neural collaborative ranking solution, we obtain an attribute-aware representation  and  for each user and item, respectively.
To predict the affinity score of a social user to an item of the information domain, we need to also learn an representation for the social user in the same latent space of the information domain. We achieve this goal by propagating  from bridge users to representations for non-bridge users of the social domain. The intuition for such representation propagation is that, if two users are strongly connected (\eg close friends with frequent interactions), it is likely that they have the similar preference on items; as such, they should have similar representations in the latent space. This suits well the paradigm of graph regularization~\cite{DBLP:conf/cikm/HeCKC15,DBLP:journals/tkde/WangFHLW17,DBLP:journals/tkde/WangFHTW16,fuli2017computational} (\aka semi-supervised learning on graph), which has two components:






\textbf{Smoothness:}~The smoothness constraint implies the structural consistency --- the nearby vertices of a graph should not vary much in their representations. Enforcing smoothness constraint in our context of social domain learning will propagate a user's representation to her neighbors, such that when a steady state reaches, all vertices should have been placed in the same latent space. The objective function for smoothness constraint is defined as:

where  denotes the strength of social connection between  and , and  (or ) denotes the outdegree of  (or ) for normalization purpose. It is worth noting that the use of normalization is the key difference with the social regularization used by \cite{DBLP:conf/wsdm/MaZLLK11,zhao2016user}, which does not apply any normalization on the smoothness constraint. As pointed out by He \etal~\cite{DBLP:conf/cikm/HeCKC15}, the use of normalization helps to suppress the impact of popular vertices, which can lead to more effective propagation. We empirically verify this point in Section 4.3.








\textbf{Fitting:}
~The fitting constraint implies the latent space consistency across two domains --- the bridge users' representations should be invariant and act as the anchors across domains. Towards this end, we encourage the two representations of the same bridge users to be close to each other. The objective function for fitting constraint is defined as,

where for each bridge user ,  (or ) is her representation of the SNS (or information domain). As such, the fitting constraint essentially acts as the bridges connecting the two latent spaces.

Lastly, we combine the smoothness constraint with the fitting constraint and obtain the objective function of the social domain learning as,

where  is a positive parameter to control the tradeoff between two constraints.



\subsubsection{\textbf{Prediction for Social Users}}
With the representations of social users and items (\ie  and ) at hand, we can feed them into the fully connected layers as Eqn.\eqref{equ:fully-connected-layer} shows and utilize the prediction layer as Eqn.\eqref{equ:prediction-layer} displays. At last, we can obtain the predicted preference , as follows,





\subsection{Training}
We adopt the alternative optimization strategy on Eqn.\eqref{equ:framework} since it can emphasize exclusive characteristics within individual domains. In the information domain, we employ stochastic gradient descent SGD) to train the attribute-aware NSCR in the mini-batch mode and update the corresponding model parameters. In particular, we first sample a batch of observed user-item interactions  and adopt negative sampling~\cite{heneural} to randomly select an unobserved item  for each . We then generate a triplet . Following that, we take a gradient step to optimize the loss function  in Eqn.\eqref{equ:information-obj}. As such, we obtain the enhanced representations of users. In the SNS, we feed the enhanced representations of bridge users into our graph Laplacian to update all representations of social users. Towards this end , we can simplify the derivative of  regarding user representation  and then obtain the close-form solution as,

where  is the embedding of social users, which includes the updated representations of bridge users from NSCR part;  and  are the similarity matrix and diagonal degree matrix of social users, respectively, whereinto  and . Thereafter, we view the newly updated representations of bridge users as the next initialization for the bridge users in NSCR. We repeat the above procedures to approximate the model parameter set . As for the regularization term in Eqn.\eqref{equ:framework}, we omit it since we utilize \emph{dropout} technique in neural network modeling to avoid overfitting.



\textbf{Dropout:}~Dropout is an effective solution to prevent deep neural networks from overfitting. The idea is to randomly drop part of neurons during training. As such, only part of the model parameters, which contribute to the final ranking, will be updated. In our neural CR model, we propose to adopt dropout on the pairwise pooling layer. In particular, we randomly drop  of  and , whereinto  is the dropout ratio. Analogous to the pooling layer, we also conduct dropout on each hidden layer.

\section{Experiments}
\label{sec:experiments}
To comprehensively evaluate our proposed method, we conducted experiments to answer the following research questions:
\begin{itemize}[leftmargin=*]
	\item\textbf{RQ1:}~Can our NSCR approach outperform the state-of-the-art recommendation methods for the new cross-domain social recommendation task?
	\item\textbf{RQ2:}~How do different hyper-parameter settings (\eg the dropout ratio and tradeoff parameters) affect NSCR?
	\item\textbf{RQ3:}~Are deeper hidden layers helpful for learning from user-item interaction data and improving the performance of NSCR?
\end{itemize}


\subsection{Data Description}

	To the best of our knowledge, there is no available public benchmark dataset that
	fits the task of cross-domain social recommendation. As such, we constructed the datasets by ourselves. We treated Trip.com as the information domain, Facebook and Twitter as the social domains. In Trip.com, we initially compiled  active users, who had at least  ratings over  items (\eg \emph{gardens by the bay} in Singapore and \emph{eiffel tower} in Pairs). We transformed their  ratings into binary implicit feedback as ground truth, indicating whether the user has rated the item.
	Moreover, we collected  general categories regarding the travel mode (\eg \emph{adventure travel}, \emph{business travel}, and \emph{nightlife}) and used them as the attributes of users and items.
Subsequently, we parsed the users' profiles to identify their aligned accounts in Facebook and Twitter, inspired by the methods in~\cite{DBLP:series/synthesis/2016NieSC,DBLP:conf/sigir/SongNZAC15}.
	We obtained  and  bridge users for Facebook and Twitter, respectively. Thereafter, we crawled the public friends or followers of each bridge user to reconstruct the social networks, resulting in  Facebook users and  Twitter users.
	However, the original social data are highly sparse, where most non-bridge users have only one friend, making it ineffective to propagate users' preferences.
To ensure the quality of the social data,
	we performed a modest filtering on the data, retraining users with at least two friends.
This results in a subset of the social data that contains  Twitter users with  social connections and  Facebook users with  social connections. The statistics of the datasets are summarized in Table~\ref{tab:data-statistics}.
	
	\subsection{Experimental Settings}
	\textbf{Evaluation Protocols:}~Given a social user, each method generates an item ranking list for the user.
	To assess the ranking list, we adopted two popular IR metrics,  and , to measure the quality of preference ranking and top- recommendation.
	\begin{itemize}[leftmargin=*]
\item\textbf{AUC:}~Area under the curve (AUC)~\cite{DBLP:conf/uai/RendleFGS09,DBLP:conf/www/HuCXCGZ13} measures the probability that a recommender system ranks a positive user-item interaction higher than negative ones:
		
		where  and  denote the sets of relevant (observed) item  and irrelevant (unobserved) item  for user , respectively; and  is the count function returning  if  and  otherwise. Below we report the averaged AUC for all testing users.
		\item\textbf{R@:}~Recall@ considers the relevant items within the top  positions of the ranking list. A higher recall with lower  indicates a better recommender system, which can be defined as,
		
		where  denotes the set of the top- ranked items for the given user . Analogous to AUC, we report the average  for all testing users.
	\end{itemize}
	
	\noindent By learning representations for social users and information-domain items together, our NSCR is capable of recommending items for both bridge and non-bridge users.
	However, due to the limitation of our static datasets, it is difficult for us to evaluate the recommendation quality for non-bridge users, since they have no interaction on the information-domain items.
	As such, we rely on the bridge users for evaluating the performance.
	Following the common practice in evaluating a recommender algorithm~\cite{heneural,DBLP:conf/uai/RendleFGS09},
	we holdout the latest  interactions of a bridge user as the test set.
	To tune hyper-parameters, we further randomly holdout  interactions from a bridge user's training data as the validation set. We feed the remaining bridge users, all the non-bridge users in SNSs, and the remaining user-item interactions in the information domains into our framework for training.
	
	\begin{table}[t]
		\centering
		\caption{Statistics of the complied datasets. The social user set includes the bridge users.}
		\vspace{-5pt}
		\label{tab:data-statistics}
		\resizebox{0.46\textwidth}{!}{
			\begin{tabular}{|l|c|c|c|}
				\hline
				\textbf{Information Domain} & \textbf{User\#}  & \textbf{Item\#}  & \textbf{Interaction\#}         \\ \hline
				Trip.com       &                 &                 &                        \\ \hline\hline
				\textbf{SNSs}   & \textbf{Bridge User\#} & \textbf{Social User\#} & \textbf{Social Connection\#}   \\ \hline
				Twitter        &                   &                 &                        \\ Facebook       &                 &                 &                        \\ \hline
		\end{tabular}}
		\vspace{-1em}
	\end{table}
	




	\textbf{Baselines:}~To justify the effectiveness of our proposal, we study the performance of the following methods:
	\begin{itemize}[leftmargin=*]
		\item\textbf{ItemPop:}~This method ranks items base on their popularity, as judged by the number of interactions. It is a non-personalized method that benchmarks the performance of a personalized system~\cite{DBLP:conf/uai/RendleFGS09}.
		\item\textbf{MF:}~This is the standard matrix factorization model that leverages only user--item interactions of the information domain for recommendation~(\cf Eqn.\eqref{equ:mf}).
		\item\textbf{SFM:}~Factorization machine~\cite{DBLP:conf/icdm/Rendle10} is a generic factorization model that is designed for recommendation with side information.
		We construct the input feature vector by using one-hot encoding on the ID and attributes of users and items.
		To adjust FM for modelling social relations, we further plug a (bridge) user's friends into the input feature vector, dubbed this enhanced model as Social-aware FM (SFM).
\item\textbf{SR:}~This~\cite{DBLP:conf/wsdm/MaZLLK11} is a state-of-the-art factorization method for social recommendation.
		It leverages social relations to regularize the latent vectors of friends to be similar.
		To incorporate attributes into their method, we adjust the similarity of two users based on their attribute sets, which leads to better performance.
\end{itemize}
	
	\noindent Note that for all model-based methods, we optimize them with the same pair-wise ranking function of Eqn.\eqref{equ:information-obj} for a fair comparison on the model's expressiveness.
	To explore the efficacy of attributes, we further explore variants that remove attribute modelling from SFM, SR, and NSCR, named as SFM-a, SR-a, and NSCR-a, respectively.


	\textbf{Parameter Settings:}~We implemented our proposed framework on the basis of Tensorflow\footnote{\url{https://www.tensorflow.org}.}, which will be made publicly available, as well as our datasets. For all the neural methods, we randomly initialized model parameters with a Gaussian distribution, whereinto the mean and standard deviation is  and , respectively. The mini-batch size and learning rate for all methods was searched in  and , respectively. We selected Adagrad as the optimizer. Moreover, we empirically set the size of hidden layer same as the embedding size (the dimension of the latent factor) and the activation function as ReLU. Without special mention, we employed two hidden layers for all the neural methods, including SFM, SR, and NSCR. We randomly generated ten different initializations and feed them into our NSCR. For other competitors, the initialization procedure is analogous to ensure the fair comparison. Thereafter, we performed paired t-test between our model and each of baselines over -round results.




	








\subsection{Performance Comparison (RQ1)}
We first compare the recommendation performance of all the methods. We then purpose to justify how the social modelling and the attribute modelling affect the recommendation performance.

\begin{table}[t]
	\centering
	\caption{Performance comparison between all the methods, when the embedding size and signiﬁcance test is based on AUC.}
	\vspace{-5pt}
	\label{tab:p-value}
	\resizebox{0.46\textwidth}{!}{\begin{tabular}{|c|c|c|c|c|c|c|}
			\hline
			\textbf{Datasets} & \multicolumn{3}{c|}{\textbf{Twitter-Trip}}                       & \multicolumn{3}{c|}{\textbf{Facebook-Trip}}                      \\ \hline
			\textbf{Methods}  & \textbf{AUC}   & \textbf{R@} & \textbf{-value} & \textbf{AUC}   & \textbf{R@} & \textbf{-value} \\ \hline\hline
			\textbf{ItemPop}  &        &              & -                         &        &              & -                         \\ \hline
			\textbf{MF}       &        &              & -                         &        &              & -                         \\ \hline
			\textbf{SFM}      &        &              & -                         &        &              & -                         \\ \hline
			\textbf{SR}       &        &              & -                         &        &              & -                         \\ \hline
			\textbf{NSCR}     &  &        & -                        &  &        & -                        \\ \hline
		\end{tabular}}
		\vspace{-1em}
	\end{table}
	
	\begin{figure*}[t]
		\centering
\subfigure[AUC on Twitter-Trip]{
			\includegraphics[width=0.235\textwidth]{Chart/without-attr-tw-auc.pdf}
			\label{fig:without-attr-tw-auc}}
		\subfigure[R@5 on Twitter-Trip]{
			\includegraphics[width=0.235\textwidth]{Chart/without-attr-tw-r.pdf}
			\label{fig:without-attr-tw-auc}}
		\subfigure[AUC on Facebook-Trip]{
			\includegraphics[width=0.235\textwidth]{Chart/without-attr-fb-auc.pdf}
			\label{fig:without-attr-fb-auc}}
		\subfigure[R@5 on Facebook-Trip]{
			\includegraphics[width=0.235\textwidth]{Chart/without-attr-fb-r.pdf}
			\label{fig:without-attr-fb-auc}}
		\vspace{-10pt}
		\caption{Performance comparison of AUC and R@ \wrt the embedding size on Twitter-Trip and Facebook-Trip datasets.}
		\vspace{-1em}
		\label{fig:performance-social}
	\end{figure*}
	
	\begin{figure*}[h]
		\centering
\subfigure[AUC on Twitter-Trip]{
			\includegraphics[width=0.235\textwidth]{Chart/with-attr-tw-auc.pdf}
			\label{fig:with-attr-tw-auc}}
		\subfigure[R@5 on Twitter-Trip]{
			\includegraphics[width=0.235\textwidth]{Chart/with-attr-tw-r.pdf}
			\label{fig:with-attr-tw-auc}}
		\subfigure[AUC on Facebook-Trip]{
			\includegraphics[width=0.235\textwidth]{Chart/with-attr-fb-auc.pdf}
			\label{fig:with-attr-fb-auc}}
		\subfigure[R@5 on Facebook-Trip]{
			\includegraphics[width=0.235\textwidth]{Chart/with-attr-fb-r.pdf}
			\label{fig:with-attr-fb-auc}}
		\vspace{-10pt}
		\caption{Performance comparison of AUC and R@ \wrt the embedding size on Twitter-Trip and Facebook-Trip datasets.}
		\vspace{-1em}
		\label{fig:performance-attribute}
	\end{figure*}
	
	\textbf{Overall Comparison:}~Table~\ref{tab:p-value} displays the performance comparison \wrt AUC and R@ among the recommendation methods on Twitter-Trip and Facebook-Trip datasets, where the embedding size is  for all the methods. We have the following findings:
	\begin{itemize}[leftmargin=*]
		\item ItemPop achieves the worst performance, indicating the necessity of modelling users' personalized preferences, rather than just recommending popular items to users. As for MF, its unsatisfied performance reflects that the independence assumption is insufficient to capture the complex and non-linear structure of user-item interactions.
		\item NSCR substantially outperforms the state-of-the-art methods, SFM and SR. We further conduct one-sample t-tests, verifying that all improvements are statistically significant with -value  . It justifies the effectiveness of our proposed framework.
		\item The performance on Twitter-Trip clearly underperforms that of Facebook-Trip. It is reasonable since more bridge users are available in Facebook, which can lead to better embedding learning in SNSs. It again verifies the significance of the bridge users.
	\end{itemize}
	
	\textbf{Effect of Social Modelling:}~To analyze the effect of social modelling, we only consider the variants, SFM-a, SR-a, and NSCR-a. Figure~\ref{fig:performance-social} presents the performance comparison \wrt the number of latent factors on two datasets. We have the following observations.
	\begin{itemize}[leftmargin=*]
		\item ItemPop and MF perform worst since neither of them considers the social connections from SNSs. It highlights the necessity of social modelling in cross-domain social recommendation.
		\item Clearly, NSCR-a significantly outperforms SFM-a and SR-a by a large margin. Formally, in terms of AUC, the relative improvement over SFM-a and SR-a, on average, is  and  respectively. While SFM-a considers modelling the social connections, it treats these connections as ordinary features, overlooking the exclusive characteristics of social networks. This leads to the poor expressiveness of the social users' embedding. On the contrary, SR-a and NSCR-a emphasizes the social modelling via the effective social regularization.
		\item Lastly, NSCR-a shows consistent improvements over SR-a, admitting the importance of the normalized graph Laplacian. It again verifies that the normalized graph Laplacian can suppress the popularity of friends and further prevent the social modelling from being dominated by popular social users.
	\end{itemize}
	
	\textbf{Effect of Attribute Modelling:}~As Figure~\ref{fig:performance-attribute} demonstrates, we verify the substantial influence of attribute modelling and the effectiveness of our pairwise pooling operation. Due to the poor performance of ItemPop and MF, they are omitted. Jointly analyzing the performance of all the methods and their variants, we find that,
	\begin{itemize}[leftmargin=*]
		\item For all methods, modelling user/item attributes can achieve significant improvements. By leveraging the similarity of users' attributes, SR enriches the pairwise similarity of any two users and strengthens their connections; meanwhile, SFM can model the correlations of user-attribute, item-attribute, and attribute-attribute, and accordingly enhances the user-item interactions. Benefiting from the pairwise pooling operation, NSCR can encode the second-order interactions between user/item and attributes and boost the representation learning. The significance of attribute is consistent with~\cite{DBLP:conf/mm/ZhangZYYGC13}.
		\item Varying the embedding size, we can see that large embedding may cause overfitting and degrade the performance. In particular, the optimal embedding size is  and  for AUC and R@, respectively. It indicates that the setting of embedding size can effect the expressiveness of our model.
	\end{itemize}
	
	\begin{figure*}[t]
		\centering
\subfigure[Training Loss]{
			\includegraphics[width=0.235\textwidth]{Chart/training-loss.pdf}
			\label{fig:training-loss}}
		\subfigure[AUC]{
			\includegraphics[width=0.235\textwidth]{Chart/training-auc.pdf}
			\label{fig:auc-sensitivity}}
		\subfigure[R@]{
			\includegraphics[width=0.235\textwidth]{Chart/training-recall.pdf}
			\label{fig:r-sensitivity}}
		\vspace{-10pt}
		\caption{Training loss and recommendation performance regarding AUC and R@ \wrt the number of iterations.}
		\vspace{-1em}
		\label{fig:sensitivity-analysis}
	\end{figure*}
	
	\begin{figure*}[h]
		\centering
\subfigure[AUC vs. dropout ratio ]{
			\includegraphics[width=0.235\textwidth]{Chart/dropout-auc.pdf}
			\label{fig:dropout-auc}}
		\subfigure[R@ vs. dropout ratio ]{
			\includegraphics[width=0.235\textwidth]{Chart/dropout-r.pdf}
			\label{fig:dropout-r}}
		\subfigure[AUC vs. tradeoff parameter ]{
			\includegraphics[width=0.235\textwidth]{Chart/tradeoff-auc.pdf}
			\label{fig:tradeoff-auc}}
		\subfigure[R@ vs. tradeoff parameter ]{
			\includegraphics[width=0.235\textwidth]{Chart/tradeoff-r.pdf}
			\label{fig:tradeoff-r}}
		\vspace{-10pt}
		\caption{Performance comparison of AUC and R@ \wrt the dropout ratio  and tradeoff parameter  on Twitter-Trip and Facebook-Trip datasets.}
		\vspace{-1em}
		\label{fig:dropout-tradeoff}
	\end{figure*}
	
	\subsection{Study of NSCR (RQ2)}
	In this subsection, we empirically study the convergence of NSCR and then purpose to analyse the influences of several factors, such as dropout ratio and tradeoff parameter, on our framework.
	
	\textbf{Convergence:}~We separately present the training loss and the performance w.r.t. AUC and R@ of each iteration in Figures~\ref{fig:training-loss},~\ref{fig:auc-sensitivity}, and~\ref{fig:r-sensitivity}. Jointly observing these Figures, we can see that training loss of NSCR gradually decreases with more iterations, whereas the performance is generally improved. This indicates the rationality of our learning framework. Moreover, the most effective updates occurs in the first  iterations, which indicates that effectiveness of our learning framework. As Figure~\ref{fig:r-sensitivity} shows, the performance regarding R@ fluctuates markedly over the iteration times, while that regarding AUC is quite stable. It is reasonable since R@ only considers the top- results rather than the relative order as AUC defined.
	
	\textbf{Impact of Dropout:}~We employ the dropout technique in NSCR to prevent our model from overfitting, instead of regularizing model parameters. Figures~\ref{fig:dropout-auc} and~\ref{fig:dropout-r} present the performance \wrt AUC and R@ of NSCR- by varying the dropout ratio  on the pairwise pooling layer, respectively. As we can see, when dropout ratio equals to , NSCR- suffers severely from overfitting. Moreover, using a dropout ratio of  and  leads to the best performance on Twitter-Trip and Facebook-Trip datasets, respectively. However, when the optimal dropout ratio exceeds the optimal settings, the performance of NSCR- greatly decreases, which suffers from insufficient information. This highlights the significance of using dropout, which can be seen as ensembling multiple sub-models~\cite{DBLP:journals/jmlr/SrivastavaHKSS14}.
	
	\textbf{Impact of Tradeoff Parameter:}~There is one positive parameter  in the social modelling, which can capture the tradeoff between the fitting regularizer and the normalized graph Laplacian, as Eqn.\eqref{equ:social-training} shows. Figures~\ref{fig:tradeoff-auc} and~\ref{fig:tradeoff-r} present the performance \wrt. AUC and R@, respectively. As we can see, setting  of  and  can lead to the optimal performance on Twitter-Trip and Facebook-Trip datasets, respectively. And the performance of NSCR- changes within small ranges nearby the optimal settings. It justifies that our model is relatively insensitive to the parameter around its optimal configuration.
	
	\begin{table}[t]
		\centering
		\caption{Recommendation performance of NSCR with different hidden layers.}
		\vspace{-5pt}
		\label{tab:deep-performance}
		\resizebox{0.46\textwidth}{!}{\begin{tabular}{|c|c|c|c|c|c|c|}
				\hline
				\textbf{Metrics} & \multicolumn{3}{c|}{\textbf{AUC}}                         & \multicolumn{3}{c|}{\textbf{R@}}                       \\ \hline
				\textbf{Factors} & \textbf{NSCR-} & \textbf{NSCR-} & \textbf{NSCR-} & \textbf{NSCR-} & \textbf{NSCR-} & \textbf{NSCR-} \\ \hline\hline
				\multicolumn{7}{|c|}{\textbf{Twitter-Trip}}                                                                                              \\ \hline
				\textbf{8}       &           &           &     &           &           &     \\ \hline
				\textbf{16}      &           &           &           &           &           &     \\ \hline
				\textbf{32}      &           &           &     &           &           &     \\ \hline
				\textbf{64}      &           &           &     &           &           &     \\ \hline
				\textbf{128}     &           &           &     &           &           &     \\ \hline\hline
				\multicolumn{7}{|c|}{\textbf{Facebook-Trip}}                                                                                             \\ \hline
				\textbf{8}       &           &           &     &           &           &     \\ \hline
				\textbf{16}      &           &           &     &           &           &     \\ \hline
				\textbf{32}      &           &           &     &           &     &           \\ \hline
				\textbf{64}      &           &           &     &           &           &     \\ \hline
				\textbf{128}     &           &           &     &           &           &     \\ \hline
			\end{tabular}}
			\vspace{-2em}
		\end{table}
		
		\subsection{Impact of Hidden Layer (RQ3)}
		To capture the complex and non-linear inherent structure of user-item interactions, we employ the a deep neural network for our task. It is curious whether NSCR can benefit from the deep architecture. Towards this end, we further investigate NSCR with different number of hidden layers. As it is computationally expensive to tune the dropout ratio  for each hidden layer, we simply apply the same settings for all layers. The empirical results on two datasets are summarized in Table~\ref{tab:deep-performance} whereinto NSCR-2 indicates the NSCR method with two hidden layers (besides the embedding layer and prediction layer), and similar notations for others. We have the following observations:
		
		\begin{itemize}[leftmargin=*]
			\item In most cases, stacking more hidden layers is helpful for the recommendation performance. NSCR- and NSCR- achieve consistent improvement over NSCR-, which has no hidden layers and directly projects the embedding to the prediction layer. We attributed the improvement to the high nonlinearity achieved by stacking more hidden layers. Our finding is consistent with \cite{DBLP:conf/cvpr/HeZRS16} and again verifies the deep neural networks have strong generalization ability. However, it is worth mentioning that such a deep architecture needs more time to optimize our framework and easily leads to the overfitting due to the limited training data in our datasets.
			\item Increasing the width of hidden layers (\ie the embedding size) from  to  can improve the performance significantly, as that of increasing their depth. However, with the embedding size of , NSCR degrades the performance. It again verifies that using a large number of the embedding size has powerful representation ability~\cite{DBLP:conf/cvpr/HeZRS16}, but may adversely hurt the generalization of the model (\eg overfitting the data)~\cite{heneural,DBLP:conf/cvpr/HeZRS16}.
		\end{itemize}
		
		
		
\section{Related Work}
\subsection{Social Recommendation}
Social recommendation aims to leverage users' social connections to enhance a recommender system~\cite{DBLP:conf/wsdm/RenLLWR17,DBLP:conf/kdd/ZhangZYZHH16}.
It works by modelling social influence, which refers to the fact that a user's decision can be affected by her friends' opinions and behaviours.
Ma \etal~\cite{DBLP:conf/wsdm/MaZLLK11} propose a social regularization term to enforce social constraints on traditional recommender systems. Based on a generative influence model, the work~\cite{DBLP:conf/sigir/YeLL12} exploits social influence from friends for item recommendation by leveraging information embedded in the user social network. The authors in~\cite{DBLP:conf/www/ZhangCYNL13} utilize social links as complementary data source to mine topic domains and employed domain-specific collaborative filtering to formulate users' interests. More recently,~\cite{DBLP:journals/tkde/JiangCCW0Y15} represents a star-structured hybrid graph centered at a user domain, which connects with other item domains, and transfers knowledge on social networks.

It is worth noting that the aforementioned studies are all based on social network relations of an information domain. While in this work, we focus on how to distill useful signal from an external social network~(\eg Facebook and Twitter), so as to improve the recommendation service of any information domain.



\subsection{Cross-Domain Recommendation}
Distinct from the traditional recommendation methods that focus on data within a single domain, cross-domain recommendation concerns data from multiple domains. A common setting is leveraging the user-item interaction of a related auxiliary domain to improve the recommendation of the target domain. However, existing cross-domain recommendation work has an underlying assumption that the target and auxiliary domains are homogeneous. Depending on~\cite{DBLP:conf/www/ElkahkySH15,DBLP:journals/tkde/JiangCCW0Y15,farseev2017crossdomain}, they can be divided into two directions. One is assuming that different domains share overlapped user or item sets. The work~\cite{DBLP:conf/um/SahebiB13} augments ratings of movies and books for the shared users and accordingly conducts CF. Based on the shared users' latent space, the authors in~\cite{DBLP:conf/kdd/ChenHL13} leveraged cluster-level tensor sharing as a social regularization to bridge the domains. One more step, the authors in~\cite{DBLP:conf/www/HuCXCGZ13} formulated a generalized triadic user-item-domain relation over the common users and accordingly to capture domain-specific user factors and item factors.
More recently, the authors~\cite{DBLP:conf/www/ElkahkySH15} proposed a multi-view deep learning recommendation system by using auxiliary rich features to represent users from different domains. Without aligned user or item, the other direction is on homogeneous data with the same rating scale. Codebook Transfer~\cite{DBLP:conf/ijcai/LiYX09} represents cluster-level rating patterns between two rating matrices in two related domains.
~\cite{DBLP:conf/kdd/TangWSS12} introduces a topic model to recommend authors to collaborate from different research fields.

Despite the compelling success achieved by previous work, little attention has been paid to recommendation across heterogeneous domains. In our settings, the source domain is a social network with user-user relations only, while the target domain is an information domain with user-item interactions. Hence, the auxiliary information is the social friendship, rather than the conventional interaction data. As a result, existing approaches can be hardly applied to this new research problem.

\section{Conclusion}


In this work, we systematically investigated cross-domain social recommendation, a practical task that has rarely been studied previously.
Towards this end, we proposed a generic neural social collaborative ranking (NSCR) solution, which seamlessly integrates user-item interactions of the information domain and user-user social relations of the social domain. To validate our solution, we constructed two real-world benchmarks of the travel domain, performing extensive experiments to demonstrate the effectiveness and rationality of our NSCR solution. The key finding of the work is that social signals contain useful cues about users' preference, even if the social signals are from social networks in a different domain. We achieved the goal by leveraging bridge users to unify the relevance signals from the two heterogeneous domains.


Due to our restricted resources in collecting cross-domain data, the result is preliminary. Here we discuss several limitations of the current work, and our plans to address them in future.
First, in this work, we studied the recommendation performance of a travel-based information domain only, which is mainly for the ease of accessing the users' account on Facebook/Twitter. This results in a relatively small number of bridge users of our cross-domain datasets.
As a future work, we will collect a larger-scale set of data from the more popular information domains, such as E-commence sites, to explore the generalization ability of our solution to other information domains.
Second, due to the small number of bridge users, we forwent the study of user cold-start problem, as further holding out bridge users to simulate the cold-start scenario will pose challenge to the stability of evaluation.
With a larger-scale cross-domain data, we will study the effectiveness of our solution for cold-start users, as well as the influence of the bridge users' percentage.
Moreover, we restricted the SNSs by emphasizing only the social connections and omitting the weak user-item interactions in user-generated-contents. We will consider the weak user-item interaction in both domains to improve the recommendation performance.





\noindent {\textbf{Acknowledgement}}
\noindent We would like to thank the anonymous reviewers for their valuable comments. NExT research is supported by the National Research Foundation, Prime Minister's Office, Singapore under its IRC@SG Funding Initiative.



\bibliographystyle{abbrv}
\balance

\bibliography{my-sigir}
\balance

\end{document}
