\documentclass[sigconf]{acmart}
\settopmatter{printacmref=false} \renewcommand\footnotetextcopyrightpermission[1]{}
\usepackage{multirow}
\usepackage{ulem}
\AtBeginDocument{\providecommand\BibTeX{{Bib\TeX}}}










\begin{document}

\title{Conditional Diffusion Feature Refinement for Continuous Sign Language Recognition}







\author{Leming Guo}
\authornotemark[1]
\affiliation{\institution{School of Computer Science and Engineering, Tianjin University of Technology, Tianjin}
  \country{China}
  }
\email{glm@stud.tjut.edu.cn}

\author{Wanli Xue}
\authornotemark[2]
\affiliation{\institution{School of Computer Science and Engineering, Tianjin University of Technology, Tianjin}
  \country{China}
  }
\email{xuewanli@email.tjut.edu.cn}


\author{Qing Guo}
\affiliation{\institution{Centre for Frontier AI Research (CFAR), 
A*STAR, Singapore}
  \country{China}
  }
\email{tsingqguo@ieee.org}

\author{Yuxi Zhou}
\affiliation{\institution{School of Computer Science and Engineering, Tianjin University of Technology, Tianjin}
  \country{China}
  }
\email{joy_yuxi@pku.edu.cn}

\author{Tiantian Yuan}
\affiliation{\institution{Technical College for the Deaf, Tianjin University of Technology, Tianjin}
  \country{China}
  }
\email{yuantt@tjut.edu.cn}



\author{Shengyong Chen}
\affiliation{\institution{School of Computer Science and Engineering, Tianjin University of Technology, Tianjin}
    \country{China}
  }
\email{sy@ieee.org}


\renewcommand{\shortauthors}{Trovato et al.}

\begin{abstract}
    In this work, we are dedicated to leveraging the denoising diffusion models' success and formulating feature refinement as the autoencoder-formed diffusion process, which is a mask-and-predict scheme.
   The state-of-the-art CSLR framework consists of a spatial module, a visual module, a sequence module, and a sequence learning function.
   However, this framework has faced sequence module overfitting caused by the objective function and small-scale available benchmarks, which results in model insufficient training. 
   To overcome the overfitting problem, some CSLR studies enforce the sequence module to learn more  visual temporal information or be guided by more informative supervision to refine its representations.
   In this work, we first conduct a conditional diffusion feature refinement~(CDR) to directly produce the sequence representations with desired properties~\textit{i.e.}, the effective short-long-term temporal dependences, and be more discriminate.
   However, due to the small-scale data, the generated sequence representations by CDR are semantic corruption.
   Therefore, to overcome this problem, we  propose a novel autoencoder-formed conditional diffusion feature refinement~(ACDR) to refine the sequence representations to equip desired properties by 
   learning the encoding-decoding optimization process in an end-to-end way.
    Specifically, for the ACDR, a noising Encoder is proposed to progressively add noise equipped with semantic conditions to the sequence representations.
    And a denoising Decoder is proposed to progressively denoise the 
    noisy sequence representations with semantic conditions.
    Therefore, the sequence representations can be imbued with the semantics of provided semantic conditions. 
    Further, a semantic constraint is employed to prevent the denoised sequence representations from semantic corruption.
    Notice that the ACDR can be viewed as an additional component applied to other methods with a similar framework.
   Extensive experiments are conducted to validate the effectiveness of our ACDR, benefiting state-of-the-art methods and achieving a notable gain on three benchmarks.
\end{abstract}


\maketitle


\begin{figure}
  \centering
  \includegraphics[width=8.3cm, height=2.5cm]{Figures/diffusion_intro.pdf}
  \caption{ Overview of the proposed ACDR approach. The ACDR conducts an encoding-decoding optimization form to add noise and denoise with semantic conditions to implicitly refine the sequence representations modeling effective short-long-term temporal dependencies and be more discriminate.  
 }
  \label{fig:fig1}
\end{figure}


\section{Introduction}
\label{sec:intro}

Sign language is a visual language that hearing-impaired people mainly rely on hand shape, hand position, hand orientation, human movement, and facial expression to understand sign language~\cite{sutton1999linguistics}. 
Further, continuous sign language recognition~(CSLR) involves capturing these elements of videos to recognize them to their corresponding sign language glosses.
Therefore, CSLR can improve the efficiency of understanding the intention of hearing-impaired people, and CSLR has received increasing attention from researchers~\cite{bragg2019sign,rastgoo2021sign}.

The current state-of-the-art framework for CSLR involves a spatial module to extract frame-wise spatial information, a visual module to capture gloss-wise temporal information, 
and a sequence module to model short-long-term temporal context and the sequence information,~\textit{i.e.}, signs order.
Finally, the baseline is optimized by the Connectionist Temporal Classification (CTC)~\cite{graves2006connectionist} loss function.
This baseline has witnessed the limitation where the sequence module is prone to overfitting, which makes its representations difficult to provide effective short-long-term temporal information for recognition and also leads to insufficient optimization information provided in backpropagation.
Further, this limitation is aggravated by small-scale datasets, resulting in a challenge to strengthening the sequence representations' generalization ability.




Currently, some methods~\cite{pu2019iterative, pu2020boosting, cheng2020fully, min2021visual,hao2021self, zuo2022c2slr} proposed temporal consistency schemes to enforce the sequence module to concentrate more on the gloss-wise temporal information learning to alleviate its overfitting. 
In addition, some works dedicate to guiding the CSLR model's representations to be more discriminated to enhance the generalization ability of the sequence representations, such as ~\cite{pu2019iterative, pu2020boosting} and ~\cite{zuo2022c2slr} employed gloss and pose keypoints location as extra supervision, and~\cite{hu2022temporal,Hu2022SelfEmphasizingNF,hu2023continuous} enforce to capture body trajectories to 
identify signs.
According to the above state-of-the-art works, we notice that the desired sequence representation should have  these desired properties,~\textit{i.e.}, effective  short-long-term temporal dependencies, and be more discriminative.
\textit{Such desired properties make access to the suitable sequence representation challenging.}

Inspired by the success of the conditional diffusion model~\cite{han2022card,2022StableDiffusion,2022imagen,Ruan2022MMDiffusionLM,Luo2022SemanticConditionalDN}, which has powerfully capable of generating the desired output equipped with the given condition~(\textit{i.e.}, the text sentence), an intuitive idea is to utilize the diffusion process fed into extra conditions to generate the desired sequence representation. 
In this work, we first conduct a conditional diffusion feature refinement in Sec.~\ref{sec_cdr} to achieve the goal via the mask-and-predict scheme.
However, limited by the small-scale training data, this refinement is unavailable to generate the desired sequence representations, even resulting in semantic corruption.


To overcome this problem, we propose an autoencoder-formed conditional diffusion feature refinement~(ACDR) that the conditional diffusion feature refinement is also adopted, and an advanced  optimization is further proposed. 
Firstly, the ACDR conducts a noising Encoder to progressively add noise equipped with semantic conditions to the sequence representations, and then the 1D-UNet will be optimized to predict the noisy sequence representations to the random Gaussian noise.
Secondly, a denoising Decoder is proposed to progressively denoise the 
the noisy sequence representations with the semantic conditions, which is able to be imbued with the semantics of provided semantic conditions. 
Further, a semantic constraint is employed to prevent the denoised sequence representations from semantic corruption.
Consequently, through the optimization of ACDR, the sequence representations can be refined to achieve these properties by 
focusing on learning the encoding-decoding process in an end-to-end way rather than directly learning the target representations.
Notice that the ACDR can be viewed as an additional component applied to all methods based on the baseline and suffering from the easy-overfitting problem.
Moreover, we also use a recent interpretation method (i.e., the compression of information
stored in weights (IIW)~\cite{PIB2022}) to evaluate the generalization capability of the ACDR-optimized sequence representations~(See the supplementary material).





\section{Related Work}
\label{sec:related work}

\subsection{Continuous sign language recognition}
\label{sec_realred_cslr}
The CSLR forces on the scenario where a video corresponds to a sequence of glosses, and the order of each gloss is consistent with signs in the video. 
Due to only sentence-level annotations and small-scale data being available in current CSLR datasets, how to utilize the data to learn stronger video representations to accurately recognize glosses, remains a fundamental research problem.
In the past, under the weakly annotated data, many state-of-the-art approaches exploit
the connectionist temporal classication (CTC)\cite{graves2006connectionist} to provide frames-wise supervision by maximizing the probabilities of all alignments paths between frames and glosses that are able to achieve by a mini-batch optimized. 
On the other hand, many state-of-the-art  methods~\cite{cui2019deep,min2021visual,hao2021self,hu2022temporal,Hu2022SelfEmphasizingNF,CVTSLR2023,hu2023continuous} consider that the baseline involves a spatial module, a visual module, a sequential module, and the CTC constraint can achieve effective performance.
However, this baseline has faced an insufficient optimizing problem caused by the eazy-overfitting sequence module, whose pool generalization representations are unable to offer sufficient temporal information to recognize signs effectively. 


To mitigate this overfitting problem, some recent methods~\cite{min2021visual,hao2021self,zuo2022c2slr} conduct temporal alignment constraints, which aim to directly refine the sequence representations to model more effective short-long-term dependencies by enforcing the sequence module focusing on learning gloss-wise temporal information.
In addition, some approaches~\cite{cheng2020fully,hao2021self,hu2022temporal,Hu2022SelfEmphasizingNF,hu2023continuous,chen2022simple,CVTSLR2023} introduce additional supervision information to enhance the CSLR model to implicitly make the sequence representations be more discriminate.
For instance, ~\cite{cheng2020fully} and ~\cite{hao2021self} produce an end-to-end epoch-wise fine-tuning strategy to produce frames-wise labels to boost the CSLR model.
~\cite{hu2022temporal} focuses on squeezing
temporal features that can capture multi-temporal patterns glosses and ~\cite{zuo2022c2slr, Hu2022SelfEmphasizingNF,hu2023continuous} aims to exploit pose heatmaps or body trajectories to dynamically
emphasize informative sign spatial regions.
~\cite{chen2022simple,CVTSLR2023} exploits the pre-trained model to provide extensive external supervision to achieve cross-modal alignment, which also improves the discrimination of the sequence representations.
Different from these methods, our method is driven to employ the powerful conditional diffusion model to implicitly refine the sequence representations to model more effective short-long-term dependencies and be more discriminate.

\subsection{Denoising diffusion models}
\label{sec_realred_diffusion_model}
The denoising diffusion model incorporates a forward Gaussian diffusion noising process and a reverse denoising generation process, which is able to refine the generated objects starting from Gaussian noise iteratively.
The denoising diffusion model has been shown to be impressive and powerful for text-to-video generation and text-to-video generation~such as Imagen~\cite{2022imagen}, DALLE-2~\cite{Ramesh2022HierarchicalTI}, Stable Diffusion~\cite{2022StableDiffusion}, Video diffusion model~\cite{Ho2022VideoDM}, Imagen Video~\cite{Ho2022ImagenVH} and MM-Diffusion ~\cite{Ruan2022MMDiffusionLM}.
In addition, denoising diffusion models have also shown significant potential in the fields of image caption~\cite{Zhu2022DDcap},~\cite{He2022DiffusionBERTIG}, image classification~\cite{han2022card}, and image segmentation~\cite{Le2023MaskDiffMM}, \cite{Wu2023MedSegDiffV2DB}, \cite{Brempong2022DenoisingPF}. 
~\cite{Luo2022SemanticConditionalDN} and \cite{He2022DiffusionBERTIG} add noise to the text with the image conditions to learn the forward diffusion process, and then generate the corresponding caption according to the image conditions in the reverse diffusion process.
~\cite{han2022card}, \cite{Le2023MaskDiffMM}, \cite{Wu2023MedSegDiffV2DB}, \cite{Brempong2022DenoisingPF} also regard the image as the condition and add noise to the ground-truth to lean the forward diffusion process, and finally generate the corresponding ground-truth via the denoising diffusion process.
In this work, we employ the denoising diffusion model strategy to refine the sequence representations equipped with semantic conditions to model practical short-long-term temporal dependencies and be more discriminative. 



\begin{figure*}
  \centering
  \includegraphics[width=17.5cm, height=5cm]{Figures/dif_pipeline.pdf}
  \caption{Pipeline of the proposed ACDR.
  The SCPM focuses on improving feature diversity. The ACDR is an autoencoder-formed framework containing a noising Encoder and a denoising Decoder. The Encoder works on adding noise with semantic conditions to sequence representation and learns added noise. The Decoder aims to denoising the noised representation with semantic conditions to achieve semantic refinement and reserve the original semantic.}
  \label{Fig:pipline}
\end{figure*}

\section{Preliminaries and Analysis}
\label{sec_pa}

\subsection{General Baseline of CSLR}
\label{sec_base}
One sign language video is always made up of contiguous signs and their corresponding glosses, which are two sequences following the sign language syntactic structure~\cite{sutton1999linguistics}.
Current state-of-the-art CSLR works~\cite{min2021visual,hao2021self,zuo2022c2slr,hu2022temporal,Hu2022SelfEmphasizingNF,hu2023continuous,CVTSLR2023} have noticed that the high-performance baseline framework~ always follows a cascaded architecture, it comprises a spatial module , a visual module , a sequence module  and 
a sequence learning function .

Formally, given one sign language video sequence with  frames , and its ground-truth sentence with  glosses , which is annotated at sentence-level, the CSLR task dedicates to learn an alignment . 
Specifically, the 2DCNN ~\textit{i.e.}, ResNet18~\cite{inproceedingsResnet} is leveraged to be , ,  are empirically set to variants or normal temporal convolution networks~(1D-TCNs)~\cite{min2021visual,hao2021self,zuo2022c2slr,hu2022temporal,chen2022simple,Hu2022SelfEmphasizingNF,hu2023continuous} and Bi-Lstm~\cite{min2021visual,hao2021self,zuo2022c2slr,hu2022temporal,Hu2022SelfEmphasizingNF,hu2023continuous,CVTSLR2023}, and  is set to the connectionist temporal classification~(CTC) loss~\cite{min2021visual,hao2021self,zuo2022c2slr,chen2022simple,hu2022temporal,Hu2022SelfEmphasizingNF,chen2022two,hu2023continuous,CVTSLR2023}.


In practice,  first extracts spatial features  from .
Then,   will model gloss-wise temporal features  from  by establishing adjacent
frames correlations and learn sign-specific knowledge.
Besides,  will excellently model sequential and contextual correlation across  to learn sequence representation .
Moreover,  are fed into the classifier  to predict corresponding logits .
Finally,  enables to alignments between  and ~ , and computes losses.
Specifically, during test stage, the predicted ground-truth sequence  will be derived  from   via the learned   straightly.



\subsection{Conditional Diffusion Feature Refinement}
\label{sec_cdr}


Due to the small-scale data and the CTC optimization, the easy-overfitting problem of the sequence module in the state-of-the-art baselines occurs, which affects the final prediction performance. 
To overcome this problem, some methods ~\cite{pu2019iterative, pu2020boosting, cheng2020fully, min2021visual,hao2021self, zuo2022c2slr} introduce extra supervision to enforce the sequence module learn more to produce representations with effective short-long-term temporal dependencies and be more discriminative.
Inspired by the success of the diffusion model~\cite{2022imagen,Ramesh2022HierarchicalTI,2022StableDiffusion,Ho2022VideoDM,Ho2022ImagenVH,Ruan2022MMDiffusionLM,He2022DiffusionBERTIG,Le2023MaskDiffMM,han2022card}, we explore imposing an end-to-end feature refinement method,~\textit{i.e.}, condition diffusion feature refinement~(CDR), which utilizes the diffusion process fed into extra conditions to generate the desired sequence representations. 
And we first conduct preliminaries to study the feature refinement method. 
The condition diffusion feature refinement comprises a diffusion module~(1D-UNet), the noising refinement process, and the denoising refinement process.
And to achieve the desired properties, the condition diffusion feature refinement extracts the gloss-wise temporal information and gloss information as semantic conditions for the semantic complementarity in terms of both local temporal context and categorical correlation.



\noindent \textbf{Gloss-wise temporal condition}. Specifically, we extract the channel knowledge from the gloss-wise temporal features~ via the softmax operation to conduct the gloss-wise temporal condition~:



\noindent \textbf{Gloss condition}.
Because the gloss feature representation can be regarded as the discriminative and important feature that spans many dimensions in visual feature space. 
, we quantify the sequence representation~ to produce the gloss condition~, inspired by the VQ-VAE~\cite{VQVAE2017}. 
Specifically, we first conduct a codebook via a pre-trained gloss embedding~\cite{chen2022two} to provide discrete latent variables~, which follow categorical distributions. 
Then the quantified representation  will be computed by the nearest neighbor look-up between  sequence representations~ and discrete latent variables ~ in the codebook.

where the look-up operation is the representation~  mapped onto the nearest element of gloss embedding ~.

\noindent\textbf{The diffusion noising process.}
For this process, the CSLR baseline in Sec.~\ref{sec_base} is adopted as the backbone to extract the sequence representation . 
And then, the Gaussian noise~ is incrementally added to the sequence representation~~() conditioned on the diffusion step  and semantic conditions , which can be represented
as the Markov chain .
As a result, all intermediate noisy latent variables are denoted as , and  is the noisy sequence representation with Gaussian distribution.
Specifically, the diffusion step  is sampled from a uniform distribution of .
And the diffusion schedule for Gaussian noise added to the sequence representation~ at diffusion step  is pre-defined as a linear noise schedule~.
Consequently, inspired by the CARD~\cite{han2022card}, given the semantic condition , the noising process conditional distributions are specified in the same fashion as the CARD~\cite{han2022card}:

\noindent where all diffusion steps including~, , and , the semantic condition  can be set to a linear combination~. 
Furthermore, given the sequence representation  and the semantic conditions  and , and the 1D-UNet~, the diffusion noising process can be optimized by the condition reconstruction regularization~:


\noindent where in the 1D-UNet, all convolution layers have 1D kernel size, and the 1D-UNet is optimized to learn the added noise at each denoise step~ via the condition reconstruction regularization.





\noindent\textbf{The diffusion denoising process.} 
In this part, we also follow the denoising form of~\cite{han2022card} to achieve the denoising process. 
For Equation~\ref{Eq:yt}, such formulation corresponds to a tractable noising process posterior:

\noindent where 

\noindent and where , , , and .

Based on the noising process posterior~(Equation~\ref{Eq:posterior}), 
the predicted noise representations~ produced by the 1D-UNet~ will be denoised with semantic conditions~ by a  Markov chain process over variables  to generate the desired sequence representations~ owning the semantic supervision of both gloss-wise temporal features and gloss features.
The diffusion denoising process can be formulated as follows:



\noindent where if , . Intuitively, through the denoising process, the denoised sequence representations  are able to be imbued with the semantics of provided semantic conditions. And  will be fed into the classifier~ to generate the final prediction and then optimized by the CTC loss.

\subsection{Problems and Motivations}
\noindent \textbf{Problem of the current diffusion feature refinement.}
In the above preliminaries, we aim to develop the desired sequence representations~, which should contribute more to effective short-long-term temporal dependencies, and be more discriminative, thus performing more effectively.
However, as shown in Table~\ref{Table:cdr}, we observe that the denoised sequence representations~ is unable to achieve an improvement, even obtains quite worse performance, and the CTC loss is difficult to achieve good convergence.
As a result, we explore evaluating the quality of ~  by adopting the maximum-mean discrepancy (MMD) and the KL-divergence to measure the difference from the initial sequence representations~.
And we observe that both the MMD value and the KL loss are very large.
These experiments phenomenons demonstrate that limited by the small-scale training data, the denoised sequence representations~ can not have the desired optimization via the conditional diffusion feature refinement.
The original semantics of the sequence module in the denoised sequence representations~ may be broken, leading to bad results.
Our aim is to achieve semantic complementation for the feature refinement rather than semantic corruption, which defeats our original intent.
\setlength{\tabcolsep}{5pt}
\begin{table}[!htbp]
\centering
\fontsize{9}{12}\selectfont
\caption{Ablation study on the CDR and compatibility to other stat-of-the-art CSLR methods on the {RWTH-2014} dataset. {C} and {C} indicate employing the gloss-wise temporal condition and gloss condition in the CDR optimization.}
\begin{tabular}{c|cc|cc} 
\toprule
Methods & {C} & {C} & Dev~(\%)~ & Test~(\%)~ \\ \midrule \multirow{1}{*}{VAC~\cite{min2021visual}} & ~ & ~ & 21.2 & 22.3 \\ +\textbf{CDR} & \checkmark & \checkmark & 94.5 & 94.8 \\ \midrule
        \multirow{1}{*}{VAC+SMKD~\cite{min2021visual,hao2021self}} & ~ & ~ & 19.8 & 20.8 \\ +\textbf{CDR} & \checkmark & \checkmark & 93.2 & 93.9 \\ \midrule
        \multirow{1}{*}{TLP~\cite{hu2022temporal}} & ~ & ~ & 19.7 & 20.8 \\ +\textbf{CDR} & \checkmark & \checkmark & 93.3 & 93.6 \\ \midrule
       \multirow{1}{*}{SEN~\cite{Hu2022SelfEmphasizingNF}}  & ~ & ~ & 19.5 & 21.0 \\ +\textbf{CDR} & \checkmark & \checkmark & 93.1 & 94.7 \\ \midrule
        \multirow{1}{*}{CorrNet~\cite{hu2023continuous}} & ~ & ~ & 19.0 & 19.7 \\ +\textbf{CDR} & \checkmark & \checkmark & 92.5 & 93.4 \\ \bottomrule
    \end{tabular}
    \label{Table:cdr}
\end{table}


\noindent \textbf{Motivations.}
According to the above preliminaries, limited by the small-scale training data, it is unavailable for the conditional diffusion feature refinement to generate the desired sequence representations.
Because the sequence representations are directly related to the final accuracy, the key problem becomes \textit{how to design an advanced optimization strategy to refine the sequence representations.}
Consequently, all the above preliminaries motivate us to develop an advanced conditional diffusion feature refinement, which enforces the sequence representations to emphasize effective short-long-term temporal dependencies, being more discriminative and reserving the original semantic distribution.  





\section{Autoencoder-formed Conditional Diffusion Feature Refinement}
\label{sec_ACDR}
In this section, inspired by the MAE that achieves feature refinement by learning the denoising process, we propose an autoencoder-formed conditional diffusion feature refinement~(ACDR) to
implicitly refine the sequence representations by 
focusing on learning the encoding-decoding process.
Figure~\ref{Fig:pipline} shows the illustration of the ACDR framework.
Specifically, the ACDR conducts a noising Encoder for noise learning, which involves the diffusion noising process in~Sec.~\ref{sec_cdr} and conducts a denoising Decoder for denoised representation constraint, which includes the diffusion denoising process in~Sec.~\ref{sec_cdr}.
  


\noindent \textbf{Noising Encoder.}
Given the sequence representation ~(), the 1D-UNet~, and the semantic conditions~, the noising Encoder will first obtain the noisy sequence representations~ via the Equation~\ref{Eq:yt} of the diffusion noising process in Sec.~\ref{sec_cdr}.
Further, to emphasize the gloss-wise temporal condition~~() and the gloss condition~~(), we also apply the diffusion noising process on them to generate corresponding noisy variables  and .
Then, ,  and  are concatenated with their corresponding non-noisy variables ,  and  on the channel dimension and project them into a latent space respectively to recover the dimension.
Finally, three projected representations are fed into the 1D-UNet and produce the predicted noise representations ,  and . 
,  and  will be optimized by the condition reconstruction regularization~~(Equation~\ref{Eq:crr}) to learn noises at each diffusion step~.
Specifically, because  and  have no condition when acting on them, the  part in ~~(Equation~\ref{Eq:crr}) will be replaced with themselves.

\noindent \textbf{Denoising Decoder.}
Based on the diffusion denoising process in Sec.~\ref{sec_cdr} and the noising Encoder process, the denoising Decoder also meets the tractable noising process posterior~(see~Equation~\ref{Eq:posterior}).
Therefore, given the noisy sequence representations~, the semantic conditions~, the denoising Decoder first denoises the predicted noise representations  with diffusion step  to generate the denoised representations  via the Equation~\ref{Eq:v_hat_0} and the Equation~\ref{Eq:v_hat_t_1}.
Through this denoising, the denoised sequence representations  are able to be imbued with the semantics of provided semantic conditions.
It is also notable that because the noising process~(Equation~\ref{Eq:v_hat_0}) in the noising Encoder follows the Markov chain, as a result, Equation~\ref{Eq:v_hat_0} and Equation~\ref{Eq:v_hat_0} can naturally adopt the DDIM sampler~\cite{2021ddim} for fast denoising process.
Furthermore, the denoising Decoder is additionally optimized by the semantic constraint~ to reserve the semantic of the original sequence representation~.



\noindent where  is set to the feature distribution measurement, such as the joint multiple kernel maximum-mean discrepancy~(JMMD), the maximum-mean discrepancy (MMD), or the mean squared loss~(MSE).
And ~ dedicated to preventing the distribution of the denoised sequence representation ~ from having semantic corruption.
In this way, the ACDR is able to implicitly refine the sequence representations~ end-to-end by allowing  to learn the encoding-decoding optimization process, rather than explicitly learning the complicated target features.

\noindent \textbf{Objective.}
Consequently, the objective of the ACDR can be formulated as follows:

\noindent where the  and  are hyperparameters for balance the contribution of the~. 
Further, while optimizing the sequence representation, the ACDR can also optimize the gloss-wise temporal and spatial representation by back-propagation schemes, enhancing their power.
It is worth noticing that the ACDR can be viewed as an additional component that can be applied to all methods based on the baseline and suffering from the easy-overfitting problem, being able to enhance their sequence representation and alleviate the overfitting problem.







\setlength{\tabcolsep}{5pt}
\begin{table}[!htbp]
\centering
\fontsize{9}{12}\selectfont
\caption{Compatibility to other stat-of-the-art CSLR methods on the~{RWTH-2014} dataset. Adding ACDR leads to a consistent performance boost. The best results and ACDR results are marked as \textbf{bold}
and \uline{underlined}, respectively.}
\begin{tabular}{c|c|c|c|c} 
\toprule
\multirow{2}{*}{Methods}  &\multicolumn{2}{c|} {Dev~(\%)~} &\multicolumn{2}{c}{Test~(\%)~} \cr \cline{2-5}
         &del/ins    &WER          &del/ins        &WER    \cr      \midrule
CMA\cite{pu2020boosting}            &7.3/2.7 & 21.3  &7.3/2.4 & 21.9 \cr SMKD\cite{hao2021self}              &6.8/2.5 & {20.8}  &6.3/2.3 & {21.0} \cr \cite{zuo2022c2slr} &- & {20.5}  &- & {20.4} \cr 
        CVT-SLR~\cite{CVTSLR2023} &6.4/2.6 & {19.8}  &6.1/2.3 & {20.1} \cr TwoStream-SLR\cite{chen2022two}              &- & \textbf{18.4}  &- & \textbf{18.8} \cr \midrule
        VAC\cite{min2021visual}              &7.9/2.5 & {21.2}  &8.4/2.6 & 22.3 \cr
        +\textbf{ACDR} &5.5/3.5 & \uline{20.5}  &5.8/2.9 & \uline{20.6} \cr \midrule
        SEN~\cite{Hu2022SelfEmphasizingNF} &5.8/2.6 & {19.5}  &7.3/4.0 & {21.0} \cr +\textbf{ACDR} &5.5/2.6 & \uline{18.8}  &5.4/2.7 & \uline{20.0} \cr \midrule
        TLP\cite{hu2022temporal}              &6.3/2.8 & {19.7}  &6.1/2.9 & {20.8} \cr +\textbf{ACDR}              &5.2/2.9 & \uline{19.0}  &5.2/3.1 & \uline{20.0} \cr \midrule
        CorrNet\cite{hu2023continuous}              &5.6/2.8 & {19.0}  &5.7/2.3 & {19.7} \cr +\textbf{ACDR}              &4.6/2.9 & \uline{18.6}  &5.3/2.6 & \uline{19.0} \cr 

        \bottomrule
    \end{tabular}
    \label{Table:2014}
\end{table}


\setlength{\tabcolsep}{1pt}
\begin{table}[!htbp]
\centering
\fontsize{9}{12}\selectfont
\caption{Compatibility to other stat-of-the-art CSLR methods on the~{RWTH-2014T} and {CSL-Daily} datasets. Adding ACDR leads to a consistent performance boost. The best results and ACDR results are marked as \textbf{bold}
and \uline{underlined}, respectively.}
\begin{tabular}{c|cc|cc} 
\toprule
\multirow{2}{*}{Methods}   & \multicolumn{2}{c|}{RWTH-2014T} &\multicolumn{2}{c}{CSL-Daily} \cr  ~ & Dev~(\%)~ & Test~(\%)~ & Dev~(\%)~ & Test~(\%)~ \\  \midrule FCN\cite{cheng2020fully}  &23.3 &25.1 & 33.2 & 32.5 \cr BN-TIN\cite{zhou2021improving}  &22.7 &23.9 & 33.6 & 33.1 \cr {SLT}\cite{camgoz2020sign} &24.6 &24.5 &33.1 &32.0 \cr {V-L Mapper}\cite{chen2022simple} &21.9 &22.5 & - & - \cr \cite{zuo2022c2slr}  &{20.2} &{20.4} & - & - \cr
        TwoStream-SLR\cite{chen2022two}  &\textbf{17.7} &\textbf{19.3}  & 25.4 & 25.3 \\  \midrule TLP~\cite{hu2022temporal}   & 19.4 & 21.2 & - & - \\ ~+\textbf{ACDR}   & \uline{17.9} & \uline{20.4} & - & - \\ \midrule SEN~\cite{Hu2022SelfEmphasizingNF}   & 19.3 & 20.7 & 31.1 & 30.7 \\ ~+\textbf{ACDR}  & \uline{18.5} & \uline{20.1} & \uline{30.0} & \uline{29.4} \\ \midrule CorrNet~\cite{hu2023continuous}   & 18.9 & 20.5 & {30.6} & {30.1} \\ ~+\textbf{ACDR}   & \uline{18.3} & \uline{20.0} & \uline{29.6} & \uline{29.0} \\ \bottomrule
    \end{tabular}
    \label{Table:2014t_csl}

\end{table}

\setlength{\tabcolsep}{9pt}
\begin{table}[!htbp]
\centering
\fontsize{9}{12}\selectfont
\caption{Performance comparison of distinct feature refinement methods on the {RWTH-2014} dataset~(Upper).
And the performance comparison of distinct semantic constraint~~(Bottom). {A} and {A} indicate employing the gloss-wise temporal condition and gloss condition in the ACDR optimization.}
\begin{tabular}{c|c|c|c} 
\toprule
Methods & Knowledge & Dev~(\%)~ & Test~(\%)~ \\ \midrule \multirow{5}{*}{VAC} & - & 21.1 & 22.3 \\ ~ &  & 20.8 & 21.0 \\\cmidrule{2-4} ~ &  & 20.7 & 20.6 \\ ~ & JMMD & 20.7 & 21.4 \\ ~ & DTW & 20.5 & 20.7 \\ \midrule \multirow{3}{*}{SEN} & MMD & 19.4 & 20.2 \\ ~ & MSE & 19.6 & 19.8 \\ ~ & JMMD & 19.4 & 19.8 \\ \bottomrule
    \end{tabular}
    \label{Table:abl_konwledge}
\end{table}











\section{EXPERIMENTS}
\subsection{Datasets and Evaluation}
\label{sec_Data_implement}
\noindent\textbf{RWTH-2014 \cite{koller2015continuous}.} It consists of video recordings of weather forecast sign language interpreters delivering 6,842 sentences interpreted by 9 signers, composed of 1,295 sign language vocabulary.


\noindent\textbf{RWTH-2014T\cite{camgoz2018neural}.} It is a widely utilized resource for continuous sign language recognition~(CSLR) and sign language translation~(SLT) tasks. It contains 1,085 vocabularies for the CSLR task. All videos are divided into 7,096, 519, and 642 videos for the training, development, and test sets.




\noindent\textbf{CSL-Daily\cite{zhou2021improving}.} It is a large Chinese CSLR dataset for both continuous sign language recognition and sign language translation~(SLT) tasks. 
It records 2000 vocabulary and it is divided into 18,401, 1,077, and 1,176 videos for the training, development, and  test sets for the CSLR task.

\noindent \textbf{Evaluation metric.} 
In this work, the word error rate~(WER) metric is adopted for the CSLR evaluation. The WER belongs to the edit distance, which measures the minimum number of substitutions~(\#sub), deletions~(\#del), and insertions~(\#ins) operation needed to convert the predicted sentence to the associated reference sentence. 
The WER calculation method is as follows:

\noindent where  are the number of substitutions, deletions, and insertions operation, respectively. 


\subsection{Implementation Details}

\noindent \textbf{Evaluators.}
In this work, to evaluate the proposed autoencoder-formed conditional diffusion feature refinement~(ACDR) by adding it 
to the current state-of-the-art methods as a component.
Specifically, these evaluators have the same baseline framework as the baseline in ~Sec.~\ref{sec_base} and encounter the overfitting problem, such as VAC~\cite{min2021visual}, TLP~\cite{hu2022temporal}, SEN~\cite{Hu2022SelfEmphasizingNF}, and  CorrNet~\cite{hu2023continuous}.
For both the training and test stages, we follow the data augmentation of the above evaluators.


\noindent \textbf{Autoencoder-formed conditional diffusion feature refinement.}
In the ACDR, the channels of 1D-UNet are set to  .
The gloss embedding for the gloss condition in Sec.~\ref{sec_cdr} is an embedding layer of one pre-trained model adopted in~\cite{chen2022simple} that has fine-tuned
on the CSLR benchmarks~\cite{koller2015continuous,camgoz2018neural,zhou2021improving}.
The diffusion step for noising process in the noising Encoder in Sec.~\ref{sec_ACDR} is set to .
And for the denoising Decoder in Sec.~\ref{sec_ACDR},  is set to the joint multiple kernel maximum-mean discrepancy~(JMMD), the DDIM sampler is used for fast sampling, and the diffusion step for the denoising process can be set to ~(Take 20 random  from  at intervals of 1000/20).
The  in  of equations can be set to , and we also conduct ablation studies to evaluate the impact of different .
The  and  in Equation~\ref{Eq:acdr} are set to  and , respectively, and we also conduct ablation studies to evaluate the impact of different  and .

\noindent \textbf{Training.}
All evaluators are trained for 50 epochs and optimized by Adam optimizer~\cite{kingma2014adam} with an initial learning rate of , a weight decay factor of , and batch size of .
And the learning rate decays~(0.2) at 25 and 40 epochs.
All experiments are implemented in PyTorch and on one A100 GPU.

\subsection{Compatibility to stat-of-the-art methods}
\label{sec:sota}

\noindent \textbf{Evaluation on RWTH-2014}.
As shown in Table~\ref{Table:2014}, we can see that with the ACDR optimization, the state-of-the-art methods~VAC~\cite{min2021visual},
TLP~\cite{hu2022temporal}, SEN~\cite{Hu2022SelfEmphasizingNF}, and CorrNet~\cite{hu2023continuous} achieve a remarkably performance improvement~(nearly 1\%). 
In particular, the performance of CorrNet is near the multi-modal method TwoStream-SLR~\cite{chen2022two}, which utilizes the pre-captured key points of signs as supervision to help the model learning.

\noindent \textbf{Evaluation on RWTH-2014T}.
Table~\ref{Table:2014t_csl} also shows that adding ACDR leads to a consistent performance boost on the RWTH-2014T dataset. 
Especially, the TLP~\cite{hu2022temporal} achieves the largest improvement compared to other methods.
We consider that the TLP focuses on capturing various temporal pattern signs to preserve discriminative gloss-wise temporal representations, which can provide an effective  gloss-wise temporal condition~ for the ACDR.

\noindent \textbf{Evaluation on CSL-Daily}.
As shown in Table~\ref{Table:2014t_csl}, when adopting the ACDR, both SEN\cite{Hu2022SelfEmphasizingNF} and CorrNet~\cite{hu2023continuous} outperform their original method by a large margin~(about~1.1\%).
These experiments validate the effectiveness of the ACDR.

\subsection{Ablation Study}
\label{sec:ablation}
In this section, all experiments, except Table~\ref{Table:ablation}, are based on the SEN~\cite{Hu2022SelfEmphasizingNF} on the RWTH-2014 dataset.

\setlength{\tabcolsep}{5pt}
\begin{table}[!htbp]
\centering
\fontsize{9}{12}\selectfont
\caption{Ablation study on the ACDR and compatibility to other stat-of-the-art CSLR methods on the {RWTH-2014} dataset. Adding ACDR leads to a consistent performance boost. {A} and {A} indicate employing the gloss-wise temporal condition and gloss condition in the ACDR optimization.}
\begin{tabular}{c|cc|cc} 
\toprule
Methods & {A} & {A} & Dev~(\%)~ & Test~(\%)~ \\ \midrule \multirow{4}{*}{Baseline} & ~ & ~ & 23.8 & 25.4 \\  ~ & \checkmark & ~ & 23.3 & 24.2 \\ ~ & ~ & \checkmark & 23.1 & 24.0 \\   
& \checkmark & \checkmark & 22.9 & 23.8 \\  \midrule \multirow{4}{*}{VAC~\cite{min2021visual}} & ~ & ~ & 21.2 & 22.3 \\ ~ & \checkmark & ~ & 20.8 & 21.0 \\ ~ & ~ & \checkmark & 20.6 & 20.6 \\ ~ & \checkmark & \checkmark & 20.5 & 20.6 \\ \midrule \multirow{4}{*}{VAC+SMKD~\cite{min2021visual,hao2021self}} & ~ & ~ & 19.8 & 20.8 \\ ~ & \checkmark & ~ & 20.1 & 20.3 \\ ~ & ~ & \checkmark & 19.8 & 20.2 \\ ~ & \checkmark & \checkmark & 19.6 & 20.1 \\ \midrule \multirow{4}{*}{TLP~\cite{hu2022temporal}} & ~ & ~ & 19.7 & 20.8 \\ ~ & \checkmark & ~ & 19.3 & 20.3 \\ ~ & ~ & \checkmark & 19.2 & 20.2 \\ ~ & \checkmark & \checkmark & 19.0 & 20.0 \\ \midrule \multirow{4}{*}{SEN~\cite{Hu2022SelfEmphasizingNF}}  & ~ & ~ & 19.5 & 21.0 \\ ~ & \checkmark & ~ & 19.7 & 20.4 \\ ~ & ~ & \checkmark & 19.1 & 20.2 \\ ~ & \checkmark & \checkmark & 18.8 & 20.0 \\ \midrule \multirow{4}{*}{CorrNet~\cite{hu2023continuous}} & ~ & ~ & 19.0 & 19.7 \\ ~ & \checkmark & ~ & 18.9 & 19.2 \\ ~ & ~ & \checkmark & 18.7 & 19.1 \\ ~ & \checkmark & \checkmark & 18.6 & 19.0 \\ \bottomrule
    \end{tabular}
    \label{Table:ablation}
\end{table}

\noindent \textbf{Ablation on ACDR.}  Table~\ref{Table:ablation} ablates the performance when varying the semantic conditions~ and .
Adding both the gloss-wise temporal condition~ and the gloss condition~ can bring better performance~(0.6\% and 0.8\% on the test set for SEN, 0.5\% and 0.6\% on the test set for CorrNet, 
and 0.6\% and 1.7\% on the test set for VAC).
Notably, adopting the gloss condition~ outperforms the gloss-wise temporal condition~ by a small gain.
This phenomenon is particularly prominent in the results of VAC.
We consider that both TLP~\cite{hu2022temporal}, SEN~\cite{Hu2022SelfEmphasizingNF}, and CorrNet~\cite{hu2023continuous} squeeze
more representative temporal features to locate signs with various lengths more precisely, resulting in powerful ~. 
However, VAC~\cite{min2021visual} only aligns the gloss-wise temporal representation to the sequence module representations, which cannot capture signs with various lengths to provide powerful ~.
Furthermore, the pre-trained language embedding layer extracts the gloss condition~, which has fine-tuned the CSLR datasets. 
As a result, the gloss condition~ can offer 
rich context information and powerful discriminative information among glosses result in a better performance than~.
Moreover, adopting both conditions ~ and ~ achieves further performance improvement, which verifies the effectiveness of exploiting the ACDR process to refine the sequence representations. 
And results in Table~\ref{Table:ablation} also validate that employing the conditional diffusion process to achieve feature refinement for CSLR is feasible.



\setlength{\tabcolsep}{5pt}
\begin{table}[!htbp]
\centering
\fontsize{9}{12}\selectfont
\caption{Ablation study on the  factor.}
\begin{tabular}{c|c|c|c|c|c} 
\toprule
 & 0.1 & 0.3 & 0.5 & 0.7 & 1.0 \\ \midrule Dev~(\%)~ & 19.0 & 19.1 & 18.8 & 18.9 & 19.1 \\ \midrule Test~(\%)~ & 20.2 & 20.1 & 20.0 & 20.1 & 19.7 \\  \bottomrule
    \end{tabular}
    \label{Table:gamma1}
\end{table}

\setlength{\tabcolsep}{5pt}
\begin{table}[!htbp]
\centering
\fontsize{9}{12}\selectfont
\caption{Ablation study on the  factor.}
\begin{tabular}{c|c|c|c|c|c} 
\toprule
 & 0.1 & 0.2 & 0.4 & 0.6 & 0.8 \\ \midrule Dev~(\%)~ & 18.8 & 19.1 & 19.1 & 19.2 & 19.6 \\ \midrule Test~(\%)~ & 20.0 & 20.1 & 20.3 & 20.4 & 20.7 \\  \bottomrule
    \end{tabular}
    \label{Table:gamma2}
\end{table}





\noindent \textbf{Ablation on the distinct  and  factors.}
Table~\ref{Table:gamma1} and Table~\ref{Table:gamma2} deliver the impact of the noising Encoder and the denoising Decoder from Equation~\ref{Eq:acdr}.
Specifically, for the experiments of Table~\ref{Table:gamma1}, we set the  to 0.1, and  for the experiments of Table~\ref{Table:gamma2}, we set the  to 0.5.
We can see that when  are set to a large value, resulting in worse results, and increasing  leads to worse performance than .
However, when the  is set to 1.0, its performance on the test set achieves improvement.
We consider that limited by the small-scale training data, although adopting the semantic constraint~, the model cannot fit the diffusion process well.
Therefore, the optimization of the ACDR relies more on the noising Encoder than the denoising Decoder, increasing the contribution of the noising Encoder gains better refinement.

\noindent \textbf{Ablation on the distinct  factors.}
In this part, we fix the  and  to  and , respectively.
Figure~\ref{Fig:tau} shows that the recognition performance fluctuates continuously with  factors increases on the dev set, and the best performance can be achieved when the  factors reach 0.2-0.5.
Notice that ~, therefore,  factor affects the
contribution of  and .
The above results also demonstrate that the achieve a balance contribution for  and  can achieve better performance.


\begin{figure}[h]
  \centering
  \includegraphics[width=0.9 \linewidth]{Figures/conditions_curve.pdf}
  \caption{Ablation study on distinct  factors for the linear combination of semantic conditions.}
 \label{Fig:tau}
\end{figure}

\noindent \textbf{Ablation on the distinct feature refinement methods and semantic constraints.}
As shown in Table~\ref{Table:abl_konwledge} upper part, we evaluate distinct feature refinement methods on the VAC.
We can see that when replacing the knowledge distillation loss with the ACDR gloss-wise temporal condition~, the VAC achieves 1.3\% WER improvement on the test set.
Besides, when comparing the gloss condition~ with other semantic knowledge transfer methods, the  achieves the best performance, which presents the effectiveness of the proposed diffusion refinement.
In addition, in Table~\ref{Table:abl_konwledge} bottom part, results of distinct semantic constraint~ are presented. 
We observe that the JMMD and MMD outperform the MSE loss, which demonstrates that the denoised sequence representations have equipped with the semantics, strong overly strong constraints can lead to worse results.


\begin{figure}[h]
  \centering
  \includegraphics[width=0.9 \linewidth]{Figures/add_noise.pdf}
  \caption{Ablation study on distinct diffusion steps for diffusion noising process.}
 \label{Fig:noising}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.9 \linewidth]{Figures/de_noise.pdf}
  \caption{Ablation study on distinct diffusion steps for diffusion denoising process.}
 \label{Fig:denoising}
\end{figure}

\noindent \textbf{Ablation on the diffusion step for both the noising process and denoising process.}
In this part, we incrementally decrease the  diffusion step for the noising process~(from 1000 to 50) and the denoising process~(from 1000 to 5).
Figure~\ref{Fig:noising} ablates that the recognition performance improves as the diffusion step of the diffusion noising process increases, and the best performance is achieved when the diffusion step reaches 300, with the WER of 18.8 on the dev set, after that, the performance gradually decreases.
Figure~\ref{Fig:denoising} delivers that the recognition performance decreases and then improves as the diffusion step of the diffusion denoising process increases, and the best performance is achieved when the diffusion step reaches 50 and 100, with the WER of 19.1, after that, the performance gradually decreases.







\section{CONCLUSION}
\label{sec:CONCLUSION}
    In this work, we explore refining the sequence representations via the denoising diffusion model process. 
    We first conduct a conditional diffusion feature refinement~(CDR) to directly generate the sequence representations with desired properties~\textit{i.e.}, the effective short-long-term temporal dependences, and be more discriminate.
   However, due to the small-scale training data, the generated sequence representations are semantic corruption.
   Therefore, to overcome this problem, we propose a novel end-to-end autoencoder-formed conditional diffusion feature refinement~(ACDR) to refine the sequence representations to achieve desired properties by 
   learning the encoding-decoding optimization process.
Specifically, a noising Encoder is proposed to progressively add noise equipped with semantic conditions to the sequence representations.
    And a denoising Decoder is proposed to progressively denoise the 
    noisy sequence representations with semantic conditions, which the sequence representations can
    be imbued with the semantics of semantic conditions. 
    Further, a semantic constraint is employed to preserve the original semantics of the sequence representations.
    The proposed ACDR can be viewed as an additional component applied to other methods with a similar framework.
   Experimental results validate the effectiveness of our
    proposed ACDR, which benefits state-of-the-art methods and achieves a notable gain on three benchmarks. 
\newpage
\bibliographystyle{ACM-Reference-Format}
\bibliography{cslr}


\newpage
\appendix


\section{Effects of the autoencoder-formed
conditional diffusion feature refinement}

In this section, we will further evaluate the effectiveness of the proposed autoencoder-formed conditional diffusion feature refinement~
(ACDR) by computing not only the word error rate (WER) metric but also the information stored in weights (IIW)~\cite{PIB2022} for distinct state-of-the-art methods. The experiment results are presented in Table~\ref{Table:sota_wer_iiw}.
IIW is a metric designed to compute the amount of information in the feature representation, which can indicate the generalization gap between the network loss on the training and test datasets. 
Therefore, it is an appropriate metric for measuring the generalization ability of the sequence representations. As shown in Table~\ref{Table:sota_wer_iiw}, when all state-of-the-art methods are enhanced with ACDR, they achieve lower generalization gap (lower IIW) and enhanced performance (lower WER).
It is worth noting that adding ACDR to refine the sequence representation consistently leads to lower IIW values, not only achieving a lower IIW of the sequence representation (see V-IIW), but also gaining a lower IIW of both the spatial representation and the gloss-wise temporal representation (see sp-IIW and gwt-IIW). 
These findings demonstrate that ACDR can effectively refine the sequence representation by providing informative gloss-wise dependencies and discriminative glosses context semantics via the diffusion process. Moreover, the refined sequence representations provide adequate short-long temporal dependencies to the classifier, resulting in robust feedback for both the visual module and spatial module, which implicitly enhances them and mitigates the sequence module overfitting problem.


\setlength{\tabcolsep}{4pt}
\begin{table}[!htbp]
\centering
\fontsize{9}{12}\selectfont
\caption{Compatibility ~(WER ~ and IIW~\cite{PIB2022}~ metrics) to distinct state-of-the-art methods on the {RWTH-2014} dataset. sp-IIW, {gwt}-IIW and V-IIW denote the IIW value computing by the spatial representation, the gloss-wise temporal representation, and the sequence representation.}
\begin{tabular}{c|c|c|c|c|c} 
\toprule
Methods & sp-IIW & {gwt}-IIW &V-IIW & Dev~(\%) & Test~(\%) \\ \midrule {VAC} & 1.9E-8 & 9.5E-7 & 2.5E-8 & 21.1 & 22.3 \\ {+ACDR} &  1.0E-8 & 2.1E-7 & 1.1E-8 & 20.5 & 20.6 \\ \midrule {SEN} & 1.3E-9 & 1.1E-7  & 7.8E-10 & 19.5 & 21.0 \\ {+ACDR} & 3.7E-10 & 9.4E-8 &1.7E-10 & 18.8 & 20.0 \\ \midrule 
        {TLP} & 1.8E-9 & 5.1E-8  & 2.4E-8 & 19.7 & 20.8 \\ {+ACDR} &1.1E-9 & 2.1E-8  & 2.7E-10 & 19.0 & 20.0 \\ \midrule 
       {CorrNet} & 6.3E-8 & 3.3E-7  & 6.9E-9 & 19.0 & 19.7 \\ {+ACDR} & 6.5E-9 & 8.5E-8 &1.1E-9 & 18.6 & 19.0 \\  


        \bottomrule
    \end{tabular}
    \label{Table:sota_wer_iiw}
\end{table}

\section{Qualitative complementary experiments}

\begin{figure*}
  \centering
  \includegraphics[width=0.8 \textwidth]{Figures/VAC_and_Diff.pdf}
\caption{ The visualizations of heatmaps for self-similarity matrices of gloss-wise temporal representation  and sequence representation , as well as similarity matrices between  and ~(the darker color represents the higher similarity). 
  VAC+ACDR refers to the VAC with its knowledge distillation loss (VA loss) replaced by ACDR optimization equipped with gloss-wise semantic condition .
  VAC+ACDR denotes the VAC enhanced by ACDR with both gloss-wise temporal semantic conditions and gloss semantic conditions.
 }
    
  \label{fig:self_corr_vac_diff}
\end{figure*}

In this section, to assess the quality of ACDR, we employ visualizations of heatmaps for self-similarity matrices of gloss-wise temporal representation~ and sequence representation ~, as well as similarity matrices between~ and ~ for qualitative evaluation.
Experiments are given in Figure~\ref{fig:self_corr_vac_diff}, Figure~\ref{fig:self_corr1}, Figure~\ref{fig:self_corr2}, Figure~\ref{fig:self_corr3}, Figure~\ref{fig:self_corr4}, and Figure~\ref{fig:self_corr5}.
The evaluated sample in both Figure~\ref{fig:self_corr_vac_diff} and  Figure~\ref{fig:self_corr1} is the video ``01April-2010-Thursday-heute-default-5'' in the RWTH-2014~\cite{koller2015continuous} test set.
The evaluated sample in both Figure~\ref{fig:self_corr2} and  Figure~\ref{fig:self_corr3} is the video ``15February-2011-Tuesday-heute-default-17'' in the RWTH-2014~\cite{koller2015continuous} test set.
And the evaluated sample in both Figure~\ref{fig:self_corr4} and  Figure~\ref{fig:self_corr5} is the video ``31March-2010-Wednesday-tagesschau-default-11'' in the RWTH-2014~\cite{koller2015continuous} test set.


Figure~\ref{fig:self_corr_vac_diff} illustrates the impact of the ACDR and the ACDR only equips with the gloss-wise semantic condition~ on the sequence representations of the VAC model. 
Specifically, when the VAC replaces its knowledge distillation loss (VA loss) with the ACDR optimization, the self-similarity of the gloss-wise temporal representation~ changed a little, while the self-similarity of the sequence representations~ and the similarity between  and  increase significantly. 
Notably, the similarity matrix between  and  reveals that a few features in  are highly similar to all features of .
These experimental results demonstrate that the sequence representations optimized by ACDR have the capability to generate informative gloss-wise temporal dependencies. 
Furthermore, when comparing VAC+ACDR with VAC+ACDR, we observe that introducing the gloss semantic condition leads to a remarkable similarity increase in the self-similarity matrix of ~. This finding suggests that feeding the sequence representations into the discriminative semantics among glosses can enhance their generalization ability, resulting in more robust feedback for the visual module.

To comprehensively illustrate the properties of the ACDR, more similarity matrices heatmaps visualizations of distinct state-of-the-art methods with distinct samples are shown in Figure~\ref{fig:self_corr1}, Figure~\ref{fig:self_corr2}, Figure~\ref{fig:self_corr3}, Figure~\ref{fig:self_corr4}, and Figure~\ref{fig:self_corr5}.
And we can observe that all of the results show a similar characteristic with the instance we gave in Figure~\ref{fig:self_corr_vac_diff}.
The CorrNet model's sequence representations~ have been observed to focus on the nearby gloss-wise temporal representation~, based on the SMKD~\cite{hao2021self}. After the ACDR optimization, the sequence representations exhibit an even stronger focus on the nearby gloss-wise temporal representation.
Moreover, the impact of ACDR on different models, including VAC, SEN, TLP, and CorrNet, is evident from the similarity matrix heatmaps in Figure~\ref{fig:self_corr_vac_diff} and Figures~\ref{fig:self_corr2}-\ref{fig:self_corr5}. The ACDR optimization demonstrates a more significant improvement in the VAC, SEN, and TLP models, corresponding to their improved word error rate (WER) performance.

In summary, the above experimental results indicate that ACDR can effectively refine the sequence representations by incorporating informative gloss-wise dependencies and discriminative glosses context semantics via the diffusion process, which are essential for enhancing the generalization ability. 
Moreover, the refined sequence representations will result in more robust feedback for both the visual module and spatial module, which implicitly enhances them and mitigates the sequence module overfitting problem.








\begin{figure*}[htp]
  \centering
  \includegraphics[ width=12.5cm, height=19.5cm]{Figures/Self_Correlation.pdf}
\caption{ The visualization of similarity matrix heatmaps for the gloss-wise temporal representation  and the sequence representation  of the state-of-the-art methods. The heatmaps are organized into three columns: (a) and (b) columns depict the self-similarity matrix heatmaps of  and , respectively, while the last column shows the similarity matrix heatmap between  and  ~(with darker colors indicating higher similarity).  
 }
  \label{fig:self_corr1}
\end{figure*}



\begin{figure*}[htp]
  \centering
  \includegraphics[width=0.8 \linewidth]{Figures/vac_sen_15.pdf}
\caption{ The visualization of similarity matrix heatmaps for the gloss-wise temporal representation  and the sequence representation  of the state-of-the-art methods. The heatmaps are organized into three columns: (a) and (b) columns depict the self-similarity matrix heatmaps of  and , respectively, while the last column shows the similarity matrix heatmap between  and  ~(with darker colors indicating higher similarity). 
 }
  \label{fig:self_corr2}
\end{figure*}


\begin{figure*}[htp]
  \centering
  \includegraphics[width=0.8 \linewidth]{Figures/tlp_corr_15.pdf}
\caption{ The visualization of similarity matrix heatmaps for the gloss-wise temporal representation  and the sequence representation  of the state-of-the-art methods. The heatmaps are organized into three columns: (a) and (b) columns depict the self-similarity matrix heatmaps of  and , respectively, while the last column shows the similarity matrix heatmap between  and  ~(with darker colors indicating higher similarity).  
 }
  \label{fig:self_corr3}
\end{figure*}


\begin{figure*}[htp]
  \centering
  \includegraphics[width=0.8 \linewidth]{Figures/vac_sen_31.pdf}
\caption{ The visualization of similarity matrix heatmaps for the gloss-wise temporal representation  and the sequence representation  of the state-of-the-art methods. The heatmaps are organized into three columns: (a) and (b) columns depict the self-similarity matrix heatmaps of  and , respectively, while the last column shows the similarity matrix heatmap between  and  ~(with darker colors indicating higher similarity).  
 }
  \label{fig:self_corr4}
\end{figure*}


\begin{figure*}[htp]
  \centering
  \includegraphics[ width=0.8 \linewidth]{Figures/tlp_corr_31.pdf}
\caption{ The visualization of similarity matrix heatmaps for the gloss-wise temporal representation  and the sequence representation  of the state-of-the-art methods. The heatmaps are organized into three columns: (a) and (b) columns depict the self-similarity matrix heatmaps of  and , respectively, while the last column shows the similarity matrix heatmap between  and  ~(with darker colors indicating higher similarity).
 }
  \label{fig:self_corr5}
\end{figure*}


















\end{document}
