\chapter{Solution Architecture}
\label{chap:sa}
We will discuss the proposed solution in detail in this chapter. Section~\ref{sec:3con} presents the system and deployment architecture of the simulator, discussing in detail how the distribution of execution is achieved, and the partitioning strategies on how the simulation is partitioned and distributed across the instances. MapReduce simulator design, and how it functions in a multi-tenanted environment using Hazelcast or Infinispan, are also addressed. 

Dynamic scaling ensures an effective usage of the available resources. Instead of having all the available instances involved into a simulation task from the beginning, more instances can be involved adaptively, based on the load. Similarly, auto scaling ensures a cost-effective solution by scaling out based on the load, using Infrastructure-as-a-Service solutions such as Amazon EC2, or on private clouds such as Eucalyptus~\cite{nurmi2009eucalyptus}. We will further discuss dynamic scaling in Section~\ref{sec:3se}. Section~\ref{sec:3perf} reasons about and analyses the main aspects that will drive the speedup by the distributed execution of the simulations. Finally, Section~\ref{sec:3arch} will lead us into the detailed software architecture of the simulator.


\section{Concurrent and Distributed Middleware Platform}
\label{sec:3con}
As designed to run top of a cluster, $Cloud^{2}Sim$ attempts to execute larger and more complicated simulations that would not run on a single node, or consume a huge amount of time. A cluster of shared resources can be built over a cluster of computers, using the in-memory data grid frameworks. Simulations are executed on the cluster, utilizing the resources such as storage, processing power, and memory, provided by the individual nodes, as indicated by Figure~\ref{fig:datagrid}. Hazelcast and Infinispan are used as the in-memory data grid libraries in $Cloud^{2}Sim$. Based on the technical white papers, Hazelcast was chosen as the base platform to distribute the CloudSim simulations.

\begin{figure}[!htbp]
\begin{center}
 \resizebox{0.8\columnwidth}{!}{
  \includegraphics[width=0.8\textwidth]{datagrid.png}
 }
\end{center}

 \caption{High Level Use-Case of $Cloud^{2}Sim$}
 \label{fig:datagrid}
\end{figure}

$Cloud^{2}Sim$ functions in two basic modes as a concurrent and distributed simulator: cloud and MapReduce. It was decided to extend an existing cloud simulator to be concurrent and distributed, instead of writing a new cloud simulator from the scratch, to be able to take advantage of existing simulations. Developed as a Java open source project, CloudSim can be easily modified by extending the classes, with a few changes to the CloudSim core. Its source code is open and maintained. Hence, CloudSim was picked as the core simulation module to build the distributed simulator of this thesis. Cloud simulation further uses Hazelcast to distribute the storage of VM, Cloudlet, and Datacenter objects and also to distribute the execution, according to the scheduling, to the instances in the cluster. Users have the freedom to choose Hazelcast based or Infinispan based distributed execution for the cloud and MapReduce simulator, as the simulator is implemented on top of both platforms following the same design. Classes of CloudSim are extended and a few are also modified to be able to extend CloudSim with further functionality.  External dependencies such as Hazelcast and Infinispan are used unmodified, for added transparency and portability. The definition of cloud simulations and MapReduce simulations are independent by design. Cloud and MapReduce simulations can be executed independently, though experiments can be run utilizing both cloud and MapReduce simulations. 

\subsection{Partitioning of the Simulation}
\label{ssec:3partition}
As multiple instances execute a single simulation, measures are taken to ensure that the output is consistent as if simulating in a single instance, while having enhanced performance and scalability. Data is partitioned across the instances by leveraging and configuring the in-memory data grid. Each instance of the cluster executes part of the logic on the objects that are stored in the local partitions of the respective nodes. The logic that is distributed is simulations that are developed by the users. This includes the application-specific logic as well as the common system executions such as data center, VM, and cloudlet creation and scheduling.

Execution of simulations is improved, by leveraging the multi-core environments, and exploiting the multi-threaded programming. While CloudSim provides some means for a concurrent execution, its support is very limited. Simulations should be executed utilizing the multi-threaded environments, where the simulator itself runs the tasks concurrently, whenever that is possible and efficient. Runnables and callables are used to submit tasks to be run in a separate thread, while the main thread is executing its task. The relevant check points ensure that the threads have finished their execution and the values are returned from the callables, as required. 

A cluster can be formed by multiple instances. Multiple clusters can be used to execute parallel cloud or MapReduce simulations, as multiple tenants of the nodes. As each cluster is unaware of the other clusters, tenant-awareness is ensured so that the parallel experiments can be independent and secured from the other parallel simulations.

Pulling data from each of the nodes for execution has a higher communication cost. To overcome this, the data locality features provided for Hazelcast distributed executors are leveraged and used appropriately to send the logic to the data instead. Partition-awareness feature of Hazelcast is exploited in storing the distributed objects, such that the data that are associated with each other are stored in the same partition to decrease the remote invocations. 

Multiple partitioning strategies were implemented for different scenarios, as shown by Figure~\ref{fig:partitionapproaches}: \textit{Simulator - Initiator based strategy}, \textit{Simulator - SimulatorSub based strategy}, and \textit{Multiple Simulator instances strategy}. We will discuss each of these partitioning strategies further below.
\begin{figure}[!htbp]
\begin{center}
 \resizebox{0.8\columnwidth}{!}{
  \includegraphics[width=0.8\textwidth]{partitionapproaches.png}
 }
\end{center}
 \caption{Partitioning Strategies}
 \label{fig:partitionapproaches}
\end{figure}

\paragraph*{1. Simulator - Initiator based Strategy}
$Simulator$ is the complete $Cloud^{2}Sim$ with the simulation running. A Hazelcast instance is started by $Cloud^{2}Sim$ $Initiator$, which keeps the computer node connected to the Hazelcast cluster, offering the resources of the node to the data grid. The $Simulator$ instance is run from the master instance, where an instance of $Initiator$ is spawned from the other instances. Simulator acts as the master, distributing the logic to the Initiator instances. Part of the logic is executed in the master itself, and the execution is partitioned uniformly among all the instances, using the ExecutorService.

Pseudocode for the Initiator is presented in Algorithm~\ref{alg:initiator}.
\begin{algorithm}
  \caption{Initiator Instance}
\label{alg:initiator}
  \begin{algorithmic}
\State $initInstance()$
\While{ $(Simulation Is Executing)$ }
\State \textbf{Receive} Executor Service Executions \textbf{From} Master
\State \textbf{Execute} simulation step on the Data Partition
\EndWhile
\State $clearDistributedObjects()$ 
  \end{algorithmic}
\end{algorithm}

Pseudocode for the Master Instance is presented in Algorithm~\ref{alg:master}. The parts of the simulation that can be distributed include,
\begin{enumerate}[i]
  \item Creation of resources, cloudlets and VMs.
  \item Allocation of resources.
  \item Independent execution of the entities such as cloudlets, VMs, and brokers.
\end{enumerate}
Further simulation components that can be distributed depend on each simulation, and each simulation should ensure to partition the simulation space accordingly to maximize and optimize the parts of the execution that can be distributed. Distributed brokers and distributed counter-parts of cloudlets and VMs are designed, such that the application developer can utilize them to ensure smooth transitioning of the simulations to the distributed environment.

\begin{algorithm}
  \caption{Simulation Master Instance}
\label{alg:master}
  \begin{algorithmic}
\State Start and Initialize $Cloud^{2}Sim$ Cluster
\Repeat
\While{ $(SimulationIsDistributable)$ }
\State \textbf{Send} Executor Service Executions To Other Instances
\State \textbf{Execute} on the Data Partition
\State \textbf{Execute} Logic Partitions of its own
\EndWhile
\State \textbf{Process} Received Partitions from Other Instances
\State \textbf{Execute} Core Simulation That cannot be Distributed.
\Until $(SimulationIsCompleted)$
\State $presentSimulationOutput()$
\State $clearDistributedObjects()$ 
  \end{algorithmic}
\end{algorithm}

\paragraph*{2. Simulator - SimulatorSub based Strategy}
One instance contains the Simulator, which is the master, where others execute SimulatorSub, which are the slave instances. Master coordinates the simulation execution. Execution is started by all the instances and parts of the execution are sent by each instance respectively to the other instances, using the ExecutorService. Hence, the load on the master is reduced. Some of the unparallelizable tasks can be delegated to the primary worker, which is an instance other than the master instance, that is decided upon the cluster formation. This mitigates overloading the master instance.

The master instance still processes the core logic that cannot be distributed, and hence the pseudocode for the master instance does not change. Execution of SimulatorSub instances is described by Algorithm~\ref{alg:simulatorsub}.
\begin{algorithm}
  \caption{SimulatorSub Instances}
\label{alg:simulatorsub}
  \begin{algorithmic}
\Repeat
\While{ $(simulationIsDistributable)$ }
\State \textbf{Send} Executor Service Executions To Other Instances
\State \textbf{Execute} on the Data Partition
\State \textbf{Execute} Logic Partitions of its own
\EndWhile
\Until $(simulationIsCompleted)$
\State $clearDistributedObjects()$ 
  \end{algorithmic}
\end{algorithm}

\paragraph*{3. Multiple Simulator Instances Strategy}
There is no predefined Simulator master in this strategy. The instance that joins first becomes the master at run time, where other instances function as $SimulatorSub$ instances. Logic is partitioned across the instances using the partitioning algorithms defined in $Cloud^{2}Sim$ distributed data center brokers. $PartitionUtil$ manages the partitioning of the data and execution, manipulating the data structures across the instances. It provides the initial and final IDs of the data structure such as cloudlets and VMs, given the total number of the data structure elements and the initial offset. Figure~\ref{fig:partition} shows a higher level view of the partitioning. Here, the distributed storage is provided by all the physical nodes that host the instances in the execution cluster. Each type of distributed objects such as cloudlets and VMs are partitioned to be stored in the instances. Partition is tracked using the object IDs, where the initial and final IDs are marked. Logic is executed in the objects that are stored in the specific instance, minimizing the remote invocations.

\begin{figure}[!htbp]
\begin{center}
 \resizebox{0.7\columnwidth}{!}{
  \includegraphics[width=0.7\textwidth]{partition.png}
 }
\end{center}
 \caption{Partition of storage and execution across the instances}
 \label{fig:partition}
\end{figure}

The \textit{Simulator - Initiator based Strategy} is chosen for implementation in tasks that are effectively scheduled by the single master to all the instances that are joined, such as the MapReduce simulator. The \textit{multiple Simulator instances strategy} is used in the CloudSim simulations such as the simulation of matchmaking-based application scheduling, where the simultaneous instances are more effective, than having a single static master that handles most of the task. The \textit{Simulator - SimulatorSub based strategy} is proposed for the compound simulations involving both Cloud and MapReduce executions, or simulating MPI workflows. The \textit{multiple Simulator instances strategy} is usually preferred over the \textit{Simulator - SimulatorSub based strategy} as it is easier to maintain since it does not fragment the logic, and also electing the master at run time is more effective in terms of scalability and fault-tolerance.

Existence of the master instance is always ensured in the \textit{multiple Simulator instances strategy}. The instance that joins the cluster as the first instance in the cluster becomes the master, where in the \textit{Simulator - SimulatorSub based strategy}, the instance of Simulator should be manually started before the sub instances, and this may become a bottleneck. Moreover, when backups are available, the \textit{multiple Simulator instances strategy} is resilient to failures as when the assigned master fails, another instance can take over as the master. This is not possible in the other strategies, as the master is chosen statically, and the other nodes do not contain the same code as the master instance.

\subsection{Multi-tenancy in $Cloud^{2}Sim$}
A multi-tenanted experiment executes over a deployment, composed of multiple clusters of (Hazelcast) instances, across multiple physical nodes. A tenant is a part of the experiment, represented by a cluster. An instance is attached to a single cluster, and is tenant-specific. Data specific to a tenant is stored in its instances of the cluster. The specific instance to store is defined by the $Cloud^{2}Sim$ design, leveraging and configuring the HazelcastInstanceAware and PartitionAware features to decide the optimal instance.

A coordinator node has instances in multiple clusters and hence enables sharing information across the tenants through the local objects of the JVM. Due to the 1:1 mapping between a cluster and a tenant, a tenant may not span across multiple clusters. This does not cause under-utilization, as multiple clusters can co-exist in and utilize the same nodes. Fault-tolerance is easily ensured by enabling synchronous backups, by just changing the configuration file. Thus, even if a node goes down, the tenants will not suffer.
\begin{figure}[ht]
\begin{center}
 \resizebox{0.6\columnwidth}{!}{
  \includegraphics[width=0.6\textwidth]{mapreducecluster.png}
 }
\end{center}
 \caption{A Multi-tenanted Deployment of $Cloud^{2}Sim$}
 \label{fig:mapreducecluster}
\end{figure}

Figure ~\ref{fig:mapreducecluster} depicts a sample deployment of 6 nodes configured into 7 clusters to run 6 experiments in parallel. Both cluster1 and cluster3 contain 2 nodes - the Master/Supervisor and one Initiator instance, running an experiment. Cluster2 contains 3 nodes, with the third node having 2 Initiator instances running. Cluster4 contains an instance of Initiator, ready to join a simulation job, when the Master instance joins. Cluster5 consists of node4, which hosts both Initiator and Master instances. Cluster6 contains node5 and node6, both running Initiator instances. Node1 hosts 2 Master instances, one in cluster1, and the other in cluster2. It also hosts an Initiator instance in cluster3. It has a Coordinator running on cluster0. Coordinator coordinates the simulation jobs running on cluster1 and cluster2 from a single point, and prints the final output resulting from both experiments or tenants. This is done externally from the parallel executions of the tenants, and enables a combined view of multi-tenanted executions.

Node - Experiment mapping can also be represented using a matrix notation of (Node X Experiment). The matrix for the multi-tenanted deployment depicted by Figure~\ref{fig:mapreducecluster} is given below.

Multi-tenanted Deployment =
\bordermatrix{\text{}&Exp_1&Exp_2& Exp_3 &Exp_4 & Exp_5 & Exp_6\cr
                Node_1&  S+C  & S+C & I & 0 & 0 & 0\cr
                Node_2& I & I & 0  &  0 & 0 & 0\cr
                Node_3& 0 & 2I & S & I & 0 & 0\cr
                Node_4& 0 & 0 & 0 & 0 & S + I & 0 \cr
                Node_5& 0  & 0 & 0 & 0 & 0  & I\cr
                Node_6& 0  & 0 & 0 & 0 & 0  & I}

Here, $S$ represents Supervisor/Master, $I$ represents Initiator, and $C$ represents coordinator. Though a deployment is better represented with nodes in horizontal and experiments/clusters in vertical, Figure~\ref{fig:mapreducecluster} is drawn with clusters in horizontal, for a better clarity of the representation.

\paragraph*{Cloud Simulations:}
$Cloud^{2}Sim$ is designed on top of CloudSim, where cloud2sim-1.0-SNAPSHOT can be built using Maven independently without rebuilding CloudSim. Modifications to CloudSim are very minimal. $Cloud^{2}Sim$ enables distributed execution of larger CloudSim simulations. The compatibility layer of $Cloud^{2}Sim$ enables the execution of the CloudSim simulations with minimal code change, on top of either the Hazelcast and Infinispan based implementations, or the pure CloudSim distribution, by abstracting away the dependencies on Hazelcast and Infinispan, and providing a compatible API. 

\paragraph*{MapReduce Simulations:}
Design of the MapReduce simulator is based on a real MapReduce implementation. A simple MapReduce application executes as the Simulator is started. The number of times map() and reduce() are invoked can easily be configured. The MapReduce simulator is designed on two different implementations, based on Hazelcast and Infinispan, making it possible to benchmark the two implementations against each other. Multiple simulations are executed in parallel, without influencing others, where an instance of a coordinating class could collect the outputs from the independent parallel MapReduce jobs carried out by different clusters. 

\section{Scalability and Elasticity}
\label{sec:3se}
$Cloud^{2}Sim$ achieves scalability through both static scaling and dynamic scaling. Static scaling is the scenario where $Cloud^{2}Sim$ uses the storage and resources that are initially made available, when instances are started and joined manually to the execution cluster. Multiple nodes can be started simultaneously at the start-up time for large simulations that require large amount of resources. $Initiator$ instances can also be started manually at a later time, to join the simulation that has already started. Simulations begin when the minimum number of instances specified have joined the simulation cluster. $Cloud^{2}Sim$ scales smoothly as more Hazelcast instances join the execution. 

Scaling can also be achieved by $Cloud^{2}Sim$ itself dynamically without manual intervention, based on the load and simulation requirements. When the load of the simulation environment goes high, $Cloud^{2}Sim$ scales itself to handle the increased load. Dynamic scaling of $Cloud^{2}Sim$ provides a cost-effective solution, instead of having multiple instances being allocated to the simulation even when the resources are under-utilized.

Since scaling introduces the possibility of nodes joining and leaving the cluster, as opposed to the static execution or manual joins and exits of instances, scalable simulation mandates availability of synchronous backup replicas, to avoid losing the distributed objects containing the simulation data upon the termination of an instance. 

A health monitor was designed to monitor the health of the instances, and trigger scaling accordingly. The health monitoring module runs from the master node and periodically checks the health of the instance by monitoring the system health parameters such as the process CPU utilization, system CPU utilization, and the load average. Based on the policies defined in the configuration file, the health monitor triggers the dynamic scaler. When the current observed value of the monitored health parameter (such as load average or process or system CPU utilization) is higher than the $maxThreshold$ and the number of total instances spawned is less than the $maxInstancesToBeSpawned$, a new instance will be added to the simulation cluster. Similarly, when the current observed value is lower than the $minThreshold$, an instance will be removed from the simulation cluster. Pseudocode for the dynamic scaling based on health monitoring is presented in Algorithm~\ref{alg:elasticity}.

\begin{algorithm}
  \caption{Dynamic Scaling}
\label{alg:elasticity}
  \begin{algorithmic}
\While{ $(TRUE)$ }
\State $getCurrentSystemHealthStatus()$
\If{$(load \geq maxThreshold$ \textbf\\{AND} $currentlySpawnedInstances < maxInstancesToBeSpawned)$}
    \State $scaleOut()$\Comment{add instance}
    \State $wait(timeBetweenScaling)$
\ElsIf{$(load\le minThreshold)$}
    \State $scaleIn()$\Comment{remove instance}
    \State $wait(timeBetweenScaling)$
\Else
    \State $wait(timeBetweenHealthChecks)$
\EndIf
    \EndWhile  
  \end{algorithmic}
\end{algorithm}

During scale out, more instances are included into the simulation cluster, where scale in removes instances from the simulation cluster, as the opposite of scale out. Dynamic scaling is done in two modes - auto scaling and adaptive scaling, as discussed below.

\subsection{Auto Scaling}
By default, the $Cloud^{2}Sim$ auto scaler spawns new instances inside the same node/computer. The auto-scaling feature is available out of the box for Hazelcast paid/enterprise versions. As $Cloud^{2}Sim$ uses the free and open source version of Hazelcast, auto scaling feature is designed on top of Hazelcast, using the health monitoring module of $Cloud^{2}Sim$. 

When there is only a limited availability of resources in the local computer clusters that is insufficient to simulate a large scenario, $Cloud^{2}Sim$ can be run on an actual cloud infrastructure. Hazelcast can be configured to form a cluster on Amazon EC2 instances, with the Hazelcast instances running on the same AWS\footnote{\url{https://aws.amazon.com/}} account. When using AWS join mechanism provided by Hazelcast to form the cluster, Hazelcast uses the access key and secret key to authorize itself into forming the cluster. If no AWS security group is mentioned, all the running EC2 instances will be tried, where mentioning a security group will limit the search to only the instances of the same security group. Ports that are involved in Hazelcast clustering should be open and permitted in the EC2 instances. Scaling can be triggered by the $Cloud^{2}Sim$ health monitoring or using the scaling policies configured with AWS Auto Scaling and Amazon Cloud Watch, as shown by Figure~\ref{fig:aws}.

\begin{figure}[!h]
\begin{center}
 \resizebox{0.6\columnwidth}{!}{
  \includegraphics[width=0.6\textwidth]{aws.png}
 }
\end{center}
 \caption{Cloud Simulations on Amazon EC2 instances}
 \label{fig:aws}
\end{figure}

\subsection{Adaptive Scaling}
Adaptive Scaling is a scenario, where in a clustered environment, more computer nodes will be involved in an application execution based on the load. More instances will be attached to the simulation cluster when the load is high, and instances will be detached or removed from simulation when the load is low. We will discuss two of the design approaches that were considered, as they appear to be the logical and more straight-forward options, and will further elaborate why they were impossible without modifying Hazelcast. The final and feasible design without requiring code modifications to Hazelcast is presented after the two failed alternatives.

\paragraph*{1. Pause and Resume approach:}
Pause and resume instances, within a single cluster, which is not available out of the box in Hazelcast.

\paragraph*{2. Group Migration approach:}
In this approach, the deployment has two groups/clusters - cluster-main and cluster-sub. Instances from a cluster know nothing about those in the other clusters. Instances in cluster-sub are basically stand-by, where the cluster-main does all the simulation work. When the cluster-main group is overloaded, more instances from cluster-sub will be added to the group cluster-main and removed from cluster-sub. When the load goes down, they will be moved back to cluster-sub. 

Hazelcast does not indicate all the running instances. $Hazelcast.getAllHazelcastInstances()$ merely provides a list of instances running inside a single JVM. $hazelcastInstance.getCluster().getMembers()$ gives the members of any given cluster. Considering these limitations, to be able to communicate with both groups, two Hazelcast instances are started in the master node - One is of the group cluster-main and a $middleMan Instance$ of the group cluster-sub. But it is not possible to change the group configuration of an instance at run time, and change its group (move from one group to another) programmatically. Hence, this design approach of migrating an instance from a group to another became infeasible in Hazelcast.

\paragraph*{3. Scaling Decisions in a separate cluster - IntelligentAdaptiveScaler approach:}
In this approach, the health monitor in the main instance monitors the load and health status of the main instance with simulation running in $cluster-main$, and shares this information with the $AdaptiveScalerProbe$ thread in $cluster-sub$, using the local objects, as they are from the same JVM. $AdaptiveScalerProbe$ shares this information with $IntelligentAdaptiveScaler$ (IAS) instances, which are threads from all the other nodes that are connected to $cluster-sub$. 

When IAS from one instance notices the high load in the master, it spawns an Initiator instance in the $cluster-main$, and sets the flag to false to avoid further scaling outs/ins. Monitoring for scaling out happens when there is no Initiator instance in the node, and monitoring for scaling in happens when there is an Initiator instance, for each individual node. This ensures 0 or 1 of Initiator instances in each node, and avoids unnecessary hits to the Hazelcast distributed objects holding the health information. Since IAS is in a separate cluster (cluster-sub) from the simulation (cluster-main), the executions are independent. 

This design was chosen for the implementation of the adaptive scaler, as it is the most feasible implementation choice. Pseudocode for $AdaptiveScalerProbe$ is further presented in Algorithm~\ref{alg:asp}, and Algorithm~\ref{alg:ias} presents $IntelligentAdaptiveScaler$.
\begin{algorithm}[h]
  \caption{Adaptive Scaler Probe Algorithm}
\label{alg:asp}
  \begin{algorithmic}
\Procedure {addInstance}{}
\State $toScaleOut \gets TRUE$ \Comment{Atomic Boolean}
\EndProcedure
\Statex
\Procedure {removeInstance}{}
\State $toScaleIn \gets TRUE$ \Comment{Atomic Boolean}
\EndProcedure
\Statex
\Procedure {probe}{}
\While{ $(TRUE)$ }
\State $wait(timeBetweenHealthChecks)$
\If{$toScaleOut$}
\State $toScaleOut \gets FALSE$
\State $nodeHealth.toScaleOut \gets TRUE$ \Comment{Distributed Map Entries}
\State $nodeHealth.toScaleIn \gets FALSE$
\ElsIf{$toScaleIn$}
\State $toScaleIn \gets FALSE$
\State $nodeHealth.toScaleIn \gets TRUE$
\State $nodeHealth.toScaleOut \gets FALSE$
\EndIf
    \EndWhile  
\EndProcedure
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}[h]
  \caption{Intelligent Adaptive Scaler Algorithm}
\label{alg:ias}
  \begin{algorithmic}
\Procedure {initHealthMap}{} \Comment{During the start up}
\State $key \gets 0$
\If{$(nodeHealth.toScaleOut = NULL)$}
\State $nodeHealth.toScaleOut \gets FALSE$
\EndIf
\If{$(nodeHealth.toScaleIn = NULL)$}
\State $nodeHealth.toScaleIn \gets FALSE$
\EndIf
\EndProcedure
\Statex
\Procedure {probe}{}
\While{ $(TRUE)$ }
\State $wait(timeBetweenHealthChecks)$
\If{$(instances.count() = 0)$}

\If{$(nodeHealth.toScaleOut)$}
\State $nodeHealth.toScaleOut \gets FALSE$ \Comment{Set to false, before the atomic decision}
\State $Atomic\{$ \Comment{Distributed atomic flag}
\State $currentValue \gets key$
\State $key \gets 1$
\State$\}$
\If{$(currentValue = 0)$} \Comment{No recent scaling by any instances}
\State $spawnInstance()$
\State $wait(timeBetweenScalingDecisions)$
\State $key \gets 0$ \Comment{Cluster may scale again now}
\EndIf
\EndIf

\ElsIf{$(nodeHealth.toScaleIn)$}
\State $nodeHealth.toScaleIn \gets FALSE$
\State $Atomic\{$
\State $currentValue \gets key$
\State $key \gets -1$
\State$\}$
\If{$(currentValue == 0)$}
\State $shutdownInstance()$
\State $wait(timeBetweenScalingDecisions)$
\State $key \gets 0$
\EndIf
\EndIf



    \EndWhile  
\EndProcedure
  \end{algorithmic}
\end{algorithm}
 
\subsection{Elastic Deployments}
Adaptive Scaling is used to create prototype deployments with elasticity. Adaptive scaling is built as shown by Figure~\ref{fig:IAS}. When the simulations complete, the Hazelcast instances running in the cluster-main will be terminated, and the distributed objects stored in the cluster-sub will be cleaned. These instances just require Hazelcast and the adaptive scaler thread to keep them connected, providing their CPU and storage for the  simulation work voluntarily, in a BOINC-like cycle sharing model. The entire simulation code can be loaded and kept only on the master and exported transparently to other nodes joining it, and execute from all the nodes, following the \textit{Simulator - Initiator based Strategy}. All the member nodes are from the same network, that they have joined by TCP-IP or multicast. Hence the cycle sharing model of $Cloud^{2}Sim$ is not public as in voluntary computing. Due to this nature, the security implications involved in voluntary computing are not applicable to $Cloud^{2}Sim$.

\begin{figure}[!htbp]
\begin{center}
 \resizebox{0.6\columnwidth}{!}{
  \includegraphics[width=0.6\textwidth]{IAS-Design.png}
 }
\end{center}
 \caption{Deployment of the Adaptive Scaling Platform}
 \label{fig:IAS}
\end{figure}

The scaling decision flag should be get and set in a concurrent and distributed environment atomically, ensuring that exactly one instance takes action of it. Access to the object that is used as the flag must be locked during update from any other instance in the distributed environment. 

\paragraph*{Parallel and Independent Simulations:}
Multiple Hazelcast clusters can be run from a single computer cluster or even a single machine. By exploiting this feature, multiple experiments can be run on $Cloud^{2}Sim$ in parallel, as different clusters are used for independent simulations. Different simulations are initialized from the same node, as shown below.

{\fontsize{10}{10}\selectfont
\begin{lstlisting}
String clusterGroup = HzConstants.MAIN_HZ_CLUSTER + id;

// Initialize the CloudSim library
HzCloudSim.init(ConfigReader.getNoOfUsers(), calendar, trace_flag, clusterGroup);
\end{lstlisting}
}
The adaptive scaling solution is further extended to have the node cluster providing its resources to different applications or simulations running on different Hazelcast clusters. Figure~\ref{fig:XIAS} shows the execution of two independent simulations in a cluster with adaptive scaling. The adaptive scaler functions as a $Coordinator$ instance, coordinating and allocating its resources to multiple tenants. Here, instead of representing the scaling decisions using single keys, distributed hash maps are used, mapping the scaling decisions and health information against the cluster or tenant ID. Similarly, the pointers to the master instances are mapped against the cluster ID, making it possible to refer to and coordinate multiple tenants from the coordinator.

\begin{figure}[!htbp]
\begin{center}
 \resizebox{0.8\columnwidth}{!}{
  \includegraphics[width=0.8\textwidth]{Extended-IAS-Deployment.png}
 }
\end{center}
 \caption{An Elastic Deployment of $Cloud^{2}Sim$}
 \label{fig:XIAS}
\end{figure}


\section{Analysis of Design Choices Regarding Speedup and Performance}
\label{sec:3perf}
Speedup in a distributed or grid environment is a measure to compare how fast is the distributed application on multiple instances compared to its serial version on a single instance~\cite{hoekstra2005introducing}. Speedup in $Cloud^{2}Sim$ measures how fast the distributed simulation executes on multiple Hazelcast instances, compared to the time taken to execute the original simulation on CloudSim. Since Hazelcast initialization can be done just once for a series of simulations and experiments, we ignore the initialization time from the calculations.
\begin{equation} \label{eq:initequation}
T_{n} = \frac{k.T_{1}}{n} + (1-k).T_{1} + S + C + \gamma + F - \theta
\end{equation}

Here,\\
n - number of instances.\\
$T_{n}$ - Time taken by the distributed simulation on n instances.\\
k - Fraction of the code that should be run in a single instance and should not be distributed.\\
C - Latency or communication costs.\\
$\gamma$ - Coordination costs.\\
S - Serialization/deserialization costs.\\
F - Fixed costs.\\
$\theta$ - Performance gain from the inherently abundant resources of the data grid.

The fixed costs, F includes costs such as the time taken to initialize the threads, distributed executor framework, and other distributed data structures. It is required to serialize and deserialize the distributed custom objects such as cloudlets and VMs, as the objects are stored in remote locations. The distributed objects are serialized and stored in a binary form, and composed back to the respective objects upon deserialization. The memory and CPU cost from the serialization and deserialization is an immediate cost imposed by the integration of Hazelcast into $Cloud^{2}Sim$, which is present even when a single instance exists in the execution cluster. This overhead is common to all the distributed execution libraries, such that even the alternatives such as Infinispan imply this overhead. Serialization cost depends on the size or scale of the distributed simulation objects, where more objects to be serialized imposed more serialization cost. Hence, serialization cost is defined as,
\begin{equation} \label{eq:sc}
S = f_{1}(s)
\end{equation}
Here,\\
s - Size of the distributed simulation objects.\\

Coordination cost is defined as the cost caused by the instances coordinating themselves in the cluster. Communication costs and coordination costs increase with the number of instances, and with the latency caused by the physical distance between the nodes containing the instances, as well as the communication medium strength. As the number of instances is increased, these costs increase further, where they do not appear in the scenario of a single instance. If the same program runs in a single instance, communication cost and coordination cost become zero, where distributing it to more and more instances increases these. While communication cost depends on the size of the simulation, coordination cost is independent on the simulation, as it is caused by the framework regardless of the application that is running on top of it. Hence, communication cost can be defined as,
\begin{equation} \label{eq:cc}
C = f_{2}(n, d, w, s)
\end{equation}
Here,\\
d - Distance between the instances.\\
w - Bandwidth.\\
s - Size of the distributed simulation objects.\\

Coordination cost can be defined as,
\begin{equation} \label{eq:cd}
\gamma = f_{3}(n, d, w)
\end{equation}

As more nodes are introduced into the simulation cluster, application space is broken down into multiple nodes. Each node has to provide resources to only a fraction of the complete application. Hence, when more resources are made available for the simulation, larger simulations can be executed, while optimizing the cache of the computer. Moreover, this also increases the usage of memory, where large objects can be stored inside the memory, avoiding potential paging of larger objects, while minimizing memory contention. Hence, 
\begin{equation} \label{eq:theta}
\theta = f_{4}(N) 
\end{equation}
Here,\\
N - Number of physical nodes involved in the experiment.\\


Considering Equation~\ref{eq:sc}, Equation~\ref{eq:cc}, Equation~\ref{eq:cd}, and Equation~\ref{eq:theta}, Equation~\ref{eq:initequation} can be presented as,
\begin{equation} \label{eq:iequation}
T_{n} = \frac{k.T_{1}}{n} + (1-k).T_{1} + f_{1}(s) + f_{2}(n, d, w, s) + f_{3}(n, d, w) + F - f_{4}(N)
\end{equation}
 
$Speedup$, 
\begin{equation} \label{eq:baseequation}
S_{n} = \frac{T_{1}}{T_{n}}
\end{equation}
where e$fficiency$ is defined as,
\begin{equation} \label{eq:basefequation}
E_{n} = \frac{S_{n}}{n} = \frac{T_{1}}{n.T_{n}}
\end{equation}

While $speedup$ measures how faster the application performs with increasing number of instances, $efficiency$ provides a normalized measurement by dividing the speedup value with the number of instances. Hence, $efficiency$ may be used as a more realistic measure to find the number of instances to be involved in any execution for a cost-effective scaling.
From equations \ref{eq:iequation} and \ref{eq:baseequation}, speedup can be formulated as below,
\begin{equation} \label{eq:finalequation}
S_{n} = \frac{T_{1}}{\frac{k.T_{1}}{n} + (1-k).T_{1} + f_{1}(s) + f_{2}(n, d, w, s) + f_{3}(n, d, w) + F - f_{4}(N)}
\end{equation}

Percentage improvement (P) in performance can be presented as,
\begin{equation} \label{eq:percentage}
P = (1 - \frac{1}{S_{n}}) * 100\end{equation}

From Equation~\ref{eq:finalequation},
\begin{equation} \label{eq:percentageFinal}
P = (\frac{k.T_{1}(1-\frac{1}{n}) + \theta - S - C - \gamma - F}{T_{1}}) * 100\%
\end{equation}


Communication cost is minimized by the partition-awareness and data locality of the design. Objects are kept in deserialized object format, when they are in simple format and are accessed locally. Objects are serialized, and stored in binary format, when they are distributed. However, serialization cost is inevitable, and does not increase with the number of instances, unlike communication costs. Hence, we may expect increased execution time when 2 nodes are used, when the serialization cost is high, followed by a speedup when executing in more nodes.

If all the Hazelcast or Infinispan instances reside inside a single computer, latency will be lower. While this increases the speedup of simulations that can run on a single computer, applications with high resource requirements will suffer from lack of memory or CPU, as all the instances should share the resources of the computer. Usually, it is expected that the instances run on different computers on a research lab cluster. Though the communication cost will be lower than the geo-distributed cluster, as all the nodes in the cluster are co-located, it will be higher than running the instances inside a single computer. 

Availability of abundant resources speed up the executions that would not run effectively on limited resources provided by a single computer. Performance gain by distributed execution $\theta$ depends on the requirements of the simulations as well as the availability or lack of the resources in the nodes contributing to the cluster. Hence, for an embarrassingly parallel application with lower memory and processing requirements, distributing the load inside a single computer using multiple in-memory data grid instances may be more effective, where a more resource-demanding simulation will perform better on a cluster with multiple physical nodes.

\section{Software Architecture and Design}
\label{sec:3arch}
Distributed storage and execution for CloudSim simulations is achieved by exploiting Hazelcast. Infinispan integration with the compatibility layer ensures easy integration of Infinispan to replace Hazelcast as the in-memory data grid for CloudSim simulations. Figure~\ref{fig:cloud2simArch} depicts a layered architecture overview of $Cloud^{2}Sim$, hiding the fine architectural details of CloudSim. 
\begin{figure}[!h]
\begin{center}
 \resizebox{0.55\columnwidth}{!}{
  \includegraphics[width=0.55\textwidth]{architecture.png}
 }
\end{center}

 \caption{$Cloud^{2}Sim$ Architecture}
 \label{fig:cloud2simArch}
\end{figure}

\subsection{$CloudSim$ Simulations}
As extending CloudSim, $Cloud^{2}Sim$ provides an API compatible with CloudSim, for the cloud simulations. Classes of CloudSim are extended as shown by Table~\ref{table:inheritance}, while preserving the invocation interfaces and code generality. This ensures easy adaptation and migration of $CloudSim$ simulations to $Cloud^{2}Sim$. Respective data structures of the CloudSim simulations can be easily ported to $Cloud^{2}Sim$ by using the extended classes as shown by Table~\ref{table:inheritance}, instead of the base CloudSim classes. By using bytecode enhancement, source code replacement/augmentation, and using object factory methods that can be intercepted or redefined, $CloudSim$ simulations can be executed on top of $Cloud^{2}Sim$, without the need to modify the simulation code.

\begin{table}[!t]
\caption{$Cloud^{2}Sim$ and CloudSim}
\label{table:inheritance}
\begin{tabular}{|c||c| |c|}
\hline
 & \textbf{Extended} & \\
\textbf{$Cloud^{2}Sim$ Class} &\textbf{CloudSim class} & \textbf{Core Responsibilities}\\
\hline
HzCloudSim & CloudSim & * Core class of the Simulator\\
& & * Initializes distributed data structures \\
\hline
HzDatacenterBroker & DatacenterBroker & * Implements distributed scheduling\\
\hline
 & - & * Starts Simulation based on the configuration \\
Cloud2SimEngine & & * Starts supportive threads \\
& & for scaling and health monitoring\\
\hline
PartitionUtil & - & Calculates the partitions of the data structures\\
\hline
HzCloudlet & Cloudlet & * Extends Cloudlet\\
\hline
HzVm & Vm & * Extends Vm\\
\hline
HazelSim & - & * Singleton of Hazelcast integration\\
\hline
HzObjectCollection & - & * Provides unified access to distributed objects\\
\hline
\end{tabular}
\end{table}

Hazelcast monitoring and heart beats are run on a separate thread, hence not interfering with the main thread that runs the simulations. Simulation objects, cloudlets and VMs were ported from Java lists to Hazelcast distributed maps. This enabled storing these objects in a distributed shared memory provided by Hazelcast spanning across the cluster. Instances of Hazelcast $IMap$ are used as the data structure. The core CloudSim class, $CloudSim$ is extended as $HzCloudSim$ to address the Hazelcast specific initializations. Similarly, $Cloudlet$ and $Vm$ are extended as $HzCloudlet$ and $HzVm$ respectively. This extended class hierarchy enabled modifying the internals of Vm and Cloudlet classes by sub-classing them to use Hazelcast distributed maps as the storage data structure, instead of Java lists, with instances of Hazelcast distributed executor service for distributed invocations of the methods. 

\subsubsection{Major Cloud Simulation Components}
Application layer provides sample cloud and MapReduce simulations, and structures that can assist developing further simulations on top of them. Existing CloudSim samples and applications can be ported to $Cloud^{2}Sim$ using this. 

\paragraph*{Compatibility Layer:}
A new package named ``compatibility'' composed of the core classes such as $HazelSim$ is placed inside CloudSim to integrate Hazelcast, Infinispan, and other new dependencies, and to enable multiple modes of operation (Such as Hazelcast or Infinispan based and regular CloudSim simulations). $HazelSim$ is the single class that is responsible for initiating the Hazelcast clusters and ensuring that the minimum number of instances are present in the cluster before the simulation begins. Hazelcast can also be configured programmatically for $Cloud^{2}Sim$ using $HazelSim$. $HzObjectCollection$ provides access to the distributed objects such as Hazelcast maps. $InfiniSim$ provides similar functionality for the Infinispan based distribution. The configuration file, $cloud2sim.properties$ is used to input MapReduce and CloudSim specific parameters such as the number of resources and users to be present in the simulation, such that simulations can be run with varying loads and scenarios, without need for recompiling.

\paragraph*{$Cloud^{2}Sim$ Core:}
The packages $cloudsim.hazelcast$ and $cloudsim.infinispan$ respectively integrate Hazelcast and Infinispan into the simulator. The concurrency layer consists of callables and runnables for asynchronous invocations to concurrently execute. As complex objects should be serialized before sending them to other instances over the wire, custom serializers are needed for $Vm$, $Cloudlet$, $Host$, $Datacenter$, and the other distributed objects to be able to distribute them across the instances, store and access them remotely in a binary format, effectively. The utilities module provides the utility methods used throughout $Cloud^{2}Sim$.

\paragraph*{Scheduling:}
The $scheduling$ package provides enhancements to the existing application scheduling capabilities of CloudSim. Matchmaking-based scheduling algorithms have to search through the complete object space to find a matching resource for the application requirements~\cite{mm,mm2}. The scheduling package handles scheduling in similar complex scenarios that involve searching large maps consisting of VMs, cloudlets, and the user requirements. Distributed application scheduling is done by the extended data center brokers that are capable of submitting the tasks and resources in a distributed manner. Moreover, strict matchmaking based algorithms and partial utility algorithms with matchmaking, require calculations and search for the objects, independent for each cloudlet. These are handled by the extended brokers.

DatacenterBroker and Datacenter are extended to provide a distributed execution. Extended brokers and their interaction with the resources and cloudlets are depicted in Figure~\ref{fig:classDiagram}.

\begin{figure}[!htbp]
\begin{center}
 \resizebox{0.7\columnwidth}{!}{
  \includegraphics[width=0.7\textwidth]{classDiagram.png}
 }
\end{center}
 \caption{Class Diagram of $Cloud^{2}Sim$ Brokers}
 \label{fig:classDiagram}
\end{figure}

\subsubsection{Distributed Execution of a Typical CloudSim Simulation}
The execution flow of a distributed simulation of an application scheduling scenario with Round Robin algorithm is shown by Figure~\ref{fig:flowmin}. It shows the simulation utilizing the core modules of $Cloud^{2}Sim$, and CloudSim to execute in a distributed manner. 
\begin{figure}[!h]
\begin{center}
 \resizebox{0.95\columnwidth}{!}{
  \includegraphics[width=0.95\textwidth]{flowmin.png}
 }
\end{center}
 \caption{Higher Level Execution flow of an application scheduler simulation}
 \label{fig:flowmin}
\end{figure}

A CloudSim simulation is started in $Cloud^{2}Sim$ by starting and initializing all the instances. Datacenters and distributed brokers are created concurrently. VMs and cloudlets are created in a distributed manner, from all the instances, where each instance holds a partition of entire VMs and cloudlets in it. Related entries are stored in the same instance or partition to minimize remote invocations. Created VMs and cloudlets are submitted by all the instances.

In a simulation such as Matchmaking based application scheduling~\cite{mm,mm2}, the major matchmaking phase consisting of matching the resources to the application can be entirely done in a distributed and independent manner. This is done by the broker in a distributed and partition-aware manner. This is the major workload of the simulation, as the simulation has to search through a large object space to find a match, where a distributed and concurrent execution helps. The searching and matching is done in each instance by the broker in the respective partitioned distributed object space.

Broker finally starts the core simulation. Since the VMs are distributed to multiple instances in Hazelcast distributed storage, the execution is distributed. However, the final outcome is presented by the master instance, collecting the outcomes from all the instances.

In a simulation where multiple VMs and cloudlets are created, and a cloudlet workload such as a matchmaking with a respective VM is involved, percentage of the independent cloudlet execution is very high among the entire execution, such that a distributed execution will provide much faster simulations.


\subsection{MapReduce Layer}
As Hazelcast MapReduce implementation is relatively new, the MapReduce layer has two implementation with Hazelcast and Infinispan, following the same design. It provides the MapReduce representation and implementations based on Hazelcast and Infinispan MapReduce modules. The minimal architecture of the MapReduce simulator of $Cloud^{2}Sim$ is depicted by Figure~\ref{fig:mapreduceImpl}, striping off the cloud simulation components. MapReduce Simulator can be configured with health monitoring and scaling. Hence, the execution time for varying the number of map() and reduce() invocations for various scenarios and simulations, as well as the health parameters such as load average and CPU utilization can be measured.
\begin{figure}[!htbp]
\begin{center}
 \resizebox{0.4\columnwidth}{!}{
  \includegraphics[width=0.4\textwidth]{mapreduceImpl.png}
 }
\end{center}
 \caption{Architecture of the MapReduce Component}
 \label{fig:mapreduceImpl}
\end{figure}

Both Hazelcast and Infinispan based MapReduce implementations have an $Initiator$ class that starts an instance of Hazelcast or Infinispan respectively and joins the main simulation cluster. While the $HzMapReduceSimulator$ or $InfMapReduceSimulator$ that runs from the master node coordinates and initiates the MapReduce jobs, the instances running $Initiator$ join the cluster and do the equal share of the jobs. The master node hosts the supervisor of the MapReduce job. The MapReduce implementation functions in verbose and non-verbose mode. In verbose mode, local progress of the individual map/reduce executions can be viewed from all the instances, where the final outcome is printed only to the master instance. Design of the $Cloud^{2}Sim$ MapReduce simulator and how it is integrated into $Cloud^{2}Sim$ is shown by Figure~\ref{fig:mrclass}.
\begin{figure}[!htbp]
\begin{center}
 \resizebox{0.60\columnwidth}{!}{
  \includegraphics[width=0.60\textwidth]{mr.png}
 }
\end{center}
 \caption{Class Diagram of the MapReduce Simulator}
 \label{fig:mrclass}
\end{figure}

The default application used to demonstrate the MapReduce simulations is a simple word count application, which lets the user visualize different MapReduce scenarios. This default implementation can be replaced by custom MapReduce implementations. Figure~\ref{fig:mrhi} shows the alternatives and execution flow of the scalable MapReduce simulation platform.
\begin{figure}[!htbp]
\begin{center}
 \resizebox{0.45\columnwidth}{!}{
  \includegraphics[width=0.45\textwidth]{mrhi.png}
 }
\end{center}
 \caption{Execution and Implementation Alternatives of the MapReduce Platform}
 \label{fig:mrhi}
\end{figure}

\subsection{Elasticity and Dynamic Scaling}
$Cloud^{2}Sim$ achieves elasticity through its implementations of dynamic scaling. Auto scaling and adaptive scaling are implemented by the packages $scale.auto$ and $scale.adaptive$. To prevent loss of information when the cluster scales in, synchronous backups are enabled by marking synchronous backup count as 1 in $hazelcast.xml$. This makes $Cloud^{2}Sim$ able to tolerate crashes, and avoid wasted work in long simulations, due to the availability of backups in different Hazelcast instances. Hazelcast stores the backups in different physical machines, whenever available, to minimize the possibility of losing all the backups during a hardware failure of a computer node. 

Since the time the instances are up and running can be measured at the individual node level, the cost of the simulation can be estimated, by assigning some cost values to the computing resources provided by the cluster instances. This is essentially viewing the service provided by the other nodes with the Hazelcast based IntelligentAdaptiveScaler as a cloud middleware Platform-as-a-Service. Moreover, the adaptive scaler design suits for any application, not limited to simulations. Hence this can be extended to use on any application that has a scaling requirement.

Distributed objects are removed by the user simulations as appropriate at the end of simulations. Hence it was decided not to use eviction policies of Hazelcast in $Cloud^{2}Sim$ by default, as it interferes with the user preference on dealing with the life-time of objects. Mostly the simulators clean the objects in the local instance and also shut down the local Hazelcast instance. The distributed objects still remaining in the Initiator instances after the simulation, are removed. This enables the Initiator instances to join the other simulations without the need to restart them.

\paragraph*{Summary:}
Cloud and MapReduce simulations can be executed on top of in-memory data grids, that executes over the computer clusters. Cycle sharing of the instances in the cluster, inspired by volunteer computing, is used as the model to achieve a scalable, adaptive, and elastic middleware platform for the simulations. Hazelcast and Infinispan are integrated into core CloudSim as a compatibility layer for a seamless integration and invocation of cloud simulations.

Multiple simulations can be executed in parallel using the $IntelligentAdaptiveScaler$ approach, where a single $Coordinator$ can ensure each tenant, that is represented by a cluster, has adequate resources allocated for an optimal execution. Multi-tenanted deployments of $Cloud^{2}Sim$ enable multiple distinct cloud and MapReduce simulations to be coordinated and scaled from a single health monitor and adaptive scaler, where a global view of the deployment is available to the $Coordinator$ node, as it consists of instances in all the clusters that it coordinates.
