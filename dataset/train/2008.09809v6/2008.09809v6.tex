\documentclass[final]{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{epstopdf}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{blindtext}

\usepackage{subfigure}
\usepackage{multirow}
\usepackage{float} 
\usepackage{color}
\usepackage{caption}

\usepackage{algorithm}
\usepackage{listings}

\def\etal{\textit{et~al.}}
\def\etc{\textit{etc.}}
\def\ie{\textit{i.e.}}
\def\wrt{\textit{w.r.t. }}
\def\vs{\textit{v.s. }}
\def\eg{\textit{e.g.}}
\usepackage{etoolbox}
\makeatletter
\AfterEndEnvironment{algorithm}{\let\@algcomment\relax}
\AtEndEnvironment{algorithm}{\kern2pt\hrule\relax\vskip3pt\@algcomment}
\let\@algcomment\relax
\newcommand\algcomment[1]{\def\@algcomment{\footnotesize#1}}
\renewcommand\fs@ruled{\def\@fs@cfont{\bfseries}\let\@fs@capt\floatc@ruled
	\def\@fs@pre{\hrule height.8pt depth0pt \kern2pt}\def\@fs@post{}\def\@fs@mid{\kern2pt\hrule\kern2pt}\let\@fs@iftopcapt\iftrue}
\makeatother



\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}

\def\cvprPaperID{****} \def\confYear{CVPR 2021}


\begin{document}
\title{Memory-based Jitter: Improving Visual Recognition on Long-tailed Data \\ with Diversity In Memory}
\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}
\author{
	Jialun Liu,\hspace{0.3cm} Jingwei Zhang,\hspace{0.3cm} Yi Yang,\hspace{0.3cm} Wenhui Li, \hspace{0.3cm} Chi Zhang, \hspace{0.3cm} Yifan Sun\\
	\\
	{Jilin University} \hspace{0.3cm}
	{Tsinghua University} \hspace{0.3cm}
	{Zhe Jiang University} \hspace{0.3cm}
	{Megvii Inc.}\\
}


\twocolumn[{
\renewcommand\twocolumn[1][]{#1}
\maketitle
\begin{center}
    \includegraphics[width=0.95\textwidth]{intro.pdf}
    \captionof{figure}{The proposed Memory-based Jitter (MBJ) enhances the tail feature diversity by accumulating historical features into a memory bank. We visualize the feature distribution of CIFAR-10 ~\cite{cifar1} with t-SNE ~\cite{van2014accelerating}. We focus on a specified class \textcolor{red}{ID-10}. \textbf{In (a)}, ID-10 has abundant training samples and is a head class. Its top-1 accuracy is 94.6\%. \textbf{In (b)}, we reduce the training samples of ID-10, so it becomes a tail class. Due to the lack of within-class diversity, its feature distribution collapses into a very limited scope and the top-1 accuracy dramatically decreases to 50.6\%. \textbf{In (c)}, MBJ collects historical features distributed among multiple training iterations into a memory bank. The historical features are scattered around the up-to-date features in the deeply-embedded space, yielding the so-called Memory-based Jitter (MBJ). Consequentially, MBJ enhances the tail data diversity and increases the classification accuracy of ID-10 from 50.6\% to 88.6\%.}
    \label{fig:intro}
\end{center}
}]

\begin{abstract}
This paper considers deep visual recognition on long-tailed data. To be general, we tackle two applied scenarios, \ie, deep classification and deep metric learning. Under the long-tailed data distribution, the majority classes (\ie, tail classes) only occupy relatively few samples and are prone to lack of within-class diversity. A radical solution is to augment the tail classes with higher diversity. To this end, we introduce a simple and reliable method named Memory-based Jitter (MBJ). We observe that during training, the deep model constantly changes its parameters after every iteration, yielding the phenomenon of \emph{weight jitters}. Consequentially, given a same image as the input, two historical editions of the model generate two different features in the deeply-embedded space, resulting in \emph{feature jitters}. Using a memory bank, we collect these (model or feature) jitters across multiple training iterations and get the so-called Memory-based Jitter. The accumulated jitters enhance the within-class diversity for the tail classes and consequentially improves long-tailed visual recognition. With slight modifications, MBJ is applicable for two fundamental visual recognition tasks, \emph{i.e.}, deep image classification and deep metric learning (on long-tailed data). Extensive experiments on five long-tailed classification benchmarks and two deep metric learning benchmarks demonstrate significant improvement. Moreover, the achieved performance are on par with the state of the art on both tasks.
\end{abstract}



\section{Introduction}
In visual recognition tasks, the long-tailed distribution of the data is a common and natural problem under realistic scenarios \cite{longtail1,longtail2,longtail3,ms1m}. A few categories (\emph{i.e.}, the head classes) occupy most of the data while the most categories (\emph{i.e.}, the tail classes) only occupy relatively few data. Such long-tailed distribution significantly challenges deep visual recognition, including both deep image classification \cite{BBN,decoupling,LDAM,longtail1,cls_longtail2,cb-focal,IEM} and deep metric learning \cite{featurecloud,featuretransfer,ms1m,YFC}. 


Explicitly, deep image classification and deep metric learning  have the fundamental differences. Specifically, there are two aspects, \emph{i.e. the task definition and the optimized objective}. On one hand, the definitions of two tasks are different. The classification task aims to recognize the already-seen classes. The categories of the training set and testing set are completely overlapped. The metric learning task aims to discriminate the unseen classes. The identities of training set and the testing set have no overlap. On the other hand, the optimized objectives of two tasks are also different. In classification task, the model aims to learn an accurate and unbiased classifier that outputs the correct label to the test instance as much as possible. In metric learning task, the model aims to learn a discriminative feature extractor that encourages the instances from the same class to be closer than those from different classes. To be general, this paper considers long-tailed visual recognition on these two elemental tasks with a uniform motivation.(As to be detailed in Section \ref{sec:cls} and Section \ref{sec:retrieval}.)

We recognize the insufficient within-class diversity of the tail classes as the most prominent reason that hinders long-tailed deep visual recognition. In the deeply-embedded feature space, a tail class is under-represented and thus hard to recognize. To validate this point, we visualize the deep embedding of CIFAR-10 dataset in Fig. \ref{fig:intro}. When a specified class (``ID-10") degrades from head (Fig.\ref{fig:intro} {(a)}) to tail (Fig.\ref{fig:intro} {(b)}), its visual concept collapses into a very limited scope in the deep embedding. Consequentially, when we employ the model for inference, samples from ``ID-10" may exceed the already-learned scope and are thus easily mis-classified. Intuitively, a radical solution is to augment the tail classes with higher diversity.

We notice two phenomena which are potential for enhancing the tail data diversity, \ie, the weight jitter and the feature jitter. During training, the deep model constantly changes its parameters after every iteration, yielding the phenomenon of \emph{weight jitter}. Consequentially, given a same image as the input, the models at two different training iterations generate two different feature representations in the deeply-embedded space, resulting in the phenomenon of \emph{feature jitter}. 

Since these jitters are distributed among historical models, we need to accumulate them across multiple training iterations for diversity enhancement. To this end, we employ a memory bank to store the desired jitters, and get the so-called Memory-based Jitter (MBJ). With slight modifications, MBJ is capable to accommodate two elemental visual recognition tasks, \ie, deep image classification and deep metric learning. \emph{On deep image classification}, MBJ collects the historical features (\ie, feature jitters). Consequentially, the feature memory bank accumulates abundant tail-feature jitters, and improves the classification accuracy on tail classes, as shown in Fig. \ref {fig:intro}{(c)}). \emph{On deep metric learning}, MBJ collects the weight vectors of the classifier layer instead of the features. Each weight vector is typically viewed as the prototype of a training class, so we name the corresponding memory bank as the prototype memory bank.  

Besides the accumulated jitter, MBJ is benefited from a novel re-sampling effect between head and tail classes. On both the classification and the deep metric learning task, MBJ assigns larger sampling rate to the tail classes (than to the head classes). Correspondingly, the tail classes occupy more memory-based jitters than the head classes, which compensates for the imbalanced distribution of the raw data. We note that some recent works \cite{BBN,decoupling} evidence that directly over-sampling the raw images, though alleviates the data imbalance problem to some extent, actually compromises deep embedding learning. In contrast, MBJ maintains the natural sampling frequency on the raw images and re-balances the head and tail classes in the memory bank. 

The main contributions of this paper are summarized as follows:

 We find that the weight jitters and the feature jitters are informative clues to gain extra diversity for tail data augmentation.

 We propose Memory-based Jitter to accumulate the jitters within a memory bank and improve deep visual recognition on long-tailed data. MBJ is compatible to two elemental visual tasks, \emph{i.e.}, deep image classification and deep metric learning, with slight modifications. 

 We conduct extensive experiments on five classification benchmarks and two metric learning benchmarks (person re-identification, in particular) under long-tailed scenario. On all these benchmarks, we demonstrate the superiority of our methods, which significantly improves the baseline and is on par with the state-of-the-art methods.


\section{Related Work}\label{sec:relate}

\subsection{Re-balancing strategy}\label{sec:relate_balance}
MBJ has a novel re-balancing strategy, compared with prior works on long-tailed visual recognition. Generally, re-balancing aims to highlight the tail classes during training. In prior works,  there are two major re-balancing types, \emph{i.e.}, \textit{re-weighting}\cite{huang2016learning, wang2017learning,cb-focal} and \textit{re-sampling}\cite{shen2016relay,resample1,resample2,resample3}. Re-weighting strategy allocates larger weights to tail classes in loss function. Re-sampling over-samples the raw images of the tail classes for training. 

Different from these prior works, MBJ re-samples the features / prototypes to highlight the tail classes. It thus avoids directly re-sampling the raw data. Since directly re-sampling the raw data actually compromises the deep embedding learning \cite{BBN,decoupling}, avoiding such operation substantially benefits MBJ. An ablation study carried out on the long-tailed CIFAR-10 dataset shows that when we completely remove the jitter augmentation, this novel re-sampling strategy still brings \textbf{} improvement over the baseline. The details are to be accessed in Section \ref{sec:abs_3}. 

\subsection{Memory-based learning}
The memory bank plays a critical role in MBJ. Both the weight jitters and the feature jitters are scattered among sequential training iterations. To accumulate these jitters for tail data augmentation, we employ a memory bank. Since memory-based learning has been explored in several computer vision domains, including unsupervised learning, semi-supervised learning and supervised learning  \cite{moco,meanteacher,temporal,XBM,fewshot1,fewshot2}, we make a detailed comparison as follows.

In unsupervised learning, \cite{moco} employs memory to include more data in the dictionary. It shows that larger optimization scope within a optimization step is beneficial for unsupervised learning \cite{moco}. In semi-supervised learning, \cite{temporal,meanteacher} enforce consistency between historical predictions. Such consistency offers auxiliary supervision for the unlabeled data. In supervised deep metric learning, \cite{XBM} uses memory to enhance the hard mining effect. Regardless of their objectives of using memory, they all hold a negative attitude towards the jitters. \cite{moco} and \cite{temporal,meanteacher} suppress the jitters with momentum and consistency constraint, respectively. \cite{XBM} tries to avoid the jitters by delaying the injection of memory. 

In contrast to their negative attitude towards jitters, we find that the jitters are informative for long-tailed visual recognition. As a major contribution of this work, we analyze the mechanism in Section \ref{sec:jitter} and experimentally validate its effectiveness in Section \ref{sec:abs_3}. 

\textbf{Moreover}, we notice a recent work IEM \cite{IEM} also employs memory for long-tailed image classification. We compare MBJ against IEM in details for clarity. Our method significantly differs from IEM in three aspects, \emph{i.e.}, the applied task, the mechanism and the achieved performance. First, IEM is specified for image classification, while MBJ improves both image classification and deep metric learning with a uniform motivation. Second, IEM considers tail classes are harder to recognize, and thus employ more prototypes from the memory for higher redundancy, while MBJ employs the jitters in memory to augment the diversity of tail data.
Finally, on image classification task, MBJ maintains competitive performance with significantly higher computing efficiency. IEM requires extraordinary large amount of memory (up to  per class), and achieves Top-1 accuracy of  on iNaturelist18. In contrast, MBJ is more memory-efficient and more accuracy. For example, on iNaturelist18 \cite{inaturalist}, MBJ only stores  memorized features in total and achieves Top-1 accuracy of  and  at  and  epochs, respectively.

With these comparison, we find that MBJ is featured for the memory-based feature augmentation. It is orthogonal to many prior works. Specifically, we note a very recent work RIDE~\cite{wang2020long} using multiple classifiers (experts) ensemble to improve the accuracy of head and tail classes, simultaneously. MBJ can be integrated into RIDE~\cite{wang2020long} for better performance gains.

\section{Proposed Method}\label{sec:methods}
Basically, MBJ accumulates historical jitters within a memory bank to enhance the diversity of the tail classes. Under this framework, MBJ adopts slight modifications to accommodate two fundamental visual recognition tasks, \emph{i.e.} feature jitters for image classification and prototype jitters for deep metric learning. In this section, we first analyze the weight jitters and the feature jitters in Section \ref{sec:jitter}. Then we introduce the MBJ for image classification and deep metric learning in Section \ref{sec:cls} and Section \ref{sec:retrieval}, respectively.

\subsection{Weight Jitters and Feature Jitters}\label{sec:jitter}
\begin{figure}[t]
	\centering
	\includegraphics[width=0.9\linewidth]{jitter.pdf} 
\caption{Quantitative statistics on feature jitters and weight jitters. We use a long-tailed CIFAR-10 \cite{cifar1} as the toy dataset and focus on the tail class / image. As the accumulated features / prototypes increase, the angular variance gradually increases.}
\vspace{-0.5cm}
\label{fig:div}
\end{figure}

\begin{figure*}[t]
	\centering
	\includegraphics[width=0.9\linewidth]{framework.pdf} 
	\caption{MBJ for deep image classification collects historical features into memory with higher concentration on tail classes. MBJ randomly samples the raw images and transforms them into a batch of features with a convolutional neural network. Given current batch features, MBJ uses a class-specific sampling strategy to collect the features from different classes. Head classes have smaller sampling probability and tail classes have larger sampling probability. These features are stored into a memory bank, \ie, feature memory. MBJ combines the feature memory and the batch features to learn the classifier. The feature memory compensates the tail classes with higher diversity. }
	\label{fig:cls}
	\vspace{-0.4cm}
\end{figure*}


To illustrate the phenomena of weight jitters and feature jitters, we conduct a toy experiment on CIFAR-10 \cite{cifar1}. We set a specified class to contain very limited samples (\ie, 50 samples) so that it turns into a tail class. We train a deep classification model to convergence and then continue the training for observation purpose. Within the following iterations, we record two objects, \ie, 1) the prototype (\ie, the weight vector in the classification layer) of the tail class and 2) the feature of a single tail sample. As the training iterates, both the prototypes and the features accumulate, allowing a quantitative statistic on their variances. We visualize the geometrical angular variance of the accumulated features / prototypes in Fig. \ref{fig:div}, from which we draw two observations. 

First, we observe considerable variance among the accumulated weight vectors (\ie, prototypes), as well as the accumulated features. It indicates that among multiple iterations, both the prototype of a single class and the feature of a single image keep on changing itself, yielding the phenomena of weight jitters and feature jitters, respectively. 

Second, we observe that the above-described jitters require certain training iterations to accumulate before they reach stable status. When there is only one single feature / prototype, the corresponding variance is naturally . As the number of accumulated features / prototypes increases, the variance gradually grows until reaching a stable level. 

Based on the above observation, we device MBJ. MBJ uses a memory bank to collect the historical features / prototypes, so as to accumulate the feature / weight jitters for tail augmentation. 

 
\subsection{MBJ for Deep Image Classification}\label{sec:cls}



The pipeline of MBJ for deep image classification is illustrated in Fig.\ref{fig:cls}. In the raw image space, MBJ randomly samples the images without re-balancing the head and tail classes. In each training iteration, the deep model transforms the raw images into a batch of features. Given the features in current mini-batch, MBJ stores them into a memory bank.
The memory bank has a larger size than the mini-batch, so it is capable to accumulate historical features across multiple training iterations. 

When collecting the features, MBJ lays emphasis on the tail classes, so that the tail and head data will be re-balanced. Specifically, MBJ assigns small sampling probabilities to head classes, as well as relatively large sampling probabilities to tail classes, which is formulated as:


where  is the corresponding sampling probability of class ,  is the sample number of the -th class,  is the total number of classes,  and  is a hyper-parameter to control the strength of re-balancing. A larger  results in higher priority on accumulating the tail features. We use  in all of our experiments.

To control the memory size, we use a queue strategy for updating the memory bank. After the memory bank reaches its size limitation, we enqueue the newest features (\ie, the features in current mini-batch), and dequeue the oldest ones.

Given the feature memory and the batch features, MBJ combines both of them to learn the classifier in a joint optimization manner. Specifically, MBJ uses the features in current mini-batch and the weight vectors in the classification layer to deduce a cross-entropy loss, \ie, the loss . Meanwhile, MBJ uses the memorized features and the weight vectors in the classification layer to deduce another cross-entropy loss, \ie, the loss . MBJ sums up those two losses by:
\begin{small}

\end{small}
in which  is a weighting factor. Algorithm~\ref{alg:cls} provides the pseudo-code of MBJ for deep image classification task.

\begin{algorithm}[h]
	\caption{Pseudocode of MBJ in Classification task.}
	\label{alg:cls}
	\definecolor{codeblue}{rgb}{0.25,0.5,0.5}
	\lstset{
		backgroundcolor=\color{white},
		basicstyle=\fontsize{7.2pt}{7.2pt}\ttfamily\selectfont,
		columns=fullflexible,
		breaklines=true,
		captionpos=b,
		commentstyle=\fontsize{7.2pt}{7.2pt}\color{codeblue},
		keywordstyle=\fontsize{7.2pt}{7.2pt},
	}
\begin{lstlisting}[mathescape,language=python]  
Train network Q conventionally until convergence.
Initialize the memory as queue M.
Set the learning rate with a small value.
#  input: data, target: labels, batch_f: batch_feature
for input, target in loader:
    batch_f = Q.forward(input) 
		
    # memory update with higher sampling probability on
    # the tail classes
    enqueue(M,(batch_f.detach(),target))
    dequeue(M)
		
    # batch loss and memory loss
    L_batch = criterion_batch(batch_f,target)
    L_memory = criterion_memory(M)
    L_total = L_memory   + L_batch  
    
    L_total.backward()
    optimizer.step()
\end{lstlisting}
\end{algorithm}


\subsection{MBJ for Deep Metric Learning}\label{sec:retrieval}
A popular baseline \cite{softtriple,featurecloud,circleloss,N-pair,HTL,svdnet,pcb} for deep metric learning is as follows: during training, we learn a classification model on the training set. The weight vectors in the classification layer are typically recognized as prototypes for each class. During testing, the distance between two images are measured under the learned deep embedding. 
Based on this baseline, MBJ collects the historical prototypes into a prototype memory with emphasis on the tail classes.

The sampling strategy is exactly the same as in Eq.\ref{eq:sampling}, so we omit the detailed description. Given the historical prototypes in memory and the up-to-date prototypes, MBJ combines both to learn the features. For clarity, we illustrate the learning process with focus on a single feature  under optimization. 

To learn with the up-to-date prototypes  ( is the total number of training classes), MBJ adopts a popular deep metric learning method, \ie, CosFace \cite{cosface}, which is formulated as:
\begin{small}

\end{small}
in which  is the number of classes,  is the prototype of the target class,  is a scale factor and  is a margin for better similarity separation.

To learn with the prototype memory, MBJ needs to deal with multiple positive prototypes associated with feature . It is because the weight vector of the target class in the classifier may be sampled at several training iterations. Let us assume there are  positive prototypes (\ie, weight vectors of the target class), and  negative prototypes  (\ie, weight vectors of the non-target class). We find that a recent deep metric learning method, \ie, Circle Loss\cite{circleloss}, allows multiple similarities associated with a single sample feature. 
Accordingly to Circle Loss, we define the loss function associated with the prototype memory by:
\begin{small}

\end{small}

Similar to Eq.\ref{eq:loss}, MBJ sums the above two losses (\ie,  and ) to optimize the feature . We note that two editions of MBJ share a unified framework, except for the jitter type. 
To improve the classification accuracy, MBJ memorizes the feature jitters; To improve the retrieval accuracy, MBJ memorizes the prototype jitters. They have a dual pattern against each other. Algorithm~\ref{alg:dml} provides the pseudo-code of MBJ for deep metric learning task.
\begin{algorithm}[t]
\caption{Pseudocode of MBJ in DML task.}
\label{alg:dml}
\definecolor{codeblue}{rgb}{0.25,0.5,0.5}
\lstset{
backgroundcolor=\color{white},
basicstyle=\fontsize{7.2pt}{7.2pt}\ttfamily\selectfont,
columns=fullflexible,
breaklines=true,
captionpos=b,
commentstyle=\fontsize{7.2pt}{7.2pt}\color{codeblue},
keywordstyle=\fontsize{7.2pt}{7.2pt},
}
\begin{lstlisting}[mathescape,language=python]  
Train network Q conventionally until convergence.
Initialize the memory as queue M.
Set the learning rate with a small value.
#  input: data, target: labels
#  batch_f: batch_feature, batch_w: batch_weight	
for input, target in loader:
    batch_f = Q.forward(input)
    batch_w = Q.fc.weight
    
    # memory update with higher sampling probability on
    # the tail calsses
    enqueue(M, (batch_w.detach(),target))
    dequeue(M)
    
    # batch loss and memory loss
    L_batch_ = criterion_batch(batch_f,batch_w,target)
    L_memory = criterion_memory(batch_f,M,target)
    L_total = L_memory   +  L_batch
    
    L_total.backward()
    optimizer.step()
\end{lstlisting}
\end{algorithm}



\subsection{Discussions}\label{sec:discussion}
MBJ is featured for its re-balancing strategy and its augmentation manner. Although re-balancing the features / prototypes (instead of re-balancing the raw data) considerably benefits MBJ (as introduced in Section \ref{sec:relate_balance}), we note that the improvement is mainly because the accumulated jitters increase the tail data diversity. Removing the jitters or using other augmentation method significantly compromise MBJ. The details are to be accessed in Section \ref{sec:abs_3}.

\begin{table*}[tb]
\small 
	\centering
	\setlength{\tabcolsep}{18.5pt}
	\begin{tabular}{l|ccc|ccc}
		\hline
		\multicolumn{1}{c|}{Dataset}          & \multicolumn{3}{c|}{Long-tailed CIFAR-10} & \multicolumn{3}{c}{Long-tailed CIFAR-100}       \\ \hline
		\multicolumn{1}{c|}{Imbalanced ratio (IR)} & 100             & 50              & 10    & 100            & 50             & 10             \\ \hline
		Basel. (CE)                           & 70.4           & 74.8           & 86.4 & 38.3          & 43.9          & 55.7          \\
		Focal Loss  ~\cite{focalloss}                      & 70.4           & 76.7           & 86.7 & 38.4          & 44.3          & 55.8          \\
		CB Focal ~\cite{cb-focal}                            & 74.6           & 79.3           & 87.1  & 39.6          & 45.2          & 58.0          \\
		LDAM-DRW ~\cite{LDAM}                             & 77.0           & 81.0           & 88.2 & 42.0          & 46.6          & 58.7          \\
		BBN ~\cite{BBN}                                  & 79.8           & 82.2           & 88.3 & 43.7          & 47.0          & 59.1          \\
		SSP ~\cite{yang2020rethinking}                                  & 77.8           & 82.1           & 88.5 & 43.4          & 47.1          & 58.9          \\ 
		De-confound-TDE  ~\cite{tang2020long}                     & 80.6            & 83.6            & 88.5  & 44.1           & 50.3           & 59.6           \\
		RIDE (4 experts)  ~\cite{wang2020long}                     & -            & -            & -  & 48.7           & 59.0           & 58.4           \\
		\hline
		MBJ                                   & \textbf{81.0}  & \textbf{86.6}  & \textbf{88.8} & \textbf{45.8} & \textbf{52.6} & \textbf{60.7} \\
		MBJ + RIDE (4 experts)                & -              & -              &-              & \textbf{\textcolor{red}{49.9}}          & \textbf{\textcolor{red}{59.8}}             & \textbf{\textcolor{red}{58.9}}             \\ 
		\hline
	\end{tabular}
	\caption{Comparison with baseline and the state-of-the-art methods on long-tailed CIFAR-10 and CIFAR-100. We report top-1 accuracy rates. The results of MBJ are in \textbf{bold}. The results of ``MBJ + RIDE'' are in \textbf{\textcolor{red}{red}}.   denotes our reproduced results with released code. }
	\label{tab:CIFAR}
\end{table*}


\begin{table*}[ht]
\small 
	\centering
	\setlength{\tabcolsep}{3.5pt}
	\begin{tabular}{lcccccccc}
		\hline
		\multicolumn{1}{c|}{\multirow{2}{*}{Methods}} & \multicolumn{4}{c|}{ImageNet-LT}                                                                        & \multicolumn{4}{c}{Places-LT}                                                             \\ \cline{2-9} 
		\multicolumn{1}{c|}{}                         & Many-shot                 & Medium-shot               & Few-shot                  & \multicolumn{1}{c|}{Overall}       & Many-shot                 & Medium-shot               & Few-shot                  & Overall              \\ \hline
		\multicolumn{1}{l|}{Basel.(CE)}               & 65.9                 & 37.5                 & 7.7                  & \multicolumn{1}{c|}{44.4}          & 45.7                 & 27.3                 & 8.2                  & 30.2                 \\
		\multicolumn{1}{l|}{Lifted Loss ~\cite{oh2016deep}}              & 35.8                 & 30.4                 & 17.9                 & \multicolumn{1}{c|}{30.8}          & 41.1                 & 35.4                 & 24.0                 & 35.2                 \\
		\multicolumn{1}{l|}{Focal Loss ~\cite{focalloss} }               & 36.4                 & 29.9                 & 16.0                 & \multicolumn{1}{c|}{30.5}          & 41.1                 & 34.8                 & 22.4                 & 34.6                 \\
\multicolumn{1}{l|}{OLTR ~\cite{liu2019large}}                     & 43.2                 & 35.1                 & 18.5                 & \multicolumn{1}{c|}{35.6}          & 44.7                 & 37.0                 & 25.3                 & 35.9                 \\
		\multicolumn{1}{l|}{Decouple-NCM ~\cite{decoupling}}             & 56.6                 & 45.3                 & 28.1                 & \multicolumn{1}{c|}{47.3}          & 40.4                 & 37.1                 & 27.3                 & 36.4                 \\
		\multicolumn{1}{l|}{Decouple-cRT ~\cite{decoupling}}             & 61.8                 & 46.2                 & 27.4                 & \multicolumn{1}{c|}{49.6}          & 42.0                 & 37.6                 & 24.9                 & 36.7                 \\
		\multicolumn{1}{l|}{Decouple--norm ~\cite{decoupling}}    & 59.1                 & 46.9                 & 30.7                 & \multicolumn{1}{c|}{49.4}          & 37.8                 & 40.7        & 31.8                 & 37.9                 \\
		\multicolumn{1}{l|}{Decouple-LWS ~\cite{decoupling}}             & 60.2                 & 47.2                 & 30.3                 & \multicolumn{1}{c|}{49.9}          & 40.6                 & 39.1                 & 28.6                 & 37.6                 \\
		\multicolumn{1}{l|}{FSA ~\cite{chu2020feature}}                      & -                 & -                 & -                 & \multicolumn{1}{c|}{-}          & 42.8                 & 37.5                 & 22.7                 & 36.4                 \\
		\multicolumn{1}{l|}{De-confound-TDE  ~\cite{tang2020long}   }          & 62.7                 & 48.8        & 31.6                 & \multicolumn{1}{c|}{51.8} & -                    & -                    & -                    & -                    \\ 
		\multicolumn{1}{l|}{RIDE (4 experts)  ~\cite{wang2020long}   }          & 67.8                 & 53.4        & 36.2                 & \multicolumn{1}{c|}{56.6} & -                    & -                    & -                    & -                    \\
		\hline
		\multicolumn{1}{l|}{MBJ}                      & \textbf{61.6}                 & \textbf{48.4}                 & \textbf{39.0}        & \multicolumn{1}{c|}{\textbf{52.1}}          & \textbf{39.5}                 & \textbf{38.2}                 & \textbf{35.5}        & \textbf{38.1}        \\ 
		\multicolumn{1}{l|}{MBJ + RIDE (4 experts)}                      & \textbf{\textcolor{red}{68.4}}                 & \textbf{\textcolor{red}{54.1}}                 & \textbf{\textcolor{red}{37.7}}        & \multicolumn{1}{c|}{\textbf{\textcolor{red}{57.7}}}          & -                 & -                 & -        & -        \\
		\hline
		& \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{}               & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} \\
		& \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{}               & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} \\
		& \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{}               & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{}
	\end{tabular}
	\vspace{-11mm}
	\caption{Long-tailed recognition accuracy on ImageNet-LT and Places-LT. On ImageNet-LT, all models used the ResNeXt-50 backbone. On Places-LT, start from an ImageNet pre-trained ResNet152. We report the Top-1 accuracy. The results of MBJ are in \textbf{bold}. The results of ``MBJ + RIDE'' are in \textbf{\textcolor{red}{red}}.  denotes our reproduced results with released code.}
	\label{tab:IP}
	\vspace{-0.5cm}
\end{table*}
\section{Experiments}\label{sec:exp}
\subsection{Datasets and setup} \label{sec:setting}
\noindent \textbf{Classification task}. 
Under long-tailed image classification scenario, we evaluate MBJ on 5 datasets, \ie, CIFAR-10, CIFAR-100, ImageNet-LT, Places-LT and iNaturalist18. 

For CIFAR datasets, we synthesize several long-tailed version, following \cite{LDAM}. We use an imbalance ratio to denote the ratio between sample size of the most frequent and least frequent class. Imbalanced ratio (IR) in our experiments are set to ,  and , respectively.

ImageNet-LT, Places-LT and iNaturalist18 are publicly available long-tailed dataset. In ImageNet-LT, the maximum of images per class is  and the minimum of images per class is . In Places-LT, the largest class has  images while the smallest ones have  images. Their test set is balanced. The iNaturalist18 dataset is a large-scale dataset with extremely imbalanced label distribution. It has  images from  classes. We adopt the official training and validation splits for our experiments. 

\noindent \textbf{Deep metric learning}. 
We employ the person re-identification (re-ID) \cite{zhong2018camstyle,sun2018beyond,zheng2019joint} task to evaluate MBJ on deep metric learning. Given a query person, re-ID aims to spot the appearance of the same person in the gallery. The keynote of re-ID is to learn accurate metric that measures the similarity between query and gallery images. We adopt two dataset, \ie, Market-1501\cite{market} and DukeMTMC-reID\cite{dukereid1,dukereid2}. Following the settings in feature cloud \cite{featurecloud}, we synthesize several long-tailed editions based on the original datasets. For comprehensive evaluation, we vary the number of head classes as ,  and , respectively. All the tail classes contain only  images per class. 

\subsection{Implementation Details} \label{sec:imp}
\noindent \textbf{Parameter Settings.} For both task, the re-balancing factor  (Eq. \ref{eq:sampling}) is set to  and the memory size is set to  ( is the total number of training classes). For classification task, the loss ratio  (Eq. \ref{eq:loss}) is set to . In deep metric learning task, the loss ratio  (Eq. \ref{eq:loss}) is set to . Please refer to supplementary materials for more details. 

\subsection{Experiments on image classification}\label{sec:cls_exp}

\subsubsection{Evaluation  on long-tailed CIFAR-10 / 100}

Table \ref{tab:CIFAR} compares MBJ with the baseline and several state-of-the-art methods on the long-tailed CIFAR-10 and CIFAR-100.
Comparing MBJ with ``Basel. (CE)'', we find that MBJ significantly improves the baseline.  Under the setting of IR , for instance, MBJ surpasses the baseline by  and  top-1 accuracy on CIFAR-10 and CIFAR-100, respectively. Comparing MBJ with several state-of-the-art methods, we clearly observe the superiority of MBJ. For example, comparing MBJ with De-confound-TDE ~\cite{tang2020long}, under all the IR settings, MBJ achieves higher classification accuracy. Especially under the setting of IR , MBJ marginally surpasses it by   and  top-1 accuracy on CIFAR-10 and CIFAR-100, respectively.

When we compare MBJ against the recent work RIDE~\cite{wang2020long}, the accuracy of MBJ is lower than that of RIDE~\cite{wang2020long}. It is because RIDE uses multiple classifiers experts (\emph{i.e.}, 4) ensemble. MBJ is featured of the memory-based feature augmentation, and thus it can be integrated into RIDE, achieving the better performance.



\subsubsection{Evaluation on ImageNet-LT and Places-LT}

The experiment results on ImageNet-LT and Places-LT are shown in Table \ref{tab:IP}. 
These two datasets offer separate evaluations under Many-shot (more than 100 training images per class), Medium-shot (20 to 100 training images per class), Few-shot (less than 20 images per class) and the Overall performance, respectively. 
From Table \ref{tab:IP}, We draw three observations as follows: First, under Many-shot and Medium-shot, MBJ achieves comparable accuracy. For example, on ImageNet-LT, MBJ is slightly lower than De-confound-TDE ~\cite{tang2020long} by  (Many-shot) and  (Medium-shot). Second, under Few-shot, MBJ exhibits significant superiority against all the competing methods. On ImageNet-LT, MBJ surpasses the second best method RIDE ~\cite{wang2020long} by  top-1 accuracy. Third, due to significant superiority under the Few-shot, as well as the comparable performance under Many-shot and Medium-shot, the Overall performance of MBJ is on par with the state of the art. 

Moreover, we notice that most the methods (including the proposed MBJ) actually lose some accuracy under the Many-shot, compared with the baseline (\ie, ``Basel''). Only RIDE~\cite{wang2020long} improves the accuracy of Many-shot, Medium-shot and Few-shot, simultaneously. We combine MBJ with RIDE, then we further improve the performance. 


\begin{table}[h]
\small 
	\centering
	\begin{tabular}{l|cc}
		\hline
		Methods               & 90 epochs & 200 epochs \\ \hline
		Basel.(CE)                    & 61.1      & 65.3          \\
IEM* ~\cite{IEM}                   & 67.0      & -          \\
		LDAM ~\cite{LDAM}                  & 64.6      & -          \\
	    FSA ~\cite{chu2020feature}         & 65.9      & -          \\
	    CE+SSP ~\cite{yang2020rethinking}  & 64.4      & -         \\
		LDAM-DRW  ~\cite{LDAM}        & 64.6     & 66.1          \\
		BBN ~\cite{BBN}                 & 66.3      & 69.7       \\
		Decouple-NCM ~\cite{decoupling}         & 58.2      & 63.1       \\
		Decouple-cRT ~\cite{decoupling}         & 65.2      & 67.6       \\
		Decouple- -norm ~\cite{decoupling} & 65.6      & 69.3       \\
		Decouple-LWS ~\cite{decoupling}         & 65.9      & 69.5       \\
		RIDE (4 experts) ~\cite{wang2020long}                                   & -         & 72.6       \\
		\hline
		MBJ                   & \textbf{66.9}      & \textbf{70.0}       \\ 
	    MBJ + RIDE (4 experts)       & -             & \textbf{\textcolor{red}{73.2}} \\
		\hline
	\end{tabular}
	\caption{Comparison with baseline and the state-of-the-art methods on iNaturalist18. All models use the ResNet-50 \cite{resnet} backbone. We report the Top-1 accuracy. IEM* denotes the IEM ~\cite{IEM} using global feature for fair comparison.  denotes the results copied from BBN ~\cite{BBN}.   denotes our reproduced results with released code. The results of MBJ are in \textbf{bold}. The results of ``MBJ + RIDE'' are in \textbf{\textcolor{red}{red}}.}  
	\label{tab:ina}
	\vspace{-6mm}
\end{table}

\subsubsection{Evaluation on iNaturalist18}

We further evaluate MBJ on the large-scale long-tailed dataset \emph{i.e.} iNaturalist18. The results are shown in Table \ref{tab:ina}. For fair comparison, we report performance achieved at  and  training epochs, following the common practice of previous works. Under both settings, MBJ achieves competitive accuracy. When MBJ is combined with RIDE~\cite{wang2020long}, ``MBJ + RIDE'' achieves the best performance.

\subsection{Experiments on deep metric learning}\label{sec:retrieve_exp}
We evaluate MBJ under a popular deep metric learning task (\ie, re-ID). We note that the long-tail problem on this task has been noticed recently, and the competing methods are relatively few. Table \ref{tab:reid} compares MBJ with re-ID baseline (CosFace ~\cite{wang2018cosface}) and a state-of-the-art method (Feature Cloud ~\cite{featurecloud}), from which we draw two observations:

First, under typical long-tailed distribution, MBJ significantly improves the re-ID baseline. When there are only 20 head classes (``H20''), MBJ achieves  and  mAP on Market-1501 and DukeMTMC-reID, respectively. 

Second, MBJ marginally surpasses the recent state-of-the-art, \ie, Feature Cloud \cite{featurecloud}. For example, under three long-tailed condition on Market-1501, MBJ achieves ,  and  mAP, which are higher than Feature Cloud by  ,  and , respectively. MBJ obtain the new state-of-the-art performance.

\begin{table*}[th]
\small 
\centering
\setlength{\tabcolsep}{7.5pt}
\begin{tabular}{c|cccccc|cccccc}
\hline
\multirow{3}{*}{Method}                            & \multicolumn{6}{c|}{Market-1501}                                                                                                                                                                    & \multicolumn{6}{c}{DukeMTMC-reID}                                                                                                                                                                  \\ \cline{2-13} 
                                                   & \multicolumn{2}{c|}{H100}           & \multicolumn{2}{c|}{H50}             & \multicolumn{2}{c|}{H20}            & \multicolumn{2}{c|}{H100}           & \multicolumn{2}{c|}{H50}             & \multicolumn{2}{c}{H20}             \\ \cline{2-13} 
                                                   & mAP                            & R-1                            & mAP                            & R-1                            & mAP                            & R-1                            & mAP                            & R-1                            & mAP                            & R-1                            & mAP                            & R-1                            \\ \hline
Baseline                                           & 62.8                           & 83.8                           & 60.5                           & 80.7                           & 55.6                           & 78.6                           & 52.6                           & 70.3                           & 48.0                           & 67.7                           & 47.0                           & 66.0                           \\
Feature cloud ~\cite{featurecloud} & 68.7                           & 86.5                           & 67.3                           & 84.9                           & 64.1                           & 83.2                           & 55.6                           & 74.8                           & 53.1                           & 73.0                           & 52.4                           & 72.7                           \\ \hline
MBJ                                                & \textbf{72.6} & \textbf{88.4} & \textbf{68.8} & \textbf{86.2} & \textbf{66.7} & \textbf{84.8} & \textbf{60.8} & \textbf{78.6} & \textbf{56.7} & \textbf{74.4} & \textbf{57.9} & \textbf{75.5} \\ \hline
\end{tabular}
\caption{Evaluation of MBJ on long-tailed re-ID task. Under each dataset, there are three different long-tailed conditions, \emph{i.e.}, ``H100'': 100 head classes. ``H50'': 50 head classes. ``H20'': 20 head classes. All the tail classes contain only  images per class. We report Rank-1 accuracy (R-1) and mAP on Market-1501 and DukeMTMC-reID. Best performance are in \textbf{bold}.}
\label{tab:reid}
\vspace{-0.5cm}
\end{table*}


\subsection{Ablation study}\label{sec:ab_exp}

\subsubsection{Modifications on jitter types}
\begin{figure}[h] 
	\centering 
	\includegraphics[width=0.85\linewidth]{ads.pdf} 
	\vspace{-0.1cm}
\caption{Three different editions of MBJ on CIFAR-10 (IR 100) and long-tailed Market-1501 (H50). MBJ-W stores weight jitters. MBJ-F stores feature jitters. MBJ-WF stores both weight and feature jitters. (a) In classification, MBJ-F achieves the best accuracy on classification. (b)In deep metric learning, MBJ-W achieves the best Rank-1 accuracy.}
\label{fig:ablation}
\end{figure}
Though MBJ shares a uniform underlying mechanism for classification and deep metric learning, it still has some task-specific modifications. For deep image classification, MBJ collects feature jitters while for deep metric learning, MBJ collects weight jitters. We explain such modifications with an ablation study. We implement three different editions of MBJ, \ie, MBJ-W (storing weights jitters), MBJ-F (storing feature jitters) and MBJ-WF (storing both weight and feature jitters). The comparison between these three modifications are shown in Fig. \ref{fig:ablation}.

For deep classification task, we draw two observations from Fig. \ref{fig:ablation}{(a)}. First, compared with the baseline, MBJ-F significantly improves the accuracy, while MBJ-W barely shows any improvement. Second, MBJ-WF is inferior to MBJ-F with . It indicates that adding weight jitters actually deteriorates MBJ-F. So we adopts feature memory to implement MBJ for classification task. 

For deep metric learning task, we draw three observations from Fig. \ref{fig:ablation} (b). First, all editions of MBJ significantly improve the re-ID accuracy over the baseline. Second, MBJ-W is superior than MBJ-F (with +1.1\% Rank-1 accuracy). Third, comparing MBJ-W with MBJ-WF, adding MBJ-F to MBJ-W does not bring incremental improvement.

Based on the above investigations, MBJ employs a respective memory type for these two long-tailed visual recognition tasks, \ie, MBJ-F (feature memory) for deep image classification and MBJ-W (prototype memory) for deep metric learning. 


\subsubsection{Decoupling re-balancing and jitters} \label{sec:abs_3}

When MBJ collects jitters into the memory bank, it has two coupling effects, \ie, a) re-balancing the head and tail distribution and b) accumulating the jitters. On the long-tailed CIFAR-10 (IR 100), we design an ablation study to decouple those two effects.
Specifically, we train three different models as follows:
1) Model RR re-balances the raw images by over-sampling the tail classes.
2) Model FR re-balances the feature by over-sampling the tail features in current mini-batch. However, it does NOT collect historical features into the memory bank. In another word, Variant A maintains the re-balancing strategy of MBJ and removes the jitters.
3) Model FR+RJ over-samples the tail features and augments them with 
Gaussian-distributed disturbance.


\begin{figure}[h] 
\small 
	\centering 
	\includegraphics[width=0.8\linewidth]{abs.pdf} 
	\vspace{-3mm}
	\caption{Ablation study on the effect of re-balancing strategy and jitters.  ``Basel.'' is the baseline without any re-balancing or tail augmentation. ``RR'' re-balances the raw images. ``FR'' adopts the feature re-balancing in MBJ and removes the feature jitters. ``FR+RJ'' replaces the feature jitters with Gaussian-distributed disturbance (for tail augmentation). The proposed MBJ marginally surpasses the baseline and all the other counterparts. It indicates that the jitter effect is the major reason for the superiority of MBJ.}
	\label{fig:nb}
	\vspace{-4mm}
\end{figure}
Fig. \ref{fig:nb} compares these three models with the baseline and MBJ, from which we draw two observations:First, comparing ``FR'' against ``RR'' and ``Basel.'', we find that re-balancing the features considerably benefits MBJ. Specifically, directly re-balancing the raw data actually brings no obvious improvement over the baseline. It is consistent with the observation in LDAM ~\cite{LDAM}. According to \cite{LDAM, BBN}, it is because directly re-sampling the raw data compromises the deep embedding learning. In contrast, re-balancing the features avoids deterioration on the deep embedding and considerably increases the top-1 accuracy by . Second, comparing ``MBJ'' against ``FR'' and ``FR+RJ'', we find that the accumulated jitters is the dominating reason for the superiority of MBJ. While re-balancing the feature (FR) improves the baseline by  top-1 accuracy, accumulating jitters (MBJ) further brings a much larger improvement of  accuracy. Moreover, though adding random disturbance (``FR+RJ'') does obtain some degree of feature augmentation as well, its improvement is very limited and is much inferior to the proposed MBJ. 

\section{Conclusion}
This paper proposes Memory-based Jitter (MBJ) to improve long-tailed visual recognition under both deep classification and deep metric learning tasks. The insights behind MBJ are two-fold. First, during training a deep model, the weight vectors and the features keep on changing after each iteration, resulting in the phenomena of (weight and feature) jitters.
Second, accumulating these jitters provides extra augmentation for the tail data. Experimental results confirm MBJ significantly improves the baseline and achieves state-of-the-art performance on both deep image classification and deep metric learning. 

An interesting observation is that MBJ favors different types of memory, depending on the specified task. For deep image classification, it favors the feature memory, while for deep metric learning, it favors the prototype memory. The underlying reasons for such preference are remained to be explored in our future work.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
