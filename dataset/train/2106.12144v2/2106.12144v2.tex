
\documentclass{article} \usepackage{iclr2022_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}

\usepackage{subfig}
\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    

\newcommand*\dbar[1]{\overline{\overline{\lower0.2ex\hbox{}}}}
\newcommand{\harrow}[1]{\mathstrut\mkern2.5mu#1\mkern-11mu\raise1.6ex\hbox{}}
 
\newcommand{\vhid}{\mathbf{h}}

\newcommand{\harrowStable}[1]{\overset{\rightharpoonup}{#1}}
\newcommand{\Janossy}{Janossy }

\usepackage{array,booktabs}       \usepackage{multirow}
\usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{pgfplots}
\pgfplotsset{compat=1.9}
\usepackage{xcolor}         \usepackage{graphicx}
\usepackage{xcolor}
\usepackage{amsthm}
\usepackage{wrapfig}
\usepackage{adjustbox}
\usepackage{bm}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}


\newcommand{\xhdr}[1]{\noindent{\bfseries #1}.}
\newcommand\todo[1]{\textcolor{red}{#1}}
\newcommand{\pj}[1]{\downarrow_{#1}\!\!}

\title{NodePiece: Compositional and Parameter-Efficient Representations of Large Knowledge Graphs}



\author{Mikhail Galkin,Etienne Denis, Jiapeng Wu and William L. Hamilton \\
Mila, McGill University\\
Montreal, Canada\\
\texttt{\{mikhail.galkin,deniseti,jiapeng.wu,hamilton\}@mila.quebec} \\
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}


\maketitle

\begin{abstract}
Conventional representation learning algorithms for knowledge graphs (KG) map each entity to a unique embedding vector. 
  Such a shallow lookup results in a linear growth of memory consumption for storing the embedding matrix and incurs high computational costs when working with real-world KGs. Drawing parallels with subword tokenization commonly used in NLP, we explore the landscape of more parameter-efficient node embedding strategies. To this end, we propose NodePiece, an anchor-based approach to learn a fixed-size entity vocabulary. In NodePiece, a vocabulary of subword/sub-entity units is constructed from anchor nodes in a graph with known relation types. Given such a fixed-size vocabulary, it is possible to bootstrap an encoding and embedding for any entity, including those unseen during training.
Experiments show that NodePiece performs competitively in node classification, link prediction, and  relation prediction tasks while retaining less than 10\% of explicit nodes in a graph as anchors and often having 10x fewer parameters. To this end, we show that a NodePiece-enabled model outperforms existing shallow models on a large OGB WikiKG 2 graph having \textasciitilde{}70x fewer parameters\footnote{The code is available on GitHub: \url{https://github.com/migalkin/NodePiece}}.
\end{abstract}
\section{Introduction}


Representation learning tasks on knowledge graphs (KGs) often require a parameterization of each unique \emph{atom} in the graph with a vector or matrix. 
Traditionally, in multi-relational KGs such \emph{atoms} constitute a set of all nodes  (entities) and relations (edge types) ~\citep{DBLP:journals/pieee/Nickel0TG16}. 
Assuming parameterization with vectors, \emph{atoms} are mapped to -dimensional vectors through shallow encoders  and  which scale linearly to the number of nodes and edge types\footnote{We then concentrate on nodes as usually their size is orders of magnitude larger than that of edge types.}, i.e., having  space complexity of the entity embedding matrix.
Albeit efficient on small conventional benchmarking datasets based on Freebase~\citep{toutanova-chen-2015-observed} (\textasciitilde{}15K nodes) and WordNet~\citep{DBLP:conf/aaai/DettmersMS018} (\textasciitilde{}40K nodes), training on larger graphs (e.g., YAGO 3-10~\citep{DBLP:conf/cidr/MahdisoltaniBS15} of 120K nodes) becomes computationally challenging. Scaling it further up to larger subsets~\citep{DBLP:conf/nips/HuFZDRLCL20,DBLP:journals/tacl/WangGZZLLT21,safavi-koutra-2020-codex} of Wikidata~\citep{DBLP:journals/cacm/VrandecicK14} requires a top-level GPU or a CPU cluster as done in, e.g., PyTorch-BigGraph~\citep{pbg} that maintains a 78M  200 embeddings matrix in memory (we list sizes of current best performing models in Table~\ref{tab:intro_params}).


Taking the perspective from NLP, shallow node encoding in KGs corresponds to shallow word embedding popularized with word2vec~\citep{DBLP:conf/nips/MikolovSCCD13} and GloVe~\citep{DBLP:conf/emnlp/PenningtonSM14} that learned a \emph{vocabulary} of 400K-2M most frequent words, treating rarer ones as \emph{out-of-vocabulary} (OOV). 
The OOV issue was resolved with the ability to build infinite combinations with a finite vocabulary enabled by \emph{subword units}. 
Subword-powered algorithms such as fastText~\citep{bojanowski-etal-2017-enriching}, Byte-Pair Encoding~\citep{sennrich-etal-2016-neural}, and WordPiece~\citep{DBLP:conf/icassp/SchusterN12} became a standard step in preprocessing pipelines of large language models and allowed to construct fixed-size token vocabularies, e.g., BERT~\citep{DBLP:conf/naacl/DevlinCLT19}  contains \textasciitilde{}30K tokens and GPT-2~\citep{radford2019language} employs \textasciitilde{}50K tokens.
Importantly, relatively small input embedding matrices enabled investing the parameters budget into more efficient encoders~\citep{kaplan2020scaling}. 

Drawing inspiration from subword embeddings in NLP, we explore how similar strategies for \emph{tokenizing} entities in large graphs can dramatically reduce parameter complexity, increase generalization, and naturally represent new unseen entities as using the same fixed vocabulary. 
To do so, tokenization has to rely on \emph{atoms} akin to subword units and not the total set of nodes.

To this end, we propose \emph{NodePiece}, an anchor-based approach to learn a fixed-size vocabulary  () of any connected multi-relational graph.
In NodePiece, the set of atoms consists of anchors and all relation types that, together, allow to construct a combinatorial number of sequences from a limited atoms vocabulary.
In contrast to shallow approaches, each node  is first tokenized into a unique  of  closest anchors and  immediate relations.  
A key element to build a node embedding is a proper encoder function  which can be designed leveraging inductive biases of an underlying graph or downstream tasks. 
Therefore, the overall parameter budget is now defined by a small fixed-size vocabulary of atoms and the complexity of the encoder function.   

Our experimental findings suggest that a fixed-size NodePiece vocabulary
paired with a simple encoder still yields competitive results on a variety of tasks including link prediction, node classification, and relation prediction. 
Furthermore, anchor-based hashing enables conventional embedding models to work in the \emph{inductive} and \emph{out-of-sample} scenarios when unseen entities arrive at inference time, which otherwise required tailored learning mechanisms.








\begin{table}[t]
\centering
\caption{Node embedding sizes of state-of-the-art KG embedding models compared to BERT Large. Parameters of type \emph{float32} take 4 bytes each. FB15k-237, WN18RR, and YAGO3-10 models as reported in \citet{DBLP:conf/iclr/SunDNT19}, OGB WikiKG2 as in \citet{zhang2019autosf}, Wikidata 5M as in \citet{DBLP:journals/tacl/WangGZZLLT21}, PBG Wikidata as in \citet{pbg}, and BERT Large as in \citet{DBLP:conf/naacl/DevlinCLT19}.}
\resizebox{\textwidth}{!}{
\begin{tabular}{@{}lcccccc|c@{}}
\toprule
 & FB15k-237 & WN18RR & YAGO3-10 & OGB WikiKG2 & Wikidata 5M & PBG Wikidata & BERT Large \\ \midrule
Vocabulary size & 15k & 40k & 120k & 2.5M & 5M & 78M & 30k \\
Embedding dim & 2000 & 1000 & 1000 & 200 & 512 & 200 & 1024 \\
GPU RAM, GB & 0.12 & 0.15 & 0.46 & 1.87 & 9.69 & 58.1 & 0.12 \\ \bottomrule
\end{tabular}}
\label{tab:intro_params}
\end{table}



\section{Related Work}

\textbf{Conventional KG embedding approaches.} 
To the best of our knowledge, all contemporary embedding algorithms~\citep{DBLP:journals/corr/abs-2002-00388, DBLP:journals/corr/abs-2006-13365} for link prediction on KGs employ shallow embedding lookups mapping each entity to a unique embedding vector thus being linear  to the total number of nodes  and size of an embedding matrix. 
This holds for different embedding families, e.g., translational~\citep{DBLP:conf/iclr/SunDNT19}, tensor factorization~\citep{DBLP:conf/icml/LacroixUO18}, convolutional~\citep{DBLP:conf/aaai/DettmersMS018}, and hyperbolic~\citep{DBLP:conf/acl/ChamiWJSRR20,DBLP:conf/nips/BalazevicAH19}. 
The same applies to relation-aware graph neural network (GNN) encoders~\citep{DBLP:conf/esws/SchlichtkrullKB18,Vashishth2020Composition-based} who still initialize each node with a learned embedding or feature vector before message passing.
Furthermore, shallow encoding is also used in higher-order KG structures such as hypergraphs~\citep{DBLP:conf/ijcai/FatemiTV020} and hyper-relational graphs~\citep{10.1145/3366423.3380257,galkin-etal-2020-message}.
NodePiece can be used as a drop-in replacement of the embedding lookup with any of those models.

\textbf{Distillation and compression.} 
Several recent techniques for reducing memory footprint of embedding matrices follow successful applications of distilling large language models in NLP~\citep{DBLP:journals/corr/abs-1910-01108}, i.e., 
 distillation~\citep{wang2020mulde,zhu2020distile} into low-dimensional counterparts, and compression of trained matrices into discrete codes~\citep{sachan-2020-knowledge}. 
However, all of them require a full embedding matrix as input which we aim to avoid designing NodePiece.

\textbf{Vocabulary reduction in recommender systems.} 
Commonly, recommender systems operate on thousands of categorical features combined in sparse high-dimensional vectors.Recent approaches~\citep{medini2021solar,liang2021anchor}  employ anchor-based hashing techniques to factorize sparse feature vectors into dense embeddings. 
Contrary to those setups, we do not expect availability of feature vectors for arbitrary KGs and rather learn vocabulary embeddings from scratch.

\textbf{Entity descriptions and language models.} 
A recent line of work such as KG-BERT~\citep{DBLP:journals/corr/abs-1909-03193}, MLMLM~\citep{DBLP:conf/acl/ClouatreTZC21}, BLP~\citep{DBLP:conf/www/DazaCG21} utilize entity descriptions passed through a language model (LM) encoder as entity embeddings suitable for link prediction.
We would like to emphasize that such approaches are rather orthogonal to NodePiece. Textual features are mostly available in Wikipedia-derived KGs like Wikidata but are often missing in domain-specific graphs like social networks and product graphs. We therefore assume textual features are not available and rather learn node representations based on their spatial characteristics. 
Still, textual features can be easily added by concatenating NodePiece-encoded features with LM-produced features.

\textbf{Out-of-sample representation learning.}
This task focuses on predictions involving previously unseen, or \emph{out-of-sample}, entities that attach to a known KG with a few edges. 
These new edges are then utilized as a context to compute its embedding. 
Previous work~\citep{wang2019logic, DBLP:conf/ijcai/HamaguchiOSM17, albooyeh-etal-2020-sample} proposed different neighborhood aggregation functions for this process or resorted to meta-learning~\citep{chen2019meta, baek2020learning, zhang2020few}.
However, all of them follow the shallow embedding paradigm.
Instead, NodePiece uses the new edges as a basis for anchor-based tokenization of new nodes in terms of an existing vocabulary. 



\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{img/NeurIPS_NodePiece_fig1_v4.pdf}
    \caption{NodePiece tokenization strategy. Given three anchors , a target node can be tokenized into a hash of top- closest anchors, their distances to the target node, and the relational context of outgoing relations from the target node. This hash sequence is passed through an injective encoder to obtain a unique embedding. Inverse relations are added to ensure connectivity.}
    \label{fig:app}
\end{figure}

\section{NodePiece Vocabulary Construction}

Given a directed KG  consisting of  nodes,  edges, and  relation types, our task is to reduce the original vocabulary size of  nodes to a smaller, fixed-size vocabulary of \emph{node pieces} akin to \emph{subword units}. 
In this work, we represent node pieces through \emph{anchor} nodes , a pre-selected set of nodes in a graph following a deterministic or stochastic strategy. 
A full NodePiece vocabulary is then constructed from anchor nodes and relation types, i.e, . 
Note that in order to maintain reachability of each node and balance in- and out-degrees we enrich  with inverse edges with inverse relation types, such that  and .
Using elements of the constructed vocabulary each node  can be \emph{tokenized} into  as a sequence of  closest anchors, discrete anchor distances, and a relational context of  immediate relations. 
Then, any encoder function  can be applied to embed the hash into a -dimensional vector.
An intuition of the approach is presented in Fig.~\ref{fig:app} with each step explained in more detail below. 


\subsection{Anchor Selection}
\label{sec:anchor_selection}

Subword tokenization algorithms such as BPE~\citep{sennrich-etal-2016-neural} employ deterministic strategies to create tokens and construct a vocabulary, e.g., based on frequencies of co-occurring n-grams, such that more frequent words are tokenized with fewer subword units. 
On graphs, such strategies might employ centrality measures like degree centrality or Personalized PageRank~\citep{ppr}. 
However, in our preliminary experiments, we found random anchor selection to be as effective as centrality-based strategies.
A choice for deterministic strategies might be justified when optimizing for certain task-specific topological characteristics, e.g., degree and PPR strategies indeed skew the distribution of shortest anchor distances towards smaller values thus increasing chances to find anchors in 2- or 3-hop neighborhood of any node (we provide more evidence for that in Appendix~\ref{app:anchors}).















\subsection{Node Tokenization}

Once the vocabulary  is constructed, each node  can be hashed (or \emph{tokenized}) into a  using 1)  nearest anchors and their discrete distances; 2)  immediate outgoing relations from the relational context of . 
Since anchor nodes are concrete nodes in , they get hashed in the same way as other non-anchor nodes.

\textbf{Anchors per node.} Given  anchor nodes, it is impractical to use all of them for encoding each node. Instead, we select  anchors per node and describe two possible strategies for that, i.e., \emph{random} and \emph{deterministic}. 
The basic random strategy uniformly samples an unordered set of  anchors from  yielding  possible combinations. 
To avoid collisions when hashing the nodes,  and  are to be chosen according to the lower bound on possible combinations that is defined by the total number of nodes, e.g., .
Note that running depth-first search (DFS) to random anchors at inference time is inefficient and, therefore,  of the random strategy has to be pre-computed.

On the other hand, the deterministic strategy selects an ordered sequence of  nearest anchors. Hence, the anchors can be obtained via breadth-first search (BFS) in the -hop neighborhood of  at inference time (or pre-computed for speed reasons).
However, the combinatorial bound is not applicable in this strategy and we need more discriminative signals to avoid hash collisions since nearby nodes will have similar anchors (we elaborate on the uniqueness issue in Appendix~\ref{app:hash_uniq}). 
Such signals have to better ground anchors to the underlying graph structure, and we accomplish that using \emph{anchor distances}\footnote{A full relational path can be mined as well but it has proven to be not scalable as each path needs to be encoded separately through a sequence encoder, e.g., GRU.} and \emph{relational context} described below.

A node residing in a disconnected component is assigned with an auxiliary \texttt{[DISCONNECTED]} token or can be turned into an anchor. 
However, the majority of existing KGs are graphs with one large connected component with very few disconnected nodes, such that this effect is negligible.

\textbf{Anchor Distances.}
Given a target node  and an anchor , we define anchor distance  as an integer denoting the shortest path distance between  and  in the original graph . 
Note that when tokenizing an anchor  with the deterministic strategy, the nearest anchor among top- is always  itself with distance .
We then map each integer to a learnable -dimensional vector  akin to \emph{relative distance encoding} scheme. 


\textbf{Relational Context.}
We also leverage the multi-relational nature of an underlying KG. 
Commonly\footnote{As of 2021, one of the largest open KGs Wikidata contains about 100M nodes and 6K edge types}, the amount of unique edge types in  is orders of magnitude smaller than the total number of nodes, i.e., . 
This fact allows to include the entire  in the NodePiece vocabulary  and further featurize each node with a unique relational context.
We construct a relational context of a node  by randomly sampling a set of  immediate unique outgoing relations starting from , i.e.,  where  denotes all outgoing relation types. 
Due to a non-uniform degree distribution, if , we add auxiliary \texttt{[PAD]} tokens to complete  to size . 





\subsection{Encoding}

At this step, a node  is tokenized into a sequence of  anchors, their  respective distances, and relational context of size :


Taking anchors vectors  and relation vectors  from the learnable NodePiece vocabulary , and anchor distances  from , we obtain a vectorized hash:


Although other operations are certainly possible, in this work, we use anchor distances as positional encodings of corresponding anchors and sum up their representations that helps to maintain the overall hash dimension of .

Finally, an encoder function  is applied to the vectorized hash to bootstrap an embedding of .
In our experiments, we probe two basic encoders: 1) MLP that takes as input a concatenated hash vector  projecting it down to ; 2) Transformer encoder~\citep{DBLP:conf/nips/VaswaniSPUJGKP17} with average pooling that takes as input an original sequence .
While MLP is faster and better scales to graphs with more edges,  Transformer is slower but requires less trainable parameters. 
As the two encoders were chosen to illustrate the general applicability of the whole approach, we leave a study of even more efficient and effective encoders for future work. 

While the nearest-neighbor hashing function has a greater number of collisions, its non-arbitrary mapping means that it is effectively permutation invariant. 
We show this in Proposition ~\ref{thm:main} through the framework of Janossy pooling and permutation sampling based SGD, -SGD  \citep{murphy2019janossy}. A proof is provided in Appendix~\ref{app:proofs}.

\begin{proposition}\label{thm:main}
The nearest-anchor encoder with  anchors and  subsampled relations, can be considered a -SGD approximation of -ary Janossy pooling with a canonical ordering induced by the anchor distances.
\end{proposition}

Janossy pooling with -SGD can be used to learn a permutation-invariant function from a broad class of permutation-sensitve functions such as MLPs \citep{murphy2019janossy}. The permutation-invariant nature of the nearest-neighbor encoding scheme combined with the lack of transductive features such as node-specific embeddings mean that NodePiece can be used for inductive learning tasks as well. 

With a fixed-size vocabulary , the overall complexity and parameter budget of downstream models are largely defined by the complexity of the encoder and its inductive biases. 
By design, the NodePiece \emph{smaller vocabulary - larger encoder} framework is similar to various Transformer-based language models~\citep{qiu2020pre} whose vocabulary size remains rather stable with the encoder being the most important part responsible for the final performance.






\section{Experiments}
\label{sec:experiments}

We design the experimental program not seeking to outperform the best existing approaches but to show the versatility of NodePiece on a variety of KG-related tasks: transductive, inductive, out-of-sample link prediction, and node classification (with relation prediction results in Appendix~\ref{app:rp}). 
With this desiderata, we formulate the following research questions: \textbf{RQ 1)} Is it necessary to map each node to a unique vector for an acceptable performance on KG tasks?; \textbf{RQ 2)} What is the effect of  hashing features?; \textbf{RQ 3)} Is there an optimal number of anchors per node, after which diminishing returns hit the performance?




\subsection{Transductive Link Prediction}
\textbf{Setup.} We run experiments on five KGs of different sizes (Appendix~\ref{app:datasets}) varying the total number of nodes from \textasciitilde{}15K to \textasciitilde{}2.5M. 
As a baseline, we compare to RotatE~\citep{DBLP:conf/iclr/SunDNT19} that remains one of state-of-the-art shallow embedding models for transductive link prediction tasks. 
To balance with NodePiece, RotatE operates on a graph with added inverse edges as well.
We report MRR with Hits@10 in the \emph{filtered}~\citep{DBLP:conf/nips/BordesUGWY13} setting as evaluation metrics, and count parameters for all models. 
On larger KGs, we also compare to a smaller RotatE with a similar parameter budget. 

In this task, NodePiece is equipped with a 2-layer MLP encoder.
For a fair comparison, we also adopt the RotatE scoring function as a link prediction decoder. 
As to the NodePiece configuration, we generally keep the number of anchors below 10\% of total nodes in respective graphs. 
We select 1k/20 for FB15k-237 (i.e., total 1000 anchors and 20 anchors per tokenized node) with 15 unique outgoing relations in the relational context; 500/50 with 4 relations for WN18RR; 7k/20 with 6 relations for CoDEx-L; 10k/20 with 5 relations for YAGO 3-10.
Other hyperparameters are listed in Appendix~\ref{app:hyperparams}.

\textbf{Discussion.}
Generally, the results suggest that a fixed-size NodePiece vocabulary of <10\% of nodes sustains 80-90\% of Hits@10 compared to 10x larger best shallow models.
Some performance loss is expected due to the compositional and compressive nature of entity tokenization.
On smaller graphs (Table~\ref{tab:tlp1}), parameter saving might not be well pronounced due to the overall small number of nodes to embed.
Still, taking even as few as 500 nodes as anchors on WN18RR retains 90\% of the best model performance. 
On bigger graphs (Table~\ref{tab:tlp2}), parameter efficiency is more pronounced, i.e., on YAGO 3-10, a RotatE model of comparable size is 20 Hits@10 points worse than a NodePiece-based one. 
This observation can be attributed to the fact the shrinking shallow models results in shrinking the embedding dimension of each node (20d for RotatE) which is inefficient on small parameter budgets. 
In contrast, a small fixed-size vocabulary allows for larger anchor embedding dimensions (100d for NodePiece with RotatE) since most of the parameter budget is defined by the encoder.

\begin{table}[t]
\centering
\caption{Transductive link prediction on smaller KGs.  results taken from~\citep{DBLP:conf/iclr/SunDNT19}.  denotes vocabulary size (anchors + relations), \#P is a total parameter count (millions). \% denotes the Hits@10 ratio based on the strongest model.}
\resizebox{\textwidth}{!}{
\begin{tabular}{@{}lcccccccccc@{}}
 & \multicolumn{5}{c}{FB15k-237} & \multicolumn{5}{c}{WN18RR} \\ \cmidrule(l){2-6} \cmidrule(l){7-11}  
 &  & \#P (M) & MRR & H@10 & \% &  & \#P (M) & MRR & H@10 & \% \\ \midrule
RotatE & 15k + 0.5k & 29 & 0.338 & 0.533 & 100 & 40k + 22 & 41 & 0.476 & 0.571 & 100 \\
NodePiece + RotatE & 1k + 0.5k & 3.2 & 0.256 & 0.420 & 79 & 500 + 22 & 4.4 & 0.403 & 0.515 & 90 \\ \midrule \midrule
\multicolumn{1}{r}{- no rel. context} & 1k + 0.5k & 2 & 0.258 & 0.425 & 80 & 500 + 22 & 4.2 & 0.266 & 0.465 & 81 \\
\multicolumn{1}{r}{- no distances} & 1k + 0.5k & 3.2 & 0.254 & 0.421 & 79 & 500 + 22 & 4.4 & 0.391 & 0.510 & 89 \\
\multicolumn{1}{r}{- no anchors, rels only} & 0 + 0.5k & 1.4 & 0.204 & 0.355 & 67 & 0 + 22 & 0.3 & 0.011 & 0.019 & 0.3 \\
\end{tabular}
}
\label{tab:tlp1}
\end{table}

\begin{table}[t]
\newcolumntype{C}{ @{}>{}@{} }
\centering
\caption{Transductive link prediction on bigger KGs. The same denotation as in Table~\ref{tab:tlp1}. Second RotatE has a similar parameter budget as a NodePiece-based model.}
\resizebox{\textwidth}{!}{
\begin{tabular}{@{}lcccccccccc@{}}
 & \multicolumn{5}{c}{CoDEx-L} & \multicolumn{5}{c}{YAGO 3-10} \\ \cmidrule(l){2-6} \cmidrule(l){7-11}  
 &  & \#P (M) & MRR & H@10 & \% &  & \#P (M) & MRR & H@10 & \% \\ \midrule
RotatE (500d) & 77k + 138 & 77 & 0.258 & 0.387  & 100 & 123k + 74 & 123 & 0.495 & 0.670  & 100 \\
RotatE (20d) & 77k + 138 & 3.8 & 0.196 & 0.322 & 83 & 123k + 74 & 4.8 & 0.121 & 0.262 & 39 \\
NodePiece + RotatE & 7k + 138 & 3.6 & 0.190 & 0.313 & 81 & 10k + 74 & 4.1 & 0.247 & 0.488 & 73 \\ \midrule \midrule
\multicolumn{1}{r}{- no rel. context} & 7k + 138 & 3.1 & 0.201 & 0.332 & 86 & 10k + 74 & 3.7 & 0.249 & 0.482 & 72 \\
\multicolumn{1}{r}{- no distances} & 7k + 138 & 3.6 & 0.179 & 0.302 & 78 & 10k + 74 & 4.1 & 0.250 & 0.491  & 73 \\
\multicolumn{1}{r}{- no anchors, rels only} & 0 + 138 & 0.6 & 0.063 & 0.121 & 31 & 0 + 74 & 0.5 & 0.025 & 0.041  & 6 \\
\end{tabular}
}
\label{tab:tlp2}
\end{table}



\begin{figure}[t]
    \centering
    \resizebox{0.65\textwidth}{!}{
    \begin{tikzpicture}[remember picture, scale=0.7]


\begin{axis}[
    title={WN18RR},
    xlabel={Anchors per node},
    ylabel={Hits @ 10},
    xmin=1, xmax=7,
    ymin=0, ymax=0.55,
    xtick={1,2,3,4,5,6,7},
    xticklabels={5,10,20,30,40,50,100},
legend pos=outer north east,
    legend cell align=left,
    ymajorgrids=true,
    xmajorgrids=true,
    grid style=dashed,
    cycle list name=color,
    width=8cm,height=7cm,
    mark size=2pt,
    every axis plot/.append style={line width=0.5mm}
]

\addplot coordinates {
    (1,0.0864)(2,0.1611)(2.5,0.2098)(3,0.2594)(3.5,0.265)
    };
    \addlegendentry{A=25}
\addplot coordinates {
    (1,0.1402)(2,0.2774)(3,0.4034)(4,0.4371)(5,0.4518)(6,0.4614)
    };
    \addlegendentry{A=50}
\addplot coordinates {
    (1,0.2298)(2,0.3993)(3,0.4595)(4,0.4639)(5,0.482)(6,0.4795)(7,0.4903)
    };
    \addlegendentry{A=100}
\addplot coordinates {
    (1,0.4231)(2,0.4682)(3,0.4923)(4,0.4988)(5,0.5027)(6,0.5063)(7,0.5082)
    };
    \addlegendentry{A=500}
\addplot coordinates {
    (1,0.455)(2,0.4783)(3,0.4926)(4,0.4997)
    };
    \addlegendentry{A=1000}
\legend{}
\end{axis}
\end{tikzpicture}
\hskip 5pt
\begin{tikzpicture}[remember picture, scale=0.7]


\begin{axis}[
    title={FB15k-237},
    xlabel={Anchors per node},
    ylabel={Hits @ 10},
    xmin=1, xmax=5,
    ymin=0.35, ymax=0.45,
xtick={1,2,3,4,5},
    xticklabels={5,10,20,30,40},
legend pos = outer north east,
    ymajorgrids=true,
    xmajorgrids=true,
    grid style=dashed,
    cycle list name=color,
    width=8cm,height=7cm,
    every axis plot/.append style={line width=0.5mm}
]

\addplot coordinates {
    (1,0.372)(2,0.396)(2.5,0.405)(3,0.407)(3.5,0.403)
    };
    \addlegendentry{A=25}
\addplot coordinates {
    (1,0.386)(2,0.389)(3,0.412)(4,0.415)(5,0.417)
    };
    \addlegendentry{A=50}
\addplot coordinates {
    (1,0.394)(2,0.403)(3,0.414)(4,0.417)(5,0.416)
    };
    \addlegendentry{A=100}
\addplot coordinates {
    (1,0.408)(2,0.413)(3,0.415)(4,0.416)(5,0.417)
    };
    \addlegendentry{A=500}
\addplot coordinates {
    (1,0.411)(2,0.418)(3,0.420)(4,0.421)(5,0.421)
    };
    \addlegendentry{A=1000}
\end{axis}
\end{tikzpicture}


%
     }
    \caption{Combinations of total anchors  and anchors per node. Denser FB15k-237 saturates faster on smaller  while sparse WN18RR saturates at around 500 anchors.}
    \label{fig:abl1}
\end{figure}

We further study the effect of different anchor selection combinations~ (Fig.~\ref{fig:abl1}). 
On WN18RR, fewer anchors with fewer anchors per node () yield relatively low accuracy but starting from  (\textasciitilde{}0.1\% of 40k nodes in the graph) the Hits@10 performance starts to saturate. 
On FB15k-237, as few as 25 anchors already exhibit the signs of saturation where a further increase to 500 or 1000 anchors only marginally improves the performance. 
We hypothesize such a difference can be explained by graph density, e.g., WN18RR is a sparse graph with a diameter of 23 and average anchor distance of 6 hops; while FB15k-237 is a denser graph with an average anchor distance of 2-3.  
Hence, on a sparse graph with longer distances, it takes more anchors to properly encode a node.

However, more precise predictions (e.g., Hits@1) reflected in the MRR metric (see Appendix~\ref{app:tr_lp}) still remain a challenging task for small vocabulary NodePiece setups, and bigger  combinations alleviate this issue.
We also observe that diminishing returns, which make further vocabulary increase less rewarding, start to appear from anchor set sizes of \textasciitilde{}1\% of total nodes.




\textbf{Ablations.}
In the ablation study, we measure the impact of relational context and anchor distances on link prediction (Table~\ref{tab:tlp1}).
Removing relational context and anchor distances does not tangibly affect the denser FB15k-237 data but does impair the accuracy on the sparser WN18RR. 
Pushing vocabulary sizes to the limit, we also investigate NodePiece behavior in the absence of anchors at all, i.e., when hashes are defined only by the relational context of size .
Interestingly, this still yields fair performance on FB15k-237 with just 7 points Hits@10 drop, but drops to zero the WN18RR performance. 
The fact that node embeddings might not be at all necessary but relations are more important supports the recent findings of \citet{DBLP:conf/icml/TeruDH20} that relies only on relations seen in a small subgraph around a target node.
However, at this point, it seems to be a virtue of graphs with a diverse set of unique relations. 
That is, FB15k-237 has 20x more unique relations than WN18RR and resulting hashes have more diverse combinations of relations which lead to more discriminative node representations. 
Additionally, we visualize anchor embedding projections in Appendix~\ref{app:umap}.

\subsubsection{OGB WikiKG 2}


\begin{wraptable}{r}{5cm}
\centering
\caption{Test MRR and parameter budget on OGB WikiKG 2.}
\resizebox{0.35\textwidth}{!}{
\begin{tabular}{ccc}
\toprule
    Model & \#Params & MRR \\ \midrule
    NP + AutoSF & 6.9M & 0.570  \\
    \multicolumn{1}{r}{ - rel. context} & 5.9M & 0.592  \\
    \multicolumn{1}{r}{ - anc. dists} & 6.9M & 0.570   \\
    \multicolumn{1}{r}{ - no anchors} & 1.3M & 0.476  \\ \midrule
    AutoSF & 500M & 0.546  \\ 
    PairRE & 500M & 0.521  \\ 
    RotatE & 1250M & 0.433  \\
    TransE & 1250M & 0.426  \\ \bottomrule
\end{tabular}}
\label{tab:ogb}
\end{wraptable}

To measure the benefits of NodePiece on large-scale KGs, we run a link prediction experiment on OGB WikiKG 2~\citep{DBLP:conf/nips/HuFZDRLCL20}, a subset of Wikidata that consists of 2.5M nodes and 16M edges. 
NodePiece is configured to sample a vocabulary 20K anchor nodes ( of total nodes) where each node is represented with  nearest anchors and a relational context of size , and we use a 2-layer MLP as a hash encoder (other hyperparameters as in Appendix~\ref{app:hyperparams}). 
Generally, such a NodePiece configuration can be paired with any link prediction decoder and we chose a non-parametric AutoSF~\citep{DBLP:conf/icde/ZhangYDC20} as one of the strongest  decoders on this graph. 
Overall, the NodePiece + AutoSF model has only 6.9M parameters, about  smaller than top shallow models.
Compared to the best reported shallow approaches, the NodePiece-enabled model exhibits (cf. Table~\ref{tab:ogb}, averaged over 10 seeds) even better performance achieved with a orders of magnitude smaller parameter budget.
We believe this result shows the effectiveness of NodePiece on large KGs with a dramatic parameter size reduction without significant performance losses. 
Ablations report the duality of a relational context, i.e., removing it from hashes leads to even higher MRR scores. On the other hand, the relational context alone with 0 learnable anchors still yields considerably better results than  larger shallow models RotatE and TransE. 

\subsection{Inductive Link Prediction}


We conduct a set of experiments on the inductive link prediction benchmark introduced by \citet{DBLP:conf/icml/TeruDH20} to measure the performance of NodePiece features in the extreme case when anchor nodes are not available and only relational context can be used to compose entity representations.

\textbf{Setup.} 
The unique feature of this benchmark compared to other evaluated tasks is that training and inference graphs are disjoint, i.e., inference at validation and test time is performed on a completely new graph comprised of new entities, and link prediction involves only entities unseen during training. 
As inference graphs are disconnected from training ones, learning anchors from the training graph is useless, so node hashes are built only using the -sized relational context. 
On top of the obtained NodePiece features we then employ a relational message passing GNN, CompGCN~\citep{Vashishth2020Composition-based}, with RotatE~\citep{DBLP:conf/iclr/SunDNT19} as a scoring function for triples. 
More details on the setup and best hyperparameters for NodePiece are presented in Appendix~\ref{app:ilp}.

\textbf{Baselines}. We compare NodePiece + CompGCN with two families of models applicable in the inductive setting, i.e., rule-based methods, Neural LP~\citep{DBLP:conf/nips/YangYC17}, DRUM~\citep{DBLP:conf/nips/SadeghianADW19}, RuleN~\citep{DBLP:conf/semweb/MeilickeFWRGS18}, and GNNs: GraIL~\citep{DBLP:conf/icml/TeruDH20} and recently proposed Neural Bellman-Ford Nets (NBFNet)~\citep{zhu2021neural}.

\textbf{Discussion.} 
Generally, the results confirm the trend identified previously: relation-only features are strong performers in dense relation-rich graphs. 
NodePiece features paired with CompGCN significantly improve over path-based methods where performance gap might reach 37 absolute Hits@10 points, e.g., in FB15k-237 V1 and NELL-995 V1. 
Comparing to GNNs, NodePiece + CompGCN outperforms GraIL by a large margin in all (except one) experiments and is competitive to NBFNet on relation-rich FB15k-237 splits.
As expected, NodePiece features are less efficient on sparse graphs (like WN18RR with few unique relations) but still outperform topology-based GraIL.

\begin{table}[t]
\centering
\caption{Inductive link prediction results, Hits@10. Best results are in \textbf{bold}, second best are \underline{underlined}.  results taken from \citet{DBLP:conf/icml/TeruDH20}. NBFNet results taken from \citet{zhu2021neural}. }
\label{tab:ilp_results}
\resizebox{\textwidth}{!}{
\begin{tabular}{@{}llcccccccccccc@{}}
\toprule
\multicolumn{1}{c}{\multirow{2}{*}{Class}} & \multicolumn{1}{l}{\multirow{2}{*}{Method}} & \multicolumn{4}{c}{FB15k-237} & \multicolumn{4}{c}{WN18RR} & \multicolumn{4}{c}{NELL-995}\\ \cmidrule(l){3-6} \cmidrule(l){7-10} \cmidrule(l){11-14}
 &  & V1 & V2  & V3 & V4 & V1  & V2 & V3 & V4 & V1 & V2 & V3 & V4 \\ \midrule
\multicolumn{1}{c}{\multirow{3}{*}{Path}} & Neural LP  & 0.529 & 0.589  & 0.529 & 0.559 & 0.744 & 0.689 & 0.462 & 0.671 & 0.408 & 0.787 & 0.827 & \underline{0.806}  \\
& DRUM  & 0.529 & 0.587 & 0.529 & 0.559 & 0.744 & 0.689 & 0.462 & 0.671 & 0.194 & 0.786 & 0.827 & \underline{0.806} \\
& RuleN  & 0.498 & 0.778 & 0.877 & 0.856 & 0.809 & 0.782 & 0.534 & 0.716 & 0.535 & 0.818 & 0.773 & 0.614 \\ \midrule
\multicolumn{1}{c}{\multirow{3}{*}{GNN}} & GraIL  & 0.642 & 0.818 & 0.828 & 0.893 & 0.825  & 0.787 & 0.584 & 0.734 & \underline{0.595} & \textbf{0.933} & \underline{0.914} & 0.732 \\
& NBFNet & \underline{0.834} & \textbf{0.949} & \textbf{0.951} & \textbf{0.960} & \textbf{0.948} & \textbf{0.905} & \textbf{0.893} & \textbf{0.890} & - & - & - & - \\
& NP + CompGCN & \textbf{0.873} & \textbf{0.939}  & \textbf{0.944} & \textbf{0.949} & \underline{0.830} & \textbf{0.886} & \underline{0.785} & \underline{0.807} & \textbf{0.890} & \underline{0.901} & \textbf{0.936} & \textbf{0.893} \\ 
\bottomrule
\end{tabular}
}
\end{table}

\subsection{Node Classification}
\textbf{Setup.} Due to the lack of established node classification datasets on multi-relational KGs, we design a multi-class multi-label task based on a triple version of a recent WD50K~\citep{galkin-etal-2020-message} extracted from Wikidata. 
The pre-processing steps are described in Appendix~\ref{app:wd50k}, and the final graph consists of 46K nodes and 222K edges.
The task belongs to the family of transductive (the whole graph is seen during training) semi-supervised (only a fraction of nodes are labeled) problems, where labels are 465 classes as seen in Wikidata. 
In a semi-supervised mode, we test the models on a graph with 5\% and 10\% of labeled nodes.
Node features have to be learned as node embeddings.

As baselines, we compare to a 2-layer MLP and CompGCN~\citep{Vashishth2020Composition-based} in a full-batch mode which is one of the strongest GNN encoders for multi-relational KGs. 
Both baselines learn a full entity vocabulary. 
We report ROC-AUC, PRC-AUC, and Hard Accuracy metrics as commonly done in standard graph benchmarks like OGB~\citep{DBLP:conf/nips/HuFZDRLCL20}. 
For PRC-AUC and Hard Accuracy, we binarize predicted logits using a threshold of 0.5. 
Hard Accuracy corresponds to the exact match of a predicted sparse 465-dimensional vector to a sparse 465-dimensional labels vector.

NodePiece is configured to have only 50 anchors and use 10 nearest anchors per node with 5 unique relations in the relational context. 
The dimensionality of anchors and relations is the same as in the baseline CompGCN.
Each epoch, we first materialize all entity embeddings through the NodePiece encoder and then send the materialized matrix to CompGCN with the class predictor.

\begin{table}[t]
\centering
\caption{Node classification results.  denotes vocabulary size (anchors + relations), \#P is a total parameter count (millions).}
\resizebox{\textwidth}{!}{
\begin{tabular}{@{}lcccccccc@{}}
 & & & \multicolumn{3}{c}{WD50K (5\% labeled)} & \multicolumn{3}{c}{WD50K (10\% labeled)} \\ \cmidrule(l){4-6} \cmidrule(l){7-9} 
 &  & \#P (M) & ROC-AUC & PRC-AUC & Hard Acc & ROC-AUC & PRC-AUC & Hard Acc \\ \midrule
MLP & 46k + 1k & 4.1 & 0.503 & 0.016 & 0.001  & 0.510 & 0.017 & 0.002 \\
CompGCN & 46k + 1k & 4.4 & 0.836 & 0.280 & 0.176 & 0.834 & 0.265 & 0.161 \\
NodePiece + GNN & 50 + 1k & 0.75 & 0.981 & 0.443 & 0.513  & 0.981 & 0.450 & 0.516 \\ \midrule \midrule
\multicolumn{1}{r}{- no rel. context} & 50 + 1k & 0.64 & 0.982 & 0.446 & 0.534 & 0.982 & 0.449 & 0.530 \\
\multicolumn{1}{r}{- no distances} & 50 + 1k & 0.74  & 0.981 & 0.448 & 0.516 & 0.981 & 0.448 & 0.513  \\
\multicolumn{1}{r}{- no anchors, rels only} & 0 + 1k & 0.54  & 0.984 & 0.453 & 0.532 & 0.984 & 0.456 & 0.533   \\
\end{tabular}
}
\label{tab:nc1}
\end{table}


\textbf{Discussion.} 
Surprisingly,  \textasciitilde{}1000x vocabulary reduction ratio (50 anchors against 46k for shallow models) greatly outperforms the baselines (Table~\ref{tab:nc1}). 
MLP, as expected, is not able to cope with the task producing random predictions. 
CompGCN, in turn, outperforms MLP demonstrating non-random outputs as seen by the ROC-AUC score of 0.836 and higher PRC-AUC and Hard Accuracy metrics. 
Still, a NodePiece-equipped CompGCN with 50 anchors reaches even higher ROC-AUC of 0.98 with considerable improvements along other metrics, i.e., +16-19 PRC-AUC points and 3x boost along the hardest accuracy metric.
We attribute such a noticeable performance difference to better generalization capabilities of the NodePiece model. 
That is, a generalization gap between training and validation metrics of the NodePiece + CompGCN is much smaller compared to the baselines who overfit rather heavily ( cf. the training curves in the Appendix~\ref{app:nc_curves}). 
The effect remains after increasing the number of labeled nodes to 10\%. 
Even with 50 anchors, the overall performance is saturated as the further increase of the vocabulary size did not bring any improvements.

\textbf{Ablations.} We probe setups where NodePiece hashes use only anchors or only relational context, and find they both deliver a similar performance. 
Following the previous experiments on dense graphs with lots of unique relations, it appears that node classification can be performed rather accurately based only on the node relational context which is captured by NodePiece hashes. 


\subsection{Out-of-sample Link Prediction}

\paragraph{Setup.}
In the out-of-sample setup, validation and test splits contain unseen entities that arrive with a few edges connected to the seen nodes.
For this experiment, we use the out-of-sample FB15k-237 split (oFB15k-237) as designed in~\citet{albooyeh-etal-2020-sample}.
We do not employ their version of WN18RR as the split contains too many disconnected entities and components in the train graph. 
Instead, using the authors script, we sample a much bigger out-of-sample version of YAGO 3-10.

As a baseline, we compare to oDistMult~\citep{albooyeh-etal-2020-sample} which aggregates embeddings of all seen neighboring nodes around the unseen one (akin to 1-layer message passing with mean aggregator).
We adopt the same evaluation protocol - given an unseen node with its connecting edges, we mask one of the edges and predict its tail or head using the rest of the edges, repeating this procedure for each edge. 
We report filtered MRR and Hits@10 as main metrics.

NodePiece enables traditional transductive-only models to perform inductive inference as both seen and unseen nodes are tokenized using the same vocabulary. 
For a smaller oFB15k-237 the NodePiece vocabulary has 1k/20 configuration with 15 relations, while in a bigger oYAGO 3-10 we use 10k/20 with 5 relations. 
For this task, we apply a transformer encoder instead of MLP. 
For a fair comparison, we use DistMult as a scoring function as well.

\begin{table}[t]
\centering
\caption{Out-of-sample link prediction.  results are taken from ~\citep{albooyeh-etal-2020-sample}.  denotes vocabulary size (anchors + relations), \#P is a total parameter count (millions).}
\resizebox{\textwidth}{!}{
\begin{tabular}{@{}lcccccccccc@{}}
 & \multicolumn{5}{c}{oFB15k-237} & \multicolumn{5}{c}{oYAGO 3-10 (117k)} \\ \cmidrule(l){2-6} \cmidrule(l){7-11}  
 &  & \#P (M) & MRR & H@10 & \% &  & \#P (M) & MRR & H@10 & \% \\ \midrule
oDistMult-ERAvg & 11k + 0.5k & 2.4 & 0.256 & 0.420 & 100 & 117k + 74 & 23.4 & OOM  & OOM & - \\
NodePiece + DistMult & 1k + 0.5k & 1 & 0.206 & 0.372 & 88 & 10k + 74 & 2.7 & 0.133 & 0.261 & 100 \\ \midrule \midrule
\multicolumn{1}{r}{- no rel. context} & 1k + 0.5k & 1 & 0.173 & 0.329 & 78 & 10k + 74 & 2.7 & 0.125 & 0.245 & 94 \\
\multicolumn{1}{r}{- no distances} & 1k + 0.5k & 1 & 0.208 & 0.372 & 88 & 10k + 74 & 2.7 & 0.133 & 0.260 & 99 \\
\multicolumn{1}{r}{- no anchors, rels only} & 0 + 0.5k & 0.8 & 0.069 & 0.127 & 30 & 0 + 74 & 0.7 & 0.015 & 0.017 & 6 \\
\end{tabular}
}
\label{tab:ooslp}
\end{table}

\textbf{Discussion.} 
The results in Table~\ref{tab:ooslp} show that a simple NodePiece-based model retains \textasciitilde{}90\% of the baseline performance on oFB15k-237, but achieved faster and computationally inexpensive compared to oDistMult. 
Moreover, while oDistMult is tailored specifically for the out-of-sample task, we did not do any task-specific modifications to the NodePiece-enabled model as it is inductive by design. 
Furthermore, oDistMult is not able to scale to a bigger oYAGO 3-10 on a 256 GB RAM machine due to the out of memory crash.
Conversely, a NodePiece-equipped model has the same computational requirements as in other tasks and converges rather quickly (40 epochs). 
Performed ablations underline the importance of having both anchors and relational context for tokenizing unseen entities.
We elaborate more on possible inference strategies for transductive and inductive tasks in Appendix~\ref{app:inference}.

\section{Conclusion}

In this paper, we have introduced NodePiece, a compositional approach for representing nodes in multi-relational graphs with a fixed-size vocabulary. 
Similar to subword units, NodePiece allows to tokenize every node as a combination of anchors and relations where the number of anchors can be 10--100 smaller than the total number of nodes. 
We show that in some tasks, node embeddings are not even necessary for getting an acceptable accuracy thanks to a rich set of relation types. 
Moreover, NodePiece is inductive by design and is able to tokenize unseen entities and perform downstream prediction tasks in the same fashion as on seen ones.

\textbf{Reproducibility Statement.}
The source code is openly available on GitHub. All hyperparameters and implementation details are presented in Appendix~\ref{app:hyperparams}. Information on the used datasets is presented in Table~\ref{tab:data} and we provide more details on dataset construction for node classification and out-of-sample link prediction tasks in Appendix~\ref{app:wd50k}.  The proof for Proposition 1 is given in the Appendix~\ref{app:proofs}.

\textbf{Ethics Statement.} 
As NodePiece is a general graph representation learning method, we do not foresee immediate ethical consequences pertaining to the method itself.

\textbf{Acknowledgements.} 
The authors would like to thank Koustuv Sinha, Gaurav Maheshwari, and Priyansh Trivedi for insightful and valuable discussions at earlier stages of this work. We also thank anonymous reviewers for the helpful comments.
This work is partially supported by the Canada CIFAR AI Chair Program and Samsung AI grant (held at Mila). We thank Mila and Compute Canada for access to computational resources.









\bibliography{bibliography}
\bibliographystyle{iclr2022_conference}

\clearpage
\appendix

\section{Implementation \& Hyperparameters}
\label{app:hyperparams}


\begin{table}[!ht]
\centering
\caption{Dataset statistics. LP - link prediction, RP - relation prediction, NC - node classification, OOS - out-of-sample. In OOS-LP, \emph{Nodes} also shows the amount of unseen nodes in validation/test.}
\resizebox{\textwidth}{!}{
\begin{tabular}{@{}lccccccc@{}}
\toprule
Dataset & \multicolumn{1}{c}{Task} & \multicolumn{1}{c}{Nodes} & \multicolumn{1}{c}{Relations} & \multicolumn{1}{c}{Edges} & \multicolumn{1}{c}{Train} & \multicolumn{1}{c}{Validation} & \multicolumn{1}{c}{Test} \\ \midrule
FB15k-237~\citep{toutanova-chen-2015-observed} & LP, RP & 14,505 & 237 & 310,079 & 272,115 & 17,526 & 20,438 \\
WN18RR~\citep{DBLP:conf/aaai/DettmersMS018} & LP, RP & 40,559 & 11 & 92,583 & 86,835 & 2824 & 2924  \\
CoDEx-Large~\citep{safavi-koutra-2020-codex} & LP & 77,951 & 69 & 612,437 & 551,193  & 30,622 & 30,622 \\
YAGO 3-10~\citep{DBLP:conf/cidr/MahdisoltaniBS15} & LP, RP & 123,143 & 37 & 1,089,000 & 1,079,040 & 4978 & 4982 \\
OGB WikiKG 2~\citep{DBLP:conf/nips/HuFZDRLCL20} & LP & 2,500,604 & 535 & 17,137,181 & 16,109,182 & 429,456 & 598,543 \\ \midrule
WD50K & NC & 46,164 & 526 & 222,563 & 4600 (N) & 4600 (N) & 4600 (N)  \\ \midrule
oFB15k-237~\citep{albooyeh-etal-2020-sample} & OOS-LP & 11k/1395/1395 & 234 & 292,173 & 193,490 & 44,601 & 54,082  \\
oYAGO 3-10 & OOS-LP & 117k/2960/2959 & 37 & 1,086,416 & 988,124 & 47,112 & 51,180 \\ \bottomrule
\end{tabular}
}
\label{tab:data}
\end{table}

\begin{table}[!ht]
    \centering
    \caption{Inductive relation prediction dataset statistics. Facts denote the size of the input graph while queries denote the triples to be predicted. Training sets contain all queries as facts. Note that in validation and test we receive a new graph disjoint from the training one, and queries are sent against this new inference graph (hence the number of entities and facts for validation and test is the same).}
    \label{tab:inductive_statistics}
    \resizebox{\textwidth}{!}{
        \begin{tabular}{llcccccccccc}
            \toprule
            \multirow{2}{*}{Dataset} & & \multirow{2}{*}{Relations} & \multicolumn{3}{c}{Train} & \multicolumn{3}{c}{Validation} & \multicolumn{3}{c}{Test} \\
            & & & Entity & Query & Facts & Entity & Query & Fact & Entity & Query & Facts \\
            \midrule
            \multirow{4}{*}{FB15k-237}
            & v1 & 183 & 2,000 & 4,245 & 4,245 & 1,500 & 206 & 1,993 & 1,500 & 205 & 1,993\\
            & v2 & 203 & 3,000 & 9,739 & 9,739 & 2,000 & 469 & 4,145 & 2,000 & 478 & 4,145 \\
            & v3 & 218 & 4,000 & 17,986 & 17,986 & 3,000 & 866 & 7,406 & 3,000 & 865 & 7,406 \\
            & v4 & 222 & 5,000 & 27,203 & 27,203 & 3,500 & 1,416 & 11,714 & 3,500 & 1,424 & 11,714 \\ \midrule
            \multirow{4}{*}{WN18RR}
            & v1 & 9 & 2,746 & 5,410 & 5,410 & 922 & 185 & 1,618 & 922 & 188 & 1,618 \\
            & v2 & 10 & 6,954 & 15,262 & 15,262 & 2,923 & 411 & 4,011 & 2,923 & 441 & 4,011\\
            & v3 & 11 & 12,078 & 25,901 & 25,901 & 5,084 & 538 & 6,327 & 5,084 & 605 & 6,327\\
            & v4 & 9 & 3,861 & 7,940 & 7,940 & 7,208 & 1,394 & 12,334 & 7,208 & 1,429 & 12,334 \\ \midrule
            \multirow{4}{*}{NELL-995}
            & v1 & 14 & 3,103 & 4,687 & 4,687 & 225 & 101 & 833 & 225 & 100 & 833 \\
            & v2 & 88 & 2,564 & 8,219 & 8,219 & 4,937 & 459 & 4,586 & 4,937 & 476 & 4,586\\
            & v3 & 142 & 4,647 & 16,393 & 16,393 & 4,921 & 811 & 8,048 & 4,921 & 809 & 8,048\\
            & v4 & 77 & 2,092 & 7,546 & 7,546 & 3,294 & 716 & 7,073 & 3,294 & 731 & 7,073 \\
            \bottomrule
        \end{tabular}
    }
\end{table}

NodePiece is implemented in Python using \emph{igraph} library (licensed under GNU GPL 2) for computing centrality measures and perform basic tokenization. 
Downstream tasks employ NodePiece in conjunction with PyTorch~\citep{DBLP:conf/nips/PaszkeGMLBCKLGA19} (BSD-style license), PyKEEN~\citep{ali2021pykeen} (MIT License), and PyTorch-Geometric~\citep{Fey/Lenssen/2019} (MIT License). 
We ran experiments on a machine with one RTX 8000 GPU and 64 GB RAM. The OGB WikiKG 2 experiments were executed on a single Tesla V100 16 GB VRAM and 64 GB RAM.
All used datasets are available under open licenses.

For all downstream tasks and datasets we employ the deterministic anchor selection strategy where 40\% of the total number of anchors  are nodes with top PPR scores, 40\% are top degree nodes, and remaining 20\% are selected randomly.
All anchor sets are non-overlapping and disjoint, i.e., if some top degree nodes have already been selected with the PPR policy, they will be skipped in favor of next nodes in the sorted list. 
The choice for this strategy is motivated in Appendix~\ref{app:anchors}.

\subsection{Datasets}
\label{app:datasets}

Details on the datasets for transductive link prediction, out-of-sample link prediction, relation prediction and node classification are collected in Table~\ref{tab:data}.
The inductive link prediction benchmark introduced by \citet{DBLP:conf/icml/TeruDH20} includes 3 graphs, FB15k-237, WN18RR, and NELL-995, each has 4 different splits that vary in the number of unique relations, number of nodes and triples at training and inference time. 
Full dataset statistics is provided in Table~\ref{tab:inductive_statistics}. 
FB15k-237 and most splits of NELL-995 can be considered as relation-rich graphs while WN18RR is a sparse graph with few relation types.

\subsection{Transductive Link Prediction}
The optimizer is Adam for all experiments. 
As RotatE is a scoring function in the complex space, the reported embedding dimensions are a sum of real and imaginary dimensions, e.g., 1000d means that both real and imaginary vectors are 500d. 

\begin{table}[!h]
\centering
\caption{NodePiece hyperparameters for transductive link prediction experiments}
\label{tab:hyperparams_np}
\begin{tabular}{@{}lccccc@{}}
\toprule
Parameter & FB15k-237 & WN18RR & CoDEx-L & YAGO 3-10 & OGB WikiKG 2 \\ \midrule
\# Anchors,  & 1000 & 500 & 7000 & 10000 & 20000 \\
\# Anchors per node,  & 20 & 50 & 20 & 20 & 20 \\
Relational context,  & 15 & 4 & 6 & 5 & 12 \\
Vocabulary dim,  & 200 & 200 & 200 & 200 & 200 \\
Batch size & 512 & 512 & 256 & 512 & 512  \\
Learning rate & 0.0005 & 0.0005 & 0.0005 & 0.00025 & 0.0001 \\
Epochs & 400 & 600 & 120 & 600 & 300k (steps) \\
Encoder type & MLP & MLP & MLP & MLP & MLP \\
Encoder dim & 400 & 400 & 400  & 400 & 400 \\
Encoder layers & 2 & 2 & 2 & 2 & 2 \\
Encoder dropout & 0.1 & 0.1 & 0.1 & 0.1 & 0.1 \\
Loss function & BCE & NSSAL & BCE & NSSAL & NSSAL \\
Margin & - & 15 & - & 50 & 50 \\
\# Negative samples & - & 20 & - & 10 & 128 \\
Label smoothing & 0.4 & - & 0.3 & - & - \\ \midrule
Training time, hours & 7 & 5.5 & 26 & 23 & 11 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[!h]
\centering
\caption{RotatE hyperparameters for transductive link prediction experiments. CoDEx-L and YAGO 3-10 also list the hyperparameters (after the symbol / ) for smaller models (reported in Table~\ref{tab:tlp2}) of the same parameter budget as NodePiece }
\label{tab:hyperparams_baseline}
\begin{tabular}{@{}lcccc@{}}
\toprule
Parameter & FB15k-237 & WN18RR & CoDEx-L & YAGO 3-10 \\ \midrule
Embedding dim,  & 2000 & 1000 & 1000 / 50 & 1000 / 40 \\
Batch size & 1024 & 512 & 512 / 512 & 1024 / 512 \\
Loss function & NSSAL & NSSAL & NSSAL & NSSAL \\
Margin & 9 & 6 & 25 / 9 & 24 / 15 \\
\# Negative samples & 256 & 1024 & 100 / 100  & 400 / 100 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Relation Prediction}

Configurations (Table~\ref{tab:hparams_relpred_np}) for the compared models are almost identical to those of the transductive link prediction experiment. 
We mostly reduce the number of epochs and negative samples as models converge faster on this task.

\begin{table}[!h]
\centering
\caption{Hyperparameters for relation prediction experiments. The content is largely identical to Table~\ref{tab:hyperparams_np}, only changed parameters are listed}
\label{tab:hparams_relpred_np}
\resizebox{\textwidth}{!}{
\begin{tabular}{@{}lcccccc@{}}
\toprule
& \multicolumn{3}{c}{NodePiece + RotatE} & \multicolumn{3}{c}{RotatE} \\ \cmidrule(l){2-4} \cmidrule(l){5-7}
Parameter & FB15k-237 & WN18RR  & YAGO 3-10 & FB15k-237 & WN18RR  & YAGO 3-10 \\ \midrule
Batch size & 512 & 512  & 512 & 512 & 512 & 512 \\
Epochs & 20 & 150 & 7 & 150 & 150 & 150 \\
Loss function & NSSAL & NSSAL & NSSAL & NSSAL & NSSAL  & NSSAL \\
Margin & 15 & 12 & 25 & 9 & 3 & 5 \\
\# Negative samples & 20 & 20  & 20 & 20 & 20 & 20 \\ \midrule
Training time, min & 25 & 30 & 25  & 28 & 10 & 57 \\
\bottomrule
\end{tabular}
}
\end{table}

\subsection{Node Classification}

In this experiment (Table~\ref{tab:hyperparams_nc}), NodePiece is used at the initial step to bootstrap a node embeddings matrix which is then sent to the CompGCN graph encoder. In contrast, CompGCN and MLP baselines use directly a trained node embedding matrix as their initial input.

\begin{table}[!h]
\centering
\caption{Hyperparameters for node classification experiments}
\label{tab:hyperparams_nc}
\begin{tabular}{@{}lccc@{}}
\toprule
Parameter & NodePiece + CompGCN & CompGCN & MLP \\ \midrule
\# Anchors,  & 50 & - & - \\
\# Anchors per node,  & 10 & - & -  \\
Relational context,  & 5 & - & -  \\
Vocabulary dim,  & 100 & 100 & 100  \\
Batch size & 512 & 512 & 512  \\
Learning rate & 0.001 & 0.001 & 0.001  \\
Epochs & 4000 & 4000 & 4000  \\
NodePiece encoder & MLP & - & -  \\
NodePiece encoder dim & 200 & - & -   \\
NodePiece encoder layers & 2 & - & -  \\
NodePiece encoder dropout & 0.1 & - & -  \\
GNN (MLP) layers & 3 & 3 & 3 \\
GNN (MLP) dropout & 0.5 & 0.5 & 0.5 \\
Loss function & BCE & BCE & BCE  \\
Label smoothing & 0.1 & 0.1 & 0.1  \\ \midrule
Training time, hours & 14 & 22 & 6  \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Out-of-sample Link Prediction}

The set of NodePiece hyperparameters (Table~\ref{tab:hparams_oos}) is similar to the set of the transductive experiments except the scoring function (DistMult), encoder function (Transformer), and number of epochs as the model converges faster. 
We do not provide a setup for the baseline oDistMult on oYAGO 3-10 as the model was not able to pre-process the dataset on a machine with 256 GB RAM.
Reported training times for NodePiece models exclude evaluation. 
Training times of the baseline oDistMult were not reported by its authors.

\begin{table}[!h]
\centering
\caption{Hyperparameters for out-of-sample link prediction experiments. The content is largely identical to Table~\ref{tab:hyperparams_np}, only changed parameters are listed}
\label{tab:hparams_oos}
\begin{tabular}{@{}lccc@{}}
\toprule
& \multicolumn{2}{c}{NodePiece + DistMult} & \multicolumn{1}{c}{oDistMult} \\ \cmidrule(l){2-3} \cmidrule(l){4-4}
Parameter & oFB15k-237  & oYAGO 3-10 & oFB15k-237 \\ \midrule
\# Anchors,  & 1000 & 10000 & -  \\
\# Anchors per node,  & 20 & 20 & -  \\
Relational context,  & 15 & 5 & -  \\
Vocabulary dim,  & 200 & 200 & 200  \\
Batch size & 256 & 256 & 1000  \\
Learning rate & 0.0005 & 0.0005 & 0.01  \\
Epochs & 40 & 40 & 1000  \\
NodePiece encoder & Transformer & Transformer & -  \\
NodePiece encoder dim & 512 & 512 & -   \\
NodePiece encoder layers & 2 & 2 & -  \\
NodePiece encoder dropout & 0.1 & 0.1 & -  \\
Loss function & Softplus & Softplus & Softplus  \\
\# Negative samples & 5 & 5 & 1 \\  \midrule
Training time, hours & 2 & 8 & -  \\
\bottomrule
\end{tabular}
\end{table}
\subsection{Deployment in Real-world Dynamic Knowledge Graphs}
\label{app:inference}
NodePiece, on account of its compositional representation, can be applied to dynamic real-world knowledge graphs where nodes are added and removed over time. That is, training on a graph snapshot we can obtain the embeddings of new nodes without re-computing and updating representations of every other node in the graph. This is valuable in settings where there are latency requirements such as many online services. For example, if a user creates an account on a social media service and begins liking content (represented as a ``like''  edge in the social network graph between the user and the content), it would be desirable to make future content recommendations rapidly reflect this new data without waiting for the next batched retraining to update the users embedding.  In order to reduce latency the entire embedding matrix can be materialized and cached ahead of time, updating embeddings as new nodes and edges are added. The parameter efficiency and compositionality of NodePeice means that for large real-world graphs NodePiece subsumes what would before have been a complex system of a large-scale embedding framework like Pytorch-BigGraph~\citep{pbg}, an OOV embedding method (e.g,. ERAvg) and a shallow embedding method (e.g., RotatE). 

\section{Limitations and Future Work}
\label{app:limits}
Our experimental results demonstrate the promise of using NodePeice to significantly reduce the parameter complexity of node embeddings. While it is difficult to prove, we  also hypothesize that the parameters required by NodePeice to maintain the same level of performance (as the graph scales) increase sublinearly according to the size of the graph. The intuition for this is twofold. First, the number of unique anchor combinations of size  that can be encoded increases according to  (i.e. ) if randomly sampled --- if the sampling is done via nearest neighbor anchor selection then the number of unique permutations is expected to increase polynomially. Second, increasing the size of the graph will only require sublinear increase in the number of anchors in order to maintain the same average node-anchor distance. Although proving causality is difficult, we believe that maintaining hashing uniqueness and node-anchor distances stable will be sufficient to maintain equivalent performance. 






\section{Anchor Selection Strategies}
\label{app:anchors}

Here, we provide more details as to anchor configurations ( nearest from total  anchors) and anchor distances. 
Recall that there exist several ways to select the total set of anchors  as stated in Section~\ref{sec:anchor_selection}, i.e., random or centrality-measure based.
Then,  anchors per node can be chosen either as  nearest (default NodePiece mode) or  random anchors.
Figure~\ref{fig:app_anchor_sel} depicts the effect of those strategies on the distribution of anchor distances (number of hops between a target node and its anchors). 
We use the configurations used in the main experiments, i.e.,  1000 anchors and 20 anchors per node for FB15k-237, and 500 anchors with 50 anchors per node on WN18RR.


\begin{figure}[t]
    \centering
    \subfloat[\centering WN18RR]{{\includegraphics[width=0.5\textwidth]{img/wn18rr_anchors_strategies_choose_50.pdf} }}\subfloat[\centering FB15k-237]{{\includegraphics[width=0.5\textwidth]{img/fb15k237_anchors_strategies_choose_20.pdf} }}\caption{Distribution of anchor distances under various anchor selection strategies. Top-bottom: mixed, degree-based, PPR-based, random. For each dataset, left: selecting  nearest anchors, right:  random anchors. (a) Selecting a fixed 500/50 configuration on WN18RR; (b) Selecting a fixed 1000/20 configuration on FB15k237. Generally, all strategies except random ones skew the distributions towards nearest anchors. }
    \label{fig:app_anchor_sel}
\end{figure}

First, we observe that PPR, degree, and mixed (40\% PPR, 40\% degree, 20\% random) strategies generally skew the distribution towards smaller anchor distances compared to random strategies. 
This fact supports the hypothesis that deterministic strategies improve the chances to find an anchor in a closer -hop neighborhood of a target node. 
Second, varying the way of selecting  anchors per node between nearest (left column) and random (right column), we also observe the skew of a distribution of anchor distances. 

\begin{figure}[t]\centering
    \subfloat[\centering WN18RR]{{\includegraphics[width=0.5\textwidth]{img/wn18rr_anchors_sampling_choose50_distr.pdf} }}\subfloat[\centering FB15k-237]{{\includegraphics[width=0.5\textwidth]{img/fb15k237_anchors_sampling_choose20_distr.pdf} }}\caption{Distribution of anchor distances under the fixed \emph{mix} anchor selection strategy when varying the total number of anchors  (50--1000 for WN18RR, 20--1000 for Fb15k-237). For each dataset, left column -  nearest anchors, right -  random anchors. On both graphs, increase in  with the nearest anchors always leads to shorter anchor distances. }\label{fig:app_anchor_sel2}\end{figure}


Next, we fix the anchor selection strategy to the \emph{mix}, fix the number of anchors per node (50 for WN18RR and 20 for FB15k-237), and vary a total number of anchors  (50 to 1000 for WN18RR and 20 to 1000 for FB15k-237) along with the method of sampling  anchors per node, i.e., nearest and random. 
Figure~\ref{fig:app_anchor_sel2} shows that increasing the total number of anchors together with  nearest anchors again skews the distribution of anchor distances towards smaller values and, hence, to higher probabilities of finding anchors in a closer neighborhood of a target node. 


We would recommend using centrality-based strategies to select  with  \emph{nearest} anchors per node if anchor distances and probability of finding anchors in a closer neighborhood are of higher importance. 

Finally, we fix the anchor selection strategy as \emph{mix}, obtain \emph{nearest} anchors per node, and under this setup study average anchor distances varying  - the number of anchors per node in various combinations of total anchors .
The results presented on Figure~\ref{fig:app_distances} suggest that sparser graphs (like WN18RR) benefit more from increasing the number of anchors , i.e., the delta between distances is much larger than that of dense FB15k-237.
The difference in distances on Figure~\ref{fig:app_distances} might also explain the performance on Figure~\ref{fig:app_lp_mrr}, i.e., generally, smaller  configurations like 25/5 are inferior on sparser graphs but perform competitively on denser ones.

\begin{figure}[!h]\centering
    \subfloat[\centering WN18RR]{{\includegraphics[width=0.5\textwidth]{img/wn18rr_study_distances.pdf} }}\subfloat[\centering FB15k-237]{{\includegraphics[width=0.5\textwidth]{img/fb15k237_study_distances.pdf} }}\caption{Average node-anchor distances when varying the total number of anchors  from 25 to 1000 and  nearest anchors per node from 5 to 50. Note that on a sparser WN18RR the gap between min and max values is much wider than of denser FB15k-237. Signs of saturation suggest that further increasing  is not beneficial. }\label{fig:app_distances}\end{figure}

\section{Embedding Visualizations}
\label{app:umap}

To further study learned representations of anchors and capabilities of the encoder, we build tSNE and UMAP projections from subsamples of FB15k-237 and WN18RR based on trained models from the transductive link prediction experiments (hyperparameters listed in Table~\ref{app:hyperparams}). 

For FB15k-237, we randomly sample 1000 entities (out of total 15K) and find their top-100 most common anchors. The anchor embeddings are extracted from the learned tensor while 1000 entity embeddings are obtained through the NodePiece encoder. 
Similarly for WN18RR, we sample 4000 entities (out of total 40K) keeping their top-100 most common anchors. As we use the RotatE decoder that assumes entities and anchors are modeled in a complex space, we visualize their real parts (e.g., first 100 dimensions out of 200).

\begin{figure}[!htb]
    \centering
    \includegraphics[width=\textwidth]{img/fb_top_100_anchors_cand1.png}
    \caption{tSNE (left) and UMAP (right) projections of 1000 encoded entities sampled randomly from FB15k-237 and their top 100 most common anchors.}
    \label{fig:fb_top_anchors}
\end{figure}

Recalling that link prediction performance of NodePiece + RotatE retains 80-90\% of the state of the art models performance, the results on Fig.~\ref{fig:fb_top_anchors} and Fig.~\ref{fig:wn_top_anchors} demonstrate that (1) NodePiece encoder is able to reconstruct clusters of similar entities; (2) anchors are well-scattered among communities. 
Albeit entity embeddings are built as a composition of  anchors, it can be seen that all communities have "specialized" nearby anchors. On a higher level, common anchors tend to be well-scattered in the space.
Less common anchors, as seen on FB15k-237 and Fig.~\ref{fig:fb_top_anchors}, tend to group together. However, thanks to the non-linear nature of the NodePiece encoder, resulting entity embeddings still form different clusters and communities not concentrated around one point.
We believe this is the effect of a compositional encoder and plan to investigate this phenomenon further.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=\textwidth]{img/wn_top_100_anchors.png}
    \caption{tSNE (left) and UMAP (right) projections of 4000 encoded entities sampled randomly from WN18RR and their top 100 most common anchors.}
    \label{fig:wn_top_anchors}
\end{figure}



\section{Transductive Link Prediction Results: MRR}
\label{app:tr_lp}

In addition to Figure ~\ref{fig:abl1} that presents Hits@10, we report variations of mean reciprocal rank (MRR) depending on combinations of  and  on Figure~\ref{fig:app_lp_mrr} from the same set of experiments. 
On sparser WN18RR, smaller  combinations like 25/5 or 50/10 struggle with more precise predictions like Hits@1 which is captured by low values of MRR. 
Starting from 500/10, the WN18RR performance starts to saturate. On the other hand, on denser FB15k-237, the difference between minimum and maximum MRR is less than 4 points, and performance exhibits signs of saturation already at 50/10.

\begin{figure}[h]
    \centering
    \resizebox{\textwidth}{!}{
    



\begin{tikzpicture}[remember picture, scale=0.8]
\begin{axis}[
    title={WN18RR},
    xlabel={Anchors per node},
    ylabel={MRR},
    xmin=1, xmax=7,
    ymin=0, ymax=0.45,
xtick={1,2,3,4,5,6,7},
    xticklabels={5,10,20,30,40,50,100},
legend pos = outer north east,
    ymajorgrids=true,
    xmajorgrids=true,
    grid style=dashed,
    cycle list name=color,
    width=8cm,height=7cm,
]

\addplot coordinates {
    (1,0.043)(2,0.0711)(2.5,0.0912)(3,0.1108)(3.5,0.1205)
    };
    \addlegendentry{A=25}
\addplot coordinates {
    (1,0.0601)(2,0.1194)(3,0.2152)(4,0.264)(5,0.2986)(6,0.3065)
    };
    \addlegendentry{A=50}
\addplot coordinates {
    (1,0.0933)(2,0.198)(3,0.296)(4,0.3292)(5,0.3582)(6,0.361)(7,0.3666)
    };
    \addlegendentry{A=100}
\addplot coordinates {
    (1,0.2243)(2,0.3225)(3,0.3642)(4,0.3733)(5,0.3885)(6,0.3954)(7,0.397)
    };
    \addlegendentry{A=500}
\addplot coordinates {
    (1,0.2653)(2,0.3405)(3,0.3743)(4,0.3879)
    };
    \addlegendentry{A=1000}
    \legend{}
\end{axis}
\end{tikzpicture}
\hskip 5pt
\begin{tikzpicture}[remember picture, scale=0.8]


\begin{axis}[
    title={FB15k-237},
    xlabel={Anchors per node},
    ylabel={MRR},
    xmin=1, xmax=5,
    ymin=0.2, ymax=0.26,
xtick={1,2,3,4,5},
    xticklabels={5,10,20,30,40},
legend pos = outer north east,
    ymajorgrids=true,
    xmajorgrids=true,
    grid style=dashed,
    cycle list name=color,
    width=8cm,height=7cm,
]

\addplot coordinates {
    (1,0.223)(2,0.236)(2.5,0.243)(3,0.245)(3.5,0.243)
    };
    \addlegendentry{A=25}
\addplot coordinates {
    (1,0.231)(2,0.238)(3,0.245)(4,0.249)(5,0.25)
    };
    \addlegendentry{A=50}
\addplot coordinates {
    (1,0.235)(2,0.241)(3,0.249)(4,0.245)(5,0.247)
    };
    \addlegendentry{A=100}
\addplot coordinates {
    (1,0.247)(2,0.248)(3,0.250)(4,0.247)(5,0.252)
    };
    \addlegendentry{A=500}
\addplot coordinates {
    (1,0.250)(2,0.254)(3,0.256)(4,0.254)(5,0.255)
    };
    \addlegendentry{A=1000}
\end{axis}
\end{tikzpicture}     }
    \caption{Combinations of total anchors  and anchors per node. Denser FB15k-237 saturates faster on smaller  while sparse WN18RR saturates at around 500 anchors. MRR metric captures all ranks. Note that performance gap on FB15k-237 is very small indicating that  saturation has occurred already with small anchor configurations.}
    \label{fig:app_lp_mrr}
\end{figure}

\section{Datasets Construction}
\label{app:wd50k}

\subsection{Node Classification: WD50K NC }
The original WD50K~\citep{galkin-etal-2020-message} contains a triple-only KG version on which we base a new dataset for semi-supervised multi-class multi-label node classification.
First, we remove all triples containing Wikidata properties \texttt{P31} (\emph{instance of}) and \texttt{P279} (\emph{subclass of}) as they already contain class information. 
We then remove nodes that became disconnected after removing those edges.
Third, using SPARQL queries, for each remaining node in a graph, we extract a 3-hop class hierarchy of Wikidata classes and their superclasses.
We only keep class labels that occur at least 50 times in the training set.
Then, we sample 10\% of nodes with labels for validation and 10\% for test, and of remaining 80\% we sample a set of nodes for the semi-supervised setup, i.e., we keep only 5\% and 10\% of those nodes.
The resulting graph has 46k nodes, 526 distinct relation types, and 465 class labels.

\subsection{Ouf-of-sample Link Prediction: oYAGO 3-10}

For sampling the out-of-sample version of a bigger YAGO 3-10 we largely follow the same original procedure described in Section 4 of \citep{albooyeh-etal-2020-sample}.
We first merge the train, validation and test triples from the original dataset for transductive link prediction. Then, from all entities appearing in at least two triples, we randomly sample 5\% of nodes to be the out-of-sample entities for validation and 5\% for test. 
All triples containing the out-of-sample entities on subject or object positions are put into validation or test, respectively, as edges that connect an unseen entity with the seen graph.




\section{Node Classification: Training Curves}
\label{app:nc_curves}

Figure~\ref{fig:app_generalization} depicts train and validation values of Hard Accuracy and PRC-AUC metrics for all the compared models on WD50K NC with 5\% of labeled nodes. 
The NodePiece model has only 50 total anchors with 10 nearest anchors per node, and 5 unique relation types in the relational context.
The performance on the dataset with 10\% of nodes is almost the same, so we report the charts only on 5\% dataset.
By the generalization gap we understand the delta between training and validation values.

The MLP baseline quickly overfits but fails to generalize on the validation. 
The generalization gap of CompGCN is smaller compared to MLP but is still significant, i.e., validation performance is 2--3 smaller than train.
Finally, the NodePiece-enabled model has the smallest generalization gaps, especially along the Hard Accuracy metric where the validation performance is very close to that of train. 
Similarly, the gap on PRC-AUC is smaller than 10 points.

As shown in the ablation study in Table~\ref{tab:nc1}, it appears that explicit node embeddings do not contribute to the classification performance. 
Hence, the baseline models tend to be overparameterized where learnable node embeddings add noise, while the NodePiece model has only a few anchors (or no anchors at all when using only the relational context), much fewer parameters, and therefore generalizes better. 
This hypothesis also explains the observation that the node classification performance does not improve when increasing  anchor configurations.

\begin{figure}[!t]
    \centering
    \includegraphics[width=\textwidth]{img/hard_and_auc.pdf}
    \caption{Generalization gap on WD50K (5\% labeled nodes). NodePiece-based model has observably smaller generalization gaps compared to the baselines. }
    \label{fig:app_generalization}
\end{figure}


\section{Proofs}
\label{app:proofs}

\begin{proposition}\label{thm:app_main}
The nearest-anchor encoder with  anchors and  subsampled relations, can be considered a -SGD approximation of -ary Janossy pooling with a canonical ordering induced by the anchor distances.
\end{proposition}
\xhdr{Proof} 

We begin by providing the definition of Janossy pooling as it was presented in the original paper \cite{murphy2019janossy}.

\begin{definition}[\Janossy pooling]
\label{def:AnySizeDf}
Let  be the union of all anchors and relations. Consider a function  on variable-length but finite sequences , parameterized by , .
A permutation-invariant function  is the \Janossy function associated with  if

where  is the set of all permutations of the integers  to , and  represents a particular reordering of the elements of sequence  according to .
We refer the operation used to construct  from  as \Janossy pooling.
\end{definition}

While Janossy pooling provides a simple approach to construct permutation-invariant functions from arbitrary permutation sensitive functions, it is computationally intractable due to the need to sum over all computations. Three general strategies proposed under this framework to overcome this combinatorial challenge: canonical orderings, -ary Janossy pooling, and -SGD approximations.  

A very effective way of reducing the complexity is to constrain the permutations to a canonical ordering that is independent of a specific adjacency matrix ordering over a given graph.  More precisely, one defines as a function  such that  and only considers functions  based on the composition  \citep{murphy2019janossy}. In the case of NodePiece we are able to define this ordering for the anchors according to their distance to the target node. Assuming that the number of relations is fixed or grows at slow rate throughout the life-cycle of a graph we can define an arbitrary ordering for relations as a canonical ordering for the relational context. However, since anchors can be equidistant such a canonical ordering does fully satisfy permutation invariance. We propose a trivial relaxation of the original definition of canonical orderings simply requiring that an ordering greatly reduce the number of unique permutations since in practice an exact canonical ordering is rarely feasible. Specifically, .



To further reduce the number of permutations we can truncate our ordered sequence . This is known as -ary Janossy pooling pooling (Definition \ref{def:GeneralPool}) and is implicitly performed by the NodePeice algorithm by varying the anchor per node parameter, , and the size of the relational context, .

\begin{definition}[-ary Janossy pooling] \label{def:GeneralPool}Fix .  
For any sequence , define  as its projection to a length 
 sequence; in particular, if , we keep the first  elements. 
Then, a -ary permutation-invariant Janossy function  is given by 

\end{definition}



Since an imperfect truncated canonical ordering may still result in a potentially intractable number of permutations, we use permutation sampling also known as -SGD to learn arbitrary functions that approximate -ary Janossy pooling. This is done by randomly ordering anchors that are equidistant resulting in a uniform sampling of possible permutations during training and evaluation. For more details on the formal definition of -SGD we point the reader to the original paper~\citep{murphy2019janossy}.

\section{Relation Prediction}
\label{app:rp}

\textbf{Setup.} 
We conduct the relation prediction experiment on the same FB15k-237, WN18RR, and YAGO 3-10 datasets. 
While link prediction deals with entities, the relation prediction model has to rank a correct relation given a \emph{(head, ?, tail)} query. 
We report MRR and Hits@10 in the filtered setting as evaluation metrics.
Similar to the link prediction configuration, we use NodePiece + 2-layer MLP and compare against RotatE of the same total parameter count. 

\textbf{Discussion.} 
The reported results (Table~\ref{tab:rp1}) demonstrate a competitive performance of NodePiece-based models with reduced vocabulary sizes bringing more than 97\% Hits@10 across graphs of different sizes. 
In the case of WN18RR and YAGO 3-10, NodePiece models with fewer anchors even slightly improve the accuracy upon the shallow embedding baseline.
The ablation study suggests that on dense graphs with a reasonable amount of unique relations having explicit learnable node embeddings might not be needed at all for this task.
That is, we see that on FB15k-237 and YAGO 3-10 the NodePiece hashes comprised only of the relational context deliver the same performance without any performance drop confirming the findings from the previous experiment. 




\begin{table}[t]
\centering
\caption{Relation prediction results.  denotes vocabulary size (anchors + relations).}
\resizebox{\textwidth}{!}{
\begin{tabular}{@{}lccccccccc@{}}
 & \multicolumn{3}{c}{FB15k-237} & \multicolumn{3}{c}{WN18RR} & \multicolumn{3}{c}{YAGO 3-10} \\ \cmidrule(l){2-4} \cmidrule(l){5-7}  \cmidrule(l){8-10} 
&  & MRR & H@10  &  & MRR & H@10 &  & MRR & H@10 \\ \midrule
RotatE & 15k + 0.5k & 0.905  & 0.979 & 40k + 22 &  0.774 & 0.897 & 123k + 74 & 0.909 & 0.992 \\
NodePiece + RotatE  & 1k + 0.5k & 0.874 & 0.971 & 500 + 22 &  0.761 & 0.985 & 10k + 74 & 0.951 & 0.997  \\ \midrule \midrule
\multicolumn{1}{r}{- no rel. context} & 1k + 0.5k & 0.876 & 0.968 & 500 + 22 & 0.541 & 0.958  & 10k + 74 & 0.898 & 0.993 \\
\multicolumn{1}{r}{- no distances} & 1k + 0.5k  & 0.877 & 0.970 & 500 + 22 & 0.746 & 0.975 & 10k + 74 & 0.943 & 0.997 \\
\multicolumn{1}{r}{- no anchors, rels only} & 0 + 0.5k & 0.873 & 0.971 & 0 + 22 & 0.545 & 0.947 & 0 + 74 & 0.951 & 0.998  \\
\end{tabular}
}
\label{tab:rp1}
\end{table}



\section{Inductive Link Prediction}
\label{app:ilp}

\textbf{Setup}. Nodes in the inference graphs do not have any associated feature vectors which makes this benchmark very relevant for graph representation learning.
Importantly, the set of relation types in the inference graphs is a subset of those seen in the training set. 
Since the relation embedding matrix can be learned on the training graph, we therefore have a uniform method for constructing node representations both on seen and unseen graphs.
As an encoder we try MLP and Transformer. 


\textbf{Evaluation Protocol}. 
Following the original work~\citep{DBLP:conf/icml/TeruDH20}, we employ a filtered setting and rank each triple against 50 random negative triples reporting the Hits@10 metric.  
This setup is motivated by computational complexity of GraIL at inference time, while NodePiece + CompGCN is as fast in the inductive inference as in the transductive regime reported in other experiments.

\textbf{Discussion}. 
For each KG, there are 4 splits of increasing size of train and inference nodes and edges.
The empirical results on this spectrum of various sizes demonstrates interesting scalability properties of NodePiece in inductive settings. Without anchor nodes, the NodePiece vocabulary size is independent of the number of nodes and edges, depending only on the number of relation types. On dense relation-rich graphs such a vocabulary is enough to yield very competitive performance.

\begin{table}[t]
\centering
\caption{Hyperparameters for inductive link prediction experiments. Entries are shared among 4 splits of each graph if not particularly specified. V1 | V2 | V3 | V4 otherwise.}
\label{tab:hyperparams_ilp}
\resizebox{\textwidth}{!}{
\begin{tabular}{@{}lccc@{}}
\toprule
Parameter & FB15k-237 & WN18RR & NELL-995 \\ \midrule
\# Anchors,  & - & - & - \\
\# Anchors per node,  & - & - & -  \\
Relational context,  & 12 & 4 | 4 | 4 | 3 & 4 | 6 | 4 | 6 \\
Vocabulary dim,  & 100 & 100 & 100  \\
Batch size & 512 & 512 & 512  \\
Learning rate & 0.0001 & 0.0001 & 0.0001  \\
Num negatives & 32 & 32 & 32  \\
Epochs & 2500 V1 | 2000 rest & 590 | 2000 | 210 | 2000 & 2000  \\
NodePiece encoder & MLP & MLP & MLP | MLP | MLP | Trf \\
NodePiece encoder dim & 200 & 200 & 200  \\
NodePiece encoder layers & 2 & 2 & 2  \\
NodePiece encoder dropout & 0.1 & 0.1 & 0.1  \\
CompGCN layers & 3 & 3 | 6 | 6 | 10 & 3 | 4 | 3 | 3 \\
CompGCN attention & yes & yes & yes \\
CompGCN dropout & 0.1 & 0.1 & 0.2 | 0.1 | 0.1 | 0.1 \\
Loss function & NSSAL & NSSAL & NSSAL  \\
Margin & 25 | 15 | 15 | 25 & 15 | 15 | 5 | 20 & 15 | 20 | 30 | 20 \\ \midrule
Training time, hours & 2 | 4 | 16 | 19 & 1 | 18 | 4 | 10 & 6 | 5 | 8 | 8  \\
\bottomrule
\end{tabular}
}
\end{table}

\section{Uniqueness of Node Hashes}
\label{app:hash_uniq}

NodePiece represents nodes as a sequence of tokens and a natural question in this context is how unique such sequences can be in light of different anchor selection and tokenization strategies. 

Assuming the input graph is a single connected component, when sampling  total anchors and selecting \emph{randomly}  anchors per node, the number of possible hash combinations is bounded by . In this scenario, uniqueness of hashes is achieved by having this number bigger than the number  of nodes in a graph, , and this happens with high probability for any reasonably large , eg,  encodes about  combinations which covers all existing public KGs combined.

In the deterministic selection of nearest anchors per node we do not have such guarantees. Nevertheless, additional sequences of relational contexts and anchor distances help to obtain more unique hashes. Collisions are possible in highly regular graphs like Wordnet, but real-world KGs like Wikidata and DBpedia do not exhibit such a regular structure. Similar to homonyms whose meaning depends on the surrounding context in a sentence, we hypothesize that adding message passing layers (that can be seen as encoding of a neighboring context for a given node) on top of NodePiece hashes might further improve the diversity of node representations. 
That said, the future work research agenda might include proving tighter theoretical bounds on hashes uniqueness and developing new anchor sampling and tokenization strategies.

\end{document}
