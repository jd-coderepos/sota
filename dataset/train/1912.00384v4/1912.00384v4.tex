\documentclass[review]{elsarticle}

\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\interdisplaylinepenalty=2500
\usepackage{algorithmic}
\usepackage{array}
\usepackage[caption=false,font=footnotesize,labelfont=sf,textfont=sf]{subfig}
\usepackage{fixltx2e}

\usepackage{comment}
\usepackage{epsfig}
\usepackage{changepage}
\usepackage{float}
\usepackage{booktabs}
\usepackage{xspace}

\usepackage{soul,xcolor}

\newcommand{\ms}[1]{\textcolor{red}{#1}}
\newcommand{\zh}[1]{\textcolor{blue}{#1}}


\usepackage{hyperref}


\journal{Pattern Recognition}

\bibliographystyle{elsarticle-num}

\begin{document}
\begin{frontmatter}
\title{Training Object Detectors from Few Weakly-Labeled and Many Unlabeled Images}


\author{Zhaohui Yang\fnref{pku}}
\author{Miaojing Shi\corref{corauthor}\fnref{miaojingshi}}
\author{Chao Xu\fnref{pku}}
\author{Vittorio Ferrari\fnref{vittorioferrari}}
\author{Yannis Avrithis\fnref{yannisavrithis}}



\cortext[corauthor]{Corresponding author. }
\fntext[pku]{Zhaohui Yang and Chao Xu are with Key Lab of Machine Perception, Dept. of Machine Intelligence, Peking University. Email: zhaohuiyang@pku.edu.cn, xuchao@cis.pku.edu.cn}
\fntext[miaojingshi]{Miaojing Shi is with King's College London. Email: miaojing.shi@kcl.ac.uk}
\fntext[vittorioferrari]{Vittorio Ferrari is with Google Research. Email: vittoferrari@gmail.com}
\fntext[yannisavrithis]{Yannis Avrithis is with Inria, Univ Rennes, CNRS, IRISA. Email: yannis@avrithis.net}















\begin{abstract}


Weakly-supervised object detection attempts to limit the amount of supervision by dispensing the need for bounding boxes, but still assumes image-level labels on the entire training set.
In this work, we study the problem of training an object detector from one or few images with image-level labels and a larger set of completely unlabeled images. This is an extreme case of semi-supervised learning where the labeled data are not enough to bootstrap the learning of a detector.
Our solution is to train a weakly-supervised student detector model
from image-level pseudo-labels generated on the unlabeled set by a teacher classifier model, bootstrapped by region-level similarities to
labeled images. Building upon the recent representative weakly-supervised  pipeline PCL~\cite{tang2018pami},
our method can use
more unlabeled images to achieve
performance competitive or superior to many recent weakly-supervised detection solutions.
Code will be made available at \url{https://github.com/zhaohui-yang/NSOD}.
\end{abstract}

\begin{keyword}
Object detection\sep weakly-supervised learning\sep semi-supervised learning\sep unlabelled set
\end{keyword}

\end{frontmatter}




\newcommand{\head}[1]{{\smallskip\noindent\bf #1}}
\newcommand{\alert}[1]{{\color{red}{#1}}}
\newcommand{\eq}[1]{(\ref{eq:#1})\xspace}

\newcommand{\red}[1]{{\color{red}{#1}}}
\newcommand{\blue}[1]{{\color{blue}{#1}}}
\newcommand{\green}[1]{{\color{green}{#1}}}
\newcommand{\gray}[1]{{\color{gray}{#1}}}

\newcommand{\citeme}[1]{\red{[XX]}}
\newcommand{\refme}[1]{\red{(XX)}}



\newcommand{\tran}{^\top}
\newcommand{\mtran}{^{-\top}}
\newcommand{\zcol}{\mathbf{0}}
\newcommand{\zrow}{\zcol\tran}

\newcommand{\ind}{\mathbbm{1}}
\newcommand{\expect}{\mathbb{E}}
\newcommand{\nat}{\mathbb{N}}
\newcommand{\zahl}{\mathbb{Z}}
\newcommand{\real}{\mathbb{R}}
\newcommand{\proj}{\mathbb{P}}
\newcommand{\prob}{\mathbf{Pr}}

\newcommand{\mif}{\textrm{if }}
\newcommand{\minimize}{\textrm{minimize }}
\newcommand{\maximize}{\textrm{maximize }}


\newcommand{\id}{\operatorname{id}}
\newcommand{\const}{\operatorname{const}}
\newcommand{\sgn}{\operatorname{sgn}}
\newcommand{\var}{\operatorname{Var}}
\newcommand{\mean}{\operatorname{mean}}
\newcommand{\trace}{\operatorname{tr}}
\newcommand{\diag}{\operatorname{diag}}
\newcommand{\vect}{\operatorname{vec}}
\newcommand{\cov}{\operatorname{cov}}

\newcommand{\softmax}{\operatorname{softmax}}
\newcommand{\clip}{\operatorname{clip}}

\newcommand{\defn}{\mathrel{:=}}
\newcommand{\peq}{\mathrel{+\!=}}
\newcommand{\meq}{\mathrel{-\!=}}

\newcommand{\floor}[1]{\left\lfloor{#1}\right\rfloor}
\newcommand{\ceil}[1]{\left\lceil{#1}\right\rceil}
\newcommand{\inner}[1]{\left\langle{#1}\right\rangle}
\newcommand{\norm}[1]{\left\|{#1}\right\|}
\newcommand{\frob}[1]{\norm{#1}_F}
\newcommand{\card}[1]{\left|{#1}\right|\xspace}
\newcommand{\diff}{\mathrm{d}}
\newcommand{\der}[3][]{\frac{d^{#1}#2}{d#3^{#1}}}
\newcommand{\pder}[3][]{\frac{\partial^{#1}{#2}}{\partial{#3^{#1}}}}
\newcommand{\ipder}[3][]{\partial^{#1}{#2}/\partial{#3^{#1}}}
\newcommand{\dder}[3]{\frac{\partial^2{#1}}{\partial{#2}\partial{#3}}}

\newcommand{\wb}[1]{\overline{#1}}
\newcommand{\wt}[1]{\widetilde{#1}}

\def\xssp{\hspace{1pt}}
\def\ssp{\hspace{3pt}}
\def\msp{\hspace{5pt}}
\def\lsp{\hspace{12pt}}

\newcommand{\cA}{\mathcal{A}}
\newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\cE}{\mathcal{E}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\cG}{\mathcal{G}}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\cI}{\mathcal{I}}
\newcommand{\cJ}{\mathcal{J}}
\newcommand{\cK}{\mathcal{K}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cM}{\mathcal{M}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\cO}{\mathcal{O}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\cQ}{\mathcal{Q}}
\newcommand{\cR}{\mathcal{R}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\cT}{\mathcal{T}}
\newcommand{\cU}{\mathcal{U}}
\newcommand{\cV}{\mathcal{V}}
\newcommand{\cW}{\mathcal{W}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}}
\newcommand{\cZ}{\mathcal{Z}}

\newcommand{\vA}{\mathbf{A}}
\newcommand{\vB}{\mathbf{B}}
\newcommand{\vC}{\mathbf{C}}
\newcommand{\vD}{\mathbf{D}}
\newcommand{\vE}{\mathbf{E}}
\newcommand{\vF}{\mathbf{F}}
\newcommand{\vG}{\mathbf{G}}
\newcommand{\vH}{\mathbf{H}}
\newcommand{\vI}{\mathbf{I}}
\newcommand{\vJ}{\mathbf{J}}
\newcommand{\vK}{\mathbf{K}}
\newcommand{\vL}{\mathbf{L}}
\newcommand{\vM}{\mathbf{M}}
\newcommand{\vN}{\mathbf{N}}
\newcommand{\vO}{\mathbf{O}}
\newcommand{\vP}{\mathbf{P}}
\newcommand{\vQ}{\mathbf{Q}}
\newcommand{\vR}{\mathbf{R}}
\newcommand{\vS}{\mathbf{S}}
\newcommand{\vT}{\mathbf{T}}
\newcommand{\vU}{\mathbf{U}}
\newcommand{\vV}{\mathbf{V}}
\newcommand{\vW}{\mathbf{W}}
\newcommand{\vX}{\mathbf{X}}
\newcommand{\vY}{\mathbf{Y}}
\newcommand{\vZ}{\mathbf{Z}}

\newcommand{\va}{\mathbf{a}}
\newcommand{\vb}{\mathbf{b}}
\newcommand{\vc}{\mathbf{c}}
\newcommand{\vd}{\mathbf{d}}
\newcommand{\ve}{\mathbf{e}}
\newcommand{\vf}{\mathbf{f}}
\newcommand{\vg}{\mathbf{g}}
\newcommand{\vh}{\mathbf{h}}
\newcommand{\vi}{\mathbf{i}}
\newcommand{\vj}{\mathbf{j}}
\newcommand{\vk}{\mathbf{k}}
\newcommand{\vl}{\mathbf{l}}
\newcommand{\vm}{\mathbf{m}}
\newcommand{\vn}{\mathbf{n}}
\newcommand{\vo}{\mathbf{o}}
\newcommand{\vp}{\mathbf{p}}
\newcommand{\vq}{\mathbf{q}}
\newcommand{\vr}{\mathbf{r}}
\newcommand{\Vs}{\mathbf{s}}
\newcommand{\vt}{\mathbf{t}}
\newcommand{\vu}{\mathbf{u}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\vw}{\mathbf{w}}
\newcommand{\vx}{\mathbf{x}}
\newcommand{\vy}{\mathbf{y}}
\newcommand{\vz}{\mathbf{z}}
\newcommand{\vone}{\mathbf{1}}
\newcommand{\vzero}{\mathbf{0}}

\newcommand{\valpha}{{\boldsymbol{\alpha}}}
\newcommand{\vbeta}{{\boldsymbol{\beta}}}
\newcommand{\vgamma}{{\boldsymbol{\gamma}}}
\newcommand{\vdelta}{{\boldsymbol{\delta}}}
\newcommand{\vepsilon}{{\boldsymbol{\epsilon}}}
\newcommand{\vzeta}{{\boldsymbol{\zeta}}}
\newcommand{\veta}{{\boldsymbol{\eta}}}
\newcommand{\vtheta}{{\boldsymbol{\theta}}}
\newcommand{\viota}{{\boldsymbol{\iota}}}
\newcommand{\vkappa}{{\boldsymbol{\kappa}}}
\newcommand{\vlambda}{{\boldsymbol{\lambda}}}
\newcommand{\vmu}{{\boldsymbol{\mu}}}
\newcommand{\vnu}{{\boldsymbol{\nu}}}
\newcommand{\vxi}{{\boldsymbol{\xi}}}
\newcommand{\vomikron}{{\boldsymbol{\omikron}}}
\newcommand{\vpi}{{\boldsymbol{\pi}}}
\newcommand{\vrho}{{\boldsymbol{\rho}}}
\newcommand{\vsigma}{{\boldsymbol{\sigma}}}
\newcommand{\vtau}{{\boldsymbol{\tau}}}
\newcommand{\vupsilon}{{\boldsymbol{\upsilon}}}
\newcommand{\vphi}{{\boldsymbol{\phi}}}
\newcommand{\vchi}{{\boldsymbol{\chi}}}
\newcommand{\vpsi}{{\boldsymbol{\psi}}}
\newcommand{\vomega}{{\boldsymbol{\omega}}}

\newcommand{\rLambda}{\mathrm{\Lambda}}
\newcommand{\rSigma}{\mathrm{\Sigma}}

\makeatletter
\newcommand*\bdot{\mathpalette\bdot@{.8}}
\newcommand*\bdot@[2]{\mathbin{\vcenter{\hbox{\scalebox{#2}{}}}}}
\makeatother

\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}
\def\eg{\emph{e.g}\onedot} \def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot} \def\Ie{\emph{I.e}\onedot}
\def\cf{\emph{cf}\onedot} \def\Cf{\emph{C.f}\onedot}
\def\etc{\emph{etc}\onedot} \def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot} \def\aka{a.k.a\onedot}
\def\etal{\emph{et al}\onedot}
\makeatother
 
\setstcolor{black}
\newcommand{\tea}[1][]{T_{#1}}
\newcommand{\stu}[1][]{U_{#1}}
\newcommand{\vsim}{\operatorname{sim}}
\newcommand{\vcls}{\sigma_{\mathrm{cls}}}
\newcommand{\vdet}{\sigma_{\mathrm{det}}}

\newcommand{\ours}{NSOD\xspace}
\newcommand{\oursg}{NSOD\xspace}
\newcommand{\oursx}{NSOD\xspace}

\newcommand{\yannis}[1]{\black{#1}}
\newcommand{\iavr}[1]{{#1}}
\newcommand{\zhaohui}[1]{{#1}}
\newcommand{\miaojing}[1]{{#1}}
\newcommand{\vitto}[1]{{VF: #1}}



\section{Introduction}\label{sec:intro}

The objective of {visual object detection} is to place a tight bounding box on every instance of an object class. With the advent of deep learning, recent methods~\cite{frcnn,ren2015nips} have significantly boosted the detection performance. Most are fully supervised, using a large amount of data with carefully annotated bounding boxes. However, annotating bounding boxes is expensive.


\iavr{To reduce} 
the amount of supervision, the most common setting is \emph{weakly-supervised object detection} (WSOD)~\cite{cinbis2014cvpr,wsddn,shi2016eccv,tang2017cvpr,tang2018eccv,shen2019cvpr,cvpr20efficientwsod}. In this setting,
we are given a set of images known to contain instances of certain classes as specified by labels, but we do not know the object locations in the form of bounding boxes or otherwise.
Many works~\cite{cinbis2014cvpr,shi2016eccv,shi2017arxiv}
formulate weakly supervised object detection as \emph{multiple instance learning} (MIL)~\cite{andrews2003nips}, which has been extended to be learnable end-to-end~\cite{wsddn,tang2017cvpr}.

There are \emph{mixed approaches} where a small number of images are annotated with bounding boxes and labels, and a large amount of images have only image-level labels~\cite{tang2016cvpr,hoffman2015cvpr,yan2017arxiv}. 
This is often referred as a \emph{semi-supervised} setting~\cite{tang2016cvpr,yan2017arxiv}, but there is no consensus.

\emph{Semi-supervised learning}~\cite{CSZ06} refers to using a small amount of labeled data and a large amount of unlabeled data. It is traditionally studied for classification~\cite{WeRC08,Lee13,RBH+15}, with one class label per image and no bounding boxes. In object detection, this would normally translate to a small number of images having labels {and} bounding boxes, and a large number of images having {no annotation at all}. This problem has been studied for the case where the fully annotated data (with bounding boxes) are enough to train a detector in the first place~\cite{1802.06964,RDG+18}, resulting in two-stage learning. But what if these data are very scarce?

\begin{figure}[t]
	\centering
	\includegraphics[width=0.8\columnwidth]{motivation3}
	\caption{We learn an object detector from a set of completely unlabeled images and one or few images per class with image-level label per image and no other information. 
}
	\label{Fig:moti}
\end{figure}

\miaojing{In this work, we study object detection in the challenging setting where only one or few images per class are given with only image-level class label per image, and a large amount of images with no annotation at all.
We use no bounding boxes or other information. This setting is illustrated in Fig.~\ref{Fig:moti}. 
Some initial exploration can be found in~\cite{ShHX15,marvaniya2012drawing,modolo2017pami} before deep learning.  
The few weakly-labeled images can be obtained via either labeling 
images from an unlabeled collection~\cite{ShHX15,marvaniya2012drawing} or using 
the top-ranking images from \emph{web image search} with the class name as the query~\cite{modolo2017pami}. Both paradigms are studied in our work.
The latter is preferable as it requires no human effort.
}

Our \miaojing{deep learning} solution is called \emph{nano-supervised object detection} (NSOD).
It begins by computing region-level class scores based on the similarity 
between the unlabeled images and 
{the few weakly-labeled images}, which we then pool into image-level class probabilities.
{This yields image-level \emph{pseudo-labels} on the entire unlabeled set, which we use to train a \emph{teacher} model on a classification task.}
Then, by predicting new image-level \iavr{multi-class} pseudo-labels on the unlabeled set, we train a \emph{student} model on a detection task, using a weakly-supervised object detection pipeline.


\head{Contributions.}
\miaojing{We study the very challenging 
problem} of training
an object detector from few 
images with only image-level labels and many images with no annotation at all. We introduce a new method for this problem that is \emph{simple}, \emph{efficient} (cost comparable to standard WSOD), and \emph{modular} (can build on any WSOD pipeline). By using the recent pipeline of PCL~\cite{tang2018pami} and more unlabeled images, we achieve performance competitive or superior to many recent WSOD solutions.
{On PASCAL VOC 2007 test set for instance, using 20 web images per class,
we get a detection mAP of 42, compared to 43.5 of PCL, which is using image-level labels on the entire training set.
}


 \section{Related Work}
\label{sec:related}



\subsection{Weakly supervised object detection (WSOD).}
In this setting, all training images have image-level class labels. A classic approach is 
\emph{multiple instance learning} (MIL)~\cite{andrews2003nips}, considering each training image as a ``bag'' and iteratively selecting {high-scoring object proposals}
from each bag, treating them as ground truth to learn an object detector. 


Bilen and Vedaldi~\cite{wsddn} introduce \emph{weakly-supervised deep detection network} (WSDDN), which pools region-level scores into image-level class probabilities and enables end-to-end learning from image-level labels. 
Tang~\etal~\cite{tang2017cvpr} extend WSDDN to multiple instance detection network including \emph{online instance classifier refinement} (OICR) and introduce a weakly-supervised {region proposal network} as a plugin~\cite{tang2018eccv}. In \emph{proposal cluster learning} (PCL)~\cite{tang2018pami}, pre-clustering of object proposals followed by OICR accelerates learning and boosts performance. 
Zeng~\etal~\cite{zeng2019wsod2} propose a novel WSOD framework with objectness distillation by jointly considering bottom-up and top-down objectness from low-level measurement and CNN confidences with an adaptive linear combination. 
Wan~\etal~\cite{pami2019minentropy} introduce a min-entropy  model to learn object locations and a metric to measure the
randomness of object localization during learning. {Ren~\etal~\cite{cvpr20efficientwsod} employ an instance-aware self-training strategy for WSOD with Concrete DropBlock. } 



Besides improvements in the network architecture, there are also attempts to incorporate additional cues into WSOD that are still weaker than bounding boxes, \eg object \emph{size}~\cite{shi2016eccv} and \emph{count}~\cite{gao2018eccv}.
It is also common to use extra data to \emph{transfer knowledge} from a source domain and help localize objects in the target domain~\cite{shi2017iccv,uijlings2018revisiting}.
Large-scale weakly-labelled \emph{web images}~\cite{guo2018eccv,tao2018tmm} and \emph{videos}~\cite{singh2019cvpr,liang2015cvpr}
with noisy labels are also common as extra data.

Our problem is different from WSOD in that the few labeled images have no bounding boxes and the bulk of the training set is completely unlabeled.
We build our work on PCL~\cite{tang2018pami} but train it with image-level pseudo-labels.


\subsection{Semi-supervised learning.}
There are several works that assume a few images are annotated with object bounding boxes and the rest still have image-level labels as in WSOD~\cite{tang2016cvpr,hoffman2015cvpr,yan2017arxiv}. These are often called \emph{semi-supervised}~\cite{tang2016cvpr,yan2017arxiv}. However, semi-supervised may also refer to the situation where some images are labeled (at image-level or with bounding boxes) and the rest have {no annotation at all}~\cite{1802.06964,RDG+18}. This situation is consistent with the standard definition of \emph{semi-supervised learning}~\cite{CSZ06}.


{Despite advances in deep semi-supervised learning~\cite{hoffer2016arxiv,LA17,TV17}, most work focuses on classification tasks.
In \emph{pseudo-label}~\cite{Lee13} for instance, classifier predictions on unlabeled data are used as labels along with true labels on labeled data.}
Few exceptions focusing on detection~\cite{1802.06964,RDG+18} still assume enough labeled images to learn a detector in the first place, which is not the case in our work.

Dong \etal~\cite{dong2018pami} use few images with object bounding boxes and class labels along with many unlabeled images. However,
this method
relies on several models and iterative training, which is computationally expensive. By contrast, we develop an efficient solution that allows us to {use more unlabeled images}.
\iavr{Shi \etal~\cite{ShHX15} use a mixture of weakly-labeled images and unlabeled images for object detection. This method involves hand-crafted features and iterative message passing, which would not be straightforward or efficient to extend to a deep learning framework.} 







\begin{figure*}[t]
	\centering
	\includegraphics[width=1\textwidth]{overview1}
	\caption{\small Overview of our \emph{nano-supervised object detection} (NSOD) framework. We are given a support set  and a large unlabeled set . { contains one or few weakly-labeled images per class, obtained from the web or randomly labeled from }.
{Using the images in  and a feature extractor pre-trained on classification, we infer image-level class probabilities of images in }
(stage 1). We then extract pseudo-labels on  and
	{train a \emph{teacher} network}
 on a -way classification task (stage 2).
 is used to classify each proposal of images in , resulting in new image-level class probabilities (stage 3). We average these with the ones obtained in stage 1, based on . Finally, we extract multi-class pseudo-labels on  and train a \emph{student} network  {on weakly-supervised detection} by PCL~\cite{tang2018pami} (stage 4).}
	\label{Fig:intro}
\end{figure*}







\subsection{\iavr{Curated data.}}
\iavr{
Investigation of unsupervised settings relies on removing the labels from labeled datasets by default. This is the case \eg for \emph{object discovery}~\cite{TLBB10,sivic2005iccv} and \emph{semi-supervised classification} until today~\cite{berthelot2019mixmatch}. Such datasets are \emph{curated}, \ie, still depict the same classes and are more or less balanced. Working with unknown classes is a different problem of \emph{open-set recognition}~\cite{bendale2016towards}. At very large scale, keeping the top-ranking examples according to predicted class scores may be enough to address this problem~\cite{yalniz2019billion}. We experiment on both curated and unlabeled data \emph{in the wild} to show the robustness of our method.
}

 \section{Method}
\label{sec:method}

\subsection{Preliminaries}

\head{Problem.}
We are given a \emph{support set}  \iavr{containing  images per class}, each associated with an image-level label over  classes. 
We are \iavr{also} given \iavr{an \emph{unlabeled}} set of images , where each image \iavr{depicts} one or more 
\iavr{instances}
of \iavr{the}  classes, along with background clutter.
\iavr{In a harder setting, images in  may depict zero or more instances of the  classes, along with instances of unknown classes or background clutter.}
There is no bounding box or any other information in either  or .
Using these data and a {feature extractor  pre-trained on classification},
the problem is to learn a detector
to recognize instances of the  classes and localize them with bounding boxes in new images.






\head{Motivation.}
This problem relates to both weakly-supervised detection and semi-supervised classification.
Similar to the former, we study multiple instance learning but without image-level labels in the unlabeled set. 
\iavr{Unlike} 
the latter, at least in its common setting where thousands of examples are used~\cite{RBH+15,LA17}, 
 is too small to bootstrap the learning of a good classifier or detector:
 can be as few as one example per class. For this reason, we
propagate labels from  to  to initiate training.









\head{Method overview.}
{As shown in Fig.~\ref{Fig:intro}, we}
\iavr{begin}
by collecting \iavr{the support set} 
(Sec.~\ref{Sec:setup}).
We extract
{object proposals}~\cite{edgeboxes} from images in  and compare region-level features 
\iavr{obtained by}
a feature extractor 
against global features 
on
. We estimate class probabilities on  by propagating these similarities to image level (stage 1,
\iavr{Sec.~\ref{Sec:webassignment}}).
We infer pseudo-labels on  and train a \emph{teacher} network  inherited from  on a -way classification task (stage 2, Sec.~\ref{Sec:teacherstudent}).
\iavr{We use } 
to classify regions in images of , resulting in new image-level class probabilities (stage 3), which we average with the ones 
of
stage 1.
Finally, we \iavr{infer} multi-class pseudo-labels on  and train a \emph{student} network  {on a WSOD task} by PCL~\cite{tang2018pami} (stage 4).












\head{\iavr{Collecting the} support set .}
\miaojing{The support set can be obtained either by
{random selection
from some existing dataset or by web image search.
The latter is preferable as 
we would like 
images to be clean, \eg depicting only one class per image.} 
We experiment with both options.
} 







\subsection{Inferring class probabilities on }
\label{Sec:webassignment}

\miaojing{
Given the support set  and corresponding labels, 
we begin by propagating
the label information from  to the unlabeled set . For each image  in , we use \emph{edge boxes}~\cite{edgeboxes} to extract a collection of  object proposals (regions). Ideally, we would like to have one label per region so we can train an object detector. Since the supervision in our case is very limited, it is not realistic to assign an accurate label per region based only on . Instead, it is more reliable to estimate image-level class probabilities on . Inspired by the two-stream CNN architecture of WSDDN~\cite{wsddn}, we introduce a new way to infer image-level probabilities on , by aggregating region-level class probabilities.
}



\head{Similarity.}
{We} extract a feature vector  for each region  of image . We do the same for each image  in , extracting a feature vector . This is a global feature vector.
Let  be the support images labeled as class , with . Let also  be the -th region of .
We define the  \emph{similarity matrix}  with elements

where  denotes cosine similarity.


\head{Voting.}
Inspired by~\cite{wsddn},
{we form  classification matrix  with each row being the softmax of the same row of , implying competition over classes per region; similarly, we form  detection matrix  with each column being the softmax of the same column of , implying competition over regions per class:}

The -th row of  expresses a vector of class probabilities for region , while the -th column of  a vector of region probabilities (spatial distribution) for class .

The final image-level class scores  are obtained by element-wise product of  and  followed by sum pooling over regions

Each score  is in  and can be interpreted as the probability of object class  occurring in image .


\head{Discussion.}
The above is a robust \emph{voting strategy} which propagates proposal-level information to the image level, while suppressing noise. Formula~\eq{sim} suggests that region  will respond for class  if it is similar to any of the support images in . While this response is noisy since it is only based on a few examples, it is only maintained if it is among the strongest over all classes and all regions {in an image}.
\miaojing{Note that in~\cite{wsddn}, softmax is applied to two separate data streams 
during learning,
whereas it is applied
to the \emph{same matrix} in our 
work.
}


\miaojing{Alternative ways to transfer label information from  to  would be to directly learn a parametric classifier on  or define a nearest-neighbor classifier on  and infer image-level labels on .
We consider such baselines in our experiments. Their performance is not satisfactory, which highlights the importance of robustly propagating labels from region to image level.
}



\subsection{Teacher and student training}
\label{Sec:teacherstudent}



{Having class probability vectors~(\ref{eq:sum}) per image in ,}
a next step would be to convert them to multi-class pseudo-labels and
train the student
directly {on a weakly-supervised detection task}.
Nevertheless, probabilities generated this way rely on the few support images in  for classification, while the object information in the unlabeled set  is not exploited. \miaojing{To further enhance performance, we 
use
\emph{distillation}~\cite{HiVD15,RDG+18} to transfer knowledge between \emph{data} (labeled to unlabeled) and \emph{models} (classification to detection).}
\iavr{In particular, we
distill knowledge from the support set  to the unlabeled set  using a teacher classifier , and then distill this knowledge from the teacher to a student detector .}




\head{Data distillation.}
{We form the teacher  as the feature extractor network  followed by
a randomly initialized -output fully-connected layer and softmax. We then fine-tune
}
on a -way classification task on
. The probabilities~(\ref{eq:sum}) are meant for multi-label classification ( independent binary classifiers), while here we are learning a single -way classifier, {\ie for mutually exclusive labels.}
Given the class probability vector  for each image  in , we take the most likely class

as a \emph{-way pseudo-label}. {We fine-tune } on these pseudo-labels with a standard cross-entropy loss.


We have also tried several multi-label variants~\cite{wei2016pami,zhu2017cvpr}, which are inferior
to the simple -way cross-entropy loss {in our experiments}. This may be attributed to the class sample distribution in  being unbalanced.



\head{Knowledge distillation.}
{The fine-tuned teacher }
encodes object information of  into its network parameters. Directly using its image-level predictions on  would not be appropriate to
{train the student }
for detection, because the latter would need multi-class labels. On the other hand, using it as feature extractor to repeat the process of Sec.~\ref{Sec:webassignment} would not make much difference either, as it still produces class probabilities based on . Instead, we use {} to \emph{directly classify object proposals in }. Each proposal ideally contains one object, so it is particularly suitable to use {} as it was designed: a -way classifier.

Given an input image  in , we collect output {class} probabilities of {} on each region  of  into a  matrix  with
{element  being the probability of class .}
From this matrix, it is possible to estimate new image-level class probabilities by , similar to~\eq{sum}.
Because it is based on {} being trained on  as classifier, while ~\eq{sum} is based on  alone, we combine their strength by averaging both into a probability vector

corresponding to image .


An image-level \emph{multi-class pseudo-label}  is then obtained from  by element-wise thresholding. An element  specifies that an object of class  occurs in image . In the absence of prior knowledge or validation data, we choose  as threshold. \iavr{Importantly, an all-negative pseudo-label  is possible, \eg when an image does not depict any known class. This simple mechanism allows our method to work in the harder setting where images in  may depict only unknown classes.}

{Those image-level pseudo-labels are all that is needed to obtain an object detector if we use any WSOD pipeline. In particular, we train the student model  on weakly-supervised detection on  using \emph{proposal cluster learning} (PCL)~\cite{tang2018pami}. Weakly-labeled images in  are also included into the training with loss weight 1.} 

\head{Inference.}
\miaojing{
At inference, the teacher classifier is not needed. The trained student detector is used directly.
}

\miaojing{
}



























%
 
\section{Experiments}
\label{sec:exp}

\subsection{Experimental setup}
\label{Sec:setup}

\head{Unlabeled set .}
We choose the standard object detection datasets PASCAL VOC 2007 and 2012~\cite{pascalvoc} {for the unlabeled set}, having 20 classes. Each dataset contains a \emph{trainval} set and a \emph{test} set. For VOC 2007, the \emph{trainval} set has 5011 images and the \emph{test} set 4952 images. For VOC 2012, the size of \emph{trainval} and \emph{test} sets are 11540 and 10991, respectively.
{We use the \emph{trainval} sets as  to train the object detector by default.  We evaluate the detector on the \emph{test} set.}
{Importantly, except for the support set, we do not use any labels, not even image-level labels in the training set.}



\begin{figure*}[t]
\centering
\includegraphics[width=1\textwidth]{google_support_example.pdf} \3pt]
\includegraphics[width=1\textwidth]{voc_support_example.pdf}
\caption{\small (Top) examples of top-ranking web images, using class names as queries. (Bottom) random selection of images from PASCAL VOC 2007.  }
\label{Fig:google}
\end{figure*}


\head{Support set .}
\iavr{Each image in the support set  should depict one of the known  classes (\emph{i.e.} 20 VOC classes). \iavr{A preferable way to collect  is from the web~\cite{modolo2017pami}: we use the class names as text queries and collect the top- results per class from web image search (\eg~Google).
The motivation is that these images}
are clean, \emph{i.e.} they mostly contain objects against a simple background and in a canonical pose and viewpoint, without clutter or occlusion (see examples in Fig.~\ref{Fig:google} (top)). Notwithstanding, they are not perfect, lacking diverse appearance and poses of the object class.
Collecting images from the web is easy
and does not need any human effort.
We choose this option by default. 
}

{Another common 
option
is to randomly 
sample  images per class
from an existing collection~\cite{ShHX15,marvaniya2012drawing} (\emph{e.g.} VOC 2007). This is a harder setting,
as these images may depict small objects, multiple instances, object classes in non-canonical pose, clutter and occlusion, \eg bottle, chair, and person in Fig.~\ref{Fig:google} (bottom). }
We experiment with both options.





\head{Networks.} We choose VGG16~\cite{vgg} as our student  by default, which is consistent with most WSOD methods~\cite{wsddn,tang2017cvpr,tang2018eccv,shen2019cvpr,tang2018pami}.
{Since the teacher network  (including the feature extractor )} is not used at inference time, we choose the more powerful ResNet-152~\cite{resnet}.
Both networks are pre-trained on the ILSVRC classification task~\cite{RDS+14}.

\head{Implementation details.} We use  images per class by default for . Following representative WSOD methods~\cite{wsddn,tang2017cvpr,tang2018pami,zhang2018cvpr}, we adopt \emph{edge boxes}~\cite{edgeboxes} to extract  proposals on average per image in .
For the default teacher model , we first resize the input image to  pixels on the short side and then crop it to . We set the batch size to  and the learning rate to  initially with cosine decay. For the default student model , we feed the network with one image per batch. The training lasts for  iterations in total; the learning rate starts at  and decays by an order of magnitude at  iterations.

\begin{table*}[t]
	\centering
	\setlength{\tabcolsep}{2pt}
	\scalebox{.58}[0.58]{
		\begin{tabular}{cccccccccccccccccccccc}
			\toprule
			\textsc{Method} & aero & bike & bird & boat & bott & bus & car & cat & char & cow & tabl & dog & hors & mbik & prsn & plat & shep & sofa & tran & tv & mAP \\
			\midrule
			\ours  & 57.9 & {59.7} & 43.2 & {10.5} & 13.1 & 62.7 & 58.6 & 43.9 & 10.6 & 51.1 & \textbf{25.7} & 49.8 & 39.3& {60.6} & 14.9 & 10.9 & {33.5} & 45.2 & {42.5} & \textbf{27.8} &{38.0} \\  \ours (07+12) & 51.5& \textbf{65.2} & \textbf {48.9} &  \textbf{13.2} & \textbf{19.7} & \textbf{64.8} & \textbf{59.3} & \textbf{55.5} & \textbf{12.4} & \textbf{59.3} & 24.3 & \textbf{54.1} & \textbf{47.4} & \textbf{62.8} & \textbf{20.7} & \textbf{15.0} & \textbf{39.5} & \textbf{51.3} & \textbf{53.8} & 21.4 & \textbf{42.0} \\
							\midrule
			\iavr{NS-FT} & 56.7 & 37.2 & 31.8 & 10.7 & 4.6 & 44.7 & 42.7 & 51.4 & 3.5 & 17.7 & 4.2 & 37.6 & 22.5 & 51.6 & 13.1 & 10.0 & 28.9 & 36.3 & 39.2 & 14.3 & \iavr{27.9} \\
			\iavr{NS-NN} & 59.2 & 33.3 & 28.3 & 22.5 & 5.4 & 43.7 & 39.3 & 32.3 & 2.3 & 40.1 & 7.5 & 42.2 & 34.2 & 33.2 & 12.6 & 7.7 & 30.5 & 31.1 & 47.6 & 13.7 & \iavr{28.3} \\
			NS-MT () & 49.6 & 33.9 & 29.6 & 15.5 & 9.5 & 47.9 & 32.9 & 49.1 & 0.2 & 13.2 & 21.1 & 34.4 & 19.7 & 31.5 & 9.6 & 9.9 & 35.6 & 43.1 & 38.9 & 15.0 & 27.0\\
			\midrule
			WSDDN~\cite{wsddn}& 39.4 &  50.1 & 31.5 & 16.3 & 12.6& 64.5  & 42.8 & 42.6 & 10.1 & 35.7 &  24.9 &  38.2 &  34.4 &  55.6 &  9.4  & 14.7 &  30.2 & 40.7& 54.7 & 46.9 & 34.8 \\
			OICR~\cite{tang2017cvpr} & \textbf{58.0} & 62.4 & 31.1 & 19.4  & 13.0  & \textbf{65.1} & 62.2 & 28.4 &  24.8& 44.7 & 30.6& 25.3 & 37.8 & 65.5 & 15.7 &24.1 &41.7& 46.9& \textbf{64.3}& 62.6& 41.2\\
			WSRPN~\cite{tang2018eccv} & 57.9 & \textbf{70.5}& 37.8 &5.7 &21.0& 66.1& \textbf{69.2}& 59.4 &3.4 &\textbf{57.1}& \textbf{57.3}& 35.2& \textbf{64.2}& \textbf{68.6}& \textbf{32.8}& \textbf{28.6}& \textbf{50.8}& 49.5& 41.1& 30.0& 45.3 \\
			PCL~\cite{tang2018pami} & 54.4& {69.0} & 39.3& 19.2& 15.7& 62.9& 64.4& 30.0& \textbf{25.1}& 52.5& 44.4& 19.6 & 39.3& 67.7 & 17.8& 22.9& 46.6& \textbf{57.5} & 58.6& \textbf{63.0} & 43.5\\
			WS-JDS~\cite{shen2019cvpr} & 52.0 & 64.5 & \textbf{45.5} & \textbf{26.7} & \textbf{27.9}& 60.5& 47.8& \textbf{59.7}& 13.0 & 50.4& 46.4& \textbf{56.3}& 49.6& 60.7 & 25.4 & 28.2 & 50.0 & 51.4& 66.5& 29.7 & \textbf{45.6}\\
			\bottomrule
		\end{tabular}
	}
	\vspace{3pt}
	\caption{Detection mAP on the \emph{test} set of PASCAL VOC 2007. \ours: our nano-supervised object detection framework;
		\iavr{NS-FT: nano-supervised fine-tuning; NS-NN: nano-supervised nearest neighbor; NS-MT: Nano-supervised mean teacher}.
		Unless otherwise stated, \ours, NS-FT, NS-NN use  support images per class by default. All compared methods~\cite{wsddn,tang2017cvpr,tang2018eccv,tang2018pami,shen2019cvpr} use the image-level labels in the unlabeled set ; \ours,
		\iavr{NS-FT, NS-NN and NS-MT}
		do not.}
	\label{tab:det_map_voc2007}
\end{table*}

\head{Evaluation protocol.}
We evaluate the performance of our NSOD framework
on both image classification and object detection.
For image classification, we measure the \emph{average precision} (AP) and \emph{mean AP} (mAP) for \emph{multi-class predictions}~\cite{wei2016pami,zhu2017cvpr}, as well as the accuracy of the top-1 class prediction per image on the \emph{trainval} set of .
For object detection, we quantify 
localization performance 
on the \emph{trainval} set by \emph{CorLoc}~\cite{wsddn,shi2016eccv,tang2017cvpr,zhang2018cvpr} and detection performance on the \emph{test} set by mAP.
At test time, the detector can localize multiple instances of the same class per image and mAP is identical to what is used to evaluate fully supervised object detectors.

\head{Evaluation scenarios.}
Below, we first present the object detection results on the \emph{test} set of  under two scenarios: \emph{support set  by web search} (\autoref{sec:web}) and \emph{by sampling VOC 2007} (\autoref{sec:voc}). Then in the ablation study (\autoref{sec:ablation}), we provide detection and classification results on the \emph{trainval} set of  using the web search scenario. 

\subsection{{Support set  by web search}}
\label{sec:web}
{We first collect the support set by web search and evaluate our NSOD on both VOC 2007 and 2012. We also combine the two sets as well as images from ImageNet as distractors to evaluate our method in the wild.}

\subsubsection{Results on VOC 2007}\label{sec:results-voc2007}
\head{Comparison to weakly-supervised methods.}
We compare to several representative WSOD methods~\cite{wsddn,tang2017cvpr,tang2018eccv,tang2018pami,shen2019cvpr} in Table~\ref{tab:det_map_voc2007}. For fair comparison, all these methods use the same VGG16 backbone as we do, without bells and whistles.
\ours requires no annotation on the unlabeled set , while weakly-supervised methods assume image-level labels for all images in .

One directly competing method is PCL trained on ground truth image-level labels in . Despite using no annotation on , \ours achieves an mAP that is only 5.5\% below that of PCL (38.0 \vs 43.5). The result is is also competitive to other methods, \eg OICR~\cite{tang2017cvpr}, WSRPN~\cite{tang2018pami}. {There are also WSOD methods employing large-scale web images/videos as extra data. For instance, \cite{tao2018tmm} and \cite{singh2019cvpr} build on the WSDDN pipeline~\cite{wsddn} and produce mAP 36.8 and 39.4 on VOC 2007, respectively. Unlike these works, our \ours uses few web images, an unlabeled set, and an advanced WSOD pipeline. Importantly, \ours also delivers competitive mAP.}
Fig.~\ref{Fig:result} gives some examples of detection results of \ours on PASCAL VOC 2007.

\head{Comparison to semi-supervised methods.}
\iavr{
We first compare NSOD to two semi-supervised baselines: (1) fine-tune the teacher  on  as a -way classifier and use it to make predictions on , referred to as NS-FT; (2) use  from  as a global feature extractor to find the nearest neighbor in  for each image in , referred to as NS-NN. In both cases, we use the same support set with NSOD, infer image-level -way pseudo-labels on  and use them to train the student  by PCL. As shown in Table~\ref{tab:det_map_voc2007}, NS-FT and NS-NN deliver a mAP of 27.9 and 28.3, respectively.} \miaojing{Comparing to the mAP 38.0 of NSOD on the same setting (), these baselines are not satisfactory.
This is due to the limited the supervision from the support set and justifies our choice of propagating labels from region to image level. }

We then adapt the \emph{mean teacher}~\cite{TV17} semi-supervised classification method to our setting. Using image-level class probabilities ~(\ref{eq:avg}), we select the top- scored images as positive for each class and the rest we treat as negative.
With those pseudo-labels, we train PCL on VGG16, applying the consistency loss of~\cite{TV17} to image-level predictions on . We call this \emph{nano-supervised mean reacher} (NS-MT).
We choose  as it works the best in practice. NS-MT then yields an mAP of 27.0 as shown in Table~\ref{tab:det_map_voc2007}. This result is lower than our \ours by {11.0\%} ({27.0} \vs 38.0). This suggests it is not straightforward to transfer a successful semi-supervised approach from the classification to the detection task.

We have also tried to directly infer object bounding
boxes on the \emph{test} set of  using naive approaches like -NN classification on regions directly. Those fail, producing
mAP lower than 10. We should emphasize the importance
of propagating similarity scores from region-level to image-level as we do in NSOD.

\subsubsection{Results on VOC 2012}
Using the same support set , we train an object detector with our \ours on VOC 2012. The mAP is reported on the \emph{test} set of VOC 2012 and compared to representative WSOD methods~\cite{tang2017cvpr,tang2018pami,zhang2018cvpr} in Table~\ref{tab:det_corloc_voc2012}.
Despite not using any VOC 2012 labels, \ours is only 4.0\% below PCL (36.6 \vs 40.6).



\begin{table*}[t]
\centering
\setlength{\tabcolsep}{2pt}
\scalebox{.58}[0.58]{
\begin{tabular}{cccccccccccccccccccccc}
\toprule
\textsc{Method} & aero & bike & bird & boat & bott & bus & car & cat & char & cow & tabl & dog & hors & mbik & prsn & plat & shep & sofa & tran & tv & mAP \\
\midrule
OICR~\cite{tang2017cvpr} & \textbf{67.7} & 61.2 &  41.5 &  \textbf{25.6} &  22.2 &  54.6 &  49.7 &  25.4 &  \textbf{19.9} &  47.0 &  18.1 &  26.0 &  38.9 &  67.7 &  2.0 & 22.6 &  41.1 &  34.3 &  37.9 & 55.3 &  37.9\\
ZLDN~\cite{zhang2018cvpr}& 54.3 & 63.7 & 43.1 & 16.9 & 21.5 &  \textbf{57.8}&  \textbf{60.4} &  50.9&  1.2 & 51.5&  \textbf{44.4} &  36.6 & \textbf{ 63.6} &  59.3& 12.8& 25.6& \textbf{47.8}& \textbf{47.2}& 48.9& 50.6& \textbf{42.9} \\
PCL~\cite{tang2018pami} & 58.2 & \textbf{66.0} & 41.8 & 24.8 & \textbf{27.2}& 55.7 & 55.2& 28.5 & 16.6& 51.0 & 17.5 & 28.6& 49.7& \textbf{70.5}& 7.1& \textbf{25.7}& 47.5 & 36.6 & 44.1& \textbf{59.2} & 40.6 \\
\midrule
\ours & 56.3 & 27.6 & 42.2 & 10.9 &  23.8 & 55.1 & 46.2 & 36.6  & 5.6  & 51.8 & 15.5 & 55.9 & 54.0 & 63.6 & \textbf{23.5} & 10.8 & 43.1 & 39.2 & \textbf{49.0} & 21.5 & 36.6 \\
\ours (07+12) &  57.3 & 50.7 & \textbf{49.2} & 11.3 & 21.2 & 56.8 & 46.4 & \textbf{55.0} &  6.6 & \textbf{52.7} & 12.8 & \textbf{61.8} & 45.8 & 64.7 & 18.9 & 10.5 & 34.9 & 41.0 & 48.1 & 19.9 & 38.6\\
\bottomrule
\end{tabular}
}
\vspace{3pt}
\caption{Detection mAP on \emph{test} set of PASCAL VOC 2012. Our \ours uses  support images per class. All compared methods~\cite{tang2017cvpr,tang2018pami,zhang2018cvpr} use the image-level labels in the unlabeled set ; our \ours does not.}
\label{tab:det_corloc_voc2012}
\end{table*}

\begin{table*}[t]
	\centering
	\setlength{\tabcolsep}{2pt}
	\scalebox{.58}[0.58]{
		\begin{tabular}{cccccccccccccccccccccc}
			\toprule
			\textsc{Method} & aero & bike & bird & boat & bott & bus & car & cat & char & cow & tabl & dog & hors & mbik & prsn & plat & shep & sofa & tran & tv & mAP \\
			\midrule
{\ours (07+Dis5k)}   & 59.3 & 35.4 & 37.6 & 16.6 & 7.5 & 59.1 & 59.0 & 42.2 & 9.0 & 47.4 & 33.2 & 50.8 & 46.3 & 52.4 & 15.1 & 18.7 & 44.2 & 50.3 & 51.6 & 35.3 & 37.6\\
			{\ours (07+Dis10k)}  & 56.5 & 36.0 & 34.6 & 12.7 & 5.7 & 56.6 & 56.2 & 40.1 & 8.5 & 44.9 & 31.1 & 46.0 & 41.6 & 55.1 & 15.7 & 15.1 & 39.9 & 46.8 & 47.6 & 31.2 & 36.5 \\
			\ours (07+12+Dis5k)  & 59.8 & 65.8 & 50.1 & 12.5 & 16.5 & 58.6 & 52.1 & 57.0 & 15.8 & 51.1 & 31.5 & 53.9 & 36.4 & 58.8 & 18.1 & 15.4 & 43.3 & 50.4 & 48.1 & 38.8 & {41.7} \\
			\ours (07+12+Dis10k) & 51.4 & 68.1 & 36.1 & 11.8 & 17.7 & 59.6 & 63.1 & 61.8 & 10.2 & 46.5 & 32.1 & 57.0 & 37.1 & 61.3 & 17.7 & 17.1 & 44.0 & 47.7 & 44.9 & 33.0 & {40.9} \\
			\bottomrule
		\end{tabular}
	}
	\vspace{3pt}
	\caption{Detection mAP on the \emph{test} set of PASCAL VOC 2007 in the presence of distractors. \ours: our object detection framework.}
	\label{tab:det_map_ablation}
\end{table*}

\subsubsection{Results on VOC 2007 + 2012}\label{sec:Results-2007-2012}
Because  is unlabeled and our method is computationally efficient, we can easily improve performance by simply using more unlabeled data.
As shown in Table~\ref{tab:det_map_voc2007} and~\ref{tab:det_corloc_voc2012}, if we train \ours on the union of VOC 2007 and VOC 2012 (07+12) on a large-scale, the mAP can be further improved on the \emph{test} set of both VOC 2007 and 2012. For instance, on VOC 2007, NSOD (07+12) yields a mAP of 42\%, which is an improvement by +4\% over using VOC 2007 alone. Since neither set is labeled, this improvement comes at almost no cost. This result is only 1.5\% below PCL (42.0 \vs 43.5), and even outperforms WSDDN~\cite{wsddn} and OICR~\cite{tang2017cvpr} when trained on VOC 2007 with image-level labels. This is a strong result that confirms the value of our core contribution; similarly, on VOC 2012, NSOD (07+12) increases the mAP to 38.6, now outperforming OICR.  



\subsubsection{Results on PASCAL VOC + Distractors}
\miaojing{Despite being used without labels, VOC 2007 and 2012 are still \emph{curated}, \ie images depict at least one of the target classes. To further validate the effectiveness of our methd, we experiment with unlabeled data in the wild for , \ie, using images depicting unknown rather than target classes. In particular, we randomly select 5k (10k) images from ImageNet~\cite{RDS+14} and use the union of this set and VOC 2007, denoted by 07+Dis5k (07+Dis10k), as . Although there may be overlap between the 1000 ImageNet classes and the 20 PASCAL VOC classes, these images mostly contain unknown classes and play the role of distractors.}  The evaluation is on the test set of VOC 2007. As shown in Table~\ref{tab:det_map_ablation}, 07+Dis5k and 07+Dis10k yield a mAP of 37.6 and 36.5 respectively, which is slightly lower than using VOC 2007 alone as  (38.0), despite the distractor set being as large or twice as large as the curated set.

Similarly, we use the union of 5k (10k) images from ImageNet as well as VOC 2007 plus 2012, denoted by 07+12+Dis5K (07+12+Dis10k) and achieving mAP 41.7 (40.9).
In other words, our method can discover the relevant data from a noisy unlabeled set and therefore improve the detection performance (\eg from 38.0 on VOC 2007 alone) with no annotation cost.     
We find that the distractors
are mostly assigned no pseudo-labels due to thresholding of ~\eq{avg} in \ours. 






\begin{figure*}[t]
    \centering
	\includegraphics[width = 1.0\textwidth]{img3-rows2-4.pdf}
	\caption{\small Detection results of \ours on PASCAL VOC 2007, using default settings ().
		Top 2 rows: positive results (red boxes). Bottom row: failure cases (white boxes).
	}
	\label{Fig:result}
\end{figure*}





\begin{figure}[t]
    \centering
	\includegraphics[width = 0.8\linewidth]{voc_support.pdf}
	\caption{\small Detection mAP of \ours, NS-Base, NS-FT and PCL on PASCAL VOC 2007, using different number  of images per class as support set.
	}
	\label{Fig:result_voc_support}
\end{figure}


\begin{table*}
	\centering
	\setlength{\tabcolsep}{2pt}
	\scalebox{.58}[0.58]{
		\begin{tabular}{cccccccccccccccccccccc}
			\toprule
			\textsc{Method} & aero & bike & bird & boat & bott & bus & car & cat & char & cow & tabl & dog & hors & mbik & prsn & plat & shep & sofa & tran & tv & mAP \\
			\midrule
			\oursg & 88.8 & 85.8 & 98.0 & 67.8 & 79.4 & 68.4 & \textbf{96.8} & 95.1 & \textbf{80.6} & 72.1 & 38.9 & 93.4 & \textbf{82.3} & 65.2 & 98.0 & 56.7 & 70.1 & 55.6 & 72.0 & 60.2 & 76.3\\
			\oursx  & 86.4 & \textbf{96.9} & 97.1 & \textbf{71.4} & \textbf{98.5} & 67.1 & 89.9 & 95.1 & 80.0 & 66.8 & 36.5 & 92.9 & 74.2 & 62.9 & 96.9 & 53.1 & 59.9 & 58.8 & 70.1 & \textbf{78.9} & 76.7\\
			\ours & \textbf{91.2} & 90.7 & \textbf{98.0} & 71.1 & 94.3 & \textbf{73.8} & 95.8 & 95.5 & 80.5 & \textbf{74.7} & \textbf{39.1} & \textbf{95.3} & 81.2 & \textbf{66.9} & \textbf{98.4} & \textbf{58.7} & \textbf{73.8} & \textbf{59.7} & \textbf{75.6} & 70.4 & \textbf{79.2}\\
			\midrule
			\textsc{Method} & aero & bike & bird & boat & bottle & bus & car & cat & chair & cow & table & dog & horse & mbike & persn & plant & sheep & sofa & train & tv & mAcc \\
			\midrule
			\oursg & 92.2 & \textbf{97.7} & 99.1 & 78.7 & 100.0 & 73.0 & 93.2 & \textbf{98.5} & \textbf{89.2} & 82.0 & 41.8 & \textbf{97.7} & 77.7 & 72.0 & \textbf{99.7} & 63.7 & 68.7 & 63.0 & 77.5 & 87.5 & 82.7\\
			\oursx &  93.1 & 93.4 & 98.2 & 79.2 & 100.0 & 78.9 & 96.3 & 96.7 & 84.0 & 83.5 & 45.1 & 95.7 & 84.2 & 72.9 & 98.5 & 73.3 & 77.2 & 66.1 & 83.3 & \textbf{87.7} & 84.3\\
			\ours & \textbf{93.8} & 92.4 & \textbf{99.3} & \textbf{80.4} & \textbf{100.0} & \textbf{81.1} & \textbf{97.8} &{97.1} & 78.6 & \textbf{86.7} & \textbf{49.7} & 97.1 & \textbf{88.2} & \textbf{77.2} & 99.6 & \textbf{79.7} & \textbf{79.1} & \textbf{67.9} & \textbf{87.5} & 85.6 & \textbf{85.9} \\
			\bottomrule
		\end{tabular}
	}
	\vspace{3pt}
	\caption{Classification mAP for multi-class prediction (top) and classification mAcc for top-1 class prediction (bottom) on the \emph{trainval} set of PASCAL VOC 2007. \ours: our Nano-supervised object detection framework.}
	\label{tab:cls_map_voc2007_ablation}
\end{table*}

\begin{table*}[t]
	\centering
	\setlength{\tabcolsep}{2pt}
	\scalebox{.58}[0.58]{
		\begin{tabular}{cccccccccccccccccccccc}
			\toprule
			\textsc{Method} & aero & bike & bird & boat & bott & bus & car & cat & char & cow & tabl & dog & hors & mbik & prsn & plat & shep & sofa & tran & tv & mAP \\
			\midrule
			\oursg & 57.2 & 52.7 & 36.0 & 14.1 & 11.0 & 50.6 & 46.9 & 35.8 & 5.7 & 47.1 & 16.1 & 52.8 & 34.3 & 54.4 & 14.8 & 11.4 & 29.0 & 48.8 & 43.4 & 13.9 & 33.9 \\
			\oursx & \textbf{58.5} & 51.5 & 37.5 & 11.6 & 10.6 & 55.3 & 48.2 & 40.4 & 5.8 & 49.9 & 16.0 & 51.3 & 31.6 & 56.3 & 14.6 & 9.0 & 34.3 & 45.5 & 42.2 & 20.3 & 34.5\\
			\ours  & 57.9 & {59.7} & 43.2 & {10.5} & 13.1 & 62.7 & 58.6 & 43.9 & 10.6 & 51.1 & \textbf{25.7} & 49.8 & 39.3& {60.6} & 14.9 & 10.9 & {33.5} & 45.2 & {42.5} & \textbf{27.8} &{38.0} \\  \midrule
			\ours () & 53.0 & 58.0 & 24.4 & 13.3 & 11.3 & 41.3 & 43.8 & 43.6 & 2.3 & 50.3 & 6.1 & 32.4 & 19.0 & 50.5 & 15.0 & 8.7 & 35.7 & 41.7 & 42.8 & 6.2 & 30.0
			\\
			\ours () & 57.2 & 27.8 & 40.4 & 9.7 & 11.2 & 61.2& 57.0 & 25.9 & 13.4 & 47.2 & 6.2 & 45.5 & 35.7 & 53.0 & 21.2 & 14.1 & 34.8 & 43.7 & 39.8 & 19.8 & 33.2\\
			\ours ()  & 57.9 & {59.7} & 43.2 & {10.5} & 13.1 & 62.7 & 58.6 & 43.9 & 10.6 & 51.1 & \textbf{25.7} & 49.8 & 39.3& {60.6} & 14.9 & 10.9 & {33.5} & 45.2 & {42.5} & \textbf{27.8} &{38.0} \\  \bottomrule
		\end{tabular}
	}
	\vspace{3pt}
	\caption{\emph{Ablation study}. Detection mAP on the \emph{test} set of PASCAL VOC 2007. \ours: our nano-supervised object detection framework.}
	\label{tab:det_map_voc2007_ablation}
\end{table*}


\subsection{{Support set by sampling VOC 2007}}
\label{sec:voc}

{As discussed in Sec.~\ref{Sec:setup}, the support set  can be collected by randomly selecting  images per class from the unlabeled set . This is more challenging than web search, as one image may depict more than one object, as shown in Fig.~\ref{Fig:google}. We randomly sample  images per class from 
VOC 2007 with image-level labels as  and evaluate on its \emph{test} set.} We compare \ours with two baselines: (1) only using  to train the student , denoted by NS-Base; (2) using NS-FT as described in Sec.~\ref{sec:results-voc2007}.

{As shown in Figure~\ref{Fig:result_voc_support}, \ours yields significantly higher mAP at every  compared to the baselines. In particularly, with small , our improvement is substantial; with  (around 30\% of VOC 2007 training data), \ours achieves accuracy already very close (on par) to PCL~\cite{tang2018pami} (dotted horizontal line) that uses image-level labels of 100\% data in VOC 2007.}


\subsection{Ablation Study}
\label{sec:ablation}

We conduct the ablation study
on our labeling strategy, support set size, and localization on the \emph{trainval} set of PASCAL VOC 2007. The support set  is collected by web search. 

\subsubsection{Labeling strategy (classification)}
Referring to \autoref{Sec:teacherstudent} in the paper, we ablate combining  and  to generate image-level pseudo-labels.  is computed based on  alone, while  is computed based on the teacher model trained on . We apply a hard threshold of  on the predicted class probabilities of  and  to generate two sets of image-level pseudo-labels. We train two different models separately on the two sets of pseudo-labels, which we denote by \oursg and \oursx, respectively.

The classification accuracy of the two sets of pseudo-labels is first evaluated on the \emph{trainval} set of VOC 2007 and shown in Table~\ref{tab:cls_map_voc2007_ablation}. It can be seen that \oursg and \oursx produce a similar classification mAP of  \vs , while the AP on individual classes differs. However, in terms of top-1 class accuracy, \oursx is better than \oursg. This is reasonable, as \oursx is fine-tuned as a -way classifier, which takes the top-1 class predictions of  as pseudo-labels. The two sets of pseudo-labels are complementary by averaging  and  according to~Eq.(4), denoted by \ours. This improves both multi-class and top-1 class predictions, reaching the highest scores of  and , respectively.

\subsubsection{Labeling strategy (detection)} 
To further investigate the complementary effect of \oursg and \oursx, we evaluate their detection result on the \emph{test} set of VOC 2007 (Table~\ref{tab:det_map_voc2007_ablation}). The mAP of \oursx (34.5) is slightly greater than that of \oursg (33.9). Their combination (our full method \ours) further increases mAP by  to .
The detection result on the \emph{test} set is consistent with the classification result on the \emph{trainval} set, which validates our idea of distilling knowledge from the support set to the unlabeled set and from the teacher to the student model.

\subsubsection{Support set size} \label{sec:support_set_size}
We evaluate performance for different number  of web images per class of the support set  in Table~\ref{tab:det_map_voc2007_ablation}: mAP is 30.0 for , 33.2 for  and 38.0 for .
Further increasing  presumably brings more noisy examples.  How to deal with large-scale noisy web images/videos is an open problem~\cite{guo2018eccv,tao2018tmm,singh2019cvpr,liang2015cvpr}. We keep  small to avoid bringing too many noisy images, while at the same time using the unlabeled unlabeled set  for
{more diversity.}


\begin{table*}[!]
	\centering
	\setlength{\tabcolsep}{2pt}
	\scalebox{.58}[0.58]{
		\begin{tabular}{cccccccccccccccccccccc}
			\toprule
			\textsc{Method} & aero & bike & bird & boat & bott & bus & car & cat & char & cow & tabl & dog & hors & mbik & prsn & plat & shep & sofa & tran & tv & mAP \\
			\midrule
			WSDDN~\cite{wsddn}& 65.1 & 58.8 & 58.5 & 33.1 & 39.8 & 68.3 & 60.2 & 59.6&  34.8 & 64.5&  30.5&  43.0&  56.8&  82.4& 25.5& 41.6& 61.5& 55.9& 65.9& 63.7& 53.5 \\
			OICR~\cite{tang2017cvpr} & 81.7 & 80.4& 48.7 & \textbf{49.5} & 32.8&  81.7 & 85.4 & 40.1 & \textbf{40.6}&  79.5 &  35.7 &  33.7 &  60.5&  88.8 &  21.8&  57.9&  76.3&  59.9&  75.3&  \textbf{81.4}&  60.6\\
			WSRPN~\cite{tang2018eccv} & 77.5 &{81.2}& 55.3& 19.7& 44.3 &80.2& \textbf{86.6} &\textbf{69.5} &10.1 &\textbf{87.7} &\textbf{68.4} &52.1 &\textbf{84.4}& \textbf{91.6}& \textbf{57.4} &\textbf{63.4} &\textbf{77.3} &58.1 &57.0 &53.8 &63.8 \\
			PCL~\cite{tang2018pami} & 79.6 & \textbf{85.5}& 62.2 & 47.9 & 37.0 & \textbf{83.8 }& {83.4} & 43.0 & 38.3 &  80.1 & 50.6 & 30.9&  57.8 &  90.8&  27.0& 58.2& 75.3& \textbf{68.5}& 75.7& 78.9& 62.7\\
			WS-JDS~\cite{shen2019cvpr} & \textbf{82.9} & 74.0& \textbf{73.4}& 47.1& \textbf{60.9}& 80.4& 77.5& 78.8& 18.6& 70.0 &56.7& 67.0 &64.5& 84.0& 47.0& 50.1& 71.9& 57.6 &\textbf{83.3} &43.5& \textbf{64.5}\\
			\midrule
			\ours & 80.0 & 73.3 & 66.1& 34.0 & 29.0& 72.6& 76.5& 56.4 & 17.7& 74.7& 47.5& 61.4& 60.5& 86.4& 31.9& 36.6& 60.8& 59.1 & 57.4& 49.1& 56.6\\
			\ours (07+12) & 78.3 & 78.4 & 70.3 & 34.0 & 34.0 & 75.1 & 76.6 & 66.9 & 24.8 & 76.0 & 45.6 & \textbf{69.8} & 67.7 & 88.8 & 34.4 & 41.4 & 67.0 & 62.1 & 67.3 & 40.9 & 60.0\\
			\bottomrule
		\end{tabular}
	}
	\vspace{3pt}
	\caption{CorLoc on the \emph{trainval} set of PASCAL VOC 2007. All compared methods~\cite{wsddn,tang2017cvpr,tang2018eccv,tang2018pami,shen2019cvpr} use the image-level labels in ; our \ours does not.}
	\label{tab:det_corloc_voc2007}
\end{table*}



\subsubsection{Localization on the trainval set}
Apart from the mAP on the \emph{test} set, in Table~\ref{tab:det_corloc_voc2007} we report CorLoc on the \emph{trainval} set of VOC 2007, as is common for weakly-supervised detection methods~\cite{wsddn,tang2017cvpr,tang2018eccv,tang2018pami,shen2019cvpr}. Our NSOD delivers CorLoc 56.6, which is very close to other WSOD methods despite using no annotations on .  Like in \autoref{sec:Results-2007-2012}, if we train \ours on the union of VOC 2007 and VOC 2012 (07+12) on a large-scale, the CorLoc of \ours(07+12) on VOC 2007 (see Table~\ref{tab:det_corloc_voc2007}) is increased to 60.0, which is only 2.7\% below PCL (62.7) and generally among the best-performing WSOD methods (\eg OICR has 60.6). 





 \section{Discussion}
\label{sec:discussion}





Our nano-supervised object detection framework (\ours) basically begins with a combination of \emph{few-shot} and \emph{semi-supervised} classification. The former is using the few images as \emph{class prototypes}~\cite{snell2017} to estimate class probabilities per region, which are propagated at image level using the \emph{voting process} of WSDDN~\cite{wsddn}. The latter is generating \emph{pseudo-labels} on the unlabeled set from these probabilities to train a classifier~\cite{Lee13}.

By using the PCL pipeline~\cite{tang2018pami} and extending the unlabeled set to both VOC 2007 and VOC 2012, our \ours achieves detection mAP very close to PCL itself trained on VOC 2007 with image-level labels. Moreover, our result is already competitive or superior to many recent WSOD solutions.

It is reasonable to expect further improvement by applying our method to very large unlabeled collections. This is facilitated by the fact that \ours is robust to unknown classes and can discover relevant data even among non-curated collections. Moreover, since \ours produces image-level pseudo-labels that can be used to train any weakly-supervised detection pipeline, further improvement could be expected by using these pseudo-labels with more advanced WSOD methods. 




%
 
\section*{Acknowledgement}

This work was partially supported by the National Natural Science Foundation of China~(NSFC) under Grant No. 61828602 and 61876007. 
\bibliography{references}

\end{document}