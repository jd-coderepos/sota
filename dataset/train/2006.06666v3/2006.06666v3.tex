\documentclass[rebuttal]{cvpr}


\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{color}
\usepackage{colortbl}
\usepackage{comment}
\usepackage{enumitem}
\usepackage{epsfig}
\usepackage{etoolbox}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage{multirow}
\usepackage[numbers,sort&compress]{natbib}
\usepackage[olditem,oldenum]{paralist}
\usepackage{subfig}
\usepackage{tabularx}
\usepackage{times}
\usepackage{transparent}
\usepackage[dvipsnames]{xcolor}



\usepackage[pagebackref,breaklinks,colorlinks,bookmarks=False]{hyperref}
\usepackage{cleveref}

\usepackage{inconsolata}

\hypersetup{
   urlcolor=blue,
   citecolor=ForestGreen,
}

\def\cvprPaperID{410}
\def\confYear{CVPR 2021}


\newcommand{\jj}[1]{\textcolor{Brown}{#1}}
\newcommand{\kd}[1]{\textcolor{Plum}{#1}}
\newcommand{\todo}[1]{\textcolor{Red}{ToDo:#1}}

\newcommand{\virtex}[0]{VirTex}

\newcommand{\imagenet}[0]{ImageNet-1k}
\newcommand{\voc}[0]{PASCAL VOC}
\newcommand{\inclf}[0]{IN-1k}
\newcommand{\vocclf}[0]{VOC07}
\newcommand{\inat}[0]{iNaturalist 2018}
\newcommand{\inatclf}[0]{iNat 18}

\newcommand{\random}[0]{Random Init}
\newcommand{\insup}[0]{IN-sup}
\newcommand{\insupfif}[0]{IN-sup-}
\newcommand{\insupten}[0]{IN-sup-}
\newcommand{\mocoin}[0]{MoCo-IN}
\newcommand{\mocococo}[0]{MoCo-COCO}
\newcommand{\swavin}[0]{SwAV-IN}
\newcommand{\swavcoco}[0]{SwAV-COCO}

\newcommand{\ttbf}[1]{\textbf{\texttt{#1}}}
\newcommand{\band}{\rowcolor{gray!20}}
\newcommand{\gray}[1]{\textcolor{gray}{#1}}
\newcommand{\orangehl}[1]{\textcolor{RedOrange}{\textbf{#1}}}

\newcommand{\drop}[1]{\textcolor{gray}{\textsubscript{--#1}}}
\newcommand{\rise}[1]{\textcolor{gray}{\textsubscript{+#1}}}

\newcommand{\Drop}[1]{\textcolor{Red}{\textsubscript{\bf --#1}}}
\newcommand{\Rise}[1]{\textcolor{Green}{\textsubscript{\bf +#1}}}

\newcommand{\attention}[1]{\ttbf{\textcolor{ForestGreen}{#1}}}


\newcommand\YAMLcolonstyle{\color{Black}\mdseries}
\newcommand\YAMLkeystyle{\color{Black}\bfseries}
\newcommand\YAMLvaluestyle{\color{RoyalBlue}\mdseries}

\makeatletter
\newcommand\language@yaml{yaml}

\expandafter\lstdefinelanguage
\expandafter{\language@yaml}
{
  keywords={true,false,null},
  keywordstyle=\color{RoyalBlue},
  basicstyle=\YAMLkeystyle,
  sensitive=false,
  comment=[l]{\#},
  commentstyle=\color{RedOrange}\ttfamily,
  stringstyle=\YAMLvaluestyle\ttfamily,
  moredelim=**[il][\YAMLcolonstyle{:}\YAMLvaluestyle]{:},
  frame=tb,
}

 
\newcommand{\rfour}{\textcolor{RoyalBlue}{\textbf{R4}}}
\newcommand{\rone}{\textcolor{RawSienna}{\textbf{R1}}}
\newcommand{\rfive}{\textcolor{Magenta}{\textbf{R5}}}

\newcommand{\fakeref}[1]{\textcolor{Red}{#1}}
\newcommand{\fakecite}[1]{\textcolor{ForestGreen}{#1}}


\begin{document}

\title{VirTex: Learning Visual Representations from Textual Annotations}
\maketitle

\noindent We thank the reviewers for their positive feedback!
We are encouraged that the reviewers thought our idea was \emph{good} [\rone], \emph{interesting and novel} [\rfour], \emph{exciting} [\rfive], and found the novelty to be the \emph{biggest strength of our paper} [\rfive].
We are glad that they thought our paper to be \emph{well-written}, \emph{technically sound}, having \emph{adequate experiments and ablations} [\rfour,\rfive], and appreciated our \emph{state-of-the-art qualitative results} [\rfour].
We are very pleased that \rfive{} has recognized potential impact of our work in different research areas, and \emph{pretty solidly recommends acceptance} of our paper!
We respond to specific reviewer concerns below.

\vspace{-5pt}
\noindent \rule{\linewidth}{0.5pt}



\noindent [\rone] \textbf{``Limited novelty, proposed method is combination of existing techniques'':}
Without any citations, we find this difficult to address.
Though \virtex{} comprises widely-used building blocks like ResNets and Transformers,
these can be changed (\fakeref{Section 4.2}) and
the novelty of our work is the idea of using language to learn visual features.
Indeed, [\rfour,\rfive] recognize this novelty and see it as a strength.
We request \rone{} to provide references to related existing techniques; we are happy to add a discussion in the final paper!

\noindent [\rone] \textbf{``Proposed method is very straightforward'':}
We agree, and think it is one of our strengths!
Recent successful pretraining techniques in vision and NLP (e.g. MoCo~[\fakecite{24}], SimCLR~[\fakecite{26}], BERT~[\fakecite{64}], GPT~[\fakecite{79}--\fakecite{81}]) are at their core, very simple and straightforward.
Despite its simplicity, \virtex{} is competitive to recent visual pretraining techniques.

\noindent [\rone] \textbf{``Insufficient comparisons. More comparison with existing vision-language methods needed'':}
We are unsure about which comparisons are missing, as \rone{} has not provided any citations.
Also, [\rfour,\rfive] differ from \rone, considering our experiments to be adequate.
We compare with leading pretraining methods --
ImageNet-supervised (\fakeref{Figure 3}), self-supervised learning, and concurrent work (\fakeref{Table 2}).
In \fakeref{L582--591}, we describe how we \emph{already} compare with \emph{seven} recent vision-language methods.
Since these methods rely on \emph{frozen} visual features that start with ImageNet-pretrained weights (\fakeref{L207--212}), our comparison with ImageNet-supervised  subsumes these methods.

\vspace{-5pt}
\noindent \rule{\linewidth}{0.5pt}



\noindent [\rfour] \textbf{``Primary benefit of this approach is pretraining when large-scale, noisy data is available from web'':}
Not quite.
\virtex{} is also beneficial in supervised regime with human-annotated images.
In \fakeref{Table 1}, we observe that \virtex{} (using captions) has superior annotation cost efficiency over pretraining methods on COCO each drawing supervision from different annotation types (labels and masks).

\noindent [\rfour] \textbf{``... but no direct modeling constraint for robustness to noisy web data'':}
We disagree with the premise of this concern.
We limit the scope of our experiments to the supervised regime (\fakeref{L20,85}).
We leave scaling to web data for future work (\fakeref{L860--863}).
Hence, it is not possible to justify any special modeling choices as of now.

Moreover, the massive scale of web data could compensate for noise and poor annotation (caption) quality.
Large-scale ConvNets (Instagram-ResNeXt~[\fakecite{45}]) and Transformers (GPT~[\fakecite{79}--\fakecite{81}]) learn effectively from web data \emph{without} any special modeling, besides vanilla model architectures.
Based on this evidence, it is unclear whether \virtex{} should require any such modeling.
We leave this for exploration in future work; we are actively working in this direction.

\vspace{-5pt}
\noindent \rule{\linewidth}{0.5pt}



\noindent [\rfive] \textbf{``Claim about using fewer images misleading'':}
We understand your concern, and thank you for your excellent suggestion!
On average, COCO contains \textbf{2.9} object classes and \textbf{5.7} instances per image.
We crop these objects and create a dataset of \textbf{860K} images.
We randomly expand bounding boxes on all edges by 0--30 pixels before cropping, to mimic ImageNet-like images.
We train a ResNet-50 with same hyperparameters as ImageNet-supervised models (SGD + momentum, 100 epochs etc.).
It achieves \textbf{79.1} \vocclf{} mAP (vs. \textbf{88.5} \virtex{}).
This shows that the data-efficiency of \virtex{} does not \emph{entirely} stem from having multiple objects per image.
However it is worth noting that unlike ImageNet, cropped COCO is much smaller (860K vs. 1.28M images) and has a non-uniform distribution.
We will include these results and discussion in the final paper.

\noindent [\rfive] \textbf{``The proposed method might be more susceptible to biases'':}
Unwanted biases introduced by language would absolutely pose a problem if our downstream task were language-based (e.g. captioning, VQA, Ref Exp.).
However, we discard the language model and solely focus on visual recongition tasks.
The fact that we match or exceed ImageNet pretraining on multiple tasks signals that language-supervised pretraining does not introduce \emph{significantly different} or \emph{additional} biases than using image labels alone.

Also, we see positive correlation between captioning and downstream vision task performance,
and our \emph{randomly selected} attention visualizations show decent visual grounding ability of \virtex{} (\fakeref{Supp, Figure 2,3,4}).
Hence, one can easily probe our models to uncover unwanted bias patterns.

\noindent [\rfive] \textbf{``Captions might not always be easier to collect; likely to be skewed towards common words'':}
Interesting thought, totally agree! We found this pattern in COCO Captions --
there are 17K mentions of \emph{dog}, yet their breeds occur rarely:
\emph{retriever} (45), \emph{shepherd} (41), \emph{terrier} (29).
However, building an ontology beforehand fails to cover the long-tail of visual concepts in complex scene images.
For example, LVIS dataset~[\fakecite{31}] built the ontology by filtering an arbitrarily large vocabulary \emph{during annotation} to cover 1200+ categories in COCO images.
Hence, an ontology of visual concepts vs. free-form captions has a \textit{coverage vs. specificity} trade-off.
We will add this discussion to the paper.




\end{document}
