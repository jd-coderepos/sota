


\begin{figure*}[ht]
	\centering
	\includegraphics[width=0.95\linewidth]{pics/grand+_pic_new2.pdf}
	\vspace{-0.15in}
	\caption{Illustration of \model.  \textmd{\small (a) \model adopts \textit{Generalized Forward Push} (\textit{GFPush}) and \textit{Top-k sparsification} to approximate the corresponding rows of propagation matrix $\mathbf{\Pi}$ for nodes in $L \cup U'$. (b) The obtained sparsified row approximations are then used to perform mini-batch random propagation to generate augmentations for nodes in the batch. (c) Finally, the calculated feature augmentations are fed into an MLP to conduct confidence-aware consistency training, which employs both supervised loss $\mathcal{L}_{sup}$ and confidence-aware consistency loss $\mathcal{L}_{con}$  for model optimization.}}
\label{fig:arc}
	\vspace{-0.06in}
\end{figure*}

\section{The \model Framework}
\label{sec:method}

In this section, we briefly review the graph random neural network (GRAND) and present its scalable solution \model for large-scale semi-supervised graph learning. 

\subsection{The Graph Random Neural Network}


Recently, Feng et al.~\cite{feng2020grand} introduce the graph neural neural network (GRAND) for semi-supervised node classification. 
GRAND is a GNN consistency regularization framework that optimizes the prediction consistency of unlabeled nodes in different augmentations. 


Specifically, it designs 
\textit{random propagation}---a mixed-order propagation strategy---to achieve graph data augmentations. 
First, the node features $\mathbf{X}$ are  randomly dropped with DropNode---a variant of dropout. 
Then the resultant corrupted feature matrix is propagated over the graph with a mixed-order matrix. 
Instead of the PPR matrix, GRAND uses an average pooling matrix $\mathbf{\Pi}^{\text{avg}}_{\text{sym}} = \sum_{n=0}^{N} \hat{\mathbf{A}}^n/(N+1)$ for propagation.
Formally, the random propagation strategy is formulated as: 
\begin{equation}
\small
\label{equ:randprop}
    \overline{\mathbf{X}} = \mathbf{\Pi}^{\text{avg}}_{\text{sym}} \cdot \text{diag}(\mathbf{z}) \cdot \mathbf{X}, \quad \mathbf{z}_i \sim \text{Bernoulli}(1 - \delta),
\end{equation}
where $\mathbf{z} \in \{0,1\}^{|V|}$ denotes the random DropNode masks drawn from Bernoulli($1 - \delta$), and $\delta$ represents DropNode probability. 
In doing so, the dropped information of each node is compensated by its neighborhoods. 
Under the homophily assumption of graph data, the resulting matrix $\overline{\mathbf{X}}$ can be seen as an effective data augmentation of the original feature matrix $\mathbf{X}$. 
Owing to the randomness of DropNode, this method can in theory generate exponentially many augmentations for each node. 


In each training step of \grand, the random propagation procedure is performed for $M$ times, leading to $M$ augmented feature matrices $\{\overline{\mathbf{X}}^{(m)}|1\leq m \leq M\}$. 
Then all the augmented feature matrices are fed into an MLP to get $M$ predictions. 
During optimization, \grand\ is trained with both the standard classification loss on labeled data and an additional \textit{consistency regularization} loss~\cite{berthelot2019mixmatch} on the unlabeled node set $U$, that is, 
\begin{equation}
\label{equ:grand_consis}
\small
    \frac{1}{M\cdot|U|}\sum_{s\in U}\sum_{m=1}^M \norm{\hat{\mathbf{Y}}_s^{(m)} - \overline{\mathbf{Y}}_s}_2^2, \quad \overline{\mathbf{Y}}_s = \sum_{m=1}^M\frac{1}{M}\hat{\mathbf{Y}}_s^{(m)},\end{equation}
where $\hat{\mathbf{Y}}_s^{(m)}$ is MLP's prediction probability for node $s$ when using $\overline{\mathbf{X}}^{(m)}_s$ as input. 
The consistency loss provides an additional regularization effect by enforcing the neural network to give similar predictions for different augmentations of unlabeled data.
With random propagation and consistency regularization, \grand\ achieves better generalization capability over conventional GNNs~\cite{feng2020grand}.




\vpara{Scalability  of \grand.} 
In practice, the $n$-th power of the adjacency matrix $\hat{\mathbf{A}}^n$ is computationally infeasible when $n$ is large~\cite{qiu2018network}. 
To avoid this issue, \grand adopts the power iteration to directly calculate the entire augmented feature matrix $\overline{\mathbf{X}}$ (in Equation~\ref{equ:randprop}), i.e., iteratively calculating and summing up the product of $\hat{\mathbf{A}}$ and $\hat{\mathbf{A}}^n\cdot\text{diag}(\mathbf{z})\cdot{\mathbf{X}}$ for $0\leq n < N$. This procedure is implemented with the sparse-dense matrix multiplication and has a linear time complexity of $\mathcal{O}(|V| + |E|)$. However, it needs to be performed for $M$ times at \textit{every} training step to generate different feature augmentations. 
Thus the total complexity of $T$ training steps becomes $\mathcal{O}(T\cdot M\cdot (|V|+|E|))$, which is prohibitively expensive when dealing with large graphs.



\subsection{Overview of \model}
\label{sec:overview}


We present \model to 
achieve both scalability and accuracy for graph based semi-supervised learning. 
It follows the general consistency regularization principle of \grand and  
comprises techniques to make it scalable to large graphs 
while maintaining \grand's flexibility and generalization capability. 


Briefly, instead of propagating features with power iteration, 
we develop an efficient approximation algorithm---generalized forward push (GFPush)---in \model to pre-compute the required row vectors of propagation matrix and perform random propagation in a mini-batch manner. 
The time complexity of this procedure is controlled by a predefined hyperparameter, avoiding the scalability limitation faced by \grand. 
Furthermore, \model\ adopts a new confidence-aware loss for consistency regularization, which makes the training process more stable and leads to better generalization performance than \grand. 



\vpara{Propagation Matrix.} In \model, we propose the following generalized mixed-order matrix for feature propagation:
\begin{equation}
\small
\label{equ:prop}
\mathbf{\Pi} = \sum_{n=0}^{N} w_n \cdot\mathbf{P}^n, \quad \mathbf{P} = \widetilde{\mathbf{D}}^{-1}\widetilde{\mathbf{A}},
\end{equation}
where $\sum_{n=0}^Nw_n=1$ and $w_n \geq 0$, $\mathbf{P}$ is the row-normalized adjacency matrix.
Different from the propagation matrices used in GRAND and other GNNs, the form of $\mathbf{\Pi}$ adopts a set of tunable weights $\{w_n|0\leq n \leq N\}$ to fuse different orders of adjacency matrices. 
By adjusting $w_n$, \model\ can {flexibly} manipulate the importance of different orders of neighborhoods to suit the diverse graphs of distinct structural properties in the real world. 
\hide{
Specifically, we consider three setups for $\mathbf{\Pi}$: 
1) {Truncated personalized PageRank matrix} $\mathbf{\Pi}^{\text{ppr}} =  \sum_{n=0}^{N}\alpha (1-\alpha)^n \mathbf{P}^{n}$; 
2) {Average pooling matrix} $\mathbf{\Pi}^{\text{avg}} = \sum_{n=0}^{N} \mathbf{P}^{n}/(N+1)$; 
3) {Single order matrix} $\mathbf{\Pi}^{\text{single}} =  \mathbf{P}^{N}$.
}

\vpara{Training Pipeline.} 
To achieve fast training, \model\ abandons the power iteration method which directly calculates the entire augmented feature matrix $\overline{\mathbf{X}}$, and instead computes each augmented feature vector separately for each node. 
Ideally, the augmented feature vector $\overline{\mathbf{X}}_s$ of node $s$ is  calculated by:


\begin{equation}
	\label{equ:mini}
\small
    \overline{\mathbf{X}}_{s} = \sum_{v \in \mathcal{N}^{\pi}_s} \mathbf{z}_v \cdot \mathbf{\Pi}{(s,v)} \cdot \mathbf{X}_v, \quad \mathbf{z}_v \sim \text{Bernoulli}(1-\delta).
\end{equation}
Here we use $\mathbf{\Pi}_s$ to denote the row vector of $\mathbf{\Pi}$ corresponding to node $s$, $\mathcal{N}^{\pi}_s$ is used to represent the indices of non-zero elements of $\mathbf{\Pi}_s$, $\mathbf{\Pi}{(s,v)}$ denotes the $v$-th element of $\mathbf{\Pi}_s$. 
This paradigm allows us to generate augmented features for only a batch of nodes in each training step, and thus enables us to use efficient mini-batch gradient descent for optimization. 

However, it is difficult to calculate the exact form of $\mathbf{\Pi}_s$ in practice. 
To address this problem, we develop several efficient methods to approximate $\mathbf{\Pi}_s$ in \model. 
The approximation procedure consists of two stages. 
In the first stage, we propose an efficient method \textit{Generalized Forward Push} (\textit{GFPush}) to compute an error-bounded approximation $\widetilde{\mathbf{\Pi}}_s$ for the row vector $\mathbf{\Pi}_s$. 
In the second stage, we adopt a \textit{top-$k$ sparsification} strategy to truncate $\widetilde{\mathbf{\Pi}}_s$ to only contain the top $k$ largest elements. 
The obtained sparsified row approximation $\widetilde{\mathbf{\Pi}}^{(k)}_s$ is used to calculate $\overline{\mathbf{X}}_s$ as a substitute of $\mathbf{\Pi}_s$ (Eq.~\ref{equ:mini}). 
For efficiency, it is required to pre-compute the corresponding row approximations for all nodes used in training.
In addition to labeled nodes,
\model\ also requires unlabeled nodes to perform consistency regularization during training. 
 To further improve efficiency, instead of using the full set of  $U$,  \model\ samples a smaller subset of unlabeled nodes $U'\subseteq U$ for consistency regularization.
As illustrated in Figure~\ref{fig:arc}, the training pipeline of \model\ consists of three steps:
\begin{itemize}
    \item \textit{Sub-matrix approximation.} 
    We obtain a sparsified row approximation $\mathbf{\Pi}^{(k)}_s$ for each node $s \in L \cup U'$ through GFPush and top-$k$ sparsification. 
    The resultant sparsified sub-matrix is used to support random propagation. 
    \item \textit{Mini-batch random propagation.} 
    At each training step, we sample a batch of nodes from $L \cup U'$ and generate multiple augmentations for each node in the batch with the approximated row vector.
    \item \textit{Confidence-aware consistency training.} 
    We feed the augmented features into an MLP to get corresponding predictions and optimize the model with both supervised loss and confidence-aware consistency loss.
\end{itemize}













\subsection{Sub-Matrix Approximation}


\vpara{Generalized Forward Push (GFPush).} It can be observed that the row-normalized adjacency matrix $\mathbf{P}=\widetilde{\mathbf{D}}^{-1}\widetilde{\mathbf{A}}$ is also the reverse random walk transition probability matrix~\cite{chen2020scalable} on $\widetilde{G}$, where row vector $\mathbf{P}_s$ denotes random walk transition probabilities starting from node $s$.
Based on this fact, we propose an efficient algorithm called \textit{Generalized Forward Push} (\textit{GFPush}) to approximate row vector $\mathbf{\Pi}_s=\sum_{n=0}^Nw_n\mathbf{P}^n_s$ with a bounded error.
GFPush is inspired by the \textit{Forward Push}~\cite{andersen2006local} algorithm for approximating personalized PageRank vector, while has much higher flexibility with the ability to approximate the generalized mixed-order matrix $\mathbf{\Pi}$.
The core idea of GFPush is to simulate an $N$-step random walk probability diffusion process from $s$ with a series of pruning operations for acceleration.
To achieve that, we should maintain a pair of vectors at each step $n$ ($0 \leq n \leq N$): 1) \textit{Reserve vector $\mathbf{q}^{(n)} \in \mathbb{R}^{|V|}$}, denoting the probability masses reserved at step $n$; 2) \textit{Residue vector $\mathbf{r}^{(n)}\in \mathbb{R}^{|V|}$}, representing the probability masses to be diffused beyond step $n$.



Algorithm~\ref{alg:GFPush} shows the pseudo-code of GFPush. At beginning, $\mathbf{r}^{(0)}$ and $\mathbf{q}^{(0)}$ are both initialized as the indicator vector $\mathbf{e}^{(s)}$ where $\mathbf{e}^{(s)}_s=1$ and  $\mathbf{e}^{(s)}_v=0$ for $v\neq s$, meaning the random walk starts from $s$ with the probability mass of 1. Other reserve and residue vectors (i.e., $\mathbf{r}^{(n)}$ and $\mathbf{q}^{(n)}, 1 \leq n \leq N$) are set to $\vec{0}$.
Then the algorithm iteratively updates reserve and residue vectors with $N$ steps. 
In the $n$-th iteration, the algorithm conducts a \textit{push} operation (Line 5--9 of algorithm~\ref{alg:GFPush}) for node $v$ which satisfies $\mathbf{r}^{(n-1)}_v>\mathbf{d}_v\cdot r_{max}$. Here $\mathbf{d}_v = \widetilde{\mathbf{D}}(v,v)$ represents the degree of $v$,
$r_{max}$ is a predefined threshold. In the push operation, the residue $\mathbf{r}^{(n-1)}_v$ of $v$ is evenly distributed to its neighbors, and the results are stored into the $n$-th residue vector
$\mathbf{r}^{(n)}$. Meanwhile, the reserve vector $\mathbf{q}^{(n)}$ is also updated to be identical with $\mathbf{r}^{(n)}$. After finishing the push operation on $v$, we reset $\mathbf{r}_v^{(n-1)}$ to 0 to avoid duplicate updates.


To gain more intuition of this procedure, we could observe that $\mathbf{r}^{(n-1)}_v/\mathbf{d}_v$ is the conditional probability that a random walk moves from $v$ to a neighboring node $u$, conditioned on it reaching $v$ with probability $\mathbf{r}^{(n-1)}_v$ at the previous step. Thus each push operation on $v$ can be seen as a one-step random walk probability diffusion process from $v$ to its neighborhoods. To ensure efficiency, GFPush only conducts push operations for node $v$ whose residue value is greater than $\mathbf{d}_v \cdot r_{max}$. Thus when the $n$-th iteration is finished, $\mathbf{q}^{(n)}$ can be seen as an approximation of the $n$-step random walk transition vector $\mathbf{P}^n_s$. And  $\widetilde{\mathbf{\Pi}}_s=\sum_{n=0}^N w_n\mathbf{q}^{(n)}$ is accordingly considered as the approximation of $\mathbf{\Pi}_s$ as returned by the algorithm. 

\vpara{Theoretical Analysis.} We have the following theorem about the bounds of time complexity, memory complexity, and approximation error of GFPush.

 \begin{thm}
 \label{thm1}
 Algorithm~\ref{alg:GFPush} has  $\mathcal{O}(N/r_{max})$  time complexity and $\mathcal{O}(N/r_{max})$ memory complexity, and returns $\widetilde{\mathbf{\Pi}}_s$ as an approximation of $\mathbf{\Pi}_s$ with the $L_1$ error bound: $\parallel\mathbf{\Pi}_s - \widetilde{\mathbf{\Pi}}_s\parallel_1 \leq N\cdot (2|E| +|V|) \cdot r_{max}$.
 \end{thm}
 
 \begin{proof}
 See Appendix~\ref{sec:proof}.
 \end{proof}
 
Theorem~\ref{thm1} 
suggests that 
the approximation precision and running cost of GFPush are negatively correlated with $r_{max}$. In practice, we could use $r_{max}$ to control the trade-off between efficiency and approximation precision.
 
\hide{
Let $\mathcal{T}_t$ be the total number of push operations performed in step $t$. When the $i-$th push operation performed on $v_i$, the value of $||\mathbf{R}^{(t)}_s||_1$ will be decreased by at least $r_{max} \cdot d_{v_i}$. Since $||\mathbf{R}^{(t)}_s||_1 \leq 1$, we must have $\sum^{\mathcal{T}_t}_{i=1}  d_{v_i} \cdot r_{max} \leq 1$, thus
\begin{equation}
\label{equ:tbound}
    \sum_{i=1}^{\mathcal{T}_t} d_{v_i} \leq \frac{1}{r_{max}}.
\end{equation}
In each push operation, we need to perform $d_{v_i}$ updates to $v_i$'s neighborhoods. So the total time of push operations in step $t$ and the count of non-zero elements of $\mathbf{R}^{(t+1)}_s$ (or $\mathbf{Q}^{(t+1)}_s$) are both bounded by $\sum_{i=1}^{\mathcal{T}_t} d_{v_i}$. 
According to Equation~\ref{equ:tbound}, we can further conclude that $\widetilde{\Pi}_s^{(\text{rw})}$ has at most $\frac{T}{r_{max}}$ non-zero elements. In implementation, all the vectors are stored as sparse vectors. Thus GFPush has an identical time complexity and memory complexity $\mathcal{O}(\frac{T}{r_{max}})$.


\vpara{Error Bound.} According to Lemma~\ref{lemma1}, we can conclude the following equations:
\begin{equation}
\begin{aligned}
    \norm{ \mathbf{P}_s^{(t)} - \mathbf{Q}_s^{(t)} }_1 
    &= \norm{ \sum_{h=0}^{t-1}  (\mathbf{A}^\mathsf{T}\mathbf{D}^{-1})^h\mathbf{R}_s^{(t-1-h)} }_1  \\
    &=  \norm{\sum_{h=0}^{t-1}\mathbf{R}_s^{(t-1-h)} }_1 \\
    & \leq \sum_{h=0}^{t-1} \norm{\mathbf{R}_s^{(t-1-h)}}_1.
\end{aligned}
\end{equation}
}


\begin{algorithm}[h]
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	\SetKwComment{Comment}{/* }{*/}
\footnotesize
\caption{GFPush}
\label{alg:GFPush}
\Input{Self-loop augmented graph $\widetilde{G}$, propagation step $N$, node $s$, threshold $r_{max}$, weight coefficients $w_n$, $0 \leq n \leq N$.}
\Output{An approximation $\widetilde{\mathbf{\Pi}}_s$ of transition vector $\mathbf{\Pi}_s$ of node $s$.}
$\mathbf{r}^{(n)} \leftarrow \vec{0}$ for $n=1,...,N$; $\mathbf{r}^{(0)} \leftarrow \mathbf{e}^{(s)}$  ($\mathbf{e}^{(s)}_s = 1$, $\mathbf{e}^{(s)}_v = 0$ for $v\neq s$).\\
$\mathbf{q}^{(n)} \leftarrow \vec{0}$ for $n=1,...,N$; $\mathbf{q}^{(0)} \leftarrow \mathbf{e}^{(s)}$.\\
\For{$n=1:N$}{
\While{there exists node $v$ with $\mathbf{r}^{(n-1)}_v> \mathbf{d}_v \cdot r_{max}$}{
		\For{each $u \in \mathcal{N}_v$}{
		\Comment{$\mathcal{N}_v$ is the neighborhood set of $v$ in graph $\widetilde{G}$. }
 			$\mathbf{r}^{(n)}_u\leftarrow \mathbf{r}^{(n)}_u + \mathbf{r}^{(n-1)}_v/\mathbf{d}_v$. 
 			
 			$\mathbf{q}^{(n)}_u \leftarrow \mathbf{r}^{(n)}_u$.
		}
 		$\mathbf{r}^{(n-1)}_v \leftarrow 0$.\\
 		\Comment{Perform a push operation on $v$.}
	}
}
$\widetilde{\mathbf{\Pi}}_s \leftarrow \sum_{n=0}^N w_n \cdot \mathbf{q}^{(n)}$.\\
\Return{$\widetilde{\mathbf{\Pi}}_s$.}
\end{algorithm}


\vpara{Top-$k$ Sparsification.} To further reduce training cost, we perform top-$k$ sparsification for $\widetilde{\mathbf{\Pi}}_{s}$. In this procedure, only the top-$k$ largest elements of $\widetilde{\mathbf{\Pi}}_{s}$ are preserved and other entries are set to $0$.
Hence the resultant sparsified transition vector $\widetilde{\mathbf{\Pi}}^{(k)}_s$ has at most $k$ non-zero elements. 
In this way, the model only considers the $k$ most important neighborhoods for each node in random propagation, which is still expected to be effective based on the clustering assumption~\cite{chapelle2009semi}.
Similar technique was also adopted by PPRGo~\cite{bojchevski2020scaling}.
We will empirically examine the effects of $k$ in Section~\ref{sec:param}.

\vpara{Parallelization.} In \model, we need to approximate row vectors for all nodes in $L\cup U'$. It could be easily checked that different row approximations are calculated independently with each other in GFPush. Thus we can launch multiple workers to approximate multiple vectors simultaneously. This procedure is implemented with multi-thread programming in our implementation. 











\subsection{Mini-Batch Random Propagation}
\label{sec:rand_prop}
\model\ adopts the sparsified row approximations of $\mathbf{\Pi}$ to perform random propagation in a mini-batch manner. 
Specifically, at the $t$-th training step, we randomly sample a batch of labeled nodes $L_t$ from $L$, and a batch of unlabeled nodes $U_t$ from $U'$. Then we calculate augmented feature vector $\overline{\mathbf{X}}_s$ for node $s \in L_t \cup U_t$ by:
\begin{equation}
\small
\label{equ:rpbatch}
\overline{\mathbf{X}}_{s} = \sum_{v \in \mathcal{N}^{(k)}_s} \mathbf{z}_v \cdot \widetilde{\mathbf{\Pi}}^{(k)}{(s,v)} \cdot \mathbf{X}_v, \quad \mathbf{z}_v \sim \text{Bernoulli}(1-\delta),
\end{equation}
where $\mathcal{N}^{(k)}_s$ denotes the non-zero indices of $\widetilde{\mathbf{\Pi}}^{(k)}_s$, $\mathbf{X}_v\in \mathbb{R}^{d_f}$ is feature vector of node $v$. At each training step, we generate $M$ augmented feature vectors $\{\overline{\mathbf{X}}^{(m)}_s|1 \leq m \leq M\}$ by repeating this procedure for $M$ times. Let $b = |L_t|+|U_t|$ denote the batch size. Then the time complexity of each batch is bounded by $\mathcal{O}(k \cdot b \cdot d_f)$.

\vpara{Random Propagation for Learnable Representations.} In Equation~\ref{equ:rpbatch}, the augmented feature vector $\overline{\mathbf{X}}_s$ is calculated with raw features $\mathbf{X}$. However, in some real applications (e.g., image or text classification), the dimension of $\mathbf{X}$ might be extremely large, which will incur a huge cost for calculation.
To mitigate this issue, we can employ a linear layer to transform each $\mathbf{X}_v$ to a low-dimensional hidden representation $\mathbf{H}_v\in \mathbb{R}^{d_h}$ firstly, and then perform random propagation with $\mathbf{H}$:
\begin{equation}
\small
\label{equ:high}
    \overline{\mathbf{X}}_{s} = \sum_{v \in \mathcal{N}^{(k)}_s} \mathbf{z}_v \cdot \widetilde{\mathbf{\Pi}}^{(k)}{(s,v)} \cdot   \mathbf{H}_v, \quad \mathbf{H}_v = \mathbf{X}_v\cdot \mathbf{W}^{(0)},
\end{equation}
where $\mathbf{W}^{(0)}\in \mathbb{R}^{d_f \times d_h}$ denotes learnable transformation matrix. In this way, the computational complexity of this procedure is reduced to $\mathcal{O}(k \cdot b \cdot d_h)$, where $d_h \ll d_f$ denotes the dimension of $\mathbf{H}_v$. 






\vpara{Prediction.} During training, the augmented feature vector $\overline{\mathbf{X}}^{(m)}_s$ is fed into an MLP model to get the corresponding outputs:
\begin{equation}
\small
\hat{\mathbf{Y}}^{(m)}_s = \text{MLP}(\overline{\mathbf{X}}^{(m)}_s, \mathbf{\Theta}),
\end{equation}
where $\hat{\mathbf{Y}}^{(m)}_s\in [0,1]^C$ denotes the prediction probabilities of $s$. $\mathbf{\Theta}$ represents MLP's parameters. 












\subsection{Confidence-Aware Consistency Training}
\model\ adopts both supervised classification loss and consistency regularization loss to optimize model parameters during training. The supervised loss is defined as the average cross-entropy over multiple augmentations of labeled nodes:
\begin{equation}
\small
\label{equ:suploss}
\mathcal{L}_{sup} = -\frac{1}{|L_t|\cdot M}\sum_{s \in L_t}\sum_{m=1}^{M}\mathbf{Y}_s \cdot \log(\hat{\mathbf{Y}}^{(m)}_s).
\end{equation}

\vpara{Confidence-Aware Consistency Loss.}
Inspired by recent advances in semi-supervised learning~\cite{berthelot2019mixmatch}, GRAND adopts an additional consistency loss to optimize the prediction consistency of multiple augmentations of unlabeled data, which is shown to be effective in improving generalization capability. \model also follows this idea, while adopts a new confidence-aware consistency loss to further improve effectiveness. 

Specifically, for node $s \in U_t$, we first calculate the distribution center by taking the average of its $M$ prediction probabilities, i.e., $\overline{\mathbf{Y}}_s=\sum_{m=1}^M \hat{\mathbf{Y}}^{(m)}_s/M$. Then we apply \textit{sharpening}~\cite{sohn2020fixmatch} trick over $\overline{\mathbf{Y}}_s$ to ``guess'' a pseudo label $\widetilde{\mathbf{Y}}_s$ for node $s$. Formally, the guessed probability on the $j$-th class of node $s$ is obtained via:\begin{equation}
\small
\label{equ:sharp}
\widetilde{\mathbf{Y}}(s,j) = \overline{\mathbf{Y}}(s,j)^{\frac{1}{\tau}}/\sum_{c=0}^{C-1}\overline{\mathbf{Y}}(s,c)^{\frac{1}{\tau}}, 
\end{equation}
where $0< \tau \leq 1$ is a hyperparameter to control the sharpness of the guessed pseudo label.  As $\tau$ decreases, $\widetilde{\mathbf{Y}}_s$ is enforced to become sharper and converges to a one-hot distribution eventually. 
Then the confidence-aware consistency loss on unlabeled node batch $U_t$ is defined as: 
\begin{equation}
\small
\label{equ:consis}
\mathcal{L}_{con} = \frac{1}{|U_t|\cdot M}\sum_{s\in U_t} \mathbb{I}(\max(\overline{\mathbf{Y}}_s)\geq \gamma) \sum_{m=1}^M \mathcal{D}(\widetilde{\mathbf{Y}}_s, \hat{\mathbf{Y}}^{(m)}_s),
\end{equation} 
where $\mathbb{I}(\max(\overline{\mathbf{Y}}_s)\geq \gamma)$ is an indicator function which outputs 1 if $\max(\overline{\mathbf{Y}}_s)\geq \gamma$ holds, and outputs 0 otherwise. $0\leq \gamma < 1$ is a predefined threshold. $\mathcal{D}(p, q)$ is a distance function which measures the distribution discrepancy between $p$ and $q$. Here we mainly consider two options for $\mathcal{D}$: \textit{$L_2$ distance} and \textit{KL divergence}.

Compared with the consistency loss used in \grand\ (Cf. Equation~\ref{equ:grand_consis}), the biggest advantage of $\mathcal{L}_{con}$ is that it only considers ``highly confident'' unlabeled nodes determined by threshold $\tau$ in optimization. This mechanism could reduce the potential training noise by filtering out uncertain pseudo-labels, further improving model's performance in practice. Combining $\mathcal{L}_{con}$ and $\mathcal{L}_{sup}$, the final loss for model optimization is defined as:


\begin{equation}
\label{equ:total_loss}
\small
\mathcal{L} = \mathcal{L}_{sup} + \lambda(t) \mathcal{L}_{con},
\end{equation}
where $\lambda(t)$ is a linear warmup function~\cite{goyal2017accurate} which increases linearly from 0 to the maximum value $\lambda_{max}$ as training step $t$ increases. 

\hide{
\vpara{Dynamic Loss Weight Scheduling.} We adopt a dynamic scheduling strategy to let $\lambda$ linearly increase with the number of training steps. Formally, at the $n$-th training step, $\lambda$ is obtained via 
\begin{equation}
\small
\label{equ:sch}
\lambda = \min(\lambda_{max},  \lambda_{max}\cdot n / n_{max}).
\end{equation}
In the first $n_{max}$ training steps, $\lambda$ increases from $\lambda_{min}$ to $\lambda_{max}$, and remains constant in the following training steps. This strategy limits $\lambda$ to a small value in the early stage of training when the generated pseudo-labels $\widetilde{\mathbf{Y}}$ are not much reliable, which will help model converge.
}

\vpara{Model Inference.}
After training, we need to infer the predictions for  unlabeled nodes. \model adopts power iteration to calculate the exact prediction results for unlabeled nodes during inference:
\begin{equation}
\small
\hat{\mathbf{Y}}^{(inf)} = \text{MLP}( \sum_{n=0}^Nw_n(\widetilde{\mathbf{D}}^{-1}\widetilde{\mathbf{A}})^n \cdot (1-\delta) \cdot \mathbf{X}, \mathbf{\Theta}),
\end{equation}
where we rescale $\mathbf{X}$ with $(1 - \delta)$  to make it identical with the expectation of the DropNode perturbed features used in training. 
Note that unlike \grand, the above power iteration process only needs to be performed once in \model, and the computational cost is acceptable in practice. 
Compared with obtaining predictions with GFPush as done in training, this inference strategy could provide more accurate predictions in theory.
Algorithm~\ref{alg:grand+} shows the entire training and inference procedure of \model.









\hide{
\begin{equation}
\hat{\mathbf{Y}}^{(inf)} = \text{MLP}(\mathbf{\Pi}\cdot (1-\delta) \cdot \mathbf{X}, \mathbf{\Theta}),
\end{equation}
}


\begin{algorithm}[t!]

	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	\SetKwComment{Comment}{/* }{*/}
	\caption{\model}
	\footnotesize
	\label{alg:grand+}
	\Input{Graph $G$, feature matrix $\mathbf{X} \in \mathbb{R}^{|V| \times d_f}$, labeled node set $L$, unlabeled node set $U$ and observed labels $\mathbf{Y}_L \in \mathbb{R}^{|L|\times C}$.}
	\Output{Classification probabilities $\hat{\mathbf{Y}}^{(inf)}$.} Sample a subset of unlabeled nodes $U'$ from $U$.\\

		\For{$s \in L \cup U'$}{     
		 $\widetilde{\mathbf{\Pi}}_s \leftarrow \text{GFPush}(G, s)$.\\
		 Obtain $\widetilde{\mathbf{\Pi}}^{(k)}_s$ by applying top-$k$ sparsification on $\widetilde{\mathbf{\Pi}}_s.$ \\
		 \Comment{Approximating row vectors with pallalization.}
		}
		\For{$t=0:T$}{
			Sample a batch of labeled nodes $L_t \subseteq L$ and a batch of unlabeled nodes $U_t \subseteq U'$.\\
			\For{$s \in L_t \cup U_t$}{
				\For{$m=1:M$}{
					Generate augmented feature vector $\overline{\mathbf{X}}^{(m)}_{s}$ with Equation~\ref{equ:rpbatch}.\\
Predict class distribution with $\hat{\mathbf{Y}}^{(m)}_s = \text{MLP}(\mathbf{\overline{X}}^{(m)}_s, \mathbf{\Theta}).$
				}
			}Compute $\mathcal{L}_{sup}$ via Equation~\ref{equ:suploss} and $\mathcal{L}_{con}$ via Equation~\ref{equ:consis}.\\
			Update the parameters $\Theta$ by mini-batch gradients descending: $\mathbf{\Theta} = \mathbf{\Theta} - \eta \nabla_\mathbf{\Theta} (\mathcal{L}_{sup} + \lambda \mathcal{L}_{con}).$\\
			\Comment{Stop training with early-stopping.}
		}


	  



		Infer classification probabilities $\hat{\mathbf{Y}}^{(inf)} =  \text{MLP}( \mathbf{\Pi}\cdot (1-\delta) \cdot \mathbf{X}, \mathbf{\Theta})$.
		
\Return{$\hat{\mathbf{Y}}^{(inf)}.$}
\end{algorithm}
\subsection{Model Analysis} 
\label{sec:model_analysis}

\vpara{Complexity Analysis.}  We provide detailed analyses for the time complexities of \model's different learning stages.
 According to Theorem~\ref{thm1}, the complexity of approximation stage (Line 2--5 of Algorithm~\ref{alg:grand+}) is $\mathcal{O}((|U'|+|L|)\cdot N/r_{max})$. As for the training stage (Line 6--15 of Algorithm~\ref{alg:grand+}), the total complexity of $T$ training steps is $\mathcal{O}(k\cdot b \cdot M \cdot T)$, which is  practically efficient for large graphs since $b$ and $k$ are usually much smaller than the graph size.
  The complexity of inference stage (Line 17 of Algorithm~\ref{alg:grand+}) is $\mathcal{O}((|V|+|E|) \cdot N)$, which is linear with the sum of node and edge counts.
 
\vpara{\model\ vs. PPRGo and GBP.}
Similar with \model,  PPRGo~\cite{bojchevski2020scaling} and GBP~\cite{chen2020scalable} also adopt matrix approximation methods to scale GNNs. However, \model\ differs from the two methods in several key aspects. PPRGo scales up APPNP by using Forward Push~\cite{andersen2006local} to approximate the ppr matrix. Compared with PPRGo, \model\ is more flexible in real applications thanks to the adopted generalized propagation matrix $\mathbf{\Pi}$ and GFPush algorithm. GBP also owns this merit by using the generalized PageRank matrix~\cite{li2019optimizing} for feature propagation. However, it directly approximates the propagation results of raw features through bidirectional propagation~\cite{banerjee2015fast}, whose computational complexity is linear with the raw feature dimension, rendering it difficult to handle datasets with high-dimensional features.
Moreover, different from PPRGo and GBP designed for the general supervised classification problem,  \model\ makes significant improvements for semi-supervised setting by adopting random propagation and consistency regularization to enhance generalization capability.


 
 
 
 
 
 
\hide{
In this section, we present the \full\ (\model) for semi-supervised learning on graphs. 
Its idea is to enable each node to randomly propagate with different subsets of neighbors in different training epochs. 
This random propagation strategy is demonstrated as an economic way for stochastic graph data augmentation, based on which we  design a consistency regularized training for improving \model's generalization capacity. 

Figure \ref{fig:arch2} illustrates the full architecture of \model.
Given an input graph, \model\ generates multiple data augmentations by performing random propagation (DropNode + propagation) multiple times at each epoch. 
In addition to the classification loss, \model\ also leverages a consistency regularization loss to enforce the models to give similar predictions across different augmentations. 
}



\hide{
\subsection{Random Propagation}


Given an input graph with its associated feature matrix, 
\model\ first conducts the random propagation process and then makes the prediction by using the simple multilayer perceptron (MLP) model. 

The motivation for random propagation is to address the non-robustness issue faced by existing GNNs~\cite{dai2018adversarial,zugner2019adversarial,zugner2018adversarial}. 
This process is coupled with the DropNode and propagation steps. 
In doing so, \model\  naturally separates the feature propagation and non-linear transformation operations in standard GNNs, enabling \model\ to reduce the risk of the overfitting and over-smoothing issues. 


\vpara{DropNode.}
\label{sec:randpro}
In random propagation, we aim to perform message passing in a random way during model training such that each node is not sensitive to specific neighborhoods. 
To achieve this, we design a simple yet effective node sampling operation---DropNode---before the propagation layer.



DropNode is designed to randomly remove some nodes' all features. 
In specific,  at each training epoch, the entire feature vector of each node is randomly discarded with a pre-defined probability, i.e., some rows of $\mathbf{X}$ are set to $\vec{0}$.  
The resultant perturbed feature matrix $\widetilde{\mathbf{X}}$ is then fed into the propagation layer. 

The formal DropNode operation is shown in Algorithm \ref{alg:dropnode}. 
First, we randomly sample a binary mask $\epsilon_i \sim Bernoulli(1-\delta)$ for each node $v_i$. 
Second, we obtain the perturbed feature matrix $\widetilde{\mathbf{X}}$ by multiplying each node's feature vector with its corresponding mask, i.e., $\widetilde{\mathbf{X}}_i=\epsilon_i \cdot \mathbf{X}_i$. 
Finally,  we scale $\widetilde{\mathbf{X}}$ with the factor of $\frac{1}{1-\delta}$ to guarantee the perturbed feature matrix is in expectation equal to $\mathbf{X}$. 
Note that the sampling procedure is only performed during training. 
During inference, we directly set $\widetilde{\mathbf{X}}$ with the original feature matrix $\mathbf{X}$. 


After DropNode, the perturbed feature matrix $\widetilde{\mathbf{X}}$ is fed into the propagation layer to perform message passing. 
Here we adopt mixed-order propagation, i.e., $\overline{\mathbf{X}} = \overline{\mathbf{A}} \widetilde{\mathbf{X}}$, 
where  $\overline{\mathbf{A}} =  \sum_{k=0}^K\frac{1}{K+1}\hat{\mathbf{A}}^k$---the average of the power series of $\hat{\mathbf{A}}$ from order 0 to order $K$. 
This kind of propagation rule enables the model to incorporate the multi-order neighborhood information, reducing the risk of over-smoothing when compared with using $\hat{\mathbf{A}}^K$ only. Similar ideas have been adopted in recent GNN studies~\cite{abu2019mixhop,abu2018n}.
  
\vpara{Prediction Module.}
After the random propagation module, the augmented feature matrix $\overline{\mathbf{X}}$ can be then fed into any neural networks for predicting nodes labels. 
In \model, we employ a two-layer MLP as the classifier, that is:
\begin{equation}
\small
\label{equ:mlp}
    P(\mathbf{Y}|\overline{\mathbf{X}};\Theta) = \sigma_2(\sigma_1(\overline{\mathbf{X}}\mathbf{W}^{(1)})\mathbf{W}^{(2)})
\end{equation}
where $\sigma_1(.)$ is the ReLU function, $\sigma_2(.)$ is the softmax function, and $\Theta=\{\mathbf{W}^{(1)} \in \mathbb{R}^{d \times d_h}, \mathbf{W}^{(2)} \in \mathbb{R}^{d_h \times C}\}$ is the model parameters. 


The MLP classification model can be also replaced with more complex and advanced GNN models, including GCN and GAT. 
The experimental results show that the replacements result in consistent performance drop across different datasets due to GNNs' over-smoothing problem (Cf. Appendix~~\ref{sec:oversmoothing_grand} for details). 


With this data flow, it can be realized that \model\ actually separates the feature propagation (i.e., $\overline{\mathbf{X}} = \overline{\mathbf{A}} \widetilde{\mathbf{X}}$ in random propagation) and transformation (i.e., $\sigma(\overline{\mathbf{X}} \mathbf{W})$ in prediction) steps, which are coupled with each other in standard GNNs (i.e., $\sigma(\mathbf{AX} \mathbf{W})$). 
This allows us to perform the high-order feature propagation $\overline{\mathbf{A}} =  \frac{1}{K+1}\sum_{k=0}^K\hat{\mathbf{A}}^k$ without increasing the complexity of neural networks, reducing the risk of overfitting and over-smoothing. 







\subsection{Consistency Regularized Training}
We show that random propagation can be seen as an efficient method for stochastic data augmentation. 
As such, it is natural to design a consistency regularized training algorithm for \model. 
  
  

\vpara{Random Propagation as Stochastic Data Augmentation.}
Random propagation randomly drops some nodes' entire features before propagation. 
As a result, each node only aggregates information from a random subset of its (multi-hop) neighborhood. 
In doing so, we are able to stochastically generate different representations for each node, which can be considered as a stochastic graph augmentation method. 
In addition, random propagation can be seen as injecting random noise into the propagation procedure. 


To empirically examine this data augmentation idea, we generate a set of augmented node representations $\overline{\mathbf{X}}$ with different drop rates in random propagation and use each $\overline{\mathbf{X}}$ to train a GCN for node classification on commonly used datasets---Cora, Citeseer, and Pubmed.  The results show that the decrease in GCN's classification accuracy is less than $3\%$ even when the drop rate is set to $0.5$. 
In other words, with half of rows in the input $\mathbf{X}$ removed (set to $\vec{0}$), random propagation is capable of generating augmented node representations that are sufficient for prediction. 


Though one single $\overline{\mathbf{X}}$ is relatively inferior to the original $\mathbf{X}$ in performance, in practice, multiple augmentations---each per epoch---are utilized for training the \model\ model. 
Similar to bagging~\cite{breiman1996bagging}, \model's random data augmentation scheme makes the final prediction model implicitly assemble models on exponentially many augmentations, yielding much better performance than the deterministic propagation used in GCN and GAT.





 
 \begin{algorithm}[tb]
\caption{Consistency Regularized Training for \model}
\small
\label{alg:2}
\begin{algorithmic}[1] \REQUIRE ~~\\
 Adjacency matrix $\hat{\mathbf{A}}$,
feature matrix $\mathbf{X} \in \mathbb{R}^{n \times d}$, 
times of augmentations in each epoch $S$, DropNode probability $\delta$.\\
\ENSURE ~~\\
Prediction $\mathbf{Z}$. \WHILE{not convergence}
\FOR{$s=1:S$} 
\STATE Apply DropNode via Algorithm \ref{alg:dropnode}: $
\widetilde{\mathbf{X}}^{(s)} \sim \text{DropNode}(\mathbf{X},\delta)$. 
\STATE Perform propagation: $\overline{\mathbf{X}}^{(s)} = \frac{1}{K+1}\sum_{k=0}^K\hat{\mathbf{A}}^k \widetilde{\mathbf{X}}^{(s)}$.
\STATE Predict class distribution using MLP: $\widetilde{\mathbf{Z}}^{(s)} = P(\mathbf{Y}|\overline{\mathbf{X}}^{(s)};\Theta)$.
\ENDFOR
\STATE Compute supervised classification loss $\mathcal{L}_{sup}$ via Eq. \ref{equ:loss} and consistency regularization loss via Eq. \ref{equ:consistency}.
\STATE Update the parameters $\Theta$ by gradients descending:
$$\nabla_\Theta \mathcal{L}_{sup} + \lambda \mathcal{L}_{con}$$
\ENDWHILE
\STATE Output prediction $\mathbf{Z}$ via Eq. \ref{equ:inference}.
\end{algorithmic}
\end{algorithm}

 
\vpara{$S-$augmentation.} Inspired by the above observation, we propose to generate $S$ different data augmentations for the input graph data $\mathbf{X}$. 
In specific, we perform the random propagation operation for $S$ times to generate $S$ augmented feature matrices $\{\overline{\mathbf{X}}^{(s)}|1\leq s \leq S\}$. 
Each of these augmented feature matrices is fed into the MLP prediction module to get the corresponding output:

\begin{equation}
 \small
     \widetilde{\mathbf{Z}}^{(s)} = P(\mathbf{Y}|\overline{\mathbf{X}}^{(s)}; \Theta), 
 \end{equation}
where $\widetilde{\mathbf{Z}}^{(s)} \in [0,1]^{n\times C}$ denotes the classification probabilities on the $s$-th augmented data $\overline{\mathbf{X}}^{(s)}$. 


\vpara{Classification Loss.}
With $m$ labeled nodes among $n$ nodes, the supervised objective of the graph node classification task in each epoch is the average cross-entropy loss over $S$ augmentations:
\begin{equation}
\small
\label{equ:loss}
	\mathcal{L}_{sup} = -\frac{1}{S}\sum_{s=1}^{S}\sum_{i=0}^{m-1}\mathbf{Y}_{i} \cdot \log \widetilde{\mathbf{Z}}_{i}^{(s)} ,
\end{equation}

\noindent where $\widetilde{\mathbf{Z}}_i^{(s)}$ is the $i$-th row vector of  $\widetilde{\mathbf{Z}}^{(s)}$. Optimizing this loss enforces the model to output the same predictions for (only) labeled nodes on different augmentations.
However, labeled data is often very rare in the semi-supervised setting, in which we would like to also make full use of unlabeled data. 





\vpara{Consistency Regularization Loss.} 
In the semi-supervised setting, we propose to optimize the consistency among $S$ augmentations for unlabeled data. Considering a simple case of $S=2$, we can
minimize the distributional distance between the two outputs, i.e.,
$
\min \sum_{i=0}^{n-1} \mathcal{D}(\widetilde{\mathbf{Z}}^{(1)}_i, \widetilde{\mathbf{Z}}^{(2)}_i)$,
where $ \mathcal{D}(\cdot,\cdot)$ is the distance function. 
To extend this idea into multiple-augmentation situation, we first 
calculate the label distribution center by taking the average of all distributions, i.e., 
 $ \overline{\mathbf{Z}}_i = \frac{1}{S}\sum_{s=1}^{S} \widetilde{\mathbf{Z}}_i^{(s)}$.
Then we minimize the distributional distance between $\widetilde{\mathbf{Z}}_i^{(s)}$ and $\overline{\mathbf{Z}}_i$, i.e., $\min \sum_{s=1}^{S}\sum_{i=0}^{n-1} \mathcal{D}(\overline{\mathbf{Z}}_i, \widetilde{\mathbf{Z}}^{(s)}_i)$.
\hide{
\begin{equation}
    \min \sum_{s=1}^{S}\sum_{i=1}^n \mathcal{D}(\overline{\mathbf{Z}}_i, \widetilde{\mathbf{Z}}^{(s)}_i).
\end{equation}
}
However, the distribution center calculated in this way is always inclined to have higher entropy values, indicating greater ``uncertainty''. 
Consequently, it will bring extra uncertainty into the model's predictions. 
To avoid this problem, we utilize the \textit{label sharpening} trick here.
Specifically, we apply a sharpening function onto the averaged label distribution to reduce its entropy~\cite{berthelot2019mixmatch}, i.e.,
\begin{equation}
\small
\label{equ:sharpen}
\overline{\mathbf{Z}}^{'}_{ik} = \overline{\mathbf{Z}}_{ik}^{\frac{1}{T}} ~\bigg/~\sum_{j=0}^{C-1}\overline{\mathbf{Z}}_{ij}^{\frac{1}{T}},
\end{equation}
where $0< T\leq 1$ acts as the ``temperature'' that controls the sharpness of the categorical distribution. 
As $T \to 0$, the sharpened label distribution will approach a one-hot distribution. 
To substitute $\overline{\mathbf{Z}}_i$ with $\overline{\mathbf{Z}}^{'}_i$, we minimize the distance between  $\widetilde{\mathbf{Z}}_i$ and $\overline{\mathbf{Z}}^{'}_i$ in \model:
\begin{equation}
\small
\label{equ:consistency}
    \mathcal{L}_{con} =   \frac{1}{S}\sum_{s=1}^{S}\sum_{i=0}^{n-1} \mathcal{D}(\overline{\mathbf{Z}}^{'}_i, \widetilde{\mathbf{Z}}^{(s)}_i).
\end{equation}


Therefore, by setting $T$ as a small value, we can enforce the model to output low-entropy predictions. 
This can be viewed as adding an extra entropy minimization regularization into the model, which assumes that the classifier's decision boundary should not pass through high-density regions of the marginal data distribution~\cite{grandvalet2005semi}. 

As for the distance function $\mathcal{D}(\cdot, \cdot)$, we adopt the squared $L_2$ loss $\mathcal{D}(\mathbf{a}, \mathbf{b})=\|\mathbf{a}-\mathbf{b}\|^2$ in our model. 
Recent studies have demonstrated that it is less sensitive to incorrect predictions~\cite{berthelot2019mixmatch} and thus is more suitable for the semi-supervised setting than cross-entropy. 



\vpara{Semi-supervised Training and Inference.}
In each epoch, we employ both the supervised classification loss in Eq. \ref{equ:loss} and the consistency regularization loss in Eq. \ref{equ:consistency} on $S$ augmentations. 
Hence, the final loss of \model\ is:
\begin{equation}
\small
\label{equ:inf}
	\mathcal{L} = \mathcal{L}_{sup} + \lambda \mathcal{L}_{con},
\end{equation}
where $\lambda$ is a hyper-parameter that controls the balance between the supervised classification and consistency regularization losses.


During inference, as mentioned in Section \ref{sec:randpro}, we directly use the original feature $\mathbf{X}$ without DropNode for propagation. 
This is justified because we scaled  the perturbed feature matrix $\widetilde{\mathbf{X}}$ during training to guarantee its expectation to match $\mathbf{X}$. 
Hence the inference formula is:
\begin{equation}
\small
\label{equ:inference}
\mathbf{Z}= P\left(\mathbf{Y}~\bigg|~\frac{1}{K+1}\sum_{k=0}^K\hat{\mathbf{A}}^k \mathbf{X};\hat{\Theta}\right),
\end{equation}
where $\hat{\Theta}$ denotes the optimized parameters after training. 
Algorithm \ref{alg:2} outlines \model's training process.  


\subsection{Complexity Analysis} 
\model\ comprises of random propagation and consistency regularized training. 
For random propagation, we compute $\overline{\mathbf{X}}$ by iteratively calculating the product of $\hat{\mathbf{A}}^k$ and $\widetilde{\mathbf{X}}$, and its time complexity is $\mathcal{O}(Kd(n+|E|))$. 
The complexity of its prediction module (2-layer MLP) is $\mathcal{O}(nd_h(d+ C))$, where $d_h$ denotes its hidden size. 
By applying consistency regularized training, the total computational complexity of \model\ is $\mathcal{O}(S(Kd(n + |E|)+ nd_h(d + C))$, \textbf{which is linear with the sum of node and edge counts.
}


}









































































\hide{



\section{Graph Random Networks}
\hide{
\begin{figure*}
  		\centering
  		\includegraphics[width=0.8 \linewidth,height =  0.25\linewidth]{grand2.pdf}
 	\caption{Diagram of the training process of \model.  
 	As a general semi-supervised learning framework on graphs, \model\ provides two mechanisms---random propagation and consistency regularization----to enhance the prediction model's robustness and generalization. In each epoch, given the input graph, \model\ first generates $S$ graph data augmentations $\{\widetilde{X}^{(s)}| 1\leq s \leq S\}$ via random propagation layer. Then, each $\widetilde{X}^{(s)}$ is fed into a prediction model, leading to the corresponding prediction distribution $\widetilde{Z}^{(s)}$. In optimization, except for optimizing the supervised classification loss $\mathcal{L}^l$ with the given labels $Y^L$, we also minimize the prediction distance among different augmentations of unlabeled nodes via unsupervised consistency loss $\mathcal{L}^u$. }
\label{fig:model}
\end{figure*}
}
To achieve a better model for semi-supervised learning on graphs, we propose Graph Random Networks (\model). In \model , different with other GNNs, each node is allowed to randomly interact with different subsets of neighborhoods in different training epochs. This random propagation mechanism is shown to be an economic way for stochastic graph data augmentation. Based on that, we also design a consistency regularized training method to improve model's generalization capacity by encouraging predictions invariant to different augmentations of the same node. 

\hide{
Graph is complex with highly tangled nodes and edges, while previous graph models, e.g., GCN and GAT, take the node neighborhood as a whole and follow a determined aggregation process recursively. These determined models, where nodes and edges interact with each other in a fixed way, suffer from overfitting and risk of being misguiding by small amount of potential noise, mistakes, and malevolent attacks. To address it, we explore graph-structured data in a random way. In the proposed framework, we train the graph model on massive augmented graph data, generated by random sampling and propagation. In different training epochs, nodes interact with different random subsets of graph, and thus mitigate the risk of being misguiding by specific nodes and edges. On the other hand, the random data augmentation alleviates the overfitting and the implicit ensemble style behind the process improves the capacity of the representation. As the model should generalize well and have a similar prediction on unlabeled data on different data augmentations, despite the potential divergence from sampling randomness, we can further introduce an unsupervised graph-based regularization to the framework.
}

\hide{
In a graph neural model, if any given node's  representation (including unlabeled nodes') has a sufficient and consistent performance in any random subgraphs, that is, any node representation disentangles with specific nodes or edge links, which can be swapped by others, the graph neural model can effectively alleviate the  overfitting to specific graph structure and being misguided by noise, mistakes, and malevolent attacks. One the other hand, the graph neural model trained on exponentially many random subgraphs will also explore graph structures in different levels as sufficiently as possible, while previous models, e.g., GCN and GAT, model the neighborhood as a whole. Based on it, the proposed model, \model, utilizes a series of random sampling strategies to explore graph structures. We found that sampling strategy coupled with graph propagation is an efficient way of graph data augmentation. By leveraging the consistency of abundant unlabeled data on different random augmentations, we can further lower the generalization loss.
}


\hide{
\subsection{Over-smoothing Problem }
The over-smoothing issue of GCNs was first studied in \cite{li2018deeper}, which indicates that node features will converge to a fixed vector as the network depth increases. This undesired convergence heavily restricts the expressive power of deep GCNs. Formally speaking, suppose $G$ has $P$ connected components $\{C_i\}^{P}_{i=1}$. Let $\mathbbm{1}^{(i)} \in \mathbb{R}^n$ denote the indication vectors for the $i$-th component $C_i$, which indicates whether a node belongs to $C_i$  i.e.,
	\begin{equation*}
\mathbbm{1}_j^{(i)} = \left\{ \begin{aligned}
1, &v_j \in C_i \\
0, &v_j \notin C_i  \end{aligned}\right..
\end{equation*}
 The over-smoothing phenomenon of GCNs is formulated as the following theorem:
\begin{theorem}
	Given a graph $G$ which has $P$ connected components $\{C_i\}^{P}_{i=1}$, for any $\mathbf{x} \in \mathbb{R}^n$, we have:
	\begin{equation*}
		\lim_{k\rightarrow +\infty} \hat{A}^k x = \widetilde{D}^{\frac{1}{2}}[\mathbbm{1}^{(1)}, \mathbbm{1}^{(2)}, ..., \mathbbm{1}^{(P)}] \hat{x}
	\end{equation*}
	where $\hat{x} \in \mathbb{R}^P$ is a vector associated with $x$.  

\end{theorem}
From this theorem, we could notice that after repeatedly performing the propagation rule of GCNs many times, node features will converge to a linear combination of $\{D^{\frac{1}{2}}\mathbbm{1}^{(i)}\}$. And the nodes in the same connected component only distinct by their degrees. In the above analyses, the activation function used in GCNs is assumed to be a linear transformation, but the conclusion can also be generalized to non-linear case\cite{oono2019graph}.
}

\begin{figure*}
    \centering
    \includegraphics[width= \linewidth]{grand6.pdf}
    \caption{Architecture of \model.}
    \label{fig:arch1}
\end{figure*}

\subsection{Architecture}
The architecture of \model\ is illustrated in Figure~\ref{fig:arch1}. Overall, the model includes two components, i.e., Random propagation module and classification module.


\subsubsection{Random Propagation Module.}
\label{sec:randpro}
\begin{figure}
    \centering
    \includegraphics[width=0.8
    \linewidth]{dropnode_vs_dropout2.pdf}
    \caption{Difference between DropNode and dropout. Dropout drops  elements of $X$ independently. While dropnode drops feature vectors of nodes (row vectors of $X$) randomly.}
    \label{fig:dropnode_vs_dropout}
\end{figure}



\begin{algorithm}[tb]
\caption{Dropnode}
\label{alg:dropnode}
\begin{algorithmic}[1] 
\REQUIRE ~~\\
Original feature matrix $X \in R^{n \times d}$, DropNode probability 
$\delta \in (0,1)$. \\
\ENSURE ~~\\
Perturbed feature matrix  $X\in R^{n \times d}$.
\IF{mode == Inference}
\STATE $\widetilde{X} = X$.
\ELSE
\STATE Randomly sample $n$ masks: $\{\epsilon_i \sim Bernoulli(1-\delta)\}_{i=1}^n$.
\STATE Obtain deformity feature matrix by  multiplying each node's feature vector with the corresponding  mask: $\widetilde{X}_{i} = \epsilon_i \cdot X_{i} $.
\STATE Scale the deformity features: $\widetilde{X} = \frac{\widetilde{X}}{1-\delta}$.
\ENDIF
\end{algorithmic}
\end{algorithm}

\label{sec:randpro}


In random propagation, we aim to perform message passing in a random way during model training. To achieve that, we add an extra node sampling operation called ``DropNode'' in front of the propagation layer.


\vpara{DropNode.} In DropNode, feature vector of each node is randomly removed (rows of $X$ are randomly set to $\vec{0}$) with a pre-defined probability $\delta \in (0,1)$ at each training epoch. The resultant perturbed feature matrix $\widetilde{X}$ is fed into the propagation layer later on.
More formally, we first randomly sample a binary mask $\epsilon_i \sim Bernoulli(1-\delta)$ for each node $v_i$, and obtain the perturbed feature matrix $\widetilde{X}$ by multiplying each node's feature vector with the corresponding mask, i.e., $\widetilde{X}_i=\epsilon_i \cdot X_i$. Furthermore, we scale $\widetilde{X}$ with a factor of $\frac{1}{1-\delta}$ to guarantee the perturbed feature matrix is equal to $X$ in expectation. Please note that the sampling procedure is only performed during training. In inference time, we directly let $\widetilde{X}$ equal to the original feature matrix $X$. The algorithm of DropNode is shown in Algorithm \ref{alg:dropnode}. 

DropNode is similar to dropout\cite{srivastava2014dropout}, a more general regularization method used in deep learning to prevent overfitting. 
In dropout, the elements of $X$ are randomly dropped out independently. While DropNode drops a node's whole features together, serving as a node sampling method. 
Figure \ref{fig:dropnode_vs_dropout} illustrates their differences.
Recent study suggests that a more structured form of dropout, e.g., dropping a contiguous region in DropBlock\cite{ghiasi2018dropblock}, is required for better regularizing CNNs on images. From this point of view, dropnode can be seen as a structured form of dropout on graph data.
We also demonstrate that dropnode is more suitable for graph data than dropout both theoretically (Cf. Section \ref{sec:theory}) and experimentally (Cf. Section \ref{sec:ablation}). 




After dropnode, the perturbed feature matrix $\widetilde{X}$ is fed into a propagation layer to perform message passing. Here we adopt mixed-order propagation, i.e.,

\begin{equation}
\label{equ:kAX}
	\overline{X} = \overline{A} \widetilde{X}.
\end{equation}
Here we define the propagation matrix as $\overline{A} =  \frac{1}{K+1}\sum_{k=0}^K\hat{A}^k$, that is, the average of the power series of $\hat{A}$ from order 0 to order $K$. This kind of propagation rule enables model to incorporate multi-order neighborhoods information, and have a lower risk of over-smoothing compared with using $\hat{A}^K$ when $K$ is large. Similar ideas have been adopted in previous works~\cite{abu2019mixhop,abu2018n}. We compute the Equation \ref{equ:kAX} by iteratively calculating the product of sparse  matrix $\hat{A}^k$ and $\widetilde{X}$. The corresponding time complexity is $\mathcal{O}(Kd(n+|E|))$.


\hide{
In other words, the original features of about $\delta |V|$ nodes are removed (set as $\vec{0}$).
Obviously, this random sampling strategy may destroy the information carried in nodes and the resultant corrupted feature matrix is insufficient for prediction. To compensate it, we try to recover the information in a graph signal propagation process, and get the recovery feature matrix $\widetilde{X}$. The propagation recovery process is:
\begin{equation}
\label{equ:kAX}
	\widetilde{X} = \frac{1}{K+1}\sum_k^K\hat{A}^k X^{'}.
\end{equation}

It also offers a general way for implicit model ensemble on exponentially many augmented data.
}
\hide{
In each epoch, given the input graph, \model\ first generates $S$ graph data augmentations $\{\widetilde{X}^{(s)}| 1\leq s \leq S\}$ via random propagation layer. Then, each $\widetilde{X}^{(s)}$ is fed into a prediction model, leading to the corresponding prediction distribution $\widetilde{Z}^{(s)}$. In optimization, except for optimizing the supervised classification loss $\mathcal{L}^l$ with the given labels $Y^L$, we also minimize the prediction distance among different augmentations of unlabeled nodes via unsupervised consistency loss $\mathcal{L}^u$.}







\hide{
\subsection{Motivation}
Our motivation is that GCNs can not sufficiently leverage unlabeled data in this task. Specifically,


\subsection{Overview of Graph Random Network}
    In this section, we provide a brief introduction of the proposed graph random network. The basic idea of \model is to promote GCNs' generalization and robustness by taking the full advantage of unlabeled data in the graph.


Random propagation layer consists of graph dropout and propagation.
We first introduce how to generate a subgraph by random sampling nodes.
Formally, we randomly sample the nodes without replacement at a probability $1-\delta$  and drop the rest.
The deformity feature matrix $X^{'}$ is formed in the following way,
\begin{align}
\label{equ:samplingX1}
\left\{
\begin{aligned}
	& Pr(X^{'}_{i}=\vec{0}) = \delta, \\&Pr(X^{'}_{i}=X_i) = 1-\delta. \end{aligned}
\right.
\end{align}

In other words, the original features of about $\delta |V|$ nodes are removed (set as $\vec{0}$).
Obviously, this random sampling strategy may destroy the information carried in nodes and the resultant corrupted feature matrix is insufficient for prediction. To compensate it, we try to recover the information in a graph signal propagation process, and get the recovery feature matrix $\widetilde{X}$. The propagation recovery process is:
\begin{equation}
\label{equ:kAX}
	\widetilde{X} = \frac{1}{K}\sum_k^K\hat{A}^k X^{'}.
\end{equation}

\hide{
\begin{equation}
\label{Equ:denoise}
\widetilde{X} = \arg\min_{\widetilde{X}} \frac{1}{2}\left\| \widetilde{X}- X^{'}\right\|^2_2 + \alpha \frac{1}{2}\left \| \widetilde{X} - D^{-\frac{1}{2}}AD^{-\frac{1}{2}} \widetilde{X}\right\|^2_2
\end{equation}  
\begin{small}
\begin{equation}
\label{Equ:denoise}
\widetilde{X}=\arg\min_{\widetilde{X}} 
   \frac{1}{2}\left(\sum_{i, j=1}^{n} A_{i j}\left\|\frac{1}{\sqrt{D_{i i}}} \widetilde{X}_{i}-\frac{1}{\sqrt{D_{j j}}} \widetilde{X}_{j}\right\|^{2}+\mu \sum_{i=1}^{n}\left\|\widetilde{X}_{i}-X^{'}_{i}\right\|^{2}\right)
\end{equation}  
\end{small}
where the first term keeps the denoised signal similar to the measurement and the second term enforces the smoothing of the solution. By setting the derivate of $\widetilde{X}$ to zero, we can obtain the solution to the Equation \ref{Equ:denoise}:
\begin{equation}
\label{Equ:denoise2}
\widetilde{X} = (I+\alpha \tilde{L})^{-1}X^{'}
\end{equation}

To avoid the inversion operation in Equation \ref{Equ:denoise2}, we derive an approximate solution of Equation \ref{Equ:denoise2} using Taylor Extension:
\begin{equation}
\label{Equ:denoise3}
\widetilde{X} = (I + \alpha \tilde{L}^2)^{-1}X^{'} \approx \sum_{i=0}^K (-\alpha\tilde{L}^2)^i X^{'} 
\end{equation}
}

Combining Equation \ref{equ:samplingX1} and \ref{equ:kAX}, the original feature matrix $X$ is firstly \hide{heavily} corrupted by sampling and then smoothed by graph propagation. In fact, the whole procedure consisting of two opposite operations provides a new representation $\widetilde{X}$ for the original $X$. Later we will verify that $\widetilde{X}$ is still a good representation substitute with a sufficient prediction ability, while the randomness inside the process helps the model avoid the over-dependence on specific graph structure and 
explore different graph structure in a more robust way.
We treat these opposite operations as a whole procedure and call them \textit{random propagation layer} in the proposed model \model. 
\hide{After filtered by random sampling operation in random propagation layer, the subsequent model parts only involve interaction among nearly half of the original node features and thus graph-structure data are disentangled somehow.}

Here, we generalize the random propagation layer with another sampling strategies. Instead of removing the feature of a node entirely by sampling once, we drop the element of the feature in each dimension one by one by multiple sampling. This is a multi-channel version of node sampling. Furthermore, we scale the remaining elements with a factor of $\frac{1}{1-\delta}$ to 
 guarantees the deformity feature matrix or adjacency matrix are the same as the original in expectation. Similar idea of dropping and rescaling has been also used in Dropout~\cite{srivastava2014dropout}. Hence we called our three graph sampling strategy graph dropout (\textit{dropnode} and \textit{dropout}), and formulate them following:
 
\vpara{Dropnode.} In dropnode, the feature vector of each node is randomly dropped with a pre-defined probability $\delta \in (0,1)$ during the propagation. More formally, we first form a deformity feature matrix $X^{'}$ in the following way:
\begin{align}
\label{equ:nodedropout}
\left\{
\begin{aligned}
& Pr(X^{'}_{i}=\vec{0}) = \delta,& \\
&Pr(X^{'}_{i}= \frac{X_i}{1-\delta} ) = 1- \delta. &
\end{aligned}
\right.
\end{align}
Then let $X^{'}$ propagate in the graph via Equation~\ref{equ:kAX}. 

\vpara{Dropout.} In dropout, the element of the feature matrix is randomly dropped with a probability $\delta \in (0,1)$ during the propagation. Formally, we have:
\begin{align}
\label{equ:featuredropout}
\left\{
\begin{aligned}
& Pr(X^{'}_{ij}=0) = \delta,& \\
&Pr(X^{'}_{ij}= \frac{X_{ij}}{1-\delta} ) = 1- \delta. &
\end{aligned}
\right.
\end{align}
Then we  propagate $X^{'}$ in the graph via Equation~\ref{equ:kAX}. 



Note that dropout in graph dropout shares the same definition of Dropout~\cite{srivastava2014dropout}, a more general method widely used in deep learning to prevent parameters from overfitting. However, dropout in graph dropout, as a multi-channel version of dropnode, is mainly developed to explore graph-structured data in the semi-supervised learning framework. The graph dropout strategy, directly applied to graph objects, is also an efficient way of graph data augmentation and  model ensemble on exponentially many subgraphs. As for the common dropout applied to the input units, the optimal probability of drop is usually closer to 0 to prevent the loss of information~\cite{srivastava2014dropout}, which is not the case in our graph dropout. In this paper we will further analyze theoretically that graph dropout help \model\ leverage unlabeled data, and dropnode and dropout play different roles. 
}


\hide{
Note that dropout is applied to neural network activation to prevent overfitting of parameters while our node dropout and edge dropout are coupled with graph propagation, performing on graph objects without parameters. We will reveal that our graph dropout strategy is an efficient way of graph data augmentation and graph models ensemble on exponentially many subgraphs. We will further analyse theoretically that graph dropout help \model\ leverage unlabeled data. \reminder{}
}

\vpara{Stochastic Graph Data Augmentation.}
Feature propagation have been proven to be an effective method for enhancing node representation by aggregating information from neighborhoods, and becomes the key step of Weisfeiler-Lehman Isomorphism test~\cite{shervashidze2011weisfeiler} and GNNs. In random propagation, some nodes are first randomly dropped before propagation. That is, each node only aggregates information from a subset of multi-hop neighborhoods randomly. In this way, random propagation module stochastically generates different representations for a node, suggesting an efficient stochastic augmentation method for graph data. From another point of view, random propagation can also be seen as injecting random noise to the propagation procedure by dropping a portion of nodes . 

We also conduct an interesting experiment to examine the validation of this data augmentation method. In this setting, we first sample a set of augmented node representations $\overline{X}$ from random propagation module with different drop rates, then use $\overline{X}$ to train a GCN for node classification. 
By examining the classification accuracy on $\overline{X}$, we can empirically analyze the influence of the injected noise on model performance. The results with different drop rates have been shown in Figure \ref{fig:redundancy}. It can be seen that the performance only decreases by less than $3\%$ as the drop rate increases from $0$ to $0.5$, indicating the augmented node representations is still sufficient for prediction\footnote{In practice, the drop rate is always set to $0.5$}. 
 Though $\overline{X}$  is still inferior to $X$ in performance, in practice, multiple augmentations will be utilized for training prediction model since the sampling will be performed in every epoch. From the view of bagging~\cite{breiman1996bagging}, this random data augmentation scheme makes the final prediction model implicitly ensemble models on exponentially many augmentations, yielding much better performances than using deterministic propagation.
 
  


\begin{figure}
  		\centering
  		\includegraphics[width = 0.8 \linewidth, height =  0.58\linewidth]{drop_rate.pdf}
  	\caption{Classification results of GCN with $\overline{X}$ as input.} 
  	\label{fig:redundancy}
\end{figure}
  






\subsubsection{Prediction Module.}
In prediction module, the augmented feature matrix $\overline{X}$ is fed into a neural network to predict nodes labels. We employ a two-layer MLP as the classifier:
\begin{equation}
\label{equ:mlp}
    P(\mathcal{Y}|\overline{X};\Theta) = \sigma_2(\sigma_1(\overline{X}W^{(1)})W^{(2)})
\end{equation}
where $\sigma_1$ is ReLU function, $\sigma_2$ is softmax function, $\Theta=\{W^{(1)}, W^{(2)}\}$ refers to model parameters. Here the classification model can also adopt more complex GNN based node classification models, e.g., GCN, GAT.  But we find the performance decreases when we replace MLP with GNNs because of the over-smoothing problem. We will explain this phenomenon in Section \ref{sec:oversmoothing}.






\hide{
From another perspective, Equation \ref{equ:samplingX1} and \ref{equ:kAX} in the random propagation layer perform a data augmentation procedure by linear interpolation~\cite{devries2017dataset} in a random subset of the multi-hop neighborhood.
}


 


\hide{
In a graph, we random sample a fixed proportion of neighborhoods for each node $v_i \in \mathcal{V}$, and let $v_i$ only interact with the sampled neighborhoods in propagation. Using this method, we are equivalent to let each node randomly perform linear interpolation with its neighbors. 

However, generating neighborhood samples for each node always require a time-consuming preprocessing. For example, the sampling method used in GraphSAGE~\cite{hamilton2017inductive} requires $k \times n$ times of sampling operations, where $k$ denotes the size of sampled neighbor set. To solve this problem, here we propose an efficient sampling method to perform random propagation --- graph dropout. Graph dropout is inspired from Dropout\cite{srivastava2014dropout}, a widely used regularization method in deep learning. \reminder{graph dropout is an efficient sampling method}
In graph dropout, we randomly drop a set of nodes or edges in propagation, which is introduced separately. \\

\vpara{Edge Dropout.} The basic idea of edge dropout is randomly dropping a fix proportion of edges during each propagation. 
Specifically, we construct a deformity feature matrix $\hat{A}^{'}$ following:
\begin{align}
\label{equ:nodedropout}
\left\{
\begin{aligned}
& Pr(\hat{A}^{'}_{ij}=\vec{0}) = \delta.& \\
&Pr(\hat{A}^{'}_{ij}= \frac{\hat{A}^{'}_{ij}}{1-\delta} ) = 1- \delta. &
\end{aligned}
\right.
\end{align}
Then we use $\hat{A}^{'}$ as the replacement of $\hat{A}$ in propagation.  


\vpara{Node Dropout.} In node dropout, the feature vector of each node is randomly dropped with a pre-defined probability $\delta \in (0,1)$ during propagation. More formally, we first form a deformity feature matrix $X^{'}$ in the following way:
\begin{align}
\label{equ:nodedropout}
\left\{
\begin{aligned}
& Pr(X^{'}_{i}=\vec{0}) = \delta.& \\
&Pr(X^{'}_{i}= \frac{X_i}{1-\delta} ) = 1- \delta. &
\end{aligned}
\right.
\end{align}
Then let $X^{'}$ propagate in graph as the substitute of $X$, i.e., $\tilde{X} = \hat{A}X$. 

Actually, node dropout can be seen as a special form of edge dropout:
}



\subsection{Consistency Regularized Training}
\label{sec:consis}
\begin{figure*}
	\centering
	\includegraphics[width= \linewidth]{grand_consis.pdf}
	\caption{Illustration of consistency regularized training for \model. \yd{how about making the random progagation module into two as well?}}
	\label{fig:arch2}
\end{figure*}
As mentioned in previous subsection, random propagation module can be seen as an efficient method of stochastic data augmentation. That inspires us to design a consistency regularized training algorithm for optimizing parameters. 
  In the training algorithm, we generate multiple data augmentations at each epoch by performing dropnode multiple times. Besides the supervised classification loss, we also add a consistency regularization loss to enforce model to give similar predictions across different augmentations. This algorithm is illustrated in Figure \ref{fig:arch2}.


\begin{algorithm}[tb]
\caption{Consistency Regularized Training for \model}
\label{alg:2}
\begin{algorithmic}[1] \REQUIRE ~~\\
 Adjacency matrix $\hat{A}$,
labeled node set $V^L$,
 unlabeled node set $V^U$,
feature matrix $X \in R^{n \times d}$, 
times of dropnode in each epoch $S$, dropnode probability $\delta$.\\
\ENSURE ~~\\
Prediction $Z$.
\WHILE{not convergence}
\FOR{$s=1:S$} 
\STATE Apply dropnode via Algorithm \ref{alg:dropnode}: $
\widetilde{X}^{(s)} \sim \text{dropnode}(X,\delta)$. 
\STATE Perform propagation: $\overline{X}^{(s)} = \frac{1}{K}\sum_{k=0}^K\hat{A}^k \widetilde{X}^{(s)}$.
\STATE Predict class distribution using MLP: $\widetilde{Z}^{(s)} = P(\mathcal{Y}|\overline{X}^{(s)};\Theta)$.
\ENDFOR
\STATE Compute supervised classification loss $\mathcal{L}_{sup}$ via Equation \ref{equ:loss} and consistency regularization loss via Equation \ref{equ:consistency}.
\STATE Update the parameters $\Theta$ by gradients descending:
$$\nabla_\Theta \mathcal{L}_{sup} + \lambda \mathcal{L}_{con}$$
\ENDWHILE
\STATE Output prediction $Z$ via Equation \ref{equ:inference}
\end{algorithmic}
\end{algorithm}

 
\subsubsection{$S-$augmentation Prediction}
At each epoch, we aim to generate $S$ different data augmentations for graph data $X$. To achieve that, we adopt $S-$sampling dropnode strategy in random propagation module. That is, we performing $S$ times of random sampling in dropnode to generate $S$ perturbed feature matrices $\{\widetilde{X}^{(s)}|1\leq s \leq S\}$. Then we propagate these features according to Equation \ref{equ:kAX} respectively, and hence obtain $S$ data augmentations $\{\overline{X}^{(s)}|1 \leq s \leq S\}$. Each of these augmented feature matrice are fed into the prediction module to get the corresponding output:

\begin{equation}
     \widetilde{Z}^{(s)} = P(\mathcal{Y}|\overline{X}^{(s)}; \Theta).
 \end{equation}
Where $\widetilde{Z}^{(s)} \in (0,1)^{n\times C}$ denotes the classification probabilities on the $s$-th augmented data. 
\subsubsection{Supervised Classification Loss.}
The supervised objective of graph node classification in an epoch is the averaged cross-entropy loss over $S$ times sampling:
\begin{equation}
\label{equ:loss}
	\mathcal{L}_{sup} = -\frac{1}{S}\sum_{s=1}^{S}\sum_{i=1}^m \sum_{j=1}^{C}Y_{i,j} \ln \widetilde{Z}_{i,j}^{(s)} ,
\end{equation}

\noindent where $Y_{i,l}$ is binary, indicating whether node $i$ has the label $l$. By optimizing this loss, we can also enforce the model to output the same predictions on multiple augmentations of a labeled node.
 However, in the semi-supervised setting, labeled data is always very rare. In order to make full use of unlabeled data, we also employ consistency regularization loss in \model. 
 
\subsubsection{Consistency Regularization Loss.} How to optimize the consistency among $S$ augmentations of unlabeled data? 
 Let's first consider a simple case, where the random propagation procedure is performed only twice in each epoch, i.e., $S=2$. Then a straightforward method is to minimize the distributional distance between two outputs:
 \begin{equation}
 \label{equ:2d}
     \min \sum_{i=1}^n \mathcal{D}(\widetilde{Z}^{(1)}_i, \widetilde{Z}^{(2)}_i),
 \end{equation}
where $ \mathcal{D}(\cdot,\cdot)$ is the distance function. Then we extend it into multiple augmentation situation. We can first calculate the label distribution center by taking average of all distributions:
\begin{equation}
    \overline{Z} = \frac{1}{S}\sum_{s=1}^{S} \widetilde{Z}^{(s)}.
\end{equation}

And we can minimize the distributional distance between $\widetilde{Z}^{(s)}$ and $\overline{Z}$, i.e., $\min \sum_{s=1}^{S}\sum_{i=1}^n \mathcal{D}(\overline{Z}_i, \widetilde{Z}^{(s)}_i)$.
\hide{
\begin{equation}
    \min \sum_{s=1}^{S}\sum_{i=1}^n \mathcal{D}(\overline{Z}_i, \widetilde{Z}^{(s)}_i).
\end{equation}
}
However, the distribution center calculated in this way is always inclined to have more entropy value, which indicates to be more ``uncertain''. 
Thus this method will bring extra uncertainty into model's predictions. To avoid this problem, We utilize the label sharpening trick in \model. Specifically, we apply a sharpening function onto the averaged label distribution to reduce its entropy:
\begin{equation}
 \overline{Z}^{'}_{i,l} = \frac{\overline{Z}_{i,l}^{\frac{1}{T}}}{\sum_{j=1}^{|\mathcal{Y}|}\overline{Z}_{i,j}^{\frac{1}{T}}},
\end{equation}
where $0< T\leq 1$ acts as the ``temperature'' which controls the sharpness of the categorical distribution. As $T \to 0$, the sharpened label distribution will approach a one-hot distribution. As the substitute of $\overline{Z}$, we minimize the distance between  $\widetilde{Z}^i$ and $\overline{Z}^{'}$ in \model:
\begin{equation}
\label{equ:consistency}
    \mathcal{L}_{con} =   \frac{1}{S}\sum_{s=1}^{S}\sum_{i=1}^n \mathcal{D}(\overline{Z}^{'}_i, \widetilde{Z}^{(s)}_i).
\end{equation}

By setting $T$ as a small value, we could enforce the model to output low-entroy predictions. This can be seen as adding an extra entropy minimization regularization into the model, which assumes that classifier's decision boundary should not pass through high-density regions of the marginal data distribution\cite{grandvalet2005semi}. As for the  distance function $\mathcal{D}(\cdot, \cdot)$, we adopt squared $L_2$ loss, i.e., $\mathcal{D}(x, y)=\|x-y\|^2$, in our model. It has been proved to be less sensitive to incorrect predictions\cite{berthelot2019mixmatch}, and is more suitable to the semi-supervised setting compared to cross-entropy. 



\subsubsection{Training and Inference}
In each epoch, we employ both supervised classification loss (Cf. Equation \ref{equ:loss}) and consistency regularization loss (Cf. Equation \ref{equ:consistency}) on $S$ times of sampling. Hence, the final loss is:
\begin{equation}
\label{equ:inf}
	\mathcal{L} = \mathcal{L}_{sup} + \lambda \mathcal{L}_{con}.
\end{equation}
Here $\lambda$ is a hyper-parameter which controls the balance between supervised classification loss and consistency regularization loss.
In the inference phase, as mentioned in Section \ref{sec:randpro}, we directly use original feature $X$ as the output of dropnode instead of sampling. Hence the inference formula is:
\begin{equation}
\label{equ:inference}
Z= P\left(\mathcal{Y}~\bigg|~\frac{1}{K+1}\sum_{k=0}^K\hat{A}^k X;\hat{\Theta}\right).
\end{equation}
Here $\hat{\Theta}$ denotes the optimized parameters after training. We summarize our algorithm in Algorithm \ref{alg:2}. 


\hide{Note that $X$ is equal to the expectation of $\widetilde{X}$s from data augmentation by multiple sampling. From the view of model bagging~\cite{breiman1996bagging}, the final graph model implicitly aggregates models trained on exponentially many subgraphs, and performs a plurality vote among these models in the inference phase, resulting in a lower generalization loss. }
 
\hide{
\begin{equation}
\label{equ:model}
	\widetilde{Z}^{(s)}=p_{\text{model}}\left(\mathcal{Y}~\bigg|~\frac{1}{K}\sum_k^K\hat{A}^k X^{'}\right) \in R^{n \times |\mathcal{Y}|}.
\end{equation}



Hence the supervised objective of graph node classification in an epoch is the averaged cross-entropy loss over $S$ times sampling:
\begin{equation}
\label{equ:loss}
	\mathcal{L}^l = -\frac{1}{S}\sum_{s}^{S}\sum_{i\in V^L } \sum_l^{|\mathcal{Y}|}Y_{i,l} \ln \widetilde{Z}_{i,l}^s ,
\end{equation}

\noindent where $Y_{i,l}$ is binary, indicating whether node $i$ has the label $l$. 


In each epoch, we employ both supervised cross-entropy loss (Cf. Eq.\ref{equ:loss}) and unsupervised consistency loss (Cf. Eq.\ref{equ:consistency}) on $S$ times of sampling. Hence, the final loss is:
\begin{equation}
\label{equ:inf}
	\mathcal{L} = \mathcal{L}^l + \lambda \mathcal{L}^u.
\end{equation}
Here $\lambda$ is a hyper-parameter which controls the balance between supervised loss and unsupervised consistency loss.
In the inference phase, the output is achieved by averaging  over the results on exponentially many augmented test data. This can be economically realized by inputting $X$ without sampling:
\begin{equation}
\label{equ:inference}
Z= p_{\text{model}}\left(\mathcal{Y}~\bigg|~\frac{1}{K}\sum_k^K\hat{A}^k X\right).
\end{equation}

Note that $X$ is the average of $X^{'}$s from data augmentation by multiple sampling. From the view of model bagging~\cite{breiman1996bagging}, the final graph model implicitly aggregates models trained on exponentially many subgraphs, and performs a plurality vote among these models in the inference phase, resulting in a lower generalization loss.  
}






\hide{
\subsection{graph propagation for smoothing}

In this section, we introduce the random propagation layer, an efficient method to perform stochastic data augmentation on the graph. We first demonstrate the propagation layer in GCNs is actually a special form of data augmentation on graph-structured data. Based on this discovery, we propose random propagation strategy, which augments each node with a part of randomized selected neighborhoods \reminder{}. Finally we show that this strategy can be efficiently instantiated with a stochastic sampling method --- graph dropout.

\subsection{Feature Propagation as Data Augmentation}
\label{sec:aug}
An elegant data augmentation technique used in supervised learning is linear interpolation \cite{devries2017dataset}. Specifically, for each example on training dataset, we first find its $K-$nearest neighboring samples in feature space which share the same class label. Then for each pair of neighboring feature vectors, we get the augmented example using interpolation:
\begin{equation}
    x^{'}_i = \lambda x_i + (1-\lambda)x_j
\end{equation}
where $x_j$ and $x_i$ are neighboring pairs with the same label $y$, $x^{'}_i$ is augmented feature vector, and $\lambda \in [0,1]$ controls degree of interpolation\reminder{}. Then the example $(x^{'}_i, y)$ is used as extra training sample to facilitate the learning task. In this way, the model is encouraged to more smooth in input space and more robust against small perturbations. \reminder{why?}


As for the problem of semi-supervised learning on graphs, we assume the graph signals are smooth, i.e., \textit{neighborhoods have similar feature vectors and similar class labels}. Thus a straightforward idea of augmenting graph-structured data is interpolating node features with one of its neighborhoods' features. However, in the real network, there exist small amounts of neighboring nodes with different labels and we can't identify that for samples with non-observable labels \reminder{?}. Thus simple interpretation with one neighbor might bring uncontrollable noise into the learning framework. 
An alternative solution is interpolating samples with multiple neighborhoods, which leads to Laplacian Smoothing:

\begin{equation}
\label{equ:lap}
x^{'}_i = (1-\lambda) x_i + \lambda \sum_j \frac{\widetilde{A}_{ij}}{\widetilde{D}_{ii}} x_j
\end{equation}.
Here we follow the definition of GCNs which adding a self-loop for each node in graph\reminder{}. Rewriting Eq.~\ref{equ:lap} in matrix form, we have:
\begin{equation}
\label{equ:lap2}
    X^{'}=  (I-\lambda I)X + \lambda \widetilde{D}^{-1} \widetilde{A}X
\end{equation}.
As pointed out by Li et.al.\cite{li2018deeper}\reminder{}, when $\lambda = 1$ and replacing $\Tilde{D}^{-1} \Tilde{A}$ with the symmetric normalized adjacency matrix $\hat{A}$, Eq.~\ref{equ:lap2} is identical to the propagation layer in GCN, i.e., $X^{'}= \hat{A}X$.

}

\hide{
\subsection{Random Propagation with Graph Dropout}
With the insight from last Section~\ref{sec:aug}, we develop a random propagation strategy for stochastic data augmentation on graph.

During each time of random propagation, we random sample a fixed proportion of neighborhoods for each node $v_i \in \mathcal{V}$, and let $v_i$ only interact with the sampled neighborhoods in propagation. Using this method, we are equivalent to let each node randomly perform linear interpolation with its neighbors. 

However, generating neighborhood samples for each node always require a time-consuming preprocessing. For example, the sampling method used in GraphSAGE~\cite{hamilton2017inductive} requires $k \times n$ times of sampling operations, where $k$ denotes the size of sampled neighbor set. To solve this problem, here we propose an efficient sampling method to perform random propagation --- graph dropout. Graph dropout is inspired from Dropout\cite{srivastava2014dropout}, a widely used regularization method in deep learning. \reminder{graph dropout is an efficient sampling method}
In graph dropout, we randomly drop a set of nodes or edges in propagation, which is introduced separately. \\

\vpara{Edge Dropout.} The basic idea of edge dropout is randomly dropping a fix proportion of edges during each propagation. 
Specifically, we construct a deformity feature matrix $\hat{A}^{'}$ following:
\begin{align}
\label{equ:nodedropout}
\left\{
\begin{aligned}
& Pr(\hat{A}^{'}_{ij}=\vec{0}) = \delta.& \\
&Pr(\hat{A}^{'}_{ij}= \frac{\hat{A}^{'}_{ij}}{1-\delta} ) = 1- \delta. &
\end{aligned}
\right.
\end{align}
Then we use $\hat{A}^{'}$ as the replacement of $\hat{A}$ in propagation.  


\vpara{Node Dropout.} In node dropout, the feature vector of each node is randomly dropped with a pre-defined probability $\delta \in (0,1)$ during propagation. More formally, we first form a deformity feature matrix $X^{'}$ in the following way:
\begin{align}
\label{equ:nodedropout}
\left\{
\begin{aligned}
& Pr(X^{'}_{i}=\vec{0}) = \delta.& \\
&Pr(X^{'}_{i}= \frac{X_i}{1-\delta} ) = 1- \delta. &
\end{aligned}
\right.
\end{align}
Then let $X^{'}$ propagate in graph as the substitute of $X$, i.e., $\tilde{X} = \hat{A}X$. 

Actually, node dropout can be seen as a special form of edge dropout: \reminder{Dropping a node is equivalent to drop all the edges start from the node.} \\

\reminder{connection between graph dropout and feature dropout. dropblock..}




}

\hide{
\section{Consistency Optimization}
\subsection{\drop: Exponential Ensemble of GCNs}
Benefiting from the information redundancy, the input of \shalf gives a relatively sufficient view for node classification.
It is straightforward to generalize \shalf to ensemble multiple \shalf s, as each set $\mathcal{C}$ provides a unique network view. In doing so, we can use $|\mathcal{C}_i \bigcap \mathcal{C}_j|$ to measure the correlation between two network data view $i$ and $j$. In particular, $\mathcal{C}$ and $V-\mathcal{C}$ can be considered independent. However, one thing that obstructs the direct application of traditional ensemble methods is that a network can generate exponential data views when sampling $n/2$ nodes from its $n$ nodes. 


\begin{figure}{
		\centering
\includegraphics[width = 1\linewidth]{fig_NSGCN.pdf}
		\caption{\sdrop and \dm. \drop: In each epoch we randomly drop half of the nodes and do the process of \half. The input in the inference phase is the original feature matrix without dropout, but with a discount factor of $0.5$. This method can be treated as the exponential ensemble of \half.
			\dm: In each epoch of \drop, we consider the half of the nodes that are dropped are independent to the remaining nodes and input them to another \drop. We minimize the disagreement of these two \drop s via the Jensen-Shannon divergence. This method can be treated as an economical co-training of two independent \drop s.}
		\label{fig:cotrain}
	}
\end{figure}

We propose a dropout-style ensemble model \drop.
In \drop, we develop a new network-sampling ensemble method---graph dropout, which samples the node set $\mathcal{C}$ from a network's node set and do the \shalf process in each training epoch. 
In the inference phase, we use the entire feature matrix with a discount factor $0.5$ as input~\cite{srivastava2014dropout}. 
The data flow in \drop\ is shown in Figure~\ref{fig:cotrain}. 


Specifically, given the recovery feature matrix $\widetilde{X}$ in a certain epoch, the softmax output of GCN in the training phase is $Z = GCN(\widetilde{X}) \in R^{n \times |\mathcal{Y}|}$. Hence the  objective of node classification in the network is


\begin{equation}
\label{equ:loss}
	Loss_{c1} = -\sum_{i\in V^L } \sum_l^{|\mathcal{Y}|}Y_{i,l} \ln Z_{i,l}
\end{equation}

\noindent where $Y_{i,l}$ is binary, indicating whether node $i$ has the label $l$.  


To make the model practical, in the inference phase, the output is achieved by using the entire feature matrix $X$ and a discount factor $0.5$~\cite{srivastava2014dropout}, that is, 

\begin{equation}
\label{equ:inf}
	\hat{Z} = GCN\left(\frac{0.5}{k}\sum_i^k\hat{A}^i X\right)
\end{equation}

The discount factor guarantees the input of GCNs in the inference phase is the same as the expected input in the training phase. Similar idea has been also used in~\cite{srivastava2014dropout}. 









\subsection{\dm: Disagreement Minimization}In this section, we present the \dm\ model, which leverages the independency between the chosen node set $\mathcal{C}$ and the dropout node set $V-\mathcal{C}$ in each epoch of \drop. 


In \drop, the network data view provided by the chosen set $\mathcal{C}$ is trained and the view provided by the remaining nodes $V-\mathcal{C}$ at each epoch is completely ignored. 
The idea behind \drop\ aims to train over as many as network views as possible. 
Therefore, in \dm, we propose to simultaneously train two \drop s with complementary inputs---$\mathcal{C}$ and $V-\mathcal{C}$---at every epoch. 
In the  training phase, we improve the two GCNs' prediction ability separately by minimizing the disagreement of these two models. 




The objective to minimize the disagreement of the two complementary \sdrop at a certain epoch can be obtained by Jensen-Shannon-divergence, that is, 

\begin{equation}
\label{equ:jsloss}
	Loss_{JS} = \frac{1}{2}\sum_{i\in V} \sum_l^{|\mathcal{Y}|} \left(Z'_{i,l} \ln \frac{Z'_{i,l}}{Z''_{i,l}} + Z''_{i,l} \ln \frac{Z''_{i,l}}{Z'_{i,l}}\right)
\end{equation}

\noindent where $Z'$ and $Z''$ are the outputs of these two \drop s in the training phase, respectively. 


The final objective of \sdm is

\begin{equation}
\label{equ:loss}
	Loss_{dm} = (Loss_{c1} + Loss_{c2})/2 + \lambda Loss_{JS}
\end{equation}

In the inference phase, the final prediction is made by the two GCNs together, i.e., 

\begin{equation}
\label{equ:inf}
	\hat{Z} = (\hat{Z}^{'} + \hat{Z}{''})/2
\end{equation}

Practically, the correlation of two \drop s and their individual prediction ability jointly affect the performance of \dm. 
The inputs to the two models are complementary  and usually uncorrelated, empowering \dm\ with more representation capacity than \drop, which is also evident from the empirical results in Section \ref{sec:exp}.  



\subsection{ Graph Dropout for Network Sampling}
In GraphSAGE~\cite{hamilton2017inductive} and FastGCN~\cite{chen2018fastgcn}, 
nodes or edges are sampled in order to accelerate training, avoid overfitting, and make the network data regularly like grids. However in the inference phase, these models may miss information when the sampling result is not representative. 

Different from previous works, our \drop\ and \dm\ models are based on the network redundancy observation and the proposed dropout-like ensemble enable the (implicit) training on exponential sufficient network data views, making them outperform not only existing sampling-based methods, but also the original GCN with the full single-view~\cite{kipf2016semi}. 
This provides 
insights into the network sampling-based GCNs and offers a new way to sample networks for GCNs.



In addition, there exist connections between the original dropout mechanism in CNNs and our dropout-like network sampling technique. 
Similarly, both techniques use a discount factor in the inference phase. 
Differently, the dropout operation in the fully connected layers in CNNs results in the missing values of the activation (hidden representation) matrix $H^{(l)}$, while our graph dropout sampling method is applied in network nodes, which removes network information in a structured way. 

Notice that very recently, DropBlock~\cite{ghiasi2018dropblock} suggested that a more structured form of dropout is required for better regularizing CNNs, and from this point of view, our graph dropout based network sampling is more in line with the idea of DropBlock. In \drop\ and \dm, dropping network information, instead of removing values from the activation functions, brings a structured way of removing information from the network. 
}






}







\hide{
We first introduce the following Lemma with the proof in Appendix A.1. \begin{lemma}
\label{lemma1}

For any reserve vector $\mathbf{q}^{(n)}$, residue vector $\mathbf{r}^{(n)}$ and random walk transition vector $\mathbf{P}^{n}_s$ ($0 \leq n \leq N$), we have:
\begin{equation}
\small
\mathbf{P}^{n}_s =  (\widetilde{\mathbf{D}}^{-1}\widetilde{\mathbf{A}})^n_s = \mathbf{q}^{(n)} + \sum_{i=1}^{n}  (\mathbf{P}^i)^\mathsf{T} \cdot \mathbf{r}^{(n-i)}
\end{equation}
\hide{For any reserve vector $\mathbf{Q}^{(n)}_s$, residue vector $\mathbf{R}^{(n)}_s$ and random walk transition vector $\mathbf{P}^{n}_s$, $0 \leq n \leq N$:
\begin{equation}
\small
    \mathbf{P}^{n}_s = \mathbf{Q}^{(n)}_s + \sum_{i=0}^{n-1}  (\mathbf{P}^\mathsf{N})^i\mathbf{R}_s^{(n-1-i)}
\end{equation}
}
\end{lemma}
From Lemma~\ref{lemma1} we can derive that 
$
\mathbf{q}^{(n)}_v =  \mathbf{P}^{n}(s,v) - \sum_{i=1}^{n} (\mathbf{P}^{i})^{\mathsf{T}}_v \cdot \mathbf{r}^{(n-i)}$. When the algorithm terminates, the elements of $\mathbf{r}^{(n-i)}$ are usually very small, thus
$\mathbf{q}^{(n)}$ can be seen as an approximation of $\mathbf{P}^n_s$. And consequently $\widetilde{\mathbf{\Pi}}_s=\sum_{n=0}^N w_n\mathbf{q}^{(n)}$ is considered as an approximation of $\mathbf{\Pi}_s$ as returned by Algorithm~\ref{alg:GFPush}.
}