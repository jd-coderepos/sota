We inject the layout information into the model in two ways. Firstly, we modify
the input embeddings of the original RoBERTa model by adding the layout term. We
also experiment with completely removing the sequential embedding term.
Secondly, we apply relative attention bias, used
\bycite{shaw2018,Huang2019,Raffel2020} in the context of sequential position.
The final architecture is depicted in Figure~\ref{fig:architecture}.

\begin{figure}[htb]
  \centering
  \includegraphics[width=122mm]{diagram.pdf}
  \caption{LAMBERT model architecture. Differences with the plain RoBERTa model
    are indicated by white text on dark blue background. \(N=12\) is the number
    of transformer encoder layers, and \(h=12\) is the number of attention heads
    in each encoder layer. \(Q\), \(K\), and \(V\) are, respectively, the queries,
    keys and values obtained by projecting the self-attention inputs.}
  \label{fig:architecture}
\end{figure}

\subsection{Background}
\label{sec:bert}

The basic Transformer encoder, used in, for instance, BERT
\citep{devlin2019-bert} and RoBERTa \citep{liu2019-roberta}, is a
sequence-to-sequence model transforming a sequence of input embeddings
\(x_i\in\RR^n\) into a sequence of output embeddings \(y_i\in\RR^m\) of the same
length, for the input/output dimensions \(n\) and \(m\). One of the main
distinctive features of this architecture is that it discards the order of its
input vectors. This allows parallelization levels unattainable for recurrent neural
networks.

In such a setting, the information about the order of tokens is preserved not by
the structure of the input. Instead, it is explicitly passed to the model, by
defining the input embeddings as
\begin{equation}
  \label{eq:bert-emb}
  x_i = s_i + p_i,
\end{equation}
where \(s_i\in\RR^n\) is the semantic embedding of the token at position \(i\),
taken from a trainable embedding layer, while \(p_i\in\RR^n\) is a
\emph{positional embedding}, depending only on \(i\). In order to avoid
confusion, we will, henceforth, use the term \emph{sequential embeddings}
instead of \emph{positional embeddings}, as the \emph{positional} might be
understood as relating to the 2-dimensional position on the page, which we will
deal with separately.

Since in RoBERTa, on which we base our approach, the embeddings \(p_i\) are
trainable, the number of pretrained embeddings (in this case~512) defines a
limit on the length of the input sequence. In general, there are many ways to
circumvent this limit, such as using predefined \citep{Vaswani2017-transformer}
or relative \citep{dai2019-transformer-xl} sequential embeddings.


\subsection{Modification of input embeddings}
\label{sec:method}


We replace the input embeddings defined in~\eqref{eq:bert-emb} with
\begin{equation}
  \label{eq:2d-emb}
  x_i = s_i + p_{i} + L(\ell_{i}).
\end{equation}
Here, \(\ell_{i}\in\RR^{k}\) stands for \emph{layout embeddings}, which are
described in detail in the next subsection. They carry the information about
the position of the \(i\)-th token on the page.

The dimension \(k\) of the layout embeddings is allowed to differ from the input
embedding dimension \(n\), and this difference is dealt with by a trainable
linear layer \(L\colon \RR^{k}\to\RR^{n}\). However, our main motivation to
introduce the adapter layer \(L\) was to gently increase the strength of the
signal of layout embeddings during training. In this way, we initially avoided
presenting the model with inputs that it was not prepared to deal with. Moreover, in
theory, in the case of non-trainable layout embeddings, the adapter layer may be
able to learn to project \(\ell_{i}\) onto a subspace of the embedding space
that reduces interference with the other terms in \eqref{eq:2d-emb}. For
instance, it is possible for the image of the adapter layer to learn to be
approximately orthogonal to the sum of the remaining terms. This would minimize
any information loss caused by adding multiple vectors. While this was our
theoretical motivation, and it would be interesting to investigate in detail how
much of it actually holds, such detailed considerations of a single model
component exceed the scope of this paper. We included the impact of using the
adapter layer in the ablation study.

We initialize the weight matrix of \(L\) according to a normal distribution
\(\mathcal{N}(0,\sigma^2)\), with the standard deviation \(\sigma\) being a
hyperparameter. We have to choose \(\sigma\) carefully, so that in the initial
phase of training, the \(L(\ell_i)\) term does not interfere overly with the
already learned representations. We experimentally determined the value
\(\sigma=0.02\) to be near-optimal\footnote{we tested the values 0.5, 0.1, 0.02,
  0.004, and 0.0008}.


\subsection{Layout embeddings}
\label{sec:context-embeddings}

In our setting, a document is represented by a sequence of tokens \(t_i\) and
their bounding boxes~\(b_i\). To each element of this sequence, we assign its
layout embedding \(\ell_i\), carrying the information about the position of the
token with respect to the whole document. This could be performed in various
ways. What they all have in common is that the embeddings \(\ell_i\) depend only
on the bounding boxes \(b_i\) and not on the tokens~\(t_i\).

We base our layout embeddings on the method originally used in
\citep{gehring2017convolutional}, and then in \citep{Vaswani2017-transformer} to
define the sequential embeddings. We first normalize the bounding boxes by
translating them so that the upper left corner is at \((0,0)\), and dividing
their dimensions by the page height. This causes the page bounding box to become
\((0, 0, w, 1)\), where \(w\) is the normalized width.

The layout embedding of a token will be defined as the concatenation of four
embeddings of the individual coordinates of its bounding box. For an integer
\(d\) and a vector of scaling factors \(\theta\in\RR^{d}\), we define the
corresponding embedding of a single coordinate \(t\) as
\begin{equation}
  \operatorname{emb_{\theta}}(t) = (\sin(t\theta); \cos(t\theta)) \in \RR^{2d},
\end{equation}
where the \(\sin\) and \(\cos\) are performed element-wise, yielding two vectors
in \(\RR^{d}\). The resulting concatenation of single bounding box coordinate
embeddings is then a vector in \(\RR^{8d}\).

In \citep[Section~3.5]{Vaswani2017-transformer}, and subsequently in other
Transformer-based models with precomputed sequential embeddings, the sequential
embeddings were defined by \(\operatorname{emb}_{\theta}\) with \(\theta\) being
a geometric progression interpolating between \(1\) and \(10^{-4}\). Unlike the
sequential position, which is a potentially large integer, bounding box
coordinates are normalized to the interval \([0,1]\). Hence, for our layout
embeddings we use larger scaling factors \((\theta_r)\), namely a geometric
sequence of length \(n/8\) interpolating between \(1\) and \(500\), where \(n\)
is the dimension of the input embeddings.

\subsection{Relative bias}
\label{sec:relative-bias}


Let us recall that in a typical Transformer encoder, a single attention head
transforms its input vectors into three sequences: queries \(q_{i}\in \RR^{d}\),
keys \(k_{i}\in\RR^{d}\), and values \(v_{i}\in\RR^{d}\). The raw attention
scores are then computed as \(\alpha_{ij} = d^{-1/2}q_{i}^{T}k_{j}\).
Afterwards, they are normalized using softmax, and used as weights in linear
combinations of value vectors.

The point of relative bias is to modify the computation of the raw attention
scores by introducing a bias term: \(\alpha'_{ij} = \alpha_{ij} + \beta_{ij}\).
In the sequential setting, \(\beta_{ij} = W(i-j)\) is a trainable weight,
depending on the relative sequential position of tokens \(i\) and \(j\). This
form of attention bias was introduced in \citep{Raffel2020}, and we will refer
to it as \emph{sequential attention bias}.

We introduce a simple and natural extension of this mechanism to the
two-dimensional context. In our case, the bias \(\beta_{ij}\) depends on the
relative positions of the tokens. More precisely, let \(C \gg 1\) be an integer
resolution factor (the number of cells in a grid used to discretize the
normalized coordinates). If \(b_{i} = (x_{1}, y_{1}, x_{2}, y_{2})\) is the
normalized bounding box of the \(i\)-th token, we first reduce it to a
\(2\)-dimensional position \((\xi_{i},\eta_{i}) = (Cx_{1}, C(y_{1}+y_{2})/2) \),
and then define
\begin{equation}
  \beta_{ij} = H(\lfloor {\xi_{i} - \xi_{j}}  \rfloor) +
  V(\lfloor{\eta_{i}-\eta_{j}} \rfloor),
\end{equation}
where \(H(\ell)\) and \(V(\ell)\) are trainable weights defined for every
integer \(\ell \in [-C, C)\). A good value for \(C\) should allow for a distinction
between consecutive lines and tokens, without unnecessarily affecting
performance. For a typical document \(C=100\) is enough, and we fix this in our
experiments.

This form of attention bias will be referred to as \emph{2D attention bias}. We
suspect that it should help in analyzing, say, tables by allowing the learning
of relationships between cells.




