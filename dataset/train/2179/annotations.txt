Question Answering#SQuAD1.1#EM#87.433
Question Answering#SQuAD1.1#F1#93.160
Question Answering#SQuAD1.1#EM#85.083
Question Answering#SQuAD1.1#F1#91.835
Question Answering#CoQA#In-domain#82.5
Question Answering#CoQA#Out-of-domain#77.6
Question Answering#CoQA#Overall#81.1
Question Answering#CoQA#In-domain#79.8
Question Answering#CoQA#Out-of-domain#74.1
Question Answering#CoQA#Overall#78.1
Question Answering#SQuAD1.1 dev#EM#84.2
Question Answering#SQuAD1.1 dev#F1#91.1
Question Answering#SQuAD1.1 dev#EM#84.1
Question Answering#SQuAD1.1 dev#F1#90.9
Question Answering#SQuAD1.1 dev#EM#80.8
Question Answering#SQuAD1.1 dev#F1#88.5
Question Answering#SQuAD2.0#EM#80.005
Question Answering#SQuAD2.0#F1#83.061
Question Answering#Quora Question Pairs#Accuracy#72.1%
Question Answering#SQuAD2.0 dev#EM#78.7
Question Answering#SQuAD2.0 dev#F1#81.9
Common Sense Reasoning#Winograd Schema Challenge#Score#62.0
Common Sense Reasoning#SWAG#Dev#86.6
Common Sense Reasoning#SWAG#Test#86.3
Common Sense Reasoning#SWAG#Dev#81.6
Common Sense Reasoning#SWAG#Test#-
Natural Language Inference#SciTail#Accuracy#92.0
Cross-Lingual Natural Language Inference#XNLI Zero-Shot English-to-Spanish#Accuracy#74.3%
Cross-Lingual Natural Language Inference#XNLI Zero-Shot English-to-German#Accuracy#70.5%
Sentiment Analysis#SST-2 Binary classification#Accuracy#94.9
Named Entity Recognition#NCBI-disease#F1#86.37
Named Entity Recognition#CoNLL 2003 (English)#F1#92.8
Named Entity Recognition#CoNLL 2003 (English)#F1#92.4
Named Entity Recognition#SciERC#F1#65.24
Named Entity Recognition#BC5CDR#F1#85.61
Text Classification#DBpedia#Error#0.64
Sentence Classification#SciCite#F1#84.4
Conversational Response Selection#PolyAI Reddit#1-of-100 Accuracy#24.0%
Natural Language Understanding#WNLI#Accuracy#65.1
Natural Language Understanding#PDP60#Accuracy#78.3
