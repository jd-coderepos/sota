\begin{wrapfigure}{R}{0.5\textwidth}
\vspace{-5pt}
  \begin{center}
    \includegraphics[width=0.5\textwidth]{./images/steerableFilters.pdf}	
  \end{center}  
  \caption{\small The dynamic filter network for learning steerable filters and several examples of learned filters.}
  \label{fig:steerableFilters}
  \vspace{-5pt}
\end{wrapfigure}
The Dynamic Filter Network can be used in different ways in a wide variety of applications. 
In this section we show its application in learning steerable filters, video prediction and also stereo prediction. 
The first application shows a simple use case of a dynamic filter network which uses a dynamic convolutional layer with two different types of inputs.
The second one shows that we can integrate the dynamic filter module with a dynamic local filtering layer in a recurrent network to predict a sequence of frames.
The third one shows its use case when there is only one kind of input.
We use \textit{Theano}~\cite{Theano-2012} based library \textit{Lasagne}~\cite{lasagne} to implement all the experiments.

\subsection{Learning steerable filters}
We first set up a simple experiment to illustrate the basics of the dynamic filter module with a dynamic convolution layer. 
The task is to filter an input image with a steerable filter of a given orientation . 
The network must learn this transformation from looking at input-output pairs, consisting of randomly chosen input images and angles together with their corresponding output.

The task of the filter-generating network here is to transform an angle into a filter, which is then applied to the input image to generate the final output.
We implement the filter-generating network as a few fully-connected layers with the last layer containing 81 neurons, corresponding to the elements of a 9x9 convolution filter. 
Figure~\ref{fig:steerableFilters} shows an example of the trained network. 
It has indeed learned the expected filters and applies the correct transformation to the image.

\subsection{Video prediction}
Here we describe how we make use of the proposed dynamic filter network for video prediction.
The architecture of our model is shown in Table~\ref{table:mnist} (right).
Given a sequence of frames, the task is to predict the sequence of future frames that directly follow the input frames.
To address this task we use the convolutional encoder-decoder as the filter-generating network where the encoder consists of several strided convolutional layers and the decoder consists of several unpooling layers and convolutional layers.
The convolutional encoder-decoder is able to exploit the spatial correlation within a frame and generates feature maps that are of the same size as the frame.
To exploit the temporal correlation between frames we add a recurrent connection inside the filter-generating network: 
we pass  the previous hidden state through two convolutional layers and take the sum with the output of the encoder to produce the new hidden state. During prediction, we propagate the prediction from the previous time step.
Note that we use a very simple recurrent architecture rather than the more advanced LSTM as in~\cite{Srivastava-ICML15,Shi-NIPS15}. 
A softmax layer is applied to each generated filter such that each filter is encouraged to have only a few non-zero elements.
To produce the prediction of the next frame, the generated filters are applied on the previous frame to transform it according to the dynamic local filtering mechanism explained in Section~\ref{sec:method}.
The use of a softmax layer helps the dynamic filtering layer to generate sharper images because each pixel in the output image comes from only a few pixels in the previous frame. 
\begin{table}[t]
\begin{center}\small
\setlength{\tabcolsep}{2pt}
\begin{tabular}[t]{ll||r|r}
\multicolumn{2}{p{1.5cm}||}{~}&
\multicolumn{2}{c}{\centering Moving MNIST}\\
\multicolumn{2}{p{1.5cm}||}{\centering ~~~~Model} &
\multicolumn{1}{c|}{\centering \# params} &
\multicolumn{1}{c}{\centering bce}\\
\hline
\multicolumn{2}{l||}{FC-LSTM~\cite{Srivastava-ICML15}} & 142,667,776 & 341.2\\
\multicolumn{2}{l||}{Conv-LSTM~\cite{Shi-NIPS15}} & 7,585,296 & 367.1\\
\hline
\multicolumn{2}{l||}{DFN (ours)} & \bf{637,361} & \bf{285.2}\\
\end{tabular}
\qquad\quad
\vtop{\vspace{-1em}\hbox{\includegraphics[width=0.50\textwidth]{./images/videoPrediction_architecture.pdf}}}
\end{center}
\caption{\small \emph{Left:} Quantitative results on Moving MNIST: number of model parameters and average binary cross-entropy (bce). \emph{Right:} 
The dynamic filter network for video prediction.}
\label{table:mnist}
\end{table}

\paragraph{Moving MNIST}
We first evaluate the method on the moving MNIST dataset~\cite{Srivastava-ICML15}. 
We follow the setting in~\cite{Srivastava-ICML15}, that is, given a sequence of 10 frames with 2 moving digits as input, we predict the following 10 frames. 
We use the code provided by~\cite{Srivastava-ICML15} to generate training samples on-the-fly, and test it on the provided test set for comparison.
Only simple preprocessing is done to convert pixel values into the range [0,1].

We use the average binary cross-entropy over the 10 frames as loss function.
The size of the dynamic filters is set to 9x9. 
We initialize all model parameters using the method proposed in~\cite{He-ICCV15} and use the Adam optimizer~\cite{adam} with a learning rate of  to update those parameters. 
The network is trained end-to-end by backpropagation for  iterations with mini-batch size of .
\begin{figure}
	\centering
	\begin{tabular}{cccc}
		\includegraphics[width=1.0\textwidth]{./images/movingMnistSequences.pdf}
	\end{tabular}
	\caption{\small Qualitative results on moving MNIST. Note that the network has learned the bouncing dynamics and separation of overlapping digits. More examples and out-of-domain results are in the supplementary material.}
	\label{fig:movingMnistSequences}
\end{figure}
The quantitative results are shown in Table~\ref{table:mnist} (left). We use the cross-entropy of predictions as the evaluation metric. Our method outperforms the state-of-the-art~\cite{Srivastava-ICML15, Shi-NIPS15} and this with a much smaller model. Figure~\ref{fig:movingMnistSequences} shows some qualitative results. With the dynamic local filtering layer our method is able to correctly learn the individual motions of digits.
The convolutional encoder-decoder has a large receptive field and this helps the model to learn the bouncing behavior and generate the right filters in case when digits bounce off the walls.
We observe that the predictions deteriorate over time, i.e. the digits become blurry. 
This is partly because of the model error: our model is not able to perfectly separate digits after an overlap, and these errors accumulate over time.
Another cause of blurring comes from an artifact of the dataset: because of imperfect cropping, it is uncertain when exactly the digit will bounce and change its direction. The behavior is not perfectly deterministic. This uncertainty combined with the pixel-wise loss function encourages the model to "hedge its bets" when a digit reaches the boundary, causing a blurry result.
This issue could be alleviated with the methods proposed in~\cite{Goodfellow-NIPS14,Goroshin-NIPS15,Larsen-ICML16}.

\paragraph{Highway Driving}
\begin{figure}[t]
	\centering
	\begin{tabular}{cccc}
		\includegraphics[width=1.0\textwidth]{./images/highwayDrivingSequences.pdf}
	\end{tabular}
	\caption{\small Qualitative results of video prediction on the Highway Driving dataset. Note the good prediction of the lanes (red), bridge (green) and a car moving in the opposite direction (purple).}
	\label{fig:highwayDrivingSequences}
\end{figure}
We also evaluate this method on real-world data. From our industrial partner we obtained a video of a car driving on the highway. Compared to natural video like UCF101 used in~\cite{Ranzato14,Mathieu-ICLR16}, the highway driving data is highly structured and much more predictable, making it a good testbed for video prediction. 
There are many cases with illumination changes such as when the car drives through a tunnel.
To address this issue, we make a small modification to the architecture as shown in Table~\ref{table:mnist} (right) by adding a dynamic per-pixel bias before the filtering operation in the dynamic local filtering layer.
Because the Highway Driving sequence is less deterministic than moving MNIST, we only predict the next 3 frames given an input sequence of 3 frames. 
For longer sequences, the uncertainty would become too high for the network to learn a reasonable prediction with a simple pixel-wise loss function. 
A sampling based method might alleviate this problem but we leave it as future work.
We split the approximately  frames of the 30-minute video into a training set of  frames and a test set of  frames. During training, segments of 6 frames are selected randomly from the training set. 

We train with a Euclidean loss function and obtain a loss of  on the test set with a model consisting of  parameters.
Figure~\ref{fig:highwayDrivingSequences} shows some qualitative results. 
Similar to the experiment on moving MNIST, the predictions get blurry over time. 
This can partly be attributed to the increasing uncertainty combined with an element-wise loss-function which encourages averaging out the possible predictions. 
Moreover, the errors accumulate over time which after a while makes the network operate in an out-of-domain regime.

We also visualize the dynamically generated filters of the trained model,
 in a flow-like manner.
For each element of a filter we compute its shift to the center in x-axis and y-axis.
Taking the weighted sum of x-axis shifts over all filter elements, we can then get the overall x-axis shift caused by one filter. Similarly, we can get the overall y-axis shift.
A filter can thus approximately be visualized as a 2-dimensional vector.
We then visualize the filters in the same way as optical flow, see Figure~\ref{fig:byproducts} (left).
Though the flow map is not that smooth, this byproduct is learned in an unsupervised way by only training on unlabeled video data which is different from e.g.~\cite{Dosovitskiy-ICCV15}. 

\begin{figure}[t]
	\centering
    \begin{tabular}{cccc}
		\includegraphics[width=1.0\textwidth]{./images/byproducts.pdf}
	\end{tabular}
	\caption{\small Some samples for video (\emph{left}) and stereo (\emph{right}) prediction and visualization of the dynamically generated filters. More examples and a video can be found in the supplementary material.}
	\label{fig:byproducts}
\end{figure}

\subsection{Stereo prediction}
We also apply the Dynamic Filtering Network to the task of stereo prediction, which we define as predicting the right view given the left view of two horizontal-disparity cameras.
This task is a variant of video prediction, where the goal is to predict a new view in space rather than in time, and from a single image rather than multiple ones. 
Flynn~\etal~\cite{Flynn-CVPR15} developed a deep network for new view synthesis from multiple views in unconstrained settings like musea, parks and streets. 
We limit ourselves to the more structured Highway Driving dataset and a classical two-view stereo setup. 

We recycle the architecture from the previous section, but drop the recurrent connection which is used to model temporal correlation. 
Besides, based on the assumption that corresponding points are on the same horizontal line, we replace the square 9x9 filters with horizontal 13x1 filters. The network is trained and evaluated on the same train- and test split as in the previous section but with the left view as input and the right one as target. It reaches a loss of  on the test set with a model consisting of  parameters. Some qualitative results are shown in Figure~\ref{fig:byproducts} (right). The network has learned to shift objects to the left depending on their distance to the camera.

We again compute flow-like maps for the generated dynamic filters.
As shown in Figure~\ref{fig:byproducts} (right) the network has effectively learned to estimate depth information from a single image.
The results suggest that it is possible to use the proposed dynamic filter network architecture to pre-train networks for optical flow and disparity map estimation in an unsupervised manner using only unlabeled data. 