\def\year{2019}\relax
\documentclass[letterpaper]{article} \usepackage{aaai19}  \usepackage{times}  \usepackage{helvet}  \usepackage{courier}  \usepackage{url}  \usepackage{graphicx}  \usepackage{pbox}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{subfigure}
\usepackage{amssymb}\usepackage{pifont}\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}

\frenchspacing  \setlength{\pdfpagewidth}{8.5in}  \setlength{\pdfpageheight}{11in}  \pdfinfo{
/Title (Read + Verify: Machine Reading Comprehension with Unanswerable Questions)
/Author (AAAI Press Staff)}
\setcounter{secnumdepth}{0}  
 \begin{document}
\title{Read + Verify: Machine Reading Comprehension \\ with Unanswerable Questions}
\author{
Minghao Hu\thanks{Contribution during internship at Microsoft Research Asia.}, 
Furu Wei, 
Yuxing Peng, 
Zhen Huang, 
Nan Yang,
Dongsheng Li
\\ 
College of Computer, National University of Defense Technology  \\
Microsoft Research Asia   \\
\{huminghao09,pengyuxing,huangzhen,dsli\}@nudt.edu.cn \\
\{fuwei,nanya\}@microsoft.com
}
\maketitle

\begin{abstract}
Machine reading comprehension with unanswerable questions aims to abstain from answering when no answer can be inferred.
In addition to extract answers, previous works usually predict an additional ``no-answer'' probability to detect unanswerable cases.
However, they fail to validate the answerability of the question by verifying the legitimacy of the predicted answer.
To address this problem, we propose a novel read-then-verify system, which not only utilizes a neural reader to extract candidate answers and produce no-answer probabilities, but also leverages an answer verifier to decide whether the predicted answer is entailed by the input snippets.
Moreover, we introduce two auxiliary losses to help the reader better handle answer extraction as well as no-answer detection, and investigate three different architectures for the answer verifier.
Our experiments on the SQuAD 2.0 dataset show that our system obtains a score of 74.2 F1 on test set, achieving state-of-the-art results at the time of submission (Aug. 28th, 2018).
\end{abstract}
 \section{Introduction}
The ability to comprehend text and answer questions is crucial for natural language processing.
Due to the creation of various large-scale datasets~\cite{Hermann15,Nguyen16,Joshi17,Kovcisky18}, remarkable advancements have been made in the task of machine reading comprehension.
Nevertheless, one important hypothesis behind current approaches is that there always exists a correct answer in the context passage. 
Therefore, the models only need to choose a most plausible text span based on the question, instead of checking if there exists an answer in the first place.  
Recently, a new version of Stanford Question Answering Dataset (SQuAD), namely SQuAD 2.0~\cite{Rajpurkar18}, has been proposed to test the ability of answering answerable questions as well as detecting unanswerable cases.
To deal with unanswerable cases, systems must learn to identify a wide range of linguistic phenomena such as negation, antonymy and entity changes between the passage and the question.

Previous works~\cite{Levy17,Clark18,kundu2018nil} all apply a shared-normalization operation between a ``no-answer'' score and answer span scores, so as to produce a probability that a question is unanswerable as well as output a candidate answer.
However, they have not considered further validating the answerability of the question by verifying the legitimacy of the predicted answer.
Here, \emph{answerability} denotes whether the question has an answer, and \emph{legitimacy} means whether the extracted text can be supported by the passage and the question.
Human, on the contrary, tends to first find a plausible answer given a question, and then checks if there exists any contradictory semantics.

\begin{figure}
\begin{center}
\includegraphics[width=2.8in]{figures/1_overview.pdf}
\vspace{-0.1cm}
\caption{An overview of our approach. 
The reader first extracts a candidate answer and produces a no-answer probability (NA Prob). The answer verifier then checks whether the extracted answer is legitimate or not. Finally, the system aggregates previous results and outputs the final prediction.
}
\label{fig1}
\vspace{-0.5cm}
\end{center}
\end{figure}

To address the above issue, we propose a \emph{read-then-verify} system that aims to be robust to unanswerable questions in this paper.
As shown in Figure \ref{fig1}, our system consists of two components: (1) a \emph{no-answer reader} for extracting candidate answers and detecting unanswerable questions, and (2) an \emph{answer verifier} for deciding whether or not the extracted candidate is legitimate. 
The key contributions of our work are three-fold.

First, we augment existing readers with two \emph{auxiliary losses}, to better handle answer extraction and no-answer detection respectively. 
Since the downstream verifying stage always requires a candidate answer, the reader must be able to extract plausible answers for all questions. 
However, previous approaches are not trained to find potential candidates for unanswerable questions.
We solve this problem by introducing an independent span loss that aims to concentrate on the answer extraction task regardless of the answerability of the question.
In order to not conflict with no-answer detection, we leverage a \emph{multi-head pointer network} to generate two pairs of span scores, where one pair is normalized with the no-answer score and the other is used for our auxiliary loss.
Besides, we present another independent no-answer loss to further alleviate the confliction, by focusing on the no-answer detection task without considering the shared normalization of answer extraction.

Second, in addition to the standard reading phase, we introduce an additional answer verifying phase, which aims at finding local entailment that supports the answer by comparing the answer sentence with the question.
This is based on the observation that the core phenomenon of unanswerable questions usually occurs between a few passage words and question words.
Take Figure \ref{fig1} for example, after comparing the passage snippet ``\emph{Normandy, a region in France}'' with the question, we can easily determine that no answer exists since the question asks for an \emph{impossible condition}\footnote{Impossible condition means that the question asks for something that is not satisfied by anything in the given passage.}.
This observation is even more obvious when \emph{antonym} or \emph{mutual exclusion} occurs, such as the question asks for ``\emph{the decline of rainforests}'' but the passage mentions that ``\emph{the rainforests spread out}''.
Inspired by recent advances in natural language inference (NLI)~\cite{Bowman15}, we investigate three different architectures for the answer verifying task.
The first one is a sequential model that takes two sentences as a long sequence, while the second one attempts to capture interactions between two sentences.
The last one is a hybrid model that combines the above two models to test if the performance can be further improved.

Lastly, we evaluate our system on the SQuAD 2.0 dataset~\cite{Rajpurkar18}, a reading comprehension benchmark augmented with unanswerable questions. Our best reader achieves a F1 score of 73.7 and 69.1 on the development set, with or without ELMo embeddings~\cite{Elmo17}.
When combined with the answer verifier, the whole system improves to 74.8 F1 and 71.5 F1 respectively.
Moreover, the best system obtains a score of 74.2 F1 on test set, achieving state-of-the-art results at the time of submission (Aug. 28th, 2018).
 \section{Background}
Existing reading comprehension models focus on answering questions where a correct answer is guaranteed to exist.
However, they are not able to identify unanswerable questions but tend to return an unreliable text span.
Consequently, we first give a brief introduction on the unanswerable reading comprehension task, and then investigate current solutions.

\subsection{Task Description}
Given a context passage and a question, the machine needs to not only find answers to answerable questions but also detect unanswerable cases. 
The passage and the question are described as sequences of word tokens, denoted as  and  respectively, where  is the passage length and  is the question length. 
Our goal is to predict an answer , which is constrained as a segment of text in the passage: , or return an empty string if there is no answer, where  and  indicate the answer boundary. 

\subsection{No-Answer Reader}
To predict an answer span, current approaches first embed and encode both of passage and question into two series of fix-sized vectors. Then they leverage various attention mechanisms, such as bi-attention~\cite{Seo17} or reattention~\cite{Hu17}, to build interdependent representations for passage and question, which are denoted as  and  respectively.
Finally, they summarize the question representation into a dense vector , and utilize the pointer network~\cite{Vinyals15} to produce two scores over passage words that indicate the answer boundary~\cite{Wang17b}:

where  and  are the \emph{span scores} for answer start and end bounds.

In order to additionally detect if the question is unanswerable, previous approaches~\cite{Levy17,Clark18,kundu2018nil} attempt to predict a special \emph{no-answer score}  in addition to the distribution over answer spans.
Concretely, a shared softmax function can be applied to normalize both of no-answer score and span scores, yielding a joint no-answer objective defined as:

where  and  are the ground-truth start and end positions, and  is 1 if the question is answerable and 0 otherwise.
At test time, a question is detected as being unanswerable once the normalized no-answer score exceeds some threshold.
 \section{Approach}
In this section we describe our proposed read-then-verify system. 
The system first leverages a neural reader to extract a candidate answer and detect if the question is unanswerable.
It then utilizes an answer verifier to further check the legitimacy of the predicted answer.
We enhance the reader with two novel auxiliary losses, and investigate three different architectures for the answer verifier.

\begin{figure*}
\begin{center}
\includegraphics[width=6.8in]{figures/2_verifier.pdf}
\vspace{-0.1cm}
\caption{An overview of answer verifiers. 
(a) Input structures for running three different models.
(b) Generative Pre-trained Transformer proposed by Radford et al.~\shortcite{Radford18}. Here, ``Masked Multi Self Attention'' refers to multi-head self-attention function~\cite{vaswani2017attention} that only attends to previous tokens. ``Add \& Norm'' indicates residual connection and layer normalization.
(c) Our proposed token-wise interaction model, which is designed to compare two sentences and aggregate the results for verifying the answer.
}
\label{fig2}
\vspace{-0.5cm}
\end{center}
\end{figure*}

\subsection{Reader with Auxiliary Losses}
Although previous no-answer readers are capable of jointly learning answer extraction and no-answer detection, there exists two problems for each individual task.
For the answer extraction, previous readers are not trained to find candidate answers for unanswerable questions.
In our system, however, the reader is required to extract a plausible answer that is fed to the downstream verifying stage for all questions.
As for no-answer detection, a \emph{confliction} could be triggered due to the shared normalization between span scores and no-answer score.
Since the sum of these normalized scores is always 1, an over-confident span probability would cause an unconfident no-answer probability, and vice versa.
Therefore, inaccurate confidence on answer span, which has been observed by Clark et al.~\shortcite{Clark18}, could lead to imprecise prediction on no-answer score.
To address the above issues, we propose two auxiliary losses to optimize and enhance each task independently without interfering with each other.

\subsubsection{Independent Span Loss}
This loss is designed to concentrate on answer extraction.  
In this task, the model is asked to extract candidate answers for all possible questions. 
Therefore, besides answerable questions, we also include unanswerable cases as positive examples, and consider the \emph{plausible answer} as gold answer\footnote{In SQuAD 2.0, the plausible answer is annotated by human for every unanswerable question. A pre-trained reader can also be used to extract plausible answers if no annotation is provided.}.
In order to not conflict with no-answer detection, we propose to use a multi-head pointer network to additionally produce another pair of span scores  and :

where multiple heads share the same network architecture but with different parameters. 

Then, we define an independent span loss as:

where  and  are the augmented ground-truth answer boundaries. The final span probability is obtained using a simple mean pooling over the two pairs of softmax-normalized span scores.

\subsubsection{Independent No-Answer Loss}
Despite a multi-head pointer network being used to prevent the confliction problem, no-answer detection can still be weakened since the no-answer score  is normalized with span scores.
Therefore, we consider exclusively encouraging the prediction on no-answer detection.
This is achieved by introducing an independent no-answer loss as:

where  is the sigmoid activation function. 
Through this loss, we expect the model to produce a more confident prediction on no-answer score  without considering the shared-normalization operation.

Finally, we combine the above losses as follows:

where  and  are two hyper-parameters that control the weight of two auxiliary losses.


\subsection{Answer Verifier}
After the answer is extracted, an answer verifier is used to compare the answer sentence with the question, so as to recognize local textual entailment that supports the answer.
Here, we define the \emph{answer sentence} as the context sentence that contains either gold answers or plausible answers.
We explore three different architectures, as shown in Figure \ref{fig2}: 
(1) a sequential model that takes the inputs as a long sequence, (2) an interactive model that encodes two sentences interdependently, and (3) a hybrid model that takes both of the two approaches into account.

\subsubsection{Model-\uppercase\expandafter{\romannumeral1}: Sequential Architecture}
In Model-\uppercase\expandafter{\romannumeral1}, we convert the answer sentence and the question along with the extracted answer into an ordered input sequence. 
Then we adapt the recently proposed Generative Pre-trained Transformer (OpenAI GPT)~\cite{Radford18} to perform the task. 
The model is a multi-layer Transformer decoder~\cite{Liu18}, which is first trained with a language modeling objective on a large unlabeled text corpus and then finetuned on the specific target task.

Specifically, given an answer sentence , a question  and an extracted answer , we concatenate the two sentences with the answer while adding a delimiter token in between to get ; A]XW_eW_pnh_n^{l_m}\mathrm{softmax}yl_s[\cdot;\cdot]<s_i, q_j>b_iQiSc_j\{(s_i, b_i)\}_{i=1}^{l_s}\{(q_j, c_j)\}_{j=1}^{l_q}FFo=F(x,y)\mathrm{gelu}\circ\tilde{s}_i\tilde{q}_ja_{ij}=-infi=jF\hat{s}_i\hat{q}_j\bar{s}_i\bar{q}_j\mathrm{gelu}\mathrm{softmax}\gamma\lambda\gamma\lambda^1^2^\dagger^3-^\dagger^3^4^5^1^2^3^4^5\dagger$ indicates unpublished works.}
\vspace{-0.3cm}
\end{center}
\end{table}

\subsection{Main Results}
We first submit our approach on the hidden test set of SQuAD 2.0 for evaluation, which is shown in Table \ref{table1}. 
We use Model-\uppercase\expandafter{\romannumeral3} as the default answer verifier, and only report the best result.
As we can see, our system obtains state-of-the-art results by achieving an EM score of 71.7 and a F1 score of 74.2 on the test set. 
Notice that SLQA+ has reached a comparable result compared to our approach. 
We argue that its promising result is largely due to its superior performance compared to our base reader\footnote{SLQA+ achieves 87.0 F1 on the SQuAD 1.1 test set, while RMR reaches 86.6.}.



\subsection{Ablation Study}
Next, we do an ablation study on the SQuAD 2.0 development set to show the effects of our proposed methods for each individual component.
Table \ref{table2} first shows the ablation results of different auxiliary losses on the reader.
Removing the independent span loss (indep-\uppercase\expandafter{\romannumeral1}) results in a performance drop for all answerable questions (HasAns), indicating that this loss helps the model in better identifying the answer boundary.
Ablating independent no-answer loss (indep-\uppercase\expandafter{\romannumeral2}), on the other hand, causes little influence on HasAns, but leads to a severe decline on no-answer accuracy (NoAns ACC).
This suggests that a confliction between answer extraction and no-answer detection indeed happens.
Finally, deleting both of two losses causes a degradation of more than 1.5 points on the overall performance in terms of F1, with or without ELMo embeddings.

Table \ref{table3} details the results of various architectures for the answer verifier.
Model-\uppercase\expandafter{\romannumeral3} outperforms all of other competitors, achieving a no-answer accuracy of 76.2. 
This illustrates that the combination of two different architectures can bring in further improvement.
Adding ELMo embeddings, however, does not boost the performance. 
We hythosize that the bytepair encoding~\cite{Sennrich16} from Model-\uppercase\expandafter{\romannumeral1} and the word/character embeddings from Model-\uppercase\expandafter{\romannumeral2} have provided enough representation capacities.

\begin{table}
\begin{center}
\begin{tabular}{l|ccccc}
\toprule
\multirow{2}*{ Configuration } & \multicolumn{2}{c}{ HasAns } & \multicolumn{2}{c}{ All } & NoAns \\
& EM & F1 & EM & F1 & ACC \\ 
\midrule
RMR    & {72.6} & {81.6} & {66.9} & {69.1} & {73.1} \\
\ \ - indep-\uppercase\expandafter{\romannumeral1}  & \underline{71.3} & \underline{80.4} & 66.0 & 68.6 & 72.8 \\
\ \ - indep-\uppercase\expandafter{\romannumeral2}  & 72.4 & 81.4 & \underline{64.0} & \underline{66.1} & \underline{69.8} \\
\ \ - both    & 71.9 & 80.9 & 65.2 & 67.5 & 71.4 \\
\midrule
RMR + ELMo    & {79.4} & {86.8} & {71.4} & {73.7} & {77.0} \\
\ \ - indep-\uppercase\expandafter{\romannumeral1}  & 78.9 & 86.5 & 71.2 & 73.5 & 76.7 \\
\ \ - indep-\uppercase\expandafter{\romannumeral2}  & 79.5 & 86.6 & \underline{69.4} & \underline{71.4} & \underline{75.1} \\
\ \ - both                                          & \underline{78.7} & \underline{86.2} & 70.0 & 71.9 & 75.3 \\
\bottomrule
\end{tabular}
\caption{\label{table2} Comparison of readers with different auxiliary losses.}
\vspace{-0.1cm}
\end{center}
\end{table}

\begin{table}
\begin{center}
\begin{tabular}{l|cc}
\toprule
Configuration & NoAns ACC\\ 
\midrule
Model-\uppercase\expandafter{\romannumeral1}            & 74.5 \\
Model-\uppercase\expandafter{\romannumeral2}            & 74.6 \\
Model-\uppercase\expandafter{\romannumeral2} + ELMo     & 75.3 \\
Model-\uppercase\expandafter{\romannumeral3}            & \bf{76.2} \\
Model-\uppercase\expandafter{\romannumeral3} + ELMo     & 76.1 \\
\bottomrule
\end{tabular}  
\caption{\label{table3} Comparison of different architectures for the answer verifier.}
\vspace{-0.5cm}
\end{center}
\end{table}


After doing separate ablations on each component, we then compare the performance of the whole system, as shown in Table \ref{table4}.
The combination of base reader with any answer verifier can always result in considerable performance gains, and combining the reader with Model-\uppercase\expandafter{\romannumeral3} obtains the best result.
We find that the improvement on no-answer accuracy is significant. 
This metric raises from 73.1 to 77.1 after adding Model-\uppercase\expandafter{\romannumeral3} to RMR, increasing by 4 absolute points.
Similar observation can be found when ELMo embeddings are used, demonstrating that the gains are consistent and stable.

In order to investigate how the readers affect the overall performance, we fix the answer verifier as Model-\uppercase\expandafter{\romannumeral3} and use DocQA~\cite{Clark18} as the base reader instead of RMR, as shown in Table \ref{table5}. 
We find that the absolute improvements are even larger: the no-answer accuracy roughly increases by 6 points when adding Model-\uppercase\expandafter{\romannumeral3} to DocQA (from 69.1 to 75.2), and 5.5 points when adding Model-\uppercase\expandafter{\romannumeral3} to DocQA + ELMo (from 70.6 to 76.1).

Finally, we plot the precision-recall curves of F1 score on the development set in Figure \ref{fig3}.
We observe that RMR + ELMo + Verifier achieves the best precision when the recall is less than 80.
After the recall exceeds 80, the precision of RMR + ELMo becomes slightly better.
Ablating two auxiliary losses, however, leads to an overall degradation on the curve, but it still outperforms the baseline by a large margin.


\subsection{Error Analysis}

\begin{table}
\begin{center}
\begin{tabular}{l|cccc}
\toprule
\multirow{2}*{ Configuration } & \multicolumn{2}{c}{ All } & NoAns \\
& EM & F1 & ACC \\ 
\midrule
RMR                                                        & 66.9 & 69.1 & 73.1 \\
\ \ + Model-\uppercase\expandafter{\romannumeral1}         & 68.3 & 71.1 & 76.2 \\
\ \ + Model-\uppercase\expandafter{\romannumeral2}         & 68.1 & 70.8 & 75.6  \\
\ \ + Model-\uppercase\expandafter{\romannumeral2} + ELMo  & 68.2 & 70.9 & 75.9  \\
\ \ + Model-\uppercase\expandafter{\romannumeral3}         & \bf{68.5} & \bf{71.5} & \bf{77.1}  \\
\ \ + Model-\uppercase\expandafter{\romannumeral3} + ELMo  & 68.5 & 71.2 & 76.5  \\
\midrule
RMR + ELMo                                                 & 71.4 & 73.7 & 77.0 \\
\ \ + Model-\uppercase\expandafter{\romannumeral1}         & 71.8 & 74.4 & 77.3 \\
\ \ + Model-\uppercase\expandafter{\romannumeral2}         & 71.8 & 74.2 & 78.1 \\
\ \ + Model-\uppercase\expandafter{\romannumeral2} + ELMo  & 72.0 & 74.3 & 78.2 \\
\ \ + Model-\uppercase\expandafter{\romannumeral3}         & \bf{72.3} & \bf{74.8} & \bf{78.6} \\
\ \ + Model-\uppercase\expandafter{\romannumeral3} + ELMo  & 71.8 & 74.3 & 78.3 \\
\bottomrule
\end{tabular}
\caption{\label{table4} Comparison of readers with different answer verifiers.}
\vspace{-0.1cm}
\end{center}
\end{table}

\begin{table}
\begin{center}
\begin{tabular}{l|cccc}
\toprule
\multirow{2}*{ Configuration } & \multicolumn{2}{c}{ All } & NoAns \\
& EM & F1 & ACC \\ 
\midrule
DocQA                                                      & 61.9 & 64.8 & 69.1 \\
\ \ + Model-\uppercase\expandafter{\romannumeral3}         & \bf{66.5} & \bf{69.2} & \bf{75.2}  \\
\midrule
DocQA + ELMo                                               & 65.1 & 67.6 & 70.6 \\
\ \ + Model-\uppercase\expandafter{\romannumeral3}         & \bf{68.0} & \bf{70.7} & \bf{76.1}  \\
\bottomrule
\end{tabular}
\caption{\label{table5} Comparison of different readers with fixed answer verifier.}
\vspace{-0.5cm}
\end{center}
\end{table}

\begin{table*}
\begin{center}
\begin{tabular}{l|ccccc}
\toprule
Configuration           & Case1 \textcolor{red}{\cmark} & Case2 \textcolor{red}{\cmark} & Case3 \textcolor{blue}{\xmark} & Case4 \textcolor{blue}{\xmark} & Case5 \textcolor{blue}{\xmark} \\ 
\midrule
RMR - both              & 27.8\% & 37.3\% & 6.5\% & 12.7\% & 15.7\% \\
RMR	                   & 27\% & \bf{39.9\%} & \bf{5.9\%} & \bf{10.2\%} & 17\% \\
RMR + Verifier          & \bf{30.3\%} & 38.2\% & 8.4\% & 11.8\% & \bf{11.3\%} \\
\midrule
RMR + ELMo - both	   & 31.5\% & 38.3\% & 5.6\% & 11.8\% & 12.8\% \\
RMR + ELMo              & 31.2\% & \bf{40.2\%} & \bf{5.5\%} & \bf{9.9\%} & 13.2\% \\
RMR + ELMo + Verifier   & \bf{32.5\%} & 39.8\% & 6.5\% & 10.3\% & \bf{10.9\%} \\
\bottomrule
\end{tabular}
\caption{\label{table6} Percentage of five categories. Correct predictions are denoted with \textcolor{red}{\cmark}, while wrong cases are marked with \textcolor{blue}{\xmark}.}
\vspace{-0.3cm}
\end{center}
\end{table*}

To perform error analysis, we first categorize all examples on the development set into 5 classes: 
\begin{itemize}
\item \emph{Case1}: the question is \emph{answerable}, the no-answer probability is \emph{less} than the threshold, and the answer is \emph{correct}. 
\item \emph{Case2}: the question is \emph{unanswerable}, and the no-answer probability is \emph{larger} than the threshold.
\item \emph{Case3}: almost the same as case1, except that the predicted answer is \emph{wrong}.
\item \emph{Case4}: the question is \emph{unanswerable}, but the no-answer probability is \emph{less} than the threshold.
\item \emph{Case5}: the question is \emph{answerable}, but the no-answer probability is \emph{larger} than the threshold.
\end{itemize}


We then show the percentage of each category in Table \ref{table6}. 
As we can see, the base reader trained with auxiliary losses is notably better at \emph{case2} and \emph{case4} compared to the baseline, implying that our proposed losses help the model mainly improve upon unanswerable cases.
After adding the answer verifier, we observe that although the system's performance on unanswerable cases slightly decreases, the results on \emph{case1} and \emph{case5} have been improved.
This demonstrates that the answer verifier does well on detecting answerable question rather than unanswerable one.
Besides, we find that the error of answer extraction is relatively small (6.5\% for Case3 in RMR + ELMo + Verifier).
However, the classification error on no-answer detection is much larger. More than 20\% of examples are misclassified even with our best system (10.3\% for Case4 and 10.9\% for Case5 in RMR + ELMo + Verifier).
Therefore, we argue that the main performance bottleneck lies in no-answer detection instead of answer extraction.

\begin{figure}
\begin{center}
\includegraphics[width=2.9in]{figures/4_pr_curve.pdf}
\vspace{-0.1cm}
\caption{Precision-Recall curves of F1 score.}
\label{fig3}
\vspace{-0.5cm}
\end{center}
\end{figure}

Next, to understand the challenges our approach faces, we manually investigate 50 incorrectly predicted unanswerable examples (based on F1) that are randomly sampled from the development set. 
Following the types of negative examples defined by Rajpurkar et al.~\shortcite{Rajpurkar18}, we categorize the sampled examples and show them in Table \ref{table7}.
As we can see, our system is good at recognize \emph{negation} and \emph{antonym}.
The frequency of negation decreases from 9\% to 0\% and only 4 antonym examples are predicted wrongly.
We think that this is because the two types are relatively easier to identify.
Both of negation and antonym only require to detect one single word in the question, such as ``\emph{never}'' or ``\emph{not}'' for negation and ``\emph{increase}'' to ``\emph{decrease}'' for antonym.
However, \emph{impossible condition} and \emph{other neutral} types roughly acount for 46\% of the error set, indicating that our system performs less effectively on these more difficult cases.

\begin{table}
\begin{center}
\begin{tabular}{l|cccc}
\toprule
\multirow{2}*{ Phenomenon } & \multicolumn{2}{c}{ Percentage } \\
& All & Error \\ 
\midrule
Negation  &  9\%  & 0\% \\    
Antonym  &  20\% & 8\% \\    
Entity Swap  & 21\% & 24\% \\    
Mutual Exclusion  & 15\% & 16\% \\    
Impossible Condition  & 4\% & 14\% \\    
Other Neutral  &  24\% & 32\% \\    
Answerable  &  7\% & 6\% \\                                                  
\bottomrule
\end{tabular}
\caption{\label{table7} Linguistic phenomena exhibited by all negative examples (statistics from Rajpurkar et al.~\shortcite{Rajpurkar18}) and sampled error cases of RMR + ELMo + Verifier.}
\vspace{-0.5cm}
\end{center}
\end{table} \section{Related Work}

\noindent{\bf Reading Comprehension Datasets.} 
Various large-scale reading comprehension datasets, such as cloze-style test~\cite{Hermann15}, answer extraction benchmark~\cite{Rajpurkar16,Joshi17} and answer generation benchmark~\cite{Nguyen16,Kovcisky18}, have been proposed.
However, these datasets still guarantee that the given context must contain an answer.
Recently, some works construct negative examples by retrieving passages for existing questions based on Lucene~\cite{tan2018know} and TF-IDF~\cite{Clark18}, or using crowdworkers to craft unanswerable questions~\cite{Rajpurkar18}.
Compared to automatically retrieved negative examples, human-annotated examples are more difficult to detect for two reasons: (1) the questions are relevant to the passage and (2) the passage contains a plausible answer to the question.
Therefore, we choose to work on the SQuAD 2.0 dataset in this paper.


\noindent{\bf Neural Networks for Reading Comprehension.} 
Neural reading models typically leverage various attention mechanisms to build interdependent representations of passage and question, and sequentially predict the answer boundary~\cite{Seo17,Hu17,Wang17b,Yu18,Hu18}.
However, these approaches are not designed to handle no-answer cases.
To address this problem, previous works~\cite{Levy17,Clark18,kundu2018nil} predict a no-answer probability in addition to the distribution over answer spans, so as to jointly learn no-answer detection as well as answer extraction.
Our no-answer reader extends existing approaches by introducing two auxiliary losses that enhance these two tasks independently.

\noindent{\bf Recognizing Textual Entailment.} 
Recognizing textual entailment (RTE)~\cite{dagan2010recognizing,Marelli14}, or known as natural language inference (NLI)~\cite{Bowman15}, requires systems to understand entailment, contradiction or semantic neutrality between two sentences.
This task is strongly related to no-answer detection, where the machine needs to understand if the passage and the question supports the answer.
To recognize entailment, various branches of works have been proposed, including encoding-based approach~\cite{bowman2016fast,mou2015natural}, interaction-based approach~\cite{Parikh16,chen2016enhanced} and sequence-based approach~\cite{Radford18}.
In this paper we investigate the last two branches and further propose a hybrid architecture that combines both of them properly.


\noindent{\bf Answer Validation.} 
Early answer validation task~\cite{magnini2002right} aims at ranking multiple candidate answers to return a most reliable one.
Later, the answer validation exercise~\cite{rodrigo2008overview} has been proposed to decide whether an answer is correct or not according to a given supporting text and a question, but the dataset is too small for neural network-based approaches. 
Recently, Tan et al.~\shortcite{tan2018know} propose to validate the candidate answer for detecting unanswerable questions, by comparing the question with the passage.
Our answer verifier, on the contrary, denoises the passage by comparing questions with answer sentences, so as to focus on finding local entailment that supports the answer.  \section{Conclusion}
We proposed a read-then-verify system that is able to abstain from answering when a question has no answer given the passage.
We first introduce two auxiliary losses to help the reader concentrate on answer extraction and no-answer detection respectively, and then utilize an answer verifier to validate the legitimacy of the predicted answer, in which three different architectures are investigated.
Our system has achieved state-of-the-art results on the SQuAD 2.0 dataset at the time of submission (Aug. 28th, 2018).
Looking forward, we plan to design new structures for answer verifiers to handle questions with more complicated inferences.

\section*{Acknowledgments}
We would like to thank Pranav Rajpurkar and Robin Jia for their helps with SQuAD 2.0 submissions. This work is supported by the Major State Research Development Program (2016YFB0201305). 
\bibliography{sections/reference}
\bibliographystyle{aaai}

\end{document}
