

\documentclass{article}

\usepackage{times}
\usepackage{graphicx} 

\usepackage{natbib}

\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{wrapfig}

\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage[nohyperref,accepted]{icml2017}

\usepackage{url}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsfonts,eucal,amsbsy}
\usepackage{mathtools}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{xcolor, colortbl}
\usepackage{fixltx2e}
\usepackage{pbox}
\usepackage{setspace}

\newcommand{\zy}[1]{\textcolor{cyan}{\bf \small [#1 --zy]}}


\DeclareMathOperator{\E}{\mathbb{E}}




\icmltitlerunning{Improved Variational Autoencoders for Text Modeling using Dilated Convolutions}

\begin{document}

\twocolumn[
\icmltitle{Improved Variational Autoencoders for Text Modeling using Dilated Convolutions}







\begin{icmlauthorlist}
\icmlauthor{Zichao Yang}{cmu}
\icmlauthor{Zhiting Hu}{cmu}
\icmlauthor{Ruslan Salakhutdinov}{cmu}
\icmlauthor{Taylor Berg-Kirkpatrick}{cmu}
\end{icmlauthorlist}

\icmlaffiliation{cmu}{Carnegie Mellon University}

\icmlcorrespondingauthor{Zichao Yang}{zichaoy@cs.cmu.edu}


\icmlkeywords{boring formatting information, machine learning, ICML}

\vskip 0.3in
]





\printAffiliationsAndNotice{}  

\begin{abstract}
Recent work on generative text modeling has found that variational
autoencoders (VAE) with LSTM decoders perform worse than
simpler LSTM language models \citep{bowman2015generating}. This negative
result is so far poorly understood, but has been attributed to the
propensity of LSTM decoders to ignore conditioning information from the encoder. In this paper, we experiment with a new type of decoder for VAE: a dilated CNN.
By changing the decoder's dilation architecture, we control the size of context
from previously generated words. In experiments, we find that there is a trade-off
between contextual capacity of the decoder and effective use of encoding information. We show that when carefully managed, VAEs can outperform LSTM language models. We demonstrate perplexity gains on two datasets, representing the first positive language modeling result with VAE.
Further, we conduct an in-depth investigation of the use of VAE
(with our new decoding architecture) for semi-supervised and unsupervised
labeling tasks, demonstrating gains over several strong baselines.
\end{abstract}


\section{Introduction}
Generative models play an important role in NLP, both in their use as language models and because of their ability to effectively learn from unlabeled data.
By parameterzing generative models using neural nets, recent work has proposed model
classes that are particularly expressive and can pontentially model a wide range of phenomena in language and other modalities. We focus on a specific
instance of this class: the variational autoencoder\footnote{The name VAE is often used to refer to both a model class and an associated inference procedure.} (VAE)~\cite{kingma2013auto}.




The generative story behind the VAE (to be described in detail in the next section)
is simple: First, a continuous latent representation is sampled from a multivariate Gaussian. Then, an output is sampled from a distribution parameterized by a neural decoder, conditioned on the
latent representation. The latent representation (treated as a latent variable during training) is intended to give the model more expressive capacity when compared with simpler
neural generative models--for example, conditional language models. The choice of decoding architecture and final output distribution, which connect the latent representation to output, depends on the kind of data being modeled. The VAE owes its name to an accompanying variational technique ~\cite{kingma2013auto} that has been successfully used to train such models on image data ~\cite{gregor2015draw,salimans2015markov,yan2016attribute2image}.


The application of VAEs to text data has been far less\ successful~\cite{bowman2015generating,miao2016neural}. The obvious choice for decoding architecture for a textual VAE is an LSTM, a typical workhorse in NLP. However, \citet{bowman2015generating} found that using an LSTM-VAE for text modeling yields higher perplexity on held-out data than using an LSTM language model. In particular, they observe that the LSTM decoder in VAE does not make effective use of the latent representation during training and, as a result, VAE collapses into a simple language model.
Related work \citep{miao2016neural,larochelle2012neural,mnih2014neural} has used simpler decoders that model text as a bag of words. Their results indicate better use of latent representations, but their decoders cannot effectively model longer-range dependencies in text and thus underperform in terms of final perplexity.

Motivated by these observations, we hypothesize that the contextual capacity of the decoder plays an important role in whether VAEs effectively condition on the latent representation when trained on text data. We propose the use of a dilated CNN as a decoder in VAE, inspired by the recent
success of using CNNs for audio, image and language modeling
~\cite{van2016wavenet, kalchbrenner2016neural, van2016conditional}. In contrast with prior work where extremely large CNNs are used, we exploit the dilated CNN for its flexibility in varying the amount of conditioning context.
In the two extremes, depending on the choice of dilation, the CNN decoder can reproduce a simple MLP using a bags of words representation of text, or can reproduce the long-range dependence of recurrent architectures (like an LSTM) by conditioning on the entire history. Thus, by choosing a dilated CNN as the decoder, we are able to conduct experiments where we vary contextual capacity, finding a sweet spot where the decoder can accurately model text but does not yet overpower the latent representation.

We demonstrate that when this trade-off is correctly managed, textual VAEs can perform substantially better than simple LSTM language models, a finding consistent with recent image modeling experiments
using variational lossy autoencoders~\cite{chen2016variational}.
We go on to show that VAEs with carefully selected CNN decoders can be quite effective for semi-supervised classification and unsupervised clustering,
outperforming several strong baselines (from ~\cite{dai2015semi}) on both text categorization and sentiment analysis.

Our contributions are as follows: First, we propose the use of a dilated CNN as a new decoder
for VAE. We then empirically evaluate several dilation architectures with different capacities, finding that
reduced contextual capacity leads to stronger reliance on latent representations.
By picking a decoder with suitable contextual capacity, we find our VAE performs better than
LSTM language models on two data sets.
We also explore the use of dilated CNN VAEs for semi-supervised
classification and find they perform better than strong baselines from
~\cite{dai2015semi}. Finally, we verify that the same framework can be used
effectively for unsupervised clustering.


\section{Model}
In this section, we begin by providing background on the use of variational autoencoders for language modeling.
Then we introduce the dilated CNN architecture that we will use as a new decoder for VAE in experiments.
Finally, we describe the generalization of VAE that we will use to conduct experiments on semi-supervised classification.


\subsection{Background on Variational Autoencoders}
Neural language models~\cite{mikolov2010recurrent} typically generate each token
 conditioned on the entire history of previously generated tokens:

State-of-the-art language models often parametrize these conditional probabilities
using RNNs, which compute an evolving hidden state
over the text which is used to predict each . This approach, though effective in modeling text, does not explicitly model variance in higher-level properties of entire utterances (e.g. topic or style) and thus can have difficulty with heterogeneous datasets.

\citet{bowman2015generating} propose a different approach to generative text modeling inspired by related work on vision \cite{kingma2013auto}. Instead of directly modeling the joint probability 
as in Equation~\ref{eq:lm}, we specify a generative process for which  is a marginal distribution. Specifically, we first generate a continuous latent vector representation  from a multivariate Gaussian prior , and then generate the text sequence 
from a conditional distribution  parameterized using a neural net (often called the generation model or decoder). Because this model incorporates a latent variable that modulates the entire generation of each whole utterance, it may be better able to capture high-level sources of variation in the data. Specifically, in contrast with Equation \ref{eq:lm}, this generating distribution conditions
on latent vector representation :


To estimate model parameters  we
would ideally like to maximize the marginal probability
.
However, computing this marginal is intractable for many decoder choices. Thus, the following variational lower bound is often used as an objective \cite{kingma2013auto}:

Here,  is an approximation to the true posterior (often called the recognition model or encoder) and is parameterized by . Like the decoder, we have a choice of neural architecture to parameterize the encoder. However, unlike the decoder, the choice of encoder does not change the model class -- it only changes the variational approximation used in training, which is a function of both the model parameters  {\emph and} the approximation parameters . Training seeks to optimize these parameters jointly using stochastic gradient ascent. A final wrinkle of the training procedure involves a stochastic approximation to the gradients of the variational objective (which is itself intractable). We omit details here, noting only that the final distribution of the posterior approximation  is typically assumed to be Gaussian so that a
re-parametrization trick can be used, and refer readers to ~\cite{kingma2013auto}.

\subsection{Training Collapse with Textual VAEs}

Together, this combination of generative model and variational inference procedure are often referred to as a variational autoencoder (VAE).
We can also view the VAE as a regularized version of the autoencoder. Note, however, that while VAEs are valid probabilistic models whose likelihood can be evaluated on held-out data, autoencoders are not valid models. If only the first
term of the VAE variational bound  is used as an objective,
the variance of the posterior probability 
will become small and the training procedure reduces to an autoencoder.
It is the KL-divergence term,
, that discourages the VAE memorizing each  as a single
latent point.

While the KL term is critical for training VAEs, historically, instability on text has been evidenced by the KL term becoming vanishingly small during training, as observed by ~\citet{bowman2015generating}. When the training procedure collapses in this way, the result is an encoder that has duplicated the Gaussian prior (instead of a more interesting posterior), a decoder that completely ignores the latent variable , and a learned model that reduces to a simpler language model. We hypothesize that this collapse condition is related to the contextual capacity of the decoder architecture. The choice encoder and decoder depends on the type of data. For images, these are typically MLPs or CNNs. LSTMs have been used for text, but have resulted in training collapse as discussed above \cite{bowman2015generating}. Here, we propose to use a dilated CNN as the decoder instead. In one extreme, when the effective contextual width of a CNN is very large, it resembles the behavior of LSTM. When the width is very small, it behaves like a bag-of-words model. The architectural flexibility of dilated CNNs allows us to change the contextual capacity and conduct experiments to validate our hypothesis:
decoder contextual capacity and effective use of encoding information are directly related.
 We next describe the details of our decoder.





\begin{figure}[!t]
  \centering
  \begin{subfigure}[t]{0.8\linewidth}
    \includegraphics[width=\linewidth]{vae_cnn.pdf}
      \caption{VAE training graph using a dilated CNN decoder.}
      \label{fig:vae}
  \end{subfigure}
  \begin{subfigure}[t]{0.8\linewidth}
      \centering
      \includegraphics[width=\linewidth]{cnn_dilation.pdf}
      \caption{Digram of dilated CNN decoder.}
      \label{fig:cnn}
  \end{subfigure}
  \caption{Our training and model architectures for textual VAE using a dilated CNN decoder.}
\end{figure}

\subsection{Dilated Convolutional Decoders \label{cnn_sec}}
The typical approach to using CNNs used for text generation~\cite{kalchbrenner2016neural} is
similar to that used for images~\cite{krizhevsky2012imagenet,he2016deep},
but with the convolution applied in one dimension. We take this approach here in defining our decoder.
\0.2cm]
{\bf Dilation}: Dilated convolution~\cite{yu2015multi} was introduced to greatly increase the
effective receptive field size without increasing the computational cost. With
dilation , the convolution is applied so that  inputs are skipped each step.
Causal convolution can be seen a special case with . With dilation, the effective
receptive size grows exponentially with network depth. In
Figure~\ref{fig:cnn}, we show dilation of sizes of 1 and 2 in the first and second layer, respectively. Suppose the dilation size in the -th layer is  and we use the same filter
size  in all layers, then the effective filter size is .
The dilations are typically set to double every layer ,
so the effective receptive field size can grow exponentially. Hence, the contextual capacity
of a CNN can be controlled across a greater range by manipulating the filter size, dilation size and network depth. We use this approach in experiments.
\0.2cm]
{\bf Overall architecture}: Our VAE architecture is shown in Figure~\ref{fig:vae}.
We use LSTM as the encoder to get the posterior probability
, which we assume to be
diagonal Gaussian. We parametrize the mean  and
variance  with LSTM output. We sample  from
, the decoder is conditioned on the sample by
concatenating  with every word embedding of the decoder input.

\subsection{Semi-supervised VAE}
In addition to conducting language modeling experiments, we will also conduct experiments on semi-supervised classification of text using our proposed decoder.
In this section, we briefly review
semi-supervised VAEs of \cite{kingma2014semi} that incorporate discrete labels as additional variables.
Given the labeled set
 and the unlabeled set
, \cite{kingma2014semi} proposed a model
whose latent representation contains continuous vector  and discrete label
:

The semi-supervised VAE fits a discriminative network
, an inference network
 and a generative network
 jointly as part of optimizing a variational lower
bound similar that of basic VAE. For labeled data ,
this bound is:

For unlabeled data , the label is treated as a latent
variable, yielding:
{\small

}
Combining the labeled and unlabeled data terms, we have the overall objective
as:

where  controls the trade off between generative and
discriminative terms.
\
  y_{i} = \frac{\exp((\log (\pi_{i}) + g_{i}) / \tau)} {\sum_{j=1}^{c}
  \exp((\log(\pi_{j}) + g_{j})/\tau)},

  \text{KL}_{\mathbf{y}} = \max(\gamma, \text{KL}(q(\mathbf{y}|\mathbf{x}) | p(\mathbf{y})).
  \label{eq:gamma}
0.2cm]
{\bf Latent representation visualization:} In order to visualize the latent representation,
we set the dimension of  to be 2
and plot the mean of posterior probability , as shown in
Figure~\ref{fig:code}. We can see distinct different characteristics of topic
and sentiment representation. In Figure~\ref{fig:yahoo_code}, we can see that documents
of different topics fall into different clusters, while in
Figure~\ref{fig:yelp_code}, documents of different ratings
form a continuum, they lie continuously on the x-axis as the review rating increases.


\begin{table}[!thp]
\small
  \centering
  \begin{tabular}{l r l}
    Model & ACCU & NLL (KL) \\
    \toprule
    LSTM-VAE-Semi & 51.9 & 345.5 (9.3) \\
    SCNN-VAE-Semi & {\bf 65.5} & 335.7 (10.4) \\
    MCNN-VAE-Semi & 64.6 & 332.8 (7.2) \\
    LCNN-VAE-Semi & 57.2 & {\bf 331.3} (2.7) \\
    \bottomrule
  \end{tabular}
  \caption{Semi-supervised VAE ablation results on Yahoo.
  We report both the NLL and classification
  accuracy of the test data. Accuracy is in percentage.
  Number of labeled samples is fixed to be 500.}
  \label{tab:yahoosemi_compare}
\end{table}

\begin{table*}[!thp]
\small
  \centering
  \scalebox{0.95}{
  \begin{subtable}{0.48\textwidth}
    \begin{tabular}{l r r r r}
      Model & 100 & 500 & 1000 & 2000 \\
      \toprule
      LSTM & 10.7 & 11.9 & 14.3 & 23.1 \\
      LA-LSTM~\cite{dai2015semi} & 20.8 & 42.2 & 50.4 & 54.7 \\
      LM-LSTM~\cite{dai2015semi} & 46.9 & 61.3 & 63.9 & 65.6 \\
      \midrule
      SCNN-VAE-Semi & 55.4 & 65.6 & 66.0 & 65.8\\
      SCNN-VAE-Semi+init & {\bf 63.8} & {\bf 65.4} & {\bf 66.6} & {\bf 67.4} \\
      \bottomrule
    \end{tabular}
    \caption{Yahoo}
    \label{tab:yahoosemi}
  \end{subtable}
  \quad
  \begin{subtable}{0.48\textwidth}
    \begin{tabular}{l r r r r}
      Model & 100 & 500 & 1000 & 2000 \\
      \toprule
      LSTM & 22.6 & 25.4 & 27.9 & 29.9 \\
      LA-LSTM~\cite{dai2015semi} & 35.2 & 46.4 & 49.8 & 52.2 \\
      LM-LSTM~\cite{dai2015semi} & 46.9 & 54.1 & 57.2 & 57.7 \\
      \midrule
      SCNN-VAE-Semi & 51.4 & 53.5 & 55.3 & 57.4 \\
      SCNN-VAE-Semi+init & {\bf 52.6} & {\bf 57.3} & {\bf 58.9} & {\bf 59.8} \\
      \bottomrule
    \end{tabular}
    \caption{Yelp}
    \label{tab:yelpsemi}
  \end{subtable}
  }
  \caption{Semi-supervised VAE results on the test set, in percentage.
  LA-LSTM and LM-LSTM come from ~\cite{dai2015semi}, they denotes
  the LSTM is initialized with a sequence autoencoder and a language model.}
  \label{tab:semi}
  \vspace{-0.2cm}
\end{table*}

\subsection{Semi-supervised VAE results}
Motivated by the success of VAEs for language modeling, we continue to explore
VAEs for semi-supervised learning. Following that of~\cite{kingma2014semi},
we set the number of labeled samples to be 100, 500, 1000 and 2000 respectively.
\0.2cm]
{\bf Comparison with related methods}: We compare Semi-supervised VAE with the
methods from \cite{dai2015semi}, which represent the previous state-of-the-art for semi-supervised
sequence learning. \citet{dai2015semi} pre-trains a classifier by initializing the parameters
of a classifier with that of a language model or a sequence autoencoder. They find it improves
the classification accuracy significantly. Since SCNN-VAE-Semi performs the best
according to Table~\ref{tab:yahoosemi_compare}, we fix
the decoder to be SCNN in this part. The detailed comparison is in
Table~\ref{tab:semi}. We can see that semi-supervised VAE performs better than LM-LSTM and
LA-LSTM from~\cite{dai2015semi}. We also initialize the encoder of the VAE with parameters
from LM and find classification accuracy further improves.
We also see the advantage of SCNN-VAE-Semi
over LM-LSTM is greater when the number of labeled samples is smaller. The
advantage decreases as we increase the number of labeled samples. When we
set the number of labeled samples to be 25k, the SCNN-VAE-Semi achieves an accuracy of 70.4,
which is similar to LM-LSTM with an accuracy of 70.5. Also, SCNN-VAE-Semi performs better on Yahoo
data set than Yelp data set. For Yelp, SCNN-VAE-Semi is a little bit worse than LM-LSTM
if the number of labeled samples is greater than 100, but becomes better when we initialize the encoder.
Figure~\ref{fig:yelp_code} explains this observation. It shows the documents are coupled
together and are harder to classify. Also, the latent representation contains
information other than sentiment, which may not be useful for classification.
\subsection{Unsupervised clustering results}
\begin{table}[!thp]
  \centering
  \begin{tabular}{l r}
    Model & ACCU  \\
    \toprule
    LSTM + GMM &  25.8  \\
    SCNN-VAE + GMM  & 56.6 \\
    SCNN-VAE + init + GMM & 57.0 \\
    \midrule
    SCNN-VAE-Unsup + init & {\bf 59.9} \\
    \bottomrule
  \end{tabular}
  \caption{Unsupervised clustering results for Yahoo data set. We run each
    model 10 times and report the best results. LSTM+GMM means we extract the
    features from LSTM language model. SCNN-VAE + GMM means we use the mean of
     as the feature. SCNN-VAE + init + GMM means
    SCNN-VAE is trained with encoder initialization.}
  \label{tab:yahoosemi_compare}
\end{table}
\begin{table*}[!thbp]
  \centering
  \small
  \begin{tabular}{r  p{12cm}}
    \toprule
    {\bf 1 star} & the food was good but the service was horrible .
    took forever to get our food .
    we had to ask twice for our check after we got our food . will not return .  \\
    {\bf 2 star} & the food was good , but the service was terrible .
    took forever to get someone to take our drink order .
    had to ask 3 times to get the check . food was ok , nothing to write about . \\
    {\bf 3 star} & came here for the first time last night .
    food was good . service was a little slow . food was just ok . \\
    {\bf 4 star} & food was good , service was a little slow ,
    but the food was pretty good . i had the grilled chicken sandwich and it
    was really good . will definitely be back !
 \\
    {\bf 5 star} & food was very good , service was fast and friendly .
    food was very good as well . will be back !                     \\
    \bottomrule
  \end{tabular}
  \caption{Text generated by conditioning on sentiment label.}
  \label{tab:yelp_sample_paper}
\end{table*}
We also explored using the same framework for unsupervised
clustering. We compare with the baselines that extract the feature with
existing models and then run Gaussian Mixture Model (GMM) on these features. We
find empirically that simply using the features does not perform well since the
features are high dimensional. We run a PCA on these features, the dimension of
PCA is selected from [8, 16, 32]. Since GMM can easily get stuck in poor local
optimum, we run each model ten times and report the best result.
We find directly optimizing  does not perform well for unsupervised
clustering and we need to initialize the encoder with LSTM language model. The
model only works well for Yahoo data set. This is potentially because
Figure~\ref{fig:yelp_code} shows that sentiment latent representations does
not fall into clusters.  in Equation~\ref{eq:gamma} is a sensitive parameter,
we select it from the range between 0.5 and 1.5 with an interval of 0.1.
We use the following evaluation protocol~\cite{makhzani2015adversarial}: after we
finish training, for cluster , we find out the validation sample  from cluster 
that has the best  and assign the label of  to all samples in
cluster . We then compute the test accuracy based on this assignment.
The detailed results are in Table~\ref{tab:yahoosemi_compare}. We can see
SCNN-VAE-Unsup + init performs better than other baselines. LSTM+GMM performs very
bad probably because the feature dimension is 1024 and is too high for
GMM, even though we already used PCA to reduce the dimension.
\0.2cm]
{\bf Conditional text generation}
With the semi-supervised VAE, we are able to generate text conditional on the
label. Due to space limitation, we only show one example of generated reviews
conditioning on review rating in Table~\ref{tab:yelp_sample_paper}.
For
each group of generated text, we fix  and vary the label , while picking  via beam search with a beam size of 10.

\section{Related work}
Variational inference via the re-parameterization trick was initially proposed
by~\cite{kingma2013auto, rezende2014stochastic} and since then, VAE has been
widely adopted as generative model for images~\cite{gregor2015draw,
  yan2016attribute2image, salimans2015markov, gregor2016towards, hu2017unifying}.

Our work is in line with previous works on combining variational inferences
with text
modeling~\cite{bowman2015generating,miao2016neural,serban2016hierarchical,
  zhang2016variational, hu2017controllable}. \cite{bowman2015generating} is the first work to
combine VAE with language model and they use LSTM as the decoder and find some
negative results. On the other hand, \cite{miao2016neural} models text as bag
of words, though improvement has been found, the model can not be used to
generate text.  Our work fills the gaps between
them. \cite{serban2016hierarchical, zhang2016variational} applies variational
inference to dialogue modeling and machine translation and found some
improvement in terms of generated text quality, but no language modeling
results are
reported. \cite{chung2015recurrent,bayer2014learning,fraccaro2016sequential}
embedded variational units in every step of a RNN, which is different from our model
in using global latent variables to learn high level features.

Our use of CNN as decoder is inspired by recent success of PixelCNN model for
images~\cite{van2016conditional}, WaveNet for audios~\cite{van2016wavenet},
Video Pixel Network for video modeling~\cite{kalchbrenner2016video} and ByteNet
for machine translation~\cite{kalchbrenner2016neural}. But in contrast to those
works showing using a very deep architecture leads to better performance, CNN
as decoder is used in our model to control the contextual capacity, leading to better performance.

Our work is closed related the recently proposed variational lossy
autoencoder~\cite{chen2016variational} which is used to predict image
pixels. They find that conditioning on a smaller window of a pixels leads to better
results with VAE, which is similar to our finding.
Much~\cite{rezende2015variational, kingma2016improving, chen2016variational} has been done to
come up more powerful prior/posterior distribution representations with techniques such as
normalizing flows. We treat this as one of our future works.
This work is largely orthogonal and could be potentially
combined with a more effective choice of decoder to yield additional gains.

There is much previous work exploring unsupervised sentence encodings, for example
skip-thought vectors~\cite{kiros2015skip}, paragraph
vectors~\cite{le2014distributed}, and sequence
autoencoders~\cite{dai2015semi}. \cite{dai2015semi} applies a pretrained
model to semi-supervised classification and find significant gains, we use this
as the baseline for our semi-supervised VAE.


\section{Conclusion}

We showed that by controlling the decoder's contextual capacity in VAE, we can improve performance on both language modeling and semi-supervised classification tasks by preventing a degenerate collapse of the training procedure. These results indicate that more carefully characterizing decoder capacity and understanding how it relates to common variational training procedures may represent important avenues for unlocking future unsupervised problems.



\bibliography{bible}
\bibliographystyle{icml2017}

\appendix
\begin{table*}[!thbp]
  \centering
  \small
  \begin{tabular}{r  p{12cm}}
    \toprule
    {\bf Society} & do you think there is a god ? \\
    {\bf Science} & how many orbitals are there in outer space ? how many orbitals are there in the solar system ? \\
    {\bf Health} & what is the difference between \_UNK and \_UNK \\
    {\bf Education} & what is the difference between a computer and a \_UNK ? \\
    {\bf Computers} & how can i make flash mp3 files ? i want to know how to make a flash video so i can upload it to my mp3 player ? \\
    {\bf Sports} & who is the best soccer player in the world ? \\
    {\bf Business} & what is the best way to make money online ? \\
    {\bf Music} & who is the best artist of all time ? \\
    {\bf Relationships} & how do i know if a guy likes me ? \\
    {\bf Politics} & what do you think about Iran ? \\
    \midrule
    {\bf Society} & what is the meaning of life ? \\
    {\bf Science} & what is the difference between kinetic energy and heat ? \\
    {\bf Health} & what is the best way to get rid of migraine headaches ? \\
    {\bf Education} & what is the best way to study for a good future ? \\
    {\bf Computers} & what is the best way to install windows xp home edition ? \\
    {\bf Sports} & who do you think will win the super bowl this year ? \\
    {\bf Business} & i would like to know what is the best way to get a good paying
                     job ? \\
    {\bf Entertainment} & what do you think is the best movie ever ? \\
    {\bf Relationships} & what is the best way to get over a broken heart ? \\
    {\bf Politics} & what do you think about the war in iraq ? \\
    \midrule
    {\bf Society} & what would you do if you had a million dollars ? \\
    {\bf Mathematics} & i need help with this math problem ! \\
    {\bf Health} & what is the best way to lose weight ? \\
    {\bf Education} & what is the best college in the world ? \\
    {\bf Computers} & what is the best way to get a new computer ? \\
    {\bf Sports} & who should i start ? \\
    {\bf Business} & what is the best way to get a good paying job ? \\
    {\bf Entertainment} & who do you think is the hottest guy in the world ? \\
    {\bf Relationships} & what should i do ? \\
    {\bf Politics} & who do you think will be the next president of the united states ? \\
    \midrule
    {\bf Society} & do you believe in ghosts ? \\
    {\bf Science} & why is the sky blue ? \\
    {\bf Health} & what is the best way to get rid of a cold ? \\
    {\bf Reference} & what do you do when you are bored ? \\
    {\bf Computers} & why ca n't i watch videos on my computer ? when i try to watch
                      videos on my computer , i ca n't get it to work on my computer
                      . can anyone help ? \\
    {\bf Sports} & what do you think about the \_UNK game ? \\
    {\bf Business} & what is the best way to get a job ? \\
    {\bf Entertainment} & what is your favorite tv show ? \\
    {\bf Relationships} & how do you know when a guy likes you ? \\
    {\bf Politics} & what do you think about this ? \\
    \midrule
    {\bf Society} & what is the name of the prophet muhammad ( pbuh ) ? i do n't know if
                    he is a jew or not . \\
    {\bf Science} & where can i find a picture of the \_UNK \_UNK \_UNK \_UNK ? i need to know
                    the name of the insect that has the name of the whale . \\
    {\bf Health} & what is the best way to get rid of a \_UNK mole ?  \\
    {\bf Reference} & does anyone know where i can find info on \_UNK \_UNK \_UNK ? i am
                      looking for the name of the \_UNK \_UNK . \\
    {\bf Computers} & does anyone know where i can find a picture of a friend 's cell
                      phone ? \\
    {\bf Sports} & does anyone know where i can find a biography of \_UNK ? \\
    {\bf Business} & does anyone know where i can find a copy of the \_UNK ? \\
    {\bf Music} & does anyone know the name of the song and who sings it ? \\
    {\bf Relationship} & how do i tell my boyfriend that i love him ? he is my best
                         friend , but i dont know how to tell him . please help ! ! !
                         ! ! ! \\
    {\bf Politics} & where is osama bin laden ? \\
    \bottomrule
  \end{tabular}
  \caption{Text generated by conditioning on topic label.}
  \label{tab:yahoo_sample}
\end{table*}

\begin{table*}[!thbp]
  \centering
  \small
  \begin{tabular}{r  p{12cm}}
    \toprule
    {\bf 1 star} & the food is good , but the service is terrible . i have been
                   here three times and each time the service has been horrible
                   . the last time we were there , we had to wait a long time
                   for our food to come out . when we finally got our food ,
                   the food was cold and the service was terrible . i will not
                   be back . \\
    {\bf 2 star} & this place used to be one of my favorite places to eat in
                    the area . \\
    {\bf 3 star} & i 've been here a few times , and the food has always been
                    good . \\
    {\bf 4 star} & this is one of my favorite places to eat in the phoenix
                    area . the food is good , and the service is friendly . \\
    {\bf 5 star} & my husband and i love this place . the food is great , the
                    service is great , and the prices are reasonable . \\
    \midrule

    {\bf 1 star} & this is the worst hotel i have ever been to . the room was dirty
                   , the bathroom was dirty , and the room was filthy .\\
    {\bf 2 star} & my husband and i decided to try this place because we had heard
                    good things about it so we decided to give it a try . the
                    service was good , but the food was mediocre at best . \\
    {\bf 3 star} & we came here on a saturday night with a group of friends . we
                    were seated right away and the service was great . the food was
                    good , but not great . the service was good and the atmosphere
                    was nice . \\
    {\bf 4 star} & my husband and i came here for brunch on a saturday night . the
                    place was packed so we were able to sit outside on the patio
                    . we had a great view of the bellagio fountains and had a great
                    view of the bellagio fountains . we sat at the bar and had a
                    great view of the bellagio fountains . \\
    {\bf 5 star} & my husband and i came here for the first time last night and
                    had a great time ! the food was amazing , the service was great
                    , and the atmosphere was perfect . we will be back ! \\
    \midrule

    {\bf 1 star} & this is the worst place i have ever been to . i will never go
                   back . \\
    {\bf 2 star} & i was very disappointed with the quality of the food and the
                   service . i will not be returning . \\
    {\bf 3 star} & this was my first time at this location and i have to say it
                   was a good experience . \\
    {\bf 4 star} & this is a great place to grab a bite to eat with friends or
                   family . \\
    {\bf 5 star} & i am so happy to have found a great place to get my nails done
                   . \\
    \midrule
    {\bf 1 star} & my wife and i have been going to this restaurant for years
                   . the last few times i have been , the service has been
                   terrible . the last time we were there , we had to wait a
                   long time for our food to arrive . the food is good , but
                   not worth the wait . \\
    {\bf 2 star} & the food is good , but the service leaves something to be
                   desired . \\
    {\bf 3 star} & i have been here a few times . the food is consistently
                   good , and the service is good . \\
    {\bf 4 star} & my wife and i have been here a few times . the food is
                   consistently good , and the service is friendly . \\
    {\bf 5 star} & my husband and i have been coming here for years . the food
                   is consistently good and the service is always great . \\
    \midrule
    {\bf 1 star} & the food was good but the service was terrible . we had to
                   wait 45 minutes for our food to come out and it was cold . i
                   will not be back .\\
    {\bf 2 star} & the food was good but the service was
                   terrible . we had a party of 6 and the food
                   took forever to come out . the food was good
                   but not worth the price . \\
    {\bf 3 star} & the food was good but the service was a little slow . we
                   had to wait a while for our food and it was n't even busy
                   . \\
    {\bf 4 star} & i have been here a few times and have never been
                   disappointed . the food was great and the service was great
                   . we will be back . \\
    {\bf 5 star} & my husband and i have been here a few times and have never
                   been disappointed . the food was great and the service was
                   great . i will definitely be back ! \\
    \midrule
    {\bf 1 star} & if i could give this place zero stars i would . i do not
                   recommend this place to anyone ! \\
    {\bf 2 star} & i do n't know what all the hype is about this place , but i
                   do n't think i will be back . \\
    {\bf 3 star} & i do n't know what all the hype is about this place , but i
                   do n't think i 'll be back .\\
    {\bf 4 star} & i 've been here a couple of times and have never been
                   disappointed . the food is fresh , the service is friendly
                   , and the prices are reasonable . \\
    {\bf 5 star} & this is the best ramen i 've ever had in my life , and i 've
                   never had a bad meal here ! \\
    \midrule
    {\bf 1 star} & this is the worst company i have ever dealt with . they do
                   n't know what they are doing . \\
    {\bf 2 star} & this is the worst buffet i have ever been to in my life
                   . the food was just ok , nothing to write home about . \\
    {\bf 3 star} & not a bad place to stay if you 're looking for a cheap
                   place to stay . \\
    {\bf 4 star} & this is a great place to stay if you 're looking for a
                   quick bite . \\
    {\bf 5 star} & i love this place ! the staff is very friendly and helpful
                   and the price is right ! \\
    \bottomrule
  \end{tabular}
  \caption{Text generated by conditioning on sentiment label.}
  \label{tab:yelp_sample}
\end{table*}



\end{document}
