\section{Experiments} \label{sec:experiments}
In this section, we 
conduct extensive experiments to show that our model outperforms many strong baselines
and dramatically reduces the number of parameters and computation for efficient multi-task learning (Tables~\ref{table:quant_result_nyu_v2_2}-\ref{table:quant_result_taskonomy}). 
Interestingly, we discover that unlike hard-parameter sharing models, our learned policy often prefers to have task-specific blocks in ResNet's  layers rather than the last few layers (Figure~\ref{fig:policy_visualization}: (a)). 
Moreover, we also show that reasonable task correlation can be obtained from our learned task-specific policy logits (Figure~\ref{fig:policy_visualization}: (b), Figure~\ref{fig:relation_domainnet}). 

\noindent\textbf{Datasets and Tasks.} We evaluate the performance of our approach using several standard  datasets, namely \textbf{NYU v2}~\cite{Silberman:ECCV12} (used for joint Semantic Segmentation and Surface Normal Prediction as in~\cite{Misra16,gao2019nddr}, as well as these two tasks together with Depth Prediction as in~\cite{liu2019end}),  \textbf{CityScapes}~\cite{cordts2016cityscapes}, considering joint Semantic Segmentation~\cite{chen2017deeplab, tao2020hierarchical, hu2020real, Hu_2020_CVPR} and Depth Prediction as in~\cite{liu2019end},
and \textbf{Tiny-Taskonomy}~\cite{zamir2018taskonomy},
with 5 sampled representative tasks (Semantic Segmentation, Surface Normal Prediction, Depth Prediction, Keypoint Detection and Edge Detection)
as in \cite{standley2019tasks}. We also test \textit{AdaShare} via performing the same task in different data domains such as image classification on 6 domains in \textbf{DomainNet}~\cite{peng2019moment} and text classification on 10 publicly available datasets from~\cite{chen2018exploring}.
More details on the datasets and tasks are included in the supplementary material.


\noindent\textbf{Baselines.} We compare our approach with following baselines. First, we consider a \textbf{Single-Task} baseline, where we train each task separately using a task-specific backbone and a task-specific head for each task.
Second, we use a popular \textbf{Multi-Task} baseline, in which all tasks share the backbone network but have separate task-specific heads at the end. Finally, we compare our method with state-of-the-art multi-task learning approaches, including \textbf{Cross-Stitch Networks}~\cite{Misra16} (CVPR'16), \textbf{Sluice Networks}~\cite{ruder122017sluice} (AAAI'19), and \textbf{NDDR-CNN}~\cite{gao2019nddr} (CVPR'19), which adopt several feature fusion layers between task-specific backbones, 
\textbf{MTAN}~\cite{liu2019end} (CVPR'19), which introduces task-specific attention modules over the shared backbone, as well as \textbf{DEN}~\cite{ahn2019deep} (ICCV'19), which uses an additional network to learn channel-wise policy for each task with RL. 
We use the same backbone and task-specific heads for all methods (including our proposed approach) for a fair comparison. 


\noindent\textbf{Evaluation Metrics.}\label{sec:evaluation_metrics}
In both NYU v2 and CityScapes, 
Semantic Segmentation is evaluated via mean Intersection over Union (mIoU) and Pixel Accuracy (Pixel Acc). For Surface Normal Prediction, we use mean and median angle distances between the prediction and ground truth of all pixels (the lower the better). We also compute the percentage of pixels whose prediction is within the angles of ,  and  to the ground truth~\cite{eigen2015predicting} (the higher the better).
For Depth Prediction, we compute absolute and relative errors as the evaluation metrics (the
lower the better) and measure the relative difference between the prediction and ground truth via the percentage of  within threshold ,  and ~\cite{eigen2014depth}  (the higher the better).  In Tiny-Taskonomy,
we compute the task-specific loss on test images as the performance measurement for a given task, as in~\cite{zamir2018taskonomy,standley2019tasks}. For image classification and text recognition, we report classification accuracy for each domain/dataset. Instead of reporting the absolute task performance with multiple metrics for each task , we follow~\cite{maninis2019attentive} and report a single relative performance  with respect to the \textbf{Single-Task} baseline to clearly show the positive/negative transfer in different baselines:

where  if a lower value represents better for the metric  and 0 otherwise.
Finally, we average over all tasks to get overall performance .


\begin{table}
    \begin{center}
\caption{\small \textbf{NYU v2 2-Task Learning}. \textit{AdaShare} achieves the best performance (bold) on 4 out of 7 metrics and second best (underlined) on 1 metric across Semantic Segmentation and Surface Normal Prediction using less than 1/2 parameters of most baselines. : Semantic Segmentation; : Surface Normal Prediction.}
        \label{table:quant_result_nyu_v2_2}
        \resizebox{1\linewidth}{!}{
        \begin{tabular}{c|c|ccc|cccccc|c}
             \Xhline{3\arrayrulewidth} 
             \multirow{3}{*}{Model} &  \cellcolor{lightgray!15}  & \multicolumn{3}{c|}{ : Semantic Seg.} & \multicolumn{6}{c|}{: Surface Normal Prediction} &   \cellcolor{lightgray!15}\\
             \cline{3-11} 
              & \cellcolor{lightgray!15} & \multirow{2}{*}{mIoU } & \multirow{2}{*}{\makecell{Pixel \\ Acc }}&  \cellcolor{lightgray!15}  & \multicolumn{2}{c}{Error } & \multicolumn{3}{c|}{, within } &  \cellcolor{lightgray!15} & \cellcolor{lightgray!15} \\
             \cline{6-10}
            & \cellcolor{lightgray!15} \multirow{-3}{*}{\makecell{\# Params  \\ (\%) }} & & & \cellcolor{lightgray!15}  \multirow{-2}{*}{\makecell{  }} &  Mean & Median &  &   &  \multicolumn{1}{c|}{30 \degree}  & \cellcolor{lightgray!15}  \multirow{-2}{*}{\makecell{ }}  & \cellcolor{lightgray!15}  \multirow{-3}{*}{ \makecell{ }}   \\
           \Xhline{3\arrayrulewidth} 
            Single-Task & \cellcolor{lightgray!15} \ \ \ \ 0.0 & 27.8 & 58.5 & \cellcolor{lightgray!15} \ \ \ \ 0.0 & 17.3 & 14.4 & 37.2 & \textbf{73.7} & \textbf{85.1} &\cellcolor{lightgray!15}  \ \ 0.0 & \cellcolor{lightgray!15} \ \ 0.0 \\
            Multi-Task & \cellcolor{lightgray!15} \textbf{-\ 50.0} & 22.6 & 55.0 & \cellcolor{lightgray!15} -\ 12.3 & 16.9 & 13.7 & 41.0 & \underline{73.1} & \underline{84.3} & \cellcolor{lightgray!15} +\ 3.1 &\cellcolor{lightgray!15}  -\ 4.6\\
            Cross-Stitch & \cellcolor{lightgray!15} \ \ \ \ 0.0 & 25.3	& 57.4 &\cellcolor{lightgray!15} -\ \ \ 5.4 & 16.6 & 13.2 & 43.7 & 72.4 & 83.8 & \cellcolor{lightgray!15} +\ 5.3 & \cellcolor{lightgray!15} -\ 0.1 \\
            Sluice & \cellcolor{lightgray!15} \ \ \ \ 0.0 & 26.6 & 59.1 &\cellcolor{lightgray!15} -\ \ \ 1.6 & 16.6 & 13.0 & 44.1 & 73.0 & 83.9 &\cellcolor{lightgray!15}  \underline{+\ 6.0} &\cellcolor{lightgray!15}  +\ 2.2\\
            NDDR-CNN & \cellcolor{lightgray!15} + \ \ 6.5 & 28.2	& 60.1&\cellcolor{lightgray!15} +\ \ \ 2.1 & 16.8 & 13.5 & 42.8 & 72.1 
            & 83.7 & \cellcolor{lightgray!15} +\ 4.1 & \cellcolor{lightgray!15} +\ 3.1 \\
            MTAN &\cellcolor{lightgray!15} +\ 23.5 & \underline{29.5}	& \underline{60.8} &\cellcolor{lightgray!15}  \underline{+\ \ \ 5.0} &  \textbf{16.5} & 13.2 & \underline{44.1} & 72.8 & 83.7 & \cellcolor{lightgray!15} +\ 5.7 &\cellcolor{lightgray!15}  \underline{+\ 5.4} \\
            DEN & \cellcolor{lightgray!15} -\ 39.0 & 26.3 & 58.8 & \cellcolor{lightgray!15} -\ \ \  2.4 & 17.0 & 14.3 & 39.5 & 72.2 & 84.7 &\cellcolor{lightgray!15}  -\ 1.2 &\cellcolor{lightgray!15}  -\ 0.6 \\
            \hline
            \textit{AdaShare} &\cellcolor{lightgray!15} \textbf{-\ 50.0} & \textbf{29.6} & \textbf{61.3} & \cellcolor{lightgray!15} \textbf{+\ \ \ 5.6}& \underline{16.6} & \textbf{12.9} & \textbf{45.0} & 72.1 & 83.2 & \cellcolor{lightgray!15} \textbf{+\ 6.2} &\cellcolor{lightgray!15}  \textbf{+\ 5.9} \\
            \Xhline{3\arrayrulewidth} 
        \end{tabular}
        }
        \vspace{-3mm}
    \end{center}
\end{table} 
\noindent\textbf{Experimental Settings.} We use Deeplab-ResNet~\cite{chen2017deeplab} with atrous convolution, a popular architecture for pixel-wise prediction tasks, as our backbone and the ASPP~\cite{chen2017deeplab} architecture as task-specific heads. 
We adopt ResNet-34 (16 blocks) for most scenarios, and use ResNet-18 (8 blocks) for the simple 2-task scenario on the NYU v2 Dataset. For DomainNet, we use the original ResNet-34 as backbone and adopt VD-CNN~\cite{conneau2016very} for text classification.
Following~\cite{wu2019fbnet}, we use Adam~\cite{kingma2014adam} to update the policy distribution parameters and SGD to update the network parameters.
At the end of the policy training, we sample select-or-skip decisions from the policy distribution to be trained from scratch. 
Specifically, we sample 8 different network architectures from the learned policy and report the best re-train performance as our result. 
We use cross-entropy loss for Semantic Segmentation as well as classification tasks, and the inverse of cosine similarity between the normalized prediction and ground truth for Surface Normal Prediction. L1 loss is used for all other tasks. Pre-training depends on tasks and we observe that it improves the overall performance of \textit{AdaShare} by 11.3\% in NYUv2 3-Task learning. However, to get rid of the unfairness brought by different pretrained model, we start from scratch for a fair comparison among different methods in all our experiments. 

\begin{table}
\parbox[t]{.45\linewidth}{
  \begin{center}
        \caption{\small \textbf{CityScapes 2-Task Learning}. \\ : Semantic Segmentation, : Depth Prediction}
        \label{table:quant_result_cityscapes}
        \resizebox{\linewidth}{!}{
        \begin{tabular}{c|c|cc|c}
            \Xhline{3\arrayrulewidth} 
            Models & \makecell{ \# Params } &\makecell{   }&  \makecell{ } &  \makecell{   } \\
            \Xhline{3\arrayrulewidth} 
Multi-Task & \textbf{-50.0} & -3.7 & -0.5 & -2.1 \\
            Cross-Stitch & 0 & -0.1 & \textbf{+5.8} & \textbf{+2.8} \\
            Sluice & 0 & -0.8 & +4.0 & +1.6 \\
            NDDR-CNN & +3.5 & \underline{+1.3} & +3.3 & +2.3 \\
            MTAN & -20.5 & +0.5 & \underline{+4.8} & \underline{+2.7} \\
            DEN & -44.0 & -3.1 & -1.6 & -2.4 \\
            \hline
            \textit{AdaShare} & \textbf{-50.0} & \textbf{+1.8} & +3.8 & \textbf{+2.8}\\
            \Xhline{3\arrayrulewidth} 
        \end{tabular}}
        \begin{flushleft}
        {\scriptsize \textbf{Single-Task.} \textbf{Seg} - mIoU: 40.2, PAcc: 74,7; \textbf{Depth} - Abs.: 0.017, Rel.: 0.33, : 70.3, 86.3, 93.3.}
        \end{flushleft}
    \end{center}}
 \hfill
\parbox[t]{.515\linewidth}{
\begin{center}
        \caption{\small \textbf{NYU v2 3-Task Learning}. : Semantic Segmentation, : Surface Normal Pred.,  : Depth Pred.}
        \label{table:quant_result_nyu_v2_3}
         \resizebox{\linewidth}{!}{
        \begin{tabular}{c|c|ccc|c}
            \Xhline{3\arrayrulewidth} 
            Models & \makecell{ \# Params } &\makecell{   }& \makecell{    } & \makecell{   } &  \makecell{   } \\
            \Xhline{3\arrayrulewidth} 
Multi-Task &  \textbf{-66.7} & -7.6 & +7.5 & \underline{+5.2} & +1.7\\
            Cross-Stitch & 0.0 & -4.9 & +4.2 & + 4.7 & + 1.3\\
            Sluice & 0.0 & -8.4 & +2.9 & 4.1 & -0.5\\
            NDDR-CNN & +5.0 & -15.0 & +2.9 & -3.5 & -5.2 \\
            MTAN & +3.7 & \underline{-4.2} & \textbf{+8.7} & + 3.8  & \underline{+2.7}\\
            DEN & -62.7 & -9.9 & +1.7 & -35.2 & -14.5\\
            \hline
            \textit{AdaShare} &\textbf{ -66.7} &\textbf{ +8.8}&  \underline{+7.9} & \textbf{+10.1} & \textbf{+8.9}\\
            \Xhline{3\arrayrulewidth} 
        \end{tabular}}
        \begin{flushleft}
         {\scriptsize \textbf{Single-Task.} \textbf{Seg} - mIoU: 27.5, PAcc: 58.9; \textbf{SN} Mean: 17.5, Median: 15.2, : 34.9, 73.3, 85.7; \textbf{Depth} - Abs.: 0.62, Rel.: 0.25, : 57.9, 85.8, 95.}
         \end{flushleft}
    \end{center}
} \vspace{-5mm}
\end{table}
 










\begin{table}
\begin{center}
        \caption{\small \textbf{Tiny-Taskonomy 5-Task Learning}. : Semantic Segmentation, : Surface Normal Prediction, : Depth Prediction, : Keypoint Estimation, : Edge Estimation.} \vspace{0.5mm}
        \label{table:quant_result_taskonomy}
        \resizebox{0.7\linewidth}{!}{
        \begin{tabular}{c|c|ccccc|c}
            \Xhline{3\arrayrulewidth} 
            Models &  \makecell{ \# Params  } & \makecell{    } & \makecell{    }&\makecell{    }  & \makecell{    }& \makecell{     } & \makecell{     } \\
            \Xhline{3\arrayrulewidth} 


            Multi-Task & \textbf{-80.0} & - 3.7 & - 1.6 & \underline{- 4.5} & 0.0  & \underline{+ 4.2} &- 1.1 \\
            Cross-Stitch &  0.0 & \underline{+ 0.9} & - 4.0 & \textbf{0.0} & - 1.0 & - 2.4 &- 1.3 \\
            Sluice &  0.0 &  -3.7 & -1.7 & -9.1 & + 0.5 & + 2.4 & - 2.3\\
            NDDR-CNN & +8.2 & -4.2 & \underline{-1.0}&\underline{- 4.5} & + 2.0 & \underline{+ 4.2} & \underline{- 0.7}\\
            MTAN & -9.8  & -8.0 & - 2.8 & \underline{- 4.5} &  0.0 & + 2.8 & - 2.5\\
            DEN & -77.6 & -28.2 & - 3.0 & -22.7 & \underline{+ 2.5} & \underline{+ 4.2} & - 9.4\\
            \hline
            \textit{AdaShare} & \textbf{-80.0} & \textbf{+ 2.3} & \textbf{- 0.7} & \underline{- 4.5} & \textbf{+ 3.0} & \textbf{+ 5.7}& \textbf{+ 1.1}\\
            \Xhline{3\arrayrulewidth} 
        \end{tabular}}\\
        \vspace{0.5mm}
         {\scriptsize \textbf{Single-Task Learning:} Seg: 0.575; SN: 0.707; Depth: 0.022; Keypoint: 0.197; Edge: 0.212}
    \end{center}
 \vspace{-3mm}
\end{table}
%
 
\textbf{Quantitative Results.} 
Table~\ref{table:quant_result_nyu_v2_2}-\ref{table:quant_result_taskonomy} show the task performance in four different learning scenarios, namely NYU-v2 2-Task Learning, CityScapes 2-Task Learning, NYU-v2 3-Task Learning and Tiny-Taskonomy 5-Task Learning. We report all metrics and the relative performance of two tasks in NYU-v2 2-Task Learning (see Table~\ref{table:quant_result_nyu_v2_2}) and  report all metrics of Single-Task Baseline
and the relative performance of other methods due to the limited space in other cases (see Table~\ref{table:quant_result_cityscapes}-\ref{table:quant_result_taskonomy}). We recommend readers to refer to supplementary material for the full comparison of all metrics.

In NYU v2 2-Task Learning, \textit{AdaShare} outperforms all the baselines on 4 metrics out of 7 and achieves the second best on 1 metric (see Table~\ref{table:quant_result_nyu_v2_2}). Compared to Single-task, Cross-Stitch, Sluice, and NDDR-CNN, which use separate backbones for each task, our approach obtains superior task performance {with less than half of the number of parameters}. Moreover, \textit{AdaShare} also outperforms the vanilla Multi-Task baseline and DEN~\cite{ahn2019deep}, the most competitive approaches in terms of number of parameters, showing that it is able to pick an optimal combination of shared and task-specific knowledge with the same number of network parameters without using any additional policy network. 

Similarly, for other learning scenarios (Table~\ref{table:quant_result_cityscapes}-\ref{table:quant_result_taskonomy}), \textit{AdaShare} significantly outperforms all the baselines on overall relative performance while saving at least 50\%, up to 80\%, of parameters compared to most of the baselines.
\textit{AdaShare} also outperforms the Multi-Task baseline and DEN with similar parameter usage. Specifically, for Semantic Segmentation in NYU-v2 3-Task Learning, we observe that the performance of all the baselines are worse than the Single-Task baseline, showing that knowledge from Surface Normal Prediction and Depth Prediction should be carefully selected in order to improve the performance of Semantic Segmentation. In contrast, our approach is still able to improve the segmentation performance instead of suffering from the negative interference by the other two tasks. The same reduction in negative transfer is also observed in Surface Normal Prediction in Tiny-Taskonomy 5-Task Learning. However, our proposed approach \textit{AdaShare} still performs the best using less than 1/5 parameters of most of the baselines (Table~\ref{table:quant_result_taskonomy}).  

Moreover, our proposed \textit{AdaShare} also achieves better overall performance across the same task on different domains. For image classification on DomainNet~\cite{peng2019moment}, \textit{AdaShare} improves average accuracy over Multi-Task baseline on 6 different visual domains by 4.6\% (62.2\% vs. 57.6\%), with the maximum 16\% improvement in \textit{quickdraw} domain. For text classification task, \textit{AdaShare} outperforms the Multi-Task baseline by 7.2\% (76.1\% vs. 68.9\%) in average over 10 different NLP datasets~\cite{chen2018exploring} and maximally improves 27.8\% in \textit{sogou\_news} dataset. 

\begin{figure*}
\begin{center}
     \includegraphics[width=\linewidth]{figures/visualized_policy_taskonomy.pdf}
\end{center}
\vspace{-5pt}
   \caption{\small \textbf{Policy Visualization and Task Correlation}. (a) We visualize the learned policy logits  in Tiny-Taskonomy 5-Task learning. The darkness of a block represents the probability of that block selected for the given task. We also provide the select-and-skip decision  from our \textit{AdaShare}. In (b), we provide the task correlation, i.e. the cosine similarity between task-specific dataset. Two 3D tasks (Surface Normal Prediction and Depth Prediction) are more correlated and so as two 2D tasks (Keypoint Detection and Edge Detection).}
   \label{fig:policy_visualization}
\vspace{-12pt}
\end{figure*}

\begin{wrapfigure}{r}{0.3\textwidth}
  \begin{center}
    \includegraphics[width=0.3\textwidth]{figures/domainnet_relation.pdf}
  \end{center}
  \caption{\small \textbf{Task Correlation in DomainNet.} Similar tasks are more correlated, such as \textit{real} is closer to \textit{painting} than \textit{quickdraw}.}\label{fig:relation_domainnet}
\end{wrapfigure}

\noindent\textbf{Policy Visualization and Task Correlation.}
In Figure~\ref{fig:policy_visualization}: (a), we visualize our learned policy distributions (via logits) and the feature sharing policy in Tiny-Taskonomy 5-Task Learning (more visualizations are included in supplementary material). We also adopt the cosine similarity between task-specific policy logits as an effective representation of task correlations (Figure~\ref{fig:policy_visualization}: (b), Figure~\ref{fig:relation_domainnet}).
We have the following key observations. (a) The execution probability of each block for task  shows that not all blocks contribute to the task equally and it allows \textit{AdaShare} to mediate among tasks and decide task-specific blocks adaptive to the given task set. (b) Our learned policy prefers to have more blocks shared only among a sub-group of tasks in ResNet's  layers, where middle/high-level features, which are more task specific, are starting to get captured. By having blocks shared by a sub-group of tasks, \textit{AdaShare} encourages the positive transfer and relieves the effect of negative transfer, resulting in better overall performance. (c) We clearly observe that Surface Normal Prediction and Depth Prediction, two different 3D tasks, are more correlated, and that Keypoint prediction and Edge detection, two different 2D tasks are more correlated (see Figure~\ref{fig:policy_visualization}: (b)). Similarly, Figure~\ref{fig:relation_domainnet} shows that the domain \textit{real} is closer to \textit{painting} than \textit{quickdraw} in DomainNet. Both results follow the intuition that similar tasks should have similar execution distribution to share knowledge. 
Note that the cosine similarity purely measures the correlation between the normalized execution probabilities of different tasks, which is not influenced by the different optimization uncertainty of different tasks.


\noindent\textbf{Computation Cost (FLOPs).}
\textit{AdaShare} requires much less computation (FLOPs) as compared to existing MTL methods. E.g., in Cityscapes 2-task, Cross-stitch/Sluice, NDDR, MTAN, DEN, and AdaShare use 37.06G, 38.32G, 44.31G, 39.18G and 33.35G FLOPs and in NYU v2 3-task, they use 55.59G, 57.21G, 58.43G, 57.71G and 50.13G FLOPs, respectively. Overall, \textit{AdaShare} offers on average about 7.67\%-18.71\% computational savings compared to state-of-the-art methods over all the tasks while achieving better recognition accuracy with about 50\%-80\% less parameters.

\noindent\textbf{Ablation Studies.}
We present four groups of ablation studies in NYU-v2 3-Task learning to test our learned policy, the effectiveness of different training losses and optimization method (Table~\ref{table:ablate_nyu_v2_3}). 

\textbf{Comparison with Stochastic Depth.} Stochastic Depth~\cite{huang2016deep} randomly drops blocks as a regularization during the training and uses the full model in the inference. 
We compare \textit{AdaShare} with Stochastic Depth in our multi-task setting and observe that \textit{AdaShare} gains more improvement (overall 5.8\% improvement in Table~\ref{table:ablate_nyu_v2_3}), which distinguishes \textit{AdaShare} from a regularization technique.

\textbf{Comparison with Random Policy.}\label{sec:ablation_random}
We perform two different experiments such as `Random \#1` experiment, where we keep the same number of skipped blocks in total for all tasks and randomize their locations and `Random \#2, where we further force the same number of skipped blocks per task as \textit{AdaShare}.
We report the best performance among eight samples in each experiment. 
In Table~\ref{table:ablate_nyu_v2_3}, both random experiments improve the performance of Multi-Task baseline by incorporating shared and task-specific blocks in the model. Also, Random \#2 works better than Random \#1, which reveals that the number of blocks assigned to each task actually matters and our method makes a good prediction of it. Our model still outperforms Random \#2, demonstrating that \textit{AdaShare} correctly predicts the location of those skipped blocks, which forms the final sharing pattern in our approach.

\begin{wraptable}{r}{0.5\linewidth}
    \begin{center}
    \vspace{-10pt}
     \caption{\small \textbf{Ablation Study on NYU v2 3-Task Learning}. : Semantic Segmentation, : Surface Normal Prediction,  : Depth Prediction.}
     \label{table:ablate_nyu_v2_3}
        \resizebox{\linewidth}{!}{
        \begin{tabular}{c|ccc|c}
            \Xhline{3\arrayrulewidth} 
            Models &\makecell{   }& \makecell{   } & \makecell{  } &  \makecell{   } \\
            \Xhline{3\arrayrulewidth} 
            Stochastic Depth & -2.4 & +7.5 & +4.0 & +3.1\\
            \hline 
            Random \# 1 & -2.3 & +5.4 & -0.8 & + 1.3 \\
            Random \# 2 & \underline{+ 3.3} & \underline{+8.4} & +8.1 & \underline{+ 6.6} \\
            \hline 
            w/o curriculum & +2.1 & +7.4 & \underline{+7.2} & + 5.6 \\
            w/o  & -4.2 & + 4.8 & +1.6 & +0.7 \\ 
            w/o  & -0.9 & \textbf{+9.0} & \underline{+8.5} & +5.6 \\
            \hline
            \textit{AdaShare}-Instance & -3.7 & + 5.3 & -22.3 & -6.9 \\
            \textit{AdaShare}-RL & -2.8 & 0.0 & -8.2 & -3.7\\
            \hline
            \textit{AdaShare} & \textbf{ +8.8} &  +7.9  &\textbf{+10.1} & \textbf{+8.9}\\
            \Xhline{3\arrayrulewidth} 
        \end{tabular}}
        \vspace{-10pt}
    \end{center}
\end{wraptable} 
\textbf{Ablation on Training Losses and Strategies.}\label{sec:ablation_training}
We perform experiments to show the effectiveness of curriculum learning, sparsity regularization  and the sharing loss  in our model. 
With all the components working, our approach works the best in all three tasks (see Table~\ref{table:ablate_nyu_v2_3}), indicating that three components benefit the policy learning.

\begin{wraptable}{r}{0.7\linewidth}{
\caption{\small{\textbf{Different Network Architectures on NYU v2 2-Task Learning.} : Semantic Segmentation, : Surface Normal Prediction.}}
\label{tab:more_comp}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|cc|c}
            \Xhline{3\arrayrulewidth} 
            Models &\makecell{   } &\makecell{   } &  \makecell{   } \\
            \Xhline{3\arrayrulewidth}
\hline
            \multicolumn{4}{c}{\textbf{WRN}} \\
            \hline
            Multi-Task & -0.35 & 9.63 & 4.64 \\
            \textit{AdaShare} & 9.36 & 11.53 & 10.44 \\
             \hline
            \multicolumn{4}{c}{\textbf{MobileNet-v2}} \\
            \hline
            Multi-Task & 0.18 & 8.02 & 4.10 \\
            \textit{AdaShare} & 4.16 & 10.61 & 7.39\\
            \Xhline{3\arrayrulewidth} 
        \end{tabular}}}
\end{wraptable}
 
\textbf{Comparison with Instance-specific Policy.} We employ the same policy network~\cite{ahn2019deep} to compute the select-and-skip decision per test image for each task~\cite{veit2018convolutional,wu2018blockdrop}. \textit{AdaShare}, with task-specific policy, outperforms the instance-specific policy (in Table~\ref{table:ablate_nyu_v2_3}), as the discrepancy among tasks dominates over the discrepancy among samples in multi-task learning. Instance-specific methods often introduce extra optimization difficulty and result in worse convergence. 

\textbf{Comparison with AdaShare-RL.} We replace Gumbel-Softmax Sampling with REINFORCE to optimize the select-or-skip policy while other parts are unchanged. Table~\ref{table:ablate_nyu_v2_3} shows \textit{AdaShare} is better than AdaShare-RL in each task and overall performance, in line with the comparison in \cite{wu2019liteeval}.

\textbf{Extension to other Architectures.}  We implement \textit{AdaShare} using Wide ResNets (WRN)~\cite{zagoruyko2016wide} and MobileNet-v2~\cite{sandler2018mobilenetv2} in addition to ResNets. \textit{AdaShare} outperforms the Multi-Task baseline by \textbf{5.8\%} and \textbf{3.2\%} using WRN and MobileNet respectively in NYU-v2 2-Task (Table~\ref{tab:more_comp}). We also observe a similar trend on CityScapes 2-Task learning. This shows effectiveness of our proposed approach across different network architectures.  

