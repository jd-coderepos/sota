\documentclass{article} \usepackage{iclr2021_conference,times}

\usepackage{hyperref}
\usepackage{url}            

\usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{multirow}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{bm}             \usepackage[normalem]{ulem} \usepackage{graphicx}
\usepackage{wrapfig}        \usepackage{xcolor}         \usepackage{footnote}       \usepackage{cite}           \usepackage[frozencache=true,cachedir=.]{minted} \usepackage[utf8]{inputenc}
\usepackage{amsthm}         \usepackage{subcaption}     





\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\embedding}{\mathrm{embedding}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\vect}{vec}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 

\graphicspath{{figures/}}   

\DeclareMathOperator{\LN}{\mathrm{LN}}
\DeclareMathOperator{\FWM}{\mathrm{FWM}}
\DeclareMathOperator{\LSTM}{\mathrm{LSTM}}

\usepackage{scrwfile}
\TOCclone[\contentsname~(\appendixname)]{toc}{atoc}
\newcommand\StartAppendixEntries{}
\AfterTOCHead[toc]{\renewcommand\StartAppendixEntries{\value{tocdepth}=-10000\relax}}
\AfterTOCHead[atoc]{\edef\maintocdepth{\the\value{tocdepth}}\value{tocdepth}=-10000\relax \renewcommand\StartAppendixEntries{\value{tocdepth}=\maintocdepth\relax}}
\newcommand*\appendixwithtoc{\cleardoublepage
  \appendix
  \addtocontents{toc}{\protect\StartAppendixEntries}
  \listofatoc
}

\title{Learning Associative Inference Using \\ Fast Weight Memory}



\author{Imanol Schlag \\
The Swiss AI Lab IDSIA / USI / SUPSI\\
\texttt{imanol@idsia.ch} \\
\And
Tsendsuren Munkhdalai \\
Microsoft Research \\
\texttt{tsendsuren.munkhdalai@microsoft.com} \\
\And
J\"urgen Schmidhuber \\
The Swiss AI Lab IDSIA / USI / SUPSI\\
\texttt{juergen@idsia.ch} \\
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}

\maketitle

\begin{abstract}
Humans can quickly associate stimuli to solve problems in novel contexts. Our novel neural network model learns state representations of facts that can be composed to perform such associative inference. To this end, we augment the LSTM model with an associative memory, dubbed \textit{Fast Weight Memory} (FWM). Through differentiable operations at every step of a given input sequence, the LSTM \textit{updates and maintains} compositional associations stored in the rapidly changing FWM weights. Our model is trained end-to-end by gradient descent and yields excellent performance on compositional language reasoning problems, meta-reinforcement-learning for POMDPs, and small-scale word-level language modelling.\looseness-1 \footnote{Source code and data used in this paper is available at \href{https://github.com/ischlag/Fast-Weight-Memory-public}{github.com/ischlag/Fast-Weight-Memory-public}}
\end{abstract}

\section{Introduction}
Humans continually adapt in order to understand new situations in changing environments. 
One important adaptive ability is \textit{associative inference} for
composing features extracted from distinct experiences and relating them to each other~\citep{schlichting2015memory, gershman2015discovering}. 
Suppose Alice has shared with you pictures of her toddler.
Later, at the office party, you see a man carrying the depicted toddler.
Since the toddler yields a shared feature in two different contexts, it may be plausible to infer that the man is Alice's partner, without ever seeing him and Alice together. 
The ability to rapidly associate and bind together novel stimuli can help to derive knowledge systematically, in addition to the knowledge gained directly from observation. 

Virtually all modern cognitive architectures applied to challenging artificial intelligence problems are based on deep artificial neural networks (NNs).
Despite their empirical successes and theoretical generality, NNs tend to struggle to generalise in situations similar to the given example~\citep{lake2017building, phillips1995connectionism, still_not_systematic_lake}. 
This weakness becomes even more severe if the training and test data exhibit systematic differences~\citep{atzmon2016learning, Agrawal2017CVQAAC}. For example, during training, the man's representation might never be associated with the toddler's, but during testing, this association might be necessary to make a useful prediction. 
In problems where humans excel, this sort of inference is likely ubiquitous since data is often combinatorially complex in a way that observations used during training will likely cover just a small fraction of all possible compositions.
Such a lack of productivity and systematicity is a long-standing argument against the use of NNs as a substrate of an artificial cognitive architecture~\citep{fodor1988connectionism, hadley1994systematicity,mclaughlin2009systematicity}.

The hidden state of a neural model is a learned representation of the task-relevant information extracted from the input.
To generalise to never-seen-before compositions of stimuli, the function which produces the state representation must be able to systematically construct \textit{all possible} states.
This requires a general and preferrably differentiable method, such as the Tensor Product Representation (TPR;~\citet{Smolensky1990TPR}). 
TPRs provide a general and differentiable method for embedding symbolic structures in vector spaces.
A TPR state representation is constructed via the tensor product (i.e. the generalised outer-product) of learned component representations.
Under certain constraints, such a mechanism guarantees a unique representation for every possible combination of components~\citep{Smolensky1990TPR, smolensky2012symbolic}.

In this work, we augment a recurrent NN (RNN) with an additional TPR-like memory representation.
To facilitate the learning of multi-step associative inference, the TPR memory can be queried multiple times in a row, allowing the model to chain together various independent associations. 
In contrast to previous work on fast weights, we apply our memory-augmented RNN to much longer sequences. This requires the model to \textit{update} its associative memory. Furthermore, we demonstrate the generality of our method by applying it to meta-reinforcement learning and small scale language modelling problems. 

In the next section, we cover related memory-augmented NNs. Section \ref{sec:method} describes the FWM in detail. Section \ref{sec:experiments} demonstrates the generality of our method through experiments in the supervised, self-supervised, and meta-reinforcement learning setting. 
The supervised-learning experiments in subsection \ref{subsec:catbAbI} consist of a more challenging version of the bAbI dataset dubbed concatenated-bAbI or catbAbI. 
The meta-reinforcement learning experiment in section \ref{subsec:metaRL} demonstrates the FWM's ability to learn to explore a partially observable environment through its ability to perform associative inference.
Finally, the self-supervised experiments in subsection \ref{subsec:LM} demonstrate that the FWM can compete with the state-of-the-art word-level language models on small benchmark datasets.

\section{Related Work}
\label{sec:related-work}
RNNs such as the Long Short-Term Memory (LSTM;~\citet{lstm97and95,Gers:2000nc}) are in theory capable of implementing any algorithm~\citep{siegelmann91turing}.
However, the linear growth of the hidden state of a fully connected RNN leads to quadratic growth in the number of trainable weights.
Early work addressed this issue through the use of additional memory~\citep{Das:92,mozer1993connectionist} and differentiable fast weights~\citep{Schmidhuber:92ncfastweights,Schmidhuber:93ratioicann}. 
Recently, memory-augmented NNs have solved algorithmic toy problems~\citep{graves2014neural, graves2016} as well as reasoning and inference problems in synthetic and natural language~\citep{weston2015memorynets, xiong2016dynamic}.

Inspired by the random-access memory of computer architectures, a common approach is to incorporate a soft and differentiable lookup table into the NN model.
Such slot-based memory matrices have shown to be difficult to train~\citep{munkhdalai2017neural} and require sophisticated mechanisms for the allocation and deallocation of memory~\citep{csordas2018improving}.
The Transformer-XL (TXL;~\citet{dai2019transformer}), an autoregressive language model variant of the Transformer~\citep{vaswani2017attention}, can be understood as a slot-based memory-augmented RNN where every new state is pushed into an immutable queue of finite size. 
Although it is recurrent, the layers of a transformer architecture are strictly forced to use inputs from a lower layer which limits its generality. Nevertheless, a sufficiently deep and well regularised TXL model has achieved state-of-the-art performance in large scale language modelling tasks.

A biologically more plausible alternative of increasing the memory capacity of NNs are fast-changing weights, i.e. stateful weights that can adapt as a function of its input.
Non-differentiable fast weights or ``dynamic links'' have been published since 1981~\citep{von1981correlation,feldman1982dynamic,hinton1987deblur}.
Subsequent work showed that a regular network can be trained by gradient descent to control the fast weights of a separate network~\citep{Schmidhuber:92ncfastweights} or of itself~\citep{Schmidhuber:93ratioicann} in an end-to-end differentiable fashion.
Recently, fast weights have made a comeback and achieved good results in small toy problems where regular NNs fall short ~\citep{Ba2016using,schlag2017gated,munkhdalai2017meta,pritzel2017neural,ha2017hypernetworks,zhang2017learning,miconi2018differentiable,miconi2018backpropamine,schlag2018nips,munkhdalai2019metalearned,Bartunov2020Meta-Learning}.

Most memory-augmented NNs are based on content-based or key-based lookup mechanisms. 
An alternative to the storage of patterns in a lookup table is the idea that patterns are reconstructed through the implicit iterative minimisation of an energy function, such as in the classical Hopfield network~\citep{steinbuch1961lernmatrix, willshaw1969non, hopfield1982neural, kanerva1988sparse} or the modern Hopfield network~\citep{krotov2016dense, demircigil2017model, ramsauer2020hopfield}.
This is often described as an \textit{auto-associative} type of memory as it reconstructs a previously stored pattern that mostly resembles the current pattern.
A much less studied variation is the \textit{hetero-associative} memory (see e.g. \citet{kosko1988bidirectional}) where the retrieved pattern is different from the input pattern. This is more relevant for our use case. 
We aim to train an LSTM to \textbf{construct}, \textbf{maintain}, and \textbf{edit} its associative memory. 
The ability to edit Hopfield networks partially is not very well studied.
For this reason, we employ a simple (multi-)linear hetero-associative memory as it is more closely related to the theory of TPRs (whose manipulation is well understood) and because the association is retrieved in a single step.

Our work directly builds on two examples of differentiable fast weight memories: the TPR-RNN by \citet{schlag2018nips} and the Metalearned Neural Memory (MNM) by \citet{munkhdalai2019metalearned}. 
The TPR-RNN is a sentence-level model for reasoning on text. It achieves excellent results on the regular bAbI tasks but it underperforms on word-level bAbI \citep{schlag2019enhancing} or algorithmic toy problems \citep{le2020self}.
In contrast, the MNM is a word-level model which augments the LSTM with a fully-connected multi-layer feed-forward network as its memory and trains it using a meta-learning objective.
Both, MNM and TPR-RNN were developed on the regular bAbI dataset which only contains short sequences and does not require the model to remove deprecated associations from its memory.
In this work, we train on an infinite sequence of bAbI stories where our FWM achieves excellent performance and improves over MNM. 
We further demonstrate strong performance in small-scale language modelling and meta reinforcement-learning which demonstrates the generality of our contribution.\looseness-1

\section{Proposed Method}
\label{sec:method}
Our FWM is a fast-changing, multi-linear map which is controlled by a slowly-changing, non-linear LSTM. 
The slow weights of the LSTM are regular NN weights which are updated during training by gradient descent.
In contrast, the fast weights of the FWM are updated by the LSTM at every step of the input sequence through a Hebb-like differentiable mechanism. 
This allows the FWM function to change rapidly even during testing---hence the name \textit{fast} weights. 
Along with updating the fast weights, the LSTM also generates a memory query which is used to retrieve information that was previously stored.
The retrieved information then becomes part of the model's output. 
\subsection{The Fast Weight Memory}
\begin{wrapfigure}{r}{6cm}
  \vspace{-20pt}
  \centering
  \includegraphics[scale=0.5]{model_illustration.png}
\caption{A simplified illustration of our proposed method where  refers to the write mechanism described in section \ref{sec:fwmwiriting}.  are the recurrent weights of the FWM which have been generated by the LSTM. The LSTM is a regular slow RNN. The residual connection between the FWM and the LSTM is not depicted.\looseness=-1}
  \label{fig:illustration}
  \vspace{-0.5cm}
\end{wrapfigure}
Given a sequence of tokens  from a vocabulary , the task of language modelling is to train a model which maximizes the joint probability  which we factorize autoregressively  where  is an artificial start token.\footnote{We use the notation  to refer to the sequence .}
In this work, we train an RNN model to encode the input sequence  into , the hidden state of the LSTM, and , the fast weight tensor of the FWM, to maximize the probability of the next token .

At step  of the input sequence, the input token  is embedded in a -dimensional vector space using a lookup table . An LSTM with  hidden units encodes the sequence of embedded tokens into a fixed size vector representation . 
The probability distribution over the next token  where  are the fast weights of the FWM at step  and  .
Note that the fast weight matrix  is a reshaped third-order tensor . This allows us to describe third-order tensor operations using matrix multiplications.
We'll now describe in detail the  function and how its fast weights are updated.\looseness=-1

\subsubsection{Writing} \label{sec:fwmwiriting}
The FWM is updated at every step  using the write mechanism described in this section.
To this end, we extract from the hidden state : the write strength  (a scalar bounded by  and  using the sigmoid function ), the two key vectors , and the new value . 

The purpose of writing to memory is to learn a \textbf{context-specific} association between the input pattern  and the output pattern . 
The usage of the tensor-product in the input pattern factorises the the representational space which guarantees unique orthogonal vector representations for novel key pairs.
A specific example of such is given and demonstrated by \citet{schlag2018nips} where the first key learns to represent an entity and the second key a specific action, thereby, learning a representational space that generalises to never seen entity and action compositions.

In stark contrast to the complex memory operations of the TPR-RNN, we employ a single, simple, and word-level operation which is closely related to the perceptron learning rule~\citep{rosenblatt1958perceptron}. It allows the model to replace the previous association  with a convex combination of the old and new value .
With the scalar  the LSTM controls if the new association fully replaces the previous value () or if the information of both are mixed together.
Our fast weight update works as follows:
First, the current value  that is associated with  is retrieved.
Second, we remove the old association from the map by subtracting  from our memory, where  vectorises the matrix.
Third, we add .
All three steps can be achieved at once using the following update rule (see appendix section \ref{appendix:sec:updateRule} for the proof):\looseness-1

To prevent the fast weights from potentially growing endlessly, we scale down the fast weights whenever . This is achieved through the following element-wise scaling.


\subsubsection{Reading}
For each step of the input sequence, the model queries the memory in order to retrieve a previously stored value. 
Due to the keys and values being generated separately, the network can retrieve values which are informationally independent from their keys. In order to perform more complex associative inference, like e.g. transitive inference (, therefore, ), we employ multiple reads where we use the retrieved value as one of the keys in the next query (see equation \ref{eq:fastRNN}). 



Here  refers to layernorm without the learned element-wise affine map~\citep{ba2016layer},  reshapes the matrix into a vector,  is the hyperbolic tangent function, and the matrices  and  are regular slow weights trained by gradient descent which allow us to decouple the dimensionality of the LSTM from the dimensionality of the FWM.
In eq. \ref{eq:fastRNN},  is the multi-linear map which we query using the LSTM-generated ``input''  and the previous retrieval  (except for the first query where both keys are LSTM-generated). \looseness-1



\section{Experiments}
\label{sec:experiments}

\subsection{Concatenated-bAbI}
\label{subsec:catbAbI}
The bAbI tasks is a popular toy dataset to benchmark neural networks with memory augmentations and reasoning capabilities~\citep{babi_tasks_weston}. 
It consists of a set of short stories with questions embedded in the text. 
The stories were generated by simulating multiple entities in a virtual environment and cover different contexts in which entities change their state on their own or through an interaction.
Each story-sample belongs to one of 20 different tasks that the authors of the dataset considered important for intelligent dialogue agents. 
The tasks contain questions which require reasoning capabilities like deduction, coreference, or counting.
All tasks require some level of symbolic reasoning, and the first neural and non-neural baselines demonstrated poor generalisation performance on test data~\citep{babi_tasks_weston}.

We aim to improve the bAbI benchmark as a means of developing intelligent dialogue agents.
To this end, we propose concatenated-bAbI (catbAbI): an infinite sequence of bAbI stories. 
catbAbI is generated from the bAbI dataset and during training, a random sample/story from any task is drawn without replacement and concatenated to the ongoing story. 
The preprocessing for catbAbI addresses several issues: it removes the supporting facts, leaves the questions embedded in the story, inserts the correct answer after the question mark, and tokenises the full sample into a single sequence of words. 
As such, catbAbI is designed to be trained in an autoregressive way and analogous to closed-book question answering.

catbAbI models can be trained in two different ways: language modelling mode (LM-mode) or question-answering mode (QA-mode).
In LM-mode, the catbAbI models are trained like autoregressive word-level language models. 
In QA-mode, the catbAbI models are only trained to predict the tokens that are answers to questions---making it more similar to regular bAbI. QA-mode is simply implemented by masking out losses on non-answer predictions.
In both training modes, the model performance is solely measured by its accuracy and perplexity when answering the questions.
Performance on non-answers is irrelevant on catbAbI because the tokens are either very predictive or inherently unpredictable, and there is nothing appealing to be learned. 
Despite measuring performance only for answers, we argue that LM-mode is interesting for three reasons. 
First, LM-mode removes the bias of knowing which words would benefit from a symbolic inference mechanism. 
Second, LM-mode trains the model on a sequence with tokens which are inherently unpredictable. Such tokens could also appear in natural language and might harm the model's ability to learn a useful representation of the story.
Indeed, in the next section, we will give evidence for such a generalisation gap.
Third, the LM-mode setting allows us to directly compare our method with state-of-the-art language models. \looseness-1

\subsubsection{Results}
We compare our FWM directly with the current state-of-the-art on word-level bAbI: Metalearned Neural Memory (MNM;~\citet{munkhdalai2019metalearned}).
We also include two strong autoregressive word-level language models as baselines: a regularized LSTM ~\citep{merity2018regularizing, lstm_still_sota_melis} and a regularized Transformer-XL (TXL;~\citet{dai2019transformer}). 
Lastly, we also evaluate Ba's Fast Weights which attend to the recent past (JBFW;~\citet{Ba2016using}) but were unable to find hyperparameters that converged.
We truncate backpropagation through time (tBPTT) to 200 tokens for all models and limited the amount of GPU memory to \texttildelow16GB for practical reasons. 
For every model, we performed a hyperparameter search in QA mode over the first 3k steps of which a smaller selection was trained for 30-60k steps. For all models, we adopt the best QA mode hyperparameters for the LM mode results.
Table \ref{tbl:catbAbI} lists the best accuracy and perplexity of each model over three seeds while figure \ref{fig:bestRuns} shows the learning curves of the best seeds.
Further hyperparameter search results can be found in the appendix section \ref{appendix:sec:catbAbIparams}.\looseness=-1
\begin{table}[h]
  \vspace{-6pt}
  \caption{
  Accuracy and perplexity on test data over three seeds of each model's best hyperparameters setting according to our hyperparameter search. Detailed hyperparameters and results can be found in the appendix section~\ref{appendix:sec:catbAbIparams}.}
  \vspace{5pt}
  \label{tbl:catbAbI}
  \centering
  \setlength{\tabcolsep}{4pt}
  \renewcommand{\arraystretch}{1}
  \begin{tabular}{cccccccc}
    \toprule
    Mode & JBFW & LSTM & TXL & MNM & FWM \\
    \midrule
QA acc   & 13.22  0.0 & 80.88\%  0.30  & 87.66\%  2.82  & 88.97\%  6.28   & \textbf{96.75\%  0.05} \\
    QA ppl   & 31.19  8.8 & 1.93  0.11     & 1.50  0.14     & 2.50  1.07      & 1.36  0.06 \\
LM acc \rule{0pt}{1.2\normalbaselineskip}
             & 0.0  0.0  & 80.15\%  0.40   & 90.23\%  1.01  & 69.30 \%  16.60 & \textbf{93.04\%  0.62} \\
    LM ppl   & 160.3  24.3& 1.84  0.02     & 1.39  0.03     & 2.60  1.02      & 1.45  0.14 \\
weights \rule{0pt}{1.2\normalbaselineskip}
                & 548k\footnotemark & 8M     & 10.5M & 1.1M  & 694k \\
    activations & 263k   & 4096   & 4.3M\footnotemark & 30.5k & 33.3k \\
    \bottomrule
  \end{tabular}
\end{table}
\begin{figure}[!ht]
  \centering
  \vspace{-3pt}
  \includegraphics[width=0.9\textwidth]{best_runs.png}
  \vspace{-10pt}
  \caption{
  QM model validation accuracy of the best-over-all seeds of each model over training steps.\looseness=-1}
  \vspace{-3pt}
  \label{fig:bestRuns}
\end{figure}

Our experiments on catbAbI show that a regularized, 4-layer deep, and residual LSTM, and a 3-layer deep TXL with attention over the last 1400 tokens, achieve strong performance on catbAbI. 
MNM, on the other hand, suffered a \texttildelow10\% drop in QA mode accuracy compared to its performance on bAbI which demonstrates the increased difficulty of catbAbI. 
The JBFW model is not able to make meaningful predictions on catbAbI which may be due to its inability of removing previous associations and fixed fast weight memory decay.
Our FWM achieves an excellent accuracy on catbAbI while being by far the smallest in parameter count and weight to activation ratio. 
The performance gap between FWM and MNM suggests the importance of our fast weight memory mechanism.
In figure \ref{fig:task15} we visualise how the FWM can chain memories from different points in time to perform transitive inference.

We chose to include the TXL model in our comparison due to its autoregressive nature and strong performance in large-scale language modelling benchmarks.
However, we point out that the TXLs context window is larger than the average bAbI story.
In this case, due to the shortness of the stories, catbAbI becomes more of an open-book problem for the TXL model since it has the capability of looking up representations of its previous input whereas the RNN models do not.
This fundamentally limits the TXL model as it can only condition its prediction on information that is no longer than its attention window to past states.
The RNN models, which are general function approximators, for better or for worse, are instead forced to learn to carry the necessary information through time. 
\footnotetext[3]{Bigger JBFW models did not improve performance. See appendix section \ref{appendix:sec:jbfw_search}.}
\footnotetext[4]{The number of immutable activations is  while the number of mutable activations is merely . Only the TXL model maintains immutable activations.\looseness-1}
\begin{figure}[!ht]
  \centering
\includegraphics[width=\textwidth]{task_15.png}
  \vspace{-3pt}
  \caption{
  A visualisation of the FWMs ability to chain independent associations to perform transitive reasoning on the catbAbI validation data.
  The colour of each grid cells represent the dot product  where  are the write keys of each previous position while  refers to the respective queries generated at ``?'' (second position from the right) for each of the  memory reads.
  The first query matches most with the keys at the recent positions where the input was ``gertrude'' and ``afraid'' (first row of grid cells). 
  The second query, which partially consists of the value retrieved from the first query, matches with the ``getrude is a mouse'' section.
  The third query, which partially consists of the value retrieved from the second query, matches with the ``mice are afraid of wolves'' section.
  Finally, the FWM correctly outputs the next word and answer to the question: wolf (not seen).
  This likely completes the deduction: gertrude is a mouse, mice are afraid of wolves, therefore, gertrude is afraid of wolves.}
  \vspace{-5pt}
  \label{fig:task15}
\end{figure}
\subsection{Meta-Reinforcement Learning}
\label{subsec:metaRL}
Meta reinforcement learning (Meta-RL) applies meta-learning~\citep{schmidhuber87,Hochreiter:01meta,finn2017model} to the field of reinforcement learning~\citep{Schmidhuber:94selfold}.
An agent is trained on multiple environments (or tasks) and receives environmental feedback as part of its input. 
To maximise its total reward in an environment, the agent has to leverage the feedback signals and adapt.
A successful agent is capable of maximising its reward in novel environments that it has not been exposed to during training.
Recent work achieved notable progress in this domain~\citep{santoro2016meta,mishra2018a,Kirsch2020Improving}. 
\begin{figure}[h]
  \centering
  \vspace{-10pt}
  \includegraphics[scale=0.6]{graphs.png}
\caption{
Two randomly generated environments with the agent's location coloured in green and the reward location coloured in yellow.
Edge labels indicate the set of valid actions (0, 1, or 2) to transition along that arrow.
Invalid actions are not visualised.
The graph and the locations of the agent and reward are set randomly at the beginning of the experiment.
If the agent reaches the reward location or did not reach it after six steps, both are randomly reset.
}
  \label{fig:graph}
  \vspace{-10pt}
\end{figure}
We experiment with tasks drawn randomly from a large set of partially observable Markov decision processes (POMDPs).
In this set, every environment consists of precisely five states and three actions. 
Globally, every environment can be viewed as a sparse directed graph where nodes are locations, and the directed edges are one-way modes of transportation---similar to a metro transit map of a city~\citep{graves2016}.
To generate a new environment, we sample the adjacency matrix of the graph such that actions are deterministic, and every location is reachable from any other location (see figure \ref{fig:graph}).
We sample graphs such that there are no actions that lead to the same location, and such that \textit{not} every action is always a valid way of transitioning. We added the exact algorithm to generate graphs, as well as further details, to the appendix section~\ref{appendix:sec:metaRL}.

The agent's goal is to reach the reward location.
Upon arrival, the agent receives the reward, followed by a random reset of the agent's and reward's location.
Whenever the agent takes an action that does not lead to a new location, it receives a penalty.
At every step, the agent receives as an input: its current location, the reward location, its last action, and the reward received so far. 
\begin{figure}[h]
  \centering
\includegraphics[scale=0.45]{RL_paper_plot.png}
\caption{
  Average total reward of the agent when trained on 600 random graphs (left plot) and tested on 600 different graphs (right plot).
  The FWM agent (blue) has a slow LSTM with 32 hidden units and a fast weight memory of size . 
  We compare to LSTM agents with different sized hidden states. 
  The largest LSTM has 4096 hidden units (red) which roughly matches the number of temporal variables of the FWM.
  The FWM has 14k trainable weights which is by far the lowest. 
  The largest LSTM has 67.4M weights which is roughly 4814 times more than the FWM.
  The relative factor of each LSTM is added to the legend.
  All LSTMs take longer to train and eventually overfit on the training data. 
  Due to the overfitting, the LSTM does not have to explore, which results in a higher total reward on training environments but a lower total reward on test environments.}
  \label{fig:RL}
  \vspace{-5pt}
\end{figure}

We run our experiment for 30 steps and compare our FWM to an LSTM baseline. 
Both methods are trained on the same training set of 600 graphs and tested on 600 novel graphs.
We optimise our agent with the Advantage Actor-Critic (A2C) algorithm, a non-asynchronous version of the A3C method~\citep{mnih2016asynchronous}.
In our experiments, the LSTM-based agent requires more episodes, a bigger network, and eventually overfit to the training graphs.
The FWM-based agent however trains faster and generalises to randomly sampled graphs. 

We argue that the bAbI stories and the episodes on the graphs are similar in the following three ways. 
First, in both problems, the network has to construct a useful and \textit{context-specific} representation from its ongoing input. 
Second, as part of its input, the network repeatedly receives an objective (the reward location versus the question) which requires the exploitation of the context-specific information. 
Third, the model has to produce a discrete sequence (actions in the environment in RL and reasoning steps in catbAbI) to optimise its training signal (high reward versus low uncertainty).

\subsection{Language Modelling}
\label{subsec:LM}
Comparing FWM to autoregressive language models on catbAbI begs the question: how does FWM perform on popular word-level language modelling datasets such as Penn Treebank (PTB;~\citet{mikolov2010recurrent}) or WikiText-2 (WT2;~\citet{merity2016pointer})?
It is unclear to which extend a symbolic inference mechanism is beneficial for language modelling.
PTB and WT2 contain virtually no questions and are constructed from Wikipedia and news articles which are designed to be easily parsed by the reader.
Nevertheless, in figure \ref{fig:phillips} we show how our FWM exploits recurring subject names to reduce its uncertainty.
Not many memory augmented NNs have been able to bridge from small and toy reasoning tasks to general language models---and those which did, underperformed~\citep{paperno-etal-2016-lambada, sukhbaatar2015end}.
\begin{table}[h]
  \vspace{-3pt}
  \caption{
  Best perplexity on the test data of Penn Treebank (PTB) and WikiText-2 (WT2) from three seeds. Detailed results can be found in the appendix in table \ref{appendix:tbl:lmresults}. All PTB models have roughly 24M parameters and all WT2 models have roughly 37M parameters. The AWD-TXL is the Transformer-XL architecture as reported by \citet{dai2019transformer} with the necessary AWD-style regularisation, model averaging, and softmax temperature tuning (see appendix section \ref{appendix:sec:LM}).\looseness-1}
  \label{tbl:LM}
  \centering
  \vspace{5pt}
  \def\mystrut{\rule{0pt}{1\normalbaselineskip}}
  \setlength{\tabcolsep}{4pt}
  \begin{tabular}{lcccccc}
    \toprule
    \multirow{2}{*}{Model} & \multicolumn{2}{c}{PTB} &
    \multicolumn{2}{c}{WT2} \\
    & Validation & Test
    & Validation & Test\\
    \midrule
    AWD-LSTM \citep{merity2018regularizing} & 60.0 
                                            & 57.3
                                            & 68.6
                                            & 65.8  \\
    AWD-TXL \citep{dai2019transformer} & - 
                                       & 54.52
                                       & -
                                       & - \\
AWD-TXL (ours)  \rule{0pt}{1.2\normalbaselineskip} & 59.39 
                    & 56.50
                    & 65.73
                    & 63.11 \\
    AWD-FWM (ours) & 56.76
                   & \textbf{54.48} 
                   & 63.98
                   & \textbf{61.65} \\
    \bottomrule 
  \end{tabular}
  \vspace{-4pt}
\end{table}
\begin{figure}[h]
  \centering
\includegraphics[width=\textwidth]{phillips.png}
  \vspace{-6pt}
  \caption{
  Loss comparison between the LSTM and our FWM on a section of the PTB test set. The colour of the grid cells in the first row stands for the cross-entropy error of the LSTM and FWM model. The second row, for their respective difference. Our FWM sometimes shows a lower error on rare subject words such as names of companies and people once they have been introduced. As seen in the red circles, the initial mentioning of ``phillips'' has similar uncertainty between the LSTM and FWM but shortly after that the subject of the sentences is more predictable and the FWM is more certain (4.3 bits difference) whereas the LSTM's uncertainty remains roughly on the same level (12.8 bits).\looseness-1}
  \label{fig:phillips}
  \vspace{-5pt}
\end{figure}
We use the regularized 3-layer AWD-LSTM~\citep{merity2018regularizing} as the slow RNN in our FWM model to minimize further hyperparameter search.
The experimental results in table \ref{tbl:LM} demonstrate a relative improvement over the AWD-LSTM baselines, which suggest the benefit of our FWM even in language modelling benchmarks. 
However, in contrast to catbAbI, all three models achieve very similar results which might indicate that PTB and WT2 do not benefit as strongly from an associative reasoning capacity. 
We added the experimental details to the appendix section~\ref{appendix:sec:LM}.

Since the publication of AWD-LSTM~\citep{merity2018regularizing}, various extensions (some of which are orthogonal to our memory augmentation) have been proposed~\citep{pmlr-v80-krause18a,merity2018regularizing,yang2018breaking}.
In this work, we are not primarily interested in beating the state-of-the-art in language modelling and leave it for future work to explore the possible synergies between these methods. \looseness-1

\section{Discussion}
An order-three memory tensor is a computationally demanding method for constructing compositional state representations. 
With vector components in , the tensor product computation alone has a space and time complexity of . 
For practical reasons, this forces the FWM to remain small, relative to the slow NN, which limits the number of associations that can be maintained at once. 
Previous work has proposed approximations of such memory tensors in a variance-optimal way~\citep{schlag2019enhancing}. 
In our ablation experiments in section \ref{appendix:sec:ablation}, we show on catbAbI that concatenating the keys results in a performance accuracy drop of \texttildelow5\%.
We also experiment with fewer read operations (smaller ) which also results in a performance degradation (appendix figure \ref{appendix:fig:readAblation}).
However, further improvements might not come from scaling up but from more general symbolic manipulations. 
We address the capacity of the FWM and the necessity of the tensor product from a linear hetero-associative memory perspective in section \ref{appendix:sec:discussion} of the appendix.
Finally, our fast weight memory can be thought of as a primitive ``working memory'' of the model---analogous to the working memory in the human brain~\citep{spalding2018ventromedial}.
This idea is supported by recent work which proposes a cognitive model of the human brain that is based on such higher-order tensors~\citep{Tresp2017TheTM}. \looseness-1

\section{Conclusion}
Our new FWM is a fast weights architecture capable of learning from synthetic data to answer questions which require various symbolic reasoning skills. 
To improve generality, we overcome issues of the popular bAbI dataset by introducing more general and more difficult variation dubbed catbAbI. 
We report excellent performance on catbAbI and compare with strong baselines based on state-of-the-art language models, as well as, the previous state-of-the-art in word-level bAbI. 
We also apply the FWM in a challenging meta-reinforcement learning environment where the agent generalises to novel environments by learning from its observations and actions. 
Finally, in a self-supervised setting, we apply the FWM to word-level language modelling on PTB and WT2 where it beats the AWD-LSTM and AWD-Transformer-XL baselines.

\section*{Acknowledgements}
We thank NVIDIA Corporation for donating several DGX
machines, and IBM for donating a Minsky machine.
This research was supported by an European
Research Council Advanced Grant (no: 742870).

\bibliography{references}
\bibliographystyle{iclr2021_conference}


\appendixwithtoc
\newpage

\section{Further Discussion}
\label{appendix:sec:discussion}
One way of assessing the capacity of the third-order tensor memory is its rank (which is analogous to the rank of a matrix). 
However, there exists no general algorithm to determine the rank of a given higher-order tensor . There exists only a loose upper bound described by ~\citep{kruskal1989rank,kolda2009tensor}.

It might be tempting to simplify the FWM by replacing the outer-product of the input with a concatenation as a means to reduce the space and time complexity. 
However, in highly compositional domains, the concatenated input will suffer from interference between memories.
Consider a problem which, from a set of 10 symbols, requires the association of any three symbols represented by the vectors . 
In the case of a concatenation, one rank of the fast weight memory is  where we refer to  as the key representation. 
The read vectors , are then concatenated and matrix multiplied to retrieve the previous association . 
Here we refer to  as the query representation. 
Since there are ten distinct symbols of which any two can behave as a key representation, there exist  unique key patterns. 
To guarantee noise-free retrieval in any context, the vectors of the key representations have to be orthogonal. 
However,  is only a  dimensional space which means that certain key representations cannot be used simultaneously without interference.
The tensor product, on the other hand, is capable of noise-free retrieval because it represents the key as  which allows for  orthogonal keys and as such the possibility of noise-free retrieval.
We conclude that if the problem is highly compositional, in a sense that every component can be composed with any other component, then the tensor product will be better suited than a concatenation. Experimentally we evaluate concatenated keys in section \ref{appendix:sec:ablation}. The results show that concatenated keys will result in a slightly worse performance (see figure \ref{appendix:fig:productAblation}). As an alternative, a non-linear memory, e.g. through the use of a softmax, would not require orthogonality in it's keys to be free of interference and could result in a larger storage capacity.

\section{Derivation of the Update Rule}
\label{appendix:sec:updateRule}
\newtheorem{theorem}{Theorem}[section]
\begin{theorem}
Given two key vectors  and two value vectors  with , a mixing coefficient , and a fast weight memory  where  refers to the vectorisation of the higher-order tensor, then the (recurrent) fast weight update rule given by  results in .
\end{theorem}

\begin{proof}

\end{proof}


\section{A Comment on the Regular bAbI Dataset and Previous Work}
The bAbI tasks is a popular toy dataset to benchmark neural networks with memory augmentations and reasoning capabilities~\citep{babi_tasks_weston}. 
It consists of a set of short stories with questions embedded in the text. 
The stories were generated by simulating multiple entities in a virtual environment and cover different contexts in which entities change their state or interact with each other.
Each story-sample belongs to one of 20 different tasks that the authors of the dataset considered important for intelligent dialogue agents. 
The tasks contain questions which require reasoning capabilities like deduction, coreference, or counting.
All tasks require some level of symbolic reasoning, and the first neural and non-neural baselines demonstrated poor generalisation performance on test data~\citep{babi_tasks_weston}.
In addition to the story sentences, the questions, and the answers, the dataset also included supporting facts which demarcated question-relevant sentences in the story. 
The stories often follow multiple parallel plots where each new sentence is advancing one of the plots by a single fact. 

The bAbI dataset did not include a strict experimental protocol which resulted in several variations that differed slightly.
Early methods achieved good results by relying on the supporting facts~\citep{weston2015memorynets, kumar2016dmn} or other supervised training signals (see e.g.~\citet{johnson2017transitions, Li2016gatedgraphs}).

Some researchers achieved great results by reformatting the data such that the question is read before the story or, similarly, by giving the model the capacity to lookup parts of the story, e.g. through some attentional mechanism, after the question has been read~\citep{sukhbaatar2015end,xiong2016dynamic,dehghani2018universal}.
Such methods have shown to be useful for answering questions while maintaining access to the full story.
We argue that this is similar to open-book question answering.
In such a setting, the model is incentivised to look up information instead of capturing the useful bits of the data it has seen. 
The advantage of the latter becomes more evident in a different scenario: imagine the model is processing a book where a user can ask a question about the content at any time.
An open-book approach will have to store all previous sentences in its memory and apply its answer-search mechanism to all of the data. 
Instead, a closed-book approach would store a compressed version of the story, or the question-relevant information of the story.

It is essential to acknowledge that the sentences in the bAbI stories of all tasks are short and simplistic.
Virtually every sentence contains precisely one fact. 
Because of that, it might be that sentence-level models have an advantage over word-level models.
Indeed, a previous sentence-level model has reported poor performance in the word-level setting~\citep{schlag2018nips}.
This limits their generality since sentences in natural language are often not limited to a single fact.

Lastly, even though the bAbI dataset was initially designed with the questions embedded in the story, virtually all methods so far preprocess the dataset such that a sample with four questions is split into four samples with one question each~\citep{weston2015memorynets}.
This arguably simplifies the problem because the model does not need to maintain the state of other entities which are not relevant to the question once it is read. However, it remains to be tested if this would result in inferior performance.

\section{Concatenated-bAbI Details}
\label{appendix:sec:catbAbI}
Concatenated-bAbI (catbAbI) is a preprocessing and experimental procedure to evaluate autoregressive models in their capability of predicting words which require certain reasoning skills (here answers of questions). 
In this work we only focused on the 10k samples per task version of bAbI but all our scripts can be applied to the 1k version as well. 
We used the same train/test/valid split of the data as in regular bAbI. 
In contrast to previous work, we do not split the stories to contain only one question.
We remove the sentence indecies and concatenate the sentences with answers following a question mark into one long sequence of words. The preprocessed data is a shuffled list of samples. Each sample comes with its task id for diagnosis. All answers are preceeded by a question mark.

To ensure that stories do not overlap and become ambiguous, we add a special end-of-story token before concatenating the new story.
For each word, the preprocessing script provides its task id to measure the performance on different tasks. 
Similarly, it also provides a special answer token which signifies if the current word is an answer or not.
Naturally, the task id and answer information are not provided to the model as an input.
The validation and test data are processed likewise, but for a proper comparison of various models, validation and test data are shuffled only once\footnote{We provide the preprocessed catbAbI data together with our code so future work can compare using the same validation and test sequence.}. During training and evaluation, the validation and test stories are drawn deterministically.


\begin{table}[h]
\caption{Statistics of the catbAbI dataset based on our preprocessing of the regular bAbI data.}
  \label{appendix:tbl:catbAbIstats}
  \centering
  \begin{tabular}{cccccccc}
    \toprule
    subset & number of tokens & number of stories & number of questions \\
    \midrule
    train & \texttildelow 5M & 56,376 & 179,909 \\
    valid & \texttildelow 560k & 6,245 & 19,907 \\
    test & \texttildelow 560k & 6,247 & 19,910 \\
    \bottomrule
  \end{tabular}
  \vspace{-5pt}
\end{table}

During training we uniformly sample stories without replacement and concatenate them into a long sequence. 
Since a question mark is not always the end of a story we resolve any ambiguity by separating the stories with a special end-of-story token. 
The model is trained on this long sequence in an autoregressive way with truncated backpropagation. 
At the end of the epoch, we fill the batch with padding symbols if the sequences in the batch have different lengths. 

In LM-mode we mask padding tokens and in QA-mode we mask everything except the steps with a question mark as input. 
At the end of the epoch we carry over the hidden states to the new epoch. 
Reseting all hidden states to the same or to zeros had a weak negative effect on final performance but was not explored thouroghly. 
For evaluation on valid and test splits a copy of the hidden state of the first batch element is used.
Evaluation on valid is done throughout training with a large batch-size to maintain speed. 
Evaluation on test is done with a batch-size of one. 
During evaluation on valid and test the samples are picked sequentially to ensure that all models are evaluated on the same valid and test sequence of bAbI stories.

\section{Ablation}
\label{appendix:sec:ablation}
We evaluate the FWM model with different number of recurrent steps. Experiments in figure \ref{appendix:fig:readAblation} indicate that just one step is already achieving over 95\% accuracy but more inference steps help on rarer but harder tasks. We also test a FWM version where the read and query keys are concatenated instead of multiplied through the tensor product. In this version, the FWM results in a weight matrix with  instead of . The results in figure \ref{appendix:fig:productAblation} indicate a drop in performance.

\begin{figure}[H]
  \centering
\includegraphics[width=\textwidth]{fwm_read_ablation.png}
  \caption{Comparison of the FWM with the same seed but with different . }
  \label{appendix:fig:readAblation}
\end{figure}

\begin{figure}[H]
  \centering
\begin{subfigure}[b]{\textwidth}
    \includegraphics[width=\textwidth]{fwm_tp_ablation.png}
  \end{subfigure}
  \begin{subfigure}[b]{\textwidth}
    \includegraphics[width=\textwidth]{fwm_tp_ablation_tasks.png}
  \end{subfigure}
  \caption{FWM model with a concatenated keys compared with the tensor product of the keys. With a concatenation of the respective keys and queries the Fast Weight tensor has a squared space and compute complexity  but performs worse on average (top figure). The performance difference is limited to more complex tasks such as 3, 14, 16, 19 (bottom figures).}
  \label{appendix:fig:productAblation}
\end{figure}

\newpage
\section{Hyperparameter search for catbAbI}
\label{appendix:sec:catbAbIparams}
Since catbAbI is an ongoing sequence of stories, backpropagation through time (BPTT) is infeasable for all models which is why we truncate BPTT to the last 200 tokens.
Hyperparameters were chosen such that they fit roughly on one GPU with 16GB of memory.
All models use a token embedding size of 256 and the Adam optimizer. 
We exclusively tuned the hyperparameters for the QM setting and transfer only the best to the LM setting. 
We run a grid search over the batch-size, learning rate, and various model specific parameters such as dropout rates or number of layers on top of additional manually chosen settings. For computational reasons we run two rounds of grid-search: an initial round of 3,000 steps of which the best are moved to the second round where we train them for 30,000 or 60,000 steps. In the following subsections we give further details for each model seperately. 

\subsection{Fast Weight Memory}
We set  and searched experimented with two seeds for batch sizes 64, 128 and learning rates 0.0001, 0.00025, 0.0005, 0.001, 0.002.
\begin{figure}[H]
  \centering
\begin{subfigure}[b]{0.8\linewidth}
    \includegraphics[width=\linewidth]{fwm_3k.png}
  \end{subfigure}
  \begin{subfigure}[b]{0.8\linewidth}
    \includegraphics[width=\linewidth]{fwm_60k.png}
  \end{subfigure}
  \caption{Top: Hyperparameter search runs for different batch sizes and learning rates of the FWM model in the QM setting with the average accuracy on all tasks. Bottom: FWM performance over 60,000 steps with three seeds.}
  \label{appendix:fig:fwm}
\end{figure}

\subsection{Metalearned Neural Memory}
We only experimented with the plastic version of MNM as it was reported to be the best. We used the same hyperparameters for the fast weights as reported by \citet{munkhdalai2019metalearned}: 3 layer of fast weights with a dimensionality of 100. We searched over the batch sizes 64, 128; learning rates 0.00025, 0.0005, 0.001, 0.002; and meta-objective coefficient (reg) 1.0, 2.0. In the first 3,000 steps the MNM didn't show any instability but for longer runs the MNM would sometimes result in NaNs or become unstable.
\begin{figure}[H]
  \centering
\begin{subfigure}[b]{\linewidth}
    \includegraphics[width=\linewidth]{mnm_3k.png}
  \end{subfigure}
  \begin{subfigure}[b]{\linewidth}
    \includegraphics[width=\linewidth]{mnm_60k.png}
  \end{subfigure}
  \caption{Top: Hyperparameter search runs for different batch sizes and learning rates of the MNM model in the QM setting with the average accuracy on all tasks. Bottom: MNM model with three different seeds, batch size 64, and learning rate 0.001 in the QM setting. Reported accuracy is the average on all tasks.}
  \label{appendix:fig:mnm}
\end{figure}


\subsection{Transformer-XL}
We ported the official Transformer-XL implementation\footnote{Source: \url{github.com/kimiyoung/transformer-xl/blob/master/pytorch/mem_transformer.py}} to our own codebase; fully reusing the model code for our catbAbI experiments. We employ a linear learning-rate warm-up schedule over the first 1000 steps and run a grid search over batch size, learning rate, number of layers, and memory length with some additional manual selected parameters. Our best setting uses a learning rate of 0.00025, memory width of 1200, a hidden state size of , an inner dimension of the fully connected part of , and 3 transformer layers. Several long runs can be seen in figure \ref{appendix:fig:txl_long}. Our experiments show how various seeds eventually become unstable and overfit. Some settings also resulted in NaNs which we have removed from figure \ref{appendix:fig:txl_long}. The best performing models and most stable where 3 layer models with a large memory and a small learning rate (see figure \ref{appendix:fig:txl_nl3_seeds}).
\begin{figure}[H]
  \centering
  \vspace{-10pt}
  \begin{subfigure}[b]{.49\linewidth}
    \includegraphics[width=\linewidth]{txl_3k_A.png}
  \end{subfigure}
  \begin{subfigure}[b]{.49\linewidth}
    \includegraphics[width=\linewidth]{txl_3k_B.png}
  \end{subfigure}
  \caption{Hyperparameter search runs for different batch sizes and learning rates of the Transformer-XL in the QM setting with the average accuracy on all tasks. Left graph varies number of layers and memory length. Right graph varies batch size and learning rate for 7 layers.}
  \label{appendix:fig:txl3k}
\end{figure}

\begin{figure}[H]
  \centering
  \vspace{-10pt}
  \begin{subfigure}[b]{.49\linewidth}
    \includegraphics[width=1.0\linewidth]{txl_long_A.png}
  \end{subfigure}
  \begin{subfigure}[b]{.49\linewidth}
    \includegraphics[width=1.0\linewidth]{txl_long_B.png}
  \end{subfigure}
  \begin{subfigure}[b]{.49\linewidth}
    \includegraphics[width=1.0\linewidth]{txl_long_C.png}
  \end{subfigure}
  \begin{subfigure}[b]{.49\linewidth}
    \includegraphics[width=1.0\linewidth]{txl_long_D.png}
  \end{subfigure}
  \caption{Long hyperparameter search runs for TXL with various layers and memory sizes. The experiments are grouped based on the number of layers. Many runs begin to diverge late into the training process.}
  \label{appendix:fig:txl_long}
\end{figure}

\begin{figure}[H]
  \centering
  \vspace{-10pt}
  \includegraphics[width=0.8\textwidth]{txl_nl3_seeds.png}
  \caption{Various seeds for the best Transformer-XL hyperparameters: 3-layers, memory windows of 1200 tokens, a learning rate of 0.00025, and a batch size of 64.}
  \label{appendix:fig:txl_nl3_seeds}
\end{figure}


\subsection{LSTM}
We heavily regularize a four-layer stack of residually connected LSTM cells, each with 512 hidden units. Inspired by AWD-LSTM~\citep{merity2018regularizing}, we use dropout in four different ways to regularize the model. We dropout the tokens of the input sequence, elements of the embedding vector, elements of the recurrent weight matrix, and elements of the of the hidden representation between LSTM layers.
\begin{figure}[H]
  \centering
\begin{subfigure}[b]{.49\linewidth}
    \includegraphics[width=\linewidth]{lstm_3k_A.png}
  \end{subfigure}
  \begin{subfigure}[b]{.49\linewidth}
    \includegraphics[width=\linewidth]{lstm_3k_B.png}
  \end{subfigure}
  \caption{Hyperparameter search runs for different batch sizes and learning rates of the LSTM in the QM setting with the average accuracy on all tasks.}
  \label{appendix:fig:lstm3k}
\end{figure}
\begin{figure}[H]
  \centering
\includegraphics[width=\textwidth]{lstm_60k.png}
  \caption{Average accuracy of three seeds of the best LSTM settings over all tasks on the catbAbI QM-mode dataset.}
  \label{appendix:fig:lstm60k}
\end{figure}

\subsection{Attention to the Recent Past Fast Weights}
\label{appendix:sec:jbfw_search}
We evaluate our own implementation of Fast Weights as introduced by \citet{Ba2016using}. They propose an RNN augmented with fast weights which modulate the slow weights of an Elman RNN using a fixed fast weight learning and decay rate (JBFW). Our hyperparameter search did not result in any model performing over 15\% on the test data. 
\begin{figure}[H]
  \centering
\includegraphics[width=0.8\textwidth]{jbfw_3k.png}
  \caption{Hyperparameter search for the Fast Weights attending to the recent past by \citet{Ba2016using}.}
  \label{appendix:fig:jbfw3k}
\end{figure}

\section{Best catbAbI Runs Broken Down by Task}
\begin{figure}[H]
  \centering
  \vspace{-10pt}
  \includegraphics[width=\textwidth]{best_all_tasks_part1.png}
  \vspace{-10pt}
  \caption{Per-task test set performance comparison of the best catbAbI runs (first part).}
  \label{appendix:fig:best_all_tasks}
\end{figure}
\begin{figure}[H]
  \centering
  \vspace{-10pt}
  \includegraphics[width=\textwidth]{best_all_tasks_part2.png}
  \vspace{-10pt}
  \caption{Per-task test set performance comparison of the best catbAbI runs (second part).}
  \label{appendix:fig:best_all_tasks_part2}
\end{figure}

\newpage
\section{Language Modelling}
\label{appendix:sec:LM}
The code of our language modelling experiments is forked from Uber AI Lab's (\href{https://github.com/uber-research/differentiable-plasticity/tree/master/awd-lstm-lm}{github.com/uber-research/differentiable-plasticity/tree/master/awd-lstm-lm}) which is itself forked from the Salesforce Language model toolkit (\href{https://github.com/Smerity/awd-lstm-lm}{github.com/Smerity/awd-lstm-lm}). The FWM uses the same three layer LSTM as the slow RNN with the same optimisations as done by ~\citet{merity2018regularizing}.
An alternative which we do not explore here is to use multiple FWM-layers each with one LSTM cell and one FWM.
We trained our model for 1000 epochs on PTB and 1600 epochs on WT2. 
Similar to \citet{merity2018regularizing} we switched from Adam to Averaged Stochastic Gradient Descent (ASGD) after 916 epochs and 1372 epochs for PTB and WT2 models respectively.
We tune the dropout parameters on the validation set and, after training, we also tune the softmax temperature (tuning the softmax temperature results in \texttildelow1 ppl of improvement). 
The embedding layers were initialized randomly from a uniform distribution, uniform(-0.25, 0.25), which was crucial in our FWM language models. 
The hyperparameters used for all reported results are in table~\ref{appendix:tbl:lmparams}.

The Transformer-XL PTB results were based using the authors official code and hyperparameter setting (see \href{http://zihangdai.github.io/misc/ptb.zip}{zihangdai.github.io/misc/ptb.zip}) which includes AWD-style regularisation, model averaging, and softmax tuning. The WT2 results are based on the same code using the best hyperparameters found by Tim Dettmers (see \href{https://github.com/TimDettmers/transformer-xl/tree/wikitext2/pytorch}{github.com/TimDettmers/transformer-xl/tree/wikitext2/pytorch}). 
\begin{table}[h]
\caption{Best hyperparameters of the FWM for our language modelling experiments}
  \centering
  \setlength{\tabcolsep}{0.15cm}
  \begin{tabular}{ccccccccc}
    \toprule
    dataset & droupout & dropoute & dropouth & dropouti & wdrop & batch size & ADAM lr & ASGD lr \\
    \midrule
    PTB & 0.4 & 0.1 & 0.3 & 0.5 & 0.66 & 20 & 0.001 & 2.0 \\
    WT2 & 0.4 & 0.1 & 0.25 & 0.7 & 0.61 & 80 & 0.001 & 0.5 \\
    \bottomrule
  \end{tabular}
  \label{appendix:tbl:lmparams}
\end{table}

\subsection{Results}
\label{appendix:sec:lmResults}
\begin{table}[H]
  \caption{The detailed \textit{evaluation results} of the FWM and Transformer-XL language model for all data partitions of the PTB and WT2 datasets using a batch size of 1. Experiment logs can be found in our git repository.}
  \vspace{6pt}
  \centering
  \setlength{\tabcolsep}{0.15cm}
  \begin{tabular}{c|cc|ccc|ccc|ccc}
    \toprule
    \multirow{2}{*}{model} & \multirow{2}{*}{dataset} & \multirow{2}{*}{seed} & \multicolumn{3}{c}{loss} & \multicolumn{3}{c}{ppl} & \multicolumn{3}{c}{bits per word} \\
                        & & & train & valid & test & train & valid & test & train & valid & test \\
    \midrule
    \multirow{6}{*}{FWM} & 
      \multirow{3}{*}{PTB} & 141 & 2.82 & 4.04 & 4.00 & 16.77 & 56.76 & \textbf{54.48} & 4.068 & 5.827 & 5.768 \\
      &  & 142 & 2.66 & 4.05 & 4.01 & 14.26 & 57.43 & 55.17 & 3.834 & 5.844 & 5.786 \\
      &  & 143 & 3.16 & 4.08 & 4.04 & 23.66 & 59.31 & 56.90 & 4.564 & 5.890 & 5.830 \\
      \rule{0pt}{1.3\normalbaselineskip} & \multirow{3}{*}{WT2} 
        & 1881 & 3.32 & 4.23 & 4.18 & 27.80 & 68.74 & 65.07 & 4.797 & 6.103 & 6.024 \\
      & & 1882 & 2.81 & 4.16 & 4.12 & 16.66 & 63.98 & \textbf{61.65} & 4.058 & 6.000 & 5.942 \\
      & & 1883 & 3.28 & 4.23 & 4.17 & 26.60 & 68.39 & 64.91 & 4.733 & 6.096 & 6.020 \\
      \midrule
      \multirow{6}{*}{TXL} & \multirow{3}{*}{PTB} 
         & 2 & 2.87 & 4.09 & 4.04 & 17.62 & 59.71 & 56.63 & 4.139 & 5.900 & 5.824 \\
      &  & 3    & 2.88 & 4.08 & 4.03 & 17.84 & 59.39 & \textbf{56.50} & 4.157 & 5.892 & 5.820\\
      &  & 1111 & 2.86 & 4.09 & 4.03 & 17.52 & 59.73 & 56.53 & 4.131 & 5.900 & 5.821 \\
        \rule{0pt}{1.3\normalbaselineskip} & \multirow{3}{*}{WT2} 
        & 444 & 2.61 & 4.19 & 4.15 & 13.60 & 65.71 & 63.28 & 13.599 & 65.706 & 63.283 \\
      & & 555 & 2.61 & 4.19 & 4.15 & 13.66 & 65.83 & 63.40 & 13.660 & 65.830 & 63.400 \\
      & & 666 & 2.61 & 4.14 & 4.19 & 13.62 & 65.73 & \textbf{63.11} & 13.622 & 65.725 & 63.109 \\
     \bottomrule
  \end{tabular}
\label{appendix:tbl:lmresults}
\end{table}

\newpage
\section{Meta Reinforcement Learning}
\label{appendix:sec:metaRL}
The meta reinforcement learning experiments trains an agent in training POMDPs and evaluates it on test POMDPs.
The environments are directed graphs with labeled edges. 
As part of the data generating process, novel graphs are sampled according the python algorithm in listing \ref{appendix:lst:genEnvs}.
Actions and states are one-hot encoded. The agent receives a 17 dimensional input: the reward location, the current location, the previous action, a fixed bit, the fractional progress as , and the current reward sum. Getting to the reward location gives a reward of 10. Choosing an invalid action gives a penalty of 0.05. We use a discounting factor of 0.9 and a value coefficient of 0.1. The entropy coefficient of A2C is set to 0.03.

The agent and reward locations are randomly selected at the beginning of the episode. With only 5 states, the reward is reachable in at most 5 steps. As elaborated in section \ref{subsec:metaRL}, such optimal behaviour is only possible once the agent has learned the graphs from its experience. Whenever the reward is placed in the environment a reset timer is set to 0. When the agent reaches the reward, or after 6 unsuccessful steps, the reset timer is set to 0 and the reward and agent are randomly placed in the environment. We train with a batch size of 600 agents and optimize the average step loss using the Adam optimizer.



\begin{listing}[H]
\begin{minted}{python}
import numpy as np
    
def sample_adjacency_matrix(n_actions, n_states, random_state):
  while True:
    A = np.zeros((n_actions, n_states, n_states))

    # every state has to be leavable by at least one action
    for from_state in range(n_states):
      to_state = random_state.choice([i for i in range(n_states)
                                      if i != from_state])
      action = random_state.randint(0, n_actions)
      A[action, from_state, to_state] = 1

    # every state has to be reachable by one or more from-states
    for to_state in range(n_states):
      # only select states which don't have any neighbours given an action
      action_list, from_list = np.where(A.sum(2) == 0)
      # remove self from the selection
      options = np.asarray(list(filter(lambda x: x[0] != to_state,
                                       zip(from_list, action_list))))
      indecies = np.arange(options.shape[0])
      chosen_idx = random_state.choice(indecies)
      from_state, action = options[chosen_idx]
      A[action, from_state, to_state] = 1

    # reject if they are not all connected
    Q = A.sum(0)
    Q[Q > 0] = 1
    for _ in range(n_states):
      Q = np.matmul(Q,Q)
    if (Q == 0).sum() == 0:
      return A
\end{minted}
\caption{Python3 code to sample new environments such that any state is reachable by any other state.}
\label{appendix:lst:genEnvs}
\end{listing}

\end{document}
