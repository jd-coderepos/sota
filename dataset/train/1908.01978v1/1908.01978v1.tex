
\documentclass[journal]{IEEEtran}

\ifCLASSINFOpdf
\else
\fi
\usepackage{times}
\usepackage{xcolor}
\usepackage{soul}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{multirow}
\usepackage{graphicx}  \usepackage{helvet}
\usepackage{courier}
\usepackage{url}
\usepackage{color}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[ruled,vlined]{algorithm2e}

\newtheorem{myDef}{Definition}
\newtheorem{myTheo}{Theorem}

\usepackage[loose]{subfigure}
\usepackage{epstopdf}

\usepackage{float}



\def\ie{{\em i.e.}}
\def\eg{{\em e.g.}}

\def\red#1{\textcolor{red}{[#1]}}
\def\blue#1{\textcolor{blue}{[#1]}}
\def\green#1{\textcolor{green}{#1}}

\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
\title{Multi-view Deep Subspace Clustering Networks}


\author{Pengfei Zhu, Binyuan Hui, Changqing Zhang, Dawei Du, Longyin Wen, Qinghua Hu

\thanks{Pengfei Zhu, Binyuan Hui, Changqing Zhang, and Qinghua Hu are with School of Computer Science and Technology, Tianjin University.
Dawei Du is with Computer Science Department, University at Albany, State University of New York. 
Longyin Wen is with JD Digits.
This work was supported by the National Natural Science Foundation of China under Grants 61876127, Natural Science Foundation of Tianjin Under Grants 17JCZDJC30800, 18YFZCGX00390, 18YFZCGX00680, and Young  Elite  Scientists   Sponsorship  Program by Tianjin.
}}

\markboth{SUBMISSION TO IEEE Transactions on Image Processing, 2019}{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Journals}

\maketitle

\begin{abstract}
Multi-view subspace clustering aims to discover the inherent structure by fusing multi-view complementary information.
Most existing methods first extract multiple types of hand-crafted features and then learn a joint affinity matrix for clustering.
The disadvantage lies in two aspects: 1) Multi-view relations are not embedded into feature learning. 2) The end-to-end learning manner of deep learning is not well used in multi-view clustering.
To address the above issues, we propose a novel multi-view deep subspace clustering network (MvDSCN) by learning a multi-view self-representation matrix in an end-to-end manner.
MvDSCN consists of two sub-networks, i.e., diversity network (Dnet) and universality network (Unet).
A latent space is built upon deep convolutional auto-encoders and a self-representation matrix is learned in the latent space using a fully connected layer.
Dnet learns view-specific self-representation matrices while Unet learns a common self-representation matrix for all views.
To exploit the complementarity of multi-view representations, Hilbert Schmidt Independence Criterion (HSIC) is introduced as a diversity regularization, which can capture the non-linear and high-order inter-view relations.
As different views share the same label space, the self-representation matrices of each view are aligned to the common one by a universality regularization.
Experiments on both multi-feature and multi-modality learning validate the superiority of the proposed multi-view subspace clustering model.
\end{abstract}

\begin{IEEEkeywords}
subspace clustering, multi-view learning, self-representation, deep clustering.
\end{IEEEkeywords}


\section{Introduction}
Subspace clustering aims to segment a set of unlabeled samples drawn from a union of multiple subspaces corresponding to different clusters into several groups.
Recently, self-representation based models have achieved superior performance in subspace clustering~\cite{Guangcan2013Robust,ji2017deep,Zhou2018DeepAS}. It assumes that a sample can be represented by a linear combination of a set of samples:
\begin{equation}\label{sr}
  \begin{array}{l}
\mathop {\min }\limits_{\bf{Z}} L({\bf{X}},{\bf{Z}}) + R({\bf{Z}}),\; \; s.t.\quad{\bf{X}} = {\bf{XZ}},
\end{array}
\end{equation}
where $\bf{X} \in \mathbb{R}^{d \times n}$ and $\bf{Z} \in \mathbb{R}^{n \times n}$ denote the training data and self-representation matrix, respectively.
$L({\bf{X}},{\bf{Z}})$ represents the reconstruction loss and $R({\bf{Z}})$ is the regularization item.
The key differences of self-representation based subspace clustering models lie in the option of the loss function and regularizer.
For $R({\bf{Z}})$, $l_0$-norm, $l_1$-norm, square of Frobenius norm, elastic net, trace Lasso, and k-block diagonal regularizer have been used under certain subspace assumptions \cite{Lu2018Subspace}.
As hand-crafted features cannot well capture the severe variations, deep subspace clustering models are developed to jointly learn hierarchical representation and cluster structure \cite{Chen2017UnsupervisedMC,Tian2017DeepClusterAG,Jiang2018WhenTL,Zhou2018DeepAS,Peng2018StructuredAF,Guo2017ImprovedDE,Li2017ProjectiveLS,Chellappa2018DeepDC,Liang2018SubGANAU,Lezama2018OLEOL}.  

\begin{figure}
	\centering
	\includegraphics[width=1\linewidth]{mv}
	\caption{Examples of multi-view learning.
A sample can be represented by different modalities, e.g., image, video and text.
Different kinds of features, e.g., SIFT, Gabor, and deep features can be extracted.
Generally, multi-view learning covers multi-modal and multi-feature learning. }
	\label{mv}
\end{figure}

The rapid growth of digital sensors and widespread application of social networks bring about the explosion of multi-modal data in multimedia analysis \cite{luo2017simple}, medical image analysis \cite{kim2015dasc}, autonomous driving \cite{chen2017multi}, etc.
As shown in Figure \ref{mv}, different types of data can be collected, including text, image, audio and video, to represent a sample.
Even for single-modal data, e.g., images or video sequences, diverse features can be extracted to capture scale, occlusion, illumination, rotation variations for robust recognition \cite{Zhao2017MultiViewCV}.
Generally, multi-view learning covers multi-modal and multi-feature learning.
Multi-modal and multi-feature information can be fused to boost the performance of subspace clustering.

Multi-view subspace clustering (MVSC) aims to utilize data collected from different modalities or represented by different types of features to discover the underlying clustering structure.
Most MVSC methods design multi-view regularizer to characterize the inter-view relationships between several types of hand-crafted features for multi-view clustering~\cite{Cui2007NonredundantMC,Gnnemann2012MultiviewCU,Wang2015RobustSC,Zhang2017LatentMS,Xu2016DiscriminativelyEK,Zhao2017MultiViewCV,Wang2018PartialMC}.
However, their performances are still not satisfactory for two reasons.
Firstly, existing methods adopt a two-stage strategy, i.e., first extracting features and then learning the affinity matrix. The features extraction process is irrelevant to the subspace clustering task.
Multi-view relationships can only work during the affinity matrix learning process, which ignores the role of inter-view relations in feature learning.
Secondly, they consider little about hierarchical representation learning in an end-to-end manner.

In this paper, we propose a multi-view deep subspace clustering networks (MvDSCN) by learning a multi-view self-representation matrix in an end-to-end manner.
MvDSCN is composed of diversity network (Dnet) and universality network (Unet).
Dnet learns view-specific self-representation matrices (${\bf{Z}}_1, {\bf{Z}}_2,...,{\bf{Z}}_v$) while Unet learns a common self-representation matrix $\bf{Z}$.
Deep convolutional auto-encoders are learned for each view with either handcrafted features or raw data as the input.
Multi-view reconstruction and self-representation losses are simultaneously minimized.
To exploit the complementary multi-view information, a diversity regularizer is defined based on Hilbert Schmidt Independence Criterion (HSIC).
Additionally, we used a universality regularizer to make view-specific $\bf{Z}_i$ close to the common $\bf{Z}$.
With diversity and universality regularization, multi-view relations are well embedded in both feature learning and self-representation stages.
Experiments on both multi-feature and multi-modal clustering validate the superiority of the proposed model to the state-of-the-art subspace clustering methods.

The structure of this paper is organized as:
Section \ref{s2} introduces the related work on multi-view learning, self-representation and auto-encoders.
Section \ref{s3} presents the proposed multi-view clustering model.
Section \ref{s4} conducts experiments on both multi-feature and multi-modal tasks.
Section \ref{s5} concludes and gives the future work.

\begin{figure*}[!htbp]
	\centering
	\includegraphics[width=1\textwidth]{artitecture.pdf}
	\caption{The network architecture of multi-view deep subspace clustering}
	\label{architecture}
\end{figure*}

\section{Related Work}
\label{s2}
In this section, we give a brief review of multi-view clustering, self-representation, and auto-encoders.
\subsection{Multi-view Clustering}
Subspace clustering aims to uncover the inherent cluster structure from data composed of multiple subspaces \cite{Zhou2018DeepAS}.
In the past few years, most existing subspace clustering methods focus on learning a good affinity matrix and then conduct spectral clustering.
Self-representation based subspace clustering methods are essentially based on the hypothesis that a sample can be reconstructed by a linear combination of other samples.
Sparse subspace clustering (SSC) imposes $l_1$-norm regularization on the representation coefficients to enhance the sparsity \cite{Elhamifar2009Sparse}.
Low rank representation (LRR) explores the multi-block diagonal property of the self-representation matrix to discover the multiple subspace structure \cite{Guangcan2013Robust,Lu2018Subspace}.
Deep subspace clustering network embeds self-representation into deep convolutional autoencoder by a fully connected layer \cite{ji2017deep}.
To conduct clustering in an end-to-end manner, clustering loss is proposed to output the clustering results directly \cite{Peng2018StructuredAF}.
Deep adversarial subspace clustering uses GAN-like model to evaluate the clustering performance besides self-representation loss \cite{Zhou2018DeepAS}.

Multi-view clustering boosts the performance of clustering by exploring the complementary information by modeling inter-view relations or learning a latent representation.
Most existing multi-view clustering methods can be considered as an extension of single-view models, including spectral clustering \cite{Wang2015RobustSC}, matrix factorization \cite{Zhao2017MultiViewCV}, and k-means \cite{Xu2016DiscriminativelyEK}, etc.
Multi-view relations can be generally categorized into universality and diversity \cite{Gnnemann2012MultiviewCU,Wang2015RobustSC,Xu2016DiscriminativelyEK,Zhang2017LatentMS}.
Universality emphasizes on that all views should be similar while diversity focuses on the inter-view complementarity and therefore induces diverse view-specific representation.
Some work build multi-view connections by a common latent representation for clustering and model multi-view relations using neural networks \cite{ZhangGeneralized}.
Deep learning has achieved superior performance in many tasks because of the end to end learning manner to a great extent.
However, the existing multi-view subspace clustering methods treat multi-view feature extraction and affinity learning as two separate stages.
Additionally, due to the view-specific characteristic, it is unreasonable to force the self-representation matrices of all views to be the same.

\subsection{Self-representation}
Self-representation reflects the intra-relations among samples, and has been widely used in image processing, clustering, feature selection, and deep learning.
In image processing, especially image denoising, non-local mean has been widely used by reconstructed a pixel or image patch using related pixels or patches in the image \cite{buades2005non}, which inspires many successful image processing models in low-level vision.
Besides pixel-level self-representation, a sample can be well reconstructed by a linear combination of bases.
Self-representation has been successfully used in clustering in that it can accurately capture the sample relations by embedding sparse, dense, or low-rank priors ~\cite{Guangcan2013Robust,ji2017deep,Lu2018Subspace}.
To alleviate the curse of dimensionality, feature selection aims to select a subset of features by evaluating the importance of features.
Feature-level self-representation assumes that one feature can be reconstructed by all features, and the self-representation coefficients can be used for feature evaluation \cite{zhu2015unsupervised,zhu2016coupled,zhu2017subspace}.
Inspired by the success of non-local mean in image denoising, a non-local neural network is proposed to utilize the relations across elements of feature maps, channels, or frames to improve the representation ability of the networks \cite{wang2018non}.

\subsection{Auto-Encoders}
Auto-encoders (AE) extract features of data by mapping the data to a low-dimensional space.
With the rapid development of deep learning, deep (or stacked) auto-encoders have become popular for unsupervised learning.
Deep auto-encoders have been widely used in dimensionality reduction \cite{Hinton2006ReducingTD} and image denoising \cite{Vincent2010StackedDA}.
Recently, deep auto-encoders have been used to initialize deep embedding networks for unsupervised clustering \cite{Xie2016UnsupervisedDE}.
The work in \cite{Peng2016DeepSC} uses a fully connected deep auto-encoders by incorporating a sparsity prior into the hidden representation learning to preserve the sparse reconstruction relation.
By comparison, \cite{ji2017deep} directly learn the affinities between all data points through a deep auto-encoder network by using a fully connected  self-representation layer.

Since convolutional layers have fewer parameters and stronger learning ability than the fully connected layer, convolutional auto-encoders(CAE) that can be trained in an end to end manner are designed for feature learning from unlabeled data.
The work in \cite{Li2018DiscriminativelyBI} is the first trial to train CAE directly in an end to end manner without pre-training.
Convolutional neural networks can be initialized by a CAE stack \cite{Masci2011StackedCA} which is an unsupervised method for hierarchical feature extraction.
CAE have been successfully used for generative adversarial networks (GANs).
\cite{Nguyen2017PlugP} combines a convolutional auto-encoder loss, a GAN loss, and a classification loss defined using a pre-trained classifier.
In the field of natural language processing, \cite{Zhang2017DeconvolutionalPR} proposed a general framework for text modeling by embedding a paragraph into a latent representation vector using CAE.



\begin{figure*}
	\centering
	\includegraphics[width=0.8\linewidth]{affinity2}
	\caption{Visualization of affinity matrices of different views.
             The first three columns are affinity matrices of view1, view2, view3 learned by DSCN \cite{ji2017deep}.
             The last column is the proposed MvDSCN obtained all views.
             The top row is the result on Yale dataset, and the bottom row is the result on ORL dataset.}
	\label{affinity}
\end{figure*}

\section{Multi-view Deep Subspace Clustering}
\label{s3}

In this section we present the proposed multi-view deep subspace clustering networks (MvDSCN).
\subsection{Network Architecture}
Let ${\bf{X}}_1,..., {\bf{X}}_i,...,{\bf{X}}_v$ denote the inputs of multiple views, where ${\bf{X}}_i \in \mathbb{R}^{n \times d_i}$, $v$, $n$ and $d_i$ are the number of views, samples, and features in the $i^{th}$ view, respectively.
$ {\bf{X}}_i$ can be hand-crafted features or raw data, such as image and RGB-D data.
The architecture of multi-view deep subspace clustering is shown in Figure \ref{architecture}.
The proposed network consists of two parts, i.e., diversity net (Dnet) that learns view-specific representation and universality net (Unet) that shares view-consistent self-representation matrix.

Dnet embeds the input ${\bf{X}}_i$ into the hidden representation ${\bf{F}}^s_i$ by the view-specific encoder for the $i^{th}$ view.
Then self-representation is conducted by a fully connected layer without bias and non-linear activations, i.e., ${\bf{F}}^s_i$=${\bf{F}}^s_i {\bf{Z}}_i$.
Unet uses a common self-representation matrix $\bf{Z}$ for all views, which is connected with hidden representation ${\bf{F}}^c_1, {\bf{F}}^c_2,..., {\bf{F}}^c_v$ of all views.
After self-representation layer, the samples are recovered by the view-specific decoders.

For both Dnet and Unet, we advocate the usage of convolutional auto-encoders with fewer parameters and stronger learning ability rather than the fully connected layer.
We use three-layer encoders with [64, 32, 16] channels, and three-layer decoders with [16, 32, 64] channels correspondingly.
We adopt a $3 \times 3$ kernel and rectified linear unit(ReLU) \cite{Nair2010RectifiedLU} for the non-linear activations.
Notably, no pooling layers are used. The latent features are then back to the space of the same size as the input via the transpose convolution layers.

\subsection{Loss Function}
The losses of the proposed MvDSCN consists of two parts, i.e., reconstruction loss by auto-encoders and self-representation loss.
\begin{equation}\label{loss}
\min \left\{ \begin{array}{l}
 \sum\limits_{i = 1}^v {\left\| {{{\bf{X}}_i} - {\bf{\hat X}}_i^s} \right\|_F^2 + \left\| {{{\bf{X}}_i} - {\bf{\hat X}}_i^c} \right\|_F^2}  +  \\
 \sum\limits_{i = 1}^v {\left\| {{\bf{F}}_i^s - {\bf{F}}_i^s{{\bf{Z}}_i}} \right\|_F^2 + \left\| {{\bf{F}}_i^c - {\bf{F}}_i^c{\bf{Z}}} \right\|_F^2}  \\
 \end{array} \right\}
\end{equation}

To embed multi-view relations into feature learning and self-representation, two types of regularizer are used.
To exploit the complementary information from multiple views, e.g., RGB and depth information, a diversity regularization is defined based on Hilbert Schmidt Independence Criterion (HSIC).
HSIC measures the nonlinear and high-order correlations and has been successfully used in multi-view subspace clustering.

Assuming that there are two variables ${\bf{A}} = \left[ {{{\bf{a}}_1},...,{{\bf{a}}_i},...,{{\bf{a}}_N}} \right]$ and ${\bf{B}} = \left[ {{{\bf{b}}_1},...,{{\bf{b}}_i},...,{{\bf{b}}_N}} \right]$,
we define a mapping $\phi ({\bf{a}})$ from ${\bf{a}} \in {\mathfrak{A}}$ to kernel space $\mathfrak{F}$, where the inner product of two vectors is defined as $k({{\bf{a}}_1},{{\bf{a}}_2}) = \left\langle {\phi ({{\bf{a}}_1}),\phi ({{\bf{a}}_2})} \right\rangle$.
Then $\varphi ({\bf{b}})$ is defined to map ${\bf{b}} \in {\mathfrak{B}}$ to kernel space $\mathfrak{G}$.
Similarly, the inner product of two vectors in $\mathfrak{G}$ is defined as $g({{\bf{b}}_1},{{\bf{b}}_2}) = \left\langle {\phi ({{\bf{b}}_1}),\phi ({{\bf{b}}_2})} \right\rangle$.
The empirical version of HSIC is induced as:
\begin{myDef}
Consider a series of $N$ independent observations drawn from $p_\mathbf{ab}$, $\mathcal{Z}:=\{(\mathbf{a}_1,\mathbf{b}_1),...,(\mathbf{a}_N,\mathbf{b}_N)\} \subseteq \mathfrak{A} \times \mathfrak{B}$, an estimator of HSIC, written as HSIC($\mathfrak{Z},\mathfrak{F},\mathfrak{G}$), is given by:
\begin{equation}{
\text{HSIC}(\mathfrak{Z},\mathfrak{F},\mathfrak{G}) =  (N-1)^{-2}tr(\mathbf{G}_1\mathbf{H}\mathbf{G}_2\mathbf{H}),
}
\end{equation}
where $tr(\cdot)$ is the trace of a square matrix. $\mathbf{G}_1$ and $\mathbf{G}_2$ are the Gram matrices with $g_{1,ij}=g_1(\mathbf{a}_i,\mathbf{a}_j)$, $g_{2,ij}=g_2(\mathbf{b}_i,\mathbf{b}_j)$. $h_{ij}=\delta_{ij}-1/N$ centers the Gram matrix which has zero mean in the feature space. Please refer to \cite{gretton2005measuring, Cao2015DiversityinducedMS} for more details about HSIC.
\end{myDef}
Based on HSIC, the diversity regularizer is defined as
\begin{equation}\label{Divregu}
  {R_d}({{\bf{Z}}_1},{{\bf{Z}}_2},...,{{\bf{Z}}_v}) = \sum\limits_{ij} {HSIC({{\bf{Z}}_i},{{\bf{Z}}_j})}
\end{equation}
The diversity regularizer in Eq. (\ref{Divregu}) can effectively exploit the complementary information from multiple views.
As all views share the same decision space, the view-specific self-representation matrices that reflect sample relations should be aligned with the common self-representation matrix in Unet. We define a centralization regularizer as follows
\begin{equation}\label{Cengru}
{R_c}({{\bf{Z}}},{{\bf{Z}}_1},{{\bf{Z}}_2},...,{{\bf{Z}}_v}) = \sum\limits_{i = 1}^v {\left\| {{\bf{Z}} - {{\bf{Z}}_i}} \right\|_F^2}
\end{equation}
By taking multi-view relations into account, the objective function becomes
\begin{equation}\label{objective}
\begin{array}{l}
 \min \left\{ \begin{array}{ll}
 \qquad \underbrace {\sum\limits_{i = 1}^v {\left\| {{{\bf{X}}_i} - {\bf{\hat X}}_i^s} \right\|_F^2 + \left\| {{{\bf{X}}_i} - {\bf{\hat X}}_i^c} \right\|_F^2} }_\text{auto-encoder\;\;loss}
  \\
  + {\lambda _1} \underbrace {\sum\limits_{i = 1}^v {\left\| {{\bf{F}}_i^s - {\bf{F}}_i^s{{\bf{Z}}_i}} \right\|_F^2 + \left\| {{\bf{F}}_i^c - {\bf{F}}_i^c{\bf{Z}}} \right\|_F^2} }_\text{self-representation\;\;loss} \\
 {\rm{ + }}{\lambda _2}\underbrace {\left( {\left\| {\bf{Z}} \right\|_p^{}{\rm{ + }}\sum\limits_{i = 1}^v {\left\| {{{\bf{Z}}_i}} \right\|_p^{}} } \right)}_\text{lp - norm\;\;regularizer} \\
 {\rm{ + }}{\lambda _3}\underbrace {\sum\limits_{i = 1}^v {\left\| {{\bf{Z}} - {{\bf{Z}}_i}} \right\|_F^2} }_\text{universality\;\; regularizer} \\
  + {\lambda _4}\underbrace {\sum\limits_{ij} {HSIC({{\bf{Z}}_i},{{\bf{Z}}_j})} }_\text{diversity\;\;regularizer} \\
 \end{array} \right\} \\
 s.t.\quad{\rm{ diag(}}{{\bf{Z}}_i}{\rm{) = }}{\bf{0}},i = 1,2,...,v,{\rm{diag(}}{\bf{Z}}{\rm{) = }}{\bf{0}} \\
 \end{array},
\end{equation}
where $\lambda_1$, $\lambda_2$, $\lambda_3$ and $\lambda_4$ are positive constants, and $\left\| {\bf{Z}} \right\|_p^{}$ is $l_p$-norm on $\bf{Z}$. Here we can also consider other types of regularizer, e.g., nuclear norm \cite{Guangcan2013Robust}, and block diagonal regularizer \cite{lu2019subspace}.
Figure \ref{affinity} shows the affinity matrix of each view learned independently by deep subspace clustering network in \cite{ji2017deep} and the one learned by our proposed MvDSCN on multi-view data.  The affinity matrix learned by MvDSCN has better block diagonal property and less noise.


\subsection{Optimization}
We use a gradient decent method to solve the problem in Eq. (\ref{objective}).
For the back propagation (BP) process, the gradients should be derived for each variable.
The encoders and decoders can be updated by standard BP.
Here we focus on the updating of self-representation layer.
As the optimization problems in Eq. (\ref{objective}) with respect to the view-specific self-representation matrices ${\bf{Z}}_1, {\bf{Z}}_2,...,{\bf{Z}}_v$ and view-consistent $\bf{Z}$ are convex, we can get the gradients easily.

When we use square of Frobenius norm regularization, the gradient for ${\bf{Z}}_i$ is
\begin{equation}
\begin{aligned}
\frac{\partial{L_{{\bf{Z}}_i}}}{\partial{{\bf{Z}}_i}}
& = 2 \lambda_1 {{{\bf{F}}_i^s}^T {{\bf{F}}_i^s} ({\bf{Z}}_i - \bf{I})}  -
2 \lambda_3 ({{\bf{Z}} - {{\bf{Z}}_i}})  \\
& + 2 \lambda_2 {\bf{Z}} + \lambda_4 (N-1)^{-2} \sum\limits_{j > i}{({\bf{H}}{{\bf{Z}}_j}{\bf{H}})}^T \\
\end{aligned}
\end{equation}
The gradient for ${\bf{Z}}$ is
\begin{equation}
\begin{aligned}
\frac{\partial{L_{\bf{Z}}}}{\partial{\bf{Z}}}
= 2 \lambda_1 \sum\limits_{i = 1}^v {{{\bf{F}}_i^c}^T {{\bf{F}}_i^c} ({\bf{Z}} - \bf{I})} + {2 \lambda_2 \bf{Z}} + 2 \lambda_3 ({\bf{Z}} - {\bf{Z}}_i)
\end{aligned}
\end{equation}


\begin{algorithm}
	\caption{The algorithm of MvDSCN.}
	\KwIn{Unlabeled multi-view data $\{{\bf{X}}_1,...,{\bf{X}}_v\}$, hyper-parameter $\lambda_1$, $\lambda_2$, $\lambda_3$ and $\lambda_4$, pre-trained epochs $n$, learning rate $\alpha_t$, initialize parameter $\theta_{\text{Dnet}}$, $\theta_{\text{Unet}}$ with random values\;
}
	\For{$j=1$ to $n$}
	{
		Update auto-encoders of $\theta_{\text{Dnet}}$\;
		Update auto-encoders of $\theta_{\text{Unet}}$\;
	}
	\While{not converged}
	{
		Compute the gradient of Eq.(6) and update auto-encoders of $\theta_{\text{Dnet}}$, $\theta_{\text{Unet}}$ \;
		Optimize ${{\bf{Z}}_1},{{\bf{Z}}_2},...,{{\bf{Z}}_v}$, and ${\bf{Z}}$ by Eqs.(7) and (8)\;
	}
	Perform spectral clustering using affinity matrix ${\bf{Z}}$\;	
	\KwOut{Clustering result $C$}.
	\label{alg}
\end{algorithm}

We first pre-train the deep auto-encoder without the self-representation layer on all multi-view data because the network is difficult to directly train from scratch and avoid the trivial all-zero solution while minimizing the loss function.
We then use the pre-trained parameters to initialize the convolutional encoder-decoder layers of both Dnet and Unet.
In the fine-tuning stage, we build a big batch using all the data to minimize the loss function.
The model is trained with Adam \cite{Kingma2015AdamAM} and an initial learning rate of $0.001$.
For the regularization hyper-parameters of self-representation loss and $l_p$-norm regularizer, we usually set $\lambda_1 = 1.0 \times 10^{\frac{k}{10} - 3}$ where the $k$ is the number of subspaces, $\lambda_2 = 1.0$, $\lambda_3 = 0.1$, $\lambda_4 = 0.1$.



Our network jointly updates Dnet and Unet.
Once the network converges, we can use the parameters of the common self-expressive layer for all views to construct an affinity matrix $\left( {\left| {\bf{Z}} \right| + {{\left| {\bf{Z}} \right|}^T}} \right)/2$ for spectral clustering.
Similar to \cite{ji2017deep}, since we have no access to labels, our training strategy is unsupervised.
Besides, the algorithm for solving MvDSCN is summarized in Alg. \ref{alg}.

\begin{table*}
	\centering
	\caption{Results on four multi-feature datasets (mean $\pm$ standard deviation). Higher value indicates better performance.}
	\begin{tabular}{|c|c|c|c|c|c|}
		
		\hline
		Datasets & Methods & NMI & ACC & AR & F-measure\\
		\hline\hline
		\multirow{12}*{Yale}
		& BestSV & 0.654 $\pm$ 0.009 & 0.616 $\pm$ 0.030 & 0.440 $\pm$ 0.011 & 0.475 $\pm$ 0.011 \\
		& LRR  & 0.709 $\pm$ 0.011 & 0.697 $\pm$ 0.000 & 0.515 $\pm$ 0.004 & 0.547 $\pm$ 0.007 \\
		& Min-Disagreement & 0.645 $\pm$ 0.005 & 0.615 $\pm$ 0.043 & 0.433 $\pm$ 0.006 & 0.470 $\pm$ 0.006 \\
		& Co-Reg & 0.648 $\pm$ 0.002 & 0.564 $\pm$ 0.000 & 0.436 $\pm$ 0.002 & 0.466 $\pm$ 0.000 \\
		& RMSC & 0.684 $\pm$ 0.033 & 0.642 $\pm$ 0.036 & 0.485 $\pm$ 0.046 & 0.517 $\pm$ 0.043 \\
		& DSCN & 0.738 $\pm$ 0.006 & 0.727 $\pm$ 0.014 & 0.509 $\pm$ 0.021 & 0.542 $\pm$ 0.019 \\
		& DCSC & 0.744 $\pm$ 0.009 & 0.733 $\pm$ 0.007 & 0.521 $\pm$ 0.011 & 0.556 $\pm$ 0.012 \\
		& DC & 0.756 $\pm$ 0.001 & 0.766 $\pm$ 0.007 & 0.553 $\pm$ 0.017 & 0.579 $\pm$ 0.004 \\
		& LMSC & 0.702 $\pm$ 0.013 & 0.670 $\pm$ 0.012 & 0.472 $\pm$ 0.018 & 0.506 $\pm$ 0.010 \\
		& DMF & 0.782 $\pm$ 0.010 & 0.745 $\pm$ 0.011 & 0.579 $\pm$ 0.002 & 0.601 $\pm$ 0.002 \\
		& MSCN  & 0.769 $\pm$ 0.003 & 0.772 $\pm$ 0.004 & 0.582 $\pm$ 0.012 & 0.598 $\pm$ 0.006 \\
		& MvDSCN & \textbf{0.797 $\pm$ 0.007} & \textbf{0.824 $\pm$ 0.004} & \textbf{0.626 $\pm$ 0.011} & \textbf{0.650 $\pm$ 0.010} \\
		\hline
		\multirow{12}*{ORL}
		& BestSV & 0.903 $\pm$ 0.016 & 0.777 $\pm$ 0.033 & 0.738 $\pm$ 0.001 & 0.711 $\pm$ 0.043 \\
		& LRR  & 0.895 $\pm$ 0.006 & 0.773 $\pm$ 0.003 & 0.724 $\pm$ 0.002 & 0.731 $\pm$ 0.004 \\
		& Min-Disagreement & 0.816 $\pm$ 0.001 & 0.734 $\pm$ 0.040 & 0.621 $\pm$ 0.003 & 0.663 $\pm$ 0.003 \\
		& Co-Reg & 0.853 $\pm$ 0.003 & 0.715 $\pm$ 0.000 & 0.602 $\pm$ 0.004 & 0.615 $\pm$ 0.000 \\
		& RMSC & 0.872 $\pm$ 0.012 & 0.723 $\pm$ 0.025 & 0.645 $\pm$ 0.029 & 0.654 $\pm$ 0.028 \\
		& DSCN & 0.883 $\pm$ 0.005 & 0.801 $\pm$ 0.009 & 0.704 $\pm$ 0.012 & 0.711 $\pm$ 0.011 \\
		& DCSC & 0.893 $\pm$ 0.003 & 0.811 $\pm$ 0.003 & 0.709 $\pm$ 0.021 & 0.718 $\pm$ 0.004 \\
		& DC	& 0.865 $\pm$ 0.011 & 0.788 $\pm$ 0.002 & 0.684 $\pm$ 0.007 & 0.701 $\pm$ 0.008 \\
		& LMSC & 0.931 $\pm$ 0.011 & 0.819 $\pm$ 0.017 & 0.769 $\pm$ 0.044 & 0.758 $\pm$ 0.009 \\
		& DMF & 0.933 $\pm$ 0.010 & 0.823 $\pm$ 0.021 & 0.783 $\pm$ 0.001 & 0.773 $\pm$ 0.002 \\
		& MSCN  & 0.928 $\pm$ 0.001 & 0.833 $\pm$ 0.008 & 0.790 $\pm$ 0.005 & 0.787 $\pm$ 0.001 \\
		& MvDSCN & \textbf{0.943 $\pm$ 0.002} & \textbf{0.870 $\pm$ 0.006} & \textbf{0.819 $\pm$ 0.001} & \textbf{0.834 $\pm$ 0.012} \\
		\hline
		\multirow{12}*{Still DB}
		& BestSV & 0.104 $\pm$ 0.078 & 0.297 $\pm$ 0.089 & 0.063 $\pm$ 0.001 & 0.221 $\pm$ 0.064 \\
		& LRR  & 0.109 $\pm$ 0.030 & 0.306 $\pm$ 0.039 & 0.066 $\pm$ 0.002 & 0.240 $\pm$ 0.052 \\
		& Min-Disagreement & 0.097 $\pm$ 0.005 & 0.336 $\pm$ 0.014 & 0.103 $\pm$ 0.013 & 0.223 $\pm$ 0.004 \\
		& Co-Reg & 0.093 $\pm$ 0.016 & 0.263 $\pm$ 0.024 & 0.092 $\pm$ 0.004 & 0.226 $\pm$ 0.035 \\
		& RMSC & 0.106 $\pm$ 0.056 & 0.285 $\pm$ 0.020 & 0.113 $\pm$ 0.063 & 0.232 $\pm$ 0.021 \\
		& DSCN  & 0.216 $\pm$ 0.011 & 0.323 $\pm$ 0.006 & 0.145 $\pm$ 0.002 & 0.293 $\pm$ 0.019 \\
		& DCSC & 0.222 $\pm$ 0.008 & 0.325 $\pm$ 0.007 & 0.148 $\pm$ 0.003 & 0.301 $\pm$ 0.002 \\
		& DC  & 0.199 $\pm$ 0.003 & 0.315 $\pm$ 0.001 & 0.131 $\pm$ 0.001 & 0.280 $\pm$ 0.011 \\
		& LMSC & 0.137 $\pm$ 0.032 & 0.328 $\pm$ 0.029 & 0.088 $\pm$ 0.007 & 0.269 $\pm$ 0.055 \\
		& DMF & 0.154 $\pm$ 0.010 & 0.336 $\pm$ 0.017 & 0.124 $\pm$ 0.001 & 0.265 $\pm$ 0.005 \\
		& MSCN  & 0.168 $\pm$ 0.001 & 0.312 $\pm$ 0.008 & 0.133 $\pm$ 0.005 & 0.261 $\pm$ 0.001 \\
		& MvDSCN & \textbf{0.245 $\pm$ 0.020} & \textbf{0.377 $\pm$ 0.023} & \textbf{0.169 $\pm$ 0.003} & \textbf{0.320 $\pm$ 0.015} \\
		\hline
		\multirow{12}*{BBCSport}
		& BestSV & 0.715 $\pm$ 0.060 & 0.836 $\pm$ 0.037 & 0.659 $\pm$ 0.005 & 0.768 $\pm$ 0.038 \\
		& LRR  & 0.690 $\pm$ 0.019 & 0.832 $\pm$ 0.026 & 0.667 $\pm$ 0.008 & 0.774 $\pm$ 0.023 \\
		& Min-Disagreement & 0.776 $\pm$ 0.019 & 0.797 $\pm$ 0.049 & 0.783 $\pm$ 0.034 & 0.260 $\pm$ 0.013 \\
		& Co-Reg & 0.718 $\pm$ 0.003 & 0.564 $\pm$ 0.000 & 0.696 $\pm$ 0.001 & 0.766 $\pm$ 0.002 \\
		& RMSC & 0.608 $\pm$ 0.007 & 0.737 $\pm$ 0.003 & 0.723 $\pm$ 0.025 & 0.655 $\pm$ 0.002 \\
		& DSCN  & 0.652 $\pm$ 0.000 & 0.821 $\pm$ 0.000 &  0.856 $\pm$ 0.001 & 0.683 $\pm$ 0.001 \\
		& DCSC & 0.683 $\pm$ 0.001 & 0.843 $\pm$ 0.000 & 0.864 $\pm$ 0.012 & 0.712 $\pm$ 0.002 \\
		& DC	& 0.556 $\pm$ 0.001 & 0.724 $\pm$ 0.000 & 0.781 $\pm$ 0.000 & 0.492 $\pm$ 0.000 \\
		& LMSC & 0.826 $\pm$ 0.006 & 0.900 $\pm$ 0.044 & 0.893 $\pm$ 0.012 & 0.887 $\pm$ 0.071 \\
		& DMF & 0.821 $\pm$ 0.003 & 0.890 $\pm$ 0.031 & 0.883 $\pm$ 0.012 & \textbf{0.889 $\pm$ 0.001} \\
		& MSCN  & 0.813 $\pm$ 0.002 & 0.888 $\pm$ 0.003 & 0.859 $\pm$ 0.001 & 0.854 $\pm$ 0.002 \\
		& MvDSCN & \textbf{0.835 $\pm$ 0.000} & \textbf{0.931 $\pm$ 0.001} & \textbf{0.909 $\pm$ 0.001} & 0.860 $\pm$ 0.000 \\
		\hline
	\end{tabular}
	\label{table:multiview}
\end{table*}



\subsection{Discussions}
Recent works on multi-view subspace clustering focus on learning a latent representation across views by dictionary learning or matrix factorization \cite{Zhang2017LatentMS,ZhangGeneralized,Zhao2017MultiViewCV}.
As shown in Eq. (\ref{latent}), a latent representation $\bf{C}$ is learned for $\bf{X}$ and then self-representation is conducted on $\bf{C}$.
All views can share the same latent representation \cite{Zhang2017LatentMS,ZhangGeneralized} but view-specific $\bf{D}$.
\begin{equation}\label{latent}
  \mathop {\min }\limits_{\{ {\bf{C}},{\bf{D}},{\bf{Z}}\} } \left\| {{\bf{X}} - {\bf{CD}}} \right\|_F^2 + \left\| {{\bf{C}} - {\bf{CZ}}} \right\|_F^2
\end{equation}
Compared with latent representation based methods, the proposed MvDSCN also learns a hidden representation $\bf{F}$ by auto-encoder $\phi$.
Auto-encoder can be considered as a mapping function which projects the input to a latent space.
MvDSCN has the following two advantages:
1) Compared with the existing shallow models, the hidden representation $\bf{F}$ is more informative by using deep convolutional auto-encoders
whether the input $\bf{X}$ is hand-crafted feature or the raw data.
2) MvDSCN joints feature learning and self-representation together in an end to end manner. Thus, the multi-view relations can guide both affinity matrix learning and feature learning.
Hence, our proposed MvDSCN can learn a good affinity matrix and therefore boost the performance of multi-view subspace clustering.




\begin{figure}
	\centering
	\includegraphics[width=1\linewidth]{rgbd2.pdf}
	\caption{Sample objects from the RGB-D Object Dataset. RGB image (left) and the corresponding depth image using a recursive median filter(right).}
	\label{fig:rgbdexample}
\end{figure}

\section{Experiments}
\label{s4}
In this section, extensive experiments are conducted to verify the effectiveness of the proposed clustering model.


\subsection{Experiment Setup}

\textbf{Datasets.}
We extensively evaluate the multi-view clustering performance of the proposed model on benchmark multi-view datasets.
\begin{itemize}
  \item  {\textbf{Yale}} is a widely used face dataset which contains 165 gray scale images, which are composed of 15 individuals with 11 images per person.
Variations of the data include center light, with glasses, happy, left light, without glasses, normal, right light, sad, sleepy, surprised and wink.
  \item {\textbf{ORL}} contains 10 different images of each of 40 distinct subjects.
For each subject, the images were taken under varying lighting conditions with different facial expressions (open / closed eyes, smiling / not smiling) and facial details (glasses / no glasses).
For the face dataset (Yale and ORL),  we adjust the image size to 48 $\times$ 48 and extract three types of features, i.e., intensity (4,096 dimensions), LBP (3,304 dimensions) and Gabor (6,750 dimensions).
The standard LBP features are then extracted from the 72 $\times$  80 loosely cropped image with a histogram size of 59 over 910 pixels.
The Gabor feature is dominated by four directions $\theta = {0^\circ , 45^\circ , 90^\circ , 135^\circ }$ and extracted at a scale of $\lambda = 4$. It has a resolution of $25 \times 30$ pixels and a loose face cropping.
Note that all descriptors except intensity are scaled to have a unit norm.
  \item {\textbf{Still DB}} consists of 467 images with 6 classes of actions.
sift bow, color sift bow and shape context bow are extracted.
  \item {\textbf{BBCSport}} contains 544 documents from the BBC Sport website of sports news articles, which are related to two viewpoints in five topical areas during 2004-2005. For each sample, there are 3,183 features for the first view and 3,203 features for the second view.
\end{itemize}

Beyond multi-feature subspace clustering, MvDSCN can be easily extended to multi-modal learning by replacing the input with data with different modalities.
We evaluate the proposed deep multi-view subspace clustering methods on real-world RGB-D Object Dataset \cite{Lai2011ALH}.
It contains visual and depth images of 300 physically distinct objects taken from multiple views and the objects are organized into 51 categories arranged by WordNet hypernym-hyponym relationships (similar to ImageNet).
Our experiment datasets is composed of 50 categories randomly selected from RGB-D Object Dataset with each class containing 10 examples.
All visual images and depth images are resized to $64 \times 64$ pixels.
We apply median filter recursively until all missing values are filled to visualize.
A subset of RGB and depth images are shown in Figure \ref{fig:rgbdexample}.

\textbf{Comparison methods.}
The performance of MvDSCN is compared with the state-of-the-art subspace clustering methods in term of four evaluation metrics.
There are six shallow and deep single-view clustering algorithms, and five multi-view clustering algorithms.
For all single-view clustering algorithms, the performance of the best view is reported.
\begin{itemize}
  \item {\textbf{BestSV}} reports the result of the individual view which achieves the best spectral clustering performance with a single view of data \cite{Ng2001OnSC}.
  \item {\textbf{LRR}} seeks the lowest-rank representation among all the candidates that can represent the data samples as linear combinations of the bases in a given dictionary with the best single view \cite{Guangcan2013Robust}.
  \item {\textbf{RMSC}} has a flavor of low-rank and sparse decomposition \cite{Xia2014RobustMS}.
It firstly construct a transition probability matrix from each single view, and then uses these matrices to recover a shared low-rank transition probability matrix as a crucial input to the standard Markov chain method for clustering.
  \item {\textbf{DSCN}} is a deep auto-encoder framework for subspace clustering with a best single view. \cite{Peng2016DeepSC}
  \item {\textbf{DCSC}} imposes a self-paced regularizer on the loss and presents a robust deep subspace clustering algorithm \cite{Jiang2018WhenTL}.
  \item {\textbf{DC}} proposes a scalable clustering approach for the unsupervised learning of convnets.
It iterates between clustering with k-means and updating its weights by predicting the cluster assignments as pseudo-labels in a discriminative loss
\cite{Caron2018DeepCF}.
\end{itemize}

\begin{itemize}
  \item {\textbf{Min-Disagreement}} creates a bipartite graph and is based on the minimizing-disagreement idea \cite{Sa2005SpectralCW}.
  \item {\textbf{Co-Reg SPC}} uses spectral clustering objective functions that implicitly combine graphs from multiple views of the data to achieve a better clustering result \cite{Kumar2011CoregularizedMS}.
  \item {\textbf{DMF}} learns the hierarchical semantics of multi-view data through the semi-nonnegative matrix factorization \cite{Zhao2017MultiViewCV}.
  \item {\textbf{LMSC}} seeks the underlying latent representation and simultaneously performs data reconstruction based on the learned latent representation \cite{Zhang2017LatentMS}.
  \item {\textbf{MSCN}} observes that spatial fusion methods in a deep
multimodal subspace clustering task relay on spatial correspondences among the modalities \cite{Abavisani2018DeepMS}.
\end{itemize}

\begin{table*}
	\centering
	\small
	\caption{Results on four multi-feature datasets (mean $\pm$ standard deviation). Higher value indicates better performance.}
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		Datasets & Views & NMI & ACC & AR & F-measure\\
		\hline\hline
		\multirow{4}*{Yale}
		& View1 & 0.738 $\pm$ 0.006 & 0.727 $\pm$ 0.014 & 0.509 $\pm$ 0.021 & 0.542 $\pm$ 0.019 \\
		& View2  & 0.613 $\pm$ 0.007 & 0.598 $\pm$ 0.008 & 0.401 $\pm$ 0.008 & 0.439 $\pm$ 0.008 \\
		& View3 & 0.545 $\pm$ 0.009 & 0.522 $\pm$ 0.012 & 0.267 $\pm$ 0.011 & 0.311 $\pm$ 0.011 \\
		& ALL  & \textbf{0.797 $\pm$ 0.007} & \textbf{0.824 $\pm$ 0.004} & \textbf{0.626 $\pm$ 0.011} & \textbf{0.650 $\pm$ 0.010} \\
		\hline
		\multirow{4}*{ORL}
		& View1	& 0.883 $\pm$ 0.005 & 0.801 $\pm$ 0.009 & 0.704 $\pm$ 0.012 & 0.711 $\pm$ 0.011 \\
		& View2  & 0.793 $\pm$ 0.011 & 0.627 $\pm$ 0.024 & 0.504 $\pm$ 0.023 & 0.516 $\pm$ 0.023 \\
		& View3 & 0.764 $\pm$ 0.009 & 0.580 $\pm$ 0.024 & 0.458 $\pm$ 0.024 & 0.471 $\pm$ 0.023 \\
		& ALL & \textbf{0.943 $\pm$ 0.002} & \textbf{0.870 $\pm$ 0.006} & \textbf{0.819 $\pm$ 0.001} & \textbf{0.834 $\pm$ 0.012} \\
		\hline
		\multirow{4}*{Still DB}
		& View1	& 0.113 $\pm$ 0.001 & 0.329 $\pm$ 0.004 & 0.083 $\pm$ 0.003 & 0.243 $\pm$ 0.014 \\
		& View2  & 0.216 $\pm$ 0.011 & 0.323 $\pm$ 0.006 & 0.145 $\pm$ 0.002 & 0.293 $\pm$ 0.019 \\
		& View3 & 0.211 $\pm$ 0.002 & 0.313 $\pm$ 0.012 & 0.142 $\pm$ 0.014 & 0.289 $\pm$ 0.007 \\
		& ALL & \textbf{0.245 $\pm$ 0.020} & \textbf{0.377 $\pm$ 0.023} & \textbf{0.169 $\pm$ 0.003} & \textbf{0.320 $\pm$ 0.015} \\
		\hline
		\multirow{3}*{BBCSport}
		& View1	& 0.617 $\pm$ 0.000 & 0.801 $\pm$ 0.000 & 0.847 $\pm$ 0.001 & 0.559 $\pm$ 0.000 \\
		& View2  & 0.652 $\pm$ 0.000 & 0.821 $\pm$ 0.000 & 0.856 $\pm$ 0.001 & 0.683 $\pm$ 0.001 \\
		& ALL & \textbf{0.835 $\pm$ 0.000} & \textbf{0.931 $\pm$ 0.001} & \textbf{0.909 $\pm$ 0.001} & \textbf{0.860 $\pm$ 0.000} \\
		\hline
	\end{tabular}
	\label{versus}
\end{table*}

\textbf{Evaluation Metrics.}
Following the experiment setting in \cite{Zhao2017MultiViewCV,Zhang2017LatentMS},
four popular metrics are used to evaluate the clustering quality, including \textbf{NMI} (Normalized Mutual Information), \textbf{ACC} (Accuracy), \textbf{F-Measure}, and \textbf{AR} (Adjusted Rand Index) which can comprehensively evaluate the performance.

The NMI calculates the normalized measure of similarity between two labels of the same data as follows:
\begin{equation}
N M I=\frac{I(l ; c)}{\max \{H(l), H(c)\}},
\end{equation}
where $I(l ; c)$ denotes the mutual information between $l$ and $c$, and $H$ represents their entropy.
Result of NMI do not change by permutations of clusters (classes), and they are normalized to the range of $[0, 1]$, with 0 meaning no correlation and 1 exhibiting perfect correlation.

The ACC score is caculated as:
\begin{equation}
A C C=\max _{m} \frac{\sum_{i=1}^{n}\left\{l_{i}=m\left(c_{i}\right)\right\}}{n},
\end{equation}
where $l_i$ is the ground-truth label, $c_i$ is the cluster assignment produced by the model.
$m(c_{i})$ is the permutation map function, which maps the
cluster labels into class labels.
$n$ is the number of samples.
The best map can be obtained by the Kuhn-Munkres algorithm.


The F-measure can be interpreted as a weighted average of the precision and recall, where an F-measure reaches its best value at 1 and worst score at 0.
The relative contribution of precision and recall to the F-measure are equal. Its formulation is:
\begin{equation}
F_{measure}=\left(\frac{\operatorname{recall}^{-1}+\operatorname{precision}^{-1}}{2}\right)^{-1}
\end{equation}

The adjusted rand index is the corrected-for-chance version of the rand index \cite{rand1971objective}.

Note that lower values indicate better performance for average entropy, and higher values indicate better performance for the other metrics.
We optimize all the parameters to achieve the best performance of the comparison method.
Especially, we run each method 30 times and report the average performance and standard deviation.
\begin{table*}
\centering
	\caption{Results on RGB-D Object datasets (mean $\pm$ standard deviation). Higher value indicates better performance.}
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		Datasets & Methods & NMI & ACC & AR & F-measure\\
		\hline\hline
		\multirow{12}*{RGB-D Object}
		& BestSV & 0.554 $\pm$ 0.006 & 0.278 $\pm$ 0.001 & 0.106 $\pm$ 0.006 & 0.125 $\pm$ 0.006 \\
		& LRR & 0.589 $\pm$ 0.002 & 0.299 $\pm$ 0.010 & 0.143 $\pm$ 0.002 & 0.156 $\pm$ 0.001 \\
		& Min-Disagreement & 0.605 $\pm$ 0.008 & 0.332 $\pm$ 0.002 & 0.160 $\pm$ 0.013 & 0.177 $\pm$ 0.011 \\
		& Co-Reg & 0.602 $\pm$ 0.007 & 0.268 $\pm$ 0.003 & 0.155 $\pm$ 0.020 & 0.175 $\pm$ 0.018 \\
		& RMSC & 0.603 $\pm$ 0.006 & 0.341 $\pm$ 0.015 & 0.162 $\pm$ 0.010 & 0.178 $\pm$ 0.010 \\
		& DSCN & 0.589 $\pm$ 0.004 & 0.339 $\pm$ 0.006 & 0.163 $\pm$ 0.004 & 0.179 $\pm$ 0.004 \\
		& DCSC & 0.591 $\pm$ 0.002 & 0.340 $\pm$ 0.002 & 0.170 $\pm$ 0.001 & 0.182 $\pm$ 0.003 \\
		& DC & 0.594 $\pm$ 0.003 & 0.340 $\pm$ 0.002 & 0.177 $\pm$ 0.004 & 0.184 $\pm$ 0.004 \\
		& LMSC & 0.593 $\pm$ 0.030 & 0.335 $\pm$ 0.028 & 0.151 $\pm$ 0.035 & 0.167 $\pm$ 0.034 \\
		& DMF & 0.549 $\pm$ 0.004 & 0.286 $\pm$ 0.006 & 0.107 $\pm$ 0.002 & 0.123 $\pm$ 0.001 \\
		& MSCN  & 0.608 $\pm$ 0.001 & 0.354 $\pm$ 0.003 & 0.190 $\pm$ 0.002 & 0.203 $\pm$ 0.004 \\
		& MvDSCN & \textbf{0.639 $\pm$ 0.003} & \textbf{0.388 $\pm$ 0.005} & \textbf{0.210 $\pm$ 0.004} & \textbf{0.225 $\pm$ 0.004} \\
		\hline
	\end{tabular}
	\label{table:multimodal}
\end{table*}

\begin{table*}
	\small
	\centering
	\caption{Results on RGB-D Object dataset (mean $\pm$ standard deviation). Higher value indicates better performance.}
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		Datasets & Views & NMI & ACC & AR & F-measure\\
		\hline\hline
		\multirow{3}*{RGB-D Object}
		& RGB & 0.589 $\pm$ 0.004 & 0.339 $\pm$ 0.006 & 0.163 $\pm$ 0.004 & 0.179 $\pm$ 0.004 \\
		& Depth  & 0.576 $\pm$ 0.004 & 0.300 $\pm$ 0.005 & 0.131 $\pm$ 0.004 & 0.147 $\pm$ 0.004 \\
		& RGB+Depth  & \textbf{0.639 $\pm$ 0.003} & \textbf{0.388 $\pm$ 0.005} & \textbf{0.210 $\pm$ 0.004} & \textbf{0.225 $\pm$ 0.004} \\
		\hline
	\end{tabular}
	\label{table:modelversus}
\end{table*}

\begin{table*}[htbp!]
	\centering
	\caption{Ablation study on RGB-D Object dataset (mean $\pm$ standard deviation). Higher value indicates better performance.}
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		Methods & NMI & ACC & AR & F-measure\\
		\hline\hline
		D-MvDSCN & 0.594 $\pm$ 0.004 & 0.343 $\pm$ 0.007 & 0.190 $\pm$ 0.006 & 0.205 $\pm$ 0.006 \\
		U-MvDSCN & 0.593 $\pm$ 0.005 & 0.350 $\pm$ 0.006 & 0.192 $\pm$ 0.006 & 0.197 $\pm$ 0.006 \\
		MvDSCN  & \textbf{0.639 $\pm$ 0.003} & \textbf{0.388 $\pm$ 0.005} & \textbf{0.210 $\pm$ 0.004} & \textbf{0.225 $\pm$ 0.004} \\
		\hline
	\end{tabular}
	\label{table:ablation}
\end{table*}

\begin{figure*}[!htbp]
	\centering
	\subfigure[$\lambda_1$]{
		\begin{minipage}{0.4\linewidth}
			\centering	\includegraphics[width=1\linewidth]{lambda_1_crop}
	\end{minipage}}
	\subfigure[$\lambda_2$]{
		\begin{minipage}{0.4\linewidth}
			\centering
			\includegraphics[width=1\linewidth]{lambda_2_crop}
		\end{minipage}
	}
	\subfigure[$\lambda_3$]{
		\begin{minipage}{0.4\linewidth}
			\centering
			\vspace{-0.5cm}
			\includegraphics[width=1\linewidth]{lambda_3_crop}
	\end{minipage}}
	\subfigure[$\lambda_4$]{
		\begin{minipage}{0.4\linewidth}
			\centering
			\vspace{-0.5cm}
			\includegraphics[width=1\linewidth]{lambda_4_crop}
	\end{minipage}}
	\caption{The effect of different parameters on MvDSCN learning.}
	\label{fig:sen}
\end{figure*}

\subsection{Results of Multi-feature Subspace Clustering}
The multi-view clustering performance of different methods is given in Table \ref{table:multiview}.
Our proposed method significantly outperforms other methods on Yale, ORL and Still DB datasets, and shows very competitive performance on BBCSport dataset.
For Yale, we raise the performance bar by around 7.9\% in ACC, 4.7\% in AR, 4.9\% in F-measure.
In addition, for ORL, we raise the performance bar by around 4.7\% in ACC, 3.6\% in AR, 6.1\% in F-measure.
On StillDB our method gains significant improvements around 9.1\%, 3.7\%, 4.5\%, 5.5\%, over the second best method in terms of NMI, ACC, AR, F-measure, respectively.
For BBCSport, the performance of the proposed method is better than DMF in terms of three evaluation metrics.
This demonstrates the effectiveness of the proposed MvDSCN on multi-feature subspace clustering task.
The performance improvement owns to two aspects, i.e., the end to end manner in learning the affinity matrix, and multi-view relations embedded into both feature learning and self-representation.
\begin{figure}[!htbp]
	\centering
	\includegraphics[width=0.9\linewidth]{NMI}
	\includegraphics[width=0.9\linewidth]{ACC}
	\caption{Performance comparison between DSCN \cite{ji2017deep} with each view and MvDSCN versus NMI and ACC.}
	\label{fig:svsm}
\end{figure}

\textbf{Single View versus Multiple Views}.
To further investigate the improvement of our method, we compare ours method with deep subspace clustering networks (DSCN) \cite{ji2017deep} with only single-view data.
Figure \ref{fig:svsm} shows the detailed results on different datasets.
According to Table \ref{versus}, the clustering performance with multiple views consistently outperform that of each single view, which empirically proves that clustering with multiple views is more robust than that with single view.








\subsection{Results of Multi-modal Subspace Clustering}
For multi-modal experiments, we use the pre-trained deep auto-encoders to extract features for the comparison shallow methods.
There are $4,096$ features for both RGB image and depth image.
The experimental results on the RGB-D dataset are presented in Table \ref{table:multimodal}.
Our method outperforms all the other competitors.
We raise the performance bar by around $3.4\%$ in NMI, $5.3\%$ in ACC, $4.8\%$ in AR, $5.8\%$ in F-measure compared with the second best method.
Compared with the shallow models that first extract deep features and then conduct subspace clustering, our proposed MvDSCN joints feature learning and subspace clustering together and multi-view relations affect on both parts. Hence, MvDSCN outperforms the state-of-the-art subspace clustering algorithms.

\textbf{Single Modal versus Multiple Modal}.
As shown in Table \ref{table:modelversus}, our methods significantly outperform subspace clustering with only single modal data, which further demonstrates the superiority of multi-modal fusion.
Overall, the RGB modality achieve better performance than Depth modality.
We improve the clustering performance by 5.0\% in NMI, 4.9\% in ACC, 4.7\% in AR and 4.6\% in F-measure when we fuse them by MvDSCN.

\subsection{Convergence Analysis}
To empirically analyze the convergence of MvDSCN, in Figure \ref{fig:converage}, we show the relationship between the loss of the MvDSCN and the clustering performance on the ORL dataset.
The reported values in this figure are normalized between zero and one.
As can be seen from the figure, the loss decreases rapidly in a few epoches.
The clustering performance increases significantly in the first few epoches and then grows slowly.
Similar results can be observed on other datasets.
\begin{figure}[!htbp]
	\centering
	\includegraphics[width=0.9\linewidth]{epochs_4}
	\caption{The loss, and clustering performance (NMI and ACC) of MvDSCN with training epoches.}
	\label{fig:converage}
\end{figure}

\subsection{Parameter Sensitivity}
To analyze the impact the parameters on the clustering performance of MvDSCN, we plot the performance of MvDSCN with different parameters in Figure \ref{fig:sen}.
There are four parameters, i.e., $\lambda_1$, $\lambda_2$, $\lambda_3$, and $\lambda_4$.
$\lambda_1$ seeks the balance between self-representation loss and reconstruction loss of auto-encoders.
$\lambda_2$ reflects the impact of the $l_p$-norm regularization.
$\lambda_3$ controls the degree of the universality regularizer, while $\lambda_4$ controls the degree of the diversity regularizer.
We fix the other three parameters and analyze the impact of the rest parameter.
The results show that the clustering performance grows with the value of $\lambda_1$ and varies little when $\lambda_1$ is above 10.
For $lambda_1$, the best performance is achieved across different datasets when $\lambda_1$ is set as 1.
Similarly, we can observe that when $\lambda_3$ and $\lambda_4$ are set as 0.1, our proposed MvDSCN achieves superior performance.
For all datasets, we fix the values of four parameters as 10, 1, 0.1, 0.1.
\subsection{Ablation Study}
To verify the effectiveness of diversity and universality regularization, we conduct the ablation study with respect to the proposed model.
D-MvDSCN represents the proposed model without diversity regularization while U-MvDSCN refers to the one without universality regularization.
Note that U-MvDSCN can be considered as the case when only the Unet part is kept. As presented in Table \ref{table:ablation}, MvDSCN substantially outperforms D-MvDSCN, which numerically indicates that we cannot ignore diversity regularization that can enhance multi-view complementary information. Besides, our proposed method performs better than U-MvDSCN.
Universality regularization forces the view-specific representation to be centralized to the common representation.
We conduct a sensitivity test for the ragularizer parameter of diversity ($\lambda_3$) and universality ($\lambda_4$) by varying from $0.001$ to $1$.
Figure \ref{fig:ablation} shows the influence of different parameter values with respect to NMI on RGB-D dataset.
Moreover, our method performs much stable when $\lambda_3$ and $\lambda_4$ becomes larger.
In sum, both diversity and universality regularization contribute to the enhancement of the proposed model in terms of multi-view clustering performance.

\begin{figure}[!htbp]
	\centering
	\includegraphics[width=0.8\linewidth]{ablation}
	\caption{Sensitivity test on $\lambda_3$ and $\lambda_4$ versus NMI on RGB-D.}
	\label{fig:ablation}
\end{figure}

\section{Conclusions}
\label{s5}
In this paper, we proposed a new multi-view deep subspace clustering network (MvDSCN).
The proposed method learns multi-view self-representation in an end-to-end manner by combining convolutional auto-encoder and self-representation together.
It consists of diversity net (Dnet) and universality net (Unet), which are connected by diversity and universality regularizer.
Experiments on both multi-feature and multi-modal tasks validate the superiority of our method compared with the state-of-the-arts.

\bibliographystyle{IEEEtran}
\bibliography{egbib}
\end{document}
