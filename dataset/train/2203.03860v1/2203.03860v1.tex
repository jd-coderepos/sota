

\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}              

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{kotex}
\usepackage{url}            \usepackage{amsfonts}       \usepackage{amssymb}
\usepackage{nicefrac}       \usepackage{microtype}      \usepackage{adjustbox}
\usepackage{multirow}
\usepackage[flushleft]{threeparttable}
\usepackage{makecell}
\usepackage{tabulary}
\usepackage{amsmath}
\usepackage{tabularx}
\usepackage{pifont}
\usepackage{kotex}
\usepackage{bbold}
\usepackage{wrapfig}
\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{scalerel}
\usepackage{caption}
\usepackage{tabularx}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}
\newcommand{\xmark}{\text{\ding{55}}}
\newcommand{\cmark}{\text{\ding{51}}}

\newcommand\junsuk[1]{{ \color{red} [\textbf{JS}: #1]}}
\newcommand\sjo[1]{{ \color{blue} [\textbf{SJO}: #1]}}
\def\cvprPaperID{573} \def\confName{CVPR}
\def\confYear{2022}



\begin{document}

\title{Weakly Supervised Semantic Segmentation using Out-of-Distribution Data}

\author{Jungbeom Lee ~ Seong Joon Oh ~ Sangdoo Yun ~ Junsuk Choe ~ Eunji Kim ~  Sungroh Yoon\thanks{Correspondence to: Sungroh Yoon (sryoon@snu.ac.kr).}\\
\small
Department of Electrical and Computer Engineering, Seoul National University\\
\small
NAVER AI Lab ~~~~~~~~~~
University of T\"ubingen\\
\small
Department of Computer Science and Engineering, Sogang University\\
\small
Interdisciplinary Program in AI, AIIS, ASRI, INMC, ISRC and NSI, Seoul National University\\
}
\maketitle


\begin{abstract}
Weakly supervised semantic segmentation (WSSS) methods are often built on pixel-level localization maps obtained from a classifier. However, training on class labels only, classifiers suffer from the spurious correlation between foreground and background cues (e.g. train and rail), fundamentally bounding the performance of WSSS. 
There have been previous endeavors to address this issue with additional supervision.
We propose a novel source of information to distinguish foreground from the background: \textbf{Out-of-Distribution (OoD) data}, or images devoid of foreground object classes. 
In particular, we utilize the \textbf{hard OoDs} that the classifier is likely to make false-positive predictions.
These samples typically carry key visual features on the background (e.g. rail) that the classifiers often confuse as foreground (e.g. train), so these cues let classifiers correctly suppress spurious background cues.
Acquiring such hard OoDs does not require an extensive amount of annotation efforts; it only incurs a few additional image-level labeling costs on top of the original efforts to collect class labels.
We propose a method, \textbf{W-OoD}, for utilizing the hard OoDs. 
W-OoD achieves state-of-the-art performance on Pascal VOC 2012.
The code is available at: \url{https://github.com/naver-ai/w-ood}.


\end{abstract}

\vspace{-1em}
\section{Introduction}
\label{sec:intro}
Pixel-wise labeling is labor-intensive \cite{cordts2016cityscapes}. Lots of research have been dedicated to supervising a semantic segmentation model with weaker forms of supervision than pixel-wise labelings, such as scribbles~\cite{tang2018normalized}, points~\cite{bearman2016s, kim2021beyond}, boxes~\cite{khoreva2017simple,song2019box, lee2021bbam}, and class labels~\cite{wang2020self, lee2021anti, lee2021reducing, lee2018robust}. 
We tackle the final category in this paper: weakly supervised semantic segmentation (WSSS) with class labels. 


WSSS methods utilizing class labels often follow a two-stage process. First, they generate pixel-level pseudo-target from a classifier using CAM variants~\cite{zhou2016learning,selvaraju2017grad}. Then, they train the main segmentation network using the pseudo-target generated in the first stage. 
Built on image-level labels only, the pseudo-target is known to suffer from the confusion between foreground and background cues. For example, given a database of duck images where ducks are typically waterborne, a classifier erroneously assigns higher scores on patches containing water than those containing ducks' feet \cite{choe2020evaluating,li2019guided, zhang2020causal, lee2021reducing, lee2021railroad, kolesnikov2016improving}. The same goes for foreground-background pairs like woodpecker-tree, snowmobile-snow, and train-rail. This is a fundamental problem that cannot be solved solely with the class labels; additional information is needed to learn to fully distinguish the foreground and background cues \cite{choe2020evaluating, li2019guided, lee2021railroad}.


\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{Figure_files/Figure_1_v7.pdf}
\vspace{-2em}
\caption{\label{fig1} (a) Classifiers often confuse background cues to be a foreground concept due to spurious correlations (\eg ``rail'' for ``train''). (b) Our W-OoD employs hard OoD images as negative samples (\eg ``rail'' is not ``train'') to resolve the confusion.}
\vspace{-1.3em}
\end{figure}
 
Researchers have thus sought various sources of additional guidance to separate the foreground and background cues, each with different pros and cons and different labeling-cost footprints. Image saliency~\cite{li2014secrets, liu2010learning} is one of the most widely used ones \cite{lee2019ficklenet, lee2021railroad, yao2021nonsalient, sun2020mining, liu2020leveraging, joon2017exploiting}, for it naturally provides the prominent foreground object in the image in a class-agnostic fashion. However, saliency is not very effective for non-salient foreground objects (\eg low-contrast objects or small objects). Low-level visual features like superpixels~\cite{kwak2017weakly, wang2018weakly}, edges~\cite{ke2021universal}, object proposals~\cite{lee2021bbam, song2019box, liu2020leveraging}, and optical flows~\cite{hong2017weakly, lee2019frame} have also been considered. 
Though cost-effective, they tend to generate inaccurate object boundaries because such low-level information does not consider semantics associated with the class.





In this paper, we propose another source of guidance that provides a distinction between the foreground and background cues. 
We propose to use the \textit{out-of-distribution (OoD)} data that do not contain any of the foreground classes of interest.
Examples include the rail-only images for the foreground class ``train'', since classifiers often confuse the rail for the train. 
By subduing the recognition of ``train'' on such rail cues in hard OoDs, models successfully distinguish such confusing cues.





Obtaining such OoDs does not incur a significant amount of additional annotation efforts compared to collecting only the image-level labels.
The OoD images are natural by-products of the typical dataset collection procedure. Vision datasets with image-level category labels (\eg Pascal \cite{everingham2010pascal}, COCO~\cite{lin2014microsoft}, LVIS \cite{gupta2019lvis}, and OpenImages \cite{kuznetsova2020open}) all start with a pool of candidate images, from which images corresponding to one of the foreground classes are selected and included in the final dataset. The remaining pool, or the \textit{candidate OoD set}, can be utilized as the source of OoD images. 

The candidate OoD set cannot be directly used for guiding the WSSS method for two reasons. First, general OoD images do not provide informative signals to distinguish difficult background cues from the foreground (\eg rail from train). Second, it may still contain foreground objects. 
We address the first problem by selecting \textit{hard OoDs} whereby classifiers falsely assign high prediction scores to one of the foreground classes.
The second problem is addressed by a human-in-the-loop process where images containing foreground objects are manually pruned. While this requires additional human efforts, we emphasize that the extra cost is negligible. As we will show later (Sec.~\ref{analysis_num_ood_image}), we only need a tiny amount of hard OoD samples to improve the localization maps: even 1 hard OoD image per class boosts the localization performance by 2.0\%p. 
Furthermore, the cost for collecting OoD samples is at the same order of magnitude as collecting the category labels for the foreground samples, as opposed to collecting \eg segmentation maps. 
One can also re-direct the budget for collecting a few labeled foreground data to collecting a similar number of hard OoD samples to dramatically improve the WSSS performance.









Given the additional guidance provided by OoD samples, we propose \textbf{W-OoD}, a method of training a classifier by utilizing the hard-OoDs. 
Note that our data collection procedure provides hard OoD samples which have different patterns and semantics in various contexts.
One could ignore this diversity and treat every hard OoD as a combined background class; this approach has proved to be sub-optimal by our experiments.
Instead, W-OoD considers every hard OoD sample with a metric-learning objective: increase the distance between the in-distribution and OoD samples in the feature space.
This forces the background cues shared by the in-distribution and OoD samples (\eg rail for train category) to be excluded from the feature-space representation. 
W-OoD results in high-quality localization maps and lead to the new state-of-the-art performance on the Pascal VOC 2012 benchmark for WSSS.












We contribute (1) a new paradigm of utilizing the OoD samples to address the spurious correlations in weakly supervised semantic segmentation (WSSS); (2) a dataset of hard OoDs for 20 Pascal categories that will be published upon acceptance; and (3) a WSSS method, W-OoD, that exploits the hard OoDs and achieves the best-known performance on the Pascal VOC 2012 benchmark for WSSS.



\section{Related Work}

\textbf{Weakly supervised learning:}
Most weakly supervised learning methods with image-level class labels are based on a class activation map (CAM)~\cite{zhou2016learning}. However, it is widely
known that a CAM is limited to identifying small discriminative parts of a target object~\cite{lee2019ficklenet, ahn2019weakly, lee2021reducing}.
Several techniques have been proposed for obtaining the entire region of the target object.
PSA~\cite{ahn2018learning} and IRN~\cite{ahn2019weakly} consider pixel relationships to extend the object region to semantically similar areas using a random walk.
SEAM~\cite{wang2020self} regularizes the classifier so that the localization maps obtained from differently transformed images are equivariant to those transformations.
AdvCAM~\cite{lee2021anti} and RIB~\cite{lee2021reducing} propose post-processing techniques of a trained classifier to obtain whole regions of the target object, by manipulating images or network weights.
Although the identified regions are successfully extended by these methods, some spuriously correlated background regions tend to be erroneously identified together. 
CDA~\cite{su2021context} adopts the cut-paste method to decouple the correlation between objects and their contextual background.
However, it is difficult to accurately decouple the correlation using only class labels, which limits the performance improvement. 


\vspace{0.2em}
\textbf{Learning with external data:}
Several studies have considered utilizing additional external information to address the issue of the spurious correlation problem.
Automated web searches can provide images~\cite{shen2018bootstrapping, jin2017webly} or
videos~\cite{hong2017weakly, lee2019frame} with class labels, although these labels may be inaccurate.
Some methods~\cite{li2019attention, sun2020mining} utilize single-label images to obtain more information about in-distribution data.
However, these additional sources still depend solely on classes of interest. Thus, they lack information about the separation between the foreground and background.
Consequently, various types of additional supervision have been adopted.
Some researchers~\cite{vilar2021extracting, sawatzky2019harvesting} employed image captions.
However, these are expensive to obtain. Moreover, modeling vision-language relationships, which is required in those methods, is a non-trivial task.
Kolesnikov \textit{et al.}~\cite{kolesnikov2016improving} proposed an active learning approach, wherein a person determines whether a specific pattern is in the foreground or not.
This is a model-specific approach, so human intervention is required whenever a new model is trained.
Saliency supervision~\cite{wang2017learning, cheng2014global} is another popular additional information source~\cite{lee2021reducing, lee2019frame, sun2020mining, wu2021embedded, yao2021nonsalient, lee2021railroad, joon2017exploiting}. 
However, it is not very effective for non-salient objects that are indistinguishable from the background or small objects~\cite{lee2021reducing, wu2021embedded, lee2021railroad}.








\section{Method}

We propose a method for collecting and utilizing OoD data for the WSSS with category labels. 
We describe the data collection procedure for hard OoD in Sec.~\ref{method_data}. In Sec. \ref{method_proposed}, we introduce the method named W-OoD that trains a classifier with the collected hard-OoDs to generate the localization maps. 
Finally in Sec. \ref{method_trainseg}, we show how to train a semantic segmentation network with the localization maps. 


\subsection{Collecting the Hard OoD Data}\label{method_data}


We describe the overall procedure for collecting an OoD dataset. The starting point is a \textit{candidate OoD set} that consists mostly of images without the foreground categories of interest. 
The aim is to refine this set into a set of hard OoDs that will be used for the downstream WSSS methods. 
The overall procedure is described in Fig.~\ref{data_collect}.

\textbf{Where to get the candidate OoDs:}
The WSSS task with category labels as the weak supervision first requires the category labels on a set of training images. 
Building a category-labeled image dataset is typically a four-step process \cite{everingham2010pascal, kuznetsova2020open, lin2014microsoft, gupta2019lvis}: (1) define the list  of foreground classes of interest, (2) acquire unlabelled images from various sources (\eg world wide web), (3) determine for each image whether it contains one of the foreground classes, and (4) tag each image with the foreground category labels.
Steps (3) and (4) are combined in some cases.
A by-product of this procedure is the set of candidate images obtained from step (2) but not selected in step (3). We refer to this set as the \textit{candidate OoD set}. 
For example, for Pascal VOC 2007~\cite{everingham2010pascal}, step (2) has yielded 44,269 candidate images for annotation.
Everingham \textit{et al.}~\cite{everingham2010pascal} report that 9,963 of them were finally selected as foreground data, while the rest were discarded.
We make use of this discarded set that is likely to consist of background images.


\textbf{Hard OoD samples via ranking and pruning:}
Unfortunately, the candidate OoD data are imperfect. OoD data are often too diverse to contain meaningful information. For example, presenting an image of fish in an aquarium as a negative sample of the foreground class ``train'' will not introduce any meaningful supervision for the classifier (See fish in Fig. \ref{data_collect}). It is the \textit{hard OoD samples} that give much information; they are OoD samples confused by a classifier to be containing the foreground object. The rail images \textit{without train} in Fig. \ref{data_collect} are examples of such. They provide informative negative supervision for the classifier to suppress the class score on spurious background cues. 
We thus rank the candidate OoD data according to the prediction scores  for the class  of interest. 
We use the classifier trained on the images with foreground objects and the corresponding labels.
We prune OoD samples with . This returns candidates for the hard OoD data.





\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{Figure_files/Figure_data_collect_v6.pdf}
\vspace{-2em}
\caption{\label{data_collect} \textbf{Collecting hard OoD data}. Starting from the candidate OoD images at the top, we sequentially prune out easy OoDs and then false negatives for each foreground class . The procedure results in the \textbf{hard OoD dataset}.}
\vspace{-1em}
\end{figure}
 
\textbf{Manual pruning of positive samples:}
It is unrealistic to assume that the candidate OoD set will be free of foreground objects. There will be many missing annotations and corner cases. When they are ranked according to the foreground prediction scores, high-ranking images are likely to contain those missing positives. 
We thus need to manually filter out those positive samples.
This manual refinement stage is the cost bottleneck in our pipeline.
The cost depends directly on the \textit{positive rate} , the proportion of positive images among the pruned set obtained by thresholding the prediction score . 
Letting  be the required number of hard OoD images, the human worker needs to check on average  images. If there are some positive images with \eg , then the annotator needs to check  images to eventually obtain  hard OoDs. We denote the resulting dataset as , the \textit{hard OoD set}.


\textbf{Surrogate source of OoD data:}
Theoretically speaking, it would be best to obtain the hard OoD set by replicating the dataset construction procedure for Pascal \cite{everingham2010pascal} to analyze and benchmark our method on Pascal. 
However, this is practically infeasible because one cannot crawl images with similar characteristics as the 500,000 initial images that Pascal authors have crawled from Flickr in 2007 \cite{everingham2010pascal}. It is also not documented which category annotation tool has been used to filter out the background set.
Another way to set up the experiment is to build a new dataset from scratch.
However, this will not allow us to use the existing WSSS benchmarks like Pascal. 
In this paper, we source the candidate OoD data from another vision dataset: OpenImages \cite{kuznetsova2020open}. 
To simulate the OoD data, we filter out 20 Pascal classes from the OpenImages dataset using the provided category labels.
Note that OpenImages category labels are noisy: 19,794 categories are labeled first through image classifiers and then are refined by crowdsourced workers \cite{kuznetsova2020open}.
This is in stark contrast to Pascal: only 20 categories are labeled by a highly controlled pool of workers at a controlled offline event (called ``annotation party'') \cite{everingham2010pascal}. 
We thus expect the candidate OoD set sourced from OpenImages to contain more noise (\ie foreground classes) than the set one would get from the original Pascal data collection process.





































\subsection{Learning with Hard OoD Dataset}\label{method_proposed}
Classifiers trained only on the in-distribution dataset  often incorrectly identify spuriously correlated background regions as class-relevant patterns.
We address this by using the hard out-of-distribution data  obtained in the previous section. 
One naive approach to utilize the hard OoD images is either to assign the uniform distribution over the labels for such images (no-information prior)~\cite{hendrycks2018deep, lee2017training, lee2021removing} or to assign the ``background'' label to such images.
However, since hard OoD images contain various semantics that convey meaningful information to each class, labeling these images with one background class ignores the diversity of OoD samples, resulting in a sub-optimal performance as shown in Sec.~\ref{exp_discuss} and Table~\ref{loss_ablation}.


To benefit from the diversity of hard-OoD images, we propose a metric-learning methodology that considers OoD images of individuals or small groups.
To compute a metric-learning objective, we use the penultimate feature  of the in-distribution classifier  for an input ; we write  (resp. ) as the feature of  (resp. ).
We train a classifier  to ensure that  is significantly different from , thereby preventing information overlap between the features.
To realize this, a clustering-based metric learning objective is proposed. 


 
Let  and  be the sets of  and , respectively.
We first construct a set of clusters  (resp. ) based on  (resp. ). 
Each cluster in  contains features of  corresponding to each class , resulting in  clusters in .
One straightforward way of constructing  is to cluster images according to their incorrectly predicted classes.
This, however, is sub-optimal in practice because such clusters are highly heterogeneous. For example, images of lakes and images of trees are semantically different, yet a cluster based on the ``bird'' class will contain both. 
Therefore, we construct  by using a -means clustering algorithm on .









We now have a set of clusters  and .
The center of each cluster is computed using .
We define the distance between the input image  and each cluster  as the distance between 's feature  and the center , as follows:

We design a loss  to ensure that the distance between  and in-distribution clusters  is small, but the distance between  and OoD clusters  is large, as shown below:

where  is the multi-hot binary vector of foreground classes in image  and  is the set of clusters in 
that are among the top- closest from  . This restriction of  ensures meaningful supervisory signals for the model.






We also use the usual classification loss . For in-distribution samples , we use the binary cross entropy (BCE) losses against the label vector . For out-of-distribution samples , we use the same loss with the zero-vector label .
The classification loss for our classifier  is then

where  is the prediction for class .
The final loss  to train a classifier  is

where  is a scalar balancing the two losses.


Because our method adds an additional regularization  to the existing classifier training, it can be seamlessly integrated into other methods, such as IRN~\cite{ahn2019weakly}, SEAM~\cite{wang2020self} and AdvCAM~\cite{lee2021anti}.




\subsection{Training Segmentation Networks}\label{method_trainseg}
The classifier  trained by Eq.~\ref{total_loss} generates a localization map using the CAM~\cite{zhou2016learning} technique.
Since the naive CAM generates low-resolution score maps and provides only rough localization of objects, recent WSSS methods~\cite{lee2019ficklenet, wang2020self, lee2021reducing, su2021context, lee2021anti, zhang2020causal} have proposed a framework for expanding the CAM score map to full resolution. They consider the CAM localization map as an initial seed and generate pseudo-ground-truth masks by refining their initial seeds with established seed refinement methods~\cite{huang2018weakly, ahn2018learning, ahn2019weakly, kolesnikov2016seed}. 
In this work, we apply the IRN framework~\cite{ahn2019weakly} on our localization maps to obtain the pseudo-ground-truth masks. They are subsequently used for training segmentation networks.


\begin{table}[tbp]
\renewcommand{\arraystretch}{0.92}
  \centering
  \caption{\textbf{W-OoD improves initial seeds.} We evaluate the qualities of various initial seeds and the effects of applygin W-OoD on them. 
Evaluated on Pascal VOC 2012 \textit{train} set. All numbers are based on our re-implementation using the official codes.}
  \vspace{-0.7em}
    \begin{tabular}{l@{\hskip 0.07in}cc@{\hskip 0.12in}c@{\hskip 0.07in}c}
     \Xhline{1pt}\-0.9em]
    ~\cite{ahn2018learning} & 49.5  & 61.9 & 72.7  & 66.9\\
    + W-OoD  & \textbf{53.3}  & \textbf{66.5} & \textbf{73.2} & \textbf{69.7}  \\
    \hline \-0.9em]
    ~\cite{lee2021anti} & 55.5  & 66.8 & 77.6 & 71.8  \\
    + W-OoD  & \textbf{59.1} & \textbf{71.5} & \textbf{77.9} & \textbf{74.6}  \\
\Xhline{1pt}
    \vspace{-1.8em}
    \end{tabular}\label{seed_improve}\end{table} \begin{table}[tbp]
\renewcommand{\arraystretch}{0.95}
  \centering
  \caption{\textbf{Quality of pseudo-GT masks.} mIoU (\%) of the initial seed (Seed), the seed with CRF (+CRF), and the pseudo ground truth mask (Mask) are evaluated on Pascal VOC 2012 \textit{train} set. All the methods based based on IRN~\cite{ahn2019weakly} with ResNet-50.}
  \vspace{-0.7em}
    \begin{tabular}{l@{\hskip 0.3in}c@{\hskip 0.1in}c@{\hskip 0.08in}c}
     \Xhline{1pt}\-0.9em]


~\cite{ahn2019weakly} & 49.5  & 54.3 & 66.3 \\
    ~\cite{liu2020weakly} & 50.2 & - & 66.8 \\
    ~\cite{zhang2020causal} & 48.8 & - & 67.9 \\
    ~\cite{su2021context} & 50.8  & - & 67.7 \\
    ~\cite{lee2021anti} & 55.6  & 62.1 & 69.9 \\
    ~\cite{kweon2021unlocking} & 56.0  & 62.8 & - \\


\-0.9em]
    IRN + W-OoD (Ours)  & 53.3 & 58.4 & 71.1  \\
    AdvCAM + W-OoD (Ours)  & \textbf{59.1} & \textbf{65.5} &\textbf{72.1}  \\

    \Xhline{1pt}
    \vspace{-2em}
    \end{tabular}\label{table_seed}\end{table}% 
\section{Experiments}
\subsection{Experimental Setup}\label{setup_sec}
\textbf{In-Distribution Dataset:} We conduct experiments on the Pascal VOC 2012~\cite{everingham2010pascal} dataset.
Following the practice in weakly-supervised semantic segmentation (WSSS)~\cite{lee2021anti, ahn2019weakly, wang2020self}, we use the augmented training set containing 10,582 training images produced by Hariharan et al.~\cite{hariharan2011semantic}.
For those training images, we only use the image-level category labels, following the protocol for WSSS.
We use the pixel-wise ground-truth masks on \textit{val} (1,449 images) and \textit{test} (1,456 images) sets only for evaluation.
We use the official Pascal VOC evaluation server for the \textit{test}-set evaluation.


\textbf{Out-of-Distribution Dataset:}
As described in Sec.~\ref{method_data}, we use the OpenImages~\cite{kuznetsova2020open} dataset to construct the candidate OoD set. As the result of prediction-score pruning and manual filtering, we obtain the hard OoD set  with 5,190 images.
Examples are shown in the Appendix.





\textbf{Reproducibility:}
We follow experimental settings of IRN~\cite{ahn2019weakly} for training a classifier and obtaining the initial seed, including the use of ResNet-50~\cite{he2016deep}. 
For the setting defined in Sec.~\ref{method_proposed}, we use , , and . 
For training a segmentation network, we use DeepLab-v2~\cite{chen2017deeplab} with two choices of backbones, ResNet-101~\cite{he2016deep} and Wide ResNet-38~\cite{wu2019wider}, following the practice in recent papers.
All the backbones are pre-trained on ImageNet~\cite{deng2009imagenet}, following existing work~\cite{ahn2018learning, wang2020self, zhang2020causal, kweon2021unlocking, li2021pseudo}.





\begin{figure*}[t]
\centering
\includegraphics[width=0.97\linewidth]{Figure_files/cam_samples_v3_noborder.pdf}
\vspace{-1em}
\caption{\label{cam_samples} \textbf{Examples of localization maps.} The localization maps are obtained from CAM (left) and AdvCAM~\cite{lee2021anti} (right). In each case, we show the results using our W-OoD method on top.
}
\vspace{-0.5em}
\end{figure*}
 \begin{figure*}[t]
\centering
\includegraphics[width=0.97\linewidth]{Figure_files/seg_results_v3.pdf}
\vspace{-1.2em}
\caption{\label{segsample} \textbf{Examples of final segmentation results.} Examples of semantic segmentation results on {Pascal VOC 2012} \textit{val} set for IRN~\cite{ahn2019weakly}, AdvCAM~\cite{lee2021anti}, and AdvCAM+Ours. 
}
\vspace{-1em}
\end{figure*}
 

\subsection{Experimental Results}
\begin{table}[t]
\renewcommand{\arraystretch}{0.95}
\centering
  \caption{\textbf{WSSS performance on Pascal.} We show results on Pascal VOC 2012 \textit{val} and \textit{test} sets. WResNet denotes Wide ResNet~\cite{wu2019wider}. Asterisks  denote reproduced numbers by us.}\label{table_semantic}
\vspace{-0.7em}
\begin{threeparttable}
\begin{tabular}{l@{\hskip 0.1in}c@{\hskip 0.1in}cc}
    \Xhline{1pt}\-0.9em]
    
\multicolumn{3}{l}{Supervision: Image-level tags + Saliency}\\
~\cite{lee2019ficklenet}  & ResNet-101 & 64.9 & 65.3\\
~\cite{sun2020mining}   & ResNet-101  & 66.2  & 66.9  \\
~\cite{yao2021nonsalient}   & ResNet-101  & 68.3  & 68.5  \\
    ^2~\cite{zhang2021affinity}   & ResNet-101  &   68.3 & 68.7\\
    ~\cite{xu2021leveraging}    & WResNet-38 & 69.0  & 68.6 \\
~\cite{wu2021embedded}    & ResNet-101 & 70.9  & 70.6 \\
\-0.9em]
     \multicolumn{3}{l}{Supervision: Image-level tags}\\
    ~\cite{ahn2019weakly}  &  ResNet-50 & 63.5 & 64.8 \\
    ~\cite{Shimoda_2019_ICCV}    & WResNet-38   & 64.9  & 65.5\\
    ~\cite{wang2020self}    & WResNet-38 & 64.5  & 65.7 \\


    ~\cite{chang2020weakly}   & ResNet-101  & 66.1  & 65.9\\


    ~\cite{zhang2020causal}   & WResNet-38  & 66.1  & 66.7  \\

    ~\cite{lee2021anti} & ResNet-101 & 67.5 & 67.1  \\
    ~\cite{kweon2021unlocking} & WResNet-38 & 68.3 & 68.0  \\


    ~\cite{li2021pseudo} & WResNet-38 & 68.5 & 69.0  \\
        AdvCAM + W-OoD (Ours) & ResNet-101 &  69.8 &  69.9 \\

    AdvCAM + W-OoD (Ours) & WResNet-38 & \textbf{70.7} & \textbf{70.1}  \\
\Xhline{1pt}
    
    \end{tabular}\end{threeparttable}
\vspace{-1em}

      \end{table} 

\textbf{Quality of localization maps:}
As mentioned in Sec.~\ref{method_proposed}, our method can be applied to other WSSS methods, since it only requires the addition of a loss term  during the classifier training.
We apply our method to three state-of-the-art WSSS methods that utilize the initial seeds: IRN~\cite{ahn2019weakly}, SEAM~\cite{wang2020self}, and AdvCAM~\cite{lee2021anti}.
Table~\ref{seed_improve} presents the qualities of the initial seeds for the considered baselines as well as respective performances when combined with our W-OoD technique. We observe that our method improves all the metrics by a large margin for all three methods. 
In particular, W-OoD training significantly improves precision values (\eg +4.7\%p for AdvCAM~\cite{lee2021anti}), indicating that the resulting localization maps bleed into the background regions less frequently. This is what we expected to see as a result of including the hard OoD samples into training.
Fig.~\ref{cam_samples} shows qualitative examples of the localization maps. 
They show that our method generates more precise maps around the actual foreground objects. Spuriously correlated background regions like rails for ``train'' and trees for ``bird'' are effectively suppressed by our method.
Additionally, we observe that our method improves recall by expanding the retrieved region of the target object, as shown in the last column in Fig.~\ref{cam_samples}. 
The increased precision gives room for further improvements in recall.


\begin{table*}[t]
  \centering
\begin{minipage}{0.66\linewidth}
  \centering
\includegraphics[width=\linewidth]{Figure_files/n_of_images_boxplot_together_v7_illu.pdf}
  \vspace{-2em}
  \captionof{figure}{\label{fig_n_ood} \textbf{Amount of hard OoD samples.} We vary number of in-distribution training data  (originally 10,582) and the hard OoD data (originally 0). (a) We fix  and vary . (b) We fix  and vary . (c) We use  and .
The box plots show the quantiles over five repeated experiments.}
\end{minipage}\hfill
\begin{minipage}{0.3\linewidth}
\center
\small
{
\begin{tabular}{ccc}
\Xhline{1pt}\-0.9em]
    \multicolumn{2}{c}{mIoU} & 49.5  &  50.0     &  52.5    &    50.2   &52.3&  53.3 \\
    \Xhline{1pt}
    \vspace{-2em}
    \end{tabular}\label{loss_ablation}\end{table} 
\noindent\textbf{Loss functions:} 
We conduct ablation studies for each loss in Eq.~\ref{total_loss}. 
Both  and  consist of terms for in-distribution  and out-of-distribution  data. The effectiveness of each loss term as well as the dataset type is presented in Table~\ref{loss_ablation}. (a) is the result of using only  for , which is our baseline. 
The performance boost for (a)(b) and (c)(e) indicates that training the classifier to predict OoD images as background ( on ) is effective, though with only marginal improvements.
The improvement along (b)(d)(f) signifies the importance of , in particular when used on the hard OoD data . We also find that  for  is useful for stabilizing the performance: in (e)(f), the standard deviation decreases from 0.82 to 0.33.





\vspace{-1em}
\subsubsection{Analysis of Results by Class}
\vspace{-0.2em}
Different object classes exhibit different amounts of spurious correlation with background. 
For example, ``train'' objects are often confused with the rail background due to their high co-occurrence with rails. 
Objects like ``tvmonitor'', on the other hand, suffer less from this issue because of the variety of the co-occuring concepts: a TV can be freely put next to a wall, furniture, window, or any other indoor objects.
We show the class-wise performances for the baseline IRN~\cite{ahn2019weakly} and ours in
Fig.~\ref{class_wise}.
First of all, we note that our method improves the class-wise performances rather proportionately: 18 out of 21 classes have seen a performance improvement.
Classes that have benefited most from our method are train, airplane, boat, bird, and horse. 
They are ones that are well-known for spurious background correlations: 
train-rail, airplane-sky/runway, boat-water, bird-tree/sky, and horse-meadow.


On the other hand, a particularly large drop in mIoU is seen for the ``dining table'' class.
We conjecture the spurious background correlation has actually been helping out the localization of the ``dining table'' objects. 
Many pixel-wise ground-truth evaluation mask for ``dining table'' objects erroneously include the objects put on it, such as plates, cutlery, and foods.
By labeling OoD images, which contain those co-occurring objects not put on a dining table, as ``no dining table'', the model may correctly assign lower ``dining table'' scores on those objects, ironically harming the final performance measured on noisy masks.
See Appendix for the examples.
We believe there will be an additional performance gain if those wrong ground-truth masks are fixed.




\vspace{-0.9em}
\subsubsection{Manifold Visualization}
\vspace{-0.4em}
To observe the training dynamics of our method, we visualize the feature manifold at different stages of the W-OoD training. 
We collect two sets of images with respective labels ``train'' and ``bird'' from  and two sets of images which are respectively falsely predicted as ``train'' and ``bird'' by  from .
Using the classifier at epoch \footnote{The classifier at  is the one trained using in-distribution images.}, we compute the features  and  from images drawn from  and , respectively.
We use t-SNE~\cite{maaten2008visualizing} to reduce the dimensionality of each feature to 2 dimensions.
Fig.~\ref{tsne_samples} visualizes the features  and  after dimensional reduction using t-SNE. 
It is observed that, at the beginning of the epoch,  and  of each class are rarely distinguishable, indicating that the classifier encodes similar information for in-distribution and OoD images. However, as W-OoD training progresses, the two features gradually become distinct.
This analysis supports the argument that our method allows the classifier to avoid modeling common information between in-distribution and OoD images, as intended.























\vspace{-0.5em}
\section{Conclusion and Future Directions}
\vspace{-0.3em}
We have proposed the use of a new source of information, the OoD data, for suppressing the spurious correlations learned by weakly supervised semantic segmentation (WSSS) methods. 
We have showcased the data collection pipeline whereby the suitable hard OoD images are obtained.
By including those images as negative samples in addition to the original in-distribution foreground samples, we have been able to train a classifier with more accurate localization maps. 
Our method achieves a performance superior to existing WSSS methods based on image-level labels.
In addition, we have empirically shown that the image-level labeling cost itself can be further reduced by using the hard OoD images, without sacrificing the WSSS performances. 
We have focused on using OoD images for training classifiers to produce accurate pseudo ground-truth masks; interesting future work will include exploiting the OoD images in training a segmentation network itself.





\bigskip
\vspace{-0.5em}
\noindent\textbf{Acknowledgements:}
This work was supported by Institute of Information \& communications Technology Planning \& Evaluation (IITP) grant funded by the Korea government (MSIT) [NO.2021-0-01343, Artificial Intelligence Graduate School Program (Seoul National University)], 
AIRS Company in Hyundai Motor and Kia through HMC/KIA-SNU AI Consortium Fund, and the Brain Korea 21 Plus Project in 2021.


{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}
\setcounter{section}{0}
\renewcommand\thesection{\Alph{section}}
\setcounter{table}{0}
\renewcommand{\thetable}{A\arabic{table}}
\setcounter{figure}{0}
\renewcommand{\thefigure}{A\arabic{figure}}
\clearpage
\section{Appendix}
\subsection{Reproducibility}
Our implementations are based on deeplab-pytorch~\cite{pytorchdeeplab} for the ResNet-101 backbone and MMsegmentation~\cite{mmseg2020} for the Wide ResNet-38 backbone.


\subsection{Additional Analysis}
\textbf{Examples of dining table:} We provide some examples of localization maps for ``dining table'' in Figure~\ref{dining_mistake},  mentioned in Section \textcolor{red}{4.3.3} in the main paper.

\textbf{Clustered OoD samples:} We provide examples of OoD samples clustered by the K-means clustering algorithm in Figure~\ref{ood_samples}.

\textbf{More examples:} Figure~\ref{cam_samples_appendix} presents examples of localization maps obtained by IRN~\cite{ahn2019weakly} and our method, for the PASCAL VOC dataset.
Figure~\ref{seg_samples_appendix} shows examples of segmentation maps predicted by IRN~\cite{ahn2019weakly}, AdvCAM~\cite{lee2021anti}, and our method.


\textbf{Hyper-parameter analysis:} We analyze the sensitivity of the mIoU of the initial seed to  and , hyper-parameters involved in the W-OoD training.
Since  and  are dependent on each other, they must be searched jointly.
As the value of  increases,  increases, so the value of  increases. Therefore,  must decrease accordingly.


\textbf{Segmentation results in mIoU with smaller :}
In the main paper, we provide the quality of the initial seed by using smaller . We here provide the final segmentation results with smaller  in Table~\ref{table_smaller}.

\textbf{Comparison of per-class mIoU scores:}
Table~\ref{class-specific-results} shows the per-class mIoU of the final segmentation obtained by our method and recently produced methods.

\newpage
\begin{table}[htbp]
\caption{Segmentation results with smaller  on Pascal VOC 2012 \textit{val} set.}\label{table_smaller}
\centering
    \begin{tabular}{lcccc}
     \Xhline{1pt}
    { }  &  0  & 20 & 500 & 5190\\ 
    \hline 
    mIoU & 67.5  &   68.1 & 69.0  & 69.8 \\
    \Xhline{1pt}
    \end{tabular}
\end{table}
\begin{table}[htbp]
  \centering
  \caption{Effect of the values of two hyper-parameters  and .}
    \begin{tabular}{ccccc}
    \Xhline{1pt}
       & \multicolumn{4}{c}{} \\
\hline\hline \-0.9em]
    \multicolumn{1}{c}{\multirow{2}[0]{*}{20}} &  0.003     &  0.005       &   0.007    &  0.01\\
    \multicolumn{1}{c}{} &   52.2    &  52.7   &      53.3 & 52.3 \\
        \hline \-0.9em]

    \multicolumn{1}{c}{\multirow{2}[0]{*}{40}} &   0.0003    & 0.0005      &     0.0007  & 0.001 \\
    \multicolumn{1}{c}{} &    50.8   &   50.8    &    51.0   & 50.7 \\
    
    \Xhline{1pt}
    \end{tabular}\label{table_tau_lambda}\end{table} 




\renewcommand{\tabcolsep}{2pt}

\begin{table*}[t]
  \caption{Comparison of per-class mIoU scores for the Pascal VOC 2012 dataset.}
\centering
  \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{lccccccccccccccccccccc|c}
    
        \Xhline{1pt}

        \-0.9em]
\multicolumn{22}{l}{Results on PASCAL VOC 2012 validation images:}\\






PSA~\cite{ahn2018learning}&   88.2    &   68.2    &   30.6    &   81.1    &   49.6   &   61.0    &    77.8  &    66.1   &    75.1   &  29.0     &   66.0    &   40.2    &    80.4 &    62.0   &   70.4    &  73.7 &  42.5   &70.7  & 42.6  &  68.1 &   51.6~ &  61.7\\
    CIAN~\cite{fan2018cian}&   88.2    &   79.5    &   32.6    &   75.7    &   56.8   &   72.1    &    85.3  &    72.9   &    81.7   &  27.6     &   73.3    &   39.8    &    76.4 &    77.0   &   74.9    &  66.8 &  46.6   &    81.0  & 29.1  &  60.4 &   53.3~ &  64.3\\
    SEAM~\cite{wang2020self} &   88.8    &   68.5    &   33.3    &  85.7    &   40.4   &   67.3    &    78.9  &    76.3   &    81.9   &  29.1     &   75.5    &   48.1    &    79.9 &    73.8   &   71.4    &  75.2 &  48.9   &    79.8  & 40.9  &  58.2 &   53.0~ &  64.5\\
    FickleNet~\cite{lee2019ficklenet} &   89.5    &   76.6    &   32.6    &   74.6    &   51.5   &   71.1    &    83.4  &    74.4   &    83.6   &  24.1     &   73.4    &   47.4    &    78.2 &    74.0   &   68.8    &  73.2 &  47.8   &    79.9  & 37.0  &  57.3 &   64.6~ &  64.9\\
    SSDD~\cite{Shimoda_2019_ICCV} &   89.0    &   62.5    &   28.9    &   83.7    &   52.9   &   59.5    &    77.6  &    73.7   &    87.0   &  34.0     &   83.7    &   47.6    &    84.1 &    77.0   &   73.9    &  69.6 &  29.8   &    84.0  & 43.2  &  68.0 &   53.4~ &  64.9\\
    BBAM~\cite{lee2021bbam} &   92.7    &   80.6    &   33.8    &  83.7    &   64.9   &   75.5    &    91.3  &    80.4   &    88.3   &  37.0     &   83.3    &   62.5    &    84.6 &    80.8   &   74.7    &  80.0 &  61.6   &    84.5  & 48.6  &  85.8 &   71.8~ &  73.7\\
    AdvCAM~\cite{lee2021anti} &   89.5    &   76.9    &   33.5    &   80.3    &   63.7   &   68.6    &    89.7  &   77.9   &    87.6   &  31.6     &   77.2    &   36.2    &    82.6 &    78.7   &   73.5    &  69.8 &  51.9   &    81.9  & 43.8  &  70.9 &   52.6  &  67.5\\
    W-OoD (ResNet-101) &   91.2    &   80.1    &   34.0    &   82.5    &   68.5   &   72.9    &    90.3  &   80.8   &    89.3   &  32.3     &   78.9    &   31.1    &    83.6 &    79.2   &   75.4    &  74.4 &  58.0   &    81.9  & 45.2  &  81.3 &   54.8  &  69.8\\
    W-OoD (WideResNet-38) &   91.0    &   80.1    &   34.1    &   88.1    &   64.8   &   68.3    &    87.4  &   84.4   &    89.8   &  30.1     &   87.8    &   34.7    &    87.5 &    85.9   &   79.8    &  75.0 &  56.4   &    84.5  & 47.8  &  80.4 &   46.4  &  70.7\\




\hline\-0.9em]
    \multicolumn{22}{l}{Results on PASCAL VOC 2012 test images:}\\
PSA~\cite{ahn2018learning}&   89.1    &   70.6    &   31.6    &   77.2    &   42.2   &   68.9    &    79.1  &    66.5   &    74.9   &  29.6     &   68.7    &   56.1    &    82.1 &    64.8   &   78.6    &  73.5 &  50.8   & 70.7  & 47.7  &  63.9 &   51.1~ &  63.7\\
    FickleNet~\cite{lee2019ficklenet} &   90.3    &   77.0    &   35.2    &   76.0    &   54.2   &   64.3    &    76.6  &    76.1   &    80.2   &  25.7     &   68.6    &   50.2    &    74.6&    71.8   &   78.3    &  69.5 &  53.8   &    76.5  & 41.8  &  70.0 &   54.2~ &  65.3\\
    SSDD~\cite{Shimoda_2019_ICCV} &   89.0    &   62.5    &   28.9    &   83.7    &   52.9   &   59.5    &    77.6  &    73.7   &    87.0   &  34.0     &   83.7    &   47.6    &    84.1 &    77.0   &   73.9    &  69.6 &  29.8   &    84.0  & 43.2  &  68.0 &   53.4~ &  64.9\\
    BBAM~\cite{lee2021bbam} &   92.8    &   83.5    &   33.4    &   88.9    &   61.8   &   72.8    &    90.3  &    83.5   &    87.6   &  34.7     &   82.9    &   66.1    &    83.9 &    81.1   &   78.3    &  77.4 &  55.2   &    86.7  & 58.5  &  81.5 &   66.4  &  73.7\\
AdvCAM~\cite{lee2021anti} &   89.3    &   79.3    &   32.5    &   80.2    &   56.3   &   62.8    &    87.2  &   80.8   &    87.0   &  28.9     &   78.3    &   41.3    &    82.1 &    80.6  &   77.7    &  68.5 &  51.2   &    80.8  & 55.3  &  60.8 &   48.1  &  67.1\\
    W-OoD (ResNet-101) &   91.4    &   85.3    &   32.8    &   79.8    &   59.0   &   68.4    &    88.1  &   82.2   &    88.3   &  27.4     &   76.7    &   38.7    &    84.3 &    81.1   &   80.3    &  72.8 &  57.8   &    82.4  & 59.5  &  79.5 &   52.6  &  69.9\\
    W-OoD (WideResNet-38) &   90.9    &   83.1    &   35.6    &   89.0    &   61.5   &   63.0    &    86.2  &   80.8   &    89.9   &  29.6     &   79.6    &   40.1    &    82.1 &    81.0   &   82.6    &  74.0 &  60.1   &    85.3  & 58.0  &  71.9 &   47.0  &  70.1\\
\Xhline{1pt}
\end{tabular}\end{adjustbox}









\label{class-specific-results}\end{table*} \begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{Figures/appendix_dining_tables_v2.pdf}
\vspace{-2em}
\caption{\label{dining_mistake} Example where the objects on the dining table are not identified as foreground by our method.}
\vspace{-1.em}
\end{figure*}
 \begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{Figure_files/appendix_ood_samples.pdf}
\vspace{-2em}
\caption{\label{ood_samples} Examples of OoD samples for each cluster, obtained by the K-means clustering algorithm..}
\vspace{-1.em}
\end{figure*}
 \begin{figure*}[t]
\centering
\includegraphics[width=1\linewidth]{Figure_files/appendix_cam_samples.pdf}
\vspace{-1em}
\caption{\label{cam_samples_appendix} Examples of localization maps obtained from CAM and CAM+W-OoD (upper), and AdvCAM~\cite{lee2021anti} and AdvCAM+W-OoD (lower).
}
\vspace{-1em}
\end{figure*}
 \begin{figure*}[t]
\centering
\includegraphics[width=1\linewidth]{Figure_files/appendix_seg_results.pdf}
\vspace{-1em}
\caption{\label{seg_samples_appendix} Examples of segmentation masks obtained by IRN~\cite{ahn2019weakly}, AdvCAM~\cite{lee2021anti}, and our method.
}
\vspace{-1em}
\end{figure*}
 
\end{document}
