\documentclass{bmvc2k}



\title{Part-based Graph Convolutional Network for Action Recognition}

\addauthor{Kalpit Thakkar}{kalpit.thakkar@research.iiit.ac.in}{1}
\addauthor{P J Narayanan}{pjn@iiit.ac.in}{1}


\addinstitution{
Center for Visual Information Technology (CVIT),
Kohli Center for Intelligent Systems (KCIS),\\
IIIT Hyderabad,
India
}


\runninghead{Kalpit Thakkar, P J Narayanan}{Part-based GCN For Action Recognition}

\def\eg{\emph{e.g}\bmvaOneDot}
\def\Eg{\emph{E.g}\bmvaOneDot}
\def\etal{\emph{et al}\bmvaOneDot}

\setlength{\belowcaptionskip}{-0.5cm}
\setlength{\parskip}{0cm}
\usepackage[compact]{titlesec} 

\begin{document}

\maketitle

\begin{abstract}
Human actions comprise of joint motion of articulated body parts or ``gestures''. Human skeleton is intuitively represented as a sparse graph with joints as nodes and natural connections between them as edges. Graph convolutional networks have been used to recognize actions from skeletal videos. We introduce a part-based graph convolutional network (PB-GCN) for this task, inspired by Deformable Part-based Models (DPMs). We divide the skeleton graph into four subgraphs with joints shared across them and learn a recognition model using a part-based graph convolutional network. We show that such a model improves performance of recognition, compared to a model using entire skeleton graph. Instead of using 3D joint coordinates as node features, we show that using relative coordinates and temporal displacements boosts performance. Our model achieves state-of-the-art performance on two challenging benchmark datasets NTURGB+D and HDM05, for skeletal action recognition.
\end{abstract}


\section{Introduction}
\label{sec:intro}
Recognizing human actions in videos is necessary for understanding them. Video modalities such as RGB, depth and skeleton provide different types of information for understanding human actions. The S-video (or Skeletal
modality) provides 3D joint locations, which is a relatively high level information compared to RGB or depth. With the release of several multi-modal datasets \cite{Shahroudy_2016_CVPR,liu2017pku,7350781}, action recognition from S-video has gained significant traction recently \cite{liu2016spatio,song2017end,liu2017global,zhang2017geometric,ke2017new}.

Graph convolutions \cite{niepert2016learning,defferrard2016convolutional,kipf2016semi} have been used to learn high level features from arbitrary graph structure. State-of-the-art action recognition from S-videos \cite{yan2018spatial,li2018spatio} use graph convolutions, wherein the whole skeleton is treated as a single graph. It is, however, natural to think of human skeleton as a combination of multiple body parts. A body-part based representation can learn the importance of each part and their relations across space and time. We present a model using part-based graph convolutional network for recognizing actions from S-videos, using a novel part-based graph convolution scheme. The model attains better performance for recognition than a model entire skeleton as a single graph. Current models for skeletal action recognition \cite{yan2018spatial,li2018spatio} use 3D coordinates as features at each vertex. Geometric features such as relative joint coordinates and motion features such as temporal displacements can be more informative for action recognition. Optical flow helps in action recognition from RGB videos \cite{wang2016temporal} and Manhattan line map helps in generating 3D layout from single image \cite{zou2018layoutnet}. Geometric feature \cite{zhang2017geometric} and kinematic features \cite{zanfir2013moving} have been used for skeletal action recognition before. Inspired by these observations, we use a geometric feature that encodes relative joint coordinates and motion feature that encodes temporal displacements at each vertex in our part-based graph convolution model to significant impact.

The major contributions of this paper are: (i) Formulation of a general part-based graph convolutional network (PB-GCN) which can be learned for any graph with well-known properties and its application to recognize actions from S-videos, (ii) Use of geometric and motion features in place of 3D joint locations at each vertex to boost recognition performance, and (iii) Exceeding the state-of-the-art on challenging benchmark datasets NTURGB+D and HDM05. The overview of our representation and signals is shown in Figure \ref{fig:overview}.
\begin{figure}[t]
\begin{center}
    \begin{tabular}{cc}
        \begin{tabular}{cc}
            \includegraphics[width=0.15\textwidth,height=0.25\textwidth]{images/camera_ready/rel_coord.pdf} &
            \includegraphics[width=0.15\textwidth,height=0.25\textwidth]{images/camera_ready/disps.pdf} \\
            \footnotesize{Relative coordinates} & \footnotesize{Temporal displacements} \\
        \end{tabular}
        &
        \begin{tabular}{c}
            \includegraphics[width=0.15\textwidth,height=0.25\textwidth]{images/camera_ready/axial_appendicular.pdf} \\
            \footnotesize{Axial (Green) \& Appendicular (Red) skeletons}
        \end{tabular} \\
        \small{(a) Geometric \& Kinematic Features} & \small{(b) Two parts} \\
        \begin{tabular}{cc}
            \includegraphics[width=0.10\textwidth]{images/camera_ready/upper_appendicular.pdf} &
            \includegraphics[width=0.10\textwidth]{images/camera_ready/upper_axial.pdf} \\
            \footnotesize{Upper} & \footnotesize{Upper} \\
            \footnotesize{Appendicular} & \footnotesize{Axial} \\
            \includegraphics[width=0.10\textwidth]{images/camera_ready/lower_appendicular.pdf} &
            \includegraphics[width=0.10\textwidth]{images/camera_ready/lower_axial.pdf} \\
            \footnotesize{Lower} & \footnotesize{Lower} \\
            \footnotesize{Appendicular} & \footnotesize{Axial} \\
        \end{tabular}
        &
        \begin{tabular}{ccc}
            \includegraphics[width=0.05\textwidth]{images/camera_ready/upper_left_appendicular.pdf} & 
            \includegraphics[width=0.10\textwidth]{images/camera_ready/upper_axial.pdf} &
            \includegraphics[width=0.05\textwidth]{images/camera_ready/upper_right_appendicular.pdf} \\
            \footnotesize{Upper Left} & \footnotesize{Upper} & \footnotesize{Upper Right} \\
            \footnotesize{Appendicular} & \footnotesize{Axial} & \footnotesize{Appendicular} \\
            \includegraphics[width=0.05\textwidth]{images/camera_ready/lower_left_appendicular.pdf} &
            \includegraphics[width=0.10\textwidth]{images/camera_ready/lower_axial.pdf} &
            \includegraphics[width=0.03\textwidth]{images/camera_ready/lower_right_appendicular.pdf} \\
            \footnotesize{Lower Left} & \footnotesize{Lower} & \footnotesize{Lower Right} \\
            \footnotesize{Appendicular} & \footnotesize{Axial} & \footnotesize{Appendicular} \\
        \end{tabular} \\
        \small{(c) Four parts} & \small{(d) Six parts}
    \end{tabular}
\end{center}
\caption{\small{\textbf{(a)} Geometric and Kinematic features, \textbf{(b)} Appendicular and axial body parts: two parts, \textbf{(c)} Dividing the appendicular and axial skeletons into upper and lower parts: four parts, \textbf{(d)} Dividing appendicular upper and lower skeletons into left and right: six parts.}}
\label{fig:overview}
\end{figure} 

\section{Related Work}
\label{sec:relatedwork}
\subsection{Non graph-based methods}
\label{sec:2_1}
Skeletal action recognition has been approached using techniques such as handcrafted feature encodings, complex LSTM networks, image encodings with pretrained CNNs and non-euclidean methods based on manifolds. Non-deep learning methods worked well initially and proved usefulness of several extracted information from S-videos such as joint angles \cite{ofli2014sequence}, distances \cite{xia2012view} and kinematic features \cite{zanfir2013moving}. These methods learn from hand designed features using shallow models which do not model spatio-temporal properties of actions very well and constrain learning capacity.
\begin{figure}[t]
    \begin{center}
        \begin{tabular}{@{}ccc@{}}
            \includegraphics[width=0.3\textwidth]{images/camera_ready/gconv_01.pdf} & 
            \includegraphics[width=0.3\textwidth]{images/camera_ready/gconv_02.pdf} &
            \includegraphics[width=0.3\textwidth]{images/camera_ready/gconv_03.pdf} \\
            \footnotesize{(a) Graph feature} & \footnotesize{(b) Convolution for receptive field of} & \multirow{2}{*}{\footnotesize{(c) Final convolution equation}} \\
            \footnotesize{and adjacency matrices} & \footnotesize{a chosen root vertex } & 
        \end{tabular}
    \end{center}
    \caption{\small{Equation-based formulation and illustration of a graph convolution}}
    \label{fig:gconv}
\end{figure}

On the other hand, LSTM-based methods were used because S-videos can be thought of as time sequences of features. Spatio-temporal LSTMs \cite{liu2016spatio, liu2017global}, attention-based LSTM \cite{song2017end} and simple LSTM networks with part-based skeleton representation \cite{tao2015moving, 7298714} have been used. These methods either use complex LSTM models which have to be trained very carefully or use part-based representation with a simple LSTM model. We propose a part-based graph convolutional network that has good learning capacity and uses a part-based representation, inheriting the good qualities of both types of aforementioned approaches. Image encodings of skeletons were proposed to facilitate usage of Imagenet pretrained CNNs to extract spatio-temporal features. Ke \etal \cite{ke2017new} generate images using relative coordinates while Du \etal \cite{7486569} and Li \etal \cite{li20183d} proposed a body part-based image encoding. Due to inherent differences in information in such image encodings and RGB images, it is almost impossible to interpret the learned filters. In contrast, our method is intuitive as it uses a graph-based representation for human skeleton.

Manifold learning techniques have been used for skeletal action recognition, where actions are represented as curves on Lie groups \cite{vemulapalli2014human} and Riemannian manifold \cite{devanne20153}. Deep learning on these manifolds is difficult \cite{huang2017deep} while deep learning on graphs (also a manifold) has developed recently \cite{defferrard2016convolutional, kipf2016semi}. Our method uses a human skeleton graph and learns a model using part-based graph convolutional network, exploiting the benefits of deep learning on graphs.

\subsection{Graph-based methods}
\label{sec:2_2}
Representing S-videos as skeleton graph sequences for recognizing actions had not been explored until recently. Li and Leung \cite{li2017graph} construct graphs using a statistical variance measure dependent on joint distances and match them for recognition. Recently, Yan \etal \cite{yan2018spatial} and Li \etal \cite{li2018spatio} proposed a spatio-temporal graph convolutional network for action recognition from S-videos. Both the methods construct graphs where the human skeleton is treated as a single graph. Our formulation explores a partitioned skeleton graph with a part-based graph convolutional network and we show that it improves recognition performance. Also, we use relative coordinates and temporal displacements as features at each vertex instead of 3D joint coordinates (see Figure \ref{fig:overview}(a)) which improves action recognition performance.

\section{Background}
\label{sec:background}
A graph is defined as  where  is the set of vertices and  is the set of edges.  is the graph adjacency matrix having  if  and  otherwise.  defines the set of vertices  in -neighborhood of  which includes neighbors having shortest path length atmost  from vertex . A labeling function  assigns a label to each vertex in a vertex set , where  is the number of unique labels. The adjacency matrix is normalized using a degree matrix as:
\begingroup
\small

\endgroup
Graph convolutions can be formulated using spectral graph theory \cite{defferrard2016convolutional} or spatial convolution \cite{niepert2016learning} on graphs. We focus on spatial convolutions in this paper as they resemble convolutions on regular grid graphs like RGB images \cite{niepert2016learning}. A graph CNN can then be formed by stacking multiple graph convolution units. Graph convolution (shown in Figure \ref{fig:gconv}) can be defined as \cite{niepert2016learning}:
\begingroup
\small

\endgroup
where,  is the root vertex at which the convolution is centered (like center pixel in an image convolution),  is a filter weight vector of size of  indexed by the label assigned to neighbor  in the -neighborhood ,  is the input feature at  and  is the convolved output feature at root vertex . Equation \ref{eq:npconv} can be written in terms of adjacency matrix as:
\begingroup
\small

\endgroup
 basically defines the neighbors at distance  and hence, Equation \ref{eq:npconv} captures a more general form of convolution by using -order neighborhood .

\begin{figure}[t]
\begin{center}
    \begin{tabular}{@{}c@{}c@{}c@{}}
        \begin{tabular}{cc}
            \includegraphics[width=0.12\textwidth]{images/camera_ready/sneigh.pdf} &
            \includegraphics[width=0.12\textwidth]{images/camera_ready/tneigh.pdf} \\
            \footnotesize{Spatial neighbours} & \footnotesize{Temporal neighbours}
        \end{tabular} &
        \begin{tabular}{c}
            \includegraphics[width=0.15\textwidth]{images/camera_ready/neigh_st.pdf} \\
            \footnotesize{Spatio-temporal neighbours}
        \end{tabular} &
        \begin{tabular}{c}
            \includegraphics[width=0.15\textwidth]{images/camera_ready/headntorso.pdf} \\
            \footnotesize{Single spatial convolution}
        \end{tabular} \\
        \small{(a)} & \small{(b)} & \small{(c)} \\
        \begin{tabular}{c}
            \includegraphics[width=0.15\textwidth]{images/camera_ready/sconv.pdf} \\
            \footnotesize{Multiple spatial convolutions} \\
            \footnotesize{across time}
        \end{tabular} &
        \begin{tabular}{c}
            \includegraphics[width=0.15\textwidth]{images/camera_ready/fagg.pdf} \\
            \footnotesize{Combination of Head and} \\
            \footnotesize{Torso parts: }
        \end{tabular} &
        \begin{tabular}{c}
            \includegraphics[width=0.15\textwidth]{images/camera_ready/tconv.pdf} \\
            \footnotesize{Temporal convolution after} \\
            \footnotesize{ is applied}
        \end{tabular} \\
        \small{(d)} & \small{(e)} & \small{(f)}
    \end{tabular}
\end{center}
\caption{\small{Spatio-temporal neighborhood for root node (in green) and depiction of convolutions in space and time dimensions. Effect of application of  is shown, where the common vertices are in darker shade.}}
\label{fig:pstgcn}
\end{figure}

\subsection{Part-based Graph}
\label{sec:3_1}
Graphs representing real world manifolds can often be thought of as being made up of several parts. For instance, a graph representing a complex molecule consists of several simple structures, such as structure of a protein biomolecule, which can be divided into several polypeptide chains that make up the complex. Similarly, human body can be visualized as connected rigid parts, much like a deformable part-based model \cite{Felzenszwalb:2005}. The graph of the skeleton of human body can be divided into parts, where each subgraph represents a part of the human body.

In general, a part-based graph can be constructed as a combination of subgraphs where each subgraph has certain properties that define it. Let us consider that a graph  has been divided into  partitions. Formally:
\begingroup
\small

\endgroup
 is the partition (or subgraph)  of the graph . We consider scenarios in which the partitions can share vertices or have edges connecting them. We proceed to explain how the part-based graph convolution is defined for the part-based graph.

\subsection{Part-based Graph Convolutions}
\label{sec:3_2}
In essence, graph convolutions over parts are aimed at capturing high-level properties of parts and learn the relations between them. In a Deformable Part-based Model, different parts are identified and relations between them are learned through the deformation of the connections between them. Similarly, graph convolutions over a part identifies the properties of that subgraph and an aggregation across subgraphs learns the relations between them. For a part-based graph, convolutions for each part are performed separately and the results are combined using an aggregation function . Using  over edges across partitions:
\begingroup
\small

\endgroup
Using  for common vertices across partitions:
\begingroup
\small

\endgroup
The convolution parameters  can be shared across parts or kept separate, while the neighbors of  only in that part  are considered. In order to combine the information across parts, the function  combines information at shared vertices (equation \ref{eq:pvagg}) or shares information through edges crossing parts (equation \ref{eq:peagg},  contains all edges connecting parts p1 and p2), according to the partition configuration. A sophisticated  can be employed to make the model powerful. Using graph convolutions, part-based graph models can learn rich representations and we demonstrate the strength of this model through application to action recognition from S-videos.

\section{Spatio-temporal Part-based Graph Convolutions}
\label{sec:stmodel}
The S-videos are represented as spatio-temporal graphs. In order to include the temporal dimension, corresponding joints in each part are connected temporally. Figure \ref{fig:pstgcn}(b) shows the spatio-temporal graph for \textit{torso} over \textit{five} frames. Adapting select-assemble-normalize (\textsc{patchy-san}) proposed by Niepert \etal \cite{niepert2016learning} we present an overview of convolution formulation for our spatio-temporal graph by extending ideas from section \ref{sec:3_2}. For in-depth understanding, we refer the reader to \cite{niepert2016learning}. We perform a spatial convolution on each partition following equation \ref{eq:pconv}, combine the convolved partitions using  and perform temporal convolution on the graph obtained by aggregating the partitions. In effect, we spatially convolve each partition independently for each frame, aggregate them at each frame and perform temporal convolution on the temporal dimension of the aggregated graph. For a possible partitioning of human skeleton, this phenomenon is shown in Figure \ref{fig:pstgcn}(c) for spatial convolution for a vertex common to torso and head, \ref{fig:pstgcn}(d) for spatial convolutions in different frames, \ref{fig:pstgcn}(e) for applying  on head + torso and \ref{fig:pstgcn}(f) for convolution on temporal dimension of the combined graph.

We first define the spatial and temporal neighborhood of a vertex in spatio-temporal graph and assign labels to the vertices in the neighborhoods, which is required to perform convolutions. For each vertex, we use 1-neighborhood  for spatial dimension  as the skeleton graph is not very large and a -neighborhood  for the temporal dimension . Figure \ref{fig:pstgcn}(a) (dashed polygons) shows the spatial \& temporal neighborhood for a \textbf{root} vertex. The different neighborhood sets for our model are defined as ( = length of shortest path between  and ):
\begingroup
\small

\endgroup
where,  represent two time instants and  is the partition index. The set of vertices  differs for each part, with some vertices shared between parts (Figure \ref{fig:overview}(c)). As temporal convolution is performed on the aggregated spatio-temporal graph,  is not part-specific. Figure \ref{fig:pstgcn}(a) shows the spatial and temporal neighborhoods for a \textbf{root} vertex in \textit{torso}. For ordering vertices in the receptive fields (or neighborhoods), we use a single label spatially  to weigh vertices in  of each vertex equally and  labels temporally  to weigh vertices across frames in  differently. The labeling functions are defined as:
\begingroup
\small

\endgroup
Using the labeled spatial and temporal receptive fields, we define the spatial and temporal convolutions as (adapted from \cite{kipf2016semi}):
\begingroup
\small

\endgroup
where,  is a normalized adjacency matrix as explained in section \ref{sec:background} for part .  for each part is same but  is part-specific.  is a part-specific channel transform kernel (pointwise operation) and  is the temporal convolution kernel.  is the output from applying  on input features  at each vertex.  is the output obtained after aggregating all partition graphs at one frame and  is the output after applying temporal convolution on  output of  frames. We use a weighted sum fusion as our :
\begingroup
\small

\endgroup

Human skeleton can be divided into two major components: (1) Axial skeleton and (2) Appendicular skeleton. The body parts included in these two components are shown in Figure \ref{fig:overview}(b). Human skeleton can be divided into parts based on these components. Different division schemes are shown in Figure \ref{fig:overview}(b), \ref{fig:overview}(c) and \ref{fig:overview}(d) and we use these schemes for experiments to test our PB-GCN.

For the final representation, we divide the human skeleton into \textit{four} parts: \textbf{head}, \textbf{hands}, \textbf{torso} and \textbf{legs}, which corresponds to a division scheme where each of the axial and appendicular skeleton are divided into upper and lower components, as illustrated in Figure \ref{fig:overview}(c). We consider left and right parts of hands and legs together in order to be agnostic to \textit{laterality} \cite{lateral} (handedness / footedness) of the human when performing an action. To show how being agnostic to laterality is helpful, we divide the upper and lower components of appendicular skeleton into left and right (shown in Figure \ref{fig:overview}(d)), resulting in six parts and show results on it. To cover all natural connections between joints in skeleton graph, we include an overlap of atleast one joint between two adjacent parts. For example, in Figure \ref{fig:overview}(c), shoulder joints are common between the head and hands. For the lower appendicular skeleton (viz. legs), we also include the joint at the base of spine to get a good overlap with lower axial skeleton.

\paragraph{Architecture and Implementation} We represent each subgraph by its adjacency matrix, normalized by corresponding degree matrix . Our model takes as input a tensor having features for each vertex in the spatio-temporal graph of S-video and outputs a vector of class scores for the video. The architecture of the graph convolutional network is similar to Yan \etal \cite{yan2018spatial} and consists of  spatio-temporal graph convolution units (each unit with the \textit{four}  kernels, \textit{one}  kernel and a residual) with an initial spatio-temporal head unit, based on a Resnet-like model \cite{he2016deep}. First three layers have 64 output channels, next three have 128 and last three have 256. We also use a learnable edge weight mask for learning edge weights in each subgraph \cite{yan2018spatial}. We use the Pytorch framework \cite{paszke2017automatic} for our implementation. The code and models are made publicly available: \href{https://github.com/dracarys983/pb-gcn}{\texttt{https://github.com/dracarys983/pb-gcn}}.

\section{Geometric \& Kinematic Signals}
\label{sec:signals}
Yan \etal \cite{yan2018spatial} use the 3D coordinates of each joint directly as the signal at each graph node. Relative coordinates \cite{zhang2017geometric, ke2017new} and temporal displacements \cite{zanfir2013moving} of joints have been used earlier for action recognition. Derived information like optical flow and Manhattan line map has been found useful on RGB images also \cite{wang2016temporal,zou2018layoutnet}. Even a CNN framework can be more effective and efficient if relevant derived information is supplied as input to the network.

We use a signal at each node that combines temporal displacements across time and relative coordinates, with respect to shoulders and hips \cite{ke2017new}.  This representation provides translation invariance to the representation \cite{verma2018feastnet} and improves skeletal action recognition performance significantly. Figure \ref{fig:overview}(a) illustrates the computation of the two signals for a single skeleton video frame. We show the effect of relative joint coordinates (geometric signal) and temporal displacements (kinematic signal) individually and the performance improvement obtained by using a combination of these signals for a baseline one-part model as well as our four part-based model in the Table \ref{tab:ablation}(b). The improvement in performance obtained using the geometric and kinematic signals is noteworthy.

\section{Experimental Setup and Results}
\label{sec:exp_res}
We use SGD as the optimizer and run the training for 80 epochs (NTURGB+D) / 120 epochs (HDM05). We set the initial learning rate to 0.1 and all the experiments are run on a cluster with  Nvidia GTX 1080Ti GPUs. The batch size is set to 64. Learning rate decay schedule (set to decay by 0.1 at epochs 20, 50 and 70 for NTURGB+D, and at epoch 80 for HDM05) is finalized using a validation set. No augmentation is performed for any of the experiments, consistent with graph-based method \cite{yan2018spatial}. We perform ablation studies on the large-scale NTURGB+D dataset (shown in Table \ref{tab:ablation}) and then compare with state-of-the-art on both HDM05 and NTURGB+D using the best configuration of our model (shown in Table \ref{tab:sota}).

\begin{table}[t]
    \begin{center}
        \small
        \begin{tabular}{cc}
            \multirow{2}{*}{(a) Performance with number of parts} & (b) Performance with various signals for \\
            & best \& worst number of parts \\
            \begin{tabular}{ccc}
            \toprule
            \multirow{2}{*}{\textbf{\#Parts}} & \multicolumn{2}{c}{\textbf{Accuracy}} \\
            & CS & CV \\
            \midrule
            One & 79.4 & 87.9 \\
            Two & 80.2 & 88.4 \\
            Four & \textbf{82.8} & \textbf{90.3} \\
            Six & 81.4 & 89.1 \\
            \bottomrule
            \end{tabular}
            &
            \begin{tabular}{ccccc}
                \toprule
                \multirow{3}{*}{\textbf{Signals}} & \multicolumn{4}{c}{\textbf{Accuracy}} \\
                & \multicolumn{2}{c}{\textbf{\#Parts=1}} & \multicolumn{2}{c}{\textbf{\#Parts=4}} \\
                & CS & CV & CS & CV \\
                \midrule
                 & 79.4 & 87.9 & 82.8 & 90.3 \\
                 & 83.6 & 87.7 & 84.6 & 88.4 \\
                 & 84.3 & 91.6 & 85.4 & 92.6 \\
                 & \textbf{85.6} & \textbf{91.8} & \textbf{87.5} & \textbf{93.2} \\
                \bottomrule
            \end{tabular}
        \end{tabular}
    \end{center}
    \caption{\small{Performance comparison for different number of parts in the skeleton graph and signals at vertices using our PB-GCN, on NTURGB+D \cite{Shahroudy_2016_CVPR} (CS: Cross Subject, CV: Cross View). The symbols for signals, : Absolute 3D joint locations, : Relative coordinates, : Temporal displacements and : Concatenation of  and .}}
    \label{tab:ablation}
\end{table}

\subsection{Datasets}
\label{sec:5_1}
\paragraph{NTURGB+D}\cite{Shahroudy_2016_CVPR}
This is currently the largest RGBD dataset for action recognition to the best of our knowledge. It has 56,880 video sequences shot with three Microsoft Kinect v2 cameras from different viewing angles. There are 60 classes among the action sequences and 3D coordinates of 25 joints are provided for each human skeleton tracked. There is a large variation in viewpoint, intra-class subjects and sequence lengths, which makes this dataset challenging. We remove 302 of the captured samples having missing or incomplete skeleton data. The protocol mentioned in Shahroudy \etal \cite{Shahroudy_2016_CVPR} is followed for comparisons with previous methods.

\paragraph{HDM05}\cite{Muller07documentationmocap}
This dataset was captured by using an optical marker-based Vicon system. It contains 2337 action sequences ranging across 130 motion classes performed by \textit{five} actors. This dataset currently has the largest number of motion classes. The actors are named ``bd'', ``bk'', ``dg'', ``mm'' and ``tr'', and 31 joints are annotated for each skeleton. This dataset is challenging due to intra-class variations induced by multiple realizations of same action and large number of motion classes. We follow the protocol given in \cite{huang2017riemannian} which is used by recent deep learning methods.

\subsection{Discussion}
\label{sec:5_2}
\paragraph{\textsc{Part-based graph model}:} Our motivation to use a part-based graph model is derived primarily from the fact that human actions are made up of ``gestures'' which represent motion of a body part. The seminal success of DPMs \cite{Felzenszwalb:2005} in detecting humans in images reinforces the motivation further. We discuss the effect of proposed spatio-temporal part-based graph model below.

\paragraph{(a) How many parts to have?} We start with a coarse-grained scheme where entire skeleton is a single part and progress towards finer representations. The different partitions are, \textit{two parts}: dividing skeleton into axial and appendicular skeleton, \textit{four parts}: as explained in section \ref{sec:stmodel} and \textit{six parts}: Assigning left and right in hands and legs. The feature at each vertex in the input is 3D coordinate of the corresponding joint. From Table \ref{tab:ablation}(a), we can see that using two parts improves over one and four improves over two. This shows that partitioning the skeleton graph into subgraphs with useful properties helps. However, dividing upper and lower skeletons into left and right in four part scheme does not improve performance, as per our intuition about \textit{laterality} mentioned in section \ref{sec:stmodel}. This experiment suggests that part-based model improves performance over single part and being agnostic to laterality is helpful. Our final model uses the \textit{four} part division of the human skeleton.

\paragraph{(b) Comparison to graph-based models} From Table \ref{tab:sota}(a) and Table \ref{tab:ablation}(b), it can be seen that our part-based model performs better than graph based model of Yan \etal \cite{yan2018spatial} even when using  as the feature at each vertex. The graph construction in \cite{yan2018spatial} uses a spatial partitioning scheme for their final model which divides the skeleton graph egde set into several partitions, while the vertex set has no partitions and contains all the joints. The difference in our model is that we divide the \textit{entire} skeleton into \textit{smaller} parts similar to human body parts and hence we use different edge set and vertex set for each part. Compared to graph based model of Li \etal \cite{li2018spatio}, our model performs significantly better on NTURGB+D as well as HDM05. However, it is possible that this is because the number of layers in the network in \cite{li2018spatio} is much smaller (2 vs 9) compared to our model. Our model outperforms both the previous graph based models proposed for skeleton action recognition on the two datasets.

\paragraph{\textsc{Geometric + Kinematic signals}:} Providing an explicit cue to a convolutional network, such as optical flow when performing action recognition from RGB videos \cite{NIPS2014_5353}, which is significant for the task at hand helps learn a richer representation by focusing on the cue. This motivates the use of geometric and kinematic features for skeletal action recognition. For the final configuration of our model, we concatenate the geometric and kinematic signals.

\paragraph{(a) Kinematic: temporal displacements} Temporal displacements provide information about the amount of motion happening between two frames. This information is synonymous to 3D scene flow of a very sparse set of points. We hypothesize that these displacements provide explicit motion information (like optical flow) which makes the model consider displacements as strong features and learn from them. Improvement in performance using this signal can be seen from Table \ref{tab:ablation}(b), for both four-part as well as one-part model across both splits of NTURGB+D.

\paragraph{(b) Geometric: relative coordinates} These provide translation invariant features as explained in \cite{verma2018feastnet} and they have been used effectively to encode skeletons by Ke \etal \cite{ke2017new} into images. Also, Zhang \etal \cite{zhang2017geometric} used relative coordinates as a geometric feature which performs much better than 3D joint locations using a simple stacked LSTM network. We can see improvements in performance provided by relative coordinates in Table \ref{tab:ablation}(b) for both global (one part) and four part-based models, which are the worst and best performing models according to Table \ref{tab:ablation}(a).

\begin{table}[t]
    \begin{center}
        \small
        \begin{tabular}{cc}
            (a) NTURGB+D & (b) HDM05 \\
            \begin{tabular}{ccc}
            \toprule
            \multirow{2}{*}{\textbf{Methods}} & \multicolumn{2}{c}{\textbf{Accuracy}} \\
            & CS & CV \\
            \midrule
            ST Attention \cite{song2017end} & 73.4 & 81.2 \\
            GCA-LSTM \cite{liu2017global} & 74.4 & 82.8 \\
            TCN \cite{kim2017interpretable} & 74.3 & 83.1 \\
            VA-LSTM \cite{zhang2017view} & 79.4 & 87.6 \\
            CNN + MTLN \cite{ke2017new} & 79.6 & 84.8 \\
            \midrule
            Deep STGC \cite{li2018spatio} & 74.9 & 86.3 \\
            STGCN \cite{yan2018spatial} & 81.5 & 88.3 \\
            \midrule
            PB-GCN & \textbf{87.5} & \textbf{93.2} \\
            \bottomrule
            \end{tabular}
            &
            \begin{tabular}{cc}
                \toprule
                \textbf{Methods} & \textbf{Accuracy} \\
                \midrule
                SPDNet \cite{huang2017riemannian} & 61.45  1.12 \\
                Lie Group \cite{vemulapalli2014human} & 70.26  2.89 \\
                LieNet \cite{huang2017deep} & 75.78  2.26 \\
                P-LSTM \cite{Shahroudy_2016_CVPR} & 73.42  2.05 \\
                \midrule
                Deep STGC \cite{li2018spatio} & 85.29  1.33 \\
                STGCN \cite{yan2018spatial} & 82.13  2.39 \\
                \midrule
                PB-GCN & \textbf{88.17}  \textbf{0.99} \\
                \bottomrule
            \end{tabular}
        \end{tabular}
    \end{center}
    \caption{\small{Performance comparison with previous methods on two benchmark datasets. The top group of results correspond to non-graph based methods and the middle corresponds to GCN based methods. PB-GCN is our part-based graph convolutional network. Evaluation protocols used: CS (Cross Subject) and CV (Cross View) for NTURGB+D \cite{Shahroudy_2016_CVPR}; 10-fold cross sample validation for HDM05 \cite{huang2017riemannian}.}}
    \label{tab:sota}
\end{table}

\subsection{Comparison to state of the art}
\label{sec:5_3}
\paragraph{\textsc{NTURGB+D}:} For this dataset, we outperform all previous state-of-the-art methods by a large margin. Even without using the signals introduced in section \ref{sec:signals}, we outperform the previous methods which can be seen in Table \ref{tab:ablation}(b) ( results). We outperform the previous state-of-the-art graph based method of Yan \etal \cite{yan2018spatial} (STGCN) which is also the state-of-the-art for skeleton based action recognition to the best of our knowledge, by a margin of \textasciitilde6\% and \textasciitilde5\% for the two protocols.

\paragraph{\textsc{HDM05}:} This is a \textasciitilde20x smaller dataset compared to NTURGB+D but contains more than twice the number of classes in NTURGB+D. The length of sequences in this dataset is longer and some of the action classes have only one sequence \cite{Cho2014ClassifyingAV}. Using the protocol of \cite{huang2017riemannian} is therefore very challenging, on which we obtain state-of-the-art results using our model. We outperform the previous state-of-the-art Deep STGC \cite{li2018spatio}, which is a network based on spectral graph convolutions for skeleton action recognition by \textasciitilde3\% at the mean accuracy.
\section{Conclusion}
\label{sec:conclusion}
In this paper, we define a partition of skeleton graph on which spatio-temporal convolutions are formalized through a part-based GCN for the task of action recognition. Such a part-based GCN learns the relations between parts and understands the importance of each part in human actions more effectively than a model that considers entire body as a single graph. We also demonstrate the benefit of giving explicit cues to the convolutional model which are significant from the point of view of the task at hand, such as relative coordinates and temporal displacements for skeletal action recognition. As a result, our model achieves state-of-the-art performance on two challenging action recognition datasets. As a future work, we would like to explore the use of part-based graph model for tasks other than action recognition, such as object detection, measuring image similarity, etc.

\bibliography{egbib}

\end{document}
