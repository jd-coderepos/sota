\section{Experiments}

We evaluate our approach on residual architectures~\cite{He2016ResNet,He2016IdentityMappings}, since they are readily compatible with the stochastic dropout.  We take as our main reference the vanilla Vision Transformer introduced by Dosovitskiy \etal~\cite{dosovitskiy2020image}. 
However, as shown  in our experiments, our approach is effective with all the residual architectures that we have considered. 


\subsection{Baseline and training settings}

We adopt the state-of-the-art training procedure proposed in the DeiT III paper~\cite{touvron2022deitIII} as baseline for transformers architectures and the training procedure from Wightman \etal~\cite{wightman2021resnet} for the convnets. 
All hyper-parameters are identical except on Imagenet-21k, where the hyper-parameter $\tau$ are adjusted depending on the training setting. We recapitulate the training hyper-parameters in Appendix~\ref{app:training_details}. 

 We additionally adopt and measure the impact of LayerDecay for the fine-tuning when transferring from Imagenet-21k to Imagenet-1k. This method slightly boosts the performance, as discussed later in this section, and in Table~\ref{tab:ablation_21k}. This method was adopted in multiple recent papers and in particular for fine-tuning of self-supervised approaches~\cite{bao2021beit,He2021MaskedAA}, yet the contribution of this fine-tuning ingredient was not quantitatively measured.

\begin{figure}
\begin{minipage}{0.48 \linewidth}
~ \hfill \quad \small ViT-B ($\SD=0.2$) \hfill  ~
    
    \includegraphics[height=\linewidth]{figs/icodist_B.pdf}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48 \linewidth}
    ~ \hfill \small ViT-L ($\SD=0.45$) \hfill  ~
    
    \includegraphics[height=\linewidth,trim={70pt 0 0 0},clip]{figs/icodist_L.pdf}
    \end{minipage}
    \vspace{-0.5em}
    \caption{Cosub as population training: each submodel extracted by a transformation $\T$ is a valid neural network. Our \ours strategy can hence be regarded as co-training a large number of subnetworks. We plot the accuracy of the submodels as a function of the number of layers that we preserve. 
    We drop layers with probability $\tau$\,=\,$0.2$ for ViT-B and $\tau$\,=\,$0.45$ for ViT-L.  
    On average our method provides a significant boost in performance for the whole population of submodels extracted from the main model. 
\label{fig:sub_mod}}
\end{figure}



\begin{table}[t]
    \centering
    \scalebox{0.8}{
    \begin{tabular}{lccccc}
        \toprule
          Model    &  \ \ ViT-S\ \  & \ ViT-M\  & \ ViT-B\  & \ \ ViT-L\ \  & \ ViT-H \   \\
          $\SD$  & ($0.05$) &  ($0.1$) & ($0.2$) & ($0.45$)& ($0.6$) \\
baseline & 81.4 & 82.5 & 83.7 & 84.5 & 84.9  \\
         \rowcolor{blue!7}
         baseline + \ours \ \ & 81.5 & 82.8 &  83.9 & 84.9 & 85.5 \\
         \bottomrule
    \end{tabular}}
\caption{Comparison of top-1 accuracy with ViT architecture trained with/without \ours~ at resolution $224$ (800 epochs) on Imagenet-1k. 
    The improvement is not significant for the smaller architecture ViT-S. Cosub is gradually more effective when we increase the model size and the SD rate.  
    \label{tab:model_size}}
\end{table}


\begin{table}
    \centering
    \scalebox{0.8}{
    \begin{tabular}{lccccc}
    \toprule
         Model & \ $\SD$ \  & Original & Baseline & \ +\ours\  & \ $\Delta$ \  \\
         \midrule
         \multicolumn{6}{c}{\bf Transformers}\\[1pt]
         ViT-L~\cite{dosovitskiy2020image}  & 0.45 & 76.5 & 84.5 & \cellcolor{blue!7} 84.9 & \diff{+0.4} \\
         CaiT-L24~\cite{touvron2021going} & 0.45 & \_   & 83.8 &  \cellcolor{blue!7} 84.4 & \diff{+0.6} \\
         PiT-B~\cite{Heo2021RethinkingSD}  & 0.25 & 82.0   & 83.8 &  \cellcolor{blue!7} 84.1 & \diff{+0.3} \\
         XCiT-L12~\cite{el2021xcit}  & 0.20 &  \_  & 82.6 &  \cellcolor{blue!7} 83.0 & \diff{+0.4} \\
         Swin-B~\cite{liu2021swin}  & 0.20 & 83.5  & 82.9 &  \cellcolor{blue!7} 83.3 & \diff{+0.4}\\
         Swin-L~\cite{liu2021swin}  & 0.45 & \_   & 80.8 &  \cellcolor{blue!7} 84.0 & \diff{+3.2}\\
              \\[-0.9em]
         \multicolumn{6}{c}{\bf Convnets}\\[1pt]
         ResNet-50~\cite{He2016ResNet} & 0.10 & 76.2 & 80.2 & \cellcolor{blue!7} 80.3 & \diff{+0.1}\\
         ResNet-101~\cite{He2016ResNet} & 0.20 & 77.4 & 81.8 & \cellcolor{blue!7} 82.1 & \diff{+0.4}\\
         ResNet-152~\cite{He2016ResNet} & 0.30 & 78.3 & 82.4 & \cellcolor{blue!7} 83.1 & \diff{+0.7}\\
         RegNet-16GF~\cite{Radosavovic2020RegNet} \quad\,  & 0.30 & 80.4 & 82.9 & \cellcolor{blue!7} 83.8 & \diff{+0.9}\\
\bottomrule

    \end{tabular}}
    \vspace{-0.7em}
    \caption{Benefit of \ours for different architectures trained from scratch on Imagenet-1k at resolution $224$. We  report top-1 acc. for the supervised baseline  and cosub, as well as  results reported in the corresponding papers  when available (trained with different settings). 
    We have adjusted the stochastic-depth drop-rate (SD) hyper-parameter for each architecture. 
    \label{tab:comp_archi}}
\end{table} 



\begin{table}
    \centering
    \scalebox{0.8}{
    \begin{tabular}{rccc}
    \toprule
         Loss & \cellcolor{blue!7} \  BCE-soft \ & \ BCE-hard \ & \ CE-hard \ \\
         \midrule
         Imagenet-val top-1 accuracy  & \cellcolor{blue!7} 83.5 & 83.5 &  81.9 \\
         \bottomrule
    \end{tabular}
    }
    \vspace{-0.7em}
    \caption{Ablation on the loss for \ours with ViT-H trained at resolution $126\!\times\!126$ on Imagenet-1k during 800 epochs. The training is inherited from DeiT-III, which also uses BCE when training with Imagenet-1k only. }
    \label{tab:loss_abl}
\end{table}


\subsection{Empirical analysis of \ours}

We perform various ablations on Imagenet-1k to analyse the impact of our training method on the learned networks.

\mypar{Performance for different model sizes and architectures} 
Table~\ref{tab:model_size} provides the results obtained by the baseline and \ours when we vary the model size of vision transformers. The stochastic depth coefficient was optimized for the baseline and we keep it unchanged with \ours. As to be expected, our method is almost neutral for small models like ViT-S: +0.1\% top-1 accuracy, which is about the standard deviation of measurements. The improvement is increasingly important for larger models, up to a significant improvement of +0.6\% top-1 accuracy for ViT-H models. 

In Table~\ref{tab:comp_archi}, we show that our approach is beneficial with all architectures that we have tried. We report the results of the original paper, evaluate the performance with our baseline training, and measure the improvement brought by \ours. For almost all architectures, we observe a significant boost in performance. The exception is the ResNet-50, for which \ours improves the top-1 accuracy by only +0.1\%, similar to our observation with ViT-S. 
In Table \ref{tab:comp_archi_imnet21k} in the appendix  we present improved results obtained for multiple architectures pre-trained with \ours on Imagenet-21k. 





\mypar{Analysis of submodel performance}
With \ours, we sample different subnetworks during  training to  performed the co-training. 
We analyse the impact of \ours on the accuracy of the sub networks themselves. 
In Figure~\ref{fig:sub_mod} we consider the accuracy of submodels of different size of ViT-B and ViT-L models.
Cosub improves the accuracy of the whole population of sub-networks and, in particular, the target network. 


\mypar{Loss formulation}
In Table~\ref{tab:loss_abl} we experiment with  different losses for \ours. 
With Imagenet1k, DeiT-III training uses BCE instead of CE for the main loss. 
With \ours, BCE is more compatible with the loss of the baseline and, as to be expected, we also observe a better performance with BCE.
We have done ablations using hard and soft targets for the \ours loss.
The results are similar, therefore by default we keep soft-targets for the \ours loss.


\begin{table}
    \centering
    \scalebox{0.8}{
    \begin{tabular}{clcc}
        \toprule
         & Method &  \quad   ViT-L  \quad \  & \ \ \   ViT-H  \ \ \ \\
         \midrule
          (a) & supervised baseline~\cite{touvron2022deitIII} & 84.5 & 84.9 \\
(b) & KD           & 85.3 & 85.3 \\
          (c) & mean teacher & 84.4 & 83.4 \\
          (d) & co-training  & 82.6 & 83.1 \\
\rowcolor{blue!7}
          (e) & \ours        & 84.9 & 85.5 \\
\quad  (b) + (e) \quad \  & KD + \ours & 85.3 & 85.7 \\
          \bottomrule
          
    \end{tabular}}
    \vspace{-0.7em}
    \caption{Training strategies with distillation. We compare on Imagenet-1k at resolution $224\times224$ different  approaches involving co- or self-training with distillation. 
    KD, mean teacher and co-training use the same $\lambda=0.5$ and same hyper-parameters as in Deit-III. Unlike \ours, the mean teacher (c) requires other hyper-parameters for EMA. We perform a small grid-search to adjust this parameters. 
    Note (last row): our approach is complementary with KD, assuming a pre-trained teacher is available beforehand. 
\label{tab:comp_different_app}}
\end{table}


\mypar{Alternative teacher/student} 
In Table \ref{tab:comp_different_app}  we report the results obtained with the different distillation or co-training approaches depicted in Figure~\ref{fig:codist}. Other approaches are not effective off-the-shelf, except KD that requires a pre-trained teacher. 
Our approach is on par with KD (lower for ViT-L, better or ViT-H), and in the last row we show that it even provides a slight boost to combine KD with \ours. 


\mypar{Impact of the \ours loss} 
The hyper-parameter $\lambda$ controls the tradeoff between the co-distillation loss and the cross-entropy classification loss. 
Setting $\lambda=1$ means that we have a regular supervised training setting, except that (i) we double the batch size by duplicating the image after data augmentation, and (ii)  stochastic depth selects different layers for each image copy. 

In Table~\ref{tab:submodel_performance}, we  measure the  impact of   $\lambda$, with all other hyper-parameters being fixed,
 for ViT-H trained at resolution $126\!\times\!126$ on Imagenet-1k.
We observe that the best ratio is to use an equal weighting of the \ours loss and the classic training loss. 
Using the \ours loss increases the performance  by 0.6\% Top-1 accuracy on Imagenet-val, which is the typical improvement that we observe for large models. 


\begin{table}
    \centering
    \scalebox{0.8}{
    \begin{tabular}{c@{\ \quad \ }c@{\ \quad \ }c@{\ \quad \ }c@{\ \quad \ }c@{\ \quad \ }c@{\ \quad \ }c}
    \toprule
        $\lambda$  & 0.1  &  0.3  & \cellcolor{blue!7}  \textbf{0.5}  & 0.7  & 0.9  & 1.0\\
        top1 accuracy      &  79.05 & 83.27 & \cellcolor{blue!7} \textbf{83.55} & 83.20 & 83.04  & 82.91 \\
        \bottomrule
    \end{tabular}}
        \vspace{-0.7em}
    \caption{Ablation of the parameter $\lambda$
controlling the weight of the co-distillation loss across submodels  (Imagenet1k-val, top1-acc).  Model ViT-H trained at resolution $126\!\times\!126$ on Imagenet-1k during 800 epochs. $\lambda=1.0$ corresponds a supervised baseline and $\lambda=0.5$ corresponds to \ours.
\label{tab:submodel_performance}}
\end{table}



\begin{table}
    \centering
    \scalebox{0.8}{
    \begin{tabular}{lcccccc}
        \toprule
        \multirow{2}{*}{Model} & \multirow{2}{*}{$\SD$} & \multirow{2}{*}{\ epochs\ } & \multicolumn{2}{c}{Baseline} & \multicolumn{2}{c}{+\ours}\\
         \cmidrule(lr){4-5} \cmidrule(lr){6-7}
         &  & & val & v2 & val & v2\\
         \midrule
         \multirow{2}{*}{CaiT-M12 \quad \ } & \multirow{2}{*}{0.20} & \pzo400 & \  83.2 \ & \ 72.9\  & \cellcolor{blue!7} \ 83.7 \ & \cellcolor{blue!7} \  \ 73.5 \ \ \\
                                   & & \pzo800 & 82.9 & 72.6 & \cellcolor{blue!7} 83.6 & \cellcolor{blue!7} 73.1 \\
        \\[-0.9em]
                                   

         \multirow{2}{*}{PiT-B} & \multirow{2}{*}{0.25} & \pzo400 & 83.8 & 73.6 & \cellcolor{blue!7} 84.1 &  \cellcolor{blue!7} 74.1\\
                                & & \pzo800 & 82.4 & 71.9 & \cellcolor{blue!7} 83.1 & \cellcolor{blue!7} 72.8  \\
        \\[-0.9em]
         \multirow{4}{*}{ViT-B} & \multirow{4}{*}{0.20} & \pzo400 & 83.1 & 72.6 & \cellcolor{blue!7} 83.2 & \cellcolor{blue!7} 73.1 \\
                                & & \pzo800 & 83.7 & 73.1 & \cellcolor{blue!7} 83.9 & \cellcolor{blue!7} 73.5 \\
                               & & 1200    & 83.3 & 72.8 & \cellcolor{blue!7} \_   & \cellcolor{blue!7} \_ \\
                               & & 1600    & 83.3 & 73.4 & \cellcolor{blue!7} \_   & \cellcolor{blue!7} \_ \\

        \\[-0.9em]
          \multirow{2}{*}{ViT-H}  & \multirow{2}{*}{0.60} & \pzo400 & 84.8 & 75.3 & \cellcolor{blue!7} 85.0 & \cellcolor{blue!7} 75.8 \\
                                 & & \pzo800 & 84.9 & 75.6 & \cellcolor{blue!7} 85.5 & \cellcolor{blue!7} 76.3 \\   
        \bottomrule
     \end{tabular}}
         \vspace{-0.7em}
    \caption{We compare ViT models trained with and without \ours on ImageNet-1k only with different number of epochs at resolution $224\!\times\!224$. 
    One can see that \ours is  more effective for larger models yielding higher values of the SD hyper-parameter $\SD$. 
    It avoids the early saturation or overfitting of the performance that we typically observe with the baseline when we increase the training time without re-adjusting hyper-parameters. See also Table~\ref{tab:submodel_performance} for a direct comparison with and without the co-distillation loss, and Table~\ref{tab:training_cost} for the corresponding training times per epoch. 
    }
    \label{tab:ablation_training_epochs}
\end{table}



 
\mypar{Number of training epochs}
In Table~\ref{tab:ablation_training_epochs} we compare results on Imagenet-1k-val and Imagenet-v2 different for architectures trained with and without \ours on Imagenet-1k only at resolution $224\!\times\!224$ with different number of epochs.
We observe less overfitting with \ours and longer training schedule. In particular, with bigger architecture like ViT-H, we observe  continuous improvement with a longer schedule where the baseline saturates.

\mypar{Training time}
In Table~\ref{tab:training_cost} we compare the training costs of \ours and DeiT-III. 
Thanks to our efficient stochastic depth formulation we maintain a similar memory peak during  training. 
For  bigger architectures the gap in training speed between \ours and the baseline is decreasing.

\begin{table}
    \centering
    \scalebox{0.8}{
    \begin{tabular}{lcccc}
    \toprule
         Training method & \ model \ & \ GPUs \ & Memory & Time  (min)   \\
                &       & used&  peak (GB) & by epoch \ \  \\
         \midrule
         \multirow{2}{*}{DeiT-III} & ViT-L & 32 & 21.4 & \pzo8 \\
                                   & ViT-H & 64 & 27.6 & 12 \\
                                                          \\ [-0.9em]
\multirow{2}{*}{DeiT-III + ESD} & ViT-L & 32 & \cellcolor{red!7}15.1 &\cellcolor{red!7}\pzo9 \\
                                      & ViT-H & 64 & \cellcolor{red!7}15.2 &\cellcolor{red!7}11 \\
                                      \\ [-0.9em]
\multirow{2}{*}{\ours (with ESD)\quad \ }  & ViT-L & 32 & \cellcolor{blue!7}26.9 & \cellcolor{blue!7}16 \\
                                  & ViT-H & 64 &\cellcolor{blue!7}25.0 &\cellcolor{blue!7}17 \\
        \bottomrule
    \end{tabular}}
    \vspace{-0.7em}
    \caption{Training times of different models trained at resolution $224\!\times\!224$ with batch size 2048 on Imagenet-1k with DeiT-III and our approach. 
    \ours uses our efficient stochastic depth (ESD), which amortizes the extra memory needed by \ours, especially for the largest models with  high stochastic depth values (0.45 for ViT-L, and 0.6 for ViT-H). 
    Timings are indicative and not representative of an optimized selection of the batch size. 
    \label{tab:training_cost}}
        \vspace{-0.5em}
\end{table}


\begin{table}
    \centering
    \scalebox{0.8}{
    \begin{tabular}{lccccc}
        \toprule
        \multirow{2}{*}{Model} &  \multirow{2}{*}{Resolution} & \multicolumn{2}{c}{Deit-III Baseline} & \multicolumn{2}{c}{+\ours}\\
                 \cmidrule(lr){3-4} \cmidrule(lr){5-6}
         &  & val & v2 & val & v2\\
         \midrule
         \multirow{3}{*}{ViT-B} & $128\times128$ & 83.5 & 73.4 & \cellcolor{blue!7} 83.8 & \cellcolor{blue!7} 74.0 \\
                                & $192\times192$ & 83.8 & 73.6 & \cellcolor{blue!7} 84.1 & \cellcolor{blue!7} 74.0 \\
                                & $224\times224$ & 83.7 & 73.1 & \cellcolor{blue!7} 83.9 & \cellcolor{blue!7} 73.5 \\
        \\[-0.9em]
          \multirow{3}{*}{ViT-L} & $128\times128$ & 84.5 & 74.7 & \cellcolor{blue!7} 85.1 & \cellcolor{blue!7} 75.5 \\
                                 & $192\times192$ & 84.9 & 75.1 & \cellcolor{blue!7} 85.2 & \cellcolor{blue!7} 75.7 \\
                                 & $224\times224$ & 84.5 & 75.0 & \cellcolor{blue!7} 84.9 & \cellcolor{blue!7} 75.6 \\
        \\[-0.9em]
          \multirow{3}{*}{ViT-H\quad \ \ \ \ } & \quad $126\times126$\quad \  & 85.1 & 75.6 & \cellcolor{blue!7}\ \ 85.6 \ \ & \cellcolor{blue!7} \ \ 76.4 \ \ \\
                                 & $182\times182$ & 85.1 & 75.9 & \cellcolor{blue!7} 85.7 & \cellcolor{blue!7} 76.6 \\
                                 & $224\times224$ & 84.9 & 75.6 & \cellcolor{blue!7} 85.5 & \cellcolor{blue!7} 76.3 \\   
        \bottomrule
     \end{tabular}}
    \vspace{-0.5em}
    \caption{Imagenet-1k val and v2 top-1 accuracy of ViT models trained with and without \ours for 800 epochs on Imagenet-1k at  different resolutions, followed by  finetuning for 20 epochs at resolution $224\!\times\!224$. 
    \label{tab:ablation_training_resolution}}
    \vspace{-0.5em}
\end{table} 
\mypar{Resolution}
In Table~\ref{tab:ablation_training_resolution} we compare  different ViT architectures trained with and without \ours at different resolutions on Imagenet-1k. We fine-tune during 20 epochs at resolution $224 \times224$ before  evaluation at this resolution.
We observe that \ours gives  significant improvements across the different resolutions and models.

\begin{table}
\centering
    \scalebox{0.8}{
    \begin{tabular}{l|cccc|cccc}
    \toprule
\quad	resol. $\rightarrow$  & 112 & 224 & 336	& 448 & 112 & 224 & 336	& 448 \\ 
	  \midrule
$\downarrow$ model    & \multicolumn{4}{c}{Imagenet-val} & \multicolumn{4}{c}{Imagenet-v2} \\
    \midrule
ViT-S & 78.0 & 83.1 & 84.6 & 85.2 & 66.6  & 73.7  & 75.1  & 76.3   \\
ViT-M & 80.6 & 85.0 & 86.0 & 86.3 & 69.6  & 76.0  & 76.8  & 77.2    \\
ViT-B & 82.8 & 86.3 & 86.9 & 87.4 & 72.1  & 77.0 & 77.9  & 78.3      \\
ViT-L & 85.4 & 87.5 & 88.1 & 88.3 & 75.7 & 79.1  & 79.8  & 80.0     \\
ViT-H & 86.2 & 88.0 &  - & - & 76.9  & 79.6  &  -  & -     \\
ViT-g & 86.5 & - &  - & -  & 77.3  &  - &  -  & -     \\
    \bottomrule
    \end{tabular}}
    
    \vspace{-0.5em}
    \caption{\textbf{Performance of  models at different resolutions}. We report the results obtained on Imagenet-val by models of different sizes pre-trained with \ours on Imagenet-21k and fine-tuned on Imagenet-1k. Training schedule: 270 epochs except ViT-g  (90 epochs). 
Except for ViT-S, the results at resolution 336 and 448 were pre-trained on Imagenet-21k at resolution 224 for efficiency reasons. We have not fine-tuned ViT-H and ViT-g at large resolutions since these models are computationally expensive. 
\label{fig:resolution}}
\end{table}


\mypar{Imagenet-21k impact of layer-decay}
In Table~\ref{tab:ablation_21k} we compare different number of epochs for the pre-training on Imagenet-21k and the finetuning on Imagenet-1k with and without layer-decay. 
We observe that both layer-decay and long training bring  improvements with \ours.


\begin{table}
    \centering
    \scalebox{0.8}{
    \begin{tabular}{lcc|ccc}
         \toprule
         \multirow{2}{*}{Method} &  Long   & \ \ \ \ Layer \ \ \  & \multicolumn{3}{c}{Model}\\
                                 & \ \ Training\ \   & \ \ Decay\ \  & \ ViT-S \  & \  ViT-B \  & \ ViT-L \  \\
        \midrule
        \multirow{1}{*}{baseline\quad \ } 
                                 & \xmarkgr & \xmarkgr & 82.6 & 85.2 & 86.8\\
                                   \\[-0.9em]
\multirow{4}{*}{\ours} & \xmarkgr & \xmarkgr &\cellcolor{blue!7}82.5 &\cellcolor{blue!7}85.8 & \cellcolor{blue!7}87.4\\
                               & \xmarkgr & \cmarkgr &\cellcolor{blue!7}82.7 & \cellcolor{blue!7}86.0 & \cellcolor{blue!7}87.5\\
                               & \cmarkgr & \xmarkgr &\cellcolor{blue!7}82.8 & \cellcolor{blue!7}86.0 & \cellcolor{blue!7}87.4\\
                               & \cmarkgr & \cmarkgr &\cellcolor{blue!7}83.1 & \cellcolor{blue!7}86.3 & \cellcolor{blue!7}87.5\\
        
        \bottomrule
    \end{tabular}}
    \vspace{-0.5em}
    \caption{We measure the impact of layer-decay during the finetuning on Imagenet-1k for models pre-trained on Imagenet-21k with \ours during 90 epochs (default) and 270 epochs (long training).  
    \label{tab:ablation_21k}}
\end{table}




\subsection{Comparisons with the state of the art}


\mypar{Imagenet-1k}
In Table~\ref{tab:mainres} we compare architectures trained with \ours with state-of-the-art results from the literature for this architecture. We observe that architectures trained with \ours are very competitive.
For instance, for ResNet-152, RegNetY-16GF, PiT-B we improve the best results reported in the literature by more than $0.9\%$ top-1 accuracy on Imagenet-1k-val.

\begin{table}
   
\centering
\scalebox{0.67}{
    \begin{tabular}{@{\ }l@{}c@{\ \ }c@{\ \ \ }r@{\ \ \ }r|cc@{\ }}
        \toprule
        Model        & nb params & throughput & FLOPs & Peak Mem & Top-1  & v2 \\
                      & ($\times 10^6$) & (im/s) & ($\times 10^9$) & (MB)\ \ \ \  & Acc.  & Acc. \\[3pt]

\toprule
\multicolumn{7}{c}{\textbf{``Traditional'' ConvNets}} \\[3pt]

   
     ResNet-50~\cite{He2016ResNet,wightman2021resnet} &  \pzo25.6    & 2587  &  4.1      & 2182 & 80.4  & 68.7 \\
    ResNet-101~\cite{He2016ResNet,wightman2021resnet}&  \pzo44.5    & 1586  &  7.9      & 2269 & 81.5  & 70.3  \\
    \rowcolor{blue!7}
    ResNet-101 -- \ours & \pzo44.5    & 1586  &  7.9      & 2269 & 82.1 & 70.8 \\
    ResNet-152~\cite{He2016ResNet,wightman2021resnet}&  \pzo60.2    &  1122 & 11.6      & 2359 & 82.0  & 70.6 \\
       \rowcolor{blue!7}
     ResNet-152 -- \ours &  \pzo60.2    &  1122 & 11.6      & 2359 &  83.1 & 72.1 \\
	 RegNetY-8GF~\cite{Radosavovic2020RegNet,wightman2021resnet}       & \pzo39.2  & 1158 & \tzo8.0 & 3939 & 82.2 & 71.1 \\
	 RegNetY-16GF~\cite{Radosavovic2020RegNet,Touvron2020TrainingDI}      & \pzo83.6  & \pzo714 & \dzo16.0 & 5204   & 82.9  & 72.4 \\

     \rowcolor{blue!7}
     RegNetY-16GF -- \ours~~\,   & \pzo83.6  & \pzo714 & \dzo16.0 & 5204   & 83.8  & 72.8 \\
     \\[-0.8em]
     

\multicolumn{7}{c}{\textbf{Vision Transformers derivatives}} \\ [5pt]
	Swin-T~\cite{liu2021swin} & \pzo28.3 & 1109\pzo & 4.5 & 3345 & 81.3 &  69.5 \\
    Swin-B~\cite{liu2021swin} & \pzo87.8  & 532 & 15.4 & 4695 & 83.5 &   \_ \\
    PiT-S~\cite{Heo2021RethinkingSD} & \pzo23.5 & 1809\pzo & 2.9 &  3293 & 80.9 & \_\\
    \rowcolor{blue!7}
     PiT-S -- \ours & \pzo23.5 & 1809\pzo & 2.9 &  3293 & 81.3 & 69.7 \\
     
    PiT-B~\cite{Heo2021RethinkingSD} & \pzo73.8 & 615 & 12.5  &  7564 & 82.0 & \_\\
    \rowcolor{blue!7}
     PiT-B -- \ours & \pzo73.8 & 615 & 12.5  &  7564 & 84.1 &74.1 \\
     \\[-0.8em]
    
 \multicolumn{7}{c}{\textbf{Vanilla Vision Transformers}} \\[3pt]

 
    ViT-S ~\cite{Touvron2020TrainingDI} & \pzo22.0  & 1891\pzo & \tzo4.6 & 987 & 79.8 & 68.1  \\
    ViT-S -- Deit III~\cite{touvron2022deitIII} & \pzo22.0  & 1891\pzo & \tzo4.6 & 987 & 81.4 &  70.5 \\
        \rowcolor{blue!7}
    ViT-S -- \ours & \pzo22.0  & 1891\pzo & \tzo4.6 & 987 & 81.5 &  70.8 \\

    ViT-B~\cite{dosovitskiy2020image}  & \pzo86.6  & 831  & \dzo17.5 & 2078 & 77.9 &  --\\
    ViT-B~-- DeiT\cite{Touvron2020TrainingDI}  & \pzo86.6  & 831  & \dzo17.5 & 2078 & 81.8 & 71.5 \\
    ViT-B~-- DeiT/distilled & \pzo86.6  & 831  & \dzo17.5 & 2078 & 83.4 & 73.2 \\
    ViT-B -- Deit III~\cite{touvron2022deitIII}  & \pzo86.6  & 831  & \dzo17.5 & 2078 & 83.8 &  73.6\\
    
    \rowcolor{blue!7}
    ViT-B -- \ours & \pzo86.6  & 831  & \dzo17.5 & 2078 & 84.2 & 74.2 \\
    
    ViT-L -- Deit III~\cite{touvron2022deitIII}  & 304.4 & 277 & 61.6 & 3789 & 84.9 & 75.1    \\
    \rowcolor{blue!7}
    ViT-L -- \ours & 304.4 & 277 & 61.6 & 3789 & 85.3 &  75.5   \\
    ViT-H -- Deit III~\cite{touvron2022deitIII}  & 632.1 & 112 & 167.4 & 6984 & 85.2 & 75.9     \\
   
    

    \rowcolor{blue!7}
    ViT-H -- \ours  & 632.1 & 112 & 167.4 & 6984 & 85.7 &  76.6 \\
    


    \bottomrule
    \end{tabular}}
    \vspace{-0.5em}
  \caption{
\textbf{Classification with Imagenet1k training.} 
We compare with models trained on Imagenet-1k only at resolution $224\!\times\!224$ without self-supervised pre-training (see supp. material for a comparison).
We report Top-1 accuracy on Imagenet-1k-val and Imagenet-v2 with different measures of complexity: throughput, FLOPs, number of parameters and peak memory usage. 
The throughput and peak memory are measured on a single V100-32GB GPU with batch size fixed to 256 and mixed precision. 
\label{tab:mainres}}
\end{table}


\mypar{Imagenet-21k} In Table~\ref{tab:mainres_22k} we compare ViT models pre-trained with \ours on Imagenet-21k and finetuned with \ours on Imagenet-1k with other architectures and our baseline DeiT-III.
Our results with vanilla ViT outperform the baseline and are competitive with  recent architectures. 

\begin{table}
\centering
\scalebox{0.66}{
    \begin{tabular}{@{\ }l@{}c@{\ \ }c@{\ \ \ }r@{\ \ \ }r|cc@{\ }}
        \toprule
        Architecture        & nb params & throughput & FLOPs & peak mem & \multicolumn{2}{c}{top1 acc.}  \\
                      & ($\times 10^6$) & (im/s) & ($\times 10^9$) & (MB)\ \ \ \  & val  & v2 \\[3pt]

\toprule
\multicolumn{7}{c}{\textbf{Convnets}}  \\[3pt]

EfficientNetV2-M~\cite{Tan2021EfficientNetV2SM} & \pzo54.1 & 312 & 25.0 & 7127 & 86.2 & 75.9 \\
EfficientNetV2-L~\cite{Tan2021EfficientNetV2SM} & 118.5 & 179 & 53.0 & 9540 & 86.8 & 76.9 \\
    \\[-0.7em]  

    ResNet-152~\cite{He2016ResNet,wightman2021resnet}&  \pzo60.2    &  1122\pzo & 11.6      & 2359 & 82.0  & 70.6 \\
\rowcolor{blue!7}
    ResNet-152 -- \ours & \pzo 60.2    &  1122\pzo & 11.6      & 2359 & 83.1 & 73.1 \\
         \\[-0.9em]  

\rowcolor{blue!7}
	 RegnetY16GF -- \ours \quad~   & \pzo83.6  & 714 & \dzo16.0 & 5204   & 84.2  & 74.7 \\

    \\[-0.9em]  

    
\rowcolor{blue!7}
    ConvNeXt-S-- \ours & \pzo 50.2 & 783 & \pzo8.7 &2218 &  85.2 &  76.0 \\     
    ConvNeXt-B \cite{Liu2022convnext}& \pzo88.6 & 563 & 15.4 &3029 &  85.8 & 75.6 \\     
\rowcolor{blue!7}
    ConvNeXt-B -- \ours & \pzo88.6 & 563 & 15.4 &3029 &  85.8 &  76.9 \\     
    
    ConvNeXt-L \cite{Liu2022convnext} & 197.8 & 344 & 34.4 & 4865 &  86.6 & 76.6\\     
    
    ConvNeXt-XL  \cite{Liu2022convnext}& 350.2 & 241 & 60.9 & 6951 &  87.0 & 77.0\\     
    \\[-0.9em]  

    

\multicolumn{7}{c}{\textbf{Transformers variations}} \\ [3pt]
     Swin-B~\cite{liu2021swin} & \pzo87.8 & 532 & 15.4 & 4695 &  85.2 & 74.6 \\
\rowcolor{blue!7}
     Swin-B -- \ours  & \pzo87.8 & 532 & 15.4 & 4695 & 86.2 &  77.2  \\
     
     Swin-L~\cite{liu2021swin} & 196.5 & 337 & 34.5 & 7350 &  86.3 & 76.3 \\
\rowcolor{blue!7}
     Swin-L -- \ours  & 196.5 & 337 & 34.5 & 7350 &  87.1 &  78.1  \\
         \\[-0.9em]  

\rowcolor{blue!7}
         PiT-B -- \ours~\cite{Heo2021RethinkingSD}       & \pzo73.8     & 615 & 12.5   & 7564 &  \cellcolor{blue!7}85.8         &  76.8 \\
            \\[-0.9em]  
\rowcolor{blue!7}
         XCiT-S12 -- \ours~\cite{el2021xcit}              &  \pzo26.2    &  1373 &  4.9 & 1330   &  \cellcolor{blue!7}84.2                   &  74.9 \\
\rowcolor{blue!7}
         XCiT-M24 -- \ours~\cite{el2021xcit}              &  \pzo84.4    & 553  &  16.2  & 2010    &  \cellcolor{blue!7}86.5                &  78.0 \\
\rowcolor{blue!7}
         XCiT-L24 -- \ours~\cite{el2021xcit}              &  189.0       & 334  &  36.1  & 3315   &  \cellcolor{blue!7}87.2                 &  77.8 \\
    \\[-0.8em]  

\multicolumn{7}{c}{\textbf{Vanilla Vision Transformers}~\cite{dosovitskiy2020image,touvron2022deitIII} } \\ [3pt]

\rowcolor{blue!7}
ViT-S -- \ours & \pzo22.0  & 1891\pzo  & \tzo4.6 & \pzo987 & 83.1 & 73.7 \\
\rowcolor{blue!7}
ViT-M -- \ours & \pzo39.0  & 1307\pzo  & \tzo8.0 & 1322 & 85.0 & 76.0 \\
ViT-B -- DeiT-III  & \pzo86.6  & 831  & \dzo17.6 & 2078 & 85.7 & 76.5 \\
\rowcolor{blue!7}
ViT-B -- \ours & \pzo86.6  & 831  & \dzo17.6 & 2078 & 86.3 & 77.0 \\

ViT-L -- DeiT-III & 304.4 & 277 & 61.6 & 3789  & 87.0 &  78.6 \\
\rowcolor{blue!7}
ViT-L -- \ours & 304.4 & 277 & 61.6 & 3789  & 87.5 &  79.1 \\

ViT-H -- DeiT-III & 632.1 & 112 & 167.4 & 6984 & 87.2 & 79.2 \\
 
\rowcolor{blue!7}
ViT-H -- \ours & 632.1 & 112 & 167.4 & 6984 & 88.0 & 80.0 \\
    \bottomrule
    \end{tabular}}
    \vspace{-0.7em}
\caption{\textbf{Classification with ImageNet-21k pretraining.} 
We report top-1 accuracy on the validation set of Imagenet1k and Imagenet-V2 with different measures of complexity. The peak memory usage is measured on a single V100-32GB GPU with batch size fixed to 256 and mixed precision. For Swin-L the memory peak is an estimation since we decreased the batch size to 128 to fit in memory.
All models are evaluated at resolution 224 except EfficientNetV2 that use resolution 480. 
ViT are pretrained wit a $\times$3 schedule, comparable to that used in the best DeiT-III baseline (270 vs.\ 240 epochs). All other \ours models are pretrained during 90 epochs on Imagenet21k, with 50 epochs of fine-tuning. The $\tau$ hyper-parameter is set per model based on prior choices or  best guess based on model size. \label{tab:mainres_22k}}
    \vspace{-0.4em}
\end{table}




\mypar{Overfitting analysis}
As recommended in prior works~\cite{wightman2021resnet,touvron2022deitIII} we perform an overfitting analysis. We evaluate our models trained with codist on Imagenet-1k val and Imagenet-v2~\cite{Recht2019Imagenetv2}. The results are reported in Figure~\ref{fig:imagenet_v2}. For ViT, we observe that \ours does not overfit more than the DeiT-III baseline~\cite{touvron2022deitIII}. Our results with other architectures in Table~\ref{tab:mainres_22k} concur with that observation: our results are comparatively stronger on Imagenet-v2 than those reported in the literature for the exact same models. 

\begin{figure}
\begin{minipage}{0.48 \linewidth}
    ~ \hfill \quad \footnotesize Imagenet-1k \hfill ~    
    
\includegraphics[height=0.95\linewidth]{figs/overfit_1k.pdf}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48 \linewidth}
    ~ \hfill \footnotesize Imagenet-21k \hfill ~
    
    \includegraphics[trim={30pt 0 0 0},clip,height=0.95\linewidth]{figs/overfit_21k.pdf}
\end{minipage}
    \vspace{-0.5em}
    \caption{Overfitting measurement: top-1 accuracy on Imagenet-val vs.\ Imagenet-v2 for models in Tables~\ref{tab:mainres} and~\ref{tab:mainres_22k} pre-trained on Imagenet-1k and Imagenet-21k, respectively. Our cosub ViTs (plain lines and points) do not overfit more than DeiT-III~\cite{touvron2022deitIII} overall. Our Imagenet-21k results for Swin and Convnext generalize much better on v2 than the original models. 
\label{fig:imagenet_v2}
    }
    \vspace{-2em}
\end{figure}



\subsection{Downstream tasks}


\mypar{Semantic segmentation}
First we evaluate our ViT models pre-trained on Imagenet with \ours for semantic segmentation on the  ADE20k dataset~\cite{Zhou2017ScenePT}. 
ADE20k consists of 20k training and 5k validation images with labels over 150 categories. 
We adopt the training schedule from Swin:~160k iterations with UperNet~\cite{xiao2018unified}. 
At test time we evaluate with a single scale and multi-scale.
Our UperNet implementation is the same as in  DeiT III~\cite{touvron2022deitIII}.
Our results are reported in Table~\ref{tab:sem_seg}.
We observe that vanilla ViTs trained with \ours outperform our baseline DeiT-III but also have better FLOPs-accuracy trade-offs than recent architectures. 

\begin{table}
        \centering
        \scalebox{0.73}{
        \begin{tabular}{lcccc}
        \toprule
             \multirow{2}{*}{Backbone}  
              & \#params \ \ & \ \  FLOPs \ \ & Single-scale & Multi-scale  \\
&  ($\times 10^6$) & ($\times 10^9$) & mIoU  & mIoU \\
            \midrule 
             DeiT-S  &  \pzo52.0 & 1099 & \_ & 44.0 \\

             Swin-T    & \pzo59.9 & \pzo945 & 44.5 & 46.1 \\

            ViT-S -- DeiT-III  & \pzo41.7 & \pzo588 & 45.6 & 46.8\\
            \rowcolor{blue!7}
            ViT-S -- \ours & \pzo41.7 & \pzo588 & \textbf{47.0} & \textbf{48.0}\\
            \multicolumn{5}{c}{}\\[-0.7em]

            PatchConvNet-B60 & 140.6 & 1258  & 48.1  & 48.6 \\

             PatchConvNet-B120  & 229.8 & 1550  & 49.4  & 50.3\\
             MAE ViT-B  & 127.7 & 1283 & 48.1 &  \_ \\
             Swin-B   & 121.0 & 1188  & 48.1 & 49.7 \\
            ViT-B -- DeiT-III  & 127.7 & 1283 & 49.3 &  50.2\\
            \rowcolor{blue!7}
            ViT-B -- \ours & 127.7 & 1283 & 49.3 &  49.9\\
            ViT-L -- DeiT-III  & 353.6 & 2231 & 51.5 & 52.0  \\    
            \rowcolor{blue!7}
            ViT-L -- \ours & 353.6 & 2231 & \textbf{52.5} & \textbf{53.1}  \\  
            \multicolumn{5}{c}{}\\[-0.7em]

            
            PatchConvNet--B60$^\dagger$  & 140.6 & 1258  & 50.5  & 51.1 \\

            Swin-B$^\dagger$ ($640\times 640$)  & 121.0 & 1841  & 50.0 & 51.6 \\
            ViT-B -- DeiT-III $^\dagger$  & 127.7 & 1283 & 53.4 & 54.1\\
            \rowcolor{blue!7}
            ViT-B -- cosub$^\dagger$  & 127.7 & 1283 & 53.7  & 54.7 \\
            PatchConvNet-L120$^\dagger$  & 383.7 & 2086  &  52.2 & 52.9   \\
            Swin-L$^\dagger$ ($640\times 640$)  & 234.0 & 3230  & \_ & 53.5 \\
            ViT-L -- DeiT-III $^\dagger$  & 353.6 & 2231 &  54.6 & 55.6 \\
            \rowcolor{blue!7}
            ViT-L -- cosub$^\dagger$  & 353.6 & 2231 & \textbf{55.7} & \textbf{56.3}  \\
        \bottomrule     
        \end{tabular}
        } 
            \vspace{-0.5em}
          \caption{\textbf{ADE20k semantic segmentation} performance using UperNet \cite{xiao2018unified},  in comparable setting as prior works~\cite{Dong2021CSWinTA,el2021xcit,liu2021swin}. 
          All models are pre-trained on Imagenet-1k,  except bottom models identified with $^\dagger$, which are pre-trained on Imagenet-21k. By default the finetuning resolution on ADE20k is $512\!\times\!512$ except when mentioned otherwise (for Swin).
       \label{tab:sem_seg}}
\end{table}

\mypar{Transfer learning}
We now measure how the performance improvements observed with \ours  translate to other classification problems. For this purpose, we performed transfer learning on the six different datasets used in DeiT-III. 
Our results are reported Table~\ref{tab:sota_tl}. 
Our pre-trained and finetuned models with \ours generally improve the baseline. The gains are overall more significant on the more challenging datasets like iNaturalist 2018 and iNaturalist 2019.



\begin{table}

    \centering
    \scalebox{0.74}{
    \begin{tabular}{l@{}c@{\ \ }c@{\ \ }cccc}
    \toprule
    Model    & Cifar-10 & Cifar-100  & Flowers & Cars & iNat-18 & iNat-19  \\
    \midrule                                                                          

    ViT-S -- DeiT-III  & 98.9 & 90.6 & 96.4 & 89.9 & 67.1 & 72.7 \\
    ViT-B -- DeiT-III  & 99.3 & 92.5 & 98.6 & 93.4 & 73.6 & 78.0\\ 
    ViT-L -- DeiT-III  & 99.3 & 93.4 & \textbf{98.9} & \textbf{94.5} & 75.6 & 79.3 \\ 
    \rowcolor{blue!7}
    ViT-S -- \ours & 99.1 & 91.7 & 97.4 & 93.0 & 70.1 & 75.6 \\
    \rowcolor{blue!7}    
    ViT-B -- \ours & 99.1 & 92.6 & 98.4 & 93.5 & 74.1 & 78.1\\ 
    \rowcolor{blue!7}
    ViT-L -- \ours & \textbf{99.4} & \textbf{93.5} & 98.8 & \textbf{94.5} & \textbf{76.2} & \textbf{80.1}  \\
    \bottomrule
    \end{tabular}}
    \vspace{-0.7em}
        \caption{
        ViT models pre-trained with \ours or DeiT-III on Imagenet-1k and finetuned on six different target datasets. 
        We note that for small datasets (CIFAR, Flowers, and Cars) our approach is useful for small models but neutral for larger models.
        The gains are more significant when transferring to the larger iNaturalist-18 and iNaturalist-19 datasets. 
    \label{tab:sota_tl}}
    \vspace{-0.57em}
\end{table} 