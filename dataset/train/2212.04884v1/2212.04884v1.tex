\section{Experiments}

We evaluate our approach on residual architectures~\cite{He2016ResNet,He2016IdentityMappings}, since they are readily compatible with the stochastic dropout.  We take as our main reference the vanilla Vision Transformer introduced by Dosovitskiy \etal~\cite{dosovitskiy2020image}. 
However, as shown  in our experiments, our approach is effective with all the residual architectures that we have considered. 


\subsection{Baseline and training settings}

We adopt the state-of-the-art training procedure proposed in the DeiT III paper~\cite{touvron2022deitIII} as baseline for transformers architectures and the training procedure from Wightman \etal~\cite{wightman2021resnet} for the convnets. 
All hyper-parameters are identical except on Imagenet-21k, where the hyper-parameter  are adjusted depending on the training setting. We recapitulate the training hyper-parameters in Appendix~\ref{app:training_details}. 

 We additionally adopt and measure the impact of LayerDecay for the fine-tuning when transferring from Imagenet-21k to Imagenet-1k. This method slightly boosts the performance, as discussed later in this section, and in Table~\ref{tab:ablation_21k}. This method was adopted in multiple recent papers and in particular for fine-tuning of self-supervised approaches~\cite{bao2021beit,He2021MaskedAA}, yet the contribution of this fine-tuning ingredient was not quantitatively measured.

\begin{figure}
\begin{minipage}{0.48 \linewidth}
~ \hfill \quad \small ViT-B () \hfill  ~
    
    \includegraphics[height=\linewidth]{figs/icodist_B.pdf}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48 \linewidth}
    ~ \hfill \small ViT-L () \hfill  ~
    
    \includegraphics[height=\linewidth,trim={70pt 0 0 0},clip]{figs/icodist_L.pdf}
    \end{minipage}
    \vspace{-0.5em}
    \caption{Cosub as population training: each submodel extracted by a transformation  is a valid neural network. Our \ours strategy can hence be regarded as co-training a large number of subnetworks. We plot the accuracy of the submodels as a function of the number of layers that we preserve. 
    We drop layers with probability \,=\, for ViT-B and \,=\, for ViT-L.  
    On average our method provides a significant boost in performance for the whole population of submodels extracted from the main model. 
\label{fig:sub_mod}}
\end{figure}



\begin{table}[t]
    \centering
    \scalebox{0.8}{
    \begin{tabular}{lccccc}
        \toprule
          Model    &  \ \ ViT-S\ \  & \ ViT-M\  & \ ViT-B\  & \ \ ViT-L\ \  & \ ViT-H \   \\
            & () &  () & () & ()& () \\
baseline & 81.4 & 82.5 & 83.7 & 84.5 & 84.9  \\
         \rowcolor{blue!7}
         baseline + \ours \ \ & 81.5 & 82.8 &  83.9 & 84.9 & 85.5 \\
         \bottomrule
    \end{tabular}}
\caption{Comparison of top-1 accuracy with ViT architecture trained with/without \ours~ at resolution  (800 epochs) on Imagenet-1k. 
    The improvement is not significant for the smaller architecture ViT-S. Cosub is gradually more effective when we increase the model size and the SD rate.  
    \label{tab:model_size}}
\end{table}


\begin{table}
    \centering
    \scalebox{0.8}{
    \begin{tabular}{lccccc}
    \toprule
         Model & \  \  & Original & Baseline & \ +\ours\  & \  \  \\
         \midrule
         \multicolumn{6}{c}{\bf Transformers}\-0.9em]
         \multicolumn{6}{c}{\bf Convnets}\-0.9em]
                                   

         \multirow{2}{*}{PiT-B} & \multirow{2}{*}{0.25} & \pzo400 & 83.8 & 73.6 & \cellcolor{blue!7} 84.1 &  \cellcolor{blue!7} 74.1\\
                                & & \pzo800 & 82.4 & 71.9 & \cellcolor{blue!7} 83.1 & \cellcolor{blue!7} 72.8  \\
        \-0.9em]
          \multirow{2}{*}{ViT-H}  & \multirow{2}{*}{0.60} & \pzo400 & 84.8 & 75.3 & \cellcolor{blue!7} 85.0 & \cellcolor{blue!7} 75.8 \\
                                 & & \pzo800 & 84.9 & 75.6 & \cellcolor{blue!7} 85.5 & \cellcolor{blue!7} 76.3 \\   
        \bottomrule
     \end{tabular}}
         \vspace{-0.7em}
    \caption{We compare ViT models trained with and without \ours on ImageNet-1k only with different number of epochs at resolution . 
    One can see that \ours is  more effective for larger models yielding higher values of the SD hyper-parameter . 
    It avoids the early saturation or overfitting of the performance that we typically observe with the baseline when we increase the training time without re-adjusting hyper-parameters. See also Table~\ref{tab:submodel_performance} for a direct comparison with and without the co-distillation loss, and Table~\ref{tab:training_cost} for the corresponding training times per epoch. 
    }
    \label{tab:ablation_training_epochs}
\end{table}



 
\mypar{Number of training epochs}
In Table~\ref{tab:ablation_training_epochs} we compare results on Imagenet-1k-val and Imagenet-v2 different for architectures trained with and without \ours on Imagenet-1k only at resolution  with different number of epochs.
We observe less overfitting with \ours and longer training schedule. In particular, with bigger architecture like ViT-H, we observe  continuous improvement with a longer schedule where the baseline saturates.

\mypar{Training time}
In Table~\ref{tab:training_cost} we compare the training costs of \ours and DeiT-III. 
Thanks to our efficient stochastic depth formulation we maintain a similar memory peak during  training. 
For  bigger architectures the gap in training speed between \ours and the baseline is decreasing.

\begin{table}
    \centering
    \scalebox{0.8}{
    \begin{tabular}{lcccc}
    \toprule
         Training method & \ model \ & \ GPUs \ & Memory & Time  (min)   \\
                &       & used&  peak (GB) & by epoch \ \  \\
         \midrule
         \multirow{2}{*}{DeiT-III} & ViT-L & 32 & 21.4 & \pzo8 \\
                                   & ViT-H & 64 & 27.6 & 12 \\
                                                          \\ [-0.9em]
\multirow{2}{*}{DeiT-III + ESD} & ViT-L & 32 & \cellcolor{red!7}15.1 &\cellcolor{red!7}\pzo9 \\
                                      & ViT-H & 64 & \cellcolor{red!7}15.2 &\cellcolor{red!7}11 \\
                                      \\ [-0.9em]
\multirow{2}{*}{\ours (with ESD)\quad \ }  & ViT-L & 32 & \cellcolor{blue!7}26.9 & \cellcolor{blue!7}16 \\
                                  & ViT-H & 64 &\cellcolor{blue!7}25.0 &\cellcolor{blue!7}17 \\
        \bottomrule
    \end{tabular}}
    \vspace{-0.7em}
    \caption{Training times of different models trained at resolution  with batch size 2048 on Imagenet-1k with DeiT-III and our approach. 
    \ours uses our efficient stochastic depth (ESD), which amortizes the extra memory needed by \ours, especially for the largest models with  high stochastic depth values (0.45 for ViT-L, and 0.6 for ViT-H). 
    Timings are indicative and not representative of an optimized selection of the batch size. 
    \label{tab:training_cost}}
        \vspace{-0.5em}
\end{table}


\begin{table}
    \centering
    \scalebox{0.8}{
    \begin{tabular}{lccccc}
        \toprule
        \multirow{2}{*}{Model} &  \multirow{2}{*}{Resolution} & \multicolumn{2}{c}{Deit-III Baseline} & \multicolumn{2}{c}{+\ours}\\
                 \cmidrule(lr){3-4} \cmidrule(lr){5-6}
         &  & val & v2 & val & v2\\
         \midrule
         \multirow{3}{*}{ViT-B} &  & 83.5 & 73.4 & \cellcolor{blue!7} 83.8 & \cellcolor{blue!7} 74.0 \\
                                &  & 83.8 & 73.6 & \cellcolor{blue!7} 84.1 & \cellcolor{blue!7} 74.0 \\
                                &  & 83.7 & 73.1 & \cellcolor{blue!7} 83.9 & \cellcolor{blue!7} 73.5 \\
        \-0.9em]
          \multirow{3}{*}{ViT-H\quad \ \ \ \ } & \quad \quad \  & 85.1 & 75.6 & \cellcolor{blue!7}\ \ 85.6 \ \ & \cellcolor{blue!7} \ \ 76.4 \ \ \\
                                 &  & 85.1 & 75.9 & \cellcolor{blue!7} 85.7 & \cellcolor{blue!7} 76.6 \\
                                 &  & 84.9 & 75.6 & \cellcolor{blue!7} 85.5 & \cellcolor{blue!7} 76.3 \\   
        \bottomrule
     \end{tabular}}
    \vspace{-0.5em}
    \caption{Imagenet-1k val and v2 top-1 accuracy of ViT models trained with and without \ours for 800 epochs on Imagenet-1k at  different resolutions, followed by  finetuning for 20 epochs at resolution . 
    \label{tab:ablation_training_resolution}}
    \vspace{-0.5em}
\end{table} 
\mypar{Resolution}
In Table~\ref{tab:ablation_training_resolution} we compare  different ViT architectures trained with and without \ours at different resolutions on Imagenet-1k. We fine-tune during 20 epochs at resolution  before  evaluation at this resolution.
We observe that \ours gives  significant improvements across the different resolutions and models.

\begin{table}
\centering
    \scalebox{0.8}{
    \begin{tabular}{l|cccc|cccc}
    \toprule
\quad	resol.   & 112 & 224 & 336	& 448 & 112 & 224 & 336	& 448 \\ 
	  \midrule
 model    & \multicolumn{4}{c}{Imagenet-val} & \multicolumn{4}{c}{Imagenet-v2} \\
    \midrule
ViT-S & 78.0 & 83.1 & 84.6 & 85.2 & 66.6  & 73.7  & 75.1  & 76.3   \\
ViT-M & 80.6 & 85.0 & 86.0 & 86.3 & 69.6  & 76.0  & 76.8  & 77.2    \\
ViT-B & 82.8 & 86.3 & 86.9 & 87.4 & 72.1  & 77.0 & 77.9  & 78.3      \\
ViT-L & 85.4 & 87.5 & 88.1 & 88.3 & 75.7 & 79.1  & 79.8  & 80.0     \\
ViT-H & 86.2 & 88.0 &  - & - & 76.9  & 79.6  &  -  & -     \\
ViT-g & 86.5 & - &  - & -  & 77.3  &  - &  -  & -     \\
    \bottomrule
    \end{tabular}}
    
    \vspace{-0.5em}
    \caption{\textbf{Performance of  models at different resolutions}. We report the results obtained on Imagenet-val by models of different sizes pre-trained with \ours on Imagenet-21k and fine-tuned on Imagenet-1k. Training schedule: 270 epochs except ViT-g  (90 epochs). 
Except for ViT-S, the results at resolution 336 and 448 were pre-trained on Imagenet-21k at resolution 224 for efficiency reasons. We have not fine-tuned ViT-H and ViT-g at large resolutions since these models are computationally expensive. 
\label{fig:resolution}}
\end{table}


\mypar{Imagenet-21k impact of layer-decay}
In Table~\ref{tab:ablation_21k} we compare different number of epochs for the pre-training on Imagenet-21k and the finetuning on Imagenet-1k with and without layer-decay. 
We observe that both layer-decay and long training bring  improvements with \ours.


\begin{table}
    \centering
    \scalebox{0.8}{
    \begin{tabular}{lcc|ccc}
         \toprule
         \multirow{2}{*}{Method} &  Long   & \ \ \ \ Layer \ \ \  & \multicolumn{3}{c}{Model}\\
                                 & \ \ Training\ \   & \ \ Decay\ \  & \ ViT-S \  & \  ViT-B \  & \ ViT-L \  \\
        \midrule
        \multirow{1}{*}{baseline\quad \ } 
                                 & \xmarkgr & \xmarkgr & 82.6 & 85.2 & 86.8\\
                                   \3pt]

\toprule
\multicolumn{7}{c}{\textbf{``Traditional'' ConvNets}} \-0.8em]
     

\multicolumn{7}{c}{\textbf{Vision Transformers derivatives}} \\ [5pt]
	Swin-T~\cite{liu2021swin} & \pzo28.3 & 1109\pzo & 4.5 & 3345 & 81.3 &  69.5 \\
    Swin-B~\cite{liu2021swin} & \pzo87.8  & 532 & 15.4 & 4695 & 83.5 &   \_ \\
    PiT-S~\cite{Heo2021RethinkingSD} & \pzo23.5 & 1809\pzo & 2.9 &  3293 & 80.9 & \_\\
    \rowcolor{blue!7}
     PiT-S -- \ours & \pzo23.5 & 1809\pzo & 2.9 &  3293 & 81.3 & 69.7 \\
     
    PiT-B~\cite{Heo2021RethinkingSD} & \pzo73.8 & 615 & 12.5  &  7564 & 82.0 & \_\\
    \rowcolor{blue!7}
     PiT-B -- \ours & \pzo73.8 & 615 & 12.5  &  7564 & 84.1 &74.1 \\
     \3pt]

 
    ViT-S ~\cite{Touvron2020TrainingDI} & \pzo22.0  & 1891\pzo & \tzo4.6 & 987 & 79.8 & 68.1  \\
    ViT-S -- Deit III~\cite{touvron2022deitIII} & \pzo22.0  & 1891\pzo & \tzo4.6 & 987 & 81.4 &  70.5 \\
        \rowcolor{blue!7}
    ViT-S -- \ours & \pzo22.0  & 1891\pzo & \tzo4.6 & 987 & 81.5 &  70.8 \\

    ViT-B~\cite{dosovitskiy2020image}  & \pzo86.6  & 831  & \dzo17.5 & 2078 & 77.9 &  --\\
    ViT-B~-- DeiT\cite{Touvron2020TrainingDI}  & \pzo86.6  & 831  & \dzo17.5 & 2078 & 81.8 & 71.5 \\
    ViT-B~-- DeiT/distilled & \pzo86.6  & 831  & \dzo17.5 & 2078 & 83.4 & 73.2 \\
    ViT-B -- Deit III~\cite{touvron2022deitIII}  & \pzo86.6  & 831  & \dzo17.5 & 2078 & 83.8 &  73.6\\
    
    \rowcolor{blue!7}
    ViT-B -- \ours & \pzo86.6  & 831  & \dzo17.5 & 2078 & 84.2 & 74.2 \\
    
    ViT-L -- Deit III~\cite{touvron2022deitIII}  & 304.4 & 277 & 61.6 & 3789 & 84.9 & 75.1    \\
    \rowcolor{blue!7}
    ViT-L -- \ours & 304.4 & 277 & 61.6 & 3789 & 85.3 &  75.5   \\
    ViT-H -- Deit III~\cite{touvron2022deitIII}  & 632.1 & 112 & 167.4 & 6984 & 85.2 & 75.9     \\
   
    

    \rowcolor{blue!7}
    ViT-H -- \ours  & 632.1 & 112 & 167.4 & 6984 & 85.7 &  76.6 \\
    


    \bottomrule
    \end{tabular}}
    \vspace{-0.5em}
  \caption{
\textbf{Classification with Imagenet1k training.} 
We compare with models trained on Imagenet-1k only at resolution  without self-supervised pre-training (see supp. material for a comparison).
We report Top-1 accuracy on Imagenet-1k-val and Imagenet-v2 with different measures of complexity: throughput, FLOPs, number of parameters and peak memory usage. 
The throughput and peak memory are measured on a single V100-32GB GPU with batch size fixed to 256 and mixed precision. 
\label{tab:mainres}}
\end{table}


\mypar{Imagenet-21k} In Table~\ref{tab:mainres_22k} we compare ViT models pre-trained with \ours on Imagenet-21k and finetuned with \ours on Imagenet-1k with other architectures and our baseline DeiT-III.
Our results with vanilla ViT outperform the baseline and are competitive with  recent architectures. 

\begin{table}
\centering
\scalebox{0.66}{
    \begin{tabular}{@{\ }l@{}c@{\ \ }c@{\ \ \ }r@{\ \ \ }r|cc@{\ }}
        \toprule
        Architecture        & nb params & throughput & FLOPs & peak mem & \multicolumn{2}{c}{top1 acc.}  \\
                      & () & (im/s) & () & (MB)\ \ \ \  & val  & v2 \3pt]

EfficientNetV2-M~\cite{Tan2021EfficientNetV2SM} & \pzo54.1 & 312 & 25.0 & 7127 & 86.2 & 75.9 \\
EfficientNetV2-L~\cite{Tan2021EfficientNetV2SM} & 118.5 & 179 & 53.0 & 9540 & 86.8 & 76.9 \\
    \-0.9em]  

\rowcolor{blue!7}
	 RegnetY16GF -- \ours \quad~   & \pzo83.6  & 714 & \dzo16.0 & 5204   & 84.2  & 74.7 \\

    \-0.9em]  

    

\multicolumn{7}{c}{\textbf{Transformers variations}} \\ [3pt]
     Swin-B~\cite{liu2021swin} & \pzo87.8 & 532 & 15.4 & 4695 &  85.2 & 74.6 \\
\rowcolor{blue!7}
     Swin-B -- \ours  & \pzo87.8 & 532 & 15.4 & 4695 & 86.2 &  77.2  \\
     
     Swin-L~\cite{liu2021swin} & 196.5 & 337 & 34.5 & 7350 &  86.3 & 76.3 \\
\rowcolor{blue!7}
     Swin-L -- \ours  & 196.5 & 337 & 34.5 & 7350 &  87.1 &  78.1  \\
         \-0.9em]  
\rowcolor{blue!7}
         XCiT-S12 -- \ours~\cite{el2021xcit}              &  \pzo26.2    &  1373 &  4.9 & 1330   &  \cellcolor{blue!7}84.2                   &  74.9 \\
\rowcolor{blue!7}
         XCiT-M24 -- \ours~\cite{el2021xcit}              &  \pzo84.4    & 553  &  16.2  & 2010    &  \cellcolor{blue!7}86.5                &  78.0 \\
\rowcolor{blue!7}
         XCiT-L24 -- \ours~\cite{el2021xcit}              &  189.0       & 334  &  36.1  & 3315   &  \cellcolor{blue!7}87.2                 &  77.8 \\
    \-0.7em]

            PatchConvNet-B60 & 140.6 & 1258  & 48.1  & 48.6 \\

             PatchConvNet-B120  & 229.8 & 1550  & 49.4  & 50.3\\
             MAE ViT-B  & 127.7 & 1283 & 48.1 &  \_ \\
             Swin-B   & 121.0 & 1188  & 48.1 & 49.7 \\
            ViT-B -- DeiT-III  & 127.7 & 1283 & 49.3 &  50.2\\
            \rowcolor{blue!7}
            ViT-B -- \ours & 127.7 & 1283 & 49.3 &  49.9\\
            ViT-L -- DeiT-III  & 353.6 & 2231 & 51.5 & 52.0  \\    
            \rowcolor{blue!7}
            ViT-L -- \ours & 353.6 & 2231 & \textbf{52.5} & \textbf{53.1}  \\  
            \multicolumn{5}{c}{}\-0.7em]

            
            PatchConvNet--B60  & 140.6 & 1258  & 50.5  & 51.1 \\

            Swin-B ()  & 121.0 & 1841  & 50.0 & 51.6 \\
            ViT-B -- DeiT-III   & 127.7 & 1283 & 53.4 & 54.1\\
            \rowcolor{blue!7}
            ViT-B -- cosub  & 127.7 & 1283 & 53.7  & 54.7 \\
            PatchConvNet-L120  & 383.7 & 2086  &  52.2 & 52.9   \\
            Swin-L ()  & 234.0 & 3230  & \_ & 53.5 \\
            ViT-L -- DeiT-III   & 353.6 & 2231 &  54.6 & 55.6 \\
            \rowcolor{blue!7}
            ViT-L -- cosub  & 353.6 & 2231 & \textbf{55.7} & \textbf{56.3}  \\
        \bottomrule     
        \end{tabular}
        } 
            \vspace{-0.5em}
          \caption{\textbf{ADE20k semantic segmentation} performance using UperNet \cite{xiao2018unified},  in comparable setting as prior works~\cite{Dong2021CSWinTA,el2021xcit,liu2021swin}. 
          All models are pre-trained on Imagenet-1k,  except bottom models identified with , which are pre-trained on Imagenet-21k. By default the finetuning resolution on ADE20k is  except when mentioned otherwise (for Swin).
       \label{tab:sem_seg}}
\end{table}

\mypar{Transfer learning}
We now measure how the performance improvements observed with \ours  translate to other classification problems. For this purpose, we performed transfer learning on the six different datasets used in DeiT-III. 
Our results are reported Table~\ref{tab:sota_tl}. 
Our pre-trained and finetuned models with \ours generally improve the baseline. The gains are overall more significant on the more challenging datasets like iNaturalist 2018 and iNaturalist 2019.



\begin{table}

    \centering
    \scalebox{0.74}{
    \begin{tabular}{l@{}c@{\ \ }c@{\ \ }cccc}
    \toprule
    Model    & Cifar-10 & Cifar-100  & Flowers & Cars & iNat-18 & iNat-19  \\
    \midrule                                                                          

    ViT-S -- DeiT-III  & 98.9 & 90.6 & 96.4 & 89.9 & 67.1 & 72.7 \\
    ViT-B -- DeiT-III  & 99.3 & 92.5 & 98.6 & 93.4 & 73.6 & 78.0\\ 
    ViT-L -- DeiT-III  & 99.3 & 93.4 & \textbf{98.9} & \textbf{94.5} & 75.6 & 79.3 \\ 
    \rowcolor{blue!7}
    ViT-S -- \ours & 99.1 & 91.7 & 97.4 & 93.0 & 70.1 & 75.6 \\
    \rowcolor{blue!7}    
    ViT-B -- \ours & 99.1 & 92.6 & 98.4 & 93.5 & 74.1 & 78.1\\ 
    \rowcolor{blue!7}
    ViT-L -- \ours & \textbf{99.4} & \textbf{93.5} & 98.8 & \textbf{94.5} & \textbf{76.2} & \textbf{80.1}  \\
    \bottomrule
    \end{tabular}}
    \vspace{-0.7em}
        \caption{
        ViT models pre-trained with \ours or DeiT-III on Imagenet-1k and finetuned on six different target datasets. 
        We note that for small datasets (CIFAR, Flowers, and Cars) our approach is useful for small models but neutral for larger models.
        The gains are more significant when transferring to the larger iNaturalist-18 and iNaturalist-19 datasets. 
    \label{tab:sota_tl}}
    \vspace{-0.57em}
\end{table} 