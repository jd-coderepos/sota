\documentclass[runningheads]{llncs}
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{multicol}
\usepackage{times}
\usepackage{epsfig}
\usepackage{float}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{lipsum}
\usepackage{dirtytalk}
\usepackage{algpseudocode}
\usepackage{adjustbox}
\usepackage{multirow}
\newcommand*\rot{\rotatebox{0}}
\newcommand*\ang{\rotatebox{50}}
\newcommand*\six{\rotatebox{60}}
\begin{document}
\title{Local Neighborhood Features for 3D Classification}


\author{Shivanand Venkanna Sheshappanavar\orcidID{0000-0003-4039-2910} \and
Chandra Kambhamettu\orcidID{0000-0001-5306-3994}}
\institute{University of Delaware, Video/Image Modeling and Synthesis (VIMS) Lab,\\ 212 Smith Hall, 18 Amstel Ave, Newark, DE 19711, USA\\
\email{ssheshap@udel.edu, chandrak@udel.edu}\\
\url{http://vims.cis.udel.edu/}}
\maketitle              \begin{abstract}
With advances in deep learning model training strategies, the training of Point cloud classification methods is significantly improving. For example, PointNeXt, which adopts prominent training techniques and InvResNet layers into PointNet++, achieves over 7\% improvement on the real-world ScanObjectNN dataset. However, most of these models use point coordinates features of neighborhood points mapped to higher dimensional space while ignoring the neighborhood point features computed before feeding to the network layers. In this paper, we revisit the PointNeXt model to study the usage and benefit of such neighborhood point features. We train and evaluate PointNeXt on ModelNet40 (synthetic), ScanObjectNN (real-world), and a recent large-scale, real-world grocery dataset, i.e., 3DGrocery100. In addition, we provide an additional inference strategy of weight averaging the top two checkpoints of PointNeXt to improve classification accuracy. Together with the abovementioned ideas, we gain \textbf{0.5\%}, \textbf{1\%}, \textbf{4.8\%}, \textbf{3.4\%}, and \textbf{1.6\%} overall accuracy on the PointNeXt model with real-world datasets, ScanObjectNN (hardest variant), 3DGrocery100's Apple10, Fruits, Vegetables, and Packages subsets, respectively. We also achieve a comparable~\textbf{0.2\%} accuracy gain on ModelNet40. 

\keywords{Point Cloud  \and 3D Object Classification \and Local Features.}
\end{abstract}
\section{Introduction}
3D Computer Vision is a vibrant research domain with broad applications in augmented/virtual reality and autonomous-driving vehicles. Over the past five years, the application of deep neural networks to 3D computer vision, especially for 3D point cloud processing, has progressed tremendously. This progress has led to state-of-the-art results on several computer vision tasks, especially 3D Object Classification, 3D Semantic Segmentation, 3D Scene Understanding, and 3D Shape retrieval. \hfill\break
\indent Point cloud Object Classification has gained traction since the pioneering work of PointNet~\cite{qi2017pointnet} that processed raw point sets through Multi-Layer Perceptrons (MLPs). However, PointNet, while aggregating features at the global level using max-pooling operation, lost valuable local geometric information. Overcoming this limitation, PointNet++~\cite{qi2017pointnet++} employed ball querying and k-Nearest Neighbor (k-NN) querying to query local neighborhoods to extract local semantic information. 

\indent Although, PointNet++~\cite{qi2017pointnet++} was effective in capturing local geometric information, it still lost contextual information due to the max-pooling operation. Several methods~\cite{qi2017pointnet++,liu2019relation,hua2018pointwise,duan2019structural,liu2019densepoint,xu2020geometry,lan2019modeling} since PointNet++ have used raw  point coordinates as input for their networks. The farthest points (FPS) are sampled from the input points, and at each FPS point, neighborhoods are queried (using a fixed radius ball or k-Nearest Neighbor (kNN) querying) and grouped into neighborhood points. From the queried neighborhood points, subtracting the anchor FPS point gives directional vectors. Then, these directional vectors are mapped to higher dimensions in each set abstraction layer to get local features. Once the local features are obtained, the original neighborhood points, the directional vectors, and the distance computed (in the case of ball querying) are discarded. For example, the most recent model PointNeXt builds upon the PointNet++ model uses ball querying for neighborhood querying using the distance from the anchor point to the neighborhood point to check if the point is within a ball of radius . PointNeXt computes the directional vectors from the neighborhood points, normalizes the vectors using the radius, and encodes them into local neighborhood features but does not use the distance and the directional vectors as additional local features. In this paper, we use the radius-normalized distance and directional vectors as additional local neighborhood features with minimal additional memory or computational costs. The key contributions to our work are as follows:

\begin{itemize}
\item We use radius -normalized neighborhood point distance as an additional neighborhood feature to improve the classification accuracy.
\item We show neighborhood-level directional vectors as additional features benefits PointNeXt~\cite{PointNeXt}.
\item We demonstrate that averaging the weights of two best model checkpoints (models saved in the same training session) benefits inference.
\item We evaluate our combined approaches on one synthetic, i.e., ModelNet40~\cite{wu20153d} and two real-world datasets, i.e., ScanObjectNN~\cite{uy2019revisiting}, and 3DGrocery100. 
\end{itemize}

\section{Related Works}

PointNet++~\cite{qi2017pointnet++} is composed of Set Abstraction (SA) Layers, which are further composed of three layers. A Farthest Point Sampling (FPS) layer to sample a subset of points from the input point cloud. A grouping layer to partition the input point cloud into common hierarchical structures. Moreover, a pointnet layer is to learn contextual representation from the grouped points. 

Using Kernel Correlation, KCNet~\cite{shen2018mining} mines local points and measures point affinity with similar geometric structures. From the surface deformations, different convolution operations~\cite{bronstein2017geometric} capture different levels of geometric information. Using the EdgeConv technique, Dynamic Graph Convolutional Neural Networks (DGCNNs)~\cite{wang2019dynamic} capture local geometric features. 
PointGrid~\cite{le2018pointgrid} proposed an integrated point and grid hybrid 3D CNN model to better represent the local geometry. DensePoint~\cite{liu2019densepoint} recursively concatenates features from MLPs to learn sufficient contextual semantic information. 

Relation Shape Convolutional Neural Network (RS-CNN)~\cite{liu2019relation} proposed a relation-shape convolution to explicitly encode the geometric relation of points for better awareness of the underlying shape. Although RS-CNN uses the Euclidean distance as one of the relations, it does not normalize the distance with the ball radius. The key difference between our approach and RS-CNN is our approach normalizes the distance (between neighborhood point to its anchor point) with the radius . While PointNeXt only normalizes directional vectors, it discards the vectors after mapping them to higher dimensions. In our approach, we use these normalized directional vectors as neighborhood features along with the radius normalized distance.

\begin{figure}[t]
\begin{center}
\includegraphics[width=0.8\linewidth]{Fig1.pdf}
\end{center}
  \caption{Airplane: \textcolor{green}{Green} points - input point cloud, \textcolor{red}{Red} point - anchor point for a neighborhood obtained from farthest point sampling step, \textcolor{blue}{Blue} points - points queried from the neighborhood of anchor point, \textcolor{red}{Red} Vectors - Directional Vectors from the anchor point to the neighbor points. The magnitude of each directional vector represents the distance of the neighbor point to its anchor point. The direction of these vectors is from the anchor point to the respective neighbor point.}
\label{Fig1}
\end{figure}

\section{Method}
In this section, we elaborate on the usage of distance and directional vectors of neighborhood points already computed at the grouping stage PointNeXt method that follows the grouping module of PointNet++~\cite{qi2017pointnet++}.

\subsection{Neighborhood point distance}
With the inception of PointNet++~\cite{qi2017pointnet++}, which introduced neighborhood querying using a fixed radius ball (as shown in figure~\ref{Fig1}) or k-nearest neighbor(kNN), several methods have continued to take advantage of these querying techniques. In the case of ball-queried neighborhood points, the distance between the anchor point and a neighborhood point is computed to determine if the given point is within the ball of radius . i.e., a point   is selected from the input point set , if it lies within the ball of radius  centered at (, , ) (obtained as  using the farthest point sampling method on the input  set). Euclidean distance  of the point from the anchor point/center of the ball is calculated, and the distance is normalized with the radius  as shown in equation \ref{first_equation}. When this radius normalized distance  is less than 1, the point is within the ball, if the distance is equal to 1, then the point is on the ball, and if the distance is greater than 1, then the point is outside the ball. A point with distance  less than 1 is selected. We save this distance and use it as an additional feature at the neighborhood level.



\subsection{Directional Vectors as neighborhood features}

After querying, the neighborhood points are centered at the anchor point by subtracting the anchor point from the neighborhood points. By doing so, the directional vectors () at the grouping level are computed as shown in Fig~\ref{Fig2} (in models that adopted the grouping layer of PointNet++~\cite{qi2017pointnet++}).  PointNeXt~\cite{PointNeXt} normalizes directional vectors using the radius  as shown in equation~\ref{second_equation}. These radius normalized directional vectors are mapped to relatively higher dimensions but are not directly included as features while hierarchically learning the discriminative features. Instead, we include these directional vectors as neighbor-level features by concatenating these vectors to the relatively-higher dimensional features in a given neighborhood. 



\begin{figure}[ht]
\centering
\begin{tabular}{ll}

\includegraphics[scale=0.25]{Fig2b.pdf} &
\includegraphics[scale=0.25]{Fig2c.pdf} \end{tabular}
\caption{Left: Input point cloud (\textcolor{green}{green} points), Farthest Point Sampled points (\textcolor{red}{red} points), Right: Directional vectors at each of the \textcolor{red}{red} points.}
\label{Fig2}
\end{figure}

\subsection{Weights-averaging of checkpoints}
Several methods save the model during training at every increment in the overall or mean accuracy and only use the best checkpoint for inference. We observe the best results of the PointNeXt model on the ScanObjectNN dataset around 190 to 210 epochs in a training cycle of 250. We noticed several epochs past the 210th epoch, for example, achieved comparable results to the best evaluation at the 210th epoch. Following the idea of weights averaging from model soups~\cite{wortsman2022model}, we take the best checkpoints from the training session, average the weights and use the weight-averaged checkpoint for inference. Weight averaging benefited in stabilizing the weights and greatly improved the test accuracy. 



\section{Experiments}
We evaluate the impact of local neighborhood features i.e., radius normalized distance and directional vectors, as additional local features in the PointNeXt model using the benchmark synthetic dataset ModelNet40~\cite{wu20153d}. ModelNet40 dataset has a total of 12,311 shapes distributed across 40 classes. We use the dataset's official split of 9843 objects for training and 2468 for testing. Additionally, we evaluate the PointNeXt model using two robust real-world datasets, i.e., the ScanObjectNN~\cite{uy2019revisiting} and 3DGrocery100. 

ScanObjectNN contains 14,510 objects obtained by perturbing the original 2902 objects. ScanObjectNN is derived from 700 unique scenes from two popular scene meshes datasets; SceneNN~\cite{hua2016scenenn} with 100 objects and ScanNet~\cite{dai2017scannet} with 1513 objects. Although the ScanObjectNN dataset has six variants, we use the hardest variant represented by PB\_T50\_RS (following PointNeXT and other recent models).  PB\_T50\_RS variant constitutes perturbed objects with 50\% translation and involves both rotation and scaling. 

3DGrocery100 dataset at a very high level contains three categories, i.e., Fruits, Vegetables, and Packages with 34, 28, and 38 classes, respectively. The fruits, vegetables, and packages have 37,587, 27,707, and 22,604 point clouds, respectively. The Fruits subset named Apple10 contains ten apple classes and 24 non-apple fruit classes. We consider each of the three categories, i.e., Fruits (non-apples), Vegetables, and Packages, as subset datasets. We train the PointNeXT model with and without our proposed additional local neighborhood features using the four subset datasets of 3DGrocery100. Finally, we discuss and analyze our experimental results on the four subsets (Apple10, Fruits, Vegetables, and Packages). We evaluate the impact of the local neighborhood features in improving the classification results on the four subsets of the grocery dataset on the PointNeXt model. 

\section{Experimental Results}
In this section, we show the benefits of using the radius -normalized distance and directional vectors as additional neighborhood features in the PointNeXt model when trained and tested on ScanObjectNN, ModelNet40, and 3DGrocery100 datasets.

\subsection{Classification of ScanObjectNN}

Table~\ref{tab1} shows the impact of radius normalized distance and directional vector concatenation to local neighborhood features in the PointNeXt model trained on the ScanObjectNN dataset. In addition, we show inference results of weights averaging the best two checkpoints.
\vspace{-1em}
\begin{table}
\centering
\caption{Additive study of sequentially adding our strategies for classification on ScanObjectNN.}\label{tab1}
\begin{tabular}{|l|c|c|c|c|c|}
\hline
Improvements &  Overall Accuracy (\%)&  & Mean Average Accuracy (\%)&\\
\hline
PointNeXt~\cite{PointNeXt} & 87.7  0.4 & - &85.8  0.6& - \\
+ distance & 87.9  0.4 & +0.2& 86.4  0.3& +0.6\\
+ directional vectors & 88.2  0.2 & +0.3& 86.9  0.2& +0.5 \\
+ best two average & 88.4  0.2 & +0.2&87.1  0.3& +0.2 \\ 
\hline
Overall Best & 88.6 & \textbf{0.5}  & 87.4 & \textbf{1.0} \\
\hline
\end{tabular}
\end{table}
\vspace{-1em}
\subsection{Classification of ModelNet40}
Table~\ref{tab2} shows a comparable overall improvement in the classification accuracy of the PointNeXt model trained on the ModelNet40 dataset.
\begin{table}
\centering
\caption{ 3D Classification accuracy of PointNeXt model with our approach on ModelNet40.}\label{tab2}
\begin{tabular}{|l|c|c|c|c|c|}
\hline
Improvements &  Overall Accuracy (\%)&  & Mean Average Accuracy (\%)&\\
\hline
PointNeXt~\cite{PointNeXt} & 93.2  0.1 & - &90.8  0.2& - \\
PointNeXt + (ours) & 93.4  0.1 & +0.2 & 90.9  0.1 & +0.1\\
\hline
\end{tabular}
\end{table}
\vspace{-1em}
\subsection{Classification of 3D Grocery}
Table~\ref{tab3} shows the impact of radius normalized distance and directional vector concatenation to neighborhood features in the PointNeXt model trained on all four subsets of the 3DGroceyr100 dataset (without colors).

\begin{table}[!h]\centering
  \caption{3D point cloud classification results on the PointNeXt model using four subsets (without colors) of the 3DGrocery100 dataset.}
  \begin{adjustbox}{width=0.9\linewidth}
  \begin{tabular}{|c|c|c|c|c|c|c|c|}
\toprule
\rot{\textbf{Subsets}}&\rot{\#Classes} &\rot{\#PCs}&\rot{Train}&\rot{Test}&\rot{PointNeXt~\cite{PointNeXt}}&PointNeXt+(ours)&\\
\midrule
\textbf{Apple10}&10&12905&9706&3199&21.6&22.6&\textbf{+1.0}\\

\textbf{Fruits}&24&24682&18406&6276&40.6&45.4&\textbf{+4.8}\\

\textbf{Vegetables}&28&27707&20720&6987&48.4&51.8&\textbf{+3.4}\\

\textbf{Packages}&38&22604&17214&5390&81.4&83.0&\textbf{+1.6}\\
\bottomrule
  \end{tabular}
  \end{adjustbox}
  \label{tab3}
\end{table}




\section{Ablation Study}

\subsection{Distance vs. -normalized distance}
From Equation~\ref{first_equation}, we know the distance of the neighborhood points to the anchor points is normalized by the radius  used to query the points. While existing methods only compute the distance to check whether a point is within the ball for querying, here we provide experimental results to show that the normalized distance gives better mean accuracy, as shown in Table~\ref{tab4}.
\vspace{-1em}
\begin{table}[!h]
\centering
\caption{Comparison of distance vs. radius normalized distance as an additional neighborhood feature. Model Used: PointNeXt~\cite{PointNeXt} and Dataset Used: ScanObjectNN.}\label{tab4}
\begin{tabular}{|l|c|c|}
\hline
Features &  OA (\%) & mAcc (\%) \\
\hline
distance & 88.3 & 86.5\\
-normalized distance & 88.3 & 86.7\\
\hline
\end{tabular}
\end{table}
\vspace{-1em}
\subsection{Weight averaging (same training session)}
During a training session, we save the checkpoints at the top 15 best results. Then, we do a systematic study of averaging the weights of these checkpoints during inference, as shown in table~\ref{tab5}. From our study, we understand that averaging the weights of top-2 checkpoints further improves classification accuracy.
\vspace{-1em}
\begin{table}[!h]
\centering
\caption{ Ablation study on weight averaging of checkpoints for classification on ScanObjectNN.}\label{tab5}
\begin{tabular}{|l|c|c|c|c|}
\hline
Improvements &  Overall Accuracy (\%)&  & Mean Average Accuracy (\%)&\\
\hline
PointNeXt & 88.1 & - &86.4& - \\
\hline
\multicolumn{5}{c}{Weight averaging (of checkpoints) from same training session}\\
\hline
+ top-2 & 88.4&\textbf{+0.3}&87.1&\textbf{+0.7} \\
+ top-3 & 88.3&+0.2&87.1&+0.7\\
+ top-5& 88.2&+0.1&86.3&-0.1\\ 
+ top-10& 88.0&-0.1&86.4&+0.0\\ 
+ top-15& 87.9&-0.2&86.3&-0.1\\ 
\hline
\end{tabular}
\end{table}
\vspace{-1em}
\section{Conclusion}
In this paper, we demonstrate the use of local neighborhood features as additional local features for point cloud classification. We also introduce an inference strategy to benefit the overall results. We demonstrate the benefits of the features and inference strategy on the state-of-the-art PointNeXt method. We evaluate the PointNeXt model on three datasets, ModelNet40, ScanObjectNN, and 3DGrocery100. The neighborhood features significantly improved the classification accuracy, particularly in the case of real-world datasets. For the PointNeXt model, the improvements of \textbf{0.5\%}, \textbf{1\%}, \textbf{4.8\%}, \textbf{3.4\%}, \textbf{1.6\%}, and \textbf{0.2\%} in the overall accuracy on the real-world ScanObjectNN (hardest variant), 3DGrocery100 dataset's subsets Apple10, Fruits, Vegetables, Packages, and synthetic dataset ModelNet40, respectively.

\begin{thebibliography}{18}

\bibitem{qi2017pointnet}
Qi, C. R., Su, H., Mo, K., \& Guibas, L. J. (2017). Pointnet: Deep learning on point sets for 3d classification and segmentation. \textit{In Proceedings of the IEEE conference on computer vision and pattern recognition} (pp. 652-660).


\bibitem{qi2017pointnet++}
Qi, C. R., Yi, L., Su, H., \& Guibas, L. J. (2017). Pointnet++: Deep hierarchical feature learning on point sets in a metric space. \textit{In Advances in neural information processing systems} (pp. 5099-5108).

\bibitem{liu2019relation}
Liu, Y., Fan, B., Xiang, S., \& Pan, C. (2019). Relation-shape convolutional neural network for point cloud analysis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 8895-8904).

\bibitem{hua2018pointwise}
Hua, B. S., Tran, M. K., \& Yeung, S. K. (2018). Pointwise convolutional neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 984-993).

\bibitem{shen2018mining}
Shen, Y., Feng, C., Yang, Y., \& Tian, D. (2018). Mining point cloud local structures by kernel correlation and graph pooling. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 4548-4557).

\bibitem{bronstein2017geometric}
Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., \& Vandergheynst, P. (2017). Geometric deep learning: going beyond euclidean data. IEEE Signal Processing Magazine, 34(4), 18-42.

\bibitem{le2018pointgrid}
Le, T., \& Duan, Y. (2018). Pointgrid: A deep network for 3d shape understanding. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 9204-9214).

\bibitem{duan2019structural}
Duan, Y., Zheng, Y., Lu, J., Zhou, J., \& Tian, Q. (2019). Structural relational reasoning of point clouds. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 949-958).

\bibitem{liu2019densepoint}
Liu, Y., Fan, B., Meng, G., Lu, J., Xiang, S., \& Pan, C. (2019). Densepoint: Learning densely contextual representation for efficient point cloud processing. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 5239-5248).

\bibitem{xu2020geometry}
Xu, M., Zhou, Z., \& Qiao, Y. (2020, April). Geometry sharing network for 3d point cloud classification and segmentation. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 34, No. 07, pp. 12500-12507).

\bibitem{lan2019modeling}
Lan, S., Yu, R., Yu, G., \& Davis, L. S. (2019). Modeling local geometric structure of 3d point clouds using geo-cnn. In Proceedings of the IEEE/cvf conference on computer vision and pattern recognition (pp. 998-1008).

\bibitem{wortsman2022model}
Wortsman, M., Ilharco, G., Gadre, S. Y., Roelofs, R., Gontijo-Lopes, R., Morcos, A. S., ... \& Schmidt, L. (2022, June). Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In International Conference on Machine Learning (pp. 23965-23998). PMLR.

\bibitem{wu20153d}
Wu, Z., Song, S., Khosla, A., Yu, F., Zhang, L., Tang, X., \& Xiao, J. (2015). 3d shape nets: A deep representation for volumetric shapes. \textit{In Proceedings of the IEEE conference on computer vision and pattern recognition} (pp. 1912-1920).

\bibitem{uy2019revisiting}
Uy, M. A., Pham, Q. H., Hua, B. S., Nguyen, T., \& Yeung, S. K. (2019). Revisiting point cloud classification: A new benchmark dataset and classification model on real-world data. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 1588-1597).

\bibitem{hua2016scenenn}
Hua, B. S., Pham, Q. H., Nguyen, D. T., Tran, M. K., Yu, L. F., \& Yeung, S. K. (2016, October). Scenenn: A scene meshes dataset with annotations. In 2016 fourth international conference on 3D vision (3DV) (pp. 92-101). Ieee.

\bibitem{dai2017scannet}
Dai, A., Chang, A. X., Savva, M., Halber, M., Funkhouser, T., \& Nie√üner, M. (2017). Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 5828-5839).

\bibitem{wang2019dynamic}
Wang, Y., Sun, Y., Liu, Z., Sarma, S. E., Bronstein, M. M., \& Solomon, J. M. (2019). Dynamic graph cnn for learning on point clouds. \textit{Acm Transactions On Graphics} (tog), 38(5), 1-12.





\bibitem{PointNeXt} Qian, Guocheng, et al. "PointNeXt: Revisiting PointNet++ with Improved Training and Scaling Strategies." \textit{arXiv preprint} arXiv:2206.04670 (2022).


\end{thebibliography}
\end{document}
