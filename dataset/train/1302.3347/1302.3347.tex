\documentclass[11pt,onecolumn,final]{article} \usepackage{a4}
\usepackage{fullpage}
\usepackage[latin1]{inputenc}
\usepackage{a4}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage[ruled,longend]{algorithm}
\usepackage{subfigure}
\usepackage{stmaryrd}
\usepackage{fancybox}
\usepackage{url}
\usepackage{cite}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{enumerate}
\usepackage{color}

\renewcommand{\baselinestretch}{1.00} 

\setlength{\parindent}{1em}

\setlength{\parskip}{1ex}

\pagestyle{plain}

\pagenumbering{arabic}


\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\polylog}{polylg}

\newcommand{\SA}[0]{\mathsf{SA}} \newcommand{\ST}[0]{\mathcal{ST}} \newcommand{\SF}[1]{T^{#1}} \newcommand{\pred}{\textsc{Pred}} 

\theoremstyle{plain}
\newtheorem{definition}{Definition}
\newtheorem{lemma}[definition]{Lemma}
\newtheorem{prop}[definition]{Proposition}
\newtheorem{theorem}[definition]{Theorem}
\newtheorem{observation}[definition]{Observation}
\newtheorem{corollary}[definition]{Corollary}
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem{problem}{Problem}\newtheorem{example}{Example}

\renewcommand{\qedsymbol}{}
\renewcommand{\proofname}{\bf Proof}



\definecolor{myRed}{rgb}{0.9,0.3,0.1}
\newcommand{\pawel}[1]{\colorbox{myRed}{Pawel: #1}}
\definecolor{myYellow}{rgb}{0.99,0.99,0.05}
\newcommand{\jo}[1]{\colorbox{myYellow}{Jo: #1}}


\title{Alphabet-Dependent String Searching with \\Wexponential Search Trees}

\author{Johannes Fischer and Pawe{\l} Gawrychowski}

\begin{document}
\maketitle

\begin{abstract}
It is widely assumed that  is the best one can do for finding a pattern of length  in a compacted trie storing strings over an alphabet of size , if one insists on linear-size data structures and deterministic worst-case running times [Cole et al., ICALP'06]. In this article, we first show that a rather straightforward combination of well-known ideas yields  deterministic worst-case searching time for static tries.


Then we move on to dynamic tries, where we achieve a worst-case bound of  per query or update, which should again be compared to the previously known  deterministic worst-case bounds [Cole et al., ICALP'06], and to the alphabet \emph{in}dependent  deterministic worst-case bounds [Andersson and Thorup, SODA'01], where  is the number of nodes in the trie. The basis of our update procedure is a weighted variant of exponential search trees which, while simple, might be of independent interest.

As one particular application, the above bounds (static and dynamic) apply to suffix trees. There, an update corresponds to pre- or appending a letter to the text, and an additional goal is to do the updates quicker than rematching entire suffixes. We show how to do this in  time, which improves the previously known  bound [Amir et al., SPIRE'05].
\end{abstract}

\section{Introduction}
Text indexing is a fundamental problem in computer science. It requires storing a text of length , composed of letters from an alphabet of size , such that subsequent pattern matching queries can be answered quickly. Typical such pattern matching queries are (1) \emph{existential} queries (deciding whether or not the pattern occurs in the text), (2) \emph{counting} queries (determining the number of occurrences), and (3) \emph{enumeration} queries (listing all positions where the pattern occurs). The text is either static, or can be modified by appending new letters.

Well-known static text indexes are \emph{suffix trees} \cite{mccreight76space} and \emph{suffix arrays} \cite{manber93suffix}. The former admit, in their plain form,  existential and counting queries for a pattern of length , while the latter achieve  time with the help of two additional arrays of size  storing the lengths of longest common prefixes needed during the binary search. With perfect hashing \cite{fredman84storing}, suffix trees achieve  time, but then the  preprocessing time is only in \emph{expectation}. Enjoying the best of both worlds, the \emph{suffix tray} of Cole et al. \cite{cole06suffix} achieves  searching time, with a linear space data structure that can be constructed in  deterministic time for a static text. In the dynamic setting, suffix trees can be updated in amortized expected constant time, where the amortization comes from the need to locate the node which should be updated, and expectation from the hashing used to store outgoing edges. If we insist on getting worst-case time bounds, a recent result of Kopelowitz~\cite{Kopelot12indexing} allows updates in  worst-case (but still expected) time. The \emph{suffix trists} of Cole et al.~\cite{cole06suffix} achieve a deterministic bound 
of  for searching, with a linear space data structure that can be updated in  deterministic and worst-case time, where  is the time required to locate the edge of the suffix tree which should be split, and the best bound on  known so far for the general case is ~\cite{Amir05towards}. In all cases,  additional time is needed for enumerating the  occurrences.

In all of the tree based structures mentioned in the previous paragraph, the crucial point is how to implement the outgoing edges of the tree such that they can be searched efficiently for a given query character. This is the general setting of \emph{trie search}, and in fact, we can and do formulate our results in terms of tries, and view suffix trees as one particular application. In this setting, it is worth mentioning the result of Andersson and Thorup \cite{andersson07dynamic}, who show how to update or search the trie in  deterministic worst-case time, for  being the number of stored strings. In this article, however, we focus on alphabet-\emph{dependent} running times, as outlined next.

\subsection{Our Result and Outline}
Our main results are summarized in the following theorems. We work in the standard word RAM model, where it is assumed that the memory consists of -bit cells that can be manipulated using standard C-like operations in  time.

In tries, we support stronger forms of existential queries, namely prefix queries (deciding whether or not the search pattern is a prefix of one of the stored strings), and predecessor queries (returning the largest string stored that is no less than the query pattern):

\begin{theorem}
  \label{thm:main}
  A compacted trie storing  static strings over an alphabet of size  can be stored in  space (in addition to the stored strings themselves) such that subsequent prefix- or predecessor queries can be answered in  deterministic worst-case time, for patterns of length . This data structure can be constructed in deterministic  time, for .
\end{theorem}

We note that while for prefix queries the ``''-term would in principle not be necessary, for the supported predecessor queries it certainly is. As one application, we mention suffix trees \cite{weiner73linear}:

\begin{corollary}
  \label{cor:main}
  Given a static text of length  over an alphabet of size , we can build in  time a linear-space data structure such that subsequent on-line pattern matching queries can be answered in  deterministic time, for patterns of length .
\end{corollary}

In the dynamic setting we get the following result:

\begin{theorem}
\label{thm:main2}
  We can maintain a linear-size structure for a trie storing strings over an alphabet of size  under adding a new string of length  in deterministic worst-case time  so that subsequent on-line prefix- or predecessor queries can be answered in  deterministic time, for patterns of length .
\end{theorem}

This result can be formulated in terms of suffix trees, too:

\begin{corollary}
  \label{cor:main2}
  We can maintain a linear-size structure for a text over an alphabet of size  under prepending a new letter in deterministic worst-case time  so that on-line pattern matching concerning the current text can be answered in  deterministic worst-case time, for patterns of length .  is the cost of updating the suffix tree for a text  over an alphabet of size  after prepending a letter.
\end{corollary}



It was already noted that the currently best bound \cite{Amir05towards} for deterministic worst-case suffix tree oracles is . Though not being the main goal of this article, in the appendix we show a better suffix tree oracle, giving us truly better total running times:

\begin{theorem}
  \label{thm:main3}
There is a deterministic worst-case suffix tree oracle with .
\end{theorem}

\subsection{Technical Contributions}
Our main technical novelty is a weighted variant of exponential search trees \cite{andersson07dynamic}, which we term \emph{wexponential search trees}. The original exponential search tree achieves  search and update times for  elements over a universe of size . Our weighted variant generalizes this to , where  is the weight of the searched element, and  is the sum of all weights. The advantage of this is that in a sequence of  hierachical accesses to such data structures, where the old ``'' is always the new ``'', the sum telescopes to  instead of . While this general idea is pervasive in data structure, we are not aware of any previous application in the doubly-logarithmic setting.

\section{Preliminaries}
\label{sect:preliminaries}
This section introduces definitions and known results that form the base of new data structure.

\subsection{Suffix Arrays}
Let  be a text consisting of  characters drawn from an ordered alphabet  of size . The substring of  ranging from  to  is denoted by , for . The substring  is called the 'th \emph{suffix} of  and is denoted by .

The \emph{suffix array}  of  is a permutation of the integers in  such that  for all . In other words,  describes the lexicographic order of the suffixes. The suffix array can be built in linear time for ~\cite{kaerkkaeinen06linear}.

\subsection{Suffix Trees}
The suffix tree  is a compacted trie over all the suffixes of . By concatenating all edge labels on a root-to-node path, its nodes spell out substrings of . If we append a unique character \not\in\SigmaTT\\label{eqn:splitters}
|S| \leq 2\frac{2f(\ell+1)}{f(\ell)-f(\ell-1)} = O(2^{(\frac{3}{2})^{\ell+1}-(\frac{3}{2})^{\ell}})=O(2^{\frac{1}{2}(\frac{3}{2})^{\ell}})=O(f^{\frac{1}{2}}(\ell)) .

w'_{i}  &\in& [\sqrt{w_{i}},w_{i}] \label{eq:wprime}\\
W'_{i} &\leq& W_{i} \label{eq:Wprime}\\
W_{i+1} &\leq& w_{i}\label{eq:W}

\frac{\lg\lg\sigma}{\lg\lg\lg\sigma}\sum_{i} \lg\frac{\lg W'_{i}}{\lg w'_{i}}
\stackrel{\eqref{eq:wprime},\eqref{eq:Wprime}}{\le} \frac{\lg\lg\sigma}{\lg\lg\lg\sigma}(\lg\lg\sigma+\sum_{i}\lg\frac{\lg{W_{i}}}{\lg w_{i}})
\stackrel{\eqref{eq:W}}{=} \frac{\lg\lg\sigma}{\lg\lg\lg\sigma}(\lg\lg\sigma+\lg\frac{\lg{W_1}}{\lg w_k})
\le \frac{\lg^{2}\lg\sigma}{\lg\lg\lg\sigma}\ .
 O\left(\frac{f^{2}(\ell+2)}{f^{2}(\ell+1)} +  f(\ell+1) + \frac{f(\ell+1)}{f(\ell)} \sqrt{f(\ell)}\lg\lg f(\ell+1)\right) = O\left(\frac{f^{2}(\ell+2)}{f^{2}(\ell+1)} + f(\ell+1)\right). 

Hence we only need to ensure that , which is equivalent to , and then , hence by the choice of  we always have enough credits to amortize the update.

\subsection{Worst-Case Version}

In this section we develop the final worst-case efficient version of our dynamic compacted trie search structure.

The only non worst-case efficient part of the previous implementation is increasing the level of a root . Instead of traversing the tail and updating all the structures as soon as its level reaches , we will execute those operations incrementally over the next  insertions in the subtree rooted at , and relax the condition on the weight of a node of level  by saying that it shouldn't exceed . As selecting the tail is done incrementally, we redefine it to be the maximal sequence of nodes of weight at least  at the moment we started the process, which requires storing at each edge a timestamp denoting its creation time.

First of all, we must make sure that there is at most one promoting process per fragment. Even though we have already chosen the tail, future insertions might increase the weight of some additional nodes to more than . Nevertheless, as we execute the procedure over just  insertions, no node in the current fragment (or in one of the new fragments) which doesn't belong to the tail can reach the weight of  before we are done. Furthermore, there is some interaction between different fragments. While we are still promoting at , new nodes might be added to the corresponding fragment. Also, different children of a node might need to be refreshing it at overlapping periods of time.

We choose the speed of the simulation so that there is enough time to process a fragment consisting of  nodes over  insertions. When splitting the current fragment into more than one, we run a depth-first search to determine the new fragments. As soon as we reach a node, we set the link to its (new) fragment. Then when a new node appears, its parent is either already processed and hence has the correct link set, or will be seen later, and we will notice the new child then. The same reasoning works for inserting the new elements into the wexponential tree stored at the last node of the tail. Refreshing a node is  a quite different issue, though, as we must somehow deal with the problem that many children might need to refresh the same parent. Each node of level  stores a list of all its children of weight at least , where we simply append a child as soon as its weight becomes large enough. As a part of our simulation we run the refreshing process. In other words, each child of a node can be potentially running a deferred process refreshing its parent. The first step of such process is making a read-only copy of the current list. As this list only grows, instead of making a physical copy we can simply store the current last element, which can be done in constant time. Then we build a new static dictionary containing all elements on this read-only copy over the insertions in the subtree rooted at the child.  As soon as the construction finishes, we substitute the old dictionary in constant time by replacing one pointer. There is just additional detail: we should first check if we are really replacing an older version, i.e., we can simply look at the number of elements stored there. This is because it might have happened that there is a refreshing process which starts later, yet finishes earlier (as there are more insertions to the corresponding subtree). This ensures that when the weight of  reaches , the static dictionary stored a its parent surely contains its edge (and, potentially, also some more recent edges).

\section{Conclusions and Open Problems}
\label{sect:conclusions}
We gave data structures for improved deterministic worst-case searching and update times in suffix trees and compacted tries in general. Whereas previous data structures either ignored the alphabet size completely \cite{andersson07dynamic} or had at least a logarithmic dependency on the alphabet size, our new structures have only a doubly-logarithmic dependency on the alphabet size. The obvious open questions are if better running times are possible, or if non-trivial lower bounds exist?

\section*{Appendix}
\appendix
\section{Better suffix tree oracle}

In this section we show how to implement a suffix tree oracle with . At a high level, we follow the approach of Breslauer and Italiano~\cite{Breslauer11fringe}, which in turn is based on Weiner's algorithm~\cite{weiner73linear}. The main difficulty is that both methods are only efficient when the alphabet is of constant size, which is not our case, and some additional ideas are required. Nevertheless, we are able to achieve  by rather simple means. Then we decrease this bound even further by applying the structure of Kopelowitz~\cite{Kopelot12indexing}, which needs some minor tweaking as we are interested in a deterministic solution.

For each node  of the suffix tree we define its -link  if the suffix locus  exists. If  is a node, then the link is called a hard -link and points to this node, and otherwise a soft -link and points to the lower end of the corresponding edge. Hard links correspond to inverse suffix links. Assuming that we maintain the suffix tree using Weiner's algorithm, the following properties hold.

\begin{observation}
\begin{enumerate}[(a)]
\item\label{obs:monotone} If  is defined and  is an ancestor of , then  is defined, too. In other words, the sets of links are monotone when we traverse the tree starting from the root.
\item\label{obs:change} As soon as  becomes hard, it stays hard and cannot change. On the other hand, a soft link may change or become hard at some point.
\item\label{obs:reconstruct}  is defined if and only if there is a descendant  of  with the hard -link defined, and in fact a stronger statement holds:  is defined iff , where  is a hard -link, and  is the (unique) highest descendant of  with the hard -link defined. (Uniqueness follows because if two nodes have a hard -link defined, then their lowest common ancestor also has it.)
\end{enumerate}
\end{observation}

Breslauer and Italiano show that to implement a suffix tree oracle, one needs to store the suffix tree such that the following four operations can be performed:

\begin{enumerate}
\item given a letter  and a node , locate the lowest ancestor  of  with the -link defined, and retrieve its ,
\item make a soft link hard,
\item create or change a soft -link,
\item split an edge  of the suffix tree, and for each existing -link  (both soft and hard) create a new soft -link , where  is the new middle node and  is the lower end of the split edge.
\end{enumerate}

After prepending a single letter to the text a constant number of operations of each type is executed.
The most problematic is the last one, as
there might be many defined -links from . To overcome this, we will maintain the soft links just at some carefully chosen nodes. For the remaining nodes we will exploit (\ref{obs:reconstruct}) to recover the missing soft links whenever we need to access them. Furthermore, during the execution of the algorithm it might happen that we haven't yet updated some soft links. Nevertheless, we always update them in a top-bottom order, so (\ref{obs:monotone}) still holds, and all hard links are eagerly created as soon as they should appear. The details are as follows.

\subsection{Tree Decomposition}
We (again) divide the nodes into heavy and light, but this time we define the \emph{weight} of a node to be the number of leaves in its subtree plus the number of hard links defined there (which due to (\ref{obs:change}) can only increase). We choose parameter  and say that all nodes of weight at least  are heavy, all nodes of weight at most  leaves are light, and the remaining nodes can be either light or heavy.

Heavy nodes store \emph{all} Wlinks in a dynamic dictionary (implemented using Prop.~\ref{thm:predecessor3}), whereas light nodes store only the hard ones. Additionally, we maintain a structure which allows us to locate for a given heavy node  its lowest ancestor  with the -link defined (point (1) above). The details on how to update this structure will be given later; we first describe how it can be used. We will consider the two possible cases where the answer is a heavy node or when it is light.

\subsection{The Answer is a Heavy Node}
First we need a result on a special case of the marked ancestor problem.

\begin{lemma}[Fringe marked ancestor (FMA) structure~\cite{Breslauer11fringe}]
  \label{lem:fringe}
  We can maintain a tree  on  nodes which are either marked or unmarked under the following operations:
  \begin{enumerate}
  \item inserting a new node in the middle of an edge, which adopts the marked status of its parent,
  \item inserting a new leaf, which is initially unmarked,
  \item marking the root or a node whose parent is already marked,
  \item locating the lowest marked ancestor of a given node.
  \end{enumerate}
\end{lemma}

Due to the way marking is done the marked nodes form a connected subtree containing the root. Hence, the answer to queries on unmarked nodes will always lie on the \emph{fringe} of this subtree. All operations cost  time~\cite{Breslauer11fringe},
which is faster than in the general setting where a lower bound of  for at least one of the operations exists~\cite{alstrup98marked}

We store a single FMA structure over the suffix tree, where all \emph{heavy} nodes are marked. This allows us to locate for a given node  its lowest heavy ancestor  in  time. Because the heavy subtree can be quite large, we now need to do the following. Define the \emph{induced heavy subtree} as the tree that contains only the heavy leaves and branching heavy nodes. We store this tree explicitly, and couple all edges of the \emph{original} suffix tree with their corresponding edges in the \emph{induced} tree by bidirectional links that allow us to move back and forth between the two trees.

In this induced tree, for each letter  we keep a separate FMA structure, where a node is marked when its -link is defined.
Due to (\ref{obs:monotone}) the marked nodes form indeed a connected subtree containing the root, so Lemma \ref{lem:fringe} can be applied.
Additionally, for each edge of the induced tree we keep an array  storing a pointer to the lowest heavy node (of the \emph{original suffix tree}) that is coupled with the edge and that has the -link defined. Due to (\ref{obs:monotone}) this array gives us the only candidate on the edge of the induced tree.

All FMA structures, arrays, and pointers to edges are enough to locate for a given heavy node  its lowest ancestor  with the -link defined. First we check if  and  belong to the same edge of the induced tree. If not, we replace  by its lowest branching ancestor  by going to the top node on the edge of the induced tree. Then we locate its lowest branching ancestor  with the -link defined using the FMA structure. Then if we look at the edge outgoing from  and leading to the subtree  belongs to,  surely is there.
The edge can be located by retrieving the corresponding letter, and return . The total time taken by those operations is , where the second addend is due to the edge retrieval. Furthermore, the total size of the structure is linear, as we need  space per edge, and we keep  FMA structures, each on  nodes.

\subsection{The Answer is a Light Node}
For each maximal subtree consisting of light nodes we store a separate structure allowing us to quickly locate for a given node  in this subtree its lowest light ancestor  with the -link defined, and retrieve its . By (\ref{obs:reconstruct}), we only have to maintain all hard links as long as given  we can quickly locate its highest descendant  with the hard -link defined. Assume that we can maintain a preorder traversal of all nodes in the subtree. Then there is just one candidate for :
the first node after  in the traversal with the hard -link defined. Hence if we maintain for each  a separate predecessor structure where we put all nodes in the subtree with the hard -links defined, we just have to perform one predecessor query. In the simplest form, the structure could be any balanced search tree, giving us a  time bound if we observe that comparing any two elements there reduces to computing the lowest common ancestor, which can be done in worst-case constant case even on dynamic trees~\cite{cole05dynamic}.

We aim to achieve better bounds, though. For this we would like to implement the predecessor structure using Prop.~\ref{thm:predecessor3}, but in order to do so we need to assign integer labels consistent with the preorder traversal to the nodes. This is exactly the POLP problem considered by Kopelowitz~\cite{Kopelot12indexing}. 

\begin{lemma}[Predecessor search on dynamic subsets of an ordered dynamic list~\cite{Kopelot12indexing}]
There exists a data structure for a dynamic ordered list of size  and disjoint subsets such that:
\begin{enumerate}
\item order queries are answered in  worst-case time,
\item creating a new node after a given node takes  worst-case expected time,
\item adding an element into any subset takes  worst-case expected time,
\item predecessor search in any subset takes  worst-case expected time.
\end{enumerate}
\end{lemma}

We apply his method with the following modifications.

\begin{enumerate}
\item We aim to achieve deterministic time bounds, and hence need to replace -fast tries by the structure from Prop.~\ref{thm:predecessor3}. In order to do so, we must verify that the following is possible: given a structure storing a set , we can incrementally perform insertion into  over  steps so that we can perform predecessor queries over the original  while the insertion is not over yet. But this is easy if the structure is an exponential search tree, which is just a multiway search tree where the insertion is done in a bottom-up order. Whenever we replace a node  by , we store the pointer to the old  at  together with a timestamp indicating when we should start using the new  to answer predecessor queries. As there is just one insertion process per structure, we never need to store more than one previous version per node.
\item As the data structure of Prop.~\ref{thm:predecessor3} is of linear size, we do not need the bucketing as in the -fast tries, which simplifies the structure a little bit as there is no interference between splitting of buckets and chunks.
\item All sets stored in the structures should be disjoint to guarantee that whenever a label of some element changes, we need to update just a single element in one of them. For this we define our list to contain all pairs  such that  is a hard link in the natural lexicographical order, i.e., extending the preorder on the nodes.
\end{enumerate}

As a result, we get both queries and updates in deterministic  worst-case time, where by update we mean either splitting an edge or creating a hard link. This is of course assuming that the weight of each light subtree stays at most . 

\subsection{Updating the Structures}
As soon as the weight of some light subtree reaches , we start the bumping process which is executed over the next  updates there. As in Section~\ref{sec:amortized search}, we define the tail to be the maximal sequence of light nodes of weight at least  starting at the root of the subtree (again, by weight we actually mean the weight at the moment when we started the process). We construct a list of those nodes, and rebuild the structures for all new smaller light subtrees. This takes  time because we defined the weight to include the number of hard links, and can be performed incrementally over  updates keeping the structure for the original subtree as long as the procedure is not over yet so that we can answer queries. Then we must make all nodes on the tail heavy, which adds one edge to the tree induced by the branching heavy nodes, hence can potentially make at most one non-branching node branching.

First of all, we must set pointers to this new edge and construct its array in  time. Then we might need to split an existing edge into two, which requires updating the pointers of all nodes there and constructing the array for the two new parts. Here we need to modify our definition of an edge slightly: we don't want its length to significantly exceed . Hence we consider the tree induced by all branching nodes and some \emph{virtual branching nodes} chosen so that each edge is of length at most  and either connects at least one real branching node, or is of length at least . In other words, whenever we have a long edge, we add a new virtual branching node in the middle. Now whenever we split an edge into two parts, we need to iterate over at most  nodes . For each of the two resulting parts we need to construct the arrays and change the pointers to the current edge. This needs to be done incrementally over the remaining  updates into the light subtree. The first difficulty is as soon as we modify the pointer to the current edge of some , the array of this edge must be ready. So it seems that we should first construct the arrays, and then set the pointers. But then some new soft links could appear, and we wouldn't yet know which array should be updated. Hence we temporarily store pointers to both the old and the new edge at each node, updating both of them whenever a soft link changes or appears, and proceed in phases as follows.

\begin{enumerate}
\item Initialize each of the new arrays to contain just nulls.
\item Insert the new middle branching node into at most  FMA structures.
\item Iterate over  and set the pointer to the new edge for each of them in this order. Furthermore, if  is defined but  is not, modify the value in the array corresponding to the current new edge in the same time step. To quickly detect all such , for each heavy node maintain a linked list, and observe that creating a new soft link  requires adding a new element to the list of , and removing one element to the list of its parent.
\item Update the array corresponding to the upper part of the edge by iterating over all letters.
\end{enumerate}

We execute this procedure incrementally over a number of updates to the light subtree. Meanwhile, the queries are answered by checking both the old and the new edge. If the only thing happening were the updates to exactly one light subtree hanging off some , we could schedule the procedure over  of them. Unfortunately, more than one subtree might grow too large at the same (or similar) time, and furthermore new nodes can appear on the edge. In other words, new branching nodes can appear, and the length of the edge can increase, forcing us to add new virtual branching nodes. Instead of running a separate process for each new branching node, we create one master process per edge and store a queue of nodes which should be made branching. The master process runs over  updates (each of them either increasing the weight of some subtree, or creating a new node on the edge), and starts as soon as we notice that the queue is nonempty or the length of the edge exceeds . Then it makes a local copy of the current queue (clearing it afterwards), and splits the edge so that all nodes in the local queue are made branching and all new edges consist of at most  nodes. To achieve the latter, if the edge was of length at least  before the update, we insert one new virtual branching node in the middle, and then even if up to  new nodes appear, the length of each new edge is still below , but at least . As a result we get a bunch of smaller edges with potentially nonempty queues, and we might need to run their master processes as soon as the first update to their parts of the tree happens. It is clear that this guarantees that the edges never become too long, but we must also verify that any node  is made branching after at most  updates to its light subtree, and this is why we have chosen the master process to run over  updates. At most  first updates to the light subtree are spent waiting for an already running process to finish, and then  belongs to some queue, so the next  updates will be used to make  (and possibly some other nodes) branching.

This proves Thm.~\ref{thm:main3}.



\bibliographystyle{abbrv}
\bibliography{paper}

\end{document}
