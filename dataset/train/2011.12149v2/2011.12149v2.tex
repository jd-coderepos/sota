

\documentclass[final]{cvpr}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{bm}
\usepackage{multirow}
\usepackage{dsfont}

\usepackage{xcolor}
\usepackage{wrapfig}
\usepackage{booktabs}
\usepackage{color}
\usepackage{verbatim}
\usepackage{caption}
\usepackage{ulem}
\usepackage{enumitem}
\usepackage{placeins}
\usepackage{ragged2e}
\usepackage{diagbox}
\usepackage{enumitem}


\usepackage{makecell}
\usepackage{subfigure}
\usepackage{arydshln}
\captionsetup{belowskip=-0.1cm}

\usepackage{diagbox}




\newcommand{\qy}[1]{\textcolor{black}{#1}}
\newcommand{\bo}[1]{\textcolor{black}{#1}}
\newcommand{\unclear}[1]{\textcolor{black}{#1}}
\newcommand{\nickname}{SpinNet}
\newcommand{\qysupp}[1]{\textcolor{black}{#1}}




\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}


\def\cvprPaperID{2104} \def\confYear{CVPR 2021}


\begin{document}

\title{\nickname{}: Learning a General Surface Descriptor for 3D Point Cloud Registration}




\author{Sheng Ao\textsuperscript{1*}, Qingyong Hu\textsuperscript{2\thanks{Equal contribution}*}, Bo Yang\textsuperscript{3}, Andrew Markham\textsuperscript{2}, 
Yulan Guo\textsuperscript{1,4} \\
\textsuperscript{1}Sun  Yat-sen University, \textsuperscript{2}University of Oxford, \\
\textsuperscript{3}The Hong Kong Polytechnic University, \textsuperscript{4}National University of Defense Technology\\
{\tt\small qingyong.hu@cs.ox.ac.uk, bo.yang@polyu.edu.hk, guoyulan@mail.sysu.edu.cn }}

\maketitle


\begin{abstract}


\qy{Extracting robust and general 3D local features is key to downstream tasks such as point cloud registration and reconstruction. Existing learning-based local descriptors are either sensitive to rotation transformations, or rely on classical handcrafted features which are neither general nor representative. In this paper, we introduce a new, yet conceptually simple, neural architecture, termed \nickname{}, to extract local features which are rotationally invariant whilst sufficiently informative to enable accurate registration. A Spatial Point Transformer is first introduced to map the input local surface into a carefully designed cylindrical space, enabling end-to-end optimization with SO(2) equivariant representation. A Neural Feature Extractor which leverages the powerful point-based and 3D cylindrical convolutional neural layers is then utilized to derive a compact and representative descriptor for matching. Extensive experiments on both indoor and outdoor datasets demonstrate that \nickname{} outperforms existing state-of-the-art techniques by a large margin. More critically, it has the best generalization ability across unseen scenarios with different sensor modalities. The code is available at \url{https://github.com/QingyongHu/SpinNet}.}
\end{abstract}



\section{Introduction}
\label{sec:Intro}
\begin{figure}[t]
	\begin{center}
		\includegraphics[width=1.0\linewidth]{figs/fig1_new.pdf}
	\end{center}
    \caption{\qy{The Feature Matching Recall (FMR) scores of different approaches on the indoor 3DMatch~\cite{Zeng2017} and outdoor ETH~\cite{pomerleau2012challenging} dataset. Note that, all methods are trained only on the 3DMatch dataset. Our method not only achieves the highest score on 3DMatch, but also has the best generalization ability across the unseen ETH dataset. 
    }}
	\label{fig: generalization}
\end{figure}

\qy{
\bo{Accurate matching of partial 3D surfaces is critical for} point cloud registration~\cite{Elbaz2017,Lu2019,Choy2020,Gojcic2020a,huang2020feature}, segmentation~\cite{yang2019learning,hu2020randla,guo2020deep}, and recognition~\cite{guo20143d,Defferrard2020,Rao2019}. Given multiple partially overlapped 3D scans, the goal of surface matching is to align these fragments according to a set of point correspondences, thus obtaining a complete 3D scene structure. \bo{To achieve this, it is of key importance to identify general and robust local geometric patterns shared between two scans. However, this is challenging, primarily because 1) different scans usually have different viewing angles, 2) the raw 3D scans are typically incomplete, noisy, and have significantly different point densities.} 
}

\qy{Early methods to extract local geometries include PS~\cite{chua1997point}, ISS~\cite{Zhong2010Intrinsic}, SHOT~\cite{mian2010repeatability} and RoPS~\cite{guo2013rotational}, which simply compute the low-level features such as faces~\cite{mian2006three,zaharescu2009surface}, corners~\cite{stein1992structural}, and handcrafted statistical histograms~\cite{novatnack2008scale}. \bo{Although they achieve encouraging results on high-quality 3D point clouds, they are not capable of generalizing to highly noisy and large-scale real-world 3D point clouds.}
}

\qy{
\bo{Recent deep neural network based approaches \cite{Zeng2017,Khoury2017,Deng2018,Poiesi2021} have yielded excellent results in learning better local point features, thanks to the availability of large-scale labeled 3D datasets. However, they have two major limitations. \textbf{First}, many of these methods such as D3Feat \cite{bai2020d3feat} and FCGF \cite{choy2019fully} rely on kernel-based point convolution \cite{kpconv} or submanifold sparse convolution \cite{sparseconv} to extract per-point features, resulting in the learned point local patterns being rotationally variant. Consequently, their performance drops dramatically when they are applied to novel 3D scans with strong rotational changes. \textbf{Second}, although a number of recent approaches \cite{Deng2018,Deng2018a,Gojcic2019,li2020end} introduce rotation-invariant point descriptors, they simply integrate the handcrafted features such as point-pairs \cite{rusu2009fast,drost2010model} and point density ~\cite{Spezialetti2019a,you2020pointwise,Spezialetti2020a}, or external local reference frames (LRFs) ~\cite{li2020end,Kim2020a,Zhao2020} into the pipeline, fundamentally limiting the representational power of the framework \cite{guo2016comprehensive}. As a result, the extracted point features, albeit being rotation invariant, are not robust and general when being applied to unseen 3D scans with noise and different point densities.}} 

\bo{In this paper, we aim to design a new neural architecture, which is able to learn descriptive local features and generalize well to unseen scenarios. This network clearly satisfies three key properties: 1) It is rotation invariant. Particularly, it learns consistent local features from 3D scans with different rotation angles; 2) It is descriptive. In essence, it preserves the prominent local patterns despite the noise, possible surface incompleteness, or different point densities; 3) It does not include any handcrafted features. Instead, it only consists of multiple point transformations and simple neural layers coupled with true end-to-end optimization. This allows the learned descriptor to be extremely representative and general for complex real-world 3D surfaces.}

\bo{Our network, named \nickname{}, mainly consists of two modules, 1) a \textbf{Spatial Point Transformer}\footnote{This is different from the Transformer for natural language processing.}, which explicitly transforms the input 3D scans into a carefully designed cylindrical space, driving the transformed scans to be SO(2) equivariant, whilst retaining point local information; 2) a \textbf{Neural Feature Extractor}, which leverages powerful point-based and convolutional neural layers to learn representative and general local patterns.}

\bo{The \textbf{Spatial Point Transformer} firstly aligns the input 3D surface according to a reference axis, eliminating the rotational variance along the Z-axis. This is followed by an additional coordinate transformation over the XY-plane with the aid of spherical voxelization, further removing the rotation variance of each spherical voxel. Lastly, the transformed local surface is formulated as a simple yet novel 3D cylindrical volume, which is amenable to consumption by the subsequent point-based and convolutional neural layers. The \textbf{Neural Feature Extractor} firstly uses simple point-based MLPs to extract a unique signature for each voxel within the cylindrical volume, generating an initial set of cylindrical feature maps. These maps are further fed into a series of novel 3D cylindrical convolutional layers, which fully exploit the rich spatial and contextual information and generate a compact and representative feature vector.}


\qy{
\bo{These two modules enable our \nickname{} to learn remarkably robust and general local features for accurate 3D point cloud registration.}
It achieves state-of-the-art performance both on the indoor 3DMatch ~\cite{Zeng2017} dataset and the outdoor ETH \cite{pomerleau2012challenging} dataset. Notably, it shows superior generalization ability across unseen scenarios. As shown in Figure \ref{fig: generalization}, being trained only on the 3DMatch dataset, the learned descriptor of our \nickname{} can achieve an average recall score of 92.8\% on the unseen outdoor ETH dataset for feature matching, significantly surpassing the state of the art by nearly 13\%. Overall, our contributions are three-fold:
}
\begin{itemize}[leftmargin=*]
\setlength{\itemsep}{0pt}
\setlength{\parsep}{0pt}
\setlength{\parskip}{0pt}
\item \qy{We propose a new neural feature learner for 3D surface matching. It is rotation invariant, representative, and has superior generalization ability across unseen scenarios.}
\item \qy{By formulating the transformed 3D surface into a cylindrical volume,  
we introduce a powerful 3D cylindrical convolution to learn rich and general features.}
\item \qy{We conduct extensive experiments and ablation studies, demonstrating the remarkable generalization of our method and providing the intuition behind our choices.}
\end{itemize}


 
\section{Related Work}
\label{sec:related_work}

\subsection{Handcrafted Descriptors}
\qy{Traditional handcrafted descriptors can be roughly divided into two categories: 1) LRF-free methods and 2) LRF-based. The LRF-free descriptors including Spin Images (SIs)~\cite{johnson1999using}, Local Surface Patch (LSP)~\cite{chen20073d} and Fast Point Feature Histograms (FPFHs)~\cite{rusu2009fast} are typically constructed by exploiting geometrical properties (\textit{e.g.} curvatures and normal deviations) of a local surface. The main drawback of these descriptors is the lack of sufficient geometric details for the local surface. The LRF-based descriptors such as Point Signature (PS)~\cite{chua1997point}, SHOT~\cite{tombari2010unique} and Rotational Projection Statistics (RoPS)~\cite{guo2013rotational} are not only able to characterize the geometric patterns of the local support region, but also effectively exploit the 3D spatial attributes. However, LRF-based methods inherently introduce rotation errors, sacrificing the feature robustness. Overall, all these handcrafted descriptors are usually tailored to specific tasks and sensitive to noise, thus not being sufficiently flexible and descriptive for complicated and novel scenarios. }

\subsection{Learning-based Descriptors}
\qy{In contrast to traditional handcrafted descriptors, recent works~\cite{Cohen2018,Esteves2018,Zhou2018, Chen2019,Yu2020,yew2020rpm} leverage data-driven deep neural networks to learn local features from large-scale datasets. These learned descriptors tend to have strong descriptive ability and robustness.} 

\textbf{Rotation Variant Descriptors.} \qy{ Zeng \textit{et al.} propose the pioneering work 3DMatch ~\cite{Zeng2017}, which takes the local volumetric patches as input, and then leverages 3D Convolutional Neural Networks (CNNs) to learn local geometric patterns. Yew and Lee introduce a weakly-supervised framework 3DFeat-Net ~\cite{Yew2018} to learn both the 3D feature detector and descriptor simultaneously. Choy \textit{et al.} build a dense feature descriptor FCGF ~\cite{choy2019fully} based on~\cite{choy20194d}. Recently, Bai \textit{et al.}~\cite{bai2020d3feat} design a pipeline to jointly learn both dense feature detectors and local feature descriptors, achieving the state-of-the-art performance on 3DMatch \cite{Zeng2017} and KITTI~\cite{geiger2012we} datasets for point cloud registration.}
\qy{However, all these methods are sensitive to rigid transformation in Euclidian space. Extensive data augmentation  can be used to alleviate this problem, however, the overall performance of subsequent tasks is still sub-optimal ~\cite{Esteves2018}.}


\textbf{Rotation Invariant Descriptors.} 
\qy{A number of recent methods have started to learn rotation-invariant descriptors. Khoury \textit{et al.}~\cite{Khoury2017} parameterize the raw point clouds with oriented spherical histograms, and then map the high-dimensional embedding to a compact descriptor through a deep neural network. Deng \textit{et al.}~\cite{Deng2018} encode the local surface using rotation-invariant Point Pair Features (PPFs). These features are then fed into multiple MLPs to learn a global descriptor. In the follow-up work~\cite{Deng2018a}, FoldingNet~\cite{Yang2018} is adopted as the backbone network to learn 3D local descriptors. Gojcic \textit{et al.}~\cite{Gojcic2019} introduce the voxelized Smoothed Density Value (SDV) to encode the local surface as a compact and rotation-invariant representation, which is fed into a Siamese architecture to learn the final descriptor.}
\qy{Overall, although these methods are indeed able to learn rotationally invariant features from the local surface, they initially rely on classical handcrafted features which significantly limits the descriptiveness of the descriptors. Additionally, most of the above handcrafted features are based on the point-pairs and point density, both of which are sensitive to noise, clutter, and distance variations, making the learned features hardly generalize to novel scenarios.}

\qy{A handful of recent works~\cite{Spezialetti2019a,you2020pointwise,Kim2020a,li2020end} try to learn rotation-invariant local descriptors with end-to-end optimization. However, they either require the computation of the point density or rely on external reference frames to achieve rotation invariance. This is usually unstable and does not generalize well to unseen datasets. 
In contrast, our \nickname{} directly transforms the point clouds into a cylindrical volume followed by a series of powerful neural layers. It learns rotation-invariant, compact, and highly descriptive local features in a truly end-to-end fashion, without relying on any handcrafted features or unstable external LRFs. This enables the learned descriptors to be extremely general for use on unseen 3D surfaces across different datasets.}


 
\section{\nickname{}}

\begin{figure*}
	\begin{center}
		\includegraphics[width=1.0\linewidth]
		{figs/fig2.pdf}
	\end{center}
	\caption{\qy{The detailed components and processing steps of our Spatial Point Transformer.}}
	\label{fig: parameterization}
\end{figure*}

\subsection{Problem Statement}
\qy{Given two partially overlapped point clouds   and . The task of point cloud registration is to find an optimal rigid transformation , as well as the point correspondences to align pairs of fragments, and finally recover the complete scene. The pair of point correspondence  is expected to satisfy:
}

\qy{where  denotes the rotation matrix,   is the translation vector, and  is the residual error. In practice, it is infeasible to simultaneously find the correspondences and estimate the transformation, due to the non-convexity of this problem~\cite{li20073d}. However, if the point subsets  and  with one-to-one correspondences can be determined, the registration problem can be simplified as a  minimization problem for the following  distance:
} 

\qy{where  is the number of successfully matched correspondences,  
is a permutation matrix whose entries satisfy  if the  point in  corresponds to  point in  and 0 otherwise.}

\qy{We propose a new surface feature learner \nickname{}, which is a mapping function , where  is equal to  under arbitrary rigid transformations such as rotation and translation, if  and  are indeed a correct match. In particular, our feature learner mainly consists of a Spatial Point Transformer and a Neural Feature Extractor.} 

 
\subsection{Spatial Point Transformer}
\label{subsec: FEL}

\bo{This module is designed to spatially transform the input 3D surfaces into a cylindrical volume, overcoming the rotation variance, whilst without dropping critical information of local patterns. As shown in Figure ~\ref{fig: parameterization}, it consists of four components, as discussed below.}

\textbf{Alignment with a Reference Axis.} \qy{Given a specific point  in a local surface, we first estimate a reference axis  oriented to the viewpoint~\cite{mian2010repeatability,ao2020repeatable} from its neighbouring point set  within a support radius . We then align  with the Z-axis using a rotation matrix . Compared with the external local reference frames which are likely to be ambiguous and unstable, our estimated  tends to be more robust and stable with regard to rotation changes \cite{petrelli2011repeatability}.}
\qy{Subsequently, the neighbouring point set  is transformed to .}
\qy{
To achieve translation invariance, we further normalize  by offsetting to the center point, \textit{i.e.}, . Hence, the obtained local patch  is aligned with the -axis, leaving the remaining rotational degree of freedom entirely on the XY-plane.}

\textbf{Spherical Voxelization.} \qy{
To further eliminate the rotational variance on the XY-plane, we leverage a rotation-robust spherical representation. In particular, we treat the patch  as a sphere, and evenly divide it into  voxels along the radial distance , elevation angle  and azimuth angle . The center of each voxel is denoted as , where , , . We then explicitly identify a set of neighboring points for the center point  of each voxel. In particular, we use the radius query to find the neighboring points  based on a fixed radius , where . Lastly, we randomly sample and preserve a fixed number of  points for each voxel, aiming for efficient computation in parallel. This spherical voxelization step is key to the successive spatial point transformation.}

\textbf{Transformation on the XY-Plane.} \qy{To enable each spherical voxel to be rotationally invariant on the XY-plane,
we proactively rotate each voxel around the Z-axis to align its center  with the YZ-plane, where the rotation matrix  is defined as:}

\qy{This removes an additional rotational degree of freedom for each voxel on the XY-plane, without dropping any local geometric patterns of each voxel. Note that, the existing methods \cite{Spezialetti2019a,you2020pointwise} usually use handcrafted features to achieve rotation invariance, resulting in the loss of the rich local patterns. Uniquely, our simple strategy to transform voxels can preserve these patterns, leaving them to be learned by the powerful neural layers.
}

{\bf Cylindrical Volume Formulation.} \qy{
\bo{Once the local patterns of each voxel are transformed, it is crucial to further preserve the larger spatial structures across multiple voxels. This requires the relative positions of all voxels to be represented in the whole framework. 
To this end, we reformulate the spherical voxels into a cylindrical volume. This is amenable to the proposed 3D cylindrical convolutional network, which guarantees the SO(2) equivariance  of the input local surface and preserves the topological patterns of multiple voxels.}}
\bo{In particular, given the transformed spherical voxels, each of which has a set of neighbouring points, we logically project them into a cylindrical volume, denoted as  and illustrated in Figure \ref{fig: parameterization}.
}

\bo{In summary, given an input surface patch, our Spatial Point Transformer explicitly aligns its Z-axis with a reference axis, and proactively transforms the spherical voxel patterns on the XY-plane, and further preserves the topological surface structures through the cylindrical volume formulation. Clearly, this module keeps all surface patterns intact for the subsequent Neural Feature Extractor to learn.}

\begin{figure*}[thb]
	\begin{center}
		\includegraphics[width=1.0\linewidth]{figs/fig33.pdf}
	\end{center}
	\caption{\qy{Illustration of the proposed Neural Feature Extractor.}}
	\label{fig: 3DCCN}
\end{figure*}
\vspace{-0.2cm}

 
\subsection{Neural Feature Extractor}
\label{subsec: CCN}
\bo{This module is designed to learn the general features from the transformed points within each cylindrical voxel using the powerful neural layers. As shown in Figure \ref{fig: 3DCCN}, it consists of two components, as discussed below.}

{\bf Point-based Layers.} \qy{ Given the points within each cylindrical voxel, we use shared MLPs followed by a max-pooling function  to learn an initial signature for each voxel. Formally, the point-based layers are defined as:}

\qy{where  is the learned features with  dimension. Note that, the MLP weights are shared across all spherical voxels. Eventually, we obtain a set of 3D cylindrical feature maps . 
}


\bo{\bf 3D Cylindrical Convolutional Layers.}
\qy{To further learn spatial structures across multiple voxels of the volume, we propose an efficient 3D Cylindrical Convolution Network (3DCCN) inspired by \cite{joung2020cylindrical}. In particular, given a voxel located at the position  on the  cylindrical feature map in the  layer, our 3DCCN is defined as follows. }

where  is the size of the kernel along the radial dimension,  and  are the height and width of the kernel respectively,  are the learnable parameters. 

\bo{Being quite different from existing convolution operations, our proposed 3DCCN is novel in the following two aspects. \textbf{First}, since the cylindrical feature maps are  continuous over a cylinder, our 3DCCN is designed to wrap around these feature maps, \textit{i.e.,} over the periodic boundary from  to . Therefore, explicit padding is not required in our 3DCCN, but required by 3D-CNN at the boundary of feature maps. \textbf{Second}, compared with the existing 3D manifold sparse convolution \cite{choy2019fully} or kernel point convolutions \cite{bai2020d3feat}, the continuous convolution around the  volume enables the obtained feature map to be SO(2) equivariant, hence to achieve the  final rotation-invariance.
}


After stacking multiple of these 3DCCN layers followed by max-pooling, the original cylindrical feature maps are compressed to a compact and representative feature vector.

\begin{comment}
\qy{As shown in Figure~\ref{fig: 3DCCN}, the 3D cylindrical space is defined as , we then formulate the 3D cylindrical feature maps and filters as continuous functions , where ,  and  represent the radial distance to the center, height, and azimuth angle of cylindrical signal respectively.  is the number of channels. Formally, 3D cylindrical convolution takes the cylinder signal  as input and convolves it with the filter :}

\end{comment}

 
\subsection{End-to-end Implementation}
\label{subsec: network}
\qy{ The Spatial Point Transformer is directly connected with the Neural Feature Extractor, followed by the existing contrastive loss \cite{bai2020d3feat} for end-to-end optimization. The widely-used \textit{hardest in batch} sampling~\cite{mishchuk2017working} is also adopted on-the-fly to maximize the distance between the closest positive and the closest negative patches. Details of the neural layers are presented in the appendix.}

\qy{We implement our \nickname{} based on the PyTorch framework. The Adam optimizer \cite{kingma2014adam} with default parameters is used. The initial learning rate is set to 0.001 and decayed with a rate of 0.5 for every 5 epochs. We train the network for 20 epochs, the best-performed model on the validation set is then used for testing. For a fair comparison, we keep the same setting for all experiments. All experiments are conducted on the platform with Intel Xeon CPU @2.30GHZ with an NVIDIA RTX2080Ti GPU.}



\begin{comment}
{\bf Architecture.} 
\qy{We stack several cylindrical convolutional layers together to learn a more descriptive feature representation with SO(2) rotation-equivariance. In addition, we follow R2D2~\cite{Revaud2019} to stack three 22 cylindrical convolution layers together to reduce the parameters and computational cost. The last feature maps are further aggregated using a non-parametric max-pooling layer, and the final output is a rotation-invariant and compact (32-dim) local feature descriptor. To avoid overfitting, the whole framework is lightweight with a small number of trainable parameters (about 2 million), which is far fewer than FCGF \cite{choy2019fully} and D3Feat \cite{bai2020d3feat}. For the detailed network architecture, please refer to the supplementary materials.}
\end{comment} 

\section{Experiments}
\label{Experiments}

We first evaluate our \nickname{} on the indoor 3DMatch dataset \cite{Zeng2017}  and the outdoor KITTI dataset \cite{geiger2012we}. We then evaluate the generalization ability of our approach across multiple unseen datasets \cite{Zeng2017, geiger2012we, pomerleau2012challenging} acquired by different sensors. Lastly, extensive ablation studies are conducted.

\textbf{Experimental Setup.} \qy{We follow~\cite{bai2020d3feat, Zeng2017} to generate training samples by only considering the point cloud fragment pairs with more than 30\% overlap in the whole dataset. For each paired fragment  and , we randomly sample a fixed number of anchor points from the overlapping region of , and then apply the ground-truth transformation  to determine the corresponding points in fragment . Considering the varying number of point cloud fragments in different datasets, we uniformly select 20 and 500 anchor points from each fragment in the 3DMatch and KITTI dataset, so as to keep a similar number of samples for training. For each anchor point, we randomly sample 2048 points from its support region. }





 
\subsection{Evaluation on Indoor 3DMatch Dataset}
\label{Ex:3dmatch}

\qy{3DMatch is a RGBD-reconstruction dataset, which consists of 62 real-world indoor scenes collected from existing dataset~\cite{valentin2016learning, shotton2013scene, xiao2013sun3d, kim2014shape2pose, valentin2016learning, dai2017bundlefusion}. We follow the official protocol provided in~\cite{bai2020d3feat} to divide the scenes into training and testing splits. Each scene contains several partially overlapped fragments, and has the ground truth transformation parameters available for evaluation. Feature Matching Recall (FMR) \cite{Deng2018a} is used as the standard metric.
}

\smallskip\noindent\textbf{Comparisons with the state-of-the-arts.} \qy{ We first compare the FMR scores achieved by our \nickname{} and strong baselines (including LMVD \cite{li2020end}, D3Feat \cite{bai2020d3feat}, FCGF \cite{choy2019fully}, PerfectMatch \cite{Gojcic2019}, PPFNet \cite{Deng2018}, and PPF-FoldNet \cite{Deng2018a}) on the 3DMatch dataset, under the conditions of sampling points =5000, distance threshold =10 cm and inlier ratio threshold =5\%. To further evaluate the robustness of all approaches against rotations, we follow \cite{Deng2018, bai2020d3feat} to build a rotated 3DMatch benchmark by applying arbitrary rotations in SO(3) group to all fragments of the dataset. 
}

\begin{table}[htb]
	\begin{center}
		\scalebox{0.75}{
			\begin{tabular}{r|cc|cc|c|c}
		       \Xhline{2\arrayrulewidth}
				\multirow{2}{*}{} & \multicolumn{2}{c|}{\bf{Origin}}  & \multicolumn{2}{c|}{\bf{Rotated}} & {Feat.} & {Rot.} \\
				& FMR (\%)      & STD         & FMR (\%)      & STD   & {dim.}   & {Aug.}    \\ \hline
				FPFH~\cite{rusu2009fast}         & 35.9          & 13.4         & 36.4          & 13.6  & {33}    & {No}    \\
				SHOT~\cite{tombari2010unique}         & 23.8          & 10.9         & 23.4          & 9.5  & {352}     & {No}     \\
				3DMatch~\cite{Zeng2017}      & 59.6          & 8.8          & 1.1           & -       & {512}     & {No}      \\
				CGF~\cite{Khoury2017}          & 58.2          & 14.2         & 58.5          & 14.0   & {32}   & {No}   \\
				PPFNet~\cite{Deng2018}       & 62.3          & 10.8         & 0.3           & -        & {64}      & {No}     \\
				PPF-FoldNet~\cite{Deng2018a}  & 71.8          & 10.5         & 73.1          & 10.4    & {512}    & {No}   \\
				PerfectMatch~\cite{Gojcic2019} & 94.7          & 2.7          & 94.9          & 2.5    & {32}  & {No}    \\
				FCGF~\cite{choy2019fully}         & 95.2          & 2.9          & 95.3          & 3.3    & {32}      & {Yes}    \\
				D3Feat-rand~\cite{bai2020d3feat} & 95.3          & 2.7          & 95.2          & 3.2   & {32}    & {Yes}     \\
				D3Feat-pred~\cite{bai2020d3feat} & 95.8          & 2.9          & 95.5          & 3.5   & {32}     & {Yes}     \\
				LMVD~\cite{li2020end} & 97.5          & 2.8          & 96.9          & -   & {32}   & {No}     \\
				\textbf{\nickname{} (Ours)}         & \textbf{97.6} & \textbf{1.9} & \textbf{97.5} & \textbf{1.9} & \textbf{32} & {\textbf{No}} \\ 
		        \Xhline{2\arrayrulewidth}
			\end{tabular}
		}
	\end{center}
	\caption{\qy{ Quantitative results on the 3DMatch dataset, STD: standard deviation. The symbol `-' means the results are unavailable or STD under low FMRs (10\%).}}
	\label{tab:3dmatch}
\end{table}

\begin{table}[thb]
	\begin{center}
	\scalebox{0.8}{
	\begin{tabular}{rcccccc}
        \Xhline{2\arrayrulewidth}
		\#\textbf{Sampled points}      & \textbf{5000}     & \textbf{2500}     & \textbf{1000} & \textbf{500} & \textbf{250}   & \textbf{Average} \\ \hline
		\multicolumn{7}{c}{Feature Matching Recall (\%)}         \\ \hline
		PerfectMatch~\cite{Gojcic2019}     & 94.7     & 94.2     & 92.6   & 90.1  & 82.9   & 90.9   \\
		FCGF~\cite{choy2019fully}       & 95.2     & 95.5     & 94.6  & 93.0  & 89.9  & 93.6       \\
		D3Feat-rand~\cite{bai2020d3feat}     & 95.3     & 95.1     & 94.2  & 93.6  & 90.8  & 93.8       \\
		D3Feat-pred~\cite{bai2020d3feat}     & 95.8     & 95.6     & 94.6 & 94.3 & 93.3  & 94.7       \\
		\textbf{\nickname{} (Ours)}             & \textbf{97.6}      &    \textbf{97.5}      & \textbf{97.3}  & \textbf{96.3} & \textbf{94.3}      & \textbf{96.6}       \\ 
        \Xhline{2\arrayrulewidth}
	\end{tabular}
	}
	\end{center}
\caption{\qy{Quantitative results on the 3DMatch dataset using different numbers of sampled points.}}
\label{tab:3dmatch_keypoints}
\end{table}

\begin{figure}[thb]
	\begin{center}
		\includegraphics[width=1.0\linewidth]{figs/fig6.pdf}
	\end{center}
	\caption{\qy{Feature matching recall on the 3DMatch dataset under different inlier distance threshold  (Left) and inlier ratio threshold  (Right).}}
	\label{fig:3dmatch_FMR}
\end{figure}




\qy{As shown in Table~\ref{tab:3dmatch}, the descriptor generated by our method achieves the highest average FMR score and the lowest standard deviation on both the original and rotated datasets, outperforming the state-of-the-art methods. Note that, several baselines \cite{choy2019fully, bai2020d3feat} require rotation-based data augmentation for training, whilst ours does not.} 



\smallskip\noindent\textbf{Performance under different number of sampled points.} \qy{ We further evaluate the performance of our \nickname{} on the 3DMatch by taking different number of sampled points as input. As shown in Table \ref{tab:3dmatch_keypoints}, the descriptor learned by our \nickname{} consistently achieves the best FMR scores when the number of sampled points is reduced from 5000 to 250. In particular, by randomly selecting points, our method even outperforms D3Feat-pred which has an explicit keypoint detection module. This demonstrates our network is highly robust and not sensitive to the number of sampled points.
}



\begin{table}[thb]
	\begin{center}
	\scalebox{0.8}{
	\begin{tabular}{r|cc|cc|c}
        \Xhline{2\arrayrulewidth}
		\multirow{2}{*}{} & \multicolumn{2}{c|}{\textbf{RTE (cm)}} & \multicolumn{2}{c|}{\textbf{RRE ()}} & \multirow{2}{*}{\textbf{Success (\%)}} \\
		& AVG                & STD               & AVG               & STD              &                       \\ \hline
		3DFeat-Net~\cite{Yew2018} & 25.9               & 26.2              & 0.57              & 0.46             & 95.97                 \\
		FCGF~\cite{choy2019fully}       & 9.52               & 1.30              & 0.30              & 0.28             & 96.57                 \\
		D3Feat-rand~\cite{bai2020d3feat}       & 8.78                   & 0.44                  & 0.32                  & 0.07                 & 99.81                      \\
		D3Feat-pred~\cite{bai2020d3feat}     & \textbf{6.90}               & \textbf{0.30}              & \textbf{0.24}              & \textbf{0.06}             & \textbf{99.81}                 \\
		\textbf{\nickname{} (Ours)}      & 9.88               & 0.50                  & 0.47                  & 0.09                 & 99.10                      \\ 
        \Xhline{2\arrayrulewidth}
	\end{tabular}
	}
	\end{center}
\caption{\qy{Quantitative results of different approaches on the KITTI odometry dataset. The scores of baselines are retrieved from ~\cite{bai2020d3feat}.}}
  \label{tab:kitti}
\end{table}



\smallskip\noindent\textbf{Performance under Different Error Thresholds.} \qy{ Additionally, we evaluate the robustness of \nickname{} by varying the error thresholds ( and ). As shown in Figure ~\ref{fig:3dmatch_FMR}, the descriptor generated by \nickname{} consistently outperforms other methods under all thresholds. It is worth noting that the FMR score of our method is significantly higher than others, when the inlier ratio threshold increases. For a stricter condition  = 0.2, our method maintains a high FMR score of 85.7\%, while D3Feat and FCGF drop to 75.8\% and 67.4\%, respectively. This highlights that our approach is more robust in harder scenarios.} 






 
\subsection{Evaluation on Outdoor KITTI Dataset}
\label{Ex:kitti}


\qy{KITTI odometry \cite{geiger2012we} is an outdoor sparse point cloud dataset acquired by Velodyne-64 3D LiDAR scanners. It consists of 11 sequences of outdoor scans. For fair comparison, we follow the same dataset splits and preprocessing methods as used in D3Feat~\cite{bai2020d3feat, choy2019fully}. Similar to ~\cite{ma2016fast}, Relative Translational Error (RTE), Relative Rotation Error (RRE), and Success rate are used as the evaluation metrics. The registration is regarded as successful if the RTE and RRE of a pair of fragments are both below the predefined thresholds 2m and 5. It is noted that the point clouds are gravity-aligned in this dataset, we follow \cite{Yew2018} to skip the alignment with a reference axis in our method. As shown in Table \ref{tab:kitti}, the results of our \nickname{} are on par with the strong baseline D3Feat. Admittedly, our \nickname{} is marginally lower than the state-of-the-art D3Feat-pred, primarily because D3Feat has a powerful joint learned descriptor and keypoint detector. Also, the well aligned point clouds in this dataset are indeed in favor of D3Feat. We leave the integration of keypoint detection for future exploration.}






 
\subsection{Generalization across Unseen Datasets}
\label{Ex:generalization}


\qy{We have conducted several groups of experiments to extensively evaluate the generalization ability of our \nickname{}. In each group, our network is trained on one dataset, and then directly tested on a completely unseen dataset.}

\begin{table}[thb]
	\begin{center}
		\scalebox{0.7}{
			\begin{tabular}{r|c|cc|cc|c}
                \Xhline{2\arrayrulewidth}
				\multirow{2}{*}{} &
				{\bf{Param.}} &
				\multicolumn{2}{c|}{\bf{Gazebo}}   & \multicolumn{2}{c|}{\bf{Wood}}     & \multirow{2}{*}{Avg.}\\
				&(Mb)  & Summer        & Winter        & Autumn        & Summer        &                          \\ \hline
				FPFH~\cite{rusu2009fast}                & -   & 38.6          & 14.2          & 14.8          & 20.8          & 22.1                     \\
				SHOT~\cite{tombari2010unique}          & -        & 73.9          & 45.7          & 60.9          & 64.0          & 61.1                     \\
				3DMatch~\cite{Zeng2017} & 13.40     & 22.8          & 8.3           & 13.9          & 22.4          & 16.9                     \\
				CGF~\cite{Khoury2017} & 1.86     & 37.5          & 13.8          & 10.4          & 19.2          & 20.2                     \\
				PerfectMatch~\cite{Gojcic2019} & 3.26  & \underline{91.3}          & \underline{84.1}          & 67.8          & 72.8          & 79.0                     \\
				FCGF~\cite{choy2019fully} & 33.48  & 22.8          & 10.0          & 14.8          & 16.8          & 16.1                     \\
				D3Feat (rand)~\cite{bai2020d3feat} & 13.42  & 45.7          & 23.9          & 13.0          & 22.4          & 26.2                     \\
				D3Feat (pred)~\cite{bai2020d3feat} & 13.42  & 85.9          & 63.0          & 49.6          & 48.0          & 61.6                     \\
				LMVD~\cite{li2020end} & 2.66  & 85.3          & 72.0          & \underline{84.0}          & \underline{78.3}          & \underline{79.9}                     \\			
				\textbf{\nickname{} (Ours)} & 2.16  & \textbf{92.9} & \textbf{91.7} & \textbf{92.2} & \textbf{94.4} & \textbf{92.8}            \\ 
		        \Xhline{2\arrayrulewidth}
			\end{tabular}
		}
	\end{center}
    \caption{\qy{Quantitative results on the ETH dataset. Note that, all methods are only trained on the indoor 3DMatch dataset. The FMR scores at  = 10cm,  = 5\% are compared.}}
	\label{tab:eth}
\end{table}


\textbf{Generalization from 3DMatch to ETH dataset.} \qy{Following the settings in~\cite{bai2020d3feat}, all models are only trained on the 3DMatch dataset, and then directly tested on the ETH dataset \cite{pomerleau2012challenging}. Note that, the ETH dataset consists of four scenes, \textit{i.e.}, Gazebo-Summer, Gazebo-Winter, Wood-Summer, and Wood-Autumn. Different from 3DMatch, the ETH dataset is acquired by static terrestrial scanners and dominated by outdoor vegetation, such as trees and bushes. In addition, the fragments of point clouds in the ETH dataset have lower resolution and contain more complex geometries compared with the 3DMatch dataset. The large domain gap between these two datasets poses a great challenge to the generalization of all approaches.}

\qy{
As shown in Table~\ref{tab:eth}, the performance of all baselines, namely D3Feat, FCGF, 3DMatch, and CGF, show a significant drop on the ETH dataset. Their FMR scores decrease up to 80\% compared with their results on the original 3DMatch dataset, as shown in Table \ref{tab:3dmatch}, and some techniques are even lower in performance than handcrafted descriptors such as SHOT. Fundamentally, the poor generalization of these methods is attributed to the fact that the descriptors learned by D3Feat, FCGF, and 3DMatch are variant to rigid transformations such as rotation and translation.} 

\bo{The descriptor generated by our \nickname{} achieves the highest FMR scores on all four scenes, significantly surpassing the second-best method (LMVD) by about 13\%. This clearly shows that our method has excellent generalization ability across the unseen dataset collected by a new sensor modality. This is primarily because our \nickname{} is explicitly designed to achieve rotational invariance. The first row of Figure \ref{fig:unseen_qualitative} shows the qualitative results.}



\begin{table}[thb]
	\begin{center}
		\scalebox{0.75}{
			\begin{tabular}{r|cc|cc}
             \Xhline{2\arrayrulewidth}
				\multirow{2}{*}{} & \multicolumn{2}{c|}{\bf{Origin}}  & \multicolumn{2}{c}{\bf{Rotated}} \\
				& FMR (\%)      & STD (\%)         & FMR (\%)      & STD (\%)   \\ \hline
				FPFH~\cite{rusu2009fast}         & 35.9          & 13.4         & \underline{36.4}          & 13.6    \\
				SHOT~\cite{tombari2010unique}         & 23.8          & 10.9         & 23.4          & 9.5     \\
				FCGF~\cite{choy2019fully}         & 32.5          & 7.4          & 1.0          & 1.0    \\
				D3Feat-rand~\cite{bai2020d3feat} & 60.7          & 7.7          & 17.2         &  4.6   \\
				D3Feat-pred~\cite{bai2020d3feat} & \underline{62.7}          & 8.1          & 17.8         &  3.2    \\
				\textbf{\nickname{} (Ours)}         & \textbf{84.5} & \textbf{5.9} & \textbf{84.2} & 5.8  \\ 
		        \Xhline{2\arrayrulewidth}
			\end{tabular}
		}
	\end{center}
	\caption{\qy{Quantitative results of different methods on the indoor 3DMatch dataset. Note that, all methods are only trained on the outdoor KITTI dataset.}}
	\label{tab:kitti_3dmatch}
\end{table}

\begin{table}[thb]
	\begin{center}
	\scalebox{0.8}{
	\begin{tabular}{r|cc|cc|c}
        \Xhline{2\arrayrulewidth}
		\multirow{2}{*}{} & \multicolumn{2}{c|}{\textbf{RTE (cm)}} & \multicolumn{2}{c|}{\textbf{RRE ()}} & \multirow{2}{*}{\textbf{Success (\%)}} \\
		& AVG                & STD               & AVG               & STD              &                       \\ \hline
		FCGF~\cite{choy2019fully}       & 27.1               & 5.58              & 1.61              & 1.51             & 24.19                 \\
		D3Feat-rand~\cite{bai2020d3feat}       & 37.8                   & 9.98                  & 1.58                  & 1.47                 & 18.47                      \\
		D3Feat-pred~\cite{bai2020d3feat}     & 31.6               & 10.1              & 1.44              & 1.35             & \underline{36.76}                 \\
		\textbf{\nickname{} (Ours)}      & \textbf{15.6}               & \textbf{1.89}                  & \textbf{0.98}                  & \textbf{0.63}                 & \textbf{81.44}                      \\ 		        \Xhline{2\arrayrulewidth}

	\end{tabular}
	}
	\end{center}
\caption{\qy{Quantitative results on the KITTI dataset. Note that, all models are trained on the indoor 3DMatch dataset, while being directly tested on the outdoor KITTI dataset. }}
  \label{tab:3dmatch_kitti}
\end{table}


\begin{figure*}[thb]
\begin{center}
\includegraphics[width=0.97\linewidth]{figs/Fig_qualitative.pdf}
\end{center}
\caption{\qy{Qualitative results of our method on unseen datasets. The first row is from 3DMatch to ETH, the second row is from KITTI to 3DMatch, and the third row is from 3DMatch to KITTI.}}
\label{fig:unseen_qualitative}
\end{figure*}


\textbf{Generalization from KITTI to 3DMatch dataset.} \qy{All models are trained on the outdoor KITTI dataset which is mainly composed of sparse LiDAR point clouds, and then directly tested on the indoor 3DMatch dataset which consists of dense point clouds reconstructed from RGBD images. 
As presented in Table \ref{tab:kitti_3dmatch}, both D3Feat and FCGF achieve poor results on the 3DMatch dataset, especially when arbitary rotation in SO(3) exists. Their scores are even lower than the traditional methods such as FPFH, primarily because both D3Feat and FCGF have large numbers of parameters and tend to overfit the KITTI dataset, without learning the representative and general local patterns that can be applicable to the unseen dataset. By comparison, our \nickname{} achieves an overall FMR score of 79.6\%, demonstrating the superior generalization across novel scenarios. The second row of Figure \ref{fig:unseen_qualitative} shows the qualitative results. }


\textbf{Generalization from 3DMatch to KITTI dataset.} \qy{Additionaly, we evaluate the generalization ability from 3DMatch to KITTI dataset. All models are only trained on the indoor 3DMatch dataset, and then directly tested on the outdoor KITTI dataset. Table \ref{tab:3dmatch_kitti} presents the quantitative results. Because these two datasets are collected by different types of sensors, there is a large gap between the data distributions. Neither FCGF nor D3Feat can effectively generalize from 3DMatch to KITTI dataset. However, our method still demonstrates an excellent success rate of 69.19\%, doubling that of the second best method. The third row of Figure \ref{fig:unseen_qualitative} shows the qualitative results.}







 
\subsection{Ablation Study}
\label{Ex: abaltion}


\qy{
To systematically evaluate the effectiveness of each component in our \nickname{}, we conduct extensive ablative experiments on both 3DMatch and ETH datasets. In particular, we train all ablated models on the 3DMatch dataset, and then directly test them on both 3DMatch and ETH datasets.}

\smallskip\noindent\textbf{(1) Only removing the alignment with a reference axis.} \qy{
Initially, the reference axis is computed to align the input patch with the Z-axis. By removing this step, the rotation invariance on SO(3) is no longer maintained.}

\begin{table}[thb]
\begin{center}
	\scalebox{0.75}{
	\begin{tabular}{l|cc|cc|c|c}
        \Xhline{2\arrayrulewidth}
		& \multicolumn{4}{c|}{\bf{3DMatch}}                       & \multicolumn{2}{c}{\bf{ETH}} \\ \hline
		& \multicolumn{2}{c|}{\bf{Origin}} & \multicolumn{2}{c|}{\bf{Rotated}} 
		& {\bf{Origin}} 
		& {\bf{Rotated}} \\
		 \hfill Inlier ratio  =  & 0.05         & 0.2         & 0.05          & 0.2         & 0.05  & 0.05 \\ \hline
		(1) W/o reference axis          & 95.1             & 80.0             & 63.0              & 23.1             & 83.9   & 13.5  \\
		(2) W/o transformation   & 93.5             & 70.8             & 87.7              &  44.8            & 60.5  & 47.7    \\
		(3) W/o Point Nets       & 94.0             &  66.3            &  93.8        &  66.1           & 42.6  & 42.4  \\
		(4) Replacing 3DCCN       & 64.4             & 10.6             & 64.4              & 10.4             & 0.0  & 0.0   \\
		(5) \textbf{The full method}     & \textbf{97.6}             & \textbf{85.7}             & \textbf{97.5}              & \textbf{86.1}             & \textbf{92.8}  &  \textbf{92.4}   \\  
        \Xhline{2\arrayrulewidth}
	\end{tabular}
	}
\end{center}
\caption{\qy{The FMR scores of all ablated models on the 3DMatch and ETH datasets with  = 0.1cm.}}
\label{tab:ablation}
\end{table}

\smallskip\noindent\textbf{(2) Only removing the transformation on the XY-plane.} \qy{The transformation employed on the XY-plane is originally designed to eliminate the rotation variance of each voxel in the plane. In this experiment, we remove the transformation and directly operate on the non-transformed spherical voxels to formulate the cylindrical volume.}

\smallskip\noindent\textbf{(3) Only replacing the point-based layers with density.} \qy{ Instead of using the Point-based layers to learn a signature for each cylindrical voxel, we manually compute the point density of each voxel as its signature. Basically, this is to validate whether our point-based learned features are more general and representative than the commonly used, yet limited, handcrafted feature. }

\smallskip\noindent\textbf{(4) Only replacing 3DCCN by MLPs.} \qy{ The 3DCNN is designed to learn larger spatial structures from multiple voxels, whilst maintaining rotation equivariance. In this experiment, we replace the 3DCNN layers with the same number of MLP layers shared by all cylindrical voxels. These MLPs are unable to learn a wide context.
}


\textbf{Analysis.} \qy{Table \ref{tab:ablation} shows the results of all ablated networks on the 3DMatch dataset, as well as the generalization performance on the ETH datasets. It can be seen that: 1) Without using the alignment of a reference axis or the transformation of spherical voxels, the ablated models are unable to effectively match the point clouds either in 3DMatch or ETH datasets, especially for the point clouds with random rotations. This shows that the proposed Spatial Point Transformer indeed plays an important role to achieve rotation invariance in our \nickname{}.
2) Without using the advanced point-based neural layers to learn the signatures for spherical voxels, the ablated method can obtain consistent results on the 3DMatch dataset using the simple handcrafted feature, \textit{i.e.,} point density, but fails to generalize to the unseen ETH dataset. This clearly demonstrates that the learned local features tend to be much more powerful and general than the handcrafted features.
3) Without using the 3DCCN to learn larger surface structures, the ablated model only obtains significantly lower scores on both the 3DMatch and ETH datasets. This demonstrates that our 3DCCN is a key to preserving the local spatial patterns.
}
 
\section{Conclusion}
\bo{In this paper, we present a new neural descriptor to learn compact representations for complex 3D surfaces. The learned representations are rotation invariant, descriptive, and able to preserve complex local geometric patterns. Extensive experiments demonstrate that our descriptor has remarkable generalization ability across unseen scenarios and achieves superior results for 3D point cloud registration. \unclear{In future, we will investigate the integration  of keypoint detector, as well as the fully-convolutional architecture.
}
} 
 
\clearpage

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\newpage
\appendix
\section*{Appendix} 
\section{Definitions of Equivariance and Invariance}
\qysupp{For a specified function  as well as a specified group action ,  is said to be equivariant with respect to transformation action  if,}

\qysupp{Analogously,  is said to be invariant to transformations  when the following equation is satisfied: }


\section{Theoretical Proof of Equivariance}


\paragraph{Lemma 1.}\textit{Given a discrete 2D rotation group\footnote{The minimum rotation unit depends on the way partion along the azimuth axis. \textit{i.e.,} .} , where , then the proposed spatial point transformer is an equivariant map for the 2D rotation group .}

\paragraph{Proof:} For a local patch , the spatial point transformer in our framework can be regarded as a mapping  from  to cylindrical volume . For a group action  in , suppose , and the rotated local neighbouring set  . On the other hand, for the rotation matrix defined in Eq.~\ref{eq3}, we have . Then, the  element  of cylindrical volume  satisfies:

where , which is the cylindrical volume corresponding to the  . Based on Eq.~\ref{Supp-eq3}, we can infer that , hence the transformed cylindrical volume  can be formulated as:

where  if , due to the periodic property of the cylindrical volume in the XY plane. On the other hand,  means rotating the cylindrical volume  around the Z-axis, that is:

which completes our proof that the spatial point transformer  is an equivariant map for the rotation group . 


\paragraph{Lemma 2.} \textit{Given a discrete 2D rotation group , where , then 3DCCN is an equivariant map for the 2D rotation group .} 

\paragraph{Proof:} The proposed 3D cylindrical convolution can be formulated as a set of convolution filter  on the cylindrical feature maps : 


where ,  and  denote radial distance, azimuth angle and height, respectively.  is the number of channels in feature map. 


Suppose a group action  in  operating on cylindrical feature maps , we have . To clarify,  the -transformed feature maps  at the coordinate  is equivalent to find the value  in the original feature map  at the coordinate . Leaving out the summation over feature maps for clarity, we have:

Using the substitution , then Eq.~\ref{Supp-eq8} can be transformed into:

which completes our proof that 3DCCN is an equivariant map for the 2D rotation group .


\begin{figure*}[thb]
	\begin{center}
		\includegraphics[width=1.0\linewidth]{figs/Network_Architecture.pdf}
	\end{center}
	\caption{Detailed architecture of our proposed 3D cylindrical convolution networks.}
	\label{fig:supp_network_architecture}
\end{figure*}

\section{Detailed Network Architecture}
\qysupp{Using 3D Cylindrical Convolution (3D-CCN) as a basic operator, we build a hierarchical learning architecture as depicted in Figure \ref{fig:supp_network_architecture}. To ensure the reproducibility of our framework, we also provide detailed information on the kernel size, stride, and the number of filters in this figure. A number of cylindrical convolution layers are stacked together to progressively learn descriptive, yet compact local feature representations. In particular, the maximum number of channels used in our cylindrical feature map is 128, which is much smaller than 1024 used in D3Feat \cite{bai2020d3feat}. This further makes our network very lightweight and less prone to overfitting.}

\section{Detailed Evaluation Metrics}
\qysupp{We further provide the detailed evaluation metrics used in our experiments (Sec.~\ref{Experiments}).}
\medskip\\
{\bf Evaluation Metrics on 3DMatch and ETH.}\quad \qysupp{We adopt Feature Matching Recall () as the main evaluation metric to evaluate the performance of the learned descriptors. Similar to~\cite{choi2015robust,Deng2018,Deng2018a,choy2019fully}, we also provide a formal definition for each metric as follows.}

\qysupp{First, suppose there are a total of  pairs of fragments in the 3DMatch dataset, where the overlap is greater than 30\%. Each pair of fragments  and  can be aligned by the ground-truth rigid transformation . Then, we randomly select  points from the two point clouds to obtain  and . In particular, a set of point correspondences  between  and  is also generated by applying nearest neighbor search  in the feature space :
}

\qysupp{Then the average feature matching recall on the 3DMatch dataset is defined as:}


\noindent \qysupp{where ,  denotes the Euclidean distance,  
and  is the inlier distance threshold and inliner ratio threshold, respectively.  is the indicator function.  denotes a set of point correspondences between  and . In particular, it is generated by applying nearest neighbor search  in the feature space :}


\begin{comment}
\qysupp{Accordingly, inlier ratio  is defined as:}


\qysupp{Different from FMR and IR metrics, which are mainly used to evaluate the quality of matching pairs, the registration recall is introduced to evaluate the performance of descriptors in an entire registration pipeline. }

\qysupp{where  is a set of  ground-truth corresponding points between  and , and  is the rigid transformation matrix estimated by using RANSAC~\cite{fischler1981random}.  is the Mean Squared Error (MSE) threshold. Following~\cite{bai2020d3feat}, we perform up to 50,000 iterations on the initial correspondence set in our experiment to estimate the rigid transformations, and set the MSE threshold to 0.2.}
\end{comment}


\noindent{\bf Evaluation Metrics on KITTI.}\quad \qysupp{Different from the indoor 3DMatch dataset, the evaluation metrics on the KITTI dataset are Relative Translational Error (), Relative Rotation Error (), and Success Rate (), respectively. According to the definitions in~\cite{ma2016fast,Yew2018,choy2019fully}, for a pair of fragments  and , the relative rotation error  is calculated as:}

\qysupp{where  and  denote the ground-truth and the estimated rotation matrix, respectively. Analogously, the relative translation error  can be calculated by:}

\qysupp{where  and  denote the ground-truth and the estimated translation matrix, respectively. Finally, success rate  is defined as:}



\begin{table*}[thb]
	\centering
	\resizebox{1.0\textwidth}{!}{\begin{tabular}{lccccccccccc}
        \toprule[1.0pt]
			& FPFH~\cite{rusu2009fast}& SHOT~\cite{Tombari2010shot}& 3DMatch~\cite{Zeng2017}& CGF~\cite{Khoury2017} & PPFNet~\cite{Deng2018}& PPF-FoldNet~\cite{Deng2018a}& PerfectMatch~\cite{Gojcic2019}  & FCGF~\cite{choy2019fully}& D3Feat~\cite{bai2020d3feat}& LMVD~\cite{li2020end} & Ours      \\
			\midrule               
			Kitchen & 30.6       & 17.8         & 57.5           & 46.1         & 89.7              & 78.7         & 97.0              & -   & -     & \textbf{99.4}   & \underline{99.2} \\
			Home 1  & 58.3        & 37.2        & 73.7          & 61.5       & 55.8        & 76.3    & 95.5           & -  &-    & \textbf{98.7}   & \underline{98.1} \\
			Home 2  & 46.6          & 33.7         & 70.7          & 56.3       & 59.1           & 61.5            & 89.4        & -   & -         & \underline{94.7}   & \textbf{96.2} \\
			Hotel 1 & 26.1     & 20.8        & 57.1     & 44.7       & 58.0         & 68.1               & \underline{96.5}  & -  & -        & \textbf{99.6}   & \textbf{99.6}\\
			Hotel 2 & 32.7       & 22.1        & 44.2           & 38.5       & 57.7        & 71.2   & 93.3        & -  & -       & \textbf{100.0}  & \underline{97.1} \\
			Hotel 3 & 50.0       & 38.9        & 63.0        & 59.3   & 61.1    & 94.4    & \underline{98.2} & -   & -     & \textbf{100.0}   & \textbf{100.0} \\
			Study   & 15.4       & 7.2        & 56.2          & 40.8      & 53.4     & 62.0     & 94.5  & - & -   & \underline{95.5} & \textbf{95.6}\\
			MIT Lab & 27.3   & 13.0   & 54.6    & 35.1   & 63.6    & 62.3    & 93.5    & -  & -    & \underline{92.2} & \textbf{94.8} \\
			\midrule          
			\textbf{Average} & 35.9  & 23.8     & 59.6   & 47.8        & 62.3    & 71.8  & 94.7  & 95.2  & 95.8  & \underline{97.5}  & \textbf{97.6} \\
			\textbf{STD} & 13.4  & 10.9     & 8.8   & 9.4  & 10.8  & 10.5    & \underline{2.7}  & 2.9  & 2.9  & 2.8  & \textbf{1.9} \\
        \toprule[1.0pt]
		\end{tabular}
	}
	\caption{Average recall (\%) {of different methods on the 3DMatch benchmark with cm and }. The symbol '-' means the results are unavailable and  means the results are reported from~\cite{Deng2018a}, which is different from Table~\ref{tab:3dmatch}. }
	\label{tab:supp_3dmatch_recall_origin}
\end{table*}

\begin{table*}[thb]
	\centering
	\resizebox{1.0\textwidth}{!}{\begin{tabular}{lccccccccccc}
        \toprule[1.0pt]
			& FPFH~\cite{rusu2009fast}& SHOT~\cite{Tombari2010shot}& 3DMatch~\cite{Zeng2017}& CGF~\cite{Khoury2017} & PPFNet~\cite{Deng2018}& PPF-FoldNet~\cite{Deng2018a}& PerfectMatch~\cite{Gojcic2019}  & FCGF~\cite{choy2019fully}& D3Feat~\cite{bai2020d3feat}& LMVD~\cite{li2020end} & Ours      \\
			\midrule               
			Kitchen & 29.1       & 17.8         & 0.4           & 44.7         & 0.2              & 78.9         & \underline{97.2}              & -   & -     & -   & \textbf{99.0} \\
			Home 1  & 59.0        & 35.6        & 1.3         & 66.7      & 0.0        & 78.2    & \underline{96.2}           & -  & -    & -   & \textbf{98.7} \\
			Home 2  & 47.1          & 33.7         & 3.4          & 52.9       & 1.4           & 64.4            & \underline{90.9}        & -   & -         & -   & \textbf{96.2} \\
			Hotel 1 & 30.1     & 21.7        & 0.4     & 44.3       & 0.4         & 67.7               & \underline{96.5}  & -  & -        & -  & \textbf{99.6}\\
			Hotel 2 & 30.0       & 24.0        & 0.0          & 44.2       & 0.0       & 62.9   & \underline{92.3}        & -  & -       & -  & \textbf{97.1} \\
			Hotel 3 & 51.9       & 33.3        & 1.0        & 63.0   & 0.0    & 96.3    & \underline{98.2} & -   & -     & -   & \textbf{100.0} \\
			Study   & 15.8       & 8.2        & 0.0         & 41.8      & 0.0     & 62.7     & \underline{94.5}  & - & -   & - & \textbf{94.9}\\
			MIT Lab & 41.6   & 62.3   & 3.9    & 45.5              & 0.0    & 67.5    & \underline{93.5}    & -  & -    & - & \textbf{94.8} \\
			\midrule          
			\textbf{Average} & 36.4  & 23.4     & 1.1   & 49.9        & 0.3    & 73.1  & 94.9  & 95.3  & 95.5  & \underline{96.9}  & \textbf{97.5} \\
			\textbf{STD} & 13.6  & 9.5     & 1.2   & 8.9  & 0.5  & 10.4    & 2.5  & 3.3  & 3.5  & -  & 1.9 \\
        \toprule[1.0pt]
		\end{tabular}
	}
	\caption{Average recall (\%) {of different methods on the rotated 3DMatch benchmark with cm and }. The symbol '-' means the results are unavailable and  means the results are reported from~\cite{Deng2018a}, which is different from Table~\ref{tab:3dmatch}. }
	\label{tab:supp_3dmatch_recall_rotate}
\end{table*}


\section{Implementation Details}
\qysupp{Here we provide extra implementation details in this section. The detailed hyperparameter settings of our \nickname{} on different datasets are listed in Table \ref{tab:hyperparameter}. In particular, we keep the same parameter settings as the training dataset when generalized to unseen datasets, except for the support radius  and query radius , due to the varying point densities in different datasets. Specifically, we follow the scheme in D3Feat \cite{bai2020d3feat} to adaptively adjust the radius according to the ratio.
}
\begin{table}[thb]
\begin{center}
\scalebox{0.9}{
\begin{tabular}{rcccccc}
\toprule[1.0pt]
Dataset &  &   &   &   &   &  \\
\midrule[0.75pt]
3DMatch \cite{Zeng2017} & 9 & 40 & 80 & 0.3m & 0.04m & 30   \\
KITTI \cite{geiger2012we}   & 9 & 30 & 60 & 2.0m & 0.30m & 30   \\
ETH \cite{pomerleau2012challenging} & 9 & 40 & 80 & 0.8m & 0.10m & 30   \\
\toprule[1.0pt]
\end{tabular}
}
\end{center}
\caption{The hyperparameters set by our method in different datasets.}
\label{tab:hyperparameter}
\end{table}

\section{Additional Results on 3DMatch}
\begin{comment}
\paragraph{Performance under different number of keypoints.} Here, we further report the detailed registration results of our \nickname{} on the 3DMatch dataset.

\begin{table}[htb]
	\begin{center}
	\scalebox{0.9}{
	\begin{tabular}{rccccc}
        \Xhline{2\arrayrulewidth}
		\#\textbf{Sampled points}      & \textbf{5000}     & \textbf{2500}     & \textbf{1000}    & \textbf{500} & \textbf{250} \\ \hline
		\multicolumn{6}{c}{Inlier Ratio (\%)}                    \\ \hline
		PerfectMatch~\cite{Gojcic2019}     & 37.7     & 34.5     & 28.3    & 23.0  & 19.1       \\
		FCGF~\cite{choy2019fully}               & 56.9     & 54.5     & 49.1    & 43.3 & 34.7       \\
		D3Feat-rand~\cite{bai2020d3feat}     & 40.6     & 38.3     & 33.3    & 28.6  & 23.5       \\
		D3Feat-pred~\cite{bai2020d3feat}     & 40.7     & 40.6     & 42.7    & 44.1  & 45.0       \\
		\textbf{\nickname{} (Ours)}            & 48.8          & 46.0         & 40.8        & 35.3  & 29.3      \\ \hline
		\multicolumn{6}{c}{Registration Recall (\%)}             \\ \hline
		PerfectMatch~\cite{Gojcic2019}     & 80.3     & 77.5     & 73.4    & 64.8   & 50.9       \\
		FCGF~\cite{choy2019fully}     & 87.3     & 85.8     & 85.8    & 81.0  & 73.0       \\
		D3Feat-rand~\cite{bai2020d3feat}     & 83.5     & 82.1     & 81.7    & 77.6   & 68.8      \\
		D3Feat-pred~\cite{bai2020d3feat}     & 82.2     & 84.4     & 84.9    & 82.5  & 79.3       \\
		\textbf{\nickname{} (Ours)}            & 86.2          & 85.5         & 84.0  & 79.3      & 71.2       \\         \Xhline{2\arrayrulewidth}
	\end{tabular}
	}
	\end{center}
\caption{\qy{Quantitative results achieved by different approaches on the 3DMatch dataset under different numbers of sampling (key) points.}}
\label{tab:detailed_key_points}
\end{table}
\end{comment}

\qysupp{For comparison, we also report the detailed quantitative results of our \nickname{} on the 3DMatch dataset in Table \ref{tab:supp_3dmatch_recall_origin} and the rotated 3DMatch dataset in Table \ref{tab:supp_3dmatch_recall_rotate}.}


\section{Additional qualitative results.}
\qysupp{As illustrated in Sec. \ref{Ex:generalization}, our \nickname{} has demonstrated  superior quantitative generalization performance across different datasets with different sensor modalities. Here, we further show additional qualitative results in this section.}

\smallskip\noindent\textbf{Additional qualitative results on the 3DMatch dataset.} \qysupp{We first show the additional qualitative results achieved by FCGF \cite{choy2019fully}, D3Feat \cite{bai2020d3feat}, and our \nickname{} on the 3DMatch dataset in Fig. \ref{fig:supp_qualitative_3DMatch}. It can be seen that the FCGF and D3Feat are prone to mismatching the fragments when the two input partial scans have relatively significant differences. However, our simple \nickname{} can always achieve consistent registration performance on this dataset, despite only being trained on the outdoor KITTI dataset with sparse LiDAR point clouds.}


\smallskip\noindent\textbf{Additional qualitative results on the KITTI dataset.} \qysupp{Then, we show the extra qualitative results achieved by FCGF \cite{choy2019fully}, D3Feat \cite{bai2020d3feat}, and our \nickname{} on the KITTI dataset in Fig. \ref{fig:supp_qualitative_KITTI}. We can clearly see that the point cloud in the KITTI dataset is significantly different from the point cloud in 3DMatch, since the KITTI dataset is mainly composed of \textit{large-scale}, \textit{sparse}, and \textit{partial} LiDAR scans. As shown in Figure, FCGF and D3Feat tend to misalign the input fragments when the scene contains lots of geometrically-similar objects (\textit{e.g.,} cars). However, our method can still achieve satisfactory registration results when only trained on the indoor 3DMatch dataset. This further validates the superior generalization ability of our method.}

\smallskip\noindent\textbf{Additional qualitative results on the ETH dataset.} \qysupp{We finally show the extra qualitative results achieved by FCGF \cite{choy2019fully}, D3Feat \cite{bai2020d3feat}, and our \nickname{} on the ETH dataset in Fig. \ref{fig:supp_qualitative_ETH}. Compared with the 3DMatch and KITTI data sets, the ETH dataset is collected by static terrestrial lasers in outdoor scenes, and is mainly composed of bushes and vegetation. As shown in Figure, it is highly challenging for FCGF and D3Feat to successfully align the input scans together, since this dataset suffers from issues such as noise, clutter, and occlusions. Nevertheless, the proposed \nickname{} can still achieve excellent performance on this dataset.}


\begin{figure*}[thb]
	\begin{center}
		\includegraphics[width=1.0\linewidth]{figs/Fig_3DMatch_qualitative_supp.pdf}
	\end{center}
	\caption{\qysupp{Additional qualitative results achieved by FCGF \cite{choy2019fully}, D3Feat \cite{bai2020d3feat}, and our \textbf{\nickname{}} on the 3DMatch dataset. Note that, all methods are only trained on the outdoor KITTI \cite{geiger2012we} dataset. Red boxes/circles show the failure cases.}}
	\label{fig:supp_qualitative_3DMatch}
\end{figure*}


\begin{figure*}[thb]
	\begin{center}
		\includegraphics[width=1.0\linewidth]{figs/Fig_KITTI_supp.pdf}
	\end{center}
	\caption{\qysupp{Additional qualitative results achieved by FCGF \cite{choy2019fully}, D3Feat \cite{bai2020d3feat}, and our \textbf{\nickname{}} on the KITTI dataset. Note that, all methods are only trained on the indoor 3DMatch~\cite{Zeng2017} dataset.  Red boxes show the failure cases.}}
	\label{fig:supp_qualitative_KITTI}
\end{figure*}



\begin{figure*}[thb]
	\begin{center}
		\includegraphics[width=1.0\linewidth]{figs/Fig_ETH_supp.pdf}
	\end{center}
	\caption{\qysupp{Additional qualitative results achieved by FCGF \cite{choy2019fully}, D3Feat \cite{bai2020d3feat}, and our \textbf{\nickname{}} on the ETH dataset. Note that, all methods are only trained on the indoor 3DMatch \cite{Zeng2017} dataset.  Red boxes/circles show the failure cases.}}
	\label{fig:supp_qualitative_ETH}
\end{figure*}









 

 \end{document}
