\section{Experimental Results}
\label{Section:Results}

In this section, we first describe the experiment setup in Section~\ref{Subsection:Analysis}. Then, we compare our method with current state-of-the-art 3D object detection methods quantitatively, and analyze our results in Section~\ref{Subsection:Analysis}, where we show the importance of using geometric primitives and discuss our advantages. Finally, we show ablation results in Section~\ref{Subsection:Ablation:Study} and qualitative comparison in Figures~(\ref{Figure:Qualitative:Result:Sun}) and (\ref{Figure:Qualitative:Result:Scan}). More results and discussions can be found in the supplemental material.

\subsection{Experimental Setup}
\label{Subsection:Setup}

\noindent\textbf{Datasets.} We employ two popular datasets ScanNet V2\cite{Dai_2017_CVPR_scannet} and SUN RGB-D V1\cite{song2015sun}. 
ScanNet is a dataset of richly-annotated 3D reconstructions of indoor scenes. It contains 1513 indoor scenes annotated with per-point instance and semantic labels for 40 semantic classes. SUN RGB-D is a single-view RGB-D dataset for 3D scene understanding, which contains 10335 indoor RGB and depth images with per-point semantic labels and object bounding boxes. For both datasets, we use the same training/validation split and BB semantic classes (18 classes for ScanNet and 10 classes for SUN RGB-D) as in VoteNet\cite{qi2019votenet} and sub-sample 40000 points from every scene. 

\noindent\textbf{Evaluation protocol.} We use Average Precision(AP) and the mean of AP across all semantic classes (mAP)\cite{song2015sun}  under different IoU values (the minimum IoU to consider a positive match). Average precision computes the average precision value for recall value over 0 to 1. IoU is given by the ratio of the area of intersection and area of union of the predicted bounding box and ground truth bounding box. Specifically, we use AP/mAP@0.25 and AP/mAP@0.5. 



\begin{table*}[t!]
\caption{3D object detection results on ScanNet V2 val dataset. We show per-category results of average precision (AP) with 3D IoU threshold 0.25 as proposed by \cite{song2015sun}, and mean of AP across all semantic classes with 3D IoU threshold 0.25.} 
\begin{adjustbox}{width=\columnwidth,center}
 \begin{tabular}{l | c | c  c  c  c  c  c  c  c  c  c  c  c  c  c  c  c  c  c | c } 
 \hline
 & RGB & cab & bed & chair & sofa & tabl & door & wind & bkshf & pic & cntr & desk & curt & fridg & showr & toil & sink & bath & ofurn & mAP\\
 \hline
 3DSIS-5\cite{Hou_2019_CVPR_3D-SIS} & \cmark & 19.8 & 69.7 & 66.2 & 71.8 & 36.1 & 30.6 & 10.9 & 27.3 & 0.0 & 10.0 & 46.9 & 14.1 & 53.8 & 36.0 & 87.6 & 43.0 & 84.3 & 16.2 & 40.2 \\ 
3DSIS\cite{Hou_2019_CVPR_3D-SIS} & \xmark & 12.8 & 63.1 & 66.0 & 46.3 & 26.9 & 8.0 & 2.8 & 2.3 & 0.0 & 6.9 & 33.3 & 2.5 & 10.4 & 12.2 & 74.5 & 22.9 & 58.7 & 7.1 & 25.4  \\
Votenet\cite{qi2019votenet} &\xmark& 36.3 & 87.9 & 88.7 & 89.6 & 58.8 & 47.3 & 38.1 & 44.6 & 7.8 & 56.1 & 71.7 & 47.2 & 45.4 & 57.1 & 94.9 & 54.7 & 92.1 & 37.2 & 58.7 \\
\hline
 Ours & \xmark & \textbf{49.4} & \textbf{88.6} & \textbf{91.8} & \textbf{90.2} & \textbf{64.9} & \textbf{61.0} & \textbf{51.9} & \textbf{54.9} & \textbf{18.6} & \textbf{62.0} & \textbf{75.9} & \textbf{57.3} & \textbf{57.2} & \textbf{75.3} & \textbf{97.9} & \textbf{67.4} & \textbf{92.5} & \textbf{53.6} & \textbf{67.2} \\
\hspace{0.1cm}\small{w\textbackslash o refine} & \xmark &  37.2 &
 89.3 & 88.4 & 88.5 & 64.4 & 53.0 & 44.2 & 42.2 & 11.1 & 51.2 & 59.8 &
 47.0 & 54.3 & 74.3 & 93.1 & 57.0 & 85.6 & 43.5 & 60.2 \\
 \hline
\end{tabular}
\label{Table:Quantitative:Result:ScanNetCat}
\end{adjustbox}
\end{table*}

\begin{figure*}[b!]
\includegraphics[width=1.0\textwidth]{Figures/compare_refinement.jpg}
\caption{\small{Effect of geometric primitive matching and refinement.}}
\label{Figure:Qual:Geometric:Constraints}
\end{figure*}

\noindent \textbf{Baseline Methods} We compare H3DNet with STAR approaches: VoteNet~\cite{qi2019votenet} is a geometric-only detector that combines deep point set networks and a voting procedure. GSPN\cite{yi2019gspn} uses a generative model for instance segmentation. Both 3D-SIS~\cite{Hou_2019_CVPR_3D-SIS} and DSS~\cite{Song_2016_CVPR} extract features from 2D images and 3D shapes to generate object proposals. F-PointNet~\cite{qi2018frustum} and 2D-Driven~\cite{Lahoud_2017_ICCV} first propose 2D detection regions and project them to 3D frustum for 3D detection. Cloud of gradient(COG)~\cite{Ren_2016_CVPR} integrates sliding windows with a 3D HoG-like feature. 











\begin{table*}[t!]
\caption{\textbf{Left:} 3D object detection results on ScanNetV2 val set. \textbf{Right:} results on SUN RGB-D V1 val set. We show mean of average precision (mAP) across all semantic classes with 3D IoU threshold 0.25 and 0.5. }
\begin{minipage}{0.49\linewidth}
\begin{adjustbox}{width=0.99\columnwidth, center}
\centering
 \begin{tabular}{l | c | c | c  } 
 \hline
 & Input  & mAP@0.25 & mAP@0.5  \\ 
 \hline
 DSS\cite{Song_2016_CVPR} & Geo  RGB & 15.2 & 6.8 \\ 
 F-PointNet\cite{qi2018frustum} & Geo + RGB & 19.8 & 10.8 \\
 GSPN\cite{yi2019gspn} & Geo + RGB & 30.6 & 17.7\\
 3D-SIS \cite{Hou_2019_CVPR_3D-SIS} & Geo + 5 views & 40.2 & 22.5 \\
 VoteNet \cite{qi2019votenet} & Geo only & 58.7 & 33.5 \\
 \hline 
 Ours & Geo only & \textbf{67.2} & \textbf{48.1} \\ 
 \hspace{0.1cm}\small{w\textbackslash o refine} & Geo only & 60.2 & 37.3 \\ 
 \hline
\end{tabular}
\end{adjustbox}
\end{minipage}
\begin{minipage}{0.49\linewidth}
\begin{adjustbox}{width=0.94\columnwidth, center}
\centering
 \begin{tabular}{l | c | c | c  } 
 \hline
 & Input  & mAP@0.25 & mAP@0.5  \\
 \hline
 DSS\cite{Song_2016_CVPR} & Geo + RGB & 42.1 & - \\ 
 COG\cite{Ren_2016_CVPR} & Geo + RGB & 47.6 & -\\ 
 2D-driven\cite{Lahoud_2017_ICCV} & Geo + RGB &  45.1 & - \\
 F-PointNet\cite{qi2018frustum} & Geo + RGB & 54.0 & -\\
 VoteNet \cite{qi2019votenet} & Geo only & 57.7 &  32.9 \\
 \hline 
 Ours & Geo only & \textbf{60.1} & \textbf{39.0}\\
 \hspace{0.1cm}\small{w\textbackslash o refine} & Geo only & 58.5 & 34.2 \\
 \hline
\end{tabular}
\end{adjustbox}
\end{minipage}
\label{Table:Quantitative:Result:All}
\end{table*}

\iffalse
\begin{table*}
\begin{adjustbox}{width=\columnwidth,center}
 \begin{tabular}{l | c | c  c  c  c  c  c  c  c  c  c | c | c  } 
 \hline
 & RGB & bathtub & bed & bkshf & chair & desk & drser & nigtstd &sofa &table & toilet & mAP.25 & mAP.5  \\
 \hline
 DSS\cite{Song_2016_CVPR} & \cmark& 44.2 & 78.8 & 11.9 & 61.2 & 20.5 & 6.4& 15.4& 53.5& 50.3& 78.9 & 42.1 & - \\ 
COG\cite{Ren_2016_CVPR} & \cmark & 58.3 & 63.7 & 31.8 & 62.2 & 45.2 & 15.5 & 27.4& 51.0 & 51.3 & 70.1 & 47.6 & -\\ 
2D-driven\cite{Lahoud_2017_ICCV} & \cmark & 43.5 & 64.5 & 31.4 & 48.3 & 27.9 & 25.9 & 41.9 & 50.4 & 37.0 & 80.4 & 45.1 & - \\
F-PointNet\cite{Qi_2018_CVPR_Frustum_PointNets} & \cmark & 43.3 & 81.1 & 33.3 & 64.2 & 24.7 & 32.0 & 58.1 & 61.1 & 51.1 & 90.9 & 54.0 & -\\
VoteNet \cite{qi2019votenet} &\xmark & 74.7 & 83.0 & 28.8 & 75.3 & 22.0 & 29.8 & 62.2 & 64.0 & 47.3 & 90.1 & 57.7 & 32.0 \\
\hline 
 Ours & \xmark & 73.8 & 85.6 & 31.0 & 76.7 & 29.6 & 33.4 & 65.5 & 66.5 & 50.8 & 88.2 & 60.1 & 39.0\\
\hspace{0.1cm}\small{w\textbackslash o refine} & \xmark & 74.1 & 86.4 & 31.3 & 76.1 & 27.1 & 26.3 & 57.9 & 64.9 & 51.6 & 89.3 & 58.5 & 34.2 \\

 \hline
\end{tabular}
\end{adjustbox}
\caption{3D object detection results on SUN RGB-D val dataset. We show per-category results of average precision (AP) with 3D IoU threshold 0.25 as proposed by \cite{song2015sun}, and mean of AP across all semantic classes with 3D IoU threshold 0.25 and 0.5.  Note that both COG \cite{Ren_2016_CVPR} and 2D-driven \cite{Lahoud_2017_ICCV} use room layout context to boost performance. For fair comparison, with previous methods, the evaluation is on the SUN RGB-D V1 data.}
\label{Table:Quantitative:Result:SUN}
\end{table*}
\fi






\iffalse
\begin{table*}
\caption{3D object detection results on ScanNet v2 val dataset. We show per-category results of average precision (AP) with 3D IoU threshold 0.5 as proposed by \cite{song2015sun}, and mean of AP across all semantic classes with 3D IoU threshold 0.5.} 
\begin{adjustbox}{width=\columnwidth,center}
 \begin{tabular}{l | c | c  c  c  c  c  c  c  c  c  c  c  c  c  c  c  c  c  c | c } 
 \hline
 & RGB & cab & bed & chair & sofa & tabl & door & wind & bkshf & pic & cntr & desk & curt & fridg & showr & toil & sink & bath & ofurn & mAP\\
 \hline
 3DSIS-5\cite{Hou_2019_CVPR_3D-SIS} & \cmark & 5.73 & 50.28 & 52.59 & 55.43 & 21.96 & 10.88 & 0.00 & 13.18 & 0.00 & 0.00 & 23.62 & 2.61 & 24.54 & 0.82 & 71.79 & 8.94 & 56.40 & 6.87 & 22.53 \\ 
3DSIS\cite{Hou_2019_CVPR_3D-SIS} & \xmark & 5.06 & 42.19 & 50.11 & 31.75 & 15.12 & 1.38 & 0.00 & 1.44 & 0.00 & 0.00 & 13.66 & 0.00 & 2.63 & 3.00 & 56.75 & 8.68 & 28.52 & 2.55 & 14.60 \\
Votenet\cite{qi2019votenet} &\xmark& 8.1 & 76.1 & 67.2 & 68.8 & 42.4 & 15.3 & 6.4 & 28.0 & 1.3 & 9.5 & 37.5 & 11.6 & 27.8 & 10.0 & 86.5 & 16.8 & 78.9 & 11.7 & 33.5 \\
\hline
 Ours & \xmark & \textbf{20.5} & \textbf{79.7} & \textbf{80.1} & \textbf{79.6} & \textbf{56.2} & \textbf{29.0} & \textbf{21.3} & \textbf{45.5} & \textbf{4.2} & \textbf{33.5} & \textbf{50.6} & \textbf{37.3} & \textbf{41.4} & \textbf{37.0} & \textbf{89.1} & \textbf{35.1} & \textbf{90.2} & \textbf{35.4} & \textbf{48.1} \\
\hspace{0.1cm}\small{w\textbackslash o refine} & \xmark & 12.4 & 80.8 & 69.3 & 71.8 & 42.6 & 19.5 & 12.3 & 26.1 &2.4 & 15.7 & 27.3 & 32.6 & 29.5 & 33.6 & 79.1 & 23.0 & 74.0 & 18.9 & 37.3\\
 \hline
\end{tabular}
\end{adjustbox}
\label{Table:Quantitative:Result:ScanNetS}
\end{table*}
\fi
















\subsection{Analysis of Results}
\label{Subsection:Analysis}

\begin{figure*}[b!]
\includegraphics[width=1.0\textwidth]{Figures/compare_scannet.jpg}
\caption{Qualitative baseline comparisons on ScanNet V2.}
\label{Figure:Qualitative:Result:Scan}
\end{figure*}

\begin{figure*}[t!]
\includegraphics[width=1.0\textwidth]{Figures/compare_sunrgbd.jpg}
\caption{Qualitative baseline comparisons on SUN RGB-D.}
\label{Figure:Qualitative:Result:Sun}
\end{figure*}

As shown in Table \ref{Table:Quantitative:Result:All}, our approach leads to an average mAP score of 67.2\%, with 3D IoU threshold 0.25 (mAP@0.25), on ScanNet V2, which is 8.5\% better than the top-performing baseline approach~\cite{qi2019votenet}. In addition, our approach is 14.6\% better than the baseline approach~\cite{qi2019votenet} with 3D IoU threshold 0.5 (mAP@0.5). For SUN RGB-D, our approach gains 2.4\% and 6.1\% in terms of mAP, with 3D IoU threshold 0.25 and 0.5 respectively. 
On both datasets, the performance gains of our approach under mAP@0.5 are larger than those under mAP@0.25, meaning our approach offers more accurate predictions than baseline approaches. Such improvements are attributed to using an overcomplete set of geometric primitives and their associated features for generating and refining object proposals. We can also understand the relative less salient improvements on SUN RGB-D than ScanNet in a similar way, i.e., labels of the former are less accurate than the latter, and the strength of H3DNet is not fully utilized on SUN RGB-D. Except for the classification and refinement module, our approach shares similar computation pipeline and complexity with VoteNet. The computation on multiple descriptor towers and proposal modules can be paralleled, which should not increase computation overhead. In our implementation, our approach requires 0.058 seconds for the last module per scan. Conceptually, our approach requires 50\% more time compared to~\cite{qi2019votenet} but operates with a higher detection accuracy.

\noindent\textbf{Improvement on thin objects.} One limitation of the current top-performing baseline~\cite{qi2019votenet} is predicting thin objects in 3D scenes, such as doors, windows and pictures. In contrast, with face and edge primitives, H3DNet is able to extract better features for those thin objects. For example, the frames of window or picture provide dense edge feature, and physical texture of curtain or shower-curtain provide dense face/surface feature. As shown in Table \ref{Table:Quantitative:Result:ScanNetCat}, H3DNet leads to significant performance gains on thin objects, such as door (13.7\%), window (13.8\%), picture (10.8\%), curtain (10.1\%) and shower-curtain (18.2\%).

\noindent\textbf{Improvement on objects with dense geometric primitives.}
Across the individual object classes in ScanNet in Table \ref{Table:Quantitative:Result:ScanNetCat}, other than those thin objects, our approach also leads to significant performance gain on cabinet (13.1\%), table (6.1\%), bookshelf (10.3\%), refrigerator (11.8\%), sink (12.7\%) and other-furniture (16.4\%). One explanation is that the geometric shapes of these object classes possess rich planar structures and/or distinct edge structures, which contribute greatly on geometric primitive detection and object refinement.

\begin{table*}[b!]
\begin{minipage}{0.48\textwidth}
  \centering
  \includegraphics[width=0.8\textwidth]{Figures/primitives.pdf}
  \captionof{figure}{Quantitative comparisons between VoteNet, our approach, ours with only face primitive and ours with only edge primitive, across sampled categories for ScanNet.}
  \label{Figure:Ablation:Primitive}
\end{minipage}
\hfill
\begin{minipage}{0.48\textwidth}
    \caption{Quantitative results without refining predicted center, size, semantic or object existence score for ScanNet, and without refining predicted angle for SUN RGB-D and differences compared with refining all.}
    \centering
  \begin{tabular}{l | c | c | c | c } 
 \hline
  & \multicolumn{2}{c}{mAP@0.25} & \multicolumn{2}{|c}{mAP@0.5} \\
 \hline
  w\textbackslash o center & 66.9 & -0.3 & 46.3 & -1.8\\ 
  w\textbackslash o size & 65.4 & -1.8 & 44.2 & -3.9\\ 
  w\textbackslash o semantic & 66.2 & -1.0 & 47.3 & -0.8\\ 
  w\textbackslash o existence & 65.2 & -1.8 & 45.1 & -3.0\\ 
  \hline\hline
  w\textbackslash o angle & 58.6 & -1.5 & 36.6 & -2.4 \\
  \hline
 \end{tabular}
\label{Table:Ablation:Refinement}
\end{minipage}
\end{table*}

\noindent\textbf{Effect of primitive matching and refinement.} Using a distance function to refine object proposals and aggregating features of matching primitives are crucial for H3DNet. On ScanNet, merely classifying the initial proposals results in a 14.6\% drop on mAP 0.5. Figure~\ref{Figure:Qual:Geometric:Constraints} shows qualitative object detection results, which again justify the importance of optimizing and refining object proposals. 

















\subsection{Ablation Study}
\label{Subsection:Ablation:Study}

\noindent\textbf{Effects of using different geometric primitives.} H3DNet can utilize different groups of geometric primitives for generating, classifying, and refining object proposals. Such choices have profound influences on the detected objects. As illustrated in Figure~\ref{Figure:Ablation:Primitive}, when only using BB edge primitives, we can see that objects with prominent edge features, i.e., window, possess accurate predictions. In contrast, objects with dense face/surface features, such as shower curtain, exhibit relative low prediction accuracy. However, these objects can be easily detected by activating BB face primitives. H3DNet, which combines BB centers, BB edge centers, and BB face centers, adds the strength of their generalization behaviors together. The resulting performance gains are salient when compared to using a single set of geometric primitives.  



\noindent\textbf{Effects of proposal refinement.} During object proposal refinement, object center, size, heading angle, semantic and existence are all optimized. As shown in Table \ref{Table:Ablation:Refinement}, without fine-tuning any of the geometric parameters of the detected objects, the performance drops, which shows the importance of this sub-module. 

\begin{table*}[t!]
\begin{minipage}{0.48\textwidth}
  \centering
  \includegraphics[width=0.8\textwidth]{Figures/threshold.pdf}
  \captionof{figure}{Quantitative comparisons between different truncation threshold  for ScanNet.}
  \label{Figure:Ablation:Threshold}
\end{minipage}
\hfill
\begin{minipage}{0.48\textwidth}
    \caption{Quantitative comparisons between different number of descriptor computation towers, among our approach and VoteNet, for ScanNet and SUN RGB-D.}
    \centering
  \begin{tabular}{c | c | c | c  } 
 \hline
 & \# of Towers  & mAP@0.25 & mAP@0.5 \\
 \hline
 \multirow{4}{2.2em}{Ours} 
  & 1 & 64.4 & 43.4 \\ 
  & 2 & 65.4 & 46.2 \\ 
  & 3 & 66.0 & 47.7 \\ 
  & 4 & 67.2 & 48.3 \\ 
 \hline
 \multirow{2}{2.2em}{Vote} & 4 (Scan) &  60.11 & 37.12 \\ 
 & 4 (SUN) & 57.5 & 32.1 \\
 \hline
 \end{tabular}
\label{Table:Ablation:FeatureExtraction}
\end{minipage}
\end{table*}

\noindent\textbf{Effect of different truncation threshold} As shown in Figure \ref{Figure:Ablation:Threshold}, with different truncation values of , results with mAP@0.25 and mAP@0.5 remain stable. It shows that our model is robust to different truncation threshold . 

\noindent\textbf{Effect of multiple descriptor computation towers.} One hyper-parameter of H3DNet is the number of descriptor computation towers. Table \ref{Table:Ablation:FeatureExtraction} shows that adding more descriptor computation towers leads to better results, yet the performance gain of adding more descriptor computation towers quickly drops. Moreover, the performance gain of H3DNet from VoteNet comes from the hybrid set of geometric primitives and object proposal matching and refinement. For example, replacing the descriptor computation tower of VoteNet by the four descriptor computation towers of H3DNet only results in modest and no performance gains on ScanNet and SUN RGB-D, respectively (See Table \ref{Table:Ablation:FeatureExtraction}). 

