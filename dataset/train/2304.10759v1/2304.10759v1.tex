

\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}              \usepackage[accsupp]{axessibility}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{bbm}
\usepackage[linesnumbered,ruled]{algorithm2e}
\usepackage{multirow}
\usepackage{booktabs,tabularx}
\usepackage{amsmath}
\usepackage{stfloats}
\usepackage[accsupp]{axessibility}  

\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


\def\cvprPaperID{8991} \def\confName{CVPR}
\def\confYear{2023}

\SetKwInput{KwInput}{Input}                \SetKwInput{KwOutput}{Output}              

\newcommand{\eq}[1]{Eq.\ref{#1}}
\newcommand{\tab}[1]{Table \ref{#1}}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}   

\begin{document}





\title{GeoLayoutLM: Geometric Pre-training for Visual Information Extraction}

\author{
Chuwei Luo$^*$, 
Changxu Cheng$^*$, 
Qi Zheng, 
Cong Yao \\
DAMO Academy, Alibaba Group\\
{\tt\small \{luochuwei,ccx0127,zhengqisjtu,yaocong2010\}@gmail.com}
}
\maketitle
\def\thefootnote{*}\footnotetext{Both authors contributed equally to this work.}
\def\thefootnote{\arabic{footnote}}


\begin{abstract}
  Visual information extraction (VIE) plays an important role in Document Intelligence. Generally, it is divided into two tasks: semantic entity recognition (SER) and relation extraction (RE). Recently, pre-trained models for documents have achieved substantial progress in VIE, particularly in SER. However, most of the existing models learn the geometric representation in an implicit way, which has been found insufficient for the RE task since geometric information is especially crucial for RE. Moreover, we reveal another factor that limits the performance of RE lies in the objective gap between the pre-training phase and the fine-tuning phase for RE. To tackle these issues, we propose in this paper a multi-modal framework, named GeoLayoutLM, for VIE. GeoLayoutLM explicitly models the geometric relations in pre-training, which we call geometric pre-training. Geometric pre-training is achieved by three specially designed geometry-related pre-training tasks. Additionally, novel relation heads, which are pre-trained by the geometric pre-training tasks and fine-tuned for RE, are elaborately designed to enrich and enhance the feature representation.
  According to extensive experiments on standard VIE benchmarks, GeoLayoutLM achieves highly competitive scores in the SER task and significantly outperforms the previous state-of-the-arts for RE (\eg, the F1 score of RE on FUNSD is boosted from 80.35\% to 89.45\%)
  \footnote{https://github.com/AlibabaResearch/AdvancedLiterateMachinery}.
\end{abstract}

\section{Introduction}
\label{sec:intro}

Visual information extraction (VIE) is a critical part in Document AI\cite{cui2021document, Zhu2016SceneTD, Long2018SceneTD}. It has attracted more and more attention from both the academic and industrial community.
VIE involves semantic entity recognition (SER, \textit{a.k.a.} entity labeling) and relation extraction (RE, \textit{a.k.a.} entity linking) from visually-rich documents (VrDs) such as forms and receipts\cite{jaume2019funsd,zhang2021entity,cui2021document,li2021structext,xu2021layoutxlm, Shi2015AnET, Zhou2017EASTAE, wang2022multi}.
Recent years have witnessed the great power of pre-trained multi-modal models \cite{xu2020layoutlm,xu2021layoutxlm,xu2020layoutlmv2,huang2022layoutlmv3,li2021structurallm,li2021structext,li2021selfdoc,appalaraju2021docformer,gu2022unified,wang2022lilt,gu2022xylayoutlm,luo2022bivldoc,hong2022bros} in VIE tasks, especially the SER task.
Compared with SER, the RE task, which aims at predicting the relation between semantic entities in documents, has not been fully explored and remains a challenging problem\cite{li2021structext,hong2022bros}. RE is essential to provide additional structural information closer to human comprehension of the VrDs\cite{zhang2021entity}. It makes the open-layout information extraction possible, e.g., for open-layout key-value linking and form-like items grouping.

\begin{figure}[tp]
  \includegraphics[width=.99\linewidth]{intro_case3.pdf}
  \caption{Incorrect relation predictions by the previous state-of-the-art model LayoutLMv3~\cite{huang2022layoutlmv3}. (a) LayoutLMv3 tends to link two entities relying more on their semantics than the geometric layout, \ie, the entity ``212-450-4785'' is linked to ``Fax Number'' regardless of their relationship in layout. (b) LayoutLMv3 successfully predicts the link in the upper half part but misses the link below, although both links are similar in geometric layout. These two examples clearly show \textbf{the importance of geometric information in relation extraction (RE)}.}
  \label{intro_case}
  \vspace{-2mm}
\end{figure}

\begin{table}[tp]
  \centering
  \begin{tabular}{lccc}
    \toprule[1pt]
    & \textbf{Precision} & \textbf{Recall} & \textbf{F1} \\
    \hline
    LayoutLMv3 & 75.82 & 85.45 & 80.35 \\
    + geometric constraint & \textbf{79.87} & 85.45 & \textbf{82.57} \\
    \bottomrule[1pt]
  \end{tabular}
\caption{The RE performance improvement by introducing a simple geometric restriction (on the FUNSD dataset).}
  \vspace{-3mm}
  \label{intro_v3_geo}
\end{table}

It is widely accepted that document layout understanding is crucial for VIE~\cite{xu2020layoutlm,xu2021layoutxlm,xu2020layoutlmv2,huang2022layoutlmv3,li2021structext,li2021selfdoc,appalaraju2021docformer,gu2022unified,wang2022lilt,gu2022xylayoutlm,luo2022bivldoc}, especially for RE~\cite{li2021structext,hong2022bros}.
The geometric relationships, a specific form for describing document layout, are important for document layout representations\cite{liu2019graph,luo2020merge,li2021structext}.
Most previous pre-trained models for VrDs learn layout representations \textit{implicitly} by adding coordinates into the model inputs, combining the relative position encoding or supervising by alignment-related pre-training tasks like text-image alignment\cite{xu2020layoutlmv2,huang2022layoutlmv3,luo2022bivldoc} and masked vision language modeling\cite{xu2020layoutlm,xu2021layoutxlm,xu2020layoutlmv2,huang2022layoutlmv3,li2021structext,li2021selfdoc,appalaraju2021docformer,gu2022unified,li2021structext,hong2022bros}.
However, it is not guaranteed that the geometric layout information is well learned in these models.
Taking the state-of-the-art model LayoutLMv3 as an example, we find it would make mistakes in certain relatively simple scenarios, where the geometric relations between entities are not complicated.
As shown in \cref{intro_case}, LayoutLMv3 seems to link two entities depending more on the semantics than the geometric layout.
This indicates that its layout understanding is not sufficiently discriminative. To further verify our conjecture, we conduct an experiment by filtering the false positive relations using a simple geometric restriction (the linkings between entities should not point up beyond a certain distance), the precision would increase by a large margin (more than 4 points) while the recall is controlled unchanged, as detailed in \cref{intro_v3_geo}. This experiment proves that LayoutLMv3 does not fully exploit the useful geometric relationship information.
Besides, most existing methods did not directly take the relation modeling into consideration in pre-training.
They usually adopt token/segment-level classification or regression, which might underperform on downstream tasks related to relation modeling.
Therefore, it is necessary to learn a better layout representation for document pre-trained models by modeling the geometric relationships between entities \textit{explicitly} during pre-training.

During RE fine-tuning, previous works usually learn a task head like a single linear or bilinear layer\cite{li2021structext,hong2022bros} from scratch.
On the one hand, since the higher-level pair relationship features, which are beyond the token or text-segment features in documents, are complex, we argue that a single linear or bilinear layer is not always adequate to make full use of the encoded features for RE.
On the other hand, the RE task head initialized randomly is prone to overfitting with limited fine-tuning data.
Since the pre-trained backbone has shown tremendous potential \cite{devlin2018bert, dosovitskiy2020image}, why not pre-train the task head in some way simultaneously? Several works \cite{hu2022p, han2021adaptive,liu2022prompt} have proved that smaller \textit{gap} between pre-training and fine-tuning leads to better performance for downstream tasks. Hence, there is still considerable room for the design and usage of the RE task head.

Based on the above observations, we establish a multimodal pre-trained framework (termed as \textbf{GeoLayoutLM}) for VIE, in which a geometric pre-training strategy is designed to explicitly utilize the geometric relationships between text-segments, and elaborately-designed RE heads are introduced to mitigate the gap between pre-training and fine-tuning on the downstream relation extraction task.
Specifically, three geometric relations are defined: the relation between two text-segments (\textbf{GeoPair}), that among multiple text-segment pairs (\textbf{GeoMPair}), and that among three text-segments (\textbf{GeoTriplet}).
Correspondingly, three self-supervised pre-training tasks are proposed.
GeoPair relation is modeled by the \textbf{D}irection and \textbf{D}istance \textbf{M}odeling (\textbf{DDM}) task in which GeoLayoutLM needs to tell the direction of a directed pair and identify whether a segment is the nearest to another one in the direction.
Furthermore, we design a brand-new pre-training objective called \textbf{D}etection of \textbf{D}irection \textbf{E}xceptions (\textbf{DDE}) for GeoMPair, enabling our model to capture the common pattern of directions among segment pairs, enhance the pair feature representation and discover the detached ones.
For GeoTriplet, we propose a \textbf{C}ollinearity \textbf{I}dentification of \textbf{T}riplet (\textbf{CIT}) task to identify whether three segments are collinear, which takes a step forward to the modeling of multi-segments relations. It is important for non-local layout feature learning especially in form-like documents.
Additionally, novel relation heads are proposed to learn better relation features,
which are pre-trained by the geometric pre-training tasks to absorb prior knowledge about geometry, thus mitigating the gap between pre-training and fine-tuning.
Extensive experiments on five public benchmarks demonstrate the effectiveness of the proposed GeoLayoutLM.

Our contributions are summarized as follows:

\begin{itemize} \setlength{\itemsep}{0pt}
  \item[1)]  This paper introduces three geometric relations in different levels and designs three brand-new geometric pre-training tasks correspondingly for learning the geometric layout representation explicitly.  To the best of our knowledge, GeoLayoutLM is the first to explore the geometric relations of multi-pair and multi-segments in document pre-training.
  \item[2)]  Novel relation heads are proposed to benefit the relation modeling. Besides, the relation heads are pre-trained by the proposed geometric tasks and fine-tuned for RE, thus mitigating the object gap between pre-training and fine-tuning.
\item[3)]  Experimental results on visual information extraction tasks including key-value linking as relation extraction, entity grouping as relation extraction, and semantic entity recognition show that the proposed GeoLayoutLM significantly outperforms previous state-of-the-arts with good interpretability. Moreover, our model has notable advantages in few-shot RE learning.
\end{itemize}









\section{Related Works}
\noindent\textbf{Visual information extraction}
Visual information extraction (VIE) aims at extracting entities from visually-rich document images, typically including semantic entity recognition (SER) and relation extraction (RE)\cite{jaume2019funsd,xu2022xfund,li2021structext,hong2022bros}.
Early works based on graph neural networks \cite{kipf2016semi,qian2018graphie,liu2019graph,qian2019graphie,luo2020merge,tang2021matchvie,yu2021pick} learned node features of text and layout in the downstream VIE tasks directly.
Recently, pre-training techniques have boosted the performance on document understanding. Various pre-training tasks are designed to learn better text/image features and their alignment for stronger multimodal document representation\cite{li2021structext,li2021selfdoc,appalaraju2021docformer,gu2022unified,wang2022lilt,gu2022xylayoutlm,lin2021vibertgrid,luo2022bivldoc,xu2020layoutlm,xu2021layoutxlm,xu2020layoutlmv2,huang2022layoutlmv3}.
Although they have achieved significant improvement on SER, RE remains largely underexplored and is also a challenging task\cite{zhang2021entity,hwang2020spatial,hong2022bros,li2021structext}.
BROS\cite{hong2022bros} encoded the relative spatial positions of texts into BERT\cite{devlin2018bert} to learn the layout representation better. In this paper, we focus on adopting pre-training to obtain better features.

\noindent\textbf{Geometric information} 
Geometric information is an important clue to represent the document layout.
Liu \etal\cite{liu2019graph} utilized relative 2D positions in GNN.
GraphNEMR\cite{luo2020merge} incorporated the 8-geometry neighbours and geometry distance information in document modeling for SER.
SPADE\cite{hwang2020spatial} re-formulated the self-attention layer by introducing a relative spatial vector which is composed of relative coordinates, distance and angle embeddings. 
StrucText\cite{li2021structext} proposed a Paired Boxes Direction task to model the geometric direction of text-segments in pre-training.
However, these works only explored pair-level geometric relations. We expand geometric relation to more than two segments: the relations of multi-pairs and triplets are fully explored.

\noindent\textbf{Pre-training / Fine-tuning}
Recent studies on pre-training also focused on alleviating the gap between the pre-training stage and the downstream fine-tuning stage\cite{chronopoulou2019embarrassingly,howard2018universal,gururangan2020don,han2021adaptive,hu2022p,liu2022prompt}.
Hu \etal\cite{hu2022p} identified and studied the training schema gap and the task knowledge gap, and converted the downstream ranking task into a pre-training schema.
Prompt-based models were proposed to adapt to various scenarios by converting downstream tasks to proper prompts which are consistent with the schema in pre-training\cite{liu2022prompt}.
Inspired by these works, we pre-train our elaborately-designed relation heads using the geometric tasks to absorb geometric knowledge adequately and improve its generalization in relation representation from the large-scale pre-training data.

\section{GeoLayoutLM}
GeoLayoutLM is a multi-modal framework for VIE. Geometric information is explicitly encoded and utilized by the novel geometry-based pre-training tasks and the pre-training of the elaborately-designed relation heads.
Additionally, an effective strategy for RE inference is introduced.



\begin{figure}[tp]
  \centering
\includegraphics[width=0.99\linewidth]{backbone4.pdf}
  \caption{An overview of GeoLayoutLM.}
  \label{model_arch}
  \vspace{-0.20in}
\end{figure}

\subsection{Model Architecture}


\subsubsection{Backbone}
\label{backbone_detail}

Inspired by the two-stream structure in METER\cite{dou2022empirical} and SelfDoc\cite{li2021selfdoc}, the backbone of GeoLayoutLM is composed of an independent vision module, a text-layout module, and interactive visual and text co-attention layers. As shown in \cref{model_arch}, the vision module takes the document image as input, and the text-layout module is fed with layout-related text embeddings.
Following LayoutLMv3\cite{huang2022layoutlmv3} and BiVLDoc\cite{luo2022bivldoc}, the text embeddings are the summation of 5 embeddings, including the token embeddings, 1D position embeddings, 1D segment rank embeddings, 1D segment BIE embeddings and 2D segment box embeddings.
The output feature of the vision module is processed by the global average pooling\cite{lin2013network} and RoI align\cite{he2017mask} to compute the global visual feature $F_{v0}$ and the $n$ visual segment features $\{F_{vi}|i\in [1,n]\}$.
Then the visual co-attention module takes $\{F_{vi}\}$ as query and $\{F_{ti}\}$ from the text-layout module as key and value for attention calculation, and outputs the fused visual features $\{M_{vi}\}$.
The fused textual features $\{M_{ti}\}$ are calculated in a similar way.
Finally, we add $M_{vi}$ and the corresponding first token feature of the segment $M_{t,b(i)}$ to obtain the $i$-th segment feature $H_i$.







\subsubsection{Relation Heads}


The semantic entity recognition (SER) in VIE is usually modeled as a token classification problem.
Learning a simple MLP classifier is effective for SER\cite{huang2022layoutlmv3}.
In the relation extraction (RE) of VIE, the final relation matrix was usually produced by a single linear or bilinear layer\cite{li2021structext,hong2022bros}.
Since the relationships of text-segments are relatively complex and related to each other, we argue that a simple linear or bilinear layer is not enough for relation modeling.

In this work, we propose two relation heads, including a Coarse Relation Prediction (CPR) head and a novel Relation Feature Enhancement (RFE) head, to enhance the relation feature representation for both relation pre-training and RE fine-tuning.
The RFE head is a lightweight transformer\cite{vaswani2017attention} consisting of a standard encoder layer, a modified decoder layer that discards the self-attention layer for computation efficiency, and a fully-connected layer followed by the sigmoid activation.
As shown in \cref{task_head_fig}, the text-segment features $\{H_i\}$ are fed into the CRP head (a bilinear layer) to predict a coarse relation matrix $r^{(0)}$.
To build the relation features $F_r$, the segment feature pairs are passed to a pair feature extractor (linearly mapping the concatenated paired features).
Then we select positive relation features $F_r^+$ based on $r^{(0)}$.
Note that $F_r^+$ probably has some false positive relation features since $r^{(0)}$ is the coarse prediction.
$F_r^+$ is then fed into the RFE encoder to capture the internal pattern of the true positive relations in each document sample, which is based on the assumption that most of the predicted positive pairs in $r^{(0)}$ are true.
All the relation features $F_r$ and the memory from the RFE encoder are fed into the RFE decoder to compute the final relation matrix $r^{(1)}$.




\subsection{Pre-training}





GeoLayoutLM is pre-trained with four self-supervised tasks simultaneously.
To learn multimodal contextual-aware text representations, the widely-used Masked Visual-Language Model (MVLM)\cite{xu2020layoutlm,xu2020layoutlmv2,huang2022layoutlmv3} is adopted on both $\{F_{ti}\}$ and $\{M_{ti}\}$.
Three proposed self-supervised geometric pre-training tasks are described in \cref{sec:geo_tasks}.











\subsubsection{Geometric Relationship}
To better represent document layout by geometric information, three geometric relationships are introduced, which are \textbf{GeoPair}, \textbf{GeoMPair} and \textbf{GeoTriplet}.
The relation between two text-segments (a pair) is denoted as \textbf{GeoPair}, which is also considered in previous works\cite{liu2019graph,luo2020merge,li2021structext} to model the relative layout information between two text-segments.
We further extend GeoPair to \textbf{GeoMPair} that is the relation among multiple segment pairs, to explore the relation of relations.
Like the relation of three points in geometry, \textbf{GeoTriplet} is also devised, which is the relation among three text-segments.

\subsubsection{Geometric Pre-training}
\label{sec:geo_tasks}


To make our model understand the geometric relationships and achieve good layout representations, we propose three geometry-related self-supervised pre-training tasks to model GeoPair, GeoMPair, and GeoTriplet respectively.
The input of these tasks are text-segments features $\{B_i\}$ which can be either of the five features: $\{H_{i}\},\{M_{vi}\},\{F_{vi}\},\{M_{t,b(i)}\},\{F_{t,b(i)}\}$, where $b(i)$ is the index of the first token of the $i$-th segment.


\begin{figure}[tp]
  \centering
\includegraphics[scale=0.8]{RFE4.pdf}
  \caption{Relation heads.}
  \label{task_head_fig}
  \vspace{-3mm}
\end{figure}

\begin{figure*}[tp]
  \centering
  \includegraphics[width=0.8\linewidth]{PTM_task3.pdf}
\vspace{-3mm}
  \caption{Geometric pre-training.}
  \label{geo_tasks_fig}
  \vspace{-6mm}
\end{figure*}



\noindent\textbf{Direction and Distance Modeling for GeoPair} To better understand the relative position relationship of two text-segments, as shown in \cref{geo_tasks_fig}(a), the Direction and Distance Modeling (DDM) is proposed, in which both the direction and distance are measured.





We consider 9 directions, including 8 neighbor ones\cite{luo2020merge} and the overlapping.
Hence, the direction modeling is exactly a 9-direction classification problem:

\begin{equation}
  P_{ij}^{direct} = \mathrm{Softmax}(\mathrm{Linear}([B_i, B_j]))
  \label{dierecion}
\end{equation}
where $P_{ij}^{direct}$ is the predicted direction probability, $[\cdot]$ is the concatenation operation.

The distance between two segments is defined as the minimum distance between the two bounding boxes\cite{luo2020merge}.
The distance is modeled as a binary classification problem that is to identify whether the $j$-th text-segment is the nearest to the $i$-th one in their direction. There is at most 1 nearest segment judged by distance in each of the 8 neighbor directions.
A bilinear layer is applied here:
\begin{equation}
  P_{ij}^{dist} = \mathrm{Sigmoid}(\mathrm{Bilinear}(B_i, B_j))
  \label{distance}
\end{equation}
where $P_{ij}^{dist}$ is the probability of the nearest pair identification.
Note that the operation in \cref{distance} shares the same process of CRP, which achieves the goals of pre-training the CRP head.

The loss function $\mathcal{L}_{DDM}$ of DDM task is defined as:
\vspace{-2mm}
\begin{equation}
  \begin{aligned}
  \mathcal{L}_{DDM} &= \mathrm{CrossEntropy}(P^{direct},Y^{direct}) \\
  &+ \mathrm{BCELoss}(P^{dist},Y^{dist})
  \end{aligned}
  \label{loss_ddm}
\end{equation}
where $Y^{direct}$ and $Y^{dist}$ are labels for direction and distance modeling.



\noindent\textbf{Detection of Direction Exceptions for GeoMPair} The relationships within a certain document area usually have some common geometric attributes. As shown in \cref{geo_tasks_fig}(b), the directions of key-value pairs are the same (arrows in red) in the wireless form area. The link in green can be easily judged as false due to its exceptional direction.
Motivated by this, the Detection of Direction Exceptions (DDE) task is proposed to model GeoMPair for the non-local layout understanding in documents.

The DDE task is to discriminate segment pairs whether their directions are exceptional in a \textsf{sample set} $S$.
A direction is regarded as an exception if the pairs with the direction have a minor ratio in the given \textsf{positive set} $S_p$.
For example, given a positive set in which more than 60\% of pairs have the same direction \textit{Right}, and a sample set, we label the right pairs in the sample set as 1, while the pairs of other directions as 0 (exception), as shown in \cref{geo_tasks_fig}(b).
The pair feature $L_i$ is built by linearly projecting the concatenated segment features.
Then, the positive set and the sample set are fed into the proposed RFE head to predict the discrimination probabilities of the sample set. The binary cross-entropy loss is applied.
\vspace{-2mm}

\begin{align}
  &P^{DDE} = \mathrm{RFE}(S_p,S) \\
  &\mathcal{L}_{DDE} = \mathrm{BCELoss}(P^{DDE}, Y^{DDE})
  \label{rfe_out}
\end{align}



\noindent\textbf{Collinearity Identification of Triplet for GeoTriplet} 
The geometric alignment of segments is an important expression of document layout, which is meaningful and involves the relation of multiple segments. 
Like the collinear attribute of three points, we define that of three segments. 
Given three text-segments $B_i$, $B_j$ and $B_k$, if the direction from $B_i$ to $B_j$, $B_j$ to $B_k$ and $B_i$ to $B_k$ are the same or antiphase, they are collinear; otherwise non-collinear. The collinear cases can be further divided into four classes: horizontal line, vertical line, forward slash and backslash.
As shown in \cref{geo_tasks_fig}(c), the left-aligned segments with the same entity tag are vertically collinear.
Correspondingly, a pre-training task called Collinearity Identification of Triplet (CIT) is proposed.
The triplet feature is the summation of three segment features since the collinear attribute is undirected. Subsequently, the 5-classification is made for CIT:
\vspace{-2mm}
\begin{align}
  &P^{CIT}_{ijk} = \mathrm{Softmax}(\mathrm{Linear}(B_i+B_j+B_k)) \\
  &\mathcal{L}_{CIT} = \mathrm{CrossEntropy}(P^{CIT}, Y^{CIT})
  \label{cit_loss}
\end{align}


The full pre-training objective of GeoLayoutLM is:
\begin{equation}
  \mathcal{L}_{pt} = \mathcal{L}_{MVLM} + \mathcal{L}_{DDM} + \mathcal{L}_{DDE} + \mathcal{L}_{CIT}
  \label{full_loss}
\end{equation}

\subsection{Fine-tuning and Inference}
During fine-tuning, the relation heads are initialized with the pre-trained parameters, which mitigates the gap between pre-training and fine-tuning.
The cross-entropy and binary cross-entropy function are utilized in SER loss $\mathcal{L}_{\mathrm{SER}}$ and RE losses $\{\mathcal{L}_{\mathrm{RE}, i}|i=0,1\}$ (corresponds to $r^{(0)}$ and $r^{(1)}$) respectively.
They are optimized together:
\vspace{-2mm}
\begin{equation}
  \mathcal{L}_{ft}=\mathcal{L}_{\mathrm{SER}}+\sum _{i=0}^1 \mathcal{L}_{\mathrm{RE},i}
  \label{loss_ft}
\end{equation}

In the RE task, for a relation pair $B_{j}\to B_{i}$, $B_{j}$ is called the father node, and $B_{i}$ is the son node.
$r_{i,j}^{(1)}$ stands for the probability that $B_{j}$ is the father of $B_{i}$.
The final relation output $\mathbb{R}$ was usually defined as: $\mathbb{R}_{ij}=\mathbbm{1}\left(r_{i,j}^{(1)}>0.5\right)$, where $\mathbbm{1}(\cdot)$ is the indicator function.
Optionally, we propose to impose the Restriction on the Selection of Fathers (RSF) for each son during the inference if some segments have several father nodes.
Specifically, the $j$-th segment is regarded as a father node of the $i$-th one only if $r_{i,j}^{(1)}>0.5$ and $r_{i,j}^{(1)}$ is close to the maximum probability:




\begin{equation}
  \mathbb{R}_{ij}=\mathbbm{1}\left(r_{i,j}^{(1)}>0.5\right)\times \mathbbm{1}\left(\max _k r_{i,k}^{(1)}<r_{i,j}^{(1)}+\tau\right)
  \label{infer_1}
\end{equation}

\noindent{where} $\tau$ is the margin between the probabilities.



We suggest an additional variance loss for RSF especially. Since the probabilities of father nodes are expected to be as close as possible, the variance of them should be as small as possible. During fine-tuning, the variance loss is exerted on the son nodes that have more than 1 father node.


\section{Experiments}
\subsection{Implementation Details}
The backbone detail is described in \cref{backbone_detail}. 
The vision module is composed of a ConvNeXt\cite{liu2022convnet} and a multi-scale FPN\cite{liao2020real}. BROS\cite{hong2022bros} is used as our text-layout module. The visual and textual co-attention modules are both equipped with a transformer decoder layer.

The document images are resized to $768\times 768$. The embedding size and feed-forward size of the co-attention module are 1024 and 4096 respectively.
In the RFE head, the relation feature size and feed-forward size are both set to 1024. The number of attention heads is 2. $\tau$ is $1e-3$.

\noindent \textbf{Pre-training Details.}
Following the LayoutLM series\cite{xu2020layoutlm,xu2020layoutlmv2,huang2022layoutlmv3}, we pre-train our model on the IIT-CDIP Test Collection 1.0\cite{lewis2006building}, which consists of around 11 million document images.
However, the original line-level OCR annotation in the dataset will lead to the monotony of paired box directions (only top and bottom exist in most documents), which is extremely harmful to geometric pre-training.
To break the imbalance in the distribution of the geometric relationship, we modify the OCR annotation by a Poisson Line Segmentation algorithm for each document with a probability of 90\%. \cref{alg_seg} lists the procedure of splitting a line.

\begin{algorithm}
  \caption{Poisson Line Segmentation}\label{alg_seg}
  \KwInput{An original line from OCR annotation $L$}
  \KwOutput{The processed line(s) $L'$}
  Get the number of words in $L_i \to N_w$\;
  $p_l = (1 - 1 / (N_w - 0.5))$; \footnotesize{\tcp{split probability}}
  \eIf{$N_w < 2 || \mathrm{rand()} > p_l$} {
    $L' \gets L$
  }{
    $N_s = \mathbf{poisson}(\lambda =\min(N_w/3, 7))$\;
    Split $L$ into $N_s$ segments $\to L'$\;
  }
\end{algorithm}

The AdamW optimizer is applied for pre-training, with the initial learning rate of 1e-5 and a linear decay learning rate scheduler.
We use a batch size of 224 to train GeoLayoutLM for 2 epochs on the IIT-CDIP dataset.
The maximum sequence length is set to 512. The maximum number of text-segments is set to 256.
Following \cite{xu2020layoutlm,xu2020layoutlmv2,huang2022layoutlmv3}, we mask 15\% text tokens in which 80\% are replaced by the [MASK] token, 10\% are replaced by a random token, and 10\% keeps unchanged.
In DDM, 16 text-segments are randomly sampled. Then, for each of them, we randomly sample 32 different text-segments to build paired text-segments.
In DDE, 40 segment pairs are randomly sampled in the document.
In CIT, 16 triple text-segments are randomly sampled.


\noindent \textbf{Fine-tuning Details.}
Following the LayoutLM series\cite{gu2022xylayoutlm,xu2020layoutlmv2,huang2022layoutlmv3}, the SER task is regarded as a sequence labeling problem aiming to tag each word with a label.
For the RE task, to conduct fair comparisons with previous methods (e.g., BROS~\cite{hong2022bros}), the ground truth entity labels are used.
We evaluate GeoLayoutLM on two popular benchmark datasets with five subtasks.
\textbf{FUNSD}\cite{jaume2019funsd} is a scanned document dataset for form understanding. It has 149 training samples and 50 test samples with multifarious layouts. We focus on both the semantic entity recognition (a.k.a. entity labeling) and the relation extraction (a.k.a. entity linking) tasks.
\textbf{CORD}\cite{park2019cord} is a camera-captured receipt dataset for information extraction. It contains 800 training, 100 validation and 100 test images. In CORD, three subtasks are evaluated including semantic entity recognition (SER), relation extraction as entity grouping (REaGRP) and relation extraction as key-value linking (REaKV).
We fine-tune our GeoLayoutLM for 200 epochs in FUNSD and 100 epochs in CORD with the batch size of 6. The learning rate is initially set to $2e-5$.

\subsection{Comparison with the SOTAs}
We compare our results with the previous state-of-the-arts. As shown in \cref{res_sota}, our GeoLayoutLM obtains the best F1 score in both semantic entity recognition (SER) and relation extraction (RE). 

\begin{table*}[tp]
  \centering
  \begin{tabular}{m{.45\columnwidth}|m{.20\columnwidth}|m{.16\columnwidth}<{\centering}|m{.16\columnwidth}<{\centering}|m{.16\columnwidth}<{\centering}|m{.16\columnwidth}<{\centering}|m{.16\columnwidth}<{\centering}}
    \toprule[1pt]
    \multirow{2}*{\textbf{Method}} & \multirow{2}*{\#\textbf{Params}} & \multicolumn{2}{c|}{\textbf{FUNSD}} & \multicolumn{3}{c}{\textbf{CORD}}\\
    \cline{3-7}
     & & SER & RE & SER & REaKV & REaGRP\\
    \hline
    BERT$_{LARGE}$\cite{devlin2018bert} & 340M & 65.63 & 29.11 & 90.25 & - & - \\
    LayoutLM$_{LARGE}$\cite{xu2020layoutlm} & 343M & 78.95 & 42.83 & 94.93 & - & - \\
    StrucTexT\cite{li2021structext} & 107M & 83.09 & 44.10 & - & - & -\\
    SERA\cite{zhang2021entity} & - & - & 65.96 & - & - & -\\
    LayoutLMv2$_{LARGE}$\cite{xu2020layoutlmv2} & 426M & 84.20 & 70.57 & 96.01 & - & 97.29\\
    BROS$_{LARGE}$\cite{hong2022bros} & 340M & 84.52 & 77.01 & 97.28 & - & 97.40\\
    LayoutLMv3$_{LARGE}$\cite{huang2022layoutlmv3} & 357M & 92.08 & 80.35$\dagger$ & 97.46 & 99.64$\dagger$ & 98.28$\dagger$\\
    \hline
    GeoLayoutLM & 399M & \textbf{92.86} & \textbf{89.45} & \textbf{97.97} & \textbf{100.00} & \textbf{99.45} \\
    \bottomrule[1pt]
  \end{tabular}
\caption{Comparison with existing models that explore both SER \& RE. The F1 score followed by $\dagger$ means it is re-implemented by us.}
  \label{res_sota}
  \vspace{-3mm}
\end{table*}

For the FUNSD SER task, GeoLayoutLM and LayoutLMv3 both significantly surpass other models.
Besides, the SER results on FUNSD and CORD also suggest that the geometric pre-training does the SER slightly more favorable than the popular text-image alignment. 
For the RE task, GeoLayoutLM significantly outperforms the previous state-of-the-art by 9.1\% on FUNSD, and reaches or nearly reaches the perfect performance in CORD. It demonstrates the great superiority of our model in extracting relations.
Even if we only fine-tune for 100 epochs, we still achieve (SER: 92.24\%, RE: 88.80\%) on FUNSD.

GeoLayoutLM backbone is slightly heavy due to the two-tower encoder.
Our vision module is flexible and can be replaced by others.
LayoutLMv3 has a coupling feature encoder for visual patches and text, which contributes to fewer parameters.
The relation head we used in LayoutLMv3 is the same as the CPR head in GeoLayoutLM (1M Params).
The proposed RFE head (14M) only constitutes 3.5\% of the total parameters.
On one Nvidia V100 GPU, the average inference time of GeoLayoutLM is 80.17ms, which is nearly the same as that of LayoutLMv3 (79.69ms).

\subsection{Ablation Study}
To better understand the effectiveness of geometric pre-training, the design of the RE heads and the RSF strategy in GeoLayoutLM, we perform plentiful ablation studies.


\noindent \textbf{Impact of Geometric Pre-training.}
To figure out how each pre-training task influences the information extraction result, we pre-train our model using different combinations of the geometric tasks while remaining the MLM task. To be efficient, only 10\% of the original pre-training data is used to train the model for 1 epoch.
The results meet our expectations completely, as shown in \cref{abl_geo}. By comparing \#0 with \#1x, we observe that the performance of SER and RE will be improved if either of the three geometric tasks is exerted paralleled to the MLM task. For SER, GeoPair contributes the most while GeoMPair does the least. For RE, GeoMPair contributes the most while GeoTriplet does the least, which may be owing to the RE head that is directly pre-trained in GeoPair and GeoMPair. By comparing \#2x with \#1x and \#3x with \#2x, we find that it is always better to pre-train with more geometric tasks, which indicates that the tasks are complementary.

\begin{table}
  \centering
  \begin{tabular}{m{.05\columnwidth}<{\centering}|m{.11\columnwidth}<{\centering}m{.14\columnwidth}<{\centering}m{.17\columnwidth}<{\centering}|m{.11\columnwidth}<{\centering}m{.11\columnwidth}<{\centering}}
    \toprule[1pt]
    \# & GeoPair & GeoMPair & GeoTriplet & SER & RE\\
    \hline
    0 &&&& 83.39 & 74.91\\
    1a & \checkmark &&& 91.80 & 82.23\\
    1b && \checkmark && 88.67 & 82.56\\
    1c &&& \checkmark & 90.78 & 78.90\\
    \hline
    2a & \checkmark & \checkmark && 91.86 & 85.22\\
    2b & \checkmark && \checkmark & 91.90 & 82.37\\
    2c && \checkmark & \checkmark & 91.39 & 84.97\\
    \hline
    3 & \checkmark & \checkmark & \checkmark & 92.17 & 85.32\\
    \bottomrule[1pt]
  \end{tabular}
  \caption{Ablation study on the geometric pre-training task in FUNSD. The first column labels the experiment settings.}
  \label{abl_geo}
  \vspace{-2mm}
\end{table}

\begin{table}[tp]
  \centering
  \begin{tabular}{lccc}
    \toprule[1pt]
    & \textbf{Entropy}$\downarrow$ & \textbf{Cross Entropy}$\downarrow$ & \textbf{Acc.}$\uparrow$ \\
    \hline
    LayoutLMv3 & 1.1423 & 0.9986 & 0.6319 \\
    GeoLayoutLM    & \textbf{0.7633} & \textbf{0.5884} & \textbf{0.8223} \\
    \bottomrule[1pt]
  \end{tabular}
  \caption{Experiments on the geometric layout understandings. The entropy of direction prediction reveals the information maintained in the backbone. The lower the Entropy and the Cross Entropy are, the more layout information the model maintains.}
  \label{tab:geo_rel_usage_intro}
  \vspace{-4mm}
\end{table}

To be interpretable,
we also investigate how much information of geometric relationship is kept after an example is encoded for the downstream information extraction task. To this end, we exert a linear classifier onto the backbone of the model fine-tuned on FUNSD (GeoLayoutLM VS LayoutLMv3), and \textit{only train the classifier} on the re-processed FUNSD dataset with pair direction labels (9-direction classification), to squeeze the direction information that is measured by the classification entropy, cross-entropy and the accuracy. As shown in \cref{tab:geo_rel_usage_intro}, GeoLayoutLM has a lower entropy and cross-entropy, and a higher accuracy, indicating that it retains much more information about geometric relations in the downstream tasks.

To further understand the geometric layout information, we conduct an embedding visualization of the left-right direction. As shown in \cref{fig:vis_emb_intro}, GeoLayoutLM has stronger distinguishable embeddings in left and right relationships that are important for document layout representation.

A case study is given in \cref{link_case}. Most of the false positive relation links predicted by LayoutLMv3 violate the geometric layout obviously. It depends on the semantic information excessively and ignores the layout more or less. For example, the entity starting with ``No.'' is linked to the number entity regardless of the geometric relationship between them. In contrast, GeoLayoutLM successfully predicted all links with a good recall. For more details, please refer to the appendix\ref{sec:case_study}.

Although the rule-based geometric constraint can bring some improvement (\cref{intro_v3_geo}), it still fall behind GeoLayoutLM because it:
(1) relies on hard-coded thresholds, which limit its adaptability and generalization when handling documents of different formats and layouts; (2) is able to prune false linkings, but cannot recover missed ones.

\begin{figure}[tp]
  \includegraphics[scale=0.38]{emb_vis.pdf}
  \caption{Comparison of the left (L) and right (R) relation features. For LayoutLMv3 and GeoLayoutLM, 2D layout positions remain unchanged and the input text tokens are set to [UNK].}
  \label{fig:vis_emb_intro}
  \vspace{-4mm}
\end{figure}

\begin{figure}[t]
  \centering
   \includegraphics[width=0.99\linewidth]{link_case4.pdf}
   \vspace{-2mm}
   \caption{RE case study. The arrows in green, red and orange denote true positive, false positive and false negative (missed) relations respectively. Best viewed by zooming up.}
   \label{link_case}
\end{figure}

\noindent \textbf{Effects of the Relation Heads.}
There are two important points in our RE task heads: the novel RFE head and its pre-training. We study the impact of them for the RE task. The coarse relation prediction (CRP) head is always kept. Besides, we do not use the RSF strategy to be elegant.

As shown in \cref{abl_link_head}, a bare CRP head not initialized by the pre-trained parameters (w/o Pt) achieves an 82.2\% F1 score owing to the strong geometry-aware backbone. Once it is initialized by pre-training (Pt), an improvement of 2.7\% F1 score is obtained. By adding the RFE head (w/o Pt), the version of CRP (w/o Pt) becomes stronger while that of CRP (Pt) even degrades a little bit. We argue that the RFE head introduces more parameters, which causes overfitting despite its superiority in relation modeling. Thus it is necessary to pre-train the RFE head. We also find that the pre-training of the RFE head is more important than that of the CRP head. By making full use of the two points, GeoLayoutLM obtains the best RE performance.

\begin{table}
  \centering
  \begin{tabular}{m{.14\columnwidth}<{\centering}m{.14\columnwidth}<{\centering}|m{.14\columnwidth}<{\centering}m{.14\columnwidth}<{\centering}|m{.12\columnwidth}<{\centering}}
    \toprule[1pt]
    \multicolumn{2}{c|}{\textbf{CRP head}} & \multicolumn{2}{c|}{\textbf{RFE head}} & \multirow{2}*{\textbf{F1}}\\
    \cline{1-4}
    w/o Pt & Pt & w/o Pt & Pt & \\
    \hline
    \checkmark & & & & 82.2\\
     & \checkmark &&& 84.9\\
    \hline
    \checkmark &&\checkmark&& 84.0\\
    & \checkmark & \checkmark && 84.0\\
    \checkmark &&&\checkmark& 86.2\\
    & \checkmark &&\checkmark& \textbf{86.9}\\
    \bottomrule[1pt]
  \end{tabular}
  \caption{Ablation study on the CRP and RFE head in FUNSD RE task. ``w/o Pt" and ``Pt" mean that the head is not pre-trained and pre-trained respectively. The RSF strategy is not used.}
  \label{abl_link_head}
  \vspace{-4mm}
\end{table}



\noindent \textbf{Effects of RSF.}
The RSF strategy is non-trivial in the fine-tuning and inference stage. It contains two parts: the post-process for inference and the variance loss in fine-tuning.

\cref{abl_rsf} gives a clear view. By the post-processing, the precision of our method is improved dramatically with a little sacrifice of recall. A bare variance loss without the post-processing does nothing to the performance since it is designed for the post-processing only. We obtain the best F1 score when using both of them.

\begin{table}
  \centering
  \begin{tabular}{m{.16\columnwidth}<{\centering}m{.15\columnwidth}<{\centering}|m{.14\columnwidth}<{\centering}m{.11\columnwidth}<{\centering}m{.10\columnwidth}<{\centering}}
    \toprule[1pt]
    \textbf{postprocess} &\textbf{variance loss} & \textbf{Precision} & \textbf{Recall} & \textbf{F1}\\
    \hline
    && 85.26 & 90.15 & 87.64\\
    \checkmark && 88.25 & 89.01 & 88.62\\
    & \checkmark & 85.06 & 90.34 & 87.62\\
    \checkmark & \checkmark & 88.94 & 89.96 & 89.45\\
    \bottomrule[1pt]
  \end{tabular}
  \caption{Ablation study on the RSF strategy in FUNSD RE task.}
  \label{abl_rsf}
  \vspace{-2mm}
\end{table}

\subsection{Few-shot RE Learning}

In real scenarios, the acquirement of the training data for document information extraction is a bottleneck due to the expensive and boring annotation work. So it is necessary to learn from only a few document samples.

To explore the ability of few-shot learning, we compare our GeoLayoutLM with another two models: a modified GeoLayoutLM whose heads are not initialized from pre-training (GeoLayoutLM*), and LayoutLMv3. We also disable the RSF strategy to make it clearer.

As shown in \cref{fewshot}, GeoLayoutLM shows great superiority in this setting. GeoLayoutLM* outperforms LayoutLMv3 but is inferior to GeoLayoutLM all the time. It suggests that the geometric pre-training endows our model with a strong ability to extract entity relations, and also emphasizes the importance of RE head pre-training. Notably, our GeoLayoutLM achieves a slightly better performance (71.53\%) using only 30 samples than LayoutLMv3 does (71.07\%) using 104 samples. The performance gap is very large when only few fine-tuning samples are available.

\begin{figure}[t]
  \centering
   \includegraphics[width=0.9\linewidth]{fewshot3.pdf}
   \caption{Comparison of few-shot learning in FUNSD RE task.}
   \label{fewshot}
   \vspace{-3mm}
\end{figure}



\section{Conclusion}

In this paper, we propose GeoLayoutLM, a geometric pre-training framework for VIE.
Three geometric relations in different levels are defined: GeoPair, GeoMPair and GeoTriplet.
Correspondingly, three specially designed pre-training objectives are introduced to model geometric relations explicitly.
Additionally, the relation heads are elaborately designed to enhance the relation feature representation, which are pre-trained by the geometric pre-training, thus mitigating the gap between pre-training and fine-tuning.
Experimental results on VIE have illustrated the effectiveness of GeoLayoutLM in both SER and RE tasks.
In the future, we will explore more effective geometric pre-training tasks, and apply our method to more tasks of visually-rich document understanding.


{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}


\newpage
\appendix
\section{Appendix}

\subsection{Case Study}
\label{sec:case_study}

\begin{figure*}[tp]
  \centering
  \includegraphics[width=1.0\linewidth]{appendix_case_study_1.pdf}
  \caption{RE case study in tables.}
  \label{appendix_case_study_1}
\end{figure*}


\begin{figure*}[tp]
  \centering
  \includegraphics[width=1.0\linewidth]{appendix_case_study_2.pdf}
  \caption{RE case study in a borderless table.}
  \label{appendix_case_study_2}
\end{figure*}

\begin{figure*}[tp]
  \centering
  \includegraphics[width=1.0\linewidth]{appendix_case_study_3.pdf}
  \caption{Failure cases comparison.}
  \label{appendix_case_study_3}
\end{figure*}


One motivation behind GeoLayoutLM is to avoid making predictions that violate the geometric layout.
As \cref{appendix_case_study_1} shows, GeoLayoutLM successfully predicts all links in a document that contains multiple tables. 
However, LayoutLMv3 tends to link two entities relying more on their semantics.
Most of the false positive relation links predicted by LayoutLMv3 are cross tables or table columns, which is counter-intuitive in terms of the geometric layout.
Another motivation behind GeoLayoutLM is to predicts right links that are similar in geometric layout.
A case study is given in \cref{appendix_case_study_2}.
In this case, LayoutLMv3 successfully predicts the link in the upper half part of the wireless form but misses the link below. For example, the ``Recipient(s)'' is linked to ``Mr. Ronald Milstein'' and ``Mr. F. Anthony Burke'', but the ``Recipient(s)'' misses ``Mr. Charles A. Blixt'' and ``Mr. Stephen R. Patton'', despite that these links are similar in geometric layout.
Our GeoLayoutLM extracts all the relations exactly.
A failure example is given in \cref{appendix_case_study_3}. 
Most of the false positive relation links predicted by GeoLayoutLM still conforms to the geometric layout. There are no false positive relation links that are cross the borderline of the table.
These examples show that GeoLayoutLM predicts relation links more accurately, and complies with the geometric layout restriction well.


\end{document}
