\appendix
\label{sec:appendix}

This supplementary material contains further details regarding our probabilistic pose and shape prediction method, and presents additional quantitative and qualitative results and comparisons with other methods. Section \ref{sec:supmat_imp_details} describes the model architecture and synthetic training data. It also validates our approach for point estimate computation, and investigates our radial tanh transform for compact distribution support. Section \ref{sec:supmat_eval_details} discusses evaluation datasets and metrics, cropped evaluation dataset generation and directional variance visualisations. Finally, Section \ref{sec:supmat_experiments} presents comparisons with other methods.

\section{Implementation Details}
\label{sec:supmat_imp_details}

\subsection{Model architecture}

An overview of our model architecture is provided in Figure \ref{fig:method} of the main manuscript. Further details regarding the CNN encoder, shape/global rotation/camera MLP and per-body-part normalising flow modules are provided below.

We use a ResNet-18 CNN encoder \cite{He2015}, which takes a proxy representation input $\mathbf{X} \in \mathbb{R}^{H \times W \times C}$ and outputs a feature vector $\boldsymbol{\phi} \in \mathbb{R}^{512}$. The proxy representation consists of an edge-image and 2D keypoint heatmaps stacked along the channel dimension, with height $H = 256$, width $W = 256$ and channels $C = 18$. The choices of proxy representation and CNN encoder follow \cite{sengupta2021hierprobhuman}.

The input features $\boldsymbol{\phi}$ are passed through the shape/global rotation/camera MLP, which outputs the parameters of a Gaussian distribution over SMPL \cite{SMPL:2015} shape, $\boldsymbol{\mu}_\beta, \boldsymbol{\sigma}^2_\beta \in \mathbb{R}^{10}$, as well as deterministic estimates of weak-perspective camera parameters $\pmb{\pi} = [s, t_x, t_y] \in \mathbb{R}^3$ and the global body rotation $\mathbf{R}_\text{glob} \in SO(3)$. The latter is predicted using the continuous 6D rotation representation proposed by \cite{Zhou_2019_CVPR}, then converted to a rotation matrix. The shape/global rotation/camera MLP has 1 hidden layer with 512 nodes and ELU activation \cite{clevert2016elu}, and an output layer with 29 nodes.


\begin{table}[t]
\centering
\small
\begin{tabular}{l c}
    \hline
    \textbf{Hyperparameter} & \textbf{Value}\\
    \hline
    Shape parameter sampling mean & 0 \\
    Shape parameter sampling std. & 1.25 \\
    Cam. translation sampling mean & (0, -0.2, 2.5) m\\
    Cam. translation sampling var. & (0.05, 0.05, 0.25) m\\
    Cam. focal length & 300.0\\
    Lighting ambient intensity range & [0.4, 0.8]\\
    Lighting diffuse intensity range & [0.4, 0.8]\\
    Lighting specular intensity range & [0.0, 0.5]\\
    Bounding box scale factor range & [0.8, 1.2]\\
    \hline
    Body-part occlusion probability & 0.1 \\
    2D joints L/R swap probability & 0.1\\
    Half-image occlusion probability & 0.05\\
    Extreme crop probability & 0.1\\
    2DKP occlusion probability & 0.1\\
    2DKP noise range & [-8, 8] pixels\\
    \hline
    \end{tabular}
\caption{List of hyperparameter values associated with synthetic training data generation and augmentation. Body-part occlusion uses the 24 DensePose \cite{Guler2018DensePose} parts. Joint L/R swap is done for shoulders, elbows, wrists, hips, knees, ankles.}
\vspace{-0.7cm}
\label{tab:supmat_synthetic_data}
\end{table}

For each SMPL body-part $i \in \{1, \ldots, 23\}$, our method outputs a normalising flow distribution over the body-part's relative rotation $\mathbf{R}_i \in SO(3)$. This is conditioned on the input features $\boldsymbol\phi$, camera estimate $\pmb{\pi}$, global rotation estimate $\mathbf{R}_\text{glob}$, a shape vector sample $\boldsymbol{\beta}$ and ancestor body-part rotation samples $\{\mathbf{R}_j\}_{j\in A(i)}$, where $A(i)$ denotes the ancestors of body-part $i$ in the SMPL kinematic tree. $\boldsymbol{\beta}$ is sampled differentiably from $\mathcal{N}(\boldsymbol{\mu}_\beta(X), \boldsymbol{\sigma}^2_\beta(X))$ using the re-parameterisation trick \cite{kingma2014autoencoding}. $\{\mathbf{R}_j\}_{j\in A(i)}$ are differentiably sampled from their own respective normalising flow rotation distributions. The conditioning variables $\{\boldsymbol{\phi}, \pmb{\pi}, \mathbf{R}_\text{glob}, \boldsymbol{\beta}, \{\mathbf{R}_j\}_{j\in A(i)} \}$ are aggregated into a context vector $\mathbf{c}_i \in \mathbb{R}^{64}$ using a context generation MLP for each body-part $i$, as shown in Figure \ref{fig:method} of the main manuscript. Each context generation MLP has 1 hidden layer with 256 nodes and ELU activation \cite{clevert2016elu}, and an output layer with 64 nodes. Note: in Eqns. \ref{eqn:joint_dist} and \ref{eqn:pose_dist} in the main manuscript, we notationally replaced the conditioning variables $\boldsymbol{\phi}, \pmb{\pi}$ and $\mathbf{R}_\text{glob}$ with $\mathbf{X}$ for simplicity, because each of these are deterministically obtained as functions of $\mathbf{X}$.


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/supmat_radialtanh_fig_v1.png}
    \caption{Comparison between the ``unscaled'' radial tanh transform proposed by \cite{falorsi2019reparameterizing} (Eqn. \ref{eqn:supmat_unscaled_radial_tanh}) and our ``scaled'' version (Eqn. \ref{eqn:supmat_radial_tanh}), in terms of sample rotation angles (or axis-angle vector magnitudes) from a randomly-initialised (i.e. un-trained) normalising flow. The unscaled transform pushes samples from the base distribution $\mathcal{N}(\mathbf{0}, \mathbf{I})$ towards the boundary of the desired support ball $B_r(\mathbf{0})$. Here, $r$ is set to $1.5\pi$ rad (i.e. 270Â°). This results in unstable training, as shown in Figure \ref{fig:supmat_init_flow_samples_fig}. Our scaled transform mitigates this behaviour.}
    \vspace{-0.3cm}
    \label{fig:supmat_radialtanh_fig}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/supmat_init_flow_samples_fig_v1.jpg}
    \caption{Effect of our ``scaled'' radial tanh transform (Eqn. \ref{eqn:supmat_radial_tanh}), and reduced-variance base distribution, on SMPL pose and shape samples after training for 1 epoch. Samples using $\mathcal{N}(\mathbf{0}, \mathbf{I})$ for base distributions and the ``unscaled'' radial tanh transform proposed by \cite{falorsi2019reparameterizing} (Eqn. \ref{eqn:supmat_unscaled_radial_tanh}) are highly unhuman (row 1). Samples become much more realistic with the scaled transform and reduced-variance base distribution (row 2 and 3). This improves training stability.}
    \vspace{-0.5cm}
    \label{fig:supmat_init_flow_samples_fig}
\end{figure}

The conditional normalising flow distribution over each body-part's rotation,  $p_{SO(3)}(\mathbf{R}_i | \mathbf{c}_i)$, is formed by pushing a flow distribution over the corresponding axis-angle vector, $p_{\mathbb{R}^3}(\mathbf{v}_i | \mathbf{c}_i)$ for $\mathbf{v}_i \in \mathbb{R}^3 \cong \mathfrak{so}(3)$, onto $SO(3)$ using the $\exp$ map (detailed by Eqn. \ref{eqn:exp_change_of_var} in the main manuscript). We use Linear Rational Spline (LRS) normalising flows \cite{dolatabadi2020lrs}, which transform a simple base distribution into a complex density function with a series of LRS coupling layer diffeomorphisms.  Specifically, let $\mathbf{z}_{k-1} \in \mathbb{R}^D$ be the input variable to the $k$-th coupling layer $f^\text{LRS}_k$, and let $\mathbf{z}_k \in \mathbb{R}^D$ be the output, such that $\mathbf{z}_k = f^\text{LRS}_k(\mathbf{z}_{k-1})$. The coupling layer \cite{dinh2015nice, dinh2017realnvp} splits the input into two parts $\mathbf{z}_{k-1}^{0:d}$ and $\mathbf{z}_{k-1}^{d:D}$. Then, the output variable is determined by
\begin{equation}
\begin{aligned}
    \mathbf{z}_k^{0:d} &= \mathbf{z}_{k-1}^{0:d}\\
    \mathbf{z}_k^{d:D} &= g(\mathbf{z}_{k-1}^{d:D}; \mathbf{w}(\mathbf{z}_{k-1}^{0:d}))
\end{aligned}
\end{equation}
where $g(\:.\:; \mathbf{w}(\mathbf{z}_{k-1}^{0:d}))$ is an \textit{element-wise} bijective and differentiable function (i.e. a diffeomorphism), whose parameters $\mathbf{w}$ depend on the first half of the input $\mathbf{z}_{k-1}^{0:d}$. Note that the Jacobian of $f^\text{LRS}_k$ is lower-triangular, and thus $\det J_{f^\text{LRS}_k}$ is easily-computed as the product of the diagonal terms of $J_{f^\text{LRS}_k}$. For an LRS coupling layer \cite{dolatabadi2020lrs}, $g$ is an element-wise spline transform. Each spline segment is a linear rational function of the form $\frac{ax+b}{cx+d}$. The parameters of $g$, $\mathbf{w}(\mathbf{z}_{k-1}^{0:d})$, are the parameters of each segment's linear rational function, and the locations of each segment's endpoints (or knots). These are obtained by passing $\mathbf{z}_{k-1}^{0:d}$ through an MLP. LRS coupling layers are able to model significantly more complex distributions \cite{dolatabadi2020lrs} than affine \cite{dinh2017realnvp} or additive \cite{dinh2015nice} coupling layers with the same number of layers composed together.

For each body-part $i$,  $p_{\mathbb{R}^3}(\mathbf{v}_i | \mathbf{c}_i)$ is implemented as an LRS-NF composed of 3 LRS coupling layer transforms, with a permutation following each coupling layer. Each layer's spline parameters are output by an MLP with 3 hidden layers that have 32 nodes each, and ELU \cite{clevert2016elu} activations.

\subsection{Radial tanh and sample angle regularisation}

We must ensure that $p_{\mathbb{R}^3}(\mathbf{v}_i | \mathbf{c}_i)$ has compact support, i.e. $p_{\mathbb{R}^3}(\mathbf{v}_i | \mathbf{c}_i) = 0$ for $\mathbf{v}_i \notin B_r(\mathbf{0})$ where $B_r(\mathbf{0})$ is an open ball of radius $\pi < r < 2\pi$. We choose $r=1.5\pi$. Towards this end, we use a radial tanh transform \cite{falorsi2019reparameterizing}, $t: \mathbb{R}^3 \rightarrow B_r(\mathbf{0})$ as the last layer of each body-part's normalising flow transform, as discussed in the main manuscript. This is reproduced here for convenience:
\begin{equation}
\label{eqn:supmat_radial_tanh}
    t(\mathbf{x}) = r \tanh\left(\frac{\|\mathbf{x}\|}{r}\right)\frac{\mathbf{x}}{\|\mathbf{x}\|}.
\end{equation}

This is slightly different to the original transform proposed in \cite{falorsi2019reparameterizing}, which is given by
\begin{equation}
\label{eqn:supmat_unscaled_radial_tanh}
    t'(x) = r \tanh\left(\|\mathbf{x}\|\right)\frac{\mathbf{x}}{\|\mathbf{x}\|} 
\end{equation}
i.e. the argument of the $\tanh$ in our transform is scaled by $1/r$. This scaling is highly beneficial for training the body pose normalising flows. To see why, consider the behaviour of $t$ and $t'$ when $\|\mathbf{x}\|$ is small, such that $\tanh\|\mathbf{x}\| \approx \|\mathbf{x}\| $. Then, $t(\mathbf{x}) \approx \mathbf{x}$ while $t'(\mathbf{x}) \approx r\mathbf{x}$. Notably, $t$ does not significantly affect points which have small magnitude, and thus are already well within the desired support $B_r(\mathbf{0})$. In contrast, $t'$ increases the magnitude of these points by a factor of $r$, pushing them towards the boundary of $B_r(\mathbf{0})$. This is illustrated by Figure \ref{fig:supmat_radialtanh_fig}, which visualises the histogram of sample magnitudes drawn from a base distribution $\mathcal{N}(\mathbf{0}, \mathbf{I})$ and the histogram of magnitudes after these samples are passed through a randomly-initialised flow ending in a radial tanh transform. Note that sample magnitudes correspond to rotation angles, as samples from $p_{\mathbb{R}^3}(\mathbf{v}_i | \mathbf{c}_i)$ are axis-angle vectors. Comparing the sample magnitude histogram for the ``unscaled'' transform \cite{falorsi2019reparameterizing} $t'$ and our ``scaled'' version $t$ demonstrates that $t'$ pushes points towards the boundary of $B_r(\mathbf{0})$ (with $r=1.5\pi$).

Large sample rotation angles from un-trained (i.e. randomly-initialised) flow distributions lead to unstable training. This is because human body-parts rarely have large rotations in natural poses, which is particularly true for torso joints in the SMPL kinematic tree. We empirically found that the flow models struggled to recover from initially too-large rotation angles during training, when using the unscaled transform $t'$. This is shown in Figure \ref{fig:supmat_init_flow_samples_fig}, where $t'$ results in extremely unhuman pose samples (row 1) after 1 training epoch, while our scaled transform $t$ gives more realistic samples. In other words, $t$ acts as a rotation angle regulariser, making use of prior domain knowledge about human body-parts often having small rotations in natural poses.

To further regularise sample rotation angles, we replace the typical base distribution $\mathcal{N}(\mathbf{0}, \mathbf{I})$ with $\mathcal{N}(\mathbf{0}, 0.6\mathbf{I})$, which has reduced variance. This results in reasonable pose samples during early training, as shown by Figure \ref{fig:supmat_init_flow_samples_fig}, row 3.

\begin{table*}[t]
\centering
\footnotesize
\renewcommand{\tabcolsep}{3pt}
\begin{tabular}{l | c c c c | c c c c | c c c c} 
\hline
\textbf{Method} & \multicolumn{4}{c|}{\textbf{3DPW}} & \multicolumn{4}{c}{\textbf{3DPW} $\mathbf{70\%}$ \textbf{Cropped}} & \multicolumn{4}{c}{\textbf{3DPW} $\mathbf{50\%}$ \textbf{Cropped}}\\
& \multicolumn{2}{c}{MPJPE (mm)} & \multicolumn{2}{c|}{MPJPE-PA (mm)} & \multicolumn{2}{c}{MPJPE (mm)} & \multicolumn{2}{c|}{MPJPE-PA (mm)} & \multicolumn{2}{c}{MPJPE (mm)} & \multicolumn{2}{c}{MPJPE-PA (mm)}\\ 
& Point & Sample Min. & Point & Sample Min. & Point & Sample Min. & Point & Sample Min.  & Point & Sample Min. & Point & Sample Min. \\
\hline
\hline
HMR \cite{hmrKanazawa17} & 130.0 & - & 76.7 & - & 177.4 & - & 96.6 & - & 214.6 & - & 120.2 & -\\
GraphCMR \cite{kolotouros2019cmr} & 119.9 & - & 70.2 & - & 120.8 & - & 74.5 & -  & 205.2 & - & 119.7 & - \\
SPIN \cite{kolotouros2019spin} & 96.9 & - & 59.0  & -  & 108.5 & - & 63.9 & - & 196.7 & - & 130.5  & -\\
PARE \cite{Kocabas_PARE_2021} & 74.5 & - & 46.5 & - & \textbf{80.0} & - & \textbf{50.6} & - & 121.8 & - & 79.7 & -\\
HybrIK \cite{li2020hybrik} & \textbf{74.1} & - & \textbf{45.0} & - & 91.9 & - & 60.7 & - & 187.5 & - & 153.0 & -\\
\hline
3D Multibodies \cite{biggs2020multibodies} & 93.8 & 74.6 \scriptsize$(\text{20.5\%})$ & 59.9 & 48.3 \scriptsize$(\text{19.4\%})$ & 110.5 & 80.9 \scriptsize$(\textbf{26.8\%})$ & 67.7 & 51.1 \scriptsize$(\text{24.5\%})$ & 190.6 & 98.4 \scriptsize$(\textbf{48.4\%})$ & 120.3 & 64.7 \scriptsize$(\textbf{46.2\%})$\\ 
Sengupta \etal \cite{sengupta2021probabilisticposeshape} & 97.1 & 84.4 \scriptsize$(
\text{13.1\%})$ & 61.1 & 52.1 \scriptsize$(\text{14.7\%})$ & 99.8 & 86.1  \scriptsize$(\text{13.7\%})$ & 62.7 & 52.2  \scriptsize$(\text{16.7\%})$ & 144.7 & 125.5 \scriptsize$(13.3\text{\%})$ & 93.6 & 76.1 \scriptsize$(\text{18.7\%})$\\
ProHMR \cite{kolotouros2021prohmr} & 97.0 & 81.5 \scriptsize$(\text{16.0\%})$ & 59.8 & 48.2 \scriptsize$(\text{19.4\%})$ & 99.4 & 84.1 \scriptsize$(\text{15.4\%})$ & 62.1 & 50.0 \scriptsize$(\text{19.5\%})$ & 143.8 & 123.3 \scriptsize$(\text{14.3\%})$ & 85.4 & 68.8 \scriptsize$(\text{19.4\%})$\\
HierProbHuman \cite{sengupta2021hierprobhuman} & 84.9 & 70.9 \scriptsize$(\text{16.5\%})$ & 53.6 & 43.8 \scriptsize$(\text{18.3\%})$ & 94.2 & 78.4 \scriptsize$(\text{16.8\%})$ & 61.6 & 49.5 \scriptsize$(\text{19.6\%})$ & 126.9 & 101.8  \scriptsize$(\text{19.8\%})$ & 87.0 & 67.7 \scriptsize$(\text{22.2\%})$\\
HuManiFlow & \textbf{83.9} & \textbf{65.1} \scriptsize$(\textbf{22.4\%})$ & \textbf{53.4} & \textbf{39.9} \scriptsize$(\textbf{25.3\%})$ & \textbf{93.5} & \textbf{71.6} \scriptsize$(\text{23.4\%})$ & \textbf{60.7} & \textbf{44.6} \scriptsize$(\textbf{26.5\%})$ & \textbf{116.4} & 86.9 \scriptsize$(\text{25.3\%})$ & \textbf{78.2} & \textbf{54.9} \scriptsize$(\text{29.8\%})$\\
\hline
\end{tabular}
\vspace{-0.1cm}
\caption{Comparison between recent deterministic (top half) and probabilistic (bottom half) pose and shape predictors in terms of accuracy on the 3DPW dataset \cite{vonMarcard2018}, as well as $50\%$ and $70\%$ cropped versions of 3DPW (see Section \ref{subsec:supmat_cropped_datasets} for cropping details). $\%$s are decreases in MPJPE(-PA) from the point-estimate to the minimum sample value computed over 100 samples. Our method, HuManiFlow, is more accurate than all current probabilistic methods. Point estimates from HuManiFlow are competitive with the state-of-the-art deterministic methods, particularly on more ambiguous and challenging cropped images.}
\label{table:supmat_3dpw_sota_comparison_accuracy}
\vspace{-0.1cm}
\end{table*}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/supmat_point_est_ll_fig_v1.png}
    \caption{Histogram of differences between point estimate log-likelihoods and maximum sample log-likelihoods for 1000 samples, computed on the test set of 3DPW \cite{vonMarcard2018} and its cropped version. Point estimates generally have higher log-likelihoods than the most likely samples, as the log-likelihood differences are typically positive, confirming that the approximate method for obtaining point estimates presented in Eqn. \ref{eqn:point_estimate} of the main manuscript is suitable.}
    \label{fig:supmat_point_est_ll}
\end{figure}

\subsection{Point estimate validity}

Eqn. \ref{eqn:point_estimate} in the main manuscript describes our approach to obtaining a point estimate $(\{\mathbf{R}_i^*\}_{i=1}^{23},\boldsymbol{\beta}^*)$ from the complex joint distribution over SMPL pose and shape $p_\text{joint}(\{\mathbf{R}_i\}_{i=1}^{23}, \boldsymbol{\beta} | \mathbf{X})$ predicted by our method. The point estimate is not, in general, the actual mode of $p_\text{joint}$. However, we empirically verify that $(\{\mathbf{R}_i^*\}_{i=1}^{23},\boldsymbol{\beta}^*)$ typically has high likelihood under $p_\text{joint}$, using the test set of 3DPW \cite{vonMarcard2018}. To do so, we compute the log-likelihood of the point estimate for each test input, $p_\text{joint}(\{\mathbf{R}^*_i\}_{i=1}^{23}, \boldsymbol{\beta}^* | \mathbf{X})$, as well as the maximum sample log-likelihood $\max_{n=1,\ldots, N} p_\text{joint}(\{\mathbf{R}^n_i\}_{i=1}^{23}, \boldsymbol{\beta}^n | \mathbf{X})$ for $N=1000$ total samples. The maximum sample log-likelihood is subtracted from the point estimate log-likelihood for each test input, and the histogram of these log-likelihood deltas is visualised in Figure \ref{fig:supmat_point_est_ll}. The point estimates generally have higher log-likelihood under $p_\text{joint}$ than the most likely samples, corroborating their validity and usefulness when a single pose and shape solution is required. A qualitative comparison between point estimates from our approach and pose and shape predictions from the state-of-the-art deterministic methods \cite{Kocabas_PARE_2021, li2020hybrik} is given in Figure \ref{fig:supmat_sota_deterministic_fig}.

\subsection{Synthetic training data}

We train our pose and shape distribution prediction method using the synthetic training data generation pipeline proposed by \cite{sengupta2021hierprobhuman}. A brief overview is given below, but we refer the reader to \cite{sengupta2021hierprobhuman} for details. Moreover, hyperparameters related to synthetic data generation and augmentation are given in Table \ref{tab:supmat_synthetic_data}.

Synthetic edge-and-keypoint-heatmap proxy representations are rendered on-the-fly during training, using ground-truth SMPL \cite{SMPL:2015} pose parameters, and randomly sampled SMPL shape parameters, camera extrinsics, lighting, backgrounds and clothing. Ground-truth pose parameters are obtained from the training splits of 3DPW \cite{vonMarcard2018}, UP-3D \cite{Lassner:UP:2017} and Human3.6M \cite{h36m_pami}, giving a total of 91106 training poses, as well as 33347 validation poses from the corresponding validation splits. SMPL shape parameters are sampled from $\mathcal{N}(\boldsymbol{\beta}; \mathbf{0}, 1.25\mathbf{I})$. RGB clothing textures are obtained from SURREAL \cite{varol17_surreal} and MultiGarmentNet \cite{bhatnagar2019mgn}, which contain 917 training textures and 108 validation textures. Random backgrounds are obtained from a subset of LSUN \cite{yu15lsun} with 397582 training backgrounds and 3000 validation backgrounds. On-the-fly rendering of training inputs is done using Pytorch3D \cite{ravi2020pytorch3d}, with a perspective camera model and Phong shading. Camera and lighting parameters are randomly sampled, with hyperparameters given in Table \ref{tab:supmat_synthetic_data}.

To bridge the synthetic-to-real domain gap, synthetic proxy representations are augmented using random body-part occlusion, 2D keypoint occlusion, noise and swapping, and extreme cropping, as detailed in Table \ref{tab:supmat_synthetic_data}.

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/supmat_sota_deterministic_fig_v2.jpg}
    \vspace{-0.1cm}
    \caption{Qualitative comparison between point estimates from our probabilistic method (HuManiFlow) and the state-of-the-art single-solution (i.e. deterministic) SMPL predictors PARE \cite{Kocabas_PARE_2021} and HybrIK \cite{li2020hybrik}. HybrIK gives highly accurate solutions on less-ambiguous images, but struggles with occlusion and truncation. Point estimates from HuManiFlow and PARE perform similarly, but predicting a distribution over pose and shape allows HuManiFlow to additionally estimate prediction uncertainty, which is visualised as directional per-vertex variance. The bottom two rows show some failure cases of our method, when faced with very challenging poses or extreme truncation. The estimated uncertainty is very high for these inputs, which may be used as a signal to discount the predictions as inaccurate.}
    \label{fig:supmat_sota_deterministic_fig}
\end{figure*}


\section{Evaluation Details}
\label{sec:supmat_eval_details}

\subsection{Datasets and metrics}

The 3DPW \cite{vonMarcard2018} and SSP-3D \cite{STRAPS2020BMVC} datasets are used for our ablation studies and comparison with other human pose and shape estimation approaches. The test set of 3DPW consists of 35515 images of 2 subjects paired with ground-truth SMPL parameters and 2D keypoint locations. SSP-3D consists of 311 images of 62 subjects with diverse body shapes, paired with pseudo-ground-truth SMPL parameters and 2D keypoint locations.

As discussed in Section \ref{sec:imp_details} of the main manuscript, we use mean-per-joint-position-error (MPJPE) and MPJPE after Procrustes analysis (MPJPE-PA) to evaluate the accuracy of our method. Both MPJPE and MPJPE-PA are in units of mm. Following \cite{hmrKanazawa17, kolotouros2019spin, kolotouros2019cmr, kolotouros2021prohmr}, MPJPE and MPJPE-PA are computed using the 14 LSP joint convention \cite{Johnson11lsp}. Sample-input consistency is quantified using 2D keypoint reprojection error (or 2DKP Error) between GT \textit{visible} 2DKPs and 2DKPs computed from predicted samples, for which we use the 17 COCO keypoint convention \cite{Lin2014MicrosoftCC}. 2DKP Error is in units of pixels, assuming a $256 \times 256$ input image. Sample diversity is measured using the spread (i.e. average Euclidean distance from the mean) of 3D visible/invisible keypoints, which is denoted as 3DKP Spread. This is also computed using the 17 COCO keypoints. 3DKP Spread is in units of mm. We recognise that 3DKP Spread is flawed as a diversity metric, since the  average Euclidean distance from the mean of 3DKPs may be too simplistic to accurately reflect the diversity of 3D body pose (i.e. body-part rotation) samples, particularly when evaluating highly complex multi-modal distributions. Future work can investigate improved diversity metrics for body pose distributions.

\begin{table}[t]
\centering
\small
\renewcommand{\tabcolsep}{3.5pt}
\begin{tabular}{l l| c c } 
\hline
\textbf{Dataset} & \textbf{Method} & \textit{Consistency} & \textit{Diversity}\\
& & 2DKP Error & 3DKP Spread\\
&  & Point / Samples & Vis. / Invis.\\
\hline
\hline
\multirow{4}{0.125\linewidth}{\textbf{3DPW} $\mathbf{70\%}$ \textbf{Cropped}} & Sengupta \etal \cite{sengupta2021probabilisticposeshape} & 7.6 / 9.9  &  39.7/ 97.2 \\
& 3D Multibodies \cite{biggs2020multibodies} & 8.1 / 11.7 & 66.5 / 125.9 \\
& ProHMR \cite{kolotouros2021prohmr} & 8.1 / 9.2 & 32.0 / 60.1 \\
& HierProbHuman \cite{sengupta2021hierprobhuman} & \textbf{7.2} / 9.5 & 41.8 / 102.3 \\
& HuManiFlow & \textbf{7.2} / \textbf{8.6} & 41.9 / 116.9 \\
\hline
\multirow{4}{0.125\linewidth}{\textbf{SSP-3D} $\mathbf{70\%}$ \textbf{Cropped}} & Sengupta \etal \cite{sengupta2021probabilisticposeshape} & 9.8 / 14.3  & 60.2 / 131.6 \\
& 3D Multibodies \cite{biggs2020multibodies} & 10.5 / 15.1 & 85.1 / 160.1 \\
& ProHMR \cite{kolotouros2021prohmr} &  9.0 / 10.2 &  37.7 / 64.4\\
& HierProbHuman \cite{sengupta2021hierprobhuman} &  7.0 / 9.8  & 55.0 / 107.1 \\
& HuManiFlow & \textbf{6.9} / \textbf{8.6} & 46.9 / 123.3 \\
\hline
\end{tabular}
\caption{Comparison between probabilistic pose and shape predictors in terms of sample-input consistency and sample diversity on $70\%$ cropped versions of 3DPW \cite{vonMarcard2018} and SSP-3D \cite{STRAPS2020BMVC}. Our method, HuManiFlow, yields the most input-consistent samples (lowest visible 2DKP error) with reasonable diversity (3DKP spread).}
\label{table:supmat_3dpw_sota_comparison_diversity_consistency}
\end{table}

\subsection{Cropped dataset generation}
\label{subsec:supmat_cropped_datasets}

To evaluate our method on highly ambiguous and challenging test inputs, we generate cropped versions of 3DPW \cite{vonMarcard2018} and SSP-3D \cite{STRAPS2020BMVC}. Cropped test images are computed from the (already pre-processed) full-view test images by (i) centering at approximately the midpoint of the subject's torso, (ii) taking a square crop with dimensions given by $\alpha \%$ of the full-view image dimensions $256 \times 256$, and (iii) resizing back to $256 \times 256$. In the main manuscript, we used $\alpha = 50\%$ for all experiments with cropped data. However, the cropping percentage may be varied to evaluate our method on images with different levels of ambiguity. In Section \ref{sec:supmat_experiments} of this supplementary material, we present additional results with $\alpha = 70\%$. Examples of $50\%$ and $70\%$ cropped test images are given in Figure \ref{fig:supmat_cropped_results}.

\subsection{Directional variance visualisation}

Figures \ref{fig:intro} and \ref{fig:comparison_fig} in the main manuscript, and Figure \ref{fig:supmat_cropped_results} in this supplementary material, visualise the per-vertex directional variance of samples drawn from predicted SMPL pose and shape distributions. For a given input image, per-vertex directional variance is computed by (i) drawing $N$ samples from the predicted distribution $\{\{\mathbf{R}^n_i\}_{i=1}^{23}, \boldsymbol{\beta}^n\}_{n=1}^N$, (ii) passing each of these through the SMPL \cite{SMPL:2015} model to obtain $N$ vertex meshes $\{\mathbf{V}^n\}_{n=1}^N$ (where each $\mathbf{V}^n \in \mathbb{R}^{6890 \times 3}$) and (iii) computing the variance (more specifically, the standard deviation) of each vertex along each of the x/y/z directions, or axes. We use $N=100$. Note that the coordinate axes are aligned with the image plane, such that the x-axis represents the horizontal direction on the image, the y-axis represents the vertical direction on the image and the z-axis represents depth perpendicular to the image.


\section{Experimental Results}
\label{sec:supmat_experiments}

\nbf{Results on $70\%$ cropped images} The main manuscript reported results on the 3DPW \cite{vonMarcard2018} and SSP-3D \cite{STRAPS2020BMVC} datasets, and $50\%$ cropped versions that are more ambiguous. In Tables \ref{table:supmat_3dpw_sota_comparison_accuracy} and \ref{table:supmat_3dpw_sota_comparison_diversity_consistency} of this supplementary material, we report additional results on $70\%$ cropped versions. Comparing this with the results in the main manuscript illustrates the behaviour of pose and shape prediction methods as ambiguity increases due to greater cropping/truncation. In Table \ref{table:supmat_3dpw_sota_comparison_accuracy}, we reproduce some of the metrics previously presented in the main manuscript (on 3DPW and 3DPW $50\%$ Cropped), for convenience. Figure \ref{fig:supmat_cropped_results} presents a qualitative comparison between our method and other probabilistic pose and shape predictors using original, $70\%$ and $50\%$ cropped images.



\nbf{Qualitative comparison with deterministic methods} Figure \ref{fig:supmat_sota_deterministic_fig} compares point estimates from our method with the state-of-the-art single-solution (i.e. deterministic) monocular SMPL prediction approaches \cite{Kocabas_PARE_2021, li2020hybrik}. Figure \ref{fig:supmat_sota_deterministic_fig} also illustrates some failure cases of our method (bottom two rows) due to challenging poses and extreme truncation. We note that our approach also tends to over-estimate body shape when the subject is wearing baggy clothes, which is due to the low-fidelity synthetic training data pipeline we adopt from \cite{sengupta2021hierprobhuman}.




\nbf{Our ablation models vs. competing methods} Table \ref{table:3dpw_ablation_distributions} in the main manuscript presents our ablation study comparing several different SMPL pose distribution modelling approaches. Some of these ablation ablation models are, in fact, very similar to previously proposed probabilistic SMPL prediction methods. Specifically, the Gaussian distribution over full-body concatenated axis-angles (row 1 of Table \ref{table:3dpw_ablation_distributions}) is similar to \cite{sengupta2021probabilisticposeshape}. The normalising flow distribution over full-body concatenated axis-angles (row 3 of Table \ref{table:3dpw_ablation_distributions}) is similar to ProHMR \cite{kolotouros2021prohmr}. However, we use linear rational spline coupling layers \cite{dolatabadi2020lrs}, which are more expressive than the additive coupling layers \cite{dinh2015nice} used by \cite{kolotouros2021prohmr}. Moreover, we do not use a 6D rotation representation \cite{Zhou_2019_CVPR} for distribution prediction, to avoid the need for an orthogonality-enforcing loss, and take into account the non-Euclidean structure of $SO(3)$. Finally, the Matrix-Fisher distribution over body-part rotations (row 7 of Table \ref{table:3dpw_ablation_distributions}) is similar to HierProbHumans \cite{sengupta2021hierprobhuman}. However, as noted in the main manuscript, \cite{sengupta2021hierprobhuman} conditions body-part rotations on ancestor \textit{distribution parameters}, while we condition directly on ancestor \textit{rotations}. Our approach is more akin to a usual autoregressive model, and allows our method to be more input-consistent. For fairness, we re-train HierProbHumans with the same losses as HuManiFlow, and report results in Figure \ref{fig:supmat_rebuttal_param_vs_rot_cond}. 

\begin{figure}[t!]
    \centering
    \includegraphics[width=\columnwidth]{figures/rebuttal_fig1.png}
    \caption{\textbf{Conditioning on rotation samples vs Matrix-Fisher parameters}, evaluated on 3DPW. Row 1 is HierProbHumans \cite{sengupta2021hierprobhuman}. Row 4 is HuManiFlow. Row 2 is HierProbHumans trained with the same losses as HuManiFlow - i.e. no point-based 3DKP losses. This increases diversity, but accuracy and consistency suffer. Row 3 improves these and maintains diversity, by changing HierProbHumans to \textit{condition on rotations}. This suggests that rotation-conditioning without point-based losses performs best. All models have the same backbone and no. of parameters (approx.), and are trained on the same data.}
    \vspace{-0.1in}
    \label{fig:supmat_rebuttal_param_vs_rot_cond}
\end{figure}


\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.94\linewidth]{figures/supmat_cropped_results.jpg}
    \vspace{-0.1cm}
    \caption{Qualitative comparison between our method (HuManiFlow), ProHMR \cite{kolotouros2021prohmr} and 3D Multibodies \cite{biggs2020multibodies} on original, $50\%$ cropped and $70\%$ cropped images (cropping details given in Section \ref{subsec:supmat_cropped_datasets}). HuManiFlow yields more \textit{diverse} pose and shape samples than ProHMR, and more \textit{input-consistent }samples than 3D Multibodies. The directional variance visualisation shows that HuManiFlow captures prediction uncertainty due to depth ambiguity (z-axis), occlusions and truncations (all-axes) in a more interpretable manner than \cite{kolotouros2021prohmr} and \cite{biggs2020multibodies}.}
    \label{fig:supmat_cropped_results}
\end{figure*}