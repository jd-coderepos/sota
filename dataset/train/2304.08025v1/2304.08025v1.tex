\def\figVisualizationsSuppResidual#1{
\begin{figure*}[#1]
\centering
\includegraphics[width=1.0\textwidth]{figures/visualization_residual.pdf}
\caption{\textbf{Visualizations for both the piecewise constant and the residual pathways} show that the introduction of the residual pathway allows our segmentation prediction to better fit the flow of deformable and articulated objects. In addition, it also relieves our segmentation module from strictly fitting the flow from 3D rotation and changing depth in a piecewise constant manner. By modeling relative motion in 2D flow, the residual pathway makes our method flexible and robust to objects with complex motion.}
\label{fig:visualizations_residual}
\end{figure*}
}

\def\figVisualizationsSuppDAVIS#1{
\begin{figure*}[#1]
\centering
\includegraphics[width=1.0\textwidth]{figures/visualization_supp1.pdf}
\caption{\textbf{Additional visualizations on DAVIS16 \cite{perazzi2016benchmark}.} Our method remains robust in scenes where there is insufficient motion information, in which cases our method leverages appearance cues to learn high-quality segmentation in (a) to (e). Our method accurately segments multiple foreground objects as foreground when they move together, which is consistent with human perception in (b). However, our method may exclude a portion of an object in (f), since the motion misses part of the front wheel of the bicycle and the structure is too small for appearance to capture.}
\label{fig:visualizations_davis}
\end{figure*}
}

\def\figVisualizationsSuppST#1{
\begin{figure*}[#1]
\centering
\includegraphics[width=1.0\textwidth]{figures/visualization_supp2.pdf}
\caption{\textbf{Additional visualizations on STv2 \cite{li2013video}.} Our method, with the residual flow, could model non-uniform 2D flow resulting from object rotation in 3D in (a), as long as the rotation flow falls within our upper bound constraint for the residual flow. Our method also captures multiple objects in a foreground group in (b), (c), and (e). Our method is robust to camera motion that leads to non-uniform background flow in (c) and misleading common motion (reflections) in (d). However, due to the relatively low image resolution, our method may miss some details of the object. For example, the legs of both animals in (f) and the wings of the bird in (g).}
\label{fig:visualizations_st}
\end{figure*}
}

\def\figVisualizationsSuppFBMS#1{
\begin{figure*}[#1]
\centering
\includegraphics[width=1.0\textwidth]{figures/visualization_supp3.pdf}
\caption{\textbf{Additional visualizations on FBMS59 \cite{brox2010freiburg,ochs2013segmentation}.} Our method is robust in scenes with complicated and distracting appearances in (a). Our method also works with fine details in (b) and (e). Our method accurately segments multiple foreground objects in (c) and (d). However, when multiple objects or object parts exist in one scene and exhibit different motion patterns, our method may be confused in (f) and (g).}
\label{fig:visualizations_fbms59}
\end{figure*}
}

\def\figTuningAMD#1{
\begin{figure}[#1]
\centerline{\includegraphics[width=1.0\linewidth]{figures/tune_amd.pdf}}
\caption{\textbf{Our hyperparameter tuning technique is model agnostic.} When using our model-agnostic hyperparameter tuning technique on AMD \cite{liu2021emergence}, the tuned hyperparameters from unsupervised motion-semantic alignment greatly resemble the ones obtained with human annotation. In this training run, the number of segmentation channels is 4 to be optimal, and the object channel index is 1 from \textit{both} our motion-semantic alignment and validation performance. Although $c_o$ varies in each training run by design \cite{liu2021emergence}, our tuning method has negligible overhead and could be performed after training to find $c_o$ \textit{within seconds}.
}
\label{fig:tuning_amd}
\end{figure}
}

\def\tabFlowMethod#1{
\begin{table}[#1]
\tablestyle{1.0pt}{1.0}
\centering
\begin{tabular}{c|c|c|c|c|c|c|c|c}
\shline
 Method & CIS & MG & EM & SIMO & Tok.Cut & GWM & OCLR & \textbf{RCF} \\
 \shline
Flow Model & PWCNet & RAFT & RAFT & RAFT & RAFT & RAFT & RAFT & \textbf{RAFT} \\
\shline
\end{tabular}
\caption{\textbf{Optical flow methods that each UVOS approach employs by default.} All methods in the table use pretrained weights for flow estimation. We utilize RAFT flow with pretrained weights from synthetic data, which is common among all the UVOS methods. Other than the methods listed in the table, AMD trains PWCNet \cite{sun2017pwc} architecture from scratch but achieves much lower performance compared to RCF.}
\label{tab:flow-method}
\end{table}
}

\def\tabAblationFlowMethod#1{
\begin{table}[#1]
\tablestyle{2pt}{1.0}
\centering
\begin{tabular}{c|c|c|c|c}
\shline
 Method & ARFlow \cite{liu2020learning} & PWCNet \cite{sun2017pwc} & GMFlow \cite{xu2022gmflow} & \textbf{RAFT} \cite{teed2020raft} \\
 \shline
DAVIS16 $\mathcal{J} (\uparrow)$ & 70.3 & 74.8 & 76.6 & \textbf{78.9} \\
\shline
\end{tabular}
\caption{\textbf{Our method with different optical flow estimation methods.} We use pretrained optical flow on synthetic data for supervised optical flow methods. We benchmark stage 1 only since we leverage motion supervision mostly in stage 1.}
\label{tab:ablation-flow-method}
\end{table}
}

\def\tabDAVISPerSequence#1{
\begin{table}[#1]
\setlength{\tabcolsep}{20pt}
\centering
\begin{tabular}{lc}
\shline
Sequence & $\mathcal{J}$ \\
\shline
blackswan & 76.2 \\
bmx-trees & 78.3 \\
breakdance & 86.1 \\
camel & 92.7 \\
car-roundabout & 80.7 \\
car-shadow & 80.4 \\
cows & 88.0 \\
dance-twirl & 90.4 \\
dog & 91.7 \\
drift-chicane & 94.1 \\
drift-straight & 65.6 \\
goat & 81.6 \\
horsejump-high & 93.4 \\
kite-surf & 53.1 \\
libby & 96.6 \\
motocross-jump & 57.0 \\
paragliding-launch & 26.0 \\
parkour & 95.8 \\
scooter-black & 72.4 \\
soapbox & 86.1 \\
\hline
Frame Avg & 83.0 \\
\shline
\end{tabular}
\caption{\textbf{Per sequence Jaccard index $\mathcal{J}$ on DAVIS16 \cite{perazzi2016benchmark}.}}
\label{tab:davis16_per_seq}
\end{table}
}

\def\tabSTPerSequence#1{
\begin{table}[#1]
\setlength{\tabcolsep}{20pt}
\centering
\begin{tabular}{lc}
\shline
Sequence & $\mathcal{J}$ \\
\shline
bird of paradise & 91.7 \\
birdfall & 60.4 \\
bmx & 76.6 \\
cheetah & 52.4 \\
drift & 86.3 \\
frog & 82.2 \\
girl & 80.6 \\
hummingbird & 67.6 \\
monkey & 82.5 \\
monkeydog & 55.5 \\
parachute & 93.2 \\
penguin & 66.2 \\
soldier & 79.8 \\
worm & 85.6 \\
\hline
Frame Avg & 79.6 \\
\shline
\end{tabular}
\caption{\textbf{Per sequence Jaccard index $\mathcal{J}$ on STv2 \cite{li2013video}.}}
\label{tab:stv2_per_seq}
\end{table}
}

\def\tabFBMSPerSequence#1{
\begin{table}[#1]
\setlength{\tabcolsep}{20pt}
\centering
\begin{tabular}{lc}
\shline
Sequence & $\mathcal{J}$ \\
\shline
camel01 & 88.3 \\
cars1 & 86.4 \\
cars10 & 38.2 \\
cars4 & 70.3 \\
cars5 & 79.3 \\
cats01 & 88.2 \\
cats03 & 82.0 \\
cats06 & 59.7 \\
dogs01 & 74.4 \\
dogs02 & 91.6 \\
farm01 & 82.6 \\
giraffes01 & 65.9 \\
goats01 & 89.8 \\
horses02 & 86.2 \\
horses04 & 88.6 \\
horses05 & 71.6 \\
lion01 & 84.9 \\
marple12 & 79.3 \\
marple2 & 73.7 \\
marple4 & 87.8 \\
marple6 & 50.8 \\
marple7 & 32.1 \\
marple9 & 38.4 \\
people03 & 42.9 \\
people1 & 86.1 \\
people2 & 88.0 \\
rabbits02 & 93.8 \\
rabbits03 & 85.9 \\
rabbits04 & 20.2 \\
tennis & 78.6 \\
\hline
Frame Avg & 72.4 \\
\shline
\end{tabular}
\caption{\textbf{Per sequence Jaccard index $\mathcal{J}$ on FBMS59 \cite{brox2010freiburg,ochs2013segmentation}.}}
\label{tab:fbms59_per_seq}
\end{table}
}

\def\tabResidualFlowInitUpperBound#1{
\begin{table}[#1]
\setlength{\tabcolsep}{2.25pt}
\centering
\begin{tabular}{lcccccccc}
\shline
Upper bound $\lambda$ & 1 & 5 & \textbf{10} & 20 & 50 & 100 & 200 & 400 \\
\shline
\textbf{Ours Init} & 72.7 & 76.5 & \textbf{78.9} & 78.3 & 78.3 & 77.4 & 72.8 & 78.3 \\
Default Init & 72.7 & 76.0 & 78.1 & 78.5 & 73.5 & 73.4 & 73.3 & {\color{red}1.0} \\
\shline
\end{tabular}
\caption{\textbf{Using a small initialization and upper bound is important for the residual flow pathway in our method.} Ours Init refers to an initialization scheme which is 10x smaller than PyTorch default init. Red color indicates {\color{red}collapses}.}
\label{tab:residual-flow-init-upper-bound}
\end{table}
}

\def\tabMotionAppearanceWD#1{
\begin{table}[#1]
\setlength{\tabcolsep}{8pt}
\centering
\begin{tabular}{lccccc}
\shline
Weight Decay & $10^{-6}$ & $10^{-4}$ & $10^{-2}$ \\
\shline
Motion-app. Alignment & -0.672 & \textbf{-0.670} & -0.768 \\
Subset 1 mIoU & 77.2 & \textbf{77.6} & 75.7 \\
Subset 2 mIoU & 77.0 & \textbf{80.5} & 72.0 \\
Subset 3 mIoU & \textbf{77.3} & 76.8 & 76.2 \\
Full val mIoU & 77.2 & \textbf{78.9} & 74.8 \\
\shline
\end{tabular}
\caption{\textbf{Applying motion-appearance alignment provides the optimal weight decay without using labels.} In contrast, using subset mIoU misses the optimal value in one of the three runs. Higher metric values indicate higher segmentation quality for all metrics.}
\label{tab:motion-appearance-alignment-wd}
\end{table}
}


\def\algHyperparamTuning#1{
\begin{algorithm}[#1]
\caption{Pseudo-code for using motion-appearance alignment for hyperparameter tuning}
\label{alg:hyperparameter-tuning}
\begin{algorithmic}
\Require A set of frames $\{I\}$ with $N$ frames
\Require A set of settings with different hyperparameter values $\{S\}$
\Ensure A chosen optimal setting $S^*$ according to motion-appearance-alignment
\For{each setting $S$ in $\{S\}$}
\State Train a model with setting $S$
\State Obtain prediction masks $\{M\}$ with trained model
\For{each frame-mask pair ($I_i$, $M_i$) in $\{I\}, \{M\}$}
    \State Calculate affinity $A$ from frozen ViT features:
    \State $A_{ij}=\mathbb{1}(\text{sim}(f_\text{aux}(I_t)_i, f_\text{aux}(I_t)_j) \geq 0.2)$
    \State Calculate cut between the predicted foreground and background $\text{Cut}(A, \vec{x})$:
    \State $\vec{x} \gets \text{Flatten}(M_i)$
    \State $\text{Cut}(A, \vec{x}) = (1-\vec{x}) A \vec{x}$
    \State Calculate normalized cut between the predicted foreground and background $\text{NCut}(A, \vec{x})$:
    \State $\text{NCut}(A, \vec{x}) = \frac{\text{Cut}(A, \vec{x})}{\sum_{i=1}^{HW} (A\vec{x})_i}
        + \frac{\text{Cut}(A, \vec{x})}{\sum_{i=1}^{HW} (A(1-\vec{x}))_i}$
    \State Calculate the motion-appearance alignment for the current frame:
    \State $L_i \gets -\text{NCut}(A, \vec{x})$
\EndFor
\State $L_{S} \gets \frac{1}{N}\sum_{i=1}^N L_i$
\EndFor
\State $S^* = \arg \max_{S} L_S$
\end{algorithmic}
\end{algorithm}
}


\figVisualizationsSuppResidual{!p}
\figVisualizationsSuppDAVIS{!p}
\figVisualizationsSuppST{!p}
\figVisualizationsSuppFBMS{!p}

\section{Additional Visualizations and Discussions}
We present additional visualizations on the three main datasets that we benchmark our method on \cite{perazzi2016benchmark,li2013video,brox2010freiburg,ochs2013segmentation}. We demonstrate high-quality segmentation in several challenging cases and discuss some limitations of our method with examples.

\subsection{Visualizations of the Residual Pathway}
As shown in \cref{fig:visualizations_residual}, the introduction of the residual pathway allows our segmentation prediction to better fit the flow of deformable and articulated objects. In addition, it also relieves our segmentation module from strictly fitting the flow from 3D rotation and changing depth in a piecewise constant manner. By modeling relative motion in 2D flow, the residual pathway makes our method flexible and robust to objects with complex motion.

\subsection{DAVIS2016, SegTrackv2, and FBMS59}
We visualize our methods on DAVIS2016, SegTrackv2, and FBMS59 in \cref{fig:visualizations_davis}, \cref{fig:visualizations_st}, and \cref{fig:visualizations_fbms59}, respectively. Our method shows great robustness in challenging scenes where there is insufficient motion information, due to its ability to leverage both motion and appearance.


\section{Additional Experiments}
Unless otherwise stated, all the ablation experiments in this section include only stage 1, as the ablations in this section are not relevant to the appearance supervision. Results are without post-processing.

\subsection{Abltion on Different Optical Flow Estimation Methods}
As listed in \cref{tab:flow-method}, almost all recent UVOS works rely on a separate optical flow model pretrained on synthetic data. We use RAFT \cite{teed2020raft} flow by default, following previous works in UVOS. AMD trains \cite{sun2017pwc} from scratch but achieves much lower mIoU.

To evaluate our method's robustness to optical flow estimation methods, we evaluate our method on PWCNet \cite{sun2017pwc}, GMFlow \cite{xu2022gmflow}, and self-supervised ARFlow \cite{liu2020learning}, in addition to RAFT \cite{teed2020raft}.

As shown in \cref{tab:ablation-flow-method}, our method suffers from a mild drop with noisier optical flow. However, our performance is largely retained without tuning the hyperparameters when employing other optical flow methods. We believe the performance gap between different optical flow estimation methods will be reduced further with additional hyperparameter tuning on each flow estimation method.

\tabFlowMethod{t!}
\tabAblationFlowMethod{t!}

\subsection{Preventing Trivial Solutions for Residual Flow Prediction}
There are two factors that prevent trivial solutions: \textbf{1)} Regularization with upper bound $\lambda$ limits the residual prediction to only capturing small relative motion~(10 pixels by default). \textbf{2)} The residual flow branch is initialized to be small, which favors the solution to be simple motion patterns.

As shown in \cref{tab:residual-flow-init-upper-bound}, the results (mIoU on DAVIS16) show that small residual initialization allows RCF to be insensitive to a large range of $\lambda$ against performance degradation or {\color{red}collapses}, even though setting $\lambda$ too large will still cause instability in the form of large mIoU fluctuations. With small residual initialization, $\lambda$ is relatively stable to tune. 

\subsection{Applying Motion-appearance Alignment to Non-method Specific Hyperparameters}
To explore the possibility of using our proposed label-free hyperparameter tuning method to tune hyperparameters that are non-method specific, we evaluate our metric on runs with three different weight decay values: $10^{-6}$ and $10^{-2}$ in addition to our default value of $10^{-4}$. We choose this range of hyperparameter values since we observed that varying the weight decay by smaller amounts had a negligible impact on the final mIoU. As in other hyperparameter tuning experiments, we randomly sample $25\%$ of the sequences from the validation set three times and evaluate the effect of using a smaller labeled validation subset for comparison. Shown in \cref{tab:motion-appearance-alignment-wd}, while the mIoU values from the labeled validation subsets vary significantly between samplings, with one of the three runs missing the optimal value, our metric follows the full validation mIoU trend and selects the best hyperparameter values among the three.

\tabResidualFlowInitUpperBound{t}
\tabMotionAppearanceWD{t}

\section{Pseudo-code for Hyperparameter Tuning With Motion-appearance Alignment}
\algHyperparamTuning{t}
We present the pseudo-code for hyperparameter tuning with motion-appearance alignment in \cref{alg:hyperparameter-tuning}.

\section{Additional Implementation Details}


Our setting mostly follows previous works \cite{liu2021emergence,choudhury2022guess}. Following the official implementation in \cite{liu2021emergence}, we treat the video frame pair $\{t, t+1\}$ as both a forward action from time $t$ to time $t+1$ and a backward action from time $t+1$ and $t$, since they follow similar rules for visual grouping. Therefore, we use this to implement a symmetric loss that applies the loss function on both forward and backward. We then sum the forward loss and backward loss up to obtain the final loss. Note that this could be understood as a data augmentation technique that always supplies a pair in forward and backward to the training batch. However, since our ResNet shares weights for each image input, the feature for each input is reused by the forward and backward action. Furthermore, our residual prediction head has four times the number of channels of the segmentation head to separately predict the forward/backward flow in horizontal/vertical directions, due to its better performance. Thus, the symmetric loss only adds marginal computation and is included in our implementation as well.

Furthermore, following \cite{liu2021emergence}, for DAVIS16, we use random crop augmentation during training to crop a square image from the original image. At test time, we directly input the original image, which is non-square. It is worth noting that the augmentation makes the image size different for training and testing, but as ResNet \cite{he2016deep} takes images of different sizes, this does not pose a problem empirically. In STv2 and FBMS59, the images have very different aspect ratios (some having a height lower than the width), and thus we resize the images to 480p as a preprocessing before the standard pipeline. We additionally use pixel-wise photo-metric transformation \cite{mmseg2020} for augmentation with the default hyperparameters for this augmentation.

As for the architecture, we found that simply taking the feature from the last ResNet stage provides insufficient detailed information for high-quality output. Instead of incorporating a more complicated segmentation head (\eg, \cite{cheng2021maskformer} in \cite{choudhury2022guess}), we chose to keep our architecture easy to implement by only changing the head in a simple fashion. Following the standard approach of multi-scale feature fusion, we resized and concatenated the feature from the first residual block and the last residual block in ResNet, which allows the feature to jointly capture high-level information and low-level details. Note that such fusion is only applied to the segmentation head, and residual prediction is simply bilinearly upsampled. Due to lower image resolution, no feature merging is performed for STv2 in stage 1. Following \cite{choudhury2022guess}, we load self-supervised ImageNet pretrained weights learned without annotation, since the training video datasets are too small for learning generalizable feature (\eg, DAVIS16/STv2/FBMS59 has only 3,455/976/13,860 frames), with DenseCL weights \cite{ILSVRC15,wang2021dense} on ResNet50 for our method. This can be replaced by training on uncurated Youtube-VOS \cite{xu2018youtube} with our training process, as in \cite{liu2021emergence}, so that one implementation can be used throughout training for simplicity in real-world applications.

In our training, we follow \cite{liu2021emergence} and use a batch size of 16 (with two images in a pair, and thus 32 images processed in each forward pass). Stage 1 and stage 2 take 200 and 40 epochs, respectively, for DAVIS16. We use a learning rate of $1\times10^{-4}$ with Adam optimizer \cite{kingma2014adam} and polynomial decay (factor of $0.9$, min learning rate of $1\times10^{-6}$). We set weight decay to $1\times10^{-4}$ for DAVIS and $1\times10^{-6}$ for STv2 and FBMS59. Due to the fact that normalized cuts is slow to optimize, we split stage 2 into two sub-stages: one with the CRF followed by one with normalized cuts optimization, each of the stage has the same number of training steps. In the CRF substage in stage 2, we set $w_\text{motion}=1$ and $w_\text{app}=10$ to balance the two losses. However, we observe training instability if we supervise the network directly by its output refined by the CRF. Therefore, we apply exponential moving averaging (EMA) to the model weights and supervise the network by the output from the EMA model, with momentum $m=0.999$. In the normalized cuts substage, we pre-generate the network's outputs and use the refinement as described in the methods section, which involves running CRF before and after normalized cuts refinement and multiplying the refined masks from the two CRF runs. This is equivalent to applying such refinement with EMA with $m=1.0$. In this substage, we set $w_\text{motion}=0.1$ and $w_\text{app}=2.0$.

\section{Per-sequence Results}
\tabDAVISPerSequence{p}
\tabSTPerSequence{p}
\tabFBMSPerSequence{p}

We list our per-sequence results on DAVIS16 \cite{perazzi2016benchmark}, STv2\cite{li2013video}, FBMS59\cite{brox2010freiburg,ochs2013segmentation} in \cref{tab:davis16_per_seq}, \cref{tab:stv2_per_seq}, and \cref{tab:fbms59_per_seq}, respectively. The results are with post-processing.

\section{Future Directions}
As our method does not impose temporal consistency, it does not effectively leverage information redundancy from neighboring frames. Using such information could make our method more robust in dealing with frames that provide insufficient motion and appearance information. Temporal consistency measures, such as matching warped predictions, could be incorporated as an additional loss term or as post-processing, as in \cite{yang2019unsupervised}.

Furthermore, our method currently does not support segmenting multiple parts of the foreground or identifying each object instance. To address this, methods such as normalized cuts \cite{shi2000normalized} could be used to split the foreground into several objects with motion and appearance input to provide signals to train the model. Another potential approach is to over-split the scene with many object channels and use other unsupervised methods such as FreeSOLO \cite{wang2021dense, wang2022freesolo} to obtain coarse segmentation proposals to merge the channels to form object instance segmentation.

\FloatBarrier

\clearpage
