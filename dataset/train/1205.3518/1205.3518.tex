\newif\ifconf\conffalse

\ifconf
\documentclass[twoside,leqno,twocolumn]{article}
\usepackage{ltexpprt}

\else
\documentclass[11pt,letterpaper]{article}
\usepackage{amsthm}
\usepackage[margin=1in,dvips]{geometry}
\fi

\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{float}
\usepackage{enumerate}

\iffalse
\newtheorem{definition}{Definition}[section]
\newtheorem{remark}{Remark}[section]
\else
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{fact}[theorem]{Fact}
\fi

\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\inner}[1]{\langle#1\rangle}
\newcommand{\floor}[1]{\left\lfloor#1\right\rfloor}
\newcommand{\norm}[2]{\left \lVert#2\right \rVert_{#1}}
\newcommand{\head}[2]{#1_{[#2]}}
\newcommand{\tail}[2]{#1_{\overline{[#2]}}}
\newcommand{\range}[3]{#1_{\overline{[#2]}\cap[#3]}}
\newcommand{\I}{\mathrm{I}}
\newcommand{\gaplinf}{\mathsf{Gap}\ell_{\infty}}
\newcommand{\gaplinfL}{\mathsf{LooseGap}\ell_{\infty}}
\newcommand{\multiLinf}{\mathsf{Multi}\ell_{\infty}}
\newcommand{\indlinf}{\mathsf{Ind}\ell_{\infty}}
\newcommand{\indlinfL}{\mathsf{LooseInd}\ell_{\infty}}
\newcommand{\IC}{\mathrm{IC}}

{\makeatletter
 \gdef\xxxmark{\expandafter\ifx\csname @mpargs\endcsname\relax \expandafter\ifx\csname @captype\endcsname\relax \marginpar{xxx}\else
       xxx \fi
   \else
     xxx \fi}
 \gdef\xxx{\@ifnextchar[\xxx@lab\xxx@nolab}
 \long\gdef\xxx@lab[#1]#2{{\bf [\xxxmark #2 ---{\sc #1}]}}
 \long\gdef\xxx@nolab#1{{\bf [\xxxmark #1]}}
}


\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator*{\E}{\mathbb{E}}
\def\R{\mathbb{R}}
\def\Z{\mathbb{Z}}
\def\eps{\epsilon}

\begin{document}

\title{Lower Bounds for Adaptive Sparse Recovery}

\author{Eric Price\\MIT\\ecprice@mit.edu \and David P. Woodruff\\IBM Almaden\\dpwoodru@us.ibm.com}
\date{}
\maketitle
\thispagestyle{empty}
\setcounter{page}{0}

\begin{abstract}
  We give lower bounds for the problem of stable sparse recovery from
  \emph{adaptive} linear measurements.  In this problem, one would
  like to estimate a vector  from  linear measurements
  .  One may choose each vector  based on
  , and must output  satisfying
  
  with probability at least , for some .  For
  , it was recently shown that this is possible with , while nonadaptively it requires
  .  It is also known that even
  adaptively, it takes  for . For ,
  there is a non-adaptive upper bound of
  .  We show:
  \begin{itemize}
  \item For , . This is tight for  and constant , and shows that the 
    dependence is correct.
  \item If the measurement vectors are chosen in  ``rounds'', then
    .  For constant , this matches
    the previously known upper bound up to an  factor in .
  \item For , . This shows that adaptivity
    cannot improve more than logarithmic factors, providing the analogue of the 
     bound for . 
  \end{itemize}
\end{abstract} 
\newpage

\section{Introduction}


\emph{Compressed sensing} or \emph{sparse recovery} studies the
problem of solving underdetermined linear systems subject to a
sparsity constraint.  It has applications to a wide variety of fields,
including data stream algorithms~\cite{M05}, medical or geological
imaging~\cite{CRT06,D06}, and genetics testing~\cite{SAZ}.  The
approach uses the power of a \emph{sparsity} constraint: a vector 
is \emph{-sparse} if at most  coefficients are non-zero.  A
standard formulation for the problem is that of \emph{stable sparse
  recovery}: we want a distribution  of matrices  such that, for any  and with probability
 over , there is an algorithm to recover
 from  with

for some parameter  and norm .  We refer to the elements
of  as \emph{measurements}.  We say Equation~\eqref{eq:lplp}
denotes \emph{ recovery}.

The goal is to minimize the number of measurements while still
allowing efficient recovery of .  This problem has recently been
largely closed: for , it is known that  is tight~(upper bounds in \cite{CRT06,GLPS}, lower
bounds in \cite{PW11,CD11}), and for  it is known that  and ~\cite{PW11} (recall that
 means  for some constant c, and
similarly  means ).

In order to further reduce the number of measurements, a number of recent works
have considered making the measurements
\emph{adaptive}~\cite{JXC,CHNR,HCN,HBCN,MSW,AWZ,IPW11}.
In this setting, one may choose each row of the matrix after seeing
the results of previous measurements.  More generally, one may split
the adaptivity into  ``rounds'', where in each round  one
chooses  based on .
At the end, one must use  to output 
satisfying Equation~\eqref{eq:lplp}.  We would still like to minimize
the total number of measurements .  In the 
setting, it is known that for arbitrarily many rounds
 measurements suffice, and for
 rounds 
measurements suffice~\cite{IPW11}.  

Given these upper bounds, two natural questions arise: first, is the
improvement in the dependence on  from  to  tight, or can the improvement be strengthened?  Second, can
adaptivity help by more than a logarithmic factor, by improving the
dependence on  or ?

A recent lower bound showed that  measurements are
necessary in a setting essentially equivalent to the 
case~\cite{ACD11}\footnote{Both our result and their result apply in
  both settings.  See Appendix~\ref{app:noisekind} for a more detailed
  discussion of the relationship between the two settings.}.  Thus, they
answer the second question in the negative for .
Their techniques rely on special properties of the
-norm; namely, that it is a rotationally invariant inner product
space and that the Gaussian is both -stable and a maximum entropy
distribution.  Such techniques do not seem useful for proving lower
bounds for .

\paragraph{Our results.} For , we show that any adaptive sparse
recovery scheme requires  measurements, or
 measurements given only  rounds.  For , this matches the upper bound of~\cite{IPW11} up to an
 factor in .  It thus shows that the 
term in the adaptive bound is necessary.

For , we show that any adaptive sparse recovery scheme requires
 measurements. This shows that
adaptivity can only give  improvements, even for .  Additionally, our bound of  improves the previous \emph{non-adaptive} lower
bound for  and small , which lost an additional 
factor~\cite{PW11}.

\paragraph{Related work.} Our work draws on the lower bounds for
non-adaptive sparse recovery, most directly~\cite{PW11}.

The main previous lower bound for adaptive sparse recovery gets  for ~\cite{ACD11}.  They consider going down a
similar path to our  lower bound, but ultimately
reject it as difficult to bound in the adaptive setting.  Combining
their result with ours gives a 
lower bound, compared with the 
upper bound. The techniques in their paper do not imply any bounds for
the  setting.

For  in the special case of adaptive Fourier measurements (where
measurement vectors are adaptively chosen from among  rows of the
Fourier matrix),~\cite{HIKP12} shows  measurements are necessary.  In this case the main difficulty with
lower bounding adaptivity is avoided, because all measurement rows are
chosen from a small set of vectors with bounded  norm;
however, some of the minor issues in using~\cite{PW11} for an adaptive
bound were dealt with there.

\paragraph{Our techniques.}  We use very different techniques for our
two bounds.

To show  for , we reduce to the information
capacity of a Gaussian channel.  We consider recovery of the vector , for  uniformly and .  Correct recovery must find , so the mutual
information  is .  On the other hand, in
the nonadaptive case~\cite{PW11} showed that each measurement 
is a power-limited Gaussian channel with constant signal-to-noise
ratio, and therefore has .  Linearity gives that
, so  in the nonadaptive case.
In the adaptive case, later measurements may ``align'' the row 
with , to increase the signal-to-noise ratio and extract more
information---this is exactly how the upper bounds work.  To deal with
this, we bound how much information we can extract as a function of
how much we know about .  In particular, we show that given a
small number  bits of information about , the posterior
distribution of  remains fairly well ``spread out''.  We then
show that any measurement row  can only extract  bits
from such a spread out distribution on .  This shows that the
information about  increases at most exponentially, so
 measurements are necessary.

To show an  bound for , 
we first establish a lower bound on the multiround distributional 
communication complexity
of a two-party communication problem that we call , for a 
distribution tailored to our application. 
We then show how to use an adaptive -approximate 
 sparse
recovery scheme  
to solve the communication problem , modifying the 
general framework
of \cite{PW11} for connecting non-adaptive schemes to communication 
complexity in
order to now support adaptive schemes. 
By the communication lower bound for , 
we obtain a lower bound on the number of measurements required of . 

In the  problem, the two players are given  and  respectively,
and they want to approximate  given the promise that all entries
of  are small in magnitude or there is a single large entry. The 
problem consists of solving multiple independent instances of  in parallel. Intuitively,
the sparse recovery algorithm needs to determine if there are entries of  that are large, which
corresponds to solving multiple instances of . 
We prove a multiround direct sum theorem for a distributional version of , 
thereby giving a distributional lower bound for . A direct sum theorem for 
has been used before for proving lower bounds
for non-adaptive schemes \cite{PW11}, but was limited to a bounded
number of rounds due to the use of a bounded round theorem in
communication complexity \cite{br11}.
We instead use the information complexity framework \cite{BJKS04} to 
lower bound the conditional mutual information between the inputs to 
and the transcript of any correct protocol for  under a certain input distribution, and prove a 
direct sum theorem for solving  instances of this problem. 
We need to condition on ``help variables'' in the mutual information which enable the players
to embed single instances of  into  in a way in which the players can use
a correct protocol on our input distribution for  
as a correct protocol on
our input distribution for ; these help variables are 
in addition to help variables used for proving
lower bounds for , which
is itself proved using information complexity. We also look 
at the conditional mutual information with respect to an input distribution 
which doesn't immediately fit into the information complexity framework. 
We relate the conditional information of the transcript with respect to this
distribution to that with respect to a more standard distribution. 
\section{Notation}

We use lower-case letters for fixed values and upper-case letters for
random variables.  We use  to denote , and 
to denote .  For a discrete random variable  with
probability , we use  or  to denote its entropy

For a continuous random variable  with pdf , we use  to
denote its differential entropy

Let  be drawn from a random variable .  Then  denotes the random variable  conditioned on .
We define .  The mutual
information between  and  is denoted .

For  and , we define  to
equal  over indices in  and zero elsewhere.

We use  to denote .

\section{Tight lower bound for  }

We may assume that the measurements are orthonormal, since this can be
performed in post-processing of the output, by multiplying  on the
left to orthogonalize .  We will give a lower bound for the
following instance:

Alice chooses random  and i.i.d.\ Gaussian noise 
with , then sets .  Bob performs  rounds of adaptive measurements on
, getting  in each round
.  Let  and  denote the random variables from which 
and  are drawn, respectively.  We will bound .

We may assume Bob is deterministic, since we are giving a lower bound
for a distribution over inputs -- for any randomized Bob that succeeds
with probability , there exists a choice of random seed such
that the corresponding deterministic Bob also succeeds with
probability .

First, we give a bound on the information received from any single
measurement, depending on Bob's posterior distribution on  at
that point:

\begin{lemma}\label{lemma:oneround}
  Let  be a random variable over  with probability
  distribution , and define
  
  Define .  Consider any fixed
  vector  independent of  with , and
  define .  Then
  
  for some constant .
\end{lemma}
\begin{proof}
  Let  for  and
  .  Define .  Then
  
  using convexity and minimizing  at .  Hence
  

  Let .  For any measurement vector , let .  Let .
  Because ,
  
  Let  be the (discrete) random variable denoting the  such that
  .  Then  is drawn from , and  has
  probability distribution .  Hence
  
  because the Gaussian distribution maximizes entropy subject to a
  power constraint.  Using the same technique as the Shannon-Hartley
  theorem,
  
  and hence by Equation~\eqref{eq:Yibound},
  

  All that requires is to show that this is .  Since , we have
  
  Now,  for  and is  for , so by Equation~\eqref{eq:it_i},
  
  Next,  for , so
  
  Plugging into Equations~\eqref{eq:splitting} and~\eqref{eq:Iint_i},
  
  To bound , we consider the partition  and .  Then
  
  But  is increasing on , so
  
  and hence .  Combining with
  Equation~\eqref{eq:nasty_intermediate} gives that
  
  as desired.
\end{proof}

\begin{theorem}
  Any scheme using  rounds with number of measurements  in each round has
  
  for some constant .
\end{theorem}
\begin{proof}
  Let the signal in the absence of noise be , and the signal in the presence of noise be  where  independently.  In round , after observations
   of , let  be the
  distribution on .  That is, 
  is Bob's posterior distribution on  at the beginning of round
  .

  We define
  
  Because the rows of  are deterministic given , Lemma~\ref{lemma:oneround} shows that any single
  measurement  satisfies
  
  for some constant .  Thus by Lemma~\ref{lemma:splitentropy}
  
  There is a Markov chain , so
  
  We define .  Therefore
  
  for some constant .  Then for some constant ,
  
  as desired.
\end{proof}


\begin{corollary}
  Any scheme using  rounds with  measurements has
  
  for some constant .  Thus for sparse recovery, .  Minimizing over , we find that  independent of .
\end{corollary}
\begin{proof}
  The equation follows from the AM-GM inequality.  Furthermore, our
  setup is such that Bob can recover  from  with large
  probability, so ; this was formally
  shown in Lemma 6.3 of~\cite{HIKP12} (modifying Lemma 4.3
  of~\cite{PW11} to adaptive measurements and ).
  The result follows.
\end{proof}


\section{Lower bound for dependence on  and  for }
\label{sec:eps}
In Section
\ref{sec:cc1} we establish a new lower bound on the communication complexity
of a two-party communication problem that we call . 
In Section \ref{sec:cc2} we
then show how to use an adaptive -approximate  sparse
recovery scheme  
to solve the communication problem . 
By the communication lower bound in Section \ref{sec:cc1}, 
we obtain a lower bound on the number of measurements required of . 

\subsection{Direct sum for distributional }\label{sec:cc1}
We assume basic familiarity with communication complexity; see the
textbook of Kushilevitz and Nisan \cite{kn97} for further background.
Our reason for using communication complexity is to prove lower
bounds, and we will do so by using information-theoretic arguments. We
refer the reader to the thesis of Bar-Yossef \cite{BarYossefThesis}
for a comprehensive introduction to information-theoretic arguments
used in communication complexity.

We consider two-party randomized communication complexity. There are two parties,
Alice and Bob, with input vectors  and  respectively, and their goal is
to solve a promise problem . The parties have private randomness. 
The communication cost of a protocol is
its maximum transcript length, over all possible inputs and random coin tosses.
The randomized communication complexity  is 
the minimum communication cost of a randomized protocol  which for every input
 outputs  with probability at least  (over the random
coin tosses of the parties). We also study the distributional
complexity of , in which the parties are deterministic and
the inputs  are drawn from distribution , and a protocol is
correct if it succeeds with probability at least  in outputting ,
where the probability is now taken over . We define 
to be the minimum communication cost of a correct protocol . 

We consider the following promise problem 
, where  is a parameter, 
which was studied in \cite{ss02,BJKS04}. The
inputs are pairs  of -dimensional vectors, with
 for all , with the promise that 
 is one of the following types of instance:
\begin{itemize}
\item NO instance: for all , , or
\item YES instance: there is a unique  for which
  , and for all , .
\end{itemize}
The goal of a protocol is to decide which of the two cases (NO or YES) the input
is in. 


Consider the distribution : for each , choose a random pair . If , then  and 
 is uniformly distributed in ; if , then 
and  is uniformly distributed on . 
Let  and . 
Next choose a random coordinate . 
For coordinate , replace  with a uniform element of 
.  Let  and .

Using similar arguments to those in \cite{BJKS04}, 
we can show that there are positive, sufficiently small 
constants  and  so that
for any randomized protocol  which succeeds with probability at
least  on distribution ,

where, with some abuse of notation, 
 is also used to denote the transcript of the corresponding randomized protocol,
and here the input  is drawn
from  conditioned on  being a NO instance. 
Here,  is randomized, and succeeds with probability at least ,
where the probability is over the joint space of the random coins of
 and the input distribution.

Our starting point for proving (\ref{eqn:icost}) is 
Jayram's lower bound for the conditional mutual information when the inputs
are drawn from a related distribution 
(reference [70] on p.182 of \cite{BarYossefThesis}),
but we require several non-trivial modifications to his argument 
in order to apply it to bound the conditional mutual information for our input distribution, 
which is  conditioned
on  being a NO instance. Essentially, we are able to show that the variation
distance between our distribution and his distribution is small, and use this to
bound the difference in the conditional mutual information between the two distributions.
The proof is rather technical, and we postpone it to Appendix \ref{app:dist}. 

We make a few simple refinements to (\ref{eqn:icost}).  Define the random variable
 which is  if  is a YES instance, and  if  is a
NO instance. Then by definition of the mutual information, 
if  is drawn from
 without conditioning on  being a NO instance, then we have

Observe that

where we assume that . 
\iffalse Next, we look at -way protocols . In this case , where  is Alice's single message to Bob, and  is
Bob's output bit. By the chain rule,

since  is just a single bit. Again, by the chain rule,

since  since the
protocol is -way. It follows by (\ref{eqn:mutInf}) that assuming
,

\fi
Define the constant . 
We now define a problem which involves solving  copies of . 
\begin{definition}[ Problem]\label{def:multi} There are  pairs of
  inputs  such that each pair
   is a legal instance of the  problem. Alice
  is given .  Bob is given .
  The goal is to output a vector , so that for at
  least a  fraction of the entries , 
  .
\end{definition}
\begin{remark}
Notice that Definition \ref{def:multi} is defining a promise problem. We will 
study the distributional complexity of this problem under the distribution
, which is a product distribution on 
the  instances . 
\end{remark}
\begin{theorem}\label{thm:theMain}

\end{theorem}
\begin{proof}
  Let  be any deterministic
  protocol for  which succeeds with probability at
  least  in solving  when the inputs are
  drawn from , where the probability is taken over the input
  distribution. We show that  has communication cost
  .

  Let  and 
  be the random variables associated with , i.e., 
   and  correspond to the random variables
   associated with the -th independent instance drawn
  according to , defined above. We let
  , , and
   equal  without . Similarly we define these
  vectors for  and .
 
By the chain rule for mutual information, 

is equal to 
Let  be the output of , and  be its -th coordinate. 
For a value , we say that  is {\it good} if

Since  succeeds with probability at least  in
outputting a vector with at least a  fraction of
correct entries, the expected probability of success over a random 
is at least , and so by a
Markov argument, there are  good indices .

Fix a value of  that is good, and consider .  By expanding the conditioning,  is equal to

For each , define a randomized protocol
 for  under distribution .
Suppose that Alice is given  and Bob is given , where . Alice sets , while Bob sets . Alice and Bob
use  and  to set their remaining inputs as follows.
Alice sets  and Bob sets .  Alice and
Bob can randomly set their remaining inputs without any communication, 
since for , conditioned on , and ,
Alice and Bob's inputs are independent.  
Alice and Bob run  on
inputs , and define  
We say a tuple  is {\it good} if

By a Markov argument, and using that  is good, we have 

Plugging into (\ref{eqn:condition}), 
 is at least a constant times
 For any  that
is good,  with
probability at least , over the joint distribution of the
randomness of  and . 
By (\ref{eqn:mutInf}),

Since there are  good indices , we have 

Since the distributional complexity  is at least the
minimum of  over 
deterministic protocols  which succeed with probability at least
 on input distribution , it follows that
.
\end{proof}

\subsection{The overall lower bound}\label{sec:cc2}
We use the theorem in the previous subsection with an extension of the method of section 6.3
of \cite{PW11}. 

  Let  be a distribution with  for all  and .  Here  is a parameter. 
Given an adaptive compressed 
  sensing scheme , we define a
  -approximate  sparse recovery
  {\it multiround bit scheme} on  as follows. 

Let  be the -th (adaptively chosen) measurement matrix of the compressed sensing scheme. 
We may assume that the union of rows in matrices  generated by 
is an orthonormal system, since the rows can be orthogonalized in a post-processing step. 
We can assume that . 

  Choose a random  
from distribution ,
  where  is a parameter.  
  We require that the compressed sensing scheme outputs a valid result of 
  -approximate recovery on  with probability at least , 
  over the choice of  and its random coins. By Yao's minimax principle, we can fix the 
  randomness of the compressed sensing scheme and
  assume that the scheme is deterministic. 

Let  be the matrix
   with entries rounded to  bits for a parameter . 
  We compute . Then, we compute 
  . From this, we compute , 
   using the algorithm specified by  as if  were equal to 
    for some . For this, we use the following lemma, which is Lemma~5.1 of~\cite{DIPW}.
\begin{lemma}\label{lem:roundingFix}
Consider any  matrix  with orthonormal rows. Let  be the result
of rounding  to  bits per entry. Then for any  there
exists an  with  and .
\end{lemma}

  In general for , given  we 
  compute , and round to 
  bits per entry to get .
  The output of the multiround bit scheme is the same as that of 
  the compressed sensing scheme. If the compressed sensing scheme uses  rounds, then the multiround
  bit scheme uses  rounds. Let  denote the total number of bits in the concatenation of discrete
  vectors . 

We give a generalization of Lemma 5.2 of \cite{PW11} which relates bit
schemes to sparse recovery schemes. Here we need to generalize the relation
from non-adaptive schemes to adaptive schemes, using Gaussian noise instead of uniform noise,
and arguing about multiple rounds of the algorithm. 

\begin{lemma}\label{lem:bits}
  For , a lower bound of  bits for a multiround bit scheme 
  with error probability at most  implies a lower bound of
   measurements for 
  -approximate sparse recovery schemes with failure probability
  at most .
\end{lemma}
\begin{proof}
  Let  be a -approximate adaptive compressed
  sensing scheme with failure probability . We will show that
  the associated multiround bit scheme has failure probability .


By Lemma \ref{lem:roundingFix}, for any vector  
we have  for a vector  with
  , so . 
Notice that .
We use the following quick suboptimal upper bound on the 
statistical distance between two univariate normal
distributions, which suffices for our purposes. 
\begin{fact}(see section 3 of \cite{p05})\label{fact:gaussian}
The variation distance between  and  is 
where . 
\end{fact}
It follows by Fact \ref{fact:gaussian} and independence across coordinates, that the variation
distance between  and 
 is the same as that between
 and , which can be upper-bounded
as

It follows that for , the variation distance is at most . 

Therefore, if  is the algorithm which
  takes  and produces , then 
   with probability
  at least . This follows since 
  and  and  have variation distance at most . 

In the second round,  is obtained,
  and importantly we have for the algorithm  in the second round,
   with probability
  at least . This follows
  since  is a deterministic function of , and  and  are independent
  since  and  are orthonormal while  is a vector of i.i.d. Gaussians (here we
  use the rotational invariance / symmetry of Gaussian space). 
  It follows by induction that with probability at least , the output
  of the multiround bit scheme agrees with that of  on input .

  Hence, if  is the number of measurements in round , and , then we have a multiround bit scheme using a
  total of  bits and with failure
  probability .
\end{proof}
The rest of the proof is similar to the proof of the non-adaptive lower bound for 
sparse recovery given in \cite{PW11}. We sketch the proof, referring the reader to \cite{PW11}
for some of the details.
Fix parameters , , , and . Given an instance  of  we define the input signal
 to a sparse recovery problem. We allocate a set  of 
disjoint coordinates in a universe of size  for each pair , and on these coordinates place the vector .  The
locations turn out to be essential for the proof of Lemma \ref{lem:noise} below, 
and are placed uniformly at random among the  total coordinates (subject to the
constraint that the  are disjoint). 
Let  be the induced distribution on .

Fix a -approximate -sparse recovery multiround bit scheme  that
uses  bits and succeeds with
probability at least  over .  
Let  be the set of top
 coordinates in . As shown in equation (14) of \cite{PW11},
 has the guarantee that if , then

(the  instead of the  factor is to handle the rounding of entries of
the  and the noise vector ). 
Next is our generalization of Lemma 6.8 of \cite{PW11}.


\begin{lemma}\label{lem:oneSide}
For  sufficiently large, suppose that 
 
Then  requires . 
\end{lemma}
\begin{proof}
We show how to use  to solve instances of 
  with probability at least , where the probability is
  over input instances to  distributed according to
  , inducing the distribution  on . The lower bound will
  follow by Theorem \ref{thm:theMain}. Let  be the output of 
  . 

  Given , Alice places  on the appropriate
  coordinates in the set  used in defining , obtaining a
  vector . 
  Given , Bob places the  on the appropriate
  coordinates in . He thus creates a vector  for which
  . In round , Alice transmits 
  to Bob, who computes  and transmits it back 
  to Alice. Alice can then compute  for a random
  . 
  We can assume all
  coordinates of the output vector  are in the real interval , since rounding the
  coordinates to this interval can only decrease the error.

  To continue the analysis, we use a proof technique of \cite{PW11} (see the proof
of Lemma 6.8 of \cite{PW11} for a comparison). 
For each  we say that  is {\it bad} if either
\begin{itemize}
\item there is no coordinate  in  for which 
yet  is a YES instance of , or
\item there is a coordinate  in  for which 
yet either  is a NO instance of  or  is
not the unique  for which . 
\end{itemize}
  The proof of Lemma 6.8 of \cite{PW11} shows that the fraction  of bad  can be made
  an arbitrarily small constant by appropriately choosing an appropriate .
  Here we choose . We also condition on  for a sufficiently
  large constant , which occurs
  with probability at least . Hence, with probability at least ,
  we have a  fraction of indices  for which the following algorithm
  correctly outputs : if there is a 
  for which , output YES, otherwise output NO. 
  It follows by Theorem \ref{thm:theMain} that 
  requires , independent of the number
  of rounds.
\end{proof}

The next lemma is the same as Lemma 6.9 of \cite{PW11}, replacing
 in the lemma statement there with the constant  and
observing that the lemma holds for compressed sensing schemes with an
arbitrary number of rounds. \begin{lemma}\label{lem:noise}
  Suppose   Then
   requires .
\end{lemma}
\begin{proof}
  As argued in Lemma 6.9 of \cite{PW11}, we have , which implies that , independent of the number  of rounds used by
  , since the only information about the signal is in the concatenation
  of . 
\end{proof}
Finally, we combine our Lemma \ref{lem:oneSide} and Lemma
\ref{lem:noise} to prove the analogue of Theorem 6.10 of \cite{PW11},
which completes this section. \begin{theorem}\label{thm:conclusion}
  Any -approximate  recovery scheme with
  success probability at least  must make
   measurements.
\end{theorem}
\begin{proof}
  We will lower bound the number of bits used by any  multiround bit scheme
  . If  succeeds with probability at least ,
  then in order to satisfy (\ref{eqn:main}), we must either have
   or
  . Since  succeeds with probability at least
  , it must either satisfy the hypothesis of Lemma
  \ref{lem:oneSide} or Lemma \ref{lem:noise}. But by these two lemmas,
  it follows that . Therefore by Lemma
  \ref{lem:bits}, any -approximate  sparse
  recovery algorithm succeeding with probability at least  requires 
  measurements.
\end{proof}

\section{Acknowledgements}

Some of this work was performed while E. Price was an intern at IBM
research, and the rest was performed while he was supported by an NSF
Graduate Research Fellowship.

\newpage
\bibliographystyle{alpha}
\bibliography{eps-sparse}



\appendix

\section{Relationship between Post-Measurement and Pre-Measurement
  noise}\label{app:noisekind}


In the setting of~\cite{ACD11}, the goal is to recover a -sparse
 from observations of the form , where  has unit norm rows
and  is i.i.d. Gaussian with variance .  By
ignoring the (irrelevant) component of  orthogonal to , this is
equivalent to recovering  from observations of the form .
By contrast, our goal is to recover  from observations of the
form , and for general  rather than only for Gaussian .

By arguments in~\cite{PW11,HIKP12}, for Gaussian  the difference
between recovering  and recovering  is minor, so any lower
bound of  in the~\cite{ACD11} setting implies a lower bound of
 in our setting.  The converse is only true for
proofs that use Gaussian , but our proof fits this category.

\section{Information Chain Rule with Linear Observations}

\begin{lemma}\label{lemma:splitentropy}
  Suppose  for  and the  are
  independent of each other and the .  Then
  
\end{lemma}
\begin{proof}
  Note that .  Thus
  
\end{proof}

\section{Switching Distributions from Jayram's Distributional Bound}\label{app:dist}
We first sketch a proof of Jayram's lower bound on the distributional
complexity of  \cite{J02}, then change it to a different distribution that
we need for our sparse recovery lower bounds in Subsection \ref{sec:change}. Let 
. Define distribution
 as follows: for each , choose a random pair . If , then  and 
 is uniformly distributed in ; if , then 
and  is uniformly distributed on . 
Let , ,  and . 

The other distribution we define is , 
which is the same as distribution  
in Section \ref{sec:eps} (we include  and  in the
notation here for clarity). This is defined by
first drawing  and  according to distribution . Then, 
we pick a random coordinate  and replace
 with a uniformly random element in the set . 

Let  be a deterministic protocol that errs with probability at most 
on input distribution . 

By the chain rule for mutual information, when  and  are
distributed according to , 

which is equal to 

Say that an index  is {\it good} if conditioned on ,
 succeeds on  with probability at least .
By a Markov argument, at least  of the indices  are good. 
Fix a good index . 

We say that the tuple  is {\it good} if conditioned on ,
, , , and ,  succeeds on 
with probability at least . By a Markov bound, with probability
at least ,  is good. Fix a good .

We can define a single-coordinate protocol  as follows. 
The parties use  and  to fill in their input
vectors  and  for coordinates . They also use 
, , and private randomness to fill in their inputs without any
communication on the remaining coordinates . They place their
single-coordinate input  on their -th coordinate. The parties
then output whatever  outputs. 

It follows that  is a 
single-coordinate protocol  which distinguishes
 from  under the uniform distribution 
with probability at least . 
For the single-coordinate problem, we need to bound
 when  is uniformly random from 
the set  if , and  is uniformly
random from the set  if . 
By the same argument as in the proof of Lemma 8.2 of 
\cite{BJKS04}, if  denotes the distribution on transcripts
induced by inputs  and  and private coins, then we have

where 

is the Hellinger distance between distributions  and  on support .
For any two distributions  and , if we define

to be the variation distance between them, then  (see Proposition 2.38 of \cite{BarYossefThesis}).
 
Finally, since  
succeeds with probability at least  on the uniform distribution
on input pair in , we have

Hence,

for each of the  good .  Thus  when inputs  and  are distributed according to
, and  succeeds with probability at least 
on  and  distributed according to .

\subsection{Changing the distribution}\label{sec:change} Consider the
distribution  We show  when  and 
are distributed according to  rather than according to
.

For  and  distributed according to , by the chain rule we again have
that  is equal to 

Again, say that an index  is good if conditioned on ,
 succeeds on  with probability at least .
By a Markov argument, at least  of the indices  are good. 
Fix a good index . 

Again, we say that the tuple  is good if conditioned on ,
, ,  and ,  succeeds on 
with probability at least . By a Markov bound, with probability
at least ,  is good. Fix a good .

As before, we can define a single-coordinate protocol . 
The parties use  and  to fill in their input
vectors  and  for coordinates . They can also use 
, , and private randomness to fill in their inputs without any
communication on the remaining coordinates . They place their
single-coordinate input , uniformly drawn from ,
on their -th coordinate. The parties output whatever  outputs. Let 
denote  for notational convenience.

The first issue is that unlike before  is not guaranteed to have success probability
at least  since  is not being run on input distribution 
in this reduction. The second issue is in bounding  since 
 is now drawn from the marginal distribution of  on coordinate . 

\iffalse
Consider a random tuple  formed by choosing  from the marginal
distribution of  and  distributed according to , and independently choosing
 uniformly at random. 
Say that  is {\it good} if  succeeds with probability at
least , conditioned on , and 
 being uniform on . 
For a good , 
it follows that
by placing  on the -th coordinate and outputting whatever  outputs, the
resulting single-coordinate protocol  for distinguishing
 from  under the uniform distribution 
succeeds with probability . 
Hence, for a good tuple , 
 represents the information cost of a protocol 
that succeeds with probability  
on the single-coordinate problem of distinguishing
the inputs  and . By (\ref{eqn:final}), we then have 
. Hence, if we can show that 
, then it
will follow that ,
which will complete the proof. 
It remains to show that . 
\fi

Notice that  with probability , which we condition on. This immediately resolves
the second issue since now the marginal distribution on  is the same under 
as it was under ; namely
it is the following distribution:  is uniformly random from 
the set  if , and  is uniformly
random from the set  if . 

We now address the first issue.  After conditioning on , we
have that  is drawn from .  If
instead  were drawn from , then after
placing  the input to  would be drawn from 
conditioned on a good tuple.  Hence in that case,  would succeed
with probability .  Thus for our actual distribution on
, after conditioning on , the success
probability of  is at least


Let  be the random variable which counts the number of 
coordinates  for which  when  and  are 
drawn from . Let  be a random
variable which counts the number of coordinates  for which
 when  and  are drawn from . 
Observe that  in  only if  and , which happens
with probability . Hence,
 is distributed as Binomial, while
 is distributed as Binomial. We use
 to denote the distribution of  and  to denote
the distribution of . Also, let  denote the 
Binomial distribution. 
Conditioned on , we have that  and  are
equal as distributions, and so 

We use the following fact:
\begin{fact}\label{fact:vd} (see, e.g., Fact 2.4 of \cite{gmrz11}). 
Any binomial distribution  with variance equal to  
satisfies . 
\end{fact}
By definition, 
Since the variance of the Binomial distribution is 
 applying
Fact \ref{fact:vd} we have

It follows that the success probability of  is at least 

Let  be an indicator random variable for the event that . Then
. Hence,

where we assume that 

Hence,  when inputs
 and  are distributed according to , and  succeeds
with probability at least  on  and  distributed
according to . 



\end{document}
