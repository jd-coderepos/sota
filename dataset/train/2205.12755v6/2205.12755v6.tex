\documentclass{article} \usepackage{iclr2023_conference,times}



\usepackage{hyperref}
\usepackage{url}

\hypersetup{
    colorlinks=true,
    citecolor=black,
    linkcolor=black,
    urlcolor=blue,
    }

\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         

\usepackage[pdftex]{graphicx}
\usepackage{caption}
\usepackage{subcaption}

\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{argmax} 


\usepackage{xspace}
\newcommand{\muNet}{2Net\xspace}
\newcommand{\plmi}{{\tiny}\scriptsize}
    
\mathchardef\mhyphen="2D

\title{An Evolutionary Approach to Dynamic \\
Introduction of Tasks in Large-scale \\
Multitask Learning Systems}





\author{Andrea Gesmundo \\
  Google Research \\
  \texttt{agesmundo@google.com} \\
  \And
  Jeff Dean \\
  Google Research \\
  \texttt{jeff@google.com} \\
}

\begin{document}

\maketitle

\begin{abstract}



Multitask learning assumes that models capable of learning from multiple tasks can achieve better quality and efficiency via knowledge transfer, a key feature of human learning.
Though, state of the art ML models rely on high customization for each task and leverage size and data scale rather than scaling the number of tasks. Also, continual learning, that adds the temporal aspect to multitask, is often focused to the study of common pitfalls such as catastrophic forgetting instead of being studied at a large scale as a critical component to build the next generation artificial intelligence.
We propose an evolutionary method capable of generating large scale multitask models that support the dynamic addition of new tasks. The generated multitask models are sparsely activated and integrates a task-based routing that guarantees bounded compute cost and fewer added parameters per task as the model expands.
The proposed method relies on a knowledge compartmentalization technique to achieve immunity against catastrophic forgetting and other common pitfalls such as gradient interference and negative transfer.
We demonstrate empirically that the proposed method can jointly solve and achieve competitive results on 69 public image classification tasks,
for example
improving the state of the art on a competitive benchmark such as cifar10 by achieving a 15\% relative error reduction compared to the best model trained on public data.  


\end{abstract}


\section{Introduction}

The success of machine learning continues to grow as it finds new applications in areas as diverse as
language generation \citep{Brown2020LanguageMA},
visual art generation \citep{Ramesh2021ZeroShotTG},
chip design \citep{Mirhoseini2020ChipPW},
protein folding \citep{Senior2020ImprovedPS}
and competitive sports \citep{Silver2016MasteringTG, Vinyals2019GrandmasterLI}.
The vast majority of machine learning models are designed and trained for a single task and specific data modality, and are often trained by starting with randomly initialized parameters, or with limited knowledge transfer from a pre-trained model.
While this paradigm has shown great success, it uses a large amount of computational resources, and does not leverage knowledge transfer
from many related tasks 
in order to achieve higher performance and efficiency.

The work presented in this paper is based on the intuition that significant advances can be 
enabled by
dynamic, continual learning 
approaches capable of achieving knowledge transfer across a very large number of tasks.
The method described in this paper can dynamically incorporate new tasks into a large running system,
can leverage pieces of a sparse multitask ML model to achieve improved quality for new tasks,
and can automatically share pieces of the model among related tasks.
This method can enhance quality on each task, and also improve efficiency in terms of convergence time, amount of training examples, energy consumption and human engineering effort. 

The ML problem framing proposed by this paper can be interpreted as a generalization and synthesis of the standard multitask and continual learning formalization,
since an arbitrarily large set of tasks can be solved jointly.
But also, over time, the set of tasks can be extended with a continuous stream of new tasks.
Furthermore, it lifts the distinction between a pretraining task and a downstream task.
As new tasks are incorporated, the system searches for how to combine the knowledge and representations already present in the system with new model capacity in order to achieve high quality for each new task.
Knowledge acquired and representations learned while solving a new task are available for use by any future task or continued learning for existing tasks.

We refer to the proposed method as “mutant multitask network” or \muNet.
This method generates a large scale multitask network that jointly solves multiple tasks to achieve increased quality and efficiency for each.
It can continuously expand the model by allowing the dynamic addition of new tasks.
The more accumulated knowledge that is embedded into the system via learning on previous tasks, the higher quality the solutions are for subsequent tasks.  Furthermore, new tasks can be solved with increasing efficiency in terms of reducing the newly-added parameters per task.
The generated multitask model is sparsely activated as it integrates a task-based routing mechanism that guarantees bounded compute cost per task as the model expands.
The knowledge learned from each task is compartmentalized in components that can be reused by multiple tasks.
As demonstrated through experiments, this compartmentalization technique avoids the common problems of multitask and continual learning models, such as catastrophic forgetting, gradient interference and negative transfer.
The exploration of the space of task routes and identification of the subset of prior knowledge most relevant for each task is guided by an evolutionary algorithm designed to dynamically adjust the exploration/exploitation balance without need of manual tuning of meta-parameters.
The same evolutionary logic is employed to dynamically tune the hyperparameters multitask model components.





\section{Related work}
The main novelty of the presented work is to propose and demonstrate a method that jointly provides all of the following properties:
1)~ability to continually learn from an unbounded stream of tasks,
2)~automate the selection and reuse of prior knowledge and representations learned for previous tasks in the solving of new tasks,
3)~search the space of possible model architectures allowing the system to dynamically extend its capacity and structure without requiring random initialization,
4)~automatically tune the hyperparameters of both the generated models and the evolutionary method, including the ability to learn schedules for each hyperparameter, rather than just constant values,
5)~ability to optimize for any reward function, also including non-differentiable factors,
6) immunity from catastrophic forgetting, negative transfer and gradient interference,
7) ability to extend any pre-existing
pre-trained model, including extending 
its architecture
and adapting the domain on which such model have been trained to other domains automatically,
8) introduction of a flexible access control list mechanism that allows  expression of a variety of privacy policies,
including allowing the use or influence of task-specific data to be constrained to just a single task or to a subset of tasks for which data or higher-level representation use should be permitted. Different lines of research have focused on distinct subsets of the many topics addressed by the proposed method.
In this section we highlight a few cornerstone publications.
Refer to Appendix~\ref{sec:ex-rel} for an extended survey.

Different methods have been proposed to achieve \textbf{dynamic architecture extensions} \citep{Chen2016Net2NetAL,Cai2018EfficientAS},
some also focusing on an unbounded stream of tasks \citep{Yoon2018LifelongLW}, or achieving immunity from catastrophic forgetting  \citep{Rusu2016ProgressiveNN,Li2018LearningWF,Rosenfeld2020IncrementalLT}.
Unlike our work, these techniques rely on static heuristics and patterns to define the the structural extensions, rather than a more open-ended learned search process.
\textbf{Neural architecture search} (NAS) \citep{Zoph2017NeuralAS} methods aim to modularize the architectural components in search spaces whose exploration can be automated with reinforcement learning or evolutionary approaches \citep{Real2019RegularizedEF,Maziarz2018EvolutionaryNeuralHA}.
More efficient (but structurally constrained) parameter sharing NAS techniques \citep{Pham2018EfficientNA,Liu2019DARTSDA} create a connection with routing methods \citep{Fernando2017PathNetEC} and \textbf{sparse activation} techniques, 
that enable the decoupling of model size growth from compute cost growth
\citep{Shazeer2017OutrageouslyLN,Du2021GLaMES}.
Evolutionary methods have also been applied with success for \textbf{hyperparameter tuning} \citep{Jaderberg2017PopulationBT}.

Cross-task \textbf{knowledge transfer}
has gained popularity, especially through transfer learning from a model
pre-trained on a large amount of data for one or a few general tasks,
and then fine-tuned on a small amount of data for a related downstream task.
This approach has been shown to be very effective in a wide variety of problems
and modalities \citep{Devlin2019BERTPO,Dosovitskiy2021AnII}.
Large scale models have recently achieved novel transfer capabilities such as few/zero shot learning \citep{Brown2020LanguageMA}.
More complex forms of knowledge transfer such as \textbf{multitask training} or \textbf{continual learning} often lead to interesting problems such as catastrophic forgetting \citep{McCloskey1989CatastrophicII,French1999CatastrophicFI}, negative transfer \citep{Rosenstein2005ToTO,Wang2019CharacterizingAA} or gradient interference \citep{Chen2018GradNormGN,Yu2020GradientSF}.
Research on these topics mostly focuses on approaches
such as weighted combination methods \citep{Liu2019LossBalancedTW,Sun2020ERNIE2A} or gradient transformations \citep{Sener2018MultiTaskLA,Kendall2018MultitaskLU},
also methods automating knowledge selection at a layer level was proposed \citep{Sun2020AdaShareLW}.
The method proposed in this paper can be considered large-scale and state-of-the-art focused progression from \citet{Gesmundo2022muNet1}.

\begin{figure}[t]
\vspace{-10pt}
\centering
\begin{minipage}{.67\textwidth}
  \centering
  \includegraphics[width=1.0\linewidth]{mutations.pdf}
\end{minipage}\begin{minipage}{.33\textwidth}
  \centering
  \includegraphics[width=0.99\linewidth]{graph-prelim.png}
\end{minipage}
\vspace{-6pt}
\caption{
Graphical representation of the two mutation types used
by the proposed method:
layer cloning mutation (left) and hyperparameter change (center).
The graph on the right represents the model generated by the 
\href{https://youtu.be/Pcin4hPGaOk}{preliminary experiment}
described in Section~\ref{section:preliminary}.
The bottom nodes display the task names, the top nodes display the
validation accuracy, and internal nodes are represented with the color of the task that has last updated the parameters of the corresponding layer.
}
\label{fig:mutations}
\vspace{-10pt}
\end{figure}

\section{Evolutionary Method}
\label{section:method}
This section defines the proposed 
method capable of generating a dynamic multitask ML system.
The multitask system is initialized with one \emph{root model}.
This model can be either pretrained or randomly initialized.
During the evolutionary process, the proposed method searches for the best model for a single task at a time,
referred to as the \emph{active task}.
During the active phase of a task, a population of models trained on the active task is evolved: the \emph{active population}.
The first time a task becomes active, its active population is empty.
For subsequent iterations, the active population is initialized with all the models trained on the active task that have been retained from previous iterations.
The active population is iteratively extended by:
1) sampling a \emph{parent model} (Section~\ref{subsection:parent-sampling}),
2) applying to the parent model a sampled set of mutations (Section~\ref{subsection:mutations}) to produce a \emph{child model},
3) performing cycles of training and validation in order to train and score the child model.
Each trained model is assigned a \emph{score} (Section~\ref{subsection:scoring}).
Early population pruning is performed by discarding the models that did not achieve a better score then their parent.
An active phase is composed of multiple \emph{generations} in which multiple batches of child models are sampled and trained in parallel.
At the end of a task active phase, only its best scoring model is retained as part of the multitask system.
A sequence of task iterations may be employed to generate solutions for a set or a stream of different tasks.
A task can become active multiple times. Details of the method are reported below (and in Algorithm~\ref{algo}).

\subsection{Parent model sampling}
\label{subsection:parent-sampling}
The first attempt to sample a parent model for the active task is done over the active population of models for that task.
The models in the active population are visited in decreasing order of score, starting with the highest scoring one.
Each model, , can be accepted as parent with probability:
.
Where  denotes the number of times the candidate model, , has been previously selected as parent to generate a child models for task .
If the current candidate parent is not selected, then iteratively the model with the next best score is considered to be selected as parent with probability .
If a full iteration on the active population is completed without a successful parent model selection, then the same method is applied to the randomly sorted list of all remaining models: all the models currently part of the multitask system that were trained on a task different from the current active task, .
This fallback list is randomly sorted since these models have not been scored for .
As a final fallback a parent is uniformly sampled among all the models currently in the system.
This method prioritizes the exploitation of high scoring models that had few attempts at generating an improved model for the active task.
But also, in combination with early pruning, it automatically transitions toward a more exploratory behavior in case the higher scoring models are unable to generate an improvement.

\subsection{Mutations}
\label{subsection:mutations}
In this work we consider Deep Neural Networks (DNN) models. 
DNN are commonly defined by their architecture and hyperparameters.
Architectures are composed of a sequence of neural network layers, each mapping an input vector into an output vector of variable dimensions.
Hyperparameters specify the configuration details such as the optimizer or data preprocessing configurations.
The presented method allows for two types of mutations (Figure~\ref{fig:mutations}):

\textbf{Hyperparameter mutations} can be applied to modify the configuration inherited from the parent.
Each hyperparameter is associated with a sorted list of valid values (Table~\ref{table:hps}).
If a hyperparameter is selected for mutation, then its new value is selected at random among the two neighbouring values in the sorted list (Figure~\ref{fig:mutations}).
This constraints hyperparameter mutations to only incremental changes.
Notice that, every hyperparameter of a child model is set to a single value. However, considering that a child model continues its ancestors training with mutated hyperparameters, then the method can be regarded as capable of defining a piece-wise constant schedule over time for each hyperparameter.



\textbf{Layer cloning mutations} create a copy of any parent model layer that can be trained by the child model.
If a layer of the parent model is not selected for cloning,
then it is shared with the child model in a frozen state to guarantee immutability of pre-existing models. Child models can train only the cloned copies of the parent layers.
The cloned layers are trained with a possibly modified version of the parent optimizer.
The configuration of the child optimizer is defined by the mutated hyperparameters.
If such optimizer is of a type that stores a state 
(i.e. momentum),
then the state is also cloned from the state saved by the ancestor that has last trained the cloned layer.
Notice that,
a trainable layer may be followed by frozen layers.
In this case the gradients for the trainable layer are propagated through the frozen layers and applied only to the parameters of the trainable layers while frozen parameters are left unchanged.
The head layer is always cloned since it always needs to be trainable.
If a child model is trained on a task different from the parent's task, then a new randomly initialized head layer is created with output shape matching the number of classes of the new task.

Each possible layer cloning or hyperparameter mutation is independently sampled to be applied with probability .
 is itself a hyperparameter that is mutated by the evolutionary process.
Thus demonstrating that automatic tuning is not only applied to selecting the hyperparameters of the generated models, but can also be applied to self-tune the configuration of the evolutionary algorithm.

\subsection{Training and scoring}
\label{subsection:scoring}
A newly sampled child model is trained on the active task for a given number of epochs.
The model is evaluated on the validation set after each epoch.
At each intermediate evaluation, the child model is assigned a score that the evolutionary algorithm aims to maximize.
The score can be defined to optimize a mixture of factors such as validation quality, inference latency, training compute or model size, depending on the applications requirements.
The presented experiments aim to compare against the state of the art for a large number of tasks without any size or compute constraint.
Therefore, the validation accuracy is used directly as the score without additional factors.
After training, only the parameters and optimizer state of the version of the child model achieving best score are retained.

\subsection{Discussion and properties}
Notice that, none of the defined mutation actions or the evolutionary algorithm allow the creation of child models that can alter the parent model in any way.
Once a model has been trained, the parameters storing its knowledge cannot be modified.
This method guarantees immunity from \textbf{catastrophic forgetting}, since the knowledge of a trained model is always preserved.
It also provides a solution to \textbf{negative transfer}, since it automates the selection of the knowledge that is most relevant for each new task.
Furthermore, it also avoids \textbf{gradient interference}, that can arise when multiple gradients are synchronously applied to the same set of parameters.
Nonetheless, models for new tasks can use knowledge and representations from prior tasks and even extend these to improve or specialize them.

The method compartmentalizes the knowledge of each task in a subset of components, allowing the implementation of different \textbf{dataset privacy control} policies. For example, we can introduce private tasks that can benefit from all 
the public knowledge embedded in the multitask system but are able to withhold the knowledge and representations 
derived from their private dataset from being used by other tasks.
This is achieved by preventing other tasks from using or cloning components trained on private data.
This also allows to remove the knowledge learned from the private dataset at any future date by simply removing its components.
This private/public distinction can be generalized into an access- control-list mechanism.
For example, a set of private tasks can share representations that are withheld from any other task.
Privacy control capabilities are empirically demonstrated in Section~\ref{section:preliminary}.


\subsection{Experimental set up}
\label{section:experiments}

This section details the instantiation of the proposed method employed in the experimental analysis.
The task type for the presented set of experiment is image classification.
This choice allows us to define a large benchmark of publicly available datasets with standardized framing.
It also allows us to build on top of state-of-the-art models whose architecture definition and checkpoints are public:
the Visual Transformer (ViT) is used as root model \citep{Dosovitskiy2021AnII}.

\textbf{Architecture} \ \  
Layer cloning mutations can create a copy of any of ViT's layers: 1) \emph{Patch embedding}: the first layer of the model
maps the input image into a sequence of embedded tokens, each corresponding to a patch of the input image. 2) \emph{Class token}: a classification token is prepended to the sequence. The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks \citep{Devlin2019BERTPO}.
3) \emph{Position embedding}: the sequence representation is then augmented with an embedding that carries each patch positional information.
4) \emph{Transformer layers}: the sequence representation generated by the input layers is iteratively transformed by a stack of transformer layers \citep{Vaswani2017AttentionIA}.
5) \emph{Model head}: a final
fully connected layer mapping the representation
produced by the top-most transformer layer into the logits.

\textbf{Parameters} \ \ 
The parameters of the root model can be either randomly initialized or loaded from a checkpoint.
The preliminary experiment demonstrates the evolution from random initialization (see Section~\ref{section:preliminary}), while the large scale experiment starts from a pretrained large ViT model (see Section~\ref{section:large-exp}).


\textbf{Hyperparameters} \ \
As default hyperparameters we use those resulting from the extensive study conducted by \citet{Steiner2021HowTT}: SGD momentum optimizer, cosine decay schedule, no weight decay, gradient clipping at global norm 1 and 386386 image resolution.
The evolutionary method can change the hyperparameters of optimizer,
image preprocessing and architecture (see Table~\ref{table:hps}).

\begin{table}[t]
\vspace{-10pt}
  \caption{
Hyperparameters valid values.
Bold vales are defaults.
This search space consists of a parametrization of the configuration for the published ViT model definition library.
  }
\vspace{-6pt}
  \label{table:hps}
  \centering
  \begin{tabular}{l}
  \toprule
 [0.10, 0.12, 0.14, 0.16, 0.18, \textbf{0.20}, 0.22, 0.24, 0.26, 0.28, 0.30] \\
Learning rate  [0.0001, 0.0002, 0.0005, 0.001, 0.002, 0.005, \textbf{0.01}, 0.02, 0.05, 0.1, 0.2, 0.5] \\
Cosine learning rate schedule warm up ratio  [0.01, 0.02, 0.05, \textbf{0.1}, 0.2, 0.3, 0.4] \\
Momentum  [0.5, 0.6, 0.7, 0.8, 0.85, \textbf{0.9}, 0.95, 0.98, 0.99] \\
Nesterov update  [\textbf{False}, True] \\
Crop input image  [False, \textbf{True}] \\
Cropped area range min  [0.01, 0.02, \textbf{0.05}, 0.1, 0.2, 0.5, 1.0] \\
Cropped aspect ratio range min  [0.25, 0.5, \textbf{0.75}, 1.0] \\
Flip left/right  [False, \textbf{True}] \\
Brightness delta  [\textbf{0.0}, 0.01, 0.02, 0.05, 0.1, 0.2] \\
Contrast delta  [\textbf{0.0}, 0.01, 0.02, 0.05, 0.1, 0.2] \\
Saturation delta  [\textbf{0.0}, 0.01, 0.02, 0.05, 0.1, 0.2] \\
Hue delta  [\textbf{0.0}, 0.01, 0.02, 0.05, 0.1, 0.2] \\
\bottomrule
  \end{tabular}
\vspace{-10pt}
\end{table}

\section{Preliminary experiment}
\label{section:preliminary}

This section describes a small scale preliminary experiment that introduces details of the method application and illustrates the privacy control and initialization capabilities. 
This experiment demonstrates the ability to generate a multitask model from random initialization and a minimal architecture rather than evolving a pretrained state-of-the-art model.
Therefore, a randomly initialized ViT Ti/16 architecture \citep{Steiner2021HowTT} 
stripped of
transformer layers is used as root model.
To allow the method to build a capable architecture,
we add an extra mutation action that can insert a new randomly initialized transformer layer just before the head layer.

Furthermore, the dataset privacy control technique (see Section~\ref{section:method}) is demonstrated by adding a private task.
Three numerals recognition tasks are used as benchmark:
\{\href{https://www.tensorflow.org/datasets/catalog/cmaterdb#cmaterdbbangla_default_config}{bangla}, 
\href{https://www.tensorflow.org/datasets/catalog/cmaterdb#cmaterdbdevanagari}{devanagari},
\href{https://www.tensorflow.org/datasets/catalog/cmaterdb#cmaterdbtelugu}{telugu}\}.
Telugu is introduced as a private task,
so that no other task can access the knowledge introduced into the system by its dataset.
However, its models can leverage the knowledge provided by the public tasks.


This short experiment is configured to perform 2 active task iterations for each task.
During each active task iteration 4 model generations are produced.
In each generation 8 child models are sampled and trained in parallel on each of the 8 TPUv3 cores.
The choice of small datasets, architecture and training budget is intended to facilitate reproducibility and fast experimental iterations.
The experiment can be reproduced by using the published artifacts and completes in less than 5 minutes.

Figure~\ref{fig:mutations}~(right) displays the resulting multitask model solving jointly the 3 tasks.
We observe a high degree of cross-task knowledge and components sharing throughout the \href{https://youtu.be/Pcin4hPGaOk}{evolutionary process}.
Even though the root model has no transformer layers, multiple randomly initialized transformer layers are inserted and trained improving the score of each task.
Note that, at any point during the evolution, the components trained on the private task (red) are only used by the private task. 



\section{Large scale continual learning experiment}
\label{section:large-exp}
This section reports a single \href{https://youtu.be/Hf88Ge0eiQ8}{large scale continual learning experiment} producing a multitask system jointly solving 69 visual tasks.
A pretrained ViT L/16 is used as root model, which has been selected for its
pretraining validation accuracy on the imagenet-21k dataset following \citet{Steiner2021HowTT}.



\subsection{ViT benchmark}
\label{subsection:imagenet}

The first tasks introduced to the system are 3 tasks on which ViT was evaluated in \citet{Dosovitskiy2021AnII}.
This experiment is configured to perform 5 active iterations for each task,
and 4 model generations for each iteration.
During each generation, 8 child models are trained in parallel on each of the 8 TPUv3 cores.
Each model training performs 4 validation cycles.
The number of train samples between validation cycles is set to  to smooth the distribution of compute across datasets of different size.
 is equivalent to 
of an epoch of the imagenet2012 training set.
The same configuration is applied in following experiments.


This configuration results in 8 epochs for imagenet, and 80 epochs for cifar. This is roughly equivalent to the fine-tuning setup of the baseline model we compare against
\citep{Dosovitskiy2021AnII}: 8 epochs for imagenet and 102.4 for cifar.
The proposed method can be considered cheaper since: 1) ViT fine-tuning has been repeated multiple times for the hyperparameters tuning process and
2) setting  results in
cheaper training steps, since parameters updates can be skipped for frozen layers and gradient propagation can be skipped for frozen layers at the base of the model (preliminary experiments have shown a 2.5-4\% training speed-up attributable to this).
In order to provide a fair comparison, as a root model is used the same ViT L/16 architecture, same checkpoint pretrained on the i12k dataset, same 384384 image resolution, 
and optimizer and prepossessing configuration.

\begin{table}[b]
\vspace{-10pt}
  \caption{
Test accuracy achieved by \muNet and by fine-tuning a comparable pretrained ViT model.
  }
\vspace{-6pt}
  \label{table:vit-bench}
  \small
  \centering
  \begin{tabular}{lccc}
    \toprule
Model
& \href{https://www.tensorflow.org/datasets/catalog/imagenet2012}{imagenet2012}
    & \href{https://www.tensorflow.org/datasets/catalog/cifar100}{cifar100}
    & \href{https://www.tensorflow.org/datasets/catalog/cifar10}{cifar10}

\\
    \midrule
    ViT L/16 fine-tuning \citep{Dosovitskiy2021AnII} & 85.30  & 93.25 & 99.15  \\
    \muNet after 5 task iterations & 86.38  & 94.75 & 99.35  \\
    \muNet after 10 task iterations & 86.66  & 94.67 & 99.38  \\
    \muNet cont. after adding VTAB-full tasks  & \textbf{86.74} & 94.67 & 99.41  \\
    \muNet cont. after adding VDD tasks  & \textbf{86.74}  & 94.74 & 99.43  \\
    \muNet cont. after adding all 69 tasks & \textbf{86.74}  & \textbf{94.95} & \textbf{99.49}  \\
    \bottomrule
  \end{tabular}
  \vspace{-10pt}
\end{table}


Table~\ref{table:vit-bench} reports the top 1 test accuracy achieved. \muNet outperforms fine-tuning with comparable training steps per task.
Extending the training with 5 additional tasks iterations leads to moderate gains on imagenet2012 and cifar10.
Notice that, for cifar100 the accuracy decreases.
This can happen since the best models are selected according the  validation accuracy and, as the model gets close to convergence, a small validation accuracy gain may lead to a noisy perturbation of the test accuracy.

To quantify knowledge transfer, we consider the model produced for each task and examine the dataset on which each layer's ancestors were trained.
On average, the layers composing the model generated for the imagenet2012 task have performed only 60.6\% of the training steps on the imagenet2012 dataset,
and have received 31.5\% of the gradient updates from cifar100 and 7.9\% from cifar10.
The layers comprising the cifar100 model have performed 42.3\% of their training on imagenet2012 and 20.6\% on cifar10.
And layers comprising the cifar10 model performed 46.1\% of training on imagenet2012 and 35.9\% on cifar100.
The tasks heterogeneity improves the representations produced by the different layers, and results in generally higher performance, as shown in Table~\ref{table:vit-bench}.

The following sections~\ref{subsection:vtab-full} and \ref{subsection:vdd} describe the extensions of the system performed by introducing additional benchmarks.
After the introduction of each benchmark, we perform an additional iteration on imagenet and cifar tasks to analyze effects of further knowledge enrichment.
As a representative example: the VDD benchmark (Section~\ref{subsection:vdd}) includes a low resolution version of cifar100. The model that will be generated for vdd/cifar100 will be a mutation of the current full resolution cifar100 model.
Afterward, the additional active task iteration on cifar100 will be performed, and the resulting improved cifar100 model will be a mutation of the low resolution vdd/cifar100 model.


After a final iteration, we note that 99.49 is the best cifar10 accuracy reported for a model trained only on public data: to the best of our knowledge, this constitutes a 15\% relative error reduction compared to the 99.40 state of the art achieved by \citet{Touvron2021GoingDW}.
\citet{Dosovitskiy2021AnII} achieves 99.50 with a double size ViT-Huge model trained on proprietary data.
The achieved cifar100 accuracy is currently outperformed only by  \citet{Ridnik2021MLDecoderSA} (95.10) and \citet{Foret2021SharpnessAwareMF} (96.08).
 


\subsection{VTAB-full benchmark}
\label{subsection:vtab-full}
Next, we introduce to the system the 19 VTAB-full tasks \citep{Zhai2019TheVT}, plus 5 additional task variants that are not included in the standard evaluation set
(Table~\ref{table:datasets}).
From this experiment onward, the infrastructure is scaled from 8 to 32 cores, as detailed in Appendix~\ref{section:repro}.
The number of task iterations is reduced from 10 to 2.
These changes lead to a roughly similar exploratory and training budget per task.
However, the increased parallelism results in faster task iterations.


\begin{table}[t]
\vspace{-10pt}
  \caption{
Test accuracy achieved on the VTAB-full benchmark by:
  1) fine-tuning with matching architecture and checkpoint (ViT L/16 i21k) reported by \citet{Steiner2021HowTT},
  2) the Sup-rotation method \citep{Gidaris2018UnsupervisedRL} that achieved the best result in the VTAB-full leaderboard \citep{Zhai2019ALS},
  3-4) \muNet results after 2 task iterations,
  5) and after an additional iteration performed after the VDD benchmark introduction. Underlined models transfer knowledge from VDD tasks.
}
\vspace{-6pt}
  \label{table:vtabfull-bench}
  \centering
  \small
  \setlength\tabcolsep{1.7pt}
  \hspace*{-26.655pt}
  \begin{tabular}{lccccccccccccccccccc}
    \toprule
Model
&
\rotatebox{90}{\href{https://www.tensorflow.org/datasets/catalog/caltech101}{caltech101}}
&

\rotatebox{90}{\href{https://www.tensorflow.org/datasets/catalog/cifar100}{cifar100}}
&

\rotatebox{90}{\href{https://www.tensorflow.org/datasets/catalog/dtd}{dtd}}
&

\rotatebox{90}{\href{https://www.tensorflow.org/datasets/catalog/oxford_flowers102}{flowers102}}
&

\rotatebox{90}{\href{https://www.tensorflow.org/datasets/catalog/oxford_iiit_pet}{pets}}
&

\rotatebox{90}{\href{https://www.tensorflow.org/datasets/catalog/sun397}{sun397}}
&

\rotatebox{90}{\href{https://www.tensorflow.org/datasets/catalog/svhn_cropped}{svhn}}
&

\rotatebox{90}{\href{https://www.tensorflow.org/datasets/catalog/patch_camelyon}{camelyon}}
&

\rotatebox{90}{\href{https://www.tensorflow.org/datasets/catalog/eurosat\#eurosatrgb_default_config}{eurosat}}
&

\rotatebox{90}{\href{https://www.tensorflow.org/datasets/catalog/resisc45}{resisc45}}
&

\rotatebox{90}{\href{https://www.tensorflow.org/datasets/catalog/diabetic_retinopathy_detection/\#diabetic_retinopathy_detectionbtgraham-300}{retinopathy}}
&



\rotatebox{90}{\href{https://www.tensorflow.org/datasets/catalog/clevr}{clevr-count}}
&

\rotatebox{90}{\href{https://www.tensorflow.org/datasets/catalog/clevr}{clevr-dist}}
&

\rotatebox{90}{\href{https://www.tensorflow.org/datasets/catalog/dmlab}{dmlab}}
&

\rotatebox{90}{\href{https://www.tensorflow.org/datasets/catalog/dsprites}{dspr-loc}}
&

\rotatebox{90}{\href{https://www.tensorflow.org/datasets/catalog/dsprites}{dspr-orient}}
&



\rotatebox{90}{\href{https://www.tensorflow.org/datasets/catalog/kitti}{kitti-dist}}
&



\rotatebox{90}{\href{https://www.tensorflow.org/datasets/catalog/smallnorb}{snorb-azim}}
&

\rotatebox{90}{\href{https://www.tensorflow.org/datasets/catalog/smallnorb}{snorb-elev}}
\\
\midrule
\citet{Steiner2021HowTT}
& \textbf{95.5} & 94.1 & 80.3 & \textbf{99.6} & 95.0 & 83.4 & 97.4 & 86.4 & 99.0 & 96.6 & 83.3 & 99.8 & 91.7 & 75.6 & 100 & 90.4 & \textbf{84.7} & 27.5 & 76.5
\\
\citet{Zhai2019ALS} 
& 94.6 & 84.8 & 75.9 & 94.7 & 91.5 & 70.2 & 97.0 & 85.9 & 98.8 & 94.9 & 79.5 & 99.8 & 92.5 & 76.5 & 100 & \textbf{96.5} & 82.3 & \textbf{100} & \textbf{98.4}
\\
\muNet cont. 1st iter.  
& 92.6 & 94.6 & 79.8 & \textbf{99.6} & \textbf{95.3} & 84.5 & 97.3 & 87.5 & 99.1 & 96.3 & 83.7 & 99.8 & 93.2 & \textbf{76.9} & 100 & 96.2 & 83.0 & 32.3 & 94.5
\\
\muNet cont. 2nd iter. 
& 92.6 & 94.6 & 80.5 & \textbf{99.6} & \textbf{95.3} & \textbf{84.8} & \textbf{97.8} & 88.4 & \textbf{99.2} & \textbf{97.0} & \textbf{84.0} & 99.8 & \textbf{94.0} & \textbf{76.9} & 100 & 96.4 & 83.0 & 33.3 & 95.1
\\
\muNet cont. after VDD 
& \underline{93.0} & \textbf{\underline{94.7}} & \textbf{81.0} & \textbf{99.6} & \textbf{95.3} & \textbf{84.8} & \textbf{97.8} & \textbf{91.1} & 99.1 & \textbf{97.0} & \textbf{84.0} & 99.8 & \textbf{94.0} & \textbf{76.9} & 100 & 96.4 & \underline{82.3} & 33.3 & 95.1
\\
    \bottomrule
  \end{tabular}
  \vspace{-10pt}
\end{table}

Table~\ref{table:vtabfull-bench} reports the achieved results along with reference models that use limited knowledge transfer capabilities.
\citet{Steiner2021HowTT} reports the quality achieved by fine-tuning a model equivalent to our root model.
This outperforms \muNet on only 2 tasks, even if it has been trained multiple times 
to perform 
hyperparameter tuning.
\citet{Zhai2019ALS} reports the results of the best model identified with a large scale study.
This state of the art model outperforms \muNet on 4 tasks.
Again, increasing number of task iterations and additional knowledge (VDD) in the system, seem to yield better quality.

\begin{table}[b]
\vspace{-10pt}
  \caption{
Test accuracy mean and std.dev. achieved on the VDD benchmark by 3 system replicas.
  }
\vspace{-6pt}
  \label{table:vdd-bench}
  \small
\setlength\tabcolsep{1.7pt}
  \hspace*{-4.452pt}
  \centering
  \begin{tabular}{lcccccccccc}
    \toprule
\begin{tabular}[b]{@{}l@{}}  Model:\\\muNet\\ cont.\end{tabular}
& 
\rotatebox{90}{\href{https://www.tensorflow.org/datasets/catalog/visual_domain_decathlon\#visual_domain_decathlonimagenet12}{imagenet}}
&
\rotatebox{90}{\href{https://www.tensorflow.org/datasets/catalog/visual_domain_decathlon\#visual_domain_decathlonsvhn}{svhn}}
&
\rotatebox{90}{\href{https://www.tensorflow.org/datasets/catalog/visual_domain_decathlon\#visual_domain_decathloncifar100}{cifar100}}
&
\rotatebox{90}{\href{https://www.tensorflow.org/datasets/catalog/visual_domain_decathlon\#visual_domain_decathlongtsrb}{gtsrb}}
&
\rotatebox{90}{\href{https://www.tensorflow.org/datasets/catalog/visual_domain_decathlon\#visual_domain_decathlondaimlerpedcls}{daimler}}
&
\rotatebox{90}{\href{https://www.tensorflow.org/datasets/catalog/visual_domain_decathlon\#visual_domain_decathlonomniglot}{omniglot}}
&
\rotatebox{90}{\href{https://www.tensorflow.org/datasets/catalog/visual_domain_decathlon\#visual_domain_decathlonucf101}{ucf101}}
&
\rotatebox{90}{\href{https://www.tensorflow.org/datasets/catalog/visual_domain_decathlon\#visual_domain_decathlonaircraft_default_config}{aircraft}}
&
\rotatebox{90}{\href{https://www.tensorflow.org/datasets/catalog/visual_domain_decathlon\#visual_domain_decathlondtd}{dtd}}
&
\rotatebox{90}{\href{https://www.tensorflow.org/datasets/catalog/visual_domain_decathlon\#visual_domain_decathlonvgg-flowers}{flowers}}


\\

1st iter. 
& 89.1\plmi0.19
& 98.2\plmi0.23
& 97.2\plmi0.34
& 99.9\plmi0.04
& 99.9\plmi0.03
& 84.5\plmi1.18
& 83.6\plmi1.79
& 64.8\plmi3.57
& 75.2\plmi1.20
& 99.0\plmi0.11

\\

2nd iter. 
& 89.2\plmi0.12
& 98.4\plmi0.23
& 97.2\plmi0.40
& 99.9\plmi0.05
& 99.9\plmi0.04
& 84.3\plmi0.27
& 85.7\plmi1.36
& 65.4\plmi3.03
& 76.0\plmi0.49
& 99.2\plmi0.11
\\
    \bottomrule
  \end{tabular}
  \vspace{-10pt}
\end{table}


\begin{table}[t]
  \vspace{-10pt}
  \caption{Test accuracy achieved on the Multitask Character Classification Benchmark
   by \muNet continued extension with 2 active task iterations and by the model that has set the state of the art using comparable data splits:
  \citet{Jeevan2022WaveMixRT} for digits, 
  \citet{Kabir2020SpinalNetDN} for letters,
  \citet{Ajayan2021EdgeTQ} for kmnist,
  \citet{An2020AnEO} for mnist,
  \citet{Hazra2021BanglaMeiteiMS} for cmaterdb/bangla.
  Underlined models reuse knowledge introduced by other character classification tasks.
  Datasets are listed in decreasing size from the biggest emnist/digits (240k samples) to the smallest telugu (2.5k).}
  \vspace{-6pt}
  \label{table:chars-bench}
  \small
  \centering
\begin{tabular}{lcccccccc}
    \toprule
Model
& 
\rotatebox{90}{\href{https://www.tensorflow.org/datasets/catalog/emnist\#emnistdigits}{emnist/}} 
\rotatebox{90}{\href{https://www.tensorflow.org/datasets/catalog/emnist\#emnistdigits}{digits}} 
&
\rotatebox{90}{\href{https://www.tensorflow.org/datasets/catalog/emnist\#emnistletters}{emnist/}}
\rotatebox{90}{\href{https://www.tensorflow.org/datasets/catalog/emnist\#emnistletters}{letters}}
&
\rotatebox{90}{\href{https://www.tensorflow.org/datasets/catalog/kmnist}{kmnist}}
&
\rotatebox{90}{\href{https://www.tensorflow.org/datasets/catalog/mnist}{mnist}}
&
\rotatebox{90}{\href{https://www.tensorflow.org/datasets/catalog/omniglot}{omniglot}}
&
\rotatebox{90}{\href{https://www.tensorflow.org/datasets/catalog/cmaterdb\#cmaterdbbangla_default_config}{cmaterdb/}}
\rotatebox{90}{\href{https://www.tensorflow.org/datasets/catalog/cmaterdb\#cmaterdbbangla_default_config}{bangla}}
&
\rotatebox{90}{\href{https://www.tensorflow.org/datasets/catalog/cmaterdb\#cmaterdbdevanagari}{cmaterdb/}}
\rotatebox{90}{\href{https://www.tensorflow.org/datasets/catalog/cmaterdb\#cmaterdbdevanagari}{devanagari}}
&
\rotatebox{90}{\href{https://www.tensorflow.org/datasets/catalog/cmaterdb\#cmaterdbtelugu}{cmaterdb/}}
\rotatebox{90}{\href{https://www.tensorflow.org/datasets/catalog/cmaterdb\#cmaterdbtelugu}{telugu}}

\\

\midrule
State of the art           & 99.77 & \textbf{95.88} & 95.40 & \textbf{99.91} &  & 99.00 &  &  \\
\muNet cont. 1st iteration & \textbf{99.82} & 93.60 & \textbf{98.68} & 99.75 & 98.72 & 98.60 & \underline{96.60} & \underline{97.80} \\
\muNet cont. 2nd iteration & \textbf{99.82} & 93.68 & 98.60 & 99.69 & \textbf{99.84} & \underline{\textbf{99.10}} & \underline{\textbf{98.00}} & \underline{\textbf{99.40}} \\
    \bottomrule
  \end{tabular}
  \vspace{-10pt}
\end{table}

\subsection{Visual Domain Decathlon (VDD) benchmark}
\label{subsection:vdd}
The VDD benchmark \citep{hakanbilensylvestrerebuffitomasjakab2017} is introduced next.
The ML methodology proposed in this paper, can achieve higher efficiency by focusing the available compute on the training of a single multitask system.
Though, the standard approach to measure variance relies on experiment repetitions.
This section demonstrates how variance can be measured for any chosen segment of the training.
In practice, 2 task iterations are performed to introduce the VDD tasks starting from the state achieved after the introduction of the last benchmark as usual.
But, this experiment is run on 3 parallel copies of the system, allowing us to compute variance of the metrics for this set of task iterations.

The VDD benchmark is composed of 10 diverse tasks.
This is also reflected in
the diverse variance ranges measured 
(see Table~\ref{table:vdd-bench}).
Variance is low for most of the tasks.
However, for ucf101 and aircraft is significantly higher.
The metrics that have highest correlation with standard deviation are error rate (linear proportionality in log scale) and number of training samples per class (inverse proportionality in log scale) (Figure~\ref{fig:std-corr}).
These can be considered metrics indicative of the complexity of the task.
Furthermore, variance decreases with the second iteration: average standard deviation of 0.87 after 1 iteration and 0.61 after the second.
These findings can support the intuitive hypothesis that tasks with higher complexity may benefit from more iterations to decrease variance and approach convergence.
The next system extension continues from the state of one randomly selected replica.

\subsection{Multitask Character Classification benchmark}
We continue extending the system by adding a set of 8 character classification tasks.
Thus offering the opportunity to study knowledge transfer across tasks with high domain correlation.


\begin{table}[b]
  \vspace{-10pt}
  \caption{
  Test accuracy achieved on the VTAB-1k benchmark by:
    1) fine-tuning ViT L/16 i21k (matching root model) \citet{Dosovitskiy2021AnII},
    2) the Sup-rotation method \citep{Gidaris2018UnsupervisedRL} that achieved the best result in the VTAB-1k leaderboard \citep{Zhai2019ALS}. 
Underlined models have at least one ancestor trained on the corresponding full form task.
    Doubly underlined model inherit directly from the current best model for the matching full form task.
  }
  \vspace{-6pt}
  \label{table:vtab1k-bench}
  \centering
  \small
  \setlength\tabcolsep{1.7pt}
  \hspace*{-30.417pt}
  \begin{tabular}{lccccccccccccccccccc}
    \toprule
Model
&
\rotatebox{90}{\href{https://www.tensorflow.org/datasets/catalog/caltech101}{caltech101}}
&

\rotatebox{90}{\href{https://www.tensorflow.org/datasets/catalog/cifar100}{cifar100}}
&

\rotatebox{90}{\href{https://www.tensorflow.org/datasets/catalog/dtd}{dtd}}
&

\rotatebox{90}{\href{https://www.tensorflow.org/datasets/catalog/oxford_flowers102}{flowers102}}
&

\rotatebox{90}{\href{https://www.tensorflow.org/datasets/catalog/oxford_iiit_pet}{pets}}
&

\rotatebox{90}{\href{https://www.tensorflow.org/datasets/catalog/sun397}{sun397}}
&

\rotatebox{90}{\href{https://www.tensorflow.org/datasets/catalog/svhn_cropped}{svhn}}
&

\rotatebox{90}{\href{https://www.tensorflow.org/datasets/catalog/patch_camelyon}{camelyon}}
&

\rotatebox{90}{\href{https://www.tensorflow.org/datasets/catalog/eurosat\#eurosatrgb_default_config}{eurosat}}
&

\rotatebox{90}{\href{https://www.tensorflow.org/datasets/catalog/resisc45}{resisc45}}
&

\rotatebox{90}{\href{https://www.tensorflow.org/datasets/catalog/diabetic_retinopathy_detection/\#diabetic_retinopathy_detectionbtgraham-300}{retinopathy}}
&



\rotatebox{90}{\href{https://www.tensorflow.org/datasets/catalog/clevr}{clevr-count}}
&

\rotatebox{90}{\href{https://www.tensorflow.org/datasets/catalog/clevr}{clevr-dist}}
&

\rotatebox{90}{\href{https://www.tensorflow.org/datasets/catalog/dmlab}{dmlab}}
&

\rotatebox{90}{\href{https://www.tensorflow.org/datasets/catalog/dsprites}{dspr-loc}}
&

\rotatebox{90}{\href{https://www.tensorflow.org/datasets/catalog/dsprites}{dspr-orient}}
&



\rotatebox{90}{\href{https://www.tensorflow.org/datasets/catalog/kitti}{kitti-dist}}
&



\rotatebox{90}{\href{https://www.tensorflow.org/datasets/catalog/smallnorb}{snorb-azim}}
&

\rotatebox{90}{\href{https://www.tensorflow.org/datasets/catalog/smallnorb}{snorb-elev}}


\\

\midrule
\citet{Dosovitskiy2021AnII}
& 90.8 & 84.1 & 74.1 & 99.3 & 92.7 & \textbf{61.0} & 80.9 & 82.5 & 95.6 & 85.2 & 75.3 & 70.3 & 56.1 & 41.9 & 74.7 & 64.9 & 79.9 & 30.5 & 41.7 
\\
\citet{Zhai2019ALS}
& \textbf{91.7} & 53.7 & 69.5 & 90.8 & 88.1 & 32.8 & 88.5 & 83.4 & 96.0 & 82.0 & 71.1 & 47.3 & 57.2 & 36.6 & 88.3 & 52.1 & 77.1 & 51.6 & 33.7
\\
\muNet cont. 1st iter.
& 87.1 & \underline{89.4} & \underline{77.6} & \underline{99.2} & \underline{\underline{\textbf{94.5}}} & 57.6 & \underline{\underline{\textbf{97.5}}} & \textbf{86.0} & \underline{\underline{\textbf{98.6}}} & \underline{\underline{\textbf{93.4}}} & 78.0 & \underline{91.2} & 59.9 & 47.6 & \underline{\underline{58.4}} &   \underline{\underline{\textbf{96.2}}} & \textbf{81.9} & \underline{\underline{32.1}} & \underline{\underline{\textbf{92.5}}}

\\
\muNet cont. 2nd iter.
& 89.9 & \underline{\textbf{90.6}} & \underline{\underline{\textbf{78.1}}} & \underline{\textbf{99.7}} & \underline{\underline{\textbf{94.5}}} & 57.6 & \underline{\underline{\textbf{97.5}}} & \textbf{86.0} & \underline{\underline{98.3}} & \underline{\underline{\textbf{93.4}}} & \underline{\underline{\textbf{83.5}}} & \underline{\underline{\textbf{99.8}}} & \underline{\underline{\textbf{90.6}}} & \underline{\underline{\textbf{76.3}}} & \underline{\underline{\textbf{100}}} & \underline{\underline{\textbf{96.2}}} & 81.7 &  \underline{\underline{\textbf{33.7}}} & \underline{\underline{\textbf{92.5}}}

\\
\bottomrule
  \end{tabular}
    \vspace{-10pt}
\end{table}


Table~\ref{table:chars-bench} reports the test accuracy achieved with 2 active tasks iterations.
We observe that tasks with more training data (left) achieve convergence in the first iteration, this hypothesis is supported by lack of significant accuracy gains with the second iteration.
While tasks with less training data (right) show a significant gain from a second training iteration.
Smaller tasks use transferred in domain knowledge: bangla top model reuses components that embed knowledge introduced by emnist/letters, while devanagari transfers from omniglot and bangla, and telugu transfers from bangla and devanagari.
Furthermore, the achieved quality is comparable to the state of the art published for each task.


\subsection{VTAB-1k benchmark}
The system is further extended by adding the 1k-samples version of the VTAB tasks.
Since the system  contains already the knowledge learned from the full version of each task, this set allows to study how effective is the proposed method at retrieving knowledge that is already embedded in the system.

Table~\ref{table:vtab1k-bench} reports results along with reference models that use limited knowledge transfer capabilities.
During the first iteration, the models generated for the short form tasks can retrieve the knowledge of the corresponding full form task also without directly mutating its model,
but rather mutating a model having at least one 
ancestor trained on the full task.
For example, the model generated for flowers102 mutates the dtd model,
that has 28 ancestors, of which only the 21 was trained on oxford\_flowers102.
However, that is enough for flowers102 to achieve 99.2\% test 
accuracy.

After only one task iteration, 5 tasks achieve better test accuracy than the reference models without reusing any knowledge introduced by the corresponding full form task.
Particularly interesting is the case of kitty-dist (a.k.a kitty/closest\_vehicle\_distance), that achieves a strong performance without transferring from the matching full form task but composing the knowledge of related tasks: kitty/closest\_object\_distance (8 ancestors), kitty/count\_vehicles (3 ancestors) and kitty/count\_vehicles (3 ancestors).  Thus learning to estimate 
distance of the closest vehicle
by combining the knowledge of recognizing vehicles and estimating the distance of the closest object.
Also, clevr-dist achieves a strong performance by inheriting from the semantically equivalent task kitti/closest\_object\_distance without reusing the knowledge introduced by clevr-dist.




\section{Conclusion}
\label{section:conclusions}

We introduced the \muNet method,
aimed at achieving state-of-the-art quality on a large task set, with the ability to dynamically introduce new tasks into the running system. The more tasks are learned the more knowledge is embedded in the system. A ViT-L architecture (307M parameters) was evolved into a multitask system with 13'087M parameters jointly solving 69 tasks.
However, as the system grows,
the sparsity in parameter activation keeps the amount of compute and the memory usage per task constant.
The average added parameters per task decreases by 38\% through the experiment, and the resulting multitask system activates only 2.3\% of the total parameters per task (see Figure~\ref{fig:perc} and Table~\ref{table:compute}).
The proposed method allows decoupling the growth of useful representations for solving new tasks from the growth of parameters/compute per task.
Furthermore, experimenting with a large number of tasks allowed
us to identify different patterns of positive knowledge transfer and composition, achieving higher efficacy on small datasets and across related tasks.
The proposed approach to mutations allows to achieve immunity against common pitfalls of multitask systems such as catastrophic forgetting, negative transfer and gradient interference, and demonstrates the key data privacy properties we want to achieve in a continual learning system.
Future work can continue to build toward systems that can acquire further capabilities and knowledge across multiple modalities.

All the experiments reported in this paper can be \textbf{reproduced} by using the public datasets and published 2Net code, and can be extended by using the published checkpoints (see Appendix~\ref{section:repro}).





\begin{figure}[t]
\vspace{-10pt}
\centering
\includegraphics[width=1.\linewidth]{perc.pdf}
\vspace{-18pt}
\caption{
Activated and added parameters per task as percentage of the total number of parameters of the multitask system, measured through the course of the large scale experiment described in Section~\ref{section:large-exp}. Vertical lines highlight the start of the introduction for each of the considered benchmark.
}
\label{fig:perc}
\vspace{-10pt}
\end{figure}


\clearpage


















\bibliography{iclr2023_conference}
\bibliographystyle{iclr2023_conference}

\clearpage
\appendix

\section{Extended related work survey}
\label{sec:ex-rel}

The proposed method is designed to learn an unbounded number of tasks in a continual learning fashion.
In such contexts it aims to learn each task with higher quality and efficiency by automating and optimizing the knowledge transfer among any subset of tasks that can provide useful knowledge to one another.
The proposed model is designed to be immune from common multitask learning pitfalls: catastrophic forgetting, gradients interference, negative transfer. 
Cross-task \textbf{transfer-learning}
has gained popularity, especially through transfer learning from a model
pre-trained on a large amount of data for one or a few general tasks,
and then fine-tuned on a small amount of data for a related downstream task.
This approach has been shown to be very effective in a wide variety of problems
across many modalities, including
language \citep{Devlin2019BERTPO,Raffel2020ExploringTL} and vision \citep{Dosovitskiy2021AnII,He2016DeepRL}.
The success of transfer-learning applications hinges on adequate prior knowledge selection to avoid typical \textbf{negative transfer} pitfalls \citep{Rosenstein2005ToTO,Wang2019CharacterizingAA}.
Common solutions rely on data or model selection techniques,
often putting emphasis on the efficiency of the exploration
\citep{Zhang2020ASO,Mensink2021FactorsOI}, also method aiming to automate knowledge selection at a layer level have been proposed \citet{Sun2020AdaShareLW}.
Transfer learning capabilities are critical for \textbf{multitask models}.
ML models trained jointly on multiple tasks 
can be affected by \textbf{gradients interference} if any subset of parameters receive gradients jointly from multiple sources \citep{Chen2018GradNormGN,Yu2020GradientSF}, and by \textbf{catastrophic forgetting} of prior knowledge as new tasks are learned \citep{McCloskey1989CatastrophicII,French1999CatastrophicFI}.
These knowledge loss problems can be alleviated with weighted combination of tasks \citep{Liu2019LossBalancedTW,Sun2020ERNIE2A} and gradient transformation methods \citep{Chen2018GradNormGN,Sener2018MultiTaskLA,Kendall2018MultitaskLU}.
Stronger guarantees are provided by methods that compartmentalize task specific knowledge in dedicated parameter subsets \citep{Rebuffi2017LearningMV,Houlsby2019ParameterEfficientTL,Rusu2016ProgressiveNN,Rosenfeld2020IncrementalLT}.
Addressing catastrophic forgetting and identifying what subset of parameters/knowledge that is beneficial to share with each task is also critical for \textbf{continual learning} or life long learning methods
\citep{McCloskey1989CatastrophicII,French1999CatastrophicFI,Ramesh2022ModelZA}.

The proposed method relies on an evolutionary approach to jointly search the spaces of models architectures, hyperparameters, and prior knowledge selection while optimizing for an possibly multi-factor non-differetiable reward function.
The automation of \textbf{hyperparameter tuning} has been commonly addressed with Bayesian optimization \citep{Srinivas2010GaussianPO,Bergstra2011AlgorithmsFH,Snoek2012PracticalBO},
evolutionary methods have also been explored for this purpose
\citep{Jaderberg2017PopulationBT,Zhang2011EvolutionaryCM}.
Hyperparameters tuning can be considered related to the \textbf{neural architecture search} (NAS), as architectures can be defined by the selection of a sequence
of architectural hyperparameters.
Initially, NAS methods have been based on reinforcement learning techniques
\citep{Zoph2017NeuralAS} but sample efficient evolutionary approaches have also proposed \citep{Real2019RegularizedEF,Maziarz2018EvolutionaryNeuralHA}.
Alternative NAS methods focusing on more efficient parameter-sharing  \citep{Pham2018EfficientNA,Liu2019DARTSDA,Kokiopoulou2019FastTA}
or optimization for multi-factor quality/cost trade-offs \citep{Tan2019MnasNetPN} have also been explored.

The proposed method is capable to dynamically extend the system, adding capacity or novel structures in an unconstrained fashion.
A few methods have been proposed to achieve \textbf{dynamic architecture extensions} \citep{Chen2016Net2NetAL,Cai2018EfficientAS}, some also focusing on an unbounded stream of tasks \citep{Yoon2018LifelongLW,Yao2020OnlineSM}, or achieving immunity from catastrophic forgetting  \citep{Rusu2016ProgressiveNN,Li2018LearningWF,Li2019LearnTG,Rosenfeld2020IncrementalLT}.

The proposed method is sparsely activated, thus the unbounded growth of knowledge and parameters is decoupled from the growth of computational cost.
The growth in capabilities of state of the art models often requires growth in terms of trainable parameters \citep{Kaplan2020ScalingLF}.
\textbf{Sparse activation} techniques at sub-layer level \citep{Shazeer2017OutrageouslyLN,Du2021GLaMES} or network route level \citep{Fernando2017PathNetEC} allow to decouple model size growth from compute cost.
This is achieved by integrating a \textbf{routing technique} that selects the appropriate subset of parameters storing the most relevant knowledge for each task, sample or token/patch.

The ability of jointly solve a \textbf{large amount of tasks} is commonly associated with progress toward Artificial General Intelligence (AGI).
Advancements in scaling language models \citep{Brown2020LanguageMA,Thoppilan2022LaMDALM} allowed to achieve novel discourse, reasoning and zero/few shot learning capabilities that can be applied to new tasks without/minimal additional training.
Recent work aims to extend these achievements beyond text modality by defining static architectures for an extended subset of modalities \citep{Alayrac2022FlamingoAV,Reed2022AGA}.
These are a few examples of the ML models contributing to the line of research achieving incremental milestone toward AGI. Though, each model is trained from scratch with considerable resources consumption.
The introduction of abstractions allowing to modularize, dynamically extend and reuse these large models may contribute to accelerate the rate of innovation. 









\section{Experiments reproducibility and details}
\label{section:repro}
All the experiments reported in this paper can be reproduced by using the following public resources:
\begin{itemize}
    \item 
The ViT model definition and checkpoints published by \citet{Steiner2021HowTT}.
These resources are available at  \href{https://github.com/google-research/vision_transformer}{https://github.com/google-research/vision\_transformer} and distributed under the \href{https://github.com/google-research/vision_transformer/blob/main/LICENSE}{Apache License 2.0}.

\item Published code of the proposed method: 
\href{https://github.com/google-research/google-research/tree/master/muNet}{https://github.com/google-research/google-research/tree/master/muNet}
\item All the used datasets are publicly available via the Tensorflow Datasets image classification,  catalog.
Refer to \href{https://www.tensorflow.org/datasets/catalog/overview}{https://www.tensorflow.org/datasets/catalog/overview} for detailed information regarding each dataset licence and other metadata.
Table \ref{table:datasets} reports exact dataset splits and reference foor each task.
\end{itemize}

We also publish the \muNet checkpoint resulting from the large-scale multitask experiment reported in Section~\ref{section:large-exp}.
This checkpoint can be used for inference on any of the 69 learned image classification tasks,
or for further analysis, or even to be extended with additional tasks or methods. For information about the checkpoint and its license refer to:
\href{https://github.com/google-research/google-research/tree/master/muNet}{https://github.com/google-research/google-research/tree/master/muNet}


References for mentioned state of the art public model for different tasks are sourced from \href{https://paperswithcode.com}{https://paperswithcode.com} as of September 2022.


The VTAB-1k results are reported for reference and are not directly comparable to the state of the art, as the benchmark definition specifies that VTAB-full tasks cannot be used for pre-training.

The initial experiments reported in Sections \ref{section:preliminary} and \ref{subsection:imagenet} have been executed on a TPUv3 \citep{Jouppi2017IndatacenterPA} machine with 8 cores.
While, all the following experiments have been executed on a larger scale infrastructure using 32 TPUv4 chips in MegaCore mode,
by using the Pathways orchestration layer \citep{Barham2022PathwaysAD}.

Table~\ref{table:compute} reports more details for each training segment.



\begin{figure}[t]
\centering
\includegraphics[width=1.\linewidth]{graph-large.png}
\caption{Graph representing the architecture of the multitask system solving jointly 69 image classification tasks generated by the large scale continual learning experiment described in Section~\ref{section:large-exp}.
Each task is identified with a unique color.
Bottom triangular nodes represent the data input of each task.
Top rectangular nodes represent the head layer of each task.
Each edges sequence of the same color connecting a task input to its head, a \emph{path}, defines the layers sequence composing the model for each task.
Each path traverses 27 round nodes representing ViT L/16 internal layers (see Section~\ref{section:experiments}) in the following order from bottom to top: patch embedding, class token, position embedding and 24 transformer layers.
Internal nodes are represented with the color of the task on which the parameters of the corresponding layer were trained last.
Except for the gray nodes that have not received gradient updates from any of the 69 tasks and still carry the parameters of the root model that were loaded from a checkpoint of a ViT L/16 pretrained on the imagenet-21k dataset (see~Section~\ref{section:large-exp})
(Video:~\href{https://youtu.be/Hf88Ge0eiQ8}{youtu.be/Hf88Ge0eiQ8}).
}
\label{fig:deca-30}
\end{figure}


\vspace{40pt}
\begin{table}[h]
\caption{
Details for the different training segments of the large scale continual learning experiment described in Section~\ref{section:large-exp}.
}
\label{table:compute}
\centering
\begin{tabular}{lcccccc}
\toprule
Training & \multicolumn{3}{c}{TPU}  & \#Tasks & \#Params & Activated params  \\
  \cmidrule(r){2-4}
segment & core-hours & \#cores & type & & (M) & per task (\%) \\
\midrule
 ViT tasks 10 iters &  4949 &  8 & TPUv3 &  3 &  659 &  46.6\% \\
 VTAB-full 2 iters &  5927 &  32 & TPUv4 &  26 &  4400 & 7.0\% \\
 ViT tasks +1 iter &  541 &  32 & TPUv4 &  26 &  4577 & 5.2\% \\
 VDD 2 iters &  1507 &  32 & TPUv4 &  36 &  7790 & 3.9\% \\
 ViT tasks +1 iter &  564 &  32 & TPUv4 &  36 &  7854 & 3.9\%  \\
 VTAB-full +1 iter &  2266 &  32 & TPUv4 &  36 &  7596 & 4.0\% \\
 Char. class. 2 iters &  1785 &  32 & TPUv4 &  44 &  9368 & 3.3\% \\
 VTAB-1k 2 iters &  271 &  32 & TPUv4 &  69 &  13087 & 2.3\%  \\
 ViT tasks +1 iter &  568 &  32 & TPUv4 &  69 &  13090 & 2.3\%  \\
\bottomrule
\end{tabular}
\end{table}

\clearpage
\begin{figure}[t]
\centering
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=1.0\linewidth]{inter-std-errr.pdf}
\end{minipage}\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=1.0\linewidth]{inter-std-tsmp.pdf}

\end{minipage}
\caption{Correlation in log scale with the standard deviation measured during the variance analysis conducted on Visual Domain Decathlon benchmark by running the training on 3 parallel replicas of the system.
We display the 2 metrics that are most correlated with the standard deviation: error rate computed on the test set (left) and training samples per class (right).
The red line is fitted to minimize the squared distance to the set of points.}
\label{fig:std-corr}
\end{figure}




\begin{figure}[t]
\centering
\includegraphics[width=1.\linewidth]{hdist.pdf}
\caption{Distributions of the hyperparameter values used by the best models of the 69 image classification tasks at the end of the large scale continual learning experiment described in Section~\ref{section:large-exp}.
}
\label{fig:dist}
\end{figure}



\begin{table}[h]
\small
\caption{Datasets splits and reference (part 1 of 2).
For each dataset used in the experiments, this table reports:
1) dataset name indicative of the Tensorflow Datasets Catalogs identification string and linking to the corresponding catalog page ("visual\_domain\_decathlon" has been abbreviated as "vdd", and "diabetic\_retinopathy\_detection" as "drd"),
2) train, validation and test data splits, represented with the \href{https://www.tensorflow.org/datasets/splits}{standard Tensorflow Datasets format} ("validation" has been abbreviated as "val").
3) corresponding scientific publication reference.
Datasets are listed in the order of introduction into the system.
\\\hspace{\textwidth}
Notes:
\\\hspace{\textwidth}
[1] The test split of the \href{https://www.tensorflow.org/datasets/catalog/imagenet_v2}{imagenet\_v2} dataset is used as validation set for \href{https://www.tensorflow.org/datasets/catalog/imagenet2012}{imagenet2012}. \\\hspace{\textwidth}
[2] The test split of the \href{https://www.tensorflow.org/datasets/catalog/cifar10_1}{cifar10\_1} dataset is used as validation set for \href{https://www.tensorflow.org/datasets/catalog/cifar10}{cifar10}.
\\\hspace{\textwidth}
[3] The VTAB-full benchmark also includes the cifar100 task. Cifar100 has been introduced to the \muNet system as part of the initial benchmark. In the VTAB-full results tables we refer to the top 1 test accuracy achieved in the latest cifar100 training iteration without retraining it as part of the VTAB-full active training iteration.
\\\hspace{\textwidth}
[4] The definition for the VTAB standard and additional tasks has been sourced from \href{https://github.com/google-research/task_adaptation/tree/master/task_adaptation/data}{https://github.com/google-research/task\_adaptation/tree/master/task\_adaptation/data}.
\\\hspace{\textwidth}
[5] VTAB additional task, not included in the standard scoring set. These tasks were added to further scale the system and analyze transfer across related tasks.
}
\label{table:datasets}
\centering
\setlength\tabcolsep{3pt}
\begin{tabular}{lcccc}
    \toprule
& \multicolumn{3}{c}{Splits}                   \\
\cmidrule(r){2-4}
    Name & Train & Val. & Test  & Reference\\
    \midrule
    \midrule

\href{https://www.tensorflow.org/datasets/catalog/imagenet2012}{imagenet2012}
& train & {\tiny \href{https://www.tensorflow.org/datasets/catalog/imagenet_v2}{imagenet\_v2}:}test & val
& \citep{Russakovsky2015ImageNetLS}
\\
\href{https://www.tensorflow.org/datasets/catalog/cifar100}{cifar100}
& train[:98\%] & train[98\%:] & test
& \citep{Krizhevsky2009LearningML}
\\
\href{https://www.tensorflow.org/datasets/catalog/cifar10}{cifar10}
& train & {\tiny \href{https://www.tensorflow.org/datasets/catalog/cifar10_1}{cifar10\_1}:}test & test
& \citep{Krizhevsky2009LearningML}
\\

\midrule
\multicolumn{5}{c}{\textbf{VTAB-full benchmark}} \\

\href{https://www.tensorflow.org/datasets/catalog/caltech101}{caltech101}
& train[:{\tiny2754}] & train[{\tiny2754}:] & test
& \citep{FeiFei2004LearningGV}
\\
\href{https://www.tensorflow.org/datasets/catalog/dtd}{dtd}
& train & val & test
& \citep{Cimpoi2014DescribingTI}
\\
\href{https://www.tensorflow.org/datasets/catalog/oxford_flowers102}{oxford\_flowers102}
& train & val & test
& \citep{Nilsback2008AutomatedFC}
\\
\href{https://www.tensorflow.org/datasets/catalog/oxford_iiit_pet}{oxford\_iiit\_pet}
& train[:{\tiny2944}] & train[{\tiny2944}:] & test
& \citep{Parkhi2012CatsAD}
\\
\href{https://www.tensorflow.org/datasets/catalog/sun397}{sun397}
& train & val & test
& \citep{Xiao2010SUNDL}
\\
\href{https://www.tensorflow.org/datasets/catalog/svhn_cropped}{svhn\_cropped}
& train[:{\tiny65931}] & train[{\tiny65931}:] & test
& \citep{Netzer2011ReadingDI}
\\
\href{https://www.tensorflow.org/datasets/catalog/patch_camelyon}{patch\_camelyon}
& train & val & test
& \citep{Veeling2018RotationEC}
\\
\href{https://www.tensorflow.org/datasets/catalog/eurosat#eurosatrgb_default_config}{eurosat/rgb}
& train[:{\tiny16200}] & train[{\tiny16200}:{\tiny21600}] & train[{\tiny21600}:]
& \citep{Helber2019EuroSATAN}
\\
\href{https://www.tensorflow.org/datasets/catalog/resisc45}{resisc45}
& train[:{\tiny18900}] & train[{\tiny18900}:{\tiny25200}] & train[{\tiny25200}:]
& \citep{Cheng2017RemoteSI}
\\
\href{https://www.tensorflow.org/datasets/catalog/diabetic_retinopathy_detection/#diabetic_retinopathy_detectionbtgraham-300}{drd/btgraham-300}
& train & val & test
& \citep{kaggle-diabetic-retinopathy}
\\
\href{https://www.tensorflow.org/datasets/catalog/clevr}{clevr/count\_cylinders}
& train[:{\tiny63000}] & train[{\tiny63000}:] & val
& \citep{Johnson2017CLEVRAD}
\\
\href{https://www.tensorflow.org/datasets/catalog/clevr}{clevr/count\_all}
& train[:{\tiny63000}] & train[{\tiny63000}:] & val
& \citep{Johnson2017CLEVRAD}
\\
\href{https://www.tensorflow.org/datasets/catalog/clevr}{clevr/closest\_object\_distance}
& train[:{\tiny63000}] & train[{\tiny63000}:] & val
& \citep{Johnson2017CLEVRAD}
\\
\href{https://www.tensorflow.org/datasets/catalog/dmlab}{dmlab}
& train & val & test
& \citep{Zhai2019TheVT}
\\
\href{https://www.tensorflow.org/datasets/catalog/dsprites}{dsprites/label\_x\_position}
& train[:{\tiny 589824}] & train[{\tiny589824}:{\tiny663552}] & train[{\tiny663552:}]
& \citep{Klindt2021TowardsND}
\\
\href{https://www.tensorflow.org/datasets/catalog/dsprites}{dsprites/label\_orientation}
& train[:{\tiny 589824}] & train[{\tiny589824}:{\tiny663552}] & train[{\tiny663552:}]
& \citep{Klindt2021TowardsND}
\\
\href{https://www.tensorflow.org/datasets/catalog/kitti}{kitti/closest\_object\_distance}
& train & val & test
& \citep{Geiger2012AreWR}
\\
\href{https://www.tensorflow.org/datasets/catalog/kitti}{kitti/count\_vehicles}
& train & val & test
& \citep{Geiger2012AreWR}
\\
\href{https://www.tensorflow.org/datasets/catalog/kitti}{kitti/closest\_vehicle\_distance}
& train & val & test
& \citep{Geiger2012AreWR}
\\
\href{https://www.tensorflow.org/datasets/catalog/smallnorb}{smallnorb/label\_category}
& train & test[:50\%] & test[50\%:]
& \citep{LeCun2004LearningMF}
\\
\href{https://www.tensorflow.org/datasets/catalog/smallnorb}{smallnorb/label\_lighting}
& train & test[:50\%] & test[50\%:]
& \citep{LeCun2004LearningMF}
\\
\href{https://www.tensorflow.org/datasets/catalog/smallnorb}{smallnorb/label\_azimuth}
& train & test[:50\%] & test[50\%:]
& \citep{LeCun2004LearningMF}
\\
\href{https://www.tensorflow.org/datasets/catalog/smallnorb}{smallnorb/label\_elevation}
& train & test[:50\%] & test[50\%:]
& \citep{LeCun2004LearningMF}
\\

\midrule
\multicolumn{5}{c}{\textbf{Visual domain decathlon benchmark}} \\

\href{https://www.tensorflow.org/datasets/catalog/visual_domain_decathlon#visual_domain_decathlonimagenet12}{vdd/imagenet12}
& train & val[:50\%] & val[50\%:]
& \citep{hakanbilensylvestrerebuffitomasjakab2017}
\\
\href{https://www.tensorflow.org/datasets/catalog/visual_domain_decathlon#visual_domain_decathlonsvhn}{vdd/svhn}
& train & val[:50\%] & val[50\%:]
& \citep{hakanbilensylvestrerebuffitomasjakab2017}
\\
\href{https://www.tensorflow.org/datasets/catalog/visual_domain_decathlon#visual_domain_decathloncifar100}{vdd/cifar100}
& train & val[:50\%] & val[50\%:]
& \citep{hakanbilensylvestrerebuffitomasjakab2017}
\\
\href{https://www.tensorflow.org/datasets/catalog/visual_domain_decathlon#visual_domain_decathlongtsrb}{vdd/gtsrb}
& train & val[:50\%] & val[50\%:]
& \citep{hakanbilensylvestrerebuffitomasjakab2017}
\\
\href{https://www.tensorflow.org/datasets/catalog/visual_domain_decathlon#visual_domain_decathlondaimlerpedcls}{vdd/daimlerpedcls}
& train & val[:50\%] & val[50\%:]
& \citep{hakanbilensylvestrerebuffitomasjakab2017}
\\
\href{https://www.tensorflow.org/datasets/catalog/visual_domain_decathlon#visual_domain_decathlonomniglot}{vdd/omniglot}
& train & val[:50\%] & val[50\%:]
& \citep{hakanbilensylvestrerebuffitomasjakab2017}
\\
\href{https://www.tensorflow.org/datasets/catalog/visual_domain_decathlon#visual_domain_decathlonucf101}{vdd/ucf101}
& train & val[:50\%] & val[50\%:]
& \citep{hakanbilensylvestrerebuffitomasjakab2017}
\\
\href{https://www.tensorflow.org/datasets/catalog/visual_domain_decathlon#visual_domain_decathlonaircraft_default_config}{vdd/aircraft}
& train & val[:50\%] & val[50\%:]
& \citep{hakanbilensylvestrerebuffitomasjakab2017}
\\
\href{https://www.tensorflow.org/datasets/catalog/visual_domain_decathlon#visual_domain_decathlondtd}{vdd/dtd}
& train & val[:50\%] & val[50\%:]
& \citep{hakanbilensylvestrerebuffitomasjakab2017}
\\
\href{https://www.tensorflow.org/datasets/catalog/visual_domain_decathlon#visual_domain_decathlonvgg-flowers}{vdd/vgg-flowers}
& train & val[:50\%] & val[50\%:]
& \citep{hakanbilensylvestrerebuffitomasjakab2017}
\\
    \midrule
\multicolumn{5}{c}{Continues in Table~\ref{table:datasets2} \dots}  \\
\bottomrule
  \end{tabular}
\end{table}




\begin{table*}[h]
\small
\caption{Datasets splits and reference (part 2 of 2).}
\label{table:datasets2}
\centering
\setlength\tabcolsep{3pt}
\begin{tabular}{lcccc}
    \toprule
& \multicolumn{3}{c}{Splits}                   \\
\cmidrule(r){2-4}
    Name & Train & Val. & Test  & Reference\\
    \midrule
    \midrule
\multicolumn{5}{c}{\dots Continues from Table~\ref{table:datasets} }\\
\midrule
\multicolumn{5}{c}{\textbf{Multitask Character Classification Benchmark}} \\

\href{https://www.tensorflow.org/datasets/catalog/emnist#emnistdigits}{emnist/digits}
& train[5\%:] & train[:5\%] & test
& \citep{Cohen2017EMNISTEM}
\\
\href{https://www.tensorflow.org/datasets/catalog/emnist#emnistletters}{emnist/letters}
&train[5\%:] & train[:5\%] & test
& \citep{Cohen2017EMNISTEM}
\\
\href{https://www.tensorflow.org/datasets/catalog/kmnist}{kmnist}
& train[5\%:] & train[:5\%] & test
& \citep{Clanuwat2018DeepLF}
\\
\href{https://www.tensorflow.org/datasets/catalog/mnist}{mnist}
& train[5\%:] & train[:5\%] & test
& \citep{LeCun1998GradientbasedLA}
\\
\href{https://www.tensorflow.org/datasets/catalog/omniglot}{omniglot}
& train & small1 & small2
& \citep{Lake2015HumanlevelCL}
\\
\href{https://www.tensorflow.org/datasets/catalog/cmaterdb#cmaterdbbangla_default_config}{cmaterdb/bangla}
& train[20\%:] & train[:20\%] & test
& \citep{Das2012AGA,Das2012ASF}
\\
\href{https://www.tensorflow.org/datasets/catalog/cmaterdb#cmaterdbdevanagari}{cmaterdb/devanagari}
& train[20\%:] & train[:20\%] & test
& \citep{Das2012AGA,Das2012ASF}
\\
\href{https://www.tensorflow.org/datasets/catalog/cmaterdb#cmaterdbtelugu}{cmaterdb/telugu}
& train[20\%:] & train[:20\%] & test
& \citep{Das2012AGA,Das2012ASF}
\\

\midrule

\multicolumn{5}{c}{\textbf{VTAB 1k benchmark}} \\

\href{https://www.tensorflow.org/datasets/catalog/caltech101}{caltech101}
& train[{\tiny:800}] & train[{\tiny2754}:{\tiny2954}] & test
& \citep{FeiFei2004LearningGV}
\\
\href{https://www.tensorflow.org/datasets/catalog/cifar100}{cifar100}
& train[:{\tiny800}] & train[{\tiny45000}:{\tiny45200}] & test
& \citep{Krizhevsky2009LearningML}
\\
\href{https://www.tensorflow.org/datasets/catalog/cifar10}{cifar10}
& train[:{\tiny800}] & train[{\tiny45000}:{\tiny45200}] & test
& \citep{Krizhevsky2009LearningML}
\\
\href{https://www.tensorflow.org/datasets/catalog/dtd}{dtd}
& train[:{\tiny800}] & val[:{\tiny200}] & test
& \citep{Cimpoi2014DescribingTI}
\\
\href{https://www.tensorflow.org/datasets/catalog/oxford_flowers102}{oxford\_flowers102}
& train[:{\tiny800}] & val[:{\tiny200}] & test
& \citep{Nilsback2008AutomatedFC}
\\
\href{https://www.tensorflow.org/datasets/catalog/oxford_iiit_pet}{oxford\_iiit\_pet}
& train[:{\tiny800}] & train[{\tiny2944}:{\tiny3144}] & test
& \citep{Parkhi2012CatsAD}
\\
\href{https://www.tensorflow.org/datasets/catalog/sun397}{sun397}
& train[:{\tiny800}] & val[:{\tiny200}] & test
& \citep{Xiao2010SUNDL}
\\
\href{https://www.tensorflow.org/datasets/catalog/svhn_cropped}{svhn\_cropped}
& train[:{\tiny800}] & train[{\tiny65931}:{\tiny66131}] & test
& \citep{Netzer2011ReadingDI}
\\
\href{https://www.tensorflow.org/datasets/catalog/patch_camelyon}{patch\_camelyon}
& train[:{\tiny800}] & val[:{\tiny200}] & test
& \citep{Veeling2018RotationEC}
\\
\href{https://www.tensorflow.org/datasets/catalog/eurosat#eurosatrgb_default_config}{eurosat/rgb}
& train[:{\tiny800}] & train[{\tiny16200}:{\tiny16400}] & train[{\tiny21600}:]
& \citep{Helber2019EuroSATAN}
\\
\href{https://www.tensorflow.org/datasets/catalog/resisc45}{resisc45}
& train[:{\tiny800}] & train[{\tiny18900}:{\tiny19100}] & train[{\tiny25200}:]
& \citep{Cheng2017RemoteSI}
\\
\href{https://www.tensorflow.org/datasets/catalog/diabetic_retinopathy_detection/#diabetic_retinopathy_detectionbtgraham-300}{drd/btgraham-300}
& train[:{\tiny800}] & val[:{\tiny200}] & test
& \citep{kaggle-diabetic-retinopathy}
\\
\href{https://www.tensorflow.org/datasets/catalog/clevr}{clevr/count\_cylinders}
& train[:{\tiny800}] & train[{\tiny63000}:{\tiny63200}] & val
& \citep{Johnson2017CLEVRAD}
\\
\href{https://www.tensorflow.org/datasets/catalog/clevr}{clevr/count\_all}
& train[:{\tiny800}] & train[{\tiny63000}:{\tiny63200}] & val
& \citep{Johnson2017CLEVRAD}
\\
\href{https://www.tensorflow.org/datasets/catalog/clevr}{clevr/closest\_object\_distance}
& train[:{\tiny800}] & train[{\tiny63000}:{\tiny63200}] & val
& \citep{Johnson2017CLEVRAD}
\\
\href{https://www.tensorflow.org/datasets/catalog/dmlab}{dmlab}
& train[:{\tiny800}] & val[:{\tiny200}] & test
& \citep{Zhai2019TheVT}
\\
\href{https://www.tensorflow.org/datasets/catalog/dsprites}{dsprites/label\_x\_position}
& train[:{\tiny800}] & train[{\tiny589824}:{\tiny590024}] & train[{\tiny663552:}]
& \citep{Klindt2021TowardsND}
\\
\href{https://www.tensorflow.org/datasets/catalog/dsprites}{dsprites/label\_orientation}
& train[:{\tiny800}] & train[{\tiny589824}:{\tiny590024}] & train[{\tiny663552:}]
& \citep{Klindt2021TowardsND}
\\
\href{https://www.tensorflow.org/datasets/catalog/kitti}{kitti/closest\_object\_distance}
& train[:{\tiny800}] & val[:{\tiny200}] & test
& \citep{Geiger2012AreWR}
\\
\href{https://www.tensorflow.org/datasets/catalog/kitti}{kitti/count\_vehicles}
& train[:{\tiny800}] & val[:{\tiny200}] & test
& \citep{Geiger2012AreWR}
\\
\href{https://www.tensorflow.org/datasets/catalog/kitti}{kitti/closest\_vehicle\_distance}
& train[:{\tiny800}] & val[:{\tiny200}] & test
& \citep{Geiger2012AreWR}
\\
\href{https://www.tensorflow.org/datasets/catalog/smallnorb}{smallnorb/label\_category}
& train[:{\tiny800}] & test[:{\tiny200}] & test[50\%:]
& \citep{LeCun2004LearningMF}
\\
\href{https://www.tensorflow.org/datasets/catalog/smallnorb}{smallnorb/label\_lighting}
& train[:{\tiny800}] & test[:{\tiny200}] & test[50\%:]
& \citep{LeCun2004LearningMF}
\\
\href{https://www.tensorflow.org/datasets/catalog/smallnorb}{smallnorb/label\_azimuth}
& train[:{\tiny800}] & test[:{\tiny200}] & test[50\%:]
& \citep{LeCun2004LearningMF}
\\
\href{https://www.tensorflow.org/datasets/catalog/smallnorb}{smallnorb/label\_elevation}
& train[:{\tiny800}] & test[:{\tiny200}] & test[50\%:]
& \citep{LeCun2004LearningMF}
\\

    \bottomrule
  \end{tabular}
\end{table*}

\clearpage

\begin{algorithm}
\caption{Pseudocode for one active task iteration}
\label{algo}
\begin{algorithmic}[1]
\State Active task: 
\State Set of all the models currently in the multitask system: 
\State Active population: \ trained on  \For{}
    \For{}
        \State \Comment{Sample parent model}
        \State Parent model:  \textbf{none}
        \For{Candidate parent model: }  
            \If{}
                \State 
                \State \textbf{break}
            \EndIf
        \EndFor
        \If{ \textbf{none}}
            \State 
        \EndIf
        \State  \Comment{Sample child model}
        \State Set of mutations: 
        \For{Candidate mutation: }
            \If{}
                \State  
            \EndIf
        \EndFor
        \State Untrained child model: 

        \State  \Comment{Train child model}
        \State Retained child model:  \textbf{none}
        \For{}
            \State 
            \If{ \textbf{none} trained on }
                \State 
            \EndIf
        \EndFor
        \If{ \textbf{none}}
            \State 
        \EndIf
    \EndFor
\EndFor
\State \Comment{Keep only the best model for }
\State  not trained on 

\end{algorithmic}
\end{algorithm}

\end{document}
