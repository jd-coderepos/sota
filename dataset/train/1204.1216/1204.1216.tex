

\documentclass[letter]{sig-alternate-2013}
\usepackage{xspace}
\usepackage{times}
\usepackage{float}
\usepackage{multirow}
\usepackage{color}
\usepackage{cite}
\usepackage{url}
\usepackage{textcomp}
\usepackage{alltt}
\usepackage{upquote}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{pdflscape}
\usepackage{multicol}
\usepackage{epsfig}
\usepackage{epstopdf}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{relsize}
\usepackage{comment}
\usepackage{caption}
\usepackage{alltt}
\usepackage{verbdef}

\usepackage{enumitem}
\usepackage[hidelinks]{hyperref}


\newcounter{copyrightbox}

\setlist{itemsep=-0.1em,topsep=-0.03em}

\lstdefinelanguage{JavaScript}{
  keywords={typeof, new, true, false, try, catch, function, return, null, switch, var, if, in, while, do, else, case, break},
  keywordstyle=\color{blue}\bfseries,
  ndkeywords={class, export, boolean, throw, implements, import, this},
  ndkeywordstyle=\color{darkgray}\bfseries,
  identifierstyle=\color{black},
  sensitive=false,
  comment=[l]{//},
  morecomment=[s]{/*}{*/},
  commentstyle=\color{purple}\ttfamily,
  stringstyle=\color{red}\ttfamily,
  morestring=[b]"
}

\lstset{
   language=JavaScript,
   backgroundcolor=\color{white},
   extendedchars=true,
   basicstyle=\fontsize{8}{9}\ttfamily,
   showstringspaces=false,
   showspaces=false,
   numbers=left,
   numberstyle=\footnotesize,
   numberblanklines=false,
   numbersep=5pt,
   tabsize=2,
   breaklines=true,
   showtabs=false,
   captionpos=b,
   belowskip=-1.5em
}

\newfont{\mycrnotice}{ptmr8t at 7pt}
\newfont{\myconfname}{ptmri8t at 7pt}
\let\crnotice\mycrnotice \let\confname\myconfname 

\permission{Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.}
\conferenceinfo{ASIA CCS'14,}{June 4--6, 2014, Kyoto, Japan. \\
{\mycrnotice{Copyright is held by the owner/author(s). Publication rights licensed to ACM.}}}
\copyrightetc{ACM \the\acmcopyr} 
\crdata{978-1-4503-2800-5/14/06\ ...\_{1\&2}_A_A_1_B_B_B_C_B_2_2_A_{1\&2}_A_B_A_A_B_B_B_B_A_A_B_A_B_C\surd\surd\surd\surd\surd\surd\surd\surd\surd\surd\surd\surd\surd\surd\surd\surd\surd\surd\surd\surd\surd\surd\surd\surd\surd\surd\surd\surd\surd\surd\surd\surd\surd\surd\surd\surd\surd\surd\surd_A_A_1_A_1_2_{1\&2}_B_2_B_A_B_2_A_C_{A.o}_{B.o}_{A.m}_{B.m}_{A.m}_{B.m}_{A.m}_{B.m}_{A.m}_{B.m}_{A.o}_A_B_{A.o}_{B.m}_{A.m}_{B.o}_{A.m}_{B.m}_{A.o}_{B.m}_{B.m}_{A.o}_A_{B.m}_{A.m}_{B.o}_{A.m}_{B.m}_B_A_A_B+1\times-1age >= 18/" is used to test for numbers. With any commas stripped, the numeric values are casted into float using the \verb"parseFloat()" API, while all other values are trimmed. When comparing the parameters across the requests of the initial valid set of submissions, it neglects the names but considers only those transformed values. A candidate parameter is finally considered dependent to another parameter of the last request only if two values are identical. The references (i.e., id attribute or XPath location) to these candidates are recorded.

\subsubsection{Valid Features Detector}
\label{sec:crs_valid_features}
This module is to determine the key features that indicate reproducible acceptances by the server. The references to the candidate elements of key features are recorded throughout the initial valid set of submissions. In the second valid set of submissions, the extracted features are relocated in the rendered server responses. Hence, the final key features to be used are all verified to be reproducible in both valid set of submissions. 

First of all, the buttons that are clicked and captured in the Action Recorder automatically qualify as the key features since they are always needed to follow the intended workflow. 

Next, CRS extracts a number of other key features by locating user-supplied parameters that are visually reflected in the rendered responses. This is achieved by searching for the presence of a parameter value in the \verb"textContent" and \verb"value" attributes of every leaf node through DOM tree traversal. This technique is commonly used in search engine keyword highlighting~\cite{keyword-highlight}. Nevertheless, CRS handles number matching differently in order to tolerate formatting issues that are introduced by the server. For instance, \verb"12345" is first converted to a regular expression object, i.e., \verb"\/1[^\d]?2[^\d]?3[^\d]?4[^\d]?5\/", and can thus find its presence in a node content such as \verb"+n/1000-n+n\times -1\times nn111\%_A_B_A_B/". It is however unlikely that checksum verification can be applied to the \verb"TO" account number since its definition is totally up to another bank. Given an authorized account, the user still has to click ``confirm'' in the review page. 

If the \textit{lower} radio button is chosen, the server will take an authorized account from the corresponding ``Bank Code and Name'', ``Account No.'', and ``Account Holder's Name'' fields. The review page will then ask the user for an OTP generated from a dedicated hardware device. This second factor authentication is to assure that the transfer is authorized by the legitimate user.
In either case, the bank sends out an SMS to the user. The \verb"TO" account number is however partially masked, and that neither the holder name nor registered status are provided.

\textbf{Vulnerability Details}. CRS is first provided with an initial valid set of user actions that go through the complete workflow to transfer \10,000 from our account to an unregistered account of our own.

We noticed an interesting implementation that a hidden field called \verb"MACcode" is set at the client-side before any form validations and submission. Its value is prepared by concatenating the transaction parameters (i.e., \verb"FROM", \verb"TO", \verb"AMT") and encoding the result by the Base64 scheme. The server was found rejecting those requests if the \verb"MACcode" received does not align with the transaction parameters. 
Since CRS places a mutated parameter back to the application, the parameter can undergo the same \verb"MACcode" algorithm at the client-side. So, it is clear that such an algorithm cannot protect message authenticity, as opposed to its confusing name.

Existing scanners miss this vulnerability owing to the enforcements of client-side pre-processing and intended workflow.

\textbf{Disclosure}. More than a week after our experiments, we disclosed the vulnerability directly to BEA. Apparently, no monitoring alarms were triggered regardless of the large transaction amount. They acknowledged our finding, and promptly fixed it within a day. Their system was later enhanced by dispatching email notifications.




%
 \section{Related Work}
\label{sec:crs_related}

\subsection{Parameter Tampering Scanners} 
\textbf{Blackbox}. Blackbox scanners are not tied to any specific frameworks and languages used on the server-side. This makes them often more generic than whitebox approaches. In general, they all work by first crawling an application to capture any requests encountered, then fuzzing it by replaying the requests with some mutated parameters. 
Proxy-based parameter tampering scanners include the HTTP fuzzer in Acunetix~\cite{acunetix}, WebScarab~\cite{webscarab}, and TamperData~\cite{tamperdata}. They accept per-request fuzzing, and thus cannot automatically observe the intended workflow. They also require considerable amount of manual configurations such as specifying (a) a self-signed SSL certificate for interceptions of encrypted traffic~\cite{sslock}, and; (b) even explicit value range of each parameter.
 
NoTamper~\cite{notamper} outperforms proxy-based parameter tampering by reducing the manual efforts needed. Similar to CRS, it automates parameter generation based on the client-side validations. With static symbolic execution and constraint solving (that currently support the event-based form submissions only), it is able to generate an exhaustive set of parameters that are rejected by the client-side. This approach can complement the Parameter Mutator of CRS. It however requires an analyst to provide two (i.e., valid and invalid) initial sets of requests though, so as to discern whether the server accepts the mutated requests.

To preserve the intended workflow during fuzzing, research work is found only for scanning other web vulnerabilities such as XSS. A state-aware fuzzer~\cite{state-aware} is able to detect whether a fuzzing attempt has triggered a state change. If changed, it backtracks to the previous state by page traversal. It thus allows every state to be completely fuzzed by all candidate parameters before moving on to the next state. However, this stateful scanner can handle static web pages only, i.e., AJAX applications are not supported.

Nonetheless, all these scanners are limited owing to their fundamental scanner design. It is very hard to enhance them in handling the one-time use tokens, cross-request parameter dependencies, and client-side pre-processing. Their fuzzing requests are thus likely barred from reaching vulnerable code.

On the other hand, there are studies that evaluate the effectiveness of existing web vulnerability scanners. According to~\cite{blackboxeval}, the commercial scanners being evaluated are found not to be as comprehensive as they are claimed to be. This evaluation however covers only vulnerabilities like XSS, and it does not cover parameter tampering. According to~\cite{whyjohnny}, the study found that none of its evaluated scanners could find a parameter tampering vulnerability. It also suggests that high-level domain knowledge on the application is needed, but can hardly be known without a human.

PAPAS~\cite{parampollution} focuses only on a specific variant of parameter tampering, namely the parameter pollution vulnerabilities. Its success relies on the use of a server-side parameter precedence algorithm that allows malicious parameter to take over the precedence of an existing parameter. Its scanning approach is thus very similar to finding reflected vectors such as XSS, and that is different from finding more general parameter tampering vulnerabilities.

\textbf{Whitebox}. When server-side source code is available, whitebox scanners can be used to compare the inconsistencies between client-side and server-side validations.

WAPTEC~\cite{waptec} extends the NoTamper~\cite{notamper} approach by applying similar coding analysis to the server-side validations. The additional knowledge eliminates the manual effort required, and improves the selection of candidate parameter. But it still suffers from the limitations of NoTamper such as the negligence of the cross-request parameter dependency.

ViewPoints~\cite{viewpoints} discovers validation inconsistencies between the client-side and server-side by differential string analysis. It supports only a specific framework of the Java web applications. Besides reporting those parameters that are client-rejected yet server-accepted, its goal is to also report parameters that are client-accepted yet server-rejected. Eliminating this latter class of inconsistencies from the application can improve in usability but not its security.

In general, what can be validated from the server and client sides is intrinsically inconsistent since the server is always more knowledgeable (e.g., access to DB) than the client, and thus these approaches must report many false positives. It is also difficult to first annotate all the expected inconsistencies particularly in real-world complex applications before running these tools.

\subsection{Parameter Tampering Mitigations}

InteGuard~\cite{integuard} is a whitebox protection approach that operates a server-side proxy to verify the parameters exchanged between the server and a client as well as those further exchanged with a third-party server. It relies on a human to provide some valid transactions for analyzing the parameter dependencies in its learning phase. Then its online defense is to drop subsequent requests that violate the expected dependencies. Its mitigation is limited only to those parameters triggered by human. CRS can automatically explore the client-side events to uncover new parameters, which can be hardly exhausted by a human in the training phase.

Some development frameworks such as .NET~\cite{dotnet} and Java~\cite{googlewebtoolkit} can replicate the server-side validations to become client-side code. However, they are often limited to simple validations that are purposely programmed for replications. Swift~\cite{swift} proposes that developers should annotate those parameters and codes that are sensitive to run only on server-side or safe for both sides to facilitate replication. But again, it is demanding to annotate even a simple program.

Server-side validations and sanitizations are still fundamental defenses against parameter tampering. For client-side validations aiming at delivering better user experience, HTML 5 Validations~\cite{html5} and JavaScript libraries such as jQuery~\cite{jquery} can be deployed.

\subsection{Others}
Compared to~\cite{sso,shop-for-free} which also explored parameter relationships, CRS systematically analyzes all the parameter dependencies between different tuples of requests and responses, as shown in Figure~\ref{fig:arch}. As a result, CRS can automatically detect server acceptances by analyzing \texttt{Req vs Resp} and \texttt{Resp vs Resp'}, which makes a significant improvement over the manual and protocol-specific approaches as with~\cite{sso,shop-for-free}. The proposed model checking also requires human effort in accurately transforming a subset of logic replica from the original application~\cite{shop-for-free}.

Some existing work also involve in replaying user actions, but none of which have identified the advantage of fuzzing inside the application context, that is, by intercepting the submission requests for fuzzing. Mugshot~\cite{mugshot} deterministically captures and replays user actions and other JavaScript functions for failure and usability analysis. Kudzu~\cite{kudzu} enhances its code coverage by exploring client-side events for the discovery of more client-side injection vulnerabilities. Ripley~\cite{ripley} maintains a server-side replica of the client-side logic in a shadow browser hosted in a trusted proxy, and attempts to automatically replicate and replay the client-side events in the replica for integrity check. Although it ensures the computation integrity of the client-side code, it does not eliminate the need for server-side validations. Acunetix Web Vulnerability Scanner~\cite{acunetixwvs} is able to generate some user action events while scanning for XSS vulnerabilities.
 \section{Conclusion}

We studied a number of real-world web applications to understand their implementations, and how they can be intercepted for vulnerability scanning. Existing parameter tampering scanners do not consider the enforcements of, among others, the intended workflow, one-time use tokens, and parameter dependency across requests, which are all common in multi-request applications. Their effectiveness of vulnerability scanning is thus severely limited. We proposed the novel CRS approach to respect all these enforcements. We realized the approach by building a generic blackbox parameter tampering scanner. In our evaluation, it is practical to drive real applications to run for fuzzing. The importance of this work is demonstrated by the vulnerabilities uncovered in real-world applications including banks, which existing scanners miss. The scanning approach can also be extended to the discovery of other web application vulnerabilities including XSS and CSRF.

\section*{Acknowledgements}
This work is partially supported by the Sir Edward Youde
Memorial Fellowship. We appreciate and thank HSBC and Hong Kong Monetary Authority
for their prompt follow-up. Special thanks are due to Chris Smith,
Clara Ho, Thomas Yung, Esmond Lee, Shu-pui Li, and James Tam
for their support. We thank for the valuable comments
from Shuo Chen, Kehuan Zhang and anonymous reviewers. 
\bibliographystyle{abbrv}
\bibliography{references}  \end{document}