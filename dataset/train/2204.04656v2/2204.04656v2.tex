\section{Experiment}
\label{sec:experiment}



\subsection{Experiment Setup}

\noindent
\textbf{Dataset.} We carry out experiments on four video-level datasets: KITTI-STEP, Cityscaeps-VPS, VSPW, VIPSeg, and YouTube-VIS. KITTI-STEP has 21 and 29 sequences for training and testing, respectively. The training sequences are further split into a training set (12 sequences) and a validation set (9 sequences). Cityscapes-VPS contains 400 training, 50 validation, and 50 test videos. Following previous work~\cite{kim2020vps}, all 30 frames are predicted during the inference, and only 6 frames with ground truth are evaluated. Both Cityscapes-VPS and KITTI-STEP have the same class number with Cityscapes dataset~\cite{cordts2016cityscapes}. However, the definition of thing and class is different. For VSPW dataset and YouTube-VIS dataset, refer to \textit{the appendix file}.



\noindent
\textbf{Implementation Details.} We implement our models in PyTorch~\cite{pytorch_paper} with MMDetection toolbox~\cite{chen2019mmdetection}. We use the distributed training framework with 8 GPUs. Each mini-batch has one image per GPU. Following previous work, we first pretrain the image baseline on Cityscapes dataset~\cite{cordts2016cityscapes} where we adopt the same pretraining setting~\cite{cheng2020panoptic,STEP} for fair comparison. For STEP pretraining, we change the class distribution of Cityscapes dataset into STEP format to avoid overfitting on STEP. Both ResNet~\cite{resnet} and Swin Transformer~\cite{liu2021swin} are adopted as the backbone networks, and other layers use Xavier initialization~\cite{xavier_init}. The optimizer is AdamW~\cite{ADAMW} with weight decay 0.0001. We adopt full image size for random crop in both pretraining and training process. For the large Swin Transformer backbone~\cite{liu2021swin}, we also pretrain our model on Mapillary dataset~\cite{neuhold2017mapillary}. More details of pretraining and finetuning can be found in the appendix file. Due to diversity of dataset, for VSPW, YouTube-VIS and VIP-Seg dataset, refer to \textit{the appendix file}.



\begin{table}[t]
  \centering
   \caption{\small \textbf{Experiment results on KITTI set with both  and  metric.} OF refers to an optical flow network~\cite{teed2020raft}. The results on validation set are shown in the several top rows, and results on test set are in the bottom rows. P means Panoptic Deeplab~\cite{cheng2020panoptic}. Following~\cite{STEP}, we keep two decimal numbers.  is obtained via average results of window size  where ~\cite{STEP}. Top: validation set. Bottom: test set. We find 0.5\% noise on this dataset, where we report the average results (three times). Axial means using extra Axial Attention~\cite{axialDeeplab}.}
  \scalebox{0.60}{
  \begin{tabular}{l c c || c c c c }
    \toprule[0.2em]
    \textbf{KITTI-STEP} & Backbone & OF & STQ & AQ & SQ & VPQ \\
    \toprule[0.2em]
    P + IoU Assoc. & ResNet50 & & 0.58 & 0.47 & 0.71 & 0.44  \\
    P + SORT & ResNet50 &  & 0.59  & 0.50 & 0.71 & 0.42  \\
    P + Mask Propagation & ResNet50 & \checkmark & 0.67 & 0.63 & 0.71 & 0.44 \\
   
    Motion-Deeplab~\cite{STEP}& ResNet50 &  & 0.58 & 0.51 & 0.67 & 0.40  \\
    VPSNet~\cite{kim2020vps}& ResNet50  & \checkmark & 0.56 & 0.52 & 0.61 & {0.43}  \\
    TubeFormer-DeepLab~\cite{kim2022tubeformer} & ResNet-50 + axial &  & 0.70 & 0.64 &  0.76 & 0.51 \\
    \hline
    Video K-Net & ResNet50 &  & 0.71 & 0.70  & 0.71  &  0.46 \\
    Video K-Net & Swin-base &  & 0.73 & 0.72 & 0.73 & 0.53 \\
    Video K-Net & Swin-large & & 0.74 & 0.73 & 0.75 & 0.56 \\
    \bottomrule[0.2em]
    Motion-Deeplab~\cite{STEP}& ResNet50 &  & 0.52 & 0.46 & 0.60 & -  \\
    \hline
    Video K-Net & ResNet50 &  & 0.59 & 0.50  & 0.62  &  - \\
    Video K-Net & Swin-base &  & 0.63 & 0.60 & 0.65 & - \\
    \bottomrule[0.2em]
  \end{tabular}
  }
 
  \label{tab:kitti_baselines}
\end{table}
 
\noindent
\textbf{Evaluation Metrics.} For VPS task, as discussed in STEP~\cite{STEP}, different metrics result in a significant gap even on the same VPS dataset since different metrics emphasize different properties. We adopt two widely used metrics: Video Panoptic Quality () and Segmentation and Tracking Quality (). The former mainly focuses on mask proposal level as PQ~\cite{kirillov2019panoptic} with different window sizes and threshold parameters, while the latter emphasizes pixel level segmentation and tracking without any thresholds.  contains geometric mean of two items: Segmentation Quality () and Association Quality () where . The former evaluates the pixel-level tracking while the latter evaluates the pixel-level segmentation results in a video clip. Due to the interpretability~\cite{STEP} of , we adopt it for ablation studies. We also report  for the benchmark comparison on both datasets. For VSS task, we report the Mean Intersection over Union (mIoU) and mean Video Consistency ()~\cite{miao2021vspw} for reference.
 

\begin{table*}[t]
  \centering
   \caption{\small \textbf{Results on Cityscapes-VPS validation set}.  is temporal window size in~\cite{kim2020vps}. All the methods use the single scale inference without other augmentations in the test stage. In each cell, we report ,  and  in order. There is about 0.5\% noise on this dataset, where we report the average results (three times).}
  \scalebox{0.65}{
  \begin{tabular}{l|c|c|c|c|c|c}
    \toprule[0.2em]
    Method & Backbone &k = 0 & k = 5 & k = 10 & k = 15 & Average\\
    \toprule[0.2em]
    VPSNet~\cite{kim2020vps} & ResNet50 & 65.0  59.0  69.4 &  57.6 45.1  66.7 & 54.4 39.2 65.6 & 52.8  35.8  65.3 & 57.5  44.8  66.7 \\
    SiamTrack~\cite{woo2021learning_associate_vps} & ResNet50 & 64.6  58.3  69.1 & 
    57.6  45.6  66.6 & 54.2  39.2  65.2 & 52.7  36.7  64.6 & 57.3  44.7  55.0
    \\
    ViP-Deeplab~\cite{ViPDeepLab} & WideResNet41~\cite{zagoruyko2016wideresnet} & 68.2  N/A  N/A  & 61.3    N/A  N/A  &  58.2  N/A  N/A & 56.2    N/A  N/A  &   60.9   N/A  N/A \\
    ViP-Deeplab~\cite{ViPDeepLab} & WideResNet41~\cite{zagoruyko2016wideresnet}+RFP~\cite{qiao2021detectors} + AutoAug~\cite{cubuk2018autoaugment} &  69.2  N/A  N/A   & 62.3   N/A   N/A &  59.2  N/A  N/A  & 57.0    N/A  N/A &  61.9  N/A   N/A  \\
    \midrule
    
    Video K-Net & ResNet50  & 65.6  57.4  71.5 & 57.7  43.4  68.2 & 54.2  36.5  67.1 & 52.3  33.1  66.3 & 57.8  45.0  66.9 \\
    Video K-Net & Swin-base~\cite{liu2021swin} & 69.2  63.6  73.3 & 62.0  51.1  70.0 & 58.4  44.7  68.3 & 55.8  39.8  67.5 & 61.2  49.6  69.5 \\
    
    Video K-Net & Swin-base + RFP~\cite{qiao2021detectors} & 70.8  63.2  76.3 & 63.1  49.3  73.2 & 59.5  43.4  72.0 & 56.8  37.0  71.1 & 62.2  49.8  71.8 \\
    \bottomrule[0.1em]
    \end{tabular}
  }
  \label{tab:city_vps_results}
\end{table*}



\begin{table}[t]
  \centering
    \caption{\small \textbf{Results on VSPW validation set}.  means that a clip with  frames is used. All methods use the same setting for fair comparison.}
  \scalebox{0.70}{
  \begin{tabular}{l c c c c c }
    \toprule[0.2em]
    \textbf{VPSW} & Backbone & mIoU &  &  \\
    \toprule[0.2em]
    DeepLabv3+~\cite{deeplabv3plus} & ResNet101 & 35.7 & 83.5 & 78.4 \\
    PSPNet+~\cite{zhao2017pyramid} & ResNet101 & 36.5 & 84.4 & 79.8 \\
    TCB(PSPNet)~\cite{miao2021vspw} & ResNet101 & 37.5 & 86.9 & 82.1  \\
    \hline
    Video K-Net (Deeplabv3+) & ResNet101  & 37.9 & 87.0 & 82.1 \\
    Video K-Net (PSPNet) & ResNet101  & 38.0 & 87.2  & 82.3 \\
    \bottomrule[0.2em]
  \end{tabular}
  }

  \label{tab:vspw}
\end{table}


\begin{table}[!t]
	\centering
	\caption{\small \textbf{Results on Video instance segmentation} AP (\%) on the YouTube-VIS-2019~\cite{vis_dataset} validation dataset. * means using deformable fpn~\cite{zhu2020deformabledetr}. Axial means using extra Axial Attention~\cite{wang2020maxDeeplab}.
	The compared methods are listed by publication date. }
	\label{tab:sota2019_vis}
  \scalebox{0.65}{
\begin{tabular}{ r|c|c|cccc}
\toprule[0.15em]
 Method&backbone& AP &  &  &  &   \\
\toprule[0.15em]
FEELVOS~\cite{voigtlaender2019feelvos}&ResNet50&26.9&42.0&29.7&29.9&33.4  \\
MaskTrack R-CNN~\cite{vis_dataset}&ResNet50&30.3&51.1&32.6&31.0&35.5  \\
{MaskProp\cite{mask_pro_vis}}&ResNet-50&40.0&-&42.9&-&-  \\
{MaskProp~\cite{mask_pro_vis}}&ResNet101 &42.5&-&45.6&-&-  \\
{STEm-Seg~\cite{Athar_Mahadevan20ECCV}}&ResNet50&30.6&50.7&33.5&31.6&37.1  \\
{STEm-Seg~\cite{Athar_Mahadevan20ECCV}}&ResNet101&34.6&55.8&37.9&34.4&41.6  \\
CompFeat~\cite{fu2021compfeat} & ResNet50  & 35.3 & 56.0 & 38.6 & 33.1 & 40.3  \\
{VisTR~\cite{VIS_TR}}&ResNet50& 36.2& 59.8& 36.9& 37.2 & 42.4  \\
{VisTR~\cite{VIS_TR}} &ResNet101&40.1& 64.0& 45.0 & 38.3 & 44.9  \\
TubeFormer-DeepLab~\cite{kim2022tubeformer} & ResNet-50 + Axial &  38.8 & - & - & 44.0 & 51.4 \\
\hline
Video K-Net & ResNet50 & 40.5 &  63.5 & 44.5 & 40.7 & 49.9 \\
Video K-Net & Swin-base & 51.4 & 77.2 & 56.1 & 49.0 & 58.4 \\
Video K-Net & Swin-base* & 54.1 & 79.0 & 59.5 & 49.7 & 59.9 \\
\hline
\end{tabular}
}
\end{table}



\begin{table}[!t]
	\centering
	\caption{\small \textbf{Results on VIPSeg-VPS~\cite{miao2022large} validation dataset.} We report VPQ and STQ for reference. Following work~\cite{miao2022large}, we report VPQ score at different window sizes (1,2,4,6).}
	\label{tab:sota_vipseg}
  \scalebox{0.65}{
\begin{tabular}{ r|c|cccccc}
\toprule[0.15em]
 Method& backbone &  &  &  &  & VPQ & STQ \\
\toprule[0.15em]
VIP-DeepLab~\cite{ViPDeepLab} & ResNet50 & 18.4 & 16.9 & 14.8 & 13.7 & 16.0 & 22.0 \\
VPSNet~\cite{kim2020vps} & ResNet50 & 19.9 & 18.1 & 15.8 & 14.5 & 17.0 & 20.8 \\
SiamTrack~\cite{woo2021learning_associate_vps} & ResNet50 & 20.0 & 18.3 & 16.0 & 14.7 & 17.2 & 21.1 \\
Clip-PanoFCN~\cite{miao2022large} & ResNet50 & 24.3 & 23.5 & 22.4 & 21.6 & 22.9 & 31.5 \\
\hline
Video K-Net & ResNet50 & 29.5 & 26.5 & 24.5 & 23.7 & 26.1 & 33.1 \\
Video K-Net & Swin-base & 43.3 & 40.5 & 38.3 & 37.2 & 39.8 & 46.3 \\
\hline
\end{tabular}
}
\end{table}



\begin{table*}[h!]
    \footnotesize
	\centering
	\caption{\small Ablation studies and comparison analysis on KITTI-STEP validation set. All the experiments use ResNet-50 as backbone.}
    \subfloat[Ablation Study on Each Components.]{
    \label{tab:ablation_a}
	    \begin{tabularx}{0.38\textwidth}{c c c c c c c} 
		        				\toprule[0.15em]
    	baseline & KAE & KL & KF & STQ & AQ & SQ  \\
        \toprule[0.15em]
        K-Net    &  &  &  & 67.5 & 65.5 & 68.9 \\
        & \checkmark &  &  & 69.3 & 69.0 & 69.8 \\
        & \checkmark & \checkmark &  & 70.2 & 71.2 & 69.7 \\
    	& \checkmark & \checkmark & \checkmark & 70.9 & 70.8 & 71.2 \\
        \bottomrule[0.1em]
	    \end{tabularx}
    } \hfill
    \subfloat[Needs of Appearance Embeddings]{
    \label{tab:ablation_b}
		\begin{tabularx}{0.28\textwidth}{c c c} 
			\toprule[0.15em]
			Method & AQ & STQ \\
			\midrule[0.15em]
            RoI-Align~\cite{qdtrack} & 68.8 & 69.1 \\
            Mask-Emb~\cite{woo2021learning_associate_vps} & 67.3 & 68.1 \\
            Ours  & 70.8 & 70.9 \\
             Ours + Mask-Emb~\cite{woo2021learning_associate_vps}  & 70.3 & 70.8 \\
			\bottomrule[0.1em]
		\end{tabularx}
    } \hfill
    \subfloat[Effect of sampling in association.]{
    \label{tab:ablation_c}
		\begin{tabularx}{0.30\textwidth}{c c c c} 
			\toprule[0.15em]
			Method & STQ & AQ & SQ  \\
			\midrule[0.15em]
			 K-Net  & 67.5 & 65.5 & 68.9 \\
            GT-based (ours) & 69.3 & 69.0 & 69.8 \\
            sampling in~\cite{qdtrack} & 63.1 & 62.1 & 64.3 \\
			\bottomrule[0.1em]
		\end{tabularx}
    } \hfill
    \vspace{2mm}
    \subfloat[Ablation Study on Linking and Fusing Stage.]{
     \label{tab:ablation_d}
	    \begin{tabularx}{0.25\textwidth}{c c c c} 
		        				\toprule[0.15em]
    		 Stage & STQ & AQ & SQ  \\
    		\toprule[0.15em]
    	    3 & 70.9 & 70.8 & 71.2 \\
    	    2 & 68.5 & 68.2 & 69.3 \\
    	    1 & 66.9 & 63.4 & 67.3 \\
        	\bottomrule[0.1em]
	    \end{tabularx}
    } \hfill
    \subfloat[Ablation Study on Training Settings ]{
     \label{tab:ablation_e}
	    \begin{tabularx}{0.35\textwidth}{c  c  c c} 
		        				\toprule[0.15em]
    		 Settings  & STQ & AQ  & SQ \\
    		\toprule[0.15em]
    	    joint training  &  70.9 & 70.8 & 71.2\\
    	   only train the key frame & 70.1 & 70.1 & 69.8 \\
        	\bottomrule[0.1em]
	    \end{tabularx}
    } \hfill
    \subfloat[Ablation Study on Kernel Fusing]{
     \label{tab:ablation_f}
	    \begin{tabularx}{0.28\textwidth}{c c c c} 
		        				\toprule[0.15em]
    		 Settings  &  STQ & AQ & SQ \\
    		 \toprule[0.15em]
    		   K-Net & 67.5 & 65.5 & 68.9 \\
    	       w Update & 70.9 & 70.8 & 71.2 \\
    	       w/o Update & 67.1 & 66.2 & 68.3 \\
        	\bottomrule[0.1em]
	    \end{tabularx}
    } \hfill
\end{table*}

\subsection{Main Results}

\noindent
\textbf{Results on KITTI-STEP.} As shown in Tab.~\ref{tab:kitti_baselines}, our method achieves 0.71 STQ and 0.46 VPQ using ResNet50 as backbone and achieves the new state-of-the-art results among previous works. After applying a stronger backbone~\cite{liu2021swin}, we obtain about 3\% gains on STQ and 7\% gains on VPQ. This suggests strong feature representation leads to better segmentation-level prediction than pixel-level prediction. Compared with VPSNet~\cite{kim2020vps} and Motion-Deeplab~\cite{STEP}, our method does not need post-processing steps or extra optical flow to warp features. Compared with Motion-Deeplab~\cite{STEP}, our method outperforms it by 13\% and 7\% on validation set and test set respectively. The results suggest the suitability of our framework to serve as a new and stronger baseline. We find 0.5\% noise on STEP validation set.

\noindent
\textbf{Results on Cityscape-VPS.} Tab.~\ref{tab:city_vps_results} compares our method with previous works on Cityscapes-VPS datasets. For ResNet50, our method outperforms VPSNet~\cite{kim2020vps} by 0.3\%. Moreover, we follow the same pretraining procedure of ViP-Deeplab~\cite{ViPDeepLab} and achieve 62.2\% VPQ. Our method with Swin-Transformer base achieves better results than ViP-Deeplab, which is trained with stronger data augmentation~\cite{cubuk2018autoaugment}. Note that we find 0.5\% noise for this dataset. Moreover, we do not use the kernel fusion on this dataset due to the low frame rate of Cityscapes-VPS.

\noindent
\textbf{Results on VSPW.} We further carry out experiments on VSPW dataset for VSS task to prove the generalization of Video K-Net. All the methods are re-implemented on our codebase and achieve higher results than the original paper~\cite{miao2021vspw}. As shown in Tab.~\ref{tab:vspw}, Video K-Net boosts Deeplabv3+~\cite{deeplabv3plus} and PSPNet~\cite{zhao2017pyramid} by a significant margin on both mIoU (2-3\%) and  (3-4\%). This proves the generalization ability of both kernel fusion and kernel linking.

\noindent
\textbf{Results on Youtube-VIS-2019.} In Tab.~\ref{tab:sota2019_vis}, we report the results on Youtube-VIS datasets. Compared with previous work VisTR~\cite{VIS_TR}, our method achieves better results (4\%) but with less training times (12 epochs vs 300 epochs).

\noindent
\textbf{Results on VIPSeg-VPS.} In Tab.~\ref{tab:sota_vipseg}, we further report the results on recently challenging VPS dataset VIPSeg~\cite{miao2022large}. Using ResNet50 backbone, our Video K-Net outperforms Clip-PanoFCN~\cite{miao2022large} by 3\% VPQ and 1.6\% STQ with only 12 epoch training (half training time of Clip-PanoFCN). We further use a larger model (Swin-base), which results in new state-of-the-art results and outperforms previous work by 16.6\% VPQ and 14.8\% STQ. 


\subsection{Ablation Study}
\label{sec:ablation}

\noindent
\textbf{Ablation Study on Each Component.} In Tab.~\ref{tab:ablation_a}, we first perform ablation studies on the effectiveness of each component. Adding KAE yields 2.8\% STQ improvements with more significant gains on AQ (about 2.4\%). It shows that directly learning kernel association is a key factor for tracking. Adding Kernel Linking module (KL) further improves AQ by 1.0\%. Finally, appending kernel fusing results in 0.7\% STQ improvement and 1.4 \% improvement on SQ. The AQ decreases a little, mainly because both KL and KF are fighting for tracking quality and segmentation quality, respectively.

\noindent
\textbf{Needs of Extra Appearance Embedding.} In Tab.~\ref{tab:ablation_b}, we present detailed comparison with recent box based appearance embedding and mask based appearance embedding. In particular, we re-implement their methods on our baseline K-Net. We also use the same embedding loss functions: Equ.~\ref{equ:qd_loss} and Equ.~\ref{equ:qd_loss_aux} for fair comparison. From the table, our simple design leads to the best result. From the last row of Tab.~\ref{tab:ablation_b}, even combing both the appearance embedding and kernels, there is no clear gain. We implement this by adding mask-grouped appearance features from backbone and kernel embeddings together. That indicates kernels have already encoded the discriminative information, which is consistent with our toy experiments results: the pure kernel information is good enough for tracking. 

\noindent
\textbf{Effect of Sampling Examples in Association.} Pang~\etal~\cite{qdtrack} employ RPN to generate large samples to improve the tracking results. However, we find that directly using this method into our framework leads to inferior results because most kernels are not used to generate masks with Hungarian assignment strategy during the training stage. Adding more samples brings noise to the segmentation prediction shown in Tab.~\ref{tab:ablation_c}, causing a marked decrease in SQ metric. Thus, we adapt to assign training kernel samples that are matched with ground truth masks. The matched kernels are more robust during training.

\noindent
\textbf{Ablation on Link Stage.} In Tab.~\ref{tab:ablation_d}, we present ablation on the stage of linking kernels, where we find linking kernels at the last stage achieves the best results. Early fusing of kernels also leads to bad results, since initial kernels are not very accurate. This results in bad misalignment on both kernel and kernel features.


\begin{figure}[!t]
	\centering
	\includegraphics[width=0.80\linewidth]{Fig/video_knet_vis.pdf}
	\caption{\small Visualization of improvements over baseline methods. Tracking improvement in red boxes and segmentation improvement in yellow boxes. Best viewed in color and by zooming in. }
	\label{fig:vis_baseline}
\end{figure}

\vspace{1mm}
\noindent
\textbf{Effect on Training Settings.} Moreover, in Tab.~\ref{tab:ablation_e}, we find joint training with both frames  and  performs better than only training . This is mainly because both the proposed Kernel Linking and Kernel Fusion need to learn to group similar kernels and separate the different kernels.

\begin{figure}[!t]
	\centering
	\includegraphics[width=1.0\linewidth]{Fig/video_knet_exp_vis.pdf}
	\caption{\small Visual Examples of our Video K-Net. The left is on Cityscapes-VPS validation set, while the right is on KITTI-STEP validation set (video sequences from top to down). The same instances are shown with the same color. Best view it on screen. }
	\label{fig:results_vis}
\end{figure}

\noindent
\textbf{Ablation on Kernel Fusing Design.} In Tab.~\ref{tab:ablation_f}, we find the importance of Kernel Updating module in Kernel Fusing. Since previous features are not aligned to the current frame, we use current-frame features to re-weight previous kernels to filter out noise and outliers in the previous kernel.

\subsection{Visualization and More Analysis}

\noindent
\textbf{Visualization over Baseline.} In Fig.~\ref{fig:vis_baseline}, we visualize the improvements over the K-Net baseline. The red boxes show the tracking consistency, while the yellow boxes show the segmentation consistency. Our Video K-Net achieves better results on both tracking and segmentation qualities.

\noindent
\textbf{More Qualitative Results.} In Fig.~\ref{fig:results_vis}, we present several visual results on Citscapes-VPS and KITTI-STEP datasets. We find most foreground objects (person and car) can be well tracked and segmented. \textit{More visual examples can be found in the appendix file}.

\noindent
\textbf{Parameter and GFLOPs} Compared with K-Net baseline, our method only adds about \textbf{2.3\% GFLOPs} and \textbf{1.8\% Parameters} given  image inputs. 

\noindent
\textbf{Limitation and Future Work.} One limitation of Video K-Net is that we only explore one frame during both training and inference. This setting is mainly for fair comparison with other works~\cite{STEP,kim2020vps}. Both exploring the \textit{multiple frames} information via kernels and handling on long video inputs will be our future work.