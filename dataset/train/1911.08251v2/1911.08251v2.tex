\documentclass{article}

\usepackage[table]{xcolor}    

\usepackage[final,nonatbib]{neurips_2019}

\usepackage[numbers]{natbib}

\usepackage{xcolor}
\usepackage{xspace}
\usepackage{comment}


\newcommand{\apx}{Appendix\xspace
}


\usepackage{pdflscape} 

\usepackage{threeparttable}


\usepackage{scalerel}




\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      

\usepackage{dsfont}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{siunitx}
\usepackage{bm}

\usepackage{graphicx}
\usepackage{float}
\usepackage{wrapfig}
\usepackage[font=footnotesize]{caption}
\usepackage{subcaption}

\usepackage{multirow}
\usepackage{makecell}

\usepackage{gensymb}

\usepackage{afterpage}
\usepackage{scrextend}

\usepackage{rotating}

\usepackage{empheq}
\usepackage[most]{tcolorbox}

\usepackage{multirow,bigdelim}
\usepackage{enumitem}

\usepackage{scalerel}

\usepackage{tikz}
\usepackage{capt-of}

\usepackage{listings}
\usepackage[numbered]{matlab-prettifier}

\definecolor{backcolour}{rgb}{0.92,0.92,0.92}

\lstdefinestyle{mymatstyle}{style=Matlab-editor,
  basicstyle=\mlttfamily\footnotesize,
  backgroundcolor=\color{backcolour},
  frame=leftline,
  numberstyle=\scriptsize,
  xleftmargin=1.em,
}



\newtcbox{\kernelspace}[1][]{nobeforeafter, math upper, tcbox raise base, enhanced,
    colframe=white!25!black,
    colback=white!92!black,
    boxrule=1.2pt,
    #1}


\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\newcommand{\lp}{\left(}
\newcommand{\rp}{\right)}
\newcommand{\lb}{\left[}
\newcommand{\rb}{\right]}

\DeclareMathOperator*{\tr}{\operatorname{tr}}

\DeclareMathOperator*{\R}{\mathbb{R}}
\DeclareMathOperator*{\Cm}{\mathbb{C}}
\DeclareMathOperator*{\Z}{\mathbb{Z}}
\DeclareMathOperator*{\N}{\mathbb{N}}

\newcommand{\GL}[1]{\ensuremath{\operatorname{GL}(#1)}}
\newcommand{\E}[1]{\ensuremath{\operatorname{E}(#1)}}
\newcommand{\SE}[1]{\ensuremath{\operatorname{SE}(#1)}}
\renewcommand{\O}[1]{\ensuremath{\operatorname{O}(#1)}}
\newcommand{\SO}[1]{\ensuremath{\operatorname{SO}(#1)}}
\newcommand{\U}[1]{\ensuremath{\operatorname{U}(#1)}}
\newcommand{\D}[1]{\ensuremath{\operatorname{D}_{#1}}}
\newcommand{\C}[1]{\ensuremath{\operatorname{C}_{#1}}}
\newcommand{\DN}{\ensuremath{\operatorname{D}_{\!N}}}
\newcommand{\CN}{\ensuremath{\operatorname{C}_{\!N}}}
\newcommand{\bC}[1]{\ensuremath{\mathbf{\operatorname{\bf C}_{#1}}}}
\newcommand{\bD}[1]{\ensuremath{\mathbf{\operatorname{\bf D}_{#1}}}}
\newcommand{\bCN}{\ensuremath{\mathbf{\operatorname{\bf C}_{\!N}}}}
\newcommand{\bDN}{\ensuremath{\mathbf{\operatorname{\bf D}_{\!N}}}}
\newcommand{\Flip}{(\{\pm 1\}, *)}
\newcommand{\bFlip}{\boldsymbol{(\{\pm} 1 \boldsymbol{\}, *)}}

\newcommand{\Ind}[2]{\ensuremath{\operatorname{Ind}_{#1}^{#2}}}
\newcommand{\Res}[2]{\ensuremath{\operatorname{Res}_{#1}^{#2}}}

\newcommand{\Span}[1]{\ensuremath{\operatorname{Sp}\lp{#1}\rp}}

\newcommand{\vc}[1]{\ensuremath{\operatorname{vec}\!\lp{#1}\rp}}

\newcommand{\PSI}[1]{
	\begin{bmatrix}
		\cos\lp#1\rp & \!\!\!         \shortminus \sin\lp#1\rp \\
		\sin\lp#1\rp & \!\!\!\phantom{\shortminus}\cos\lp#1\rp \\
	\end{bmatrix}
}
\newcommand{\PSIP}[1]{
	\begin{bmatrix}
		         \shortminus \sin\lp#1\rp & \!\!\!         \shortminus \cos\lp#1\rp \\
		\phantom{\shortminus}\cos\lp#1\rp & \!\!\!         \shortminus \sin\lp#1\rp \\
	\end{bmatrix}
}
\newcommand{\PSIS}[1]{
	\begin{bmatrix}
		\cos\lp#1\rp & \!\!\!\phantom{\shortminus}\sin\lp#1\rp \\
		\sin\lp#1\rp & \!\!\!         \shortminus \cos\lp#1\rp \\
	\end{bmatrix}
}

\newcommand{\XI}[1]{
	\begin{bmatrix}
		1 & \!\!\! 0 \\
		0 & \!\!\! #1 \\
	\end{bmatrix}
}


\newcommand{\X}{\mathcal{X}}
\newcommand{\Loss}{\mathcal{L}}
\newcommand{\K}{\mathcal{K}}

\newcommand{\BF}{\mathcal{BF}}

\newcommand{\muvec}{{\bm \mu}}
\newcommand{\pivec}{{\bm \pi}}
\newcommand{\thetavec}{{\bm \theta}}
\newcommand{\bvec}{{\bm b}}
\newcommand{\mvec}{{\bm m}}
\newcommand{\ovec}{{\bm o}}
\newcommand{\tvec}{{\bm t}}
\newcommand{\vvec}{{\bm v}}
\newcommand{\wvec}{{\bm w}}
\newcommand{\xvec}{{\bm x}}
\newcommand{\yvec}{{\bm y}}
\newcommand{\zvec}{{\bm z}}
\newcommand{\pvec}{{\bm p}}
\newcommand{\qvec}{{\bm q}}
\newcommand{\Xvec}{{\bm X}}
\newcommand{\Yvec}{{\bm Y}}
\newcommand{\Zvec}{{\bm Z}}
\newcommand{\Sigvec}{{\bm \Sigma}}
\newcommand{\zerovec}{{\bm 0}}

\newcommand{\xtvec}{{\tilde{\xvec}}}
\newcommand{\ytvec}{{\tilde{\yvec}}}


\newcommand{\ind}{\,\rotatebox[origin=c]{90}{}\,}
\usepackage{centernot}
\newcommand{\nind}{\centernot{\rotatebox[origin=c]{90}{}}}

\usepackage{mathtools}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}

\DeclareMathSymbol{\shortminus}{\mathbin}{AMSa}{"39}
\usepackage{tabu}
\usepackage{diagbox}

\newlength{\secBefore}
\newlength{\secAfter}
\newlength{\subsecBefore}
\newlength{\subsecAfter}
\setlength{\secBefore}{-1.25ex}
\setlength{\secAfter}{-1.5ex}
\setlength{\subsecBefore}{-.6ex}
\setlength{\subsecAfter}{-.8ex}



\newcounter{magicrownumbers}
\newcommand\rownumber{\stepcounter{magicrownumbers}\arabic{magicrownumbers}}


\makeatletter
\def\blfootnote{\gdef\@thefnmark{}\@footnotetext}
\makeatother


\graphicspath{./images/}

\title{
    General E(2)\,-\,Equivariant Steerable CNNs
    }


\author{Maurice Weiler \\
	University of Amsterdam, QUVA Lab \\
	\texttt{m.weiler@uva.nl} \\
	\And
	Gabriele Cesa \\
	University of Amsterdam \\
	\texttt{cesa.gabriele@gmail.com} \\
}

\begin{document}

	\maketitle
	
	\blfootnote{* Equal contribution, author ordering determined by random number generator.}
	\blfootnote{ This research has been conducted during an internship at QUVA lab, University of Amsterdam.}

	\vspace*{-2.5ex}
	
\begin{abstract}

The big empirical success of group equivariant networks has led in recent years to the sprouting of a great variety of equivariant network architectures.
A particular focus has thereby been on rotation and reflection equivariant CNNs for planar images.
Here we give a general description of -equivariant convolutions in the framework of \emph{Steerable CNNs}.
The theory of Steerable CNNs thereby yields constraints on the convolution kernels which depend on group representations describing the transformation laws of feature spaces.
We show that these constraints for arbitrary group representations can be reduced to constraints under irreducible representations.
A general solution of the kernel space constraint is given for arbitrary representations of the Euclidean group  and its subgroups.
We implement a wide range of previously proposed and entirely new equivariant network architectures and extensively compare their performances.
-steerable convolutions are further shown to yield remarkable gains on CIFAR-10, CIFAR-100 and STL-10 when used as a drop-in replacement for non-equivariant convolutions.

\end{abstract} 	\vspace*{-1.2ex}


	
\vspace*{\secBefore}
\section{Introduction}\label{sec:introduction}
\vspace*{\secAfter}

The equivariance of neural networks under symmetry group actions has in the recent years proven to be a fruitful prior in network design.
By guaranteeing a desired transformation behavior of convolutional features under transformations of the network input, equivariant networks achieve improved generalization capabilities and sample complexities compared to their non-equivariant counterparts. 
Due to their great practical relevance, a big pool of rotation- and reflection- equivariant models for planar images has been proposed by now.
Unfortunately, an empirical survey, reproducing and comparing all these different approaches, is still missing.

An important step in this direction is given by the theory of \emph{Steerable CNNs}~\cite{Cohen2017-STEER,3d_steerableCNNs,Cohen2018-IIR,generaltheory,gauge} which defines a very general notion of equivariant convolutions on homogeneous spaces.
In particular, steerable CNNs describe -equivariant (i.e. rotation- and reflection-equivariant) convolutions on the image plane .
The feature spaces of steerable CNNs are thereby defined as spaces of \emph{feature fields}, characterized by a group representation which determines their transformation behavior under transformations of the input.
In order to preserve the specified transformation law of feature spaces, the convolutional kernels are subject to a linear constraint, depending on the corresponding group representations.
While this constraint has been solved for specific groups and representations~\cite{Cohen2017-STEER,3d_steerableCNNs}, no general solution strategy has been proposed so far.
In this work we give a general strategy which reduces the solution of the kernel space constraint under arbitrary representations to much simpler constraints under single, \emph{irreducible} representations.

Specifically for the Euclidean group  and its subgroups, we give a general solution of this kernel space constraint.
As a result, we are able to implement a wide range of equivariant models, covering regular GCNNs
\cite{Cohen2016-GCNN,Weiler2018-STEERABLE,Hoogeboom2018-HEX,bekkers2018roto,Dieleman2016-CYC,Kondor2018-GENERAL},
classical Steerable CNNs~\cite{Cohen2017-STEER}, Harmonic Networks~\cite{Worrall2017-HNET}, gated Harmonic Networks~\cite{3d_steerableCNNs}, Vector Field Networks~\cite{Marcos2017-VFN},
Scattering Transforms \cite{sifre2012combined,Sifre2013-GSCAT,bruna2013invariant,sifre2014rigid,oyallonDeepRotoTranslationScattering2015}
and entirely new architectures, in one unified framework.
In addition, we are able to build hybrid models, mixing different field types (representations) of these networks both over layers and within layers.


We further propose a group restriction operation, allowing for network architectures which are decreasingly equivariant with depth.
This is useful e.g. for natural images which show low level features like edges in arbitrary orientations but carry a sense of preferred orientation globally.
An adaptive level of equivariance accounts for the resulting loss of symmetry in the hierarchy of features.

Since the theory of steerable CNNs does not give a preference for any choice of group representation or equivariant nonlinearity, we run an extensive benchmark study, comparing different equivariance groups, representations and nonlinearities.
We do so on MNIST~12k, rotated MNIST~ and reflected and rotated MNIST~ to investigate the influence of the presence or absence of certain symmetries in the dataset.
A drop in replacement of our equivariant convolutional layers is shown to yield significant gains over non-equivariant baselines on CIFAR10, CIFAR100 and STL-10.

Beyond the applications presented in this paper, our contributions are of relevance for general steerable CNNs on homogeneous spaces \cite{Cohen2018-IIR,generaltheory} and gauge equivariant CNNs on manifolds \cite{gauge} since these models obey the same kind of kernel constraints.
More specifically, -dimensional manifolds, endowed with an orthogonal structure group  (or subgroups thereof), necessitate \emph{exactly} the kernel constraints solved in this paper.
Our results can therefore readily be transferred to e.g. spherical CNNs 
\cite{Cohen2018-S2CNN,gauge,kondorClebschGordanNets2018,estevesLearningEquivariantRepresentations2018,perraudinDeepSphereEfficientSpherical2018,jiang2019spherical}
or more general models of geometric deep learning
\cite{poulenardMultidirectionalGeodesicNeural2018a,masciGeodesicConvolutionalNeural2015,brunaSpectralNetworksDeep,boscainiLearningClassSpecific2015}.


 	

\vspace*{\secBefore}
\section{General E(2)\,-\,Equivariant Steerable CNNs}
\label{sec:e2cnns}
\vspace*{\secAfter}

Convolutional neural networks process images by extracting a hierarchy of feature maps from a given input signal.
The convolutional weight sharing ensures the inference to be translation-equivariant which means that a translated input signal results in a corresponding translation of the feature maps.
However, vanilla CNNs leave the transformation behavior of feature maps under more general transformations, e.g. rotations and reflections, undefined.
In this work we devise a general framework for convolutional networks which are equivariant under the Euclidean group , that is, under isometries of the plane .
We work in the framework of steerable CNNs
\cite{Cohen2017-STEER,3d_steerableCNNs,Cohen2018-IIR,generaltheory,gauge} 
which provides a quite general theory for equivariant CNNs on homogeneous spaces, including Euclidean spaces  as a specific instance.
Sections \ref{sec:feature_fields} and \ref{sec:steerable_convolutions} briefly review the theory of Euclidean steerable CNNs
as described in \cite{3d_steerableCNNs}.
The following subsections explain our main contributions:
a decomposition of the kernel space constraint into irreducible subspaces (\ref{sec:irrep_decomposition}),
their solution for  and subgroups (\ref{sec:kernel_constraint_solution_main}),
an overview on the group representations used to steer features, their admissible nonlinearities and their use in related work (\ref{sec:representations}),
the group restriction operation (\ref{sec:restriction})
and implementation details~(\ref{sec:implementation}). 	

\vspace*{\subsecBefore}
\subsection{Isometries of the Euclidean plane }
\label{sec:euclidean_group}
\vspace*{\subsecAfter}

\begin{table}
\newcolumntype{R}{>{}r<{}}
\newcolumntype{Q}{>{}r<{}} \newcolumntype{L}{>{}l<{}}
\newcolumntype{M}{R@{}L}
\newcolumntype{N}{Q@{}L}
\def\arraystretch{1.35}
\centering
\begin{tabular}{rcMN}
                       \ & order  &    \ &                                      &             &  \\ \hline
    orthogonal         \ & -           &           \ &                                      &  &  \\
special orthogonal \ & -           &          \ &                                      &  &  \\
    cyclic             \ & N           &           \ &                                      &             &  \\
    reflection         \ & 2           &  \ &                     &             &  \\
    dihedral           \ & 2N          &           \ &  &             &  \\
\end{tabular}
\vspace*{1ex}
\caption{Overview over the different groups covered in our framework.}
\label{tab:groups_arxiv}
\vspace*{-5ex}
\end{table}

The Euclidean group  is the group of isometries of the plane , consisting of translations, rotations and reflections.
Characteristic patterns in images often occur at arbitrary positions and in arbitrary orientations.
The Euclidean group therefore models an important factor of variation of image features.
This is especially true for images without a preferred global orientation like satellite imagery or biomedical images but often also applies to low level features of globally oriented images.

One can view the Euclidean group as being constructed from the translation group  and the orthogonal group  via the semidirect product operation as .
The orthogonal group thereby contains all operations leaving the origin invariant, i.e. continuous rotations and reflections.
In order to allow for different levels of equivariance and to cover a wide spectrum of related work we consider subgroups of the Euclidean group of the form , defined by subgroups .
Specifically,  could be either the special orthogonal group , the group  of the reflections along a given axis, the cyclic groups , the dihedral groups  or the orthogonal group  itself.
While  describes continuous rotations (without reflections),  and  contain  discrete rotations by angles multiple of  and, in the case of , reflections.
 and  are therefore discrete subgroups of order  and , respectively.
For an overview over the groups and their interrelations see Table \ref{tab:groups_arxiv}.

Since the groups  are semidirect products, one can uniquely decompose any of their elements into a product  where  and  \cite{Cohen2018-IIR} which \mbox{we will do in the rest of the paper.}

 	

\vspace*{\subsecBefore}
\subsection{E(2)\,-\,steerable feature fields}
\label{sec:feature_fields}
\vspace*{\subsecAfter}


Steerable CNNs define feature spaces as spaces of \emph{steerable feature fields}
 which associate a -dimensional feature vector  to each point  of a base space, in our case the plane .
In contrast to vanilla CNNs, the feature fields of steerable CNNs are associated with a transformation law which specifies their transformation under actions of  (or subgroups) and therefore endows features with a notion of \emph{orientation}\footnote{
    Steerable feature fields can therefore be seen as fields of \emph{capsules} \cite{Sabour2017-DYNCAPS}.
}.
Formally, a feature vector  encodes the coefficients of a coordinate independent geometric feature relative to a choice of reference frame or, equivalently, image orientation (see \apx~\ref{sec:gauge_cnns}).

\begin{wrapfigure}[12]{r}{0.46\textwidth}
    \begin{subfigure}{.47\linewidth}
        \includegraphics[width=\linewidth]{figures/scalar_field_2.png}\subcaption*{scalar field }\end{subfigure}
    \hspace*{1ex}~
    \begin{subfigure}{.47\linewidth}
        \includegraphics[width=\linewidth]{figures/vector_field_2.png}\subcaption*{vector field }\end{subfigure}
    \vspace*{-.6ex}
    \caption{
        Transformation behavior of -fields.
        }
    \label{fig:fields_trafo}
\end{wrapfigure}
An important example are \emph{scalar} feature fields , describing for instance gray-scale images or temperature and pressure fields.
The Euclidean group acts on scalar fields by moving each pixel to a new position, that is,  for some ; \mbox{see Figure~\ref{fig:fields_trafo}, left.}
\mbox{\emph{Vector}~fields} , like optical flow or gradient images, on the other hand transform as .
In contrast to the case of scalar fields, each vector is therefore not only moved to a new position but additionally changes its orientation via the action of ; \mbox{see Figure~\ref{fig:fields_trafo},~right.}

The transformation law of a general feature field  is fully characterized by its \emph{type} .
Here  is a group representation, specifying how the  channels of each feature vector  mix under transformations.
A representation satisfies  and therefore models the group multiplication  as multiplication of  matrices  and ;~see \apx~\ref{apx:repr_theory}.
More specifically, a -field transforms under the \emph{induced representation}\footnote{
    Induced representations are the most general transformation laws compatible with convolutions \cite{Cohen2018-IIR,generaltheory}.
}\footnote{
    Note that this simple form of the induced representation is a special case for semidirect product groups.
}
 of~~as
\vspace*{-.2ex}
\vspace*{-.5ex}

As in the examples above, it transforms feature fields by moving the feature vectors from  to a new position  and acting on them via .
We thus find scalar fields to correspond to the \emph{trivial representation}  which reflects that the scalar values do not change when being moved.
Similarly, a vector field corresponds to the standard representation  of .

In analogy to the feature spaces of vanilla CNNs comprising multiple channels, the feature spaces of steerable CNNs consist of multiple feature fields , each of which is associated with its own \emph{type} .
A stack  of feature fields is then defined to be concatenated from the individual feature fields and transforms under the direct sum  of the individual representations.
Since the direct sum representation is block diagonal, the individual feature fields are guaranteed to transform independently from each other.
A common example for a stack of feature fields are RGB images .
Since the color channels transform independently under rotations we identify them as three independent scalar fields.
The stacked field representation is thus given by the direct sum  of three trivial representations.
While the input and output types of steerable CNNs are given by the learning task, the user needs to specify the types  of intermediate feature fields as hyperparameters, similar to the choice of channels for vanilla CNNs.
We discuss different choices of representations in Section~\ref{sec:representations} and investigate them empirically in Section~\ref{sec:mnist_benchmark}. 	

\vspace*{\subsecBefore}
\subsection{E(2)\,-\,steerable convolutions}
\label{sec:steerable_convolutions}
\vspace*{\subsecAfter}

In order to preserve the transformation law of steerable feature spaces, each network layer is required to be equivariant under the group actions.
As proven for Euclidean groups\footnote{
    Proofs for more general cases can be found in \cite{Cohen2018-IIR,generaltheory}.
}
in \cite{3d_steerableCNNs}, the most general \emph{equivariant linear map} between 
steerable feature spaces, transforming under  and ,
is given by \emph{convolutions} with \emph{-steerable kernels}\footnote{
    As  returns a matrix of shape  for each position , its discretized version can be represented by a tensor of shape  as usually done in deep learning frameworks.
}
, satisfying a kernel constraint

Intuitively, this constraint determines the form of the kernel in transformed coordinates  in terms of the kernel in non-transformed coordinates  and thus its response to transformed input fields.
It~ensures~that~the~output feature fields transform as specified by  when the input fields are being transformed by ; see \apx \ref{apx:equivariance_conv} for a proof.

Since the kernel constraint is linear, its solutions form a linear subspace of the vector space of unconstrained kernels considered in conventional CNNs.
It is thus sufficient to solve for a basis of the -steerable kernel space in terms of which the equivariant convolutions can be parameterized.
The lower dimensionality of the restricted kernel space enhances the parameter efficiency of steerable CNNs over conventional CNNs similarly to the increased parameter efficiency of CNNs over MLPs by translational weight sharing.

 	

\vspace*{\subsecBefore}
\subsection{Irrep decomposition of the kernel constraint}
\label{sec:irrep_decomposition}
\vspace*{\subsecAfter}

The kernel constraint \eqref{eq:kernel_constraint} in principle needs to be solved individually for each pair of input and output types  and  to be used in the network\footnote{
		A numerical solution technique which is based on a Clebsch-Gordan decomposition of tensor products of irreps has been proposed in~\cite{3d_steerableCNNs}.
		While this technique would generalize to arbitrary representations it becomes prohibitively expensive for larger representations as considered here; see \apx~\ref{apx:comparison_SE3Nets}.
}.
Here we show how the solution of the kernel constraint for arbitrary representations can be reduced to much simpler constraints under \emph{irreducible representations} (irreps).
Our approach relies on the fact that any representation of a finite or compact group decomposes under a change of basis into a direct sum of irreps, each corresponding to an invariant subspace of the representation space  on which  acts.
Denoting the change of basis by , this means that one can always write

where  are the irreducible representations of  and the index set  encodes the types and multiplicities of irreps present in .
A~decomposition can be found by exploiting basic results of character theory and linear algebra \cite{serre1977linear}.

The decomposition of  and  in the kernel constraint \eqref{eq:kernel_constraint} leads to

The left and right multiplication with a direct sum of irreps reveals that the constraint decomposes into \emph{independent} constraints

on blocks  in  corresponding to invariant subspaces of the full space of equivariant kernels; see \apx \ref{apx:constraint_decomposition} for a visualization.
In order to solve for a basis of equivariant kernels satisfying the original constraint \eqref{eq:kernel_constraint}, it is therefore sufficient to solve the irrep constraints \eqref{eq:irrep_constraint} to obtain bases for each block, revert the change of basis and take the union over different blocks.
Specifically, given \mbox{-dimensional bases}

for the blocks  of , we get a -dimensional~basis

of solutions of \eqref{eq:kernel_constraint}.
Here  denotes a block  being filled at the corresponding location of a matrix of the shape of  with all other blocks being set to zero; see \apx~\ref{apx:constraint_decomposition}.
The completeness of
\makebox[\linewidth][s]{the basis found this way is guaranteed by construction if the bases for each block  are complete.}


Note that while this approach shares some basic ideas with the solution strategy proposed in~\cite{3d_steerableCNNs}, it is computationally more efficient for large representations; see \apx~\ref{apx:comparison_SE3Nets}.
We want to emphasize that this strategy for reducing the kernel constraint to irreducible representations is not restricted to subgroups of  but applies to steerable CNNs in general.

 	

\vspace*{\subsecBefore}
\subsection{General solution of the kernel constraint for O(2) and subgroups}
\label{sec:kernel_constraint_solution_main}
\vspace*{\subsecAfter}


In order to build isometry-equivariant CNNs on 
we need to solve the irrep constraints \eqref{eq:irrep_constraint} for the specific case of  being  or one of its subgroups.
For this purpose note that the action of  on  is norm-preserving, that is, .
The constraints \eqref{eq:kernel_constraint} and \eqref{eq:irrep_constraint} therefore only restrict the \emph{angular parts} of the kernels but leave their radial parts free.
Since furthermore all irreps of  correspond to one unique \emph{angular frequency}
(see \apx \ref{apx:irreps}),
it is convenient to expand the kernel w.l.o.g. in terms of an (angular) Fourier series
~\-6pt]
                                                    \!\!\!                \shortminus \!\cos(m\phi) \!\!\!\!
                                               \end{array}
                                               \Bigg]\Bigg[
                                               \begin{array}{c}
                                                    \!\!\cos(m\phi) \!\!\!\!\-6pt]
                                                    \!\!\!\sin\!\big(\!(\!m\!\shortminus\!n\!)\phi\big) & \!\!\!\phantom{\shortminus}\!\cos\!\big(\!(\!m\!\shortminus\!n\!)\phi\big)\!\!\!\!
                                               \end{array}
                                               \Bigg]
                                                ,\!
                                               \Bigg[
                                               \begin{array}{cc}
                                                    \!\!\!\cos\!\big(\!(\!m\! +         \!n\!)\phi\big) & \!\!\!\phantom{\shortminus}\!\sin\!\big(\!(\!m\! +         \!n\!)\phi\big)\!\!\!\! \
    \tilde{\rho}\ :=\ \operatorname{Res}_H^G(\rho):\,H\to\GL{\mathbb{R}^c},\ \ h\mapsto\rho(h) \,,
-8pt]
    \multirow{2}{*}{restriction depth} & \multicolumn{2}{c}{\hspace{-25pt}MNIST~rot}                                     & \multicolumn{4}{c}{MNIST~12k}                                                                                                         \\
    \cmidrule(lr{25pt}){2-3} \cmidrule(lr){4-7} 
                                       & group                             & test error (\%)               & group                             & test error (\%)               & group                             & test error (\%)               \\
    \4pt]
    1                                  & \multirow{5}{*}{}  &  & \multirow{5}{*}{}   &  & \multirow{5}{*}{}   &  \\
    2                                  &                                   &  &                                   &  &                                   &  \\
    3                                  &                                   &  &                                   &  &                                   &  \\
    4                                  &                                   &  &                                   &  &                                   &  \\
    5                                  &                                   &  &                                   &  &                                   &  \1pt]
\bottomrule
\end{tabular}







     }
    \vspace*{6pt}
    \caption{
        Effect of the group restriction operation at different depths of the network on MNIST~rot and MNIST~12k.
        Before restriction, the models are equivariant to a larger symmetry group than the group of global symmetries of the corresponding dataset.
        A restriction at larger depths leads to an improved accuracy.
        All restricted models perform better than non-restricted, and hence globally invariant, models.
    }
    \label{tab:mnist_restriction}
    \vspace*{-3ex}
\end{table} 

We again build models which apply gated nonlinearities.
As for , this leads to a greatly improved performance of the pure irrep models, see rows 54-55.
In addition we adapt the gated nonlinearity to the \textit{induced} irrep models (rows 56-57).
Here we apply an independent gate to each of the two permuting sub-fields (gate).
In order to be equivariant, the gates need to permute under reflections as well, which is easily achieved by deriving them from  fields instead of scalar fields.
The gated induced irrep model achieves the best results among all -steerable networks, however, it is still not competitive compared to the  models with large .






\vspace*{\subsecBefore}
\subsection{MNIST group restriction experiments}
\label{sec:mnist_restriction}
\vspace*{\subsecAfter}

All transformed MNIST datasets clearly show local rotational and reflectional symmetries but differ in the level of symmetry present at the global scale.
While the  and -equivariant models in the last section could exploit these local symmetries, their global invariance leads to a considerable loss of information.
On the other hand, models which are equivariant to the symmetries present at the global scale of the dataset only are not able to generalize over all local symmetries.
The proposed group restriction operation allows for models which are locally equivariant but are globally invariant only to the level of symmetry present in the data.
Table~\ref{tab:mnist_restriction} reports the results of models which are restricted after different layers.
Specifically, on MNIST~rot the -equivariant model introduced in the last section is restricted to , while we restrict the  and the  model~to~the~trivial~group~ on MNIST~12k.
The overall trend is that a restriction at later stages of the model improves the performance.
All restricted models perform significantly better than the invariant models.
Figure~\ref{fig:mnist_regular} shows that this behavior is consistent for different orders .






\vspace*{\subsecBefore}
\subsection{On the convergence of Steerable CNNs}
\label{sec:mnist_rot_convergence}
\vspace*{\subsecAfter}

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{figures/mnist_rot_convergence.pdf}
    \begin{minipage}{.95\linewidth}
        \vspace*{2pt}
        \caption{
            Validation errors and losses during the training of a conventional CNN and -equivariant models on MNIST~rot.
            Networks with higher levels of equivariance converge significantly faster.
        }
        \label{fig:mnist_convergence}
    \end{minipage}
\end{figure}

In our experiments we find that steerable CNNs converge significantly faster than non-equivariant CNNs.
Figure~\ref{fig:mnist_convergence} shows this behavior for the regular -steerable CNNs from Section~\ref{sec:mnist_benchmark} in comparison to a conventional CNN, corresponding to , on MNIST~rot.
The rate of convergence thereby increases with the order  and, as already observed in Figure~\ref{fig:mnist_regular}, saturates at approximately .
All models share approximately the same number of parameters.

The faster convergence of equivariant networks is explained by the fact that they generalize over -transformed images by design which reduces the amount of intra-class variability which they have to learn\footnote{
    Mathematically, -steerable CNNs classify \textit{equivalence classes} of images defined by the equivalence relation
    .
    Instead, MLPs learn to classify each image individually and conventional CNNs classify equivalence classes defined by translations, i.e. above equivalence classes for .
    For more details see Section~2 of~\cite{Weiler2018-STEERABLE}.
}.
In contrast, a conventional CNN has to learn to classify all transformed versions of each image explicitly which requires either an increased batch size or more training iterations.
The enhanced data efficiency of -steerable CNNs can therefore lead to a reduced training time.



\vspace*{\subsecBefore}
\subsection{Competitive MNIST rot experiments}
\label{sec:mnist_rot}
\vspace*{\subsecAfter}

As a final experiment on MNIST~rot we are replicating the regular  model from~\cite{Weiler2018-STEERABLE} which was the previous SOTA.
It is mostly similar to the models evaluated in the previous sections but is wider, uses larger kernel sizes and adds additional fully connected layers; see Table~\ref{tab:large_architecture} in the \apx.
As reported in Table~\ref{tab:mnist_final}, our reimplementation matches the accuracy of the original model.
Replacing the regular feature fields with the quotient representations used in Section~\ref{sec:mnist_benchmark} leads to slightly better results.
We refer to \apx~\ref{apx:quotient_models} for more insights on the improved performance of the quotient model.
A further significant improvement and a new state of the art is being achieved by a -equivariant model, which is restricted to  after the penultimate layer.

\begin{figure}
\begin{minipage}{\linewidth}
    \centering
    \begin{minipage}{0.45\linewidth}
    \begin{table}[H]
    \scalebox{.93}{
    \centering
    \small
    \setlength{\tabcolsep}{4pt}\renewcommand\arraystretch{.95}\begin{tabular}{ccll}
        \toprule
        model                       & group                 & representation & test error (\%) \\
        \midrule
        \cite{Cohen2016-GCNN}       &                  & regular/scalar &  \\
        \cite{Cohen2016-GCNN}       &                  & regular        &  \\
        \cite{Worrall2017-HNET}     &               & irreducible    &  \\
        \cite{Laptev_2016_CVPR}     & -                     & -              &  \\
        \cite{Marcos2017-VFN}       &               & regular/vector &  \\
        Ours                        &               & regular        &  \\
        \cite{Weiler2018-STEERABLE} &               & regular        &  \\
        Ours                        &               & quotient       &  \\
        Ours                        &  & regular        &  \\
        \bottomrule
    \end{tabular}}
    \vspace*{1.5pt}
    \caption{
        Final runs on MNIST rot
    }
    \label{tab:mnist_final}
    \end{table} 
    \end{minipage}
    \hfill
    \begin{minipage}{0.52\linewidth}
    \begin{table}[H]
    \scalebox{.93}{
    \centering
    \small
    \setlength{\tabcolsep}{5pt}\renewcommand\arraystretch{.90}\begin{tabular}{lll}
        \toprule
        model                         & CIFAR-10                                 & CIFAR-100        \\
        \midrule
        wrn28/10 \hspace{2.7ex} \cite{widenet}
                                            &                                    &           \\
        wrn28/10\phantom{*}\     &            &  \\
        wrn28/10*\               &            &  \\
        wrn28/10\phantom{*}\     &            &  \\
        wrn28/10\phantom{*}\     &            &  \\
        wrn28/10\phantom{*}\     &            &  \\
        \midrule
        wrn28/10 \hspace{2.7ex} \ \cite{autoaugment} \hspace{2.15ex} AA
                                      &   &  \\
        wrn28/10*\            AA  &            &  \\
        wrn28/10\phantom{*}\  AA  &            &  \\
        \bottomrule
    \end{tabular}}
    \vspace*{2.5pt}
    \caption{
        Test errors on CIFAR (AA=autoaugment)
    }
    \label{tab:cifar}
    \end{table} 
    \end{minipage}
\end{minipage}
\vspace*{-2ex}
\end{figure}




\vspace*{\subsecBefore}
\subsection{CIFAR experiments}
\label{sec:cifar}
\vspace*{\subsecAfter}

The statistics of natural images are typically invariant under global translations and reflections but are not under global rotations.
Here we are investigating the practical benefit of -steerable convolutions for such images by classifying CIFAR-10 and CIFAR-100.
For this purpose we implement several  and -equivariant versions of WideResNet~\cite{widenet}.
Different levels of equivariance, stated in the model specifications in Table~\ref{tab:cifar}, are thereby used in the three main blocks of the network (i.e. between pooling layers).
Regular representations are used throughout the whole model except for the last convolution which maps to a scalar field to produce invariant predictions.
For a fair comparison we scale the width of all layers such that the number of parameters of the original wrn28/10 model is approximately preserved.
Note that, due to their enhanced parameter efficiency, our models become wider than conventional CNNs.
Since this implies a higher computational cost, we add an equivariant model, marked by an additional *, which has about the same number of channels as the non-equivariant wrn28/10.
For rotation order  we are further using  kernels to mitigate the discretization artifacts of steering  kernels by  degrees.
All runs use the same training procedure as reported in~\cite{widenet} and \apx~\ref{apx:cifar}.
We want to emphasize in particular that we perform \textit{no further hyperparameter tuning}.

The results of the  model in Table~\ref{tab:cifar} confirm that incorporating the global symmetries of the data already yields a significant boost in accuracy.
Interestingly, the  model, which is purely rotation but not reflection-equivariant, achieves better results, which shows that it is worthwhile to leverage local rotational symmetries.
Both symmetries are respected simultaneously by the wrn28/10  model.
While this model performs better than the two previous ones on CIFAR-10, it surprisingly yields slightly worse result on CIFAR-100.
This might be due to the higher dimensionality of its feature fields which, despite the model having more channels in total, leads to less independent fields.
The best results (without using auto augment) are obtained by the  model which suggests that rotational symmetries are useful even on a larger scale.
The small wrn28/10*  model shows a remarkable gain compared to the non-equivariant wrn28/10 baseline \textit{despite not being computationally more expensive}.
To investigate whether equivariance is useful even when a powerful data augmentation policy is available, we further rerun both  models with \textit{AutoAugment}~(AA)~\cite{autoaugment}.
As without AA, both the computationally cheap wrn28/10* model and the wider wrn28/10 version outperform the wrn28/10 baseline by a large margin.





\vspace*{\subsecBefore}
\subsection{STL-10 experiments}
\label{sec:STL10}
\vspace*{\subsecAfter}

\begin{wrapfigure}[13]{r}{0.42\textwidth}
\vspace*{-5ex}
\begin{minipage}{\linewidth}
\begin{table}[H]
\centering
\scalebox{.93}{
    \small
    \begin{tabular}{@{\ }l@{\ }c@{\ \ }c@{\ \ }c@{\,}}
        \toprule
        model                 & group       & \#params & \ test error (\%)                     \\
        \midrule
        wrn16/8 \cite{cutout} & -           &  11M     &  \\
        wrn16/8*              &  &   5M     &  \\
        wrn16/8               &  &  10M     &  \\
        wrn16/8*              &  & 4.2M     &  \\
        wrn16/8               &  &  12M     &  \\
        \bottomrule
    \end{tabular}
    }
    \vspace*{6pt}
    \caption{
        Test errors of different equivariant models on the STL-10 dataset.
        Models with * are not scaled to the same number of parameters as the original model but preserve the number of channels of the baseline.
    }
    \label{tab:stl10}
\end{table}
\end{minipage}
\end{wrapfigure}
In order to test whether the previous results generalize to natural images of higher resolution we run additional experiments on STL-10~\cite{stl10}.
While this dataset was originally intended for semi-supervised learning tasks, its  training images are also being used for supervised classification in the low data regime~\cite{cutout}.
We adapt the experiments in~\cite{cutout} by replacing the non-equivariant convolutions of their wrn16/8 model, which was the previous supervised SOTA, with -steerable convolutions.
As in the CIFAR experiments, all intermediate features transform according to regular representations.
A final, invariant prediction is generated via a convolution to scalar fields.
We are again using steerable convolutions as a mere drop-in replacement, that is, we use the same training setting and hyperparameters as in the original paper.
The four adapted models, reported in Table~\ref{tab:stl10}, are equivariant under either the action of  in all blocks or the actions of ,  and  in the respective blocks.
For both choices we build a large model, whose width is scaled up to approximately match the number of parameters of the baseline, and a small model, which preserves the number of channels and thus compute and memory requirements, but is more parameter efficient.
\begin{wrapfigure}{r}{0.45\textwidth}
\vspace*{-4.8ex}
\begin{minipage}{\linewidth}
    \begin{figure}[H]
        \centering
        \includegraphics[width=.92\linewidth]{figures/stl10_ablation.pdf}
        \begin{minipage}{.9\linewidth}
            \vspace*{2pt}
            \caption{
                Data ablation study on STL-10.
                The equivariant models yield significantly improved results on all dataset sizes.
            }
            \label{fig:stl10_ablation}
        \end{minipage}
    \end{figure}
\end{minipage}
\end{wrapfigure}

\vspace*{-1.2ex}
As expected, all models improve significantly over the baseline with larger models outperforming smaller ones.
However, due to their extended equivariance, the small  model performs better than the large  model.
In comparison to the CIFAR experiments, rotational equivariance seems to give a more significant boost in accuracy.
This is expected since the higher resolution of  pixels of the STL-10 images allows for more detailed local patterns which occur in arbitrary orientations.

Figure~\ref{fig:stl10_ablation} reports the results of a data ablation study which investigates the performance of the  models for smaller training set sizes.
The results validate that the gains from incorporating equivariance are consistent over all training sets.
More information on the exact training procedures is given in \apx~\ref{apx:stl10}.


























 	

\vspace*{\secBefore}
\section{Conclusions}
\label{sec:conclusion}
\vspace*{\secAfter}


A wide range of rotation- and reflection-equivariant models has been proposed in the recent years.
In this work we presented a general theory of -equivariant steerable CNNs which describes many of these models in one unified framework.
By analytically solving the kernel constraint for any representation of  or its subgroups we were able to reproduce and systematically compare these models.
We further proposed a group restriction operation which allows us to adapt the level of equivariance to the symmetries present on the corresponding length scale.
When using -steerable convolutions as drop in replacement for conventional convolution layers we obtained significant improvements on CIFAR-10, CIFAR-100 and STL-10 without additional hyperparameter tuning.
While the kernel expansion leads to a small overhead during train time, the final kernels can be stored such that during test time steerable CNNs are computationally not more expensive than conventional CNNs of the same width.
Due to the enhanced parameter efficiency of equivariant models it is a common practice to adapt the model width to match the parameter cost of conventional CNNs.
Our results show that even non-scaled models outperform conventional CNNs in accuracy.
Since steerable CNNs converge faster than non-equivariant CNNs, they can even be cheaper to train.

We believe that equivariant CNNs will in the long term become the default choice for tasks like biomedical imaging, where symmetries are present on a global scale.
The impressive results on natural images demonstrate the great potential of applying -steerable CNNs to more general vision tasks which involve only local symmetries.
Future research still needs to investigate the wide range of design choices of steerable CNNs in more depth and collect evidence on whether our findings generalize to different settings.
We hope that our library\footnote{The library is available at \url{https://github.com/QUVA-Lab/e2cnn}.}
will help equivariant CNNs to be adopted by the community and facilitate further research.

 
	\vfill
	\subsubsection*{Acknowledgments}
	\vspace*{-1ex}
	{\small
	We would like to thank Taco Cohen for fruitful discussions on an efficient implementation and helpful feedback on the paper and Daniel Worrall for elaborating on the real valued implementation of Harmonic Networks.
	}

	\newpage
	\title{
		General E(2)\,-\,Equivariant Steerable CNNs \\
		Appendix 
		}
	\author{}
	\maketitlesup
	\vspace*{-10ex}

	\appendix

	
\vspace*{5ex}
\section{Local gauge equivariance of E(2)-steerable CNNs}
\label{sec:gauge_cnns}
\vspace*{1ex}


\begin{minipage}{\linewidth}
    \centering
    \begin{tikzpicture}
        \node[] at (0,0) {\includegraphics[width=.98\linewidth]{figures/gauge_trafos_viewpoints.png}};
        \node[anchor=north west,inner sep=0, align=left] at   (-3.6,  2.3) {\small active trafo};
\node[anchor=south west,inner sep=0, align=right] at (-3.65, -0.8) {\small global \\ \small coord. trafo \\ \small (passive)};
\node[anchor=south west,inner sep=0, align=center] at (1.95,  1.25) {\small local  \\ \small coord. trafo \
    \rho: G \to \operatorname{GL}(\mathbb{R}^n)
    \quad\text{such that}\quad
    \rho(g_1 g_2) = \rho(g_1) \rho(g_2) \quad \forall g_1, g_2 \in G \,.

    (\rho_1 \oplus \rho_2)(g) = 
    \begin{bmatrix}
    \rho_1(g) & 0 		 \\
    0  		  & \rho_2(g)\\
    \end{bmatrix}\,,

    \bigoplus\nolimits_i \rho_i(g)\ =\ \rho_1(g) \oplus \rho_2(g) \oplus \dots

    \rho(g) =
    Q
    \left[
    \bigoplus\nolimits_{i\in I}\psi_i(g)
    \right]
    Q^{-1}

    \Res{H}{G}(\rho): H \to \GL{{\R}^n},\ h\mapsto\rho\big|_H(h)
\label{eq:induced_vector_def}
    w = \sum_{gH} e_{gH} \otimes w_{gH} \ \in{\R}^{n{|G|\over|H|}}\,,

\label{eq:g_decomposition_h-fct}
    g=\mathcal{R}(gH) \text{h}(g) \,.
\label{eq:induced_rep_finite_def}
    \left[\Ind{H}{G} \rho \right]\!\!(\tilde{g}) \sum_{gH} e_{gH} \otimes w_{gH}\ :=\ \sum_{gH} e_{\tilde{g}gH} \otimes \rho\big(\text{h}(\tilde{g}\mathcal{R}(gH))\big)\, w_{gH} \,,

\\
\Ind{H}{G}\rho (\tilde{g})
\cdot
\left[
{\setlength\arraycolsep{3pt}
    \def\arraystretch{1.5}
    \begin{array}{c}
    \vdots \\
    \hline
     w_{gH} \vphantom{\vdots} \\
    \hline
    \vdots \\
    \hline
    \vdots \\
    \hline
    \vdots \\
    \end{array}
}
\right]
\quad
=
\quad
\left[
{\setlength\arraycolsep{3pt}
    \def\arraystretch{1.5}
    \begin{array}{cc}
    \vdots \\
    \hline
    \vdots \\
    \hline
    \vdots \\
    \hline 
        \rho(\text{h}(\tilde{g}\mathcal{R}(gH))) w_{gH} \vphantom{\vdots} \\
    \hline
    \vdots \\
    \end{array}
}
\right]
\!\!
{\setlength\arraycolsep{3pt}
    \def\arraystretch{1.5}
    \begin{array}{l}
    \vphantom{\vdots} \\
    \arrayrulecolor{white}
    \hline
    \left. \!\smallstrut \right\} gH \\
\arrayrulecolor{white}
    \hline
    \vphantom{\vdots} \\
    \arrayrulecolor{white}
    \hline
    \left. \!\smallstrut \right\} \tilde{g}gH = \tilde{g}\mathcal{R}(gH)H \\
\arrayrulecolor{white}
    \hline
    \vphantom{\vdots} \\
    \end{array}
}

\label{eq:induced_field}
    [\Ind{H}{G}\rho(\tilde{g}) \cdot f](gH) = \rho(\text{h}(\tilde{g} \mathcal{R}(\tilde{g}^{-1}gH))) f(\tilde{g}^{-1}gH) \,.

\label{eq:induced_field_semidirect}
    [\Ind{H}{G}\rho(\tilde{g}) \cdot f](gH) = \rho(\text{h}(\tilde{g}))\, f(\tilde{g}^{-1}gH)

\mathbin{\rotatebox[origin=c]{  0  }{}},
\mathbin{\rotatebox[origin=c]{ 22.5}{}},
\mathbin{\rotatebox[origin=c]{ 45  }{}},
\mathbin{\rotatebox[origin=c]{ 67.5}{}},
\mathbin{\rotatebox[origin=c]{ 90  }{}},
\mathbin{\rotatebox[origin=c]{112.5}{}},
\mathbin{\rotatebox[origin=c]{135  }{}}\ \text{and}\ 
\mathbin{\rotatebox[origin=c]{157.5}{}},

\mathbin{\rotatebox[origin=c]{  0  }{}},
\mathbin{\rotatebox[origin=c]{ 22.5}{}},
\mathbin{\rotatebox[origin=c]{ 45  }{}}\ \text{and}\ 
\mathbin{\rotatebox[origin=c]{ 67.5}{}}.

\mathbin{\rotatebox[origin=c]{  0}{}},
\mathbin{\rotatebox[origin=c]{ 45}{}},
\mathbin{\rotatebox[origin=c]{ 90}{}},
\mathbin{\rotatebox[origin=c]{135}{}},
\mathbin{\rotatebox[origin=c]{180}{}},
\mathbin{\rotatebox[origin=c]{225}{}},
\mathbin{\rotatebox[origin=c]{270}{}}\ \text{and}\ 
\mathbin{\rotatebox[origin=c]{315}{}}

\mathbin{\rotatebox[origin=c]{  0}{}},
\mathbin{\rotatebox[origin=c]{ 45}{}},
\mathbin{\rotatebox[origin=c]{ 90}{}},
\mathbin{\rotatebox[origin=c]{135}{}},
\mathbin{\rotatebox[origin=c]{180}{}},
\mathbin{\rotatebox[origin=c]{225}{}},
\mathbin{\rotatebox[origin=c]{270}{}}\ \text{and}\ 
\mathbin{\rotatebox[origin=c]{315}{}}.

    \rho_\text{quot}^{\DN\!/\Flip}(s) e_{r\Flip} = e_{sr\Flip} = 
    \begin{cases}
        e_{r\Flip}                         & \text{for }\ s=+1 \\
        e_{r^{-1}s\Flip} = e_{r^{-1}\Flip} & \text{for }\ s=-1
    \end{cases}

    f_\text{in}(x)
    \ &\mapsto\ 
    \left(\left[\operatorname{Ind}_{G}^{(\R^2,+)\rtimes G}\,\rho_\text{in}\ \right](gt)f_\text{in}\right)(x)
    \ \ :=\ 
    \ \, \rho_\text{in}(g)\,f_\text{in}\left(g^{-1}(x-t)\right)
    \
Here we show that the -steerability \eqref{eq:kernel_constraint} of convolution kernels is \emph{sufficient} to guarantee the equivariance of the mapping.
We therefore define the convolution (or correlation) operation of a feature field with a -steerable kernel  as usual by


The convolution with a transformed input field then gives

i.e. it satisfies the desired equivariance condition

We used the kernel steerability \eqref{eq:kernel_constraint} in the second step to identify  with .
In the third step we substituted  which does not affect the integral measure since

for an orthogonal transformation .

A proof showing the -steerability of the kernel to not only be sufficient but necessary is given in \cite{3d_steerableCNNs}.









\vspace*{4ex}

\subsection{Equivariance of spatially localized nonlinearities}
\label{apx:equivariance_nonlin}

We consider nonlinearities of the form

which act spatially localized on feature vectors .
These localized nonlinearities are used to define nonlinearities  acting on entire feature fields  by mapping each feature vector individually, that is,

In order for  to be equivariant under the action of induced representations it is sufficient to require

since then







\subsubsection{Equivariance of individual subspace nonlinearities w.r.t. direct sum representations}
\label{apx:equivariance_direct_sum}

The feature spaces of steerable CNNs comprise multiple feature fields  which are concatenated into one big feature field  defined by .
By definition  transforms under  if each  transforms under .
If  is an equivariant nonlinearity satisfying  for all , then

i.e. the concatenation of feature fields respects the equivariance of the individual nonlinearities.
Here we defined  as acting individually on the corresponding  in , that is,
.

To proof this statement consider without loss of generality the case of two feature fields  and  with corresponding representations  and  and equivariant nonlinearities  and .
Then it follows for all  that

The general case follows by induction.








\subsubsection{Equivariance of norm nonlinearities w.r.t. unitary representations}
\label{apx:equivariance_nonlin_norm}

We define unitary representations to preserve the norm of feature vectors, i.e.
.
Norm nonlinearities are functions of the type
,
where  is some nonlinearity acting on norm of a feature vector.
Since norm nonlinearities preserve the orientation of feature vectors they are equivariant under the action of unitary representations:









\subsubsection{Equivariance of pointwise nonlinearities w.r.t. regular and quotient representations}
\label{apx:equivariance_nonlin_quotient}

Quotient representations act on feature vectors by permuting their entries according to the group composition as defined by
.
The permutation of vector entries commutes with pointwise nonlinearities  which are being applied to each entry of a feature vector individually:

Any pointwise nonlinearity is therefore equivariant under the action of quotient representations.
The same holds true for regular representations  which are a special case of quotient representations for the choice 







\subsubsection{Equivariance of vector field nonlinearities w.r.t. regular and standard representations}
\label{apx:equivariance_nonlin_vector}


Vector field nonlinearities map an -dimensional feature field which is transforming under the regular representation  of  to a vector field.
As the elements of  correspond to rotations by angles  we can write the action of the cyclic group in this specific case as

and the feature vector as
.
In this convention vector field nonlinearities are defined as

The maximum operation  thereby returns the maximal field value which is invariant under transformations of the regular input field.
Observe that

such that

transforms under the standard representation

of .
This proofs that the resulting feature field indeed transforms like a vector field.

The original paper \cite{Marcos2017-VFN} used a different convention , returning the integer index of the maximal vector entry.
This leads to a corresponding rotation angle  in terms of which the vector field nonlinearity reads









\subsubsection{Equivariance of nonlinearities w.r.t. induced representations}
\label{apx:equivariance_nonlin_induced}

Consider a group  with two representations  and .
Suppose we are given an equivariant non-linearity  with respect to the actions of  and , that is,
.
Then an \emph{induced} non-linearity , equivariant w.r.t. the induced representations  and  of , can be defined as applying  independently to each of the  different -dimensional subspaces of the representation space which are being permuted by the action of , see \apx~\ref{apx:repr_theory}. 
The permutation of the subspaces commutes with the individual action of the nonlinearity  on the subspaces, while the non-linearity  itself commutes with the transformation within the subspaces through  by assumption.
Expressing the feature vector as  this is seen by:


 	
\section{Visualizations of the irrep kernel constraint}
\label{apx:constraint_decomposition}

The irrep kernel constraint

decomposes into independent constraints

on invariant subspaces corresponding to blocks  of .
This is the case since the direct sums of irreps on the right hand side are block diagonal:
-2em]
            \vdots                     & \vdots                     & \ddots\!    \\
        \end{array}
        }
        \!\!\right)
        }_{
        \textstyle\begin{array}{c}
            \kappa(gx)
        \end{array}
    }
    =
    \underbrace{
        \left(\!\!
        {\setlength\arraycolsep{3pt}
         \def\arraystretch{1.5}
        \begin{array}{c|c|c}
            \psi_{i_1}\!(g)\!\! &                     &              \\ \hline
                                & \psi_{i_2}\!(g)\!\! &              \\ \hline \-2em]
            \vdots                    & \vdots                    & \ddots\!     \\
        \end{array}
        }
        \!\!\right)
        }_{
        \textstyle\begin{array}{c}
            \kappa(x)
        \end{array}
    }
        \!\cdot\!
    \underbrace{
        \left(\!\!
        {\setlength\arraycolsep{3pt}
         \def\arraystretch{1.5}
        \begin{array}{c|c|c}
            \psi_{j_1}^{\shortminus1}\!(g)\!\! &                                    &            \\ \hline
                                               & \psi_{j_2}^{\shortminus1}\!(g)\!\! &            \\ \hline \

A basis  for the space of -steerable kernels satisfying the independent constraints~\eqref{eq:irrep_constraint} on  contributes to a part of the full basis

of -steerable kernels satisfying the original constraint~\eqref{eq:kernel_constraint}.
Here we defined a zero-padded block


 	
\section{Solutions of the kernel constraints for irreducible representations}
\label{apx:kernel_constraint_solution_apx}


In this section we are deriving analytical solutions of the kernel constraints

for irreducible representations  of  and its subgroups.
The linearity of the constraint implies that the solution space of G-steerable kernels forms a linear subspace of the unrestricted kernel space

of square integrable functions .

Section~\ref{apx:conventions} introduces the conventions, notation and basic properties used in the derivations.
Since our numerical implementation is on the real field we are considering real-valued irreps.
It is in general possible to derive all solutions considering complex valued irreps of .
While this approach would simplify some steps it 
comes with an overhead of relating the final results back to the real field which leads to further complications, see \apx~\ref{apx:incompleteness_hnets}.
An overview over the real-valued irreps of  and their properties is given in Section~\ref{apx:irreps}.

We present the analytical solutions of the irrep kernel constraints for all possible pairs of irreps in Section~\ref{apx:analytical_irrep_bases}.
Specifically, the solutions for  are given in Table~\ref{tab:SO2_irrep_solution_appendix} while the solutions for , ,  and  are given in
Table~\ref{tab:O2_irrep_solution_appendix},
Table~\ref{tab:Reflection_irrep_solution_appendix},
Table~\ref{tab:CN_irrep_solution_appendix} and
Table~\ref{tab:DN_irrep_solution_appendix} respectively.

Our derivation of the irrep kernel bases is motivated by the observation that the irreps of  and subgroups are harmonics, that is, they are associated to one particular angular frequency.
This suggests that the kernel constraint \eqref{eq:irrep_constraint_apx} decouples into simpler constraints on individual Fourier modes.
In the derivations, presented in Section~\ref{apx:derivation_irrep_constraints}, we are therefore defining the kernels in polar coordinates  and expand them in terms of an orthogonal, angular, Fourier-like basis.
A projection on this orthogonal basis then yields constraints on the expansion coefficients.
Only specific coefficients are allowed to be non-zero; these coefficients parameterize the complete space of -steerable kernels satisfying the irrep constraint~\eqref{eq:irrep_constraint_apx}.
The completeness of the solution follows from the completeness of the orthogonal basis.

We start with deriving the bases for the simplest cases  and  in sections~\ref{apx:derivation_irrep_constraint_SO2} and~\ref{apx:derivation_irrep_constraint_Flip}.
The -steerable kernel basis for  forms a subspace of the kernel basis for  such that it can be easily derived from this solution by adding the additional constraint coming from the reflectional symmetries in .
This additional constraint is imposed in Section~\ref{apx:derivation_irrep_constraint_O2}.
Since  is a subgroup of discrete rotations in  their derivation is mostly similar.
However, the discreteness of rotation angles leads to  systems of linear congruences modulo  in the final step.
This system of equations is solved in Section~\ref{apx:derivation_irrep_constraint_CN}.
Similar to how we derived the kernel basis for  from , we derive the basis for  from  by adding reflectional constraints from  in Section~\ref{apx:derivation_irrep_constraint_DN}.



\subsection{Conventions, notation and basic properties}
\label{apx:conventions}

Throughout this section we denote rotations in  and  by  with  and  respectively.
Since  can be seen as a semidirect product of rotations and reflections we decompose orthogonal group elements into a unique product  where  is a reflection and .
Similarly, we write  for the dihedral group , in this case with .

The action of a rotation  on  in polar coordinates  is given by .
An element  of  or  acts on  as  where the symbol  denotes both group elements in  and numbers in .



We denote a  orthonormal matrix with positive determinant, i.e. rotation matrix for an angle , by:

We define the orthonormal matrix with negative determinant corresponding to a reflection with respect to the horizontal axis as:

and a general orthonormal matrix with negative determinant, i.e. reflection with respect to the axis , as:


Hence, we can express any orthonormal matrix in the form:


where  and .

Moreover, these properties will be useful later:

\begingroup
\addtolength{\jot}{.6em}
-.6em]
		\Leftrightarrow\quad \exists t\in\Z \text{ s.t. }\ \alpha\ &=\ \beta + 2t\pi
5pt]
						 		\phantom{,}
					 		} 
			     		 & \makecell{
				     		 	, \5pt]
	 						 	\phantom{,} \\
 					 	 	} 
						 & \makecell{
							 	, \.75ex]
We first consider the case of 2-dimensional irreps both in the input and in output, that is,  and , where .
This means that the kernel has the form .
To reduce clutter we will from now on suppress the indices  corresponding to the input and output irreps in the input and output fields.

We expand each entry of the kernel  in terms of an (angular) Fourier series\footnote{
For brevity, we suppress that frequency 0 is associated to only half the number of basis elements which does not affect the validity of the derivation.
}

and, for convenience, perform a change of basis to a different, non-sparse, orthogonal basis 
The last four matrices are equal to the first four, except for their opposite frequency.
Moreover, the second matrices of each row are equal to the first matrices, with a phase shift of  added.
Therefore, we can as well write:

\scalebox{.985}{\parbox{\linewidth}{
}}
Notice that the first matrix evaluates to  while the second evaluates to .
Hence, for  we can compactly write:

As already shown in Section \ref{sec:kernel_constraint_solution_main}, we can w.l.o.g. consider the kernels as being defined only on the angular component  by solving only for a specific radial component .
As a result, we consider the basis

of the unrestricted kernel space
which we will constrain in the following by demanding

where we dropped the unrestricted radial part.

We solve for a basis of the subspace satisfying this constraint by projecting both sides on the basis elements defined above.
The inner product on  is hereby defined as

where  denotes the \textit{Frobenius} inner product between 2 matrices.

First consider the projection of the \textit{lhs} of the kernel constraint~\eqref{eq:angular_kernel_constraint_appendix} on a generic basis element .
Defining the operator  by , the projection gives:


Next consider the projection of the \textit{rhs} of Eq.~\eqref{eq:angular_kernel_constraint_appendix}:

	
Finally, we require the two projections to be equal for all rotations in , that is,

or, explicitly, with  and :

Using the property in Eq. \eqref{eq:trick_trigonometry} then implies that for each  in  there exists a  such that:

Since the constraint needs to hold for any  this results in the condition  on the frequencies occurring in the -steerable kernel basis.
Both  and  are left unrestricted such that we end up with the four-dimensional basis

for the angular parts of equivariant kernels for .
This basis is explicitly written out in the lower right cell of Table~\ref{tab:SO2_irrep_solution_appendix}.




~\.75ex]
For the case of 1-dimensional irreps in both the input and output, i.e.  the kernel has the form .
As a scalar function in , it can be expressed by the Fourier decomposition of its angular part:

As before, we can w.l.o.g. drop the dependency on the radial part as it is not restricted by the constraint.
We are therefore considering the basis
-1pt]
			\{0, \pi/2\} &\!\!\text{otherwise}
		\end{cases}
		\ 
	\Bigg\}

	\kappa(\phi + \theta)\ &=\ \psi_m^{\SO2}(r_\theta)\kappa(\phi) \psi_n^{\SO2}(r_\theta)^{-1} \mkern-80mu&\forall\theta,\phi\in[0,2\pi)&\\
	\Leftrightarrow\quad
	\kappa(\phi + \theta)\ &=\ \kappa(\phi) \mkern-80mu&\forall\theta,\phi\in[0,2\pi)&,

	\langle b_{\mu',\gamma'},\ R_{\theta} \kappa \rangle
	&= {1\over2\pi}\int d\phi \ b_{\mu',\gamma'}(\phi) \lp R_{\theta} \kappa\rp(\phi) \\
	&= {1\over2\pi}\int d\phi \ b_{\mu',\gamma'}(\phi) \kappa(\phi + \theta) \\
\intertext{As before we expand the kernel in the linear combination of the basis:}
	&= \sum_{\mu, \gamma} w_{\mu, \gamma} {1\over2\pi}\int d\phi \ b_{\mu',\gamma'}(\phi) \cos(\mu\phi + \mu\theta + \gamma) \\
	&= \sum_{\mu, \gamma} w_{\mu, \gamma} {1\over2\pi}\int d\phi \ \cos(\mu'\phi + \gamma') \cos(\mu\phi + \mu\theta + \gamma) \\
\intertext{With  this results in:}
	&=\sum_{\mu, \gamma} w_{\mu, \gamma} {1\over2\pi}\int d\phi \ {1\over2}\big( \cos(\mu'\phi + \gamma' - \mu\phi-\mu\theta-\gamma) \\
	& \mkern164mu + \cos(\mu'\phi + \gamma' + \mu\phi+\mu\theta+\gamma) \big) \\
&= \sum_{\mu, \gamma} w_{\mu, \gamma} {1\over2}\big( {1\over2\pi}\int d\phi \cos((\mu' -\mu)\phi + (\gamma'-\gamma) - \mu\theta) \\
	& \mkern88mu + {1\over2\pi}\int d\phi  \cos((\mu' + \mu)\phi + (\gamma'+\gamma) + \mu\theta) \big)\\
	&= \sum_{\mu, \gamma} w_{\mu, \gamma} {1\over2}\lp \delta_{\mu, \mu'}\cos((\gamma'-\gamma) - \mu\theta) + \delta_{\mu, -\mu'}\cos((\gamma'+\gamma) + \mu\theta) \rp\\
\intertext{Since  and  imply  this simplifies further to}
	&= {1\over2} \sum_{\gamma} w_{\mu', \gamma} \lp\cos((\gamma'-\gamma) - \mu'\theta) + \delta_{\mu', 0}\cos(\gamma'+\gamma) \rp.

\langle b_{\mu',\gamma'},\ \kappa \rangle
	&\ =\ {1\over2\pi}\int d\phi \ b_{\mu',\gamma'}(\phi) \kappa(\phi) \\
&\ =\ \sum_{\mu, \gamma} w_{\mu, \gamma} {1\over2\pi}\int d\phi \ b_{\mu',\gamma'}(\phi) \cos(\mu\phi + \gamma) \\
	&\ =\ \sum_{\mu, \gamma} w_{\mu, \gamma} {1\over2\pi}\int d\phi \ \cos(\mu'\phi + \gamma') \cos(\mu\phi + \gamma) \\
	&\ =\ {1\over2} \sum_{\gamma} w_{\mu', \gamma} \lp\cos(\gamma'-\gamma) + \delta_{\mu', 0}\cos((\gamma'+\gamma)) \rp

	\langle b_{\mu',\gamma'},  R_{\theta} \kappa \rangle &= \langle b_{\mu',\gamma'},  \kappa \rangle  & \forall\theta\in[0,2\pi)\phantom{,}\!\\
\sum_{\gamma} w_{\mu', \gamma} \lp\cos((\gamma'\!-\!\gamma) \!-\! \mu'\theta) + \delta_{\mu', 0}\cos((\gamma'\!+\!\gamma)) \rp &=
	\sum_{\gamma} w_{\mu', \gamma} \lp\cos(\gamma'\!-\!\gamma) \!+\! \delta_{\mu', 0}\cos((\gamma'\!+\!\gamma)) \rp \span \\
	&&\forall\theta\in[0,2\pi)\phantom{,}\!

	&&\sum_{\gamma} w_{0, \gamma} \lp\cos(-\gamma) + \cos(\gamma) \rp &= \sum_{\gamma} w_{0, \gamma} \lp\cos(-\gamma) + \cos(\gamma) \rp \\
\intertext{As  and :}
	&\Leftrightarrow &w_{0, 0} \lp\cos(0) + \cos(0) \rp &= w_{0, 0} \lp\cos(0) + \cos(0) \rp \\
	&\Leftrightarrow &w_{0,0} &= w_{0,0}

	&&\sum_{\gamma} w_{\mu', \gamma} \cos((\gamma'-\gamma) - \mu'\theta)\ &=\ \sum_{\gamma} w_{\mu', \gamma} \cos(\gamma'-\gamma) &\forall\theta\in[0,2\pi) \\
	&\Leftrightarrow &w_{\mu', 0} \cos(\gamma' \shortminus \mu'\theta) + w_{\mu', {\pi\over2}} \sin(\gamma' \shortminus \mu'\theta)\ &=\ w_{\mu', 0} \cos(\gamma') + w_{\mu', {\pi\over2}} \sin(\gamma') &\forall\theta\in[0,2\pi) \\
&\Leftrightarrow &- \mu'\theta\ &=\ 2t\pi &\forall\theta\in[0,2\pi)&,
[box=\kernelspace]{align}
\label{eq:so2_1x1_basis}
	\mathcal{K}^{\SO2}_{\psi_m\leftarrow\psi_n}\ =\ 
	\big\{ b_{0,0}(\phi) = 1\big\}
-4.ex]
\paragraph{1 and 2-dimensional irreps:}~\
	\kappa(r, \phi)\ =\ \sum_{\mu=0}^{\infty}\ 
	 \ &A_{0,\mu}(r) \begin{bmatrix} \cos(\mu\phi) \\ 0 \end{bmatrix}\ 
	+\  B_{0,\mu}(r) \begin{bmatrix} \sin(\mu\phi) \\ 0 \end{bmatrix}\\
	+\ &A_{1,\mu}(r) \begin{bmatrix} 0 \\ \cos(\mu\phi) \end{bmatrix}\ 
	+\  B_{1,\mu}(r) \begin{bmatrix} 0 \\ \sin(\mu\phi) \end{bmatrix}

	\kappa(r, \phi) \!=\!\!\!\sum_{\mu=-\infty}^{\infty} \sum_{\gamma \in \{0, {\pi\over2}\}} \!\!\!\!\! 
	w_{\gamma,\mu}(r) \! \begin{bmatrix} \cos(\mu\phi + \gamma) \\ \sin(\mu\phi + \gamma) \end{bmatrix}.
\label{eq:so2_1x2_starting_basis}
	\left\{ b_{\mu,\gamma}(\phi) = \begin{bmatrix} \cos(\mu\phi + \gamma) \\ \sin(\mu\phi + \gamma) \end{bmatrix} \ \bigg|\ \ \mu \in \Z,\ \gamma \in \left\{0, {\pi\over2}\right\}\right\}

	\kappa(\phi + \theta)\ &=\ \psi_m^{\SO2}(r_\theta)\kappa(\phi) \psi_0^{\SO2}(r_\theta)^{-1} \mkern-60mu&\forall\theta,\phi\in[0,2\pi)&\\
	\Leftrightarrow\quad
	\kappa(\phi + \theta)\ &=\ \psi_m^{\SO2}(r_\theta)\kappa(\phi) \mkern-60mu&\forall\theta,\phi\in[0,2\pi)&.

	\langle b_{\mu',\gamma'},\ R_{\theta} \kappa \rangle
	&= {1\over2\pi}\int d\phi \ b_{\mu',\gamma'}(\phi)^T \lp R_{\theta} \kappa\rp(\phi) \\
	&= {1\over2\pi}\int d\phi \ b_{\mu',\gamma'}(\phi)^T \kappa(\phi + \theta)\,, \\
\intertext{which, after expanding the kernel in terms of the basis reads:}
	&= \sum_{\mu, \gamma} w_{\mu, \gamma} {1\over2\pi}\int d\phi \ b_{\mu',\gamma'}(\phi)^T 
	\begin{bmatrix} \cos(\mu(\phi + \theta) + \gamma) \\ \sin(\mu(\phi + \theta) + \gamma) \end{bmatrix} \\
	&= \sum_{\mu, \gamma} w_{\mu, \gamma} {1\over2\pi}\int d\phi \ 
	\begin{bmatrix} \cos(\mu'\phi + \gamma') & \sin(\mu'\phi + \gamma') \end{bmatrix}
	\begin{bmatrix} \cos(\mu(\phi + \theta) + \gamma) \\ \sin(\mu(\phi + \theta) + \gamma) \end{bmatrix} \\
&= \sum_{\mu, \gamma} w_{\mu, \gamma} {1\over2\pi}\int d\phi \ 
	\cos((\mu' - \mu) \phi + (\gamma' -\gamma) - \mu\theta). \\
\intertext{As before, the integral is non-zero only if the frequency is , i.e. iff  and thus:}
	&= \sum_{\gamma} w_{\mu', \gamma} \cos((\gamma' -\gamma) - \mu'\theta)

	&\langle b_{\mu',\gamma'},\ \psi_m^{\SO2}(r_\theta)\kappa(\cdot) \rangle \\
	&\qquad\qquad\qquad= {1\over2\pi}\int d\phi \ b_{\mu',\gamma'}(\phi)^T \psi_m(r_\theta) \kappa(\phi) \\
&\qquad\qquad\qquad= \sum_{\mu, \gamma} w_{\mu, \gamma} {1\over2\pi}\int d\phi \ b_{\mu',\gamma'}(\phi)^T 
	\psi_m(r_\theta) \begin{bmatrix} \cos(\mu\phi + \gamma) \\ \sin(\mu\phi + \gamma) \end{bmatrix}\\
	&\qquad\qquad\qquad= \sum_{\mu, \gamma} w_{\mu, \gamma} {1\over2\pi}\int d\phi \ 
	\begin{bmatrix} \cos(\mu'\phi + \gamma') & \sin(\mu'\phi + \gamma') \end{bmatrix}
	\psi_m(r_\theta) \begin{bmatrix} \cos(\mu\phi + \gamma) \\ \sin(\mu\phi + \gamma) \end{bmatrix}\\
	&\qquad\qquad\qquad= \sum_{\mu, \gamma} w_{\mu, \gamma} {1\over2\pi}\int d\phi \ \cos(\mu'\phi + \gamma' -\mu\phi - \gamma -m\theta) \\
	&\qquad\qquad\qquad= \sum_{\mu, \gamma} w_{\mu, \gamma} {1\over2\pi}\int d\phi \ \cos((\mu' - \mu)\phi + \gamma' - \gamma -m\theta)\,. \\
\intertext{The integral is non-zero only if the frequency is , i.e. : }
	&\qquad\qquad\qquad= \sum_{\gamma} w_{\mu', \gamma} \cos(\gamma'-\gamma -m\theta)

	&&\langle b_{\mu',\gamma'},  R_{\theta} \kappa \rangle &= \langle b_{\mu',\gamma'},  \psi_m(r_\theta)\kappa(\cdot) \rangle &\mkern-50mu\forall\theta\in[0,2\pi)\\
	&\Leftrightarrow &\sum_{\gamma} w_{\mu', \gamma} \cos(\gamma'-\gamma - \mu'\theta) &=
		\sum_{\gamma} w_{\mu', \gamma} \cos(\gamma'-\gamma -m\theta) &\mkern-50mu\forall\theta\in[0,2\pi)\\
	&\Leftrightarrow\!&w_{\mu', 0} \cos(\!\gamma' \!-\! \mu'\theta) + w_{\mu', {\pi\over2}} \sin(\!\gamma' \!-\! \mu'\theta) &=
		w_{\mu', 0} \cos(\gamma' \!-\! m\theta) + w_{\mu', {\pi\over2}} \sin(\gamma' \!-\!m\theta) \\
		&&&&\mkern-50mu\forall\theta\in[0,2\pi)\\
	&\Leftrightarrow &\gamma' - \mu'\theta &= \gamma' - m\theta +2t\pi &\mkern-50mu\forall\theta\in[0,2\pi)\\
	&\Leftrightarrow &\mu'\theta &= m\theta +2t\pi &\mkern-50mu\forall\theta\in[0,2\pi)&,
[box=\kernelspace]{align}
\label{eq:so2_2x1_basis}
	\mathcal{K}^{\SO2}_{\psi_m\leftarrow\psi_n}\ =\ 
	\left\{ b_{m,\gamma}(\phi) = \begin{bmatrix} \cos(m\phi + \gamma)\\ \sin(m\phi + \gamma) \end{bmatrix} \ \bigg|\ \gamma \in \left\{0, {\pi\over2}\right\}\right\}
-4.ex]
\paragraph{2 and 1-dimensional irreps:}~box=\kernelspace]{align}
\label{eq:o2_2x1_basis}
	\mathcal{K}^{\O2}_{\psi_{i,m}\leftarrow\psi_{j,n}}\ =\
	\left\{ b_{\mu,i{\pi\over2}}(\phi) = \begin{bmatrix} \cos(\mu\phi + i{\pi\over2}) & \sin(\mu\phi + i{\pi\over2}) \end{bmatrix} \ \bigg|\ \ \mu = n \right\},
.75ex]
During the derivation of the solutions for 's 2-dimensional irreps in \apx \ref{apx:derivation_irrep_constraint_SO2}, we assumed continuous angles only in the very last step.
The constraint in Eq. \eqref{eq:so2_2x2_solution} therefore holds for  as well.
Specifically, it demands that for each  there exists a  such that:

The last result corresponds to a system of  linear congruence equations modulo  which require  to divide  for each non-negative integer  smaller than .
Note that solutions of the constraint for  already satisfy the constraints for  such that it is sufficient to consider

The resulting basis

for  thus coincides mostly with the basis~\ref{eq:so2_2x2_basis} for  but contains solutions for aliased frequencies, defined by adding .
The bottom right cell in Table~\ref{tab:CN_irrep_solution_appendix} gives the explicit form of this basis.

~\.75ex]
The same trick could be applied to solve the remaining three cases.
However, since  has an additional one dimensional irrep of frequency  for even  it is convenient to rederive all cases.
We therefore consider  and , where .
Note that  for .

We use the same Fourier basis 
-1pt]
            \{0, \pi/2\} &\!\!\text{otherwise}
        \end{cases}
        \ 
    \Bigg\}

	& \langle b_{\mu',\gamma'},  R_{\theta} \kappa \rangle
	= {1\over2} \sum_{\gamma} w_{\mu', \gamma} \lp\cos((\gamma'-\gamma) - \mu'\theta) + \delta_{\mu', 0}\cos(\gamma'+\gamma) \rp &&

	&\langle b_{\mu',\gamma'},  \psi_m^{\CN}(r_\theta)\kappa\ \psi_n^{\CN}(r_\theta)^{-1} \rangle && \\
	&\qquad\qquad= {1\over2\pi}\int d\phi \ b_{\mu',\gamma'}(\phi) \psi_m^{\CN}(r_\theta)\kappa(\phi)\psi_n^{\CN}(r_\theta)^{-1} \,, && \\
\intertext{which by expanding the kernel in the linear combination of the basis and writing the respresentations out yields:}
	&\qquad\qquad= \sum_{\mu, \gamma} w_{\mu, \gamma} {1\over2\pi}\int d\phi \ b_{\mu',\gamma'}(\phi) \cos(m\theta) b_{\mu,\gamma}(\phi) \cos(n\theta)^{-1} && \\
	&\qquad\qquad= \sum_{\mu, \gamma} w_{\mu, \gamma} {1\over2\pi}\int d\phi \ \cos(\mu'\phi + \gamma') \cos(m\theta) \cos(\mu\phi + \gamma) \cos(n\theta)^{-1} && \\
\intertext{Since  the inverses can be dropped and terms can be collected via trigonometric identities:}
	&\qquad\qquad= \sum_{\mu, \gamma} w_{\mu, \gamma} {1\over2\pi}\int d\phi \ \cos(\mu'\phi + \gamma') \cos(m\theta) \cos(\mu\phi + \gamma) \cos(n\theta) && \\
	&\qquad\qquad= \sum_{\mu, \gamma} w_{\mu, \gamma} \cos(m\theta) \cos(n\theta) {1\over2\pi}\int d\phi \ \cos(\mu'\phi + \gamma')  \cos(\mu\phi + \gamma) && \\
	&\qquad\qquad= \sum_{\mu, \gamma} w_{\mu, \gamma} \cos((\pm m \pm n) \theta) {1\over4\pi}\!\! \int\!\! d\phi  \Big(\!\! \cos(\!(\mu'\!\shortminus \mu)\phi + \gamma'\!\shortminus\gamma) + \cos((\mu' + \mu)\phi + \gamma'\!+\gamma) \!\Big) && \\
	&\qquad\qquad= \sum_{\mu, \gamma} w_{\mu, \gamma} \cos((\pm m \pm n) \theta) {1\over2} \lp \delta_{\mu, \mu'}\cos(\gamma'-\gamma) + \delta_{\mu+\mu', 0}\cos(\gamma'+\gamma) \rp && \\
	&\qquad\qquad= {1\over2} \sum_{\mu', \gamma} w_{\mu', \gamma} \cos((\pm m \pm n) \theta) \lp \cos(\gamma'-\gamma) + \delta_{\mu', 0}\cos(\gamma'+\gamma) \rp && 

    && \qquad\qquad\qquad\qquad\qquad \langle b_{\mu'\!,\gamma'},  R_{\theta} \kappa \rangle 
    & = \Big\langle b_{\mu'\!,\gamma'}, \psi_m^{\CN}(r_\theta)\kappa\ \psi_n^{\CN}(r_\theta)^{-1} \Big\rangle && \\
&\Leftrightarrow &\sum_{\gamma} w_{\mu', \gamma} \lp\cos((\gamma' - \gamma) -  \mu'\theta) + \delta_{\mu', 0}\cos(\gamma'+\gamma) \rp = \qquad\qquad\qquad\qquad\qquad\qquad \span&&\\
&& \span \qquad\qquad\qquad\qquad\qquad = \sum_{\mu', \gamma} w_{\mu', \gamma} \cos((\pm m \pm n) \theta) \Big( \cos(\gamma' -\gamma) + \delta_{\mu', 0}\cos(\gamma'+\gamma) \Big)&&

			&				 & \span\span \sum_{\gamma} \!w_{0, \gamma} \!\lp\cos(-\gamma)  \!+\! \cos(\gamma)\rp
			& = \cos(\!(\!\pm m\! \pm\! n) \theta) \sum_{\gamma} \!w_{0, \gamma}  \Big(\! \cos(-\gamma) \!+\! \cos(\gamma) \!\Big) \\
			&\Leftrightarrow & \span\span w_{0, 0} 2 \cos(0) + w_{0, {\pi\over2}} 0 
			& = \cos((\pm m \pm n) \theta) \lp w_{0, 0} 2 \cos(0) + w_{0, {\pi\over2}} 0 \rp \\
			&\Leftrightarrow & \span\span w_{0, 0} &= \cos((\pm m \pm n) \theta) w_{0, 0} \\
			\intertext{If , the coefficient  is forced to .
				Conversely:
			}
			&                & \span\span 										\cos((\pm m \pm n)\theta) &= 1 \\
		    &\Leftrightarrow & \exists t \in \Z \text{ s.t. } \quad\quad &&			 (\pm m \pm n )\theta &= 2t\pi \\
\intertext{Using :}
		    &\Leftrightarrow & \exists t \in \Z \text{ s.t. } \quad\quad && (\pm m \pm n )p {2\pi\over N} &= 2t\pi \\
		    &\Leftrightarrow & \exists t \in \Z \text{ s.t. } \quad\quad &&				   (\pm m \pm n)p &= tN
		
			&				 & \span\span
				\sum_{\gamma} w_{\mu', \gamma} \cos(\gamma'-\gamma - \mu'\theta) &=
				\cos((\pm m \pm n) \theta) \sum_{\gamma} w_{\mu', \gamma} \cos(\gamma'-\gamma) \\
			&\Leftrightarrow & \span\span
				w_{\mu', 0} \cos(\gamma'- \mu'\theta) + w_{\mu', {\pi\over2}} \sin(\gamma'- \mu'\theta) = \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \span \\
			&& \span\span\span
				\qquad\qquad \cos((\pm m \pm n) \theta) \lp w_{\mu', 0} \cos(\gamma') + w_{\mu', {\pi\over2}} \sin(\gamma') \rp\\
			\intertext{Since  we have , therefore:}
			&\Leftrightarrow & \span\span
w_{\mu', 0} \cos(\gamma'  \shortminus   \mu'\theta) +w_{\mu', {\pi\over2}} \sin(\gamma' \shortminus \mu'\theta) = \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \span \\
			&& \span\span
				\span \qquad\qquad = w_{\mu', 0} \cos(\gamma'+ (\pm m  \pm  n) \theta) + w_{\mu', {\pi\over2}} \sin(\gamma' + (\pm m  \pm  n) \theta)\\
			\intertext{Using the property in Eq. \eqref{eq:trick_trigonometry}:}
			&\Leftrightarrow & \exists t \in \Z \text{ s.t. } \quad\quad &&
				\gamma' - \mu'\theta &= \gamma' + (\pm m \pm n)\theta +2t\pi\\
			&\Leftrightarrow & \exists t \in \Z \text{ s.t. } \quad\quad &&
				\mu'\theta &= (\pm m \pm n)\theta +2t\pi\\
			\intertext{Using :}
			&\Leftrightarrow & \exists t \in \Z \text{ s.t. } \quad\quad &&
				\mu'p {2\pi\over N} &= (\pm m \pm n)p {2\pi\over N} +2t\pi\\
			&\Leftrightarrow & \exists t \in \Z \text{ s.t. } \quad\quad &&
				\mu'p &= (\pm m \pm n)p + tN\\
			&\Leftrightarrow & \exists t \in \Z \text{ s.t. } \quad\quad &&
				(\pm m \pm n + \mu') p &= tN\\

	\forall p\in\{0, 1, \dots, N-1\}\ \ \exists t \in \Z \text{ s.t. } \quad (\pm m\pm n+\mu') p = tN \,.
[box=\kernelspace]{align}
\label{eq:cn_1x1_basis}
    \mathcal{K}^{\CN}_{\psi_{m}\leftarrow\psi_{n}}\!\! =\!
    \left\{\!b_{\mu,\gamma}(\phi)\! =\! \cos(\mu\phi\! +\! \gamma) \bigg| \mu\!=\!(m\! +\! n \!\!\! \mod\! N)\!+\! tN, \gamma\! \in\! \left\{0, {\pi\over2}\right\}\!, \mu\! \neq\! 0 \! \vee\! \gamma\! =\! 0 \right\}_{t \in \N} \!\!\!\!\!

	\langle b_{\mu',\gamma'},  R_{\theta} \kappa \rangle
	&= \sum_{\gamma} w_{\mu', \gamma} \cos((\gamma' -\gamma) - \mu'\theta)

	&\langle b_{\mu',\gamma'},  \psi_m^{\CN}(r_\theta)\kappa(\cdot) \psi_n^{\CN}(r_\theta)^{-1}\rangle \\
	&\qquad\qquad= {1\over2\pi}\int d\phi \ b_{\mu',\gamma'}(\phi)^T \psi_m^{\CN}(r_\theta) \kappa(\phi) \psi_n^{\CN}(r_\theta)^{-1}\\
&\qquad\qquad= \sum_{\mu, \gamma} w_{\mu, \gamma} {1\over2\pi}\int d\phi \ b_{\mu',\gamma'}(\phi)^T 
	\psi_m^{\CN}(r_\theta) \begin{bmatrix} \cos(\mu\phi + \gamma) \\ \sin(\mu\phi + \gamma) \end{bmatrix} \psi_n^{\CN}(r_\theta)^{-1}\\
	&\qquad\qquad= \sum_{\mu, \gamma} w_{\mu, \gamma} {1\over2\pi}\!\int\!\! d\phi 
	\begin{bmatrix} \cos(\mu'\phi + \gamma') & \sin(\mu'\phi + \gamma') \end{bmatrix}
	\psi_m^{\CN}(r_\theta)\! \begin{bmatrix} \cos(\mu\phi + \gamma) \\ \sin(\mu\phi + \gamma) \end{bmatrix}\! \psi_n^{\CN}(r_\theta)^{\shortminus1}\\
	&\qquad\qquad= \sum_{\mu, \gamma} w_{\mu, \gamma} \lp {1\over2\pi}\int d\phi \ \cos(\mu'\phi + \gamma' -\mu\phi - \gamma -m\theta) \rp \psi_n^{\CN}(r_\theta)^{-1} \,. \\
\intertext{The integral is non-zero only if the frequency is , i.e. iff : }
	&\qquad\qquad= \sum_{\gamma} w_{\mu', \gamma} \cos(\gamma'-\gamma -m\theta) \psi_n^{\CN}(r_\theta)^{-1} \\
	&\qquad\qquad= \sum_{\gamma} w_{\mu', \gamma} \cos(\gamma'-\gamma -m\theta) \cos(\pm n \theta) \\
\intertext{Since  for some  one has  which allows to add the following zero summand and simplify:}
	&\qquad\qquad= \sum_{\gamma} w_{\mu', \gamma} \lp \cos(\gamma'-\gamma -m\theta) \cos(\pm n \theta) - \sin(\gamma'-\gamma -m\theta) \sin(\pm n \theta) \rp\\
	&\qquad\qquad= \sum_{\gamma} w_{\mu', \gamma} \cos(\gamma'-\gamma -(m \pm n) \theta)

&&
	\langle b_{\mu',\gamma'},  R_{\theta} \kappa \rangle &= \langle b_{\mu',\gamma'},  \psi_m^{\CN}(r_\theta)\kappa(\cdot)\psi_n^{\CN}(r_\theta)^{-1} \rangle && \forall \theta \in  \left\{ p {2\pi\over N} \right\}\\
	\Leftrightarrow\quad&&
	\sum_{\gamma} w_{\mu', \gamma} \cos(\gamma'-\gamma - \mu'\theta) &=
	\sum_{\gamma} w_{\mu', \gamma} \cos(\gamma'-\gamma -(m \pm n) \theta) && \forall \theta \in  \left\{ p {2\pi\over N} \right\} \\
	\Leftrightarrow\quad&&
	w_{\mu'\!,\mkern-1mu 0} \cos(\gamma' \!\shortminus \!\mu'\theta) \!+ \!w_{\mu'\!,\mkern-2mu{\pi\over2}} \sin(\gamma' \!\shortminus \!\mu'\theta) &=
	w_{\mu'\!,\mkern-1mu 0} \cos(\gamma' \!\shortminus \!(m \!\pm \!n)\theta) \!+\! w_{\mu'\!,\mkern-2mu{\pi\over2}} \sin(\gamma' \!\shortminus\!(m \!\pm \!n)\theta) 
	\span\span
	\\
	&&
	& && \forall \theta \in  \left\{ p {2\pi\over N} \right\}\\
\intertext{Using the property in Eq. \eqref{eq:trick_trigonometry}, this requires that for each  there exists a  such that:}
	\Leftrightarrow\quad&&
	\gamma' - \mu'\theta &= \gamma' - (m \pm n)\theta +2t\pi && \forall \theta \in  \left\{ p {2\pi\over N} \right\} \\
	\Leftrightarrow\quad&&
	\mu'\theta &= (m \pm n)\theta +2t\pi && \forall \theta \in  \left\{ p {2\pi\over N} \right\} \\
\intertext{Since  with  we find that}
	\Leftrightarrow\quad&&
	\mu'p{2\pi\over N} &= (m \pm n) p{2\pi\over N} +2t\pi && \mkern-45mu\forall p \in \{0, \dots, N\!\shortminus\!1\} \\
	\Leftrightarrow\quad&&
	\mu'p &= (m \pm n) p + tN && \mkern-45mu\forall p \in \{0, \dots, N\!\shortminus\!1\} \\
\Leftrightarrow\quad&&
	\mu' &= (m \pm n) + tN \\
	\Leftrightarrow\quad&&
	\mu' - (m \pm n) &= tN \,,
[box=\kernelspace]{align}
\label{eq:cn_2x1_basis}
	\mathcal{K}^{\CN}_{\psi_{m}\leftarrow\psi_{n}}\ =\
	\left\{ b_{\mu,\gamma}(\phi) = \begin{bmatrix} \cos(\mu\phi + \gamma)\\ \sin(\mu\phi + \gamma) \end{bmatrix} \ \bigg|\ \ \mu = (m \pm n) + tN,\ \gamma \in \left\{0, {\pi\over2}\right\}\right\}_{t \in \Z}
-4.ex]
\paragraph{2 and 1-dimensional irreps:}~box=\kernelspace]{align}
\label{eq:dn_2x2_basis}
    \mathcal{K}^{\DN}_{\psi_{i,m}\leftarrow\psi_{j,n}}\ =\
    \left\{ b_{\mu,0,s}(\phi) = \psi(\mu\phi) \xi(s) \ \bigg|\ \ \mu = m - sn +tN, s \in \{\pm 1\} \right\}_{t \in \Z}
-4.ex]
\paragraph{1-dimensional irreps:}~box=\kernelspace]{align}
\label{eq:dn_2x1_basis}
    \mathcal{K}^{\DN}_{\psi_{i,m}\leftarrow\psi_{j,n}}\ =\
    \left\{ b_{\mu,\gamma}(\phi) = \begin{bmatrix} \cos(\mu\phi + \gamma)\\ \sin(\mu\phi + \gamma) \end{bmatrix} \ \bigg|\ \ \mu = (m \pm n) + tN,\ \gamma = j {\pi\over2}\right\}_{t \in \Z}
-4.ex]
\paragraph{2 and 1-dimensional irreps:}~box=\kernelspace]{align}
\label{eq:dn_1x2_basis}
    \mathcal{K}^{\DN}_{\psi_{i,m}\leftarrow\psi_{j,n}}\! =\!
    \left\{\! b_{\mu,\gamma}(\phi) = \begin{bmatrix} \cos(\mu\phi + \gamma) \!&\! \sin(\mu\phi + \gamma) \end{bmatrix} \bigg|\ \mu = (\pm m + n) + tN,\ \gamma = i {\pi\over2}\!\right\}_{t \in \Z}\!\!\!\!\!\!\!
-3ex]
Table~\ref{tab:DN_irrep_solution_appendix} shows these solutions in its top right cells.

 

\subsubsection{Kernel constraints at the origin}
\label{apx:stabilizer_solution}

Our derivations rely on the fact that the kernel constraints restrict only the angular parts of the unconstrained kernel space  which suggests an independent solution for each radius .
Particular attention is required for kernels defined at the origin, i.e. when .
The reason for this is that we are using polar coordinates  which are ambiguous at the origin where the angle is not defined.
In order to stay consistent with the solutions for  we still define the kernel at the origin as an element of .
However, since the coordinates  map to the same point for all , we need to demand the kernels to be angularly constant, that is, .
This additional constraint restricts the angular Fourier bases used in the previous derivations to zero frequencies only.
Apart from this, the kernel constraints are the same for  and  which implies that the G-steerable kernel bases at  are given by restricting the bases derived in
\ref{apx:derivation_irrep_constraint_SO2},
\ref{apx:derivation_irrep_constraint_Flip},
\ref{apx:derivation_irrep_constraint_O2},
\ref{apx:derivation_irrep_constraint_CN} and
\ref{apx:derivation_irrep_constraint_DN}
to the elements indexed by frequencies .




\subsection{Complex valued representations and Harmonic Networks}
\label{apx:incompleteness_hnets}


Instead of considering real (irreducible) representations we could have derived all results using complex representations, acting on complex feature maps.
For the case of  and  this would essentially not affect the derivations since their complex and real irreps are equivalent, that is, they coincide up to a change of basis.
Conversely, all complex irreps of  and  are 1-dimensional which simplifies the derivations in complex space.
However, the solution spaces of complex G-steerable kernels need to be translated back to a real valued implementation.
This translation has some not immediately obvious pitfalls which can lead to an underparameterized implementation in real space.
In particular, Harmonic Networks~\cite{Worrall2017-HNET} were derived with a complete solution in complex space; however, their real valued implementation is using a -steerable kernel space of half the dimensionality as ours.
We will in the following explain why this is the case.

In the complex field, the irreps of  are given by  with frequencies .
Notice that these complex irreps are indexed by positive and negative frequencies while their real counterparts, defined in \apx~\ref{apx:irreps}, only involve non-negative frequencies.
As in~\cite{Worrall2017-HNET} we consider complex feature fields  which are transforming according to complex irreps of .
A complex input  field  of type  is mapped to
a complex output field  of type 
via the cross-correlation

with a complex filter .
The (angular part of the) complete space of equivariant kernels between  and  was in~\cite{Worrall2017-HNET} proven to be parameterized by

where  is a complex weight which scales and phase-shifts the complex exponential.
We want to point out that an equivalent parametrization is given in terms of the real and imaginary parts  and  of the weight , i.e.

The real valued implementation of Harmonic Networks models the complex feature fields  of type  by splitting them in two real valued channels  which contain their real and imaginary part.
The action of the complex irrep  is modeled accordingly by a rotation matrix of the same, potentially negative\footnote{This establishes an isomorphism between  and  depending on the sign of .}
frequency.
A real valued implementation of the cross-correlation~\eqref{eq:complex_cross_correlation} is built using a real kernel  as specified by

The complex steerable kernel~\eqref{eq:hnet_basis} is then given by

While this implementation models the \textit{complex} Harmonic Networks faithfully in real space, it does not utilize the complete -steerable kernel space when the real feature fields are interpreted as fields transforming under the real irreps  as done in our work.
More specifically, the kernel space used in~\eqref{eq:complex_solutions_implementation} is only 2-dimensional while our basis~\eqref{eq:so2_2x2_basis} for the same case is 4-dimensional.
The additional solutions with frequency  are missing.

The lower dimensionality of the complex solution space can be understood by analyzing the relationship between 's real and complex irreps.
On the complex field, the real irreps become reducible and decomposes into two 1-dimensional complex irreps with opposite frequencies:

Indeed,  has only half as many real irreps as complex ones since positive and negative frequencies are conjugated to each other, i.e. they are equivalent up to a change of basis: .
It follows that a real valued implementation of a complex  fields as a 2-dimensional  fields implicitly adds a complex  field.
The intertwiners between two real fields of type  and  therefore do not only include the single complex intertwiner between complex fields of type  and , but four complex intertwiners between fields of type  and .
The real parts of these intertwiners correspond to our four dimensional solution space.

In conclusion,~\cite{Worrall2017-HNET} indeed found the complete solution on the complex field.
However, by implementing the network on the real field, negative frequencies are implicitly added to the feature fields which allows for our larger basis~\eqref{eq:so2_2x2_basis} of steerable kernels to be used without adding an overhead.

  	
\section{Alternative approaches to compute kernel bases and their complexities}
\label{apx:comparison_SE3Nets}

The main challenge of building steerable CNNs is to find the space of solutions of the kernel space constraint in Eq.~\ref{eq:kernel_constraint}.
Several recent works tackle this problem for the very specific case of features which transform under \textit{irreducible} representations of .
The strategy followed in~\cite{TensorFieldNets,Kondor2018-NBN,kondorClebschGordanNets2018,anderson2019cormorant} is based on well known analytical solutions and does not generalize to arbitrary representations.
In contrast,~\cite{3d_steerableCNNs} present a numerical algorithm to solve the kernel space constraint.
While this algorithm was only applied to solve the constraints for irreps, it generalizes to arbitrary representations.
However, the computational complexity of the algorithm scales unfavorably in comparison to the approach proposed in this work.
We will in the following review the kernel space solution algorithm of~\cite{3d_steerableCNNs} for general representations and discuss its complexity in comparison to our approach.

The algorithm proposed in~\cite{3d_steerableCNNs} is considering the same kernel space constraint

as in this work.
By vectorizing the kernel the constraint can be brought in the form

where the second step assumes the input representation to be unitary, that is, to satisfy .
A Clebsch-Gordan decomposition, i.e. a decomposition of the tensor product representation into a direct sum of irreps  of , then yields\footnote{For the irreps of  it is well known that  and .}

Through a change of variables  this simplifies to

which, in turn, decomposes into  independent constraints

Each of these constraints can be solved independently to find a basis for each .
The kernel basis is then found by inverting the change of basis and the vectorization, i.e. by computing .

For the case that  and  are Wigner D-matrices, i.e. irreps of , the change of basis  is given by the Clebsch-Gordan coefficients of .
These well known solutions were used in~\cite{TensorFieldNets,Kondor2018-NBN,kondorClebschGordanNets2018,anderson2019cormorant} to build the basis of steerable kernels.
Conversely, the authors of~\cite{3d_steerableCNNs} solve for the change of basis  numerically.
Given \textit{arbitrary} unitary representations  and  the numerical algorithm solves for the change of basis in

This linear constraint on , which is a specific instance of the Sylvester equation, can be solved by vectorizing , i.e.

where  is the identity matrix on ~and~
In principle there is one Sylvester equation for each group element , however, it is sufficient to consider the \textit{generators} of  only, since the solutions found for the generators will automatically hold for all group elements.
One can therefore stack the matrices

for the generators of  into a bigger matrix and solve for  as the null space of this stacked matrix.
The linearly independent solutions  in the null space correspond to the Clebsch-Gordan coefficients for .

This approach does not rely on the analytical Clebsch-Gordan coefficients, which are only known for specific groups and representations, and therefore works for any choice of representations.
However, applying it naively to large representations can be extremely expensive.
Specifically, computing the null space to solve the (stacked) Sylvester equation for  generators of  via a \textit{SVD}, as done in~\cite{3d_steerableCNNs}, scales as .
This is the case since the matrix which is multiplying  is of shape
.
Moreover, the change of basis matrix  itself has shape

which implies that the change of variables\footnote{No inversion from  to  is necessary if the Sylvester equation is solved directly for .}
from  to  has complexity .
In~\cite{3d_steerableCNNs} the authors only use irreducible representations which are relatively small such that the bad complexity of the algorithm is negligible.

In comparison, the algorithm proposed in this work is based on an \textit{individual} decomposition of the representations  and  into irreps and leverages the analytically derived kernel space solutions between the irreps of .
The independent decomposition of the input and output representations leads to a complexity of only
.
We further apply the input and output changes of basis  and  independently to the irreps kernel solutions  which leads to a complexity of 
.
The improved complexity of our implementation makes working with large representations as used in this work, for instance , possible.

 	
\section{Additional information on the training setup}
\label{apx:training_setup}

\begin{figure}[H]
    \begin{minipage}{\linewidth}
        \centering
        \begin{minipage}{0.45\linewidth}
            \centering
            \begin{table}[H]
                \centering
                \scalebox{.85}{
                    \begin{tabular}{lr}
                        \toprule
                        layer                           & output fields \\
                        \midrule
                        conv block   (pad 1)  &            16 \\
                        conv block   (pad 2)  &            24 \\
                        max pooling           &            24 \\
                        conv block   (pad 2)  &            32 \\
                        conv block   (pad 2)  &            32 \\
                        max pooling           &            32 \\
                        conv block   (pad 2)  &            48 \\
                        conv block            &            64 \\
                        invariant projection            &            64 \\
                        global average pooling          &            64 \\
                        fully connected                 &            64 \\
                        fully connected + softmax       &            10 \\
                        \bottomrule
                    \end{tabular}
                    
                }
                \vspace*{2pt}
                ~\\~
                \caption{
                    Basic model architecture from which all models for the MNIST benchmarks in Tables~\ref{tab:mnist_comparison} and \ref{tab:mnist_restriction} are being derived.
                    Each convolution block includes a convolution layer, batch-normalization and a nonlinearity.
                    The first fully connected layer is followed by batch-normalization and ELU.
                    The width of each layer is expressed as the number of fields of a regular  model with approximately the same number of parameters.
                }
                \label{tab:small_architecture}
\end{table} 
        \end{minipage}
        \hfill
        \hspace{0.05\linewidth}
        \begin{minipage}{0.45\linewidth}
            \begin{table}[H]
            \centering
                \scalebox{.85}{
                    \begin{tabular}{lr}
                        \toprule
                        layer                           &  output fields\\
                        \midrule
                        conv block            &            24 \\
                        conv block   (pad 3)  &            32 \\
                        max pooling           &            32 \\
                        conv block   (pad 3)  &            36 \\
                        conv block   (pad 3)  &            36 \\
                        max pooling           &            36 \\
                        conv block   (pad 3)  &            64 \\
                        conv block            &            96 \\
                        invariant projection            &            96 \\
                        global average pooling          &            96 \\
                        fully connected                 &            96 \\
                        fully connected                 &            96 \\
                        fully connected + softmax       &            10 \\
                        \bottomrule
                    \end{tabular}
                    
                }
                \vspace*{4pt}
                \caption{
                    Model architecture for the final MNIST-rot experiments (replicated from \cite{Weiler2018-STEERABLE}).
                    Each fully connected layer follows a dropout layer with ; the first two fully connected layers are followed by batch normalization and ELU.
                    The width of each layer is expressed in terms of regular feature fields of a  model.
                }
                \vspace*{10pt}
                \label{tab:large_architecture}
            \end{table}  
        \end{minipage}
    \end{minipage}
    \vspace*{-12pt}
\end{figure}


\subsection{Benchmarking on transformed MNIST datasets}
\label{apx:mnist_benchmark_training}

Each model reported in Sections~\ref{sec:mnist_benchmark}, \ref{sec:mnist_restriction} and~\ref{sec:mnist_rot_convergence} is derived from the architecture reported in Table~\ref{tab:small_architecture}.
The width of each model's layers is thereby scaled such that the total number of parameters is matched and the relative width of layers coincides with that reported in Table\ref{tab:small_architecture}.
Training is performed with a batch size of 64 samples, using the \emph{Adam} optimizer.
The learning rate is initialized to  and decayed exponentially by a factor of  per epoch, starting after a burn in phase of 10 epochs.
We train each model for 30 epochs and test the model which performed best on the validation set.
A weight decay of  is being used for all convolutional layers and the first fully connected layer.
In all experiments, we build steerable bases with Gaussian radial profiles of width  for all except the outermost ring where we use .
We apply a strong bandlimiting policy which permits frequencies up to  for radii  in a  kernel and up to  for radii  in a  kernel.
The strong cutoff in the rings of maximal radius is motivated by our empirical observation that these rings introduce a relatively high equivariance error for higher frequencies.
This is the case since the outermost ring ranges out of the sampled kernel support.
During training, data augmentation with continuous rotations and reflections is performed (if these are present in the dataset) to not disadvantage non-equivariant models.
In the models using group restriction, the restriction operation is applied after the convolution layers but before batch normalization and non-linearities.




\subsection{Competitive runs on MNIST rot}
\label{apx:mnist_final}

In Table \ref{tab:mnist_final} we report the performances of some of our best models.
Our experiments are based on the best performing, -equivariant model of \cite{Weiler2018-STEERABLE} which defined the state of the art on rotated MNIST at the time of writing.
We replicate their model architecture, summarized in Table~\ref{tab:large_architecture}, though our models have a different frequency bandlimit and width  for the Gaussian radial profiles as discussed in the previous subsection.
As before, the table reports the width of each layer in terms of number of fields in the  regular model.

As commonly done, we train our final models on the 10000 + 2000 training and validation samples.
Training is performed for 40 epochs with an initial learning rate , which is being decayed by a factor of , starting after  epochs.
As before, we use the \textit{Adam} optimizer with a batch size of 64, this time using L1 and L2 regularization with a weight of .
The fully connected layers are additionally regularized using dropout with a probability of .
We are again using train time augmentation.





\subsection{CIFAR experiments}
\label{apx:cifar}

The equivariant models used in the experiments on CIFAR-10 and CIFAR-100 are adapted from the original WideResNet models by replacing conventional with -steerable convolutions and scaling the number of feature fields such that the total number of parameters is preserved.
For blocks which are equivariant under  or  we use  kernels instead of  kernels to allow for higher frequencies. 
All models use regular feature fields in all but the final convolution layer, which maps to a scalar field (conv2triv) to produce invariant predictions.
We use a frequency cut-off of  times the ring's radius, e.g.  for rings of radii .
These higher bandlimits in comparison to the MNIST experiments are motivated by the fact that the corresponding bases introduce small discretization errors, which is no problem for the classification of natural images.
In the contrary, this leads to the models having a strong bias towards being equivariant, but might allow them to break equivariance if necessary.
The widths of the bases' rings is chosen to be  in all rings.

The training process is the same as used for WideResNets: we train for 200 epochs with a batch size of 128. 
We optimize the model with SGD, using an initial learning rate of , momentum  and a weight decay of .
The learning rate is decayed by a factor of  every 60 epochs.
We perform a standard data augmentation with random crops, horizontal flips and normalization.
No CutOut is done during the normal experiments but it is used in the AutoAugment policies.



\subsection{STL-10 experiments}
\label{apx:stl10}

The models for our STL-10 experiments are adapted from~\cite{cutout}.
However, according to an issue\footnote{\url{https://github.com/uoguelph-mlrg/Cutout/issues/2}}
in the authors' GitHub repository, the publication states some model parameters and the training setup wrongly.
Our adaptations are therefore based on the setting reported on~GitHub.
Specifically, we use patches of  pixels for cutout and the stride of the first convolution layer in the first block is 2 instead of 1.
Moreover, we normalize input features using CIFAR-10 statistics.
Though these statistics are very close to the statistics of STL-10, they might, as the authors of~\cite{cutout} suggest, cause non-negligible changes in performance because of the small training set size of STL-10.

As before, regular feature fields are used throughout the whole model except for the last convolution layer which maps to trivial fields.
In the small model, which does not preserve the number of parameters but the number of channels, we still scale up the number of output channels of the very first convolution layer (before the first residual block).
As the first convolution layer originally has  output channels and our model is initially equivariant to  (whose regular representation spans  channels), the first convolution layer would only be able to learn 1 single independent filter (repeated  times, rotated and reflected).
Hence, we increase the number of output channels of the first convolution layer by the square root of the group size () leading to  channels, i.e.  regular fields.
We use a ring width of  for the kernel basis except for the outermost ring where we use  and use a frequency cut-off factor of  for the rings' radii, i.e. cutoffs of .

We are again exactly replicating the training process as reported in the publication \cite{cutout}.
Only the labeled subset of the training set is used, that is, the  unlabeled training images are discarded.
Training is performed for 1000 epochs with a batch size of 128, using SGD with Nesterov momentum of 0.9 and weight decay of .
The learning rate is initialized to  and decayed by a factor of  at , ,  and  epochs.
During training, we perform data augmentation by zero-padding with  pixels and randomly cropping patches of  pixels, mirroring them horizontally and applying CutOut.

In the data ablation study, reported in Figure~\ref{fig:stl10_ablation}, we use the same models and training procedure as in the main experiment on the full STL-10 dataset.
For every single run, we generate new datasets by mixing the original training, validation and test set and sample reduced datasets such that all classes are balanced.
The results are averaged over 4 runs on each of the considered training set sizes of 250, 500, 1000, 2000 or 4000.
The validation and test sets contain 1000 and 8000 images, which are resampled in each run as well.


 	
\section{Additional information on the irrep models}
\label{apx:irrep_models}


\paragraph{SO(2) models}
We experiment with some variants (rows~37-44) of the Harmonic Network model in row~30 of Table~\ref{tab:mnist_comparison}, varying in either the non-linearity or the invariant map applied.
All of these models are therefore to be analyzed relative to this baseline.
First, we try to use \emph{squashing} nonlinearities~\cite{Hinton2018-EMCAPS} (row~37) instead of norm-ReLUs on each non-trivial irrep.
This variant performs consistently worse than the original model.
In the baseline variant, we generate invariant features via a convolution to scalar fields in the last layer (\emph{conv2triv}).
This, however, reduces the utilization of high frequency irrep fields in the penultimate layer.
The reason for this is that the kernel space for mappings from high frequency- to scalar fields consists of kernels of a high angular frequency, which will be cut off by our bandlimiting.
To overcome this problem, we propose to instead compute the norms of all non-trivial fields to produce invariant features.
This enables us to use all irreps in the output of the last convolutional layer.
However, we find that combining invariant norm mappings with norm-ReLUs does not improve on the baseline model, see row~38.
In row~39 we consider a variant which applies norm-ReLUs on the direct sum of multiple non-trivial irrep fields, each with multiplicity 1, together (\textit{shared norm-ReLU}), while the scalar fields are still being acted on by ELUs.
This is legitimate since the direct sum of unitary representations is itself unitary.
After the last convolutional layer, the invariant projection preserves the trivial fields but computes the norm of each composed field.
This model significantly outperforms all previous variants on all datasets.
The model in row~40 additionally merges the scalar fields to such combined fields instead of treating them independently.
This architecture performs significantly worse than the previous variants.

We further explore four different variations which are applying \textit{gated nonlinearities} (rows~41-44).
These models distinguish from each other by 1) their mapping to invariant features and 2) whether the gate is being applied to each non-trivial field independently or being shared between multiple non-trivial fields.
We find that the second choice, i.e. sharing gates, does not significantly affect the performances (row 41 vs. 42 and 43 vs. 44).
However, mapping to invariant features by taking the norm of all non-trivial fields performs consistently better than applying \textit{conv2triv}.
Overall, gated nonlinearities perform significantly better than any other choice of nonlinearity on the tested  irrep models.



\paragraph{O(2) models}

Here we will give more details on the -specific operations which we introduce to improve the performance of the -equivariant models, reported in rows 45-57 of Table~\ref{tab:mnist_comparison}.

\begin{itemize}[leftmargin=7ex]
\setlength{\itemindent}{-4ex}
    \item O(2)-\textit{conv2triv:}
        As invariant map of the  irrep models in rows 46-49 and 54 we are designing a last convolution layer which is mapping to an output representation , that is, to scalar fields  and sign-flip fields  in equal proportions.
        Since the latter are not invariant under reflections, we are in addition taking their absolute value.
        The resulting, invariant output features are then multiple fields .
        The motivation for not convolving to trivial representations of  directly via \textit{conv2triv} is that the steerable kernel space for mappings between irreps of  does not allow for mapping between  and  (see Table~\ref{tab:O2_irrep_solution_appendix}), which would lead to dead neurons.
\end{itemize}
The models in rows 50-53, 56 and 57 operate on -fields whose representations are induced from the irreps of .
Per definition, this representation acts on feature vectors  in , which we treat in the following as functions .
We further identify the coset  in the quotient space  by its representative  in the reflection group.
Eq.~\ref{eq:induced_field}~defines the action of the induced representation on a feature vector by
2ex]
    \  =&\ 
    \begin{cases}
        \quad
        \psi_k^{\SO2}\big(\tilde{r})      \,f\big(\tilde{s}s\SO2\big) \quad& \text{for }\ s=+1 \
where we used Eq.~\ref{eq:g_decomposition_h-fct} to compute

Intuitively, this action describes a permutation of the subfields (indexed by ) via the reflection  and a rotation of the subfields by  and , respectively.
Specifically, for , the induced representation is for all  instantiated by
2ex]
        \begin{bmatrix}
            0 & 1 \\
            1 & 0
        \end{bmatrix}
        \qquad&\text{for }\ \tilde{s}=-1 \ \ ,
    \end{cases}

    \big[\Ind{\SO2}{\O2}\psi_{k>0}^{\SO2}\big](\tilde{r}\tilde{s})
    \ =\ 
    \begin{cases}
        \left[
        \begin{array}{c|c}
            \1.3ex]
            \hline\1.3ex]
        \end{array}
        \right]
        \qquad&\text{for }\ \tilde{s}=+1 \-1.75ex]
            0 & \!\psi_{k>0}^{\SO2}(\tilde{r})\!\!\!
            \-1.2ex]
            \!\!\psi_{k>0}^{\SO2}(-\tilde{r})\!\! & 0
            \
We adapt the conv2triv and norm invariant maps, as well as the norm-ReLU and the gated nonlinearities to operate on -fields as follows:
\begin{itemize}[leftmargin=7ex]
\setlength{\itemindent}{-4ex}
    \item \textit{-conv2triv:}
        Instead of applying \textit{O(2)-conv2triv} to compute invariant features, we apply convolutions to -fields which are invariant under rotations but behave like regular -fields under reflections.
        These fields are subsequently mapped to a scalar field via -pooling, i.e. by taking the maximal response over the two subfields.
    \item \textit{-norm:}
        An alternative invariant map is defined by computing the norms of the subfields of each final -field and applying -pooling over the result.
    \item \textit{ norm-ReLU:}
        It would be possible to apply a norm-ReLU to a -field for  as a whole, that is, to compute the norm of both subfields together.
        Instead, we apply two individual norm-ReLUs to the subfields.
        Since the fields permute under reflections, we need to choose the bias parameter of the two norm-ReLUs to be equal.
    \item \textit{ gate:}
        Similarly, we could apply a single gate to each -field.
        However, we apply an individual gate to each subfield.
        In this case it is necessary that the gates permute together with the -fields to ensure equivariance.
        This is achieved by computing the gates from -fields, which contain two permuting scalar fields.
\end{itemize}

Empirically we find that  models perform much better than pure irrep models, despite both of them being equivalent up to a change of basis.
In particular, the induced representations decompose for some change of basis matrices  and  into:

The difference between both bases is that the induced representations disentangle the action of reflections into a permutation, while the direct sum of irreps is modeling reflections in each of its sub-vectorfields independently as an inversion of the vector direction and rotation orientation.
Note the analogy to the better performance of regular representations in comparison to a direct sum of the respective irreps.


 
	\newpage
\bibliography{neurips_2019.bbl}

\end{document}
