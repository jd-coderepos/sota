\documentclass{article}

\usepackage[table]{xcolor}    

\usepackage[final,nonatbib]{neurips_2019}

\usepackage[numbers]{natbib}

\usepackage{xcolor}
\usepackage{xspace}
\usepackage{comment}


\newcommand{\apx}{Appendix\xspace
}


\usepackage{pdflscape} 

\usepackage{threeparttable}


\usepackage{scalerel}




\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      

\usepackage{dsfont}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{siunitx}
\usepackage{bm}

\usepackage{graphicx}
\usepackage{float}
\usepackage{wrapfig}
\usepackage[font=footnotesize]{caption}
\usepackage{subcaption}

\usepackage{multirow}
\usepackage{makecell}

\usepackage{gensymb}

\usepackage{afterpage}
\usepackage{scrextend}

\usepackage{rotating}

\usepackage{empheq}
\usepackage[most]{tcolorbox}

\usepackage{multirow,bigdelim}
\usepackage{enumitem}

\usepackage{scalerel}

\usepackage{tikz}
\usepackage{capt-of}

\usepackage{listings}
\usepackage[numbered]{matlab-prettifier}

\definecolor{backcolour}{rgb}{0.92,0.92,0.92}

\lstdefinestyle{mymatstyle}{style=Matlab-editor,
  basicstyle=\mlttfamily\footnotesize,
  backgroundcolor=\color{backcolour},
  frame=leftline,
  numberstyle=\scriptsize,
  xleftmargin=1.em,
}



\newtcbox{\kernelspace}[1][]{nobeforeafter, math upper, tcbox raise base, enhanced,
    colframe=white!25!black,
    colback=white!92!black,
    boxrule=1.2pt,
    #1}


\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\newcommand{\lp}{\left(}
\newcommand{\rp}{\right)}
\newcommand{\lb}{\left[}
\newcommand{\rb}{\right]}

\DeclareMathOperator*{\tr}{\operatorname{tr}}

\DeclareMathOperator*{\R}{\mathbb{R}}
\DeclareMathOperator*{\Cm}{\mathbb{C}}
\DeclareMathOperator*{\Z}{\mathbb{Z}}
\DeclareMathOperator*{\N}{\mathbb{N}}

\newcommand{\GL}[1]{\ensuremath{\operatorname{GL}(#1)}}
\newcommand{\E}[1]{\ensuremath{\operatorname{E}(#1)}}
\newcommand{\SE}[1]{\ensuremath{\operatorname{SE}(#1)}}
\renewcommand{\O}[1]{\ensuremath{\operatorname{O}(#1)}}
\newcommand{\SO}[1]{\ensuremath{\operatorname{SO}(#1)}}
\newcommand{\U}[1]{\ensuremath{\operatorname{U}(#1)}}
\newcommand{\D}[1]{\ensuremath{\operatorname{D}_{#1}}}
\newcommand{\C}[1]{\ensuremath{\operatorname{C}_{#1}}}
\newcommand{\DN}{\ensuremath{\operatorname{D}_{\!N}}}
\newcommand{\CN}{\ensuremath{\operatorname{C}_{\!N}}}
\newcommand{\bC}[1]{\ensuremath{\mathbf{\operatorname{\bf C}_{#1}}}}
\newcommand{\bD}[1]{\ensuremath{\mathbf{\operatorname{\bf D}_{#1}}}}
\newcommand{\bCN}{\ensuremath{\mathbf{\operatorname{\bf C}_{\!N}}}}
\newcommand{\bDN}{\ensuremath{\mathbf{\operatorname{\bf D}_{\!N}}}}
\newcommand{\Flip}{(\{\pm 1\}, *)}
\newcommand{\bFlip}{\boldsymbol{(\{\pm} 1 \boldsymbol{\}, *)}}

\newcommand{\Ind}[2]{\ensuremath{\operatorname{Ind}_{#1}^{#2}}}
\newcommand{\Res}[2]{\ensuremath{\operatorname{Res}_{#1}^{#2}}}

\newcommand{\Span}[1]{\ensuremath{\operatorname{Sp}\lp{#1}\rp}}

\newcommand{\vc}[1]{\ensuremath{\operatorname{vec}\!\lp{#1}\rp}}

\newcommand{\PSI}[1]{
	\begin{bmatrix}
		\cos\lp#1\rp & \!\!\!         \shortminus \sin\lp#1\rp \\
		\sin\lp#1\rp & \!\!\!\phantom{\shortminus}\cos\lp#1\rp \\
	\end{bmatrix}
}
\newcommand{\PSIP}[1]{
	\begin{bmatrix}
		         \shortminus \sin\lp#1\rp & \!\!\!         \shortminus \cos\lp#1\rp \\
		\phantom{\shortminus}\cos\lp#1\rp & \!\!\!         \shortminus \sin\lp#1\rp \\
	\end{bmatrix}
}
\newcommand{\PSIS}[1]{
	\begin{bmatrix}
		\cos\lp#1\rp & \!\!\!\phantom{\shortminus}\sin\lp#1\rp \\
		\sin\lp#1\rp & \!\!\!         \shortminus \cos\lp#1\rp \\
	\end{bmatrix}
}

\newcommand{\XI}[1]{
	\begin{bmatrix}
		1 & \!\!\! 0 \\
		0 & \!\!\! #1 \\
	\end{bmatrix}
}


\newcommand{\X}{\mathcal{X}}
\newcommand{\Loss}{\mathcal{L}}
\newcommand{\K}{\mathcal{K}}

\newcommand{\BF}{\mathcal{BF}}

\newcommand{\muvec}{{\bm \mu}}
\newcommand{\pivec}{{\bm \pi}}
\newcommand{\thetavec}{{\bm \theta}}
\newcommand{\bvec}{{\bm b}}
\newcommand{\mvec}{{\bm m}}
\newcommand{\ovec}{{\bm o}}
\newcommand{\tvec}{{\bm t}}
\newcommand{\vvec}{{\bm v}}
\newcommand{\wvec}{{\bm w}}
\newcommand{\xvec}{{\bm x}}
\newcommand{\yvec}{{\bm y}}
\newcommand{\zvec}{{\bm z}}
\newcommand{\pvec}{{\bm p}}
\newcommand{\qvec}{{\bm q}}
\newcommand{\Xvec}{{\bm X}}
\newcommand{\Yvec}{{\bm Y}}
\newcommand{\Zvec}{{\bm Z}}
\newcommand{\Sigvec}{{\bm \Sigma}}
\newcommand{\zerovec}{{\bm 0}}

\newcommand{\xtvec}{{\tilde{\xvec}}}
\newcommand{\ytvec}{{\tilde{\yvec}}}


\newcommand{\ind}{\,\rotatebox[origin=c]{90}{$\models$}\,}
\usepackage{centernot}
\newcommand{\nind}{\centernot{\rotatebox[origin=c]{90}{$\models$}}}

\usepackage{mathtools}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}

\DeclareMathSymbol{\shortminus}{\mathbin}{AMSa}{"39}
\usepackage{tabu}
\usepackage{diagbox}

\newlength{\secBefore}
\newlength{\secAfter}
\newlength{\subsecBefore}
\newlength{\subsecAfter}
\setlength{\secBefore}{-1.25ex}
\setlength{\secAfter}{-1.5ex}
\setlength{\subsecBefore}{-.6ex}
\setlength{\subsecAfter}{-.8ex}



\newcounter{magicrownumbers}
\newcommand\rownumber{\stepcounter{magicrownumbers}\arabic{magicrownumbers}}


\makeatletter
\def\blfootnote{\gdef\@thefnmark{}\@footnotetext}
\makeatother


\graphicspath{./images/}

\title{
    General E(2)\,-\,Equivariant Steerable CNNs
    }


\author{Maurice Weiler$^*$ \\
	University of Amsterdam, QUVA Lab \\
	\texttt{m.weiler@uva.nl} \\
	\And
	Gabriele Cesa$^{*\dagger}$ \\
	University of Amsterdam \\
	\texttt{cesa.gabriele@gmail.com} \\
}

\begin{document}

	\maketitle
	
	\blfootnote{* Equal contribution, author ordering determined by random number generator.}
	\blfootnote{$\dagger$ This research has been conducted during an internship at QUVA lab, University of Amsterdam.}

	\vspace*{-2.5ex}
	
\begin{abstract}

The big empirical success of group equivariant networks has led in recent years to the sprouting of a great variety of equivariant network architectures.
A particular focus has thereby been on rotation and reflection equivariant CNNs for planar images.
Here we give a general description of $\E2$-equivariant convolutions in the framework of \emph{Steerable CNNs}.
The theory of Steerable CNNs thereby yields constraints on the convolution kernels which depend on group representations describing the transformation laws of feature spaces.
We show that these constraints for arbitrary group representations can be reduced to constraints under irreducible representations.
A general solution of the kernel space constraint is given for arbitrary representations of the Euclidean group $\E2$ and its subgroups.
We implement a wide range of previously proposed and entirely new equivariant network architectures and extensively compare their performances.
$\E2$-steerable convolutions are further shown to yield remarkable gains on CIFAR-10, CIFAR-100 and STL-10 when used as a drop-in replacement for non-equivariant convolutions.

\end{abstract} 	\vspace*{-1.2ex}


	
\vspace*{\secBefore}
\section{Introduction}\label{sec:introduction}
\vspace*{\secAfter}

The equivariance of neural networks under symmetry group actions has in the recent years proven to be a fruitful prior in network design.
By guaranteeing a desired transformation behavior of convolutional features under transformations of the network input, equivariant networks achieve improved generalization capabilities and sample complexities compared to their non-equivariant counterparts. 
Due to their great practical relevance, a big pool of rotation- and reflection- equivariant models for planar images has been proposed by now.
Unfortunately, an empirical survey, reproducing and comparing all these different approaches, is still missing.

An important step in this direction is given by the theory of \emph{Steerable CNNs}~\cite{Cohen2017-STEER,3d_steerableCNNs,Cohen2018-IIR,generaltheory,gauge} which defines a very general notion of equivariant convolutions on homogeneous spaces.
In particular, steerable CNNs describe $\E2$-equivariant (i.e. rotation- and reflection-equivariant) convolutions on the image plane $\R^2$.
The feature spaces of steerable CNNs are thereby defined as spaces of \emph{feature fields}, characterized by a group representation which determines their transformation behavior under transformations of the input.
In order to preserve the specified transformation law of feature spaces, the convolutional kernels are subject to a linear constraint, depending on the corresponding group representations.
While this constraint has been solved for specific groups and representations~\cite{Cohen2017-STEER,3d_steerableCNNs}, no general solution strategy has been proposed so far.
In this work we give a general strategy which reduces the solution of the kernel space constraint under arbitrary representations to much simpler constraints under single, \emph{irreducible} representations.

Specifically for the Euclidean group $\E2$ and its subgroups, we give a general solution of this kernel space constraint.
As a result, we are able to implement a wide range of equivariant models, covering regular GCNNs
\cite{Cohen2016-GCNN,Weiler2018-STEERABLE,Hoogeboom2018-HEX,bekkers2018roto,Dieleman2016-CYC,Kondor2018-GENERAL},
classical Steerable CNNs~\cite{Cohen2017-STEER}, Harmonic Networks~\cite{Worrall2017-HNET}, gated Harmonic Networks~\cite{3d_steerableCNNs}, Vector Field Networks~\cite{Marcos2017-VFN},
Scattering Transforms \cite{sifre2012combined,Sifre2013-GSCAT,bruna2013invariant,sifre2014rigid,oyallonDeepRotoTranslationScattering2015}
and entirely new architectures, in one unified framework.
In addition, we are able to build hybrid models, mixing different field types (representations) of these networks both over layers and within layers.


We further propose a group restriction operation, allowing for network architectures which are decreasingly equivariant with depth.
This is useful e.g. for natural images which show low level features like edges in arbitrary orientations but carry a sense of preferred orientation globally.
An adaptive level of equivariance accounts for the resulting loss of symmetry in the hierarchy of features.

Since the theory of steerable CNNs does not give a preference for any choice of group representation or equivariant nonlinearity, we run an extensive benchmark study, comparing different equivariance groups, representations and nonlinearities.
We do so on MNIST~12k, rotated MNIST~$\SO2$ and reflected and rotated MNIST~$\O2$ to investigate the influence of the presence or absence of certain symmetries in the dataset.
A drop in replacement of our equivariant convolutional layers is shown to yield significant gains over non-equivariant baselines on CIFAR10, CIFAR100 and STL-10.

Beyond the applications presented in this paper, our contributions are of relevance for general steerable CNNs on homogeneous spaces \cite{Cohen2018-IIR,generaltheory} and gauge equivariant CNNs on manifolds \cite{gauge} since these models obey the same kind of kernel constraints.
More specifically, $2$-dimensional manifolds, endowed with an orthogonal structure group $\O2$ (or subgroups thereof), necessitate \emph{exactly} the kernel constraints solved in this paper.
Our results can therefore readily be transferred to e.g. spherical CNNs 
\cite{Cohen2018-S2CNN,gauge,kondorClebschGordanNets2018,estevesLearningEquivariantRepresentations2018,perraudinDeepSphereEfficientSpherical2018,jiang2019spherical}
or more general models of geometric deep learning
\cite{poulenardMultidirectionalGeodesicNeural2018a,masciGeodesicConvolutionalNeural2015,brunaSpectralNetworksDeep,boscainiLearningClassSpecific2015}.


 	

\vspace*{\secBefore}
\section{General E(2)\,-\,Equivariant Steerable CNNs}
\label{sec:e2cnns}
\vspace*{\secAfter}

Convolutional neural networks process images by extracting a hierarchy of feature maps from a given input signal.
The convolutional weight sharing ensures the inference to be translation-equivariant which means that a translated input signal results in a corresponding translation of the feature maps.
However, vanilla CNNs leave the transformation behavior of feature maps under more general transformations, e.g. rotations and reflections, undefined.
In this work we devise a general framework for convolutional networks which are equivariant under the Euclidean group $\E2$, that is, under isometries of the plane $\R^2$.
We work in the framework of steerable CNNs
\cite{Cohen2017-STEER,3d_steerableCNNs,Cohen2018-IIR,generaltheory,gauge} 
which provides a quite general theory for equivariant CNNs on homogeneous spaces, including Euclidean spaces $\R^d$ as a specific instance.
Sections \ref{sec:feature_fields} and \ref{sec:steerable_convolutions} briefly review the theory of Euclidean steerable CNNs
as described in \cite{3d_steerableCNNs}.
The following subsections explain our main contributions:
a decomposition of the kernel space constraint into irreducible subspaces (\ref{sec:irrep_decomposition}),
their solution for $\E2$ and subgroups (\ref{sec:kernel_constraint_solution_main}),
an overview on the group representations used to steer features, their admissible nonlinearities and their use in related work (\ref{sec:representations}),
the group restriction operation (\ref{sec:restriction})
and implementation details~(\ref{sec:implementation}). 	

\vspace*{\subsecBefore}
\subsection{Isometries of the Euclidean plane $\mathbb{R}^2$}
\label{sec:euclidean_group}
\vspace*{\subsecAfter}

\begin{table}
\newcolumntype{R}{>{}r<{}}
\newcolumntype{Q}{>{$\!\!\!\!\!$}r<{}} \newcolumntype{L}{>{}l<{}}
\newcolumntype{M}{R@{${}{}$}L}
\newcolumntype{N}{Q@{${}{}$}L}
\def\arraystretch{1.35}
\centering
\begin{tabular}{rcMN}
                       \ & order $|G|$ & $G\leq\!\O2$   \ &                                      &            $(\mathbb{R}^2,+)$ & $\,\rtimes\, G            $ \\ \hline
    orthogonal         \ & -           & $\O2$          \ &                                      & $\E2 \cong\,(\mathbb{R}^2,+)$ & $\,\rtimes\, \O2          $ \\
special orthogonal \ & -           & $\SO2$         \ &                                      & $\SE2\cong\,(\mathbb{R}^2,+)$ & $\,\rtimes\, \SO2         $ \\
    cyclic             \ & N           & $\CN$          \ &                                      &            $(\mathbb{R}^2,+)$ & $\,\rtimes\, \CN          $ \\
    reflection         \ & 2           & $(\{\pm1\},*)$ \ & $\ \cong\, \D1$                    &            $(\mathbb{R}^2,+)$ & $\,\rtimes\, (\{\pm1\},*) $ \\
    dihedral           \ & 2N          & $\DN$          \ & $\ \cong\, \CN\rtimes(\{\pm1\},*)$ &            $(\mathbb{R}^2,+)$ & $\,\rtimes\, \DN          $ \\
\end{tabular}
\vspace*{1ex}
\caption{Overview over the different groups covered in our framework.}
\label{tab:groups_arxiv}
\vspace*{-5ex}
\end{table}

The Euclidean group $\E2$ is the group of isometries of the plane $\R^2$, consisting of translations, rotations and reflections.
Characteristic patterns in images often occur at arbitrary positions and in arbitrary orientations.
The Euclidean group therefore models an important factor of variation of image features.
This is especially true for images without a preferred global orientation like satellite imagery or biomedical images but often also applies to low level features of globally oriented images.

One can view the Euclidean group as being constructed from the translation group $(\R^2,+)$ and the orthogonal group $\O2=\{O\in\R^{2\times2} \,|\, O^TO=\operatorname{id}_{2\times2}\}$ via the semidirect product operation as $\E2\cong(\R^2,+)\rtimes\O2$.
The orthogonal group thereby contains all operations leaving the origin invariant, i.e. continuous rotations and reflections.
In order to allow for different levels of equivariance and to cover a wide spectrum of related work we consider subgroups of the Euclidean group of the form $(\R^2,+)\rtimes G$, defined by subgroups $G\leq\O2$.
Specifically, $G$ could be either the special orthogonal group $\SO2$, the group $(\{\pm1\},\,*)$ of the reflections along a given axis, the cyclic groups $\CN$, the dihedral groups $\DN$ or the orthogonal group $\O2$ itself.
While $\SO2$ describes continuous rotations (without reflections), $\CN$ and $\DN$ contain $N$ discrete rotations by angles multiple of $\frac{2\pi}{N}$ and, in the case of $\DN$, reflections.
$\CN$ and $\DN$ are therefore discrete subgroups of order $N$ and $2N$, respectively.
For an overview over the groups and their interrelations see Table \ref{tab:groups_arxiv}.

Since the groups $(\R^2,+)\rtimes G$ are semidirect products, one can uniquely decompose any of their elements into a product $tg$ where $t\in(\R^2,+)$ and $g\in G$ \cite{Cohen2018-IIR} which \mbox{we will do in the rest of the paper.}

 	

\vspace*{\subsecBefore}
\subsection{E(2)\,-\,steerable feature fields}
\label{sec:feature_fields}
\vspace*{\subsecAfter}


Steerable CNNs define feature spaces as spaces of \emph{steerable feature fields}
$f:\R^2\to\R^c$ which associate a $c$-dimensional feature vector $f(x)\in\R^c$ to each point $x$ of a base space, in our case the plane $\R^2$.
In contrast to vanilla CNNs, the feature fields of steerable CNNs are associated with a transformation law which specifies their transformation under actions of $\E2$ (or subgroups) and therefore endows features with a notion of \emph{orientation}\footnote{
    Steerable feature fields can therefore be seen as fields of \emph{capsules} \cite{Sabour2017-DYNCAPS}.
}.
Formally, a feature vector $f(x)$ encodes the coefficients of a coordinate independent geometric feature relative to a choice of reference frame or, equivalently, image orientation (see \apx~\ref{sec:gauge_cnns}).

\begin{wrapfigure}[12]{r}{0.46\textwidth}
    \begin{subfigure}{.47\linewidth}
        \includegraphics[width=\linewidth]{figures/scalar_field_2.png}\subcaption*{scalar field $\rho(g)=1$}\end{subfigure}
    \hspace*{1ex}~
    \begin{subfigure}{.47\linewidth}
        \includegraphics[width=\linewidth]{figures/vector_field_2.png}\subcaption*{vector field $\rho(g)=g$}\end{subfigure}
    \vspace*{-.6ex}
    \caption{
        Transformation behavior of $\rho$-fields.
        }
    \label{fig:fields_trafo}
\end{wrapfigure}
An important example are \emph{scalar} feature fields ${s:\R^2\to\R}$, describing for instance gray-scale images or temperature and pressure fields.
The Euclidean group acts on scalar fields by moving each pixel to a new position, that is, $s(x)\mapsto s\left((tg)^{-1}x\right) = s\left(g^{-1}(x-t)\right)$ for some $tg\in(\R^2,+)\rtimes G$; \mbox{see Figure~\ref{fig:fields_trafo}, left.}
\mbox{\emph{Vector}~fields} ${v:\R^2\to\R^2}$, like optical flow or gradient images, on the other hand transform as ${v(x)\mapsto g\cdot v\left(g^{-1}(x-t)\right)}$.
In contrast to the case of scalar fields, each vector is therefore not only moved to a new position but additionally changes its orientation via the action of $g\in G$; \mbox{see Figure~\ref{fig:fields_trafo},~right.}

The transformation law of a general feature field $f:\R^2\to\R^c$ is fully characterized by its \emph{type} $\rho$.
Here $\rho:G\mapsto\GL{\R^c}$ is a group representation, specifying how the $c$ channels of each feature vector $f(x)$ mix under transformations.
A representation satisfies $\rho(g\tilde{g})=\rho(g)\rho(\tilde{g})$ and therefore models the group multiplication $g\tilde{g}$ as multiplication of $c\times c$ matrices $\rho(g)$ and $\rho(\tilde{g})$;~see \apx~\ref{apx:repr_theory}.
More specifically, a $\rho$-field transforms under the \emph{induced representation}\footnote{
    Induced representations are the most general transformation laws compatible with convolutions \cite{Cohen2018-IIR,generaltheory}.
}\footnote{
    Note that this simple form of the induced representation is a special case for semidirect product groups.
}
${\left[\operatorname{Ind}_G^{(\R^2,+)\rtimes G}\!\rho\right]}$ of~${(\R^2,+)\rtimes G}$~as
\vspace*{-.2ex}
\vspace*{-.5ex}
\begin{align}\label{eq:induced_rep_translations}
    f(x)\ \ \mapsto\ \ 
    \left(\left[\operatorname{Ind}_G^{(\R^2,+)\rtimes G}\rho\right]\!\!(tg)\cdot f\right)\! (x)
    \ \ :=\ \ \rho(g)\cdot f\left(g^{-1}(x-t)\right).
\end{align}
As in the examples above, it transforms feature fields by moving the feature vectors from $g^{-1}(x-t)$ to a new position $x$ and acting on them via $\rho(g)$.
We thus find scalar fields to correspond to the \emph{trivial representation} $\rho(g)=1\ \forall g\in G$ which reflects that the scalar values do not change when being moved.
Similarly, a vector field corresponds to the standard representation $\rho(g)=g$ of $G$.

In analogy to the feature spaces of vanilla CNNs comprising multiple channels, the feature spaces of steerable CNNs consist of multiple feature fields $f_i\!:\R^2\to\R^{c_i}$, each of which is associated with its own \emph{type} $\rho_i\!:G\to\GL{\R^{c_i}}$.
A stack $f=\bigoplus_i f_i$ of feature fields is then defined to be concatenated from the individual feature fields and transforms under the direct sum $\rho=\bigoplus_i\rho_i$ of the individual representations.
Since the direct sum representation is block diagonal, the individual feature fields are guaranteed to transform independently from each other.
A common example for a stack of feature fields are RGB images $f\!\!:\R^2\!\to\!\R^3$.
Since the color channels transform independently under rotations we identify them as three independent scalar fields.
The stacked field representation is thus given by the direct sum $\bigoplus_{i=1}^3 1=\operatorname{id}_{3\times3}$ of three trivial representations.
While the input and output types of steerable CNNs are given by the learning task, the user needs to specify the types $\rho_i$ of intermediate feature fields as hyperparameters, similar to the choice of channels for vanilla CNNs.
We discuss different choices of representations in Section~\ref{sec:representations} and investigate them empirically in Section~\ref{sec:mnist_benchmark}. 	

\vspace*{\subsecBefore}
\subsection{E(2)\,-\,steerable convolutions}
\label{sec:steerable_convolutions}
\vspace*{\subsecAfter}

In order to preserve the transformation law of steerable feature spaces, each network layer is required to be equivariant under the group actions.
As proven for Euclidean groups\footnote{
    Proofs for more general cases can be found in \cite{Cohen2018-IIR,generaltheory}.
}
in \cite{3d_steerableCNNs}, the most general \emph{equivariant linear map} between 
steerable feature spaces, transforming under $\rho_\text{in}$ and $\rho_\text{out}$,
is given by \emph{convolutions} with $G$\emph{-steerable kernels}\footnote{
    As $k:\R^2\to\R^{c_\text{out}\times c_\text{in}}$ returns a matrix of shape $(c_\text{out},c_\text{in})$ for each position $x\in\R^2$, its discretized version can be represented by a tensor of shape $(c_\text{out},c_\text{in},X,Y)$ as usually done in deep learning frameworks.
}
$k:\R^2\to\R^{c_\text{out}\times c_\text{in}}$, satisfying a kernel constraint
\begin{align}\label{eq:kernel_constraint}
    k(gx)\ =\ \rho_\text{out}(g)k(x)\rho_\text{in}(g^{-1}) \quad\forall g\in G,\ x\in {\R}^2 \,.
\end{align}
Intuitively, this constraint determines the form of the kernel in transformed coordinates $gx$ in terms of the kernel in non-transformed coordinates $x$ and thus its response to transformed input fields.
It~ensures~that~the~output feature fields transform as specified by $\operatorname{Ind}\rho_\text{out}$ when the input fields are being transformed by $\operatorname{Ind}\rho_\text{in}$; see \apx \ref{apx:equivariance_conv} for a proof.

Since the kernel constraint is linear, its solutions form a linear subspace of the vector space of unconstrained kernels considered in conventional CNNs.
It is thus sufficient to solve for a basis of the $G$-steerable kernel space in terms of which the equivariant convolutions can be parameterized.
The lower dimensionality of the restricted kernel space enhances the parameter efficiency of steerable CNNs over conventional CNNs similarly to the increased parameter efficiency of CNNs over MLPs by translational weight sharing.

 	

\vspace*{\subsecBefore}
\subsection{Irrep decomposition of the kernel constraint}
\label{sec:irrep_decomposition}
\vspace*{\subsecAfter}

The kernel constraint \eqref{eq:kernel_constraint} in principle needs to be solved individually for each pair of input and output types $\rho_\text{in}$ and $\rho_\text{out}$ to be used in the network\footnote{
		A numerical solution technique which is based on a Clebsch-Gordan decomposition of tensor products of irreps has been proposed in~\cite{3d_steerableCNNs}.
		While this technique would generalize to arbitrary representations it becomes prohibitively expensive for larger representations as considered here; see \apx~\ref{apx:comparison_SE3Nets}.
}.
Here we show how the solution of the kernel constraint for arbitrary representations can be reduced to much simpler constraints under \emph{irreducible representations} (irreps).
Our approach relies on the fact that any representation of a finite or compact group decomposes under a change of basis into a direct sum of irreps, each corresponding to an invariant subspace of the representation space $\R^c$ on which $\rho$ acts.
Denoting the change of basis by $Q$, this means that one can always write
$\rho = Q^{-1}\left[\bigoplus_{i\in I}\psi_i\right]Q$
where $\psi_i$ are the irreducible representations of $G$ and the index set $I$ encodes the types and multiplicities of irreps present in $\rho$.
A~decomposition can be found by exploiting basic results of character theory and linear algebra \cite{serre1977linear}.

The decomposition of $\rho_\text{in}$ and $\rho_\text{out}$ in the kernel constraint \eqref{eq:kernel_constraint} leads to
\begin{align*}
    k(gx)\ =&\ 
    Q_\text{out}^{-1}
    \left[\bigoplus\nolimits_{i\in I_\text{out}}\psi_i(g)\right]
    Q_\text{out}
    \ k(x)\ 
    Q_\text{in}^{-1}
    \left[\bigoplus\nolimits_{j\in I_\text{in}}\psi_j^{-1}(g)\right]
    Q_\text{in}
    \ \ &\forall g\in G,\ x\in\mathbb{R}^2 ,
\intertext{
which, defining a kernel relative to the irrep bases as $\kappa:=Q_\text{out} k Q_\text{in}^{-1}$, implies
}
    \kappa(gx)\ =&\
    \phantom{Q_\text{out}^{-1}}
    \left[\bigoplus\nolimits_{i\in I_\text{out}}\psi_i(g)\right]
    \phantom{Q_\text{out}}
    \ \kappa(x)\ 
    \phantom{Q_\text{in}^{-1}}
    \left[\bigoplus\nolimits_{j\in I_\text{in}}\psi_j^{-1}(g)\right]
    \phantom{Q_\text{in}}
    \ \ &\forall g\in G,\ x\in\mathbb{R}^2 .
\end{align*}
The left and right multiplication with a direct sum of irreps reveals that the constraint decomposes into \emph{independent} constraints
\begin{align}\label{eq:irrep_constraint}
    \kappa^{ij}(gx)\ =\ \psi_i(g)\ \kappa^{ij}(x)\ \psi_j^{-1}(g)
    \qquad\forall g\in G,\ x\in\mathbb{R}^2
    \quad\text{where}\ \ i\in I_\text{out},\ j\in I_\text{in}
\end{align}
on blocks $\kappa^{ij}$ in $\kappa$ corresponding to invariant subspaces of the full space of equivariant kernels; see \apx \ref{apx:constraint_decomposition} for a visualization.
In order to solve for a basis of equivariant kernels satisfying the original constraint \eqref{eq:kernel_constraint}, it is therefore sufficient to solve the irrep constraints \eqref{eq:irrep_constraint} to obtain bases for each block, revert the change of basis and take the union over different blocks.
Specifically, given \mbox{$d_{ij}$-dimensional bases}
$\!\big\{\kappa^{ij}_1, \cdots\!, \kappa^{ij}_{d_{ij}}\big\}$
for the blocks $\kappa^{ij}$ of $\kappa$, we get a $d\!=\!\sum_{ij}\!d_{ij}$-dimensional~basis
\begin{align}\label{eq:kernel_basis}
    \big\{k_1,\cdots,k_d\big\}\ :=\ 
    \bigcup\nolimits_{i\in I_\text{out}} \bigcup\nolimits_{j\in I_\text{in}} \left\{Q^{-1}_\text{out}\,\overline{\kappa}^{ij}_1 Q_\text{in},\, \cdots,\, Q^{-1}_\text{out}\,\overline{\kappa}^{ij}_{d_{ij}} Q_\text{in}\right\}
\end{align}
of solutions of \eqref{eq:kernel_constraint}.
Here $\overline{\kappa}^{ij}$ denotes a block $\kappa^{ij}$ being filled at the corresponding location of a matrix of the shape of $\kappa$ with all other blocks being set to zero; see \apx~\ref{apx:constraint_decomposition}.
The completeness of
\makebox[\linewidth][s]{the basis found this way is guaranteed by construction if the bases for each block $ij$ are complete.}


Note that while this approach shares some basic ideas with the solution strategy proposed in~\cite{3d_steerableCNNs}, it is computationally more efficient for large representations; see \apx~\ref{apx:comparison_SE3Nets}.
We want to emphasize that this strategy for reducing the kernel constraint to irreducible representations is not restricted to subgroups of $\O2$ but applies to steerable CNNs in general.

 	

\vspace*{\subsecBefore}
\subsection{General solution of the kernel constraint for O(2) and subgroups}
\label{sec:kernel_constraint_solution_main}
\vspace*{\subsecAfter}


In order to build isometry-equivariant CNNs on $\R^2$
we need to solve the irrep constraints \eqref{eq:irrep_constraint} for the specific case of $G$ being $\O2$ or one of its subgroups.
For this purpose note that the action of $G$ on $\R^2$ is norm-preserving, that is, $|g.x|=|x|\,\ \forall g\in G,\,x\in\R^2$.
The constraints \eqref{eq:kernel_constraint} and \eqref{eq:irrep_constraint} therefore only restrict the \emph{angular parts} of the kernels but leave their radial parts free.
Since furthermore all irreps of $G$ correspond to one unique \emph{angular frequency}
(see \apx \ref{apx:irreps}),
it is convenient to expand the kernel w.l.o.g. in terms of an (angular) Fourier series\begin{align}\label{eq:fourier_decomposition_kernel}
    \kappa_{\alpha\beta}^{ij} \big(x(r,\phi)\big)\ \ =\ \ A_{\alpha\beta,0}(r) + \sum\nolimits_{\mu=1}^\infty \Big[\,A_{\alpha\beta,\mu}(r)\cos(\mu\phi)\ +\ B_{\alpha\beta,\mu}(r)\sin(\mu\phi) \,\Big]
\end{align}
~\\[-2.ex]
with real-valued, radially dependent coefficients $A_{\alpha\beta,\mu}:\R^+\to\R$ and $B_{\alpha\beta,\mu}:\R^+\to\R$ for each matrix entry $\kappa_{\alpha\beta}^{ij}$ of block $\kappa^{ij}$.
By inserting this expansion into the irrep constraints \eqref{eq:irrep_constraint} and projecting on individual harmonics we obtain constraints on the Fourier coefficients, forcing most of them to be zero.
The vector spaces of $G$-steerable kernel blocks $\kappa^{ij}$ satisfying the irrep constraints \eqref{eq:irrep_constraint} are then parameterized in terms of the remaining Fourier coefficients.
The completeness of this basis follows immediately from the completeness of the Fourier basis.
Similar approaches have been followed in simpler settings for the cases of $\CN$ in \cite{Weiler2018-STEERABLE}, $\SO2$ in \cite{Worrall2017-HNET} and $\SO3$ in \cite{3d_steerableCNNs}.

The resulting bases for the angular parts of kernels for each pair of irreducible representations of $\O2$ are shown in Table~\ref{tab:O2_irrep_solution}.
It turns out that each basis element is harmonic and associated to one unique angular frequency.
\apx \ref{apx:kernel_constraint_solution_apx} gives an explicit derivation and the resulting bases for all possible pairs of irreps for all groups $G\leq\O2$ following the strategy presented in this section.
The analytical solutions for $\SO2$, $\Flip$, $\CN$ and $\DN$ are found in Tables
\ref{tab:SO2_irrep_solution_appendix},
\ref{tab:Reflection_irrep_solution_appendix},
\ref{tab:CN_irrep_solution_appendix} and
\ref{tab:DN_irrep_solution_appendix}.
Since these groups are subgroups of $\O2$, they enforce a weaker kernel constraint as compared to $\O2$.
As a result, the bases for $G<\O2$ are higher dimensional, i.e. they allow for a wider range of kernels.
A higher level of equivariance therefore leads simultaneously to a guaranteed behavior of the inference process under transformations and on the other hand to an improved parameter efficiency.
\begin{table}[h!]\vspace*{-1.2ex}
\centering \scalebox{.86}{\setlength{\tabcolsep}{1pt}\renewcommand\arraystretch{1.8}\begin{tabu}{c|[1pt]c|c|c}\diagbox[height=14pt]{\raisebox{-2pt}{$\psi_i$}}{\ \ \raisebox{7pt}{$\psi_j$}}
                  & trivial       & sign-flip     & frequency $n\in\N^+$                            \\ \tabucline[1pt]{-}
        trivial   & $\big[1\big]$ & $\varnothing$ & $\big[\sin(n\phi),\,          \shortminus \cos(n\phi)\big] $ \\ \hline
        sign-flip & $\varnothing$ & $\big[1\big]$ & $\big[\cos(n\phi),\, \phantom{\shortminus}\sin(n\phi)\big] $ \\ \hline
        \makecell{frequency\\$m\in\N^+$}
& $\Bigg[
                                               \begin{array}{c}
                                                    \!\!\!\phantom{\shortminus}\!\sin(m\phi) \!\!\!\!\\[-6pt]
                                                    \!\!\!                \shortminus \!\cos(m\phi) \!\!\!\!
                                               \end{array}
                                               \Bigg]$
& $\Bigg[
                                               \begin{array}{c}
                                                    \!\!\cos(m\phi) \!\!\!\!\\[-6pt]
                                                    \!\!\sin(m\phi) \!\!\!\!
                                               \end{array}
                                               \Bigg]$
& $\Bigg[
                                               \begin{array}{cc}
                                                    \!\!\!\cos\!\big(\!(\!m\!\shortminus\!n\!)\phi\big) & \!\!\!         \shortminus \!\sin\!\big(\!(\!m\!\shortminus\!n\!)\phi\big)\!\!\!\! \\[-6pt]
                                                    \!\!\!\sin\!\big(\!(\!m\!\shortminus\!n\!)\phi\big) & \!\!\!\phantom{\shortminus}\!\cos\!\big(\!(\!m\!\shortminus\!n\!)\phi\big)\!\!\!\!
                                               \end{array}
                                               \Bigg]
                                                ,\!
                                               \Bigg[
                                               \begin{array}{cc}
                                                    \!\!\!\cos\!\big(\!(\!m\! +         \!n\!)\phi\big) & \!\!\!\phantom{\shortminus}\!\sin\!\big(\!(\!m\! +         \!n\!)\phi\big)\!\!\!\! \\[-6pt]
                                                    \!\!\!\sin\!\big(\!(\!m\! +         \!n\!)\phi\big) & \!\!\!         \shortminus \!\cos\!\big(\!(\!m\! +         \!n\!)\phi\big)\!\!\!\!
                                               \end{array}
                                               \Bigg]$
\end{tabu}}\vspace*{1ex}\caption{Bases for the angular parts of $\O2$-steerable kernels satisfying the irrep constraint~\eqref{eq:irrep_constraint} for different pairs of input field irreps $\psi_j$ and output field irreps $\psi_i$.The different types of irreps are explained in \apx~\ref{apx:irreps}.}\label{tab:O2_irrep_solution}\end{table}

\vspace*{-4.32ex} 	

\vspace*{\subsecBefore}
\subsection{Group representations and nonlinearities}
\label{sec:representations}
\vspace*{\subsecAfter}


A question which so far has been left open is which field types, i.e. which representations $\rho$ of $G$, should be used in practice.
Considering only the convolution operation with $G$-steerable kernels for the moment, it turns out that any change of basis $P$ to an \emph{equivalent representation} $\widetilde\rho:=P^{-1}\rho P$ is irrelevant.
To see this, consider the irrep decomposition $\rho=Q^{-1}\left[\bigoplus_{i\in I}\psi_i\right]Q$ used in the solution of the kernel constraint to obtain a basis $\{k_i\}_{i=1}^d$ of $G$-steerable kernels as defined by Eq.~\eqref{eq:kernel_basis}.
Any equivalent representation will decompose into $\widetilde{\rho}=\widetilde{Q}^{-1}\left[\bigoplus_{i\in I}\psi_i\right]\widetilde{Q}$ with $\widetilde{Q}=QP$ for some $P$ and therefore result in a kernel basis $\{P_\text{out}^{-1} k_i P_\text{in}\}_{i=1}^d$ which entirely negates changes of bases between equivalent representations.
It would therefore w.l.o.g. suffice to consider direct sums of irreps $\rho=\bigoplus_{i\in I}\psi_i$ as representations only, reducing the question on which representations to choose to the question on which types and multiplicities of irreps to use.

In practice, however, convolution layers are interleaved with other operations which are sensitive to specific choices of representations.
In particular, nonlinearity layers are required to be equivariant under the action of specific representations.
The choice of group representations in steerable CNNs therefore restricts the range of admissible nonlinearities, or, conversely, a choice of nonlinearity allows only for certain representations.
In the following we review prominent choices of representations found in the literature in conjunction with their compatible nonlinearities.

All equivariant nonlinearities considered here act spatially localized, that is, on each feature vector $f(x)\in\R^{c_\text{in}}$ for all $x\in\R^2$ individually.
They might produce different types of output fields ${\rho_\text{out}:G\to\GL{\R^{c_\text{out}}}}$, that is, $\sigma:\R^{c_\text{in}}\to\R^{c_\text{out}},\ f(x)\mapsto\sigma(f(x))$.
As proven in \apx \ref{apx:equivariance_nonlin}, it is sufficient to require the equivariance of $\sigma$
under the actions of $\rho_\text{in}$ and $\rho_\text{out}$, i.e. $\sigma\circ\rho_\text{in}(g)=\rho_\text{out}(g)\circ\sigma\ \ \forall g\in G$, for the nonlinearities to be equivariant under the action of induced representations
when being applied to a whole feature field as $\sigma(f)(x):=\sigma(f(x))$.

A general class of representations are \emph{unitary representations} which preserve the norm of their representation space, that is, they satisfy
$\left|\rho_\text{unitary}(g)f(x)\right| = \big|f(x)\big|\ \ \forall\ g\!\!\in\!\!G$.
As proven in \mbox{\apx~\ref{apx:equivariance_nonlin_norm}}, nonlinearities which solely act on the \emph{norm} of feature vectors but preserve their orientation are equivariant w.r.t. unitary representations.
They can in general be decomposed in
${\sigma_\text{norm}:\R^c\to\R^c,\ f(x)\mapsto\eta\big(|f(x)|\big) \frac{f(x)}{|f(x)|}}$
for some nonlinear function
$\eta:\R_{\geq0}\to\R_{\geq0}$
acting on the norm of feature vectors.
\emph{Norm-ReLUs}, defined by $\eta(|f(x)|)=\operatorname{ReLU}(|f(x)|-b)$ where $b\in\R^+$ is a learned bias, were used in \cite{Worrall2017-HNET,3d_steerableCNNs}.
In \cite{Sabour2017-DYNCAPS}, the authors consider \emph{squashing nonlinearities} $\eta(|f(x)|)=\frac{|f(x)|^2}{|f(x)|^2+1}$.
\emph{Gated nonlinearities} were proposed in \cite{3d_steerableCNNs} as conditional version of norm nonlinearities.
They act by scaling the norm of a feature field by learned sigmoid gates $\frac{1}{1+e^{-s(x)}}$, parameterized by a scalar feature field $s$.
All representations considered in this paper are unitary such that their fields can be acted on by norm-nonlinearities.
This applies specifically also to all \emph{irreducible representations} $\psi_i$ of $G\leq\O2$ which are discussed in detail in Section~\ref{apx:irreps}.

A common choice of representations of \emph{finite} groups like $\CN$ and $\DN$ are \emph{regular representations}.
Their representation space $\R^{|G|}$ has dimensionality equal to the order of the group, e.g. $\R^N$ for $\CN$ and $\R^{2N}$ for $\DN$.
The action of the regular representation is defined by assigning each axis $e_g$ of $\R^{|G|}$ to a group element $g\in G$ and permuting the axes according to
$\rho_\text{reg}^G(\tilde{g}) e_g := e_{\tilde{g}g}$.
Since this action is just permuting channels of $\rho_\text{reg}^G$-fields, it commutes with pointwise nonlinearities like ReLU; a proof is given in \apx \ref{apx:equivariance_nonlin_quotient}.
While regular steerable CNNs were empirically found to perform very well, they lead to high dimensional feature spaces with each individual field consuming $|G|$ channels.
Regular steerable CNNs were investigated for planar images in \cite{Cohen2016-GCNN,Weiler2018-STEERABLE,Hoogeboom2018-HEX,bekkers2018roto,Dieleman2016-CYC,sifre2014rigid,oyallonDeepRotoTranslationScattering2015,diaconu2019learning}, for spherical CNNs in \cite{Cohen2018-S2CNN,gauge} and for volumetric convolutions in \cite{winkels3DGCNNsPulmonary2018,Worrall2018-CUBENET}.
Further, the translation of feature maps of conventional CNNs can be viewed as action of the regular~representation~of~the~translation~group.

Closely related to regular representations are \emph{quotient representations}.
Instead of permuting $|G|$ channels indexed by $G$, they permute $|G|/|H|$ channels indexed by cosets $gH$ in the quotient space $G/H$ of a subgroup $H\leq G$.
Specifically, they act on axes $e_{gH}$ of $\R^{|G|/|H|}$ as defined by $\rho_\text{quot}^{G/H}(\tilde{g})e_{gH}:=e_{\tilde{g}gH}$.
This definition covers regular representations as a special case for the trivial subgroup $H=\{e\}$.
As permutation representations, quotient representations~allow~for~pointwise nonlinearities; see \apx \ref{apx:equivariance_nonlin_quotient}.
A benefit of quotient representations over regular representation is that they require by a factor of $|H|$ less channels per feature field.
On the other hand, they enforce more symmetries in the feature fields which result in a restriction of the $G$-steerable kernel basis; see \apx~\ref{apx:quotient_models} for details and intuitive examples.
Quotient representations were~considered~in~\cite{Cohen2017-STEER,Kondor2018-GENERAL}.

Both regular and quotient representations can be viewed as being \textit{induced} from the trivial representation of a subgroup $H\leq G$, specifically
$\rho_\text{reg}^G  = \operatorname{Ind}_{\{e\}}^G1$ and
$\rho_\text{quot}^G = \operatorname{Ind}_{  H  }^G1$.
More generally, any representation $\tilde{\rho}:H\to\GL{\R^c}$ can be used to define an \emph{induced representations}\footnote{
    The induction from $H$ to $G$ considered here is conceptually equivalent to the induction~\eqref{eq:induced_rep_translations} from $G$ to $G\rtimes(\R^2,+)$ but applies to representations acting on a single feature vector rather than on a full feature field.
}
$\rho_\text{ind} = \operatorname{Ind}_H^G \tilde{\rho}:G\to\GL{\R^{c\cdot|G:H|}}$.
Here $|G:H|$ denotes the index of $H$ in $G$ which corresponds to $|G|/|H|$ if $G$ and $H$ are both finite.
Admissible nonlinearities of induced \mbox{representations can be} constructed from valid nonlinearities of $\tilde{\rho}$.
For more information on inductions we refer \mbox{to \apx~\ref{apx:repr_theory}.}

Regular and quotient fields can furthermore be acted on by nonlinear pooling operators.
Via a \emph{group pooling} or projection operation $\max:\R^c\to\R,\ f(x)\to\max(f(x))$ the works \cite{Cohen2016-GCNN,Weiler2018-STEERABLE,bekkers2018roto,Worrall2018-CUBENET,winkels3DGCNNsPulmonary2018} extract the maximum value of a regular or quotient field.
The invariance of the maximum operation implies that the resulting features form \emph{scalar fields}.
Since group pooling operations discard information on the feature orientations entirely, vector field nonlinearities $\sigma_\text{vect}:\R^N\to\R^2$ for regular representations of $\CN$ were proposed in \cite{Marcos2017-VFN}.
Vector field nonlinearities do not only keep the maximum response $\max(f(x))$ but also its index $\arg\max(f(x))$.
This index corresponds to a rotation angle $\theta=\frac{2\pi}{N}\arg\max(f(x))$ which is used to define a vector field with elements $v(x)=\max(f(x))(\cos(\theta),\sin(\theta))^T$.
The equivariance of this operation is proven in \ref{apx:equivariance_nonlin_vector}.


In general, any pair of feature fields $f_1:\R^2\to\R^{c_1}$ and $f_2:\R^2\to\R^{c_2}$ can be combined via the tensor product operation $f_1\otimes f_2$.
Given that the individual fields transform under \emph{arbitrary} representations $\rho_1$ and $\rho_2$, their product transforms under the \textit{tensor product representation} $\rho_1\!\otimes\!\rho_2:G\to\GL{\R^{c_1\cdot c_2}}$.
Since the tensor product is a nonlinear transformation by itself, no additional nonlinearity needs to be applied.
Tensor product nonlinearities have been discussed in~\cite{Kondor2018-NBN,3d_steerableCNNs,kondorClebschGordanNets2018,anderson2019cormorant}.


Any pair $f_1$ and $f_2$ of feature fields can furthermore be concatenated by taking their direct sum $f_1\oplus f_2:\R^2\to\R^{c_1+c_2}$ which we used in Section~\ref{sec:feature_fields} to define feature spaces comprising multiple feature fields.
The concatenated field transforms according to the \textit{direct sum representation} as
$\big(\rho_1\oplus\rho_2\big)(g)\big(f_1\oplus f_2\big) := \rho_1(g)f_1 \oplus \rho_2(g)f_2$.
Since each constituent $f_i$ of a concatenated field transforms independently it can be viewed as an individual feature and is equivariant under the action of its corresponding nonlinearity $\sigma_i$; see Section~\ref{apx:equivariance_direct_sum}.


The theory of steerable CNNs does not prefer any of the here presented representations or nonlinearities over each other.
We are therefore extensively comparing different choices in Section~\ref{sec:mnist_benchmark}.

 	

\vspace*{\subsecBefore}
\subsection{Group restrictions and inductions}
\label{sec:restriction}
\vspace*{\subsecAfter}

The key idea of equivariant networks is to exploit symmetries in the distribution of characteristic patterns in signals.
The level of symmetry present in data might thereby vary over different length scales.
For instance, natural images typically show small features like edges or intensity gradients in arbitrary orientations and reflections.
On a larger length scale, however, the rotational symmetry is broken as manifested in visual patterns exclusively appearing upright but still in different reflections.
Each individual layer of a convolutional network should therefore be adapted to the symmetries present in the length scale of its fields of view.

A loss of symmetry can be implemented by \textit{restricting} the equivariance constraints at a certain depth to a subgroup $(\R^2,+)\rtimes H \leq (\R^2,+)\rtimes G$, where $H\leq G$; e.g. from rotations and reflections $G=\O2$ to mere reflections $H=(\{\pm1\},*)$ in the example above.
This requires the feature fields produced by a layer with a higher level of equivariance to be reinterpreted in the following layer as fields transforming under a subgroup.
Specifically, a $\rho$-field, transforming under a representation $\rho:G\to\GL{\R^c}$, needs to be reinterpreted as a $\tilde{\rho}$-field, where $\tilde{\rho}:H\to\GL{\R^c}$ is a representation of the subgroup $H\leq G$.
This is naturally achieved by defining $\tilde{\rho}$ to be the \emph{restricted representation}
\begin{align}
    \tilde{\rho}\ :=\ \operatorname{Res}_H^G(\rho):\,H\to\GL{\mathbb{R}^c},\ \ h\mapsto\rho(h) \,,
\end{align}
defined by restricting the domain of $\rho$ to $H$.
Since a subsequent $H$-steerable convolution layers can map fields of arbitrary representations we can readily process the resulting $\operatorname{Res}_H^G(\rho)$-field further.

Conversely, it is also possible that local patterns are aligned, while patterns which emerge on a larger scale are more symmetrically distributed.
This can be exploited by lifting a $\rho$ field, transforming under $H\leq G$,
to an induced $\Ind{H}{G}\rho$ field, transforming under $G$.

The effect of enforcing different levels of equivariance through group restrictions is investigated empirically in Sections~\ref{sec:mnist_restriction}, \ref{sec:mnist_rot}, \ref{sec:cifar}~and~\ref{sec:STL10}.

 	
\vspace*{\subsecBefore}
\subsection{Implementation details}
\label{sec:implementation}
\vspace*{\subsecAfter}


$\E2$-steerable CNNs rely on convolutions with $\O2$-steerable kernels.
Our implementation therefore involves 1) computing a basis of steerable kernels, 2) the expansion of a steerable kernel in terms of this basis with learned expansion coefficients and 3) running the actual convolution routine.
Since the kernel basis depends only on the chosen representations it is precomputed before training.


Given an input and output representation $\rho_\text{in}$ and $\rho_\text{out}$ of $G\leq\O2$, we first precompute a basis $\{k_1,\dots k_d\}$ of $G$-steerable kernels satisfying Eq.~\eqref{eq:kernel_constraint}.
In order to solve the kernel constraint we compute the types and multiplicities of irreps in the input and output representations using character theory \cite{serre1977linear}.
The change of basis can be obtained by solving the linear system of
\mbox{equations $\rho(g)=Q^{-1}[\bigoplus_{i\in I}\psi_i(g)]Q\ \forall g\in G$.}
For each pair $\psi_i$, $\psi_j$ of irreps occurring in $\rho_\text{out}$ and $\rho_\text{in}$ we retrieve the analytical solutions $\{\kappa^{ij}_1,\dots,\kappa^{ij}_{d_{ij}}\}$ listed in \apx~\ref{apx:analytical_irrep_bases}.
Together with the change of basis matrices $Q_\text{in}$ and $Q_\text{out}$, they fully determine the angular parts of the basis $\{k_1,\dots,k_d\}$ of G-steerable kernels via Eq.~\eqref{eq:kernel_basis}.
Since the kernel space constraint affects only the angular behavior of the kernels we are free to choose any radial profile.
Following \cite{Weiler2018-STEERABLE} and \cite{3d_steerableCNNs}, we choose Gaussian radial profiles $\exp\left(\frac{1}{2\sigma^2}(r\shortminus\!R)^2\right)$ of width $\sigma$, centered at radii $R=1,\dots,\floor{s/2}$.

In practice, we consider digitized signals on a pixel grid\footnote{
    Note that this prevents equivariance from being exact for groups which are not symmetries of the grid.
    Specifically, for $\Z^2$ only subgroups of $\D4$ are exact symmetries which motivated their use in \cite{Cohen2016-GCNN,Dieleman2016-CYC,Cohen2017-STEER}.
}
 $\Z^2$.
Correspondingly, we sample the analytically found kernel basis $\{k_1,\dots,k_d\}$ on a square grid of size $s\!\times\!s$ 
to obtain their numerical representation of shape $(d,c_\text{out},c_\text{in},s,s)$.
In this process it is important to prevent aliasing effects.
Specifically, each basis kernel corresponds to one particular angular harmonic; see Table~\ref{tab:O2_irrep_solution}.
When being sampled with a too low rate, a basis kernel can appear as a lower harmonic and might therefore introduce non-equivariant kernels to the sampled basis.
For this reason, preventing aliasing is necessary to guarantee (approximate) equivariance.
In order to ensure a faithful discretization, note that each Gaussian radial profile defines a ring whose circumference, and thus angular sampling rate, is proportional to its radius.
It is therefore appropriate to bandlimit the kernel basis by a cutoff frequency which is chosen in proportion to the rings' radii.
Since the basis kernels are harmonics of specific angular frequencies this is easily implemented by discarding high frequency solutions.

In typical applications the feature spaces are defined to be composed of multiple independent feature fields.
Since the corresponding representations are block diagonal, this implies that the actual constraint~\eqref{eq:kernel_constraint} decomposes into multiple simpler constraints\footnote{
    The same decomposition was used in a different context in Section~\ref{sec:irrep_decomposition}.
}
which we leverage in our implementation to improve its computational efficiency.
Assuming the output and input representations of a layer to be given by
$\rho_\text{out} = \bigoplus_\gamma \rho_{\text{out},\gamma}$
and
$\rho_\text{in}  = \bigoplus_\delta  \rho_{\text{in}, \delta}$
respectively, the constraint on the full kernel space is equivalent to constraints on its blocks $k^{\gamma\delta}$ which map between the independent fields transforming under $\rho_{\text{in},\delta}$ and $\rho_{\text{out},\gamma}$.
Our implementation therefore computes a sampled basis
$\big\{k^{\gamma\delta}_1,\dots,k^{\gamma\delta}_{d^{\gamma\delta}}\big\}$
of $k^{\gamma\delta}$ for each pair
$\left(\rho_{\text{in},\delta},\,\rho_{\text{out},\gamma}\right)$
of input and output representations individually.

At runtime, the convolution kernels are expanded by contracting the sampled kernel bases with learned weights.
Specifically, each basis
$\big\{k^{\gamma\delta}_1,\dots,k^{\gamma\delta}_{d^{\gamma\delta}}\big\}$,
realized by a tensor of shape $(d^{\gamma\delta}, c_{\text{out},\gamma}, c_{\text{in},\delta}, s, s)$,
is expanded into the corresponding block $k^{\gamma\delta}$ of the kernel by contracting it with a tensor of learned parameters of shape $(d^{\gamma\delta})$.
This process is sped up further by batching together multiple occurrences of the same pair of representations and thus block bases.

The resulting kernels are then used in a standard convolution routine.
In practice we find that the time spent on the actual convolution of reasonably sized images outweighs the cost of the kernel expansion.
In evaluation mode the parameters are not updated such that the kernel needs to be expanded only once and can then be reused.
$\E2$-steerable CNNs therefore have no computational overhead in comparison to conventional CNNs~at~test~time.

Our implementation is provided as a PyTorch extension which is available at 
\url{https://github.com/QUVA-Lab/e2cnn}.
The library provides equivariant versions of many neural network operations, including G-steerable convolutions, nonlinearities, mappings to produce invariant features, spatial pooling, batch normalization and dropout.
Feature fields are represented by \textit{geometric tensors}, which are wrapping a \texttt{torch.Tensor} object and augment it, among other things, with their transformation law under the action of a symmetry group.
This allows for a dynamic type-checking which prevents the user from applying operations to geometric tensors whose transformation law does not match the transformation law expected by the operation.
The user interface hides most complications on group theory and solutions of the kernel space constraint and requires the user only to specify the transformation laws of feature spaces.
For instance, a $\C8$-equivariant convolution operation, mapping a RGB image, identified as three scalar fields, to ten regular feature fields, would be instantiated by:
\begin{center}
\begin{minipage}{.75\linewidth}
\begin{lstlisting}[style=mymatstyle]
r2_act = Rot2dOnR2(N=8)
feat_type_in  = FieldType(r2_act,  3*[r2_act.trivial_repr])
feat_type_out = FieldType(r2_act, 10*[r2_act.regular_repr])
conv_op = R2Conv(feat_type_in, feat_type_out, kernel_size=5)
\end{lstlisting}
\end{minipage}
\end{center}
Everything the user has to do is to specify that the group $\C8$ acts on $\R^2$ by rotating it (line~1) and to define the types $\rho_\text{in}=\bigoplus_{i=1}^3 1$ and $\rho_\text{out}=\bigoplus_{i=1}^{10}\rho_\text{reg}^{\C4}$ of the input and output feature fields (lines~2~and~3), which are subsequently passed to the constructor of the steerable convolution (line~4).
 	
\vspace*{\secBefore}
\section{Experiments}
\label{sec:experiments}
\vspace*{\secAfter}


Since the framework of general $\E2$-equivariant steerable CNNs supports many choices of groups, representations and nonlinearities, we first run an extensive benchmark study over the space of supported models in Section~\ref{sec:mnist_benchmark}.
As the performance of the models depends heavily on the level of symmetry present in the data, we evaluate each model on three different versions of the MNIST dataset:
on untransformed digits, randomly rotated digits and simultaneously rotated and reflected digits, corresponding to transformations in $\{e\}$, in $\SO2$ and $\O2$, respectively.
Section~\ref{sec:mnist_restriction} further investigates the effect of the \textit{invariance} of models to different global symmetries.
We discuss how too high levels of invariance can hurt the performance and demonstrate how it can be prevented via group restrictions.
The convergence rate of equivariant models is being explored in Section~\ref{sec:mnist_rot_convergence}.
In addition, we present two models which improve upon the previous state of the art on MNIST~rot in Section~\ref{sec:mnist_rot}.

The insights from these benchmark experiments are then applied to build several models for classifying CIFAR-10 and CIFAR-100 images in Section~\ref{sec:cifar} and STL-10 images in Section~\ref{sec:STL10}.
These datasets consist of globally aligned, natural images which allows us to validate that exploiting local symmetries is advantageous.
In order to compare the benefit from different local symmetries we \textit{restrict} the equivariance of our models to different subgroups at varying depths.
All models are built by replacing the non-equivariant convolutions of well established baseline models with our $G$-steerable convolutions.
Despite not tuning any hyperparameters, we find that all of our models significantly outperform the baselines.
This holds true even for settings in which a strong auto-augmentation policy is being used.

All of our experiments are found in a dedicated repository at \url{https://github.com/gabri95/e2cnn_experiments}.


\vspace*{\subsecBefore}
\subsection{Model benchmarking on transformed MNIST datasets}
\label{sec:mnist_benchmark}
\vspace*{\subsecAfter}

We first perform a comprehensive benchmarking to compare the impact of the different design choices covered in this work.
Each model is evaluated on three different versions of the MNIST dataset, each of which consists of 12000 training images and 50000 test images.
As non-transformed version we use the official
\href{https://sites.google.com/a/lisa.iro.umontreal.ca/public_static_twiki/variations-on-the-mnist-digits}{MNIST~12k}
dataset.
The official
\href{https://sites.google.com/a/lisa.iro.umontreal.ca/public_static_twiki/variations-on-the-mnist-digits}{rotated~MNIST}
dataset, often called MNIST~rot, differs from MNIST~12k by a random $\SO2$ rotation of each digit.
In addition we build MNIST~$\O2$ whose digits are both rotated and reflected.
These datasets allow us to study the benefit from different levels of $G$-steerability in the presence or absence of certain symmetries.
In order to not disadvantage models with lower levels of equivariance and since it would be done in real scenarios we train all models using augmentation by the transformations present in the corresponding dataset.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/datasets.pdf}
    \vspace*{-3ex}
    \caption{
        Test errors of $\CN$ and $\DN$ regular steerable CNNs for different orders $N$ for all three MNIST variants.
        \textit{Left:}
            All equivariant models improve upon the non-equivariant CNN baseline on MNIST~$\O2$.
            The error decreases before saturating at around 8 to 12 orientations.
            Since the dataset contains reflected digits, the $\DN$-equivariant models perform better than their $\CN$ counterparts.
        \textit{Middle:}
            Since the intraclass variability of MNIST~rot is reduced, the performances of the $\CN$ model and the baseline CNN improve on this dataset.
            In contrast, the $\DN$ models are invariant to reflections such that they can't distinguish between MNIST~$\O2$ and MNIST~rot.
            For $N=1$ this leads to a worse performance than that of the baseline.
            Restricted dihedral models, denoted by $\DN\!|_5\!\CN$, make use of the local reflectional symmetries but are not globally invariant.
            This makes them perform even better than the $\CN$ models.
        \textit{Right:}
            On MNIST~12k the globally invariant models $\CN$ and $\DN$ don't yield better results than the baseline, however, the restricted (i.e. non-invariant) models $\CN\!|_5\{e\}$ and $\DN\!|_5\{e\}$ do.
            For more details see the main text.
        }
    \label{fig:mnist_regular}
    \vspace*{-2ex}
\end{figure}

Table~\ref{tab:mnist_comparison} shows the test errors of $57$ different models on the three MNIST variants.
The first four columns state the equivariance groups, representations, nonlinearities and invariant maps which distinguish the models.
Column five cites related work which proposed the corresponding model design.
The statistics of each entry are averaged over (at least) 6 samples.
All models in these experiments are derived from the base architecture described in Table~\ref{tab:small_architecture} in \apx~\ref{apx:training_setup}.
The actual width of each model is adapted such that the number of parameters is approximately preserved.
Note that this results in different numbers of channels, depending on the parameter efficiency of the corresponding models.
All models apply some form of \textit{invariant mapping} to scalar fields followed by spatial pooling after the last convolutional layer such that the predictions are guaranteed to be invariant under the equivariance group of the model.
The number of invariant features passed to the fully connected classifier is approximately kept constant by adapting the width of the last convolutional layer to the invariant mapping used.
In the remainder of this subsection we will guide through the results presented in Table~\ref{tab:mnist_comparison}.
For more information on the training setup, see \apx~\ref{apx:mnist_benchmark_training}.


\paragraph{Regular steerable CNNs:}
Due to their popularity we first cover steerable CNNs whose features transform under \textit{regular} representations of $\CN$ and $\DN$ for varying orders $N$.
Note that these models correspond to group convolutional CNNs~\cite{Cohen2016-GCNN,Weiler2018-STEERABLE}.
For the dihedral models we choose a vertical reflection axis.
We use ELUs~\cite{clevert2016ELUs} as pointwise nonlinearities and perform group pooling (see Section~\ref{sec:representations}) as invariant map after the final convolution.
Overall, regular steerable CNNs perform very well.
The reason for this is that feature vectors, transforming under regular representations, can encode any function on the group.

Figure~\ref{fig:mnist_regular} summarizes the results for all regular steerable CNNs on all variants of MNIST (rows 2-10 and 19-27 in Table~\ref{tab:mnist_comparison}).
For MNIST~$\O2$ and MNIST~rot the prediction accuracies improve with $N$ but start to saturate at approximately 8 to 12 rotations.
On MNIST~$\O2$ the $\DN$ models perform consistently better than the $\CN$ models of the same order $N$.
This is the case since the dihedral models are guaranteed to generalize over reflections which are present in the dataset.
All equivariant models outperform the non-equivariant CNN baseline.

On MNIST~rot, the accuracy of the $\CN$-equivariant models improve significantly in comparison to their results on MNIST~$\O2$ since the intra-class variability is reduced.
In contrast, the test errors of the $\DN$-equivariant models is the same on both datasets.
The reason for this result is the reflection invariance of the $\DN$ models which implies that they can't distinguish between reflected digits.
For $N=1$ the dihedral model is purely reflection- but not rotation invariant and therefore performs even worse than the CNN baseline.
This issue is resolved by restricting the dihedral models after the penultimate convolution to $\CN\leq\DN$, such that the group pooling after the final convolution results in only $\CN$-invariant features.
This model, denoted in the figure by $\DN\!|_5\!\CN$, achieves a slightly better accuracy than the pure $\CN$-equivariant model since it can leverage local reflectional symmetries\footnote{
    The group restricted models are not listed in Table~\ref{tab:mnist_comparison} but are discussed in Section~\ref{sec:mnist_restriction}.
}.


\afterpage{ \clearpage \begin{table}
        \begin{center}
            \vspace*{-7ex}
            \makebox[\textwidth]{
                \scalebox{.78}{
                    

\setlength{\tabcolsep}{4pt}\renewcommand\arraystretch{1.3}

\setlength{\aboverulesep}{0pt}
\setlength{\belowrulesep}{0pt}

\rowcolors{2}{gray!12.5}{white}
\begin{tabular}{>{\tiny\color{gray}}llllll@{\ \,}c@{\ }ccc}
\toprule
           & \hspace{-1ex}group                        &  representation                     & $\vphantom{\bigg|}$                                            &               nonlinearity               &  invariant map                                  &          citation                                                                                                      & MNIST\,$\O2$                    &   MNIST\,rot                     &   MNIST\,12k                    \\ \toprule

\rownumber &              $\!\{e\}$                    & \multicolumn{2}{l}{(conventional CNN)}                                                               &                                   ELU    & \qquad -                                        &                                                                                                                      - &  $5.53\scriptstyle\,\pm\,0.20$  &   $2.87\scriptstyle\,\pm\,0.09$  &   $0.91\scriptstyle\,\pm\,0.06$ \\ 

\toprule
\rownumber &              $\C{1}$                      &                                     &                                                                &                                          &                                                 &                                                                            \cite{Weiler2018-STEERABLE,bekkers2018roto} &  $5.19\scriptstyle\,\pm\,0.08$  &   $2.48\scriptstyle\,\pm\,0.13$  &   $0.82\scriptstyle\,\pm\,0.01$ \\ \rownumber &              $\C{2}$                      &                                     &                                                                &                                          &                                                 &                                                                            \cite{Weiler2018-STEERABLE,bekkers2018roto} &  $3.29\scriptstyle\,\pm\,0.07$  &   $1.32\scriptstyle\,\pm\,0.02$  &   $0.87\scriptstyle\,\pm\,0.04$ \\ \rownumber &              $\C{3}$                      &                                     &                                                                &                                          &                                                 &                                                                                                                      - &  $2.87\scriptstyle\,\pm\,0.04$  &   $1.19\scriptstyle\,\pm\,0.06$  &   $0.80\scriptstyle\,\pm\,0.03$ \\ \rownumber &              $\C{4}$                      &                                     &                                                                &                                          &                                                 &             \hspace*{-4ex} \cite{Cohen2016-GCNN,Cohen2017-STEER,Weiler2018-STEERABLE,bekkers2018roto,Dieleman2016-CYC} &  $2.40\scriptstyle\,\pm\,0.05$  &   $1.02\scriptstyle\,\pm\,0.03$  &   $0.99\scriptstyle\,\pm\,0.03$ \\ \rownumber &              $\C{6}$                      &                                     &                                                                &                                          &                                                 &                                                                                               \cite{Hoogeboom2018-HEX} &  $2.08\scriptstyle\,\pm\,0.03$  &   $0.89\scriptstyle\,\pm\,0.03$  &   $0.84\scriptstyle\,\pm\,0.02$ \\ \rownumber &              $\C{8}$                      &                                     &                                                                &                                          &                                                 &                                                                            \cite{Weiler2018-STEERABLE,bekkers2018roto} &  $1.96\scriptstyle\,\pm\,0.04$  &   $0.84\scriptstyle\,\pm\,0.02$  &   $0.89\scriptstyle\,\pm\,0.03$ \\ \rownumber &             $\C{12}$                      &                                     &                                                                &                                          &                                                 &                                                                                            \cite{Weiler2018-STEERABLE} &  $1.95\scriptstyle\,\pm\,0.07$  &   $0.80\scriptstyle\,\pm\,0.03$  &   $0.89\scriptstyle\,\pm\,0.03$ \\ \rownumber &             $\C{16}$                      &                                     &                                                                &                                          &                                                 &                                                                            \cite{Weiler2018-STEERABLE,bekkers2018roto} &  $1.93\scriptstyle\,\pm\,0.04$  &   $0.82\scriptstyle\,\pm\,0.02$  &   $0.95\scriptstyle\,\pm\,0.04$ \\ \rownumber &             $\C{20}$                      &   \multirow{-9}{*}{regular}         &  \multirow{-9}{*}{$\rho_\text{reg}$}                           &                                          &                                                 &                                                                                            \cite{Weiler2018-STEERABLE} &  $1.95\scriptstyle\,\pm\,0.05$  &   $0.83\scriptstyle\,\pm\,0.05$  &   $0.94\scriptstyle\,\pm\,0.06$ \\ \cmidrule(lr){2-4}
\cmidrule(lr){2-4}
\rownumber &              $\C{4}$                      &                                     & $5\rho_\text{reg}\!\oplus\!2\rho_\text{quot}^{\nicefrac{\C{ 4}}{\!\C{2}}}\!\oplus\!2\psi_0$ 
                                                                                                                                                              &                                          &                                                 &                                                                                                 \cite{Cohen2017-STEER} &  $2.43\scriptstyle\,\pm\,0.05$  &   $1.03\scriptstyle\,\pm\,0.05$  &   $1.01\scriptstyle\,\pm\,0.03$ \\ \rownumber &              $\C{8}$                      &                                     & $5\rho_\text{reg}\!\oplus\!2\rho_\text{quot}^{\nicefrac{\C{ 8}}{\!\C{2}}}\!\oplus\!2\rho_\text{quot}^{\nicefrac{\C{ 8}}{\!\C{4}}}\!\oplus\!2\psi_0$
                                                                                                                                                              &                                          &                                                 &                                                                                                                      - &  $2.03\scriptstyle\,\pm\,0.05$  &   $0.84\scriptstyle\,\pm\,0.05$  &   $0.91\scriptstyle\,\pm\,0.02$ \\ \rownumber &             $\C{12}$                      &                                     & $5\rho_\text{reg}\!\oplus\!2\rho_\text{quot}^{\nicefrac{\C{12}}{\!\C{2}}}\!\oplus\!2\rho_\text{quot}^{\nicefrac{\C{12}}{\!\C{4}}}\!\oplus\!3\psi_0$
                                                                                                                                                              &                                          &                                                 &                                                                                                                      - &  $2.04\scriptstyle\,\pm\,0.04$  &   $0.81\scriptstyle\,\pm\,0.02$  &   $0.95\scriptstyle\,\pm\,0.02$ \\ \rownumber &             $\C{16}$                      &                                     & $5\rho_\text{reg}\!\oplus\!2\rho_\text{quot}^{\nicefrac{\C{16}}{\!\C{2}}}\!\oplus\!2\rho_\text{quot}^{\nicefrac{\C{16}}{\!\C{4}}}\!\oplus\!4\psi_0$
                                                                                                                                                              &                                          &                                                 &                                                                                                                      - &  $2.00\scriptstyle\,\pm\,0.01$  &   $0.86\scriptstyle\,\pm\,0.04$  &   $0.98\scriptstyle\,\pm\,0.04$ \\ \rownumber &             $\C{20}$                      &    \multirow{-5}{*}{quotient}       & $5\rho_\text{reg}\!\oplus\!2\rho_\text{quot}^{\nicefrac{\C{20}}{\!\C{2}}}\!\oplus\!2\rho_\text{quot}^{\nicefrac{\C{20}}{\!\C{4}}}\!\oplus\!5\psi_0$
                                                                                                                                                              &                   \multirow{-19}{*}{ELU} &                                                 &                                                                                                                      - &  $2.01\scriptstyle\,\pm\,0.05$  &   $0.83\scriptstyle\,\pm\,0.03$  &   $0.96\scriptstyle\,\pm\,0.04$ \\ \cmidrule(lr){2-5}
\cmidrule(lr){2-5}
\rownumber &                                           &  regular/scalar                     & $\psi_0\xrightarrow{\text{conv}}\rho_\text{reg}\xrightarrow{G\text{-pool}}\psi_0$
                                                                                                                                                              &                         ELU, $G$-pooling &                                                 &                                                                               \cite{Cohen2016-GCNN,marcos2016learning} &  $2.02\scriptstyle\,\pm\,0.02$  &   $0.90\scriptstyle\,\pm\,0.03$  &   $0.93\scriptstyle\,\pm\,0.04$ \\ \rownumber &                                           &  regular/vector                     & $\psi_1\xrightarrow{\text{conv}}\rho_\text{reg}\xrightarrow{\text{vector pool}}\psi_1$
                                                                                                                                                              &                       vector field       &                                                 &                                                                                   \cite{Marcos2017-VFN,marcos2018land} &  $2.12\scriptstyle\,\pm\,0.02$  &   $1.07\scriptstyle\,\pm\,0.03$  &   $0.78\scriptstyle\,\pm\,0.03$ \\ \rownumber &              \multirow{-3}{*}{$\C{16}$}   &    mixed vector                     & $\rho_\text{reg}\!\oplus\!\psi_1\!\xrightarrow{\!\text{conv}\!}\!2\rho_\text{reg}\!\xrightarrow[\text{pool}]{\!\!\text{vector}}\!\rho_\text{reg}\!\oplus\!\psi_1$
                                                                                                                                                              &                        ELU, vector field &               \multirow{-26}{*}{$G$-pooling}    &                                                                                                                      - &  $1.87\scriptstyle\,\pm\,0.03$  &   $0.83\scriptstyle\,\pm\,0.02$  &   $0.63\scriptstyle\,\pm\,0.02$ \\ 

\toprule

\rownumber &              $\D{1}$                      &                                     &                                                                &                                          &                                                 &                                                                                                                      - &  $3.40\scriptstyle\,\pm\,0.07$  &   $3.44\scriptstyle\,\pm\,0.10$  &   $0.98\scriptstyle\,\pm\,0.03$ \\ \rownumber &              $\D{2}$                      &                                     &                                                                &                                          &                                                 &                                                                                                                      - &  $2.42\scriptstyle\,\pm\,0.07$  &   $2.39\scriptstyle\,\pm\,0.04$  &   $1.05\scriptstyle\,\pm\,0.03$ \\ \rownumber &              $\D{3}$                      &                                     &                                                                &                                          &                                                 &                                                                                                                      - &  $2.17\scriptstyle\,\pm\,0.06$  &   $2.15\scriptstyle\,\pm\,0.05$  &   $0.94\scriptstyle\,\pm\,0.02$ \\ \rownumber &              $\D{4}$                      &                                     &                                                                &                                          &                                                 &                                                    \hspace*{-2ex} \cite{Cohen2016-GCNN,Cohen2017-STEER,Veeling2018-qh} &  $1.88\scriptstyle\,\pm\,0.04$  &   $1.87\scriptstyle\,\pm\,0.04$  &   $1.69\scriptstyle\,\pm\,0.03$ \\ \rownumber &              $\D{6}$                      &                                     &                                                                &                                          &                                                 &                                                                                               \cite{Hoogeboom2018-HEX} &  $1.77\scriptstyle\,\pm\,0.06$  &   $1.77\scriptstyle\,\pm\,0.04$  &   $1.00\scriptstyle\,\pm\,0.03$ \\ \rownumber &              $\D{8}$                      &                                     &                                                                &                                          &                                                 &                                                                                                                      - &  $1.68\scriptstyle\,\pm\,0.06$  &   $1.73\scriptstyle\,\pm\,0.03$  &   $1.64\scriptstyle\,\pm\,0.02$ \\ \rownumber &             $\D{12}$                      &                                     &                                                                &                                          &                                                 &                                                                                                                      - &  $1.66\scriptstyle\,\pm\,0.05$  &   $1.65\scriptstyle\,\pm\,0.05$  &   $1.67\scriptstyle\,\pm\,0.01$ \\ \rownumber &             $\D{16}$                      &                                     &                                                                &                                          &                                                 &                                                                                                                      - &  $1.62\scriptstyle\,\pm\,0.04$  &   $1.65\scriptstyle\,\pm\,0.02$  &   $1.68\scriptstyle\,\pm\,0.04$ \\ \rownumber &             $\D{20}$                      &   \multirow{-9}{*}{regular}         &  \multirow{-9}{*}{$\rho_\text{reg}$}                           &                    \multirow{-9}{*}{ELU} &         \multirow{-9}{*}{$G$-pooling}          &                                                                                                                       - &  $1.64\scriptstyle\,\pm\,0.06$  &   $1.62\scriptstyle\,\pm\,0.05$  &   $1.69\scriptstyle\,\pm\,0.03$ \\ \cmidrule(lr){2-5}
\cmidrule(lr){2-5}
\rownumber &             $\D{16}$                      &  regular/scalar                     & $\psi_{0,0}\xrightarrow{\text{conv}}\rho_\text{reg}\xrightarrow{G\text{-pool}}\psi_{0,0}$
                                                                                                                                                              &                       ELU, $G$-pooling   &                                                 &                                                                                                                      - &  $1.92\scriptstyle\,\pm\,0.03$  &   $1.88\scriptstyle\,\pm\,0.07$  &   $1.74\scriptstyle\,\pm\,0.04$ \\ 

\toprule

\rownumber &                                           & irreps $\leq1$                      & $\bigoplus_{i=0}^1 \psi_{i}$                                   &                                          &                                                 &                                                                                                                      - &  $2.98\scriptstyle\,\pm\,0.04$  &   $1.38\scriptstyle\,\pm\,0.09$  &   $1.29\scriptstyle\,\pm\,0.05$ \\ \rownumber &                                           & irreps $\leq3$                      & $\bigoplus_{i=0}^3 \psi_{i}$                                   &                                          &                                                 &                                                                                                                      - &  $3.02\scriptstyle\,\pm\,0.18$  &   $1.38\scriptstyle\,\pm\,0.09$  &   $1.27\scriptstyle\,\pm\,0.03$ \\ \rownumber &                                           & irreps $\leq5$                      & $\bigoplus_{i=0}^5 \psi_{i}$                                   &                                          &                                                 &                                                                                                                      - &  $3.24\scriptstyle\,\pm\,0.05$  &   $1.44\scriptstyle\,\pm\,0.10$  &   $1.36\scriptstyle\,\pm\,0.04$ \\ \rownumber &                                           & irreps $\leq7$                      & $\bigoplus_{i=0}^7 \psi_{i}$                                   &                                          &                                                 &                                                                                                                      - &  $3.30\scriptstyle\,\pm\,0.11$  &   $1.51\scriptstyle\,\pm\,0.10$  &   $1.40\scriptstyle\,\pm\,0.07$ \\ \cmidrule(lr){3-4}
\cmidrule(lr){3-4}
\rownumber &                                           & $\mathbb{C}$-irreps $\leq1$         & $\bigoplus_{i=0}^1 \psi^{\Cm}_{i}$                             &                                          &                                                 &                                                                                                \cite{Worrall2017-HNET} &  $3.39\scriptstyle\,\pm\,0.10$  &   $1.47\scriptstyle\,\pm\,0.06$  &   $1.42\scriptstyle\,\pm\,0.04$ \\ \rownumber &                                           & $\mathbb{C}$-irreps $\leq3$         & $\bigoplus_{i=0}^3 \psi^{\Cm}_{i}$                             &                                          &                                                 &                                                                                                \cite{Worrall2017-HNET} &  $3.48\scriptstyle\,\pm\,0.16$  &   $1.51\scriptstyle\,\pm\,0.05$  &   $1.53\scriptstyle\,\pm\,0.07$ \\ \rownumber &                                           & $\mathbb{C}$-irreps $\leq5$         & $\bigoplus_{i=0}^5 \psi^{\Cm}_{i}$                             &                                          &                                                 &                                                                                                                      - &  $3.59\scriptstyle\,\pm\,0.08$  &   $1.59\scriptstyle\,\pm\,0.05$  &   $1.55\scriptstyle\,\pm\,0.06$ \\ \rownumber &                                           & $\mathbb{C}$-irreps $\leq7$         & $\bigoplus_{i=0}^7 \psi^{\Cm}_{i}$                             &         \multirow{-8}{*}{ELU, norm-ReLU} &                   \multirow{-8}{*}{conv2triv}   &                                                                                                                      - &  $3.64\scriptstyle\,\pm\,0.12$  &   $1.61\scriptstyle\,\pm\,0.06$  &   $1.62\scriptstyle\,\pm\,0.03$ \\ \cmidrule(lr){3-5}
\cmidrule(lr){3-5}
\rownumber &                                           &                                     &                                                                &                              ELU, squash &                                                 &                                                                                                                      - &  $3.10\scriptstyle\,\pm\,0.09$  &   $1.41\scriptstyle\,\pm\,0.04$  &   $1.46\scriptstyle\,\pm\,0.05$ \\ \cmidrule(lr){5-6}
\cmidrule(lr){5-6}
\rownumber &                                           &                                     &                                                                &                           ELU, norm-ReLU &                                                 &                                                                                                                      - &  $3.23\scriptstyle\,\pm\,0.08$  &   $1.38\scriptstyle\,\pm\,0.08$  &   $1.33\scriptstyle\,\pm\,0.03$ \\ \rownumber &                                           &                                     &                                                                &                    ELU, shared norm-ReLU &                                                 &                                                                                                                      - &  $2.88\scriptstyle\,\pm\,0.11$  &   $1.15\scriptstyle\,\pm\,0.06$  &   $1.18\scriptstyle\,\pm\,0.03$ \\ \rownumber &                                           &                                     &                                                                &                         shared norm-ReLU &                   \multirow{-3}{*}{norm}        &                                                                                                                      - &  $3.61\scriptstyle\,\pm\,0.09$  &   $1.57\scriptstyle\,\pm\,0.05$  &   $1.88\scriptstyle\,\pm\,0.05$ \\ \cmidrule(lr){5-6}
\cmidrule(lr){5-6}
\rownumber &                                           &                                     &                                                                &                              ELU, gate   &                                                 &                                                                                                                      - &  $2.37\scriptstyle\,\pm\,0.06$  &   $1.09\scriptstyle\,\pm\,0.03$  &   $1.10\scriptstyle\,\pm\,0.02$ \\ \rownumber &                                           &                                     &                                                                &                       ELU, shared gate   &                   \multirow{-2}{*}{conv2triv}   &                                                                                                                      - &  $2.33\scriptstyle\,\pm\,0.06$  &   $1.11\scriptstyle\,\pm\,0.03$  &   $1.12\scriptstyle\,\pm\,0.04$ \\ \cmidrule(lr){6-6}
\cmidrule(lr){6-6}
\rownumber &                                           &                                     &                                                                &                              ELU, gate   &                                                 &                                                                                                                      - &  $2.23\scriptstyle\,\pm\,0.09$  &   $1.04\scriptstyle\,\pm\,0.04$  &   $1.05\scriptstyle\,\pm\,0.06$ \\ \rownumber &  \multirow{-16}{*}{\hspace{-1ex}$\SO2$}   &   \multirow{-8}{*}{irreps $\leq3$}  & \multirow{-8}{*}{$\bigoplus_{i=0}^3 \psi_{i}$}                 &                       ELU, shared gate   &                   \multirow{-2}{*}{norm}        &                                                                                                                      - &  $2.20\scriptstyle\,\pm\,0.06$  &   $1.01\scriptstyle\,\pm\,0.03$  &   $1.03\scriptstyle\,\pm\,0.03$ \\ 

\toprule

\rownumber &                                           &   irreps $ =  0$                    & $\psi_{0,0}$                                                   &                                 ELU      &                               \quad -          &                                                                                                                       - &  $5.46\scriptstyle\,\pm\,0.46$ &   $5.21\scriptstyle\,\pm\,0.29$  &   $3.98\scriptstyle\,\pm\,0.04$ \\ \cmidrule(lr){3-6}
\cmidrule(lr){3-6}
\rownumber &                                           &   irreps $\leq1$                    & $\psi_{0,0}\oplus\psi_{1,0}\oplus2\psi_{1,1}$                  &                                          &                                                 &                                                                                                                      - &  $3.31\scriptstyle\,\pm\,0.17$  &   $3.37\scriptstyle\,\pm\,0.18$  &   $3.05\scriptstyle\,\pm\,0.09$ \\ \rownumber &                                           &   irreps $\leq3$                    & $\psi_{0,0}\oplus\psi_{1,0}\bigoplus_{i=1}^3 2\psi_{1,i}$      &                                          &                                                 &                                                                                                                      - &  $3.42\scriptstyle\,\pm\,0.03$  &   $3.41\scriptstyle\,\pm\,0.10$  &   $3.86\scriptstyle\,\pm\,0.09$ \\ \rownumber &                                           &   irreps $\leq5$                    & $\psi_{0,0}\oplus\psi_{1,0}\bigoplus_{i=1}^5 2\psi_{1,i}$      &                                          &                                                 &                                                                                                                      - &  $3.59\scriptstyle\,\pm\,0.13$  &   $3.78\scriptstyle\,\pm\,0.31$  &   $4.17\scriptstyle\,\pm\,0.15$ \\ \rownumber &                                           &   irreps $\leq7$                    & $\psi_{0,0}\oplus\psi_{1,0}\bigoplus_{i=1}^7 2\psi_{1,i}$      &         \multirow{-4}{*}{ELU, norm-ReLU} &    \multirow{-4}{*}{$\O2$-conv2triv\!\!\!} &                                                                                                                           - &  $3.84\scriptstyle\,\pm\,0.25$  &   $3.90\scriptstyle\,\pm\,0.18$  &   $4.57\scriptstyle\,\pm\,0.27$ \\ \cmidrule(lr){3-6}
\cmidrule(lr){3-6}
\rownumber &                                           &   $\Ind{}{}$-irreps $\leq1$         & $\Ind{}{}\psi^{\SO2}_0\oplus\Ind{}{}\psi^{\SO2}_1$             &                                          &                                                 &                                                                                                                      - &  $2.72\scriptstyle\,\pm\,0.05$  &   $2.70\scriptstyle\,\pm\,0.11$  &   $2.39\scriptstyle\,\pm\,0.07$ \\ \rownumber &                                           &   $\Ind{}{}$-irreps $\leq3$         & $\Ind{}{}\psi^{\SO2}_0\bigoplus_{i=1}^3\Ind{}{}\psi^{\SO2}_i$  &                                          &                                                 &                                                                                                                      - &  $2.66\scriptstyle\,\pm\,0.07$  &   $2.65\scriptstyle\,\pm\,0.12$  &   $2.25\scriptstyle\,\pm\,0.06$ \\ \rownumber &                                           &   $\Ind{}{}$-irreps $\leq5$         & $\Ind{}{}\psi^{\SO2}_0\bigoplus_{i=1}^5\Ind{}{}\psi^{\SO2}_i$  &                                          &                                                 &                                                                                                                      - &  $2.71\scriptstyle\,\pm\,0.11$  &   $2.84\scriptstyle\,\pm\,0.10$  &   $2.39\scriptstyle\,\pm\,0.09$ \\ \rownumber &                                           &   $\Ind{}{}$-irreps $\leq7$         & $\Ind{}{}\psi^{\SO2}_0\bigoplus_{i=1}^7\Ind{}{}\psi^{\SO2}_i$  &\multirow{-4}{*}{ELU, $\Ind{}{}$ norm-ReLU}&   \multirow{-4}{*}{$\Ind{}{}$-conv2triv\!\!\!} &                                                                                                                      - &  $2.80\scriptstyle\,\pm\,0.12$  &   $2.85\scriptstyle\,\pm\,0.06$  &   $2.25\scriptstyle\,\pm\,0.08$ \\ \cmidrule(lr){3-6}
\cmidrule(lr){3-6}
\rownumber &                                           &                                     &                                                                &                                          &                            $\O2$-conv2triv         &                                                                                                                   - &  $2.39\scriptstyle\,\pm\,0.05$  &   $2.38\scriptstyle\,\pm\,0.07$  &   $2.28\scriptstyle\,\pm\,0.07$ \\ \rownumber &                                           & \multirow{-2}{*}{irreps $\leq3$}    & \multirow{-2}{*}{$\psi_{0,0}\oplus\psi_{1,0}\bigoplus_{i=1}^3 2\psi_{1,i}$}      &                            \multirow{-2}{*}{ELU, gate}  &                                    norm         &                                                                                     - &  $2.21\scriptstyle\,\pm\,0.09$  &   $2.24\scriptstyle\,\pm\,0.06$  &   $2.15\scriptstyle\,\pm\,0.03$ \\ \cmidrule(lr){3-5}
\cmidrule(lr){3-5}
\rownumber &                                           &                                     &                                                                &                                          &                    $\Ind{}{}$-conv2triv\!\!\!     &                                                                                                                    - &  $2.13\scriptstyle\,\pm\,0.04$  &   $2.09\scriptstyle\,\pm\,0.05$  &   $2.05\scriptstyle\,\pm\,0.05$ \\ \rownumber &  \multirow{-13}{*}{\hspace{-1ex}$\O2$}    & \multirow{-2}{*}{$\Ind{}{}$-irreps $\leq3$}         & \multirow{-2}{*}{$\Ind{}{}\psi^{\SO2}_0\bigoplus_{i=1}^3\Ind{}{}\psi^{\SO2}_i$}  &            \multirow{-2}{*}{ELU, $\Ind{}{}$ gate}  &                         $\Ind{}{}$-norm         &                                                                          - &  $1.96\scriptstyle\,\pm\,0.06$  &   $1.95\scriptstyle\,\pm\,0.05$  &   $1.85\scriptstyle\,\pm\,0.07$ \\ \bottomrule
\end{tabular}                 }
            }
            \vspace*{4pt}
            \captionsetup{width=.8\paperwidth}
            \caption{
                Extensive comparison of $G$-steerable CNNs for different choices of groups $G$, representations, nonlinearities and final $G$-invariant maps on three transformed MNIST datasets.
                Multiplicities of representations are reported in relative terms;~the~actual~multiplicities are integer multiples with a depth dependent factor.
                All models apply a $G$-invariant map after the convolutions to guarantee an invariant prediction.
                Citations give credit to the works which proposed the corresponding model design.
                For reference see Sections~\ref{sec:representations},~\ref{sec:mnist_benchmark},~\ref{apx:quotient_models}~and~\ref{apx:irrep_models}.
            }
            \label{tab:mnist_comparison}
        \end{center}
    \end{table}
\thispagestyle{empty}
\clearpage }





For MNIST~12k the non-restricted $\DN$ models perform again worse than the $\CN$ models since they are insensitive to the chirality of the digits.
In order to explain the non-monotonic trend of the curves of the $\CN$ and $\DN$ models, notice that some of the digits are approximately related by symmetry transformations\footnote{
    E.g.
    $\mathbin{\rotatebox[origin=c]{180}{$\mathbf{9}$}}$
    and
    $\mathbin{\rotatebox[origin=c]{180}{$\mathbf{6}$}}$
    ($6$ and $9$)
    or
    $\mathbin{\rotatebox[origin=c]{180}{\reflectbox{$\mathbf{5}$}}}$
    and
    $\mathbin{\rotatebox[origin=c]{180}{\reflectbox{$\mathbf{2}$}}}$
    ($2$ and $5$)
    are related by a rotations by $\pi$ and might therefore be confused by all models $C_{2k}$ and $D_{2k}$ for $k\in\N$.
    Similarly,
    \scalerel*{\setbox0=\hbox{\raisebox{-52pt}{\includegraphics{figures/4_7.png}}}\dp0=0pt\box0}{X}
    and
    \scalerel*{\setbox0=\hbox{\raisebox{-52pt}{\includegraphics{figures/7_4.png}}}\dp0=0pt\box0}{X}
    ($4$ and $7$)
    are related by a reflection and a rotation by $\pi/2$ and might be confused by all models $D_{4k}$.
}.
If these transformations happen to be part of the equivariance group w.r.t. which the model is invariant the predictions are more likely to be confused.
This is mostly the case for $N$ being a multiple of 2 or 4 or for large orders $N$, which include almost all orientations.
Once again, the restricted models, here $\DN\!|_5\{e\}$ and $\CN\!|_5\{e\}$, show the best results since they exploit local symmetries but preserve information on the global orientation.
Since the restricted dihedral model generalizes over local reflections, its performance is consistently better than that of the restricted cyclic model.



\paragraph{Quotient representations:}
As an alternative to regular representations we experiment with some mixtures of quotient representations of $\CN$ (rows 11-15).
These models differ from the regular models by enforcing more symmetries in the feature fields and thus kernels.
The individual feature fields are lower dimensional; however, by fixing the number of parameters, the models use more different fields which in this specific case leads to approximately the same number of channels and therefore compute and memory requirements.
We do not observe any significant difference in performance between regular and quotient representations.
\apx~\ref{apx:quotient_models} gives more intuition on our specific choices~of quotient representations and which symmetries they enforce.
Note that the space of possible quotient representations and their multiplicities is very large and still needs to be investigated more thoroughly.



\paragraph{Group pooling and vector field nonlinearities:}
For $\C{16}$ we implement a group pooling network (row 16) and a vector field network (row 17).
These models map regular feature fields, produced by each convolutional layer, to scalar fields and vector fields, respectively; see Section~\ref{sec:representations}.
These pooling operations compress the features in the regular fields, which can lead to lower memory and compute requirements.
However, since we fix the number of parameters, the resulting models are ultimately much wider than the corresponding regular steerable CNNs.
Since the pooling operations lead to a loss of information, both models perform worse than their purely regular counterpart on MNIST~$\O2$ and MNIST~rot.
Surprisingly, the group pooling network, whose features are orientation unaware, performs better than the vector field network.
On MNIST~12k the group pooling network closes up with the regular steerable CNN while the vector field network achieves an even better result.
We further experiment with a model which applies vector field nonlinearities to only half of the regular fields and preserves the other half (row 18).
This model is on par with the regular model on both transformed MNIST versions but achieves the overall best result on MNIST~12k.
Similar to the case of $\C{16}$, the group pooling network for $\D{16}$ (row 28) performs worse than the corresponding regular model, this time also on MNIST~12k.



\paragraph{SO(2) irrep models:}
The feature fields of all $\SO2$-equivariant models which we consider are defined to transform under irreducible representations; see \apx~\ref{apx:repr_theory} and~\ref{apx:irreps}.
Note that this covers scalar fields and vector fields which transform under $\psi_0^{\SO2}$ and $\psi_1^{\SO2}\!$, respectively.
Overall these models are not competitive compared to the regular steerable CNNs.
This result is particularly important for $\SE3\cong(\R^3,+)\rtimes\SO3$-equivariant CNNs whose feature fields are often transforming under the irreps of $\SO3$~\cite{TensorFieldNets,3d_steerableCNNs,Kondor2018-NBN,kondorClebschGordanNets2018,anderson2019cormorant}.

\begin{minipage}{\linewidth}
The models in rows 29-32 are inspired by Harmonic Networks~\cite{Worrall2017-HNET} and consist of irrep fields with the same multiplicity up to a certain threshold.
All models apply ELUs on scalar fields and norm-ReLUs (see Section~\ref{sec:representations}) on higher order fields.
The projection to invariant features is done via a convolution to scalar features (conv2triv) in the last convolutional layer.
We find that irrep fields up to order $1$ and $3$ perform equally well while higher thresholds yield worse results.
The original implementation of Harmonic Networks considered complex irreps of $\SO2$ which results in a lower dimensional steerable kernel basis as discussed in \apx~\ref{apx:incompleteness_hnets}.
We reimplemented these models and found that their reduced kernel space leads to consistently worse results (rows 33-36).
\end{minipage}

For the model containing irreps up to order $3$ we implemented some alternative variants.
For instance, the model in row 38 does not \textit{convolve} to trivial features in the last layer but computes these by taking the norms of all non-scalar fields.
This does not lead to significantly different results.
\apx~\ref{apx:irrep_models} discusses all variations in detail.

By far the best results are achieved by the models in rows 41-44, which replace the norm-ReLUs with gated nonlinearities, see Section~\ref{sec:representations}.
This observation is in line with the results presented in~\cite{3d_steerableCNNs}, where gated nonlinearities were proposed.




\paragraph{O(2) models:}
As for $\SO2$, we are investigating $\O2$-equivariant models whose features transform under irreps up to a certain order and apply norm-ReLUs (rows 46-49).
In this case we choose twice the multiplicity of 2-dimensional fields than scalar fields, which reflects the multiplicity of irreps contained in the regular representation of $\O2$.
Invariant predictions are computed by convolving in equal proportion to fields which transform under trivial irreps $\psi_{0,0}^{\O2}$ and sign-flip irreps $\psi_{1,0}^{\O2}$ (see \apx~\ref{apx:irreps}), followed by taking the absolute value of the latter ($\O2$-conv2triv).
We again find that higher irrep thresholds yield worse results, this time already starting from order $1$.
In particular, these models perform worse than their $\SO2$-equivariant counterparts even on MNIST~$\O2$.
This suggests that the kernel constraint for this particular choice of representations is too restrictive.

If only scalar fields, corresponding to the trivial irrep $\psi_{0,0}^{\O2}$, are chosen, the kernel constraint becomes $k(gx)=k(x)\ \ \forall g\in\O2$ and therefore allows for isotropic kernels only.
This limits the expressivity of the model so severely that it performs even worse than a conventional CNN on MNIST~rot and MNIST~12k while being on par for MNIST~$\O2$, see row 45.
Note that isotropic kernels correspond to vanilla graph convolutional networks (cf. the results and discussion in~\cite{gauge}).

In order to improve the performance of $\O2$-steerable CNNs, we propose to use representations $\Ind{\SO2}{\O2}\psi_k^{\SO2}$, which are induced from the irreps of $\SO2$ (see \apx~\ref{apx:repr_theory} for more details on induction).
By the definition of induction, this leads to pairs of fields which transform according to $\psi_k^{\SO2}$ under rotations but permute under reflections.
The multiplicity of the irreps of $\O2$ contained in this induced representation coincides with the multiplicities chosen in the pure $\O2$ irrep models.
However, the change of basis, relating both representations, does not commute with the nonlinearities, such that the networks behave differently.
We apply $\Ind{}{}\,$norm-ReLU nonlinearities to the induced $\O2$ models which compute the norm of each of the permuting subfields individually but share the norm-ReLU parameters (the bias) to guarantee equivariance.
In order to project to final, invariant features, we first apply a convolution producing $\Ind{\SO2}{\O2}\psi_0^{\SO2}$ fields ($\Ind{}{}$-conv2triv).
Since these transform like the regular representation of $\Flip\cong\O2/\SO2$, we can simply apply $G$-pooling over the two reflections.
The results, given in rows 50-53, show that these models perform significantly better than the $\O2$ irreps models and outperform the $\SO2$ irrep models on MNIST~$\O2$.
More specific details on all induced $\O2$ model operations are given in \apx~\ref{apx:irrep_models}.

\begin{table}[t]
    \centering
    \small
    \scalebox{.93}{
        
\begin{tabular}{c@{\hspace{25pt}}c@{\hspace{6pt}}c@{\hspace{25pt}}c@{\hspace{6pt}}c@{\hspace{20pt}}c@{\hspace{6pt}}c}
\toprule\\[-8pt]
    \multirow{2}{*}{restriction depth} & \multicolumn{2}{c}{\hspace{-25pt}MNIST~rot}                                     & \multicolumn{4}{c}{MNIST~12k}                                                                                                         \\
    \cmidrule(lr{25pt}){2-3} \cmidrule(lr){4-7} 
                                       & group                             & test error (\%)               & group                             & test error (\%)               & group                             & test error (\%)               \\
    \\[-8pt]\midrule
    (0)                                & $\C{16}$                          & $0.82\scriptstyle\,\pm\,0.02$ & $\{e\}$                           & $0.82\scriptstyle\,\pm\,0.01$ & $\{e\}$                           & $0.82\scriptstyle\,\pm\,0.01$ \\[4pt]
    1                                  & \multirow{5}{*}{$\D{16},\C{16}$}  & $0.86\scriptstyle\,\pm\,0.05$ & \multirow{5}{*}{$\D{16},\{e\}$}   & $0.79\scriptstyle\,\pm\,0.03$ & \multirow{5}{*}{$\C{16},\{e\}$}   & $0.80\scriptstyle\,\pm\,0.03$ \\
    2                                  &                                   & $0.82\scriptstyle\,\pm\,0.03$ &                                   & $0.74\scriptstyle\,\pm\,0.03$ &                                   & $0.77\scriptstyle\,\pm\,0.03$ \\
    3                                  &                                   & $0.77\scriptstyle\,\pm\,0.03$ &                                   & $0.73\scriptstyle\,\pm\,0.03$ &                                   & $0.76\scriptstyle\,\pm\,0.03$ \\
    4                                  &                                   & $0.79\scriptstyle\,\pm\,0.03$ &                                   & $0.72\scriptstyle\,\pm\,0.02$ &                                   & $0.77\scriptstyle\,\pm\,0.03$ \\
    5                                  &                                   & $0.78\scriptstyle\,\pm\,0.04$ &                                   & $0.68\scriptstyle\,\pm\,0.04$ &                                   & $0.75\scriptstyle\,\pm\,0.02$ \\[4pt]
    no restriction                     & $\D{16}$                          & $1.65\scriptstyle\,\pm\,0.02$ & $\D{16}$                          & $1.68\scriptstyle\,\pm\,0.04$ & $\C{16}$                          & $0.95\scriptstyle\,\pm\,0.04$ \\[1pt]
\bottomrule
\end{tabular}







     }
    \vspace*{6pt}
    \caption{
        Effect of the group restriction operation at different depths of the network on MNIST~rot and MNIST~12k.
        Before restriction, the models are equivariant to a larger symmetry group than the group of global symmetries of the corresponding dataset.
        A restriction at larger depths leads to an improved accuracy.
        All restricted models perform better than non-restricted, and hence globally invariant, models.
    }
    \label{tab:mnist_restriction}
    \vspace*{-3ex}
\end{table} 

We again build models which apply gated nonlinearities.
As for $\SO2$, this leads to a greatly improved performance of the pure irrep models, see rows 54-55.
In addition we adapt the gated nonlinearity to the \textit{induced} irrep models (rows 56-57).
Here we apply an independent gate to each of the two permuting sub-fields ($\Ind{}{}\,$gate).
In order to be equivariant, the gates need to permute under reflections as well, which is easily achieved by deriving them from $\Ind{\SO2}{\O2}\psi_0^{\SO2}$ fields instead of scalar fields.
The gated induced irrep model achieves the best results among all $\O2$-steerable networks, however, it is still not competitive compared to the $\DN$ models with large $N$.






\vspace*{\subsecBefore}
\subsection{MNIST group restriction experiments}
\label{sec:mnist_restriction}
\vspace*{\subsecAfter}

All transformed MNIST datasets clearly show local rotational and reflectional symmetries but differ in the level of symmetry present at the global scale.
While the $\DN$ and $\O2$-equivariant models in the last section could exploit these local symmetries, their global invariance leads to a considerable loss of information.
On the other hand, models which are equivariant to the symmetries present at the global scale of the dataset only are not able to generalize over all local symmetries.
The proposed group restriction operation allows for models which are locally equivariant but are globally invariant only to the level of symmetry present in the data.
Table~\ref{tab:mnist_restriction} reports the results of models which are restricted after different layers.
Specifically, on MNIST~rot the $\D{16}$-equivariant model introduced in the last section is restricted to $\C{16}$, while we restrict the $\D{16}$ and the $\C{16}$ model~to~the~trivial~group~$\{e\}$ on MNIST~12k.
The overall trend is that a restriction at later stages of the model improves the performance.
All restricted models perform significantly better than the invariant models.
Figure~\ref{fig:mnist_regular} shows that this behavior is consistent for different orders $N$.






\vspace*{\subsecBefore}
\subsection{On the convergence of Steerable CNNs}
\label{sec:mnist_rot_convergence}
\vspace*{\subsecAfter}

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{figures/mnist_rot_convergence.pdf}
    \begin{minipage}{.95\linewidth}
        \vspace*{2pt}
        \caption{
            Validation errors and losses during the training of a conventional CNN and $\CN$-equivariant models on MNIST~rot.
            Networks with higher levels of equivariance converge significantly faster.
        }
        \label{fig:mnist_convergence}
    \end{minipage}
\end{figure}

In our experiments we find that steerable CNNs converge significantly faster than non-equivariant CNNs.
Figure~\ref{fig:mnist_convergence} shows this behavior for the regular $\CN$-steerable CNNs from Section~\ref{sec:mnist_benchmark} in comparison to a conventional CNN, corresponding to $N=1$, on MNIST~rot.
The rate of convergence thereby increases with the order $N$ and, as already observed in Figure~\ref{fig:mnist_regular}, saturates at approximately $N=8$.
All models share approximately the same number of parameters.

The faster convergence of equivariant networks is explained by the fact that they generalize over $G$-transformed images by design which reduces the amount of intra-class variability which they have to learn\footnote{
    Mathematically, $G$-steerable CNNs classify \textit{equivalence classes} of images defined by the equivalence relation
    $f\sim f'\Leftrightarrow\ \exists\ tg\in(\R^2,+)\rtimes G\ \ \text{s.t.}\ \ f(x)=f\big(g^{-1}(x-t)\big)$.
    Instead, MLPs learn to classify each image individually and conventional CNNs classify equivalence classes defined by translations, i.e. above equivalence classes for $G=\{e\}$.
    For more details see Section~2 of~\cite{Weiler2018-STEERABLE}.
}.
In contrast, a conventional CNN has to learn to classify all transformed versions of each image explicitly which requires either an increased batch size or more training iterations.
The enhanced data efficiency of $\E2$-steerable CNNs can therefore lead to a reduced training time.



\vspace*{\subsecBefore}
\subsection{Competitive MNIST rot experiments}
\label{sec:mnist_rot}
\vspace*{\subsecAfter}

As a final experiment on MNIST~rot we are replicating the regular $\C{16}$ model from~\cite{Weiler2018-STEERABLE} which was the previous SOTA.
It is mostly similar to the models evaluated in the previous sections but is wider, uses larger kernel sizes and adds additional fully connected layers; see Table~\ref{tab:large_architecture} in the \apx.
As reported in Table~\ref{tab:mnist_final}, our reimplementation matches the accuracy of the original model.
Replacing the regular feature fields with the quotient representations used in Section~\ref{sec:mnist_benchmark} leads to slightly better results.
We refer to \apx~\ref{apx:quotient_models} for more insights on the improved performance of the quotient model.
A further significant improvement and a new state of the art is being achieved by a $\D{16}$-equivariant model, which is restricted to $\C{16}$ after the penultimate layer.

\begin{figure}
\begin{minipage}{\linewidth}
    \centering
    \begin{minipage}{0.45\linewidth}
    \begin{table}[H]
    \scalebox{.93}{
    \centering
    \small
    \setlength{\tabcolsep}{4pt}\renewcommand\arraystretch{.95}\begin{tabular}{ccll}
        \toprule
        model                       & group                 & representation & test error (\%) \\
        \midrule
        \cite{Cohen2016-GCNN}       & $\C4$                 & regular/scalar & $3.21\phantom{0} \scriptstyle\,\pm\,0.0012$ \\
        \cite{Cohen2016-GCNN}       & $\C4$                 & regular        & $2.28\phantom{0} \scriptstyle\,\pm\,0.0004$ \\
        \cite{Worrall2017-HNET}     & $\SO2\ $              & irreducible    & $1.69$ \\
        \cite{Laptev_2016_CVPR}     & -                     & -              & $1.2\phantom{00}$ \\
        \cite{Marcos2017-VFN}       & $\C{17}$              & regular/vector & $1.09$ \\
        Ours                        & $\C{16}$              & regular        & $0.716 \scriptstyle\,\pm\,0.028$ \\
        \cite{Weiler2018-STEERABLE} & $\C{16}$              & regular        & $0.714 \scriptstyle\,\pm\,0.022$ \\
        Ours                        & $\C{16}$              & quotient       & $0.705 \scriptstyle\,\pm\,0.025$ \\
        Ours                        & $\D{16}\!|_5\!\C{16}$ & regular        & $0.682 \scriptstyle\,\pm\,0.022$ \\
        \bottomrule
    \end{tabular}}
    \vspace*{1.5pt}
    \caption{
        Final runs on MNIST rot
    }
    \label{tab:mnist_final}
    \end{table} 
    \end{minipage}
    \hfill
    \begin{minipage}{0.52\linewidth}
    \begin{table}[H]
    \scalebox{.93}{
    \centering
    \small
    \setlength{\tabcolsep}{5pt}\renewcommand\arraystretch{.90}\begin{tabular}{lll}
        \toprule
        model                         & CIFAR-10                                 & CIFAR-100        \\
        \midrule
        wrn28/10 \hspace{2.7ex} \cite{widenet}
                                            & $3.87$                                   & $18.80$          \\
        wrn28/10\phantom{*}\ $\D1\D1\D1$    & $3.36 \scriptstyle\,\pm\,0.08$           & $17.97 \scriptstyle\,\pm\,0.11$ \\
        wrn28/10*\           $\D8\D4\D1$    & $3.28 \scriptstyle\,\pm\,0.10$           & $17.42 \scriptstyle\,\pm\,0.33$ \\
        wrn28/10\phantom{*}\ $\C8\C4\C1$    & $3.20 \scriptstyle\,\pm\,0.04$           & $16.47 \scriptstyle\,\pm\,0.22$ \\
        wrn28/10\phantom{*}\ $\D8\D4\D1$    & $3.13 \scriptstyle\,\pm\,0.17$           & $16.76 \scriptstyle\,\pm\,0.40$ \\
        wrn28/10\phantom{*}\ $\D8\D4\D4$    & $2.91 \scriptstyle\,\pm\,0.13$           & $16.22 \scriptstyle\,\pm\,0.31$ \\
        \midrule
        wrn28/10 \hspace{2.7ex} \ \cite{autoaugment} \hspace{2.15ex} AA
                                      & $2.6\phantom{0} \scriptstyle\,\pm\,0.1$  & $17.1\phantom{0} \scriptstyle\,\pm\,0.3$ \\
        wrn28/10*\           $\D8\D4\D1$ AA  & $2.39 \scriptstyle\,\pm\,0.11$           & $15.55 \scriptstyle\,\pm\,0.13$ \\
        wrn28/10\phantom{*}\ $\D8\D4\D1$ AA  & $2.05 \scriptstyle\,\pm\,0.03$           & $14.30 \scriptstyle\,\pm\,0.09$ \\
        \bottomrule
    \end{tabular}}
    \vspace*{2.5pt}
    \caption{
        Test errors on CIFAR (AA=autoaugment)
    }
    \label{tab:cifar}
    \end{table} 
    \end{minipage}
\end{minipage}
\vspace*{-2ex}
\end{figure}




\vspace*{\subsecBefore}
\subsection{CIFAR experiments}
\label{sec:cifar}
\vspace*{\subsecAfter}

The statistics of natural images are typically invariant under global translations and reflections but are not under global rotations.
Here we are investigating the practical benefit of $G$-steerable convolutions for such images by classifying CIFAR-10 and CIFAR-100.
For this purpose we implement several $\DN$ and $\CN$-equivariant versions of WideResNet~\cite{widenet}.
Different levels of equivariance, stated in the model specifications in Table~\ref{tab:cifar}, are thereby used in the three main blocks of the network (i.e. between pooling layers).
Regular representations are used throughout the whole model except for the last convolution which maps to a scalar field to produce invariant predictions.
For a fair comparison we scale the width of all layers such that the number of parameters of the original wrn28/10 model is approximately preserved.
Note that, due to their enhanced parameter efficiency, our models become wider than conventional CNNs.
Since this implies a higher computational cost, we add an equivariant model, marked by an additional *, which has about the same number of channels as the non-equivariant wrn28/10.
For rotation order $N=8$ we are further using $5\times5$ kernels to mitigate the discretization artifacts of steering $3\times3$ kernels by $45$ degrees.
All runs use the same training procedure as reported in~\cite{widenet} and \apx~\ref{apx:cifar}.
We want to emphasize in particular that we perform \textit{no further hyperparameter tuning}.

The results of the $\D1\D1\D1$ model in Table~\ref{tab:cifar} confirm that incorporating the global symmetries of the data already yields a significant boost in accuracy.
Interestingly, the $\C8\C4\C1$ model, which is purely rotation but not reflection-equivariant, achieves better results, which shows that it is worthwhile to leverage local rotational symmetries.
Both symmetries are respected simultaneously by the wrn28/10 $\D8\D4\D1$ model.
While this model performs better than the two previous ones on CIFAR-10, it surprisingly yields slightly worse result on CIFAR-100.
This might be due to the higher dimensionality of its feature fields which, despite the model having more channels in total, leads to less independent fields.
The best results (without using auto augment) are obtained by the $\D8\D4\D4$ model which suggests that rotational symmetries are useful even on a larger scale.
The small wrn28/10* $\D8\D4\D1$ model shows a remarkable gain compared to the non-equivariant wrn28/10 baseline \textit{despite not being computationally more expensive}.
To investigate whether equivariance is useful even when a powerful data augmentation policy is available, we further rerun both $\D8\D4\D1$ models with \textit{AutoAugment}~(AA)~\cite{autoaugment}.
As without AA, both the computationally cheap wrn28/10* model and the wider wrn28/10 version outperform the wrn28/10 baseline by a large margin.





\vspace*{\subsecBefore}
\subsection{STL-10 experiments}
\label{sec:STL10}
\vspace*{\subsecAfter}

\begin{wrapfigure}[13]{r}{0.42\textwidth}
\vspace*{-5ex}
\begin{minipage}{\linewidth}
\begin{table}[H]
\centering
\scalebox{.93}{
    \small
    \begin{tabular}{@{\ }l@{\ }c@{\ \ }c@{\ \ }c@{\,}}
        \toprule
        model                 & group       & \#params & \ test error (\%)                     \\
        \midrule
        wrn16/8 \cite{cutout} & -           &  11M     & $          12.74 \scriptstyle\pm0.23$ \\
        wrn16/8*              & $\D1\D1\D1$ &   5M     & $          11.05 \scriptstyle\pm0.45$ \\
        wrn16/8               & $\D1\D1\D1$ &  10M     & $          11.17 \scriptstyle\pm0.60$ \\
        wrn16/8*              & $\D8\D4\D1$ & 4.2M     & $          10.57 \scriptstyle\pm0.70$ \\
        wrn16/8               & $\D8\D4\D1$ &  12M     & $\phantom{1}9.80 \scriptstyle\pm0.40$ \\
        \bottomrule
    \end{tabular}
    }
    \vspace*{6pt}
    \caption{
        Test errors of different equivariant models on the STL-10 dataset.
        Models with * are not scaled to the same number of parameters as the original model but preserve the number of channels of the baseline.
    }
    \label{tab:stl10}
\end{table}
\end{minipage}
\end{wrapfigure}
In order to test whether the previous results generalize to natural images of higher resolution we run additional experiments on STL-10~\cite{stl10}.
While this dataset was originally intended for semi-supervised learning tasks, its $5000$ training images are also being used for supervised classification in the low data regime~\cite{cutout}.
We adapt the experiments in~\cite{cutout} by replacing the non-equivariant convolutions of their wrn16/8 model, which was the previous supervised SOTA, with $\DN$-steerable convolutions.
As in the CIFAR experiments, all intermediate features transform according to regular representations.
A final, invariant prediction is generated via a convolution to scalar fields.
We are again using steerable convolutions as a mere drop-in replacement, that is, we use the same training setting and hyperparameters as in the original paper.
The four adapted models, reported in Table~\ref{tab:stl10}, are equivariant under either the action of $\D1$ in all blocks or the actions of $\D8$, $\D4$ and $\D1$ in the respective blocks.
For both choices we build a large model, whose width is scaled up to approximately match the number of parameters of the baseline, and a small model, which preserves the number of channels and thus compute and memory requirements, but is more parameter efficient.
\begin{wrapfigure}{r}{0.45\textwidth}
\vspace*{-4.8ex}
\begin{minipage}{\linewidth}
    \begin{figure}[H]
        \centering
        \includegraphics[width=.92\linewidth]{figures/stl10_ablation.pdf}
        \begin{minipage}{.9\linewidth}
            \vspace*{2pt}
            \caption{
                Data ablation study on STL-10.
                The equivariant models yield significantly improved results on all dataset sizes.
            }
            \label{fig:stl10_ablation}
        \end{minipage}
    \end{figure}
\end{minipage}
\end{wrapfigure}

\vspace*{-1.2ex}
As expected, all models improve significantly over the baseline with larger models outperforming smaller ones.
However, due to their extended equivariance, the small $\D8\D4\D1$ model performs better than the large $\D1\D1\D1$ model.
In comparison to the CIFAR experiments, rotational equivariance seems to give a more significant boost in accuracy.
This is expected since the higher resolution of $96\times96$ pixels of the STL-10 images allows for more detailed local patterns which occur in arbitrary orientations.

Figure~\ref{fig:stl10_ablation} reports the results of a data ablation study which investigates the performance of the $\D8\D4\D1$ models for smaller training set sizes.
The results validate that the gains from incorporating equivariance are consistent over all training sets.
More information on the exact training procedures is given in \apx~\ref{apx:stl10}.


























 	

\vspace*{\secBefore}
\section{Conclusions}
\label{sec:conclusion}
\vspace*{\secAfter}


A wide range of rotation- and reflection-equivariant models has been proposed in the recent years.
In this work we presented a general theory of $\E2$-equivariant steerable CNNs which describes many of these models in one unified framework.
By analytically solving the kernel constraint for any representation of $\O2$ or its subgroups we were able to reproduce and systematically compare these models.
We further proposed a group restriction operation which allows us to adapt the level of equivariance to the symmetries present on the corresponding length scale.
When using $G$-steerable convolutions as drop in replacement for conventional convolution layers we obtained significant improvements on CIFAR-10, CIFAR-100 and STL-10 without additional hyperparameter tuning.
While the kernel expansion leads to a small overhead during train time, the final kernels can be stored such that during test time steerable CNNs are computationally not more expensive than conventional CNNs of the same width.
Due to the enhanced parameter efficiency of equivariant models it is a common practice to adapt the model width to match the parameter cost of conventional CNNs.
Our results show that even non-scaled models outperform conventional CNNs in accuracy.
Since steerable CNNs converge faster than non-equivariant CNNs, they can even be cheaper to train.

We believe that equivariant CNNs will in the long term become the default choice for tasks like biomedical imaging, where symmetries are present on a global scale.
The impressive results on natural images demonstrate the great potential of applying $\E2$-steerable CNNs to more general vision tasks which involve only local symmetries.
Future research still needs to investigate the wide range of design choices of steerable CNNs in more depth and collect evidence on whether our findings generalize to different settings.
We hope that our library\footnote{The library is available at \url{https://github.com/QUVA-Lab/e2cnn}.}
will help equivariant CNNs to be adopted by the community and facilitate further research.

 
	\vfill
	\subsubsection*{Acknowledgments}
	\vspace*{-1ex}
	{\small
	We would like to thank Taco Cohen for fruitful discussions on an efficient implementation and helpful feedback on the paper and Daniel Worrall for elaborating on the real valued implementation of Harmonic Networks.
	}

	\newpage
	\title{
		General E(2)\,-\,Equivariant Steerable CNNs \\
		Appendix 
		}
	\author{}
	\maketitlesup
	\vspace*{-10ex}

	\appendix

	
\vspace*{5ex}
\section{Local gauge equivariance of E(2)-steerable CNNs}
\label{sec:gauge_cnns}
\vspace*{1ex}


\begin{minipage}{\linewidth}
    \centering
    \begin{tikzpicture}
        \node[] at (0,0) {\includegraphics[width=.98\linewidth]{figures/gauge_trafos_viewpoints.png}};
        \node[anchor=north west,inner sep=0, align=left] at   (-3.6,  2.3) {\small active trafo};
\node[anchor=south west,inner sep=0, align=right] at (-3.65, -0.8) {\small global \\ \small coord. trafo \\ \small (passive)};
\node[anchor=south west,inner sep=0, align=center] at (1.95,  1.25) {\small local  \\ \small coord. trafo \\[1.2ex] \small (passive)};
\end{tikzpicture}
    \captionsetup{skip=-18.5ex, belowskip=1ex, margin={33.5ex, 1.25ex}}
    \captionof{figure}{
        \mbox{Different viewpoints on transformations of signals on~$\R^2\!\!\!.$}
        \textit{Top Left:} In our work we considered active rotations of the signal while keeping the coordinate frames fixed.
        \textit{Bottom Left:} The equivalent, passive interpretation views the transformation as a global rotation of reference frames \mbox{(a global gauge transformation).}
        \mbox{\textit{Right:} Local gauge transformations
        }rotate reference frames independently from each other. $\E2$-steerable CNNs are equivariant w.r.t. both global and local gauge transformations.
    }
    \label{fig:gauge}
\end{minipage}
\vspace*{1ex}

The $\E2$-equivariant steerable CNNs considered in this work were derived in the classical framework of steerable CNNs on Euclidean spaces $\R^d$ (or more general homogeneous spaces) \cite{Cohen2017-STEER,3d_steerableCNNs,Cohen2018-IIR,generaltheory}.
This~formulation considers \emph{active transformations} of signals, in our case translations, rotations and reflections of images.
Specifically, an active transformation by a group element $tg\in(\R^2,+)\rtimes G$ moves signal values from $x$ to $g^{-1}(x-t)$; see Eq.~\eqref{eq:induced_rep_translations} and Figure~\ref{fig:gauge}, top~left.
The proven equivariance properties of the proposed $\E2$-equivariant steerable CNNs guarantee the specified transformation behavior of the feature spaces under such active transformations.
However, our derivations so far don't prove any equivariance guarantees for \emph{local}, independent rotations or reflections of small patches in an image.

The appropriate framework for analyzing local transformations is given by Gauge Equivariant Steerable CNNs \cite{gauge}.
In contrast to active transformations, Gauge Equivariant CNNs consider \emph{passive gauge transformations}; see Figure~\ref{fig:gauge}, right.
Adapted to our specific setting, each feature vector $f(x)$ is being expressed relative to a \emph{local reference frame} (or gauge) $\big(e_1(x),\, e_2(x)\big)$ at $x\in\R^2$.
A \emph{gauge transformation} formalizes a change of local reference frames by the action of position dependent elements $g(x)$ of the \emph{gauge group} (or structure group), in our case rotations and reflections in $G\leq\O2$.
Since gauge transformations act independently on each position, they model independent transformation of local patches in an image.
As derived in \cite{gauge}, the demand for \emph{local gauge equivariance} results in the same kernel constraint as in Eq.~\eqref{eq:kernel_constraint}.
This implies that our models are automatically locally gauge equivariant\footnote{
    Conversely, the equivariance under local gauge transformations $g(x)\!\in\!\O2$ implies the equivariance under active isometries.
    In the case of the Euclidean space $\R^2$ these isometries are given by the Euclidean group $\E2$.
}.

More generally, the kernel constraint~\eqref{eq:kernel_constraint} applies to arbitrary 2-dimensional Riemannian manifolds $M$ with structure groups $G\leq\O2$.
The presented solutions of the kernel space constraint therefore describe spherical CNNs
\cite{Cohen2018-S2CNN,gauge,kondorClebschGordanNets2018,estevesLearningEquivariantRepresentations2018,perraudinDeepSphereEfficientSpherical2018,jiang2019spherical}
or convolutional networks on triangulated meshes
\cite{poulenardMultidirectionalGeodesicNeural2018a,masciGeodesicConvolutionalNeural2015,brunaSpectralNetworksDeep,boscainiLearningClassSpecific2015}
for different choices of structure groups and group representations.

 	
\section{A short primer on group representation theory}
\label{apx:repr_theory}

Linear group representations model abstract algebraic group elements via their action on some vector space, that is, by representing them as linear transformations (matrices) on that space.
Representation theory forms the backbone of Steerable CNNs since it describes the transformation law of their feature spaces.
It is furthermore widely used to describe fields and their transformation behavior in physics.

Formally, a linear representation $\rho$ of a group $G$ on a vector space (representation space) $\R^n$ is a group homomorphism from $G$ to the general linear group $\GL{\R^n}$ (the group of invertible $n\times n$ matrices), i.e. it is a map
\begin{align*}
    \rho: G \to \operatorname{GL}(\mathbb{R}^n)
    \quad\text{such that}\quad
    \rho(g_1 g_2) = \rho(g_1) \rho(g_2) \quad \forall g_1, g_2 \in G \,.
\end{align*}
The requirement to be a homomorphism, i.e. to satisfy $\rho(g_1 g_2)=\rho(g_1)\rho(g_2)$, ensures the compatibility of the matrix multiplication $\rho(g_1)\rho(g_2)$ with the group composition $g_1 g_2$ which is necessary for a well defined group action.
Note that group representations do not need to model the group \textit{faithfully} (which would be the case for an isomorphism instead of a homomorphism).

A simple example is the \textit{trivial representation} $\rho:G\to\GL{\R}$ which maps any group element to the identity, i.e. $\forall g\in G\quad \rho(g)=1$.
The 2-dimensional rotation matrices $\psi(\theta)=\PSI{\theta}$ are an example of a representation of $\SO{2}$ (whose elements are identified by a rotation angle $\theta$).

\paragraph{Equivalent representations}
Two representations $\rho$ and $\rho'$ on $\R^n$ are called \textit{equivalent} iff they are related by a change of basis $Q\in\GL{\R^n}$, i.e. $\rho'(g) = Q \rho(g) Q^{-1}$ for each $g\in G$.
Equivalent representations behave similarly since their composition is basis independent as seen by $\rho'(g_1)\rho'(g_2) = Q\rho(g_1)Q^{-1}Q\rho(g_2)Q^{-1} = Q\rho(g_1)\rho(g_2)Q^{-1}$.

\paragraph{Direct sums}
Two representations can be combined by taking their \textit{direct sum}.
Given representations $\rho_1:G\to\GL{\R^n}$ and $\rho_2:G\to\GL{\R^m}$, their direct sum $\rho_1\oplus\rho_2:G\to\GL{\R^{n+m}}$ is~defined~as
\[
    (\rho_1 \oplus \rho_2)(g) = 
    \begin{bmatrix}
    \rho_1(g) & 0 		 \\
    0  		  & \rho_2(g)\\
    \end{bmatrix}\,,
\]
i.e. as the direct sum of the corresponding matrices.
Its action is therefore given by the independent actions of $\rho_1$ and $\rho_2$ on the orthogonal subspaces $\R^n$ and $\R^m$ in $\R^{n+m}$.
The direct sum admits an obvious generalization to an arbitrary number of representations $\rho_i$:
\[
    \bigoplus\nolimits_i \rho_i(g)\ =\ \rho_1(g) \oplus \rho_2(g) \oplus \dots
\]

\paragraph{Irreducible representations}
The action of a representation might leave a subspace of the representation space invariant.
If this is the case there exists a change of basis to an equivalent representation which is decomposed into the direct sum of two independent representations on the invariant subspace and its orthogonal complement.
A representation is called \textit{irreducible} if no non-trivial invariant subspace exists.

Any representation $\rho: G \to \R^n$ of a compact group $G$ can therefore be decomposed as
\[
    \rho(g) =
    Q
    \left[
    \bigoplus\nolimits_{i\in I}\psi_i(g)
    \right]
    Q^{-1}
\]
where $I$ is an index set specifying the irreducible representations $\psi_i$ contained in $\rho$ and $Q$ is a change of basis.
In proofs it is therefore often sufficient to consider irreducible representations which we use in Section~\ref{sec:irrep_decomposition} to solve the kernel constraint.

\paragraph{Regular and quotient representations}
A commonly used representation in equivariant deep learning is the \textit{regular representation}.
The regular representation of a finite group $G$ acts on a vector space $\R^{|G|}$ by permuting its axes.
Specifically, associating each axis $e_g$ of $\R^{|G|}$ to an element $g\in G$, the representation of an element $\tilde{g}\in G$ is a permutation matrix which maps $e_g$ to $e_{\tilde{g}g}$.
For instance, the regular representation of the group $\C4$ with elements $\{p {\pi\over 2} | p=0,\dots,3 \}$ is instantiated by:
\begin{center}
\setlength{\tabcolsep}{8pt}\renewcommand\arraystretch{1.4}\begin{tabular}{c|cccc}
    $\phi$ & $0$ & $\frac{\pi}{2}$ & $\pi$ & $\frac{3\pi}{2}$ \\ \hline
    $\rho_\text{reg}^{\C4}(\phi)$ 
    {\def\arraystretch{1.5} $\vphantom{\begin{bmatrix} c\\r\\a\\p \end{bmatrix}}$} &
    {\def\arraystretch{1.25}$\begin{bmatrix}
        1 & 0 & 0 & 0 \\
        0 & 1 & 0 & 0 \\
        0 & 0 & 1 & 0 \\
        0 & 0 & 0 & 1 \\
    \end{bmatrix}$}
    &
    {\def\arraystretch{1.25}$\begin{bmatrix}
        0 & 0 & 0 & 1 \\
        1 & 0 & 0 & 0 \\
        0 & 1 & 0 & 0 \\
        0 & 0 & 1 & 0 \\
    \end{bmatrix}$}
    &
    {\def\arraystretch{1.25}$\begin{bmatrix}
        0 & 0 & 1 & 0 \\
        0 & 0 & 0 & 1 \\
        1 & 0 & 0 & 0 \\
        0 & 1 & 0 & 0 \\
    \end{bmatrix}$}
    &
    {\def\arraystretch{1.25}$\begin{bmatrix}
        0 & 1 & 0 & 0 \\
        0 & 0 & 1 & 0 \\
        0 & 0 & 0 & 1 \\
        1 & 0 & 0 & 0 \\
    \end{bmatrix}$}
\end{tabular}
\end{center}
A vector $v=\sum_g v_g e_g$ in $\R^{|G|}$ can be interpreted as a scalar function $v:G\to\R,\,g\mapsto v_g$ on $G$.
Since $\rho(h)v = \sum_g v_g e_{hg} = \sum_{\tilde{g}} v_{h^{-1}\tilde{g}} e_{\tilde{g}}$ the regular representation corresponds to a left translation $[\rho(h)v](g)=v_{h^{-1}g}$ of such functions.

Very similarly, the quotient representation $\rho_\text{quot}^{G/H}$ of $G$ w.r.t. a subgroup $H$ acts on $\R^{|G|/|H|}$ by permuting its axes.
Labeling the axes by the cosets $gH$ in the quotient space $G/H$, it can be defined via its action $\rho_\text{quot}^{G/H}(\tilde{g})e_{gH}=e_{\tilde{g}gH}$.
An intuitive explanation of quotient representations is given in \apx~\ref{apx:quotient_models}.

Regular and trivial representations are two specific cases of quotient representations obtained by choosing $H=\{e\}$ or $H=G$, respectively.
Vectors in the representation space $\R^{|G|/|H|}$ can be viewed as scalar functions on the quotient space $G/H$.
The action of the quotient representations on $v$ then corresponds to a left translation of these functions on $G/H$.



\paragraph{Restricted representations}

Any representation $\rho:G\to\GL{\R^n}$ can be uniquely restricted to a representation of a subgroup $H$ of $G$ by restricting its domain of definition:
\[
    \Res{H}{G}(\rho): H \to \GL{{\R}^n},\ h\mapsto\rho\big|_H(h)
\]


\paragraph{Induced Representations}

Instead of restricting a representation from a group $G$ to a subgroup $H\leq G$, it is also possible to \textit{induce} a representation of $H$ to a representation of $G$.
In order to keep the presentation accessible we will first only consider the case of finite groups $G$ and $H$.

Let $\rho:H\to\GL{\R^n}$ be any representation of a subgroup $H$ of $G$.
The induced representation $\Ind{H}{G}(\rho)$ is then defined on the representation space $\R^{n|G|/|H|}$ which can be seen as one copy of $\R^n$ for each of the $|G|/|H|$ cosets $gH$ in the quotient set $G/H$.
For the definition of the induced representation it is customary to view this space as the tensor product $\R^{|G|/|H|}\otimes\R^n$ and to write vectors in this space as\footnote{The vector can equivalently be expressed as $w=\bigoplus_{gH}w_{gH}$, however, we want to make the tensor product basis explicit.}
\begin{align}\label{eq:induced_vector_def}
    w = \sum_{gH} e_{gH} \otimes w_{gH} \ \in{\R}^{n{|G|\over|H|}}\,,
\end{align}
where $e_{gH}$ is a basis vector of $\R^{|G|/|H|}$, associated to the coset $gH$, and $w_{gH}$ is some vector in the representation space $\R^n$ of $\rho$.
Intuitively, $\Ind{H}{G}(\rho)$ acts on $\R^{n|G|/|H|}$ by $i)$ permuting the $|G|/|H|$ subspaces associated to the cosets $gH$ and $ii)$ acting on each of these subspaces via $\rho$.

To formalize this intuition, note that any element $g\in G$ can be identified by the coset $gH$ to which it belongs and an element $\text{h}(g)\in H$ which specifies its position within this coset.
Hereby $\text{h}:G\to H$ expresses $g$ relative to an arbitrary \textit{representative}\footnote{
    Formally, a representative for each coset is chosen by a map $\mathcal{R}:G/H\to G$ such that it projects back to the same coset, i.e. $\mathcal{R}(gH)H=gH$.
    This map is therefore a \textit{section} of the principal bundle $G\overset{\pi}{\rightarrow}G/H$ with fibers isomorphic to $H$ and the projection given by $\pi(g):=gH$.
}
$\mathcal{R}(gH)\in G$ of $gH$ and is defined as $\text{h}(g) := \mathcal{R}(gH)^{-1}g$ from which it immediately follows that $g$ is decomposed relative to $\mathcal{R}$ as
\begin{align}
\label{eq:g_decomposition_h-fct}
    g=\mathcal{R}(gH) \text{h}(g) \,.
\end{align}

The action of an element $\tilde{g} \in G$ on a coset $gH \in G/H$ is naturally given by $\tilde{g}gH \in G/H$.
This action defines the aforementioned permutation of the $n$-dimensional subspaces in $\R^{n|G|/|H|}$ by sending $e_{gH}$ in Eq.~\eqref{eq:induced_vector_def} to $e_{\tilde{g}gH}$.
Each of the $n$-dimensional, translated subspaces $\tilde{g}gH$, is in addition transformed by the action of $\rho\big(\text{h}(\tilde{g}\mathcal{R}(gH))\big)$.
This $H$-component $\text{h}(\tilde{g}\mathcal{R}(gH))=\mathcal{R}(\tilde{g}gH)^{-1}\tilde{g}\mathcal{R}(gH)$ of the \mbox{$\tilde{g}$ action} within the cosets accounts for the relative choice of representatives $\mathcal{R}(\tilde{g}gH)$ and $\mathcal{R}(gH)$.
Overall, the action of $\Ind{H}{G}(\rho(\tilde{g}))$ is given by
\begin{align}\label{eq:induced_rep_finite_def}
    \left[\Ind{H}{G} \rho \right]\!\!(\tilde{g}) \sum_{gH} e_{gH} \otimes w_{gH}\ :=\ \sum_{gH} e_{\tilde{g}gH} \otimes \rho\big(\text{h}(\tilde{g}\mathcal{R}(gH))\big)\, w_{gH} \,,
\end{align}
which can be visualized as:
\newcommand\smallstrut{\rule[-0pt]{0pt}{12pt}}
\[
\\
\Ind{H}{G}\rho (\tilde{g})
\cdot
\left[
{\setlength\arraycolsep{3pt}
    \def\arraystretch{1.5}
    \begin{array}{c}
    \vdots \\
    \hline
     w_{gH} \vphantom{\vdots} \\
    \hline
    \vdots \\
    \hline
    \vdots \\
    \hline
    \vdots \\
    \end{array}
}
\right]
\quad
=
\quad
\left[
{\setlength\arraycolsep{3pt}
    \def\arraystretch{1.5}
    \begin{array}{cc}
    \vdots \\
    \hline
    \vdots \\
    \hline
    \vdots \\
    \hline 
        \rho(\text{h}(\tilde{g}\mathcal{R}(gH))) w_{gH} \vphantom{\vdots} \\
    \hline
    \vdots \\
    \end{array}
}
\right]
\!\!
{\setlength\arraycolsep{3pt}
    \def\arraystretch{1.5}
    \begin{array}{l}
    \vphantom{\vdots} \\
    \arrayrulecolor{white}
    \hline
    \left. \!\smallstrut \right\} gH \\
\arrayrulecolor{white}
    \hline
    \vphantom{\vdots} \\
    \arrayrulecolor{white}
    \hline
    \left. \!\smallstrut \right\} \tilde{g}gH = \tilde{g}\mathcal{R}(gH)H \\
\arrayrulecolor{white}
    \hline
    \vphantom{\vdots} \\
    \end{array}
}
\]

Both quotient representations and regular representations can be viewed as being induced from trivial representations of a subgroup.
Specifically, let $\rho_\text{triv}^{\{e\}}:\{e\}\to\GL{R}=\{(+1)\}$ be the trivial representation of the the trivial subgroup.
Then $\Ind{\{e\}}{G}\rho_\text{triv}^{\{e\}}:G\to\GL{{\R}^{|G|}}$ is the regular representation which permutes the cosets $g\{e\}$ of $G/\{e\}\cong G$ which are in one to one relation to the group elements themself.
For $\rho_\text{triv}^H:H\to\GL{\R}=\{(+1)\}$ being the trivial representation of an arbitrary subgroup $H$ of $G$, the induced representation $\Ind{H}{G}\rho_\text{triv}^H:G\to\GL{{\R}^{|G|/|H|}}$ permutes the cosets $gH$ of $H$ and thus coincides with the quotient representation $\rho_\text{quot}^{G/H}$.

Note that a vector in $\R^{|G|/|H|}\otimes\R^n$ is in one-to-one correspondence to a function $f:G/H\to\R^n$.
The induced representation can therefore equivalently be defined as acting on the space of such functions as\footnote{
    The rhs. of Eq.~\eqref{eq:induced_rep_finite_def} corresponds to
    $[\Ind{H}{G}\rho(\tilde{g}) \cdot f](\tilde{g}gH) = \rho(\text{h}(\tilde{g} \mathcal{R}(gH))) f(gH)$.
}
\begin{align}
\label{eq:induced_field}
    [\Ind{H}{G}\rho(\tilde{g}) \cdot f](gH) = \rho(\text{h}(\tilde{g} \mathcal{R}(\tilde{g}^{-1}gH))) f(\tilde{g}^{-1}gH) \,.
\end{align}
This definition generalizes to non-finite groups where the quotient space $G/H$ is not necessarily finite anymore.

For the special case of semidirect product groups $G=N\rtimes H$ it is possible to choose representatives of the cosets $gH$ such that the elements $\text{h}(\tilde{g}\mathcal{R}(g'H))=\text{h}(\tilde{g})$ become independent of the cosets~\cite{Cohen2018-IIR}.
This simplifies the action of the induced representation to
\begin{align}
\label{eq:induced_field_semidirect}
    [\Ind{H}{G}\rho(\tilde{g}) \cdot f](gH) = \rho(\text{h}(\tilde{g}))\, f(\tilde{g}^{-1}gH)
\end{align}
which corresponds to Eq.~\eqref{eq:induced_rep_translations} for the group $G=\E2=(\R^2,+)\rtimes\O2$, subgroup $H=\O2$ and quotient space $G/H=\E2/\O2=\R^2$.
 	
\section{An intuition for quotient representation fields}
\label{apx:quotient_models}
The quotient representations of $\CN$ in rows 11-15 of Table~\ref{tab:mnist_comparison} and in Table~\ref{tab:mnist_final} are all of the form $\rho_\text{quot}^{\CN\!\!/\!\C{M}}$ with $\C{M}\leq\CN$.
By the definition of quotient representations, given in Section~\ref{sec:representations}, this implies features which are \textit{invariant} under the action of $\C{M}$.
For instance, $\rho_\text{quot}^{\CN\!\!/\!\C2}$-fields encode features like lines, which are invariant under rotations by $\pi$.
Similarly, $\rho_\text{quot}^{\CN\!\!/\!\C4}$ features are invariant under rotations by $\pi/2$, and therefore describe features like a cross.
The $N/M$ channels of a $\rho_\text{quot}^{\CN\!\!/\!\C{M}}$-field respond to different orientations of these patterns, e.g. to $+$ and $\times$ for the two channels of $\rho_\text{quot}^{\C8\!\!/\!\C4}$.
A~few~more examples are given by the $16/2=8$ channels of $\rho_\text{quot}^{\C{16}\!\!/\!\C2}$, which respond to the patterns
$$
\mathbin{\rotatebox[origin=c]{  0  }{$\scaleobj{1.2}{-}$}},
\mathbin{\rotatebox[origin=c]{ 22.5}{$\scaleobj{1.2}{-}$}},
\mathbin{\rotatebox[origin=c]{ 45  }{$\scaleobj{1.2}{-}$}},
\mathbin{\rotatebox[origin=c]{ 67.5}{$\scaleobj{1.2}{-}$}},
\mathbin{\rotatebox[origin=c]{ 90  }{$\scaleobj{1.2}{-}$}},
\mathbin{\rotatebox[origin=c]{112.5}{$\scaleobj{1.2}{-}$}},
\mathbin{\rotatebox[origin=c]{135  }{$\scaleobj{1.2}{-}$}}\ \text{and}\ 
\mathbin{\rotatebox[origin=c]{157.5}{$\scaleobj{1.2}{-}$}},
$$
respectively, or the $16/4=4$ channels of $\rho_\text{quot}^{\C{16}\!\!/\!\C4}$, which respond to
$$
\mathbin{\rotatebox[origin=c]{  0  }{$\scaleobj{1.2}{+}$}},
\mathbin{\rotatebox[origin=c]{ 22.5}{$\scaleobj{1.2}{+}$}},
\mathbin{\rotatebox[origin=c]{ 45  }{$\scaleobj{1.2}{+}$}}\ \text{and}\ 
\mathbin{\rotatebox[origin=c]{ 67.5}{$\scaleobj{1.2}{+}$}}.
$$

In principle, each of these patterns\footnote{Or more generally, \textit{any} possible pattern.}
could be encoded by a \textit{regular} feature field of $\CN$.
A regular field of type $\rho_\text{reg}^{\CN}$ comprises $N$ instead of $N/M$ channels, which detect arbitrary patterns in $N$ orientations, for instance,
$$
\mathbin{\rotatebox[origin=c]{  0}{$\scaleobj{1.2}{\rtimes}$}},
\mathbin{\rotatebox[origin=c]{ 45}{$\scaleobj{1.2}{\rtimes}$}},
\mathbin{\rotatebox[origin=c]{ 90}{$\scaleobj{1.2}{\rtimes}$}},
\mathbin{\rotatebox[origin=c]{135}{$\scaleobj{1.2}{\rtimes}$}},
\mathbin{\rotatebox[origin=c]{180}{$\scaleobj{1.2}{\rtimes}$}},
\mathbin{\rotatebox[origin=c]{225}{$\scaleobj{1.2}{\rtimes}$}},
\mathbin{\rotatebox[origin=c]{270}{$\scaleobj{1.2}{\rtimes}$}}\ \text{and}\ 
\mathbin{\rotatebox[origin=c]{315}{$\scaleobj{1.2}{\rtimes}$}}
$$
for $N=8$.
In the case of $\C{M}$-\textit{symmetric} patterns, e.g. crosses for $M=4$, this becomes
$$
\mathbin{\rotatebox[origin=c]{  0}{$\scaleobj{1.2}{\times}$}},
\mathbin{\rotatebox[origin=c]{ 45}{$\scaleobj{1.2}{\times}$}},
\mathbin{\rotatebox[origin=c]{ 90}{$\scaleobj{1.2}{\times}$}},
\mathbin{\rotatebox[origin=c]{135}{$\scaleobj{1.2}{\times}$}},
\mathbin{\rotatebox[origin=c]{180}{$\scaleobj{1.2}{\times}$}},
\mathbin{\rotatebox[origin=c]{225}{$\scaleobj{1.2}{\times}$}},
\mathbin{\rotatebox[origin=c]{270}{$\scaleobj{1.2}{\times}$}}\ \text{and}\ 
\mathbin{\rotatebox[origin=c]{315}{$\scaleobj{1.2}{\times}$}}.
$$
As evident from this example, the repetition after $N/M$ orientations (here $8/4=2$), introduces a redundancy in the responses of the regular feature fields.
A quotient representation $\rho_\text{quot}^{\CN\!\!/\!\C{M}}$ addresses this redundancy by a-priori assuming the $\C{M}$ symmetry to be present and storing only the $N/M$ non-redundant responses.
If symmetric patterns are important for the learning task, a quotient representation can therefore save computational, memory and parameter cost.

In our experiments we mostly used quotients by $\C2$ and $\C4$ since we assumed the corresponding symmetric patterns ($\,|$ and $+$) to be most frequent in MNIST.
As hypothesized, our model, which uses the representations
$5\rho_\text{reg}\!\oplus2\rho_\text{quot}^{\nicefrac{\C{16}}{\C{2}}}\!\oplus2\rho_\text{quot}^{\nicefrac{\C{16}}{\C{4}}}\!\oplus4\psi_0$
of $\C{16}$, improves slightly upon a purely regular model with the same number of parameters, see Table~\ref{tab:mnist_final}.
By mixing regular, quotient and trivial\footnote{Trivial representations $\psi^0\cong\rho_\text{quot}^{G/G}$ can themself be seen as an extreme case of quotient representations which are invariant to the full group $G$.}
representations, our model keeps a certain level of expressiveness in its feature fields but incorporates a-priori known symmetries and compresses the model.

We want to emphasize that quotient representations are expected to severely harm the model performance if the assumed symmetry does not actually exist in the data or is unimportant for the inference.
Since the space of possible quotient representations and their multiplicities is very large, it might be necessary to apply some form of \textit{neural architecture search} to find beneficial combinations.
As a default choice we recommend the user to work with regular representations.

Further, note that the intuition given above is specific for the case of quotient representations $\rho_\text{quot}^{G\!/\!N}$ where $N\trianglelefteq G$ is a \textit{normal} subgroup (which is always the case for $\CN$).
Since normal subgroups imply $gN\!=\!Ng\ \ \forall g\!\in\!G$ by definition, the action of the quotient representation by any element $n\in N$ is given by 
$\rho_\text{quot}^{G\!/\!N}(n)e_{gN} = e_{ngN} = e_{nNg} = e_{Ng} = e_{gN}$, that it, it describes $N$-\textit{invariant} feature fields.
The quotient representations $\rho_\text{quot}^{G\!/\!H}$ for general, potentially non-normal subgroups $H\leq G$ also imply certain symmetries in the feature fields but are not necessarily $H$-invariant.
For instance, the quotient representation $\rho_\text{quot}^{\DN\!/\!\CN}$ is invariant under rotations since $\CN$ is a normal subgroup of $\DN\cong\CN\rtimes\Flip$, while the quotient representation $\rho_\text{quot}^{\DN\!/\Flip}$ is not invariant since $\Flip$ is not a normal subgroup of $\DN$.
In the latter case one has instead
\[
    \rho_\text{quot}^{\DN\!/\Flip}(s) e_{r\Flip} = e_{sr\Flip} = 
    \begin{cases}
        e_{r\Flip}                         & \text{for }\ s=+1 \\
        e_{r^{-1}s\Flip} = e_{r^{-1}\Flip} & \text{for }\ s=-1
    \end{cases}
\]
for all $s\in\Flip$ and representatives $r\in\CN$.
The feature fields are therefore not invariant under the action of $\Flip$ but become reversed.


 	
\section{Equivariance of E(2)\,-\,steerable CNNs}

\subsection{Equivariance of E(2)\,-\,steerable convolutions}
\label{apx:equivariance_conv}

Assume two feature fields $f_\text{in}:\R^2\to\R^{c_\text{in}}$ of type $\rho_\text{in}$ and $f_\text{out}:\R^2\to\R^{c_\text{out}}$ of type $\rho_\text{out}$ to be given.
Under actions of the Euclidean group these fields transform as
\begin{align*}
    f_\text{in}(x)
    \ &\mapsto\ 
    \left(\left[\operatorname{Ind}_{G}^{(\R^2,+)\rtimes G}\,\rho_\text{in}\ \right](gt)f_\text{in}\right)(x)
    \ \ :=\ 
    \ \, \rho_\text{in}(g)\,f_\text{in}\left(g^{-1}(x-t)\right)
    \\[3pt]
    f_\text{out}(x)
    \ &\mapsto\ 
    \left(\left[\operatorname{Ind}_{G}^{(\R^2,+)\rtimes G}\rho_\text{out}\right](gt)f_\text{out}\right)(x)
    \ :=\ 
    \rho_\text{out}(g)f_\text{out}\left(g^{-1}(x-t)\right) \,.
\end{align*}
Here we show that the $G$-steerability \eqref{eq:kernel_constraint} of convolution kernels is \emph{sufficient} to guarantee the equivariance of the mapping.
We therefore define the convolution (or correlation) operation of a feature field with a $G$-steerable kernel $k:\R^2\to\R^{c_\text{out}\times c_\text{in}}$ as usual by
\[
    f_\text{out}(x)
    \ :=\ \left(k\ast f_\text{in}\right)(x)
    \  =\ \int_{\R^2} k(y)f_\text{in}(x+y) \,\operatorname{d}\!y \,.
\]

The convolution with a transformed input field then gives
\begin{align*}
    &
    \int_{\R^2} \operatorname{d}\!y\ k(y)
    \left(\left[\operatorname{Ind}_{G}^{(\R^2,+)\rtimes G}\rho_\text{in}\right]\!(gt)\,f_\text{in}\right)(x+y) \\
    =\ \ &
    \int_{\R^2} \operatorname{d}\!y\ k(y)
    \rho_\text{in}(g)f_\text{in}\left(g^{-1}(x+y-t)\right) \\
    =\ \ &
    \int_{\R^2} \operatorname{d}\!y\ 
    \rho_\text{out}(g)k(g^{-1}y)\rho_\text{in}(g)^{-1}\ 
    \rho_\text{in}(g)f_\text{in}\left(g^{-1}(x+y-t)\right) \\
    =\ \ &
    \rho_\text{out}(g)
    \int_{\R^2} \operatorname{d}\!\tilde{y}\ k(\tilde{y})
    f_\text{in}\left(g^{-1}(x-t)+\tilde{y}\right) \\
    =\ \ &
    \rho_\text{out}(g) f_\text{out}\left(g^{-1}(x-t)\right) \\
    =\ \ &
    \left(\left[\operatorname{Ind}_{G}^{(\R^2,+)\rtimes G}\rho_\text{out}\right](gt)f_\text{out}\right)(x) \,,
\end{align*}
i.e. it satisfies the desired equivariance condition
\[
    k\ast \left(\left[\operatorname{Ind}_{G}^{(\R^2,+)\rtimes G}\rho_\text{in}\right]\!(gt) f_\text{in}\right)
    \ =\ \left[\operatorname{Ind}_{G}^{(\R^2,+)\rtimes G}\rho_\text{out}\right]\!(gt) \big(k\ast f_\text{in}\big) \,.
\]
We used the kernel steerability \eqref{eq:kernel_constraint} in the second step to identify $k(x)$ with $\rho_\text{out}(g)k(g^{-1}x)\rho_\text{in}(g^{-1})$.
In the third step we substituted $\tilde{y}=g^{-1}y$ which does not affect the integral measure since
$\left|\det\left(\frac{\partial y}{\partial\tilde{y}}\right)\right| = \left|\det(g)\right| = 1$
for an orthogonal transformation $g\in G$.

A proof showing the $G$-steerability of the kernel to not only be sufficient but necessary is given in \cite{3d_steerableCNNs}.









\vspace*{4ex}

\subsection{Equivariance of spatially localized nonlinearities}
\label{apx:equivariance_nonlin}

We consider nonlinearities of the form
\[
    \sigma:\mathbb{R}^{c_\text{in}}\to\mathbb{R}^{c_\text{out}},\ f(x)\mapsto\sigma\big(f(x)\big) \,,
\]
which act spatially localized on feature vectors $f(x)\in\mathbb{R}^{c_\text{in}}$.
These localized nonlinearities are used to define nonlinearities $\bar{\sigma}$ acting on entire feature fields $f:\mathbb{R}^2\to\mathbb{R}^{c_\text{in}}$ by mapping each feature vector individually, that is,
\[
    \bar\sigma:\ f\mapsto\bar\sigma(f) \quad\text{such that}\quad \bar\sigma(f)(x):=\sigma\big(f(x)\big) \,.
\]
In order for $\bar\sigma$ to be equivariant under the action of induced representations it is sufficient to require
\[
    \sigma\circ\rho_\text{in}(g)\ =\ \rho_\text{out}(g)\circ\sigma \qquad\forall g\in G
\]
since then
\begin{align*}
    \bar\sigma\left( \left[\operatorname{Ind}_G^{(\mathbb{R}^2,+)\rtimes G}\rho_\text{in}\right](gt)f \right)(x)
    \ =&\ \sigma\left( \rho_\text{in}(g) f(g^{-1}(x-t)) \right) \\
    \ =&\ \rho_\text{out}(g) \sigma\left( f(g^{-1}(x-t)) \right) \\
    \ =&\ \rho_\text{out}(g) \bar\sigma\big(f\big)(g^{-1}(x-t)) \\
    \ =&\ \left[\operatorname{Ind}_G^{(\mathbb{R}^2,+)\rtimes G}\rho_\text{out}\right] \bar\sigma(f)(x) \,.
\end{align*}






\subsubsection{Equivariance of individual subspace nonlinearities w.r.t. direct sum representations}
\label{apx:equivariance_direct_sum}

The feature spaces of steerable CNNs comprise multiple feature fields $f_i:\R^2\to\R^{c_{\text{in},i}}$ which are concatenated into one big feature field $f:\R^2\to\R^{\sum_i c_{\text{in},i}}$ defined by $f:=\bigoplus_i f_i$.
By definition $f(x)$ transforms under $\rho_\text{in}=\bigoplus_i\rho_{\text{in},i}$ if each $f_i(x)$ transforms under $\rho_{\text{in},i}$.
If $\sigma_i:\R^{c_{\text{in},i}}\to\R^{c_{\text{out},i}}$ is an equivariant nonlinearity satisfying $\sigma_i\circ\rho_{\text{in},i}(g) = \rho_{\text{out},i}(g)\circ\sigma_i$ for all $g\in G$, then
\[
      \left(\bigoplus\nolimits_i\sigma_i\right)          \circ \left(\bigoplus\nolimits_i\rho_{\text{in},i}(g)\right)
    = \left(\bigoplus\nolimits_i\rho_{\text{out},i}(g)\right) \circ \left(\bigoplus\nolimits_i\sigma_i\right)
    \quad\forall g\in G,
\]
i.e. the concatenation of feature fields respects the equivariance of the individual nonlinearities.
Here we defined $\bigoplus_i\sigma_i:\R^{\sum_i c_{\text{in},i}} \to \R^{\sum_i c_{\text{out},i}}$ as acting individually on the corresponding $f_i(x)$ in $f(x)$, that is,
$\left(\bigoplus_i\sigma_i\right)\left(\bigoplus_i f_i(x)\right) = \bigoplus_i \left(\sigma_i\circ\operatorname{proj}_i\right)(f(x))$.

To proof this statement consider without loss of generality the case of two feature fields $f_1$ and $f_2$ with corresponding representations $\rho_1$ and $\rho_2$ and equivariant nonlinearities $\sigma_1$ and $\sigma_2$.
Then it follows for all $g\in G$ that
\begin{align*}
         &\big(\sigma_1\oplus\sigma_2\big) \big((\rho_{1,\text{in}}\oplus\rho_{2,\text{in}})(g)\big) \big(f_1\oplus f_2\big) \\
    =\ \ &\sigma_1\left(\rho_1(g)\,f_1\right) \oplus \sigma_2\left(\rho_2(g)\,f_2\right) \\
    =\ \ &\rho_1(g)\,\sigma_1\!\left(f_1\right) \,\oplus \rho_2(g)\,\sigma_2\!\left(f_2\right) \\
    =\ \ &\big((\rho_{1,\text{out}}\oplus\rho_{2,\text{out}})(g)\big) \big(\sigma_1\oplus\sigma_2\big) \big(f_1\oplus f_2\big) \,.
\end{align*}
The general case follows by induction.








\subsubsection{Equivariance of norm nonlinearities w.r.t. unitary representations}
\label{apx:equivariance_nonlin_norm}

We define unitary representations to preserve the norm of feature vectors, i.e.
$\big|\rho_\text{iso}(g)f(x)\big| = \big|f(x)\big|\ \forall g\in G$.
Norm nonlinearities are functions of the type
$\sigma_\text{norm}\big(f(x)\big) := \eta\big(|f(x)|\big) \frac{f(x)}{|f(x)|}$,
where $\eta:\R_{\geq0}\to\R$ is some nonlinearity acting on norm of a feature vector.
Since norm nonlinearities preserve the orientation of feature vectors they are equivariant under the action of unitary representations:
\begin{align*}
    \sigma_\text{norm}\big( \rho_\text{iso}(g) f(x) \big)
    \ =&\ \eta\left(\big|\rho_\text{iso}(g)f(x)\big|\right) \frac{\rho_\text{iso}(g)f(x)}{\big|\rho_\text{iso}(g)f(x)\big|} \\
    \ =&\ \eta\left(\big|f(x)\big|\right) \frac{\rho_\text{iso}(g)f(x)}{\big|f(x)\big|} \\
    \ =&\ \rho_\text{iso}(g) \sigma_\text{norm}\big( f(x) \big) \,.
\end{align*}








\subsubsection{Equivariance of pointwise nonlinearities w.r.t. regular and quotient representations}
\label{apx:equivariance_nonlin_quotient}

Quotient representations act on feature vectors by permuting their entries according to the group composition as defined by
$\rho_\text{quot}^{G/H}(\tilde{g}) e_{gH} := e_{\tilde{g}gH}$.
The permutation of vector entries commutes with pointwise nonlinearities $\sigma:\mathbb{R}\to\mathbb{R}$ which are being applied to each entry of a feature vector individually:
\begin{align*}
    \sigma_\text{pt}\big( \rho_\text{quot}^{G/H}(\tilde{g})\, f(x) \big)
    \ =&\ \sigma_\text{pt}\left( \rho_\text{quot}^{G/H}(\tilde{g}) \sum_{gH\in G/H} f_{gH}(x)\, e_{gH} \right) \\
    \ =&\ \sigma_\text{pt}\left( \sum_{gH\in G/H} f_{gH}(x)\, e_{\tilde{g}gH} \right) \\
    \ =&\ \sum_{gH\in G/H} \sigma_\text{pt}\big(f_{gH}(x)\big)\, e_{\tilde{g}gH} \\
    \ =&\ \rho_\text{quot}^{G/H}(\tilde{g}) \sum_{gH\in G/H} \sigma_\text{pt}\big(f_{gH}(x)\big)\, e_{gH} \\
    \ =&\ \rho_\text{quot}^{G/H}(\tilde{g})  \sigma_\text{pt}\big(f(x)\big) \,.
\end{align*}
Any pointwise nonlinearity is therefore equivariant under the action of quotient representations.
The same holds true for regular representations $\rho_\text{reg}^G=\rho_\text{quot}^{G/\{e\}}$ which are a special case of quotient representations for the choice $H=\{e\}$







\subsubsection{Equivariance of vector field nonlinearities w.r.t. regular and standard representations}
\label{apx:equivariance_nonlin_vector}


Vector field nonlinearities map an $N$-dimensional feature field which is transforming under the regular representation $\rho_\text{reg}^{\CN}$ of $\CN$ to a vector field.
As the elements of $\CN$ correspond to rotations by angles $\theta_p\in\left\{p\frac{2\pi}{N}\right\}_{p=0}^{N-1}$ we can write the action of the cyclic group in this specific case as
$\rho_\text{reg}^{\CN}(\tilde{\theta}) e_\theta := e_{\tilde{\theta}+\theta}$
and the feature vector as
$f(x)=\sum\limits_{\theta\in\CN} f_\theta(x)e_\theta$.
In this convention vector field nonlinearities are defined as
\[
    \sigma_\text{vec}\big(f(x)\big)\ :=\ \max\big(f(x)\big) \begin{pmatrix} \cos(\arg\max f(x)) \\ \sin(\arg\max f(x)) \end{pmatrix} \,.
\]
The maximum operation $\max:\R^N\to\R$ thereby returns the maximal field value which is invariant under transformations of the regular input field.
Observe that
\begin{align*}
         \arg\max\left(\rho_\text{reg}^{\CN}(\tilde{\theta}) f(x)\right)
    \ &=\ \arg\max\left(\rho_\text{reg}^{\CN}(\tilde{\theta}) \sum_{\theta\in\CN} f_\theta(x)e_\theta \right) \\
    \ &=\ \arg\max\left(\sum_{\theta\in\CN} f_\theta(x)e_{\tilde{\theta}+\theta} \right) \\
    \ &=\ \tilde{\theta} + \arg\max\left(f(x) \right)
\end{align*}
such that
\begin{align*}
    \sigma_\text{vec}\big(\rho_\text{reg}^{\CN}(\tilde{\theta}) f(x)\big)
    \ &=\ \max\big(f(x)\big)
            \begin{pmatrix}
                \cos(\tilde{\theta} + \arg\max f(x)) \\
                \sin(\tilde{\theta} + \arg\max f(x))
            \end{pmatrix} \\
    \ &=\ \max\big(f(x)\big)
            \begin{pmatrix}
                \cos(\tilde{\theta}) &          - \sin(\tilde{\theta}) \\
                \sin(\tilde{\theta}) & \phantom{-}\cos(\tilde{\theta})
            \end{pmatrix}
            \begin{pmatrix}
                \cos(\arg\max f(x)) \\
                \sin(\arg\max f(x))
            \end{pmatrix}
\end{align*}
transforms under the standard representation
$\rho(\tilde{\theta})=
            \begin{pmatrix}
                \cos(\tilde{\theta}) &          - \sin(\tilde{\theta}) \\
                \sin(\tilde{\theta}) & \phantom{-}\cos(\tilde{\theta})
            \end{pmatrix}
$
of $\CN$.
This proofs that the resulting feature field indeed transforms like a vector field.

The original paper \cite{Marcos2017-VFN} used a different convention $\arg\max:\R^N\to\{0,\dots,N\shortminus1\}$, returning the integer index of the maximal vector entry.
This leads to a corresponding rotation angle $\theta=\frac{2\pi}{N}\arg\max(f(x))\in\CN$ in terms of which the vector field nonlinearity reads
$\sigma_\text{vec}\big(f(x)\big) = \max\big(f(x)\big) \begin{pmatrix} \cos(\theta) \\ \sin(\theta) \end{pmatrix}$








\subsubsection{Equivariance of nonlinearities w.r.t. induced representations}
\label{apx:equivariance_nonlin_induced}

Consider a group $H < G$ with two representations $\rho_\text{in}: H \to \GL{\R^{c_\text{in}}}$ and $\rho_\text{out}: H \to \GL{\R^{c_\text{out}}}$.
Suppose we are given an equivariant non-linearity $\sigma: \R^{c_\text{in}} \to \R^{c_\text{out}}$ with respect to the actions of $\rho_\text{in}$ and $\rho_\text{out}$, that is,
$\rho_\text{out}(h)\circ\sigma\ =\ \sigma\circ\rho_\text{in}(h) \quad\forall h\in H$.
Then an \emph{induced} non-linearity $\tilde{\sigma}$, equivariant w.r.t. the induced representations $\Ind{H}{G}\rho_\text{in}$ and $\Ind{H}{G}\rho_\text{out}$ of $G$, can be defined as applying $\sigma$ independently to each of the $|G:H|$ different $c_\text{in}$-dimensional subspaces of the representation space which are being permuted by the action of $\Ind{H}{G}\rho_\text{in}$, see \apx~\ref{apx:repr_theory}. 
The permutation of the subspaces commutes with the individual action of the nonlinearity $\tilde{\sigma}$ on the subspaces, while the non-linearity $\sigma$ itself commutes with the transformation within the subspaces through $\rho$ by assumption.
Expressing the feature vector as $f(x) = \sum_{gH} e_{gH} \otimes f_{gH}(x)$ this is seen by:
\begin{align*}
          \tilde{\sigma} \big( \Ind{H}{G}\rho_\text{in}(\tilde{g})\, f(x) \big)
    \ =&\ \tilde{\sigma}\left( \Ind{H}{G}\rho_\text{in}(\tilde{g}) \sum_{gH\in G/H} e_{gH} \otimes f_{gH}(x)\right) \\
    \ =&\ \tilde{\sigma}\left( \sum_{gH\in G/H} \Ind{H}{G}\rho_\text{in}(\tilde{g}) \lp e_{gH} \otimes f_{gH}(x) \rp \right) \\
    \ =&\ \tilde{\sigma}\left( \sum_{gH\in G/H} e_{\tilde{g}gH} \otimes \rho_\text{in}(\text{h}(\tilde{g}r(gH))f_{gH}(x)\right) \\
    \ =&\ \sum_{gH\in G/H} e_{\tilde{g}gH} \otimes \sigma\big( \rho_\text{in}(\text{h}(\tilde{g}r(gH))f_{gH}(x)\big) \\
    \ =&\ \sum_{gH\in G/H} e_{\tilde{g}gH} \otimes \rho_\text{out}(\text{h}(\tilde{g}r(gH)) \sigma\big(f_{gH}(x)\big) \\
    \ =&\ \Ind{H}{G}\rho_\text{out}(\tilde{g})\, \sum_{gH\in G/H} e_{gH} \otimes \sigma\left(f_{gH}(x)\right) \\
    \ =&\ \Ind{H}{G}\rho_\text{out}(\tilde{g})\, \tilde{\sigma} \big( f(x) \big)
\end{align*}

 	
\section{Visualizations of the irrep kernel constraint}
\label{apx:constraint_decomposition}

The irrep kernel constraint
\[
    \kappa(gx)\ =\
    \left[\bigoplus\nolimits_{i\in I_\text{out}}\psi_i(g)\right]
    \ \kappa(x)\ 
    \left[\bigoplus\nolimits_{j\in I_\text{in}}\psi_j^{-1}(g)\right]
    \qquad \forall g\in G,\ x\in\mathbb{R}^2
\]
decomposes into independent constraints
\[
    \kappa^{ij}(gx)\ =\ \psi_i(g)\ \kappa^{ij}(x)\ \psi_j^{-1}(g)
    \qquad\forall g\in G,\ x\in\mathbb{R}^2
    \quad\text{where}\ \ i\in I_\text{out},\ j\in I_\text{in} \,,
\]
on invariant subspaces corresponding to blocks $\kappa^{ij}$ of $\kappa$.
This is the case since the direct sums of irreps on the right hand side are block diagonal:
\begin{equation*}
\vspace*{1ex}
\resizebox{.99\hsize}{!}{$
    \underbrace{
        \left(\!\!
        {\setlength\arraycolsep{3pt}
         \def\arraystretch{1.5}
        \begin{array}{c|c|c}
            \kappa^{i_1 j_1}\!(gx)\!\! & \kappa^{i_1 j_2}\!(gx)\!\! & \!\dots\!\! \\ \hline
            \kappa^{i_2 j_1}\!(gx)\!\! & \kappa^{i_2 j_2}\!(gx)\!\! & \!\dots\!\! \\ \hline \\[-2em]
            \vdots                     & \vdots                     & \ddots\!    \\
        \end{array}
        }
        \!\!\right)
        }_{
        \textstyle\begin{array}{c}
            \kappa(gx)
        \end{array}
    }
    =
    \underbrace{
        \left(\!\!
        {\setlength\arraycolsep{3pt}
         \def\arraystretch{1.5}
        \begin{array}{c|c|c}
            \psi_{i_1}\!(g)\!\! &                     &              \\ \hline
                                & \psi_{i_2}\!(g)\!\! &              \\ \hline \\[-2em]
                                &                     & \!\ddots\! \\
        \end{array}
        }
        \!\!\right)
        }_{
        \textstyle\begin{array}{c}
            \bigoplus_{i\in I_\text{out}} \psi_i(g)
        \end{array}
    }
        \!\cdot\!
    \underbrace{
        \left(\!\!
        {\setlength\arraycolsep{3pt}
         \def\arraystretch{1.5}
        \begin{array}{c|c|c}
            \kappa^{i_1 j_1}\!(x)\!\! & \kappa^{i_1 j_2}\!(x)\!\! & \!\dots\!\!\ \\ \hline
            \kappa^{i_2 j_1}\!(x)\!\! & \kappa^{i_2 j_2}\!(x)\!\! & \!\dots\!\!\ \\ \hline \\[-2em]
            \vdots                    & \vdots                    & \ddots\!     \\
        \end{array}
        }
        \!\!\right)
        }_{
        \textstyle\begin{array}{c}
            \kappa(x)
        \end{array}
    }
        \!\cdot\!
    \underbrace{
        \left(\!\!
        {\setlength\arraycolsep{3pt}
         \def\arraystretch{1.5}
        \begin{array}{c|c|c}
            \psi_{j_1}^{\shortminus1}\!(g)\!\! &                                    &            \\ \hline
                                               & \psi_{j_2}^{\shortminus1}\!(g)\!\! &            \\ \hline \\[-2em]
                                               &                                    & \!\ddots\! \\
        \end{array}
        }
        \!\!\right)
        }_{
        \textstyle\begin{array}{c}
            \bigoplus_{j\in I_\text{in}} \psi^{\shortminus1}_j(g)
        \end{array}
    }
$}
\end{equation*}

A basis $\big\{\kappa^{ij}_1, \cdots, \kappa^{ij}_{d_{ij}}\big\}$ for the space of $G$-steerable kernels satisfying the independent constraints~\eqref{eq:irrep_constraint} on $\kappa^{ij}$ contributes to a part of the full basis
\begin{align}
    \big\{k_1,\cdots,k_d\big\}\ :=\ 
    \bigcup\nolimits_{i\in I_\text{out}} \bigcup\nolimits_{j\in I_\text{in}} \left\{Q^{-1}_\text{out}\,\overline{\kappa}^{ij}_1 Q_\text{in},\, \cdots,\, Q^{-1}_\text{out}\,\overline{\kappa}^{ij}_{d_{ij}} Q_\text{in}\right\}.
\end{align}
of $G$-steerable kernels satisfying the original constraint~\eqref{eq:kernel_constraint}.
Here we defined a zero-padded block
\[
    \overline{\kappa}^{ij}
    \ :=\ 
    \left(
    {\setlength\arraycolsep{3pt}
     \def\arraystretch{1.5}
    \begin{array}{c|c|c}
        \ 0\ \ &\ 0          \ &\ \ 0\ \\ \hline
        \ 0\ \ &\ \kappa^{ij}\ &\ \ 0\ \\ \hline
        \ 0\ \ &\ 0          \ &\ \ 0\ \\
    \end{array}
    }
    \right).
\]

 	
\section{Solutions of the kernel constraints for irreducible representations}
\label{apx:kernel_constraint_solution_apx}


In this section we are deriving analytical solutions of the kernel constraints
\begin{align}\label{eq:irrep_constraint_apx}
    \kappa^{ij}(gx)\ =\ \psi_i(g)\ \kappa^{ij}(x)\ \psi_j^{-1}(g)
    \qquad\forall g\in G,\ x\in\mathbb{R}^2
\end{align}
for irreducible representations $\psi_i$ of $\O2$ and its subgroups.
The linearity of the constraint implies that the solution space of G-steerable kernels forms a linear subspace of the unrestricted kernel space
$k\in L^2\!\left(\R^2\right)^{c_\text{out}\times c_\text{in}}$
of square integrable functions $k:\R^2\to\R^{c_\text{out}\times c_\text{in}}$.

Section~\ref{apx:conventions} introduces the conventions, notation and basic properties used in the derivations.
Since our numerical implementation is on the real field we are considering real-valued irreps.
It is in general possible to derive all solutions considering complex valued irreps of $G\leq\O2$.
While this approach would simplify some steps it 
comes with an overhead of relating the final results back to the real field which leads to further complications, see \apx~\ref{apx:incompleteness_hnets}.
An overview over the real-valued irreps of $G\leq\O2$ and their properties is given in Section~\ref{apx:irreps}.

We present the analytical solutions of the irrep kernel constraints for all possible pairs of irreps in Section~\ref{apx:analytical_irrep_bases}.
Specifically, the solutions for $\SO2$ are given in Table~\ref{tab:SO2_irrep_solution_appendix} while the solutions for $\O2$, $\Flip$, $\CN$ and $\DN$ are given in
Table~\ref{tab:O2_irrep_solution_appendix},
Table~\ref{tab:Reflection_irrep_solution_appendix},
Table~\ref{tab:CN_irrep_solution_appendix} and
Table~\ref{tab:DN_irrep_solution_appendix} respectively.

Our derivation of the irrep kernel bases is motivated by the observation that the irreps of $\O2$ and subgroups are harmonics, that is, they are associated to one particular angular frequency.
This suggests that the kernel constraint \eqref{eq:irrep_constraint_apx} decouples into simpler constraints on individual Fourier modes.
In the derivations, presented in Section~\ref{apx:derivation_irrep_constraints}, we are therefore defining the kernels in polar coordinates $x=x(r,\phi)$ and expand them in terms of an orthogonal, angular, Fourier-like basis.
A projection on this orthogonal basis then yields constraints on the expansion coefficients.
Only specific coefficients are allowed to be non-zero; these coefficients parameterize the complete space of $G$-steerable kernels satisfying the irrep constraint~\eqref{eq:irrep_constraint_apx}.
The completeness of the solution follows from the completeness of the orthogonal basis.

We start with deriving the bases for the simplest cases $\SO2$ and $\Flip$ in sections~\ref{apx:derivation_irrep_constraint_SO2} and~\ref{apx:derivation_irrep_constraint_Flip}.
The $G$-steerable kernel basis for $\O2$ forms a subspace of the kernel basis for $\SO2$ such that it can be easily derived from this solution by adding the additional constraint coming from the reflectional symmetries in $\Flip\cong\O2/\SO2$.
This additional constraint is imposed in Section~\ref{apx:derivation_irrep_constraint_O2}.
Since $\CN$ is a subgroup of discrete rotations in $\SO2$ their derivation is mostly similar.
However, the discreteness of rotation angles leads to $N$ systems of linear congruences modulo $N$ in the final step.
This system of equations is solved in Section~\ref{apx:derivation_irrep_constraint_CN}.
Similar to how we derived the kernel basis for $\O2$ from $\SO2$, we derive the basis for $\DN$ from $\CN$ by adding reflectional constraints from $\Flip\cong\DN/\CN$ in Section~\ref{apx:derivation_irrep_constraint_DN}.



\subsection{Conventions, notation and basic properties}
\label{apx:conventions}

Throughout this section we denote rotations in $\SO2$ and $\CN$ by $r_\theta$ with $\theta\in[0,2\pi)$ and $\theta\in\left\{p\frac{2\pi}{N}\right\}_{p=0}^{N-1}$ respectively.
Since $\O2\cong\SO2\rtimes\Flip$ can be seen as a semidirect product of rotations and reflections we decompose orthogonal group elements into a unique product $g=r_\theta s\in\O2$ where $s\in\Flip$ is a reflection and $r_\theta\in\SO2$.
Similarly, we write $g=r_\theta s\in\DN$ for the dihedral group $\DN\cong\CN\rtimes\Flip$, in this case with $r_\theta\in\CN$.

The action of a rotation $r_\theta$ on $\mathbb{R}^2$ in polar coordinates $x(r,\phi)$ is given by $r_\theta.x(r,\phi) = x(r,r_\theta.\phi) = x(r, \phi+\theta)$.
An element $g = r_{\theta}s$ of $\O2$ or $\DN$ acts on $\mathbb{R}^2$ as $ g.x(r, \phi) = x(r, r_\theta s.\phi) = x(r, s\phi + \theta)$ where the symbol $s$ denotes both group elements in $\Flip$ and numbers in $\{\pm1\}$.



We denote a $2\!\times\!2$ orthonormal matrix with positive determinant, i.e. rotation matrix for an angle $\theta$, by:
$$\psi(\theta) = \PSI{\theta}$$
We define the orthonormal matrix with negative determinant corresponding to a reflection with respect to the horizontal axis as:
$$\xi(s=\shortminus1)=\XI{\shortminus1}$$
and a general orthonormal matrix with negative determinant, i.e. reflection with respect to the axis $2\theta$, as:
$$\PSIS{\theta} = \PSI{\theta} \XI{-1}$$

Hence, we can express any orthonormal matrix in the form:
$$\PSI{\theta} \begin{bmatrix}1 & 0 \\ 0 & s \end{bmatrix} = \psi(\theta)\xi(s)$$

where $\xi(s) = \XI{s}$ and $s \in \Flip$.

Moreover, these properties will be useful later:

\begingroup
\addtolength{\jot}{.6em}
\begin{align}
	\label{eq:orthogonal_properties_rotflip_commute}
	\psi(\theta) \xi(s)\ &=\ \xi(s) \psi(s\theta) \\
	\label{eq:orthogonal_properties_flip_inverse}
	\xi(s)^{-1}\ &=\ \xi(s)^T = \xi(s) \\
	\label{eq:orthogonal_properties_rot_inverse}
	\psi(\theta)^{-1}\ &=\ \psi(\theta)^{T} = \psi(-\theta) \\
	\label{eq:orthogonal_properties_rot_sum}
	\psi(\theta_1) \psi(\theta_2)\ &=\ \psi(\theta_1 + \theta_2) = \psi(\theta_2) \psi(\theta_1) \\
	\label{eq:orthogonal_properties_trace_flip}
	\tr(\psi(\theta) \xi(-1))\ &=\ \tr \PSIS{\theta} = 0 \\
	\label{eq:orthogonal_properties_trace_rot}
	\tr(\psi(\theta))\ &=\ \tr \PSI{\theta} = 2 \cos(\theta) \\
	\label{eq:trick_trigonometry}
	w_1 \cos(\alpha) + w_2 \sin(\alpha)\ &=\ w_1 \cos(\beta) + w_2 \sin(\beta) \ \ \forall w_1,w_2\in\R \nonumber \\[-.6em]
		\Leftrightarrow\quad \exists t\in\Z \text{ s.t. }\ \alpha\ &=\ \beta + 2t\pi
\end{align}
\endgroup



\subsection{Irreducible representations of G$\leq$O(2)}
\label{apx:irreps}

\paragraph{Special Orthogonal group SO(2):}
$\SO2$ irreps would decompose into complex irreps of $\U1$ on the complex field but since we are implementing the theory with real-valued variables we will not consider these.
Except for the trivial representation $\psi_0$, all the other irreps are 2-dimensional rotation matrices with frequencies $k\in\N^+$.
\begin{itemize}
\renewcommand\labelitemi{--}
	\item $\psi_0^{\SO2}(r_\theta) = 1 $\\
	\item $\psi_k^{\SO2}(r_\theta) = \PSI{k\theta} = \psi(k\theta),\quad k \in \mathbb{N^+}$
\end{itemize}


\paragraph{Orthogonal group O(2):}
$\O2$ has two 1-dimensional irreps: the trivial representation $\psi_{0,0}$ and a representation $\psi_{1, 0}$ which assigns $\pm1$ to reflections.
The other representations are rotation matrices precomposed by a reflection.
\begin{itemize}
\renewcommand\labelitemi{--}
	\item $\psi_{0,0}^{\O2}(r_\theta s)=1$ 
	\item $\psi_{1, 0}^{\O2}(r_\theta s)=s$
	\item $\psi_{1, k}^{\O2}(r_\theta s) = 
	\PSI{k\theta}
	\begin{bmatrix}
		1 & 0 \\
		0 & s \\
	\end{bmatrix} = \psi(k\theta)\xi(s),
	\quad k\in\mathbb{N^+}$\end{itemize}


\paragraph{Cyclic groups $\bCN$:}
The irreps of $\CN$ are identical to the irreps of $\SO2$ up to frequency $\floor{N/2}$.
Due to the discreteness of rotation angles, higher frequencies would be aliased.
\begin{itemize}
\renewcommand\labelitemi{--}
	\item $\psi_0^{\CN}(r_\theta) = 1 $
	\item $\psi_k^{\CN}(r_\theta) = \PSI{k\theta} = \psi(k\theta), \quad k \in\{1, \dots, \floor{{N-1\over2}}\}$
\end{itemize}
If $N$ is even, there is an additional 1-dimensional irrep corresponding to frequency $\floor{{N\over2}}={N\over2}$:
\begin{itemize}
\renewcommand\labelitemi{--}
	\item $\psi_{N/2}^{\CN}(r_\theta) = \cos\left(\frac{N}{2} \theta\right)\ \in\ \{\pm1\}$ since $\theta\in\{p\frac{2\pi}{N}\}_{p=0}^{N-1}$
\end{itemize}


\paragraph{Dihedral groups $\bDN$:}
Similarly, $\DN$ consists of irreps of $\O2$ up to frequency $\floor{N/2}$.
\begin{itemize}
\renewcommand\labelitemi{--}
	\item $\psi_{0, 0}^{\DN}(r_\theta s) = 1$
	\item $\psi_{1, 0}^{\DN}(r_\theta s) = s$
	\item $\psi_{1, k}^{\DN}(r_\theta s) = 
	\PSI{k\theta}
	\begin{bmatrix}
	1 & 0 \\
	0 & s \\
	\end{bmatrix} = \psi(k\theta)\xi(s), \quad k\in\{1,\dots,\floor{{N-1\over2}}\}$
\end{itemize}
If $N$ is even, there are two 1-dimensional irreps:
\begin{itemize}
\renewcommand\labelitemi{--}
	\item $\psi_{0, N/2}^{\DN}(r_\theta s) = \phantom{s}\cos\left(\frac{N}{2} \theta\right)\ \in \{\pm1\}\quad$ since $\theta\in\{p\frac{2\pi}{N}\}_{p=0}^{N-1}$
	\item $\psi_{1, N/2}^{\DN}(r_\theta s) = 		  s \cos\left(\frac{N}{2} \theta\right)\ \in \{\pm1\}\quad$ since $\theta\in\{p\frac{2\pi}{N}\}_{p=0}^{N-1}$ 
\end{itemize}


\paragraph{Reflection group $\bFlip \cong \bD1 \cong \bC2$:}
The reflection group $\Flip$ is isomorphic to $\D1$ and $\C2$.
For this reason, it has the same irreps of these two groups:
\begin{itemize}
	\renewcommand\labelitemi{--}
	\item $\psi_{0}^{\Flip}(s) = 1$
	\item $\psi_{1}^{\Flip}(s) = s$
\end{itemize}



\clearpage


\newcommand\onestrut{\rule[-10pt]{0pt}{25pt}}
\newcommand\twostrut{\rule[-19pt]{0pt}{43pt}}
\newcommand\Twostrut{\rule[-28pt]{0pt}{65pt}}
\newcommand\Tstrut{\rule{0pt}{8ex}}         \newcommand\Bstrut{\rule[-4ex]{0pt}{0pt}}   


\subsection{Analytical solutions of the irrep kernel constraints}
\label{apx:analytical_irrep_bases}
~\\
\begin{table}[h!]\begin{center}
	Special Orthogonal Group $\SO2$
\end{center}
	\centering \scalebox{.94}{\setlength{\tabcolsep}{4pt}\renewcommand\arraystretch{1.6}\begin{tabu}{c|[1pt]c|c}\diagbox[height=14pt]{\raisebox{-2pt}{$\psi_m$}}{\ \ \raisebox{7pt}{$\psi_n$}}
			& $\psi_0$       &  $\psi_n$, $n\in\N^+$                            \\
			\tabucline[1pt]{-}
			$\psi_0$   & $\big[1  \big]$  & $\big[\cos(n\phi)\, \sin(n\phi)\big], \big[\shortminus \sin(n\phi) \, \cos(n\phi)\big] $ \onestrut \\ \hline
			
			\makecell{$\psi_m$,\\$m\in\N^+$}
& 
				\makecell{
					$\Bigg[\begin{array}{c}
						\!\!\!\phantom{\shortminus}\!\cos(m\phi) \!\!\!\!\\ \!\!\!\phantom{\shortminus}\!\sin(m\phi) \!\!\!\!
					\end{array} \Bigg]$, \\ [12pt]
					$\Bigg[\begin{array}{c}
						\!\!\!                \shortminus \!\sin(m\phi) \!\!\!\!\\ \!\!\!\phantom{\shortminus}\!\cos(m\phi) \!\!\!\!
					\end{array} \Bigg]$\phantom{,}\\
				}
& 
				\makecell{
					$\Bigg[\begin{array}{cc}
						\!\!\!\cos\!\big(\!(\!m\!-\!n\!)\phi\big) & \!\!\!                \shortminus \!\sin\!\big(\!(\!m\!-\!n\!)\phi\big) \!\!\!\!\\
						\!\!\!\sin\!\big(\!(\!m\!-\!n\!)\phi\big) & \!\!\!\phantom{\shortminus}\!\cos\!\big(\!(\!m\!-\!n\!)\phi\big) \!\!\!\!\\
					\end{array} \Bigg]
					\!,\!
					\Bigg[\begin{array}{cc}
						\!\!\!                \shortminus  \sin\!\big(\!(\!m\!-\!n\!)\phi\big) & \!\!\!\shortminus\!\cos\!\big(\!(\!m\!-\!n\!)\phi\big) \!\!\!\!\\
						\!\!\!\phantom{\shortminus} \cos\!\big(\!(\!m\!-\!n\!)\phi\big) & \!\!\!\shortminus\!\sin\!\big(\!(\!m\!-\!n\!)\phi\big) \!\!\!\!\\
					\end{array} \Bigg],$ \\ [12pt]
					$\Bigg[\begin{array}{cc}
						\!\!\!\cos\!\big(\!(\!m\! +         \!n\!)\phi\big) & \!\!\!\phantom{\shortminus}\!\sin\!\big(\!(\!m\! +         \!n\!)\phi\big) \!\!\!\!\\
						\!\!\!\sin\!\big(\!(\!m\! +         \!n\!)\phi\big) & \!\!\!                \shortminus \!\cos\!\big(\!(\!m\! +         \!n\!)\phi\big) \!\!\!\!\\
					\end{array} \Bigg]
					\!,\!
					\Bigg[\begin{array}{cc}
						\!\!\!                \shortminus  \sin\!\big(\!(\!m\! +         \!n\!)\phi\big) & \!\!\!\phantom{\shortminus}\!\cos\!\big(\!(\!m\! +         \!n\!)\phi\big) \!\!\!\!\\
						\!\!\!\phantom{\shortminus} \cos\!\big(\!(\!m\! +         \!n\!)\phi\big) & \!\!\!\phantom{\shortminus}\!\sin\!\big(\!(\!m\! +         \!n\!)\phi\big) \!\!\!\!\\
					\end{array} \Bigg]\phantom{,}$ \\
				}
			\Twostrut\\
		\end{tabu}}\vspace*{2ex}\captionsetup{width=\linewidth}
	\caption{Bases for the angular parts of $\SO2$-steerable kernels satisfying the irrep constraint~\eqref{eq:irrep_constraint} for different pairs of input field irreps $\psi_{n}$ and output field irreps $\psi_{m}$.
		The different types of irreps are explained in~\ref{apx:irreps}.}\label{tab:SO2_irrep_solution_appendix}


\vspace*{6ex}
\begin{center}
	Orthogonal Group $\O2$
\end{center}
	\centering \scalebox{.94}{\setlength{\tabcolsep}{4pt}\renewcommand\arraystretch{1.6}\begin{tabu}{c|[1pt]c|c|c}\diagbox[height=14pt]{\raisebox{-2pt}{$\psi_{i,m}$}}{\ \ \raisebox{7pt}{$\psi_{j,n}$}}
			               & $\psi_{0,0}$  & $\psi_{1,0}$   & $\psi_{1,n}$, $n\in\N^+$                                     \\ \tabucline[1pt]{-}
			  $\psi_{0,0}$ & $\big[1\big]$ & $\varnothing$  & $\big[\shortminus\sin(n\phi)\,           \cos(n\phi) \big]$ \onestrut \\ \hline
			  $\psi_{1,0}$ & $\varnothing$ & $\big[1 \big]$ & $\big[           \cos(n\phi)\,\phantom{-}\sin(n\phi) \big]$ \onestrut \\ \hline
			\makecell{$\psi_{1,m}$,\\ $m\in\N^+$}
& $\Bigg[\begin{array}{c}
				\!\!\!         \shortminus \!\sin(m\phi) \!\!\!\!\!\\
				\!\!\!\phantom{\shortminus}\!\cos(m\phi) \!\!\!\!\!\\
			\end{array} \Bigg]$
& $\Bigg[\begin{array}{c}
				\!\!\!\phantom{\shortminus}\!\cos(m\phi) \!\!\!\!\!\\
				\!\!\!\phantom{\shortminus}\!\sin(m\phi) \!\!\!\!\!\\
			\end{array} \Bigg]$
& $\Bigg[\begin{array}{cc}
				\!\!\!\cos\!\big(\!(\!m\!\shortminus\!n\!)\phi\big) & \!\!\!                \shortminus \!\sin\!\big(\!(\!m\!\shortminus\!n\!)\phi\big) \!\!\!\!\!\\
				\!\!\!\sin\!\big(\!(\!m\!\shortminus\!n\!)\phi\big) & \!\!\!\phantom{\shortminus}\!\cos\!\big(\!(\!m\!\shortminus\!n\!)\phi\big) \!\!\!\!\!\\
			\end{array} \Bigg]
			\!,\!
			\Bigg[\begin{array}{cc}
				\!\!\!\cos\!\big(\!(\!m\! +         \!n\!)\phi\big) & \!\!\!\phantom{\shortminus}\!\sin\!\big(\!(\!m\! +  \!n\!)\phi\big) \!\!\!\!\!\\
				\!\!\!\sin\!\big(\!(\!m\! +         \!n\!)\phi\big) & \!\!\!         \shortminus \!\cos\!\big(\!(\!m\! +  \!n\!)\phi\big) \!\!\!\!\!\\
			\end{array} \Bigg]$
		\end{tabu}}\vspace*{2ex}\captionsetup{width=\linewidth}
	\caption{Bases for the angular parts of $\O2$-steerable kernels satisfying the irrep constraint~\eqref{eq:irrep_constraint} for different pairs of input field irreps $\psi_{j,n}$ and output field irreps $\psi_{i,m}$.
		The different types of irreps are explained in~\ref{apx:irreps}.}\label{tab:O2_irrep_solution_appendix}


\vspace*{6ex}
\begin{center}
	Reflection group $\Flip$
\end{center}
	\centering \scalebox{.94}{\setlength{\tabcolsep}{4pt}\renewcommand\arraystretch{1.6}\begin{tabu}{c|[1pt]c|c}\diagbox[height=14pt]{\raisebox{-2pt}{$\psi_i$}}{\ \ \raisebox{7pt}{$\psi_j$}}
			           & $\psi_{0}$       & $\psi_{1}$                                     \\ \tabucline[1pt]{-}
			$\psi_{0}$ & $\quad\Big[\cos\big(\mu(\phi-\beta)\big)\Big]\quad$ & $\quad\Big[\sin\big(\mu(\phi-\beta)\big)\Big]\quad$ \onestrut\\ \hline
			$\psi_{1}$ & $\quad\Big[\sin\big(\mu(\phi-\beta)\big)\Big]\quad$ & $\quad\Big[\cos\big(\mu(\phi-\beta)\big)\Big]\quad$ \onestrut\\
		\end{tabu}}\vspace*{2ex}\captionsetup{width=\linewidth}
	\caption{Bases for the angular parts of $\Flip$-steerable kernels satisfying the irrep constraint~\eqref{eq:irrep_constraint} for different pairs of input field irreps $\psi_j$ and output field irreps $\psi_i$ for $i,j\in\{0,1\}$.
		The different types of irreps are explained in~\ref{apx:irreps}.
		The group is assumed to act by reflecting over an axis defined by the angle $\beta$.
		Note that the bases shown here are a special case of the bases shown in Table~\ref{tab:DN_irrep_solution_appendix} since $\Flip\cong\D1$.
	}\label{tab:Reflection_irrep_solution_appendix}\end{table}




\begin{landscape}
\begin{table}[h!]\vspace*{-8ex}
\begin{center}
	Cyclic groups $\CN$
\end{center}
	\centering \scalebox{.94}{\setlength{\tabcolsep}{4pt}\renewcommand\arraystretch{1.6}\begin{tabu}{c|[1pt]c|c|c}\diagbox[height=19pt, width=75pt]{\raisebox{0pt}{$\psi_{m}$}}{\ \ \raisebox{7pt}{$\psi_{n}$}}
			& $\psi_{0}$   & $\psi_{N/2}$ (if $N$ even)    & $\psi_{n}$ with $n\in \N^+$ and $1 \leq n < N/2$           \\ \tabucline[1pt]{-}
			$\psi_{0}$   
						 & \makecell{
						 		$\big[    \cos(\hat{t}N\phi) \big]$, \\[5pt]
						 		$\big[\;\!\sin(\hat{t}N\phi) \big]$\phantom{,}
					 		} 
			     		 & \makecell{
				     		 	$\Big[  \cos\lp\!\lp\hat{t}\!+\!{1\over2}\rp\!N\phi\rp \Big]$, \\[5pt] 
				     		 	$\Big[\,\sin\lp\!\lp\hat{t}\!+\!{1\over2}\rp\!N\phi\rp \Big]$\phantom{,}
		     		 	    }  \twostrut
						 & \makecell{
							 	$\big[ 	          \shortminus \sin((n+tN)\phi) \, \phantom{\shortminus}\cos((n+tN)\phi) \big]$, \\ [5pt]
							 	$\big[\: \phantom{\shortminus}\cos((n+tN)\phi) \, \phantom{\shortminus}\sin((n+tN)\phi) \big]$\phantom{,}
						 }  \\ 
						 \hline
			$\psi_{N/2}$ ($N$ even)  
						 & \makecell{
							 	$\Big[    \cos\lp\!\lp\hat{t}\!+\!{1\over2}\rp\!N\phi\rp \Big]$, \\ [5pt]
							 	$\Big[\;\!\sin\lp\!\lp\hat{t}\!+\!{1\over2}\rp\!N\phi\rp \Big]$\phantom{,} \\
						 	} \twostrut
 						 & \makecell{
	 						 	$\big[\cos(\hat{t}N\phi) \big]$, \\[5pt]
	 						 	$\big[\,\sin(\hat{t}N\phi) \big]$\phantom{,} \\
 					 	 	} 
						 & \makecell{
							 	$\Big[		           \shortminus \sin\lp\lp n+\lp t\!+\!{1\over2}\rp\!N\rp\!\phi\rp \, \phantom{\shortminus}\cos\lp\lp n+\lp t\!+\!{1\over2}\rp\!N\rp\!\phi\rp \Big]$, \\[5pt]
							 	$\Big[\: \phantom{\shortminus}\cos\lp\lp n+\lp t\!+\!{1\over2}\rp\!N\rp\!\phi\rp \, \phantom{\shortminus}\sin\lp\lp n+\lp t\!+\!{1\over2}\rp\!N\rp\!\phi\rp \Big]$\phantom{,} \\
							 } \twostrut \\ 
						 \hline
			\makecell{$\psi_{m}$,\\$m\in \N^+$\\$ 1 \leq m < N/2$}
& \makecell{
				$\Bigg[\begin{array}{c}
					\!\!\!                \shortminus \!\sin((m+tN)\phi) \!\!\!\!\!\\
					\!\!\!\phantom{\shortminus}\!\cos((m+tN)\phi) \!\!\!\!\!\\
				\end{array} \Bigg]$, \\ [12pt]
				$\Bigg[\begin{array}{c}
					\!\!\!\phantom{\shortminus}\!\cos((m+tN)\phi) \!\!\!\!\!\\
					\!\!\!\phantom{\shortminus}\!\sin((m+tN)\phi) \!\!\!\!\!\\
				\end{array} \Bigg]$\phantom{,}\\
			}
& \makecell{
				$\Bigg[\begin{array}{c}
					\!\!\!         \shortminus \!\sin\lp\lp m+\lp t\!+\!{1\over2}\rp\!N\rp\!\phi\rp \!\!\!\!\!\\
					\!\!\!\phantom{\shortminus}\!\cos\lp\lp m+\lp t\!+\!{1\over2}\rp\!N\rp\!\phi\rp \!\!\!\!\!\\
				\end{array} \Bigg]$ , \\ [12pt]
				$\Bigg[\begin{array}{c}
					\!\!\!\phantom{\shortminus}\!\cos\lp\lp m+\lp t\!+\!{1\over2}\rp\!N\rp\!\phi\rp \!\!\!\!\!\\
					\!\!\!\phantom{\shortminus}\!\sin\lp\lp m+\lp t\!+\!{1\over2}\rp\!N\rp\!\phi\rp \!\!\!\!\!\\
				\end{array} \Bigg]$ \phantom{,} \\
			}
& \makecell{
				$\Bigg[\begin{array}{cc}
					\!\!\!\cos\!\big(\!(\!m\!-\!n +tN\!)\phi\big) & \!\!\!         \shortminus \!\sin\!\big(\!(\!m\!-\!n +tN\!)\phi\big) \!\!\!\!\!\\
					\!\!\!\sin\!\big(\!(\!m\!-\!n +tN\!)\phi\big) & \!\!\!\phantom{\shortminus}\!\cos\!\big(\!(\!m\!-\!n +tN\!)\phi\big) \!\!\!\!\!\\
				\end{array} \Bigg]
				\!,
				\Bigg[\begin{array}{cc}
					\!\!\!                \shortminus \sin\!\big(\!(\!m\!-\!n +tN\!)\phi\big) & \!\!\!		 \shortminus \!\cos\!\big(\!(\!m\!-\!n +tN\!)\phi\big) \!\!\!\!\!\\
					\!\!\!\phantom{\shortminus}\cos\!\big(\!(\!m\!-\!n +tN\!)\phi\big) & \!\!\!		 \shortminus \!\sin\!\big(\!(\!m\!-\!n +tN\!)\phi\big) \!\!\!\!\!\\
				\end{array} \Bigg],$ \\ [12pt]
				$\Bigg[\begin{array}{cc}
					\!\!\!\cos\!\big(\!(\!m\! +         \!n +tN\!)\phi\big) & \!\!\!\phantom{\shortminus}\!\sin\!\big(\!(\!m\! +         \!n +tN\!)\phi\big) \!\!\!\!\!\\
					\!\!\!\sin\!\big(\!(\!m\! +         \!n +tN\!)\phi\big) & \!\!\!                 \shortminus \!\cos\!\big(\!(\!m\! +         \!n +tN\!)\phi\big) \!\!\!\!\!\\
				\end{array} \Bigg]
				\!,
				\Bigg[\begin{array}{cc}
					\!\!\!         \shortminus \sin\!\big(\!(\!m\! +         \!n +tN\!)\phi\big) & \!\!\! \phantom{\shortminus} \!\cos\!\big(\!(\!m\! +         \!n +tN\!)\phi\big) \!\!\!\!\!\\
					\!\!\!\phantom{\shortminus}\cos\!\big(\!(\!m\! +         \!n +tN\!)\phi\big) & \!\!\! \phantom{\shortminus} \!\sin\!\big(\!(\!m\! +         \!n +tN\!)\phi\big) \!\!\!\!\!\\
				\end{array} \Bigg]\phantom{,}$  \\
			} \Twostrut\\
		\end{tabu}}\vspace*{2ex}\captionsetup{width=.8\linewidth}
	\caption{Bases for the angular parts of $\CN$-steerable kernels for different pairs of input and output fields irreps $\psi_n$ and $\psi_m$.
		The full basis is found by instantiating these solutions for each $t\!\in\!\Z$ or $\hat{t} \in \N$.
		The different types of irreps are explained in \apx~\ref{apx:irreps}.
	}\label{tab:CN_irrep_solution_appendix}



\vspace*{4ex}
\begin{center}
	Dihedral groups $\DN$
\end{center}

	\centering \scalebox{.94}{\setlength{\tabcolsep}{4pt}\renewcommand\arraystretch{1.6}\begin{tabu}{c|[1pt]c|c|c|c|c}\diagbox[height=19pt, width=75pt]{\raisebox{0pt}{$\psi_{i,m}$}}{\ \ \raisebox{7pt}{$\psi_{j,n}$}}
			& $\psi_{0,0}$ 
			& $\psi_{1,0}$  
			& $\psi_{0,N/2}$ (if $N$ even) 
			& $\psi_{1,N/2}$ (if $N$ even)  
			& $\psi_{1,n}$ with $n\in \N^+$ and $1 \leq n < N/2$          
			\\ \tabucline[1pt]{-}
			$\psi_{0,0}$   
			& $\big[\cos(\hat{t}N\phi) \big]$
			& $\big[\sin(\hat{t}N\phi) \big]$
			& $\Big[\cos\lp\!\lp\hat{t}\!+\!{1\over2}\rp\!N\phi\rp \Big]$ \onestrut
			& $\Big[\sin\lp\!\lp\hat{t}\!+\!{1\over2}\rp\!N\phi\rp \Big]$ \onestrut
			& $\big[\shortminus\sin((n+tN)\phi) \, \phantom{\shortminus}\cos((n+tN)\phi) \big]$
			\\ \hline
			$\psi_{1,0}$   
			& $\big[\sin(\hat{t}N\phi) \big]$
			& $\big[\cos(\hat{t}N\phi) \big]$
			& $\Big[\sin\lp\!\lp\hat{t}\!+\!{1\over2}\rp\!N\phi\rp \Big]$ \onestrut
			& $\Big[\cos\lp\!\lp\hat{t}\!+\!{1\over2}\rp\!N\phi\rp \Big]$ \onestrut
			& $\big[\!\!\!\phantom{\shortminus} \cos\lp\lp n+tN\rp\!\phi\rp \, \phantom{\shortminus}\sin\lp\lp n+tN\rp\!\phi\rp \big]$
			\\ \hline
			$\psi_{0,N/2}$ ($N$ even)  
			& $\Big[\!\cos\lp\!\lp\hat{t}\!+\!{1\over2}\rp\!N\phi\rp \Big]$ \onestrut
			& $\Big[\sin\lp\!\lp\hat{t}\!+\!{1\over2}\rp\!N\phi\rp \Big]$ \onestrut
			& $\big[\cos(\hat{t}N\phi) \big]$ 
			& $\big[\sin(\hat{t}N\phi) \big]$
			& $\Big[\shortminus\sin\!\lp\mkern-2mu \lp n+\lp t\!+\!{1\over2}\rp\!N\rp\!\phi\rp \, \phantom{\shortminus}\cos\!\lp\mkern-2mu \lp n+\lp t\!+\!{1\over2}\rp\!N\rp\!\phi\rp \Big]$ \onestrut
			\\ \hline
			$\psi_{1,N/2}$ ($N$ even)  
			& $\Big[\sin\lp\!\lp\hat{t}\!+\!{1\over2}\rp\!N\phi\rp \Big]$ \onestrut
			& $\Big[\!\cos\lp\!\lp\hat{t}\!+\!{1\over2}\rp\!N\phi\rp \Big]$ \onestrut
			& $\big[\sin(\hat{t}N\phi) \big]$
			& $\big[\cos(\hat{t}N\phi) \big]$ 
			& $\Big[\,\phantom{\shortminus}\cos\!\lp\mkern-2mu \lp n+\lp t\!+\!{1\over2}\rp\!N\rp\!\phi\rp \, \phantom{\shortminus}\sin\!\lp\mkern-2mu \lp n+\lp t\!+\!{1\over2}\rp\!N\rp\!\phi\rp \Big]$ \onestrut
			\\ \hline
			\makecell{$\psi_{1,m}$,\\$m\in \N^+$\\$ 1 \leq m < N/2$}
& 
				$\Bigg[\begin{array}{c}
					\!\!\!         \shortminus \!\sin((m+tN)\phi) \!\!\!\!\!\\
					\!\!\!\phantom{\shortminus}\!\cos((m+tN)\phi) \!\!\!\!\!\\
				\end{array} \Bigg]$
			&
				$\Bigg[\begin{array}{c}
					\!\!\!\cos((m+tN)\phi) \!\!\!\!\!\\
					\!\!\!\sin((m+tN)\phi) \!\!\!\!\!\\
				\end{array} \Bigg]$
& 
				$\Bigg[\begin{array}{c}
					\!\!\!         \shortminus \!\sin\!\lp\mkern-2mu \lp m+\lp t\!+\!{1\over2}\rp\!N\rp\!\phi\rp \!\!\!\!\!\\
					\!\!\!\phantom{\shortminus}\!\cos\!\lp\mkern-2mu \lp m+\lp t\!+\!{1\over2}\rp\!N\rp\!\phi\rp \!\!\!\!\!\\
				\end{array} \Bigg]$
			&
				$\Bigg[\begin{array}{c}
					\!\!\!\cos\!\lp\mkern-2mu \lp m+\lp t\!+\!{1\over2}\rp\!N\rp\!\phi\rp \!\!\!\!\!\\
					\!\!\!\sin\!\lp\mkern-2mu \lp m+\lp t\!+\!{1\over2}\rp\!N\rp\!\phi\rp \!\!\!\!\!\\
				\end{array} \Bigg]$
& \makecell{
				$\Bigg[\begin{array}{cc}
					\!\!\!\cos\!\big(\!(\!m\! -\! n +tN\!)\phi\big) & \!\!\!         \shortminus \!\sin\!\big(\!(\!m\! -\! n +tN\!)\phi\big) \!\!\!\!\!\\
					\!\!\!\sin\!\big(\!(\!m\! -\! n +tN\!)\phi\big) & \!\!\!\phantom{\shortminus}\!\cos\!\big(\!(\!m\! -\! n +tN\!)\phi\big) \!\!\!\!\!\\
				\end{array} \Bigg],$ \\ [12pt]
				$\Bigg[\begin{array}{cc}
					\!\!\!\cos\!\big(\!(\!m\! +\! n +tN\!)\phi\big) & \!\!\!\phantom{\shortminus}\!\sin\!\big(\!(\!m\! +\! n +tN\!)\phi\big) \!\!\!\!\!\\
					\!\!\!\sin\!\big(\!(\!m\! +\! n +tN\!)\phi\big) & \!\!\!         \shortminus \!\cos\!\big(\!(\!m\! +\! n +tN\!)\phi\big) \!\!\!\!\!\\
				\end{array} \Bigg]\phantom{,}$ \\
			} \Twostrut\\

			\end{tabu}}\vspace*{2ex}\captionsetup{width=.8\linewidth}
	\caption{Bases for the angular parts of $\DN$-steerable kernels for different pairs of input and output fields irreps $\psi_{j,n}$ and $\psi_{i,m}$.
		The full basis is found by instantiating these solutions for each $t\!\in\!\Z$ or $\hat{t}\!\in\!\N$.
		The different types of irreps are explained in \apx~\ref{apx:irreps}.
		The solutions here shown are for a group action where the reflection is defined around the horizontal axis ($\beta=0$).
		For different axes $\beta\neq0$ substitute $\phi$ with $\phi-\beta$.
	}\label{tab:DN_irrep_solution_appendix}\end{table}
\end{landscape}




\subsection{Derivations of the kernel constraints}
\label{apx:derivation_irrep_constraints}

Here we solve the kernel constraints for the irreducible representations of $G\leq\O2$.
Since the irreps of $G$ are either 1- or 2-dimensional, we distinguish between mappings between 2-dimensional irreps, mappings from 2- to 1-dimensional and 1- to 2-dimensional irreps and mappings between 1-dimensional irreps.
We are first exclusively considering positive radial parts $r>0$ in the following sections.
The constraint at the origin $r=0$ requires some additional considerations which we postpone to Section~\ref{apx:stabilizer_solution}.

\vspace*{1.ex}


\subsubsection{Derivation for SO(2)}
\label{apx:derivation_irrep_constraint_SO2}

\paragraph{2-dimensional irreps:}~\\[.75ex]
We first consider the case of 2-dimensional irreps both in the input and in output, that is, $\rho_\text{out} = \psi_m^{\SO2}$ and $\rho_\text{in} = \psi_n^{\SO2}$, where $\psi_k^{\SO2}(\theta)= \PSI{k \theta}$.
This means that the kernel has the form $\kappa^{ij}: \mathbb{R}^2 \to \mathbb{R}^{2\times2}$.
To reduce clutter we will from now on suppress the indices $ij$ corresponding to the input and output irreps in the input and output fields.

We expand each entry of the kernel $\kappa$ in terms of an (angular) Fourier series\footnote{
For brevity, we suppress that frequency 0 is associated to only half the number of basis elements which does not affect the validity of the derivation.
}
\begin{align*}
	\kappa(r, \phi)\ =\ \sum_{\mu=0}^{\infty}\ 
	 \ &A_{00,\mu}(r) \begin{bmatrix} \cos(\mu\phi) & 0 \\ 0 & 0 \end{bmatrix}\ 
	+\  B_{00,\mu}(r) \begin{bmatrix} \sin(\mu\phi) & 0 \\ 0 & 0 \end{bmatrix}\\
	+\ &A_{01,\mu}(r) \begin{bmatrix} 0 & \cos(\mu\phi) \\ 0 & 0 \end{bmatrix}\ 
	+\  B_{01,\mu}(r) \begin{bmatrix} 0 & \sin(\mu\phi) \\ 0 & 0 \end{bmatrix}\\
	+\ &A_{10,\mu}(r) \begin{bmatrix} 0 & 0 \\ \cos(\mu\phi) & 0 \end{bmatrix}\ 
	+\  B_{10,\mu}(r) \begin{bmatrix} 0 & 0 \\ \sin(\mu\phi) & 0 \end{bmatrix}\\
	+\ &A_{11,\mu}(r) \begin{bmatrix} 0 & 0 \\ 0 & \cos(\mu\phi) \end{bmatrix}\ 
	+\  B_{11,\mu}(r) \begin{bmatrix} 0 & 0 \\ 0 & \sin(\mu\phi) \end{bmatrix}
\end{align*}
and, for convenience, perform a change of basis to a different, non-sparse, orthogonal basis \begin{align*}
	\kappa(r, \phi) = \sum_{\mu=0}^{\infty}
	 \ & w_{0,0,\mu}(r) \PSI{\mu\phi}             \!\!\!\! &+&\ w_{0,1,\mu}(r) \PSI{\mu\phi + {\pi\over 2}} &&\\
	+\ & w_{1,0,\mu}(r) \PSIS{\mu\phi}            \!\!\!\! &+&\ w_{1,1,\mu}(r) \PSIS{\mu\phi + {\pi\over 2}} &&\\
	+\ & w_{2,0,\mu}(r) \PSI{\shortminus\mu\phi}  \!\!\!\! &+&\ w_{2,1,\mu}(r) \PSI{\shortminus\mu\phi + {\pi\over 2}} &&\\
	+\ & w_{3,0,\mu}(r) \PSIS{\shortminus\mu\phi} \!\!\!\! &+&\ w_{3,1,\mu}(r) \PSIS{\shortminus\mu\phi + {\pi\over 2}}\!.&&
\end{align*}
The last four matrices are equal to the first four, except for their opposite frequency.
Moreover, the second matrices of each row are equal to the first matrices, with a phase shift of ${\pi\over2}$ added.
Therefore, we can as well write:

\scalebox{.985}{\parbox{\linewidth}{\begin{align*}
\kappa(r,\!\phi) =\!\!\!\!\sum_{\mu=\shortminus\infty\ \ }^{\infty}\!\!\!\!\!\!\!\!\!\sum_{\ \ \ \ \gamma \in \{0, {\pi\over2}\}} \!\!\!\!\!\! w_{0,\gamma,\mu}(r) \!\! \PSI{\mu\phi \!+\!\gamma} \! + \! w_{1,\gamma,\mu}(r)\!\! \PSIS{\mu\phi\! +\! \gamma}
\end{align*}
}}
Notice that the first matrix evaluates to $\psi(\mu\phi + \gamma) \xi(1) = \psi(\mu\phi + \gamma)$ while the second evaluates to $\psi(\mu\phi + \gamma) \xi(-1)$.
Hence, for $s \in \{\pm 1\}$ we can compactly write:
\begin{align*}
	\kappa(r, \phi) = \sum_{\mu=-\infty}^{\infty} \sum_{\gamma \in \{0, {\pi\over2}\}} \sum_{s\in\{\pm1\}} w_{s, \gamma,\mu}(r) \psi(\mu\phi +\gamma) \xi(s)
\end{align*}
As already shown in Section \ref{sec:kernel_constraint_solution_main}, we can w.l.o.g. consider the kernels as being defined only on the angular component $\phi\in[0,2\pi)=S^1$ by solving only for a specific radial component $r$.
As a result, we consider the basis
\begin{align}
	\label{eq:so2_2x2_starting_basis}
	\left\{ b_{\mu,\gamma,s}(\phi) = \psi(\mu\phi +\gamma) \xi(s) \ \bigg|\ \ \mu \in \Z,\ \gamma \in \left\{0, {\pi\over2}\right\},\ s \in \Flip \right\}
\end{align}
of the unrestricted kernel space
which we will constrain in the following by demanding
\begin{align}
\label{eq:angular_kernel_constraint_appendix}
	\kappa(\phi + \theta) &= \psi_m^{\SO2}(r_\theta)\kappa(\phi) \psi_n^{\SO2}(r_\theta)^{-1} \quad\forall \phi,\theta\in[0,2\pi),
\end{align}
where we dropped the unrestricted radial part.

We solve for a basis of the subspace satisfying this constraint by projecting both sides on the basis elements defined above.
The inner product on $L^2\!\left(S^1\right)^{2\times2}$ is hereby defined as
\[
	\langle k_1, k_2\rangle = {1\over4\pi}\int d\phi \langle k_1(\phi), k_2(\phi)\rangle_{F} = {1\over4\pi}\int d\phi \ \tr\lp k_1(\phi)^T k_2(\phi) \rp,
\]
where $\langle \cdot, \cdot\rangle_F$ denotes the \textit{Frobenius} inner product between 2 matrices.

First consider the projection of the \textit{lhs} of the kernel constraint~\eqref{eq:angular_kernel_constraint_appendix} on a generic basis element $b_{\mu',\gamma',s'}(\phi) = \psi(\mu'\phi +\gamma') \xi(s')$.
Defining the operator $R_{\theta}$ by $\lp R_{\theta} \kappa\rp (\phi) := \kappa(\phi + \theta)$, the projection gives:
\begin{align*}
	\langle b_{\mu',\gamma',s'},  R_{\theta} \kappa \rangle
	&= {1\over4\pi}\int d\phi \ \tr\lp  b_{\mu',\gamma',s'}(\phi)^T \lp R_{\theta} \kappa\rp(\phi) \rp \\
	&= {1\over4\pi}\int d\phi \ \tr\lp  b_{\mu',\gamma',s'}(\phi)^T \kappa(\phi + \theta) \rp. \\
\intertext{By expanding the kernel in the linear combination of the basis we further obtain}
	&= {1\over4\pi} \int d\phi \ \tr \lp b_{\mu',\gamma',s'}(\phi)^T \lp \sum_{\mu} \sum_{\gamma} \sum_{s} w_{s, \gamma,\mu} \psi\lp\mu(\phi+\theta) +\gamma\rp \xi(s)\rp \rp,\\
\intertext{which, observing that the trace, sums and integral commute, results in:}
	&= {1\over4\pi}\sum_{\mu} \sum_{\gamma} \sum_{s} w_{s, \gamma,\mu} \tr \lp \int d\phi\ \ b_{\mu',\gamma',s'}(\phi)^T  \psi\lp\mu(\phi+\theta) +\gamma\rp	 \xi(s)\rp \\
	&= {1\over4\pi}\sum_{\mu} \sum_{\gamma} \sum_{s} w_{s, \gamma,\mu} \tr \lp \int d\phi\ \lp\psi(\mu'\phi +\gamma') \xi(s')\rp^T  \psi\lp\mu(\phi+\theta) +\gamma\rp	 \xi(s)\rp \\
	&= {1\over4\pi}\sum_{\mu} \sum_{\gamma} \sum_{s} w_{s, \gamma,\mu} \tr \lp \int d\phi\ \xi(s')^T\psi(\mu'\phi +\gamma')^T \psi\lp\mu(\phi+\theta) +\gamma\rp	 \xi(s)\rp \\
\intertext{Using the properties in Eq. \eqref{eq:orthogonal_properties_flip_inverse} and \eqref{eq:orthogonal_properties_rot_inverse} then yields:}
	&= {1\over4\pi}\sum_{\mu} \sum_{\gamma} \sum_{s} w_{s, \gamma,\mu} \tr \lp \int d\phi\ \xi(s') \psi(-\mu'\phi -\gamma') \psi\lp\mu(\phi+\theta) +\gamma\rp	 \xi(s)\rp \\
&= {1\over2}\sum_{\mu} \sum_{\gamma} \sum_{s} w_{s, \gamma,\mu} \tr \lp \xi(s') \psi(\gamma-\gamma') \lp\!{1\over2\pi}\!\!\int\!d\phi\ \psi((\mu-\mu')\phi)\! \rp \psi(\mu\theta) \xi(s)\!\rp \\
\intertext{In the integral, each cell of the matrix $\psi((\mu-\mu') \phi)$ contains either a sine or cosine.
	As a result, if $\mu-\mu' \neq 0$, all these integrals evaluate to $0$. Otherwise, the cosines on the diagonal evaluate to $1$, while the sines integrate to $0$.
	The whole integral evaluates to $\delta_{\mu,\mu'} \operatorname{id}_{2\times2}$, such that}
&= {1\over2}\sum_{\gamma} \sum_{s} w_{s, \gamma,\mu'} \tr \lp \xi(s') \psi(\gamma-\gamma')  \psi(\mu'\theta) \xi(s)	\rp, \\
\intertext{which, using the property in Eq. \eqref{eq:orthogonal_properties_rotflip_commute} leads to}
	&= {1\over2}\sum_{\gamma} \sum_{s} w_{s, \gamma,\mu'} \tr \lp \psi(s'(\gamma-\gamma' + \mu'\theta)) \xi(s'*s)\rp. \\
\intertext{Recall the propetries of the trace in Eq. \eqref{eq:orthogonal_properties_trace_flip}, \eqref{eq:orthogonal_properties_trace_rot}.
If $s'*s = -1$, i.e. $s'\neq s$, the matrix has a trace of~0:}
	&= {1\over2}\sum_{\gamma} \sum_{s} w_{s, \gamma,\mu'} \delta_{s',s} 2\cos(s'(\gamma-	\gamma' +\mu'\theta))\\
	\intertext{Since $\cos(-\alpha) = \cos(\alpha)$ and $s' \in \{\pm1\}$:}
	&= {1\over2}\sum_{\gamma} \sum_{s} w_{s, \gamma,\mu'} \delta_{s',s} 2\cos(\gamma-	\gamma' +\mu'\theta)\\
	&= \sum_{\gamma} w_{s', \gamma,\mu'}  \cos((\gamma - \gamma') +\mu'\theta)
\end{align*}

Next consider the projection of the \textit{rhs} of Eq.~\eqref{eq:angular_kernel_constraint_appendix}:
\begin{align*}
&\langle b_{\mu',\gamma',s'},\ \psi_m^{\SO2}(r_\theta)\kappa(\cdot) \psi_n^{\SO2}(r_\theta)^{-1} \rangle \\
&\qquad\qquad= {1\over4\pi}\int d\phi \ \tr\lp  b_{\mu',\gamma',s'}(\phi)^T \psi_m^{\SO2}(r_\theta)\kappa(\phi) \psi_n^{\SO2}(r_\theta)^{-1} \rp \\
&\qquad\qquad= {1\over4\pi}\int d\phi \ \tr\lp  b_{\mu',\gamma',s'}(\phi)^T \psi(m\theta)\kappa(\phi) \psi(-n\theta) \rp \\
\intertext{An expansion of the kernel in the linear combination of the basis yields:}
&\qquad\qquad= {1\over4\pi}\int d\phi \ \tr\lp  b_{\mu',\gamma',s'}(\phi)^T \psi(m\theta) \lp \sum_{\mu} \sum_{\gamma} \sum_{s} w_{s, \gamma,\mu} \psi\lp\mu\phi +\gamma\rp \xi(s)\rp \psi(-n\theta) \rp\\
&\qquad\qquad= {1\over4\pi}\sum_{\mu} \sum_{\gamma} \sum_{s} w_{s, \gamma,\mu} \tr\lp \int d\phi \  b_{\mu',\gamma',s'}(\phi)^T \psi(m\theta) \psi\lp\mu\phi +\gamma\rp \xi(s) \psi(-n\theta) \rp\\
&\qquad\qquad= {1\over4\pi}\sum_{\mu} \sum_{\gamma} \sum_{s} w_{s, \gamma,\mu} \tr\lp \int d\phi \  \xi(s') \psi(-\mu'\phi -\gamma') \psi(m\theta) \psi\lp\mu\phi +\gamma\rp \xi(s) \psi(-n\theta) \!\rp\\
&\qquad\qquad= {1\over2}\sum_{\mu} \sum_{\gamma} \sum_{s} w_{s, \gamma,\mu} \tr\!\lp\!\xi(s') \psi(\gamma\shortminus\gamma') \psi(m\theta) \!\lp\!{1\over2\pi}\!\int\! d\phi \ \psi((\mu\shortminus\mu')\phi) \!\rp \xi(s) \psi(\shortminus n\theta) \!\rp\\
\intertext{Again, the integral evaluates to $\delta_{\mu,\mu'} \operatorname{id}_{2\times2}$:}
&\qquad\qquad= {1\over2}\sum_{\mu} \sum_{\gamma} \sum_{s} w_{s, \gamma,\mu} \delta_{\mu,\mu'} \tr\lp  \xi(s') \psi(\gamma -\gamma') \psi(m\theta) \xi(s) \psi(-n\theta) \rp\\
&\qquad\qquad= {1\over2} \sum_{\gamma} \sum_{s} w_{s, \gamma,\mu'} \tr\lp  \xi(s') \psi(\gamma -\gamma') \psi(m\theta) \xi(s) \psi(-n\theta) \rp\\
&\qquad\qquad= {1\over2} \sum_{\gamma} \sum_{s} w_{s, \gamma,\mu'} \tr\lp  \psi(s'(\gamma -\gamma' +m\theta -ns\theta)) \xi(s'*s) \rp\\
\intertext{For the same reason as before, the trace is not zero if and only if $s'=s$:}
&\qquad\qquad= {1\over2} \sum_{\gamma} \sum_{s} w_{s, \gamma,\mu'} \delta_{s',s} 2\cos(s'(\gamma -\gamma' +m\theta -ns\theta))\\
\intertext{Since $\cos(-\alpha) = \cos(\alpha)$ and $s' \in \{\pm1\}$:}
&\qquad\qquad= \sum_{\gamma} w_{s', \gamma,\mu'} \cos(\gamma -\gamma' +m\theta -ns'\theta)\\
&\qquad\qquad= \sum_{\gamma} w_{s', \gamma,\mu'} \cos((\gamma - \gamma') + (m -ns')\theta)
\end{align*}
	
Finally, we require the two projections to be equal for all rotations in $\SO2$, that is,
\begin{align*}
	\sum_{\gamma} w_{s', \gamma,\mu'}  \cos((\gamma - \gamma') +\mu'\theta) &=  \sum_{\gamma} w_{s', \gamma,\mu'} \cos((\gamma -\gamma') + (m -ns')\theta) \qquad\forall\theta\in[0,2\pi),
\end{align*}
or, explicitly, with $\gamma \in \{0, {\pi\over 2}\}$ and $\cos(\alpha + {\pi\over 2}) = -\sin(\alpha)$:
\begin{align*}
	     &w_{s',0,\mu'}  \cos(\mu'\theta - \gamma')     \mkern-30mu&&-\ w_{s',{\pi\over 2},\mu'}  \sin(\mu'\theta - \gamma') &&\\
	=\ \ &w_{s', 0,\mu'} \cos((m -ns')\theta - \gamma') \mkern-30mu&&-\ w_{s',{\pi\over 2},\mu'} \sin((m -ns')\theta - \gamma' ) \qquad\forall\theta\in[0,2\pi) &&
\end{align*}
Using the property in Eq. \eqref{eq:trick_trigonometry} then implies that for each $\theta$ in $[0,2\pi)$ there exists a $t\in\Z$ such that:
\begin{align*}
	&\Leftrightarrow&	\mu'\theta - \gamma'	\ &=\ (m -ns')\theta - \gamma' +2t\pi & \\ &\Leftrightarrow&	(\mu' - (m -ns'))\theta \ &=\ 2t\pi							  & \stepcounter{equation}\tag{\theequation}\label{eq:so2_2x2_solution} \end{align*}
Since the constraint needs to hold for any $\theta \in [0, 2\pi)$ this results in the condition $\mu' = m-sn'$ on the frequencies occurring in the $\SO2$-steerable kernel basis.
Both $\gamma$ and $s$ are left unrestricted such that we end up with the four-dimensional basis
\begin{empheq}[box=\kernelspace]{align}
\label{eq:so2_2x2_basis}
	\!\!\!\!\mathcal{K}^{\SO2}_{\psi_m\leftarrow\psi_n} = 
	\left\{ b_{\mu,\gamma,s}(\phi) = \psi\big(\mu\phi +\gamma\big) \xi(s) \ \bigg|\ \mu=(m\shortminus sn),\ \gamma\in\left\{0,{\pi\over2}\right\},\ s\in\{\pm1\} \right\} \!\!\!\!
\end{empheq}
for the angular parts of equivariant kernels for $m,n>0$.
This basis is explicitly written out in the lower right cell of Table~\ref{tab:SO2_irrep_solution_appendix}.




~\\[-4.ex]
\paragraph{1-dimensional irreps:}~\\[.75ex]
For the case of 1-dimensional irreps in both the input and output, i.e. $\rho_\text{out} = \rho_\text{in} = \psi_0^{\SO2}$ the kernel has the form $\kappa^{ij}: \mathbb{R}^2 \to \mathbb{R}^{1\times1}$.
As a scalar function in $L^2(\R^2)$, it can be expressed by the Fourier decomposition of its angular part:
\begin{align*}
\kappa(r, \phi)\ &=\ w_{0,0} + \sum_{\mu=1}^{\infty} \sum_{\gamma \in \{0, {\pi\over2}\}} w_{\mu, \gamma}(r) \cos(\mu\phi + \gamma)
\end{align*}
As before, we can w.l.o.g. drop the dependency on the radial part as it is not restricted by the constraint.
We are therefore considering the basis
\begin{align}\label{eq:so2_1x1_starting_basis}
\Bigg\{ b_{\mu,\gamma}(\phi) = \cos(\mu\phi +\gamma) \ \Bigg|\ \ \mu \in \N,\ \gamma \in 
		\begin{cases}
			\{0\}		 &\!\!\text{if}\ \mu=0 \\[-1pt]
			\{0, \pi/2\} &\!\!\text{otherwise}
		\end{cases}
		\ 
	\Bigg\}
\end{align}
of angular kernels in $L^2(S^1)^{1\times1}$.
The kernel constraint in Eq. \eqref{eq:irrep_constraint} then requires
\begin{align*}
	\kappa(\phi + \theta)\ &=\ \psi_m^{\SO2}(r_\theta)\kappa(\phi) \psi_n^{\SO2}(r_\theta)^{-1} \mkern-80mu&\forall\theta,\phi\in[0,2\pi)&\\
	\Leftrightarrow\quad
	\kappa(\phi + \theta)\ &=\ \kappa(\phi) \mkern-80mu&\forall\theta,\phi\in[0,2\pi)&,
\end{align*}
i.e. the kernel has to be \textit{invariant} to rotations.

Again, we find the space of all solutions by projecting both sides on the basis defined above.
Here, the projection of two kernels is defined through the standard inner product $\langle k_1, k_2\rangle = {1\over2\pi}\int d\phi k_1(\phi) k_2(\phi)$ on $L^2(S^1)$.

We first consider the projection of the \textit{lhs}:
\begin{align*}
	\langle b_{\mu',\gamma'},\ R_{\theta} \kappa \rangle
	&= {1\over2\pi}\int d\phi \ b_{\mu',\gamma'}(\phi) \lp R_{\theta} \kappa\rp(\phi) \\
	&= {1\over2\pi}\int d\phi \ b_{\mu',\gamma'}(\phi) \kappa(\phi + \theta) \\
\intertext{As before we expand the kernel in the linear combination of the basis:}
	&= \sum_{\mu, \gamma} w_{\mu, \gamma} {1\over2\pi}\int d\phi \ b_{\mu',\gamma'}(\phi) \cos(\mu\phi + \mu\theta + \gamma) \\
	&= \sum_{\mu, \gamma} w_{\mu, \gamma} {1\over2\pi}\int d\phi \ \cos(\mu'\phi + \gamma') \cos(\mu\phi + \mu\theta + \gamma) \\
\intertext{With $\cos(\alpha)\cos(\beta) = {1\over2}\lp \cos(\alpha-\beta) + \cos(\alpha+\beta)\rp$ this results in:}
	&=\sum_{\mu, \gamma} w_{\mu, \gamma} {1\over2\pi}\int d\phi \ {1\over2}\big( \cos(\mu'\phi + \gamma' - \mu\phi-\mu\theta-\gamma) \\
	& \mkern164mu + \cos(\mu'\phi + \gamma' + \mu\phi+\mu\theta+\gamma) \big) \\
&= \sum_{\mu, \gamma} w_{\mu, \gamma} {1\over2}\big( {1\over2\pi}\int d\phi \cos((\mu' -\mu)\phi + (\gamma'-\gamma) - \mu\theta) \\
	& \mkern88mu + {1\over2\pi}\int d\phi  \cos((\mu' + \mu)\phi + (\gamma'+\gamma) + \mu\theta) \big)\\
	&= \sum_{\mu, \gamma} w_{\mu, \gamma} {1\over2}\lp \delta_{\mu, \mu'}\cos((\gamma'-\gamma) - \mu\theta) + \delta_{\mu, -\mu'}\cos((\gamma'+\gamma) + \mu\theta) \rp\\
\intertext{Since $\mu, \mu' \geq 0$ and $\mu = -\mu'$ imply $\mu = \mu' = 0$ this simplifies further to}
	&= {1\over2} \sum_{\gamma} w_{\mu', \gamma} \lp\cos((\gamma'-\gamma) - \mu'\theta) + \delta_{\mu', 0}\cos(\gamma'+\gamma) \rp.
\end{align*}

A projection of the \textit{rhs} yields:
\begin{align*}
\langle b_{\mu',\gamma'},\ \kappa \rangle
	&\ =\ {1\over2\pi}\int d\phi \ b_{\mu',\gamma'}(\phi) \kappa(\phi) \\
&\ =\ \sum_{\mu, \gamma} w_{\mu, \gamma} {1\over2\pi}\int d\phi \ b_{\mu',\gamma'}(\phi) \cos(\mu\phi + \gamma) \\
	&\ =\ \sum_{\mu, \gamma} w_{\mu, \gamma} {1\over2\pi}\int d\phi \ \cos(\mu'\phi + \gamma') \cos(\mu\phi + \gamma) \\
	&\ =\ {1\over2} \sum_{\gamma} w_{\mu', \gamma} \lp\cos(\gamma'-\gamma) + \delta_{\mu', 0}\cos((\gamma'+\gamma)) \rp
\end{align*}

The projections are required to coincide for all rotations:
\begin{align*}
	\langle b_{\mu',\gamma'},  R_{\theta} \kappa \rangle &= \langle b_{\mu',\gamma'},  \kappa \rangle  & \forall\theta\in[0,2\pi)\phantom{,}\!\\
\sum_{\gamma} w_{\mu', \gamma} \lp\cos((\gamma'\!-\!\gamma) \!-\! \mu'\theta) + \delta_{\mu', 0}\cos((\gamma'\!+\!\gamma)) \rp &=
	\sum_{\gamma} w_{\mu', \gamma} \lp\cos(\gamma'\!-\!\gamma) \!+\! \delta_{\mu', 0}\cos((\gamma'\!+\!\gamma)) \rp \span \\
	&&\forall\theta\in[0,2\pi)\phantom{,}\!
\end{align*}

We consider two cases:
\begin{itemize}
\item[$\bullet\,\mu'\!=\!0$]
In this case, the basis in Eq.\eqref{eq:so2_1x1_starting_basis} is restricted to the single case $\gamma' = 0$ (as $\gamma' = {\pi\over2}$ and $\mu' = 0$ together lead to a null basis element).
Then:
\begin{align*}
	&&\sum_{\gamma} w_{0, \gamma} \lp\cos(-\gamma) + \cos(\gamma) \rp &= \sum_{\gamma} w_{0, \gamma} \lp\cos(-\gamma) + \cos(\gamma) \rp \\
\intertext{As $\gamma \in \{0, {\pi\over 2}\}$ and $\cos(\pm {\pi\over 2}) = 0$:}
	&\Leftrightarrow &w_{0, 0} \lp\cos(0) + \cos(0) \rp &= w_{0, 0} \lp\cos(0) + \cos(0) \rp \\
	&\Leftrightarrow &w_{0,0} &= w_{0,0}
\end{align*}
which is always true.

\item[$\bullet\,\mu'\!>\!0$] Here:
\begin{align*}
	&&\sum_{\gamma} w_{\mu', \gamma} \cos((\gamma'-\gamma) - \mu'\theta)\ &=\ \sum_{\gamma} w_{\mu', \gamma} \cos(\gamma'-\gamma) &\forall\theta\in[0,2\pi) \\
	&\Leftrightarrow &w_{\mu', 0} \cos(\gamma' \shortminus \mu'\theta) + w_{\mu', {\pi\over2}} \sin(\gamma' \shortminus \mu'\theta)\ &=\ w_{\mu', 0} \cos(\gamma') + w_{\mu', {\pi\over2}} \sin(\gamma') &\forall\theta\in[0,2\pi) \\
&\Leftrightarrow &- \mu'\theta\ &=\ 2t\pi &\forall\theta\in[0,2\pi)&,
\end{align*}
where Eq. \eqref{eq:trick_trigonometry} was used in the last step.
From the last equation one can see that $\mu'$ must be zero.
Since this contradicts the assumption that $\mu'\geq0$, no solution exists.


\end{itemize}

This results in a one dimensional basis of isotropic (rotation invariant) kernels
\begin{empheq}[box=\kernelspace]{align}
\label{eq:so2_1x1_basis}
	\mathcal{K}^{\SO2}_{\psi_m\leftarrow\psi_n}\ =\ 
	\big\{ b_{0,0}(\phi) = 1\big\}
\end{empheq}
for $m=n=0$, i.e. trivial representations.
The basis is presented in the upper left cell of Table~\ref{tab:SO2_irrep_solution_appendix}.




~\\[-4.ex]
\paragraph{1 and 2-dimensional irreps:}~\\[.75ex]
Finally, consider the case of a 1-dimensional irrep in the input and a 2-dimensional irrep in the output, that is, $\rho_\text{out} = \psi_m^{\SO2}$ and $\rho_\text{in} = \psi_0^{\SO2}$.
The corresponding kernel $\kappa^{ij}: \mathbb{R}^2\to\mathbb{R}^{2\times1}$ can be expanded in the following generalized Fourier series on $L^2(\R^2)^{2\times1}$:
\begin{align*}
	\kappa(r, \phi)\ =\ \sum_{\mu=0}^{\infty}\ 
	 \ &A_{0,\mu}(r) \begin{bmatrix} \cos(\mu\phi) \\ 0 \end{bmatrix}\ 
	+\  B_{0,\mu}(r) \begin{bmatrix} \sin(\mu\phi) \\ 0 \end{bmatrix}\\
	+\ &A_{1,\mu}(r) \begin{bmatrix} 0 \\ \cos(\mu\phi) \end{bmatrix}\ 
	+\  B_{1,\mu}(r) \begin{bmatrix} 0 \\ \sin(\mu\phi) \end{bmatrix}
\end{align*}
As before, we perform a change of basis to produce a non-sparse basis
\begin{align*}
	\kappa(r, \phi) \!=\!\!\!\sum_{\mu=-\infty}^{\infty} \sum_{\gamma \in \{0, {\pi\over2}\}} \!\!\!\!\! 
	w_{\gamma,\mu}(r) \! \begin{bmatrix} \cos(\mu\phi + \gamma) \\ \sin(\mu\phi + \gamma) \end{bmatrix}.
\end{align*}
Dropping the radial parts as usual, this corresponds to the complete basis
:
\begin{align}\label{eq:so2_1x2_starting_basis}
	\left\{ b_{\mu,\gamma}(\phi) = \begin{bmatrix} \cos(\mu\phi + \gamma) \\ \sin(\mu\phi + \gamma) \end{bmatrix} \ \bigg|\ \ \mu \in \Z,\ \gamma \in \left\{0, {\pi\over2}\right\}\right\}
\end{align}
of angular kernels on $L^2(S^1)^{2\times1}$.

The constraint in Eq.~\eqref{eq:irrep_constraint} requires the kernel space to satisfy
\begin{align*}
	\kappa(\phi + \theta)\ &=\ \psi_m^{\SO2}(r_\theta)\kappa(\phi) \psi_0^{\SO2}(r_\theta)^{-1} \mkern-60mu&\forall\theta,\phi\in[0,2\pi)&\\
	\Leftrightarrow\quad
	\kappa(\phi + \theta)\ &=\ \psi_m^{\SO2}(r_\theta)\kappa(\phi) \mkern-60mu&\forall\theta,\phi\in[0,2\pi)&.
\end{align*}
We again project both sides of this equation on the basis elements defined above where the projection on $L^2(S^1)^{2\times1}$ is defined by
$\langle k_1, k_2\rangle = {1\over2\pi}\int d\phi\ k_1(\phi)^T k_2(\phi)$.

Consider first the projection of the \textit{lhs}
\begin{align*}
	\langle b_{\mu',\gamma'},\ R_{\theta} \kappa \rangle
	&= {1\over2\pi}\int d\phi \ b_{\mu',\gamma'}(\phi)^T \lp R_{\theta} \kappa\rp(\phi) \\
	&= {1\over2\pi}\int d\phi \ b_{\mu',\gamma'}(\phi)^T \kappa(\phi + \theta)\,, \\
\intertext{which, after expanding the kernel in terms of the basis reads:}
	&= \sum_{\mu, \gamma} w_{\mu, \gamma} {1\over2\pi}\int d\phi \ b_{\mu',\gamma'}(\phi)^T 
	\begin{bmatrix} \cos(\mu(\phi + \theta) + \gamma) \\ \sin(\mu(\phi + \theta) + \gamma) \end{bmatrix} \\
	&= \sum_{\mu, \gamma} w_{\mu, \gamma} {1\over2\pi}\int d\phi \ 
	\begin{bmatrix} \cos(\mu'\phi + \gamma') & \sin(\mu'\phi + \gamma') \end{bmatrix}
	\begin{bmatrix} \cos(\mu(\phi + \theta) + \gamma) \\ \sin(\mu(\phi + \theta) + \gamma) \end{bmatrix} \\
&= \sum_{\mu, \gamma} w_{\mu, \gamma} {1\over2\pi}\int d\phi \ 
	\cos((\mu' - \mu) \phi + (\gamma' -\gamma) - \mu\theta). \\
\intertext{As before, the integral is non-zero only if the frequency is $0$, i.e. iff $\mu' - \mu = 0$ and thus:}
	&= \sum_{\gamma} w_{\mu', \gamma} \cos((\gamma' -\gamma) - \mu'\theta)
\end{align*}

For the \textit{rhs} we obtain:
\begin{align*}
	&\langle b_{\mu',\gamma'},\ \psi_m^{\SO2}(r_\theta)\kappa(\cdot) \rangle \\
	&\qquad\qquad\qquad= {1\over2\pi}\int d\phi \ b_{\mu',\gamma'}(\phi)^T \psi_m(r_\theta) \kappa(\phi) \\
&\qquad\qquad\qquad= \sum_{\mu, \gamma} w_{\mu, \gamma} {1\over2\pi}\int d\phi \ b_{\mu',\gamma'}(\phi)^T 
	\psi_m(r_\theta) \begin{bmatrix} \cos(\mu\phi + \gamma) \\ \sin(\mu\phi + \gamma) \end{bmatrix}\\
	&\qquad\qquad\qquad= \sum_{\mu, \gamma} w_{\mu, \gamma} {1\over2\pi}\int d\phi \ 
	\begin{bmatrix} \cos(\mu'\phi + \gamma') & \sin(\mu'\phi + \gamma') \end{bmatrix}
	\psi_m(r_\theta) \begin{bmatrix} \cos(\mu\phi + \gamma) \\ \sin(\mu\phi + \gamma) \end{bmatrix}\\
	&\qquad\qquad\qquad= \sum_{\mu, \gamma} w_{\mu, \gamma} {1\over2\pi}\int d\phi \ \cos(\mu'\phi + \gamma' -\mu\phi - \gamma -m\theta) \\
	&\qquad\qquad\qquad= \sum_{\mu, \gamma} w_{\mu, \gamma} {1\over2\pi}\int d\phi \ \cos((\mu' - \mu)\phi + \gamma' - \gamma -m\theta)\,. \\
\intertext{The integral is non-zero only if the frequency is $0$, i.e. $\mu' - \mu = 0$: }
	&\qquad\qquad\qquad= \sum_{\gamma} w_{\mu', \gamma} \cos(\gamma'-\gamma -m\theta)
\end{align*}

Requiring the projections to be equal implies
\begin{align*}
	&&\langle b_{\mu',\gamma'},  R_{\theta} \kappa \rangle &= \langle b_{\mu',\gamma'},  \psi_m(r_\theta)\kappa(\cdot) \rangle &\mkern-50mu\forall\theta\in[0,2\pi)\\
	&\Leftrightarrow &\sum_{\gamma} w_{\mu', \gamma} \cos(\gamma'-\gamma - \mu'\theta) &=
		\sum_{\gamma} w_{\mu', \gamma} \cos(\gamma'-\gamma -m\theta) &\mkern-50mu\forall\theta\in[0,2\pi)\\
	&\Leftrightarrow\!&w_{\mu', 0} \cos(\!\gamma' \!-\! \mu'\theta) + w_{\mu', {\pi\over2}} \sin(\!\gamma' \!-\! \mu'\theta) &=
		w_{\mu', 0} \cos(\gamma' \!-\! m\theta) + w_{\mu', {\pi\over2}} \sin(\gamma' \!-\!m\theta) \\
		&&&&\mkern-50mu\forall\theta\in[0,2\pi)\\
	&\Leftrightarrow &\gamma' - \mu'\theta &= \gamma' - m\theta +2t\pi &\mkern-50mu\forall\theta\in[0,2\pi)\\
	&\Leftrightarrow &\mu'\theta &= m\theta +2t\pi &\mkern-50mu\forall\theta\in[0,2\pi)&,
\end{align*}
where we made use of Eq.~\eqref{eq:trick_trigonometry} once again.
It follows that $\mu' = m$, resulting in the two-dimensional basis
\begin{empheq}[box=\kernelspace]{align}
\label{eq:so2_2x1_basis}
	\mathcal{K}^{\SO2}_{\psi_m\leftarrow\psi_n}\ =\ 
	\left\{ b_{m,\gamma}(\phi) = \begin{bmatrix} \cos(m\phi + \gamma)\\ \sin(m\phi + \gamma) \end{bmatrix} \ \bigg|\ \gamma \in \left\{0, {\pi\over2}\right\}\right\}
\end{empheq}
of equivariant kernels for $m>0$ and $n=0$.
This basis is explicitly given in the lower left cell of Table~\ref{tab:SO2_irrep_solution_appendix}.




~\\[-4.ex]
\paragraph{2 and 1-dimensional irreps:}~\\[.75ex]
The case for 2-dimensional input and 1-dimensional output representations, i.e. $\rho_\text{in} = \psi_n^{\SO2}$ and $\rho_\text{out} = \psi_0^{\SO2}$, is identical to the previous one up to a transpose.
The final two-dimensional basis for $m=0$ and $n>0$ is therefore given by
\begin{empheq}[box=\kernelspace]{align}
\label{eq:so2_1x2_basis}
	\mathcal{K}^{\SO2}_{\psi_m\leftarrow\psi_n}\ =\ 
	\left\{ b_{n,\gamma}(\phi) = \begin{bmatrix} \cos(n\phi + \gamma) & \sin(n\phi + \gamma) \end{bmatrix} \ \bigg|\ \gamma \in \left\{0, {\pi\over2}\right\}\right\}
\end{empheq}
as shown in the upper right cell of Table~\ref{tab:SO2_irrep_solution_appendix}.

 \vspace*{2.ex}

~\\
\subsubsection{Derivation for the reflection group}
\label{apx:derivation_irrep_constraint_Flip}

The action of the reflection group $\Flip$ on $\R^2$ depends on a choice of reflection axis, which we specify by an angle $\beta$.
More precisely, the element $s\in \Flip$ acts on $x = (r, \phi) \in \mathbb{R}^2$ as
\[
    s.x(r, \phi)\ :=\ x(r, s.\phi)\ :=\ x(r, 2\beta\delta_{s,-1} +s\phi)\ =\ 
    \begin{cases}
        x(r, \phi)         & \text{if } s=\phantom{\shortminus}1 \\
        x(r, 2\beta-\phi)  & \text{if } s=\shortminus1 \ .
    \end{cases}
\]
The kernel constraint for the reflection group is therefore being made explicit by
\begin{align*}
	\kappa(r, s.\phi) 						\ =&\ \rho_\text{out}(s) \kappa(r, \phi) \rho_\text{in}(s)^{-1} && \forall s \in \Flip, \phi \in\ [0, 2\pi) \\
	\Leftrightarrow\quad
	\kappa(r, \delta_{s,-1}2\beta +s\phi)	\ =&\ \rho_\text{out}(s) \kappa(r, \phi) \rho_\text{in}(s)^{-1} && \forall s \in \Flip, \phi \in\ [0, 2\pi) \\
    \Leftrightarrow\quad
    \kappa(r, \delta_{s,-1}2\beta +s\phi)   \ =&\ \rho_\text{out}(s) \kappa(r, \phi) \rho_\text{in}(s)      && \forall s \in \Flip, \phi \in\ [0, 2\pi) \,,
\intertext{
    where we used the identity $s^{-1}=s$.
	For $s=+1$ the constraint is trivially true. We will thus in the following consider the case $s = -1$, that is,
}
\kappa(r, 2\beta -\phi)\ =&\ \ \rho_\text{out}(-1)\kappa(r, \phi)\rho_\text{in}(-1) && \forall \phi \in\ [0, 2\pi) \,. \\
\intertext{In order to simplify this constraint further we define a transformed kernel $\kappa'(r,\phi)\ :=\ \ \kappa(r, \phi+\beta)$ which is oriented relative to the reflection axis.
The transformed kernel is then required to satisfy}
    \kappa'(r, \beta -\phi)\ =&\ \ \rho_\text{out}(-1)\kappa'(r, \phi - \beta)\rho_\text{in}(-1)  && \forall \phi \in\ [0, 2\pi) \,, \\
\intertext{which, with the change of variables $\phi'=\phi-\beta\,$, reduces to the constraint for equivariance under reflections around the x-axis, i.e. the case for $\beta=0\,$:}
    \kappa'(r, -\phi') =&\ \ \rho_\text{out}(-1)\kappa'(r, \phi') \rho_\text{in}(-1)  && \forall \phi' \in\ [0, 2\pi) \,. \\
\end{align*}
As a consequence we can retrieve kernels equivariant under reflections around the $\beta$-axis through
\[
    \kappa(r, \phi)\ :=\ \ \kappa'(r, \phi - \beta) \,.
\]
We will therefore without loss of generality consider the case $\beta=0$ only in the following.



~\\[-4.ex]
\paragraph{1-dimensional irreps:}~\\[.75ex] 

The reflection group $\Flip$ has only two irreps, namely the trivial representation $\psi_0^{\Flip}(s)=1$ and the sign-flip representation $\psi_1^{\Flip}(s)=s$.
Therefore only the 1-dimensional case with a kernel of form $\kappa:\R^2\to\R^{1\times1}$ exists.
Note that we can write the irreps out as $\psi_{f}^{\Flip}(s)=s^f$, in particular $\psi_{f}^{\Flip}(-1)=(-1)^f$.

Consider the output and input irreps $\rho_\text{out}=\psi_{i}^{\Flip}$ and $\rho_\text{in}=\psi_{j}^{\Flip}$ (with $i, j \in \{0, 1\}$) and the usual 1-dimensional Fourier basis for scalar functions in $L^2(S^1)$ as before:
\begin{align}
\label{eq:flip_1x1_starting_basis}
\Bigg\{ b_{\mu,\gamma}(\phi) = \cos(\mu\phi +\gamma) \ \Bigg|\ \ \mu \in \N,\ \gamma \in 
        \begin{cases}
            \{0\}        &\!\!\text{if}\ \mu=0 \\[-1pt]
            \{0, \pi/2\} &\!\!\text{otherwise}
        \end{cases}
        \ 
    \Bigg\}
\end{align}

Defining the reflection operator $S$ by its action $\lp S\,\kappa\rp (\phi) := \kappa(-\phi)$, we require the projections of both sides of the kernel constraint on the same basis element to be equal as usual.
Specifically, for a particular basis $b_{\mu',\gamma'}$:
\[
    \langle b_{\mu',\gamma'}, S\ \kappa \rangle = \langle b_{\mu',\gamma'}, \psi_{i}^{\Flip}(-1)\kappa(\cdot)\psi_{j}^{\Flip}(-1)\rangle
\]

The \textit{lhs} implies
\begin{align*}
    \langle b_{\mu',\gamma'}, S\ \kappa \rangle
    &= \sum_{\mu, \gamma} w_{\mu,\gamma} {1\over2\pi} \int d\phi \ \ b_{\mu',\gamma'}(\phi) b_{\mu,\gamma}(-\phi) \\
    &= \sum_{\mu, \gamma} w_{\mu,\gamma} {1\over2\pi} \int d\phi \ \ \cos(\mu'\phi + \gamma') \cos(-\mu\phi + \gamma) \\
    &= \sum_{\mu, \gamma} w_{\mu,\gamma} {1\over2\pi} \int d\phi \ \ {1\over2} \lp \cos((\mu' + \mu)\phi + (\gamma'-\gamma)) + \cos((\mu' - \mu)\phi + (\gamma'+\gamma))\rp\\
    &= \sum_{\gamma} w_{\mu',\gamma} {1\over2} \lp \cos(\gamma'+ \gamma) + \delta_{\mu', 0}\cos(\gamma'-\gamma)\rp
\end{align*}
while the \textit{rhs} leads to
\begin{flalign*}
    &\langle b_{\mu',\gamma'}, \psi_m^{\Flip}(-1) \kappa(\cdot) \psi_n^{\Flip}(-1) \rangle &&\\
    &\qquad\qquad= \sum_{\mu, \gamma} w_{\mu,\gamma} {1\over2\pi} \int d\phi \ \ b_{\mu',\gamma'}(\phi) \psi_i^{\Flip}(-1) b_{\mu,\gamma}(\phi) \psi_j^{\Flip}(-1)&&\\
    &\qquad\qquad= \sum_{\mu, \gamma} w_{\mu,\gamma} {1\over2\pi} \int d\phi \ \ \cos(\mu'\phi + \gamma') (-1)^{i} \cos(\mu\phi + \gamma) (-1)^{j}&&\\
    &\qquad\qquad= (-1)^{i+j} \sum_{\mu, \gamma} w_{\mu,\gamma} {1\over2\pi} \int d\phi \ \ \cos(\mu'\phi + \gamma') \cos(\mu\phi + \gamma)&&\\
&\qquad\qquad= (-1)^{i+j} \sum_{\gamma} w_{\mu',\gamma} {1\over2} \lp \cos(\gamma'- \gamma) + \delta_{\mu', 0}\cos(\gamma'+\gamma)\rp \,.&&
\end{flalign*}

Now, we require both sides to be equal, that is, \begin{align*}
\sum_{\gamma} w_{\mu',\gamma} {1\over2} \lp \cos(\gamma'+ \gamma) + \delta_{\mu', 0}\cos(\gamma'\shortminus\gamma)\rp &=
(\shortminus1)^{i+j} \sum_{\gamma} w_{\mu',\gamma} {1\over2} \lp \cos(\gamma'\shortminus \gamma) + \delta_{\mu', 0}\cos(\gamma'+\gamma)\rp
\end{align*}
and again consider two cases for $\mu'$:
\begin{itemize}
	\item[$\bullet\,\mu' = 0$]
		The basis in Eq.\eqref{eq:flip_1x1_starting_basis} is restricted to the single case $\gamma' = 0$. Hence:
		\begin{align*}
			&& \sum_{\gamma} w_{0,\gamma} {1\over2} \lp \cos(\gamma) + \cos(-\gamma)\rp &= (-1)^{i+j} \sum_{\gamma} w_{0,\gamma} {1\over2} \lp \cos(- \gamma) + \cos(\gamma)\rp\\
		\intertext{As $\gamma \in \{0, {\pi\over 2}\}$ and $\cos(\pm{\pi\over 2}) = 0$:}
			\Leftrightarrow\quad&& w_{0,0} {1\over2} \lp \cos(0) + \cos(0)\rp &= (-1)^{i+j} w_{0,0} {1\over2} \lp \cos(- 0) + \cos(0)\rp \\
			\Leftrightarrow\quad&& w_{0,0} &= (-1)^{i+j} w_{0,0}\\
		\end{align*}
		Which is always true when $i = j$, while it enforces $w_{0,0} = 0$ when $i \neq j$.
	\item[$\bullet\,\mu' > 0$] In this case we get:
		\begin{align*}
								&&
			\sum_{\gamma} w_{\mu',\gamma} {1\over2} \cos(\gamma'+ \gamma) &= (-1)^{i+j} \sum_{\gamma} w_{\mu',\gamma} {1\over2} \cos(\gamma'- \gamma)\\
			\Leftrightarrow\quad&&
			(1 - (-1)^{i+j})w_{\mu',0} \cos(\gamma') &= (1 + (-1)^{i+j})w_{\mu, {\pi\over2}}\sin(\gamma') 
		\end{align*}
		If $i + j \equiv 0 \mod 2$, the equation becomes $\sin(\gamma') = 0$ and, so, $\gamma' = 0$.
		Otherwise, it becomes $\cos(\gamma') = 0$, which means $\gamma' = {\pi\over2}$.
		Shortly, $\gamma' = (i+j \mod 2) {\pi\over2}$.
\end{itemize}

As a result, only half of the basis for $\beta=0$ is preserved: \begin{align}
\label{eq:flip_1x1_basis}
\mathcal{K}^{\Flip,\beta=0}_{\psi_i\leftarrow\psi_j}\ =\ 
	\left\{ b_{\mu,\gamma}(\phi) = \cos(\mu\phi +\gamma) \ \bigg|\ \ \mu \in \N,\ \gamma = (i+j \mod 2){\pi\over2},\ \mu > 0 \vee \gamma = 0 \right\}
\end{align}

The solution for a general reflection axis $\beta$ is therefore given by
\begin{empheq}[box=\kernelspace]{align}
\label{eq:flip_1x1_general_basis}
    \mathcal{K}^{\Flip,\beta}_{\psi_i\leftarrow\psi_j} =
\left\{ b_{\mu,\gamma}(\phi)\! =\! \cos(\mu(\phi \shortminus \beta) +\gamma) \bigg| \mu \in \N, \gamma\! =\! (i+j \!\!\!\mod 2){\pi\over2}, \mu > 0 \vee \gamma = 0 \! \right\}\!\!\!\!
\end{empheq}
which is visualized in Table~\ref{tab:Reflection_irrep_solution_appendix} for the different cases of irreps for $i,j\in\{0,1\}$.

 \vspace*{2.ex}

\subsubsection{Derivation for O(2)}
\label{apx:derivation_irrep_constraint_O2}

The orthogonal group $\O2$ is the semi-direct product between the rotation group $\SO2$ and the reflection group $\Flip$, i.e. $\O2 \cong \SO2 \rtimes \Flip$.
This justifies a decomposition of the constraint on $\O2$-equivariant kernels as the union of the constraints for rotations and reflections.
Consequently, the space of $\O2$-equivariant kernels is the intersection between the spaces of $\SO2$- and reflection-equivariant kernels.

\paragraph{Proof}
~\\[2ex]
\textit{Sufficiency}:
\begin{addmargin}[2em]{0em}
	Assume a rotation- and reflection-equivariant kernel, i.e. a kernel which for all $r \in {\R}^+_0$ and $\phi \in [0, 2\pi)$ satisfies
	\begin{align*}
		\kappa(r, r_{\theta} \phi)
		&\ =& \hspace*{-1em}
		     \lp\Res{\SO2}{\O2}\rho_\text{out}\rp\!\!(r_\theta)\ &\kappa(r, \phi)  \lp\Res{\SO2}{\O2}\rho_\text{in}\rp^{-1}\!\!(r_\theta) && \forall\ r_\theta \in \SO{2}\phantom{.} \\
		&\ =& \rho_\text{out}(r_\theta)\                          &\kappa(r, \phi)\ \rho_\text{in}^{-1}(r_\theta) \\
	\intertext{and}
		\kappa(r, s \phi)
		&\ =& \hspace*{-1em}
			\lp\Res{\Flip}{\O2}\rho_\text{out}\rp\!\!(s)\         &\kappa(r, \phi)  \lp\Res{\Flip}{\O2}\rho_\text{in}\rp^{-1}\!\!(s) && \forall\ s \in \Flip \\
		&\ =& \phantom{_\theta}\rho_\text{out}(s)\                &\kappa(r, \phi)\ \rho_\text{in}^{-1}(s) \,.
	\end{align*}
	Then, for any $g=r_{\theta}s \in\O2$, the kernel constraint becomes:
	\begin{align*}
		\kappa(r, g \phi) &= \rho_\text{out}(g)\ \kappa(r, \phi)\ \rho_\text{in}^{-1}(g) \\
		\Leftrightarrow\quad
		\kappa(r, r_\theta s \phi) &= \rho_\text{out}(r_\theta s)\ \kappa(r, \phi)\ \rho_\text{in}^{-1}(r_\theta s) \\
		\Leftrightarrow\quad
		\kappa(r, r_\theta s \phi) &= \rho_\text{out}(r_\theta) \rho_\text{out}(s)\ \kappa(r, \phi)\ \rho_\text{in}^{-1}(s)\rho_\text{in}^{-1}(r_\theta) \,.\\
	\intertext{Applying reflection-equivariance this equation simplifies to}
		\Leftrightarrow\quad
		\kappa(r, r_\theta s \phi) &= \rho_\text{out}(r_\theta)\ \kappa(r, s\phi)\ \rho_\text{in}^{-1}(r_\theta) \,,\\
	\intertext{which, applying rotation-equivariance yields}
		\Leftrightarrow\quad
		\kappa(r, r_\theta s \phi) &= \kappa(r, r_\theta s \phi) \,.
	\end{align*}
	Hence any kernel satisfying both $\SO2$ and reflection constraints is also $\O2$ equivariant.
\end{addmargin}

\textit{Necessity}:
\begin{addmargin}[2em]{0em}
	Trivially, $\O2$ equivariance implies equivariance under $\SO2$ and reflections.
	Specifically, for any $r \in {\R}^+_0$ and $\phi \in [0, 2\pi)$, the equation
	\begin{align*}
	\kappa(r, g \phi) &\ =\ \rho_\text{out}(g)\ \kappa(r, \phi)\ \rho_\text{in}^{-1}(g) && \forall\ g = r_\theta s \in \O{2} \\
	\end{align*}
	implies
	\begin{align*}
		\kappa(r, r_{\theta} \phi)
		&\ =& \rho_\text{out}(r_\theta)\                          &\kappa(r, \phi)\ \rho_\text{in}^{-1}(r_\theta) \\
		&\ =& \hspace*{-1em}
		     \lp\Res{\SO2}{\O2}\rho_\text{out}\rp\!\!(r_\theta)\ &\kappa(r, \phi)  \lp\Res{\SO2}{\O2}\rho_\text{in}\rp^{-1}\!\!(r_\theta) && \forall\ r_\theta \in \SO{2}\phantom{.} \\
	\intertext{and}
		\kappa(r, s \phi)
		&\ =& \phantom{_\theta}\rho_\text{out}(s)\                &\kappa(r, \phi)\ \rho_\text{in}^{-1}(s) \\
		&\ =& \hspace*{-1em}
			\lp\Res{\Flip}{\O2}\rho_\text{out}\rp\!\!(s)\         &\kappa(r, \phi)  \lp\Res{\Flip}{\O2}\rho_\text{in}\rp^{-1}\!\!(s) && \forall\ s \in \Flip.
\end{align*}
\end{addmargin}

This observation allows us to derive the kernel space for $\O2$ by intersecting the previously derived kernel space of $\SO2$ with the kernel space of the reflection group:
\begin{align*}
&\quad\mathcal{K}^{\O2}_{\rho_\text{out}\leftarrow\rho_\text{in}} =
	\big\{\kappa\ |\ \kappa(r, g.\phi) = \rho_\text{out}(g) \kappa(r, \phi) \rho_\text{in}(g)^{-1}\ \forall\ g \in \O2 \big\} &&\\
	& \qquad\qquad\qquad\qquad\qquad\qquad          = \ \phantom{\cap}\ \big\{\kappa\ |\ \kappa(r, r_{\theta}.\phi) = \rho_\text{out}(r_{\theta}) \kappa(r, \phi) \rho_\text{in}(r_{\theta})^{-1}\ \forall\ r_{\theta} \in \SO2 \big\} &&\\
	& \qquad\qquad\qquad\qquad\qquad\qquad \phantom{=}\          \cap \ \big\{\kappa\ |\ \kappa(r, \phantom{_{\theta}}s.\phi) = \phantom{_{\theta}}\rho_\text{out}(s)\kappa(r, \phi)\rho_\text{in}(s)^{-1}\phantom{_{\theta}}\,\,\forall\ s \in \Flip \big\} &&
\end{align*}

As $\O2$ contains all rotations, it does also contain all reflection axes. 
Without loss of generality, we define $s \in \O2$ as the reflection along the $x$-axis. 
A reflection along any other axis $\beta$ is associated with the group element $r_{2\beta}s \in \O2$, i.e. the combination of a reflection with a rotation of $2\beta$.
As a result, we consider the basis for reflection equivariant kernels derived for $\beta = 0$ in Eq. \eqref{eq:flip_1x1_basis}.

Therefore, to derive a basis associated to a pair of input and output representations $\rho_\text{in}$ and $\rho_\text{out}$, we restrict the representations to $\SO2$ and the reflection group, compute the two bases using the results found in \apx \ref{apx:derivation_irrep_constraint_SO2} and in \apx \ref{apx:derivation_irrep_constraint_Flip}, and, finally, take their intersection.

~\\[-4.ex]
\paragraph{2-dimensional irreps:}~\\[.75ex] 
The restriction of any 2-dimensional irrep $\psi^{\O2}_{1,n}$ of $\O2$ to the reflection group decomposes into the direct sum of the two 1-dimensional irreps of the reflection group, i.e. into the diagonal matrix
\[
	\Res{\Flip}{\O2}\psi_{1,n}(s) = \left(\psi_0^{\Flip} \oplus \psi_1^{\Flip}\right)\!(s) = 
	\begin{bmatrix}
		\psi_0^{\Flip}(s) & \!\!\! 0 \\
		0 & \!\!\!\psi_1^{\Flip}(s) \\
	\end{bmatrix} 
	= \XI{s}.
\]
It follows that the restricted kernel space constraint decomposes into independent constraints on each entry of the original kernel.
Specifically, for output and input representations $\rho_\text{out} = \psi_{1,m}^{\O2}$ and $\rho_\text{in} = \psi_{1,n}^{\O2}$, the constraint becomes
\[
	\kappa(s.x) = 
	\\
	\underbrace{
		\left(
		{\setlength\arraycolsep{3pt}
			\def\arraystretch{1.5}
			\begin{array}{c|c}
			\psi_0^{\Flip}(s) & \\ \hline
			& \psi_1^{\Flip}(s) \\
			\end{array}
		}
		\right)
	}_{
	\textstyle\begin{array}{c}
		\Res{\Flip}{\O2}\rho_\text{out}(s)
	\end{array}
	}
	\cdot
	\underbrace{
		\left(
		{\setlength\arraycolsep{3pt}
			\def\arraystretch{1.5}
			\begin{array}{c|c}
			\kappa^{00} & \kappa^{01}\\ \hline
			\kappa^{10} & \kappa^{11}\\
			\end{array}
		}
		\right)
	}_{
	\textstyle\begin{array}{c}
		\kappa(x)
	\end{array}
	}
	\cdot
	\underbrace{
		\left(
		{\setlength\arraycolsep{3pt}
			\def\arraystretch{1.5}
			\begin{array}{c|c}
			\psi_0^{\Flip}(s)^{-1} & \\ \hline
			& \psi_1^{\Flip}(s)^{-1} \\
			\end{array}
		}
		\right)
	}_{
	\textstyle\begin{array}{c}
		\Res{\Flip}{\O2}\rho_\text{in}(s)
	\end{array}
	}
\]
We can therefore solve for a basis for each entry individually following \apx~\ref{apx:derivation_irrep_constraint_Flip} to obtain the complete basis
\begin{align*}
	\big\{b^{00}_{\mu,0\phantom{{\pi\over2}}}(\phi) = \begin{bmatrix} \cos(\mu\phi) & 0 \\ 0 & 0 \end{bmatrix}  \bigg|\ \mu \in {\N}^{\phantom{+}} \big\} \ \ \cup \ \ &           
	\big\{b^{01}_{\mu,{\pi\over2}\phantom{0}}(\phi) = \begin{bmatrix} 0 & \sin(\mu\phi) \\ 0 & 0 \end{bmatrix}  \bigg|\ \mu \in {\N}^+ \big\} \ \ \cup \ \ & \\
	\big\{b^{10}_{\mu,{\pi\over2}\phantom{0}}(\phi) = \begin{bmatrix} 0 & 0 \\ \sin(\mu\phi) & 0 \end{bmatrix}  \bigg|\ \mu \in {\N}^+ \big\} \ \ \cup \ \ &           
	\big\{b^{11}_{\mu,0\phantom{{\pi\over2}}}(\phi) = \begin{bmatrix} 0 & 0 \\ 0 & \cos(\mu\phi)\hspace*{-1pt} \end{bmatrix}  \bigg|\ \mu \in {\N}^{\phantom{+}} \big\}. \ \ \phantom{\cup} \ \ &
\end{align*}


Through the same change of basis applied in the first paragraph of \apx \ref{apx:derivation_irrep_constraint_SO2}, we get the following equivalent basis for the same space:
\begin{align*}
	   &\bigg\{b_{\mu,s}(\phi) = \PSI{\mu\phi}\XI{s} \bigg\}_{\mu \in \Z, s \in \{\pm 1\}} \\
	=\ &\big\{ b_{\mu,s}(\phi)  = \psi(\mu\phi)\xi(s) \big\}_{\mu \in \Z, s \in \{\pm 1\}}\ . \stepcounter{equation}\tag{\theequation}\label{eq:o2_2x2_starting_basis}
\end{align*}

On the other hand, 2-dimensional $\O2$ representations restrict to the $\SO2$ irreps of the corresponding frequency, i.e. 
\[
	\Res{\SO2}{\O2}\rho_\text{in}\ =\ \Res{\SO2}{\O2}\psi_{1,n}^{\O2}(r_\theta)\ =\ \psi_n^{\SO2}(r_\theta)
\]
and
\[
	\Res{\SO2}{\O2}\rho_\text{out}\ =\ \Res{\SO2}{\O2}\psi_{1,m}^{\O2}(r_\theta)\ =\ \psi_m^{\SO2}(r_\theta).
\]

In \apx \ref{apx:derivation_irrep_constraint_SO2}, a basis for $\SO2$-equivariant kernels with respect to a $\psi_n^{\SO2}$ input field and $\psi_m^{\SO2}$ output field was derived starting from the basis in Eq. \eqref{eq:so2_2x2_starting_basis}.
Notice that the basis of reflection-equivariant kernels in Eq. \eqref{eq:o2_2x2_starting_basis} contains exactly half of the elements in Eq. \eqref{eq:so2_2x2_starting_basis}, indexed by $\gamma = 0$.
A basis for $\O2$-equivariant kernels can be found by repeating the derivations in \apx \ref{apx:derivation_irrep_constraint_SO2} for $\SO2$-equivariant kernels using only the subset in Eq. \eqref{eq:o2_2x2_starting_basis} of reflection-equivariant kernels.
The resulting two-dimensional $\O2$-equivariant basis, which includes the $\SO2$-equivariance conditions ($\mu = m - sn$) and the reflection-equivariance conditions ($\gamma = 0$), is given by
\begin{empheq}[box=\kernelspace]{align}
\label{eq:o2_2x2_basis}
	\mathcal{K}^{\O2}_{\psi_{i,m}\leftarrow\psi_{j,n}}\ =\
	\left\{ b_{\mu,0,s}(\phi) = \psi(\mu\phi) \xi(s) \ \bigg|\ \ \mu = m - sn, s\in \{\pm1\} \right\} \ ,\!\!\!
\end{empheq}
where $i = j = 1$ and $m, n > 0$.
See the bottom right cell in Table~\ref{tab:O2_irrep_solution_appendix}.



~\\[-4.ex]
\paragraph{1-dimensional irreps:}~\\[.75ex] 
$\O2$ has two 1-dimensional irreps $\psi^{\O2}_{0,0}$ and $\psi^{\O2}_{1,0}$(see \apx \ref{apx:irreps}).
Both are trivial under rotations and each of them corresponds to one of the two reflection group's irreps, i.e.
\begin{alignat*}{2}
	\Res{\Flip}{\O2}\psi_{i,0}^{\O2}(s) &= \psi_i^{\Flip}(s) &= s^i\\
	\intertext{and}
	\Res{\SO2}{\O2}\psi_{i,0}^{\O2}(r_\theta) &= \psi_0^{\SO2}(r_\theta) &= 1.
\end{alignat*}
Considering output and input representations $\rho_\text{out} = \psi_{i,0}^{\O2}$ and $\rho_\text{in} = \psi_{j,0}^{\O2}$, it follows that:
\begin{alignat*}{4}
	\Res{\Flip}{\O2}\rho_\text{in}  &= \Res{\Flip}{\O2}\psi_{j,0}^{\O2}&&= \psi_j^{\Flip}\\
	\Res{\Flip}{\O2}\rho_\text{out} &= \Res{\Flip}{\O2}\psi_{i,0}^{\O2}&&= \psi_i^{\Flip}\\
	\Res{\SO2}{\O2}\rho_\text{in}   &= \Res{\SO2}{\O2}\ \psi_{j,0}^{\O2}&&= \psi_0^{\SO2}\\
	\Res{\SO2}{\O2}\rho_\text{out}  &= \Res{\SO2}{\O2}\ \psi_{i,0}^{\O2}&&= \psi_0^{\SO2}
\end{alignat*}

In order to solve the $\O2$ kernel constraint consider again the reflectional constraint and the $\SO2$ constraint.
Bases for reflection-equivariant kernels with above representations were derived in \apx \ref{apx:derivation_irrep_constraint_Flip} and are shown in Eq. \eqref{eq:flip_1x1_basis}.
These bases form a subset of the Fourier basis in Eq.~\eqref{eq:so2_1x1_starting_basis} which is being indexed by $\gamma = (i + j \mod 2) {\pi\over2}$.
On the other hand, the full Fourier basis was restricted by the $\SO2$ constraint to satisfy $\mu = 0$ and therefore $\gamma = 0$, see Eq.~\eqref{eq:so2_1x1_basis}. 
Intersecting both constraints therefore implies $i=j$, resulting in the $\O2$-equivariant basis
\begin{empheq}[box=\kernelspace]{align}
\label{eq:o2_1x1_basis}
	\mathcal{K}^{\O2}_{\psi_{i,m}\leftarrow\psi_{j,n}}\ =\
	\begin{cases}
		\left\{ b_{0,0}(\phi) = 1 \right\} &\ \text{if $i = j$, } \\
		\qquad\ \,\emptyset				   &\ \text{else}
	\end{cases}
\end{empheq}
for $m, n = 0$ which is shown in the top left cell in Table~\ref{tab:O2_irrep_solution_appendix}.



~\\[-4.ex]
\paragraph{1 and 2-dimensional irreps:}~\\[.75ex] 
Now we consider the 2-dimensional output representation $\rho_\text{out} = \psi_{1,m}^{\O2}$ and the 1-dimensional input representation $\rho_\text{in} = \psi_{j,0}^{\O2}$.

Following the same strategy as before we find the reflectional constraints for these representations to be given by \[
\kappa(s.x) = 
\\
\underbrace{
	\left(
	{\setlength\arraycolsep{3pt}
		\def\arraystretch{1.5}
		\begin{array}{c|c}
		\psi_0^{\Flip}(s) & \\ \hline
		& \psi_1^{\Flip}(s) \\
		\end{array}
	}
	\right)
}_{
\textstyle\begin{array}{c}
\Res{\Flip}{\O2}\rho_\text{out}(s)
\end{array}
}
\cdot
\underbrace{
	\left(
	{\setlength\arraycolsep{3pt}
		\def\arraystretch{1.5}
		\begin{array}{c}
		\kappa^{00} \\ \hline
		\kappa^{10} \\
		\end{array}
	}
	\right)
}_{
\textstyle\begin{array}{c}
\kappa(x)
\end{array}
}
\cdot
\underbrace{
	\left(
	{\setlength\arraycolsep{3pt}
		\def\arraystretch{1.5}
		\begin{array}{c}
		\psi_j^{\Flip}(s)^{-1} \\
		\end{array}
	}
	\right)
}_{
\textstyle\begin{array}{c}
\Res{\Flip}{\O2}\rho_\text{in}(s)
\end{array}
}\ ,
\]
and therefore to decompose into two independent constraints on the entries $\kappa^{00}$ and $\kappa^{10}$.
Solving for a basis for each entry and taking their union as before we get\footnote{Notice that for $\mu=0$ some of the elements of the set are zero and are therefore not part of the basis. We omit this detail to reduce clutter.}
\begin{align*}
\bigg\{b^{00}_{\mu}(\phi) = \begin{bmatrix} \cos(\mu\phi + j{\pi\over2}) \\ 0 \end{bmatrix} \bigg\}_{\mu \in \N } \ &\cup \ \ 
	\bigg\{b^{10}_{\mu}(\phi) = \begin{bmatrix} 0 \\ \sin(\mu\phi -j{\pi\over2}) \end{bmatrix} \bigg\}_{\mu \in \N },
\end{align*}
which, through a change of basis, can be rewritten as
\begin{align}
\label{eq:o2_1x2_starting_basis}
	\bigg\{b_{\mu,j{\pi\over2}}(\phi) = \begin{bmatrix} \cos(\mu\phi + j{\pi\over2})\\ \sin(\mu\phi + j{\pi\over2}) \end{bmatrix} \bigg\}_{\mu \in \Z }.
\end{align}

We intersect this basis with the basis of $\SO2$ equivariant kernels 
with respect to a $\Res{\SO2}{\O2}\rho_\text{in} = \psi_0^{\SO2}$ input field and $\Res{\SO2}{\O2}\rho_\text{out} = \psi_m^{\SO2}$ output field as derived in \apx \ref{apx:derivation_irrep_constraint_SO2}.
Both constraints, that is, $\gamma = j{\pi\over2}$ for the reflection group and $\mu=m$ for $\SO2$ (see Eq.~\eqref{eq:so2_1x2_starting_basis}), 
define the one-dimensional basis for $\O2$-equivariant kernels for $n=0$, $m>0$ and $i=1$ as
\begin{empheq}[box=\kernelspace]{align}
\label{eq:o2_1x2_basis}
	\mathcal{K}^{\O2}_{\psi_{i,m}\leftarrow\psi_{j,n}}\ =\
	\left\{ b_{\mu,j{\pi\over2}}(\phi) = \begin{bmatrix} \cos(\mu\phi + j{\pi\over2})\\ \sin(\mu\phi + j{\pi\over2}) \end{bmatrix} \ \bigg|\ \ \mu = m \right\},
\end{empheq}
see the bottom left cell in Table~\ref{tab:O2_irrep_solution_appendix}.


~\\[-4.ex]
\paragraph{2 and 1-dimensional irreps:}~\\[.75ex] 
As already argued in the case for $\SO2$, the basis for 2-dimensional input representations $\rho_\text{in}=\psi_{1,n}^{\O2}$ and 1-dimensional output representations $\rho_\text{out}=\psi_{i,0}^{\O2}$ is identical to the previous basis up to a transpose, i.e. it is given by
\begin{empheq}[box=\kernelspace]{align}
\label{eq:o2_2x1_basis}
	\mathcal{K}^{\O2}_{\psi_{i,m}\leftarrow\psi_{j,n}}\ =\
	\left\{ b_{\mu,i{\pi\over2}}(\phi) = \begin{bmatrix} \cos(\mu\phi + i{\pi\over2}) & \sin(\mu\phi + i{\pi\over2}) \end{bmatrix} \ \bigg|\ \ \mu = n \right\},
\end{empheq}
where $j=1$, $n>0$ and $m=0$.
This case is visualized in the top right cell of Table~\ref{tab:O2_irrep_solution_appendix}.
 \vspace*{2.ex}


\subsubsection{Derivation for $\bCN$}
\label{apx:derivation_irrep_constraint_CN}

The derivations for $\CN$ coincide mostly with the derivations done for $\SO2$ with the difference that the projected constraints need to hold for discrete angles $\theta \in \big\{p {2\pi\over N} \,|\, p=0,\dots,N-1\big\}$ only.
Furthermore, $\CN$ has one additional 1-dimensional irrep of frequency$N/2$ if (and only if) $N$ is even.

\paragraph{2-dimensional irreps:}~\\[.75ex]
During the derivation of the solutions for $\SO2$'s 2-dimensional irreps in \apx \ref{apx:derivation_irrep_constraint_SO2}, we assumed continuous angles only in the very last step.
The constraint in Eq. \eqref{eq:so2_2x2_solution} therefore holds for $\CN$ as well.
Specifically, it demands that for each $\theta \in \{p {2\pi\over N} \,|\, p=0,\dots,N-1\}$ there exists a $t\in\Z$ such that:
\begin{align*}
&				& (\mu' - (m -ns'))\theta &= 2t\pi &&\\
&\Leftrightarrow& (\mu' - (m -ns'))p {2\pi\over N} &= 2t\pi &&\\
	&\Leftrightarrow& (\mu' - (m -ns'))p  &= tN &&
\end{align*}
The last result corresponds to a system of $N$ linear congruence equations modulo $N$ which require $N$ to divide $(\mu'-(m-ns'))p$ for each non-negative integer $p$ smaller than $N$.
Note that solutions of the constraint for $p=1$ already satisfy the constraints for $p\in2,\dots,N-1$ such that it is sufficient to consider
\begin{align*}
	&				& (\mu' - (m - ns')) 1  &= tN &&\\
	&\Leftrightarrow& \mu' &= m - ns'+ tN \,.	  &&
\end{align*}
The resulting basis
\begin{empheq}[box=\kernelspace]{align}
\label{eq:cn_2x2_basis}
	\!\!\!\!\!\!\mathcal{K}^{\CN}_{\psi_{m}\leftarrow\psi_{n}} \!\! =\!
	\left\{ b_{\mu,\gamma,s}(\phi) = \psi(\mu\phi +\gamma) \xi(s) \bigg|\, \mu=m\shortminus sn+tN, \gamma\in\left\{0,{\pi\over2}\right\}s \in \{\pm1\} \right\}_{t\in\Z} \!\!\!\!\!\!\!
\end{empheq}
for $m,n>0$ thus coincides mostly with the basis~\ref{eq:so2_2x2_basis} for $\SO2$ but contains solutions for aliased frequencies, defined by adding $tN$.
The bottom right cell in Table~\ref{tab:CN_irrep_solution_appendix} gives the explicit form of this basis.

~\\[-4.ex]
\paragraph{1-dimensional irreps:}~\\[.75ex]
The same trick could be applied to solve the remaining three cases.
However, since $\CN$ has an additional one dimensional irrep of frequency $N/2$ for even $N$ it is convenient to rederive all cases.
We therefore consider $\rho_\text{out}=\psi_m^{\CN}$ and $\rho_\text{in}=\psi_n^{\CN}$, where $m,n \in \{0, N/2\}$.
Note that $\psi_m^{\CN}(\theta),\psi_n^{\CN}(\theta) \in \{\pm 1\}$ for $\theta\in\{p {2\pi\over N} \,|\, p=0,\dots,N-1\}$.

We use the same Fourier basis 
\begin{align}\label{eq:cn_1x1_starting_basis}
\Bigg\{ b_{\mu,\gamma}(\phi) = \cos(\mu\phi +\gamma) \ \Bigg|\ \ \mu \in \N,\ \gamma \in 
        \begin{cases}
            \{0\}        &\!\!\text{if}\ \mu=0 \\[-1pt]
            \{0, \pi/2\} &\!\!\text{otherwise}
        \end{cases}
        \ 
    \Bigg\}
\end{align}
and the same projection operators as used for $\SO2$.

Since the \textit{lhs} of the kernel constraint does not depend on the representations considered its projection $\langle b_{\mu',\gamma'},  R_{\theta} \kappa \rangle$ is the same found for $\SO2$:
\begin{align*}
	& \langle b_{\mu',\gamma'},  R_{\theta} \kappa \rangle
	= {1\over2} \sum_{\gamma} w_{\mu', \gamma} \lp\cos((\gamma'-\gamma) - \mu'\theta) + \delta_{\mu', 0}\cos(\gamma'+\gamma) \rp &&
\end{align*}

For the \textit{rhs} we find
\begin{align*}
	&\langle b_{\mu',\gamma'},  \psi_m^{\CN}(r_\theta)\kappa\ \psi_n^{\CN}(r_\theta)^{-1} \rangle && \\
	&\qquad\qquad= {1\over2\pi}\int d\phi \ b_{\mu',\gamma'}(\phi) \psi_m^{\CN}(r_\theta)\kappa(\phi)\psi_n^{\CN}(r_\theta)^{-1} \,, && \\
\intertext{which by expanding the kernel in the linear combination of the basis and writing the respresentations out yields:}
	&\qquad\qquad= \sum_{\mu, \gamma} w_{\mu, \gamma} {1\over2\pi}\int d\phi \ b_{\mu',\gamma'}(\phi) \cos(m\theta) b_{\mu,\gamma}(\phi) \cos(n\theta)^{-1} && \\
	&\qquad\qquad= \sum_{\mu, \gamma} w_{\mu, \gamma} {1\over2\pi}\int d\phi \ \cos(\mu'\phi + \gamma') \cos(m\theta) \cos(\mu\phi + \gamma) \cos(n\theta)^{-1} && \\
\intertext{Since $\cos(n\theta)\in\{\pm 1\}$ the inverses can be dropped and terms can be collected via trigonometric identities:}
	&\qquad\qquad= \sum_{\mu, \gamma} w_{\mu, \gamma} {1\over2\pi}\int d\phi \ \cos(\mu'\phi + \gamma') \cos(m\theta) \cos(\mu\phi + \gamma) \cos(n\theta) && \\
	&\qquad\qquad= \sum_{\mu, \gamma} w_{\mu, \gamma} \cos(m\theta) \cos(n\theta) {1\over2\pi}\int d\phi \ \cos(\mu'\phi + \gamma')  \cos(\mu\phi + \gamma) && \\
	&\qquad\qquad= \sum_{\mu, \gamma} w_{\mu, \gamma} \cos((\pm m \pm n) \theta) {1\over4\pi}\!\! \int\!\! d\phi  \Big(\!\! \cos(\!(\mu'\!\shortminus \mu)\phi + \gamma'\!\shortminus\gamma) + \cos((\mu' + \mu)\phi + \gamma'\!+\gamma) \!\Big) && \\
	&\qquad\qquad= \sum_{\mu, \gamma} w_{\mu, \gamma} \cos((\pm m \pm n) \theta) {1\over2} \lp \delta_{\mu, \mu'}\cos(\gamma'-\gamma) + \delta_{\mu+\mu', 0}\cos(\gamma'+\gamma) \rp && \\
	&\qquad\qquad= {1\over2} \sum_{\mu', \gamma} w_{\mu', \gamma} \cos((\pm m \pm n) \theta) \lp \cos(\gamma'-\gamma) + \delta_{\mu', 0}\cos(\gamma'+\gamma) \rp && 
\end{align*}

We require the projections to be equal for each $\theta=p{2\pi\over N}$ with $p\in\{0,\dots,N-1\}$:
\begin{align*}
    && \qquad\qquad\qquad\qquad\qquad \langle b_{\mu'\!,\gamma'},  R_{\theta} \kappa \rangle 
    & = \Big\langle b_{\mu'\!,\gamma'}, \psi_m^{\CN}(r_\theta)\kappa\ \psi_n^{\CN}(r_\theta)^{-1} \Big\rangle && \\
&\Leftrightarrow &\sum_{\gamma} w_{\mu', \gamma} \lp\cos((\gamma' - \gamma) -  \mu'\theta) + \delta_{\mu', 0}\cos(\gamma'+\gamma) \rp = \qquad\qquad\qquad\qquad\qquad\qquad \span&&\\
&& \span \qquad\qquad\qquad\qquad\qquad = \sum_{\mu', \gamma} w_{\mu', \gamma} \cos((\pm m \pm n) \theta) \Big( \cos(\gamma' -\gamma) + \delta_{\mu', 0}\cos(\gamma'+\gamma) \Big)&&
\end{align*}
Again, we consider two cases for $\mu'$:
\begin{itemize}
	\item[$\bullet\,\mu' = 0$]:
		The basis in Eq.\eqref{eq:cn_1x1_starting_basis} is restricted to the single case $\gamma' = 0$.
		\begin{align*}
			&				 & \span\span \sum_{\gamma} \!w_{0, \gamma} \!\lp\cos(-\gamma)  \!+\! \cos(\gamma)\rp
			& = \cos(\!(\!\pm m\! \pm\! n) \theta) \sum_{\gamma} \!w_{0, \gamma}  \Big(\! \cos(-\gamma) \!+\! \cos(\gamma) \!\Big) \\
			&\Leftrightarrow & \span\span w_{0, 0} 2 \cos(0) + w_{0, {\pi\over2}} 0 
			& = \cos((\pm m \pm n) \theta) \lp w_{0, 0} 2 \cos(0) + w_{0, {\pi\over2}} 0 \rp \\
			&\Leftrightarrow & \span\span w_{0, 0} &= \cos((\pm m \pm n) \theta) w_{0, 0} \\
			\intertext{If $\cos((\pm m \pm n)\theta) \neq 1$, the coefficient $w_{0,0}$ is forced to $0$.
				Conversely:
			}
			&                & \span\span 										\cos((\pm m \pm n)\theta) &= 1 \\
		    &\Leftrightarrow & \exists t \in \Z \text{ s.t. } \quad\quad &&			 (\pm m \pm n )\theta &= 2t\pi \\
\intertext{Using $\theta = p {2\pi \over N}$:}
		    &\Leftrightarrow & \exists t \in \Z \text{ s.t. } \quad\quad && (\pm m \pm n )p {2\pi\over N} &= 2t\pi \\
		    &\Leftrightarrow & \exists t \in \Z \text{ s.t. } \quad\quad &&				   (\pm m \pm n)p &= tN
		\end{align*}


	\item[$\bullet\,\mu' > 0$]:
		\begin{align*}
			&				 & \span\span
				\sum_{\gamma} w_{\mu', \gamma} \cos(\gamma'-\gamma - \mu'\theta) &=
				\cos((\pm m \pm n) \theta) \sum_{\gamma} w_{\mu', \gamma} \cos(\gamma'-\gamma) \\
			&\Leftrightarrow & \span\span
				w_{\mu', 0} \cos(\gamma'- \mu'\theta) + w_{\mu', {\pi\over2}} \sin(\gamma'- \mu'\theta) = \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \span \\
			&& \span\span\span
				\qquad\qquad \cos((\pm m \pm n) \theta) \lp w_{\mu', 0} \cos(\gamma') + w_{\mu', {\pi\over2}} \sin(\gamma') \rp\\
			\intertext{Since $(\pm m \pm n) \theta \in \{\shortminus\pi, 0, \pi\}$ we have $\cos((\pm m \pm n)\theta) = \pm 1 $, therefore:}
			&\Leftrightarrow & \span\span
w_{\mu', 0} \cos(\gamma'  \shortminus   \mu'\theta) +w_{\mu', {\pi\over2}} \sin(\gamma' \shortminus \mu'\theta) = \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \span \\
			&& \span\span
				\span \qquad\qquad = w_{\mu', 0} \cos(\gamma'+ (\pm m  \pm  n) \theta) + w_{\mu', {\pi\over2}} \sin(\gamma' + (\pm m  \pm  n) \theta)\\
			\intertext{Using the property in Eq. \eqref{eq:trick_trigonometry}:}
			&\Leftrightarrow & \exists t \in \Z \text{ s.t. } \quad\quad &&
				\gamma' - \mu'\theta &= \gamma' + (\pm m \pm n)\theta +2t\pi\\
			&\Leftrightarrow & \exists t \in \Z \text{ s.t. } \quad\quad &&
				\mu'\theta &= (\pm m \pm n)\theta +2t\pi\\
			\intertext{Using $\theta = p {2\pi \over N}$:}
			&\Leftrightarrow & \exists t \in \Z \text{ s.t. } \quad\quad &&
				\mu'p {2\pi\over N} &= (\pm m \pm n)p {2\pi\over N} +2t\pi\\
			&\Leftrightarrow & \exists t \in \Z \text{ s.t. } \quad\quad &&
				\mu'p &= (\pm m \pm n)p + tN\\
			&\Leftrightarrow & \exists t \in \Z \text{ s.t. } \quad\quad &&
				(\pm m \pm n + \mu') p &= tN\\
\end{align*}
\end{itemize}
In both cases $\mu'=0$ and $\mu'>0$ we thus find the constraints
\[
	\forall p\in\{0, 1, \dots, N-1\}\ \ \exists t \in \Z \text{ s.t. } \quad (\pm m\pm n+\mu') p = tN \,.
\]
It is again sufficient to consider the constraint for $p=1$ which results in solutions with frequencies $\mu'=\pm m\pm n+tN$.
As $(\pm m \pm n) \in \left\{0, \pm {N\over2}, \pm N\right\}$, all valid solutions are captured by $\mu' = (m+n\mod N)+tN$, resulting in the basis
\begin{empheq}[box=\kernelspace]{align}
\label{eq:cn_1x1_basis}
    \mathcal{K}^{\CN}_{\psi_{m}\leftarrow\psi_{n}}\!\! =\!
    \left\{\!b_{\mu,\gamma}(\phi)\! =\! \cos(\mu\phi\! +\! \gamma) \bigg| \mu\!=\!(m\! +\! n \!\!\! \mod\! N)\!+\! tN, \gamma\! \in\! \left\{0, {\pi\over2}\right\}\!, \mu\! \neq\! 0 \! \vee\! \gamma\! =\! 0 \right\}_{t \in \N} \!\!\!\!\!
\end{empheq}
for $n, m \in \left\{0, {N\over2}\right\}$.
See the top left cells in Table~\ref{tab:CN_irrep_solution_appendix}.


\paragraph{1 and 2-dimensional irreps}
Next consider a 1-dimensional irrep $\rho_\text{in}=\psi_n^{\CN}$ with $n\in\{0,{N\over2}\}$ in the input and a 2-dimensional irrep $\rho_\text{out}=\psi_m^{\CN}$ in the output.
We derive the solutions by projecting the kernel constraint on the basis introduced in Eq.~\eqref{eq:so2_1x2_starting_basis}.

For the \textit{lhs} the projection coincides with the result found for $\SO2$ as before:
\begin{align*}
	\langle b_{\mu',\gamma'},  R_{\theta} \kappa \rangle
	&= \sum_{\gamma} w_{\mu', \gamma} \cos((\gamma' -\gamma) - \mu'\theta)
\end{align*}

An expansion and projection of \textit{rhs} gives:
\begin{align*}
	&\langle b_{\mu',\gamma'},  \psi_m^{\CN}(r_\theta)\kappa(\cdot) \psi_n^{\CN}(r_\theta)^{-1}\rangle \\
	&\qquad\qquad= {1\over2\pi}\int d\phi \ b_{\mu',\gamma'}(\phi)^T \psi_m^{\CN}(r_\theta) \kappa(\phi) \psi_n^{\CN}(r_\theta)^{-1}\\
&\qquad\qquad= \sum_{\mu, \gamma} w_{\mu, \gamma} {1\over2\pi}\int d\phi \ b_{\mu',\gamma'}(\phi)^T 
	\psi_m^{\CN}(r_\theta) \begin{bmatrix} \cos(\mu\phi + \gamma) \\ \sin(\mu\phi + \gamma) \end{bmatrix} \psi_n^{\CN}(r_\theta)^{-1}\\
	&\qquad\qquad= \sum_{\mu, \gamma} w_{\mu, \gamma} {1\over2\pi}\!\int\!\! d\phi 
	\begin{bmatrix} \cos(\mu'\phi + \gamma') & \sin(\mu'\phi + \gamma') \end{bmatrix}
	\psi_m^{\CN}(r_\theta)\! \begin{bmatrix} \cos(\mu\phi + \gamma) \\ \sin(\mu\phi + \gamma) \end{bmatrix}\! \psi_n^{\CN}(r_\theta)^{\shortminus1}\\
	&\qquad\qquad= \sum_{\mu, \gamma} w_{\mu, \gamma} \lp {1\over2\pi}\int d\phi \ \cos(\mu'\phi + \gamma' -\mu\phi - \gamma -m\theta) \rp \psi_n^{\CN}(r_\theta)^{-1} \,. \\
\intertext{The integral is non-zero only if the frequency is $0$, i.e. iff $\mu'=\mu$: }
	&\qquad\qquad= \sum_{\gamma} w_{\mu', \gamma} \cos(\gamma'-\gamma -m\theta) \psi_n^{\CN}(r_\theta)^{-1} \\
	&\qquad\qquad= \sum_{\gamma} w_{\mu', \gamma} \cos(\gamma'-\gamma -m\theta) \cos(\pm n \theta) \\
\intertext{Since $\pm n\theta=p\pi$ for some $p\in\N$ one has $\sin(\pm n\theta)=0$ which allows to add the following zero summand and simplify:}
	&\qquad\qquad= \sum_{\gamma} w_{\mu', \gamma} \lp \cos(\gamma'-\gamma -m\theta) \cos(\pm n \theta) - \sin(\gamma'-\gamma -m\theta) \sin(\pm n \theta) \rp\\
	&\qquad\qquad= \sum_{\gamma} w_{\mu', \gamma} \cos(\gamma'-\gamma -(m \pm n) \theta)
\end{align*}

Requiring the projections to be equal then yields:
\begin{align*}
&&
	\langle b_{\mu',\gamma'},  R_{\theta} \kappa \rangle &= \langle b_{\mu',\gamma'},  \psi_m^{\CN}(r_\theta)\kappa(\cdot)\psi_n^{\CN}(r_\theta)^{-1} \rangle && \forall \theta \in  \left\{ p {2\pi\over N} \right\}\\
	\Leftrightarrow\quad&&
	\sum_{\gamma} w_{\mu', \gamma} \cos(\gamma'-\gamma - \mu'\theta) &=
	\sum_{\gamma} w_{\mu', \gamma} \cos(\gamma'-\gamma -(m \pm n) \theta) && \forall \theta \in  \left\{ p {2\pi\over N} \right\} \\
	\Leftrightarrow\quad&&
	w_{\mu'\!,\mkern-1mu 0} \cos(\gamma' \!\shortminus \!\mu'\theta) \!+ \!w_{\mu'\!,\mkern-2mu{\pi\over2}} \sin(\gamma' \!\shortminus \!\mu'\theta) &=
	w_{\mu'\!,\mkern-1mu 0} \cos(\gamma' \!\shortminus \!(m \!\pm \!n)\theta) \!+\! w_{\mu'\!,\mkern-2mu{\pi\over2}} \sin(\gamma' \!\shortminus\!(m \!\pm \!n)\theta) 
	\span\span
	\\
	&&
	& && \forall \theta \in  \left\{ p {2\pi\over N} \right\}\\
\intertext{Using the property in Eq. \eqref{eq:trick_trigonometry}, this requires that for each $\theta$ there exists a $t \in \Z$ such that:}
	\Leftrightarrow\quad&&
	\gamma' - \mu'\theta &= \gamma' - (m \pm n)\theta +2t\pi && \forall \theta \in  \left\{ p {2\pi\over N} \right\} \\
	\Leftrightarrow\quad&&
	\mu'\theta &= (m \pm n)\theta +2t\pi && \forall \theta \in  \left\{ p {2\pi\over N} \right\} \\
\intertext{Since $\theta = p{2\pi\over N}$ with $p \in \{0, \dots, N-1\}$ we find that}
	\Leftrightarrow\quad&&
	\mu'p{2\pi\over N} &= (m \pm n) p{2\pi\over N} +2t\pi && \mkern-45mu\forall p \in \{0, \dots, N\!\shortminus\!1\} \\
	\Leftrightarrow\quad&&
	\mu'p &= (m \pm n) p + tN && \mkern-45mu\forall p \in \{0, \dots, N\!\shortminus\!1\} \\
\Leftrightarrow\quad&&
	\mu' &= (m \pm n) + tN \\
	\Leftrightarrow\quad&&
	\mu' - (m \pm n) &= tN \,,
\end{align*}
which implies that $N$ needs to divide $\mu'-(m\pm n)$. 
It follows that the condition holds also for any other $p$.
This gives the basis
\begin{empheq}[box=\kernelspace]{align}
\label{eq:cn_2x1_basis}
	\mathcal{K}^{\CN}_{\psi_{m}\leftarrow\psi_{n}}\ =\
	\left\{ b_{\mu,\gamma}(\phi) = \begin{bmatrix} \cos(\mu\phi + \gamma)\\ \sin(\mu\phi + \gamma) \end{bmatrix} \ \bigg|\ \ \mu = (m \pm n) + tN,\ \gamma \in \left\{0, {\pi\over2}\right\}\right\}_{t \in \Z}
\end{empheq}
for $m>0$ and $n\in\left\{0,{N\over2}\right\}$; see the bottom left cells in Table~\ref{tab:CN_irrep_solution_appendix}.

~\\[-4.ex]
\paragraph{2 and 1-dimensional irreps:}~\\[.75ex]
The basis for 2-dimensional input and 1-dimensional output representations, i.e. $\rho_\text{in}=\psi_n^{\CN}$ and $\rho_\text{out}=\psi_m^{\CN}$ with $n>0$ and $m\in\{0,{N\over2}\}$, is identical to the previous one up to a transpose:
\begin{empheq}[box=\kernelspace]{align}
\label{eq:cn_1x2_basis}
	\mathcal{K}^{\CN}_{\psi_{m}\leftarrow\psi_{n}} \!=\!
	\left\{ b_{\mu,\gamma}(\phi) \!= \!\begin{bmatrix} \cos(\mu\phi + \gamma)\!\! &\!\! \sin(\mu\phi + \gamma) \end{bmatrix} \bigg|\ \mu \!= \!(\pm m \!+ \!n)\! + \! tN,\ \gamma \in \left\{0, {\pi\over2}\right\}\!\right\}_{t \in \Z} \!\!\!\!\!\!\!
\end{empheq}
~\\[-3ex]
for $n > 0$ and $m \in \left\{0, {N\over2}\right\}$.
See the top right cells in Table~\ref{tab:CN_irrep_solution_appendix}.



 \vspace*{2.ex}

\subsubsection{Derivation for $\bDN$}
\label{apx:derivation_irrep_constraint_DN}

A solution for $\DN$ can easily be derived by repeating the process done for $\O2$ in \apx \ref{apx:derivation_irrep_constraint_O2} but starting from the bases derived for $\CN$ in \apx \ref{apx:derivation_irrep_constraint_CN} instead of those for $\SO2$.

In contrast to the case of $\O2$-equivariant kernels, the choice of reflection axis $\beta$ is not irrelevant since $\DN$ does not act transitively on axes.
More precisely, the action of $\DN$ defines equivalence classes $\beta \cong \beta' \Leftrightarrow \exists\ 0 \leq n < N\ :\ \beta = \beta' + n {2\pi \over N}$ of axes which can be labeled by representatives $\beta \in [0, {2\pi \over N})$.
For the same argument considered in \apx~\ref{apx:derivation_irrep_constraint_Flip} we can without loss of generality consider reflections along the axis $\beta=0$ in our derivations and retrieve kernels $\kappa'$, equivariant to reflections along a general axis $\beta$, as $\kappa'(r, \phi) = \kappa(r, \phi-\beta)$.

\paragraph{2-dimensional irreps:}~\\[.75ex]
For 2-dimensional input and output representations $\rho_\text{in} = \psi_{1,n}^{\DN}$ and $\rho_\text{out} = \psi_{1,m}^{\DN}$, the final basis is
\begin{empheq}[box=\kernelspace]{align}
\label{eq:dn_2x2_basis}
    \mathcal{K}^{\DN}_{\psi_{i,m}\leftarrow\psi_{j,n}}\ =\
    \left\{ b_{\mu,0,s}(\phi) = \psi(\mu\phi) \xi(s) \ \bigg|\ \ \mu = m - sn +tN, s \in \{\pm 1\} \right\}_{t \in \Z}
\end{empheq}
where $i = j = 1$ and $m, n > 0$.
These solutions are written out explicitly in the bottom right of Table~\ref{tab:DN_irrep_solution_appendix}.




~\\[-4.ex]
\paragraph{1-dimensional irreps:}~\\[.75ex]
$\DN$ has 1-dimensional representations $\rho_\text{in}=\psi_{j,n}^{\DN}$ and $\rho_\text{out}=\psi_{i,m}^{\DN}$ for $m,n\in\{0,{N\over2}\}$.
In these cases we find the bases
\begin{empheq}[box=\kernelspace]{align*}
\stepcounter{equation}\tag{\theequation}\label{eq:dn_1x1_basis}
    & \!\! \mathcal{K}^{\DN}_{\psi_{i,m}\leftarrow\psi_{j,n}} \! =
    & \hspace*{-1em} \Bigg\{ b_{\mu,\gamma}(\phi) = \cos(\mu\phi + \gamma) \bigg|\ 
      			& \mu= ( m + n\!\!\mod N)+ tN, \\
&&			&\gamma = (i+ j\!\!\mod 2){\pi\over2},\ \mu \neq 0 \vee \gamma = 0 \Bigg\}_{t \in \N} \!\!\!\!\!
\end{empheq}
which are shown in the top left cells of Table~\ref{tab:DN_irrep_solution_appendix}.

~\\[-4.ex]
\paragraph{1 and 2-dimensional irreps:}~\\[.75ex]
For 1-dimensional input and 2-dimensional output representations, that is, $\rho_\text{in}=\psi_{j,n}^{\DN}$ and $\rho_\text{out}=\psi_{1,m}^{\DN}$ with $i=1$, $m>0$ and $n\in\{0,{N\over 2}\}$, the kernel basis is given by:
\begin{empheq}[box=\kernelspace]{align}
\label{eq:dn_2x1_basis}
    \mathcal{K}^{\DN}_{\psi_{i,m}\leftarrow\psi_{j,n}}\ =\
    \left\{ b_{\mu,\gamma}(\phi) = \begin{bmatrix} \cos(\mu\phi + \gamma)\\ \sin(\mu\phi + \gamma) \end{bmatrix} \ \bigg|\ \ \mu = (m \pm n) + tN,\ \gamma = j {\pi\over2}\right\}_{t \in \Z}
\end{empheq}
See the bottom left of Table~\ref{tab:DN_irrep_solution_appendix}.

~\\[-4.ex]
\paragraph{2 and 1-dimensional irreps:}~\\[.75ex] 
Similarly, for 2-dimensional input and 1-dimensional output representations $\rho_\text{in}=\psi_{1,n}^{\DN}$ and $\rho_\text{out}=\psi_{i,m}^{\DN}$ with $j=1$, $n>0$ and $m\in\{0,{N\over2}\}$, we find:
\begin{empheq}[box=\kernelspace]{align}
\label{eq:dn_1x2_basis}
    \mathcal{K}^{\DN}_{\psi_{i,m}\leftarrow\psi_{j,n}}\! =\!
    \left\{\! b_{\mu,\gamma}(\phi) = \begin{bmatrix} \cos(\mu\phi + \gamma) \!&\! \sin(\mu\phi + \gamma) \end{bmatrix} \bigg|\ \mu = (\pm m + n) + tN,\ \gamma = i {\pi\over2}\!\right\}_{t \in \Z}\!\!\!\!\!\!\!
\end{empheq}
~\\[-3ex]
Table~\ref{tab:DN_irrep_solution_appendix} shows these solutions in its top right cells.

 

\subsubsection{Kernel constraints at the origin}
\label{apx:stabilizer_solution}

Our derivations rely on the fact that the kernel constraints restrict only the angular parts of the unconstrained kernel space $L^2(\R^2)^{c_\text{out}\times c_\text{in}}$ which suggests an independent solution for each radius $r\in\R^+\cup\,\{0\}$.
Particular attention is required for kernels defined at the origin, i.e. when $r=0$.
The reason for this is that we are using polar coordinates $(r,\phi)$ which are ambiguous at the origin where the angle is not defined.
In order to stay consistent with the solutions for $r>0$ we still define the kernel at the origin as an element of $L^2(S^1)^{c_\text{out}\times c_\text{in}}$.
However, since the coordinates $(0,\phi)$ map to the same point for all $\phi\in[0,2\pi)$, we need to demand the kernels to be angularly constant, that is, $\kappa(\phi)=\kappa(0)$.
This additional constraint restricts the angular Fourier bases used in the previous derivations to zero frequencies only.
Apart from this, the kernel constraints are the same for $r=0$ and $r>0$ which implies that the G-steerable kernel bases at $r=0$ are given by restricting the bases derived in
\ref{apx:derivation_irrep_constraint_SO2},
\ref{apx:derivation_irrep_constraint_Flip},
\ref{apx:derivation_irrep_constraint_O2},
\ref{apx:derivation_irrep_constraint_CN} and
\ref{apx:derivation_irrep_constraint_DN}
to the elements indexed by frequencies $\mu=0$.




\subsection{Complex valued representations and Harmonic Networks}
\label{apx:incompleteness_hnets}


Instead of considering real (irreducible) representations we could have derived all results using complex representations, acting on complex feature maps.
For the case of $\O2$ and $\DN$ this would essentially not affect the derivations since their complex and real irreps are equivalent, that is, they coincide up to a change of basis.
Conversely, all complex irreps of $\SO2$ and $\CN$ are 1-dimensional which simplifies the derivations in complex space.
However, the solution spaces of complex G-steerable kernels need to be translated back to a real valued implementation.
This translation has some not immediately obvious pitfalls which can lead to an underparameterized implementation in real space.
In particular, Harmonic Networks~\cite{Worrall2017-HNET} were derived with a complete solution in complex space; however, their real valued implementation is using a $G$-steerable kernel space of half the dimensionality as ours.
We will in the following explain why this is the case.

In the complex field, the irreps of $\SO2$ are given by $\psi^{\Cm}_k(\theta) = e^{ik\theta}\in\Cm$ with frequencies $k\in\Z$.
Notice that these complex irreps are indexed by positive and negative frequencies while their real counterparts, defined in \apx~\ref{apx:irreps}, only involve non-negative frequencies.
As in~\cite{Worrall2017-HNET} we consider complex feature fields $f^\mathbb{C}:\R^2\to\Cm$ which are transforming according to complex irreps of $\SO2$.
A complex input  field $f^\mathbb{C}_\text{in}: \R^2\to\Cm$ of type $\psi^{\mathbb{C}}_n$ is mapped to
a complex output field $f^\mathbb{C}_\text{out}:\R^2\to\Cm$ of type $\psi^{\mathbb{C}}_m$
via the cross-correlation
\begin{align}\label{eq:complex_cross_correlation} 
    f^\mathbb{C}_\text{out} = k^\mathbb{C} \star f^\mathbb{C}_\text{in} \ .
\end{align}
with a complex filter $k^{\mathbb{C}}:\R^2\to\mathbb{C}$.
The (angular part of the) complete space of equivariant kernels between $f^\mathbb{C}_\text{in}$ and $f^\mathbb{C}_\text{out}$ was in~\cite{Worrall2017-HNET} proven to be parameterized by
\[
    k^{\mathbb{C}}(\phi)\ =\ w\; e^{i(m-n)\phi},
\]
where $w\in\Cm$ is a complex weight which scales and phase-shifts the complex exponential.
We want to point out that an equivalent parametrization is given in terms of the real and imaginary parts $w^\text{Re}$ and $w^\text{Im}$ of the weight $w$, i.e.
\begin{align}\label{eq:hnet_basis}
    k^{\mathbb{C}}(\phi)
    \ &=\ w^\text{Re}e^{i(m-n)\phi} + i\;\!w^\text{Im}e^{i (m-n)\phi} \notag \\
    \ &=\ w^\text{Re}e^{i(m-n)\phi} +      w^\text{Im}e^{i((m-n)\phi+\pi/2)}\,.
\end{align}
The real valued implementation of Harmonic Networks models the complex feature fields $f^{\Cm}$ of type $\psi^{\Cm}_k(\theta)$ by splitting them in two real valued channels $f^{\R}:=(f^\text{Re},f^\text{Im})^T$ which contain their real and imaginary part.
The action of the complex irrep $\psi^{\Cm}_k(\theta)$ is modeled accordingly by a rotation matrix of the same, potentially negative\footnote{This establishes an isomorphism between $\psi^{\Cm}_k(\theta)$ and $\psi^{\R}_{|k|}(\theta)$ depending on the sign of $k$.}
frequency.
A real valued implementation of the cross-correlation~\eqref{eq:complex_cross_correlation} is built using a real kernel $k:\R^2\to\R^{2\times2}$ as specified by
\begin{align*}
    \begin{bmatrix}
        f^{Re}_\text{out} \\
        f^{Im}_\text{out}
    \end{bmatrix}
    &= 
    \begin{bmatrix}
        k^{Re} &          -  k^{Im} \\
        k^{Im} & \phantom{-} k^{Re}
    \end{bmatrix} 
    \star
    \begin{bmatrix}
        f^{Re}_\text{in} \\
        f^{Im}_\text{in}
    \end{bmatrix}.
\end{align*}
The complex steerable kernel~\eqref{eq:hnet_basis} is then given by
\begin{align}\label{eq:complex_solutions_implementation}
    k(\phi) &= w^\text{Re} \PSI{(m \shortminus n)\phi}        \!\!\!\!\!\! &&+ w^\text{Im} \PSIP{(m \shortminus n)\phi} &&\nonumber\\
            &= w^\text{Re} \qquad\qquad \psi\lp (m-n)\phi \rp \!\!\!\!\!\! &&+ w^\text{Im} \qquad\qquad \psi \lp (m-n)\phi + {\pi\over2} \rp &&
\end{align}
While this implementation models the \textit{complex} Harmonic Networks faithfully in real space, it does not utilize the complete $\SO2$-steerable kernel space when the real feature fields are interpreted as fields transforming under the real irreps $\psi^{\R}_k$ as done in our work.
More specifically, the kernel space used in~\eqref{eq:complex_solutions_implementation} is only 2-dimensional while our basis~\eqref{eq:so2_2x2_basis} for the same case is 4-dimensional.
The additional solutions with frequency $m+n$ are missing.

The lower dimensionality of the complex solution space can be understood by analyzing the relationship between $\SO2$'s real and complex irreps.
On the complex field, the real irreps become reducible and decomposes into two 1-dimensional complex irreps with opposite frequencies:
\begin{align*}
    \psi^{\R}_k(\theta) &= 
    {1\over\sqrt{2}} 
    \begin{bmatrix} 1 & -i \\ -i & 1\end{bmatrix}
    \begin{bmatrix} e^{ik\theta} & 0 \\ 0 & e^{-ik\theta} \end{bmatrix}
    {1\over\sqrt{2}}
    \begin{bmatrix} 1 &  i \\  i & 1\end{bmatrix}
\end{align*}
Indeed, $\SO2$ has only half as many real irreps as complex ones since positive and negative frequencies are conjugated to each other, i.e. they are equivalent up to a change of basis: $\psi^{\R}_k(\theta) = \xi(-1)\psi^{\R}_{-k}(\theta)\xi(-1)$.
It follows that a real valued implementation of a complex $\psi^{\mathbb{C}}_k$ fields as a 2-dimensional $\psi^{\R}_k$ fields implicitly adds a complex $\psi^{\mathbb{C}}_{-k}$ field.
The intertwiners between two real fields of type $\psi^{\R}_n$ and $\psi^{\R}_m$ therefore do not only include the single complex intertwiner between complex fields of type $\psi^{\Cm}_n$ and $\psi^{\Cm}_m$, but four complex intertwiners between fields of type $\psi^{\Cm}_{\pm n}$ and $\psi^{\Cm}_{\pm m}$.
The real parts of these intertwiners correspond to our four dimensional solution space.

In conclusion,~\cite{Worrall2017-HNET} indeed found the complete solution on the complex field.
However, by implementing the network on the real field, negative frequencies are implicitly added to the feature fields which allows for our larger basis~\eqref{eq:so2_2x2_basis} of steerable kernels to be used without adding an overhead.

  	
\section{Alternative approaches to compute kernel bases and their complexities}
\label{apx:comparison_SE3Nets}

The main challenge of building steerable CNNs is to find the space of solutions of the kernel space constraint in Eq.~\ref{eq:kernel_constraint}.
Several recent works tackle this problem for the very specific case of features which transform under \textit{irreducible} representations of $\SE3\cong(\R^3,+)\rtimes\SO3$.
The strategy followed in~\cite{TensorFieldNets,Kondor2018-NBN,kondorClebschGordanNets2018,anderson2019cormorant} is based on well known analytical solutions and does not generalize to arbitrary representations.
In contrast,~\cite{3d_steerableCNNs} present a numerical algorithm to solve the kernel space constraint.
While this algorithm was only applied to solve the constraints for irreps, it generalizes to arbitrary representations.
However, the computational complexity of the algorithm scales unfavorably in comparison to the approach proposed in this work.
We will in the following review the kernel space solution algorithm of~\cite{3d_steerableCNNs} for general representations and discuss its complexity in comparison to our approach.

The algorithm proposed in~\cite{3d_steerableCNNs} is considering the same kernel space constraint
\[
    k(gx) = \rho_\text{out}(g) k(x) \rho_\text{in}^{-1}(g) \quad\forall g\in G
\]
as in this work.
By vectorizing the kernel the constraint can be brought in the form
\begin{align*}
    \vc{k}(gx) =& \lp \rho_\text{out} \otimes \big(\rho_\text{in}^{-1}\big)^T \rp (g) \vc{k}(x) \\
               =& \lp \rho_\text{out} \otimes      \rho_\text{in}             \rp (g) \vc{k}(x) \,,
\end{align*}
where the second step assumes the input representation to be unitary, that is, to satisfy $\rho_\text{in}^{-1}=\rho_\text{in}^T$.
A Clebsch-Gordan decomposition, i.e. a decomposition of the tensor product representation into a direct sum of irreps $\psi_j$ of $G$, then yields\footnote{For the irreps of $\SO3$ it is well known that $\mathcal{J}=\{|j-l|,\dots,j+l\}$ and $|\mathcal{J}|=2\min(j,l)+1$.}
\[
    \vc{k}(gx) = Q^{-1} \lp \bigoplus\nolimits_{J \in \mathcal{J}} \psi_J \rp\!(g)\, Q \vc{k}(x)
\]
Through a change of variables $\eta(x) := Q \vc{k}\!(x)$ this simplifies to
\[
    \eta(gx) = \lp \bigoplus\nolimits_{J \in \mathcal{J}} \psi_J \rp\! (g) \eta(x)
\]
which, in turn, decomposes into $|\mathcal{J}|$ independent constraints
\[
    \eta_J(gx) = \psi_J (g) \eta_J(x) \ .
\]
Each of these constraints can be solved independently to find a basis for each $\eta_J$.
The kernel basis is then found by inverting the change of basis and the vectorization, i.e. by computing $k(x)=\operatorname{unvec}\!\big(Q^{-1}\eta(x)\big)$.

For the case that $\rho_\text{in}=\psi_j$ and $\rho_\text{out}=\psi_l$ are Wigner D-matrices, i.e. irreps of $\SO3$, the change of basis $Q$ is given by the Clebsch-Gordan coefficients of $\SO3$.
These well known solutions were used in~\cite{TensorFieldNets,Kondor2018-NBN,kondorClebschGordanNets2018,anderson2019cormorant} to build the basis of steerable kernels.
Conversely, the authors of~\cite{3d_steerableCNNs} solve for the change of basis $Q$ numerically.
Given \textit{arbitrary} unitary representations $\rho_\text{in}$ and $\rho_\text{out}$ the numerical algorithm solves for the change of basis in
\begin{align*}
    \big(\rho_\text{in}\otimes\rho_\text{out}\big)(g)\ &=\ Q^{-1}\lp \bigoplus_{J\in\mathcal{J}}\psi_J(g)\rp Q &\forall& g\in G \\
    \Leftrightarrow \mkern140mu
    0\ &=\ Q\big(\rho_\text{in}\otimes\rho_\text{out}\big)(g)\ -\ \lp\bigoplus_{J\in\mathcal{J}}\psi_J(g)\rp Q &\forall& g\in G \,.
\end{align*}
This linear constraint on $Q$, which is a specific instance of the Sylvester equation, can be solved by vectorizing $Q$, i.e.
\[
    \Big[ I\otimes\big(\rho_\text{in}\otimes\rho_\text{out}\big)(g)\ -\ \Big(\!\bigoplus\nolimits_{J\in\mathcal{J}}\psi_J\Big)(g)\otimes I \Big] \vc{Q}\ =\ 0
    \qquad\forall g\in G \,,
\]
where $I$ is the identity matrix on $\R^{\dim(\rho_\text{in}\otimes\rho_\text{out})} = \R^{\dim(\rho_\text{in})\dim(\rho_\text{out})}$~and~${\vc{Q}\in\R^{\dim(\rho_\text{in})^2\dim(\rho_\text{out})^2}\!\!\!.}$
In principle there is one Sylvester equation for each group element $g\in G$, however, it is sufficient to consider the \textit{generators} of $G$ only, since the solutions found for the generators will automatically hold for all group elements.
One can therefore stack the matrices
${\Big[ I\otimes\big(\rho_\text{in}\otimes\rho_\text{out}\big)(g) - \Big(\!\bigoplus\nolimits_{J\in\mathcal{J}}\psi_J\Big)(g)\otimes I \Big]}$
for the generators of $G$ into a bigger matrix and solve for $Q$ as the null space of this stacked matrix.
The linearly independent solutions $Q^J$ in the null space correspond to the Clebsch-Gordan coefficients for $J\in\mathcal{J}$.

This approach does not rely on the analytical Clebsch-Gordan coefficients, which are only known for specific groups and representations, and therefore works for any choice of representations.
However, applying it naively to large representations can be extremely expensive.
Specifically, computing the null space to solve the (stacked) Sylvester equation for $\mathcal{G}$ generators of $G$ via a \textit{SVD}, as done in~\cite{3d_steerableCNNs}, scales as $\mathcal{O}\big(\dim(\rho_\text{in})^6\dim(\rho_\text{out})^6\mathcal{G}\big)$.
This is the case since the matrix which is multiplying $\vc{Q}$ is of shape
$\dim(\rho_\text{in})^2\dim(\rho_\text{out})^2\mathcal{G} \times \dim(\rho_\text{in})^2\dim(\rho_\text{out})^2$.
Moreover, the change of basis matrix $Q$ itself has shape
$\dim(\rho_\text{in})\dim(\rho_\text{out}) \times \dim(\rho_\text{in})\dim(\rho_\text{out})$
which implies that the change of variables\footnote{No inversion from $Q$ to $Q^{-1}$ is necessary if the Sylvester equation is solved directly for $Q^{-1}$.}
from $\eta$ to $k$ has complexity $\mathcal{O}\big(\dim(\rho_\text{in})^2\dim(\rho_\text{out})^2\big)$.
In~\cite{3d_steerableCNNs} the authors only use irreducible representations which are relatively small such that the bad complexity of the algorithm is negligible.

In comparison, the algorithm proposed in this work is based on an \textit{individual} decomposition of the representations $\rho_\text{in}$ and $\rho_\text{out}$ into irreps and leverages the analytically derived kernel space solutions between the irreps of $G\leq\O2$.
The independent decomposition of the input and output representations leads to a complexity of only
$\mathcal{O}\big(\big(\dim(\rho_\text{in})^6+\dim(\rho_\text{in})^6\big)\mathcal{G}\big)$.
We further apply the input and output changes of basis $Q_\text{in}$ and $Q_\text{out}$ independently to the irreps kernel solutions $\kappa^{ij}$ which leads to a complexity of 
$\mathcal{O}\big(\dim(\rho_\text{in})\dim(\rho_\text{out})^2 + \dim(\rho_\text{out})\dim(\rho_\text{in})^2\big)$.
The improved complexity of our implementation makes working with large representations as used in this work, for instance $\dim(\rho_\text{reg}^{\D{20}})=40$, possible.

 	
\section{Additional information on the training setup}
\label{apx:training_setup}

\begin{figure}[H]
    \begin{minipage}{\linewidth}
        \centering
        \begin{minipage}{0.45\linewidth}
            \centering
            \begin{table}[H]
                \centering
                \scalebox{.85}{
                    \begin{tabular}{lr}
                        \toprule
                        layer                           & output fields \\
                        \midrule
                        conv block  $7\times7$ (pad 1)  &            16 \\
                        conv block  $5\times5$ (pad 2)  &            24 \\
                        max pooling $2\times2$          &            24 \\
                        conv block  $5\times5$ (pad 2)  &            32 \\
                        conv block  $5\times5$ (pad 2)  &            32 \\
                        max pooling $2\times2$          &            32 \\
                        conv block  $5\times5$ (pad 2)  &            48 \\
                        conv block  $5\times5$          &            64 \\
                        invariant projection            &            64 \\
                        global average pooling          &            64 \\
                        fully connected                 &            64 \\
                        fully connected + softmax       &            10 \\
                        \bottomrule
                    \end{tabular}
                    
                }
                \vspace*{2pt}
                ~\\~
                \caption{
                    Basic model architecture from which all models for the MNIST benchmarks in Tables~\ref{tab:mnist_comparison} and \ref{tab:mnist_restriction} are being derived.
                    Each convolution block includes a convolution layer, batch-normalization and a nonlinearity.
                    The first fully connected layer is followed by batch-normalization and ELU.
                    The width of each layer is expressed as the number of fields of a regular $\C{16}$ model with approximately the same number of parameters.
                }
                \label{tab:small_architecture}
\end{table} 
        \end{minipage}
        \hfill
        \hspace{0.05\linewidth}
        \begin{minipage}{0.45\linewidth}
            \begin{table}[H]
            \centering
                \scalebox{.85}{
                    \begin{tabular}{lr}
                        \toprule
                        layer                           &  output fields\\
                        \midrule
                        conv block  $9\times9$          &            24 \\
                        conv block  $7\times7$ (pad 3)  &            32 \\
                        max pooling $2\times2$          &            32 \\
                        conv block  $7\times7$ (pad 3)  &            36 \\
                        conv block  $7\times7$ (pad 3)  &            36 \\
                        max pooling $2\times2$          &            36 \\
                        conv block  $7\times7$ (pad 3)  &            64 \\
                        conv block  $5\times5$          &            96 \\
                        invariant projection            &            96 \\
                        global average pooling          &            96 \\
                        fully connected                 &            96 \\
                        fully connected                 &            96 \\
                        fully connected + softmax       &            10 \\
                        \bottomrule
                    \end{tabular}
                    
                }
                \vspace*{4pt}
                \caption{
                    Model architecture for the final MNIST-rot experiments (replicated from \cite{Weiler2018-STEERABLE}).
                    Each fully connected layer follows a dropout layer with $p=0.3$; the first two fully connected layers are followed by batch normalization and ELU.
                    The width of each layer is expressed in terms of regular feature fields of a $\C{16}$ model.
                }
                \vspace*{10pt}
                \label{tab:large_architecture}
            \end{table}  
        \end{minipage}
    \end{minipage}
    \vspace*{-12pt}
\end{figure}


\subsection{Benchmarking on transformed MNIST datasets}
\label{apx:mnist_benchmark_training}

Each model reported in Sections~\ref{sec:mnist_benchmark}, \ref{sec:mnist_restriction} and~\ref{sec:mnist_rot_convergence} is derived from the architecture reported in Table~\ref{tab:small_architecture}.
The width of each model's layers is thereby scaled such that the total number of parameters is matched and the relative width of layers coincides with that reported in Table\ref{tab:small_architecture}.
Training is performed with a batch size of 64 samples, using the \emph{Adam} optimizer.
The learning rate is initialized to $10^{-3}$ and decayed exponentially by a factor of $0.8$ per epoch, starting after a burn in phase of 10 epochs.
We train each model for 30 epochs and test the model which performed best on the validation set.
A weight decay of $10^{-7}$ is being used for all convolutional layers and the first fully connected layer.
In all experiments, we build steerable bases with Gaussian radial profiles of width $\sigma=0.6$ for all except the outermost ring where we use $\sigma = 0.4$.
We apply a strong bandlimiting policy which permits frequencies up to $0, 2, 2$ for radii $0, 1, 2$ in a $5\times5$ kernel and up to $0, 2, 3, 2$ for radii $0, 1, 2, 3$ in a $7\times7$ kernel.
The strong cutoff in the rings of maximal radius is motivated by our empirical observation that these rings introduce a relatively high equivariance error for higher frequencies.
This is the case since the outermost ring ranges out of the sampled kernel support.
During training, data augmentation with continuous rotations and reflections is performed (if these are present in the dataset) to not disadvantage non-equivariant models.
In the models using group restriction, the restriction operation is applied after the convolution layers but before batch normalization and non-linearities.




\subsection{Competitive runs on MNIST rot}
\label{apx:mnist_final}

In Table \ref{tab:mnist_final} we report the performances of some of our best models.
Our experiments are based on the best performing, $\C{16}$-equivariant model of \cite{Weiler2018-STEERABLE} which defined the state of the art on rotated MNIST at the time of writing.
We replicate their model architecture, summarized in Table~\ref{tab:large_architecture}, though our models have a different frequency bandlimit and width $\sigma$ for the Gaussian radial profiles as discussed in the previous subsection.
As before, the table reports the width of each layer in terms of number of fields in the $\C{16}$ regular model.

As commonly done, we train our final models on the 10000 + 2000 training and validation samples.
Training is performed for 40 epochs with an initial learning rate $0.015$, which is being decayed by a factor of $0.8$, starting after $15$ epochs.
As before, we use the \textit{Adam} optimizer with a batch size of 64, this time using L1 and L2 regularization with a weight of $10^{-7}$.
The fully connected layers are additionally regularized using dropout with a probability of $p=0.3$.
We are again using train time augmentation.





\subsection{CIFAR experiments}
\label{apx:cifar}

The equivariant models used in the experiments on CIFAR-10 and CIFAR-100 are adapted from the original WideResNet models by replacing conventional with $G$-steerable convolutions and scaling the number of feature fields such that the total number of parameters is preserved.
For blocks which are equivariant under $\D8$ or $\C8$ we use $5\times5$ kernels instead of $3\times3$ kernels to allow for higher frequencies. 
All models use regular feature fields in all but the final convolution layer, which maps to a scalar field (conv2triv) to produce invariant predictions.
We use a frequency cut-off of $3$ times the ring's radius, e.g. $0, 3, 6$ for rings of radii $0, 1, 2$.
These higher bandlimits in comparison to the MNIST experiments are motivated by the fact that the corresponding bases introduce small discretization errors, which is no problem for the classification of natural images.
In the contrary, this leads to the models having a strong bias towards being equivariant, but might allow them to break equivariance if necessary.
The widths of the bases' rings is chosen to be $\sigma = 0.45$ in all rings.

The training process is the same as used for WideResNets: we train for 200 epochs with a batch size of 128. 
We optimize the model with SGD, using an initial learning rate of $0.1$, momentum $0.9$ and a weight decay of $5\cdot10^{-4}$.
The learning rate is decayed by a factor of $0.2$ every 60 epochs.
We perform a standard data augmentation with random crops, horizontal flips and normalization.
No CutOut is done during the normal experiments but it is used in the AutoAugment policies.



\subsection{STL-10 experiments}
\label{apx:stl10}

The models for our STL-10 experiments are adapted from~\cite{cutout}.
However, according to an issue\footnote{\url{https://github.com/uoguelph-mlrg/Cutout/issues/2}}
in the authors' GitHub repository, the publication states some model parameters and the training setup wrongly.
Our adaptations are therefore based on the setting reported on~GitHub.
Specifically, we use patches of $60\times60$ pixels for cutout and the stride of the first convolution layer in the first block is 2 instead of 1.
Moreover, we normalize input features using CIFAR-10 statistics.
Though these statistics are very close to the statistics of STL-10, they might, as the authors of~\cite{cutout} suggest, cause non-negligible changes in performance because of the small training set size of STL-10.

As before, regular feature fields are used throughout the whole model except for the last convolution layer which maps to trivial fields.
In the small model, which does not preserve the number of parameters but the number of channels, we still scale up the number of output channels of the very first convolution layer (before the first residual block).
As the first convolution layer originally has $16$ output channels and our model is initially equivariant to $\D8$ (whose regular representation spans $16$ channels), the first convolution layer would only be able to learn 1 single independent filter (repeated $16$ times, rotated and reflected).
Hence, we increase the number of output channels of the first convolution layer by the square root of the group size ($\sqrt{16} = 4$) leading to $4 \cdot 16 = 64$ channels, i.e. $64/16 = 4$ regular fields.
We use a ring width of $\sigma=0.6$ for the kernel basis except for the outermost ring where we use $\sigma = 0.4$ and use a frequency cut-off factor of $3$ for the rings' radii, i.e. cutoffs of $0,3,6,\dots\;$.

We are again exactly replicating the training process as reported in the publication \cite{cutout}.
Only the labeled subset of the training set is used, that is, the $100000$ unlabeled training images are discarded.
Training is performed for 1000 epochs with a batch size of 128, using SGD with Nesterov momentum of 0.9 and weight decay of $5\cdot10^{-4}$.
The learning rate is initialized to $0.1$ and decayed by a factor of $5$ at $300$, $400$, $600$ and $800$ epochs.
During training, we perform data augmentation by zero-padding with $12$ pixels and randomly cropping patches of $96\times96$ pixels, mirroring them horizontally and applying CutOut.

In the data ablation study, reported in Figure~\ref{fig:stl10_ablation}, we use the same models and training procedure as in the main experiment on the full STL-10 dataset.
For every single run, we generate new datasets by mixing the original training, validation and test set and sample reduced datasets such that all classes are balanced.
The results are averaged over 4 runs on each of the considered training set sizes of 250, 500, 1000, 2000 or 4000.
The validation and test sets contain 1000 and 8000 images, which are resampled in each run as well.


 	
\section{Additional information on the irrep models}
\label{apx:irrep_models}


\paragraph{SO(2) models}
We experiment with some variants (rows~37-44) of the Harmonic Network model in row~30 of Table~\ref{tab:mnist_comparison}, varying in either the non-linearity or the invariant map applied.
All of these models are therefore to be analyzed relative to this baseline.
First, we try to use \emph{squashing} nonlinearities~\cite{Hinton2018-EMCAPS} (row~37) instead of norm-ReLUs on each non-trivial irrep.
This variant performs consistently worse than the original model.
In the baseline variant, we generate invariant features via a convolution to scalar fields in the last layer (\emph{conv2triv}).
This, however, reduces the utilization of high frequency irrep fields in the penultimate layer.
The reason for this is that the kernel space for mappings from high frequency- to scalar fields consists of kernels of a high angular frequency, which will be cut off by our bandlimiting.
To overcome this problem, we propose to instead compute the norms of all non-trivial fields to produce invariant features.
This enables us to use all irreps in the output of the last convolutional layer.
However, we find that combining invariant norm mappings with norm-ReLUs does not improve on the baseline model, see row~38.
In row~39 we consider a variant which applies norm-ReLUs on the direct sum of multiple non-trivial irrep fields, each with multiplicity 1, together (\textit{shared norm-ReLU}), while the scalar fields are still being acted on by ELUs.
This is legitimate since the direct sum of unitary representations is itself unitary.
After the last convolutional layer, the invariant projection preserves the trivial fields but computes the norm of each composed field.
This model significantly outperforms all previous variants on all datasets.
The model in row~40 additionally merges the scalar fields to such combined fields instead of treating them independently.
This architecture performs significantly worse than the previous variants.

We further explore four different variations which are applying \textit{gated nonlinearities} (rows~41-44).
These models distinguish from each other by 1) their mapping to invariant features and 2) whether the gate is being applied to each non-trivial field independently or being shared between multiple non-trivial fields.
We find that the second choice, i.e. sharing gates, does not significantly affect the performances (row 41 vs. 42 and 43 vs. 44).
However, mapping to invariant features by taking the norm of all non-trivial fields performs consistently better than applying \textit{conv2triv}.
Overall, gated nonlinearities perform significantly better than any other choice of nonlinearity on the tested $\SO2$ irrep models.



\paragraph{O(2) models}

Here we will give more details on the $\O2$-specific operations which we introduce to improve the performance of the $\O2$-equivariant models, reported in rows 45-57 of Table~\ref{tab:mnist_comparison}.

\begin{itemize}[leftmargin=7ex]
\setlength{\itemindent}{-4ex}
    \item O(2)-\textit{conv2triv:}
        As invariant map of the $\O2$ irrep models in rows 46-49 and 54 we are designing a last convolution layer which is mapping to an output representation $\rho_\text{out}=\psi_{0,0}^{\O2}\oplus\psi_{1,0}^{\O2}$, that is, to scalar fields $f_{0,0}$ and sign-flip fields $f_{1,0}$ in equal proportions.
        Since the latter are not invariant under reflections, we are in addition taking their absolute value.
        The resulting, invariant output features are then multiple fields $f_{0,0}\oplus |f_{1,0}|$.
        The motivation for not convolving to trivial representations of $\O2$ directly via \textit{conv2triv} is that the steerable kernel space for mappings between irreps of $\O2$ does not allow for mapping between $\psi_{0,0}^{\O2}$ and $\psi_{1,0}^{\O2}$ (see Table~\ref{tab:O2_irrep_solution_appendix}), which would lead to dead neurons.
\end{itemize}
The models in rows 50-53, 56 and 57 operate on $\Ind{\SO2}{\O2}\psi_k^{\SO2}$-fields whose representations are induced from the irreps of $\SO2$.
Per definition, this representation acts on feature vectors $f$ in $\R^{\text{dim}(\psi_k^{\SO2})}\otimes\R^{|\O2:\SO2|}$, which we treat in the following as functions ${f:\O2/\SO2\to\R^{\text{dim}(\psi_k^{\SO2})}}$.
We further identify the coset $s\SO2$ in the quotient space $\O2/\SO2$ by its representative $\mathcal{R}(s\SO2):=s\in\Flip$ in the reflection group.
Eq.~\ref{eq:induced_field}~defines the action of the induced representation on a feature vector by
\begin{align*}
    \Big(\!\big[\Ind{\SO2}{\O2}\psi_k^{\SO2}\big](\tilde{r}\tilde{s}) \,f \Big)(s\SO2)
    \ :=&\ 
    \psi_k^{\SO2}\big(\operatorname{h}\big(\tilde{r}\tilde{s}\mathcal{R}((\tilde{r}\tilde{s})^{-1}s\SO2)\big)\big) \,f\big((\tilde{r}\tilde{s})^{-1}s\SO2\big) \\
    \  =&\ 
    \psi_k^{\SO2}\big(\operatorname{h}(\tilde{r}s)\big) \,f\big(\tilde{s}s\SO2\big) \\[2ex]
    \  =&\ 
    \begin{cases}
        \quad
        \psi_k^{\SO2}\big(\tilde{r})      \,f\big(\tilde{s}s\SO2\big) \quad& \text{for }\ s=+1 \\[1ex]
        \psi_k^{\SO2}\big(\tilde{r}^{-1}) \,f\big(\tilde{s}s\SO2\big) \quad& \text{for }\ s=-1 \ ,
    \end{cases}
\end{align*}
where we used Eq.~\ref{eq:g_decomposition_h-fct} to compute
\[
    \operatorname{h}(\tilde{r}s)
    \ :=\ \mathcal{R}\big(\tilde{r}s\SO2\big)^{-1}\tilde{r}s
    \  =\ s^{-1}\tilde{r}s
    \  =\
    \begin{cases}
        \tilde{r}       \quad&\text{for }\ s=+1 \\
        \tilde{r}^{-1}  \quad&\text{for }\ s=-1 \ .
    \end{cases}
\]
Intuitively, this action describes a permutation of the subfields (indexed by $s$) via the reflection $\tilde{s}$ and a rotation of the subfields by $\tilde{r}$ and $\tilde{r}^{-1}$, respectively.
Specifically, for $k=0$, the induced representation is for all $\tilde{r}$ instantiated by
\begin{align}
\label{eq:ind_so2_o2_trivial}
    \big[\Ind{\SO2}{\O2}\psi_0^{\SO2}\big](\tilde{r}\tilde{s})
    \ =\ 
    \begin{cases}
        \begin{bmatrix}
            1 & 0 \\
            0 & 1
        \end{bmatrix}
        \qquad&\text{for }\ \tilde{s}=+1 \\[2ex]
        \begin{bmatrix}
            0 & 1 \\
            1 & 0
        \end{bmatrix}
        \qquad&\text{for }\ \tilde{s}=-1 \ \ ,
    \end{cases}
\end{align}
that is, it coincides with the regular representation of the reflection group.
Similarly, for $k>0$, it is for all $\tilde{r}$ given by the $4\times4$ matrices
\begin{align*}
    \big[\Ind{\SO2}{\O2}\psi_{k>0}^{\SO2}\big](\tilde{r}\tilde{s})
    \ =\ 
    \begin{cases}
        \left[
        \begin{array}{c|c}
            \\[-1.75ex]
            \!\!\psi_{k>0}^{\SO2}(\tilde{r}) \!\! & 0
            \\[1.3ex]
            \hline\\[-1.2ex]
            0 & \!\psi_{k>0}^{\SO2}(-\tilde{r})\!\!\!
            \\[1.3ex]
        \end{array}
        \right]
        \qquad&\text{for }\ \tilde{s}=+1 \\[6ex]
        \left[
        \begin{array}{c|c}
            \\[-1.75ex]
            0 & \!\psi_{k>0}^{\SO2}(\tilde{r})\!\!\!
            \\[1.3ex]
            \hline\\[-1.2ex]
            \!\!\psi_{k>0}^{\SO2}(-\tilde{r})\!\! & 0
            \\[1.3ex]
        \end{array}
        \right]
        \qquad&\text{for }\ \tilde{s}=-1 \ \ .
    \end{cases}
\end{align*}
We adapt the conv2triv and norm invariant maps, as well as the norm-ReLU and the gated nonlinearities to operate on $\Ind{\SO2}{\O2}$-fields as follows:
\begin{itemize}[leftmargin=7ex]
\setlength{\itemindent}{-4ex}
    \item \textit{$\Ind{}{}$-conv2triv:}
        Instead of applying \textit{O(2)-conv2triv} to compute invariant features, we apply convolutions to $\Ind{\SO2}{\O2}\psi_0^{\SO2}$-fields which are invariant under rotations but behave like regular $\Flip$-fields under reflections.
        These fields are subsequently mapped to a scalar field via $G$-pooling, i.e. by taking the maximal response over the two subfields.
    \item \textit{$\Ind{}{}$-norm:}
        An alternative invariant map is defined by computing the norms of the subfields of each final $\Ind{\SO2}{\O2}\psi_k^{\SO2}$-field and applying $G$-pooling over the result.
    \item \textit{$\Ind{}{}$ norm-ReLU:}
        It would be possible to apply a norm-ReLU to a $\Ind{\SO2}{\O2}\psi_k^{\SO2}$-field for $k>0$ as a whole, that is, to compute the norm of both subfields together.
        Instead, we apply two individual norm-ReLUs to the subfields.
        Since the fields permute under reflections, we need to choose the bias parameter of the two norm-ReLUs to be equal.
    \item \textit{$\Ind{}{}$ gate:}
        Similarly, we could apply a single gate to each $\Ind{\SO2}{\O2}\psi_k^{\SO2}$-field.
        However, we apply an individual gate to each subfield.
        In this case it is necessary that the gates permute together with the $\Ind{\SO2}{\O2}\psi_k^{\SO2}$-fields to ensure equivariance.
        This is achieved by computing the gates from $\Ind{\SO2}{\O2}\psi_0^{\SO2}$-fields, which contain two permuting scalar fields.
\end{itemize}

Empirically we find that $\Ind{\SO2}{\O2}$ models perform much better than pure irrep models, despite both of them being equivalent up to a change of basis.
In particular, the induced representations decompose for some change of basis matrices $Q_0$ and $Q_{>0}$ into:
\begin{align*}
    \Ind{\SO2}{\O2}\psi^{\SO2}_0     \,\ &=&\mkern-145mu Q_0    \,\Big[ \psi^{\O2}_{0,0}  \ \oplus&\ \psi^{\O2}_{1,0}   \Big]\, Q_0^{-1}    \\
    \Ind{\SO2}{\O2}\psi^{\SO2}_{k>0} \,\ &=&\mkern-145mu Q_{>0} \,\Big[ \psi^{\O2}_{1,k>0}\ \oplus&\ \psi^{\O2}_{1,k>0} \Big]\, Q_{>0}^{-1}
\end{align*}
The difference between both bases is that the induced representations disentangle the action of reflections into a permutation, while the direct sum of irreps is modeling reflections in each of its sub-vectorfields independently as an inversion of the vector direction and rotation orientation.
Note the analogy to the better performance of regular representations in comparison to a direct sum of the respective irreps.


 
	\newpage
\bibliography{neurips_2019.bbl}

\end{document}
