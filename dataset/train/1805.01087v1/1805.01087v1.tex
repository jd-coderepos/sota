

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2018}
\usepackage{times}
\usepackage{latexsym}
\usepackage{enumitem}

\usepackage{url}

\aclfinalcopy \def\aclpaperid{1249} 



\newcommand\BibTeX{B{\sc ib}\TeX}

\usepackage{yfonts}
\usepackage{graphicx} \usepackage{subcaption}

\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\newcommand\mycommfont[1]{\footnotesize\ttfamily\textcolor{gray}{#1}}
\SetCommentSty{mycommfont}


\usepackage{stackengine}  \usepackage{tikz}

\usepackage{verbatim}
\usepackage{etoolbox}
\usepackage{framed}
\usepackage{xcolor}
\usepackage{color}
\usepackage{amsmath}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}

\newcommand{\xm}[1]{\textcolor{blue}{\bf\small [#1 --XM]}}
\newcommand{\gn}[1]{\textcolor{magenta}{\bf\small [#1 --GN]}}
\newcommand{\np}[1]{\textcolor{purple}{\bf\small [#1 --NP]}}

\usepackage{multirow}
\usepackage{arydshln}

\title{Stack-Pointer Networks for Dependency Parsing}

\author{
Xuezhe Ma \\ Carnegie Mellon University \\ {\tt xuezhem@cs.cmu.edu}
\And Zecong Hu\Thanks{Work done while at Carnegie Mellon University.} \\ Tsinghua University \\ {\tt huzecong@gmail.com}
\And Jingzhou Liu \\ Carnegie Mellon University \\ {\tt liujingzhou@cs.cmu.edu}
\AND 
Nanyun Peng \\ University of Southern California \\ {\tt npeng@isi.edu} 
\And Graham Neubig \and Eduard Hovy \\ Carnegie Mellon University \\ {\tt \{gneubig, ehovy\}@cs.cmu.edu}
}

\date{}

\usepackage{etoolbox}
\makeatletter
\patchcmd\@combinedblfloats{\box\@outputbox}{\unvbox\@outputbox}{}{\errmessage{\noexpand\@combinedblfloats could not be patched}}\makeatother

\begin{document}

\maketitle

\begin{abstract}
We introduce a novel architecture for dependency parsing: \emph{stack-pointer networks} (\textbf{\textsc{StackPtr}}). 
Combining pointer networks~\citep{vinyals2015pointer} with an internal stack, the proposed model first reads and encodes the whole sentence, then builds the dependency tree top-down (from root-to-leaf) in a depth-first fashion. 
The stack tracks the status of the depth-first search and the pointer networks select one child for the word at the top of the stack at each step. 
The \textsc{StackPtr} parser benefits from the information of the whole sentence and all previously derived subtree structures, and removes the left-to-right restriction in classical transition-based parsers.
Yet, the number of steps for building any (including non-projective) parse tree is linear in the length of the sentence just as other transition-based parsers, yielding an efficient decoding algorithm with  time complexity. 
We evaluate our model on 29 treebanks spanning 20 languages and different dependency annotation schemas, and achieve state-of-the-art performance on 21 of them.
\end{abstract}

\section{Introduction}
\label{sec:intro}
Dependency parsing, which predicts the existence and type of linguistic dependency relations between words, is a first step towards deep language understanding. Its importance is widely recognized in the natural language processing (NLP) community, with it benefiting a wide range of NLP applications, such as coreference resolution~\citep{ng:2010:ACL,durrett-klein:2013:EMNLP,ma-hovy:2016:NAACL}, sentiment analysis~\citep{tai-socher-manning:2015:ACL-IJCNLP}, machine translation~\citep{bastings-EtAl:2017:EMNLP2017}, information extraction~\citep{nguyen:2009:EMNLP,angeli:2015:ACL,peng2017cross}, 
word sense disambiguation~\citep{fauceglia-EtAl:2015:EVENTS}, and low-resource languages processing~\citep{mcdonald-EtAl:2013:Short,ma-xia:2014:P14-1}.
There are two dominant approaches to dependency parsing~\citep{Buchholz:2006,nivre:CoNLL2007}: local and greedy \emph{transition-based} algorithms~\citep{yamada2003statistical,Nivre:2004,zhang-nivre:2011,chen-manning:EMNLP2014},  and the globally optimized \emph{graph-based} algorithms~\citep{eisner1996three,McDonald:2005,McDonald:2005b,Koo:2010}.

Transition-based dependency parsers read words sequentially (commonly from left-to-right) and build dependency trees incrementally by making series of multiple choice  decisions. The advantage of this formalism is that the number of operations required to build any projective parse tree is linear with respect to the length of the sentence. 
The challenge, however, is that the decision made at each step is based on local information, leading to error propagation and worse performance compared to graph-based parsers on root and long dependencies~\citep{mcdonald2011analyzing}.
Previous studies have explored solutions to address this challenge.
Stack LSTMs~\citep{dyer-EtAl:2015:ACL-IJCNLP,ballesteros-dyer-smith:2015:EMNLP,ballesteros-EtAl:2016:EMNLP2016} are capable of learning representations of the parser state that are sensitive to the complete contents of the parser's state. 
\citet{andor-EtAl:2016:P16-1} proposed a globally normalized transition model to replace the locally normalized classifier.
However, the parsing accuracy is still behind state-of-the-art graph-based parsers~\citep{dozat2017:ICLR}. 

Graph-based dependency parsers, on the other hand, learn scoring functions for parse trees and perform exhaustive search over all possible trees for a sentence to find the globally highest scoring tree. 
Incorporating this global search algorithm with distributed representations learned from neural networks, neural graph-based parsers~\citep{TACL885,wang-chang:2016:P16-1,kuncoro-EtAl:2016:EMNLP2016,dozat2017:ICLR} have achieved the state-of-the-art accuracies on a number of treebanks in different languages.
Nevertheless, these models, while accurate, are usually slow (e.g. decoding is  time complexity for first-order models~\cite{McDonald:2005,McDonald:2005b} and higher polynomials for higher-order models~\citep{mcdonald2006online,Koo:2010,ma2012probabilistic,ma-zhao:2012:POSTERS}).

In this paper, we propose a novel neural network architecture for dependency parsing, \emph{stack-pointer networks} (\textbf{\textsc{StackPtr}}).
\textsc{StackPtr} is a transition-based architecture, with the corresponding asymptotic efficiency, but still maintains a global view of the sentence that proves essential for achieving competitive accuracy.
Our \textsc{StackPtr} parser has a pointer network~\citep{vinyals2015pointer} as its backbone, and is equipped with an internal stack to maintain the order of head words in tree structures. 
The \textsc{StackPtr} parser performs parsing in an incremental, top-down, depth-first fashion; at each step, it generates an arc by assigning a child for the head word at the top of the internal stack. 
This architecture makes it possible to capture information from the whole sentence and all the previously derived subtrees, while maintaining a number of parsing steps linear in the sentence length.

We evaluate our parser on 29 treebanks across 20 languages and different dependency annotation schemas, and achieve state-of-the-art performance on 21 of them. 
The contributions of this work are summarized as follows: 
\begin{enumerate}[topsep=0pt,itemsep=-1.5ex,label=(\roman*)]
\item We propose a neural network architecture for dependency parsing that is simple, effective, and efficient. 
\item Empirical evaluations on benchmark datasets over 20 languages show that our method achieves state-of-the-art performance on 21 different treebanks\footnote{Source code is publicly available at \url{https://github.com/XuezheMax/NeuroNLP2}}.
\item Comprehensive error analysis is conducted to compare the proposed method to a strong graph-based baseline using biaffine attention~\citep{dozat2017:ICLR}.
\end{enumerate}

\section{Background}
\label{sec:model}
We first briefly describe the task of dependency parsing, setup the notation, and review Pointer Networks~\citep{vinyals2015pointer}.

\begin{figure*}
\centering
\stackinset{l}{}{b}{}{
	\begin{subfigure}{0.3\linewidth}
    	\tikz{\node[draw,dashed]{
        	\includegraphics[width=\linewidth]{tree.pdf}
        }}
		\renewcommand{\thesubfigure}{a} \caption{}
	\end{subfigure}}{
	\begin{subfigure}{\linewidth}
		\includegraphics[width=\linewidth]{stack_ptr.pdf}
		\renewcommand{\thesubfigure}{b}
		\caption{\label{fig:model}}
	\end{subfigure}
}
\caption{Neural architecture for the \textsc{StackPtr} network, together with the decoding procedure of an example sentence. 
The BiRNN of the encoder is elided for brevity. 
For the inputs of decoder at each time step, vectors in red and blue boxes indicate the sibling and grandparent.}\label{fig:architecture}
\end{figure*}

\subsection{Dependency Parsing and Notations}
Dependency trees represent syntactic relationships between words in the sentences through labeled directed edges between head words 
and their dependents. 
Figure~\ref{fig:architecture} (a) shows a dependency tree for the sentence, ``But there were no buyers''.

In this paper, we will use the following notation: 

\textbf{Input}:  represents 
a generic sentence, where  is the th word.

\textbf{Output}:  represents a generic (possibly non-projective) dependency tree, where each path , w_{i,1}, w_{i,2}, \cdots, w_{i,l_i}'' is an universal virtual root that is added to each tree.

\textbf{Stack}:  denotes a stack configuration, which is a sequence of words. We use  to represent a stack configuration that pushes word  into the stack .

\textbf{Children}:  denotes the list of all the children (modifiers) of word .

\subsection{Pointer Networks}\label{subsec:ptr}
Pointer Networks (\textsc{Ptr-Net}) \citep{vinyals2015pointer} are a variety of neural network capable of learning the conditional probability of an output sequence with elements that are discrete tokens corresponding to positions in an input sequence.
This model cannot be trivially expressed by standard sequence-to-sequence networks~\citep{sutskever2014sequence} due to the variable number of input positions in each sentence. 
\textsc{Ptr-Net} solves the problem by using attention~\citep{bahdanau2015,luong-pham-manning:2015:EMNLP} as a pointer to select a member of the input sequence as the output.

Formally, the words of the sentence  are fed one-by-one into the encoder (a multiple-layer bi-directional RNN), producing a sequence of \emph{encoder hidden states }. 
At each time step , the decoder (a uni-directional RNN) receives the input from last step and outputs \emph{decoder hidden state }. 
The \emph{attention vector } is calculated as follows: 

where  is the \emph{attention scoring function}, which has several variations such as dot-product, concatenation, and biaffine~\citep{luong-pham-manning:2015:EMNLP}. 
\textsc{Ptr-Net} regards the attention vector  as a probability distribution over the source words, i.e. it uses  as pointers to select the input elements.

\section{Stack-Pointer Networks}
\label{subsec:stackptr}


\subsection{Overview}
Similarly to \textsc{Ptr-Net}, \textsc{StackPtr} first reads the whole sentence and encodes each word into the encoder hidden state . 
The internal stack  is always initialized with the root symbol \t\sigmaw_pph_ta^tca^t(w_h, w_c)w_cw_hw_c\sigma \rightarrow \sigma|w_cw_hc = hw_hw_h\sigma\mathrm{ch}(w_i)s_is_i\sigmah_i\beta_tth, g, s\beta_t\mathbf{W}, \mathbf{U}, \mathbf{V}\mathbf{b}s_ih_ih_ts_ih_ts_iP_{\theta}(\mathbf{y}|\mathbf{x})\thetap_{<i}c_{i,j}jp_ic_{i,<j}p_iP_{\theta}(c_{i,j}|c_{i,<j}, p_{<i},\mathbf{x}) = a^ta^tnn2n-1na^tO(n)O(n^2)O(n^3)\beta_1 = \beta_2 = 0.9\eta_0 = 0.001\eta\rho = 0.755.0110101\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm=> 97\%\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm(\beta_1, \beta_2)\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm$0.36 \\
\hline
\end{tabular}
}
\caption{Parsing performance of different variations of our model on both the development and test sets for three languages, together with the \textsc{BiAF} parser as the baseline. Best results are highlighted with bold print.}
\label{tab:main:results}
\end{table*}

\end{document}
