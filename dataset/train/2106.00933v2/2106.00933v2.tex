

\pdfoutput=1
\documentclass[11pt,a4paper]{article}
\usepackage{authblk}
\usepackage[hyperref]{acl2021}
\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{fbox}
\usepackage{longfbox}
\usepackage{soul}
\usepackage{booktabs}
\usepackage{xstring}
\usepackage{fontawesome}
\usepackage{tikz-dependency}
\usepackage{varwidth}
\usepackage{gb4e}
\usepackage{tabularx}

\noautomath


\usepackage{microtype}

\aclfinalcopy \def\aclpaperid{2633} 



\newcommand\BibTeX{B\textsc{ib}\TeX}
\newcommand{\cfbox}[2]{\colorlet{currentcolor}{.}{\color{#1}\fbox{\color{currentcolor}#2}}}
\newcommand*{\email}[1]{\small\href{mailto:#1}{#1}\par
    }

\title{OntoGUM: Evaluating Contextualized SOTA Coreference Resolution on 12 More Genres}

\author[1]{\textbf{Yilun Zhu}}
\author[2,3]{\textbf{Sameer Pradhan}}
\author[1]{\textbf{Amir Zeldes}}
\affil[1]{Department of Linguistics, Georgetown University}
\affil[2]{Linguistic Data Consortium, University of Pennsylvania}
\affil[3]{cemantix.org}
\affil[ ]{\tt \email{yz565@georgetown.edu, pradhan@cemantix.org, Amir.Zeldes@georgetown.edu}}


\date{}

\begin{document}
\maketitle

\begin{abstract}
SOTA coreference resolution produces increasingly impressive scores on the OntoNotes benchmark. However lack of comparable data following the same scheme for more genres makes it difficult to evaluate generalizability to open domain data. 
This paper provides a dataset and comprehensive evaluation showing that the latest neural LM based end-to-end systems degrade very substantially out of domain.
We make an OntoNotes-like coreference dataset called OntoGUM publicly available, converted from GUM, an English corpus covering 12 genres, using deterministic rules, which we evaluate. Thanks to the rich syntactic and discourse annotations in GUM, we are able to create the largest human-annotated coreference corpus following the OntoNotes guidelines, and the first to be evaluated for consistency with the OntoNotes scheme. Out-of-domain evaluation across 12 genres shows nearly 15-20\% degradation for both deterministic and deep learning systems, indicating a lack of generalizability or covert overfitting in existing coreference resolution models. 



\end{abstract}


\section{Introduction}
Coreference resolution is the task of grouping referring expressions that point to the same entity, such as noun phrases and the pronouns that refer to them. The task entails detecting correct mention or `markable' boundaries and creating a link with previous mentions, or antecedents. A coreference chain is a series of decisions which groups the markables into clusters. As a key component in Natural Language Understanding (NLU), the task can benefit a series of downstream applications such as Entity Linking, Dialogue Systems, Machine Translation, Summarization, and more \cite{poesio2016anaphora}.


In recent years, deep learning models have achieved high scores in coreference resolution. The end-to-end approach \cite{lee-etal-2017-end, lee-etal-2018-higher} jointly scoring mention detection and resolution currently not only beats earlier rule-based and statistical methods but also outperforms other deep learning approaches \cite{wiseman-etal-2016-learning, clark-manning-2016-deep, clark-manning-2016-improving}. Additionally, language models trained on billions of words significantly improve performance by providing rich word and context-level information for classifiers \citep{lee-etal-2018-higher, DBLP:journals/corr/abs-1907-10529, Joshi2019BERTFC}. 







\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

\begin{table*}[t!hb]
    \centering \small
    \begin{tabular}{l|C{1.45cm}|C{1.45cm}|C{1.45cm}|C{1.45cm}|C{1.45cm}|C{1.45cm}|C{1.45cm}}
    Genre & Documents & Tokens & Mentions & Proper & Pron. & Other & Clusters \\
    \hline\hline
    \textit{academic (ac)} & 16 & 15,112 & 1,232 & 283 & 262 & 687 & 421 \\
    \textit{bio (bi)} & 20 & 17,963 & 2,312 & 934 & 796 & 582 & 487 \\
    \textit{conversation (cn)} & 5 & 5,701 & 1,027 & 40 & 728 & 259 & 176 \\
    \textit{fiction (fc)} & 18 & 16,312 & 2,740 & 259 & 1,700 & 781 & 469 \\
    \textit{interview (it)} & 19 & 18,060 & 2,622 & 501 & 1,223 & 898 & 608 \\
    \textit{news (nw)} & 21 & 14,094 & 1,803 & 796 & 340 & 667 & 477 \\
    \textit{reddit (rd)} & 18 & 16,286 & 2,297 & 117 & 1,336 & 844 & 578 \\
    \textit{speech (sp)} & 5 & 4,834 & 601 & 171 & 245 & 185 & 134 \\
    \textit{textbook (tx)} & 5 & 5,379 & 466 & 108 & 165 & 193 & 133 \\
    \textit{vlog (vl)} & 5 & 5,189 & 882 & 22 & 600 & 260 & 149 \\
    \textit{voyage (vy)} & 17 & 14,967 & 1,339 & 564 & 300 & 475 & 348 \\
    \textit{whow (wh)} & 19 & 16,927 & 2,057 & 53 & 1,001 & 1,003 & 491 \\
    \hline
    Total & 168 & 150,824 & 19,378 & 3,848 & 8,696 & 68,34 & 4,471 \\
    \hline
    \end{tabular}

    \caption{Genre-breakdown Statistics of OntoGUM.}
    \label{tab:genre_stats}
\end{table*}

However, scores on the identity coreference layer of benchmark OntoNotes dataset \cite{pradhan-etal-2013-towards} do not reflect the generalizability of these systems. \citet{moosavi-strube-2017-lexical} pointed out that lexicalized coreference resolution models, including neural models using word embeddings, face a covert overfitting problem because of a large overlap between the vocabulary of coreferring mentions in the OntoNotes training and evaluation sets. This suggests that higher scores on OntoNotes-test may not indicate a better solution to the coreference resolution task. 

To investigate the generalization problem of neural models, several projects have tested other datasets consistent with the OntoNotes scheme. \citet{moosavi-strube-2018-using} conducted out-of-domain evaluation on WikiCoref \cite{ghaddar-langlais-2016-wikicoref}, a small dataset employing the same coreference definitions. Results showed that neural models (with fixed embeddings) do not achieve comparable performance (16.8\% degradation in score) as on OntoNotes.
More recently, the e2e model using BERT \cite{Joshi2019BERTFC} showed gains on the GAP corpus \cite{webster2018gap} using contextualized embeddings; however GAP only contains name-pronoun coreference, a very specific subset of coreference, and is limited in domain to the same single source -- Wikipedia. 


Though previous work has already identified the overfitting problem, it also has three main shortcomings. First, the scale of out-of-domain evaluation has been small and homogeneous: WikiCoref only contains 30 documents with $\sim$60K tokens,  much smaller than the OntoNotes test set, and the single genre Wiki domain in both WikiCoref and GAP is arguably not very far from some OntoNotes materials. 
Second, pretrained LMs, e.g. BERT \cite{devlin2018bert}, popularized after the WikiCoref paper, can learn better representations of markables and surrounding sentences. Other than GAP, which targets a highly specific subtask, no study has investigated if contextualized embeddings encounter the same overfitting problem identified by \citeauthor{moosavi-strube-2017-lexical}. Third, previous work may underestimate the performance degradation on WikiCoref in particular due to bias: In \citet{moosavi-strube-2018-using}, embeddings were also trained on Wikipedia themselves, potentially making the model easier to learn coreference relations in Wikipedia text, despite limitations in other genres.




In this paper, we explore the generalizability of existing coreference models on a new benchmark dataset, which we make freely available. Compared with work using WikiCoref and GAP, our contributions can be summarized as follows:



\begin{itemize}
    \item We propose OntoGUM, the largest open, gold dataset consistent with OntoNotes, with 168 documents ($\sim$150K tokens, 19,378 mentions, 4,471 coref chains) in 12 genres,\footnote{\textbf{Text:} News/Fiction/Bio/Academic/Forum/Travel/How-to/Textbook; \textbf{Speech:} Interview/Political/Vlog/Conversation.} including conversational genres, which complement OntoNotes for training and evaluation.
    
    \item We show that the SOTA neural model with contextualized embeddings encounter nearly 15\% performance degradation on OntoGUM, showing that the overfitting problem is not overcome by contextualized language models.
    
    \item We give a genre-by-genre analysis for two popular systems, revealing relative strengths and weaknesses of current approaches and the range of easier/more difficult targets for coreference resolution.
    


\end{itemize}

\section{Related Work}











\paragraph{OntoNotes and similar corpora}
OntoNotes is a human-annotated corpus with documents annotated with multiple layers of linguistic information including syntax, propositions, named entities, word sense, and within document coreference \citep{weischedel-handbook-2011-ontonotes,pradhan-etal-2013-towards}.  It covers three languages---English, Chinese and Arabic. The English subcorpus has 3,493 documents and $\sim$1.6 million words. WikiCoref, which is annotated for anaphoric relations, has 30 documents from English Wikipedia \cite{ghaddar-langlais-2016-wikicoref}, containing 7,955 mentions in 1,785 chains, following OntoNotes guidelines.




\paragraph{GUM} The Georgetown University Multilayer (GUM) corpus \citep{Zeldes2017} is an open-source corpus of richly annotated texts from 12 types, including 168 documents and over 150K tokens. Though it originally contains more coreference phenomena than OntoNotes using more exhaustive guidelines, it also contains rich syntactic, semantic and discourse annotations which allow us to create the OntoGUM dataset described below. We also note that due to its smaller size (currently about 10\% the size of the OntoNotes coreference dataset), it is not possible to train SOTA neural approaches directly on this dataset while maintaining strong performance.

\paragraph{Other corpora} As mentioned above, GAP is a gender-balanced labeled corpus of ambiguous pronoun-name pairs, used for out-of-domain evaluation but limited in coreferent types and genre. Several other comprehensive coreference datasets exist as well, such as ARRAU \cite{poesio-etal-2018-anaphora} and PreCo \cite{chen-etal-2018-preco}, but these corpora cannot be used for out-of-domain evaluation because they do not follow the OntoNotes scheme. Their conversion has not been attempted to date.






























\begin{table*}[t!hb]
    \centering\small
    \begin{tabular}{l|cccccccccc|ccc}


    \multirow{2}{*}{Genre} & \multicolumn{3}{c}{MUC} & \multicolumn{3}{c}{B$^3$} & \multicolumn{3}{c}{CEAF$_{\phi4}$} && \multicolumn{3}{c}{Mention Detection} \\
     & P & R & F1 & P & R & F1 & P & R & F1 & Avg. F1 & P & R & F1\\
     \hline\hline
    
& \multicolumn{13}{c}{dcoref}\\\hline
    \textit{ac} & 35.1 & 37.5 & 36.2 & 32.6 & 34.4 & 33.5 & 35.7 & 37.5 & 36.6 & 35.4 & 48.3 & 51.3 & 49.8 \\
    \textit{bi} & 58.0 & 61.6 & 59.8 & 36.8 & 43.6 & 39.9 & 32.1 & 33.5 & 32.8 & 44.1 & 58.9 & 62.3 & 60.6 \\
    \textit{cn} & 62.2 & 52.9 & 57.1 & 40.5 & 36.7 & 38.5 & 37.1 & 38.2 & 37.6 & 44.4 & 76.6 & 67.8 & 72.0\\
    \textit{fc} & 57.7 & 43.9 & 49.9 & 50.4 & 33.2 & 40.0 & 37.1 & \textcolor{red}{49.0} & \textcolor{red}{42.2} & 44.0 & 68.2 & 59.0 & 63.3\\
    \textit{it} & 57.3 & 53.3 & 55.2 & \textcolor{blue}{29.3} & \textcolor{blue}{21.6} & \textcolor{blue}{24.8} & \textcolor{blue}{22.4} & \textcolor{blue}{24.6} & \textcolor{blue}{23.5} & \textcolor{blue}{27.6} & 64.3 & 60.3 & 62.2\\
    \textit{nw} & 57.6 & 55.2 & 56.4 & 45.7 & 42.3 & 44.0 & 39.6 & 32.5 & 35.7 & 45.3 & 44.0 & 50.2 & 46.9 \\
    \textit{rd} & 59.6 & 65.1 & 62.3 & 38.3 & 53.5 & 44.6 & 32.9 & 34.0 & 33.5 & 46.8 & 60.5 & 64.6 & 62.5 \\
    \textit{sp} & 50.6 & 56.2 & 53.2 & 40.1 & 43.9 & 41.9 & \textcolor{red}{46.5} & 38.6 & \textcolor{red}{42.2} & 45.8 & 63.5 & 64.2 & 63.9\\
    \textit{tx} & 36.0 & 34.2 & 35.1 & 32.7 & 31.0 & 31.9 & 23.9 & 39.9 & 29.9 & 32.3 & 18.1 & 45.8 & 26.0 \\
    \textit{vl} & \textcolor{red}{63.6} & \textcolor{red}{69.4} & \textcolor{red}{66.4} & \textcolor{red}{56.4} & \textcolor{red}{60.8} & \textcolor{red}{58.5} & 31.4 & 36.2 & 33.6 & \textcolor{red}{52.8} & 76.4 & 76.8 & 76.6\\
    \textit{vy} & \textcolor{blue}{34.7} & 37.1 & 35.9 & 30.7 & 28.7 & 29.7 & 29.7 & 35.8 & 32.5 & 32.7 & 46.6 & 62.4 & 53.3\\
    \textit{wh} & 35.8 & \textcolor{blue}{24.2} & \textcolor{blue}{28.9} & 30.0 & 24.5 & 27.0 & 29.9 & 34.0 & 31.8 & 29.2 & 50.0 & 42.9 & 46.2\\

    \hline
    \rule{0pt}{2ex}All OntoGUM & 45.7 & 47.0 & 46.3 & 17.1 & 38.1 & 37.6 & 33.4 & 37.3 & 35.3 & 39.7 & 56.2 & 59.1 & 57.6 \\
    \hline
    \rule{0pt}{2ex}OntoNotes & 57.5 & 61.8 & 59.6 & 68.2 & 68.4 & 68.3 & 47.7 & 43.4 & 45.5 & 57.8 & 66.8 & 75.1 & 70.7 \\
    
\hline\hline
    
    & \multicolumn{13}{c}{\citet{DBLP:journals/corr/abs-1907-10529}}\\\hline
    \textit{ac} & 84.5 & 53.0 & 65.1 & 83.3 & 48.5 & 61.3 & \textcolor{red}{83.2} & 47.0 & 60.1 & 62.2 & 91.0 & 55.2 & 68.7 \\
    \textit{bi} & 85.8 & 74.7 & 79.8 & 61.4 & 64.3 & 62.8 & 65.4 & 49.9 & 56.6 & 66.4 & 87.7 & 74.5 & 80.5\\
    \textit{cn} & 85.0 & 73.4 & 78.7 & 67.9 & 64.5 & 66.2 & 70.2 & 51.1 & 59.1 & 68.0 & 93.0 & 77.9 & 84.8\\
    \textit{fc} & \textcolor{red}{87.0} & 62.5 & 73.0 & 78.8 & 54.1 & 64.1 & 62.5 & 53.1 & 57.4 & 64.8 & 91.1 & 67.7 & 77.7\\
    \textit{it} & 83.9 & 71.8 & 77.4 & 76.1 & 60.4 & 67.3 & 72.9 & 50.6 & 59.7 & 68.2 & 85.9 & 70.4 & 77.3\\
    \textit{nw} & 65.3 & 65.8 & 65.5 & 60.1 & 59.6 & 59.9 & 58.9 & 54.3 & 56.5 & 60.6 & 71.9 & 70.5 & 71.2 \\
    \textit{rd} & 76.7 & 67.4 & 71.7 & 67.5 & 60.3 & 63.7 & 69.5 & \textcolor{blue}{40.5} & \textcolor{blue}{51.1} & 61.7 & 85.3 & 68.1 & 75.8 \\
    \textit{sp} & 83.3 & 63.4 & 72.0 & 71.2 & 56.6 & 63.1 & 77.3 & 57.3 & \textcolor{red}{65.8} & 67.0 & 91.9 & 69.4 & 79.0\\
    \textit{tx} & \textcolor{blue}{50.0} & 66.6 & 57.1 & \textcolor{blue}{45.2} & 65.7 & 53.6 & \textcolor{blue}{55.6} & 55.6 & 55.6 & \textcolor{blue}{55.5} & 60.0 & 72.2 & 65.5\\
    \textit{vl} & \textcolor{red}{86.1} & \textcolor{red}{86.1} & \textcolor{red}{86.1} & 78.4 & \textcolor{red}{79.8} & \textcolor{red}{79.1} & 63.6 & 47.7 & 54.5 & \textcolor{red}{73.3} & 89.4 & 85.4 & 87.4\\
    \textit{vy} & 69.0 & 70.4 & 69.7 & 52.7 & 64.1 & 57.9 & 65.9 & 53.0 & 58.8 & 62.1 & 78.9 & 75.5 & 77.2\\
    \textit{wh} & 84.8 & \textcolor{blue}{40.9} & \textcolor{blue}{55.2} & \textcolor{red}{83.4} & \textcolor{blue}{39.2} & \textcolor{blue}{53.3} & 71.4 & \textcolor{red}{57.4} & 63.6 & 57.4 & 93.2 & 52.4 & 67.1\\

    \hline
    \rule{0pt}{2ex}All OntoGUM & 79.7 & 66.3 & 72.4 & 69.5 & 58.58 & 63.7 & 67.7 & 50.7 & 58.0 & 64.6 & 85.4 & 69.2 & 76.5\\
    
    \hline    
    
    \rule{0pt}{2ex}OntoNotes & 85.8 & 84.8 & 85.3 & 78.3 & 77.9 & 78.1 & 76.4 & 74.2 & 75.3 & 79.6 & 89.1 & 86.5 & 87.8\\
    
    \hline
\end{tabular}
    \caption{Results on the OntoGUM's test dataset with the deterministic coref model (top) and the SOTA coreference model (bottom). The blue text is the lowest score across 12 genres and red text is the highest.}
    \label{tab:res}
\end{table*}

\paragraph{Coreference resolution systems} Prior to the introduction of deep learning systems, the coreference task was approached using deterministic linguistic rules \citep{lee-etal-2013-deterministic, recasens-etal-2013-life} and statistical approaches
\citep{durrett-klein-2013-easy, durrett-klein-2014-joint}.
More recently, three neural models achieved SOTA performance on this task: 1) ranking the candidate mention pairs \citep{wiseman-etal-2015-learning,clark-manning-2016-deep}, 2) modeling global features of entity clusters \citep{clark-manning-2015-entity, clark-manning-2016-improving, wiseman-etal-2016-learning}, and 3) end-to-end (e2e) approaches with joint loss for mention detection and coreferent pair scoring \cite{lee-etal-2017-end, lee-etal-2018-higher, fei-etal-2019-end}. The e2e method has become the dominant one, gaining the best scores on OntoNotes. To investigate differences between deterministic and deep learning models on unseen data, we evaluate the two approaches on OntoGUM.


\section{Dataset Conversion}
GUM's annotation scheme subsumes all markables and coreference chains annotated in OntoNotes, meaning we do not need human annotation to recognize additional mentions in the conversion process, though mention boundaries differ subtly (e.g.~for appositions and verbal mentions). Since GUM has gold syntax trees, we were able to process the entire conversion automatically. Additionally, most coreference evaluations use gold speaker information in OntoNotes, which is available in GUM (for \textit{fiction}, \textit{reddit} and spoken data) and could be assembled automatically as well.

The conversion is divided into two parts: removing coreference relations not included in the OntoNotes scheme, and removing or adjusting markables. For coreference relation deletion, we cut chains by removing expletive cataphora, and identifying the definiteness of nominal markables, since indefinites cannot be anaphors in OntoNotes. In addition to modifying existing mention clusters, we also remove particular coreference relations and mention spans, such as Noun-Noun compounding (only included in OntoNotes for proper-name modifiers), bridging anaphora, copula predicates, nested entities (`i-within-i'= single mentions containing coreferring pronouns), and singletons (all not included in OntoNotes). We note that singletons are removed as the final step, in order to catch singletons generated during the conversion process. We also contract verbal markable spans to their head verb, and merge appositive constructions into single mentions, following the OntoNotes guidelines.
\footnote{The code and dataset are publicly available at \url{https://github.com/yilunzhu/ontogum}.}

To evaluate conversion accuracy, three annotators, including an original OntoNotes project member, conducted an agreement study on 3 documents, containing 2,500 tokens and 371 output mentions. Re-annotating from scratch based on OntoNotes guidelines, the conversion achieves a span detection score of $\sim$96 and CoNLL coreference score of $\sim$92, approximately the same as human agreement scores on OntoNotes. After adjudication, the conversion was found to make only 8/371 errors, in addition to 2 errors due to mistakes in the original GUM data, meaning that degradation due to conversion errors is marginal, and consistency should be close to the variability in OntoNotes itself.







\section{Experiments}
We evaluate two systems on the 12 OntoGUM genres, using the official CoNLL-2012 scorer \cite{pradhan-etal-2012-conll, pradhan-etal-2014-scoring}. The primary score is the average F1 of three metrics -- MUC, B$^3$, and CEAF$_{\phi4}$.

\paragraph{Deterministic coreference model}

We first run the deterministic system (dcoref, part of Stanford CoreNLP, \citealt{manning-EtAl:2014:P14-5})
on the OntoGUM benchmark, as it remains a popular option for off-the-shelf coreference resolution. As a rule-based system, it does not require training data, so we directly test it on OntoGUM's test set. However, POS tags, lemmas, and named-entity (NER) information are predicted by CoreNLP, which does have a domain bias favoring newswire. The system's multi-sieve structure and token-level features such as gender and number remain unchanged. We expect that the linguistic rules will function similarly across datasets and genres, notwithstanding biases of the tools providing input features to those rules.


\paragraph{SOTA neural model} Combining the e2e approach with a contextualized LM and span masking is the current SOTA on OntoNotes. The system
utilizes the pretrained SpanBERT-large model, fine-tuned on the OntoNotes training set. Hyperparameters are identical to the evaluation of OntoNotes test to ensure comparable results between the benchmarks. We note that while we choose the SOTA system as a `best case scenario', most off-the-shelf neural NLP toolkits (e.g. spaCy) actually use somewhat simpler e2e models than SpanBERT-large, due to memory/performance constraints.



\section{Results}


\paragraph{OntoGUM vs. OntoNotes} The last rows in each half of Table \ref{tab:res} give overall results for the systems on each benchmark. e2e+SpanBERT encounters a substantial degradation of 15 points (19\%) on OntoGUM, likely due to lower test set lexical and stylistic overlap, including novel mention pairs. We note that its average score of 64.6 is somewhat optimistic, especially given that the system receives access to gold speaker information wherever available (including in \textit{fc}, \textit{cn} and \textit{it}, some of the better scoring genres), which is usually unrealistic. dcoref, assumed to be more stable across genres, also sees losses on OntoGUM of over 18 points (30\%). We believe at least part of the degradation may be due to mention detection, which is trained on different domains for both systems (see the last three columns in the table). These results suggest that input data from CoreNLP degrades substantially on OntoGUM, or that some types of coreferent expressions in OntoGUM are linguistically distinct from those in OntoNotes, or both, making OntoGUM a challenging benchmark for systems developed using OntoNotes. 


\paragraph{Comparing genres} Both systems degrade more on specific genres. For example, while \textit{vl} (with gold speaker information) fares well for both systems, neither does well on \textit{tx}, and even the SOTA system falls well below (or around) 60s for the \textit{nw}, \textit{wh} and \textit{tx} genres. This might be surprising for \textit{vl}, which contains transcripts of spontaneous unedited speech from YouTube Creative Commons vlogs quite unlike OntoNotes data; conversely the result is less expected for carefully edited texts which are somewhat similar to data in OntoNotes: OntoNotes contains roughly 30\% newswire text, and it is not immediately clear that GUM's \textit{nw} section, which comes from recent Wikinews articles, differs much in genre. Examples (\ref{ex:ultrasounds})--(\ref{ex:report}) illustrate incorrectly predicted coreference chains from both sources and the type of language they contain.

\begin{exe}
\ex \textit{I've been here just crushing ultrasounds ... I've been like crushing \cfbox{red}{these} all day today ... I got sick when I was on Croatia for vacation. I have no idea what it says, but I think \cfbox{red}{they}'re cough drops.} (example from a radiologist's vlog, incorrect: ultrasounds $\neq$ cough drops) \label{ex:ultrasounds}

\ex \textit{\cfbox{red}{The report} has prompted calls for all edible salt to be iodised ... Tasmania was excluded from \lfbox[border-style={solid,none,solid,solid},border-color=red]{the study - where a voluntary iodine for-} \lfbox[border-style={solid,none,solid,none},border-color=red]{tification program using iodised salt in} \lfbox[border-style={solid,solid,solid,none},border-color=red]{bread}, is ongoing} (newswire example, incorrect span and coref: [the study - where a voluntary...]) \label{ex:report}
\end{exe}

These examples show that errors occur readily even in quite characteristic news writing, while genre disparity by itself does not guarantee low performance, as in the case of the vlogs whose lanugage is markedly different. In sum, these observations suggest that accurate coreference for downstream applications cannot be expected in some common well edited genres, despite the prevalence of news data in OntoNotes (albeit specifically from the Wall Street Journal, around 1990). This motivates the use of OntoGUM as a test set for future benchmarking, in order to give the NLP community a realistic idea of the range of performance we may see on contemporary data `in the wild'.

We also suspect that prevalence of pronouns and gold speaker information produce better scores in the results. Table \ref{tab:mention_pair} ranks genres by their e2e CoNLL score, and gives the proportions of pronouns, as well as score rankings for span detection. Because pronouns are usually easier to detect and pair than nouns \citep{durrett-klein-2013-easy}, more pronouns usually means higher scores. On genres with more than 50\% pronouns and gold speakers (\textit{vl}, \textit{it}, \textit{cn}, \textit{sp}, \textit{fc}) e2e gets much higher results, while genres with few pronouns ($<$30\%) have lower scores (\textit{ac}, \textit{vy}, \textit{nw}). This diversity over 12 genres supports the usefulness of OntoGUM, which can evaluate the genrealizability of coreference systems.

\begin{table}[h!tb]
    \centering\small
    \begin{tabular}{l|c|c|c|c|c}
& PRON (R) & Other (R) & Total & CoNLL & Span \\\hline
        \textit{vl} & 600 (.66) & 309 (.34) & 909 & 1 & 1\\
        \textit{it} & 1223 (.45) & 1485 (.55) & 2708 & 2 & 6\\
        \textit{cn} & 729 (.61) & 323 (.39) & 1052 & 3 & 2\\
        \textit{sp} & 245 (.40) & 364 (.60) & 609 & 4 & 4\\
        \textit{bi} & 796 (.34) & 1529 (.66) & 2325 & 5 & 3\\
        \textit{fc} & 1700 (.61) & 1091 (.39) & 2791 & 6 & 5\\
        \textit{ac} & 262 (.21) & 997 (.79) & 1259 & 7 & 10\\
        \textit{vy} & 300 (.22) & 1053 (.78) & 1353 & 8 & 7\\
        \textit{rd} & 1337 (.55) & 1077 (.45) & 2414 & 9 & 8\\
        \textit{nw} & 340 (.19) & 1483 (.81) & 1823 & 10 & 9\\
        \textit{wh} & 1001 (.47) & 1129 (.53) & 2130 & 11 & 11\\
        \textit{tx} & 165 (.34) & 315 (.66) & 480 & 12 & 12\\
        \hline
    \end{tabular}
    \caption{Mention-type counts (ratios) \& ranks of SOTA scores by genre (CoNLL score + span detection).}
    \label{tab:mention_pair}
\end{table}


\section{Conclusion}

This paper presented OntoGUM, the largest open, gold coreference dataset following the OntoNotes scheme, adding several new genres (including more spoken data) to the OntoNotes family. The corpus is automatically converted from GUM by modifying the existing markable spans and coreference relations using multi-layer annotations, such as dependency trees. Results showed a lack of generalizability of existing systems, especially in genres low in pronouns and lacking speaker information. We suspect that at least part of the success of SOTA approaches is due to correct mention detection and high matching scores in genres rich in pronouns, and more so with gold speaker information. Success for other types of mentions in OntoNotes data appears to be much more sensitive to lexical features, performing well on the benchmark test set with high lexical overlap to the training data, but degrading very substantially outside of it, even on newswire texts from our OntoGUM data. This supports use of this challenging dataset for future work, which we hope will benefit evaluations of systems targeting the OntoNotes standard. 


\bibliography{acl2021}
\bibliographystyle{acl_natbib}


\end{document}
