\documentclass{article} \usepackage{iclr2023_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak


\newcommand{\bbox}{\hfill }


\newcommand{\benr}{}
\newcommand{\benrr}{}
\newcommand{\ben}{}
\newcommand{\benn}{}
 
\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx} 
\usepackage{amssymb} 
\usepackage{amsmath} 
\usepackage{multirow} 
\usepackage{float} 
\usepackage{makecell} 
\usepackage{wrapfig} 
\usepackage{slashed} 
\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{graphicx}
\usepackage{subcaption}




\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}

\title{Boosting Out-of-Distribution Detection with Multiple Pre-trained Models}





\author{Feng Xue\\
Hunan University\\
\And
Zi He\\
Hunan University \\
\And
Chuanlong Xie\\
Beijing Normal University\\
\AND 
Falong Tan \thanks{Correspondence to: falongtan@hnu.edu.cn} \\
Hunan University\\
\And
Zhenguo Li\\
Huawei Noah's Ark Lab\\
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}


\maketitle

\begin{abstract}

Out-of-Distribution (OOD) detection, i.e., identifying whether an input is sampled from a novel distribution other than the training distribution, is a critical task for safely deploying machine learning systems in the open world.
Recently, post hoc detection utilizing pre-trained models has shown promising performance and can be scaled to large-scale problems.
This advance raises a natural question: Can we leverage the diversity of multiple pre-trained models to improve the performance of post hoc detection methods?
In this work, we propose a detection enhancement method by ensembling multiple detection decisions derived from a zoo of pre-trained models. Our approach uses the p-value instead of the commonly used hard threshold and leverages a fundamental framework of multiple hypothesis testing to control the true positive rate of In-Distribution (ID) data.
We focus on the usage of model zoos and provide systematic empirical comparisons with current state-of-the-art methods on various OOD detection benchmarks.
The proposed ensemble scheme shows consistent improvement compared to single-model detectors and significantly outperforms the current competitive methods. Our method substantially improves the relative performance by  and  on the CIFAR10 and ImageNet benchmarks.



\end{abstract}

\section{Introduction}

Deep neural networks have achieved empirical success in many applications, but generalization robustness has always been a thorny problem in deep learning.
A sophisticated and well-trained deep neural network can provide excellent test performance on identically distributed (ID) test data but may fail to make accurate predictions on inputs from outside the training distribution \cite {nguyen2015deep}.
This poses a big obstacle to the generalization of deep neural network models.
Especially in safety-critical applications, it is better to identify out-of-distribution (OOD) inputs ahead of time rather than letting the model make predictions that may be unreliable. 


On the basis of pre-trained deep neural networks, many recent works on post hoc OOD detection have proposed diverse score functions to distinguish OOD samples utilizing the output probability, logits, gradients, and features of the pre-trained classifier. 
At the same time, some works also propose new training strategies to encourage the network to learn more features that may not be relevant to the OOD classification task.
For example,   MSP \citep{hendrycks17baseline} uses the maximum softmax probability, Energy score \citep{liu2020energy} considers the logits, and GradNorm \citep{huang2021importance} employs the vector norm of gradients.
Based on these frameworks, several improved methods such as ODIN \citep{liang2018enhancing}, Adjusted Energy Score \citep{lin2021mood}, ReAct \citep{sun2021react} are proposed to enhance the performance of OOD detection.
These score functions above measure the similarity between a test input and the training (ID) data through pre-trained feature extractors or classifiers.
There are also many distance-based algorithms that directly quantify the distance of samples in the embedding space extracted from a pre-trained model and regard a test input as an OOD sample when it is far from the ID data.
\cite{lee2018simple} assumes the conditional distribution of features given the class label is a Gaussian distribution and derives a confidence score based on the Mahalanobis distance. 
SSD \citep{sehwag2021ssd} considers self-supervised pre-training and a Mahalanobis distance. 
\cite{tack2020csi} uses contrastive learning with distributionally-shifted augmentations for pre-training and proposes a detection score specific to their training scheme.
\cite{sun2022knn} studies the nearest-neighbor distance and demonstrates the efficacy of non-parametric modeling of the feature distribution for OOD detection tasks.


The performance of post hoc detection highly depends on the quality of pre-training.
The most commonly used model architectures in OOD detection include convolutional networks such as ResNet \citep{he2016deep}, DenseNet \citep{huang2017densely} and Wide-ResNet \citep{Zagoruyko2016WRN}, and of course Transformer models such as Swin \citep{liu2021swinv2} or ViT \citep{dosovitskiy2021an}.
In general, the pre-trained models focus on the features related to classification tasks and the learnt representation may be insufficiently rich for OOD detection.
Therefore, researchers have proposed ideas such as contrastive learning \citep{winkens2020contrastive,tack2020csi}, adversarial training \cite{biggio2018wild,miller2020adversarial,chalapathy2019deep} , outlier exposure \citep{hendrycks2018deep,papadopoulos2021outlier} or other  auxiliary artificially synthesized data \citep{lee2017training} and auxiliary loss function \citep{vyas2018out} to encourage models to learn high-level, task-agnostic and comprehensive features, which makes the model more robust and efficient in the downstream detection task. 
These models trained with different architectures and training strategies can extract diverse features that may complement each other well.
So, a natural question is raised:
\begin{center}
{\it Can we leverage the diversity of multiple pre-trained models to improve \\
the performance of post hoc OOD detectors?}
\end{center}
To answer this question, we first build a model zoo that captures as many properties of the input as possible and remains sensitive to distributional changes.
Then we reformulate the OOD detection task to check whether there exists a model in the model zoo that can identify the test input as an OOD sample.
Section~\ref{sec31} shows that the naive ensemble of multiple OOD detection decisions cannot maintain the true positive rate of the ID data (TPR). Therefore, we propose an ensemble scheme to integrate the results of multiple OOD detectors and provide theoretical guarantees that our method can keep TPR at the target level.
In Section~\ref{Experiments}, we also report the empirical TPR of our method, which is close to the target TPR level. 


Ensembling is not new to OOD detection.
\cite{morningstar2021density} combines multiple test statistics from generative models to differentiate ID and OOD data.
\cite{haroush2021statistical} uses both the Simes' method and Fisher's method to summarize p-values computed for each channel and layer of a deep neural network.
\cite{bergamin2022model} shows that combining different types of test statistics using Fisher’s method overall leads to a more accurate out-of-distribution test.
Recently, \cite{magesh2022multiple} proposes an ensemble framework that combines any number of different test statistics using the Benjamini–Yekutieli procedure \citep{benjamini2001control} and a conformal p-value estimator \citep{vovk1999machine}.
In this work, we develop a simple and fundamental ensemble scheme for using model zoos in OOD detection and name our method {\bf Z}oo-based {\bf O}OD {\bf D}etection {\bf E}nhancement ({\bf ZODE}).
Our method directly estimates the p-values according to its definition and employs the Benjamini–Hochberg procedure \citep{benjamini1995controlling} to control TPR.
Then, we provide theoretical guarantees and empirical validation to show that ZODE can maintain the TPR close to its target level. 
On the other hand, we focus on the settings of the model zoo and conduct systematic experiments to demonstrate the superiority of our approach.
First, we show that ZODE can consistently improve current OOD detectors. 
Second, by comparing single-model detectors with the ZODE-ensembled detector, we find that ZODE can exploit the diversity of multiple pre-trained models and leverage complementarity among single-model detectors.
Finally, our approach significantly improves current SOTA performance. 


We summarize our contributions as follows:
\begin{itemize}
\item[] 
We provide novel insights into OOD detection from the perspective of the model zoo. 
We propose an enhancement scheme, ZODE, for OOD detection by exploiting the diversity of pre-trained models.
The proposed method is inspired by a simple and fundamental framework of multiple hypothesis testing. Our theoretical results and experiments clearly show that ZODE can leverage the complementarity among single-model detectors to improve performance.
\item[] We point out that the naive ensemble of multiple OOD detectors leads to lower TPR. Then we provide theoretical analysis and empirical validation to demonstrate that our proposed method can maintain TPR well under the settings of the model zoo.
\item[] Extensive experiments show that our method can effectively and consistently improve the power of identifying OOD samples.
On a commonly used CIFAR10 benchmark, our method significantly improves the SOTA result of the average false positive rate from  to . 
For a challenging OOD detection task based on ImageNet, we show that our method is scalable to large-scale problems and significantly improves the SOTA result of the average false positive rate from  to . 
\end{itemize}


\section{Preliminaries}\label{Preliminaries}

Out-of-Distribution Detection aims to check whether a test input is generated from the training distribution or not. 
It is a one-sample hypothesis testing problem if we can only access the training data. 
We denote  and  as the input and label space respectively and let   be the training distribution over . 
Suppose that  is a neural network trained on data drawn from  to predict the label of input  
Let  denote the marginal distribution on  Then we call  an in-distribution (ID) sample, otherwise, we identify it as an ``unknown" input, called out-of-distribution (OOD) data. 
At test time, OOD detection distinguishes OOD samples and ID samples by using a decision function:

where  is a test input,  is a score function that gives higher scores for ID data and lower for OOD data, and  is the threshold. In this work, we consider post hoc OOD detection in which the score function  is derived from a pre-trained classifier , i.e.  

We denote  as the distribution of  with  and any pre-trained model 
Then, if  is an ID sample, the score  is an ID value following the distribution 
Therefore, given a pre-trained model zoo , we strengthen the OOD detection problem to:   
\begin{center}
    \it{ Is there  that would allow us to identify  as an OOD sample?} \end{center}
In this work, we proposed an approach to achieve the goal of this OOD detection problem. 


\section{Methodology}\label{Method}

\subsection{Naive ensemble cannot maintain TPR}\label{sec31}

To leverage the model zoo  for OOD detection, a straightforward way is to execute the detection procedure in Eq.(\ref{detection}) based on each pre-trained model:

and identify  as an OOD sample if there exists  such that , i.e., 

In other words,  is classified as an ID sample only if all detectors ,   agree that  is an ID sample.
However, this simple approach is not easy to control the true positive rate of the ID data (TPR). In practice, the threshold  is chosen so that a high fraction (e.g. ) of ID data is correctly identified. 
We denote the target level of the true positive rate of the ID data as  and write . 
Therefore, each detector  has a  probability of misidentifying an ID sample as an OOD sample.
When ensembling multiple single-model detectors, the probability of making mistakes also accumulates.
It is easy to see that {\bf the detector  can misidentify an ID sample as an OOD sample with probability more than , specifically  when detectors are independent.}
As more and more pre-trained models become available, this error probability of  increases until it becomes  
This implies that the naive ensembled detector cannot maintain the target TPR level. 
On the other hand, by fixing , we can assign a low probability to  to make sure 
In this case,  should be very large and even close to 
This greatly reduces the probability of successfully identifying OOD data, as each single-model detector becomes very conservative and can only identify extreme OOD data. 
In this work, we develop an ensemble scheme that can maintain the target TPR level while keeping a high probability of successfully identifying OOD data.


\subsection{Using P-value for OOD detection}\label{sec32}

However, directly integrating score functions is uninterpretable and lacks theoretical guarantees.
Therefore, we use the p-value for OOD detection.
P-value \citep{abramovich2013statistical} is defined in the framework of statistical hypothesis testing.
In OOD detection, the p-value is a probability measure that quantifies how extreme the observed score is when the input is an ID sample \citep{cai2020real,morningstar2021density,haroush2021statistical,bergamin2022model,magesh2022multiple,kaur2022idecode}.
For example, we identify an input  as an OOD sample (reject the null hypothesis) when the observed detection score  is smaller than a critical value .
Given a test sample , the lower value of , the more likely  is not drawn from the training distribution. Hence, the p-value of  is the probability that  is less than   under the ID distribution, that is, 

In general, if the p-value of  is less than , we can determine that  is an OOD sample at the significance level . In Appendix~\ref{App:C}, we show that using the p-value is equivalent to using the hard threshold  in Eq. (\ref{detection}).


Suppose the test input  is an ID sample that  and the detection score  is a continuous random variable. 
We write  as the p-value of  and let  be the cumulative distribution function of  with 
Then we have

It follows from the continuity of  and Lemma 21.1 of \cite{van2000asymptotic} that
\benrr
\sP(p_0 < \alpha) & = & 1- \sP\big( F(S(\rvx^{*})) \geq \alpha \big) \\
    & = & 1- \sP\big( S(\rvx^{*}) \geq F^{-1}(\alpha) \big) = F(F^{-1}(\alpha)) = \alpha. 
\eenrr
This implies that {\bf the p-value of  follows a uniform distribution }.
In the following, we will use this property to develop an ensemble scheme (Theorem~\ref{Theorem1} and Lemma~\ref{lemma2}). 

\subsection{TPR controlling for ensemble}

According to Eq. (\ref{p-value}), the p-value relies on the score function , which is derived from a pre-trained model , i.e.  
Given one pre-trained model, we can construct a score function and compute the p-value of a test input.
But when multiple pre-trained models are accessible, how to fuse the single-model results to leverage the diversity of multiple pre-trained models while strictly maintaining TPR on ID data?

We borrow the idea of the Benjamini-Hochberg procedure \citep{benjamini1995controlling} and propose an ensemble scheme for OOD detection via p-value correction. 
Consider a model zoo with  pre-trained models:  and a score function 
Given a test input  and a pre-trained model , we compute the score value  and obtain the corresponding p-value 
Going through all pre-trained models, we obtained  p-values: , and sort them in ascending order:  
Then, we identify the test input  as an OOD sample if there exists an integer  such that  Here ‘TPR’ is a predetermined TPR level of the ID data. In general, it is taken to be  
We call the proposed method Zoo-based OOD Detection Enhancement (ZODE) and present the details of ZODE in Algorithm~\ref{algorithm1}.
Next, we provide theoretical guarantees that Algorithm~\ref{algorithm1} can maintain the target TPR level on ID data.


\begin{theorem}\label{Theorem1}
Suppose a pre-trained model zoo  is accessible and the score function is  Let  be a predetermined TPR level for the ID Data. If the test input  is an ID sample that  and  is independent of  for , then Algorithm 1 can identify  as an ID data with probability larger than 
\end{theorem}

{\bf Remark.} 
Here we assume that  is independent of , which leads to the independence between  and  for . 
This assumption can hold if different pre-trained models learn completely different features. 
In this case, the model zoo haves the desired diversity.
In practice, the pre-trained models can still be very diverse but different models may extract related features.
Therefore, we report the empirical TPR of our method in Section~\ref{Experiments}.
One can find that ZODE can still maintain the empirical TPR not less than the target level though the p-values may be related. 
The proof is postponed to Appendix~\ref{App:A}. In Appendix~\ref{App:B}, we analyze the detection power of Algorithm~\ref{algo1} as  tends to infinity, which implies that FPR is guaranteed as the size of the model zoo increases. Appendix~\ref{App:D} presents more discussions about the ensemble scheme and compares the BH procedure with three baseline ensemble schemes.

{\bf Computational complexity.} In Algorithm 1, we decompose ZODE into three stages: inference, testing, and ensemble. The inference stage requires computing the score of all validation samples for all pre-trained models,  and its computational complexity is  times that of post hoc OOD detection using a single pre-trained model since ZODE uses  pre-trained models. However, the inference stage only needs the ID data and can be done before deploying the OOD detector. Therefore, its computational complexity does not increase the detection time when testing new inputs. In addition, the inference stage is only feed-forward and is easily parallelizable. Therefore, the computational burden is not heavy. In this work, all experiments can be done using one NVIDIA Tesla V100 GPU. 


{\bf Interpretability.} One of the benefits of ZODE is interpretability. If a test input is classified as an OOD sample, we can track which pre-models lead to this detection decision.
At Step 9 of Algorithm~\ref{algorithm1}, if  and , , 
then there are  pre-trained models, corresponding to  respectively,  that identify the test input as an OOD sample.
In our experiments, we exploit this interpretability to find that there are OOD images that only one pre-trained model can detect. This implies that ZODE leverages the complementarity between all single-model detectors.



{\bf Limitation.} The limitation of ZODE is that the testing stage takes up a lot of storage space. Post hoc OOD detection computes the score of all validation samples and selects a hard threshold by the quantile of the empirical distribution of the detection score. Therefore, post hoc OOD detection only passes the threshold from the inference stage to the testing stage. In Algorithm~\ref{algo1}, the testing stage requires the score of validation samples to compute p-values.
Therefore, the testing stage of ZODE takes up more storage space than post hoc OOD detection methods.




\begin{algorithm}[t]
\caption{{ZODE}: Zoo-based OOD Detection Enhancement}\label{algorithm1}
\label{algo1}
\begin{algorithmic}[1]
\REQUIRE Training data , pre-trained model zoo , test sample , detection score ,  TPR level for ID data ‘TPR’;\\ 
\STATE \bf Stage 1. Inference
\STATE Compute the score value of ,  and ;
\STATE \bf Stage 2. Testing
\FOR {} 
\STATE Estimate the p-value of  given :

\ENDFOR\\
\STATE \bf Stage 3. Ensemble
\STATE Sort   in
ascending order: ;
\IF {  such that } 
\RETURN  is an OOD sample;
\ELSE
\RETURN  is an ID sample.
\ENDIF
\end{algorithmic}
\end{algorithm}





\section{Experiments}\label{Experiments}

In this section, we demonstrate the effectiveness of our proposed method. 
First, we evaluate whether our model zoo and ensemble scheme can enhance OOD detectors. Second, we demonstrate that ZODE exploits the diversity of pre-trained models and leverages the complementarity between the single-model detectors to achieve superior performance.
Finally, we show that our method can significantly improve the current SOTA results. 

\textbf{Dataset}: 
We evaluate our proposed method on the CIFAR benchmarks. 
We use CIFAR10 \citep{krizhevsky2009learning} as the ID data and evaluate OOD detectors on six OOD datasets: SVHN \citep{netzer2011reading}, LSUN \citep{yu2015lsun}, iSUN \citep{xu2015turkergaze}, Texture \citep{cimpoi2014describing}, Places365 \citep{zhou2017places}, and CIFAR100 \citep{krizhevsky2009learning}.
We then consider more challenging benchmarks based on ImageNet, i.e., large-scale OOD detection tasks. The ID data is ImageNet-1K \citep{deng2009imagenet}. 
We evaluate OOD detectors on four test datasets that are subset of : Places365 \citep{zhou2017places}, iNaturalist \citep{van2018inaturalist}, SUN \citep{xiao2010sun},   and Texture \citep{cimpoi2014describing} with different categories of each other. 

\textbf{Metrics}: We evaluate OOD detection methods by the following three metrics: (1) the true positive rate of the ID samples (TPR); (2) the false positive rate of OOD samples when the true positive rate of the ID samples is about  (FPR); (3) the area under the receiver operating characteristic curve (AUC). For single-model detectors, the hard threshold is determined by  Therefore, the first metric aims to check whether our ensemble scheme can maintain the TPR level close to . FPR and AUC are often used in the literature to reflect the capabilities of OOD detectors. 
For the AUC metric, we use grid values of TPR ranging from  to  with a gap of 0.0005 and obtain the corresponding FPR to compute the area under the receiver operating characteristic curve. 


\textbf{Enhanced OOD detection}: We consider three OOD detection methods: MSP \citep{hendrycks17baseline}, Energy \citep{liu2020energy} and KNN \citep{sun2022knn}.
MSP is a simple baseline method that uses maximum softmax probabilities as the detection score. 
In some experiments, MSP can yield surprisingly good results when used on top of a large pre-trained model that has been fine-tuned on the ID data \citep{fort2021exploring}.
The energy-based model \citep{lecun2006} maps a test input to a scalar that is higher for OOD samples and lower for the training data. \cite{liu2020energy} proposes an energy score that uses the logits output by a pre-trained classifier.
\cite{sun2022knn} uses the feature distance between the test input and the -th nearest ID sample and proposes a KNN-based detector. 
These three OOD detection methods represent three kinds of detectors based on probability, logit, and distance, respectively.
We take them as the baseline methods and denote our enhanced methods by `ZODE-MSP', `ZODE-Energy', and `ZODE-KNN' respectively.


\subsection{Evaluation on CIFAR10 Benchmarks}

{\bf Model Zoo.} We build a model zoo with seven pre-trained models: ResNet18, ResNet34, ResNet50, ResNet101, ResNet152 \citep{he2016deep}, DenseNet \citep{huang2017densely} and ResNet18 \citep{sun2022knn}.
Here ResNet and DenseNet are two backbones routinely used in the literature on OOD detection. Therefore, we consider different architectures and use six models trained by cross-entropy loss. In addition, we also notice the effect of the loss function and introduce the model ResNet18 which is trained with contrastive loss.
In summary, our model zoo contains diversity derived from different architectures and different training strategies.


\begin{table}[t]
\caption{{\bf Results on CIFAR10.} Comparison with competitive OOD detection methods. The results of all competitors are from \cite{sun2022knn}. All values are percentages.  indicates smaller values are better and vice versa.}
\label{table1}
\begin{center}
\resizebox{\textwidth}{!}{\begin{tabular}{llllllllllllll}
\hline\hline
\multicolumn{14}{c}{\bf OOD Dataset}\\
Method &  & \multicolumn{2}{c}{\bf SVHN} & \multicolumn{2}{c}{\bf LSUN} & \multicolumn{2}{c}{\bf iSUN} &
\multicolumn{2}{c}{\bf Texture} &
\multicolumn{2}{c}{\bf Places365} & 
\multicolumn{2}{c}{\bf Average}
\\
& TPR & FPR & AUC & FPR & AUC & FPR & AUC & FPR & AUC & FPR & AUC & FPR & AUC \\
\hline
MSP & 95.00 & 59.66 & 91.25 & 45.21 & 93.80 & 54.57 & 92.12 & 66.45 & 88.50 & 62.46 & 88.64 & 57.67 & 90.86 \\
ODIN & 95.00 & 20.93 & 95.55 & 7.26 & 98.53 & 33.17 & 94.65 & 56.40 & 86.21 & 63.04 & 86.57 & 36.16 & 92.30 \\
Energy & 95.00 & 54.41 & 91.22 & 10.19 & 98.05 & 27.52 & 95.59 & 55.23 & 89.37 & 42.77 & 91.02 & 38.02 & 93.05 \\
GODIN & 95.00 & 15.51 & 96.60 & 4.90 & 99.07 & 34.03 & 94.94 & 46.91 & 89.69 & 62.63 & 87.31 & 32.80 & 93.52 \\
Mahalanobis & 95.00 & 9.24 & 97.80 & 67.73 & 73.61 & 6.02 & 98.63 & 23.21 & 92.91 & 83.50 & 69.56 & 37.94 & 86.50 \\
KNN & 95.00 & 24.53 & 95.69 & 25.29 & 95.96 & 25.55 & 95.26 & 27.57 & 94.71 & 50.90 & 89.14 & 30.77 & 94.15 \\
CSI & 95.00 & 37.38 & 94.69 & 5.88 & 98.86 & 10.36 & 98.01 & 28.85 & 94.87 & 38.31 & 93.04 & 24.16 & 95.89 \\
SSD+ & 95.00 & {\bf 1.51} & {\bf 99.68} & 6.09 & 98.48 & 33.60 & 95.16 & 12.98 & 97.70 & 28.41 & 94.72 & 16.52 & 97.15 \\
KNN+ & 95.00 & 2.42 & 99.52 & 1.78 & 99.48 & 20.06 & 96.74 & 8.09 & 98.56 & 23.02 & 95.36 & 11.07 & 97.93 \\
\hline
{\bf ZODE}-MSP & 95.04 & 52.44 & 92.86 & 15.11 & 97.62 & 30.98 & 95.63 & 43.16 & 94.68 & 43.58 & 94.55 & 37.05 &  95.07 \\
{\bf ZODE}-Energy & 95.07 & 50.05 & 92.26 & 3.12 & 99.29 & 16.03 & 97.09 & 37.34 & 95.14 & 19.52 & 96.95 & 25.21 & 96.15 \\
{\bf ZODE}-Mahalanobis & 94.99 & 18.24 & 96.30 & 6.28 & 98.48 & 7.17 & 98.55 & 3.88 & 99.12 & 72.25 & 85.93 & 21.56 & 95.68 \\
{\bf ZODE}-KNN & 94.96 & 2.12 & 99.43 & {\bf 1.50} & {\bf 99.61} & {\bf 5.48} & {\bf 98.70} & {\bf 0.16} & {\bf 99.88} & {\bf 9.91} & {\bf 97.99} & {\bf 3.83} & {\bf 99.12} \\
\hline\hline
\end{tabular}}
\end{center}
\end{table}


\begin{table}[t]
\caption{{\bf Results on CIFAR10.} We compare the ZODE-KNN detector with the single-model KNN detector.  All values are percentages.  indicates smaller values are better and vice versa.}
\label{table2}
\begin{center}
\resizebox{\textwidth}{!}{
\begin{tabular}{llllllllllllll}
\hline\hline
\multicolumn{14}{c}{\bf OOD Dataset}\\
Method & & \multicolumn{2}{c}{\bf SVHN} & \multicolumn{2}{c}{\bf LSUN} & \multicolumn{2}{c}{\bf iSUN} &
\multicolumn{2}{c}{\bf Texture} &
\multicolumn{2}{c}{\bf Places365} & 
\multicolumn{2}{c}{\bf Average}
\\
& TPR & FPR & AUC & FPR & AUC & FPR & AUC & FPR & AUC & FPR & AUC & FPR & AUC \\
\hline
ResNet18 & 95.00 & 27.97 & 95.49 & 18.50 & 96.84 & 24.68 & 95.52 & 26.74 & 94.97 & 47.95 & 90.02 & 29.17 & 94.57\\
ResNet18* & 95.00 & 2.42 &  {\bf 99.52} & 1.78 & 99.48 & 20.06 & 96.74 & 8.09 & 98.57 & 22.82 & 95.32 & 11.03 & 97.93\\
ResNet34 & 95.00 & 26.53 & 95.85 & 10.22 & 98.39 & 29.45 & 95.15 & 31.65 & 94.53 & 36.59 & 92.75 & 26.89 & 95.33\\
ResNet50 & 95.00 &  17.31 & 97.40 & 7.10 & 98.83 & 17.32 & 97.26 & 20.85 & 96.59 & 41.35 & 91.61 & 20.79 & 96.34\\
ResNet101 & 95.00 & 25.73 & 96.12 & 6.65 & 98.90 & 19.84 & 96.80 & 18.42 & 96.89 &  40.57 & 92.15 & 22.24 & 96.17\\
ResNet152 & 95.00 & 34.96 & 94.98 & 7.22 & 98.88 & 22.30 & 96.66 & 20.76 & 96.60 & 38.57 & 92.36 & 24.76 & 95.90 \\
DenseNet & 95.00 & 10.22 & 98.18 & 7.90 & 98.60 & 10.87 & 97.94 &  20.78 & 96.25 & 50.14 & 88.92 & 19.98 & 95.98\\
\hline
{\bf ZODE}-KNN & 94.96 & {\bf 2.12} &  99.43 & {\bf 1.50} & {\bf 99.61} & {\bf 5.48} & {\bf 98.70} & {\bf 0.16} & {\bf 99.88} & {\bf 9.91} & {\bf 97.99} & {\bf 3.83} & {\bf 99.12} \\
\hline\hline
\end{tabular}}
\end{center}
\end{table}


{\bf ZODE maintains TPR.} According to Section~\ref{sec31}, one of the challenges of ensembling OOD detectors is to control the true positive rate of the ID data. Theorem~\ref{Theorem1} states that if different pre-trained models learn completely different features, ZODE can keep TPR close to the target level.
In Table~\ref{table1}, we report the empirical TPR of ZODE, which is close to the target level .


{\bf ZODE-KNN achieves superior performance.} We compare our method with competitive OOD detection methods, including MSP \citep{hendrycks17baseline}, ODIN \citep{liang2018enhancing}, Energy \citep{liu2020energy}, GODIN \citep{hsu2020generalized}, Mahalanobis \citep{lee2018simple}, KNN \citep{sun2022knn}, CSI \citep{tack2020csi}, SSD+ \citep{sehwag2021ssd}, as well as KNN+ \citep{sun2022knn}. 
We cite the results of the competitors reported in \cite{sun2022knn}. 
For a fair comparison, we set  in the experiments of ZODE-KNN, which is the same as \cite{sun2022knn}. 
We can find that compared to the best baseline KNN+, ZODE-KNN reduces the FPR from  to , which significantly improves the relative detection accuracy by . Note that ZODE-KNN significantly reduces FPR when OOD samples are drawn from iSUN, Texture, and Places365. For LSUN, ZODE-KNN slightly improves the performance of KNN+.
In addition, SSD+ outperforms ZODE-KNN on SVHN. 
Overall, ZODE-KNN significantly improves the performance of existing methods on these five OOD datasets.


{\bf ZODE achieves consistent improvements.} We consider three different kinds of OOD detection scores. MSP \citep{hendrycks17baseline} is based on the probabilities, Energy \citep{liu2020energy} uses the logits, and Mahalanobis \citep{lee2018simple} and KNN \citep{sun2022knn} quantify the distance in the embedding space.
Then we compare them with the corresponding enhanced detectors: ZODE-MSP, ZODE-Energy, ZODE-Mahalanobis, and ZODE-KNN.
For ZODE-MSP, ZODE-Energy, and ZODE-Mahalanobis, we use the same settings as \cite{hendrycks17baseline} and \cite{liu2020energy}.
We find that ZODE-enhanced detectors consistently improve the performance of the corresponding baselines (Table~\ref{table1}).


\begin{figure}[t!]
  \centering
  \begin{subfigure}[b]{0.24\linewidth}
     \includegraphics[width=0.95\linewidth]{figs/cifar_resnet18_3146.jpg}
     \caption{ResNet18}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\linewidth}
    \includegraphics[width=0.95\linewidth]{figs/cifar_resnet34_89.jpg}
    \caption{ResNet34}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\linewidth}
    \includegraphics[width=0.95\linewidth]{figs/cifar_resnet50_716.jpg}
    \caption{ResNet50}
  \end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
    \includegraphics[width=0.95\linewidth]{figs/cifar_resnet101_210.jpg}
    \caption{ResNet101}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\linewidth}
    \includegraphics[width=0.95\linewidth]{figs/cifar_resnet152_601.jpg}
    \caption{ResNet152}
  \end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
    \includegraphics[width=0.95\linewidth]{figs/cifar_densenet_1478.jpg}
    \caption{DenseNet}
  \end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
    \includegraphics[width=0.95\linewidth]{figs/cifar_resnet18-supcon_385.jpg}
    \caption{ResNet18}
  \end{subfigure}
  \caption{{\bf Places365.} Example OOD images that only one single-model detector can identify.}
  \label{fig1}
\end{figure}


\begin{table*}[t]\small
\caption{{\bf Results on CIFAR10 for CIFAR100 as OOD.} The results of GRAM and MaSF are from \cite{haroush2021statistical}. We cite the results of SSD and SSD+ reported in \citep{sehwag2021ssd}.  All values are percentages.  indicates smaller values are better and vice versa.}
\label{table3}
\begin{subtable}[t]{0.49\textwidth}
\caption{Comparison with baseline methods}
\label{tab3:1}
\begin{center}
\begin{tabular}{llll}
\hline\hline
Method &  TPR & FPR & AUC
\\
\hline
GRAM & 95.00 & 51.00 & 83.30 \\
MaSF & 95.00 & 58.20 & 86.10 \\
SSD & 95.00 & 50.78 & 90.63\\
SSD+ & 95.00 & 38.50 & 93.40\\
KNN & 95.00 & 52.54 & 89.69\\
KNN+ & 95.00 & 38.83 & 92.75\\
\hline
{\bf ZODE}-KNN & 94.96 & {\bf18.29} & {\bf97.12}\\
\hline\hline
\end{tabular}
\end{center}
\end{subtable}
\begin{subtable}[t]{0.49\textwidth}
\caption{Ensembled vs Single-model}
\label{tab3:2}
\begin{center}
\begin{tabular}{lrrr}
\hline
\hline
Method &  TPR & FPR & AUC \\
\hline
ResNet18   & 95.00 & 52.24 & 89.69  \\
ResNet18*  & 95.00 & 38.83 & 92.75  \\
ResNet34   & 95.00 & 46.74 & 91.04  \\
ResNet50   & 95.00 & 47.14 & 90.64  \\
ResNet101  & 95.00 & 47.07 & 90.87  \\
ResNet152  & 95.00 & 47.72 & 90.84  \\
DenseNet   & 95.00 & 49.43 & 89.80 \\
\hline
{\bf ZODE}-KNN & 94.96 & {\bf 18.29} & {\bf 97.12} \\
\hline
\hline
\end{tabular}
\end{center}
\end{subtable}
\end{table*}

{\bf ZODE leverages the complementarity between the single-model detectors.} Table~\ref{table2} reports the results of all single-model detectors derived from our model zoo and KNN score. 
It is easy to see that the ZODE-ensembled KNN detector significantly outperforms all single-model KNN detectors on LSUN, iSUN, Texture, and Places365.
Compared with the best single-model baseline, ZODE reduces the FPR from  to , which significantly improves the relative detection accuracy by . 
Moreover, ZODE improves the performance sharply on Texture and Places365. 
This implies that the superior performance of ZODE does not fully come from any single-model detector. Therefore, our ensemble procedure works and is necessary for the improvements.

We further take Place365 as an example to illustrate that ZODE exploits the diversity of multiple pre-trained models.
At step 9 of Algorithm~\ref{algorithm1}, if  and , ,  then there is only one pre-trained model that can help to identify the test input as an OOD sample.
Figure~\ref{fig1} presents seven such images and each image corresponds to one pre-trained model in our model zoo.


{\bf Evaluations on CIFAR10 vs CIFAR100.} We consider a challenging OOD detection task that identifies OOD samples drawn from CIFAR100 when the ID data is CIFAR10.
Table~\ref{tab3:1} summarizes a detailed comparison with GRAM \citep{sastry2019zero}, MaSF \citep{haroush2021statistical}, SSD \citep{sehwag2021ssd}, and KNN \citep{sun2022knn}. 
Compared with the best baseline SSD+, ZODE reduces the FPR by , which is a relative  improvement in detection power. 
The results in Table~\ref{tab3:2} clearly show that ZODE significantly outperforms the single-model-based KNN detectors and our ensemble scheme fully leverages the complementarity between the single-model detectors.




\subsection{Evaluation on ImageNet Benchmarks}

{\bf Model zoo and implementation details.} We use five pre-trained models to build a model zoo, consisting of models with different architectures and different pre-training strategies. 
The models are as follows: ResNet50* \citep{sun2022knn}, semi-weekly supervised ResNeXt101 32x16d \citep{yalniz2019billion}, Swinv2-B256, Swinv2-B384, and Swinv2-L256 \citep{liu2021swinv2}. Significantly, resolutions of Swinv2-B256, Swinv2-B384, and Swinv2-L256 are 256x256, 256x256, and 384x384 respectively. ResNet50* is trained with SupCon loss \citep{khosla2020supervised}, which pulls points belonging to the same class together in the embedding space and separates samples from different classes.
ResNeXt101 is pre-trained on Billion-scale images associated with meta information semantically relevant to ImageNet, which achieves 84.8\% top-1 accuracy on ImageNet. The three Swinv2 models are pre-trained at higher resolution, and their top-1 accuracy on Imagenet all exceed  84\%.
In the following, we only report the results of ZODE-KNN based on the model zoo.
The hyperparameter  is taken to be , which makes the empirical TPR of ZODE-KNN close to 
We use  for ResNet50*, which is same as \cite{sun2022knn}. For the rest models, we selected  from  that minimize the FPR.

{\bf ZODE+KNN achieves superior performance.} In Table~\ref{table4}, we compare ZODE-KNN with competitive OOD detection methods, including MSP \citep{hendrycks17baseline}, ODIN \citep{liang2018enhancing}, Energy \citep{liu2020energy}, GODIN \citep{hsu2020generalized}, Mahalanobis \citep{lee2018simple}, KNN \citep{sun2022knn}, SSD+ \citep{sehwag2021ssd}, as well as KNN+ \citep{sun2022knn}. 
ZODE-KNN outperforms the best baseline KNN+ uniformly on all four OOD datasets, substantially reducing the average FPR from  to , which achieves a relative  improvement in detection power. 
Especially when test datasets are iNaturalist and Textures, ZODE-KNN reduces the relative FPR by  and  respectively, which highlights the effectiveness of ZODE.


\begin{table}[t]
\caption{{\bf Results on ImageNet.} All results of the competitors are cited from \cite{sun2022knn}. Methods reported are all based on ID data only (ImageNet-1k). All values are percentages.  indicates smaller values are better and vice versa.}
\label{table4}
\begin{center}
\resizebox{\textwidth}{!}{\begin{tabular}{llllllllllll}
\hline\hline
\multicolumn{12}{c}{\bf OOD Dataset}\\
Method &  & \multicolumn{2}{c}{\bf iNaturalist} & \multicolumn{2}{c}{\bf SUN} & \multicolumn{2}{c}{\bf Places} &
\multicolumn{2}{c}{\bf Textures} &
\multicolumn{2}{c}{\bf Average}
\\
& TPR & FPR & AUC & FPR & AUC & FPR & AUC & FPR & AUC & FPR & AUC \\
\hline
MSP & 95.00 & 54.99 & 87.74 & 70.83 & 80.86 & 73.99 & 79.76 & 68.00 & 79.61 & 66.95 & 81.99 \\
ODIN & 95.00 & 47.66 & 89.66 & 60.15 & 84.59 & 67.89 & 81.78 & 50.23 & 85.62 & 56.48 & 85.41 \\
Energy & 95.00 & 55.72 & 89.95 & 59.26 & 85.89 & 64.92 & 82.86 & 53.72 & 85.99 & 58.41 & 86.17 \\
GODIN & 95.00 & 61.91 & 85.40 & 60.83 & 85.60 & 63.70 & 83.81 & 77.85 & 73.27 & 66.07 & 82.02   \\
Mahalanobis & 95.00 & 97.00 & 52.65 & 98.50 & 42.41 & 98.40 & 41.79 & 55.80 & 85.01 & 87.43 & 55.47\\
KNN & 95.00 & 59.00 & 86.47 & 68.82 & 80.72 & 76.28 & 75.76 & 11.77 & 97.07 & 53.97 & 85.01 \\
SSD+ & 95.00 & 57.16 & 87.77 & 78.23 & 73.10 & 81.19 & 70.97 & 36.37 & 88.52 & 63.24 & 80.09 \\
KNN+ & 95.00 & 30.18 & 94.89 & 48.99 & 88.63 & 59.15 & 84.71 & 15.55 & 95.40 & 38.47 & 90.91 \\
\hline
{\bf ZODE}-KNN & 94.89 & \bf{5.01} & \bf{98.60} & \bf{48.87} & \bf{90.37} & \bf{53.96} & \bf{88.07} & \bf{4.57} & \bf{98.93} & \bf{28.10} & \bf{93.99} \\
\hline\hline
\end{tabular}}
\end{center}
\end{table}

{\bf ZODE combines the advantages of the single-model detectors.} In Table \ref{table5}, we report the performance of every single-model detector derived from our model zoo. We highlight three trends: (1) ZODE-KNN outperforms the best single-model KNN detector with a relative  improvement in FPR. 
This implies that ZODE works in the ImageNet benchmarks and the ensemble scheme of ZODE-KNN is necessary for the improvements. 
(2) ZODE combines the advantages of single-model detectors. In Table \ref{table5}, 
we can observe that ResNet50* and ResNeXt101 32x16 perform well on Textures, but underperform on iNaturalist, while the Swin models show the opposite performance. However, the ZODE-ensembled detector achieves strong and stable performance in all test datasets.
(3) ZODE leverages the complementarity between the single-model detectors.
Similar to the discussions in Figure~\ref{fig1}, we find some images in Textures that can be successfully identified as OOD samples and the detection decision depends only on one single-model detector.
Figure 1 presents five such images and each image corresponds to one pre-trained model
in our model zoo.


\begin{table}[t]
\caption{{\bf Results on ImageNet.} Comparison with single-model detectors and ZODE.  All values are percentages.  indicates smaller values are better and vice versa.}
\label{table5}
\begin{center}
\resizebox{\textwidth}{!}{\begin{tabular}{llllllllllll}
\hline\hline
\multicolumn{12}{c}{\bf OOD Dataset}\\
Method &  & \multicolumn{2}{c}{\bf iNaturalist} & \multicolumn{2}{c}{\bf SUN} & \multicolumn{2}{c}{\bf Places} &
\multicolumn{2}{c}{\bf Textures} &
\multicolumn{2}{c}{\bf Average}
\\
& TPR & FPR & AUC & FPR & AUC & FPR & AUC & FPR & AUC & FPR & AUC  \\
\hline
ResNet50* & 95.00 & 30.18 & 94.89 & 48.99 & 88.63 & 59.15 & 84.71 & 15.55 & 95.40 & 38.47 & 90.91  \\
ResNext101 32x16 & 95.00 & 15.24 & 96.78 & 56.06 & 88.60 & 61.74 & 86.29 & 26.06 & 93.53 & 39.78 & 91.30  \\
Swinv2-B256 & 95.00 & 9.11 & 97.93 & 58.16 & 88.78 & 58.66 & 87.13 & 41.24 & 89.67& 41.79 & 90.88  \\
Swinv2-B384 & 95.00 & 5.65 & 98.50 & 49.59 & 90.28 & {\bf 52.27} & {\bf 88.44} & 38.37 & 89.99 & 36.47 & 91.80  \\
Swinv2-L256 & 95.00 & 6.98 & 98.44 & 52.43 & 89.49 & 53.81 & 88.07 & 39.26 & 89.92 & 38.12 & 91.48  \\
\hline
{\bf ZODE}-KNN & 94.89 & {\bf 5.01} & {\bf 98.60} & {\bf 48.87} & {\bf 90.37} & 53.96 & 88.07 & {\bf 4.57} & {\bf 98.93} & {\bf 28.10} & {\bf 93.99} \\
\hline\hline
\end{tabular}}
\end{center}
\end{table}



\begin{figure}[t!]
  \centering
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=0.9\linewidth]{figs/imagenet_resnet50.jpg}
     \caption{ResNet50*}
  \end{subfigure}
  \begin{subfigure}[b]{0.36\linewidth}
    \includegraphics[width=0.9\linewidth]{figs/imagenet_resnet101.jpg}
    \caption{ResNeXt101}
  \end{subfigure}
  \begin{subfigure}[b]{0.26\linewidth}
    \includegraphics[width=0.9\linewidth]{figs/imagenet_swinv2_B256.jpg}
    \caption{Swinv2-B256}
  \end{subfigure}
    \begin{subfigure}[b]{0.34\linewidth}
    \includegraphics[width=0.9\linewidth]{figs/imagenet_swinv2_B384.jpg}
    \caption{Swinv2-B384}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=0.9\linewidth]{figs/imagenet_swinv2_L256.jpg}
    \caption{Swinv2-L256}
  \end{subfigure}
  \caption{ {\bf Textures.} Example OOD images that only one single-model detector can identify.}
  \label{fig2}
\end{figure}






\section{Conclusion}\label{Conclusion}

In this paper, we exploit the diversity of multiple pre-trained models in a model zoo to improve the performance of post hoc OOD detection.
We propose, ZODE, an efficient and fundamental ensemble scheme for combining multiple detection decisions.
Extensive experiments show that ZODE can effectively solve the missed detection problem of single-model detectors by exploiting the complementarity of multiple detectors.
We find that ZODE combined with the KNN detector \citep{sun2022knn} works very well.
On a wide range of OOD detection benchmarks, ZODE-KNN significantly improves the current SOTA results.






\bibliography{iclr2023_conference}
\bibliographystyle{iclr2023_conference}



\appendix

\section{Proof of Theorem~\ref{Theorem1}}\label{App:A}

{\bf Theorem.} {\it Suppose a pre-trained model zoo  is accessible and the score function is  Let  be the target TPR level for the ID data. If the test input  is an ID sample that  and  is independent of  for , then Algorithm~\ref{algo1} can identify  as an ID data with probability larger than }


{\bf Proof.} Before proving the theorem, we state a useful lemma that provides the distribution of p-values under the ID distribution.


\begin{lemma}\label{lemma2}
Suppose the test input  is an ID sample that  and the detection score  is a continuous random variable.
We write the p-value of  as
\benrr
p_0 = P\big(S(\rvx;\phi) \leq S(\rvx^{*};\phi) \big| \rvx \sim \gD_{id}\big).
\eenrr
Then  follows the uniform distribution .
\end{lemma}



{\bf Proof.} Let  be the cumulative distribution function of  with 
Then,
\benrr
p_0 = \sP\big(S(\rvx;\phi) \leq S(\rvx^{*};\phi) \big| \rvx \sim \gD_{id}\big) = F_\phi(S(\rvx^{*};\phi)).
\eenrr
By the continuity of  and Lemma 21.1 of \cite{van2000asymptotic}, we have
\benrr
\sP(p_0 < \alpha) & = & 1- \sP\big( F_\phi(S(\rvx^{*};\phi)) \geq \alpha \big) \\
    & = & 1- \sP\big( S(\rvx^{*};\phi) \geq F_\phi^{-1}(\alpha) \big) = F_\phi(F_\phi^{-1}(\alpha)) = \alpha. 
\eenrr
Hence  follows the uniform distribution .


\bbox


According to Lemma~\ref{lemma2}, for any , 
\benrr
p_i = \sP\big(S(\rvx;\phi_i) \leq S(\rvx^{*};\phi_i) \big| \rvx \sim \gD_{id}\big) \,\, \sim \,\, U[0,1],
\eenrr
and the density function of  is 

Then, the joint probability density of the ordered values  is 

We denote  and define an event :  Then we have

Next, we prove that for any  and , 

It is easy to see that Eq. (\ref{Eq8}) holds when 
Suppose Eq. (\ref{Eq8}) holds for  Then for , we have

which implies that Eq.(\ref{Eq8}) also holds for 
Hence, the proof is finished.

\bbox


\section{The power of Algorithm~\ref{algo1}}\label{App:B}

In this section, we study the FPR of Algorithm~\ref{algo1} from the asymptotic perspective, i.e.,  tends to infinity. According to Section~\ref{sec32} and Appendix~\ref{App:A}, the p-value of an ID sample follows the uniform distribution 
Also, if a pre-trained model fails to identify OOD samples, the p-value derived from the model follows the uniform distribution.
For an OOD dataset, we assume that there is a fixed proportion  of pre-trained models that can recognize the OOD data points.
We call these models active models and denote the set of active models as .
Then pre-trained models that fail to identify OOD samples belong to the set .
We have, for any ,
\benrr
\sP(p_j \leq u | \phi_j \in \gA^c) = u \quad \text{and} \quad \sP(p_j \leq u | \phi_j \in \gA) = G(u), 
\eenrr
where  is a cumulative distribution function different from that of the uniform distribution .
Therefore, the p-values of the OOD dataset are sampled from a mixture model with a cumulative distribution function:
\benrr
F(u) = (1-\pi) u + \pi G(u).
\eenrr
Let  be the number of pre-trained models that classify the OOD input as an OOD sample.
That is  and ,  with .
Table~\ref{tableA10} summarizes the OOD detection result with Algorithm~\ref{algo1}.
If , Algorithm~\ref{algo1} successfully detects an OOD sample. Next, we study the average power:
\benrr
\E\big(\frac{S}{m_1}\big) = \E\big( \frac{k \times \frac{S}{k}}{m_1}\big) = \E\big( \frac{\frac{k}{m} \times (1-\frac{V}{k})}{\frac{m_1}{m}}\big). 
\eenrr
According to \cite{chi2007performance},  converges to a positive value  as , which serves as the limit of the proportion of rejected p-values.
By Theorem 1 and its lemma in \cite{benjamini1995controlling}, 
\benrr
\E\big(1 - \frac{V}{k} \big)\geq 1- \frac{m_0}{m} \alpha = 1- (1-\pi)\alpha.
\eenrr
Therefore, if  is sufficiently large, then we have
\benrr
\E\big(\frac{S}{m_1}\big) \geq \frac{p_*(\alpha, F)(1- (1-\pi)\alpha)}{\pi} \geq \frac{1}{\pi m} = \frac{1}{m_1}.
\eenrr
This implies that if  is sufficiently large,  is greater or equal to 1 with high probability. In other words, if the number of pre-trained models is sufficiently large, Algorithm~\ref{algo1} can identify OOD samples with high probability.

\begin{table}[t]
\caption{{\bf OOD detection.}}
\label{tableA10}
\begin{center}
\begin{tabular}{cc|cc|c}
\hline\hline
 & & \multicolumn{2}{c|}{Truth} &   \\
\hline
 & & ID & OOD & \\
\hline
\multirow{2}{*}{Detect}  
 & ID & U & T & \\
  & OOD & V & S &  \\
\hline
  &  &  &  &  \\
\hline\hline
\end{tabular}
\end{center}
\end{table}






\section{Using p-value for OOD detection}\label{App:C}


In OOD detection, the p-value is a probability measure that quantifies how extreme the observed score is when the input is an ID sample \citep{cai2020real,morningstar2021density,haroush2021statistical,bergamin2022model,magesh2022multiple,kaur2022idecode}.
Given a test sample , the lower value of , the more likely  is not drawn from the training distribution. Hence, the p-value of  is the probability that  is less than   under the ID distribution: 
\benrr
\text{P-value of } \rvx^* = \sP\big( S(\rvx;\phi) \leq S(\rvx^*;\phi) \big| \rvx\sim \gD_{id}\big).
\eenrr
In practice, {\bf using the p-value is equivalent to using the hard threshold}  
We denote  as validation data sampled from the ID distribution  and sort their detection score in ascending order:
\benrr
S(\rvx_{(1)};\phi) \leq S(\rvx_{(2)};\phi) \leq \cdots \leq S(\rvx_{(n)};\phi).
\eenrr
In post hoc OOD detection, the threshold  is determined by correctly classifying  of validation data as ID samples, i.e., TPR is at least . Therefore,
\benrr
S(\rvx_{(\lfloor 0.05n \rfloor)};\phi) \leq \lambda_\phi \leq S(\rvx_{(\lfloor 0.05n \rfloor+1)};\phi),
\eenrr
where  is the floor function. 
On the other hand, the p-value of  less than  implies that 

where  is the empirical distribution of  
Therefore, when the sample size  is sufficiently large, the OOD region derived from the critical value  is the same as the OOD region determined by the p-value 


\section{Using BH procedure for ensemble}\label{App:D}


In this work, we use the Benjamini–Hochberg procedure \citep{benjamini1995controlling} to ensemble multiple detection decisions (p-values) to exploit the diversity of pre-trained models. Please see Steps 7 - 13 in Algorithm~\ref{algo1}.
Recall that  is the target TPR level and  
According to Theorem~\ref{Theorem1}, this procedure controls the true positive rate of ID data at a level greater than  by assuming independent p-values. 
\cite{benjamini2001control} points out that the procedure is also available for cases with certain types of positively related p-values.



In this section, we consider three ensemble schemes, Naive, Average, and Voting, as competitors to illustrate the superior of the BH procedure. Here `Naive' represents the naive ensemble scheme in Eq.(\ref{naive}), `Average' refers to the ensemble scheme that identifies a test input as an OOD sample if the average p-value is smaller than , and `Voting' is the majority voting that identifies a test input as an OOD sample if more than half of the p-values are less than 
.
The score function is the KNN score \citep{sun2022knn} with  and  is . The results are presented in Table \ref{tableA8} and \ref{tableA9}.
We can find that the empirical TPR of our method is well-controlled and is close to the target TPR level. The naive ensemble scheme cannot maintain the target TPR level. Therefore its low FPR is unreliable. This observation is consistent with our theoretical results in Section~\ref{sec31}. The average ensemble scheme can maintain its empirical TPR larger than  but its FPR is very large. Overall, the voting ensemble is comparable to our scheme. Its TPR on ImageNet is a bit out of control.


\begin{table}[t]
\caption{{\bf Results on CIFAR10.} Comparison with three baseline ensemble schemes. All values are percentages.  indicates smaller values are better and vice versa.}
\label{tableA8}
\begin{center}
\resizebox{\textwidth}{!}{\begin{tabular}{llllllllllllll}
\hline\hline
\multicolumn{14}{c}{\bf OOD Dataset}\\
Method &  & \multicolumn{2}{c}{\bf SVHN} & \multicolumn{2}{c}{\bf LSUN} & \multicolumn{2}{c}{\bf iSUN} &
\multicolumn{2}{c}{\bf Texture} &
\multicolumn{2}{c}{\bf Places365} & 
\multicolumn{2}{c}{\bf Average}
\\
& TPR & FPR & AUC & FPR & AUC & FPR & AUC & FPR & AUC & FPR & AUC & FPR & AUC \\
\hline
Naive & 69.89 & 0.16 & 99.18 & 0.12 & 99.39 & 0.53 & 98.21 & 0.00 & 99.74 & 0.14 & 97.36 & 0.95 & 98.78 \\
Average & 100.00 & 19.21 & 99.98 & 6.92 & 100.00 & 20.04 & 99.98 & 23.69 & 100.00 & 69.76 & 99.97 & 27.92 & 99.98 \\
Voting & 99.64 & 5.43 & 99.82 & 2.46 & 99.95 & 6.68 & 99.82 & 0.39 & 99.99 & 8.99 & 99.86 & 4.79 & 99.89 \\
BH & 94.96 & 2.12 & 99.43 & 1.50 & 99.61 & 5.48 & 98.70 & 0.16 & 99.88 & 9.91 & 97.99 & 3.83 & 99.12 \\
\hline\hline
\end{tabular}}
\end{center}
\end{table}



\begin{table}[t]
\caption{{\bf Results on ImageNet.} Comparison with three baseline ensemble schemes. All values are percentages.  indicates smaller values are better and vice versa.}
\label{tableA9}
\begin{center}
\resizebox{\textwidth}{!}{\begin{tabular}{llllllllllll}
\hline\hline
\multicolumn{12}{c}{\bf OOD Dataset}\\
Method &  & \multicolumn{2}{c}{\bf iNaturalist} & \multicolumn{2}{c}{\bf SUN} & \multicolumn{2}{c}{\bf Places} &
\multicolumn{2}{c}{\bf Textures} &
\multicolumn{2}{c}{\bf Average}
\\
& TPR & FPR & AUC & FPR & AUC & FPR & AUC & FPR & AUC & FPR & AUC  \\
\hline
Naive & 88.53 & 2.00 & 98.54 & 29.53 & 89.60 & 36.72 & 86.93 & 1.45 & 99.00 & 17.43 & 93.52  \\
Average & 97.41 & 11.64 & 98.57 & 58.21 & 91.43 & 61.40 & 89.13 & 44.77 & 93.55 & 44.05 & 93.15  \\
Voting & 93.59 & 3.99 & 98.66 & 43.16 & 90.26 & 48.18 & 87.98 & 11.74 & 97.14 & 26.76 & 93.51  \\
BH & 94.89 & 5.01 & 98.60 & 48.87 & 90.37 & 53.96 & 88.07 & 4.57 & 98.93 & 28.10 & 93.99 \\
\hline\hline
\end{tabular}}
\end{center}
\end{table}


\section{Influence of hyperparameter}



In Secction~\ref{Experiments},  we find that the ZODE-KNN detector works very well and significantly improves the best baseline method on a wide range of OOD detection benchmarks.
The KNN detector \citep{sun2022knn} uses the distance between the test input and the -th nearest ID sample as the detection score.
Here  is a hyperparameter that needs to be predetermined.
In this section, we use CIFAR10 to investigate the effect of the choice of  on ZODE.
We consider  and , and report the results in Table~\ref{tableA6}. 
One can find that the choice of k affects the performance of ZODE-KNN.
But the influence is not significant compared to the improvements of ZODE-KNN. Both ZODE-KNN(k=1) and ZODE-KNN(k=50) significantly outperform the best baseline methods.
In Table~\ref{tableA7}, we report the detailed comparison between the ZODE-ensembled KNN detector and the single-model KNN detectors derived from our model zoo.
We observe a similar phenomenon in that the choice of k affects the performance of both ensembled and single-model detectors, while the influence is not significant compared to the improvement caused by the ensemble scheme.


\begin{table}[t]
\caption{{\bf Results on CIFAR10.} Comparison with competitive OOD detection methods. The results of all competitors are from \cite{sun2022knn}. All values are percentages.  indicates smaller values are better and vice versa.}
\label{tableA6}
\begin{center}
\resizebox{\textwidth}{!}{\begin{tabular}{llllllllllllll}
\hline\hline
\multicolumn{14}{c}{\bf OOD Dataset}\\
Method &  & \multicolumn{2}{c}{\bf SVHN} & \multicolumn{2}{c}{\bf LSUN} & \multicolumn{2}{c}{\bf iSUN} &
\multicolumn{2}{c}{\bf Texture} &
\multicolumn{2}{c}{\bf Places365} & 
\multicolumn{2}{c}{\bf Average}
\\
& TPR & FPR & AUC & FPR & AUC & FPR & AUC & FPR & AUC & FPR & AUC & FPR & AUC \\
\hline
MSP & 95.00 & 59.66 & 91.25 & 45.21 & 93.80 & 54.57 & 92.12 & 66.45 & 88.50 & 62.46 & 88.64 & 57.67 & 90.86 \\
ODIN & 95.00 & 20.93 & 95.55 & 7.26 & 98.53 & 33.17 & 94.65 & 56.40 & 86.21 & 63.04 & 86.57 & 36.16 & 92.30 \\
Energy & 95.00 & 54.41 & 91.22 & 10.19 & 98.05 & 27.52 & 95.59 & 55.23 & 89.37 & 42.77 & 91.02 & 38.02 & 93.05 \\
GODIN & 95.00 & 15.51 & 96.60 & 4.90 & 99.07 & 34.03 & 94.94 & 46.91 & 89.69 & 62.63 & 87.31 & 32.80 & 93.52 \\
Mahalanobis & 95.00 & 9.24 & 97.80 & 67.73 & 73.61 & 6.02 & 98.63 & 23.21 & 92.91 & 83.50 & 69.56 & 37.94 & 86.50 \\
KNN & 95.00 & 24.53 & 95.69 & 25.29 & 95.96 & 25.55 & 95.26 & 27.57 & 94.71 & 50.90 & 89.14 & 30.77 & 94.15 \\
CSI & 95.00 & 37.38 & 94.69 & 5.88 & 98.86 & 10.36 & 98.01 & 28.85 & 94.87 & 38.31 & 93.04 & 24.16 & 95.89 \\
SSD+ & 95.00 & {\bf 1.51} & {\bf 99.68} & 6.09 & 98.48 & 33.60 & 95.16 & 12.98 & 97.70 & 28.41 & 94.72 & 16.52 & 97.15 \\
KNN+ & 95.00 & 2.42 & 99.52 & 1.78 & 99.48 & 20.06 & 96.74 & 8.09 & 98.56 & 23.02 & 95.36 & 11.07 & 97.93 \\
\hline
{\bf ZODE}-KNN(k=1) & 95.00 & 2.60 & 99.42 & 2.34 & 99.46 & 7.29 & 98.47 & 0.44 & 99.73 & 11.79 & 97.77 & 4.89 & 98.97 \\
{\bf ZODE}-KNN(k=50) & 94.96 & 2.12 & 99.43 & {\bf 1.50} & {\bf 99.61} & {\bf 5.48} & {\bf 98.70} & {\bf 0.16} & {\bf 99.88} & {\bf 9.91} & {\bf 97.99} & {\bf 3.83} & {\bf 99.12} \\
\hline\hline
\end{tabular}}
\end{center}
\end{table}


\begin{table}[t]
\caption{{\bf Results on CIFAR10}. Comparison with single-model detectors at different  levels. Setting  and  respectively, we compare the performance with single-model detectors and ZODE.}
\label{tableA7}
\begin{center}
\resizebox{\textwidth}{!}{
\begin{tabular}{lllllllllllllll}
\hline\hline
\multicolumn{15}{c}{\bf OOD Dataset}\\
K & Method & & \multicolumn{2}{c}{\bf SVHN} & \multicolumn{2}{c}{\bf LSUN} & \multicolumn{2}{c}{\bf iSUN} &
\multicolumn{2}{c}{\bf Texture} &
\multicolumn{2}{c}{\bf Places365} &
\multicolumn{2}{c}{\bf Average}
\\
& & TPR & FPR & AUC & FPR & AUC & FPR & AUC & FPR & AUC & FPR & AUC & FPR & AUC \\
\hline
\multirow{8}{*}{} 
& ResNet18 & 95.00 &  50.34 & 91.45 & 25.44 & 95.67 & 30.53 & 94.71 & 33.44 & 93.69 & 48.40 & 90.05 &  37.63 & 93.11  \\
& ResNet18* & 95.00 & {\bf 2.16} & {\bf 99.56} & 4.45 & 98.78 & 21.99 & 96.56 & 9.88 & 98.34 & 23.87 & 95.38 &  12.47 & 97.72\\
& ResNet34 & 95.00 & 28.09 & 95.55 & 11.80 & 98.13 & 30.90 & 94.97 & 32.19 & 94.24 & 38.67 & 92.41 & 28.19 & 95.06 \\
& ResNet50 & 95.00 &  22.37 & 96.25 & 7.15 & 98.67 & 18.40 & 96.90 & 23.63 & 95.72 & 41.00 & 91.64 & 22.51 & 95.84\\
& ResNet101 & 95.00 & 25.45 & 95.96 & 8.46 & 98.62 & 20.92 & 96.54 & 21.03 & 96.31 &  41.07 & 92.18 & 23.39 & 95.92\\
& ResNet152 & 95.00 & 33.79 & 94.16 & 8.48 & 98.63 & 19.06 & 96.93 & 20.62 & 96.50 & 37.97 & 92.52 & 23.98 & 95.75 \\
& DensNet & 95.00 & 13.46 & 97.76 & 9.41 & 98.31 & 13.70 & 97.58 &  24.29 & 95.46 & 51.49 & 89.07 & 22.47 & 95.64\\
& {\bf ZODE}-KNN & 95.00 & 2.60 & 99.42 & {\bf 2.34} & {\bf 99.46} & {\bf 7.29} & {\bf 98.47} & {\bf 0.44} & {\bf 99.73} & {\bf 11.79} & {\bf 97.77} & {\bf 4.89} & {\bf 98.97} \\
\hline
\multirow{8}{*}{}  
& ResNet18 & 95.00 & 27.97 & 95.49 & 18.50 & 96.84 & 24.68 & 95.52 & 26.74 & 94.97 & 47.95 & 90.02 & 29.17 & 94.57\\
& ResNet18* & 95.00 & 2.42 &  {\bf 99.52} & 1.78 & 99.48 & 20.06 & 96.74 & 8.09 & 98.57 & 22.82 & 95.32 &  11.03 & 97.93\\
& ResNet34 & 95.00 & 26.53 & 95.85 & 10.22 & 98.39 & 29.45 & 95.15 & 31.65 & 94.53 & 36.59 & 92.75 & 26.89 & 95.33 \\
& ResNet50 & 95.00 &  17.31 & 97.40 & 7.10 & 98.83 & 17.32 & 97.26 & 20.85 & 96.59 & 41.35 & 91.61 & 20.79 & 96.34\\
& ResNet101 & 95.00 & 25.73 & 96.12 & 6.65 & 98.90 & 19.84 & 96.80 & 18.42 & 96.89 &  40.57 & 92.15 & 22.24 & 96.17\\
& ResNet152 & 95.00 & 34.96 & 94.98 & 7.22 & 98.88 & 22.30 & 96.66 & 20.76 & 96.60 & 38.57 & 92.36 & 24.76 & 95.90 \\
& DensNet & 95.00 & 10.22 & 98.18 & 7.90 & 98.60 & 10.87 & 97.94 &  20.78 & 96.25 & 50.14 & 88.92 & 19.98 & 95.98\\
& {\bf ZODE}-KNN & 94.96 & {\bf 2.12} &  99.43 & {\bf 1.50} & {\bf 99.61} & {\bf 5.48} & {\bf 98.70} & {\bf 0.16} & {\bf 99.88} & {\bf 9.91} & {\bf 97.99} & {\bf 3.83} & {\bf 99.12} \\
\hline\hline
\end{tabular}}
\end{center}
\end{table}






\section{More Related work}\label{Related work}

\textbf{Score function for OOD detection.} Many recent works have proposed to use models pre-trained with multi-class classification tasks to do OOD detection.
A natural idea to distinguish OOD samples is to use the softmax confidence score based on the classifier output and a normalized probability vector.
However, neural networks might give an OOD sample a high confidence prediction \citep{guo2017calibration,hein2019relu}.
To deal with this issue, several studies have improved scoring methods and also proposed new scoring measures, such as OpenMax score \citep{bendale2015towards}, maximum softmax probability \citep{hendrycks17baseline}, ODIN score \citep{liang2018enhancing}, deep ensembles \citep{lakshminarayanan2017simple}, Mahalanobis distance-based score \citep{lee2018simple}, Gram matrix \citep{sastry2019zero}, energy score \citep{liu2020energy}, \citep{lin2021mood}, activation rectification (ReAct) \citep{sun2021react}, gradient-based score (GradNorm) \citep{huang2021importance}, and ViM score \citep{wang2022vim}.
In this work, we consider KNN score \cite{sun2022knn} that employs the non-parametric nearest neighbor approach to quantify the distance between the test input and the training data.    
 


\textbf{ Model-training strategies for OOD detection.} Besides proposing efficient score functions, another area of research focuses on choosing strategies for training deep models, including generative adversarial networks \citep{schlegl2017unsupervised}, convolutional network \citep{sabokrou2018deep} and transfer representation-learning \citep{andrews2016transfer}. 
These classical neural network architectures, such as ResNet \citep{he2016deep}, DenseNet \citep{huang2017densely} and Swin \citep{liu2021swin} etc, show different effects in downstream tasks when trained with different training strategies or regularization methods \citep{ericsson2021well}.
For example, utilizing auxiliary OOD training data like artificially synthesized data from GANs \citep{lee2017training} and unlabeled data for outlier exposure \citep{hendrycks2018deep} can explicitly regularize the model. 
Another method to fine-tune the model is to modify the loss function or use auxiliary objectives. Some loss functions encourage the predictive distribution of OOD sample toward uniform distribution \citep{lee2017training,hendrycks2018deep}, and also some by adding contrastive loss \citep{winkens2020contrastive}, margin loss 
 \citep{vyas2018out} or objective function of adversarial learning \citep{biggio2018wild,miller2020adversarial,chalapathy2019deep}
can force models to learn more high-level, task-agnostic, comprehensive features from the training dataset, to enable it robust enough for downstream tasks with various distribution shifts. These works either modify the neural network and objective function or require additional data, at the cost of computational cost.



\end{document}
