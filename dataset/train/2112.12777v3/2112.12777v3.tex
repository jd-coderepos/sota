\appendix
\begin{center}
\textbf{\Large Appendix}
\end{center}

In this appendix, we provide additional information and ablation studies
relating to \methodName.
We begin by providing more details about the datasets used
for each task (Sec.~\ref{sec:dataset-detils}).
We then provide ablation studies that investigate:
(1) The influence of the  hyperparameter on the proposed
DIS normalisation scheme (Sec.~\ref{sec:topk-supp});
(2) Whether effective querybanks can also be constructed from the
training set using IS normalisation,
rather than DIS normalisation (Sec.~\ref{sec:effective-is});
(3) How embedding dimensionality influences the effectiveness
of \methodName (Sec.~\ref{sec:embd-effect}).
Next, we present comparisons on additional datasets for the text-video retrieval task (Sec.~\ref{additional}). 
In Sec.~\ref{sec:complexity} we discuss the
complexity of each normalisation technique.
In Sec.~\ref{sec:cent} we present a comparison
with CENT~\cite{suzuki2013centering} normalisation.
Then, we provide details on the \textit{skewness}
metric reported in the submission (Sec.~\ref{sec:skewness}),
offer a more complete set of metrics
across ablations (Sec.~\ref{sec:other-metrics})
and
give more details about the text and video experts used in
this work (Sec.~\ref{sec:experts}).
Finally, we report metrics indicating how \methodName performs on
video-text retrieval (Sec.~\ref{sec:t2v})
and provide some additional qualitative
results (Sec.~\ref{sec:qualitative}).


\section{Dataset details}
\label{sec:dataset-detils}
In this section, we describe the splits and datasets employed for all tasks considered in this work.

\subsection{Text-video retrieval}
For the task of text-video retrieval we test our approach on seven current benchmarks.

\indent \textbf{MSR-VTT}~\cite{xu2016msr} contains around
10k videos, each having 20 captions.
For the task of text-video retrieval,
we follow prior works~\cite{liu2019use, croitoru2021teachtext}
and we report results on the official split (\texttt{full}) which
contains 2,990 videos for testing and 497 for validation.
Since a number of recent works
\cite{liu2019use,gabeur2020multi,patrick2020support,croitoru2021teachtext} 
also report results on the 1k-A split,
we compare against these method on this split as well.
The 1k-A split contains 1,000 videos for testing
and around 9,000 for training.
We use the same videos and captions as defined in~\cite{liu2019use} which are used by other works~\cite{yu2018joint,gabeur2020multi,patrick2020support} for evaluation.
We report the results using models trained for 100 epochs.

\indent \textbf{MSVD}~\cite{chen2011collecting} has
1,970 videos and around 80k captions.
We report results on the standard split using in prior
works~\cite{venugopalan2015sequence,xu2015jointly,liu2019use,croitoru2021teachtext}
which consists of 1,200 videos for training, 100 for validation and 670 for testing. 

\indent \textbf{DiDeMo}~\cite{anne2017localizing} has 10,464 videos.
They are collected from a large-scale creative commons
collection~\cite{thomee2016yfcc100m} and are varied in content
(concerts, sports, pets etc.).
For each video, there are 3-5 pairs of descriptions.
For the task of text-video retrieval,
we use the paragraph video retrieval protocol as defined in
prior works~\cite{zhang2018cross,liu2019use,croitoru2021teachtext}.
This means that we the split consisting of 8,392 for training,
1,065 validation and 1,004 test videos.

\indent \textbf{LSMDC}~\cite{rohrbach2017movie}
contains 118,081 short video clips extracted from 202 movies.
Each clip has a textual description which consist in a caption
which is extracted either from the movie script or transcribed
from descriptive video services (DVS) for the visually impaired.
We use the official splits as defined in the Large Scale Movie
Description Challenge (LSMDC). The testing split contains 1,000 videos.

\indent \textbf{VaTeX}~\cite{wang2019vatex} contains 3,4911 videos and
has multilingual captions in Chinese and English.
Each video has 10 captions for each language.
As for the other datasets, we follow the same protocol as defined
in prior works~\cite{chen2020fine,patrick2020support,croitoru2021teachtext}
and use 1,500 videos for testing, while there are 1,500 videos for validation.
Please note that in this work, we use only the English annotations.

\indent \textbf{QuerYD}~\cite{oncescu20queryd} has 1,815 videos for training,
388 for validation and 390 for testing.
The videos are extracted from YouTube and are varied in content.
The dataset has 31,441 textual descriptions.
13,019 of these are precisely localized in the video with start time and end
time annotations while the other 18,422 are coarsely localized.
In this work, we do not use the localization annotations and report results
on the official splits following prior work on
text-video retrieval~\cite{croitoru2021teachtext}.

\indent \textbf{ActivityNet}~\cite{caba2015activitynet} contains
20k videos and has around 100K descriptive sentences.
The videos are extracted from YouTube.
We use a paragraph video retrieval as defined in
prior works~\cite{zhang2018cross, liu2019use, croitoru2021teachtext}.
We report results on the \texttt{val1} split.
The training split consists of 10,009 videos, while there are 4,917
videos for testing.

\subsection{Text-image retrieval}

For text image retrieval, we report results on the \textbf{MSCoCo}~\cite{Chen2015MicrosoftCC} dataset. It consists of 123k images with 5 captions for each sentence. We report results for the 5k test split.

\subsection{Text-audio retrieval}

For text audio retrieval,
we report results on the \textbf{AudioCaps}~\cite{Kim2019AudioCapsGC} dataset
which comprises sounds with event descriptions.
We use the same setup as prior work~\cite{oncescu2021audio}
where 49,291 samples are used for training, 428 for validation and 816 for testing.

\subsection{Image-to-image retrieval}

\indent \textbf{CUB-200-2011}~\cite{Wah2011TheCB} contains
11,788 images with 200 classes.
The training split consist of the first 100 classes
(5,863 images) while the testing split contains the remaining
classes (5,924 images).
We use the same setup as used in prior work~\cite{roth2020revisiting}.

\indent \textbf{Stanford Online Products}~\cite{Song2016DeepML}
contains 120,053 images with products from 22,634 classes.
We use the provided train and test splits containing 59,551
and 60,502 images respectively,
as used in prior works~\cite{Song2016DeepML, roth2020revisiting}.

\section{The influence of the Top-k hyperparameter on DIS normalisation}
\label{sec:topk-supp}
\begin{table}
\begin{center}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|c|c|c|c|c}\hline \hline Querybank Source Data & Topk &  &   &  &  \\\hline \textbf{No querybank} & - &&&&\\\hline
\hline
\textit{In Domain} \\
\hline
\textbf{MSR-VTT}  & 1
&&&&\\\textbf{MSR-VTT}  & 2
&&&&\\\textbf{MSR-VTT}  & 3
&&&&\\\textbf{MSR-VTT}  & 5
&&&&\\\textbf{MSR-VTT}  & 10 &&&&\\\hline
\hline
\textit{Far Domain} \\
\hline
\textbf{LSMDC} & 1
&&&&\\\textbf{LSMDC} & 2
&&&&\\\textbf{LSMDC} & 3
&&&&\\\textbf{LSMDC} & 5
&&&&\\\textbf{LSMDC} & 10
&&&&\\\hline
\end{tabular}}
\end{center}
\vspace{\spacebefore{}}
\caption{\textbf{
The influence of the  hyperparameter on DIS normalisation.} 
Performance is reported on MSR-VTT \texttt{full} split~\cite{xu2016msr},
while querybanks of 5,000 samples
are sampled from the training sets of different datasets.
We observe that for \textit{Far Domain} querybanks,
 performs the best,
while retaining good performance for \textit{In Domain} querybanks.
\label{tab:topk-activations}}
\vspace{\spaceafter{}}
\end{table} In Tab.~\ref{tab:topk-activations} we show the influence of  in the Top-k selection employed when constructing the gallery activation set (introduced in Sec.~3.4 of the main paper).
We observe that choosing  offers a 
good trade-off between good performance when
constructing \textit{In Domain} querybanks
and robustness when constructing
\textit{Far Domain} querybanks.
We therefore use  for all reported experiments.

\section{Can effective querybanks can be constructed from the training set with IS normalisation?}
\label{sec:effective-is}
\begin{table}
\begin{center}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|c|c|c|c|c}\hline \hline Querybank Source&Size&&&&\\\hline \textbf{No querybank} & - &&&&\\\textbf{Training set} & 60k &&&&\\

\textbf{Val set} & 10k
&&&&\\

\textbf{Test set} & 60k &&&&\\\hline \end{tabular}}
\end{center}
\vspace{\spacebefore{}}
\caption{\textbf{Effective querybanks can be constructed
from the training set.}
Performance is reported on MSR-VTT
\texttt{full} split~\cite{xu2016msr} using IS normalisation.
We observe that a querybank of 60K samples from the training
set performs comparably to a test set querybank.
\label{tab:querybank-train-vs-test-is}}
\vspace{\spaceafter{}}
\end{table} In the main submission, we showed that effective querybanks can be constructed
from the training set when employing DIS normalisation.
Here, we show that this property also applies to IS normalisation,
supporting our hypothesis that \methodNameLong has the general property of not requiring
concurrent access to multiple test queries for appropriate normalisation
strategies.
In Tab.~\ref{tab:querybank-train-vs-test-is} we report the results of
selecting queries from training, validation or testing split to form
the querybank when employing IS normalisation.
Similarly to DIS, we observe that training set querybanks perform
comparably to test set querybanks for IS normalisation.

\section{The influence of embedding dimensionality on the effectiveness of \methodName}
\label{sec:embd-effect}

\begin{figure}
    \centering
    \includegraphics[trim={0cm 0.5cm 0cm 0.2cm},clip,width=\linewidth]{images/embedding_study.png}
    \caption{
    \textit{(Left)}:
    \textbf{The influence of embedding dimension on \methodName effectiveness}.
    We observe that \methodName brings a large increase in performance in all cases
    \textit{(Right)}:
    \textbf{The influence of number of used video embeddings on \methodName effectiveness.}
    We observe that our method is more effective with an increased number of modalities.
    }
    \mbox{}\vspace{-0.7cm} \\
    \label{fig:embedding-size}
\end{figure}

Radovanovic et al.~\cite{radovanovic2010hubs} posit that hubness
is a phenomenon that is:
(i) inherent to high dimensional spaces;
(ii) heavily influenced by the
\textit{intrinsic dimensionality} of the data.
To investigate these perspectives, we study the improvement
yielded by \methodName over embeddings of different dimensionality,
reporting results in Fig.~\ref{fig:embedding-size} (left).
We observe that \methodName brings around the same gain
when changing the embedding size.
We can interpret this finding within the framework
of~\cite{radovanovic2010hubs} as making the statement
that changing the shared embedding dimensionality
\textit{does not} influence intrinsic dimensionality.
To provide further analysis, 
we make a crude approximation to increasing/decreasing
intrinsic dimensionality by increasing/decreasing the
number of modalities employed in the video embedding.
Intuitively, since audio provides a different ``view'' of a sample
to visual data,
we expect a joint embedding with access to more modalities
to exhibit higher intrinsic dimensionality than one with only visual
cues.
We plot the effect of these changes
in Fig.~\ref{fig:embedding-size} (right).
We observe a slight increase in performance gain when
applying \methodName with an increased number of modalities,
which accords with the Radovanovic~\cite{radovanovic2010hubs} hypothesis.

\begin{figure*}
    \centering
    \includegraphics[trim={0cm 0.5cm 0cm 0.2cm},clip,width=\linewidth]{images/supp/hubness_retrieval_supp.png}
    \vspace{-0.3cm}
    \caption{\textbf{Distribution of number of times each video is retrieved before and after applying \methodName.} We observe that \methodName reduces the maximum number of retrievals for any individual video. Furthermore, we note that with \methodName, previously unretrieved videos become possible to retrieve.}
    \mbox{}\vspace{-0.7cm} \\
    \label{fig:hubness-before-after}
\end{figure*}

\section{Additional text-video retrieval results}
\label{additional}
In Tab.~\ref{tab:msrvtt-final-sota},~\ref{tab:activity-net-final-sota} we report additional comparisons with state of the art on the MSR-VTT \texttt{full} split as well as ActivityNet~\cite{caba2015activitynet}.
In both cases, we observe that \methodName yields improvements.
We also explore the use of \methodName with CLIP4Clip~\cite{luo2021clip4clip}---for this, we train models using the code made available by the authors.
For CLIP4Clip experiments, we use a  value of  with the exception of LSMDC where  is .
\begin{table}
\begin{center}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|c|c|c|c}\hline \hline Model&&&&\\\hline Dual\cite{dong2019dual}&&&&\\HGR\cite{chen2020fine}&&&&\\MoEE\cite{miech2018learning}&&&&\\CE\cite{liu2019use}&&&&\\\hline CE+\cite{croitoru2021teachtext}&&&&\\\textbf{CE+ (+\methodName)}&&&&\\\hline
TT-CE+\cite{croitoru2021teachtext}&&&&\\\textbf{TT-CE+ (+\methodName)}&&&&\\\hline
CLIP4Clip\cite{luo2021clip4clip}&&&&\\
\textbf{CLIP4Clip (+QB-Norm)}&&&&\\
\hline \end{tabular}}
\end{center}
\vspace{\spacebefore{}}
\caption{\textbf{MSR-VTT full split: comparison to state of the art.}\\ denotes results obtained training using the official code.} \label{tab:msrvtt-final-sota}
\vspace{\spaceafter{}}
\end{table}

\begin{table}
\begin{center}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|c|c|c|c}\hline \hline Model&&&&\\\hline MoEE\cite{miech2018learning}&&&&\\CE\cite{liu2019use}&&&&\\TT-CE&&&&\\Frozen\cite{bain2021frozen}&&&&\\CLIP4Clip~\cite{luo2021clip4clip}&&&&\\
\hline CE+\cite{croitoru2021teachtext}&&&&\\\textbf{CE+ (+\methodName)}&&&&\\\hline
TT-CE+\cite{croitoru2021teachtext}&&&&\\\textbf{TT-CE+ (+\methodName)}&&&&\\\hline
CLIP4Clip~\cite{luo2021clip4clip}&&&&\\\textbf{CLIP4Clip (+\methodName)}&&&&\\\hline
\end{tabular}}
\end{center}
\vspace{\spacebefore{}}
\caption{\textbf{DiDeMo: Comparison to state of the art methods}.\\ denotes results obtained training using the official code. 
\label{tab:didemo-final-sota}}
\vspace{\spaceafter{}}
\end{table}

\begin{table}
\begin{center}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|c|c|c|c}\hline \hline Model&&&&\\\hline MoEE\cite{miech2018learning}&&&&\\CE\cite{liu2019use}&&&&\\MMT\cite{gabeur2020multi}&&&&\\Frozen\cite{bain2021frozen}&&&&\\CLIP4Clip~\cite{luo2021clip4clip}&&&&\\
\hline CE+\cite{croitoru2021teachtext}&&&&\\\textbf{CE+ (\methodName)}&&&&\\\hline
TT-CE+\cite{croitoru2021teachtext}&&&&\\\textbf{TT-CE+ (\methodName)}&&&&\\\hline CLIP4Clip~\cite{luo2021clip4clip}&&&&\\\textbf{CLIP4Clip (+\methodName)}&&&&\\\hline \end{tabular}}
\end{center}
\vspace{\spacebefore{}}
\caption{\textbf{LSMDC: Comparison to state of the art methods}.\\ denotes results obtained training using the official code.
\label{tab:lsmdc-final-sota}}
\vspace{\spaceafter{}}
\end{table}

\begin{table}
\begin{center}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|c|c|c|c}\hline \hline Model&&&&\\\hline MoEE\cite{miech2018learning}&&&&\\CE\cite{liu2019use}&&&&\\HSE\cite{zhang2018cross}&&&&\\MMT\cite{gabeur2020multi}&&&&\\SSB\cite{patrick2020support}&&&&\\CLIP4Clip\cite{luo2021clip4clip}&&&&\\
\hline
TT-CE+\cite{croitoru2021teachtext}&&&&\\\textbf{TT-CE+ (+\methodName)}&&&&\\\hline CLIP4Clip~\cite{luo2021clip4clip}&&&&\\
\textbf{CLIP4Clip (+QB-Norm)}&&&&\\
\hline
\end{tabular}}
\end{center}
\vspace{\spacebefore{}}
\caption{\textbf{ActivityNet: Comparison to state of the art methods}.\\ denotes results obtained training using the official code. \label{tab:activity-net-final-sota}}
\vspace{\spaceafter{}}
\end{table} 

\section{The computational complexity of normalisation strategies}
\label{sec:complexity}
As discussed in the main paper in Sec.3.4,
we use various normalization techniques in conjunction with \methodName.
In this section, we describe the computational cost of each technique
in the context of its influence on inference time.
For clarity of exposition,
we consider exact similarity searches,
but note that in practice approximate nearest neighbour implementations
are employed for large-scale deployments~\cite{johnson2019billion}.
All strategies incur an initial cost that corresponds to
pre-computing the similarity between a test query and
all the videos from the gallery,
,
where  represents the number of videos in the gallery.
We further assume that we have pre-computed and
stored all similarities between each query in the
querybank and videos from the gallery.
This assumption incurs both computational
and storage costs of
, where  represents the number of
queries in the querybank.

\indent \textit{Globally-Corrected (GC) retrieval}~\cite{dinu2014improving}
involves determining the rank of the test query with respect
to the querybank for each gallery item.
Since we assume that we have pre-computed similarities between
the querybank and the gallery, we also pre-compute an initial
ranking over querybank elements for each gallery item.
For each test query, we establish its rank amongst the querybank
for every target item by performing a binary search over the
sorted list of pre-computed similarities.
This incurs an inference time cost of .

\indent \textit{Cross-Domain Similarity Local
Scaling (CSLS)}~\cite{conneau2018word}
consists of finding the most similar queries
from the querybank for each gallery video
and finding the  gallery videos
(here  is a hyperparameter of CSLS)
that are most similar to the test query.
For the former, we can pre-compute, for each video in the gallery,
the  most similar queries from the querybank and store the
average similarity into a vector of size .
For the latter,
we must compute (during inference)
the average similarity of the  most similar
items among the gallery to our test query.
Using \textit{quickselect}, this can be done in  time on
average (note that we do not require the top  element similarities
to be sorted, since they will be averaged).


\indent \textit{Inverted Softmax (IS)}~\cite{smith2017offline}
involves normalizing the final similarity by the sum of the
similarities given the querybank.
However, the softmax denominator can be pre-computed by summing
the querybank similarities for each gallery item and storing the results
into a vector of size .
During inference the similarities are divided by this pre-computed sum,
which adds only constant-time overhead.
Pre-computing the sum in this manner
also reduces the storage cost associated with the querybank
from  to 
(since we can discard the memory allocated to store the similarities
between each query in the querybank and each video in the gallery).

\indent \textit{Dynamic Inverted Softmax} (DIS).
Since DIS involves applying IS dynamically,
the computation of the normalization for each test query
is done in constant time as described above for IS.
The additional gallery activation set employed by DIS
can be pre-computed and stored for an additional
 storage cost.
There is an additional cost during inference:
the top- search to determine the video originally retrieved
by the test query (which determines whether normalisation is performed).
This can be done in linear time ().

\section{Comparison to CENT}
\label{sec:cent}
\begin{table}
\begin{center}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|c|c|c|c}\hline \hline Model&&&&\\\hline Baseline&&&&\\CENT~\cite{suzuki2013centering}& &  &  &  \\DIS& &  &  &  \\\hline \end{tabular}}
\end{center}
\vspace{\spacebefore{}}
\caption{\textbf{MSR-VTT full split} Comparison with CENT for a seed of TT-CE+\cite{croitoru2021teachtext} model. \label{tab:msrvtt-cent}}
\vspace{\spaceafter{}}
\end{table} 
In Tab.~\ref{tab:msrvtt-cent}
we show how CENT~\cite{suzuki2013centering} normalisation performs
in comparison to an unnormalised baseline and \methodNameLong with DIS.
Since we found CENT to consistently harm performance for cross-modal
retrieval, we did not include it in all experiments in the main paper.

\section{Hubness and Skewness}
\label{sec:skewness}
We use the skewness metric as defined in~\cite{radovanovic2010hubs} to measure hubness:


where  and  are the mean and standard deviation of .  represents the k-occurrence distribution and is defined as follows  where 


Here  represents a video embedding and  a set of queries.
To compute these statistics,
In practice, we use the we use , following~\cite{Feldbauer2018FastAH}
for the k-occurences distribution, employing the implementation of~\cite{Feldbauer2020}.


As shown in Tab. 3 in the main paper, skewness and hence hubness is reduced after applying \methodName. The same can be seen in Fig.~\ref{fig:hubness-before-after} which depicts the distribution of number of times each video is retrieved before and after using \methodName.
We observe that the maximum number of times a video
is retrieved is reduced, indicating a hubness reduction.

\section{Additional ablations on other metrics}
\label{sec:other-metrics}
In the main paper,
to maintain conciseness we report ablation plots for the influence of querybank size
and inverse temperature  using the geometric mean of R1, R5 and R10.
For completeness, in this section we show results on each metric individually.
As seen in Fig.~\ref{fig:num-queries-and-hyperparam-r1},
the individual metrics reflect the trend shown for the geometric means,
aligning with the results shown in the main paper.

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.5\textwidth}
        \includegraphics[trim={0cm 0.5cm 0cm 0.2cm},clip,width=0.95\linewidth]{images/supp/no_queries_vs_perf_r1.png}
    \end{subfigure}
    \begin{subfigure}[b]{0.5\textwidth}
        \includegraphics[trim={0cm 0.5cm 0cm 0.2cm},clip,width=0.95\linewidth]{images/supp/no_queries_vs_perf_r5.png}
    \end{subfigure}
    \begin{subfigure}[b]{0.5\textwidth}
        \includegraphics[trim={0cm 0.5cm 0cm 0.2cm},clip,width=0.95\linewidth]{images/supp/no_queries_vs_perf_r10.png}
    \end{subfigure}
    \vspace{-0.3cm}
    \caption{Retrieval results reported for a TT-CE+~\cite{croitoru2021teachtext} model on the MSR-VTT~\cite{xu2016msr} \texttt{validation} split in terms of R@1, R@5 and R@10.
    \textit{Left:} \textit{The influence of querybank size on retrieval performance.}
    We observe that performance grows steadily with increasing querybank size, but saturates.
    \textit{Right:} \textit{The influence of inverse temperature, .} Performance varies smoothly with inverse temperature, peaking at a value of 20.}
    \mbox{}\vspace{-0.7cm} \\
    \label{fig:num-queries-and-hyperparam-r1}
\end{figure}

\begin{figure*}
    \centering
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[trim={0cm 0.5cm 0cm 0.2cm},clip,width=0.8\linewidth]{images/supp/ex1.png}
        \caption{}
    \end{subfigure}
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[trim={0cm 0.5cm 0cm 0.2cm},clip,width=0.8\linewidth]{images/supp/ex2.png}
        \caption{}
    \end{subfigure}
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[trim={0cm 0.5cm 0cm 0.2cm},clip,width=0.8\linewidth]{images/supp/ex_failure.png}
        \caption{}
    \end{subfigure}
    \vspace{-0.3cm}
    \caption{\textbf{Qualitative results for the text video retrieval task.} 
    We show queries and frames from the retrieved videos.
    For the first two example queries,
    we observe that the use of \methodName leads to the retrieval of the
    correct target video.
    The third query represents a failure case in which the target video is not
    retrieved.
    However, we nevertheless observe qualitatively
    that for this example, the video retrieved
    with \methodName is more related to the query
    than the video retrieved without \methodName.
    }
    \mbox{}\vspace{-0.7cm} \\
    \label{fig:qualitative}
\end{figure*}

\section{Video and text embeddings (experts) description used for video retrieval}
\label{sec:experts}
For this work, we used the pretrained weights provided by TT-CE+ \cite{croitoru2021teachtext} and CE+ \cite{liu2019use} (\url{https://github.com/albanie/collaborative-experts}). These models use a set of pretrained experts. Below, we summarise how these experts were extracted.
\begin{itemize}
    \item Two action experts are used: \textit{Action(KN)} and \textit{Action(IG)}. \textit{Action(KN)} is a 1024-dimensional embedding produced by an I3D architecture trained on Kinetics~\cite{carreira2017quo}. The embeddings are extracted from frame clips at 25fps and center cropped to 224 pixels. For \textit{Action(IG)} the model is a 34-layer R(2+1)D~\cite{tran2018closer}, trained on IG-65m~\cite{ghadiyaram2019large}
    \item Two forms of object experts: \textit{Obj(IN)} and \textit{Obj(IG)}. For extracting \textit{Obj(IN)} a SENet-154~\cite{hu2019squeeze} model trained on ImageNet was used. For extracting \textit{Obj(IG)} a ResNext-101~\cite{xie2017aggregated} model trained on Instagram data with weakly labelled hashtags ~\cite{mahajan2018exploring} was used. Both of the embeddings are extracted at 25fps. 
    \item For producing an audio expert a VGGish model trained from audio classification on the YouTube-8m dataset~\cite{hershey2017} was used.
    \item For the scene expert a DenseNet-161~\cite{huang2017densely} pretrained on Places365~\cite{zhou2017places} was used. The scene embedding has a 2208 dimension.
    \item For the speech expert, the Google Cloud API (to transcribe the speech content) is used.
    \item For the text we use GPT2-xl~\cite{radford2019language} finetuned as provided by the authors. The size of the final pre-trained embedding is 1600.
\end{itemize}

For CLIP2Video~\cite{fang2021clip2video} we used the model as it is provided online \url{https://github.com/CryhanFang/CLIP2Video}. The model receives as input the raw frames and raw queries. For CLIP4Clip~\cite{luo2021clip4clip}, we use the online code \url{https://github.com/ArrowLuo/CLIP4Clip} and re-train the model for each dataset where we present results since weights are not available online.
For the other tasks we followed the instructions given on the official repositories. For MMT-Oscar~\cite{geigle2021retrieve} we used the pretrained weights and the features provided at \url{https://github.com/UKPLab/MMT-Retrieval}. For RDML~\cite{roth2020revisiting} we used the models provided at \url{https://github.com/Confusezius/Deep-Metric-Learning-Baselines}. For audio retrieval~\cite{oncescu2021audio} we used the pretrained weights and models provided at \url{https://github.com/oncescuandreea/audio-retrieval}.

\section{v2t performance metrics}
\label{sec:t2v}
\begin{table}
\begin{center}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|c|c|c|c|c}\hline \hline Model&Task&&&&\\\hline CE+\cite{croitoru2021teachtext}&v2t&&&&\\\textbf{CE+ (+\methodName)}&v2t&&&&\\\hline
TT-CE+\cite{croitoru2021teachtext}&v2t&&&&\\\textbf{TT-CE+ (+\methodName)}&v2t&&&&\\\hline \end{tabular}}
\end{center}
\vspace{\spacebefore{}}
\caption{\textbf{MSR-VTT full split: Comparison to state of the art - v2t task.}\label{tab:msrvtt-v2t}}
\vspace{\spaceafter{}}
\end{table} In Tab.~\ref{tab:msrvtt-v2t} we report metrics indicating the
performance of \methodName on the reverse task of video-text retrieval
(in which videos are used as queries to retrieve descriptions).
We apply \methodName with DIS normalisation using all videos from the training split
to construct the querybank.
We observe that \methodName yields a striking boost in performance.

\section{Qualitative results}
\label{sec:qualitative}
In Fig.~\ref{fig:qualitative} we provide some qualitative examples,
illustrating cases for which the \methodName model correctly retrieves
videos that are not retrieved without \methodName.
Examining failure cases, we found qualitative examples for which
the retrieval ranking produced with \methodName was more ``reasonable''
(as shown in the bottom set of Fig.~\ref{fig:qualitative}).
However, in line with prior work~\cite{radovanovic2010hubs} suggesting that
hubness is a property of the distribution
(rather than driven by individual samples),
we did not observe consistent, obvious qualitative trends
among the samples that were corrected, or remaining failure cases.
As an example, we observed gains for queries with both shorter
and longer, highly descriptive captions.
