\documentclass{article} \usepackage{iclr2023_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{url}
\usepackage{multirow}
\usepackage{floatrow}
\usepackage{longtable}
\usepackage{wrapfig}




\title{GOOD: Exploring Geometric Cues for Detecting Objects in an Open World}


\author{Haiwen Huang \; Andreas Geiger \; Dan Zhang \\
Bosch Industry on Campus Lab, University of Tübingen\\ Autonomous Vision Group, University of Tübingen\\
Max Planck Institute for Intelligent Systems, Tübingen \;
Bosch Center for Artificial Intelligence\\
\texttt{\{haiwen.huang, a.geiger\}@uni-tuebingen.de, Dan.Zhang2@de.bosch.com} }

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy 

\begin{document}

\maketitle

\begin{figure}[h]
     \centering
     \begin{subfigure}[b]{0.244\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/fig1/gt/ADE_val_00000669_numbox_20.jpg}
         \caption{Ground truth (20)}
         \label{fig:y equals x}
     \end{subfigure}
     \begin{subfigure}[b]{0.244\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/fig1/oln/ADE_val_00000669_gtnum20_tpnum13_vis_num13.jpg}
         \caption{OLN (13)}
         \label{fig:three sin x}
     \end{subfigure}
     \begin{subfigure}[b]{0.244\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/fig1/ggn/ADE_val_00000669_gtnum20_tpnum14_vis_num14.jpg}
         \caption{GGN (14)}
         \label{fig:five over x}
     \end{subfigure} 
     \begin{subfigure}[b]{0.244\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/fig1/midow/ADE_val_00000669_gtnum20_tpnum18_vis_num18.jpg}
         \caption{GOOD (18)}
         \label{fig:five over x}
     \end{subfigure} 
     \\
     \begin{subfigure}[b]{0.244\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/fig1/gt/ADE_val_00001463_numbox_22.jpg}
         \caption{Ground truth (22)}
         \label{fig:y equals x}
     \end{subfigure}
     \begin{subfigure}[b]{0.244\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/fig1/oln/ADE_val_00001463_gtnum22_tpnum6_vis_num6.jpg}
         \caption{OLN (6)}
         \label{fig:three sin x}
     \end{subfigure}
     \begin{subfigure}[b]{0.244\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/fig1/ggn/ADE_val_00001463_gtnum22_tpnum6_vis_num6.jpg}
         \caption{GGN (6)}
         \label{fig:five over x}
     \end{subfigure} 
     \begin{subfigure}[b]{0.244\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/fig1/midow/ADE_val_00001463_gtnum22_tpnum15_vis_num15.jpg}
         \caption{GOOD (15)}
         \label{fig:five over x}
     \end{subfigure} 
        \caption{\textbf{Comparison of GOOD with different baselines.} Images in the first column are from validation sets of ADE20K~\citep{zhou2019semantic}. From the second to fourth columns we show the detection results of three open-world object detection methods: OLN~\cite{kim_learning_2021}, GGN~\cite{wang2022open}, and our Geometry-guided Open-world Object Detector (GOOD). The shown detection results are true-positive proposals from the top 100 proposals of each method. The numbers of true positive proposals or ground truth objects are denoted in parentheses. All models are trained on the RGB images from the PASCAL-VOC classes of the COCO dataset~\citep{CoCodataset}, which do not include houses, trees, or kitchen furniture. Both OLN and GGN fail to detect many objects not seen during training. GOOD generalizes better to unseen categories by exploiting the geometric cues.}\label{fig:detection-vis}
\end{figure}

\begin{abstract}

We address the task of open-world class-agnostic object detection, i.e., detecting every object in an image by learning from a limited number of base object classes. State-of-the-art RGB-based models suffer from overfitting the training classes and often fail at detecting novel-looking objects. This is because RGB-based models primarily rely on appearance similarity to detect novel objects and are also prone to overfitting short-cut cues such as textures and discriminative parts. 
To address these shortcomings of RGB-based object detectors, we propose incorporating geometric cues such as depth and normals, predicted by  general-purpose monocular estimators.
Specifically, we use the geometric cues to train an object proposal network for pseudo-labeling unannotated novel objects in the training set. 
Our resulting Geometry-guided Open-world Object Detector (GOOD) significantly improves detection recall for novel object categories and already performs well with only a few training classes.
Using a single ``person'' class for training on the COCO dataset, GOOD  surpasses SOTA methods by 5.0\% AR@100, a relative improvement of 24\%.

\end{abstract}



\section{Introduction}
The standard object detection task is to detect objects from a predefined class list. However, when deploying the model in the real world, it is rarely the case that the model will only encounter objects from its predefined taxonomy. In the open-world setup, object detectors are required to detect all the objects in the scene even though they have only been trained on objects from a limited number of classes. 
Current state-of-the-art object detectors typically struggle in the open-world setup. As a consequence, open-world object detection has gained increased attention over the last few years~\citep{jaiswal2021class, kim_learning_2021, joseph_towards_2021, wang2022open}.
In this work, we specifically address the task of open-world class-agnostic object detection, which is a fundamental task for downstream applications like open-world multi-object tracking~\citep{liu2022openingtracking}, robotics~\citep{jiang2019openrobots}, and autonomous AI agents~\citep{liu2021self}.


One reason for the failure of current object detectors in the open-world setting is that during training, they are penalized for detecting unlabeled objects in the background and are thus discouraged from detecting them.
Motivated by this, previous works have designed different architectures~\citep{kim_learning_2021, konan2022extending} and training pipelines~\citep{saito_learning_2021, wang2022open} to avoid suppressing the unannotated objects in the background, which have led to significant performance improvements. However, these methods still suffer from overfitting the training classes. 
Training only on RGB images, they mainly rely on appearance cues to detect objects of new categories and have great difficulty generalizing to novel-looking objects.
Also, there are known short-cut learning problems with regard to training on RGB images~\cite{geirhos2018, geirhos2020shortcut, sauer2021counterfactual} -- there is no constraint for overfitting the textures or the discriminative parts of the known classes during training.
In this work, we propose to tackle this challenge by incorporating geometry cues extracted by general-purpose monocular estimators from the RGB images. We show that such cues significantly improve detection recall for novel object categories on challenging benchmarks.

Estimating geometric cues such as depth and normals from a single RGB image has been an active research area for a long time.
Such mid-level representations possess built-in invariance to many changes (e.g., brightness, color) and are more class-agnostic than RGB signals, see Figure~\ref{fig:mid-vis}. In other words, there is less discrepancy between known and unknown objects in terms of geometric cues.
In recent years, thanks to stronger architectures and larger datasets~\citep{transformer_depth, midas, eftekhar2021omnidata}, monocular estimators for mid-level representations have significantly advanced in terms of prediction quality and generalization to novel scenes. These models are able to compute high-quality geometric cues efficiently when used off-the-shelf as pre-trained models on new datasets. Therefore, it becomes natural to ask if these models can provide additional knowledge for current RGB-based open-world object detectors to overcome the generalization problem.


\begin{figure}[t]
    \centering
     \parbox{\textwidth}{
    \parbox[b]{.2\textwidth}{\includegraphics[width=0.20\textwidth]{iclr2023/figures/fig2/000000159791.jpg}
      \includegraphics[width=0.20\textwidth]{iclr2023/figures/fig2/000000005802.jpg}
}
    \parbox[b]{.2\textwidth}{\includegraphics[width=0.20\textwidth]{iclr2023/figures/fig2/depth000000159791.jpg}
      \includegraphics[width=0.20\textwidth]{iclr2023/figures/fig2/depth000000005802.jpg}
}
    \parbox[b]{.2\textwidth}{\includegraphics[width=0.20\textwidth]{iclr2023/figures/fig2/normal000000159791.jpg}
    \includegraphics[width=0.20\textwidth]{iclr2023/figures/fig2/normal000000005802.jpg}}
\parbox[b]{.4\textwidth}{\includegraphics[width=0.39\textwidth]{iclr2023/figures/fig2/generalization_gap_with_title.pdf}
    }}
    
    \caption{\textbf{Geometry cues are complementary to appearance cues for object localization.} The depth and normal cues of the RGB image are extracted using off-the-shelf general-purpose monocular predictors. \textbf{Left:}
    Geometric cues abstract away the appearance details and focus on more holistic information such as object shapes and relative spatial locations (depth) and directional changes (normals). 
    \textbf{Right:}
    By incorporating geometric cues, GOODs generalize better than the RGB-based model OLN~\citep{kim_learning_2021}, i.e., much smaller AR gaps between the base and novel classes.}\vspace{-0.2em}\label{fig:mid-vis} 
\end{figure}

In this paper, we propose to use a pseudo-labeling method for incorporating geometric cues into open-world object detector training.
We first train an object proposal network on the predicted depth or normal maps to discover novel unannotated objects in the training set. The top-ranked novel object predictions are used as pseudo boxes for training the open-world object detector on the original RGB input. We observe that incorporating the geometry cues can significantly improve the detection recall of unseen objects, especially those that differ strongly from the training objects, as shown in Figure~\ref{fig:detection-vis} and~\ref{fig:mid-vis}.
We speculate that this is due to the complementary nature of geometry cues and the RGB-based detection cues: the geometry cues help discover novel-looking objects that RGB-based detectors cannot detect, and the RGB-based detectors can make use of more annotations with their strong representation learning ability to generalize to novel, unseen categories.

Our resulting Geometry-guided Open-world Object Detector (GOOD) surpasses the state-of-the-art performance on multiple benchmarks for open-world class-agnostic object detection.
Thanks to the rich geometry information, GOOD can generalize to unseen categories with only a few known classes for training.
Particularly, with a single training class ``person'' on the COCO dataset~\citep{CoCodataset}, GOOD can surpass SOTA methods by 5.0\% AR@100 (a relative improvement of 24\%) on detecting objects from non-person classes. With 20 PASCAL-VOC classes for training, GOOD surpasses SOTA methods even by 6.1\% AR@100 in detecting non-VOC classes.
Furthermore, we also analyze the advantages of geometric cues and show that they are less sensitive to semantic shifts across classes, and are better than other strategies for improving generalization.


 \section{Related work}

\textbf{Open-world class-agnostic object detection} is the task of localizing all the objects in an image by learning with only a limited number of object classes (base classes). 
The core problem with standard object detection training is that the model is trained to classify the unannotated objects as background and thus is suppressed to detect them at inference time. 
To solve this issue, \citet{kim_learning_2021} proposed object localization network (OLN), which replaces the classification heads of Faster RCNN~\citep{ren_faster_2015} with class-agnostic objectness heads so that the training loss is only calculated on positive samples, i.e., known objects, and thus not suppressing the detection of unannotated novel objects. 
\citet{saito_learning_2021} synthesized a training set by copy-pasting known objects onto synthetic backgrounds. However, the model struggles with the synthetic-to-real domain gap in solving the object detection task. 
Besides background non-suppression, a more proactive approach is to exploit unannotated objects for training.~\citet{wang2022open} built upon traditional learning-free methods and developed a pairwise affinity predictor to discover unannotated objects. Their object detector, GGN, is then trained using the newly-discovered object masks and ground truth base class annotations as supervision. 
Finally, another promising direction is to use open-world knowledge from large pretrained multi-modal models. Recently,~\citet{minderer2022simple, Maaz2022Multimodal} made use of pretrained language-vision model~\citep{radford2021learning} to detect open-world objects using text queries. Our work is most related to OLN and GGN. We used the OLN architecture and training loss, but additionally incorporated geometry cues through our pseudo-labeling method. GGN used the pairwise affinity for pseudo labeling. However, since the pairwise affinity is trained on RGB inputs using the base class annotations, GGN still suffers from the overfitting problems of RGB-based methods. Our experiments showed geometric cues as a better source of pseudo boxes.


\textbf{Incorporating geometric cues for generalization.} 
The estimation of geometric cues has been an active research area for decades.
With the introduction of deep neural networks, the seminal work by~\citet{eigen2014depth, eigen2015predicting} significantly improved over early works~\citep{hoiem2005automatic, hoiem2005geometric, hoiem2007recovering, hoiem2008putting, saxena2005learning, saxena20083, saxena2008make3d}. Recent progress in estimating geometric cues can be attributed to the use of modern architectures~\citep{midas}, stronger training strategies~\citep{zamir2020robust} and large-scale datasets~\citep{eftekhar2021omnidata}. 
In particular, Omnidata~\citep{eftekhar2021omnidata} has made significant headway in prediction quality and cross-dataset generalization. 
Since geometric cues abstract away the appearance details and retain more holistic information about the objects, such as shapes, they have been incorporated into many applications for generalization.
For example, \citet{Xian2022gin} incorporated them into the 3D shape completion pipeline to generalize to novel classes. \citet{Yu2022MonoSDF} used them to guide the optimization of neural implicit surface models for tackling scenes captured from sparse viewpoints. \citet{pmlr-v155-chen21f} applied mid-level visual representations to reinforcement training and gained robustness under domain shifts. 
In this work, we propose to incorporate geometric cues through pseudo-labeling and also demonstrate large performance gains on open-world class-agnostic object detection benchmarks.

 \begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{iclr2023/figures/fig3/fig3_rectangle.pdf}
    \caption{\textbf{Overview of the geometry-guided pseudo labeling method.} It consists of two training phases. Phase I: the RGB input is firstly preprocessed by the off-the-shelf model to extract the geometry cues, which are then used to train an object proposal network with the base class bounding box annotations. The proposal networks then pseudo-label the training samples, discovering unannotated novel objects. The top-ranked pseudo boxes are added to the annotation pool for  Phase II training, i.e., a class-agnostic object detector is directly trained on the RGB input using both the base class and pseudo annotations. At inference time, we only need the model from Phase II.
    }
    \label{fig:pipeline}
\end{figure}

\section{Method}
Our goal is to incorporate geometric cues for an improved open-world class-agnostic object detection performance. Concretely, we propose a pseudo labeling method, which can effectively utilize the geometric cues to detect unannotated novel objects in the training set and then use them for training the object detector. See the overview of our method in Figure~\ref{fig:pipeline}.

\subsection{open-world class-agnostic object detection problem}
Current state-of-the-art object detection methods work well under the closed-world assumption. They are trained with a set of object bounding box annotations  from a pre-specified list  of semantic classes, i.e., the base classes. At test time, their generalization is evaluated by detecting the objects from the known base classes. The standard training loss of object detection for an image  has two parts: classification loss and bounding box regression loss

where  is the index of an anchor from the candidate set ,  are the predicted label and bounding box coordinates, and  are the corresponding ground truths.  is the total number of anchors in the candidate set .  is the size of the subset , which only contains the anchors with the ground truth label . Note  equals  only when the anchor  can be associated to an annotated object bounding box from the known classes ; otherwise it equals  (background). 

From the closed-world to the open-world setup, the generalization goal extends to localizing every object in the image, which can belong to an unknown novel class . Under the training loss in (\ref{std_loss}), an unannotated object will be classified as ``background''. As a result, the model will treat similar types of objects as ``background'' at inference time. To avoid suppressing the detection of novel objects in the background, \citet{kim_learning_2021} proposed to replace the classification loss in (\ref{std_loss}) by the objectness score prediction loss, yielding

where  and  are the predicted objectness score and its ground truth of anchor . In doing so, only the anchors with  are involved in training, completely removing any ``background'' prediction. At inference time, the objectness score is used to rank the detections. However, since these anchors only capture the annotated objects from the base classes, this loss modification cannot effectively mitigate the overfitting to the base classes. We further resort to adding additional ``objects'' into training, especially novel ones with very different appearances than objects from the base classes.

\subsection{Exploiting geometric cues}
Models trained on RGB images tend to over-rely on the appearance cues for object detection. Therefore, it is hard for them to detect novel objects that appear very differently from the base classes. For instance, a model trained on cars is likely to detect trucks, but unlikely to also detect sandwiches. Involving such novel objects, e.g., food, into training is then an effective way to mitigate the appearance bias towards the base classes, e.g., vehicles. To this end, we exploit two types of geometric cues, i.e., depth and normals, for detecting unannotated novel objects in the training set, see some examples in Figure~\ref{fig:mid-vis}. Both of them are common geometric cues that capture local information. Depth focuses on the relative spatial difference of objects and abstracts away the details on the object surfaces. Surface normals focus on the directional difference and remain the same on flat surfaces. Compared with the original RGB image, they discard most appearance details and focus on the geometry information such as object shapes and relative spatial locations. Models trained with them can thus discover many novel-looking objects that RGB-based ones cannot detect.


We use off-the-shelf pretrained models to extract geometric cues. Specifically, we use Omnidata models~\citep{eftekhar2021omnidata} trained using cross-task consistency~\citep{zamir2020robust} and 2D/3D data augmentations~\citep{kar20223d}.
The training dataset for the models is the Omnidata Starter Dataset (OSD)~\citep{eftekhar2021omnidata} which contains 2200 real and rendered scenes. 
Despite the difference between the OSD to the object detection benchmark datasets, the Omnidata model can produce high-quality results, implying that the invariances behind these geometric cues are robust. 



\subsection{Pseudo labeling method}\label{sec:pipeline}
To use the geometric cues to discover unannotated novel objects in the training set, we first train an object proposal network on the depth or normal input using the same training loss as in (\ref{eq:oln}), i.e., Phase-I training in Figure~\ref{fig:pipeline}. 
Then, this object proposal network will pseudo-label the training images using its detected bounding boxes. 
After filtering out the detected bounding boxes which overlap with the base class annotations, we then add the remaining top- boxes to the ground truth annotations. 
Here  is determined for each detector on a small holdout validation set. Finally, we train a new class-agnostic object detector using the RGB image as input and the extended bounding box annotation pool as ground truth, i.e., Phase-II in Figure~\ref{fig:pipeline}. The training loss is

Compared with (\ref{eq:oln}), the anchors that overlap with the pseudo boxes of the detected novel objects, i.e., , are also involved in training. 
The pseudo boxes can be acquired from a single source, i.e., one of the geometric cues, and from both, i.e., pseudo label ensembling. We name our method - when using a specific geometric cue  as the pseudo labeling source, whereas - stands for ensembling the pseudo labels from both the depth and normals. 


Inspired by previous works in self-training~\citep{xie2020self, sohn2020fixmatch, xu2021softteacher}, we use strong data augmentation during Phase-II to counteract the noise in pseudo boxes and further boost the performance of GOOD.  
Specifically, for Phase-II training, we use AutoAugment~\citep{cubuk2019autoaugment} which includes random resizing, flip, and cropping. 



 



\section{Experiments}
\textbf{Benchmarks.}
We target two major challenges of open-world class-agnostic object detection: \textit{cross-category and cross-dataset generalization}. 
For the cross-category evaluation, we follow the common practice in the literature~\citep{kim_learning_2021,wang2022open} to split the class category list into two parts. One is used as the base class for training, whereas the other is reserved only for testing cross-category generalization. Specifically, we adopt two splits of the  classes in the COCO dataset~\citep{CoCodataset}.
The first benchmark splits the COCO classes into a single ``person'' class and 79 non-person classes. This is to stress-test the generalization ability of the methods. We follow~\cite{wang2022open} to choose the ``person'' category as the training class because it contains diverse viewpoints and shapes. The second benchmark splits the COCO classes into  PASCAL-VOC~\citep{Everingham10} classes for training and the other  for testing. 
For the cross-dataset evaluation, we use the ADE20K dataset~\citep{zhou2019semantic} for testing. We compare models trained using only  PASCAL-VOC classes or all  COCO classes on detecting objects in ADE20K.
This is to evaluate open-world class-agnostic object detectors when used in the wild.


\textbf{Implementation.}
We use the same architecture as OLN in~\citep{kim_learning_2021} for both Phase I and Phase II training. OLN is built on top of a standard Faster RCNN \citep{ren_faster_2015} architecture with a ResNet-50 backbone pretrained on ImageNet~\citep{imagenet_cvpr09}. We implement our method using MMDetection framework~\citep{mmdetection} and use the SGD optimizer with an initial learning rate of  and batch size of . The models with data augmentation are all trained for  epochs. Other models are trained for  epochs. 
We did not find training for longer epochs beneficial for models without data augmentation.
The optimal number of pseudo boxes, i.e., , varies across input types and is determined on a small holdout validation set.
See Appendix~\ref{appendix: implementation} for further  details. 



\textbf{Evaluation metrics.}
Following~\citep{kim_learning_2021, wang2022open}, we use Average Recall (AR@k) over multiple IoU thresholds (0.5:0.95), and set the detection budget k as  by default. All ARs are shown in percentage. AR and AR respectively denote the AR score on detecting all classes (including the base and novel ones) and on detecting the novel classes. To evaluate AR, we do not count the boxes associated to the base classes into the budget k. The same protocol is applied when evaluating per-class ARs and ARs for small, medium and large size of objects, i.e., AR. 


\subsection{Detecting unknown objects in an open world}
Table~\ref{table:cross-category-main} and~\ref{table:cross-dataset-main} compare open-world class-agnostic object detectors on two cross-category benchmarks and two cross-dataset benchmarks, respectively. Our method GOOD, which incorporates geometric cues, considerably outperforms state-of-the-art RGB-based open-world class-agnostic object detection methods, i.e., OLN~\citep{kim_learning_2021} and GGN~\citep{wang2022open}. OLN did not involve any novel object into training. Although GGN proposed to use an intermediate pairwise affinity (PA) representation for pseudo labeling, the PA predictor is still trained on the RGB images, therefore it still has the bias towards the known classes as other RGB-based methods. 




\begin{table}[t]

\begin{subtable}{\linewidth}
\centering
{\scalebox{0.95}{
			\resizebox{\textwidth}{!}{
\begin{tabular}{l|l|l|l|l|l|l|l|l|l|l}
						\toprule
                &  \multicolumn{5}{c|}{\textbf{PersonNon-Person}} &  \multicolumn{5}{c}{\textbf{ VOCNon-VOC}}\\
               \midrule
						 Method &  AR & AR  & AR &  AR & AR & AR & AR  & AR &  AR & AR \\ \midrule
                     \textcolor[RGB]{128,128,128}{FRCNN (oracle)} & \textcolor[RGB]{128,128,128}{55.9} & \textcolor[RGB]{128,128,128}{53.4} & \textcolor[RGB]{128,128,128}{37.9} & \textcolor[RGB]{128,128,128}{59.5} & \textcolor[RGB]{128,128,128}{73.0} & \textcolor[RGB]{128,128,128}{55.9} & \textcolor[RGB]{128,128,128}{52.6} & \textcolor[RGB]{128,128,128}{37.1}	& \textcolor[RGB]{128,128,128}{60.0} & \textcolor[RGB]{128,128,128}{73.1} \\
		               \midrule
                 FRCNN (cls-agn)  &25.7 & 12.2& 8.7 & 12.4 & 18.2& 38.5 & 27.3   & 10.8    & 30.2  & 55.8    \\
                             OLN~\citep{kim_learning_2021} & 30.9& 16.5  &8.7 & 14.7 & 33.4 & 47.5 &33.2   & 18.7    & 39.3  & 58.6\\
                            GGN~\citep{wang2022open} & 30.3& 20.7 & 12.0 & 25.6 & 29.6 &39.8 &31.5 & 11.8 & 37.4 & \textbf{63.8}\\
                            
                             SelfTrain-RGB   & 32.5 &18.7 & 11.3 & 18.6 & 32.6 & 48.1
& 37.4   &\textbf{22.8}  & 43.9 & 57.7  \\ 
 \midrule
                             GOOD-Depth   & 37.0 & 25.6 & 12.8 & 30.4 & \textbf{42.4} &  \textbf{49.6}& 39.0  & 21.1 & 47.5 & 63.2   \\ 
                             GOOD-Normal &35.6 & 23.7&13.9  &27.6 & 36.0 & \textbf{49.6} & 38.9 & 21.2 & 47.9 & 62.0 \\
                             GOOD-Both  & \textbf{37.3} &  \textbf{25.9} & \textbf{14.2} & \textbf{32.6} & 38.9 & 49.5  & \textbf{39.3} & 21.6 & \textbf{48.2} & 62.4 \\ 



                  \bottomrule
						\end{tabular}
}}}
    \caption{\textbf{Cross-category benchmarks}}\label{table:cross-category-main}
\end{subtable}\\ \vspace{3pt}
\begin{subtable}{\linewidth}
\centering
{\scalebox{0.8}{
			\resizebox{\textwidth}{!}{
\begin{tabular}{l|l|l|l|l|l|l|l|l}
						\toprule
                &  \multicolumn{4}{c|}{\textbf{VOCADE20K}} &  \multicolumn{4}{c}{\textbf{COCOADE20K}}\\
                \midrule
						 Method  & AR  & AR & AR & AR &  AR & AR & AR & AR  \\ \midrule
		   FRCNN (cls-agn)&  22.6 & 15.5 & 23.7 &26.5& 25.9 & 20.5 & 28.5 & 27.4 \\ 
                             OLN~\citep{kim_learning_2021}    &	29.2 &	19.7&	30.7&	34.4  &  32.9&	25.1&	35.9&	35.6 \\
                            GGN~\citep{wang2022open}  & 27.0 &  16.9 & 27.5 & 33.6 & 29.8 & 18.9 & 29.1 & 38.2  \\                      
                            SelfTrain-RGB &  	27.7 &	18.4&	29.7 &	32.4 & 	33.8 &	\textbf{26.5}&	37.6&	35.4\\
                            \midrule
                             GOOD-Depth   &	33.9 &	21.1 &	35.9 &	41.2&  \textbf{35.3} &	25.7 &38.0 & 39.6 \\ 
                            GOOD-Normal   &	33.4 &	22.0 &	36.2 &	38.8& 	33.5&	25.7&	37.1&	35.5\\                       
                             GOOD-Both  & \textbf{34.0}& 21.9 & \textbf{37.0} & \textbf{39.9} &	\textbf{35.3}&	25.1&	38.2&	\textbf{39.9} \\   
                   \bottomrule
						\end{tabular}
}}}
    \caption{\textbf{Cross-dataset benchmarks}}\label{table:cross-dataset-main}
\end{subtable}
\caption{\textbf{Detecting unknown objects in an open world.}  is a standard Faster R-CNN detector trained on all COCO classes and serves as a performance upper bound on the cross-category benchmarks. 
     is a Faster R-CNN trained in a class-agnostic manner, serving as a baseline for comparison.
    Our geometry-guided methods -s are compared with SOTA open-world class-agnostic object detectors, i.e., OLN and GGN.  is RGB-based self-training, i.e., using the RGB image instead of geometric cues for pseudo labeling. We only report AR in b) as the classes in ADE20K do not exactly match those in COCO.}\label{table: main_results}
\end{table}
 
On the cross-category benchmarks shown in Table~\ref{table:cross-category-main}, with a single training class ``person'' on the COCO dataset, GOOD can surpass SOTA methods by 5.0\% AR@100 on detecting objects from non-person classes, a relative improvement of 24\%. With 20 PASCAL-VOC classes for training, GOOD surpasses SOTA methods by 6.1\% AR@100 in detecting non-VOC classes, a relative improvement over 18\%. On the cross-dataset benchmarks, Table~\ref{table:cross-dataset-main} shows that GOOD achieves 2.4\% to 4.7\% gain on AR@100 in different setups. We observe that GOOD is particularly strong when there are fewer training classes, i.e., person  non-person and VOC  ADE20k. For RGB-based methods, the overfitting problems become more severe as the object diversity from the base classes reduces. In contrast, the geometric cues can still detect novel-looking objects, which are particularly helpful to training with only a limited number of base class annotations. Finally, ensembling both geometric cues offers additional performance gains.



\subsection{Advantages of geometric cues}

\textbf{Geometric cues are less sensitive to appearance shifts across classes.}
We first compare per-novel-class AR@5 of the object proposal network trained on geometric cues with that trained on the RGB image. Here, AR@5 is of interest as geometric cues are used to discover novel-looking objects during Phase-I and no more than five pseudo boxes per image will be used in Phase-II, see Figure~\ref{fig:pipeline}.

Figure~\ref{fig:depth_classwise} shows that geometric cues can achieve much higher per-novel-class AR@5 than RGB in many categories. An example is the novel  supercategory ``food'', including classes such as ``hot dog'' and ``sandwich''. The base classes, belonging to the supercategory ``person'', ``animal'', ``vehicle'', and ``indoor'', have very different appearances to the ``food'' supercategory. The RGB-based model has difficulty detecting food using appearance cues.
In contrast, the  geometric cues can generalize across supercategories.
For categories that geometric cues are worse than RGB, we find that they typically are of small sizes, such as knives, forks, and clocks. This shows that while abstracting away appearance details, geometric cues may also lose some information about small objects. This again shows complementariness of RGB and geometric cues.


\begin{figure}[t]
    \centering
\includegraphics[width=1.0\textwidth]{iclr2023/figures/fig4/ar5_novel_pseudo_boxes_depth_normal_line_only.png}
\caption{\textbf{Per-novel-class AR@5 difference comparison of pseudo boxes on COCO VOC  Non-VOC}. We train the object proposal network on the geometric cues (Phase-I in Figure~\ref{fig:pipeline}) and also directly on the RGB image. We show their per-novel-class AR@5 differences, which is defined as  with . The geometric cues outperform the RGB image on those classes above the zero difference line. We also highlight some classes where RGB and geometric cues have big differences.
    }
    \label{fig:depth_classwise}
\end{figure}





\textbf{Geometric cues are better than edges and pairwise affinities.}
We further compare the geometric cues with two other mid-level representations: 2D edge and PA. 
The 2D edge map is extracted using the Holistically-nested Edge Detection (HED) model~\citep{xie2015holistically}, which shows more robust performance across the datasets than more recent methods. The HED model is trained on the Berkeley Segmentation Dataset and Benchmark (BSDS500) dataset~\citep{amfm_pami2011} which contains 500 images with segmentation annotations. PA can be thought of as a learned object boundary predictor.~\citet{wang2022open} trained it directly on RGB images and then grouped the predictions into object masks using a combination of  traditional grouping methods~\citep{shi2000normalized, arbelaez2006UCM, MCG}. 
We compare these four data modalities as the source of pseudo labeling. 
From Table~\ref{table:comparison_pseudo_box}, we can see that depth and normals outperform 2D edge and PA on detecting novel objects. 
We speculate that this is because 2D edge and PA mainly capture object boundaries, whereas depth and normals have an extra spatial understanding of the scene and can thus better detect objects in complex scenes. 
Owing to their better pseudo labels, GOOD-Depth/Normal outperform GOOD-Edge/PA on the final AR@100, i.e., 39.0\%/38.9\% vs. 38.1\%/37.1\%, see Appendix~\ref{appdex:modalities}.


\begin{table*}[!t]
					\begin{tabular}{l|l|l|l|l|l}
						\toprule
						Modality & AR @ 1 &   AR @ 5      & AR @ 5 &  AR @ 5 & AR@5    \\ \midrule
                             RGB & 4.0&		12.6& \textbf{5.4} & 16.4 & 21.8	 \\
 Depth & \textbf{4.7}&		\textbf{13.2}&	2.4 & 16.0 & \textbf{31.6}	\\
Normal &4.3&		12.8&	3.0 & \textbf{16.6} & 27.7	\\
Edge & 3.2&		10.4& 3.5 & 14.5 & 18.5	 \\
                    Pairwise affinity (PA) & 3.2 & 8.1 & 0.5 & 9.1 & 30.6
                             \\
                    \bottomrule
						\end{tabular}
	\caption{\textbf{Comparison on different data modalities for pseudo labeling}. We report AR@k achieved by the object proposal network trained on different modalities in Phase-I, where the benchmark is VOC  Non-VOC. Geometric cues (depth and normal) are stronger in discovering novel objects than the edge and pairwise affinity~\citep{wang2022open}.}
	\label{table:comparison_pseudo_box}
\end{table*}
 

\begin{table}
    \caption{\textbf{Comparison of GOOD and two other strategies on VOC to non-VOC}. ShapeBias replaced the backbone of SelfTrain-RGB with a stylized ImageNet pre-trained backbone. ScaleAug applied multi-scale augmentation to the RGB input for collecting pseudo boxes at different scales.}
\begin{tabular}{l|l|l|l|l|l}
						\toprule
						Method &  AR & AR & AR & AR & AR  \\ \midrule
                            SelfTrain-RGB & 48.1 & 37.4   &\textbf{22.8}  & 43.9 & 57.7 \\ \midrule
                             ShapeBias  & 48.8 & 36.5 & 21.4 & 42.5 & 58.6 \\
                             ScaleAug & 48.5 & 37.9 & 21.2 & 44.7 & 62.2\\
                             GOOD-Both & \textbf{49.5}  & \textbf{39.3} & 21.6 & \textbf{48.2} & \textbf{62.4}  \\
                    \bottomrule
						\end{tabular}


	\label{table:generalization_comparison}\vspace{-0.2em}
\end{table}
\textbf{Geometric cues are better than adding shape bias and multi-scale augmentation.}
In the previous part, we showed that geometric cues can detect novel objects from very different supercategories, owing to their less sensitivity to detailed appearance changes in objects such as textures. It is thus natural to compare with RGB-based model with inductive shape bias to counteract the texture bias. We adopt the stylized ImageNet pretrained ResNet-50 from~\citep{geirhos2018} as the backbone for SelfTrain-RGB. This backbone is trained using heavy style-based augmentation to mitigate the texture bias, where texture is one of the most discussed appearance features prone to overfit by RGB-based models. It showed performance improvements on the COCO object detection benchmark under the closed-world assumption~\citep{geirhos2018}.
Moreover, we observe from Figure~\ref{fig:depth_classwise} that RGB is relatively stronger in detecting smaller objects. This leads to the question of whether augmenting SelfTrain-RGB with pseudo boxes extracted at different input scales can already help this RGB-based method achieve comparable performance to those incorporating geometric cues.

From Table~\ref{table:generalization_comparison}, we can see that while the shape bias backbone can improve the AR of SelfTrain-RGB, it degrades the performance on novel classes, i.e., AR, indicating that shape bias obtained from training on ImageNet may not be very helpful in generalizing to novel objects. As for the multi-scale augmentation, although it can improve the detection recall of medium and large objects, the overall performance is still worse than our models that incorporate geometry cues. Overall, these comparisons show that geometry provides strong cues for object localization.



\subsection{Ablation studies}\label{sec:ablation}
In this section, we conduct ablation studies to understand the effectiveness of pseudo-labeling, data augmentation, and the influence of the number of base classes. Figure~\ref{fig:plr_effectiveness}) shows the AR@100 achieved by the object proposal network (Phase-I) and the object detector (Phase-II). The latter uses the pseudo boxes generated by the former, where the former can be trained with different data modalities. The RGB-based object proposal network outperforms the depth- and normal-based one on AR@100. This indicates that depth and normal maps cannot simply replace RGB image for object detection. As evidenced in Table~\ref{table:comparison_pseudo_box} and Figure~\ref{fig:depth_classwise}, they are competitive on AR@k with , meaning their top predictions are of high quality. Pseudo labeling is thus an effective way for geometry-guided open-world object detector training. 

We further study the effect of AutoAugment~\citep{cubuk2019autoaugment}. Using it during Phase-II training, we achieve higher AR@100 as shown in Figure~\ref{fig:aug_effectiveness}). Data augmentation is helpful to counteract the noise in pseudo labels. However, using AutoAugment to train OLN on the ground truth base class annotations, we observe only improvement in recalling the base class objects (from 58.4\% to 61.7\% AR@100), but degradation in novel object detection. This shows that data augmentation via random resizing, cropping, and flipping cannot improve generalization across categories. 

Finally, we study the influence of the number of base classes. As shown in Table~\ref{tab:base_class}, we split the COCO dataset based on the supercategories to create six different splits. We then train GOOD and OLN on these splits and evaluate them on ADE20K. More base classes in training allow object detectors to learn a more generic sense of objectness so that they can better detect novel objects. As shown in Figure~\ref{fig:base_classes}, both GOOD and OLN achieve better  AR@100 along with the number of base classes, and their performance gap reduces. However, the annotation cost also grows along with the number of classes. GOOD can achieve a similar AR@100 as OLN with only half the number of base classes, e.g.,  vs. . This shows that GOOD is more effective at learning a generic sense of objectness and less prone to overfitting to the base classes.

\begin{figure}[t]\centering
    \begin{subfigure}[b]{0.33\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/fig6/ablation_phase.pdf}
    \caption{Effectiveness of pseudo labeling.}
         \label{fig:plr_effectiveness}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/fig6/ablation_aug.pdf}
         \caption{Effectiveness of data augmentation.}
         \label{fig:aug_effectiveness}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.255\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/appendix/varying_num.pdf}
         \caption{Varying the number of base classes.}
         \label{fig:base_classes}
     \end{subfigure}
  \caption{\textbf{Ablation studies.} (a) and (b) are conducted on COCO VOC to non-VOC benchmark. The study on the number of base classes (c) uses ADE20K as the evaluation benchmark.}\label{fig:ablation}\vspace{-0.2em}
\end{figure}


\begin{table}
\centering
        \resizebox{0.85\columnwidth}{!}{\begin{tabular}{l|cccccc}
\toprule
Supercategories       & Person & +Vehicle & \begin{tabular}[c]{@{}l@{}}+Outdoor, \\ Animal\end{tabular} & \begin{tabular}[c]{@{}l@{}}+Accessory, \\ Sports\end{tabular} & \begin{tabular}[c]{@{}l@{}}+Kitchen, \\ Food\end{tabular} & \begin{tabular}[c]{@{}l@{}}+Furniture, Electronic, \\ Appliance, Indoor\end{tabular} \\
\midrule 
\# Training classes   & 1      & 9        & 24                                                          & 39                                                            & 56                                                        & 80                                                                                   \\
\# Training images    & 64115  & 74152    & 92169                                                       & 93939                                                         & 107036                                                    & 117266                                                                               \\
\# Training instances & 609666 & 654460   & 715582                                                      & 727207                                                        & 824535                                                    & 860001             \\
\bottomrule
\end{tabular}}
\caption{\textbf{Base class choices for studying the influence of the number of base classes.}}
\label{tab:base_class}
\end{table}

 \section{Conclusion}
In this paper, we proposed a method GOOD for tackling the challenging problem of open-world class-agnostic object detection. It exploits the easy-to-obtain geometric cues such as depth and normals for detecting unannotated novel objects in the training set. As the geometric cues focus on object shapes and relative spatial locations rather than appearances, they can detect novel objects that RGB-based methods cannot detect. By further involving these novel objects into RGB-based object detector training, GOOD demonstrates strong generalization performance across categories as well as datasets. 

\subsection*{Acknowledgment}
Haiwen Huang and Dan Zhang were supported by Bosch Industry on Campus Lab at University of Tübingen. Andreas Geiger was supported by the ERC Starting Grant LEGO-3D (850533) and the DFG EXC number 2064/1 - project number 390727645. Haiwen Huang would like to thank Tianlin Ye for her emotional support throughout the project and Jojumon Kavalan for generously providing access to his Internet during the rebuttal period. 

\bibliography{iclr2023/iclr2023_conference}
\bibliographystyle{iclr2023/iclr2023_conference}

\newpage


\appendix



\section{Implementation details}\label{appendix: implementation}

GOOD uses OLN~\citep{kim_learning_2021} as the architecture for both Phase-I and Phase-II training. OLN is built on top of Faster RCNN~\citep{ren_faster_2015}.
For open-world object detection, the classification heads are replaced with the objectness score prediction heads, i.e., predicting the centerness and IoU of each bounding box proposal at the two stages, respectively. We use the objectness score  for ranking the pseudo boxes and selecting the top  pseudo boxes per image.
The optimal  choice for GOOD-Depth and GOOD-Normal is 1 and for SelfTrain-RGB is 3.

For Phase-I training, we trained the proposal network with loss as given in Eq.~\ref{eq:oln} and used the SGD optimizer with an initial learning rate of 0.01 and batch size of 16 for 8 epochs. 
For Phase-II training, unless otherwise stated, we trained the object detector with loss as given in Eq.~\ref{eq:good} and used SGD optimizer with an initial learning rate of  and batch size of  for  epochs with AutoAugment.
For GOOD-Both, we merge the pseudo boxes generated by object proposal networks separately trained on depth and normal maps by filtering out the overlapping boxes. Specifically, if the IoU of two pseudo boxes is larger than 0.5, they are seen as overlapping with each other, and the one with lower objectness score will be filtered out. For other ensembling  experiments, if not specified, pseudo boxes are also merged as described above.

We use the DPT-Hybrid models from Omnidata repository~\citep{eftekhar2021omnidata} for off-the-shelf inference of geometric cues. The DPT-Hybrid models~\citep{Ranftl2021} have a hybrid architecture of attention layers and convolutional layers. They are trained on the Omnidata Starter Datset~\citep{eftekhar2021omnidata} for one week with 2D and 3D data augmentations~\citep{kar20223d}, and one week with cross-task consistency~\citep{zamir2020robust} on 4 V100 GPUs. To infer on RGB images, we first pad images to sizes divisible by 32 without resizing, then feed them to the DPT-Hybrid model. Note although the original models are trained on 384384 image patches, we find that inferring on the original resolution of COCO produces better visual results than on the resolution of 384384.


\section{More benchmarks}
We further evaluate our approach on more benchmarks. Specifically, we evaluated the baselines and GOOD on the cross-category benchmark LVIS COCO to non-COCO and cross-dataset benchmark COCO to UVO. The results are shown in Table~\ref{table:more-benchmarks}. As expected, the performance gains are smaller than those observsed on benchmarks with smaller number of base classes such as COCO VOC to non-VOC. But still, GOOD surpasses all the state-of-the-art RGB-based methods. This shows that even in extreme conditions when the number of base classes is large, GOOD can still be helpful in improving the open-world performance.
\begin{table*}[!t]
	\begin{center}
		\scalebox{0.95}{
			\resizebox{\textwidth}{!}{
\begin{tabular}{l|l|l|l|l|l|l|l|l|l}
						\toprule
                &  \multicolumn{5}{c|}{\textbf{LVIS COCONon-COCO}} &  \multicolumn{4}{c}{\textbf{COCOUVO}}\\
                \midrule
						 Method  & AR & AR  & AR & AR & AR &  AR & AR & AR & AR  \\ \midrule
		   FRCNN (cls-agn)& 26.6 &  21.0 &  14.9 &	32.7 &	36.2 & 42.3 & 22.2 & 38.3 & 52.0 \\ 
                             OLN~\citep{kim_learning_2021}     & 32.2 &	27.4 & 17.9	& 44.7 & 53.1  & 49.2 & 35.0 & 48.7 & 55.1\\
                            GGN~\citep{wang2022open}  & 27.1 & 22.5 &	15.7 & 35.5 & 38.4 &  45.6 & 25.6 &  43.2 & 54.9  \\
                            SelfTrain-RGB & 32.6 & 28.3 &	19.0 &	45.7 &	52.2 & 48.7 &	35.8	& 48.8 & 53.5\\
                            \midrule
                             GOOD-Depth   &	32.8 &28.3 & 18.3 & 46.8 & \textbf{54.1} &  \textbf{50.3}	& 35.6 & 49.9	& \textbf{56.4}\\ 
                            GOOD-Normal   &	\textbf{33.4} & \textbf{29.2}	& \textbf{19.3} & \textbf{49.8}	&	53.3	& 49.8	& 35.8	& 50.0	& 55.0 \\        
                             GOOD-Both  & 33.2 & 29.0 & 19.0 &	 47.7	& 53.0  & \textbf{50.3} & \textbf{36.1}	& \textbf{50.2}	& 55.4 \\
                    \bottomrule
						\end{tabular}
}}
	\end{center}
	\caption{\textbf{More benchmarks.} The same methods in Table~\ref{table: main_results} are compared. }\vspace{-0.2em}
	\label{table:more-benchmarks}
\end{table*}
 
\section{More ablation studies}

\begin{figure}[t]\centering
    \begin{subfigure}[b]{0.48\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/appendix/stack_vs_ensemble.pdf}
    \caption{Choice of ensembling geometric cues.}
         \label{fig:ensemble_geometric_cues}
     \end{subfigure} 
     \hfill
     \begin{subfigure}[b]{0.48\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/appendix/stacktwo_vs_stackthree.pdf}
         \caption{Fusing inputs: whether to use RGB.}
         \label{fig:stacking}
     \end{subfigure} 
     \begin{subfigure}[b]{0.48\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/appendix/both_vs_all.pdf}
         \caption{Ensemble pseudo boxes: whether to use RGB.}
         \label{fig:goodall}
     \end{subfigure} 
     \vspace{0.2em}
     \begin{subfigure}[b]{0.5\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/appendix/pseudo_boxes_rgb_depth_normal_large_legend.png}
         \caption{Histograms of top1 pseudo box sizes from models trained on COCO VOC with different modalities.}
         \label{fig:pseudo_box_histogram}
     \end{subfigure}
  \caption{\textbf{More ablation studies.} }\label{fig:more_ablation}
\end{figure}


\subsection{Ensembling ways of geometric cues}
There are two possible ways to ensemble geometric cues: (1) Stack the two geometric cues together and train a single object proposal network on these stacked inputs in Phase-I; (2) Train two  object proposal networks and extract pseudo boxes separately, then merge them into a single pseudo box pool for Phase-II training. The details of the merging process is described in Appendix~\ref{appendix: implementation}. We conduct ablation studies on these two methods. From Figure~\ref{fig:ensemble_geometric_cues}, we demonstrate that empirically, ensembling pseudo labels is slightly better than using stacked inputs for Phase-I training. Throughout the paper, we use the pseudo label ensembling for GOOD-Both.

\subsection{On incorporating RGB in Phase-I}
It is  natural to think of incorporating RGB in Phase-I. 
To do so, we can also consider the two ways for ensembling geometric cues.
If we stack RGB with geometric cues to train the proposal network, the model will tend to make more use of RGB to optimize the target localization loss. This is because RGB is a much stronger input signal than geometric cues in the closed-world setup — AR@100 is 58.3 for RGB inputs alone and 44.9 when stacking depth and normals on COCO VOC classes. In the extreme case, the model can even completely ignore geometric cues. This reliance on RGB inputs prevents the model from making the best use of geometric cues to discover novel objects in Phase-I, which is crucial for open-world object detection.
As shown in Figure~\ref{fig:stacking}, stacking RGB with geometric cues leads to inferior performance across many benchmarks.

Alternatively, we can train a separate object proposal on RGB inputs, extract pseudo boxes, and merge them with those extracted from models trained on geometric cues. We can name this method ``GOOD-All''. In Figure~\ref{fig:goodall}, we compare GOOD-All with GOOD-Both and find that adding pseudo boxes from RGB (i.e., GOOD-All) either leads to no performance gains or even worsens the performance on benchmarks like VOC to ADE20K. To understand this, we note that object proposal networks trained on RGB favor smaller detection boxes, as evidenced in our visualizations (Figure~\ref{fig:pseudo-1},~\ref{fig:pseudo-2}) and more quantitatively in histograms (Figure~\ref{fig:pseudo_box_histogram}). These smaller detection boxes can either be small objects or just textures and parts of larger objects, which can potentially hurt the performance of the final detector to detect large objects. This is consistent with our observations in Table~\ref{table:goodall}. Compared to GOOD-Both, the gains in AR are usually too small to compensate for the losses in AR, leading to the inferior overall performance of GOOD-All.


\begin{table}
\centering
{\scalebox{0.95}{
			\resizebox{\textwidth}{!}{
\begin{tabular}{l|l|l|l|l|l|l|l|l|l}
						\toprule
                &  \multicolumn{5}{c|}{\textbf{VOCNon-VOC}} &  \multicolumn{4}{c}{\textbf{ VOCADE20K}}\\
               \midrule
						 Method &  AR & AR  & AR &  AR & AR & AR  & AR &  AR & AR \\ \midrule
                    
                             GOOD-Both   & \textbf{49.5}  & \textbf{39.3} & 21.6 & \textbf{48.2} & \textbf{62.4} & \textbf{34.0}& 21.9 & \textbf{37.0} & \textbf{39.9} \\ 
                             GOOD-All & 48.5 & \textbf{39.3}& \textbf{22.7} &47.6 	& 60.8 & 33.3 & \textbf{22.9} & 36.2 & 38.0\\
                  \bottomrule
						\end{tabular}
}}}
    \caption{\textbf{More comparison of GOOD-Both and GOOD-All.} For GOOD-All, the performance gains in detecting small objects (AR) are too small to compensate for the losses in detecting larger objects (AR and (AR)), leading to overall inferior performance.}\label{table:goodall}
\end{table}

\subsection{Architecture choice}
In principle, our approach is model agnostic and is therefore compatible with both proposal-free and proposal-based object detectors. To demonstrate this, besides the two-stage proposal-based detectors in the main paper, we also experiment with a more recent single-stage proposal-free object detector FCOS~\citep{tian2019fcos}. 
Specifically, we kept Phase I unchanged, i.e., generating the geometric cue-based pseudo boxes using OLN as the architecture. In Phase II, we train a class-agnostic FCOS using the extracted pseudo boxes together with the groundtruth annotations of the base classes.
The experiment results are shown in Table~\ref{table:fcos}. We can see that GOOD can  significantly improve FCOS in terms of detecting novel objects and OLN is a stronger architecture than FCOS to be used in GOOD.


\begin{table}
\centering
{\scalebox{0.95}{
			\resizebox{\textwidth}{!}{
\begin{tabular}{l|l|l|l|l|l|l|l|l|l}
						\toprule
                &  \multicolumn{5}{c|}{\textbf{VOCNon-VOC}} &  \multicolumn{4}{c}{\textbf{ VOCADE20K}}\\
               \midrule
						 Method &  AR & AR  & AR &  AR & AR & AR  & AR &  AR & AR \\ \midrule
                            
                             FCOS   & 43.6 & 29.3 & 	14.6	 & 35.7	& 50.2 & 25.6 &	17.2 & 26.5 & 30.9\\
                             OLN & 47.5 &33.2   & 18.7    & 39.3  & 58.6 &29.2 &	19.7&	30.7&	34.4  \\
                             GOOD-Both (FCOS) & 48.4 & 36.3 & 18.4 &  46.5 & 58.0 & 32.5 & 20.1 &	34.5 &39.5\\
                             GOOD-Both (OLN) & 49.5  & 39.3 & 21.6 & 48.2 & 62.4 & 34.0& 21.9 &37.0 & 39.9 \\
                  \bottomrule
						\end{tabular}
}}}
    \caption{\textbf{Architecture choice.} FCOS is a single-stage proposal-free object detector, and OLN is a two-stage proposal-based object detector (modified from Faster R-CNN). GOOD can significantly improve the open-world performance of both architectures.}\label{table:fcos}
\end{table}




\section{More discussion on different modalities for GOOD}\label{appdex:modalities}

In this section, we further discuss how different modalities behave and can be further ensembled to boost the performance. 
We first compare different modalities used for Phase-I training and pseudo labeling in GOOD on the COCO VOC to non-VOC benchmark. 
In Table~\ref{table:modality-comparison-good}, we show that pseudo-labeling using the geometric cues leads to stronger performances. This agrees with our study in Table~\ref{table:comparison_pseudo_box} where we found that pseudo boxes from proposal networks trained on geometric cues have higher AR@5, indicating that they are of higher quality.


\begin{table*}[!thbp]
	\begin{center}
\begin{tabular}{l|l|l|l|l}
						\toprule
						Modality &  AR & AR  &  AR  & AR  \\ \midrule
       SelfTrain-RGB  & 37.4  & \textbf{22.8} &	43.9 &	57.7 \\ 
GOOD-Edge & 38.1	& 21.8	& 45.7	& 60.2 \\
                             GOOD-PA  & 37.1 & 18.6 & 43.9 & \textbf{65.3} \\
                             GOOD-Depth   &   \textbf{39.0} & 	21.1 &	47.5	 & 63.2\\ 
                             GOOD-Normal &  38.9	& 21.4	& \textbf{47.9} & 62.4 \\
                    \bottomrule
						\end{tabular}
\end{center}
	\caption{\textbf{Comparison of GOOD using different modalities on COCO VOC to non-VOC benchmark.} }
	\label{table:modality-comparison-good}
\end{table*}


\subsection{Complementariness of different modalities}

In the main paper, we have combined the pseudo boxes only from the geometric cues, i.e., depth and normals. GOOD-Both provides additional performance gains over GOOD-Depth and GOOD-Normal. As we have more than two sources of pseudo labeling, it is natural to examine further if they are complementary and thus can be jointly exploited. We first evaluate the overlap of their top- ranked pseudo boxes. Their most confident novel object detections best convey their bias in generalization. We can observe from Figure~\ref{fig:overlap} that the overlap is low across all the input types. Table~\ref{table:comparison_pseudo_box} and Table~\ref{table:modality-comparison-good} further reveal their complementariness in detecting different sizes of objects. Both observations motivate us to ensemble different sources of pseudo boxes, exploiting their diversity for training the object detector. 

To decide which modality to ensemble first, we designed a simple greedy algorithm based on the overlap of pseudo boxes and the potential performance gain of adding the modality to the ensemble. 
Specifically, starting with the best-performing modality (depth), we incrementally ensemble more sources of pseudo boxes by selecting the source with the highest   score, where  is the performance gain against a vanilla OLN, and  is the overlap of the pseudo boxes with the current ensemble pseudo boxes. The performance is evaluated on a holdout validation set. The chosen ensemble order for the COCO VOC to non-VOC benchmark is: depth, normal, PA, edge, RGB.

We show the ensembling results in Figure~\ref{fig:topk_ensemble}.
Two baselines for using multiple pseudo boxes from RGB are considered: \textbf{SelfTrain-RGB} is using the top-k pseudo boxes from a single RGB-based object proposal netowrk for retraining, and \textbf{SelfTrain-RGB (ens)} is using pseudo boxes extracted and merged from k independently trained RGB-based object proposal networks for retraining.  We can see that ensembling pseudo sources from multiple modalities is  better than adding more pseudo boxes from a single source (RGB).


\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.5\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/overlap_heatmap_iou07.png}
    \caption{Overlap of top1 pseudo boxes.}
         \label{fig:overlap}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/appendix/new_ensemble.pdf}
         \caption{Influence of top-k when ensembling.}
         \label{fig:topk_ensemble}
     \end{subfigure}
    \caption{\textbf{Complementariness of different representations.} In (b), \textbf{SelfTrain-RGB} is using the top-k pseudo boxes from a single RGB-based object proposal netowrk for retraining, and \textbf{SelfTrain-RGB (ens)} is using pseudo boxes merged from k independently trained RGB-based object proposal netowrks for retraining.}
    \label{fig:complementary}
\end{figure}


 


\section{More visualization} 
\subsection{Visualization of geometric cues}
We visualize more examples of geometric cues in Figure~\ref{fig:geo-vis1} and Figure~\ref{fig:geo-vis2}. We demonstrate that the inferred geometric cues are of high quality in diverse scenes.

\subsection{Visualization of pseudo boxes from Phase-I}
We also provide visualization of pseudo boxes in Figure~\ref{fig:pseudo-1} and Figure~\ref{fig:pseudo-2}. 
The pseudo boxes are generated from OLNs trained on RGB, depth, and normals, respectively.
We find that pseudo boxes from RGB-based models generally tend to target small objects, textures, and parts of objects. This  again shows that RGB-based models over-rely on appearance cues and can overfit to textures and discriminative parts of the training classes.

\subsection{Visualization of GOOD detections on novel objects}
We further added more visualization examples of GOOD detection results in Figure~\ref{fig:novel-1}. The test images contain objects that are seen neither in the GOOD training set (COCO) nor the Omnidata model training set. The presented examples include new technology devices, spaceships, dinosaurs, aliens, and sea scenes. We can see that GOOD can still make reasonable object bounding box predictions even though these objects have never appeared in the training set.



\begin{figure}
     \centering
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/vis-mid/000000000247.jpg}
\label{fig:y equals x}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/vis-mid/depth000000000247.jpg}
\label{fig:three sin x}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/vis-mid/normal000000000247.jpg}
\label{fig:five over x}
     \end{subfigure}
     \\
      \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/vis-mid/000000000382.jpg}
\label{fig:y equals x}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/vis-mid/depth000000000382.jpg}
\label{fig:three sin x}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/vis-mid/normal000000000382.jpg}
\label{fig:five over x}
     \end{subfigure}
     \\
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/vis-mid/000000000785.jpg}
         \label{fig:y equals x}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/vis-mid/depth000000000785.jpg}
         \label{fig:three sin x}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/vis-mid/normal000000000785.jpg}
         \label{fig:five over x}
     \end{subfigure}
     \\
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/vis-geo/rgb/000000000139.jpg}
         \label{fig:y equals x}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/vis-geo/depth/000000000139.jpg}
         \label{fig:three sin x}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/vis-geo/normal/000000000139.jpg}
         \label{fig:five over x}
     \end{subfigure}
     \\
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/vis-geo/rgb/000000001000.jpg}
         \label{fig:y equals x}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/vis-geo/depth/000000001000.jpg}
         \label{fig:three sin x}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/vis-geo/normal/000000001000.jpg}
         \label{fig:five over x}
     \end{subfigure}
     \\
    \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/vis-geo/rgb/000000002149.jpg}
         \label{fig:y equals x}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/vis-geo/depth/000000002149.jpg}
         \label{fig:three sin x}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/vis-geo/normal/000000002149.jpg}
         \label{fig:five over x}
     \end{subfigure}
     \\
        \caption{\textbf{Visualization of geometric cues.} From left to right: RGB, depth, normals.}
        \label{fig:geo-vis1}
\end{figure}

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/vis-geo/rgb/000000007574.jpg}
\label{fig:y equals x}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/vis-geo/depth/000000007574.jpg}
\label{fig:three sin x}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/vis-geo/normal/000000007574.jpg}
\label{fig:five over x}
     \end{subfigure}
     \\
      \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/vis-geo/rgb/000000009769.jpg}
\label{fig:y equals x}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/vis-geo/depth/000000009769.jpg}
\label{fig:three sin x}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/vis-geo/normal/000000009769.jpg}
\label{fig:five over x}
     \end{subfigure}
     \\
      \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/vis-geo/rgb/000000011760.jpg}
\label{fig:y equals x}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/vis-geo/depth/000000011760.jpg}
\label{fig:three sin x}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/vis-geo/normal/000000011760.jpg}
\label{fig:five over x}
     \end{subfigure}
     \\
      \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/vis-geo/rgb/000000563604.jpg}
\label{fig:y equals x}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/vis-geo/depth/000000563604.jpg}
\label{fig:three sin x}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/vis-geo/normal/000000563604.jpg}
\label{fig:five over x}
     \end{subfigure}
     \\
      \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/vis-geo/rgb/000000570664.jpg}
\label{fig:y equals x}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/vis-geo/depth/000000570664.jpg}
\label{fig:three sin x}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/vis-geo/normal/000000570664.jpg}
\label{fig:five over x}
     \end{subfigure}
     \\
      \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/vis-geo/rgb/000000572555.jpg}
\label{fig:y equals x}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/vis-geo/depth/000000572555.jpg}
\label{fig:three sin x}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/vis-geo/normal/000000572555.jpg}
\label{fig:five over x}
     \end{subfigure}
        \caption{\textbf{Visualization of geometric cues.} From left to right: RGB, depth, normals.}
        \label{fig:geo-vis2}
\end{figure}

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/vis-pseudo/rgb/000000103806.jpg}
\label{fig:three sin x}
     \end{subfigure}
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/vis-pseudo/depth/000000103806.jpg}
\label{fig:five over x}
     \end{subfigure} 
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/vis-pseudo/normal/000000103806.jpg}
\label{fig:y equals x}
     \end{subfigure}
     \\
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/vis-pseudo/rgb/000000310325.jpg}
\label{fig:three sin x}
     \end{subfigure}
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/vis-pseudo/depth/000000310325.jpg}
\label{fig:five over x}
     \end{subfigure} 
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/vis-pseudo/normal/000000310325.jpg}
\label{fig:y equals x}
     \end{subfigure}
     \\
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/vis-pseudo/rgb/000000503772.jpg}
\label{fig:three sin x}
     \end{subfigure}
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/vis-pseudo/depth/000000503772.jpg}
\label{fig:five over x}
     \end{subfigure} 
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/vis-pseudo/normal/000000503772.jpg}
\label{fig:y equals x}
     \end{subfigure}
     \\
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/vis-pseudo/rgb/000000310177.jpg}
\label{fig:three sin x}
     \end{subfigure}
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/vis-pseudo/depth/000000310177.jpg}
\label{fig:five over x}
     \end{subfigure} 
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/vis-pseudo/normal/000000310177.jpg}
\label{fig:y equals x}
     \end{subfigure}

\caption{\textbf{Top3 pseudo boxes after filtering out those that overlap with known (VOC) class bounding boxes.} The pseudo boxes are generated from OLNs trained on RGB, depth, and normals, respectively.
OLN trained on RGB tends to detect small objects and parts of objects.}
        \label{fig:pseudo-1}
\end{figure}








\begin{figure}
     \centering

     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/vis-pseudo/rgb/000000186322.jpg}
\label{fig:three sin x}
     \end{subfigure}
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/vis-pseudo/depth/000000186322.jpg}
\label{fig:five over x}
     \end{subfigure} 
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/vis-pseudo/normal/000000186322.jpg}
\label{fig:y equals x}
     \end{subfigure}
     \\
\begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/vis-pseudo/rgb/000000410437.jpg}
\label{fig:three sin x}
     \end{subfigure}
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/vis-pseudo/depth/000000410437.jpg}
\label{fig:five over x}
     \end{subfigure} 
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/vis-pseudo/normal/000000410437.jpg}
\label{fig:y equals x}
     \end{subfigure}
     \\
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/vis-pseudo/rgb/000000577564.jpg}
\label{fig:three sin x}
     \end{subfigure}
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/vis-pseudo/depth/000000577564.jpg}
\label{fig:five over x}
     \end{subfigure} 
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/vis-pseudo/normal/000000577564.jpg}
\label{fig:y equals x}
     \end{subfigure}
     \\
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/vis-pseudo/rgb/000000199346.jpg}
\label{fig:three sin x}
     \end{subfigure}
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/vis-pseudo/depth/000000199346.jpg}
\label{fig:five over x}
     \end{subfigure} 
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/vis-pseudo/normal/000000199346.jpg}
\label{fig:y equals x}
     \end{subfigure}
     \\
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/vis-pseudo/rgb/000000028655.jpg}
\label{fig:three sin x}
     \end{subfigure}
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/vis-pseudo/depth/000000028655.jpg}
\label{fig:five over x}
     \end{subfigure} 
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/vis-pseudo/normal/000000028655.jpg}
\label{fig:y equals x}
     \end{subfigure}
     \\
    \caption{\textbf{Top3 pseudo boxes after filtering out those that overlap with known (VOC) class bounding boxes.} The pseudo boxes are generated from OLNs trained on RGB, depth, and normals, respectively.
OLN trained on RGB often detect textures or small parts of the objects.}
        \label{fig:pseudo-2}
\end{figure}


\begin{figure}
     \centering

     \begin{subfigure}[b]{0.31\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/good_detections/2.png}
\label{fig:five over x}
     \end{subfigure} 
     \begin{subfigure}[b]{0.31\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/good_detections/spaceship.png}
\label{fig:three sin x}
     \end{subfigure}
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/good_detections/aliens.png}
\label{fig:five over x}
     \end{subfigure} 
     \\
     \begin{subfigure}[b]{0.31\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/good_detections/dinosaur.png}
\label{fig:three sin x}
     \end{subfigure}
     \begin{subfigure}[b]{0.31\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/good_detections/transformers.png}
\label{fig:three sin x}
     \end{subfigure}
     \begin{subfigure}[b]{0.31\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/good_detections/dinosaurs.png}
\label{fig:five over x}
     \end{subfigure} 
     \\
     \begin{subfigure}[b]{0.33\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/good_detections/avatar.jpeg}
\label{fig:three sin x}
     \end{subfigure}
     \begin{subfigure}[b]{0.31\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/good_detections/n_1.png}
\label{fig:five over x}
     \end{subfigure} 
     \begin{subfigure}[b]{0.28\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/good_detections/erwater.jpeg}
\label{fig:five over x}
     \end{subfigure} 
     \\
      \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/good_detections/ron_man.jpeg}
\label{fig:five over x}
     \end{subfigure} 
     \begin{subfigure}[b]{0.31\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/good_detections/marvel.png}
\label{fig:three sin x}
     \end{subfigure}
     \begin{subfigure}[b]{0.31\textwidth}
         \centering
         \includegraphics[width=\textwidth]{iclr2023/figures/good_detections/trek.png}
\label{fig:five over x}
     \end{subfigure} 
     \\
    \caption{\textbf{GOOD detections on novel objects.} Only top 20 detection boxes are shown with the images. The novel objects are  seen neither in GOOD training, nor in Omnidata training set.}
        \label{fig:novel-1}
\end{figure}
 


\end{document}
