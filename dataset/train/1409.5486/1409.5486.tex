\setlength\abovedisplayskip{3pt}
  \setlength\belowdisplayskip{3pt}
  \setlength\abovedisplayshortskip{3pt}
  \setlength\belowdisplayshortskip{3pt}
\subsection{Proof of Theorem~\ref{thm:1}}
\label{appendix}

\begin{proof}
Suppose $\pbar$ satisfies $\phi$ with probability $1$, then the set of states of $M_{\Pro, \pbar}$ written as $MC_{\pbar}$ can be represented as a disjoint union of $T_{\pbar}$ transient states and $R^j_{\pbar}$ closed irreducible sets of recurrent classes~\cite{Durrett2012}:
\begin{equation} 
MC_{\pbar} = T_{\pbar} \sqcup R_{\pbar}^1 \sqcup \dotsc \sqcup R_{\pbar}^n
\end{equation}


\begin{prop}
\label{prop:rec}
Policy $\pbar$ satisfies $\phi$ with probability $1$ if and only if there exits $(\good,\bad) \in F_\Pro$ such that $\bad \in \trans$ and $\recc^j \cap \good \neq  \emptyset$ for all recurrent classes $\recc^j$.
\end{prop}

We omit the proof of Proposition~\ref{prop:rec}; however, it readily follows Definition~\ref{def:accept}.


Let $\Pi^*$ be the finite set of optimal policies that optimize the expected future utility.  We constructively show that for large enough values of $\gamma$, the discount factor and $w_B$, the negative reward on non accepting states, all policies $\pi^* \in \Pi^*$ satisfy $\phi$ with probability $1$.



Suppose $\pi^* \in \Pi^*$ does not satisfy $\phi$. Then one of the following two cases must be true:

\begin{itemize}
\item {\bf Case 1:} There exists a recurrent class $\recco^j$ such that $\recco^j \cap \good = \emptyset$. This means with policy $\pi^*$ it is possible to visit $\good$ only finitely often.
\item {\bf Case 2:} There exists $b \in \bad$ such that $b$ is recurrent. That is for some recurrent class of the $M_{\Pro, \pi^*}$,  $b \in \recco^j$. This translates to the possibility of visiting a state in $\bad$ infinitely often.
\end{itemize}

 We let $\Pi^* = \Pi_1 \cup \Pi_2$, where $\Pi_1 (\Pi_2)$ is the set of optimal policies that do not satisfy $\phi$ by violating {\bf Case 1} ({\bf Case 2}). Notice that this is not a disjoint union. 


In addition, we know that the vector of utilities for any policy $\pi^* \in \Pi^*$ is ${\bf U}_{\pi^*} \in \mathbb{R}^N$, where $N = |MC_{\pi^*}|$ is the number of states of $M_{\Pro, \pi^*}$:

\begin{equation}
\label{eq:utility}
\begin{array}{ll}
&{\bf U}_{\pi^*} = \sum_{n=0}^{\infty} \gamma^n P_{\pi^*}^n {\bf W}   
\end{array}
\end{equation}

In this equation ${\bf U}_{\pi^*} = [U_{\pi^*}(s_0) \dotsc U_{\pi^*}(s_N)]^\top$ and ${\bf W} = [W(s_0) \dotsc W(s_N)]^\top$ and $P_{\pi^*}$ is the transition probability matrix with entries $p_{\pi^*}(s_i,s_j)$ which are the probability of transitioning from $s_i$ to $s_j$ using policy $\pi^*$.

We partition the vectors in equation~\eqref{eq:utility} into its transient and recurrent classes:

\begin{equation}
\label{eq:matrixutility}
\begin{bmatrix}
\Utb_{\pi^*}\\
\Ur_{\pi^*}
\end{bmatrix} = \sum_{n=0}^{\infty} \gamma^n
\begin{bmatrix}
P_{\pi^*}(T,T) & [P_{\pi^*}^{tr_1} \dotsc  P^{tr_m}_{\pi^*}]\\
{\bf 0}_{(\sum_{i=1}^mN_i\times q)} & P_{\pi^*}(R,R)
\end{bmatrix}^n
\begin{bmatrix}
\Wtb \\
\Wr
\end{bmatrix}
\end{equation}

In equation~\eqref{eq:matrixutility}, $\Utb_{\pi^*}$ is a vector representing the utility of every transient state. Assuming we have $q$ transient states, $P_{\pi^*}(T,T)$ is a $q\times q$ probability transition matrix containing the probability of transitioning from one transient state to another. 
Assuming there are $m$ different recurrent classes, ${\bf 0}_{(\sum_{i=1}^mN_i\times q)}$ is a zero matrix representing the probability of transitioning from any of the $m$ recurrent classes, each with size $N_i$ to any of the transient states. This probability is equal to $0$ for all of these entries.


On the other hand, ${\bf P_{\pi^*}} = [ P_{\pi^*}^{tr_1}\dotsc P_{\pi^*}^{tr_m}]$ is a $q \times \sum_{i=1}^m N_i$ matrix, where each $P_{\pi^*}^{tr_k}$ is a $q\times N_k$ matrix whose elements denote the probability of transitioning from any transient state $t_j$, $j\in \{ 1,\dotsc, q\}$ to every state of the $k$th recurrent class $R_{\pi^*}^k$.


Finally, $P_{\pi^*}(R,R)$ is a block diagonal matrix with $m$ blocks of size $\sum_{i=1}^m N_i \times \sum_{i=1}^m N_i$ for every recurrent class that states the probabilities of transitioning from one recurrent state to another. It is clear that $P_{\pi^*}(R,R)$ is a stochastic matrix since each block of $N_i \times N_i$ is a stochastic matrix~\cite{Durrett2012}.
From equation~\eqref{eq:matrixutility}, we can conclude:

\begin{align}
\label{eq:ur}
\Ur_{\pi^*} & = \sum_{n=0}^\infty \gamma^n 
\begin{bmatrix}
{\bf 0} & P_{\pi^*}(R,R)^n
\end{bmatrix}
\begin{bmatrix}
\Wtb \\
\Wr
\end{bmatrix}  \\
&= \sum_{n=0}^\infty \gamma^n P_{\pi^*}^n(R,R) \Wr 
 \end{align}

Also with some approximations, a lower bound on $\Utb_{\pi^*}$ can be found:

\begin{align}
\label{eq:ut}
&\sum_{n=0}^\infty \gamma^n
\begin{bmatrix}
P_{\pi^*}^n(T,T) & {\bf P}_{\pi^*} P_{\pi^*}^n(R,R)
\end{bmatrix}
\begin{bmatrix}
\Wtb \\
\Wr
\end{bmatrix} <  \Utb_{\pi^*}  \\
&\sum_{n=0}^{\infty} \gamma^n P_{\pi^*}^n(T,T) \Wtb + \sum_{n=0}^\infty \gamma^n {\bf P}_{\pi^*}P_{\pi^*}^n(R,R)\Wr < \Utb_{\pi^*} 
\end{align}

\noindent {\bf Case 1:}\\
We first consider all policies $\pi^* \in \Pi_1$. These are policies that violate case $1$, thus for $\pi^*$ there exists some $j$ such that $\recco^j \cap \good = \emptyset$. We choose any state $s \in \recco^j$.
Then we use equation~\eqref{eq:ur} to show that any policy $\pi^*$ over state $s$ has a non-positive utility $U_{\pi^*}(s) \leq 0$.

In equation~\eqref{eq:up0}, $k_1 = \sum_{j=0}^{i-1}N_j$, $k_2 = \sum_{j=i+1}^mN_j$, ${\bf p}^{rr_i}_{\pi^*}$ is the vector that corresponds to transition probabilities from $s \in \recco^j$ to any other state in the same recurrent class using policy $\pi^*$.  ${\bf W}_j = [W(s_1^j) \dotsc W(s_{Nj}^j)]$ is the vector for the reward values of the recurrent class $\recco^j$. Since none of these states are in $\good$, we conclude that for all elements $ w \in {\bf W}_j, \:w \leq 0$. 

\begin{align}
\label{eq:up0}
U_{\pi^*}(s) =& \Urso = \sum_{n=0}^\infty \gamma^n
\begin{bmatrix}
{\bf 0}_{k_1\times q} & {\bf p}_{\pi^*}^{rr_j}&{\bf 0}_{k_2\times q}
\end{bmatrix} \Wr \\
=&\sum_{n=0}^\infty \gamma^n {\bf p}_{\pi^*}^{rr_j} {\bf W}_j \leq 0 \implies U_{\pi^*}(s) \leq 0
\end{align}
We first consider the case that $s$ is in a recurrent class of $MC_{\pbar}$.
\begin{itemize}
\setlength{\leftmargin}{0pt}
\item If $s$ is in some recurrent class $s \in \recc^j$, by proposition~\ref{prop:rec}, $\recc^j \cap \good \neq \emptyset$. Therefore, there is at least one $s_g \in \good$ such that $s_g \in \recc^j$ and $s\in \recc^j$.
In addition, we know that all states in $\bad$ are in the transient class. Therefore the vector of rewards in this recurrent class ${\bf W}_j$ as defined previously contains non-negative elements. That is for all elements $w \in {\bf W}_j,\: 0 \leq w$ and there exists at least one $ w_g \in {\bf W}_j, \: 0<w_g$.

\begin{align}
\label{eq:upbar}
0<\sum_{n=0}^\infty \gamma^n {\bf p}^{rr_j}_{\bar{\pi}} {\bf W}_j 
 \implies 0 < U_{\bar{\pi}}(s)
\end{align}

We have shown that for some $s$, and any policy $\pi^* \in \Pi_1$, $\Uo(s) < \Ubar(s)$ which contradicts the optimality assumption of $\pi^*$ for the case where $s \in \recc^j$. Thus, we must have that $s$ is in a transient class of $MC_{\pbar}$. 


\item If $s$ is in a transient class $s\in \trans$, we first find a lower bound on $\Ut_{\bar{\pi}}(s)$, and show this lower bound can be greater than any positive number for large enough choice of $\gamma$. 
Note that at minimum all the states in the transient set of $\bar{\pi}$ will have utility of $w_B <0$, that is ${\bf W}^{\text{trans}} =  {\bf W}_B=[w_B \dotsc w_B]$, and there will be only one state $s_g \in \good$ that lives in the recurrent class. That is $w_G \in \Wr$ has a positive reward. 

\begin{prop}
\label{prop:N}
For transient states $t_1, t_2 \in T$,  there exists $N < \infty$ such that:
\begin{equation}
 \sum_{n=0}^{\infty} p^n(t_1,t_2) < N,
 \end{equation}
that is, the infinite sum is bounded~\cite{Durrett2012}. 
\end{prop}



We assume $\mathfrak{q} : = |T_{\pi}|$ is the number of transient states.

In addition, $P_{\pbar}^n(R,R)$ is a stochastic matrix with row sum of $1$~\cite{Durrett2012}.


\begin{align}
\label{eq:lowerbound}
 &\sum_{n = 0}^\infty \gamma^n P_{\pbar}^n(T,T){\bf W}^{\text{tr}} +
 \gamma^n {\bf P}_{\pbar}P_{\pbar}^n(R,R) \Wr < {\bf U}_{\bar{\pi}}^{\text{tr}} \\
 &N_1 \mathbb{I}_{\mathfrak{q} \times \mathfrak{q}}{\bf W}_B  + 
 \sum_{n=0}^\infty \gamma^n {\bf P}_{\pbar}P_{\pbar}^n(R,R) \Wr  < {\bf U}_{\bar{\pi}}^{\text{tr}}
 \end{align}







\begin{prop}
\label{prop:bound}
If $p^n(s,s)$ is the probability of returning from a state $s$ to itself in $n$ time steps, there exists a lower bound on $\sum_{n=0}^\infty \gamma^n p^n(s,s)$.\\ 
First, there exists $\bar{n}$ such that $p^{\bar{n}}(s,s)$ is nonzero and bounded. That is $s$ visits itself after $\bar{n}$ time steps with a nonzero probability. \\
Also we know
$(p^{\bar{n}}(s,s))^n < p^{n\bar{n}}(s,s) $. Therefore:
\begin{align}
 \sum_{n=0}^\infty \gamma^n p^n(s,s) & >\sum_{n=0}^\infty \gamma^{n\bar{n}}p^{n\bar{n}}(s,s) \\
& >\sum_{n=0}^\infty (\gamma^{\bar{n}})^n (p^{\bar{n}}(s,s))^n  \\
&>\frac{1}{1-\gamma^{\bar{n}}} \bar{p} 
\end{align}
\end{prop}







 Going back to equation~\eqref{eq:lowerbound}, we find a stricter lower bound on the utility of every state ${\bf U}_{\bar{\pi}}^{\text{tr}}(s)$ using proposition~\ref{prop:bound}:


\begin{align}
\label{eq:case1}
& N_1w_B +  \frac{1}{1-\gamma^{\bar{n}}} \bar{m} < U_{\bar{\pi}} (s) = {\bf U}_{\bar{\pi}}^{\text{tr}} (s) \\
\label{eq:case1b}
&\text{If } 0<N_1w_B + \frac{1}{1-\gamma^{\bar{n}}} \bar{m} \\
\label{eq:case1c}
&\implies
U_{\pi^*}(s) < U_{\pbar}(s)
\end{align}
 
 Here $\bar{m} = \max (\bar{M})$ and $\bar{M} < {\bf P}_{\pbar}\bar{P}\Wr$, where $\bar{P}$ is a block matrix whose nonzero elements are $\bar{p}$ bounds derived from proposition~\ref{prop:bound}.
 
 



For a fixed $w_B$, we can select a large enough $\gamma$ so equation~\eqref{eq:case1b} holds for all $\pi^* \in \Pi_1$.
This condition implies equation~\eqref{eq:case1c} which contradicts with optimality of any $\pi^* \in \Pi_1$. Therefore, $\pi^*$ cannot be optimal unless it visits $\good$ infinitely often.
\end{itemize}

\noindent {\bf Case 2:}\\
Now we consider case $2$, where $\pi^* \in \Pi_2$.  Here for some $b \in \bad$, $b \in \recco^j $. In addition, this state is in the transient class of $\pbar$, $b \in \trans$.
Using the same procedure as the previous case, we find the following upper bound.


\begin{align}
{\bf U}_{\bar{\pi}}^{\text{tr}} &>\sum_{n = 0}^\infty \gamma^n P_{\pbar}^n(T,T){\bf W}^{\text{tr}}  \\
&>\sum_{n=0}^\infty  P_{\pbar}^n (T,T){\bf W}^{\text{tr}} \\
 \text{ (Proposition~\ref{prop:N})}\quad \quad &> N_2{\mathbb I}_{\mathfrak{q}\times \mathfrak{q}} {\bf W}_B\\
\implies & 
{\bf U}_{\bar{\pi}} (b)  > N_2w_B 
\end{align}




We know that $b$ is in the recurrent class while using policy $\pi^*$. So we can use equation~\eqref{eq:ur} to find a bound on the utility. An upper bound assumes that all the other states in the recurrent class have positive reward of $w_G$.

\begin{align}
\label{eq:ur1}
&\Ur_{\pi^*} 
 = \sum_{n=0}^\infty \gamma^n P_{\pi^*}^n(R,R) \Wr \implies \\
 &U^{\text{rec}}_{\pi^*}(b) \leq \sum_{n=0}^\infty \gamma^n w_G  + \sum_{n=0}^\infty \gamma^n p_{\pi^*}^n(b,b) w_B \\
 &< w_G \frac{1}{1-\gamma}  + w_B\sum_{n=0}^\infty \gamma^n p_{\pi^*}^n(b,b)
\end{align}

If the following condition in equation~\eqref{eq:optcondition} holds, we conclude that for a state $b$,  $U_{\pi^*}(b) < U_{\bar{\pi}}(b)$ which violates the optimality of $\pi^*$.

\begin{align}
\label{eq:optcondition}
&U_{\pi^*}(b) <  w_G \frac{1}{1-\gamma}+ w_B\sum_{n=0}^\infty \gamma^n p_{\pi^*}^n(b,b) < N_2w_B <U_{\bar{\pi}}(b)
\end{align}

We only need to enforce:
\begin{align}
\label{eq:optcondition2}
&w_G\frac{1}{1-\gamma}  + w_B\sum_{n=0}^\infty \gamma^n p_{\pi^*}^n(b,b) < N_2w_B 
\end{align}

Since there are only a finite number of policies in $\Pi_2$, from all policies $\pi^* \in \Pi_2$, we can find $\bar{p}$ such that:
\begin{equation}
\sum_{n=0}^\infty \gamma^n p_{\pi^*}^n(b,b) < \sum_{n=0}^\infty \gamma^n \bar{p}
\end{equation}

Therefore equation~\eqref{eq:optcondition2} can be simplified:
\begin{align}
\label{eq:optcondition3}
&w_G \frac{1}{1-\gamma} + w_B \sum_{n=0}^\infty \gamma^n \bar{p} < N_2w_B\\
&w_G \frac{1}{1-\gamma} + w_B \frac{1}{1- \gamma}\bar{p} < N_2w_B \\
&(w_G + w_B \bar{p})(\frac{1}{1-\gamma}) < N_2 w_B \\
\label{eq:optcondition3d}
&(w_G + w_B \bar{p}) - N_2 w_B(1-\gamma) < 0
\end{align}



We assumed without loss of generality $w_G =1$. For a fixed value of $\gamma$, we choose $w_B$ small enough so all $\pi^* \in \Pi_2$ satisfy equation~\eqref{eq:optcondition3d} and violate the optimality condition.

As a result, any optimal policy must satisfy case $2$, which is visiting a state in $\bad$ only finitely often.

For optimal policies $\pi^* \in \Pi_1 \cap \Pi_2$, we need to find $\gamma$ and $w_B$ such that both conditions for case 1 and case 2 are satisfied. That is:

\begin{equation}
\label{eq:final}
\begin{cases}
0<N_1w_B ( 1-\gamma ^ {\bar n}) +  \bar{M} \\
(1 + w_B \bar{p}) - N_2 w_B(1-\gamma) < 0
\end{cases}
\end{equation}

We select a pair of $\gamma$ and $w_B$ so the system of equations in~\eqref{eq:final} is satisfied. This solution can be found as follows:

First, for a small real number $0<\epsilon < \bar{M}$, we select $w_B^*$ so:

\begin{equation}
1 + w_B^* \bar{p} < -\epsilon
\end{equation}

Then, $\gamma^*$ is selected so the following holds:

\begin{equation}
\max \{ -N_1 w_B^* (1 - (\gamma^*)^{\bar{n}}) , -N_2 w_B^* (1 -\gamma^*) \} < \epsilon 
\end{equation}
The pair of $(w_B^*, \gamma^*)$ satisfy equation~\eqref{eq:final}, and as a result none of the policies $\pi^* \in \Pi^*$ are optimal.
\end{proof}
















