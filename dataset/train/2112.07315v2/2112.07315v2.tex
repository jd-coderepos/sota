



\section{Experiments}




\begin{table*}[bt]
\setlength{\abovecaptionskip}{0.05in}
\setlength{\belowcaptionskip}{-0.05in}
\centering
\resizebox{1.\linewidth}{!}{
\begin{tabular}{lcccccccccc}
\toprule
\multirow{2}{*}{\textbf{Method}}  & \multirow{2}{*}{\textbf{AKAB}}     & \multirow{2}{*}{\textbf{RCAB}}  & \multirow{2}{*}{\textbf{KAD}}   & \multirow{2}{*}{\textbf{Pyramid KAD}}     & \multicolumn{3}{c}{\textbf{Synthetic}}   & \multicolumn{3}{c}{\textbf{Real-World}}   \\ \cmidrule(lr){6-8} \cmidrule(lr){9-11}

    &    &      &     &      &PSNR   & SSIM  & LPIPS  &PSNR   & SSIM  & LPIPS    \\  \midrule 

KBNet-A    & \XSolidBrush     &\XSolidBrush   & \XSolidBrush     & \XSolidBrush  & 35.24   & 0.8927 & 0.1751     & 47.12   & 0.9797 & 0.0320 \\
KBNet-B    & \Checkmark     & \XSolidBrush       & \XSolidBrush     & \XSolidBrush  & 36.53 & 0.9067 & 0.1422   & 47.61   & 0.9818 & 0.0302    \\
KBNet-C    &\Checkmark    & \Checkmark       & \XSolidBrush     & \XSolidBrush  & 36.74  & 0.9142 & 0.1383     & 47.68   & 0.9821 & 0.0298   \\
KBNet-D    &\Checkmark     & \Checkmark       & \Checkmark    & \XSolidBrush   & 36.87  & 0.9185 & 0.1290     & 47.87   & 0.9830 & 0.0278   \\
KBNet-E    &\Checkmark     & \Checkmark       & \Checkmark    & \Checkmark   & 37.29  & 0.9219 & 0.1203     & 48.27   & 0.9856 & 0.0248   \\

\bottomrule
\end{tabular}}
\caption{Ablation study on our main components. The baseline is a multi-frame SR network that adopts normal deformable convolution to align frames and reconstructs SR results by several residual blocks.}
\label{table:ablation}
\end{table*}

\subsection{Datasets and Implementations}
\label{ex:setting}

\noindent \textbf{Synthetic dataset.}
Our method is trained on Zurich RAW to the RGB dataset \cite{ignatov2020replacing} which consists of 46,839 HR images. For the synthetic setting, we focus on the anisotropic Gaussian kernels. Following \cite{bell2019blind}, we fix the blur kernel size to 31, The kernel width of both axes are uniformly sampled in the range . And we also rotate the kernel by an angle uniformly distributed in . The RAW burst images are synthesized by randomly translating and rotating a high-quality sRGB image, and blurring and downsampling it with kernels generated from the above procedure as \cref{eq:mf_degrad}. In the RAW space, we add noises draw from Poisson-Gaussian distribution with sigma 0.26. Then we convert the low-quality images to RAW format using an inverse camera pipeline \cite{brooks2019unprocessing}. The test sets are generated by applying anisotropic Gaussian kernels on 1204 HR images of the validation set in \cite{ignatov2020replacing} with different kernel width ranges of ,  and . We assign different random seed values to ensure that different blur kernels are selected for different images. PSNR, SSIM \cite{wang2004image}, and the learned perceptual score LPIPS \cite{zhang2018unreasonable} are used as the evaluation metrics on synthetic datasets.


\noindent \textbf{Real-world dataset.}
For real-world image evaluation, we use the BurstSR dataset~\cite{bhat2021deep} which contains pairs of real-world burst images and corresponding ground truth HR images captured by a handheld smartphone camera and DSLR camera, respectively. Each burst in this dataset contains 14 raw images and is cropped to .
We perform super-resolution by a scale factor of 4 in all experiments. Note that the ground truth images are not well aligned with RAW inputs, thus we adopt aligned PSNR, SSIM, and LPIPS as the evaluation metrics as in \cite{bhat2021deep}.

\noindent \textbf{Training details.}
We train the proposed KBNet on aforementioned synthetic datasets for 300 epochs. And as a common practice, we fine-tune the trained model on real-world dataset for 40 epochs. During training, the burst size is fixed to  and the batch size is 16. We use Adam \cite{kingma2014adam} optimizer with =0.9, =0.999 and =. The learning rate is initialized as 0.0002 and then decreases to half every 100 epochs. Our models are implemented by the PyTorch framework with 2 Titan Xp GPUs. 


\subsection{Ablation Study}

In this section, We conduct ablation study to analyze the impact of the main components of the proposed framework: adaptive kernel-aware block (\textbf{AKAB}), kernel-aware deformable convolution (\textbf{KAD}) and \textbf{Pyramid KAD}. In addition, we also give an attention to the \textbf{RCAB} in reconstruction. To conveniently illustrate the superiority of each module, we implement a baseline model (KBNet-A) which only contains a restorer that adopts normal DCN as the alignment module and uses residual blocks for both feature extraction and HR image reconstruction. All methods are evaluated on both the synthetic data and the real data.

The comparison among the baseline and our methods with different modules (KBNet-B through KBNet-E) are reported in \Cref{table:ablation}, from which we have the following observations. First, the AKAB module plays a vital role for extracting useful features by considering multiple degradation information. Compared with the baseline, the model with AKAB improves the results about 1.3 dB on the synthetic dataset and 0.4 dB on the real dataset. Second, the degradation and multi-scale information is also essential in the alignment. By utilizing the multi-scale features and kernel maps, the Pyramid KAD could achieve an impressive performance even the burst frames are noisy and blurry. Third, although the RCAB performs well on synthetic dataset, the improvement is incremental when finetune it on the real world dataset, which further demonstrates that the AKAB and the Pyramid KAD are key contributions of our work.

We also retrain an EDVR~\cite{wang2019edvr} model and our KBNet with the Pyramid, Cascading, and Deformable (PCD) alignment module~\cite{wang2019edvr} on the datasets. The results are shown in Table \ref{table:edvr}. With the proposed pyramid KAD, our model can perform alignment with degradation information in each pyramid level, which leads to important improvement.

\begin{table}[ht]
\setlength{\abovecaptionskip}{0.05in}
\setlength{\belowcaptionskip}{-0.1in}
\centering
\resizebox{1.\linewidth}{!}{
\begin{tabular}{lcccccc}
\toprule

\multirow{2}{*}{Method} & \multicolumn{3}{c}{Synthetic}  & \multicolumn{3}{c}{Real-world}     \\ \cmidrule(lr){2-4} \cmidrule(lr){5-7}
& PSNR  & SSIM   & LPIPS    & PSNR  & SSIM   &LPIPS   \\ \midrule

EDVR~\cite{wang2019edvr} & 36.34 &0.906 &0.138  & 47.48 &0.982 &0.031 \\
KBNet+PCD  & 37.02 &0.920 &0.124  & 48.14 &0.984 &0.026        \\
KBNet+Pyramid KAD  & \textbf{37.29} & \textbf{0.922} & \textbf{0.120}  & \textbf{48.27} &\textbf{0.986} &\textbf{0.025} \\

\bottomrule
\end{tabular}}
\caption{Comparison of our method with EDVR.}
\label{table:edvr}
\end{table}


\begin{table*}[t]
\setlength{\abovecaptionskip}{0.05in}
\setlength{\belowcaptionskip}{-0.05in}
\centering
\resizebox{.98\linewidth}{!}{
\begin{tabular}{lccccccccc}
\toprule
\multirow{2}{*}{Method} & \multicolumn{3}{c}{}              & \multicolumn{3}{c}{}  & \multicolumn{3}{c}{}   \\ \cmidrule(lr){2-4} \cmidrule(lr){5-7}  \cmidrule(lr){8-10}
& PSNR  & SSIM   & LPIPS    & PSNR  & SSIM   & LPIPS  & PSNR  & SSIM   & LPIPS \\ 

\midrule
        
DAN \cite{luo2020unfolding} & 33.38 &0.8543 &0.1712        &32.69 & 0.8321 & 0.2210     &31.98 &0.8255 &0.2688  \\
    
DBSR \cite{bhat2021deep} & 35.52 &0.9086 &0.1300        &35.94 & 0.9015 & 0.1625     &33.34 &0.8633 &0.2730  \\
EBSR \cite{luo2021ebsr} & 35.67 &0.9156 &0.1149        &36.39 & 0.9095 & 0.1434     &33.57 &0.8673 &0.2669  \\
DeepREP \cite{bhat2021deepre} & 36.46 &0.9233 &0.1104        &36.26 & 0.9082 & 0.1510     &33.49 &0.8664 &0.2721  \\
    
KBNet(Ours) & \textbf{37.43} &\textbf{0.9314} &\textbf{0.0967}        &\textbf{37.27} & \textbf{0.9172} & \textbf{0.1237}     &\textbf{35.28} &\textbf{0.8924} &\textbf{0.1941}  \\

\bottomrule
\end{tabular}
}
\captionof{table}{Comparison of our method with existing MFSR approaches on the Synthetic test dataset, for scale factor 4. The kernel width  is split into three ranges. `*' means it is a single image blind super-resolution method.}
\label{table:syn}
\end{table*}

\begin{figure*}
\centering
	\begin{minipage}[h]{.97\linewidth}
	\setlength{\abovecaptionskip}{0.0in}
    \setlength{\belowcaptionskip}{0.1in}
		\centering
		\includegraphics[width=.95\linewidth]{figs/cmp_syn5.pdf}
		\caption{Qualitative comparison of our method with other MFSR approaches on \textbf{synthetic} dataset.}
		\label{fig:cmp_syn}
	\end{minipage}
	\begin{minipage}[h]{.97\linewidth}
	\setlength{\abovecaptionskip}{0.0in}
    \setlength{\belowcaptionskip}{-0.0in}
		\centering
		\includegraphics[width=.95\linewidth]{figs/cmp_real4.pdf}
		\caption{Qualitative comparison of our method with other MFSR approaches on real-world \textbf{BurstSR} dataset.}
		\label{fig:cmp_real}
	\end{minipage}
\end{figure*}


\subsection{Comparisons with State-of-the-Art Methods}

We compare KBNet with other state-of-the-art learning-based raw bursts SR methods, such as DBSR \cite{bhat2021deep}, EBSR \cite{luo2021ebsr}, and DeepREP \cite{bhat2021deepre}. Both DBSR and DeepREP are proposed by Bhat et al. \cite{bhat2021deep,bhat2021deepre}. The former uses a flow-based alignment network with an attention-based fusion to handle the raw burst inputs. The latter employs a deep reparametrization of the MAP to solve image restoration problems. And EBSR is the winner solution of the NTIRE21 Burst Super-Resolution Challenge \cite{bhat2021ntire}. All of these methods are implemented from their official code repositories and re-trained with our multiple degradation setting following Sec. \ref{ex:setting}. We also finetune these models and evaluate them on the real-world dataset. The burst sizes of all methods are fixed to 14. In addition, we compare a single image blind SR model DAN \cite{luo2020unfolding} which estimates a kernel for the first burst frame and restores the HR image conditioned on that kernel and the first LR frame. Note that DAN adopts an iteratively predicting strategy and chooses to stretch the kernel embedding in reconstruction as the same as IKC \cite{gu2019blind}.











\noindent \textbf{Evaluation on synthetic data.}
Firstly, we evaluate the proposed KBNet on the synthetic dataset as introduced in \cref{ex:setting}. Quantitative results are shown in \Cref{table:syn}. Our method achieves the best results and significantly outperforms other burst super-resolution methods. 
As the table illustrated, all the MFSR methods outperform DAN \cite{luo2020unfolding} with great improvements of 3+ dB on kernels width in range  in terms of PSNR. These MFSR mthods do not explicitly utilize degradation information in the restoring and thus are powerless when facing complex degradations. In contrast, the proposed KBNet significantly outperforms other MFSR methods over all kernel width ranges. The qualitative comparison are shown in Fig. \ref{fig:cmp_syn}. 
The super-resolved images produced by our KBNet are visually pleasant and have rich details, which demonstrates the superior of our method, and indicates that involving degradation information into restoration can help to obtain informative features and thus improve SR results.


\noindent \textbf{Evaluation on real-world data.}
Now we conduct the experiment of evaluating models that are pre-trained on synthetic dataset and finetuned on the real-world dataset. Note that the ground-truth kernels of real-world images are not available, which are required by the KBNet in the kernel estimation learning process. Alteratively, we freeze the kernel estimator and only finetune the image restorer. The quantitative results are shown in \Cref{table:real}. As we can see, the DeepREP \cite{bhat2021deepre} significantly outperforms DBSR \cite{bhat2021deep} and EBSR \cite{luo2021ebsr}, but is still inferior to the proposed KBNet. Visual comparisons on the real-world images are shown in Fig. \ref{fig:cmp_real}. It is obviously that the results produced by the KBNet have favorable perceptual quality in edges and details, and is robust to real-world noises. 



\subsection{Analysis about kernels and frame numbers.}

We assumes that all frames of a burst sequence are captured by a smartphone under the burst shooting mode in which different degradation kernels can be produced by hand-shaking or different shooting parameters. Visual examples of the estimated kernels are shown in Fig. \ref{fig:show_k}.
On the synthetic setting, most kernels can be accurately estimated by our method, which helps us to restore HR images. For real-world images, the degradation kernel should always follow the Gaussian distribution depends on the depth at each pixel and the focal length of the camera~\cite{chaudhuri1999depth}. Thus our method prefers to generate some Gaussian-like kernels despite the kernel estimator is not optimized to fit the real dataset. The experiment in Table \ref{table:real-kernel} illustrates that the estimated kernels are also useful for restoring HR images.

Moreover, we investigate the impact of multiple frames and compare KBNet with other MFSR methods. Here we don't conduct the results of EBSR because both it's training and testing require fixing the frame number to 14. The results on PSNR are shown in Fig. \ref{fig:cmp_psnr}. As the frame number increases, all MFSR methods could achieve higher performances. The proposed KBNet outperforms other methods over all frames and datasets.


\begin{figure}[t]
\setlength{\abovecaptionskip}{0.05in}
\centering
  \includegraphics[width=1.\linewidth]{figs/show_k3.pdf}
  \caption{Examples of estimated kernels and super-resolved images. \textbf{(a)} The first 6 kernels of the corresponding synthetic image. The top row of the kernel is the estimated kernels and second row of kernel is the ground-truth kernels. \textbf{(b)} The first 12 kernels of the corresponding real image.}
  \label{fig:show_k}
\end{figure}


\begin{table}[t]
\setlength{\abovecaptionskip}{0.05in}
\centering
\resizebox{1.\linewidth}{!}{
\setlength{\tabcolsep}{1.mm}{
\begin{tabular}{lccccc}
\toprule

Method & DAN \cite{luo2020unfolding}  & DBSR \cite{bhat2021deep}    & EBSR \cite{luo2021ebsr}  & DeepREP \cite{bhat2021deepre}    & KBNet(Ours)     \\  \hline

PSNR & 46.18 &47.48 &47.25   & 48.15 & \textbf{48.27}   \\
SSIM & 0.9777 &0.9824 &0.9800   & 0.9842 & \textbf{0.9856}   \\
LPIPS & 0.0389 &0.0326 &0.0356   & 0.0265 & \textbf{0.0248}   \\

\bottomrule
\end{tabular}}
}
\caption{Quantitative comparison of the proposed method with existing MFSR approaches on the real-world BurstSR 4 dataset. `*' means it is a single image blind SR method.}
\label{table:real}
\end{table}


\begin{table}[t]
\setlength{\abovecaptionskip}{0.05in}
\centering
\resizebox{.95\linewidth}{!}{
\begin{tabular}{lccc}
\toprule
Method & PSNR   & SSIM    & LPIPS   \\ \hline
KBNet \textit{w/o} using kernel & 47.54 &0.9824 &0.0299          \\
KBNet, fixed Gaussian kernel & 48.12 &0.9838 &0.0264          \\
KBNet, estimated kernel & 48.27 &0.9856 &0.0248          \\

\bottomrule
\end{tabular}
}
\caption{Different kernel strategies on real-world dataset.}
\label{table:real-kernel}
\end{table}



















\subsection{Synthetic Model Transferring}

The motivation of this experiment is that making paired SR datasets for real-world photography applications is really challenging since the LR images and GT are usually captured from different devices (e.g., smartphones and DSLR cameras). The mismatch of image qualities and colors would make it extremely difficult to train a SR model across modalities. 
In such a situation, we would like to train the model only in the multi-degradation environment and apply it to the real scenes directly, which seems like a zero-shot transferring problem. 


\begin{figure}[t]
\setlength{\abovecaptionskip}{0.04in}
\setlength{\belowcaptionskip}{-0.1in}
\centering
	\begin{minipage}[t]{0.495\linewidth}
		\centering
		\includegraphics[width=1.\linewidth]{figs/compare_psnr_syn3.pdf}
	\end{minipage}
	\begin{minipage}[t]{0.495\linewidth}
		\centering
		\includegraphics[width=1.\linewidth]{figs/compare_psnr_real3.pdf}
	\end{minipage}
	\caption{Comparison of the PSNR performances among different MFSR approaches on synthetic and real-world datasets with different number of frames.}
	\label{fig:cmp_psnr}
\end{figure}

To illustrate the idea, we provide the comparison of transferring models under bicubic degradation and blind degradation in \Cref{table:transfer}. Under the multi degradation environment, all MFSR methods can achieve higher performances compared with their bicubicly trained models. And the proposed KBNet remarkablely outperforms other methods and can produce visually pleasant results on real images, which indicate that even if the KBNet is trained on synthesized image pairs, it still has the ability to generalize to images in real applications.

\begin{table}[t]
\setlength{\abovecaptionskip}{0.05in}
\setlength{\belowcaptionskip}{-0.1in}
\centering
\resizebox{1.\linewidth}{!}{
\setlength{\tabcolsep}{1.mm}{
\begin{tabular}{lcccccc}
\toprule

\multirow{2}{*}{Method} & \multicolumn{3}{c}{Bicubic Degradation}  & \multicolumn{3}{c}{Blind Degradation}     \\ \cmidrule(lr){2-4} \cmidrule(lr){5-7}
& PSNR  & SSIM   & LPIPS    & PSNR  & SSIM   &LPIPS   \\ \midrule

DBSR \cite{bhat2021deep} & 44.64 &0.967 &0.079 & 45.18(\textcolor{red}{+0.53}) &0.974(\textcolor{red}{+0.007}) &0.048(\textcolor{cyan}{-0.031}) \\
EBSR \cite{luo2021ebsr} & 44.32 &0.963 &0.082  & 44.96(\textcolor{red}{+0.64}) &0.970(\textcolor{red}{+0.007}) &0.048(\textcolor{cyan}{-0.034})        \\
DeepREP \cite{bhat2021deepre} & 44.80 &0.968 &0.080   & 45.39(\textcolor{red}{+0.59}) &0.974(\textcolor{red}{+0.006}) &0.045(\textcolor{cyan}{-0.035})   \\
\midrule
KBNet(Ours) & - & - & -  & \textbf{45.68} &\textbf{0.979} &\textbf{0.042} \\

\bottomrule
\end{tabular}}
}
\caption{Quantitative results of transferring models from different synthetic environments  to real-world images. The improvements on each metric are marked in colors.}
\label{table:transfer}
\end{table}

 




