In this section we introduce the \acronym model.





\subsection{Preliminaries}



A graph  is defined by a set of vertices  of size  and an adjacency matrix  of size  that can be symmetric or a-symmetric. The entry  indicates an edge from vertex  to vertex .
Each vertex is associated with a vector of  real-valued features, and we denote the stacked matrix of feature vectors over all the vertices by . We denote the 'th column of this matrix by ; i.e., entry  in   holds the value of the \textsuperscript{th} feature of vertex . The \textsuperscript{th} entry of a vector  is denoted by  or . 
Recall that  is the number of walks of length  between vertex  to vertex .
We focus on two tasks: \textit{graph labeling} and \textit{vertex labeling} (each includes both classification and regression). In the former, the goal is to assign a label to the entire graph while in the latter, the goal is to assign a label to individual vertices.\footnote{Predicting properties of edges, can be performed using the dual line graph as described in the Appendix.}
In vertex labeling tasks, \acronym takes in a graph along with one of its vertices and predicts the label of that vertex. In contrast, for graph labeling tasks, \acronym is fed with a graph and predicts the label for the entire graph.
Given the variety of graph tasks, \acronym comes in multiple variants.
It's important to note that trees are also considered graphs. To prevent any confusion, we use terms like \textit{root, node, split-node, leaf} to refer to decision tree nodes, and \textit{vertex, vertices} for graph vertices.
















\subsection{A Split Function Specialized for Graphs}
For the sake of clarity, we begin by highlighting the parts of the standard DTs framework TREE-G modifies. 
During the inference phase in standard DTs, a vector example  traverses the tree until a leaf is reached and the value stored in the leaf is reported. At each split-node, the 'th feature of  is compared against a threshold, . During training, the goal is to select in each split node, the optimal index  and threshold value , with respect to some optimization criterion e.g. Gini score or the -loss.
Training is conducted greedily by converting a leaf into a decision node with two new leaves to minimize loss.

\acronym preserves the same framework, but instead of selecting a single feature that will be compared to a threshold as a split function, a more elaborate function  is used to integrate vertex features with graph structure. 
All other parts of the decision trees framework are preserved, including the greedy training process. Hence, it can be embedded in ensemble methods, such as Random Forests or Gradient Boosted Trees\footnote{In our experiments, we used GBT with TREE-G estimators} as well as be limited to a certain size either by limiting its depth during training or by using pruning.


The split function used in \acronym incorporates the vertex features, adjacency matrix, and a subset of the vertices of the graph. 
It is parameterized by  parameters  and  for vertex labeling tasks, and an additional parameter  for graph labeling tasks, as explained in details in \Secref{subsection:vertex_level} and \Secref{subsection:graph_level_tasks}. 
Overall, while the binary rule in standard trees is of the form , the binary rule in TREE-G is of the form  for vertex labeling tasks and  for graph labeling tasks. Here  is a matrix of the stacked vertex feature vectors,  is an adjacency matrix, and  is an index of a vertex in the graph.


For the rest of this paper we will focus only on the novel split function  TREE-G introduces, and how it enables learning effective decision trees for graph and vertex predictive tasks.
For the sake of clarity, we first focus on vertex labeling, and explain graph labeling in \Secref{subsection:graph_level_tasks}.























\subsubsection{\acronym for Vertex  Labeling}\label{subsection:vertex_level}
We begin with the assumption of a trained tree and demonstrate its use during the inference phase. Training details are provided in \Secref{sec:training}.
In vertex labeling, a vertex  is given for inference, together with its graph  and vertices' feature matrix .
The split function applied to the input in \acronym is based on a feature, say , propagated through walks of length  over parts of the graph. For example, for some vertex  in a graph, the question asked by the split function may be: is the sum of the \textsuperscript{th} feature, limited to the neighbors of the vertex , greater than ?  Moreover, the decision can use information only from vertices that satisfy additional constraints but we begin the exposition without such constraints and defer this discussion to later.

Splits are based on propagation of feature values over walks in the graph that are computed by the function . Here, the parameters  and  are used to define the propagation of the \textsuperscript{th} feature through walks of length  in the graph. When  the graph structure is ignored since . However, when  for example, the \textsuperscript{th} entry in   is the sum of the values of the \textsuperscript{th} feature over the neighbors of the \textsuperscript{th} vertex. In directed graphs it may be useful to consider walks in the opposite direction by taking powers of . Note also that if  is the constant feature , then  is the number of walks
of length  that end in vertex .

Next, we introduce a way to further restrict the walks used for feature propagation.  
We would like to adapt the values of  to focus on a subset of the vertices .
Hence, the split function can use a graph's substructure rather than its entirety. We present intuitive and computationally efficient methods below for this purpose.\footnote{This selection allows computing only once the powers of the adjacency matrix, as a pre-processing step. See more in the Appendix.}

\begin{enumerate}
\item \textbf{Source Walks}: Only consider walks starting in the vertices in . 
\item \textbf{Cycle Walks}: Only consider walks starting and ending in the same vertex in . 
\end{enumerate}

These restrictions can be implemented by zeroing out parts of the walks matrix  before propagating the feature values  through it.
For source walks, zero out the \textsuperscript{th} column in  for each vertex . For cycle walks, zero out all  except for the \textsuperscript{th} value on the main diagonal, for each vertex .
Hence, we define a mask matrix that corresponds to one of the two options described above (i.e., source or cycle walks), and denote it by . The parameter  indicates the type of walk restrictions, where  indicates source walks and  indicates cycle walks.  We apply  to  using an element-wise multiplication denoted by .

The key challenge is to design a policy to select the set . Clearly, we cannot add  as a feature that the DT enumerates over, since there are exponentially many and their definition would not generalize across different graphs. Our main insight involves determining the subset  only after receiving the graph . To achieve that, we utilize ancestor split-nodes to create candidate subsets of vertices to choose from.
To that end, in each split-node in \acronym two actions are performed: (1) the split function is applied to the example to determine to which child it should continue, and (2) the split-node generates two subsets of the vertices of the given graph. We denote the two generated subsets at a split-node  to be  and .
For clarity, we'll first present the full expression for the split function  and defer the detailed definitions of these subsets to \Secref{subsection:subset_generation}.

Every split-node in the tree uses one of the subsets generated by its ancestors in the tree. In a node , the parameter  points to the ancestor and the parameter  indicates if the chosen subset is  or .
Therefore, the subset  to be used in node  is well-defined when  has to evaluate the input graph  since all its ancestor nodes have already been evaluated. 
Notice that  is graph dependent, and therefore translates to different subsets for different graphs. To allow a split-node to focus on the entire graph we use the convention that if  then , regardless of the value of . 
Since the root node has no ancestors, the root node always points to itself and therefore uses all the vertices as a subset.




Finally, given a vertex example , the adjacency matrix , and the stacked feature vectors of the graph's vertices , the split function \acronym computes is defined as follows:

Then, the binary decision at a split-node will be given by the binary rule:








 \begin{figure*}[t]
    \centering
   \includegraphics[width=1\linewidth]{images/two_graphs_example.png}
    \caption{The same trained \acronym instance is applied to two graphs of different sizes during inference. Each split-node uses one subset among the subsets generated in its ancestor nodes, and the set of all vertices . The subset to use in each split-node is uniquely defined by a pointer  that points to the ancestor where the subset is generated, and  that indicates which of the two subsets generated in that ancestors should be used. Each split-node generates two subsets, by splitting its used subset.
    The subsets are computed using the same rules but translate to different subsets with respect to the given graphs.
    }
        \label{Figure:attention}
    \end{figure*}

    
\subsection{Subsets Generation}\label{subsection:subset_generation}
As mentioned above, each split-node  generates two subsets of the vertices  and , which may be consumed by its descendant split-nodes in the tree.
Recall that a split function in  uses the parameters  and a threshold . 
Informally speaking  are the vertices that increase the value of  to be greater than  while  are the vertices that decrease  to be smaller than . Formally speaking, 
the node  already uses a subset of the vertices . 
Then, the generated subset  contains the vertices among  that satisfy the criterion in \Eqref{eq:decision_func_at_node} with the selected parameters and threshold. Similarly, the generated subset  is the vertices among  that do not satisfy the criterion. That it:

where .
In particular, it holds that   and .
Notice that subsets are generated separately for every graph example, however, they are generated from the same rule. Moreover, the definition of subsets is invariant to isomorphism and it does not assume a graph size, which allows \acronym to be applied to graphs of varying sizes including sizes that were not seen during training.
Notice that in vertex-labeling tasks, each split-node splits the vertices that arrive at it into two sets, one that would proceed left and the other would proceed right when traversing the tree. At the same time, it also splits the vertices of its used subsets into two sets  and . Since the vertices that arrive at a node are not necessarily the vertices composing its used subset, it is also true that  and  do not necessarily correspond with the vertices that traverse left and right.

\Figref{Figure:attention} demonstrates how the same trained \acronym instance is applied to two graphs of different sizes during inference. Each split-node uses one subset among the subsets generated in its ancestor nodes, and the set of all vertices . The subset to use in each split-node is uniquely defined by a pointer   that points to the ancestor where the subset is generated (marked by dashed lines), and  that indicates which of the two subsets generated in that ancestors should be used. Each split-node generates two subsets, by splitting its used subset. The subsets are computed using the same rules but translate to different subsets with respect to the given graphs.
Since node  is the root, it points to itself and uses the set  of all vertices of the graph. It generates two subsets  by partitioning . For both graph examples, the same function is applied to generate , but it results in different subsets that correspond to the given graphs.
Split-node  points to its ancestor  with , therefore . It generates two subsets by partitioning its used subset  into two new subsets:  and .
Node  also points to its ancestor , with . Therefore it uses the subset , and generated two subsets by splitting it.
For the sake of clarity in this figure, we provide the generic split function. Our goal in this figure is to show the general mechanism and how the same instance is applied to different graphs. We provide a detailed example of the split functions that produce these subsets, as well as an example of a trained \acronym over a real-world dataset, in the appendix.


\Lemref{thm:GTA_ge_dt} shows the theoretical merits of a split function that can focus on subsets. Specifically, it demonstrates that without this mechanism there exist graphs with merely  vertices that are indistinguishable.

The selected subsets can also be used to explain the decisions a TREE-G makes by noting that vertices that appear more often in the used subsets are likely to have a larger influence on the prediction. Therefore, it is possible to visualize these vertices as an explainability aid. Due to space limitations, this is discussed and demonstrated at length only in the Appendix.


\subsection{Training \acronym{}s}\label{sec:training}
As \acronym preserves most of the standard DTs framework, training it uses the same greedy process as standard DTs. First, an optimization criterion is selected, for example, the Gini score or the -loss. For each leaf, a grid search is used to find the parameters that will reduce the score the most if the leaf was to become a split-node and the potential score reduction is recorded. The leaf that creates the largest score reduction is selected and is converted into a split-node using the optimal parameters that were found. This process repeats until a stop criterion is satisfied. Popular criteria include bounds on the size of the tree, bounds on the number of training examples that reach each leaf, or bounds on the loss reduction. Note that the only difference between the training process described here and the training process of standard DTs is in the grid search for the optimal split parameters where TREE-G tunes additional parameters. This induces a penalty in terms of running time that can be controlled (see \Secref{sec:theoretical_analysis}).
For example, it helps to bound the possible values of . Bounding the search space for the pointer  can also improve efficiency.
In particular, it is possible to limit every split-node to consider only pointers to ancestors at a limited distance  from it.
Empirically, we discovered that bounding  and 
is sufficient to achieve high performance without incurring a significant penalty.

In the description above we described the way a single \acronym is learned and used. Multiple \acronym{}s can be used to form ensembles using Gradient Boosted Trees~\cite{gbt}, Random Forests~\cite{rf}, or any other ensemble learning method.


\subsection{\acronym for Graph Labeling}\label{subsection:graph_level_tasks}
So far we have discussed \acronym in the context of vertex labeling tasks. 
We now shift to graph labeling tasks in which a graph is given and its label needs to be predicted. While the general structure of \acronym is preserved, some modifications are required.

In graph labeling tasks, only the adjacency matrix  and the matrix of stacked feature vectors  are given. Since there is no target vertex , instead of taking some entry of the vector  to compare against a threshold, the split function aggregates all its entries to produce a scalar. This aggregation should be permutation-invariant to be stable under graph isomorphism. We use one of  for this purpose. The selected aggregation function is specified as another parameter, , to the split function .
Hence, the split function used in graph labeling tasks is defined as follows:

In addition, for graph labeling tasks, two additional walk types are used:
\begin{enumerate}
\setcounter{enumi}{2}
\item \textbf{Target Walks}: Only consider walks ending in the vertices in .
\item \textbf{Target-Source Walks}:  Only consider walks starting and ending in  vertices of .
\end{enumerate}
Therefore the parameter  can take a value of  for target walks or  for target-source walks. Similarly to types  and , it can be implemented by masking some entries of the matrix  given a subset of the vertices .
For target walks, zero out are the rows of  corresponding to vertices not in . 
For target-source walks, zero out are the rows and columns of  corresponding to vertices not in . In the appendix we empirically show that the four walk types are not superfluous.
Notice that target masking (including target-source) is not useful and leads to redundant computation in the vertex labeling setting since in this setting only the \textsuperscript{th} coordinate of the vector  is used when making predictions for the \textsuperscript{th} vertex. However, in the graph labeling, predictions are made for the entire graph and use the entire vector.

Masking types that include zeroing out rows, eventually zero out entries in the vector . As in graph labeling tasks this vector is aggregated, zeros may affect the results of some aggregations e.g. . Therefore, in the case of target, target-source and cycle walks, the aggregation is only performed on the entries that are in the selected subset ; i.e., 


As in the vertex labeling setting, after the graph is introduced to a split node, two subsets of the vertices are generated to be used by the descendant split-nodes in the tree, using the same definitions as in vertex labeling.
In particular, the chosen aggregation is not part of the subsets generation mechanism. An exception is made when the selected aggregation function in the split-node is \emph{sum}, in which case the subsets are defined with respect to a scaled threshold  instead of . This is because in this case any value that is greater than this scaled threshold contributes to  towards passing the threshold.

 







