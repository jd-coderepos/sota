\section{Results and Discussions}
\label{sec:Results_and_Discussions}

This section analyzes the different page closure policy prediction schemes compared with using HAPPY by looking at execution time, accuracy and scalability. Before jumping to the result graphs the following summary might be helpful: \\

\begin{itemize}

\item The HAPPY implementation of Hybrid page policy is called
  Hybrid-HAPPY for brevity.

\item The HAPPY implementation of Intel-adaptive open-page policy is
  called Intel-adaptive-HAPPY for brevity.



\item The results in Figure~\ref{fig:Final_Results} and Figure~\ref{fig:hpc_results}-\ref{fig:multithread_thread_results} are
  normalized to the `static profiling'; the lower the bar
  the better performance.





\end{itemize}


\subsection{Prediction Accuracy} 

Understanding the prediction accuracy for the different types of page closure predictors has its pitfalls. For instance, prediction accuracy in the case of Hybrid predictors is straight forward as the prediction outcome is either opening or closing a page (binary classification). However, in the case of the Intel-adaptive technique the accuracy needs to be described based on the timeout value (regression). Consider a scenario where a page has to be open for 40 clock cycles to get a page hit and Intel-adaptive predicts 39 clock cycle. In this case the prediction accuracy should be calculated differently. 

To have a fair evaluation across all the predictors with a different nature of prediction, we calculate the prediction accuracy based on the Page-Hit and the Page-Miss prediction outcome. In fact, the main purpose of using page policy predictors is to increase the page hits and reduce the page misses in the DRAM. Thus, we calculate the \textit{Oracle page-hits} (maximum possible page-hits when having a perfect predictors) and \textit{Oracle page-misses} (minimum possible page-misses when having a perfect predictor) and evaluate the actual page-hits and page-misses occurred during execution time of each workloads against the oracle numbers. Figure~\ref{fig:Accuracy_graph} presents the prediction accuracy (GMEAN) of different predictors across all the workloads evaluated in this work. The open-page and close-page policies deliver the maximum prediction accuracy for page-hits and page-misses, respectively. This happens because an open-page policy leaves all the page open and then it can cover all the possible page-hits in the system and non of the page misses. A close-page policy behaves similarly but in the opposite scenario.
The hybrid-page policy delivers a moderate page-miss and page-hit prediction accuracy (around 60\%). The Intel-adaptive and fixed-open both deliver a good prediction accuracy for both page-hits (80\% and 75.8\%) and page-misses (83.5\% and 90.4\%) respectively. Overall, the HAPPY implementation of both Intel-adaptive and hybrid are slightly more accurate than the original implementation. This prediction accuracy numbers justify the execution time presented in Figure~\ref{fig:Final_Results}. Also, from the accuracy results this can be concluded that the page-hit prediction accuracy has a higher impact on the overall execution time than the page-miss prediction accuracy. 





\begin{figure}[!htb]
\centering
\includegraphics[scale=0.2]{Prediction_Accuracy.pdf}
\caption{Prediction accuracy for different predictors.}
\label{fig:Accuracy_graph}
\end{figure}


\begin{figure*}
\centering
\includegraphics[scale=0.15]{Single_Thread_Summary}
\caption{Average relative performance to static profiling for all the single-thread workloads.}
\label{fig:Final_Results}

\vspace{5 mm}

\centering
\includegraphics[scale=0.15]{HPC_Results.pdf}
\caption{Relative performance to static profiling for HPC workloads.}
\label{fig:hpc_results}

\vspace{5 mm}

\centering
\includegraphics[scale=0.15]{SPEC_Results.pdf}
\caption{Relative performance to static profiling for SPEC workloads.}
\label{fig:spec_results}

\vspace{5 mm}

\centering
\includegraphics[scale=0.15]{PARSEC_Results.pdf}
\caption{Relative performance to static profiling for PARSEC, BIOBENCH and Commercial workloads.}
\label{fig:parsec_results}

\vspace{5 mm}

\centering
\includegraphics[scale=0.15]{Multithread_Results.pdf}
\caption{Relative performance to static profiling for Multi-Core workloads.}
\label{fig:multithread_thread_results}
\end{figure*}








\subsection{Performance Analysis}
Figure~\ref{fig:Final_Results} summarizes the performance of different prediction algorithms, normalized to the `Static Profiling', for all the benchmarks. Each bargraph in this figure represents the Geometrical Mean (GMEAN) of the execution time for the number of running workloads for each category. The detailed performance of the prediction algorithms for individual workloads is presented in Figures~\ref{fig:hpc_results}--\ref{fig:parsec_results}. These figures  again confirm that a static page closure policy cannot deliver the optimum execution time for all the workloads. The corresponding workloads for HPC and SPEC benchmarks mostly prefer open-page policy. On the other hand, the corresponding workloads for PARSEC, BIOBENCH and COMMERCIAL workloads mostly prefer open-page policy.







{\bf Overview:} Our experimental results show that the best page closure prediction scheme (i.e.\ Intel-adaptive-HAPPY) delivers 5\% and 8\% better average performance across all the workloads (up to 12\% and 20\%) in comparison with open-page and close-page policy respectively. Overall, the HAPPY implementation of both Hybrid and Intel-adaptive achieved similar performance when compared with the original implementation of these page closure policies albeit with a much lower hardware overhead. Comparing the Intel-adaptive with the Intel-adaptive-HAPPY page policy shows that the HAPPY implementation can reduce the cost of implementation by 5 for the evaluated 64~GB memory size (up to 40 for a memory size of 512~GB) while improving the performance up to 2\%. Similar behavior can be observed for Hybrid and Hybrid-HAPPY. Hybrid-HAPPY shows 182,000 reduction in cost of implementation for the evaluated 64~GB memory size (up to 1.2M for memory size of 512~GB) while improving the performance up to 3\% for some workloads.

Similarly, as Figure~\ref{fig:multithread_thread_results} presents, for multi-thread applications, even with a very high MF, HAPPY performance is consistent and it delivers similar performance to the original implementation. The experimental results show that Intel-adaptive-HAPPY delivers 5\% and 14\% better average performance across all the workloads (up to 9\% and 22\%) in comparison with open-page and close-page policy respectively. 








{\bf Sensitivity to Address Mapping Schemes:} To investigate the sensitivity of HAPPY to different address mappings we select the best page closure policy (i.e.\ Intel-adaptive-HAPPY) across all the predictors presented in this paper and evaluate it with the three address mappings presented in Figure~\ref{fig:Address_Mapping_fig}. Figures~\ref{fig:Mapping_Sens_Hit_graph} and  \ref{fig:Mapping_Sens_Miss_graph} illustrate the prediction accuracy of Intel-adaptive (original and HAPPY implementation) using the different mapping schemes. These results show that the HAPPY implementation of Intel-adaptive always delivers identical or slightly better results than the original implementation no matter which address mapping is used.

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.21]{Page_Hit_Prediction_Accuracy.pdf}
\caption{Page-hit prediction accuracy with different address mappings.}
\label{fig:Mapping_Sens_Hit_graph}
\end{figure}


\begin{figure}[!htb]
\centering
\includegraphics[scale=0.21]{Page_Miss_Prediction_Accuracy.pdf}
\caption{Page-miss prediction accuracy with different address mappings.}
\label{fig:Mapping_Sens_Miss_graph}
\end{figure}






\begin{table*}[!htb]
\centering
  \begin{tabular}{| m{4cm} | m{12cm} | }
	\hline
	\multicolumn{2}{|c|}{\textbf{Multi-Core Workloads}} \\		
	\hline
	MIX1:  (w-D3-D3-F) & MIX12: (b-n-s-w)\\
	\hline
	MIX2:  (D1-D5-E-F) & MIX13: (w-D1-D5-x-y-z-t-F) \\
	\hline
	MIX3: (D1-x-y-F) & MIX14: (D1-D4-D5-x-z-x-B-F) \\
	\hline
	MIX4: (D2-D4-A-F) & MIX15: (D1-D4-D5-j-E-D5-v-F) \\
	\hline
	MIX5: (D2-D4-j-E) & MIX16: (D2-D4-D5-z-A-A-D4-F) \\
	\hline
	MIX6: (D2-y-t-F) & MIX17: (C5-C6-u-l-e-o-p-h) \\
	\hline
	MIX7: (D4-D5-g-F) & MIX18: (C13-C14C17-C18-C21-C2-C4-v-k-l-c-m-e-n-h-s) \\
	\hline
	MIX8: (D4-x-x-F) & MIX19: (C13-C18-C21-C2-C6-u-v-C21-u-l-l-o-t-p-h-s) \\
	\hline
	MIX9: (x-y-z-F) & MIX20: (C14-C17-C21-C22-C2-C4-C5-C8-C14-C21-C4-k-e-o-a-p) \\
	\hline
	MIX10: (C21-C22-C4-b) & MIX21: (C17-C21-u-C17-q-q-i-t-o-b-o-a-t-p-q-i) \\
	\hline
	MIX11: (C5-C6-u-e) & MIX22: (C18-C22-C5-C6-C8-u-k-l-d-e-n--o-p-q-h-r) \\	
	\hline

  \end{tabular}
  \caption{Randomly generated Multi-Core workloads. }
  \label{table:multicore_workloads}
\end{table*}

\subsection{Sensitivity to Memory Size}

We have evaluated HAPPY for up to 64~GB DRAM size and the results shows that HAPPY has a consistent behaviour as the memory size increase. Our experimental results suggests that the effective factor on HAPPY performance is the utilization of memory address space rather than the size of memory. For this reason, we considered a 4~GB memory organization with up to 99.8\% memory space utilization for our multithread experiments (results are presented in Figure~\ref{fig:multithread_thread_results}). Even in this situation, the results show that HAPPY delivers a competitive performance against the original implementation of both Hybrid and Intel-adaptive page policies while reduces the hardware overhead significantly. 




\subsection{Scalability with Memory Size}



Figure~\ref{fig:Scalability_graph} depicts the required storage (bytes) for each prediction algorithm for different sizes of memory. The HAPPY implementation of the hybrid prediction technique is orders of magnitude (e.g.\ up to 1.2M) cheaper than the original implementation while we show that it delivers similar performance to original implementation. In the case of Intel-adaptive page closure policy, the HAPPY implementation requires slightly more resources than the original implementation for memory sizes of less than 8~GB. However, as the memory size grows, the Intel-adaptive-HAPPY outperforms the scalability over the original implementation up to 40 for a memory size of 512~GB. Table~\ref{table:Required_Counters_For_Each_Scheme} depicts the required performance counters for different page closure policies with and without HAPPY considering a memory system with X channels, Y ranks, Z banks and W rows.

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.18]{Scalibility.pdf}
\caption{Scalability of different page closure prediction algorithms.}
\label{fig:Scalability_graph}
\end{figure}



\begin{table}[!htb]
\small
\centering
  \begin{tabular}{| c | c |}
	\hline
	\textbf{Implementation} & \textbf{Required Counters} \\
	\hline
	Hybrid &  \\
	\hline	
	Hybrid-HAPPY &  \\
	\hline
	Intel-adaptive &  \\
	\hline
	Intel-HAPPY &  \\
	\hline		
  \end{tabular}
  \caption{Required performance counters for different page closure policies.}
  \label{table:Required_Counters_For_Each_Scheme}
\end{table}














\subsection{Prediction Algorithms - Weakness \& Strength}

Due to the nature of implementation and structure of each predictor, each of them might or might not work in a specific situation. Here, we discuss such situations.

{\bf Static Policies:} the open-page policy works best for high locality workloads but degrades the performance of DRAMs significantly for the workloads with highly random  or dynamic memory accesses. The close-page policy has the completely opposite behavior. PARSEC and SPEC workloads are good examples which show the different behavior of open-page and close-page policy.



{\bf Fixed-Open:} The performance of this type of algorithms is fairly susceptible to its predefined timeout value. Similarly to the methodology presented in \cite{kaseridis2011minimalist}, we select this value to be equal to t\textsubscript{RC}, that is the minimum time limitation between consecutive accesses to different rows within a bank, in the experiments. This time delay provides enough opportunity to capture a possible page hit; it does, according to the results presented in Figure~\ref{fig:Final_Results}. However, for non-memory intensive threads with high locality or memory intensive with low locality (e.g.\ `mummer' and `tigr') this technique might not work well. The reason is that, for the first category, the time interval between memory requests might be higher than the fixed timeout value which means this technique will close the row before a page hit happens. Similarly, for the second category the time interval between memory requests might be lower than the fixed timeout value, which means that a row would be kept open for an unnecessary time which, most likely, would lead to get other page conflicts.

{\bf Hybrid:} the integrated saturating counters employed in this category (either the original or HAPPY implementation) train by the number of page-hits and page-conflicts that they face. Therefore, the prediction accuracy of these types of techniques is fairly sensitive to the distribution of page hits/conflicts within DRAMs. For instance, `streamcluster'  presents such a behavior.  

{\bf Intel-adaptive:} our experiments show that this prediction algorithm is the best across all the presented schemes in this paper. However, one weakness of this technique is the updating granularity of TR. In our experiments, every time that checking of the MC suggests a more or less aggressive page closure policy, TR is incremented or decremented by one respectively.  Updating granularity by one step (increment or decrement) delivers a fine tuning of the TR but reduces the training rate of the overall prediction technique. This means that, for workloads where the application access pattern behavior changes frequently (e.g.\ between high and low locality accesses pattern) within different time phases, the Intel-adaptive scheme might not be able deliver its best performance.  Similar behavior can be observed in `canneal' or `comm1'.

{\bf HAPPY:} so far we have just explained that advantages of HAPPY. However, like all the other proposed techniques, HAPPY also has weaknesses. Considering the global nature of a HAPPY implementation it is expected that HAPPY cannot perform as efficiently as fine grain schemes for workloads with fairly dynamic behavior targeting a small part of DRAMs locally. This can be seen in workloads like `tigr'.


\subsection{Flexibility}

HAPPY is the proof of the concept that the physical address bits can be the source of useful information that can be extracted using the right encoding and decoding techniques. This makes HAPPY a fairly flexible tool that can be applied to different prediction algorithms that have not been practical due to the cost of implementation, making them feasible. In this paper we applied HAPPY to two completely different prediction schemes and showed how the performance and scalability of these scheme improved. However, page closure policies are not the only candidate that can take advantage of the knowledge presented by HAPPY in this paper and we will present more interesting show cases in the near future.

