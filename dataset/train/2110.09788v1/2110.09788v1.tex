\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage{subfiles}


\usepackage{xr-hyper}       \usepackage{xr}
\makeatletter
\newcommand*{\addFileDependency}[1]{\typeout{(#1)}
  \@addtofilelist{#1}
  \IfFileExists{#1}{}{\typeout{No file #1.}}
}
\makeatother

\newcommand*{\myexternaldocument}[1]{\externaldocument{#1}\addFileDependency{#1.tex}\addFileDependency{#1.aux}}
\myexternaldocument{appendix_1067}


\usepackage[pagenumbers]{cvpr} 

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow, makecell} \usepackage{bbding}
\usepackage{float}
\usepackage{url}

\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}



\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


\def\cvprPaperID{1067} \def\confName{CVPR}
\def\confYear{2022}


\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 
\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\begin{document}

\title{CIPS-3D: A 3D-Aware Generator of GANs\\Based on Conditionally-Independent Pixel Synthesis}

\author{Peng Zhou, Lingxi Xie, Bingbing Ni, Qi Tian\\
Shanghai Jiao Tong University,\quadHuawei Inc.\\
{\tt\small \{zhoupengcv, nibingbing\}@sjtu.edu.cn}, {\tt\small 198808xc@gmail.com}, \\  {\tt\small tian.qi1@huawei.com}
}





\twocolumn[{\maketitle
      \vspace{-1.3cm}
      \centering
      \begin{figure}[H]
        \hsize=\textwidth
        \footnotesize
        \centering
        \renewcommand{\tabcolsep}{2pt} \renewcommand{\arraystretch}{2}
        \resizebox{\textwidth}{!}{\begin{tabular}[t]{l|r}\resizebox{!}{6cm}{\begin{tabular}[]{c}
                \includegraphics[width=\linewidth]{figures/figs/generated_images/giraffe/000042.jpg}
                \includegraphics[width=\linewidth]{figures/figs/generated_images/giraffe/000058.jpg}
                \\
                \resizebox{!}{0.7cm}{(a) GIRAFFE~\cite{niemeyer2021GIRAFFE}}
                \\
                \includegraphics[width=\linewidth]{figures/figs/generated_images/pigan/rank0_b43_idx0.jpg}
                \includegraphics[width=\linewidth]{figures/figs/generated_images/pigan/rank0_b60_idx0.jpg}
                \\
                \resizebox{!}{0.7cm}{(b) pi-GAN~\cite{chan2021piGAN}}
              \end{tabular}} \hspace{0.02cm} & \hspace{0.02cm}
            \resizebox{!}{6cm}{\centering
              \renewcommand{\tabcolsep}{5pt} \renewcommand{\arraystretch}{2}
              \begin{tabular}[]{ccc|c|cc}
\includegraphics[width=\linewidth]{figures/figs/generated_images/seed_61884897_interp_all_layers_000.jpg}    &
                \includegraphics[width=\linewidth]{figures/figs/generated_images/seed_61884897_interp_all_layers_099.jpg}    &
                \includegraphics[width=\linewidth]{figures/figs/generated_images/seed_61884897_070.jpg}                      &
                \includegraphics[width=\linewidth]{figures/figs/generated_images/seed_5467_inr_000.jpg}                      &
                \includegraphics[width=\linewidth]{figures/figs/generated_images/seed_2906319_mixed_179.jpg}                 &
                \includegraphics[width=\linewidth]{figures/figs/generated_images/seed_2906319_mixed_015.jpg}
                \\ \includegraphics[width=\linewidth]{figures/figs/generated_images/seed_47120606_000.jpg}                      &
                \includegraphics[width=\linewidth]{figures/figs/generated_images/seed_47120606_099.jpg}                      &
                \includegraphics[width=\linewidth]{figures/figs/generated_images/seed_47120606_interp_256_128_64_32_068.jpg} &
                \includegraphics[width=\linewidth]{figures/figs/generated_images/seed_7034_inr_000.jpg}                      &
                \includegraphics[width=\linewidth]{figures/figs/generated_images/seed_2906319_mixed_138.jpg}                 &
                \includegraphics[width=\linewidth]{figures/figs/generated_images/seed_2906319_mixed_104.jpg}
                \\
                \resizebox{!}{0.7cm}{Content}                                                                                & \resizebox{!}{0.7cm}{Style} & \resizebox{!}{0.7cm}{Stylization} & \resizebox{!}{0.7cm}{MetFaces} & \multicolumn{2}{c}{\resizebox{!}{0.7cm}{Stylization and multiple views}}
                \\
                \\
                \multicolumn{6}{c}{              \resizebox{!}{0.7cm}{(c) All these images are synthesized by CIPS-3D at the resolution of .}}
              \end{tabular}}
          \end{tabular}}
        \caption{Three types of 3D-aware GANs. (a): There are apparent artifacts in the images generated by GIRAFFE~\cite{niemeyer2021GIRAFFE}. (b): The images generated by pi-GAN~\cite{chan2021piGAN} are blurred and lack details. (c): CIPS-3D can generate photo-realistic high-fidelity images. We fine-tune the base model trained on FFHQ so that the transferred model can generate other types of style images. Then we interpolate the base model and the transferred model to create a new model that can generate stylized images. CIPS-3D enables one to manipulate the pose of the stylized faces (the rightmost images) explicitly. For details, please refer to~\cref{sec:finetune,sec:interpolation}.}
        \label{fig:first_page_fig}
      \end{figure}
      \vspace{-0.3cm}
    }]


\begin{abstract}
  \vspace{-0.3cm}
  The style-based GAN (StyleGAN) architecture achieved state-of-the-art results for generating high-quality images, but it lacks explicit and precise control over camera poses. The recently proposed NeRF-based GANs made great progress towards 3D-aware generators, but they are unable to generate high-quality images yet. This paper presents \textbf{CIPS-3D}, a style-based, 3D-aware generator that is composed of a shallow NeRF network and a deep implicit neural representation (INR) network. The generator synthesizes each pixel value independently without any spatial convolution or upsampling operation. In addition, we diagnose the problem of mirror symmetry that implies a suboptimal solution and solve it by introducing an auxiliary discriminator. Trained on raw, single-view images, CIPS-3D sets new records for 3D-aware image synthesis with an impressive FID of  for images at the  resolution on FFHQ. We also demonstrate several interesting directions for CIPS-3D such as transfer learning and 3D-aware face stylization. The synthesis results are best viewed as videos, so we recommend the readers to check our github project at \url{https://github.com/PeterouZh/CIPS-3D}.

\end{abstract}



\vspace{-0.3cm}
\section{Introduction}
\label{sec:intro}

Generative Adversarial Networks (GANs)~\cite{goodfellow2014Generative} can synthesize high-fidelity images~\cite{brock2018Large,karras2017Progressive,karras2019StyleBased,karras2019Analyzing,karras2020Training,karras2021AliasFree} but lack an explicit control mechanism to adjust the viewpoint for the generated object. Previous methods alleviate this problem by finding the latent semantic vectors existing in pre-trained 2D GAN models~\cite{jahanian2020steerabilitya,shen2020Interpreting,harkonen2020GANSpace,shen2020ClosedForm}. However, these methods can only roughly change the pose of the object implicitly and fail to render the object from arbitrary camera poses. Several 3D-aware methods have been proposed to enable explicit control over the camera pose~\cite{zhu2018Visual,henzler2019Escaping,nguyen-phuoc2019HoloGAN}. However, these models are often limited to low-resolution images with disappointing artifacts.

Recently, there has been a growing interest in leveraging the neural radiance fields (NeRF)~\cite{mildenhall2020NeRF} to build 3D-aware GANs. To our knowledge, there exist two types of 3D-aware generators: (i) using a pure NeRF network as the generator~\cite{schwarz2020GRAF,chan2021piGAN}; (ii) generating low-resolution feature maps with a NeRF network and then upsampling with a 2D CNN decoder~\cite{niemeyer2021GIRAFFE,anonymous2021StyleNeRF}. The former suffers from a low capacity of the generator because the NeRF network is memory-intensive, which limits the depth of the generator. Thus the synthesized images are blurred and lack sharp details (see~\cref{fig:first_page_fig}b). The latter is susceptible to aliasing due to non-ideal upsampling filters (see~\cref{fig:first_page_fig}a)~\cite{karras2021AliasFree,parmar2021Buggy}.

This paper presents CIPS-3D, an approach to synthesize each pixel value independently, just as its 2D version did~\cite{anokhin2021Image}. The  generator consists of a shallow 3D NeRF network (to alleviate memory complexity) and a deep 2D implicit neural representation (INR) network (to enlarge the capacity of the generator)~\cite{park2019DeepSDF,mescheder2019Occupancy,chen2019Learninga}, without any spatial convolution or up-sampling operations. Interestingly, the design of our generator is consistent with the well-known semantic hierarchical principle of GANs~\cite{bau2018GAN,yang2020Semantic}, where the early layers (\ie, the shallow NeRF network in our generator) determine the pose, and the middle and higher layers (\ie, the INR network in our generator) control semantic attributes and color scheme, respectively. The early NeRF network enables us to control the camera pose explicitly.

We found that CIPS-3D suffers from a mirror symmetry problem, which also exists in other 3D-aware GANs such as GIRAFFE~\cite{niemeyer2021GIRAFFE} and StyleNeRF~\cite{anonymous2021StyleNeRF}. Rather than simply attributing this issue to the dataset bias, we explain why this problem exists. Going one step further, we propose to utilize an auxiliary discriminator to regularize the output of the NeRF network, thus successfully solving this problem (see~\cref{sec:mirror_symmetry}). To train CIPS-3D at high resolution, we propose a training strategy named partial gradient backpropagation. Moreover, we provide a more efficient implementation for the Modulated Fully Connected layer (ModFC)~\cite{karras2019Analyzing,anokhin2021Image} to speed up the training (see ~\cref{sec:deep_inr}).

We validate the advantages of our approach on high-resolution face datasets including FFHQ~\cite{karras2019StyleBased}, MetFaces~\cite{karras2020Training}, BitmojiFaces~\cite{BitmojiFaces}, CartoonFaces~\cite{CartoonFaces}, and an animal dataset, AFHQ~\cite{choi2020StarGAN}. For 3D-aware image synthesis, CIPS-3D achieves state-of-the-art FID scores of  and  on FFHQ at  and  resolution, respectively, surpassing the StyleNeRF~\cite{anonymous2021StyleNeRF} proposed very recently. Moreover, we verify that CIPS-3D works pretty well in transfer learning settings and show its application for 3D-aware face stylization (see~\cref{sec:finetune,sec:interpolation}). We will release the code to the public. We hope that CIPS-3D will serve as a good base model for downstream tasks such as 3D-aware GAN inversion and 3D-aware image editing.

\section{Related Work}

Implicit neural representation is a powerful tool for representing scenes in a continuous and memory-cheap way compared to mesh/voxel-based ones. It is usually implemented by a multilayer perception (MLP). The implicit representation has been widely applied in 3D tasks~\cite{chen2019Learninga,park2019DeepSDF,mescheder2019Occupancy,saito2019PIFu,littwin2019Deep,genova2019Learning,genova2020Local} as well as some 2D tasks such as image generation~\cite{skorokhodov2021Adversarial,anokhin2021Image} and super-resolution~\cite{chen2021Learning,xu2021UltraSR}. Equipped with volume rendering~\cite{kajiya1984Ray}, NeRF-based methods~\cite{mildenhall2020NeRF,martin-brualla2021NeRF,zhang2020NeRF,chen2021MVSNeRF,wang2021NeRF,barron2021MipNeRF,jang2021CodeNeRFa,yang2021Learning} enable novel view synthesis by learning an implicit function for a specific scene. Recently, there has been a trend combining NeRF with GANs~\cite{goodfellow2014Generative,arjovsky2017Wasserstein,gulrajani2017Improved,radford2015Unsupervised} to design 3D-aware generators~\cite{schwarz2020GRAF,chan2021piGAN,niemeyer2021GIRAFFE,anonymous2021StyleNeRF,niemeyer2021CAMPARI,devries2021Unconstraineda}.

Like GIRAFFE~\cite{niemeyer2021GIRAFFE} and StyleNeRF~\cite{anonymous2021StyleNeRF}, CIPS-3D utilizes NeRF to render features instead of RGB colors. However, our method differs from GIRAFFE and StyleNeRF in several ways. Both GIRAFFE and StyleNeRF adopt a two-stage strategy, where they render low-resolution feature maps first and then upsample the feature maps using a CNN decoder. CIPS-3D synthesizes each pixel independently without any up-sampling and spatial convolution operations. Moreover, CIPS-3D represents 3D shape and appearance with NeRF and INR networks, respectively, which is convenient for transfer learning settings.







\section{Method}

\begin{figure*}[!t]
  \begin{center}
\includegraphics[width=\linewidth]{figures/framework.pdf}
  \end{center}
  \vspace{-0.65cm}
  \caption{The style-based 3D-aware generator with detailed hyperparameters. The NeRF network is shallow to save runtime memory. The INR network is deep to increase the capacity of the generator. We disentangle 3D shape and appearance, where the NeRF network is responsible for the 3D shape and the INR network for appearance. The auxiliary discriminator helps to overcome the problem of mirror symmetry (see~\cref{sec:mirror_symmetry}). For the INR network, each ModFC is followed by a LeakyReLU (not shown here). }
  \vspace{-0.5cm}
  \label{fig:framework}
\end{figure*}



\subsection{NeRF Network for 3D Shape}


\paragraph{Shallow NeRF Network}

A neural radiance field~\cite{mildenhall2020NeRF} is a continuous function  whose inputs are 3D coordinates  and viewing direction , and whose outputs are emitted colors  and volume density . A multi-layer perceptron (MLP) is usually used to parameterize the continuous function :

The 3D coordinates are sampled along camera rays. Each ray corresponds to a pixel of the rendered image. Thus to render a high-resolution image of size , the number of rays is large (\ie, ). Besides, to obtain accurate 3D shape, many points need to be sampled on each ray. As a result, the NeRF network is memory-intensive.

To alleviate memory complexity, we adopt a shallow NeRF network to represent 3D shape while assigning the task of synthesizing high-fidelity appearance to a deep 2D INR network. In particular, a shallow NeRF network (see Fig.~\ref{fig:framework}b) containing only three SIREN blocks~\cite{sitzmann2020Implicita} is used as the initial layers of the GAN's generator.



\paragraph{Modulated SIREN Block}
The vanilla NeRF is restricted to a specific scene with fixed geometry. Because the generated images are various for GANs, the shape of each image is different. To render different shapes with one NeRF, we condition the NeRF network on a noise vector  so that different shapes can be obtained by sampling  . In particular, like StyleGAN~\cite{karras2019StyleBased}, we use a mapping network  to map  to , and use  to modulate the feature maps of the NeRF network (see~\cref{fig:framework}a).

We adopt the strategy of pi-GAN~\cite{chan2021piGAN}: modulating features with FiLM~\cite{perez2017FiLM,48757}, followed by a SIREN activation function~\cite{sitzmann2020Implicita}. The modulated SIREN block is given by

where  and  represent frequency and phase, respectively. The block contains a FC layer with  and  as the weight matrix and the bias. As shown in~\cref{fig:framework}b, the NeRF network only contains three SIREN blocks to minimize runtime memory complexity.

Our experiments show that the viewing direction  will cause inconsistencies of face identities under multiple views (see~\cref{fig:ablations_xyz_d}a). Thus we do not take  as input, which is different from the original NeRF~\cite{mildenhall2020NeRF}. Furthermore, instead of predicting the color , we let the NeRF network predict a more general feature ~\cite{niemeyer2021GIRAFFE}. As a result, the proposed NeRF function is given by

where  is a feature vector corresponding to point .  is a shape code that is shared by all pixels of a generated image.



\begin{figure*}[!t]
  \centering
  \begin{minipage}[t]{0.68\linewidth}
    \begin{center}
\includegraphics[height=3.4cm]{figures/partial_grad.pdf}
    \end{center}
    \vspace{-0.6cm}
    \caption{Partial gradient backpropagation. In the training phase, the gradient calculation is turned on during the forward pass only for the green rays sampled randomly. The remaining rays do not participate in the backpropagation (the grey rays).}
    \vspace{-0.4cm}
    \label{fig:partial_grad}
  \end{minipage}\hspace{0.002\linewidth}
  \vline
  \hspace{0.005\linewidth}
  \centering
  \begin{minipage}[t]{0.3\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figures/modfc.pdf}
    \vspace{-0.7cm}
    \caption{An efficient implementation of modulated fully connected layer (ModFC) using .}
    \vspace{-1cm}
    \label{fig:modfc}
  \end{minipage}
\end{figure*}



\paragraph{Volume Rendering}
As described in~\cref{equ:nerf_feature}, the neural radiance field represents a scene as the volume density  and feature vector  at any point in space. Let  be the camera origin. For each pixel, we cast a ray  from origin  towards the pixel. We sample points along the camera ray  and transform the 3D coordinates of the points into volume densities and feature vectors using~\cref{equ:nerf_feature}. Using the classical volume rendering~\cite{kajiya1984Ray}, the overall feature vector  corresponding to the ray  is given by

where  and  are near and far bounds, respectively.





\subsection{INR Network for Appearance}
\label{sec:deep_inr}

\paragraph{Deep INR Network}
To generate images at  resolution, the NeRF network outputs feature maps of shape , where  is the feature vector calculated by~\cref{equ:volume_render}. Next, we need to convert these feature maps into the RGB space. We adopt an implicit neural representation (INR) network, where each pixel value is calculated independently~\cite{anokhin2021Image} given the feature vector . As shown in~\cref{fig:framework}c, the INR network contains nine blocks, and each block contains two modulated fully connected layers (ModFC)~\cite{karras2019Analyzing,anokhin2021Image}. We add a tRGB (\ie, a fully connected layer) layer after each block to convert the intermediate feature maps to RGB values. The final RGB values are the summation of all intermediate RGB values.


\paragraph{Partial Gradient Backpropagation}
\label{sec:partial_gradient}
Our generator network is a columnar structure without any up-sampling or down-sampling operations. Therefore, directly training it on high-resolution images is challenging due to limited GPU memory. Note that the generator synthesizes each pixel value independently. Leveraging this property, we propose a training strategy named \textit{partial gradient backpropagation} for training on high-resolution images.

As shown in Fig.~\ref{fig:partial_grad}, to synthesize an image at  resolution, we randomly sample  rays (green rays in Fig.~\ref{fig:partial_grad}), and then convert these rays into  RGB values, with the gradient calculation enabled. On the other hand, the remaining  rays (grey rays in Fig.~\ref{fig:partial_grad}) are converted to RGB values, but the gradient calculation is disabled to save memory. Finally, all the RGB values are combined into a high-resolution image, which will be presented to the discriminator. This strategy allows us to train the generator on high-resolution images. Compared with the patch-based method~\cite{schwarz2020GRAF,anokhin2021Image}, partial gradient backpropagation ensures that the discriminator observes full natural images rather than image patches at low resolution.





\paragraph{Efficient Implementation for ModFC}
Like the NeRF network, the INR network also adopts a style-based architecture. As shown in~\cref{fig:framework}d, a mapping network  turns  into , where the stochasticity of appearance comes from the code . Then,  is mapped to style vectors using affine layers (\ie, FC layer). The style vectors are injected into the INR network using Modulated Fully Connected (ModFC) layers.

CIPS~\cite{anokhin2021Image} regards ModFC as a special case of  convolutional layer and implements ModFC with the off-the-shelf modulated convolutional layer~\cite{karras2019Analyzing}. The modulated convolution is implemented using grouped convolution, which is not efficient for ModFC. In fact, we can directly utilize the batch matrix multiplication () to implement ModFC more efficiently. As shown in Fig.~\ref{fig:modfc}, ModFC consists of \textsf{Mod}, \textsf{Demod}, and a batch matrix multiplication operation. Mathematically, let  be the weights of a fully connected layer,  be a batch of style vectors, and  be the input with  being the length of the sequence. We first resize  and  to shapes of  and , respectively. The \textsf{Mod} operation is given by , where  stands for tensor-broadcasting multiplication and . The \textsf{Demod} operation is given by , where  is a small constant and . Finally, we use  to linearly map the input  (\ie, , and ), which is achieved through the batch matrix multiplication function\footnote{The function is  in PyTorch.}. Experiments substantiate that this implementation is more efficient than the implementation using grouped convolution (see~\cref{fig:params_speed}).



\subsection{Overcoming Mirror Symmetry Issue with Auxiliary Discriminator}
\label{sec:mirror_symmetry}


\begin{figure}[!t]
  \centering
  \begin{minipage}[]{\linewidth}
    \footnotesize
    \centering
    \renewcommand{\tabcolsep}{1pt} \renewcommand{\arraystretch}{1}
    \resizebox{\linewidth}{!}{\begin{tabular}{*{7}{m{0.15\textwidth}}}
\includegraphics[width=\linewidth]{figures/figs/mirror_symm/giraffe/seed_57417_000.jpg} &
        \includegraphics[width=\linewidth]{figures/figs/mirror_symm/giraffe/seed_57417_009.jpg} &
        \includegraphics[width=\linewidth]{figures/figs/mirror_symm/giraffe/seed_57417_018.jpg} &
        \includegraphics[width=\linewidth]{figures/figs/mirror_symm/giraffe/seed_57417_034.jpg} &
        \includegraphics[width=\linewidth]{figures/figs/mirror_symm/giraffe/seed_57417_050.jpg} &
        \includegraphics[width=\linewidth]{figures/figs/mirror_symm/giraffe/seed_57417_060.jpg} &
        \includegraphics[width=\linewidth]{figures/figs/mirror_symm/giraffe/seed_57417_069.jpg}
        \\
        \multicolumn{7}{c}{(a) GIRAFFE~\cite{niemeyer2021GIRAFFE} (mirror symmetry)}
        \\
        \includegraphics[width=\linewidth]{figures/figs/mirror_symm/pigan/seed_7559_022.jpg}    &
        \includegraphics[width=\linewidth]{figures/figs/mirror_symm/pigan/seed_7559_027.jpg}    &
        \includegraphics[width=\linewidth]{figures/figs/mirror_symm/pigan/seed_7559_032.jpg}    &
        \includegraphics[width=\linewidth]{figures/figs/mirror_symm/pigan/seed_7559_034.jpg}    &
        \includegraphics[width=\linewidth]{figures/figs/mirror_symm/pigan/seed_7559_037.jpg}    &
        \includegraphics[width=\linewidth]{figures/figs/mirror_symm/pigan/seed_7559_042.jpg}    &
        \includegraphics[width=\linewidth]{figures/figs/mirror_symm/pigan/seed_7559_047.jpg}
        \\
        \multicolumn{7}{c}{(b) pi-GAN~\cite{chan2021piGAN} (no mirror symmetry)}
        \\
        \includegraphics[width=\linewidth]{figures/figs/mirror_symm/no_aux/seed_8624_026.jpg}   &
        \includegraphics[width=\linewidth]{figures/figs/mirror_symm/no_aux/seed_8624_029.jpg}   &
        \includegraphics[width=\linewidth]{figures/figs/mirror_symm/no_aux/seed_8624_033.jpg}   &
        \includegraphics[width=\linewidth]{figures/figs/mirror_symm/no_aux/seed_8624_034.jpg}   &
        \includegraphics[width=\linewidth]{figures/figs/mirror_symm/no_aux/seed_8624_036.jpg}   &
        \includegraphics[width=\linewidth]{figures/figs/mirror_symm/no_aux/seed_8624_040.jpg}   &
        \includegraphics[width=\linewidth]{figures/figs/mirror_symm/no_aux/seed_8624_043.jpg}
        \\
        \multicolumn{7}{c}{(c) CIPS-3D w/o auxiliary discriminator (mirror symmetry)}
        \\
        \includegraphics[width=\linewidth]{figures/figs/mirror_symm/cips3d/seed_4840_022.jpg}   &
        \includegraphics[width=\linewidth]{figures/figs/mirror_symm/cips3d/seed_4840_027.jpg}   &
        \includegraphics[width=\linewidth]{figures/figs/mirror_symm/cips3d/seed_4840_032.jpg}   &
        \includegraphics[width=\linewidth]{figures/figs/mirror_symm/cips3d/seed_4840_034.jpg}   &
        \includegraphics[width=\linewidth]{figures/figs/mirror_symm/cips3d/seed_4840_037.jpg}   &
        \includegraphics[width=\linewidth]{figures/figs/mirror_symm/cips3d/seed_4840_042.jpg}   &
        \includegraphics[width=\linewidth]{figures/figs/mirror_symm/cips3d/seed_4840_047.jpg}
        \\
        \multicolumn{7}{c}{(d) CIPS-3D w/ auxiliary discriminator (no mirror symmetry)}
      \end{tabular}}
    \vspace{-0.3cm}
    \caption{The images of each row are synthesized with different yaw angles. Mirror symmetry exists in (a) and (c). (d): The auxiliary discriminator helps to overcome the mirror symmetry. }
    \vspace{0.3cm}
    \label{fig:mirror_symm_figs}
  \end{minipage}\\
  \centering
  \begin{minipage}[t]{0.6\linewidth}
    \centering
\includegraphics[height=2.6cm]{figures/mirror_symm.pdf}
    \vspace{-0.3cm}
    \caption{The direction of the bangs changes suddenly near the yaw angle of . Please zoom in to see the yaw angles (at the upper left corner of the images).}
    \label{fig:mirror_symm_coord}
  \end{minipage}
  \vline
  \vspace{0.01\linewidth}
  \centering
  \begin{minipage}[t]{0.37\linewidth}
    \centering
\includegraphics[width=\linewidth]{figures/figs/plot_euclidean_distance_of_pos_emb/euclidean_pos_emb.pdf}
\vspace{-0.7cm}
    \caption{As  increases, the distance between  and its symmetry point  is less than the distance between  and its neighbor . }
    \label{fig:pos_encoding}
  \end{minipage}
\end{figure}




In practice, we found a mirror symmetry problem for the generator composed of the NeRF network and the INR network. As shown in Fig.~\ref{fig:mirror_symm_figs}c and Fig.~\ref{fig:mirror_symm_coord}, the direction of the bangs changes suddenly near the yaw angle of . Interestingly, GIRAFFE~\cite{niemeyer2021GIRAFFE}, composed of a deeper NeRF network and a CNN decoder, also has this disturbing problem (see Fig.~\ref{fig:mirror_symm_figs}a). We identify two sources for the mirror symmetry: (i) the positional encoding function~\cite{mildenhall2020NeRF}, and (ii) the mirror symmetry of the input coordinates of the NeRF network.

The positional encoding function , mapping a scalar to a high-dimensional vector, is given by

where  is a scalar, and  is a hyperparameter determining the dimension of the mapping space.


\textbf{Definition 1.}
Let  and  be two metric spaces. A mapping function  is called distance preserving if for any , one has



\textbf{Proposition 1.} A positional encoding function  is given by

where  is defined by~\cref{equ:position_encoding}. Then  \footnote{ is often adopted by NeRF-based networks.} is not distance preserving.

\textit{\textbf{Proof.}}\quad
As shown in Fig.~\ref{fig:mirror_symm_coord}, let , , and , then we have

where  represents Euclidean distance, and  and  are symmetric with respect to the  plane. We apply  to , , and  respectively, and draw the distance between them in Fig.~\ref{fig:pos_encoding}. Then we know that

Supposing that  is distance preserving, according to~\cref{equ:distance_preserving} and~\cref{equ:d_ab_lt_d_ac}, we get

which contradicts the fact of ~\cref{equ:dT_ab_gt_dT_ac}. Thus  is not distance preserving and we finish the proof.
\hfill

\cref{equ:dT_ab_gt_dT_ac} shows that after positional encoding, the distance between  and its symmetry point  is less than the distance between  and its neighbor . This may cause the network to predict similar appearance for  and , causing mirror symmetry. Therefore, we discard the fixed positional encoding function and adopt a learnable strategy, \ie, the coordinates  are mapped to a high-dimensional space by a fully connected layer followed by a  activation~\cite{sitzmann2020Implicit}. However, mirror symmetry remains.

After closer inspection, we found that the essence of mirror symmetry lies in the symmetry of the coordinate system. As shown in Fig.~\ref{fig:mirror_symm_coord}, the coordinates of  and  are almost the same except that the  coordinate differs by a minus sign. The network tends to leverage the symmetry of coordinates to learn a symmetrical appearance. Note that the symmetry is a double-edged sword, and it facilitates the fitting of symmetrical objects~\cite{wu2020Unsupervised,pan20212D} such as human faces, cat faces, cars, \etc.

We notice that pi-GAN~\cite{chan2021piGAN}, whose generator is a pure NeRF network, does not suffer from mirror symmetry (see Fig.~\ref{fig:mirror_symm_figs}b). This substantiates that the discriminator can prevent the NeRF network from falling into the pitfall of mirror symmetry. However, both GIRAFFE's generator (NeRF + CNN decoder) and our generator (NeRF + INR network) suffer from mirror symmetry. It turns out that the discriminator cannot effectively regularize the NeRF network when there is a 2D network between the 3D NeRF network and the discriminator. Therefore, we propose to utilize an auxiliary discriminator to supervise the output of the NeRF network directly. In particular, the output of the NeRF network is mapped to the RGB space by a fully connected layer (see Fig.~\ref{fig:framework}b), and the RGB is presented to the auxiliary discriminator. As shown in Fig.~\ref{fig:mirror_symm_figs}d, the mirror symmetry disappeared after applying the auxiliary discriminator.









\section{Experiments}

\subsection{Implementation details}

During training, we let the virtual camera be located on the surface of a unit sphere and look at the origin. The pitch and yaw are randomly sampled from predefined distributions, respectively. Both the main discriminator and the auxiliary discriminator adopt the StyleGAN2~\cite{karras2019Analyzing} discriminator architecture, but the auxiliary discriminator has fewer channels. The model is trained with a standard non-saturating logistic GAN loss with R1 penalty~\cite{mescheder2018Which}. We adopt Adam~\cite{kingma2014Adam} to train the networks with  and . Following pi-GAN~\cite{chan2021piGAN}, we adopt a progressive training strategy where we start training at  resolution and progressively increase the resolution up to . Note that the generator architecture remains fixed, but the resolution of the generator is increased by sampling more rays. We train the model with eight Tesla V100 GPUs and batch size of . When the resolution of the generator reaches , the number of rays participating in the gradient calculation is set to  to save GPU VRAM.


\begin{table}[t]
  \caption{Comparison with SOTA on FFHQ. We computed FID and KID () between 50k generated images and all training images using the torch-fidelity library~\cite{obukhov2020torchfidelity}.  stands for quoting from the paper. CIPS-3D achieves the state-of-the-art for 3D-aware GANs, surpassing the very recent method StyleNeRF by clear margins. }
  \vspace{-0.3cm}
  \label{tab:fid_results}
  \centering
  \resizebox{0.9\linewidth}{!}{\centering
    \begin{tabular}{llcccc}
      \toprule
                           & \multicolumn{1}{l}{\multirow{2}{*}{Methods}} & \multicolumn{2}{c|}{}           & \multicolumn{2}{c}{}
      \\ \cmidrule{3-6}
      \multicolumn{2}{c}{} &                      & \multicolumn{1}{c|}{} &              &                    \\
      \midrule
      \multirow{2}{*}{2D}  & StyleGAN2~\cite{karras2019Analyzing}         &                                &                       &           &  \\
                           & CIPS~\cite{anokhin2021Image}                 &                                        &                               &                   &           \\
      \midrule
      \multirow{4}{*}{3D}  & GIRAFFE~\cite{niemeyer2021GIRAFFE}           &                                        &                               & -                        & -               \\
                           & pi-GAN~\cite{chan2021piGAN}                  &                                        &                               &                   &          \\
                           & StyleNeRF~\cite{anonymous2021StyleNeRF}      &                               &                      & -                        & -               \\
                           & CIPS-3D (ours)                               &                                &                       &          &  \\
      \bottomrule
    \end{tabular}
}
  \vspace{-0.3cm}
\end{table}

\begin{figure}[!t]
  \begin{subfigure}[]{0.49\linewidth}
    \centering
    \captionsetup{justification=centering,margin=0cm}
    \includegraphics[width=\linewidth]{figures/figs/nerfgan_pigan_r64/nerfgan_pigan_r64.pdf}
\caption{Training at  resolution.}
    \label{fig:sub:progressive_64}
  \end{subfigure}\hspace{0.01\linewidth}
  \begin{subfigure}[]{0.49\linewidth}
    \centering
    \captionsetup{justification=centering,margin=0cm}
    \includegraphics[width=\linewidth]{figures/figs/nerfgan_pigan_r128/nerfgan_pigan_r128.pdf}
\caption{Training at  resolution.}
    \label{fig:sub:progressive_128}
  \end{subfigure}
  \vspace{-0.3cm}
  \caption{Progressive training on FFHQ. Generators are initially trained at  resolution (a), then at  resolution (b). pi-GAN converges steadily at low resolution, but deteriorates at higher resolution. For efficiency of training, FID is calculated between  generated images and  real images.}
  \label{fig:progressive_training}
  \vspace{-0.3cm}
\end{figure}

\begin{figure}[!t]
  \centering
  \includegraphics[width=0.65\linewidth]{figures/figs/plot_time_comparision/param_training_speed_modfc_speed.pdf}
\vspace{-0.3cm}
  \caption{(a) and (b): CIPS-3D has more generator parameters, but its training speed is higher than that of pi-GAN which adopts a pure NeRF generator. (c): Compared with group conv,  improves the speed of ModFC considerably. Please refer to \cref{sec:exp:evaluation} for details.}
  \label{fig:params_speed}
  \vspace{-0.5cm}
\end{figure}


\subsection{Evaluation}
\label{sec:exp:evaluation}

\paragraph{Comparison with SOTA}
We evaluate image quality using Fréchet Inception Distance (FID)~\cite{heusel2017GANs} and Kernel Inception Distance (KID)~\cite{binkowski2021Demystifying}. The baseline models contain the current start-of-the-art 3D-aware GANs: GIRAFFE~\cite{niemeyer2021GIRAFFE}, pi-GAN~\cite{chan2021piGAN}, and StyleNeRF~\cite{anonymous2021StyleNeRF}. We also present the results of 2D GANs, such as StyleGAN2~\cite{karras2019Analyzing} and CIPS~\cite{anokhin2021Image}, for your reference. As shown in \cref{tab:fid_results}, our method sets new records for 3D-aware GANs on FFHQ~\cite{karras2019StyleBased} with impressive FID scores of  and  for images at  and  resolution, respectively. Note that our method even outperforms the StyleNeRF~\cite{anonymous2021StyleNeRF} proposed very recently, in terms of FID ( \textit{vs.} ) and KID ( \textit{vs.} ) at  resolution. However, our method is still inferior to the 2D state of the art, StyleGAN2, leaving room for improvement in future work.
We present some generated images in~\cref{fig:first_page_fig}. Compared to other 3D-aware GANs, CIPS-3D generates images with sharper details.


\paragraph{Progressively Growing Training}
\cref{fig:progressive_training} shows the FID curves over the course of progressive training. CIPS-3D is clearly better than pi-GAN, especially at higher resolution. Note that it is not easy to train pi-GAN at high resolution. First of all, the generator of pi-GAN is a pure NeRF architecture being memory-intensive. Secondly, as shown in~\cref{fig:sub:progressive_128}, the convergence of pi-GAN has already deteriorated in the middle resolution (\ie, ). Additionally, we found that pi-GAN is sensitive to hyperparameters, and increasing the depth and capacity of the generator may cause pi-GAN to fail to converge. CIPS-3D circumvents these difficulties by designing a new generator architecture consisting of a shallow 3D NeRF network and a deep 2D INR network.



\paragraph{Efficiency Comparison}
As shown in Fig.~\ref{fig:params_speed}a and \ref{fig:params_speed}b, CIPS-3D has more parameters but is more efficient than pi-GAN that adopts a pure NeRF generator. To increase the number of parameters, we increase the depth of the INR network ( layers) and the dimension of the fully connected layers ( dimensions). Although there are more layers, the training speed of CIPS-3D is still higher than that of pi-GAN because the 2D INR network is inherently higher than the 3D NeRF network in terms of training efficiency. \cref{fig:params_speed}c compares the speed of ModFC using different implementation methods. Compared with group conv,  improves the speed of ModFC considerably (\ie, from  batches/second to  batches/second, with batch size of , input/output dimension of ). We evaluate the speed  times and report the average runtime.














\subsection{Ablation Studies}
\label{sec:exp:ablation}

We conduct ablation studies to help understand the individual components. As shown in the first two rows of \cref{tab:ablations}, discarding the viewing direction  for the NeRF network improves FID moderately. \cref{fig:ablations_xyz_d} shows the images generated by these two comparison methods. We deliver two messages. First, incorporating the viewing direction  as input leads to inconsistencies in face identity (Fig.~\ref{fig:ablations_xyz_d}a). Second, the generator will suffer from the mirror symmetry issue regardless of whether the viewing direction  is used as input or not. We have explained the reason for the mirror symmetry in \cref{sec:mirror_symmetry} and think it is due to the inherent symmetry of the coordinate system.

The second and third rows of \cref{tab:ablations} indicate that the learnable positional encoding function (FC + ) deteriorates FID compared to the commonly used fixed positional encoding function. After adopting the auxiliary discriminator, the problem of mirror symmetry is solved, and the FID is improved as well (the third row \textit{vs.} the fourth row in \cref{tab:ablations}). Since the fixed positional encoding function (\ie, \cref{equ:proposition}) is not distance preserving, it theoretically may strengthen the problem of mirror symmetry, as analyzed in~\cref{sec:mirror_symmetry}. Thus we decided to adopt the learnable positional encoding function. Our final method includes using only coordinates as input, a learnable positional encoding function, and an auxiliary discriminator.

Next, we study the effect of the number of pixels participating in calculating the gradient on performance. \cref{fig:grad_points} plots the FID curves over the course of training. Ablations were conducted at  resolution on FFHQ. It is evident that too few pixels participating in gradient backpropagation during training will harm the performance. When the number of pixels for the gradient backpropagation reaches , the performance is comparable to that of using all pixels (\ie, ). In addition, we experimentally found that using a small number of pixels (\eg, ) can still achieve the performance of using all pixels, but at the cost of more training iterations.



\begin{figure}[!t]
  \footnotesize
  \centering
  \renewcommand{\tabcolsep}{5pt} \renewcommand{\arraystretch}{0}
  \resizebox{\linewidth}{!}{\begin{tabular}{ccc|ccc}
      \includegraphics[width=\linewidth]{figures/figs/nerf_inr_rgb/seed_5575_nerf_021.jpg} &
      \includegraphics[width=\linewidth]{figures/figs/nerf_inr_rgb/seed_5575_nerf_034.jpg} &
      \includegraphics[width=\linewidth]{figures/figs/nerf_inr_rgb/seed_5575_nerf_056.jpg} &
      \includegraphics[width=\linewidth]{figures/figs/nerf_inr_rgb/seed_7275_nerf_021.jpg} &
      \includegraphics[width=\linewidth]{figures/figs/nerf_inr_rgb/seed_7275_nerf_034.jpg} &
      \includegraphics[width=\linewidth]{figures/figs/nerf_inr_rgb/seed_7275_nerf_056.jpg}
      \\
\includegraphics[width=\linewidth]{figures/figs/nerf_inr_rgb/seed_5575_inr_021.jpg}  &
      \includegraphics[width=\linewidth]{figures/figs/nerf_inr_rgb/seed_5575_inr_034.jpg}  &
      \includegraphics[width=\linewidth]{figures/figs/nerf_inr_rgb/seed_5575_inr_056.jpg}  &
      \includegraphics[width=\linewidth]{figures/figs/nerf_inr_rgb/seed_7275_inr_021.jpg}  &
      \includegraphics[width=\linewidth]{figures/figs/nerf_inr_rgb/seed_7275_inr_034.jpg}  &
      \includegraphics[width=\linewidth]{figures/figs/nerf_inr_rgb/seed_7275_inr_056.jpg}
\end{tabular}
  }
  \vspace{-0.3cm}
  \caption{Top: Images synthesized by the NeRF network. Bottom: Corresponding images synthesized by the INR network. }
  \label{fig:nerf_inr_imgs}
  \vspace{-0.5cm}
\end{figure}



\begin{figure}[!t]
  \centering
  \begin{minipage}[]{\linewidth}
    \captionof{table}{Ablation studies on FFHQ at  resolution. We also present the images generated by each compared method in \cref{fig:ablations_xyz_d}a, \cref{fig:ablations_xyz_d}b, \cref{fig:mirror_symm_figs}c, and \cref{fig:mirror_symm_figs}d, respectively. The proposed auxiliary discriminator eliminates the problem of mirror symmetry. FID is calculated between  generated images and  real images. Please refer to \cref{sec:exp:ablation} for details.}
    \vspace{-0.3cm}
    \label{tab:ablations}
    \centering
    \resizebox{\linewidth}{!}{\begin{tabular}{ccccc|cc}
        \toprule
        \multicolumn{2}{c|}{Input} & \multicolumn{2}{c}{Positional Encoding} & \multirowcell{2}{Aux                                                                             \\ Disc} & \multirowcell{2}{Mirror                             \\ Symmetry} & \multirow{2}{*}{FID}      \\ \cmidrule{1-4}
                        & \multicolumn{1}{c|}{}              & Fixed Function       &  &                &                &         \\ \midrule
        \CheckmarkBold             & \CheckmarkBold                          & \CheckmarkBold       &                               &                & \CheckmarkBold & 
        \\ \CheckmarkBold             &                                         & \CheckmarkBold       &                               &                & \CheckmarkBold & 
        \\
\CheckmarkBold             &                                         &                      & \CheckmarkBold                &                & \CheckmarkBold & 
        \\
        \CheckmarkBold             &                                         &                      & \CheckmarkBold                & \CheckmarkBold &                & 
        \\
        \bottomrule
      \end{tabular}
    }
  \end{minipage}\\
  \centering
  \begin{minipage}[]{\linewidth}
    \footnotesize
    \centering
    \renewcommand{\tabcolsep}{0.5pt} \renewcommand{\arraystretch}{1}
    \resizebox{\linewidth}{!}{\begin{tabular}{*{7}{m{0.15\textwidth}}}
        \includegraphics[width=\linewidth]{figures/figs/ablation_xyz_d_posenc/seed_4686_011.jpg} &
        \includegraphics[width=\linewidth]{figures/figs/ablation_xyz_d_posenc/seed_4686_020.jpg} &
        \includegraphics[width=\linewidth]{figures/figs/ablation_xyz_d_posenc/seed_4686_030.jpg} &
        \includegraphics[width=\linewidth]{figures/figs/ablation_xyz_d_posenc/seed_4686_036.jpg} &
        \includegraphics[width=\linewidth]{figures/figs/ablation_xyz_d_posenc/seed_4686_043.jpg} &
        \includegraphics[width=\linewidth]{figures/figs/ablation_xyz_d_posenc/seed_4686_052.jpg} &
        \includegraphics[width=\linewidth]{figures/figs/ablation_xyz_d_posenc/seed_4686_058.jpg}
        \\
        \multicolumn{7}{c}{(a) Ablation study: }
        \\
        \includegraphics[width=\linewidth]{figures/figs/ablation_xyz_posenc/seed_3619_011.jpg}   &
        \includegraphics[width=\linewidth]{figures/figs/ablation_xyz_posenc/seed_3619_020.jpg}   &
        \includegraphics[width=\linewidth]{figures/figs/ablation_xyz_posenc/seed_3619_030.jpg}   &
        \includegraphics[width=\linewidth]{figures/figs/ablation_xyz_posenc/seed_3619_034.jpg}   &
        \includegraphics[width=\linewidth]{figures/figs/ablation_xyz_posenc/seed_3619_036.jpg}   &
        \includegraphics[width=\linewidth]{figures/figs/ablation_xyz_posenc/seed_3619_052.jpg}   &
        \includegraphics[width=\linewidth]{figures/figs/ablation_xyz_posenc/seed_3619_058.jpg}
        \\
        \multicolumn{7}{c}{(b) Ablation study: }
      \end{tabular}
    }
\vspace{-0.3cm}
    \caption{These images are generated by the ablation methods of the first two rows of \cref{tab:ablations}. The images of each row are synthesized with different yaw angles. Please zoom in to see the pitch and yaw angles (at the upper left corner of the images). (a): Incorporating the viewing direction  as input leads to inconsistencies in face identity (please compare the leftmost and rightmost images). Interestingly, StyleNeRF~\cite{anonymous2021StyleNeRF} claimed that the mirror symmetry is caused by the viewing direction . However, in our experiments, both (a) and (b) suffer from the mirror symmetry issue, indicating that the viewing direction  is not the root cause of the mirror symmetry. PE: positional encoding.}
    \vspace{-0.3cm}
    \label{fig:ablations_xyz_d}
  \end{minipage}
\end{figure}

\begin{figure}[!t]
  \centering
  \includegraphics[width=0.65\linewidth]{figures/figs/ablation_grad_points/nerfgan_pigan_r64.pdf}
\vspace{-0.3cm}
  \caption{Ablations for partial gradient computation on FFHQ at  resolution. Too few pixels for the gradient backpropagation during training harms the performance. Using more pixels will narrow the performance gap with using all pixels.}
  \vspace{-0.4cm}
  \label{fig:grad_points}
\end{figure}








\begin{figure*}[!t]
  \footnotesize
  \centering
  \resizebox{\linewidth}{!}{\renewcommand{\tabcolsep}{1pt} \renewcommand{\arraystretch}{0.9}
    \centering
    \begin{tabular}{c}

      \resizebox{\linewidth}{!}{\renewcommand{\tabcolsep}{1pt} \renewcommand{\arraystretch}{0.9}
        \centering
        \begin{tabular}{cccc|cccc}
\resizebox{!}{3cm}{
            \begin{tabular}[b]{c}
\multicolumn{1}{c}{\includegraphics[width=\linewidth]{figures/figs/freeze_nerf/seed_919_inr_000.jpg}}
            \end{tabular}}
                                           &
          \resizebox{!}{3cm}{\begin{tabular}[b]{c}
\multicolumn{1}{c}{\includegraphics[width=\linewidth]{figures/figs/freeze_nerf/seed_919_inr_035.jpg}}\\
\multicolumn{1}{c}{\includegraphics[width=\linewidth]{figures/figs/freeze_nerf/seed_919_inr_070.jpg}}
            \end{tabular}}
                                           &
\resizebox{!}{3cm}{
            \begin{tabular}[b]{c}
\multicolumn{1}{c}{\includegraphics[width=\linewidth]{figures/figs/freeze_nerf/seed_2771_inr_000.jpg}}
            \end{tabular}}
                                           &
          \resizebox{!}{3cm}{\begin{tabular}[b]{c}
\multicolumn{1}{c}{\includegraphics[width=\linewidth]{figures/figs/freeze_nerf/seed_2771_inr_035.jpg}}\\
\multicolumn{1}{c}{\includegraphics[width=\linewidth]{figures/figs/freeze_nerf/seed_2771_inr_070.jpg}}
            \end{tabular}}

          \hspace{0.01cm}                  & \hspace{0.01cm}
\resizebox{!}{3cm}{
            \begin{tabular}[b]{c}
\multicolumn{1}{c}{\includegraphics[width=\linewidth]{figures/figs/freeze_nerf/seed_872_inr_000.jpg}}
            \end{tabular}}
                                           &
          \resizebox{!}{3cm}{\begin{tabular}[b]{c}
\multicolumn{1}{c}{\includegraphics[width=\linewidth]{figures/figs/freeze_nerf/seed_872_inr_035.jpg}}\\
\multicolumn{1}{c}{\includegraphics[width=\linewidth]{figures/figs/freeze_nerf/seed_872_inr_070.jpg}}
            \end{tabular}}
                                           &
\resizebox{!}{3cm}{
            \begin{tabular}[b]{c}
\multicolumn{1}{c}{\includegraphics[width=\linewidth]{figures/figs/freeze_nerf/seed_4851_inr_000.jpg}}
            \end{tabular}}
                                           &
          \resizebox{!}{3cm}{\begin{tabular}[b]{c}
\multicolumn{1}{c}{\includegraphics[width=\linewidth]{figures/figs/freeze_nerf/seed_4851_inr_035.jpg}}\\
\multicolumn{1}{c}{\includegraphics[width=\linewidth]{figures/figs/freeze_nerf/seed_4851_inr_070.jpg}}
            \end{tabular}}
          \\
          \multicolumn{4}{c}{MetFaces}     & \multicolumn{4}{c}{BitmojiFaces}
          \\
\resizebox{!}{3cm}{
            \begin{tabular}[b]{c}
\multicolumn{1}{c}{\includegraphics[width=\linewidth]{figures/figs/freeze_nerf/seed_3562_inr_000.jpg}}
            \end{tabular}}
                                           &
          \resizebox{!}{3cm}{\begin{tabular}[b]{c}
\multicolumn{1}{c}{\includegraphics[width=\linewidth]{figures/figs/freeze_nerf/seed_3562_inr_035.jpg}}\\
\multicolumn{1}{c}{\includegraphics[width=\linewidth]{figures/figs/freeze_nerf/seed_3562_inr_070.jpg}}
            \end{tabular}}
                                           &
\resizebox{!}{3cm}{
            \begin{tabular}[b]{c}
\multicolumn{1}{c}{\includegraphics[width=\linewidth]{figures/figs/freeze_nerf/seed_3568_inr_000.jpg}}
            \end{tabular}}
                                           &
          \resizebox{!}{3cm}{\begin{tabular}[b]{c}
\multicolumn{1}{c}{\includegraphics[width=\linewidth]{figures/figs/freeze_nerf/seed_3568_inr_035.jpg}}\\
\multicolumn{1}{c}{\includegraphics[width=\linewidth]{figures/figs/freeze_nerf/seed_3568_inr_070.jpg}}
            \end{tabular}}

          \hspace{0.01cm}                  & \hspace{0.01cm}
\resizebox{!}{3cm}{
            \begin{tabular}[b]{c}
\multicolumn{1}{c}{\includegraphics[width=\linewidth]{figures/figs/freeze_nerf/seed_6774_inr_000.jpg}}
            \end{tabular}}
                                           &
          \resizebox{!}{3cm}{\begin{tabular}[b]{c}
\multicolumn{1}{c}{\includegraphics[width=\linewidth]{figures/figs/freeze_nerf/seed_6774_inr_035.jpg}}\\
\multicolumn{1}{c}{\includegraphics[width=\linewidth]{figures/figs/freeze_nerf/seed_6774_inr_070.jpg}}
            \end{tabular}}
                                           &
\resizebox{!}{3cm}{
            \begin{tabular}[b]{c}
\multicolumn{1}{c}{\includegraphics[width=\linewidth]{figures/figs/freeze_nerf/seed_3336_inr_000.jpg}}
            \end{tabular}}
                                           &
          \resizebox{!}{3cm}{\begin{tabular}[b]{c}
\multicolumn{1}{c}{\includegraphics[width=\linewidth]{figures/figs/freeze_nerf/seed_3336_inr_035.jpg}}\\
\multicolumn{1}{c}{\includegraphics[width=\linewidth]{figures/figs/freeze_nerf/seed_3336_inr_070.jpg}}
            \end{tabular}}
          \\
          \multicolumn{4}{c}{CartoonFaces} & \multicolumn{4}{c}{AFHQ}
          \\
          \multicolumn{8}{c}{\resizebox{!}{0.25cm}{(a) Images synthesized by transferred models.}}
\end{tabular}
      }
      \\
      \midrule
      \resizebox{\linewidth}{!}{\renewcommand{\tabcolsep}{3pt} \renewcommand{\arraystretch}{3}
        \centering
        \begin{tabular}{c|cccccccc}
\includegraphics[width=\linewidth]{figures/figs/interpolation/seed_9283765_mixed_035.jpg}
          \includegraphics[width=\linewidth]{figures/figs/interpolation/seed_9283765_mixed_000.jpg}  \hspace{0.4cm}  & \hspace{0.4cm}
          \includegraphics[width=\linewidth]{figures/figs/interpolation/seed_9283765_interp_000.jpg}                 &
          \includegraphics[width=\linewidth]{figures/figs/interpolation/seed_9283765_interp_012.jpg}                 &
          \includegraphics[width=\linewidth]{figures/figs/interpolation/seed_9283765_interp_024.jpg}                 &
          \includegraphics[width=\linewidth]{figures/figs/interpolation/seed_9283765_interp_036.jpg}                 &
          \includegraphics[width=\linewidth]{figures/figs/interpolation/seed_9283765_interp_048.jpg}                 &
          \includegraphics[width=\linewidth]{figures/figs/interpolation/seed_9283765_interp_060.jpg}                 &
          \includegraphics[width=\linewidth]{figures/figs/interpolation/seed_9283765_interp_070.jpg}                 &
          \includegraphics[width=\linewidth]{figures/figs/interpolation/seed_9283765_interp_080.jpg}
          \\ \includegraphics[width=\linewidth]{figures/figs/interpolation/seed_75123153_mixed_044.jpg}
          \includegraphics[width=\linewidth]{figures/figs/interpolation/seed_75123153_mixed_000.jpg}  \hspace{0.4cm} & \hspace{0.4cm}
          \includegraphics[width=\linewidth]{figures/figs/interpolation/seed_75123153_interp_000.jpg}                &
          \includegraphics[width=\linewidth]{figures/figs/interpolation/seed_75123153_interp_012.jpg}                &
          \includegraphics[width=\linewidth]{figures/figs/interpolation/seed_75123153_interp_024.jpg}                &
          \includegraphics[width=\linewidth]{figures/figs/interpolation/seed_75123153_interp_036.jpg}                &
          \includegraphics[width=\linewidth]{figures/figs/interpolation/seed_75123153_interp_048.jpg}                &
          \includegraphics[width=\linewidth]{figures/figs/interpolation/seed_75123153_interp_060.jpg}                &
          \includegraphics[width=\linewidth]{figures/figs/interpolation/seed_75123153_interp_080.jpg}                &
          \includegraphics[width=\linewidth]{figures/figs/interpolation/seed_75123153_interp_099.jpg}
          \\ \includegraphics[width=\linewidth]{figures/figs/interpolation/seed_9117009_mixed_067.jpg}
          \includegraphics[width=\linewidth]{figures/figs/interpolation/seed_9117009_mixed_000.jpg}  \hspace{0.4cm}  & \hspace{0.4cm}
          \includegraphics[width=\linewidth]{figures/figs/interpolation/seed_9117009_interp_000.jpg}                 &
          \includegraphics[width=\linewidth]{figures/figs/interpolation/seed_9117009_interp_012.jpg}                 &
          \includegraphics[width=\linewidth]{figures/figs/interpolation/seed_9117009_interp_024.jpg}                 &
          \includegraphics[width=\linewidth]{figures/figs/interpolation/seed_9117009_interp_036.jpg}                 &
          \includegraphics[width=\linewidth]{figures/figs/interpolation/seed_9117009_interp_048.jpg}                 &
          \includegraphics[width=\linewidth]{figures/figs/interpolation/seed_9117009_interp_060.jpg}                 &
          \includegraphics[width=\linewidth]{figures/figs/interpolation/seed_9117009_interp_080.jpg}                 &
          \includegraphics[width=\linewidth]{figures/figs/interpolation/seed_9117009_interp_099.jpg}
          \\ \includegraphics[width=\linewidth]{figures/figs/interpolation/seed_6133434_mixed_044.jpg}
          \includegraphics[width=\linewidth]{figures/figs/interpolation/seed_6133434_mixed_000.jpg}  \hspace{0.4cm}  & \hspace{0.4cm}
          \includegraphics[width=\linewidth]{figures/figs/interpolation/seed_6133434_interp_000.jpg}                 &
          \includegraphics[width=\linewidth]{figures/figs/interpolation/seed_6133434_interp_012.jpg}                 &
          \includegraphics[width=\linewidth]{figures/figs/interpolation/seed_6133434_interp_024.jpg}                 &
          \includegraphics[width=\linewidth]{figures/figs/interpolation/seed_6133434_interp_036.jpg}                 &
          \includegraphics[width=\linewidth]{figures/figs/interpolation/seed_6133434_interp_048.jpg}                 &
          \includegraphics[width=\linewidth]{figures/figs/interpolation/seed_6133434_interp_060.jpg}                 &
          \includegraphics[width=\linewidth]{figures/figs/interpolation/seed_6133434_interp_070.jpg}                 &
          \includegraphics[width=\linewidth]{figures/figs/interpolation/seed_6133434_interp_080.jpg}
          \\
          \resizebox{!}{2.0cm}{(b) Stylization (multi-views)}                                                        & \resizebox{!}{1.5cm}{Content} & \multicolumn{6}{c}{\resizebox{!}{2.2cm}{(c) Images synthesized by interpolated models (linear interpolation).}} & \resizebox{!}{1.5cm}{Style}
        \end{tabular}
      }
    \end{tabular}
  }
  \vspace{-0.3cm}
  \caption{(a): Fine-tuning the base model trained on FFHQ to generate images in other domains (please refer to~\cref{sec:finetune} for details). (b) and (c): Interpolating the base model and the transferred model to generate stylized images (please refer to~\cref{sec:interpolation} for details). CIPS-3D enables us to manipulate the pose of the generated faces explicitly. }
  \label{fig:finetune_and_interpolation}
  \vspace{-0.5cm}
\end{figure*}



\subsection{Transfer Learning}
\label{sec:finetune}

In~\cref{fig:nerf_inr_imgs}, we show the output images of the NeRF network and the corresponding output images of the INR network. The INR output images share the same pose as the NeRF images but own much richer textures. This indicates that the NeRF network is responsible for the posture and the INR network is responsible for the texture. Inspired by the FreezeG~\cite{FreezeG} which freezes the early layers of the generator, we freeze the NeRF network of our generator trained on FFHQ and only fine-tune the INR network for transfer learning settings. Freezing NeRF is critical for transfer learning because there are too few images in the target domain. It is almost impossible to learn 3D shapes from so few images. However, by reusing the weights of NeRF, we only need to update the 2D INR network to render textures of other domains.

We verified the efficacy of the method on four datasets, including MetFaces~\cite{karras2020Training}, BitmojiFaces~\cite{BitmojiFaces}, CartoonFaces~\cite{CartoonFaces}, and even the animal dataset, AFHQ~\cite{choi2020StarGAN}. As shown in~\cref{fig:finetune_and_interpolation}a, the transferred models generate images of different domains. Moreover, we can easily control the pose of the generated faces by explicitly manipulating the NeRF network.



\subsection{3D-aware Face Stylization}
\label{sec:interpolation}


We consider interpolating the original base model trained on FFHQ and the transferred models (fine-tuned on target datasets) to create new models to generate images in novel domains. Note that the weights of the NeRF network of the base mode and the transferred models are completely equal. We test two types of interpolating methods: (i) linearly interpolating all INR layers, and (ii) replacing the higher layers of the INR network of the base model with the corresponding layers of the transferred model~\cite{pinkney2020Resolution,huang2020Unsupervised}. The former produces images between two domains, as shown in~\cref{fig:finetune_and_interpolation}c. The images smoothly fade from one domain to another. The latter generates images that combine the structural characteristics of the content images and the style characteristics of the style images (see~\cref{fig:finetune_and_interpolation}b). Besides, CIPS-3D allows one to manipulate the pose of all the stylized faces explicitly.




\vspace{-0.1cm}
\section{Limitation and Future Work}
\vspace{-0.1cm}
CIPS-3D works in a noise-to-image manner. Thus current stylization is limited to randomly generated images. To edit a real image, we need to project the image to the latent space of the generator. For this purpose, we need to study 3D-aware GAN inversion, and we leave this to future work.




\vspace{-0.1cm}
\section{Conclusion}
\vspace{-0.1cm}

This paper presents a style-based 3D-aware generator that synthesizes pixel values independently without any spatial convolution or up-sampling operation. We find that the symmetry of the input coordinates leads to the problem of mirror symmetry and propose to utilize an auxiliary discriminator to solve this problem. We look forward to applying the proposed generator to more interesting applications such as 3D-aware GAN inversion and image-to-image translation.


\clearpage
{\small
  \bibliographystyle{ieee_fullname}
  \bibliography{egbib}
}

\clearpage


\end{document}
