

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  



\IEEEoverridecommandlockouts                              

\usepackage{graphics} \usepackage{epsfig} 

\renewcommand{\rmdefault}{ptm}
\renewcommand{\sfdefault}{phv}


\usepackage{amsmath} \usepackage{amssymb}  \usepackage{caption}
\usepackage{subfig}
\usepackage{multirow}
\usepackage{color}

\usepackage{amssymb, eucal, xfrac}
\usepackage[margin=6pt,font=small,labelfont=bf,labelsep=endash,tableposition=top]{caption}

\def\spadesuit{\dag}
\def\spadesuit{\dag}
\def\clubsuit{\ddag}


\usepackage{xcolor}		
\usepackage[colorlinks = true,
            linkcolor = purple,
            urlcolor  = blue,
            citecolor = cyan,
            anchorcolor = black]{hyperref}	
            
            
            
            
            
\title{\LARGE \bf FastFlowNet: A Lightweight Network for Fast Optical Flow Estimation
}


\author{Lingtong Kong, 
~ ~ ~  Chunhua Shen, 
 ~ ~ ~ Jie Yang\thanks{ Institute of Image Processing and Pattern Recognition, Shanghai Jiao Tong University, China.  The University of Adelaide, Australia.}
\thanks{ Corresponding author.
}
}




\begin{document}


\maketitle
\thispagestyle{empty}
\pagestyle{empty}


\begin{abstract}

    Dense optical flow estimation plays a key role in many robotic vision tasks. 
In the past few years, 
    with the advent of deep learning,  we have witnessed great progress in optical flow estimation. 
    However, current networks often consist of 
    a large number of parameters and require heavy computation costs, 
largely hindering its application on low power-consumption devices such as mobile phones.
In this paper, we tackle this challenge and design a lightweight model
for fast and accurate optical flow prediction.
    Our proposed FastFlowNet follows the widely-used
    \textit{coarse-to-fine} paradigm
    with following innovations. First, a new head enhanced pooling pyramid (HEPP) feature extractor is employed to intensify high-resolution pyramid features while reducing parameters. Second, we introduce a new center dense dilated correlation (CDDC) layer for constructing compact cost volume that can keep large search radius with reduced computation burden. Third, an efficient shuffle block decoder (SBD) is implanted into each pyramid level to accelerate flow estimation with marginal drops in accuracy. Experiments on both synthetic Sintel data and real-world KITTI datasets demonstrate the effectiveness of the proposed approach, which needs 
    only   computation of comparable networks to achieve 
\textit{on par} accuracy.  
    In particular, FastFlowNet only contains 1.37M parameters; and can
execute 
    at 90 FPS (with a single GTX 1080Ti) or 5.7 FPS (embedded Jetson TX2 GPU)
on a pair of Sintel images of resolution  .


Code is 
available at: 
\def\UrlFont{\tt \color{blue}}
\url{https://git.io/fastflow}



\end{abstract}


\section{Introduction}

Optical flow estimation is a fundamental task in computer vision and 
benefits 
a variety of downstream vision tasks, including target tracking~\cite{9093517}, autonomous navigation~\cite{8460569}, obstacle avoidance \cite{Kahlouche_2007} and action-based human-robot interaction~\cite{8793825,Yang_2021}. Given two time-adjacent image frames,  optical flow
offers rich information for robotics to interact in complex dynamic environments,
by estimating the projected 2D velocity field on the image plane, which is caused by relative 3D motion between an observer and a scene.  
However, extracting  accurate 
motion information from RGB images 
is a complicated and computationally intensive task.
Decades of research efforts have been spent on optical flow estimation. 
While traditional methods~\cite{Horn_1981,Brox_2011,Zach_2007,Kroeger_2016} of optimizing an energy function with brightness constancy and spatial smoothness usually fail in large movement and illumination change cases, recent deep learning approaches significantly surpass them in both accuracy and speed thanks to large synthetic datasets and powerful GPUs. Existing Convolutional Neural Networks (CNNs) based architectures can be categorized into two classes:
the 
encoder-decoder structure and the coarse-to-fine structure.

\begin{figure}[t]
	\centering
	\vspace{0.8 em}
	\includegraphics[width=0.45\textwidth]{fig1.pdf}
	\caption{FastFlowNet can estimate accurate optical flow in real-time on embedded devices, which consumes only  computation of comparable networks to achieve \textit{on par} performance.}
	\label{fig1}
\end{figure}



Representative works of the first category 
include  
FlowNet~\cite{Fischer_2015} and FlowNet2 \cite{Ilg_2017}. The pioneering FlowNet~\cite{Fischer_2015} designs  
two versions of models,  named FlowNetS and FlowNetC. They both adopt the U-shape network. Specifically, FlowNetS only concatenates two input images while FlowNetC also contains a correlation layer. The successor FlowNet2~\cite{Ilg_2017} further cascades multiple FlowNet models and 
employs 
carefully designed learning schedules on multiple datasets. FlowNet2 shows that 
CNNs can learn to estimate accurate optical flow with orders of magnitude faster than traditional competitors. However, these networks typically use a 
large number of channels in low resolutions for feature encoding, resulting in large
model sizes, \textit{e.g.}, FlowNet2 containing more than 160M parameters. Moreover, the correlation layer in FlowNetC locates at a relative high resolution with large search radius, which leads to heavy computation burden. Both shortcomings  
have hindered them for lightweight mobile applications.



On the other hand, some approaches attempt 
to estimate residual flow among decomposed spatial levels where SPyNet~\cite{Ranjan_2017} 
may be the first one  in this category.
SPyNet  first constructs image pyramids, then concatenates the upsampled prior flow, first image and warped second image to estimate residual flow fields in a coarse-to-fine manner. Since each sub-module is only responsible for the small magnitude residual flow, SPyNet~\cite{Ranjan_2017} can achieve the 
accuracy of FlowNet~\cite{Fischer_2015} with only 1.2M parameters. 
Almost at the same time, PWC-Net~\cite{Sun_2018_CVPR} and LiteFlowNet~\cite{Hui_2018_CVPR} replace image pyramids  with better feature pyramids  and introduce the correlation layer into each spatial level for better correspondence representation. Highly ranked results confirm the effectiveness of coarse-to-fine based approaches.
As a result, following works appeared along this line of research \cite{Hur_2019, Yin_2019, zhao2020maskflownet, Kong_2020, hofinger2019improving}.





Most recent works on optical flow \cite{zhao2020maskflownet, hofinger2019improving, teed2020raft} have focused almost exclusively on pursuing accuracy, the resulting methods with intensive computation  are hardly deployed to 
energy-constrained embedded devices, such as unmanned aerial vehicles (UAV)
and mobile phones.
For example, as we 
show later, 
the efficient LiteFlowNet~\cite{Hui_2018_CVPR} barely breaks the 1 FPS barrier when inferring Sintel resolution images () on Jetson TX2, which is still far from practical deployment.




To speed up accurate optical flow estimation and facilitate practical applications, we propose a lightweight and fast network,
termed 
\textbf{FastFlowNet} in this paper. Our model is based on the widely used coarse-to-fine residual structure and we improve it on three aspects: 1) 
\textit{pyramid feature extraction}, 2) \textit{cost volume construction} and 3) \textit{optical flow decoding}, covering all components of flow estimation pipeline. 

First, we present a head enhanced pooling pyramid (HEPP) feature extractor, which uses convolution layers at 
higher levels while adopting parameter-free pooling at 
other lower levels. This efficient module can be viewed 
as a combination of feature pyramid in PWC-Net~\cite{Sun_2018_CVPR} and pooling pyramid of SPyNet~\cite{Ranjan_2017}, which 
aims to extract good matching features  with  considerably 
reduced parameters and computation. Second, recent studies \cite{Sun_2018_CVPR, hofinger2019improving} have shown that increasing the  search radius of the correlation layer can improve flow accuracy. However, feature channels of the cost volume are squared in terms of the search radius and computation complexity of following decoder network is  4th power 
of the search radius. In FastFlowNet, we keep the search radius to be 4 as in  PWC-Net~\cite{Sun_2018_CVPR} for perceiving large movement. Differently, to reduce computation burden, we down sample feature channels in large residual regions and propose a new 
center dense dilated correlation (CDDC) layer for constructing compact cost volume. Our motivation is that residual flow distributions are more focused on small motions, and experiments show that CDDC behaves better than other compression methods. Third, we observe that flow decoders at 
each pyramid level occupy a relatively large proportion of parameters and computation as for the whole network. To further reduce computation and meanwhile retain 
superior performance, we build a new shuffle block decoder (SBD) module referring to the lightweight ShuffleNet~\cite{8578814}, for its low computation budget and high classification precision. Different from ShuffleNet~\cite{8578814} as a backbone network, our SBD module is employed for regressing optical flow, which 
is only located in the middle part of decoder network. The overall architecture of FastFlowNet is illustrated 
in Fig.~\ref{fig2}.




Based on the above improvements, our proposed network can achieve impressive 
performance on the Sintel~\cite{Butler_2012} and KITTI~\cite{7298925} benchmarks with significantly reduced computation cost. 
In particular, FastFlowNet runs  faster than PWC-Net~\cite{Sun_2018_CVPR} with  computation; and  faster than LiteFlowNet~\cite{Hui_2018_CVPR} with  computation  when predicting quarter resolution flow fields. Moreover, it only contains 1.37M parameters which is as light as SPyNet~\cite{Ranjan_2017}, but behaves much better with  speed up, confirming state-of-the-art size-accuracy trade-off. Thanks to the coarse-to-fine structure, FastFlowNet can naturally trade accuracy for speed according to specific robotic applications. For example, our model can process  resolution images within a range of - FPS on a single  Jetson TX2 GPU. To our knowledge, FastFlowNet represents the first real-time solution for accurate optical flow on embedded devices.





\section{Related work}

FlowNet~\cite{Fischer_2015} is the first work to use CNNs for optical flow estimation. It propose two variants of FlowNetS and FlowNetC together with the synthetic FlyingChairs dataset for end-to-end training. The improved version 
FlowNet2~\cite{Ilg_2017} fuses cascaded FlowNets with a small displacement module, and uses FlyingThings3D~\cite{Mayer_2016} dataset for a further fine-tuning schedule. These encoder-decoder based networks have exceeded variational solutions with orders of magnitude faster speed. However, these models are not sufficiently compact and fast for mobile applications. 
\begin{figure*}[t]
	\centering
	\includegraphics[width=.85\textwidth]{fig2.pdf}
	\caption{Architecture overview of the proposed \textbf{FastFlowNet}. HEPP, CDDC and SBD are efficient modules for extracting pyramid feature, constructing cost volume and regressing optical flow respectively. Only two levels are shown here for clarity.}
	\label{fig2}
\end{figure*}

Inspired by the traditional image pyramid, SPyNet~\cite{Ranjan_2017} estimates residual flow in decomposed spatial levels with only 1.2M parameters, by 
sacrificing some performance accuracy. 
Following the coarse-to-fine residual structures, \textit{i.e.}, PWC-Net~\cite{Sun_2018_CVPR} and LiteFlowNet~\cite{Hui_2018_CVPR} employ feature pyramid, warping and correlation in each level to achieve good 
accuracy with a reduced number of  parameters, representing the most efficient optical flow architecture. 

Most recent works on optical flow focus on improving accuracy.
IRR-PWC~\cite{Hur_2019} shares the flow decoder and context network among all spatial scales for joint optical flow and occlusion estimation. HD3-Flow~\cite{Yin_2019} decomposes contiguous flow fields into discrete grids and adopts a better backbone. FDFlowNet~\cite{Kong_2020} employs better U-shape network and partial fully connected flow estimators for efficient inference. MaskFlowNet~\cite{zhao2020maskflownet} adds additional occlusion mask module into PWC-Net~\cite{Sun_2018_CVPR} and uses 
cascaded refinement. Recently, RAFT~\cite{teed2020raft} outperforms other approaches by first calculating all-pairs similarity and then performing iterations at a high resolution. The price is that 
it incurs significantly larger 
computation burden. 
We instead  pay more attention to designing a compact model while retaining good optical flow accuracy here. 

\section{Our method}
\subsection{Overview of the Approach}

Given two time-adjacent input images ,
our 
proposed FastFlowNet exploits the coarse-to-fine residual structure to estimate gradually refined optical flow .
But it is extensively reformed to accelerate inference by properly reducing parameters and computation cost. To this end, we first replace 
the dual convolution feature pyramid in PWC-Net~\cite{Sun_2018_CVPR} with the head enhanced pooling pyramid for enhancing high resolution pyramid feature and reducing model size. Then we propose a novel center dense dilated correlation layer for constructing compact cost volume while keeping the large search radius. Finally, new shuffle block decoders are employed at each pyramid level to regress optical flow with significantly cheaper computation.
The structure details of the model are listed in Table~\ref{tab1}.


\subsection{Head Enhanced Pooling Pyramid}
Traditional methods~\cite{Bouguet_2000, Brox_2011} apply image pyramid to optical flow estimation for speeding up optimization or dealing with large displacement. SPyNet~\cite{Ranjan_2017} transfers this  classical paradigm and first introduces the pooling based image pyramid to deep models. Since raw images are variant to illumination changes and the fixed pooling pyramid is vulnerable to noise~\cite{Brox_2004}, such as shadows and reflection, PWC-Net~\cite{Sun_2018_CVPR} and LiteFlowNet~\cite{Hui_2018_CVPR} replace it by learnable feature pyramids with significant improvement. Concretely, they gradually expands feature channels while shrinking spatial size by half to extract robust matching features.



However, large channel numbers in low resolutions result in a large number of 
parameters. Moreover, it can be redundant for the coarse-to-fine structure, since low level pyramid features are only responsible to estimate coarse flow fields. Thus, we combine the feature pyramid at  high levels with a pooling pyramid at other lower  levels to obtain both strengths as shown in Fig.~\ref{fig2}. On the other hand, high resolution pyramid features are  relatively shallow in the PWC-Net, as each pyramid level only contains two convolutions with 33 kernel that has small receptive field. Therefore,  we add one more convolution at 
high levels to intensify pyramid features  with a small extra cost. By balancing computation among different scales, we have presented a head enhanced pooling pyramid feature extractor as listed in the top of Table~\ref{tab1}. Like FlowNetC, PWC-Net and LiteFlowNet, HEPP generates six pyramid levels from   resolution (level 1) to  resolution (level 6) by a scale factor of 2.






\subsection{Center Dense Dilated Correlation}
One crucial step in modern optical flow architectures is to calculate feature correspondence by the inner product based correlation layer~\cite{Fischer_2015}. Given two pyramid features of  in level , like many coarse-to-fine residual approaches, we first adopt the bilinear interpolation based warping operation~\cite{Ranjan_2017, Ilg_2017} to warp the second feature  according to  upsampled previous flow field . The warped target feature  can significantly reduce displacement caused by large motions, which 
helps to narrow the search region and simplify the task to estimate relatively small residual flow. Recent models~\cite{Sun_2018_CVPR}\cite{Hui_2018_CVPR} construct cost volume by correlating source features with corresponding warped target features within a local square area that can be formulated as

where  and  represent spatial and offset coordinates.  is the length of input features.  means search radius; and  stands for inner product.



Existing works~\cite{Sun_2018_CVPR, hofinger2019improving} have shown that increasing the search radius when building the cost volume can lead to lower end-point error during both training and testing, especially for large displacement cases. However, feature channels of the cost volume are squared in terms of the 
search radius and computation complexity of following decoder network becomes 
fourth power. As shown in Fig.~\ref{fig3_1}, many flow networks~\cite{Sun_2018_CVPR,Kong_2020,Hur_2019,Yin_2019} set , and the resulting large computation burden has impeded low-power applications. A simple 
way to reduce computation cost is to decrease , such as setting  in Fig~.\ref{fig3_2}. It can reduce cost volume features from 81 to 49, however this method is at the expense of sacrificing perception range and accuracy.

\begin{figure}[t]
	\captionsetup[subfigure]{farskip=1pt}
	\centering
	\subfloat[]
	{
		\includegraphics[width=0.3\linewidth]{fig3_1.pdf}\label{fig3_1}
	}
	\subfloat[]
	{
		\includegraphics[width=0.3\linewidth]{fig3_2.pdf}\label{fig3_2}
	}
	\subfloat[CDDC]
	{
		\includegraphics[width=0.3\linewidth]{fig3_3.pdf}\label{fig3_3}
	}
	\caption{Different approaches for constructing the cost volume. Orange squares represent considered search grids. (a) and (b) are traditional square matching costs while (c) is the new proposed center dense dilated cost volume.}
	\vspace{-5mm}
	\label{fig3}
\end{figure}

Inspired by the atrous spatial pyramid pooling (ASPP) module in DeepLabv3~\cite{chen2017rethinking}, we propose a novel center dense dilated correlation (CDDC) layer that samples search grids densely around the center while down sampling grid points in large motion areas as shown in Fig.~\ref{fig3_3}. 

Different from ASPP which uses parallel atrous convolutions to capture multi-scale context information, our proposed CDDC is designed to reduce computation when constructing large radius cost volume. In FastFlowNet, it outputs 53 feature channels which has similar computation budget with traditional  setting. Our motivation is that the residual flow distribution is more focused on small motions. Our experiments show that CDDC behaves better than the naive compression method.

\begin{table}[t]
	\centering
	\large
	\renewcommand\arraystretch{1.06}
	\caption{Architecture of FastFlowNet. HEPP, MFC, SBD represents head enhanced pooling pyramid, multiple feature construction and shuffle block decoder. MFC and SBD share the same structure across all pyramid levels, thus only level 5 are listed for brevity. `Pool' stands for average pooling. Fconv5\_2, fconv5\_3 and fconv5\_4 are convolutions with group 3. `Shuffle' means the channel shuffle operation~\cite{8578814}. Convolution layers except upconv6 (deconvolution) and fconv5\_7 are followed by a LeakyReLU activation.}
	\resizebox{0.48\textwidth}{!}{
		\begin{tabular}{ c|c|cc|c|c }
			\hline
			& Layer Name & Kernel & Stride & Input & Ch I/O \\
			\hline
			\multirow{10}{*}{\rotatebox{90}{HEPP}} & pconv1\_1 & 33 & 2 & img1 or img2 & 3/16 \\
			~ & pconv1\_2 & 33 & 1 & pconv1\_1 & 16/16 \\
			~ & pconv2\_1 & 33 & 2 & pconv1\_2 & 16/32 \\
			~ & pconv2\_2 & 33 & 1 & pconv2\_1 & 32/32 \\
			~ & pconv2\_3 & 33 & 1 & pconv2\_2 & 32/32 \\
			~ & pconv3\_1 & 33 & 2 & pconv2\_3 & 32/64 \\
			~ & pconv3\_2 & 33 & 1 & pconv3\_1 & 64/64 \\
			~ & pconv3\_3 & 33 & 1 & pconv3\_2 & 64/64 \\
			~ & pool4 & 22 & 2 & pconv3\_3 & 64/64 \\
			~ & pool5 & 22 & 2 & pool4 & 64/64 \\
			~ & pool6 & 22 & 2 & pool5 & 64/64 \\
			\hline
			\multirow{5}{*}{\rotatebox{90}{MFC}} & upconv6 & 44 & 1/2 & flow6 & 2/2 \\
			~ & warp5 & - & - & pool5b,upconv6 & 64,2/64 \\
			~ & cddc5 & 11 & 1 & pool5a,warp5 & 64,64/53 \\
			~ & rconv5 & 33 & 1 & pool5a & 64/32 \\
			\cline{3-5}
			~ & concat5 & \multicolumn{3}{|c|}{rconv5 + cddc5 + upconv6} & 32,53,2/87 \\ 
			\hline
			\multirow{10}{*}{\rotatebox{90}{SBD}} & fconv5\_1 & 33 & 1 & concat5 & 87/96 \\
			~ & fconv5\_2 & 33 & 1 & fconv5\_1 & 332/332 \\
			~ & shuffle5\_2 & - & - & fconv5\_2 & 332/323 \\
			~ & fconv5\_3 & 33 & 1 & shuffle5\_2 & 332/332 \\
			~ & shuffle5\_3 & - & - & fconv5\_3 & 332/323 \\
			~ & fconv5\_4 & 33 & 1 & shuffle5\_3 & 332/332 \\
			~ & shuffle5\_4 & - & - & fconv5\_4 & 332/323 \\
			~ & fconv5\_5 & 33 & 1 & shuffle5\_4 & 96/64 \\
			~ & fconv5\_6 & 33 & 1 & fconv5\_5 & 64/32 \\
			~ & fconv5\_7 & 33 & 1 & fconv5\_6 & 32/2 \\
			\hline
	\end{tabular}}
	\label{tab1}
\end{table}






\subsection{Shuffle Block Decoder}
After building the cost volume, coarse-to-fine models usually concatenate context features, cost volume and upsampled previous flow as inputs for the following decoder networks. Due to 
each pyramid level, the decoder part takes up most parameters and computation of the whole network. Thus, better speed-accuracy trade-off is critically important.  

PWC-Net~\cite{Sun_2018_CVPR} shows that the dense connected flow decoder can improve accuracy after fine-tuning on the  FlyingThings3D~\cite{Mayer_2016} dataset at the price of increasing the model size and computation. LiteFlowNet~\cite{Hui_2018_CVPR} uses sequential connected flow estimator and also shows improved 
performance. To achieve better trade-off between these two schemes, 
FDFlowNet~\cite{Kong_2020} employs the partial fully connected structure. However, all these methods can not well support real-time inference on embedded systems.

Thanks to the compact cost volume built by CDDC, we can directly decrease maximal feature channels in decoder network from 128 to current 96 without issues. To further reduce computation and model size, we reform the middle three 96-channel convolutions into group convolutions followed by channel shuffle operations, which we term `shuffle block'. Different from ShuffleNet~\cite{8578814} as a backbone network, our shuffle block decoder is employed for regressing optical flow. As listed in the bottom of Table~\ref{tab1}, each decoder network contains three shuffle blocks with group , which 
can efficiently reduce computation with marginal drops in accuracy.

\begin{figure*}[t]
	\centering
	\includegraphics[width=\textwidth]{fig4.pdf}
	\caption{Visual examples of anytime inference. FastFlowNet can naturally trade speed for accuracy according to specific robotic applications. Black arrow means inference timeline which is measured on an  embedded Jetson TX2 GPU.}
	\label{fig4}
\end{figure*}

\begin{table}[t]
	\centering
	\large
	\renewcommand\arraystretch{1.06}
	\caption{Ablation study on structure variants with different pyramid feature extractor, cost volume constructor and optical flow decoder. Flow accuracy is measured by end point error. FLOPs are calculated on Sintel resolution images.}
	\resizebox{0.48\textwidth}{!}{
		\begin{tabular}{ ccc|cc|cc }
			\hline
			Pyramid & Cost & Decoder & Sintel & KITTI & Params.  & FLOPs \\
			\hline
			conv &  &  & 3.12 & 2.37 & 4.23M & 28.2G \\
			conv+pool &  &  & 3.00 & 2.25 & 3.22M & 27.7G \\
			HEPP &  &  & 2.92 & 2.13 & 3.27M & 28.3G \\
			HEPP &  &  & 2.93 & 2.33 & 2.18M & 19.1G \\
			HEPP & CDDC &  & 2.95 & 2.18 & 2.20M & 19.2G \\
			HEPP & CDDC &  & 3.04 & 2.22 & 1.57M & 14.0G \\
			HEPP & CDDC &  & 3.11 & 2.29 & 1.37M & 12.2G \\
			HEPP & CDDC &  & 3.22 & 2.47 & 1.26M & 11.3G \\
			HEPP & CDDC &  & 3.35 & 2.59 & 1.16M & 10.4G \\
			\hline
	\end{tabular}}
	\label{tab2}
\end{table}

\subsection{Loss Function}
Since FastFlowNet has the same pyramid scales as FlowNet~\cite{Fischer_2015} and PWC-Net~\cite{Sun_2018_CVPR}, we adopt the same multi-scale L2 loss for training

where  computes L2 norm between predicted and ground-truth flow fields. When fine-tuning on datasets with real scene structure, such as KITTI, we use following robust loss

where  denotes L1 norm,  is a small constant and  makes loss more robust for large magnitude outliers. For fair comparison with previous methods~\cite{Fischer_2015, Ilg_2017, Sun_2018_CVPR, Hui_2018_CVPR}, weights in Eq.~\eqref{eq2} and Eq.~\eqref{eq3} are set to  and .


\begin{table*}[h!]
	\footnotesize
	\renewcommand\arraystretch{1.1}
	\caption{Performance comparison on benchmark datasets. Speed and computation are measured on Sintel resolution images. Results 
	in parentheses are that of networks being trained on the same dataset. Default accuracy metric is the end-point error.}
\centering
		\resizebox{0.99\textwidth}{!}{
			\begin{tabular}{ r|cc|cc|cc|ccc|cc|cc }
				\hline
				\multirow{2}{*}{Method} & \multicolumn{2}{|c|}{Sintel Clean} & \multicolumn{2}{|c|}{Sintel Final} & \multicolumn{2}{|c | }{KITTI 2012} & \multicolumn{3}{|c|}{KITTI 2015} & Params & FLOPs & \multicolumn{2}{|c }{Time (s)} \\
				& train & test & train & test & train & test & train & train (Fl-all) & test (Fl-all) & (M) & (G) & 1080Ti & TX2 \\
				\hline
				FlowNetC \cite{Fischer_2015} & 4.31 & 7.28 & 5.87 & 8.81 & 9.35 & - & - & - & - & 39.18 & 6055.3 & 0.029 & 0.427 \\
				FlowNetC-ft & (3.78) & 6.85 & (5.28) & 8.51 & 8.79 & - & - & - & - & 39.18 & 6055.3 & 0.029 & 0.427 \\
				FlowNet2 \cite{Ilg_2017} & 2.02 & 3.96 & 3.14 & 6.02 & 4.09 & - & 10.06 & 30.37\% & - & 162.52 & 24836.4 & 0.116 & 1.547 \\
				FlowNet2-ft & (1.45) & 4.16 & (2.01) & 5.74 & (1.28) & 1.8 & (2.30) & (8.61\%) & 11.48\% & 162.52 & 24836.4 & 0.116 & 1.547 \\
				\hline
				SPyNet \cite{Ranjan_2017} & 4.12 & 6.69 & 5.57 & 8.43 & 9.12 & - & - & - & - & 1.20 & 149.8 & 0.050 & 0.918 \\
				SPyNet-ft & (3.17) & 6.64 & (4.32) & 8.36 & (4.13) & 4.7 & - & - & 35.07\% & 1.20 & 149.8 & 0.050 & 0.918 \\
				PWC-Net \cite{Sun_2018_CVPR} & 2.55 & - & 3.93 & - & 4.14 & - & 10.35 & 33.67\% & - & 8.75 & 90.8 & 0.034 & 0.485 \\
				PWC-Net-ft & (2.02) & 4.39 & (2.08) & 5.04 & (1.45) & 1.7 & (2.16) & (9.80\%) & 9.60\% & 8.75 & 90.8 & 0.034 & 0.485 \\
				LiteFlowNet\cite{Hui_2018_CVPR} & 2.48 & - & 4.04 & - & 4.00 & - & 10.39 & 28.50\% & - & 5.37 & 163.5 & 0.055 & 0.907 \\
				LiteFlowNet-ft & (1.35) & 4.54 & (1.78) & 5.38 & (1.05) & 1.6 & (1.62) & (5.58\%) & 9.38\% & 5.37 & 163.5 & 0.055 & 0.907 \\
				\hline
				PWC-Net-small & 2.83 & - & 4.08 & - & - & - & - & - & - & 4.08 & - & 0.024 & - \\
				LiteFlowNetX & 3.58 & - & 4.79 & - & 6.38 & - & 15.81 & 34.90\% & - & 0.90 & - & 0.035 & - \\
				FastFlowNet & 2.89 & - & 4.14 & - & 4.84 & - & 12.24 & 33.10\% & - & 1.37 & 12.2 & 0.011 & 0.176 \\
				FastFlowNet-ft & (2.08) & 4.89 & (2.71) & 6.08 & (1.31) & 1.8 & (2.13) & (8.21\%) & 11.22\% & 1.37 & 12.2 & 0.011 & 0.176 \\
				\hline
		\end{tabular}}
\label{tab3}
\end{table*}


\section{Experiments}
\subsection{Implementation Details}
To compare FastFlowNet with other networks, we follow the two-stage training strategy proposed in FlowNet2~\cite{Ilg_2017}. Ground truth optical flow is divided by 20 and down sampled as supervision at different levels. Since final flow prediction is in a quarter resolution, we use bilinear interpolation to obtain the full resolution optical flow. During both training and fune-tuning, we use the same data augmentation methods as in \cite{Ilg_2017}, including mirror, translate, rotate, zoom, squeeze and color jitter. All our experiments are implemented in PyTorch and conducted on a machine equipped with 4 NVIDIA GTX 1080 Ti GPU cards. To compare performance of different optical flow models on mobile devices, we further evaluate inference time 
on the embedded Jetson TX2 GPU.

We first train FastFlowNet on FlyingChairs~\cite{Fischer_2015} using  learning schedule~\cite{Ilg_2017}, \textit{i.e.}, the learning rate is initially set to  and decays half at 300k, 400k and 500k iterations. We crop  patches during data augmentation and adopt a batch size of 8. Then the model is fine-tuned on FlyingThings3D~\cite{Mayer_2016} using the  schedule~\cite{Ilg_2017}, \textit{i.e.}, the learning rate is initially set to  and decays by half at 200k, 300k and 400k iterations. Random crop size is  and the batch size is set to 4. The Adam~\cite{Adam_2014} optimizer and multi-scale L2 loss in Eq.~\ref{eq2} are always used. We call the model {\bf FastFlowNet} after above two-stage learning schedules whose results on Sintel and KITTI training sets are listed in Table~\ref{tab3}.






\subsection{Ablation Study}
To explore the efficient optical flow structure with low computation cost, we perform  ablations on the pyramid feature extractor, cost volume constructor and optical flow decoder. Results of the gradually lightened models are listed in Table~\ref{tab2}. All variants are first trained on FlyingChairs and evaluated on Sintel Clean training sets. To test in large displacement cases, which are common when robots move forward or backward, they are fine-tuned on KITTI 2015 training subset for 5-fold cross-validation. Since change of inference time 
is not distinct based on our PyTorch implementation, we use FLOPs~\cite{howard2017mobilenets, 8578814} which means number of floating point multiply-add operations to measure real computation complexity. In Table~\ref{tab2}, `conv' represents dual convolution feature pyramid used in PWC-Net, `conv+pool' stands for combining dual convolution feature pyramid in top 3 levels with pooling pyramid in bottom 3 levels. Note that traditional  cost volume has sequential connected flow decoders, whose maximal feature channel is 128 rather than 96, in order to make it fully convergent.  means no grouping convolution without shuffle block in flow decoder.



Surprisingly, the lighter `conv+pool' pyramid feature extractor behaves better than the traditional `conv' method. Our analysis is that parameter-free average pooling may share gradients among different levels during back-propagation, which may have  made pyramid features more robust to scale changes. 

By replacing `conv+pool' with HEPP, we can achieve  better results with small additional overhead. Then, we decrease the search radius for constructing lightened  cost volume to reduce computation. 
Flow accuracy on Sintel shows 
almost no change, while results on large displacement KITTI decline clearly. To mitigate performance degradation, we adopt CDDC to build a compact cost volume with larger receptive field at each pyramid level. Although it does not improve on Sintel training which contains 
many 
small movements, it works 
better on the more challenging KITTI 2015 validation set. 

Finally, we employ SBD for further compression on parameters and computation. In Table~\ref{tab3}, we gradually lighten the network by setting incremental group numbers in SBD. It can be seen that computation reduction becomes relatively small when , whereas 
clear 
performance degradation appears. Thus, we adopt  in FastFlowNet which shows a 
better computation-accuracy trade-off.


\subsection{MPI Sintel}

When fine-tuning on the Sintel~\cite{Butler_2012} training sets, we crop  patches and set the batch size to 4. Robust loss in Eq.~\ref{eq3} is adopted with . Like PWC-Net, we set the initial learning rate to  and change 
it to ,  and  every 150k iterations in total 600k iterations,  
where the learning rate decays to zero in each period. We report results on the Sintel test sets by submitting predictions to the MPI Sintel online benchmark, which are listed in Table~\ref{tab3}. For mobile applications, we also measure computation burden of different networks in FLOPs and real inference time 
on both desktop GTX 1080Ti GPU and embedded Jetson TX2. All experiments are conducted in PyTorch for fair comparison.

In Table~\ref{tab3}, FastFlowNet outperforms FlowNetC~\cite{Fischer_2015} in all items due to the advanced coarse-to-fine residual structure. It runs  faster than FlowNetC on both GTX 1080Ti and Jetson TX2 with  less computation. The reason why competition complexity is not proportional to inference time 
is that correlation and warping layers have large memory access cost (MAC) while they are low FLOPs operations. 


FastFlowNet outperforms SPyNet~\cite{Ranjan_2017} by  and  on Sintel Clean and Final test sets respectively. Although containing slightly 
more parameters, our network runs  faster with  less computation. Compared with the state-of-the-art LiteFlowNet~\cite{Hui_2018_CVPR}, we can approach  of its performance with  less (only 12.2 GFLOPs) computation cost.

To further verify the effectiveness of the proposed flow architecture, we compare FastFlowNet with compact 
versions of PWC-Net~\cite{Sun_2018_CVPR} and LiteFlowNet~\cite{Hui_2018_CVPR} provided in their papers. PWC-Net-small is the small version of PWC-Net by dropping dense connections in flow decoders;  and LiteFlowNetX is the small variant by shrinking convolution channels in feature pyramid and flow estimators. We can see that FastFlowNet achieves 
almost the same good results with PWC-Net-small on Sintel training datasets after two-stage pretraining, while being additional  samller and  faster.

\subsection{KITTI}
To test the proposed architecture on more challenging large displacement real world data, we fine-tune FastFlowNet on mixed KITTI 2012~\cite{Geiger2012CVPR} and KITTI 2015~\cite{7298925} training sets. Like~\cite{Ilg_2017, Sun_2018_CVPR, Hui_2018_CVPR}, we crop  patches and adopt a batch size of 4. The same fine-tuning schedule on Sintel is employed and  in robust loss is set to  for outlier suppression. To adapt to driving scenes, spatial augmentation methods of rotate, zoom and squeeze are skipped with a probability of  as~\cite{Sun_2018_CVPR, Hui_2018_CVPR}. Then we evaluate the  fine-tuned FastFlowNet on the online KITTI benchmarks which are listed in Table~\ref{tab3}.

Compared with the encoder-decoder based FlowNet2~\cite{Ilg_2017}, our model can achieve \textit{on par }
or better results on KITTI test datasets while being  smaller and  faster.

We can achieve  similar accuracy compared 
to PWC-Net~\cite{Sun_2018_CVPR} with  less multiply-add computation cost. 
For LiteFlowNetX which simply reduces feature channels, our
proposed HEPP, CDDC and SBD modules work better on all metrics confirming effectiveness of our contribution. Some visual results in anytime setting~\cite{8794003, dovesi2019realtime} are  shown 
in Fig.~\ref{fig4}.


\section{Conclusion}
In this paper, we have proposed a fast and lightweight network for accurate optical flow estimation for 
mobile applications. Our method is based on the well-known coarse-to-fine structure and is extensively reformed to accelerate inference by properly reducing parameters and computation. The proposed head enhanced pooling pyramid, center dense dilated correlation and shuffle block decoder are efficient modules for pyramid feature extraction, cost volume construction and optical flow decoding, which 
covers all components of flow estimation pipeline. We have evaluated each component on both synthetic Sintel and real-world KITTI datasets to verify their effectiveness. The proposed FastFlowNet can be directly plugged into any optical flow based robotic vision systems with much less resource consumption,
achieving good accuracy. 






\bibliographystyle{ieeetr}
\bibliography{refs}


\end{document}
