\documentclass{article}

\PassOptionsToPackage{numbers, compress}{natbib}






\usepackage[final]{neurips_2019}



\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{natbib}
\usepackage{amsmath}
\usepackage{boldline}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{hyperref}

\newcommand{\etal}{\textit{et al.}}
\newcommand{\eg}{\textit{e.g.}}
\newcommand{\ie}{\textit{i.e.}}



\title{Learning to Predict Layout-to-image Conditional Convolutions for Semantic Image Synthesis}



\author{
  Xihui Liu \\
  The Chinese University of Hong Kong \\
  \texttt{xihuiliu@ee.cuhk.edu.hk} \\
  \And
  Guojun Yin \\
  University of Science and Technology of China \\
  \texttt{gjyin91@gmail.com} \\
  \AND
  Jing Shao \\
  SenseTime Research \\
  \texttt{shaojing@sensetime.com} \\
  \And
  Xiaogang Wang \\
  The Chinese University of Hong Kong \\
  \texttt{xgwang@ee.cuhk.edu.hk} \\
  \And
  Hongsheng Li \\
  The Chinese University of Hong Kong \\
  \texttt{hsli@ee.cuhk.edu.hk} \\
}

\begin{document}

\maketitle
\begin{abstract}
  Semantic image synthesis aims at generating photorealistic images from semantic layouts. Previous approaches with conditional generative adversarial networks (GAN) show state-of-the-art performance on this task, which either feed the semantic label maps as inputs to the generator, or use them to modulate the activations in normalization layers via affine transformations. We argue that convolutional kernels in the generator should be aware of the distinct semantic labels at different locations when generating images. In order to better exploit the semantic layout for the image generator, we propose to predict convolutional kernels conditioned on the semantic label map to generate the intermediate feature maps from the noise maps and eventually generate the images. Moreover, we propose a feature pyramid semantics-embedding discriminator, which is more effective in enhancing fine details and semantic alignments between the generated images and the input semantic layouts than previous multi-scale discriminators. We achieve state-of-the-art results on both quantitative metrics and subjective evaluation on various semantic segmentation datasets, demonstrating the effectiveness of our approach.\footnote{Code is available at \href{https://github.com/xh-liu/CC-FPSE}{https://github.com/xh-liu/CC-FPSE}}
 \end{abstract}

\section{Introduction}


Recently, generative adversarial networks (GAN)~\cite{goodfellow2014gan} have shown stunning results in generating photorealistic images of faces~\cite{karras2017progressive,karras2018style} and simple objects~\cite{zhang2018self,brock2018large,lucic2019high}.
However, generating photorealistic images for complex scenes with different types of objects and stuff remains a challenging problem.
We consider semantic image synthesis, which aims at generating photorealistic images conditioned on semantic layouts.
It has wide applications on controllable image synthesis and interactive image manipulation.
State-of-the-art methods are mostly based on Generative Adversarial Networks (GAN).

A fundamental question to semantic image synthesis is how to exploit the semantic layout information in the generator. 
Most previous GAN-based approaches feed the label maps as inputs, and generate images by an encoder-decoder network~\cite{isola2017image,wang2018high,park2019semantic}.
Nonetheless, since the semantic label maps are only fed into the network once at the input layer, the layout information cannot be well preserved in the generator.
To mitigate the problem, SPADE~\cite{park2019semantic} uses the label maps to predict spatially-adaptive affine transformations for modulating the activations in normalization layers.
However, such feature modulation by simple affine transformations is limited in representational power and flexibility.


On the other hand, we rethink the functionality of convolutional layers for image synthesis. 
In a generation network, each convolutional layer learns ``how to draw'' by generating fine features at each location based on a local neighborhood of input features.
The same translation-invariant convolutional kernels are applied to all samples and at all spatial locations, irrespective of different semantic labels at different locations, as well as the unique semantic layout of each sample.
Our argument is that different convolutional kernels should be used for generating different objects or stuff.




Motivated by the two aforementioned aspects, we propose to predict spatially-varying conditional convolution kernels based on the input semantic layout, so that the layout information can more explicitly and effectively control the image generation process.
However, naively predicting all convolutional kernels is infeasible, because it requires a large amount of learnable parameters, which causes overfitting and requires too much GPU memory.
Inspired by recent works on lightweight convolutional neural networks~\cite{chollet2017xception,howard2017mobilenets,ma2018shufflenet}, we propose to predict the depthwise separable convolution, which factorizes a convolutional operation into a conditional depthwise convolution and a conventional pointwise convolution (\ie~ convolution). 
The conditional kernel weights for each spatial location are predicted from the semantic layout by a global-context-aware weight prediction network.
Our proposed conditional convolution enables the semantic layout to better control the generation process, without a heavy increase in network parameters and computational cost.


Most existing methods for semantic image synthesis adopt a multi-scale PatchGAN discriminator~\cite{wang2018high,park2019semantic}, but its limited representation power cannot match the increased capacity of the generator.
We believe that a robust discriminator should focus on two indispensable and complementary aspects of the images: \textit{high-fidelity details}, and \textit{semantic alignment with the input layout map}.
Motivated by the two principles, we propose to utilize multi-scale feature pyramids for promoting high-fidelity details such as texture and edges, and exploit patch-based semantic-embeddings to enhance the spatial semantic alignment between the generated images and the input semantic layout.










The contribution of this paper are summarized as follows.
(1) We propose a novel approach for semantic image synthesis by learning to predict layout-to-image conditional convolution kernels based on the semantic layout. 
Such conditional convolution operations enable the semantic layout to adaptively control the generation process based on distinct semantic labels at different locations.
(2) We propose a feature pyramid semantics-embedding discriminator which is more effective in encouraging high-fidelity details and semantic alignment with the input layout
map.
(3) With the proposed approach CC-FPSE, we achieve state-of-the-art results on CityScapes, COCO-Stuff, and ADE20K datasets, demonstrating the effectiveness of our approach in generating images with complex scenes.


\begin{figure}[t]
\centering
\includegraphics[width=1\linewidth]{figures/fig_intro_v3}
\caption{Semantic image synthesis results by previous approaches and our approach. Best viewed in color. Zoom in for details. Key differences are highlighted by red boxes.}
\label{fig:pipeline}
\end{figure}

\section{Related Work}


\noindent\textbf{Generative adversarial networks} (GAN)~\cite{goodfellow2014gan} has made great success in image synthesis~\cite{brock2018large,karras2018style,lucic2019high}. 
Conditional GANs synthesize images based on given conditions, which can be labels~\cite{zhang2018self,brock2018large}, sentence descriptions~\cite{zhang2017stackgan,xu2017attngan}, or semantic layout in our task.
Our work is also related to image-to-image translation~\cite{isola2017image,liu2017unsupervised}, which translates a possible representation of an image into another representation.

\noindent\textbf{Semantic image synthesis} aims at synthesizing photorealistic images given the semantic layout.
Pix2pix~\cite{isola2017image} adopted an encoder-decoder generator which takes semantic label maps as inputs.
Pix2pixHD~\cite{wang2018high} proposed a coarse-to-fine generator and multi-scale discriminators to generate high-resolution images.
SPADE~\cite{park2019semantic} used the semantic label maps to predict affine transformation parameters for modulating the activations in normalization layers.
Besides GAN-based approaches, CRN~\cite{chen2017photographic} used a cascaded refinement network with regression loss as training supervisions.
SIMS~\cite{qi2018semi} developed a semi-parametric approach, by retrieving fragments and refining them with a refinement network.
Our method differ from previous GAN-based approaches in how the semantic layout information controls the generation process.
We propose to predict spatially-varying convolutional kernels conditioned on the semantic layout, so that it can explicitly control the generation process.

\noindent\textbf{Dynamic filter networks}~\cite{jia2016dynamic} was the first attempt to generate dynamic filters based on the inputs.
Ha~\etal~\cite{ha2016hypernetworks} proposed HyperNetworks, where a hyper-network is used to generate weights for another network.
This idea has been applied to different applications such as neural style transfer~\cite{shen2018neural}, super-resolution~\cite{jo2018deep,hu2019meta}, image segmentation~\cite{harley2017segmentation,wu2018dynamic}, motion prediction~\cite{xue2016visual} and tracking~\cite{li2017tracking}.
However, most of them only predicted a limited number of filters, and it would be computation and memory extensive if we use dynamically predicted filters in each layer.
Su~\etal~\cite{su2019pixel} proposed pixel-adaptive CNN which multiplies the conventional convolutional filter with a spatially-varying kernel to obtain convolutional kernels.
Zhao~\etal~\cite{zhao2018dynamic} adopted a shared filter bank and predict adaptive weights to linearly combine the basis filters.
Such operations are still based on the conventional convolutions.So the input information has limited capacity in controlling or influencing the adaptive convolutional kernels, and the behaviors of the generation networks were still dominated by the conventional convolutional kernels.
Our approach differs from previous work in several aspects.
Firstly, we predict the convolutional kernels conditioned on the layout information, so that the conditional information can explicitly control the generation process.
Secondly, we reduce the computation and memory costs by introducing depthwise separable convolutions, while enable the conditional information to control the generation process by directly predicting the convolutional kernel weights.

\section{Method}
\begin{figure}[t]
\centering
\includegraphics[width=1\linewidth]{figures/pipeline_v6}
\caption{(Left) The structure of a Conditional Convolution Block (CC Block). (Right) The overall framework of our proposed CC-FPSE. The weight prediction network predicts weights for CC Blocks in the generator. The conditional convolution generator is built up of Conditional Convolution (CC) Blocks shown on the left. The feature pyramid semantics-embedding (FPSE) discriminator predicts real/fake scores as well as semantic alignment scores. L-ReLU in the CC Block denotes Leaky ReLU.}
\label{fig:pipeline}
\end{figure}

We propose a novel approach for semantic image synthesis with conditional generative adversarial networks.
The proposed framework, CC-FPSE, is composed of a novel generator  with conditional convolutions predicted by the weight prediction network, and a feature-pyramid semantics-embedding discriminator  as shown in Figure \ref{fig:pipeline} (right). 
The proposed generator  is able to fully utilize the semantic layout information to control the image generation process by predicting the convolution kernels in multiple layers of the generation network with limited computational resources.
The proposed discriminator  is able to supervise the generation of fine details and forces the spatial alignment between the generated image and the input semantic layout by embedding both images and label maps into a joint feature space.

\subsection{Learning to Predict Conditional Convolutions for Image Generator}
Our proposed generator  takes a low-resolution noise map as input. It alternatively uses the proposed conditional convolution blocks~\cite{he2016deep} and upsampling layers to gradually refine the intermediate feature maps and eventually generate the output image.
In conventional convolution layers, the same convolution kernels are applied to all samples and at all spatial locations regardless of their distinct semantic layout. 
We argue that such convolution operation is not flexible and effective enough for semantic image synthesis.
In semantic image synthesis, the convolution layers gradually generate refined features at each location given the coarse features in a local neighborhood.
Since different objects or stuff should be generated differently, we would like the convolution layer to be aware of the unique semantic label at the target location.


In order to better incorporate the layout information into the image generation process, we propose to predict convolutional kernel weights based on the semantic layout.
Given the input feature map , we aim to produce the output feature map  by a convolution layer with kernel size .
We adopt a weight prediction network that takes the semantic label map as input and outputs the predicted convolutional kernel weights for each conditional convolution layer.
However, naively predicting all the kernel weights causes excessive computational costs and GPU memory usage.
To solve the problem, we factorize the convolutional layer into depthwise convolution and pointwise convolution, and only predict the weights of the lightweight depthwise convolutions.





\subsubsection{Efficient Conditional Convolution Blocks for Image Generation}
A conventional convolution kernel has  weight parameters. A naive solution for generating the spatially-varying convolution kernel needs to predict  weight parameters.
This is impractical because the convolution operation is the basic building blocks of the generator  and would be stacked for multiple times in the generator. 
Such a network is not only computation and memory intensive, but also prone to overfit the training data.

To solve the problem, we introduce depthwise separable convolution~\cite{chollet2017xception} and only predict the depthwise convolutional kernel weights, which substantially reduces the number of parameters to predict.
In particular, we factorize the convolutional kernel into a conditional depthwise convolution and a conventional pointwise convolution (\ie,  convolution).
The conditional depthwise convolution performs spatial filtering over each input channel independently, and its spatially-varying kernel weights are dynamically predicted based on the semantic layout.
The predicted weights for the conditional convolution layer are denoted as , and the output feature maps are denoted as .
The conditional depthwise convolution is formulated as,

where  denotes the spatial coordinates of the feature maps,  denotes the convolution kernel size, and  denotes the channel index. 
The  convolutional kernels in  with kernel size  operates at each channel and each spatial location of  independently to generate output feature maps.
Then we exploit a conventional pointwise convolution ( convolution) to map the  input channels to  output channels, and the output is denoted as .

In addition, we also propose a conditional attention operation to gate the information flow passed to the next layer.
The conditional attention weights are predicted in the same way as the conditional convolution kernels, which will be detailed later.
An element-wise product between the predicted attention weights  and the convolution output  produces the gated feature maps,

where  is the channel index and  denotes the spatial location in the feature maps.

The size of predicted parameters in the conditional convolution and the conditional attention are  ( in our implementation) and , respectively.
The parameter size is reduced by  times compared to directly predicting the whole convolutional kernel weights.



By predicting unique convolutional kernel weights for each spatial location, the image generation process becomes more flexible and adaptive to the semantic layout conditions.
In the meantime, we keep an affordable parameter size and computational cost by introducing the depthwise separable convolutions.
We define a ResBlock-like structure, named Conditional Convolution Block, with the operations introduced above. 
As shown in Figure~\ref{fig:pipeline} (left), it includes a conventional batch normalization layer, a conditional depthwise convolution with , a conventional pointwise convolution, followed by a conditional attention layer, and finally the the non-linear activation layer. There are also identity additive skip connections for evert two such blocks. 






\subsubsection{Conditional Weight Prediction and Overall Generator Structure}
The conditional weight prediction network predicts the conditional weights  given the input semantic layout.
A simple design of the weight prediction network would be simply stacking multiple convolutional layers.
In SPADE~\cite{park2019semantic}, two convolutional layers of kernel size  are applied to the downsampled semantic label map to generate the adaptive scale and bias for their proposed adaptive normalization layer.
But downsampling a semantic label map to a very small size, \eg, , by nearest neighbor interpolation will inevitably lose much useful information.
Moreover, such a structure only has a receptive field of , which restricts the weight prediction from incorporating long-range context information.
If there is a large area of the same semantic label, pixels inside this area can only access a  local neighborhood with identical semantic labels.
So they will be processed by identical predicted weights, regardless of their relative positions inside the object or stuff.

Therefore, we design a global-context-aware weight prediction network with a feature pyramid structure~\cite{lin2017feature}.
The architecture of our weight prediction network is shown in Figure~\ref{fig:pipeline} (right).
The label map is first downsampled through the layout encoder, and then upsampled by the decoder with lateral connections from the encoder.
The features at different levels of the feature pyramid are concatenated with the original semantic map to obtain the global-context-aware semantic feature maps, which are used to predict the conditional convolution weights and conditional attention weights separately.
We use two convolutional layers to predict the conditional convolution weights.
To predict the conditional attention weights, we adopt two convolutional layers and a Sigmoid activation layer.


With the encoder-decoder structure of the weight prediction network, our predicted weights are aware of not only the local neighborhood, but also long-range context and relative locations.


The overall generator network  is built of a series of Conditional Convolution Blocks and upsampling layers, with conditional weights predicted by the weight prediction network .
\subsection{Feature Pyramid Semantics-Embedding Discriminator}

We believe that a good discriminator should focus on two indispensable and complementary aspects: \textit{high-fidelity details such as texture and edges}, and \textit{semantic alignment with the input semantic map}.
Existing methods for semantic image synthesis apply a multi-scale PatchGAN discriminator~\cite{wang2018high,park2019semantic}, where images concatenated with the semantic label maps are scaled to multiple resolutions and fed into different discriminators with identical structure.
But it still struggles to discriminate the fine details, and does not pose strong constraints on the spatial semantic alignment between the generated image and the input label map.








Motivated by the aforementioned two design principles of discriminators, we propose a more effective design for the discriminator .
We create multi-scale feature pyramids for promoting high-fidelity details such as texture and edges and exploit a semantics-embedding discriminator to force the spatial semantic alignment between the generated images and the input semantic layout.

\subsubsection{Feature Pyramid Discriminator}
Current image generation methods tend to generate images with blurry edges, textures and obvious artifacts.
This problem suggests that we should cast more attention on low-level details when designing the discriminator architectures.
On the other hand, the discriminator should also have a global view of the high-level semantics.
The previously introduced multi-scale PatchGAN discriminator~\cite{wang2018high} attempts to balance large receptive field and fine details by multiple discriminators at different scales.
The same image at different scales are independently fed into different discriminators, leading to increased network parameters, memory footprint and computational cost. 



Inspired by the evolution from image pyramids to feature pyramids~\cite{lin2017feature}, we propose a single feature pyramid discriminator to produce a multi-scale feature representation with both global semantics and low-level texture and edge information.
As shown in Figure.~\ref{fig:pipeline}(right), our feature pyramid discriminator takes the input image at a single scale.
The bottom-up pathway produces a feature hierarchy consisting of multi-scale feature maps and the top-down pathway gradually upsamples the spatially coarse but semantically rich feature maps.
The lateral combines the high-level semantic feature maps from the top-down pathway with the low-level feature maps from the bottom-up pathway. 
As a result, the combined multi-scale features are semantically strong, as well as containing finer low-level details such as edges and textures.
So the discriminator would pose stronger constraints on both the semantic information and the fine details. 

\subsubsection{Semantic Embeddings for Discriminator}
In the conventional discriminators for semantic image synthesis, an image and its corresponding semantic label map is concatenated and fed into the discriminator as its inputs.
However, there is no guarantee that the discriminator makes use of the label maps for distinguishing real/fake images.
In other words, the discriminator could satisfy the training constraints by only discriminating whether an image is real or not, without considering whether it matches well with the label map.
Inspired by projection discriminator~\cite{miyato2018cgans} which computes the dot product between the class label and image feature vector as part of the output discriminator score, we adapt this idea to our scenerio where the condition is the spatial label map.
In order to encourage the semantic alignment between generated images and the conditional semantic layout, we propose a patch-based semantics embedding discriminator.


Our discriminator takes only the real or generated images as inputs, and produces a set of feature pyramids  at different scales.
 denotes feature maps at a spatial resolution of  with  channels.
The feature vector at each spatial location of  represents a patch in the original image.
The conventional PatchGAN discriminator tries to classify if each patch is real or not, by predicting a score for each spatial location in the feature map .
While we force the discriminator to classify not only real or fake images, but also whether the patch features match with the semantic labels in that patch within a joint embedding space. 


We downsample the label map to the same spatial resolution as , and embed the one-hot label at each spatial location into a -dimensional vector. 
The embedded semantic layout is denoted as .
We calculate the inner product between each spatial location of  and , to obtain a semantic matching score map, where each value represents the semantic alignment score of the corresponding patch in the original image.
The semantic matching score is added with the conventional real/fake score as the final discriminator score.
In this way, not only does the discriminator guide the generator to generate high-fidelity images, but also it drives the generated images to be better semantically aligned with the conditional semantic layout.

\subsection{Loss Functions and Training Scheme}
The generator and the discriminator of our network are trained alternatively, where the discriminator adopts the hinge loss for distinguishing real/fake images while the generator is optimized with multiple losses, including the hinge-based adversarial loss, discriminator feature matching loss, and perceptual loss, following previous works~\cite{wang2018high,park2019semantic},

where  is a real image,  is the semantic label map, and  is the input noise map.
 denotes the perceptual loss, which matches the VGG extracted features between the generated images and the original images.
 denotes the discriminator feature matching loss, which matches the discriminator intermidiate features between the generated images and the original images.
 and  denote the weights for the perceptual loss and feature matching loss, respectively.

\section{Experiments}



\subsection{Datasets and Evaluation Metrics}
We experiment on Cityscapes~\cite{cordts2016cityscapes}, COCO-Stuff~\cite{caesar2018coco}, and ADE20K~\cite{zhou2017scene} datasets.
The Cityscapes dataset has 3,000 training images and 500 validation images of urban street scenes.
COCO-Stuff is the most challenging dataset, containing 118,000 training images and 5,000 validation images from complex scenes.
ADE20K dataset provides 20,000 training images and 2,000 validation images from both outdoor and indoor scenes.
All images are annotated with semantic segmentation masks.

We evaluate our approach from three aspects.
We firstly compare synthesized images by our approach and previous approaches, and conduct a human perceptual evaluation to compare the visual quality of the generated images.
We then evaluate the segmentation performance of the generated images using a segmentation model pretrained on the original datasets.
We use the same segmentation models as those in~\cite{park2019semantic} for testing. The segmentation performance is measured by mean Intersection-over-Union (mIOU) and pixel accuracy.
Finally, we calculate the distribution distances between the generated images and real images by the Fr\'echet Inception Distance (FID)~\cite{heusel2017gans}.


\subsection{Implementation Details}
The training and generated image resolution is  for COCO-Stuff and ADE20K datasets, and  for Cityscapes dataset.
For the generator, synchronized batch normalization between different GPUs is adopted for better estimating the batch statistics.
For the discriminator, we utilize instance normalization.
We use Leaky ReLU activations, to avoid sparse gradients caused by ReLU activation.
We adopt ADAM~\cite{kingma2014adam} optimizer with learning rate  for the generator and  for the discriminator.
The weights for the perceptual loss  is 10 and the weight discriminator feature matching loss  is 20.
Following~\cite{park2019semantic}, to enable multi-modal synthesis and style-guided synthesis, we apply a style encoder and a KL-Divergence loss with loss weight 0.05.
Our models are trained on 16 TITANX GPUs, with a batch size of 32.
We train 200 epochs for Cityscapes and ADE20K datasets, and 100 epochs for COCO-Stuff dataset.
Code is available at \href{https://github.com/xh-liu/CC-FPSE}{https://github.com/xh-liu/CC-FPSE}.

\subsection{Qualitative Results and Human Perceptual Evaluation}
\begin{figure}[t]
\centering
\includegraphics[width=1\linewidth]{figures/results_compare_v4}
\caption{Results comparison with previous approaches. Better viewed in color. Zoom in for details.}
\label{fig:result_compare}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=1\linewidth]{figures/results_ours_v6}
\caption{Semantic image synthesis results on COCO and ADE20K. Better viewed in color.}
\label{fig:result_ours}
\end{figure}
We compare our results with previous approaches pix2pixHD~\cite{wang2018high} and SPADE~\cite{park2019semantic}, as shown in Figure~\ref{fig:result_compare}.
The images generated by our approach show significant improvement over previous approaches for challenging scenes. 
They have finer details such as edges and textures, and less artifacts, and matches better with the input semantic layout. 
Figure~\ref{fig:result_ours} shows more images generated by our proposed approach. More results and comparisons are provided in the supplementary material.

We also conduct a human perception evaluation to compare the generated image quality between our method and the previous state-of-the-art method, SPADE~\cite{park2019semantic}.
In particular, we randomly sample 500 semantic label maps from the validation set of each dataset.
At each experiment, the worker is shown a semantic label map with two generated images by our approach and SPADE, respectively.
The worker is required to choose an image with higher quality that matches better with the semantic layout.
We found that in Cityscapes, COCO-Stuff, and ADE20K datasets respectively, 55\%, 76\%, and 61\% images generated by our method is preferred compared to SPADE.
The human perceptual evaluation validates that our approach is able to generate higher-fidelity images that are better spatially aligned with the semantic layout.

\subsection{Quantitative Results}
\begin{table}
\centering
\small
\caption{Results by the proposed and previous approaches on multiple public datasets. Higher mIOU/accuracy and lower FID score indicate better performance.}
 \label{tb:results}
\begin{tabular}{lccccccccc}
\hlineB{2}
\multicolumn{1}{c}{} & \multicolumn{3}{c}{COCO-Stuff}                & \multicolumn{3}{c}{Cityscapes}                & \multicolumn{3}{c}{ADE20K}        \\ \cline{2-10} 
\multicolumn{1}{c}{} & mIOU          & Accu          & FID           & mIOU          & Accu          & FID           & mIOU      & Accu      & FID       \\ \hline
CRN~\cite{chen2017photographic}           & 23.7          & 40.4          & 70.4          & 52.4          & 77.1          & 104.7         & 22.4      & 68.8      & 73.3      \\
SIMS~\cite{qi2018semi}          & N/A           & N/A           & N/A           & 47.2          & 75.5          & \textbf{49.7} & N/A       & N/A       & N/A       \\
pix2pixHD~\cite{wang2018high}     & 14.6          & 45.7          & 111.5         & 58.3          & 81.4          & 95.0          & 20.3      & 69.2      & 81.8      \\
SPADE~\cite{park2019semantic}         & 37.4          & 67.9          & 22.6          & 62.3          & 81.9          & 71.8          & 38.5      & 79.9      & 33.9      \\ 
Ours                 & \textbf{41.6} & \textbf{70.7} & \textbf{19.2} & \textbf{65.5} & \textbf{82.3} & 54.3 & \textbf{43.7} & \textbf{82.9} & \textbf{31.7} \\ \hlineB{2}
\end{tabular}
\end{table}

\begin{table}
\centering
\scriptsize
\caption{Ablation studies on COCO-Stuff dataset.}
\label{tb:ablation}
\begin{tabular}{lcccccccc}
\hlineB{2}
\multicolumn{1}{c}{} & Baseline & (1)         & (2)         & (3)       & (4)         & (5)          & (6)        & CC-FPSE (Ours) \\ \hline
Generator            & SPADE    & CC w/o FP   & CC w/ FP    & CC w/o FP & SPADE w/ FP & SPADE w/ FP  & CC w/ FP   & CC w/ FP       \\
Discriminator        & MsPatch  & MsPatch     & MsPatch     & FP+SE     & MsPatch+SE  & FP+SE        & MsPatch+SE & FP+SE          \\ \hline
mIOU                 & 35.2     & 36.2        & 36.7        & 40.4      & 38.0        & 39.17        & 40.4       & 41.3           \\ \hlineB{2}
\end{tabular}
\end{table}
Table~\ref{tb:results} shows the segmentation performance and FID scores of results by our approach and those by previous approaches.
CRN~\cite{chen2017photographic} uses cascaded refinement networks with regression loss, without using GAN for training.
SIMS is a semi-parametric approach which retrieves reference segments from a memory bank and refines the canvas by a refinement network.
Both pix2pixHD~\cite{wang2018high} and SPADE~\cite{park2019semantic} are GAN-based approaches.
Pix2pixHD takes the semantic label map as the generator input, and uses a multi-scale generator and multi-scale discriminator to generate high-resolution images.
SPADE takes a noise vector as input, and the semantic label map are used for modulating the activations in normalization layers by learned affine transformations.
Our approach performs consistently better than previous approaches, which demonstrate the effectiveness of the propose approach.
Note that SIMS has better FID scores than GAN-based approaches, because it generates images by refining segments retrieved from the real data.
However, it has poor segmentation performance, because it might retrieve semantically mismatched patches. 


\subsection{Ablation Studies}

We conduct controlled experiments to verify the effectiveness of each component in our approach.
We use the SPADE~\cite{park2019semantic} model as our baseline, and gradually add or eliminate each component to the framework.
Our model is denoted as CC-FPSE in the last column.
The segmentation mIOU scores of the generated images by each experiment are shown in Table~\ref{tb:ablation}.\footnote{To be comparable with ablation study results in~\cite{park2019semantic}, we report the model performance at 50 epochs.}


\noindent\textbf{Conditional convolutions for generator.}
We firstly replace the SPADE layer with our conditional convolution layers to incorporate the semantic layout information in the experiments denoted as ``CC''.
By comparing baseline with (1) (CC generator vs SPADE generator, both with MsPatch discriminator), (5) with CC-FPSE (ours) (CC generator vs SPADE generator, both with FPSE discriminator), and (4) with (6) (CC generator vs SPADE generator, both with MsPatch+SE discriminator), results indicate that our conditional convolutions are able to better exploit the semantic layout information for adaptively generating high-quality images.

\noindent\textbf{Feature pyramid weight prediction network.}
Next, we replace the feature pyramid structure with a stack of two convolutional layers in the weight prediction network, and this experiment is denoted as ``w/ FP'' and ``w/o FP''.
Comparing (1) with (2), and (3) with CC-FPSE (Ours), we found that removing the feature pyramid structure for the weight prediction network leads to inferior performance, indicating that the global and long-range information are necessary for predicting the convolutional weights.

\noindent\textbf{FPSE Discriminator.}
We fix our proposed generator (``CC w/ FP'' or ``SPADE w/ FP'') and test different designs of the discriminator, to demonstrate the effectiveness of our FPSE discriminator.
We force the spatial semantic alignment with the semantic layout, by the introduced semantics-embedding constraint for the discriminator. Comparing (2) with (6) indicates the effectiveness of the semantics embedding discriminator.
With the semantics-embedding constraint, the discriminator is driven to classify the correspondence between the image patches and the semantic layout.
So the generator is encouraged to generate images that are better aligned with the semantic layout.
Furthermore, we replace the multiscale discriminator with the feature pyramid structure, denoted as ``FP+SE'', which is our proposed discriminator design. The comparison between (6) and last column CC-FPSE (Ours) indicates that the feature pyramid discriminator structure, which combines the low-level and semantic features at different scales, leads to further performance improvement.

















\section{Conclusion}
We propose a novel approach (CC-FPSE) for image synthesis from a given semantic layout via better using the semantic layout information to generate images with high-quality details and well aligned semantic meanings.
Our generator is able to better exploit the semantic layout to control the generation process, by predicting the spatially-varying weights for the conditional convolution layers.
Our feature pyramid semantics-embedding discriminator guides the generator to generate images that contain high-fidelity details and aligns well with the conditional semantic layout.
Our approach achieves state-of-the art performance and is able to generate photorealistic images on Cityscapes, COCO-Stuff, and ADE20K datasets.

\newpage
\subsubsection*{Acknowledgments}
This work is supported in part by SenseTime Group Limited, in part by the General Research Fund through the Research Grants Council of Hong Kong under Grants CUHK14202217, CUHK14203118, CUHK14207319, CUHK14208417, CUHK14239816, and in part by CUHK Direct Grant. We thank Lu Sheng for proofreading and helpful suggestions on the paper.

\bibliographystyle{plainnat}

\small

\bibliography{bib}

\normalsize

\section{Appendix}

Examples of generated images by our approach are shown in Figure~\ref{fig:ours}. Over proposed approach is able to synthesis images of diverse scenes.
Moreover, we show the semantic image synthesis results compared to previous approaches pix2pixHD and SPADE in Figure~\ref{fig:compare}.
Some differences between the generated images of different approaches are highlighted in red boxes.
Our proposed approach generates high-quality images with fine details. 
It can generate small objects based on the label map, while previous approaches are likely to ignore them. 
For example, in the first row of Figure~\ref{fig:compare}, our approach generates a driver inside the bus based on the semantic layout, while other approaches fails to generate the driver.

\begin{figure}[h]
\centering
\includegraphics[width=1\linewidth]{figures/supp_ours_v7}
\caption{Semantic image synthesis results by our proposed approach. Best viewed in color. Zoom in for details.}
\label{fig:ours}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=1\linewidth]{figures/supp_ours4_v1}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=1\linewidth]{figures/supp_compare2_v9}
\label{fig:compare}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=1\linewidth]{figures/supp_compare_v6}
\caption{Semantic image synthesis results by previous approaches and our approach. Best viewed in color. Zoom in for details.}
\label{fig:compare}
\end{figure}


\end{document}
