\documentclass[11pt]{article}

\usepackage[latin1]{inputenc}    
\usepackage[OT1]{fontenc}
\usepackage[english]{babel}
\usepackage{fullpage}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{graphicx}
\usepackage{dsfont}
\usepackage{wrapfig}
\usepackage{comment}
\usepackage{rotating}
\usepackage{hyperref} 
\usepackage{subfigure}
\usepackage{algorithmic}
\usepackage[ruled,linesnumbered,shortend,vlined,noend]{algorithm2e}
\usepackage[all]{xy}

\newcommand{\C}{\mathbb{C}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Rop}{\mathbb{R}^{\text{op}}}
\newcommand{\Rext}{\mathbb{R}_{\text{ext}}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\X}{\mathbb{X}}
\newcommand{\Y}{\mathbb{Y}}
\newcommand{\T}{\mathbb{T}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\HS}{\mathcal{H}}
\newcommand{\V}{\mathcal{V}}
\newcommand{\W}{\mathcal{W}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\J}{\mathcal{J}}
\newcommand{\limninf}{\underset{n\rightarrow+\infty}{\rm lim}}
\newcommand{\card}{{\rm card}}
\newcommand{\symdiff}{{\scriptstyle\triangle}}
\newcommand{\e}{\varepsilon}
\newcommand{\proba}[1]{\mathbb{P}\left(#1\right)}

\newcommand{\Ord}{\mathrm{Ord}}
\newcommand{\Ext}{\mathrm{Ext}}
\newcommand{\Rel}{\mathrm{Rel}}
\newcommand{\Dg}{\mathrm{Dg}}

\newcommand{\im}{\mathrm{im}}
\newcommand{\Crit}{\mathrm{Crit}}
\newcommand{\Rips}{\mathrm{Rips}}

\newcommand{\kPSS}{k_{\rm PSS}}
\newcommand{\kPWG}{k_{\rm PWG}}
\newcommand{\kSW}{k_{\rm SW}}
\newcommand{\SW}{{\rm SW}}

\newcommand{\SpD}{{\mathcal D}}
\newcommand{\SpfD}{{\mathcal D}_{\rm f}}
\newcommand{\SpfbD}{{\mathcal D}_{\rm f}^{\rm b}}
\newcommand{\SpND}{{\mathcal D}_N^{\rm b}}
\newcommand{\SpLD}{{\mathcal D}_{\rm f}^L}
\newcommand{\SpNLD}{{\mathcal D}_N^L}

\DeclareMathOperator*{\argmin}{\arg\!\min}
\DeclareMathOperator*{\argmax}{\arg\!\max}

\newtheorem{thm}{Theorem}[section]
\newtheorem{cor}[thm]{Corollary}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{defin}[thm]{Definition}
\newtheorem{rem}[thm]{Remark}

\newcommand{\eqdef}{\ensuremath{\stackrel{\mbox{\upshape\tiny def.}}{=}}} 
\title{Sliced Wasserstein Kernel for Persistence Diagrams}
\author{Mathieu Carri\`ere, Marco Cuturi, Steve Oudot}
\date{}

\begin{document}

\maketitle

\begin{abstract}
Persistence diagrams play a key role in topological data
analysis (TDA), in which they are routinely used to describe
 topological properties of complicated shapes. persistence diagrams
enjoy strong stability properties and have proven their utility in
various learning contexts.  They do not, however, live in a space
naturally endowed with a Hilbert structure and are usually compared
with non-Hilbertian distances, such as the bottleneck distance. To
incorporate persistence diagrams in a convex learning pipeline, several kernels have been
proposed with a strong emphasis on the stability of the resulting RKHS
distance w.r.t. perturbations of the persistence diagrams.  In this article, we use the
Sliced Wasserstein approximation of the Wasserstein distance to
define a new kernel for persistence diagrams, which is not only provably stable but
also discriminative (with a bound depending on the number of points in the persistence diagrams) 
w.r.t. the first diagram distance between persistence diagrams. 
We also demonstrate its practicality, by
developing an approximation technique to reduce kernel computation
time, and show that our proposal compares favorably to existing
kernels for persistence diagrams on several benchmarks.
\end{abstract}

\section{Introduction}

Topological Data Analysis (TDA) is an emerging trend in data
science, grounded on topological methods to design descriptors
for complex data---see e.g.~\cite{Carlsson09b} for an introduction to
the subject.  The descriptors of TDA can be used in various contexts,
in particular statistical learning and geometric inference, where they
provide useful insight into the structure of data.  Applications
of TDA can be found in a number of scientific areas, including
computer vision~\cite{Li14}, materials science~\cite{Hiraoka16}, and
brain science~\cite{Singh08}, to name a few.  The tools developed in
TDA are built upon persistent homology
theory~\cite{Edelsbrunner10,Oudot15}, and their main output is a
descriptor called {\em persistence diagram}, which encodes the
topology of a space at all scales in the form of a point cloud with
multiplicities in the plane ---see Section~\ref{sec:persHom} for more details.

\paragraph{Persistence diagrams as features.} 
The main strength of persistence diagrams is their stability with respect to perturbations of the data~\cite{Chazal09c,Chazal13b}.
On the downside, their use in learning tasks is not straightforward.
Indeed, a large class of learning methods, such as SVM or PCA, requires
a Hilbert structure on the descriptors space, which is not the case
for the space of persistence diagrams. Actually, many simple operators of , such
as addition, average or scalar product, have no analogues in that
space. Mapping persistence diagrams to vectors in  or in some infinite-dimensional Hilbert space 
is one possible approach to facilitate their use in discriminative settings.


\paragraph{Related work.} A series of recent contributions have proposed kernels for persistence diagrams,
falling into two classes.  The first class of methods builds explicit
feature maps: one can, for instance, compute and sample functions extracted from
persistence diagrams~\cite{Bubenik15,Adams17,Robins16}; sort the entries of
the distance matrices of the persistence diagrams~\cite{Carriere15a}; treat the
points of the persistence diagrams as roots of a complex polynomial, whose coefficients are
concatenated~\cite{diFabio15}.
The second class of methods, which is more relevant to our work, defines implicitly feature maps by focusing instead on building kernels for persistence diagrams. For
instance, \cite{Reininghaus15} use
solutions of the heat differential equation in the plane and compare them
with the usual  dot product.
~\cite{Kusano16} handle a persistence diagram as a discrete measure on the plane,
and follow by using kernel mean embeddings with Gaussian kernels---see
Section~\ref{sec:expe} for precise definitions.
Both kernels are provably {\em stable}, in the sense that the metric they induce in their respective reproducing kernel Hilbert space (RKHS) 
is bounded above by the distance between persistence diagrams. 
Although these kernels are injective, there is no evidence that their induced RKHS distances are discriminative and therefore follow 
the geometry of the diagram distances, which are more widely accepted distances to compare persistence diagrams.


More generally, one of the reasons why the derivation of kernels for persistence diagrams is not  
straightforward is that the natural metrics between persistence diagrams, the {\em diagram distances}
are not negative semi-definite. Indeed, these diagram distances are very similar to the 
{\em Wasserstein distance}~\cite[\S6]{Villani09} between probability measures, which is not negative
semi-definite. However, a relaxation of this metric called the {\em Sliced Wasserstein distance}~\cite{Rabin11}
has recently been shown to be negative semi-definite and was used to derive kernels for probability distributions
in~\cite{Kolouri16}.    

\paragraph{Contributions.} In this article, 
we use the Sliced Wasserstein
distance of~\cite{Rabin11} to define a new kernel for persistence diagrams, which we prove
to be both stable and discriminative. Specifically, we provide distortion bounds on the Sliced Wasserstein distance that quantify its ability to 
mimic the diagram distances between persistence diagrams. This
is in contrast to other kernels for persistence diagrams, which only focus on
stability. We also propose a simple approximation algorithm
to speed up the computation of that kernel, confirm experimentally its discriminative power and show 
that it outperforms experimentally both proposals of \cite{Kusano16} and \cite{Reininghaus15} in several supervised classification problems.


\section{Background}

\subsection{Persistent Homology}
\label{sec:persHom}

\newcommand{\dg}{\operatorname{Dg}}
\newcommand{\distb}{\operatorname{d_b}}
\newcommand{\cost}{\operatorname{c}}


\begin{figure*}
\centering
\subfigure[]{
\centering
\label{fig::sublevel_sets}
 \includegraphics[height=4.2cm]{images/superlevel_set}
}
\subfigure[]{\label{fig::barcode1}
\includegraphics[height=4.2cm]{images/noisy_function}
}
\subfigure[]{\label{fig::barcode2}
\includegraphics[height=4.2cm]{images/persistence_example}
}
\caption{\label{fig:barcode}Sketch of persistent homology: 
(a)~the horizontal lines are the boundaries of sublevel sets , which are colored in decreasing shades of grey.
The vertical dotted lines are the boundaries of their different connected components.
For instance, a new connected component is created in the sublevel set  
when , and it is merged (destroyed) when ; its lifespan is represented by a copy of the point with coordinates
 in the persistence diagram of~ (Figure~(c)); 
(b)~a piecewise-linear approximation  (blue) of the function  (red) from sampled values; 
(c)~superposition of  (red) and  (blue), showing the partial matching of minimum cost (magenta) between the
two persistence diagrams.}
\end{figure*}


Persistent homology is a technique inherited from algebraic
topology for computing stable descriptors on real-valued
functions. Given  as input, persistent homology
outputs a planar point set with multiplicities, called the 
{\em  persistence diagram} of~ and denoted by .
Note that the coordinates of the points belong to the extended real line . 
See Figure~\ref{fig:barcode} for an example. 
To understand the meaning of each point in this diagram, it suffices to know that, to compute , 
persistent homology considers the family of {\em sublevel sets} of
, i.e. the sets of the form  for ,
  and it records the {\em topological events} (e.g. creation or merge
  of a connected component, creation or filling of a loop, void, etc.)
  that occur in  as  ranges from  to
    .  Then, each point  represents the lifespan
    of a particular {\em topological feature} (connected component,
    loop, void, etc.), with its creation and destruction times as
    coordinates. See again Figure~\ref{fig:barcode} for an
    illustration.

For the interested reader, we point out that the mathematical tool
used by persistent homology to track the topological events in the
family of sublevel sets is {\em homological algebra}, which turns the
parametrized family of sublevel sets into a parametrized family of
vector spaces and linear maps. Computing persistent homology then
boils down to computing a family of bases for the vector spaces, which
are compatible with the linear maps. It will be no surprise to the
reader familiar with matrix reduction techniques that the simplest way
to implement the compatible basis computation is using Gaussian elimination. 

\paragraph*{Distance between persistence diagrams.}
We now define the {\em th diagram distance} between persistence diagrams.
Let  and  be two persistence diagrams. 
Let  be a {\em partial bijection} between  and .
Then, for any point , the -{\em cost} of  is defined as ,
and for any point , the -{\em cost} of  is defined as 
, where  is the projection 
onto the diagonal~.
The cost  is defined as:

We then define the {\em th diagram distance}
  as the cost of the best partial bijection: 
In the particular case , the cost of  is defined as 

The corresponding distance  is often called the {\em bottleneck distance}.
One can show that  when .
A fundamental property of persistence diagrams is their stability with
respect to (small) perturbations of their originating functions.
Indeed, the {\em stability theorem}~\cite{Bauer13b,Chazal09a,Chazal16a,Cohen07}
asserts that 
for any
, we have


In practice, persistence diagrams can be used as descriptors for data
via the choice of appropriate filtering functions~, e.g.  distance
to the data in the ambient space, eccentricity, curvature, etc. The
main strengths of the obtained descriptors are: (a) to be provably stable as
 mentioned previously; (b) to be invariant under reparametrization of the
data; and (c) to encode information about the topology of the data,
which is complementary and of an essentially different nature compared
to geometric or statistical quantities.
These properties have made persistence diagrams useful in a variety of
contexts, including the ones mentioned in the introduction of the
paper.
For further details on persistent homology and on applications of
persistence diagrams, the interested reader can refer
e.g. to~\cite{Oudot15} and the references therein.

\paragraph{Notation.} Let  be the space of persistence diagrams with at most countably many points,
 be the space of finite and bounded persistence diagrams, and
 be the space of bounded persistence diagrams with less than  points.
Obviously, we have the following sequence of (strict) inclusions: .


\subsection{Kernel Methods}
\label{sec:kernelMethods}

\textbf{Positive Definite Kernels.} Given a set , a function  
is called a {\em positive definite kernel} if for all integers , for all families  of points in , 
the matrix  is itself positive semi-definite. For brevity we will refer to positive definite 
kernels as kernels in the rest of the paper.
It is known that kernels generalize scalar products, in the sense that, given a kernel , there exists a Reproducing Kernel Hilbert Space
(RKHS)  and a {\em feature map}  such that .
A kernel  also induces a distance  on  that can be computed as the Hilbert norm of the difference between two embeddings:
 
We will be particularly interested in this distance, since one of the goals we will aim for will be that of designing a kernel 
 for persistence diagrams such that  has low distortion with respect to the first diagram distance .

\paragraph*{Negative Definite and RBF Kernels.} A standard way to construct a kernel is to exponentiate the negative of a 
Euclidean distance. Indeed, the Gaussian kernel for vectors with parameter  does follow that template approach: 
. An important theorem of~\cite{Berg84} (Theorem 3.2.2, p.74) states that 
such an approach to build kernels, namely setting 

for an arbitrary function  can only yield a valid positive definite kernel for all  
if and only if  is a \emph{conditionally negative definite} function, namely that, for all integers , 
 for all , and for all  such that , 
one has .


Unfortunately, as observed experimentally in Appendix~A of~\cite{Reininghaus14},   is not conditionally negative definite (in practice, it only suffices to sample a family of point 
clouds to observe experimentally that more often than not the inequality above will be violated for a particular weight vector ).
Actually, as observed in~\cite{Padellini17}, even the square of the diagram distances  cannot be used to define Gaussian kernels.
Indeed, it was noted in Theorem~6 of~\cite{Feragen15} that, if the square of a distance  defined on a geodesic space  is conditionally negative definite,
then the metric space  is flat, or CAT. However, since the metric space , equipped with , , is not CAT for any
---which is due to the non-uniqueness of geodesics, see~\cite{Turner14}---it follows that  is not conditionally negative definite.
 
In this article, we use an approximation of  with the {\em Sliced Wasserstein distance}, which is provably 
conditionally negative definite, and we use it to define a RBF kernel that can be easily tuned thanks to its bandwidth parameter .



\subsection{Wasserstein distance for unnormalized measures on }
\label{sec:wasserstein}
The Wasserstein distance~\cite[\S6]{Villani09} is a distance between probability measures. 
For reasons that will become clear in the next section, we will focus on a variant of that distance: 
the 1-Wasserstein distance for \emph{nonnegative}, not necessarily normalized, measures on the real line~\cite[\S2]{Santambrogio15}. 
Let  and  be two nonnegative measures on the real line such that 
 and  are equal to the same number . 
We define the three following objects:


where  is the set of measures on  with marginals  and , 
and  and  the generalized quantile functions of the probability measures  and  respectively. 

\begin{prop}
We have . 
Additionally 
\emph{(i)}  is conditionally negative definite on the space of measures of mass ; 
\emph{(ii)} for any three positive measures  such that , 
we have .
\end{prop}


\begin{proof} The equality between~(\ref{eq:optimal}) and~(\ref{eq:quantile}) is known for probability measures on the 
real line---see Proposition 2.17 in~\cite{Santambrogio15} for instance, and can be trivially generalized to unnormalized measures. 
The equality between~(\ref{eq:optimal}) and~(\ref{eq:Kanto}) is due to the well known Kantorovich duality for a distance 
cost~\cite[Particular case 5.4]{Villani09} which can also be trivially generalized to unnormalized measures, 
which proves the main statement of the proposition. 

The definition of  shows that the Wasserstein distance 
is the  norm of , and is therefore conditionally negative definite (as the  distance 
between two direct representations of  and  as functions  and ), proving point (i). 
The second statement is immediate.
\end{proof}



\begin{rem} For two unnormalized uniform empirical measures 
 and  of the same size, with ordered 
 and , one has:
, where  and .
\end{rem}









\section{The Sliced Wasserstein Kernel}

\subsection{The Sliced Wasserstein Kernel}

In this section we define a new kernel between persistence diagrams, called the {\em Sliced Wasserstein} kernel,
based on the Sliced Wasserstein metric of~\cite{Rabin11}. The idea underlying this
metric is to slice the plane with lines passing through the origin, to
project the measures onto these lines where  is computed, 
and to integrate those distances over all possible lines.  Formally:

\begin{defin}  
Given  with , let  denote the line , and
let  be the orthogonal projection onto .
Let  be two persistence diagrams, and let  and 
,
and similarly for , where  is the orthogonal projection onto the diagonal.
Then, the {\em Sliced Wasserstein distance} 
is defined as:

\end{defin} 



























Note that, by symmetry, one can restrict on the half-circle  and normalize by  instead of .
Since  is conditionally negative definite,
we can deduce that  itself is conditionally negative definite:

\begin{lem}\label{lem:nd}
 is conditionally negative definite on .
\end{lem}





\begin{proof}

Let ,  such that  and .
Given , we let 
, 
 and .
Then:
  
The result follows by linearity of integration.







\end{proof}

Hence, the theorem of~\cite{Berg84}
allows us to define a valid kernel with: 




\subsection{Metric Equivalence}


We now give the main theoretical result of
this article, which states that  is {\em strongly equivalent} to .  
This has to be compared with~\cite{Reininghaus15} and~\cite{Kusano16}, which
only prove stability and injectivity. Our equivalence result
states that , in addition to be stable
and injective, preserves the metric between persistence diagrams, which should
intuitively lead to an improvement of the classification power. This
intuition is illustrated in Section~\ref{sec:expe} and
Figure~\ref{fig:Airplanedistances}, where we show an improvement of
classification accuracies on several benchmark applications.

\subsubsection{Stability}

\begin{thm}\label{th:stab}
  is stable with respect to  on .
For any , one has: 
\end{thm}

\begin{proof}
Let  be such that . Let , and
let  and 
. Let  be the one-to-one bijection between  and 
induced by , and
let  be the 
one-to-one bijection between  and 
induced by the partial bijection achieving .
Then  naturally induces a one-to-one matching 
between  and  with:


Now, one has the following inequalities:


Hence, we have
.
\end{proof}

We now prove the discriminativity of .
For this, we need a stronger assumption on the persistence diagrams, namely their cardinalities have not only to be finite, but also bounded
by some .










\subsubsection{Discriminativity}

\begin{thm}\label{th:discr}
  is {\em discriminative} with respect to  on .
For any , one has: 
where . 
\end{thm}

\begin{proof}
Let . Let  be the subset of the circle delimited by the angles .
Let us consider the following set:

and similarly:

Now, we let  be the union of these sets, 
and sort  in decreasing order.
One has  since a vector  that is orthogonal to a line defined by a specific pair of 
points  appears exactly once in .

For any  that is between two consecutive , the order of the projections 
onto  of the points of both  and  remains the same. Given any point , 
we let  be its matching point according 
to the matching given by .
Then, one has the following equalities:




\begin{figure}\begin{center} 
\includegraphics[width=15cm]{images/cosineConcavity}
\caption{\label{fig:cosineConc}
The integral of  has a lower bound that depends on the length of the integral support.
In particular, when , this integral is more than  
by the Cauchy-Schwarz inequality.}
\end{center}\end{figure}

We need to lower bound .
Since , one can show that this integral cannot be less than  
using cosine concavity---see Figure~\ref{fig:cosineConc}. 
Hence, we now have the following lower bound:
 


Let . Then, one has:



Hence,  is discriminative.
\end{proof}

In particular, Theorems~\ref{th:stab} and~\ref{th:discr} allow us to show that , the distance induced by  in its RKHS,
is also equivalent to  in a broader sense: there exist continuous, positive and monotone functions  such that 
and . \\
 
The condition on the cardinalities of persistence diagrams can be relaxed. Indeed, one can prove that the feature map  induced by  
is injective when the persistence diagrams are only assumed to be finite and bounded:

\begin{prop}\label{prop:inj}
The feature map  is continuous and injective with respect to  on .
\end{prop}	

\begin{proof}
Note that if the persistence diagrams have bounded cardinalities, Proposition~\ref{prop:inj} is an immediate consequence of Theorem~\ref{th:discr}.
One has that  is continous since  is stable (cf Theorem~\ref{th:stab}).
Now, let . such that  . 
We necessarily have .
Assume that . 
Then, there must be a point  in  that is not in .
The Sliced Wasserstein distance being , there must be, for every , a point  in  
that has the same projection onto  as : , i.e. 
, the line defined by the pair . 
All these lines  intersect at .
Thus,  for any , hence 
must include an infinite number of points,
which is impossible. Thus,  and  is injective. 

\end{proof}

In particular,  can be turned into a universal kernel by considering   (cf Theorem~1 in~\cite{Kwitt15}).
This can be useful in a variety of tasks, including tests on distributions of persistence diagrams.


\subsection{Computation}\label{sec:comput}

\paragraph*{Approximate computation.} In practice, we propose to approximate  in 
time using Algorithm~\ref{alg:aksw}. This algorithm first samples  directions
in the half-circle ; it then computes, for
each sample  and for each persistence diagram , the scalar
products between the points of  and , to sort them next in a
vector . Finally, the -norm between the vectors 
is averaged over the sampled directions:

Note that one can easily adapt the proof of Lemma~\ref{lem:nd} to show that  
is negative semi-definite
by using the linearity of the sum. Hence, this approximation remains a kernel.
If the two persistence diagrams have cardinalities bounded by ,
then the running time of this procedure is . 
This approximation of   is useful since, as shown in
Section~\ref{sec:expe}, we have observed empirically that just a few directions are sufficient to get good classification accuracies.

\begin{algorithm}
\caption{Approximate computation of }
\label{alg:aksw}
\begin{algorithmic}
\STATE {\bfseries Input:} , .
\STATE Add  to  and vice-versa.
\STATE Let ; ; ;
\FOR{}
	\STATE Store the products  in an array ;
	\STATE Store the
 products  in an array ;
	\STATE Sort  and  in ascending order;
	\STATE ;
	\STATE ;
\ENDFOR
\STATE {\bfseries Output:} ;
\end{algorithmic}
\end{algorithm}  

\paragraph*{Exact computation.} A persistence diagram is said to be in {\em general position} if it has no triplet of aligned points.  
If the persistence diagrams have cardinalities bounded by , then the exact kernel computation for persistence diagrams in general position can be done in  time with 
Algorithm~\ref{alg:ksw}. In practice, given  and , we slightly modify them with infinitesimally small random perturbations. The resulting persistence diagrams 
 and  are in general position and we can approximate
 with .	

\begin{algorithm}
\caption{Exact computation of }\label{alg:ksw}
\KwIn{ with ,  with }
Let , , , ;\\
\For{}{
    Add  to ;
  }
\For{}{
    Add  to ;
  }
\For{}{
  \For{}{
    \For{}{
      Add  to ;
    }
  }
  Sort  in ascending order;\\
  \For{}{
    Add  to ;
  }
  Sort  in ascending order;\\
  Let ; \\
  \For{}{
    Let  such that ;\\
    Add  to ; Add  to ;\\
    Swap  and ;
  }
  \For{}{
    Add  to 
  }
}
\For{}{
  Let , ;\\
  Let  and ;\\
  \While{}{
  ;\\
  ;\\
  {\bf{\text if }}  {\bf {\text then }}; {\bf{\text else }};\\
  ;
  }
}
{\bf{\text return }};
\end{algorithm}  



\section{Experiments}
\label{sec:expe}

In this section, we compare  to  and  on 
several benchmark applications for which persistence diagrams have been proven useful. We compare these kernels in terms of classification 
accuracies and compuational cost. We review first our experimental setting, and review these tasks one by one.

\paragraph*{Experimental setting}
 All kernels are handled with the LIBSVM~\cite{Chang01} implementation of -SVM, and results are averaged over 10 runs
on a 2.4GHz Intel Xeon E5530 Quad Core.
The cost factor  is cross-validated in the following grid: .
Table~\ref{table:sum} summarizes the properties of the datasets we consider, namely number of labels, as well as training and test instances 
for each task. Figure~\ref{fig:taskltm} and \ref{fig:task2} illustrate how we use persistence diagrams to represent complex data.
We first describe the two baselines we considered, along with their parameterization, followed by our proposal.

\begin{table}[t]
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{|l|c|c|c|}
\hline
 Task &        Training &                               Test &                       Labels \\
\hline
Orbit &        175 &                                    75 &                         5  \\
Texture &      240 &                                    240 &                        24  \\
Human &        415 &                                    1618 &                       8 \\
Airplane &     300 &                                    980 &                        4 \\
Ant &          364 &                                    1141 &                       5 \\
Bird &         257 &                                    832 &                        4 \\
FourLeg &      438 &                                    1097 &                       6 \\
Octopus &      334 &                                    1447 &                       2 \\
Fish &         304 &                                    905 &                        3 \\
\hline          
\end{tabular}
\end{sc}
\end{small}
\caption{\label{table:sum} Number of instances in the training set, the test set and number of labels.}
\end{center}
\vskip -0.1in
\end{table}

\begin{table}[t]
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}

\begin{tabular}{|l|lll|}
\hline 
Task &          ()&     (1000) &                    (6)                                  \\
\hline 
Orbit &         &           &                                                \\        
Texture &       &    &                                               \\                                           
\hline 
Task &          &                &                                                     \\
\hline 
Human &         &           &                      \\
Airplane &      &           &                       \\
Ant &           &           &                       \\
Bird &          &           &                  \\
FourLeg &       &           &                      \\
Octopus &       &           &                       \\
Fish &          &           &                 \\
\hline                                                            
\end{tabular}
\end{sc}
\end{small}

\caption{\label{table:Acc} Classification accuracies (\%) for the benchmark applications.}
\end{center}
\vskip -0.1in
\end{table}

\begin{table}[t]
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{|l|llll|}
\hline 
Task &          () &   (1000) &     (6) &         \\
\hline 
Orbit &         &        &      &                     \\        
Texture &       &         &     &                      \\                                           
\hline 
Task &          &               &            &            (10) \\
\hline 
Human &         &         &      &   \\
Airplane &      &       &    &     \\
Ant &           &       &      &     \\
Bird &          &       &    &     \\
FourLeg &       &        &      &     \\
Octopus &       &       &      &     \\
Fish &          &      &    &     \\
\hline                                                            
\end{tabular}

\end{sc}
\end{small}
\caption{\label{table:Gram} Gram matrices computation time (s) for the benchmark applications.
As explained in the text,  represents the size of the set of possible parameters, and we have  for , 
 for  and  for .  is a constant that depends only on the
training size. In all our applications, it is less than s.}
\end{center}
\vskip -0.1in
\end{table}

\begin{figure*}[t] 
\centering
\includegraphics[width=0.98\textwidth]{images/ltm/taskltm}
\caption{\label{fig:taskltm} Sketch of the orbit recognition task. Each parameter  in the 5 possible choices
leads to a specific behavior of the orbit. 
The goal is to recover parameters from the persistent homology of orbits in the test set.}
\end{figure*}

\begin{figure*}[t] 
\centering
\includegraphics[width=0.98\textwidth]{images/task2}
\caption{\label{fig:task2} Examples of persistence diagrams computed on texture images from the \emph{OUTEX00000} dataset
and persistence diagrams computed from points on 3D shapes. One can see that corresponding points in different shapes have
similar persistence diagrams.}
\end{figure*}

\begin{figure}\centering
\includegraphics[width=7.5cm]{images/accVSdir.pdf}\ \ \includegraphics[width=7.5cm]{images/accVSdirprinceton.pdf} \\
\includegraphics[width=7.5cm]{images/timeVSdir.pdf} \ \ \includegraphics[width=7.5cm]{images/timeVSdirprinceton.pdf} \\
\includegraphics[width=7.5cm]{images/ratiooutex.pdf}\ \ \includegraphics[width=7.5cm]{images/ratioprinceton.pdf}
\caption{\label{fig:plots} The first column corresponds to the orbit recognition and the texture classification while the second
column corresponds to 3D shape segmentation. 
On each column, the first row shows the dependence of the accuracy on the number of directions,
the second row shows the dependence of a single Gram matrix computation time, and the third row
shows the dependence of the ratio of the approximation of  and the exact .
Since the box plot of the ratio for orbit recognition is very similar to that of 3D shape segmentation,
we only give the box plot of texture classification in the first column. }
\end{figure}


\paragraph*{PSS.} The {\em Persistence Scale Space} kernel ~\cite{Reininghaus15} is
defined as the scalar product of the two solutions of the heat diffusion equation
with initial Dirac sources located at the points of the persistence diagram. It has the following closed form expression:

where  is the symmetric of  along the diagonal.
Since there is no clear heuristic on how to tune , this parameter is chosen in the applications 
by ten-fold cross-validation with random 50\%-50\%
training-test splits and with the following set of  values: , , , , 
, , , , , , ,  and . 

\paragraph*{PWG.} Let  and  and  be two persistence diagrams.
Let  be the Gaussian kernel with parameter .
Let  be the RKHS associated to .

Let  
be the kernel mean embedding of  weigthed by the diagonal distances.
Let  be defined similarly. Let .
The {\em Persistence Weighted Gaussian} kernel ~\cite{Kusano16, Kusano17} is
defined as the Gaussian kernel with parameter  on :

The authors in~\cite{Kusano16} provide heuristics to compute ,  and 
and give a rule of thumb to tune . Hence, in the applications we select  according to the rule of thumb, and
 we use ten-fold cross-validation with random 50\%-50\% training-test splits to chose ,  and . 
The ranges of possible values is obtained 
by multiplying the values computed with the heuristics
with the following range of  factors: , , ,  and ,
leading to  different sets of parameters. \\

\paragraph*{Parameters for .} The kernel we propose has only one parameter, the bandwidth  in Eq.~\ref{eq:kSW}, 
which we choose
using ten-fold cross-validation with random 50\%-50\% training-test splits. 
The range of possible values is obtained by computing the squareroot of the median, the first and the last deciles 
of all  in the training set,
then by multiplying these values by the following range of  factors: , , ,  and , 
leading to  possible values. 


\paragraph*{Parameter Tuning.} The bandwidth of  is, in practice, easier to tune than the parameters of its two competitors
when using grid search. Indeed, as is the case for all infinitely divisible kernels, the Gram matrix does not need to be 
recomputed for each choice of , since it only suffices to compute all the Sliced Wasserstein distances between persistence diagrams 
in the training set once. On the contrary, neither  nor  share this property, and require recomputations for each hyperparameter choice.
Note however that this improvement may no longer hold if one uses other methods to tune parameters.  
For instance, using  without cross-validation is possible with the heuristics given by the authors in~\cite{Kusano16}, 
and leads to smaller training times, but also to worse accuracies. 



\subsection{3D shape segmentation}
Our first task, whose goal is to produce point classifiers for 3D shapes, follows that presented in~\cite{Carriere15a}.

\paragraph*{Data.}  We use some categories of the mesh segmentation benchmark of Chen et al.~\cite{Chen09},
which contains 3D shapes classified in several categories (``airplane'', ``human'', ``ant''...).
For each category, our goal is to design a classifier that can assign, to each point in the shape,
a label that describes the relative location of that point in the shape. For instance, possible labels are, for the human category, 
``head'', ``torso'', ``arm''...
To train classifiers, we compute a persistence diagram per point using the geodesic distance function to this point---see~\cite{Carriere15a} for details.
We use 1-dimensional persistent homology (0-dimensional would not be informative since the shapes are connected,
leading to solely one point with coordinates  per persistence diagram). 
For each category, the training set contains one hundredth of the points of the first five 3D shapes,
and the test set contains one hundredth of the points of the remaining shapes in that category. Points in
training and test sets are evenly sampled. See Figure~\ref{fig:task2}.
Here, we focus on comparison between persistence diagrams, and not
on achieving state-of-the-art results. It has been proven that persistence diagrams bring complementary information
to classical descriptors in this task---see~\cite{Carriere15a}, 
hence reinforcing their discriminative power with appropriate kernels is of great interest.
Finally, since data points are in , we set the  parameter of  to . 

\paragraph*{Results.} Classification accuracies are given in Table~\ref{table:Acc}.
For most categories,  outperforms competing kernels by a significant margin.
The variance of the results over the run is also less than that of its competitors. 
However, training times are not better in general. 
Hence, we also provide the results for an approximation of  with  directions.
As one can see from Table~\ref{table:Acc} and from Figure~\ref{fig:plots}, this approximation leaves the accuracies almost unchanged, 
while the training times become comparable with the ones of the other competitors. Moreover, 
according to Figure~\ref{fig:plots}, using even less directions
would slightly decrease the accuracies, but still outperform the competitors performances,
while decreasing even more the training times. 


\subsection{Orbit recognition}\label{sec:expeorbit}

In our second experiment, we use synthetized data.
The goal is to retrieve parameters of dynamical system orbits,
following an experiment proposed in~\cite{Adams17}.

\paragraph*{Data.} We study the {\em linked twist map}, a discrete dynamical system modeling
fluid flow. It was used in~\cite{Hertzsch07} to model flows in DNA microarrays.
Its orbits can be computed given a parameter  and
initial positions  as follows:



Depending on the values of , the orbits may exhibit very different behaviors. For instance,
as one can see in Figure~\ref{fig:taskltm}, when  is 3.5, there seems to be no interesting topological features
in the orbit, while voids form for  parameters around 4.3.
Following~\cite{Adams17}, we use 5 different parameters , that act as labels.
For each parameter, we generate 100 orbits with 1000 points and random initial positions. We then compute
the persistence diagrams of the distance functions to the point clouds with the GUDHI library~\cite{gudhi} and we use them (in all homological dimensions) 
to produce an orbit classifier
that predicts the parameter values, by training over a 70\%-30\% training-test split of the data.
Since data points are in , we set the  parameter of  to . \\

\paragraph*{Results.} Since the persistence diagrams contain thousands of points, we use kernel approximations
to speed up the computation of the Gram matrices.
In order for the approximation error to be bounded by , 
we use an approximation of  with  directions (as one can see from Figure~\ref{fig:plots}, 
this has a small impact on the accuracy), we approximate  with  random Fourier features~\cite{Rahimi08},
and we approximate  using Fast Gauss Transform~\cite{Morariu09}
with a normalized error of . 
One can see from Table~\ref{table:Acc} that the accuracy is increased a lot with .
Concerning training times, there is also a large improvement since we tune the parameters with grid search. 
Indeed, each Gram matrix needs not be recomputed for each parameter when using .

\subsection{Texture classification}

Our last experiment is inspired from~\cite{Reininghaus15} and~\cite{Li14}. 
We use the \emph{OUTEX00000} data base~\cite{Ojala02} for texture classification. 

\paragraph*{Data.} persistence diagrams are obtained for each texture image by computing first the sign component of CLBP descriptors~\cite{Guo10} 
with radius  and  neighbors for each image,
and then compute the persistent homology of this descriptor using the GUDHI library~\cite{gudhi}. 
See Figure~\ref{fig:task2}.
Note that, contrary to the experiment of~\cite{Reininghaus15}, we do not downsample the images to  images,
but keep the original  images. 
Following~\cite{Reininghaus15}, we restrict the focus to 0-dimensional persistent homology.
We also use the first 50\%-50\% training-test split given in the database to produce classifiers. 
Since data points are in , we set the  parameter of  to . 

\paragraph*{Results} We use the same approximation procedure as in Section~\ref{sec:expeorbit}.
According to Figure~\ref{fig:plots}, even though the approximation of  is rough,
this has again a small impact on the accuracy, while reducing the training time by a significant margin.
As one can see from Table~\ref{table:Acc}, 
using  leads to almost state-of-the-art results~\cite{Ojala02, Guo10},
closely followed by the accuracies of  and .
The best timing is given by , again because we use grid search. 
Hence,  almost achieves the best result, and its training time is
better than the ones of its competitors, due to the grid search parameter tuning.






\begin{figure}\centering
\includegraphics[width=8cm]{images/distance/distances.pdf}
\caption{\label{fig:Airplanedistances} We show how the metric  is distorted.
Each point represents a pair of persistence diagrams and its abscissae is the first diagram distance between them. 
Depending on the point color, its ordinate is the logarithm of the distance between persistence diagrams in the RKHS induced by either
 (blue points),  (green points),  (red points) and a Gaussian kernel on  (black points).  }
\end{figure}

\subsection{Metric Distortion.} 

To illustrate the equivalence theorem, we also show in Figure~\ref{fig:Airplanedistances} 
a scatter plot where each point represents the comparison of two persistence diagrams taken from the Airplane segmentation data set. 
Similar plots can be obtained with the other datasets considered here.
For all points, the x-axis quantifies the first diagram distance  for that pair,
while the y-axis is the logarithm of the RKHS distance induced by either , ,  
or a Gaussian kernel directly applied to , to obtain comparable quantities. 
We use the parameters given by the cross-validation procedure described above.
One can see that the distances induced by  are less spread than the others,
suggesting that the metric induced by  is more discriminative.
Moreover the distances given by  and the Gaussian kernel on  exhibit the same behavior, 
suggesting that  is the best natural equivalent of a Gaussian kernel for persistence diagrams. 



\section{Conclusion}

In this article, we introduce the {\em Sliced Wasserstein kernel},
a new kernel for persistence diagrams that is provably {\em equivalent} to the first
diagram distance between persistence diagrams. We provide fast algorithms to approximate it,
and show on several datasets substantial improvements in accuracy and training times 
(when tuning parameters is done with grid search) over competing kernels. 
A particularly appealing property of that kernel is that it is infinitely divisible, 
substantially facilitating the tuning of parameters through cross validation.

\paragraph{Acknowledgements.}
SO was supported by ERC grant Gudhi and by ANR project TopData. 
MC was supported by a {\em chaire de l'IDEX Paris Saclay}.

\bibliography{biblio}
\bibliographystyle{plain}

\end{document}
