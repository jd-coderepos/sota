\documentclass[11pt]{article}
\usepackage{fullpage}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[boxed,lined]{algorithm2e}


\usepackage{versions}\excludeversion{INUTILE}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newenvironment{proof}{\textit{Proof:}}{\hfill \paragraph{} }
\newtheorem{example}{Example}


\providecommand{\pii}{\ensuremath{\pi^{-1}}}  

\newcommand{\Oh}[1]
    {\ensuremath{\mathcal{O}\left( {#1} \right)}}
\newcommand{\occ}[2]
    {\ensuremath{|{#2}|_{#1}}}
\newcommand{\access}
    {\ensuremath{\mathsf{access}}}
\newcommand{\rank}
    {\ensuremath{\mathsf{rank}}}
\newcommand{\select}
    {\ensuremath{\mathsf{select}}}
\newcommand{\runs}
    {\ensuremath{\mathsf{runs}}}
\newcommand{\sets}
    {\ensuremath{\mathsf{sets}}}
\newcommand{\union}
    {\ensuremath{\mathsf{union}}}
\newcommand{\find}
    {\ensuremath{\mathsf{find}}}
\newcommand{\HH}{\mathcal{H}}
\newcommand{\Ho}{\HH_0}
\newcommand{\Hk}{\HH_k}

\newcommand{\libcds}{{\sc Libcds}}

\newcommand{\mFromRaman}{\ensuremath\mu} 

\newcommand{\mapping}{\ensuremath{{m}}}
\newcommand{\Mapping}{\ensuremath{{M}}}

\newcommand{\Cs}{\mathcal{C}}

\begin{document}

\title{Efficient Fully-Compressed Sequence Representations
\thanks{Funded in part by Fondecyt Project 1-110066, Chile.
An early version of this article appeared in {\em Proc. 21st Annual 
International Symposium on Algorithms and Computation (ISAAC)}, part II,
pp. 215--326, 2010.}
}

\author{J\'{e}r\'{e}my Barbay\thanks{Dept. of Computer Science, 
        University of Chile. {\tt \{jbarbay|gnavarro|yasha\}@dcc.uchile.cl}.}
\and
    Francisco Claude\thanks{David R. Cheriton School of Computer Science,
University of Waterloo, Canada. {\tt fclaude@cs.uwaterloo.ca}.}
\and
    Travis Gagie\thanks{Dept. of Computer Science, Aalto University, Finland. 
    {\tt travis.gagie@gmail.com}.}
\and
    Gonzalo Navarro
\and
    Yakov Nekrich
}

\date{}
\maketitle

\setcounter{footnote}{0}

\begin{abstract}
  We present a data structure that stores a sequence  over
  alphabet  in  bits, where
   is the zero-order entropy of .
This structure supports the queries \access, \rank\ and \select,
  which are fundamental building blocks for many other compressed data 
  structures, in worst-case time 
  and average time .
The worst-case complexity matches the best previous results, yet these had
  been achieved with data structures using  bits. 
  On highly compressible sequences the  bits of the redundancy 
  may be significant compared to the the  bits that encode the data. 
  Our representation, instead, compresses the redundancy as well. Moreover, our 
  average-case complexity is unprecedented.
  
  Our technique is based on partitioning the alphabet into characters of 
  similar frequency. The subsequence corresponding to each group can then be 
  encoded using fast uncompressed representations without harming the overall
  compression ratios, even in the redundancy.
  
  The result also improves upon the best current compressed
  representations of several other data structures.
For example, we achieve
   compressed redundancy, retaining the best time complexities, for the 
  smallest existing full-text self-indexes; 
   compressed permutations  with times for  and 
   improved to loglogarithmic; and 
   the first compressed representation of dynamic collections of 
  disjoint sets.
We also point out various applications to inverted indexes,
  suffix arrays, binary relations, and data compressors.

  Our structure is practical on large alphabets. Our experiments show that,
  as predicted by theory, it dominates the space/time tradeoff map
  of all the sequence representations, both in synthetic and application
  scenarios.
\end{abstract}

\section{Introduction} \label{sec:introduction}

A growing number of important applications require data
representations that are space-efficient and at the same time support
fast query operations.  In particular, suitable representations of
{\em sequences} supporting a small set of basic operations yield
space- and time-efficient implementations for many other data
structures such as full-text indexes \cite{GGV03,GMR06,FMMN07,NM07},
labeled trees \cite{BHMR07,BGMR07,FLMM09}, binary relations
\cite{BHMR07,BCN10}, permutations \cite{BN09} and two-dimensional
point sets \cite{MN07,BLNS09}, to name a few.

Let  be a sequence of characters belonging to alphabet
.  In this article we focus on the following set of
operations, which is sufficient for many applications:
\begin{description}
\item[] returns the th character of sequence ,
  which we denote ;
\item[] returns the number of occurrences of
  character  up to position  in ; and
\item[] returns the position of the th
  occurrence of  in .
\end{description}

Table~\ref{tab:previous} shows the best sequence representations and
the complexities they achieve for the three queries, where 
refers to the -th order empirical entropy of  \cite{Man01}. To
implement the operations efficiently, the representations require some
{\em redundancy} space on top of the  or  bits
needed to encode the data.
For example, multiary wavelet trees (row 1) represent  within
zero-order entropy space plus just  bits of redundancy, and
support queries in time . This is
very attractive for relatively small alphabets, and even constant-time
for polylog-sized ones. For large , however, all the other
representations in the table are exponentially faster, and some even
achieve high-order compression. However, their redundancy is higher,
 bits. While this is still asymptotically negligible
compared to the size of a plain representation of , on highly
compressible sequences such redundancy is not always negligible
compared to the space used to encode the compressed data. This raises
the challenge of retaining the efficient support for the queries {\em
  while compressing the index redundancy} as well.

In this paper we solve this challenge in the case of zero-order
entropy compression, that is, the redundancy of our data structure is
asymptotically negligible compared to the zero-order compressed text
size (not only compared to the plain text size), plus  bits. The
worst-case time our structure achieves is , which
matches the best previous results for large . Moreover, the
average time is {\em logarithmic on the entropy of the sequence},
, under reasonable assumptions on the query
distribution. This average time complexity is also unprecedented: the
only previous entropy-adaptive time complexities we are aware of come
from Huffman-shaped wavelet trees \cite{GGV03}, which have 
recently been shown capable of achieving 
query time with just  bits of redundancy \cite[Thm.~5]{BN11}.


\begin{table}[t]
  \caption{
    Best previous bounds and our new ones for data structures supporting
    \access, \rank\ and \select.
The space bound of the form  holds for any ,
and those of the form  hold for any constant .
On average  becomes  in our time complexities (see 
Corollary~\ref{cor:partitioning-avg}) and in row 1 \cite[Thm.~5]{BN11}.
  }
\label{tab:previous}
\medskip
\resizebox{\textwidth}{!}
{\begin{tabular}
{c@{\hspace{1ex}}|@{\hspace{2ex}}c@{\hspace{3ex}}c@{\hspace{3ex}}c@{\hspace{3ex}}c@{\hspace{1ex}}c}
                           & space (bits)                                                & \access                               & \rank                                        & \select                                  \\
\hline \n \Ho (s) + o(n)1ex]
\cite[Lem.~4.1]{BHMR07}  &             &     &  &  \n \Hk (s) + o(n\lg \sigma)1ex] \cite[Thm.~2.2]{GMR06} &  &  &                         &  \-1ex]
Thm.~\ref{thm:partitioning} &                                &                  &                         &                                  \n \Ho(s) + o(n)(\Ho(s)+1)1ex]
Cor.~\ref{cor:partitioning-constant} &  & 
&                         &  \s [1..n]\sigma \le n\mapping\mapping [a] ~=~ \lceil \lg (n / \occ{a}{s}) \lg n \rceil, \left[1..\left\lceil \lg^2 n \right\rceil \right]\mapping [a]0 \leq \ell \leq \lceil \lg^2 n\rceils_\ell
  [1..\occ{\ell}{t}] s_\ell [t.\rank_\ell(i)] = \mapping.\rank_\ell (s [i]),
s.\access (i) & = & m.\select_\ell(s_\ell.\access(t.\rank_\ell(i))),
                        ~\textrm{where}~\ell = t.\access(i); \1ex]
s.\select_a (i) & = & t.\select_\ell (s_\ell.\select_c (i))
                        ~\textrm{where}~\ell = \mapping.\access(a)
                        ~\textrm{and}~ c = \mapping.\rank_\ell(a).
 \label{eq:H}
\Ho(s) ~~=~~ \sum_{a\in[1..\sigma]}
\frac{\occ{a}{s}}{n}\lg\frac{n}{\occ{a}{s}}.
 \label{eq:sumclase}
\sum_{c,~\ell=\mapping[c]} \occ{c}{s}
~~=~~ |s_\ell|\,.

\ell ~~=~~ \lceil \lg(n/\occ{a}{s})\lg n \rceil 
	& = & \lceil \lg(n/\occ{b}{s})\lg n \rceil, \nonumber \\
\hspace*{-3cm} \textrm{therefore \hspace{2cm}} 
\lg (n / \occ{b}{s}) - \lg (n / \occ{a}{s}) 
	& < & 1 / \lg n, \nonumber \\
\hspace*{-3cm} \textrm{and so \hspace{5.5cm}} 
\occ{a}{s}  & < & 2^{1 / \lg n} \occ{b}{s}\,. \label{eq:ineq}

\sum_{b,~\ell=\mapping[b]} \occ{a}{s} & < & \nonumber
\sum_{b,~\ell=\mapping[b]}  2^{1/\lg n}\,\occ{b}{s},\1ex]
\sigma_\ell & < & 2^{1/\lg n}\,|s_\ell|/\occ{a}{s}\,. \label{eq:ineq2}

& & \lefteqn{n \Ho (t) ~+~ \sum_\ell |s_\ell| \lg \sigma_\ell} \1ex]
& < & \sum_\ell \sum_{a,~\ell=\mapping[a]} \occ{a}{s} \lg (n / |s_\ell|) 
      ~+~ \sum_\ell \sum_{a,~\ell=\mapping[a]} \occ{a}{s} \lg \left( 2^{1/
                            \lg n} |s_\ell| / \occ{a}{s} \right)\1ex]
& = & \sum_a \occ{a}{s} \lg (n / \occ{a}{s}) ~+~ n / \lg n \
\end{proof}

In other words, if we represent  with  bits per character
and each  with  bits per character,
we achieve a good overall compression.
Thus we can obtain a very compact representation
of a sequence  by storing a compact representation of  and storing
each  as an ``uncompressed'' sequence over an alphabet
of size .

\subsection{Concrete representation}
\label{sec:repr}

We represent  and  as multiary wavelet trees~\cite{FMMN07};
 we represent each  as either a multiary wavelet tree or an instance of
Golynski et al.'s~\cite[Thm.~2.2]{GMR06}  \access/\rank/\select\ data structure,
depending on whether  or not.
The wavelet tree for  uses at most
 bits and operates
in constant time, because its alphabet size is polylogarithmic
(i.e., ).
If  is represented as a wavelet tree, it uses at most
 bits\footnote{This is achieved by using block sizes
of length  and not , at the price
of storing universal tables of size  bits.
Therefore all of our  expressions involving  and other variables will be
asymptotic in .}
and again operates in constant time because ; otherwise it uses at most
 bits
(the latter because ).
Thus in either case the space for  is bounded by
 bits.
Finally, since  is a sequence of length  over an alphabet of size
, the wavelet tree for  takes
 bits and also operates in constant time. Because of the convexity property we referred to in the
beginning of this section, , the space for 
 is
.

Therefore we have  bits for , 

bits for the  sequences, and  bits for . Using
Lemma~\ref{lem:space}, this adds up to
, 
where the  term is .

Using the variant of Golynski et al.'s data structure \cite[Thm.~4.2]{GMR06}, 
that gives constant-time \select, and  time for \rank\ and
\access, we obtain our first result in Table~\ref{tab:previous} (row 4). 
To obtain our second result (row 5), we use instead Grossi et al.'s result 
\cite[Cor.~2]{GOR10}, which gives constant-time \access, and 
time for \rank\ and \select. We note that their structure takes space 
, 
yet we only need this to be at most 
.

\begin{theorem} \label{thm:partitioning}
We can store  over effective alphabet  in
 
bits and support \access, \rank\ and \select\ queries in
, , and  time,
respectively (variant (i)); or in
,  and
 time, respectively (variant (ii)).
\end{theorem}

We can refine
the time complexity by noticing that the only non-constant times are due to
operating on some sequence , where the alphabet is of size
, where  is the character
in question, thus . If we
assume that the characters  used in queries distribute with the same
frequencies as in sequence  (e.g., \access\ queries refer to randomly chosen
positions in ), then the average query time becomes 

by the log-sum inequality\footnote{Given  pairs of numbers 
, it holds that .
Use  and  to obtain the result.}.

\begin{corollary} \label{cor:partitioning-avg}
The  time complexities in
Theorem~\ref{thm:partitioning} are also
, where  stands for  in the 
{\access} query, and for the character argument in the 
 and  queries. If these characters  distribute on queries
with the same frequencies as , the average time complexity for those
operations is .
\end{corollary}

Finally, to obtain our last result in Table~\ref{tab:previous} we use again
Golynski et al.'s representation \cite[Thm.~4.2]{GMR06}. Given 
 extra space to store the inverse of a
permutation inside chunks, it answers \select\ queries in time  
and \access\ queries in time  (these two complexities can be
interchanged), and \rank\ queries in time
. While we initially considered
 to achieve the main result, using a constant
 yields constant-time \select\ and \access\ simultaneously.

\begin{corollary} \label{cor:partitioning-constant}
We can store  over effective alphabet  in
 bits, for any constant , and 
support \access,  and \select\ queries in
, , and  time,
respectively (variant (i)); or in , , and , respectively (variant
(ii)).
\end{corollary}

\subsection{Handling arbitrary alphabets} \label{sec:effective}

In the most general case,  is a sequence over an alphabet 
that is not an effective alphabet, and  characters from 
occur in .
Let  be the set of elements that occur in ; we can map
characters from  to elements of  by replacing
each  with its rank in .
All elements of  are stored in the ``indexed dictionary'' (ID) data
structure described by Raman et al.~\cite{RRR02}, so that the
following queries are supported in constant time:
for any  its rank in  can be found (for any
 the answer is );
and for any  the -th smallest element in 
can be found. The ID structure uses
 bits
of space, where
 is the base of the natural logarithm and
 is the maximal element in ;
the value of  can be specified with additional
 bits.
We replace every element in  by its rank in , and the
resulting sequence is stored using Theorem~\ref{thm:partitioning}.
Hence, in the general case the space usage is increased by
 bits and
the asymptotic time complexity of queries remains unchanged.
Since we are already spending  bits in our data
structure, this increases the given space only by 
.

\subsection{Application to fast encode/decode}

Given a sequence  to encode, we can build mapping  from its
character frequencies , and then encode each  as the pair
. 
Lemma~\ref{lem:space} (and some of the discussion that 
follows in Section~\ref{sec:repr}) shows that the overall output size is
 bits if we represent the sequence of pairs 
by partitioning it into three sequences: (1) the left part of the pairs in one 
sequence, using Huffman coding on chunks (see next); (2) the right part of the 
pairs corresponding to values where  in a second sequence, 
using Huffman coding on chunks; (3) the remaining right parts of the pairs, 
using plain encoding in  bits (note 
). The Huffman coding on chunks groups
 characters, so that even in the case of the left 
parts, where the alphabet is of size , the total length 
of a chunk is at most  bits, and hence the Huffman coding
table occupies just  bits. 
The redundancy on top of  adds up to  
bits in sequences (1) and (2) (one bit
of Huffman redundancy per chunk) and  in sequence (3) 
(one bit, coming from the ceil function, per 
encoded bits).

The overall encoding time is . A pair  is 
decoded as , where after reading  we can
compute  to determine whether  is encoded in sequence (2) or
(3). Thus decoding also takes constant time if we can decode Huffman codes in
constant time. This can be achieved by using canonical codes and limiting the
height of the tree \cite{MT97,GN09}. 

This construction gives an interesting space/time tradeoff with respect to
classical alternatives. Using just Huffman coding yields 
encoding/decoding time, but only guarantees  bits of space. 
Using arithmetic coding achieves  bits, but encoding/decoding
is not linear-time. The tradeoff given by our encoding,  bits
and linear-time decoding, is indeed the
reason why it is used in practice in various folklore applications, as
mentioned in the Introduction. In Section~\ref{sec:exp-compr} we
experimentally evaluate these ideas and show they are practical. 
Next, we give more far-fetched applications
of the \rank/\select\ capabilities of our structure, which go much beyond
the mere compression.

\section{Applications to text indexing}
\label{sec:app-text}

Our main result can be readily carried over various types of indexes for
text collections. These include self-indexes for general texts, and positional
and non-positional inverted indexes for natural language text collections.

\subsection{Self-indexes}
\label{sec:higher-order}

A self-index represents a sequence and supports
operations related to text searching on it.
A well-known self-index~\cite{FMMN07} achieves -th order entropy space
by partitioning the Burrows-Wheeler transform~\cite{BW94}
of the sequence and encoding
each partition to its zero-order entropy. Those partitions must support
queries \access\ and \rank. By using Theorem~\ref{thm:partitioning} to
represent such partitions, we achieve the following result, improving previous
ones~\cite{FMMN07,GMR06,BHMR07}.

\begin{theorem} \label{thm:self}
  Let  be a sequence over effective alphabet .
Then we can represent  using  bits,
  for any  and constant
  , while supporting the following queries:
 count the number of occurrences of a pattern  in ,
  in time ;
 locate any such occurrence in time ;
 extract  in time .
\end{theorem}

\begin{proof}
To achieve  space, the Burrows-Wheeler transformed text  is 
partitioned into  sequences  \cite{FMMN07}. 
Since , it follows that . The space our Theorem~\ref{thm:partitioning} achieves using 
such a partition is 
.
Let  (so  whenever ) and 
classify the sequences  according to whether  (short 
sequences) or not (long sequences). The total space occupied by the short 
sequences can be bounded by  bits. In turn, the space occupied by the
long sequences can be bounded by
 bits, for some constants . An argument very
similar to the one used by Ferragina et al.~\cite[Thm.~4.2]{FMMN07} shows
that these add up to 
. Thus the space
is . Other structures required by the alphabet
partitioning technique \cite{FMMN07} add  more bits if .

The claimed time complexities stem from the
\rank\ and \access\ times on the partitions.
The partitioning scheme \cite{FMMN07} adds
just constant time overheads. Finally, to achieve the claimed locating and 
extracting times we sample one out of every  text positions.
This maintains our lower-order space term  within 
. 
\end{proof}

In case  is not the effective alphabet we proceed as described 
in Section~\ref{sec:effective}.
Our main improvement compared to Theorem~4.2 of Barbay et al.~\cite{BHMR07}
is that we have compressed the redundancy from  to 
. Our improved locating times, instead, just owe to the denser
sampling, which Barbay et al.\ could also use.

Note that, by using the zero-order representation of Golynski et
al.~\cite[Thm.~4]{GRR08}, we
could achieve even better space,  bits, and time complexities
 instead of .\footnote{One can retain  
in the denominator by using block sizes depending on  and not on , 
as explained in the footnote at the beginning of Section~\ref{sec:repr}.}
Such complexities are convenient for not so large alphabets.



\subsection{Positional inverted indexes}

These indexes retrieve the positions of any word in a text. They
may store the text compressed up to the zero-order
entropy of the {\em word} sequence , which allows direct access to
any word. In addition they store the list of the positions where each distinct
word occurs. These lists can be compressed up to a second zero-order entropy
space \cite{NM07}, so the overall space is at least . By regarding
 as a sequence over an alphabet  (corresponding here to the
vocabulary), Theorem~\ref{thm:partitioning} represents  within 
 bits, which provides state-of-the-art
compression ratios. Variant  supports 
constant-time access to any text word , and access to the th 
entry of the list of any word  () in time . 
These two time complexities are exchanged in variant , or both can be 
made constant by spending  redundancy for any constant 
 (using Corollary~\ref{cor:partitioning-constant}). 
The length of the inverted lists can be stored within 
 bits (we also need at least this space to store the sequence 
content of each word identifier).

Apart from supporting this basic access to the list of each word, this
representation easily supports operations that are more complex to implement
on explicit
inverted lists \cite{BLOLS09}. For example, we can find the phrases formed
by two words  and , that appear  and  times, by finding the 
occurrences of one and verifying the other in the text, in time 
. Other more sophisticated intersection algorithms
\cite{BLOLS09} can be implemented by supporting operations such as ``find the
position in the list of  that follows the th occurrence of word ''
(, in time ) or ``give
the list of word  restricted to the range  in the collection''
(, for , until exceeding , in time
 plus  per retrieved occurrence).
In Section~\ref{sec:exp-invl} we evaluate this representation in practice.

\subsection{Binary relations and non-positional inverted indexes}
\label{sec:binrels}

Let , where  are called {\em labels}
and  are called {\em objects}, be a binary relation consisting 
of  pairs. Barbay et al.~\cite{BGMR07} represent the relation as follows. 
Let  be the labels related to an object 
. Then we define sequence . The 
representation for  is the concatenated sequence 
, of length , and 
the bitmap , of length 
.

This representation allows one to efficiently support various queries 
\cite{BGMR07}:
\begin{description}
\item[{\tt table\_access}]:
is  related to ?,
	;
\item[{\tt object\_select}]:
the th label related to an object , 
	; 
\item[{\tt object\_nb}]:
the number of labels an object  is related to, 
	; 
\item[{\tt object\_rank}]:
the number of labels  an object  is related to,
	carried out with a predecessor search in 
        , an
        area of length .
	The predecessor data structure requires  bits as it
        is built over values sampled every  positions, and the
	query is completed with a binary search;
\item[{\tt label\_select}]:
the th object related to a label , 
	;
\item[{\tt label\_nb}]:
the number of objects a label  is related to,
	. It can also be solved like {\tt object\_nb},
        using a bitmap similar to  that traverses the table label-wise;
\item[{\tt label\_rank}]:
the number of objects  a label  is related to,
	. 
\end{description}

Bitmap  can be represented within  bits and support all the operations in constant time 
\cite{RRR02}, and its label-wise variant needs  bits.
The rest of the space and time complexities depend on how we represent .

Barbay et al.~\cite{BGMR07} used Golynski et al.'s representation for 
\cite{GMR06}, so they achieved  bits of space,
and the times at rows 2 or 3 in Table~\ref{tab:previous} for the
operations on  (later, Barbay et al.~\cite{BHMR07} achieved  bits and slightly worse times).
By instead representing  using Theorem~\ref{thm:partitioning},
we achieve compressed redundancy and slightly improve the times. 

To summarize, we achieve  bits, and solve 
{\tt label\_nb} and {\tt object\_nb} in constant
time, and {\tt table\_access} and {\tt label\_rank} in time .
For {\tt label\_select}, {\tt object\_select} and {\tt object\_rank} we
achieve times ,  and , 
respectively, or ,  and , 
respectively. Corollary~\ref{cor:partitioning-constant} yields a slightly
larger representation with improved times, and a multiary wavelet tree
\cite[Thm.~4]{GRR08} achieves less space and different times; we leave the 
details to the reader.

A non-positional inverted index is a binary relation that associates each
vocabulary word with the documents where it appears. A typical representation
of the lists encodes the differences between consecutive values, achieving
overall space , where word  appears in
 documents \cite{WMB99}. In our representation as a binary relation,
it turns out that , and thus the
space achieved is comparable to the classical schemes. Within this space,
however, the representation offers various interesting operations apart
from accessing the th element of a list (using {\tt label\_select}),
including support for various list intersection algorithms;
see Barbay et al.~\cite{BGMR07,BHMR07} for more details.

\section{Compressing permutations} \label{sec:permutations}

Barbay and Navarro \cite{BN09} measured the compressibility of a permutation 
 in terms of the entropy of the distribution of the lengths of {\em runs} 
of different kinds. Let  be covered by  runs (using any of the 
previous definitions of runs~\cite{LP94,BN09,Meh79})
of lengths .
Then  is called the {\em entropy} of the runs (and, because , it also holds ).
In their most recent variant \cite{BN11} they were able to store  in
 
bits for runs consisting of interleaved sequences of increasing or decreasing
values, and  
bits for contiguous sequences of increasing or decreasing values (or,
alternatively, interleaved sequences of consecutive values). 
In all cases they can compute  and  in 
 time, which on average drops to 
 if the queries are 
uniformly distributed in .

We now show how to use \access/\rank/\select\ data structures to support the 
operations more efficiently while retaining compressed redundancy space.
In general terms, we exchange their  space term by 
, and improve their times to  in the
worst case, and to  on average (again, this is an
improvement only if  is not too small).

We first consider interleaved sequences of
increasing or decreasing values as first defined by
Levcopoulos and Petersson \cite{LP94} for adaptive
sorting, and later on for compression~\cite{BN09}, and then give
improved results for more restricted classes of runs.
In both cases we first consider the application of the permutation
 and its inverse, , and later show how to extend the
support to the iterated application of the permutation, ,
extending and improving previous results~\cite{MRRR03}.

\begin{theorem} \label{thm:runs}
  Let  be a permutation on  elements that consists of 
  interleaved increasing or decreasing runs, of lengths .
  Suppose we have a data structure that stores a sequence 
  over effective alphabet  within  bits,
  supporting queries \access, \rank, and \select\ in time .
Then, given its run decomposition, we can store  in 
   bits,
  and perform  and  queries in time .
\end{theorem}

\begin{proof}
We first replace all the elements of the th run by , for .
Let  be the resulting sequence and let  be  permuted according to 
, that is, .
We store  and  using the given sequence representation, and also store 
 bits indicating whether each run is increasing or decreasing. Note that
, which gives the claimed space.

Notice that an increasing run preserves the relative order of the elements of
a subsequence. Therefore, if  is part of an increasing run, then 
, so

If, instead,  is part of a decreasing run, then , so

A  query is symmetric (exchange  and  in the formulas).  
Therefore we compute  and  with  calls to \access,
\rank, and \select\ on  or .
\end{proof}



\begin{example}
Let  be formed by three runs (indicated by
the different fonts). Then  and 
.
\end{example}

By combining Theorem~\ref{thm:runs} with the representations in 
Theorem~\ref{thm:partitioning}, we obtain a result that improves upon previous 
work \cite{BN09,BN11} in time complexity. Note that if the queried positions 
are uniformly distributed in , then all the \access, \rank, and
\select\ queries follow the same character distribution of the runs, and
Corollary~\ref{cor:partitioning-avg} applies.
Note also that the  bits are contained in  because
.

\begin{corollary} \label{cor:runs}
  Let  be a permutation on  elements that consists of 
  interleaved increasing or decreasing runs, of lengths .
Then, given its run decomposition, we can store  in 
   bits and perform  and  queries in
   time.
  On uniformly distributed queries the average times are
  .
\end{corollary}

\smallskip

The case where the runs are contiguous is handled within around half the 
space, as a simplification of Theorem~\ref{thm:runs}.

\begin{corollary} \label{cor:contruns-gral}
  Let  be a permutation on  elements that consists of 
  contiguous increasing or decreasing runs, of lengths .
  Suppose we have a data structure that stores a sequence 
  over effective alphabet  within  bits,
  supporting queries \access\ and \rank\ in time , 
  and \select\ in time .
Then, given its run decomposition, we can store  in 
   bits of 
  space, and perform  queries in time  and 
   queries in time .
\end{corollary}
\begin{proof}
We proceed as in Theorem~\ref{thm:runs}, yet now sequence  is of the form
, and therefore it can be represented
as a bitmap . The required
operations are implemented as follows: ,
,
,
and . Those operations are solved in
constant time using a representation for  that takes
 bits
\cite{RRR02}. Added to the  bits that mark increasing or decreasing
sequences, this gives the claimed space. The claimed time complexities 
correspond to the operations on , as those in  take constant time.
\end{proof}

Once again, by combining the corollary with representation  in 
Theorem~\ref{thm:partitioning}, we 
obtain results that improve upon previous work \cite{BN09,BN11}.
The  bits are in  
because they are  as long as , and otherwise they are
, and .

\begin{corollary} \label{cor:contruns}
  Let  be a permutation on  elements that consists of 
  contiguous increasing or decreasing runs, of lengths .
Then, given its run decomposition, we can store  in 
   bits and 
  perform  queries in time  and  queries in
  time  (and  on average for 
  uniformly distributed queries).
\end{corollary}

\smallskip

If  is formed by interleaved but strictly incrementing () or 
decrementing () runs, then  is formed by contiguous runs,
in the same number and length \cite{BN09}. This gives an immediate 
consequence of Corollary~\ref{cor:contruns-gral}.

\begin{corollary} \label{cor:strict_runs}
  Let  be a permutation on  elements that consists of 
  interleaved strict increasing or decreasing runs, of lengths .
  Suppose we have a data structure that stores a sequence 
  over effective alphabet  within  bits,
  supporting queries \access\ and \rank\ in time , 
  and \select\ in time .
Then, given its run decomposition, we can store  in 
   bits of 
  space, and perform  queries in time  
  and  queries in time .
\end{corollary}

\smallskip

For example we can achieve the same space of Corollary~\ref{cor:contruns}, yet
with the times for  and  reversed.
Finally, if we consider runs for  that are both contiguous and 
incrementing or decrementing, then  so are the runs of .
Corollary~\ref{cor:contruns-gral} can be further simplified as both  and 
can be represented with bitmaps.

\begin{corollary} \label{cor:contstrictruns}
  Let  be a permutation on  elements that consists of 
  contiguous and strict increasing or decreasing runs, of lengths .
Then, given its run decomposition, we can store  in 
   bits, and perform  
  and  in  time.
\end{corollary}

\smallskip

We now show how to achieve exponentiation,  or ,
within compressed space.
Munro et al.~\cite{MRRR03} reduced the problem of supporting
exponentiation on a permutation  to the support of the direct and
inverse application of another permutation, related but with quite
distinct runs than . Combining it with any of our results does yield 
compression, but one where the space depends on the lengths of both the 
runs and cycles of .
The following construction, extending the technique by Munro et 
al.~\cite{MRRR03}, retains the compressibility in terms of the runs 
of , which is more natural. 
It builds an index that uses small additional space to
support the exponentiation, thus allowing the compression of the main
data structure with any of our results.

\begin{theorem} \label{thm:exponent} Suppose we have a representation
  of a permutation  on  elements that supports queries  
  in time  and queries  in time . 
  Then for any , we can build a data structure that takes 
   bits and, used in conjunction with operation 
  or , supports  and  queries in 
   time.
\end{theorem}

\begin{proof}
The key to computing  is to discover that  is in a cycle of length
 and to assign it a position  within its cycle (note
 is arbitrary, yet we must operate consistently once it is assigned).
Then  lies in the same cycle, at position 
, hence  or
. Thus all we need is to 
find out  and , compute , and finally find the position  in
 that corresponds to the th element of the cycle.

We decompose  into its cycles and, for every cycle of length at least ,
store the cycle's length  and an array containing the position  in 
 of every th element in the cycle. Those positions  are called 
`marked'.  We also store a binary sequence , so that  iff  
is marked. For each marked element  we record to which cycle  belongs and 
the position  of  in its cycle.

To compute , we repeatedly apply  at most  times until 
we either loop or find a marked element. In the first case, we have found ,
so we can assume , compute , and apply  at most  
more times to find  in the loop. If we reach a 
marked element, instead, we have stored 
the cycle identifier to which  belongs, as well as  and . Then we 
compute  and know that the previous marked position is 
. The corresponding position  is found
at cell  of the array of positions of marked elements, and we finally
move from  to  by applying  times operation ,
.
A  query is similar (note that it does not need to use  as
we can always move forward). Moreover, we can also proceed using 
instead of , whichever is faster, to compute both  and 
.

The space is  both for the samples and for a compressed
representation of bitmap . Note that we only compute \rank\ at the
positions  such that . Thus we can use the ID structure 
\cite{RRR02}, which uses  bits.
\end{proof}

\subsection {Application to self-indexes}

These results on permutations apply to a second family of self-indexes, which
is based on the representation of the so-called  function 
\cite{GV05,GGV03,Sad03}. Given the suffix array  of sequence 
over alphabet ,  is defined as . Counting, locating, and extracting is carried
out through permutation , which replaces  and . It is known 
\cite{GV05} that  contains  contiguous increasing runs so that 
, which allows for its compression. Grossi et 
al.~\cite{GGV03} represented  within  bits,
while supporting operation  in constant time, or within  while supporting  in time . 
By using Corollary~\ref{cor:contruns}, we can achieve the
unprecedented space  and support  in
constant time. In addition we can support the inverse  in time 
. Having both  and  allows for
bidirectional indexes \cite{RNOM09}, which can for example display a snippet 
around any occurrence found without the need for any extra space for sampling. 
Our construction of 
Theorem~\ref{thm:exponent} can be applied on top of any of those 
representations so as to support operation , which is useful for
example to implement compressed suffix trees, yet the particularities of
 allow for sublogarithmic-time solutions \cite{GGV03}. Note also that
using Huffman-shaped wavelet trees to represent the permutation \cite{BN11} 
yields even less space,  bits, and the 
time complexities are relevant for not so large alphabets.

\section{Compressing functions} \label{sec:functions}

Hreinsson, Kr{\o}yer and Pagh~\cite{HKP09} recently showed how, given a
domain  of numbers that
fit in a machine word, they can represent any  in compressed form and provide constant-time evaluation.
Let us identify function  with the sequence of values
. Then their representation uses
at most  bits, for any constant .
We note that this bound holds even when  is much larger than . 

In the special case where  and , 
we can achieve constant-time evaluation and a better space
bound using our sequence representations. Moreover, we can support extra
functionality such as computing the pre-image of an element.
A first simple result is obtained by representing  as a sequence.

\begin{lemma} \label{lem:string_function}
Let  be a function.
We can represent  using  bits and
compute  for any  in  time, and any element of 
 for any  in time , or vice 
versa.  Using more space,  bits for any constant 
, we support both queries in constant time.
The size  is always computed in  time.
\end{lemma}
\begin{proof}
We represent sequence  using Theorem~\ref{thm:partitioning} or
Corollary~\ref{cor:partitioning-constant},
so  and the th element of  is
. To compute  in constant time we store a
binary sequence
, so
that . The space is the one
needed to represent  plus  bits to 
represent  using an ID 
\cite{RRR02}. This is  if ,
and otherwise it is . This extra space is also necessary
because  may not be the effective alphabet of sequence 
(if  is not surjective).
\end{proof}

Another source of compressibility frequently arising in real-life functions
is nondecreasing or nonincreasing runs. Let us start by allowing interleaved 
runs. Note that in this case , where 
equality is achieved if we form runs of equal values only. 

\begin{theorem} \label{thm:function}
  Let  be a 
  function such that sequence  consists of
   interleaved non-increasing or non-decreasing runs.
  Then, given its run decomposition,
   we can represent  in  bits and compute  for any , and
  any element in  for any ,
  in time .
  The size  is computed in  time.
\end{theorem}

\begin{proof}
We store function  as a combination of the permutation  that
stably sorts the values , plus the binary sequence  of
Lemma~\ref{lem:string_function}.
Therefore, it holds
 
Similarly, the th element of  is 

Since  has the same runs as  (the runs in  can have equal
values but those of  cannot), we can represent  using
Corollary~\ref{cor:runs} to obtain the claimed time and space complexities.
\end{proof}

\begin{example}
Let . The odd positions form an increasing
run  and the even positions form . The permutation
 sorting the values is , and its inverse is
. The bitmap  is .
\end{example}

If we consider only contiguous runs in , we obtain the following result by 
representing  with Corollary~\ref{cor:contruns}.
Note the entropy of contiguous runs is no longer upper bounded by .

\begin{corollary} \label{cor:contiguous_function}
Let  be a function, where 
sequence  consists of  contiguous non-increasing or
non-decreasing runs. Then, given its run decomposition, we can represent  
in  bits, and 
compute any  in  time, as well as 
retrieve any element in  in time . 
The size  can be computed in  time.
\end{corollary}

In all the above results we can use Huffman-shaped wavelet trees 
\cite{BN11} to obtain an alternative space/time tradeoff. We leave the
details to the reader.

\subsection{Application to binary relations, revisited} 

Recall Section~\ref{sec:binrels}, where we represent a binary relation in
terms of a sequence  and a bitmap .
By instead representing  as a function, we can capture another source of
compressibility, and achieve slightly different time complexities.
Note that  corresponds to the distribution of the number 
of objects associated with a label , let us call it
 .
On the other hand, if we regard the contiguous increasing runs of , the 
entropy corresponds to the distribution of the number  of labels 
associated with an object , let us call it
.

While Section~\ref{sec:binrels} compresses  in terms of , we can use Corollary~\ref{cor:contiguous_function} to achieve
 
bits of space. Since  and 
is the th element of , this representation solves 
{\tt label\_nb}, {\tt object\_nb} and {\tt object\_select} in constant time, 
and {\tt label\_select} and {\tt object\_rank} in time .
Operations {\tt label\_rank} and {\tt table\_access} require ,
which is not directly supported. The former can be solved in time
 as a predecessor search in 
(storing absolute samples every  positions), and the 
latter in time  as the difference between two 
{\tt object\_rank} queries.
We can also achieve 
bits using Huffman-shaped wavelet trees; we leave the details to the reader.

\section{Compressing dynamic collections of disjoint sets} \label{sec:unionfind}

Finally, we now give what is, to the best of our knowledge, the first result 
about storing a compressed collection of disjoint sets while supporting 
operations \union\ and \find~\cite{TvL84}.  The key point in the next theorem 
is that, as the sets in the collection  are merged, our space bound shrinks 
with the zero-order entropy of the distribution of the function  that 
assigns elements to sets in . We define ,
where  are the sizes of the sets, which add up to .

\begin{theorem} \label{thm:disjoint}
  Let  be a collection of disjoint sets whose union is .
For any , we can store  in 
  bits and perform any sequence of  \union\ and \find\
  operations in  total time,
  where  is the inverse Ackermann's function.
\end{theorem}

\begin{proof}
We first use
Theorem~\ref{thm:partitioning} to store the sequence  in which
 is the representative of the set containing .  We then store the 
representatives in a standard disjoint-set data structure ~\cite{TvL84}. 
Since , our data
structures take 
bits.  We can perform a query  on  by performing
, and perform a 
operation on  by performing .

As we only need \access\ functionality on , we use a simple variant of
Theorem~\ref{thm:partitioning}. We support only \rank\ and \select\ on 
the multiary wavelet tree that represents sequence , and store the 
subsequences as plain arrays. The mapping  is of length , so it
can easily be represented in plain form to support constant-time operations,
within  bits. This yields constant time , and therefore
the cost of the  \union\ and \find\ operations is 
\cite{TvL84}.

For our data structure to shrink as we merge sets, we keep
track of  and, whenever it shrinks by a factor of , we rebuild our entire data structure on the updated values
. First, note that all those \find\ operations
take  time because of path-compression~\cite{TvL84}: Only the first
time one accesses a node  it may occur that the representative is not
directly 's parent. Thus the overall time can be split into  time
for the  instructions  plus  for the  times a node
 is visited for the first time.

Reconstructing the structure of Theorem~\ref{thm:partitioning} also takes 
 time. The plain structures for  and  are easily
built in linear time, and so is the multiary wavelet tree supporting \rank\ and
\access~\cite{FMMN07}, as it requires just tables of sampled counters.

Since  is always less than , we rebuild only 
 times. Thus the
overall cost of rebuilding is . This completes
our time complexity.

Finally, the space term  is
absorbed by  by slightly adjusting , and
this gives our final space formula.
\end{proof}

\section{Experimental results}
\label{sec:exper}

In this section we explore the performance of our structure in practice. We 
first introduce, in Section~\ref{sec:dense}, a simpler and more practical 
alphabet partitioning scheme we call ``dense'', which experimentally performs 
better than the 
one we describe in Section~\ref{sec:partitioning}, but on which we could not 
prove useful space bounds. Next, in Section~\ref{sec:exp-compr} we study the 
performance of both alphabet partitioning methods, as well as the optimal one 
\cite{Sai05}, in terms of compression ratio and decompression performance. 
Given the results of these experiments, we continue only with our dense
partitioning for the rest of the section.

In Section~\ref{sec:exp-seqs} we compare our new sequence representation
with the state of the art, considering the tradeoff between space and time
of operations , , and . Then,
Sections~\ref{sec:exp-invl}, \ref{sec:exp-ssa}, and \ref{sec:exp-graph} 
compare the same data structures on different real-life applications of 
sequence representations. In the first, the operations are used to emulate an
inverted index on the compressed sequence using (almost) no
extra space. In the second, they are used to emulate self-indexes for text
\cite{NM07}. In the third, they provide access to direct and reverse neighbors 
on graphs represented with adjacency lists.

The machine used for the experiments has an
Intel\textsuperscript{\textregistered}
Xeon\textsuperscript{\textregistered} E5620 at GHz, GB of
RAM. We did not use multithreading in our implementations; times
are measured using only one thread, and in RAM.
The operating system is Ubuntu 10.04, with
kernel 2.6.32-33-server.x86\_64. The code was compiled using GNU/GCC
version 4.4.3 with optimization flags \verb|-O9|.

Our code is available in \libcds\ version , downloadable
from \verb|http://libcds.recoded.cl/|.
 

\subsection{Dense alphabet partitioning}
\label{sec:dense}

Said \cite{Sai05} proved that an optimal assignment to sub-alphabets must
group consecutive symbols once sorted by frequency. A simple alternative
to the partitioning scheme presented in Section~\ref{sec:partitioning}, and
that follows this optimality principle, is to make mapping  
group elements into consecutive chunks of doubling size, that
is, , where  is the
rank of  according to its frequency. 
The rest of the scheme to define  and the sequences
 is as in Section~\ref{sec:partitioning}. The
classes are in the range ,
and each element in  is encoded in  bits. As we use all the
available bits of each symbol in sequences  (except possibly in the
last one), we call this scheme {\em dense}.

We show that this scheme is not much worse than the one proposed in
Section~\ref{sec:partitioning} (which will be called {\em sparse}). 
First consider the total number of bits we use to encode the sequences 
, .
Since  because the symbols are sorted by decreasing
frequency, it holds that  and 
.
Now consider the number of bits we use to encode 
.  
We could store each element  of  in 
 bits using
-codes \cite{WMB99}, and such encoding would be lower bounded by 
. Thus

(recall Section~\ref{sec:repr}). It follows that the total encoding length is
 bits.

Apart from the pretty tight upper bound, it is not evident whether this scheme 
is more or less efficient than the sparse encoding. Certainly the dense scheme 
uses the least possible number of classes (which could allow storing  in 
plain form using  bits per symbol). On the other hand, the sparse
method uses in general more classes, which allows for smaller sub-alphabets 
using fewer bits per symbol in sequences . As we will see in 
Section~\ref{sec:exp-compr}, the dense scheme uses less space than the sparse 
one for , but more for the sequences .

\begin{example} \label{ex:dense}
Consider the same sequence  of 
Ex.~\ref{ex:1}. The dense partitioning will assign ,
,
. 
So the sequence of sub-alphabet identifiers is , and the
subsequences are , 
, and .

This dense scheme uses 16 bits for the sequences , and the
zero-order compressed  requires  bits. The overall
compression is 2.34 bits per symbol. The sparse partitioning of 
Ex.~\ref{ex:1} used 10 bits in the sequences , and the zero-order
compressed  required  bits. The total gives
2.22 bits per symbol. In our real applications, the dense partitioning
performs better.
\end{example}

Note that the question of space optimality is elusive in this scenario.
Since the encoding in  plus that in the corresponding sequence 
forms a unique code per symbol, the optimum is reached when we choose one
sub-alphabet per symbol, so that the sequences  require zero bits
and all the space is in . The alphabet partitioning
always gives away some space, in exchange for faster decompression (or, in 
our case, faster // operations).

Said's optimal partitioning \cite{Sai05} takes care of this problem by using
a parameter  that is the maximum number of sub-alphabets to use. 
We sort the alphabet by decreasing frequency and call  the total number
of bits required to encode the symbols  of the alphabet using 
a partitioning into at most  sub-alphabets. In general, we can make a
sub-alphabet with the symbols  and solve optimally the rest, but if
 we are forced to choose . When we can choose, the optimization
formula is as follows: 

where  is the total frequency of symbols -th to -th in . The first 
term of the sum accounts for the increase in , the second for
the size in bits of the new sequence , and the third for the smaller
subproblem, where it also holds  for any . This dynamic 
programming algorithm requires  space and  time.
We call this partitioning method {\em optimal}.

\begin{example}
The optimal partitioning using 3 classes just like the dense approach in
Ex.~\ref{ex:dense} leaves \texttt{'a'} in its own class, then groups
\texttt{'l'}, \texttt{' '}, \texttt{'b'} and \texttt{'r'} in a second class,
and finally leaves \texttt{'d'} alone in a third class. The overall space
 is 2.23 bits per
symbol, less than the 2.34 reached by the dense partitioning. If, instead,
we let it use four classes, it gives the same solution as the sparse method in 
Ex.~\ref{ex:1}.
\end{example}

Finally, in Section~\ref{sec:partitioning} we represent the sequences 
 with small alphabets  using wavelet trees (just 
like ) instead of using the representation of Golynski et al.~\cite{GMR06}, 
which is used for large . In theory, this is because
Golynski et al.'s representation does not ensure sublinearity on smaller
alphabets when used inside our scheme. While this may appear to be a theoretical
issue, the implementation of such data structure (e.g., in \libcds) is indeed 
unattractive for small alphabets. For this reason, we also avoid using it on 
the chunks where  is small (in our case, the first ones). Note 
that using a wavelet tree for  and then another for the symbols in a 
sequence  is equivalent to replacing the wavelet tree leaf 
corresponding to  in  by the whole wavelet tree of . The
space used by such an arrangement is worse than the one obtained by building,
from scratch, a wavelet tree for  where the symbols  are
actually replaced by the corresponding symbol .

In our {\em dense} representation we use a parameter  that
controls the minimum  value that is represented outside of . All 
the symbols that would belong to , for , are
represented directly in . Note that, by default, since ,
we have .

\subsection{Compression performance}
\label{sec:exp-compr}

For all the experiments in Section~\ref{sec:exper}, except 
Section~\ref{sec:exp-graph}, we used real datasets extracted from Wikipedia. 
We considered two large collections, Simple English and Spanish, dated from
06/06/2011 and 03/02/2010, respectively. Both are regarded as sequences of
words, not characters. These collections contain several versions of each
article. Simple English, in addition, uses a reduced vocabulary. We collected 
a sample of  versions at random from all the documents of Simple
English, which makes a long and repetitive sequence over a small alphabet. 
For the Spanish collection, which features a much richer vocabulary, we took 
the oldest version of each article, which yields a sequence of similar
size, but with a much larger alphabet.

We generated a single sequence containing the word identifiers of all the
articles concatenated, obtained after stemming the collections using
Porter for English and Snowball for Spanish. Table \ref{tab:data}
shows some basic characteristics of the sequences obtained\footnote{The code for generating these sequences is available at
\texttt{https://github.com/fclaude/txtinvlists}.}.

\begin{table}
\begin{center}
\begin{tabular}{l|r|r|r|r}
Collection & Articles & Total words  & Distinct words 
& Entropy () \\ \hline
{\tt Simple English} &  &  &  &  \\
{\tt Spanish} &  &  &  &  
\end{tabular}
\caption{Main characteristics of the datasets used.}
\label{tab:data}
\end{center}
\end{table}

We measured the compression ratio achieved by the three partitioning
schemes, {\tt dense}, {\tt sparse}, and {\tt optimal}. For {\tt dense} we
did not include any individual symbols (other than the most frequent) in
sequence , i.e., we let . For {\tt optimal} we 
allow  sub-alphabets, just like {\tt dense}.

In all cases, the symbols in each  are represented using  bits. The sequence of classes , instead, is represented
in three different forms: {\tt Plain} uses a fixed number of bits per symbol,
 where  is the maximum class; {\tt Huff} uses
Huffman coding of the symbols\footnote{We use G. Navarro's Huffman
implementation; the code is available in {\sc Libcds}.}, and {\tt AC} uses
Arithmetic coding of the symbols\footnote{We use the code by J. Carpinelli,
A. Moffat, R. Neal, W. Salamonsen, L. Stuiver, A. Turpin
and I. Witten, available at 
{\tt http://ww2.cs.mu.oz.au/alistair/arith\_coder/arith\_coder-3.tar.gz}.
We modified the decompressor to read the whole
stream before timing decompression.}. The former encodings are faster,
whereas the latter use less space. In addition we consider compressing the
original sequences using Huffman ({\tt Huffman}) and Arithmetic coding ({\tt
Arith}).

\begin{figure}[tb]
\centerline{\includegraphics[width=0.49\textwidth]{plots/decompressionSWH.pdf}
\includegraphics[width=0.49\textwidth]{plots/decompressionESA.pdf}}
\caption{Space versus decompression time for basic and alphabet-partitioned
schemes. The vertical line marks the zero-order entropy of the sequences. AC
can slightly break the entropy barrier on Simple English because it is 
adaptive.}
\label{fig:spacetime}
\end{figure}

As explained, the main interest in using alphabet partitioning in a 
compressor is to speed up decompression without sacrificing too much space.
Figure~\ref{fig:spacetime} compares all these alternatives in terms of space
usage (percentage of the original sequence) and decompression time per symbol. 
It can be seen that alphabet partitioning combined with {\tt AC} compression
of  wastes almost no space due to the partitioning, and speeds up 
considerably the decompression of the bare {\tt AC} compression.
However, bare {\tt Huffman} also uses the same size and decompresses several
times faster. Therefore, alphabet partitioning combined with {\tt AC}
compression is not really interesting. The other extreme is the combination
with a {\tt Plain} encoding of . In the best combinations, this alphabet
partitioning wastes close to 10\% of space, and in exchange decompresses
around 30\% faster than bare {\tt Huffman}. The intermediate combination,
{\tt Huff}, wastes less than 1\% of space, while improving decompression time
by almost 25\% over bare {\tt Huffman}.

Another interesting comparison is that of partitioning methods. In all cases,
variant {\tt dense} performs better than {\tt sparse}. The difference is larger
when combined with {\tt Plain}, where {\tt sparse} is penalized for the larger
alphabet size of , but still there is a small difference when combined with
{\tt AC}, which shows that  has also (slightly) lower entropy in variant 
{\tt dense}. 
Table~\ref{tab:breakdown} gives a breakdown of the bits per symbol in 
versus the sequences  in all the methods. It can be seen that 
{\tt sparse} leaves much more information on sequence  than the 
alternatives, which makes it less appealing since the operation of  is 
slower than that of the other sequences. However, this can be counterweighted 
by the fact that {\tt sparse} produces many more sequences with alphabet size 
1, which need no time for accessing. It is also confirmed that {\tt dense} 
leaves slightly less information on  than {\tt optimal}, and that the 
difference in space between the three alternatives is almost negligible 
(unless we use {\tt Plain} to encode , which is not interesting).


\begin{table}[tb]
\begin{center}
\begin{tabular}{l|rrr|rrr}
Combination & \multicolumn{3}{c|}{Simple English} & \multicolumn{3}{c}{Spanish}  \\
	    & ~~ & ~~ & ~~ & ~~ & ~~ & ~~ \\
\hline
{\tt Plain-dense}    &  &  &  &  &  &  \\
{\tt Plain-sparse}	&  &  &  &  &  &  \\
{\tt Plain-optimal}	&  &  &  &  &  &  \\
{\tt Huff-dense}	  &  &  &  &  &  &  \\
{\tt Huff-sparse}	  &  &  &  &  &  &  \\
{\tt Huff-optimal}	&  &  &  &  &  &  \\
{\tt AC-dense}		  &  &  &  &  &  &  \\
{\tt AC-sparse}		  &  &  &  &  &  &  \\
{\tt AC-optimal}	  &  &  &  &  &  &  \\
\end{tabular}
\caption{Breakdown, in bits per symbol, of the space used in sequence 
versus the space used in all the sequences , for the different
combinations. The third column in each collection is the percentage of symbols
that lie in sequences  with alphabet sizes .}
\label{tab:breakdown}
\end{center}
\end{table}

Finally, let us consider how the partitioning method affects decompression 
time, given an encoding method for . For method {\tt AC}, {\tt sparse} is
significantly slower. This is explained by the  component having many
more bits, and the decompression time being dominated by the processing of 
by the (very slow) arithmetic decoder. For method {\tt Plain}, instead, {\tt 
sparse} is slightly faster, despite the fact that it uses more space. Since now 
the reads on  and  take about the same time, this difference is 
attributable to the fact that {\tt sparse} leaves more symbols on sequences 
 with alphabets of size 1, where only one read in  is needed to 
decode the symbol (see Table~\ref{tab:breakdown}).
For {\tt Huff} all the times are very similar, and very close to the fastest
one. Therefore, for the rest of the experiments we use the variant {\tt Huff}
with {\tt dense} partitioning, which performs best in space/time.

\subsection{Rank, select and access}
\label{sec:exp-seqs}

We now consider the efficiency in the support for the operations \rank,
\select, and \access. We compare our sequence representation with the
state of the art, as implemented in \libcds\ v1.0.10, a library of
highly optimized implementations of compact data structures. As said,
\libcds\ already includes the implementation of our new structure.

We compare six data structures for representing sequences. Those based on 
wavelet trees are obtained in \libcds\ by combining sequence representations 
(\verb|WaveletTreeNoptrs|, \verb|WaveletTree|) with bitmap representations
(\verb|BitSequenceRG|, \verb|BitSequenceRRR|) for the data on wavelet tree 
nodes.  

\begin{itemize}
\item \verb|WTNPRG|: Wavelet tree without pointers, obtained as
  \verb|WaveletTreeNoptrs|\verb|BitSequenceRG| in \libcds. 
  This corresponds to the 
  basic balanced wavelet tree structure \cite{GGV03}, where all the bitmaps 
  of a level are concatenated \cite{MN07}. The bitmaps are represented in 
  plain form and their operations are implemented using a one-level directory 
  \cite{GGMN05} (where \rank\ is implemented in time proportional to a
  sampling step and \select\ uses a binary search on \rank). The space is
   and the times are . In
  practice the absence of pointers yields a larger number of operations to
  navigate in the wavelet tree, and also \select\ operation on bitmaps is
  much costlier than \rank. A space/time tradeoff is obtained by varying
  the sampling step of the bitmap \rank\ directories.
\item \verb|WTNPRRR|: Wavelet tree without pointers with bitmap compression, 
  obtained in \libcds\ as \verb|WaveletTreeNoptrs|\verb|BitSequenceRRR|. 
  This is similar to \verb|WTNPRG|, but the bitmaps are represented in
  compressed form using the FID technique \cite{RRR02} (\select\ is also
  implemented with binary search on \rank). The space is
   and the times are . In practice
  the FID representation makes it considerably slower than the version with
  plain bitmaps, yet \select\ operation is less affected.
  A space/time tradeoff is obtained by varying the sampling
  step of the bitmap \rank\ directories.
\item \verb|GMR|: The representation proposed by Golysnki et al.~\cite{GMR06}, 
  named \verb|SequenceGMR| in \libcds. The space is ,
  yet the lower-order term is sublinear on , not . The time is
   for \select\ and  for \rank\ and \access, although
  on average \rank\ is constant-time. A space/time tradeoff, which in practice
  affects only the time for \access, is obtained by varying the permutation 
  sampling inside the chunks \cite{GMR06}.
\item \verb|WTRG|: Wavelet tree with pointers and Huffman shape, obtained as
  \verb|WaveletTree|\verb|BitSequenceRG| in \libcds. 
  The space is . The time is 
  , but in our experiments it will be  for \access,
  since the positions are chosen at random from the sequence and then we 
  navigate less frequently to deeper Huffman leaves.
\item \verb|WTRRR|: Wavelet tree with pointers, obtained with
  \verb|WaveletTree|+ \verb|BitSequenceRRR| in \libcds. The space is
  . The time is as in the previous
  structure, except that in practice the FID representation is considerably
  slower.
\item \verb|AP|: Our new alphabet partitioned structure, named 
  \verb|SequenceAlphPart| in \libcds. We use dense partitioning and include
  the  most frequent symbols directly in , . 
  Sequence  is represented with a \verb|WTRG| (since its alphabet is small 
  and the pointers pose no significant overhead), and the sequences 
   are represented with structures \verb|GMR|. The space is 
  , although the lower-order term is actually sublinear on 
   (and only
  very slightly on ). The times are as in \verb|GMR|, although there is a
  small additive overhead due to the wavelet tree on . A space/time tradeoff
  is obtained with the permutations sampling, just as in \verb|GMR|.
\end{itemize}

Figure \ref{fig:opers} shows the results obtained for both text collections,
giving the average over  measures. The \rank\ queries were generated
by choosing a symbol from  and a position from , both
uniformly at random. For \select\ we chose the symbol  in the same way, and
the other argument uniformly at random in . Finally, for \access\ 
we generated the position uniformly at random in . Note that the latter
choice favors Huffman-shaped wavelet trees, on which we descend to leaf  with
probability , whereas for \rank\ and \select\ we descend to any leaf
with the same probability.

\begin{figure}
\centerline{\includegraphics[height=0.3\textheight,width=0.49\textwidth]{plots/rankSWAH.pdf}
\includegraphics[height=0.3\textheight,width=0.49\textwidth]{plots/rankESA.pdf}}

\centerline{\includegraphics[height=0.3\textheight,width=0.49\textwidth]{plots/selectSWAH.pdf}
\includegraphics[height=0.3\textheight,width=0.49\textwidth]{plots/selectESA.pdf}}

\centerline{\includegraphics[height=0.3\textheight,width=0.49\textwidth]{plots/accessSWAH.pdf}
\includegraphics[height=0.3\textheight,width=0.49\textwidth]{plots/accessESA.pdf}}

\caption{Time for the three operations. The  axis starts at the entropy of the sequence.}
\label{fig:opers}
\end{figure}

Let us first analyze the case of Simple English, where the alphabet is
smaller. Since  is 1000 times smaller than , the 
 terms of Huffman-shaped wavelet trees are not significant,
and as a result the variant \texttt{WTRRR} reaches the least space,
essentially . It is followed by three variants that use
similar space: \texttt{WTRG} (which has an additional -bit overhead),
\texttt{AP} (whose  space term is higher than that of wavelet
trees), and \texttt{WTNPRRR} (whose sublinear space term is of the form
, that is, uncompressed). The remaining structures,
\texttt{WTNPRG} and \texttt{GMR}, are not compressed and use much more space.


In terms of time, structure \texttt{AP} is faster than all the others
except \texttt{GMR} (which in exchange uses much more space). The exception
is on \access\ queries, where as explained Huffman-shaped wavelet trees,
\texttt{WTRG} and \texttt{WTRRR}, are favored and reach the same performance
of \texttt{AP}. In general, the rule is that variants using plain bitmaps are
faster than those using FID compression, and that variants using pointers
and Huffman shape are faster than those without pointers (as the latter need
additional operations to navigate the tree). These differences are smaller on
\select\ queries, where the binary searches dominate most of the time spent.

The Spanish collection has a much larger alphabet:  is only 100 times
smaller than . This impacts on the  bits used by the
pointer-based wavelet trees, and as a result the space of \texttt{AP},
, is unparalleled. Variants \texttt{WTRRR} and
\texttt{WTRG} use significantly more space and are followed, far away,
by \texttt{WTNPRRR}, which has uncompressed redundancy. The
uncompressed variants \texttt{WTNPRG} and \texttt{RG} use significantly more
space. The times are basically as on Simple English.

This second collection illustrates more clearly that, for large alphabets,
our structure \texttt{AP} sharply dominates the whole space/time tradeoff.
It is only slightly slower than \texttt{GMR} in some cases, but in exchange
it uses half the space. From the wavelet trees, the most competitive alternative
is \texttt{WTRG}, but it always loses to \texttt{AP}. The situation is not too
different on smaller alphabets (as in Simple English), except that variant
\texttt{WTRRR} uses clearly less space, yet at the expense of doubling
the operation times of \texttt{AP}.

\subsection{Intersecting inverted lists}
\label{sec:exp-invl}

An interesting application of \rank/\select\ operations on large alphabets
was proposed by Clarke et al.~\cite{CCT00}, and recently implemented by
Arroyuelo et al.~\cite{AGO10} using wavelet trees. The idea is to represent
the text collections as a sequence of word tokens (as done for Simple English
and Spanish), use a compressed and \rank/\select/\access-capable sequence 
representation for them, and use those operations to emulate an inverted index 
on the collection, without spending any extra space on storing explicit 
inverted lists. 

More precisely, given a collection of  documents , we
concatenate them in , and build
an auxiliary bitmap  where we mark the
beginning of each document with a 1. We can provide access to the text of any
document in the collection via \access\ operations on sequence
 (and \select\ on ). In order to emulate the inverted list of 
a given term , we just need to list all the distinct documents where  
occurs. This is achieved by iterating on procedure 
{\em nextDoc} of Algorithm~\ref{alg:nextdoc} (called initially with 
 and then using the last  value returned).

\begin{algorithm}[htb]
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
\Input{}
\Output{next document after  that contains }
\SetKwFunction{nextDoc}{nextDoc}

\;
\;
\Return{}
\caption{Function {\em nextDoc}, retrieves the next document 
after  containing . The  axis starts at the entropy of the sequence.}
\label{alg:nextdoc}
\end{algorithm}

Algorithm \ref{alg:nextdoc} also allows one to test whether a given document 
contains a term or not ( contains  iff ). 
Using this primitive we implemented Algorithm~\ref{alg:intersect}, which
intersects several lists (i.e., returns the documents where all the given
terms appear) based on the algorithm by Demaine et al.~\cite{DLM00}. We
tested this algorithm for both Simple English and Spanish collections,
searching for phrases extracted at random from the collection. We considered 
phrases of lengths 2 to 16. We averaged the results over  queries. As all 
the results were quite similar, we only show the cases of 2 and 6 words.
We tested the same structures as in Section~\ref{sec:exp-seqs}.

\begin{algorithm}[htb]
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
\SetKwFunction{nextDoc}{\textit{nextDoc}}
\Input{}
\Output{documents that contain }

sort  by increasing number of occurrences in the collection\;
\;
\;
\While{ is valid}{
  \If{ are contained in  (i.e., 
			for )}{
    Add  to \;
    
    }
  \Else {
    Let  be the first word not contained in \;
    
  }  
}
\Return{}
\caption{Retrieving the documents where all  appear.}
\label{alg:intersect}
\end{algorithm}

Figure \ref{fig:inters} shows the results obtained by the different 
structures. For space, of course, the results are as before: \texttt{AP}
is the best on Spanish and is outperformed by \texttt{WTRRR} on Simple
English. With respect to time, we observe that Huffman-shaped wavelet trees
are favored compared to the random \rank\ and \select\ queries of
Section~\ref{sec:exp-seqs}. The reason is that the queries in this 
application, at least in the way we have generated them, do not distribute
uniformly at random: the symbols for \rank\ and \select\ are chosen
according to their probability in the text, which favors Huffman-shaped
trees. As a result, structures \texttt{WTRG} perform similarly to 
\texttt{AP} in time, whereas \texttt{WTRRR} is less than twice as slow.

\begin{figure}[tb]
\centerline{\includegraphics[width=0.49\textwidth]{plots/intersectionSWH-2.pdf}
\includegraphics[width=0.49\textwidth]{plots/intersectionESA-2.pdf}}

\centerline{\includegraphics[width=0.49\textwidth]{plots/intersectionSWH-6.pdf}
\includegraphics[width=0.49\textwidth]{plots/intersectionESA-6.pdf}}
\caption{Results for intersection queries. The  axis starts at the entropy of the sequence.}
\label{fig:inters}
\end{figure}

\subsection{Self-indexes}
\label{sec:exp-ssa}

A second application of the sequence operations on large alphabets was
explored by Fari\~na et al.~\cite{FBNCPR11}. The idea is to take 
a self-index \cite{NM07} designed for text composed of characters, and apply 
it to a word-tokenized text, in order to carry out word-level searches on
natural language texts. This requires less space and time than the
character-based indexes and competes successfully with word-addressing
inverted indexes. One of the variants they explore is to build an FM-index 
\cite{FM05,FMMN07} on words \cite{CN08}. The FM-index represents the
Burrows-Wheeler transform (BWT) \cite{BW94}  of .
Using \rank\ and \access\ operations on  the FM-index can, 
among other operations, {\em count} the number of occurrences of a pattern 
 (in our case, a phrase of  words) in . This requires
 applications of \rank\ and \access\ on . A self-index 
is also able to retrieve any passage of the original sequence .

We implemented the word-based FM-index with the same structures measured so 
far, plus a new variant called \verb|APRRR|. This is a version of \verb|AP| 
where the bitmaps of the wavelet tree of  are represented using FIDs
\cite{RRR02}. The reason is that it was proved \cite{MN07impl} that the
wavelet tree of , if the bitmaps are represented using
Raman et al.'s FID \cite{RRR02}, achieves space .
Since the wavelet tree  of sub-alphabets of  is a 
coarsened version of that of , we expect it to take advantage
of Raman et al's representation.

We extracted phrases at random text positions, of lengths 2 to 16, and 
counted their number of occurrences using the FM-index. We averaged the
results over  searches. As the results
are similar for all lengths, we show the results for lengths 2 and 8.
Figure \ref{fig:count} shows the time/space tradeoff obtained. 

\begin{figure}[tb]
\centerline{\includegraphics[width=0.45\textwidth]{plots/countSWH-2.pdf}
\includegraphics[width=0.45\textwidth]{plots/countESA-2.pdf}}

\centerline{\includegraphics[width=0.45\textwidth]{plots/countSWH-8.pdf}
\includegraphics[width=0.45\textwidth]{plots/countESA-8.pdf}}
\caption{Time for counting queries on word-based FM-indexes. The vertical line marks the zero-order entropy of the sequences; remember that some schemes achieve high-order entropy spaces.}
\label{fig:count}
\end{figure}

Confirming the theoretical results \cite{MN07impl}, the versions using
compressed bitmaps require much less space than the other alternatives.
In particular, \texttt{APRRR} uses much less space than \texttt{AP}, 
especially on Simple English. In this text the least space is reached by
\texttt{WTRRR}. On Spanish, instead, the  bits of 
Huffman-shaped wavelet trees become
relevant and the least space is achieved by \texttt{APRRR}, closely followed
by \texttt{AP} and \texttt{WTNPRRR}. The space/time tradeoff is dominated
by \texttt{APRRR} and \texttt{AP}, the two variants of our structure.

\subsection{Navigating graphs}
\label{sec:exp-graph}

Finally, our last application scenario is the compact representation of
graphs. Let  be a directed graph. If we concatenate the adjacency
lists of the nodes, the result is a sequence  over an alphabet of
size . If we add a bitmap  that marks with a 1 the beginning
of the lists, it is very easy to retrieve the adjacency list of any node , that is, its {\em neighbors}, with one \select\ operation  on 
followed by one \access\ operation on  per neighbor
retrieved.\footnote{Note that this works well as long as each node points to
at least one node. We solve this problem by keeping an additional bitmap
marking the nodes whose list is not empty.}

It is not hard to reach this space with a classical graph representation.
However, classical representations do not allow one to retrieve efficiently the
{\em reverse neighbors} of , that is, the nodes that point to it. The
classical solution is to double the space to represent the transposed graph.
Our sequence representation, however, allows us to retrieve the reverse
neighbors using \select\ operations on , much as
Algorithm~\ref{alg:nextdoc} retrieves the documents where a term  appears:
our ``documents'' are the adjacency lists of the nodes, and the document
identifier is the node  that points to the desired node. Similarly,
it is possible to determine whether a given node  points to a given node
, which is not an easy operation with classical adjacency lists. This
idea has not only been used in this simple form \cite{CN08}, but also in 
more sophisticated scenarios where it was combined with grammar compression
of the adjacency lists, or with other transformations, to compress Web graphs 
and social networks \cite{CNtweb10,CNlncs10,HN11}.

For this experiment we used two crawls obtained from the well-known 
{\em WebGraph} project\footnote{{\tt http://law.dsi.unimi.it}}.
The main characteristics of these crawls are shown in Table \ref{tab:crawls}. 
Note that the alphabets are comparatively much larger than on documents,
just around 22--26 times smaller than the sequence length.

Figure \ref{fig:graphs} shows the results obtained. The nodes are sorted
alphabetically by URL. A well-known property of Web graphs \cite{BV04} is that 
nodes tend to point to other nodes of the same domain. This property turns into
substrings of nearby symbols in the sequence, and this turns into runs of
0s or 1s in the bitmaps of the wavelet trees. This makes variants like
\texttt{WTNPRRR} very competitive in space, whereas \texttt{APRRR} does not
benefit so much. The reason is that the partitioning into
classes reorders the symbols, and the property is lost. Note that variant
{\tt WTRRR} does not perform well in space, since the number of nodes is too
large for a pointer-based tree to be advantageous. For the same reason, even
{\tt WTRG} uses more space than {\tt GMR}\footnote{Note that \texttt{WTRRR} is
almost 50\% larger than \texttt{WTRG}. This is because the former is a more
complex structure and requires a larger (constant) number of pointers to be
represented. Multiplying by the  nodes of the Huffman-shaped wavelet
tree makes a significant difference when the alphabet is so large.}.
Overall, we note that our variants largely dominate the space/time tradeoff,
except that \texttt{WTNPRRR} uses less space (but much more time).

\begin{table}
\begin{center}
\begin{tabular}{l|c|c|l}
{ Name} & {Nodes } & {Edges } & ~Plain adj. list (bits per edge)\\
\hline
EU (EU-2005) &  &  &  \\
In (Indochina-2002) &  &  &  \\
\end{tabular}
\caption{Description of the Web crawls considered.}
\label{tab:crawls}
\end{center}
\end{table}

\begin{figure}[tb]
\centerline{\includegraphics[width=0.49\textwidth]{plots/graphs-direct-eu.pdf}
\includegraphics[width=0.49\textwidth]{plots/graphs-direct-in.pdf}}

\centerline{\includegraphics[width=0.49\textwidth]{plots/graphs-reverse-eu.pdf}
\includegraphics[width=0.49\textwidth]{plots/graphs-reverse-in.pdf}}
\caption{Performance on Web graphs, to retrieve direct and reverse neighbors. The vertical line marks the bits per edge required by a plain adjacency list representation.}
\label{fig:graphs}
\end{figure}


\section{Conclusions and future work}

We have presented the first zero-order compressed representation of sequences
supporting queries \access, \rank, and \select\ in loglogarithmic time, so that 
the
redundancy of the compressed representation is also compressed. That is, our
space for sequence  over alphabet  is  instead of the usual  bits.
This is very important in many practical applications where the data is so
highly compressible that a redundancy of  bits would dominate 
the overall space. While there exist representations using even  
bits, ours is the first one supporting the operations in time
 while breaking the  redundancy barrier.
Moreover, our time complexities are adaptive to the compressibility of the
sequence, reaching average times  under reasonable
assumptions. We have given various byproducts of the result, where the 
compressed-redundancy property carries over representations of text indexes,
permutations, functions, binary relations, and so on. It is likely that still 
other data structures can benefit from our compressed-redundancy representation.
Finally, we have shown experimentally that our representation is highly 
practical, on large alphabets, both in synthetic and real-life application 
scenarios.

\medskip

On the other hand, various interesting challenges on sequence representations
remain open:
\begin{enumerate}
\item Use  bits of space, rather than
 \cite{BHMR07,GOR10} or our
 bits, while still supporting the queries 
, , and  efficiently. 
\item Remove the  term from the redundancy while retaining
loglogarithmic query times. Golynski et al.~\cite{GRR08} have achieved 
 bits of space, but the time complexities are exponentially
higher on large alphabets, .
\item Lower the  redundancy term, which may be not negligible on 
highly compressible sequences.
Our  redundancy is indeed .
That of Golynski et al.~\cite{GRR08}, ,
is more attractive, at least for small alphabets. Moreover, for the binary 
case, P\u{a}tra\c{s}cu~\cite{Pat08} obtained  for any 
constant , and this is likely to carry over multiary wavelet trees.
\end{enumerate}

After the publication of the conference version of this paper, Belazzougui
and Navarro \cite{BNesa11} achieved a different tradeoff for one of our
byproducts (Theorem~\ref{thm:self}). By spending  further bits, they
completely removed the terms dependent on  in all time complexities,
achieving ,  and  times for counting,
locating and extracting, respectively. Their technique is based in monotone
minimum perfect hash functions (mmphfs), which can also be used to improve
some of our results on permutations, for example obtaining constant time for
query  in Theorem~\ref{thm:runs} and thus improving all the derived
results\footnote{Djamal Belazzougui, personal communication.}.

This is just one example of how lively current research is on this 
fundamental problem. Another
example is the large amount of recent work attempting to close the gap
between lower and upper bounds when taking into account compression, time
and redundancy
\cite{GGGRR07,Gol07,GRR08,Pat08,GORR09,Pat09,Gol09,GOR10}. Very recently,
Belazzougui and Navarro \cite{BNlower11} proved a lower bound of
 for operation  on a RAM machine
of word size , which holds for any space of the form , and
achieved this time within  bits of space. Then, making use
of the results we present in this paper, they reduced the space to
 bits. This is just one example of how our technique
can be easily used to move from linear-space to compressed-redundancy-space
sequence representations.

\paragraph*{Acknowledgments.}

We thank Djamal Belazzougui for helpful comments on a draft of this paper,
and Meg Gagie for righting our grammar.

\bibliographystyle{plain}
\bibliography{paper}

\end{document}
