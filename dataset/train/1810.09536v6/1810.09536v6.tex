\documentclass{article} \usepackage{iclr2019_conference,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{wrapfig}
\usepackage{booktabs}  
\usepackage{subcaption}
\usepackage{lscape}
\usepackage{bm}
\usepackage{todonotes}
\usepackage{multirow}
\usepackage{amsfonts}
\usepackage{forest}
\forestset{
    nice empty nodes/.style={
        for tree={
            s sep=0.1em, 
            l sep=0.33em,
            inner ysep=0.4em, 
            inner xsep=0.05em,
            l=0,
            calign=midpoint,
            fit=tight,
            where n children=0{
               tier=word,
               minimum height=1.25em,
            }{},
            where n children=2{
               l-=1em,
            }{},
            parent anchor=south,
            child anchor=north,
            delay={if content={}{
                    inner sep=0pt,
                    edge path={\noexpand\path [\forestoption{edge}] 
                    			(!u.parent anchor) 
                               -- (.south)\forestoption{edge label};}
                }{}}
        },
    },
}


\newcommand{\cumax}{\mathrm{cumax}}
\newcommand\alex[1]{\textcolor{red}{Alex: #1}}

\title{Ordered Neurons: \\ Integrating Tree Structures \\ into Recurrent Neural Networks}



\author{Yikang Shen\thanks{Equal contribution. \texttt{\{yi-kang.shen,jing.shan.shawn.tan\}@umontreal.ca}.}\\
Mila/Universit\'e de Montr\'eal and Microsoft Research \\
Montr\'eal, Canada \\
\And
Shawn Tan\\
Mila/Universit\'e de Montr\'eal \\
Montr\'eal, Canada \\
\AND
Alessandro Sordoni \\
Microsoft Research \\
Montr\'eal, Canada \\
\And
Aaron Courville \\
Mila/Universit\'e de Montr\'eal \\
Montr\'eal, Canada \\
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy 

\begin{document}


\maketitle

\begin{abstract}
Natural language is hierarchically structured: smaller units (e.g., phrases) are nested within larger units (e.g., clauses). When a larger constituent ends, all of the smaller constituents that are nested within it must also be closed. While the standard LSTM architecture allows different neurons to track information at different time scales, it does not have an explicit bias towards modeling a hierarchy of constituents. This paper proposes to add such an inductive bias by~\emph{ordering} the neurons; a vector of master input and forget gates ensures that when a given neuron is updated, all the neurons that follow it in the ordering are also updated. Our novel recurrent architecture, \emph{ordered neurons} LSTM (ON-LSTM), achieves good performance on four different tasks: language modeling, unsupervised parsing, targeted syntactic evaluation, and logical inference\footnote{The code can be found at \url{https://github.com/yikangshen/Ordered-Neurons}.}.
\end{abstract}

\section{Introduction}


Natural language has a sequential overt form as spoken and written, but the underlying structure of language is not strictly sequential. This structure is usually tree-like. Linguists agree on a set of rules, or~\emph{syntax}, that determine this structure~\citep{chomsky1956three,chomsky1965,sandra2014morphological} and dictate how single words compose to form meaningful larger units, also called ``constituents''~\citep{koopman2013introduction}.
The human brain can also implicitly acquire the latent structure of language~\citep{dehaene2015neural}: during language acquisition, children are not given annotated parse trees.
This observation brings more interest in latent structure induction with artificial neural network approaches, which are inspired by information processing and communication patterns in biological nervous systems. From a practical point of view, integrating a tree structure into a neural network language model may be important for multiple reasons:
\begin{enumerate}[label=(\roman*)]
    \item to obtain a hierarchical representation with increasing levels of abstraction, a key feature of deep neural networks \citep{bengio2009learning, lecun2015deep, schmidhuber2015deep};
    \item to model the compositional effects of language~\citep{koopman2013introduction,socher2013recursive} and help with the long-term dependency problem~\citep{bengio2009learning,tai2015improved} by providing shortcuts for gradient backpropagation~\citep{chung2016hierarchical};
    \item to improve generalization via a better inductive bias and at the same time potentially reducing the need of a large amount of training data.
\end{enumerate}
\begin{figure}[t]
\centering
	\scalebox{0.8}{
	\begin{forest}
		shape=coordinate,
		where n children=0{
			tier=word
		}{},
		nice empty nodes
        [[[[Interest] [expense]] [[in] [[[the] [1988]], [[third] [quarter]]]]] [[was] [[75.3] [million]]]]
	\end{forest}} 
	\scalebox{0.8}{
	\begin{forest}
		shape=coordinate,
		where n children=0{
			tier=word
		}{},
		nice empty nodes
		[[[[Interest] [expense]] [[in] [[the] [1988] [third] [quarter]]]] [[was] [[75.3] [million]]]]
    \end{forest}}
	\vspace{0.3em}
	\caption{Binary parse tree inferred by our model (left) and its corresponding ground-truth (right).}
\end{figure}
The study of deep neural network techniques that can infer and use tree structures
to form better representations of natural language sentences has received a great deal of attention in recent years~\citep{bowman2016fast,yogatama2016learning, shen2017neural,jacob2018learning,choi2018learning,williams2018latent,shi2018tree}.



Given a sentence, one straightforward way of predicting the corresponding latent tree structure is through a supervised syntactic parser. 
Trees produced by these parsers have been used to guide the composition of word semantics into sentence semantics \citep{socher2013recursive,bowman2015tree}, or even to help next word prediction given previous words~\citep{wu2017sequence}.
However, supervised parsers are limiting for several reasons:
i) few languages have comprehensive annotated data for supervised parser training; 
ii) in some domains, syntax rules tend to be broken (e.g. in tweets); and
iii) languages change over time with use, so syntax rules may evolve.


On the other hand, \emph{grammar induction}, defined as the task of learning the syntactic structure from raw corpora without access to expert-labeled data, remains an open problem.
Many such recent attempts suffer from inducing a trivial structure (e.g., a left-branching or right-branching tree~\citep{williams2018latent}),
or encounter difficulties in training caused by learning branching policies with Reinforcement Learning (RL)~\citep{yogatama2016learning}.
Furthermore, some methods are relatively complex to implement and train, like the PRPN model proposed in~\cite{shen2017neural}.


Recurrent neural networks (RNNs) have proven highly effective at the task of language modeling \citep{merityRegOpt, melis2017state}.
RNNs explicitly impose a chain structure on the data.
This assumption may seem at odds with the latent non-sequential structure of language and may pose several difficulties for the processing of natural language data with deep learning methods, giving rise to problems such as capturing long-term dependencies~\citep{bengio2009learning}, achieving good generalization~\citep{bowman2015tree}, handling negation~\citep{socher2013recursive}, etc. Meanwhile, some evidence exists that LSTMs with sufficient capacity potentially implement
syntactic processing mechanisms by encoding the tree structure implicitly, as shown by~\cite{gulordava2018colorless,kuncoro2018lstms} and very recently by~\cite{lakretz2019emergence}.
We believe that the following question remains: Can better models of language be obtained by architectures equipped with an inductive bias towards learning such latent tree structures?






In this work, we introduce~\emph{ordered neurons}, a new inductive bias for recurrent neural networks.
This inductive bias promotes differentiation of the life cycle of information stored inside each neuron: high-ranking neurons will store long-term information which is kept for a large number of steps, while low-ranking neurons will store short-term information that can be rapidly forgotten.
To avoid a strict division between high-ranking and low-ranking neurons, we propose a new activation function, the cumulative softmax, or , to actively allocate neurons to store long/short-term information. 
We use the  function to produce a vector of “master” input and forget gates ensuring  that when a given neuron is updated (erased), all of the neurons that follow it in the ordering are also updated (erased).
Based on the  and the LSTM architecture, we have designed a new model, ON-LSTM, that is biased towards performing tree-like composition operations. Our model achieves good performance on four tasks: language modeling, unsupervised constituency parsing, targeted syntactic evaluation \citep{marvin2018targeted} and logical inference \citep{bowman2015tree}.
The result on unsupervised constituency parsing suggests that the proposed inductive bias aligns with the syntax principles proposed by human experts better than previously proposed models.
The experiments also show that ON-LSTM performs better than standard LSTM models in tasks requiring capturing long-term dependencies and achieves better generalization to longer sequences.





%
 
\section{Related Work}


There has been prior work leveraging tree structures for natural language tasks in the literature.
\cite{socher2010learning, alvarez2016tree, zhou2017generative, zhang2015top} use supervised learning on expert-labeled treebanks for predicting parse trees.
~\cite{socher2013recursive} and~\cite{tai2015improved} explicitly model the tree-structure using parsing information from an external parser.
Later,~\cite{bowman2016fast} exploited guidance from a supervised parser~\citep{klein2003accurate} in order to train a stack-augmented neural network.

Theoretically, RNNs and LSTMs can model data produced by context-free grammars and context-sensitive grammars \citep{gers2001lstm}.
However, recent results suggest that introducing structure information into LSTMs is beneficial. \cite{kuncoro2018lstms} showed that RNNGs~\citep{dyer2016recurrent}, which have an explicit bias to model the syntactic structures, outperform LSTMs on the subject-verb agreement task \citep{linzen2016assessing}.
In our paper, we run a more extensive suite of grammatical tests recently provided by~\cite{marvin2018targeted}.
~\cite{bowman2014recursive,bowman2015tree} also demonstrate that tree-structured models are more effective for downstream tasks whose data was generated by recursive programs.
Interestingly,~\cite{shi2018tree} suggests that while the prescribed grammar tree may not be ideal, some sort of hierarchical structure, perhaps task dependent, might help.
However, the problem of efficiently inferring such structures from observed data remains an open question.





The task of learning the underlying grammar from data is known as~\emph{grammar induction} ~\citep{chen1995bayesian, cohen2011unsupervised}. Early work incorporated syntactic structure in the context of language modeling~\citep{roark2001probabilistic, charniak2001immediate, chelba2000structured}.
More recently, there have been attempts at incorporating some structure for downstream tasks using neural models~\citep{grefenstette2015learning, sun2017neural,joulin2015inferring}.
Generally, these works augment a main recurrent model with a stack and focus on solving algorithmic tasks.
\cite{yogatama2018memory} focus on language modeling and syntactic evaluation tasks \citep{linzen2016assessing} but they do not show the extent to which the structure learnt by the model align with gold-standard parse trees. \cite{shen2017neural} introduced the Parsing-Reading-Predict Networks (PRPN) model, which attempts to perform parsing by solving a language modeling task. The model uses self-attention to compose previous states, where the range of attention is controlled by a learnt ``syntactic distance''. The authors show that this value corresponds to the depth of the parse tree. However, the added complexity in using the PRPN model makes it unwieldy in practice.

Another possible solution is to develop models with varying time-scales of recurrence as a way of capturing this hierarchy. \cite{el1996hierarchical, schmidhuber1991neural, lin1998learning} describe models that capture hierarchies at pre-determined time-scales.
More recently,~\cite{koutnik2014clockwork} proposed Clockwork RNN, which segments the hidden state of a RNN by updating at different time-scales.
These approaches typically make a strong assumption about the regularity of the hierarchy involved in modelling the data.
\cite{chung2016hierarchical} proposed a method that, unlike the Clockwork RNN, would learn a multi-scale hierarchical recurrence.
However, the model still has a pre-determined depth to the hierarchy, depending on the number of layers.
Our work is more closely related to~\cite{rippel2014learning}, which propose to induce a hierarchy in the representation units by applying ``nested'' dropout masks: units are not dropped independently at random but whenever a unit is dropped, all the units that follow in the ordering are also dropped. 
Our work can be seen as a soft relaxation of the dropout by means of the proposed  activation.
Moreover, we propose to condition the update masks on the particular input and apply our overall model to sequential data. Therefore, our model can adapt the structure to the observed data, while both Clockwork RNN and nested dropout impose a predefined hierarchy to hidden representations.


















 
\section{Ordered Neurons}
Given a sequence of tokens  and its corresponding constituency tree~(Figure~\ref{fig:tree}(a)), our goal is to infer the unobserved tree structure while processing the observed sequence,~i.e. while computing the hidden state  for each time step .
At each time step,  would ideally contain a information about all the nodes on the path between the current leaf node  and the root . In Figure~\ref{fig:tree}(c), we illustrate how  would contain information about all the constituents that include the current token  even if those are only partially observed.
This intuition suggests that each node in the tree can be represented by a set of neurons in the hidden states. However, while the dimensionality of the hidden state is fixed in advance, the length of the path connecting the leaf to the root of the tree may be different across different time steps and sentences. Therefore, a desiderata for the model is to dynamically reallocate the dimensions of the hidden state to each node.
\begin{figure}[t]
\centering
\includegraphics[width=1.\linewidth]{figure/hypothesis.png}
\caption{Correspondences between a constituency parse tree and the hidden states of the proposed ON-LSTM.
A sequence of tokens  and its corresponding constituency tree are illustrated in (a).
We provide a block view of the tree structure in (b), where both~ and~ nodes span more than one time step.
The representation for high-ranking nodes should be relatively consistent across multiple time steps.
(c) Visualization of the update frequency of groups of hidden state neurons. At each time step, given the input word, dark grey blocks are completely updated while light grey blocks are partially updated.
The three groups of neurons have different update frequencies. Topmost groups update less frequently while lower groups are more frequently updated.
}
\label{fig:tree}
\end{figure}

Given these requirements, we introduce \emph{ordered neurons}, an inductive bias that forces neurons to represent information at different time-scales. In our model, high-ranking neurons contain long-term or global information that will last anywhere from several time steps to the entire sentence, representing nodes near the root of the tree. Low-ranking neurons encode short-term or local information that only last one or a few time steps, representing smaller constituents, as shown in Figure~\ref{fig:tree}(b). The differentiation between high-ranking and low-ranking neurons is learnt in a completely data-driven fashion by controlling the update frequency of single neurons: to erase (or update) high-ranking neurons, the model should first erase (or update) all lower-ranking neurons. In other words, some neurons always update more (or less) frequently than the others, and that order is pre-determined as part of the model architecture.
























%
 
\section{ON-LSTM}
In this section, we present a new RNN unit, ON-LSTM (``\emph{ordered neurons} LSTM'').
The new model uses an architecture similar to the standard LSTM, reported below:

The difference with the LSTM is that we replace the update function for the cell state  with a new function that will be explained in the following sections.
The forget gates  and input gates  are used to control the erasing and writing operation on cell states , as before.
Since the gates in the LSTM act independently on each neuron, it may be difficult in general to discern a hierarchy of information between the neurons. To this end, we propose to make the gate for each neuron dependent on the others by enforcing the order in which neurons should be updated.


\subsection{Activation Function: }
To enforce an order to the update frequency, we introduce a new activation function:

where  denotes the cumulative sum.
We will show that the vector  can be seen as the expectation of a binary gate . This binary gate splits the cell state into two segments: the 0-segment and the 1-segment. Thus, the model can apply different update rules on the two segments to differentiate long/short-term information. Denote by~ a categorical random variable representing the index for the first  in :

The variable  represents the split point between the two segments. We can compute the probability of the -th value in  being 1 by evaluating the probability of the disjunction of any of the values before the -th being the split point, that is . Since the categories are mutually exclusive, we can do this by computing the cumulative distribution function:

Ideally,  should take the form of a discrete variable.
Unfortunately, computing gradients when a discrete variable is included in the computation graph is not trivial~\citep{schulman2015gradient}, so in practice we use a continuous relaxation by computing the quantity , obtained by taking a cumulative sum of the softmax.
As  is binary, this is equivalent to computing . Hence, . 
\subsection{Structured Gating Mechanism}
Based on the  function, we introduce a master forget gate  and a master input gate :

Following the properties of the  activation, the values in the master forget gate are monotonically increasing from 0 to 1, and those in the master input gate are monotonically decreasing from 1 to 0.
These gates serve as high-level control for the update operations of cell states. Using the master gates, we define a new update rule:





In order to explain the intuition behind the new update rule, we assume that the master gates are binary:
\begin{itemize}
    \item The master forget gate  controls the erasing behavior of the model. Suppose  and the split point is . Given the Eq. (\ref{eq:onforget}) and (\ref{eq:onupdate}), the information stored in the first  neurons of the previous cell state  will be completely erased.
In a parse tree (e.g. Figure~\ref{fig:tree}(a)), this operation is akin to closing previous constituents.
    A large number of zeroed neurons,~i.e. a large , represents the end of a high-level constituent in the parse tree, as most of the information in the state will be discarded. Conversely, a small  represents the end of a low-level constituent as high-level information is kept for further processing.
    
    \item The master input gate  is meant to control the writing mechanism of the model. 
    Assume that  and the split point is . 
    Given Eq. (\ref{eq:oninput}) and (\ref{eq:onupdate}), a large  means that the current input  contains long-term information that needs to be preserved for several time steps.
    Conversely, a small  means that the current input  just provides local information that could be erased by  in the next few time steps.
    
    \item The product of the two master gates  represents the overlap of  and .
    Whenever an overlap exists (), the corresponding segment of neurons encodes the incomplete constituents that contain some previous words and the current input word .
    Since these constituents are incomplete, we want to update the information inside the respective blocks. 
    The segment is further controlled by the  and  in the standard LSTM model to enable more fine-grained operations within blocks.
    For example, in Figure~\ref{fig:tree}, the word  is nested into the constituents  and . At this time step, the overlap gray blocks would represent these constituents, such that  and  can decide whether to reset or update each individual neurons in these blocks.
\end{itemize}







As the master gates only focus on coarse-grained control, modeling them with the same dimensions as the hidden states is computationally expensive and unnecessary.
In practice, we set  and  to be  dimensional vectors, where  is the dimension of hidden state, and  is a chunk size factor.
We repeat each dimension  times, before the element-wise multiplication with  and . The downsizing significantly reduces the number of extra parameters that we need to add to the LSTM. 
Therefore, every neuron within each -sized chunk shares the same master gates.



%
 
\section{Experiments}
We evaluate the proposed model on four tasks: language modeling, unsupervised constituency parsing, targeted syntactic evaluation \citep{marvin2018targeted}, and logical inference \citep{bowman2015tree}.

\subsection{Language Modeling}
Word-level language modeling is a macroscopic evaluation of the model's ability to deal with various linguistic phenomena (e.g. co-occurence, syntactic structure, verb-subject agreement, etc).
We evaluate our model by measuring perplexity on the Penn TreeBank (PTB) \citep{marcus1993building, mikolov2012statistical} task.

For fair comparison, we closely follow the model hyper-parameters, regularization and optimization techniques introduced in AWD-LSTM \citep{merityRegOpt}.
Our model uses a three-layer ON-LSTM model with 1150 units in the hidden layer and an embedding of size 400. 
For master gates, the downsize factor .
The total number of parameters was slightly increased from 24 millions to 25 millions with additional matrices for computing master gates.
We manually searched some of the dropout values for ON-LSTM based on the validation performance.
The values used for dropout on the word vectors, the output between LSTM layers, the output of the final LSTM layer, and embedding dropout where (0.5, 0.3, 0.45, 0.1) respectively. 
A weight-dropout of 0.45 was applied to the recurrent weight matrices.

\begin{table}
\centering
\scalebox{0.85}{
\begin{tabular}{l|ccc}
\toprule
\bf Model & \bf Parameters & \bf Validation &  \bf Test \\
\midrule
\citet{zaremba2014recurrent} - LSTM (large) & 66M &  &  \\
\citet{gal2016theoretically} - Variational LSTM (large, MC) & 66M &  &  \\
\citet{kim2016character} - CharCNN & 19M &  &  \\
\citet{merity2016pointer} - Pointer Sentinel-LSTM & 21M &  &  \\
\citet{grave2016improving} - LSTM &  &  &  \\
\citet{grave2016improving} - LSTM + continuous cache pointer &  &  &  \\
\citet{inan2016tying} - Variational LSTM (tied) + augmented loss & 51M &  &  \\
\citet{zilly2016recurrent} - Variational RHN (tied) & 23M &  &  \\
\citet{zoph2016neural} - NAS Cell (tied) & 54M &  &  \\
\citet{shen2017neural} - PRPN-LM &  &  &  \\
\citet{melis2017state} - 4-layer skip connection LSTM (tied) & 24M &  &  \\
\citet{merityRegOpt} - AWD-LSTM - 3-layer LSTM (tied) & 24M &  &  \\
\midrule
\textbf{ON-LSTM} - 3-layer (tied) & 25M &  &  \\
\midrule
\citet{yang2017breaking} - AWD-LSTM-MoS* & 22M &  &  \\
\bottomrule
\end{tabular}
}
\caption{
Single model perplexity on validation and test sets for the Penn Treebank language modeling task.
Models labelled \textit{tied} use weight tying on the embedding and softmax weights \citep{inan2016tying, press2017using}.
Models labelled * focus on improving the softmax component of RNN language model. Their contribution is orthogonal to ours.
}
\label{table:PTBresults}
\end{table}

As shown in Table~\ref{table:PTBresults}, our model performs better than the standard LSTM while sharing the same number of layers, embedding dimensions, and hidden states units.
Recall that the master gates only control how information is stored in different neurons. It is interesting to note that we can improve the performance of a strong LSTM model without adding skip connections or a significant increase in the number of parameters.

\subsection{Unsupervised Constituency Parsing}
The unsupervised constituency parsing task compares the latent stree structure induced by the model with those annotated by human experts.
Following the experiment settings proposed in \citet{htut2018grammar}, we take our best model for the language modeling task, and test it on WSJ10 dataset and WSJ test set.
WSJ10 has 7422 sentences, filtered from the WSJ dataset with the constraint of 10 words or less, after the removal of punctuation and null elements \citep{klein2002generative}.
The WSJ test set contains 2416 sentences with various lengths.
It is worth noting that the WSJ10 test set contains sentences from the training, validation, and test set of the PTB dataset, while WSJ test uses the same set of sentences as the PTB test set.

To infer the tree structure of a sentence from a pre-trained model, we initialize the hidden states with the zero vector, then feed the sentence into the model as done in the language modeling task. At each time step, we compute an estimate of :

where  is the probability distribution over split points associated to the master forget gate and  is the size of the hidden state. Given , we can use the top-down greedy parsing algorithm proposed in~\citet{shen2017neural} for unsupervised constituency parsing. We first sort the  in decreasing order. For the first  in the sorted sequence, we split the sentence into constituents . Then, we recursively repeat this operation for constituents  and , until each constituent contains only one word.

\begin{table*}[t]
\small
\centering
\scalebox{0.85}{
\setlength{\tabcolsep}{2.8 pt} 
\begin{tabular}{llllccccccccc}
\toprule
& \bf \multirow{3}{*}{\shortstack[l]{Training \\Data}} & \bf \multirow{3}{*}{\shortstack[l]{Training \\Object}} & \bf \multirow{3}{*}{\shortstack[l]{Vocab \\Size}} & \multicolumn{4}{c}{\bf Parsing F1} & \bf \multirow{3}{*}{\shortstack[l]{Depth \\WSJ}} & \multicolumn{4}{c}{\bf \multirow{2}{*}{\shortstack{Accuracy on WSJ by Tag}}} \\ 
\bf Model & & & & \multicolumn{2}{c}{\bf WSJ10} & \multicolumn{2}{c}{\bf WSJ } & & \bf \multirow{2}{*}{\shortstack{ADJP}} & \bf \multirow{2}{*}{\shortstack{NP}} & \bf \multirow{2}{*}{\shortstack{PP}} & \bf \multirow{2}{*}{\shortstack{INTJ}} \\
\bf  &   &  &  & \bf  & \bf max &  & \bf max  &  &   &   &   &   \\
 \midrule
PRPN-UP &  AllNLI Train & LM & 76k & 66.3 (0.8) & 68.5 &  38.3 (0.5) & 39.8 & 5.8 & 28.7 & 65.5 & 32.7 &  \it 0.0 \\
PRPN-LM & AllNLI Train & LM & 76k & 52.4 (4.9) & 58.1 &  35.0 (5.4) &  42.8 & 6.1 & 37.8 & 59.7 & \bf 61.5 & \bf 100.0 \\
\midrule
PRPN-UP  & WSJ Train & LM & 15.8k &  62.2 (3.9) & 70.3 &  26.0 (2.3) & 32.8 & 5.8 & 24.8 & 54.4 & 17.8 &  \it 0.0 \\
PRPN-LM & WSJ Train & LM & 10k & 70.5 (0.4) & 71.3 &  37.4 (0.3) & 38.1 & 5.9 & 26.2 &  \bf 63.9 & 24.4 &   \it 0.0 \\
\midrule
\textbf{ON-LSTM} 1st-layer & WSJ Train & LM & 10k & 35.2 (4.1) & 42.8 & 20.0 (2.8) & 24.0 & 5.6 & 38.1 & 23.8 & 18.3 & \bf 100.0  \\
\textbf{ON-LSTM} 2nd-layer & WSJ Train & LM & 10k & 65.1 (1.7) & 66.8 & 47.7 (1.5) & \bf 49.4 & 5.6 & \bf 46.2 & 61.4 & 55.4 & \it 0.0  \\
\textbf{ON-LSTM} 3rd-layer & WSJ Train & LM & 10k & 54.0 (3.9) & 57.6 & 36.6 (3.3) & 40.4 & 5.3 & 44.8 & 57.5 & 47.2 & \it 0.0  \\
\midrule
300D ST-Gumbel & AllNLI Train  & NLI & -- & -- & -- & 19.0 (1.0) & 20.1 & -- & \it 15.6 & \it 18.8 & \it \it 9.9 & 59.4  \\
\hspace{1em} w/o Leaf GRU & AllNLI Train & NLI & -- & -- &  -- & 22.8 (1.6) & 25.0 & -- & 18.9 & 24.1 & \it 14.2 & 51.8  \\
300D RL-SPINN & AllNLI Train & NLI & -- &  -- & -- & \it 13.2 (0.0) & \it 13.2 & -- & \it 1.7 & \it 10.8 & \it 4.6 & 50.6  \\
\hspace{1em} w/o Leaf GRU & AllNLI Train & NLI & -- & -- & -- & \it 13.1 (0.1) & \it 13.2 & --  & \it 1.6 & \it 10.9 & \it 4.6 & 50.0 \\
\midrule 
CCM   &  WSJ10 Full & -- & -- & -- & 71.9 & -- & -- & -- & -- & -- & -- & -- \\
DMV+CCM  & WSJ10 Full & -- & -- & -- & 77.6 & -- &  -- & -- & -- & -- & -- & -- \\
UML-DOP &   WSJ10 Full & -- & -- & -- & \bf 82.9 & -- & -- & -- & -- & -- & -- & --  \\
\midrule
Random Trees & --  & -- & -- & 31.7 (0.3) & 32.2 & 18.4 (0.1) & 18.6 & 5.3 &17.4 & 22.3 & 16.0 & 40.4 \\
Balanced Trees & -- & -- & -- & 43.4 (0.0) & 43.4 & 24.5 (0.0) & 24.5 & 4.6 & 22.1 & \textit{20.2} & \textit{9.3} & 55.9 \\
Left Branching & --  & -- & -- & \it 19.6 (0.0) & \it 19.6 & \it 9.0 (0.0) & \it 9.0 & 12.4 & -- & -- & -- & --  \\
Right Branching &  -- & -- & -- & 56.6 (0.0) & 56.6 & 39.8 (0.0) & 39.8  &  12.4 & -- & -- & -- & --  \\
\bottomrule 
\end{tabular}
}
\caption{ Unlabeled parsing F1 results evaluated on the full WSJ10 and WSJ test set. 
Our language model has three layers, each of them provides a sequence of . 
We provide the parsing performance for all layers.
Results with RL-SPINN and ST-Gumbel are evaluated on the full WSJ~\citep{williams2017broad}. 
PRPN models are evaluated on the WSJ test set~\citep{htut2018grammar}. We run the model with 5 different random seeds to calculate the average F1. The~\textit{Accuracy} columns represent the fraction of ground truth constituents of a given type that correspond to constituents in the model parses.
We use the model with the best F1 score to report ADJP, NP, PP, and INTJ.
WSJ10 baselines are from~\citet[][CCM]{klein2002generative}, \citet[][DMV+CCM]{klein2005natural}, and \citet[][UML-DOP]{bod2006all}.
As the WSJ10 baselines are trained using POS tags, they are not strictly comparable with the latent tree learning results.
Italics mark results that are worse than the random baseline.
}
\label{tab:wsj-table}
\end{table*} 

The performance is shown in Table \ref{tab:wsj-table}.
The second layer of ON-LSTM achieves state-of-the-art unsupervised constituency parsing results on the WSJ test set, while the first and third layers do not perform as well.
One possible interpretation is that the first and last layers may be too focused on capturing local information useful for the language modeling task as they are directly exposed to input tokens and output predictions respectively, thus may not be encouraged to learn the more abstract tree structure.
Since the WSJ test set contains sentences of various lengths which are unobserved during training, we find that ON-LSTM provides better generalization and robustness toward longer sentences than previous models.
We also see that ON-LSTM model can provide strong results for phrase detection, including ADJP (adjective phrases), PP (prepositional phrases), and NP (noun phrases).
This feature could benefit many downstream tasks, like question answering, named entity recognition, co-reference resolution, etc.

\subsection{Targeted Syntactic Evaluation}
Targeted syntactic evaluation tasks have been proposed in~\citet{marvin2018targeted}. It is a collection of tasks that evaluate language models along three different structure-sensitive linguistic phenomena: subject-verb agreement, reflexive anaphora and negative polarity items.
Given a large number of minimally different pairs of English sentences, each consisting of a grammatical and an ungrammatical sentence, a language model should assign a higher probability to a grammatical sentence than an ungrammatical one.

Using the released codebase\footnote{\url{https://github.com/BeckyMarvin/LM_syneval}. We notice that the test set generated from the code is different from the one used in the original paper~\citet{marvin2018targeted}. Therefore, our results are not strictly comparable with the results in~\citet{marvin2018targeted}.} and the same settings proposed in~\citet{marvin2018targeted}, we train both our ON-LSTM model and a baseline LSTM language model on a 90 million word subset of Wikipedia.
Both language models have two layers of 650 units, a batch size of 128, a dropout rate of 0.2, a learning rate of 20.0, and were trained for 40 epochs.
The input embeddings have 200 dimensions and the output embeddings have 650 dimesions.

\begin{table*}[t]
\centering
\scalebox{0.80}{
\begin{tabular}{l c c c c}
\toprule
& ON-LSTM & LSTM\\
\midrule
\large Short-Term Dependency \\
\midrule
\textsc{Subject-verb agreement:}\vspace{0.2em}\\
Simple                                          & 0.99 & \bf 1.00  \\
In a sentential complement                      & 0.95 & \bf 0.98  \\
Short VP coordination                           & 0.89 & \bf 0.92 \\
In an object relative clause                    & 0.84 & \bf 0.88  \\
In an object relative (no \textit{that})        & 0.78 & \bf 0.81  \\
\midrule
\textsc{Reflexive anaphora:}\vspace{0.2em}\\
Simple                                          & \bf 0.89 & 0.82  \\
In a sentential complement                      & \bf 0.86 & 0.80  \\
\midrule
\textsc{Negative polarity items:}\vspace{0.2em}\\
Simple (grammatical vs. intrusive)              & 0.18 & \bf 1.00  \\
Simple (intrusive vs. ungrammatical)            & \bf 0.50 & 0.01 \\
Simple (grammatical vs. ungrammatical)          & 0.07 & \bf 0.63\\
\midrule
\large Long-Term Dependency \\
\midrule
\textsc{Subject-verb agreement:}\vspace{0.2em}\\
Long VP coordination                            & \bf 0.74 & \bf 0.74  \\
Across a prepositional phrase                   & 0.67 & \bf 0.68  \\
Across a subject relative clause                & \bf 0.66 & 0.60  \\
Across an object relative clause                & \bf 0.57 & 0.52  \\
Across an object relative (no \textit{that})    & \bf 0.54 & 0.51  \\
\midrule
\textsc{Reflexive anaphora:}\vspace{0.2em}\\
Across a relative clause                        & 0.57 & \bf 0.58  \\
\midrule
\textsc{Negative polarity items:}\vspace{0.2em}\\
Across a relative clause (grammatical vs. intrusive)    & 0.59 & \bf 0.95  \\
Across a relative clause (intrusive vs. ungrammatical)    & \bf 0.20 & 0.00  \\
Across a relative clause (grammatical vs. ungrammatical)    & \bf 0.11 & 0.04  \\
\bottomrule
\end{tabular}
}
\caption{Overall accuracy for the ON-LSTM and LSTM on each test case. ``Long-term dependency'' means that an unrelated phrase (or a clause) exist between the targeted pair of words, while ``short-term dependency'' means there is no such distraction.}
\label{table:syneval}
\end{table*}

Table \ref{table:syneval} shows that the ON-LSTM performs better on the long-term dependency cases, while the baseline LSTM fares better on the short-term ones.
This is possibly due to the relatively small number of units in the hidden states, which is insufficient to take into account both long and short-term information.
We also notice that the results for NPI test cases have unusually high variance across different hyper-parameters.
This result maybe due to the non-syntactic cues discussed in \citet{marvin2018targeted}.
Despite this, ON-LSTM actually achieves better perplexity on the validation set.

\subsection{Logical Inference}
We also analyze the model's performance on the logical inference task described in~\citet{bowman2015tree}. 
This task is based on a language that has a vocabulary of six words and three logical operations, . 
There are seven mutually exclusive logical relations that describe the relationship between two sentences: two types of entailment, equivalence, exhaustive and non-exhaustive contradiction, and two types of semantic independence. 
Similar to the natural language inference task, this logical inference task requires the model to predict the correct label given a pair of sentences.
The train/test split is as described in the original codebase\footnote{\url{https://github.com/sleepinyourhat/vector-entailment}}, and 10\% of training set is set aside as the validation set. 

We evaluate the ON-LSTM and the standard LSTM on this dataset.
Given a pair of sentences , we feed both sentences into an RNN encoder, taking the last hidden state  as the sentence embedding.
The concatenation of  is used as input to a multi-layer classifier, which gives a probability distribution over seven labels.
In our experiment, the RNN models were parameterised with 400 units in one hidden layer, and the input embedding size was 128.
A dropout of 0.2 was applied between different layers.
Both models are trained on sequences with 6 or less logical operations and tested on sequences with at most 12 operations.

\begin{figure}
    \centering
    \includegraphics[width=0.48\textwidth]{figure/logic.PNG}
    \caption{Test accuracy of the models, trained on short sequences () in logic data. The horizontal
axis indicates the length of the sequence, and the vertical axis indicates the accuracy of model’s performance on the corresponding test set.}
\label{fig:logic}
\end{figure}


Figure \ref{fig:logic} shows the performance of ON-LSTM and standard LSTM on the logical inference task. While both models achieve nearly 100\% accuracy on short sequences (), ON-LSTM attains better performance on sequences longer then 3.
The performance gap continues to increase on longer sequences () that were not present during training. 
Hence, the ON-LSTM model shows better generalization while facing structured data with various lengths and comparing to the standard LSTM. A tree-structured model can achieve strong performance on this dataset~\citep{bowman2015tree}, since it is provided with the ground truth structure as input.
The recursive application of the same composition function is well suited for this task.
We also include the result of RRNet~\citep{jacob2018learning}, which can induce the latent tree structure from downstream tasks. Note that the results may not be comparable, because the hyper-parameters for training were not provided. 


\section{Conclusion}
In this paper, we propose \emph{ordered neurons}, a novel inductive bias for recurrent neural networks. 
Based on this idea, we propose a novel recurrent unit, the ON-LSTM, which includes a new gating mechanism and a new activation function .
This brings recurrent neural networks closer to performing tree-like composition operations, by separately allocating hidden state neurons with long and short-term information.
The model performance on unsupervised constituency parsing shows that the ON-LSTM induces the latent structure of natural language in a way that is coherent with human expert annotation.
The inductive bias also enables ON-LSTM to achieve good performance on language modeling, long-term dependency, and logical inference tasks.



\bibliography{ref}
\bibliographystyle{iclr2019_conference}

\newpage
\appendix
\setcounter{figure}{0} \renewcommand{\thefigure}{A.\arabic{figure}}

\newpage


\section{Sample Parses from the model with the best perplexity}

\begin{figure*}[ht]
	\centering
\begin{tabular}{c c}
	\scalebox{0.75}{
	\begin{forest}
		shape=coordinate,
		where n children=0{
			tier=word
		}{},
		nice empty nodes
        [[[The], [RTC]], [[needs], [[[the], [[[most], [able]], [competent]]], [[management], [available]]]]]
	\end{forest}} &
	\scalebox{0.75}{
	\begin{forest}
		shape=coordinate,
		where n children=0{
			tier=word
		}{},
		nice empty nodes
		[[[The], [RTC]], [[needs], [[[the], [[most], [able], [competent]], [management]], [available]]]]
    \end{forest}}
    \vspace{0.3em} \\
    
    \vspace{1.0em}
	
	\scalebox{0.75}{
	\begin{forest}
		shape=coordinate,
		where n children=0{
			tier=word
		}{},
		nice empty nodes
        [[[Resolution], [[Funding], [Corp.]]], [[to], [[[sell], [4.5]], [[billion], [[30-year], [bonds]]]]]]
	\end{forest}} &
	\scalebox{0.75}{
	\begin{forest}
		shape=coordinate,
		where n children=0{
			tier=word
		}{},
		nice empty nodes
		[[[Resolution], [Funding], [Corp.]], [[to], [[sell], [[[4.5], [billion]], [30-year], [bonds]]]]]
    \end{forest}}
    \vspace{0.3em}\\
	
    \vspace{1.0em}
	
	\scalebox{0.75}{
	\begin{forest}
		shape=coordinate,
		where n children=0{
			tier=word
		}{},
		nice empty nodes
        [[[[Interest] [expense]] [[in] [[[the] [1988]], [[third] [quarter]]]]] [[was] [[75.3] [million]]]]
	\end{forest}} &
	\scalebox{0.75}{
	\begin{forest}
		shape=coordinate,
		where n children=0{
			tier=word
		}{},
		nice empty nodes
		[[[[Interest] [expense]] [[in] [[the] [1988] [third] [quarter]]]] [[was] [[75.3] [million]]]]
    \end{forest}}
	\vspace{0.3em}\\

	\vspace{1.0em}
	
	\scalebox{0.75}{
	\begin{forest}
		shape=coordinate,
		where n children=0{
			tier=word
		}{},
		nice empty nodes
        [[[All] [prices]] [[are] [[as] [[of] [[monday] [['s] [close]]]]]]]
	\end{forest}}\hspace{1.0em} &
	\scalebox{0.75}{
	\begin{forest}
		shape=coordinate,
		where n children=0{
			tier=word
		}{},
		nice empty nodes
		[[[All], [prices]], [[are], [[as], [[of], [[[Monday], ['s]], [close]]]]]]
    \end{forest}}
	\vspace{0.3em}\\
    
    \vspace{1.0em}
	
	\scalebox{0.75}{
	\begin{forest}
		shape=coordinate,
		where n children=0{
			tier=word
		}{},
		nice empty nodes
        [[That], [['ll], [[[save], [[[us], [time]], [and]]], [[get], [[people], [involved]]]]]]
	\end{forest}} &
	
	\scalebox{0.75}{
	\begin{forest}
		shape=coordinate,
		where n children=0{
			tier=word
		}{},
		nice empty nodes
		[[That], [['ll], [[[save], [us], [time]], [and], [[get], [[people], [involved]]]]]]
    \end{forest}}
	\vspace{0.3em}\\
	
    \vspace{1.0em}
	
	\scalebox{0.75}{
	\begin{forest}
		shape=coordinate,
		where n children=0{
			tier=word
		}{},
		nice empty nodes
        [[[A], [decision]], [[is], [[[n't], [expected]], [[until], [[some], [[time], [[next], [year]]]]]]]]
	\end{forest}} &
	\scalebox{0.75}{
	\begin{forest}
		shape=coordinate,
		where n children=0{
			tier=word
		}{},
		nice empty nodes
		[[[A], [decision]], [[is], [n't], [[expected], [[until], [[[some], [time]], [[next], [year]]]]]]]
    \end{forest}}
\end{tabular}


\caption{\emph{Left} parses are from the 2nd layer of the ON-LSTM model, \emph{Right} parses are converted from human expert annotations (removing all punctuations).}
\label{fig:sample-trees} 
\end{figure*}



    




~ 

~ \\
    
~ \\
    
%
 
\end{document}
