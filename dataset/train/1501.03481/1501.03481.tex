\section{Experimental Setup and Measurement Methodology}   
\label{sec:Experimental Setup and Measurement Methodology}
Our experimental setup includes three platforms on which we execute our
OptionPricer program and collect workload-specific performance and 
energy metrics. 
This Section defines our metrics, 
describes the platforms used and presents salient details of our methodology
used to obtain the power readings and calculate the energy consumption.
A complete description of our methodology is available  
in \cite{Georgakoudis2014}.

\subsection{Definition of Metrics} \label{sec:platforms:metrics}
Option pricing in finance takes place by consuming 
a live streaming data feed of stock market prices, often within the 
context of high frequency trading (HFT), and for pre-trade risk analytics.
The execution time characteristics of option pricing are different 
from those of numerical simulation in computational science using HPC.
By contrast to scientific codes which have measurable setup and 
post-processing phases, financial option pricing runs relatively small 
standalone kernels, such as MC and BT, at very high frequency with 
little set up and post processing work.
Option pricing on live market data feeds is actually a form of event 
processing, where the event is the arrival of a price update on the 
underlying stock. Based on these distinctions we present and use three 
workload-specific metrics to compare servers under financial analytics 
workloads:
\begin{description}[leftmargin=0cm]
  \item[QoS] New prices may arrive at any time in a trading 
             session. This means that any contracts not yet 
             priced using the previous price update are 
             abandoned and deemed unusable. Related to the 
             Time/option metric below, 
             but also dependent on market activity, we define 
             the Quality of Service metric (QoS) as the ratio 
             of successful to the total requested option 
             price evaluations. The QoS metric is an 
             application-specific measure on meeting 
             option pricing performance requirements. It is 
             useful for characterizing application-related 
             performance and scalability offered by deploying 
             multiple nodes. It is worth noting that QoS 
             depends on the rate of stock price changes and other 
             market activities at the time of its calculation,
             so it will be different each time it is calculated 
             in a live market scenario.
  \item[Joules/option] (J/Opt or J) The energy consumed per execution of a pricing kernel is
         a fundamental metric.
In the case of 
         an actively traded stock, with a high 
         number of defined option contracts, this building
         block is executed repeatedly throughout the trading day.
         Correspondingly, a reduction in this value can result in significant
         energy savings for providers offering option pricing services.
 \item[Time/option] (S/Opt or S)
         In contrast to providers, end users, particularly those engaged in HFT, 
         are sensitive to end-to-end latency, thereby constraining the elapsed 
         time per option metric. This metric in turn can be used to evaluate the 
         total time to price all contracts for a given stock. Option pricing 
         shares this time-to-solution performance metric in common with HPC applications. 
\end{description}
\subsection{Hardware Platforms}  \label{sec:platforms:platforms}
We used three platforms, one state-of-the-art server architecture with Intel Sandy Bridge 
processors (briefly referred to as ``Intel'' in the rest of this paper),
one state-of-the-art 
HPC architecture with Intel Xeon Phi Knights Corner coprocessor 
(referred to as ``Xeon Phi'') and a Calxeda ECX-1000 microserver with 
ARM Cortex A9 processors, packaged in a Boston Viridis
rack-mounted unit (referred to as ``Viridis''). 
We used the  version of the GCC compiler and the Intel Compiler ICC 
version  
for code generation, the latter only on Intel platforms.  
The three platforms offer the possibility of 
scaling their frequency and voltage through a DVFS interface.  
We conducted experiments only with the highest voltage-frequency 
settings on each 
platform, to which we refer as performance mode. Previous work shows 
that performance mode is the most energy efficient too~\cite{Georgakoudis2014}.
The details of the platforms are as follows:
\begin{description}[leftmargin=0cm]
\item[Intel] is an x86-64 server with Sandy Bridge architecture, 
             with 2 Intel Xeon CPU E5-2650 processors 
             operating at a frequency of 2.00GHz and equipped 
             with 8 cores each. The machine has 32GB of DRAM (4  8GB DDR3 @ 1600Mhz). 
             The server runs on Linux CentOS 6.5 with kernel 
             version  ().
\item[Xeon Phi] (Knights Corner) is a many core, 
                x86-64 co-processor board (5110P model) over PCIe. 
                It features the many integrated cores (MIC) 
                architecture which offers sixty, 4-way hyperthreaded 
                cores, each equipped with a very wide (512-bit) vector 
                unit. The board has more than ~GB of GDDR5 DRAM. 
                and the clock frequency is  GHz. 
                High performance and high energy efficiency are 
                the result of featuring a highly parallel many core 
                design while running in low clock speeds.
                The system runs on Linux kernel 2.6.38.8+mpss3.2.1.
\item[Viridis]   is a 2U rack mounted server containing sixteen  
                 microserver nodes connected internally by a high-speed 10 Gb Ethernet network. 
                 The platform appears logically as sixteen servers within one box. 
                 Each node is a Calxeda EnergyCore ECX-1000 comprising 4 ARM Cortex A9 cores and 4~GB of DRAM
                 running Ubuntu 12.04 LTS. 
                 Viridis has a frequency of GHz.
\end{description}
Note, when referring to the different platform settings later we will use the 
following notation to represent the platform configuration 
[Nodes used  Cores Used  Threads per Core].
\subsection{Software}
Starting from a common C code base, we created versions which use the 
vector units on each platform. We achieved this in three different 
ways
\begin{itemize} 
   \item creating assembler code implementations of hotspot loops
   \item using compile intrinsic C functions which map to assembler
         instructions
   \item using the auto vectorization functionality of the kernel.  
\end{itemize}   
\begin{table}[tbhp]   
  \footnotesize
  \centering
  \caption{List of labels, VEC TYPE, defining the preparation of the executable binary}
  \begin{tabular}{ll} 
     \toprule
       \multicolumn{1}{c}{\bf VEC TYPE} & \multicolumn{1}{c}{\bf Description}        \\
     \midrule 
       \textbf{AVX256}     & Assembler code using AVX 256-bit instructions on the Intel Sandyridge. \\       
      \midrule       
       \textbf{INTRINSICS} & Compiler supplied C functions on any platform (ARM 128-bit, Intel 256-bit, Xeon Phi 512-bit) \\    
      \midrule       
       \textbf{KNC512}     & Assembler code for 512-bit vector instruction set on the Xeon Phi (Knights Corner). \\ 
      \midrule       
       \textbf{NEON128}    & Assembler code for the ARM NEON 128-bit unit. \\ 
      \midrule
       \textbf{AUTOVECT}   & Compiler auto-vectorization on all platforms \\
     \bottomrule
  \end{tabular}
   \label{tab:binary_labels_table} 
\end{table}  
Table \ref{tab:binary_labels_table} defines the labels corresponding to the 
type of binary. Each experiment, reported later in this paper, is conducted
by executing one type of binary on one platform and is labeled accordingly.
\subsection{Summary of Methodology}  \label{sec:platforms:methodology}
For our experiments, we collected Facebook stock price ticks during a 
full New York Stock Exchange session and replayed them using UDP 
multicast to all nodes in each of our platforms, as shown in 
Figure~\ref{fig:fig1}. This is as close as an experiment 
needs to be to reality without any external glitches or factors 
affecting the setup or measurements. 
Detection of a change in the Facebook stock price triggers computation 
of new prices for 617 Facebook European options at the maximum speed 
feasible.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.4\textwidth]{fig/tcp-setup}
  \caption{Financial trace data measurement setup}
 \label{fig:fig1}
\end{figure}
\par
Next we discuss on the power measurement methodology.
The exact form of the current supply path to the CPU differs from one 
platform to the next but to provide a fair basis for comparison 
we identified two distinct points on the path,
shown in Figure~\ref{fig:currentsupplypath}, which are measurable 
on all platforms. We continuously monitored power on each platform
at these points during our experiments. 
To isolate the energy consumption of processor packages, we capture 
power consumption at the point before the VRM, which we label PRE-VRM. 
For the Intel server, PRE-VRM measurement
is facilitated by reading the Running Average Power Limit (RAPL) 
counters while the same
functionality on Viridis is available through the Intelligent 
Platform Management Interface (IPMI)
counters, which is also available on the Xeon Phi platform
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.45\textwidth]{fig/power-measurement-points}
  \caption{The path of the current supply to the CPU showing points at which
           we measured power. PSU is the power supply unit and VRM the 
           voltage regulator module.}
  \label{fig:currentsupplypath}
\end{figure}
\par
Figure~\ref{fig:powervstime} shows the power versus time plot for a
standalone execution of the MC kernel. The BT execution plot is similar. 
\begin{figure}[htbp]
   \centering
   \includegraphics[width=0.48\textwidth]{fig/power-prof}
   \caption{CPU power vs. time for the MC kernel}
   \label{fig:powervstime}
\end{figure}
The profile of instantaneous power versus time follows 
a very sharp trapezoidal shape: the CPU is fully utilized during execution and 
there are no periods of inactivity. This is a common feature with other 
numerically intensive HPC applications. It means that the measured 
average power is a representative measure of energy consumption 
throughout kernel execution. 
