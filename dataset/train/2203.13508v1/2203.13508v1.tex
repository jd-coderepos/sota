\newpage
\appendix
\section{Theoretical derivations for BDDMs}
\label{appendix:A}

In this section, we provide the theoretical supports for the following:
\begin{itemize}
    \item The derivation for upper bounding  (see Appendix \ref{proof:upper_bound}).
    \item The score network  trained with  for the reverse process  can be re-used for the reverse process  (see Appendix \ref{proof:score_network_loss}).
    \item The schedule network  can be trained with  after the score network  is optimized. (see Appendix \ref{proof:scheduling_network_loss}).
\end{itemize}

\subsection{Deriving an upper bound for noise scale}
\label{proof:upper_bound}
Since monotonic noise schedules have been successfully applied to in many prior arts including DPMs \citep{ho2020denoising,kingma2021variational} and score-based methods \citep{yang2020, yang2020improved}, we also follow the monotonic assumption and derive an upper bound for  as below: 
\begin{customRemark}{1}
Suppose the noise schedule for sampling is monotonic, i.e., , then, for ,  satisfies the following inequality:

\end{customRemark}

\begin{proof}
By the general definition of noise schedule, we know that  (Note: no inequality sign in between).
Given that , we also have . First, we show that :

Next, we show that :

Now, we have .
When , we can show that :

By the assumption of monotonic sequence, we also have . Knowing that  is always true, we obtain a tighter bound for : .
\end{proof}

\subsection{Deriving the training objective for score network}
\label{proof:score_network_loss}

First, followed from the data distribution modeling of BDDMs as proposed in Eq. (\ref{eq:theta_bddm}):

we can derive a new lower bound to the log marginal likelihood as follows:
\begin{proposition}
\label{prop:1}
Given , the following lower bound holds for :

where 

\end{proposition}
\begin{proof}

\end{proof}
Next, we show that the score network  trained with  can be re-used in BDDMs. We first provide the derivation for Eq. (\ref{eq:q_hat}- \ref{eq:q_hat_def}). We have

\begin{proposition}
\label{prop:theta}
Suppose , then any solution satisfying  also satisfies .
\end{proposition}
\begin{proof}
By the definition in Eq. (\ref{eq:reverse}), we have

Here, from the training objective in Eq. (\ref{eq:lddpm}), since , the noise scale argument for the score network is known to be . Therefore, we can use  instead of  for expanding . Since  and  are two isotropic Gaussians with the same variance, the KL divergence is a scaled -norm of their means' difference:

which is proportional to  as defined in Eq. (\ref{eq:lddpm}). Thus,


\end{proof}

Next, we can simplify  to a reconstruction loss for :

where  can be efficiently sampled using the reverse process in \citep{jiaming2021}. Yet, in practice, similar to the training in \citep{jiaming2021, nanxin2020, zhifeng2021}, we dropped  when  training . In theory, we know that  achieves its optimal value at , which shares a similar objective as . By minimizing , we train a score network  that best minimizes  for all . Since the first diffusion step has the smallest effect on corrupting  (i.e., ), it suffices to consider a , in which case we can jointly minimize  by minimizing .



In this sense, during training, given , we can train the score network with the same training objective as in DDPMs and DDIMs. Practically, it is beneficial for BDDMs as we can re-use the score network  of any well-trained DDPM or DDIM.


\subsection{Deriving the training objective for schedule network}
\label{proof:scheduling_network_loss}
Given that  can be trained to maximize the log evidence with the pre-specified noise schedule  for training, the consequent question of interest in BDDMs is how to find a fast and good enough noise schedule  for sampling given an optimized . In BDDMs, this problem is reduced to how to effectively learn the network parameters  .
\begin{proposition}
\label{prop:phi}
Suppose  has been optimized and hypothetically converged to the optimal , where by optimal it means that with  we have  given . When  is unknown but we have  and , we can minimize the gap between the optimal lower bound  and , i.e, , by minimizing the following objective with respect to :

where



\end{proposition}
\begin{proof}
Note that , ,  and . When  is given to , we can express the probability as follows:

where, different from , from Eq. (\ref{eq:def_given_x0}) to Eq. (\ref{eq:def_E_z}), instead of conditioning on a specific , when  is given  can be generated using any .

From this, we can express the gap between  and  in the following form:

Next, we evaluate the above KL divergence term. By definition, we have

Together with Eq. (\ref{eq:forward_rep}), we have

where

\end{proof}
As we use a schedule network  to estimate  from  as defined in Eq. (\ref{eq:q_phi}), we obtain the final step loss for learning :


This proposed objective for training the schedule network can be interpreted as to better model the data distribution (i.e., maximizing ) by correcting the gradient scale  for the next reverse step (from  to ) given the gradient vector  estimated by the score network .

























\section{Experimental details}
\label{appendix:B}
\subsection{Conventional grid search algorithm for DDPMs}
\label{sec:gridsearch}
We reproduced the grid search algorithm in \citep{nanxin2020}, in which a 6-step noise schedule was searched. In our paper, we generalized the grid search algorithm by similarly sweeping the -step noise schedule over the following possibilities with a bin width :

where  denotes the cartesian product applied on two sets. LS-MSE was used as a metric to select the solution during the search. When , we resemble the GS algorithm in \citep{nanxin2020}. Note that above searching method normally does not scale up to  steps for its exponential computational cost .

\subsection{Hyperparameter setting in BDDMs}
Algorithm \ref{alg:training_phi} took a skip factor  to control the stride for training the schedule network. The value of  would affect the coverage of step sizes when training the schedule network, hence affecting the predicted number of steps  for inference -- the higher  is, the shorter the predicted inference schedule tends to be. We set  for training the BDDM vocoders in this paper.

For initializing Algorithm \ref{alg:nspred} for noise scheduling, we could take as few as  training sample for validation, perform a grid search on the hyperparameters  for , i.e.,  possibilities in total, and use the PESQ measure as the selection metric. Then, the predicted noise schedule corresponding to the maximum PESQ was stored and applied to the online inference afterward, as shown in Algorithm \ref{alg:sampling}. Note that this searching has a complexity of only  (e.g.,  in this case), which is much more efficient than  in the conventional grid search algorithm in \citep{nanxin2020}, as discussed in Section \ref{sec:gridsearch}.




\subsection{Implementation details}
Our proposed BDDMs and the baseline methods were all implemented with the Pytorch library. The score networks for the LJ and VCTK speech datasets were trained from scratch on a single NVIDIA Tesla P40 GPU with batch size  for about 1M steps, which took about 3 days.

For the model architecture, we used the same architecture as in DiffWave \citep{zhifeng2021} for the score network with 128 residual channels; we adopted a lightweight GALR network \citep{lam2021effective} for the schedule network. GALR was originally proposed for speech enhancement, so we considered it well suited for predicting the noise scales. For the configuration of the GALR network, we used a window length of 8 samples for encoding, a segment size of 64 for segmentation and only two GALR blocks of 128 hidden dimensions, and other settings were inherited from \citep{lam2021effective}. To make the schedule network output with a proper range and dimension, we applied a sigmoid function to the last block's output of the GALR network. Then the result was averaged over the segments and the feature dimensions to obtain the predicted ratio: , where  denotes the GALR network,  denotes the average pooling operation applied to the segments and the feature dimensions, and . The same network architecture was used for the NE approach for estimating  and was shown better than the ConvTASNet used in the original paper \citep{san2021noise}. It is also notable that the computational cost of a schedule network is indeed fractional compared to the cost of a score network, as predicting a noise scalar variable is intrinsically a relatively much easier task. Our GALR-based schedule network, while being able to produce stable and reliable results, was about 3.6 times faster than the score network. The training of schedule networks for BDDMs took only 10k steps to converge, which consumed no more than an hour on a single GPU.





Regarding the image generation task, to demonstrate the generalizability of our method, we directly adopted a \href{https://heibox.uni-heidelberg.de/d/01207c3f6b8441779abf/}{\textcolor{blue}{\underline{score network}}} pre-trained on the CIFAR-10 dataset implemented by a third-party open-source \href{https://github.com/pesser/pytorch_diffusion}{\textcolor{blue}{\underline{repository}}}. Regarding the schedule network, to demonstrate that it does not have to use specialized architecture, we replaced GALR by the VGG11 \citep{karen2014}, which was also used by as a noise estimator in \citep{san2021noise}. The output dimension (number of classes) of VGG11 was set to . Similar to the setting for GALR in speech synthesis, we added a sigmoid activation to the last layer to ensure a  output. Similar to the training in speech domain, we trained the VGG11-based schedule networks while freezing the score networks for 10k steps, which normally can be finished in about two hours.

Our code for the speech vocoding and the image generation experiments will be uploaded to Github after the final decision of ICLR is released.

\subsection{Crowd-sourced subjective evaluation}
All our Mean Opinion Score (MOS) tests were crowd-sourced. We refer to the MOS scores in~\citep{protasio_ribeiro_crowdmos_2011}, and the scoring criteria have been included in Table~\ref{matrix:naturalness} for completeness. The samples were presented and rated one at a time by the testers.

\begin{table}[t]
\centering
  \caption{Ratings that have been used in evaluation of speech naturalness of synthetic samples.}
  \label{matrix:naturalness}
  \begin{tabular}{ccc}
  \toprule
  Rating & Naturalness & Definition                           \\
  \midrule
  1      & Unsatisfactory        &  Very annoying, distortion is objectionable. \\
  2      & Poor       &  Annoying distortion, but not objectionable. \\
  3      & Fair       &  Perceptible distortion, slightly annoying.\\
  4      & Good       & Slight perceptible level of distortion, but not annoying.\\
  5      & Excellent  & Imperceptible level of distortion.\\
  \bottomrule
  \end{tabular}
  \end{table}

\begin{table}[t]
\centering
\caption{Performances of different noise schedules on the multi-speaker VCTK speech dataset, each of which used the same score network \citep{nanxin2020}  that was trained on VCTK for about 1M iterations.
}
\label{tab:vctk}
\begin{tabular}{lccccc}
 \toprule
        {\bfseries Noise schedule} & \bfseries LS-MSE () & \bfseries MCD () &\bfseries STOI () &\bfseries PESQ () & \bfseries MOS () \\
 \midrule
 \multicolumn{6}{l}{\bf DDPM \citep{ho2020denoising, nanxin2020}} \\
 \quad 8 steps (Grid Search) & 101 & \bf 2.09 & \bf 0.787 & \bf 3.31 & \bf 4.22  0.04\\
 \quad 1,000 steps (Linear) & 85.0 & 2.02 & 0.798 & 3.39 & 4.40  0.05\\
 \midrule
 \multicolumn{6}{l}{\bf DDIM \citep{jiaming2021}} \\
 \quad 8 steps (Linear) & 553 & 3.20 & 0.701 & 2.81 & 3.83  0.04\\
 \quad 16 steps (Linear) & 412 & 2.90 & 0.724 & 3.04 & 3.88  0.05\\
 \quad 21 steps (Linear) & 355 & 2.79 & 0.739 & 3.12 & 4.12  0.05\\
 \quad 100 steps (Linear) & 259 & 2.58 & 0.759 & 3.30 & 4.27  0.04\\
 \midrule
 \multicolumn{6}{l}{\bf NE \citep{san2021noise}} \\
 \quad 8 steps (Linear) & 208 & 2.54 & 0.740 & 3.10 & 4.18  0.04\\ 
 \quad 16 steps (Linear) & 183 & 2.53 & 0.742 & 3.20 & 4.26  0.04\\
 \quad 21 steps (Linear) & 852 & 3.57 & 0.699 & 2.66 & 3.70  0.03\\
 \midrule
 \multicolumn{6}{l}{\bf BDDM } \\
 \quad 8 steps  & \bf 98.4 & 2.11 & 0.774 & 3.18 & 4.20  0.04 \\
 \quad 16 steps  & \bf 73.6 & \bf 1.93 & \bf 0.813 & \bf 3.39 & \bf 4.35  0.05\\
 \quad 21 steps  & \bf 76.5 & \bf 1.83 & \bf 0.827 & \bf 3.43 & \bf 4.48  0.06 \\
 \bottomrule
\end{tabular}
\end{table}
\section{Additional experiments}
\label{appendix:C}
A demonstration page at \textcolor{blue}{\url{https://bilateral-denoising-diffusion-model.github.io}} shows some samples generated by BDDMs trained on LJ speech and VCTK datasets.

\subsection{Multi-speaker speech synthesis}
In addition to the single-speaker speech synthesis, we evaluated BDDMs on the multi-speaker speech synthesis benchmark VCTK \citep{yamagishi2019vctk}. VCTK consists of utterances sampled at  KHz by  native English speakers with various accents. 
We split the VCTK dataset for training and testing: 100 speakers were used for training the multi-speaker model and 8 speakers for testing.
We trained on a 44257-utterance subset (40 hours) and evaluated on a held-out 100-utterance subset. For the score network, we used the Wavegrad architecture \citep{nanxin2020} so as to examine whether the superiority of BDDMs remains in a different dataset and with a different score network architecture.


Results are presented in Table \ref{tab:vctk}. For this multi-speaker VCTK dataset, we obtained consistent observations with that for the single-speaker LJ dataset presented in the main paper.
Again, the proposed BDDM with only 16 or 21 steps outperformed the DDPM with 1,000 steps. To the best of our knowledge, ours was the first work that reported this degree of superior. When reducing to 8 steps, BDDM obtained performance on par with (except for a worse PESQ) the costly grid-searched 8 steps (which were unscalable to more steps) in DDPM. 
For NE, we could again observe a degradation from its 16 steps to 21 steps, indicating the instability of NE for the VCTK dataset likewise. In contrast, BDDM gave continuously improved performance while increasing the step number.


\pgfplotscreateplotcyclelist{colorlist}{blue,every mark/.append style={fill=blue},mark=*\\red,every mark/.append style={fill=red},mark=*\\green!80!black,every mark/.append style={fill=green!80!black},mark=*\\brown,every mark/.append style={fill=brown},mark=*\\pink,every mark/.append style={fill=pink},mark=*\\cyan,every mark/.append style={fill=cyan},mark=*\\green!60!black,densely dashed,every mark/.append style={
solid,fill=brown!100!black},mark=otimes*\\brown,densely dashed,every mark/.append style={solid,fill=gray},mark=otimes*\\blue,densely dashed,mark=star,every mark/.append style=solid\\red,densely dashed,every mark/.append style={solid,fill=red!80!black},mark=diamond*\\}






\subsection{Comparing different reverse processes for BDDMs}
This section demonstrates that BDDMs do not restrict the sampling procedure to a specialized reverse process in Algorithm \ref{alg:sampling}. In particular, we evaluated different reverse processes, including that of DDPMs as shown in Eq. (\ref{eq:reverse}) and DDIMs \citep{jiaming2021}, for BDDMs and compared the objective scores on the generated samples. DDIMs \citep{jiaming2021} formulate a non-Markovian generative process that accelerates the inference while keeping the same training procedure as DDPMs. The original generative process in Eq. (\ref{eq:reverse}) in DDPMs is modified into

where  is a sub-sequence of length  of  with , and  is defined as its complement; Therefore, only part of the models are used in the sampling process. 

To achieve the above, DDIMs defined a prediction function  that depends on  to predict the observation  given  directly:

By leveraging this prediction function, the conditionals in Eq. (\ref{eq:ddim}) are formulated as

where the detailed derivation of  and  can be referred to \citep{jiaming2021}. In the original DDIMs, the accelerated reverse process produces samples over the subsequence of  indexed by : . In BDDMs, to apply the DDIM reverse process, we use the  predicted by the schedule network in place of a subsequence of the training schedule .

Finally. the objective scores are given in Table \ref{tab:vctk_reverse_comp}. Note that the subjective evaluation (MOS) is omitted here since the other assessments above have shown that the MOS scores are highly correlated with the objective measures, including STOI and PESQ. They indicate that applying BDDMs to either DDPM or DDIM reverse process leads to comparable and competitive results. Meanwhile, the results show some subtle differences: BDDMs over a DDPM reverse process gave slightly better samples in terms of signal error and consistency metrics (i.e., LS-MSE and MCD), while BDDM over a DDIM reverse process tended to generate better samples in terms of intelligibility and perceptual metrics (i.e., STOI and PESQ).
\begin{table}[t]
\centering
\caption{Performances of different reverse processes for BDDMs on the VCTK speech dataset, each of which used the same score network \citep{nanxin2020}  and the same noise schedule.
}
\label{tab:vctk_reverse_comp}
\begin{tabular}{lcccc}
 \toprule
        {\bfseries Noise schedule} & \bfseries LS-MSE () & \bfseries MCD () &\bfseries STOI () &\bfseries PESQ () \\
 \midrule
 \multicolumn{5}{l}{\bf BDDM (DDPM reverse process)} \\
 \quad 8 steps  & \bf 91.3 & \bf 2.19 & 0.936 & 3.22\\
 \quad 16 steps  & \bf 73.3 & \bf 1.88 & 0.949 & 3.32\\
 \quad 21 steps  & \bf 72.2 & \bf 1.91 & 0.950 & 3.33\\
 \midrule
 \multicolumn{5}{l}{\bf BDDM (DDIM reverse process)} \\
 \quad 8 steps  & 91.8 & \bf 2.19 & \bf 0.938 & \bf 3.26\\
 \quad 16 steps  & 77.7 & 1.96 & \bf 0.953 & \bf 3.37\\
 \quad 21 steps  & 77.6 & 1.96 & \bf 0.954 & \bf 3.39\\
 \bottomrule
\end{tabular}
\end{table}
  



\begin{table}[!t]
\centering
\caption{Comparing sampling methods for DDPM with different number of sampling steps in terms of FIDs in CIFAR10.}
\label{tab:cifar10}
\begin{tabular}{l|c|c}
 \toprule
\textbf{Sampling method} & \textbf{Sampling steps} & \textbf{FID}\\ 
 \midrule
DDPM (baseline) \citep{ho2020denoising} &  & \\
 \midrule
DDPM (sub-VP) \citep{yang2021} &  & \\
 \midrule
\multirow{2}{*}{DDPM (DP + reweighting) \citep{watson2021learning}} &  &  \\
 &  & \\
 \midrule
\multirow{2}{*}{DDIM (quadratic) \citep{jiaming2021}} &  & \\
&  & \\
 \midrule
\multirow{2}{*}{FastDPM (approx. STEP) \citep{kong2021fast}} &  & \\
&  & \\
 \midrule
\multirow{2}{*}{\tablefootnote{Our implementation was based on \url{https://github.com/openai/improved-diffusion}}Improved DDPM (hybrid) \citep{nichol2021improved}} &  &  \\
 &  &  \\
 \midrule
\multirow{1}{*}{VDM (augmented) \citep{kingma2021variational}} &  & \tablefootnote{The authors of VDM claimed that they tuned the hyperparameters only for minimizing the likelihood and did not pursue further tuning of the model to improve FID.}\\
 \midrule
 \multirow{2}{*}{Ours BDDM} &  & \\
 &  & \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Unconditional image generation}
For the unconditional image generation task, we evaluated the proposed BDDMs on the benchmark CIFAR-10 (32  32) dataset. The score functions, including those initially proposed in DDPMs \citep{ho2020denoising} or DDIMs \citep{jiaming2021} and those pre-trained in the above third-party implementations, are all conditioned on a discrete step-index. We estimated the noise schedule  in continuous space using the VGG11 schedule network and then mapped it to discrete time schedule using the approximation method in \citep{kong2021fast}. 

Table \ref{tab:cifar10} shows the performances of different sampling methods for DDPMs in CIFAR-10. By setting the maximum number of sampling steps () for noise scheduling, we can fairly compare the improvements achieved by BDDMs against related methods in the literature in terms of FID. Remarkably, BDDMs with 100 sampling steps not only surpassed the 1000-step DDPM baseline, but also produced the SOTA FID performance amongst all generative models using less than or equal to  sampling steps.









\begin{figure}[H]
    \centering
    \includegraphics[scale=0.8]{iclr2022/three_steps.pdf}
    \caption{Spectrum plots of the speech samples produced by BDDM within 3 sampling steps. The first row shows the spectrum of a random signal for starting the reverse process. Then, from the top to the bottom, we show the spectrum of the resultant signal after each step of the reverse process performed by the BDDM. We also provide the corresponding WAV files on our demo page.}
\end{figure}

