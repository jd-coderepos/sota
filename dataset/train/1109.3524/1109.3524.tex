

Like a majority of numerical methods used in computational mechanics, the \ibm\ requires tools for sparse linear algebra. The matrices $A$, $Q$ and $Q^T$ are sparse, the vectors $q^{n+1}$ and $\lambda$ are dense, and we need to operate on these objects via matrix-vector, matrix-matrix, and even a triple-matrix multiplication. To take advantage of the {\gpu}, we need efficient means of both representing and operating on these matrices and vectors on the device. Currently, there are two tools available for this purpose: {\cusparse}, part of {\NV}'s {\cuda}, or the external library, {\cusp}. The {\cusp} library is being developed by several {\NV} employees with minimal software dependencies and released freely under an open-source license. We chose to use the {\cusp} library for several reasons: it is actively developed and separate from the main {\cuda} distribution, allowing for faster addition of new features (such as new pre-conditioners, solvers, \emph{etc.}); and, all objects/methods from the library are usable on both {\cpu} and {\gpu}. This allows us the flexibility to, for example, perform branching-heavy code on the {\cpu}, before trivially transferring to the device and running (for instance) a linear solve, where it will be significantly faster. It also allows us to maintain both a {\cpu} and {\gpu} code with less effort.

\begin{figure*}\centering
	\subfloat[Timing breakdown]{
		\includegraphics[width=0.45\textwidth]{breakdown_stack.pdf}\label{fig:breakdown}}
	\subfloat[Solving linear equations]{
		\includegraphics[width=0.45\textwidth]{solve_comparison.pdf}\label{fig:solve_speed}}
		\caption{\small (a)  Timing breakdown for a flapping airfoil at $Re=75$ using the \gpu\ code. (b) Comparison of time taken to solve a system of linear equations $Ax=b$ on the {\cpu} and {\gpu}. $A$ is chosen as the standard $5$-pt Poisson stencil.}
\end{figure*}

To illustrate the importance of having efficient linear algebra methods, we show in Figure \ref{fig:breakdown} a breakdown of the timings from an example run on the {\gpu} the with the \ibm\ ($4000$ time steps of a flapping airfoil at $Re = 75$). The mesh consists of $930\times 654$ cells, resulting in systems of over $600,000$ unknowns. Even in this moderately sized test, the runtime is dominated by the solution of the coupled linear system for pressure and forcing terms, denoted by `Solve 2'.
Speeding up this linear solve is the major motivation for using the {\gpu}.

At this point, it is interesting to consider the potential benefits of using the {\gpu} for our linear systems. Sparse linear systems are a well known example of a bandwidth-limited problem\,---\,the potential speedup from using a {\gpu} will at best be the ratio of the bandwidths of the {\cpu} and {\gpu} used. For the Intel Xeon X5650 processors in our workstation, Intel quotes a maximum bandwidth of $32$ GB/s\footnote{\href{http://ark.intel.com/products/47922/Intel-Xeon-Processor-X5650-(12M-Cache-2_66-GHz-6_40-GTs-Intel-QPI)}{http://ark.intel.com/products/47922/Intel-Xeon-Processor-X5650-(12M-Cache-2\_66-GHz-6\_40-GTs-Intel-QPI)}}, while {\NV} quotes $144$ GB/s for the Tesla C2050 cards we use\footnote{\href{http://www.nvidia.com/docs/IO/43395/NV_DS_Tesla_C2050_C2070_jul10_lores.pdf}{http://www.nvidia.com/docs/IO/43395/NV\_DS\_Tesla\_C2050\_C2070\_jul10\_lores.pdf}}. Using these numbers, we can see that the best-case speedup should be $~4.5\times$ for a purely bandwidth-bound problem, while for computationally bound problems the speedup can be much higher. In practice, even the worst case speedup is definitely worthwhile, certainly enough to justify the extra complexity involved in using the {\gpu}.

Figure \ref{fig:solve_speed} shows a timing comparison between the {\cpu} and {\gpu} using {\cusp}'s conjugate gradient solver. The system being solved in this case is given by a traditional 5-point Poisson stencil, which while not directly used in the \ibm\ code, gives a good measure of relative performance that can potentially be obtained. The plot shows the wall-clock time required to solve to a relative accuracy of $10^{-5}$ for numbers of unknowns ranging from $2500$ to $4\times 10^6$. For large systems, the {\gpu} solve is significantly faster, with a speedup of $8\times$ for the largest system shown. This indicates that even though our actual system may be much harder to solve, the potential speedup is significant.

\begin{figure}[h]\centering
	\includegraphics[width=0.45\textwidth]{sparsity.png}
	\caption{\small Example of sparsity pattern for $Q^{T}B^{N}Q$ matrix, showing the coupling terms in the right and lower sections of the matrix}
\label{fig:sparsity}
\end{figure}

For the particular linear system we're interested in, we also have to take into account that it has an unusual non-zero structure, shown in Figure \ref{fig:sparsity}, due to the coupled nature of the variables being solved for (pressure and forcing terms). This coupled nature leads to conflicting approaches being optimal for different sections of the matrix. The pressure terms are a standard 5-point Poisson stencil (to be extended to 7-point in 3D), and as such would be a good candidate for a hierarchical method using a smoother, such as algebraic multigrid (AMG). However, the forcing terms and coupling sections of the matrix would be unsuitable for this kind of approach. Instead, a standard solver such as CG (Conjugate Gradient) would be better suited.

To tackle this difficulty, we can choose one of two possible approaches: \emph{(i)} use a suitable pre-conditioner for the system to make it better suited for solution with a standard iterative solver, most likely CG, or \emph{(ii)} investigate other forms of solvers, such as AMG.
While option \emph{(ii)} could potentially have the greatest effect on both convergence rate and total time to solution, our choices are somewhat limited, given the tools we are using. Either we rely on the smoothed aggregation AMG that is part of {\cusp}, or we would need to develop our own from scratch---at this time there are no other \gpu-enabled AMG codes available for public use. 

To provide further insight into our options, we can test representative matrices from different applications of our code (with differing complexities of immersed boundaries) and compare a standard CG solver against the form of AMG available in {\cusp}, an aggregative AMG. The CG solver will be used both without a preconditioner, as well as using the diagonal and smoothed-aggregation preconditioners from {\cusp}. For each problem, we show the wall-clock time taken to solve the system to a relative accuracy of $10^{-5}$, the same accuracy used in previous tests. A summary of the matrices produced from each system can be seen in Table \ref{tab:test_details}, and the timing results are shown in Figure \ref{fig:solver_speeds}.

From these results, it can be plainly seen that PCG with smoothed-aggregation AMG as the preconditioner is the fastest on all tests, and so this is the combination of solver and preconditioner used in all our subsequent runs.

\begin{figure*}\centering
	\includegraphics[width=0.6\textwidth]{solver_speeds.pdf}
	\caption{\small Comparison of {\gpu} solvers on real test problems - Conjugate Gradient, Preconditioned Conjugate Gradient with diagonal and Smoothed Aggregation preconditioners, and a full solve using Smoothed Aggregation}
	\label{fig:solver_speeds}
\label{fig:solver_speeds}
\end{figure*}

\begin{table*}
\begin{center}
\small
	\begin{tabular}{| c | c | c | c | c | }
	\hline
		System \# & Domain Size & \# Bdy Points & \# Unknowns & \# Non-zeros \\
	\hline
		1 & $330\times330$ & 158 & 109216 & 554752 \\
		2 & $986\times986$ & 786 & 973768 & 4914520 \\
		3 & $930\times654$ & 101 & 608422 & 3045406 \\
		4 & $690\times690$ & 474 & 477048 & 2412448 \\
	\hline
	\end{tabular}
	\caption{\small Systems $1$ and $2$ correspond to flow over a cylinder at $Re = 40$ and $3000$ respectively, system $3$ is for a flapping-wing calculation, and system $4$ is from a synthetic test with $3$ moving cylinders.}
	\label{tab:test_details}
\end{center}
\end{table*}

Our choice of tools allows us to easily perform all sparse linear algebra operations on the {\gpu}. On the other hand, there are parts of the algorithm that cannot easily be expressed using linear algebra, such as generating the convection term using a finite-difference stencil and applying boundary conditions to the velocities (which involves modifying select values of appropriate arrays). One possible way of performing these actions is to transfer data from the {\gpu}, perform the calculations on the {\cpu} and transfer the modified vector(s) back to the {\gpu} every time step\,---\,but this incurs a prohibitively high cost in memory transfers. The alternative, which we have done, is to use custom-written {\cuda} kernels utilizing all appropriate techniques, including the use of shared memory, to perform these operations on the {\gpu}. This requires access to the underlying data from the {\cusp} data structures, which can be done easily using the \textsl{Thrust} library, on which {\cusp} was built. 


\section{Strategy for \gpu\ implementation}

Here we discuss some important implementation details that are used to decrease the total runtime, or to decrease the amount of memory needed, or both.

\begin{itemize}

\item \emph{Keep everything on the {\gpu}: } To reduce costly memory transfers, everything that can be performed on the {\gpu} is, regardless of its suitability. For instance, the code to enforce boundary conditions operates on very few values, has almost no computational complexity and is highly divergent based on the boundary conditions chosen (\emph{i.e.}, Dirichlet / Neumann, \emph{etc.}). Thus, it will always perform rather badly on the device, giving essentially no speedup. However, it would take significantly longer to transfer the necessary data back to the host and perform the operation there. This means taking the time to write and test this kernel gives us an overall speedup, due to this elimination of transfers. This is only one example, but all but one element of the total algorithm has been transferred, yielding significant savings in the total run time. The final element still on the {\cpu} involves updating the forcing terms in $Q$, and as such involves a very small number of entries compared to any other array. Thus, this transfer ends up being an inconsequential part of the total algorithm.

\item \emph{Triple Matrix Product: } To generate the left-hand side of equation (\ref{ibpm2}), we need to multiply 3 sparse matrices, $Q^{T}$, $B^{N}$ and $Q$. To do this within the current {\cusp} framework, we would need to perform two separate matrix-matrix multiplies, the first for $Q^{T}B^{N}$ and then the result of this multiplied by $Q$. This method involves creating a temporary matrix in memory, and for higher-order approximations of $B^{N}$, say, $3^{rd}$-order, this temporary can be quite large, in the order of $700\mathrm{MB}$. {\cusp} performs matrix-matrix multiplies in the following form (using Matlab notation, where $A\verb|[range,range]|$ denotes a sub matrix of $A$, and the `$\verb|:|$' symbol denotes all columns/rows depending on context; thus, $A\verb|[slice,:]|$ refers to all columns in a range of rows denoted by $\verb|slice|$)

\begin{algorithmic}
	\REQUIRE Input sparse matrices $A$, $B$, result matrix $C$
	\STATE Split $A$ into slices $\verb|A[slice,:]|$ s.t. each $\verb|A[slice,:]|\cdot \verb|B|$ fits in device memory
	\FOR{$\forall$ slices}
		\STATE $\verb|slice| \gets \verb|A[slice,:]|\cdot \verb|B|$
		\STATE add slice to list of slices
	\ENDFOR
	\STATE Assemble $C$ from all computed slices
\end{algorithmic}

\noindent
The multiplication of $\verb|A[slice,:]|\cdot \verb|B|$ is here being performed by a helper function within \cusp\ (\verb|cusp::detail::device::spmm_coo_helper|).

We propose a routine to perform a triple matrix product of the form $D \gets A\cdot B\cdot C$ by using this helper function repeatedly to ensure the full intermediate product need not be calculated. We do this by realizing the following:
\begin{eqnarray*}
	\verb|temp_slice| & = & \verb|A[slice,:]|\cdot \verb|B| \\
	\verb|D[slice,:]| & = & \verb|temp_slice|\cdot \verb|C|.
\end{eqnarray*}

Thus, we can form slices of the final result while only storing a slice as an intermediate by applying the helper function twice in succession. This alleviates the need to create and store the full intermediate matrix. While at first glance the intermediate, $B^{N}Q$ might be computed once and used in both (\ref{ibpm2}) and (\ref{ibpm3}), we explain below how this term is unnecessary in (\ref{ibpm3}).

\item \emph{Use optimized routines where available: } In equation (\ref{ibpm3}), we have to calculate the product $B^{N}Q\lambda$, where $B^{N}$ and $Q$ are sparse matrices, and $\lambda$ is a dense vector. If we implement this naively, we perform the matrix-matrix product of $B^{N}Q$ then multiply this with the vector $\lambda$. However, the Sparse matrix-vector product (SpMV) in {\cusp} has been optimized to a significant state, as demonstrated by the co-authors of {\cusp}\cite{BellGarland2009}. Therefore, we prefer to implement this operation as a pair of SpMV operations, first $Q\lambda$ resulting in a vector, then the result of this with $B^{N}$, exchanging the matrix-matrix product with a SpMV. This also reduces the amount of memory needed, as we have no need to store the extra matrix obtained by $B^{N}Q$.

\item \emph{Re-use the hierarchy generated by the smoothed-aggregation preconditioner: } The most expensive part of our second solve, Equation \eqref{ibpm2} for moving bodies, is the generation of the hierarchy for the smoothed aggregation preconditioner. When bodies are stationary, we have no need to recompute this at each time step, and can use the same hierarchy for the entire simulation, effectively amortising the high cost over the total run. When we have moving bodies, this hierarchy should be recalculated every time step. We propose to instead re-use the hierarchy for several time steps. To do so, we need to investigate the balance of not performing the expensive calculation every time step, but potentially having a lower convergence rate for the steps where the hierarchy is re-used. For a flapping airfoil, Figure \ref{fig:npc_steps} shows the effect on the total run time of recalculating the preconditioner every $n$ time steps, clearly showing that in order to reduce run time for this problem, we should recalculate the preconditioner every 2 or 3 time steps.

\begin{figure}[h]
\centering
	\includegraphics[width=0.45\textwidth]{npc_steps.pdf}
	\caption{\small Timings for a flapping airfoil run, re-calculating the Smoothed Aggregation precondtioner every number of time steps.}
	\label{fig:npc_steps}
\end{figure}

\end{itemize}
