\documentclass[a4paper]{article}

\usepackage{INTERSPEECH2022}

\usepackage{hyperref}
\usepackage{pdfpages}


\newcommand\blfootnote[1]{\begingroup
  \renewcommand\thefootnote{}\footnote{#1}\addtocounter{footnote}{-1}\endgroup
}




\usepackage{amsmath,graphicx,amsfonts, subcaption}
\usepackage{xcolor}
\usepackage[normalem]{ulem}

\def\x{{\mathbf x}}
\def\L{{\cal L}}

\title{Unsupervised Source Separation via Bayesian Inference\\ in the Latent Domain}

\name{
Michele Mancusi\thanks{* Equal contribution}, Emilian Postolache, Giorgio Mariani, Marco Fumero, Andrea Santilli, Luca Cosmo, Emanuele Rodol√†
}

\address{ Sapienza University of Rome, Ca' Foscari University of Venice, University of Lugano}
\email{mancusi@di.uniroma1.it}


\begin{document}




\maketitle


\begin{abstract}
State of the art audio source separation models rely on supervised data-driven approaches, which can be expensive in terms of labeling resources. On the other hand, approaches for training these models without any direct supervision are typically high-demanding in terms of memory and time requirements, and remain impractical to be used at inference time. We aim to tackle these limitations by proposing a simple yet effective unsupervised separation algorithm, which operates directly on a latent representation of time-domain signals. Our algorithm relies on deep Bayesian priors in the form of pre-trained autoregressive networks to model the probability distributions of each source. We leverage the low cardinality of the discrete latent space, trained with a novel loss term imposing a precise arithmetic structure on it, to perform exact Bayesian inference without relying on an approximation strategy. 
We validate our approach on the Slakh dataset \cite{manilow2019}, demonstrating results in line with state of the art supervised approaches while requiring fewer resources with respect to other unsupervised methods.
\end{abstract}

\noindent\textbf{Index Terms}: 
Signal separation, Autoregressive generative models, Bayesian inference, Unsupervised learning


\section{Introduction}
\label{sec:introduction}
Generative models have reached promising results in a wide range of domains, including audio, and can be used to solve different tasks in unsupervised learning. A relevant problem in the musical domain is the task of source separation of different instruments. Given the sequential nature of music and the high variability of rhythm,  timbre and melody, autoregressive models \cite{larochelle11} represent a popular and effective choice  to process data on such domain, showcasing high multi-modality in the modeled probability distributions. The widely adopted WaveNet autoregressive architecture \cite{oord2016} works in the temporal domain. Given that audio signals are typically sampled at high frequencies (e.g.  kHz) for music, the choice of modeling the data distribution directly in the time domain leads to short contexts being captured by neural computations and quick saturation of memory. Nevertheless, existing unsupervised approaches for source separation operate in the time domain \cite{jayaram2021}. In order to capture longer contexts and to reduce memory burden, different quantization schemes have been introduced for autoregressive models \cite{oord2017,razavi2019}, where chunks in time are mapped to sequences of latent tokens belonging to a small vocabulary. OpenAI's Jukebox \cite{dhariwal:2020} follows this approach and excels as an architecture that can capture very long contexts, generating highly consistent tracks.
Leveraging the useful properties of this architecture, we propose a novel approach to unsupervised source separation that works directly on quantized latent domains. 

Our contributions can be summarized as follows:
\begin{enumerate}
    \item We perform source separation applying exact Bayesian inference directly in the latent domain, exploiting the relative small size of the latent dictionary. We do not rely on any approximation strategy, such as variational inference or Langevin dynamics. 
    \item We introduce LQ-VAE: a quantized autoencoder trained with a novel loss that  imposes an algebraic structure on the discrete latent space. This allows us to alleviate noisy and distorted samples which arise from a vanilla quantization approach.
 \end{enumerate}
 \section{Related work}
\label{sec:related}

The problem of source separation has been classically tackled in an unsupervised fashion \cite{comon:1994}, where the sources to be separated from a mixture signal are unknown \cite{Smaragdis:2014}.
With the advent of deep learning, most source separation tasks applied to musical data started relying on supervised learning, training models on data with known correspondence between sources. Recently, following the success of deep generative models, there has been a renewed interest in unsupervised methods.
\subsection{Supervised source separation}
Supervised source separation aims to map high dimensional observations of audio mixtures to a smaller dimensional space and apply, explicitly or implicitly, a mask to filter out the sources from the latent representation of the mixtures in a supervised way.
Most of these works can be divided into \emph{frequency-domain} or \emph{waveform-domain} approaches. 
The former \cite{Roweis:2000} operate on the spectral representation of the input mixtures.
This line of works has highly benefited from  the incoming of deep learning techniques from simple fully connected networks \cite{Uhlich:2015}, LSTM \cite{Uhlich:2017}, and CNN coupled with recurrent approaches \cite{liu:2018,Takashi2018}.
Recent approaches such as \cite{choi:2020} and \cite{takahashi:2020} hold the state of the art in music source separation over the dataset MUSDB18 \cite{musdb18}, by respectively extending the conditional U-net architecture of \cite{meseguer-brocal2019} to  multi-source  separation, and by exploiting multi-dilated convolution that applies different dilation factors in each layer to model different resolutions simultaneously.
In contrast, waveform domain approaches process the mixtures directly in the time domain to overcome phase estimation, which is necessary when converting the signal from the frequency domain. The method of \cite{defossez2019} performs in line with the state of the art by extending a WaveNet-like architecture, coupled with an LSTM in the latent space.

The main limitation of these state-of-the-art methods for audio source separation is that they require large amounts of fully separated, labeled data to perform the training.

\subsection{Unsupervised source separation}

Recent approaches in unsupervised source separation leverage self-supervised learning. A prominent baseline is MixIt \cite{wisdom2020}, which trains a model by trying to separate sources from a mixture of mixtures. Although promising, such model suffers from the \emph{over-separation} problem, where at test time a number of sources that is greater than those present in the mixture are estimated. As such, stems can be split across different output tracks. Generative approaches instead overcome this problem by imposing that a model should output an individual stem.

Closer to our work, \cite{narayanaswamy2020unsupervised} proposes to leverage generative priors in the form of GANs trained on individual sources. They use projected gradient descent optimization to search in the source-specific latent spaces and effectively recover the constituent sources in the time domain. Although promising, GANs suffer from modal collapse, so their performance is limited in the musical domain, where variability is abundant.
\cite{jayaram2021} proposes to use Langevin dynamics on the global log-likelihood of the audio sequences to parallelize the sampling procedure of autoregressive
models used as Bayesian priors. 
This approach produces good results but with a high computational cost due to the need of training distinct models for each noise level, and due to the costly optimization procedure in the time domain.


Differently, our inference procedure has much lower computational and memory requirements, allowing us to efficiently run the model on a single GPU. In addition, we can perform exact Bayesian inference without relying on an approximation scheme of the posterior (e.g., its score).
 \section{Background}

\label{sec:background}
   In this section we briefly introduce the background concepts necessary to understand our architecture, which builds upon  \cite{dhariwal:2020}. The overall architecture can be split into two parts: (i) a quantization module mapping the input sequences to a discrete latent space, and (ii) an autoregressive prior (one per source) which models the distribution of a given source in the discrete latent space. We point the reader to \cite{dhariwal:2020} for a deeper understanding.

\subsection{Quantization module}
Let us consider an input sequence   of length , which represents a normalized waveform in the time domain. In order to be representative of an expressive portion of the audio sequence,  should be large. However, due to the complexity of modern neural architectures, choosing a large enough value of  is not always feasible. To reduce the dimensionality of the space one can leverage the VQ-VAE architecture \cite{oord2017} to map large continuous sequences in the time domain to smaller sequences in a discrete latent domain. A VQ-VAE is composed of three blocks: 
\begin{itemize}
    \item A convolutional encoder , with , where  is the  length of the latent sequence and  denotes the number of channels;
    \item A bottleneck block  where   is a vector quantizer, mapping the sequence of latent vectors  into the sequence of nearest neighbors contained in a codebook  of learned latent codes, and  is an indexer mapping the codes  into the associated codebook indices . Note that since  is bijective, the codes  and their indices  are semantically equivalent, but we shall use the term `codes' for the vectors in  and `latent indices' for the associated integers;
    \item A decoder  mapping the discrete sequence back into the time domain.
\end{itemize}
The VQ-VAE is trained by minimizing the composite loss:
 where:

where  is the stop-gradient operator and  is the commitment loss weight. The losses  and  update the entries of the codebook  during the training procedure. In addition, we introduce a novel loss term , described in Section , which imposes a precise algebraic structure on the latent space, facilitating the task of source separation.

\subsection{Latent autoregressive priors}
Once the VQ-VAE is trained, time domain data  can be mapped to latent sequences . Autoregressive priors  can then be learned over the discrete domain.
In this work, the autoregressive models are based on a deep scalable Transformer architecture as in \cite{dhariwal:2020}.
In order to generate new time-domain examples, sequences of latent indices are sampled from  via ancestral sampling and then mapped back to the time domain via the decoder of the VQ-VAE.  \section{Method}
\label{sec:method}

\begin{figure}[t!]
  \centering
    \def\svgwidth{\columnwidth}\scalebox{0.9}{
\begingroup \makeatletter \providecommand\color[2][]{\errmessage{(Inkscape) Color is used for the text in Inkscape, but the package 'color.sty' is not loaded}\renewcommand\color[2][]{}}\providecommand\transparent[1]{\errmessage{(Inkscape) Transparency is used (non-zero) for the text in Inkscape, but the package 'transparent.sty' is not loaded}\renewcommand\transparent[1]{}}\providecommand\rotatebox[2]{#2}\newcommand*\fsize{\dimexpr\f@size pt\relax}\newcommand*\lineheight[1]{\fontsize{\fsize}{#1\fsize}\selectfont}\ifx\svgwidth\undefined \setlength{\unitlength}{595.27559055bp}\ifx\svgscale\undefined \relax \else \setlength{\unitlength}{\unitlength * \real{\svgscale}}\fi \else \setlength{\unitlength}{\svgwidth}\fi \global\let\svgwidth\undefined \global\let\svgscale\undefined \makeatother \begin{picture}(1,1.41428571)\lineheight{1}\setlength\tabcolsep{0pt}\put(0,0){\includegraphics[width=\unitlength,page=1]{figures/inference_new2.pdf}}\end{picture}\endgroup  }


\caption{In our method, two autoregressive priors  and  are trained on different instrument sources in the latent domain. At each step  they provide the joint prior . The prior is combined with a -isotropic Gaussian likelihood  in order to compute the posterior  from which new samples are drawn.}
\label{fig:method_1}

\end{figure}

\begin{figure}[t!]
  \centering
  \def\svgwidth{\columnwidth}\scalebox{1.0}{
\begingroup \makeatletter \providecommand\color[2][]{\errmessage{(Inkscape) Color is used for the text in Inkscape, but the package 'color.sty' is not loaded}\renewcommand\color[2][]{}}\providecommand\transparent[1]{\errmessage{(Inkscape) Transparency is used (non-zero) for the text in Inkscape, but the package 'transparent.sty' is not loaded}\renewcommand\transparent[1]{}}\providecommand\rotatebox[2]{#2}\newcommand*\fsize{\dimexpr\f@size pt\relax}\newcommand*\lineheight[1]{\fontsize{\fsize}{#1\fsize}\selectfont}\ifx\svgwidth\undefined \setlength{\unitlength}{841.88976378bp}\ifx\svgscale\undefined \relax \else \setlength{\unitlength}{\unitlength * \real{\svgscale}}\fi \else \setlength{\unitlength}{\svgwidth}\fi \global\let\svgwidth\undefined \global\let\svgscale\undefined \makeatother \begin{picture}(1,0.70707071)\lineheight{1}\setlength\tabcolsep{0pt}\put(0,0){\includegraphics[width=\unitlength,page=1]{figures/lqvae.pdf}}\end{picture}\endgroup  }
\caption{
Training scheme of the LQ-VAE:
reconstructions ,  are obtained from input pairs  as in the VQ-VAE, leading to the loss  (Eq. ). To this loss we add the post-quantization linearization loss  (Eq. \eqref{eq:ll}), that is computed by matching time-domain sums with latent vector sums.}
\label{fig:method_2}
\vspace{-0.2cm}
\end{figure}

The proposed algorithm is composed of two parts. A first \emph{separation phase} in the latent domain, in which we sequentially sample from an exact posterior  on discrete indices.
A following \emph{rejection sampling procedure} based on a (scaled) global posterior conditioned on the separation results, which we use to sort the proposed solutions and select the most promising one.

\subsection{Latent Bayesian source separation}
     Our task is to separate a mixture signal  into  and , where  and  represent the distributions of each instrument class in the time domain. In a Bayesian framework, a candidate solution  is distributed according to the posterior , where the priors ,  are typically deep generative models and the likelihood  is parameterized as .
     
     In this work, we follow the Bayesian approach but we work in the latent domain.
     After training the VQ-VAE on an {\em arbitrary} audio dataset (with samples lying also outside  and ), we learn two latent autoregressive priors  and  over the two instrument classes. The priors do not require any correspondence between the sources, being trained in a completely unsupervised setting. We assume the two priors to be independent, i.e. . Therefore, for each step , we can compute the posterior distribution 
          .
          
          The random variable  is a function of the mixture .
One can choose to model  in multiple ways; a naive approach is to choose  as the identity and set , thus computing  the likelihood function directly in the time domain. This approach, however, requires the  decoding of at least  possible latent indices in order to locally compare the mixture  with the hypotheses  and . Note that this corresponds to a lower bound, given that the convolutional nature of the decoder requires a larger past context to produce meaningful results. Differently, we propose to define  in the latent domain, setting . This approach is preferable since it does not require decoding the hypotheses at each step , resulting in lower memory usage and computation time.
Our method benefits from the choice of operating in the latent space, thanks to the relatively small size of the priors and the likelihood function domain (we choose , as in \cite{dhariwal:2020}). 
          In addition, by exploiting the Transformer architecture, the prior distributions can be computed in parallel. For these reasons, evaluating and sampling from   at each  is computationally feasible and has  memory complexity. See Figure \ref{fig:method_1} for a visual description of the inference algorithm.
 

         
\subsection{Latent likelihood via LQ-VAE} \label{sec:likelihood}
 In this section we describe how we model the likelihood function and introduce the LQ-VAE model. Following \cite{jayaram2020} we chose a -isotropic Gaussian likelihood, setting:

The hyper-parameter  balances the trade-off between the likelihood and the priors. Lower values promote the likelihood: the separated tracks combine perfectly with , but may not sound like the instrument of the class they belong to. Instead, higher values of  give importance to the priors: the separated tracks contain only sounds from the corresponding source distribution, but may not mix back to  (not resembling the sources).
The logarithm of the likelihood is: 
At each step , we compare a variable term  with a constant matrix  representing all possible (scaled) sums over all codes in . This term can be precomputed once and then reused during inference, saving additional computational resources.

We observed that performing separation with the likelihood in Eq. \eqref{eq:latent_likelihood} using a VQ-VAE trained with the loss in
Eq. \eqref{eq:loss_vqvae}, results in disturbed and noisy outcomes. Such behavior is expected because the standard VQ-VAE does not impose any algebraic structure on the discrete domain; therefore, summing codes as in Eq. \eqref{eq:latent_likelihood} does not lead to meaningful results. This problem can be lifted by enforcing a post-quantization linearization loss on the VQ-VAE: 

where  is defined as in Eq.   and

Minimizing this loss pushes the quantized latent code representing a mixture of two arbitrary source signals ( term) to be equal to the sum of the quantized latent codes, corresponding to the single sources ( term), therefore enforcing the discrete codes to behave in an approximately linear way.
We shall refer to the VQ-VAE trained as above, as a {\em Linearly Quantized Variational Autoencoder} (LQ-VAE). See Figure \ref{fig:method_2} for a visual illustration of the LQ-VAE training procedure.
\begin{table}[t!]
\centering
\setlength{\tabcolsep}{3pt}
\resizebox{\columnwidth}{!}{\begin{tabular}{lcccccl}
\hline
\multicolumn{1}{|l|}{\textbf{Method}}       & \multicolumn{1}{l}{\textbf{Drums}} & \multicolumn{1}{l|}{\textbf{Bass}}          & \multicolumn{1}{l}{\textbf{Drums}} & \multicolumn{1}{l|}{\textbf{Guitar}}        & \multicolumn{1}{l}{\textbf{Guitar}} & \multicolumn{1}{l|}{\textbf{Bass}}  \\ 
\hline
\multicolumn{1}{|l|}{Ours (best)}            & \textbf{5.83}          & \multicolumn{1}{c|}{\textbf{7.42}} & \textbf{8.33}             & \multicolumn{1}{c|}{3.80}          & 3.75             & \multicolumn{1}{c|}{\textbf{8.65}}  \\ 
\hline
\multicolumn{1}{|l|}{Ours (rej)} & 4.08                      & \multicolumn{1}{c|}{5.31}          & 6.93                      & \multicolumn{1}{c|}{2.48}          & 1.95                     & \multicolumn{1}{c|}{6.35}          \\ \hline
\multicolumn{1}{|l|}{Demucs}            & 5.42                      & \multicolumn{1}{c|}{5.36}          &              5.80              &       5.36          &            \multicolumn{1}{|c}{6.42}       &
\multicolumn{1}{c|}{7.68}                                                     \\ \hline
\multicolumn{1}{|l|}{TasNet}       & 5.51                      & \multicolumn{1}{c|}{5.43}                                  & \multicolumn{1}{c}{5.87}       &
\multicolumn{1}{c|}{\textbf{5.47}}& \multicolumn{1}{c}{\textbf{7.80}}       &
\multicolumn{1}{c|}{8.46}                                               \\ \hline




\multicolumn{1}{|l|}{rPCA\cite{rpca}}            & 0.60                      & \multicolumn{1}{c|}{1.05}          & 2.27                      & \multicolumn{1}{c|}{-0.42}          & 0.52                    & \multicolumn{1}{c|}{-1.12}          \\ \hline
\multicolumn{1}{|l|}{ICA\cite{ica}}           & -0.99                     & \multicolumn{1}{c|}{-1.53}         & \multicolumn{1}{c}{-0.53} & \multicolumn{1}{c|}{-3.23}         & -0.73                   & \multicolumn{1}{c|}{-2.79}        \\ \hline
\multicolumn{1}{|l|}{HPSS \cite{hpss}}            & -0.56                     & \multicolumn{1}{c|}{-0.33}          & 0.31                     & \multicolumn{1}{c|}{-2.72}          & 0.15                     & \multicolumn{1}{c|}{-0.38}          \\ \hline
\multicolumn{1}{|l|}{REPET\cite{repet}}          & 0.53                      & \multicolumn{1}{c|}{1.54}           & 2.91                      & \multicolumn{1}{c|}{0.11} & 0.40                    & \multicolumn{1}{c|}{-1.09}           \\ \hline
\multicolumn{1}{|l|}{FT2D \cite{ft2d}}            & 0.59                      & \multicolumn{1}{c|}{1.31}          & 2.63                      & \multicolumn{1}{c|}{-0.15}          & 0.65                    & \multicolumn{1}{c|}{-1.02}         \\ \hline
\end{tabular}
}
\vspace{0.1cm}
\caption{SDR scores evaluated on Slakh2100 test set. All methods are unsupervised except those marked with . The \textnormal{rej} attribute indicates that the solutions were obtained by the rejection sampling procedure with . The scores are computed according to the implementation in \cite{SiSEC18}}.
\label{table:sdr}
\end{table}

\begin{table}[t!]
\setlength{\tabcolsep}{3pt}
\centering
\resizebox{\columnwidth}{!}{\begin{tabular}{lcccccl}
\hline
\multicolumn{1}{|l|}{\textbf{Rejection }}       & \multicolumn{1}{l}{\textbf{Drums}} & \multicolumn{1}{l|}{\textbf{Bass}}          & \multicolumn{1}{l}{\textbf{Drums}} & \multicolumn{1}{l|}{\textbf{Guitar}}        & \multicolumn{1}{l}{\textbf{Guitar}} & \multicolumn{1}{l|}{\textbf{Bass}}  \\


\hline
\multicolumn{1}{|l|}{0}            & \textbf{4.08}          & \multicolumn{1}{c|}{\textbf{5.31}} & \textbf{6.93}             & \multicolumn{1}{c|}{\textbf{2.48}}          & \textbf{1.95}             & \multicolumn{1}{c|}{\textbf{6.35}}  \\ 

\hline
\multicolumn{1}{|l|}{0.5} &3.61                      & \multicolumn{1}{c|}{4.78}          & 6.69                      & \multicolumn{1}{c|}{2.17}          & 1.68                     & \multicolumn{1}{c|}{6.00}          \\ \hline


\multicolumn{1}{|l|}{1}            & 2.94                      & \multicolumn{1}{c|}{4.03}          &              6.44              &       1.95         &            \multicolumn{1}{|c}{1.15}       &
\multicolumn{1}{c|}{5.35}                                                      \\ \hline
\end{tabular}}
\caption{Ablation study for rejection parameter .} \label{table:rejection_ablation}
\end{table}
\begin{table}[t!]
\centering
\begin{tabular}{lccl}
\hline
\multicolumn{1}{|l|}{\textbf{Method}}       &  \multicolumn{1}{l}{\textbf{Drums}} & \multicolumn{1}{l|}{\textbf{Piano}}         \\ 
\hline
\multicolumn{1}{|l|}{Ours}            &  \textbf{0.68}            & \multicolumn{1}{c|}{\textbf{3.66}} \\ 
\hline
\multicolumn{1}{|l|}{Ours (rejection )} & 0.08                     & \multicolumn{1}{c|}{2.75}          \\ \hline
\multicolumn{1}{|l|}{GAN [20]}             & -3.16                     & \multicolumn{1}{c|}{-2.26}         \\ \hline
\end{tabular}
\caption{SDR table evaluated on the test set of \cite{narayanaswamy2020unsupervised}.}\label{table:gan}
\vspace{-0.9cm}
\end{table}

\subsection{Rejection sampling}

Given the low memory requirements of our method, at inference time we can sample in parallel multiple solutions  in the same batch. Autoregressive models tend to accumulate errors over the course of ancestral sampling, therefore the quality of the solutions varies across the batch. 
In order to select a solution, we look at the posterior , conditioned by the sampling event. We obtain the priors  and  by normalizing  and  over the batch (computed by integrating over  during the inference). For numerical stability, we scale their logits by the length of the latent sequences . The likelihood function  is computed directly in the time domain, with the decoding pass being executed only once at the end of the sampling procedure. The hyper-parameter  plays a similar role to the  used in Eq. \eqref{eq:latent_likelihood}. We can balance the likelihood and the priors by setting:
 and solving for . Albeit natural, this framework does not lead to the best selection. We performed an ablation study by weighting the contribution of the global likelihood with a scalar  (using ) and the best empirical results are obtained when the global likelihood is not taken into account (, see Table \ref{table:rejection_ablation}.  We call this selection criterion \textit{prior-based rejection sampling}.

%
 \section{Results}
\label{sec:experiments}

We validate our approach on \textit{Slakh2100}  \cite{manilow2019}: 
a large musical source dataset containing mixed tracks separated into  instrument categories. We select tracks from the classes `drum', `bass' and `guitar' coming from the training and test splits, sub-sampled at a frequency of kHz. We train the convolutional LQ-VAE over mixtures obtained by randomly mixing sources from the individual tracks of the training set. The LQ-VAE has a downsampling factor of  and uses a dictionary of  latent codes. After training the LQ-VAE, we train two autoregressive models, one per source, on latent codes extracted from  tracks each. In all our separation experiments we fixed  in Eq. \eqref{eq:sei}.
In Table \ref{table:sdr} we compare our method with two state-of-the-art supervised approaches and different non-learning based unsupervised methods. To this end, we iterate on the test split of \cite{manilow2019} made up of about  different songs, and for each we extract  random chunks each of  seconds. 


In order to strengthen our empirical evaluation, we show in Table \ref{table:gan} results of our model applied to a different validation data set in order to perform a comparison with the GAN model of \cite{narayanaswamy2020unsupervised}. We evaluate both methods over the test dataset
proposed in \cite{narayanaswamy2020unsupervised}, consisting of 1000 mixtures of 1 second each. Each mixture combines a drum sample with a piano track randomly, thus independence
in the test data is assumed, resulting in a more artificial setting with respect
to the one present in Slakh2100. For \cite{narayanaswamy2020unsupervised} we use the pre-trained model
given by the authors while for our method we use the ‚Äúdrums‚Äù and ‚Äúpiano‚Äù priors trained on Slakh2100 thus showing the cross-dataset generalization
capability of our model.


All our experiments are performed on a Nvidia RTX 3080 GPU with  GB of VRAM. With this GPU our method can sample a batch of  candidate solutions ( for each instrument) simultaneously. The code to reproduce our experiments is available at \url{https://github.com/michelemancusi/LQVAE-separation}.
Interestingly, even if solutions selected by the rejection sampling algorithm have slightly lower metrics than supervised approaches, by individually selecting the best solution for each instrument we achieve performance in line with the state of the art (especially on `bass` and `drum` stems). This testifies the quality of our separation. 
Remarkably, our method employs  minutes on average for sampling a track of  seconds,
compared to the more than  minutes of \cite{jayaram2021}.











 \section{Conclusions}
\label{sec:conclusion}
In this work, we introduced a simple algorithm to perform exact Bayesian inference in the discrete latent domain.
Our method allows to achieve good separation results while being much faster than other likelihood-based unsupervised approaches.


The main bottleneck of our method lies in the rejection sampling strategy. Future work will attempt to improve this aspect by investigating the design of more accurate learning-based rejection samplers. Other benefits could come from the adoption of multi-level VQ-VAEs \cite{dhariwal:2020} or by leveraging deeper autoregressive priors.

%
 

\bibliographystyle{IEEEtran}
\bibliography{bibliography}

\end{document}
