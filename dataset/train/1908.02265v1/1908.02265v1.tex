\documentclass{article}





\usepackage[sort&compress]{natbib}
\usepackage{style/neurips_2019_author_response}

\setcitestyle{numbers}
\citestyle{plain}
\bibliographystyle{unsrtnat}

\usepackage{wrapfig}
\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{multirow}
\usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{epigraph}
\usepackage{soul}
\usepackage[dvipsnames]{xcolor}
\usepackage{paralist}
\usepackage{graphicx}
\usepackage{subcaption}



\newcommand{\vlbert}{ViLBERT}

\newcommand{\vilbert}[1]{\includegraphics[height=#1]{figures/vilbert_trim.png}}

\newcommand{\mlm}{\mathcal{L}_{\mbox{MLM}}}
\newcommand{\nsp}{\mathcal{L}_{\mbox{NSP}}}

\newcommand{\xhdr}[1]{\vspace{0pt}\noindent\textbf{#1}}

\newcommand{\sml}[1]{\textcolor{Black}{#1}}
\newcommand{\jl}[1]{\textcolor{Black}{#1}}
 \parskip=5pt
\abovedisplayskip 3.0pt plus2pt minus2pt\belowdisplayskip \abovedisplayskip
\renewcommand{\baselinestretch}{1} 


\newenvironment{packed_enum}{
\begin{enumerate}
  \setlength{\itemsep}{1pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
}
{\end{enumerate}}

\newenvironment{packed_item}{
\begin{itemize}
  \setlength{\itemsep}{1pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
}{\end{itemize}}

\newlength{\sectionReduceTop}
\newlength{\sectionReduceBot}
\newlength{\subsectionReduceTop}
\newlength{\subsectionReduceBot}
\newlength{\abstractReduceTop}
\newlength{\abstractReduceBot}
\newlength{\captionReduceTop}
\newlength{\captionReduceBot}
\newlength{\subsubsectionReduceTop}
\newlength{\subsubsectionReduceBot}

\newlength{\eqnReduceTop}
\newlength{\eqnReduceBot}

\newlength{\horSkip}
\newlength{\verSkip}

\newlength{\figureHeight}
\setlength{\figureHeight}{1.7in}

\setlength{\horSkip}{-.09in}
\setlength{\verSkip}{-.1in}



\setlength{\subsectionReduceTop}{-0.09in}
\setlength{\subsectionReduceBot}{-0.09in}
\setlength{\sectionReduceTop}{-0.12in}
\setlength{\sectionReduceBot}{-0.12in}
\setlength{\subsubsectionReduceTop}{-0.06in}
\setlength{\subsubsectionReduceBot}{-0.05in}
\setlength{\abstractReduceTop}{-0.05in}
\setlength{\abstractReduceBot}{-0.10in}


\setlength{\eqnReduceTop}{-0.05in}
\setlength{\eqnReduceBot}{-0.05in}




\setlength{\captionReduceTop}{-0.14in}
\setlength{\captionReduceBot}{-0.12in}


\newcommand{\csection}[1]{\vspace{\sectionReduceTop}\section{#1}\vspace{\sectionReduceBot}}
\newcommand{\csubsection}[1]{\vspace{\subsectionReduceTop}\subsection{#1}\vspace{\subsectionReduceBot}}
\newcommand{\csubsubsection}[1]{\vspace{\subsubsectionReduceTop}\subsubsection{#1}\vspace{\subsubsectionReduceBot}}
\newcommand{\cabstract}[1]{\vspace{\abstractReduceTop}\begin{abstract}#1\end{abstract}\vspace{\abstractReduceBot}}


\setlength{\abovecaptionskip}{6pt plus 0pt minus 0pt}
\setlength{\textfloatsep}{12pt plus 0pt minus 0pt}

\setlength{\floatsep}{15pt plus 0pt minus 0pt} \usepackage{style/mysymbols}

\usepackage{tikzpagenodes}


\newcommand{\one}{\textbf{\textcolor[HTML]{f35400}{R1}}}
\newcommand{\two}{\textbf{\textcolor[HTML]{2980f9}{R2}}}
\newcommand{\three}{\textbf{\textcolor[HTML]{55c055}{R3}}}

\setlength{\parskip}{2pt}
\renewcommand{\xhdr}[1]{\noindent \textbf{#1}}

\begin{document}



We thank the reviewers for the thoughtful feedback! We are encouraged that all voted to accept, finding the paper clear / well-organized [\one]; our approach ``very interesting'' [\three] and novel [\two~\three]; our results significant and well-demonstrated [\one~\two]; and likely to be built on by the community [\one]. We are pleased they recognized the value of transferring visio-linguistic pretraining [\one~\two~\three] and the demonstrated benefits of our co-attentional two-stream model over a direct extension of BERT [\two~\three]. We respond to select comments below but will address all feedback.





\xhdr{Improved performance.} After submission, refined LR schedules raised performance across all tasks -- passing the recent VQA challenge winner, setting a new SoTA. Will update paper with details and release the codebase if accepted.
\begin{wrapfigure}{l}{0.42\textwidth}
\vspace{-14pt}
\includegraphics[width=1.02\linewidth]{figures/attn_viz_tall.png} 
\vspace{-20pt}
\end{wrapfigure}

\vspace{-10pt}\xhdr{[\one] Visualization of coattention over multimodal inputs.} Visualizing BERT-style models is an open research area; we applied the method of [Vig.~arXiv 2019] and observed some trends -- providing representative examples on the left. 
\textbf{[Top]} We show attention for each layer (rows) and head (cols) with attention focus (colored lines) shown going from source to target (left-to-right). textimage co-attention tends to be better grounded in early layers before converging to a set of somewhat arbitrary regions as depth increases -- see attention focus concentrate more over layers. In contrast, imagetext co-attention often focuses on the high-level sentence representation in the \texttt{SEP} token already developed on the text side 
early on, but spreads out somewhat later. \textbf{[Bottom]} We also show the most attended patch for each attention head for each word in the first layer for an example. Many heads focus on a small set of ``default'' patches (faded for clarity); but, the noun phrases surrounding ``squirrel'' and ``bench'' focus more on relevant regions. 

\begin{wrapfigure}{r}{0.15\textwidth}
\vspace{-15pt}
\includegraphics[width=1\linewidth]{figures/word_freq.png} 
\vspace{-20pt}
\end{wrapfigure}
\xhdr{[\one] Additional result analysis.} We investigate the RefCOCO+ task. For each noun occurring in a referring expression, we counted the number of instances where ViLBERT (full) succeeded and ViLBERT (w/o pretrain) fails (and vice versa). The wordcloud on the right shows those nouns with the highest performance delta. We will perform more task specific task in supplementary.

\begin{wrapfigure}{r}{0.25\textwidth}
\vspace{-14pt}
\includegraphics[width=1\linewidth]{figures/examples.png} 
\vspace{-20pt}
\end{wrapfigure}\xhdr{[\two] All tasks are ``image captioning (or closely related)'' so this is ``effectively transfer learning from a large captioning dataset to a small one.''}
We respectfully disagree.
Due to its automatic collection from the web, Conceptual Captions (CC) is fairly distinct from curated vision-and-language datasets (examples right).
Even for the closely-related caption-based image retrieval task, it was not obvious to us that this weakly-aligned web data would help. Further, our other transfer tasks differ significantly from CC. 
VQA and VCR both ask grounded questions like ``Is there something to cut the vegetables with?'' (VQA) This is not caption-like and requires reasoning (knives cut) and grounding. VCR extends to answer justifications like "[Person3] is delivering food to the table, and she might not know whose order is whose" that often refer to actions and intentions of individuals.
Referring expressions focus on aligning small image regions with short focused text like ``guy in yellow dribbling ball'' -- both being quite different from whole-image descriptive captioning. \emph{However, a common need for visual grounding underpins these tasks and is precisely what we target with our pretraining strategy.}

\setlength{\tabcolsep}{4pt}
\begin{wraptable}{r}{0.21\textwidth}
\vspace{-14pt}
\footnotesize
\addtolength{\tabcolsep}{-2pt}
\begin{tabular}{l c c}
\toprule
\scriptsize Method & \scriptsize VQA & \scriptsize RefCOCO+ \\
\midrule
full & \textbf{66.59} & \textbf{70.38}\\
w/o corr & 64.85 & 68.04 \\
w/o align & 64.61 & 68.49 \\
w/o mask & 42.43 & 10.00 \\
\bottomrule
\end{tabular}
\vspace{-15pt}
\end{wraptable}
\xhdr{[\two] Additional pretraining ablations.} Great suggestions! We report separate image-text pretraining (\textbf{w/o corr} -- masking loss only and zeroed co-attn), without alignment loss (\textbf{w/o align}), and without masking loss (\textbf{w/o mask}) ablations to the right (only two tasks due to time). All ablations degrade performance -- especially \textbf{w/o mask} which struggles to train downstream tasks. These ablations are valuable and will be added to the paper.

\xhdr{[\three] Given the use of Conceptual Captions (CC), are the comparisons to baselines fair?} We believe these comparisons are fair. We agree that CC is a large, additional data source; however, being able to leverage this additional data for a diverse range of vision and language tasks is precisely our contribution! Existing approaches to vision and language tasks are simply not designed to do so -- for instance, it is unclear how to train a standard VQA model like BAN with CC captioning data. Arguing from analogy, the widespread transfer of deep models pretrained on ImageNet also leveraged more data during pretraining; however, we do not find it unfair to pre-deep learning approaches that were not equipped to leverage that data. Finally, note that unlike ImageNet, CC is webly supervised, and did not involve expensive human annotation. \sml{We acknowledge that in caption-based image retrieval, CC data could have been used to pretrain existing work for a more direct architectural comparison -- we will address.}
 


\xhdr{[\three] ``If I understood correctly, [the w/o pretrain model] does not use any visual features.} That is not the case.  Like all our models, the ``w/o pretrain'' model is initialized from a trained visual feature extractor (Faster RCNN) and language model (BERT). We use ``w/o pretrain'' to note that the model has not undergone our visio-linguistic pretraining on the CC dataset (L264). To reduce confusion, we will use ``w/o grounding pretraining'' and clarify relevant sentences.




\end{document}
