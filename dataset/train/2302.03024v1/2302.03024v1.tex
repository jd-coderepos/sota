
\documentclass{article} \usepackage{iclr2023_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}
\usepackage{wrapfig}
\usepackage{xspace}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{makecell}
\usepackage{algorithm}
\usepackage[frozencache,cachedir=.]{minted} 
\usepackage{lipsum}
\usepackage{colortbl}
\definecolor{mygray}{gray}{.93}
\usepackage{transparent}



\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}

\def\eg{\emph{e.g}\onedot} \def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot} \def\Ie{\emph{I.e}\onedot}
\def\cf{\emph{c.f}\onedot} \def\Cf{\emph{C.f}\onedot}
\def\etc{\emph{etc}\onedot} \def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\etal{\emph{et al}\onedot}

\newcommand\figcaption{\def\@captype{figure}\caption} 
\newcommand\tabcaption{\def\@captype{table}\caption}

\makeatother

\title{AIM: Adapting Image Models for Efficient Video Action Recognition}





\author{Taojiannan Yang\thanks{Work done during an internship at Amazon Web Services.}, Yi Zhu, Yusheng Xie, Aston Zhang, Chen Chen, Mu Li \\
	University of Central Florida \quad Amazon Web Services
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}
	
	\maketitle
	
	\begin{abstract}
		Recent vision transformer based video models mostly follow the ``\textit{image pre-training then finetuning}" paradigm and have achieved great success on multiple video benchmarks. 
		However, full finetuning such a video model could be computationally expensive and unnecessary, given the pre-trained image transformer models have demonstrated exceptional transferability. 
		In this work, we propose a novel method to Adapt pre-trained Image Models (AIM) for efficient video understanding.
		By freezing the pre-trained image model and adding a few lightweight Adapters, we introduce spatial adaptation, temporal adaptation and joint adaptation to gradually equip an image model with spatiotemporal reasoning capability.
		We show that our proposed AIM can achieve competitive or
		even better performance than prior arts with substantially fewer tunable parameters on four video action recognition benchmarks.
		Thanks to its simplicity, our method is also generally applicable to different image pre-trained models, which has the potential to leverage more powerful image foundation models in the future. The project webpage is \url{https://adapt-image-models.github.io/}.
\end{abstract}
	
	\section{Introduction}
The ``pre-training then finetuning'' paradigm has played an important role in computer vision. 
	The key to this paradigm is a well pre-trained image model, which can provide strong transferability to downstream tasks through finetuning. 
	Recently, large foundation models \citep{clip, yuan2021florence, tong2022videomae,align, beitv3} can even demonstrate remarkable few-/zero-shot performance  given their learned superior visual representations.
	
	In video understanding, a common practice is also bootstrapping from an image pre-trained model and then finetuning on the video data. 
There are two dominating directions as shown in Fig.\ \ref{fig:teaser}, one is to extend an image model with additional temporal module \citep{lin2019tsm,zhu2019hidden,arnab2021vivit}, the other is to inflate an image model to a video model \citep{i3d, liu2022videoswin}.
However, there exists at least two drawbacks for the aforementioned methods.
	First, most approaches require full finetuning (\ie, updating all the model parameters during training) to achieve promising results on common video benchmarks. 
	This is quite costly in terms of both computation and memory footprint, \eg, 1200 Tesla V100 GPU hours to train \citet{liu2022videoswin}.
Second, it also remains questionable that whether it is necessary to full finetune pre-trained image models given that they have demonstrated excellent transferability. An inadequate finetuning on downstream data might destroy the well generalized representations from such foundation models. 
	
	To overcome the drawbacks, a research direction termed parameter-efficient transfer learning has been trending in natural language processing (NLP) \citep{adapter, prompttuning, ben-zaken-etal-2022-bitfit,hu2022lora}. 
	The goal is to only finetune a small number of (extra) parameters while keeping large pre-trained language models \citep{devlin2018bert, gpt} frozen to attain strong performance. 
	With the rise of large vision transformer (ViT) models, such techniques have been recently introduced to computer vision for efficient transfer learning. 
	However, existing works either focus on tuning a pre-trained image model for image tasks (image-to-image) \citep{visualpixelprompt, convadapter, vpt}, or tuning a pre-trained video model for video tasks (video-to-video) \citet{chen2022adaptformer}. 
	Directly leveraging pre-trained image models for efficient transfer learning to video tasks (image-to-video) is less explored, because image models lack the capability of temporal reasoning.
	
	


	




	In this work, we introduce a new way to \textbf{A}dapt pre-trained \textbf{I}mage transformer \textbf{M}odels (AIM) for efficient video action recognition.
	By freezing the pre-trained image model and adding a few lightweight adapters \citep{adapter} during finetuning, we show that our proposed AIM can achieve competitive or even better results than previous state-of-the-art methods with substantially fewer tunable parameters (Fig.\ \ref{fig:teaser} right). 
	To be specific, we first introduce adapter after self-attention layer in a transformer block to perform \textit{spatial adaptation}. We show that a well pre-trained image model is sufficiently good for spatial modeling in video understanding.
	Then for temporal modeling, we simply reuse the image pre-trained self-attention layer but apply it to the temporal dimension of video input, forcing it to model the relationship across different frames.
	An adapter is also appended for \textit{temporal adaptation}. 
	Finally, we perform \textit{joint adaptation} by adding another adapter in parallel to the MLP layer in a transformer block.
To summarize, we make the following contributions:
	
	
	
	\begin{figure*}[t]
		\vspace{-5pt}
		\centering
		\includegraphics[width=0.95\linewidth]{fig/teaser2-L.pdf}
		\vspace{-6pt}
		\caption{\textbf{Left}: Pipeline comparison between traditional full finetuning  and our efficient finetuning. \textbf{Right}: Performance comparison on K400 dataset \citep{kay2017kinetics}. Bubble size indicates GFLOPS at inference time. Our proposed AIM achieves the highest accuracy while enjoying significantly less number of tunable parameters and GFLOPS.}
		\label{fig:teaser}
	\end{figure*}
	
\setlength{\leftmargini}{12pt}
	\begin{enumerate}
\item We propose a new way to adapt pre-trained image transformer models for efficient video understanding. Our method is generally applicable to different image pre-trained models, simple to implement, and cost-effective to train.
\item Our method is significantly more efficient than full finetuning a video model, \eg,
		on Swin-B backbone, we can reduce the memory footprint by  and training time by  compared to VideoSwin \citep{liu2022videoswin}.
		\item AIM achieves comparable or higher performance than previous full finetuned state-of-the-arts on 4 video action recognition benchmarks, \eg,  on K400 with 38M tunable parameters.
\item Our method also brings data efficiency,  \eg, AIM outperforms counterpart TimeSformer \citep{timesformer} by  absolute accuracy improvement when using   of the training data.
	\end{enumerate}
	
	\section{Related Work}
	
	\textbf{Image pre-trained models.} 
ViT \citep{vit} and its variants \citep{liu2021swin, wang2021pyramidvit, yuan2021tokenstotoken, dong2022cswin} have been proposed to achieve state-of-the-art performance on image recognition. 
	Once trained, these models could also serve as good initialization for transfer learning to downstream tasks.
	In terms of training techniques, they are commonly trained on large-scale labeled datasests \citep{deng2009imagenet,jft300m,zhai_cvpr2022_scalingvit} in a supervised manner.
To alleviate the labeling cost, self-supervised learning methods \citep{mocov3,beit,zhou2021ibot,mae,xie2022simmim} are introduced to learn effective representations from unlabeled data. 
	Recent works \citep{clip, align, yuan2021florence,beitv3} adopt large-scale multimodal data (\eg, image-text pairs) for model training, which leads to even more powerful visual representations. 
In this work, thanks to the simplicity of our proposed method, we could take advantage of these well pre-trained image models and adapt them efficiently to solve video tasks.
	
	\textbf{Video action recognition.} 
A paradigm shift from using convolutional networks \citep{i3d,r21d,yang2021mutualnet,lin2019tsm,feichtenhofer2019slowfast} to transformers has been observed for video action recognition. Most works use image pre-trained models as initialization and extend them to video models by introducing new temporal modules \citep{timesformer, arnab2021vivit, zhang2021vidtr, yan2022multiview} or inflate them to video models \citep{liu2022videoswin}. 
Another direction is to directly pre-train a video model in a self-supervised manner \citep{kuang2021video, feichtenhofer2022videomae, zolfaghari2021crossclr, tan2021vimpac}.
However, all these models are full finetuned on video data, which makes the training cost unaffordable to most researchers and practitioners. 
	There are some recent works \cite{xclip, promptclip} extending CLIP to perform action recognition, but they are multimodal methods which requires additional text branch. 
	Our proposed AIM leverages existing pre-trained image models (no need for video model pre-training), only tunes a small number of model parameters (much more efficient than full finetuning), and achieves comparable or even better performance than previous state-of-the-arts.
	


	




	\textbf{Parameter-efficient finetuning} techniques \citep{adapter,hu2022lora,prompttuning,li-liang-2021-prefix,unifiedadapter,ben-zaken-etal-2022-bitfit,fixsparsemask,qing2022mar} are first proposed in NLP since full finetuning the increasingly larger language models for various downstream tasks becomes less feasible. 
	Their goal is to reduce the number of trainable parameters thus lowering the computation cost, while reaching or surpassing the performance of full finetuning. 
Recently, parameter-efficient transfer learning is also studied in computer vision \citep{vpt,visualpixelprompt,chen2022adaptformer,convadapter,gao2022dept}. 
All these methods focus on adapting models in the same domain (\eg, image-to-image or video-to-video), while our method adapts an image model for video tasks. 
	One concurrent work \citep{frozenclip} also studies how to adapt image pre-trained models for video action recognition. However, there are several major differences. First, they add new trainable decoder branches, which consist of 3D convolutions and cross-frame attention, to the frozen image encoder. We simply reuse image pre-trained self-attention to perform temporal modeling, while enjoying better performance and less tunable parameters. Second, our method is shown to be compatible with different image models, while \citet{frozenclip} only shows its effectiveness on CLIP image encoder.


	\section{Methodology}
	\label{sec:method}
	
	In this section, we first briefly describe ViT and video baselines (Sec. \ref{subsec:preliminary}). Then we introduce spatial adaptation (Sec. \ref{subsec:spatial adapter}), temporal adaptation (Sec. \ref{subsec:temporal module}) and joint adaptation (Sec. \ref{subsec:joint module}), to show how we adapt a pre-trained image model for effective video modeling step-by-step.


	\subsection{Preliminary}
	\label{subsec:preliminary}
	Since Vision Transformer (ViT) \citep{vit} is proposed, transformer-based models have been widely adopted in various computer vision tasks, including video action recognition. 
	In this work, we focus on adapting pre-trained image transformer models and compare to full finetuned video transformer models, unless otherwise stated. 
	
	More specifically, ViT handles an image as a sequence of small patches. 
	Given input image , ViT first splits the image to  non-overlapping patches and maps each patch to a -dim patch embedding via a trainable linear projection \citep{qian2021blending,qian2022makes}. 
	Here,  is the image resolution and  is the number of channels. Patch embeddings , where  and  denotes the patch size. 
	Then a learnable [class] token is prepended to  as . 
	To encode positional information, positional embeddings  are added to  as , where  is the final input being fed to a sequence of transformer blocks.
	Each transformer block is composed of a multiheaded self-attention (MSA) and a MLP layer, together with Layernorm (LN) and skip connections, see Fig.~\ref{fig:adapter}(b). 
The computation of a standard transformer block can be written as  
	
	
	where  and  denotes the input and output of the -th transformer block. Finally, the learned [class] token  from the last transformer block is used as global visual representation and fed into a classification head to make the prediction.
	
	\textbf{Space-only and space-time models for video}.
	A video is a stack of frames with temporal structure. 
	Hence, video understanding requires the model to learn both good appearance representations in each frame (spatial modeling) and also infer the temporal structured information across frames (temporal modeling).
	In order to leverage an image transformer model for video tasks, one key thing is how to perform temporal modeling. 
	A simple baseline, termed space-only model, process each video frame independently by an image model.
	Given , where  is the number of frames, space-only model will get  [class] tokens where each [class] token stands for the representation of each frame. These  [class] tokens will be averaged as a way of temporal modeling for final prediction. 
	In order to enhance the capability of temporal modeling, recent works \citep{timesformer,arnab2021vivit,zhang2021vidtr} introduce space-time model by adding new temporal modules to image models. 
	These models are now the top performers on most video action recognition benchmarks, however, their training costs are prohibitively high due to full finetuning. 
	Given the increasingly larger but more powerful pre-trained image models, in this work, we study how to efficiently adapt them for video action recognition. 






	
	\begin{figure*}[t]
		
		\centering
		\includegraphics[width=0.85\linewidth]{fig/structure_1color.pdf}
		\vspace{-2pt}
		\caption{We show how we adapt a standard ViT block (b) for video action recognition, by gradually adding spatial adaptation (c), temporal adaptation (d) and joint adaptation (e). 
Note that S-MSA and T-MSA share weights but are applied to different input dimensions. During training, only newly added Adapters are updated while all the other layers are frozen.}
		\label{fig:adapter}
		\vspace{-20pt}
	\end{figure*}
	
	\vspace{-10pt}
	\subsection{Spatial Adaptation}
	\label{subsec:spatial adapter}
	Since image pre-trained models have been trained on large-scale datasets and demonstrated strong transferability to downstream tasks, we believe they could achieve good spatial modeling in video action recognition with minimal finetuning. 
	
	Inspired by efficient finetuning techniques \citep{adapter, prompttuning, li-liang-2021-prefix, ben-zaken-etal-2022-bitfit} in NLP, we adopt Adapter \citep{adapter} due to its simplicity. 
	As shown in Fig.\ \ref{fig:adapter}(a), Adapter is a bottleneck architecture which consists of two fully connected (FC) layers and an activation layer in the middle.
	The first FC layer projects the input to a lower dimension and the second FC layer projects it back to the original dimension.
	To adapt the pre-trained spatial features to target video data, we add an Adapter after the self-attention layer as shown in Fig.\ \ref{fig:adapter}(c), which we term as spatial adaptation.
	During training, all the other layers of the transformer model are frozen while only the Adapters are updated. 
	In Table~\ref{tab:baseline}, we show that our spatial adaptation strategy achieves comparable performance with the full finetuned space-only baseline.
	This indicates that spatial adaptation helps the frozen image model to learn good spatial representations from video data. 
	However, the overall performance after spatial adaptation still has a large gap to a full finetuned video model because spatial adaptation alone lacks the ability to learn temporal information in videos.
	
	
	\subsection{Temporal Adaptation}
	\label{subsec:temporal module}
	
	To capture temporal information more effectively, previous methods usually incorporate new temporal modules to pre-trained image models because it is commonly believed that image models cannot infer temporal structured information in videos. 
However, adding new temporal modules, either temporal attention \citep{timesformer,zhang2021vidtr} or temporal encoder/decoder \citep{arnab2021vivit,frozenclip}, will introduce sizable number of extra tunable parameters.
	In addition, these new modules require full finetuning, which is inefficient. 
	
	To address this problem, we present a new strategy: \textit{reuse the pre-trained self-attention layer in the image model to do temporal modeling}. 
	More specifically, we denote the original self-attention layer as S-MSA for spatial modeling, and the reused self-atentnion layer as T-MSA for temporal modeling. 
	As shown in Fig.\ \ref{fig:adapter}(d), we put T-MSA in front of S-MSA. 
	Now given the video patch embedding ,  we first reshape it into , where  is the number of spatial patches and  is the number of frames. 
	Then we feed  into the T-MSA where it tries to learn the relationship among the  frames. 
	Note that T-MSA and S-MSA are the same layer (i.e., pre-trained MSA in the image model) and kept frozen during model tuning, but just applied to different input dimensions. 
	This explicit operation helps our model with enhanced temporal modeling, while keeping the number of parameters fixed.
	In the end, similar to spatial adaptation, we add another Adapter after the reused temporal attention layer to adapt its features on video data, which we term as temporal adaptation.
	The structure of the Adapter is the same as in spatial adaptation but without the skip connection. 
	The reason is we want to initialize the adapted model to be close to the original model \citep{adapter}, thus we need to initialize the adapter to zero and remove the skip connection here to detach the effect of temporal adaptation at the beginning of training. 
As seen in Table~\ref{tab:baseline}, temporal adaptation helps to close the gap to full finetuned video models while only introducing another lightweight Adapter into the transformer block.
	


	\subsection{Joint Adaptation}
	\label{subsec:joint module}
	
	Spatial and temporal adaptation are performed sequentially to different input dimensions with their individual purposes. 
	It would be desirable to jointly tune the  representations for spatiotemporal reasoning. 
	To this end, we further introduce an Adapter in parallel to the MLP layer, which we term as joint adaptation. 
This Adapter has the same structure as the one in temporal adaptation.


	The final structure of a transformer block in our proposed AIM is presented in Fig.\ \ref{fig:adapter}(e). 
	The computation of the adapted block can be written as
	
	
	
	where , ,  denotes the temporal adapted, spatial adapted, and jointly adapted output in the  -th transformer block. 
	Here,  is a scaling factor to control the weight of the output from Adapter. 
	For the final prediction, we simply take the average of the [class] tokens of each input frame and feed it to the classification head.
	
	
	\section{Experiments}
	\label{sec:experiments}
	
	\textbf{Datasets.} We evaluate the proposed method on four widely adopted video action recognition benchmarks, Kinetics-400 (K400) \citep{kay2017kinetics}, Kinetics-700 (K700) \citep{k700}, Something-something-v2 (SSv2) \citep{goyal2017something} and Diving-48 \citep{diving48}. K400 contains around 240K training videos and 20K validation videos in 400 human action classes. The videos are all trimmed to around 10 seconds. K700 is an extended version of K400 which contains around 530K training videos and 34K validation videos in 700 classes. SSv2 contains 168.9K training videos and 24.7K validation videos in 174 classes. 
	SSv2 is more challenging because it requires stronger temporal modeling \citep{zhu2020videosurvey,sevilla2021only}.
Diving-48 contains 15.9K training videos and 2K validation videos in 48 fine-grained diving actions. 
	It is designed to be unbiased towards static representations, which means a model cannot simply rely on the objects or background to determine the action. 
	\vspace{-2ex}
	\subsection{Effectiveness of Components}
	\label{subsec:baseline_exp}
	\vspace{-2ex}
	
	To demonstrate the effectiveness of our proposed components in Sec.\ \ref{sec:method}, we compare our method to three baselines. 
	The first baseline is a frozen space-only model. Recall in Sec.\ \ref{subsec:preliminary}, space-only model processes input frames independently and performs temporal average pooling in the end. We freeze the image backbone and only tune the classification head, which is also known as linear probing \citep{moco}. 
	The second baseline is a full finetuned space-only model. It should be able to learn spatial information from video data, but still has difficulties in capturing temporal information. 
	The third baseline is a full finetuned space-time video model, which should serves as oracle. 
	Here we choose TimeSformer \citep{timesformer} because we are based on the same ViT-B backbone and share a similar structure (\ie, divided space-time attention).
	
	In the experiments, we use the ViT-B/16 pre-trained on IN-21K as image backbone, and we compare the proposed method with the baselines on SSv2 \citep{goyal2017something} where temporal modeling is critical. 
The results for three baselines are shown in Tab.\ \ref{tab:baseline} top. 
	We can see that the frozen space-only model only needs to  tune 0.1M parameters, but it also performs much worse than the full finetuned video model (15.1 vs 59.5). 
	Full finetuning the space-only model allows it to learn improved spatial representations from video data and largely improves the performance (15.1  36.2). 
	However, it also significantly increases the number of tunable parameters and still has a large gap from the full finetuned video model due to  lack of temporal modeling.
	The third baseline, full finetuned video model, achieves the highest accuracy due to its strong spatiotemporal reasoning capability, but the number of tunable parameters increases again to 121M.
	
	Our goal is to add a few tunable parameters to the frozen space-only model and close the gap to full finetuned video model. 
	As shown in Tab.\ \ref{tab:baseline} bottom, after spatial adaptation, the frozen space-only model achieves comparable performance with the full finetuned space-only model (36.7 vs 36.2), with significantly less number of tunable parameters (3.7M vs 86M).
	This means spatial adaptation is able to help frozen image models to achieve good spatial modeling on video data.
	In addition, adding temporal adaptation further boosts the performance to 61.2, which is even higher than the full finetuned video model. 
	This indicates that our temporal adaptation introduces strong temporal modeling to the space-only model. 
	Finally, joint adaptation is incorporated to tune the features for improved spatiotemporal reasoning, which is our method AIM. 
	We not only close the gap to full finetuned space-time video model but obtain higher accuracy (62 vs 59.5) with fewer number of tunable parameters (14.3M vs 86M).
	These results successfully validate the effectiveness of our proposed adaptation strategies. 
	
	Furthermore, our method could easily take advantage of stronger pre-trained image models and adapt them for video action recognition. 
	For example, simply switch the ViT-B/16 pre-trained on IN-21K to CLIP pre-trained, we obtain another accuracy boost (62.0  66.4)
	
	




	\begin{table}[t]
		\caption{Effectiveness of proposed components. We compare to three baselines on Something-something-v2 dataset. Spatial adaptation, temporal adaptation and joint adaptation gradually
			add spatiotemporal reasoning to the frozen image model. Views = \#frames  \#temporal  \#spatial.}
		\begin{center}
			\resizebox{\linewidth}{!}{
				\begin{tabular}{l|c|cc|cc|c}
					\hline
					Methods & Pretrain & Param (M) & \makecell{Tunable \\ Param (M)} & Top-1 & Top-5 & Views \\
					\hline
					Frozen space-only & IN-21K & 86 & 0.1 & 15.1 & 36.9 & 813 \\
					Finetuned space-only & IN-21K & 86 & 86 & 36.2 & 68.1 & 813 \\
					Finetuned space-time \citep{timesformer} & IN-21K & 121 & 121 & 59.5 & 85.6 & 813 \\
					\hline
					Frozen space-only + spatial adaptation & IN-21K & 89 & 3.7 & 36.7 & 68.3 & 813 \\
					\hspace{2.65cm} + temporal adaptation & IN-21K & 97 & 10.8 & 61.2 & 87.7 & 813 \\
					\hspace{2.65cm} + joint adaptation (AIM) & IN-21K & 100 & 14.3 & \textbf{62.0} & 87.9 & 813 \\
					\hline
					AIM & CLIP & 100 & 14.3 & \textbf{66.4} & 90.5 & 813 \\
					\hline
				\end{tabular}
			}
		\end{center}
		\label{tab:baseline}
		\vspace{-4ex}
	\end{table}
	
	\vspace{-1ex}
	\subsection{Comparisons to the State of the art}
	\vspace{-1ex}
	In this section, we compare the proposed method with state-of-the-art video models on four video action recognition benchmarks. For all the experiments, we use the ViT models pre-trained by CLIP \citep{clip}. 
	We mostly follow the training settings in \citet{liu2022videoswin}, and more implementation details can be found in Appendix.
	
	\subsubsection{Results on Kinetics-400 and Kinetics-700}
	\label{subsubsec:k400_results}


	
	Tab.\ \ref{tab:k400} presents the comparisons with state-of-the-art video models on K400 dataset. 
	First, we can see that with ViT-B/16 backbone, our method only needs to tune 11M parameters for competitive performance, which is much smaller than previous video models. 
Taking input of 8 frames as an example, AIM ViT-B/16 achieves 83.9 top-1 accuracy while only requiring 606 GFLOPs. 
	When using 16 input frames, our method even outperforms MTV-L \citep{yan2022multiview}, which requires more than 10 computations (1214 vs 18050 GFLOPs).
When switching to larger backbone ViT-L/14, we achieve the highest accuracy  on K400 dataset, with 38M tunable parameters. 
	
Note that several works also leverage CLIP pre-trained models to do video action recognition.
	However, ActionCLIP \citep{wang2021actionclip} and X-CLIP \citep{xclip} are multimodal methods which require additional text branch and tune the whole model end-to-end. PromptCLIP \citep{promptclip} applies prompt tuning \citep{prompttuning} to CLIP  and adds several temporal blocks for temporal modeling.
EVL \citep{frozenclip} introduces a new decoder branch to learn temporal information. However, AIM simply re-uses image pre-trained self-attention for temporal modeling. This makes AIM much simpler than previous methods, yet achieving better performance at much less tunable parameters. The simplicity also makes AIM much easier to adapt to different model architectures (single modal or multi-modal models). But previous methods such as ActionCLIP/X-CLIP/PromptCLIP cannot leverage pure image backbone because they need an additional text branch.


	Furthermore, we evaluate our method on K700 dataset in Tab.~\ref{tab:k700}. We can see that AIM ViT-B/16 with 11M tunable parameters is able to outperform MTV-L (875M) and MViTv2-B (51M). And AIM ViT-L/14 (38M) achieves comparable performance with MaskFeat (218M) \citep{maskfeat}. Note that MaskFeat uses larger input resolution (312 vs 224) and more input frames (40 vs 32) than us.
	This again justifies the effectiveness of our efficient adaptation pipeline. 


	
	
	\begin{table}[t]
		\caption{Comparison to state-of-the-art on Kinetics-400. Views = \#frames  \#temporal  \#spatial.}
		\vspace{-2ex}
		\begin{center}
			\resizebox{\linewidth}{!}{
				\begin{tabular}{l|c|ccc|cc|c}
					\hline
					Methods & Pretrain & GFLOPs & \makecell{Param \\ (M)} & \makecell{Tunable \\ Param (M)} & Top-1 & Top-5 & Views \\
\hline
					MViT-B \citep{mvit} & - & 4095 & 37 & 37 & 81.2 & 95.1 & 6433 \\
					UniFormer-B \citep{li2021uniformer} & IN-1K & 3108 & 50 & 50 & 83.0 & 95.4 & 3243 \\
					TimeSformer-L \citep{timesformer} & IN-21K & 7140 & 121 & 121 & 80.7 & 94.7 & 6413 \\
					ViViT-L/162 FE \citep{arnab2021vivit} & IN-21K & 3980 & 311 & 311 & 80.6 & 92.7 & 3211 \\
					VideoSwin-L \citep{liu2022videoswin} & IN-21K & 7248 & 197 & 197 & 83.1 & 95.9 & 3243 \\
					MViTv2-L () \citep{mvitv2} & IN-21K & 42420 & 218 & 218 & 86.1 & 97.0 & 3235 \\
					MTV-L \citep{yan2022multiview} & JFT & 18050 & 876 & 876 & 84.3 & 96.3 & 3243 \\
TokenLearner-L/10 \citep{ryoo2021tokenlearner} & JFT & 48912 & 450 & 450 & 85.4 & 96.3 & 6443 \\
					PromptCLIP A7 \citep{promptclip} & CLIP & - & - & - & 76.8 & 93.5 & 1651 \\
					ActionCLIP \citep{wang2021actionclip} & CLIP & 16890 & 142 & 142 & 83.8 & 97.1 & 32103 \\
					X-CLIP-L/14 \citep{xclip} & CLIP & 7890 & 420 & 420 & 87.1 & 97.6 & 843 \\
					EVL ViT-L/14 \citep{frozenclip} & CLIP & 8088 & 368 & 59 & 87.3 & - & 3231 \\
					\hline
					AIM ViT-B/16 & CLIP & 606 & 97 & 11 & 83.9 & 96.3 & 831 \\
					AIM ViT-B/16 & CLIP & 1214 & 97 & 11 & 84.5 & 96.6 & 1631 \\
					AIM ViT-B/16 & CLIP & 2428 & 97 & 11 & 84.7 & 96.7 & 3231 \\
					
					AIM ViT-L/14 & CLIP & 2802 & 341 & 38 & 86.8 & 97.2 & 831 \\
					AIM ViT-L/14 & CLIP & 5604 & 341 & 38 & 87.3 & 97.6 & 1631 \\
					AIM ViT-L/14 & CLIP & 11208 & 341 & 38 & \textbf{87.5} & \textbf{97.7} & 3231 \\
					\hline
				\end{tabular}
			}
		\end{center}
		\label{tab:k400}
		\vspace{-2ex}
	\end{table}
	
	\subsubsection{Results on Something-Something-v2}
Tab. \ref{tab:ssv2} presents the performance comparisons on SSv2. 
	Based on CLIP ViT-L/14, our method achieves competitive or better performance than most  prior arts.
	In terms of fair comparison to EVL, which also uses CLIP pre-trained image encoder, we achieve significantly higher accuracy (  ), while introducing  less tunable parameters (50M  175M). 
	Note that to introduce temporal modeling into image model, EVL adds 12 layers of decoder blocks, while our method  simply reuse image pre-trained self-attention layers to achieve stronger temporal modeling . 
	
	However, our method falls behind some full finetuned video models \citep{girdhar2022omnivore,mvitv2,li2021uniformer}. 
	One reason is that SSv2 is a ``temporal-heavy" dataset \citep{sevilla2021only}, which requires model to really understand the temporal evolution within a video.
	In order to obtain high accuracy, most previous video models are first pre-trained on some video datasets (such as K400/K600) to learn good spatiotemporal representations, then finetuned on SSv2. 
	But our method still starts from the image pre-trained model.
	Another reason is that simply reusing the image pre-trained self-attention for temporal modeling may not be able to fully capture the complicated temporal information in SSv2 videos. 
	This suggests that we need to conduct more temporal adaptation for these challenging ``temporal-heavy" datasets.
	
	
	
	\begin{table}[t]
		\caption{Comparison to state-of-the-art on Something-Something-v2. K400/K600 indicates the model is pre-trained on both IN-21K and K400/K600.}
		\vspace{-1ex}
		\begin{center}
			\resizebox{\linewidth}{!}{
				\begin{tabular}{l|c|ccc|cc|c}
					\hline
					Methods & Pretrain & GFLOPs & \makecell{Param \\ (M)} & \makecell{Tunable \\ Param (M)} & Top-1 & Top-5 & Views \\
					\hline
					TimeSformer-L \citep{timesformer} & IN-21K & 7140 & 121 & 121 & 62.4 & - & 6413 \\
					MTV-B \citep{yan2022multiview} & IN-21K & 4790 & 310 & 310 & 67.6 & 90.4 & 3243 \\
					MViT-B \citep{mvit} & K400 & 510 & 37 & 37 & 67.1 & 90.8 & 3213 \\
					MViTv2-B \citep{mvitv2} & K400 & 675 & 51 & 51 & 70.5 & 92.7 & 4013 \\
					ViViT-L/162 \citep{arnab2021vivit} & K400 & 11892 & 311 & 311 & 65.4 & 89.8 & 1643 \\
					VideoSwin-B \citep{liu2022videoswin} & K400 & 963 & 89 & 89 & 69.6 & 92.7 & 3211 \\
					Omnivore \citep{girdhar2022omnivore} & K400 & - & - & - & 71.4 & 93.5 & 3213 \\
					MViTv2-L () \citep{mvitv2} & K400 & 8484 & 213 & 213 & \textbf{73.3} & \textbf{94.1} & 3213 \\
					
UniFomer-B \citep{li2021uniformer} & K600 & 777 & 50 & 50 & 71.2 & 92.8 & 3213 \\
					CoVeR \citep{cover} & JFT-3B & - & - & - & 70.9 & - & - \\
EVL ViT-B/16 \citep{frozenclip} & CLIP & 2047 & 182 & 86 & 62.4 & - & 3213 \\
					EVL ViT-L/14 \cite{frozenclip} & CLIP & 9641 & 484 & 175 & 66.7 & - & 3213 \\
					\hline
					AIM ViT-B/16 & CLIP & 624 & 100 & 14 & 66.4 & 90.5 & 813 \\
					AIM ViT-B/16 & CLIP & 1248 & 100 & 14 & 68.1 & 91.8 & 1613 \\
					AIM ViT-B/16 & CLIP & 2496 & 100 & 14 & 69.1 & 92.2 & 3213 \\
					AIM ViT-L/14 & CLIP & 2877 & 354 & 50 & 67.6 & 91.6 & 813 \\
					AIM ViT-L/14 & CLIP & 5754 & 354 & 50 & 69.4 & 92.3 & 1613 \\
					AIM ViT-L/14 & CLIP & 11508 & 354 & 50 & 70.6 & 92.7 & 3213 \\
					\hline
				\end{tabular}
			}
		\end{center}
		\label{tab:ssv2}
		\vspace{-4ex}
	\end{table}
	
\subsubsection{Results on Diving-48}
A diving class in Diving-48 \citep{diving48} is defined by the combination of takeoff, movements in flight and entry, thus it requires the model to differentiate such fine-grained actions. 
	As shown in Tab.\ \ref{tab:diving48}, our method with 11M tunable parameters outperforms all prior methods. AIM ViT-L/14 further improves the top-1 accuracy to 90.6\%.
Comparing to ORViT \citep{orvit}, despite they leverage additional object tracking model, our method still outperforms it with much less tunable parameters. 
	This suggests that efficient finetuning can handle fine-grained action recognition. 
	
	
	\begin{figure}
		\begin{minipage}[t]{0.49\linewidth}
			\centering
			\tabcaption{Comparisons on Kinetics-700.}
\resizebox{\linewidth}{!}{
				\begin{tabular}{l|c|c|c}
					\hline
					Method & Pretrain & \makecell{Tunable \\ Param} & Top-1 \\
					\hline
					VidTR-L \citep{zhang2021vidtr} & IN-21K & 91 & 70.2  \\
					MTV-L \citep{yan2022multiview} & IN-21K & 876 & 75.2 \\
					MViTv2-B \citep{mvitv2} & - & 51 & 76.6 \\
					MViTv2-L () \citep{mvitv2} & IN-21K & 218 & 79.4 \\
					MaskFeat () \citep{maskfeat} & K700 & 218 & \textbf{80.4} \\
					\hline
					AIM ViT-B/16 & CLIP & 11 & 76.9 \\
					AIM ViT-L/14 & CLIP & 38 & \textbf{80.4} \\
					\hline
				\end{tabular}
			}
			\label{tab:k700}
		\end{minipage}\hspace{4pt}
		\begin{minipage}[t]{0.49\linewidth}
\centering
			\tabcaption{Comparisons on Diving-48.}
\resizebox{\linewidth}{!}{
				\begin{tabular}{l|c|c|c}
					\hline
					Method & Pretrain & \makecell{Tunable \\ Param} & Top-1 \\
					\hline
TimeSformer-L \citep{timesformer} & IN-21K & 121 & 81.0  \\
					VideoSwin-B \citep{liu2022videoswin} & IN-21K & 88 & 81.9 \\
					BEVT \citep{wang2022bevt} & K400 & 88 & 86.7 \\
					SIFAR-B-14 \citep{sifar} & IN-21K & 87 & 87.3 \\
					\color{gray} ORViT \citep{orvit} & \color{gray} IN-21K &  \color{gray} 160 & \color{gray} 88.0 \\
					\hline
					AIM ViT-B/16 & CLIP & 11 & 88.9 \\
					AIM ViT-L/14 & CLIP & 38 & \textbf{90.6} \\
					\hline
				\end{tabular}
			}
			\label{tab:diving48}
		\end{minipage}\vspace{-2ex}
	\end{figure}
	\vspace{-1ex}
	\section{Discussion}
	\vspace{-1ex}
\begin{wraptable}{r}{0.5\textwidth}
		\vspace{-15pt}
\caption{Performance of using different pre-trained models on K400.}
		\vspace{-10pt}
		\begin{center}
			\resizebox{0.5\textwidth}{!}{
				\begin{tabular}{l|cc|ccc|c}
					\hline
					Model & Backbone & Pretrain & \makecell{Tunable \\ Param (M)} & \makecell{Mem \\ (G)} & \makecell{Time \\ (H)} & Top-1  \\
					\hline
					TimeSformer & ViT-B & IN-21K & 121 & 10 & 20 & 78.5 \\
					AIM & ViT-B &  IN-21K & 11 & 7 & 15 & 78.8 \\
					TimeSformer & ViT-B & CLIP & 121 & 10 & 20 & 82.0 \\
					AIM & ViT-B & CLIP & 11 & 7 & 15 & 83.9 \\
					\hline
					VideoSwin-B & Swin-B & IN-21K & 88 & 18 & 64 & 82.7 \\
					AIM & Swin-B & IN-21K & 9.2 & 9 & 37 & 82.1 \\
					\hline
				\end{tabular}}
		\end{center}
		\label{tab:pretrain}
		\vspace{-8pt}
	\end{wraptable}
	\textbf{Different Pre-trained Models.} Here we demonstrate the effectiveness of AIM on different pre-trained models. In Table \ref{tab:pretrain}, we first show AIM based on ViT-B backbone. We compare AIM to TimeSformer because we use the same backbone (ViT-B) and have a similar structure (i.e., both using divided space-time attention).
	As can be seen, AIM achieves better performance than full finetuned TimeSformer under both IN-21K and CLIP pre-trained weights. Then we apply AIM to Swin-B backbone and compare it to VideoSwin when we both use Swin-B and IN-21K pre-training. Similarly, AIM achieves comparable performance with full finetuned VideoSwin.
	
	\textbf{Data Efficiency.} One advantage of our efficient tuning paradigm is that we can keep the well pre-trained image representations intact. In the scenario where downtream data is insufficient, our method will be less prone to over-fitting compared to full finetuning. 
	In Fig.\ \ref{fig:dataefficiency}, we compare AIM with full finetuned TimeSformer under different amounts of training data on K400. For fair comparison, both AIM and TimeSformer use CLIP pre-trained ViT-B/16 as backbone. 
	We can observe that under all scenarios, our method AIM outperforms full finetuned TimeSformer. 
	In particular, when the amount of data becomes less, the advantage of AIM becomes larger. 
	For example, when there is only  of training data, we  outperform TimeSformer by a significant margin of .
	
	\textbf{Training Cost.} Tab.\ \ref{tab:pretrain} also shows the training time (hours) and memory cost (GB) of our method and full finetuning on different backbones. All metrics are measured on 8 Tesla V100 GPUs. Compared to TimeSformer, we reduce the memory cost by 30\% and training time by 25\%. Compared to VideoSwin, we reduce the memory cost by 50\% and training time by 42\%.


\textbf{Position of Adapters.} By default, we add Adapters to every ViT block (12 blocks in total). Here we study the effect of adding Adapters in different layers. We add Adapters to the bottom 6 blocks (close to the input), top 6 blocks (close to the output) and one every two blocks. All these variants have the same number of tunable parameters.
	As can be seen in Tab.\ \ref{tab:positionadapter},  adding Adapters to the bottom 6 blocks yields much worse performance than others. We hypothesize that the shallow layers learn generic representations which do not need much adaptation, while deeper layers learn task-specific features like temporal information thus feature adaptation is important. 
	Adding Adapters to the top 6 blocks achieves comparable performance with adding to all blocks while saving half of the parameters. 
	This could serve as a good candidate when training resources are more limited. 


	\textbf{Bottleneck Ratio of Adapters.} 
	By tuning the bottleneck ratio of Adapters, we can easily control the number of tunable parameters. 
	Here we study how the bottleneck ratio of Adapters affects the final performance. 
	The results in Tab.\ \ref{tab:bottleneckratio} reveal that a larger bottleneck ratio tends to achieve better performance, but it will also introduce more tunable parameters. 
	The performance plateaus after bottleneck ratio goes beyond 0.25. Note that a small ratio of 0.0625 could still achieve 83.3\% top-1 accuracy on K400, which is competitive among state-of-the-art video models in Tab.\ \ref{tab:k400} while introducing only 3M tunable parameters.
	
	\begin{figure}
		\begin{minipage}[t]{0.5\linewidth}
			\vspace{3pt}
			\centering
			\includegraphics[width=\linewidth]{fig/dataefficiency.pdf}
			\vspace{-15pt}
			\figcaption{Data efficiency comparison. AIM outperforms full finetuned TimeSformer under all scenarios, especially in low data regime.}
			\label{fig:dataefficiency}
		\end{minipage}
		\hspace{0.3cm}
		\begin{minipage}[t]{0.48\linewidth}
			\vspace{-5pt}
			\centering
			\tabcaption{Effect of position of Adapters. Skip means adding Adapters every two blocks.}
			\vspace{5pt}
			\resizebox{0.7\linewidth}{!}{
				\begin{tabular}{l|c|c}
					\hline
					Position & \makecell{Tunable \\ Param (M)} & Top-1 \\
					\hline
					Bottom 6 & 5.6 & 80.7 \\
					Top 6 & 5.6 & 83.3 \\
					Skip & 5.6 & 83.2 \\
					All & 11 & \textbf{83.9} \\
					\hline
				\end{tabular}
			}
			\label{tab:positionadapter}
			\tabcaption{Effect of bottleneck ratio of Adapters.}
			\vspace{5pt}
			\resizebox{0.7\linewidth}{!}{
				\begin{tabular}{c|c|c}
					\hline
					Ratio & \makecell{Tunable \\ Param (M)} & Top-1 \\
					\hline
					0.0625 & 3 & 83.3  \\
					0.125 & 5.6 & 83.4 \\
					0.25 & 11 & \textbf{83.9} \\
					0.5 & 21 & 83.8 \\
					\hline
				\end{tabular}
			}
			\label{tab:bottleneckratio}
		\end{minipage}\vspace{-3ex}
	\end{figure}


	
\vspace{-2ex}
	\section{Conclusion}
	\vspace{-2ex}
	In this work, we propose a new way to efficiently transfer pre-trained image models for video action recognition. 
We introduce spatial adaptation, temporal adaptation and joint adaptation to gradually add spatiotemporal reasoning to an image model.
Since only newly added Adapters are updated, our training cost is substantially lower than other full finetuned video models. 
	Yet we achieve comparable or even better performance than prior arts on four benchmarks. 
	Our method is simple and generally applicable, which has the potential to leverage more powerful image foundation models in the future. 
Despite all the benefits, one limitation is that our simple strategy of reusing spatial attention for temporal modeling might not be strong enough for temporally challenging videos.
	Since video temporal modeling can be viewed as a form of sequence modeling, we might be able to reuse pre-trained weights from text or audio models instead of image models in the future. 


	
	
	




	
	


	
	\bibliography{iclr2023_conference}
	\bibliographystyle{iclr2023_conference}
	
	\appendix
	


	\section{Implementation details}
	\label{app:implementation}
	\subsection{Kinetics 400/700}
	We add spatial/temporal/joint adaptation in every ViT block as shown in Fig.\ \ref{fig:adapter}. The bottleneck ratios of all adapters are set to 0.25 and the scaling factor is set to 0.5. The first FC layer in Adapters is randomly initialized and the second FC layer is initialized to zero. In this way, the adapted model is close to the pre-trained model at the beginning of training. We largely follow the training settings and data augmentations in \citet{liu2022videoswin}. Specifically, the model is trained for 30 epochs using AdamW \citep{kingma2014adam} optimizer with a batchsize of 64. The base learning rate is 3e-4 and weight decay is 5e-2. The learning rate is warmed up from 0 in the first 3 epochs and then decays following a cosine schedule. The stochastic depth rate is 0.2 for both ViT-B and ViT-L. For inference, we sample three clips along the temporal dimension. The final performance is evaluated by the ensemble of three views. We evaluate the model on 8, 16, 32 frames and the sampling interval is 16, 8, 4, respectively.
	
	\subsection{Something-something-v2}
	We add spatial/temporal/joint adaptation in every ViT block as shown in Fig.\ \ref{fig:adapter}. We additionally add one adapter before T-MSA to enhance the temporal modeling. The bottleneck ratios of all adapters are set to 0.25 and the scaling factor is set to 0.5. We follow \citet{liu2022videoswin} to use stronger data augmentations including label smoothing, RandAugment \citep{cubuk2020randaugment} and random erasing \citep{randomerasing}. The model is trained for 50 epochs using AdamW \citep{kingma2014adam} optimizer. The other training settings are the same as Kinetics-400. We uniformly sample 8, 16, 32 frames in the experiments. For inference, we sample three spatial crops. The final performance is evaluated by the ensemble of three views.
	
	\subsection{Diving-48}
	We add spatial/temporal/joint adaptation in every ViT block as shown in Fig.\ \ref{fig:adapter}. The bottleneck ratios of all adapters are set to 0.25 and the scaling factor is set to 0.5. The model is trained for 50 epochs. The other training settings and data augmentations are the same as K400. We uniformly sample 8, 16, 32 frames in the experiments. For inference, we only sample 1 temporal clip.
	
	\section{Visualization}
	\begin{figure}[t]
		\centering
		\includegraphics[width=0.99\linewidth]{fig/visualization.pdf}
\caption{Attention map visualizations of  AIM variants and the full finetuned TimeSformer. With the help of temporal adaptation (TA), our method is able to focus on motion salient regions which helps to make a correct prediction.}
		\label{fig:visual}
\end{figure}
	
	In this section, we present the attention map visualizations of the frozen space-only model, Spatial Adaptation (SA) model, Spatial Adaptation plus Temporal Adaptation (TA) model, and the full finetuned TimeSformer.
	
	On Fig. \ref{fig:visual} left, we visualize an action ``Brush Painting'' from Kinetics-400 dataset. 
	We can see that the attention maps of the frozen space-only model are very scattered, and it doesn't attend to the brush region in the first two frames. 
	Adding SA enhances the attention on the brush, but the model still focuses on areas that are unrelated to the action. 
	Further adding TA helps the model to learn temporal information. 
	We can see that the model now focuses more on the brush painting area, which is similar to what full finetuned TimeSformer does. 
	
	On Fig. \ref{fig:visual} right, we visualize an action ``Something falling like a rock'' from Something-Something-v2 dataset. 
To correctly recognize this action, the model needs to learn how the object moves in the input frames. 
	We first observe that both the frozen space-only model and SA model have good attention on the object, but they fail to model the movement of the object which leads to wrong prediction. 
	In contrast, TA helps the model to learn the relationship among input frames.
The attention map shows that the model not only focuses on the object but also learns the track of the object. 
	Instead, TimeSformer always attends to the bottom region without showing the object path. 


	\section{Per-class analysis}
	\begin{figure}
		\centering
		\includegraphics[width=0.75\linewidth]{fig/classaccuracy.pdf}
\caption{The figure shows the differences of each class's accuracy of AIM and TimeSformer on Something-Something-v2. Here we only plot the top-5 and bottom-5 classes.}
		\label{fig:classaccuracy}
\end{figure}
	
	In Tab. \ref{tab:ssv2}, we show that AIM still falls behind some SoTA full finetuned video models on the ``temporal-heavy'' Something-Something-v2 (SSv2) dataset. We conjecture one reason is that simply reusing the image pre-trained self-attention for temporal modeling may not be able to fully capture the complicated temporal information in some nuanced action classes in SSv2. 
	To provide further insights, we compute the per-class accuracy differences of AIM and TimeSformer on SSv2 and show the top-5 and bottom-5 classes in Fig. \ref{fig:classaccuracy}. 
	We can see that the classes where AIM performs better are normal action classes with decent motion. 
	The classes where AIM performs worse are those with minor differences (\eg, ``Pulling something from left to right'' vs. ``Pulling something from right to left''). 
	In order to tell these actions apart, the model needs to distinguish between the nuances, especially in motion. 
	Given most of model parameters are frozen in our method, AIM may lack the capacity to capture such complex temporal information.
	
	\section{More comparisons of training cost}
	In Table \ref{tab:pretrain}, we demonstrate the training efficiency of AIM based on ViT-B and Swin-B backbones. In this section, we show more comparisons with full finetuned baselines based on ViT-L and Swin-L backbones. The results are shown in Table \ref{tab:memorycost}. We can see that TimeSformer with a ViT-L backbone needs 21.2G GPU memory, and VideSwin with a Swin-L backbone cannot fit into an 8 Tesla V100 32G GPU server. In both cases, AIM can significantly reduce the memory usage to 14.3G and 13.7G, respectively. This makes large model training more memory-friendly (runnable on most GPUs with 15G memory and more) , and thus more affordable for most researchers and practioners.
	
	Furthermore, beyond memory saving, optimizing number of tunable parameters has potential benefits in other applications such as communication-efficient distributed learning (e.g., federated learning where the tunable model parameters are communicated between the central server and local clients) and privacy preserving federated learning \cite{fl1, fl2}. Tuning less parameters could also be beneficial when the downstream data is limited because fully finetuning a large model on limited data may suffer from serious overfitting.  This can be observed from the results in Fig. \ref{fig:dataefficiency}  where AIM obtains larger accuracy improvements over the full finetuned baseline when there is only small amount of training data.
	
	
	\begin{table}[t]
		\caption{Comparisons of the training memory cost of AIM and full finetuned models based on large image pre-trained backbones. AIM significantly reduces the memory cost and makes large model training easier.}
		\begin{center}
			\begin{tabular}{l|cc}
				\hline
				Model & Backbone &  Mem (G)   \\
				\hline
				TimeSformer \cite{timesformer} & ViT-L & 21.2 \\
				AIM & ViT-L & 14.3 \\
				\hline
				VideoSwin \cite{liu2022videoswin} & Swin-L & Out of Memory \\
				AIM & Swin-L & 13.7 \\
				\hline
			\end{tabular}
		\end{center}
		\label{tab:memorycost}
	\end{table}
	
	
	\section{Comparison to EVL under different pre-trained datasets}
	In this section, we compare AIM with EVL \cite{frozenclip}, which is the most recent SoTA image-to-video efficient finetuning method based on frozen pre-trained ViT. As shown in the Table \ref{tab:compareevl}, AIM consistently outperforms EVL under both IN-21K and CLIP pre-training as well. And AIM uses considerably less tunable number of parameters than EVL.
	
	
	\begin{table}[t]
		\caption{Comparisons with EVL under different pre-trained datasets. AIM outperforms EVL under different pre-training and uses less number of tunable parameters.}
		\begin{center}
			\begin{tabular}{l|cc|cccc}
				\hline
				Model & Backbone & Pretrain & \makecell{Tunable \\ Param (M)} &  Mem (G) & Time (H) & Top-1  \\
				\hline
				EVL \cite{frozenclip} & ViT-B & IN-21K & 36.3 & 4.2 & 29 & 75.4 \\
				AIM & ViT-B & IN-21K & 11 & 7 & 15 & \textbf{78.8} \\
				\hline
				EVL \cite{frozenclip} & ViT-B & CLIP & 36.3 & 4.2 & 29 & 82.9 \\
				AIM & ViT-B & CLIP & 11 & 7 & 15 & \textbf{83.9} \\
				\hline
			\end{tabular}
		\end{center}
		\label{tab:compareevl}
	\end{table}
	
	\section{Pseudo-code of the adapted ViT block}
	As explained in the paper, AIM is effective and simple to implement. In Algorithm \ref{alg:aim}, we show the PyTorch style pseudo-code on how to apply AIM to a ViT block.
	
	\begin{algorithm}[h!]
		\caption{Pseudo-code of an adapted ViT block}
		\label{alg:aim}
		\begin{minted}[fontsize=\footnotesize]{python}
			class TransformerBlock():
			
			def __init__(self, dim, num_head, mlp_ratio, scale):
			## Layers in the original ViT block
			self.attn = MultiheadAttention(dim, num_head)
			self.norm1 = LayerNorm(dim)
			self.mlp = MLP(dim, mlp_ratio)
			self.norm2 = LayerNorm(dim)
			
			## Adapters
			self.s_adapter = Adapter(dim)
			self.t_adapter = Adapter(dim)
			self.mlp_adapter = Adapter(dim)
			self.scale = scale
			
			def forward(x):
			## x in shape [N+1, BT, D]
			
			## temporal adaptation
			xt = rearrange(x, 'n (b t) d -> t (b n) d', t=num_frames)
			xt = self.t_adapter(self.attn(self.norm1(x)))
			xt = rearrange(x, 't (b n) d -> n (b t) d', n=num_patches)
			x = x + xt
			
			## spatial adaptation
			x = x + self.s_adapter(self.attn(self.norm1(x)))
			
			## joint adaptation
			x_norm = self.norm2(x)
			x = x + self.mlp(x_norm) + self.scale * self.mlp_adapter(x_norm)
			
			return x
			
		\end{minted}
	\end{algorithm}
	
	
\end{document}
