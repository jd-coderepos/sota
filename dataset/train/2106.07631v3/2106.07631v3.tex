\documentclass{article}







\usepackage[nonatbib,final]{neurips_2021}



\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         \usepackage{makecell}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{pifont}
\usepackage{enumitem}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{listings}

\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}

\newcommand{\app}[1]{#1}
\newcommand{\todoX}[1]{\textcolor{blue}{#1}}
\newcommand{\p}[1]{\textbf{#1.}}






\title{Improved Transformer for High-Resolution GANs}



\author{
Long Zhao\thanks{This work was done while Long Zhao was a student researcher at the Google Brain team. Correspondence to: Long Zhao (\href{mailto:lz311@cs.rutgers.edu}{lz311@rutgers.edu}) and Han Zhang (\href{mailto:zhanghan@google.com}{zhanghan@google.com}).}
\quad 
Zizhao Zhang 
\quad 
Ting Chen 
\quad
Dimitris~N.~Metaxas
\quad
Han Zhang  \\
 Rutgers University \quad Google Cloud AI \quad Google Research\\
}

\begin{document}

\maketitle

\begin{abstract}


  Attention-based models, exemplified by the Transformer, can effectively model long range dependency, but suffer from the quadratic complexity of self-attention operation, making them difficult to be adopted for high-resolution image generation based on Generative Adversarial Networks (GANs). In this paper, we introduce two key ingredients to Transformer to address this challenge. First, in low-resolution stages of the generative process, standard global self-attention is replaced with the proposed multi-axis blocked self-attention which allows efficient mixing of local and global attention. Second, in high-resolution stages, we drop self-attention while only keeping multi-layer perceptrons reminiscent of the implicit neural function. To further improve the performance, we introduce an additional self-modulation component based on cross-attention. The resulting model, denoted as HiT, has a nearly linear computational complexity with respect to the image size and thus directly scales to synthesizing high definition images. We show in the experiments that the proposed HiT achieves state-of-the-art FID scores of 30.83 and 2.95 on unconditional ImageNet  and FFHQ , respectively, with a reasonable throughput. We believe the proposed HiT is an important milestone for generators in GANs which are completely free of convolutions. Our code is made publicly available at \url{https://github.com/google-research/hit-gan}.
\end{abstract} 
\section{Introduction}

Attention-based models demonstrate notable learning capabilities for both encoder-based and decoder-based architectures~\cite{wang2018non,zhang2019self} due to their self-attention operations which can capture long-range dependencies in data. Recently, Vision Transformer~\cite{dosovitskiy2021image}, one of the most powerful attention-based models, has achieved a great success on encoder-based vision tasks, specifically image classification~\cite{dosovitskiy2021image,touvron2020training}, segmentation~\cite{liu2021swin,wang2021max}, and vision-language modeling~\cite{radford2021learning}. However, applying the Transformer to image generation based on Generative Adversarial Networks (GANs) is still an open problem.





The main challenge of adopting the Transformer as a decoder/generator lies in two aspects. On one hand, the quadratic scaling problem brought by the self-attention operation becomes even worse when generating pixel-level details for high-resolution images. For example, synthesizing a high definition image with the resolution of  leads to a sequence containing around one million pixels in the final stage, which is unaffordable for the standard self-attention mechanism. On the other hand, generating images from noise inputs poses a higher demand for spatial coherency in structure, color, and texture than discriminative tasks, and hence a more powerful yet efficient self-attention mechanism is desired for decoding feature representations from inputs.

In view of these two key challenges, we propose a novel Transformer-based decoder/generator in GANs for high-resolution image generation, denoted as HiT.
HiT employs a hierarchical structure of Transformers and divides the generative process into low-resolution and high-resolution stages, focusing on feature decoding and pixel-level generating, respectively. Specifically, its low-resolution stages follow the design of Nested Transformers~\cite{zhang2021aggregating} but enhanced by the proposed multi-axis blocked self-attention to better capture global information. Assuming that spatial features are well decoded after low-resolution stages, in high-resolution stages, we drop all self-attention operations in order to handle extremely long sequences for high definition image generation. The resulting high-resolution stages of HiT are built by multi-layer perceptrons (MLPs) which have a linear complexity with respect to the sequence length. Note that this design aligns with the recent findings~\cite{tolstikhin2021mlp,touvron2021resmlp} that pure MLPs manage to learn favorable features for images, but it 
simply reduces to an implicit neural function~\cite{mescheder2019occupancy,mildenhall2020nerf,park2019deepsdf} in the case of generative modeling. To further improve the performance, we present an additional cross-attention module that acts as a form of self-modulation~\cite{chen2019self}. In summary, this paper makes the following contributions:

\begin{itemize}[leftmargin=7.5mm]
    \setlength\itemsep{0.3em}
    \item We propose HiT, a Transformer-based generator for  high-fidelity image generation. Standard self-attention operations are removed in the high-resolution stages of HiT, reducing them to an implicit neural function. The resulting architecture easily scales to high definition image synthesis (with the resolution of ) and has a comparable throughput to StyleGAN2~\cite{karras2020analyzing}.
    
    \item To tame the quadratic complexity and enhance the representation capability of self-attention operation in low-resolution stages, we present a new form of sparse self-attention operation, namely multi-axis blocked self-attention. It captures local and global dependencies within non-overlapping image blocks in parallel by attending to a single axis of the input tensor at a time, each of which uses a half of attention heads. The proposed multi-axis blocked self-attention is efficient, simple to implement, and yields better performance than other popular self-attention operations~\cite{liu2021swin,vaswani2021scaling,zhang2021aggregating} working on image blocks for generative tasks.
    
    \item In addition, we introduce a cross-attention module performing attention between the input and intermediate features. This module  re-weights intermediate features of the model as a function of the input, playing a role as self-modulation~\cite{chen2019self}, and provides important global information to high-resolution stages where self-attention operations are absent.
    
    \item The proposed HiT obtains competitive FID~\cite{heusel2017gans} scores of 30.83 and 2.95 on unconditional ImageNet~\cite{russakovsky2015imagenet}  and FFHQ~\cite{karras2020analyzing} , respectively, highly reducing the gap between ConvNet-based GANs and Transformer-based ones. We also show that HiT not only works for GANs but also can serve as a general decoder for other models such as VQ-VAE~\cite{van2017neural}. Moreover, we observe that the proposed HiT can obtain more performance improvement from regularization than its ConvNet-based counterparts. To the best of our knowledge, these are the best reported scores for an image generator that is completely free of convolutions, which is an important milestone towards adopting Transformers for high-resolution generative modeling.
\end{itemize} \section{Related work}

\p{Transformers for image generation} There are two main streams of image generation models built on Transformers~\cite{vaswani2017attention} in the literature. One stream of them~\cite{esser2021taming,parmar2018image} is inspired by auto-regressive models that learn the joint probability of the image pixels. The other stream focuses on designing Transformer-based architecture for generative adversarial networks (GANs)~\cite{goodfellow2014generative}. This work follows the spirit of the second stream. GANs have made great progress in various image generation tasks, such as image-to-image translation~\cite{tian2018cr,zhao2018learning,zhao2020towards,zhu2017unpaired} and text-to-image synthesis~\cite{zhang2021cross,zhang2017stackgan}, but most of them depend on ConvNet-based backbones. Recent attempts~\cite{jiang2021transgan,lee2021vitgan} build a pure Transformer-based GAN by a careful design of attention hyper-parameters as well as upsampling layers. However, such a model is only validated on small scale datasets (e.g., CIFAR-10~\cite{krizhevsky2009learning} and STL-10~\cite{coates2011analysis} consisting of  images) and does not scale to complex real-world data. To our best knowledge, no existing work has successfully applied a Transformer-based architecture completely free of convolutions for high-resolution image generation in GANs.

GANformer~\cite{hudson2021generative} leverages the Transformer as a plugin component to build the bipartite structure to allow long-range interactions during the generative process, but its main backbone is still a ConvNet based on StyleGAN~\cite{karras2020analyzing}. GANformer and HiT are different in the goal of using attention modules. GANformer utilizes the attention mechanism to model the dependences of objects in a generated scene/image. Instead, HiT explores building efficient attention modules for synthesizing general objects. Our experiments demonstrate that even for simple face image datasets, incorporating the attention mechanism can still lead to performance improvement. We believe our work reconfirms the necessity of using attention modules for general image generation tasks, and more importantly, we present an efficient architecture for attention-based high-resolution generators which might benefit the design of future attention-based GANs.

\p{Attention models} Many efforts~\cite{beltagy2020longformer,ho2019axial,wang2020axial} have been made to reduce the quadratic complexity of self-attention operation. When handling vision tasks, some works~\cite{liu2021swin,vaswani2021scaling,zhang2021aggregating} propose to compute local self-attention in blocked images which takes advantage of the grid structure of images. \cite{cao2021video} also presents local self-attention within image blocks but it does not perform self-attention across blocks as in HiT. The most related works to the proposed multi-axis blocked self-attention are~\cite{ho2019axial,wang2020axial} where they also compute attention along axes. But our work differs notably in that we compute different attentions within heads on blocked images. Some other works~\cite{chen2021crossvit,hudson2021generative,jaegle2021perceiver} avoid directly applying standard self-attention to the input pixels, and perform attention between the input and a small set of latent units. Our cross-attention module differs from them in that we apply cross-attention to generative modeling designed for pure Transformer-based architectures.  

\p{Implicit neural representations} The largest popularity of implicit neural representations/functions (INRs) is studied in 3D deep learning to represent a 3D shape in a cheap and continuous way~\cite{mescheder2019occupancy,mildenhall2020nerf,park2019deepsdf}. Recent studies~\cite{anokhin2021image,dupont2021generative,lin2021infinitygan,skorokhodov2021adversarial} explore the idea of using INRs for image generation, where they learn a hyper MLP network to predict an RGB pixel value given its coordinates on the image grid. Among them, \cite{anokhin2021image,skorokhodov2021adversarial} are closely related to our high-resolution stages of the generative process. One remarkable difference is that our model is driven by the cross-attention module and features generated in previous stages instead of the hyper-network presented in~\cite{anokhin2021image,skorokhodov2021adversarial}. \section{Approach}





\subsection{Main Architecture}



\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{figs/architecture.pdf}
  \vspace{-0.8em}
  \caption{\textbf{HiT architecture}. In each stage, input the feature is first organized into blocks (denoted as ). HiT's low-resolution stages follow the decoder design of Nested Transformer~\cite{zhang2021aggregating} which is then enhanced by the proposed multi-axis blocked self-attention. We drop self-attention operations in the high-resolution stages, resulting in implicit neural functions. The model is further boosted by cross-attention modules which allow intermediate features to be modulated directly by the input latent code. The detailed algorithm can be found in \app{Algorithm~\ref{alg:app:hit} in Appendix~\ref{sec:app:implementation}}.}
  \label{fig:architecture}
\end{figure}

In the case of unconditional image generation, HiT takes a latent code  as input and generates an image of the target resolution through a hierarchical structure. The latent code is first projected into an initial feature with the spatial dimension of  and channel dimension of . 
During the generation process, we gradually increase the spatial dimension of the feature map while reducing the channel dimension in multiple stages. We divide the generation stages into low-resolution stages and high-resolution stages to balance feature dependency range in decoding and computation efficiency. The overview of the proposed method is illustrated in Figure~\ref{fig:architecture}. 



In low-resolution stages, we allow spatial mixing of information by efficient attention mechanism. We follow the decoder form of Nested Transformer~\cite{zhang2021aggregating} where in each stage, the input feature is first divided into non-overlapping blocks where each block can be considered as a local patch. After being combined with learnable positional encoding, each block is processed independently via a shared attention module. We enhance the local self-attention of Nested Transformer by the proposed multi-axis blocked self-attention that can produce a richer feature representation by explicitly considering local (within blocks) as well as global (across blocks) relations. We denote the overall architecture of these stages as multi-axis Nested Transformer.

Assuming that spatial dependency is well modeled in the low-resolution stages, the high-resolution stages can focus on synthesizing pixel-level image details purely based on the local features. Thus in the high-resolution stage, we remove all self-attention modules and only maintain MLPs which can further reduce computation complexity. The resulting architecture in this stage can be viewed as an implicit neural function conditioned on the given latent feature as well as positional information.

We further enhance HiT by adding a cross-attention module at the beginning of each stage which allows the network to directly condition the intermediate features on the initial input latent code. This kind of self-modulation layer leads to improved generative performance, especially when self-attention modules are absent in high-resolution stages. In the following sections, we provided detailed descriptions for the two main architectural components of HiT: (i) multi-axis blocked self-attention module and (ii) cross-attention module, respectively. 

\subsection{Multi-Axis Blocked Self-Attention}

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{figs/multi-axis.pdf}
  \vspace{-0.8em}
  \caption{\textbf{Multi-axis self-attention architecture.} The different stages of multi-axis self-attention for a  input with the block size of . The input is first blocked into  non-overlapping  patches. Then regional and dilated self-attention operations are computed along two different axes, respectively, each of which uses a half of attention heads. The attention operations run in parallel for each of the tokens and their corresponding attention regions, illustrated with different colors. The spatial dimensions after attention are the same as the input image.}
  \label{fig:multi-axis}
\end{figure}

Different from the blocked self-attention in Nested Transformer~\cite{zhang2021aggregating}, the proposed multi-axis blocked self-attention performs attention on more than a single axis. The attentions performed in two axes correspond to two forms of sparse self-attention, namely regional attention and dilated attention. Regional attention follows the spirit of blocked local self-attention~\cite{liu2021swin,vaswani2021scaling} where tokens attend to their neighbours within non-overlapped blocks. To remedy the loss of global attention, dilated attention captures long-range dependencies between tokens across blocks: it subsamples attention regions in a manner similar to dilated convolutions with a fixed stride equal to the block size.  Figure~\ref{fig:multi-axis} illustrates an example of these two attentions. 

To be specific, given an input of image with size , it is blocked into a tensor  of the shape  representing  non-overlapping blocks each with the size of . Dilated attention mixes information along the first axis of  while keeping information along other axes independent; regional attention works in an analogous manner over the second axis of . They are straightforward to implement: attention over the -th axis of  can be implemented by einsum operation which is available in most deep learning libraries. 



We mix regional and dilated attention in a single layer by computing them in parallel, each of which uses a half of attention heads. Our method can be easily extended to model more than two axes by performing attention on each of the axis in parallel. Axial attention~\cite{ho2019axial,huang2019ccnet,wang2020axial} can be viewed as a special case of our method, where the blocking operation before attention is removed. However, we find blocking is the key to achieve significant performance improvement in the experiment. This is because the blocked structure reveals a better inductive bias for images. Compared with \cite{ho2019axial,liu2021swin,wang2020axial} where different attention modules are interleaved in consecutive layers, our approach aggregates local and global information in a single round, which is not only more flexible for architecture design but also shown to yield better performance than interleaving in our experiment.

\p{Balancing attention between axes} In each multi-axis blocked self-attention module, the input feature is blocked in a balanced way such that we have . This ensures that regional and dilated attention is computed on an input sequence of a similar length, avoiding half of the attention heads are attended to a too sparse region. In general, performing dot-product attention between two input sequences of length  requires a total of  computation. When computing the balanced multi-axis blocked attention on an image with the block size of , i.e., , we perform attention on  sequences of length , which is a total of  computation. This leads to an  saving in computation over standard self-attention.



\subsection{Cross-Attention for Self-Modulation}

To further improve the global information flow, we propose to let the intermediate features of the model directly attend to a small tensor projected from the input latent code. This is implemented via a cross-attention operation and can be viewed as a form of self-modulation~\cite{chen2019self}. The proposed technique has two benefits. First, as shown in~\cite{chen2019self}, self-modulation stabilizes the generator towards favorable conditioning values and also appears to improve mode coverage. Second, when self-attention modules are absent in high-resolution stages, attending to the input latent code provides an alternative way to capture global information when generating pixel-level details.

Formally, let  be the first-layer feature representation of the -th stage. The input latent code  is first projected into a 2D spatial embedding  with the resolution of  and dimension of  by a linear function.  is then treated as the query and  as the key and value. We compute their cross-attention following the update rule: , where  represents the standard mulit-head self-attention,  is the output, and  is the learnable positional encoding having the same shape as . Note that  is shared across all stages. For an input feature with the sequence length of , the embedding size is a pre-defined hyperparameter far less than  (i.e., ). Therefore, the resulting cross-attention operation has linear complexity .

In our initial experiments, we find that compared with cross-attention, using AdaIN~\cite{huang2017arbitrary} and modulated layers~\cite{karras2020analyzing} for Transformer-based generators requires much higher memory cost during model training, which usually leads to out-of-memory errors when the model is trained for generating high-resolution images. As a result, related work like ViT-GAN~\cite{lee2021vitgan}, which uses AdaIN and modulated layers, can only produce images up to the resolution of . Hence, we believe cross-attention is a better choice for high-resolution generators based on Transformers. \section{Experiments}

\subsection{Experiment Setup}

\p{Datasets} We validate the proposed method on three datasets: ImageNet~\cite{russakovsky2015imagenet}, CelebA-HQ~\cite{karras2018progressive}, and FFHQ~\cite{karras2020analyzing}. ImageNet (LSVRC2012)~\cite{russakovsky2015imagenet} contains roughly 1.2 million images with 1000 distinct categories and we down-sample the images to  and  by bicubic interpolation. We use random crop for training and center crop for testing. This dataset is challenging for image generation since it contains samples with diverse object categories and textures. We also adopt ImageNet as the main test bed during the ablation study.

CelebA-HQ~\cite{karras2018progressive} is a high-quality version of the CelebA dataset~\cite{liu2015faceattributes} containing 30,000 of the facial images at  resolution. To align with~\cite{karras2018progressive}, we use these images for both training and evaluation. FFHQ~\cite{karras2020analyzing} includes vastly more variation than CelebA-HQ in terms of age, ethnicity and image background, and also has much better coverage of accessories such as eyeglasses, sunglasses, and hats. This dataset consists of 70,000 high-quality images at  resolution, out of which we use 50,000 images for testing and train models with all images following~\cite{karras2020analyzing}. We synthesize images on these two datasets with the resolutions of  and . 

\p{Evaluation metrics} We adopt the Inception Score (IS)~\cite{salimans2016improved} and the Fr\'echet Inception Distance (FID)~\cite{heusel2017gans} for quantitative evaluation. Both metrics are calculated based on a pre-trained Inception-v3 image classifier~\cite{szegedy2016rethinking}. Inception score computes KL-divergence between the real image distribution and the generated image distribution given the pre-trained classifier. Higher inception scores mean better image quality. FID is a more principled and comprehensive metric, and has been shown to be more consistent with human judgments of realism~\cite{heusel2017gans,zhang2021cross}. Lower FID values indicate closer distances between synthetic and real data distributions. In our experiments, 50,000 samples are randomly generated for each model to calculate the inception score and FID on ImageNet and FFHQ, while 30,000 samples are produced for comparison on CelebA-HQ. Note that we follow~\cite{salimans2016improved} to split the synthetic images into groups (5000 images per group) and report their averaged inception score.

\subsection{Implementation Details}

\p{Architecture configuration} In our implementation, HiT starts from an initial feature of size  projected from the input latent code. We use pixel shuffle~\cite{shi2016real} for upsampling the output of each stage, as we find using nearest neighbors leads to model failure which aligns with the observation in Nested Transformer~\cite{zhang2021aggregating}. The number of low-resolution stages is fixed to be 4 as a good trade-off between computational speed and generative performance. For generating images larger than the resolution of , we scale HiT to different model capacities -- small, base, and large, denoted as HiT-\{S, B, L\}. We refer to \app{Table~\ref{tbl:app:architecture} in Appendix~\ref{sec:app:architecture}} for their detailed architectures.

It is worth emphasizing that the flexibility of the proposed multi-axis blocked self-attention makes it possible for us to build smaller models than \cite{liu2021swin}. This is because they interleave different types of attention operations and thus require the number of attention blocks in a single stage to be even (at least 2). In contrast, our multi-axis blocked self-attention combines different attention outputs within heads which allows us to use an arbitrary number (e.g., 1) of attention blocks in a model.

\p{Training details} In all the experiments, we use a ResNet-based discriminator following the architecture design of~\cite{karras2020analyzing}. Our model is trained with a standard non-saturating logistic GAN loss with  gradient penalty~\cite{mescheder2018training} applied to the discriminator.  penalty penalizes the discriminator for deviating from the Nash-equilibrium by penalizing the gradient on real data alone. The gradient penalty weight is set to 10. Adam~\cite{kingma2014adam} is utilized for optimization with  and . The learning rate is 0.0001 for both the generator and discriminator. All the models are trained using TPU for one million iterations on ImageNet and 500,000 iterations on FFHQ and CelebA-HQ. We set the mini-batch size to 256 for the image resolution of  and  while to 32 for the resolution of . To keep our setup simple, we do not employ any training tricks for GAN training, such as progressive growing, equalized learning rates, pixel normalization, noise injection, and mini-batch standard deviation that recent literature demonstrate to be cruicial for obtaining state-of-the-art results~\cite{karras2018progressive,karras2020analyzing}. When training on FFHQ and CelebA-HQ, we utilize balanced consistency regularization (bCR)~\cite{zhang2020consistency,zhao2021improved} for additional regularization where images are augmented by flipping, color, translation, and cutout as in~\cite{zhao2020differentiable}. bCR enforces that for both real and generated images, two sets of augmentations applied to the same input image should yield the same output. bCR is only added to the discriminator with the weight of 10. We also decrease the learning rate by half to stabilize the training process when bCR is used. 

\subsection{Results on ImageNet}

\begin{table}[t]
\begin{minipage}[t]{.52\linewidth}
  \caption{Comparison with the state-of-the-art methods on the ImageNet  dataset.  is based on a supervised pre-trained ImageNet classifier.}
  \label{tbl:imagenet}
  \centering
  \begin{tabular}{l|cc}
    \toprule
    Method & FID  & IS \\
    \midrule
    Vanilla GAN~\cite{goodfellow2014generative} & 54.17 & 14.01\\
    PacGAN2~\cite{lin2018pacgan} & 57.51 & 13.50\\
    MGAN~\cite{hoang2018mgan} & 58.88 & 13.22\\
    Logo-GAN-AE~\cite{sage2018logo} & 50.90	& 14.44\\
    Logo-GAN-RC~\cite{sage2018logo} & 38.41 & 18.86\\
    SS-GAN (sBN)~\cite{chen2019rotation} & 43.87 & -\\
    Self-Conditioned GAN~\cite{liu2020diverse} & 40.30 & 15.82\\
    \midrule
    ConvNet- & 37.18 & 19.55\\
    HiT (Ours) & \textbf{30.83} & \textbf{21.64}\\
    \bottomrule
  \end{tabular}
\end{minipage}
\hfill
\begin{minipage}[t]{.44\linewidth}
  \caption{Reconstruction FID on the ImageNet  dataset. We note that VQ-VAE-2 utilizes a hierarchical organization of VQ-VAE and thus has two codebooks .}
  \label{tbl:vqvae}
  \centering
  \vspace{.22em}
  \begin{tabular}{l|c|c}
    \toprule
    Method & \makecell{Embedding\\size and } & FID \\
    \midrule
    VQ-VAE~\cite{van2017neural} & 32, 1024 & 75.19\\
    DALL-E~\cite{ramesh2021zero} & 32, 8192 & 34.30\\
    \midrule
    VQ-VAE-2~\cite{razavi2019generating} & \makecell{64, 512\\32, 512} & 10.00\\
    \midrule
    VQGAN~\cite{esser2021taming} & 16, 1024 & 8.00\\
    \midrule
    VQ-HiT (Ours) & 16, 1024 & \textbf{6.37}\\
    \bottomrule
  \end{tabular}
\end{minipage}
\end{table}

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{figs/fig_imagenet.jpg}
  \vspace{-0.8em}
  \caption{Unconditional image generation results of HiT trained on ImageNet .}
  \label{fig:imagenet}
\end{figure}

\p{Unconditional generation} We start by evaluating the proposed HiT on the ImageNet  dataset, targeting the unconditional image generation setting for simplicity. In addition to recently reported state-of-the-art GANs, we implement a ConvNet-based generator following the widely-used architecture from~\cite{zhang2019self} while using the exactly same training setup of the proposed HiT (e.g., losses and  gradient penalty) denoted as ConvNet- for a fair comparison. The results are shown in Table~\ref{tbl:imagenet}. We can see that HiT outperforms the previous ConvNet-based methods by a notable margin in terms of both IS and FID scores. Note that as reported in~\cite{donahue2019large}, BigGAN~\cite{brock2019large} achieves 30.91 FID on unconditional ImageNet, but its model is far larger (more than 70M parameters) than HiT (around 30M parameters). Therefore, the results are not directly comparable. Even though, HiT (30.83 FID) is slightly better than BigGAN and achieves the state of the art among models with a similar capacity as shown in Table~\ref{tbl:imagenet}. We also note that \cite{chen2019self,liu2020diverse} leverage auxiliary regularization techniques and our method is complementary to them. Examples generated by HiT on ImageNet are shown in Figure~\ref{fig:imagenet}, from which we observe nature visual details and diversity.

\p{Reconstruction} We are also interested in the reconstruction ability of the proposed HiT and evaluate by employing it as a decoder for the vector quantised variational auto-encoder (VQ-VAE~\cite{van2017neural}), a state-of-the-art approach for visual representation learning. In addition to the reconstruction loss and adversarial loss, our HiT-based VQ-VAE variant, namely VQ-HiT, is also trained with the perceptual loss~\cite{johnson2016perceptual} following the setup of~\cite{esser2021taming,zhu2020domain}. Please refer to \app{Appendix~\ref{sec:app:vqhit}} for more details on the architecture design and model training. We evaluate the metric of reconstruction FID on ImageNet  and report the results in Table~\ref{tbl:vqvae}. Our VQ-HiT attains the best performance while providing significantly more compression (i.e., smaller embedding size and fewer number of codes in the codebook ) than~\cite{ramesh2021zero,razavi2019generating,van2017neural}.

\begin{table}[t]
\caption{Comparison with the state-of-the-art methods on CelebA-HQ (\textbf{left}) and FFHQ (\textbf{right}) with the resolutions of  and .  bCR is not applied at the  resolution.}
\label{tbl:facehq}
\centering
\begin{minipage}[t]{.48\linewidth}
  \centering
  \begin{tabular}{l|cc}
    \toprule
    & \multicolumn{2}{c}{FID  (CelebA-HQ)}\\
    Method &  & \\
    \midrule
    VAEBM~\cite{xiao2021vaebm} & 20.38 & -\\
    StyleALAE~\cite{pidhorskyi2020adversarial} & 19.21 & -\\
    PG-GAN~\cite{karras2018progressive} & 8.03 & -\\
    COCO-GAN~\cite{lin2019coco} & - & 9.49\\
    VQGAN~\cite{esser2021taming} & 10.70 & -\\
    StyleGAN~\cite{karras2019style} & - & \textbf{5.17}\\
    \midrule
HiT-B (Ours) & \textbf{3.39} & 8.83\\
    \bottomrule
  \end{tabular}
\end{minipage}
\hfill
\begin{minipage}[t]{.48\linewidth}
  \centering
  \begin{tabular}{l|cc}
    \toprule
    & \multicolumn{2}{c}{FID  (FFHQ)}\\
    Method &  & \\
    \midrule
    U-Net GAN~\cite{schonfeld2020u} & 7.63 & -\\
    StyleALAE~\cite{pidhorskyi2020adversarial} & - & 13.09\\
    VQGAN~\cite{esser2021taming} & 11.40 & -\\
    INR-GAN~\cite{skorokhodov2021adversarial} & 9.57 & 16.32\\
    CIPS~\cite{anokhin2021image} & 4.38 & 10.07\\
    StyleGAN2~\cite{karras2020analyzing} & 3.83 & \textbf{4.41}\\
    \midrule
HiT-B (Ours) & \textbf{2.95} & 6.37\\
    \bottomrule
  \end{tabular}
\end{minipage}
\end{table}

\begin{table}[t]
  \caption{Comparison with the main competing methods in terms of number of network parameters, throughput, and FID on FFHQ . The throughput is measured on a single Tesla V100 GPU.}
  \label{tbl:throughput}
  \centering
  \begin{tabular}{cl|cc|c}
    \toprule
    Architecture & Model & \makecell{\#params\images / sec)} & \makecell{FID \million)} & \makecell{Throughput\
\label{eq:app:attention}
\begin{split}
   y  &= x + \text{Attention}(x^{\prime}, x^{\prime}, x^{\prime}),  \; \text{where }  x^{\prime} = \text{LN}(x)\\
   x  &= y + \text{MLP}(\text{LN}(y)),
\end{split}

    \mathcal{L}_D &= -\mathbb{E}_{x \sim P_x}[ \log (D(x))] - \mathbb{E}_{z \sim P_z}[ \log (1 - D(G(z)))] + \gamma \cdot \mathbb{E}_{x \sim P_x}[ {\left\| \nabla_x D(x) \right\|}^2_2],\\
    \mathcal{L}_G &= - \mathbb{E}_{z \sim P_z}[ \log (D(G(z))) ],

    \mathcal{L}_D &= -\mathbb{E}_{x \sim P_x}[ \log (D(x))] - \mathbb{E}_{z \sim P_z}[ \log (1 - D(G(z)))] + \gamma \cdot \mathbb{E}_{x \sim P_x}[ {\left\| \nabla_x D(x) \right\|}^2_2],\\
    \mathcal{L}_{\text{VAE}} &= {\left\| x - G(E(x)) \right\|}^2_2 + \lambda_1 \cdot {\left\| F(x) - F(G(E(x))) \right\|}^2_2 - \lambda_2 \cdot \mathbb{E}_{z \sim P_z}[ \log (D(G(E(x)))) ],

where  is the input image to be reconstructed,  and  are the perceptual loss weight and discriminator loss weight, and  denotes the VGG feature extraction model. We set  and  in the experiments. Adam~\cite{kingma2014adam} is utilized for optimization with  and . The learning rate is 0.0001 for both the auto-encoder and discriminator. We set the mini-batch size to 256 and train the model for 500,000 iterations.

\subsection{More Ablation Studies}

We implement two additional variants of HiT where the model uses only blocked axial attention and Nested Transformer~\cite{zhang2021aggregating}, respectively. The results on the ImageNet  dataset are shown in Table~\ref{tbl:app:ablation}. We can observe that using only blocked multi-axis attention can still lead to significant performance improvement which demonstrates its effectiveness. We also find that incorporating cross-attention and multi-axis attention as in HiT yields much better performance than vanilla Nested Transformer.

\begin{table}[htbp]
\caption{Comparison with different variants of the proposed HiT on the ImageNet  dataset.}
\label{tbl:app:ablation}
\centering
  \begin{tabular}{l|c|cc}
    \toprule
    Method & FID  & IS \\
    \midrule
    Baseline (INR) & 56.33 & 16.19\\
    Nested Transformer~\cite{zhang2021aggregating} & 42.52 & 18.68\\
    HiT w/ only blocked multi-axis attention & 35.43 & 19.75\\
    HiT w/ only cross-attention & 35.94 & 19.42\\
    HiT (Full model) & \textbf{31.87} & \textbf{21.32}\\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{More Qualitative Results}

We include more visual results that illustrate various aspects related to generated image quality of HiT. Figure~\ref{fig:app:imagenet} shows uncurated results on the ImageNet  dataset. We randomly sample from the ConvNet baseline and the proposed HiT as qualitative examples for a side by side comparison. From the results, we can see that the samples generated by HiT exhibit much more diversities in object category, texture, color, and image background than the ConvNet baseline. Figure~\ref{fig:app:facehq} shows additional hand-picked synthetic face images illustrating the quality and diversity achievable using our method on the CelebA-HQ  and  dataset.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\linewidth]{figs/fig_supp_imagenet.jpg}
  \vspace{-0.8em}
  \caption{Uncurated ImageNet  samples from ConvNet- (\textbf{left}, FID: 37.18, IS: 19.55) and the proposed HiT (\textbf{right}, FID: 30.83, IS: 21.64). Results are randomly sampled from batches.}
  \label{fig:app:imagenet}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\linewidth]{figs/fig_supp_facehq_small.jpg}
  \vspace{-0.8em}
  \caption{Additional synthetic face images by HiT-B on CelebA-HQ  and .}
  \label{fig:app:facehq}
\end{figure}

\p{Interpolation} We conclude the visual results with the demonstration of the flexibility of HiT. As well as many other generators, the proposed HiT generators have the ability to interpolate between input latent codes with meaningful morphing. Figure~\ref{fig:app:interpolation} illustrates the synthetic face results on the CelebA-HQ  dataset. As expected, the change between the extreme images occurs smoothly and meaningfully with respect to different facial attributes including gender, expression, eye glass, and view angle.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\linewidth]{figs/fig_supp_interpolation.jpg}
  \vspace{-0.8em}
  \caption{Latent linear morphing on the CelebA-HQ  dataset between two synthetic face images -- the left-most and right-most ones. HiT-B is able to produce meaningful interpolations of facial attributes in terms of gender, expression, eye glass, and view angle (from \textbf{top} to \textbf{bottom}).}
  \label{fig:app:interpolation}
\end{figure} 
\end{document}
