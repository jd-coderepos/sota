\documentclass{article}


\PassOptionsToPackage{numbers, sort&compress}{natbib}






\usepackage[preprint]{neurips_2023}








\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         

\usepackage{multirow}
\usepackage{makecell}
\usepackage{graphics}
\usepackage[pdftex]{graphicx}
\usepackage{floatrow}
\usepackage{caption}
\usepackage{subcaption}
\newfloatcommand{capbtabbox}{table}[][\FBwidth]

 \makeatletter
\def\@fnsymbol#1{\ensuremath{\ifcase#1\or \dagger\or \ddagger\or
  \mathsection\or \mathparagraph\or \|\or **\or \dagger\dagger
  \or \ddagger\ddagger \else\@ctrerr\fi}}
\makeatother

\usepackage{multirow}
\usepackage{amsthm,amsmath,amssymb,lipsum}
\usepackage{mathrsfs}
\usepackage{enumerate}
\usepackage{colortbl}
\usepackage{enumitem}
\usepackage{wrapfig}
\usepackage{bbding}
\usepackage{pifont}
\usepackage{wasysym}
\usepackage{textcomp}

\usepackage{color}
\def\eg{{\it{e.g.}}}
\def\etal{{\it{et al.}}}
\def\ie{{\it{i.e.}}}

\definecolor{pretty-blue}{RGB}{0, 113, 188}
\definecolor{linecolor}{gray}{.90} \definecolor{linecolor2}{gray}{.93} \definecolor{linecolor1}{gray}{.97} 

\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\com}[1]{{\color{pretty-blue}\texttt{#1}}}
\newcommand\tab[1][5mm]{\hspace*{#1}}

\usepackage{float}
\usepackage{colortbl}
\usepackage[colorlinks=True,
            linkcolor=red,
            anchorcolor=blue,  
            pagebackref,
            urlcolor=black,
            citecolor=pretty-blue,
            ]{hyperref}

\title{{
Point-GCC
}: Universal Self-supervised 3D Scene Pre-training via \underline{G}eometry-\underline{C}olor \underline{C}ontrast}


\author{Guofan Fan
  \And
  Zekun Qi \\
  \And
  Wenkai Shi
  \And
  Kaisheng Ma\thanks{Corresponding Author.}
  \AND
  \normalfont  Xi'an Jiaotong University \and 
  \normalfont  Tsinghua University \\
}


\begin{document}


\maketitle


\begin{abstract}
  Geometry and color information provided by the point clouds are both crucial for 3D scene understanding. Two pieces of information characterize the different aspects of point clouds, but existing methods lack an elaborate design for the discrimination and relevance. Hence we explore a 3D self-supervised paradigm that can better utilize the relations of point cloud information. Specifically, we propose a universal 3D scene pre-training framework via \textbf{G}eometry-\textbf{C}olor \textbf{C}ontrast (Point-GCC), which aligns geometry and color information using a Siamese network. 
  To take care of actual application tasks, we design 
  (i) hierarchical supervision with point-level \textit{contrast and reconstruct} and object-level \textit{contrast} based on the novel deep clustering module to close the gap between pre-training and downstream tasks; 
  (ii) architecture-agnostic backbone to adapt for various downstream models. Benefiting from the object-level representation associated with downstream tasks, Point-GCC can directly evaluate model performance and the result demonstrates the effectiveness of our methods. Transfer learning results on a wide range of tasks also show consistent improvements across all datasets. \eg, new state-of-the-art object detection results on SUN RGB-D and S3DIS datasets. Codes will be released at \url{https://github.com/Asterisci/Point-GCC}.
\end{abstract}
\section{Introduction}
\vspace{-0.2cm}
3D Self-supervised learning (SSL) has received abundant attention recently because of remarkable improvement on various downstream tasks. 3D scene datasets are tiny compared to the 2D field because 3D point cloud labeling is time-consuming and labor-intensive, which dramatically impedes the improvements of supervised methods. 
Hence many works~\cite{PointContrast,ACT23,recon23,LANG-3D} explore pre-training models out of 3D labeled data to transfer knowledge for downstream tasks. 
The goal of self-supervised learning can be summarized as learning rich representations from unlabeled data and helping to improve performance on downstream tasks with labeled data. 
Most existing works follow the paradigm in the previous 2D field, such as contrastive learning~\cite{PointContrast,DepthContrast,CSC,Ponder} and masked autoencoder (MAE)~\cite{PointMAE,recon23,PointBERT,PointM2AE22}. 
After standing on the shoulders of giants in the 2D field, we could further see the particularity of 3D representation learning as follows:
\begin{itemize}[leftmargin=15pt, topsep=0pt, 
itemsep=0pt, parsep=2pt, partopsep=0pt
]
    \item \textbf{Unique information.} 3D scene point cloud contains various information such as geometry and color, which makes 3D point cloud data different from 2D image data. 
    Most existing methods \cite{PointContrast,CSC,DepthContrast} treat all information of each point as an entirety in model architecture design. 
    We argue that directly concatenating all information can not adapt the model to discriminately learn different aspects of point clouds. Although some works~\cite{DEMF, TokenFusion} propose the two-stream architecture that encodes point cloud by 3D network and images by 2D network, it needs extra 2D data, and 3D network can not clearly learn the discrimination between different information. Considering these additional differences may be beneficial for effective representation learning.
    \item \textbf{Mismatch between pre-training and downstream tasks.} Previous pre-training works ~\cite{PointContrast,CSC,PointMAE,PointM2AE22} design their self-supervised point-level tasks, such as contrast and reconstructing between specific points. 
    However, 3D scene downstream tasks mostly focus the object representations such as object detection and instance segmentation. The gap in supervision level between pre-training and downstream tasks may hinder the improvements of 3D self-supervised learning.
    \item \textbf{Architecture diversity.} 
    The 3D point cloud field has grown rapidly in recent years~\cite{PointNet++,MinkowskiEngine,GroupFree,FCAF3D,PointNext}, and the popular architecture appears changeable and specific for downstream tasks. Hence a universal pre-training framework is important that can implement various existing methods for all kinds of tasks and is easy to adapt for future architecture.
\end{itemize}
To mitigate the aforementioned problems, we explore a 3D self-supervised paradigm that can better utilize the relations of point cloud information. Most 3D scene datasets~\cite{dai2017scannet, song2015sun, armeni_cvpr16} provide geometry and color information, representing different aspects of the point cloud. Geometry information describes the outline of objects and can easily distinguish between them, while color information refines the internal characteristics of objects and gives a more accurate view of each object. What's more, different information has inherent relevance. For instance, we can roughly infer the geometric structure of the object from a color photo and vice versa. Motivated by the difference and relevance inherent in the information, we propose a self-supervised 3D scene pre-training framework via \textbf{G}eometry-\textbf{C}olor \textbf{C}ontrast (Point-GCC), which uses a Siamese network to extract representations and implements elaborate hierarchical supervision. To bridge the gap between pre-training and downstream tasks, the hierarchical supervision contains point-level supervision that aims to align point-wise features and object-level supervision based on a novel deep clustering module to provide better object-level representations strongly associated with downstream tasks.  
Additionally, the universal Siamese network is designed as an architecture-agnostic backbone so that various downstream models can easily be adapted in a plug-and-play way.

In extensive experiments, we directly perform a fully unsupervised semantic segmentation task without fine-tuning to evaluate the quality of the pre-training model. The result outperforms the previous method with +7.8\% mIoU on ScanNetV2, which proves that Point-GCC has learned rich object representations through our paradigm. 
Furthermore, we choose a broad downstream task to demonstrate our generality: object detection, semantic segmentation and instance segmentation on 
ScanNetV2~\cite{dai2017scannet}, SUN RGB-D~\cite{song2015sun} and S3DIS~\cite{armeni_cvpr16} datasets. 
Remarkably, our results indicate general improvements across all tasks and datasets. For example, we achieves new state-of-the-art results with 69.7\% AP, 54.0\% AP on SUN RGB-D and 75.1\% AP, 56.7\% AP on S3DIS datasets. Compared with previous pre-training methods, our method achieves higher AP by +3.1\% on ScanNetV2 and +1.1\% on SUN RGB-D.
Our contributions can be summarized as follows:
\begin{itemize}[leftmargin=15pt, topsep=0pt, 
itemsep=0pt, parsep=2pt, partopsep=0pt
]
    \item We propose a new universal self-supervised 3D scene pre-training framework, called Point-GCC, which aligns geometry and color information via a Siamese network with hierarchical supervision. To the best of our knowledge, this is the Ô¨Årst study to explore the alignment between geometry and color information of point cloud via the pre-training approach.
    \item We design a novel deep clustering module to generate object pseudo-labels based on the inherent feature consistency of the two pieces of information. The result demonstrates that Point-GCC has learned rich object representations by clustering.
    \item Extensive experiments show that Point-GCC is a general pre-training framework with an architecture-agnostic backbone, significantly improving performance on a wide range of downstream tasks and achieving new state-of-the-art on multiple datasets.
\end{itemize} \section{Related Work}
\vspace{-0.2cm}
\subsection{3D Scene Understanding}
\vspace{-0.25cm}
Most 3D scene understanding works are still specially designed for downstream tasks, such as object detection~\cite{VoteNet, GroupFree, FCAF3D, TR3D, ThreeDETR}, semantic segmentation~\cite{SL3D, WYPR, MPRM, PointNet++}, and instance segmentation~\cite{TD3D, PointGroup, SoftGroup, HAIS}. The model architecture can be summarized as a backbone module extracting the features of point clouds, and a downstream head adapting for the special task. According to the processing method, these works can be roughly divided into two categories: point-based methods and voxel-based methods. Point-based methods~\cite{VoteNet, GroupFree,SL3D} is widely used in point clouds thanks to the effectiveness of PointNet++~\citep{PointNet++}, which alternately use farthest point algorithm and multi-layer perceptron to sample and extract the features of point. Voxel-based methods~\cite{FCAF3D, TR3D,TD3D, PointGroup, SoftGroup, HAIS} is recently popular because of the better performance and efficiency on many downstream tasks than point-based methods, which operate 3D sparse convolution on regular voxels transformed from irregular point clouds. We pre-train on both point-based PointNet++ and voxel-based 3D sparse convolution backbone and fine-tune on multiple downstream methods to give a comprehensive view of our work.

\subsection{3D Self-supervised Learning}
\vspace{-0.25cm}
Compared to 2D vision or natural language, the 3D vision has a more serious problem of data scarcity~\citep{ACT23} which limits the downstream performance of 3D tasks. To solve the raising problem, 3D self-supervised learning (SSL) has gotten more attention in recent years. The mainstream SSL methods can be roughly divided into two categories: contrastive learning and reconstructive learning. Contrastive learning is motivated to learn the invariant representation from different paired carriers such as view augmentation~\citep{SimCLR, PointContrast} or different data formats~\citep{CLIP, ULIP22}. Reconstructive learning is designed to reconstruct the disturbed data to learn geometry knowledge between patches~\citep{BERT, BEiT}. Motivated by the success of masked autoencoder~\citep{MAE} in 2D, the MAE-style self-supervised method became popular in point cloud~\citep{PointMAE, PointM2AE22}. Recently, some works find that the \textit{pattern difference} between the two methods 
 in attention area~\citep{DarkMIM22} and scaling performance~\citep{recon23}. 
Based on previous work, we consider the color and geometry of scene point clouds as two views for contrastive learning, and use a \textit{swapped reconstruct} strategy for reconstructive learning. Therefore, Point-GCC achieves the integration of two methods and derives benefits from both of them.

\subsection{Deep Clustering for Self-supervised Learning}
\vspace{-0.25cm}
Deep Clustering~\citep{long2015learning, caron2018deep, chang2017deep, xie2016unsupervised, bachman2019learning, caron2020unsupervised, DeepAlignedClustering} aims to learn better features and discover data labels using deep neural networks, which has been broadly applied in self-supervised and semi-supervised learning. DeepCluster~\cite{DeepCluster} uses the off-the-shelf K-means algorithm  pseudo-labels as supervision which learns comparative representations for self-supervised learning.
SeLa~\cite{SELA} proposes a simultaneous clustering and representation learning method using the Sinkhorn-Knopp algorithm to generate pseudo-labels with equal partitions quickly. SwAV~\cite{SWAV} combines contrastive learning and deep clustering, which enforces consistency between cluster assignments from different views of the same image. In this work, we attempt to apply deep clustering in 3D self-supervised learning field, which generates pseudo-labels based on the inherent feature consistency of the geometry and color information of the point cloud. \begin{figure}[ht!]
  \begin{center}
\includegraphics[width=\linewidth]{fig/fig_overview.pdf}
  \vspace{-5pt}
  \caption{\textbf{Overview of our Point-GCC framework}. Point-GCC utilizes the Siamese network to extract the features of geometry and color with positional embedding respectively. Then we implement the hierarchical supervision on extracted features which contains point-level \textit{contrast and reconstruct} and object-level \textit{contrast} based on the deep clustering module.}\label{fig:archi}
  \end{center}
  \vspace{-25pt}
\end{figure} \section{Point-GCC: Pre-training via Geometry-Color Contrast}
\vspace{-0.2cm}
Existing methods mainly focus on geometric information, but our goal is to enhance the 3D representation capability by better utilizing all the information discriminately in scene point clouds. Therefore, a novel \textit{Geometry-Color Contrast} method is proposed to address this motivation. 
Figure \ref{fig:archi} illustrates the overall framework of Point-GCC.
We first perform a Siamese backbone to extract the features of the geometry and color information respectively in Section \ref{sec:siamese}. To carefully align the features belonging to different information, we propose the point-level supervision via combining the contrastive and reconstructive learning in Section \ref{sec:point-level}, then we design an unsupervised deep clustering module to generate object pseudo-labels and perform object-level contrastive learning between high-confidence object samples in Section \ref{sec:object-level}. The final hierarchical supervision is described in Section \ref{sec:overall}.
In Section \ref{sec:adapt}, we propose a new method directly evaluating the pre-training model on unsupervised semantic segmentation to demonstrate the effectiveness of our method.

\subsection{Siamese Architecture}
\label{sec:siamese}
\vspace{-0.25cm}
\textbf{Information split and embedding.} In 3D scene datasets, a point  is usually associated geometry information represented by the coordinates  and color information represented by RGB value . Different from previous pre-training methods regarding a single point as an atom unit, we split the point cloud into two parts, the geometry and color respectively. Then we project them to universal embedding space  by Equation~\ref{eq:embedding}. Additionally, to distinguish similar colors in different coord, we add an extra weakly positional embedding  to the color embedding with the Euclidean norm of coord.
Note that we remove all embedding modules in fine-tuning stage to keep our framework plug-and-play in order that more existing methods can benefit from ours.

\textbf{Siamese architecture-agnostic backbone.} We use a symmetric Siamese network  to separately encode geometry features  and color features . Since we attempt to help more existing architectures learn better representations from the combination of geometry and color information, we do not modify any backbone architecture. So that we can directly reuse the core module for standard segmentation with any backbone architecture. In other words, the backbone encodes input  and extracts feature . To align the two information, Siamese backbone  encodes the geometry embedding  and color embedding  with weakly positional embedding  to geometry features  and color features  respectively:

where  is corresponding linear layer of each embedding,   is the Siamese network.
\subsection{Point-level Supervision}
\vspace{-0.25cm}
\label{sec:point-level}
Inspired by the success of associating contrastive learning and reconstructive learning in recent work~\citep{recon23}, 
We propose our point-level supervision elaborately designed for our Siamese architecture, which first contrast and then \textit{swapped reconstruct} the features to benefit from different paradigms.

\textbf{Contrastive learning.} The geometry features  and color feature  are point-wise aligned because they are split from the same point cloud  and extracted by the Siamese segmentation-style backbone network. We apply the InfoNCE loss aiming to pull positive pairs close, and push negative pairs away across the geometry features and color features:

where  is the temperature hyper-parameter, we follow the previous works~\citep{PointContrast} to set it as 0.4.  and  correspond to matched -normalized feature  and  from same point , which represent a pair of positive sample. And  with other  except  represent negative pairs.

\textbf{Reconstructive learning.} Based on our Siamese architecture, we apply the reconstructive learning by \textit{swapped reconstruct} strategy instead of mask strategy, which solves the raising problem about the distribution mismatch between training and testing data in masked autoencoding for point cloud~\cite{MaskPoint}. Specifically, we simply project the geometry features  and color features  to reconstruct color  and geometry . The reconstructive loss is the mean squared error (MSE) between the reconstructed and original information of each point: 

where  is the number of points,  and  represent the reconstruct prediction,  and  represent the reconstruct targets which both scale to between 0 and 1 for stability training loss.

\subsection{Object-level Supervision}
\label{sec:object-level}
\vspace{-0.25cm}
Point-level supervision is widely applied in 3D self-supervised learning, which provides rich representations for downstream tasks. However, the object representation strongly associated with downstream tasks hasn't been noticed before. We propose our object-level supervision driven by the novel unsupervised deep clustering module. The clustering module generates pseudo-label predictions  and  for the geometry features  and color features  respectively, and enforces consistent prediction between geometry prediction  and color prediction  of same point . 
We argue that the pseudo-labels represent more various object features, which are not restricted by human annotations with fixed object classes. To achieve robust supervision among these object-level pseudo labels, we sample the high-confidence object features based on the prediction confidence score and apply object-level contrastive learning according to pseudo labels.
\begin{figure}
  \centering
  \includegraphics[width=0.95\linewidth]{fig/fig_clustering.pdf}
  \caption{(a) The deep clustering module obtains pseudo prediction for different features and enforces consistent with the swapped partition distribution from the Sinkhorn-Knop algorithm. (b) Point-GCC generates the pseudo-labels by utilizing cluster prediction from both branches and projects to ground-truth labels for unsupervised semantic segmentation using Hungarian matching alignment.}
\label{fig:adapt}
\vspace{-5pt}
\end{figure} 
\textbf{Deep clustering via swapped prediction.} We apply the swapped prediction~\cite{SWAV} in 2D contrastive learning to our model, which predicts the pseudo label of an image from the clustering result of another view. In our framework, we swap the cluster target of different information features, and predict the pseudo label from the other information feature based on the inherent consistency of the two types of information as shown in Figure \ref{fig:adapt}(a). For pseudo label classes , we use a learnable matrix  to represent the cluster centroids, and calculate the similarity  between the -normalized features  and cluster centroids .
To avoid the degeneration problem that all features collapse into the same prediction, the Sinkhorn-Knopp algorithm ~\cite{Sinkhorn} is used to generate the equal partition cluster distribution  from the similarity  by converting pseudo-label generation to an optimal transport problem. And the learnable prediction  is computed by , where  is the temperature hyper-parameter. We set all hyper-parameter in swapped prediction same to the previous works~\cite{SWAV} in 2D. Finally, The swapped prediction loss is the cross entropy losses between the learnable prediction  and swapped equal partition distribution :

where  is the cross-entropy loss between the prediction and target. 

\textbf{Object-level contrastive learning.} For the features  with corresponding pseudo prediction  and confidence score from deep clustering, we pick features with confidence scores higher than the picking threshold to alleviate the noise from unsupervised clustering. Then we compute the mean features of high-confidence samples from geometry and color branches, respectively. We take the two types of mean features with the same pseudo-label as positive pairs, oppositely with different pseudo-label as positive pairs, and apply the InfoNCE loss at object-level:

where  is the temperature hyper-parameter, we set it to 0.4 following the above-mentioned setting.  is the -normalized mean feature with pseudo-label .  and  represent a pair of positive sample with same pseudo-label . And  with  corresponding different pseudo-label  represent negative samples.
\begin{table}
\centering
\resizebox{0.85\linewidth}{!}
{
   \begin{tabular}{lcccccc}
    \toprule
    \multirow{2}{*}{Method} & \multicolumn{2}{c}{ScanNetV2} & \multicolumn{2}{c}{SUN RGB-D} & \multicolumn{2}{c}{S3DIS} \\
    \cmidrule(r){2-3}
    \cmidrule(r){4-5}
    \cmidrule(r){6-7}
     & AP & AP &  AP & AP &  AP & AP   \\     
    \midrule
    \multicolumn{7}{c}{\textbf{Supervised Only}} \\
    \midrule
    VoteNet~\cite{VoteNet} & 58.6 & 33.5 & 57.7 & - & - & - \\
    GroupFree-3D~\cite{GroupFree} & 66.3 & 47.8 & - & - & - & - \\
    FCAF3D~\cite{FCAF3D} & 71.5 & 57.3 & 64.2 & 48.9 & 66.7 & 45.9 \\
    TR3D~\cite{TR3D} & 72.9 & 59.3 & 67.1 & 50.4 & 74.5 & 51.7 \\
    \midrule
    \multicolumn{7}{c}{\textbf{Self-supervised Pre-training}} \\
    \midrule
    VoteNet~\cite{VoteNet} & 58.6 & 33.5 & 57.7 & - & - & - \\
    + PointContrast~\cite{PointContrast} & 59.2 & 38.0 & 57.5 & 34.8 & - & - \\
    + DepthContrast~\cite{DepthContrast} & 62.1 & 39.1 & 60.4 & 35.4 & - & - \\
    + CSC~\cite{CSC} & - & 39.3 & - & 36.4 & - & - \\
    + Ponder~\cite{Ponder} & 63.6 & 41.0 & 61.0 & 36.6 & - & - \\
    \rowcolor{linecolor2}+ Point-GCC & \underline{65.3}~\tiny{\textcolor{red}{(+3.0)}} & \underline{44.1}\tiny{\textcolor{red}{(+3.3)}} & \underline{61.3}~\tiny{\textcolor{red}{(+1.5)}} & \underline{37.7}~\tiny{\textcolor{red}{(+2.0)}} & - & - \\
    \midrule
    VoteNet+FF~\cite{TR3D} & - & - & 64.5 & 39.2 & - & - \\
    \rowcolor{linecolor2}+ Point-GCC & - & - & \underline{64.9}~\tiny{\textcolor{red}{(+0.4)}} & \underline{41.3}~\tiny{\textcolor{red}{(+2.1)}} & - & - \\
    \midrule
    GroupFree-3D~\cite{GroupFree} & 66.3 & 47.8 & - & - & - & - \\
    \rowcolor{linecolor2}+ Point-GCC & \underline{68.1}~\tiny{\textcolor{red}{(+1.8)}} & \underline{49.2}~\tiny{\textcolor{red}{(+1.4)}} & - & - & - & - \\
    \midrule
    TR3D~\cite{TR3D} & 72.9 & 59.3 & 67.1 & 50.4 & 74.5 & 51.7 \\
    \rowcolor{linecolor2}+ Point-GCC & \textbf{\underline{73.1}}~\tiny{\textcolor{red}{(+0.2)}} & \textbf{\underline{59.6}}~\tiny{\textcolor{red}{(+0.3)}} & \underline{67.7}~\tiny{\textcolor{red}{(+0.6)}} & \underline{51.0}~\tiny{\textcolor{red}{(+0.6)}} & 74.9~\tiny{\textcolor{red}{(+0.4)}} & 53.2~\tiny{\textcolor{red}{(+1.5)}} \\
    \rowcolor{linecolor}+ Point-GCC & - & - & - & - & \textbf{\underline{75.1}}~\tiny{\textcolor{red}{(+0.6)}} & \textbf{\underline{56.7}}~\tiny{\textcolor{red}{(+5.0)}} \\
    \midrule
    TR3D+FF~\cite{TR3D} & - & - & 69.4 & 53.4 & - & - \\
    \rowcolor{linecolor2}+ Point-GCC & - & - & \textbf{\underline{69.7}}~\tiny{\textcolor{red}{(+0.3)}} & \textbf{\underline{54.0}}~\tiny{\textcolor{red}{(+0.6)}} & - & - \\
    \bottomrule
  \end{tabular}
\vspace{-2cm}
}
\caption{3D Object detection results on ScanNetV2, SUN RGB-D, S3DIS validation set. The overall best results are \textbf{bold}, and the best results with the same baseline model are \underline{underlined}. + means fine-tuning with pre-training on the corresponding dataset. * means that we evaluate the performance on VoteNet with the stronger MMDetection3D implementation for a fair comparison.  means with extra training dataset ScanNetV2.}
\label{detection-table}
\vspace{-5pt}
\end{table}

\subsection{Overall Hierarchical Loss}
\label{sec:overall}
\vspace{-0.25cm}
Our framework contains hierarchical supervision in point-level and object-level, and the Ô¨Ånal loss is a combination of the four losses above-mentioned:

where ,  and  are the loss weight hyper-parameters, we set them to 100, 100 and 1 respectively to balance the magnitude of losses.

\subsection{Adapt to unsupervised semantic segmentation}
\label{sec:adapt}
\vspace{-0.25cm}
Due to the pseudo-label from object-level supervision, Point-GCC can adapt to unsupervised downstream tasks without fine-tuning. 
Meanwhile, previous pre-training methods evaluate the performance by transfer learning on downstream tasks. The results can be greatly affected by the fine-tuning setting and are not intuitive between different baselines. 
As shown in Figure \ref{fig:adapt}(b), we generate the final pseudo-labels by utilizing cluster prediction from geometry and color branch. During the evaluation stage, we use the Hungarian matching alignment~\cite{DeepAlignedClustering} to project the pseudo-labels to ground-truth labels because we are agnostic to the ground truth in pre-training. Although our method is not speciÔ¨Åcally designed for unsupervised downstream tasks, we find that the process is more intuitive and fair for evaluating the performance of pre-training methods. \section{Experiments}
\vspace{-0.2cm}
To analyze the 3D representation learned by Point-GCC, we conduct extensive experiments on multiple datasets and tasks described in Section \ref{setting}. First we evaluate on fully unsupervised semantic segmentation tasks to validate the effectiveness of object representation in Section \ref{unsupervised-semantic}. Then we expand experiments by transfer learning on multiple downstream tasks and datasets in Section \ref{downstream}.
\subsection{Experiment setting}
\vspace{-0.25cm}
\label{setting}
\textbf{Dataset. } We use three popular indoor scene datasets: ScanNetV2~\cite{dai2017scannet}, SUN RGB-D~\cite{song2015sun}, S3DIS~\cite{armeni_cvpr16} in our experiments. \textbf{ScanNetV2} is a 3D reconstruction dataset, which provides 1513 indoor scans with a total of 20 classes. \textbf{SUN RGB-D} is a monocular RGB-D image dataset, which provides 10335 RGB-D images from four different sensors with a total of 37 classes. \textbf{S3DIS} is another 3D indoor scene dataset, which provides 271 point cloud scenes across 6 areas with 13 classes.

\textbf{Implementation details.} We implement Point-GCC built upon the MMDetection3D ~\cite{mmdet3d2020} framework. We use the AdamW optimizer with an initial learning rate of 0.001 and weight decay of 0.0001. Other implementation details are followed the default scheme. 
To ensure fair comparability of results, we refer to selecting downstream models implemented by MMDetection3D. In downstream task experiments, we decay the learning rate by 0.5, and other settings follow the original implementation. The full detail settings are provided in the Appendix.

\subsection{Fully unsupervised semantic segmentation}
\vspace{-0.25cm}
\label{unsupervised-semantic}
We evaluate our pre-training model on fully unsupervised semantic segmentation tasks using the method in Section \ref{sec:adapt} to validate the effectiveness of object representation. As shown in Table \ref{semantic-table}, our method surpasses previous unsupervised methods by a huge margin and is closer to the weakly-supervised method, despite Point-GCC being not speciÔ¨Åcally designed for unsupervised downstream tasks. With the same backbone PointNet++, Point-GCC surpasses previous work SL3D~\cite{SL3D} by +9.8\% mIoU, and +7.8\% mIoU compared with more powerful Point Transformer on ScanNetV2 dataset. The result proves that Point-GCC has learned rich object representation in unsupervised pre-training.

\textbf{Fine-tuning semantic segmentation.} Additionally, we fine-tune the pre-training model for semantic segmentation to verify the consistent improvement of our method. With supervised fine-tuning, the model gains significant improvements by +5.4\% mIoU on ScanNetV2 dataset, which proves that our method has learned intrinsic representations of the point cloud.

\begin{table}
\centering
\resizebox{0.85\linewidth}{!}
{
 \begin{tabular}{lcccc}
    \toprule
    Method & Supervision & Backbone & Pseudo Classes & mIoU \\
    \midrule
    \multicolumn{4}{c}{\textbf{Unsupervised Method}} \\
    \midrule
    SL3D~\cite{SL3D} & unsupervised & PointNet++ & 400 & 8.5 \\
    SL3D~\cite{SL3D} & unsupervised & Point Transformer & 800 & 10.5 \\
    \rowcolor{linecolor2}Point-GCC & unsupervised & PointNet++ & 20 & 18.3 \\
    \midrule
    \multicolumn{4}{c}{\textbf{Weakly-supervised Method}} \\
    \midrule
    WyPR~\cite{WYPR} & scene-level & PointNet++ & 20 & 29.6 \\
    MPRM~\cite{MPRM} & subcloud-level & KPConv & 20 & 41.0 \\
    \midrule
    \multicolumn{4}{c}{\textbf{Supervised Method}} \\
    \midrule
    PointNet++(SSG)~\cite{PointNet++} & supervised & PointNet++ & 20 & 54.4 \\
    \rowcolor{linecolor2}+ Point-GCC & supervised & PointNet++ & 20 & \textbf{\underline{59.8}}~\tiny{\textcolor{red}{(+5.4)}} \\
    \bottomrule
  \end{tabular}
}
    \vspace{-0.2cm}
    \caption{3D semantic segmentation results on ScanNetV2 dataset by different level of supervision. The overall best results are \textbf{bold}. + means fine-tuning with pre-training on the corresponding dataset.}
  \label{semantic-table}
\end{table}
\subsection{Transfer learning on downstream tasks}
\label{downstream}
\vspace{-0.25cm}
\textbf{3D Object detection.} For 3D object detection task, we pre-train the PointNet++~\cite{PointNet++} backbone for VoteNet~\cite{VoteNet}, VoteNet+FF~\cite{TR3D} and GroupFree-3D~\cite{GroupFree} and the MinkResNet~\cite{MinkowskiEngine} backbone for TR3D~\cite{TR3D}, TR3D+FF~\cite{TR3D} respectively.
Table \ref{detection-table} shows the results on ScanNetV2, SUN-RGBD, and S3DIS datasets. Our method gains stable and significant improvements for various settings. Compared with previous 3D self-supervised methods with the common baseline model VoteNet, our method achieves higher AP than the previous highest model Ponder~\cite{Ponder} by +3.1\% on ScanNetV2 and +1.1\% on SUN RGB-D. For more recent models, our model also significantly boosts VoteNet+FF, GroupFree-3D, TR3D, TR3D+FF on multiple datasets and achieves new state-of-the-art results with 69.7\% AP, 54.0\% AP on SUN RGB-D and 75.1\% AP, 56.7\% AP on S3DIS datasets.

\textbf{3D Instance segmentation.} For 3D instance segmentation task, we pre-train the MinkResNet backbone for TD3D~\cite{TD3D} on ScanNet and S3DIS datasets.
Table \ref{instance-table} shows the results on ScanNetV2 and S3DIS validation sets. Downstream models gain remarkable performance
by +1.1\% AP on ScanNetV2, +1.9\% on S3DIS and +1.5\% on S3DIS with extra train data, demonstrating our method's general improvement across multiple settings.

Interestingly, the improvements for the PointNet++ backbone widely surpass the MinkResNet backbone. We guess that sparse convolution architecture implicitly aligns the color information from features and the geometry information from fine-grained sparse voxel operation. It may be a kind of explanation for why 3D sparse convolution has better performance and efficiency on various tasks.

\begin{table}
\centering
\resizebox{0.85\linewidth}{!}
{
   \begin{tabular}{lccccccc}
    \toprule
    \multirow{2}{*}{Method} & \multicolumn{3}{c}{ScanNetV2} & \multicolumn{4}{c}{S3DIS} \\
    \cmidrule(r){2-4}
    \cmidrule(r){5-8}
     & AP & AP &  AP & AP & AP & Prec & Rec   \\
    \midrule
    \multicolumn{8}{c}{\textbf{Supervised Only}} \\
    \midrule
    PointGroup~\cite{PointGroup}& 34.8 & 56.7 & 71.3 & - & 57.8 & 61.9 & 62.1 \\
    HAIS~\cite{HAIS} & 43.5 & 64.4 & 75.6 & - & - & 71.1 & 65.0 \\
    SoftGroup~\cite{SoftGroup} & 45.8 & 67.6 & 78.9 & 51.6 & 66.1 & 73.6 & 66.6 \\
    \midrule
    \multicolumn{8}{c}{\textbf{Self-supervised Pre-training}} \\
    \midrule
    TD3D~\cite{TD3D} & 46.2 & 71.1 & 81.3 & 48.6 & 65.1 & 74.4 & 64.8 \\
    \rowcolor{linecolor2}+ Point-GCC & \textbf{\underline{47.3}}~\tiny{\textcolor{red}{(+1.1)}} & \textbf{\underline{71.3}}~\tiny{\textcolor{red}{(+0.2)}} & \textbf{\underline{81.6}}~\tiny{\textcolor{red}{(+0.3)}} & \underline{50.5}~\tiny{\textcolor{red}{(+1.9)}} & \underline{65.4}~\tiny{\textcolor{red}{(+0.3)}} & \underline{75.5}~\tiny{\textcolor{red}{(+1.1)}} & \underline{65.9}~\tiny{\textcolor{red}{(+1.1)}} \\
    \midrule
    TD3D~\cite{TD3D}& - & - & - & 52.1 & 67.2 & 75.2 & 68.7 \\
    \rowcolor{linecolor}+ Point-GCC & - & - & - & \textbf{\underline{53.6}}~\tiny{\textcolor{red}{(+1.5)}} & \textbf{\underline{68.4}}~\tiny{\textcolor{red}{(+1.2)}} & \textbf{\underline{76.6}}~\tiny{\textcolor{red}{(+1.4)}} & \textbf{\underline{69.5}}~\tiny{\textcolor{red}{(+0.8)}} \\
    \bottomrule
  \end{tabular}
}
\vspace{-0.2cm}
\caption{3D instance segmentation results on ScanNetV2 and S3DIS dataset. The overall best results are \textbf{bold}, and the best results with the same baseline model are \underline{underlined}. + means fine-tuning with pre-training on the corresponding dataset.  means with extra training dataset ScanNetV2.}
\label{instance-table}
\end{table}
\begin{table}
\centering
\resizebox{0.75\linewidth}{!}
{
 \begin{tabular}{ccccccc}
    \toprule
    \multicolumn{2}{c}{Point-level} & \multicolumn{2}{c}{Object-level} & 
    \makecell[c]{Unsupervised \\ Segmentation} & 
    \multicolumn{2}{c}{\makecell[c]{Object \\ Detection}} \\
    \cmidrule(r){1-2}
    \cmidrule(r){3-4}
    \cmidrule(r){5-5}
    \cmidrule(r){6-7}
     Contra. & Recon. & Cluster. & Contra. & mIoU & AP &  AP \\
    \midrule
     &  &  &  & 18.27 & 65.3 & 44.1 \\
     &  &  & \scalebox{0.85}{\ding{55}} & 16.07 & 65.0 & 43.6 \\
     &  & \scalebox{0.85}{\ding{55}} & \scalebox{0.85}{\ding{55}} & - & 64.8 & 43.0 \\
     & \scalebox{0.85}{\ding{55}} & \scalebox{0.85}{\ding{55}} & \scalebox{0.85}{\ding{55}} & - & 64.4 & 42.8 \\
    \scalebox{0.85}{\ding{55}} &  & \scalebox{0.85}{\ding{55}} & \scalebox{0.85}{\ding{55}} & - & 63.3 & 42.7 \\
    \scalebox{0.85}{\ding{55}} & \scalebox{0.85}{\ding{55}} & \scalebox{0.85}{\ding{55}} & \scalebox{0.85}{\ding{55}} & - & 62.3 & 40.8 \\
    \bottomrule
  \end{tabular}
}
  \vspace{-0.2cm}
  \caption{Ablation study of the hierarchical supervision. - means the model can't perform the unsupervised segmentation task due to the lack of the pseudo-label.}
  \label{hierarchical-supervision}
\end{table}
\begin{table}[t]
\begin{floatrow}
\capbtabbox{
 \centering
 \begin{tabular}{cccc}
    \toprule
    Color &  Geometry & \multicolumn{2}{c}{Object Detection} \\
    \cmidrule(r){3-4}
    Branch & Branch & AP &  AP \\
    \midrule
     &  & 64.8 & 43.0 \\
    \scalebox{0.85}{\ding{55}} &  & 62.4 & 40.9 \\
     & \scalebox{0.85}{\ding{55}} & 62.5 & 39.4 \\
    \scalebox{0.85}{\ding{55}} & \scalebox{0.85}{\ding{55}} & 62.3 & 40.8 \\
    \bottomrule
  \end{tabular}
\vspace{-0.2cm}
}{
  \caption{Ablation study of the Geometry-Color Contrast approach.}
  \label{Ablation-Branch}
}
\capbtabbox{
\centering
 \begin{tabular}{ccc}
    \toprule
    Object Picking & \multicolumn{2}{c}{Object Detection} \\
    \cmidrule(r){2-3}
    Threshold & AP &  AP \\
    \midrule
    only max score & 64.5 & 43.0 \\
    2.0 / class num. & 65.3 & 44.1 \\
    1.8 / class num. & 64.4 & 43.7\\
    1.5 / class num. & 64.2 & 43.2 \\
    \bottomrule
  \end{tabular}
  \vspace{-0.2cm}
}{
  \caption{Ablation study of the Object sampling strategy.}
  \label{sampleing-strategy}
}
\end{floatrow}
\end{table}
\subsection{Ablation study And Discussion}
\label{Ablation}
\vspace{-0.25cm}
To analyze the effectiveness of our approach, we further explore additional experiments to measure the contribution of each component to the Ô¨Ånal representation quality. For efficiency, all ablation experiments are implemented with VoteNet setting on pre-training and object detection.

\textbf{Hierarchical supervision.} To further explore the improvement of our hierarchical supervision, we conduct ablation studies with different components. Table \ref{hierarchical-supervision} shows the unsupervised semantic segmentation results with pre-training and object detection results with fine-tuning. The results show that both contrastive learning and reconstructive learning in point-level contribute to the final results. Even though just with point-level supervision, our method has achieved higher AP and AP than the previous best model Ponder by +1.2\% and +2.0\%. Furthermore, the swapped prediction and object-level contrastive learning also provide remarkable improvements for AP and AP, especially AP. 
Intuitively, the improvement of AP is more significant than AP demonstrating that object-level supervision improves the model with a more precise view of objects.

\textbf{Geometry-Color Contrast.} To verify the importance of our Geometry-Color Contrast approach, we compare the results with a single reconstruction branch setting. Table \ref{Ablation-Branch} shows object detection results with different pre-training branches. The results show that the performance with a single branch of whether geometry or color reconstruction obviously declines, which proves our Geometry-Color Contrast plays an essential role in the significant performance.

\textbf{Object sampling strategy.} The result in table \ref{hierarchical-supervision} shows that object-level supervision provides the most obvious boost for AP. We compare the results with different object sampling strategies to analyze the object samples used in object-level contrastive learning. The results in table \ref{sampleing-strategy} show that the more confident object samples are, the greater performance we achieve. However, only using the maximum score sample, the performance decays because of over-fitting.
\begin{figure}
  \centering
  \vspace{-0.2cm}
  \includegraphics[width=0.85\linewidth]{fig/fig_vis.pdf}
  \vspace{-0.2cm}
  \caption{The visualization of reconstruction results from Point-GCC. Note that we decrease the point size in geometry reconstruction to avoid the block from noisy points.}
\label{fig:vis}
\end{figure} \subsection{Visualization}
\label{visualization}
\vspace{-0.25cm}
Figure~\ref{fig:vis} shows the visualization of geometry and color reconstruction results from our method. The results show that our method can generate high-quality complement from one type of information in the point cloud consistently. The method may contain potential applications such as depth estimation and texture generation. \section{Conclusions}
\vspace{-0.2cm}
In this paper, we propose a new universal self-supervised 3D scene pre-training framework via \textbf{G}eometry-\textbf{C}olor \textbf{C}ontrast (Point-GCC), which utilizes an architecture-agnostic Siamese network with hierarchical supervision. Extensive experiments show that Point-GCC significantly improves performance on unsupervised tasks without fine-tuning and a wide range of downstream tasks, especially achieving new state-of-the-art results on multiple datasets.

To the best of our knowledge, Point-GCC is the first study to explore the self-supervised paradigm that can better utilize the relations of different point cloud information, hence we elaborately design our plug-and-play pre-training framework to help improve various existing downstream methods, instead of directly designing a new architecture. We hope our work could attract more attention about the discriminative information of point cloud, which may inspire future point cloud representation learning works. 
\medskip


{
\small
\bibliographystyle{plain}
\bibliography{cite}
}

\appendix

\section{Implementation Details}\label{app:impl_detail}
In this section, we provide the details and hyperparameters for pre-training and transfer learning.

\subsection{Experimental details}
\textbf{Pre-training Details}~ Our pre-training settings are based on the default detection configs in MMDetection3D~\cite{mmdet3d2020}. For all settings, we use the AdamW optimizer with an initial learning rate of 0.001 and weight decay of 0.0001. More detailed training configurations are shown in Table~\ref{tab:hyper_params}.

\textbf{Downstream Transferring Details}~ For 3D object detection task, we fine-tune with the PointNet++~\cite{PointNet++} backbone for VoteNet~\cite{VoteNet}, VoteNet+FF~\cite{TR3D} and GroupFree-3D~\cite{GroupFree} and the MinkResNet~\cite{MinkowskiEngine} backbone for TR3D~\cite{TR3D}, TR3D+FF~\cite{TR3D} respectively. For 3D instance segmentation task, we fine-tune with the MinkResNet backbone for TD3D~\cite{TD3D} on ScanNet and S3DIS datasets. For 3D semantic segmentation task, we fine-tune with PointNet++ backbone for PointNet++(SSG)~\cite{PointNet++}. We follow the default setting in the corresponding downstream model.  More detailed training configurations are shown in Table~\ref{tab:hyper_params}.

\begin{table}[h!]
\centering
\resizebox{\linewidth}{!}
{
\begin{tabular}{lccccc}
 \toprule
 Config & VoteNet & GroupFree-3D & PointNet++(SSG) & TR3D & TD3D \\
 \midrule
 backbone & PointNet2SASSG & PointNet2SASSG & PointNet2SASSG & MinkResNet & MinkResNet \\
 \midrule
 \multicolumn{6}{c}{\textbf{Pre-training Details}} \\
 \midrule
 optimizer & AdamW & AdamW & AdamW & AdamW & AdamW \\
 learning rate & 1e-3 & 1e-3 & 1e-3 & 1e-3 & 1e-3 \\
 weight decay & 1e-4 & 1e-4 & 1e-4 & 1e-4 & 1e-4 \\
 training epochs & 400 & 400 & 400 & 200 & 200 \\
 lr scheduler & cosine & cosine & cosine & cosine & cosine \\
 batch size & 4x8 & 4x8 & 4x8 & 4x4 & 4x4 \\
 augmentation & default & default & default & default & default \\
 \midrule
 \multicolumn{6}{c}{\textbf{Downstream Transferring Details}} \\
 \midrule
 optimizer & AdamW & AdamW & Adam & AdamW & AdamW \\
 learning rate & 4e-3 & 3e-3 & 1e-3 & 1e-3 & 1e-3 \\
 weight decay & 1e-2 & 5e-4 & 1e-2 & 1e-4 & 1e-4 \\
 training epochs & 72 & 160 & 200 & 24 & 66 \\
 lr scheduler & step & step & cosine & step & step \\
 batch size & 8x8 & 8x4 & 16x2 & 16x1 & 6x1 \\
 augmentation & default & default & default & default & default \\
 \midrule
 GPU device & RTX 2080Ti & RTX 2080Ti & RTX 2080Ti & RTX 3090 & RTX 3090 \\
\bottomrule
\end{tabular}
}
\caption{Training recipes for pretraining and downstream fine-tuning.}
\label{tab:hyper_params}
\end{table}

\subsection{Implementation details of Deep Clustering}

We provide a pseudo-code for Deep Clustering via Swapped Prediction training loop in Pytorch style as follows. All of the hyperparameters are the same as the previous works~\cite{SWAV} in 2D. The temperature is set to 0.1 and the Sinkhorn regularization parameter is set to 0.05 for all runs.
\\
\\
\com{\# C: prototypes (DxK) i.e., linear layer} 
\\
\com{\# feat: feature from backbone + projection head}
\\
\com{\# temp: temperature}
\\
\\
\code{def swapped\_prediction(feat\_geo, feat\_color):}
\\
\\
\tab \com{\# normalize prototypes}
\\
\tab \code{with torch.no\_grad():}
\\
\tab \tab \code{C = normalize(C, dim=0, p=2)}
\\
\\
\tab \com{\# normalize features}
\\
\tab \code{norm\_geo = normalize(feat\_geo, dim=-1, p=2)}
\\
\tab \code{norm\_color = normalize(feat\_color, dim=-1, p=2)}
\\
\\
\tab \com{\# compute scores}
\\
\tab \code{scores\_geo = mm(norm\_geo, C)}
\\
\tab \code{scores\_color = mm(norm\_color, C)}
\\
\\
\tab \com{\# compute assignments}
\\
\tab \code{with torch.no\_grad():}
\\
\tab \tab \code{q\_geo = sinkhorn(scores\_geo)}
\\
\tab \tab \code{q\_color = sinkhorn(scores\_color)}
\\
\\
\tab \com{\# convert scores to probabilities}
\\
\tab \code{p\_geo = Softmax(scores\_geo / temp)}
\\
\tab \code{p\_color = Softmax(scores\_color / temp)}
\\
\\
\tab \com{\# swap prediction problem}
\\
\tab \code{loss = - 0.5 * mean(q\_geo * log(p\_color) + q\_color * log(p\_geo))}
\\
\\
\com{\# Sinkhorn-Knopp}
\\
\code{def sinkhorn(scores, eps=0.05, niters=3):}
\\
\tab \code{Q = exp(scores / eps).T}
\\
\tab \code{Q /= sum(Q)}
\\
\tab \code{K, B = Q.shape}
\\
\tab \code{u, r, c = zeros(K), ones(K) / K, ones(B) / B}
\\
\tab \code{for \_ in range(niters):}
\\
\tab \tab \code{u = sum(Q, dim=1)}
\\
\tab \tab \code{Q *= (r / u).unsqueeze(1)}
\\
\tab \tab \code{Q *= (c / sum(Q, dim=0)).unsqueeze(0)}
\\
\tab \code{return (Q / sum(Q, dim=0, keepdim=True)).T}
\\
\section{Additional Experiments}\label{app:add_exp}
\subsection{Additional comparison}\label{app:add_eva}
\vspace{-0.1cm}
Some baseline models~\cite{VoteNet, GroupFree} do not use color information. Table~\ref{tab:color-baseline} shows the additional baseline model with color information for a fair comparison from our reproduction and other works~\cite{Ponder,TokenFusion}, which get slight improvement and even decrease. The results prove what we mentioned in the main paper: directly concatenating all information can not adapt the model to discriminately learn different aspects of point clouds, demonstrating our work's necessity.
\begin{table}[h!]
\centering
{
\begin{tabular}{lccc}
    \toprule
    \multirow{2}{*}{Method} & \multirow{2}{*}{Input} & \multicolumn{2}{c}{Object Detection} \\
    \cmidrule(r){3-4}
    & & AP &  AP \\
    \midrule
    VoteNet~\cite{VoteNet} & xyz+height & 58.6 & 33.5 \\
    VoteNet~\cite{Ponder} & xyz+color+height & 58.8 & 33.4 \\
    VoteNet & xyz+height & 62.3 & 40.8 \\
    VoteNet & xyz+color & 61.8 & 39.9 \\
    \rowcolor{linecolor2}+ Point-GCC & xyz+color & \underline{65.3}~\tiny{\textcolor{red}{(+3.0)}} & \underline{44.1}\tiny{\textcolor{red}{(+3.3)}}  \\
    \midrule
    GroupFree-3D~\cite{GroupFree} & xyz & 66.3 & 47.8 \\
    GroupFree-3D~\cite{TokenFusion} & xyz+color & 66.3 & 47.0 \\
    \rowcolor{linecolor2}+ Point-GCC & xyz+color & \underline{68.1}~\tiny{\textcolor{red}{(+1.8)}} & \underline{49.2}~\tiny{\textcolor{red}{(+1.4)}} \\
    \bottomrule
\end{tabular}
}
\caption{3D Object detection results on ScanNetV2 validation set. * means the VoteNet with the stronger MMDetection3D implementation for a fair comparison.}
\label{tab:color-baseline}
\end{table}
\subsection{Weakly positional embedding}
To further explore the improvement of positional embedding, we conduct an additional ablation study with different settings. Table \ref{tab:pos} shows the object detection results with different positional embedding. 
The results show that our model has stable effects under different position encoding conditions. This may be because we remove the positional embedding and adjust the input channel in the downstream fine-tuning stage so that the impact on the downstream task is reduced.
\begin{table}[h!]
\centering
{
\begin{tabular}{lcc}
    \toprule
    \multirow{2}{*}{Positional Embedding} & \multicolumn{2}{c}{Object Detection} \\
    \cmidrule(r){2-3}
    & AP &  AP \\
    \midrule
    no pos & 64.5 & 43.7 \\
    xy pos & 64.7 & 43.8 \\
    \rowcolor{linecolor2}norm pos & 65.3 & 44.1 \\
    \bottomrule
\end{tabular}
}
\caption{Ablation study of the positional embedding. \textbf{no pos} means without positional embedding. \textbf{xy pos} means with the positional embedding of x and y, and the corresponding task aims only to reconstruct z, \ie, height estimation task.}
\label{tab:pos}
\end{table}
\subsection{Pseudo-label Classes}\label{app:add_eva}
We compare the results with different pseudo-label classes in deep clustering to analyze the effect of cluster distribution. The results in table \ref{tab:Pseudo-label} show that more pseudo-label classes degrade the performance of downstream tasks. We guess that finer-grained labels in the ScanNet dataset, such as matching to ScanNet200~\cite{Scannet200_22}, show more apparent characteristics of long-tailed data, contrary to our assumption of equal cluster distribution because we are agnostic to the ground truth labels in pre-training.
\begin{table}[h!]
\centering
{
\begin{tabular}{lcc}
    \toprule
    \multirow{2}{*}{Pseudo-label Classes} & \multicolumn{2}{c}{Object Detection} \\
    \cmidrule(r){2-3}
    & AP &  AP \\
    \midrule
    \rowcolor{linecolor2}20 & 65.3 & 44.1 \\
    40& 64.3 & 43.7 \\
    200 & 63.9 & 42.6\\
    \bottomrule
\end{tabular}
}
\caption{Ablation study of the pseudo-label classes in Deep Clustering.}
\label{tab:Pseudo-label}
\end{table}

\subsection{Error elimination}
To obtain statistically significant results, we train our models five times and evaluate each trained model five times independently on erratic downstream tasks. Table~\ref{tab:error} shows the best and the average value (in brackets).
\begin{table}[h!]
\centering
\resizebox{0.9\linewidth}{!}
{
   \begin{tabular}{lcccccc}
    \toprule
    \multirow{2}{*}{Method} & \multicolumn{2}{c}{ScanNetV2} & \multicolumn{2}{c}{SUN RGB-D} & \multicolumn{2}{c}{S3DIS} \\
    \cmidrule(r){2-3}
    \cmidrule(r){4-5}
    \cmidrule(r){6-7}
     & AP & AP &  AP & AP &  AP & AP   \\     
    \midrule
    VoteNet+FF~\cite{TR3D} & - & - & 64.5 (63.7) & 39.2 (38.1) & - & - \\
    \rowcolor{linecolor2}+ Point-GCC & - & - & \underline{64.9}(64.2) & \underline{41.3}(40.6) & - & - \\
    \midrule
    GroupFree-3D~\cite{GroupFree} & 66.3 (65.7) & 47.8 (47.7) & - & - & - & - \\
    \rowcolor{linecolor2}+ Point-GCC & \underline{68.1}(67.3) & \underline{49.2}(48.7) & - & - & - & - \\
    \midrule
    TR3D~\cite{TR3D} & 72.9 (72.0) & 59.3 (57.4) & 67.1 (66.3) & 50.4 (49.6) & 74.5 (72.1) & 51.7 (47.6) \\
    \rowcolor{linecolor2}+ Point-GCC & \textbf{\underline{73.1}}(72.2) & \textbf{\underline{59.6}}(57.2) & \underline{67.7}(66.1) & \underline{51.0}(49.9) & 74.9(72.6) & 53.2(50.9) \\
    \rowcolor{linecolor}+ Point-GCC & - & - & - & - & \textbf{\underline{75.1}}(73.5) & \textbf{\underline{56.7}}(54.4) \\
    \midrule
    TR3D+FF~\cite{TR3D} & - & - & 69.4 (68.7) & 53.4 (52.4) & - & - \\
    \rowcolor{linecolor2}+ Point-GCC & - & - & \textbf{\underline{69.7}} (69.0) & \textbf{\underline{54.0}} (53.3) & - & - \\
    \bottomrule
  \end{tabular}
}
\caption{3D Object detection results on ScanNetV2 validation set. The main result is the best value, and the number within the bracket is the average value across 25 trials following previous works~\cite{GroupFree, TR3D}.  means with extra training dataset ScanNetV2.}
\vspace{-5pt}
\label{tab:error}
\end{table}
\begin{figure}[h!]
  \centering
  \vspace{-0.4cm}
  \includegraphics[width=\linewidth]{fig/fig_vis-supp.pdf}
  \vspace{-0.3cm}
  \caption{The visualization of unsupervised semantic segmentation results. For better visualization, we use the Hungarian matching alignment to project the pseudo-labels to ground-truth labels.}
\label{fig:vis-supp}
\end{figure} \section{Additional Visualization}\label{app:impl_detail}
\subsection{Unsupervised semantic segmentation visualization}
We provide additional visualization results of unsupervised semantic segmentation. Figure~\ref{fig:vis-supp} shows that our method can clearly distinguish the main parts of different objects without supervision. However, for small or complex objects, the segment may be merged into others or ignored because of the equal partition cluster distribution from the Sinkhorn-Knopp algorithm~\cite{Sinkhorn}. The result demonstrates that our pre-training approach helps the model learn object representations to enhance performance on downstream tasks. 


\end{document}
