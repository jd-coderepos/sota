\subsection{Proofs for Local Variance}
\label{app-local}
\subsubsection{Computation for Example~\ref{ex:local-mem}}\label{app:local-example}
We have


Throughout this section we use the following three simple lemmas. The first one allows us to reduce convex combinations of two-dimensional vectors (typically vectors consisting of the mean-payoff and variance) to combinations of just two vectors.
\begin{lemma}\label{lem:approx-two}
Let  be a sequence of points in  and  satisfy . Then there are two vectors  and  and a number  such that

\end{lemma}
\begin{proof}
Denote by  the point  and by  the set .
If all the points of  lie in the same line, then clearly there must be some .
Assume that this is not true. Then the convex hull  of  is a convex polygon whose vertices are some of the points of . Consider a point  where . The point  lies on the boundary of  and thus, as  is a convex polygon,  lies on the line segment between two vertices, say , of . Thus there is  such that

This finishes the proof.
\end{proof}
\noindent
The following lemma shows how to minimize the mean square deviation (to which our notion of variance is a special case).
\begin{lemma}\label{lem:min-var}
Let  such that , let  and let us consider the following function of one real variable:

Then the function  has a unique minimum in .
\end{lemma}
\begin{proof}
By taking the first derivative of  we obtain

Thus  iff .
Moreover, by taking the second derivative we obtain , and thus  is a minimum.
\end{proof}
The following lemma shows that frequencies of actions determine (in some cases) the mean-payoff as well as the variance.
\begin{lemma}\label{lem:freq-var}
Let  be a memoryless strategy and let  be a BSCC of . Consider frequencies of individual actions  when starting in a state :  where  assigns  to  and  to all other actions (note that the values do not depend on which  we choose).
Then  determine uniquely all of , , and  as follows: 

\end{lemma}
\begin{proof}
We have

and

Finally, it is easy to see that the local and hybrid variance coincide in BSCCs since almost all runs have the same frequencies of actions. This gives us the result for the local variance.
\end{proof}

\medskip
\noindent
\subsubsection{Proof of Proposition~\ref{prop:strong-opt-local}.}\label{app-strong-opt}
We obtain the proof from the following slightly weaker version.
\begin{proposition}\label{prop:eps-opt-local}
Let us fix a MEC  and let . There are two frequency functions  and , and a number  such that:

\end{proposition}
\noindent
Before we prove Proposition~\ref{prop:eps-opt-local}, let us show that it indeed implies Proposition~\ref{prop:strong-opt-local}.
There is a sequence , two functions  and , and  such that as  
\begin{itemize}
\item  
\item  converges pointwise to 
\item  converges pointwise to 
\item  converges to 
\end{itemize}
It is easy to show that  as well as  are frequency functions. Moreover, 
as 

and

we obtain

This finishes a proof of Proposition~\ref{prop:strong-opt-local}. It remains to prove Proposition~\ref{prop:eps-opt-local}.

\medskip\noindent
\begin{proof}[Proof of Proposition~\ref{prop:eps-opt-local}.]
Given  we denote by  the set of all runs  such that

Note that 

By Lemma~\ref{lem:approx-two}, there are  and  such that  and  and 

Let us concentrate on  and construct a frequency function  on  such that

Intuitively, we obtain  as a vector of frequencies of individual actions on an appropriately chosen run of . Such frequencies determine the average and variance close to  and , respectively. We have to deal with some technical issues, mainly with the fact that the frequencies might not be well defined for almost all runs (i.e. the corresponding limits might not exist). This is solved by a careful choice of subsequences as follows.
\begin{claim}\label{claim:subsequence-local}
For every run  there is a sequence of numbers  such that
all the following limits are defined:

and for every action  there is a number  such that

(Here  if , and  otherwise.)

Moreover, for almost all runs  of  we have that  is a frequency function on  and that 
 determines , i.e.,
 and .
\end{claim}
\begin{proof}
We start by taking a sequence  such that

Existence of such a sequence follows from the fact that every sequence of real numbers has a subsequence which converges to the lim sup of the original sequence.

Now we extract a subsequence  of  such that 

using the same argument.

Now assuming an order on actions, , we define  for  so that  is the sequence , and
every  is a subsequence of  
such that the following limit exists (and is equal to a number )

We take  to be the desired sequence .

Now we have to prove that  is a frequency function on  for almost all runs of . Clearly,  for all . Also,

To prove the third condition from the definition of frequency functions, we invoke the law of large numbers (SLLN) \cite{Billingsley:book}. Given a run , an action , a state  and , define

By SLLN and by the fact that in every step the distribution on the next states depends just on the chosen action, for almost all runs  the following limit is defined and the equality holds whenever :

We obtain

Here  is the -th state of , and  for  and  otherwise.


\end{proof}
\noindent
Now pick an arbitrary run  of  such that  is a frequency function. Then

Similarly, for  we obtain  such that 

This together with the equation~(\ref{eq:approx-two}) from page~\pageref{eq:approx-two} proves Proposition~\ref{prop:eps-opt-local}:

This finishes the proof of Proposition~\ref{prop:eps-opt-local}.
\end{proof}

\subsubsection{Details for proof of Proposition~\ref{prop:local-main}}\label{app-local-main}
We have

Here  and  are conditional expectations of  and , respectively, on runs of .
Thus



We define memoryless strategies  and  in  as follows: Given  such that  and , we put

In the remaining states  the strategy  (or ) behaves as a memoryless deterministic strategy reaching  (or , resp.) with probability one.

Given a BSCC  of  (or  of ), we write  (or , resp.)

Denoting by  the tuple  we obtain

Here  and  denote the expected mean-payoff and the expected local variance, resp., on almost all runs of either  or  initiated in any state of  (note that almost all such runs have the same mean-payoff and the local variance due to ergodic theorem).
Note that the second equality follows from the fact that  (or ) iff  for a BSCC  of  (or of ). The third inequality follows from Lemma~\ref{lem:min-var}. The last equality follows from Lemma~\ref{lem:freq-var} and the fact that  is the frequency of firing  on almost all runs initiated in .

By~Lemma~\ref{lem:approx-two}, there are two components  and  such that

In what follows we use the following definition: Let  be a memoryless randomized strategy on a MEC  and let  be a BSCC of . We say that a strategy  is {\em induced} by  if 
\begin{enumerate}
\item  for all  and 
\item in all  the strategy  corresponds to a memoryless deterministic strategy which reaches a state of  with probability one
\end{enumerate}
(Note that the above definition is independent of the strategy  once it generates the same BSCC .)

The strategies  and  induced by  and , resp., generate single-BSCC Markov chains  and  satisfying for every state  the following

Here the last equality follows from the fact that almost all runs in  (and also in ) have the same mean-payoff. Thus for almost all runs the local variance is equal to the hybrid one. This shows that in , a convex combination of two memoryless (possibly randomized) strategies is sufficient to optimize the mean-payoff and the local variance. 

Now we show that these strategies may be even deterministic.
\begin{claim}\label{prop:MR-MD}
Let .
There are {\em memoryless deterministic} strategies  in , each generating a single BSCC, and numbers  such that

and

\end{claim}
\begin{proof}
It suffices to concentrate on . 
By~\cite{derman1970finite},  is equal to a convex combination of the values  for some memoryless deterministic strategies , i.e. there are  such that  and .
For all  and  denote
 a memoryless deterministic strategy such that  on all , and on other states  is defined so that
 is reached with probability 1, independent of the starting state.
For all  we have ,
while for  we have . Hence
.
Since , we apply Lemma~\ref{lem:approx-two} and get
there are two memoryless deterministic single-BSCC strategies  and  such that

which together with Lemma~\ref{lem:freq-var} implies that

and

Here the inequality follows from Lemma~\ref{lem:min-var}. So

Finally, we show that . Since  has a single BSCC, almost all runs have the same mean payoff. Hence, .
\end{proof}
\noindent
By Claim~\ref{prop:MR-MD},
 
and so by Lemma~\ref{lem:approx-two}, there are  and a number  such that

Define memoryless deterministic strategies  and  in  so that for every  and  we have
 and  for .





\subsubsection{Proof of Equation~(\ref{eqn-t2})}\label{app-eqn-t2}
We have

Here  is an arbitrary state of .
\subsubsection{Proof of Theorem~\ref{thm:local-np-alg}}\label{app-local-np-alg}
First, we show that if there is  in  such that
, then there is a strategy  in  such that . 
Consider the 3-memory stochastic update strategy  from Proposition~\ref{prop:local-main} satisfying
. Define a memoryless strategy  in  that mimics  as follows (we denote the only memory element of  by ):
\begin{itemize}
\item ,  , ,
\item   for all 
\item 
\item 
\item 
\end{itemize}
It is straightforward to verify that 



\noindent
Second, we show that if there is  in  satisfying 
, then
there is the desired 3-memory stochastic update strategy  in . Moreover, we show that existence of such  is decidable in polynomial time and also that the strategy is computable in polynomial time (if it exists).

By~\cite{BBCFK:MDP-two-views}, there is a 2-memory stochastic update strategy  for  such that 

Moreover, existence of such  is decidable in polynomial time and also  is computable in polynomial time (if it exists). We show how to transform, in polynomial time, the strategy  to the desired .

In~\cite{BBCFK:MDP-two-views}, the strategy  is constructed using a memoryless deterministic strategy  on  as follows: The strategy  has two memory elements, say . In  the strategy  behaves as a memoryless randomized strategy. After updating (stochastically) its memory element to , which may happen {\em only} in a BSCC of , the strategy  behaves as  and no longer updates its memory. 
Note that if  changes its memory element while still being in states of the form  then from this moment on the second component is always . However, such a strategy may be improved by moving to  (or to ) when its memory changes to  because the values of  in states of the form  are so large that moving to any state with  or  in the second component is better than staying in them. Obviously, there are only polynomially many improvements of this kind and all of them can be done in polynomial time.

So we may safely assume that the strategy  stays in  on states of , i.e. behaves as a memoryless randomized strategy on these states. We define the 3-memory stochastic update strategy  on  with memory elements   which in the memory element  mimics the behavior of  on states of the form . Once  chooses the action  (or ) the strategy  changes its memory element to  (or to ) and starts playing according to  (or to , resp.)
 
 Formally, we define
\begin{itemize}
\item ,  and 
\item   for all 
\item 
\item 
\item 
\end{itemize}
It is straightforward to verify that 




