\subsection{Proofs for Local Variance}
\label{app-local}
\subsubsection{Computation for Example~\ref{ex:local-mem}}\label{app:local-example}
We have
\begin{eqnarray*}
 \Ex{\sigma'}{s_1}{\lrvname} &=&  f(a)(0-\Ex{\sigma'}{s_1}{\lraname})^2 +  (f(b)+f(c))(2-\Ex{\sigma'}{s_1}{\lraname})^2\\
 &=& f(a)(-2+2f(a)))^2 + (1 - f(a))(2f(a))^2\\
 &=& 4f(a) - 8f(a)^2 + 4f(a)^3 + 4f(a)^2 - 4f(a)^3\\
 &=& 4f(a) - 4f(a)^2 \ge 0.64
\end{eqnarray*}

Throughout this section we use the following three simple lemmas. The first one allows us to reduce convex combinations of two-dimensional vectors (typically vectors consisting of the mean-payoff and variance) to combinations of just two vectors.
\begin{lemma}\label{lem:approx-two}
Let $(a_1,b_1), (a_2,b_2),\ldots, (a_m,b_m)$ be a sequence of points in $\Rset^2$ and $c_1,c_2,\ldots , c_m \in (0,1]$ satisfy $\sum_{i=1}^{m} c_i=1$. Then there are two vectors $(a_k,b_k)$ and $(a_{\ell},b_{\ell})$ and a number $p\in [0,1]$ such that
\[
\sum_{i=1}^{m} c_i (a_i,b_i)\quad  \geq \quad p (a_k,b_k) + (1-p) (a_{\ell},b_{\ell})
\]
\end{lemma}
\begin{proof}
Denote by $(x,y)$ the point $\sum_{i=1}^{m} c_i (a_i,b_i)$ and by $H$ the set $\{(a_i,b_i)\mid 1\leq i\leq m\}$.
If all the points of $H$ lie in the same line, then clearly there must be some $(a_k,b_k)\leq (x,y)$.
Assume that this is not true. Then the convex hull $\mathcal{C}(H)$ of $H$ is a convex polygon whose vertices are some of the points of $H$. Consider a point $(x',y)$ where $x'=\min\{z\mid z\leq x, (z,y)\in \mathcal{C}(H)\}$. The point $(x',y)$ lies on the boundary of $\mathcal{C}(H)$ and thus, as $\mathcal{C}(H)$ is a convex polygon, $(x',y)$ lies on the line segment between two vertices, say $(a_k,b_k),(a_{\ell},b_{\ell})$, of $\mathcal{C}(H)$. Thus there is $p\in [0,1]$ such that
\[(x',y)=p(a_k,b_k)+(1-p)(a_{\ell},b_{\ell})\leq (x,y)=\sum_{i=1}^{m} c_i (a_i,b_i)\,.\]
This finishes the proof.
\end{proof}
\noindent
The following lemma shows how to minimize the mean square deviation (to which our notion of variance is a special case).
\begin{lemma}\label{lem:min-var}
Let $a_1,\ldots,a_m\in \Rset$ such that $\sum_{i=0}^{m} a_i = 1$, let $r_1,\ldots,r_m\in \Rset$ and let us consider the following function of one real variable:
\[
V(x)=\sum_{i=1}^m a_i \left(r_i - x\right)^2
\]
Then the function $V$ has a unique minimum in $\sum_{i=1}^m a_i r_i$.
\end{lemma}
\begin{proof}
By taking the first derivative of $V$ we obtain
\[
\frac{\delta V}{\delta x} = -2\cdot  \sum_{i=1}^m a_i \left(r_i - x\right)
  = -2\cdot \left(\sum_{i=1}^m a_i r_i\right)+2x
\]
Thus $\frac{\delta{V}}{\delta x}(x)=0$ iff $x=\sum_{i=1}^m a_i r_i$.
Moreover, by taking the second derivative we obtain $\frac{\delta^2 V}{\delta x^2}=2>0$, and thus $\sum_{i=1}^m a_i r_i$ is a minimum.
\end{proof}
The following lemma shows that frequencies of actions determine (in some cases) the mean-payoff as well as the variance.
\begin{lemma}\label{lem:freq-var}
Let $\mu$ be a memoryless strategy and let $D$ be a BSCC of $G^{\mu}$. Consider frequencies of individual actions $a\in \BSCCact{D}$ when starting in a state $s\in \BSCCstate{D}$: $\Ex{\mu}{s}{\lraname^{I_a}}$ where $I_a$ assigns $1$ to $a$ and $0$ to all other actions (note that the values do not depend on which $s$ we choose).
Then $\Ex{\mu}{s}{\lraname^{I_a}}$ determine uniquely all of $\Ex{\mu}{s}{\lraname}$, $\Ex{\mu}{s}{\lrvhname}$, and $\Ex{\mu}{s}{\lrvlname}$ as follows: 
\[
\Ex{\mu}{s}{\lraname}=\sum_{a\in A} r(a)\cdot \Ex{\mu}{s}{\lraname^{I_a}}
\qquad
\text{and}
\qquad
\Ex{\mu}{s}{\lrvhname}=\Ex{\mu}{s}{\lrvlname}=\sum_{a\in A} (r(a)-\Ex{\mu}{s}{\lraname})^2\cdot \Ex{\mu}{s}{\lraname^{I_a}}
\]
\end{lemma}
\begin{proof}
We have
\[
\Ex{\mu}{s}{\lraname} = \Ex{\mu}{s}{\lim_{i\rightarrow \infty} \frac{1}{i}\cdot \sum_{j=1}^i r(A_j)} 
   =  \Ex{\mu}{s}{\lim_{i\rightarrow \infty} \frac{1}{i}\cdot \sum_{j=1}^i \sum_{a\in A} r(a) I_a(A_j)} 
   =  \sum_{a\in A} r(a)\cdot \Ex{\mu}{s}{\lim_{i\rightarrow \infty} \frac{1}{i}\cdot \sum_{j=1}^i I_a(A_j)} 
   =  \sum_{a\in A} r(a)\cdot \Ex{\mu}{s}{\lraname^{I_a}}
\]
and
\begin{multline*}
\Ex{\mu}{s}{\lrvhname} = 
\Ex{\mu}{s}{\lim_{i\rightarrow \infty} \frac{1}{i}\cdot \sum_{j=1}^i (r(A_j)-\Ex{\mu}{s}{\lraname})^2} 
  = 
  \Ex{\mu}{s}{\lim_{i\rightarrow \infty} \frac{1}{i}\cdot \sum_{j=1}^i \sum_{a\in A} (r(a)-\Ex{\mu}{s}{\lraname})^2\cdot I_a(A_j)} \\
  = \sum_{a\in A} (r(a)-\Ex{\mu}{s}{\lraname})^2 \cdot 
  \Ex{\mu}{s}{\lim_{i\rightarrow \infty} \frac{1}{i}\cdot \sum_{j=1}^i I_a(A_j)} 
  = \sum_{a\in A} (r(a)-\Ex{\mu}{s}{\lraname})^2\cdot \Ex{\mu}{s}{\lraname^{I_a}}
\end{multline*}
Finally, it is easy to see that the local and hybrid variance coincide in BSCCs since almost all runs have the same frequencies of actions. This gives us the result for the local variance.
\end{proof}

\medskip
\noindent
\subsubsection{Proof of Proposition~\ref{prop:strong-opt-local}.}\label{app-strong-opt}
We obtain the proof from the following slightly weaker version.
\begin{proposition}\label{prop:eps-opt-local}
Let us fix a MEC $C$ and let $\varepsilon>0$. There are two frequency functions $f_{\varepsilon}:\MECact{C}\rightarrow [0,1]$ and $f'_{\varepsilon}:\MECact{C}\rightarrow [0,1]$, and a number $p_{\varepsilon}\in [0,1]$ such that:
\[
p_{\varepsilon}\cdot (\lraname[f_{\varepsilon}],\lrvname[f_\varepsilon])+(1-p_{\varepsilon})\cdot
 (\lraname[f'_{\varepsilon}],\lrvname[f'_\varepsilon])\quad \leq \quad
(\Ex{\zeta}{s_0}{\lraname}, \Ex{\zeta}{s_0}{\lrvlname})+(\varepsilon,\varepsilon)
\]
\end{proposition}
\noindent
Before we prove Proposition~\ref{prop:eps-opt-local}, let us show that it indeed implies Proposition~\ref{prop:strong-opt-local}.
There is a sequence $\varepsilon_1,\varepsilon_2,\ldots$, two functions $f_C$ and $f'_C$, and $p_C\in [0,1]$ such that as $n\rightarrow \infty$ 
\begin{itemize}
\item $\varepsilon_n\rightarrow 0$ 
\item $f_{\varepsilon_n}$ converges pointwise to $f_C$
\item $f'_{\varepsilon_n}$ converges pointwise to $f'_C$
\item $p_{\varepsilon_n}$ converges to $p_C$
\end{itemize}
It is easy to show that $f_C$ as well as $f'_C$ are frequency functions. Moreover, 
as 
\[
\lim_{n\rightarrow \infty} (\Ex{\zeta}{s_0}{\lraname}, \Ex{\zeta}{s_0}{\lrvlname})+(\varepsilon_n,\varepsilon_n)=
(\Ex{\zeta}{s_0}{\lraname}, \Ex{\zeta}{s_0}{\lrvlname})
\]
and
\[
\lim_{n\rightarrow \infty}  p_{\varepsilon_n}\cdot (\lraname[f_{\varepsilon_n}],\lrvname[f_{\varepsilon_n}])+(1-p_{\varepsilon_n})\cdot
 (\lraname[f'_{\varepsilon_n}],\lrvname[f'_{\varepsilon_n}])
  =  p_C\cdot (\lraname[f_C],\lrvname[f_C])+(1-p_C)\cdot
 (\lraname[f'_C],\lrvname[f'_C])
\]
we obtain
\[
p_C\cdot (\lraname[f_C],\lrvname[f_C])+(1-p_C)\cdot
 (\lraname[f'_C],\lrvname[f'_C])\le (\Ex{\zeta}{s_0}{\lraname}, \Ex{\zeta}{s_0}{\lrvlname})
\]
This finishes a proof of Proposition~\ref{prop:strong-opt-local}. It remains to prove Proposition~\ref{prop:eps-opt-local}.

\medskip\noindent
\begin{proof}[Proof of Proposition~\ref{prop:eps-opt-local}.]
Given $\ell,k\in \Zset$ we denote by $A^{\ell,k}$ the set of all runs $\omega\in R_C$ such that
\[
(\ell\cdot \varepsilon,k\cdot \varepsilon) \quad \leq \quad (\lraname(\omega),\lrvlname(\omega))\quad < \quad (\ell\cdot \varepsilon,k\cdot \varepsilon)+(\varepsilon,\varepsilon)
\]
Note that 
\[
\sum_{\ell,k\in \Zset} \Prb_{s_0}^{\zeta}(A^{\ell,k}|R_C)\cdot (\ell\cdot \varepsilon,k\cdot \varepsilon) \quad \leq \quad
(\Ex{\zeta}{s_0}{\lraname|R_C}, \Ex{\zeta}{s_0}{\lrvlname|R_C})
\]
By Lemma~\ref{lem:approx-two}, there are $\ell,k,\ell',k'\in \Zset$ and $p\in [0,1]$ such that $\Prb_{s_0}^{\zeta}(A^{\ell,k}|R_C)>0$ and $\Prb_{s_0}^{\zeta}(A^{\ell',k'}|R_C)>0$ and 
\begin{equation}\label{eq:approx-two}
p\cdot (\ell\cdot \varepsilon,k\cdot \varepsilon)+(1-p)\cdot (\ell'\cdot \varepsilon,k'\cdot \varepsilon)\leq \sum_{\ell,k\in \Zset} \Prb_{s_0}^{\zeta}(A^{\ell,k}|R_C)\cdot (\ell\cdot \varepsilon,k\cdot \varepsilon)\leq
(\Ex{\zeta}{s_0}{\lraname|R_C}, \Ex{\zeta}{s_0}{\lrvlname|R_C})
\end{equation}
Let us concentrate on $(\ell\cdot \varepsilon,k\cdot \varepsilon)$ and construct a frequency function $f$ on $C$ such that
\[
 (\lraname[f],\lrvname[f])
\quad \leq \quad (\ell\cdot \varepsilon,k\cdot \varepsilon)+(\varepsilon,\varepsilon)
\]
Intuitively, we obtain $f$ as a vector of frequencies of individual actions on an appropriately chosen run of $R_C$. Such frequencies determine the average and variance close to $\ell\cdot \varepsilon$ and $k\cdot \varepsilon$, respectively. We have to deal with some technical issues, mainly with the fact that the frequencies might not be well defined for almost all runs (i.e. the corresponding limits might not exist). This is solved by a careful choice of subsequences as follows.
\begin{claim}\label{claim:subsequence-local}
For every run $\omega\in R_C$ there is a sequence of numbers $T_1[\omega],T_2[\omega],\ldots$ such that
all the following limits are defined:
\[
\lim_{i\rightarrow \infty} \frac{1}{T_i[\omega]} \sum_{j=1}^{T_i[\omega]} r(A_j(\omega))\quad = \quad \lraname(\omega)
\qquad
\text{and}
\qquad
\lim_{i\rightarrow \infty} \frac{1}{T_i[\omega]} \sum_{j=1}^{T_i[\omega]} (r(A_j(\omega))-\lraname(\omega))^2\quad \le \quad \lrvlname(\omega)
\]
and for every action $a\in A$ there is a number $f_{\omega}(a)$ such that
\[
\lim_{i\rightarrow \infty} \frac{1}{T_i[\omega]} \sum_{j=1}^{T_i[\omega]} I_a(A_j(\omega))\quad = \quad f_{\omega}(a)
\]
(Here $I_a(A_j(\omega))=1$ if $A_j(\omega)=a$, and $I_a(A_j(\omega))=0$ otherwise.)

Moreover, for almost all runs $\omega$ of $R_C$ we have that $f_{\omega}$ is a frequency function on $C$ and that 
$f_{\omega}$ determines $(\lraname(\omega),\lrvlname(\omega))$, i.e.,
$\lraname(\omega)=\lraname(f_{\omega})$ and $\lrvlname(\omega)\ge\lrvname(f_{\omega})$.
\end{claim}
\begin{proof}
We start by taking a sequence $T'_1[\omega],T'_2[\omega],\ldots$ such that
\[
\lim_{i\rightarrow \infty} \frac{1}{T'_i[\omega]} \sum_{j=1}^{T'_i[\omega]} r(A_j(\omega))\quad = \quad \lraname(\omega)
\]
Existence of such a sequence follows from the fact that every sequence of real numbers has a subsequence which converges to the lim sup of the original sequence.

Now we extract a subsequence $T''_1[\omega],T''_2[\omega],\ldots$ of $T'_1[\omega],T'_2[\omega],\ldots$ such that 
\begin{equation}\label{eq:subsequence-local}
\lim_{i\rightarrow \infty} \frac{1}{T''_i[\omega]} \sum_{j=1}^{T''_i[\omega]} (r(A_j(\omega))-\lraname(\omega))^2\quad \le \quad \lrvlname(\omega)
\end{equation}
using the same argument.

Now assuming an order on actions, $a_1,\ldots,a_m$, we define $T^{k}_1[\omega],T^{k}_2[\omega],\ldots$ for $0\leq k\leq m$ so that $T^{0}_1[\omega],T^{0}_2[\omega],\ldots$ is the sequence $T''_1[\omega],T''_2[\omega],\ldots$, and
every $T^{k+1}_1[\omega],T^{k+1}_2[\omega],\ldots$ is a subsequence of $T^{k}_1[\omega],T^{k}_2[\omega],\ldots$ 
such that the following limit exists (and is equal to a number $f_{\omega}(a_{k+1})$)
\[
\lim_{i\rightarrow \infty} \frac{1}{T^{k+1}_i[\omega]} \sum_{j=1}^{T^{k+1}_i[\omega]} I_{a_{k+1}}(A_j(\omega))
\]
We take $T^{m}_1[\omega],T^{m}_2[\omega],\ldots$ to be the desired sequence $T_1[\omega],T_2[\omega],\ldots$.

Now we have to prove that $f_{\omega}$ is a frequency function on $C$ for almost all runs of $R_C$. Clearly, $0\leq f_{\omega}(a)\leq 1$ for all $a\in \MECact{C}$. Also,
\[
\sum_{a\in \MECact{C}} f_{\omega}(a)=\sum_{a\in \MECact{C}} \lim_{i\rightarrow \infty} \frac{1}{T_i[\omega]} \sum_{j=1}^{T_i[\omega]} I_{a}(A_j(\omega))=\lim_{i\rightarrow \infty} \frac{1}{T_i[\omega]} \sum_{j=1}^{T_i[\omega]} \sum_{a\in \MECact{C}}I_{a}(A_j(\omega))
=\lim_{i\rightarrow \infty} \frac{1}{T_i[\omega]} \sum_{j=1}^{T_i[\omega]} 1=1
\]
To prove the third condition from the definition of frequency functions, we invoke the law of large numbers (SLLN) \cite{Billingsley:book}. Given a run $\omega$, an action $a$, a state $s$ and $k\geq 1$, define
\[
N^{a,s}_k(\omega)=\begin{cases}
  1 & \text{ $a$ is executed at least $i$ times, and $s$ is visited just after the $i$-th execution of $a$; }\\
  0 & \text{ otherwise.}
\end{cases}
\]
By SLLN and by the fact that in every step the distribution on the next states depends just on the chosen action, for almost all runs $\omega$ the following limit is defined and the equality holds whenever $f_\omega(a) > 0$:
\[
\lim_{j\rightarrow \infty} \frac{\sum_{k=1}^j N^{a,s}_k(\omega)}{j} = \delta(a)(s)
\]
We obtain
\begin{eqnarray*}
\sum_{a\in \MECact{C}} f_{\omega}(a)\cdot \delta(a)(s) & = &  \sum_{a\in \MECact{C}} \lim_{i\rightarrow \infty} \frac{1}{T_i[\omega]} \sum_{j=1}^{T_i[\omega]} I_a(A_j(\omega))\cdot   
  \lim_{i\rightarrow \infty} \frac{1}{i}\sum_{k=1}^{i} N^{a,s}_k(\omega) \\
& = & \sum_{a\in \MECact{C}} \lim_{i\rightarrow \infty} \frac{1}{T_i[\omega]} \sum_{j=1}^{T_i[\omega]} I_a(A_j(\omega))\cdot \lim_{i\rightarrow \infty} \frac{1}{\sum_{j=1}^{T_i[\omega]} I_a(A_j(\omega))}\sum_{k=1}^{\sum_{j=1}^{T_i[\omega]} I_a(A_j(\omega))} N^{a,s}_k(\omega) \\
& = & \sum_{a\in \MECact{C}} \lim_{i\rightarrow \infty} \frac{1}{T_i[\omega]}\sum_{k=1}^{\sum_{j=1}^{T_i[\omega]} I_a(A_j(\omega))} N^{a,s}_k(\omega) \\
& = & \lim_{i\rightarrow \infty} \frac{1}{T_i[\omega]}\sum_{a\in \MECact{C}} \sum_{k=1}^{\sum_{j=1}^{T_i[\omega]} I_a(A_j(\omega))} N^{a,s}_k(\omega) \\
& = & \lim_{i\rightarrow \infty} \frac{1}{T_i[\omega]}\sum_{j=1}^{T_i[\omega]} I_s(S_j(\omega)) \\
& = & \lim_{i\rightarrow \infty} \frac{1}{T_i[\omega]}\sum_{j=1}^{T_i[\omega]} \sum_{a\in \mathit{Act}(s)} I_a(A_j(\omega)) \\
& = & \sum_{a\in \mathit{Act}(s)}\lim_{i\rightarrow \infty} \frac{1}{T_i[\omega]}\sum_{j=1}^{T_i[\omega]}  I_a(A_j(\omega)) \\
& = & \sum_{a\in \mathit{Act}(s)} f_{\omega}(a)
\end{eqnarray*}
Here $S_j(\omega)$ is the $j$-th state of $\omega$, and $I_s(t)=1$ for $s=t$ and $I_s(t)=0$ otherwise.
\begin{eqnarray*}
\lraname(\omega) & = & \lim_{i\rightarrow \infty} \frac{1}{T_i[\omega]} \sum_{j=1}^{T_i[\omega]} r(A_j(\omega)) \\
& = & \lim_{i\rightarrow \infty} \frac{1}{T_i[\omega]} \sum_{j=1}^{T_i[\omega]} \sum_{a\in \MECact{C}} I_a(A_j(\omega))\cdot r(a) \\
& = & \sum_{a\in \MECact{C}} r(a)\cdot \lim_{i\rightarrow \infty} \frac{1}{T_i[\omega]} \sum_{j=1}^{T_i[\omega]} I_a(A_j(\omega)) \\
& = & \sum_{a\in \MECact{C}} r(a)\cdot f_{\omega}(a)\\
& = & \lraname[f_{\omega}]
\end{eqnarray*}
\begin{eqnarray*}
\lrvlname(\omega) & \ge & \lim_{i\rightarrow \infty} \frac{1}{T_i[\omega]} \sum_{j=1}^{T_i[\omega]} (r(A_j(\omega))-\lraname(\omega))^2 \\
& = & \lim_{i\rightarrow \infty} \frac{1}{T_i[\omega]} \sum_{j=1}^{T_i[\omega]} \sum_{a\in \MECact{C}} I_a(A_j(\omega))\cdot (r(a)-\lraname(\omega))^2 \\
& = & \sum_{a\in \MECact{C}} (r(a)-\lraname(\omega))^2 \cdot \lim_{i\rightarrow \infty} \frac{1}{T_i[\omega]} \sum_{j=1}^{T_i[\omega]} I_a(A_j(\omega)) \\
& = & \sum_{a\in \MECact{C}} (r(a)-\lraname(\omega))^2 \cdot f_{\omega}(a)\\
& = & \lrvname[f_{\omega}]
\end{eqnarray*}
\end{proof}
\noindent
Now pick an arbitrary run $\omega$ of $A^{k,\ell}$ such that $f_{\omega}$ is a frequency function. Then
\[
(\lraname(f_{\omega}),\lrvname(f_{\omega}))\le(\lraname(\omega),\lrvlname(\omega))\leq (\ell\cdot \varepsilon,k\cdot\varepsilon)+(\varepsilon,\varepsilon)
\]
Similarly, for $\ell',k'$ we obtain $f'_{\omega}$ such that 
\[
(\lraname(f'_{\omega}),\lrvname(f'_{\omega}))\le(\lraname(\omega),\lrvlname(\omega))\leq (\ell'\cdot \varepsilon,k'\cdot\varepsilon)+(\varepsilon,\varepsilon)
\]
This together with the equation~(\ref{eq:approx-two}) from page~\pageref{eq:approx-two} proves Proposition~\ref{prop:eps-opt-local}:
\begin{align*}
p\cdot (\lraname(f_{\omega}),\lrvname(f_{\omega})) +(1-p)\cdot  (\lraname(f'_{\omega}),\lrvname(f'_{\omega})) &
   \leq p\cdot \left((\ell\cdot \varepsilon,k\cdot \varepsilon)+(\varepsilon,\varepsilon)\right)+(1-p)\cdot \left((\ell'\cdot \varepsilon,k'\cdot \varepsilon)+(\varepsilon,\varepsilon)\right) \\
  & \leq  (\Ex{\zeta}{s_0}{\lraname|R_C}, \Ex{\zeta}{s_0}{\lrvlname|R_C})+(\varepsilon,\varepsilon)
\end{align*}
This finishes the proof of Proposition~\ref{prop:eps-opt-local}.
\end{proof}

\subsubsection{Details for proof of Proposition~\ref{prop:local-main}}\label{app-local-main}
We have
\[
\Ex{\zeta}{s_0}{\lraname}=\sum_{C\in \Mec(G)} \Prb(R_C)\cdot \Ex{\zeta}{s_0}{\lraname\mid R_C}
\qquad
\text{and}
\qquad
\Ex{\zeta}{s_0}{\lrvlname}=\sum_{C\in \Mec(G)} \Prb(R_C)\cdot \Ex{\zeta}{s_0}{\lrvlname\mid R_C}
\]
Here $\Ex{\zeta}{s_0}{\lraname\mid R_C}$ and $\Ex{\zeta}{s_0}{\lraname\mid R_C}$ are conditional expectations of $\lraname$ and $\lrvlname$, respectively, on runs of $R_C$.
Thus
\begin{equation}\label{eq:ev-complete}
(\Ex{\zeta}{s_0}{\lraname}, \Ex{\zeta}{s_0}{\lrvlname})\quad = \quad \sum_{C\in \Mec(G)} \Prb(R_C)\cdot \left(\Ex{\zeta}{s_0}{\lraname\mid R_C},\Ex{\zeta}{s_0}{\lrvlname\mid R_C}\right)
\end{equation}


We define memoryless strategies $\kappa$ and $\kappa'$ in $C$ as follows: Given $s\in \MECstate{C}$ such that $\sum_{b\in A(s)} f_C(b)>0$ and $a\in A(s)$, we put
\[
\kappa(s)(a)=f_C(a)\ /\ \sum_{b\in A(s)} f_C(b)
\qquad
\text{and}
\qquad
\kappa'(s)(a)=f_C(a)\ /\ \sum_{b\in A(s)} f_C(b)
\]
In the remaining states $s$ the strategy $\kappa$ (or $\kappa'$) behaves as a memoryless deterministic strategy reaching $\{s\in \MECstate{C}\mid \sum_{b\in \act{s}} f_C(b)>0\}$ (or $\{s\in \MECstate{C}\mid \sum_{b\in \act{s}} f'_C(b)>0\}$, resp.) with probability one.

Given a BSCC $D$ of $C^{\kappa}$ (or $D'$ of $C^{\kappa'}$), we write $f_C(D)=\sum_{a\in \BSCCact{D}} f_C(a)$ (or $f'_C(D')=\sum_{a\in \BSCCact{D'}} f'_C(a)$, resp.)

Denoting by $L$ the tuple $(\Ex{\zeta}{s_0}{\lraname|R_C}, \Ex{\zeta}{s_0}{\lrvlname|R_C})$ we obtain
\begin{eqnarray*}
L & = & p_C\cdot (\lraname[f_C],\lrvname[f_C]) + (1-p_C)\cdot
 (\lraname[f'_C],\lrvname[f'_C]) \\
  & = & \sum_{D\in \BSCCuni{C^{\kappa}}} p_C\cdot f_C(D)\cdot \left(\sum_{a\in \BSCCact{D}} \frac{f_C(a)}{f_C(D)}\cdot r(a),\sum_{a\in \BSCCact{D}} \frac{f_C(a)}{f_C(D)}\cdot (r(a)-\lraname[f_C])^2\right) \\
  & & +\ \sum_{D\in \BSCCuni{C^{\kappa'}}} (1-p_C)\cdot f'_C(D)\cdot \left(\sum_{a\in \BSCCact{D}} \frac{f'_C(a)}{f'_C(D)}\cdot r(a),\sum_{a\in \BSCCact{D}} \frac{f'_C(a)}{f'_C(D)}\cdot (r(a)-\lraname[f'_C])^2\right) \\
& \geq & \sum_{D\in \BSCCuni{C^{\kappa}}} p_C\cdot f_C(D)\cdot \left(\sum_{a\in \BSCCact{D}} \frac{f_C(a)}{f_C(D)}\cdot r(a),\sum_{a\in \BSCCact{D}} \frac{f_C(a)}{f_C(D)}\cdot (r(a)-\sum_{b\in \BSCCact{D}} \frac{f_C(b)}{f_C(D)}\cdot r(b))^2\right) \\
  & & +\ \sum_{D\in \BSCCuni{C^{\kappa'}}} (1-p_C)\cdot f'_C(D)\cdot \left(\sum_{a\in \BSCCact{D}} \frac{f'_C(a)}{f'_C(D)}\cdot r(a),\sum_{a\in \BSCCact{D}} \frac{f'_C(a)}{f'_C(D)}\cdot (r(a)-\sum_{b\in \BSCCact{D}} \frac{f'_C(b)}{f'_C(D)}\cdot r(b))^2\right) \\
  & = & \sum_{D\in \BSCCuni{C^{\kappa}}} p_C\cdot f_C(D)\cdot \left(\Exp_D(\lraname),\Exp_D(\lrvlname)\right)
    + \sum_{D\in \BSCCuni{C^{\kappa'}}} (1-p_C)\cdot f'_C(D)\cdot \left(\Exp_{D}(\lraname),\Exp_{D}(\lrvlname)\right) \\
\end{eqnarray*}
Here $\Exp_D(\lraname)$ and $\Exp_D(\lrvlname)$ denote the expected mean-payoff and the expected local variance, resp., on almost all runs of either $C^{\kappa}$ or $C^{\kappa'}$ initiated in any state of $D$ (note that almost all such runs have the same mean-payoff and the local variance due to ergodic theorem).
Note that the second equality follows from the fact that $f_C(a)>0$ (or $f'_C(a)>0$) iff $a\in \BSCCact{D}$ for a BSCC $D$ of $C^{\kappa}$ (or of $C^{\kappa'}$). The third inequality follows from Lemma~\ref{lem:min-var}. The last equality follows from Lemma~\ref{lem:freq-var} and the fact that $f_C(a)/f_C(D)$ is the frequency of firing $a$ on almost all runs initiated in $D$.

By~Lemma~\ref{lem:approx-two}, there are two components $D,D'\in \BSCCuni{C^{\kappa}}\cup \BSCCuni{C^{\kappa'}}$ and $0\leq d_C\leq 1 $ such that
\[
L  \quad \geq \quad d_C \cdot \left(\Exp_D(\lraname),\Exp_D(\lrvlname)\right) 
 + (1-d_C)\cdot \left(\Exp_{D'}(\lraname),\Exp_{D'}(\lrvlname)\right) 
\]
In what follows we use the following definition: Let $\nu$ be a memoryless randomized strategy on a MEC $C$ and let $K$ be a BSCC of $C^{\nu}$. We say that a strategy $\mu_K$ is {\em induced} by $K$ if 
\begin{enumerate}
\item $\mu_K(s)(a)=\nu(s)(a)$ for all $s\in \BSCCstate{K}$ and $a\in \BSCCact{K}$
\item in all $s\in S\smallsetminus (\BSCCstate{K})$ the strategy $\mu_K$ corresponds to a memoryless deterministic strategy which reaches a state of $K$ with probability one
\end{enumerate}
(Note that the above definition is independent of the strategy $\nu$ once it generates the same BSCC $K$.)

The strategies $\mu_D$ and $\mu_{D'}$ induced by $D$ and $D'$, resp., generate single-BSCC Markov chains $C^{\mu_D}$ and $C^{\mu_{D'}}$ satisfying for every state $s\in C\cap S$ the following
\begin{eqnarray*}
L &= & (\Ex{\zeta}{s_0}{\lraname|R_C}, \Ex{\zeta}{s_0}{\lrvlname|R_C}) \\
  &\geq & d_C \cdot \left(\Exp_D(\lraname),\Exp_D(\lrvlname)\right) 
 + (1-d_C)\cdot \left(\Exp_{D'}(\lraname),\Exp_{D'}(\lrvlname)\right) \\
  & = & d_C\cdot (\Ex{\mu_D}{s}{\lraname}, \Ex{\mu_D}{s}{\lrvlname}) + (1-d_C)\cdot (\Ex{\mu_{D'}}{s}{\lraname}, \Ex{\mu_{D'}}{s}{\lrvlname}) \\
  & = &  d_C\cdot (\Ex{\mu_D}{s}{\lraname}, \Ex{\mu_D}{s}{\lrvhname}) + (1-d_C)\cdot (\Ex{\mu_{D'}}{s}{\lraname}, \Ex{\mu_{D'}}{s}{\lrvhname}) 
\end{eqnarray*}
Here the last equality follows from the fact that almost all runs in $C^{\mu_{D}}$ (and also in $C^{\mu_{D'}}$) have the same mean-payoff. Thus for almost all runs the local variance is equal to the hybrid one. This shows that in $C$, a convex combination of two memoryless (possibly randomized) strategies is sufficient to optimize the mean-payoff and the local variance. 

Now we show that these strategies may be even deterministic.
\begin{claim}\label{prop:MR-MD}
Let $s\in S$.
There are {\em memoryless deterministic} strategies $\chi_1,\chi_2,\chi'_1,\chi'_2$ in $C$, each generating a single BSCC, and numbers $0\leq \nu,\nu'\leq 1$ such that
\[
(\Ex{\mu_D}{s}{\lraname}, \Ex{\mu_D}{s}{\lrvhname}) \ge  \nu\cdot (\Ex{\chi_1}{s}{\lraname}, \Ex{\chi_1}{s}{\lrvhname})+
(1-\nu)\cdot (\Ex{\chi_2}{s}{\lraname}, \Ex{\chi_2}{s}{\lrvhname})
 \geq  \nu\cdot (\Ex{\chi_1}{s}{\lraname}, \Ex{\chi_1}{s}{\lrvlname})+
(1-\nu)\cdot (\Ex{\chi_2}{s}{\lraname}, \Ex{\chi_2}{s}{\lrvlname})
\]
and
\[
(\Ex{\mu_{D'}}{s}{\lraname}, \Ex{\mu_{D'}}{s}{\lrvhname}) \ge \nu'\cdot (\Ex{\chi'_1}{s}{\lraname}, \Ex{\chi'_1}{s}{\lrvhname})+
(1-\nu')\cdot (\Ex{\chi'_2}{s}{\lraname}, \Ex{\chi'_2}{s}{\lrvhname})
 \geq \nu'\cdot (\Ex{\chi'_1}{s}{\lraname}, \Ex{\chi'_1}{s}{\lrvlname})+
(1-\nu')\cdot (\Ex{\chi'_2}{s}{\lraname}, \Ex{\chi'_2}{s}{\lrvlname})
\]
\end{claim}
\begin{proof}
It suffices to concentrate on $\mu_D$. 
By~\cite{derman1970finite}, $\Ex{\mu_D}{s_0}{\lraname^{I_a}}$ is equal to a convex combination of the values $\Ex{\iota_i}{s_0}{\lraname^{I_a}}$ for some memoryless deterministic strategies $\iota_1,\ldots,\iota_m$, i.e. there are $\gamma_1,\ldots,\gamma_m > 0$ such that $\sum_{i=1}^m \gamma_i=1$ and $\sum_{i=1}^m \gamma_i \cdot \Ex{\iota_i}{s_0}{\lraname^{I_a}} = \Ex{\mu_D}{s_0}{\lraname^{I_a}}$.
For all $1\le i \le m$ and $D\in\BSCCuni{C^{\iota_i}}$ denote
$\iota_{i,D}$ a memoryless deterministic strategy such that $\iota_{i,D}(s)=\iota_{i}(s)$ on all $s\in D\cap S$, and on other states $\iota_{i,D}$ is defined so that
$D\cap S$ is reached with probability 1, independent of the starting state.
For all $a\in D\cap A$ we have $\Ex{\iota_{i,D}}{s_0}{\lraname^{I_a}} = \Pr{\iota_i}{s_0}{\reach(D)}\cdot \Ex{\mu_D}{s_0}{\lraname^{I_a}}$,
while for $a\not\in D\cap A$ we have $\Ex{\iota_{i,D}}{s_0}{\lraname^{I_a}} = 0$. Hence
$\sum_{i=1}^m\sum_{D\in\BSCCuni{C^{\iota_i}}} \gamma_i \cdot\Pr{\iota_i}{s_0}{\reach(D)} \cdot \Ex{\iota_{i,D}}{s_0}{\lraname^{I_a}} = \Ex{\iota_i}{s_0}{\lraname^{I_a}}$.
Since $\sum_{i=1}^m\sum_{D\in\BSCCuni{C^{\iota_i}}} \gamma_i \cdot\Pr{\iota_i}{s_0}{\reach(D)}=1$, we apply Lemma~\ref{lem:approx-two} and get
there are two memoryless deterministic single-BSCC strategies $\chi_1,\chi_2$ and $0\leq \nu\leq 1$ such that
\[
\Ex{\mu_D}{s_0}{\lraname^{I_a}}=\nu \Ex{\chi_1}{s_0}{\lraname^{I_a}}+ (1-\nu) \Ex{\chi_2}{s_0}{\lraname^{I_a}}
\]
which together with Lemma~\ref{lem:freq-var} implies that
\begin{eqnarray*}
\Ex{\mu_D}{s}{\lraname} & = & \sum_{a\in A} r(a)\cdot \Ex{\mu_D}{s}{\lraname^{I_a}} \\
  & = & \sum_{a\in A} r(a)\cdot \left(\nu \Ex{\chi_1}{s}{\lraname^{I_a}}+(1-\nu) \Ex{\chi_2}{s}{\lraname^{I_a}}\right) \\
  & = & \nu \sum_{a\in A} r(a)\cdot \Ex{\chi_1}{s}{\lraname^{I_a}} + (1-\nu)\sum_{a\in A} r(a)\cdot \Ex{\chi_2}{s}{\lraname^{I_a}} \\
  & = & \nu \Ex{\chi_1}{s}{\lraname}+(1-\nu) \Ex{\chi_2}{s}{\lraname}
\end{eqnarray*}
and
\begin{eqnarray*}
\Ex{\mu_D}{s}{\lrvhname} & = & \sum_{a\in A} (r(a)-\Ex{\mu_D}{s}{\lraname})^2\cdot \Ex{\mu_D}{s}{\lraname^{I_a}} \\
  & = & \sum_{a\in A} (r(a)-\Ex{\mu_D}{s}{\lraname})^2\cdot (\nu\Ex{\chi_1}{s}{\lraname^{I_a}}+(1-\nu)\Ex{\chi_2}{s}{\lraname^{I_a}}) \\
  & = & \nu \sum_{a\in A} (r(a)-\Ex{\mu_D}{s}{\lraname})^2\cdot \Ex{\chi_1}{s}{\lraname^{I_a}} +
  (1-\nu) \sum_{a\in A} (r(a)-\Ex{\mu_D}{s}{\lraname})^2\cdot \Ex{\chi_2}{s}{\lraname^{I_a}} \\
  & \geq & \nu \sum_{a\in A} (r(a)-\Ex{\chi_1}{s}{\lraname})^2\cdot \Ex{\chi_1}{s}{\lraname^{I_a}} +
  (1-\nu) \sum_{a\in A} (r(a)-\Ex{\chi_2}{s}{\lraname})^2\cdot \Ex{\chi_2}{s}{\lraname^{I_a}} \\
  & = & \nu \Ex{\chi_1}{s}{\lrvhname} +
  (1-\nu) \Ex{\chi_2}{s}{\lrvhname}
\end{eqnarray*}
Here the inequality follows from Lemma~\ref{lem:min-var}. So
\[
(\Ex{\mu_D}{s}{\lraname},\Ex{\mu_D}{s}{\lrvhname})\geq \nu (\Ex{\chi_1}{s}{\lraname},\Ex{\chi_1}{s}{\lrvhname}) + (1-\nu) (\Ex{\chi_2}{s}{\lraname},\Ex{\chi_2}{s}{\lrvhname})
\]
Finally, we show that $\Ex{\chi_1}{s}{\lrvhname}\geq \Ex{\chi_1}{s}{\lrvlname}$. Since $\chi_1$ has a single BSCC, almost all runs have the same mean payoff. Hence, $\Ex{\chi_1}{s}{\lrvhname}= \Ex{\chi_1}{s}{\lrvlname}$.
\end{proof}
\noindent
By Claim~\ref{prop:MR-MD},
 \begin{eqnarray*}
 L & \geq & d_C\cdot (\Ex{\mu_D}{s}{\lraname}, \Ex{\mu_D}{s}{\lrvhname}) +(1-d_C)\cdot
  (\Ex{\mu_{D'}}{s}{\lraname}, \Ex{\mu_{D'}}{s}{\lrvhname}) \\
   & \geq & d_C\cdot \nu\cdot (\Ex{\chi_1}{s}{\lraname}, \Ex{\chi_1}{s}{\lrvlname})
    +  d_C\cdot (1-\nu) \cdot (\Ex{\chi_2}{s}{\lraname}, \Ex{\chi_2}{s}{\lrvlname}) \\
   & & +\  (1-d_C) \cdot \nu' \cdot (\Ex{\chi'_1}{s}{\lraname}, \Ex{\chi'_1}{s}{\lrvlname})
    + (1-d_C) \cdot (1-\nu') \cdot (\Ex{\chi'_2}{s}{\lraname}, \Ex{\chi'_2}{s}{\lrvlname})
 \end{eqnarray*}
and so by Lemma~\ref{lem:approx-two}, there are $\pi_C, \pi'_C \in \{\chi_1,\chi_2,\chi'_1,\chi'_2\}$ and a number $h_C$ such that
\begin{eqnarray*}
L &= & (\Ex{\zeta}{s_0}{\lraname|R_C}, \Ex{\zeta}{s_0}{\lrvlname|R_C}) \\
  & \ge & h_C\cdot (\Ex{\pi_C}{s}{\lraname}, \Ex{\pi_C}{s}{\lrvlname}) + (1-h_C)\cdot (\Ex{\pi'_C}{s}{\lraname}, \Ex{\pi'_C}{s}{\lrvlname})
\end{eqnarray*}
Define memoryless deterministic strategies $\pi$ and $\pi'$ in $G$ so that for every $s\in S$ and $a\in A$ we have
$\pi(s)(a):=\pi_C(s)(a)$ and $\pi'(s)(a):=\pi'_C(s)(a)$ for $s\in \MECstate{C}$.





\subsubsection{Proof of Equation~(\ref{eqn-t2})}\label{app-eqn-t2}
We have
\begin{eqnarray*}
\lefteqn{(\Ex{\zeta}{s_0}{\lraname},\Ex{\zeta}{s_0}{\lrvlname}) }\\& = & \!\!\Big(\!\!\!\!\sum_{C\in \Mec(G)}\!\!\!\! \Pr{\zeta}{s_0}{R_C}\cdot \Ex{\zeta}{s_0}{\lraname\mid R_C},
   \!\!\!\!\sum_{C\in \Mec(G)}\!\!\!\! \Pr{\zeta}{s_0}{R_C}\cdot \Ex{\zeta}{s_0}{\lrvlname\mid R_C}\Big) \\
  & \geq  & \!\!\Big(\!\!\!\!\sum_{C\in \Mec(G)}\!\!\!\! \Pr{\sigma}{s_0}{R_C}{\cdot} h_C {\cdot} \Exp^{\pi}_{s[C]}[\lraname] + 
 \Pr{\sigma}{s_0}{R_C}{\cdot} (1{-}h_C) {\cdot} \Exp^{\pi'}_{s[C]}[\lraname], \\
  & &\; \sum_{C\in \Mec(G)}\!\!\!\! \Pr{\sigma}{s_0}{R_C}{\cdot} h_C {\cdot} \Exp^{\pi}_{s[C]}[\lrvlname] + 
 \Pr{\sigma}{s_0}{R_C}{\cdot} (1{-}h_C) {\cdot} \Exp^{\pi'}_{s[C]}[\lrvlname]\Big)\\
  & = &\!\! (\Ex{\sigma}{s_0}{\lraname},\Ex{\sigma}{s_0}{\lrvlname})
\end{eqnarray*}
Here $s[C]$ is an arbitrary state of $\MECstate{C}$.
\subsubsection{Proof of Theorem~\ref{thm:local-np-alg}}\label{app-local-np-alg}
First, we show that if there is $\zeta$ in $G$ such that
$(\Ex{\zeta}{s_0}{\lraname}, \Ex{\zeta}{s_0}{\lrvlname})\leq (u,v)$, then there is a strategy $\rho$ in $G[\pi,\pi']$ such that $(\Ex{\rho}{s_{in})}{\lraname^{r_1}},\Ex{\rho}{s_{in}}{\lraname^{r_2}})\leq (u,v)$. 
Consider the 3-memory stochastic update strategy $\sigma$ from Proposition~\ref{prop:local-main} satisfying
$(\Ex{\sigma}{s_0}{\lraname}, \Ex{\sigma}{s_0}{\lrvlname})\leq (u,v)$. Define a memoryless strategy $\rho$ in $G[\pi,\pi']$ that mimics $\sigma$ as follows (we denote the only memory element of $\rho$ by $\bullet$):
\begin{itemize}
\item $\rho(s_{in},\bullet)(\mathit{default}) = \alpha(m_1)$,  $\rho(s_{in},\bullet)([\pi]) = \alpha(m_2)$, $\rho(s_{in},\bullet)([\pi']) = \alpha(m'_2)$,
\item $\rho((s,m_1),\bullet)(a)=\sigma_n(s,m_1)(a)\cdot \sigma_u(a,s,m_1)(m_1)$  for all $a\in A$
\item $\rho((s,m_1),\bullet)(\pi)=\sigma_u(a,s,m_1)(m_2)$
\item $\rho((s,m_1),\bullet)(\pi')=\sigma_u(a,s,m_1)(m'_2)$
\item $\rho((s,m_2),\bullet)(\mathit{default})=\rho((s,m'_2),\bullet)(\mathit{default})=1$
\end{itemize}
It is straightforward to verify that 
\[
(\Ex{\sigma}{s_0}{\lraname}, \Ex{\sigma}{s_0}{\lrvlname})\quad = \quad (\Ex{\rho}{s_{in}}{\lraname^{r_1}}, \Ex{\rho}{s_{in}}{\lraname^{r_2}})\quad \leq \quad (u,v)
\]


\noindent
Second, we show that if there is $\rho'$ in $G[\pi,\pi']$ satisfying 
$(\Ex{\rho'}{s_{in}}{\lraname^{r_1}}, \Ex{\rho'}{s_{in}}{\lraname^{r_2}}) \leq (u,v)$, then
there is the desired 3-memory stochastic update strategy $\sigma$ in $G$. Moreover, we show that existence of such $\sigma$ is decidable in polynomial time and also that the strategy is computable in polynomial time (if it exists).

By~\cite{BBCFK:MDP-two-views}, there is a 2-memory stochastic update strategy $\sigma'$ for $G[\pi,\pi']$ such that 
\[(\Ex{\sigma'}{s_{in}}{\lraname^{r_1}}, \Ex{\sigma'}{s_{in}}{\lraname^{r_2}})\leq (u,v)\]
Moreover, existence of such $\sigma'$ is decidable in polynomial time and also $\sigma'$ is computable in polynomial time (if it exists). We show how to transform, in polynomial time, the strategy $\sigma'$ to the desired $\sigma$.

In~\cite{BBCFK:MDP-two-views}, the strategy $\sigma'$ is constructed using a memoryless deterministic strategy $\xi$ on $G[\pi,\pi']$ as follows: The strategy $\sigma'$ has two memory elements, say $n_1,n_2$. In $n_1$ the strategy $\sigma'$ behaves as a memoryless randomized strategy. After updating (stochastically) its memory element to $n_2$, which may happen {\em only} in a BSCC of $G[\pi,\pi']^{\xi}$, the strategy $\sigma'$ behaves as $\xi$ and no longer updates its memory. 
Note that if $\sigma'$ changes its memory element while still being in states of the form $(s,m_1)$ then from this moment on the second component is always $m_1$. However, such a strategy may be improved by moving to $(s,m_2)$ (or to $(s,m'_2)$) when its memory changes to $n_2$ because the values of $\vec{r}$ in states of the form $(s,m_1)$ are so large that moving to any state with $m_2$ or $m'_2$ in the second component is better than staying in them. Obviously, there are only polynomially many improvements of this kind and all of them can be done in polynomial time.

So we may safely assume that the strategy $\sigma'$ stays in $n_1$ on states of $\{(s,m_1)\mid s\in S\}$, i.e. behaves as a memoryless randomized strategy on these states. We define the 3-memory stochastic update strategy $\sigma$ on $G$ with memory elements $m_1,m_2,m'_2$  which in the memory element $m_1$ mimics the behavior of $\sigma'$ on states of the form $(s,m_1)$. Once $\sigma'$ chooses the action $[\pi]$ (or $[\pi']$) the strategy $\sigma$ changes its memory element to $m_2$ (or to $m'_2$) and starts playing according to $\pi$ (or to $\pi'$, resp.)
 
 Formally, we define
\begin{itemize}
\item $\alpha(m_1)=\sigma'_n(s_{in},n_1)(\mathit{default})$, $\alpha(m_1)=\sigma'_n(s_{in},n_1)([\pi])$ and $\alpha(m_1)=\sigma'_n(s_{in},n_1)([\pi'])$
\item $\sigma_n(s,m_1)(a)=\sigma'_n((s,m_1),n_1)(a)\  / \ \sum_{b\in A} \sigma'_n((s,m_1),n_1)(b)$  for all $a\in A$
\item $\sigma_u(a,s,m_1)(m_1)=\sum_{b\in A} \sigma'_n((s,m_1),n_1)(b)$
\item $\sigma_u(a,s,m_1)(m_2)=\sigma'_n(a,(s,m_1),n_1)([\pi])$
\item $\sigma_u(a,s,m_1)(m'_2)=\sigma'_n(a,(s,m_1),n_1)([\pi'])$
\end{itemize}
It is straightforward to verify that 
\[
(\Ex{\sigma}{s_0}{\lraname}, \Ex{\sigma}{s_0}{\lrvlname})\quad = \quad (\Ex{\sigma'}{s_{in}}{\lraname}, \Ex{\sigma'}{s_{in}}{\lrvlname})\quad \leq \quad (u,v)
\]



