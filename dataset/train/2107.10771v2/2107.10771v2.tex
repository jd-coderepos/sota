
\section{Experiments}

\noindent \textbf{Datasets.}
We evaluate our method on several large-scale video datasets with different properties, requiring our models to understand different aspects of action recognition task.

{\textit{Something-Something}} includes V1~\cite{goyal2017something} and V2~\cite{mahdisoltani2018fine} versions, which are two large-scale crowd-sourcing video datasets for action recognition.
There are about 110k (V1) and 220k (V2) videos covering 174 fine-grained action categories with diverse objects and scenes,
focusing on humans performing pre-defined basic actions.
In this dataset, the actions are performed with different objects so that models are required to understand the basic actions instead of recognizing the appearance of the objects or the background scenes.
Moreover, the spatial and the temporal scales of the objects and the events vary hugely across different videos, as shown in Fig.~\ref{fig_1},
which is suitable for verifying the flexible spatial-temporal modeling ability of the proposed method.





{\textit{Kinetics}}~\cite{carreira2017quo} is a challenging human action recognition dataset, which contains 400 and 600 human action classes.
This dataset includes human-object interactions such as playing instruments, as well as human-human interactions such as shaking hands and hugging.
Compared to the temporal reasoning required by the actions in Something-Something, the actions in this dataset heavily rely on the appearance of the objects.
We evaluate our models on the trimmed version to evaluate its capacity in modeling the appearances and the interaction among objects.
The experiments are conducted on the validation set of Kinetics-400~\cite{carreira2017quo} because there are many well-known baseline methods.



{\textit{Diving48}}~\cite{li2018resound} includes more than 18K video clips for 48 unambiguous diving classes.
This proves to be a challenging task for modern action recognition systems as dives include three stages (takeoff, flight, entry) and thus require modeling of long-term temporal dynamics.
This requires both multi-scale temporal modeling and the perceiving of long-range dependencies.
Therefore, we conduct experiments on this dataset to verify the multi-scale spatial-temporal modeling ability of our method.
We report the accuracy on the first version of the official validation split, which has been adopted by several previous methods.\footnote{http://www.svcl.ucsd.edu/projects/resound/Diving48\_\{train/test\}.json}



\noindent \textbf{Implementation Detail}
We implement our model in Pytorch, and we adopt ResNet50~\cite{he2016deep} pretrained on ImageNet~\cite{deng2009imagenet} as the backbone.
Following previous works~\cite{kwon2020motionsqueeze}\cite{li2020tea}, we also insert temporal convolutions with kernel size 3 and the motion excitation (ME) module proposed in \cite{li2020tea} before each 33 convolutions of bottleneck layers of the original ResNet50, aiming to enhance its basic temporal modeling ability. We also incorporate these changes into all baselines in the ablation study for a fair comparison. 
The parameters within the EABs and SOI-Tr are randomly initialized.
For the spatial dimension of the sampled clips, the short-side of the frames are resized to  and then cropped to . We perform random cropping and flipping as data augmentation during training.
It's worth mentioning that we do not perform horizontal flipping on the moving direction related action classes such as \textit{``moving something from left to right''}.
We train the network with a batch size of 64 and optimize it using SGD with an initial learning rate of 0.01 for 40 epochs, and decay it by a factor of 10 for every 10 epochs. The total training epochs are about 70.
The dropout ratio is set to 0.5. The weight decay is set to  and  for Something/Diving48 and Kinetics-400, respectively.\footnote{We adopt the same hyper-parameter settings as \href{https://github.com/MCG-NJU/TDN}{the official codebase of TDN}
for a fair comparison.}






\spaceabovesubsection
\subsection{Comparison with State-of-the-Arts}



 \textbf{Something V1 and V2.}
We first compare our method with the other state-of-the-art approaches on Something V1 and Something V2 datasets, as shown in Tab.~\ref{tab_something_sota}.
The previous approaches are divided into four groups: 3D CNNs, object interaction modeling enhanced 3D CNNs, 2D CNNs, and 2D CNNs enhanced with short-term motion representation.

\begin{table*}[!ht]
	\centering
	\caption{Comparison to state-of-the-arts on Something-Something V1\&V2 datasets.
	Following TDN~\cite{wang2020tdn}, we adopt the \textit{1-clip and center-crop} inference scheme where only a center crop of 224224 from a single clip is used for evaluation.
	\textsubscript{8F} and \textsubscript{16F} indicate the sampling segment number of the input video is 8 and 16, respectively.
	The result of EAN is produced by averaging the predicted action scores from the EAN and EAN models, which follows TSM~\cite{lin2019tsm}.
	 indicates the paper didn't provide the results.
	}
	\scalebox{1.0}{
		\renewcommand{\arraystretch}{1.0}
		\setlength{\tabcolsep}{0.8mm}
		
		\begin{tabular}{ccccccccccc}
			\Xhline{2\arrayrulewidth}
			\multirow{2}{*}{\tabincell{c}{\textbf{Method}} } &
			\multirow{2}{*}{\tabincell{c}{\textbf{Backbone}} } &
			\multirow{2}{*}{\tabincell{c}{\textbf{Pre-train}} } &
			\multirow{2}{*}{\tabincell{c}{\textbf{Frames}} } &
			\multirow{2}{*}{\tabincell{c}{\textbf{GFLOPs}} } &
			\multicolumn{2}{c}{\textbf{Something V1}}  & \multicolumn{2}{c}{\textbf{Something V2}} \\
			&&&&& \textbf{Top1 (\%)} & \textbf{Top5 (\%)}  & \textbf{Top1 (\%)} & \textbf{Top5 (\%)}\\
			\hline 
			\multicolumn{2}{l}{\tabincell{l}{\textbf{3D CNNs:}}} \\
			
			I3D~\cite{carreira2017quo}  & 3D-ResNet50 & Kinetics & 322 & 306 & 41.6 &72.2 &-&- \\ 
			Non-local I3D~\cite{wang2018non}  & 3D-ResNet50 & Kinetics & 322 & 336 & 44.4&76.0 &-&- \\ 
			\arrayrulecolor{mygray}\cdashline{1-9}[5pt/3pt]
			ECO(En)~\cite{zolfaghari2018eco} & BNInc+3D-ResNet18 & Kinetics & 92  & 267  & 46.4 &- &-&- \\  
			\arrayrulecolor{gray}\cdashline{1-9}[5pt/3pt]
			S3D-G~\cite{xie2018rethinking}  & InceptionV1 & ImageNet & 64  & 71  & 48.2&78.7 &-&- \\
			\hline
			\multicolumn{2}{l}{\tabincell{l}{\textbf{3D CNNs + Object interaction:}}} \\
			
			GCN + Non-local~\cite{wang2018videos} & 3D-ResNet50 & Kinetics & 322 & 606 & 46.1&76.8 &-&- \\
			I3D + STIN + OIE~\cite{materzynska2020something} & I3D & Kinetics & 32 & 154 & - &- & 60.2 &84.4 \\
			\hline 	
			\multicolumn{2}{l}{\tabincell{l}{\textbf{2D CNNs:}}} \\
			TSN~\cite{wang2016temporal}  & BN-Inception & ImageNet & 8  & 16  & 19.5 &- &33.4 & -\\ 
			MultiScale TRN~\cite{zhou2018temporal}  & BN-Inception & ImageNet & 8  & 16  & 34.4 &63.2 &48.8 &77.6  \\ 
			\arrayrulecolor{gray}\cdashline{1-9}[5pt/3pt]
			TSM~\cite{lin2019tsm}   & ResNet-50 & Kinetics & 8  & 33  & 45.6 &74.2 &58.8&85.4  \\ 
			TSM~\cite{lin2019tsm}   & ResNet-50 & Kinetics & 16  & 65  & 47.2 &77.1 &63.4& 88.5 \\ 
			\arrayrulecolor{gray}\cdashline{1-9}[5pt/3pt]
			TANet~\cite{liu2020tam}  & ResNet-50 & ImageNet & 8  & 33 & 46.5 &75.8& 60.5&86.2 \\
			TANet~\cite{liu2020tam}  & ResNet-50 & ImageNet & 16  & 66 & 47.6& 77.7& 62.5&87.6 \\
			TANet~\cite{liu2020tam}  & ResNet-50 & ImageNet & 8+16  & 99 & 50.6& 79.3 & -&- \\
			\arrayrulecolor{gray}\cdashline{1-9}[5pt/3pt]
			TEINet~\cite{liu2020teinet}  & ResNet-50 & ImageNet & 8  & 33 & 47.4 &-& 61.3&- \\
			TEINet~\cite{liu2020teinet}  & ResNet-50 & ImageNet & 16  & 66 & 49.9& -& 62.1&- \\
			TEINet~\cite{liu2020teinet}  & ResNet-50 & ImageNet & 8+16  & 99 & 52.5& -& 65.5&89.8 \\
			\arrayrulecolor{gray}\cdashline{1-9}[5pt/3pt]
			STM~\cite{jiang2019stm}   & ResNet-50 & ImageNet & 830 & 990 & 49.2&79.3 &62.3&88.8 \\ 
			STM~\cite{jiang2019stm}   & ResNet-50 & ImageNet & 1630 & 2010 & 50.7&80.4 &64.2&89.8 \\ 
			\arrayrulecolor{gray}\cdashline{1-9}[5pt/3pt]
			GST~\cite{luo2019grouped}  & ResNet-50 & ImageNet & 8  & 29  & 47.0&76.1 &-&- \\
			GST~\cite{luo2019grouped}  & ResNet-50 & ImageNet & 16  & 59  & 48.6&77.9 & 62.6&87.9\\ 
			\arrayrulecolor{gray}\cdashline{1-9}[5pt/3pt]
			TEA~\cite{li2020tea}  & ResNet-50 & ImageNet & 8  & 35  & 48.9&78.1 &-&-\\
			TEA~\cite{li2020tea}  & ResNet-50 & ImageNet & 16  & 70  & 51.9&80.3 &-&-\\ 
			TEA~\cite{li2020tea}  & ResNet-50 & ImageNet & 1630 & 2100 & 52.3&81.9 &65.1&89.9\\  
			\arrayrulecolor{gray}\cdashline{1-9}[5pt/3pt]
			\arrayrulecolor{gray}\cdashline{1-9}[5pt/3pt]
			
			
			EAN(Ours) & ResNet-50 & ImageNet & 8 & 36  &{51.9} &{79.5} &{63.5}&{88.2}\\ 
			EAN(Ours) & ResNet-50 & ImageNet & 16  & 72  & {53.4} & {81.4} &{64.6}&{89.1} \\ 
			EAN(Ours)  & ResNet-50 & ImageNet & 8+16 & 108  & \textbf{55.8} & \textbf{83.1} &\textbf{66.6}&\textbf{89.9}\\
			
			\hline 	
			\multicolumn{2}{l}{\tabincell{l}{\textbf{2D CNNs + Short-term motion:}}} \\
			TRN~\cite{zhou2018temporal}  & BN-Inception & ImageNet & 87  & - & 42.0 &- &55.5 & 83.1  \\ 
			TSM~\cite{lin2019tsm}  & ResNet-50 & ImageNet & 167  & - & 52.6& 81.9& 66.0& 90.5&  \\
			\arrayrulecolor{gray}\cdashline{1-9}[5pt/3pt]
			PAN~\cite{zhang2020pan} & ResNet-50 & ImageNet &  85  		& 68 & 50.5 &79.2 &63.8 &88.6 \\
			PAN~\cite{zhang2020pan} & ResNet-101 & ImageNet &  (85)2  &  503 &55.3 &82.8 &66.5 &90.6 \\
			\arrayrulecolor{gray}\cdashline{1-9}[5pt/3pt]
			TDN~\cite{wang2020tdn} & ResNet-50 & ImageNet &  85  		& 36 & 52.3 & 80.6 &64.0 &88.8 \\
			TDN~\cite{wang2020tdn} & ResNet-50 & ImageNet &  165  		& 72 & 53.9& 82.1 &65.3 &89.5 \\
			TDN~\cite{wang2020tdn} & ResNet-50 & ImageNet &  (8+16)5  		& 108 & 55.1 & 82.9 &67.0& 90.3 \\
			\arrayrulecolor{gray}\cdashline{1-9}[5pt/3pt]
			EAN(Ours) & ResNet-50 & ImageNet &  85  		& 37 & {53.4} & {81.1} &{65.2} &{89.4} \\
			EAN(Ours) & ResNet-50 & ImageNet &  165  		& 74 & {54.7}& {82.3} &{66.6} &{90.3} \\
			EAN(Ours) & ResNet-50 & ImageNet &  (8+16)5  		& 111 & \textbf{57.2} & \textbf{83.9} &\textbf{68.8}& \textbf{91.4} \\
			\Xhline{2\arrayrulewidth}
		\end{tabular}
	}
	
	\label{tab_something_sota}
\end{table*}

Our method outperforms all methods built with 3D convolutions and meanwhile achieves higher efficiency. For example, compared with Non-local I3D~\cite{wang2018non}, our EAN\textsubscript{8F(RGB+LMC)} model achieves 8.8\% higher Top1 accuracy (44.4\% \textit{vs.} 53.2\% on Something V1) with only  computational cost.


\begin{table*}[!th]
	
	\caption{Comparison to state-of-the-arts on Kinetics-400 dataset.
	Following TDN~\cite{wang2020tdn}, we adopt the \textit{10-clip and 3-crop} inference scheme where three crops of 256256 frames and 10 clips are used for testing.
	Therefore, the computational cost of the same model here is 30 heavier than that on Something datasets.
	 indicates the paper didn't provide the results.}
	\centering
	\scalebox{1.02}{
		\renewcommand{\arraystretch}{1.0}
		\setlength{\tabcolsep}{2.0mm}
		\begin{tabular}{ccccccc}
			\Xhline{2\arrayrulewidth}
			\textbf{Method} & \textbf{Backbone} & \textbf{Pre-train} & \textbf{Frames} & \textbf{GFLOPs} & \textbf{Top1 (\%)} & \textbf{Top5 (\%)} \\
			\hline
			ARTNet~\cite{wang2018appearance}  & ResNet-18 & ImageNet & 16250 & 23.5250 & 70.7 &89.3 \\ 
			I3D~\cite{carreira2017quo}  & Inception V1 & ImageNet & 64N/A & 108N/A & 72.1 &90.3 \\ 
			I3D~\cite{carreira2017quo}  & Inception V1 & None & 64N/A & 108N/A & 67.5 &87.2 \\ 
			I3D+NL~\cite{wang2018non}  & 3D-ResNet-101 & ImageNet & 3260 & 35960 & 77.7&93.3 \\ 
			
			ECO(En)~\cite{zolfaghari2018eco} & BNInc\&3D-ResNet-18 & None & 92  & 267  & 70.0 &- \\ 
			
			SlowOnly~\cite{feichtenhofer2019slowfast} & 3D-ResNet-50 & None & 830 & 41.930 & 74.8 & 91.6 \\
			SlowFast~\cite{feichtenhofer2019slowfast} & 3D-ResNet-50 & None & (4+32)30 & 36.130 & 75.6 & 92.1 \\ 
			SlowFast+NL~\cite{feichtenhofer2019slowfast} & 3D-ResNet-101 & None & (16+64)30 & 23430 & \textbf{79.8} & \textbf{93.9} \\ 
			
			\hline 	
			TSN~\cite{wang2016temporal}  & BN-Inception & ImageNet & 2510 & 5310 & 69.1 &88.7 \\ 
			TSN~\cite{wang2016temporal}  & Inception v3 & ImageNet & 2510 & 8010 & 72.5 &90.2 \\ 
			
			R(2+1)D~\cite{tran2018closer}  & ResNet-34 & None & 3210 & 15210 & 72.0 & 90.0 \\ 
			
			TSM~\cite{lin2019tsm}   & ResNet-50 & ImageNet & 830 & 3330 & 74.1 &- \\ 
			TSM~\cite{lin2019tsm}   & ResNet-50 & ImageNet & 1630 & 6530 & 74.7 &-  \\ 
			
			STM~\cite{jiang2019stm}   & ResNet-50 & ImageNet & 1630 & 6730 & 73.7&91.6  \\ 
			
			TEINet~\cite{liu2020teinet}  & ResNet-50 & ImageNet & 830 & 3330 & 74.9&91.8 \\ 
			TEINet~\cite{liu2020teinet}  & ResNet-50 & ImageNet & 1630 & 6630 & 76.2&92.5 \\
			
			TANet~\cite{liu2020teinet}  & ResNet-50 & ImageNet & 830 & 4330 & 76.1&92.3 \\ 
			TANet~\cite{liu2020teinet}  & ResNet-50 & ImageNet & 1612 & 8612 & 76.9&92.9 \\
			TEA~\cite{li2020tea}  & ResNet-50 & ImageNet & 1630 & 7030 & 76.1&92.5 \\ 
			PAN~\cite{zhang2020pan}  & ResNet-50 & ImageNet &  (85)2 & 270 & 75.3&92.4 \\
			TDN~\cite{wang2020tdn}  & ResNet-50 & ImageNet & (85)30 & 3630 & 76.6&92.8 \\
			TDN~\cite{wang2020tdn}  & ResNet-50 & ImageNet & (165)30 & 7230 & 77.5&93.2 \\
			TDN~\cite{wang2020tdn}  & ResNet-50 & ImageNet & (8+16)530 & 10830 & 78.4&93.6 \\
			TDN~\cite{wang2020tdn}  & ResNet-101 & ImageNet & (8+16)530 & 19830 & \textbf{79.4}&\textbf{94.4} \\
			\hline 
			EAN(Ours) 	& ResNet-50 & ImageNet & (85)30 &  3730 & 77.1 &93.3 \\
			EAN(Ours)& ResNet-50 & ImageNet & (165)30 &  7430 & 78.3 &93.7 \\
			EAN(Ours)& ResNet-50 & ImageNet & (8+16)530 &  11130 & \textbf{79.0} &\textbf{94.1} \\
			\Xhline{2\arrayrulewidth}
		\end{tabular}
	}
	
	\label{tab_k400_sota}
\end{table*}


We also compare our method with the two methods~\cite{materzynska2020something}\cite{wang2018videos} that first detect the objects of the input frames in the RGB space and then model the object interactions.
Although we do not use the pretrained object detector or extra object bounding box annotations to get the proposal regions, our method still significantly outperforms them.
Specifically, our improvements over GCN + Non-local~\cite{wang2018videos} and I3D + STIN + OIE~\cite{materzynska2020something} are \textbf{11.1\%} (on Something V1) and \textbf{8.6\%} (on Something V2), respectively, in terms of the Top1 recognition accuracy.
This proves the superiority of the end-to-end object detection scheme and the Transformer architecture adopted in our method.


As for the 2D CNN-based methods, we compare our EAN architecture with them for a fair comparison, where only one frame is sampled from each segment.
Our models achieve the best performance under all settings of different input frame numbers.
The performances of TSN and TRN are relatively inferior to other methods because both the two methods only model the temporal information upon the highest-level feature maps from the backbone CNN. TEA is superior to all other 2D CNNs because it explores multi-scale spatial-temporal information. Compared with TEA, our method outperforms it consistently with the different input frame numbers. When using 8 and 16 input frames, the improvements are 3.0\% and 1.5\% on Something V1 dataset. The reason is that the multi-scale architecture of TEA is based on the hand-crafted Res2Net, which is static and not adaptive to the video. In contrast, the spatial-temporal modeling architecture of our method is dynamic and adaptive.





We further compare the improved EAN architecture with the other recent 2D CNNs that also take advantage of the short-term motion information, where 5 adjacent frames are sampled from each segment. Compared with the optical flow-based methods, \textit{i.e.}, TRN and TSM, 
our smallest model EAN already outperforms them by 11.2\% and 0.6\%, respectively. It's worth noting that the computational complexity of our LMC motion feature produced from the input video of 40 frames is only 1.1 GFLOPs, while the computational complexity of FlowNet2.0~\cite{ilg2017flownet} is 2006 GFLOPs for the same video.
In other words, the proposed LMC module is 1823 more efficient than optical flow modality, while achieving better performance for the action recognition task.

By averaging the predictions from EAN and EAN, the resulted model EAN boosts the action recognition performance to a new state-of-the-art level, \textit{i.e.}, 57.2\% (\textbf{ +2.1\%}) on Something V1 and 68.8\% (\textbf{ +1.8\%}) on Something V2, when using the recent method TDN as the anchor.
Compared with the PAN model, the improvement of our method is 1.9\% on Something V1, even though that PAN adopts a much heavier backbone network, \textit{i.e.}, 2D-ResNet101.


		\begin{table}[!tb]
			\caption{Comparison of different models in terms of input clip setting and their key modules, \textit{i.e.}, local spatial-temporal modeling module, global aggregation module and short-term motion modeling module.
				PA, TSM and ME indicate the appearance of persistence module~\cite{zhang2020pan}, the temporal shift module~\cite{lin2019tsm} and 
				the motion excitation module~\cite{li2020tea}, respectively. 
				L/SDM represents the long/short-term temporal difference modules in TDN~\cite{wang2020tdn}.
				AVG indicates the spatial-temporal average pooling operation.
				The model performances on Something V1 are also reported.
			}
		\centering
			\renewcommand{\arraystretch}{1.0}
			\setlength{\tabcolsep}{0.7mm}
			\begin{tabular}{cccccc}
				\Xhline{2\arrayrulewidth}
				\multirow{2}{*}{\textbf{Model}}& \multicolumn{1}{c}{\textbf{Input clip}} & \multicolumn{3}{c}{\textbf{Key components}} &
				\multirow{2}{*}{\textbf{Top1 (\%)}}
				 \\
				& segmentframe  & Local   & Global  & Motion\\  
				\hline
				TSM\textsubscript{8F} & 8  1 & TSM & AVG &- & 45.6 \\
				ResNet baseline & 8  1 & ME & AVG &- & 48.6 \\
				
				EAN\textsubscript{8F(RGB)} & 8  1 & EAB & SOI-Tr &- &51.9 \\ 
				\hline
				PAN\textsubscript{8F(RGB+PAN)} & 8  5 & TSM & AVG & PA & 50.5 \\ 
				TDN\textsubscript{8F(RGB+SDM)} & 8  5 & LDM & AVG & SDM & 52.3\\
				EAN\textsubscript{8F(RGB+LMC)} & 8  5 & EAB & SOI-Tr & LMC & \textbf{53.4} \\ 
				
				\Xhline{2\arrayrulewidth}
			\end{tabular}
		
		\label{tab_method_compare}
	
\end{table}

Furthermore, we summarize the key differences among our adopted ResNet baseline, our variant models and
other recent relevant methods, as shown in Tab.~\ref{tab_method_compare}.
Particularly, TDN also uses a short-term temporal difference module (SDM) to exploit short-term motion information in the low-level feature space, and fuse the motion features into the backbone in the early stage. Nevertheless, our method outperforms TDN by 1.1\% on Something V1.
The consistent improvements of our method over the other methods strongly justify the superiority of the proposed event scale adaptive spatial-temporal modeling paradigm by EAB, sparse object interaction modeling scheme by SOI-Tr, and high-order motion representation in latent space by LMC.








 \textbf {Kinetics-400.}
To verify that our method also effectively captures rich object appearance cues and the interactions among them, we compare our method with other state-of-the-art results on the Kinetics-400, as shown in Tab.~\ref{tab_k400_sota}.
When compared with the methods based on 2D CNNs, our method outperforms all of them when using the same backbone network, and demonstrates a better trade-off between the action recognition accuracy and the computational complexity.
For example, when equipped with the same ResNet-50 backbone, our method outperforms the recent method TDN by 0.6\%.
When adopting the ResNet-101 backbone, TDN shows the strongest result among all 2D CNNs. Nevertheless, this also increases the computation cost of TDN, which is even close to the 3D CNN method, \textit{i.e.}, SlowFast + NL network.
Our EAN models achieve the best complexity performance trade-off among all state-of-the-art methods.



\begin{table}[!b]
\caption{
	Comparison to state-of-the-arts on Diving48.
	We adopt the \textit{single clip or twice clips} inference schemes where a center crop of 224224 from a single clip or twice clips is used.
	 indicates the paper didn't provide the results.
}
	\centering
	\scalebox{1.0}{
		\renewcommand{\arraystretch}{1.0}
		\setlength{\tabcolsep}{3.0mm}
		\begin{tabular}{cccc}
			\Xhline{2\arrayrulewidth}
			\textbf{Method} & \textbf{Pre-train} & \textbf{Frames} & \textbf{Top1 (\%)} \\  
			\hline
			TSN (from \cite{li2018resound}) & ImageNet & 8 & 16.7 \\ 
			TRN (from \cite{li2018resound})& ImageNet & 8 & 22.8 \\ 
			C3D (from \cite{li2018resound})& ImageNet & 64 & 27.6 \\ 
			R(2+1)D (from \cite{bertasius2018learning}) & Kinetics&- & 28.9 \\ 
			P3D (from~\cite{luo2019grouped}) & ImageNet&16 & 32.4 \\ 
			C3D (from~\cite{luo2019grouped}) & ImageNet &16& 34.5 \\ 
			Kanojia~\etal~\cite{kanojia2019attentive} & ImageNet &64& 35.6 \\ 
			TEA-ResNet50~\cite{li2018resound} & ImageNet &16&  36.0 \\
			CorrNet-101~\cite{wang2020video} & - &3210& 38.6 \\ 
			GST ~\cite{luo2019grouped} & ImageNet &16& 38.8  \\  
			\hline  
			Ours & ImageNet &16&  {40.4} \\ 
			Ours & ImageNet &162&  \textbf{41.7} \\ 
			\Xhline{2\arrayrulewidth}
		\end{tabular}
	}
	
	\label{tab_diving48_sota}
\end{table}

 \textbf {Diving48.}
To prove that our method can model subtle fine-grained motion cues, we test our method on Diving48.
This dataset requires modeling the subtle body motions in long-short terms and includes much fewer videos compared with Something-Something and Kinetics.
We input 16 frames to the network and sample {two} clips from the video during inference.
The results are shown in Tab.~\ref{tab_diving48_sota}.
Our method outperforms the recent state-of-the-art GST~\cite{luo2019grouped} when using single clip (\textbf{ +1.6\%}) or twice clips (\textbf{ +2.9\%}) as the input videos.


\spaceabovesubsection
\subsection{Ablation Studies for EAN}
We conduct extensive ablation studies on Something V1~\cite{goyal2017something} dataset to demonstrate the superiority of the proposed framework by answering the following questions.
The variant models in this section are derived from the EAN model.
The input clip is always with 8 frames.

\textit{\textbf{Q1: Are the proposed EAB and SOI-Tr effective and necessary?}}
As mentioned in Sec.~\ref{approach}, in our framework, the EAB extracts more accurate local spatial-temporal representation and 
the SOI-Tr derives global object interaction representation from the video. To confirm that both two representations are effective and necessary for a high-performance action recognition framework, we conduct ablation experiments. Specifically, we equip ResNet baseline with the two proposed modules separately and analyze their impact on the performance.

\begin{table}[!htbp]
\caption{
	Comparison of the performance of using different spatial-temporal modeling modules.
}
	\centering
	\setlength{\tabcolsep}{1.5mm}
	\renewcommand{\arraystretch}{1}
	
	\begin{tabular}{lcccc}
		\Xhline{2\arrayrulewidth}
		\multirow{2}{*}{\tabincell{c}{\textbf{Method} } } &\multirow{2}{*}{\tabincell{c}{\textbf{Param}} }  &\multirow{2}{*}{\tabincell{c}{\textbf{FLOPs}} } & \multicolumn{2}{c}{\textbf{Something V1}}  \\
		&&& \scriptsize{\textbf{Top1 (\%)}} & \scriptsize{\textbf{Top5 (\%)}} \\ 
		\hline
		ResNet baseline	  &24.0M &33.1G & 48.6 & 77.5  \\
		ResNet+EABs &29.5M &35.3G   & 50.8 & 78.4 \\
		ResNet+SOI-Tr &30.3M &33.8G   & 49.3 & 77.9 \\
		\hline
		\textbf{ResNet+EABs+SOI-Tr} &36.0M &36.1G & \textbf{51.9} & \textbf{79.5} \\
		\Xhline{2\arrayrulewidth}
		
	\end{tabular}
	\label{tab_ab_pathway}
\end{table}


As shown in Tab.~\ref{tab_ab_pathway}, both the two modules demonstrate strong video modeling capability.
When the ResNet baseline is enhanced with the EABs, the Top1 accuracy is significantly improved by 2.2\%. The reason is that the features extracted by the ResNet baseline are not accurate enough, and the proposed EABs can refine the features with the dynamic spatial-temporal kernel.
For a more intuitive understanding, we will visualize
the refined feature maps by our method in section~\ref{sec_eab}.
Then, we observe that the ResNet + SOI-Tr baseline also outperforms the original ResNet baseline by 0.7\% in terms of Top1 accuracy, while only introducing an extra 0.7 GFLOPs computation cost.
Finally, simultaneously using EAB and SOI-Tr boosts the performance to 51.9\%, which proves the complementarity of the two proposed modules.



\begin{figure}[!htb]
	\centering
	\begin{minipage}[t]{0.76\linewidth}
		\centering
		\centerline{\includegraphics[width=8.0cm]{new_fig/pathway_comp-eps-converted-to.pdf}}
	\end{minipage}
	\caption {
		The top 5 action classes that are significantly improved after introducing the SOI-Tr module.
	}
	\label{fig_ab_pathway_comp}
\end{figure}



\begin{figure}[!thp]
	\centering
	\begin{minipage}[t]{0.74\linewidth}
		\centering
		\centerline{\includegraphics[width=8.2cm]{new_fig/vis_SOITr-eps-converted-to.pdf}}
	\end{minipage}
	\caption {
		Visualization of one video clip from the most improved category by the SOI-Tr.
		The input clip is first processed by EABs to obtain the spatial-temporal feature map {}.
		Then, SOI-Tr calculates the saliency map {} of the most concentrated object and the interactions of this object across the temporal axis. We take the 4th frame as the anchor and show the attention vector {}.
	}
	\spaceabovetab
	\label{fig_vis_ean_soitr}
\end{figure}
We also plot the top 5 classes that are significantly improved after introducing the SOI-Tr. As shown in Fig.~\ref{fig_ab_pathway_comp},
we find that the most improved instances can be roughly divided into two groups:
{(a)} The instances that require tracking the state of a certain object over the whole clip, such as the videos of \textit{``Lifting a surface ... ''} and \textit{``Pulling two ends ... ''}.
{(b)} The instances that contain multiple objects and the interactions between them, such as the videos of \textit{``Pretending to put ... ''}.
This is aligned with the motivation of introducing SOI-Tr, \textit{i.e.}, accurately modeling the long-range object interactions benefits the recognition of some complex actions.




To systematically understand how the EABs and SOI-Tr improve the recognition performance, we randomly select one video from the category \textit{``Lifting a surface with something on it but not enough for it to slide down''} and visualize it.
In Fig.~\ref{fig_vis_ean_soitr}, we can clearly see that the original feature  before global modeling concentrates on the background or the board, omitting the main object, \textit{i.e.}, the small sliding box. This makes sense because both the spatial area and the motion magnitude of the board are more obvious than the small box. After introducing the SOI-Tr, the object detector first finds the main object. Then, the Transformer model builds the long-range dependencies across the whole clip. We also notice that the board in the first frame is also detected. But, this background object will be neglected in the self-attention model because its weight is only 0.11.


\begin{table}[!thbp]
		\caption{Study on the location of EAB and SOI-Tr.	}
	\centering
	\setlength{\tabcolsep}{1.5mm}
	\renewcommand{\arraystretch}{1.0}
	\begin{tabular}{lccccc}
		\Xhline{2\arrayrulewidth}
		\multirow{2}{*}{\tabincell{c}{\textbf{EAB} } } &\multirow{2}{*}{\tabincell{c}{\textbf{SOI-Tr}} } 
		& \multirow{2}{*}{\tabincell{c}{\textbf{Param}} }
		&\multirow{2}{*}{\tabincell{c}{\textbf{FLOPs}} } & \multicolumn{2}{c}{\textbf{Something V1}}  \\
		&&&& \scriptsize{\textbf{Top1 (\%)}} & \scriptsize{\textbf{Top5 (\%)}} \\ 
		\hline
		Stage 12 & Stage 35 &35.6M & 35.8G & 49.4 & 78.2\\
		Stage 13 & Stage 45 &34.8M & 35.9G & 50.4 & 79.2\\
		Stage 14 & Stage 5 		&36.0M & 36.1G & \textbf{51.9} & \textbf{79.5}\\
		Stage 15 & - 		 		&65.8M & 36.2G & 50.8 & 79.1\\
		\Xhline{2\arrayrulewidth}
		
	\end{tabular}

	\label{tab_insert_blk}
\end{table}

\textit{\textbf{Q2: Where to insert the proposed modules?}}
We perform an ablation study on which stage to use local operator (EAB) and global operator (SOI-Tr). The results are shown in Tab.~\ref{tab_insert_blk}.
From these results, we see that adding more EABs into the main network only slightly increases the computational cost due to the high efficiency of the bottleneck designing and group convolution. When some EABs are replaced with the SOI-Tr, the performance decreases consistently. This implies that the local spatial-temporal information is crucial for action recognition, which cannot be substituted by the high-level object interaction information. We also try to build the network only with EABs, the result is also inferior to the original hybrid model (convolution+self-attention). The setting of using EAB after stage 14 and SOI-Tr after stage 5 obtains the best recognition accuracy and is with reasonable complexity. 



\textit{\textbf{Q3: Is the prior assumption of SOI-Tr reasonable?}}
To prove the end-to-end foreground object detector and the sparsity assumption for object interactions are both important for SOI-Tr, we train other variant models where we replace our detected object regions with the same number of the fixed regions or the regions detected by a pre-trained Faster RCNN~\cite{ren2016faster} model.
When the number of the boxes output from Faster RCNN is too small, we pad it with the central region of the frames.
The performances of the models are compared in Tab.~\ref{tab_interact_models}.

\begin{table}[!htbp]
	\vspace{-3mm}
	\caption{
		Study on various object region sampling strategies.
	}
	\centering
	\setlength{\tabcolsep}{4.5mm}
	\renewcommand{\arraystretch}{1.0}
	\begin{tabular}{lccc}
		\Xhline{2\arrayrulewidth}
		\multirow{2}{*}{\tabincell{c}{\textbf{Regions} } }
		&\multirow{2}{*}{\tabincell{c}{\textbf{FLOPs}} } & \multicolumn{2}{c}{\textbf{Something V1}}  \\
		&& \scriptsize{\textbf{Top1 (\%)}} & \scriptsize{\textbf{Top5 (\%)}} \\ 
		\hline
		None & 35.3G & 50.8 & 78.4 \\
		\hline
		Fixed & 35.8G & 50.9 & 78.4 \\
		Faster RCNN & 71.3G & 51.1 & 78.7 \\
		All & 36.4G & 51.3 & 79.1 \\
		Our Det-Net & 36.1G & \textbf{51.9} & \textbf{79.5} \\
		\Xhline{2\arrayrulewidth}
	\end{tabular}

	\label{tab_interact_models}
	\spacebelowtab
\end{table}



 First, we notice that building the interaction model upon the fixed regions already slightly improves the performance, proving that the interaction modeling is beneficial to the action recognition. Then we use the Faster RCNN detector to predict more accurate foreground regions. Surprisingly, the performance improvement is negligible. This may be ascribed to the fact that most frames only contain one or two objects, which cover fewer regions compared with the ``fixed region'' scheme. In contrast, the Saliency-Net embedded in our method always detects enough salient regions in an end-to-end manner and obtains the best performance, \textit{i.e.}, 51.9\%. Also, it is computationally efficient due to the shared feature extractor with the other parts of the framework. We emphasize that our embedded Saliency-Net outperforms Faster RCNN by 0.8\% while running 118 faster.
 
 We further try to leverage all positions to build a dense interaction model, as shown in the penultimate row of Tab.~\ref{tab_interact_models}. However, the performance is obviously inferior to our method. This strongly supports our assumption that most regions are only background noises for the final prediction and leveraging all of them will deteriorate the final performance.
 




\subsection{Further Studies for EAB}\label{sec_eab}
In this section, we make further studies on the aspects that impact the effectiveness of EAB.

\textbf{Large receptive field and multi-scale modeling are important.}
To verify this, we introduce the following baselines: 

\begin{figure}[!htb]
	\centering
	\begin{minipage}[b]{0.9\linewidth}
		\centering
		\centerline{\includegraphics[width=8cm]{new_fig/EAB_baseline-eps-converted-to.pdf}}
	\end{minipage}
	\caption {Illustrations of the baseline spatial-temporal blocks.
		The representation signs are the same meaning as Fig.~\ref{fig_ean}.
	}
\spacebelowtab
	\label{fig_base_arch}
\end{figure}



	\noindent {\textsf{\small (1) S-Block}}.
	It is implemented with a (2+1)D convolution with kernel size  and group size 3, as shown in Fig.~\ref{fig_base_arch} (a), which only captures single-scale features with a small receptive field.
	
	\noindent \textsf{\small (2) L-Block}.
	It is implemented with a (2+1)D convolution with kernel size , group size 3, and dilation size , as shown in Fig.~\ref{fig_base_arch} (b), which captures single-scale features with a larger receptive field.
	
	\noindent \textsf{\small (3) Incep-Block}.
	It is implemented with a group of (2+1)D convolutions in an Inception-style, as shown in Fig.~\ref{fig_base_arch} (c), which captures multi-scale features with a larger receptive field.
	 The only difference between this baseline and EAB is that the  is replaced with an identity mapping operation.







\begin{table}[!htbp]
		\caption{
		Comparison of different spatial-temporal blocks. RFS denotes the receptive field size.
	}
	\centering
	\setlength{\tabcolsep}{0.64mm}
	\renewcommand{\arraystretch}{1.0}
	\begin{tabular}{lccccccc}
		\Xhline{2\arrayrulewidth}
		\multirow{2}{*}{\tabincell{c}{\textbf{Models}} } & 
		\multirow{2}{*}{\textbf{\tabincell{c}{RFS}}}&
		
		\multirow{2}{*}{\textbf{\tabincell{c}{Multi\\scale?}}} & 
		\multirow{2}{*}{\tabincell{c}{\textbf{Param}}} & 
		\multirow{2}{*}{\tabincell{c}{\textbf{FLOPs}}} & 
		\multicolumn{2}{c}{\textbf{Something V1}}  \\
		&&&&& \scriptsize{\textbf{Top1 (\%)}} & \scriptsize{\textbf{Top5 (\%)}} \\ 
		\hline
		Only SOI-Tr &-&-&30.3M& 33.8G &49.3 & 77.9 \\
		\hline
		+S-Block &&-&30.9M &36.1G   & 49.6 & 78.1 \\
		+L-Block &&-&30.9M &36.1G   & 50.3 & 78.4 \\
		+Incep-Block &&\checkmark &30.9M & 36.0G & 50.8 & 78.8 \\
		\hline
		\textbf{+EAB} &\textbf{} &\checkmark &36.0M &36.1G  &\textbf{51.9} & \textbf{79.5}   \\
\Xhline{2\arrayrulewidth}
	\end{tabular}
	\label{tab_st_model}
\end{table}


We compare our method with the proposed baseline methods in Tab.~\ref{tab_st_model}. First, it can be seen that EAB outperforms the S-Block baseline by a large margin (51.9\% \textit{vs.} 49.6\%). The improvement is originated from two aspects: (1) The large spatial-temporal kernel within EAB enables the larger receptive field and aggregates more local information. (2) The explicit multi-scale modeling introduces richer feature representation.
It is necessary to validate the independent contribution from the two aspects. We first compare the S-Block baseline with the L-Block baseline. L-Block has a larger spatial-temporal receptive field size but the same number of parameters. We can see that the recognition accuracy is improved by 0.7\%.
Then, we build the Incep-Block baseline by enhancing the L-Block baseline with multi-scale modeling capability. This improvement further improves the recognition accuracy. From the comparisons above, we verify that both the two aspects facilitate the action task, and multi-scale architecture fully exploits the large receptive field.

\begin{figure}[!thbp]
	\centering
	\centerline{\includegraphics[width=8.2cm]{new_fig/vis_EAB_feature-eps-converted-to.pdf}}
	\caption {Comparison of the features from Stage4 of EAN and the evolution of the prediction scores. The proposed EAB can discover more semantically-aligned regions for actions, and also suppress the noisy information from backgrounds to yield correct prediction. \textit{Zoom in for better visualization.}
	}
\label{fig_vis_feature}
\end{figure}

We randomly select one video from Something V1 dataset and visualize the  feature map output from Stage4 (this is before the inserting of the SOI-Tr), as shown in Fig.~\ref{fig_vis_feature}. It clearly demonstrates that our method can discover more semantically consistent regions for actions, and in the meantime reduce noisy backgrounds for correct prediction. Moreover, the feature activation heatmaps of our method are better spatially aligned with the target object (see the water).
We also show the state evolution process in Fig.~\ref{fig_vis_feature}. Interestingly, our method detects the start and end points of actions although only trained with classification labels.





\textbf{Dynamic architecture matters.}
From Tab.~\ref{tab_st_model}, we notice that the performance gap between the Incep-Block baseline and EAB is still rather large, \textit{i.e.}, 1.1\% Top1 accuracy. We conjecture this is due to the dynamic architecture of EAB. As mentioned in section~\ref{EAB}, the inference pathway for EAB is determined by the kernel fusion matrix . For more detailed analysis, we propose two kernel fusion strategies:


\noindent {\textsf{\small (1) Channel Shuffle}}. We replace  with a conventional fusion method, \textit{i.e.}, Channel Shuffle operation~\cite{zhang2018shufflenet}, which enables the communication of the features of different groups.

\noindent {\textsf{\small (2) Static Matrix.}} The  is a learnable matrix during training. But, it's a fixed matrix during inference.






\begin{table}[!h]
	\caption{
		Comparison of different feature fusion methods.
	}
	\centering
	\small
	\setlength{\tabcolsep}{1.8mm}
	\renewcommand{\arraystretch}{1.0}
	\begin{tabular}{lccccc}
		\Xhline{2\arrayrulewidth}
		\multirow{2}{*}{\tabincell{c}{ \textbf{Methods} } } &
		\multirow{2}{*}{\tabincell{c}{\textbf{Param}  } } & 
		\multirow{2}{*}{\tabincell{c}{\textbf{FLOPs}  } } & 
		\multicolumn{2}{c}{\textbf{Something V1}}  \\
		
		&&& \scriptsize{\textbf{Top1 (\%)}} & \scriptsize{\textbf{Top5 (\%)}} \\ 
		\hline
		Identity (Incep-Block) & 30.9M &36.0G & 50.8 & 78.8 \\
		\hline
		Channel Shuffle & 30.9M &35.9G & 51.1 & 78.9 \\
		Static Matrix & 30.9M &36.0G & 51.3 & 79.2 \\
		Dynamic Matrix & 36.0M & 36.1G & \textbf{51.9} & \textbf{79.5} \\
		\Xhline{2\arrayrulewidth}
	\end{tabular}
	
	\label{tab_ft_inter}
\end{table}

Both the above two baselines belong to the static architecture but they are similar to the EAB in terms of the network details, which are perfect for studying the impact of dynamic modeling. We compare their performances in Tab.~\ref{tab_ft_inter}. We observe that the performance improves consistently with a more complex kernel fusion strategy, \textit{i.e.}, Identity Channel Shuffle Static Matrix Dynamic Matrix.
The Dynamic fusion matrix adopted by EAB shows the best performance (51.9\%) with negligible extra cost.



\textbf{Kernel visualization.}
To verify that the dynamic kernel fusion matrix  of EAB is indeed adaptive to the scales of the main objects and the key events within different videos, we conduct a group of experiments by augmenting one anchor video and observing the change of the weights of the fixed-scale kernels. The augmented videos and the kernel weight changing procedure are illustrated in Fig.~\ref{fig_m}. We first see, both the weight distributions along the temporal axis or the spatial axis are not sparse, \textit{i.e.}, all kernels are activated. This supports our assumption that the optimal spatial-temporal kernel for the video is with an unknown complex shape and cannot be accurately replaced by one kernel of fixed-scale. Also, the distributions do not follow some simple distributions such as Uniform or Gaussian, indicating that the kernel weights cannot be trivially hand-crafted and are required to be learned from data.
When we spatially zoom in the anchor video by 1.6, the main objects in the video, \textit{i.e.}, the hand and the stick, are easier to be discovered, we see that EAB is more inclined to exploit the spatial convolutions of small kernels such as that of size 11 instead of that of size 55
. Similarly, when we sample the frames with 2 higher frame-rate, the object motions become slower and the small temporal convolutions such as that with kernel size 1 are fully used.




\begin{figure}[!t]
	\centering
	\centerline{\includegraphics[width=8cm]{new_fig/vis_dyn_new-eps-converted-to.pdf}}
	\caption {Visualizing the dynamic kernel fusion matrix  of the proposed EAB via the kernel weights. For each spatial or temporal kernel, its weight is computed by summing the matrix values connected to it.
		In the first row, we give an anchor video.
		Below it, we show the impact to the kernel weights by changing the spatial or temporal scales of the anchor video. The kernel weights of anchor, spatially-, and temporally-augmented videos are indicated by \textcolor{gray}{gray}, \textcolor{green}{green}, and \textcolor{brown}{brown} bars, respectively.
		``S-3''\&``T-3'' denotes the fixed-size spatial\&temporal kernel of size 33\&3.
	}
	\label{fig_m}
\end{figure}




\begin{table}[!thbp]
	\caption{
		Ablation on detailed designs of EAB.
	}
	\centering
	\setlength{\tabcolsep}{2.0mm}
	\renewcommand{\arraystretch}{1.0}
	\begin{tabular}{lcccc}
		\Xhline{2\arrayrulewidth}
		\multirow{2}{*}{\tabincell{c}{\textbf{Design} } } &
		\multirow{2}{*}{\tabincell{c}{\textbf{Param} } } &
		\multirow{2}{*}{\tabincell{c}{\textbf{FLOPs} } } &
		\multicolumn{2}{c}{\textbf{Something V1}}  \\
		&&& \scriptsize{\textbf{Top1 (\%)}} & \scriptsize{\textbf{Top5 (\%)}} \\ 
		\hline
		Without Max Pool &{36.0M}& {36.1G}& {50.6} & {78.4}   \\
		Avg Pool &{36.0M}& {36.1G}& {51.4} & {79.8}   \\
		Without inter ReLU &{36.0M}& {36.1G}& {50.9} & {79.0}   \\
		Without dilation &{37.2M}& {37.5G} & \textbf{51.9} & \textbf{79.8}   \\
		(1+1+1)D  &{35.7M}& {35.7G}& {51.2} & {79.2}   \\
		\hline
		Ours &{36.0M}& {36.1G}& \textbf{51.9} & {79.5}   \\
		\Xhline{2\arrayrulewidth}
	\end{tabular}
	
	\label{tab_arch_details}
\end{table}

\textbf{\textbf{Studies on EAB details.}}
In this part, we conduct experiments to verify whether all the designs of EAB contribute to the final performance.
As shown in Tab.~\ref{tab_arch_details},
the max pooling operation significantly improves the performance ({1.3\%} \textit{w.r.t} Top1 accuracy), and meanwhile our method is not sensitive to specific implementation of this operation. 
Both average pooling and max pooling operators achieve excellent performance.
Max pooling demonstrates a slight advantage over average pooling because the regions of the key objects and frames related to action only cover a small proportion of the input video data.
Also, we find that the extra non-linearity introduced by the intermediate ReLU operations between spatial- and temporal-filters also benefit the performance, which is consistent with the conclusion from previous work~\cite{tran2018closer}.
Besides, we demonstrate that the dilated convolution achieves comparable performance with the ordinary convolution while it is much more efficient.
Finally, we also try to decompose the 2D spatial convolution into two stacking 1D convolutions. But this brings a slight performance drop. To summarize, the extensive experiments in this section prove the necessity of detailed designs in EAB.




\section{Erroneous Cases and Limitations}

Although the quantitative results on standard benchmarks and the extensive analysis above have verified the effectiveness of the proposed framework, it inevitably has some limitations, which lead to erroneous recognition results.





One limitation is caused by the simple architecture of ESP-Net within EAB. ESP-Net is responsible for perceiving the event scales within the input video, composed of two convolution layers followed by a global average pooling operation. Although this simple ``average'' operation is lightweight in terms of the computational cost, it also makes the statistical results of the video biased to the large objects.
As shown in Fig.~\ref{fig_error_eab}, the feature activations are dominated by the large-area human hand shadow, neglecting the real objects (the human hand and the charger) involved in the action \textit{plugging something into something}.



\begin{figure}[!thp]
	\centering
	\centerline{\includegraphics[width=8.2cm]{error_cases/error_case_analysis_EAB-eps-converted-to.pdf}}
	\caption {
		Recognition error caused by EAB.
	The shadow of the human hand instead of the real hand and the charger is attended, causing the recognition result changing from \textit{plugging something into something} to \textit{moving part of something}.
	}
	\label{fig_error_eab}
\end{figure}
\begin{figure}[!thp]
	\centering
	\centerline{\includegraphics[width=8.2cm]{error_cases/error_case_analysis_SOITr-eps-converted-to.pdf}}
	\caption{
		Recognition error caused by SOI-Tr.
		We visualize the foreground object distribution maps, where only parts of the {towel} are detected in this case.
		Therefore, the global state \textit{folding} of the {towel} can not be perceived, resulting the biased action recognition result \textit{touching}.
	}
	\label{fig_error_soitr}
\end{figure}

Another limitation is originated from the proposed SOI-Tr.
The adaptiveness of SOI-Tr lies in detecting different foreground objects for different input videos.
Nevertheless, the adaptiveness may be limited by the representation of the objects, \textit{i.e.}, points in the feature map, which correspond to fixed-size regions within the input video. Therefore, the granularity and the scale of the detected foreground objects are not flexible enough.
As shown in Fig.~\ref{fig_error_soitr}, only small parts of the towel can be detected.
Therefore, the global state \textit{folding} of the towel can not be perceived. Instead, the local states of the wrongly attended objects, \textit{i.e.}, the human hand and the partial towel, contribute to the wrong prediction \textit{touching part of something}.














