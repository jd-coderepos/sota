

\documentclass[journal]{IEEEtran}





















\ifCLASSINFOpdf
\usepackage[pdftex]{graphicx}
\else
\usepackage[dvips]{graphicx}
\fi






\usepackage{amsmath}













































\hyphenation{op-tical net-works semi-conduc-tor}






\usepackage{amssymb}
\usepackage{float}
\usepackage{subcaption}
\usepackage{multirow}	
\usepackage{etoolbox} 


\usepackage{tikz}
\usetikzlibrary{fit,shapes.misc,calc}






\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{assumption}[theorem]{Assumption}





\definecolor{light-grey}{gray}{0.80}
\definecolor{mid-grey}{gray}{0.50}
\definecolor{dark-grey}{gray}{0.25}




\newcommand{\lgcol}[1]{{\color{light-grey}#1}}
\newcommand{\gcol}[1]{{\color{mid-grey}#1}}
\newcommand{\bcol}[1]{{\color{black}#1}}
\newcommand{\rcol}[1]{{\color{red}#1}}
\newcommand{\kcol}[1]{{\color{black}#1}}
\newcommand{\ccol}[1]{{\color{cyan}#1}}
\newcommand{\mcol}[1]{{\color{magenta}#1}}


\newcommand{\mbb}{\mathbb}
\newcommand{\mbf}{\mathbf}
\newcommand{\mrm}{\mathrm}
\newcommand{\mcal}{\mathcal}

\newcommand{\subjto}{\text{s.t.}}

\newcommand{\gO}{ {\color{light-grey}0} }

\newcommand{\tran}{\intercal}

\newcommand{\textQ}{Q}

\newcommand{\conv}[1]{\mathrm{conv}\left(#1\right)}
\newcommand{\rdim}[1]{\mathbb{R}^{#1}}
\newcommand{\sdim}[1]{\mathbb{S}^{#1}}
\newcommand{\spdim}[1]{\mathbb{S}_{+}^{#1}}
\newcommand{\sppdim}[1]{\mathbb{S}_{++}^{#1}}
\newcommand{\expval}[2]{\mathbb{E}_{#1}\left[#2\right]}
\newcommand{\trace}[1]{\mathrm{tr}\left(#1\right)}
\newcommand{\vect}[1]{\mathrm{vec}\left(#1\right)}
\newcommand{\diag}[1]{\mathrm{diag}\left(#1\right)}
\newcommand{\rank}[1]{\mathrm{rank}\left(#1\right)}
\newcommand{\ident}[1]{\mathbb{I}_{#1}}
\newcommand{\argmin}[1]{\underset{#1}{\arg\min}\,\,}
\newcommand{\argmax}[1]{\underset{#1}{\arg\max}\,\,}

\newcommand{\setdef}[2]{\left\{{#1} \,\middle|\, #2 \right\}}

\newcommand{\redtext}[1]{ {\color{red}#1} }

\newcommand{\intd}[1]{\mathrm{d}#1}



\newcommand{\xinX}{x\!\in\!\mathcal{X}}
\newcommand{\uinU}{u\!\in\!\mathcal{U}}
\newcommand{\spaceX}{\mathcal{X}}
\newcommand{\spaceU}{\mathcal{U}}
\newcommand{\spaceXbyU}{\mathcal{X}\times\mathcal{U}}
\newcommand{\spaceXbyUcompact}{\mathcal{X}\!\times\!\mathcal{U}}
\newcommand{\funcSpaceX}{\smash{\mcal{F}(\mcal{X})}}
\newcommand{\approxFuncSpaceX}{\smash{\hat{\mcal{F}}(\mcal{X})}}
\newcommand{\approxFuncSpaceXindex}[1]{\smash{\hat{\mcal{F}}_{#1}(\mcal{X})}}
\newcommand{\funcSpaceXU}{\smash{\mcal{F}(\mcal{X} \!\times\! \mcal{U})}}
\newcommand{\approxFuncSpaceXU}{\smash{\hat{\mcal{F}}(\mcal{X} \!\times\! \mcal{U})}}
\newcommand{\approxFuncSpaceXUindex}[1]{\smash{\hat{\mcal{F}}_{#1}(\mcal{X} \!\times\! \mcal{U})}}



\newcommand{\tikzorigin}{
	\draw[black,very thick,rounded corners] (0.0cm,0.0cm) circle (0.2cm);
	\draw[red,line width = 1.2pt,->] (0.0cm,0.0cm) -- (1.0cm,0.0cm);
	\draw[blue,line width = 1.2pt,->] (0.0cm,0.0cm) -- (0.0cm,1.0cm);
}

\definecolor{gray10}{gray}{0.10}
\definecolor{gray20}{gray}{0.20}
\definecolor{gray30}{gray}{0.30}
\definecolor{gray40}{gray}{0.40}
\definecolor{gray50}{gray}{0.50}
\definecolor{gray60}{gray}{0.60}
\definecolor{gray70}{gray}{0.70}
\definecolor{gray80}{gray}{0.80}
\definecolor{gray90}{gray}{0.90}
\definecolorset{rgb}{}{}{myred,     0.7922,0.0000,0.1255} \definecolorset{rgb}{}{}{myltred,   0.9569,0.6471,0.5098} \definecolorset{rgb}{}{}{myblue,    0.1216,0.4706,0.7059} \definecolorset{rgb}{}{}{myltblue,  0.6510,0.8078,0.8902} \definecolorset{rgb}{}{}{mygreen,   0.2000,0.6275,0.1725} \definecolorset{rgb}{}{}{myltgreen, 0.6980,0.8745,0.5412} \definecolorset{rgb}{}{}{mypurple,  0.5961,0.3059,0.6392} \definecolorset{rgb}{}{}{myltpurple,0.8706,0.7961,0.8941} \definecolorset{rgb}{}{}{myorange,  0.9882,0.5529,0.3490} \definecolorset{rgb}{}{}{myyellow,  0.9961,0.8784,0.5647} 


\definecolorset{rgb}{}{}{matlabred,    0.8941,0.1020,0.1098} \definecolorset{rgb}{}{}{matlabblue,   0.2157,0.4941,0.7216} \definecolorset{rgb}{}{}{matlabpurple, 0.5961,0.3059,0.6392} \definecolorset{rgb}{}{}{matlaborange, 1.0000,0.4980,0.0000} 


\newcommand{\blkcol}[1]{{\color{black}#1}}
\newcommand{\gcolThirty}[1]{{\color{gray30}#1}}
\newcommand{\gcolFifty}[1]{{\color{gray50}#1}}

 





\newtoggle{doublecolumn}







\begin{document}



\toggletrue{doublecolumn}



\title{
Performance guarantees for model-based\\Approximate Dynamic Programming\\in continuous spaces
}





\author{Paul~N.~Beuchat,
	Angelos~Georghiou,
	and~John~Lygeros,~\IEEEmembership{Fellow,~IEEE}\thanks{
 Automatic Control Laboratory at ETH Z\"{u}rich, Switzerland, {\tt\footnotesize \{beuchatp,jlygeros\}@ethz.ch}}\thanks{
 Desautels Faculty of Management, McGill University, Montreal,
	Canada, {\tt\footnotesize angelos.georghiou@mcgill.ca}}}






















\maketitle






\begin{abstract}
We study both the value function and Q-function formulation of the Linear Programming  approach to Approximate Dynamic Programming. The approach is model-based and optimizes over a restricted function space to approximate the value function or Q-function.
Working in the discrete time, continuous space setting, we provide guarantees for the fitting error and online performance of the policy.
In particular, the \emph{online performance guarantee} is obtained by analyzing an iterated version of the greedy policy, and
the \emph{fitting error guarantee} by analyzing an iterated version of the Bellman inequality.
These guarantees complement the existing bounds that appear in the literature.
The Q-function formulation offers benefits, for example, in decentralized controller design, however it can lead to computationally demanding optimization problems.
To alleviate this drawback, we provide a condition that simplifies the formulation, resulting in improved computational times.
\end{abstract} 




\section{Introduction} \label{sec:intro}

In 1952, Bellman proposed Dynamic Programming (DP) as a solution method for discrete time stochastic optimal control problems \cite{bellman_1952_theory}.
The solution of the Bellman equation is the optimal cost-to-go function, also called the value function, which characterizes the performance of the optimal control policy.
For continuous state and action spaces, the value function takes values in an infinite dimensional function space and the policy involves computation of a multi-variate expectation.
The continuous state, input, and disturbance spaces could be discretized to leverage the extensive literature on solving the Bellman equation for finite spaces \cite{bertsekas_2017_DP_vol1,puterman_2005_MDPs}, however this is computationally infeasible even for small problems.
As such, an extensive body of literature has proposed and studied various approximation methods for continuous space problems, including a range of model-free methods that learn the value function from interactions with an un-modelled system, \cite{sutton_1988_TDlearning,bertsekas_1995_neuroBook,reviewer_suggestion_2016_modelfree,google_2015_atari}.
In this paper we focus on the model-based method named the Linear Programming (LP) approach to Approximate Dynamic Programming (ADP) \cite{schweitzer_originalMDP} that takes advantage of model knowledge for the system dynamics, cost function, and exogenous disturbance as part of the algorithm for approximating the value function.





The LP approach to ADP has been formulated for value functions \cite{vanRoy_linApproxDP} and \textQ-functions \cite{vanroy_decentADP}, though much of the model-based LP approach literature focuses on value function approximation.
The \textQ-function has the property that the optimal control policy can be expressed without involving any of the terms that describe the model.
This property was exploited in \cite{vanroy_decentADP,beuchat_2016_ECC_PWMQ} in a model-based setting.
In particular, the work of \cite{beuchat_2016_ECC_PWMQ} provides numerical evidence that the \textQ-function approximation can provide better online performance compared to the value function approximation, while the work of \cite{vanroy_decentADP} uses the \textQ-function approximation for decentralized control design in finite spaces.
A wide range of model-free, data-driven ADP methods also use the \textQ-function and exploit the fact that the resulting optimal policy does not directly involve any of the terms that describe the model. There are many success stories from these model-free methods, for example \cite{watkins_1989_learningFromDelayedRewards, reviewer_suggestion_2018_Qlearning_experience_replay,meyn_2017_zap_qlearning}.
In this paper we provide some discussion to compare model-based and model-free approaches to ADP.








\renewcommand{\arraystretch}{1.1}
\begin{table*}[t]
\centering
\vspace{0.0cm}
\captionsetup{justification=centering}
	\caption{Road map to performance guarantees for the Linear Programming approach to Approximate Dynamic Programming. The bold entries represent contributions of this paper.}
\vspace{-0.1cm}
\begin{tabular}{|l|l|c|c|c|c|}
		\hline
		\multirow{2}{1.4cm}{Spaces}
		& \multirow{2}{1.4cm}{Bound Type}
		& \multicolumn{2}{|c|}{Non-iterated}
		& \multicolumn{2}{|c|}{Iterated}
\\
		\cline{3-6}
		&
		& Value functions
		& \textQ-functions
		& Value functions
		& \textQ-functions
		\\
		\hline\hline
\multirow{3}{1.2cm}{Finite}
		&
		Online performance
		&
		{\color{gray30}\cite[Theorem 1]{vanRoy_linApproxDP}}
		&
		{\color{gray30}\cite[Theorem 1]{vanroy_decentADP}}
		&
		\textemdash
		&
		\textemdash
		\\
		\cline{2-6}
		&
		Infinity norm
		&
		{\color{gray30}\cite[Theorem 2]{vanRoy_linApproxDP}}
		&
		\textemdash
		&
		\textemdash
		&
		\textemdash
		\\
		\cline{2-6}
		&
		Lyapunov-based
		&
		{\color{gray30}\cite[Theorem 3]{vanRoy_linApproxDP}}
		&
		\textemdash
		&
		\textemdash
		&
		\textemdash
		\\
		\hline
		\hline
		\multirow{3}{1.2cm}{Continuous}
		&
		Online performance
		&
		\textbf{Theorem \ref{theorem:online_performance_bound_iterated_Vform}}
		&
		\textbf{Theorem \ref{theorem:online_performance_bound_iterated_Qform}}
		&
		\textbf{Theorem \ref{theorem:online_performance_bound_iterated_Vform}}
		&
		\textbf{Theorem \ref{theorem:online_performance_bound_iterated_Qform}}
		\\ 
		\cline{2-6}
		&
		Infinity norm
		&
		{\color{gray30}\cite[\S 4.2]{boyd_iteratedBellman}}
		&
		{\color{gray30}\cite[Theorem 4.1]{beuchat_2016_ECC_PWMQ}}
		&
		{\color{gray30}\cite[\S 4.2]{boyd_iteratedBellman}}
		&
		{\color{gray30}\cite[Theorem 4.1]{beuchat_2016_ECC_PWMQ}}
		\\
		\cline{2-6}
		&
		Lyapunov-based
		&
		\textbf{Theorem \ref{theorem:bound_lyapunov_iterated_Vform}}
		&
		\textbf{Theorem \ref{theorem:bound_lyapunov_iterated_Qform}}
		&
		\textbf{Theorem \ref{theorem:bound_lyapunov_iterated_Vform}}
		&
		\textbf{Theorem \ref{theorem:bound_lyapunov_iterated_Qform}}
		\\
		\hline
		
	\end{tabular}
\\
\vspace{0.1cm}
Note: Entries marked with ~``\textemdash"~ are bounds that do not exist in the literature for the finite space setting.
\vspace{-0.2cm}
\label{tab:bounds_roadmap}
\end{table*}
\renewcommand{\arraystretch}{1.0}








Motivated by the empirical success of the model-based LP approach, \cite{novoa_2009_ADPforVehicleRouting,stellato_2017_ADPforPowerElectronics}, a key challenge is to provide theoretical guarantees on the quality of the approximation and the online performance.
In \cite{vanRoy_linApproxDP}, the authors presented a variant of the LP approach with theoretical guarantees for finite space problems.
They provided three guarantees for the value function formulation: (i) a bound on the \emph{online performance} of the control policy, (ii) a bound on how close the approximate value function is to the optimal in an \emph{infinity norm} sense, and (iii) a bound on how close the approximate value function is to the optimal using a \emph{Lyapunov-based} analysis.
A number of works use \cite{vanRoy_linApproxDP} as a basis for deriving additional performance guarantees.
An online performance bound for \textQ-functions was developed in \cite{vanroy_decentADP} for the finite space setting.
An infinity norm bound for value function approximation was provided in \cite{boyd_iteratedBellman} for the continuous space setting by considering an iterated version of the Bellman inequality.
An iterated version of the infinity norm bound for \textQ-functions was given in continuous spaces  by \cite{beuchat_2016_ECC_PWMQ}.
A Lyapunov-based bound was presented in \cite{vivekFarias_2012_smoothedLPapproach} for finite space by analyzing a smoothed version of the LP approach.
As many practical control problems involve continuous state, action, and disturbance spaces, it is valuable to derive online performance and Lyapunov-based bounds for the continuous space setting.
In this paper we address this gap in the literature for value functions and \textQ-functions.




The first contribution of this paper derives novel theoretical guarantees for the value function and \textQ-function approximation using the model-based LP approach in continuous spaces, while the second contribution improves the scalability of the formulation. In particular, the contributions of the paper are:
\begin{itemize}
\item We prove a continuous space online performance bound by analyzing an iterated version of the greedy policy.
When using the non-iterated greedy policy, our bounds form the counterpart to the bounds derived in \cite[Theorem 1]{vanRoy_linApproxDP} and \cite[Theorem 1]{vanroy_decentADP} for finite spaces.
	
\item We prove a continuous space Lyapunov-based bound by analyzing the iterated Bellman inequality.
When using the non-iterated Bellman inequality, our bounds forms the counterpart to the bound derived in \cite[Theorem 3]{vanRoy_linApproxDP} for finite spaces.
Additionally, our bounds contain \cite[\S 4.2]{boyd_iteratedBellman} and \cite[Theorem 4.1]{beuchat_2016_ECC_PWMQ} as a special case with a Lyapunov function that is constant for all states and inputs.
	
\item Approximating the \textQ-function using the LP formulation can be computationally demanding. We provide a condition that substantially decreases the optimization problem size for the \textQ-function formulation, making the method suitable for practical applications.
\end{itemize}


The existing results and contributions of this paper are summarized in Table~\ref{tab:bounds_roadmap} for the performance guarantees.
In support of the contributions, we provide numerical results to demonstrate the bounds, the performance of the iterated policy, and the potential of \textQ-functions for continuous space distributed control applications.
Section \ref{sec:dp} presents the Dynamic Programming formulation.
Section \ref{sec:adp} introduces the approximation methods and the iterated policy.
Section \ref{sec:bounds} provides the theoretical guarantees for both the value function and \textQ-function formulations and contrasts with theoretical results from the model-free literature.
Section \ref{sec:unify} provides conditions under which the \textQ-function formulation can be simplified.
Section \ref{sec:numerical} uses numerical examples to demonstrate the theory.


\emph{Notation:}
 () is the space of non-negative (positive) scalars;
 is the space of symmetric matrices of size ;
 is the space of positive integers;
 is the  identity matrix;
given , the infinity norm is , and the weighted 1-norm is . 



\section{Dynamic Programming (DP) Formulation} \label{sec:dp}


\subsection{Problem Formulation and Assumptions} \label{sec:dp_prob_form_and_assumptions}


We consider infinite horizon, stochastic optimal control problems with a discounted cost objective. The state of the system at time  is denoted by  .
The state is influenced by control decisions , and stochastic disturbances  distributed according to probability measure  that is used in all expectations.
The state evolves according to , where .
At time , the system incurs the stage cost , where  is the discount factor.
By  we denote the set of all feasible policies, i.e., , with  measurable, see \cite[Definition 2.2.3]{hernandez_2012_discreteTimeMCP}.
We restrict our attention to deterministic stationary policies and define the online performance for a fixed policy and initial state  as,
	
The objective is to find the policy that minimizes \eqref{eq:onlinePerformane}.


To pose this problem in the DP formulation, we work in the same setting as \cite[Section 6.3]{hernandez_2012_discreteTimeMCP}, specifically under \cite[Assumptions 4.2.1(a), 4.2.1(b), 4.2.2]{hernandez_2012_discreteTimeMCP}.
The assumptions ensure that from the class of time-varying stochastic policies, the minimum is attained by a stationary deterministic policy, see \cite[Theorem 4.2.3]{hernandez_2012_discreteTimeMCP}.
Additionally, under the assumptions it can be seen that an initial state distribution  and an admissible policy  define a Markov chain. Let  denote the probability distribution of the state at time , given that the initial states are distributed according to  that is concentrated on  and the system evolves autonomously under the fixed policy .
Finally,  and  are defined as the vector spaces of bounded, real-valued, Borel-measurable functions on  and  respectively, where \cite[Definition 6.3.2, 6.3.4]{hernandez_2012_discreteTimeMCP} provides the definitions of boundedness.



\subsection{Bellman Equation and Operator} \label{sec:dp_bellman_eq}

We now re-cast the stochastic optimal control problem in the dynamic programming formulation. The value function  represents the optimal cost-to-go from any state of the system if the optimal control policy is played, and is the solution of the Bellman equation \cite{bellman_1952_theory},
	
 is known as the Bellman operator, and the  operator is used to define an auxiliary function  that represents the cost of making decision  now and then playing optimally.The Bellman equation in terms of  is thus,

for all  and all . The -operator is the equivalent of  for the so-called \textQ-functions.
The \textQ-function is an example of a \emph{post-decision value function} \cite[\S 4.6]{powell_ADPBook}.



The optimal policy can be defined using  or  by,

		\pi^\ast(x)
			\,=&\, \argmin{u\in\mcal{U}} \, l\left( x , u \right) \, + \, \gamma \, \expval{}{V^{\ast}\left(g\left(x,u,\xi\right)\right)}
			\,,
			\label{eq:bellmanpolicy_Vform}
		\\
		=&\, \argmin{u\in\mcal{U}} \, Q^{\ast}(x,u)
			\,.
			\label{eq:bellmanpolicy_Qform}
	
Note that evaluating \eqref{eq:bellmanpolicy_Vform} requires use of the dynamics, stage cost, and expectation with respect to , whereas \eqref{eq:bellmanpolicy_Qform} involves only .
The existence of a , , and  that are Borel-measurable and attain the infimum is ensured by \cite[Assumptions 4.2.1(a), 4.2.1(b), 4.2.2]{hernandez_2012_discreteTimeMCP}.




\subsection{LP Reformulation for \textQ-functions} \label{sec:dp_lp}

Inspired by the LP reformulation of \eqref{eq:bellman} \cite{hernandez_2012_discreteTimeMCP}, we derive an LP whose optimal solution  solves equation \eqref{eq:bellman_Qform}.
Equation \eqref{eq:bellman_Qform} can be relaxed to an inequality,
	
called the \emph{-operator inequality}. One can show that operator  is monotone, and satisfies value iteration convergence \cite{vanroy_decentADP}. Therefore any  satisfying \eqref{eq:F_operator_inequality} will be a point-wise under-estimator of .
Hence a solution of the following program,
	
coincides with the solution of \eqref{eq:bellman_Qform} for -almost all (-a.a) , where  is a finite measure on  that assigns positive mass to all open subsets of ; see Appendix \ref{app:LPequiv_Qform} for details.
The equivalence between \eqref{eq:bellman_Qform} and \eqref{eq:LP_approach_to_DP_Qform} requires that  is the function space over which the decision variable  is optimized, see \cite[\S 6.3]{hernandez_2012_discreteTimeMCP}. Intuitively speaking,  is rich enough to satisfy  with equality, point-wise for all  and all .


The feasible region of \eqref{eq:LP_approach_to_DP_Qform} can be increased by using an iterated -operator inequality.
A \textQ-function satisfying , with , will be a point-wise under-estimator of .
By  we denote  applications of the  operator, and under \cite[Assumptions 4.2.1(a), 4.2.1(b), 4.2.2]{hernandez_2012_discreteTimeMCP} we have that .
The same reasoning as with \eqref{eq:LP_approach_to_DP_Qform} also establishes that a solution of the following program:
	
coincides with the solution of \eqref{eq:bellman_Qform} for -a.a .



The constraint in \eqref{eq:LP_approach_to_DP_iterated_Qform} is non-linear in  due to nested minimizations and expectations. A linear reformulation is obtained by introducing additional decision variables and constraints. Following the lines of \cite[Theorem 2]{vanroy_decentADP} and \cite[\S 3.4]{boyd_iteratedBellman}, problem \eqref{eq:LP_approach_to_DP_iterated_Qform} is equivalent to the infinite dimensional linear program:

where the inequality constraints hold for all  and .
The propositions necessary to show the equivalence between \eqref{eq:LP_approach_to_DP_iterated_Qform} and \eqref{eq:LP_approach_to_DP_iterated_Qform_full} are given in Appendix \ref{app:LPreformulation_propositions}.




The introduction of the iterated -operator inequality is seemingly unnecessary as it does not change the solution of problem \eqref{eq:LP_approach_to_DP_Qform}, however, it can improve the approximation quality in Section \ref{sec:adp} where the decision variables are restricted to a finite dimensional space.
The LP reformulation \eqref{eq:LP_approach_to_DP_iterated_Qform_full} is necessary for applying the approximation techniques of Section \ref{sec:adp}.


\vspace{0.1cm}
\subsection{Sources of Intractability} \label{sec:dp_intractabilities}


Solving \eqref{eq:LP_approach_to_DP_iterated_Qform_full} for , and implementing \eqref{eq:bellmanpolicy_Qform}, is in general intractable. The difficulties can be categorized as:
	\begin{enumerate}
		\renewcommand{\labelenumi}{(D\theenumi)}
		\item  and  are infinite dimensional spaces;
		
		\item Problem \eqref{eq:LP_approach_to_DP_iterated_Qform_full} has infinite number of constraints;
		
		\item Objective of \eqref{eq:LP_approach_to_DP_iterated_Qform_full} involves a multidimensional integral;
		
		\item The -operator involves an infinite dimensional integral over ;
		
		\item Since  can be any element of , the policy \eqref{eq:bellmanpolicy_Qform} may be intractable;
	\end{enumerate}

Difficulties (D1-D5) relate to the so-called  \emph{curse of dimensionality} \cite{powell_knowAboutADP}, and apply also to the iterated value function formulation in the continuous space setting \cite{boyd_iteratedBellman}. 




\section{Approximate Dynamic Programming (ADP)}\label{sec:adp}


\begin{table*} [t] \centering
\caption{Examples of overcoming (D2-D5)}
\vspace{-0.1cm}
\begin{tabular}{|c|l|l|l|}
		\hline
		Ref.		& Problem instance studied:   & Class of basis functions  & Overcome (D2) by:
		\\
		\hline \cite{boyd_iteratedBellman}, \cite{boyd_2011_minmax}, \cite{boyd_2013_iteratedApproxValueFunctions}
		&
		Linear-quadratic problems
		&
		Quadratic
		&
		S-procedure
		\\
		\hline
		\cite{lasserre_2009_soc_via_occupation_measures}, \cite{boyd_iteratedBellman}, \cite{summers_ADPwithSOS}
		&
		Polynomial problems
		&
		Polynomial
		&
		Sum-of-squares
		\\ 
		\hline
		\cite{vanroy_sampDP}, \cite{boyd_2012_quadraticADP}, \cite{sutter_2014_ADPsamp}
		&
		Finite, linear-quadratic, non-linear
		&
		Finite, quadratic, non-linear
		&
		Sampling
		\\
		\hline
		\cite{nikos_2013_ADPforReachability}, \cite{nikos_2015_ADPforReachability_arXiv}
		&
		Stochastic reachability
		&
		Radial basis functions
		&
		Sampling
		\\
		\hline
		\cite{swaroop_2010_pwcValueFunc}
		\cite{swaroop_2011_pwcValueFunc_forPerimeterPatrol}
		&
		Perimeter surveillance
		&
		Piecewise-constant
		&
		Exact Reformulation
		\\
		\hline
		
	\end{tabular}
\vspace{-0.2cm}
\label{tab:D2_D5_reformuation_summary}
\end{table*}









\subsection{The Approximate LP} \label{sec:adp_approxlp}

As suggested in \cite{schweitzer_originalMDP}, we restrict the value functions and \textQ-functions to take values in the span of a finite family of basis functions  and . We parameterize the restricted function spaces as

for . The subscript  is used to highlight that the restricted function space can be different for each of the value functions and \textQ-functions. If desired, all of the restricted spaces can be taken to be the same.


An approximate solution of  \eqref{eq:LP_approach_to_DP_iterated_Qform} is obtained by the program:
	
where the only change from \eqref{eq:LP_approach_to_DP_iterated_Qform} was to replace  by . The optimization variables are now the 's in the definition of .
To apply existing methods for the LP approach to ADP, we make the constraint in \eqref{eq:LP_approach_to_ADP_iterated_Qform} linear by applying  Proposition \ref{proposition:F_operator_inequality_reformulation} and \ref{proposition:F_operator_iterated_inequality_reformulation} with all the additional value functions and \textQ-functions restricted to  and  respectively.
The additional decision variables and constraints introduced by this linear reformulation are a drawback that we address in Section \ref{sec:unify}.




In general, a solution of \eqref{eq:LP_approach_to_ADP_iterated_Qform}, denoted , will not solve the Bellman equation \eqref{eq:bellman_Qform}.
The following lemma, which follows from \cite[Lemma 1]{vanRoy_linApproxDP}, provides the intuition that  is the closest under-estimator of  weighted by .

\vspace{0.1cm}

\begin{lemma} \label{lemma:approxLP_for_Q_solves_min_1norm}
	 is an optimal solution of \eqref{eq:LP_approach_to_ADP_iterated_Qform} if and only if it is an optimal solution of the following program
		
\end{lemma}




A natural choice for the online policy is to replace  in equation \eqref{eq:bellmanpolicy} with the solution of \eqref{eq:LP_approach_to_ADP_iterated_Qform},
	
often referred to as the \emph{greedy policy}.
A good approximation of the optimal \textQ-function is one for which the online performance of \eqref{eq:approxpolicy_Qform} is near optimal. Although Lemma \ref{lemma:approxLP_for_Q_solves_min_1norm} shows that  is the closest approximate \textQ-function for a given set of basis functions, it reveals nothing about the sub-optimality of policy \eqref{eq:approxpolicy_Qform}. In Section \ref{sec:bounds} we show that the online performance of \eqref{eq:approxpolicy_Qform} can be bounded by how well  approximates .



Problem \eqref{eq:LP_approach_to_ADP_iterated_Qform} overcomes difficulty (D1) as  and  are parameterized by a finite dimensional decision variable.
There are a number of choices of  and  that address (D2-D5). The possible choices depend on the class of the stage cost and dynamics, the description of  and , and the distribution of the exogenous disturbance.
Table \ref{tab:D2_D5_reformuation_summary} summarizes examples found in the literature, where the applicability, approximation quality, and computational burden depends on the problem data and design choices made when a practitioner implements the chosen algorithm.





In \eqref{eq:LP_approach_to_DP_iterated_Qform_full} the specific choice of  does not affect the optimal solution. This is no longer the case in \eqref{eq:LP_approach_to_ADP_iterated_Qform} where the choice of  plays a central role in determining the quality of .
Lemma \ref{lemma:approxLP_for_Q_solves_min_1norm} suggests that one can influence the approximation quality by an appropriate choice of , which is commonly referred to as the \emph{relevance weighting}.
To partly alleviate the dependency on the choice of relevance weighting, \cite{beuchat_2016_ECC_PWMQ} suggests solving \eqref{eq:LP_approach_to_ADP_iterated_Qform} for multiple choices of , and using the point-wise maximum from the family of approximations in the greedy policy. They argue that improved online performance can be achieved with this approach.
Note that if the restricted function space is chosen such that , then the optimal solution of \eqref{eq:LP_approach_to_ADP_iterated_Qform} is  as long as  assigns positive mass to all open subsets of .


For completeness and comparison, we state without derivation that approximate iterated LP for the value function formulation of ADP, as introduced in \cite{boyd_iteratedBellman},

Weighting  here is the counterpart of the relevance weighting in the objective of \eqref{eq:LP_approach_to_ADP_iterated_Qform} and similarly it plays a central role in determining the quality of .
The constraint is called the iterated Bellman inequality and the LP reformulation of the non-linear operator  is given in \cite[\S 3.4]{boyd_iteratedBellman}.



We note that, under the assumptions of Section \ref{sec:dp_prob_form_and_assumptions}, programs \eqref{eq:LP_approach_to_ADP_iterated_Qform} and \eqref{eq:LP_approach_to_ADP_iterated_Vform} are always feasible.
Specifically, under \cite[Assumption 4.2.1(a)]{hernandez_2012_discreteTimeMCP} that the stage cost is non-negative, the choice  for all , , is feasible for both the iterated -operator and Bellman inequality constraints.





\subsection{Iterated Greedy Policy}  \label{sec:adp_improvedPolicy}

The following policy attempts to bridge the gap between finite horizon and two stage problems. Given  and an approximate \textQ-function, we define the iterated greedy policy by
	
The policy may improve upon \eqref{eq:approxpolicy_Qform} for any , where we use the convention that  and hence \eqref{eq:approxpolicy_Qform} and \eqref{eq:approxpolicy_iterated_Qform} coincide when .
However, computing this iterated policy is complicated by the nested expectations and minimizations arising from the  term. Using similar arguments, an iterated greedy policy using an approximate value function, , would be,
	
which also involves nested expectations and minimizations, and coincides with the usual greedy policy for .


Writing out the iterations of the  or  operator, it can be seen that the iterated greedy policy is exactly the generic form of a -stage stochastic programming problem \cite[section 3.1]{shapiro_2014_lectures_on_stoch_prog}.
Popular approximate solution methods for such stochastic programs are Model Predictive Control (MPC) \cite{rawlings_1999_MPCtextbook,camacho_2007_MPC} and Affine Decision Rules (ADR) \cite{ben-tal_adjustable_2004,angelos_2010_generalizedDR}.
In \cite{morari_2014_MPCBook} the authors analyze and provide algorithms for computing an MPC policy parametric in the current state , referred to as \emph{explicit MPC}. For example, when the dynamics are linear and the stage cost quadratic, as is the case in the Section \ref{sec:numerical} examples, the explicit MPC policy is shown to be piecewise linear \cite[\S 6.3]{morari_2014_MPCBook}.
Solving  \eqref{eq:approxpolicy_iterated_Qform} or \eqref{eq:approxpolicy_iterated_Vform} with an MPC approach would be equivalent to a finite horizon MPC formulation, with a time horizon of  steps, and  or  as the terminal cost.
Further details on the connection between ADP and MPC policies are given in \cite{bertsekas_2005_fromADPtoMPC}.


In Section \ref{sec:bounds}, we give a bound on the sub-optimality of the online performance achieved by \eqref{eq:approxpolicy_iterated_Qform} or \eqref{eq:approxpolicy_iterated_Vform}. This indicates that a tighter performance bound can be achieved through the iterated greedy policy. In Section \ref{sec:numerical} we use a numerical example to demonstrate the potential of this interpretation. 




\section{PERFORMANCE BOUNDS FOR ADP} \label{sec:bounds}

In this section, we present performance guarantees for the continuous space setting. The \emph{online performance} bounds in Section \ref{sec:bounds_online} and \emph{Lyapunov-based} bounds in Section \ref{sec:bounds_fitting_lyap} are novel for the continuous space setting and represent a contribution of this paper.
To assist the reader, Table \ref{tab:bounds_roadmap} summarizes the proposed bounds and those found in the literature, \cite{vanRoy_linApproxDP}, \cite{vanroy_decentADP}, \cite{beuchat_2016_ECC_PWMQ}, and \cite{boyd_iteratedBellman}.


Note that the bounds in Section \ref{sec:bounds_fitting_infty} and \ref{sec:bounds_fitting_lyap} require that the restricted functions spaces \eqref{eq:approx_func_spaces} are all the same, which we denote as  and  throughout this section.




\subsection{Online Performance Bound}  \label{sec:bounds_online}

We present first a bound on the online performance of playing the iterated greedy policy \eqref{eq:approxpolicy_iterated_Qform} or \eqref{eq:approxpolicy_iterated_Vform}.
These bounds only require the approximate value function or \textQ-function to be a point-wise under-estimator of  or  respectively.
To this end, we introduce two measures: the \emph{expected state-action frequency},  defined on , and its marginal on the state space,  defined on , called the \emph{expected state frequency}. For any Borel sets  and  the measures are defined as:
	
See \cite[6.3.6]{hernandez_2012_discreteTimeMCP} for further details.
One can show that  is a probability measure.
From Section \ref{sec:dp_prob_form_and_assumptions} it is clear that  in \eqref{eq:onlinePerformane} is a point-wise over-estimator of .
Given a function , define the following: .



\vspace{0.2cm}

\begin{theorem} \label{theorem:online_performance_bound_iterated_Qform}
	Let  be such that  for all  and all , and let  be a -iterated policy defined in \eqref{eq:approxpolicy_iterated_Qform}. Then the sub-optimality of the online performance is bounded as,
	
\end{theorem}
\vspace{0.1cm}
The proof is given in Appendix \ref{app:proof_bound_online_performance}.

\vspace{0.2cm}

\begin{theorem} \label{theorem:online_performance_bound_iterated_Vform}
	Let  be such that  for all , and let  be a -iterated policy defined in \eqref{eq:approxpolicy_iterated_Vform}. Then the sub-optimality of the online performance is bounded as,
	
\end{theorem}
\vspace{0.1cm}
The proof is a minor adaptation of the proof of Theorem \ref{theorem:online_performance_bound_iterated_Qform}.
Notice that for , Theorems \ref{theorem:online_performance_bound_iterated_Qform} and \ref{theorem:online_performance_bound_iterated_Vform} are reminiscent of the finite space versions, \cite[Theorem 1]{vanroy_decentADP} and \cite[Theorem 1]{vanRoy_linApproxDP} respectively. The proofs, however, require a different analysis due to the consideration of continuous spaces.
Fig. \ref{fig:online_bound_visualisation} visualizes the quantities involved.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.24\textwidth]{img/Online_Perf_Bound_v03.pdf}
	\caption[Visualizing the components of the online performance bound]
	{
		The upper plot shows the online performance of policy  is a point-wise over-estimator of , and that by assumption of Theorem \ref{theorem:online_performance_bound_iterated_Vform}  is a point-wise under-estimator. The lower plot highlights that the 1-norm weightings in Theorems \ref{theorem:online_performance_bound_iterated_Qform} and \ref{theorem:online_performance_bound_iterated_Vform},  and , can differ significantly.
	}
	\label{fig:online_bound_visualisation}
\end{figure}


The following insights apply to Theorem \ref{theorem:online_performance_bound_iterated_Qform} and \ref{theorem:online_performance_bound_iterated_Vform}:
\begin{itemize}
	\item They provide the reassurance for continuous space problems that when policy \eqref{eq:approxpolicy_iterated_Qform} or \eqref{eq:approxpolicy_iterated_Vform} uses an under-estimator, the sub-optimality of the online performance is bounded by how closely  or  fits  or  respectively.
	
	\item They motivate the potential benefit of considering a -iterated policy based on an under-estimator. Although  and  are not contractive with respect to the weighted 1-norm, it is expected that the right hand side gets smaller as  increases, and hence the online sub-optimality is more tightly bounded.
\end{itemize}



\vspace{0.2cm}

\subsection{Infinity-norm Bound}  \label{sec:bounds_fitting_infty}


We present now a result that bounds the fitting of  or  relative to  or , by how close  or  is to the span of the basis functions.
These bound were reported in \cite{beuchat_2016_ECC_PWMQ} and \cite{boyd_iteratedBellman} and are included here for completeness.

\vspace{0.1cm}

\begin{theorem} \label{theorem:inf_norm_bound_iterated_Qform}
	Let  be the solution of \eqref{eq:bellman_Qform} and  be the solution of \eqref{eq:LP_approach_to_ADP_iterated_Qform} for a given choice  and  then,
	
	
\end{theorem}
\vspace{-0.1cm}
The proof was first reported in our preliminary study \cite[Theorem 4.1]{beuchat_2016_ECC_PWMQ}. It is included in Appendix \ref{app:proof_bound_inf_norm_Qform} in the interest of completeness.

\vspace{0.1cm}

\begin{theorem} \label{theorem:inf_norm_bound_iterated_Vform}
	Let  be the solution of \eqref{eq:bellman} and  be the solution of \eqref{eq:LP_approach_to_ADP_iterated_Vform} for a given choice  and  then,
	
\end{theorem}
\vspace{0.1cm}
The proof is given in \cite[\S 4.3]{boyd_iteratedBellman}.

\vspace{0.2cm}

Comparing the left-hand-side in Theorem \ref{theorem:inf_norm_bound_iterated_Vform} to the right-hand-side in Theorem \ref{theorem:online_performance_bound_iterated_Vform}, the choice  means that the online performance is bounded by Theorem \ref{theorem:inf_norm_bound_iterated_Vform}. For Theorems \ref{theorem:inf_norm_bound_iterated_Qform} and \ref{theorem:online_performance_bound_iterated_Qform} to be combined in a similar way, the relevance weighting  should satisfy,
	
In both cases, choosing  as described is a difficult task since it is a circular requirement: the choice of  affects the solution of the approximate LP, which in turn affects the approximate policy, which affects the expected state frequency , which in turn affects the desired relevance weighting .


The following insights apply to Theorem \ref{theorem:inf_norm_bound_iterated_Qform} and \ref{theorem:inf_norm_bound_iterated_Vform}:
\begin{itemize}
	\item As  and  may be very large, the bounds may be too conservative for practical use. We investigate this limitation, which affects all similar bounds in literature, through numerical examples in Section \ref{sec:numerical}.
	
	\item The right-hand-side of the bounds hold for any choice of the relevance weightings. Thus, the bounds do not provide any intuition for how to choose  or .
	
	\item A large  tightens the bound via the  term. The upper bound on  is dictated by the size of the approximate LP that can be solved in the time frame available.
	
	\item The right-hand-side of the bounds may be infinite in some cases. Consider for example a linear-quadratic problem on unbounded spaces where  is known to be quadratic. If  is the space of affine functions, then  is infinite for all elements from .
\end{itemize}


\subsection{Lyapunov-based Bound}  \label{sec:bounds_fitting_lyap}

Finally, we derive Lyapunov-based bounds that are novel for the continuous space setting.
To this end, for functions  we define an operator ,
and for functions  we define an operator ,
Given that the system is in state , the function  represents the worst case expected value of  at the next state. For \textQ-functions, given further that action  will be applied, the function  represents the worst case expected value two times steps into the future.
It is readily shown that both  and  are monotone operators.


Given function  and , let,

be the maximum ratio of the worse case expected value at a future time step, to the value in the current state(-by-input).


\vspace{0.1cm}

\begin{definition} \label{def:lyap_func_Vform}
	A function  is called a \underline{\emph{Lyapunov function}} if .
\end{definition}

\vspace{0.1cm}

\begin{definition} \label{def:lyap_func_Qform}
	A function  is called a \underline{\emph{Lyapunov \textQ-function}} if .
\end{definition}

\vspace{0.1cm}

For any positive function, , let  denote the map , and similarly for a strictly positive . Now we can state the bounds.

\vspace{0.1cm}

\begin{theorem} \label{theorem:bound_lyapunov_iterated_Vform}
	Let  be the solution of \eqref{eq:bellman} and  be the solution of \eqref{eq:LP_approach_to_ADP_iterated_Vform} for a given choice  and . Then, for any Lyapunov function ,
	
\end{theorem}
\vspace{0.1cm}
The proof is given in Appendix \ref{app:proof_bound_lyap_Vform}.


\vspace{0.1cm}

\begin{theorem} \label{theorem:bound_lyapunov_iterated_Qform}
	Let  be the solution of \eqref{eq:bellman_Qform} and  be the solution of \eqref{eq:LP_approach_to_ADP_iterated_Qform} for a given choice  and . Then, for any Lyapunov \textQ-function :
	
\end{theorem}
\vspace{0.1cm}
The proof follows by modifying the proof for Theorem \ref{theorem:bound_lyapunov_iterated_Vform}.
When , Theorem \ref{theorem:bound_lyapunov_iterated_Vform} is reminiscent of the finite space version \cite[Theorem 3]{vanRoy_linApproxDP}. The proof requires an adapted analysis due to the consideration of the iterated Bellman inequality.


The following insights apply to Theorem \ref{theorem:bound_lyapunov_iterated_Vform} and \ref{theorem:bound_lyapunov_iterated_Qform}:
\begin{itemize}
	\item Theorems \ref{theorem:inf_norm_bound_iterated_Qform} and \ref{theorem:inf_norm_bound_iterated_Vform} are a special case of \ref{theorem:bound_lyapunov_iterated_Qform} and \ref{theorem:bound_lyapunov_iterated_Vform} because the function that returns a constant value for all ,  is a Lyapunov function with .
	
	\item As the inverse of the Lyapunov function weights the infinity norm term, the bounds may be tighter than Theorems \ref{theorem:inf_norm_bound_iterated_Qform} and \ref{theorem:inf_norm_bound_iterated_Vform}. To see this, consider that in regions where  or  are large, the Lyapunov function may also be chosen to be large and hence reduce the worst case error in those regions. Section \ref{sec:numerical_1d} provides an example where, for larger , the bound tightening is significant.
	
	\item The relevance weighting now appears on the right-hand side of the bound. This indicates that an appropriate choice of relevance weighting is the one which gives the tightest bound. However, finding the combination of a relevance weighting and Lyapunov function that yields the tightest bound is, in general, a difficult problem.
\end{itemize}

We refer to \cite[\S 5]{vanRoy_linApproxDP} for some discussion on the choice of Lyapunov functions for finite space problems.
















\subsection{Comparison to Temporal Difference and \textQ-learning} \label{sec:unify:qlearning_comparison}


Model-free approaches, such as Temporal Difference (TD) learning \cite{sutton_1988_TDlearning} and \textQ-learning \cite{watkins_1989_learningFromDelayedRewards}, aim to optimize the control policy based only on data collected through interactions with the system.
Recent results demonstrate many successes and great potential of these methods, see for example \cite{reviewer_suggestion_2016_modelfree,google_2015_atari,reviewer_suggestion_2018_Qlearning_experience_replay,meyn_2017_zap_qlearning,reviewer_suggestion_2017_policy_gradient,google_2017_alphago}.
By contrast, model-based approaches assume complete and accurate knowledge of the underlying system model when synthesizing a control policy, and through this provide theoretical analysis and performance guarantees.
In particular, the LP approach to ADP ensures that the approximate value functions and \textQ-functions are point wise under-estimators of  and , thus facilitating the theoretical guarantees presented in Sections \ref{sec:bounds_online}, \ref{sec:bounds_fitting_infty}, \ref{sec:bounds_fitting_lyap}.
In this section we compare aspects of the model-based LP approach to ADP with examples from the TD and \textQ-learning literature.
We refer the reader to \cite[Chapter 8]{sutton_2018_rlbook} for further discussion on comparing model-based and model-free approaches.




Similar to the LP approach, many variants of TD learning also use a linearly parameterized function space for the approximation architecture, for example,  \cite{sutton_1988_TDlearning,bradtke_1996_lstd,boyan_2002_lstd}. In \cite{tsitsiklis_1997_TD_analysis} the authors provide theoretical guarantees on the approximation quality of the solution from TD learning. In particular, \cite[Theorem 1]{tsitsiklis_1997_TD_analysis} provides an approximation quality bound that is reminiscent of Theorem \ref{theorem:inf_norm_bound_iterated_Vform}. However, as TD learning is designed for autonomous systems, there is no notion of an online performance guarantee.
For a controlled system, the Actor-Critic algorithm in \cite{konda_2003_actorcritic_journal} uses TD learning in the critic step, and in the actor step it makes gradient updates in the control policy space. The authors show convergence of their Actor-Critic algorithm to a local minimum with respect to the parametrization of the control policy. However, they do not provide any bound on the sub-optimality of the resulting policy.
By contrast, the model-based LP approach to ADP allows one to compute online performance guarantees such as those offered by Theorems \ref{theorem:online_performance_bound_iterated_Qform} and \ref{theorem:online_performance_bound_iterated_Vform}.



Recent \textQ-learning methods utilize Neural Networks for the restricted function space \cite[Section 6.3.1]{bertsekas_2017_DP_vol1}, \cite{reviewer_suggestion_2017_policy_gradient}, and demonstrate many successes, for example playing games \cite{google_2015_atari,google_2017_alphago} and regulating a two-degree-of-freedom helicopter \cite{reviewer_suggestion_2018_helicopter}.
This suggests that Neural Networks can be an interesting choice of restricted function space for the LP approach to ADP presented in this paper.
However, the non-linear nature of Neural Networks will complicate the analysis of the LP approach and makes for an interesting future research direction, potentially providing guarantees for a fixed Neural Network architecture. 




\section{PARTICULAR \textQ-FUNCTION FORMULATIONS} \label{sec:unify}


In this section, we consider cases for which the \textQ-function formulation can be simplified. We first present the condition which facilitates this simplification, thus making the formulation computationally efficient. We then provide two problem classes for which the condition is satisfied. In particular, this formulation can be beneficial for the decentralized control designs that we discuss in Section \ref{sec:unify_structured_functions}.

\subsection{Condition for equivalence} \label{sec:unify_lemma}

Applying Propositions \ref{proposition:F_operator_inequality_reformulation} and \ref{proposition:F_operator_iterated_inequality_reformulation} to \eqref{eq:LP_approach_to_ADP_iterated_Qform}, the approximate LP for the \textQ-function formulation is,

			\hspace{-0.3cm}
			\max_{\hat{Q}_j,\hat{V}_j}
				\,&\quad \int_{\mcal{X} \times \mcal{U}} \, \hat{Q}_0(x,u) \,\, c(x,u) \, \intd{x} \intd{u}
				\nonumber
			\\
			\subjto \,&\quad \hat{Q}_j \in \approxFuncSpaceXUindex{j}
				,\,
				\hat{V}_j \in \approxFuncSpaceXindex{j}
				\,,
				&& \hspace{-0.10cm} j=0,\dots,M\!-\!1
				\,,
				\nonumber
			\\
			\,&\quad \hat{Q}_{j}(x,u) \,\leq\, \mcal{T}_u \hat{V}_{j}(x,u)
				\,,
				&& \hspace{-0.65cm} j=0,\dots,M\!-\!1
				\,,
				\label{eq:PropOfEquiv_forQ_01}
			\\
			\,&\quad \hat{V}_{j}(x) \,\leq\, \hat{Q}_{j+1}(x,u)
				\,,
				&& \hspace{-0.65cm} j=0,\dots,M\!-\!2
				\,,
				\label{eq:PropOfEquiv_forQ_02}
			\\
			\,&\quad \hat{V}_{M-1}(x) \,\leq\, \hat{Q}_{0}(x,u)
				\,,
				\label{eq:PropOfEquiv_forQ_03}
		
where the inequality constraints hold for all  and all . Now consider the following formulation with  fewer \textQ-functions and  fewer infinite constraints:
	
			\hspace{-0.3cm}
			\max_{\hat{Q}_0 , \hat{V}_j}
				\,&\quad \int_{\mcal{X} \times \mcal{U}} \, \hat{Q}_{0}(x,u) \,\, c(x,u) \, \intd{x} \intd{u}
				\nonumber
			\\
			\subjto \,&\quad \hat{Q}_{0} \in \approxFuncSpaceXUindex{0}
				,\,
				\hat{V}_{j} \in \approxFuncSpaceXindex{j}
				\,,
				&& \hspace{-0.10cm} j=0,\dots,M\!-\!1
				\,,
				\nonumber
			\\
			\,&\quad \hat{Q}_{0}(x,u) \,\leq\, \mcal{T}_u \hat{V}_{0} (x,u)
				\,,
				\label{eq:PropOfEquiv_forV_01}
			\\
			\,&\quad \hat{V}_{j-1}(x) \,\leq\, \mcal{T}_u \hat{V}_{j}(x,u)
				\,,
				&& \hspace{-0.65cm} j=1,\dots,M\!-\!1
				\,,
				\label{eq:PropOfEquiv_forV_02}
			\\
			\,&\quad \hat{V}_{M-1}(x) \leq \hat{Q}_{0}(x,u)
				\,,
				\label{eq:PropOfEquiv_forV_03}
		
where the inequality constraints hold for all  and all . In Lemma \ref{lemma:Qform_Vform_equivalence} below, we provide a condition for when \eqref{eq:PropOfEquiv_forQ} and \eqref{eq:PropOfEquiv_forV} are equivalent. 

\vspace{0.1cm}

\begin{lemma} \label{lemma:Qform_Vform_equivalence}
	If the sets  and  are chosen such that for all  there exists a  with
	
	for ,
then the approximate LP \eqref{eq:PropOfEquiv_forQ} and \eqref{eq:PropOfEquiv_forV} have the same optimal value and there is a mapping between feasible and optimal solutions in both problems.
\end{lemma}
\vspace{0.1cm}
The proof is given in Appendix \ref{app:Qform_equivalence}.



\subsection{Input constrained, Linear-Quadratic control} \label{sec:unify_LQ}

In the case of linear dynamics, quadratic cost function, and control actions constrained to lie in a polytopic feasible set, then the value function and \textQ-function is known to be piece-wise quadratic \cite[Theorem 6.7]{morari_2014_MPCBook}. Hence, quadratic basis functions defined as,

are reasonable choices, see \cite[\S 6]{boyd_iteratedBellman}. The 's and 's from \eqref{eq:approx_func_spaces} are the coefficients of the monomials. In this setting, for any quadratic value function, the term
	
will be quadratic in , and requires knowledge of the first and second moments of the exogenous disturbance. As  is taken to be the space of all quadratic functions in , the condition of Lemma \ref{lemma:Qform_Vform_equivalence} is satisfied.



\subsection{Structured \textQ-functions for decentralized control} \label{sec:unify_structured_functions}

Consider a decentralized control problem with  agents. The input for each agent, , can only depend on a locally available portion of the state vector, , i.e., a decentralized policy is of the form,
	
This framework is not readily addressed by traditional DP formulations.



As the greedy policy \eqref{eq:approxpolicy_Qform} is a constrained optimization problem, decentralized control is realized if both the objective and constraint set have the required separable structure. For the constraint  we assume that the set is separable, i.e., . As The \textQ-function is the objective of \eqref{eq:approxpolicy_Qform}, it will be separable if the \textQ-function is a sum of per-agent \textQ-functions that only depend on  and . Let  denote the set of functions with the separable structure:
	
where  can be any function of the full state vector. The term  is allowed because it does not affect the decision made by evaluating the greedy policy. This separable \textQ-function structure is as suggested in \cite{vanroy_decentADP}.



It is necessary to enforce the structural constraint \eqref{eq:Q_decent_structure} on  in both \eqref{eq:PropOfEquiv_forQ} and \eqref{eq:PropOfEquiv_forV}. The remaining \textQ-functions and value functions need not have the decentralized structure enforced.
Lemma \ref{lemma:Qform_Vform_equivalence} allows a different restricted function space for , and hence can be applied to the decentralized control formulation.


The value function formulation can also be used to approximate a solution to the decentralized control problem. It requires the assumption that , and the restriction on the approximate value function that . 








\section{NUMERICAL RESULTS} \label{sec:numerical}

In this section we present three numerical examples to highlight various aspects of the theory presented above. The first example numerically evaluates the performance bounds from Section \ref{sec:bounds}, the second assesses the potential of the iterated approximate policy presented in Section \ref{sec:adp}, whereas the third demonstrates using \textQ-function for a distributed control setting as per Section \ref{sec:unify_structured_functions}. The second example also provides empirical evidence that the \textQ-function formulation can achieve tighter lower bounds.


In all numerical examples we use a Linear Quadratic Regulator (LQR) as a point of comparison. This is a linear state feedback controller synthesized via the Riccati equation for a system with linear dynamics and quadratic stage cost.
The code to generate the results is found at \cite{beuchat_2017_ADPToolbox}.




\subsection{Evaluation of Performance Bounds} \label{sec:numerical_1d}

We use a one dimensional example from \cite{boyd_iteratedBellman} with  to highlight that although the iterated value function gives an tighter lower bound of the optimal cost-to-go, it can have both worse online performance, and a worse online performance bound. The dynamics, costs, and constraints are given by,
	
with the exogenous disturbance and initial condition distributed as  and , respectively. The benefit of using a 1-dimensional example is that the value function and optimal policy ( and ) can be effectively approximated by using a discretization method and used to directly asses the quality of approximation.


We use the space of univariate quadratics as , without a linear term due to the problem symmetry, and we choose the state-relevance weighting as the initial state distribution, i.e.,

We compare approximate value functions, solved via the iterated approximate LP with  and their respective approximate policies.



Table \ref{tab:OnlineBounds_for1DExample} shows the bound of Theorem \ref{theorem:online_performance_bound_iterated_Vform} for this example, and Table \ref{tab:FittingBounds_for1DExample} shows the bounds of Theorems \ref{theorem:inf_norm_bound_iterated_Vform} and \ref{theorem:bound_lyapunov_iterated_Vform}.
For completeness, details on the computation of , , , , and the Lyupanov functions are given in Appendix \ref{app:supplement_for_performance_bounds_numerical}.
 

\begin{table} [h]
	\centering
	\caption{Bounds for example \ref{sec:numerical_1d}. The last column is the percentage decrease from the right-hand-side of Theorem \ref{theorem:inf_norm_bound_iterated_Vform} to the right-hand-side of Theorem \ref{theorem:bound_lyapunov_iterated_Vform}, and  is for the Lyapunov function that gives the smallest value for right-hand-side of Theorem \ref{theorem:bound_lyapunov_iterated_Vform}.}
	\begin{tabular}{|c|c c c c c|}
\iftoggle{doublecolumn}{
			\hline
			\multirow{2}{*}{} 
			& \multirow{2}{1.6cm}{LHS of Thm. \ref{theorem:inf_norm_bound_iterated_Vform} \& \ref{theorem:bound_lyapunov_iterated_Vform}}
& \multirow{2}{1.1cm}{RHS of Thm. \ref{theorem:inf_norm_bound_iterated_Vform}}
			& \multirow{2}{1.1cm}{RHS of Thm. \ref{theorem:bound_lyapunov_iterated_Vform}}
			& \multirow{2}{*}{}
			& \multirow{2}{*}{}
}{
			\hline
			\multirow{2}{*}{} 
			& \multirow{2}{1.8cm}{LHS of Thm. \ref{theorem:inf_norm_bound_iterated_Vform} \& \ref{theorem:bound_lyapunov_iterated_Vform}}
& \multirow{2}{1.4cm}{RHS of Thm. \ref{theorem:inf_norm_bound_iterated_Vform}}
			& \multirow{2}{1.4cm}{RHS of Thm. \ref{theorem:bound_lyapunov_iterated_Vform}}
			& \multirow{2}{*}{}
			& \multirow{2}{*}{}
}
\\
		&&&&&
		\\
		\hline
		
		& 
& 
		& 
		& 
		& 
		\\

		& 
& 
		& 
		& 
		& 
		\\

		& 
& 
		& 
		& 
		& 
		\\
		\hline
	\end{tabular}
	\label{tab:FittingBounds_for1DExample}
\end{table}




\begin{table} [h]
	\centering
	\caption{Online performance bound of Theorem \ref{theorem:online_performance_bound_iterated_Vform} for example \ref{sec:numerical_1d}. Evaluated numerically using  Monte Carlo simulations per controller.}
	\begin{tabular}{|c c|c c c|}
		\hline
		
		& 
		&  
		& 
		& 
		\\
		\hline
		\multicolumn{2}{|c|}{LQR}
		& 
		& 
		& 
		\\
 & 
		& 
		& 
		& 
		\\
 & 
		& 
		& 
		& 
		\\
 & 
		& 
		& 
		& 
		\\
 & 
		& 
		& 
		& 
		\\
 & 
		& 
		& 
		& 
		\\
 & 
		& 
		& 
		& 
		\\
 & 
		& 
		& 
		& 
		\\
 & 
		& 
		& 
		& 
		\\
		\hline
	\end{tabular}
	\label{tab:OnlineBounds_for1DExample}
\end{table}


Table \ref{tab:FittingBounds_for1DExample} shows that, for this example, the bounds of Theorems \ref{theorem:inf_norm_bound_iterated_Vform} and \ref{theorem:bound_lyapunov_iterated_Vform} are conservative, but the Lyapnov-based approach tightens the bound for all values of . Interestingly, the benefit of the Lyapunov-based bound is more pronounced for larger .
To understand the reasoning for this example, see that that a  closer to  coincides with a Lyapunov function that minimizes the term:
	
However, a larger  is required to ensure that the denominator term  does not dominate the bound. For each value of , there is a sweet spot that gives the tightest bound, see Appendix \ref{app:supplement_for_performance_bounds_numerical}. This highlights the benefit of deriving the Lyapunov-based bound using the iterated Bellman formulation.


As indicated by the dependence on , the right-hand-side of Theorems \ref{theorem:inf_norm_bound_iterated_Vform} and \ref{theorem:bound_lyapunov_iterated_Vform} are improved by more than an order of magnitude in going from  to . However, as the bounds are anyway conservative, it is not clear that in general the left-hand-side of Theorem \ref{theorem:inf_norm_bound_iterated_Vform} and \ref{theorem:bound_lyapunov_iterated_Vform} should decrease as  increases.
For this example, the choice  means the left-hand-side is the under-estimation error of the optimal cost-to-go, , and numerically agree with \cite{boyd_iteratedBellman}.









\begin{table*}[t] \centering
	\caption{Results averaged over  randomly generated -dimensional examples for each . For the online performance, the expectation over  is computed using  samples, and expectation with respect to  is computed from  Monte Carlo simulation each of length  time steps. For the lower bounds the expectation over  is computed from the same  samples. In order to aggregate results across different systems, the costs and computation times are normalized with respect to the average performance of the MPC controller with horizon . The column ``Controller computation" relates to the average computation time in milliseconds to compute the control action at each time step, using a single thread on a 3.00Ghz Xeon processor. The ratio to the controller with the highest computation load is shown in the ``speed-up" column.
	}
	
	\vspace{0.1cm}
	\begin{tabular}{|cc|l|cccc|cccc|cc|}
		\hline
		\multicolumn{3}{|c|}{\textbf{Description}} &
		\multicolumn{4}{|c|}{\textbf{Normalized cost, }} &
		\multicolumn{4}{|c|}{\textbf{Normalized cost, }} &
		\multicolumn{2}{|c|}{\textbf{Controller computation}}
		\\
		\multicolumn{3}{|c|}{} &
		\textbf{avg.} &
		\textbf{} &
		\textbf{min.} &
		\textbf{max.} &
		\textbf{avg.} &
		\textbf{} &
		\textbf{min.} &
		\textbf{max.} &
		\textbf{time (ms)} &
		\textbf{speed-up}
		\\
		\hline\hline
		\multirow{13}{0.0cm}{\rotatebox{90}{Online}} &
		\multirow{13}{0.1cm}{\rotatebox{90}{Performance}} &
		LQR &
		  &    &    &    &
		  &    &    &    &   & 
		\\
&&  with ,  &
		  &    &    &    &
		  &    &    &    &    &  
		\\
&&  with ,  &
		  &    &    &    &
		  &    &    &    &    &  
		\\
&&  with ,  &
		  &    &    &    &
		  &    &    &    &    &  
		\\
&&  with ,  &
		  &    &    &    &
		  &    &    &    &    &  
		\\
&& MPC: ,  &
		  &    &    &    &
		  &    &    &    &    &  
		\\
&&  with ,  &
		  &    &    &    &
		  &    &    &    &    &  
		\\
&&  with ,  &
		  &    &    &    &
		  &    &    &    &    &  
		\\
&&  with ,  &
		  &    &    &    &
		  &    &    &    &    &  
		\\
&&  with ,  &
		  &    &    &    &
		  &    &    &    &    &  
		\\
&& MPC: ,  &
		  &    &    &    &
		  &    &    &    &    &  
		\\
		\hline\hline
		&&Optimal, , and    &
		\multicolumn{4}{|c|}{not available} &
		\multicolumn{4}{|c|}{not available} &
		 &
		
		\\
		\hline\hline
		\multirow{4}{0.0cm}{\rotatebox{90}{Lower}} &
		\multirow{4}{0.1cm}{\rotatebox{90}{Bound}} &
		 with  &
		  &    &    &    &
		  &    &    &    &       &     
		\\
&&  with  &
		  &    &    &    &
		  &    &    &    &       &     
		\\
&&  with  &
		  &    &    &    &
		  &    &    &    &       &     
		\\
&&  with  &
		  &    &    &    &
		  &    &    &    &       &     
		\\
		\hline
	\end{tabular}
	
	\label{tab:numerical_results_NDexample}
\end{table*}












Table \ref{tab:OnlineBounds_for1DExample} shows that, for this example, the online performance bounds of Theorem \ref{theorem:online_performance_bound_iterated_Vform} is also conservative, and that the iterated policy tightens the bound.
The difficulty in choosing the state-relevance weighting is highlighted by the fact that the approximate value function with  gives a better lower-bound of  but has worse online performance.
For the  approximate LP, it would be possible to choose a  different from  that yields a value function similar to  with .
Thus there is an inherent discrepancy between choosing a  that maximizes the lower-bound of , useful for assessing sub-optimality, and choosing a  that achieves the best online performance. 


The bottom five rows of Table \ref{tab:OnlineBounds_for1DExample} show that, as expected, the iterated policy improves both the online performance and the online performance bound. As nice feature of this bound is that it theoretically converges to  as  increases. However, for higher dimensional systems the Bellman operator can only be approximated for a low number of iterations.
We study this in more detail in the next section.
















\subsection{High-dimensional example} \label{sec:numerical_nd}

To highlight the potential of the iterated greedy policy, proposed in Section \ref{sec:adp_improvedPolicy}, on a system of higher dimension we consider an input constrained Linear Quadratic Regulator (LQR) problem. The system dynamics are , with , , , and the matrices , , , of compatible size, describe the linear dynamics.
The  and  matrices are randomly generated with  scaled to be marginally stable, and the results are averaged over the performance on 20 separate extractions for each .
In all cases  is an identity matrix, and the exogenous disturbance and initial condition are distributed as  and , respectively.
The  space is unconstrained, while the  space is a hyper-rectangle with the lower and upper bounds chosen to make the constrains relevant for the whole horizon.



Table \ref{tab:numerical_results_NDexample} presents the online performance results of using quadratic approximate value functions and \textQ-functions, parameterized as in \eqref{eq:quadratic_basis_function_space}. We solve \eqref{eq:PropOfEquiv_forV} with  and  and simulate both the greedy policy \eqref{eq:approxpolicy_Qform}, and the iterated greedy policy \eqref{eq:approxpolicy_iterated_Qform} and \eqref{eq:approxpolicy_iterated_Vform} with . As discussed in Section \ref{sec:adp_improvedPolicy}, the iterated greedy policy is approximated with an MPC-reformulation, where  corresponds to a prediction horizon of . For a comparison controller, we use MPC with prediction horizon  and the Riccati equation solution as the terminal cost function, implemented with the batch approach as detailed in \cite[\S 8.2]{morari_2014_MPCBook}.
The online performance is computed as  using  Monte Carlo samples from , and the expectation with respect to  computed from  Monte Carlo simulations each of length  time steps.
The table also presents lower-bounds on the value function implied by each approximation. The lower-bound is computed as  and  respectively, with the expectations computed using the same  Monte Carlo samples from .


For this example,  with  gives a tighter lower bound when compared to  with , the same trend as for the 1 dimensional example of Section \ref{sec:numerical_1d}. By contrast, the online performance of the greedy policy using  with  is improved compared to using  with .
The iterated greedy policy achieves a noticeable improvement in the online performance, compared to the standard greedy policy, both for the value function and \textQ-function formulation.







\begin{figure*} [t]
	\raggedright
\begin{tikzpicture}
\node[inner sep=0pt,anchor=south west] at (0.0cm,1.0cm){
		\includegraphics[width=5.6cm]
		{img/smd_layout_3agentstring_stochastic.pdf}
	};
\node[inner sep=0pt,anchor=south west] at (6.1cm,0.6cm){
		\includegraphics[width=3.7cm]
		{img/smd_Qstructure_3agentstring_withCommunication.pdf}
	};
\node[inner sep=0pt,anchor=south west] at (10.9cm,0.6cm){
		\includegraphics[width=6.8cm]
		{img/distributedControl_cost_vs_communication.pdf}
	};
\node[align=center , anchor=center , rotate=0] at (2.8cm,0.0cm) {\small{ (a) }};
	\node[align=center , anchor=center , rotate=0] at (7.95cm,0.0cm) {\small{ (b) }};
	\node[align=center , anchor=center , rotate=0] at (10.5cm,0.0cm) {\small{ (c) }};
\coordinate (GO) at (9.64cm,-0.2cm);
\node[align=center , rotate=0] at ()
	{\small{ Number neighbour communication }};
\node[align=center , anchor=north , rotate=0] at () {\footnotesize{  }};
	\node[align=center , anchor=north , rotate=0] at () {\footnotesize{  }};
	\node[align=center , anchor=north , rotate=0] at () {\footnotesize{  }};
	\node[align=center , anchor=north , rotate=0] at () {\footnotesize{  }};
	\node[align=center , anchor=north , rotate=0] at () {\footnotesize{  }};
\node[align=right , anchor=east , rotate=0] at () {\footnotesize{  }};
	\node[align=right , anchor=east , rotate=0] at () {\footnotesize{  }};
	\node[align=right , anchor=east , rotate=0] at () {\footnotesize{  }};
\draw[gray70,line width = 0.5pt,fill=white] () rectangle ();
\draw[matlabblue,line width = 1.2pt,solid]  () -- ();
	\draw[matlabblue,line width = 1.2pt,dashed] () -- ();
	\node[right] at () {\color{matlabblue}{{\small}}};
\draw[matlabpurple,line width = 1.2pt,solid]  () -- ();
	\draw[matlabpurple,line width = 1.2pt,dashed] () -- ();
	\node[right] at () {\color{matlabpurple}{\small}};
\draw[matlaborange ,line width = 1.2pt,solid]  () -- ();
	\draw[matlaborange ,line width = 1.2pt,dashed] () -- ();
	\node[right] at () {\color{matlaborange}{\small}};
	\end{tikzpicture}
\caption[Short-hand caption]{
(a) Schematic of coupled oscillator model used to demonstrate using \textQ-functions for distributed control. The constituent sub-systems are the masses , that respectively make decision  based on state measurements  and  in the decentralized setting, and may also have access to the state measurements of neighbouring masses in the distributed setting.
(b) Quadratic approximate \textQ-function structure used for the  matrix from \eqref{eq:quadratic_basis_function_space}, i.e., each square represents the coefficient of an order 2 monomial. When only the dark shaded elements are non-zero the greedy policy is decentralized, and when additionally the dotted elements are non-zero the greedy policy is distributed with nearest neighbour communication.
(c) Online performance (solid) and lower bounds (dashed) for the coupled oscillator example versus the communication connections. The centralized optimal (dotted) is 757.4 for this example. The horizontal axis is the number of neighbouring oscillators, in each direction, from which state measurements are available for making control decisions. Thus,  represents a decentralized controller, and  represents a controller with nearest neighbour communication.
	}
	\label{fig:smd}
\end{figure*}















The most striking feature of this numerical example is the similarity between the iterated greedy policy and the MPC controller used for comparison. They only differ in the time horizon and choice of terminal cost function. The results in Table \ref{tab:numerical_results_NDexample} highlight that the  and  encode a sufficient approximation of the cost-to-go function to allow for a shorter horizon to be used; for example, using  for the iterated greedy policy results in comparable performance to an MPC controller with horizon  and an LQR based terminal cost, but at a fraction of the computational cost. In all cases, computing the policy involves solving a Quadratic Program with the number of decision variables and constraints proportional to the prediction horizon.



This numerical example also indicates that for a system where the input constraints are active at the end of the MPC prediction horizon, choosing an approximate value function or approximate \textQ-function for the terminal cost can lead to improved online performance. This comes at the expense of solving the approximate LP \eqref{eq:LP_approach_to_ADP_iterated_Qform} or \eqref{eq:LP_approach_to_ADP_iterated_Vform}, which for larger  is more computationally demanding than solving the Riccati equation.
The computation time was 20 seconds for  and 24 minutes for  on a 4.0Ghz Intel Core i7 processor, with Appendix \ref{app:Sprocedure_reformulation} providing details of how the problem was reformulated for a commercial solver.
As the approximate LP only needs to be solved once for a particular system, this computation can be performed off-line, and the result offers improvements for the online performance and computation as demonstrated by this example.















\subsection{Coupled Oscillator Example} \label{sec:numerical_smd}







To demonstrate the application of \textQ-functions for distributed control we use a string of coupled oscillators, visualized as a spring-mass-dampener system in Fig.\,\ref{fig:smd}(a). Each mass is considered as a separate system, and needs to make its control decision based on the measurement of its own state, and possibly that of its nearest neighbours.



The coupled oscillator can be modelled by a linear system readily derived by writing the equations of motion for each mass. The state vector is the position and velocity of each mass, denoted as  and  respectively. Each mass can be controlled by a driving force  applied to the mass. The exogenous driving force is  and the factor  represents an external influence. The spring constant and dampening ratio of the elements connecting mass  to mass  are denoted by  and  respectively. The fixed wall is represented as .


The online performance of the distributed control policies is compared to the optimal centralized policy. To be able to compute the centralized optimal, we use a quadratic stage and unconstrained state and action spaces. The stage cost for each mass is , with a discount factor of , and the dynamics is converted to discrete time with a  second sampling time. 
Fig.\,\ref{fig:smd}(a) shows a system with  masses for clarity, but for the numerical results in Fig.\,\ref{fig:smd}(c) we simulate a system with  masses.


As described in Section \ref{sec:unify_structured_functions}, an approximate \textQ-function can lead to a decentralized greedy policy if given an appropriate structure. The  is quadratic for this coupled oscillator example and due to the dynamic coupling the optimal greedy policy does not have a separable structure. Thus, for the restricted function space  we use quadratic functions parameterized as in \eqref{eq:quadratic_basis_function_space} with the  matrix restricted to have the structure shown in Fig.\,\ref{fig:smd}(b). The structure is shown for a three mass example and is readily extended for a longer string of masses. The approximate greedy policy is decentralized if the shaded structure is used, and distributed with nearest neighbour communication if additionally the dotted elements are non-zero. Note that .




Fig.\,\ref{fig:smd}(c) presents the online performance results of using structured approximate \textQ-functions for decentralized and distributed control of the coupled oscillator system with 20 masses and the parameters randomly drawn from a uniform distribution on the following ranges: , , , .
The exogenous disturbance and initial condition are assumed to be distributed according to , , and  respectively.



We solve \eqref{eq:PropOfEquiv_forV} with  and simulate greedy policy \eqref{eq:approxpolicy_Qform}. The online performance is computed using  Monte Carlo samples from the initial state distribution, and the expectation with respect to  is computed from  Monte Carlo simulations each of length  time steps.
As a datum, the online performance of the centralized LQR controller is 757.4 which lies between the upper and lower bound curves in Fig.\,\ref{fig:smd}(c).
For each approximate \textQ-function, the lower-bound is computed from the same initial condition samples. Note that these are all lower bounds on the centralized LQR performance because problem \eqref{eq:PropOfEquiv_forV} is formulated to approximate the centralized problem.



The results in Fig.\,\ref{fig:smd}(c) show that, for this example, the decentralized/distributed ADP approach using \textQ-functions, can produce near centralized optimal performance: within  in the decentralized case, within  in the distributed, nearest neighbour communication, case. The online performance is significantly influenced by the choice of . 



\section{Conclusions} \label{sec:conclusion}

In this paper we derived theoretical performance guarantees for the Linear Programming Approach to Approximate Dynamic Programming in continuous spaces.
We analyzed an iterated version of the greedy policy to provide a guarantee that the online performance of the policy is bounded.
We provided a Lyapunov-based bound on the approximation quality of a solution using the LP approach with the iterated Bellman inequality. This bound demonstrates a  tightening, compared to the bound presented in \cite{boyd_iteratedBellman}, on the numerical example for which the bounds were evaluated.




We proposed a condition that allows for a more efficient iterated \textQ-function formulation. A numerical case study on linear-quadratic examples with a 50-dimensional state vector demonstrates the potential for large-scale systems.
Using an approximate value function or \textQ-function as a terminal cost for an MPC type controller achieves for these examples comparable online performance with one fifth of the online computational load.
The proposed condition applies also when using \textQ-functions in a decentralized control framework. The online performance using decentralized \textQ-functions is within  of the optimal centralized performance for the coupled oscillator example.


As future work, we aim to adapt the LP approach to ADP to address the challenge of tuning the relevance weighting parameter, and through this reduce the conservativeness of the theoretical guarantees.
The numerical results demonstrate potential benefits for application to physical systems, particularly for systems with fast dynamics where the online policy is implemented on an embedded micro-controller with limited computation resources. 


\appendices











\section{Equivalence of \textQ-function LP} \label{app:LPequiv_Qform}

The theorem below provides the condition on  under which the solution of  \eqref{eq:bellman_Qform}, the \textQ-function variant of the Bellman equation, is feasible and optimal for the LP \eqref{eq:LP_approach_to_DP_Qform}.
Let  denote the discrete-time transition kernel describing the state evolution under the dynamics and the exogenous and control inputs, i.e., given a Borel set ,
	
represents the probability that state  will be in  given that the system is currently in state  and input  is played.
Furthermore, let  and  be defined as the vector spaces of finite signed measures on  and  respectively, bounded as per \cite[Definition 6.3.2, 6.3.4]{hernandez_2012_discreteTimeMCP}.
We use  throughout as short-hand notation for .





Given  we define an operator  as,
	
Thus, the dual LP of \eqref{eq:LP_approach_to_DP_Qform} is,

where  is the non-negative variant of .
Now we state the requirement on the state-by-input relevance weighting for  \eqref{eq:LP_approach_to_DP_Qform} to recover  for -a.a. .

\vspace{0.1cm}

\begin{theorem} \label{theorem:Q_primal_equiv}
	Under \cite[Assumptions 4.2.1(a), 4.2.1(b), 4.2.2]{hernandez_2012_discreteTimeMCP}, if  and  satisfy,
		
	with , then,
the optimal values of \eqref{eq:LP_approach_to_DP_Qform} and \eqref{eq:LP_approach_to_DP_Qform_dual} coincide with,

\end{theorem}

\vspace{0.1cm}

\begin{IEEEproof}
	As the term  in the objective of \eqref{eq:LP_approach_to_DP_Qform_dual} is an additive constant with respect to the decision variable , we have by \cite[Theorem 6.3.7]{hernandez_2012_discreteTimeMCP} that the optimal value of \eqref{eq:LP_approach_to_DP_Qform_dual}, denoted , satisfies,
		
	The first equality follows from \cite[Theorem 6.3.7]{hernandez_2012_discreteTimeMCP}. The second equality substitutes \eqref{theorem:Q_primal_equiv_eq1} for  and uses Fubini's theorem to switch the order of integration. The third equality is the definition of integration with respect to the transition kernel, and the final equality is the definition of .
	
	The strong duality between \eqref{eq:LP_approach_to_DP_Qform_dual} and \eqref{eq:LP_approach_to_DP_Qform} follows from \cite[Theorem 6.3.8]{hernandez_2012_discreteTimeMCP}. They use the sequence of value functions:  and for all  and 
		
	see \cite[equation 6.3.38]{hernandez_2012_discreteTimeMCP}, to show that both the primal and dual programs converger to .
By defining a \textQ-function to match each value function for all  and 
		
	it follows that problems \eqref{eq:LP_approach_to_DP_Qform} and \eqref{eq:LP_approach_to_DP_Qform_dual} have the same optimal value, and that problem \eqref{eq:LP_approach_to_DP_Qform} attains the supremum.
\end{IEEEproof}


 




\section{Propositions for LP reformulation} \label{app:LPreformulation_propositions}

This appendix states the propositions necessary for the reformulation of the non-linear iterated -operator inequality constraint as a set of linear constraints.

\vspace{0.1cm}
\begin{proposition} \label{proposition:F_operator_inequality_reformulation}
	For an arbitrary  the following statements are equivalent:
	\vspace{0.1cm}
	\begin{enumerate}
		\renewcommand{\labelenumi}{(\roman{enumi})}
		\item  for all  and all ;
		
		\vspace{0.1cm}
		
		\item There exists  such that  and   for all  and all .
	\end{enumerate}
\end{proposition}
\vspace{0.1cm}
The proof is given in \cite[Theorem 2]{vanroy_decentADP}.
Note, if  is in some subset of , then the reformulation is only sufficient, i.e., (ii)(i).

\vspace{0.1cm}
\begin{proposition} \label{proposition:F_operator_iterated_inequality_reformulation}
	For an arbitrary  the following are equivalent:
	\vspace{0.1cm}
	\begin{enumerate}
		\renewcommand{\labelenumi}{(\roman{enumi})}
		\item  for all  and all ;
		
		\vspace{0.1cm}
		
		\item There exists  such that:

		
	\end{enumerate}
	where the inequalities hold for all  and all .
\end{proposition}
\vspace{0.1cm}
The proof follows from \cite[\S 3.4]{boyd_iteratedBellman}.
Note, if for any ,  is in some subset of , then the reformulation is only sufficient, i.e., (ii)(i). 









\section{Proof of online performance bound} \label{app:proof_bound_online_performance}

Given a measure  (see \cite[Definition 6.3.4]{hernandez_2012_discreteTimeMCP}), a feasible policy , and a Borel set , define the operator  as,
	
Thus  represents the discounted difference in occupancy measure between two time steps of the stochastic process. Given a function , and the same feasible policy, consider also the operator  defined as,
	
Thus  represents the expected value of discounted difference between two time steps of the stochastic process. Both operators define a continuous linear map on the corresponding spaces and are adjoints of each other, i.e.,
	
see \cite[Section 6.3]{hernandez_2012_discreteTimeMCP}. The online performance bound for finite space is proven by inverting the transition kernel matrix, see \cite[Theorem 1]{vanRoy_linApproxDP}. The adjoint property of  and  can be seen as a counterpart to inverting the transition kernel.

A required identity is that the online performance can be expressed in terms of the stage cost and the frequency measure defined in Section \ref{sec:bounds_online}. Given a policy,  and the expected state frequency with respect to that policy, , the online performance is expressed as:
	
When the left hand side is integrated over the initial state distribution, , then  is chosen accordingly.


A final identity relates the initial state distribution to the expected state frequency. Given any  the following relation holds:
	
 This identity stems from \cite[eq. (6.3.10)]{hernandez_2012_discreteTimeMCP}.

\vspace{0.2cm}

We now have all the tools required to prove Theorem \ref{theorem:online_performance_bound_iterated_Qform}.


 





\vspace{0.2cm}
\begin{IEEEproof}[Proof of Theorem \ref{theorem:online_performance_bound_iterated_Qform}]
		
	For all ,
	
	for all  and , and hence also for all .
	
	
	Recalling the notation , we have,
	
	
	The first equality and first inequality hold by the point-wise ordering of \eqref{eq:online_performance_bound_iterated_Qform_01}. The second equality uses \eqref{eq:appendix_online_perf_measure_identity} for the first term and \eqref{eq:appendix_measures_identity} for the second term. The third equality uses \eqref{eq:appendix_operators_adjoint}, while the fourth uses \eqref{eq:lasserre_T_pi_operator_for_functions} to expand the  operator, and then the definition of the -operator and the chosen policy to construct the first term. The last inequality and equality follow from the point-wise ordering of \eqref{eq:online_performance_bound_iterated_Qform_01} and the definition of the 1-norm. The factor  was introduced so that the scaling in the 1-norm is a probability measure.
\end{IEEEproof}

\vspace{0.2cm}

 









\section{Proof of Infinity-norm bound} \label{app:proof_bound_inf_norm_Qform}

The proof of Theorem \ref{theorem:inf_norm_bound_iterated_Qform} uses two additional lemmas that are presented first, and then we present the proof of Theorem \ref{theorem:inf_norm_bound_iterated_Qform}.
Lemma \ref{Qform_constraint_violation_infinity_norm} provides a point-wise bound on how much the -iterated -operator inequality is violated for any given  function, from the restricted function space or otherwise. This is used in the proof of Lemma \ref{lemma:Q_feasible_after_downshift}, which shows that given a , it can be downshifted by a certain constant amount to satisfy the iterated -operator inequality. The constant by which it is downshifted relates directly to the constant on the right-hand-side of Theorem \ref{theorem:inf_norm_bound_iterated_Qform}.
The proof here is an adaptation to -functions of the proof for Value functions that is given in \cite[\S 4.3]{boyd_iteratedBellman}.


\vspace{0.1cm}

\begin{lemma} \label{Qform_constraint_violation_infinity_norm}
	For any  and  iterations,
	
	for all  and all .
\end{lemma}

\vspace{0.1cm}

\begin{IEEEproof}
	Starting from the terms not involving ,
	
	The first inequality follows from the definition of the -norm, and the second inequality comes from  and the -norm definition. Finally, the third inequality is due to the -contractive property of the -operator. Re-arranging, the result follows.
\end{IEEEproof}


\vspace{0.1cm}



\begin{lemma} \label{lemma:Q_feasible_after_downshift}
	Let  be an arbitrary element from the basis functions set, and let  be defined as,
	
	then  satisfies the iterated -operator inequality, and if  allows for affine combinations of the basis functions, then  is also an element of .
\end{lemma}

\vspace{0.2cm}

\begin{IEEEproof}
	Let  denote the constant \emph{downwards shift term} for notational convenience. Using the definition of the -operator we see that for any function ,
	
	where the equalities hold for all , . The first equality comes from the definition of the -operator, and the second equality holds as  is an additive constant in the objective of the minimization.
	
	Iterating the same argumentation -times leads to
	
	where the equivalences hold point-wise for all , . Now we show that  satisfies the iterated -operator inequality,
	
	where the first equality comes from \eqref{lemma:Q_feasible_after_downshift_eq2}, the inequality is a direct application of Lemma \ref{Qform_constraint_violation_infinity_norm} to the term  and holds for all , , and the final equality follows from \eqref{lemma:Q_feasible_after_downshift_eq1}.
	
	
	Finally, if  allows for affine combinations of the basis functions, then  implies  as the \emph{downward shift term} is an additive constant.
\end{IEEEproof}




\vspace{0.2cm}



\begin{IEEEproof}[Proof of of Theorem \ref{theorem:inf_norm_bound_iterated_Qform}]
	
	Given any , construct  following Lemma \ref{lemma:Q_feasible_after_downshift} to be feasible for the approximate iterated LP.
Working from the left hand side of equation \eqref{eq:theorem:inf_norm_bound_iterated_Qform_bound},
	
	where the first inequality holds by Lemma \ref{lemma:approxLP_for_Q_solves_min_1norm} because  is also feasible for \eqref{eq:LP_approach_to_ADP_iterated_Qform}, the second inequality by assuming without loss of generality that  is a probability measure, the third inequality is an application of the triangle inequality, the first equality stems directly from the definition of , and the final is an algebraic manipulation.
As this argumentation holds for any ,  the result follows.
\end{IEEEproof}



 







\section{Proof of Lyapunov-based bound} \label{app:proof_bound_lyap_Vform}



The proof of Theorem \ref{theorem:bound_lyapunov_iterated_Vform} uses four lemmas that are derived first, and then we present the proof of Theorem \ref{theorem:bound_lyapunov_iterated_Vform}.
Lemma \ref{lemma:T_iterated_V_diff_leq_H_iterated_V_diff} bounds the difference after applying  iterations of the Bellman operator to 2 different Value functions. The bound is given by  iterations of the  operator introduced in Section \ref{sec:bounds_fitting_lyap} and is used in Lemma \ref{lemma:T_iterated_V_geq_V_minusLyap} to give a bound on how much the -iterated Bellman inequality is violated for any given Value function. This constraint violation bound is given in terms of a Lyapunov function and is used in Lemma \ref{lemma:feasible_forIterated_V_by_shifting} to prove that given any , it can be downshifted by a scalar multiple of a Lyapunov function to satisfy the -iterated Bellman inequality. The Lyapunov function appearing in the downshift relates directly to the Lyapunov function and relevance weighting on the right-hand-side of the Theorem \ref{theorem:bound_lyapunov_iterated_Vform} bound.
The proof of Theorem \ref{theorem:bound_lyapunov_iterated_Vform} is reminiscent of that for \cite[Theorem 3]{vanRoy_linApproxDP}, but requires an adapted analysis for consideration of the iterated Bellman inequality and continuous spaces.


\vspace{0.2cm}


\begin{lemma} \label{lemma:T_iterated_V_diff_leq_H_iterated_V_diff}
	For any two functions ,
	
	for all , and any .
\end{lemma}

\vspace{0.2cm}

\begin{IEEEproof}
	The lemma will be proven by induction. For , we first show that the inequality hold without . Letting  denote the minimizer for  and  for ,

where the inequalities hold for all . The first equality is the definition of  in terms of , and the first inequality holds by definition of  being the minimizer for . The second inequality holds as the same  appears in both terms. The final inequality holds by definition of  and .
	
	
	An entirely analogous argument establishes that  is bounded above by the same final term in \eqref{eq:lemma:T_iterated_V_diff_leq_H_iterated_V_diff_eq01}. Hence the result for  follows as,
	
	where the inequalities hold for all . The first inequality follows from \eqref{eq:lemma:T_iterated_V_diff_leq_H_iterated_V_diff_eq01}. The second inequality uses \cite[Lemma 1.7.2]{christensen_2010_functions} to exchange the expectation and absolute value. The final equivalence is the definition of  as per Section \ref{sec:bounds_fitting_lyap}.

	Assume the statement holds true for some , i.e.,
	
	and show it therefore holds true for :
	
	where the inequalities hold for all . The first equivalence splits  so that the induction assumption can be used to establish the first inequality. The second inequality uses \eqref{eq:lemma:T_iterated_V_diff_leq_H_iterated_V_diff_eq02} and the monotonicity property of . The final equivalence follows by algebra.
	
	By induction the claim holds for any integer .
\end{IEEEproof}



\vspace{0.2cm}


\begin{lemma} \label{lemma:T_iterated_V_geq_V_minusLyap}
	For any positive function , any function , and any integer ,
	
	for all , where .
\end{lemma}

\vspace{0.2cm}

\begin{IEEEproof}
	First we find a relation between , , and  based on the weighted infinity norm.
	
where the inequalities hold for all . The first inequality comes from the definition of the weighted -norm. The first equality holds as  is a strictly positive function, and the final inequality stems from the definition of .
	
	Thus,
	 
	where the inequalities hold for all . The first inequality is a consequence of \eqref{eq:01_for_T_iterated_V_geq_V_minusLyap}. The second inequality uses the fact that  and the definition of . The third inequality is a direct application of Lemma \ref{lemma:T_iterated_V_diff_leq_H_iterated_V_diff}. The fourth inequality uses \eqref{eq:01_for_T_iterated_V_geq_V_minusLyap} and the monotonicity of operator . The two equalities follow from simple algebra. 
\end{IEEEproof}


\vspace{0.2cm}


\begin{lemma} \label{lemma:lyapunov_function_bound_V_HV}
	Given any Lyapunov function  (Definition \ref{def:lyap_func_Vform}), and its respective Lyapunov constant , then,
	
	for all .
\end{lemma}

\vspace{0.2cm}

\begin{IEEEproof}
By the definition of the Lyapunov function that  for all , thus we get that,
	
	where the inequality holds for all  by the monotone property of  for any . Iterating the same argumentation -times leads to,
	
for all . As  is strictly positive, this implies that,
	
for all . Manipulating the left-hand-side,
	
Hence the result follows.
\end{IEEEproof}


\vspace{0.2cm}


\begin{lemma} \label{lemma:feasible_forIterated_V_by_shifting}
	Let  be a Lyapunov function (Definition \ref{def:lyap_func_Vform}) and  arbitrary, and define  as,
	
	where , then  for all , i.e., it is feasible for the approximate iterated LP. Additionally, if  then  is an element of .
\end{lemma}

\vspace{0.2cm}

\begin{IEEEproof}
	Starting from the right-hand-side of the iterated Bellman inequality,

	
where the inequality holds for all . The first equality is simple algebra and the first inequality is from the definition of . The second inequality is a direct application of Lemma \ref{lemma:T_iterated_V_diff_leq_H_iterated_V_diff}. The second equality follows from the definition of  given in \eqref{lemma:feasible_forIterated_V_by_shifting_eq01}. The third inequality stems from applying Lemma \ref{lemma:T_iterated_V_geq_V_minusLyap} to the  term. The last equality again uses the definition of  and the last inequality follows from Lemma \ref{lemma:lyapunov_function_bound_V_HV}.
	
	By \eqref{lemma:feasible_forIterated_V_by_shifting_eq01},  is a linear combination of  and . As  and  are both elements of , so is .
\end{IEEEproof}



\vspace{0.2cm}


\begin{IEEEproof}[Proof of of Theorem \ref{theorem:bound_lyapunov_iterated_Vform}]

	Given any , construct  following Lemma \ref{lemma:feasible_forIterated_V_by_shifting} to be feasible for the approximate iterated LP. Working from the left hand side of the bound,

where the inequalities hold for all . The first inequality follows from Lemma \ref{lemma:approxLP_for_Q_solves_min_1norm} and Lemma \ref{lemma:feasible_forIterated_V_by_shifting}. The first equality is the definition of the weighted -norm and holds as  is strictly positive. The second inequality holds because the objective of the supremum is non-negative for all . The second equality is the definition of the weighted -norm and weighted -norm. The final inequality follows by the triangle inequality. The final equality stems from using \eqref{lemma:feasible_forIterated_V_by_shifting_eq01} by taking the weighted -norm of  and then some simple algebra.
As the inequality established holds for any , it also holds when the infimum over all  is taken on the right-hand-side.
	
\end{IEEEproof}
 









\section{Proofs of equivalent \textQ-function formulation} \label{app:Qform_equivalence}


\begin{IEEEproof}[Proof of of Lemma \ref{lemma:Qform_Vform_equivalence}]
	
	We shall show that any feasible solution of \eqref{eq:PropOfEquiv_forQ} corresponds to a feasible solution of \eqref{eq:PropOfEquiv_forV} with the same objective value, and vice versa.
Note that for the proof superscript  indicates a decision variable of problem \eqref{eq:PropOfEquiv_forV}.
	
	Suppose that  ,  is a feasible solution of \eqref{eq:PropOfEquiv_forQ}, and take the following decision variables for \eqref{eq:PropOfEquiv_forV},

We now check feasibility for the constraints of \eqref{eq:PropOfEquiv_forV}.

for all  and , thus \eqref{eq:PropOfEquiv_forV_01} is satisfied. We have that for ,
		
for all  and , thus \eqref{eq:PropOfEquiv_forV_02} are satisfied. Finally,

for all  and , thus \eqref{eq:PropOfEquiv_forV_03} is also satisfied, and the considered decision variables are feasible for problem \eqref{eq:PropOfEquiv_forV}. As , the objective values are equal. This completes the equivalence in one direction.
	
	Suppose that ,  is a feasible solution of \eqref{eq:PropOfEquiv_forV}, and take the following decision variables for \eqref{eq:PropOfEquiv_forQ},

where the choices of  are valid by the assumption.
We now check the feasibility for the constraints of \eqref{eq:PropOfEquiv_forQ}.

for all  and , and for   we have,

for all  and , thus \eqref{eq:PropOfEquiv_forQ_01} are satisfied. We have that for ,

for all  and , thus \eqref{eq:PropOfEquiv_forQ_02} are also satisfied. Finally,

for all  and , thus \eqref{eq:PropOfEquiv_forQ_03} is also satisfied, and the considered decision variables are feasible for problem \eqref{eq:PropOfEquiv_forQ}. As , the objective values are equal.
\end{IEEEproof}


 




\section{Computing , , , and Lyapunov Functions} \label{app:supplement_for_performance_bounds_numerical}

This appendix provides additional details for the numerical example of Section \ref{sec:numerical_1d}.


The value function was computed on the interval  at  evenly spaced discretization points.
The  and  are computed for   samples from . The expectation with respect to  is empirically evaluated using  extractions from the disturbance process, different for each , and each is simulated for  time steps.
The boundary of  was not reached by any sample.
Fig. \ref{fig:1d_example_Vfunctions} shows on the upper plot  (black dashed), the approximate value functions,  (blue), and the online performance,  (red). The lower plot depicts the initial state distribution  (green), and the discounted state occupancy measure  (purple) that arises from playing the approximate policy.
The  and  are shown only for  Bellman iterations because they are similar for all choices of .





\begin{figure}
\iftoggle{doublecolumn}{
		\begin{center}
\includegraphics[width = 0.40 \textwidth]{img/Sys001D_005_Vhat_and_mu_v02.pdf}
		\end{center}
		\vspace{0.20cm}
}{
		\begin{center}
\includegraphics[width = 7.1cm]{img/Sys001D_005_Vhat_and_mu_v02.pdf}
		\end{center}
		\vspace{-0.28cm}
		\hspace{3.57cm}
}
\begin{tikzpicture}[overlay]
\node[align=center , rotate=0] at (4.4cm,0.1cm) { State Space,  };
\node[align=center , rotate=0] at (2.60cm,0.60cm) {  };
		\node[align=center , rotate=0] at (4.48cm,0.60cm) {  };
		\node[align=center , rotate=0] at (6.24cm,0.60cm) {  };
\node[align=right , rotate=0] at (0.55cm,1.02cm) {  };
		\node[align=right , rotate=0] at (0.55cm,2.00cm) {  };
\node[align=right , rotate=0] at (0.60cm,3.10cm) {  };
		\node[align=right , rotate=0] at (0.60cm,4.50cm) {  };
		\node[align=right , rotate=0] at (0.60cm,5.90cm) {  };
\draw[gray70,line width = 0.5pt,fill=white] (3.3cm,5.6cm) rectangle (5.3cm,7.0cm);
\draw[black,line width = 1.2pt,dashed] (3.5cm,6.7cm) -- (4.1cm,6.7cm);
		\node[right] at (4.3cm,6.7cm) {{\small}};
\draw[myblue,line width = 1.2pt,solid] (3.5cm,6.3cm) -- (4.1cm,6.3cm);
		\node[right] at (4.3cm,6.3cm) {\color{myblue}{\small}};
\draw[myred ,line width = 1.2pt,solid] (3.5cm,5.9cm) -- (4.1cm,5.9cm);
		\node[right] at (4.3cm,5.9cm) {\color{myred}{\small}};
\draw[gray70,line width = 0.5pt,fill=white] (5.2cm,1.5cm) rectangle (7.7cm,2.1cm);
\draw[mygreen,line width = 0.8pt,fill=myltgreen] (5.4cm,1.7cm) rectangle (5.8cm,1.9cm);
		\node[right] at (5.9cm,1.8cm) {\color{mygreen}{\small}};
\draw[mypurple,line width = 0.8pt,fill=myltpurple] (6.6cm,1.7cm) rectangle (7.0cm,1.9cm);
		\node[right] at (7.1cm,1.8cm) {\color{mypurple}{\small}};
\node[align=center, myblue, rotate=00] at (4.40cm,2.96cm) {{\small}};
		\node[align=center, myblue, rotate=00] at (4.40cm,2.54cm) {{\small}};
		\node[align=center, myblue, rotate=65] at (7.45cm,6.60cm) {{\small}};
		\node[align=center, myblue, rotate=75] at (6.94cm,6.60cm) {{\small}};
		\node[align=center, myblue, rotate=57] at (7.50cm,5.50cm) {{\small LQR}};
	\end{tikzpicture}
	\caption[Value functions and occupancy measures]
	{
		Value functions and occupancy measures for the 1-dimensional example of Section \ref{sec:numerical_1d}. The approximate value functions  (blue) are labelled with the number of  bellman iterations used, and are point-wise under-estimators of  (dashed black). The online performance  (red) and discounted occupancy measure  (purple) are shown for the approximate policy arising from , for the  and LQR policies the  and  results are indistinguishable on the scale of this graph. The initial state distribution  (green) is shown for comparison, and  was used for computing . The dashed blue line labelled LQR is the approximate value functions that arises from using , i.e., unconstrained input, in the approximate LP.
	}
	\label{fig:1d_example_Vfunctions}
\end{figure}


Fig. \ref{fig:1d_example_Vfunctions} provides the visual insight necessary to explain the numerical trend observed in the data of Table \ref{tab:OnlineBounds_for1DExample} that for  the online performance of the greedy policy is slightly worse and the bound significantly more conservative.
It is clear from Fig. \ref{fig:1d_example_Vfunctions} that  with  gives a better point-wise lower-bound in the region near , compared to . As  is more concentrated near  than , the bound is tighter.
The difference in online performance is also explained by the difference of the approximate value functions in the region near . As the greedy policy is closely related to the gradient of the value function, in regions where the gradient of a  closely approximates that of , the approximate greedy policy will generate near-optimal control actions.
In Fig. \ref{fig:1d_example_Vfunctions} it is clear that in the region near the origin  matches the gradient of  much better than .
Due to the input constraints of this problem, outside of that region all value functions that rise steeply enough lead to the same performance because the input saturates at .




To explain the computation of Lyapunov functions, first recall that the restricted function space used for the one dimensional example is the space of univariate quadratics, with  as the quadratic coefficient,  as the constant offset, and the linear term omitted.
From the definition of  and the  operator, it is clear that if a function  is a Lyapunov function then , with , is also a Lyapunov function. Moreover, the right-hand-side of Theorem \ref{theorem:bound_lyapunov_iterated_Vform} is unchanged by this positive scaling. Thus, without loss of generality we fix  and parametrize candidate Lyapunov functions by the quadratic co-efficient.


To compute the set of Lyapunov functions and their corresponding  value, we take a brute force approach. As discussed in Section \ref{sec:bounds_fitting_lyap}, a constant function, i.e., , is a Lyapunov function with . For this system, with stable linear dynamics,  increases with . To find the set of Lyapunov functions, we increase  in small increments, and compute the value of  by discretizing the state space on a sufficiently large interval. The relationship of  versus  is shown in Fig. \ref{fig:appendix:lyap_beta_vs_P}.


All Lyapunov functions yield a valid bound, and the Lyapunov with the tightest bound changes based on the number of Bellman inequality iterations . To provide some insight, Fig. \ref{fig:appendix:lyap_RHS_vs_beta} shows the right-hand-side of Theorem \ref{theorem:bound_lyapunov_iterated_Vform} for the choice  versus  for this example.


\begin{figure}
\iftoggle{doublecolumn}{
		\begin{center}
\includegraphics[width = 6.0cm]{img/lyap_beta_vs_P.pdf}
		\end{center}
		\vspace{0.10cm}
}{
		\begin{center}
\includegraphics[width = 6.1cm]{img/lyap_beta_vs_P.pdf}
		\end{center}
		\vspace{-0.35cm}
		\hspace{3.47cm}
}
\begin{tikzpicture}[overlay]
\node[align=center , anchor=south  , rotate=0] at (4.3cm,-0.15cm) {\small{Quadratic coefficient, }};
	\node[align=center , anchor=center , rotate=90] at (0.3cm,1.9cm) {\small{  }};
\node[align=center , anchor=north , rotate=0] at (1.55cm,0.75cm) {\small{  }};
	\node[align=center , anchor=north , rotate=0] at (4.4cm,0.75cm) {\small{  }};
	\node[align=center , anchor=north , rotate=0] at (7.3cm,0.75cm) {\small{  }};
\node[align=right , anchor=east , rotate=0] at (1.60cm,0.86cm) {\small{  }};
\node[align=right , anchor=east , rotate=0] at (1.60cm,3.08cm) {\small{  }};
	\end{tikzpicture}
	\caption[Lyapunov function  versus ]
	{
		Set of Lyapunov functions, parametrized by the quadratic coefficient, for the one dimensional example of Section \ref{sec:numerical_1d} and the corresponding .
	}
	\label{fig:appendix:lyap_beta_vs_P}
\end{figure}





\begin{figure}
\iftoggle{doublecolumn}{
		\begin{center}
\includegraphics[width = 6.0cm]{img/lyap_RHS_vs_beta.pdf}
		\end{center}
		\vspace{0.10cm}
}{
		\begin{center}
\includegraphics[width = 6.00cm]{img/lyap_RHS_vs_beta.pdf}
		\end{center}
		\vspace{-0.38cm}
		\hspace{3.53cm}
}
\begin{tikzpicture}[overlay]
\node[align=center , anchor=south  , rotate=0] at (4.3cm,-0.15cm) {\small{  }};
	\node[align=center , anchor=center , rotate=90] at (0.3cm,2.4cm) {\small{RHS of Theorem \ref{theorem:bound_lyapunov_iterated_Vform}}};
\node[align=center , anchor=north , rotate=0] at (1.55cm,0.75cm) {\small{  }};
	\node[align=center , anchor=north , rotate=0] at (4.44cm,0.75cm) {\small{  }};
	\node[align=center , anchor=north , rotate=0] at (7.40cm,0.75cm) {\small{  }};
\node[align=right , anchor=east , rotate=0] at (1.60cm,0.86cm) {\small{  }};
	\node[align=right , anchor=east , rotate=0] at (1.60cm,2.34cm) {\small{  }};
	\node[align=right , anchor=east , rotate=0] at (1.60cm,3.84cm) {\small{  }};
\node[align=left , anchor=west , rotate=90] at (4.17cm,1.70cm) {\small{}};
	\node[align=left , anchor=west , rotate=90] at (5.51cm,1.70cm) {\small{}};
	\node[align=left , anchor=west , rotate=90] at (6.09cm,1.70cm) {\small{}};
	\node[align=left , anchor=west , rotate=90] at (6.62cm,1.70cm) {\small{}};
\draw[matlabred    ,line width = 0.5pt] (4.175,2.85) -- (4.175,3.20);
	\draw[matlabblue   ,line width = 0.5pt] (5.515,1.80) -- (5.515,1.36);
	\draw[matlabpurple ,line width = 0.5pt] (6.090,1.80) -- (6.090,1.24);
	\draw[matlaborange ,line width = 0.5pt] (6.620,1.80) -- (6.620,1.17);
	\end{tikzpicture}
	\caption[Lyapunov function  versus ]{
		Right-hand-side of Theorem \ref{theorem:bound_lyapunov_iterated_Vform} for the one dimensional example of Section \ref{sec:numerical_1d}, evaluated for the choice . The circles mark the minimum for each curve, showing that for each , a different Lyapunov function achieves the tightest bound.
	}
	\label{fig:appendix:lyap_RHS_vs_beta}
\end{figure} 




\section{Reformulation of Bellman Inequality} \label{app:Sprocedure_reformulation}

This appendix provides a sufficient reformulation of the Bellman Inequality that is used in the numerical examples for solving programs \eqref{eq:LP_approach_to_ADP_iterated_Vform} and \eqref{eq:PropOfEquiv_forQ} to find an approximate value function and \textQ-function respectively.
See Section \ref{sec:numerical_nd} for the definitions of , , and  as the linear dynamics, and \eqref{eq:quadratic_basis_function_space} for the specification of the quadratic basis functions.
We introduce , , to denote the lower and upper bounds that describe each coordinate of the  space.
To concisely represent the quadratic stage cost we introduce the matrix  that takes the the form .
The notation  places the vector argument on the diagonal of an otherwise zero matrix, and  is the standard basis column vector with  in the  element and zeros elsewhere, with the dimension clear from context.


Using this notation, each inequality of the form  for all ,  is sufficiently reformulated as the following LMI:
	
where  indicates that the matrix is symmetric, and the , , are the auxiliary variables introduced when using the S-procedure to reformulate the for all  part of the constraint.



The objective function of programs \eqref{eq:LP_approach_to_ADP_iterated_Vform} and \eqref{eq:PropOfEquiv_forQ} is linear in the decision variables and evaluation of the objective requires the first and second moments of the relevance weighting parameter. For the \textQ-function formulation, the objective is:
	
where  and  are the first and second moments of the measure , and  denotes the trace of a square matrix. 








\bibliographystyle{IEEEtran}
\bibliography{bibliography.bib}



\end{document}
