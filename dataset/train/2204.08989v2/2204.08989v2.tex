\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{mathtools}
\usepackage{glossaries}
\usepackage{booktabs}

\begin{document}
\title{Efficient Deep Learning-based Estimation of the Vital Signs on Smartphones}

\author{

\IEEEauthorblockN{\textsuperscript{*} Taha Samavati\textsuperscript{†}, 
\textsuperscript{*} Mahdi Farvardin \textsuperscript{†},
Aboozar Ghaffari\textsuperscript{+}}


\IEEEauthorblockA{\textsuperscript{†}\textit{ Department of Computer Engineering, Iran University of Science and Technology, Tehran, Iran}\\
\textsuperscript{+}\textit{ Department of Electrical Engineering, Iran University of Science and Technology, Tehran, Iran}\\
tahasamavati12@gmail.com\\
mahdi.farvardin@gmail.com\\
aboozar\_ghaffari@iust.ac.ir}
\thanks{* Equal Contribution}
}

\maketitle







\begin{abstract}

Nowadays, due to the widespread use of smartphones in everyday life and the improvement of computational capabilities of these devices, many complex tasks can now be deployed on them. Concerning the need for continuous monitoring of vital signs, especially for the elderly or those with certain types of diseases, the development of algorithms that can estimate vital signs using smartphones has attracted researchers worldwide. Such algorithms estimate vital signs (heart rate and oxygen saturation level) by processing an input PPG signal. These methods often apply multiple pre-processing steps to the input signal before the prediction step. This can increase the computational complexity of these methods, meaning only a limited number of mobile devices can run them. Furthermore, multiple pre-processing steps also require the design of a couple of hand-crafted stages to obtain an optimal result. This research proposes a novel end-to-end solution to mobile-based vital sign estimation by deep learning. The proposed method does not require any pre-processing. Due to the use of fully convolutional architecture, the parameter count of our proposed model is, *on average, a quarter of the ordinary architectures that use fully-connected layers as the prediction heads. As a result, the proposed model has less over-fitting chance and computational complexity. A public dataset for vital sign estimation, including 62 videos collected from 35 men and 27 women, is also provided. The experimental results demonstrate state-of-the-art estimation accuracy.

\end{abstract}

\begin{IEEEkeywords}
Deep Learning, Artificial Intelligence, Computer Vision, Heart-rate Estimation, SpO2 Estimation, Vital Signs Estimation, Non-Contact Estimation
\end{IEEEkeywords}


\section{Introduction}
Vital signs need to be monitored regularly, especially in the elderly or individuals with certain medical disorders. Nowadays, anyone has a smartphone and uses it for a variety of tasks on a daily basis. Due to the rapid development of both hardware and software, smartphones can perform more complex tasks. Having mentioned these, one can take advantage of smartphones for estimating and monitoring vital signs with near-clinical accuracy. Heart rate (HR) and Oxygen saturation level (SpO2) are two of the most significant vital signs in the human body. Heart Rate is an important indicator of people's physiological state and needs to be measured in many circumstances, especially for healthcare and medical purposes. The oxygen saturation level in our bloodstream, also known as SpO2, shows how much oxygen is carried by the blood. Measuring SpO2 levels is essential as a major deviation from normal levels (typically more than 95\% at sea level) \cite{b1} indicates a dangerous health condition or a sign of a serious disease (including COPD, Asthma, interstitial lung diseases, sequelae of tuberculosis, lung cancer, chronic obstructive pulmonary disease, and COVID-19) \cite{b2},\cite{b3} can cause significant drops in SpO2 levels.

To predict the vital signs using optical measurement methods, one should first obtain the Photoplethysmography signal. Photoplethysmography (PPG) is an uncomplicated and inexpensive optical measurement method that is often used for heart rate monitoring purposes. PPG is a non-invasive technology that uses a light source and a photodetector at the surface of the skin to measure the volumetric variations of blood circulation \cite{b4}. The PPG signal can also be obtained by smartphones with both a camera and a flashlight. By covering the camera with a finger while its flashlight is on, one can obtain photo readings and further process them to obtain the PPG signal. Various methods have been proposed for heart rate and SpO2 estimation. Some of these methods only rely on signal processing algorithms to extract the vital signs from the input PPG signal \cite{b5}, \cite{b6}, \cite{b7}. while some use both signal processing and deep learning methods \cite{b8,b9}. The rest focus on developing end-to-end deep learning methods \cite{b10}, \cite{b11}, \cite{b12}. These mentioned methods either need multiple pre-processing stages or have a high computational burden to be run on mid-range or low-end smartphones. 

In this article, a set of architectures for real-time heart rate and SpO2 estimation on mobile devices are proposed. These methods estimate heart rate and SpO2 in an end-to-end manner without any pre-processing steps. This makes it easy to deploy the models on mobile devices. Compared to prior proposed architectures that used fully connected layers following the feature extraction stage, fully convolutional architectures do not use dense connections after extracting features; hence they have fewer parameters while achieving better accuracy. As another contribution of this research, a public dataset of smartphone videos named MTHS is provided, containing extracted PPG signals from 62 distinct patients with their corresponding ground truth HRs and SpO2s. More detailed information about the dataset can be found in Section \ref{datasets}. 

\section{Related Works}
\textbf{Heat rate Estimation:}
Research \cite{b8} utilizes a convolutional-based neural network to estimate the heart rate from PPG signals acquired from smartphone captured videos. The input videos are first converted to a set of 3 channel 1D signals. These signals are the mean values of the frames' red, green, and blue channels. Multiple pre-processing algorithms were applied to the signals before feeding them to the network. These pre-processing steps include multiple handcrafted steps such as denoising, applying moving average, and PCA. Another drawback of this method is the use of fully connected layers following the convolutional layers, which increases the network parameters and increases the over-fitting chance compared to the newer fully convolutional architectures.

Research \cite{b9} tends to estimate heart rate by taking FFT of single-channel time-series PPG along with 3-axis accelerometer motion signals and feeds the resulting four-channel signals to the proposed neural network. The input signal is clipped between 0-4 Hz to remove unwanted frequencies. The proposed model consists of  8 channel 1D convolution-max pool layers followed by a fully connected network. This model can be further improved by using fully convolutional architecture. The authors also claim that utilizing all the PPG channels can improve the estimation accuracy, which is left for future work.

Research \cite{b10} proposes an end-to-end deep learning model to estimate heart rate from PPG signals acquired by a wrist-worn device. The proposed method does not require any pre-processing steps or any motion data. Nevertheless, the proposed method achieves competitive results compared to those using motion signals. In this work, eight consecutive one-second PPG data with a sample rate of 125 Hz are fed to a set of convolutional layers and an LSTM layer in parallel. The produced feature vectors are then concatenated together and fed to another LSTM layer. Finally, a linear layer predicts the heart rate. Aside from the advantage of eliminating the need for pre-processing steps, due to the presence of LSTM layers, the model still has a relatively high computational complexity.

\textbf{SpO2 Esitmation:}
Research \cite{b13} estimates spo2 from smartphone videos captured from finger-tips. After the pre-processing step, which includes motion removal, a convolutional neural network is proposed for SpO2 estimation, which has only two 1D convolutional layers with a large filter length followed by max pool and dropout. A pulse oximeter records ground truth heart rate and SpO2, while an iPhone 7 plus is used to record fingertip videos. The researchers conclude that the performance of their proposed method does not improve when increasing the frame rate above 30fps. They also suggest using Raw PPG instead of pre-processed (band-pass filtered) as it achieves the best performance.

Research \cite{b14}, proposes a convolutional neural network followed by a fully connected layer. The model takes in the mean channel values of gain applied RGB channels and predicts the SpO2 values. The captured video, from which the mean signals are extracted, has a time length of three seconds with a sampling rate of 30 fps. However, the collected dataset has limited coverage of patients. The proposed model uses fully connected layers after the CNN layers, resulting in higher parameter count and over-fitting chance compared to the fully convolutional architectures.

\cite{b16} extracts seven features from the PPG signal using two python kits, namely HeartPy and Neurokit. The researchers use machine learning methods to estimate SpO2 levels. The best performing method was the Random Forest regressor, which has achieved an MAE of 1.45 on BIDMC's test set. Although it is not stated as an advantage in the original work, the random forest regressor can also be deployed on mobile devices after performing proper conversion. How ever, in this method multiple pre-processing methods were applied to the input signal which can be listed as a draw back for this work. Furthermore, the machine learning are outdated and being surpassed by deep learning models.

\section{Proposed Methods}
We propose a highly efficient real-time algorithm to estimate heart rate and SpO2 on smartphones and mobile devices. The proposed deep learning method has fewer parameters than the previously proposed architectures for vital sign estimation. It also does not require performing any pre-processing on the input PPG signal.

Given an image sequence or a video taken from fingertips, MTVital estimates both HR and SpO2 in real-time. Our proposed network estimates vital signs in a fully convolutional manner. This results in a network with 4x fewer parameters than the conventional methods, which use a fully connected network after the convolutional layers. Moreover, having fewer parameters decreases the chance of over-fitting, which can be beneficial when having a small-sized dataset. The commonly used batch normalization layers are not used to further decrease the computational complexity. The proposed architecture eliminates the need for implementing handcrafted stages such as designing and applying bandpass filters. We propose three types of architectures shown in Fig \ref{fig:model_archs}. The first architecture is a stack of 1D convolutional layers followed by a global average pooling (GAP) layer that produces the vital sign of interest (Heart rate or SpO2). The second one, Residual FCN, is deeper and adds residual connections to the normal convolutional layers. The last model, named DCT, first applies a Discrete Cosine Transformation (DCT) to the input PPG signal and removes the unwanted coefficients corresponding to the un-related frequencies. Afterward, a series of convolutional layers learn the mapping to the desired vital sign. The transformation and coefficient filtering procedures are implemented in the model itself and can be directly deployed to other platforms without the need for re-implementation of these steps in other platform-specific programming languages. In order to compare the performance of these methods against the conventional architectures, which use fully connected layers as the prediction heads, a base model is also implemented. The base model has a series of 1D convolutional layers followed by batch normalization as well as a fully connected network after the feature extraction phase. The model details such as parameter counts are listed in Table \ref{table:model-details}. The size of trained models is less than a MB on disk. Except for the Residual FCN model, all other models require less than one Mega FLOPs. The experiments clearly show that the best-performing architecture is the fully convolutional one. More detailed results are discussed in section \ref{section_exp_res}.

\begin{table}[h]
\begin{center}
\caption{Details of the proposed networks.}
\label{table:model-details}
\begin{tabular}{llll}
\toprule
Model Name & Parameters  & FLOPs  & Size \\ \hline
Base                    & 29665 & 0.141 M  & 450 Kb     \\
FCN                   & 8037 & 0.485 M  & 170 Kb     \\
Residual FCN             & 53713 & 7.17 M & 800 Kb     \\
DCT                  & 3266 & 0.902 M & 150 Kb \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=\textwidth]{VITAL.png}
    \caption{The proposed architectures for vital sign estimation, there are four different architectures provided. The Base model represents the commonly used architecture for this task, which is used as a baseline. The guide section provides detailed information about the blocks in the proposed architectures.}
    \label{fig:model_archs}
\end{figure*}

At inference time, the network estimates the vital signs from the fingertips. Therefore it first needs an image sequence or video; we obtain these by asking the patient to cover the smartphone's camera with his/her fingertip while the flashlight is on. Depending on the task, a single channel (the Red) is used for heart rate estimation, and all RGB channels are used as input for SpO2 estimation. The image sequence is captured and processed simultaneously so that for each image in the input sequence, mean values for RGB channels are computed and listed in three arrays separately. After 10 seconds, the resulting mean RGB signals are extracted to be fed into MTVital for estimation of vital signs. As depicted in Figure \ref{fig:model_archs}, the input PPG signal is first transformed to have zero mean and unit standard deviation. The model performs the latter process.

\section{Datasets}\label{datasets}
This section briefly discusses the datasets that are used to benchmark our proposed methods. In the following, the details of these datasets are explained.

\textbf{BIDMC\cite{b15}: }This dataset contains PPG, impedance respiratory signal, and electrocardiogram (ECG) for 53 patients. Each recording is eight minutes long and also includes ground truth data for heart rate (HR), respiratory rate (RR), and blood oxygen saturation level (SpO2) sampled at 1Hz. It should be noted that, despite that the PPG signal was not acquired by a smartphone camera, our model can be trained on any type of input PPG signal.

\textbf{MTHS:} This research proposes a dataset from 62 patients (35 men and 27 women) that contains both hr and SpO2 labels sampled at 1Hz. The PPG signal is acquired at 30 FPS using a smartphone's camera. The data collection procedure is explained in Section \ref{data_collection_sec}.

\section{Data Collection} \label{data_collection_sec}
Since this use case involves using a smartphone to estimate the vital signs, the PPG is acquired using the phone's RGB camera. As no dataset was found publicly available with a PPG signal acquired using a smartphone, we decided to collect such a dataset named MTHS. As described in the previous section, the MTHS dataset contains 30Hz PPG signals obtained from 62 patients, including 35 men and 27 women. The ground truth data includes heart rate and oxygen saturation levels sampled at 1Hz. The HR and SPo2 measurement is obtained using a pulse oximeter (M70). An iPhone 5s was used to obtain the PPG recordings at 30 fps. The flashlight is kept on during the recording phase. The patients were asked to fully cover the camera and the flashlight with their fingertips. Figure \ref{fig:setup-pics} shows the data collection procedure. The dataset is openly available on Github\footnote{https://github.com/MahdiFarvardin/MTVital}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth/2]{datacollection-setup.png}
    \caption{The setup for data collection.}
    \label{fig:setup-pics}
\end{figure}

Figures \ref{fig:BIDMC-hr-stats} and \ref{fig:BIDMC-spo2-stats} provide a statistical insight over the collected dataset.
\begin{figure}[htp]
    \centering
    \includegraphics[width=\textwidth/2]{hr_stats.png}
    \caption{Heart rate distributation for the MTHS dataset.}
    \label{fig:BIDMC-hr-stats}
\end{figure}

\begin{figure}[htp]
    \centering
    \includegraphics[width=\textwidth/2]{spo2_stats.png}
    \caption{SpO2 distribution for the MTHS dataset.}
    \label{fig:BIDMC-spo2-stats}
\end{figure}


\section{Experimental Results}
\label{section_exp_res}
In this section, The proposed methods are evaluated on a number of benchmark datasets, and the results are included. Furthermore, some visualizations of predictions and estimated values are provided.

The proposed architectures, namely BASE, FCN, Residual-FCN, and DCT, are experimented with different loss functions. These loss functions include Mean Absolute Error (MAE), Mean Squared Error (MSE), huber, and log-cosh. The mentioned networks are trained for 125 epochs on two datasets, including BIDMC \cite{b15} and our proposed dataset called MTHS. The base model is based on common and older architectures in regression which use a CNN feature extractor followed by a feed-forward network. The FCN model has a fully convolutional architecture with about 4x lower parameters than the base model. This architecture makes over-fitting less likely to happen and highly efficient for deployment on mobile devices. As explained before, the DCT model has the same fully-convolutional architecture but works on the frequency domain. After taking the DCT of the input signal and filtering out DC coefficients and high-frequency coefficients that do not contain our desired frequencies for HR, a stack of 1D convolutions is applied to regress HR and SpO2 values. In the following, the performance of the proposed methods on each dataset is reported.

\subsection{Experimental Results on BIDMC}
The BIDMC dataset is split into three sets of train, validation, and test set with slice size of 0.8, 0.04, and 0.16, respectively, seeded with the value of 1400. Below is the test MAE obtained by each of our models trained with different loss functions. According to the results in Table \ref{table:BIDMC-MAE-hr}, our proposed fully convolutional architecture with residual connections outperforms other architectures achieving an MAE of 1.36 on the BIDMC heart rate test set. Our experiments show that the MAE and log-cosh losses are good choices. During the experiments on this dataset, it was found that the proposed FCN-DCT model performs better when removing the inverse DCT transformation.


\begin{table}[h]
\caption{Heart rate estimation results on the BIDMC test set (MAE). The results are reported for all proposed architectures with different loss functions.}
\label{table:BIDMC-MAE-hr}
\begin{tabular}{lllll}
\toprule
Model Name / Loss Function & MSE  & MAE  & Huber & Log-cosh \\ \hline
Base                     & 2.33 & 2.77  & 2.20  & 2.85     \\
FCN                      & 1.91 & 1.86  & 1.94  & 2.01     \\
Residual FCN            & 1.38 & 1.44 & 1.43 & \textbf{1.36}     \\
DCT (No inverse)        & 3.08 & 2.02 & 2.24  & 2.21    \\
\bottomrule
\end{tabular}
\end{table}

The SpO2 estimation results on MTHS are shown in table \ref{table:BIDMC-MAE-spo2}. These results demonstrate that the best performing architecture for SpO2 estimation is Residual FCN, supervised with Log-cosh loss. This proves the effectiveness of incorporating residual blocks in the model architecture, providing a more robust and accurate feature extraction and representation. Regarding the experimental results, it is observed that a single loss function that works best for a specific architecture does not necessarily work best on other architectures. 

\begin{table}[h]
\caption{Spo2 estimation results on BIDMC test set (MAE). The results are reported for all proposed architectures with different loss functions.}
\label{table:BIDMC-MAE-spo2}
\begin{tabular}{lllll}
\toprule
Model Name / Loss Function & MSE  & MAE  & Huber & Log-cosh \\ \hline
Base                     & 2.25 & 1.94  & 2.04  & 1.97     \\
FCN                      & 2.08 & 1.85  & 1.93  & 1.98     \\
Residual FCN            & 1.20 & 1.02 & 1.07 & \textbf{1.00}     \\
DCT (No inverse)        & 2.25 & 2.12 & 2.16  & 2.08    \\
\bottomrule
\end{tabular}
\end{table}

Figures \ref{fig:BIDMC-pred-hr} and \ref{fig:BIDMC-pred-spo2}, provide two plots for both HR and SpO2 predictions on BIDMC test set with corresponding ground truth. Each plot contains 60 samples.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth/2]{hr-bidmc.png}
    \includegraphics[width=\textwidth/2]{hr-bidmc1.png}
    \caption{A plot of ground truth heart rate and its estimation by the best performing model (Residual FCN) on the BIDMC test set.}
    \label{fig:BIDMC-pred-hr}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth/2]{bidmc-spo2_1.png}
    \includegraphics[width=\textwidth/2]{bidmc_spo2_2.png}
    \caption{A plot of ground truth SpO2 and its estimation by the best performing model (Residual FCN) on the BIDMC test set.}
    \label{fig:BIDMC-pred-spo2}
\end{figure}

\subsection{Experimental Results on MTHS}
MTHS is split into three sets of train, validation, and test set with slice size of 0.68, 0.12, and 0.2, respectively, seeded with the value of 1400. Below is the test MAE obtained by each of our models trained with different loss functions. According to the results in Table \ref{table:MTHS-MAE-hr}, same as before, our proposed fully convolutional architecture with residual connections outperforms other architectures achieving an MAE of 6.59 on the testing subset of HR data. Our experiments show that the MAE and log-cosh losses are good choices.


\begin{table}[htbp]
\caption{Heart rate estimation results on MTHS test set (MAE). The results are reported for all proposed architectures with different loss functions.}
\label{table:MTHS-MAE-hr}
\begin{tabular}{lllll}
\toprule
Model Name / Loss Function & MSE  & MAE  & Huber & Log-cosh \\ \hline
Base                     & 10.92 & 10.75  & 9.95  & 10.62     \\
FCN                      & 8.98 & 8.42  & 8.58  & 8.35     \\
Residual FCN             & 8.33 & \textbf{6.59} & 6.92  & 6.97     \\
DCT                     & 10.09 & 9.02 & 9.80  & 8.67    \\
\bottomrule
\end{tabular}
\end{table}

The SpO2 estimation results are provided in the table \ref{table:MTHS-MAE-spo2}. The best performing model (Residual FCN) achieves the MAE of 1.24 on the MTHS test set. It is worth to note that the DCT model having only one tenth of the base model's parametes, achieves better results . However one might consider the extra FLOPs requierd by the DCT model.

\begin{table}[htbp]
\caption{SpO2 estimation results on MTHS test set (MAE). The results are reported for all proposed architectures with different loss functions.}
\label{table:MTHS-MAE-spo2}
\begin{tabular}{lllll}
\toprule
Model Name / Loss Function & MSE  & MAE  & Huber & Log-cosh \\ \hline
Base                    & 1.63 & 1.43  & 1.50  & 1.54     \\
FCN                      & 1.72 & 1.64  & 1.40  & 1.46     \\
Residual FCN             & 1.36 & \textbf{1.24} & 1.34  & 1.32     \\
DCT                      & 1.47 & 1.36 & 1.41  & 1.41   \\
\bottomrule
\end{tabular}
\end{table}

In figures \ref{fig:MTHS-pred-hr} and \ref{fig:MTHS-pred-spo2}, sixty prediction samples are provided for both HR and SpO2 estimation along with their corresponding ground truths.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth/2]{hr_mths_plot.png}
    \caption{A plot of ground truth heart rate and its estimation by the best performing model (Residual FCN) on the MTHS test set (MAE).}
    \label{fig:MTHS-pred-hr}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth/2]{spo2_mths_plot.png}
    \caption{A plot of ground truth spo2 and its estimation by the best performing model (Residual FCN) on the MTHS test set (MAE).}
    \label{fig:MTHS-pred-spo2}
\end{figure}

\subsection{Comparison with other methods on the BIDMC dataset}
In the following, the performance of the proposed models is compared against other methods. During the research process, no research was found reporting the HR results on the BIDMC test set. Most existing research has focused on predicting the Respiratory Rate (RR), which we have not evaluated our models on. Table \ref{table:BIDMC_comparison_spo2} compares the performance of the proposed models when at the task of SpO2 estimation with other state existing models on the the BIDMC test set.

\begin{table}[htbp]
\centering
\caption{Performance comparison of SpO2 estimation on BIDMC's test set.}
\label{table:BIDMC_comparison_spo2}
\begin{tabular}{lllll}
\toprule
Model Name / Metric & MAE \\ \hline
\cite{b16}               & 1.45     \\
Base                     & 1.94     \\
FCN                      & 1.85     \\
Residual FCN            & \textbf{1.00}     \\
DCT (No inverse)        & 2.08    \\
\bottomrule
\end{tabular}
\end{table}

\section{Algorithm Deployment on Smart Phone}
Figure \ref{fig:application-screenshot} shows some screenshots of the developed application. The application is developed for the android platform and is compatible with version 5 (Lollipop) and above. The deep learning models for heart rate and SpO2 estimation have been converted to TFLite \cite{b17} and integrated into the application. The user is requested to cover the camera while the flashlight is on at the running time. After the finger is detected, the estimation process starts. The application captures video frames at 30 fps. Every 10 seconds, a mean signal of red, green, and blue channels is computed and given to the deep learning model to infer the vital signs.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth/2]{screenshots.png}
    \caption{Screenshots of the developed application showing the measurement and profile sections. MTVital is deployed on the android application for vital sign estimation. The predicted vital signs are displayed to the user and updated every 10 seconds.}
    \label{fig:application-screenshot}
\end{figure}

\section{Conclusion and Future Work}
Due to the necessity of regular monitoring of vital signs, especially in the elderly, one can take advantage of smartphones for estimating and monitoring vital signs with acceptable accuracy. This research proposed an end-to-end, real-time, deep learning model for smartphone-based vital sign estimation. Our method has fewer parameters than previous methods and does not require any pre-processing steps. The proposed model can be deployed even on low-end devices due to the low computational complexity. There is also a public dataset of smartphone videos named MTHS provided which contains extracted PPG signals from 62 distinct patients with their corresponding ground truth HRs and SpO2s. A series of experiments were performed on unsupervised learning methods during this research, but the performance was not satisfying. Even though fully convolutional structures can have lower parameters and better performance than the ones with fully connected prediction heads, the FLOPs required by FCN models are higher. Given the lack of good quality and quantity data for smartphone-based vital sign estimation, the authors believe there is a strong potential for unsupervised learning methods that can be used in this field. Another possible future work is to collect SpO2 and HR data from people with respiratory-related diseases; Since our dataset only contains the data of healthy subjects.

\section*{Acknowledgment}
Part of this research was supported by Iran's National Elites Foundation. Special Thanks to Vahid Bastani, Mahdis Habibpour, Ali Farvardin, and Changiz Javadian for their kind support and help during this project.

\begin{thebibliography}{00}

\bibitem{b1} American Thoracic Society, \& American College of Chest Physicians. (2003). American Thoracic Society/American College of Chest Physicians statement on cardiopulmonary exercise testing. Am J Respir Crit Care Med, 167, 211-277.

\bibitem{b2} Vold, M. L., Aasebø, U., Wilsgaard, T., \& Melbye, H. (2015). Low oxygen saturation and mortality in an adult cohort: the Tromsø study. BMC pulmonary medicine, 15(1), 1-12.

\bibitem{b3} Goyal, D. K., Mansab, F., \& Bhatti, S. (2021). Room to breathe: the impact of oxygen rationing on health outcomes in SARS-CoV2. Frontiers in Medicine, 967.

\bibitem{b4} Castaneda, D., Esparza, A., Ghamari, M., Soltanpur, C., \& Nazeran, H. (2018). A review on wearable photoplethysmography sensors and their potential future applications in health care. International journal of biosensors \& bioelectronics, 4(4), 195–202. https://doi.org/10.15406/ijbsbe.2018.04.00125

\bibitem{b5} Siddiqui, S. A., Zhang, Y., Feng, Z., \& Kos, A. (2016). A pulse rate estimation algorithm using PPG and smartphone camera. Journal of medical systems, 40(5), 1-6.

\bibitem{b6} Ding, X., Nassehi, D., \& Larson, E. C. (2018). Measuring oxygen saturation with smartphone cameras using convolutional neural networks. IEEE journal of biomedical and health informatics, 23(6), 2603-2610.

\bibitem{b7} Nemcova, A., Jordanova, I., Varecka, M., Smisek, R., Marsanova, L., Smital, L., \& Vitek, M. (2020). Monitoring of heart rate, blood oxygen saturation, and blood pressure using a smartphone. Biomedical Signal Processing and Control, 59, 101928.

\bibitem{b8}A. H. Ayesha, D. Qiao and F. Zulkernine, "Heart Rate Monitoring Using PPG With Smartphone Camera," 2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM), 2021, pp. 2985-2991, doi: 10.1109/BIBM52615.2021.9669735.

\bibitem{b9} Reiss A, Indlekofer I, Schmidt P, Van Laerhoven K. Deep PPG: Large-Scale Heart Rate Estimation with Convolutional Neural Networks. Sensors (Basel). 2019 Jul 12;19(14):3079. doi: 10.3390/s19143079. PMID: 31336894; PMCID: PMC6679242.

\bibitem{b10} Shyam, A., Ravichandran, V., Preejith, S. P., Joseph, J., \& Sivaprakasam, M. (2019, July). PPGnet: Deep network for device independent heart rate estimation from photoplethysmogram. In 2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC) (pp. 1899-1902). IEEE.

\bibitem{b11} Kang, J., Yang, S., \& Zhang, W. (2022). TransPPG: Two-stream Transformer for Remote Heart Rate Estimate. arXiv preprint arXiv:2201.10873.

\bibitem{b12} Yang, Z., Wang, H., \& Lu, F. (2021). Assessment of Deep Learning-based Heart Rate Estimation using Remote Photoplethysmography under Different Illuminations. arXiv preprint arXiv:2107.13193.

\bibitem{b13} X. Ding, D. Nassehi and E. C. Larson, "Measuring Oxygen Saturation With Smartphone Cameras Using Convolutional Neural Networks," in IEEE Journal of Biomedical and Health Informatics, vol. 23, no. 6, pp. 2603-2610, Nov. 2019, doi: 10.1109/JBHI.2018.2887209.

\bibitem{b14} Hoffman, J. S., Viswanath, V., Ding, X., Thompson, M. J., Larson, E. C., Patel, S. N., \& Wang, E. (2021). Smartphone camera oximetry in an induced hypoxemia study. arXiv preprint arXiv:2104.00038.

\bibitem{b15} Pimentel, M. A., Johnson, A. E., Charlton, P. H., Birrenkott, D., Watkinson, P. J., Tarassenko, L., \& Clifton, D. A. (2016). Toward a robust estimation of respiratory rate from pulse oximeters. IEEE Transactions on Biomedical Engineering, 64(8), 1914-1923.

\bibitem{b16} B. Koteska, H. Mitrova, A. M. Bogdanova and F. Lehocki, "Machine learning based SpO2 prediction from PPG signal's characteristics features," 2022 IEEE International Symposium on Medical Measurements and Applications (MeMeA), 2022, pp. 1-6.

\bibitem{b17} TensorFlow. 2022. TensorFlow Lite | ML for Mobile and Edge Devices. [online] Available at: <https://www.tensorflow.org/lite> [Accessed 5 April 2022].


\end{thebibliography}

\end{document}