\documentclass{article} \usepackage[nonatbib, final]{mod_neurips}
\usepackage[numbers]{styles/natbib}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{color}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{subcaption}
\usepackage{float}
\usepackage{styles/svg}
\usepackage{lipsum} \usepackage{bm}
\usepackage{multicol}
\usepackage{booktabs}
\usepackage{array}
\usepackage{wrapfig}
\usepackage{enumitem}
\usepackage{caption}
\usepackage{subcaption}


\clearpage{}\makeatletter
\def\blfootnote{\xdef\@thefnmark{}\@footnotetext}
\makeatother

\newcommand{\cmt}[1]{{\footnotesize\textcolor{red}{#1}}}
\newcommand{\todo}[1]{\cmt{(TODO: #1)}}

\newcommand{\eg}{e.g.\ }
\newcommand{\ie}{i.e.\ }
\newcommand{\etal}{et al.\ }

\newcommand{\E}[2]{\operatorname{\mathbb{E}}_{#1}\left[#2\right]}
\newcommand{\EEE}{\mathbb{E}}
\newcommand{\density}{p}
\newcommand{\proposal}{q}  \newcommand{\target}{p}  \newcommand{\prop}{P}
\newcommand{\kl}[2]{\mathrm{D_{KL}}\left(#1\;\middle\|\;#2\right)}
\newcommand{\entropy}{\mathcal{H}}
\newcommand{\ent}{\mathcal{H}}
\newcommand{\sdots}{\,\cdot\,}
\newcommand{\func}{\mathbf{f}}

\newcommand{\ones}{\boldsymbol{1}}
\newcommand{\eye}{\boldsymbol{I}}
\newcommand{\zeros}{\boldsymbol{0}}

\newcommand{\sspace}{\mathcal{S}}
\newcommand{\aspace}{\mathcal{A}}
\newcommand{\state}{\mathbf{s}}
\newcommand{\sz}{{\state_0}}
\newcommand{\stm}{{\state_{t-1}}}
\newcommand{\st}{{\state_t}}
\newcommand{\sti}{{\state_t^{(i)}}}
\newcommand{\sT}{{\state_T}}
\newcommand{\stp}{{\state_{t+1}}}
\newcommand{\stpi}{{\state_{t+1}^{(i)}}}
\newcommand{\pdyn}{\density_\state}
\newcommand{\horizon}{T}
\newcommand{\action}{\mathbf{a}}
\newcommand{\az}{{\action_0}}
\newcommand{\atm}{{\action_{t-1}}}
\newcommand{\at}{{\action_t}}
\newcommand{\ati}{{\action_t^{(i)}}}
\newcommand{\atj}{{\action_t^{(j)}}}
\newcommand{\attildej}{{\tilde{\action}_t^{(j)}}}
\newcommand{\atij}{{\action_t^{(i,j)}}}
\newcommand{\atp}{{\action_{t+1}}}
\newcommand{\aT}{{\action_T}}
\newcommand{\atk}{{\action_t^{(k)}}}
\newcommand{\aTm}{\action_{\horizon-1}}
\newcommand{\opt}{^*}

\newcommand{\dataset}{\mathcal{D}}
\newcommand{\traj}{\tau}
\newcommand{\ptraj}{\density_\traj}
\newcommand{\visits}{\rho}  

\newcommand{\reward}{r}
\newcommand{\rz}{\reward_0}
\newcommand{\rt}{\reward_t}
\newcommand{\rti}{\reward^{(i)}_t}
\newcommand{\rmin}{r_\mathrm{min}}
\newcommand{\rmax}{r_\mathrm{max}}
\newcommand{\return}{\eta}




\newcommand{\V}{V}
\newcommand{\Vsoft}{V_\mathrm{soft}}
\newcommand{\Vsoftparams}{V_\mathrm{soft}^\qparams}
\newcommand{\Vhatsoftparams}{\hat V_\mathrm{soft}^\qparams}
\newcommand{\Vhatsoft}{\hat V_\mathrm{soft}}
\newcommand{\Vhard}{V^{\dagger}}
\newcommand{\Q}{Q}
\newcommand{\Qsoft}{Q_\mathrm{soft}}
\newcommand{\Qsoftparams}{Q_\mathrm{soft}^\qparams}
\newcommand{\Qhatsoft}{\hat Q_\mathrm{soft}}
\newcommand{\Qhatsoftparams}{\hat Q_\mathrm{soft}^{\bar\qparams}}
\newcommand{\Qhard}{Q^{\dagger}}
\newcommand{\A}{A}
\newcommand{\Asoft}{A_\mathrm{soft}}
\newcommand{\Asoftparams}{A_\mathrm{soft}^\qparams}
\newcommand{\Ahatsoft}{\hat A_\mathrm{soft}}

\newcommand{\energy}{\mathcal{E}}

\newcommand{\task}{\mathcal{T}}
\newcommand{\loss}{\mathcal{L}}
\newcommand{\data}{\mathcal{D}}
\newcommand{\inp}{\mathbf{x}}
\newcommand{\out}{\mathbf{y}}
\newcommand{\learner}{f}
\newcommand{\lossi}{\loss_{\task_i}}
\newcommand{\losst}{\loss_{\task}}
\newcommand{\buffer}{\mathcal{B}}

\newcommand{\ftml}{FTML }
\newcommand{\mdml}{MDML }

\newcommand{\cW}{\mathcal{W}}
\newcommand{\cM}{\mathcal{M}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cA}{\mathcal{A}}
\newcommand{\cT}{\mathcal{T}}
\newcommand{\cG}{\mathcal{G}}
\newcommand{\cR}{\mathcal{R}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cO}{\mathcal{O}}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\cB}{\mathcal{B}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\cF}{\mathcal{F}}

\newcommand{\bR}{\mathbb{R}}
\newcommand{\bE}{\mathbb{E}}
\newcommand{\bP}{\mathbb{P}}
\newcommand{\bT}{\mathbb{T}}

\newcommand{\Vpi}{V^{\pi}}
\newcommand{\Qpi}{Q^{\pi}}
\newcommand{\Api}{A^{\pi}}
\newcommand{\pith}{\pi_\theta}
\newcommand{\Vth}{\hat{V}_\theta}

\newcommand{\Vht}{\hat{V}}
\newcommand{\Vst}{V^*}
\newcommand{\piht}{\hat{\pi}}
\newcommand{\pist}{\pi^*}
\newcommand{\Vpiht}{V^{\hat{\pi}}}
\newcommand{\tauht}{\hat{\tau}}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}

\newcommand{\ftil}{\tilde{f}}
\newcommand{\wtil}{\tilde{w}}
\newcommand{\fht}{\hat{f}}



\DeclareMathOperator*{\argmin}{\mathrm{argmin}}

\newcommand{\param}{{\bm{\phi}}}               \newcommand{\paramspace}{\Phi}

\newcommand{\prior}{{\bm{\theta}}}               \newcommand{\priorspace}{\Theta}
\newcommand{\udparam}{\param}     \newcommand{\dparam}{{\param'}}             \newcommand{\mlprior}{\prior^*_{\mathrm{ML}}}
\newcommand{\fn}{\mathcal{L}}                  \newcommand{\udfn}{\tilde{\fn}}        \newcommand{\fnht}{\hat{\fn}}        \newcommand{\ud}{\alg}
\newcommand{\fnreg}{F^\lambda}
\newcommand{\dloss}{\ell}            \newcommand{\alg}{\mathcal{A}lg}
\newcommand{\alghat}{\alg^\delta}
\newcommand{\algwidehat}{\widehat{\alg}}
\newcommand{\algstar}{\mathcal{A}lg^\star}

\newcommand{\px}{\mathbf{u}}    \newcommand{\py}{\mathbf{v}}    \newcommand{\pz}{\mathbf{z}}    \newcommand{\step}{k} 
\newcommand{\udpx}{\tilde{\px}}
\newcommand{\udpy}{\tilde{\py}}

\newcommand{\eps}{\epsilon}
\newcommand{\grad}{\bm{d}}
\newcommand{\pgrad}{\nabla}

\newcommand{\datatr}{\data^{\mathrm{tr}}}
\newcommand{\datatest}{\data^{\mathrm{test}}}\clearpage{}
\graphicspath{{figures/}{../figures/}{./}{../../figures/}}

\hypersetup{
 colorlinks=True,
 linkcolor=blue,
 citecolor=blue,
 urlcolor=blue}

\newcommand{\ar}[1]{\textsf{\color{magenta} AR : #1}}
\newcommand{\sk}[1]{\textsf{\color{magenta} SK : #1}}
\newcommand{\cf}[1]{\textsf{\color{magenta} CF : #1}}

\makeatletter
\def\blfootnote{\xdef\@thefnmark{}\@footnotetext}
\makeatother

\title{Meta-Learning with Implicit Gradients}
\author{
	Aravind Rajeswaran \hspace*{7pt} Chelsea Finn \hspace*{7pt} Sham Kakade \hspace*{7pt} Sergey Levine \
    \label{eq:objective}
    \overbrace{ \mlprior := \argmin_{\prior \in \priorspace} F(\prior)}^{\mathrm{outer-level}} \, , 
    \textrm{ where } F(\prior) 
    = \frac{1}{M} \sum_{i=1}^M \ \fn \bigg( \overbrace{ \alg \big( \prior, \datatr_i \big) }^{\mathrm{inner-level}}, \ \datatest_i \bigg).
  
    \label{eq:maml_gd}
  \param_i \equiv \alg(\prior, \datatr_i) = \prior - \alpha \nabla_\prior \loss(\prior,
\datatr_i).  \hspace*{15pt} \text{(inner-level of MAML)}

    \label{eq:update_rule_supervised}
    \algstar(\prior, \datatr_i) = \underset{\dparam \in \paramspace}{\argmin}~\loss(\dparam, \datatr_i) + \frac{\lambda}{2}~||\dparam - \prior||^2.
  
  \fn_i(\param) := \fn \big(\param, \ \datatest_i \big), & 
  \fnht_i(\param) :=  \fn \big(\param, \datatr_i \big), &
\alg_i \big( \prior \big) :=  \alg \big( \prior, \datatr_i \big).

\begin{aligned}
\label{eq:update_rule}
& \mlprior := \argmin_{\prior \in \priorspace} F(\prior) \, , 
\textrm{ where } F(\prior) 
= \frac{1}{M} \sum_{i=1}^M \ \fn_i \big( \algstar_i (\prior) \big), \text{ and} \\
& \algstar_i(\prior) := \underset{\dparam \in \paramspace}{\argmin} \ G_i(\dparam, \prior), \text{ where } G_i(\dparam, \prior) = \fnht_i(\dparam) + \frac{\lambda}{2}~||\dparam - \prior||^2.
\end{aligned}

\grad_\prior \fn_i(\alg_i(\prior))
= {\frac{d\alg_i(\prior)}{d\prior} \pgrad_\param \fn_i(\param)\mid_{\param=\alg_i(\prior)}}
= {\frac{d\alg_i(\prior)}{d\prior} \pgrad_\param \fn_i(\alg_i(\prior))}

    \label{eq:outer_update_rule}
    \prior \leftarrow \prior - \eta \ \frac{1}{M} \sum_{i=1}^M \frac{d \algstar_i (\prior)}{d \prior} \ \pgrad_\phi \fn_i(\algstar_i(\prior)).

    \label{eq:alg_derivative}
    \frac{d \algstar_i(\prior)}{d \prior} = \left( \eye + \frac{1}{\lambda}~ \pgrad_\param^2 \fnht_i (\param_i) \right)^{-1}.

\| \alg_i(\prior) - \algstar_i(\prior) \| \leq \delta

\| \bm{g}_i - \bigg(\eye + \frac{1}{\lambda}~\pgrad_\param^2 \fnht_i(\param_i) \bigg)^{-1} \pgrad_\param \fn_i(\param_i) \| \leq \delta'
\label{eq:sub_problem}
\min_{\bm{w}} \ \ \bm{w}^\top \bigg(\eye + \frac{1}{\lambda}~\pgrad_\param^2 \fnht_i(\param_i) \bigg) \bm{w} - \bm{w}^\top \pgrad_\param \fn_i(\param_i)

\| \bm{g}_i - \grad_\prior \fn_i(\algstar_i(\prior)) \| \leq \eps

\| \bm{g} - \grad_\prior \fn_i(\alg_i(\prior)) \| \leq \eps

  ||\bm{g}_i - \grad_\prior \fn_i(\algstar_i(\prior))|| \leq \epsilon\, .

\|\nabla F(\prior) \| \leq \eps

\prior^{k+1} = \prior^k - \eta \ \frac{1}{M} \sum_{i=1}^M \grad_\prior \fn_i(\alg_i(\prior^k)).
\prior^{k+1} = \prior^k - \eta \ \frac{1}{M} \sum_{i=1}^M \pgrad_\param \fn_i (\param_i) \mid_{\param_i = \alg_i(\prior^k)}\prior^{k+1} = \prior^k - \eta \ \frac{1}{M} \sum_{i=1}^M (\prior^k - \param_i).
  ||\nabla f(x) || \leq B \, .

|| M(x) - M(x^\prime) || \leq \rho ||x - x^\prime|| 
\, ,

  || \nabla f(x) - \nabla f(x^\prime) || \leq L ||x - x^\prime|| 

|| \nabla f(x) - \nabla f(x^\prime) || \geq \mu ||x - x^\prime|| 
\, .

\|x -x^\star\| \leq \delta

\textrm{\# gradient computations of } f(\cdot) \leq 2\sqrt{\kappa}
\,  \log\left( 2\kappa \frac{ \|x^\star\| }{\delta}\right) \, .

\| \bm{g}_i - \grad_\prior \fn_i(\algstar_i(\prior)) \| \leq \eps

\| \bm{g} - \grad_\prior \fn_i(\alg_i(\prior)) \| \leq \eps

    \frac{d \algstar_i(\prior)}{d \prior} = \left( \eye + \frac{1}{\lambda}~ \pgrad_\param^2 \fnht_i (\param_i) \right)^{-1}.

\pgrad_\dparam G(\dparam, \prior) \mid_{\dparam = \udparam} \ = 0 \implies \pgrad \fnht(\udparam) + \lambda (\udparam - \prior) = 0 \implies \udparam = \prior - \frac{1}{\lambda}~\pgrad \fnht(\udparam),

\frac{d \udparam}{d \prior} = \eye - \frac{1}{\lambda}~\pgrad^2 \fnht(\udparam) \frac{d \udparam}{d \prior}
\implies 
\left( \eye + \frac{1}{\lambda}~\pgrad^2 \fnht(\udparam) \right) \frac{d \udparam}{d \prior} = \eye.

G_i(\dparam, \prior) := \fnht_i(\dparam) + \frac{\lambda}{2}~||\dparam - \prior||^2.

\kappa :=\frac{\beta}{\mu}  \, .

    \|\algstar_i(\prior) \|\leq D \, .
  
  \|\param_i-\algstar_i(\prior)\|\leq \delta

    \|\bm{g}_i- \left( \eye + \frac{1}{\lambda}~ \pgrad^2 \fnht_i (\param) \right)^{-1}\pgrad_\param \fn_i(\param)\|
\leq \delta^\prime \, .

\|\bm{g}_i - \grad_\prior \fn_i(\algstar_i(\prior)) \| \leq 
\left(2 \frac{\lambda\rho}{\mu^2} B
                + \frac{\lambda L}{\mu}\right)\delta+ \delta^\prime

\grad_\prior \fn_i(\algstar_i(\prior)) =
\left( \eye + \frac{1}{\lambda}~ \pgrad^2 \fnht_i (\algstar_i(\prior)) \right)^{-1}\pgrad_\param \fn_i(\algstar_i(\prior))
    
&&
\|\grad_\prior \fn(\algstar(\prior))-\bm{g}\| \\
&\leq &
\|\grad_\prior \fn(\algstar(\prior))-\left( \eye + \frac{1}{\lambda}~ \pgrad^2 \fnht (\param) \right)^{-1}\pgrad_\param \fn(\param)  \|+ \delta^\prime\\
&\leq &
\|\grad_\prior \fn(\algstar(\prior))-\left( \eye + \frac{1}{\lambda}~ \pgrad^2 \fnht (\param) \right)^{-1}\pgrad_\param \fn(\algstar(\prior))  \|
                +\\
&&\|\left( \eye + \frac{1}{\lambda}~ \pgrad^2 \fnht (\param) \right)^{-1}
\left(\pgrad_\param \fn(\algstar(\prior))  -\pgrad_\param \fn(\param)\right)\|
 + \delta^\prime

&&\|\left( \eye + \frac{1}{\lambda}~ \pgrad^2 \fnht (\param) \right)^{-1}
\left(\pgrad_\param \fn(\algstar(\prior))  -\pgrad_\param \fn(\param)\right)\|
\\
&\leq &
\|\left( \eye + \frac{1}{\lambda}~ \pgrad^2 \fnht (\param) \right)^{-1}\|
\|\pgrad_\param \fn(\algstar(\prior))  -\pgrad_\param \fn(\param)\| \\
&\leq &
\lambda L \|\left( \lambda \eye + \pgrad^2 \fnht (\param) \right)^{-1}\|
\|\algstar(\prior)  - \param\| \\
&= &
\lambda L \| \nabla^2_\param G(\param,\prior)^{-1}\|
\|\algstar(\prior)  - \param\| \\
&\leq & \frac{\lambda L}{\mu}\delta

&&
\|\grad_\prior \fn(\algstar(\prior))-\left( \eye + \frac{1}{\lambda}~ \pgrad^2 \fnht (\param) \right)^{-1}\pgrad_\param \fn(\algstar(\prior))  \| \\
&= &
\| \left(\left( \eye + \frac{1}{\lambda}~ \pgrad^2 \fnht (\algstar(\prior)) \right)^{-1}
-\left( \eye + \frac{1}{\lambda}~ \pgrad^2 \fnht (\param) \right)^{-1}\right) \pgrad_\param \fn(\algstar(\prior))\| 
\\
&\leq &
\lambda \| \left( \lambda \eye + \pgrad^2 \fnht (\algstar(\prior)) \right)^{-1}
-\left( \lambda \eye + \pgrad^2 \fnht (\param) \right)^{-1}\| B,
  
\Delta := 
  \pgrad^2 \fnht (\algstar(\prior))-\pgrad^2 \fnht (\param) , \quad
  M:= \nabla^2_\param G(\param,\prior)= \lambda \eye + \pgrad^2 \fnht (\param)

\|M^{-1}\Delta\| \leq \|\Delta\|/\mu \leq \rho
\delta/\mu \leq 1/2,
 
  &&\| \left( \lambda\eye + \pgrad^2 \fnht (\algstar(\prior)) \right)^{-1}
-\left( \lambda \eye + \pgrad^2 \fnht (\param)
     \right)^{-1}     \|\\
  &=&\| \left( M +\Delta \right)^{-1}-M^{-1}\|\\
  &\leq& \|M^{-1}\|\| \left( \eye + M^{-1}\Delta \right)^{-1}-\eye\|\\
  &=& \|M^{-1}\|\| \left( \eye + M^{-1}\Delta \right)^{-1}\left(\eye-\left(\eye + M^{-1}\Delta\right)\right)\|\\
  &\leq& \|M^{-1}\| \| \left( \eye + M^{-1}\Delta \right)^{-1}\| \| M^{-1}\Delta\|\\
  &\leq& \frac{1}{\mu} \cdot 2 \cdot \frac{\rho \delta}{\mu}=2 \frac{\rho}{\mu^2} \delta .

B_1& :=& 2 \frac{\lambda\rho}{\mu^2} B+ \frac{\lambda L}{\mu}\\

2\sqrt{\kappa} \, \log\left( 8\kappa D\left(\frac{ B_1}{\eps}+\frac{\rho}{\mu}\right)\right) 

2\sqrt{\kappa} \, \log\left( 4\kappa \frac{ (\lambda / \mu)B}{\eps}\right) \, .

\|\bm{g}_i - \grad_\prior \fn_i(\algstar_i(\prior)) \| \leq 
\eps.

2\log\left( 2\kappa \frac{ \|D\| }{\delta}\right) \leq
2\sqrt{\kappa} \, \log\left( 8\kappa D\left(\frac{ B_1}{\eps}+\frac{\rho}{\mu}\right)\right) 
  
2\log\left( 2\kappa \frac{ \|x^\star\| }{\delta}\right) \leq
2\log\left( 4\kappa \frac{ (\lambda / \mu)B }{\eps}\right) ,

\fnht_i(\param) = \frac{1}{2} \bE_{(\inp, \out)\sim \datatr_i} \left[ \| h_\param(\inp) - \out \|^2 \right] = \frac{1}{2} \param^T A_i \param + \param^T b_i,

G_i(\dparam, \prior) = \frac{1}{2} \dparam^T A_i \dparam + \dparam^T b_i + \frac{\lambda}{2} (\dparam - \prior)^T (\dparam - \prior)

\algstar_i(\prior) = \left( A_i + \lambda \eye \right)^{-1} \left( \lambda \prior - b_i \right) 

\grad_\prior \fn_i(\algstar_i(\prior)) = \lambda (A_i + \lambda \eye)^{-1} \pgrad_\param \fn_i(\prior) \mid_{\param = \algstar_i(\prior)}.

We compare this gradient with the gradients computed by the iMAML and MAML algorithms. We considered the case of , , , and , for the presented results.

\subsection{Omniglot and Mini-ImageNet experiments}

We follow the standard training and evaluation protocol as in prior works~\cite{mann, matchingnets, maml}. 

\paragraph{Omniglot Experiments} The GD version of iMAML uses 16 gradient steps for 5-way 1-shot and 5-way 5-shot settings, and 25 gradient steps for 20-way 1-shot and 20-way 5-shot settings. A regularization strength of  was used for both.  steps of conjugate gradient was used to compute the meta-gradient for each task in the mini-batch, and the meta-gradients were averaged before taking a step with the default parameters of Adam in the outer loop.

The Hessian-free version of MAML proceeds by using Hessian-free or Newton-CG method for solving the inner optimization problem (with respect to ) with objective . This method proceeds by constructing a local quadratic approximation to the objective and approximately computing the Newton direction with conjugate gradient.  CG steps are used for this process in our experiments. This allows us to compute the search direction, following which a step size has to be picked. We pick the step size through line-search. This procedure of computing the approximate Newton direction and linesearch is repeated  times in our experiments to solve the inner optimization problem well.

\paragraph{Mini-ImageNet}
For the GD version of iMAML, 10 GD steps were used with regularization strength of . Again, 5 CG steps are used to compute the meta-gradient. Similarly, in the Hessian-Free variant, we again use  CG steps to compute the search direction followed by line search. This process is repeated  times to solve the inner level optimization. Again, to compute the meta-gradient, 5 steps of CG are used.

\end{document}