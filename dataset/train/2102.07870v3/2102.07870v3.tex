
\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\graphicspath{{figures/}}
\usepackage{subfigure}
\usepackage{amsmath}
\pagenumbering{arabic}
\usepackage{tikz}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{mystyle}
\usepackage{amsthm}
\usepackage[english]{babel}
\usepackage{comment}
\usepackage{enumitem}
\usepackage[colorinlistoftodos,prependcaption,textsize=tiny]{todonotes}
\usepackage{multicol}
\usepackage{booktabs} \usepackage{adjustbox}
\usepackage[belowskip=-1pt,aboveskip=1pt]{caption}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{xargs}  
\usepackage{amssymb}\usepackage{pifont}\usepackage{algorithmic}
\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}\setlength{\intextsep}{10pt plus 2pt minus 2pt}
\usepackage{hyperref}
\usepackage[frozencache]{minted}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\usepackage[accepted]{icml2021}



\icmltitlerunning{Momentum Residual Neural Networks}

\begin{document}

\twocolumn[
\icmltitle{Momentum Residual Neural Networks}





\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Michael E. Sander}{ens,cnrs}
\icmlauthor{Pierre Ablin}{ens,cnrs}
\icmlauthor{Mathieu Blondel}{google}
\icmlauthor{Gabriel Peyr\'e}{ens,cnrs}
\end{icmlauthorlist}

\icmlaffiliation{ens}{Ecole Normale Sup\'erieure, DMA, Paris, France}
\icmlaffiliation{cnrs}{CNRS, France}
\icmlaffiliation{google}{Google Research, Brain team}

\icmlcorrespondingauthor{Michael Sander}{michael.sander@ens.fr}
\icmlcorrespondingauthor{Pierre Ablin}{pierre.ablin@ens.fr}
\icmlcorrespondingauthor{Mathieu Blondel}{mblondel@google.com}
\icmlcorrespondingauthor{Gabriel Peyr\'e}{gabriel.peyre@ens.fr}

\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]





\printAffiliationsAndNotice{} 

\begin{abstract}
The training of deep residual neural networks (ResNets) with backpropagation has a memory cost that increases linearly with respect to the depth of the network. 
A way to circumvent this issue is to use reversible architectures.
In this paper, we propose to change the forward rule of a ResNet by adding a momentum term. The resulting networks, momentum residual neural networks (Momentum ResNets),
are invertible.
Unlike previous invertible architectures, they can be used as a drop-in replacement for any existing ResNet block.
We show that Momentum ResNets can be interpreted in the infinitesimal step size regime as second-order ordinary differential equations (ODEs) and exactly characterize how adding momentum progressively increases the representation capabilities of Momentum ResNets: they can learn any linear mapping up to a multiplicative factor, while ResNets cannot.
In a learning to optimize setting, 
where convergence to a fixed point is required, we show theoretically and empirically that our method succeeds while existing invertible architectures fail.  
We show on CIFAR and ImageNet that Momentum ResNets have the same accuracy as ResNets, while having a much smaller memory footprint, and show that pre-trained Momentum ResNets are promising for fine-tuning models.
\end{abstract}

\vspace{-1em}
\section{Introduction}
\label{submission}

\paragraph{Problem setup.}

As a particular instance of deep learning \citep{cite-key,Goodfellow-et-al-2016}, residual neural networks \citep[ResNets]{he2015deep} have achieved great empirical successes due to extremely deep representations and their extensions keep on outperforming state of the art on real data sets \citep{alex2019big,touvron2020fixing}.
Most of deep learning tasks involve graphics processing units (GPUs), where memory is a practical bottleneck in several situations \citep{wang2018superneurons,peng2017large,zhu2017unpaired}.
Indeed, backpropagation, used for optimizing deep architectures, requires to store values (activations) at each layer during the evaluation of the network (forward pass). Thus, the depth of deep architectures is constrained by the amount of available memory. The main goal of this paper is to explore the properties of a new model, Momentum ResNets, that circumvent these memory issues by being invertible: the activations at layer  is recovered exactly from activations at layer .
This network relies on a modification of the ResNet's forward rule which makes it exactly invertible in practice. 
Instead of considering the feedforward relation for a ResNet (residual building block)

we define its momentum counterpart, which iterates 
where  is a parameterized function,  is a velocity term and  is a momentum term. This radically changes the dynamics of the network, as shown in the following figure.
\begin{figure}[H]
\centering
\includegraphics[width=0.7\columnwidth]{Res_Mom_1D.pdf} 
\caption{\textbf{Comparison of the dynamics} of a ResNet (left) and a Momentum ResNet with  (right) with \textbf{tied weights} between layers,  for all . The evolution of the activations at each layer is shown (depth 15). Models try to learn the mapping  in . The ResNet fails (the iterations approximate the solution of a first-order ODE, for which trajectories don’t cross, cf.\ Picard-Lindelof theorem)
while the Momentum ResNet leverages the changes in velocity to model more complex dynamics.}\label{fig:Res_Mom}
\vspace{-1em}
\end{figure}
In contrast with existing reversible models, Momentum ResNets can be integrated seamlessly in any deep architecture which uses residual blocks as building blocks (cf.\ in Section~\ref{section:mom_net}).
\vspace{-1em}
\paragraph{Contributions.}

We  introduce momentum residual neural networks (Momentum ResNets), a new deep model that relies on a simple modification of the ResNet forward rule and which, without any constraint on its architecture, is perfectly invertible. We show that the memory requirement of Momentum ResNets is arbitrarily reduced by changing the momentum term  (Section~\ref{section:memory_cost}), and show that they can be used as a drop-in replacement for traditional ResNets.

On the theoretical side,  we show that Momentum ResNets are easily used in the learning to optimize setting, where other reversible models fail to converge (Section~\ref{sec:role_momentum}). We also investigate the approximation capabilities of Momentum ResNets, seen in the continuous limit as second-order ODEs (Section~\ref{section:representation_capabilities}). We first show in Proposition~\ref{prop:bigger_set} that Momentum ResNets can represent a strictly larger class of functions than first-order neural ODEs. Then, we give more detailed insights by studying the linear case, where we formally prove in Theorem~\ref{th:representability} that Momentum ResNets with linear residual functions have universal approximation capabilities, and precisely quantify how the set of representable mappings for such models grows as the momentum term  increases. This theoretical result is a first step towards a theoretical analysis of representation capabilities of Momentum ResNets.
 
 Our last contribution is the experimental validation of Momentum ResNets on various learning tasks. We first show that Momentum ResNets separate point clouds that ResNets fail to separate (Section~\ref{sec:point_clouds}). We also show on image datasets (CIFAR-10, CIFAR-100, ImageNet) that Momentum ResNets have similar accuracy as ResNets, with a smaller memory cost (Section~\ref{sec:experiments_images}). We also show that parameters of a pre-trained model are easily transferred to a Momentum ResNet which achieves comparable accuracy in only few epochs of training. We argue that this way to obtain pre-trained Momentum ResNets is of major importance for fine-tuning a network on new data for which memory storage is a bottleneck.  We provide a Pytorch package with a method that takes a torchvision ResNet model and returns its Momentum counterpart that achieves similar accuracy with very little refit.  We also experimentally validate our theoretical findings in the learning to optimize setting, by confirming that Momentum ResNets perform better than RevNets \citep{gomez2017reversible}. 
 Our code is available at \url{https://github.com/michaelsdr/momentumnet}.

\section{Background and previous works.}

\paragraph{Backpropagation.}
\textit{Backpropagation} is the method of choice to compute the gradient of a scalar-valued function.
It operates using the chain rule with a backward traversal of the computational graph \citep{Computational_Graphs}. It is also known as reverse-mode automatic differentiation \citep{baydin2015automatic, 1986Natur.323..533R,verma2000introduction, doi:10.1137/1.9780898717761}. The computational cost is similar to the one of evaluating the function itself. The only way to back-propagate gradients through a neural architecture without further assumptions is to store all the intermediate activations during the forward pass. This is the method used in common deep learning libraries such as Pytorch~\citep{paszke2017automatic}, Tensorflow~\citep{tensorflow2015-whitepaper} and JAX~\citep{jacobsen2018irevnet}. A common way to reduce this memory storage is to use checkpointing: activations are only stored at some steps and the others are recomputed between these check-points as they become needed in the backward pass~(e.g., \citet{martens2012training}).
\vspace{-1em}
\paragraph{Reversible architectures.}
However, models that allow backpropagation without storing any activations have recently been developed. They are based on two kinds of approaches.
The first is \textit{discrete} and relies on finding ways to easily invert the rule linking activation  to activation  \citep{gomez2017reversible,chang2017reversible,Haber_2017,jacobsen2018irevnet, behrmann2019invertible}.
In this way, it is possible to recompute the activations \emph{on the fly} during the backward pass: activations do not have to be stored.
However, these methods either rely on restricted architectures where there is no straightforward way to transfer a well performing non-reversible model into a reversible one, or do not offer a fast inversion scheme when recomputing activations backward. In contrast, our proposal can be applied to any existing ResNet and is easily inverted.
The second kind of approach is \textit{continuous} and
relies on ordinary differential equations (ODEs), where ResNets are interpreted as continuous dynamical systems \citep{E_2017,chen2018neural,dupont2019augmented,sun2018stochastic,E_2018,lu2017finite,ruthotto2018deep}. This allows one to import theoretical and numerical advances from ODEs to deep learning.
These models are often called neural ODEs~\citep{chen2018neural} and can be trained by using an adjoint sensitivity method \citep{Pontryagin:234445}, solving ODEs backward in time. This strategy avoids performing reverse-mode automatic differentiation 
through the operations of the ODE solver and leads to a  memory footprint.
However, defining the neural ODE counterpart of an existing residual architecture is not straightforward: optimizing ODE blocks is an infinite dimensional problem requiring a non-trivial time discretization, and the performances of neural ODEs depend on the numerical integrator for the ODE \citep{gusak2020towards}.
In  addition, ODEs cannot always be numerically reversed, because of stability issues: numerical errors can occur and accumulate when a system is run backwards \citep{gholami2019anode, dupont2019augmented}. Thus, in practice, neural ODEs are seldom used in standard deep learning settings.  Nevertheless, recent works \citep{zhang2019anodev2, queiruga2020continuous} incorporate ODE blocks in neural architectures to achieve comparable accuracies to ResNets on CIFAR.

\vspace{-0.5em}
\if 0
\begin{table*}[h!]
    \centering
    \begin{tabular}{ |p{4cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|}
     \hline
\textbf{Model} & Neural ODE &  i-ResNet & i-RevNet & RevNet & Momentum ResNet\\
 \hline
   Closed-form inversion & \cmark &  \xmark & \cmark & \cmark & \cmark \\
   \hline
 Same parameters  & \xmark &  \cmark & \xmark & \xmark & \cmark \\
 \hline
 Unconstrained training & \cmark &  \xmark & \cmark & \cmark & \cmark \\ 
 \hline
\end{tabular}
\caption{\label{tab:comparison_invertible_architectures} {\bf Comparison of reversible residual architectures} }
\end{table*}
 \fi 

\vspace{-0.5em}
 \paragraph{Representation capabilities.}

Studying the representation capabilities of such models is also important, as it gives insights regarding their performance on real world data. It is well-known that a single residual block has universal approximation capabilities \citep{Cybenkot2006ApproximationBS}, meaning that on a compact set any continuous function can be uniformly approximated with a one-layer feedforward fully-connected neural network. However, neural ODEs have limited representation capabilities. 
\citet{dupont2019augmented} propose to lift points in higher dimensions by concatenating vector fields of data with zeros in an extra-dimensional space, and show that the resulting augmented neural ODEs (ANODEs) achieve lower loss and better generalization on image classification and toy experiments.  \citet{li2019deep} show that, if the output of the ODE-Net is composed with elements of a terminal family, then universal approximation capabilities are obtained for the convergence in  norm for , which is insufficient \citep{teshima2020universal}. In this work, we consider the representation capabilities in  norm of the ODEs derived from the forward iterations of a ResNet.  Furthermore, \citet{zhang2019approximation} proved that doubling the dimension of the ODE leads to universal approximators, although this result has no application in deep learning to our knowledge.  In this work, we show that in the continuous limit, our architecture has better representation capabilities than Neural ODEs. We also prove its universality in the linear case.
\vspace{-0.5em}
\paragraph{Momentum in deep networks.} Some recent works \citep{he2020momentum, chun2020momentum,  nguyen2020momentumrnn, li2018optimization} have explored momentum in deep architectures. However, these methods differ from ours in their architecture and purpose. \citet{chun2020momentum} introduce a momentum to solve an optimization problem for which the iterations do not correspond to a ResNet. 
\citet{nguyen2020momentumrnn} (resp. \citet{he2020momentum}) add momentum in the case of RNNs (different from ResNets) where the weights are tied to alleviate the vanishing gradient issue (resp. link the key and query encoder layers). \citet{li2018optimization} consider a particular case where the linear layer is tied and is a symmetric definite matrix. In particular, none of the mentioned architectures are invertible, which is one of the main assets of our method.
\vspace{-1em}
\paragraph{Second-order models} We show that adding a momentum term corresponds to an Euler integration scheme for integrating a second-order ODE. Some recently proposed architectures \citep{norcliffe2020second, rusch2020coupled, lu2017finite, massaroli2020dissecting} are also motivated by second-order differential equations. \citet{norcliffe2020second} introduce second-order dynamics to model second-order dynamical systems, whereas our model corresponds to a discrete set of equations in the continuous limit. Also, in our method, the neural network only acts on , so that although momentum increases the dimension to , the computational burden of a forward pass is the same as a ResNet of dimension . \citet{rusch2020coupled} propose second-order RNNs, whereas our method deals with ResNets. Finally, the formulation of LM-ResNet in \citet{lu2017finite} differs from our forward pass (), even though they both lead to second-order ODEs. Importantly, none of these second-order formulations are invertible.
\vspace{-1em}
\paragraph{Notations}
For , we denote by ,  and  the set of real matrices, of invertible matrices, and of \textbf{real} matrices that are diagonalizable in . 
\section{Momentum Residual Neural Networks}\label{section:mom_net}
We now introduce Momentum ResNet, a simple transformation of \textbf{any} ResNet into a model with a small memory requirement, and that can be seen in the continuous limit as a second-order ODE.
\vspace{-1em}
\subsection{Momentum ResNets}\label{section:Momentum_nets}

\paragraph{Adding a momentum term in the ResNet equations.}

For \textbf{any} ResNet which iterates~\eqref{eq:ResNet},
we define its Momentum counterpart, which iterates~\eqref{eq:Momentum ResNet},
where  is the velocity  initialized with some value  in , and  is the so-called momentum term. This approach generalizes gradient descent algorithm with momentum \citep{ruder2016overview},
for which  is the gradient of a function to minimize. 

\paragraph{Initial speed and momentum term.}

 In this paper, we consider initial speeds  that depend on  through a simple relation. The simplest options are to set  or . We prove in Section~\ref{section:representation_capabilities} that this dependency between  and  has an influence on the set of mappings that Momentum ResNets can represent.
The parameter  controls how much a Momentum ResNet diverges from a ResNet, and also the amount of memory saving. The closer  is to , the closer Momentum ResNets are to ResNets, but the less memory is saved. In our experiments, we use , which we find to work well in various applications.

\paragraph{Invertibility.}

Procedure~\eqref{eq:Momentum ResNet} is inverted through

so that activations can be reconstructed on the fly during the backward pass in a Momentum ResNet. 
In practice, in order to exactly reverse the dynamics, the information lost by the finite-precision multiplication by  in~\eqref{eq:Momentum ResNet} has to be efficiently stored. We used the algorithm from \citet{10.5555/3045118.3045343} to perform this reversible multiplication. It consists in maintaining an information buffer, that is, an integer that stores the bits that are lost at each iteration, so that multiplication becomes reversible. We further describe the procedure in Appendix~\ref{app:memory_savings}.  Note that there is always a small loss of floating point precision due to the addition of the learnable mapping . In practice, we never found it to be a problem: this loss in
precision can be neglected compared to the one due to the multiplication by .

\newcommand{\myrot}[1]{\rotatebox{80}{#1}}
 \begin{table}[h!]
    \centering
    \caption{\label{tab:comparison_invertible_architectures} {\bf Comparison of reversible residual architectures} }
    \vskip 0.15in
    \begin{tabular}{ |c|c|c|c|c|c|}
   \cline{2-6}
    \multicolumn{1}{c|}{} & 
  \myrot{Neur.ODE\ } &  \myrot{i-ResNet} & \myrot{i-RevNet} & \myrot{RevNet} & \myrot{\textbf{Mom.Net}}\\
 \hline
   Closed-form inversion & \cmark &  \xmark & \cmark & \cmark & \cmark \\
   \hline
 Same parameters  & \xmark &  \cmark & \xmark & \xmark & \cmark \\
 \hline
 Unconstrained training & \cmark &  \xmark & \cmark & \cmark & \cmark \\ 
 \hline
\end{tabular}
\vskip -0.15in
\end{table}

\paragraph{Drop-in replacement.}
Our approach makes it possible to turn any existing ResNet into a reversible one.  In other words, a ResNet can be transformed into its Momentum counterpart without changing the structure of each layer. For instance, consider a ResNet-152 \citep{he2015deep}. It is made of  layers (of depth , ,  and ) and can easily be turned into its Momentum ResNet counterpart 
by changing the forward equations~\eqref{eq:ResNet} 
into~\eqref{eq:Momentum ResNet} in the  layers. No further change is needed and Momentum ResNets take the exact same parameters as inputs: they are a \textit{drop-in replacement}. This is not the case of other reversible models. Neural ODEs \citep{chen2018neural} take continuous parameters as inputs. i-ResNets \citep{behrmann2019invertible} cannot be trained by plain SGD since the spectral norm of the weights requires constrained optimization. i-RevNets \citep{jacobsen2018irevnet} and RevNets \citep{gomez2017reversible} require to train two networks with their own parameters for each residual block, split the inputs across convolutional channels, and are half as deep as ResNets: they do not take the same parameters as inputs. Table~\ref{tab:comparison_invertible_architectures} summarizes the properties of reversible residual architectures.  We discuss in further details the differences between RevNets and Momentum ResNets in sections \ref{sec:role_momentum} and \ref{sec-numerics-lista}.


\subsection{Memory cost}\label{section:memory_cost}

Instead of storing the full data at each layer, we only need to store the bits lost at each multiplication by  (cf.\ ``intertibility''). For an architecture of depth , this corresponds to storing  values for each sample ( if  is close to ).
To illustrate, we consider two situations where storing the activations is by far the main memory bottleneck. First, consider a toy feedforward architecture where , with  and , where  and , with a depth . We suppose that the weights are the same at each layer.
The training set is composed of  vectors . 
For \textbf{ResNets}, we need to store the weights of the network and the values of all activations for the training set at each layer of the network.
In total, the memory needed is  per iteration. 
In the case of \textbf{Momentum ResNets}, if  is close to  we get a memory requirement of .
This proves that the memory dependency in the depth  is arbitrarily reduced by changing the momentum . The memory savings are confirmed in practice, as shown in Figure~\ref{fig:memo_theory}. 

\begin{figure}[H]
\centering
\includegraphics[width=0.7\columnwidth]{memory_theory.pdf} 
\caption{{\bf Comparison of memory needed} (calculated using a profiler) for computing gradients of the loss, with ResNets (activations are stored) and Momentum ResNets (activations are not stored). We set ,  and  at each depth. Momentum ResNets give a nearly constant memory footprint.
}\label{fig:memo_theory}
\vspace{-1em}
\end{figure}

As another example, consider a ResNet-152 \citep{he2015deep} which can be used for ImageNet classification \citep{deng2009imagenet}. Its layer named ``\texttt{conv4\_x}'' has a depth of : it has  M parameters, whereas storing the activations would require storing  times more parameters. Since storing the activations is here the main obstruction, the memory requirement for this layer can be arbitrarily reduced by taking  close to .




\subsection{The role of momentum}\label{sec:role_momentum}

When  is set to  in~\eqref{eq:Momentum ResNet}, we recover a ResNet. Therefore, Momentum ResNets are a generalization of ResNets. When , one can scale  to get in~\eqref{eq:Momentum ResNet}
a symplectic scheme \citep{Hairer:1250576} that recovers a special case of other popular invertible neural network: RevNets \citep{gomez2017reversible} and Hamiltonian Networks \citep{chang2017reversible}. 
A RevNet iterates 
where  and  are two learnable functions.


The usefulness of such architecture depends on the task.
RevNets have encountered success for classification and regression.
However, we argue that RevNets cannot work in some settings.
For instance, under mild assumptions, the RevNet iterations do not have attractive fixed points when the parameters are the same at each layer: , .
We rewrite \eqref{eq:revnet} as  with .
\begin{proposition}[Instability of fixed points]
\label{prop:revnet_fix}
Let  a fixed point of the RevNet iteration~\eqref{eq:revnet}. Assume that  (resp. ) is differentiable at  (resp. ), with Jacobian matrix  (resp. ) . The Jacobian of  at  is . 
If  and  are invertible, then there exists  such that  and .
\end{proposition}
This shows that  cannot be a stable fixed point.
As a consequence, in practice, a RevNet cannot have converging iterations: according to~\eqref{eq:revnet}, if  converges then  must also converge, and their limit must be a fixed point. 
The previous proposition shows that it is impossible.

This result suggests that RevNets should perform poorly in problems where one expects the iterations of the network to converge.
For instance, as shown in the experiments in Section~\ref{sec-numerics-lista}, this happens when we use reverible dynamics in order to \emph{learn to optimize} \citep{10.5555/3045118.3045343}.
In contrast, the proposed method can converge to a fixed point as long as the momentum term  is strictly less than .
\vspace{-1em}
\paragraph{Remark.} Proposition \ref{prop:revnet_fix} has a continuous counterpart. Indeed, in the continuous limit, \eqref{eq:revnet} writes . The corresponding Jacobian in  is . The eigenvalues of this matrix are the square roots of those of : they cannot all have a real part  (same stability issue in the continuous case).

\subsection{Momentum ResNets as continuous models}

\begin{figure}[H]
\includegraphics[width=\columnwidth]{fig_recap.pdf} 
\caption{\textbf{Overview of the four different paradigms.} }\label{fig:recap}
\vspace{-1em}
\end{figure}

\paragraph{Neural ODEs: ResNets as first-order ODEs.}
The ResNets equation~\eqref{eq:ResNet} with initial condition  (the input of the ResNet) can be seen as a discretized Euler scheme of the ODE  with .
Denoting  a time horizon, the neural ODE maps the input  to the output , and, as in \citet{chen2018neural}, is trained by minimizing a loss . 

\paragraph{Momentum ResNets as second-order ODEs.}

Let . We can then rewrite \eqref{eq:Momentum ResNet} as 

which corresponds to a Verlet integration scheme \citep{Hairer:1250576} with step size  of the differential equation 
Thus, in the same way that ResNets can be seen as discretization of first-order ODEs, Momentum ResNets can be seen as discretization of second-order ones. 
Figure~\ref{fig:recap} sums up these ideas.
\section{Representation capabilities}\label{section:representation_capabilities}
We now turn to the analysis of the representation capabilities of Momentum ResNets in the continuous setting. In particular, we precisely characterize the set of mappings representable by Momentum ResNets with linear residual functions. 
\subsection{Representation capabilities of first-order ODEs}\label{section:representation_capabilitie}
We consider the first-order model 


We denote by  the solution at time  starting at initial condition . It is called the \textit{flow} of the ODE. For all , where  is a time horizon,  is a homeomorphism: it is continuous, bijective with continuous inverse. 

\paragraph{First-order ODEs are not universal approximators.}

ODEs such as~\eqref{eq:first_order_ODE} are not universal approximators. Indeed, the function mapping an initial condition to the flow at a certain time horizon  cannot represent every mapping . For instance when , the mapping  cannot be approximated by a first-order ODE, since  should be mapped to  and  to , which is impossible without intersecting trajectories~\citep{dupont2019augmented}. In fact, the homeomorphisms represented by~\eqref{eq:first_order_ODE} are orientation-preserving: if  is a compact set and  is a homeomorphism represented by~\eqref{eq:first_order_ODE}, then  is in the connected component of the identity function on  for the topology of the uniform convergence (see details in Appendix~\ref{app:prop_connected}). 

\subsection{Representation capabilities of second-order ODEs}
We consider the second-order model for which we recall that Momentum ResNets are a discretization: 

In Section~\ref{sec:role_momentum}, we showed that Momentum ResNets generalize existing models when setting  or . We now state the continuous counterparts of these results.
Recall that . When , we recover the first-order model.
\begin{proposition}[Continuity of the solutions]\label{prop:eps_0}We let  (resp. ) be the solution of~\eqref{eq:first_order_ODE} (resp.~\eqref{eq:pertu}) on , with initial conditions  and .
Then   as .
\end{proposition} 
The proof of this result relies on the implicit function theorem and can be found in Appendix~\ref{app:prop_eps_0}.
Note that Proposition~\ref{prop:eps_0} is true whatever the initial speed . 
When , one needs to rescale  to study the asymptotics: the solution of  converges to the solution of  (see details in Appendix~\ref{app:prop_eps_infty}).
 These results show that in the continuous regime, Momentum ResNets also interpolate between  and .

\paragraph{Representation capabilities of a model~\eqref{eq:pertu} on the  space.}
We recall that we consider initial speeds  that can depend on the input  (for instance  or ). We therefore assume  such that  is solution of~\eqref{eq:pertu}. We emphasize that  is not always a homeomorphism. For instance, 

solves
 with . All the trajectories intersect at time . It means that Momentum ResNets can learn mappings that are not homeomorphisms, which suggests that increasing  should lead to better representation capabilities. The first natural question is thus whether, given , there exists some  such that  associated to~\eqref{eq:pertu} satisfies .
In the case where  is an arbitrary function of , the answer is trivial since~\eqref{eq:pertu} can represent any mapping, as proved in Appendix~\ref{app:learn_init}.
This setting does not correspond to the common use case of ResNets, which take advantage of their depth, so it is important to impose stronger constraints on the dependency between  and .  
For instance, the next proposition shows that even if one imposes , a second-order model is at least as general as a first-order one.

\begin{proposition}[Momentum ResNets are at least as general]\label{prop:bigger_set}
There exists a function  such that for all  solution of~\eqref{eq:first_order_ODE},  is also solution of the second-order model

with 
.
\end{proposition}

Furthermore, even with the restrictive initial condition ,  for  can always be represented by a second-order model~\eqref{eq:pertu}
(see details in Appendix~\ref{app:prop_lambda}).
This supports the claim that the set of representable mappings increases with . 

\subsection{Universality of Momentum ResNets with linear residual functions}

As a first step towards a theoretical analysis of the universal representation capabilities of Momentum ResNets, we now investigate the linear residual function case.
Consider the second-order linear ODE 

with . We assume without loss of generality that the time horizon is . We have the following result.
\begin{proposition}[Solution of~\eqref{eq:second_order_lin}]\label{prop:sol_second_order}
At time , \eqref{eq:second_order_lin} defines the linear mapping  where 

\end{proposition}
Characterizing the set of mappings representable by~\eqref{eq:second_order_lin} is thus equivalent to precisely analyzing the range .


\paragraph{Representable mappings of a first-order linear model.}


When , Proposition~\ref{prop:eps_0} shows that
 . 
The range of the matrix exponential is indeed the set of representable mappings of a first order linear model 

and this range is known~\citep{andrica2010image} to be
.
This means that one can only learn mappings that are the square of invertible mappings with a first-order linear model~\eqref{eq:first_order_lin}. 
To ease the exposition and exemplify the impact of increasing , we now consider the case of matrices with real coefficients that are diagonalizable in , . 
Note that the general setting of arbitrary matrices is exposed in Appendix~\ref{app:th_representability} using Jordan decomposition. Note also that  is dense in  \citep{10.2307/2160975}. 
Using Theorem~1 from~\citet{culver1966existence}, we have that if , then  is represented by a first-order model~\eqref{eq:first_order_lin} \textbf{if and only if}  is non-singular and for all eigenvalues  with ,  is of even multiplicity order. This is restrictive because it forces negative eigenvalues to be in pairs. We now generalize this result and show that increasing  leads to less restrictive conditions.
\begin{figure}[H]
\centering
 \includegraphics[width=\columnwidth]{fig_vp_eps.pdf}
\caption{Left: \textbf{Evolution of  defined in Theorem~\ref{th:representability}}.  is non increasing, stays close to  when  and close to  when . Right: \textbf{Evolution of the real eigenvalues}  and  of representable matrices in  by~\eqref{eq:second_order_lin} when  for different values of . The grey colored areas correspond to the different representable eigenvalues. When ,  or  and . When , single negative eigenvalues are acceptable.}\label{fig:linear}
\vspace{-1em}
\end{figure}
\paragraph{Representable mappings by a second-order linear model.}
Again, by density and for simplicity, we focus on matrices in , and we state and prove the general case in Appendix~\ref{app:th_representability}, making use of Jordan blocks decomposition of matrix functions \citep{gant} and localization of zeros of entire functions \citep{runckel1969zeros}. 
The range of  over the reals has for form . It plays a pivotal role to control the set of representable mappings, as stated in the theorem bellow. Its minimum value can be computed conveniently since it satisfies
 where
.
\begin{theorem}[Representable mappings with linear residual functions]\label{th:representability}
Let .
Then  is represented by a second-order model~\eqref{eq:second_order_lin} \textbf{if and only if}  such that
,  is of even multiplicity order.
\end{theorem}
Theorem~\ref{th:representability} is illustrated in Figure \ref{fig:linear}. 
A consequence of this result is that the set of representable linear mappings is \textbf{strictly increasing} with .
Another consequence is that one can learn \textbf{any} mapping up to scale using the ODE~\eqref{eq:second_order_lin}: if , there exists  such that for all , one has . Theorem~\ref{th:representability} shows that  is represented by a second-order model~\eqref{eq:second_order_lin}.

\section{Experiments}

We now demonstrate the applicability of Momentum ResNets through experiments. We used Pytorch and Nvidia Tesla V100 GPUs. 
\subsection{Point clouds separation}\label{sec:point_clouds}
\begin{figure}[H]
\includegraphics[width=\columnwidth]{Res_Mom_Sep.pdf} 
\caption{\textbf{Separation of four nested rings} using a ResNet (upper row) and a Momentum ResNet (lower row). From left to right, each figure represents the point clouds transformed at layer . The ResNet fails whereas the Momentum ResNet succeeds.}\label{fig:four_imbricated}
\vspace{-1em}
\end{figure}
We experimentally validate the representation capabilities of Momentum ResNets on a challenging synthetic classification task. 
As already noted \citep{dupont2019augmented}, neural ODEs ultimately fail to break apart nested rings.
We experimentally demonstrate the advantage of Momentum ResNets by separating  nested rings ( classes). We used the same structure for both models:  with , , , and a depth . Evolution of the points as depth increases is shown in Figure~\ref{fig:four_imbricated}.  The fact that the trajectories corresponding to the ResNet panel don't cross is because, with this depth, the iterations approximate the solution of a first order ODE, for which trajectories cannot cross, due to the Picard-Lindelof theorem.

\subsection{Image experiments}\label{sec:experiments_images}


We also compare the accuracy of ResNets and Momentum ResNets on real data sets: CIFAR-10, CIFAR-100 \citep{CIFAR} and  ImageNet \citep{deng2009imagenet}.  We used existing ResNets architectures. We recall that Momentum ResNets can be used as a drop-in replacement and that it is sufficient to replace every residual building block with a momentum residual forward iteration. We set  in the experiments. More details about the experimental setup are given in Appendix~\ref{app:experiment_details}.








\paragraph{Results on CIFAR-10 and CIFAR-100.} 

\begin{table}[H]
\vskip -0.15in
\centering
\caption{\label{tab:results_CIFAR}\textbf{Test accuracy for CIFAR} over 10 runs for each model}
\vskip 0.15in
\begin{adjustbox}{width=\columnwidth,center}
\begin{tabular}{|l|l|l|}
  \hline
  \textbf{Model} & \textbf{CIFAR-10} & \textbf{CIFAR-100} \\ \hline
  {Momentum ResNet, } & {} & {} \\ \hline
  {Momentum ResNet, } &  &  \\ \hline
  {ResNet} &  &  \\ \hline
\end{tabular}
\end{adjustbox}

\end{table}


For these data sets,
we used a ResNet-101 \citep{he2015deep} and a Momentum ResNet-101 and compared the evolution of the test error and test loss. Two kinds of Momentum ResNets were used: one with an initial speed  and the other one where the initial speed  was learned: . These experiments show that Momentum ResNets perform similarly to ResNets. Results are summarized in Table~\ref{tab:results_CIFAR}.


 






\begin{figure*}[ht]
 \centering
\includegraphics[width=0.61\textwidth]{beta_ResNet.pdf} 
 \unskip\ \vrule\
 \includegraphics[width=0.37\textwidth]{rev_res_101.pdf} 
 \caption{Upper row: \textbf{Robustness of final accuracy w.r.t } when training Momentum ResNets 101 on CIFAR-10. We train the networks with a momentum  and evaluate their accuracy with a different momentum  at test time. We optionally refit the networks for  epochs. We recall that  corresponds to a classical ResNet and  corresponds to a Momentum ResNet with optimal memory savings.
Lower row: \textbf{Top-1 classification error on ImageNet (single crop)} for  different residual architectures of depth 101 with the same number of parameters. Final test accuracy is  for the ResNet-101 and  for the  other invertible models.  In particular, our model achieve the same performance as a RevNet with the same number of parameters.}
\label{fig:beta_learning_curves}
\vspace{-1em}
 \end{figure*}
 
\paragraph{Effect of the momentum term .}

Theorem~\ref{th:representability} shows the effect of  on the representable mappings for linear ODEs.
To experimentally validate the impact of , we train a Momentum ResNet-101 on CIFAR-10 for different values of the momentum at train time, . We also evaluate Momentum ResNets trained with  and  with no further training for several values of the momentum at test time, . In this case, the test accuracy never decreases by more than .  We also refit for  epochs Momentum ResNets trained with  and . This is sufficient to obtain similar accuracy as models trained from scratch. Results are shown in Figure~\ref{fig:beta_learning_curves} (upper row). This indicates that the choice of  has a limited impact on accuracy.  In addition, learning the parameter  does not affect the accuracy of the model. Since it also breaks the method described in \ref{section:memory_cost}, we fix  in all the experiments.



\paragraph{Results on ImageNet.}
For this data set, we used a ResNet-101, a Momentum ResNet-101, and a RevNet-101. For the latter, we used the procedure from \citet{gomez2017reversible} and adjusted the depth of each layer for the model to have approximately the same number of parameters as the original ResNet-101. Evolution of test errors are shown in Figure~\ref{fig:beta_learning_curves} (lower row), where comparable performances are achieved.
\vspace{-0.5em}
\paragraph{Memory costs.}
We compare the memory (using a memory profiler) for performing one epoch as a function of the batch size for two datasets: ImageNet (depth of 152) and CIFAR-10 (depth of 1201). Results are shown in Figure~\ref{fig:memory_time} and illustrate how Momentum ResNets can benefit from increased batch size, especially for very deep models. We also show in Figure~\ref{fig:memory_time} the final test accuracy for a full training of Momentum ResNets on CIFAR-10 as a function of the memory used (directly linked to  (section \ref{section:memory_cost})).

 \begin{figure}[ht]
 \center  \includegraphics[width=1\columnwidth]{imagenet_memory_152.pdf}
 \vspace{0.5em}
  \unskip\ \hrule\
  \includegraphics[width=0.63\columnwidth]{memory_rebuttal_Rev.pdf}  \caption{Upper row: \textbf{Memory used} (using a profiler) for a ResNet and a Momentum ResNet on one training epoch, as a function of the batch size.  Lower row: \textbf{Final test accuracy} as a function of the memory used (per epoch) for training Momentum ResNets-101 on CIFAR-10.} 
 \label{fig:memory_time} 
 \end{figure}
\paragraph{Ability to perform pre-training and fine-tuning.}

It has been shown \citep{tajbakhsh2016convolutional} that in various medical imaging applications the use of a pre-trained model on ImageNet
with adapted fine-tuning outperformed a model trained from scratch. In order to easily obtain pre-trained Momentum ResNets for applications where memory could be a bottleneck, we transferred the learned parameters of a ResNet-152 pre-trained on ImageNet to a Momentum ResNet-152 with . In only  epoch of additional training we reached a top-1 error of  and in  additional epochs a top-1 error of . We then empirically compared the accuracy of these pre-trained models by fine-tuning them on new images: the \texttt{hymenoptera}\footnote{\href{url}{https://www.kaggle.com/ajayrana/hymenoptera-data}} data set. 

\begin{figure}[H]
\begin{minipage}[c]{0.55\linewidth}
        \includegraphics[width=\textwidth]{mom_4_res_2.pdf}
    \end{minipage}\hfill
    \begin{minipage}[c]{0.42\linewidth}
        \vspace{-1em}
    \caption{\textbf{Accuracy} as a function of time on \texttt{hymenoptera} when fine-tuning a ResNet-152 and a Momentum ResNet-152 with batch sizes of  and , respectively, as permitted by memory. }\label{fig:fine_tuning}
    \end{minipage}
\end{figure}
\vspace{-1em}
As a proof of concept, suppose we have a GPU with  Go of RAM. The images have a resolution of  pixels so that the maximum batch size that can be taken for fine-tuning the ResNet-152 is , against  for the Momentum ResNet-152.  As suggested in \citet{tajbakhsh2016convolutional} (“if the distance between the source and target applications is significant, one may need to fine-tune the early layers as well”), we fine-tune the whole network in this proof of concept experiment.  In this setting the Momentum ResNet leads to faster convergence when fine-tuning, as shown in Figure~\ref{fig:fine_tuning}: Momentum ResNets can be twice as fast as ResNets to train when samples are so big that only few of them can be processed at a time.  In contrast, RevNets \citep{gomez2017reversible} cannot as easily be used for fine-tuning since, as shown in~\eqref{eq:revnet}, they require to train two distinct networks.

\paragraph{Continuous training.} 


We also compare accuracy when using first-order ODE blocks \citep{chen2018neural} and second-order ones on CIFAR-10. In order to emphasize the influence of the ODE, we considered a neural architecture which down-sampled the input to have a certain number of channels, and then applied  successive ODE blocks. Two types of blocks were considered: one corresponded to the first-order ODE~\eqref{eq:first_order_ODE} and the other one to the second-order ODE~\eqref{eq:pertu}. Training was based on the odeint function implemented by \citet{chen2018neural}.
Figure~\ref{fig:continuous_CIFAR_10} shows the final test accuracy for both models as a function of the number of channels used. As a baseline, we also include the final accuracy when there are no ODE blocks. We see that an ODE Net with momentum significantly outperforms an original ODE Net when the number of channels is small. Training took the same time for both models. 
\begin{figure}[ht]
\begin{minipage}[c]{0.6\linewidth}
        \includegraphics[width=\textwidth]{comparison_ODE.pdf}
    \end{minipage}\hfill
    \begin{minipage}[c]{0.35\linewidth}
        \vspace{-1em}
    \caption{\textbf{Accuracy after  iterations on CIFAR-10} with or without momentum, when varying the number of channels.}\label{fig:continuous_CIFAR_10}
    \end{minipage}
    \vspace{-1em}
\end{figure} 









\subsection{Learning to optimize}
\label{sec-numerics-lista}

We conclude by illustrating the usefulness of our Momentum ResNets in the \emph{learning to optimize} setting, where one tries to learn to minimize a function.
We consider the Learned-ISTA (LISTA) framework~\citep{gregor2010learning}. Given a matrix , and a hyper-parameter , the goal is to perform the sparse coding of a vector , by finding  that minimizes the Lasso cost function ~\citep{tibshirani1996regression}.
In other words, we want to compute a mapping .
The ISTA algorithm~\citep{daubechies2004iterative} solves the problem, starting from , by iterating , with  a step-size.
Here,  is the soft-thresholding operator.
The idea of~\citet{gregor2010learning} is to view  iterations of ISTA as the output of a neural network with  layers that iterates , with parameters  and . 
We call  the network function, which maps  to the output .
Importantly, this network can be seen as a residual network, with residual function .
ISTA corresponds to fixed parameters between layers:  and , but these parameters can be learned to yield better performance.
We focus on an ``unsupervised'' learning setting, where we have some training examples , and use them to learn parameters  that quickly minimize the Lasso function . In other words, the parameters  are estimated by minimizing the cost function .
The performance of the network is then measured by computing the testing loss, that is the Lasso loss on some unseen testing examples.

We consider a Momentum ResNet and a RevNet variant of LISTA which use the residual function . For the RevNet, the activations  are first duplicated: the network has twice as many parameters at each layer.
The matrix  is generated with i.i.d. Gaussian entries with  , , and its columns are then normalized to unit variance.
Training and testing samples  are generated as normalized Gaussian i.i.d. entries. More details on the experimental setup are added in Appendix~\ref{app:experiment_details}. 
The next Figure~\ref{fig:lista} shows the test loss of the different methods, when the depth of the networks varies.
\begin{figure}[H]
\centering
\includegraphics[width=0.8\columnwidth]{lista.pdf} 
\caption{\textbf{Evolution of the test loss} for different models as a function of depth in the Learned-ISTA (LISTA) framework.}
\label{fig:lista}
\end{figure}
As predicted by Proposition~\ref{prop:revnet_fix}, the RevNet architecture fails on this task: it cannot have converging iterations, which is exactly what is expected here.
In contrast, the Momentum ResNet works well, and even outperforms the LISTA baseline.
This is not surprising: it is known that momentum can accelerate convergence of first order optimization methods.
\vspace{-1em}
\section*{Conclusion}

This paper introduces Momentum ResNets, new invertible residual neural networks operating with a significantly reduced memory footprint compared to ResNets. In sharp contrast with existing invertible architectures, they are made possible by a simple modification of the ResNet forward rule. This simplicity offers both theoretical advantages (better representation capabilities, tractable analysis of linear dynamics) and practical ones (drop-in replacement, speed and memory improvements for model fine-tuning). Momentum ResNets interpolate between ResNets () and RevNets (), and are a natural second-order extension of neural ODEs. As such, they can capture non-homeomorphic dynamics and converging iterations. As shown in this paper, the latter is not possible with existing invertible residual networks, although crucial in the learning to optimize setting.
\vspace{-2em}
\section*{Acknowledgments}

This work was granted access to the HPC resources of IDRIS under the allocation 2020-[AD011012073] made by GENCI. This work was supported in part by the French government under management of Agence Nationale de la Recherche as part of the “Investissements d’avenir” program, reference ANR19-P3IA-0001 (PRAIRIE 3IA Institute). This work was
supported in part by the European Research Council (ERC project NORIA).  The authors would like to thank David Duvenaud and Dougal Maclaurin for their helpful feedbacks.  M. S. thanks Pierre Rizkallah and Pierre Roussillon for fruitful discussions. 
\bibliography{example_paper}
\bibliographystyle{icml2020}


\onecolumn

\icmltitle{Appendix}
\appendix

In Section~\ref{app:proofs} we give the proofs of all the Propositions and the Theorem. In Section~\ref{app:additional_results} we give other theoretical results to validate statements made in the paper. Section~\ref{app:memory_savings} presents the algorithm from \citet{10.5555/3045118.3045343}.  Section~\ref{app:experiment_details} gives details for the experiments in the paper. We derive the formula for backpropagation in Momentum ResNets in Section~\ref{app:backprop_mom_nets}. Finally, we present additional figures in Section~\ref{app:figures}.

\section{Proofs}\label{app:proofs}

\paragraph{Notations}
\begin{itemize}
    \item  is the set of  infinitely differentiable functions from  to  with value  in .
    \item If  is a function, we denote by , when it exists, the partial derivative of  with respect to .
    \item For a matrix , we denote by  the Jordan block of size  associated to the eigenvalue  .
\end{itemize}




\setcounter{subsection}{-1}


\subsection{Instability of fixed points -- Proof of Proposition~\ref{prop:revnet_fix}}
\begin{proof}
Since  is a fixed point of the RevNet iteration, we have



Then, a first order expansion, writing  and  gives at order one


We therefore obtain at order one

which shows that  is indeed the Jacobian of  at .
We now turn to a study of the spectrum of .
We let  an eigenvalue of , and vectors ,  such that  is the corresponding eigenvector, and study the eigenvalue equation

which gives the two equations



We start by showing that  by contradiction.
Indeed, if , then \eqref{eq:app:eig_eq1} gives , which implies  since  is invertible. Then, \eqref{eq:app:eig_eq2} gives , which also implies . This contradicts the fact that  is an eigenvector (which is non-zero by definition).

Then, the first equation~\eqref{eq:app:eig_eq1} gives , and multiplying \eqref{eq:app:eig_eq2} by  on the left gives


We also cannot have , since it would imply .
Then, dividing \eqref{eq:app:eig_eq3} by  shows that  is an eigenvalue of .

Next, we let  the eigenvalue of  such that .
The equation can be rewritten as the second order equation 


This equation has two solutions , , and since the constant term is , we have . Taking modulus, we get 
, which shows that necessarily, either  or .


Now, the previous reasoning is only a necessary condition on the eigenvalues, but we can now prove the advertised result by going backwards: we let  an eigenvalue of , and  the associated eigenvector. We consider  a solution of  such that  and . Then, we consider . We just have to verify that  is an eigenvector of . By construction, \eqref{eq:app:eig_eq1} holds.
Next, we have 

Leveraging the fact that  is an eigenvector of , we have , and finally:


Which recovers exactly \eqref{eq:app:eig_eq2}:  is indeed an eigenvalue of .
\end{proof}


\subsection{Momentum ResNets in the limit  -- Proof of Proposition~\ref{prop:eps_0}}\label{app:prop_eps_0}

\begin{proof}
We take  without loss of generality.
We are going to use the implicit function theorem. 
Note that  is solution of ~\eqref{eq:pertu} if and only if  is solution of

Consider for  

so that  is solution of ~\eqref{eq:pertu} if and only if  satisfies  
Let .
One has 
 is differentiable everywhere, and at  we have

 is continuous, and it is invertible with continuous inverse because it is linear and continuous, and because  if and only if

which is equivalent to 

which is equivalent, because this equation is linear to .
Using the implicit function theorem, we know that there exists two neighbourhoods  and  of  and  and a continuous function  such that

This in particular ensures that  converges uniformly to  as  goes to 
\end{proof}




\subsection{Momentum ResNets are more general than neural ODEs -- Proof of Proposition~\ref{prop:bigger_set}}
\begin{proof}
If  satisfies~\eqref{eq:first_order_ODE} we get by derivation that

Then, if we define , we get that  is also solution of the second-order model

with 
.
\end{proof} 


\subsection{Solution of~\eqref{eq:second_order_lin} -- Proof of Proposition~\ref{prop:sol_second_order}}

\eqref{eq:second_order_lin} writes



For which the solution at time  writes



The calculation of this exponential gives

Note that it can be checked directly that this expression satisfies~\eqref{eq:second_order_lin} by derivations.
At time  this effectively gives .

\subsection{Representable mappings for a Momentum ResNet with linear residual functions -- Proof of Theorem~\ref{th:representability}}\label{app:th_representability}

In what follows, we denote by  the function of matrices defined by 

Because , we choose to work on . 


We first need to prove that  is surjective on .

\subsubsection{Surjectivity on  of }
\begin{lemma}[Surjectivity of ]\label{lemma:surjectivity}
For ,  is surjective on . 
\end{lemma}

\begin{proof}
Consider

For , we have , and because  is surjective on , it is sufficient to prove that  is surjective on .
Suppose by contradiction that there exists  such that , .
Then  is an entire function  \cite{levin1996lectures} of order 1 with no zeros. Using Hadamard's factorization theorem \cite{conway2012functions}, this implies that there exists  such that , 

However, since  is an even function one has that  

so that , . Necessarily, , which is absurd because  is not constant.\\
\end{proof}

We first prove Theorem~\ref{th:representability} in the diagonalizable case. 
\subsubsection{Theorem~\ref{th:representability} in the diagonalizable case}

\begin{proof}


\textbf{Necessity}
Suppose that  can be represented by a second-order model~\eqref{eq:second_order_lin}. This means that there exists a real matrix  such that 
with  real and

with 

 commutes with  so that there exists  such that  is diagonal and  is triangular. Because , we have that ,  there exists  such that . Because , necessarily, . In addition, . Because  is real, each  must be associated with  in . Thus,  appears in pairs in .
\paragraph{Sufficiency}
Now, suppose that  with ,   is of even multiplicity order. We are going to exhibit a  real such that . Thanks to Lemma~\ref{lemma:surjectivity}, we have that  is surjective. Let .
\begin{itemize}
    \item If  and  or  then there exists  by Lemma~\ref{lemma:surjectivity} such that . 
    \item If  and , then because  is continuous and goes to infinity when  goes to infinity, there exists  such that . 
  
\end{itemize}

In addition, there exist ,  such that 

with ,
and 

with  and .
\\

Let  and  be such that  and . For , one has . Indeed, writing  with , the fact that  implies that . Writing  with , we get that .
Then

is such that , and  is represented by a second-order model~\eqref{eq:second_order_lin}.
\end{proof} 

We now state and demonstrate the general version of Theorem~\ref{th:representability}.

First, we need to demonstrate properties of the complex derivatives of the entire function .

\subsubsection{The entire function  has a derivative with no-zeros on .}

\begin{lemma}[On the zeros of ]\label{lemma:zeros_derivative}
 we have .
\end{lemma}

\begin{proof}
One has 
so that  and it is sufficient to prove that the zeros of  are all real. 

We first show that  belongs to the Laguerre-Pólya class \cite{craven2002iterated}. The Laguerre-Pólya class is the set of entire functions that are the uniform limits on compact sets of  of polynomials with only real zeros. To show that  belongs to the Laguerre-Pólya class, it is sufficient to show \citep[p. 22]{dryanov1999approximation} that:
\begin{itemize}
    \item The zeros of  are all real.
    \item If  denotes the sequence of real zeros of , one has .
    \item  is of order .
\end{itemize}
First, the zeros of  are all real, as demonstrated in \citet{runckel1969zeros}. Second, if  denotes the sequence of real zeros of , one has  as , so that . Third,  is of order .
Thus, we have that  is indeed in the Laguerre-Pólya class.

This class being stable under differentiation, we get that  also belongs to the Laguerre-Pólya class. So that the roots of  are all real, and hence those of  as well.

\end{proof}
\subsubsection{Theorem~\ref{th:representability} in the general case}

When , we have in the general case the following from \citet{culver1966existence}:

Let . Then  can be represented by a first-order model~\eqref{eq:first_order_lin} \textbf{if and only if}  is not singular and each Jordan block of  corresponding to an eigen value   occurs an even number of time. 

We now state and demonstrate the equivalent of this result for second order models~\eqref{eq:second_order_lin}.

\begin{theorem}[Representable mappings for a Momentum ResNet with linear residual functions -- General case]

Let .


If  can be represented by a second-order model~\eqref{eq:second_order_lin}, then each Jordan block of A corresponding to an eigen value   occurs an even number of time.

Reciprocally, if each Jordan block of A corresponding to an eigen value   occurs an even number of time, then  can be represented by a second-order model. 
\end{theorem}
\begin{proof}


We refer to the arguments from \citet{culver1966existence} and use results from \citet{gant} for the proof. 

Suppose that  can be represented by a second-order model~\eqref{eq:second_order_lin}. This means that there exists  such that . The fact that  is real implies that its Jordan blocks are:


Let  be an eigenvalue of  such that . Necessarily, , and  thanks to Lemma~\ref{lemma:zeros_derivative}. We then use Theroem 9 from \citet{gant} (p. 158) to get that the Jordan blocks of  corresponding to   are 

 Since  , we can conclude that the Jordan blocks of A corresponding  occur an even number of time.

Now, suppose that each Jordan block of  corresponding to an eigen value 

occurs an even number of times.
Let  be an eigenvalue of .
\begin{itemize}
    \item If   we can write, because  is surjective (proved in Lemma~\ref{lemma:surjectivity}),  with . Necessarily, because  is real, the Jordan blocks of  corresponding to  have to be associated to those corresponding to . In addition, thanks to Lemma~\ref{lemma:zeros_derivative},  
    \item If , we can write, because  is surjective,  with . In addition, .
    \item If , then there exists  such that  and  because, if  is such that , we have that  on .
     \item If , there exists  such that . Necessarily,  but .
\end{itemize}

This shows that the Jordan blocks of  are necessarily of the form 



Let  be such that its Jordan blocks are of the form 



 Then again by the use of Theorem 7 from \citet{gant} (p. 158), because if  with , , we have that  is similar to . 
 Thus  writes  with . 
 Then,  satisfies  and .
\end{proof}

\section{Additional theoretical results}\label{app:additional_results}

\subsection{On the convergence of the solution of a second order model when }\label{app:prop_eps_infty}
\begin{proposition}[Convergence of the solution when ]\label{prop:eps_infty}

We let  (resp. ) be the solution of  (resp. ) on , with initial conditions  and .
Then  converges uniformly to  as . 
\end{proposition}

\begin{proof}
The equation  with ,  writes in phase space 


It then follows from the Cauchy-Lipschitz Theorem with parameters \citep[Theorem 2, Chapter 2]{perko2013differential} that the solutions of this system are continuous in the parameter . That is  converges uniformly to  as .

\end{proof}

\subsection{Universality of Momentum ResNets}\label{app:learn_init}

\begin{proposition}[When  is free any mapping can be represented]\label{learn_init_speed}

 Consider , and the ODE
 
 Then .
\end{proposition}

\begin{proof}
This is because the solution is . 
\end{proof}


\subsection{Non-universality of Momentum ResNets when }


\begin{proposition}[When  there are mappings that cannot be learned if the equation is autonomous.]\label{proposition:limits_init_0}

When , consider the autonomous ODE 
If there exists  such that   and  then  cannot be represented by~\eqref{eq:limits_init_0}.
\end{proposition}
This in particular proves that  for  cannot be represented by this ODE with initial conditions . 


 \begin{proof}
 Consider such an  and . Since , that  and that  is continuous, we know that there exists  such that .
We denote , solution of 


Since , one can write  as a derivative: . 
The energy  satisfies: 

So that 

In other words: 

So that 
We now apply the exact same argument to the solution starting at . Since  there exists  such that . So that:

So that .
We get that 

This implies that  on , so that the first solution is constant and  which is absurd because .
\end{proof}
\subsection{When  there are mappings that can be represented by a second-order model but not by a first-order one.}\label{app:prop_lambda}

\begin{proposition}\label{prop:lambda}
There exits  such that the solution of 

with initial condition 
at time  is

\end{proposition}

\begin{proof}
Consider the ODE

with initial condition 
The solution of this ODE is 

which at time 1 gives:

\end{proof}

\subsection{Orientation preservation of first-order ODEs}\label{app:prop_connected}

\begin{proposition}[The homeomorphisms represented by~\eqref{eq:first_order_ODE} are orientation preserving.]\label{prop:connexe}
If  is a compact set and  is a homeomorphism represented by~\eqref{eq:first_order_ODE}, then  is in the connected component of the identity function on  for the  topology.
\end{proposition}
We first prove the following: 
\begin{lemma}\label{lemma:compactness}
Consider  a compact set. Suppose that ,  is defined for all .
Then 

is compact as well.
\end{lemma}
\begin{proof}
We consider  a sequence in . Since  is compact, we can extract sub sequences ,  that converge respectively to  and . We denote them  and  again for simplicity of the notations.
We have that:

Thanks to Gronwall's lemma, we have

where  is 's Lipschitz constant. So that  as .
In addition, it is obvious that  as .
We conclude that

so that  is compact.
\end{proof}

\begin{proof}
Let's denote by  the set of homeomorphisms defined on . The application 

defined by

is continuous. Indeed, we have for any  in  that 

where  bounds the continuous function  on  defined in lemma~\ref{lemma:compactness}.
Since  does not depend on , we have that 

as , which proves that  is continuous.
Since , we get that ,  is connected to .
\end{proof}



\subsection{On the linear mappings represented by autonomous first order ODEs in dimension }

Consider the autonomous ODE


\begin{theorem}[Linearity]\label{theo1}
Suppose . If~\eqref{eq:autonomous} represents a linear mapping  at time , we have that  is linear.
\end{theorem}
\begin{proof}
If , consider some . Since , there exists, by Rolle's Theorem a  such that . Then . But since the constant solution  then solves , we get by the unicity of the solutions that . So that . Since this is true for all , we get that . We now consider the case where  and .
Consider some . If , then the solution constant to  solves~\eqref{theo1}, and thus cannot reach  at time  because . Thus,  if . Second, if the trajectory starting at  crosses  and , then by the same argument we know that , which is absurd. So that, , ,  .
We can thus rewrite~\eqref{theo1} as 

Consider  a primitive of .
Integrating~\eqref{inver}, we get

In other words, :

We derive this equation and get:

This proves that .
We now suppose that .
We also have that 

But when ,  so that 

and  is linear.
The case  treats similarly by changing  to .
\end{proof}




\subsection{There are mappings that are connected to the identity that cannot be represented by a first order autonomous ODE}

In bigger dimension, we can exhibit a matrix in  (and hence connected to the identity) that cannot be represented by the autonomous ODE~\eqref{eq:autonomous}.
\begin{proposition}[A non-representable matrix]\label{prop:contre_ex}
Consider the matrix 
 where  and .
Then  and  cannot be represented by~\eqref{eq:autonomous}.
\end{proposition}
\begin{proof}
The fact that  is because  has two single negative eigenvalues, and because .
We consider the point . At time , it has to be in . Because the trajectory are continuous, there exists  such that the trajectory is at  at time , and thus at  at time , and again at  at time . However, the particle is at  at time . All of this is true because the equation is autonomous. Now, we showed that trajectories starting at  and  would intersect at time  at , which is absurd. Figure~\ref{fig:contre_ex} illustrates the paradox.
\end{proof}

\begin{figure}[ht]
\begin{center}
\centerline{\includegraphics[width=0.6\columnwidth]{contre_ex_lin.pdf}}
\caption{Illustration of Proposition~\ref{prop:contre_ex}. The points starting at  and  are distinct but their associated trajectories would have to intersect in , which is impossible.}
\label{fig:contre_ex}
\end{center}
\end{figure}







\section{Exact multiplication}\label{app:memory_savings}

\begin{algorithm}
   \caption{Exactly reversible multiplication by a ratio, from \citet{10.5555/3045118.3045343}}
   \label{alg:reversible-mult}
\begin{algorithmic}[1]
   \STATE {\bfseries Input:} Information buffer , value , ratio   
   \STATE  
   \STATE   \label{step:f2}
   \STATE                 \label{step:f3}
   \STATE          \label{step:b1}
   \STATE          \label{step:b2}
   \STATE           \label{step:b3}
   \STATE \textbf{return} updated buffer , updated value 
\end{algorithmic}
\end{algorithm}


We here present the algorithm from~\citet{10.5555/3045118.3045343}. In their paper, the authors represent  as a rational number, . The information is lost during the integer division of  by  in~\eqref{eq:Momentum ResNet}. The store this information, it is sufficient to store the remainder  of this integer division.  is stored in an “information buffer” . To update , one has to left-shift the bits in  by multiplying it by  before adding . The entire procedure is illustrated in Algorithm~\ref{alg:reversible-mult} from~\citet{10.5555/3045118.3045343}.
 






















\section{Experiment details}\label{app:experiment_details}

In all our image experiments, we use Nvidia Tesla V100 GPUs.
\\
\\
For our experiments on CIFAR-10 and 100, we used a batch-size of  and we employed SGD with a momentum of . The training was done over  epochs. The initial learning rate was  and was decayed by a factor  at epoch . A constant weight decay was set to . Standard inputs preprocessing as proposed in Pytorch \citep{paszke2017automatic} was performed.  
\\
\\
For our experiments on ImageNet, we used a batch-size of  and we employed SGD with a momentum of . The training was done over  epochs. The initial learning rate was  and was decayed by a factor  every  epochs. A constant weight decay was set to . Standard inputs preprocessing as proposed in Pytorch \citep{paszke2017automatic} was performed: normalization, random croping of size  pixels, random horizontal flip. 
\\
\\
For our experiments in the continuous framework, we adapted the code made available by \citet{chen2018neural} to work on the CIFAR-10 data set and to solve second order ODEs. We used a batch-size of , and used SGD with a momentum of . The initial learning rate was set to  and reduced by a factor  at iteration . The training was done over  epochs. 
\\
\\
For the learning to optimize experiment, we generate a random Gaussian matrix  of size . The columns are then normalized to unit variance.
We train the networks by stochastic gradient descent for  iterations, with a batch-size of  and a learning rate of .
The samples  are generated as follows:
we first sample a random Gaussian vector , and then we use , which ensures that every sample verify . This way, we know that the solution  is zero if and only if . The regularization is set to .
\section{Backpropagation for Momentum ResNets}\label{app:backprop_mom_nets}

In order to backpropagate the gradient of some loss in a Momentum ResNet, we need to formulate an explicit version of~\eqref{eq:Momentum ResNet}.
Indeed,~\eqref{eq:Momentum ResNet} writes explicitly 

Writing , the backpropagation for Momentum ResNets then writes, for some loss  


We implement these formula to obtain a custom Jacobian-vector product in Pytorch.

\section{Additional figures}\label{app:figures}

\subsection{Learning curves on CIFAR-10}

We here show the learning curves when training a ResNet-101 and a Momentum ResNet-101 on CIFAR-10.

\begin{figure}[H]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{CIFAR_10_Comparison.pdf}}
\caption{Test error and test loss as a function of depth on CIFAR-10 with a ResNet-101 and two Momentum ResNets-101.}
\end{center}
\vskip -0.2in
\end{figure}


\end{document}
