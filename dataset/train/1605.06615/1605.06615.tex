\documentclass[preprint,12pt]{elsarticle}
\usepackage{lineno,subfigure}
\usepackage{color}


\newcommand{\Oh}[1]{\ensuremath{\mathcal{O}\!\left({#1}\right)}}
\newcommand{\depth}{\ensuremath{\mathsf{depth}}}
\newcommand{\nodes}{\ensuremath{\mathsf{nodes}}}
\newcommand{\leaves}{\ensuremath{\mathsf{leaves}}}
\newcommand{\wmm}{\texttt{WMM}}	
\newcommand{\perm}{\texttt{PERM}}	
\newcommand{\tablen}{\texttt{TABLE}}	
\newcommand{\tables}{\texttt{TABLE}}	
\newcommand{\tablee}{\texttt{TABLE}}

\newcommand{\B}{\mathcal{B}}
\renewcommand{\log}{\lg}

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}

\begin{document}

\begin{frontmatter}

\title{Efficient and Compact Representations of Some Non-Canonical Prefix-Free Codes}

\author[udc]{Antonio Fari\~na}
\author[udp,cebib]{Travis Gagie\corref{cor}}
\author[lodz]{Szymon Grabowski}
\author[upo,cnr]{\mbox{Giovanni Manzini}}
\author[cebib,imfd,chile]{Gonzalo Navarro}
\author[ebay]{Alberto Ord\'o\~nez}

\cortext[cor]{Corresponding author: {\tt travis.gagie@dal.ca}; Faculty of 
Computer Science, Dalhousie University, 6050 University Avenue, PO BOX 15000,
Halifax, Nova Scotia B3H 4R2, Canada.}

\address[udc]{Universidade da Coru\~na and Centro de Investigaci\'on CITIC, A Coru\~na, Spain}
\address[udp]{Dalhousie University, Canada}
\address[cebib]{Center for Biotechnology and Bioengineering (CeBiB), Chile}
\address[lodz]{Institute of Applied Computer Science, Lodz University of Technology, Poland}
\address[upo]{Department of Computer Science, University of Pisa, Italy}
\address[cnr]{IIT-CNR, Pisa, Italy}
\address[imfd]{Millennium Institute for Foundational Research on Data (IMFD), Chile}
\address[chile]{Department of Computer Science, University of Chile, Chile}
\address[ebay]{Pinterest Inc., CA, USA}

\begin{abstract}
For many kinds of prefix-free codes there are efficient and compact alternatives to the traditional tree-based representation.  Since these put the codes into canonical form, however, they can only be used when we can choose the order in which codewords are assigned to symbols.  In this paper we first show how, given a probability distribution over an alphabet of  symbols, we can store an optimal alphabetic prefix-free code in  bits such that we can encode and decode any codeword of length  in  time, where  is the maximum codeword length.  With  further bits, for any constant , we can encode and decode  time.  We then show how to store a nearly optimal alphabetic prefix-free code in  bits such that we can encode and decode in constant time.  We also consider a kind of optimal prefix-free code introduced recently where the codewords' lengths are non-decreasing if arranged in lexicographic order of their reverses.  We reduce their storage space to  while maintaining encoding and decoding times in . We also show how, with  further bits, we can encode and decode in constant time.
All of our results hold in the word-RAM model.
\end{abstract}

\begin{keyword}
compact data structures \sep prefix-free codes \sep alphabetic codes \sep wavelet matrix
\MSC[2010] 68P05 \sep 68P30 \sep 94A45
\end{keyword}

\end{frontmatter}



\section{Introduction}
\label{sec:introduction}

Prefix-free codes are a fundamental tool in data compression;
they are used in one form or another in almost every compression tool. 
Prefix-free codes allow assigning variable-length codewords to symbols 
according to their probabilities in a way that the encoded stream can be
decoded unambiguously \cite[Ch.\ 5]{CT06}. Their best-known representative, 
Huffman codes \cite{Huf52}, yield the optimal encoded file size for a given 
probability distribution. Fast encoding and decoding algorithms for prefix-free 
codes are then of utmost relevance. When the source alphabet is large (e.g., in
word-based natural language compression \cite{Mof89,ZMNBY00}, East Asian or
numeric alphabets) or when the text is short compared to the alphabet (e.g.,
for compression boosting \cite{FGMS05} or adaptive compression \cite{BFNP07}),
a second concern is the space spent in storing the codewords of all the source 
symbols, because it could outweigh the compression savings.

The classical encoding and decoding algorithms for a codeword of length  take in the word-RAM model  and  time, respectively, using 
bits of space, where  is the size of the source alphabet and  is
the maximum codeword length. For encoding we just store each codeword in plain 
form, whereas for decoding we use a binary tree  where each leaf corresponds
to a symbol and the path from the root to the leaf spells out its code, if we 
interpret going left as a  and going right as a . Faster decoding is 
possible if we use the so-called canonical codes, where the leaves are sorted 
left-to-right by depth, and by symbol upon ties \cite{SK64}. Canonical codes 
enable -time encoding and decoding while using 
 bits of space, again in the word-RAM model. 
In theory, both encoding and decoding 
can be done even in constant time with canonical codes \cite{GNNO15}.

Both the original and the canonical Huffman codes achieve optimality by 
reordering the leaves as necessary. There are applications for which the 
codes must be so-called alphabetic, that is, the leaves must respect, 
left-to-right, the alphabetic order of the source symbols. This allows
lexicographically comparing strings directly in compressed form, which enables
lexicographic data structures on the compressed strings \cite{BNO12,MPBCCN15}
and compressed data structures that represent point sets as sequences of 
coordinates \cite{Nav14}. Optimal alphabetic (prefix-free) codes achieve 
codeword lengths close to those of Huffman 
codes \cite{HT71}. Interestingly, since the mapping between symbols and leaves 
is fixed, alphabetic codes need only store the topology of the binary tree 
 used for decoding, which can be represented more succinctly than optimal 
prefix-free codes, in  bits \cite{MR01}, so that encoding and 
decoding can still be done in time  \cite{GNNO15}. As far as we 
know, there are no equivalents to the fast and compact representations of 
canonical codes for alphabetic codes.

There are other cases where canonical prefix-free codes cannot be used.
Wavelet matrices, for example, serve as compressed representations of discrete 
grids and sequences over large alphabets \cite{CNO15}. They are compressed with
an optimal prefix-free code where the codewords' lengths are non-decreasing if 
arranged in lexicographic order of their {\em reverses}. They 
represent the code in  bits, and encode and decode a codeword 
of length  in time . 

\paragraph{Our contributions}

In Section~\ref{sec:alphabetic} we show how, given a probability distribution,
we can store an optimal alphabetic prefix-free code in  
bits such that we can encode and decode any codeword of length  in 
 time. This time decreases to  
if we use  additional bits, for any constant .
We then show in Section~\ref{sec:alphabetic2}
how to store a nearly optimal alphabetic prefix-free code in 
 bits such that we can encode and decode in constant time.  
These, and all of our results, hold in the word-RAM model.

In Section~\ref{sec:matrices} we consider the optimal prefix-free codes used
for wavelet matrices \cite{CNO15}. We show how to store such a code in 
 bits and still encode and decode any symbol in 
 time. We also show that, using  further bits, 
we can encode and decode in constant time.
Our first variant is simple enough to be implementable. Our
experiments show that on large alphabets it uses 20--30 times less space 
than a classical implementation, at the price of being 10--20 times slower at 
encoding and 10--30 at decoding.

An early version of this paper appeared in {\em Proc. SPIRE 2016} 
\cite{SPIRE16}. This extended version includes much more detailed 
explanations as well as new results for fast encoding and decoding of
optimal alphabetic codes (Section~\ref{sec:alphabetic}).

\section{Basic Concepts}

\subsection{Assumptions}
\label{sec:assumptions}

Our results hold in the word-RAM model, where the computer word has  bits 
and all the basic arithmetic and logical operations can be carried out in 
constant time. We assume for simplicity that the maximum codeword length is 
, so that any codeword can be accessed in  time. We assume 
binary codewords, which are the most popular because they provide the best 
compression, though our results generalize to larger alphabets.

We generally express the space in bits, but when we say  space, we
mean  words of space, that is,  bits. 

By  we denote the logarithm to the base  by default.

\subsection{Basic data structures}
\label{sec:basics}

\paragraph{Predecessors}
This predecessor problem consists in building a data structure on the integers
 such that later, given an integer ,
we return the largest  such that . In the RAM model, with
, it can be 
solved with structures using  bits in  time, as 
well as in  time, among other tradeoffs \cite{PT06}. It is 
also possible to find the answer in time  using exponential 
search.

\paragraph{Bitmaps}
A bitmap  is an array of  bits that supports two operations:
 counts the number of bits  in , and 
 gives the position of the th  in  (we use  by
default). Both operations can be supported in constant time if we store  
bits on top of the  bits used for  itself \cite{Cla96,Mun96}. When  
has  s and  or , it can be represented
in compressed form, using  bits in total for any 
, so that  and  are supported in time  \cite{Pat08}.
All these results require the RAM model of computation with .

\paragraph{Variable-length arrays} An array storing  nonempty strings of 
lengths   can be stored by concatenating the strings and 
adding a bitmap of the same
length of the concatenation, .
We can then determine in constant time that
the th string lies between positions  and  
in the concatenated sequence.

\paragraph{Wavelet trees}
A wavelet tree \cite{GGV03} is a binary tree used to represent a sequence
, which efficiently supports the queries  (the symbol
),  (the number of symbols  in ), and 
 (the position of the th occurrence of symbol  in ).
In this paper we use a wavelet tree variant \cite{BN12} that
uses  bits, where the alphabet of  is
, and supports the three operations in time 
.


\subsection{Prefix-free codes}
\label{sec:prefix}

A {\em prefix-free code} (or instantaneous code) is a mapping from a {\em 
source alphabet}, of size , to a sequence of bits, so that each source 
symbol is assigned a {\em codeword} in a way that no codeword is a prefix of 
any other. A sequence of source symbols is then encoded as a sequence of bits 
by replacing each source symbol by its codeword. Compression can be obtained 
by assigning shorter codewords to more frequent symbols \cite[Ch.\ 5]{CT06}.
When the code is prefix-free, we can unambiguously determine each original 
symbol from the concatenated binary sequence, as soon as the last bit of the
symbol's codeword is read. An {\em optimal} prefix-free code minimizes the 
length of the binary sequence and can be obtained with the Huffman algorithm 
\cite{Huf52}.

For constant-time encoding, we can just store a table of  bits, where 
 is the maximum codeword length, where the codeword of each source symbol is
stored explicitly using standard bit manipulation of computer words
\cite[Sec.\ 3.1]{Nav16}. Since , we have to write only 
words per symbol. Decoding is a bit less trivial. The classical solution for
decoding a prefix-free code is to store a binary tree , where each leaf 
corresponds to a source symbol and each root-to-leaf path spells the codeword 
of the leaf, if we write a  whenever we go left and a  whenever we go 
right. Unless the code is obviously suboptimal, every internal node of  has
two children and thus  has  nodes. Therefore, it can be 
represented in  bits, which also includes the space to 
store the source symbols assigned to the leaves. By traversing  from the 
root and following left or right as we read a  or a , respectively, we 
arrive in  time at the leaf storing the symbol that is encoded with 
 bits in the binary sequence. 

Since , the above classical solution takes 
 bits of space. We can reduce the space to 
 bits by deleting the encoding table and adding instead
parent pointers to , so that from any leaf we can extract the
corresponding codeword in reverse order. Both encoding and decoding take
 time in this case.

Figure~\ref{fig:huffman} shows an example of Huffman coding.

\begin{figure}[t]
\begin{center}
\includegraphics[width=0.8\textwidth]{figures/0huffman.pdf}
\end{center}
\vspace*{-5mm}
\caption{An example of Huffman coding. A sequence of symbols on top, the 
symbol frequencies on the left, the Huffman tree  in the center, and
the corresponding codewords on the right. The blue numbers on the tree nodes
show the total frequencies in the subtrees. The sequence uses 
bits in plain form, but  bits in Huffman-compressed form.}
\label{fig:huffman}
\end{figure}

\subsection{Canonical prefix-free codes}

By the Kraft Inequality~\cite{Kra49}, we can put any prefix-free code into 
{\em canonical form} \cite{SK64} while maintaining all the codeword lengths.
In the canonical form, the leaves of lower depth are always to the left of 
leaves of higher depth, and leaves of the same depth respect the lexicographic 
order of the source symbols, left to right. 

Canonical codes enable faster encoding and decoding, and/or lower space usage.
Moffat and Turpin \cite{MT97} give practical data structures that can encode
and decode a codeword of  bits in time . Apart from the
 bits they use to store the symbols at the leaves, they 
need  bits for encoding and decoding; they do not store the
binary tree  explicitly. They use the  bits
to map from a symbol  to its left-to-right leaf position  
and back. Given the increasing positions and codewords of the leftmost leaves
of each length, they find the codeword of a given leaf position  by finding 
the predecessor position  of , and adding  to the codeword of
, interpreted as a binary number. For decoding, they extend all those 
first codewords of each length to length , by padding them with s on 
their right. Then, interpreting the first  bits of the encoded stream
as a number , they find the predecessor  of  among the
padded codewords, corresponding to leaf position . The leaf position of the
encoded source symbol is then , where  is the 
depth of the leaf . This is also used to advance by  bits in the 
encoded sequence. The time  is obtained with exponential 
search (binary search would yield ); the other predecessor time complexities also hold.  

Figure~\ref{fig:canonical} continues our example with a canonical Huffman code.

\begin{figure}[t]
\begin{center}
\includegraphics[width=0.8\textwidth]{figures/1canhuffman.pdf}
\end{center}
\vspace*{-5mm}
\caption{The canonical code corresponding to Figure~\ref{fig:huffman}. To
encode a symbol, the table  gives its leaf rank , whose predecessor 
we find in the ranks of table , together with its length . We then add
 to the codeword associated with . To decode , a predecessor 
search for  on the padded codewords of  finds . Its associated 
length  and leaf position  are in . We use them 
to obtain the entry in  storing the symbol.}
\label{fig:canonical}
\end{figure}


Gagie et al.~\cite{GNNO15} improve upon this scheme both in space and time,
by using more sophisticated data structures. They show that, using 
 bits of space,
constant-time encoding and decoding is possible.

\subsection{Alphabetic codes}
\label{sec:defalpha}

A prefix-free code is {\em alphabetic} if the codewords (regarded as binary 
strings) maintain the lexicographic order of the corresponding source symbols. 
If we build the binary tree  of such a code, the leaves enumerate the source
symbols in order, left to right. Hu and Tucker \cite{HT71} showed how to build 
an optimal alphabetic code, whose codewords are at most one bit longer than the
optimal prefix-free codes on average \cite{CT06}.

Figure~\ref{fig:alpha} gives an alphabetic code tree for our running example.

\begin{figure}[t]
\begin{center}
\includegraphics[width=0.8\textwidth]{figures/2hutucker.pdf}
\end{center}
\vspace*{-5mm}
\caption{An alphabetic code corresponding to the frequencies of 
Figure~\ref{fig:huffman}. The compressed sequence is  bits long.}
\label{fig:alpha}
\end{figure}

In an alphabetic code we do not need to map from symbols to leaf positions,
so the sheer topology of  is sufficient to describe the code.
Such a topology can be described in  bits, in a way that the
tree navigation operations can be simulated in constant time, as well as 
obtaining the left-to-right position of a given leaf and vice versa \cite{MR01}.
With such a representation, we can then simulate the  encoding and
decoding algorithms described in Section~\ref{sec:prefix} \cite{GNNO15}.

On the other hand, there is no such a thing like a canonical alphabetic code,
because the leaf left-to-right order cannot be altered. Indeed, no faster
encoding and decoding algorithms exist for alphabetic codes. Our first 
contribution, in Sections~\ref{sec:alphabetic} and \ref{sec:alphabetic2}, 
is a data structure of
 bits that encodes and decodes in time 
, and even  if we spend 
 further bits, for any constant .
While this increases the space compared to the -bit basic 
structure, we show that  bits of space are sufficient to encode
and decode in constant time, if we let the average codeword length increase
by a factor of  over the optimal.

\subsection{Codes for wavelet matrices}
\label{sec:defwm}

Claude et al.\ \cite{CNO15} showed how to build an optimal prefix-free code 
such that all the codewords of length  come before the prefixes of
length  of longer codewords in the lexicographic order of the reversed
binary strings.
Specifically, they first build a classical Huffman code and then use the 
Kraft Inequality to build another code with the same codeword lengths and 
with the desired property. They store an -bit mapping between 
symbols and their codewords, which allows them to encode and decode 
codewords of length  in time . They use such codes to 
compress wavelet matrices, which are data structures aimed to represent
sequences on large alphabets. Thus, it is worthwhile to devise more space 
economical codeword representations. 

Figure~\ref{fig:wmm} gives a code tree of this type for our running example.

\begin{figure}[t]
\begin{center}
\includegraphics[width=0.8\textwidth]{figures/3codesWT.pdf}
\end{center}
\vspace*{-5mm}
\caption{A code for wavelet matrices corresponding to the frequencies of 
Figure~\ref{fig:huffman}.}
\label{fig:wmm}
\end{figure}

Our second contribution, in Section~\ref{sec:matrices}, is a representation
for these codes that uses  bits, with the same  
encoding and decoding time. With  further bits, for any 
constant , we achieve constant encoding and decoding time.

\section{Optimal Alphabetic Codes}
\label{sec:alphabetic}



In this section we consider how to efficiently store alphabetic (prefix-free)
codes; recall Section~\ref{sec:defalpha}.
We describe a structure called BSD \cite{GHSV07}, and then how 
we use it to build our fast and compact data structures to store optimal 
alphabetic codes. We finally show how to make it faster using more space.

\subsection{Binary Searchable Dictionaries (BSD)}

Gupta et al.~\cite{GHSV07} describe a structure called {\em BSD}, which encodes
 binary strings of length  using a trie that is analogous to the binary 
tree  we described above to store the code (except that here all the strings
have the same length ). Let us say that the identifier of a string is its 
lexicographic position, that is, the left-to-right position of its leaf in the
trie. Their structure supports extraction of the th string (which is 
equivalent to our encoding), and fast computation of the identifier of a given 
string (which is equivalent to our decoding), both in  time.

To achieve this, Gupta et al.\ define a complete binary search tree  on the 
strings with lexicographic order (do not confuse  with the binary trie;
there is one node in  per trie leaf). The complete tree can be stored 
without pointers. Each node  of  represents a string , which is not 
explicitly stored. Instead, it stores a suffix , 
where  is the length of the longest prefix  shares with some , 
over the ancestors  of  in . For the root  of  it holds that
.

For both operations, we descend in  until reaching the desired node.
We start at the root  of , where we know . The invariant is that, as
we descend, we know  for the current node  and  for all of its 
ancestors  in  (which we have traversed).
Further, we keep track of the most recent ancestors  and  from where 
our path went to the left and to the right, respectively, and therefore it 
holds that  if  and  if  \cite{GHSV07}.
Whenever we choose the child  of  to follow, we compute
 by composing , which restores 
the invariant. The procedure ends after  constant-time steps, and
we can do the concatenation that computes  in constant time in the RAM 
model.

To extract the th string, we navigate from the root towards the th node
of . Because  is a complete binary search tree, we know algebraically 
whether the -th node is , or it is to the left or to the right of . 
If it is , we already know , as explained, and we are done. Otherwise, 
we choose the proper child  of  and continue the search.
Finding  from its string  is analogous, except that we compare  with
 numerically (in constant time in the RAM model) to determine whether we
have found  or we must go left or right. Because  is complete, we know
algebraically the identifier  of each node  without need of storing it.

Gupta et al.~\cite{GHSV07} show that, surprisingly, the sum of the lengths of 
all the strings  is bounded by the number of edges in the trie.
Our data structure for optimal alphabetic codes builds on this BSD data 
structure.

\subsection{Our data structure}

Given an optimal alphabetic code over a source alphabet of size  with 
maximum codeword length , we store the lengths of the  codewords 
using  bits, and then pad the codewords on the right
with 0s
up to length .  We divide the lexicographically sorted padded codewords into
blocks of size  (the last block may be smaller). We collect the first padded
codeword of every block in a predecessor data structure, and store all the 
(non-padded) codewords of each block in a BSD data structure, one per block.

The predecessor data structure then stores  numbers
in a universe of size . As seen in Section~\ref{sec:basics}, the structure
uses  bits and
answers predecessor queries in time .

Each BSD structure, on the other hand, stores (at most)  strings .
Unlike the original BSD structure, our codewords are of varying length
(those lengths were stored separately, as indicated). This does not invalidate
the argument that the sum of the strings  adds up to the number of edges in
the trie of the  codewords: what Gupta et al.~\cite[Lem.~3]{GHSV07} show
is that each edge of the trie is mentioned in only one string , with no
reference to the code lengths. 
We vary its encoding, though: We store all the strings  of the BSD, 
in the same order of the nodes of , concatenated in a variable-length array 
as described in Section~\ref{sec:basics}. With constant-time  we find 
where is  in the concatenation, and with another  time we extract
it in the RAM model. 

Considering the extra space needed to find in constant time where is , we 
spend  bits per trie edge. Since the trie stores up to  consecutive
leaves of the whole binary tree  (and internal nodes of  have two
children because the alphabetic code is optimal), it follows that the trie
has  nodes: There are  trie nodes with two children because
there are  leaves in the trie, and the trie nodes with one child are those
leading to the leftmost and rightmost trie leaves. Since the leaves are of 
depth , there are  of those trie nodes too.
Therefore, we use  bits per BSD structure,
adding up to  bits overall. 

The total space is then dominated by the  bits 
spent in
storing the lengths of the codewords. On top of that, the predecessor data 
structure uses  bits and the BSD structures use other 
bits.

To encode symbol , we go to the th BSD structure and
find the th string inside it, with .
The algorithm is identical to that for BSD, except that each  has 
variable length; recall that we have those lengths  stored explicitly. 
We thus update  when moving to node
.

To decode, we store in a number  the first  bits of the stream, find its 
predecessor in our structure, and decode  in the corresponding BSD structure.
The only difference is that, when we compare  with , their lengths
differ (because we do not know the length  of the codeword we seek, which
prefixes ). Since the code is prefix-free, it follows that the codeword we
look for is  if , otherwise we go left or right
according to which is smaller between those -bit numbers. When we find
the proper node , the source symbol is the position  of  (which we
compute algebraically, as explained) and the length of the codeword is 
.

In both cases, the time is  to find the proper node in the BSD
plus, in the case of decoding,  time for the predecessor search.
As before, we can also encode and decode a codeword of length  in time 
 using the basic -bit representation. We can even choose
the smallest by attempting the encoding/decoding up to  steps, and then
switch to the -time procedure if we have not yet finished.

\begin{theorem}
\label{thm:optimal}
Given a probability distribution over an alphabet of  symbols, we 
can build an optimal alphabetic prefix-free code and store it in 
 bits, where  is the maximum codeword length, 
such that we can encode and decode any codeword of length  in 
 time.
The result assumes a -bit RAM computation model with .
\end{theorem}

Figure~\ref{fig:bsd} shows our structure for the codewords tree of
Figure~\ref{fig:wmm}. Note that, for each BSD structure, the length of the
concatenated strings  equals the number of edges in the corresponding
piece of the codewords tree. For example, to encode the symbol {\bf 3}, we
must encode the 4th symbol of . We start at the root  (corresponding
to symbol {\bf 2}), with . We know algebraically that the root
corresponds to the 3rd symbol, so we go right to , the node representing
the symbol {\bf 3}. Since ,  is encoded with respect to the
nearest ancestor where we went right, that is, from the root . We have
 stored explicitly, so we build . Since we know algebraically that we arrived at the 4th symbol, 
we are done: the codeword for {\bf 3} is . Let us now decode .
The predecessor search tells it appears in . We start at the root 
(which encodes {\bf 6}). Since its extended codeword, , is
larger than , we go left to the node  that represents {\bf 5}. Since
,  is represented with respect to the last ancestor where we
went left, that is, . So we compose . Now, since  is larger than our codeword , we 
again
go left to the node  that represents {\bf 4}. Since , 
is also represented with respect to the last node where we went left, that is,
. So we compose .
We have found the code sought, , and we algebraically know that the node
corresponds to the source symbol {\bf 4}.

\begin{figure}[t]
\begin{center}
\includegraphics[width=\textwidth]{figures/4BSDs.pdf}
\end{center}
\vspace*{-5mm}
\caption{Our representation of the code for wavelet matrices of 
Figure~\ref{fig:wmm}. For each BSD structure we only store the
concatenated strings , their bitmap , and the code lengths .
The first codes of each BSD structure are stored in the predecessor structure
on the bottom, padded to  bits.}
\label{fig:bsd}
\end{figure}

\subsection{Faster operations}

In order to reduce the time  to , we 
manage to encode and decode in constant time the codewords of length up to
, for some constant . For the longer codewords,
since , it holds that , and thus
we already process them in time .

For encoding, we store a bitmap , so that  iff the
length of the codeword of the th source symbol is at most . We also 
store a table  so that, if , then  stores 
the codeword of the th source symbol (only  source symbols can have
codewords of length up to ). To encode , we check
. If , then we output the codeword  in constant
time; otherwise we encode  as in Theorem~\ref{thm:optimal}

For decoding, we build a table  where, for any 
, if the binary representation of  is prefixed by the 
codeword of the th codeword, which is of length , then
. Instead, if no codeword prefixes , then .
We then read the next  bits of the stream and extract the first  of those
 bits in a number . If , then we have decoded the symbol
 in constant time and advance in the stream by  bits. Otherwise, we
proceed with the  bits we have read as in Theorem~\ref{thm:optimal}.

The encoding and decoding time is then always bounded by , as 
explained. The space for , , and  is 
 bits, because  and
.

\begin{corollary}
\label{cor:optimal}
Given a probability distribution over an alphabet of  symbols, we can 
build an optimal alphabetic prefix-free code and store it in  bits, where  is the maximum codeword length and  
is any positive constant, such that we can encode and decode any codeword of 
length  in  time.
The result assumes a -bit RAM computation model with .
\end{corollary}

\section{Near-Optimal Alphabetic Codes}
\label{sec:alphabetic2}

Our approach to storing a nearly optimal alphabetic code compactly has two parts: first, we show that we can build such a code so that the expected codeword length is  times the optimal, the codewords tree  has height at most , and each subtree rooted at depth  is completely balanced. Then, we manage to store such a tree in  bits so that encoding and decoding take  time.

\subsection{Balancing the codewords tree}

Evans and Kirkpatrick~\cite{EK04} showed how, given a binary tree on  leaves, we can build a new binary tree of height at most  on the same leaves in the same left-to-right order, such that the depth of each leaf in the new tree is at most 1 greater than its depth in the original tree. We can use their result to restrict the maximum codeword length of an optimal alphabetic code, for an alphabet of  symbols, to be at most , while forcing its expected codeword length to increase by at most a factor of . To do so, we build the tree  for an optimal alphabetic code and then rebuild, according to Evans and Kirkpatrick's construction, each subtree rooted at depth .  The resulting tree, , has height at most  and any leaf whose depth increases was already at depth at least .
Although there are better ways to build a tree  with such a height 
limit \cite{Wes76,Ita76}, our construction is sufficient to obtain an 
expected codeword length for  that is  times the optimal.

Further, let us take  and completely balance each subtree rooted at depth .
The height does not increase and any leaf whose depth increases was already at depth at least , so the expected codeword length increases by at most a factor of

Let  be the resulting tree.  Since the expected codeword length of  is in turn a factor of  larger than that of , the expected codeword length of  is also a factor of  larger than
the optimal. The tree  then describes our suboptimal code.

\subsection{Representing the balanced tree}

To represent , we store a bitmap  in which  if and only if the th left-to-right leaf is:
\begin{itemize}
\item of depth less than , or 
\item the leftmost leaf in a subtree rooted at depth .  
\end{itemize}
Note that each  of  corresponds to a node of  with depth at most
. Since there are
 such nodes,  can be 
represented in compressed form as described in Section~\ref{sec:basics}, using
 bits,
supporting  and  in time . For any constant , the
term  is dominated by the second
component, .

For encoding in constant time we store an array , which explicitly stores the codewords assigned to 
the leaves of  where , in the same order of . That is, if
, then the code assigned to the symbol  is stored at . Since the codewords are of length at most ,  requires 
 bits of space, for any constant . We can
also store the length of the code within the same asymptotic space.

To encode the symbol , we check whether  and, if so, we simply look up the codeword in  as explained.  If , we find the preceding 1 at  with , which marks the leftmost leaf in the subtree rooted at depth  that contains the th leaf in . Since the subtree is completely balanced, we can compute the code for the symbol  in constant time from that of the symbol : The balanced subtree has  leaves, where
, and its height is . Then
the first  codewords are of the same length of the codeword for ,
and the last  have one bit less. Thus, if , the codeword
for  is , of the same length of that of ; otherwise
it is one bit shorter, .

To be able to decode quickly, we store an array  such that, if the -bit binary representation of  is prefixed by the th codeword, then  stores  and the length of that codeword. If, instead, the -bit binary representation of  is the path label to the root of a subtree of  with size more than 1, then  stores the position  in  of the leftmost leaf in that subtree (thus ).  Again,  takes  bits for any constant .

Given a string prefixed by the th codeword, we take the prefix of length  of that string (padding with 0s on the right if necessary), view it as the binary representation of a number , and check .  This either tells us immediately  and the length of the th codeword, or tells us the position  in  of the leftmost leaf in the subtree containing the desired leaf. In the latter case, since the subtree is completely balanced, we can compute  in constant time: We find , ,
and  as done for encoding. We then take the first  bits of the string
(including the prefix we had already read, and padding with a 0 if necessary), and interpret it as the number . Then, if
, it holds . Otherwise, the code is 
one bit shorter and the decoded symbol is .

Figure~\ref{fig:alphabal} shows an example, where we have balanced from level
 instead of level  (which is what the formulas indicate) so that the
tree of Figure~\ref{fig:alpha} undergoes some change. The subtrees starting
at the two children of the root are then balanced and made complete. The array
 gives the codeword of the first leaves of both subtrees and  gives the
position in bitmap  of the codewords of the nodes rooting the balanced
subtrees. To encode {\bf 2}, since it is the 3rd symbol (), we compute
, , , and 
. The complete subtree then has  leaves and its 
height is . The first  leaves are of
depth  like , and the other  are of depth . Since
, our codeword is of length  and is computed as
. Instead, to decode , we truncate it to length ,
obtaining . Since , the code is in the subtree that starts at
 in . We compute , , and  as before. The first 
bits of our code is , which we had to pad with a . Since 
, the code is of length  and
the source symbol is , that is, {\bf 4}.

\begin{figure}[t]
\begin{center}
\includegraphics[width=0.8\textwidth]{figures/5bbal.pdf}
\end{center}
\vspace*{-5mm}
\caption{The alphabetic tree of Figure~\ref{fig:alpha} balanced from
level . The resulting compressed sequence length is now  bits (larger
than a plain code, in this toy example).}
\label{fig:alphabal}
\end{figure}

\begin{theorem}
\label{thm:nearly_optimal}
Given a probability distribution over an alphabet of  symbols, we can build an alphabetic prefix-free code whose expected codeword length is at most a factor of  more than optimal and store it in  bits, for any constant , such that we can encode and decode any symbol in constant time .
\end{theorem}

\section{Efficient Codes for Wavelet Matrices}
\label{sec:matrices}

We now show how to efficiently represent the prefix-free codes for wavelet 
matrices; recall Section~\ref{sec:defwm}. We first describe a representation
based on the wavelet trees of Section~\ref{sec:basics}. This is then used to
design a space-efficient version that encodes and decodes codewords of length 
 in time , and then a larger one that encodes and decodes
in constant time.

\subsection{Using wavelet trees}

Given a code for wavelet matrices, we reassign the codewords of the same length such that the lexicographic order of the reversed codewords of that length is the same as that of their symbols. This preserves the property that the codewords
of some length are numerically smaller than the corresponding prefixes of 
longer codewords in the lexicographic order of their reverses. The positive
aspect of this reassignment is that all the information on the code can be 
represented in  bits as a sequence , 
where  is the depth of the leaf encoding symbol  in the codewords tree 
. We can represent  with a wavelet tree using  bits\footnote{Since ,  because
 is increasing for , thus 
for all  and .}
(Section~\ref{sec:basics}), and then:
\begin{itemize}
\item  is the length  of the codeword of symbol ; 
\item  is the position (in reverse lexicographic order) of the 
leaf representing symbol  among those of codeword length ; and 
\item  is the symbol corresponding to the th codeword 
of length  (in reverse lexicographic order).
\end{itemize}

Those operations take time , because the 
alphabet of  is . Since we assume  
(Section~\ref{sec:assumptions}), this time is .



We are left with two subproblems. For decoding the first symbol encoded in a binary string, we need to find the length  of its codeword and the lexicographic rank  of its reverse among the reversed codewords of that length. With that information we have that the source symbol is . For encoding a symbol , instead, we find the length  of its codeword and the lexicographic rank  of its reverse among the reversed codewords of length . Then we must find the codeword given  and . 

We first present a solution that takes  further bits and works in  time. We then present a solution that takes  further bits, for any constant ,
 and works in less time.

\subsection{A space-efficient representation}

For each depth  between 0 and , let  be the total number of nodes at depth  in  and let  be the number of leaves at depth .  Let  be a node other than the root, let  be 's parent, let  be the lexicographic rank (counting from 1) of 's reversed path label among all the reversed path labels of nodes at 's depth, and let  be defined analogously for . Then note the following facts:
\begin{enumerate}
\item Because  is optimal, every internal node has two children, so half the non-root nodes are left children and half are right children.  
\item Because the reversed path labels of the left children at any depth 
start with a , they are all lexicographically less than the reversed path 
labels of all the right children at the same depth, which start with a .  
\item Because of the ordering properties of these codes, the reversed path labels of all the leaves at any depth are lexicographically less than the reversed path labels of all the internal nodes at that depth.
\end{enumerate}

It then follows that:
\begin{itemize}
\item  is a leaf if and only if ;
\item  is 's left child if and only if ;
\item if  is 's left child then ; and
\item if  is 's right child then .
\end{itemize}
Of course, by rearranging terms we can also compute  in terms of .

We store  and  for  between 0 and , which 
requires  bits. With the formulas above, we can decode the
first codeword, of length , from a binary string as follows:
We start at the root , , and descend in  until we reach the leaf 
 whose path
label is that codeword, and return its depth  and the lexicographic rank 
 of its reverse path label among all the reversed path labels of nodes 
at that depth. We then compute  from  and  as described with the
wavelet tree. Note that these nodes  are
conceptual: we do not represent the nodes explicitly, but we still can compute
 as we descend left or right; we also know when we have reached a 
conceptual leaf. 

For encoding , we obtain as explained, with the wavelet tree, its length 
 and the rank  of its reversed codeword among the reversed
codewords of that length. Then we use the formulas to walk up towards the
root, finding in each step the rank  of the parent  of , and
determining if  is a left or right child of . This yields the 
bits of the codeword of  in reverse order (0 when  is a left child of
 and 1 otherwise), in overall time . This completes our first
solution, which we evaluate experimentally in Section~\ref{sec:exp}.

\begin{theorem}
\label{thm:matrices0}
Consider an optimal prefix-free code in which all the codewords of length 
 come before the prefixes of length  of longer codewords in the 
lexicographic order of the reversed binary strings.
We can store such a code in  bits --- possibly after swapping symbols' codewords of the same length --- where  is the alphabet size and  is the maximum codeword length, so that we can encode and decode any codeword of length  in 
 time.
The result assumes a -bit RAM computation model with .
\end{theorem}

Figure~\ref{fig:wmmeffic} shows our representation for the codewords tree of
Figure~\ref{fig:wmm}. To decode , we start at the root with .
The next bit to decode is a , so we must go right: the node of depth  is
then . The next bit to decode is again
a , so we go right again: the node of depth  is . The last bit to decode is a , so we go left: the node of
depth  is . Now we are at a leaf (because
) whose depth is  and its rank is . The 
corresponding symbol is then , that is, symbol {\bf 7}. 
Instead, to encode {\bf 3}, the symbol number , we compute its codeword 
length  and its rank . Our leaf then corresponds 
to , and we discover the code in reverse order by waking upwards to the 
root. Since , we are a left child (so the codeword ends
with a ) and our parent has . Since , this node is a right child (so the codeword ends with )
and its parent has . Finally, the new
node is a left child because , and therefore the
codeword is .

\begin{figure}[t]
\begin{center}
\includegraphics[width=0.8\textwidth]{figures/6odesWMeffic.pdf}
\end{center}
\vspace*{-5mm}
\caption{Our representation for the tree of Figure~\ref{fig:wmm}. We only
store the sequence  and the values  and  at each level.
For each node  we show its  value.}
\label{fig:wmmeffic}
\end{figure}

Figure~\ref{fig:wmmeffic2} shows another example with a sequence producing
a less regular tree.
Consider decoding . We start at the root with .
The first bit to decode is a , so we go right and obtain 
. The next bit is also
a , so we go right again and get . The third bit to decode is also a , so we 
go right again to get  
(that is, the th node of level , minus the leaf with code ,
shifted by all the  nodes of level  that descend by a  
and thus precede our node). Finally, the next bit is a , so we
go left, to node  (that is, the th 
node of level  minus the  leaves of that level). Now we are 
at a leaf because . We leave to the reader 
finding the corresponding symbol {\bf 5} in , as done for the
previous example, as well as working out the decoding of the same 
symbol.

\begin{figure}[t]
\begin{center}
\includegraphics[width=0.8\textwidth]{figures/7codesEx2.pdf}
\end{center}
\vspace*{-5mm}
\caption{The representation of a less regular code, with the same notation
of Figure~\ref{fig:wmmeffic}, produced for the sequence ``''.}
\label{fig:wmmeffic2}
\end{figure}

\subsection{Faster and larger}

We now show how to speed up the preceding procedure so that we 
can perform  steps on the tree in constant time, for some given .
From the formulas that relate  and  it is apparent that, given a node
 and the following  bits to decode, the node  we will arrive at 
depends only on the  and  values at the depths
. More precisely, the value  
is  plus a number that depends only on the involved 
depths and the  bits of the codeword to decode. Similarly, given ,
the last  bits leading to it, and the rank  of the ancestor  of
 at distance , depend on the same values of  and .

Let us first consider encoding a source symbol. We obtain its codeword length
 and rank  from the wavelet tree, and then extract the codeword.
Consider all the path labels of a particular length that end with a particular 
suffix of length : the lexicographic ranks of their reverses are consecutive,
forming an interval. We can then partition the nodes at any depth  by 
those intervals of rank values. 

Let  be a node at depth ,  be its ancestor at distance , and  
and  be the rank values of  and , respectively. As per the previous
paragraph, the partition interval where  lies determines the last  bits
of 's path label, and it also determines the difference between  and 
. For example, in level  of Figure~\ref{fig:wmmeffic2} and taking
, the codes of the nodes  with rank  end with , those 
with ranks  end with , those with ranks  end with , 
and those with ranks  end with . The differences  are
 for the termination ,  for ,  for , and  for 
, the same for all the ranks in the same intervals.

We can then compute the codeword of length  in  chunks of  bits
each, by starting at depth  and using the formulas to climb by  
steps at a time until reaching the root (the last chunk may have less than 
bits).

For each depth  having  nodes, we store a bitmap , where
 if  is the first rank of the interval that ends with the same 
 bits (or the same  bits if ). A table 
 then stores those  bits and 
the difference that must be added to each  in that interval to make it
. Across all the depths, the bitmaps  add up to  bits
because  has  nodes.
Further, there are at most  partitions in each depth, so the tables 
 add up to  entries, each using  bits:
 bits of the chunk and  bits to encode , since 
ranks are at most .
In total, we use  bits, which setting 
, for any constant , is 
 because  and . 
We can then encode any symbol in time
, that is, a constant.

For decoding we store a table that stores, for every depth  that is a 
multiple of , and every sequence  of  bits, a cell  with
the value to be added to  in order to become , where  is any
node at depth  and  is the node we reach from  if we 
descend using the  bits of .  This table then has  
entries, each using  bits to encode the value to be added. 
With , the space is  bits and we arrive 
at the desired leaf after  steps (note that our formulas allow 
us identifying leaves). Once we arrive at a leaf at depth , we know the 
codeword length  and the rank , so we use the wavelet tree to 
compute the source symbol in constant time.

The obvious problem with this scheme is that it only works if the length 
of the codeword we find is a multiple of . Otherwise,
in the last step we will try to advance by  bits when the leaf is at less
distance. In this case our computation of  will give an incorrect result.

Note from our formulas that the nodes  at depth  with
 are leaves and the others are internal nodes.
Let  be any node at depth  and  be the bits of a potential
path of length  descending from . If  descends from  by the sequence
 of the first  bits of , then the difference 
 depends only on , , and  (indeed, our table 
stores precisely  at cell ). Therefore, the nodes  that 
become 
leaves at depth  are those with . We can
then descend from node  by a path with  bits  iff ,
with


We then extend our tables in the following way. For every cell  we
now store  values , with , and the associated 
values . Note that , so this sequence 
is nondecreasing. We make it strictly increasing by removing the smaller  
values upon ties. To find out how much we can descend from an internal node 
 at depth  by the  bits , 
we find  such that , and then we can 
descend by  steps (and by  steps if ). To descend by
 steps to the descendant node , we compute .

We find  with a predecessor search on the  values .
One of the predecessor algorithms surveyed in Section~\ref{sec:basics} runs 
in time , which is constant in the RAM model with 
because . Therefore, the encoding time is still 
. The space is now multiplied by  because the
values  and  also fit in  bits, and thus
it is still  bits.

\begin{theorem}
\label{thm:matrices}
Consider an optimal prefix-free code in which all the codewords of length 
 come before the prefixes of length  of longer codewords in the 
lexicographic order of the reversed binary strings.
We can store such a code in  bits --- possibly after swapping symbols' codewords of the same length --- where  is the alphabet size,  is the maximum codeword length, and  is any positive constant, so that we can encode and decode any codeword in constant time.
The result assumes a -bit RAM computation model with .
\end{theorem}


\section{Experiments} \label{sec:exp}

\newcommand{\EsWiki}{\texttt{EsWiki}}
\newcommand{\EsInv}{\texttt{EsInv}}
\newcommand{\Indo}{\texttt{Indo}}
\newcommand{\ESInvD}{\texttt{ESInvD}}
\newcommand{\HH}{\mathcal{H}}
\newcommand{\LL}{\mathcal{L}}

We have run experiments to compare the solution of Theorem~\ref{thm:matrices0}
(referred to as \wmm\ in the sequel, for Wavelet Matrix Model) with the only
previous encoding, that is,
the one used by Claude et al.~\cite{CNO15} (denoted \tablen). Note that
our codes are not canonical, so other solutions \cite{GNNO15} do not apply.

Claude et al.~\cite{CNO15} use for encoding a single table of  bits
storing the code of each symbol, and thus they easily encode in constant time.
For decoding, they have tables separated by codeword length . In each such
table, they store the codewords of that length and the associated symbol,
sorted by codeword. This requires  further bits, and
permits decoding by binary searching the codeword found in the wavelet matrix.
Since there are at most  codewords of length , the binary search
takes time .

For the sequence  used in our \wmm, we use binary Huffman-shaped wavelet
trees with plain bitmaps (i.e., not compressed). The structures for supporting 
/ require \% extra space, so the total space is 
, where  is the per-symbol 
zero-order entropy of the sequence . We
also add a small index to speed up select queries \cite{NPsea12.1} (at
decoding), which is parameterized with a sampling value that we set to
. Finally, we store the values  and ,
which add an insignificant  bits in total.

We used a prefix of three datasets in \texttt{http://lbd.udc.es/research/ECRPC}.
The first one, \EsWiki, contains a sequence of word
identifiers generated by using the Snowball algorithm to apply stemming to the Spanish Wikipedia.
The second one, \EsInv, contains a concatenation of differentially encoded inverted lists extracted
from a random sample of the Spanish Wikipedia.
The third dataset, \Indo\ was created with the concatenation of the adjacency lists of Web graph
{\tt Indochina-2004}, from {\tt http://law.di.unimi.it/datasets.php}.

Table~\ref{tab:coll0} provides some statistics about the datasets, starting with
the number of symbols in the sequence () and the alphabet size ().
 is the entropy, in bits per symbol, of the frequency distribution 
observed in the sequence. This is close to the average length  of 
encoded and decoded codewords. The last columns show the maximum codeword 
length  and the zero-order entropy of the sequence , , in bits 
per symbol. This is a good approximation to the per-symbol size of our 
wavelet tree for .

\begin{table}[t]
\begin{center}
\resizebox{\textwidth}{!}
{\begin{tabular}{|l||r|r|r|r|r|}
\hline
Collection & ~Length & ~Alphabet    & ~Entropy   & ~max code     & Entropy of level \\
           & ~()~& ~size ()~& ~() & ~length() & ~entries () \\
\hline
\EsWiki	& 200,000,000 & 1,634,145 & 11.12 & 28 & 2.24 \\
\EsInv & 300,000,000 & 1,005,702 & 5.88  & 28 & 2.60 \\
\Indo   & 120,000,000 & 3,715,187 & 16.29 & 27 & 2.51 \\
\hline
\end{tabular}}
\caption{Main statistics of the texts used.}
\label{tab:coll0}
\end{center}
\end{table}


Our test machine has an Intel(R) Core(tm) i7-3820@3.60GHz CPU (4 cores/8 siblings) and 64GB of DDR3 RAM.
It runs Ubuntu Linux 12.04 (Kernel 3.2.0-99-generic). The compiler used was g++ version 4.6.4 and we
set compiler optimization flags to \texttt{-O9}. All our experiments run in a single core and time measures
refer to CPU {\em user-time}. The data to be compressed is streamed from the local disk and also output to disk using the regular buffering mechanism from the OS.

  \begin{figure}[htbp]
  \begin{center}

  \includegraphics[angle=-0,width=0.47\textwidth]{figures/eswiki/codigos.eps}
  \includegraphics[angle=-0,width=0.47\textwidth]{figures/eswiki/symbols.eps}
  \includegraphics[angle=-0,width=0.47\textwidth]{figures/esinv/codigos.eps}
  \includegraphics[angle=-0,width=0.47\textwidth]{figures/esinv/symbols.eps}
  \includegraphics[angle=-0,width=0.47\textwidth]{figures/indo/codigos.eps}
  \includegraphics[angle=-0,width=0.47\textwidth]{figures/indo/symbols.eps}
  \end{center}
  \vspace{-0.3cm}
  \caption{Size of code representations versus compression time (left) and decompression time (right).
  Time is measured in nanoseconds per symbol.}
  \label{fig:exp.enc.dec}

  \end{figure}

Figure~\ref{fig:exp.enc.dec} compares the space required by both
code representations and their compression and decompression times.
As expected, the space per symbol of our new code representation, \wmm, is
close to , whereas that of \tablen\ is close to .
This explains the large difference in space between both representations,
a factor of 23--30 times. For decoding we show the effect of adding the
structure that speeds up select queries.

The price of our representation is the encoding and decoding time. While the
\tablen\ approach encodes using a single table access, in 9--18 nanoseconds,
our representation needs 130--230, which is 10--21 times slower. For
decoding, the binary search performed by \tablen\ takes 20--45 nanoseconds,
whereas our \wmm\ representation requires 500--700 in the slowest and smallest
variant (i.e., 11--30 times slower). Our faster variants require 300--500
nanoseconds, which is still 6.5--27 times slower.

\section{Conclusions}

A classical prefix-free code representation uses  bits, where
 is the source alphabet size and  the maximum codeword length, and
encodes in constant time and decodes a codeword of length  in time
.
Canonical prefix codes can be represented in  bits, so that
one can encode and decode in constant time. In this paper we have considered
two families of codes that cannot be put in canonical form. Alphabetic codes
can be represented in  bits, but encoding and decoding
takes time . We showed how to store an optimal alphabetic code in  bits such that encoding and decoding any codeword of length  takes  time. We also showed how to store it in  bits, 
where  is any positive constant, such that encoding and decoding any such codeword takes  time.  We thus answered an open problem from the conference version of this paper \cite{SPIRE16}.  We then gave an approximation that worsens the average code
length by a factor of , but in exchange
requires only  bits and encodes and decodes in constant time. 

We then consider a family of codes where, at any level, the strings leading to 
leaves lexicographically precede the strings leading to internal nodes, if we 
read them upwards.
For those we obtain a representation using  bits and encoding
and decoding in time , and even in constant time 
if we use  further bits, where  is again any positive constant.  We have implemented the simple version of these codes, which
are used for compressing wavelet matrices \cite{CNO15}, and shown that our encodings are
significantly smaller than classical ones in practice (up to 30 times),
albeit also slower (up to 30 times).
We note that in situations when our encodings are small enough to fit in a faster level of the memory hierarchy, they are likely to be also significantly faster than classical ones.

We leave as an open question extending our results to dynamic coding
\cite{Gag04,GKN09,GN09,GN11,GILMN18} and to codes with unequal codeword-symbol
costs \cite{GN09,GL08}. 


\section*{Acknowledgements}

This research was carried out in part at University of A Coru\~na, Spain, while the second author was visiting from the University of Helsinki and the sixth author was a PhD student there. It started at a StringMasters workshop at the Research Center on Information and Communication Technologies (CITIC) of the University of A Coru\~na.  The workshop was funded in part by European Union's Horizon 2020 research and innovation programme under the Marie Sk{\l}odowska-Curie grant agreement No 690941 (project BIRDS).  The authors thank Nieves Brisaboa and Susana Ladra.

The first author was supported by the CITIC research center funded by Xunta de Galicia/FEDER-UE 2014-2020 Program, grant CSI:ED431G 2019/01; by MICIU/FEDER-UE, grant BIZDEVOPSGLOBAL: RTI2018-098309-B-C32; and by Xunta de Galicia/FEDER-UE, ConectaPeme grant GEMA: IN852A 2018/14. 
The second author was supported by Academy of Finland grants 268324 and 250345 (CoECGR), Fondecyt Grant 1-171058, and NSERC grant RGPIN-07185-2020. The fourth author was supported by PRIN grant 2017WR7SHH, and by the INdAM-GNCS Project 2020 {\sl MFAIS-IoT}. The fifth author was supported by Fondecyt Grant 1-200038, Basal Funds FB0001, and ANID -- Millennium Science Initiative Program -- Code
ICN17\_002, Chile. 



\begin{thebibliography}{10}
\expandafter\ifx\csname url\endcsname\relax
  \def\url#1{\texttt{#1}}\fi
\expandafter\ifx\csname urlprefix\endcsname\relax\def\urlprefix{URL }\fi
\expandafter\ifx\csname href\endcsname\relax
  \def\href#1#2{#2} \def\path#1{#1}\fi

\bibitem{SPIRE16}
A.~Fari{\~n}a, T.~Gagie, G.~Manzini, G.~Navarro, A.~{Ord{\'o\~n}ez}, Efficient
  and compact representations of some non-canonical prefix-free codes, in:
  Proc. 23rd International Symposium on String Processing and Information
  Retrieval (SPIRE), 2016, pp. 50--60.

\bibitem{CT06}
T.~Cover, J.~Thomas, Elements of Information Theory, 2nd Edition, Wiley, 2006.

\bibitem{Huf52}
D.~A. Huffman, A method for the construction of minimum-redundancy codes,
  Proceedings of the Institute of Electrical and Radio Engineers 40~(9) (1952)
  1098--1101.

\bibitem{Mof89}
A.~Moffat, Word-based text compression, Software Practice and Experience 19~(2)
  (1989) 185--198.

\bibitem{ZMNBY00}
N.~Ziviani, E.~Moura, G.~Navarro, R.~Baeza-Yates, Compression: A key for
  next-generation text retrieval systems, IEEE Computer 33~(11) (2000) 37--44.

\bibitem{FGMS05}
P.~Ferragina, R.~Giancarlo, G.~Manzini, M.~Sciortino, Boosting textual
  compression in optimal linear time, Journal of the ACM 52~(4) (2005)
  688--713.

\bibitem{BFNP07}
N.~R. Brisaboa, A.~Fari{\~n}a, G.~Navarro, J.~Param{\'a}, Lightweight natural
  language text compression, Information Retrieval 10 (2007) 1--33.

\bibitem{SK64}
E.~S. Schwartz, B.~Kallick, Generating a canonical prefix encoding,
  Communications of the ACM 7 (1964) 166--169.

\bibitem{GNNO15}
T.~Gagie, G.~Navarro, Y.~Nekrich, A.~{Ord{\'o\~n}ez}, Efficient and compact
  representations of prefix codes, IEEE Transactions on Information Theory
  61~(9) (2015) 4999--5011.

\bibitem{BNO12}
N.~Brisaboa, G.~Navarro, A.~Ord{\'o}{\~n}ez, Smaller self-indexes for natural
  language, in: Proc. 19th International Symposium on String Processing and
  Information Retrieval (SPIRE), 2012, pp. 372--378.

\bibitem{MPBCCN15}
M.~A. Mart{\'i}nez-Prieto, N.~Brisaboa, R.~C{\'a}novas, F.~Claude, G.~Navarro,
  Practical compressed string dictionaries, Information Systems 56 (2016)
  73--108.

\bibitem{Nav14}
G.~Navarro, Wavelet trees for all, Journal of Discrete Algorithms 25 (2014)
  2--20.

\bibitem{HT71}
T.~C. Hu, A.~C. Tucker, Optimal computer search trees and variable-length
  alphabetical codes, SIAM Journal of Applied Mathematics 21~(4) (1971)
  514--532.

\bibitem{MR01}
J.~I. Munro, V.~Raman, Succinct representation of balanced parentheses and
  static trees, SIAM Journal of Computing 31~(3) (2001) 762--776.

\bibitem{CNO15}
F.~Claude, G.~Navarro, A.~{Ord{\'o\~n}ez}, The wavelet matrix: An efficient
  wavelet tree for large alphabets, Information Systems 47 (2015) 15--32.

\bibitem{PT06}
M.~P{\u{a}}tra\c{s}cu, M.~Thorup, Time-space trade-offs for predecessor search,
  in: Proc. 38th Annual ACM Symposium on Theory of Computing (STOC), 2006, pp.
  232--240.

\bibitem{Cla96}
D.~R. Clark, Compact {PAT} trees, Ph.D. thesis, University of Waterloo, Canada
  (1996).

\bibitem{Mun96}
J.~I. Munro, Tables, in: Proc. 16th Conference on Foundations of Software
  Technology and Theoretical Computer Science (FSTTCS), 1996, pp. 37--42.

\bibitem{Pat08}
M.~P\v{a}tra\c{s}cu, Succincter, in: Proc. 49th Annual IEEE Symposium on
  Foundations of Computer Science (FOCS), 2008, pp. 305--313.

\bibitem{GGV03}
R.~Grossi, A.~Gupta, J.~S. Vitter, High-order entropy-compressed text indexes,
  in: Proc. 14th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), 2003,
  pp. 841--850.

\bibitem{BN12}
D.~Belazzougui, G.~Navarro, Optimal lower and upper bounds for representing
  sequences, ACM Transactions on Algorithms 11~(4) (2015) article 31.

\bibitem{Nav16}
G.~Navarro, Compact Data Structures -- A practical approach, Cambridge
  University Press, 2016.

\bibitem{Kra49}
L.~G. Kraft, A device for quantizing, grouping, and coding amplitude modulated
  pulses, {M.Sc.} thesis, EE Dept., MIT (1949).

\bibitem{MT97}
A.~Moffat, A.~Turpin, On the implementation of minimum-redundancy prefix codes,
  IEEE Transactions on Communications 45~(10) (1997) 1200--1207.

\bibitem{GHSV07}
A.~Gupta, W.-K. Hon, R.~Shan, J.~S. Vitter, Compressed data structures:
  Dictionaries and data-aware measures, Theoretical Computer Science 387~(3)
  (2007) 313--331.

\bibitem{EK04}
W.~Evans, D.~G. Kirkpatrick, Restructuring ordered binary trees, Journal of
  Algorithms 50 (2004) 168--193.

\bibitem{Wes76}
R.~L. Wessner, Optimal alphabetic search trees with restricted maximal height,
  Information Processing Letters 4 (1976) 90--94.

\bibitem{Ita76}
A.~Itai, Optimal alphabetic trees, {SIAM} Journal of Computing 5 (1976) 9--18.

\bibitem{NPsea12.1}
G.~Navarro, E.~Providel, Fast, small, simple rank/select on bitmaps, in: Proc.
  11th International Symposium on Experimental Algorithms (SEA), 2012, pp.
  295--306.

\bibitem{Gag04}
T.~Gagie, Dynamic {Shannon} coding, in: Proc. 12th Annual European Symposium on
  Algorithms (ESA), 2004, pp. 359--370.

\bibitem{GKN09}
T.~Gagie, M.~Karpinski, Y.~Nekrich, Low-memory adaptive prefix coding, in:
  Proc. 19th Data Compression Conference (DCC), 2009, pp. 13--22.

\bibitem{GN09}
T.~Gagie, Y.~Nekrich, Worst-case optimal adaptive prefix coding, in: Proc. 16th
  International Symposium on Algorithms and Data Structures (WADS), 2009, pp.
  315--326.

\bibitem{GN11}
T.~Gagie, Y.~Nekrich, Tight bounds for online stable sorting, Journal of
  Discrete Algorithms 9~(2) (2011) 176--181.

\bibitem{GILMN18}
M.~J. Golin, J.~Iacono, S.~Langerman, J.~I. Munro, Y.~Nekrich, Dynamic trees
  with almost-optimal access cost, in: Proc. 26th Annual European Symposium on
  Algorithms (ESA), 2018, pp. 38:1--38:14.

\bibitem{GL08}
M.~J. Golin, J.~Li, More efficient algorithms and analyses for unequal letter
  cost prefix-free coding, {IEEE} Transactions on Information Theory 54~(8)
  (2008) 3412--3424.

\end{thebibliography}

\end{document}
