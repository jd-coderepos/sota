        
        \section{Results}
        


        We focus on:
        
        \begin{enumerate}
            \item Evaluating the impact of MURAL's text-text loss by comparing \alignmling and \muralbase, \textbf{especially for under-resourced languages}.
            \item Understanding the impact of training data scale by comparing Alt-Text+MBT to CC12M+EOBT.
            \item Situating our best model, \murallarge, with respect to previous work.
        \end{enumerate}
        \noindent
        We number the rows in our results tables to ease reference in our discussion and across tables.
        
        \textbf{Multi30k and MSCOCO.} Table \ref{tab:results-standard-datasets} compares MURAL and previous results \cite{burns2020eccv, huang2020m3p,zhou2021uc2, jia2021scaling} in both zero-shot and fine-tuned settings.
        
        The additional text-text task used by \muralbase improves zero-shot performance on Czech, a relatively lower-resourced language, by a large margin over \alignmling (4 vs 6), 47.9  64.6,  while nearly matching or somewhat exceeding performance on higher-resource languages. 
        
        Large, noisy pre-training greatly reduces the need for fine-tuning. M3P sees huge performance gains by fine-tuning\footnote{Fine-tuned on Multi30k and MSCOCO combined, trained for 40k steps and learning rate sweeping of 1e-5, 5e-5, and 1e-4. Other hyperparameters are kept the same.} (1 vs 10), sometimes 3x the zero-shot performance. Both \alignmling and \muralbase see large gains, but their zero-shot performance is already near M3P's fine-tuned performance for highly resourced languages. \murallarge's zero-shot (7) actually exceeds M3P's fine-tuned performance (10) and almost matches UC2's fine-tuned performance (11).
        
        
        \begin{table*}[t]
        \scalebox{0.68}{
            \begin{tabular}{lll|ccccccccc|ccccccccc}
                & & & \multicolumn{9}{c}{Well-resourced} & \multicolumn{8}{c}{Under-resourced}\\
                & & Model & en & de   & fr  & cs & ja & zh & ru & pl & tr & tg    & uz  & ga  & be  & mg  & ceb & ht  & war \\
                \hline
                \multirow{4}{*}{\rotatebox[origin=c]{90}{Zero-shot}}
                & (3) & \alignen & 46.5 & 33.9 & 42.3 & 32.4 & 29.9 & 36.2 & 40.1 & 39.2 & 40.5 & 30.0 & 23.4 & 26.1 & 27.3 & 33.6 & 34.9 & 41.6 & n/a \\
                & (4) & \alignmling & 46.7 & 33.5 & 45.0 & 26.5 & 33.6 & 35.2 & 30.9 & 29.9 & 31.4 & 21.2 & 15.6 & 12.9 & 8.9 & 23.9 & 31.0 & 33.1 & 24.0  \\
                & (6) & \muralbase & 46.4 & 33.9 & 44.8 & 31.5 & 34.3 & 35.6 & 33.7 & 33.2 & 34.7 & 35.3 & 24.1 & 20.8 & 21.4 & 33.0 & 35.7 & 39.1 & 26.1\\
& (7) & \murallarge & \bf 60.7 & \bf 46.1 & \bf 60.0 & \bf 43.6 & \bf 48.1 & \bf 49.9 & \bf 45.7 & \bf 45.8 & \bf 49.8 & \bf 45.7 & \bf 33.7 & \bf 30.8 & \bf 33.4 & \bf 45.6 & \bf 45.6 & \bf 52.4 & \bf 37.7\\
                \hline
                \multirow{4}{*}{\rotatebox[origin=c]{90}{Fine-tuned}}
                & (21) & \alignen &66.4 & 48.8 & 58.5 & 44.7 & 40.2 & 48.2 & 55.2 & 52.0 & 58.0 & 47.0 & 29.6 & 32.7 & 37.7 & 44.2 & 48.4 & 53.5 & n/a* \\
               
                & (18) & \alignmling & 75.6 & 69.2 & 76.2 & 65.5 & 64.4 & 78.2 & 68.3 & 68.3 & 75.0 & 53.0 & 36.3 & 35.8 & 50.3 & 45.0 & 72.4 & 62.5 & 78.1 \\
                & (19) & \muralbase & 77.1 & 70.0 & 77.2 & 68.4 & 64.8 & 79.6 & 70.8 & 70.7 & 78.2 & 64.2 & 44.1 & 41.9 & 59.3 & 55.1 & 76.4 & 67.6 & 79.0 \\
                & (20) & \murallarge & \bf 82.4 & \bf 76.3 & \bf 83.3 & \bf 74.5 & \bf 71.9 & \bf 86.7 & \bf 77.4 & \bf 77.4 & \bf 85.7 & \bf 72.9 & \bf 53.5 & \bf 51.4 & \bf 69.8 & \bf 62.3 & \bf 82.3 & \bf 76.7 & \bf 84.2\\
              \end{tabular}
              }
              \caption{Mean Recall on WIT for English (en); German (de); French (fr); Czech (cs); Japanese (ja); Chinese (zh); Russian (ru); Polish (pl); Turkish (tr); Tajik (tg); Uzbek (uz); Irish (ga); Belarusian (be); Malagasy (mg); Cebuano (ceb); Haitian (ht); Waray-Waray (war); : Translation system not available}
        \label{tab:wit_both}
        \end{table*}
        
        
        Even with far less data than AT+MBT, \muralbase trained on CC12M+EOBT (5) has much stronger zero-shot performance than M3P (1). With fine-tuning, \muralbase (CC12M+EOBT) improves on both fine-tuned M3P and UC2 (14 vs 10,11), except for Japanese. Though MURAL benefits from four times more image-text pairs than the others (CC12m  CC3M), both M3P and UC2 are more complex cross-encoder models that require other resources. M3P uses several different losses and it relies on a synthetic code-switched data generation process and a pretrained Faster-RCN model to obtain object bounding boxes and labels. MURAL is simpler: it is a dual encoder using just two loss types, and it works directly on raw text and pixels.
        
        
        
        The \textit{translate-train} strategy works well compared to using only multilingual image-text pairs (2 vs 4; 12 vs 13) and versus text-text training (2 vs 6; 12 vs 15). Given this, using translate-train (2) to increase language diversity in image-text pairs \textit{combined} with text-text pair training (6) may yield even more gains. As a zero-shot strategy, \textit{translate-test} also works well . This suggests that SMALR's combination of multilingual encoding and translate-test \cite{burns2020eccv} may improve zero-shot performance further with MURAL (i.e., 3+6+SMALR).
        
        Like others before, we find that training larger models on data of this scale produces remarkable gains: \murallarge obtains big improvements even over \muralbase. \murallarge's results are state-of-the-art for all languages except English (where the larger, English-only \alignhuge is best). \murallarge does this while--as a dual encoder--also supporting efficient retrieval. This makes a huge difference when retrieving from billions of items rather than the 1k to 5k examples of Multi30k's and MS-COCO's test sets (for which expensive, exhaustive comparisons can be performed with cross-encoders). See \citet{geigle-etal-retrieval} for extensive discussion and experiments around the computational cost of cross-encoders versus dual encoders for retrieval.
        






        \paragraph{Wikipedia Image Text Results.} We extracted two subsets of WIT for evaluation: 1) well-resourced languages and 2) under-resourced languages (more details in Appendix \ref{appendix:wit}). There are no prior results; here, we compare MURAL with \alignmling and \alignen using the translate-test baseline. Table \ref{tab:wit_both} shows \muralbase achieves slightly better zero-shot performance compared to \alignmling on well-resourced languages, and a large boost on the under-represented ones. These results confirm our hypothesis of combining two tasks to address data scarcity in cross modal pairs. For WIT, \murallarge again shows that increasing model capacity improves zero-shot performance dramatically (row 7).
        
        \begin{table}
        \scalebox{.68}{
              \begin{tabular}{ll|ccccccc}
                    & Model & it & es & ru & zh & pl & tr & ko \\
                    \hline
                    -- & mUSE+M3L & 78.9 & 76.7 & 73.6 & 76.1 & 71.7 & 70.9 & 70.7 \\
                    (4) & \alignmling  & 87.9 & 88.8 & 82.3 & 86.5 & 79.8 & 73.5 & 76.6 \\
                    (6) & \muralbase &  88.4 & 89.6 & 83.6 & 88.3 & 86.1 & 84.8 & 82.4 \\
                    (7) & \murallarge & \bf 91.8 & \bf 92.9 & \bf 87.2 & \bf 89.7 & \bf 91.0 & \bf 89.5 & \bf 88.1\\
\end{tabular}
          }
          \centering
        \caption{XTD zero-shot Text{}Image Recall@10. }
        \label{tab:xtd10}
        \end{table}
        
        With WIT, the translate-test strategy again proves effective (row 3). It is comparable to both \muralbase and \alignmling in a zero-shot setting-- each wins some contests. Nevertheless, translate-test fails for the extremely under-resourced Waray-Waray language because the NMT system lacks support for it. In all, we found that 27 of WIT's 108 languages lacked NMT support. Thus, we cannot fully rely on translation systems for many under-represented languages; this further bolsters exploration into pivoting on images to overcome data scarcity. Furthermore, simple dual-encoder models are fast and simple at test-time, and thus scale better than translate-test. 
        
        \begin{table*}[t]
            \centering
            \scalebox{0.65}{
            \begin{tabular}{ll|cccc|cccc|cccc|cccc}
                & & \multicolumn{4}{c|}{Image  Text}  & \multicolumn{4}{c}{Text  Image} & \multicolumn{4}{c}{Text  Text} & \multicolumn{4}{c}{Image  Image}\\
                & Model & R@1 & R@5 & R@10 & avg r & R@1 & R@5 & R@10 & avg r & R@1 & R@5 & R@10 & avg r & R@1 & R@5 & R@10 & avg r\\
                \hline
                (22) & \cxcbest & 55.9 & 84.2 & 91.8 & - & 41.7 & 72.3 & 83.0 & - & 42.4 & 64.9 & 74.0 & - & 38.5 & 73.6 & 84.9 & - \\
                (13) & \alignmling & 67.1 & 89.0 & 94.2 & 3.6 & 50.0 & 77.3 & 85.9 & 11.5 & 43.5 & 64.7 & 73.5 & 45.4 & 42.6 & 76.6 & 86.2 & 16.0  \\
                (15) & \muralbase & 65.8 & 89.1 & 94.3 & 3.2 & 49.7 & 77.5 & 86.0 & 11.0 & 43.9 & 64.9 & 73.9 & \bf 44.9 & 43.9 & 76.7 & 86.5 & 16.1 \\
                (16) & \murallarge & 74.6 & 92.8 & 96.6 & \bf 2.3 & 57.8 & 83.1 & 90.0 & \bf \pz9.4 & \bf 46.5 & \bf 67.5 & \bf 76.1 & 47.8 & \bf 50.3 & \bf 81.8 & \bf 90.1 & \bf 12.4\\
                (17) & \alignhuge &  \bf 78.1 & \bf 94.3 & \bf 97.4 & - & \bf 61.8 & \bf 84.9 & \bf 91.1 & - & 45.4 & 66.8 & 75.2 & - & 49.4 & 81.4 & 89.1 & - \\
            \end{tabular}
          }
        \caption{CxC Image{}text (left), Text{}Text (middle), and  Image{}Image (right) retrieval results. \cxcbest is the strongest model of \citet{parekh-etal-2021-crisscrossed}. \cxcbest and \alignhuge are fine-tuned on MSCOCO data, while \alignmling, \muralbase, and \murallarge are fine-tuned on both Multi30K and MSCOCO data).}
        \label{tab:cxc_retrieval}
        \end{table*}
        
        Finally, both \alignmling and MURAL models benefit from fine-tuning on in-domain multilingual image-text training pairs,\footnote{We fine-tune on WIT training split for 300K steps with initial learning rate 1e-4. Other hyper-parameters are the same as pre-training.} when available; both obtain very large gains across all languages, and also easily beat the translate-test baseline fine-tuned on WIT-en (18, 19, 20 vs 21).
        
        
        \textbf{XTD.} As shown in Table \ref{tab:xtd10}, both ALIGN and MURAL obtain massive gains over the best strategy reported by \citet{aggarwal2020towards}---mUSE \cite{yang2019multilingual} with a multimodal metric loss (M3L). \murallarge shows especially strong performance across all languages. Note that we only obtained these scores after all experimentation was done on other datasets---this is methodologically important as there is neither training data nor development data for XTD.
        
        
        \begin{table}[t]
        \scalebox{0.78}{
            \centering
            \begin{tabular}{ll|ccc}
                & \multirow{2}{*}{Model} & STS & SIS &  SITS \\
                & & avg  std & avg  std & avg  std\\
                \hline
                (22) & \cxcbest & \bf 74.5  0.4 & 74.5  0.9 & 61.9  1.3 \\
                (13) & \alignmling & 72.7  0.4 & \bf 80.4  0.7 & 63.7  1.3 \\
                (15) & \muralbase & 73.9  0.4 & 80.0  0.7 & 64.0  1.2  \\
                (16) & \murallarge & 74.1  0.4 & \bf 80.4  0.7 &  67.1  1.3  \\
                (17) & \alignhuge  & 72.9   0.4 & 77.2  0.8 & \bf 67.6  1.2 \\
            \end{tabular}
          }
        \caption{Semantic Simliarity using CxC.}
        \label{tab:cxc_correlation}
        \end{table}




        






        \textbf{Crisscrossed Captions.} 
        For CxC image-text retrieval (Table \ref{tab:cxc_retrieval}), \alignhuge scores highest across all metrics; it is the largest model and was trained only on English Alt-Text. \alignmling also beats \muralbase for image-text retrieval, but the latter comes back with better text-text and image-image scores. This indicates that MURAL's text-text task balances both encoders better than a loss focused only on image-text pairs. Similarly, \murallarge beats \alignhuge for both text-text and image-image retrieval, despite the fact that \alignhuge uses a much larger image encoder.
        


        The correlation results given in Table \ref{tab:cxc_correlation} tell an interesting story. Contrary to intuition and retrieval results,  Semantic Image Similarity (SIS) seems connected with multilinguality, as all Alt-Text models (\alignmling, \muralbase, \murallarge) perform nearly the same (and better).  \cxcbest scores the highest on Semantic Text Similarity (STS) followed closely by \murallarge. It is worth noting that \cxcbest was trained with MSCOCO co-captions which could explain its high correlation.
Semantic Image-Text Similarity (SITS) agrees with Image-Text retrieval results the most, with both \murallarge and \alignhuge performing considerably better than others. However, with the SITS metric, the gap between both these models diminishes, indicating that \alignhuge is probably more focused on getting positive matches while \murallarge captures non-matches more effectively.
        


        The combined retrieval and correlation lens of CxC indicates there is much more to evaluating multimodal representations than the predominant cross-modal retrieval tasks. Ranking a set of items in a manner consistent with human similarity judgments is arguably a harder task than getting a single paired item to be more similar than nearly all others. These two perspectives may reveal useful tensions in finer-grained semantic distinctions. In fact, it is with these correlation measures that we expect cross-encoders to shine compared to the retrieval-oriented dual encoders.