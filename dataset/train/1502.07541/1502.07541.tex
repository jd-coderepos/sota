\documentclass[10pt,double]{IEEEtran}



\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{latexsym}
\usepackage{verbatim}
\usepackage{graphicx}


\newtheorem{thm}{Theorem}
\newtheorem{prop}{Proposition}
\newtheorem{lem}{Lemma}
\newtheorem{cor}{Corollary}

\newtheorem{property}{Property}
\newtheorem{defn}{Definition}
\newtheorem{example}{Example}
\newtheorem{problem}{Problem}
\newtheorem{condition}{Condition}
\newtheorem{fact}{Fact}
\newtheorem{procedure}{Procedure}
\newtheorem{question}{Question}
\newtheorem{idea}{Idea}
\newenvironment{rmk}{\textit{Remark:}}


\newenvironment{densitemize}{\begin{list}               { \hfill}{
        \setlength{\leftmargin}{\parindent}
        \setlength{\parsep}{0.04\baselineskip}
        \setlength{\itemsep}{0.5\parsep}
        \setlength{\labelwidth}{\leftmargin}
        \setlength{\labelsep}{0em}}
    }
{\end{list}}

\providecommand{\eref}[1]{\eqref{eq:#1}}  \providecommand{\cref}[1]{Chapter~\ref{chap:#1}}
\providecommand{\sref}[1]{Section~\ref{sec:#1}}
\providecommand{\fref}[1]{Figure~\ref{fig:#1}}
\providecommand{\sfref}[1]{Fig.~\ref{subfig:#1})}
\providecommand{\tref}[1]{Table~\ref{tab:#1}}
\providecommand{\aref}[1]{Algorithm~\ref{alg:#1}}
\providecommand{\lref}[1]{Lemma~\ref{lem:#1}}
\providecommand{\thref}[1]{Theorem~\ref{thm:#1}}

\providecommand{\R}{\ensuremath{\mathbb{R}}}
\providecommand{\C}{\ensuremath{\mathbb{C}}}
\providecommand{\N}{\ensuremath{\mathbb{N}}}
\renewcommand{\S}{\ensuremath{\mathbb{S}}}
\providecommand{\Z}{\ensuremath{\mathbb{Z}}}

\providecommand{\bfP}{\mathbf{P}}

\providecommand{\abs}[1]{\left|#1\right|}
\providecommand{\norm}[1]{\left\lVert#1\right\rVert}
\providecommand{\inprod}[1]{\left\langle#1\right\rangle}
\providecommand{\set}[1]{\left\{#1\right\}}
\providecommand{\seq}[1]{\left<#1\right>}
\providecommand{\bydef}{\overset{\text{def}}{=}}
\providecommand{\diag}{\mathop{\mathrm{diag}}}
\providecommand{\rank}{\mathop{\mathrm{rank}}}

\providecommand{\parder}[2]{{\partial{#1} \over \partial{#2}}}
\providecommand{\parderr}[2]{{\partial^2{#1} \over \partial{#2}^2}}
\providecommand{\Rint}{\int_{\R}}

\providecommand{\di}{\ensuremath{~\text{d}}}
\providecommand{\e}{\ensuremath{\mathrm{e}}}
\providecommand{\I}{\ensuremath{j}}
\providecommand{\epi}[1]{\e^{\hspace{1pt}\I{#1}}}
\providecommand{\eni}[1]{\e^{-\I{#1}}}




\renewcommand{\vec}[1]{\ensuremath{\boldsymbol{#1}}}
\providecommand{\mat}[1]{\ensuremath{\boldsymbol{#1}}}
\providecommand{\wh}[1]{\ensuremath{\widehat{#1}}}
\providecommand{\wt}[1]{\ensuremath{\widetilde{#1}}}


\providecommand{\calS}{\mathcal{S}}
\providecommand{\calP}{\mathcal{P}}
\providecommand{\calR}{\mathcal{R}}
\providecommand{\calH}{\mathcal{H}}
\providecommand{\calM}{\mathcal{M}}
\providecommand{\calV}{\mathcal{V}}
\providecommand{\calF}{\mathcal{F}}
\providecommand{\calG}{\mathcal{G}}
\providecommand{\calO}{\mathcal{O}}
\providecommand{\calY}{\mathcal{Y}}

\providecommand{\mA}{\mat{A}} \providecommand{\mB}{\mat{B}}
\providecommand{\mC}{\mat{C}} \providecommand{\mD}{\mat{D}}
\providecommand{\mE}{\mat{E}} \providecommand{\mH}{\mat{H}}
\providecommand{\mI}{\mat{I}} \providecommand{\mJ}{\mat{J}} 
\providecommand{\mK}{\mat{K}} \providecommand{\mL}{\mat{L}} 
\providecommand{\mN}{\mat{N}}
\providecommand{\mM}{\mat{M}} \providecommand{\mP}{\mat{P}} 
\providecommand{\mQ}{\mat{Q}} \providecommand{\mR}{\mat{R}}
\providecommand{\mS}{\mat{S}} \providecommand{\mU}{\mat{U}} 
\providecommand{\mV}{\mat{V}} \providecommand{\mT}{\mat{T}}
\providecommand{\mW}{\mat{W}}
\providecommand{\mSigma}{\mat{\Sigma}}
\providecommand{\mGm}{\mat{\Gamma}} \providecommand{\mG}{\mat{G}}
\providecommand{\mPi}{\mat{\Pi}}
\providecommand{\mX}{\mat{X}}\providecommand{\mY}{\mat{Y}}
\providecommand{\mZ}{\mat{Z}}


\providecommand{\va}{\vec{a}} \providecommand{\vb}{\vec{b}}
\providecommand{\vc}{\vec{c}} \providecommand{\vd}{\vec{d}}
\providecommand{\ve}{\vec{e}} \providecommand{\vf}{\vec{f}}
\providecommand{\vE}{\vec{E}} \providecommand{\vF}{\vec{F}}
\providecommand{\vg}{\vec{g}}
\providecommand{\vh}{\vec{h}} \providecommand{\vk}{\vec{k}}
\providecommand{\vm}{\vec{m}} \providecommand{\vn}{\vec{n}} 
\providecommand{\vl}{\vec{l}} \providecommand{\vp}{\vec{p}}
\providecommand{\vq}{\vec{q}} \providecommand{\vr}{\vec{r}}
\providecommand{\vs}{\vec{s}}
\providecommand{\vt}{\vec{t}}
\providecommand{\vu}{\vec{u}} \providecommand{\vw}{\vec{w}}
\providecommand{\vU}{\vec{U}}
\providecommand{\vx}{\vec{x}} \providecommand{\vy}{\vec{y}}
\providecommand{\vz}{\vec{z}} \providecommand{\vi}{\vec{i}}
\providecommand{\vj}{\vec{j}} \providecommand{\vzero}{\vec{0}}
\providecommand{\vgm}{\vec{\gamma}} \providecommand{\vv}{\vec{v}}
\providecommand{\vlambda}{\vec{\lambda}}
\providecommand{\vmu}{\vec{\mu}}
\providecommand{\vomg}{\vec{\omega}}
\providecommand{\vsigma}{\vec{\sigma}}


\providecommand{\whu}{\wh{u}} \providecommand{\whf}{\wh{f}}



\providecommand{\w}{\omega}

\providecommand{\conv}{\ast}
\providecommand{\circonv}[1]{\circledast_{#1}}

\renewcommand{\div}{\mathop{\mathbf{div}}}
\providecommand{\grad}{\mathop{\text{\bf grad}}} 
\pdfpageattr {/Group << /S /Transparency /I true /CS /DeviceRGB>>} 

\usepackage{bbm}
\usepackage{bm}
\usepackage{dsfont}
\usepackage{algorithm}
\usepackage{subfigure}
\usepackage{multicol}
\usepackage{algpseudocode}
\usepackage{xcolor}
\usepackage[pagebackref=false,
            citecolor=blue,
            urlcolor=blue,
            colorlinks=true]{hyperref}
\usepackage{hyperref}
\usepackage[framemethod=tikz]{mdframed}
\usepackage{booktabs}
\usepackage{multirow} 
\usepackage{tabularx}
\usepackage{pifont}
\usepackage[numbered]{mcode}
\usepackage{mathtools}
\usepackage{float}
\lstset{xleftmargin=5mm}
\DeclareRobustCommand{\mtx}{\texorpdfstring{}{0 \ 1; -1 0}}

\hyphenation{formulation}

\newcommand{\EDM}{\ensuremath{\mathrm{edm}}}
\newcommand{\EDMset}{\ensuremath{\mathbb{EDM}}}
\newcommand{\EDMgram}{\ensuremath{\mathcal{K}}}
\newcommand{\GramSmall}{\ensuremath{\mathcal{G}}}
\newcommand{\vone}{\ensuremath{\vec{\mathit{1}}}}
\renewcommand{\vzero}{\ensuremath{\vec{\mathit{0}}}}
\newcommand{\mzero}{\ensuremath{\mat{\mathit{0}}}}
\newcommand{\T}{\ensuremath{\top}}
\newcommand{\trace}{\ensuremath{\mathop{\mathrm{trace}}}}
\newcommand{\mDelta}{\bm{\mathit{\Delta}}}
\newcommand{\mLambda}{\bm{\mathit{\Lambda}}}
\renewcommand{\diag}{\mathop{\mathrm{diag}}}
\newcommand{\embdim}{\ensuremath{\mathrm{embdim}}}
\newcommand{\affdim}{\ensuremath{\mathrm{affdim}}}
\newcommand{\SVD}{\ensuremath{\mathrm{SVD}}}
\newcommand{\EVD}{\ensuremath{\mathrm{EVD}}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}


\newcommand{\note}[1]{{\color{red}{\textbf{#1}}}}


\definecolor{border_color}{rgb}{0.0, 0.0, 0.0}
\definecolor{fill_color}{rgb}{0.9, 0.9, 0.9}
\newmdenv[innerlinewidth=0.25pt,
          linecolor=border_color, 
	      backgroundcolor=fill_color,  
          innerleftmargin=6pt, 
          innerrightmargin=6pt,
          innertopmargin=6pt,
          innerbottommargin=6pt]{spmagbox}

\definecolor{nice_blue}{rgb}{0.5 0.1 0.9}
\newcommand{\rev}[1]{{#1}}




\makeatletter
\newlength{\continueindent}
\setlength{\continueindent}{14em}

\renewenvironment{algorithmic}[1][0]{\edef\ALG@numberfreq{#1}\def\@currentlabel{\theALG@line}\setcounter{ALG@line}{0}\setcounter{ALG@rem}{0}\let\\\algbreak \expandafter\edef\csname ALG@currentblock@\theALG@nested\endcsname{0}\expandafter\let\csname ALG@currentlifetime@\theALG@nested\endcsname\relax \begin{list}{\ALG@step}{\rightmargin\z@ \itemsep\z@ \itemindent\z@ \listparindent2em\partopsep\z@ \parskip\z@ \parsep\z@ \labelsep 0.5em \topsep 0.2em\ifthenelse{\equal{#1}{0}}{\labelwidth 0.5em}{\labelwidth 1.2em}\leftmargin\labelwidth \addtolength{\leftmargin}{\labelsep}
      \ALG@tlm\z@ }\parshape 2 \leftmargin \linewidth \continueindent \dimexpr\linewidth-\continueindent\relax
   \setcounter{ALG@nested}{0}\ALG@beginalgorithmic }{\ALG@closeloops \expandafter\ifnum\csname ALG@currentblock@\theALG@nested\endcsname=0\relax \else \PackageError{algorithmicx}{Some blocks are not closed!!!}{}\fi \ALG@endalgorithmic \end{list}}\makeatother



\title{{\fontsize{1.18cm}{1em}\selectfont Euclidean Distance Matrices} \\ \huge{Essential Theory, Algorithms and Applications}}
\author{Ivan Dokmani\'{c}, Reza Parhizkar, Juri Ranieri and Martin Vetterli
\thanks{Ivan Dokmani\'{c}, Juri Ranieri and Martin Vetterli are with the School of Computer and Communication
Sciences, Ecole Polytechnique F\'{e}d\'{e}rale de Lausanne (EPFL), CH-1015
Lausanne, Switzerland (e-mails: \{ivan.dokmanic, juri.ranieri@epfl.ch,
martin.vetterli\}@epfl.ch). Reza Parhizkar is with macx red AG (Financial services), CH-6300 Zug, Switzerland (e-mail: reza.parhizkar@gmail.com).}\thanks{Ivan Dokmani\'c and Juri Ranieri were supported by the ERC Advanced
Grant---Support for Frontier Research---SPARSAM, Nr: 247006. Ivan Dokmani\'c
was also supported by the Google PhD Fellowship.} }


\begin{document}
\maketitle





\begin{abstract}
Euclidean distance matrices (EDM) are matrices of squared distances between
points. The definition is deceivingly simple: thanks to their many useful
properties they have found applications in psychometrics, crystallography,
machine learning, wireless sensor networks, acoustics, and more. Despite the
usefulness of EDMs, they seem to be insufficiently known in the signal
processing community. Our goal is to rectify this mishap in a concise
tutorial.

We review the fundamental properties of EDMs, such as  rank or
(non)definiteness. We show how various EDM properties can be used to design
algorithms for completing and denoising distance data. Along the way, we
demonstrate applications to microphone position calibration, ultrasound
tomography, room reconstruction from echoes and phase retrieval. By spelling
out the essential algorithms, we hope to fast-track the readers in applying
EDMs to their own problems. Matlab code for all the described algorithms, and
to generate the figures in the paper, is available online. Finally, we suggest
directions for further research.
\end{abstract}



\section{Introduction}

Imagine that you land at Geneva airport with the Swiss train schedule but no
map. Perhaps surprisingly, this may be sufficient to reconstruct a rough (or
not so rough) map of the Alpine country, even if the train times poorly
translate to distances or some of the times are unknown. The way to do it is
by using Euclidean distance matrices (EDM): for a quick illustration, take a
look at the ``\textbf{Swiss Trains}'' box.

An EDM is a matrix of squared Euclidean distances between points in a
set.\footnote{\rev{While there is no doubt that a Euclidean distance matrix
should contain Euclidean distances, and not the squares thereof, we adhere to
this semantically dubious convention for the sake of compatibility with most
of the EDM literature. Often, working with squares does simplify the
notation.}} We often work with distances because they are convenient to
measure or estimate. In wireless sensor networks for example, the sensor nodes
measure
\rev{received signal strengths} of the packets sent by other nodes, or time-of-arrival (TOA) of pulses emitted by their neighbors \cite{Patwari:2005kc}.
Both of these proxies allow for distance estimation between pairs of nodes,
thus we can attempt to reconstruct the network topology. This is often termed
\emph{self-localization}
\cite{Alfakih1999,dohetry2001,Biswas2004}. The molecular conformation problem
is another instance of a distance problem \cite{Havel1985281},
and so is reconstructing a room's geometry from echoes
\cite{Dokmanic:2013dz}. Less
obviously, sparse phase retrieval \cite{Ranieri:2013tx} can be converted to a
distance problem, and addressed using EDMs.

\begin{figure}
\centering
\includegraphics[width=3.5in]{fig1.pdf}
\caption{Two real-world applications of EDMs. Sensor network localization
from estimated pairwise distances is illustrated on the left, with one
distance missing because the corresponding sensor nodes are too far apart to
communicate. In the molecular conformation problem on the right, we aim to
estimate the locations of the atoms in a molecule from their pairwise
distances. Here, due to the inherent measurement uncertainty, we know the
distances only up to an interval.}
  \vspace{-2mm}
\label{fig:intro}
\end{figure}

Sometimes the data are not metric, but we seek a
metric representation, as it happens commonly in psychometrics
\cite{torgerson1952}. As a matter of fact, the psychometrics community is
at the root of the development of a number of tools related to EDMs, including
multidimensional scaling (MDS)---the problem of finding the best point set
representation of a given set of distances. More abstractly, people are
concerned with EDMs for objects living in high-dimensional vector spaces, such
as images \cite{Weinberger2004}.

EDMs are a useful description of the point sets, and a starting point for
algorithm design. A typical task is to retrieve the original point
configuration: it may initially come as a surprise that this requires no more
than an eigenvalue decomposition (EVD) of a symmetric
matrix.\footnote{\rev{Because the EDMs are symmetric, we choose to use EVDs
instead of singular value decompositions. That the EVD is much more efficient for
symmetric matrices was suggested to us by one of the reviewers of the initial
manuscript, who in turn received the advice from the numerical analyst Michael
Saunders.}} In fact, the majority of Euclidean distance problems require the
reconstruction of the point set, but always with one or more of the following
twists:
\begin{enumerate}
	\item Distances are noisy,
	\item Some distances are missing,
	\item Distances are unlabeled.
\end{enumerate}
For two examples of applications requiring a solution of EDM problems with
different complications, see Fig. \ref{fig:intro}.
\begin{figure}[t]
\fontfamily{lmss}\selectfont
\begin{spmagbox}
\textbf{Swiss Trains (Swiss Map Reconstruction)}
\\
\fontsize{9pt}{10pt}
\begin{center}
	\includegraphics[width=3.3in]{fig2.pdf}
\end{center}
\vspace{8mm}

\selectfont
Consider the following matrix of times in minutes it takes to travel by train
between some Swiss cities:

The numbers were taken from the Swiss railways timetable. The matrix
was then processed using the classical MDS algorithm (Algorithm
\ref{alg:classical_mds}), which is basically an EVD. The obtained city
configuration was rotated and scaled to align with the actual map. Given all
the uncertainties involved, the fit is remarkably good. Not all trains drive
with the same speed; they have varying numbers of stops, railroads are not
straight lines (lakes and mountains). This result may be regarded as
anecdotal, but in a fun way it illustrates the power of the EDM toolbox.
Classical MDS could be considered the simplest of the available tools, yet it
yields usable results with erroneous data. On the other hand, it might be that
Swiss trains are just so good. 
\end{spmagbox}
\end{figure}


There are two fundamental problems associated with distance geometry
\cite{Liberti:2012ut}: (i) given a matrix, determine whether it is an EDM,
(ii) given a possibly incomplete set of distances, determine whether there
exists a configuration of points in a given embedding dimension---dimension of the
smallest affine space comprising the points---that generates the distances.



\subsection{Prior Art}

The study of point sets through pairwise distances, and so of EDMs, can be
traced back to the works of Menger
\cite{Menger:1928bc}, Schoenberg \cite{Schoenberg:1935dk},
Blumenthal \cite{Blumenthal:1953ie}, and Young and Householder
\cite{young1938}.

An important class of EDM tools was initially developed for the purpose of
data visualization. In 1952, Torgerson introduced the notion of MDS
\cite{torgerson1952}. He used distances to quantify the
\emph{dissimilarities} between pairs of objects that are not necessarilly
vectors in a metric space. Later in 1964, Kruskal suggested the notion of
\emph{stress} as a measure of goodness-of-fit for non-metric data
\cite{kruskal1964}, again representing experimental dissimilarities between
objects. 

A number of analytical results on EDMs were developed by Gower
\cite{gower1982, gower1}. In his 1985 paper \cite{gower1}, he gave a complete
characterization of the EDM rank. Optimization with EDMs requires good
geometric intuitions about matrix spaces. In 1990, Glunt \cite{Glunt1990} and
Hayden \cite{Hayden1990} with their co-authors provided insights into the
structure of the convex cone of EDMs. An extensive treatise on EDMs with many
original results and an elegant characterization of the EDM cone is given by
Dattorro \cite{Dattorro:2011wa}.

In early 1980s, Williamson, Havel and W\"{u}thrich developed the idea of
extracting the distances between pairs of hydrogen atoms in a protein, using
nuclear magnetic resonance (NMR). The extracted distances were then used to
reconstruct 3D shapes of molecules\footnote{W\"{u}thrich received the Nobel
Prize for chemistry in 2002.} \cite{Havel1985281}. The NMR spectrometer
(together with some post-processing) outputs the distances between pairs of
atoms in a large molecule. The distances are not specified for all atom pairs,
and they are uncertain---given only up to an interval. This setup lends itself
naturally to EDM treatment; for example, it can be directly addressed using
MDS \cite{trosset1998}. Indeed, the crystallography community also contributed
a large number of important results on distance geometry. In a different
biochemical application, comparing distance matrices yields efficient
algorithms for comparing proteins from their 3D structure
\cite{Holm:1993dx}.

In machine learning, one can learn manifolds by finding an EDM with a low
embedding dimension that preserves the geometry of local neighborhoods.
Weinberger and Saul use it to learn image manifolds
\cite{Weinberger2004}. Other examples of using Euclidean distance geometry in
machine learning are results by Tenenbaum, De Silva and Langford
\cite{Tenenbaum2000} on image understanding and handwriting recognition, Jain
and Saul \cite{jain2004} on speech and music, and Demaine and \emph{et al.}
\cite{Demaine:2009dw} on music and musical rhythms.

With the increased interest in sensor networks, several EDM-based approaches
were proposed for sensor localization
\cite{Alfakih1999,dohetry2001,Biswas2004,Dattorro:2011wa}. 
Connections between EDMs, multilateration and semidefinite programming are
expounded in depth in \cite{So:2007cz}, especially in the context of sensor
network localization.

Position calibration in ad-hoc microphone arrays is often done with sources at
unknown locations, such as handclaps, fingersnaps or randomly placed
loudspeakers
\cite{Crocco:2012eu,Pollefeys:2008ho,Dokmanic:2014tc}. This gives us distances
(possibly up to an offset time) between the microphones and the sources and
leads to the problem of multi-dimensional unfolding
\cite{Schonemann:1970wd}.

All of the above applications work with labeled distance data. In certain TOA-
based applications one loses the labels---the correct permutation of the
distances is no longer known. This arises in reconstructing the geometry of a
room from echoes
\cite{Dokmanic:2013dz}.
Another example of unlabeled distances is in sparse phase retrieval, where the
distances between the unknown non-zero lags in a signal are revealed in its
autocorrelation function \cite{Ranieri:2013tx}. \rev{Recently, motivated by
problems in crystallography, Gujarahati and co-authors published an algorithm for reconstruction of Euclidean networks from
unlabeled distance data \cite{Gujarathi:2014cz}.}



\subsection{Our Mission}

We were motivated to write this tutorial after realizing that EDMs are not
common knowledge in the signal processing community, perhaps for the lack of a
compact introductory text. This is effectively illustrated by the anecdote
that, not long before writing this article, one of the authors had to add the
(rather fundamental) rank property to the Wikipedia page on EDMs.\footnote{We
are working on improving that page substantially.} In a compact tutorial we do
not attempt to be exhaustive; much more thorough literature reviews are
available in longer expos\'es on EDMs and distance geometry
\cite{Liberti:2012ut,Krislock:2012xx,Mucherino:2012hw}. Unlike these works
that take the most general approach through graph realizations, we opt to show
simple cases through examples, and to explain and spell out a set of basic
algorithms that anyone can use immediately. Two big topics that we discuss are
not commonly treated in the EDM literature: localization from unlabeled
distances, and multidimensional unfolding (applied to microphone
localization). On the other hand, we choose to not explicitly discuss the
sensor network localization (SNL) problem, as the relevant literature is
abundant.

Implementations of all the algorithms are available
online.\footnote{\url{http://lcav.epfl.ch/ivan.dokmanic}} Our hope is that
this will provide a good entry point for those wishing to learn much more, and
inspire new approaches to old problems.

\begin{table}[h!]
  \centering
  \caption{Summary of notation}
  \begin{tabular}{@{}p{0.5in}p{2.7in}@{}}
  \toprule
  {\bf Symbol} & {\bf Meaning} \\
  \midrule
   &
  Number of points (columns) in \\
   &
  Dimensionality of the Euclidean space\\
   &
  Element of a matrix  on the th row and the th column\\
   &
  A Euclidean distance matrix \\
   &
  Euclidean distance matrix created from columns in  \\ 
   &
  Matrix containing the squared distances between the columns of  and  \\
   &
  Euclidean distance matrix created from the Gram matrix  \\
   &
  Geometric centering matrix \\
   &
  Restriction of  to non-zero entries in  \\
   &
  Mask matrix, with ones for observed entries \\
   &
  Set of real symmetric positive semidefinite matrices in  \\
   &
  Affine dimension of the points listed in  \\
   &
  Hadamard (entrywise) product of  and  \\
   &
  Noise corrupting the  distance\\
   &
  th vector of the canonical basis\\
   &
  Frobenius norm of , \\
  \bottomrule
  \end{tabular}
  \label{tab:notation-summary}
\end{table}


\section{From Points to EDMs and Back} \label{sec:from_points_to_edms_and_back}
  
The principal EDM-related task is to reconstruct the original point set. This
task is an inverse problem to the simpler forward problem of finding the EDM
given the points. Thus it is desirable to have an analytic expression for the
EDM in terms of the point matrix. Beyond convenience, we can expect such an
expression to provide interesting structural insights. We will define notation
as it becomes necessary---a summary is given in Table
\ref{tab:notation-summary}.

Consider a collection of  points in a -dimensional Euclidean space,
ascribed to the columns of matrix , . Then the squared distance between  and  is given as

where  denotes the Euclidean norm. Expanding the norm
yields

From here, we can read out the matrix equation for ,

where  denotes the column vector of all ones and  is a
column vector of the diagonal entries of . We see that  is in
fact a function of . For later reference, it is convenient to
define an operator  similar to , that operates
directly on the Gram matrix ,


The EDM assembly formula \eqref{eq:edm_assemble} or
\eqref{eq:edm_gram_assemble} reveals an important property: Because the rank
of  is at most  (it has  rows), then the rank of  is
also at most . The remaining two summands in
\eqref{eq:edm_assemble} have rank one. By rank inequalities, rank of a sum of
matrices cannot exceed the sum of the ranks of the summands. With this
observation, we proved one of the most notable facts about EDMs:

\begin{thm}[Rank of EDMs]
\label{thm:edm_rank}
Rank of an EDM corresponding to points in  is at most .
\end{thm}

This is a powerful theorem: it states that the rank of an EDM is independent
of the number of points that generate it. In many applications,  is three
or less, while  can be in the thousands. According to Theorem
\ref{thm:edm_rank}, rank of such practical matrices is at most five. The proof
of this theorem is simple, but to appreciate that the property is not obvious,
you may try to compute the rank of the matrix of non-squared distances.

What really matters in Theorem \ref{thm:edm_rank} is the affine dimension of
the point set---the dimension of the smallest affine subspace that contains
the points, denoted by . For example, if the points lie on a
plane (but not on a line or a circle) in , rank of the corresponding EDM
is four, not five. This will be clear from a different perspective in the next
subsection, as any affine subspace is just a translation of a linear subspace.
An illustration for a 1D subspace of  is provided in Fig.
\ref{fig:affine}: Subtracting \emph{any} point in the affine subspace from all
its points translates it to the parallel linear subspace that contains the
zero vector.

\begin{figure}
\centering
\includegraphics[width=3.5in]{fig3.pdf}
\caption{Illustration of the relationship between an affine subspace and its
parallel linear subspace. The points  live in an
affine subspace---a line in  that does not contain the origin. In (A),
the vector  is subtracted from all the points, and the new point list
is . While the
columns of  span , the columns of  only span the 1D subspace
of ---the line through the origin. In (B), we subtract a different
vector from all points: the centroid . The translated vectors  again span
the same 1D subspace.}
\label{fig:affine}
\end{figure}



\subsection{Essential Uniqueness} \label{sub:essential_uniqueness}

When solving an inverse problem, we need to understand what is recoverable and
what is forever lost in the forward problem. Representing sets of points by
distances usually increases the size of the representation. For most
interesting  and , the number of pairwise distances is larger than the
size of the coordinate description, , so an EDM holds
more scalars than the list of point coordinates. Nevertheless, some
information is lost in this encoding, namely the information about the
absolute position and orientation of the point set. Intuitively, it is clear
that rigid transformations (including reflections) do not change distances
between the fixed points in a point set. This intuitive fact is easily deduced
from the EDM assembly formula \eqref{eq:edm_assemble}. We have seen in
\eqref{eq:edm_assemble} and \eqref{eq:edm_gram_assemble} that  is
in fact a function of the Gram matrix .

\begin{figure}[t]
\centering
\includegraphics[width=3.3in]{fig4.pdf}
\caption{Illustration of a rigid transformation in 2D. Here the points set is
transformed as . Rotation matrix \mtx,
corresponds to a counterclockwise rotation of 90. The translation
vector is . The shape is drawn for visual reference.}
\label{fig:procrustes}
\end{figure}


This makes it easy to show algebraically that rotations and
reflections do not alter the distances. Any rotation/reflection can be
represented by an orthogonal matrix  acting on the
points . Thus for the rotated point set  we can write


where we invoked the orthogonality of the rotation/reflection matrix,
.

Translation by a vector  can be expressed as

Using , one can directly verify that this transformation leaves
\eqref{eq:edm_assemble} intact. In summary,


The consequence of this invariance is that we will never be able to
reconstruct the absolute orientation of the point set using only the
distances, and the corresponding degrees of freedom will be chosen freely.
\rev{Different reconstruction procedures will lead to different realizations of the
point set, all of them being rigid transformations of each other.} Fig.
\ref{fig:procrustes} illustrates a point set under a rigid transformation. It
is clear that the distances between the points are the same for all three
shapes.



\subsection{Reconstructing the Point Set From Distances} \label{sub:reconstructing_the_point_set_from_distances}

The EDM equation \eqref{eq:edm_assemble} hints at a procedure to compute the
point set starting from the distance matrix. 
Consider the following choice: let the first point   be at the origin.
Then the first column of  contains the squared norms of the point
vectors,

Consequently, we can immediately construct the term 
and its transpose in \eqref{eq:edm_assemble}, as the diagonal of  contains exactly the norms squared . Concretely, 

where  is the first column of . We thus obtain the Gram matrix
from 
\eqref{eq:edm_assemble} as

The point set can be found by an EVD, , where  with all eigenvalues
 non-negative, and  orthonormal, as  is a symmetric
positive semidefinite matrix. Throughout the paper we assume that the
eigenvalues are sorted in the order of decreasing magnitude, . We can now set
. Note that
we could have simply taken  as the reconstructed point set, but if the Gram matrix really
describes a -dimensional point set, the trailing eigenvalues will be
zeroes, so we choose to truncate the corresponding rows.

\rev{It is straightforward to verify that the reconstructed point set
 generates the original EDM, ; as we have learned,
 and  are related by a rigid transformation. The described procedure
is called the \emph{classical MDS}, with a particular choice of the coordinate
system:  is fixed at the origin.}

In \eqref{eq:classical_mds_simple} we subtract a structured rank-2 matrix
 from . A more systematic approach to
the classical MDS is to use a generalization of
\eqref{eq:classical_mds_simple} by Gower
\cite{gower1982}. Any such subtraction that makes the right hand side of
\eqref{eq:classical_mds_simple} positive semidefinite (PSD), \emph{i.e.}, that
makes  a Gram matrix, can also be modeled by multiplying  from both
sides by a particular matrix. This is substantiated in the following result.

\begin{thm}[Gower \cite{gower1982}]
\label{thm:sdp_equivalence}
 is an EDM if and only if

is PSD for any  such that  and .
\end{thm}
In fact, if \eqref{eq:sdp_equivalence} is PSD for one such , then it is
PSD for all of them. In particular, define the \emph{geometric centering
matrix} as

Then  being positive semidefinite is equivalent to
 being an EDM. Different choices of  correspond to different
translations of the point set. 

The classical MDS algorithm with the geometric centering matrix is spelled out
in Algorithm \ref{alg:classical_mds}. \rev{Whereas so far we have assumed that
the distance measurements are noiseless, Algorithm \ref{alg:classical_mds} can
handle noisy distances too, as it discards all but the  largest
eigenvalues.}

\begin{algorithm}[t]
\caption{Classical MDS}
\label{alg:classical_mds}
\begin{algorithmic}[1]
\Function{ClassicalMDS}{}
	\State  \Comment{Geometric centering matrix}
	\State  \Comment{Compute the Gram matrix}
\State 
	\State \Return{}
\EndFunction
\end{algorithmic}
\end{algorithm}

It is straightforward to verify that \eqref{eq:classical_mds_simple}
corresponds to . Think about what this means in terms of the
point set:  selects the first point in the list, . Then
 translates the points so that  is
translated to the origin. Multiplying the definition \eqref{eq:edm_assemble}
from the right by  and from the left by  will annihilate the two rank-1 matrices,  and
. We see that the remaining term has the form , and the reconstructed point set will have the first point at
the origin!

On the other hand, setting  places the centroid of
the point set at the origin of the coordinate system. For this reason, the
matrix  is called the \emph{centering matrix}. To better
understand why, consider how we normally center a set of points given in
. 

First, we compute the centroid as the mean of all the points

Second, we subtract this vector from all the points in the set,

In complete analogy with the reasoning for , we can see that the
reconstructed point set will be centered at the origin.


\subsection{Orthogonal Procrustes Problem}

Since the absolute position and orientation of the points are lost when going
over to distances, we need a method to align the reconstructed point set with
a set of \emph{anchors}---points whose coordinates are fixed and known.

This can be achieved in two steps, sometimes called Procrustes analysis.
Ascribe the anchors to the columns of , and suppose that we want to align
the point set  with the columns of . \rev{Let  denote the
submatrix (a selection of columns) of  that should be aligned with the
anchors. We note that the number of anchors---columns in ---is typically
small compared with the total number of points---columns in .}

In the first step, we remove the means  and  from matrices
 and , obtaining the matrices  and
. In the second step, termed orthogonal Procrustes analysis,
we are searching for the rotation and reflection that best maps
 onto ,


\rev{The Frobenius norm  is simply the -norm of
the matrix entries, . 

The solution to
\eqref{eq:orthogonal_procrustes}---found by Sch\"onemann in his PhD thesis
\cite{Schonemann:1964tj}---is given by the singular value decomposition (SVD).
Let ; then we can continue computing
\eqref{eq:orthogonal_procrustes} as follows 


where , and we used the orthogonal invariance
of the Frobenius norm and the cyclic invariance of the trace. The
last trace expression in \eqref{eq:procrustes_solution} is equal to
. Noting that  is also an
orthogonal matrix, its diagonal entries cannot exceed 1. Therefore, the
maximum is achieved when  for all , meaning that the
optimal  is an identity matrix. It follows that . 

Once the optimal rigid transformation has been found, the alignment can be
applied to the entire point set as

}

\subsection{Counting the Degrees of Freedom}

It is interesting to count how many degrees of freedom there are in different
EDM related objects. Clearly, for  points in  we have

degrees of freedom: If we describe the point set by the list of coordinates,
the size of the description matches the number of degrees of freedom. Going
from the points to the EDM (usually) increases the description size to
, as the EDM lists the distances between all the pairs of
points. 
By Theorem \ref{thm:edm_rank} we know that the EDM has
rank at most .

\rev{Let us imagine for a moment that we do not know any other EDM-specific
properties of our matrix, except that it is symmetric, positive, zero-diagonal
(or \emph{hollow}), and that it has rank . The purpose of this exercise
is to count the degrees of freedom associated with such a matrix, and to see
if their number matches the intrinsic number of the degrees of freedom of the
point set, . If it did, then these properties would completely
characterize an EDM. We can already anticipate from Theorem
\ref{thm:sdp_equivalence} that we need more properties: a certain matrix related to the
EDM---as given in \eqref{eq:sdp_equivalence}---must be PSD. Still, we want to
see how many degrees of freedom we miss.

We can do the counting by looking at the EVD of a symmetric matrix, . The diagonal matrix  is specified by  degrees
of freedom, because  has rank . The first eigenvector of length 
takes up  degrees of freedom due to the normalization; the second one
takes up , as it is in addition orthogonal to the first one; for the last
eigenvector, number , we need  degrees of freedom. We do not
need to count the other eigenvectors, because they correspond to zero
eigenvalues. The total number is then


}

For large  and fixed , it follows that

Therefore, even though the rank property is useful and we will show efficient
algorithms that exploit it, it is still not a \emph{tight} property (symmetry
and hollowness included). For , the ratio
\eqref{eq:dof_ratio} is , so loosely speaking, the rank property
has 30\% determining scalars too many, which we need to set consistently.
Put differently, we need 30\% more data in order to exploit the rank property
than we need to exploit the full EDM structure. Again loosely phrased, we can
assert that for the same amount of data, the algorithms perform at least
30\% worse if we only exploit the rank property, without
\emph{EDMness}.

The one-third gap accounts for various geometrical constraints that must be
satisfied. The redundancy in the EDM representation is what makes denoising
and completion algorithms possible, and thinking in terms of degrees of freedom
gives us a fundamental understanding of what is achievable. Interestingly, the
above discussion suggests that for large  and large , little is
lost by only considering rank.

\rev{Finally, in the above discussion, for the sake of simplicity we ignored the
degrees of freedom related to absolute orientation. These degrees of freedom,
not present in the EDM, do not affect the large- behavior.}

\subsection{Summary}

Let us summarize what we have achieved in this section:
\begin{itemize}
	\item We explained how to algebraically construct an EDM given the list of
	point coordinates,
	\item We discussed the essential uniqueness of the point set; information
	about the absolute orientation of the points is irretrievably lost when
	transitioning from points to an EDM,
  \item We explained classical MDS---a simple eigenvalue-decomposition-based algorithm
  (Algorithm \ref{alg:classical_mds}) for reconstructing the original points---along with discussing parameter choices that lead to different centroids in
  reconstruction,
	\item Degrees-of-freedom provide insight into scaling behavior. We
	showed that the rank property is pretty good, but there is more to it than
	just rank.
\end{itemize}



\section{EDMs as a Practical Tool} \label{sec:edms_as_a_practical_tool}

We rarely have a perfect EDM. Not only are the entries of the measured matrix
plagued by errors, but often we can measure just a subset. There are various
sources of error in distance measurements: we already know that in NMR
spectroscopy, instead of exact distances we get intervals. Measuring distance
using received powers or TOAs is subject to noise, sampling errors and model
mismatch.

Missing entries arise because of the limited radio range, or because of the
nature of the spectrometer. Sometimes the nodes in the problem at hand are
asymmetric by definition; in microphone calibration we have two types:
microphones and calibration sources. This results in a particular block
structure of the missing entries (we will come back to this later, but you can
fast-forward to Fig.~\ref{fig:mdu} for an illustration).

It is convenient to have a single statement for both EDM approximation and EDM
completion, as the algorithms described in this section handle them at once.

\begin{problem}
\label{prob:approximation_completion}
Let . We are given a noisy observation of the distances
between  pairs of points from . That is, we have a
noisy measurement of  entries in ,

for , where  is some index set, and  absorbs
all errors. The goal is to reconstruct the point set  in the given
embedding dimension, so that the entries of  are close in some
metric to the observed entries .
\end{problem}

To concisely write down completion problems, we define the mask matrix 
as follows,

This matrix then selects elements of an EDM through a Hadamard (entrywise)
product. For example, to compute the norm of the difference between the
observed entries in  and , we write .
Furthermore, we define the indexing  to mean the restriction of
 to those entries where  is non-zero. The meaning of  is that we assign the observed part of  to the observed
part of .

\subsection{Exploiting the Rank Property} \label{sub:exploiting_the_rank_property}

Perhaps the most notable fact about EDMs is the rank property established in
Theorem \ref{thm:edm_rank}: The rank of an EDM for points living in  is
at most . This leads to conceptually simple algorithms for EDM completion
and denoising. Interestingly, these algorithms exploit only the rank of the
EDM. There is no explicit Euclidean geometry involved, at least not before
reconstructing the point set.

We have two pieces of information: a subset of potentially noisy distances,
and the desired embedding dimension of the point configuration. The latter
implies the rank property of the EDM that we aim to exploit. We may try to
alternate between enforcing these two properties, and hope that the algorithm
produces a sequence of matrices that converges to an EDM. If it does, we have
a solution. Alternatively, it may happen that we converge to a matrix with the
correct rank that is not an EDM, or that the algorithm never converges. The
pseudocode is listed in Algorithm \ref{alg:rank_complete_edm}.

\begin{algorithm}[t]
\caption{Alternating Rank-Based EDM Completion}
\label{alg:rank_complete_edm}
\begin{algorithmic}[1]
\Function{RankCompleteEDM}{}

	\State  \Comment{Initialize observed entries}
	\State  \Comment{Initialize unobserved entries}
	\Repeat
			\State 
			\State  \Comment{Enforce known entries}
			\State  \Comment{Set the diagonal to zero}
			\State  \Comment{Zero the negative entries}
	\Until{Convergence or MaxIter}
	\State \Return{\mD}
\EndFunction
\break \raggedright
\Function{EVThreshold}{}
  \State 
  \State 
  \vspace{-3mm}
  \State 
  \State \Return{}
\EndFunction
\end{algorithmic}
\end{algorithm}

A different, more powerful approach is to leverage algorithms for low rank
matrix completion developed by the compressed sensing community. For example,
OptSpace \cite{Keshavan:2010bt} is an algorithm for recovering a low-rank
matrix from noisy, incomplete data. \rev{Let us take a look at how OptSpace works.
Denote by  the rank- matrix that we seek to recover,
by  the measurement noise, and by  the mask corresponding to the measured entries; for simplicity we
choose . The measured noisy and incomplete matrix is then given as

Effectively, this sets the missing (non-observed) entries of the matrix to
zero. OptSpace aims to minimize the following cost function,

where , , and  such that . Note that  need
not be diagonal.

The cost function \eqref{eq:optspace_cost_function} is not convex, and
minimizing it is \emph{a priori} difficult \cite{Keshavan:2012tb} due to many
local minima. Nevertheless, Keshavan, Montanari and Oh \cite{Keshavan:2010bt}
show that using the gradient descent method to solve
\eqref{eq:optspace_cost_function} yields the global optimum with high
probability, provided that the descent is correctly initialized.

Let  be the SVD of
. Then we define the scaled rank- projection of  as
. The fraction of observed entries is denoted by , so
that the scaling factor compensates the smaller \emph{average} magnitude of
the entries in  in comparison with . The SVD of  is
then used to initialize the gradient descent, as detailed in Algorithm
\ref{alg:optspace}.

Two additional remarks are due in the description of OptSpace. First, it can
be shown that the performance is improved by zeroing the \emph{over-represented} rows and columns. A row (\emph{resp}. column) is over-represented
if it contains more than twice the average number of observed entries per row
(\emph{resp.} column). These heavy rows and columns bias the corresponding
singular vectors and singular values, so (perhaps surprisingly) it is better
to throw them away. We call this step ``Trim'' in Algorithm
\ref{alg:optspace}.

Second, the minimization of \eqref{eq:optspace_cost_function} does not have to
be performed for all variables at once. In \cite{Keshavan:2010bt}, the authors
first solve the easier, convex minimization for , and then with the
optimizer  fixed, they find the matrices  and  using the
gradient descent. These steps correspond to lines 6 and 7 of Algorithm
\ref{alg:optspace}.} For an application of OptSpace in calibration of
ultrasound measurement rigs, see the ``\textbf{Calibration in Ultrasound
Tomography}'' box.

\begin{algorithm}[t]
\caption{{\sc OptSpace} \cite{Keshavan:2010bt}} 
\label{alg:optspace} 
\begin{algorithmic}[1]
\Function{OptSpace}{}
	\State 
\State 
\State  First  columns of 
  \State  First  columns of 
  \State  \Comment{Eq. \eqref{eq:optspace_cost_function}}
	\State  \Comment{See the note below}
  \vspace{1mm}
	\State \Return{}
\EndFunction
\end{algorithmic}
 Line 7: gradient descent starting at 
\end{algorithm}



\begin{figure*}[t]
\fontfamily{lmss}\selectfont
\begin{spmagbox}
\textbf{Calibration in Ultrasound Tomography}
\\
\label{sub:calibration}
\fontsize{9pt}{10pt}\selectfont




\begin{multicols}{2}


The rank property of EDMs, introduced in Theorem \ref{thm:edm_rank} can be
leveraged in calibration of ultrasound tomography devices. An example device
for diagnosing breast cancer is a circular ring with thousands of ultrasound
transducers, placed around the breast \cite{dur07}. The setup is shown in Fig.
\ref{fig:ultrasound}\textsl{A}. 

Due to manufacturing errors, the sensors are not located on a perfect circle.
This uncertainty in the positions of the sensors negatively affects the
algorithms for imaging the breast. Fortunately, we can use the measured
distances between the sensors to calibrate their relative positions. We can
estimate the distances by measuring the times-of-flight (TOFs) between pairs
of transducers in a homogeneous environment, \emph{e.g.} in water.

We cannot estimate the distances between all pairs of sensors because the
sensors have limited beam widths (it is hard to manufacture omni-directional
ultrasonic sensors). Therefore, the distances between neighboring sensors are
unknown, contrary to typical SNL scenarios where only the distances between
nearby nodes can be measured. Moreover, the distances are noisy and some of
them are unreliably estimated. This yields a noisy and incomplete EDM, whose
structure is illustrated in Figure
\ref{fig:ultrasound}\textsl{B}. 

Assuming that the sensors lie in the same plane, the original EDM produced by
them would have a rank less than five. We can use the rank property and a
low-rank matrix completion method, such as OptSpace (Algorithm
\ref{alg:optspace}), to complete and denoise the measured matrix
\cite{parhizkar:2013a}. Then we can use the classical MDS in Algorithm
\ref{alg:classical_mds} to estimate the relative locations of the ultrasound
sensors. 

For reasons mentioned above, SNL-specific algorithms are suboptimal when
applied to ultrasound calibration. An algorithm based on the rank property
effectively solves the problem, and enables one to derive upper bounds on the
performance error calibration mechanism, with respect to the number of sensors
and the measurement noise. The authors in \cite{parhizkar:2013a} show that the
error vanishes as the number of sensors increases.

\includegraphics[width=3.1in]{fig5.pdf}
\caption{\fontfamily{lmss}\selectfont(A) Ultrasound transducers lie on an approximately circular ring. The ring surrounds the breast and after each transducer fires an ultrasonic signal, the sound speed distribution of the breast is estimated. A precise knowledge of the sensor locations is needed to have an accurate reconstruction of the enclosed medium. (B) Because of the limited beam width of the transducers, noise and imperfect TOF estimation methods, the measured EDM is incomplete and noisy. Gray areas show missing entries of the matrix.}
\label{fig:ultrasound}


\end{multicols}
\end{spmagbox}
\end{figure*}

\subsection{Multidimensional Scaling} \label{sub:multidimensional_scaling}

Multidimensional scaling refers to a group of techniques that, given a set of
noisy distances, find the best fitting point conformation. It was originally
proposed in psychometrics \cite{kruskal1964,torgerson1952} to visualize the
(dis-)similarities between objects. Initially, MDS was defined as the \emph{problem}
of representing distance data, but now the term is commonly used to refer to
\emph{methods} for solving the problem \cite{Borg2005}.

Various cost functions were proposed for solving MDS. In Section
\ref{sub:reconstructing_the_point_set_from_distances}, we already encountered
one method: the classical MDS. This method minimizes the Frobenius norm of the
difference between the input matrix and the Gram matrix of the points in the
target embedding dimension. 

The Gram matrix contains inner products; rather than with inner products, it
is better to directly work with the distances. A typical cost function
represents the dissimilarity of the observed distances and the distances
between the estimated point locations. An essential observation is that the feasible
set for these optimizations is not convex (EDMs with embedding dimensions
smaller than  lie on the boundary of a cone \cite{Dattorro:2011wa}, which
is a non-convex set).

A popular dissimilarity measure is \emph{raw stress}
\cite{kruskal1964b}, defined as the value of 

where  defines the set of revealed elements of the distance matrix .
The objective function can be concisely written as ; a drawback of this cost function is that it
is not globally differentiable. Approaches described in the
literature comprise iterative majorization \cite{DeLeeuw1977}, various methods
using convex analysis \cite{mathar1991} and steepest descent methods
\cite{guttman1968}.

Another well-known cost function is \emph{s-stress},

Again, we write the objective concisely as . It was first studied by Takane, Young and De Leeuw
\cite{takane1977}. Conveniently, the s-stress objective is everywhere
differentiable, but at a disadvantage that it favors large over small
distances. Gaffke and Mathar \cite{gaffke1989} propose an algorithm to find
the global minimum of the s-stress function for embedding dimension .
EDMs with this embedding dimension exceptionally constitute a convex set
\cite{Dattorro:2011wa}, but we are typically interested in embedding
dimensions much smaller than . The s-stress minimization in
\eqref{eq:s-stress} is not convex for . It was analytically shown to
have saddle points \cite{rezathesis13}, but interestingly, no analytical
non-global minimizer has been found \cite{rezathesis13}.

Browne proposed a method for computing s-stress based on Newton-Raphson root
finding \cite{Browne1987}. Glunt reports that the method by Browne  converges
to the global minimum of \eqref{eq:s-stress} in 90\% of the test cases in his
dataset\footnote{While the experimental setup of Glunt
\cite{gluntEmbedding1991} is not detailed, it was mentioned that the EDMs were
produced randomly.} \cite{gluntEmbedding1991}.

The cost function in \eqref{eq:s-stress} is separable across points  and
across coordinates , which is convenient for distributed implementations.
Parhizkar \cite{rezathesis13} proposed an alternating coordinate descent
method that leverages this separability, by updating a single coordinate of a
particular point at a time. The s-stress function restricted to the th
coordinate of the th point is a fourth-order polynomial,

where  lists the polynomial coefficients for th point
and th coordinate. For example, , that
is, four times the number of points connected to point . Expressions for
the remaining coefficients are given in \cite{rezathesis13}; \rev{in the
pseudocode (Algorithm
\ref{alg:alt_coor_desc}), we assume that these coefficients are returned by the
function ``GetQuadricCoeffs'', given the noisy incomplete matrix , the
observation mask  and the dimensionality }. The global minimizer of
\eqref{eq:4th_order} can be found analytically by calculating the roots of its
derivative (a cubic). The process is then repeated over all coordinates ,
and points , until convergence. The resulting algorithm is remarkably
simple, yet empirically converges fast. It naturally lends itself to a
distributed implementation. We spell it out in Algorithm
\ref{alg:alt_coor_desc}.

When applied to a large dataset of random, noiseless and complete distance
matrices, Algorithm \ref{alg:alt_coor_desc} converges to the global minimum of
\eqref{eq:s-stress} in more than 99\% of the cases \cite{rezathesis13}.

\begin{algorithm}[t]
\caption{Alternating Descent \cite{rezathesis13}}
\label{alg:alt_coor_desc} 
\begin{algorithmic}[1]
\Function{AlternatingDescent}{}

	\State  \Comment{Initialize the point set}
	\Repeat
		\For{} \Comment{Points}
			\For{} \Comment{Coordinates}
				\State 
				\State   \Comment{Eq. \eqref{eq:4th_order}}
			\EndFor
		\EndFor
	\Until{Convergence or MaxIter}
	\State \Return{\mX}
\EndFunction
\end{algorithmic}
\end{algorithm}


\subsection{Semidefinite Programming} \label{sub:semidefinite_programming}

Recall the characterization of EDMs \eqref{eq:sdp_equivalence} in Theorem
\ref{thm:sdp_equivalence}. It states that  is an EDM if and only if the
corresponding geometrically centered Gram matrix  is
positive-semidefinite. Thus, it establishes a one-to-one correspondence
between the cone of EDMs, denoted by , and the intersection of the
symmetric positive-semidefinite cone  with the geometrically centered
cone . The latter is defined as the set of all symmetric matrices
whose column sum vanishes,


We can use this correspondence to
cast EDM completion and approximation as semidefinite programs. While
\eqref{eq:sdp_equivalence} describes an EDM of an -point configuration in
any dimension, we are often interested in situations where . It is
easy to adjust for this case by requiring that the rank of the centered Gram
matrix be bounded. One can verify that

when . That is, EDMs with a particular embedding dimension  are
completely characterized by the rank and definiteness of .

Now we can write the following rank-constrained semidefinite program for
solving Problem \ref{prob:approximation_completion},

The second constraint is just a shorthand for writing . We note that this is equivalent to MDS with the s-stress
cost function, thanks to the rank characterization
\eqref{eq:lower_dim_characterization}.

Unfortunately, the rank property makes the feasible set in
\eqref{eq:rank_constrained_sdp} non-convex, and solving it exactly becomes
difficult. This makes sense, as we know that s-stress is not convex.
Nevertheless, we may \emph{relax} the hard problem, by simply omitting the
rank constraint, and hope to obtain a solution with the correct dimensionality,

We call \eqref{eq:relaxed_sdp} a semidefinite relaxation (SDR) of the
rank-constrained program
\eqref{eq:rank_constrained_sdp}. 

The constraint , or equivalently, , means
that there are no strictly positive definite solutions ( has a nullspace,
so at least one eigenvalue must be zero). In other words, there exist no
strictly feasible points
\cite{Krislock:2012xx}. This may pose a numerical problem, especially for
various interior point methods. The idea is then to reduce the size of the
Gram matrix through an invertible transformation, somehow removing the part of
it responsible for the nullspace. \rev{In what follows, we describe how to
construct this smaller Gram matrix.} 

A different, equivalent way to phrase the multiplicative characterization
\eqref{eq:sdp_equivalence} is the following statement: a symmetric hollow
matrix  is an EDM if and only if it is negative semidefinite on
 (on all vectors  such that ). Let
us construct an orthonormal basis for this orthogonal complement---a subspace
of dimension ---and arrange it in the columns of matrix . We demand


There are many possible choices for , but all of them obey that
. The following choice is given in \cite{Alfakih1999},

where  and .

\rev{With the help of the matrix , we can now construct the sought Gramian
with reduced dimensions.} For an EDM ,
 is an  PSD matrix. This
can be verified by substituting \eqref{eq:gram_small} in
\eqref{eq:edm_gram_assemble}. Additionally, we have that

Indeed,   is an invertible mapping from
 to  whose inverse is exactly . Using
these notations we can write down an equivalent optimization program that is
numerically more stable than \eqref{eq:relaxed_sdp} \cite{Alfakih1999}:


On the one hand, with the above transformation the constraint  became implicit in the objective, as  by \eqref{eq:V_demands}; on the other hand, the feasible set is
now the full semidefinite cone .

Still, as Krislock \& Wolkowicz mention \cite{Krislock:2012xx}, by omitting
the rank constraint we allow the points to move about in a larger space, so we
may end up with a higher-dimensional solution even if there is a completion in
dimension .

There exist various heuristics for promoting lower rank. One such heuristic
involves the trace norm---the convex envelope of rank. The trace or nuclear
norm is studied extensively by the compressed sensing community. In contrast
to the common wisdom in compressed sensing, the trick here is to maximize the
trace norm, not to minimize it. The mechanics are as follows: maximizing the
sum of squared distances between the points will stretch the configuration as
much as possible, subject to available constraints. But stretching favors
smaller affine dimensions (imagine pulling out a roll of paper, or stretching
a bent string). Maximizing the sum of squared distances can be rewritten as
maximizing the sum of norms in a centered point configuration---but that is
exactly the trace of the Gram matrix  \cite{Weinberger2004}. This idea has been successfully put to
work by Weinberger and Saul \cite{Weinberger2004} in manifold learning, and by
Biswas \emph{et al.} in SNL \cite{Biswas:2006cm}.

\begin{algorithm}[t]
\caption{{Semidefinite Relaxation (Matlab/CVX)}}
\vspace{-5mm}
\begin{lstlisting}
function [EDM, X] = sdr_complete_edm(D, W, lambda)

n = size(D, 1);
x = -1/(n + sqrt(n));
y = -1/sqrt(n);
V = [y*ones(1, n-1); x*ones(n-1) + eye(n-1)];
e = ones(n, 1);

cvx_begin sdp
    variable G(n-1, n-1) symmetric;
    B = V*G*V';
    E = diag(B)*e' + e*diag(B)' - 2*B;
    maximize trace(G) ...
           - lambda * norm(W .* (E - D), 'fro');
    subject to
        G >= 0;
cvx_end

[U, S, V] = svd(B);
EDM = diag(B)*e' + e*diag(B)' - 2*B;
X = sqrt(S)*V';
\end{lstlisting}
\label{alg:SDR_matlab}
\end{algorithm}


Noting that  because , we write the following SDR,

Here we opted to include the data fidelity term in the Lagrangian form, as
proposed by Biswas
\cite{Biswas:2006cm}, but it could also be moved to constraints. Finally, in all of
the above relaxations, it is straightforward to include upper and lower bounds
on the distances. Because the bounds are linear constraints, the resulting
programs remain convex; this is particularly useful in the molecular
conformation problem. A Matlab/CVX \cite{cvx,gb08} implementation of the SDR
\eqref{eq:sdp_trace_maximization} is given in Algorithm \ref{alg:SDR_matlab}.



\subsection{Multidimensional Unfolding: A Special Case of Completion} \label{sub:box_unfolding}

\begin{figure}
\centering
\includegraphics[width=3.5in]{fig6.pdf}
\caption{Microphone calibration as an example of MDU. We can measure only the
propagation times from acoustic sources at unknown locations, to microphones
at unknown locations. The corresponding revealed part of the EDM has a
particular off-diagonal structure, leading to a special case of EDM
completion.}
\label{fig:mdu}
\end{figure}

Imagine that we partition the point set into two subsets, and that we can
measure the distances between the points belonging to different subsets, but
not between the points in the same subset. Metric multidimensional unfolding (MDU)
\cite{Schonemann:1970wd} refers to this special case of EDM completion. 

MDU is relevant for position calibration of ad-hoc sensor networks, in
particular of microphones. Consider an ad-hoc array of  microphones at
unknown locations. We can measure the distances to  point sources, also at
unknown locations, for example by emitting a pulse (we assume that the sources
and the microphones are synchronized). We can always permute the points so
that the matrix assumes the structure shown in Fig. \ref{fig:mdu}, with the
unknown entries in two diagonal blocks. This is a standard scenario described
for example in \cite{Crocco:2012eu}.

One of the early approaches to metric MDU is that of Sch\"onemann
\cite{Schonemann:1970wd}. We go through the steps of the algorithm, and then
explain how to solve the problem using the EDM toolbox. The goal is to make a
comparison, and emphasize the universality and simplicity of the introduced
tools.


\begin{figure}[t!]
  \centering
  \includegraphics[width=3.5in]{fig7.pdf}
  \caption{Comparison of different algorithms applied to completing an EDM
  with random deletions. For every number of deletions, we generated 2000
  realizations of 20 points uniformly at random in a unit square. Distances to
  delete were chosen uniformly at random among the resulting  pairs; \rev{20 deletions correspond to  10\% of the number of
  distance pairs and to 5\% of the number of matrix entries; 150 deletions
  correspond to  80\% of the distance pairs and to  38\% of
  the number of matrix entries.} Success was declared if the Frobenius norm of
  the error between the estimated matrix and the true EDM was less than 1\% of
  the Frobenius norm of the true EDM.}
  \label{fig:success_random}
\end{figure}


Denote by  the unknown microphone locations,
and by  the unknown source locations. The
distance between the th microphone and th source is

so that in analogy with \eqref{eq:edm_assemble} we have 

where we overloaded the  operator in a natural way. We use  to
avoid confusion with the standard Euclidean . Consider now two geometric
centering matrices of sizes  and , denoted  and . Similarly to \eqref{eq:centering_derivation}, we have

This means that

is a matrix of inner products between vectors  and .
We used tildes to differentiate this from \emph{real} inner products betwen
 and , because in \eqref{eq:quasi_inner_product}, the points in
 and  are referenced to different coordinate systems. The
centroids  and  generally do not coincide. There are different
ways to decompose  into a product of two full rank matrices, call
them  and ,

We could for example use the SVD, , and set
 and . Any two such decompositions are
linked by some invertible transformation ,

We can now write down the conversion rule from what we can measure to what we
can compute,

where  and  can be computed according to
\eqref{eq:quasi_gram_decomposition_AB}. Because we cannot reconstruct the
absolute position of the point set, we can arbitrarily set , and
. Recapitulating, we have that

and the problem is reduced to computing  and  so that
\eqref{eq:mdu_equivalent} hold, or in other words, so that the right hand
side be consistent with the data . We reduced MDU to a relatively
small problem: in 3D, we need to compute only ten scalars. Sch\"onemann
\cite{Schonemann:1970wd} gives an algebraic method to find these parameters,
and mentions the possibility of least squares, while Crocco, Bue and Murino
\cite{Crocco:2012eu} propose a different approach using non-linear least
squares.


This procedure seems quite convoluted. Rather, we see MDU as a special case of
matrix completion, with the structure illustrated in Fig.
\ref{fig:mdu}.

More concretely, represent the microphones and the sources by a set of  points, ascribed to the columns of matrix . Then
 has a special structure as seen in Fig. \ref{fig:mdu},


We define the mask matrix for MDU as

With this matrix, we can simply invoke the SDR in Algorithm
\ref{alg:SDR_matlab}. We could also use Algorithm
\ref{alg:rank_complete_edm}, or Algorithm \ref{alg:alt_coor_desc}. Performance
of different algorithms is compared in Section
\ref{sub:performance_comparison_of_algorithms}.


It is worth mentioning that SNL specific algorithms that exploit the
particular graph induced by limited range communication do not perform well on
MDU. This is because the structure of the missing entries in MDU is in a
certain sense opposite to the one of SNL. 





\subsection{Performance Comparison of Algorithms} \label{sub:performance_comparison_of_algorithms}



\begin{figure}[t!]
\centering
   \includegraphics[width=3.5in]{fig8.pdf}
   \caption{Comparison of different algorithms applied to multidimensional
   unfolding with varying number of acoustic events . For every number of
   acoustic events, we generated 3000 realizations of  microphone
   locations uniformly at random in a unit cube. \rev{Percentage of missing
   matrix entries is given as , so that the ticks on
   the abscissa correspond to  (non-monotonic in 
   with the minimum for ).} Success was declared if the Frobenius norm of
   the error between the estimated matrix and the true EDM was less than 1\%
   of the Frobenius norm of the true EDM.}
   \label{fig:success_mdu}
\end{figure}

We compare the described algorithms in two different EDM completion settings.
In the first experiment (Figs. \ref{fig:success_random} and
\ref{fig:noise_random}), the entries to delete are chosen uniformly at random.
The second experiment (Figs. \ref{fig:success_mdu} and \ref{fig:noise_mdu})
tests performance in MDU, where the non-observed entries are highly
structured. \rev{In Figs. \ref{fig:success_random} and \ref{fig:success_mdu}, we
assume that the observed entries are known exactly, and we plot the success
rate (percentage of accurate EDM reconstructions) against the number of
deletions in the first case, and the number of calibration events in the
second case. Accurate reconstruction is defined in terms of the relative
error. Let  be the true, and  the estimated EDM. The relative
error is then , and we declare success if
this error is below .

To generate Figs. \ref{fig:noise_random} and \ref{fig:noise_mdu} we varied the
amount of random, uniformly distributed jitter added to the distances, and for
each jitter level we plotted the relative error. The exact values of
intermediate curves are less important than the curves for the smallest and
the largest jitter, and the overall shape of the ensemble.

\begin{figure}[t!]
\centering
\includegraphics[width=3.5in]{fig9.pdf}
\caption{\rev{Comparison of different algorithms applied to completing an EDM with
random deletions and noisy distances. For every number of deletions, we
generated 1000 realizations of 20 points uniformly at random in a unit square.
In addition to the number of deletions, we varied the amount of jitter added
to the distances. Jitter was drawn from a centered uniform distribution, with
the level increasing in the direction of the arrow, from 
(no jitter) for the darkest curve at the bottom, to 
for the lightest curve at the top, in 11 increments. For every jitter level,
we plotted the mean relative error  for all algorithms.}}
\label{fig:noise_random}
\end{figure}

A number of observations can be made about the performance of algorithms.
Notably, OptSpace (Algorithm \ref{alg:optspace}) does not perform well
for randomly deleted entries when ; it was designed for larger
matrices. For this matrix size, the mean relative reconstruction error
achieved by OptSpace is the worst of all algorithms (Fig.
\ref{fig:noise_random}). In fact, the relative error in the noiseless case was
rarely below the success threshold (set to ) so we omitted the corresponding near-zero curve from Fig.
\ref{fig:success_random}. Furthermore, OptSpace assumes that the pattern of
missing entries is random; in the case of a blocked deterministic structure
associated with MDU, it never yields a satisfactory completion.

On the other hand, when the unobserved entries are randomly scattered in the
matrix, and the matrix is large---in the ultrasonic calibration example the
number of sensors  was  or more---OptSpace is a very fast and
attractive algorithm. To fully exploit OptSpace,  should be even larger, in
the thousands or tens of thousands.

SDR (Algorithm \ref{alg:SDR_matlab}) performs well in all scenarios. For both
the random deletions and the MDU, it has the highest success rate, and it
behaves well with respect to noise. Alternating coordinate descent (Algorithm
\ref{alg:alt_coor_desc}) performs slightly better in noise for small number of
deletions and large number of calibration events, but Figs.
\ref{fig:success_random} and \ref{fig:success_mdu} indicate that for certain
realizations of the point set it gives large errors. If the worst-case
performance is critical, SDR is a better choice. We note that in the
experiments involving the SDR, we have set the multiplier  in
\ref{eq:sdp_trace_maximization} to the square root of the number of missing
entries. This choice was empirically found to perform well.

The main drawback of SDR is speed; it is the slowest among the tested
algorithms. To solve the semidefinite program we used CVX \cite{cvx,gb08}, a
Matlab interface to various interior point methods. For larger matrices
(\emph{e.g.}, ), CVX runs out of memory on a desktop computer, and
essentially never finishes. Matlab implementations of alternating coordinate
descent, rank alternation (Algorithm \ref{alg:rank_complete_edm}), and
OptSpace are all much faster.

The microphone calibration algorithm by Crocco \cite{Crocco:2012eu} performs
equally well for any number of acoustic events. This may be explained by the
fact that it always reduces the problem to ten unknowns. It is an attractive
choice for practical calibration problems with a smaller number of calibration
events. Algorithm's success rate can be further improved if one is prepared to
run it for many random initializations of the non-linear
optimization step.

\begin{figure}[t!]
\centering
\includegraphics[width=3.5in]{fig10.pdf}
\caption{\rev{Comparison of different algorithms applied to multidimensional
unfolding with varying number of acoustic events  and noisy distances. For
every number of acoustic events, we generated 1000 realizations of 
microphone locations uniformly at random in a unit cube. In addition to the
number of acoustic events, we varied the amount of random jitter added to the
distances, with the same parameters as in Fig.~\ref{fig:noise_random}. For
every jitter level, we plotted the mean relative error  for all algorithms.}}
\label{fig:noise_mdu}
\end{figure}

Interesting behavior can be observed for the rank alternation in MDU. Figs.
\ref{fig:success_mdu} and \ref{fig:noise_mdu} both show that at low noise
levels, the performance of the rank alternation becomes worse with the number
of acoustic events. At first glance, this may seem counterintuitive, as more
acoustic events means more information; one could simply ignore some of them,
and perform at least equally well as with fewer events. But this reasoning
presumes that the method is \emph{aware} of the geometrical meaning of the
matrix entries; on the contrary, rank alternation is using only rank.
Therefore, even if the percentage of the observed matrix entries grows until a
certain point, the size of the structured blocks of unknown entries grows as
well (and the percentage of known entries in columns/rows corresponding to
acoustic events decreases). This makes it harder for a method that does not
use geometric relationships to complete the matrix. A loose comparison can be
made to image inpainting: If the pixels are missing randomly, many methods
will do a good job; but if a large patch is missing, we cannot do much without
additional structure (in our case geometry), no matter how large the rest of
the image is.

To summarize, for smaller matrices the SDR seems to be the best overall
choice. For large matrices the SDR becomes too slow and one should turn to
alternating coordinate descent, rank alternation or OptSpace. Rank alternation
is the simplest algorithm, but alternating coordinate descent performs better.
For very large matrices ( on the order of thousands or tens of thousands),
OptSpace becomes the most attractive solution.} We note that we deliberately
refrained from making detailed running time comparisons, due to the diverse
implementations of the algorithms.





\subsection{Summary} 

In this section we discussed:
\begin{itemize}
	\item The problem statement for EDM completion and denoising, and how to
	easily exploit the rank property (Algorithm
	\ref{alg:rank_complete_edm}),
	\item Standard objective functions in MDS: raw stress and s-stress, and
	 a simple algorithm to minimize s-stress (Algorithm
	\ref{alg:alt_coor_desc}),
	\item Different semidefinite relaxations that exploit the connection
	between EDMs and PSD matrices,
	\item Multidimensional unfolding, and how to solve it efficiently using
	EDM completion,
  \item Performance of the introduced algorithms in two very different
  scenarios: EDM completion with randomly unobserved entries, and EDM
  completion with deterministic block structure of unobserved entries (MDU).
\end{itemize}






\section{Unlabeled Distances} \label{sec:unlabeled_distances}

In certain applications we can measure the distances between the points, but
we do not know the correct labeling. That is, we know all the entries of an
EDM, but we do not know how to arrange them in the matrix. As illustrated in
Fig. \ref{fig:unlabeled_distances}\textsl{A}, we can imagine having a set of
sticks of various lengths. The task is to work out the correct way to connect
the ends of different sticks so that no stick is left hanging open-ended.

In this section we exploit the fact that in many cases, distance labeling is
not essential. For most point configurations, there is no other set of points
that can generate the corresponding set of distances, up to a rigid
transformation.

Localization from unlabeled distances is relevant in various calibration
scenarios where we cannot tell apart distance measurements belonging to
different points in space. This can occur when we measure times of arrivals of
echoes, which correspond to distances between the microphones and the image
sources (see Fig. \ref{fig:room_hearing_is})
\cite{Dokmanic:2014tc, Dokmanic:2013dz}. Somewhat surprisingly, the same
problem of unlabeled distances appears in sparse phase retrieval; to see how,
take a look at the ``\textbf{Phase Retrieval}'' box.

No efficient algorithm currently exists for localization from unlabeled
distances in the general case of noisy distances. \rev{We should mention,
however, a recent polynomial-time algorithm (albeit of a high degree) by
Gujarathi and \emph{et al.} \cite{Gujarathi:2014cz}, that can reconstruct
relatively large point sets from unordered, noiseless distance data.}

At any rate, the number of assignments to test is sometimes sufficiently small
so that an exhaustive search does not present a problem. We can then use EDMs
to find the best labeling. The key to the unknown permutation problem is the
following fact.

\begin{thm}
	\label{thm:unlabeled}
  Draw  independently from some
  absolutely continuous probability distribution (\emph{e.g.} uniformly at
  random) on . Then with probability 1, the obtained
  point configuration is the unique \rev{(up to a rigid transformation)} point
  configuration in  that generates the set of distances
  .
\end{thm}

This fact is a simple consequence of a result by Boutin and Kemper
\cite{Boutin:2004gb} who give a characterization of point sets reconstructible
from unlabeled distances.

Figs. \ref{fig:unlabeled_distances}\textsl{B} and
\ref{fig:unlabeled_distances}\textsl{C} show two possible arrangements of the
set of distances in a tentative EDM; the only difference is that the two
hatched entries are swapped. But this simple swap is not harmless: there is no
way to attach the last stick in Fig. \ref{fig:unlabeled_distances}\textsl{D},
while keeping the remaining triangles consistent. We could do it in a higher
embedding dimension, but we insist on realizing it in the plane.

\begin{figure}[t]
\centering
\includegraphics[width=3.5in]{fig11.pdf}
\caption{Illustration of the uniqueness of EDMs for unlabeled distances. A set
of unlabeled distance (A) is distributed in two different ways in a tentative
EDM with embedding dimension 2 (B and C). The correct assignment yields the
matrix with the expected rank (C), and the point set is easily realized in the
plane (E). On the contrary, swapping just two distances (hatched squares in
(B) and (C)) makes it impossible to realize the point set in the plane (D).
Triangles that do not coincide with the swapped edges can still be placed, but
in the end we are left with a hanging orange stick that cannot attach itself
to any of the five nodes.}
\label{fig:unlabeled_distances}
\end{figure}

What Theorem \ref{thm:unlabeled} does not tell us is how to identify the
correct labeling. But we know that for most sets of distances, only one
(correct!) permutation can be realized in the given embedding dimension. Of
course, if all the labelings are unknown and we have no good heuristics to
trim the solution space, finding the correct labeling is difficult, as noted
in \cite{Gujarathi:2014cz}. Yet there are interesting situations where this
search is feasible because we can augment the EDM point by point. We describe
one such situation in the next subsection.

\begin{figure*}[t]
\fontfamily{lmss}\selectfont
\begin{spmagbox}
\textbf{EDM Perspective on Sparse Phase Retrieval (The Unexpected Distance Structure)}
\\
\label{sub:box_phase_retrieval_by_edms}
\fontsize{9pt}{10pt}\selectfont
\begin{multicols}{2}

In many cases, it is easier to measure a signal in the Fourier domain.
Unfortunately, it is common in these scenarios that we can only reliably
measure the magnitude of the Fourier transform (FT). We would like to recover
the signal of interest from just the magnitude of its FT, hence the name
\emph{phase retrieval}. X-ray crystallography
\cite{Millane:1990pt} and speckle imaging in astronomy
\cite{Beavers:1989tj} are classic examples of phase retrieval problems. In
both of these applications the signal is spatially sparse. We can model it as

 

where  are the amplitudes and   the locations of the  Dirac
deltas in the signal. In what follows, we discuss the problem on
1-dimensional domains, that is , knowing that a multidimensional
phase retrieval problem can be solved by solving many 1-dimensional problems
\cite{Ranieri:2013tx}.

Note that measuring the magnitude of the FT of  is equivalent to
measuring its autocorrelation function (ACF). For a sparse , the ACF
is also sparse and given as

where we note the presence of differences between the locations  in the
support of the ACF. As  is symmetric, we do not know the order of
, and so we can only know these differences up to a sign, which is
equivalent to knowing the distances .

For the following reasons, we focus on the recovery of the support of the
signal  from the support of the ACF : i) in certain
applications, the amplitudes  may be all equal, thus limiting their role
in the reconstruction; ii) knowing the support of  and its ACF
is sufficient to exactly recover the signal 
\cite{Ranieri:2013tx}. 

The recovery of the support of  from the one of  corresponds
to the localization of a set of  points from their \emph{unlabeled}
distances: we have access to all the pairwise distances but we do not know
which pair of points corresponds to any given distance. This can be recognized
as an instance of the \emph{turnpike problem}, whose computational complexity
is believed not to be NP-hard but for which no polynomial time algorithm is
known \cite{Lemke:2002um}.

From an EDM perspective, we can design a reconstruction algorithm recovering
the support of the signal  by labeling the distances obtained from the
ACF such that the resulting EDM has rank smaller or equal than 3. This can be
regarded as unidimensional scaling with unlabeled distances, and the algorithm
to solve it is similar to echo sorting (Algorithm \ref{alg:echo_sorting}).


\vspace{5mm}

\includegraphics[width=3.1in]{fig12.pdf}

\caption{\fontfamily{lmss}\selectfont A graphical representation of the phase retrieval problem for
1-dimensional sparse signals. (A) We measure the ACF of the signal and we
recover a set of distances (\emph{sticks} in Fig.
\ref{fig:unlabeled_distances}) from its support. (B) These are the unlabeled
distances between all the pairs of Dirac deltas in the signal . We
exactly recover the support of the signal if we correctly label the distances.}

\end{multicols}
\end{spmagbox}
\end{figure*}

\subsection{Hearing the Shape of a Room \protect{\cite{Dokmanic:2013dz}}} \label{sub:hearing_the_shape_of_a_room}

An important application of EDMs with unlabeled distances is the
reconstruction of the room shape from echoes. An acoustic setup is shown in
Fig. \ref{fig:room_hearing_is}\textsl{A}, but one could also use radio
signals. Microphones pick up the convolution of the sound emitted by the
loudspeaker with the room impulse response (RIR), which can be estimated by
knowing the emitted sound. An example RIR recorded by one of the microphones
is illustrated in Fig.
\ref{fig:room_hearing_is}\textsl{B}, with peaks highlighted in green.
Some of these peaks are first-order echoes coming from different walls, and
some are higher-order echoes or just noise.

Echoes are linked to the room geometry by the image source model
\cite{Allen:1979ua}. According to this model, we can replace echoes by image
sources (IS)---mirror images of the true sources across the corresponding walls.
Position of the image source of  corresponding to wall  is computed as

\begin{figure}[t]
\centering
\includegraphics[width=3.4in]{fig13.pdf}
\caption{(A) Illustration of the image source model for first- and second-order
echoes. Vector  is the outward-pointing unit normal associated with the
th wall. Stars denote the image sources, and  is the image
source corresponding to the second-order echo. Sound rays corresponding to
first reflections are shown in blue, and the ray corresponding to the
second-order reflection is shown in red. (B) Early part of a typical recorded
room impulse response.}
\label{fig:room_hearing_is}
\end{figure}


where  is any point on the th wall, and  is the unit normal
vector associated with the th wall, see Fig.
\ref{fig:room_hearing_is}\textsl{A}.
A convex room with planar walls is completely determined by the locations of
first-order ISs \cite{Dokmanic:2013dz}, so by reconstructing their locations,
we actually reconstruct the room's geometry.

We assume that the loudspeaker and the microphones are synchronized so that
the times at which the echoes arrive directly correspond to distances. The
challenge is that the distances---the green peaks in
Fig.~\ref{fig:room_hearing_is}\textsl{B}---are unlabeled: it might happen that
the th peak in the RIR from microphone 1 and the th peak in the RIR from
microphone 2 come from different walls, especially for larger microphone
arrays. Thus, we have to address the problem of echo sorting, in order to
group peaks corresponding to the same image source in RIRs from different
microphones.

Assuming that we know the pairwise distances between the microphones , we can create an EDM corresponding to the microphone
array. Because echoes correspond to image sources, and image sources are just
points in space, we attempt to grow that EDM by adding one point---an image
source---at a time. To do that, we pick one echo from every microphone's
impulse response, augment the EDM based on echo arrival times, and check how
far the augmented matrix is from an EDM with embedding dimension three, as we
work in 3D space. The distance from an EDM is measured with s-stress cost
function. It was shown in \cite{Dokmanic:2013dz} that a variant of Theorem
\ref{thm:unlabeled} applies to image sources when microphones are thrown at
random. Therefore, if the augmented matrix satisfies the EDM properties,
almost surely we have found a good image source. With probability 1, no
other combination of points could have generated the used distances.

The main reason for using EDMs and s-stress instead of, for instance, the rank
property, is that we get robust algorithms. Echo arrival times are corrupted
with various errors, and relying on the rank is too brittle. It was verified
experimentally \cite{Dokmanic:2013dz} that EDMs and s-stress yield a very
robust filter for the correct combinations of echoes.

Thus we may try all feasible combinations of echoes, and expect to get exactly
one ``good'' combination for every image source that is ``visible'' in the
impulse responses. In this case, as we are only adding a single point, the
search space is small enough to be rapidly traversed exhaustively. Geometric
considerations allow for a further trimming of the search space: \rev{because we
know the diameter of the microphone array, we know that an echo from a
particular wall must arrive at all the microphones within a temporal window
corresponding to the array's diameter.}

The procedure is as follows: collect all echo arrival times received by the
th microphone in the set , and fix  corresponding to a
particular image source. Then Algorithm \ref{alg:echo_sorting} finds echoes in
other microphones' RIRs that correspond to this same image source. Once we
group all the peaks corresponding to one image source, we can determine its
location by multilateration (\emph{e.g.} by running the classical MDS), and
then repeat the process for other echoes in .

\rev{To get a ballpark idea of the number of combinations to test, suppose that we
detect 20 echoes per microphone\footnote{\rev{We do not need to look beyond
early echoes corresponding to at most three bounces. This is convenient, as
echoes of higher orders are challenging or impossible to isolate.}}, and that the
diameter of the five-microphone array is  m. Thus for every peak time
 we have to look for peaks in the remaining four microphones that arrived
within a window around  of length , where  m/s is the
speed of sound. This is approximately  ms, and in a typical room we can
expect about five early echoes within a window of that duration. Thus we have
to compute the s-stress for  matrices of size , which can be done in a matter of seconds on a desktop computer. In fact,
once we assign an echo to an image source, we can exclude it from further
testing, so the number of combinations can be further reduced.}

\begin{algorithm}[t]
\caption{Echo Sorting \cite{Dokmanic:2013dz}}
\label{alg:echo_sorting}
\begin{algorithmic}[1]
\Function{EchoSort}{}
	\State 
	\State 
	\ForAll{}
		\State  \Comment{ is the sound speed}
		\State 
		\If{}
			\State 
			\State 
		\EndIf
	\EndFor
	\State \Return 
\EndFunction
\end{algorithmic}
\end{algorithm}
Algorithm \ref{alg:echo_sorting} was used to reconstruct rooms with centimeter
precision \cite{Dokmanic:2013dz} with one loudspeaker and an array of five
microphones. The same algorithm also enables a dual application: indoor
localization of an acoustic source using only one microphone---a feat not
possible if we are not in a room \cite{Parhizkar:2014kn}.



\subsection{Summary} 

To summarize this section:
\begin{itemize}
	\item We explained that for most point sets, the distances they generate
	are unique; there are no other point sets generating the same distances,
  \item In room reconstruction from echoes, we need to identify the correct
  labeling of the distances to image sources. EDMs act as a robust filter for
  echoes coming from the same image source,
	\item Sparse phase retrieval can be cast as a distance problem, too. The
	support of the ACF gives us distances between the deltas in the original
	signal. Echo sorting can be adapted to solve the problem from the EDM
	perspective.
\end{itemize}





\begin{table}[tb]
\centering
\caption{Applications of EDMs with different twists}
\begin{tabular}{@{}r | p{1.4cm} p{1.4cm} p{1.4cm} }
\toprule[1.2pt]
Application & missing \hspace{1mm}distances & noisy distances & unlabeled distances \\
\midrule
Wireless sensor networks  & \ding{52} & \ding{52} &  \\
Molecular conformation & \ding{52} & \ding{52} &  \\
Hearing the shape of a room &  & \ding{52} & \ding{52}  \\
Indoor localization &  & \ding{52} & \ding{52} \\
Calibration & \ding{52} & \ding{52} &  \\
Sparse phase retrieval &  & \ding{52} & \ding{52} \\
  \bottomrule[1.2pt]
\end{tabular}

\label{tab:EDMuncer}
\end{table}



\section{Ideas for Future Research} \label{sec:ideas_for_future_research}

Even problems that at first glance seem to have little to do with EDMs,
sometimes reveal a distance structure when you look closely. A good example is
sparse phase retrieval.

The purpose of this paper is to convince the reader that Euclidean distance
matrices are powerful objects with a multitude of applications (Table
\ref{tab:EDMuncer} lists various flavors), and that they should belong to any
practitioner's toolbox. We have an impression that the power of EDMs and the
associated algorithms has not been sufficiently recognized in the signal
processing community, and our goal is to provide a good starting reference. To
this end, and perhaps to inspire new research directions, we list several
EDM-related problems that we are curious about and believe are important.

\paragraph{Distance matrices on manifolds} If the points lie on a particular
manifold, what can be said about their distance matrix? We know that if the
points are on a circle, the EDM has rank three instead of four, and this
generalizes to hyperspheres \cite{gower1}. But what about more general
manifolds? Are there invertible transforms of the data or of the Gram matrix
that yield EDMs with a lower rank than the embedding dimension suggests? What
about different distances, \emph{e.g.} the geodesic distance on the manifold? Answers
to these questions have immediate applications in machine learning, where the
data can be approximately assumed to be on a smooth surface
\cite{Tenenbaum2000}.


\paragraph{Projections of EDMs on lower dimensional subspaces} What happens
to an EDM when we project its generating points to a lower dimensional space?
What is the minimum number of projections that we need to be able to
reconstruct the original point set? Answers to these questions have
significant impact on imaging applications such as X-ray crystallography and
seismic imaging. What happens when we only have partial distance observations
in various subspaces? What are other useful low-dimensional structures on
which we can observe the high-dimensional distance data?


\paragraph{Efficient algorithms for distance labeling} Without
application-specific heuristics to trim down the search space, identifying
correct labeling of the distances quickly becomes an arduous task. Can we
identify scenarios for which there are efficient labeling algorithms? What
happens when we do not have the labeling, but we also do not have the complete
collection of \emph{sticks}? What can we say about uniqueness of incomplete
unlabeled distance sets? \rev{Some of the questions have been answered by
Gujarathi \cite{Gujarathi:2014cz}, but many remain. The quest is on for faster
algorithms, as well as algorithms that can handle noisy distances.

In particular, if the noise distribution on the unlabeled distances is known,
what can we say about the distribution of the reconstructed point set (taking
in some sense the \emph{best} reconstruction over all labelings)? Is it
compact, or we can \emph{jump} to totally wrong assignments with positive
probability?}


\paragraph{Analytical local minimum of s-stress} Everyone agrees that there
are many, but to the best of our knowledge, no analytical minimum of s-stress
has yet been found.



\section{Conclusion}

At the end of this tutorial, we hope that we succeeded in showing how
universally useful EDMs are, and that we inspired readers coming across this
material for the first time to dig deeper. Distance measurements are so common
that a simple, yet sophisticated tool like EDMs deserves attention. A good
example is the semidefinite relaxation: even though it is generic, it is the
best performing algorithm for the specific problem of ad-hoc microphone array
localization. Continuing research on this topic will bring new revolutions,
like it did in the 80s in crystallography. Perhaps the next one will be fueled
by solving the labeling problem.

\section*{Acknowledgments}

We would like to thank Dr. Farid M. Naini: without his help, the numerical
simulations for this paper would have taken forever. We would also like to
thank the anonymous reviewers for their numerous insightful suggestions that
have improved the revised manuscript.

\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}\typeout{** loaded for the language `#1'. Using the pattern for}\typeout{** the default language instead.}\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{Patwari:2005kc}
N.~Patwari, J.~N. Ash, S.~Kyperountas, A.~O. Hero, R.~L. Moses, and N.~S.
  Correal, ``{Locating the Nodes: Cooperative Localization in Wireless Sensor
  Networks},'' \emph{IEEE Signal Process. Mag.}, vol.~22, no.~4, pp. 54--69,
  Jul. 2005.

\bibitem{Alfakih1999}
A.~Y. Alfakih, A.~Khandani, and H.~Wolkowicz, ``Solving {E}uclidean Distance
  Matrix Completion Problems via Semidefinite Programming,''
  \emph{Comput. Optim. Appl.}, vol.~12, no. 1-3, pp.
  13--30, Jan. 1999.

\bibitem{dohetry2001}
L.~Doherty, K.~Pister, and L.~El~Ghaoui, ``Convex Position Estimation in
  Wireless Sensor Networks,'' in \emph{Proc. IEEE INFOCOM}, vol.~3, 2001, pp.
  1655--1663.

\bibitem{Biswas2004}
P.~Biswas and Y.~Ye, ``Semidefinite Programming For Ad Hoc Wireless Sensor
  Network Localization,'' in \emph{Proc. ACM/IEEE IPSN}, 2004, pp. 46--54.



\bibitem{Havel1985281}
T.~F. Havel and K.~W\"{u}thrich, ``An Evaluation of the Combined Use of Nuclear
  Magnetic Resonance and Distance Geometry for the Determination of Protein
  Conformations in Solution,'' \emph{J. Mol. Biol.}, vol. 182,
  no.~2, pp. 281--294, 1985.





\bibitem{Dokmanic:2013dz}
I.~Dokmani{\'c}, R.~Parhizkar, A.~Walther, Y.~M. Lu, and M.~Vetterli,
  ``{Acoustic Echoes Reveal Room Shape},'' \emph{Proc. Natl. Acad. Sci.}, vol.
  110, no.~30, Jun. 2013.



\bibitem{Ranieri:2013tx}
J.~Ranieri, A.~Chebira, Y.~M. Lu, and M.~Vetterli, ``{Phase Retrieval for
  Sparse Signals: Uniqueness Conditions},'' \emph{submitted to IEEE Trans. Inf.
  Theory}, Jul. 2013.

\bibitem{torgerson1952}
W.~S. Torgerson, ``Multidimensional Scaling: I. Theory and Method,''
  \emph{Psychometrika}, vol.~17, pp. 401--419, 1952.

\bibitem{Weinberger2004}
K.~Q. Weinberger and L.~K. Saul, ``Unsupervised Learning of Image Manifolds by
  Semidefinite Programming,'' in \emph{Proc. IEEE CVPR}, 2004.

\bibitem{Liberti:2012ut}
L.~Liberti, C.~Lavor, N.~Maculan, and A.~Mucherino, ``{Euclidean Distance
  Geometry and Applications},'' \emph{SIAM Rev.}, vol.~56, no.~1, pp. 3--69,
  2014.

\bibitem{Menger:1928bc}
K.~Menger, ``{Untersuchungen {\"U}ber Allgemeine Metrik},'' \emph{Math. Ann.},
  vol. 100, no.~1, pp. 75--163, Dec. 1928.



\bibitem{Schoenberg:1935dk}
I.~J. Schoenberg, ``{Remarks to Maurice Frechet's Article ``Sur La D\'{e}finition
  Axiomatique D'Une Classe D'Espace Distanc\'{e}s Vectoriellement Applicable Sur
  L'Espace De Hilbert},'' \emph{Ann. Math.}, vol.~36, no.~3, p. 724, Jul. 1935.


\bibitem{Blumenthal:1953ie}
L.~M. Blumenthal, \emph{{Theory and Applications of Distance Geometry}}.\hskip
  1em plus 0.5em minus 0.4em\relax Clarendon Press, 1953.

\bibitem{young1938}
G.~Young and A.~Householder, ``Discussion of a Set of Points in Terms of Their
  Mutual Distances,'' \emph{Psychometrika}, vol.~3, no.~1, pp. 19--22, 1938.

\bibitem{kruskal1964}
J.~B. Kruskal, ``Multidimensional Scaling by Optimizing Goodness of Fit to a
  Nonmetric Hypothesis,'' \emph{Psychometrika}, vol.~29, no.~1, pp. 1--27,
  1964.

\bibitem{gower1982}
J.~C. Gower, ``Euclidean Distance Geometry,'' \emph{Math. Sci.}, vol.~7, pp. 1--14, 1982.

\bibitem{gower1}
------, ``Properties of {E}uclidean and non-{E}uclidean Distance Matrices,''
  \emph{Linear Algebra Appl.}, vol.~67, pp. 81--97, 1985.

\bibitem{Glunt1990}
W.~Glunt, T.~L. Hayden, S.~Hong, and J.~Wells, ``An Alternating Projection
  Algorithm for Computing the Nearest {E}uclidean Distance Matrix,''
  \emph{SIAM J. Matrix Anal. Appl.}, vol.~11, no.~4, pp. 589--600,
  1990.

\bibitem{Hayden1990}
T.~L. Hayden, J.~Wells, W.-M. Liu, and P.~Tarazaga, ``The Cone of Distance
  Matrices,'' \emph{Linear Algebra Appl.}, vol. 144, no.~0, pp.
  153--169, 1990.

\bibitem{Dattorro:2011wa}
J.~Dattorro, \emph{{Convex Optimization {\&} Euclidean Distance
  Geometry}}.\hskip 1em plus 0.5em minus 0.4em\relax Meboo, 2011.

\bibitem{trosset1998}
M.~W. Trosset, ``Applications of Multidimensional Scaling to Molecular
  Conformation,'' \emph{Comp. Sci. Stat.}, vol.~29, pp.
  148--152, 1998.



\bibitem{Holm:1993dx}
L.~Holm and C.~Sander, ``{Protein Structure Comparison by Alignment of Distance
  Matrices},'' \emph{J. Mol. Biol.}, vol. 233, no.~1, pp.
  123--138, Sep. 1993.

\bibitem{Tenenbaum2000}
J.~B. Tenenbaum, V.~De Silva, and J.~C. Langford, ``A Global Geometric
  Framework for Nonlinear Dimensionality Reduction,'' \emph{Science}, vol. 290,
  no. 5500, pp. 2319--2323, 2000.

\bibitem{jain2004}
V.~Jain and L.~Saul, ``Exploratory Analysis and Visualization of Speech and
  Music by Locally Linear Embedding,'' in \emph{IEEE Trans. Acoust., Speech, Signal
  Process.}, vol.~3, 2004.

\bibitem{Demaine:2009dw}
E.~D. Demaine, F.~Gomez-Martin, H.~Meijer, D.~Rappaport, P.~Taslakian, G.~T.
  Toussaint, T.~Winograd, and D.~R. Wood, ``{The Distance Geometry of Music},''
  \emph{Comput. Geom.}, vol.~42, no.~5, pp. 429--454, Jul. 2009.

\bibitem{So:2007cz}
A.~M.-C. So and Y.~Ye, ``{Theory of Semidefinite Programming for Sensor Network
  Localization},'' \emph{Math. Program.}, vol. 109, no. 2-3, pp. 367--384, Mar.
  2007.

\bibitem{Crocco:2012eu}
M.~Crocco, A.~D. Bue, and V.~Murino, ``{A Bilinear Approach to the Position
  Self-Calibration of Multiple Sensors},'' \emph{IEEE Trans. Signal Process.},
  vol.~60, no.~2, pp. 660--673, 2012.

\bibitem{Pollefeys:2008ho}
M.~Pollefeys and D.~Nister, ``{Direct Computation of Sound and Microphone
  Locations from Time-Difference-of-Arrival Data},'' in \emph{Proc. Intl.
  Workshop on HSC}.\hskip 1em plus 0.5em minus 0.4em\relax Las Vegas,
  2008, pp. 2445--2448.





\bibitem{Dokmanic:2014tc}
I.~Dokmani{\'c}, L.~Daudet, and M.~Vetterli, ``{How to Localize Ten Microphones
  in One Fingersnap},'' \emph{Proc. EUSIPCO}, 2014.



\bibitem{Schonemann:1970wd}
P.~H. Sch{\"o}nemann, ``{On Metric Multidimensional Unfolding},''
  \emph{Psychometrika}, vol.~35, no.~3, pp. 349--366, 1970.

\bibitem{Gujarathi:2014cz}
S.~R. Gujarathi, C.~L. Farrow, C.~Glosser, L.~Granlund, and P.~M. Duxbury,
  ``{Ab-Initio Reconstruction of Complex Euclidean Networks in Two
  Dimensions},'' \emph{Physical Review E}, vol.~89, no.~5, 2014.

\bibitem{Krislock:2012xx}
N.~Krislock and H.~Wolkowicz, ``{Euclidean Distance Matrices and
  Applications},'' in \emph{Handbook on Semidefinite, Conic and Polynomial
  Optimization}.\hskip 1em plus 0.5em minus 0.4em\relax Boston, MA: Springer
  US, Jan. 2012, pp. 879--914.

\bibitem{Mucherino:2012hw}
A.~Mucherino, C.~Lavor, L.~Liberti, and N.~Maculan, \emph{{Distance Geometry:
Theory, Methods, and Applications}}. \hskip 1em plus 0.5em minus 0.4em\relax New York, NY:
  Springer Science {\&} Business Media, Dec. 2012.

\bibitem{Schonemann:1964tj}
P.~H. Sch{\"o}nemann, ``{A Solution of the Orthogonal Procrustes Problem With
  Applications to Orthogonal and Oblique Rotation},'' Ph.D. dissertation,
  University of Illinois at Urbana-Champaign, 1964.
  
\bibitem{Keshavan:2010bt}
R.~H. Keshavan, A.~Montanari, and S.~Oh, ``{Matrix Completion From a Few
  Entries},'' \emph{IEEE Trans. Inf. Theory}, vol.~56, no.~6, pp. 2980--2998,
  Jun. 2010.

\bibitem{Keshavan:2012tb}
------, ``{Matrix Completion from Noisy Entries},'' \emph{arXiv}, Apr. 2012.


\bibitem{dur07}
N.~Duric, P.~Littrup, L.~Poulo, A.~Babkin, R.~Pevzner, E.~Holsapple, O.~Rama,
  and C.~Glide, ``Detection of Breast Cancer with Ultrasound Tomography: First
  Results with the Computed Ultrasound Risk Evaluation (CURE) Prototype,''
  \emph{J. Med. Phys.}, vol.~34, no.~2, pp. 773--785, 2007.

\bibitem{parhizkar:2013a}
R.~Parhizkar, A.~Karbasi, S.~Oh, and M.~Vetterli, ``Calibration Using Matrix
  Completion with Application to Ultrasound Tomography,'' \emph{IEEE Trans.
  Signal Process.}, July 2013.

\bibitem{Borg2005}
I.~Borg and P.~Groenen, \emph{Modern Multidimensional Scaling: Theory and
  Applications}.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 2005.

\bibitem{kruskal1964b}
J.~B. Kruskal, ``Nonmetric Multidimensional Scaling: A Numerical Method,''
  \emph{Psychometrika}, vol.~29, no.~2, pp. 115--129, 1964.

\bibitem{DeLeeuw1977}
J.~De~Leeuw, ``Applications of Convex Analysis to Multidimensional Scaling,''
  in \emph{Recent Developments in Statistics}, J.~Barra, F.~Brodeau, G.~Romier,
  and B.~V. Cutsem, Eds.\hskip 1em plus 0.5em minus 0.4em\relax North Holland
  Publishing Company, 1977, pp. 133--146.

\bibitem{mathar1991}
R.~Mathar and P.~J.~F. Groenen, ``Algorithms in Convex Analysis Applied to
  Multidimensional Scaling,'' in \emph{Symbolic-numeric data analysis and
  learning}, E.~Diday and Y.~Lechevallier, Eds.\hskip 1em plus 0.5em minus
  0.4em\relax Nova Science, 1991, pp. 45--56.

\bibitem{guttman1968}
L.~Guttman, ``A General Nonmetric Technique for Finding the Smallest Coordinate
  Space for a Configuration of Points,'' \emph{Psychometrika}, vol.~33, no.~4,
  pp. 469--506, 1968.

\bibitem{takane1977}
Y.~Takane, F.~Young, and J.~De~Leeuw, ``Nonmetric Individual Differences
  Multidimensional Scaling: An Alternating Least Squares Method with Optimal
  Scaling Features,'' \emph{Psychometrika}, vol.~42, no.~1, pp. 7--67, 1977.

\bibitem{gaffke1989}
N.~Gaffke and R.~Mathar, ``A Cyclic Projection Algorithm via Duality,''
  \emph{Metrika}, vol.~36, no.~1, pp. 29--54, 1989.

\bibitem{rezathesis13}
R.~Parhizkar, ``Euclidean Distance Matrices: Properties, Algorithms and
  Applications,'' Ph.D. dissertation, Ecole Polytechnique Federale de Lausanne
  (EPFL), 2013.



\bibitem{Browne1987}
M.~Browne, ``The Young-Householder Algorithm and the Least Squares
  Multidimensional Scaling of Squared Distances,'' \emph{J. Classif.}, vol.~4,
  no.~2, pp. 175--190, 1987.

\bibitem{gluntEmbedding1991}
W.~Glunt, T.~L. Hayden, and W.-M. Liu, ``The Embedding Problem for Predistance
  Matrices,'' \emph{Bull. Math. Biol.}, vol.~53, no.~5, pp.
  769--796, 1991.


\bibitem{Biswas:2006cm}
P.~Biswas, T.~C. Liang, K.~C. Toh, Y.~Ye, and T.~C. Wang, ``{Semidefinite
  Programming Approaches for Sensor Network Localization With Noisy Distance
  Measurements},'' \emph{{IEEE} Trans. Autom. Sci. Eng.}, vol.~3, no.~4, pp. 360--371, 2006.



\bibitem{cvx}
M.~Grant and S.~Boyd, ``{CVX}: Matlab Software for Disciplined Convex
  Programming, version 2.1,'' \url{http://cvxr.com/cvx}, Mar. 2014.

\bibitem{gb08}
------, ``Graph Implementations for Nonsmooth Convex Programs,'' in
  \emph{Recent Advances in Learning and Control}, ser. Lecture Notes in Control
  and Information Sciences, V.~Blondel, S.~Boyd, and H.~Kimura, Eds.\hskip 1em
  plus 0.5em minus 0.4em\relax Springer-Verlag Limited, 2008, pp. 95--110,
  \url{http://stanford.edu/~boyd/graph_dcp.html}.

\bibitem{Boutin:2004gb}
M.~Boutin and G.~Kemper, ``{On Reconstructing N-Point Configurations from the
  Distribution of Distances or Areas},'' \emph{Adv. Appl. Math.}, vol.~32,
  no.~4, pp. 709--735, May 2004.

\bibitem{Allen:1979ua}
J.~B. Allen and D.~A. Berkley, ``{Image Method for Efficiently Simulating
  Small-room Acoustics},'' \emph{J. Acoust. Soc. Am.}, vol.~65, no.~4, pp.
  943--950, 1979.



\bibitem{Millane:1990pt}
R.~P. Millane, ``{Phase Retrieval in Crystallography and Optics},'' \emph{J.
  Opt. Soc. Am. A}, vol.~7, no.~3, pp. 394--411, Mar. 1990.

\bibitem{Beavers:1989tj}
W.~Beavers, D.~E. Dudgeon, J.~W. Beletic, and M.~T. Lane, ``{Speckle Imaging
  Through the Atmosphere},'' \emph{Linconln Lab. J.}, vol.~2,
  pp. 207--228, 1989.



\bibitem{Lemke:2002um}
S.~S. Skiena, W.~D. Smith, and P.~Lemke, ``{Reconstructing Sets from Interpoint
  Distances},'' in \emph{ACM SCG}.\hskip 1em plus
  0.5em minus 0.4em\relax 1990, pp. 332--339.



\bibitem{Parhizkar:2014kn}
R.~Parhizkar, I.~Dokmani{\'c}, and M.~Vetterli, ``{Single-Channel Indoor Microphone Localization},'' \emph{Proc. IEEE ICASSP}, Florence, 2014.


\end{thebibliography}
 
\end{document}
