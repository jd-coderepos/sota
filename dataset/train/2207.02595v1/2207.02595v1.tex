

\documentclass[runningheads]{llncs}
\usepackage{graphicx}
\usepackage{makecell}

\usepackage{tikz}
\usepackage{comment}
\usepackage{amsmath,amssymb} \usepackage{color}
\usepackage{booktabs}
\usepackage{arydshln}
\usepackage{multirow}
\usepackage{hyperref}

\hypersetup{colorlinks=true,linkcolor=blue,filecolor=magenta,urlcolor=cyan,
            pdftitle={Overleaf Example},pdfpagemode=FullScreen}

\usepackage[accsupp]{axessibility}  



\usepackage{caption}
\captionsetup{font=footnotesize}
\captionsetup[sub]{font=footnotesize}



\begin{document}

\pagestyle{headings}
\mainmatter
\def\ECCVSubNumber{1225}  

\newcommand{\blue}[1]{\textbf{\textcolor{mblue}{#1}}}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\bred}[1]{\textbf{\textcolor{red}{#1}}}
\newcommand{\green}[1]{\textcolor{mgreen}{#1}}
\newcommand{\yellow}[1]{\textcolor{yellow}{#1}}
\newcommand{\gray}[1]{\textcolor{mgray}{#1}}
\newcommand{\supplement}{supplementary materials}
\newcommand{\textbfg}[1]{\textbf{\textcolor{mgreen}{#1}}}


\title{Quality Assessment in the Time Dimension: \\ Inter-frame Distortion and Time-series Attention} 

\title{FAST-VQA: Efficient End-to-end Video Quality Assessment with Fragment Sampling} 

\titlerunning{ECCV-22 submission ID \ECCVSubNumber} 
\authorrunning{ECCV-22 submission ID \ECCVSubNumber} 
\author{Anonymous ECCV submission}
\institute{Paper ID \ECCVSubNumber}





\titlerunning{FAST-VQA}


\author{Haoning Wu\inst{1,2,3} \and
Chaofeng Chen\inst{1,2} \and
Jingwen Hou\inst{2} \and
Liang Liao\inst{1,2} \and
Annan Wang\inst{1,2} \and
Wenxiu Sun\inst{3} \and
Qiong Yan\inst{3} \and
Weisi Lin\inst{2}}
\authorrunning{H. Wu \textit{et al.}}
\institute{S-Lab, Nanyang Technological University \and
School of Computer Science and Engineering, Nanyang Technological University \and
Sensetime Research and Tetras AI\\
\email{haoning001@e.ntu.edu.sg}
}

\newcommand{\frag}[0]{\textbf{{\textit{fragments}}}}

\newcommand{\cfchen}[1]{\textcolor{red}{CF: #1}}
\newcommand{\cfcomment}[1]{\textcolor{blue}{\emph{CF Comment: #1}}}
\newcommand{\LL}[1]{\textcolor{red}{\emph{LL: #1}}}



\maketitle

\definecolor{mgray}{gray}{0.45}
\definecolor{mred}{RGB}{238, 34, 12}
\definecolor{mgreen}{RGB}{1, 127, 0}
\definecolor{mblue}{RGB}{0, 0, 180}

\begin{abstract}





Current deep video quality assessment (VQA) methods are usually with high computational costs when evaluating high-resolution videos. This cost hinders them from learning better video-quality-related representations via end-to-end training. Existing approaches usually consider naive sampling to reduce the computational cost, such as \textit{resizing} and \textit{cropping}. However, they obviously corrupt quality-related information in videos and are thus not optimal to learn good representations for VQA. Therefore, there is an eager need to design a new quality-retained sampling scheme for VQA. In this paper, we propose Grid Mini-patch Sampling (GMS), which allows consideration of local quality by sampling patches at their raw resolution and covers global quality with contextual relations via mini-patches sampled in uniform grids. These mini-patches are spliced and aligned temporally, named as \frag. We further build the Fragment Attention Network (FANet) specially designed to accommodate \frag~as inputs.
Consisting of \frag~and FANet, the proposed FrAgment Sample Transformer for VQA (\textbf{FAST-VQA}) enables efficient end-to-end deep VQA and learns effective video-quality-related representations. It improves state-of-the-art accuracy by around \textbf{} while reducing \textbf{} FLOPs on 1080P high-resolution videos. The newly learned video-quality-related representations can also be transferred into smaller VQA datasets and boost the performance on these scenarios. Extensive experiments show that FAST-VQA has 
good performance on inputs of various resolutions while retaining high efficiency. We publish our code at \url{https://github.com/timothyhtimothy/FAST-VQA}.

\keywords{Video Quality Assessment, \frag, Quality-retained Sampling, End-to-End Learning, State-of-the-Art, High Efficiency}








\end{abstract}


\section{Introduction}
\label{sec:1}


\begin{figure}[]
    \centering
    \includegraphics[width=0.93\linewidth]{Author Guidelines for ECCV Submission/Figure1E.pdf}
    \vspace{-5pt}
    \caption{Motivation for \frag: (a) The computational cost (FLOPs\&Memory at Batch Size 4) for existing VQA methods is high especially on high-resolution videos. (b) Sampling approaches. Naive approaches such as \textit{resizing}~\cite{musiq,dbcnn} and \textit{cropping}~\cite{crop1,crop2} cannot preserve video quality well. Zoom in for clearer view.}
    \label{fig:1}
    \vspace{-19pt}
\end{figure}










More and more videos with a variety of contents are collected in-the-wild and uploaded to the Internet every day. With the growth of high-definition video recording devices, a growing proportion of these videos are in high resolution (e.g. ). Classical video quality assessment (VQA) algorithms based on handcrafted features are difficult to handle these videos with diverse content and degradation. In recent years, deep-learning-based VQA methods \cite{vsfa,mdtvsfa,pvq,mlsp,cnn+lstm,cnntlvqm} have shown better performance on in-the-wild VQA benchmarks \cite{vqc,kv1k,ytugc,pvq}. However, the computational cost of deep VQA methods increases quadratically when applied to high resolution videos, and a video of size  would require  floating point operations (FLOPs) than normal  inputs (as Fig.~\ref{fig:1}(a) shows), limiting these methods from practical applications. It is urgent to develop new VQA methods that are both effective and efficient.

Meanwhile, with high memory cost noted in Fig.~\ref{fig:1}(a), existing methods usually regress quality scores with \red{\textbf{fixed}} features extracted from pre-trained networks for classification tasks~\cite{he2016residual,irnv2,r3d} to alleviate memory shortage problem on GPUs instead of end-to-end training, preventing them from learning \textit{video-quality-related representations} that better represent quality information and limiting their accuracy. Existing approaches apply naive sampling on images or videos by resizing~\cite{musiq,dbcnn} or cropping~\cite{crop1,crop2} (as Fig.~\ref{fig:1}(b) shows) to reduce this cost and enable end-to-end training. However, they both cause artificial quality corruptions or changes during sampling, \emph{e.g.}, resizing corrupts local textures that are significant for predicting video quality, while cropping causes mismatched global quality with local regions. Moreover, the severity of these problems increases with the raw resolution of the video, making them unsuitable for VQA tasks.








\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{Author Guidelines for ECCV Submission/Figure2D_compressed.pdf}
    \caption{\textbf{\textit{Fragments}}, in spatial view (a) and temporal view (b). Zoom-in views of mini-patches show that \textbf{\textit{fragments}} can retain spatial local quality information (a), and spot temporal variations such as shaking across frames (b). In (a), spliced mini-patches also keep the global scene information of original frames.}
    \label{fig:2}
    \vspace{-20pt}
\end{figure}


















To improve the practical efficiency and the training effectiveness of deep VQA methods, we propose a new sampling scheme, Grid Mini-patch Sampling (GMS), to retain the sensitivity to original video quality. GMS cuts videos into spatially uniform non-overlapping grids, randomly sample a mini-patch from each grid, and then splice mini-patches together. In temporal view, we constrain the position of mini-patches to align across frames, in order to ensure the sensitivity on temporal variations. We name these temporally aligned and spatially spliced mini-patches as \frag. As shown in Fig.~\ref{fig:2}, The proposed fragments can well preserve the sensitivity on both spatial and temporal quality. First, it preserves the local texture-related quality information (\emph{e.g.}, spot blurs happened in \textit{video 1/2}) by retaining the original resolution in patches. Second, benefiting from the globally uniformly sampled grids, it covers the global quality even though different regions have different qualities (\emph{e.g.}, \textit{video 3}). Third, by splicing the mini-patches, \frag~retains contextual relations of patches so that the model can learn global scene information of the original frames. At last, with temporal alignment, \frag~preserve temporal quality sensitivity by retaining the inter-frame variations in mini-patches from raw resolution, so they can be used to spot temporal distortions in videos and distinguish between severely shaking videos (\emph{e.g.}, \textit{video 5}) from relatively stable shots (\emph{e.g.}, \textit{video 6}).
















However, it is non-trivial to build a network using the proposed \frag~as inputs. The network should follow two principles: 1) It should better extract the quality-related information preserved in \frag, including the retained local textures inside the raw resolution patches and the contextual relations between the spliced mini-patches; 2) It should distinguish the artificial discontinuity between mini-patches in \frag~from the authentic quality degradation in the original videos. Based on these two principles, we propose a Fragment Attention Network (FANet) with Video Swin Transformer Tiny (Swin-T)~\cite{swin3d}  as the backbone. Swin-T has a hierarchical structure and processes inputs with patch-wise operations, which is naturally suitable for proceeding with proposed \frag.

\begin{figure}[]
    \centering
    \includegraphics[width=0.92\linewidth]{Author Guidelines for ECCV Submission/Figure3D_compressed.pdf}
    \vspace{-5pt}
    \caption{Motivation for the two proposed modules in FANet: (a) Gated Relative Position Biases (GRPB); (b) Intra-Patch Non-Linear Regression (IP-NLR) head. The structures for the two modules are illustrated in Fig.~\ref{fig:5}.}
    \label{fig:3}
    \vspace{-18pt}
\end{figure}

Furthermore, to avoid the negative impact of discontinuity between mini-patches on quality prediction, we propose two novel modules, \emph{i.e.}, Gated Relative Position Biases (GRPB) and Intra-Patch Non-Linear Regression (IP-NLR), to correct for the self-attention computation and the final score regression in the FANet respectively. Specifically, considering that some pairs in the same attention window might have the same relative position (\textit{e.g.}, Fig.~\ref{fig:3}(a) \red{A}-\green{C}, \red{D}-\yellow{E}, \red{A}-\red{B}), 
but the cross-patch attention pairs (\red{A}-\green{C}, \red{D}-\yellow{E}) are in far actual distances while intra-patch attention pairs (\red{A}-\red{B}) are in much nearer actual distances in the original video, we propose GRPB to explicitly distinguish these two kinds of attention pairs to avoid confusion of discontinuity between patches and authentic video artifacts. In addition, due to the discontinuity, different mini-patches contain diverse quality information (Fig.~\ref{fig:3}(b)), thus pooling operation before score regression applied in existing methods may confuse the information. To address this issue, we design IP-NLR as a quality-sensitive head, which first regresses the quality scores of mini-patches independently with non-linear layers and pools them after the regression.
























In summary, we propose the FrAgment Sample Transformer for VQA (\textbf{FAST-VQA}), with the following contributions:

\begin{enumerate}
\item We propose \frag, a new sampling strategy for VQA that preserves both local quality and unbiased global quality with contextual relations via uniform Grid Mini-patch Sampling (GMS). The \frag~can reduce the complexity of assessing 1080P videos by 97.6\% and enables effective end-to-end training of VQA with quality-retained video samples.

\item We propose the Fragment Attention Network (FANet) to learn the local and contextual quality information from \frag, in which the Gated Relative Position Biases (GRPB) module is proposed to distinguish the intra-patch and cross-patch self-attention and the Intra-Patch Non-Linear Regression (IP-NLR) is proposed for better quality regression from \frag.



\item The proposed FAST-VQA can learn \textit{video-quality-related representations} efficiently through end-to-end training. These quality features help FAST-VQA to be \textbf{10\%} more accurate than the existing state-of-the-art approaches and \textbf{8\%} better than full-resolution Swin-T baseline with fixed recognition features. Through transfer learning, these quality features also significantly improve the best benchmark performance for small VQA datasets.
\end{enumerate}


\section{Related Works}


\paragraph{{Classical VQA Methods}} Classical VQA methods \cite{vbliinds,viideo,tlvqm,videval,rapique,tpqi} handcrafted features to evaluate video quality. Among recent works, TLVQM \cite{tlvqm} uses a combination of spatial high-complexity and temporal low-complexity handcraft features and VIDEVAL \cite{videval} ensembles different handcraft features to model the diverse authentic distortions. However, the reasons affecting the video quality are quite complicated and cannot be well captured with these handcrafted features.



\paragraph{Fixed-feature-based Deep VQA Methods} Due to the extremely high computational cost of deep networks on high resolution videos, existing deep VQA methods train only a feature regression network with fixed deep features. Among them, VSFA \cite{vsfa} uses the features extracted by pre-trained ResNet-50 \cite{he2016residual} from ImageNet-1k \cite{imagenet} and GRU~\cite{gru} for temporal regression. MLSP-FF~\cite{mlsp} also uses heavier Inception-ResNet-V2 \cite{irnv2} for feature extraction. Some methods~\cite{pvq,lsctphiq} use the features extractor pre-trained with IQA datasets~\cite{koniq,paq2piq}. PVQ~\cite{pvq} also extracts features pretrained on action recognition dataset \cite{k400data} for better perception on inter-frame distortion. These methods are limited by their high computational cost on high resolution videos. Additionally, without end-to-end training, fixed features pretrained by other tasks are not optimal for extracting quality-related information, which also limits the accuracy of quality assessment.

\paragraph{VQA Datasets} 

Tab.~\ref{tab:videodatasize} shows common VQA datasets, other video datasets and their sizes. The early VQA datasets~\cite{cvd,qualcomm} are synthesized with specialized distortion and have a very small volume. Some recent in-the-wild VQA datasets like KoNViD-1k~\cite{kv1k}, YouTube-UGC~\cite{ytugc} and LIVE-VQC~\cite{vqc} are still small compared to datasets for other video tasks such as \cite{k400data,activitynet,ava}.
Recently, LSVQ\cite{pvq}, a large-scale VQA dataset with 39,076 videos is publicly available. With end-to-end deep learning of the proposed FAST-VQA, the \textit{video-quality-related} features learnt on large-scale LSVQ dataset can be transferred into smaller VQA datasets to reach better performance.

\begin{table}[]
    \centering
    \vspace{-25pt}
    \setlength\tabcolsep{2.5pt}
    \caption{Common datasets in VQA and other video tasks. Most common VQA datasets are to small (noted in \red{red}) to learn sufficient quality representations independently.}
    \resizebox{0.75\textwidth}{!}{\begin{tabular}{l|c|c|c} \hline
         Dataset & Task & Distortion Type & Size  \\ \hline
         Kinetics-400 \cite{k400data} & Video Recognition & NA & 306,245 \\
         ActivityNet \cite{activitynet} & Video Action Localization & NA & 27,801\\
         AVA \cite{ava} & Atomic Action Detection & NA & 386,000\\\hdashline
         CVD2014 \cite{cvd} & Video Quality Assessment & Synthetic In-capture & \red{234} \\
KoNViD-1k \cite{kv1k} & Video Quality Assessment & In-the-wild & \red{1,200} \\
         LIVE-VQC \cite{vqc} & Video Quality Assessment & In-the-wild & \red{585} \\
         Youtube-UGC \cite{ytugc} & Video Quality Assessment & In-the-wild & \red{1,147} \\\hdashline
         LSVQ \cite{pvq} & Video Quality Assessment & In-the-wild & 39,076 \\ \hline
    \end{tabular}}
    \label{tab:videodatasize}
    \vspace{-20pt}
\end{table}


\paragraph{Vision Transformers} Vision transformers~\cite{vit,deit,vivit,mvit,swin2d} have shown effective on computer vision tasks. They cut images or videos into non-overlapping patches as input and perform self-attention operations between them. The patch-wise operations in vision transformers naturally distinguish the edges of mini-patches and are suitable for handling with the proposed \frag. 

\begin{figure*}[htbp]
    \centering
    \vspace{-5pt}
    \includegraphics[width=\linewidth]{Author Guidelines for ECCV Submission/Figure4D_compressed.pdf}
    \caption{The pipeline for sampling \frag~with Grid Mini-patch Sampling (GMS), including grid partition, patch sampling, patch splicing, and temporal alignment. After GMS, the \frag~are fed into the FANet (Fig.~\ref{fig:5}).}
    \label{fig:4}
    \vspace{-15pt}
\end{figure*}

\section{Approach}
\label{section:method}


In this section, we introduce the full pipeline of the proposed FAST-VQA method. An input video is first sampled into \frag~via Grid Mini-patch Sampling (GMS, Sec.~\ref{section:fragment}). After sampling, the resultant fragments are fed into the Fragment Attention Network (FANet, Sec.~\ref{section:fanet}) to get the final prediction of the video's quality. We introduce both parts in the following subsections.
  




\subsection{Grid Mini-patch Sampling (GMS)}\label{section:fragment}

To well preserve the original video quality after sampling, we follow several important principles when designing the sampling process for \frag. We will illustrate the process along with these principles below.

\paragraph{Preserving global quality: uniform grid partition.}  To include each region for quality assessment and uniformly assess quality in different areas, we design the grid partition to cut video frames into uniform grids with each grid having the same size (as shown in Fig.~\ref{fig:4}). We cut the -th video frame  into  uniform grids with the same sizes, denoted as , where  denotes the grid 
in the -th row and -th column. The uniform grid partition process is formalized as follows.

where  and  denote the height and width of the video frame.

\paragraph{Preserving local quality: raw patch sampling.} To preserve the local textures (\textit{e.g.} blurs, noises, artifacts) that are vital in VQA, we select raw resolution patches without any resizing operations to represent local textural quality in grids. We employ random patch sampling to select one mini-patch  of size of  from each grid . The patch sampling process is as follows.


where  is the patch sampling operation for frame  and grid .

\paragraph{Preserving temporal quality: temporal alignment.} It is widely recognized by early works \cite{deepvqa,tlvqm,pvq} that inter-frame temporal variations are influential to video qualities. To retain the raw temporal variations in videos (with  frames), we strictly align the sample areas during patch sampling operations  in different frames, as the following constraint shows.



\paragraph{Preserving contextual relations: patch splicing.} Existing works~\cite{sfa,vsfa,mlsp} have shown that the global scene information and contextual information affects quality predictions. To keep the global scene information of the original videos, we keep the contextual relations of mini-patches by splicing them into their original positions, as the following equation shows:


where  denote the spliced and temporally aligned mini-patches after the Grid Mini-patch Sampling (GMS) pipeline, named as \frag.
\begin{comment}
\subsection{Grid Mini-patch Sampling (GMS)}\label{section:fragment}

To well preserve the original video quality after sampling, we follow several important principles when designing the sampling process for \frag. We will illustrate the process along with these principles below.

\paragraph{Preserving local quality: retaining original resolution.} Most common distortion types in natural VQA, such as motion blurs, defocus blurs, noises and compression artifacts, are closely related to local textural information. These quality-related textures are highly localized and would be corrupted by resizing or discontinuous sampling, as discussed by several existing works~\cite{tlvqm,videval}. Therefore, we sample patches directly from the original resolution frames without any resizing operations to preserve the local quality information.

\paragraph{Preserving global quality: grid partition and patch sampling.} Besides retaining local quality, the uniformity of sampling also affects the accuracy of the global quality prediction as the quality of local areas may differ from the overall video. For instance, if we only sample patches from the dark areas (with lower perceptual quality) in the frames shown in Fig.~\ref{fig:4}, the predictions will be lower than the ground truth quality of the video. To keep the uniformity of sampling, we design the grid partition (Fig.~\ref{fig:4}(a), Eq.~\ref{eq:1}) to cut video frames into  uniform grids (), and random patch sampling (, Fig.~\ref{fig:4}(b), Eq.~\ref{eq:2}) to uniformly sample mini-patches () with size  from , as shown in Fig.~\ref{fig:4}.






\paragraph{Preserving contextual relations: patch splicing.} Many existing works (\cite{sfa,vsfa,mlsp}) have shown that contextual relations and global scene information affects the quality judgements, for instance, the clear skies and the blurry areas might share similar textures but with obviously different quality. We preserve these contextual relations by splicing the mini-patches together in the original positions of their corresponding grids via patch splicing (Fig.~\ref{fig:4}(c), Eq.~\ref{eq:3}).





\paragraph{Preserving temporal quality: temporal alignment.} It is widely recognized by early works \cite{deepvqa,tlvqm,pvq} that inter-frame temporal variations are influential to video qualities. We strictly align the sample areas across frames during patch sampling  (constraint in Eq.~\ref{eq:2}), and note the spliced and temporally aligned mini-patches as \frag~). 

Denote  as crops in  horizontally and  vertically of frame  in  ,  as the patch sampling operation for frame , the whole pipeline for Grid Mini-patch Sampling (GMS) is expressed as follows:

 
\end{comment}




\subsection{Fragment Attention Network (FANet)}\label{section:fanet}



\paragraph{The Overall Framework.} Fig.~\ref{fig:5} shows the overall framework of FANet. It uses a Swin-T with four hierarchical self-attention layers as backbone. We also design the following modules to adapt it to fragments well.
\paragraph{Gated Relative Position Biases.} Swin-T adds relative position bias (RPB) that uses learnable Relative Bias Table () to represent the relative positions of pixels in attention pairs (). For \frag, however, as discussed in Fig.~\ref{fig:3}(a), the cross-patch pairs have much large actual distances than intra-patch pairs and should not be modeled with the same bias table. Therefore, we propose the gated relative position biases (GRPB, Fig.~\ref{fig:5}(b)) that uses learnable real position bias table () and pseudo position bias table () to replace . 
The mechanisms of them are the same as  but they are learnt separately and used for intra-patch and cross-patch attention pairs respectively.
Denote  as the intra-patch gate ( if  are in the same mini-patch else ), the self-attention matrix () with GRPB is calculated as:



\begin{figure}[]
    \centering
    \includegraphics[width=0.96\linewidth]{Author Guidelines for ECCV Submission/Figure5D_compressed.pdf}
    \vspace{-10pt}
    \caption{The overall framework for FANet, including the Gated Relative Position Biases (GRPB) and Intra-Patch Non-Linear Regression (IP-NLR) modules. The input \frag~come from Grid Mini-patch Sampling (Fig.~\ref{fig:4}).}
    \label{fig:5}
    \vspace{-18pt}
\end{figure}

\vspace{-10pt}

where  is the relative position of pair  in \frag.

\paragraph{Intra-Patch Non-Linear Regression.} As illustrated in Fig.~\ref{fig:3}(b), different mini-patches have diverse qualities due to discontinuity between them. If we pool features from different patches before regression, the quality representations of mini-patches will be confused with each other. To avoid this problem, we design the Intra-Patch Non-Linear Regression (IP-NLR, Fig.~\ref{fig:5}(c)) to regress the features via non-linear layers () first, and perform pooling following the regression. Denote features as , output score as , pooling operation as , the IP-NLR can be expressed as follows:

\vspace{-10pt}


\section{Experiments}

In the experiment part, we conduct several experiments to evaluate and analyze the performance of the proposed FAST-VQA model.



\subsection{Evaluation Setup}

\paragraph{Implementation Details} We use the Swin-T~\cite{swin3d} pretrained on Kinetics-400~\cite{k400data} dataset to initialize the backbone in FANet. As Tab.~\ref{tab:impdetail} shows, we implement two sampling densities for \frag:  FAST-VQA (normal density) and FAST-VQA-M (lower density \& higher efficiency), and accomodate window sizes in FANet to the input sizes.  Without special notes, all ablation studies are on variants of FAST-VQA. We use PLCC (Pearson linear correlation coef.) and SRCC (Spearman rank correlation coef.) as metrics and use differentiable PLCC loss  as loss function. We set the training batch size as 16.

\vspace{-25pt}
\begin{table}[]
    \centering
\setlength\tabcolsep{5pt}
    \caption{Comparison of FAST-VQA and FAST-VQA-M with lower sampling density.}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{l|c|c|c|c|c|c} \hline
\makecell[c]{Methods}&\makecell{Number of\\ Frames ()}&\makecell[c]{Patch Size\\ ()}&\makecell[c]{Number of \\Grids ()}&\makecell[c]{Window Size \\ in FANet}&FLOPs&Parameters\\\hline
    \textbf{FAST-VQA}&32&32&7&(8,7,7)&279G&27.7M\\
    \textbf{FAST-VQA-M}&16&32&4&(4,4,4)&46G&27.5M\\
 \hline
    \end{tabular}
    }
    \label{tab:impdetail}
    \vspace{-14pt}
\end{table}

\paragraph{Training \& Benchmark Sets} We use the large-scale LSVQ{}\cite{pvq} dataset with 28,056 videos for training FAST-VQA. For evaluation, we choose 4 testing sets to test the model trained on LSVQ. The first two sets, LSVQ and LSVQ are official intra-dataset test subsets for LSVQ, while the LSVQ consists of 7,400 various resolution videos from 240P to 720P, and LSVQ consists of 3,600 1080P high resolution videos. We also evaluate the generalization ability of FAST-VQA on cross-dataset evaluations on KoNViD-1k \cite{kv1k} and LIVE-VQC \cite{vqc}, two widely-recognized in-the-wild VQA benchmark datasets. 





\subsection{Benchmark Results}


\begin{table}[]
\footnotesize
\vspace{-25pt}
\caption{Comparison with existing methods (classical and deep) and our baseline (Full-res Swin-T \textit{features}). The 1st/2nd best scores are colored in \textbf{\red{red}} and \blue{blue}, respectively.} \label{table:peer}
\vspace{-3mm}
\setlength\tabcolsep{5.4pt}
\renewcommand\arraystretch{1.25}
\footnotesize
\center
\resizebox{1.\textwidth}{!}{\begin{tabular}{l:l|cc|cc|cc|cc}
\hline
\multicolumn{2}{l|}{Type/} & \multicolumn{4}{c|}{Intra-dataset Test Sets}        &  \multicolumn{4}{c}{Cross-dataset Test Sets}            \\ \hdashline
\multicolumn{2}{l|}{\textbf{Testing Set}/}         & \multicolumn{2}{c|}{\textbf{LSVQ}}   & \multicolumn{2}{c|}{\textbf{LSVQ}}        &  \multicolumn{2}{c|}{\textbf{KoNViD-1k}}  & \multicolumn{2}{c}{\textbf{LIVE-VQC}}             \\ \hline
Groups~~~ & ~Methods                   & SRCC & PLCC    & SRCC & PLCC           & SRCC & PLCC                      & SRCC & PLCC                               \\ \hline 
\multirow{3}{0pt}{{{Existing Classical}}} &~BRISQUE\cite{brisque}    & 0.569 &  0.576  & 0.497 & 0.531     & 0.646 &     0.647                   & 0.524 &  0.536 \\
&~TLVQM\cite{tlvqm}        & 0.772 &  0.774  & 0.589 & 0.616     & 0.732 &     0.724                   & 0.670 &  0.691 \\
&~VIDEVAL\cite{videval}      & 0.794 &  0.783  & 0.545 & 0.554     & 0.751 &     0.741                   & 0.630 &  0.640 \\\hdashline    
\multirow{3}{0pt}{{{Existing Deep}}} &~VSFA\cite{vsfa}          & 0.801 &  0.796  & 0.675 & 0.704     & 0.784 &     0.794                   & 0.734 &  0.772 \\

&~PVQ\cite{pvq}   & 0.814 &  0.816  & 0.686 &   0.708         & 0.781 &     0.781                   & 0.747 &  0.776                               \\ 
&~PVQ\cite{pvq}   & 0.827 &  0.828 & 0.711 &  0.739     & 0.791 &     0.795  & 0.770 &  0.807   \\ \hdashline
\multicolumn{2}{l|}{Full-res Swin-T\cite{swin3d} \textit{features}} &  0.835 & 0.833 & 0.739 & 0.753 & 0.825 & 0.828         &  \blue{0.794} & 0.809 \\ \hline
\multicolumn{2}{l|}{\textbf{FAST-VQA-M} (Ours)} &  \blue{0.852} & \blue{0.854} & \blue{0.739} & \blue{0.773} & \blue{0.841} & \blue{0.832} & {0.788} & \blue{0.810} \\ \hline
\multicolumn{2}{l|}{\textbf{FAST-VQA} (Ours)} &  \textbf{\red{0.876}} & \textbf{\red{0.877}}  & \textbf{\red{0.779}} & \textbf{\red{0.814}} & \textbf{\red{0.859}} & \textbf{\red{0.855}}& \textbf{\red{0.823}} & \textbf{\red{0.844}}  \\ \hline
\multicolumn{2}{l|}{\textit{Improvement} to PVQ} &  \textit{+6\%} & \textit{+6\%} & \textit{+10\%} & \textit{+10\%} & \textit{+9\%} & \textit{+8\%} & \textit{+7\%} & \textit{+5\%}  \\ \hline
\end{tabular}}
\vspace{-4mm}

\end{table}

In Tab.~\ref{table:peer}, we compare with existing classical and deep VQA methods and our baseline, the full-resolution Swin-T with feature regression instead of end-to-end training (denoted as `Full-res Swin-T \textit{features}'). With its video-quality-related representations, FAST-VQA achieves at most 10\% improvement to PVQ, the existing state-of-the-art on LSVQ. Even the efficient version FAST-VQA-M can outperform existing state-of-the-art. FAST-VQA also shows significant improvement to its fixed-feature-based baseline with the same backbone, demonstrating that the proposed new {{quality-retained sampling}} with {{end-to-end training}} scheme for VQA is not only much more efficient (with only 2.36\% FLOPs required on 1080P videos) but also notably more accurate (with 8.10\% improvement on PLCC metric for LSVQ) than the 
existing fixed-feature-based paradigm.


\subsection{Efficiency of FAST-VQA}
\label{sec:eff}

\begin{figure}[t]
    \centering
\includegraphics[width=\linewidth]{Author Guidelines for ECCV Submission/FigureA1.pdf}
    \vspace{-15pt}
    \caption{The Performance-FLOPs curve of proposed \green{\textbf{FAST-VQA}} and baseline methods.}
    \label{fig:a1}
\end{figure}

\begin{table*}[t]
\centering
    \caption{FLOPs and running time (on GPU/CPU, average of ten runs) comparison of FAST-VQA, state-of-the-art methods and our baseline on different resolutions. We \textbf{boldface} FLOPs  500G and running time  1s.} \label{tab:efficiency}
    \vspace{3pt}
    \setlength\tabcolsep{6pt}
    \renewcommand\arraystretch{1.15}
\resizebox{.92\textwidth}{!}{
    \begin{tabular}{l|c:c|c:c|c:c}
    \hline
    {} & \multicolumn{2}{c|}{540P} & \multicolumn{2}{c|}{720P} & \multicolumn{2}{c}{1080P}\\
    \cline{2-7}
    {Method} & FLOPs(G) & Time(s)  & FLOPs(G) & Time(s) & FLOPs(G) & Time(s)\\
    \hline
     VSFA\cite{vsfa} & 10249\red{} & 2.603/92.761 & 18184\red{} & 3.571/134.9 & 40919\red{} & 11.14/465.6 \\ 
     PVQ\cite{pvq} & 14646\red{} & 3.091/97.85 & 22029\red{} & 4.143/144.6 & 58501\red{} & 13.79/538.4 \\ \hdashline
     Full-res Swin-T\cite{swin3d} \textit{feat.} & 3032\red{} & 3.226/102.0 & 5357\red{} & 5.049/166.2 & 11852\red{} & 8.753/234.9 \\ \hline
        \textbf{FAST-VQA} (Ours) & {\textbf{279}{}}& \textbfg{0.044}/9.019 & {\textbf{279}{}}& \textbfg{0.043}/9.530 & {\textbf{279}{}} & \textbfg{0.045}/9.142 \\ 
    \textbf{FAST-VQA-M} (Ours) & \textbfg{46}\green{}& \textbfg{0.019/0.729} & \textbfg{46}\green{}& \textbfg{0.019/0.613} & \textbfg{46}\green{} & \textbfg{0.019/0.714} \\ \hline
    \end{tabular}

}
\end{table*}





To demonstrate the efficiency of FAST-VQA, we compare the FLOPs and running times on CPU/GPU (average of ten runs per sample) of the proposed FAST-VQA with existing deep VQA approaches on different resolutions, see Tab.~\ref{tab:efficiency}. We also draw the performance-FLOPs curve on LSVQ and LIVE-VQC in Fig.~\ref{fig:a1}. As we can see, FAST-VQA reduces up to  FLOPs and  running time than PVQ while obtaining notably better performance. The more efficient version, FAST-VQA-M, only requires  FLOPs of PVQ and  FLOPs of our full-resolution baseline while still achieving slightly better performance. Moreover, FAST-VQA (especially FAST-VQA-M) also runs very fast even on CPU, which reduces the hardware requirements for the applications of deep 
VQA methods. All these comparisons show the unprecedented efficiency of proposed FAST-VQA. \footnote{Also, RAPIQUE\cite{rapique} can also infer rapidly on CPU that requires \textbf{17.3s} for 1080P videos. Yet, it is not compatible with GPU Inference due to its handcrafted branch.}

\subsection{Transfer Learning with Video-quality-related Representations} 

\vspace{-5pt}
\begin{table}[]
\footnotesize
\caption{The finetune results on LIVE-VQC, KoNViD, CVD2014 and YouTube-UGC datasets, compared with existing classical and fixed-backbone deep VQA methods, and ensemble approaches of classical (C) and deep (D) branches.} \label{table:vqc}
\setlength\tabcolsep{5.5pt}
\renewcommand\arraystretch{1.25}
\footnotesize
\centering
\resizebox{1.\textwidth}{!}{\begin{tabular}{l:l|cc|cc|cc|cc|cc}
\hline
\multicolumn{2}{l|}{\textbf{Finetune Dataset}/}         & \multicolumn{2}{c|}{LIVE-VQC}   & \multicolumn{2}{c|}{KoNViD-1k}        &  \multicolumn{2}{c|}{CVD2014}   &  \multicolumn{2}{c|}{LIVE-Qualcomm}  & \multicolumn{2}{c}{YouTube-UGC}             \\ \hline
Groups~~~ &~Methods                 & SRCC & PLCC    & SRCC & PLCC           & SRCC & PLCC          &SRCC&PLCC            &SRCC& PLCC                               \\ \hline 
\multirow{3}{0pt}{{{Existing Classical}}} &~TLVQM\cite{tlvqm}        & 0.799 &  0.803  & 0.773 & 0.768     & 0.83 &    0.85               & 0.77 & 0.81 & 0.669 &  0.659 \\

&~VIDEVAL\cite{videval}      & 0.752 &  0.751  & 0.783 & 0.780     & NA &     NA    & NA & NA               & 0.779 &  0.773 \\
&~RAPIQUE\cite{rapique}      & 0.755 &  0.786  & 0.803 & 0.817     & NA &     NA  & NA & NA                 & 0.759 &  0.768 \\\hdashline 
\multirow{4}{0pt}{{{Existing \textbf{Fixed} Deep}}} &~VSFA\cite{vsfa}          & 0.773 &  0.795  & 0.773 & 0.775     & 0.870 &     0.868 & 0.737 & 0.732 & 0.724 &  0.743\\
&~PVQ\cite{pvq}   & \blue{0.827} &  \blue{0.837}  & 0.791 &   0.786       & NA & NA  & NA &     NA                   & NA &  NA\\ 
&~GST-VQA\cite{gstvqa}  & NA &  NA  & 0.814 &   0.825         & 0.831 & 0.844  & 0.801 &  0.825  & NA & NA\\ 
&~CoINVQ\cite{rfugc} & NA &  NA & 0.767 &  0.764  & NA & NA &  NA & NA   & \blue{0.816} &     {0.802}  \\ \hdashline
\multirow{2}{0pt}{{{Ensemble C+D}}} & CNN+TLVQM\cite{cnntlvqm}        & 0.825 & 0.834 & 0.816 & 0.818 & 0.863 & 0.880  & \blue{0.810} & 0.833 & NA & NA \\
& CNN+VIDEVAL\cite{videval}        & 0.785 & 0.810 & 0.815 & 0.817 & NA & NA  & NA & NA & 0.808 & \blue{0.803} \\\hdashline
\multicolumn{2}{l|}{Full-res Swin-T\cite{swin3d} \textit{features}} & 0.799 & 0.808 & 0.841 & 0.838 & 0.868 & 0.870 & 0.788 & 0.803 & 0.798 & 0.796 \\ \hline

\multicolumn{2}{l|}{{FAST-VQA-M} (Ours)} & 0.803 & 0.828 & \blue{0.873} & \blue{0.872} & \blue{0.877} & \blue{0.892} & 0.804 & \blue{0.838} & 0.768 & 0.765\\ \hline
\multicolumn{2}{l|}{{FAST-VQA} \textit{w/o VQ-representations} (Ours)} & 0.765 & 0.782 & 0.842 & 0.844 & 0.871 & 0.888 & 0.756 & 0.778 & 0.794 & 0.784 \\ \hdashline
\multicolumn{2}{l|}{\textbf{FAST-VQA} (ours)} &  \textbf{\red{0.849}} & \textbf{\red{0.865}} & \textbf{\red{0.891}} & \textbf{\red{0.892}} & \textbf{\red{0.891}} & \textbf{\red{0.903}} & \textbf{\red{0.819}} & \textbf{\red{0.851}} & \textbf{\red{0.855}} & \textbf{\red{0.852}}  \\ \hline
\multicolumn{2}{l|}{Improvements led by \textit{VQ-representations}} & \red{+11.0\%} & \red{+10.6\%}  & \red{+5.8\%}  &\red{+5.7\%}  & \red{+2.3\%}  & \red{+1.7\%}  & \red{+8.3\%}  & \red{+9.4\%}  & \red{+7.7\%}  & \red{+8.7\%}  \\  \hline
\end{tabular}}
\vspace{-12pt}
\end{table}

FAST-VQA also makes the pretrain-finetune scheme on VQA possible with affordable computation resources. With FAST-VQA, we can pretrain with large VQA datasets in end-to-end manner to learn quality related features, and then transfer to specific VQA scenarios where only small datasets are available. Note that this manner is not applicable to current methods due to their high computational load (as discussed in Sec.~\ref{sec:eff}). We use LSVQ as the large dataset and choose four small datasets representing diverse scenarios, including LIVE-VQC (real-world mobile photography, 240P-1080P), KoNViD-1k (various contents collected online, all 540P), CVD2014 (synthetic in-capture distortions, 480P-720P), LIVE-Qualcomm (selected types of distortions, all 1080P) and YouTube-UGC (user-generated contents, including computer graphic contents, 360P-2160P\footnote{Due to privacy reasons, the current public version of YouTube-UGC is incomplete and only with 1147 videos. The peer comparison is only for reference.}). We divide each dataset into random splits for 10 times and report the average result on the test splits. As Tab.~\ref{table:vqc} shows, with video-quality-related representations, the proposed FAST-VQA outperforms the existing state-of-the-arts on all these scenarios while obtaining much higher efficiency. Note that YouTube-UGC contains 4K(2160P) videos but FAST-VQA still performs well. Even without video-quality-related representations, FAST-VQA also still achieves competitive performance, while these features steadily improve the performance. It implies that the pretrained FAST-VQA can be able to serve as a strong backbone that boost further downstream tasks related to video quality.

\begin{comment}
\begin{table}[]
\footnotesize
\renewcommand\arraystretch{1.15}
\setlength\tabcolsep{2pt}
\centering
\vspace{-20pt}
\caption{The mean and (\textit{std. dev.}) of results on LIVE-VQC, KoNViD, CVD2014 and YouTube-UGC datasets, compared to baseline methods and \textit{best existing} method. The improvements are compared to FAST-VQA.} 
\begin{tabular}{l|ccc}
\hline
\text{\textbf{LIVE-VQC} (585)} & SRCC(std)               & PLCC(std)                 & KRCC(std)   \\ \hline
\textit{best existing:} PVQ\cite{pvq}          & \blue{0.827}       & \blue{0.837}        & NA     \\ 
\hdashline
Full-res Swin-T\cite{swin3d} \textit{features} &  0.799(0.033) & 0.808(0.028) & 0.613(0.036)  \\ \hline
FAST-VQA & {0.765(0.039)} & {0.782(0.034)} & {0.573(0.039)} \\ \hline
\textbf{FAST-VQA}   & \red{0.849(0.025)}\red{} & \red{0.862(0.019)}\red{} & \red{0.664(0.028)} \\ \hline
\hline
\text{\textbf{KoNViD-1k} (1200)} & SRCC(std)               & PLCC(std)                 & KRCC(std)   \\ \hline
\textit{best existing:} GST-VQA\cite{gstvqa} & 0.814(0.026)          & 0.825(0.043)          & 0.621(0.027)      \\ 
\hdashline
Full-res Swin-T\cite{swin3d} \textit{features} &  0.841(0.018) & 0.838(0.025) & 0.648(0.019)  \\ \hline
FAST-VQA  & \blue{0.842(0.012)} & \blue{0.844(0.011)} & \blue{0.651(0.015)} \\ \hline
\textbf{FAST-VQA}  & \red{0.891(0.009)}\red{} & \red{0.892(0.008)}\red{} & \red{0.715(0.011)} \\ \hline
\hline
\textbf{CVD2014} (234) & SRCC(std)               & PLCC(std)                 & KRCC(std)   \\ \hline
\textit{best existing:} VSFA\cite{vsfa}             & 0.870(0.037)          & 0.868(0.032)          & 0.695(0.047)      \\ 
\hdashline
Full-res Swin-T\cite{swin3d} \textit{features} &  0.869(0.027) & 0.878(0.025) & 0.698(0.035)  \\
\hline
FAST-VQA & \blue{0.871(0.032)} & \blue{0.888(0.017)} & \blue{0.699(0.040)} \\ \hline

\textbf{FAST-VQA} & \red{0.891(0.030)}\red{} & \red{0.903(0.019)}\red{} & \red{0.721(0.031)} \\ \hline
\hline
\textbf{YouTube-UGC} (1147) & SRCC(std)               & PLCC(std)                 & KRCC(std)   \\ \hline
\textit{best existing:} CoINVQ\cite{rfugc}         & \blue{0.816}          & \blue{0.802}          & NA     \\  \hdashline
Full-res Swin-T\cite{swin3d} \textit{features} &  0.798(0.027) & 0.796(0.021) & 0.603(0.024)  \\ \hline
FAST-VQA & {0.794(0.016)} & {0.784(0.016)} & {0.596(0.017)} \\ \hline
\textbf{FAST-VQA} & \red{0.855(0.008)}\red{} & \red{0.852(0.011)}\red{} & \red{0.667(0.012)} \\ \hline


\end{tabular}
\vspace{-20pt}
\end{table}
\end{comment}



\subsection{Ablation Studies on \textit{fragments}}

For the first part of ablation studies, we prove the effectiveness of \frag~by comparing with other common sampling approaches and different variants of fragments (Tab.~\ref{tab:gmstfa}). We keep the FANet structure fixed during this part.

\begin{table}[]
\footnotesize
\vspace{-6pt}
\caption{Ablation study on \frag: comparison with resizing, cropping (Group 1) and different variants for fragments (Group 2).} 
\setlength\tabcolsep{5pt}
\renewcommand\arraystretch{1.15}
\footnotesize
\centering
\label{tab:resizecrop}
\resizebox{.8\textwidth}{!}{\begin{tabular}{l|cc|cc|cc|cc}
\hline
\textbf{Testing Set}/         & \multicolumn{2}{c|}{\textbf{LSVQ}}   & \multicolumn{2}{c|}{\textbf{LSVQ}}        &  \multicolumn{2}{c|}{\textbf{KoNViD-1k}}  & \multicolumn{2}{c}{\textbf{LIVE-VQC}}             \\ \cline{2-9}
Methods/Metric                    & SRCC & PLCC    & SRCC & PLCC           & SRCC & PLCC                      & SRCC & PLCC     \\ \hline     
\multicolumn{9}{l}{{Group 1: Naive Sampling Approaches}} \\ \hdashline
\textit{bilinear resizing} &  0.857 & 0.859 & 0.752 & 0.786 & 0.841 & 0.840         &  0.772 & 0.814 \\ \hdashline
\textit{random cropping} &  0.807 & 0.812 & 0.643 & 0.677 & 0.734 & 0.776         &  0.740 & 0.773 \\
- test with 3 crops &  0.838 & 0.835 & 0.727 & 0.754 & 0.841 & 0.827         &  0.785 & 0.809 \\
- test with 6 crops &  0.843 & 0.844 & 0.734 & 0.761 & 0.845 & 0.834         &  0.796 & 0.817 \\ \hline
\multicolumn{9}{l}{{Group 2: Variants of \frag}}    \\ \hdashline                
\textit{random mini-patches} &  0.857 & 0.861 & 0.754 & 0.790 & 0.844 & 0.845 &  0.792 & 0.818 \\ 
\textit{shuffled mini-patches} &  0.858 & 0.863 & 0.761 & 0.799 & 0.849 & 0.847 &  0.796 & 0.821 \\ \hdashline
\textit{w/o} temporal alignment &  0.850 & 0.853 & 0.736 & 0.779 & 0.823 & 0.816         &  0.764 & 0.802 \\
\hline
\textit{\textbf{fragments}} (ours) &  \textbf{\red{0.876}} & \textbf{\red{0.877}}  & \textbf{\red{0.779}} & \textbf{\red{0.814}} & \textbf{\red{0.859}} & \textbf{\red{0.855}}& \textbf{\red{0.823}} & \textbf{\red{0.844}} \\ \hline
\end{tabular}}
\label{tab:gmstfa}
\vspace{-15pt}
\end{table}

\paragraph{Comparing with resizing/cropping} In Group 1 of Tab.~\ref{tab:resizecrop}, we compare the proposed fragments with two common sampling approaches: \textit{bilinear resizing} and \textit{random cropping}. The proposed \textit{fragments} are notably better than bilinear resizing on \textbf{high-resolution} (LSVQ) (+4\%) and \textbf{cross-resolution} (LIVE-VQC) scenarios (+4\%). Fragments still lead to non-trivial 2\% improvements than resizing on lower-resolution scenarios where the problems of resizing is not that severe. This proves that keeping local textures is vital for VQA. Fragments also largely outperform single random crop as well as ensemble of multiple crops, suggesting that retaining the uniform global quality is also critical to VQA.

\paragraph{Comparing with variants of fragments} We also compare with three variants of \frag~in Tab.~\ref{tab:gmstfa}, Group 2. We prove the effectiveness of uniform grid partition by comparing with \textit{random mini-patches} (ignore grids while sampling), and the importance of retaining contextual relations by comparing with \textit{shuffled mini-patches}. Fragments show notable improvements than both variants. Moreover, the proposed fragments show much better performance than the variant \textit{without} temporal alignment especially on high resolution videos, suggesting that preserving the inter-frame temporal variations is necessary for fragments. 

\begin{table}
\setlength\tabcolsep{5pt}
\renewcommand\arraystretch{1.15}
\footnotesize
\vspace{-22pt}
\caption{Ablation study on FANet design: the effects for GRPB and IP-NLR modules.} 
\centering
\resizebox{.8\textwidth}{!}{\begin{tabular}{l|cc|cc|cc|cc}
\hline
\textbf{Testing Set}/         & \multicolumn{2}{c|}{\textbf{LSVQ}}   & \multicolumn{2}{c|}{\textbf{LSVQ}}        &  \multicolumn{2}{c|}{\textbf{KoNViD-1k}}  & \multicolumn{2}{c}{\textbf{LIVE-VQC}}             \\ \cline{2-9}
Variants/Metric                   & SRCC & PLCC    & SRCC & PLCC           & SRCC & PLCC                      & SRCC & PLCC     \\ \hline                
\textit{w/o} GRPB &  0.873 & 0.872 & 0.769 & 0.805 & 0.854 & 0.853 &  0.808 & 0.832 \\ 
\textit{semi}-GRPB on Layer 1/2 &  0.873 & 0.875 & 0.772 & 0.809 & 0.856 & 0.851 &  0.812 & 0.838 \\ \hdashline
\textit{linear} Regression &  0.872 & 0.873 & 0.768 & 0.803 & 0.847 & 0.849         &  0.810 & 0.835 \\
\textit{PrePool non-linear} Regression &  0.873 & 0.874 & 0.771 & 0.805 & 0.851 & 0.850         &  0.813 & 0.834 \\\hline
\textbf{FANet} (ours) &  \textbf{\red{0.876}} & \textbf{\red{0.877}}  & \textbf{\red{0.779}} & \textbf{\red{0.814}} & \textbf{\red{0.859}} & \textbf{\red{0.855}}& \textbf{\red{0.823}} & \textbf{\red{0.844}}  \\ \hline
\end{tabular}}
\label{tab:netdesign}
\vspace{-22pt}
\end{table}


\subsection{Ablation Studies on FANet}


\paragraph{Effects of GRPB and IP-NLR} In the second part of ablation studies, we analyze the effects of two important designs in FANet: the proposed Gated Relative Position Biases (GRPB) and Intra-Patch Non-Linear Regression (IP-NLR) VQA Head as in Tab.~\ref{tab:netdesign}. We compare the IP-NLR with two variants: the linear regression layer and the non-linear regression layers with pooling before regression (\textit{PrePool}). Both modules lead to non-negligible improvements especially on high-resolution (LSVQ) or cross-resolution (LIVE-VQC) scenarios. As  the discontinuity between mini-patches is more obvious in high-resolution videos, this result suggests that the corrected position biases and regression head are helpful on solving the problems caused by such discontinuity.

\subsection{Reliability and Robustness Analyses}

As FAST-VQA is based on samples rather than original videos while a single sample for \frag~only keeps 2.4\% spatial information in 1080P videos, it is important to analyze the reliability and robustness of FAST-VQA predictions.

\paragraph{Reliability of Single Sampling.} We measure the reliability of single sampling in FAST-VQA by two metrics: 1) the assessment stability of different single samplings on the same video; 2) the relative accuracy of single sampling compared with multiple sample ensemble. As shown in Tab.~\ref{tab:stability}, the normalized \textit{std. dev.} of different sampling on a same video is only around 0.01, which means the sampled fragments are enough to make very stable predictions. Compared with 6-sample ensemble, sampling only once can already be 99.40\% as accurate even on the pure high-resolution test set (LSVQ). They prove that a single sample of \frag~is enough stable and reliable for quality assessment even though only a small proportion of information is kept during sampling.

\begin{table}
\center
\setlength\tabcolsep{6pt}
\renewcommand\arraystretch{1.15}
\footnotesize
\vspace{-18pt}
\caption{Assessment stability and relative accuracy of single sampling of \frag.} 
\resizebox{.96\textwidth}{!}{\begin{tabular}{l|c|c|c|c}
\hline
\textbf{Testing Set}/         & \multicolumn{1}{|c|}{\textbf{LSVQ}}   & \multicolumn{1}{|c|}{\textbf{LSVQ}}        &  \multicolumn{1}{|c|}{\textbf{KoNViD-1k}}  & \multicolumn{1}{|c}{\textbf{LIVE-VQC}}             \\ \cline{2-5}
Score Range & 0-100 & 0-100 & 1-5 & 0-100 \\ \hline
\textit{std. dev.} of Single Samplings  & 0.65 & 0.79 & 0.046 & 1.07 \\ 
Normalized \textit{std. dev.} & 0.0065 & 0.0079 & 0.0115 & 0.0107 \\ \hline
Relative Pair Accuracy compared with 6-samples & 99.59\% & 99.40\% & 99.45\% & 99.52\%  \\ \hline
\end{tabular}}
\label{tab:stability}
\vspace{-20pt}
\end{table}



\paragraph{Robustness on Different Resolutions}
    To analyze the robustness of FAST-VQA on different resolutions, we divide the cross-resolution VQA benchmark set LIVE-VQC into three resolution groups: (A) 1080P (110 videos); (B) 720P (316 videos); (C) 540P (159 videos) to see the performance of FAST-VQA on different resolutions, compared with several variants. As the results shown in Tab.~\ref{tab:resolution}, the proposed FAST-VQA shows good performance ( SRCC\&PLCC) on all resolution groups and most superior improvement than other variants on Group (A) with 1080P high-resolution videos, proving that FAST-VQA is robust and reliable on different resolutions of videos.

\begin{table}
\center
\setlength\tabcolsep{5pt}
\renewcommand\arraystretch{1.15}
\footnotesize
\vspace{-16pt}
\caption{Performance comparison on different resolution groups of LIVE-VQC dataset.} 
\resizebox{.8\textwidth}{!}{\begin{tabular}{l|ccc|ccc|ccc}
\hline
\textbf{Resolution}     & \multicolumn{3}{c|}{(A): 1080P}   & \multicolumn{3}{c|}{(B): 720P}        &  \multicolumn{3}{|c}{(C): 540P}    \\ \cline{2-10}
Variants                    & SRCC & PLCC & KRCC    & SRCC & PLCC  & KRCC         & SRCC & PLCC                & KRCC     \\ \hline    
\textit{Full-res} Swin \textit{features} (Baseline) & 0.771 & 0.774 & 0.584 & 0.796 & 0.811 & 0.602 & 0.810 & 0.853 & 0.625 \\ \hdashline
\textit{bilinear resizing} (Sampling Variant) & 0.758 & 0.773 & 0.573 & 0.790 & 0.822 & 0.599 & 0.835 & 0.878 & 0.650 \\ \hdashline
\textit{random cropping} (Sampling Variant) & 0.765 & 0.768 & 0.565 & 0.774 & 0.787 &  0.581 & 0.730 & 0.809 & 0.535 \\  \hdashline

\textit{w/o} GRPB (FANet Variant) & 0.796 & 0.785 & 0.598 & 0.802 & 0.820 & 0.608 & 0.834 & 0.883 & 0.649 \\ \hline
\textbf{FAST-VQA} (Ours) & \bred{0.807} & \bred{0.806} & \bred{0.610} & \bred{0.803} & \bred{0.825} & \bred{0.610} & \bred{0.840} & \bred{0.885} & \bred{0.654} \\ \hline

\end{tabular}}
\label{tab:resolution}
\vspace{-25pt}
\end{table}

\begin{figure}[]
    \centering
    \includegraphics[width=0.88\linewidth]{Author Guidelines for ECCV Submission/Figure7_compressed.pdf}
    \vspace{-12pt}
    \caption{Spatial-temporal patch-wise local quality maps, where \textbf{\red{red}} areas refer to low predicted quality and \textbf{\green{green}} areas refer to high predicted quality. This sample video is a 1080P video selected from LIVE-VQC~\cite{vqc} dataset. Zoom in for clearer view.}
    \label{fig:6}
    \vspace{-16pt}
\end{figure}

\subsection{Qualitative Results: Local Quality Maps}

 The proposed IP-NLR head with patch-wise independent quality regression enables FAST-VQA to generate patch-wise local quality maps, which helps us to qualitatively evaluate what quality information can be learned in FAST-VQA. We show the patch-wise local quality maps and the re-projected frame quality maps for a 1080P video (from LIVE-VQC~\cite{vqc} dataset) in Fig.~\ref{fig:6}. As the patch-wise quality maps and re-projected quality maps in Fig.~\ref{fig:6} (column 2\&4) shows, FAST-VQA is sensitive to textural quality information and distinguishes between clear (Frame 0) and blurry textures (Frame 12/24). It demonstrates that FAST-VQA with \frag~(column 3) as input is sensitive to local texture quality. Furthermore, the qualities of the action-related areas are notably different from the background areas, showing that FAST-VQA effectively learns the global scene information and contextual relations in the video.

\section{Conclusions}

Our paper has shown that proposed \frag~are effective samples for video quality assessment (VQA) that better retain quality information in videos than naive sampling approaches, to tackle the difficulties as results of high computing and memory requirements when high-resolution videos are to be evaluated. Based on \frag, the proposed end-to-end FAST-VQA achieves higher efficiency ( FLOPs) and accuracy ( PLCC) simultaneously than existing state-of-the-art method PVQ on 1080P videos. We hope that the FAST-VQA can bring deep VQA methods into practical use for videos in any resolutions.

\section{Acknowledgement}
This study is supported under the RIE2020 Industry Alignment Fund – Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contribution from the industry partner(s).







\begin{comment}
\appendix

\section{Additional Results}
\label{sec:b}

\subsection{Result with Patch Labels}

The LSVQ dataset crops videos into spatial-temporal crops and additionally labels the qualities of crops, while PVQ \cite{pvq} uses these \textit{patch labels} for training and gets better performance. We also use these additional labels for training, by regarding the video crops as independent training videos, the same way as PVQ does. As shown in Tab.~\ref{tab:patches}, the patch labels contribute to performance on cross-dataset scenarios such as KonViD-1k and LIVE-VQC, but reduce the intra-dataset performance on LSVQ. This might be due to the `\textit{patching up}' adds domain gap between the original videos in test set and cropped video patches in training set, but lead to cross-dataset gain due to data augmentation effect. We report the result in our main paper without these additional labels, and will make future explorations on how these patch labels can be better used.


\begin{table}[]
\footnotesize
\vspace{-8pt}
\caption{Results with additional patch labels.} \label{tab:patches}
\vspace{-10pt}
\setlength\tabcolsep{5pt}
\renewcommand\arraystretch{1.15}
\footnotesize
\center
\resizebox{0.95\textwidth}{!}{\begin{tabular}{l|cc|cc|cc|cc}
\hline
\textbf{Testing Set}/         & \multicolumn{2}{|c|}{\textbf{LSVQ}}   & \multicolumn{2}{|c|}{\textbf{LSVQ}}        &  \multicolumn{2}{|c|}{\textbf{KoNViD-1k}}  & \multicolumn{2}{|c}{\textbf{LIVE-VQC}}             \\ \cline{2-9}
Methods                    & SRCC & PLCC    & SRCC & PLCC           & SRCC & PLCC                      & SRCC & PLCC                               \\ \hline 
PVQ (\textit{w/o} patch)\cite{pvq}   & 0.814 &  0.816  & 0.686 &   0.708         & 0.781 &     0.781                   & 0.747 &  0.776                               \\ 
+ \textit{patch labels as independent sub-videos}   & 0.827 &  0.828 & 0.711 &  0.739     & 0.791 &     0.795  & 0.770 &  0.807   \\ \hdashline
\gray{FAST-VQA-M} (Ours) &  \gray{0.852} & \gray{0.854} & \gray{0.739} & \gray{0.773} & \gray{0.841} & \gray{0.832} & \gray{0.788} & \gray{0.810} \\
+ \textit{patch labels as independent sub-videos}   &  \gray{0.846} & \gray{0.849
} & \gray{0.738} & \gray{0.776} & \gray{0.850} & \gray{0.847} & \gray{0.790} & \gray{0.818} \\ \hdashline

\textbf{FAST-VQA}(ours) &  \textbf{\red{0.876}} & \textbf{\red{0.877}} & \textbf{\red{0.779}} & \textbf{\red{0.814}} & {{0.859}} & {{0.855}} & {{0.823}} & 0.844 \\
+ \textit{patch labels as independent sub-videos}  &  {0.867} & {0.869} & {0.767} & {0.800} & \textbf{\red{0.862}} & \textbf{\red{0.860}} & \textbf{\red{0.826}} & \textbf{\red{0.845}} \\ \hline
\end{tabular}}
\vspace{-9pt}

\end{table}

\subsection{Cross-Datasets Results on More Datasets}

We show our cross-dataset results on more NR-VQA datasets CVD2014, LIVE-Qualcomm and YouTube-UGC as a complementary to Tab.~\ref{table:peer}, though no published approaches have evaluated on such settings so we are not able to benchmark them. These datasets are all with scenarios slightly different from LSVQ so direct cross-data results are relatively less competitive; for example, CVD2014\cite{cvd} and LIVE-Qualcomm\cite{qualcomm} are synthetic in-capture VQA datasets, where YouTube-UGC\cite{ytugc} contains a large proportion of non-natural algorithm-generated videos. Still, these datasets are common VQA datasets consisted of videos free from artificial distortions, so we also report the cross-dataset performance on them for a reference for future approaches to taken into comparison.


\begin{table}[]
\caption{Cross-dataset performance on more datasets, including CVD2014, LIVE-Qualcomm and YouTube-UGC.} \label{table:crosscomp}
\setlength\tabcolsep{12pt}
\renewcommand\arraystretch{1.15}
\footnotesize
\center
\resizebox{.9\linewidth}{!}{\begin{tabular}{l|cc|cc|cc}
\hline
{\textbf{Training Set}/} &   \multicolumn{6}{c}{\textbf{LSVQ}}            \\ \hdashline
{\textbf{Testing Set}/}         & \multicolumn{2}{c|}{\textbf{CVD2014}}   & \multicolumn{2}{c|}{\textbf{YouTube-UGC}}        &  \multicolumn{2}{c}{\textbf{LIVE-Qualcomm}}           \\ \hline
Methods                    & SRCC & PLCC    & SRCC & PLCC           & SRCC & PLCC                              \\ \hline 
{\textbf{FAST-VQA} (Ours)} &  {{0.807}} & {{0.816}}  & {{0.730}} & {{0.745}} & {0.734} & {0.748} \\ \hline
\end{tabular}}
\vspace{-18pt}
\end{table}




\subsection{Full Results on Finetune Experiments}
\label{sec:a2}

\paragraph{KoNViD-1k} KoNViD-1k~\cite{kv1k} is the most commonly recognized in-the-wild VQA benchmark dataset, containing 1200 videos in the same resolution. As shown in Tab~\ref{tab:kv1k} KoNViD-1k, FAST-VQA can already reach good performance without video-quality-related feature, while the features contribute to 6\% significant improvement. It further proves the effectiveness of these features.

\begin{table*}[]
\footnotesize
\renewcommand\arraystretch{1.15}
\setlength\tabcolsep{7pt}
\centering
\vspace{-20pt}
\caption{The mean and (\textit{std. dev.}) of results on KoNViD-1k dataset with video-quality-related representations, compared with classical \& deep VQA methods.} \label{tab:kv1k}
\resizebox{0.92\textwidth}{!}{\begin{tabular}{l|ccc} \hline

\text{\textbf{KoNViD-1k} (1200)} & SRCC(std)               & PLCC(std)                 & KRCC(std)   \\ \hline
V-BLIINDS\cite{vbliinds}        &  0.710(0.031)          & 0.704(0.030)          & 0.519(0.026)      \\ 
TLVQM\cite{tlvqm}           & 0.773(0.024)          & 0.768(0.023)          & 0.577(0.022)      \\ 
VIDEVAL\cite{videval}          & 0.783(0.021)          & 0.780(0.021)          & 0.585(0.021)     \\
RAPIQUE\cite{rapique}          & 0.803         & 0.817       & NA      \\ 
\hdashline
3D-CNN + LSTM\cite{cnn+lstm} & 0.808  & 0.800 & NA \\
CNN-TLVQM\cite{cnntlvqm}          & 0.83       & 0.83        & NA     \\
PVQ\cite{pvq}          & 0.791       & 0.786        & NA     \\ 
VSFA\cite{vsfa}             & 0.773(0.019)          & 0.775(0.019)          & 0.578(0.019)      \\ 
GST-VQA\cite{gstvqa} & 0.814(0.026)          & 0.825(0.043)          & 0.621(0.027)      \\ MLSP-FF\cite{mlsp}         & 0.82(0.02)          & NA                   & NA   \\
\hdashline
Full-res Swin-T\cite{swin3d} \textit{features} &  0.841(0.018) & 0.838(0.025) & 0.648(0.019)  \\ \hline
 FAST-VQA-M    &  0.825(0.014)      & 0.825(0.013)         & 0.634(0.017)    \\
 FAST-VQA-M    &  \blue{0.873(0.012)}          & \blue{0.872(0.012)}          & \blue{0.689(0.015)}      \\ \hdashline
FAST-VQA  & {0.842(0.012)} & {0.844(0.011)} & {0.651(0.015)} \\ 
\textbf{FAST-VQA}  & \bred{0.891(0.009)} & \bred{0.892(0.008)} & \bred{0.715(0.011)} \\ \hline
\end{tabular}}
\vspace{-20pt}
\end{table*}

\paragraph{LIVE-VQC} LIVE-VQC\cite{vqc} dataset is a cross-resolution in-the-wild dataset which contains 585 videos, with 19\% of videos in 1080P and 73\% of them 720P. Tab.~\ref{tab:vqc} shows the performance comparison on LIVE-VQC dataset. The video quality features help FAST-VQA reach state-of-the-art performance in this dataset and lead to 10\% improvement than the variant without video quality features, showing the power of the proposed pretrain-finetune scheme in FAST-VQA.

\begin{table*}[]
\footnotesize
\renewcommand\arraystretch{1.15}
\setlength\tabcolsep{7pt}
\centering
\vspace{-20pt}
\caption{The mean and (\textit{std. dev.}) of results on LIVE-VQC dataset with video-quality-related representations, compared with classical \& deep VQA methods.} \label{tab:vqc}
\resizebox{0.92\textwidth}{!}{\begin{tabular}{l|ccc} \hline
\text{\textbf{LIVE-VQC}} (585) & SRCC(std)               & PLCC(std)                 & KRCC(std)   \\ \hline
V-BLIINDS\cite{vbliinds}        & 0.694(0.050)          & 0.718(0.050)          & 0.508(0.042)     \\ 
TLVQM\cite{tlvqm}           & 0.799(0.036)          & 0.803(0.036)          & 0.608(0.037)     \\ 
VIDEVAL\cite{videval}          & 0.752(0.039)          & 0.751(0.042)          & 0.564(0.036)    \\
RAPIQUE\cite{rapique}          & 0.755       & 0.786        & NA      \\ 
\hdashline
PVQ\cite{pvq}          & \blue{0.827}       & \blue{0.837}        & NA     \\ 
CNN-TLVQM\cite{cnntlvqm}          & \blue{0.83}       & \blue{0.84}        & NA     \\
VSFA\cite{vsfa}             & 0.773(0.027)          & 0.795(0.026)          & 0.581(0.031)\\ 
MLSP-FF\cite{mlsp}         & 0.72(0.06)          & NA                   & NA   \\\hdashline
Full-res Swin-T\cite{swin3d} \textit{features} &  0.799(0.033) & 0.808(0.028) & 0.613(0.036)  \\ \hline
FAST-VQA-M    & 0.754(0.034)         & 0.772(0.027)          & 0.563(0.031)    \\
FAST-VQA-M    &  0.803(0.032)          & 0.828(0.030)          & 0.614(0.033)      \\ \hdashline
FAST-VQA & {0.765(0.039)} & {0.782(0.034)} & {0.573(0.039)} \\ 
\textbf{FAST-VQA}   & \bred{0.849(0.025)} & \bred{0.865(0.019)} & \bred{0.664(0.028)} \\ \hline
\end{tabular}}
\vspace{-20pt}
\end{table*}

\paragraph{CVD2014} CVD2014~\cite{cvd} dataset is a synthetic VQA dataset that focus on in-capture distortions. Tab.~\ref{tab:cvd} shows the performance comparison on CVD2014 dataset, where the FAST-VQA reaches state-of-the-art even without video-quality-related representations and video-quality-related representations from LSVQ training still further improves 1.5\% performance.

\begin{table*}[]
\footnotesize
\renewcommand\arraystretch{1.15}
\setlength\tabcolsep{7pt}
\centering
\vspace{-20pt}
\caption{The mean and (\textit{std. dev.}) of results on CVD2014 dataset with video-quality-related representations, compared with classical \& deep VQA methods.} \label{tab:cvd}
\resizebox{0.92\textwidth}{!}{\begin{tabular}{l|ccc} \hline

\textbf{CVD2014} (234) & SRCC(std)               & PLCC(std)                 & KRCC(std)   \\ \hline
V-BLIINDS\cite{vbliinds}        &  0.746(0.056)          & 0.753(0.053)          & 0.562(0.057)      \\ 
TLVQM\cite{tlvqm}           & 0.83(0.04)          & 0.85(0.04)          & NA    \\ \hdashline
VSFA\cite{vsfa}             & 0.870(0.037)          & 0.868(0.032)          & 0.695(0.047)      \\ 
MLSP-FF\cite{mlsp}         & 0.77(0.06)          & NA                   & NA   \\ \hdashline
Full-res Swin-T\cite{swin3d} \textit{features} &  0.869(0.027) & 0.878(0.025) & 0.698(0.035)  \\ \hdashline
FAST-VQA-M    &  0.857(0.028)         & 0.867(0.019)         & 0.674(0.029)     \\
FAST-VQA-M    & \blue{0.877(0.035)}         & \blue{0.892(0.019)}          & \blue{0.705(0.041)}   \\ \hdashline
FAST-VQA & {0.871(0.032)} & {0.888(0.017)} & {0.699(0.040)} \\

\textbf{FAST-VQA} & \bred{0.891(0.030)} & \bred{0.903(0.019)} & \bred{0.721(0.031)} \\ \hline
\end{tabular}}
\vspace{-20pt}
\end{table*}

\paragraph{YouTube-UGC} YouTube-UGC\cite{ytugc,ytugccc} dataset is a challenging dataset for FAST-VQA as it contains 10\% \textbf{4K} videos with 20-second duration, where FAST-VQA only samples 0.1\% information on these videos for evaluation (FAST-VQA-M only samples 0.02\%). However, the proposed FAST-VQA still reaches state-of-the-art performance, as shown in Tab.~\ref{tab:ugc}, while the video-quality-related representations contribute to more than 10\% improvement.We also note that FAST-VQA-M \textit{does not perform well} on this dataset, which suggests that the density of FAST-VQA-M cannot well handle 4K videos.

\begin{table*}[]
\footnotesize
\renewcommand\arraystretch{1.15}
\setlength\tabcolsep{7pt}
\centering
\vspace{-20pt}
\caption{The mean and (\textit{std. dev.}) of results on YouTube-UGC dataset with video-quality-related representations, compared with classical \& deep VQA methods.} \label{tab:ugc}
\resizebox{0.92\textwidth}{!}{\begin{tabular}{l|ccc} \hline

\textbf{YouTube-UGC} (1147) & SRCC(std)               & PLCC(std)                 & KRCC(std)   \\ \hline
TLVQM\cite{tlvqm}           & 0.669(0.030)          & 0.659(0.030)          & 0.482(0.025)   \\
RAPIQUE\cite{rapique}   & 0.759  &  0.768  & \\
VIDEVAL\cite{videval}   & 0.779(0.025)          & 0.773(0.025)          & 0.583(0.023)   \\ \hdashline
VSFA\cite{vsfa}         & 0.724          & 0.743          & NA     \\ 
CoINVQ\cite{rfugc}         & \blue{0.816}          & \blue{0.802}          & NA     \\  \hdashline
Full-resolution Swin-T \textit{features}\cite{swin3d} &  0.798(0.027) & 0.796(0.021) & 0.603(0.024)  \\ \hdashline
FAST-VQA-M    &  0.645(0.029)         & 0.628(0.034)         & 0.459(0.025)    \\
FAST-VQA-M   &  0.768(0.020)          & 0.765(0.019)         & 0.572(0.022)      \\ \hdashline
FAST-VQA & {0.794(0.016)} & {0.784(0.016)} & {0.596(0.017)} \\

\textbf{FAST-VQA} & \bred{0.855(0.008)} & \bred{0.852(0.011)} & \bred{0.667(0.012)} \\ \hline
\end{tabular}}
\vspace{-20pt}
\end{table*}







\subsection{Resolution Sensitivity}

To demonstrate that keeping the original resolution during sampling is quite important, we use LSVQ test set in downsample evaluation to prove that \frag~ and FAST-VQA are sensitive to resolution changes: We resize these 1080P high-resolution videos into 540P(2X), 360P(3X), 270P(4X) and sample \textit{{fragments}} from the resized videos. As the results in Tab.~\ref{tab:cres} shows, the proposed FAST-VQA is sensitive to large-scale downsampling, and suffer around 20\% accuracy drop when we try to downsample these videos in large strides into lower resolution. This proves that keeping the raw resolution patches are important in sampling \frag.
 
\begin{table}
\center
\setlength\tabcolsep{4pt}
\renewcommand\arraystretch{1.15}
\footnotesize
\vspace{-10pt}
\caption{Relative accuracy drop on downsampling videos before FAST-VQA.} 
\label{tab:cres}
\resizebox{\textwidth}{!}{\begin{tabular}{l|cc|cc|cc|cc}
\hline
\textbf{Resolution}  & \multicolumn{2}{|c|}{\textit{Original}}   & \multicolumn{2}{|c|}{540P (2X)}   & \multicolumn{2}{|c|}{360P (3X)}       &  \multicolumn{2}{|c}{270P (4X)}     \\ \cline{2-9}
Results                    & SRCC & PLCC   & SRCC & PLCC  & SRCC & PLCC  & SRCC & PLCC     \\ \hline 
Correlation \textit{with GT Labels} & 0.779 & 0.814 & 0.745 & 0.794 & 0.637 & 0.716 & 0.566 & 0.646 \\
-- \textit{accuracy drop with } & -- & -- & 4.4\% & 2.5\% & 18.3\% & 12.1\% & 27.4\% & 20.7\% \\  \hdashline
Correlation \textit{with Original} & 1.000 & 1.000 & 0.875 & 0.902 & 0.700 & 0.771 & 0.602 & 0.676 \\
\hline
\end{tabular}}
\vspace{-10pt}
\end{table}








\section{Other Technical Details}
\label{sec:c}


\subsection{Metrics \& Loss Functions}


\paragraph{Metrics} We use three metrics, PLCC, SRCC and KRCC, for evaluating the accuracy of quality predictions. The Pearson Linear Correlation Coefficient (\textbf{PLCC}) computes the linear correlation of predicted scores and ground truth scores in a series. The Spearman Rank-order Correlation Coefficient (\textbf{SRCC}) will first rank the labels in both series and computes the PLCC between the two rank series. The Kendall Rank-order Correlation Coefficient (\textbf{KRCC}) computes the rank-pair accuracy and measures how much proportion relative relations of score pairs are predicted correctly.

\paragraph{Loss Functions} We use the differentiable PLCC-induced loss

as the training loss function, where  denotes the mean value for a, and  and  refers to predictions and ground truth labels in a batch. 


\subsection{Batch Size \& Training/Inference Memory Cost}

We show the training and inference memory cost for FAST-VQA and FAST-VQA-M in  Tab.~\ref{tab:trainingcost} and Tab.~\ref{tab:infercost}. The FAST-VQA with batch size  will require around 28GB graphic memory to be trained, which needs one Tesla V100 (32GB) GPU or four GTX1080Ti GPUs. We also provide versions for FAST-VQA with smaller batch sizes, which will require less graphic memory but might need more time for convergence during training. The more efficient variant, FAST-VQA-M with batch size , will only require 3.2GB graphic memory for training, and can be easily reproduced with most current GPU devices. During inference, both variants require affordable memory costs.

\begin{table}[]
    \scriptsize
    \centering
    \setlength\tabcolsep{2pt}
    \renewcommand\arraystretch{1.15}
    \vspace{-20pt}
    \caption{Training memory costs with different batch sizes.}
    \begin{tabular}{l|c|c|c|c} \hline
         Method & Input Size & Attn. Window Size & Batch Size & Training Cost  \\ \hline
         FAST-VQA & (32,224,224)  & (8,7,7) & 16 & 28.0GB \\
         FAST-VQA-B8 & (32,224,224)  & (8,7,7) & 8 & 15.8GB \\
         FAST-VQA-B4 & (32,224,224)  & (8,7,7) & 4 & 9.3GB \\\hdashline
         FAST-VQA-M & (16,128,128) & (4,4,4) &  16 & 3.2GB \\
         FAST-VQA-M-B8 & (16,128,128) & (4,4,4) & 8 & 2.1GB \\ \hline
    \end{tabular}
    \label{tab:trainingcost}
    \centering
    \setlength\tabcolsep{4pt}
    \renewcommand\arraystretch{1.15}
    \vspace{5pt}
    \caption{Inference memory costs of FAST-VQA and FAST-VQA-M.}
    \vspace{-10pt}
    \begin{tabular}{l|c|c|c} \hline
         Method & Input Size & Attn. Window Size & Inference Cost  \\ \hline
         FAST-VQA & (4,32,224,224)  & (8,7,7)  & 6.2GB \\
         FAST-VQA-M & (4,16,128,128) & (4,4,4) & 2.5GB \\ \hline
    \end{tabular}
    \label{tab:infercost}
\end{table}
\section{Additional Results}

\subsection{Result on Different Resolutions}

As the proposed \textit{\red{fragments}} sub-sample videos from diverse resolutions into the same scale, it is important to see the performance of FAST-VQA on different resolutions. We use the LIVE-VQC, the cross-resolution VQA dataset collected from natural photography as the evaluation benchmark. We divide it into three resolution groups (with size in parenthesis): (A) 1080P (110); (B) 720P (316); (C) 540P (159). As the results shown in Tab.~\ref{tab:resolution}, the proposed fragment sampling has superior performance than \textit{bilinear resizing} especially on Group (A) with 1080P videos and \textit{random cropping} on all resolution groups. The FAST-VQA is also notably better than \textit{w/o} GRPB variant on 1080P videos. On other groups, the proposed FAST-VQA is also with best result.

\begin{table}
\center
\setlength\tabcolsep{2.4pt}
\renewcommand\arraystretch{1.15}
\footnotesize
\vspace{-25pt}
\caption{Results on different resolution sub-groups of LIVE-VQC dataset.} 
\begin{tabular}{l|ccc|ccc|ccc}
\hline
\textbf{Resolution}     & \multicolumn{3}{|c|}{A: 1080P}   & \multicolumn{3}{|c|}{B: 720P}        &  \multicolumn{3}{|c}{C: 540P}    \\ \cline{2-10}
Variants                    & SRCC & PLCC & KRCC    & SRCC & PLCC  & KRCC         & SRCC & PLCC                & KRCC     \\ \hline     
\textit{bilinear resizing} & 0.758 & 0.773 & 0.573 & 0.790 & 0.822 & 0.599 & 0.835 & 0.878 & 0.650 \\ \hdashline
\textit{random cropping} & 0.765 & 0.768 & 0.565 & 0.774 & 0.787 &  0.581 & 0.730 & 0.809 & 0.535 \\  \hdashline

\textit{w/o} GRPB & 0.796 & 0.785 & 0.600 & 0.802 & 0.820 & 0.608 & 0.834 & 0.883 & 0.649 \\ \hdashline
\textbf{FAST-VQA} & \red{0.807} & \red{0.806} & \red{0.610} & \red{0.803} & \red{0.825} & \red{0.610} & \red{0.840} & \red{0.885} & \red{0.654} \\ \hline

\end{tabular}
\label{tab:resolution}
\end{table}


\subsection{Result with Patch Annotations}

The LSVQ dataset crops videos into spatial-temporal patches and additionally labels their patch qualities, while \cite{pvq} uses these \blue{Patch Annotations} for training and gets better performance. We also use these additional labels for training. As shown in Tab.~\ref{tab:patches}, the patch labels contribute to performance on cross-dataset scenarios such as KonViD-1k and LIVE-VQC, but reduce the intra-dataset performance on LSVQ. This might be due to the `patching up' adds domain gap between the original videos in test set and cropped video patches in training set, but lead to cross-dataset gain due to data augmentation effect. We report the result in our main paper without these annotations.


\begin{table}[]
\footnotesize
\vspace{-8pt}
\caption{Results with additional patch labels.} \label{tab:patches}
\vspace{-10pt}
\setlength\tabcolsep{3pt}
\renewcommand\arraystretch{1.15}
\footnotesize
\center
\begin{tabular}{l|cc|cc|cc|cc}
\hline
\textbf{Testing Set}/         & \multicolumn{2}{|c|}{\textbf{LSVQ}}   & \multicolumn{2}{|c|}{\textbf{LSVQ}}        &  \multicolumn{2}{|c|}{\textbf{KoNViD-1k}}  & \multicolumn{2}{|c}{\textbf{LIVE-VQC}}             \\ \cline{2-9}
Methods                    & SRCC & PLCC    & SRCC & PLCC           & SRCC & PLCC                      & SRCC & PLCC                               \\ \hline 
PVQ (\textit{w/o} patch)\cite{pvq}   & 0.814 &  0.816  & 0.686 &   0.708         & 0.781 &     0.781                   & 0.747 &  0.776                               \\ 
+ \textit{patch labels}   & 0.827 &  0.828 & 0.711 &  0.739     & 0.791 &     0.795  & 0.770 &  0.807   \\ \hdashline
\gray{FAST-VQA-M (Ours)} &  \gray{0.852} & \gray{0.854} & \gray{0.739} & \gray{0.773} & \gray{0.841} & \gray{0.832} & \gray{0.788} & \gray{0.810} \\
+ \textit{patch labels}   &  \gray{0.846} & \gray{0.849
} & \gray{0.738} & \gray{0.776} & \gray{0.850} & \gray{0.847} & \gray{0.790} & \gray{0.818} \\ \hdashline

\textbf{FAST-VQA(ours)} &  \red{0.876} & \red{0.877} & \red{0.779} & \red{0.814} & {0.859} & {0.855} & {0.823} & \red{0.844}  \\
+ \textit{patch labels}  &  {0.867} & {0.869} & {0.767} & {0.800} & \red{0.862} & \red{0.860} & \red{0.826} & \red{0.844}  \\ \hline
\end{tabular}
\vspace{-9pt}

\end{table}



\subsection{Full Analysis on Fintune Results}

\paragraph{CVD2014} CVD2014\cite{cvd} dataset is the first VQA dataset that focus on in-capture distortions. Tab.~\ref{tab:cvd} shows the performance comparison on CVD2014 dataset, where the FAST-VQA reaches state-of-the-art even without video-quality-related representations. Our model trained on LSVQ (same setting as Tab.~\ref{table:peer}) performs relatively worse, which is probably due to the domain gap between synthetic and in-the-wild VQA datasets. However, despite the difference, the video-quality-related representations from LSVQ training still further improves 1.5\% performance, showing the generalization ability of these feature representations.

\begin{table*}[]
\footnotesize
\renewcommand\arraystretch{1.15}
\setlength\tabcolsep{3pt}
\centering
\vspace{-20pt}
\caption{The mean and (\textit{std. dev.}) of results on CVD2014 dataset with video-quality-related representations, compared with classical \& deep VQA methods.} \label{tab:cvd}
\begin{tabular}{l|ccc} \hline

\textbf{CVD2014} (234) & SRCC(std)               & PLCC(std)                 & KRCC(std)   \\ \hline
V-BLIINDS\cite{vbliinds}        &  0.746(0.056)          & 0.753(0.053)          & 0.562(0.057)      \\ 
TLVQM\cite{tlvqm}           & 0.83(0.04)          & 0.85(0.04)          & NA    \\ \hdashline
VSFA\cite{vsfa}             & 0.870(0.037)          & 0.868(0.032)          & 0.695(0.047)      \\ 
MLSP-FF\cite{mlsp}         & 0.77(0.06)          & NA                   & NA   \\ \hdashline
Full-resolution Swin-T \textit{features}\cite{swin3d} &  0.869(0.027) & 0.878(0.025) & 0.698(0.035)  \\ \hdashline
FAST-VQA \textit{{trained on LSVQ}} & {0.830} & {0.816} & {0.639} \\ \hline
FAST-VQA (Ours, \textit{{from recognition}}) & \blue{0.871(0.032)} & \blue{0.888(0.017)} & \blue{0.699(0.040)} \\ \hline

FAST-VQA (Ours, \textit{\red{from VQA}}) & \red{0.891(0.030)} & \red{0.903(0.019)} & \red{0.721(0.031)} \\ \hline
\end{tabular}
\vspace{-20pt}
\end{table*}

\paragraph{YouTube-UGC} YouTube-UGC dataset is a challenging dataset for FAST-VQA as it contains 10\% \textbf{4K} videos with 20-second duration, where FAST-VQA only sub-samples 0.1\% information on these videos for evaluation. However, the proposed FAST-VQA still reaches state-of-the-art performance, as shown in Tab.~\ref{tab:ugc}. Our model trained on LSVQ also does not perform very well, yet these video-quality-related representations have contributed to more than 10\% performance gain, proving the extraordinary generalization ability of these features.

\begin{table*}[]
\footnotesize
\renewcommand\arraystretch{1.15}
\setlength\tabcolsep{3pt}
\centering
\vspace{-20pt}
\caption{The mean and (\textit{std. dev.}) of results on YouTube-UGC dataset with video-quality-related representations, compared with classical \& deep VQA methods.} \label{tab:ugc}
\begin{tabular}{l|ccc} \hline

\textbf{YouTube-UGC} (1147) & SRCC(std)               & PLCC(std)                 & KRCC(std)   \\ \hline
V-BLIINDS\cite{vbliinds}        &  0.746(0.056)          & 0.753(0.053)          & 0.562(0.057)      \\ 
TLVQM\cite{tlvqm}           & 0.669(0.030)          & 0.659(0.030)          & 0.482(0.025)   \\
RAPIQUE\cite{rapique}   & 0.759  &  0.768  & \\
VIDEVAL\cite{videval}   & 0.779(0.025)          & 0.773(0.025)          & 0.583(0.023)   \\ \hdashline
VSFA\cite{vsfa}         & 0.724          & 0.743          & NA     \\ 
CoINVQ\cite{rfugc}         & \blue{0.816}          & \blue{0.802}          & NA     \\  \hdashline
Full-resolution Swin-T \textit{features}\cite{swin3d} &  0.798(0.027) & 0.796(0.021) & 0.603(0.024)  \\ \hdashline
FAST-VQA \textit{{trained on LSVQ}} & {0.728} & {0.746} & {0.531} \\ \hline
FAST-VQA (Ours, \textit{{from recognition}}) & {0.794(0.016)} & {0.784(0.016)} & {0.596(0.017)} \\ \hline

FAST-VQA (Ours, \textit{\red{from VQA}}) & \red{0.855(0.008)} & \red{0.852(0.011)} & \red{0.667(0.012)} \\ \hline
\end{tabular}
\vspace{-20pt}
\end{table*}


\subsection{Fine-tune Performance for FAST-VQA-M}

\begin{table*}[]
\footnotesize
\renewcommand\arraystretch{1.15}
\setlength\tabcolsep{3pt}
\centering
\vspace{-20pt}
\caption{The results of FAST-VQA-M on several datasets with video-quality-related representations. \textit{VAR} is the abbreviation for video action recognition.} \label{tab:faster}
\begin{tabular}{l|l|ccc} \hline

Dataset Name & \multicolumn{1}{c|}{Method} & SRCC(std)               & PLCC(std)                 & KRCC(std)   \\ \hline

\multirow{4}{}{\textbf{KoNViD}} & FAST-VQA-M (\textit{{from VAR}})    &  0.825(0.014)      & 0.825(0.013)         & 0.634(0.017)    \\
 & FAST-VQA-M (\textit{\red{from VQA}})    &  0.873(0.012)          & 0.872(0.012)          & 0.689(0.015)      \\
 \cdashline{2-5}
 & FAST-VQA (\textit{from VAR})        & {0.842(0.012)} & {0.844(0.011)} & {0.651(0.015)} \\ 
& FAST-VQA (\textit{\red{from VQA}}) & \red{0.891(0.009)} & \red{0.892(0.008)} & \red{0.715(0.011)} \\ \hline
\multirow{4}{}{\textbf{LIVE-VQC}} & FAST-VQA-M (\textit{{from VAR}})    & 0.754(0.034)         & 0.772(0.027)          & 0.563(0.031)    \\
 & FAST-VQA-M (\textit{\red{from VQA}})    &  0.803(0.032)          & 0.828(0.030)          & 0.614(0.033)      \\
 \cdashline{2-5}
 & FAST-VQA (\textit{from VAR})        & {0.765(0.039)} & {0.782(0.034)} & {0.573(0.039)} \\ 
 & FAST-VQA (\textit{\red{from VQA}})   & \red{0.849(0.025)} & \red{0.862(0.019)} & \red{0.664(0.028)} \\  \hline
\multirow{4}{}{\textbf{CVD2014}} & FAST-VQA-M (\textit{{from VAR}})    &  0.857(0.028)         & 0.867(0.019)         & 0.674(0.029)     \\
 & FAST-VQA-M (\textit{\red{from VQA}})    &  0.877(0.035)          & 0.892(0.019)          & 0.705(0.041)      \\
 \cdashline{2-5}
 & FAST-VQA (\textit{from VAR})        & {0.871(0.032)} & {0.888(0.017)} & {0.699(0.040)} \\
 & FAST-VQA (\textit{\red{from VQA}})  & \red{0.891(0.030)} & \red{0.903(0.019)} & \red{0.721(0.031)} \\ \hline
 \multirow{4}{}{\textbf{YouTube-UGC}} & FAST-VQA-M (\textit{{from VAR}})    &  0.645(0.029)         & 0.628(0.034)         & 0.459(0.025)    \\
 & FAST-VQA-M (\textit{\red{from VQA}})    &  0.768(0.020)          & 0.765(0.019)         & 0.572(0.022)      \\
 \cdashline{2-5}
 & FAST-VQA (\textit{from VAR})    &   {0.794(0.016)} & {0.784(0.016)} & {0.596(0.017)} \\
 & FAST-VQA (\textit{\red{from VQA}}) & \red{0.855(0.008)} & \red{0.852(0.011)} & \red{0.667(0.012)} \\ \hline
\end{tabular}
\vspace{-20pt}
\end{table*}

In Tab.~\ref{tab:faster}, we show the results of FAST-VQA-M on small datasets with different initialization, compared with FAST-VQA in same settings. FAST-VQA-M with video-quality-related representations can outperform FAST-VQA (without video-quality-related representations)  on KoNViD-1k, LIVE-VQC and CVD2014 at only 1/6 FLOPs and 1/3 sampling density, proving the effectiveness of these task-specific features. The situation is special on YouTube-UGC with several 4K 20-second videos while FAST-VQA-M has much worse results, suggesting that the effect on sampling density is related to the original video resolution.






\section{Extended Analysis}




\subsection{Resolution Sensitivity}

\paragraph{Relative Accuracy} To show that the proposed \textit{\red{fragments}} are sensitive to resolution changes, we use LSVQ test set for downsample evaluation: We resize them into 540P(2X), 360P(3X), 270P(4X) and transform them into \textit{\red{fragments}}. As the results in Tab.~\ref{tab:cres} shows, the proposed FAST-VQA is sensitive to large-scale downsampling, and suffer around 20\% accuracy drop when we try to downsample these videos in large strides into lower resolution.

\begin{table}
\center
\setlength\tabcolsep{1.2pt}
\renewcommand\arraystretch{1.15}
\footnotesize
\vspace{-10pt}
\caption{Relative accuracy drop on downsampling videos before FAST-VQA.} 
\label{tab:cres}
\begin{tabular}{l|cc|cc|cc|cc}
\hline
\textbf{Resolution}  & \multicolumn{2}{|c|}{\textit{Original}}   & \multicolumn{2}{|c|}{540P (2X)}   & \multicolumn{2}{|c|}{360P (3X)}       &  \multicolumn{2}{|c}{270P (4X)}     \\ \cline{2-9}
Results                    & SRCC & PLCC   & SRCC & PLCC  & SRCC & PLCC  & SRCC & PLCC     \\ \hline 
Correlation \textit{with GT Labels} & 0.779 & 0.814 & 0.745 & 0.794 & 0.637 & 0.716 & 0.566 & 0.646 \\
-- \textit{accuracy drop with } & -- & -- & 4.4\% & 2.5\% & 18.3\% & 12.1\% & 27.4\% & 20.7\% \\  \hdashline
Correlation \textit{with Original} & 1.000 & 1.000 & 0.875 & 0.902 & 0.700 & 0.771 & 0.602 & 0.676 \\
\hline
\end{tabular}
\vspace{-10pt}
\end{table}

\subsection{Effect of Sampling Density}

TBA.







\section{Other Technical Details}



\subsection{Metrics \& Loss Functions}


\paragraph{Metrics} We use three metrics, PLCC, SRCC and KRCC, for evaluating the accuracy of quality predictions. The Pearson Linear Correlation Coefficient (\textbf{PLCC}) computes the linear correlation of predicted scores and ground truth scores in a series. The Spearman Rank-order Correlation Coefficient (\textbf{SRCC}) will first rank the labels in both series and computes the PLCC between the two rank series. The Kendall Rank-order Correlation Coefficient (\textbf{KRCC}) computes the rank-pair accuracy and measures how many relative relations of score pairs are predicted correctly.

\paragraph{Loss Functions} To better help the network to learn predictions with both high linearity and monotonicity to the ground truth scores, we use a combination of normalized linearity loss  as introduced in \cite{qaloss} and monotonicity regularization  which computes margin ranking loss on all  pairs in a batch, as Eq.~\ref{eq:5} shows.

\label{eq:5} We set  in our practice.


\subsection{Batch Size \& Training/Inference Memory Cost}

Both FAST-VQA and FAST-VQA-M has relatively low training cost. As Tab.~\ref{tab:trainingcost} shows, the FAST-VQA with batch size  will require around 28GB graphic memory to be trained, which needs only one Tesla V100 (32GB) GPU or four GTX1080Ti GPUs. We also find out that with  the FAST-VQA still gets similar result, except for slower convergence. This will help reproducing FAST-VQA with limited computational resources. 
The more efficient edition, FAST-VQA-M with batch size , will only require 3.2GB graphic memory for training, and can be easily reproduced with most current GPU. 

\begin{table}[]
    \centering
    \setlength\tabcolsep{2pt}
    \renewcommand\arraystretch{1.15}
    \vspace{-20pt}
    \caption{Training memory costs with different batch sizes.}
    \begin{tabular}{l|c|c|c|c} \hline
         Method & Input Size & Attn. Window Size & Batch Size & Training Cost  \\ \hline
         FAST-VQA & (32,224,224)  & (8,7,7) & 16 & 28.0GB \\
         FAST-VQA-B8 & (32,224,224)  & (8,7,7) & 8 & 15.8GB \\ \hdashline
         FAST-VQA-M & (16,128,128) & (4,4,4) &  16 & 3.2GB \\
         FAST-VQA-M-B8 & (16,128,128) & (4,4,4) & 8 & 2.1GB \\ \hline
    \end{tabular}
    \centering
    \setlength\tabcolsep{4pt}
    \renewcommand\arraystretch{1.15}
    \vspace{5pt}
    \caption{Inference memory costs of FAST-VQA and FAST-VQA-M.}
    \vspace{-10pt}
    \begin{tabular}{l|c|c|c} \hline
         Method & Input Size & Attn. Window Size & Inference Cost  \\ \hline
         FAST-VQA & (4,32,224,224)  & (8,7,7)  & 6.2GB \\
         FAST-VQA-M & (4,16,128,128) & (4,4,4) & 2.5GB \\ \hline
    \end{tabular}
    \label{tab:trainingcost}
\end{table}
\end{comment}
\clearpage
\bibliographystyle{splncs04}
\bibliography{egbib}
\end{document}