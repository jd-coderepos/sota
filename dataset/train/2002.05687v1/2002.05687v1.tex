\documentclass{article}


\usepackage{arxiv}

\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{lipsum}
\usepackage{graphicx}
\usepackage{caption} 
\usepackage{float}

\title{Tree-SNE: Hierarchical Clustering \\ and Visualization Using -SNE}


\author{
  Isaac Robinson \\
  Yale University\\
  New Haven, CT 06511 \\
  \texttt{isaac.robinson@yale.edu} \\
\And
 Emma Pierce-Hoffman \\
  Yale University\\
  New Haven, CT 06511 \\
  \texttt{emma.pierce-hoffman@yale.edu} \\
}

\begin{document}
\maketitle

\begin{abstract}
-SNE and hierarchical clustering are popular methods of exploratory data analysis, particularly in biology. Building on recent advances in speeding up -SNE and obtaining finer-grained structure, we combine the two to create tree-SNE, a hierarchical clustering and visualization algorithm based on stacked one-dimensional -SNE embeddings. We also introduce alpha-clustering, which recommends the optimal cluster assignment, without foreknowledge of the number of clusters, based off of the cluster stability across multiple scales. We demonstrate the effectiveness of tree-SNE and alpha-clustering on images of handwritten digits, mass cytometry (CyTOF) data from blood cells, and single-cell RNA-sequencing (scRNA-seq) data from retinal cells. Furthermore, to demonstrate the validity of the visualization, we use alpha-clustering to obtain unsupervised clustering results competitive with the state of the art on several image data sets. Software is available at \url{https://github.com/isaacrob/treesne}. 
\end{abstract}

\keywords{visualization \and clustering \and hierarchical clustering \and -SNE \and spectral clustering \and embedding}


\section{Introduction}

-SNE (van der Maaten and Hinton 2008) is a widely-used method of visualizing high-dimensional data in low dimensions. It is motivated by minimizing the Kullback-Leibler divergence between the distributions of pairwise affinities among observations in the high-dimensional and low-dimensional spaces. Its predecessor, SNE (Hinton and Roweis 2002), uses a Gaussian kernel to transform the low-dimensional distances into affinities, while -SNE uses a heavier-tailed -distribution with one degree of freedom. As noted by van der Maaten and Hinton (2008), the heavier tails of the -distribution compared to the Gaussian distribution help to alleviate the “crowding problem” so that distinct blobs appear in the low-dimensional embedding. 

Since -SNE gives better-separated clusters than SNE, Kobak et al. (2019) used the Fourier transform (FFT)-accelerated interpolation-based -SNE (FIt-SNE) approximation from Linderman et al. (2019) to implement a fast version of -SNE for smaller, fractional degrees of freedom. They used a scaled -distribution kernel defined as  where the degrees of freedom are . Since the degrees of freedom are positive (),  is also positive (). Kobak et al. found that  (which corresponds to ) produces -SNE plots with tighter, smaller blobs, capable of capturing finer-grained structures than standard -SNE. For instance, standard -SNE with  separates the images of handwritten digits from the MNIST data set by digit, whereas Kobak et al. found that two-dimensional -SNE with  results in further separation into blobs representing different handwriting styles of each digit (see Figure \ref{fig:MNIST-2d-t-SNE-variations}).

In a lecture at Yale University in fall 2019, Stefan Steinerberger argued that the -SNE optimization problem can be interpreted as a dynamic systems problem as opposed to a probability distribution matching problem, thereby justifying using , for which the resulting kernel is not a valid probability distribution (Steinerberger 2019). He argued that this allows for an even more fine-grained look at the high dimensional data. -SNE has also been shown to cluster well-separated data reliably in any embedding dimension (Linderman and Steinerberger 2019). Therefore, the current theory suggests that for well-clustered data, one-dimensional -SNE can be leveraged to convey the same information as two-dimensional -SNE in a more compact manner (Linderman et al. 2019). 

Based on those results, and motivated by the popularity of -SNE and hierarchical clustering individually, we present tree-SNE, a hierarchical clustering method based on one-dimensional -SNE with decreasing values of  and perplexity at each level. Tree-SNE allows for visualization and elucidation of high-dimensional hierarchical structures by creating -SNE embeddings with increasingly heavy tails to reveal increasingly fine-grained structure, and then stacking these embeddings to create a tree-like structure. We then run spectral clustering on each one-dimensional embedding, computationally determining the number of distinct clusters in the embedding. The number of clusters will increase as  decreases. We define the alpha-clustering of the data to be the cluster assignment that is stable across the largest range of  values, and then we demonstrate that alpha-clustering is competitive with the state of the art in unsupervised clustering algorithms on several data sets, including MNIST (LeCun et al. 1998) and COIL-20 (Nene et al. 1996a). Figure \ref{fig:USPS} shows example visualizations produced by tree-SNE on the USPS (Hull 1994) handwritten digits data set. 

\begin{figure}[htp]
    \centering
    \captionsetup{width=.9\linewidth}
    \includegraphics[width=\textwidth]{figures/usps_100_truelabels_NICE.jpg}
    \includegraphics[width=\textwidth]{figures/usps_100_alphaclustlabels_difcolorsthantrue.jpg}
    \caption{Tree-SNE on the USPS handwritten digits data set. Top image: the tree is colored by true digit labels. Bottom image: the tree is colored by alpha-clustering labels. Note that the alpha-clustering labels do not correspond to the digits in each cluster, which accounts for the most of the color differences between the two plots. In all tree-SNE visualizations, the y-axis shows the layer number and the x-axis is the one-dimensional -SNE embedding coordinate. }
    \label{fig:USPS}
\end{figure}

\section{Methods}
\label{sec:headings}

In this section, we provide a description of our algorithm and motivations for various design decisions. Section 2.1 discusses tree-SNE and section 2.2 discusses alpha-clustering.

\subsection{Overview of tree-SNE}
Tree-SNE reveals hierarchical structures in high dimensional data by creating many -SNE embeddings with changing perplexity and degrees of freedom of the -distribution. As both perplexity and degrees of freedom decrease, smaller, more granular clusters are produced in the embedding. In this paper, we restrict ourselves to one-dimensional -SNE embeddings for computational efficiency and because it makes clustering easier. In the future, two-dimensional embeddings could also be used to create 3-D tree structures that are potentially more informative. However, we will demonstrate that these one-dimensional embeddings are sufficiently informative to provide potentially valuable insight into hierarchical structures of data.

\subsubsection{Generating tree-SNE embeddings} \label{tree-sne-methods}

We begin with a standard one-dimensional -SNE () embedding with high perplexity, by default the square root of the number of data points. A high starting perplexity increases the effective number of neighbors used by -SNE (van der Maaten and Hinton 2008), which means that larger clusters will tend to form, capturing more global structures in the data (Kobak and Berens 2019). By using a large perplexity at the beginning, tree-SNE can display the entire spectrum of data organization, from global structures at the base of the tree to very fine-grained structures at the top. The default initial perplexity value of  was inspired by analysis of perplexity in -SNE by Oskolkov (2019) and gives favorable results in practice. The exact starting value of perplexity did not appear to be important across many trials, as long as it is high, as -SNE is fairly robust to small changes in perplexity (van der Maaten and Hinton 2008), and most of the interesting features of the data emerge in further embeddings from the adjustments of perplexity and .

Many one-dimensional -SNE embeddings (typically in the range of 30 to 100) are stacked on top of each other to create the tree-SNE visualization, with the first layer at the bottom of the plot, and  and perplexity decreasing in each subsequent layer. To generate each sequential level, the  used in the previous level is multiplied by a constant factor , which can be expressed as , where  refers to the value of  used in the th embedding layer. With  representing the the perplexity on level , we let . In practice,  is very close to . In this way,  approaches  and perplexity approaches  as the number of levels  gets large, for a constant . Perplexity of  corresponds to entropy of  in the embedding, and  of  means our kernel function  for any points ,  in the low-dimensional embedding. Both of these parameters can be thought of as optimizing for clusters containing only a single data point each, as there is no pressure for points to form clusters under these settings. Accordingly, we observe that clusters become smaller and more numerous moving upward through the layers. 

When creating each layer, the -SNE embedding is initialized with the  embedding from the previous level, rather than a standard random initialization. This is done so that each layer is a refinement of the clustering found on the previous layer, with larger clusters breaking into smaller clusters on subsequent levels of the tree. As a result, the path of a single observation or a group of observations can be traced vertically through the resulting tree visualization.  

While the FIt-SNE paper (Linderman et al. 2019) suggests that high initial exaggeration and learning rates improve the embedding quality, we use smaller learning rates and no early exaggeration after the first embedding to ensure relative stability from layer to layer. With smaller initial exaggeration and learning rates, each embedding does not change drastically from its initialization embedding, which is necessary for an interpretable tree structure to emerge. We do, however, use a constant exaggeration factor (default 12), as was done by Kobak and Berens (2019), to increase cluster separation, based on the finding by Linderman et al. (preprint 2017) that late stage exaggeration increases differentiation of clusters. 

\subsubsection{Differentiating factors of tree-SNE}

Visualizing data at multiple levels of granularity in this way can be useful both to uncover inherently hierarchical structures within the data and to explore new data, because it can be difficult to determine the optimal level of granularity on which to view or cluster unlabeled data. In standard -SNE it is possible to attempt to evaluate the data across multiple scales by using different perplexity and/or degrees of freedom settings. However, the shattering of global structure is a common complaint of -SNE (Kobak and Berens 2019), so it can be difficult to relate a -SNE plot at one scale to another of a different scale, because the global organization of the data is not necessarily preserved. A hierarchical visualization approach such as this is necessary to enable comparison across scales. Visualizing the data with tree-SNE as opposed to -SNE allows a researcher (or our alpha-clustering algorithm) to decide which scale is best for their data by showing them a more in-depth view of the hierarchical organization than is possible with standard -SNE.

\subsection{Description of alpha-clustering}

Alpha-clustering extends and validates the tree-SNE algorithm by assigning cluster labels based on the -SNE embeddings and then automatically selecting the best level of granularity for clustering the data, the result of which can then be compared to other unsupervised clustering algorithms to show that the structure uncovered by tree-SNE indeed captures meaningful aspects of the data. A researcher does not of course have to use alpha-clustering to use tree-SNE. However, since alpha-clustering runs on top of an expressive visualization tool, it provides a powerful opportunity for explainable unsupervised clustering which is difficult to achieve in other approaches, many of which rely on deep neural networks. It is easy to understand why alpha-clustering makes its decisions, and cluster labels can be examined on multiple levels of granularity so a researcher can choose the level that fits their needs, or the alpha-clustering algorithm will recommend an optimal level.

Figure \ref{fig:MNIST-alpha-clustering-all-best} provides further insight into the alpha-clustering process. Cluster labels are assigned on each layer of the tree, and in the top image in Figure \ref{fig:MNIST-alpha-clustering-all-best}, each layer is colored by the cluster labels for that individual layer. Then, the most stable clustering out of all of the layers is defined as the alpha-clustering. In the second image in Figure \ref{fig:MNIST-alpha-clustering-all-best}, all layers of the tree are colored by the alpha-clustering labels.  

\begin{figure}[htp]
    \centering
    \captionsetup{width=.9\linewidth}
    \includegraphics[width=\textwidth]{figures/mnist10k_100_allclusters.jpg}
    \includegraphics[width=\textwidth]{figures/mnist10k_100_bestclusters_viridis.jpg}
    \caption{Tree-SNE with alpha-clustering on the MNIST handwritten digits data set. In the first visualization, each layer of the tree is colored by spectral clustering labels for that layer. In the second visualization, the tree is colored by alpha-clustering labels.}
    \label{fig:MNIST-alpha-clustering-all-best}
\end{figure}

\subsubsection{Outline}

Alpha-clustering works by searching for a cluster assignment for the data that is maximally invariant across multiple levels of clustering granularity. As one-dimensional -SNE tends to generate easily distinguishable clumps of data, we identify and count the clumps on each layer of the tree-SNE tree, then we define the alpha-clustering as the clustering for which the number of clumps does not change across the largest range of  values. 

In other words, define a clustering algorithm  that accepts a layer numbered  and produces a clustering. If we observe that the clustering algorithm  produces the same clustering on all layers between layers  and  (where ) such that , then we call clustering  a stable clustering. Each level's embedding is generated by a unique value of , so each stable clustering has a nonzero  range equal to  where  and  are the first and last layers that produce the stable clustering. We define the alpha-clustering to be the stable clustering for which the  range is the largest. Recall that the relationship between the value of  used to generate embeddings  and  is  for some . This means that  varies the most in the first few layers, and as we add more layers the difference in magnitude between  and  rapidly decreases. Therefore, since alpha-clustering determines clustering stability based on the magnitude of the range of  values that it spans, it will tend to select for clusterings that appear earlier in the tree structure. This helps prevent alpha-clustering from over-fitting to noise in the data, as could happen if we selected a clustering that persists across multiple levels higher up the tree.

\subsubsection{Spectral clustering and determination of }

Alpha-clustering uses spectral clustering on each layer of the tree. To perform spectral clustering, we first generate a graph as described in Section \ref{snn-section}. Then we compute the graph Laplacian, for which the number of zero-valued eigenvalues gives the number of connected components in the graph (von Luxburg 2006). Therefore, as one-dimensional -SNE embeddings tend to give clearly distinct clusters, if we can build a graph that captures that connectivity, then the graph Laplacian will tell us the number of clusters () that are present. This allows us to avoid the need for the user to specify a number of clusters, which can be a drawback of many traditional forms of clustering when the structure of the data is unknown. It is important to note that if we were to implement a version of tree-SNE based on two-dimensional -SNE embeddings, this approach would not be as successful, as it would be more difficult to build a clearly meaningfully disconnected graph on the more complex shapes that can appear in two-dimensional embeddings.

\subsubsection{Shared nearest neighbors} \label{snn-section}

For each one-dimensional embedding, we generate a graph via shared nearest neighbors (SNN) (Ertöz 2002). SNN tends to generate more disconnected graphs than k-Nearest Neighbors (k-NN), because in order for two vertices to be connected via an edge, they need to both be nearest neighbors of each other, which reduces the probability of two vertices on the edges of clusters being randomly connected. In order to reduce the probability that two distinct clusters are connected by an edge in the graph, we want to minimize the number of neighbors used; however, the number of neighbors should not be too small, because that could result in a cluster breaking into multiple disconnected components. To balance these requirements, we choose the number of neighbors to be , where  is the number of observations, and  by default. This choice was inspired by the disconnection criterion of random graphs (Erdős and Rényi 1960) and was seen to perform well in practice. 

\subsubsection{Subsampling}

To improve the efficiency of spectral clustering on large data sets, since it is run on every layer of the tree, we only perform spectral clustering on a random subsample of 2000 observations. Then a k-nearest neighbors classifier is used to cluster the remaining observations. This greatly speeds up the implementation and does not significantly impact the performance of spectral clustering as long as we sample a large enough number of points. Using a k-nearest neighbors classifier also helps to stop individual points that appear separate from clusters from appearing to form their own clusters, by connecting them to the closest large cluster. For intuition as to why we can subsample in this way without ruining performance, see the Nyström method of kernel approximation (Williams and Seeger 2000). The Nyström method could also be used in potential future implementations of alpha-clustering to circumvent the need for a k-nearest neighbors classifier.

\subsubsection{Differentiating factors of alpha-clustering}

An important characteristic of alpha-clustering is that it does not require the user to provide , the number of clusters, and instead determines the optimal level of clustering granularity from the tree-SNE embedding. In addition to the optimal alpha-clustering labels, the clustering labels at every level of the tree-SNE embedding are also returned, so the user can explore patterns on each level of granularity.

Unlike most common clustering algorithms, alpha-clustering is easily interpretable because of the accompanying visualization. The user can see the separation between clusters on each individual level, which provides more insight into the data organization than the black box of a neural network and circumvents the need for an external data visualization method to accompany other clustering algorithms like Louvain (Blondel et al. 2008) or K-Means. Not only that, but the user can see the full depth of organization of the data through the hierarchical visualization, which itself provides more flexibility to color by different features or labels than a typical dendrogram. This enables the user to explore the reasons behind specific splits in the tree and track the progression of points and clusters through the hierarchy. This helps address the increasing need for explainable machine learning methods.

\section{Demonstration on MNIST}

\subsection{Algorithm choices}

To demonstrate the reasoning behind some of our algorithmic decisions, we run variations on tree-SNE with various aspects missing in Figure \ref{fig:MNIST-tree-SNE-variations}. 

\begin{figure}[H]
    \centering
    \captionsetup{width=.9\linewidth}
    \includegraphics[width=\textwidth]{figures/mnist_true_labels_no_dynamic_df.jpg}
    \includegraphics[width=\textwidth]{figures/mnist_true_labels_no_dynamic_perp.jpg}
    \includegraphics[width=\textwidth]{figures/mnist_true_labels_widest.jpg}
    \caption{MNIST tree-SNE (10,000 images). In the first visualization, we do not vary degrees of freedom as we create more layers. In the second visualization, we do not vary perplexity. The last visualization shows the entire tree-SNE algorithm applied to MNIST. The data points are colored by their true labels in all three plots.}
    \label{fig:MNIST-tree-SNE-variations}
\end{figure}

In the top visualization in Figure \ref{fig:MNIST-tree-SNE-variations}, we run tree-SNE without decreasing the degrees of freedom at each level, meaning we only decrease perplexity. Notice how the data is organized and for the most part captures the main structure of MNIST; the visualization is split into different colors showing the grouping of different MNIST digits, and except for fours and nines they are cleanly divided. However, there is no cluster separation, and the visualization does not divide the structure into any substructures. 

The middle visualization in Figure \ref{fig:MNIST-tree-SNE-variations} does not decrease perplexity, and only decreases the degrees of freedom (). The clusters become tighter as  decreases, but they do not split into smaller clusters. We observe in Figure \ref{fig:MNIST-2d-t-SNE-variations} that in two-dimensional -SNE, decreasing  is sufficient to reveal cluster substructure, and decreasing perplexity in tandem is not necessary, as was previously shown in Kobak et al. (2019). However, we see in the middle plot of Figure \ref{fig:MNIST-tree-SNE-variations} that decreasing  alone is not enough to produce finer-grained clusters in one-dimensional -SNE embeddings. 

The final visualization in Figure \ref{fig:MNIST-tree-SNE-variations} is our technique, tree-SNE, which decreases  and perplexity in tandem through the layers. Notice how clusters on earlier levels break into smaller subclusters on later levels, unlike the two other visualizations. We observe that decreasing both  and perplexity appears to be necessary to reveal cluster substructures in one-dimensional -SNE, and tree-SNE therefore takes this approach,  balancing the spreading and contracting forces of decreasing perplexity and the degrees of freedom to build a tree structure containing meaningful hierarchy.

\begin{figure}[htp]
    \centering
    \captionsetup{width=.9\linewidth}
    \includegraphics[width=0.9\textwidth]{figures/tsne2d_grid_alpha_perp.jpg}
    \caption{Two-dimensional t-SNE on 10,000 MNIST digits. The top row has  = 1 and the bottom row has . The left column has perplexity = 30, while the right column has perplexity = 8. The plots are colored by truth labels for the digits.}
    \label{fig:MNIST-2d-t-SNE-variations}
\end{figure}

\subsection{Showing hierarchy} \label{sec:hierarchy}

In order to illustrate how tree-SNE displays hierarchical organization of data, we visualized the MNIST handwritten digits data set, then we examined the averaged digit images for each cluster on 17 of the levels (every fifth level between zero and 80, inclusive). The results are shown in two different formats in Figure \ref{fig:MNIST-hierarchy-example}. 

The top plot in Figure \ref{fig:MNIST-hierarchy-example} shows the tree-SNE visualization of MNIST data that was used for this analysis of hierarchical organization. It was run with a different random seed than the visualization in Figure \ref{fig:MNIST-tree-SNE-variations}, but it is highly similar in appearance. This plot is colored by true digit labels, and we observe that the digits separate cleanly for the most part by level 21, the level chosen by alpha-clustering, with two notable exceptions: four is grouped with nine, and three is grouped with five. In both cases, these digits split farther up the tree. It makes sense that these pairs of numbers remain un-separated longer than other digits, considering their similar appearances. 

The middle plot in Figure \ref{fig:MNIST-hierarchy-example} shows within-cluster average digits for each cluster identified by spectral clustering on each of 17 levels of the tree-SNE visualization from the top image. Each averaged image is plotted in the location corresponding to the middle of the cluster from which it was generated, so that the images can be viewed in the tree-SNE tree structure. We observe that the clusters are refined moving up the tree, as  and perplexity decrease. Clusters representing multiple digits separate by digit, and clusters representing a single digit split into smaller clusters representing different handwriting variations for that digit. For instance, ones split into vertical and tilted versions around level 35, and twos split into bubbly and straight sub-groups at level 55. There is a clear hierarchy of digit types, with distinctions becoming finer-grained at the top of the plot. From this visualization, we see that tree-SNE's branching structure conveys meaningful information about the hierarchical organization of the data.

Some overlap between images was unavoidable while preserving the tree structure, so the bottom visualization in the same figure (Figure \ref{fig:MNIST-hierarchy-example}) is provided for larger views of the within-cluster average digits. Within-cluster average digits are shown for ten of the 17 layers shown in the middle plot, stacked vertically in the same order as they appear in the tree-SNE visualization. The second row from the bottom represents the stable clustering identified by alpha-clustering, in which only fours and nines and threes and fives have yet to separate cleanly. We draw attention to the top row, where we observe that the rightmost cluster of fours that is last to split from nines is triangular in shape, so in fact is more similar in appearance to nines than to the other fours. Based on these visualizations of within-cluster averaged digits, we see that tree-SNE produces meaningful clusters on each embedding, with hierarchical organization from layer to layer.

\begin{figure}[H]
    \centering
    \captionsetup{width=.9\linewidth}
    \includegraphics[width=\textwidth]{figures/mnist10k_100_truelabels.jpg}
\includegraphics[width=\textwidth]{figures/mnist10k_avgimages_tree_attempt1_noframe.jpg}
    \includegraphics[width=0.9\textwidth]{figures/mnist10k_avgimages_alphaclust_stacked_tighter.jpg}
\caption{MNIST (10,000 samples) tree-SNE with averaged images of digits within each cluster. The top plot is a tree-SNE visualization of MNIST colored by the true digit labels. The middle plot shows the within-cluster average digits superimposed on the tree structure. The bottom visualization shows within-cluster average digits for a sample of layers. Clusters determined via alpha-clustering on each layer.}
    \label{fig:MNIST-hierarchy-example}
\end{figure}

\subsection{Alpha-clustering}

We apply alpha-clustering to MNIST. Recall that alpha-clustering determines the level with the clustering that best fits the overall structure of the data. Figure \ref{fig:MNIST-clusters} shows the tree-SNE embedding of MNIST from Figure \ref{fig:MNIST-tree-SNE-variations} in the top image, and the same embedding with data points on all layers colored by alpha-clustering labels in the bottom plot. As discussed in Section \ref{sec:hierarchy}, alpha-clustering correctly groups six of the ten MNIST classes, and combines the remaining four into two different clusters of two each. It combines four and nine, and three and five. Looking closer at the tree structure in the top plot of Figure \ref{fig:MNIST-clusters}, we see that the split that separates three from five happens at roughly the same level that one splits into two different sub-classes. Since alpha-clustering selects the optimal cluster assignment from a single layer, choosing a clustering level that separates three and five would mean also subdividing the ones into multiple clusters. 

In order to determine the quality of a clustering assignment, we consider the normalized mutual information (NMI) (Strehl and Ghosh 2002) of a clustering. The NMI is defined as  where  and  are two different cluster assignments for the data,  is the mutual information, and  is the entropy. Values of NMI fall between  and , with  representing a complete match between  and . We can assess the quality of alpha-clustering by computing its NMI relative to the ground truth labels:  = alpha-clustering labels and  = true labels. On MNIST (10,000 samples), alpha-clustering has an NMI score of 0.84. Importantly, there are is no clustering assignment on any individual layer with a higher NMI. This shows that alpha-clustering is selecting the best possible clustering from all levels of the tree. 

\begin{figure}[htp]
    \centering
    \captionsetup{width=.9\linewidth}
    \includegraphics[width=\textwidth]{figures/mnist_true_labels_widest.jpg}
    \includegraphics[width=\textwidth]{figures/mnist_cluster_labels.jpg}
    \caption{MNIST tree-SNE embeddings. The top image is colored by true digit labels, and the bottom image is colored by alpha-clustering labels. As with Figure \ref{fig:USPS}, note that the alpha-clustering labels do not correspond to the digits in each cluster, which accounts for the most of the color differences between the two plots.}
    \label{fig:MNIST-clusters}
\end{figure}

\section{Experiments}

We demonstrate tree-SNE on several popular clustering data sets and two biological data sets. The purpose of running tree-SNE on the clustering data sets is to show that it can generate a competitive and interpretable clustering, which means that one-dimensional -SNE embeddings created in this way are indeed able to express meaningful organization of high-dimensional data. Applying tree-SNE to biological data sets allows us to demonstrate its performance on diverse types of data and compare it to standard visualization techniques in biology. 

\subsection{Experimental Setup}

Tree-SNE has three major parameters: the ratio , the initial perplexity, and the number of layers. For the following experiments, we set the starting perplexity to be the square root of the number of observations in the data set (), as described in Section \ref{tree-sne-methods}. Tree-SNE embeddings tend to be exhibit highly  branched structures corresponding to very local structures in the data for , so by default we set the lower bound on  to 0.01. Given this fixed lower bound on , we can solve for the ratio  required to decrease  from 1 to 0.01 in a given number of layers : 
Using these sensible defaults,  and perplexity are automatically determined for a given data set and a desired number of layers for the tree. For testing alpha-clustering without visualization, we use 30 layers, and for visualization, we use 100 layers. Using fewer layers has minimal effect on the alpha-clustering and is more computationally efficient for repeated experimentation, while a larger number of layers is ideal for visualization because it produces a higher-resolution plot. For example, for 30 layers,  is , and for 100 layers,  is . Alpha-clustering has one major parameter, , where  is the number of nearest neighbors used to generated the shared nearest neighbors graph for spectral clustering. We report our results with the default of , unless otherwise noted.

\subsection{Results of alpha-clustering}

We apply alpha-clustering to four standard clustering data sets and compare the NMI to that of several state of the art clustering methods. The results are shown in Table \ref{tab:alpha-clustering}, which is based on Table 4 from Gultepe and Makrehchi (2008). Alpha-clustering achieves a competitive NMI on all of the data sets.

\begin{table}[H]
 \caption{Performance of different clustering algorithms}
  \centering
  \begin{tabular}{lllll}
    \toprule
    Method     & COIL-20     & COIL-100     & USPS & MNIST \\
    \midrule
    \multicolumn{1}{c}{Deep Learning} \\
    \cmidrule(r){1-1}
    JULE* (2016) & \bf 1.000  & \bf 0.985    & \bf 0.913 & 0.913     \\
    DBC* (2017)     & 0.895  & 0.905 & 0.724 & \bf 0.917  \\
    IEC* (2016)    & - & 0.787 & 0.641 & 0.542      \\
    DEPICT* (2017)    & - & - & \bf 0.927 & \bf 0.917     \\
    AEC* (2013)  & - & - & 0.651 & 0.669 \\
    NMF-D* (2014) & 0.692 & 0.719 & 0.287 & 0.152 \\
    TSC-D* (2016) & 0.928 & - & - & 0.651 \\
    DCEC* (2017) & - & - & 0.826 & 0.885 \\
    SpectralNet* (2018) & - & - & - & \bf 0.924 \\
    \cmidrule(r){1-1}
    \multicolumn{1}{c}{Non-deep learning} \\
    \cmidrule(r){1-1}
    ICA BSS* (2018)     & \bf 0.965 & \bf 0.962 & 0.868 & 0.824      \\
    AC-PIC* (2013)     & 0.855 & 0.840 & 0.840 & 0.017      \\
    K-means* & 0.774 & 0.775 & 0.613 & 0.490 \\ DBSCAN & 0.892 & 0.705 & 0.299 & - \\
    \cmidrule(r){1-1}
    Ours        & \bf 0.958 & \bf 0.926** & \bf 0.885 & 0.864 \\ 
    \bottomrule
  \end{tabular}
  \captionsetup{width=.9\linewidth}
  \caption*{Table 1. The NMI scores of various clustering algorithms on different data sets, with top three in bold. For DBSCAN,  was tuned until the correct number of clusters was obtained. We performed DBSCAN, K-Means, and tree-SNE benchmarking; the rest of the values are reported from the original papers. * means the clustering algorithm requires knowing the number of clusters beforehand. ** this was run with , whereas the rest were run with the default . The methods in the table are: Joint Unsupervised Learning (JULE, Yang et al. 2016), Discriminatively Boosted Clustering (DBC, Li et al. 2018), Infinite Ensemble Clustering (IEC, Liu et al. 2016), Autoencoder-based Clustering (AEC, Song et al. 2013), NMF with Deep learning model (NMF-D, Trigeorgis et al. 2014), Task-specific Deep Architecture for Clustering (TSC-D, Wang et al. 2016), Deep Convolutional Embedded Clustering (DCEC, Guo et al. 2017), SpectralNet (Shaham et al. 2018), Independent Component Analysis Blind Source Separation (ICA BSS, Gultepe and Makrehchi 2018), Agglomerative Clustering via Path Integral (AC-PIC, Zhang et al. 2013), K-Means, Density-Based Spatial Clustering of Applications with Noise (DBSCAN, Ester et al. 1996). Benchmarking data sets: COIL-20 (Nene et al. 1996a), COIL-100 (Nene et al. 1996b), USPS (Hull 1994), MNIST 60,000 samples (LeCun et al. 1998).}
  \label{tab:alpha-clustering}
\end{table}

Note that while most of the other methods featured in Table \ref{tab:alpha-clustering} require specifying a number of clusters, alpha-clustering determines the best number of clusters from the data.
In biomedical applications, the optimal number of clusters is often not known {\em a priori}, so algorithms such as tree-SNE that do not require the number of clusters as input may be particularly useful.

If the number of clusters is known beforehand, the  parameter can be tuned as needed such that alpha-clustering yields closer to the correct number of clusters. This is particularly useful when the data has a large number of clusters, as some clusters may be artificially merged in the SNN graph, and a lower  value will decrease the number of neighbors used to construct the graph and reduce this merging behavior. We find that decreasing  from our default of  to  increases the performance of of alpha-clustering on COIL-100 from  to  and increases the number of clusters found from 76 to 101. Note that changing  to give the correct number of clusters does not always increase the NMI, as the clusters that split apart with lower  do not necessarily correlate with true label clusters. For example, tuning  on COIL-20 to give the correct number of clusters results in a decreased NMI of 0.951.

It should be noted that all of the benchmarking data sets used are image data sets: the two Columbia Object Image Library data sets, COIL-20 (Nene et al. 1996a) and COIL-100 (Nene et al. 1996b); as well as two handwritten digit data sets, USPS (Hull 1994) and MNIST (LeCun et al. 1998). These data sets were chosen due to readily available comparison metrics from other methods. However, convolutional neural network-based approaches have an advantage on these data sets because they are designed to process image data.

\subsection{Results of tree-SNE}

We demonstrate running tree-SNE on several different data sets, including mass cytometry (CyTOF) data from bone marrow cells (Bendall, et al. 2011) and single-cell RNA-sequencing (scRNA-seq) data from retinal cells (Shekhar et al. 2016). We compare tree-SNE with -SNE (van der Maaten and Hinton 2008) and PHATE (Moon et al. 2019), standard approaches in biomedical data visualization.

\subsubsection{CyTOF}

\begin{figure}[htp]
    \centering
    \captionsetup{width=.9\linewidth}
    \includegraphics[width=\textwidth]{figures/treesne_cytof_cd8.jpg}
    \includegraphics[width=\textwidth]{figures/treesne_cytof_cd45ra.jpg}
    \caption{CyTOF tree-SNE embeddings. The top embedding is labeled by the level of CD8 in the sample, and the bottom is labeled by CD45ra expression.}
    \label{fig:CyTOF-tree-SNE}
\end{figure}

Figure \ref{fig:CyTOF-tree-SNE} shows a tree-SNE embedding of bone marrow single-cell mass cytometry (CyTOF) data (Bendall et al. 2011), colored by the expression levels of specific markers in each cell. The top visualization is colored by intensity of CD8, which is a marker for cytotoxic T cells, and the bottom is colored by intensity of CD45ra, which is a marker for naive T cells (Golubovskaya and Wu 2016). Notice how in the top visualization most of the cytotoxic T cells are in the same bright arm, which subdivides at higher embedding levels. This indicates that tree-SNE separates cytotoxic T cells on lower levels of the embedding, and then splits them into subgroups higher up as  decreases. Importantly, the CD8 branch splits in two about halfway up the plot. The CD45ra visualization (bottom of Figure \ref{fig:CyTOF-tree-SNE}) reveals that the split in the cytotoxic T cell line is mediated by the presence of CD45ra: CD45ra is present in one of the branches but not the other. This indicates that tree-SNE is splitting cytotoxic T cells () into naive () and mature cells (). In this way, tree-SNE very clearly reveals the hierarchical organization of the CyTOF data and facilitates identification of the features mediating the branching. 

For comparison with established visualization methods for biological data, we show the same data embedded via -SNE (van der Maaten and Hinton 2008) in Figure \ref{fig:CyTOF--SNE} and via PHATE (Moon et al. 2019) in Figure \ref{fig:CyTOF-PHATE}. 

\begin{figure}[htp]
    \centering
    \captionsetup{width=.9\linewidth}
    \begin{minipage}[b]{0.4\textwidth}
    \includegraphics[width=200pt]{figures/tsne_cd8.jpg}
    \end{minipage}
    \begin{minipage}[b]{0.4\textwidth}
    \includegraphics[width=200pt]{figures/tsne_cd45ra.jpg}
    \end{minipage}
    \caption{CyTOF -SNE embeddings. The left is colored by CD8, and the right is colored by CD45ra.}
    \label{fig:CyTOF--SNE}
\end{figure}

\begin{figure}[htp]
    \centering
    \captionsetup{width=.9\linewidth}
    \begin{minipage}[b]{0.4\textwidth}
    \includegraphics[width=200pt]{figures/phate_cd8.jpg}
    \end{minipage}
    \begin{minipage}[b]{0.4\textwidth}
    \includegraphics[width=200pt]{figures/phate_cd45ra.jpg}
    \end{minipage}
    \caption{CyTOF PHATE embeddings. The left is colored by CD8, and the right is colored by CD45ra.}
    \label{fig:CyTOF-PHATE}
\end{figure}

\subsubsection{scRNA-seq} \label{sec:shekhar}

We demonstrate tree-SNE on single-cell RNA-sequencing (scRNA-seq) data from mouse retinal cells and compare the results with the findings from the original paper by Shekhar et al. (2016). Shekhar et al. used Louvain community detection (Blondel et al. 2008) to detect clusters, two-dimensional -SNE to visualize the clusters, and hierarchical clustering with Euclidean distances and average linkage to produce a hierarchical depiction of the relationships between different mouse retinal bipolar cells (BCs). We run tree-SNE on the same data and label it according to the labels assigned by Shekhar et al. from clustering and morphological analysis. Some pre-processing, mediated by alpha-clustering, was performed to roughly isolate the cone bipolar cells, which encompass the cell subtypes evaluated by hierarchical clustering by Shekhar et al. The full process is described in Appendix A (Section \ref{sec:shekhar-appendix}). Figure \ref{fig:scRNA-seq-tree-SNE} shows the tree-SNE embedding of the data subset. 

Although the tree-SNE hierarchy and the dendrogram created by Shekhar et al. do not match entirely, there are strong similarities between the two. Notice how BC1A and BC1B split apart at a high level of the tree, and how they are more closely related to BC2, BC3A, BC3B, and BC4 than to the BC5 subtypes or BC7. Observe also how tree-SNE show the same organization of the four subtypes of BC5 as Shekhar et al. discovered: BC5A and BC5D are closely related, and BC5B and BC5C are closely related. Tree-SNE also shows further branching of some of the cell types, and these subdivisions may warrant further examination.

\begin{figure}[htp]
    \centering
    \captionsetup{width=.9\linewidth}
    \includegraphics[width=\textwidth]{figures/shekar_tree_sne_37_pc_true_labels.jpg}
    \caption{scRNA-seq tree-SNE embedding colored by the labels from Shekhar et al. (2016).}
    \label{fig:scRNA-seq-tree-SNE}
\end{figure}

\subsubsection{Swiss roll}

A common concern about -SNE is that it does not fully capture some standard artificial data sets, including the famous Swiss roll (Linderman and Steinerberger 2019). In the case of the Swiss roll, -SNE does not fully unwind the manifold, as seen in Figure \ref{fig:swiss-roll-t-SNE}. However, tree-SNE does for the most part unwind the Swiss roll, as seen in the top plot of Figure \ref{fig:swiss-roll-tree-SNE}. Notice how on the bottom two levels of the embedding, the order of the points is wrong in the same way that it is wrong in the -SNE embedding, with purple in the middle as opposed to at the end. However, tree-SNE's multi-scale approach fixes this issue and places the purple segment where it should be, correctly capturing the manifold. This phenomenon is highlighted in the bottom plot of Figure \ref{fig:swiss-roll-tree-SNE}, which shows only the first four levels of the tree-SNE plot.

Additionally, notice that there is very little cluster separation in the tree-SNE embedding of the Swiss roll in Figure \ref{fig:swiss-roll-tree-SNE}, and no well-defined branching. This shows that tree-SNE does not artificially introduce clusters or hierarchical structures when applied to data lacking this type of organization. We illustrate this point further in the next section (Section \ref {sec:noise}). This suggests that the clusters that emerge in other datasets are unlikely to be artifacts of noise, as otherwise similar clusters would have emerged here.

Although we demonstrate tree-SNE on the Swiss roll data set because it is a common "toy" data set and the behavior of tree-SNE when applied to the Swiss roll is interesting, tree-SNE is by no means a dimensionality reduction method. It is designed to visualize hierarchical cluster structures and is limited in its ability to represent manifolds.  

\begin{figure}[htp]
    \centering
    \captionsetup{width=.9\linewidth}
    \begin{minipage}[b]{0.4\textwidth}
    \includegraphics[width=200pt]{figures/swiss_roll.jpg}
    \end{minipage}
    \begin{minipage}[b]{0.4\textwidth}
    \includegraphics[width=200pt]{figures/t-sne_swiss_roll.jpg}
    \end{minipage}
    \caption{Swiss roll data set in full 3-D (left) and its -SNE embedding (right), both colored by labels corresponding to the position of points along the major axis of the manifold.}
    \label{fig:swiss-roll-t-SNE}
\end{figure}

\begin{figure}[H]
    \centering
    \captionsetup{width=.9\linewidth}
    \includegraphics[width=500pt]{figures/tree_sne_swiss_roll.jpg}
    \includegraphics[width=200pt]{figures/tree_sne_swiss_roll_bottom_4.jpg}
    \caption{Swiss roll tree-SNE embedding with 100 layers (top) and just the bottom four layers (bottom), again colored by labels corresponding to the position of points along the major axis of the manifold.}
    \label{fig:swiss-roll-tree-SNE}
\end{figure}

\subsection{Random noise} \label{sec:noise}

To demonstrate that tree-SNE does not artificially introduce clusters or hierarchical structure into data lacking those structures, we run tree-SNE on 5,000 random samples drawn from a 100-dimensional uniform distribution over the range [0,1). The results are displayed in Figure \ref{fig:noise-tree-SNE}. Observe how, unlike the tree-SNE visualizations on meaningfully organized data, the tree does not begin to branch until level 50, and the subdivisions thereafter are long, without further branching. This shows that tree-SNE does not tend to produce clear cluster separation or hierarchical structure by mere artifact. Further note that there are many points seemingly randomly scattered in the embedding, which does not tend to appear in well-organized data.

\begin{figure}[htp]
    \centering
    \captionsetup{width=.9\linewidth}
    \includegraphics[width=500pt]{figures/uniform_noise_tree_sne.jpg}
    \caption{Uniform noise tree-SNE embedding.}
    \label{fig:noise-tree-SNE}
\end{figure}

In Figure \ref{fig:plane-noise}, we generate 5,000 samples of uniform noise in a two-dimensional plane in 100-dimensional space, then embed the samples with both tree-SNE and -SNE. The tree-SNE plot does not have as long of an initial stem as it does in Figure \ref{fig:noise-tree-SNE}, because the data lies along a plane rather than completely randomly throughout the 100 dimensions. The tree-SNE plot still has relatively uniformly distributed branches without a clear hierarchy, which does not tend to happen with organized data. The standard two-dimensional -SNE embedding is provided for comparison. The -SNE embedding gives the impression of clumping or clustering, despite the fact that the data is completely random, whereas tree-SNE does not show any clusters. 

Together, these experiments on random noise illustrate that tree-SNE does not introduce artificial structure into unstructured data, and the tree-SNE plots of noise have a characteristic profile that is distinguishable from the plots of well-organized data. 

\begin{figure}[htp]
    \centering
    \captionsetup{width=.9\linewidth}
    \includegraphics[width=\textwidth]{figures/plane_noise_tree_sne.jpg}
    \includegraphics[width=200pt]{figures/plane_noise_t_sne.jpg}
    \caption{Random samples from a plane in 100-dimensional space visualized by tree-SNE (top) and -SNE (bottom).}
    \label{fig:plane-noise}
\end{figure}



\section{Conclusion}

We present tree-SNE, a novel hierarchical visualization method for high-dimensional data, and a companion clustering method, alpha-clustering. Tree-SNE provides a way to explore the organization of data at multiple levels of granularity, which is not available in many other visualization techniques. Alpha-clustering is built on top of the tree-SNE embedding and provides quantitative validation that tree-SNE reveals meaningful structures in the data. It also achieves NMI scores on various data sets that are competitive with state-of-the-art clustering approaches, without ever needing to specify the number of clusters. Finally, we demonstrate the viability of this approach on several diverse real-world data sets. Our analysis suggests that tree-SNE offers promise in terms of yielding insights not currently revealed by other visualization approaches. 

In the future, we intend to modify the tree-SNE algorithm to be able to leverage neural network embeddings, with the hope that this would help us be more competitive with neural network-based approaches, particularly on image or sequential data. We hope to create a means for out-of-sample extension of alpha-clustering such that the clustering approach can be used for more than exploratory data analysis, perhaps leveraging previous work on parametric -SNE (van der Maaten 2009). We also want to explore the idea of a variation of alpha-clustering that varies scale () differently for different branches, acknowledging that a global scale may not be optimal for all data sets. In addition, we want to investigate using alternative clustering approaches instead of subsampled SNN-based spectral clustering, perhaps leveraging the Nyström method. And finally, we want to extend tree-SNE to support stacked two-dimensional -SNE plots.

\section{Acknowledgements}

The authors thank Stefan Steinerberger for inspiration, support, and advice; George Linderman for enabling one-dimensional -SNE with degrees of freedom < 1 in the FIt-SNE package; Scott Gigante for data pre-processing and helpful discussions of visualizations and alpha-clustering; Smita Krishnaswamy for encouragement and feedback; and Ariel Jaffe for discussing the Nyström method and its relationship to subsampled spectral clustering.

\section{Appendix}

\subsection{Appendix A: Methods for subsetting scRNA-seq data with alpha-clustering} \label{sec:shekhar-appendix}

In Section \ref{sec:shekhar}, we applied tree-SNE to a subset of a scRNA-seq data set from retinal cells (Shekhar et al. 2016). Now, we demonstrate using alpha-clustering to partition the full data set from Shekhar et al. to obtain the subset of interest used in Section \ref{sec:shekhar}. Prior to running tree-SNE, we use PCA to reduce the dimensions of the data set from 24,576 features to 100 dimensions to increase the efficiency of the algorithm. We start with the tree-SNE embedding of the full scRNA-seq data set, in Figure \ref{fig:RNA-full-tree-SNE}, colored by the labels from Shekhar et al. Notice that the Muller Glia (teal, leftmost branch) and Rod BC (dark green, rightmost branch) cell types take up a lot of space in the visualization, crowding the main group of cone bipolar cell (BC) subtypes of interest. We would like to remove the Müller glia and rod BCs to reduce this crowding, and we will use alpha-clustering labels to partition the data to do so. 

\begin{figure}[htp]
    \centering
    \captionsetup{width=.9\linewidth}
    \includegraphics[width=\textwidth]{figures/shekar_tree_sne_true_labels.jpg}
    \caption{Tree-SNE embedding of the whole scRNA-seq data set, colored by labels from Shekhar et al (2016).}
    \label{fig:RNA-full-tree-SNE}
\end{figure}

Figure \ref{fig:RNA-clusters-tree-SNE} shows the same tree-SNE embedding of the full scRNA-seq data set, this time colored by alpha-clustering labels. Alpha-clustering automatically determined that the optimal number of clusters was three, and these three clusters correspond to (left to right) Müller glia, cone BC subtypes and photoreceptor (PR) cells, and rod BCs. 

\begin{figure}[htp]
    \centering
    \captionsetup{width=.9\linewidth}
    \includegraphics[width=\textwidth]{figures/shekar_tree_sne_clusters.jpg}
    \caption{Tree-SNE embedding of the whole data set of scRNA-seq data, colored by alpha-clustering labels.}
    \label{fig:RNA-clusters-tree-SNE}
\end{figure}

Since alpha-clustering has already partitioned the data, it is simple to remove clusters 0 and 2 and re-apply tree-SNE to only the observations within cluster 1, allowing for a visualization that has more space to preserve the hierarchical organization of the cells of interest. This new embedding is shown in Figure \ref{fig:RNA-subset-tree-SNE}. Notice that on the far right, there is still a small group of rod PR cells present in this plot, which are not a cell type of interest. 

\begin{figure}[htp]
    \centering
    \captionsetup{width=.9\linewidth}
    \includegraphics[width=\textwidth]{figures/shekhar_subset_tree_sne_true_labels.jpg}
    \caption{Tree-SNE embedding of a subset of the scRNA-seq data, excluding cells that are not of interest.}
    \label{fig:RNA-subset-tree-SNE}
\end{figure}

Running alpha-clustering on the tree-SNE embedding from Figure \ref{fig:RNA-subset-tree-SNE} yields an optimal cluster assignment of just two clusters; again, the number of clusters was automatically detected by the algorithm. Figure \ref{fig:RNA-subset-clusters-tree-SNE} is colored by the alpha-clustering labels, showing that the two clusters discovered from this subset of the data correspond to the majority of the tree (left) and the rod PR cells (right). 

\begin{figure}[htp]
    \centering
    \captionsetup{width=.9\linewidth}
    \includegraphics[width=\textwidth]{figures/shekhar_subset_tree_sne_clusters.jpg}
    \caption{Tree-SNE embedding of a subset of the scRNA-seq data, colored by alpha-clustering labels.}
    \label{fig:RNA-subset-clusters-tree-SNE}
\end{figure}

The alpha-clustering labels make it trivial to remove cluster 1 and re-embed the remaining data from cluster 0. For this final round of tree-SNE, we also reduce the number of PCA components used to 37 to more closely match the methods from Shekhar et al. (2016) and enable more direct comparison. The final tree-SNE embedding is shown in Figure \ref{fig:RNA-subsubset-tree-SNE} (which is the same as Figure \ref{fig:scRNA-seq-tree-SNE} shown previously), and is discussed in more detail in Section \ref{sec:shekhar}.

\begin{figure}[H]
    \centering
    \captionsetup{width=.9\linewidth}
    \includegraphics[width=\textwidth]{figures/shekar_tree_sne_37_pc_true_labels.jpg}
    \caption{Tree-SNE embedding of only the cone BC subtypes in the scRNA-seq data (with a few rogue cone photoreceptor cells on the right side), colored by labels from Shekhar et al. (2016).}
    \label{fig:RNA-subsubset-tree-SNE}
\end{figure}

The ability to automatically detect, partition, and remove particular significantly different subsets of the data is, to the best of our knowledge, unique to tree-SNE and alpha-clustering. This allows a researcher to very easily select and hone in on subsets of interest in their data.

\bibliographystyle{unsrt}  



\begin{thebibliography}{1}

\bibitem{bendall2011single}
Bendall, S.C. et al.
\newblock Single-cell mass cytometry of differential immune and drug responses across a human hematopoietic continuum.
\newblock {\em Science} \textbf{332}, 687–696 (2011). doi:10.1126/science.1198704.

\bibitem{blondel2008louvain}
Blondel, V.D., Guillaume, J-L., Lambiotte, R. \& Lefebvre, E. 
\newblock Fast unfolding of communities in large networks.
\newblock {\em Journal of Statistical Mechanics: Theory and Experiment} \textbf{2008}(10), 10008 (2008). doi:10.1088/1742-5468/2008/10/P10008.

\bibitem{dizaji2017depict}
Dizaji, K. G. et al.
\newblock Deep Clustering via Joint Convolutional Autoencoder Embedding and Relative Entropy Minimization.
\newblock {\em arXiv preprint} arXiv:1704.06327 (2017).

\bibitem{erdos1960random}
Erdös, P. \& Rényi, A.
\newblock On the evolution of random graphs.
\newblock {\em Magayr Tud. Akad. Mat. Kutato Int. Kozl.} \textbf{5}, 17-61 (1960).

\bibitem{ertoz2002snn}
Ertöz, L., Steinbach, M. \& Kumar, V.
\newblock A new shared nearest neighbor clustering algorithm and its applications.
\newblock In {\em Workshop on Clustering High Dimensional Data and its Applications. Second SIAM International Conference on Data Mining, Arlington, VA, USA} (2002). 

\bibitem{ester1996dbscan}
Ester, M. et al.
\newblock A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise.
\newblock In {\em Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining, Portland, OR}, in print. {\em AAAI Press} 226–231 (1996).

\bibitem{golub2016tcells}
Golubovskaya, V. \& Wu, L.
\newblock Different Subsets of T Cells, Memory, Effector Functions, and CAR-T Immunotherapy.
\newblock {\em Cancers} \textbf{8}(3), 36 (2016). doi:10.3390/cancers8030036.

\bibitem{gultepe2018improving}
Gultepe, E. and Makrehchi, M.
\newblock Improving clustering performance using independent component analysis and unsupervised feature learning.
\newblock {\em Hum. Cent. Comput. Inf. Sci.} \textbf{8}, 25 (2018). doi:10.1186/s13673-018-0148-3.

\bibitem{guo2017dcec}
Guo, X., Liu, X., Zhu, E. \& Yin, J.
\newblock Deep clustering with convolutional autoencoders.
\newblock In {\em International Conference on Neural Information Processing}, in print. {\em Springer} 373–382 (2017).

\bibitem{hinton2002sne}
Hinton, G. \& Roweis, S.
\newblock Stochastic neighbor embedding.
\newblock In {\em Proceedings of the 15th International Conference on Neural Information Processing Systems (NIPS’02)}, in print. {\em MIT Press} 857–864 (2002).

\bibitem{hull1994usps}
Hull, J.
\newblock A database for handwritten text recognition research.
\newblock In {\em IEEE Transactions on Pattern Analysis and Machine Intelligence} \textbf{16}(5), 550-554 (1994). doi:10.1109/34.291440.

\bibitem{kobak2019art}
Kobak, D. \& Berens, P.
\newblock The art of using t-SNE for single-cell transcriptomics.
\newblock {\em Nature Communications} \textbf{10}, 5416 (2019). doi:10.1038/s41467-019-13056-x.

\bibitem{kobak2019heavy}
Kobak, D., Linderman, G., Steinerberger, S., Kluger, Y. \& Berens, P.
\newblock Heavy-tailed kernels reveal a finer cluster structure in -SNE visualisations.
\newblock In {\em Proceedings of the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases}, in print. arXiv:1902.05804 (2019).

\bibitem{lecun1998mnist}
LeCun, Y., Bottou, L., Bengio, Y., \& Haffner, P.
\newblock Gradient-based learning applied to document recognition.
\newblock In {\em Proceedings of the IEEE} \textbf{86}(11), 2278–2324 (1998). doi:10.1109/5.726791.

\bibitem{li2018dbc}
Li, F., Qiao, H. \& Zhang, B. 
\newblock Discriminatively boosted image clustering with fully convolutional auto-encoders.
\newblock {\em Pattern Recognition} \textbf{83}, 161-173 (2018). doi:10.1016/j.patcog.2018.05.019.

\bibitem{linderman2017efficient}
Linderman, G. C., Rachh, M., Hoskins, J. G., Steinerberger, S. \& Kluger, Y.
\newblock Efficient algorithms for t-distributed stochastic neighborhood embedding.
\newblock {\em arXiv preprint} arXiv:1712.09005 (2017).

\bibitem{linderman2019fast}
Linderman, G. C., Rachh, M., Hoskins, J. G., Steinerberger, S. \& Kluger, Y.
\newblock Fast interpolation-based -SNE for improved visualization of single-cell RNA-seq data.
\newblock {\em Nat. Methods} \textbf{16}, 243 (2019). doi:10.1038/s41592-018-0308-4.

\bibitem{linderman2019provably}
Linderman, G. C. \& Steinerberger, S.
\newblock Clustering with -SNE, provably.
\newblock {\em SIAM J. Math. Data Sci.} \textbf{1}, 313-332 (2019). doi: 10.1137/18M1216134.

\bibitem{liu2016iec}
Liu, H., Shao, M., Li, S. \& Fu, Y.
\newblock Infinite ensemble for image clustering.
\newblock In {\em Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining}, in print. {\em ACM} 1745-1754 (2016).

\bibitem{moon2019phate}
Moon, K.R. et al.
\newblock Visualizing structure and transitions in high-dimensional biological data.
\newblock {\em Nature Biotechnology} \textbf{37}, 1482–1492 (2019). doi:10.1038/s41587-019-0336-3.

\bibitem{nene1996coil20}
Nene, S.A., Nayar, S.K. \& Murase, H.
\newblock Columbia Object Image Library (COIL-20).
\newblock Technical Report No. CUCS-005-96, Department of Computer Science, Columbia University (1996).

\bibitem{nene1996coil100}
Nene, S.A., Nayar, S.K. \& Murase, H.
\newblock Columbia Object Image Library (COIL-100).
\newblock Technical Report No. CUCS-006-96, Department of Computer Science, Columbia University (1996).

\bibitem{oskolkov2019perp}
Oskolkov, N.
\newblock How to tune hyperparameters of tSNE [blog post].  
\newblock Towards Data Science. 18 July 2019. [Accessed 9 February 2020] https://towardsdatascience.com/how-to-tune-hyperparameters-of-tsne-7c0596a18868. 

\bibitem{shaham2018spectralnet}
Shaham, U. et al. 
\newblock SpectralNet: spectral clustering using deep neural networks.
\newblock {\em arXiv preprint} 	arXiv:1801.01587 (2018).

\bibitem{shekhar2016single}
Shekhar, K. et al.
\newblock Comprehensive classification of retinal bipolar neurons by single-cell transcriptomics.
\newblock {\em Cell} \textbf{166}, 1308–1323 (2016). doi:10.1016/j.cell.2016.07.054.

\bibitem{song2013aec}
Song, C. et al.
\newblock Auto-encoder based data clustering.
\newblock In {\em Iberoamerican Congress on Pattern Recognition}, in print. {\em Springer} 117-124 (2013). doi:10.1007/978-3-642-41822-8\_15.

\bibitem{steinerberger2019lecture}
Steinerberger, S.
\newblock Lecture, Math 421a: The Mathematics of Data Science.
\newblock {\em Yale University} (Fall 2019).

\bibitem{strehl2002nmi}
Strehl, A. \& Ghosh, J.
\newblock Cluster Ensembles – A Knowledge Reuse Framework for
Combining Multiple Partitions.
\newblock {\em Journal of Machine Learning Research} \textbf{3}, 583-617 (2002). doi:10.1162/153244303321897735.

\bibitem{trigeorgis2014nmfd}
Trigeorgis, G., Bousmalis, K., Zafeiriou, S. \& Schuller, B.
\newblock A Deep Semi-NMF Model for Learning Hidden Representations.
\newblock In {\em Proceedings of the 31st International Conference on Machine Learning}, in print. {\em PMLR} \textbf{32}(2) 1692-1700 (2014).

\bibitem{vander2008tsne}
van der Maaten, L. \& Hinton, G.
\newblock Visualizing data using -SNE.
\newblock {\em J. Mach. Learning Res.} \textbf{9}, 2579–2605 (2008).

\bibitem{vander2009parametrictsne}
van der Maaten, L.
\newblock Learning a Parametric Embedding by Preserving Local Structure.
\newblock In {\em Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics}, in print. {\em PMLR} \textbf{5} 384-391 (2009).

\bibitem{vonluxburg2006spectral}
von Luxburg, U. 
\newblock A tutorial on spectral clustering.
\newblock {\em Stat. Comput.} \textbf{17}, 395–416 (2007). doi:10.1007/s11222-007-9033-z.

\bibitem{wang2016tscd}
Wang, Z. et al.
\newblock Learning a task-specific deep architecture for clustering.
\newblock In {\em Proceedings of the 2016 SIAM International Conference on Data Mining}, in print. {\em SIAM} 369-377 (2016). doi:10.1137/1.9781611974348.42.

\bibitem{williams2000nystrom}
Williams, C. K. I. \& Seeger, M.
\newblock Using the Nyström method to speed up kernel machines.
\newblock In {\em Proceedings of the 13th International Conference on Neural Information Processing Systems (NIPS’00)}, in print. {\em MIT Press} 661–667 (2000).

\bibitem{yang2016jule}
Yang, J., Parikh, D. \& Batra, D.
\newblock Joint Unsupervised Learning of Deep Representations and Image Clusters.
\newblock In {\em 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, in print 5147-5156 (2016). doi:10.1109/CVPR.2016.556.

\bibitem{zhang2013acpic}
Zhang, W., Zhao, D. \& Wang, X. 
\newblock Agglomerative clustering via maximum incremental path integral.
\newblock {\em Pattern Recognition} \textbf{46}, 3056–3065 (2013). doi:10.1016/j.patcog.2013.04.013.

\end{thebibliography}


\end{document}
