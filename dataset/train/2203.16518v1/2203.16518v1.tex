\section{Experiments}
\label{sec:exp}

\begin{table*}[!t]
    \centering
    \resizebox{\textwidth}{!}{
        \begin{tabular}{l|l|ccccc|ccccc|cccc}
        \hline
        \multicolumn{2}{c|}{}
            & \multicolumn{5}{c|}{Top-1 Predicted Verb}
            & \multicolumn{5}{c|}{Top-5 Predicted Verbs}
            & \multicolumn{4}{c}{Ground-Truth Verb}  
        \\
        \hline
            &  
            &       &       &       & grnd & grnd
            &       &       &       & grnd & grnd
            &       &       & grnd & grnd  
        \\
        Set & Method
            & verb & value & value-all & value & value-all
            & verb & value & value-all & value & value-all
            & value & value-all & value & value-all
        \\
        \hline
        \hline
            & \multicolumn{15}{c}{\textit{Methods for Situation Recognition}}
        \\
        \cline{2-16}
        \multirow{9.5}{*}{dev}
            & CRF \cite{yatskar2016situation}
            & 32.25 & 24.56 & 14.28 & -- & -- 
            & 58.64 & 42.68 & 22.75 & -- & -- 
            & 65.90 & 29.50 & -- & -- 
        \\
            & CRF w/ DataAug \cite{yatskar2017commonly}
            & 34.20 & 26.56 & 15.61 & -- & --
            & 62.21 & 46.72 & 25.66 & -- & --
            & 70.80 & 34.82 & -- & --
        \\
            & RNN w/ Fusion \cite{mallya2017recurrent}
            & 36.11 & 27.74 & 16.60 & -- & --
            & 63.11 & 47.09 & 26.48 & -- & --
            & 70.48 & 35.56 & -- & --
        \\
            & GraphNet \cite{li2017situation}
            & 36.93 & 27.52 & 19.15 & -- & --
            & 61.80 & 45.23 & 29.98 & -- & --
            & 68.89 & 41.07 & -- & --
        \\
            & CAQ w/ RE-VGG \cite{cooray2020attention}
            & 37.96 & 30.15 & 18.58 & -- & --
            & 64.99 & 50.30 & 29.17 & -- & --
            & \textbf{73.62} & 38.71 & -- & --
        \\
            & Kernel GraphNet \cite{suhail2019mixture}
            & \textbf{43.21} & \textbf{35.18} & \textbf{19.46} & -- & --
            & \textbf{68.55} & \textbf{56.32} & \textbf{30.56} & -- & --
            & 73.14 & \textbf{41.68} & -- & --
        \\
        \cline{2-16}
            & \multicolumn{15}{c}{\textit{Methods for Grounded Situation Recognition}}
        \\
        \cline{2-16}
            & ISL \cite{pratt2020grounded}
            & 38.83 & 30.47 & 18.23 & 22.47 & 7.64
            & 65.74 & 50.29 & 28.59 & 36.90 & 11.66
            & 72.77 & 37.49 & 52.92 & 15.00 
        \\
            & JSL \cite{pratt2020grounded}
            & 39.60 & 31.18 & 18.85 & 25.03 & 10.16
            & 67.71 & 52.06 & 29.73 & 41.25 & 15.07
            & 73.53 & 38.32 & 57.50 & 19.29
        \\
            & GSRTR \cite{cho2021gsrtr}
            & 41.06 & 32.52 & 19.63 & 26.04 & 10.44
            & 69.46 & 53.69 & 30.66 & 42.61 & 15.98
            & 74.27 & 39.24 & 58.33 & 20.19
        \\
            & \cellcolor[gray]{0.9}\mbox{CoFormer} (Ours)
            & \cellcolor[gray]{0.9}\textbf{44.41} & \cellcolor[gray]{0.9}\textbf{35.87} & \cellcolor[gray]{0.9}\textbf{22.47} & \cellcolor[gray]{0.9}\textbf{29.37} & \cellcolor[gray]{0.9}\textbf{12.94}
            & \cellcolor[gray]{0.9}\textbf{72.98} & \cellcolor[gray]{0.9}\textbf{57.58} & \cellcolor[gray]{0.9}\textbf{34.09} & \cellcolor[gray]{0.9}\textbf{46.70} & \cellcolor[gray]{0.9}\textbf{19.06} 
            & \cellcolor[gray]{0.9}\textbf{76.17} & \cellcolor[gray]{0.9}\textbf{42.11} & \cellcolor[gray]{0.9}\textbf{61.15} & \cellcolor[gray]{0.9}\textbf{23.09} 
        \\
        \hline
            & \multicolumn{15}{c}{\textit{Methods for Situation Recognition}}
        \\
        \cline{2-16}
        \multirow{9.5}{*}{test}
             & CRF \cite{yatskar2016situation}
             & 32.34 & 24.64 & 14.19 & -- & --
             & 58.88 & 42.76 & 22.55 & -- & --
             & 65.66 & 28.96 & -- & --
        \\
            & CRF w/ DataAug \cite{yatskar2017commonly}
            & 34.12 & 26.45 & 15.51 & -- & --
            & 62.59 & 46.88 & 25.46 & -- & --
            & 70.44 & 34.38 & -- & --
        \\
            & RNN w/ Fusion \cite{mallya2017recurrent}
            & 35.90 & 27.45 & 16.36 & -- & --
            & 63.08 & 46.88 & 26.06 & -- & --
            & 70.27 & 35.25 & -- & --
        \\
            & GraphNet \cite{li2017situation}
            & 36.72 & 27.52 & 19.25 & -- & --
            & 61.90 & 45.39 & 29.96 & -- & --
            & 69.16 & 41.36 & -- & --
        \\
            & CAQ w/ RE-VGG \cite{cooray2020attention}
            & 38.19 & 30.23 & 18.47 & -- & --
            & 65.05 & 50.21 & 28.93 & -- & --
            & \textbf{73.41} & 38.52 & -- & --
        \\
            & Kernel GraphNet \cite{suhail2019mixture}
            & \textbf{43.27} & \textbf{35.41} & \textbf{19.38} & -- & --
            & \textbf{68.72} & \textbf{55.62} & \textbf{30.29} & -- & --
            & 72.92 & \textbf{42.35} & -- & --
        \\
        \cline{2-16}
            & \multicolumn{15}{c}{\textit{Methods for Grounded Situation Recognition}}
        \\
        \cline{2-16}
            & ISL \cite{pratt2020grounded}
            & 39.36 & 30.09 & 18.62 & 22.73 & 7.72
            & 65.51 & 50.16 & 28.47 & 36.60 & 11.56
            & 72.42 & 37.10 & 52.19 & 14.58
        \\
            & JSL \cite{pratt2020grounded}
            & 39.94 & 31.44 & 18.87 & 24.86 & 9.66
            & 67.60 & 51.88 & 29.39 & 40.60 & 14.72
            & 73.21 & 37.82 & 56.57 & 18.45
        \\
            & GSRTR \cite{cho2021gsrtr}
            & 40.63 & 32.15 & 19.28 & 25.49 & 10.10
            & 69.81 & 54.13 & 31.01 & 42.50 & 15.88
            & 74.11 & 39.00 & 57.45 & 19.67
        \\
            & \cellcolor[gray]{0.9}\mbox{CoFormer} (Ours)
            & \cellcolor[gray]{0.9}\textbf{44.66} & \cellcolor[gray]{0.9}\textbf{35.98} & \cellcolor[gray]{0.9}\textbf{22.22} & \cellcolor[gray]{0.9}\textbf{29.05} & \cellcolor[gray]{0.9}\textbf{12.21} 
            & \cellcolor[gray]{0.9}\textbf{73.31} & \cellcolor[gray]{0.9}\textbf{57.76} & \cellcolor[gray]{0.9}\textbf{33.98} & \cellcolor[gray]{0.9}\textbf{46.25} & \cellcolor[gray]{0.9}\textbf{18.37} 
            & \cellcolor[gray]{0.9}\textbf{75.95} & \cellcolor[gray]{0.9}\textbf{41.87} & \cellcolor[gray]{0.9}\textbf{60.11} & \cellcolor[gray]{0.9}\textbf{22.12} 
        \\
        \hline
    \end{tabular}}
    \vspace{-1mm}
    \caption{
        Quantitative evaluations of methods in SR and GSR.
        SR models are evaluated on the imSitu dataset, and GSR models are evaluated on the SWiG dataset.
        The only difference between the two datasets is the existence of bounding box annotation.
    }
    \vspace{-1mm}
    \label{table:result}
\end{table*}

\begin{table*}[!t]
    \centering
    \resizebox{\textwidth}{!}{
        \begin{tabular}{l|ccc|ccc|cccc}
        \hline
        \multicolumn{1}{c|}{}
            & \multicolumn{3}{c|}{\;Top-1 Predicted Verb\;}
            & \multicolumn{3}{c|}{\;Top-5 Predicted Verbs\;}
            & \multicolumn{4}{c}{Ground-Truth Verb}  
        \\
        \hline
            &       &        & grnd
            &       &        & grnd 
            &       &        & grnd & grnd   
        \\
            Method
            & verb & value & value
            & verb & value & value
            & value & value-all & value & value-all
        \\
        \hline
        \hline
            w/o Gaze-S1 Transformer
            & 42.46 & 34.21 & 28.23
            & 70.89 & 55.47 & 45.34  
            & 76.02 & 41.96 & 61.21 & 23.15
        \\
            w/o Gaze-S2 Transformer
            & 43.02 & 31.24 & 23.27 
            & 71.17 & 51.70 & 36.59 
            & 69.68 & 32.94 & 48.44 & 13.05
        \\
            w/o Noun Classifiers on Gaze-S1 Transformer
            & 41.30 & 33.33  & 27.50 
            & 69.76 & 55.05  & 44.96 
            & 75.97 & 41.94 & \textbf{61.32} & \textbf{23.39}
        \\
            w/o Gradient Flow from Gaze-S2 Transformer to Glance Transformer
            & 42.96 & 33.82 & 25.77
            & 70.97 & 54.59 & 41.11
            & 73.91 & 38.59 & 55.10 & 17.10
        \\
            w/o Verb Token in Gaze-S2 Transformer
            & 44.36 & 35.57 & 29.16 
            & 72.84 & 56.79 & 46.19
            & 74.53 & 39.83 & 60.07 & 21.83
        \\
            \cellcolor[gray]{0.9}\mbox{CoFormer} (Ours)
            & \cellcolor[gray]{0.9}\textbf{44.41} & \cellcolor[gray]{0.9}\textbf{35.87} & \cellcolor[gray]{0.9}\textbf{29.37} 
            & \cellcolor[gray]{0.9}\textbf{72.98} & \cellcolor[gray]{0.9}\textbf{57.58} & \cellcolor[gray]{0.9}\textbf{46.70} 
            & \cellcolor[gray]{0.9}\textbf{76.17} & \cellcolor[gray]{0.9}\textbf{42.11} & \cellcolor[gray]{0.9}61.15 & \cellcolor[gray]{0.9}23.09
        \\
        \hline
    \end{tabular}}
    \vspace{-1mm}
    \caption{
        Ablation study of \mbox{CoFormer} on the SWiG dev set.
        The contributions of different components used in our model are evaluated.
    }
    \label{table:ablation}
\end{table*}

\mbox{CoFormer} is evaluated on the SWiG dataset~\cite{pratt2020grounded}, which is constructed by adding box annotations to the imSitu dataset~\cite{yatskar2016situation}.
The imSitu dataset contains 75K, 25K and 25K images for train, development and test set, respectively.
This dataset contains 504 verbs, 11K nouns and 190 roles.
The number of roles in the frame of a verb ranges from 1 to 6. 
Each image is paired with the annotation of a verb, and three nouns from three different annotators for each role.
In addition to this annotation, the SWiG dataset provides a box annotation for each role (except role \textit{Place}).

\subsection{Evaluation Metric}
\noindent \textbf{Metric Details.} 
The prediction accuracy of verb is measured by \textit{verb}, that of noun is evaluated by \textit{value} and \mbox{\textit{value-all}}, and that of grounded noun is assessed by \mbox{\textit{grounded-value}} and \mbox{\textit{grounded-value-all}}.
Regarding to the noun metrics, \textit{value} measures whether a noun is correct for each role, and \mbox{\textit{value-all}} measures whether all nouns are correct for entire roles in a frame simultaneously. 
The noun prediction is considered correct if the predicted noun matches any of the three noun annotations given by three annotators.
For the grounded noun metrics, \mbox{\textit{grounded-value}} measures whether a noun and its grounding are correct for each role, and \mbox{\textit{grounded-value-all}} measures whether all nouns and their groundings are correct for entire roles in a frame simultaneously. 
The grounding prediction is considered correct if the predicted box existence is correct and the predicted bounding box has \mbox{Intersection-over-Union} (IoU) value at least 0.5 with the box annotation.
Note that the above metrics are calculated per verb and then averaged over all verbs, since the number of roles in a frame depends on a verb and each verb might be associated with a different number of samples in the dataset.
\begin{figure*}[t!]
    \centering
    \includegraphics[width=\textwidth]{Arxiv_Figures/Figure78_revise6.pdf}
    \caption{
    Attention scores from IL token to image features, from RL token to role features, and on \mbox{frame-role} queries.
    We visualize the attention scores computed from the last \mbox{self-attention} layer of the encoder in Glance transformer, the encoder in \mbox{Gaze-S1} transformer, and the decoder in \mbox{Gaze-S2} transformer, respectively.
    Higher attention scores are highlighted in red color on images.
    }
    \label{fig:qual_verb}
\end{figure*}
\begin{figure*}[t!]
    \centering
    \includegraphics[width=\textwidth]{Arxiv_Figures/Qual_Attn_4.pdf}
    \caption{
    Attentions scores from \mbox{frame-role} queries to image features.
    We visualize the attention scores computed from the last cross-attention layer of the decoder in \mbox{Gaze-S2} transformer.
    Higher attention scores are highlighted in red color on images.
    }
    \label{fig:qual_role_attn}
\end{figure*}

\noindent \textbf{Evaluation Settings.} 
Three evaluation settings are proposed for comprehensive evaluation: \textit{\mbox{Top-1} Predicted Verb}, \textit{\mbox{Top-5} Predicted Verbs}, and \textit{\mbox{Ground-Truth} Verb}.
In \textit{\mbox{Top-1} Predicted Verb} setting, the predicted nouns and their groundings are considered incorrect if the \mbox{top-1} verb prediction is incorrect.
In \textit{\mbox{Top-5} Predicted Verbs} setting, the predicted nouns and their groundings are considered incorrect if the \mbox{ground-truth} verb is not contained in the \mbox{top-5} predicted verbs.
In \textit{\mbox{Ground-Truth} Verb} setting, the predicted nouns and their groundings are obtained by conditioning on the \mbox{ground-truth} verb.
\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.965\textwidth]{Arxiv_Figures/prediction_results_2.pdf}
    \caption{
    Prediction results.
    Dashed boxes denote incorrect grounding predictions.
    Incorrect noun predictions are highlighted in gray color.
    }
    \vspace{-2mm}
    \label{fig:qual_pred}
\end{figure*}

\subsection{Implementation Details}
We use \mbox{ResNet-50}~\cite{resnet} pretrained on ImageNet~\cite{deng2009imagenet} as a CNN backbone following existing models~\cite{pratt2020grounded,cho2021gsrtr} in GSR.
Given an image, the CNN backbone extracts image features of size $h \times w \times c$, where $h = w = 22$ and $c=2048$.
The embedding dimension of each token is $d=512$.
We employ AdamW Optimizer~\cite{loshchilov2018decoupled} with $10^{-4}$ weight decay, $\beta_1=0.9$, and $\beta_2=0.999$.
We train \mbox{CoFormer} with $10^{-4}$ learning rate ($10^{-5}$ for the CNN backbone) which decreases by a factor of $10$ at epoch $30$.
Training \mbox{CoFormer} with batch size of $16$ for $40$ epochs takes about $30$ hours on four RTX 3090 GPUs.
Complete details including loss coefficients are provided in the supplementary material.

\subsection{Quantitative Evaluations}
\mbox{CoFormer} achieves the state of the art in all evaluations as shown in Table~\ref{table:result}.
Existing SR models~\cite{mallya2017recurrent, li2017situation, suhail2019mixture, cooray2020attention} use at least two \mbox{VGG-16}~\cite{vggnet} backbones, and GSR models~\cite{pratt2020grounded} employ two \mbox{ResNet-50}~\cite{resnet} backbones for verb and noun prediction, while \mbox{CoFormer} only employs a single \mbox{ResNet-50} backbone. 
Compared with GSRTR~\cite{cho2021gsrtr},
the improvements in the verb prediction accuracies range from 3.35\%p to 4.03\%p.
Regarding to the noun prediction accuracies, 
the improvements range from 1.84\%p to 3.89\%p, and those in the grounded noun prediction accuracies range from 2.11\%p to 4.09\%p.
These results demonstrate that the proposed
collaborative framework is effective for GSR.

\noindent \textbf{Ablation Study.} 
We analyze the effects of different components in \mbox{CoFormer} as shown in Table~\ref{table:ablation}.
When we train our model without using \mbox{Gaze-S1} transformer or \mbox{Gaze-S2} transformer,
the accuracies in verb prediction or grounded noun prediction 
largely decrease, which demonstrates 
the effectiveness of the collaborative framework.
Training our \mbox{CoFormer} without using the two noun classifiers placed on top of \mbox{Gaze-S1} transformer leads to significant drops in the verb prediction accuracies.
In this case, it is difficult for role features to learn involved nouns and their relations, while the encoder in \mbox{Gaze-S1} transformer aggregates the role features through self-attentions.
To figure out whether \mbox{Gaze-S2} transformer assists Glance transformer by forcing it to implicitly consider involved nouns, we train \mbox{CoFormer} by restricting the flow of loss gradients through the aggregated image features from \mbox{Gaze-S2} transformer to Glance transformer.
As shown in the fourth row of Table~\ref{table:ablation}, the verb prediction accuracies drop, which demonstrates that \mbox{Gaze-S2} transformer supports Glance transformer via loss gradients through the aggregated image features. 
In \mbox{CoFormer}, each \mbox{frame-role} query is constructed by an addition of a role token embedding and a verb token embedding.
We study how effective it is by training \mbox{CoFormer} without using a verb token embedding for the construction of \mbox{frame-role} queries. 
The fifth row of Table~\ref{table:ablation} shows that the grounded noun prediction accuracies drop, 
which demonstrates that the verb token embedding is helpful for grounded noun prediction.

\subsection{Qualitative Evaluations}
We visualize the attention scores computed in the attention layers of \mbox{CoFormer}.  
Figure~\ref{fig:qual_verb}(a) shows that IL token captures the essential features to estimate a verb for two \textit{Boating} images.
Figure~\ref{fig:qual_verb}(b) shows how much RL token focuses on the roles in the frame of the \mbox{ground-truth} verb, and the classification results from the noun classifier placed on top of the encoder in \mbox{Gaze-S1} transformer; attention scores among 190 roles sum to 1.
This demonstrates that RL token effectively
captures involved nouns and their relations through self-attentions in the encoder of \mbox{Gaze-S1} transformer. 
Figure~\ref{fig:qual_verb}(c) shows how role relations are captured through self-attentions on \mbox{frame-role} queries, which demonstrates that \mbox{CoFormer} similarly captures the relations if the situations in images are similar; attention scores sum to 1 in each column. 
Figure~\ref{fig:qual_role_attn} shows the local regions where \mbox{frame-role} queries focus on, and the predicted grounded nouns corresponding to the queries.
Figure~\ref{fig:qual_pred} shows prediction results of \mbox{CoFormer} on the SWiG test set.
The first row shows the correct predictions, and the second row shows several incorrect predictions.
