

\pdfoutput=1
\documentclass[11pt]{article}

\usepackage[dvipsnames]{xcolor}
\usepackage[final]{naacl2021}

\usepackage[utf8]{inputenc}


\usepackage{times}
\usepackage{latexsym}
\usepackage{xspace}
\usepackage{enumitem}
\renewcommand{\UrlFont}{\ttfamily\small}

\usepackage{url}
\usepackage[dvipsnames]{xcolor}
\usepackage{booktabs}
\usepackage{diagbox}
\usepackage{multirow}
\usepackage{balance}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{microtype}
\usepackage{wrapfig,lipsum}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{makecell}
\usepackage[T1]{fontenc}
\usepackage{soul}
\usepackage{tabularx, booktabs}
\usepackage{todonotes}
\usepackage{comment}
\newcommand{\cbb}{\mathbf{c}}
\newcolumntype{Y}{>{\centering\arraybackslash}X}
\newcolumntype{L}{>{\arraybackslash}X}
\newcolumntype{M}[1]{>{\centering\arraybackslash}m{#1}}

\algnewcommand\algorithmicinput{\textbf{Input:}}
\algnewcommand\algorithmicoutput{\textbf{Output:}}
\algnewcommand\algorithmicparameter{\textbf{Parameters:}}
\algnewcommand\INPUT{\item[\algorithmicinput]}
\algnewcommand\OUTPUT{\item[\algorithmicoutput]}
\algnewcommand\PARAMETER{\item[\algorithmicparameter]}


\newcommand\BibTeX{B{\sc ib}\TeX}
\newcommand\confname{EMNLP-IJCNLP 2019}
\newcommand\conforg{SIGDAT}

\newcommand{\bertbase}{BERT}
\newcommand{\bertlarge}{BERT}
\newcommand{\clstoken}{\texttt{[CLS]}}
\newcommand{\septoken}{\texttt{[SEP]}}
\newcommand{\kldiv}{\text{KL}}

\newcommand{\eat}[1]{\ignorespaces}
\RequirePackage{latexsym}
\RequirePackage{amsmath}
\RequirePackage{amsthm}
\RequirePackage{amssymb}
\RequirePackage{bm}


\newcommand{\R}{\mathbb{R}}

\newcommand{\zeromatrix}{\mathbf{0}}
\newcommand{\bD}{\mathbf{D}}
\newcommand{\bE}{\mathbf{E}}
\newcommand{\bP}{\mathbf{P}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\br}{\mathbf{r}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}

\newcommand{\mC}{\mathcal{C}}
\newcommand{\mD}{\mathcal{D}}
\newcommand{\mE}{\mathcal{E}}
\newcommand{\mG}{\mathcal{G}}
\newcommand{\mH}{\mathcal{H}}
\newcommand{\mK}{\mathcal{K}}
\newcommand{\mL}{\mathcal{L}}
\newcommand{\mN}{\mathcal{N}}
\newcommand{\mO}{\mathcal{O}}
\newcommand{\mP}{\mathcal{P}}
\newcommand{\mR}{\mathcal{R}}

\newcommand{\iL}{\mathit{L}}
\newcommand{\iQ}{\mathit{Q}}
\newcommand{\iR}{\mathit{R}}
\newcommand{\iU}{\mathit{U}}
\newcommand{\iV}{\mathit{V}}
\newcommand{\iX}{\mathit{X}}
\newcommand{\iY}{\mathit{Y}}

\newcommand{\sigmoid}{\text{sigm}}
\newcommand{\KL}{\text{KL}}
\newcommand{\Loss}{\mathcal{L}}


\newcommand{\bilinear}[3]{#1^{\top}#2#3}
\newcommand{\bidiag}[3]{(#1 \circ #3) ^{\top} #2}
\newcommand{\diag}[1]{\text{diag}\left(#1\right)}
\newcommand{\biband}[3]{#1^{\top}#2#3}

\newcommand{\leftarc}[1]{{[#1]}-}
\newcommand{\rightarc}[1]{-{[#1]}}
\newcommand{\wildcard}{}

\newcommand{\gene}[1]{{\texttt{#1}}}
\newcommand{\rev}[1]{#1}

\newcommand{\neigh}[1]{\text{nb}\left(#1\right)}
\newcommand{\diagm}[1]{\text{diag}\left(#1\right)}
\newcommand{\ind}[1]{\mathbf{1}\left(#1\right)}
\newcommand{\argmin}[1]{\underset{#1}{\text{argmin}}}
\newcommand{\BigO}[1]{\ensuremath{\operatorname{O}\bigl(#1\bigr)}}
\newcommand{\cheading}[2]{\textbf{#1\hfill #2}}
\newcommand{\highest}[1]{\textbf{#1}}





\newcommand{\ab}{\mathbf{a}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\db}{\mathbf{d}}
\newcommand{\eb}{\mathbf{e}}
\newcommand{\fb}{\mathbf{f}}
\newcommand{\gb}{\mathbf{g}}
\newcommand{\hb}{\mathbf{h}}
\newcommand{\ib}{\mathbf{i}}
\newcommand{\jb}{\mathbf{j}}
\newcommand{\kb}{\mathbf{k}}
\newcommand{\lb}{\mathbf{l}}
\newcommand{\mb}{\mathbf{m}}
\newcommand{\nbb}{\mathbf{n}}
\newcommand{\ob}{\mathbf{o}}
\newcommand{\pb}{\mathbf{p}}
\newcommand{\qb}{\mathbf{q}}
\newcommand{\rb}{\mathbf{r}}
\newcommand{\sbb}{\mathbf{s}}
\newcommand{\tb}{\mathbf{t}}
\newcommand{\ub}{\mathbf{u}}
\newcommand{\vb}{\mathbf{v}}
\newcommand{\wb}{\mathbf{w}}
\newcommand{\xb}{\mathbf{x}}
\newcommand{\yb}{\mathbf{y}}
\newcommand{\zb}{\mathbf{z}}

\newcommand{\abtil}{\tilde{\ab}}
\newcommand{\bbtil}{\tilde{\bb}}
\newcommand{\cbtil}{\tilde{\cbb}}
\newcommand{\dbtil}{\tilde{\db}}
\newcommand{\ebtil}{\tilde{\eb}}
\newcommand{\fbtil}{\tilde{\fb}}
\newcommand{\gbtil}{\tilde{\gb}}
\newcommand{\hbtil}{\tilde{\hb}}
\newcommand{\ibtil}{\tilde{\ib}}
\newcommand{\jbtil}{\tilde{\jb}}
\newcommand{\kbtil}{\tilde{\kb}}
\newcommand{\lbtil}{\tilde{\lb}}
\newcommand{\mbtil}{\tilde{\mb}}
\newcommand{\nbtil}{\tilde{\nbb}}
\newcommand{\obtil}{\tilde{\ob}}
\newcommand{\pbtil}{\tilde{\pb}}
\newcommand{\qbtil}{\tilde{\qb}}
\newcommand{\rbtil}{\tilde{\rb}}
\newcommand{\sbtil}{\tilde{\sbb}}
\newcommand{\tbtil}{\tilde{\tb}}
\newcommand{\ubtil}{\tilde{\ub}}
\newcommand{\vbtil}{\tilde{\vb}}
\newcommand{\wbtil}{\tilde{\wb}}
\newcommand{\xbtil}{\tilde{\xb}}
\newcommand{\ybtil}{\tilde{\yb}}
\newcommand{\zbtil}{\tilde{\zb}}

\newcommand{\atil}{\tilde{a}}
\newcommand{\btil}{\tilde{b}}
\newcommand{\ctil}{\tilde{c}}
\newcommand{\dtil}{\tilde{d}}
\newcommand{\etil}{\tilde{e}}
\newcommand{\ftil}{\tilde{f}}
\newcommand{\gtil}{\tilde{g}}
\newcommand{\htil}{\tilde{h}}
\newcommand{\itil}{\tilde{i}}
\newcommand{\jtil}{\tilde{j}}
\newcommand{\ktil}{\tilde{k}}
\newcommand{\ltil}{\tilde{l}}
\newcommand{\mtil}{\tilde{m}}
\newcommand{\ntil}{\tilde{n}}
\newcommand{\otil}{\tilde{o}}
\newcommand{\ptil}{\tilde{p}}
\newcommand{\qtil}{\tilde{q}}
\newcommand{\rtil}{\tilde{r}}
\newcommand{\stil}{\tilde{s}}
\newcommand{\ttil}{\tilde{t}}
\newcommand{\util}{\tilde{u}}
\newcommand{\vtil}{\tilde{v}}
\newcommand{\wtil}{\tilde{w}}
\newcommand{\xtil}{\tilde{x}}
\newcommand{\ytil}{\tilde{y}}
\newcommand{\ztil}{\tilde{z}}

\newcommand{\ahat}{\hat{a}}
\newcommand{\bhat}{\hat{b}}
\newcommand{\chat}{\hat{c}}
\newcommand{\dhat}{\hat{d}}
\newcommand{\ehat}{\hat{e}}
\newcommand{\fhat}{\hat{f}}
\newcommand{\ghat}{\hat{g}}
\newcommand{\hhat}{\hat{h}}
\newcommand{\ihat}{\hat{i}}
\newcommand{\jhat}{\hat{j}}
\newcommand{\khat}{\hat{k}}
\newcommand{\lhat}{\hat{l}}
\newcommand{\mhat}{\hat{m}}
\newcommand{\nhat}{\hat{n}}
\newcommand{\ohat}{\hat{o}}
\newcommand{\phat}{\hat{p}}
\newcommand{\qhat}{\hat{q}}
\newcommand{\rhat}{\hat{r}}
\newcommand{\shat}{\hat{s}}
\newcommand{\that}{\hat{t}}
\newcommand{\uhat}{\hat{u}}
\newcommand{\vhat}{\hat{v}}
\newcommand{\what}{\hat{w}}
\newcommand{\xhat}{\hat{x}}
\newcommand{\yhat}{\hat{y}}
\newcommand{\zhat}{\hat{z}}

\newcommand{\abhat}{\hat{\ab}}
\newcommand{\bbhat}{\hat{\bb}}
\newcommand{\cbhat}{\hat{\cb}}
\newcommand{\dbhat}{\hat{\db}}
\newcommand{\ebhat}{\hat{\eb}}
\newcommand{\fbhat}{\hat{\fb}}
\newcommand{\gbhat}{\hat{\gb}}
\newcommand{\hbhat}{\hat{\hb}}
\newcommand{\ibhat}{\hat{\ib}}
\newcommand{\jbhat}{\hat{\jb}}
\newcommand{\kbhat}{\hat{\kb}}
\newcommand{\lbhat}{\hat{\lb}}
\newcommand{\mbhat}{\hat{\mb}}
\newcommand{\nbhat}{\hat{\nb}}
\newcommand{\obhat}{\hat{\ob}}
\newcommand{\pbhat}{\hat{\pb}}
\newcommand{\qbhat}{\hat{\qb}}
\newcommand{\rbhat}{\hat{\rb}}
\newcommand{\sbhat}{\hat{\sb}}
\newcommand{\tbhat}{\hat{\tb}}
\newcommand{\ubhat}{\hat{\ub}}
\newcommand{\vbhat}{\hat{\vb}}
\newcommand{\wbhat}{\hat{\wb}}
\newcommand{\xbhat}{\hat{\xb}}
\newcommand{\ybhat}{\hat{\yb}}
\newcommand{\zbhat}{\hat{\zb}}

\newcommand{\Atil}{\tilde{A}}
\newcommand{\Btil}{\tilde{B}}
\newcommand{\Ctil}{\tilde{C}}
\newcommand{\Dtil}{\tilde{D}}
\newcommand{\Etil}{\tilde{E}}
\newcommand{\Ftil}{\tilde{F}}
\newcommand{\Gtil}{\tilde{G}}
\newcommand{\Htil}{\tilde{H}}
\newcommand{\Itil}{\tilde{I}}
\newcommand{\Jtil}{\tilde{J}}
\newcommand{\Ktil}{\tilde{K}}
\newcommand{\Ltil}{\tilde{L}}
\newcommand{\Mtil}{\tilde{M}}
\newcommand{\Ntil}{\tilde{N}}
\newcommand{\Otil}{\tilde{O}}
\newcommand{\Ptil}{\tilde{P}}
\newcommand{\Qtil}{\tilde{Q}}
\newcommand{\Rtil}{\tilde{R}}
\newcommand{\Stil}{\tilde{S}}
\newcommand{\Ttil}{\tilde{T}}
\newcommand{\Util}{\tilde{U}}
\newcommand{\Vtil}{\tilde{V}}
\newcommand{\Wtil}{\tilde{W}}
\newcommand{\Xtil}{\tilde{X}}
\newcommand{\Ytil}{\tilde{Y}}
\newcommand{\Ztil}{\tilde{Z}}

\newcommand{\abar}{\bar{a}}
\newcommand{\bbar}{\bar{b}}
\newcommand{\cbar}{\bar{c}}
\newcommand{\dbar}{\bar{d}}
\newcommand{\ebar}{\bar{e}}
\newcommand{\fbar}{\bar{f}}
\newcommand{\gbar}{\bar{g}}
\newcommand{\hbr}{\bar{h}}
\newcommand{\ibar}{\bar{i}}
\newcommand{\jbar}{\bar{j}}
\newcommand{\kbar}{\bar{k}}
\newcommand{\lbar}{\bar{l}}
\newcommand{\mbar}{\bar{m}}
\newcommand{\nbar}{\bar{n}}
\newcommand{\obar}{\bar{o}}
\newcommand{\pbar}{\bar{p}}
\newcommand{\qbar}{\bar{q}}
\newcommand{\rbar}{\bar{r}}
\newcommand{\sbar}{\bar{s}}
\newcommand{\tbar}{\bar{t}}
\newcommand{\ubar}{\bar{u}}
\newcommand{\vbar}{\bar{v}}
\newcommand{\wbar}{\bar{w}}
\newcommand{\xbar}{\bar{x}}
\newcommand{\ybar}{\bar{y}}
\newcommand{\zbar}{\bar{z}}

\newcommand{\abbar}{\bar{\ab}}
\newcommand{\bbbar}{\bar{\bb}}
\newcommand{\cbbar}{\bar{\cb}}
\newcommand{\dbbar}{\bar{\db}}
\newcommand{\ebbar}{\bar{\eb}}
\newcommand{\fbbar}{\bar{\fb}}
\newcommand{\gbbar}{\bar{\gb}}
\newcommand{\hbbar}{\bar{\hb}}
\newcommand{\ibbar}{\bar{\ib}}
\newcommand{\jbbar}{\bar{\jb}}
\newcommand{\kbbar}{\bar{\kb}}
\newcommand{\lbbar}{\bar{\lb}}
\newcommand{\mbbar}{\bar{\mb}}
\newcommand{\nbbar}{\bar{\nbb}}
\newcommand{\obbar}{\bar{\ob}}
\newcommand{\pbbar}{\bar{\pb}}
\newcommand{\qbbar}{\bar{\qb}}
\newcommand{\rbbar}{\bar{\rb}}
\newcommand{\sbbar}{\bar{\sbb}}
\newcommand{\tbbar}{\bar{\tb}}
\newcommand{\ubbar}{\bar{\ub}}
\newcommand{\vbbar}{\bar{\vb}}
\newcommand{\wbbar}{\bar{\wb}}
\newcommand{\xbbar}{\bar{\xb}}
\newcommand{\ybbar}{\bar{\yb}}
\newcommand{\zbbar}{\bar{\zb}}

\newcommand{\Ab}{\mathbf{A}}
\newcommand{\Bb}{\mathbf{B}}
\newcommand{\Cb}{\mathbf{C}}
\newcommand{\Db}{\mathbf{D}}
\newcommand{\Eb}{\mathbf{E}}
\newcommand{\Fb}{\mathbf{F}}
\newcommand{\Gb}{\mathbf{G}}
\newcommand{\Hb}{\mathbf{H}}
\newcommand{\Ib}{\mathbf{I}}
\newcommand{\Jb}{\mathbf{J}}
\newcommand{\Kb}{\mathbf{K}}
\newcommand{\Lb}{\mathbf{L}}
\newcommand{\Mb}{\mathbf{M}}
\newcommand{\Nb}{\mathbf{N}}
\newcommand{\Ob}{\mathbf{O}}
\newcommand{\Pb}{\mathbf{P}}
\newcommand{\Qb}{\mathbf{Q}}
\newcommand{\Rb}{\mathbf{R}}
\newcommand{\Sbb}{\mathbf{S}}
\newcommand{\Tb}{\mathbf{T}}
\newcommand{\Ub}{\mathbf{U}}
\newcommand{\Vb}{\mathbf{V}}
\newcommand{\Wb}{\mathbf{W}}
\newcommand{\Xb}{\mathbf{X}}
\newcommand{\Yb}{\mathbf{Y}}
\newcommand{\Zb}{\mathbf{Z}}

\newcommand{\Abtil}{\tilde{\Ab}}
\newcommand{\Bbtil}{\tilde{\Bb}}
\newcommand{\Cbtil}{\tilde{\Cb}}
\newcommand{\Dbtil}{\tilde{\Db}}
\newcommand{\Ebtil}{\tilde{\Eb}}
\newcommand{\Fbtil}{\tilde{\Fb}}
\newcommand{\Gbtil}{\tilde{\Gb}}
\newcommand{\Hbtil}{\tilde{\Hb}}
\newcommand{\Ibtil}{\tilde{\Ib}}
\newcommand{\Jbtil}{\tilde{\Jb}}
\newcommand{\Kbtil}{\tilde{\Kb}}
\newcommand{\Lbtil}{\tilde{\Lb}}
\newcommand{\Mbtil}{\tilde{\Mb}}
\newcommand{\Nbtil}{\tilde{\Nb}}
\newcommand{\Obtil}{\tilde{\Ob}}
\newcommand{\Pbtil}{\tilde{\Pb}}
\newcommand{\Qbtil}{\tilde{\Qb}}
\newcommand{\Rbtil}{\tilde{\Rb}}
\newcommand{\Sbtil}{\tilde{\Sbb}}
\newcommand{\Tbtil}{\tilde{\Tb}}
\newcommand{\Ubtil}{\tilde{\Ub}}
\newcommand{\Vbtil}{\tilde{\Vb}}
\newcommand{\Wbtil}{\tilde{\Wb}}
\newcommand{\Xbtil}{\tilde{\Xb}}
\newcommand{\Ybtil}{\tilde{\Yb}}
\newcommand{\Zbtil}{\tilde{\Zb}}

\newcommand{\Abar}{\bar{A}}
\newcommand{\Bbar}{\bar{B}}
\newcommand{\Cbar}{\bar{C}}
\newcommand{\Dbar}{\bar{D}}
\newcommand{\Ebar}{\bar{E}}
\newcommand{\Fbar}{\bar{F}}
\newcommand{\Gbar}{\bar{G}}
\newcommand{\Hbar}{\bar{H}}
\newcommand{\Ibar}{\bar{I}}
\newcommand{\Jbar}{\bar{J}}
\newcommand{\Kbar}{\bar{K}}
\newcommand{\Lbar}{\bar{L}}
\newcommand{\Mbar}{\bar{M}}
\newcommand{\Nbar}{\bar{N}}
\newcommand{\Obar}{\bar{O}}
\newcommand{\Pbar}{\bar{P}}
\newcommand{\Qbar}{\bar{Q}}
\newcommand{\Rbar}{\bar{R}}
\newcommand{\Sbar}{\bar{S}}
\newcommand{\Tbar}{\bar{T}}
\newcommand{\Ubar}{\bar{U}}
\newcommand{\Vbar}{\bar{V}}
\newcommand{\Wbar}{\bar{W}}
\newcommand{\Xbar}{\bar{X}}
\newcommand{\Ybar}{\bar{Y}}
\newcommand{\Zbar}{\bar{Z}}

\newcommand{\Abbar}{\bar{\Ab}}
\newcommand{\Bbbar}{\bar{\Bb}}
\newcommand{\Cbbar}{\bar{\Cb}}
\newcommand{\Dbbar}{\bar{\Db}}
\newcommand{\Ebbar}{\bar{\Eb}}
\newcommand{\Fbbar}{\bar{\Fb}}
\newcommand{\Gbbar}{\bar{\Gb}}
\newcommand{\Hbbar}{\bar{\Hb}}
\newcommand{\Ibbar}{\bar{\Ib}}
\newcommand{\Jbbar}{\bar{\Jb}}
\newcommand{\Kbbar}{\bar{\Kb}}
\newcommand{\Lbbar}{\bar{\Lb}}
\newcommand{\Mbbar}{\bar{\Mb}}
\newcommand{\Nbbar}{\bar{\Nb}}
\newcommand{\Obbar}{\bar{\Ob}}
\newcommand{\Pbbar}{\bar{\Pb}}
\newcommand{\Qbbar}{\bar{\Qb}}
\newcommand{\Rbbar}{\bar{\Rb}}
\newcommand{\Sbbar}{\bar{\Sb}}
\newcommand{\Tbbar}{\bar{\Tb}}
\newcommand{\Ubbar}{\bar{\Ub}}
\newcommand{\Vbbar}{\bar{\Vb}}
\newcommand{\Wbbar}{\bar{\Wb}}
\newcommand{\Xbbar}{\bar{\Xb}}
\newcommand{\Ybbar}{\bar{\Yb}}
\newcommand{\Zbbar}{\bar{\Zb}}

\newcommand{\Ahat}{\hat{A}}
\newcommand{\Bhat}{\hat{B}}
\newcommand{\Chat}{\hat{C}}
\newcommand{\Dhat}{\hat{D}}
\newcommand{\Ehat}{\hat{E}}
\newcommand{\Fhat}{\hat{F}}
\newcommand{\Ghat}{\hat{G}}
\newcommand{\Hhat}{\hat{H}}
\newcommand{\Ihat}{\hat{I}}
\newcommand{\Jhat}{\hat{J}}
\newcommand{\Khat}{\hat{K}}
\newcommand{\Lhat}{\hat{L}}
\newcommand{\Mhat}{\hat{M}}
\newcommand{\Nhat}{\hat{N}}
\newcommand{\Ohat}{\hat{O}}
\newcommand{\Phat}{\hat{P}}
\newcommand{\Qhat}{\hat{Q}}
\newcommand{\Rhat}{\hat{R}}
\newcommand{\Shat}{\hat{S}}
\newcommand{\That}{\hat{T}}
\newcommand{\Uhat}{\hat{U}}
\newcommand{\Vhat}{\hat{V}}
\newcommand{\What}{\hat{W}}
\newcommand{\Xhat}{\hat{X}}
\newcommand{\Yhat}{\hat{Y}}
\newcommand{\Zhat}{\hat{Z}}

\newcommand{\Abhat}{\hat{\Ab}}
\newcommand{\Bbhat}{\hat{\Bb}}
\newcommand{\Cbhat}{\hat{\Cb}}
\newcommand{\Dbhat}{\hat{\Db}}
\newcommand{\Ebhat}{\hat{\Eb}}
\newcommand{\Fbhat}{\hat{\Fb}}
\newcommand{\Gbhat}{\hat{\Gb}}
\newcommand{\Hbhat}{\hat{\Hb}}
\newcommand{\Ibhat}{\hat{\Ib}}
\newcommand{\Jbhat}{\hat{\Jb}}
\newcommand{\Kbhat}{\hat{\Kb}}
\newcommand{\Lbhat}{\hat{\Lb}}
\newcommand{\Mbhat}{\hat{\Mb}}
\newcommand{\Nbhat}{\hat{\Nb}}
\newcommand{\Obhat}{\hat{\Ob}}
\newcommand{\Pbhat}{\hat{\Pb}}
\newcommand{\Qbhat}{\hat{\Qb}}
\newcommand{\Rbhat}{\hat{\Rb}}
\newcommand{\Sbhat}{\hat{\Sb}}
\newcommand{\Tbhat}{\hat{\Tb}}
\newcommand{\Ubhat}{\hat{\Ub}}
\newcommand{\Vbhat}{\hat{\Vb}}
\newcommand{\Wbhat}{\hat{\Wb}}
\newcommand{\Xbhat}{\hat{\Xb}}
\newcommand{\Ybhat}{\hat{\Yb}}
\newcommand{\Zbhat}{\hat{\Zb}}

\newcommand{\Acal}{\mathcal{A}}
\newcommand{\Bcal}{\mathcal{B}}
\newcommand{\Ccal}{\mathcal{C}}
\newcommand{\Dcal}{\mathcal{D}}
\newcommand{\Ecal}{\mathcal{E}}
\newcommand{\Fcal}{\mathcal{F}}
\newcommand{\Gcal}{\mathcal{G}}
\newcommand{\Hcal}{\mathcal{H}}
\newcommand{\Ical}{\mathcal{I}}
\newcommand{\Jcal}{\mathcal{J}}
\newcommand{\Kcal}{\mathcal{K}}
\newcommand{\Lcal}{\mathcal{L}}
\newcommand{\Mcal}{\mathcal{M}}
\newcommand{\Ncal}{\mathcal{N}}
\newcommand{\Ocal}{\mathcal{O}}
\newcommand{\Pcal}{\mathcal{P}}
\newcommand{\Qcal}{\mathcal{Q}}
\newcommand{\Rcal}{\mathcal{R}}
\newcommand{\Scal}{{\mathcal{S}}}
\newcommand{\Tcal}{{\mathcal{T}}}
\newcommand{\Ucal}{\mathcal{U}}
\newcommand{\Vcal}{\mathcal{V}}
\newcommand{\Wcal}{\mathcal{W}}
\newcommand{\Xcal}{\mathcal{X}}
\newcommand{\Ycal}{\mathcal{Y}}
\newcommand{\Zcal}{\mathcal{Z}}

\newcommand{\bA}{\mathbb{A}}
\newcommand{\bB}{\mathbb{B}}
\newcommand{\bC}{\mathbb{C}}
\newcommand{\bF}{\mathbb{F}}
\newcommand{\bG}{\mathbb{G}}
\newcommand{\bH}{\mathbb{H}}
\newcommand{\bI}{\mathbb{I}}
\newcommand{\bJ}{\mathbb{J}}
\newcommand{\bK}{\mathbb{K}}
\newcommand{\bL}{\mathbb{L}}
\newcommand{\bM}{\mathbb{M}}
\newcommand{\bN}{\mathbb{N}}
\newcommand{\bO}{\mathbb{O}}
\newcommand{\bQ}{\mathbb{Q}}
\newcommand{\bR}{\mathbb{R}}
\newcommand{\bS}{{\mathbb{S}}}
\newcommand{\bT}{{\mathbb{T}}}
\newcommand{\bU}{\mathbb{U}}
\newcommand{\bV}{\mathbb{V}}
\newcommand{\bW}{\mathbb{W}}
\newcommand{\bY}{\mathbb{Y}}
\newcommand{\bZ}{\mathbb{Z}}

\newcommand{\Atilde}{\widetilde{A}}
\newcommand{\Btilde}{\widetilde{B}}
\newcommand{\Ctilde}{\widetilde{C}}
\newcommand{\Dtilde}{\widetilde{D}}
\newcommand{\Etilde}{\widetilde{E}}
\newcommand{\Ftilde}{\widetilde{F}}
\newcommand{\Gtilde}{\widetilde{G}}
\newcommand{\Htilde}{\widetilde{H}}
\newcommand{\Itilde}{\widetilde{I}}
\newcommand{\Jtilde}{\widetilde{J}}
\newcommand{\Ktilde}{\widetilde{K}}
\newcommand{\Ltilde}{\widetilde{L}}
\newcommand{\Mtilde}{\widetilde{M}}
\newcommand{\Ntilde}{\widetilde{N}}
\newcommand{\Otilde}{\widetilde{O}}
\newcommand{\Ptilde}{\widetilde{P}}
\newcommand{\Qtilde}{\widetilde{Q}}
\newcommand{\Rtilde}{\widetilde{R}}
\newcommand{\Stilde}{\widetilde{S}}
\newcommand{\Ttilde}{\widetilde{T}}
\newcommand{\Utilde}{\widetilde{U}}
\newcommand{\Vtilde}{\widetilde{V}}
\newcommand{\Wtilde}{\widetilde{W}}
\newcommand{\Xtilde}{\widetilde{X}}
\newcommand{\Ytilde}{\widetilde{Y}}
\newcommand{\Ztilde}{\widetilde{Z}}

\renewcommand{\vec}[1]{\mathbf{\boldsymbol{#1}}}

\newcommand{\avec}{\vec{a}}
\newcommand{\bvec}{\vec{b}}
\newcommand{\cvec}{\vec{c}}
\newcommand{\dvec}{\vec{d}}
\newcommand{\evec}{\vec{e}}
\newcommand{\fvec}{\vec{f}}
\newcommand{\gvec}{\vec{g}}
\newcommand{\hvec}{\vec{h}}
\newcommand{\ivec}{\vec{i}}
\newcommand{\jvec}{\vec{j}}
\newcommand{\kvec}{\vec{k}}
\newcommand{\lvec}{\vec{l}}
\newcommand{\mvec}{\vec{m}}
\newcommand{\nvec}{\vec{n}}
\newcommand{\ovec}{\vec{o}}
\newcommand{\pvec}{\vec{p}}
\newcommand{\qvec}{\vec{q}}
\newcommand{\rvec}{\vec{r}}
\newcommand{\svec}{\vec{s}}
\newcommand{\tvec}{\vec{t}}
\newcommand{\uvec}{\vec{u}}
\newcommand{\vvec}{\vec{v}}
\newcommand{\wvec}{\vec{w}}
\newcommand{\xvec}{\vec{x}}
\newcommand{\yvec}{\vec{y}}
\newcommand{\zvec}{\vec{z}}

\newcommand{\va}{\vec{a}}
\newcommand{\vecb}{\vec{b}}
\newcommand{\vc}{\vec{c}}
\newcommand{\vd}{\vec{d}}
\newcommand{\veee}{\vec{e}}
\newcommand{\vf}{\vec{f}}
\newcommand{\vg}{\vec{g}}
\newcommand{\vh}{\vec{h}}
\newcommand{\vi}{\vec{i}}
\newcommand{\vo}{\vec{o}}
\newcommand{\vp}{\vec{p}}
\newcommand{\vq}{\vec{q}}
\newcommand{\vr}{\vec{r}}
\newcommand{\vs}{\vec{s}}
\newcommand{\vt}{\vec{t}}
\newcommand{\vu}{\vec{u}}
\newcommand{\vv}{\vec{v}}
\newcommand{\vx}{\vec{x}}
\newcommand{\vw}{\vec{w}}
\newcommand{\vy}{\vec{y}}
\newcommand{\vz}{\vec{z}}

\newcommand{\Avec}{\vec{A}}
\newcommand{\Bvec}{\vec{B}}
\newcommand{\Cvec}{\vec{C}}
\newcommand{\Dvec}{\vec{D}}
\newcommand{\Evec}{\vec{E}}
\newcommand{\Fvec}{\vec{F}}
\newcommand{\Gvec}{\vec{G}}
\newcommand{\Hvec}{\vec{H}}
\newcommand{\Ivec}{\vec{I}}
\newcommand{\Jvec}{\vec{J}}
\newcommand{\Kvec}{\vec{K}}
\newcommand{\Lvec}{\vec{L}}
\newcommand{\Mvec}{\vec{M}}
\newcommand{\Nvec}{\vec{N}}
\newcommand{\Ovec}{\vec{O}}
\newcommand{\Pvec}{\vec{P}}
\newcommand{\Qvec}{\vec{Q}}
\newcommand{\Rvec}{\vec{R}}
\newcommand{\Svec}{\vec{S}}
\newcommand{\Tvec}{\vec{T}}
\newcommand{\Uvec}{\vec{U}}
\newcommand{\Vvec}{\vec{V}}
\newcommand{\Wvec}{\vec{W}}
\newcommand{\Xvec}{\vec{X}}
\newcommand{\Yvec}{\vec{Y}}
\newcommand{\Zvec}{\vec{Z}}

\newcommand{\Amat}{\Ab}
\newcommand{\Bmat}{\Bb}
\newcommand{\Cmat}{\Cb}
\newcommand{\Dmat}{\Db}
\newcommand{\Emat}{\Eb}
\newcommand{\Fmat}{\Fb}
\newcommand{\Gmat}{\Gb}
\newcommand{\Hmat}{\Hb}
\newcommand{\Imat}{\Ib}

\newcommand{\Vmat}{\Vb}
\newcommand{\Wmat}{\Wb}
\newcommand{\Xmat}{\Xb}
\newcommand{\Ymat}{\Yb}
\newcommand{\Zmat}{\Zb}


\newcommand{\yvecbar}{\bar{\vec{y}}}
\newcommand{\wvecbar}{\bar{\vec{w}}}
\newcommand{\xvecbar}{\bar{\vec{x}}}
\newcommand{\yvectil}{\tilde{\vec{y}}}
\newcommand{\yvechat}{\hat{\vec{y}}}






\ifx\BlackBox\undefined
\newcommand{\BlackBox}{\rule{1.5ex}{1.5ex}}  \fi

\ifx\QED\undefined
\def\QED{~\rule[-1pt]{5pt}{5pt}\par\medskip}
\fi

\ifx\proof\undefined
\newenvironment{proof}{\par\noindent{\em Proof:\ }}{\hfill\BlackBox\
    \label{eqn:span_begin_prob}
    P_b(i^k) = {\exp(s_b(i^k)) \over Z_b},

P(i^k,j^k) = P_b(i^k) P_e(j^k) = {\exp (s_b(i^k) + s_e(j^k)) \over Z_b Z_e}.

\label{eqn:mh_standard}
\svec^k_{i,j} = \text{MultiHead}( \qvec_i , \mvec^k_{j}) \in \RR^{|\text{Head}|}

\label{eqn:mh_standard_bias}
\svec^k_{i,j} = \text{MultiHead}( \qvec_i , \mvec^k_{j}) +\bvec_{k} 

\label{eq:ll}
\mathcal{L}(\mathbf{x}, \mathbf{y}; \theta) = \text{log}  p_{\theta}(\mathbf{y} | \mathbf{x}) =  \sum_{i}^{N} \text{log} p_{\theta}(y_i | \mathbf{x}, y_{1:i-1})

g_{\mathbf{V}} = - \nabla_{\mathbf{V}} \mathcal{L}(\mathbf{x}, \mathbf{y}; \theta) \\
\hat{\mathbf{V}} = \mathbf{V} + \text{SG} ( \epsilon g_{\mathbf{V}} / || g_{\mathbf{V}} ||_2 )

\mathcal{L}_{\text{AT}}(\mathbf{x}, \mathbf{y}; \theta) = \mathcal{L}_{}(\mathbf{x}, \mathbf{y}; \hat{\theta})

\mathcal{L} = \alpha \mathcal{L}(\mathbf{x}, \mathbf{y}; \theta) + \beta    \mathcal{L}_{\text{AT}}(\mathbf{x}, \mathbf{y}; \theta)






%
 

\section{Experiments: Hybrid Model}
\label{sec:exp_hybrid}
In this part, we study whether it is advantageous to combine the generative and extractive readers in a hybrid fashion.
Specifically, we leverage the improved extractive and generative models from \autoref{sec:exp_extractive} and \autoref{sec:exp_generative}, respectively.
Similar to previous experiments, the same retriever is used for retrieving top 100 passages for each given question for both extractive and generative readers.

In order to evaluate the advantage of the hybrid of the extractive and generative models (UnitedQA), we include two homogeneous ensemble baselines, one consisting of only extractive readers (UnitedQA-E++) and the other hybrid of exclusively generative models (UnitedQA-G++). Each model is trained independently with different random seeds.
In our study, a simple linear interpolation over all model predictions is used for producing the final answer.
Specifically, we combine three models for each ensemble case.
For homogeneous ensemble cases, the majority prediction is used.
For the hybrid of extractive and generative readers, we select a three-model combination from the set of three generative and three extractive models base on their performance on the dev set.
In addition, we use a scalar  for weighting extractive predictions and  for scaling generative predictions. The answer with the highest weighted vote is then predicted as the final answer.
The results are summarized in lower part of \autoref{tab:extractive_vs_sota}.

As expected, all ensemble models show an improvement over their single model counterparts.
However, it is worth noting that the two homogeneous ensemble baselines, UnitedQA-E++ and UnitedQA-G++, only provide marginal improvements over the corresponding best single models.
The significant improvement brought by our proposed hybrid approach indicates the benefit of combining extractive and generative readers for open-domain QA.

Moreover, we evaluate our hybrid model trained on NaturalQuestions directly to the dev and test sets introduced by the EfficientQA competition \footnote{\url{https://efficientqa.github.io/}}.
As shown in \autoref{tab:exp_efficient_qa}, our UnitedQA again outperforms both single models and homogeneous ensembles on the dev set, and is the best performing system based on the EfficientQA leaderboard.

\begin{table}[t]
    \centering
    \begin{tabular}{l|c|c}
    \hline
\toprule
Model &  Dev & Test\\
\midrule
UnitedQA-G & 48.6 & - \\
UnitedQA-E & 51.1 & - \\
\midrule
UnitedQA-G++ & 50.5 & -\\
UnitedQA-E++ & 51.9 & -\\
UnitedQA & \textbf{54.1} & \textbf{54.0}\\
\midrule
2nd best & - & 53.9 \\
\bottomrule
    \end{tabular}
    \caption{Results on the dev and test sets of EfficientQA. Exact match score is reported. The 2nd best system's result is from the official leaderboard\footnote{\url{https://efficientqa.github.io/unrestricted_leaderboard.html}}.}
    \label{tab:exp_efficient_qa}
\end{table}


\subsection{Analysis}
\label{ssec:gen_ext_analyses}
Given the advantage of the hybrid approach to open-domain QA, 
we study in detail the behavior of our SOTA extractive and generative readers (\ie United-E and United-G).
First, we evaluate the impact of the retrieval model. 
Second, following \citet{lewis2020question}, we conduct a break-down evaluation of the readers to investigate what drives their overall performance, \ie the extent of memorizing vs generalization.
Lastly, we carry out a manual inspection of the prediction errors made by the extractive and generative models, respectively.

\noindent
\textbf{Impact of Retrieval Recall.} 
Here, we vary the number of retrieved passages during inference and report the evaluation results in terms of end-to-end QA exact match score of United-E and United-G along with the corresponding top- retrieval accuracy.
The results are summarized in \autoref{tab:topk_eval}.
As expected, when the number of retrieved passages increases, both top- retrieval accuracy and the end-to-end QA performance improve.
However, there is a noticeable gap between the improvement of retrieving more passages (i.e., recall) and that of the corresponding end-to-end QA performance, especially for the extractive reader.
This is likely caused by additional noise introduced with improved retrieval recall.
Specifically, only half of the retriever improvement can be effectively utilized by the extractive model while the generative model can benefit more from retrieving more passages.
This suggests that by concatenating all passages in vector space, the generative model are more effective in de-noising in comparison to the extractive model.




\begin{table}[t]
    \centering
    \begin{tabular}{@{\hskip2pt}c|c|c|c|c@{\hskip2pt}}
    \hline
\toprule
& & Top-20  & Top-100 &  \\
\midrule
\multirow{3}{*}{NQ}
& Retrieval & 78.4 & 85.4 & +9 \\
& United-E & 49.8 & 51.9 & +4 \\
& United-G & 49.3 & 52.3 & +6 \\
\midrule
\multirow{3}{*}{TriviaQA}
& Retrieval & 79.9 & 84.4 & +6 \\
& United-E & 67.1 & 68.9 & +3 \\
& United-G & 63.4 & 67.0 & +6 \\
\bottomrule
    \end{tabular}
    \caption{Retieval top- accuracy and end-to-end QA extact match scores on the test sets of NaturalQuestions (NQ) and TriviaQA. United-E and United-G stand for our extractive and generative models respectively.}
    \label{tab:topk_eval}
\end{table}

\begin{table*}[t]
    \centering
     \begin{tabular}{@{\hskip2pt}cc|c|M{1.5cm}M{1.5cm}M{1.5cm}M{1.5cm}M{1.5cm}@{\hskip2pt}}
    \hline
\toprule
  Dataset & Model & Total & Question Overlap & No Question Overlap & Answer Overlap &Answer Overlap Only & No Overlap \\
\midrule
\multirow{2}{*}{NaturalQuestions}
& UnitedQA-G     & \textbf{52.3} & \textbf{72.2} & 40.5 & \textbf{62.7} & \textbf{45.4} & 34.0 \\
& UnitedQA-E     & 51.9          & 69.4 & \textbf{41.5} & 60.1 & 45.1 & \textbf{37.6} \\
\midrule
\multirow{2}{*}{TriviaQA}
& UnitedQA-G     & 67.0          & 86.6          & 62.3          & 76.5          & 69.1          & 42.6 \\
& UnitedQA-E     & \textbf{68.9} & \textbf{89.3} & \textbf{62.7} & \textbf{78.6} & \textbf{70.6} & \textbf{44.3} \\
\bottomrule
    \end{tabular}
    \caption{Breakdown evaluation on NaturalQuestions and TriviaQA based on test splits defined in \cite{lewis2020question}. Exact match score is reported. UnitedQA-E and UnitedQA-G denotes our extractive and generative models respectively.}
    \label{tab:breakdown_eval}
\end{table*}
\noindent
\textbf{Breakdown Evaluation.}
Following \citet{lewis2020question}, we carry out a breakdown evaluation of model performance over the NaturalQuestions and TriviaQA test sets.
Given their superior performance, we again only consider our improved extractive and generative models, \ie UnitedQA-E\textsubscript{large} and UnitedQA-G respectively.
The evaluation is summarized in \autoref{tab:breakdown_eval}.
In comparison to their corresponding overall performance, both the extractive and generative models achieve much better performance on the ``Overlap'' categories (\ie ``Question Overlap'' and ``Answer Overlap'') for both NaturalQuestions and TrivaQA, which indicates that both models perform well for question and answer memorization.
Different from question and answer memorization, there is a pronounced performance drop for both models on the "Answer Overlap Only" category where certain amount of relevance inference capability is required to succeed.
Lastly, we see that both extractive and generative models suffer some significant performance degradation 
for the "No Overlap" column which highlights model's generalization evaluation.
Nevertheless, the extractive model demonstrate a better QA generalization by achieving a better performance on the "No Overlap" category on both datasets.

\noindent
\textbf{Error Analysis.}
Here, we conduct analyses into prediction errors made by the extractive and generative models based on automatic evaluation.
For this study, we use the dev set introduced by the EfficientQA competition\footnote{\url{https://efficientqa.github.io/}} which is constructed in the same way as the original NaturalQuestions dataset. 
Specifically, we group prediction errors into three categorizes: 1) common prediction errors made by 
both the extractive and generative models, 
2) prediction errors made by the extractive model, 3) prediction errors produced by the generative model.
In the following, we first carry out a manual inspection into the common errors. Then, we compare the prediction errors made by extractive and generative models, respectively.

First of all, there is an error rate of  of those consensus predictions made by both extractive and generative models according to the automatic evaluation. Based on  randomly selected examples, we find that around  of those predictions are actually valid answers as shown in the top part of \autoref{tab:pred_err}. In addition to predictions that are answers at different granularity or semantically equivalent ones, some of those prediction errors are likely caused by the ambiguity in questions. As the given example in \autoref{tab:pred_err}, based on the specificity, the model prediction is also a valid answer.
This highlights the limitation of the current evaluation metric, which does not accurately estimate the existing open-domain QA system capabilities.
As shown in the bottom part of \autoref{tab:pred_err}, most of representative errors are due to the confusion of related concepts, entities or events that are mentioned frequently together with the corresponding gold answers. 

Next, all questions from the dev set are categorized based the \textit{WH} question word, \ie \textit{what, which, when, who, how, where}.
We then report the relative performance change of each \textit{WH} category for both extractive and generative models over their corresponding overall prediction accuracy in \autoref{fig:rel_acc_wh_type}.
First, it is easy to see that both extractive and generative models achieve the best performance for entity related \textit{who} questions, which is likely to be the result of high ratio of questions and answers of this type seen during training. 
In contrast, the answers to \textit{what} questions can play a much richer syntactic role in context, making it more difficult for both extractive and generative models to perform well.
Very interestingly, the generative model exhibits the strength for temporal reasoning, whereas the extractive model does not.
This difference suggests that it is worth exploring better temporal modeling strategies to improve the extractive model in the future.


\begin{figure}
    \centering
    \includegraphics[width=0.48\textwidth]{figs/ext_vs_gen_wh_type_acc.pdf}
    \caption{Relative accuracy of different \textit{WH} questions. The relative accuracy is the relative change of a \textit{WH} category accuracy to the overall model accuracy.}
    \label{fig:rel_acc_wh_type}
\end{figure}



\begin{table*}[t]
    \centering
     \begin{tabular}{@{\hskip2pt}c@{\hskip2pt}|@{\hskip2pt}l@{\hskip2pt}}
    \hline
\toprule
Valid answers\\
\midrule
\multirow{2}{*}{Answer in different granularity} & Q: When was harry potter and the deathly hallows part 2 movie released\\
& Prediction: 2011 / Gold: 15 July 2011 \\
\multirow{2}{*}{Semantically equivalent answer} & Q: minimum age limit for chief justic of india\\
& Prediction: 65 / Gold: 65 years \\
\multirow{2}{*}{Ambiguity question} & Q: who won her first tennis grand slam in 2018\\
& Prediction: Carolin Wozniacki / Gold: Simona Halep\\
\midrule
Wrong answers\\
\midrule
\multirow{2}{*}{Part as whole error} & Q: the official U.S. poverty line is based on the cost of what\\
& Prediction: food / Gold: ICP purchasing power \\
\multirow{2}{*}{Entity Confusion} & Q: actor who played tommy in terms of endearment\\
& Prediction: Jeff Daniels / Gold: Troy Bishop\\
\multirow{2}{*}{Temporal confusion}  & Q: when did the saskatchewan roughriders last won the grey cup\\
& Prediction: 2007 / Gold: 2013\\
\midrule
    \end{tabular}
    \caption{Examples of prediction errors as judged by the automatic evaluation.}
    \label{tab:pred_err}
\end{table*}








 \section{Related Work}
\label{sec:related_work}
\textbf{Open-domain Question Answering.}
Open-domain QA requires a system to answer questions based on evidence retrieved from a large corpus such as Wikipedia \cite{trec8_qa,drqa}.
Recent progress has been made towards improving evidence retrieval through both sparse vector models like TF-IDF or BM25 \cite{drqa,min-etal-2019-discrete},
and dense vector models based on BERT \cite{lee-etal-2019-latent,karpukhin-etal-2020-dense,guu2020realm}.
Generally, the dense representations complement the sparse vector methods for passage retrieval
as they can potentially give high similarity to semantically related text pairs, even without exact lexical overlap.
Unlike most of existing work focusing on a pipeline model, \citet{lee-etal-2019-latent} propose a pre-training objective for jointly training both the retrieval encoder and reader.
Their approach outperforms most of recent pipeline methods on multiple open-domain QA datasets, and is further extended by \citet{guu2020realm} with asynchronously re-indexing the passages during the training.
Instead, in this work, we focus on developing a hybrid approach for open-domain QA.
By simply combing answer predictions from our improved extractive and generative models, our UnitedQA achieves significant improvements over recent state-of-the-art models.



\noindent
\textbf{Reading Comprehension with Noisy Labels.}
There has been a line of work on improving distantly-supervised reading comprehension models by developing learning methods and model architectures that can better use noisy labels. Most of them focus on the document-level QA, where all paragraphs share the same document context.
\citet{clarkgardner} propose a paragraph-pair ranking objective for learning with multiple paragraphs so that the model can distinguish relevant paragraphs from irrelevant ones.
In \cite{lin-etal-2018-denoising}, a coarse-to-fine model is proposed to handle label noise by aggregating information from relevant paragraphs and then extracting answers from selected ones.
\citet{min-etal-2019-discrete} propose a hard EM learning scheme where only passage-level loss is considered for document-level QA. More recently, different probabilistic assumptions with corresponding training and inference methods are examined in \cite{cheng-etal-2020-probabilistic} again for document-level QA with distant supervision.
In our work, we further extend the multi-objective formulation proposed in \cite{cheng-etal-2020-probabilistic} with the hard EM learning \cite{min-etal-2019-discrete} for
enhancing extractive Open-domain QA, where the input passages are given by a retrieval model and are typically from different documents.
 

\section{Conclusion}
\label{sec:conclusion}
In this study, we propose a hybrid model for open-domain QA, called UnitedQA, which combines the strengths of extractive and generative readers.
We validate the effectiveness of UnitedQA on two popular open-domain QA benchmarks, NaturalQuestions and TriviaQA.
Our results show that the proposed UnitedQA model
significantly outperforms single extractive and generative models as well as their corresponding homogeneous ensembles, and creates new state-of-the-art on both benchmarks.
We also perform a comprehensive empirical study to investigate the relative contributions of different components of our model and the techniques we use to improve the readers. 
 \section*{Acknowledgments}
We thank Yuning Mao for valuable discussions and comments, Microsoft Research Technology Engineering team for setting up GPU machines. 
 

\bibliography{ref,qa}
\bibliographystyle{acl_natbib}
\clearpage


\end{document}