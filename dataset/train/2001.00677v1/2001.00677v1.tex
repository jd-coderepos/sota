Our evaluation consists of image classification and human activity recognition (HAR). For the first part, we consider visual domain adaptation experiments commonly used in the UDA literature. They are constructed on MNIST, MNIST-M, Street View House Numbers (SVHN), Synthetic Digits (SYN DIGITS), CIFAR-10 and STL-10. For HAR experiments, we evaluate on OPPORTUNITY \cite{ordonez2016deep} and WiFi \cite{yousefi2017survey} datasets. We use A $\rightarrow$ B to denote the domain adaptation task with source domain A and target domain B. 


The architecture of the embedding encoder $f$ is task-specific: we use similar architectures to \cite{shu2018dirt} for visual recognition and WiFi datasets; we employ state-of-the-art DeepConvLSTM architecture \cite{ordonez2016deep} for OPPORTUNITY. For the domain discriminator $D$, we forward the extracted feature to a simple fully-connected layer ($x-128-\text{ReLU}-1$). We adopt shallow fully-connected networks for embedding classifier $g$.




\begin{table}[t]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{cccccc}
\hline


Source & \textbf{MNIST} & \textbf{SVHN} & \textbf{SYN} & \textbf{CIFAR} & \textbf{STL} \\
Target & \textbf{MNISTM} & \textbf{MNIST} & \textbf{SVHN} & \textbf{STL} & \textbf{CIFAR} \\ \hline

DAN \cite{long2015learning} & 76.9 & 71.1 & 88.0 & - & - \\
DANN \cite{ganin2016domain} & 81.5 & 71.1 & 90.3 & - & - \\
DRCN \cite{Ghifary2016DeepRN} & - & 82.0 & - & 66.4 & 58.7 \\
ATT \cite{saito2017asymmetric}  & 94.2 & 86.2 & 92.9 & - & - \\
$\Pi$-model \cite{French2017SelfensemblingFV} & - & 92.0 & 94.2 & 76.3 & 64.2 \\
JDDA \cite{chen2019joint} & 88.4 & 94.2 & - & - & - \\
\hline
Source-Only & 62.5 & 72.6 & 88.1 & 75.9 & 61.8 \\ 
VADA \cite{shu2018dirt} & 97.7 & 97.9 & 94.8 & 80.0 & 73.5 \\
DIRT-T \cite{shu2018dirt} & 98.9 & \textbf{99.4} & 96.1 & 80.0 & 75.3 \\
\textbf{IIMT} & \textbf{99.5} & 97.3 & \textbf{97.0} & \textbf{83.1} & \textbf{81.6} \\
\hline
\end{tabular}
}
\caption{Test set accuracy ($\%$) on visual UDA benchmarks.}
\label{tab.visual.domain.experiments}
\end{table}
























\begin{table}[t]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{cccccccc}
\hline
\multicolumn{1}{c}{\textbf{Method}} &
\textbf{\begin{tabular}[c]{@{}c@{}} 1 $\rightarrow$ 2 \\ \end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}} 1 $\rightarrow$ 3 \\ \end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}} 2 $\rightarrow$ 1 \end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}} 2  $\rightarrow$ 3 \\ \end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}} 3 $\rightarrow$ 1 \\ \end{tabular}} &
\textbf{\begin{tabular}[c]{@{}c@{}} 3 $\rightarrow$ 2 \\ \end{tabular}} &
\textbf{\begin{tabular}[c]{@{}c@{}} Avg \\ \end{tabular}}\\   \hline

Source-Only                 &	0.652 &	0.640 &	0.696 &	0.637 &	 0.659 &	0.631 &	0.652 \\ 
DANN &	0.768 &	0.731 &	0.785 &	0.694 &	 0.746 &	0.725 &	0.741 \\ 
VADA     &	0.776 &	0.747 & 0.797 &	0.734 &	0.726 &	0.720 &  0.750 \\
\textbf{IIMT}     & \textbf{0.809} &	\textbf{0.780}	& \textbf{0.813} &	\textbf{0.745} &	\textbf{0.831} &	\textbf{0.787} &	\textbf{0.794} \\
\hline
\end{tabular}
}
\caption{Test set weighted F1 score on OPPORTUNITY. }
\label{tab.oppo.experiments}
\end{table}


\begin{table}[t]
\centering
\scalebox{0.77}{
\begin{tabular}{cc}
\hline
\multicolumn{1}{c}{\textbf{Method}} &
\textbf{\begin{tabular}[c]{@{}c@{}} Room A $\rightarrow$ Room B \\ \end{tabular}} \\   \hline 
Source-Only & 36.4   \\ 
DANN & 38.7 \\
VADA \cite{shu2018dirt} & 53.0 \\
DIRT-T \cite{shu2018dirt} & 53.0 \\
\textbf{IIMT} & \textbf{60.6} \\
\hline
\end{tabular}
}
\caption{Test set accuracy ($\%$) on WiFi HAR UDA task.}
\label{tab.non-visual-domain.wifi.experiments}
\end{table}


\subsection{Evaluation on Visual Recognition}
Our benchmarking results on common visual domain adaptation tasks are summarize in Table \ref{tab.visual.domain.experiments}. For digits classification UDA tasks, our proposed approach outperforms the state-of-the-art results achieved by DIRT-T for MNIST $\rightarrow$ MNISTM and SYN DIGITS $\rightarrow$ SVHN. Note that DIRT-T accuracy is close to $100\%$ and further improvement is very valuable. For SVHN $\rightarrow$ MNIST, our performance is on par with VADA while only scoring lower than DIRT-T. 











For object recognition UDA tasks CIFAR $\leftrightarrow$STL, the two adaptation directions have different degrees of difficulty. Since STL is much smaller, a model trained on it without any adaptation performs badly on the target domain. For both directions, our algorithm significantly outperforms state-of-the-art methods: in CIFAR $\rightarrow$ STL, we achieve $3.1\%$ margin-of-improvement; for STL $\rightarrow$ CIFAR, we achieve $8.1\%$ and $6.3\%$ margin-of-improvement on VADA and DIRT-T respectively. Note that DIRT-T had a remarkable improvement of $11\%$ over $\Pi$-model for STL $\rightarrow$ CIFAR. The capability to achieve significant further improvement demonstrates that, the proposed approach is highly effective in exploiting the target domain unlabeled data, even when source domain labels are limited.

























\subsection{Evaluation on Human Activity Recognition}
In building HAR system for fitness monitoring and assisted living \cite{mohr2017personal,song2018optimizing}, acquiring training data requires careful system setup, long-term human subjects involvement and laborious labeling efforts. Since repeating such procedure for new sensing environment is prohibitive in practice, UDA becomes crucial in the deployment of practical HAR systems. We conduct experiments on this problem with both IMU sensor and WiFi based HAR datasets. Note that \textit{domain} refers to human subject in the first application and corresponds to sensing room in the second problem. 




Sensor-based HAR is performed on the public OPPORTUNITY repository containing sensor signals of sporadic gestures. Based on the first $3$ subjects, we construct $6$ cross-subject UDA experiments to explicitly investigate the influence of user (domain) change to the classifier accuracy. The sensor signals are segmented using a $0.8$s moving window with half overlapping. Due to the imbalanced class distribution (caused by the \textit{null} class), we use the weighted F1 score as the evaluation metric. The experimental results are summarized in Table \ref{tab.oppo.experiments}. First, we observe that without domain adaptation, all classifiers have inferior performance on the target subject, thus evidencing the necessity of UDA in HAR. Second, possibly due to the high class imbalance, VADA performance is only slightly better than conventional adversarial training. On the other hand, our proposed method significantly outperforms both approaches, achieving an averaged improvement over $0.04$.





\begin{table}[t]
\centering
\scalebox{0.77}{
\begin{tabular}{ccc}
\hline
\textbf{Method} & \textbf{STL $\rightarrow$ CIFAR} & \textbf{1 $\rightarrow$ 2} \\   \hline 
Source-Only & 61.8 & 0.652  \\ 
DANN  & 63.4 & 0.768 \\
Add intra-domain $\mathcal{L}_s$, $\mathcal{L}_t$  &  75.7 &  0.787\\
Add inter-domain $\mathcal{L}_q$ & 79.2 & 0.798 \\
Add inter-domain $\mathcal{L}_z$ & 81.6 & 0.809 \\
\hline
\end{tabular}
}
\caption{Ablation experiments for STL $\rightarrow$ CIFAR and OPPORTUNITY 1 $\rightarrow$ 2 tasks. Each row corresponds to adding the specified component(s) to the previous row.}
\label{tab.visual-domain.ablation.experiments}
\end{table}









We conduct experiments on the same WiFi activity recognition dataset \cite{yousefi2017survey} used in \cite{shu2018dirt}. In here the CSI from commercial WiFi systems was collected in $2$ different rooms. We conduct the adaptation experiment from Room A to B in recognizing the $7$ physical activities such as walk, fall, pickup etc. The $90$-dimensional CSI time series are temporally segmented with a $1$s moving window and $0.2$s stride. As can be seen from the experimental results in Table \ref{tab.non-visual-domain.wifi.experiments}, our approach significantly outperforms VADA/DIRT-T.


Finally, we perform an ablation analysis in order to study the contribution of each mixup component and the results are listed in Table \ref{tab.visual-domain.ablation.experiments}. We observe that each proposed component is effective in achieving performance gain. Importantly, the performance with intra-domain mixup training is already slightly better than VADA (see Table \ref{tab.visual.domain.experiments} and \ref{tab.oppo.experiments}). This demonstrates that intra-domain mixup is as effective as VAT \cite{shu2018dirt} in enforcing the locally-Lipschitz constraint. While \cite{shu2018dirt} utilizes VAT on each domain, we additionally perform cross-domain mixup. The two inter-domain components in Table \ref{tab.visual-domain.ablation.experiments} gain collective improvement of $6\%$ for STL $\rightarrow$ CIFAR and $0.02$ for 1 $\rightarrow$ 2.





















