\documentclass{article}
\usepackage{spconf, amsmath, graphicx}
\usepackage{pifont, diagbox, multirow, float, wrapfig, booktabs, breqn}
\usepackage[dvipsnames]{xcolor}
\usepackage{soul}

\definecolor{MyPurple}{HTML}{990099}
\definecolor{MyGray}{HTML}{666666}
\newcommand{\cmark}{\text{\ding{51}}}
\newcommand{\xmark}{\text{\ding{55}}}
\newcommand{\TG}[1]{\textcolor{red}{(TG: #1)}}

\title{FusionCount: Efficient Crowd Counting via Multiscale Feature Fusion}


\threeauthors
{Yiming Ma}
    {
    Warwick Mathematics Institute\\
    University of Warwick, UK
	}
{Victor Sanchez}
    {
    Department of Computer Science\\
    University of Warwick, UK
    }
{Tanaya Guha}
    {
    School of Computing Science\\
    University of Glasgow, UK
    }









\begin{document}
\date{}
\ninept
\maketitle
\begin{abstract}
State-of-the-art crowd counting models follow an encoder-decoder approach. Images are first processed by the encoder to extract features. Then, to account for perspective distortion, the highest-level feature map is fed to extra components to extract multiscale features, which are the input to the decoder to generate crowd densities. However, in these methods, features extracted at earlier stages during encoding are underutilised, and the multiscale modules can only capture a limited range of receptive fields, albeit with considerable computational cost. This paper proposes a novel crowd counting architecture (\emph{FusionCount}), which exploits the adaptive fusion of a large majority of encoded features instead of relying on additional extraction components to obtain multiscale features. Thus, it can cover a more extensive scope of receptive field sizes and lower the computational cost. We also introduce a new channel reduction block, which can extract saliency information during decoding and further enhance the model's performance. Experiments on two benchmark databases demonstrate that our model achieves state-of-the-art results with reduced computational complexity.
\end{abstract}

\begin{keywords}
Crowd density estimation, multiscale feature fusion, efficient crowd counting
\end{keywords}

\section{Introduction}
\label{sec:intro}

Crowd counting aims to automatically estimate the number of individuals present in a scene from an image or video. It can be applied in numerous areas, such as traffic control \cite{6514618}, biological studies \cite{lu2017tasselnet}, and recently,  social distancing monitoring \cite{9562868}.

Over the years, models for crowd counting have evolved from using classical regression models, such as random forests \cite{lempitsky2010learning} and Gaussian processes \cite{5459191}, to high-performing convolutional neural networks (CNNs) \cite{MCNN, CSRNet, CAN, BayesianLoss}. These deep nets usually adopt an encoder-decoder approach: First, an image is fed to the encoder to learn data representations (feature maps). The decoder then exploits the highest-level representation (the output from the encoder's last layer) to generate the density map, which is the distribution of the crowd. Since the convolutional and pooling blocks of VGG networks \cite{VGG}, where each layer exploits kernels of a fixed size, constitute most encoders, the size of receptive fields remains constant across the last encoded feature map. Thus, this representation can only handle images where crowds are of similar scales. However, people are usually depicted in various sizes because of the camera perspective, and as a result, the encoded feature should also have different receptive field sizes to model the scale variation. Multi-column structures \cite{MCNN, SANet} have been proposed to solve this problem. However, it has been recently shown that features from each column are almost identical, and training deep models of this type can be very unproductive \cite{CSRNet}. Therefore, to solve this scale issue, state-of-the-art methods \cite{CAN, M-SFANet, SMANet} employ a multiscale module that further processes the encoded representation and generates a feature map with different receptive field sizes. However, such a strategy ignores that those feature maps extracted by shallower encoding layers already provide information about different scales, and leveraging extra components makes the overall model more computationally expensive. 

Hence, our contribution in this paper is a novel multiscale mechanism that addresses the scale issue by leveraging the majority of features generated from the encoder to avoid extra feature-extraction modules and keep the computational cost low. This design incorporates a comprehensive range of receptive field sizes (6 to 192), covering 
almost all possible scales a person can depict in a crowd image. Experiments on two benchmark databases (ShanghaiTech A \& B \cite{MCNN}) demonstrate that our model can achieve state-of-the-art or comparable results with significantly fewer floating-point operations.
 
\section{Related Work}
\label{sec:related work}

Early crowd counting approaches \cite{983420, 4761705, 5206621} are based on object detectors, while later works tend to avoid them because of their sensitivity to occlusion and the enormous efforts required to annotate bounding boxes. Some of these non-detection-based methods \cite{lempitsky2010learning, 5459191, chen2012feature, 6619163} treat crowd counting as a regression problem: they learn low-level feature representations first, from which the total count is then directly regressed. The training losses of these approaches depend only on the ground-truth count (a scalar) and do not consider crowd density distribution, hence suffering poor generalisation. Thus, these models have soon been superseded by algorithms \cite{CP-CNN, Hydra-CNN, CAN, M-SFANet} that instead predict crowd densities, and these density-based methods primarily rely on CNNs.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{images/encoding.png}
    \caption{The encoder of our proposed model FusionCount: Only the first 17 layers of the original VGG-16 are leveraged, and feature maps are collected starting from the third layer. Numbers in \textcolor{MyPurple}{purple} are features' receptive field sizes and those tuples () in \textcolor{MyGray}{gray} indicate their sizes, assuming the input image has the size of . Features with the same spatial resolution are grouped together for the first-phase fusion.}
    \label{fig:encoder}
\end{figure*}

Since 3D spatial locations have to be projected onto a 2D space while an RGB crowd image is captured from the real world, people can be depicted in different sizes due to perspective distortion, and the variation in scales can severely affect density estimation. To solve this problem, in \cite{Geometric}, extra geometric information is exploited to adapt their model to different scenes, but this information is not always provided. Therefore, later methods tend to learn the scales of crowds implicitly. For example, Hydra-CNN \cite{Hydra-CNN} divides an image into a pyramid of patches, each representing a different scale and fed to a separate encoder head. Then, all encoded features are concatenated without further processing and utilised to generate the density map. This approach neglects the fact that scales vary continuously across the whole image. CAN \cite{CAN} is then proposed to address this issue. The whole model involves only one encoder, so a spatial pyramid pooling module \cite{SpatialPyramidPooling} is leveraged to make it scale-aware. Then, features are averaged according to learnable weights to ensure that receptive field sizes of the fused feature changes smoothly. However, this architecture is less efficient since it does not exploit low-level features extracted during encoding. These representations, along with the final high-level feature map, can provide information about different scales because they have disparate receptive field sizes. Also, the multiscale module in \cite{CAN} uses only four filter sizes, thereby covering a limite range of scales.
 
\section{Our Model}
\label{sec:method}

This section exhaustively describes our proposed crowd counting model, \emph{FusionCount}. It extensively leverages representations learned during encoding to compute \emph{first-phase} multiscale features, and its decoder further fuses these scale-aware features to generate the density map.

\subsection{Encoding}
\label{subsec:encoding}

Following common practices \cite{CSRNet, CAN} in the field, VGG-16 \cite{VGG} is employed as our model's encoder. The last max-pooling and fully connected layers are removed since they are accountable for class prediction. Therefore, the encoder comprises 13 convolutional layers and four max-pooling layers. Since the two feature maps extracted before the first max-pooling operation are not sufficiently informative, only features learned afterwards are preserved and fused. Thus, as shown in Fig.~\ref{fig:encoder}, the encoder outputs 15 feature maps in total, denoted by , . These feature maps are divided into four groups according to their heights and widths:  -- ,  -- ,  -- , and  -- .

































 
\subsection{Feature Fusion}
\label{ssec:feature_fusion}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.48\textwidth]{images/feature_fusion.png}
    \caption{The feature fusion modules of FusionCount: In each group, weights are computed from contrast features. Then features from convolutional layers are averaged by using these weights and subsequently concatenated with the feature map from the pooling layer.}
    \label{fig:feature_fusion}
\end{figure}

Suppose  is a group of feature maps of interest, where  is generated by a max-pooling layer and has  channels, while the rest are are produced by convolutional layers and have  channels. To assemble a scale-aware representation from them, we adopt a similar strategy as the one proposed in \cite{CAN}, and our method is illustrated in Fig.~\ref{fig:feature_fusion}. Given that scales change continuously accross an image, the receptive field size of the output multiscale feature, which models the scales, should also be spatially continuous. To accomplish this aim, we fuse these features via weighted averaging, where weight maps are learned from each feature's spatial importance.

Firstly, since  and  () may not have the same number of channels, we use point-wise convolution to expand , and denote the output by . Notice that since  is the output of a max-pooling layer and the derivation of  does not involve any integration of neighbouring spatial information,  is less influenced by background noise. With such an advantage, all other representations are compared against it. Namely, following \cite{CAN}, we propose a similar concept to \emph{contrast features}:

with . Since  and  () have different receptive field sizes, the contrast features  incorporate disparities between any spatial location and its neighbouring pixels. Thus, they can facilitate learning the boundary of each person, thereby determining each receptive field size's relative importance. We then compute the weights  as follows:

where  denotes the sigmoid function.\footnote{Based on experiements (see Section \ref{ssec:contrast_features}), the performance of the overall model drops if contrast features are not utilised to compute weights.} Using these weights, ,  we then combine  adaptively and concatenate the averaged feature map with . Finally, we add a bottleneck layer to reduce computation. This process can be expressed as:

where , Conv, ``'' and ``'' denote the fused feature, the bottleneck layer, channel-wise concatenation, and element-wise product, respectively.
 
\subsection{Decoding}
\label{ssec:decoding}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.48\textwidth]{images/decoding.png}
    \caption{The decoding process of FusionCount: starting from , the proposed channel reduction module first decreases its number of channels. The result is then upsampled and fused with another firs-phase multiscale feature .}
    \label{fig:decoder}
\end{figure}

As shown in Fig.~\ref{fig:feature_fusion}, we denote the first-phase multiscale features generated from the four groups via \eqref{multiscale} as , ,  and . These features are further fused in the reverse order (see Fig.~\ref{fig:decoder}). To combine  and  via addition, we first transform the shape of . Specifically, the number of channels of  is lowered, while its spatial size is expanded. Traditionally, a single point-wise convolutional kernel is used to reduce the nubmer of channels. However, inspired by dilated convolution, which has been shown to have the ability to extract deeper salient information while maintaining the spatial resolution \cite{CSRNet}, we propose a new channel reduction module. Our two-stream module comprises a dilated convolution block and one bottleneck layer, and the two columns are connected via addition. \footnote{The effectiveness of the combination of the dilated convolution block and the point-wise convolution is verified in Section \ref{ssec:ablation_study}.} To match the spatial resolution of , we use bilinear interpolation to double the size of   after channel redcution. Then these two features can be fused by summation. The new fused feature is then combined with  following a similar pattern --- we first modify its dimensions via the proposed channel reduction module and bilinear interpolation, and then add it to  . This process is followed iteratively until all features are fused. Finally, we feed the final fused feature to the output layer to generate the estimation.
  
\section{Experiments}
\label{sec:experiments}

\subsection{Datasets}
\label{ssec:datasets}

We use the ShanghaiTech A \& B datasets \cite{MCNN} for model evaluation and comparison. In ShanghaiTech A, there are 482 crowd images collected from the Internet. Three hundred of them constitute the training set, and the rest comprise the test set. Scenes in this dataset are highly congested, with an average count of about 501. Also, since images have different resolutions (height and width values range from 182 to 1024), training models on this dataset can be tricky. The size of ShanghaiTech B is larger (716 instances in total; 400 for training and 316 for testing), with an average count of approximately 123. Images from this dataset are taken from a surveillance view in a shopping street and are therefore less crowded. Also, considering that these images have a fixed resolution (), this dataset is more suitable for real-world applications e.g., public safety monitoring.
 
\subsection{Experiment Settings}
\label{ssec:settings}

During recent years, loss functions based on probability theory, such as Bayes' theorem and Wasserstein distance, have been shown to help models achieve stronger generalisation capabilities and have thus gained increased popularity. We use the DM-Count loss \cite{DMCount} to supervise the training of FusionCount. An Adam optimiser \cite{Adam} with a learning rate 1e-5 and batch size two is leveraged for optimisation. In order to make fair comparisons with other approaches, we use the default data splits. Given that some images have intractably large sizes, from each input image, two patches with a size of  are cropped and used for training. Our model and its training are implemented in the PyTorch \cite{PyTorch} 1.10 framework, and the platform for training is a server with an NVIDIA RTX 3090 GPU and Ubuntu 20.04 LTS OS.
 
\subsection{Results}
\label{ssec:results}

\begin{table}[t]
\centering \scriptsize
\renewcommand*{\arraystretch}{1.4}
\caption{Comparision of our model FusionCount with state-of-the-art models of similar sizes. The best and the second best results are indicated in \textbf{bold} and  \underline{underlined} typefaces, respectively.}
\begin{tabular}{ l | c | c | c | c | c }
\toprule
\multirow{2}{*}{\bf Model} & \multirow{2}{*}{\bf Mult-Adds} & \multicolumn{2}{c|}{\bf SH A} & \multicolumn{2}{c}{\bf SH B} \\ \cline{3-6}
& & MAE & RMSE & MAE & RMSE \\
\midrule
CSRNet \cite{CSRNet} & 856.99 G & 68.2 & 115.0 & 10.6 & 16.0 \\
CAN \cite{CAN} & 908.05 G & 62.3 & \underline{100.0} & 7.8 & \underline{12.2} \\
BL \cite{BayesianLoss} & \underline{853.70 G} & 62.8 & 101.8 & 7.7 & 12.7 \\
DM-Count \cite{DMCount} & \underline{853.70 G} & \textbf{59.7} & \textbf{95.7} & \underline{7.4} & \textbf{11.8} \\
\bf FusionCount (ours) & \textbf{815.00 G} & \underline{62.2} & 101.2 & \textbf{6.9} & \textbf{11.8} \\
\bottomrule
\end{tabular}
\label{table:2}
\end{table}

Following prior works \cite{CSRNet, CAN, DMCount}, the mean absolute error (MAE) and the root mean squared error (RMSE) of total counts are employed as evaluation metrics.
FusionCount is compared with state-of-the-art models that have a similar computational complexity (quantified by the number of multiplications and additions involved in the inference on a  RGB image). In particular, CSRNet \cite{CSRNet}, CAN \cite{CAN}, BL \cite{BayesianLoss} and DM-Count \cite{DMCount}, all of which employ VGGs \cite{VGG} as encoders, are encompassed for comparison. CSRNet uses dilated convolution in decoding to extract saliency. In addition to this characteristic, CAN also includes a spatial pyramid pooling block to generate multiscale features. BL and DM-Count have no innovations in terms of model architectures, and their contributions are particularly novel loss functions based on probability theory. Table \ref{table:2} shows detailed performance comparisons. For the case of ShanghaiTech A, partly due to the difficulty in training, FusionCount's results are not the best but still comparable: it achieves the second-lowest mean absolute counting error. On ShanghaiTech B, FusionCount outperforms the other evaluated models under both metrics. These results are remarkable, especially considering our model's low computational complexity.

\begin{figure}[H]
\centerline{GT: 1603; Pred: 1634.79; RE: 1.98\%} \medskip
\vspace{-0.5em}
\begin{minipage}[t]{0.49\linewidth}
    \centering
    \centerline{\includegraphics[width=4.0cm]{images/sha_img_116_orig.jpg}}
\end{minipage}
\hfill
\begin{minipage}[t]{0.49\linewidth}
    \centering
    \centerline{\includegraphics[width=4.0cm]{images/sha_img_116.jpg}}
\end{minipage}
\centerline{GT: 521; Pred: 525.97; RE: 0.95\%} \medskip
\begin{minipage}[t]{0.49\linewidth}
    \centering
    \centerline{\includegraphics[width=4.0cm]{images/sha_img_96_orig.jpg}}
\end{minipage}
\hfill
\begin{minipage}[t]{0.49\linewidth}
    \centering
    \centerline{\includegraphics[width=4.0cm]{images/sha_img_96.jpg}}
\end{minipage}
\centerline{GT: 300; Pred: 302.10; RE: 0.70\%} \medskip
\vspace{-0.5em}
\begin{minipage}[t]{0.49\linewidth}
    \centering
    \centerline{\includegraphics[width=4.0cm]{images/shb_img_21_orig.jpg}}
\end{minipage}
\hfill
\begin{minipage}[t]{0.49\linewidth}
    \centering
    \centerline{\includegraphics[width=4.0cm]{images/shb_img_21.jpg}}
\end{minipage}
\centerline{GT: 176; Pred: 176.08; RE: 0.05\%} \medskip
\begin{minipage}[t]{0.49\linewidth}
    \centering
    \centerline{\includegraphics[width=4.0cm]{images/shb_img_30_orig.jpg}}
\end{minipage}
\hfill
\begin{minipage}[t]{0.49\linewidth}
    \centering
    \centerline{\includegraphics[width=4.0cm]{images/shb_img_30.jpg}}
\end{minipage}
\caption{Qualitative results of FusionCount: Four ground-truth density maps are visualised along with their estimations by our model FusionCount. The top two images are from ShanghaiTech A and the bottom two are from ShanghaiTech B. Ground-truth annotations are marked by \textcolor{NavyBlue}{blue} dots, and the \textcolor{YellowOrange}{orange} shades represent the estimated densities. We also report `GT' i.e., ground-truth counts of people in the images, `Pred' i.e., the predicted counts. `RE' denotes the correpsonding relateive error.}
\label{fig:shab_vis}
\end{figure}

Four instances from the test sets of ShanghaiTech A \& B are depicted in Fig.~\ref{fig:shab_vis}. The left column shows the original input images, and the right column illustrates the ground-truth and the predicted density maps, which are indicated by blue dots and orange shades, respectively. The example in the first row proves that our model can still work well in highly crowded cases. Although in this  image, there are over 1,600 people, our model still achieves an accurate prediction with a relatively small error (1.98\%). Other rows demonstrate that our model is capable of effectively dealing with scale changes. In these images, crowds in the lower parts of the scene have larger scales, while those in the upper part of the scene have the smallest scales. Our model can make correct predictions for both cases with negligible errors (0.95\%, 0.70\% and 0.05\%, respectively). Thus, all these four instances confirm our model's strong counting ability. 
 
\subsection{Effectiveness of Contrast Features}
\label{ssec:contrast_features}

To confirm that the contrast features can boost the model's performance, we create a variant for our model, whose only difference from FusionCount is the feature fusion strategy. In this variant, weights are directly generated from encoded features  instead of contrast features . We train this variant on ShanghaiTech B \cite{MCNN} with the same setting. The MAE and RMSE of this variant are 7.6 and 12.9, respectively, which are larger than those of FusionCount (6.9 and 11.8). 
 
\subsection{Ablation Study}
\label{ssec:ablation_study}

This section proves the effectiveness of the channel reduction module proposed in Section \ref{ssec:decoding}. Experiments are conducted on ShanghaiTech B \cite{MCNN}. The results, which are shown in Table \ref{table:3}, demonstrate that both the point-wise convolutional layer and the two chained dilated convolutional layers are indispensable. Theoretically, they work in a complementary way in reducing the number of channels and extract saliency.

\begin{table}[htbp]
\centering \caption{Effects of point-wise convolution and the number of dilated convolutional layers in the channel reduction module.}
\vspace{1mm}
\renewcommand*{\arraystretch}{1.4}
\begin{tabular}{ c | c | c | c | c | c }
\toprule
\multirow{2}{*}{Point-wise Conv} & \multicolumn{3}{c|}{Dilated Conv No.} & \multirow{2}{*}{MSE} & \multirow{2}{*}{RMSE} \\
\cline{2-4}
              & 2      & 1      & 0      &              &  \\
\cline{1-6}
\cmark        & \cmark &        &        & \textbf{6.9} & \textbf{11.8} \\
\cmark        &        & \cmark &        & 7.6          & 13.0 \\
\cmark        &        &        & \cmark & 8.8          & 15.0 \\
\xmark        & \cmark &        &        & 9.5          & 15.9 \\
\bottomrule
\end{tabular}
\label{table:3}
\end{table}  
\section{Conclusion}
\label{sec:conclusion}

In this paper, we proposed a new crowd counting architecture, FusionCount, which smartly utilises a large majority of features generated during the encoding process to handle perspective distortion. Unlike existing approaches, FusionCount avoids further extraction of multiscale features, thereby significantly reducing overall computation. To this end, We have also improved an existing multiscale fusion mechanism and devised a novel channel reduction block. Experiments on the ShanghaiTech databases demonstrated that FusionCount can outperform relevant state-of-the-art approaches of similar computational complexity. As part of our future work, we are working on accounting for any contextual information in the features fused at the decoding process. Such information can help to more effectively deal with scale changes, as the way the first-stage fusion of encoded features does.
 



\bibliographystyle{IEEEbib}
\bibliography{refs}
\end{document}
