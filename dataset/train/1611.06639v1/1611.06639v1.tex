



\documentclass[11pt]{article}
\usepackage{coling2016}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}

\usepackage{amsmath}
\usepackage{multirow}

\usepackage{graphicx}
\usepackage{subfigure}

\usepackage{amsfonts}



\newcommand\BibTeX{B{\sc ib}\TeX}

\title{Text Classification Improved by Integrating Bidirectional LSTM\\ with Two-dimensional Max Pooling}

\author{Peng Zhou, \ Zhenyu Qi\thanks{Correspondence author: zhenyu.qi@ia.ac.cn}, \ Suncong Zheng, \ Jiaming Xu, \ Hongyun Bao, \ Bo Xu \\
  (1) Institute of Automation, Chinese Academy of Sciences, China \
\begin{bmatrix}
i_t\\f_t\\o_t\\\hat{c}_t
\end{bmatrix}
 = 
\begin{bmatrix}
\sigma\\ \sigma\\ \sigma\\ \tanh
\end{bmatrix}
W\cdot \lbrack h_{t-1}, x_t \rbrack\\

c_t & = & f_t \odot c_{t-1} + i_t \odot \hat{c}_t\\
h_t & = & o_t \odot \tanh(c_t)

h_i=[\overrightarrow{h_i} \oplus \overleftarrow{h_i}]

o_{i, j} = f(\mathbf{m}  \cdot H_{i:i+k-1, \; j:j+d-1} + b)

O = [o_{1,1}, o_{1,2}, \cdots, o_{l-k+1, d^w-d+1}] 

p_{i,j} = down{\left(O_{i:i+p_1, \; j:j+p_2}\right)}

h^* = [p_{1, 1},  p_{1,  1+p_2}, \cdots, p_{1+(l-k+1/p_1-1) \cdot p_1, 1+(d^w-d+1/p_2-1) \cdot p_2}]

\hat{p}\left(y | s\right) & = & softmax \left(W^{\left(s\right)} h^* + b^{\left(s\right)}\right)\\
\hat{y} &  = & \arg \max_y \hat{p}\left(y | s\right)

J\left(\theta\right) = -\frac{1}{m}\sum_{i=1}^{m}t_i\log(y_i)  + \lambda{\Vert\theta\Vert}_F^2

where  is the one-hot represented ground truth,  is the estimated probability for each class by softmax,  is the number of target classes, and  is an L2 regularization hyper-parameter. Training is done through stochastic gradient descent over shuffled mini-batches with the AdaDelta \cite{zeiler2012adadelta} update rule. Training details are further introduced in Section~\ref{hyper-parameter}.







\begin{table*}[!t]
\centering
\begin{tabular}{c||c|c|c|c|c|c|c|c}
\hline
{\bf{Data}} & {c} & {l} & {m} & {train} & {dev} & {test} & {} & {} \\

\hline
SST-1    &5    &18    &51    &8544    &1101    &2210    &17836    &12745\\
SST-2    &2    &19    &51    &6920    &872    &1821    &16185    &11490\\
Subj    &2    &23    &65    &10000    &-    &\textbf{CV}    &21057    &17671\\
TREC    &6    &10    &33    &5452    &-    &500    &9137    &5990\\
MR    &2    &21    &59    &10662    &-    &\textbf{CV}    &20191    &16746\\
20Ng    &4    &276    &11468    &7520    &836    &5563    &51379    &30575\\
\hline
\end{tabular}
\caption{Summary statistics for the datasets. c: number of target classes, l: average sentence length, m: maximum sentence length, train/dev/test: train/development/test set size, : vocabulary size, : number of words present in the set of pre-trained word embeddings, \textbf{CV}: 10-fold cross validation.}\label{tab:result}
\end{table*}

\section{Experimental Setup}
\subsection{Datasets}
The proposed models are tested on six datasets. Summary statistics of the datasets are in Table 1.
\begin{itemize}
\item \textbf{MR}\footnote{https://www.cs.cornell.edu/people/pabo/movie-review-data/}:    Sentence polarity dataset from \newcite{pang2005seeing}. The task is to detect positive/negative reviews.

\item \textbf{SST-1}\footnote{http://nlp.stanford.edu/sentiment/}:   Stanford Sentiment Treebank is an extension of MR from \newcite{socher2013recursive}. The aim is to classify a review as fine-grained labels (very negative, negative, neutral, positive, very positive).

\item \textbf{SST-2}:    Same as SST-1 but with neutral reviews removed and binary labels (negative, positive). For both experiments, phrases and sentences are used to train the model, but only sentences are scored at test time \cite{socher2013recursive,le2014distributed}. Thus the training set is an order of magnitude larger than listed in table 1.

\item \textbf{Subj}\footnote{http://www.cs.cornell.edu/people/pabo/movie-review-data/}:    Subjectivity dataset \cite{pang2004sentimental}. The task is to classify a sentence as being subjective or objective.

\item \textbf{TREC}\footnote{http://cogcomp.cs.illinois.edu/Data/QA/QC/}:    Question classification dataset \cite{li2002learning}. The task involves classifying a question into 6 question types (abbreviation, description, entity, human, location, numeric value).

\item \textbf{20Newsgroups}\footnote{http://web.ist.utl.pt/acardoso/datasets/}:    The 20Ng dataset contains messages from twenty newsgroups. We use the bydate version preprocessed by \newcite{cachopo2007improving}. We select four major categories (comp, politics, rec and religion) followed by \newcite{hingmire2013document}. 
\end{itemize}



\subsection{Word Embeddings}
The word embeddings are pre-trained on much larger unannotated corpora to achieve better generalization given limited amount of training data \cite{turian2010word}. In particular, our experiments utilize the GloVe embeddings\footnote{http://nlp.stanford.edu/projects/glove/} trained by \newcite{pennington2014glove} on 6 billion tokens of Wikipedia 2014 and Gigaword 5. Words not present in the set of pre-trained words are initialized by randomly sampling from uniform distribution in . The word embeddings are fine-tuned during training to improve the performance of classification.

\subsection{Hyper-parameter Settings}
\label{hyper-parameter}
For datasets without a standard development set we randomly select  of the training data as the development set. The evaluation metric of the 20Ng is the Macro-F1 measure followed by the state-of-the-art work and the other five datasets use accuracy as the metric. 
The final hyper-parameters are as follows. 

The dimension of word embeddings is 300, the hidden units of LSTM is 300. We use 100 convolutional filters each for window sizes of (3,3), 2D pooling size of (2,2). We set the mini-batch size as 10 and the learning rate of AdaDelta as the default value 1.0.
For regularization, we employ Dropout operation \cite{hinton2012improving} with dropout rate of 0.5 for the word embeddings, 0.2 for the  BLSTM layer and 0.4 for the penultimate layer, we also use l2 penalty with coefficient  over the parameters.


These values are chosen via a grid search on the SST-1 development set. We only tune these hyper-parameters, and more finer tuning, such as using different numbers of hidden units of LSTM layer, or using wide convolution \cite{kalchbrenner2014convolutional}, may further improve the performance.



\begin{table*}[!t]
\centering
\begin{tabular}{c||l|c|c|c|c|c|c}
\hline
{\bf{NN}} & {\bf{Model}} & {\bf{SST-1}} & {\bf{SST-2}} & {\bf{Subj}} & {\bf{TREC}} & {\bf{MR}} & {\bf{20Ng}} \\
\hline
\multirow{2}{*}{ReNN}
&RNTN \cite{socher2013recursive}    &45.7    &85.4    &-    &-    &-    &-\\
&DRNN \cite{irsoy2014deep}    &49.8    &86.6    &-    &-    &-    &-\\
\hline
\multirow{8}{*}{CNN}
&DCNN \cite{kalchbrenner2014convolutional}    &48.5    &86.8    &-    &93.0    &-    &-\\
&CNN-non-static \cite{kim2014convolutional}    &48.0    &87.2    &93.4    &93.6    &-    &-\\
&CNN-MC \cite{kim2014convolutional}    &47.4    &88.1    &93.2    &92    &-    &-\\
&TBCNN\cite{mou2015discriminative}    &51.4    &87.9    &-    &96.0    &-    &-\\
&Molding-CNN \cite{lei2015molding} &51.2   &88.6    &-    &-    &-   &-\\
&CNN-Ana \cite{zhang2015sensitivity}    &45.98    &85.45    &93.66    &91.37    &81.02    &-\\
&MVCNN \cite{yin2016multichannel}    &49.6    &89.4    &93.9    &-    &-   &-\\  \hline
\multirow{7}{*}{RNN}
&RCNN \cite{lai2015recurrent}     &47.21    &-    &-    &-    &-   &96.49\\
&S-LSTM \cite{zhu2015long}    &-    &81.9    &-    &-    &-   &-\\
&LSTM \cite{tai2015improved}    &46.4    &84.9    &-    &-    &-   &-\\
&BLSTM \cite{tai2015improved}     &49.1    &87.5     &-    &-    &-   &-\\
&Tree-LSTM \cite{tai2015improved}    &51.0    &88.0    &-    &-    &-   &-\\
&LSTMN \cite{cheng2016long}    &49.3    &87.3    &-    &-    &-   &-\\
&Multi-Task \cite{liu2016recurrent}    &49.6    &87.9    &94.1    &-    &-   &-\\
\hline
\multirow{7}{*}{Other}
&PV \cite{le2014distributed}    &48.7    &87.8    &-    &-    &-   &-\\
&DAN \cite{iyyer2015deep}    &48.2    &86.8    &-    &-    &-   &-\\
&combine-skip \cite{kiros2015skip}    &-    &-    &93.6    &92.2    &76.5    &-\\
&AdaSent \cite{zhao2015self}    &-    &-    &\textbf{95.5}    &92.4    &\textbf{83.1}    &-\\
&LSTM-RNN \cite{le2015compositional}    &49.9    &88.0    &-    &-    &-   &-\\
&C-LSTM \cite{zhou2015c}    &49.2    &87.8    &-    &94.6    &-   &-\\
&DSCNN \cite{zhang2016dependency}    &49.7    &89.1    &93.2    &95.4    &81.5    &-\\
\hline
\multirow{4}{*}{ours}
&BLSTM    &49.1    &87.6    &92.1    &93.0    &80.0    &94.0\\
&BLSTM-Att    &49.8    &88.2    &93.5    &93.8    &81.0    &94.6\\
&BLSTM-2DPooling    &50.5    &88.3    &93.7    &94.8    &81.5    &95.5\\
&BLSTM-2DCNN    &\textbf{52.4}    &\textbf{89.5}    &94.0    &\textbf{96.1}   &82.3     &\textbf{96.5}\\
\hline



\end{tabular}
\caption{Classification results on several standard benchmarks. \textbf{RNTN}: Recursive deep models for semantic compositionality over a sentiment treebank \protect\cite{socher2013recursive}. \textbf{DRNN}: Deep recursive neural networks for compositionality in language \protect\cite{irsoy2014deep}. \textbf{DCNN}: A convolutional neural network for modeling sentences \protect\cite{kalchbrenner2014convolutional}. \textbf{CNN-nonstatic/MC}: Convolutional neural networks for sentence classification \protect\cite{kim2014convolutional}. \textbf{TBCNN}: Discriminative neural sentence modeling by tree-based convolution \protect\cite{mou2015discriminative}. \textbf{Molding-CNN}: Molding CNNs for text: non-linear, non-consecutive convolutions \protect\cite{lei2015molding}. \textbf{CNN-Ana}: A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional Neural Networks for Sentence Classification \protect\cite{zhang2015sensitivity}. \textbf{MVCNN}: Multichannel variable-size convolution for sentence classification \protect\cite{yin2016multichannel}. \textbf{RCNN}: Recurrent Convolutional Neural Networks for Text Classification \protect\cite{lai2015recurrent}. \textbf{S-LSTM}: Long short-term memory over recursive structures \protect\cite{zhu2015long}. \textbf{LSTM/BLSTM/Tree-LSTM}: Improved semantic representations from tree-structured long short-term memory networks \protect\cite{tai2015improved}. \textbf{LSTMN}: Long short-term memory-networks for machine reading \protect\cite{cheng2016long}. \textbf{Multi-Task}: Recurrent Neural Network for Text Classification with Multi-Task Learning \protect\cite{liu2016recurrent}. \textbf{PV}: Distributed representations of sentences and documents \protect\cite{le2014distributed}. \textbf{DAN}: Deep unordered composition rivals syntactic methods for text classification \protect\cite{iyyer2015deep}. \textbf{combine-skip}: skip-thought vectors \protect\cite{kiros2015skip}. \textbf{AdaSent}: Self-adaptive hierarchical sentence model \protect\cite{zhao2015self}. \textbf{LSTM-RNN}: Compositional distributional semantics with long short term memory \protect\cite{le2015compositional}. \textbf{C-LSTM}: A C-LSTM Neural Network for Text Classification \protect\cite{zhou2015c}. \textbf{DSCNN}: Dependency Sensitive Convolutional Neural Networks for Modeling Sentences and Documents \protect\cite{zhang2016dependency}.} \end{table*}





\section{Results}
\subsection{Overall Performance}
This work implements four models, BLSTM, BLSTM-Att, BLSTM-2DPooling, and BLSTM-2DCNN. Table 2 presents the performance of the four models and other state-of-the-art models on six classification tasks. The BLSTM-2DCNN model achieves excellent performance on 4 out of 6 tasks. Especially, it achieves  and  test accuracies on SST-1 and SST-2 respectively.




BLSTM-2DPooling performs worse than the state-of-the-art models. While we expect performance gains through the use of 2D convolution, we are surprised at the magnitude of the gains. BLSTM-CNN beats all baselines on SST-1, SST-2, and TREC datasets. As for Subj and MR datasets, BLSTM-2DCNN gets a second higher accuracies. 
Some of the previous techniques only work on sentences, but not paragraphs/documents with several sentences. Our question becomes whether it is possible to use our models for datasets that have a substantial number of words, such as 20Ng and where the content consists of many different topics. For that purpose, this paper tests the four models on document-level dataset 20Ng, by treating the document as a long sentence. Compared with RCNN \cite{lai2015recurrent}, BLSTM-2DCNN achieves a comparable result.

Besides, this paper also compares with ReNN, RNN, CNN and other neural networks:
\begin{itemize}
\item{Compared with ReNN, the proposed two models do not depend on  external language-specific features such as dependency parse trees.}

\item{CNN extracts features from word embeddings of the input text, while BLSTM-2DPooling and BLSTM-2DCNN captures features from the output of BLSTM layer, which has already extracted features from the original input text.}

\item{BLSTM-2DCNN is an extension of BLSTM-2DPooling, and the results show that BLSTM-2DCNN can capture more dependencies in text.}

\item{AdaSent utilizes a more complicated model to form a hierarchy of representations, and it outperforms BLSTM-2DCNN on Subj and MR datasets. Compared with DSCNN \cite{zhang2016dependency}, BLSTM-2DCNN outperforms it on five datasets.}
\end{itemize}
 

Compared with these results, 2D convolution and 2D max pooling operation are more effective for modeling sentence, even document. To better understand the effect of 2D operations, this work conducts a sensitivity analysis on SST-1 dataset.



\begin{figure}
\begin{tabular}{lr}
\begin{minipage}[t]{0.45\linewidth}
\centering
\includegraphics[scale=0.4]{r1.eps}  \caption{Fine-grained sentiment classification accuracy  sentence length.}
\label{pool}
\end{minipage}
\hspace{0.2in}
\begin{minipage}[t]{0.45\linewidth}
\centering
\includegraphics[scale=0.4]{pooling_final1.eps}  \caption{Prediction accuracy with different size of 2D filter and 2D max pooling.}
\label{pool}
\end{minipage}
\end{tabular}
\end{figure}





\subsection{Effect of Sentence Length}
Figure 2 depicts the performance of the four models on different length of sentences. In the figure, the x-axis represents sentence lengths and y-axis is accuracy. The sentences collected in test set are no longer than 45 words. The accuracy here is the average value of the sentences with length in the window . Each data point is a mean score over 5 runs, and error bars have been omitted for clarity.

It is found that both BLSTM-2DPooling and BLSTM-2DCNN outperform the other two models. This suggests that both 2D convolution and 2D max pooling operation are able to encode semantically-useful structural information. At the same time, it shows that the accuracies decline with the length of sentences increasing. In future work, we would like to investigate neural mechanisms to preserve long-term dependencies of text.





\subsection{Effect of 2D Convolutional Filter and 2D Max Pooling Size}
We are interested in what is the best 2D filter and max pooling size to get better performance. We conduct experiments on SST-1 dataset with BLSTM-2DCNN and set the number of feature maps to 100.





To make it simple, we set these two dimensions to the same values, thus both the filter and the pooling are square matrices. For the horizontal axis, c means 2D convolutional filter size, and the five different color bar charts on each c represent different 2D max pooling size from 2 to 6. Figure 3 shows that different size of filter and pooling can get different accuracies. 
The best accuracy is 52.6 with 2D filter size (5,5) and 2D max pooling size (5,5), this shows that finer tuning can further improve the performance reported here. And if a larger filter is used, the convolution can detector more features, and the performance may be improved, too. However, the networks will take up more storage space, and consume more time. 



\section{Conclusion}
This paper introduces two combination models, one is BLSTM-2DPooling, the other is BLSTM-2DCNN, which can be seen as an extension of BLSTM-2DPooling. Both models can hold not only the time-step dimension but also the feature vector dimension information. The experiments are conducted on six text classificaion tasks. The experiments results demonstrate that BLSTM-2DCNN not only outperforms RecNN, RNN and CNN models, but also works better than the BLSTM-2DPooling and DSCNN \cite{zhang2016dependency}. Especially, BLSTM-2DCNN achieves highest accuracy on SST-1 and SST-2 datasets. To better understand the effective of the proposed two models, this work also conducts a sensitivity analysis on SST-1 dataset. It is found that large filter can detector more features, and this may lead to performance improvement. 





\section*{Acknowledgements}
We thank anonymous reviewers for their constructive comments. This research was funded by the National High Technology Research and Development Program of China (No.2015AA015402), and the National Natural Science Foundation of China (No. 61602479), and the Strategic Priority Research Program of the Chinese Academy of Sciences (Grant No. XDB02070005).




\bibliographystyle{acl}
\bibliography{coling2016}

\end{document}
