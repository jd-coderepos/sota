\section{Experiments}\label{Se:experiments}
We evaluate two sets of image captions.
The first set consists of the fused captions from the \fusecap{} dataset, described in \Cref{Se:method}. The second set includes captions generated by the trained captioning model discussed in \Cref{subsec:stage2}. 
The objective for both sets is to produce captions that are descriptive and accurate.
Traditionally, image captioning methods are assessed with n-gram-based metrics like BLEU \cite{papineni2002bleu}, CIDEr \cite{vedantam2015cider}, and ROUGE \cite{lin2004rouge}. 
These metrics compare the tested captions to reference captions, assuming the latter represents the ideal image descriptions that the tested captions aim for.
However, as highlighted earlier, captions in existing datasets often fail to provide a comprehensive description of images.
This implies that given our emphasis on enhancing descriptiveness, relying on broad and non-specific reference captions for evaluation would be inappropriate.
Consequently, n-gram-based metrics do not effectively measure or promote this quality \cite{zhu2023chatgpt}. 

We, therefore adopt CLIPScore~\cite{hessel2022clipscore}, a reference-free metric that measures the alignment between textual and visual embeddings generated by a pre-trained CLIP model \cite{radford2021learning}.
Since CLIPScore is not dependent on a reference caption, its score is not restricted by the descriptiveness of the original captions.
Moreover, in terms of accuracy, it has been shown to exhibit a higher correlation with human judgments than n-gram-based metrics \cite{hessel2022clipscore}.


Besides evaluating enriched captions with CLIPScore, we consider the ability to perform image-text retrieval as a pertinent metric for caption evaluation.
Comprehensive captions should inherently serve as distinct image descriptors, thereby improving retrieval precision.
To further assess the quality of captions in the \fusecap{} dataset, we conducted a human evaluation study.


\subsection{FuseCap Dataset}
\label{Se:data-validation}
In this subsection, we evaluate the enriched captions produced by the caption augmentation approach introduced in the \Cref{Se:method}.
In particular, we carry out a qualitative human-study alongside quantitative evaluations.
Examples of enriched captions via FuseCap are showcased in \Cref{fig:generatior_example2}.
\vspace{-20pt}



\begin{table}[b]
  \centering
  \vspace{-9pt}
  \begin{tabular}{clccccc}
  \toprule
    Dataset & Captions & Mean & Voting  \\
    \midrule
    \multirow{2}{*}{COCO} & Original & 76.7 & 31.7\%\\
& \fusecap{} & \textbf{80.3} & \textbf{67.6\%} \\
    \midrule
    \multirow{2}{*}{SBU} & Original & 71.9 & 32.1\%\\
& \fusecap{} & \textbf{75.5} & \textbf{60.2\%} \\
    \midrule
    \multirow{2}{*}{CC} & Original & 72.6 & 34.7\%\\
& \fusecap{} & \textbf{75.4} & \textbf{59.7\%} \\
    \bottomrule
  \end{tabular}
\caption{\textbf{\fusecap{} data quantitative evaluation.} CLIPScore-based comparison between the original captions of common image-text datasets with our enriched ones. ``Mean'' indicated the mean CLIPScore and ``Voting'' expresses CLIP's preference in a one-vs-one setting. As can be seen, \fusecap{} obtains significantly improved results in both metrics.}
    \label{tab:caption_clips}
\end{table}





\paragraph{Qualitative Evaluation}
We conducted a thorough human evaluation study to assess the ability of our enriched captions to be both descriptive and relevant to the images. Specifically, we randomly sampled 400 pairs from the COCO dataset 
and provided 40 participants with (1) the original caption and (2) the enriched caption.
Our study engaged a pool of random internet users as participants. To minimize biases and ensure an impartial evaluation, they completed the survey unaware of the specific research objectives or goals.
The evaluators were asked the following:
\textit{``Does caption 2 provide an additional meaningful and truthful description of the image compared to caption 1?''}.
As the enriched captions are much more detailed, as can be seen in \Cref{fig:generatior_example2}, the question focuses on whether they are accurate.
Our study results indicate that participants find the enriched captions at least as good as the original ones in $72.9\%$ of the images. This finding highlights the proposed method's effectiveness in enhancing the captions' descriptiveness while preserving alignment and relevancy to the images.

\vspace{-10pt}
\paragraph{Quantitative Evaluation}
We evaluate the enriched captions in comparison to the original captions for each dataset under consideration. To this end, we randomly selected 5000 images from each dataset and report the CLIPScore obtained with both types of captions.
As depicted in \Cref{tab:caption_clips}, the enriched captions generated by our proposed method consistently achieve a higher CLIPScore (Mean) on average by $4.6\%$.
In addition, given an image and two captions (original and enriched), we utilize CLIP to measure which caption is preferred. We evaluate this on the different datasets and summarize it under ``Voting'', which our captions outperform current ones on average by $29.7\%$.
These results demonstrate the effectiveness of our approach in generating enriched captions that better reflect the content of the images.
\vspace{-20pt}

\paragraph{Image-Text Retrieval}\label{Se:image-text-retrieval}
To further demonstrate the descriptiveness and accuracy of the \fusecap{} data, we assess its performance in the image-text retrieval task.
This task involves matching images to text queries and vice versa.
If the enriched data is descriptive and accurate, retrieval performance should improve since the additional details can serve as a discriminative factor in establishing these correspondences.
Our training methodology aligns with the original BLIP model.
After pre-training the model on a large-scale dataset as in \Cref{subsec:stage2}, the model is fine-tuned for image-text retrieval on the COCO training set using both ITC and ITM losses.
For the inference step, we employ the method proposed by \cite{li2021align}, previously integrated into BLIP.
This method involves the selection of $K$ candidates based on feature similarity, followed by their re-ranking using respective ITM values.
We report $R@N$ that corresponds to the accuracy of retrieving the true text/image among the top $N$ retrieved results.
The details of the competing models are provided in \Cref{Se:caption-generation}, with the exception that the versions discussed here have been fine-tuned for the retrieval tasks.
As illustrated in \Cref{coco_retrieval}, the use of fused captions contributes significantly to the enhancement of retrieval performance on the COCO test set\footnote{While we used enriched captions for training and testing BLIP$_\fusecap{}$, for the other baselines we used the original dataset without enrichment.}.
For example, compared to the model trained on corresponding non-enriched data, the $R@1$ score for image-to-text retrieval increased by $22.1\%$, and for text-to-image retrieval, it increased by $34.8\%$.


\begin{table}
  \centering
\begin{tabular}{l *{3}{c@{\hspace{5pt}}} | *{3}{c@{\hspace{5pt}}}}
    \toprule
    & \multicolumn{6}{c}{COCO Retrieval} \\
    & \multicolumn{3}{c}{img $\rightarrow$ text} & \multicolumn{3}{c}{text $\rightarrow$ img} \\
    \cmidrule(lr){2-4} \cmidrule(lr){5-7}
    Model & 
    R\kern-0.1em@\kern-0.1em1 & R\kern-0.1em@\kern-0.1em5 & R\kern-0.1em@\kern-0.1em10 & R\kern-0.1em@\kern-0.1em1 & R\kern-0.1em@\kern-0.1em5 & R\kern-0.1em@\kern-0.1em10 \\
    \midrule
    BLIP$\dag$ & 75.1 & 92.7 & 96.4 & 58.2 & 82.4 & 89.2 \\
    BLIP-L & 82.4 & 95.4 & 97.9 & 65.2 & 86.3 & 91.8 \\
    BLIP2 & 85.4 & 97.0 & 98.5 & 68.3 & 87.7 & 92.6 \\
    \midrule
    BLIP$^*_\fusecap{}$ & \textbf{97.2} & \textbf{99.5} & \textbf{99.9} & \textbf{93.0} & \textbf{97.4} & \textbf{98.3} \\
    \bottomrule
\end{tabular}
\vspace{7pt}
  \caption{\textbf{Image-text retrieval results.} Performance on COCO retrieval (test sets). The ``*'' symbol indicates that the model was trained and tested on our enriched dataset. These results attest that given rich captions, BLIP$_\fusecap{}$ significantly outperforms existing methods, which utilize standard captions.
}
  \label{coco_retrieval}
  \vspace{-0.3cm}
\end{table}





