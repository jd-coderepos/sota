\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\graphicspath{{Figures/}}
\usepackage{amsmath, amssymb}
\usepackage{relsize}
\usepackage{amssymb}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{blindtext}
\usepackage{booktabs}
\usepackage{bm}
\usepackage{mmstyles}
\usepackage[section]{placeins}
\usepackage[numbers]{natbib}
\usepackage{diagbox}
\usepackage{caption}
\usepackage{float}
\usepackage{enumitem}
\usepackage{tabularx}
\usepackage{makecell}
\newcolumntype{Y}{>{\centering\arraybackslash}X}

\usepackage{cuted}
\usepackage{capt-of}
\usepackage{flushend}

\captionsetup[table]{skip=2pt}
\captionsetup[figure]{skip=2pt}
\renewcommand{\arraystretch}{1.1}

\newcolumntype{L}{>{\centering\arraybackslash}X}
\newcolumntype{C}{>{\centering\arraybackslash}X}

\newcommand{\vect}[1]{\boldsymbol{#1}}
\newcommand{\ignore}[1]{}



\usepackage{xcolor}
\definecolor{todo}{rgb}{1,.5,0} \newcommand{\todo}[1]{\textcolor{todo}{(note: #1)}}

\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}
\usepackage[british,UKenglish,USenglish,american]{babel}

\cvprfinalcopy 

\def\cvprPaperID{3032} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ifcvprfinal\pagestyle{empty}\fi

\begin{document}

\newenvironment{absolutelynopagebreak}
{\par\nobreak\vfil\penalty0\vfilneg
	\vtop\bgroup}
{\par\xdef\tpd{\the\prevdepth}\egroup
	\prevdepth=\tpd}

\title{Cross-domain Correspondence Learning for Exemplar-based Image Translation}
\author{Pan Zhang
	\thanks{Author did this work during the internship at Microsoft Research Asia.}
	, Bo Zhang, Dong Chen, Lu Yuan, Fang Wen
	\\
	University of Science and Technology of China \quad
	Microsoft Research Asia \quad  Microsoft Cloud+AI
}


\maketitle
\thispagestyle{empty}

\maketitle
%
 \begin{abstract}
	\vspace{-1em}
We present a general framework for exemplar-based image translation, which synthesizes a photo-realistic image from the input in a distinct domain (\eg, semantic segmentation mask, or edge map, or pose keypoints), given an exemplar image. The output has the style (\eg, color, texture) in consistency with the semantically corresponding objects in the exemplar.
We propose to jointly learn the cross-domain correspondence and the image translation, where both tasks facilitate each other and thus can be learned with weak supervision.
    The images from distinct domains are first aligned to an intermediate domain where dense correspondence is established. Then, the network synthesizes images based on the appearance of semantically corresponding patches in the exemplar.
    We demonstrate the effectiveness of our approach in several image translation tasks. Our method is superior to state-of-the-art methods in terms of image quality significantly, with the image style faithful to the exemplar with semantic consistency. Moreover, we show the utility of our method for several  applications.
    
\end{abstract}
\section{Introduction}
\label{sec:introduction}

\begin{figure}[t!]
\centering
\small
\setlength\tabcolsep{0pt}
\includegraphics[width=1.0\columnwidth]{Figures/teaser_1116.pdf}
\caption{\textbf{Exemplar-based image synthesis.} Given the exemplar images (1st row), our network translates the inputs, in the form of segmentation mask, edge and pose, to photo-realistic images (2nd row). Please refer to \emph{supplementary material} for more results.}
\label{fig:teaser1}
\end{figure}







Conditional image synthesis aims to generate photo-realistic images based on certain input data~\cite{isola2017image,wang2018high,zhu2017unpaired,chen2017photographic}. We are interested in a specific form of conditional image synthesis, which converts a semantic segmentation mask, an edge map, and pose keypoints to a photo-realistic image, given an exemplar image, as shown in Figure~\ref{fig:teaser1}. We refer to this form as \emph{exemplar-based image translation}. It allows more flexible control for multi-modal generation according to a user-given exemplar.


Recent methods directly learn the mapping from a semantic segmentation mask to an exemplar image using neural networks~\cite{huang2018multimodal,park2019semantic,ma2018exemplar,wang2019example}. Most of these methods encode the style of the exemplar into a latent style vector, from which the network synthesizes images with the desired style similar to the examplar. However, the style code only characterizes the global style of the exemplar, regardless of spatial relevant information. Thus, it causes some local style ``wash away" in the ultimate image.

To address this issue, the \emph{cross-domain correspondence} between the input and the exemplar has to be established before image translation. As an extension of Image Analogies~\cite{hertzmann2001image}, Deep Analogy~\cite{liao2017visual} attempts to find a dense semantically-meaningful correspondence between the image pair. It leverages deep features of VGG pretrained on real image classification tasks for matching. We argue such representation may fail to handle a more challenging mapping from mask (or edge, keypoints) to photo since the pretrained network does not recognize such images. In order to consider the mask (or edge) in the training, some methods~\cite{gu2019mask,yi2019apdrawinggan,chang2018pairedcyclegan} explicitly separate the exemplar image into semantic regions and learns to synthesize different parts individually. In this way, it successfully generates high-quality results. However, these approaches are task specific, and are unsuitable for general translation.

How to find a more general solution for \emph{exemplar-based image translation} is non-trivial. We aim to learn the dense semantic correspondence for cross-domain images (\eg, mask-to-image, edge-to-image, keypoints-to-image, etc.), and then use it to guide the image translation. It is weakly supervise learning, since we have neither the correspondence annotations nor the synthesis ground truth given a random exemplar.

In this paper, we propose a \emph{{C}r{O}ss-domain {C}{O}rre{S}pondence network} (\emph{CoCosNet}) that learns cross-domain correspondence and image translation simultaneously. The network architecture comprises two sub-networks: 1) \emph{Cross-domain correspondence Network} transforms the inputs from distinct domains to an intermediate feature domain where reliable dense correspondence can be established; 2) \emph{Translation network}, employs a set of spatially-variant de-normalization blocks~\cite{park2019semantic} to progressively synthesizes the output, using the style details from a warped exemplar which is semantically aligned to the mask (or edge, keypoints map) according to the estimated correspondence. Two sub-networks facilitate each other and are learned end-to-end with novel loss functions. Our method outperforms previous methods in terms of image quality by a large margin, with instance-level appearance being faithful to the exemplar. Moreover, the cross-domain correspondence implicitly learned enables some intriguing applications, such as image editing and makeup transfer. Our contribution can be summarized as follows:
\begin{itemize}[leftmargin=*]
\itemsep0em
\item We address the problem of learning dense cross-domain correspondence with weak supervision---joint learning with image translation.
\item With the cross-domain correspondence, we present a general solution to exemplar-based image translation, that for the first time, outputs images resembling the fine structures of the exemplar at instance level.
\item Our method outperforms state-of-the-art methods in terms of image quality by a large margin in various application tasks.
\end{itemize}



 \section{Related Work}
\label{sec:related_work}

\noindent\textbf{Image-to-image translation}~~
The goal of image translation is to learn the mapping between different image domains. Most prominent contemporary approaches solve this problem through conditional generative adversarial network~\cite{mirza2014conditional} that leverages either paired data~\cite{isola2017image,wang2018high,park2019semantic} or unpaired data~\cite{zhu2017unpaired,yi2017dualgan,kim2017learning,liu2017unsupervised,royer2017xgan}. Since the mapping from one image domain to another is inherently multi-modal, following works promote the synthesis diversity by performing stochastic sampling from the latent space~\cite{zhu2017toward,huang2018multimodal,lee2018diverse}. However, none of these methods allow delicate control of the output since the latent representation is rather complex and does not have an explicit correspondence to image style. In contrast, our method supports customization of the result according to a user-given exemplar, which allows more flexible control for multi-modal generation.

\noindent\textbf{Exemplar-based image synthesis}~~
Very recently, a few works~\cite{qi2018semi,wang2019example,ma2018exemplar,riviere2019inspirational,bansal2019shapes} propose to synthesize photorealistic images from semantic layout under the guidance of exemplars. Non-parametric or semi-parametric approaches~\cite{qi2018semi,bansal2019shapes} synthesize images by compositing the image fragments retrieved from a large database. 
Mainstream works, however, formulate the  problem as image-to-image translation. \citet{huang2018multimodal} and \citet{ma2018exemplar} propose to employ Adaptive Instance Normalization (AdaIN)~\cite{huang2017arbitrary} to transfer the style code from the exemplar to the source image. \citet{park2019semantic} learn an encoder to map the exemplar image into a vector from which the images are further synthesized. The style consistency discriminator is proposed in~\cite{wang2019example} to examine whether the image pairs exhibit a similar style. However, this method requires to constitute style consistency image pairs from video clips, which makes it unsuitable for general image translation. Unlike all of the above methods that only transfer the global style, our method transfers the fine style from a semantically corresponding region of the exemplar. Our work is inspired by the recent exemplar-based image colorization~\cite{zhang2019deep,he2018deep}, but we solve a more general problem: translating images between distinct domains.

\noindent\textbf{Semantic correspondence}~~
Early studies~\cite{lowe2004distinctive,dalal2005histograms,tola2009daisy} on semantic correspondence focus on matching hand-crafted features. With the advent of the convolutional neural network, deep features are proven powerful to represent the high-level semantics. \citet{long2014convnets} first propose to establish semantic correspondence by matching deep features extracted from a pretrained classification model. Following works further improve the correspondence quality by incorporating additional annotations~\cite{zhou2016learning,choy2016universal,ham2017proposal,han2017scnet,kim2017fcss,lee2019sfnet}, adopting coarse-to-fine strategy~\cite{liao2017visual} or retaining reliable sparse matchings~\cite{aberman2018neural}.
However, all these methods can only handle the correspondence between natural images instead of cross-domain images, \eg, edge and photorealistic images. We explore this new scenario and implicitly learns the task with weak supervision.


\begin{figure*}[t!]
	\centering
	\includegraphics[width=1.0\linewidth]{diagram3.pdf}
	\caption{\textbf{The illustration of the \emph{CoCosNet} architecture.} Given the input  and the exemplar , the correspondence submodule adapts them into the same domain , where dense correspondence can be established. Then, the translation network generates the final output based on the warped exemplar  according to the correspondence, yielding an exemplar-based translation output.}
	\label{fig:overview}
\end{figure*} \section{Approach}
\label{sec:method}

We aim to learn the translation from the source domain  to the target domain  given an input image  and an exemplar image . The generated output is desired to conform to the content as  while resembling the style from semantically similar parts in . For this purpose, the correspondence between  and , which lie in different domains, is first established, and the exemplar image is warped accordingly so that its semantics is aligned with  (Section~\ref{sec:cross_domain}). Thereafter, an image is synthesized according to the warped exemplar (Section~\ref{sec:translation}). The whole network architecture is illustrated in Figure~\ref{fig:overview}, by the example of mask to image synthesis.

\subsection{Cross-domain correspondence network}
\label{sec:cross_domain}
Usually the semantic correspondence is found by matching patches~\cite{liao2017visual,lee2019sfnet} in the feature domain with a pre-trained classification model. However, pre-trained models are typically trained on a specific type of images, \eg, natural images, so the extracted features cannot generalize to depict the semantics for another domain. Hence, prior works cannot establish the correspondence between heterogeneous images, \eg, edge and photo-realistic images. To tackle this, we propose a novel cross-domain correspondence network, mapping the input domains to a shared domain  in which the representation is capable to represent the semantics for both input domains. As a result, reliable semantic correspondence can be found within domain .

\vspace{0.4em}
\noindent\textbf{Domain alignment} As shown in Figure~\ref{fig:overview}, we first adapt the input image and the exemplar to a shared domain . To be specific,  and  are fed into the feature pyramid network that extracts multi-scale deep features by leveraging both local and global image context~\cite{ronneberger2015u,lin2017feature}. The extracted feature maps are further transformed to the representations in , denoted by  and  respectively (, are feature spatial size;  is the channel-wise dimension). Let  and  be the domain transformation from the two input domains respectively, so the adapted representation can be formulated as,

where  denotes the learnable parameter. The representation  and  comprise discriminative features that characterize the semantics of inputs. Domain alignment is, in practice, essential for correspondence in that only when  and  reside in the same domain can they be further matched with some similarity measure. 
 
\vspace{0.4em}
\noindent\textbf{Correspondence within shared domain}
We propose to match the features of  and  with the correspondence layer proposed in~\cite{zhang2019deep}. Concretely, we compute a correlation matrix  of which each element is a pairwise feature correlation,

where  and  represent the channel-wise centralized feature of  and  in position  and , \ie,  and .
 indicates a higher semantic similarity between  and . 

Now the challenge is how to learn the correspondence without direct supervision. Our idea is to jointly train with image translation. The translation network may find it easier to generate high-quality outputs only by referring to the correct corresponding regions in the exemplar, which implicitly pushes the network to learn the accurate correspondence. In light of this, we warp  according to  and obtain the warped exemplar . Specifically, we obtain  by selecting the most correlated pixels in  and calculating their weighted average,

Here,  is the coefficient that controls the sharpness of the softmax
and we set its default value as . In the following, images will be synthesized conditioned on  and the correspondence network, in this way, learns its assignment with indirect supervision.

\subsection{Translation network}
\label{sec:translation}
Under the guidance of , the translation network  transforms the constant code  to the desired output . In order to preserve the structural information of , we employ the spatially-adaptive denormalization (SPADE) block~\cite{park2019semantic} to project the spatially variant exemplar style to different activation locations. As shown in Figure~\ref{fig:overview}, the translation network has  layers with the exemplar style progressively injected. As opposed to~\cite{park2019semantic} which computes layer-wise statistics for batch normalization (BN), we empirically find the normalization that computes the statistics at each spatial position, the positional normalization (PN)~\cite{2019arXiv190704312L}, better preserves the structure information synthesized in prior layers. Hence, we propose to marry positional normalization and spatially-variant denormalization for high-fidelity texture transfer from the exemplar. 

Formally, given the activation  before the  normalization layer, we inject the exemplar style through,

where the statistic value  and  are calculated exclusively across channel direction compared to BN. The denormalization parameter  and  characterize the style of the exemplar, which is mapped from  with the projection  parameterized by , \ie,

We use two plain convolutional layers to implement  so  and  have the same spatial size as . With the style modulation for each normalization layer, the overall image translation can be formulated as

where  denotes the learnable parameter.

\subsection{Losses for exemplar-based translation}
We jointly train the cross-domain correspondence along with image synthesis with following loss functions, hoping the two tasks benefit each other. 


\vspace{0.4em}
\noindent\textbf{Losses for pseudo exemplar pairs}~~We construct exemplar training pairs by utilizing paired data  that are semantically aligned but differ in domains.
Specifically, we apply random geometric distortion to  and get the distorted image , where  denotes the augmentation operation like image warping or random flip. When  is regarded as the exemplar, the translation of  is expected to be its counterpart . In this way, we obtain pseudo exemplar pairs. We propose to penalize the difference between the translation output and the ground truth  by minimizing the \emph{feature matching loss}~\cite{johnson2016perceptual,isola2017image,chen2017photographic}

where  represents the activation of layer  in the pre-trained VGG-19 model and  balance the terms.

\vspace{0.4em}
\noindent\textbf{Domain alignment loss}~We need to make sure the transformed embedding  and  lie in the same domain. To achieve this, we once again make use of the image pair , whose feature embedding should be aligned exactly after domain transformation: 

Note that we perform channel-wise normalization as the last layer of  and  so minimizing this domain discrepancy will not lead to a trivial solution (\ie, small magnitude of activations).  


\vspace{0.4em}
\noindent\textbf{Exemplar translation losses}~~The learning with pair or pseudo exemplar pair is hard to generalize to general cases where the semantic layout of exemplar differs significantly from the source image. To tackle this, we propose the following losses.

First, the ultimate output should be consistent with the semantics of the input , or its counterpart . We thereby penalize the \emph{perceptual loss} to minimize the semantic discrepancy:

Here we choose  to be the activation after 4\_2 layer in the VGG-19 network since this layer mainly contains high-level semantics. 

On the other hand, we need a loss function that encourages  to adopt the appearance from the semantically corresponding patches from . To this end, we employ the \emph{contextual loss} proposed in ~\cite{mechrez2018contextual} to match the statistics between  and , which is

where  and  index the feature map of layer  that contains  features, and  controls the relative importance of different layers. Still, we rely on pretrained VGG features. As opposed to  which mainly utilizes high-level features, the contextual loss uses 2\_2 up to 5\_2 layers since low-level features capture richer style information (\eg, color or textures) useful for transferring the exemplar appearance.

\vspace{0.4em}
\noindent\textbf{Correspondence regularization}~~Besides, the learned correspondence should be cycle consistent, \ie, the image should match itself after forward-backward warping, which is  
where  is the forward-backward warping image. Indeed, this objective function is crucial because the rest loss functions, imposed at the end of the network, are weak supervision and cannot guarantee that the network learns a meaningful correspondence. Figure~\ref{fig:ablation} shows that without  the network fails to learn the cross-domain correspondence correctly although it is still capable to generate plausible translation result. The regularization  enforces the warped image  remain in domain  by constraining its backward warping, implicitly encouraging the correspondence to be meaningful as desired. 

\vspace{0.4em}
\noindent\textbf{Adversarial loss}~~We train a discriminator~\cite{goodfellow2014generative} that discriminates the translation outputs and the real samples of domain . Both the discriminator  and the translation network  are trained alternatively until synthesized images look indistinguishable to real ones. The adversarial objectives of  and  are respectively defined as:

where  is a hinge function used to regularize the discriminator~\cite{zhang2018self,brock2018large}. 

\vspace{0.4em}
\noindent\textbf{Total loss}~~In all, we optimize the following objective,

where weights  are used  to balance the objectives. 



 \section{Experiments}
\label{sec:experiments}
\begin{figure*}[t]
    \center
    \small
    \setlength\tabcolsep{1pt}
    {
    \renewcommand{\arraystretch}{0.6}
    \begin{tabular}{cccccccc}
        Input & Ground truth & Pix2pixHD~\cite{wang2018high} & MUINT~\cite{huang2018multimodal} & EGSC-IT~\cite{ma2018exemplar} & SPADE~\cite{park2019semantic} & Ours & Exemplar\0.5ex]
    \includegraphics[width=0.23\columnwidth]{Figures/ade20k/ADE_val_00001517.jpg}&
    \includegraphics[width=0.23\columnwidth]{Figures/ade20k/ADE_val_00001517_synthesis.jpg}&
    \includegraphics[width=0.23\columnwidth]{Figures/ade20k/ADE_val_00001517_ref.jpg}& ~ &
    \includegraphics[width=0.23\columnwidth]{Figures/ade20k/ADE_val_00001599.jpg}& \includegraphics[width=0.23\columnwidth]{Figures/ade20k/ADE_val_00001599_out.jpg} & 
    \includegraphics[width=0.23\columnwidth]{Figures/ade20k/ADE_val_00001599_ref.jpg}& ~ &
    \includegraphics[width=0.23\columnwidth]{Figures/ade20k/ADE_val_00001996.jpg}&
    \includegraphics[width=0.23\columnwidth]{Figures/ade20k/ADE_val_00001996_synthesis.jpg}&
    \includegraphics[width=0.23\columnwidth]{Figures/ade20k/ADE_val_00001996_ref.jpg}\\
    \includegraphics[width=0.23\columnwidth]{Figures/ade20k/ADE_val_00001697.jpg}&
    \includegraphics[width=0.23\columnwidth]{Figures/ade20k/ADE_val_00001697_synthesis.jpg}&
    \includegraphics[width=0.23\columnwidth]{Figures/ade20k/ADE_val_00001697_ref.jpg}& ~ & 
    \includegraphics[width=0.23\columnwidth]{Figures/ade20k/ADE_val_00001232.jpg}&
    \includegraphics[width=0.23\columnwidth]{Figures/ade20k/ADE_val_00001232_synthesis.jpg}& 
    \includegraphics[width=0.23\columnwidth]{Figures/ade20k/ADE_val_00001232_ref.jpg}& ~ &
    \includegraphics[width=0.23\columnwidth]{Figures/ade20k/ADE_val_00001349.jpg}&
    \includegraphics[width=0.23\columnwidth]{Figures/ade20k/ADE_val_00001349_synthesis.jpg}&
    \includegraphics[width=0.23\columnwidth]{Figures/ade20k/ADE_val_00001349_ref.jpg}\\
\end{tabular}
}
\caption{\textbf{Our results of segmentation mask to image synthesis (ADE20k dataset).}}
\label{figure:ade20k_results}
\end{figure*}

\begin{figure*}[t]
\center
\small
\setlength\tabcolsep{0pt}
{
\renewcommand{\arraystretch}{0.0}
\begin{tabular}{@{}rrccccccc@{}}
    &
    \raisebox{0.25\height}{\rotatebox{90}{Exemplars}}&
    \includegraphics[width=0.25\columnwidth]{Figures/celebahq/00040_0_ref.jpg}&
    \includegraphics[width=0.25\columnwidth]{Figures/celebahq/00040_2_ref.jpg}&
    \includegraphics[width=0.25\columnwidth]{Figures/celebahq/00040_5_ref.jpg}&
    \includegraphics[width=0.25\columnwidth]{Figures/celebahq/14_ref.jpg}&
    \includegraphics[width=0.25\columnwidth]{Figures/celebahq/00040_19_ref.jpg}&
    \includegraphics[width=0.25\columnwidth]{Figures/celebahq/00040_13_ref.jpg}&
    \includegraphics[width=0.25\columnwidth]{Figures/celebahq/16_ref.jpg}\\
    


    \raisebox{1.05\height}{\rotatebox{90}{Edge}}&
    \includegraphics[width=0.25\columnwidth]{Figures/celebahq/00040e.jpg}&
    \includegraphics[width=0.25\columnwidth]{Figures/celebahq/00040e_0.jpg}&
    \includegraphics[width=0.25\columnwidth]{Figures/celebahq/00040e_2.jpg}&
    \includegraphics[width=0.25\columnwidth]{Figures/celebahq/00040e_5.jpg}&
    \includegraphics[width=0.25\columnwidth]{Figures/celebahq/14.jpg}&
    \includegraphics[width=0.25\columnwidth]{Figures/celebahq/00040e_19.jpg}&
    \includegraphics[width=0.25\columnwidth]{Figures/celebahq/00040e_13.jpg}&
    \includegraphics[width=0.25\columnwidth]{Figures/celebahq/16.jpg}\\
\end{tabular}
}
\caption{\textbf{Our results of edge to face synthesis (CelebA-HQ dataset).} First row: exemplars. Second row: our results.}
\label{figure:cebeba_result}
\end{figure*}

\begin{figure}[t]
    \center
    \small
    \setlength\tabcolsep{0pt}
    {
    \renewcommand{\arraystretch}{0.0}
    \begin{tabular}{cccccccc}
        & 
        \includegraphics[width=0.12\columnwidth]{Figures/deepfashion/crop_654_6_ref.jpg} &
        \includegraphics[width=0.12\columnwidth]{Figures/deepfashion/crop_654_14_ref.jpg} &
        \includegraphics[width=0.12\columnwidth]{Figures/deepfashion/crop_654_16_ref.jpg}&
        &
        \includegraphics[width=0.12\columnwidth]{Figures/deepfashion/crop_774_9_ref.jpg} &
        \includegraphics[width=0.12\columnwidth]{Figures/deepfashion/crop_774_10_ref.jpg} &
        \includegraphics[width=0.12\columnwidth]{Figures/deepfashion/crop_774_5_ref.jpg}\\
        
        \includegraphics[width=0.12\columnwidth]{Figures/deepfashion/crop_654_label.jpg} &
        \includegraphics[width=0.12\columnwidth]{Figures/deepfashion/crop_654_6.jpg} &
        \includegraphics[width=0.12\columnwidth]{Figures/deepfashion/crop_654_14.jpg} &
        \includegraphics[width=0.12\columnwidth]{Figures/deepfashion/crop_654_16.jpg} &
        \includegraphics[width=0.12\columnwidth]{Figures/deepfashion/crop_774_label.jpg} &
        \includegraphics[width=0.12\columnwidth]{Figures/deepfashion/crop_774_9.jpg} &
        \includegraphics[width=0.12\columnwidth]{Figures/deepfashion/crop_774_10.jpg} &
        \includegraphics[width=0.12\columnwidth]{Figures/deepfashion/crop_774_5.jpg}\\
    \end{tabular}
    }
    \caption{\textbf{Our results of pose to body synthesis (DeepFashion).} First row: exemplars. Second row: our results.}
    \label{figure:pose}
\end{figure}


\begin{table}[!t]
    \footnotesize
    \centering 
    \setlength\tabcolsep{2.5pt}
    \caption{\textbf{Image quality comparison.} Lower FID or SWD score indicates better image quality. The best scores are highlighted.}
    \begin{tabular}{@{}lcccccccc@{}}
    \toprule
    \multirow{2}{*}{} & \multicolumn{2}{c}{ADE20k} & \multicolumn{2}{c}{ADE20k-\emph{outdoor}} & \multicolumn{2}{c}{CelebA-HQ} & \multicolumn{2}{c}{DeepFashion}\\
    \cmidrule(lr){2-3} 
    \cmidrule(lr){4-5} 
    \cmidrule(lr){6-7} 
    \cmidrule(lr){8-9} 
     & FID & SWD & FID & SWD & FID & SWD & FID & SWD\\ \midrule
     Pix2pixHD & 81.8 & 35.7 & 97.8 & 34.5 & 62.7 & 43.3 & 25.2 & \textbf{16.4}\\
     SPADE & 33.9 & 19.7 & 63.3 & 21.9 & 31.5 & 26.9 & 36.2 & 27.8\\
     MUNIT & 129.3 & 97.8 & 168.2 & 126.3 & 56.8 & 40.8 & 74.0 & 46.2\\
     SIMS & N/A & N/A & 67.7 & 27.2 & N/A & N/A & N/A & N/A \\
     EGSC-IT & 168.3 & 94.4 & 210.0 & 104.9 & 29.5 & 23.8  & 29.0 & 39.1\\
     \emph{Ours} & \textbf{26.4} & \textbf{10.5} & \textbf{42.4} & \textbf{11.5} & \textbf{14.3} & \textbf{15.2} & \textbf{14.4} & 17.2\\
     \bottomrule
    \end{tabular}
    \label{table:image_quality}
\end{table}


\noindent\textbf{Implementation}~~We use Adam~\cite{kingma2014adam} solver with . Following the TTUR~\cite{heusel2017gans}, we set imbalanced learning rates,  and  respectively, for the generator and discriminator. Spectral normalization~\cite{miyato2018spectral} is applied to all the layers for both networks to stabilize the adversarial training. Readers can refer to the supplementary material for detailed network architecture. We conduct experiments using 8 32GB Tesla V100 GPUs, and it takes roughly 4 days to train 100 epochs on the ADE20k dataset~\cite{zhou2017scene}.   

\noindent\textbf{Datasets}~~We conduct experiments on multiple datasets with different sorts of image representation. All the images are resized to 256256 during training. 
\begin{itemize}[leftmargin=*]
\itemsep0em 
\item {ADE20k}~\cite{zhou2017scene} consists of 20k training images, each image associated with a 150-class segmentation mask. This is a challenging dataset for most existing methods due to its large diversity.
\item {ADE20k-\emph{outdoor}} contains the outdoor images extracted from ADE20k, as the same protocol in SIMS~\cite{qi2018semi}.
\item {CelebA-HQ}~\cite{liu2015faceattributes} contains high quality face images. We connect the face landmarks for face region, and use Canny edge detector to detect edges in the background. We perform an edge-to-face translation on this dataset. 
\item {Deepfashion}~\cite{liuLQWTcvpr16DeepFashion} consists of 52,712 person images in fashion clothes. We extract the pose keypoints using the  OpenPose~\cite{cao2018openpose}, and learn the translation to human body.
\end{itemize}
 
\begin{table}[!t]
    \footnotesize
    \setlength\tabcolsep{4.5pt}
    \centering 
    \caption{\textbf{Comparison of semantic consistency.} The best scores are highlighted.} 
\begin{tabular}{@{}lcccc@{}}
    \toprule
    & ADE20k & ADE20k-\emph{outdoor} & CelebA-HQ & DeepFashion\\
    \midrule
    Pix2pixHD & 0.833 & 0.848 & 0.914 & 0.943\\
    SPADE     & 0.856 & 0.867 & 0.922 & 0.936\\
    MUNIT     & 0.723 & 0.704 & 0.848 & 0.910\\
    SIMS      & N/A & 0.822 & N/A & N/A\\
EGSC-IT   & 0.734 & 0.723 & 0.915 & 0.942\\
    \emph{Ours} & \textbf{0.862} & \textbf{0.873} & \textbf{0.949} & \textbf{0.968}\\
    \bottomrule
    \end{tabular}
    \label{table:semantic_consistency}
\end{table}
\begin{table}[!t]
    \footnotesize
    \centering 
    \setlength\tabcolsep{0.9pt}
    \caption{\textbf{Comparison of style relevance.} A higher score indicates a higher appearance similarity relative to the exemplar. The best scores are highlighted.}
    \begin{tabularx}{\columnwidth}{@{}lYYYYYY@{}}
    \toprule
    \multirow{2}{*}{} & \multicolumn{2}{c}{ADE20k} & \multicolumn{2}{c}{CelebA-HQ} & \multicolumn{2}{c}{DeepFashion}\\
    \cmidrule(lr){2-3}
    \cmidrule(lr){4-5}
    \cmidrule(lr){6-7}
    & Color & Texture & Color & Texture & Color & Texture\\ 
    \midrule
    SPADE     & 0.874 & 0.892 & 0.955 & 0.927 & 0.943 & 0.904\\
    MUNIT     & 0.745 & 0.782 & 0.939 & 0.884 & 0.893 & 0.861\\
    EGSC-IT   & 0.781 & 0.839 & 0.965 & 0.942 & 0.945 & 0.916\\
    \emph{Ours} &  \textbf{0.962} & \textbf{0.941} & \textbf{0.977} & \textbf{0.958} & \textbf{0.982} & \textbf{0.958}\\
    \bottomrule
    \end{tabularx}
    \label{table:exemplar_relevance}
\end{table}

\begin{figure}[t!]
    \begin{center}
    \includegraphics[width=1.0\linewidth]{user_study.pdf}
    \end{center}
    \caption{\textbf{User study results.}}
    \label{figure:user_study}
\end{figure}


\noindent\textbf{Baselines}~~We compare our method with state-of-the-art image translation methods: 1) Pix2pixHD~\cite{wang2018high}, a leading supervised approach; 2) SPADE~\cite{park2019semantic}, a recently proposed supervised translation method which also supports the style injection from an exemplar image; 3) MUNIT~\cite{huang2018multimodal}, an unsupervised method that produces multi-modal results; 4) SIMS~\cite{qi2018semi}, which synthesizes images by compositing image segments from a memory bank; 5) EGSC-IT~\cite{ma2018exemplar}, an exemplar-based method that also considers the semantic consistency but can only mimic the global style. These methods except Pix2pixHD can generate exemplar-based results, and we use their released codes in this mode to train on several datasets. Since it is computationally prohibitive to prepare a database using SIMS, we directly use their reported figures. As we aim to propose a general translation framework, we do not include other task-specific methods. To provide the exemplar for our method, we first train a plain translation network to generate natural images and use them to retrieve the exemplars from the dataset. 

\vspace{0.4em}
\noindent\textbf{Quantitative evaluation}~~We evaluate different methods from three aspects. 
\begin{itemize}[leftmargin=*]
    \itemsep0em
    \item We use two metrics to measure image quality. First, we use the Fr\'echet Inception Score (FID)~\cite{heusel2017gans} to measure the distance between the distributions of synthesized images and real images. While FID measures the semantic realism, we also adopt sliced Wasserstein distance (SWD)~\cite{karras2017progressive} to measure their statistical distance of low-level patch distributions. Measured by these two metrics, Table~\ref{table:image_quality} shows that our method significantly outperforms prior methods in almost all the comparisons. Our method improves the FID score by 7.5 compared to previous leading methods on the challenging ADE20k dataset.
    \item The ultimate output should not alter the input semantics. To evaluate the semantic consistency, we adopt an ImageNet pretrained VGG model~\cite{brock2018large}, and use its high-level features maps, ,  and , to represent high-level semantics. We calculate the cosine similarity for these layers and take the average to yield the final score. Table~\ref{table:semantic_consistency} shows that our method best maintains the semantics during translation.
    \item Style relevance. We use low level features  and  respectively to measure the color and texture distance between the semantically corresponding patches in the output and the exemplar. We do not include Pix2pixHD as it does not produce an exemplar-based translation. Still, our method achieves considerably better instance-level style relevance as shown in Table~\ref{table:exemplar_relevance}.
\end{itemize}







\noindent\textbf{Qualitative comparison}~~Figure~\ref{fig:comparison} provides a qualitative comparison of different methods. It shows that our \emph{CocosNet} demonstrates the most visually appealing quality with much fewer artifacts. Meanwhile, compared to prior exemplar-based methods, our method demonstrates the best style fidelity, with the fine structures matching the semantically corresponding regions of the exemplar. This also correlates with the quantitative results, showing the obvious advantage of our approach. We show diverse results by changing the exemplar image in Figure~\ref{figure:ade20k_results}-\ref{figure:pose}. Please refer to the supplementary material for more results.




\begin{figure}[!t]
\center
\small
\setlength\tabcolsep{0pt}
{
\renewcommand{\arraystretch}{0.0}
\begin{tabular}{cccc}
\includegraphics[width=0.25\columnwidth]{Figures/corr/60.jpg} & 
    \includegraphics[width=0.25\columnwidth]{Figures/corr/02_1_front_.jpg}&
    \includegraphics[width=0.25\columnwidth]{Figures/corr/ADE_val_00000517.jpg}&
    \includegraphics[width=0.25\columnwidth]{Figures/corr/ADE_val_00001501.jpg}
    \\
    \includegraphics[width=0.25\columnwidth]{Figures/corr/ref_60.jpg} &
    \includegraphics[width=0.25\columnwidth]{Figures/corr/ref_02_1_front.jpg}&
    \includegraphics[width=0.25\columnwidth]{Figures/corr/ref_ADE_val_00000517.jpg} &
    \includegraphics[width=0.25\columnwidth]{Figures/corr/ref_ADE_val_00001501.jpg}\\
\end{tabular}
}
\caption{\textbf{Sparse correspondence of different domains.} Given the manual annotation points in domain A (first row), our method finds their corresponding points in domain B (second row).}
\label{fig:correspondence}
\end{figure}

\begin{table}[!t]
    \footnotesize
    \setlength\tabcolsep{4.5pt}
    \centering 
    \caption{\textbf{Ablation study.}} 
    \begin{tabular}{@{}lccc@{}}
    \toprule
    & FID  & Semantic consistency  & Style (color/texture)  \\
    \midrule
    w/o  & 14.4 & 0.948 & 0.975 / 0.955\\
    w/o  & 21.1 & 0.933 & \textbf{0.983} / 0.957\\
    w/o & 59.3 & 0.852 & 0.971 / 0.852\\
    w/o & 28.4 & 0.931 & 0.954 / 0.948\\
    w/o & 19.3 & 0.929 & 0.981 / 0.951\\
    \emph{Full} & \textbf{14.3} & \textbf{0.949} & 0.977 / \textbf{0.958} \\
    \bottomrule
    \end{tabular}
    \label{table:ablation}
\end{table}

\begin{figure}[!t]
\center
\small
\setlength\tabcolsep{1pt}
{
\renewcommand{\arraystretch}{0.6}
\begin{tabular}{lccc}
    & Input/Exemplar  & Dense warping & Final output\0.5em]
    
    &
    \raisebox{0.25\height}{\rotatebox{90}{Exemplars}}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/ADE20k/96_0_ref.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/ADE20k/96_1_ref.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/ADE20k/96_4_ref.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/ADE20k/96_5_ref.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/ADE20k/96_6_ref.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/ADE20k/96_7_ref.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/ADE20k/96_14_ref.jpg}\\

    \raisebox{0.95\height}{\rotatebox{90}{Mask}}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/ADE20k/ADE_val_00000097.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/ADE20k/96_0.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/ADE20k/96_1.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/ADE20k/96_4.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/ADE20k/96_5.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/ADE20k/96_6.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/ADE20k/96_7.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/ADE20k/96_14.jpg}\0.5em]
    
    &
    \raisebox{0.25\height}{\rotatebox{90}{Exemplars}}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/CelebAHQ/2585_0_ref.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/CelebAHQ/2585_2_ref.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/CelebAHQ/2585_5_ref.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/CelebAHQ/2585_6_ref.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/CelebAHQ/2585_7_ref.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/CelebAHQ/2585_8_ref.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/CelebAHQ/2585_14_ref.jpg}\\
    
    \raisebox{0.95\height}{\rotatebox{90}{Mask}}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/CelebAHQ/25807.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/CelebAHQ/2585_0.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/CelebAHQ/2585_2.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/CelebAHQ/2585_5.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/CelebAHQ/2585_6.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/CelebAHQ/2585_7.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/CelebAHQ/2585_8.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/CelebAHQ/2585_14.jpg}\\
\end{tabular}
}
\caption{\textbf{Our results of mask-to-image synthesis (CelebAHQ dataset).} In each group, the first row shows exemplars, and the second row shows the segmentation masks along with our results.}
\label{figure:CelebAHQM_0_result}
\end{figure*}


\begin{figure*}[h!]
\center
\small
\setlength\tabcolsep{0pt}
{
\renewcommand{\arraystretch}{0.0}
\begin{tabular}{@{}rrccccccc@{}}
    &
    \raisebox{0.25\height}{\rotatebox{90}{Exemplars}}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Flickr/23_3_ref.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Flickr/23_4_ref.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Flickr/23_7_ref.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Flickr/23_9_ref.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Flickr/23_10_ref.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Flickr/23_12_ref.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Flickr/23_16_ref.jpg}\\

    \raisebox{0.95\height}{\rotatebox{90}{Mask}}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Flickr/beach_1089.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Flickr/23_3.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Flickr/23_4.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Flickr/23_7.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Flickr/23_9.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Flickr/23_10.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Flickr/23_12.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Flickr/23_16.jpg}\\ [0.5em]
    
    


&
    \raisebox{0.25\height}{\rotatebox{90}{Exemplars}}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Flickr/1915_1_ref.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Flickr/1915_3_ref.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Flickr/1915_5_ref.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Flickr/1915_6_ref.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Flickr/1915_7_ref.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Flickr/1915_9_ref.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Flickr/1915_12_ref.jpg}\\

    \raisebox{0.95\height}{\rotatebox{90}{Mask}}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Flickr/waterfall_1441.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Flickr/1915_1.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Flickr/1915_3.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Flickr/1915_5.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Flickr/1915_6.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Flickr/1915_7.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Flickr/1915_9.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Flickr/1915_1.jpg}\\ [0.5em]
    
    
    &
    \raisebox{0.25\height}{\rotatebox{90}{Exemplars}}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Flickr/1227_0_ref.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Flickr/1227_1_ref.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Flickr/1227_5_ref.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Flickr/1227_4_ref.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Flickr/1227_6_ref.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Flickr/1227_9_ref.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Flickr/1227_18_ref.jpg}\\

    \raisebox{0.95\height}{\rotatebox{90}{Mask}}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Flickr/landscape_27091.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Flickr/1227_0.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Flickr/1227_1.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Flickr/1227_5.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Flickr/1227_4.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Flickr/1227_6.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Flickr/1227_9.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Flickr/1227_18.jpg}\\ [0.5em]
    
    
    &
    \raisebox{0.25\height}{\rotatebox{90}{Exemplars}}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Flickr/1064_2_ref.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Flickr/1064_3_ref.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Flickr/1064_7_ref.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Flickr/1064_13_ref.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Flickr/1064_14_ref.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Flickr/1064_18_ref.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Flickr/1064_19_ref.jpg}\\

    \raisebox{0.95\height}{\rotatebox{90}{Mask}}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Flickr/landscape_21121.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Flickr/1064_2.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Flickr/1064_3.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Flickr/1064_7.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Flickr/1064_13.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Flickr/1064_14.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Flickr/1064_18.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Flickr/1064_19.jpg}\\ 
\end{tabular}
}
\caption{\textbf{Our results of mask-to-image synthesis (Flickr dataset).} In each group, the first row shows exemplars, and the second row shows the segmentation masks along with our results.}
\label{figure:Flickr_0_result}
\end{figure*}


\clearpage
\FloatBarrier
\noindent\textbf{Edge-to-face}~~  Figure~\ref{figure:CelebAHQE_0_result} shows additional results of edge-to-face synthesis on CelebA-HQ dataset.

\begin{figure*}[h!]
\center
\small
\setlength\tabcolsep{0pt}
{
\renewcommand{\arraystretch}{0.0}
\begin{tabular}{@{}rrccccccc@{}}
    &
    \raisebox{0.25\height}{\rotatebox{90}{Exemplars}}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/CelebAHQ/2959_3_ref.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/CelebAHQ/2959_4_ref.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/CelebAHQ/2959_5_ref.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/CelebAHQ/2959_6_ref.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/CelebAHQ/2959_7_ref.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/CelebAHQ/2959_8_ref.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/CelebAHQ/2959_16_ref.jpg}\\

    \raisebox{0.95\height}{\rotatebox{90}{Edge}}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/CelebAHQ/2959.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/CelebAHQ/2959_3.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/CelebAHQ/2959_4.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/CelebAHQ/2959_5.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/CelebAHQ/2959_6.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/CelebAHQ/2959_7.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/CelebAHQ/2959_8.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/CelebAHQ/2959_16.jpg}\0.5em]
    
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Application/Makeup2/1431.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Application/Makeup2/5796.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Application/Makeup2/10662.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Application/Makeup2/11718.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Application/Makeup2/29884.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Application/Makeup2/29847.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Application/Makeup2/29938.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Application/Makeup2/29989.jpg}\\

    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Application/Makeup2/1431_makeup.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Application/Makeup2/5796_makeup.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Application/Makeup2/10662_makeup.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Application/Makeup2/11718_makeup.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Application/Makeup2/29884_makeup.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Application/Makeup2/29847_makeup.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Application/Makeup2/29938_makeup.jpg}&
    \includegraphics[width=0.125\columnwidth]{Figures/Supplementary/Application/Makeup2/29989_makeup.jpg}\\
\end{tabular}
}
\caption{\textbf{Makeup transfer.} Given a portrait along with makeup edits (1st column), we can transfer the makeup to other portraits by matching the semantic correspondence.}
\label{figure:makeup_result}
\end{figure*}

\clearpage
\subsection{Implementation Details}
The detailed architecture of \emph{CoCosNet} is shown in Table~\ref{tab:architecture of CoCosNet}, with the naming convention as the CycleGAN. 

\vspace{0.4em}
\noindent\textbf{Cross-domain correspondence network}~~
Two domain adaptors without weight sharing are used to adapt the input image and the exemplar to a shared domain . The domain adaptors comprise several Conv-InstanceNorm-LeakReLU blocks and the spatial size of features in  is 6464. Once the intermediate domain  is found, a shared adaptive feature block further transforms the features from two branches to the representation suitable for correspondence. The correlation layer computes pairwise affinity values between 40961 normalized features vectors. We downscale the exemplar image to 6464 to fit the size of correlation matrix, and thus obtain the warped image on this scale. We use synchronous batch normalization within this sub-network. 

\vspace{0.4em}
\noindent\textbf{Translation network}~~
The translation network generates the final output based on the style of the warped exemplar. We encode the exemplar style through two convolutional layers, which outputs  and  to modulate the normalization layer in the generator network. We have seven such style encoder, each responsible for modulating an individual normalization layer. The generator consists of seven normalization layer, which progressively utilizes the style code to synthesize the final output. The generator also employs a nonlocal block so that a larger receptive field can be utilized to enhance the global structural consistency. We use positional normalization within this sub-network.

\noindent\textbf{Warm-up strategy}~~
For the most challenging ADE20k dataset, a mask warm strategy is used. At the beginning of the training, we explicitly provide the segmentation mask for the domain adaptors, and employ cross-entropy loss to encourage that the masks are correctly aligned after dense warping. Such warm-up helps speed up the convergence of the correspondence network and improve the correspondence accuracy. 
After training 80 epochs, we replace the segmentation masks with Gaussian noise. We just use the segmentation mask for warm up and there is no need to provide the masks during inference.

\begin{table}[!tbh]
  \small
  \begin{center}
    \caption{\textbf{The architecture of \emph{CoCosNet}.} k3s1 indicates the convolutional layer with kernel size 3 and stride 1. The th style encoder outputs features with dimensions matching the th Resblock in the generator.}
    \label{tab:architecture of CoCosNet}
    \begin{tabular}{@{}l|l|l|l@{}}
      \toprule
      {Sub-network} & {Module} & {Layers in the module} & {Output shape (HWC)}\\
      \midrule
      \multirow{7}{2.5cm}{Correspondence Network} 
      & \multirow{6}{3.0cm}{Domain adaptor} & Conv2d / k3s1 & 25625664\\ 
      & & Conv2d / k4s2 & 128128128\\
      & & Conv2d / k3s1 & 128128256\\
      & & Conv2d / k4s2 & 6464512\\
      & & Conv2d / k3s1 & 6464512\\
      & & Resblock / k3s1 & 6464256\\
      \cmidrule{2-4}
      & \multirow{2}{3.0cm}{Adaptive feature block} & Resblock & 6464256\\
      & & Conv2d / k1s1 & 6464256\\
      \cmidrule{2-4}
      &Correspondence&Correlation\&warping & 64643\\
      \midrule
      \multirow{7}{2.5cm}{Translation Network}
      & \multirow{3}{3.0cm}{Style encoder} & Bilinear interpolation & 3\\ 
      & & Conv2d / k3s1 & 128\\
      & & Conv2d / k3s1 & \\
      \cmidrule{2-4}
      & \multirow{5}{3.0cm}{Generator} & Conv2d / k3s1 & 881024\\ 
      &  & Resblock & 128128256\\
      &  & Nonlocal & 128128256\\
      &  & Resblock & 25625664\\
      &  & Conv2d / k3s1 & 2562563\\
      \bottomrule
    \end{tabular}
  \end{center}
\end{table}

\clearpage
\subsection{Detailed User Study Results}
Figure~\ref{figure:userstudy_result} shows the detailed results of user study. In ADE20k, there are 67.3\%  and 91.9\% users respectively that prefer the image quality and style relevance for our method. Regarding edge-to-face translation on CelebA-HQ, 91.3\% users prefer our image quality while 90.6\% users believes our method most resembles the exemplar. For pose synthesis on DeepFashion dataset, 90.6\% and 98.8\% users prefer our results  according to the image quality and the style resemblance respectively.

\begin{figure*}[h!]
\center
\small
\setlength\tabcolsep{2pt}
{
\renewcommand{\arraystretch}{8.0}
\begin{tabular}{@{}cc@{}}
    \includegraphics[width=0.45\columnwidth]{Figures/Supplementary/user_study1.eps}&
    \includegraphics[width=0.45\columnwidth]{Figures/Supplementary/user_study2.eps}\\
    
    \includegraphics[width=0.45\columnwidth]{Figures/Supplementary/user_study3.eps}&
    \includegraphics[width=0.45\columnwidth]{Figures/Supplementary/user_study4.eps}\\
    
    \includegraphics[width=0.45\columnwidth]{Figures/Supplementary/user_study5.eps}&
    \includegraphics[width=0.45\columnwidth]{Figures/Supplementary/user_study6.eps}\\
\end{tabular}
}
\caption{\textbf{Detailed user study results for ADE20k, CelebA-HQ and DeepFashion dataset.}}
\label{figure:userstudy_result}
\end{figure*}

\clearpage
\subsection{Multimodal results for Flickr dataset}
Similar to the practice in~\cite{park2019semantic}, we collect 56,568 landscape images from Flickr. The semantic segmentation masks are computed using a pre-trained  UPerNet101~\cite{xiao2018unified} network. By feeding different exemplar, our method supports multimodal landscape synthesis. Figure~\ref{figure:multimodal} shows highly realistic landscape results using the images in Flicker dataset. 

\begin{figure}[h!]
    \center
    \small
    \setlength\tabcolsep{2pt}
    {
    \renewcommand{\arraystretch}{1.0}
    \begin{tabular}{@{}ccccc@{}}
        \includegraphics[width=0.15\columnwidth]{Figures/flickr/23_18.jpg}&
        \includegraphics[width=0.15\columnwidth]{Figures/flickr/23_16.jpg}&
        \includegraphics[width=0.15\columnwidth]{Figures/flickr/23_7.jpg}&
        \includegraphics[width=0.15\columnwidth]{Figures/flickr/23_10.jpg}&
        \includegraphics[width=0.15\columnwidth]{Figures/flickr/23_9.jpg}\\
        
        \includegraphics[width=0.15\columnwidth]{Figures/flickr/56_19.jpg}&
        \includegraphics[width=0.15\columnwidth]{Figures/flickr/56_17.jpg}&
        \includegraphics[width=0.15\columnwidth]{Figures/flickr/56_16.jpg}&
        \includegraphics[width=0.15\columnwidth]{Figures/flickr/56_14.jpg}&
        \includegraphics[width=0.15\columnwidth]{Figures/flickr/56_10.jpg}\\
        
        \includegraphics[width=0.15\columnwidth]{Figures/flickr/717_19.jpg}&
        \includegraphics[width=0.15\columnwidth]{Figures/flickr/717_18.jpg}&
        \includegraphics[width=0.15\columnwidth]{Figures/flickr/717_16.jpg}&
        \includegraphics[width=0.15\columnwidth]{Figures/flickr/717_15.jpg}&
        \includegraphics[width=0.15\columnwidth]{Figures/flickr/717_7.jpg}\\
        
        \includegraphics[width=0.15\columnwidth]{Figures/flickr/1766_1.jpg}&
        \includegraphics[width=0.15\columnwidth]{Figures/flickr/1766_12.jpg}&
        \includegraphics[width=0.15\columnwidth]{Figures/flickr/1766_15.jpg}&
        \includegraphics[width=0.15\columnwidth]{Figures/flickr/1766_17.jpg}&
        \includegraphics[width=0.15\columnwidth]{Figures/flickr/1766_7.jpg}\\
        
        \includegraphics[width=0.15\columnwidth]{Figures/flickr/1772_10.jpg}&
        \includegraphics[width=0.15\columnwidth]{Figures/flickr/1772_17.jpg}&
        \includegraphics[width=0.15\columnwidth]{Figures/flickr/1772_19.jpg}&
        \includegraphics[width=0.15\columnwidth]{Figures/flickr/1772_3.jpg}&
        \includegraphics[width=0.15\columnwidth]{Figures/flickr/1772_9.jpg}\\
        
        \includegraphics[width=0.15\columnwidth]{Figures/flickr/1672_2.jpg}&
        \includegraphics[width=0.15\columnwidth]{Figures/flickr/1672_10.jpg}&
        \includegraphics[width=0.15\columnwidth]{Figures/flickr/1672_13.jpg}&
        \includegraphics[width=0.15\columnwidth]{Figures/flickr/1672_7.jpg}&
        \includegraphics[width=0.15\columnwidth]{Figures/flickr/1672_8.jpg}\\
    \end{tabular}
    }
    \caption{\textbf{Multimodal results of Flickr dataset.} We only present the final synthesis results here.}
    \label{figure:multimodal}
\end{figure}

\clearpage
\subsection{Limitation}
As an exemplar-based approach, our method may not produce satisfactory results due to one-to-many and many-to-one mappings as shown in Figure~\ref{figure:multiple instances}. We leave further research tackling these issues as future work.



\begin{figure*}[h!]
\center
\small
\setlength\tabcolsep{2pt}
{
\renewcommand{\arraystretch}{1.0}
\begin{tabular}{@{}ccc@{}}
    Input & Exemplar & Synthesis \\
    \includegraphics[width=0.15\columnwidth]{Figures/Supplementary/ADE_val_00001805.jpg}&
    \includegraphics[width=0.15\columnwidth]{Figures/Supplementary/ADE_val_00001805_ref.jpg}&
    \includegraphics[width=0.15\columnwidth]{Figures/Supplementary/ADE_val_00001805_out.jpg}\\
    \includegraphics[width=0.15\columnwidth]{Figures/Supplementary/ADE_val_00000156.jpg}&
    \includegraphics[width=0.15\columnwidth]{Figures/Supplementary/ADE_val_00000156_ref.jpg}&
    \includegraphics[width=0.15\columnwidth]{Figures/Supplementary/ADE_val_00000156_out.jpg}\\
\end{tabular}
}
\caption{\textbf{Limitation.} Our method may produce mixed color artifact due the one-to-many mapping (1st row). Besides, the multiple instances (pillows in the figure) may use the same style in the cases of many-to-one mapping (2nd row).}
\label{figure:multiple instances}
\end{figure*}

Another limitation is that the computation of the correlation matrix takes tremendous GPU memory,
which makes our method hardly scale for high resolution images. We leave the solve of this issue in future work.





 
\end{document}
