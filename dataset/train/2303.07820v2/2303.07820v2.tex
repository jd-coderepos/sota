


\section{Experiments}
\label{sec:exp}

In this section, we empirically evaluate our proposed backbone network equipped with adaptive rotated convolution on various detection networks. We begin by introducing the detailed experiment setup, which includes datasets and training configurations, in \cref{sec:exp_setting}. Then the main results of our method with various rotated object detectors on two commonly used datasets are presented in \cref{sec:exp_various}. The comparison results with competing approaches are demonstrated in \cref{sec:exp_sota}. Finally, the ablation studies in \cref{sec:exp_abl} and the visualization results in \cref{sec:exp_vis} further validate the effectiveness of the proposed method.





\begin{table*}[ht]
\begin{center}
\resizebox{1.0\textwidth}{!}{
\setlength{\tabcolsep}{1.1mm}
\begin{tabular}{l|c|c c c c c c c c c c c c c c c|l}
    \toprule
    Method & Backbone & PL & BD & BR & GTF & SV & LV & SH & TC & BC & ST & SBF & RA & HA & SP & HC &  mAP \\
    \midrule
    \multicolumn{18}{l}{{\bf \emph{Single-stage methods}}} \\
    \midrule
DRN~\cite{pan2020dynamic} & H104 & 88.91 & 80.22 & 43.52 & 63.35 & 73.48 & 70.69 & 84.94 & 90.14 & 83.85 & 84.11 & 50.12 & 58.41 & 67.62 & 68.60 & 52.50 &  70.70 \\
		R3Det~\cite{yang2021r3det} & R101 & 88.76 & 83.09 & 50.91 & 67.27 & 76.23 & 80.39 & 86.72 & 90.78 & 84.68 & 83.24 & 61.98 & 61.35 & 66.91 & 70.63 & 53.94 &  73.79 \\
		PIoU~\cite{chen2020piou} & DLA34 & 80.90 & 69.70 & 24.10 & 60.20 & 38.30 & 64.40 & 64.80 & 90.90 & 77.20 & 70.40 & 46.50 & 37.10 & 57.10 & 61.90 & 64.00 &  60.50 \\
		RSDet~\cite{qian2021learning} & R101 & 89.80 & 82.90 & 48.60 & 65.20 & 69.50 & 70.10 & 70.20 & 90.50 & 85.60 & 83.40 & 62.50 & 63.90 & 65.60 & 67.20 & 68.00 &  72.20 \\
		DAL~\cite{ming2021dynamic} & R50 & 88.68 & 76.55 & 45.08 & 66.80 & 67.00 & 76.76 & 79.74 & 90.84 & 79.54 & 78.45 & 57.71 & 62.27 & 69.05 & 73.14 & 60.11 &  71.44 \\
        SANet~\cite{han2021align} & R50 & 89.30 & 80.11 & 50.97 & 73.91 & 78.59 & 77.34 & 86.38 & 90.91 & 85.14 & 84.84 & 60.45 & 66.94 & 66.78 & 68.55 & 51.65 &  74.13 \\
	    G-Rep~\cite{hou2022g} & R101 & 88.89 & 74.62 & 43.92 & 70.24  & 67.26 & 67.26 & 79.80 & 90.87 & 84.46 & 78.47 & 54.59 & 62.60 & 66.67 & 67.98 & 52.16 &  70.59 \\
    \midrule
    
    \multicolumn{18}{l}{{\bf \emph{Two-stage methods}}} \\
    \midrule
		ICN~\cite{azimi2018towards} & R101 & 81.36 & 74.30 & 47.70 & 70.32 & 64.89 & 67.82 & 69.98 & 90.76 & 79.06 & 78.20 & 53.64 & 62.90 & 67.02 & 64.17 & 50.23 &  68.16 \\
		CAD-Net~\cite{zhang2019cad} & R101 & 87.80 & 82.40 & 49.40 & 73.50 & 71.10 & 63.50 & 76.60 & 90.90 & 79.20 & 73.30 & 48.40 & 60.90 & 62.00 & 67.00 & 62.20 &  69.90 \\
		RoI Trans~\cite{ding2019learning} & R101 & 88.64 & 78.52 & 43.44 & 75.92 & 68.81 & 73.68 & 83.59 & 90.74 & 77.27 & 81.46 & 58.39 & 53.54 & 62.83 & 58.93 & 47.67 &  69.56 \\
		SCRDet~\cite{yang2019scrdet} & R101 & 89.98 & 80.65 & 52.09 & 68.36 & 68.36 & 60.32 & 72.41 & 90.85 & 87.94 & 86.86 & 65.02 & 66.68 & 66.25 & 68.24 & 65.21 &  72.61 \\
		G. Vertex~\cite{xu2020gliding} & R101 & 89.64 & 85.00 & 52.26 & 77.34 & 73.01 & 73.14 & 86.82 & 90.74 & 79.02 & 86.81 & 59.55 & 70.91 & 72.94 & 70.86 & 57.32 &  75.02 \\
		FAOD~\cite{li2019feature} & R101 & 90.21 & 79.58 & 45.49 & 76.41 & 73.18 & 68.27 & 79.56 & 90.83 & 83.40 & 84.68 & 53.40 & 65.42 & 74.17 & 69.69 & 64.86 &  73.28 \\
		CenterMap~\cite{wang2020learning} & R50  & 88.88 & 81.24 & 53.15 & 60.65 & 78.62 & 66.55 & 78.10 & 88.83 & 77.80 & 83.61 & 49.36 & 66.19 & 72.10 & 72.36 & 58.70 &  71.74 \\
		FR-Est~\cite{fu2020point} & R101 & 89.63 & 81.17 & 50.44 & 70.19 & 73.52 & 77.98 & 86.44 & 90.82 & 84.13 & 83.56 & 60.64 & 66.59 & 70.59 & 66.72 & 60.55 &  74.20 \\
		Mask OBB~\cite{wang2019mask} & R50  & 89.61 & 85.09 & 51.85 & 72.90 & 75.28 & 73.23 & 85.57 & 90.37 & 82.08 & 85.05 & 55.73 & 68.39 & 71.61 & 69.87 & 66.33 &  74.86 \\
		ReDet~\cite{han2021redet} & ReR50 & 88.79 & 82.64 & 53.97 & 74.00 & 78.13 & 84.06 & 88.04 & 90.89 & 87.78 & 85.75 & 61.76 & 60.39 & 75.96 & 68.07 & 63.59 &  76.25 \\
		AOPG~\cite{cheng2022anchor} & R101  & 89.14 & 82.74 & 51.87 & 69.28 & 77.65 & 82.42 & 88.08 & 90.89 & 86.26 & 85.13 & 60.60 & 66.30  & 74.05 & 67.76 & 58.77 &  75.39 \\
		SASM~\cite{hou2022shape} & R50  & 86.42 & 78.97 & 52.47 & 69.84 & 77.30 & 75.99 & 86.72 & 90.89 & 82.63 & 85.66 & 60.13 & 68.25 & 73.98 & 72.22 & 62.37 &  74.92 \\
		
        \midrule
        \multirow{3.7}{*}{Oriented }
        & R50 & 89.48 & 82.59 & 54.42 & 72.58 & 79.01 & 82.43 & 88.26 & 90.90 & 86.90 & 84.34 & 60.79 & 67.08 & 74.28 & 69.77 & 54.27 &  75.81 \\
        & \cellcolor{lightgray!30}{\bf ARC-R50} & \cellcolor{lightgray!30}89.40 & \cellcolor{lightgray!30}82.48 & \cellcolor{lightgray!30}55.33 & \cellcolor{lightgray!30}73.88 & \cellcolor{lightgray!30}79.37 & \cellcolor{lightgray!30}84.05 & \cellcolor{lightgray!30}88.06 & \cellcolor{lightgray!30}90.90 & \cellcolor{lightgray!30}86.44 & \cellcolor{lightgray!30}84.83 & \cellcolor{lightgray!30}63.63 & \cellcolor{lightgray!30}70.32 & \cellcolor{lightgray!30}74.29 & \cellcolor{lightgray!30}71.91 & \cellcolor{lightgray!30}65.43 &  \cellcolor{lightgray!30}\textbf{77.35} \\
        \cmidrule{2-18}
        \multirow{1}{*}{R-CNN~\cite{xie2021oriented}}
        & R101 & 89.51 & 84.50 & 54.67 & 73.10 & 78.77 & 82.87 & 88.08 & 90.90 & 86.97 & 85.38 & 63.06 & 67.35 & 75.91 & 68.73 & 51.91 &  76.11 \\
        & \cellcolor{lightgray!30}{\bf ARC-R101} & \cellcolor{lightgray!30}89.39 & \cellcolor{lightgray!30}83.58 & \cellcolor{lightgray!30}57.51 & \cellcolor{lightgray!30}75.94 & \cellcolor{lightgray!30}78.75 & \cellcolor{lightgray!30}83.58 & \cellcolor{lightgray!30}88.08 & \cellcolor{lightgray!30}90.90 & \cellcolor{lightgray!30}85.93 & \cellcolor{lightgray!30}85.38 & \cellcolor{lightgray!30}64.03 & \cellcolor{lightgray!30}68.65 & \cellcolor{lightgray!30}75.59 & \cellcolor{lightgray!30}72.03 & \cellcolor{lightgray!30}65.68 &  \cellcolor{lightgray!30}\textbf{77.70} \\
    \bottomrule
\end{tabular}
}
\end{center}
\vskip -0.1in
\caption{{\bf Experimental results on the DOTA dataset compared with state-of-the-art methods.} In the backbone column, H104 denotes the 104-layer hourglass network~\cite{yang2017stacked}, DLA34 refers to the 34-layer deep layer aggregation network~\cite{zhou2019objects}, R50 and R101 stand for ResNet-50 and ResNet-101~\cite{he2016deep}, respectively, ReR50 is proposed in ReDet~\cite{han2021redet} with rotation-equivariant operations, ARC-R50 and ARC-R101 is the proposed backbone network which replaces the 33 convolutions in ResNets with the proposed adaptive rotated convolution.}
\label{tab:sota}
\vspace{-0.3cm}
\end{table*}




\begin{table}
\begin{center}
  \resizebox{0.4\textwidth}{!}{
  \begin{tabular}{l|c|l}
    \toprule
    Method & Backbone & mAP \\
    \midrule
    R3Det~\cite{yang2021r3det} & R152 & 76.47 \\
    SASM~\cite{hou2022shape} & RX101 & 79.17 \\
    SANet~\cite{han2021align} & R50 & 79.42 \\
    ReDet~\cite{han2021redet} & ReR50 & 80.10 \\
    R3Det-GWD~\cite{yang2021rethinking} & R152 & 80.19 \\
    R3Det-KLD~\cite{yang2021learning} & R152 & 80.63 \\
    AOPG~\cite{cheng2022anchor} & R50 & 80.66 \\
    KFIoU~\cite{yang2022kfiou} & Swin-T & 80.93 \\
    RVSA~\cite{wang2022advancing} & ViTAE-B & 81.24 \\
    \midrule
    \multirow{2}{*}{Oriented R-CNN~\cite{xie2021oriented}}
    & R50 &  80.62 \\
    & \cellcolor{lightgray!30 }{\bf ARC-R50} 
    & \cellcolor{lightgray!30}{\bf 81.77} \\
    \bottomrule
  \end{tabular}
  }
\end{center}
\vskip -0.1in
\caption{{\bf Experiment results on the DOTA dataset.} The results are obtained under {\bf \emph{multi-scale training and testing}} strategies.}
\label{tab:exp_dotams}
\vspace{-0.4cm}
\end{table}




\subsection{Experiment settings}
\label{sec:exp_setting}
{\bf Datasets.} We evaluate the proposed methods on two widely used oriented object detection benchmarks, \ie, DOTA-v1.0~\cite{xia2018dota} and HRSC2016~\cite{liu2016ship}.

DOTA~\cite{xia2018dota} is a large-scale rotated object detection dataset containing 2806 photographs and 188282 instances with oriented bounding box annotations. The following fifteen object classes are covered in this dataset: Plane (PL), Baseball diamond (BD), Bridge (BR), Ground track field (GTF), Small vehicle (SV), Large vehicle (LV), Ship (SH), Tennis court (TC), Basketball court (BC), Storage tank (ST), Soccer-ball field (SBF), Roundabout (RA), Harbor (HA), Swimming pool (SP), and Helicopter (HC). The image size of the DOTA dataset is extensive: from 800800 to 40004000 pixels. We crop the raw images into 10241024 patches with a stride of 824, which means the pixel overlap between two adjacent patches is 200. With regard to multi-scale training and testing, we first resize the raw pictures at three scales (0.5, 1.0, and 1.5) and crop them into 10241024 patches with the stride of 524. Following the common practice, we use both the training set and the validation set for training, and the testing set for testing. The mean average precision and the average precision of each category are obtained by submitting the testing results to the official evaluation server of the DOTA dataset.

HRSC2016~\cite{liu2016ship} is another widely-used arbitrary-oriented object detection benchmark. It contains 1061 images with sizes ranging from 300300 to 1500900. Both the training set (436 images) and validation set (181 images) are used for training and the remaining for testing. For the evaluation metrics on the HRSC2016~\cite{liu2016ship}, we report the COCO~\cite{lin2014microsoft} style mean average precision (mAP) as well as the average precision under 0.5 and 0.75 threshold (AP and AP). In the data pre-processing procedure, we do not change the aspect ratios of images.

{\bf Implementation Details.} The results on DOTA and HRSC2016 are obtained  using the MMRotate~\cite{zhou2022mmrotate} toolbox, except that Oriented R-CNN~\cite{xie2021oriented} is implemented with OBBDetection codebase. On the DOTA dataset, we train all the models for 12 epochs. On the HRSC2016 dataset, the Rotated RetinaNet~\cite{lin2017focal} is trained for 72 epochs, while SANet~\cite{han2021align} and Oriented R-CNN~\cite{xie2021oriented} are trained for 36 epochs. The detailed configuration of various detectors is provided in the supplementary material. Although the detailed training configuration varies among different detectors and different datasets, we keep it the same between the experiments using our proposed backbone networks and baseline backbone networks for fair comparisons.



\subsection{Effectiveness on various architectures}
\label{sec:exp_various}

We compare the backbone network equipped with our proposed ARC module with counterparts that use a canonical ResNet-50~\cite{he2016deep}. The experiment results on DOTA~\cite{xia2018dota} dataset and HRSC2016~\cite{liu2016ship} dataset are shown in \cref{tab:various} and \cref{tab:exp_hrsc}, respectively. From the results, we find that our method significantly improves the generalization ability of various rotated object detection networks. On the most popular oriented object detection benchmark DOTA-v1.0, the proposed method achieves significant improvement on both single-stage and two-stage detectors. 

For single-stage detectors, our method could improve 3.03\% mAP for Rotated RetinaNet~\cite{lin2017focal}, 2.62\% mAP for R3Det~\cite{yang2021r3det}, and 1.36\% mAP for SANet~\cite{han2021align}. For two-stage detectors, our method could also get more than 1.5\% mAP improvement (+1.60\% mAP for Rotated Faster R-CNN~\cite{ren2015faster}, +4.16\% mAP for CFA~\cite{guo2021beyond}
, and +1.54\% mAP for Oriented R-CNN~\cite{xie2021oriented}). On the HRSC2016~\cite{liu2016ship} dataset, the proposed method could also achieve remarkable improvement (1.27\% mAP improvement for  Rotated RetinaNet~\cite{lin2017focal}, 2.12\% for SANet~\cite{han2021align}, and 1.84\% for Oriented R-CNN~\cite{xie2021oriented}). These experimental results verify that the proposed backbone networks are compatible with various detection network architectures and can effectively improve their performance on the oriented object detection tasks.




\begin{figure}[!t]
  \centering
  \includegraphics[width=1.0\linewidth]{figs/ablation_e_r.pdf}
  \caption{Ablation studies on the influence of adaptive kernel rotation and the effect of conditional computation. The experiments are conducted on Oriented R-CNN with DOTA dataset. }
  \label{fig:abl_rotate_experts}
  \vskip -0.1 in
\end{figure}



\subsection{Comparison with state-of-the-art methods.}
\label{sec:exp_sota}

We report full experimental results, including the average precision of each category and the mean average precision (mAP) on the DOTA dataset to make a fair comparison with the previous methods. We combine the proposed backbone network with one of the highly competitive method Oriented R-CNN~\cite{xie2021oriented}. The single-scale and the multi-scale training and testing results are shown in \cref{tab:sota} and \cref{tab:exp_dotams}, respectively. When we use the ARC-ResNet-50 backbone, the adaptive rotated convolution can improve the mAP of Oriented R-CNN by 1.54\% over the static convolution under the single scale training and testing strategy. As the depth of backbone network goes deeper to 101, the adaptive rotated convolution can still enhance the mAP by 1.59\%. Under the multi-scale training and multi-scale testing strategy, our method reaches 81.77\% mAP with ResNet-50 as the base model. This result is highly competitive and surpasses all other existing methods, even when compared with counterparts with vision transformer backbone~\cite{yang2022kfiou, wang2022advancing} or with advanced model pretraining mechanism~\cite{he2022masked, wang2022advancing}.






\subsection{Ablation studies}
\label{sec:exp_abl}

We conduct ablation studies to analyze how different design choices affect the rotated object detection performance. We first demonstrate that the adaptive kernel rotation mechanism significantly outperforms the static convolution approach. We further present the effectiveness of the conditional computation mechanism and the ablation studies on kernel number . We then examine the effect of replacing different stages of the backbone network. Finally, the architecture design of our routing function is studied. 




\begin{table}[t]
  \centering
  \resizebox{0.99\linewidth}{!}
  {
  \begin{tabular}{c|c|ccc|c}
    \toprule
    Backbone &  & Params(M) & FLOPs(G) & FPS (img/s) & mAP \\  \midrule
R50  & 1 & 41.14 & 211.43 & 29.9 & 75.81 \\  R101 & 1 & 60.13 & 289.33 & 27.6 & 76.11 \\  \cmidrule{1-6}
ARC-R50 & 1 & 41.18 & 211.85 & 29.6 & 76.97 \\  ARC-R50 & 2 & 52.25 & 211.89 & 29.2 & 77.17 \\  ARC-R50 & 4 & 74.38 & 211.97 & 29.2 & 77.35 \\  ARC-R50 & 6 & 96.52 & 212.06 & 29.1 & 77.38 \\  \bottomrule
  \end{tabular}
  }
  \vskip 0.15 cm
  \caption{Ablation studies on the kernel number . The experiments are conducted on Oriented R-CNN with DOTA dataset.}
  \label{tab:ablation_number_kernel}
  \vskip -0.4 cm
\end{table}










\begin{figure*}[!t]
  \centering
  \includegraphics[width=1.00\linewidth]{figs/vis.pdf}
  \caption{Visualization results of the oriented object detection on the test dataset of DOTA~\cite{xia2018dota}. The upper row displays the bounding boxes predicted by Oriented R-CNN~\cite{xie2021oriented} with ResNet-50 backbone as baseline. Right column shows the predictions of the Oriented R-CNN equipped with our proposed ARC module. \textbf{Zoom in for best view.}}
  \label{fig:vis_box}
  \vspace{-0.1cm}
\end{figure*}


{\bf Adaptive kernel rotation.} First, wecompare the performance between rotating the convolution kernels in a data-dependent manner and the static convolution on the DOTA dataset. The experiment results are shown in \cref{fig:abl_rotate_experts}. From the comparison between the first two rows in \cref{fig:abl_rotate_experts}, we find that by employing the adaptive kernel rotation, the performance of the object detector boosts (+1.16\% mAP on Oriented R-CNN~\cite{xie2021oriented}, which is highly competitive with SOTA methods). This experiment verifies the effectiveness of the proposed adaptive convolution kernel rotation approach for its adaptability  on capturing oriented objects.


{\bf Conditional computation.} 
The results in row 2 and row 3 of \cref{fig:abl_rotate_experts} show the effect of adopting more adaptive rotated kernels. By endowing the convolution kernel with more direction of rotation, the performance of the oriented object detector further increases. This is because the objects in an image usually have multiple orientations, and a single adaptive kernel is insufficient. Adopting more kernels with different orientation angles can endow the backbone feature extractor with more flexibility to produce high-quality features of these arbitrarily oriented objects.



{\bf Ablation on kernel number.}
\cref{tab:ablation_number_kernel} reports the information on the parameters, FLOPs, and inference speed (in FPS) and performance (in mAP) of different kernel numbers .
The FLOPs are calculated with  image resolution.
The FPS is tested on an RTX 3090 with batch size 1, utilizing FP16 and torch.compile().
The results demonstrate that, as we progressively increase the number of kernels, the mAP exhibits a consistent upward trend. Meanwhile, FLOPs and FPS are essentially unchanged.
This phenomenon demonstrate that the proposed method achieves remarkable improvement in mAP while maintaining a high level of efficiency, with only a marginal increase of 0.002\% in FLOPs and less than 2.7\% drop in FPS compared to the baseline model.
Note that the number of parameters is no longer the bottleneck for evaluating model efficiency.



\begin{table}[ht]
\begin{center}
  \begin{tabular}{c c c c|l}
    \toprule
    Stage1 & Stage2 & Stage3 & Stage4 & mAP \\
    \midrule
    - &   -    &   -    & - & 75.81 \\
    - &   -    &   -    & \cmark & 77.17 \\
    - &   -    & \cmark & \cmark & 77.29\\
    - & \cmark & \cmark & \cmark & {\bf 77.35} \\
    \bottomrule
  \end{tabular}
\end{center}
\vskip -0.1in
\caption{Ablation studies of the replacement strategy on the DOTA dataset. The experiments are conduct on Oriented R-CNN with ARC-ResNet-50 backbone network, which have 3, 4, 6, 3 blocks on each stage, respectively. The symbol `\cmark' means we replace all the convolution layers in this stage, while the symbol `-' means we do not replace any convolution blocks in this stage. }
\label{tab:replace}
\vskip -0.2in
\end{table}


{\bf Replacement strategy.} Since the feature pyramid network (FPN)~\cite{lin2017feature} is attached to the last three stages of the backbone network, we do not replace the convolution layer in the first stage and ablate the replacement strategy in the last three stages. The ablation experiments are conducted on Oriented R-CNN~\cite{xie2021oriented} with a 50-depth backbone network on the DOTA dataset. Initially, we replace all the 33 convolution with the proposed ARC module in the last stage of the backbone network, and the mAP metric gets a 1.36\% improvement over the baseline model. We further replace the convolution layer of more stages at the backbone network, and the performance on the oriented object detection benchmark improves steadily. As a result, we choose to replace all the last three stages in the backbone network.





{\bf The structure of the routing function.} We ablate two designs in the routing function. The first design is spatial information encoding, which adds a depthwise convolution module before the average pooling layer (see \cref{fig:module}{\color{red} (c)}). The second design uses the combination weights prediction branch to adaptively combine the weight of each kernel (the branch producing  in \cref{fig:module}{\color{red} (c)}) rather than simply taking the average value of them. We conduct the experiments with Oriented R-CNN~\cite{xie2021oriented} on the DOTA dataset~\cite{xia2018dota}, and the corresponding experiment results are shown in \cref{tab:router}. When we add the spatial encoding modules, the performance can increase from 76.41\% to 76.80\%. This is because the additional convolution layer helps the routing function to capture the spatial orientation information from the feature maps. Meanwhile, the introduction of the adaptive combination could also get a 0.47\% reward in mAP, which shows the advantage of adopting the adaptiveness among different rotated kernels. When we use both of the designs, the oriented object detector achieves the highest performance. As a result, we choose to adopt both of the two designs into our proposed routing function.




\begin{table}[t]
\begin{center}
  \resizebox{0.45\textwidth}{!}{
  \begin{tabular}{c|c c c c}
    \toprule
    adaptive combination & \xmark & \xmark & \cmark & \cmark \\
    spatial info. encoding     & \xmark & \cmark & \xmark & \cmark \\
    \midrule
    mAP & 76.41 & 76.80 & 76.98 & \textbf{77.35} \\
    \bottomrule
  \end{tabular}
  }
\end{center}
\vspace{-0.2cm}
\caption{Ablation studies on the structure of the routing function. \emph{Adaptive combination} means using the combination weights prediction branch to adaptively combine the weight of each kernel, and \emph{spatial encoding} refers to adding a DWConv-LN-ReLU module before the average pooling layer.}
\label{tab:router}
\vspace{-0.4cm}
\end{table}


\subsection{Visualization}
\label{sec:exp_vis}


To give a deep understanding of our method, we visualize the predicted oriented bounding boxes and the corresponding scores. The experiment is conducted with Oriented R-CNN~\cite{xie2021oriented} detector on the test set of DOTA. By comparing the results between the detector with our proposed backbone and that with the baseline backbone in \cref{fig:vis_box}, the proposed method shows its superiority. To be specific, thanks to its adaptiveness in handling arbitrarily oriented object instances, our method has a superior locating and identification ability on both small (\eg ship in the third column), and median (\eg plane and harbor in the first two columns), as well large (\eg tennis court and soccer ball field in the last column) oriented object instances.
