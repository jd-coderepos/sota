

\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}              

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

\usepackage{microtype}      \usepackage{xcolor}         \usepackage{enumitem}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{wrapfig}
\usepackage{colortbl}

\DeclareMathOperator*{\argmax}{arg\,max}

\usepackage[normalem]{ulem}
\newcommand{\syh}[1]{{\color{green}#1}}
\newcommand{\syhd}[1]{{\color{green}\sout{#1}}}
\newcommand{\syhc}[2]{{\color{green}\sout{#1}#2}}

\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


\def\cvprPaperID{1447} \def\confName{CVPR}
\def\confYear{2023}


\begin{document}

\title{Open Vocabulary Object Detection with \\ Proposal Mining and Prediction Equalization}



\author{Peixian Chen\footnotemark[1], ~~
Kekai Sheng\footnotemark[1], ~~
Mengdan Zhang\footnotemark[4], ~~
Mingbao Lin, ~~ \\
Yunhang Shen, ~~ 
Shaohui Lin, ~~
Bo Ren, ~~
Ke Li, ~~ \\
[0.2cm]
 Tencent Youtu Lab  ~~~ 
 East China Normal University
}

\maketitle



\begin{abstract}
Although learning from a pre-trained vision-language model is efficacious for open-vocabulary object detection~(OVD) that identifies objects beyond the training vocabulary, two issues remain open, including proposal-level vision-language alignment and base-novel category prediction balance.
In this paper, we introduce a novel proposal \textbf{M}ining and prediction \textbf{E}qualization framework for open-vocabulary object \textbf{Det}ection~(MEDet) to alleviate these issues.
Specifically, we perform proposal mining by refining the inherited vision-semantic knowledge in a coarse-to-fine and online manner, allowing for detection-oriented proposal-level feature alignment.
Meanwhile, we equalize prediction confidence by reinforcing novel category predictions with an offline class-wise adjustment, permitting the overall OVD performance gains.
Extensive experiments demonstrate the superiority of MEDet over the state-of-the-art methods.
In particular, we increase AP of novel categories from  to  on MS COCO and obtain  mask AP on LVIS with gains of .
For the sake of reproducibility, code is anonymously released~\footnote{\url{https://github.com/peixianchen/MEDet}}.


\end{abstract}

 \renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[1]{These authors contributed equally to this work.}
\footnotetext[4]{Corresponding author.}
\section{Introduction}
\label{Sec:intro}
Developing object detection is one of the most elementary missions in computer vision. Though efforts have been made~\cite{CascadeRCNN,centernet,fasterrcnn}, the remarkable success mostly relies on the permit of accessing fully annotated datasets. However, annotating datasets requires a time laborious and lavish process, which barricades their scalability in category size and further prevents their applicability in real-world applications.
Fortunately, the recent prosperity of pre-trained vision-language models~(VLMs)~\cite{clip} opens a new horizon for open-vocabulary object detection~(OVD) that expands the detection vocabulary beyond the training categories.


\begin{figure}[!t]
    \centering
    \begin{subfigure}[b]{0.43\textwidth}
         \centering
         \includegraphics[width=\linewidth]{latex/image/RPNCLIP.pdf}
         \caption{}
         \label{fig:proposal_problem}
    \end{subfigure}
\begin{subfigure}[b]{0.43\textwidth}
         \centering
         \includegraphics[width=\linewidth]{latex/image/ConfidenceBias.pdf}
         \caption{}
         \label{fig:confidence_biase}
    \end{subfigure}
    \vspace{-1.0em}
    \caption{
    (a): Pre-trained CLIP model~\cite{clip} produces inaccurate proposal-concept pairs in the left image, which are well refined by our Online Proposal Mining~(OPM) in the right image.
    (b): Under confidence of novel category predictions is observed from the histograms of confidence scores on COCO~\cite{coco_zeroshot}.
}
    \vspace{-1.5em}
    \label{fig:teaser}
\end{figure}




Typically, OVD methods~\cite{ovrcnn, vild,regionCLIP,detic} are accomplished by first excavating an unbounded vocabulary of concepts as well as their general vision representations from image-caption pairs, and then transferring the general vision-language knowledge 
to the common detection learning process on the box-annotated data with base categories alone.
Albeit their promising progress, we realize that two critical facts are sorely neglected in most existing methods including \textit{proposal-level vision-language alignment and base-novel category prediction equalization}, 
which hamper the performance boost of existing OVD methods.






For the first issue, it is widely known that proposal annotations are the most beneficial hints for object detection. 
However, off-the-shelf studies dwell on weaker image-level vision-language annotations since most VLMs~\cite{clip,pixelbert,gao2022pyramidclip} are pre-trained upon a whole caption text describing an entire image.
Inaccurate proposal-concept pairs stem from a direct extension to excavate proposal-level vision-language annotations.
Left of Fig.\,\ref{fig:proposal_problem} illustrate an experiment \emph{w.r.t}. CLIP~\cite{clip} where two inaccurate proposal-concept pairs arise: (1) A big ambiguity stems from the matching between located proposals and concepts of ``person'' and ``skis''. (2) The proposal of ``person'' only encloses part of the person within the image. 
Therefore, simply transferring the coarse vision-language knowledge to OVD models~\cite{ovrcnn} or exploiting the noisy proposal-concept pairs for fine-grained detection model, even with distillation~\cite{vild}, may interfere with accurate object localization and degrade proposal-level vision-language alignment. 
We realize that opulent concepts describing proposal information are widely distributed in image-level caption texts. For example, Fig.\,\ref{fig:proposal_problem} is in line with the caption ``a person jumping a pair of skis in the air'' where ``person'' and ``skis'' are indeed fine-grained proposal concepts. 
Exploiting these proposal concepts might help overcome inaccurate proposal-concept pairs and uphold accurate object localization.
Recent Object Centric OVD~\cite{objcentric-ovd} solves this issue by training on filtered proposal-concept pairs. However, the performance gains rely on a pre-trained MViT detector~\cite{Maaz2022Multimodal} upon large-scale box annotations.










As for the second problem, we empirically observe that
existing optimized OVD models are prone to making imprecise predictions biased towards base categories. An illustrative example is given in Fig.\,\ref{fig:confidence_biase} where we count instances \emph{w.r.t}. the prediction score from Detic model~\cite{detic} on COCO dataset~\cite{coco_zeroshot}.
As we can observe that Detic model can well distinguish base categories such as ``person'' and ``car'' with most instances being given a high prediction score. On the contrary, novel categories like ``airplane'' and ``backpack'' are often endowed with a very low prediction score which indicates a large amount of mispredicted instances. Moreover, we empirically find that most mispredicted novel instances are grouped into base categories. Thus, the performance of OVD is still far from satisfactory for real-world applications.
Basically, we attribute the prediction bias issue to OVD training paradigm and class-imbalanced training datasets. 
For the training paradigm, as pointed out by previous works~\cite{ovrcnn,regionCLIP}, the detectors in OVD are fine-tuned upon base categories, which often leads to catastrophic forgetting of the general vision-language knowledge in pre-trained VLMs, in particular to novel categories.
As for class imbalance, OVD task requires abundant base training instances collected beforehand.
Nonetheless, training samples for novel categories are scarcely excavated. Then, it causes insufficient learning of proposal-level vision-language knowledge for novel categories and the optimized detectors are biased towards base categories in inference.



























In this paper, we present MEDet, a novel proposal \textbf{M}ining and prediction \textbf{E}qualization framework for open-vocabulary object \textbf{Det}ection, to overcome the above two issues. Fig.\,\ref{fig:framework} depicts the overall framework of our MEDet where proposal-level vision-language alignment and base-novel category prediction equalization are respectively accomplished via an online proposal mining (OPM) in the training and via an offline class-wise adjustment (OCA) in the inference.
The OPM filters out low-qualified alignments in a three-step coarse-to-fine fashion. 
It first augments text embedding discrimination by online interacting with the corresponding image embedding information via a cross-modality-based transformer block\,\cite{ViT}. 
Then, it further removes noisy proposal-concept pairs by building their semantic similarity distribution.
Finally, to clean proposal fragments holding broken objects, OPM further merge two proposals if they are of high overlapping.
Built upon OPM, an Iterative Matching with Recurrent Attention Memory (IMRAM)\,\cite{imram}  is used to discover the full latent proposal-level vision-language alignments, resulting in more accurate proposal-concept pairs than common pre-trained VLMs, as shown in the right of Fig.\,\ref{fig:proposal_problem}.








After training the OVD model, our OCA module further post-processes the trained OVD model for a better prediction. It performs density-based clustering~\cite{rodriguez2014clustering} upon all dataset proposals in compliance with the same predicted concept. And then a de-bias term vector is offline computed based on the clustering density and online deployed to adjust the prediction of an incoming proposal.
Experiments on COCO~\cite{coco_zeroshot} and LVIS~\cite{lvis} benchmarks demonstrate that our method outperforms other cutting-edge OVD methods~\cite{ovrcnn,vild,detic,regionCLIP}.
For example, MEDet reaches  AP50 for novel categories on the COCO dataset, which considerably suppresses the \textit{state-of-the-art} approaches by . 









\begin{figure*}
     \centering
     \includegraphics[width=0.9\linewidth]{latex/image/framework.pdf}
     \vspace{-0.5em}
     \caption{The framework of our MEDet.
     With a mini-batch of data from both the detection dataset and the image-caption dataset, MEDet jointly trains object detection (\textcolor[RGB]{17,160,255}{blue} part) and learns rich proposal-level vision-language knowledge by conducting online proposal mining (\textcolor[RGB]{239,115,33}{orange} part) in an end-to-end manner. After the standard detection inference (\textcolor[RGB]{153, 204, 153}{green} part), we propose offline class-wise adjustment to handle the confidence bias between the base and novel categories. The \textbf{\textcolor[RGB]{10,112,192}{}} means a frozen network. The \textbf{\textcolor[RGB]{255,232,0}{}} means contributions of this paper. }
\label{fig:framework}
     \vspace{-1.0em}
\end{figure*}












%
 




\section{Related work}

\textbf{Open-Vocabulary Object Detection}.
Typically, researchers scale up the vocabulary size for object detection by exploiting rich knowledge within pre-trained VLMs~\cite{clip,pixelbert,gao2022pyramidclip}.
OVR-CNN~\cite{ovrcnn} first learned a projection layer behind the backbone of Faster R-CNN~\cite{fasterrcnn} to align visual space with textual space~\cite{pixelbert}, and then fine-tuned the detector with only base categories.
Distillation-based approaches~\cite{vild,zsdyolo} aligned the vision extractors with both image and text encoders of CLIP~\cite{clip}. 
RegionCLIP~\cite{regionCLIP} first leveraged CLIP model to match image regions with template captions, then pre-trained the model to align the region-concept pairs in the feature space and finally transferred the pre-trained model to downstream object detection.
However, its training pipeline is complex, and the pseudo region-concept pairs are noisy.
In this paper, we propose online proposal mining~(OPM) within an end-to-end OVD network and simplify the training pipeline.
Moreover, OPM provides more reliable region-concept pairs for acquiring rich proposal-level vision-language knowledge.
Recently, Zhou~\etal\cite{detic} applied additional
image-level supervision (\textit{e.g.}, predefined classification categories) to the largest proposal and did not supervise other outputs for image-labelled data. 
A limitation is that it requires image-level annotations within a predefined hand-crafted taxonomy and only learns the classes within the taxonomy. 

\textbf{Debiasing Strategies in Object Detection}.
Real-world detection datasets~\cite{coco_zeroshot,lvis,pan2020DRN} typically exhibit class-imbalance label distributions, making it difficult to learn generalized representation across classes.
Existing debiasing approaches can be roughly categorized into four mainstreams: (1) re-weighting~\cite{chang2021resampling,lvis,shen2016relay} to sample more instances of rare classes; 
(2) 
cost-sensitive learning~\cite{tan2020equalizationloss,tan2021equalizationlossv2,wang2021seesawloss} to spare more attention on hard examples;
(3) knowledge transfer~\cite{jamal2020rethinking,kang2019decoupling,liu2019large} to learn generalized representation progressively;
and (4) post calibration~\cite{li2020balancedgroupsoftmax,menon2020logitadjustment,pan2021norcal,tang2020causal} to refine the model's output during the inference. To apply the above methods, we need the information on the rare target classes (\emph{e.g.}, class frequency) in advance.
Unfortunately, 
in the setting of OVD~\cite{vild,ovrcnn}, we do not have access to the 
definition of \textit{novel} categories during training, let alone class frequencies. The scarcity of information makes it difficult and ineffective to leverage the popular imbalance debiasing schemes. Besides, the prediction bias also results from the training strategies of OVD models. Existing methods adopt model freezing~\cite{ovrcnn} and focal scaling~\cite{regionCLIP} to alleviate the 
catastrophic  
forgetting the general knowledge in the pre-trained VLMs when fine-tuning the detectors on the base categories.


%
 \section{Methodology}
\label{sec:Method}



\subsection{Preliminary}
\label{sec:Preliminary}

Considering a base dataset for object detection  where  denotes the -th image containing  bounding box and class pairs , and  denotes the base category set, OVD intends to learn a detector that recognizes well-unseen novel categories  with .
To this end, an image-caption dataset  is introduced in VLMs-based OVD methods~\cite{ovrcnn, detic,regionCLIP} in which  is a caption sentence describing the image ,  is a set of word concepts (nouns) extracted from , and  is the union of all concepts in .
Then, the OVD models are trained upon both  and . Typically, the overall training loss in existing studies can be generally formulated as follows:

where  denotes the constraints for RPN,  is the classification loss and  regularizes the bounding box regression. These first three constraints are enforced upon the base dataset . As for , it represents the binary-cross entropy (BCE) applied to model embedding similarity of image  and its caption sentence .










\subsection{Framework of MEDet}
\label{sec:LearningMEDet}

MEDet models proposal-level vision-language alignment in the training stage by an online proposal mining~(OPM) mechanism in Sec.\,\ref{sec:OPM} and achieves base-novel category prediction equalization in the testing stage via an offline class-wise adjustment~(OCA) in Sec.\,\ref{sec:CBA}.
Before diving into a detailed discussion, we first outline the framework of our MEDet, as illustrated in Fig.\,\ref{fig:framework}.


In the training stage, for incoming mini-batch data: 
\textbf{First}, we feed to a text encoder the category concepts of the base dataset and word concepts from current image-caption mini-batch samples to obtain their text embeddings  and .
Similarly, we have image embeddings  and  from an image encoder. 
\textbf{Second}, for the text embeddings  and , we boost their discriminative ability by injecting image information in Sec.\,\ref{sec:OPM}, results of which are denoted as  and . 
\textbf{Third}, the base data (, ) are used for a standard two-stage detection training~\cite{fasterrcnn}.
Considering the inaccessibility of bounding boxes, the image-caption pair (, ) further goes through a series of noise removal and fragment mergence in Sec.\,\ref{sec:OPM} 
to derive a concept set  and a proposal set  for the -th image . Here, the -th concept  pairs with  proposal embeddings  where .
Also, we let  and  denote all concept sets and proposal sets in  except these from the -th samples.


With this proposal-level vision-language knowledge, the most common way to align these pairs is through one-by-one contrastive loss~\cite{regionCLIP}, which however suffers from performance damage if mismatched pairs stem from the knowledge set. To avoid this obstacle, we adopt the Iterative Matching with Recurrent Attention Memory (IMRAM)~\cite{imram} to mitigate the negative impact. 
IMRAM considers two independent Recurrent Attention Memory (RAM) blocks, \ie,  and , to augment  and  in an iterative manner. The -th step is briefly described as:

where  is a reconstruction of  established upon the concept set  and similar for . The cross-modal cosine similarity is computed as:




Here,  is built upon the concept set , therefore, the similarity is modeled between the proposal embedding  and all the concepts, leading to a major difference to the one-by-one contrastive loss~\cite{regionCLIP}, details of which can be referred to~\cite{imram}. 
The final similarity between the set pair  and  is: .
Similarly, we obtain the similarity of  and , both of which play as negative factors to raise up positive factor of  in a contrastive style:

where ,  is the margin, and  is the training batch size.
Combining Eq.\,(\ref{all}) and Eq.\,(\ref{equ:imram}) leads to our final training objective:




As for the inference stage, we devise an offline class-wise adjustment in Sec.\,\ref{sec:CBA} to accomplish the goal of base-novel category prediction equalization.


















\subsection{Online Proposal Mining}
\label{sec:OPM}


In this subsection, we formally introduce our online proposal mining (OPM) that extracts fine-grained proposal-level vision-language knowledge from the image-caption dataset  to promote detection performance.
Our OPM is a three-step pipeline including concept augmentation, noisy removal and fragment mergence.



\textbf{Concept Augmentation}. Current OVD implementations~\cite{regionCLIP,vild,detic} train detectors by embedding a concept as the results of a prompt template, \emph{e.g.}, \textit{\underline{It is a photo of} [concept]}. The shared prompt templates lead to less discriminative text features since images are much more diverse~\cite{he2021masked} compared to highly-semantic and information-dense texts. 
Though prompt ensemble~\cite{vild} or prompt design~\cite{DetPro} might be an alternative, they cannot eliminate the information variance between texts and images.
To mitigate this issue, at the end of the backbone, we append a cross-modality attention-based transformer block~\cite{ViT} to enhance the text embedding discrimination by injecting image embedding information. 


Giving text embeddings   in a batch, the augmentation is dynamically adopted for each image :

where  denotes cross attention of different modalities;  is the feed-forward network consisting of two linear layers; ,  and  represent projection matrices. For a brief description, we omit the subscript  for each image.
Given that the base image-category pair  are accomplished with bounding box information, they are directly used for a standard two-stage detection training as stated in Sec.\,\ref{sec:LearningMEDet}.
Differently, we further refine image-caption (, ) by noise removal and fragment mergence due to their invisibility of bounding boxes.



\textbf{Noise Removal}.
Considering an image , we have its augmented concept embedding set  and image-level embedding . Then, as shown in Fig.\,\ref{fig:framework},  is sent to the RPN to derive a proposal set  where  is the feature embedding of the -th proposal,  and  are its coordinates and objectiveness score. To match a proposal  with an accurate feature embedding , one naive approach is to consider the cosine similarity. However, as shown in Fig.\,(\ref{fig:proposal_problem}), highly-overlapped proposals are often identified as different categories due to semantic confusion. Instead, we introduce a similarity entropy measurement.


We first measure the semantic similarity between  and  in relation to their cosine similarity and objectiveness score of  as:

which is then normalized by the softmax function to obtain the similarity entropy for the proposal embedding  as:





Here, we would like to stress the efficacy of . A large  value manifests the proposal embedding  is almost equally matched with every concept embedding, which in turn indicates poor proposal mining.
On the contrary, a small  means  is well matched with some particular concept, in which case  should be preserved.

To this end, we regard the entire image as a proposal and first discard these proposals whose similarity entropy is larger than that of the image proposal since an entire image is a very rough proposal therefore any poorer proposal can be safely removed. Next, we match each concept embedding  with proposals of the top- largest semantic similarity to form positive proposal-level vision-language pairs. 
Lastly, we further filter out pairs for each concept whose largest semantic similarity is smaller than that between the concept and the image proposal.
Note that, we do not calculate the similarity entropy for each concept like that for each proposal in Eq.\,(\ref{equ:SimilarityEntropy}) to delete  unreliable proposal-concept pairs, mostly because each concept often matches several different proposals in the OVD task. Therefore, the value of similarity entropy fails to reflect the quality of concepts.






\textbf{Fragment Mergence}.
Albeit the removal of noisy proposal-concept pairs, it remains some fragments that merely contain part of an object. Therefore, we propose to merge these fragments in compliance with their spatial relationships. For each concept , we compute the IoU (intersection over union) between its two matched proposals  and  as the spatial similarity evaluation.
These two proposals are retained if IoU is below a threshold  and merged into a larger one otherwise. We repeat these steps until no proposals can be merged, 
obtaining the final concept set  and proposal set  for the -th image  as utilized in Sec.\,\ref{sec:LearningMEDet}.












\subsection{Offline Class-wise Adjustment}
\label{sec:CBA}












Suppose a total of  concept embeddings are obtained upon the whole training set, denoted as . For an incoming proposal embedding  in inference, OVD performs a linear projection from visual features to concept embeddings and prediction scores of  are obtained as:


Nevertheless, as analyzed in Sec.\,\ref{Sec:intro}, the training paradigm and class-imbalanced datasets cause immoderate reliance of OVD models on the base dataset and damage the performance. 
Such OVD settings as well as the scarcity of information (\emph{e.g.}, the class name and frequency) of  to test in inference, hinder the deployment of debiasing methods~\cite{tan2020equalizationloss,chang2021resampling,li2020balancedgroupsoftmax} for a better training paradigm.














Inspired by ~\cite{menon2020logitadjustment} where the Bayes-optimal problem is analyzed in a post-hoc adjustment manner, we propose an offline class-wise adjustment~(OCA) to post-process the trained OVD model for a better prediction. For easy understanding, we first give our refined concept prediction of  as:

where  denotes our de-bias term and  is a scalar to control de-bias degree.


After training, for the -th concept  associated with its proposal embedding set  extracted from the entire training set, we perform density-based clustering~\cite{rodriguez2014clustering} which automatically forms  cluster centers upon these proposals , leading to an average of  proposal density for each cluster where  since some outlier proposals will be removed by~\cite{rodriguez2014clustering}. 


Then, the -th de-bias term  for concept  is computed:



We observe many common concepts such as ``dog'' and ``person'', embrace diverse images and result in a very large  than others. The square root operation well prevents these illegitimate clustering. Our de-bias design is built upon a simple posterior truth:
a large number of proposals will be mistakenly attributed to concept  to which we suppose the trained OVD model is prone, leading to either a large  or  and finally a stronger  . Therefore, our design in Eq.\,(\ref{de-bias}) decreases the tendency of mistaken predictions.





Notice we do not conduct any optimization, thus our OCA is flexible for any novel categories. Besides, the estimation of de-bias term  can be offline implemented once-for-all. Thus, it does not increase any computation burden in inference. Importantly, we find the calculated  can be well reused on other OVD methods as listed in Tab.\,\ref{tab:CAug_CBA}.


















%
 





\section{Experiments}
\label{sec:experiments}

\subsection{Setup}
\textbf{Datasets \& Metric}.
We evaluate our method on two standard open-vocabulary detection benchmarks modified from COCO~\cite{mscoco} and LVIS~\cite{lvis}. COCO Caption~\cite{cococaption} and Conceptual Caption (CC)~\cite{CC3M} are used respectively for OVD on COCO and LVIS to learn a large vocabulary of concepts .
COCO Caption has the same images and train/test split as the COCO Object dataset, which has  images and  captions.
We parse the captions by Scene-Graph-Parser~\cite{schuster2015scenegraph} and get  noun concepts for COCO Caption and  noun concepts for CC. 
On COCO, we follow the data split of~\cite{ovrcnn} with  base categories  and  novel categories , which are subsets of  COCO object classes. The rest  categories  are also evaluated to further investigate the generalization of OVD models. On LVIS, following~\cite{vild}, we use the training/validation images and adopt the category split with  base categories (common and frequent objects) and  novel categories (rare objects). 
We adopt the standard object detection metrics: mean Average Precision (mAP) and AP50
. On COCO, we mimic the generalized setting~\cite{vscoco_zeroshot} and report AP50s for base and novel categories.
On LVIS, we use a standard class-agnostic mask head~\cite{MaskRCNN} to produce segmentation masks for boxes. Following~\cite{detic}, the mask mAPs for novel categories and all categories are used for evaluation.



\begin{table}[t]
    \centering
    \caption{Results of OVD on COCO dataset~\cite{coco_zeroshot}.
    MEDet equipped with online proposal mining and offline class-wise adjustment outperforms other methods on the novel categories.}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{l|cc|ccc}
        \toprule
        \multirow{2}{*}{Method} & \multicolumn{2}{c|}{Detector Training}  & \multicolumn{3}{c}{COCO Generalized (48+17)} \\
        &Backbone &Box generator  & Novel & \color{gray}{Base} & \color{gray}{All} \\
        \midrule
        Base-only (CLIP) & - & -  & 1.3 & \color{gray}{48.7} & \color{gray}{39.3} \\
        WSDDN~\cite{wsddn} & - & - & 20.5 &\color{gray}{23.4} & \color{gray}{24.6} \\
        Cap2Det~\cite{ye2019Cap2Det} & - & -  & 20.3 & \color{gray}{20.1}  & \color{gray}{20.1} \\
        PL~\cite{rahman2020PL} & RN50-FPN & COCO Base (48) & 4.12 & \color{gray}{35.9}  & \color{gray}{27.9} \\
        OVR-CNN~\cite{ovrcnn} & RN50-C4 & COCO Base (48) & 22.8 & \color{gray}{46.0}  & \color{gray}{39.9} \\
        HierKD~\cite{hierKD} & RN50-C4 & COCO Base (48) & 20.3 & \color{gray}{51.3}  & \color{gray}{43.2} \\
        ViLD~\cite{vild} & RN50-FPN & COCO Base (48) & 27.6  & \color{gray}{59.5} & \color{gray}{51.3}\\
        RegionCLIP~\cite{regionCLIP} & RN50-C4 & LVIS (1203) & 26.8 &  \color{gray}{54.8}  & \color{gray}{47.5}\\
        Detic~\cite{detic} & RN50-C4 & COCO Base (48)  & 29.1 &  \color{gray}{52.4} &\color{gray}{46.2}\\
        \midrule
        \rowcolor{gray!25} MEDet (Ours) &RN50-C4 & COCO Base (48) & \textbf{32.6} & \color{gray}{53.5} & \color{gray}{48.0} \\
        \bottomrule
    \end{tabular}
    }
    \label{tab:on_COCO}
\end{table}




\textbf{Implementation Details}.
\label{sec:implementation_details}
We leverage the CLIP text-encoder~\cite{clip} on ViT-B-32~\cite{dosovitskiy2020vit} to convert concepts to text embeddings. 
For COCO, we use Faster R-CNN~\cite{fasterrcnn} with the RN50-C4 configuration and train  iterations of batch size  for the detection data and batch size  for the caption data on 8 V100 GPUs. The learning rate is initially  and multiplied by  at the  and  steps.
For LVIS, we use CenterNet2~\cite{centernet} with the RN50-FPN architecture and the same data augmentation and learning schedules. 
All models are pre-trained for  iterations on the detection dataset and the caption dataset using loss  in Eq.\,(\ref{all}).

To merge fragmented proposals, we set  to . And we set the margin  of  to .
In OCA, we set  as .
For the detailed ablation results, please refer to Sec.\,\ref{sec:ablation_investigation}.

\subsection{Main Results}
\label{sec:main_results}
\textbf{OVD on COCO}.
The comparisons of OVD results on the COCO dataset are shown in Tab.\,\ref{tab:on_COCO}.
``Base-only'' means training Faster R-CNN only on the detection data of base categories, and using text embeddings of class names from CLIP to replace the classifier's weights.
Compared with weakly supervised methods such as WSDDN~\cite{wsddn} and Cap2Det~\cite{ye2019Cap2Det}, and zero-shot methods PL~\cite{rahman2020PL}, our MEDet obtains a significant improvement on all metrics.
Compared with the OVD method OVR-CNN~\cite{ovrcnn}, our MEDet also demonstrates superiority (\emph{e.g.},  \vs  on ).




\begin{table}[t]
   \centering
   \caption{Results of OVD on LVIS dataset~\cite{lvis}. MEDet better explores proposal-level vision-language knowledge on Conceptual Caption dataset and diminishes prediction bias among categories.}
    \footnotesize{
    \begin{tabular}{l|ccc}
        \toprule
Method & Distill & Novel & \color{gray}{All} \\
        \midrule
        WSDDN~\cite{wsddn} &  & 16.5  &\color{gray}{30.0}  \\
        ViLD~\cite{vild}   &  & 16.8  &\color{gray}{25.2}  \\
        RegionCLIP~\cite{regionCLIP} &  & 17.1  &\color{gray}{28.2}  \\
        DetPro~\cite{DetPro} &  & 19.8 &\color{gray}{25.9} \\
        Detic~\cite{detic} &  & 21.0  &\color{gray}{30.9}  \\
        \midrule
\rowcolor{gray!25} MEDet (Ours) &   & \textbf{22.4}  &\color{gray}{34.4} \\
        \bottomrule
    \end{tabular}
    \label{tab:on_LVIS}
    }
\end{table}


\begin{table}[t]
    \centering
    \caption{
    A comparison of generalization capability of OVD methods on retained 15 categories of COCO dataset~\cite{mscoco}.}
    \footnotesize{
    \begin{tabular}{l|ccc}
        \toprule
        Method & Retain & Novel & \color{gray}{All} \\
\midrule
        OVR-CNN~\cite{ovrcnn}  & 11.5  & 22.9  & \color{gray}{38.1}  \\
        Detic-80~\cite{detic} & 11.5 & 27.3  & \color{gray}{38.3}  \\
        Detic-65~\cite{detic}  & 5.2 & 9.2  & \color{gray}{34.1} \\
        \midrule
        \rowcolor{gray!25} MEDet (Ours) & \textbf{18.6} & \textbf{32.6} & \color{gray}{42.4} \\
        \bottomrule
    \end{tabular}
    }
    \label{tab:coco_other_15}
\end{table}



As for the performance on \emph{novel} categories , the core in OVD task, even using a weaker configuration the proposed MEDet outperforms other methods. For example,
ViLD~\cite{vild} adopts advanced training strategies (\emph{e.g.}, model distillation, model ensemble, and data augmentation), yet MEDet is substantially better on  ( \vs ) and still competitive on  with a weaker backbone (RN50-C4 \vs RN50-FPN) and a simple training scheme.
RegionCLIP~\cite{regionCLIP} uses a stronger box generator trained on the large box-supervision dataset LVIS for generating proposal-concept pairs, while our MEDet conducts online proposal mining merely on the COCO dataset and acquires better proposal-level vision-language alignment, achieving  AP50 gain on .
The results of Detic~\cite{detic} are reproduced based on the official implementation and it labels the caption data with all the 80 categories in COCO.
Contrastively, without knowing novel categories beforehand in the training, MEDet achieves higher results on all metrics (\emph{e.g.},  \vs  on ).



\textbf{OVD on LVIS}.
To further verify the competitiveness of our method, we conduct comparison experiments on the LVIS benchmark~\cite{lvis}, as shown in Tab.\,\ref{tab:on_LVIS}.
Compared to recent OVD methods~\cite{regionCLIP,vild} that learn general proposal-level vision-language knowledge via distillation from CLIP, our MEDet online explores such knowledge  on the Conceptual Caption dataset in a self-driving way and
obtains significant improvements on the  novel categories  ( \vs , ).
Besides, our method also outperforms Detic ( \vs ) without requiring one hand-crafted taxonomy. The comparisons demonstrate the effectiveness and flexibility of the proposed MEDet in the OVD scenario.

\textbf{Generalization of MEDet}.
Besides the evaluation experiments on  and , we also analyze the performance on the retained  classes  in the COCO dataset~\cite{coco_zeroshot} to further investigate the generalization ability of OVD models.
Tab.\,\ref{tab:coco_other_15} lists the results.
Note that, when using COCO Caption, Detic~\cite{detic} requires a hand-crafted taxonomy to map each concept to one class in a set .
``Detic-80'' uses  ( classes) as  
and works well on . But when  is  (65 classes), ``Detic-65'' performs worse in novel categories.
In contrast, our MEDet works better on both  and . The results ensure that the proposed method utilizes the caption dataset more effectively for generalization toward the OVD setting. 


\begin{figure*}[t]
   \centering
   \begin{subfigure}[t]{0.22\textwidth}
        \centering
        \includegraphics[width=\linewidth]{latex/image/OPMsteps0.pdf}
        \caption{Original Images}
        \label{fig:proposal_opm_OriginalImage}
   \end{subfigure}
   \begin{subfigure}[t]{0.22\textwidth}
        \centering
        \includegraphics[width=\linewidth]{latex/image/OPMsteps1.pdf}
        \caption{Proposal Filtering}
        \label{fig:proposal_opm_ProposalFilter}
   \end{subfigure}
   \begin{subfigure}[t]{0.22\textwidth}
        \centering
        \includegraphics[width=\linewidth]{latex/image/OPMsteps2.pdf}
        \caption{Top-3 Proposals}
        \label{fig:proposal_opm_top3}
   \end{subfigure}
   \begin{subfigure}[t]{0.22\textwidth}
        \centering
        \includegraphics[width=\linewidth]{latex/image/OPMsteps3.pdf}
        \caption{Proposal Merging}
        \label{fig:proposal_opm_ProposalMerge}
   \end{subfigure}
   \caption{Qualitative results of each step in OPM.
   (b) \textbf{Noise Removal}: Filter top-100 proposals from RPN by similarity entropy, where \textcolor[RGB]{153,204,153}{green} and \textcolor{gray}{gray} boxes are preserved and removed proposals, respectively.
   (c) \textbf{Noise Removal}: Select proposals with top-3  scores for every concept.
   (d) \textbf{Fragment Mergence}: Merge proposals to eliminate fragmented proposals.}
   \label{fig:proposal_opm}
\end{figure*}



\subsection{Ablation Studies}
\label{sec:ablation_investigation}
\textbf{Components of MEDet}.
Tab.\,\ref{tab:ablation_on_MEDet} shows the ablation studies of components in MEDet. The pre-trained model only uses constraints , and is trained  steps without Online Proposal Mining (OPM) and Offline Class-wise Adjustment (OCA).
When we apply OPM, the AP50 on novel categories reaches  (). 
The performance is also higher than other OVD methods in Tab.\,\ref{tab:on_COCO}. It means that OPM effectively explores proposal-level vision-language knowledge. 
In the third row, the OCA handles the confidence bias and thus boosts the performance on novel categories by . Lastly, we replace the  matching loss~\cite{imram} (\emph{i.e.}, Eq.\,(\ref{equ:imram})) with a common grounding loss used by~\cite{ovrcnn} to verify the rationality of . Although the performance on novel categories drops by  ( \vs ), it is also competitive compared with the other methods in Tab.\,\ref{tab:on_COCO}.


\begin{table}[t]
    \centering
    \caption{Ablation studies of MEDet. Lines~2--3 show the increase in accuracy after the addition of our proposed OPM and OCA modules. For `MEDet w/o ' we use a cross-modal attention used in OVR~\cite{ovrcnn} to learn the vision-language knowledge.}
    \footnotesize{
        \begin{tabular}{l|c|c|c}
            \toprule
            Method & Novel & \color{gray}{Base} & \color{gray}{All} \\
            \midrule
            Pre-trained model &17.6 &\color{gray}{43.3} &\color{gray}{36.6} \\
+ OPM  & 32.0 &\color{gray}{53.1} &\color{gray}{47.5}  \\
            + OPM + OCA (MEDet)  &32.6 &\color{gray}{53.4} &\color{gray}{47.9}  \\
            \midrule
            MEDet w/o  &30.8 &\color{gray}{52.8} &\color{gray}{47.0} \\
            MEDet &32.6 &\color{gray}{53.4} &\color{gray}{47.9} \\
            \bottomrule
        \end{tabular}
        \label{tab:ablation_on_MEDet}
        }
\end{table}

\textbf{Effectiveness of OPM}.
In Tab.\,\ref{tab:ablation_on_OPM}, we remove the OCA module to verify the rationality of OPM. 
The result shows that Concept Augmentation can effectively consider the semantic relationship among different concepts and the image embedding to adapt the text embeddings online. When used, the result on novel categories is increased by . 
When removing the Noise Removal step, we directly use proposals of the top-100 objectness scores to align with concepts. The result on novel categories is only  () due to noisy proposals. And disabling merging fragmented proposals may lose some large proposals, thus the performance decreases by  ( \vs ).

In order to further demonstrate the effectiveness of the OPM, we  show the qualitative results of each step in OPM.  Fig.\,\ref{fig:proposal_opm_ProposalFilter} shows the result after proposal filtering via similarity entropy( Eq.\,(\ref{equ:SimilarityEntropy})). The gray proposals are obtained according to the top-100 objectness scores from RPN. The green proposals are retained after removing incorrect proposals. 
Fig.\,\ref{fig:proposal_opm_top3} shows proposals of the TOP-3  scores ( Eq.\,(\ref{equ:sc})) for every concept. Actually, we can select proposals for objects of different sizes and types. Fig.\,\ref{fig:proposal_opm_ProposalMerge} shows the refined proposals after fragment mergence. In a word, our OPM removes the most noise and obtains more reliable proposal-level vision-language knowledge.

\begin{table}[t]
        \centering
        \caption{Effectiveness of each step in OPM.}
        \footnotesize{
         \begin{tabular}{l|c|c|c}
            \toprule
            Strategy & Novel & \color{gray}{Base} & \color{gray}{All} \\
            \midrule
            w/o Concepts Augmentation & 31.0 &\color{gray}{52.9} &\color{gray}{47.2} \\
            w/o Noise Removal &29.3 &\color{gray}{52.4} &\color{gray}{46.5} \\
            w/o Fragment Mergence &30.4 &\color{gray}{52.8} &\color{gray}{47.0} \\
            \midrule
            OPM  &32.0 &\color{gray}{53.1} &\color{gray}{47.5}\\
            \bottomrule
        \end{tabular}
        }
        \label{tab:ablation_on_OPM}
\end{table}
    
\begin{table}[t]
    \centering
    \caption{Effectiveness of OCA introduced to other OVD methods.}
    \footnotesize{
    \begin{tabular}{l|c|c}
            \toprule
            Method & Novel & \color{gray}{All} \\
            \midrule
            OVR-CNN~\cite{ovrcnn} &22.8	&\color{gray}{44.3} \\
OVR-CNN~\cite{ovrcnn} + OCA &25.8	& \color{gray}{45.3} \\
            \midrule
            Detic~\cite{detic} &28.7	&\color{gray}{45.1} \\
Detic~\cite{detic} + OCA &29.8	&\color{gray}{46.8} \\ 
            \bottomrule
    \end{tabular}
    }
    \label{tab:CAug_CBA}
    \vspace{-1em}
    
\end{table}

\textbf{Effectiveness of OCA}.
Tab.\,\ref{tab:CAug_CBA} shows the improvement of OCA when it is used in different OVD models.
As the results of rows 2--4 (+ OCA), the improvements in both novel and base categories are generally higher than . 
Notably, in the OCA experiment, we don't recalculate the bias according to different models. Instead, the bias   obtained from our MEDet model is reused directly to the inference stage of OVR-CNN and Detic, which fully demonstrates that the bias of concepts obtained from OCA can be reused by other OVD methods when training on the same caption dataset. 

To further understand the rationale and effectiveness of the proposed OCA, we analyze the AR, and AP, and estimate  for each category. The results are shown in Fig.~\ref{fig:AR_AP_beta_CBA}.
As we observe that: for the category with lower , such as umbrella (id 21,  AP50,  AR, ) and scissors (id 63,  AP50,  AR, ), the optimized model generates under-confident predictions, and generally has sub-optimal AP or AR performance, thus we should conduct less debiasing adjustment  on it. After adjusting by OCA, the AP and AR on novel categories are improved, especially AR.
On the other hand, for the category with higher , such as ``person'' (id 0,  AP50,  AR, )), the model produces over-confidence outputs, and thus has higher AR performance but inferior AP performance.
So we need to conduct more debiasing adjustments on it.
After using OCA, the AP and AR on base categories are also improved. 
With the help of our OCA, we handle the confidence bias well and promote the overall OVD result.






\begin{figure}[tbp]
  \centering
  \begin{subfigure}[t]{.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{latex/image/AP_AR_woCBA.pdf}
        \caption{}
        \label{fig:AP_AR_woCBA}
  \end{subfigure}
  \begin{subfigure}[t]{.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{latex/image/AP_AR_withCBA.pdf}
        \caption{}
        \label{fig:AP_AR_with_CBA}
  \end{subfigure}
  \caption{\textbf{The AP, AR, and  for each category on COCO~\cite{coco_zeroshot} when \textbf{(a)} we don't apply OCA or \textbf{(b)} we apply OCA.}
  The results indicate the positive contribution from OCA to the OVD task.}
  \label{fig:AR_AP_beta_CBA}
\end{figure}


\subsection{Hyper-parameter Analysis}

\textbf{Different  in OPM}.
To investigate the  in OPM, we analyze the AP50 results on COCO dataset with varied  in Tab.~\ref{tab:ablation_on_IOU}. The observations show that a small threshold () causes too many proposals to be merged and the accuracy is reduced. And a high threshold () makes only a few (or no) proposals be merged.



\textbf{Different  in OCA}.
To investigate the optimal  of Eq.(\ref{eq:PDB}) in OCA, we analyze the OVD performance with varied . The AP50 results on COCO dataset~\cite{coco_zeroshot} are listed in Tab.\,\ref{tab:effect_of_gamma_in_CBA}.
The observations show the optimal  and indicate that the positive effect of OCA is relatively insensitive to .

\begin{table}[t]
    \centering
    \caption{The results of various  in OPM on COCO.}
    \footnotesize{
    \begin{tabular}{l|ccccc}
        \toprule
         & w/o & 0.2 & 0.4 & 0.6 & 0.8 \\
        \midrule
        Novel & 30.4 &30.5  &30.8  & 31.0 &30.4  \\
        \color{gray}{Base}  & \color{gray}{52.8} &\color{gray}{52.6}  &\color{gray}{52.6}  & \color{gray}{52.9} &\color{gray}{52.0}  \\
\bottomrule
    \end{tabular}
    }
    \label{tab:ablation_on_IOU}
    \vspace{-1em}
    
\end{table}
\begin{table}[t]
    \centering
    \caption{The results of various  in OCA on COCO.}
    \footnotesize{
    \begin{tabular}{l|cccccc}
        \toprule
         & 0.0 & 0.2 & 0.4 & 0.6 & 0.8 & 1.0 \\
        \midrule
        Novel & 32.0 & 32.5 & 32.6 & 32.6 & 32.6 & 32.4 \\
        \color{gray}{Base}  & \color{gray}{53.1} & \color{gray}{53.3} & \color{gray}{53.4} & \color{gray}{53.5} & \color{gray}{53.4} & \color{gray}{53.3} \\
        \bottomrule
    \end{tabular}
    }
    \label{tab:effect_of_gamma_in_CBA}
\end{table}










%
 \section{Conclusion}


In this paper, we present MEDet, a novel and effective framework to solve the two observable challenges for better OVD results.
To sharpen the vision-language knowledge of caption datasets from image-level to proposal-level and learn an unbounded vocabulary of concepts, we devise an online proposal mining scheme in an end-to-end network to fully explore the knowledge of caption data. Moreover, we propose an offline class-wise adjustment to generate accurate predictions on both base and novel categories with less bias.
Experiments on popular COCO and LVIS benchmarks verify that our MEDet outperforms the counterpart approaches in detecting objects of novel categories.
%
 
{\small
\bibliographystyle{ieee_fullname}
\bibliography{PaperForReview}
}

\end{document}
