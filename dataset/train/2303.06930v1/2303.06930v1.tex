\section{Experiments}
\label{sec:exp}
In this section, we conduct experiments on multiple benchmark datasets with simulated and real-world label noises. We strictly follow the experimental settings in previous literature~\cite{ortego2021multi,li2020dividemix,li2021learning,liu2020early} for fair comparisons.

\subsection{Experiments on simulated datasets}

\paragraph{Datasets.} 
Following~\cite{ortego2021multi,li2020dividemix,li2021learning,liu2020early}, we validate our method on \bd{CIFAR-10/100}~\cite{krizhevsky2009learning}, which contains 50K and 10K images with size  for training and testing, respectively. We leave 5K images from the training set as the validation set for hyperparameter tuning, then train the model on the full training set for fair comparisons.
Two types of label noise are simulated: \emph{symmetric} and \emph{asymmetric} label noise. Symmetric noise randomly assigns the labels of the training set to random labels with predefined percentages, \textit{a.k.a}, noise ratio, which includes 20\%, 50\%, 80\%, and 90\% on two datasets in this paper. Asymmetric noise takes into account the class semantic information, and the labels are only changed to similar classes (\eg, truck  automobile). Here, only experiments on the CIFAR-10 dataset with 40\% noise ratio for asymmetric noise are conducted; otherwise, the classes with above  label noise cannot be distinguished.

\begin{table*}[t]
    \centering
    \caption{
        Comparisons with state-of-the-art methods on simulated datasets.
        The results for previous methods are copied from~\cite{li2021learning,li2022selective} to avoid the bias of self-implementation, and we strictly follow their experimental settings.
        Each runs has been repeated 3 times with different randomly-generated noise and we report the mean and std values of \emph{last } 5 epochs.
    }
    \label{tab:results_on_cifar}
    \begin{tabular}{rrrrrrrrcrrrr}
    
    & \multicolumn{6}{c}{\bd{CIFAR-10}} & & \multicolumn{5}{c}{\bd{CIFAR-100}} \\
    \multirow{2}{*}{Noise type/rate} & \multicolumn{4}{c}{\textit{Sym.}} & \textit{Asym.} & \textit{Avg.} & & \multicolumn{4}{c}{\textit{Sym.}} & \textit{Avg.} \\ 
    \cmidrule{2-7}\cmidrule{9-13}
    & 20\% & 50\% & 80\% & 90\% & 40\% & & &20\%  & 50\%  & 80\%  & 90\% & \\
    \shline
    Cross-Entropy        & 82.7  & 57.9  & 26.1  & 16.8  & 76.0 & 51.9 & & 61.8  & 37.3  & 8.8 & 3.5 &  27.8 \\
    Mixup~(17')~\cite{zhang2017mixup} & 92.3  & 77.6  & 46.7  & 43.9  & 77.7 & 67.6 & & 66.0  & 46.6  & 17.6  & 8.1 & 34.6 \\
    P-correction~(19')~\cite{yi2019probabilistic} & 92.0  & 88.7  & 76.5  & 58.2  & 91.6 & 81.4 && 68.1  & 56.4  & 20.7  & 8.8 & 38.5 \\
    M-correction~(19')~\cite{arazo2019unsupervised} & 93.8  & 91.9  & 86.6  & 68.7  & 87.4 & 85.7 && 73.4  & 65.4  & 47.6  & 20.5 & 51.7 \\
    ELR~(20')~\cite{liu2020early} & 93.8  & 92.6  & 88.0  & 63.3  & 85.3  & 84.6 && 74.5  & 70.2  & 45.2  & 20.5  & 52.6 \\
    DivideMix~(20')~\cite{li2020dividemix} & \underline{95.0} & 93.7  & \underline{92.4} & 74.2  & 91.4  & 89.3 && 74.8 & 72.1  & 57.6 & 29.2 &  58.4 \\
    MOIT~(21')~\cite{ortego2021multi} & 93.1  & 90.0 & 79.0 & 69.6 & 92.0 & 84.7& & 73.0 & 64.6 & 46.5 & 36.0 & 55.0 \\
    RRL~(21')~\cite{li2021learning} & \bd{95.8}  & \bd{94.3} & \underline{92.4} & 75.0  & 91.9 & 89.8 && \bd{79.1} & \bd{74.8}  & 57.7 & 29.3  & 60.2 \\
    Sel-CL+~(22')~\cite{li2022selective} & \underline{95.5}  & \underline{93.9} & 89.2 & \underline{81.9}  & \bd{93.4} & \underline{90.7} && 76.5 & 72.4  & \underline{59.6} & \underline{48.8}  & \underline{64.3} \\
    \hline
    \methodname (\bd{ours}) & 95.0\ & \underline{93.9} & \bd{92.5} & \bd{89.4} & \underline{92.6} & \bd{92.7} && \underline{78.0} & \underline{73.3} & \bd{65.0} & \bd{54.5} & \bd{67.7} \\
     & \std{0.1}\ & \std{0.1} & \std{0.2} & \std{0.2} & \std{0.1} &  && \std{0.2} & \std{0.2} & \std{0.3} & \std{0.5} & \\
    \end{tabular}
\end{table*}


\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{qualitive_results.pdf}
    \caption{
        Qualitative results.
        For the model trained on CIFAR-10 with 90\% \textit{sym.} noise at 200th epoch, we show t-SNE visualizations for the learned representations of (a) testing set where different color denotes different class predicted by  and (b) 10K samples from training set colored by the true labels; the gray `+' denotes the samples with noisy labels.  (c) The histogram of  for full training set colored by the clean and noisy labels. (d) The validation accuracy across training of CIFAR-10 and CIFAR-100 on 90\% \textit{sym.} noise.
    }
    \label{fig:qualitive_results}
\end{figure*}

\paragraph{Training details.} 
Same as previous works~\cite{ortego2021multi,li2020dividemix,li2021learning,liu2020early}, we use a PreAct ResNet-18~\cite{he2016identity} as the encoder. We adopt SGD optimizer to train our model with a momentum of 0.9, a weight decay of 0.001, and a batch size of 256 for 200 epochs. The learning rate are linearly warmed up to 0.03 for 20 epochs and decayed with the cosine schedule.
The data augmentations of~\cite{chen2020simple} are applied to two views~(ResizedCrop, ColorJitter, and \textit{etc}). Only crop and horizontal flip are employed for mixup. Both projection and classification heads are a two-layer MLP with the dimension 128 and the number of classes. The temperature  of contrastive loss and the  of mixup are 0.25 and 1. The settings are shared for all experiments, which are significantly different from~\cite{li2020dividemix,li2021learning,liu2020early} that adopt specific configurations for different datasets and even for different noise ratios/types.

\paragraph{Quantitative results.}
Table~\ref{tab:results_on_cifar} presents the the comparisons with existing state-of-the-art methods. Our method yields competitive performance on low noise ratios, but promising improvements over recent methods on extreme noise ratios and the most challenging CIFAR-100 dataset with 100 classes. In particular, with 90\% label noise, there are 7.5\% and 5.7\% improvements on for CIFAR-10 and CIFAR-100, respectively. We stress that the hyperparameters are consistent for different noise ratios/types. In practical scenarios, the noise ratio for a particular dataset is unknown, so it is hard to tune the hyper-parameters for better performance. Therefore, these results indicate the strong generalization ability of our method regardless of noise ratios/types.

For fair comparisons, following~\cite{ortego2021multi,li2022selective}, we performed extra experiments on fine-tuning the classification network for 70 epochs with the detected clean samples and mixup augmentation, termed TCL+. Table~\ref{tab:results_on_cifar_plus} shows that under low label noise~(below 50\%), TCL+ achieves significant improvements over TCL and outperforms the recent state-of-the-art methods. The benefits from the detected clean subset and longer training, which can fully utilize the useful supervision signals from labeled examples.


In Appendix~\ref{sec:knn}, we also perform the -NN classification over the learned representations, which indicates that our method has maintained meaningful representations better than the pure unsupervised learning model.
In Appendix~\ref{sec:asym_noise}, we provide more experimental results and analysis on asymmetric label noise and imbalance data.


\begin{table}[t]
   \centering
   \scalebox{0.95}{
   \begin{tabular}{rrrrrrr}
   & \multicolumn{3}{c}{\bd{CIFAR-10}} & & \multicolumn{2}{c}{\bd{CIFAR-100}} \\
   & \multicolumn{2}{c}{\textit{Sym.}} & \textit{Asym.} & & \multicolumn{2}{c}{\textit{Sym.}} \\ 
   \cmidrule{2-4} \cmidrule{6-7}
   & 20\% & 50\% & 40\% & &20\%  & 50\% \\
   \shline
   DivideMix~\cite{li2020dividemix} & 95.0 & 93.7  & 91.4 & & 74.8 & 72.1  \\
   MOIT~\cite{ortego2021multi} & 93.1  & 90.0 & 92.0 & & 73.0 & 64.6 \\
   MOIT+~\cite{ortego2021multi} & 94.1  & 91.8 & 93.3 & & 75.9 & 70.6 \\
   RRL~\cite{li2021learning} & \underline{95.8} & \underline{94.3} & 91.9 & & \underline{79.1} & \bd{74.8}  \\
   Sel-CL+~\cite{li2022selective} & 95.5  & 93.9 & \underline{93.4} & & 76.5 & 72.4 \\
   \hline
   \methodname (\bd{ours}) & 95.0\ & 93.9 &  92.6 & & 78.0 & 73.3 \\
   TCL+ (\bd{ours}) & \bd{96.0} & \bd{94.5} & \bd{93.7} & & \bd{79.3} & \underline{74.6} \\
   \shline
   \end{tabular}
   }
   \caption{
      Comparisons with SOTAs under \emph{low} label noise.
   }
   \label{tab:results_on_cifar_plus}
\end{table}

\paragraph{Qualitative results.} Figs.~\ref{fig:qualitive_results}(a) and (b) visualize the learned representations with extremely high noise ratio, demonstrating that our method can produce distinct structures of learned representations with meaningful semantic information. Especially, Fig.~\ref{fig:qualitive_results}(b) presents the samples with noisy labels in the embedding space, in which the label noise can be accurately detected by our proposed method. In addition, by visualizing the histogram of  for the training set in Fig.~\ref{fig:qualitive_results}(c), we confirm that the proposed method can effectively distinguish the samples noisy and clean labels.
We visualize the validation accuracy across training in Fig.~\ref{fig:qualitive_results}(d). As expected, \methodname performs stable even with the extreme 90\% label noise.

\subsection{Ablation study}

We conduct ablation studies to validate our motivation and design with the following baselines, and the results are shown in Table~\ref{tab:ablation_study}.

\begin{enumerate}[label=\color{red!70!black}(\roman*),wide,labelindent=0pt,itemsep=0ex,parsep=0pt,topsep=0pt]
\item\label{bl_1} \textbf{Baseline.}\quad
We start the baseline method by removing the proposed noisy label detection and bootstrap cross-supervision, where the model is directly guided by noisy labels.  As expected, the performance significantly degrades for the extremely high noise ratio~(\ie, 90\%).


\item\label{bl_2} \textbf{Label Noise Detection.}\quad
We assess the effectiveness of different detection methods including the cross-entropy loss~\cite{arazo2019unsupervised,li2020dividemix}, -NN search~\cite{ortego2021multi}, and our out-of-distribution~(OOD) detection. For fair comparisons, the predictions from the images before mixup are employed as the true labels in Eq.~\eqref{eq:convex_comb}.
Obviously, the label noise detection has alleviated the degeneration to some degree~(Exp.~\ref{bl_1}), where our method consistently outperforms other baselines.
Fig.~\ref{fig:label_noise_detection_AUC} visualizes their AUCs across training.
The proposed OOD detection is better at distinguishing clean and wrong labels. Thanks to the representations learned by contrastive learning, -NN search performs better than cross-entropy loss. However, it is limited due to the use of the original labels to detect noisy ones, while our method constructs a GMM using the model predictions.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.75\linewidth]{label_noise_detection_AUC.pdf}
    \caption{
        Training curve of AUC for noisy label detection trained on CIFAR-10 with 90\% \textit{sym.} noise. 
    }
    \label{fig:label_noise_detection_AUC}\end{figure}

\item\label{bl_3} \textbf{Target Estimation.}\quad
Another key component is the cross-supervision that bootstraps the true targets from the predictions of another data augmentation. We replace it with the temporal ensembling~\cite{liu2020early}, where the hyperparameters are set as suggested by~\cite{liu2020early}. Furthermore, Exp.~\ref{bl_3} estimates true targets from the images before mixup~\cite{li2020dividemix,liu2020early,arazo2019unsupervised}. The results suggest that our bootstrap cross-supervision has shown strong robustness on 90\% label noise.

\begin{table}[t]
    \centering
    \scalebox{.73}{
    \begin{tabular}{llcccccccc}
    \shline
    \multicolumn{2}{l}{Dataset}   & \multicolumn{4}{c}{\bd{CIFAR-10}} && \multicolumn{3}{c}{\bd{CIFAR-100}} \\
    \midrule
    \multicolumn{2}{l}{\multirow{3}{*}{Noise type/rate}} & \multicolumn{2}{c}{\textit{Sym.}} & \textit{Asym.} & \textit{Avg.} && \multicolumn{2}{c}{\textit{Sym.}} & \textit{Avg.} \\
    \cmidrule{3-6}\cmidrule{8-10}
    & & 50\% & 90\% & 40\% && & 50\%  & 90\%  \\
    \shline
    \ref{bl_1} & Baseline  & 70.0 & 20.6 &77.5 &    56.1  &&    47.3 &  6.8 &   27.1    \\
    \ref{bl_2} & Loss~\cite{arazo2019unsupervised,li2020dividemix} & 92.5 & 75.9 & 73.2 &  80.6  && 71.2 & 16.0 &  43.6   \\
    & -NN~\cite{ortego2021multi} & 92.9 &     79.7 &  91.3 &  88.0  && 70.3 &     39.8 &  55.1   \\
            & OOD~(\bd{ours}) & 93.1 & 82.1 & 92.0 & 89.1  && 70.7 & 45.9 & 58.3  \\
    \ref{bl_3} & \emph{Ensem.}~\cite{liu2020early}  & 91.3 & 72.7 & 89.8 & 84.6 && 68.2 & 36.9 & 52.6 \\
            & ~(\bd{ours})  & 93.9  & 89.4  & 92.6 & 92.0 && 73.3  & 54.5 & 63.9   \\
    \ref{bl_4} & w/o    & 92.0 & 34.5 & 90.3 & 72.3  && 68.5 & 24.3 & 46.4   \\
    \ref{bl_5} & w/o    & 91.8 & 84.6 & 89.7 & 88.7 && 69.4 & 48.4 & 58.9   \\
    \ref{bl_6} & MoCo     & 94.4 &  90.7 &  93.1 &  92.7  &&    74.0 &  57.3 & 65.6   \\
    \shline
    \end{tabular}
    }
    \caption{
    Ablation results of different components in \methodname.}\label{tab:ablation_study}
   
\end{table}

\item\label{bl_4} \textbf{Without .}\quad
We remove  and the results indicate that it plays an important role, especially on extremely high label noise. Removing each term in  obtains similar results. We argue that  works in two aspects: 1) it can avoid the model collapse which outputs single classes, and 2) it can encourage the model to have high confidence for the predictions, which has shown its effectiveness for unlabeled data in semi-supervised learning.
\item\label{bl_5} \textbf{Without .}\quad 
We remove  and the performance has decreased, as expected, but is still more promising than other baselines.  has leveraged mixup augmentation to regularize both classification and representation learning. Appendix~\ref{sec:knn} shows the evaluation of -NN classification, demonstrating that  can also greatly improve the learned representations.
\item\label{bl_6} \textbf{Contrastive Framework.}\quad
We implement \methodname into another contrastive framework for representation learning, \ie, MoCo~\cite{he2020momentum}. Based on the MoCo framework, our method has achieved more improvements in various experiments, which benefits from a large number of negative examples in the memory queue and a moving-averaged encoder~(we set the queue size and the factor of moving-average to 4,096 and 0.99, respectively).

\end{enumerate}


\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{ablation_hyperparameters.pdf}
    \caption{
        Ablation results for hyperparameters.
    }
    \label{fig:ablation_hyperparameters}\end{figure}

\paragraph{Hyperparameters.}
We evaluate the most essential hyperparameters  to our design, including the temperature  for contrastive loss and update frequency for \methodname on CIFAR-10 with 90\% symmetric noise. Here, the update frequency denotes how may epochs that we update the parameters of \methodname,  and .
Fig.~\ref{fig:ablation_hyperparameters} shows that our method is robust to different choices of hyperparameters.
Even though \methodname updates for every 32 epochs, our method has still performed well, which indicates that the computational cost can be significantly reduced.

\subsection{Results on real-world datasets}

\paragraph{Datasets and training details.} 
We validate our method on two real-word noisy datasets: WebVision~\cite{li2017webvision} and Clothing1M~\cite{xiao2015learning}. Webvision contains millions of noisily-labeled images collected from the web using the keywords of ImageNet ILSVRC12~\cite{deng2009imagenet}. Following the convension~\cite{ortego2021multi,li2020dividemix,li2021learning,liu2020early}, we conducted experiments on the first 50 classes of the Google image subset, termed WebVision~(mini) and evaluated on both WebVision and ImageNet validation set. Clothing1M consists of 14 kinds of images
collected from online shopping websites. Only the noisy training set is used in our experiments.
We used a batch size of 256 on 4 GPUs, and trained a ResNet-50 for 40 epochs~(without warm-up) on Clothing1M and a ResNet-18 for 130 epochs~(warm-up 10 epochs) on WebVision, respectively. Following~\cite{li2020dividemix,li2021learning,liu2020early}, for Clothing1M, the encoder is initialized with ImageNet pre-trained weights, the initial learning rate is 0.01, and 256 mini-batches are sampled as one epoch. Other hyper-parameters are kept to be the same without further tuning.

\paragraph{Quantitative results.} 
Tables~\ref{tab:webvision} and~\ref{tab:clothing1m} present the results on WebVision and Clothing1M datasets. Our method outperforms state-of-the-art methods on both datasets, demonstrating its superior performance in handling real-world noisy datasets. We note that after checking the Clothing1M dataset, there are still lots of mislabeled images in the testing set. Therefore, the results on Clothing1M may not be such reliable as other datasets to evaluate the true performance of different methods.

\begin{table}
        \begin{tabular}{r|cc|cc}
               & \multicolumn{2}{c|}{\bd{WebVision}} & \multicolumn{2}{c}{\bd{ILSVRC12}} \\
               & top1 & top5 & top1 & top5 \\
            \shline
            Forward~\cite{patrini2017making}   & 61.1 & 82.6 & 57.3 & 82.3 \\
            D2L~\cite{ma2018dimensionality}   & 62.6 & 84.0 & 57.8 & 81.3 \\
            Iterative-CV~\cite{chen2019understanding}   & 65.2 & 85.3 & 61.6 & 84.9 \\
            Decoupling~\cite{malach2017decoupling}   & 62.5 & 84.7 & 58.2 & 82.2 \\
            MentorNet~\cite{jiang2018mentornet}   & 63.0 & 81.4 & 57.8 & 79.9 \\
            Co-teaching~\cite{han2018co}   & 63.5 & 85.2 & 61.4 & 84.7 \\
            ELR~\cite{liu2020early}   & 76.2 & 91.2 & 68.7 & 87.8 \\
            DivideMix~\cite{li2020dividemix}   & 77.3 &  91.6 & 75.2 & 90.8 \\
            RRL~\cite{li2021learning}   & 76.3 &  91.5 & 73.3 & 91.2 \\
            NGC~\cite{li2020mopro}   & \bd{79.1} & 91.8 & 74.4 & 91.0 \\
            MOIT~\cite{ortego2021multi}   & 77.9 & 91.9 & 73.8 &91.7 \\
            \hline
            \methodname (\bd{ours}) & \bd{79.1} & \bd{92.3} & \bd{75.4} & \bd{92.4} \\
            \end{tabular}
            \caption{Results on WebVision (mini).}\label{tab:webvision}
\end{table}
\begin{table}
    \begin{tabular}{rc}
        Method        & Acc~(\%) \\
        \shline
        Cross-Entropy  & 69.2      \\
        Label Correction~\cite{arazo2019unsupervised}      & 71.0      \\
        Joint-Opt~\cite{tanaka2018joint}   & 72.2      \\
        ELR~\cite{liu2020early}           & 72.8      \\
        SL~\cite{wang2019symmetric}  & 74.4      \\
        DivideMix~\cite{li2020dividemix}  & 74.4      \\
        MentorMix~\cite{jiang2020beyond}  & 74.3     \\
        RRL~\cite{li2021learning}  & \bd{74.8}     \\
        \hline
        \methodname (\bd{ours})                         & \textbf{74.8}  \\
    \end{tabular}
    \caption{Results on Clothing1M.
    }\label{tab:clothing1m}
\end{table}