\appendixpage

\setcounter{theorem}{0}    


\section{Gradients at Initialization}
\label{appendix:ini_grad}

Here, we first reveal that Pre-LN does not suffer from the gradient vanishing. 
Then we establish that only the Post-LN decoder suffers from the gradient vanishing, but not the Post-LN encoder. 
For simplicity, we use  to denote gradients, \ie,  where  is the training objective. 
Following the previous study~\cite{Bengio1994LearningLD,Glorot2010UnderstandingTD,He2015DelvingDI,saxe2013exact}, we analyze the gradient distribution at the very beginning of training, assume that the randomly initialized parameters and the partial derivative with regard to module inputs are independent.


\subsection{Pre-LN Analysis}
\label{subsec:preln-analysis}
For Pre-LN encoders, we have 
 and
. 
At initialization, the two terms on the right part are approximately independent and . Therefore we have .
Similarly, we can get  thus . 
Applying the same analysis to Pre-LN decoders, we can get .
Thus, lower layers have larger gradients than higher layers, and gradients do not vanish in the backpropagation. 
\begin{remark}
For Pre-LN, if  and the derivatives of modules in the -th sub-layer are independent, then .
\label{remark: pre-ln-gradient}
\end{remark}


\subsection{Post-LN Encoder Analysis}
\label{subsec:postln-encoder-analysis}
Different from Pre-LN,  and  are associated with not only the residual connection but the layer normalization, which makes it harder to establish the connection on their gradients.
After making assumptions on the model initialization, we find that lower layers in Post-LN encoder also have larger gradients than higher layers, and gradients do not vanish in the backpropagation through the encoder.   

\begin{theorem}
For Post-LN Encoders, if  and  in the Layer Norm are initialized as  and  respectively; all other parameters are initialized by symmetric distributions with zero mean;  and  are subject to symmetric distributions with zero mean; the variance of  is  (\ie, normalized by Layer Norm);  and the derivatives of modules in -th sub-layer are independent, we have .
\end{theorem}
\begin{proof}
We first prove , \ie, the backpropagation through FFN sublayers does not suffer from gradient vanishing. 
In Post-LN encoders, the output of FFN sublayers is calculated as  where .
Since at initialization,  and  are independently randomized by symmetric distributions, we have  and 
 
where . 
Referring to the dimension of  as , \citet{He2015DelvingDI} establishes that

Since in Post-LN,  is the output of layer norm, we have . 
Thus, 

Assuming different terms are also independent in the backpropagation, we have
 
At initialization, \citet{He2015DelvingDI} establishes that

Therefore, we have

Combining Equation~\ref{eqn:ffn-sigma} with Equation~\ref{eqn:ffn-back-propagation}, we have

which shows the backpropagation through FFN sublayers does not suffer from gradient vanishing. 

Now we proceed to prove that, , \ie, the backpropagation through Self-Attention sublayers do not suffer from gradient vanishing. 
In Post-LN encoders, the output of Self-Attention sublayers are calculated as 
 where  and . 
At initialization, since , , , and  are independently randomized by symmetric distributions, we have , thus , where .

Referring  as , we have 

Similar to \citet{He2015DelvingDI}, we have

Since  is the output of layer norm, we have . Thus, 

In the backpropagation, we have

At initialization, we assume  and model parameters are independent~\cite{He2015DelvingDI}, thus

Therefore, we have

Integrating Equation~\ref{eqn:satt-sigma} with Equation~\ref{eqn:satt-back-propagation}, we have

Combining Equation~\ref{eqn:ffn-no-vanish} and Equation~\ref{eqn:satt-no-vanish}, we have .
\end{proof}




\subsection{Post-LN Decoder Analysis}
\label{subsec:postln_decoder_analysis}
In Post-LN, the Encoder-Attention sub-layer suffers from gradient vanishing. 
The Encoder-Attention sub-layer calculates outputs as 
 where  and . 
Here  is encoder outputs and  is the row-wise softmax function. 
In the backpropagation, 
All of the backpropagations from  to  went through the softmax function, we have . 
Thus, those backpropagations suffer from gradient vanishing. 
This observation is further verified in Figure~\ref{fig:gradient-histogram}, as the encoder attention bars (gradients of encoder-attention outputs) are always shorter than self-attention bars (gradients of encoder-attention inputs), while adjacent self-attention bars and fully connected bars usually have the same length. 


\subsection{Distributes of Unbalanced Gradients}
\label{subsec:attention_gradients}

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{fig/self-attn-distribution-radam-four.pdf}
\caption{
Relative Norm of Gradient (, where  is the checkpoint of -th epoch) and Update () of Self-Attention Parameters in 12-Layer Pre-LN.
}
\label{fig:update-with-gradient-four}
\end{figure}

As in Figure~\ref{fig:all_histogram} and Figure~\ref{fig:update-with-gradient-four}, the gradient distribution of Attention modules is unbalanced even for Pre-LN. 
Specifically, parameters within the softmax function (\ie,  and ) suffer from gradient vanishing (\ie, ) and have smaller gradients than other parameters.

With further analysis, we find it is hard to neutralize the gradient vanishing of softmax. 
Unlike conventional non-linear functions like ReLU or sigmoid, softmax has a dynamic input length (\ie, for the sentences with different lengths, inputs of softmax have different dimensions). 
Although this setting allows Attention modules to handle sequential inputs, it restricts them from having stable and consistent backpropagation. 
Specifically, let us consider the comparison between softmax and sigmoid. 
For the sigmoid function, although its derivation is smaller than 1, this damping effect is consistent for all inputs. 
Thus, sigmoid can be neutralized by a larger initialization~\cite{Glorot2010UnderstandingTD}. 
For softmax, its damping effect is different for different inputs and cannot be neutralized by a static initialization. 


Also, we observe that adaptive optimizers largely address this issue. 
Specifically, we calculate the norm of parameter change in consequent epochs (\eg,  where  is the checkpoint saved after  epochs) and visualize the relative norm (scaled by the largest value in the same network) in Figure~\ref{fig:update-with-gradient-four}.  
Comparing the relative norm of parameter gradients and parameter updates, we notice that: although the gradient distribution is unbalanced, adaptive optimizers successfully assign different learning rates to different parameters and lead to consistent update magnitudes. 
This result explains why the vanilla SGD fails for training Transformer (\ie, lacking the ability to handle unbalanced gradient distributions). 
Besides, it implies that the unbalanced gradient distribution (\eg, gradient vanishing) has been mostly addressed by adaptive optimizers and may not significantly impact the training instability. 

\section{Proof of Theorem~\ref{theo:layer-dependency-and-shift}}
\label{appendix:theo2}

Here, we elaborate the derivation for Theorem~\ref{theo:layer-dependency-and-shift}, which establishes the relationship between layer number and output fluctuation brought by parameter change. 

\begin{theorem}
Consider a -layer Transformer , where  is the input and  is the parameter. 
If the layer dependency stays the same after a parameter change 
(\ie,  has the same value after changing  to , where  is randomly initialized and  is independent to ), the output change (\ie, ) can be estimated as  where  is a constant.
\end{theorem}

\begin{proof}

We refer the module in  sub-layer as , where  is the normalized residual output and  is the normalized module output. 
The final output is marked as .
To simplify the notation, we use the superscript  to indicate variables related to , \eg,  and .

At initialization, all parameters are initialized independently.
Thus ,  and  are independent and . 
Also, since -layer and -layer share the residual connection to previous layers,  we have .
Thus  and 


Now, we proceed to analyze . Specifically, we have

Since  is randomly initialized,  should have the same value for all layers, thus we use a constant  to refer its value ( and ). 
As to , since the sub-layer of Transformers are mostly using linear weights with ReLU nonlinearity and , we have . 
Thus, we can rewrite Equation~\ref{eqn:a-decompose} and get  

With Equation~\ref{eqn:recurrent}, we have

Therefore, we have .
\end{proof}

\section{\our Implementation Details}
\label{appendix:implement}

As introduced in Section~\ref{subsec:admin}, we introduce a new set of parameters to rescale the module outputs. 
Specifically, we refer these new parameters as  and construct the Post-LN sub-layer as:

where  is the element-wise product. 

After training, \our can be reparameterized as the conventional Post-LN structure (\ie, removing ). 
Specifically, we consider . 
Then, for feedforward sub-layers, we have

It can be reparameterized by changing , ,  to , ,  respectively, \ie, 

For Self-Attention sub-layers, we have 
 
It can be reparameterized by changing , , , ,  to , , ,   respectively, \ie, 

For Encoder-Attention sub-layers, we have 
 
It can be reparameterized by changing , ,  to , ,  respectively, \ie, 

It is easy to find  in all three situations. 

From the previous analysis, it is easy to find that introducing the additional parameter  is equivalent to rescale some model parameters. 
In our experiments on IWSLT14 De-En, we find that directly rescaling initialization parameters can get roughly the same performance with introducing . 
However, it is not very stable when conducting training in a half-precision manner. 
Accordingly, we choose to add new parameters  instead of rescaling parameters. 

\section{Experimental Setup}
\label{appendix:exp}

Our experiments are based on the implementation from the fairseq package~\citep{ott2019fairseq}.
As to pre-processing, we follow the public released script from previous work~\citep{ott2019fairseq,lu2020understanding}. 
For WMT'14 datasets, evaluations are conducted on the provided `newstest14` file, and more details about them can be found in \citet{bojar2014findings}. 
For the IWSLT'14 De-En dataset, more analysis and details can be found in \citet{cettolo2014report}. 

\begin{table}[t]
\centering
\caption{ReZero Performance on IWSLT'14 De-En. Models are Transformer-small w. 6-layer encoder \& decoder.}
\label{tab:rezero}
\begin{tabular}{r|ccccc}
\toprule
Models & Admin & Post-LN & Pre-LN & ReZero & ReZero+Post-LN \\
\midrule
BLEU & 35.670.15 & 35.640.23 & 35.500.04 & 33.670.14 & 34.670.08 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[t]
\centering
\caption{Performance and model size on WMT'14 En-Fr (AL-BL refers A-layer encoder \& B-layer decoder).}
\label{tab:wmt14fr}
\begin{tabularx}{\linewidth}{lccYY}
\toprule
Methods & Param. \# & dim() in FFN & Enc\#-Dec\# & BLEU \\
\midrule
T5-Base~\cite{raffel2019exploring} & 220 M &  & 6L-6L & 41.2 \\
T5-Large~\cite{raffel2019exploring} & 770 M &  & 12L-12L & 41.5 \\
T5-3B~\cite{raffel2019exploring} & 3 B &  & 24L-24L & 42.6 \\
T5-11B~\cite{raffel2019exploring} & 11 B &  & 24L-24L & 43.4 \\
\midrule
Trans.Big-RNMT+~\cite{Chen2018TheBO} & 377 M &   & 6L-6L & 41.12 \\
DynamicConv~\cite{wu2018pay} & 213 M &  & 7L-7L & 43.2 \\
DG-Transformer~\cite{Wu2019DepthGF} & 264 M &  & 8L-8L & 43.27 \\
Prime~\cite{zhao2019muse} & 252 M &  & 6L-6L & 43.48 \\
\midrule
Pre-LN (60L--12L) & 262 M &   & 60L-12L & 43.10 \\
Admin (60L--12L) & 262 M &  & 60L-12L & 43.80 \\
\bottomrule
\end{tabularx}
\end{table}

As to model specifics, we directly adopt Transformer-small configurations on the IWSLT'14 De-En dataset and stacks more layers over the Transformer-base model on the WMT'14 En-De and WMT'14 En-Fr datasets.
Specifically, on the IWSLT'14 De-En dataset, we use word embedding with 512 dimensions and 6-layer encoder/decoder with 4 heads and 1024 feedforward dimensions; on the WMT'14 En-De and WMT'14 En-Fr datasets, we use word embedding with 512 dimension and 8-head encoder/decoder with 2048 hidden dimensions. 
Label smoothed cross entropy is used as the objective function with an uncertainty ~\citep{szegedy2016rethinking}. 

For Model training, we use RAdam as the optimizer~\cite{Liu2019OnTV} and adopt almost all hyper-parameter settings from \citet{lu2020understanding}.
Specifically, for the WMT'14 En-De and WMT'14 En-Fr dataset, all dropout ratios (including (activation dropout and attention dropout) are set to 0.1. 
For the IWSLT'14 De-En dataset, after-layer dropout is set to , and a weight decay of  is used. 
As to optimizer, we set ,  use inverse sqrt learning rate scheduler with a warmup phrase (8000 steps on the WMT'14 En-De/Fr dataset, and 6000 steps on the IWSLT'14 De-En dataset). 
The maximum learning rate is set to  on the WMT'14 En-De dataset and  on the IWSLT'14 De-En and WMT'14 En-Fr datasets. 
We conduct training for  epochs on the WMT'14 En-De dataset,  epochs on the IWSLT'14 De-En dataset and  epochs on the WMT'14 En-Fr dataset, while the last 10 checkpoints are averaged before inference. 

On the IWSLT'14 De-En dataset, we conduct training on one NVIDIA GeForce GTX 1080 Ti GPU and set the maximum batch size to be . 
On the WMT'14 En-De dataset, we conduct training on four NVIDIA Quadro R8000 GPUs and set maximum batch size (per GPU) as . 
On the WMT'14 En-Fr dataset, we conduct training with the Nvidia DGX-2 server (6L-6L uses 4 NVIDIA TESLA V100 GPUs and 60L-16L uses 16 NVIDIA TESLA V100 GPUs) and set the maximum batch size (per GPU) as  for 6L-6L and  for 60L-16L. 
On the IWSLT'14 De-En dataset, Transformer-small models (w. 37 M Param.) take a few hours to train.
On the WMT'14 En-De dataset, 6L-6L models (w. 63 M Param.) take  day to train, 
12L-12L (w. 107M Param.) models take  days to train, 
and 18L-18L (w. 151M Param.) models take  days to train. 
On the WMT'14 En-Fr dataset, 6L-6L models (w. 67 M Param.) takes  days to train, and 60L-12L models (w. 262M Param.) takes  days to train. 
All training is conducted in half-precision with dynamic scaling (with a 256-update scaling window and a 0.03125 minimal scale).
All our implementations and pre-trained models would be released publicly. 

\section{Comparison to ReZero}
\label{appendix:rezero}

Here, we first conduct comparisons with ReZero~\cite{Bachlechner2020ReZeroIA} under two configurations--the first employs the original ReZero model, and the second adds layer normalizations in a Post-LN manner. 
As summarized in Table~\ref{tab:rezero}, the ReZero initialization leads to a performance drop, no matter layer normalization is used or not. 
It verifies our intuition that over small dependency restricts the model potential. 
At the same time, we find that adding layer normalization to ReZero helps to improve the performance. 
Intuitively, as dropout plays a vital role in regularizing Transformers, layer normalization helps to not only stabilize training but alleviate the impact of turning off dropouts during the inference. 

\section{Performance on the WMT'14 En-Fr}
\label{appendix:enfr}

To explore the potential of \our, we conduct experiments with 72-layer Transformers on the WMT'14 En-Fr dataset (with a 60-layer encoder and 12-layer decoder, we add less layers to decoder to encourage the model to rely more on the source context). 

As in Table~\ref{tab:wmt14fr}, \our (60L--12L) achieves a BLEU score of 43.80, the new state-of-the-art on this long-standing benchmark. 
This model has a 60-layer encoder and a 12-layer decoder, which is significantly deeper than other baselines.
Still, since the number of parameters increases in a quadratic speed with regard to hidden dimensions and a linear speed with regard to layer numbers, our model has roughly the same number of parameters with other baselines. 
It is worth mentioning that \our even achieves better performance than all variants of pre-trained T5 models, which demonstrates the great potential of our proposed method.
Also, \our achieves a better performance than Pre-LN (60L--12L), which further verifies that the Pre-LN architecture restricts deep models' potential. 




