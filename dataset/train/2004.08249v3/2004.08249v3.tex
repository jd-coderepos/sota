\appendixpage

\setcounter{theorem}{0}    


\section{Gradients at Initialization}
\label{appendix:ini_grad}

Here, we first reveal that Pre-LN does not suffer from the gradient vanishing. 
Then we establish that only the Post-LN decoder suffers from the gradient vanishing, but not the Post-LN encoder. 
For simplicity, we use $\Delta \xb$ to denote gradients, \ie, $\Delta \xb = \frac{\partial \mathcal{L}}{\partial \xb}$ where $\mathcal{L}$ is the training objective. 
Following the previous study~\cite{Bengio1994LearningLD,Glorot2010UnderstandingTD,He2015DelvingDI,saxe2013exact}, we analyze the gradient distribution at the very beginning of training, assume that the randomly initialized parameters and the partial derivative with regard to module inputs are independent.


\subsection{Pre-LN Analysis}
\label{subsec:preln-analysis}
For Pre-LN encoders, we have 
$ \xb\ep_{2i} = \xb\ep_{2i-1} + \FFN(\LN(\xb\ep_{2i-1}))$ and
$ \Delta\xb\ep_{2i-1} = \Delta\xb\ep_{2i} (1 + \frac{\partial \FFN(\LN(\xb\ep_{2i-1})) }{\partial \xb\ep_{2i}})$. 
At initialization, the two terms on the right part are approximately independent and $E[\frac{\partial \FFN(\LN(\zb\ep_{2i-1})) }{\partial \xb\ep_{2i}}] = 0$. Therefore we have $ \Var[\Delta\xb\ep_{2i-1}] \geq \Var[\Delta\xb\ep_{2i}]$.
Similarly, we can get $ \Var[\Delta\xb\ep_{2i-2}] \geq \Var[\Delta\xb\ep_{2i-1}]$ thus $\forall i \leq j, \Var[\Delta\xb\ep_{i}] \geq \Var[\Delta\xb\ep_{j}]$. 
Applying the same analysis to Pre-LN decoders, we can get $\forall i \leq j, \Var[\Delta\xb\pd_{i}] \geq \Var[\Delta\xb\pd_{j}]$.
Thus, lower layers have larger gradients than higher layers, and gradients do not vanish in the backpropagation. 
\begin{remark}
For Pre-LN, if $\forall i, \Delta \xb_i^{(p\cdot)}$ and the derivatives of modules in the $i$-th sub-layer are independent, then $\forall i \leq j, \Var[\Delta \xb^{(p\cdot)}_{i}] \geq \Var[\Delta \xb^{(p\cdot)}_{j}]$.
\label{remark: pre-ln-gradient}
\end{remark}


\subsection{Post-LN Encoder Analysis}
\label{subsec:postln-encoder-analysis}
Different from Pre-LN, $\xb_{i}^{(oe)}$ and $\xb_{i-1}^{(oe)}$ are associated with not only the residual connection but the layer normalization, which makes it harder to establish the connection on their gradients.
After making assumptions on the model initialization, we find that lower layers in Post-LN encoder also have larger gradients than higher layers, and gradients do not vanish in the backpropagation through the encoder.   

\begin{theorem}
For Post-LN Encoders, if $\bgamma$ and $\bnu$ in the Layer Norm are initialized as $1$ and $0$ respectively; all other parameters are initialized by symmetric distributions with zero mean; $\xb_{i}^{(oe)}$ and $\Delta \xb_i^{(oe)}$ are subject to symmetric distributions with zero mean; the variance of $\xb_{i}^{(oe)}$ is $1$ (\ie, normalized by Layer Norm); $\Delta \xb_i^{(oe)}$ and the derivatives of modules in $i$-th sub-layer are independent, we have $\Var[\Delta \xb_{i-1}] \geq \Var[\Delta \xb_{i}]$.
\end{theorem}
\begin{proof}
We first prove $\Var[\Delta \xb\eo_{2i-1}] \geq \Var[\Delta \xb\eo_{2i}]$, \ie, the backpropagation through FFN sublayers does not suffer from gradient vanishing. 
In Post-LN encoders, the output of FFN sublayers is calculated as $\xb\eo_{2i} = \LN(\bbb\eo_{2i})$ where $\bbb\eo_{2i} =
\xb\eo_{2i-1} + \max(0, \xb\eo_{2i-1}\wf)\wff$.
Since at initialization, $\wf$ and $\wff$ are independently randomized by symmetric distributions, we have $\E[\bbb\eo_{2i}] = 0$ and 
$$
\xb\eo_{2i} = \frac{\xb\eo_{2i-1} + \max(\xb\eo_{2i-1}\wf, 0)\wff}{\sigma_{b,2i}}
$$ 
where $\sigma_{b,2i}^2 = \Var[\bbb\eo_{2i}]$. 
Referring to the dimension of $\wf$ as $D \times D_f$, \citet{He2015DelvingDI} establishes that
$$
\Var[\max(\xb\eo_{2i-1}\wf, 0)\wff] = \frac{1}{2} D D_f \Var[w^{(1)}] \Var[w^{(2)}] \Var[\xb\eo_{2i-1}].
$$
Since in Post-LN, $\xb\eo_{2i-1}$ is the output of layer norm, we have $\Var[\xb\eo_{2i-1}] = 1$. 
Thus, 
\begin{align}
\sigma_{b,2i}^2 & = \Var[\bbb\eo_{2i}] = \Var[\xb\eo_{2i-1}] + \Var[\max(\xb\eo_{2i-1}\wf, 0)\wff] \nonumber\\
& = 1 + \frac{1}{2} D D_f \Var[w^{(1)}] \Var[w^{(2)}].
\label{eqn:ffn-sigma}
\end{align}
Assuming different terms are also independent in the backpropagation, we have
$$
\Var[\Delta \xb\eo_{2i-1}] \geq \Var[\frac{1}{\sigma_{b, 2i}} (\Delta \xb\eo_{2i} + \Delta \xb\eo_{2i} \frac{\partial \max(\xb\eo_{2i-1}\wf, 0)\wff}{\partial \xb\eo_{2i-1}})].
$$ 
At initialization, \citet{He2015DelvingDI} establishes that
$$
\Var[\Delta \xb\eo_{2i} \frac{\partial \max(\xb\eo_{2i-1}\wf, 0)\wff}{\partial \xb\eo_{2i-1}}] = \frac{1}{2} D D_f \Var[w^{(1)}] \Var[w^{(2)}] \Var[\Delta \xb\eo_{2i}].
$$
Therefore, we have
\begin{align}
\Var[\Delta \xb\eo_{2i-1}]& \geq  \frac{1}{\sigma^2_{b, 2i}} (1 + \frac{1}{2} D D_f \Var[w^{(1)}] \Var[w^{(2)}]) \Var[\Delta \xb\eo_{2i}].
\label{eqn:ffn-back-propagation}
\end{align}
Combining Equation~\ref{eqn:ffn-sigma} with Equation~\ref{eqn:ffn-back-propagation}, we have
\begin{align}
\Var[\Delta \xb\eo_{2i-1}] \geq \Var[\Delta \xb\eo_{2i}]
\label{eqn:ffn-no-vanish}
\end{align}
which shows the backpropagation through FFN sublayers does not suffer from gradient vanishing. 

Now we proceed to prove that, $\Var[\Delta \xb\eo_{2i-2}] \geq \Var[\Delta \xb\eo_{2i-1}]$, \ie, the backpropagation through Self-Attention sublayers do not suffer from gradient vanishing. 
In Post-LN encoders, the output of Self-Attention sublayers are calculated as 
$\xb\eo_{2i-1} = \LN(\bbb\eo_{2i-1})$ where $\bbb\eo_{2i-1} = \xb\eo_{2i-2} + \ab\eo_{2i-1}$ and $\ab\od_{2i-1} = \sum_{h} \softmax(\xb\eo_{2i-2} \wq_h \wk_h \xbteo_{2i-2})\xb\eo_{2i-2} \wv_h \wo_h$. 
At initialization, since $\wq$, $\wk$, $\wv$, and $\wo$ are independently randomized by symmetric distributions, we have $\E[\bbb\od_{2i-1}] = 0$, thus $\xb\eo_{2i-1} = \frac{\bbb\eo_{2i-1}}{\sigma_{b, 2i-1}}$, where $\sigma^2_{b, 2i-1} = \Var[\bbb\eo_{2i-1}] =  \Var[\xb\eo_{2i-2}] + \Var[\ab\eo_{2i-1}]$.

Referring $\E[\softmax^2(\xb\eo_{2i-2} \wq_h \wk_h \xbteo_{2i-2})]$ as $P_h$, we have 
$$
\Var[\ab\od_{2i-1}] = \Var[\xb\eo_{2i-2} \wv_h \wo_h] H P_h.
$$
Similar to \citet{He2015DelvingDI}, we have
$$
\Var[\xb\eo_{2i-2} \wv_h \wo_h] = \frac{D^2}{H} \Var[\xb\eo_{2i-2}] \Var[w^{(V_1)}] \Var[w^{(V_2)}].
$$
Since $\xb\eo_{2i-2}$ is the output of layer norm, we have $\Var[\xb\eo_{2i-2}] = 1$. Thus, 
\begin{align}
\sigma^2_{b, 2i-1} = 1 + D^2 P_h \Var[\xb\eo_{2i-2}] \Var[w^{(V_1)}] \Var[w^{(V_2)}].
\label{eqn:satt-sigma}
\end{align}
In the backpropagation, we have
\begin{align*}
& \Var[\Delta \xb\eo_{2i-2}] \geq \Var[\frac{1}{\sigma_{b, 2i-1}} (\Delta \xb\eo_{2i-1} + \Delta \xb\eo_{2i-1} \sum_h \frac{\partial\softmax(\xb\eo_{2i-2} \wq_h \wk_h \xbteo_{2i-2})\xb\eo_{2i-2} \wv_h \wo_h}{\partial \xb\eo_{2i-2}})] \\
&\geq \frac{1}{\sigma^2_{b, 2i-1}}(\Var[\Delta \xb\eo_{2i-1}] + \Var[\Delta \xb\eo_{2i-1} \sum_h \softmax(\xb\eo_{2i-2} \wq_h \wk_h \xbteo_{2i-2})\frac{\partial\xb\eo_{2i-2} \wv_h \wo_h}{\partial \xb\eo_{2i-2}} ])
\end{align*}
At initialization, we assume $\Delta \xb\eo_{2i-1}$ and model parameters are independent~\cite{He2015DelvingDI}, thus
\begin{align*}
& \Var[\Delta \xb\eo_{2i-1} \sum_h \softmax(\xb\eo_{2i-2} \wq_h \wk_h \xbteo_{2i-2})\frac{\partial\xb\eo_{2i-2} \wv_h \wo_h}{\partial \xb\eo_{2i-2}}] \\
=&  D^2 P_h \Var[\Delta \xb\eo_{2i-1}] \Var[w^{(V_1)}] \Var[w^{(V_2)}]
\end{align*}
Therefore, we have
\begin{align}
\Var[\Delta \xb\eo_{2i-2}]& \geq  \frac{1}{\sigma^2_{b, 2i-1}} (1 + D^2 P_h \Var[w^{(V_1)}] \Var[w^{(V_2)}]) \Var[\Delta \xb\eo_{2i-1}].
\label{eqn:satt-back-propagation}
\end{align}
Integrating Equation~\ref{eqn:satt-sigma} with Equation~\ref{eqn:satt-back-propagation}, we have
\begin{align}
\Var[\Delta \xb\eo_{2i-2}] \geq \Var[\Delta \xb\eo_{2i-1}].
\label{eqn:satt-no-vanish}
\end{align}
Combining Equation~\ref{eqn:ffn-no-vanish} and Equation~\ref{eqn:satt-no-vanish}, we have $\Var[\Delta \xb_{i-1}] \geq \Var[\Delta \xb_{i}]$.
\end{proof}




\subsection{Post-LN Decoder Analysis}
\label{subsec:postln_decoder_analysis}
In Post-LN, the Encoder-Attention sub-layer suffers from gradient vanishing. 
The Encoder-Attention sub-layer calculates outputs as 
$\xb\od_{3i-1} = \LN(\bbb\od_{3i-1})$ where $\bbb\od_{3i-1} = \xb\od_{3i-2} + \ab\od_{3i-1}$ and $\ab\od_{3i-1} = \sum_{h} \softmax(\xb\od_{3i-2} \wq_h \wk_h {\xb^T}^{(oe)})\xb^{(oe)} \wv_h \wo_h$. 
Here $\xb^{(oe)}$ is encoder outputs and $\softmax$ is the row-wise softmax function. 
In the backpropagation, $\Delta \xb\od_{3i-2} \approx \frac{\Delta \xb\od_{3i-1}}{\sigma_{b, 3i-1}} (1 + 
\frac{\partial \ab\od_{3i-1} }{\partial \xb\od_{3i-2}}).$
All of the backpropagations from $\ab\od_{3i-1} $ to $\xb\od_{3i-2}$ went through the softmax function, we have $\Var[\frac{\partial \ab\od_{3i-1} }{\partial \xb\od_{3i-2}}] + 1 \leq \sigma^2_{b, 3i-1}$. 
Thus, those backpropagations suffer from gradient vanishing. 
This observation is further verified in Figure~\ref{fig:gradient-histogram}, as the encoder attention bars (gradients of encoder-attention outputs) are always shorter than self-attention bars (gradients of encoder-attention inputs), while adjacent self-attention bars and fully connected bars usually have the same length. 


\subsection{Distributes of Unbalanced Gradients}
\label{subsec:attention_gradients}

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{fig/self-attn-distribution-radam-four.pdf}
\caption{
Relative Norm of Gradient ($\Delta W_i$, where $W_i$ is the checkpoint of $i$-th epoch) and Update ($|W_{i+1} - W_i|$) of Self-Attention Parameters in 12-Layer Pre-LN.
}
\label{fig:update-with-gradient-four}
\end{figure}

As in Figure~\ref{fig:all_histogram} and Figure~\ref{fig:update-with-gradient-four}, the gradient distribution of Attention modules is unbalanced even for Pre-LN. 
Specifically, parameters within the softmax function (\ie, $\wk$ and $\wv$) suffer from gradient vanishing (\ie, $\frac{\partial \softmax(x_0, \cdots, x_i, \cdots)}{\partial x_i} \leq 1$) and have smaller gradients than other parameters.

With further analysis, we find it is hard to neutralize the gradient vanishing of softmax. 
Unlike conventional non-linear functions like ReLU or sigmoid, softmax has a dynamic input length (\ie, for the sentences with different lengths, inputs of softmax have different dimensions). 
Although this setting allows Attention modules to handle sequential inputs, it restricts them from having stable and consistent backpropagation. 
Specifically, let us consider the comparison between softmax and sigmoid. 
For the sigmoid function, although its derivation is smaller than 1, this damping effect is consistent for all inputs. 
Thus, sigmoid can be neutralized by a larger initialization~\cite{Glorot2010UnderstandingTD}. 
For softmax, its damping effect is different for different inputs and cannot be neutralized by a static initialization. 


Also, we observe that adaptive optimizers largely address this issue. 
Specifically, we calculate the norm of parameter change in consequent epochs (\eg, $|\wk_{t+1} - \wk_{t}|$ where $\wk_{t}$ is the checkpoint saved after $t$ epochs) and visualize the relative norm (scaled by the largest value in the same network) in Figure~\ref{fig:update-with-gradient-four}.  
Comparing the relative norm of parameter gradients and parameter updates, we notice that: although the gradient distribution is unbalanced, adaptive optimizers successfully assign different learning rates to different parameters and lead to consistent update magnitudes. 
This result explains why the vanilla SGD fails for training Transformer (\ie, lacking the ability to handle unbalanced gradient distributions). 
Besides, it implies that the unbalanced gradient distribution (\eg, gradient vanishing) has been mostly addressed by adaptive optimizers and may not significantly impact the training instability. 

\section{Proof of Theorem~\ref{theo:layer-dependency-and-shift}}
\label{appendix:theo2}

Here, we elaborate the derivation for Theorem~\ref{theo:layer-dependency-and-shift}, which establishes the relationship between layer number and output fluctuation brought by parameter change. 

\begin{theorem}
Consider a $N$-layer Transformer $\hxb = \cF(\hxb_0, W)$, where $\hxb_0$ is the input and $W$ is the parameter. 
If the layer dependency stays the same after a parameter change 
(\ie, $\beta_{i, j}$ has the same value after changing $W$ to $W^*$, where $W$ is randomly initialized and $\delta = W^* - W$ is independent to $W$), the output change (\ie, $\Var[\cF(\xb_0, W) - \cF(\xb_0, W^*)]$) can be estimated as $\sum_{i=1}^N \beta^2_{i, i} C$ where $C$ is a constant.
\end{theorem}

\begin{proof}

We refer the module in $i$ sub-layer as $\ab_i = \cG_i(\hxb_{i-1}, W_i)$, where $\hxb_{i} = \sum_{j\leq i} \beta_{i, j} \hab_j$ is the normalized residual output and $\hab_i = \frac{\ab_i}{\sqrt{\Var{\ab_i}}}$ is the normalized module output. 
The final output is marked as $\hxb = \cF(\xb_0, W) = \sum_{j\leq N} \beta_{N, j} \hab_j$.
To simplify the notation, we use the superscript $*$ to indicate variables related to $W^*$, \eg, $\hxb^* = \cF(\xb_0, W^*)$ and $\ab_i^* = \cG_i(\hxb^*_{i-1}, W^*_i)$.

At initialization, all parameters are initialized independently.
Thus $\forall i \neq j$, $\hab_i$ and $\hab_j$ are independent and $1 = \Var[\sum_{j\le i} \beta_{i, j}\hab_j] = \sum_{j \le i} \beta_{i, j}^2$. 
Also, since $k$-layer and $(k+1)$-layer share the residual connection to previous layers, $\forall i, j \leq k$ we have $\frac{\beta_{i, k}}{\beta_{j, k}} = \frac{\beta_{i, k+1}}{\beta_{j, k+1}}$.
Thus $\forall i \leq k, \beta^2_{i, k + 1} = (1 - \beta^2_{k, k}) \beta^2_{i, k}$ and 
\begin{align}
\Var[\hxb_i - \hxb^*_i] & = \Var[\sum_{j \leq i} \beta_{i, j} (\hab_j - \hab_j^*)] = \sum_{j \leq i} \beta^2_{i, j} \Var[\hab_j - \hab_j^*] \nonumber\\
& = \beta^2_{i, i} \Var[\hab_i - \hab_i^*] + (1 - \beta^2_{i, i}) \Var[\hxb_i - \hxb^*_i]. \label{eqn:recurrent}
\end{align}

Now, we proceed to analyze $\Var[\hab_i - \hab_i^*]$. Specifically, we have
\begin{align}
\Var[\hab_i - \hab_i^*] &= \Var[\cG_i(\hxb_{i-1}, W_i) - \cG_i(\hxb^*_{i-1}, W_i^*)] \nonumber \\
&= \Var[\cG_i(\hxb_{i-1}, W_i) - \cG_i(\hxb^*_{i-1}, W_i) + \cG_i(\hxb^*_{i-1}, W_i^*) - \cG_i(\hxb^*_{i-1}, W_i^*)] \nonumber \\
&= \Var[\cG_i(\hxb_{i-1}, W_i) - \cG_i(\hxb^*_{i-1}, W_i)] + \Var[\cG_i(\hxb^*_{i-1}, W_i) - \cG_i(\hxb^*_{i-1}, W_i^*)]. 
\label{eqn:a-decompose}
\end{align}
Since $W$ is randomly initialized, $\Var[\cG_i(\hxb^*_{i-1}, W_i) - \cG_i(\hxb^*_{i-1}, W_i^*)]$ should have the same value for all layers, thus we use a constant $C$ to refer its value ($C=\Var[\cG_i(\hxb^*_{i-1}, W_i) - \cG_i(\hxb^*_{i-1}, W_i^*)]$ and $C \approx |\delta| \cdot |\nabla \cG_i(\hxb^*_{i-1}, W_i)|$). 
As to $\Var[\cG_i(\hxb_{i-1}, W_i) - \cG_i(\hxb^*_{i-1}, W_i)]$, since the sub-layer of Transformers are mostly using linear weights with ReLU nonlinearity and $1 = \Var[\cG_i(\hxb_{i-1}, W_i)] = \Var[\hxb_{i-1}]$, we have $\Var[\cG_i(\hxb_{i-1}, W_i) - \cG_i(\hxb^*_{i-1}, W_i)] \approx \Var[\hxb_{i-1} - \hxb^*_{i-1}]$. 
Thus, we can rewrite Equation~\ref{eqn:a-decompose} and get  
$$
\Var[\hab_i - \hab_i^*] \approx \Var[\hxb_{i-1} - \hxb^*_{i-1}] + C
$$
With Equation~\ref{eqn:recurrent}, we have
\begin{align*}
\Var[\hxb_i - \hxb^*_i] &= \beta^2_{i, i} \Var[\hab_i - \hab_i^*] + (1 - \beta^2_{i, i}) \Var[\hxb_i - \hxb^*_i] \\
& \approx \beta^2_{i, i} (\Var[\hxb_{i-1} - \hxb^*_{i-1}] + C) + (1 - \beta^2_{i, i}) \Var[\hxb_i - \hxb^*_i] \\
& = \Var[\hxb_i - \hxb^*_i] + \beta^2_{i, i} C 
\end{align*}
Therefore, we have $\Var[\cF(\xb_0, W) - \cF(\xb_0, W^*)] \approx \sum_{i=1}^N \beta^2_{i, i} C$.
\end{proof}

\section{\our Implementation Details}
\label{appendix:implement}

As introduced in Section~\ref{subsec:admin}, we introduce a new set of parameters to rescale the module outputs. 
Specifically, we refer these new parameters as $\omega$ and construct the Post-LN sub-layer as:
$$
\xb_{i} = \LN (\bbb_i), \mbox{where}\, \bbb_i=\xb_{i-1} \cdot \bomega_i + f_i (\xb_{i-1})
$$
where $\cdot$ is the element-wise product. 

After training, \our can be reparameterized as the conventional Post-LN structure (\ie, removing $\bomega_i$). 
Specifically, we consider $\xb_i = \frac{\bbb_i}{\sigma_{b}}\bgamma + \bnu$. 
Then, for feedforward sub-layers, we have
$$
\bbb_{i} = \xb_{i-1} \cdot \omega +\max(0, \xb_{i-1}\wf)\wff, \mbox{where}\, \xb_{i} = \frac{\bbb_{i-1}}{\sigma_b}\bgamma + \bnu. 
$$
It can be reparameterized by changing $\bgamma$, $\bnu$, $\wf$ to $\bgamma\bomega_i$, $\bnu\bomega_i$, $\frac{1}{\bomega_i}\wf$ respectively, \ie, 
$$
\bbb'_{i} = \xb'_{i-1} + \max(0, \xb'_{i-1}\frac{1}{\bomega_i}\wf)\wff, \mbox{where}\, \xb'_{i-1} = \frac{\bbb'_{i-1}}{\sigma_{b}}\bgamma\bomega_i + \bnu\bomega_i. 
$$
For Self-Attention sub-layers, we have 
$$
\bbb_{i} = \xb_{i-1} + \sum_{h} \softmax(\xb_{i-1} \wq_h \wk_h \xb_{i-1})\xb_{i-1} \wv_h \wo_h, \mbox{where}\, \xb_{i} = \frac{\bbb_{i-1}}{\sigma_b}\bgamma + \bnu.
$$ 
It can be reparameterized by changing $\bgamma$, $\bnu$, $\wq_h$, $\wk_h$, $\wv_h$ to $\bgamma\bomega_i$, $\bnu\bomega_i$, $\frac{1}{\bomega_i}\wq_h$, $\frac{1}{\bomega_i}\wk_h$ $\frac{1}{\bomega_i}\wv_h$ respectively, \ie, 
$$
\bbb'_{i} = \xb'_{i-1} + \sum_{h} \softmax(\xb'_{i-1} \frac{1}{\bomega_i}\wq_h \wk_h \frac{1}{\bomega_i}\xb'_{i-1})\xb'_{i-1} \frac{1}{\bomega_i}\wv_h \wo_h, \mbox{where}\, \xb'_{i-1} = \frac{\bbb'_{i-1}}{\sigma_{b}}\bgamma\bomega_i + \bnu\bomega_i.
$$
For Encoder-Attention sub-layers, we have 
$$
\bbb_{i} = \xb_{i-1} + \sum_{h} \softmax(\xb_{i-1} \wq_h \wk_h \xb^{\cdot e})\xb^{\cdot e} \wv_h \wo_h, \mbox{where}\, \xb_{i} = \frac{\bbb_{i-1}}{\sigma_b}\bgamma + \bnu.
$$ 
It can be reparameterized by changing $\bgamma$, $\bnu$, $\wq_h$ to $\bgamma\bomega_i$, $\bnu\bomega_i$, $\frac{1}{\bomega_i}\wq_h$ respectively, \ie, 
$$
\bbb'_{i} = \xb'_{i-1} + \sum_{h} \softmax(\xb'_{i-1} \frac{1}{\bomega_i}\wq_h \wk_h \xb^{\cdot e})\xb^{\cdot e} \frac{1}{\bomega_i}\wv_h \wo_h, \mbox{where}\, \xb'_{i-1} = \frac{\bbb'_{i-1}}{\sigma_{b}}\bgamma\bomega_i + \bnu\bomega_i.
$$
It is easy to find $\bbb'_i = \bbb_i$ in all three situations. 

From the previous analysis, it is easy to find that introducing the additional parameter $\bomega_i$ is equivalent to rescale some model parameters. 
In our experiments on IWSLT14 De-En, we find that directly rescaling initialization parameters can get roughly the same performance with introducing $\bomega_i$. 
However, it is not very stable when conducting training in a half-precision manner. 
Accordingly, we choose to add new parameters $\bomega_i$ instead of rescaling parameters. 

\section{Experimental Setup}
\label{appendix:exp}

Our experiments are based on the implementation from the fairseq package~\citep{ott2019fairseq}.
As to pre-processing, we follow the public released script from previous work~\citep{ott2019fairseq,lu2020understanding}. 
For WMT'14 datasets, evaluations are conducted on the provided `newstest14` file, and more details about them can be found in \citet{bojar2014findings}. 
For the IWSLT'14 De-En dataset, more analysis and details can be found in \citet{cettolo2014report}. 

\begin{table}[t]
\centering
\caption{ReZero Performance on IWSLT'14 De-En. Models are Transformer-small w. 6-layer encoder \& decoder.}
\label{tab:rezero}
\begin{tabular}{r|ccccc}
\toprule
Models & Admin & Post-LN & Pre-LN & ReZero & ReZero+Post-LN \\
\midrule
BLEU & 35.67$\pm$0.15 & 35.64$\pm$0.23 & 35.50$\pm$0.04 & 33.67$\pm$0.14 & 34.67$\pm$0.08 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[t]
\centering
\caption{Performance and model size on WMT'14 En-Fr (AL-BL refers A-layer encoder \& B-layer decoder).}
\label{tab:wmt14fr}
\begin{tabularx}{\linewidth}{lccYY}
\toprule
Methods & Param. \# & dim($W^{(1)}$) in FFN & Enc\#-Dec\# & BLEU \\
\midrule
T5-Base~\cite{raffel2019exploring} & 220 M & $512\times2048$ & 6L-6L & 41.2 \\
T5-Large~\cite{raffel2019exploring} & 770 M & $1024\times4096$ & 12L-12L & 41.5 \\
T5-3B~\cite{raffel2019exploring} & 3 B & $1024\times16384$ & 24L-24L & 42.6 \\
T5-11B~\cite{raffel2019exploring} & 11 B & $1024\times65536$ & 24L-24L & 43.4 \\
\midrule
Trans.Big-RNMT+~\cite{Chen2018TheBO} & 377 M &  $1024\times8192$ & 6L-6L & 41.12 \\
DynamicConv~\cite{wu2018pay} & 213 M & $1024\times4096$ & 7L-7L & 43.2 \\
DG-Transformer~\cite{Wu2019DepthGF} & 264 M & $1024\times4096$ & 8L-8L & 43.27 \\
Prime~\cite{zhao2019muse} & 252 M & $1024\times4096$ & 6L-6L & 43.48 \\
\midrule
Pre-LN (60L--12L) & 262 M &  $512\times2048$ & 60L-12L & 43.10 \\
Admin (60L--12L) & 262 M & $512\times2048$ & 60L-12L & 43.80 \\
\bottomrule
\end{tabularx}
\end{table}

As to model specifics, we directly adopt Transformer-small configurations on the IWSLT'14 De-En dataset and stacks more layers over the Transformer-base model on the WMT'14 En-De and WMT'14 En-Fr datasets.
Specifically, on the IWSLT'14 De-En dataset, we use word embedding with 512 dimensions and 6-layer encoder/decoder with 4 heads and 1024 feedforward dimensions; on the WMT'14 En-De and WMT'14 En-Fr datasets, we use word embedding with 512 dimension and 8-head encoder/decoder with 2048 hidden dimensions. 
Label smoothed cross entropy is used as the objective function with an uncertainty $= 0.1$~\citep{szegedy2016rethinking}. 

For Model training, we use RAdam as the optimizer~\cite{Liu2019OnTV} and adopt almost all hyper-parameter settings from \citet{lu2020understanding}.
Specifically, for the WMT'14 En-De and WMT'14 En-Fr dataset, all dropout ratios (including (activation dropout and attention dropout) are set to 0.1. 
For the IWSLT'14 De-En dataset, after-layer dropout is set to $0.3$, and a weight decay of $0.0001$ is used. 
As to optimizer, we set $(\beta_1, \beta_2) = (0.9, 0.98)$,  use inverse sqrt learning rate scheduler with a warmup phrase (8000 steps on the WMT'14 En-De/Fr dataset, and 6000 steps on the IWSLT'14 De-En dataset). 
The maximum learning rate is set to $1e^{-3}$ on the WMT'14 En-De dataset and $7e^{-4}$ on the IWSLT'14 De-En and WMT'14 En-Fr datasets. 
We conduct training for $100$ epochs on the WMT'14 En-De dataset, $90$ epochs on the IWSLT'14 De-En dataset and $50$ epochs on the WMT'14 En-Fr dataset, while the last 10 checkpoints are averaged before inference. 

On the IWSLT'14 De-En dataset, we conduct training on one NVIDIA GeForce GTX 1080 Ti GPU and set the maximum batch size to be $4096$. 
On the WMT'14 En-De dataset, we conduct training on four NVIDIA Quadro R8000 GPUs and set maximum batch size (per GPU) as $8196$. 
On the WMT'14 En-Fr dataset, we conduct training with the Nvidia DGX-2 server (6L-6L uses 4 NVIDIA TESLA V100 GPUs and 60L-16L uses 16 NVIDIA TESLA V100 GPUs) and set the maximum batch size (per GPU) as $8000$ for 6L-6L and $5000$ for 60L-16L. 
On the IWSLT'14 De-En dataset, Transformer-small models (w. 37 M Param.) take a few hours to train.
On the WMT'14 En-De dataset, 6L-6L models (w. 63 M Param.) take $\sim1$ day to train, 
12L-12L (w. 107M Param.) models take $\sim2$ days to train, 
and 18L-18L (w. 151M Param.) models take $\sim3$ days to train. 
On the WMT'14 En-Fr dataset, 6L-6L models (w. 67 M Param.) takes $\sim2$ days to train, and 60L-12L models (w. 262M Param.) takes $\sim 2.5$ days to train. 
All training is conducted in half-precision with dynamic scaling (with a 256-update scaling window and a 0.03125 minimal scale).
All our implementations and pre-trained models would be released publicly. 

\section{Comparison to ReZero}
\label{appendix:rezero}

Here, we first conduct comparisons with ReZero~\cite{Bachlechner2020ReZeroIA} under two configurations--the first employs the original ReZero model, and the second adds layer normalizations in a Post-LN manner. 
As summarized in Table~\ref{tab:rezero}, the ReZero initialization leads to a performance drop, no matter layer normalization is used or not. 
It verifies our intuition that over small dependency restricts the model potential. 
At the same time, we find that adding layer normalization to ReZero helps to improve the performance. 
Intuitively, as dropout plays a vital role in regularizing Transformers, layer normalization helps to not only stabilize training but alleviate the impact of turning off dropouts during the inference. 

\section{Performance on the WMT'14 En-Fr}
\label{appendix:enfr}

To explore the potential of \our, we conduct experiments with 72-layer Transformers on the WMT'14 En-Fr dataset (with a 60-layer encoder and 12-layer decoder, we add less layers to decoder to encourage the model to rely more on the source context). 

As in Table~\ref{tab:wmt14fr}, \our (60L--12L) achieves a BLEU score of 43.80, the new state-of-the-art on this long-standing benchmark. 
This model has a 60-layer encoder and a 12-layer decoder, which is significantly deeper than other baselines.
Still, since the number of parameters increases in a quadratic speed with regard to hidden dimensions and a linear speed with regard to layer numbers, our model has roughly the same number of parameters with other baselines. 
It is worth mentioning that \our even achieves better performance than all variants of pre-trained T5 models, which demonstrates the great potential of our proposed method.
Also, \our achieves a better performance than Pre-LN (60L--12L), which further verifies that the Pre-LN architecture restricts deep models' potential. 




