\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}



\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\usepackage{subfigure}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{graphicx} \usepackage{epstopdf}
\usepackage{makecell}
\usepackage{gensymb} \usepackage{multirow}
\usepackage{caption}
\usepackage{hyperref}


\usepackage{array}
\usepackage{tikz}
\newcommand*{\circled}[1]{\lower.7ex\hbox{\tikz\draw (0pt, 0pt)circle (.5em) node {\makebox[1em][c]{\small #1}};}}

\usepackage{verbatim}
\usepackage{color,xcolor}

\iccvfinalcopy 

\def\iccvPaperID{****} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ificcvfinal\pagestyle{empty}\fi

\begin{document}

\title{BoPR: Body-aware Part Regressor for Human Shape and Pose Estimation}
\author{Yongkang Cheng,  \hspace{1 mm} Shaoli Huang\thanks{Corresponding author.}, \hspace{1 mm} Jifeng Ning,\hspace{1 mm} Ying Shan \\
Northwest A\&F University \qquad Tencent AI Lab\\
{\tt\small \{shaolihuang,yingsshan\}@tencent.com} \qquad {\tt\small yongkangcheng@nwafu.edu.cn} \qquad {\tt\small njf@nwsuaf.edu.cn}}

\ificcvfinal\thispagestyle{empty}\fi
\twocolumn[{
\maketitle
\begin{center}
    \captionsetup{type=figure}
    \vspace{-2em}
    \includegraphics[width=0.85\textwidth]{title.pdf}
    \vspace{-1em}
    \captionof{figure}{\textbf{Illustration of our method's main superiority over existing approaches.} When given a challenging input (a), PARE (b)~\cite{kocabas2021pare} and CLIFF (c)~\cite{li2022cliff} struggle to accurately estimate the pose of the forearm due to occlusion and depth ambiguity. Our method (d), on the other hand, tackles these challenges by exploiting body-part attention dependency in per-part estimation rather than relying on independent part regressors or global regression. This approach enables our method to more effectively reason about the relative spatial relationship between different body parts and the body as a whole, leading to improved occlusion handling and reduced depth ambiguity.}


\label{fig:title}
\end{center}
}]

\begin{abstract}
This paper presents a novel approach for estimating human body shape and pose from monocular images that effectively addresses the challenges of occlusions and depth ambiguity. Our proposed method BoPR, the \textbf{Bo}dy-aware \textbf{P}art \textbf{R}egressor, first extracts features of both the body and part regions using an attention-guided mechanism. We then utilize these features to encode extra part-body dependency for per-part regression, with part features as queries and body feature as a reference. This allows our network to infer the spatial relationship of occluded parts with the body by leveraging visible parts and body reference information. Our method outperforms existing state-of-the-art methods on two benchmark datasets, and our experiments show that it significantly surpasses existing methods in terms of depth ambiguity and occlusion handling. These results provide strong evidence of the effectiveness of our approach.The code and data are available for research purposes at \href{https://github.com/cyk990422/BoPR}{ https://github.com/cyk990422/BoPR}.




\end{abstract}
\vspace{-1em}

\section{Introduction}

Human body shape and pose estimation from monocular images has become increasingly important in computer vision due to its wide range of applications in fields such as human-computer interaction, virtual reality, and digital human animation. This task involves reconstructing the human body by obtaining parameters of a human body model (such as SMPL~\cite{loper2015smpl} and GHUM~\cite{xu2020ghum}) from single RGB images. Regression-based methods~\cite{guler2019holopose,kanazawa2018end,omran2018neural,pavlakos2018learning,kocabas2021spec,kocabas2021pare,li2022cliff} that predict human model parameters from image features have made significant progress in recent years and have become the leading paradigm. However, they still face several limitations.

\begin{figure*}[t]
  \begin{center}
    \includegraphics[width=0.9\linewidth]{arc.pdf}
  \end{center}
  \caption{\textbf{BoPR network architecture.} Given an input image, our method first extracts body and part features based on a soft attention mechanism. Then each part feature is concatenated with the body feature as an input token to the transformer to encode body-aware part features for camera prediction and SMPL parameter regression.}
  \label{fig:arc}
  \end{figure*}


One major limitation is the handling of severe occlusions. Occlusion occurs when a body part is obscured by another object or body part, making it difficult to estimate its pose accurately. Most existing methods based on global feature learning usually struggle with occlusions, which limits their performance in real-world scenarios. Recent work~\cite{kocabas2021pare} has attempted to alleviate this issue by leveraging neighboring visible parts to improve the estimation of occluded parts. However, this strategy is unreliable as it may mistake the pose prediction of an adjacent visible part as the occluded one in some cases, for example, when a leg is impeded mainly by one arm.

Another limitation is the handling of depth ambiguity. Depth ambiguity occurs when the relative depth of two body parts cannot be determined from a single image, leading to errors in pose estimation. This is particularly challenging in monocular RGB settings together with self-occlusion. For example, as shown in Figure~\ref{fig:title}, given a human with hands behind the back, existing methods fail to estimate a reasonable pose of the forearms. However, a human can easily infer that the hands are behind the back in this image from a global perspective by using a combination of visual cues of the whole body, such as the torso orientation, upper arms, and missing forearms. Inspired by this, we propose a body-aware part regressor that aims to learn such global visual cues to infer the accurate pose of each body part.

Our method mainly consists of the attention-guided encoder that learns feature extraction associated with soft attention and the body-aware regression that enriches part representation with part-body dependency for part regressor. 
The former, built on ConvNets, extracts body and part features based on the exact soft attention mechanism. But the difference is that the body reference feature consists of two components, auxiliary supervised by global regression of 2D and 3D key points, respectively. This design encourages the body reference feature to encode more global information, such as 2D and 3D dependencies between parts.
The second module encodes a body-aware feature for each part by querying the part feature from the body reference feature using a two-layer transformer. Then, the resultant features are fed into a one-layer transformer to learn per-part regression with standard losses and a reference-plane consistent loss. During training, the body-aware regression component learns the part representation and correlation with the reference body, which guides the feature encoder to learn attention that finds helpful information from the part and the whole body. Therefore, our method incorporates local visual cues around the part and related global information from the entire body, allowing inferring the spatial relationship of occluded parts with the body to improve estimation accuracy.\\
\\
Our experiments first show that our method consistently outperforms state-of-the-art approaches on 3DPW~\cite{von2018recovering} and AGORA~\cite{patel2021agora} datasets. Then we evaluate our method on 3DPW-OCC~\cite{zhang2020object} and 3DOH\cite{zhang2020object}, demonstrating its occlusion handling effectiveness. We also compare axis MJE on the 3DPW-TEST dataset. The result shows that our method's performance gain over state-of-the-art methods is attributed mainly to the much lower error of Z-axis MJE. This demonstrates that it effectively reduces depth ambiguity in estimation. Finally, our qualitative results and ablation study confirm the proposed method's effectiveness.














\section{Related Work}
Our proposed method is a regression-based framework for 3D human body reconstruction from a single RGB image, which can effectively tackle the issues of occlusion and depth ambiguity. Therefore we review the related works that can be divided into regression-based methods, occlusion handling, and reducing depth ambiguity.



\textbf{Regression-based methods.} Estimating human mesh and pose from single images has attracted increasing attention in the computer vision community.  The mainstream methods in this task include optimization and regression-based. Optimization-based approaches concentrate on the optimization algorithms to fit parametric models (e.g., SMPL~\cite{loper2015smpl}) based on 2D observations such as keypoints and silhouettes. These methods have been criticized for having long optimization processes and high sensitivity to initialization~\cite{kolotouros2019learning}. Therefore, regression-based approaches  have become a more popular paradigm in recent years, as they can directly and fast regress parameters from image features.  HMR ~\cite{kanazawa2018end} is a milestone work that introduces reprojection loss of keypoints as weak supervision, enabling training regression models on in-the-wild datasets.  Recent work continues to explore more advanced weak supervision loss, such as introducing fine-grained correspondences ~\cite{omran2018neural,rueegg2020chained,xu2019denserac,zhang2020learning}, providing pseudo 3D ground truth ~\cite{kolotouros2019learning,joo2020exemplar}, and improving reprojection loss~\cite{kocabas2021spec,li2022cliff}. For instance, SPIN~\cite{kolotouros2019learning} incorporates the optimization process into the regression framework, allowing better-optimized 3D results as supervision for the network. SPEC~\cite{kocabas2021spec} estimates the perspective camera from a single image and utilizes it to improve reprojection loss. CLIFF~\cite{li2022cliff} proposes considering the box information and computing the reprojection loss from the original images, which facilitates predicting global rotations. However, all these methods employ global representation for regression, making them sensitive to occlusion and partially visible humans.  



\textbf{Occlusion handling.}
Handling occlusion is challenging yet crucial for human shape and pose estimation.  Simulating data containing occlusion is a straightforward way towards this issue. This line of work attempts to generate training data of occluded human bodies by cropping images~\cite{biggs20203d,joo2020exemplar,rockwell2020full}, overlaying objects~\cite{georgakis2020hierarchical,sarandi2018robust}, and augmenting feature maps~\cite{cheng20203d}. Despite the success achieved in reducing occlusion sensitivity, the realisticness of synthetic data is usually poor, resulting in unsatisfactory performance in dealing with real occlusion data. The other line of work relies on visibility cues to aid in dealing with occlusion. Cheng et al.~\cite{cheng2019occlusion} obtains the keypoint visibility label and masks the loss of invisible points in training, encouraging the network to infer occluded joints from visible parts. Yao et al.~\cite{yao2022learning} proposes VisDB, which first trains a network to predict the coordinates and visibility label of the mesh vertex and then incorporates them to regress the SMPL parameters. However, VisDB is a two-stage framework and requires a test-time optimization procedure to obtain the final results.  In contrast, PARE~\cite{kocabas2021pare} is an end-to-end learning framework that  infers the occluded parts by exploiting the attention mechanism to find helpful information from neighboring visible parts. However, the visual cues of adjacent visible parts sometimes are unreliable or insufficient to infer the occluded parts.  Unlike these approaches, Our method leverages visible parts and body reference information to infer the occluded parts.










\textbf{Reducing depth ambiguity.}
Depth ambiguity is inherent in estimating 3D pose from a single RGB image. To address this issue, Wang et al.~\cite{wang2019not} predicts a set of pose attributes that indicate the relative location of a limb joint with respect to the torso, which provides an explicit prior to reduce depth ambiguity.  Yao et al.~\cite{yao2022learning} predicts the vertex's visibility in the depth axis to resolve depth ordering ambiguities. The capability of these methods to deal with depth ambiguity mainly relies on the prediction of depth-related attribute labels. However, these label estimators are reasoned with local visual cues, suffering significant instability. For example, perturbation of local pixels will lead to wrong label predictions. In contrast, our method can effectively reduce the ambiguity of parts in depth space by learning a prior of part poses referring to the body.





\section{Approach}
As depicted in Figure \ref{fig:arc}, our method mainly includes an attention-guided encoder and a body-aware regression module. The first component aims to learn proper attention maps to locate informative regions for extracting parts and body features. The second module encodes body-aware part features based on these output features and feeds them into a one-layer transformer for camera and SMPL parameter regression. We introduce our method in more detail as follows. 

 

\subsection{Preliminaries}

\textbf{SMPL Model.} Our approach utilizes the SMPL parametric model to represent the human body. The model requires two essential characteristics: pose, denoted as , and shape, denoted as . 
The SMPL model produces a positioned 3D mesh  as a differentiable function. The reconstructed 3D joints are obtained as , where  and  is the pre-trained linear regression matrix.

\subsection{Attention-guided Encoder}
The attention-guided encoders(AGE) are designed to extract body reference and part query features from a single RGB image. We use a CNN backbone to extract feature volumes from the input image. From there, we leverage four convolutional layers to obtain the body attention map, body feature volume, part attention map, and part feature volume, respectively. 

\noindent\textbf{Body-Reference Feature.}
The first task of the AGE module is to extract body reference features. To achieve this, we pass the features extracted by the backbone network through a convolutional layer to obtain the body attention map , which is supervised by the body segmentation map. Next, we use two additional convolutional layers to obtain a 2D feature  and a 3D feature . These volumes help encode more global features and establish dependencies between 2D and 3D. To aid learning, we use global 2D/3D keypoint information for supervision. Then, we concatenate the two learned features to construct a global reference feature block . Finally, we extract body reference features  by performing Hadamard operations on the body attention map and the global reference feature block:

where  is the Hadamard product and  is used as a soft attention mask to aggregate features. By using this approach, we can better capture the global features of the human body and establish the relationship between 2D and 3D.

\noindent\textbf{Part Feature.}
Another task of the AGE module is to extract part-query features. To accomplish this, we feed the feature volume extracted by the backbone network to two convolutional layers, which respectively extract part-3D features  and part-attention maps . The learning of attention is supervised using part-segmentation maps. Similar to the extraction process for body-reference features, we perform a Hadamard operation on the part-feature block and the part-attention map to extract part-query features. This approach allows us to focus on the relevant parts of the body and extract features that are important for accurate 3D reconstruction.

\begin{figure}
  \centering
  \includegraphics[width=0.9\linewidth]{RD_TT.pdf}
  \vspace{-1em}
  \caption{\noindent\textbf{Illustration of the calculation of the relative depth from the torso plane.} }
  \label{rp}
\end{figure}

\subsection{Body-aware Regression}
A body-aware regression module (BAR) is introduced to encode body-aware features for each part while establishing associations between parts. These features are then utilized as input to the regression transformer, which predicts SMPL model parameters, camera parameters, and relative torso plane depth. Finally, the model parameters in the output are fed into the SMPL model to generate a 3D human body.
 \\
\noindent\textbf{Body-aware Part Feature.}
The BAR module first encodes body-aware features for each part. To achieve this, we concatenate the body-reference features from the AGE module to the part-query features and establish a set of tokens (where  means concatenate operation.) as the input of the two-layer body-aware transformer. Using this encoder, we query part features from body-reference features to encode body-aware part features , building part representations and dependencies to reference body. The resulting body-aware part feature sequences can  not only capture the unique characteristics of each part but also combine relevant global information from the entire body, which is crucial for accurate 3D reconstruction. By using the transformer to establish dependencies between the parts and reference body, our feature encoder can effectively exploit the spatial relationship between parts and bodies to generate more informative part representations. Overall, our feature encoder is capable of finding useful information from parts and the whole body and outputs a set of body-aware part features that are highly informative for the subsequent regression task. Specifically, the formula is as follows:

where  is the weight, Q, K and V are the matrices after linear mapping of input, and  is the sequence length which is used to transform the attention matrix into a standard normal distribution.

\noindent\textbf{Part Regressor.}
To help regression transformers better learn and perceive the correlation between parts and bodies, we propose a reference plane consistency loss. As shown in the figure~\ref{rp}, we select the shoulders, hips and pelvis points on the human torso to establish the frontal plane of the torso, the side plane of the torso and the cross-section of the torso respectively. In order to correspond to the 24 parts in the AGE module, we input the label parameters into the SMPL model to generate gt\_vertices, and take the center of each part area as the joint point. Define the joint point to bring into the plane equation greater than 0 as the positive direction, and calculate the depth value according to the distance from the point to the plane. Use this as the ground-truth label for relative depth, and construct explicit planar consistency constraints. This constraint helps the regression transformer to learn the spatial information between parts and bodies to solve the problem of depth ambiguity.

Next, we take the output of the body-awareness encoder as the input to a single-layer regression transformer to learn dependencies among the features of each body-aware part. The final BAR module outputs a set of SMPL model parameters, camera parameters, and relative depth  to the torso plane. We use the standard loss and the reference plane consistency loss for each part for training the network. 

































\subsection{Loss Functions}
We formulate the training of BoPR as a supervised learning problem, in which we aim to minimize the discrepancy between the predicted outputs and the ground truth annotations. To this end, we define a set of loss functions that capture different aspects of the model's performance. Given a training dataset , where  is the total number of training images,  denotes the RGB image,  represent the ground truth SMPL parameters,  represents the relative depths of points corresponding to 24 SMPL parts to three torso planes,  denotes the ground truth 3D coordinates and its confidence of the body joints, where  is the number of joints of a person, and  denotes the ground truth 2D coordinates of the body joints. We also generate  and , which serve as segmentation labels for whole bodies and ground truth bodily parts, respectively.

As an additional step for supervising body and parts attention, we project the vertices generated by the SMPL model onto pixel coordinates to generate segmentation labels  and . This allows us to supervise attention learning at the pixel level, providing more detailed information for evaluating model performance.

Let  denote the pose parameters and  is the shape parameters, we use SMPL model output  which denotes the body vertices.To compute the keypoint loss,we need the SMPL 3D joints  with a pretrained linear regressor .With the inferred weak-perspective camera,we compute the 2D projection of the 3D joints ,as ,where  is the camera rotation matrix and  is the orthographic projection.

We use the combination of loss functions to train BoPR, including AFE loss and BAR loss:

where each term is calculated as:

where  represents the ground truth for the corresponding variable . In addition, auxiliary supervision losses are ,  and . Among them, for , we only supervise on the COCO-EFT dataset. While body reference features are supervised for all training epochs.
























\begin{table*}
	\centering
\begin{tabular}{lcccccc}
        \toprule[1.5pt]
		\multirow{2}{*}{Method} & \multicolumn{3}{c}{3DPW-ALL}   & \multicolumn{3}{c}{3DPW-TEST} \\
		& MJE & PAMJE & PVE & MJE & PAMJE & PVE  \\ \hline
		VIBE~\cite{kocabas2020vibe} & 93.5 & 56.5 & 113.4 &82.7 &51.9 &99.1\\ 
		MEVA~\cite{luo20203d} & 86.9 & 54.7 & - &- &- &- \\
  MAED~\cite{wan2021encoder} & - & - & - &79.1 & 45.7 &92.6 \\ 
  \hline
		Pose2Mesh~\cite{choi2020pose2mesh} & 89.2 & 58.9 & -& 89.5 & 56.3&105.3 \\
		I2L-MeshNet~\cite{moon2020i2l} & 93.2 & 58.6 & - & -& -&- \\
            METRO~\cite{lin2021end} &- &- &- &77.1 &47.9 &88.2\\
            Mesh-G~\cite{lin2021mesh} &- &- &- &74.7 &45.6 &87.7\\
            HybrIK~\cite{li2021hybrik} & 80.0 & 48.8 & 94.5 & 74.1 & 45.0 & 86.5\\
            FastMETRO–L–H64~\cite{cho2022cross} &- &- &- &73.5 &44.6 &84.1\\
            \hline
		SPIN~\cite{kolotouros2019learning} & 96.9 & 59.2 &116.4 & -& -&-\\
		HMR-EFT~\cite{joo2020exemplar} &85.1 & 52.2 & 118.5 &- &- &-\\
		ROMP~\cite{sun2021monocular} & 82.7 & 60.5 & - & 76.7 & 47.3 & 93.4\\
		PARE~\cite{kocabas2021pare} & 82.0 & 50.9 & 97.9 & 74.5 & 46.5 & 88.6\\ 
            VisDB~\cite{yao2022learning} &- &- &- & 72.1 &44.1 &83.5\\
            CLIFF(HR-W48)~\cite{li2022cliff} & - & - & - & 69.0 & 43.0 & 81.2\\ \hline
		BoPR(HR-W32) & 76.1 & 45.9 & 90.9 & 68.8 & \textbf{41.8} & 81.7  \\
BoPR(HR-W48) & \textbf{72.0} & \textbf{45.3} & \textbf{86.5} & \textbf{65.4} & 42.5 & \textbf{80.8} \\ 
        \bottomrule[1.5pt]
	\end{tabular}
\caption{\textbf{Performance comparison on the 3DPW dataset.} For the evaluation on 3DPW-ALL, all methods are trained without 3DPW dataset. However, for the one on 3DPW-TEST, all methods involve 3DPW training sets in training. }
\label{tab:3dpw}
\end{table*}





\section{Experiments}

\noindent\textbf{Implementation Details.}
In our implementation, the input size of the network is . We use standard data augmentation practices during training, including random rotations, scaling, horizontal flipping, and cropping. Also, we adopt the widely used Adam optimizer to train the network for 240K steps with a batch size of 64 and a learning rate of . 



\noindent\textbf{Datasets.}
For the experiments on the 3DPW dataset, we used four large-scale datasets for training, including COCO-EFT (74K)~\cite{lin2014microsoft}, MPII (14K)~\cite{andriluka20142d}, MPI-INF-3DHP (90K)~\cite{mehta2017monocular}, and Human3.6M (292K)~\cite{ionescu2013human3}. We first pre-trained the network on COCO-EFT (74K) and then sequentially fine-tuned it on the mixed and 3DPW datasets. After fine-tuning the network on the 3DPW datasets, we further fine-tuned the model on Agora~\cite{patel2021agora} and evaluated its performance on the Agora test set.




\begin{table}
\centering
\begin{tabular}{lcccc}
\toprule[1.5pt]
\multirow{2}{*}{Method} & \multicolumn{2}{c}{Agora}  \\ 
 & MJE  & V2V  \\ \hline
SPIN~\cite{kolotouros2019learning} & 153.4 & 148.9 \\ 
PARE~\cite{kocabas2021pare} & 146.2 & 140.9 \\ 
ROMP~\cite{sun2021monocular} & 108.1 & 103.4 \\
BEV~\cite{sun2022putting} & 105.3 & 100.7 \\ 
Hand4Whole~\cite{sun2022putting} & 89.8 & 84.8 \\ 
CLIFF~\cite{li2022cliff} & 81.0 & 76.0 \\ 
\hline
BoPR & \textbf{79.9} & \textbf{74.5}  \\ \bottomrule[1.5pt]
\end{tabular}
\caption{\textbf{Performance comparison on the AGORA dataset.} Here, all the comparing methods adopt weak perspective projection and also fine-tune the model on the AGORA training set (except PARE).}
\label{tab:agora}
\end{table}

\noindent\textbf{Evaluation Metrics.}
We evaluate our method using three common metrics: MJE~\cite{ionescu2013human3}, PAMJE~\cite{gower1975generalized}, and PVE~\cite{pavlakos2018learning}. These metrics calculate millimeter-scale Euclidean distances between 3D points in predicted and ground-truth data. We also report the results of axis MJE (x-MJE, y-MJE, and z-MJE) to analyze the detailed improvements of each axis.












\noindent\textbf{Comparison to the state-of-the-art.}
We first comprehensively compare BoPR with three types of state-of-the-art methods, including video-based, model-free, and model-based methods on the 3DPW dataset. In this experiment, we report two evaluation results, namely 3DPW-ALL and 3DPW-TEST. Here,  all methods do not include the 3DPW training dataset in training to evaluate the 3DPW-ALL setting, while for the 3DPW-TEST, they all fine-tune the network on the 3DPW training dataset. In addition, we provide the results of two network structures (HR-W48 and HR-W32) on these two settings, respectively. Table~\ref{tab:3dpw} presents the comparison results with various methods. Our method's performance using the HR-W32 backbone network has surpassed almost all existing methods in all metrics but is only slightly lower PVE than the CLIFF method using the HR-W48 backbone network. However, when using the HR-W48 backbone network, our method significantly outperforms the previous state-of-the-art method CLIFF on 3DPW-TEST, and MJE is reduced from 69.0 to 65.4. It is also worth mentioning that the evaluation results in 3DPW-ALL can better reflect the generalization ability of the method. Yet, in this setting, our method also outperforms the previous state-of-the-art method HybrIK by a large margin (MJE from 80.0 to 72.0).



\begin{figure}
  \centering
  \includegraphics[width=0.9\linewidth]{att.pdf}
  \caption{\noindent\textbf{The role of body-reference feature.} From left to right: input image, body attention map, the result of discarding body reference features, and the result of the full model.}
  \label{fig:att}
\end{figure}

\begin{table}
\centering
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{lcccc}
\toprule[1.5pt]
\multirow{2}{*}{Method} & \multicolumn{4}{c}{3DPW-Test}  \\ 
 & MJE & MJE & MJE & MJE  \\ \hline
PARE(HR-W32) &74.5 &30.3 &28.9 &78.9 \\ 
CLIFF(HR-W48) &72.7 &27.1 &27.0 &51.8 \\ \hline
BoPR(HR-W32) &68.8 &25.9 &27.5 &\textbf{46.6} \\
BoPR(HR-W48) &\textbf{65.1} &\textbf{22.0} &\textbf{23.4} &48.1 \\\bottomrule[1.5pt]
\end{tabular}
}
\caption{\textbf{Axis-wise performance comparison on the 3DPW-Test.} All methods have been trained on dataset with 3DPW.}
\label{tab:occ}
\end{table}


\begin{table}
\centering
\begin{tabular}{lcccc}
\toprule[1.5pt]
\multirow{2}{*}{Method} & \multicolumn{2}{c}{3DPW-OCC} & \multicolumn{2}{c}{3DOH} \\ 
 & MJE & PAMJE & MJE & PAMJE \\ \hline
PARE(HR-W32) &84.9 &57.5 &94.9 &62.4  \\ 
CLIFF(HR-W48) &- &- &80.4 &58.7 \\\hline
BoPR(HR-W32) &81.7 &49.5 &82.7 &\textbf{55.5} \\
BoPR(HR-W48) &\textbf{74.6} &\textbf{48.3} &\textbf{78.3} &56.3 \\\bottomrule[1.5pt]
\end{tabular}

\caption{\textbf{Performance comparison on occlusion datasets 3DPW-OCC and 3DOH.} For the evaluation of 3DPW-OCC, all methods are trained without the 3DPW dataset. While for 3DOH, all methods fine-tune the network on the 3DPW training set.}
\label{table1}
\end{table}

\begin{table}
\centering
\begin{tabular}{lcccc}
\toprule[1.5pt]
\multirow{2}{*}{Method} & \multicolumn{3}{c}{3DPW}  \\ 
 & MJE & PAMJE & PVE  \\ \hline
PARE &91.0 &56.7 &108.2 \\ 
PARE w TF &91.5 &55.7 &108.6\\ \hline
BoPR w/o  &89.4 &55.8 &106.7\\
BoPR w/o  &90.6 &54.8 &107.2\\
BoPR w/o  &89.0 &54.2 &105.7\\
BoPR &\textbf{88.6} &\textbf{53.7} &\textbf{105.3}\\
\bottomrule[1.5pt]
\end{tabular}

\caption{\textbf{Ablation study of BoPR on 3DPW}.}
\label{tab:abl}
\end{table}

\begin{table*}
\centering
\begin{tabular}{lcccccccc}
\toprule[1.5pt]
\multirow{2}{*}{Method} & \multicolumn{1}{c}{Elbow} & \multicolumn{1}{c}{Wrist} & \multicolumn{1}{c}{Head}  & \multicolumn{1}{c}{Knee} & \multicolumn{1}{c}{Ankle} & \multicolumn{1}{c}{Hip} & \multicolumn{1}{c}{Neck} & \multicolumn{1}{c}{Shoulder}\\ 
 & MJE & MJE & MJE & MJE  & MJE  & MJE  & MJE  & MJE\\ \hline
PARE &79.2 &110.5 &93.8 &70.3 &120.8 &29.1 &64.6 &63.2 
\\ \hline
BoPR &\textbf{69.3} \textcolor{green}{(-9.9)} &\textbf{98.6} \textcolor{green}{(-11.9)} &\textbf{74.5} \textcolor{green}{(-19.3)} &\textbf{61.9} \textcolor{green}{(-8.4)} &\textbf{107.7} \textcolor{green}{(-13.1)} &\textbf{28.3} \textcolor{green}{(-0.8)} &\textbf{53.7} \textcolor{green}{(-10.9)}&\textbf{53.6} \textcolor{green}{(-9.6)}\\\bottomrule[1.5pt]
\end{tabular}
\caption{\textbf{Per-part performance comparison on the 3DPW-Test.} All methods have been trained on dataset with 3DPW.}
\label{tab:perpart}
\end{table*}



We also evaluate our method on a multi-person dataset AGORA and compare it with state-of-the-art methods. Furthermore, since camera parameter estimation plays a crucial role in the performance of the AGORA dataset,  we compare our method with those using weak-perspective projection in training for a fair comparison. Table~\ref{tab:agora} shows the result where our method performs better than multi-person-based approaches and previous state-of-the-art method CLIFF.













\noindent\textbf{Improved occlusion handling.}
Table ~\ref{tab:occ} showcases the results of our proposed BoPR method, the baseline PARE method, and the CLIFF method on the occlusion datasets 3DPW-OCC and 3DOH. All methods were evaluated on 3DPW-OCC without using the 3DPW training dataset. When evaluating 3DOH, they fine-tuned the network on the 3DPW training dataset. Both evaluations excluded their own training set to ensure a better comparison of the generalization ability on occlusion datasets. BoPR outperformed PARE in both MJE and PAMJE and showed superior performance compared to CLIFF and PARE on 3DOH. Specifically, on the 3DPW-OCC dataset, BoPR reduced MJE and PAMJE by 5.8\% and 10.9\%, respectively, compared to PARE. On 3DOH, BoPR achieved a 2.6\% improvement over CLIFF. These results suggest that part-aware regression with body-reference conditioning enhances occlusion handling and improves accuracy and generalization in challenging scenarios.



\noindent\textbf{Reduced depth ambiguity.}
To further analyze the issue of depth ambiguity in 3D human reconstruction, we evaluate the average per joint position error (MJE) in the x, y, and z axis on the 3DPW-Test dataset. As shown in Table 4, we can see that the error on the z-axis is significantly higher than that on the x and y-axis in existing methods. This indicates that the depth ambiguity problem in 3D human reconstruction is a significant challenge that needs to be addressed. However, our approach outperforms the state-of-the-art methods by a significant margin in the z-axis metric, achieving an 11.1\% improvement compared to CLIFF. This result highlights the importance of body-aware feature encoding, which enables the inference of spatial relationships between body and parts through local visual clues around body parts and relevant global information from the whole body, effectively alleviating the problem of depth ambiguity.


\begin{figure*}
  \centering
  \includegraphics[width=0.95\linewidth]{quali.pdf}
  \vspace{-1em}
  \caption{Qualitative comparison of PARE, CLIFF, and our method BOPR on in-the-wild datasets including LSPET (Row1-3) and 3DPW (Rows4-5).}\label{fig:qua}
\end{figure*}

\noindent\textbf{Does Body-aware features help?}
To understand the role of body-aware features, we first visualize the attention map that produces body-reference features in Figure~\ref{fig:att}. We can find that these attention maps mainly focus on the torso area of the human body and have strong attention to the upper spine, shoulders, and hips. Interestingly, these body parts are crucial in determining global information, such as orientation, body shape, and overall body posture. In addition, we also compare the prediction results after zeroing the reference features with the original method. As shown in the last two columns of Figure~\ref{fig:att}, after the reference feature is set to 0, the global scale of the mesh and the prediction of the relative position relationship of the parts relative to the body significantly deteriorated. This suggests that body-aware features encode global scale information and component-body dependencies.

\noindent\textbf{Which part benefits most?} To understand the performance improvement of our method for each part, we provide per-part performance comparison results in Table~\ref{tab:perpart}. We can see that compared with PARE, our approach has a slight error drop for the Hip joint, but the error drop for other joints is all above 10mm. Moreover, the errors of challenging joints such as the Wrist and Ankle reduce by 12mm and 13mm, respectively. The most significant error drop (by 19mm)  is the Head joint.








\noindent\textbf{Qualitative comparison.}
In Figure~\ref{fig:qua}, we compare the performance of PARE, CLIFF, and BoPR on different test datasets. To render the human geometry created by the SMPL model into images, we use the predicted camera parameters and weak perspective projection~\cite{kissos2020beyond}. The results show that PARE and CLIFF struggle to accurately infer the depth information between limbs and exhibit ambiguities in the orientation of the human body under large occlusions. However, thanks to the body reference condition dependency established by the BAR module, BoPR can more accurately recover these motions, especially for complex actions in the LSPET~\cite{johnson2011learning} dataset.More examples are in Sup.Mat.






\noindent\textbf{Ablation Experiments.}
In this experiment, we use HR-W32 as the backbone and train all the comparing methods on a small COCO-2014-EFT (22K), part of the COCO-EFT dataset.  We first incorporate PARE with a transformer regressor and evaluate the performance on the 3DPW dataset. Table~\ref{tab:abl} shows that combining two techniques slightly reduces the PAMJE yet  increases the MJE and PVE.  In contrast, our proposed method improves PARE by a large margin, indicating that learning body-aware part features contributes significantly to the performance gain. We also ablate the auxiliary losses, namely the 2D keypoint , 3D keypoint , and the reference distance . The result in Table~\ref{tab:abl} shows all the loss helps to improve the effect.






\section{Conclusion}
The proposed body-aware part regressor significantly improves over existing methods for human body shape and pose estimation from monocular images. This is achieved by incorporating two vital technical insights. First, we exploit the soft attention mechanism in the attention-guided encoder to capture global information and dependencies between body parts. Moreover, we enrich part representation with part-body dependence by leveraging local and global information. Furthermore, these insights allow the proposed method to address the limitations of current approaches, such as handling severe occlusions and depth ambiguity, and achieve state-of-the-art performance in various datasets and scenarios. Overall, this work contributes to advancing the state-of-the-art in human body shape and pose estimation and provides valuable technical insights for future research.
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}