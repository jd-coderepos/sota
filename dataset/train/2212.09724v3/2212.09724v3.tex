\subsection{Preliminaries}
\noindent \textbf{Knowledge Graph.} Given a set of entities $\mathcal{E}$ and a set of relations $\mathcal{R}$, a knowledge graph can be defined as a collection of facts $\mathcal{F} \subseteq \mathcal{E} \times \mathcal{R} \times \mathcal{E}$ where for each fact $f = (h, r, t)$, $\;h,t \in \mathcal{E}, r \in \mathcal{R}$.\\

\noindent \textbf{Link Prediction.} The task of link prediction is to infer the missing facts in a KG\@. Given a link prediction query $(h, r, ?)$ or $(?, r, t)$, the model ranks the target entity among the set of candidate entities. For the query $(h, r, ?)$, $h$ and $r$ correspond to the source entity and the query relation, respectively.
\subsection{Retriever}
The function of the retriever module is to select a relevant subgraph of the KG as the query context.
We use the following off-the-shelf methods to generate subgraph inputs for the Transformer-based reader module in our framework.

\begin{figure*}[htbp]
  \centering
  \includegraphics[width=\textwidth]{figs/embed_layer.pdf}
  \caption{Schematic representation of embedding layer for subgraph input. The Transformer input is the sum of the token lookup embedding, the token type embedding, and the segment embedding.}
  \label{fig:model_embed}
\end{figure*}

\subsubsection{Uninformed Search-based Retrieval Strategies}
\begin{enumerate}[label=\alph*)]
\item\textbf{Breadth-first search}: For breadth-first search, we sample edges starting from the source entity in breadth-first order till we reach the context budget.
\item\textbf{One-hop neighborhood}: The one-hop neighborhood includes edges in the immediate one-hop neighborhood of the source entity.
\end{enumerate}

\subsubsection{Informed Search-based Retrieval Strategies}

We use \textbf{MINERVA} \citep{das2018go} as our informed search-based retrieval strategy.
It formulates KG link prediction as the generation of multi-hop paths from the source entity to the target entity. The environment is represented as a Markov Decision Process (MDP) on the KG, where the reinforcement learning agent gets a positive reward on reaching the target. The set of paths generated by MINERVA provides an interpretable provenance for KG link prediction. The retriever model utilizes the union of these paths decoded using beam search as the subgraph output.



Among these approaches, breadth-first search and one-hop neighborhood use uninformed search, \ie, they only enrich the query using surrounding context without explicitly going toward the target entity. On the other hand, the subgraph obtained using the MINERVA retriever aims to provide context which encloses information towards reaching the target entity.

\subsection{Reader Architecture}

\noindent \textbf{Embedding Layer.}
The input to the Transformer is obtained by summing the token lookup embedding, the token type embedding, and the segment embedding (Figure~\ref{fig:model_embed}):
\begin{itemize}
\item \textbf{Token lookup embedding}: We use learned lookup embeddings for all entities and relations. These lookup embeddings store the global semantic information for each token.
    \item \textbf{Token type embedding}: Entity and relation tokens have different semantics, so we use token type embeddings to help the model distinguish between them.
    \item \textbf{Segment embedding}: It denotes whether a particular entity token corresponds to the terminal entity in a path beginning from the source entity. This helps the model to differentiate between the terminal tokens, which are more likely to correspond to the final answer vs.\ other tokens.
\end{itemize}
The input to the reader module is the query, \eg, $(h, r, ?)$, and its associated context, a subgraph of the KG\@. The input sequence for the subgraph encoder is constructed by concatenating the sets of node and edge tokens.
Each edge corresponds to a unique token in the input, though there might be multiple edges with the same predicate.
At a high level, the query and subgraph are first encoded by their respective Transformer encoders.  The query self-attention encoder takes ``\texttt{[CLS], [source entity], [query relation]}'' as the input sequence with a fully-connected attention structure. Then the cross-attention module is used to modulate the subgraph representation, conditioned on the query.\\

\noindent \textbf{Graph-induced Self-Attention.} The attention structure ($\mathcal{A}_i$) governs the set of tokens that a particular token can attend to in the self-attention layer of the subgraph encoder. It helps incorporate the (sub)graph structure into the transformer representations.
Inspired by KG-augmented PLMs \cite{he-etal-2020-bert, he2021klmo}, we define the attention structure (Figure~\ref{fig:model_overall}) such that 1) all node tokens can attend to each other; 2) all edge tokens can attend to each other; 3) for a particular triple $(h, r, t)$, the token pairs $(h, r)$ and $(r, t)$ can attend to each other, and 4) each token attends to itself. This design is motivated by the need to maintain a balance between the immediate graph neighborhood of a token vs.\ its global context in the subgraph.

More formally, let the subgraph consist of $m$ nodes and $n$ edges.
Let ${\{\bm{h}_i^{\ell}\}}_{i=1}^{m+n}$ denote the hidden representations of the tokens in layer $\ell$ of the subgraph self-attention encoder,
\begin{align}
    \bm{h}_{i}^{\ell+1} &= \mathbf{O}^{\ell} \ \bigparallel_{k=1}^{H} \bigg(\sum_{j \in \mathcal{A}_i} w_{ij}^{k,\ell} \mathbf{V}^{k,\ell}\bm{h}_j^{\ell} \bigg),\\
    w_{ij}^{k,\ell} &= \textnormal{softmax}_{j \in \mathcal{A}_i} \bigg( \frac{\mathbf{Q}^{k, \ell} \bm{h}_i^{\ell}\cdot \mathbf{K}^{k, \ell}\bm{h}_j^{\ell}}{\sqrt{d_k}}\bigg).
\end{align}
\noindent Here, $\mathbf{Q}^{k,\ell}, \mathbf{K}^{k,\ell}, \mathbf{V}^{k,\ell} \in \mathbb{R}^{d_k \times d}$, $\mathbf{O}^{\ell} \in \mathbb{R}^{d \times d}$ are projection matrices for the $k^{th}$ attention head in layer $l$, $H$ denotes the number of attention heads per layer, $d_k$ denotes the hidden dimension of keys and $\|$ denotes concatenation.\\

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.45\textwidth]{figs/model_figure.pdf}
    \caption{Reader module architecture. In the subgraph self-attention encoder, the graph-induced self-attention design governs the attention between tokens. In the cross-attention encoder, the link prediction query hidden states serve as the query input, while the subgraph hidden states serve as the key and value inputs.}
    \label{fig:model_overall}
\end{figure}




\noindent \textbf{Cross-Attention.} To answer a link prediction query, the model needs a way to filter the context in the subgraph that is relevant to a particular link prediction query. To accomplish this, we introduce cross-attention from the query to the subgraph (Figure~\ref{fig:model_overall}). Following \citet{vaswani2017attention}, the queries come from query hidden states, whereas the keys and values come from the subgraph hidden states. The resultant representation encodes the subgraph information that is relevant to the query at hand. 
More formally, Let $\bm{e}^{\textnormal{q}, \ell}_i$ and $\{\bm{e}^{\textnormal{sub}}_j\}_{j=1}^{m+n}$ denote the self-attention hidden representations of $i^{th}$ query token and the subgraph tokens, respectively,

\begin{align}
&\textrm{Cross-Attention}(\{\bm{e}^{\textnormal{q}, \ell}_i\}, \{\bm{e}^{\textnormal{sub}}_j\}_{j=1}^{m+n}) \nonumber\\
&= \mathbf{O}^{\ell} \ \bigparallel_{k=1}^{H} \bigg(\sum_{j=1}^{m+n} w_{ij}^{k,\ell} \mathbf{V}^{k,\ell}\bm{e}_j^{\textnormal{sub}} \bigg),\\
w_{ij}^{k,\ell} &= \textnormal{softmax}\bigg( \frac{\mathbf{Q}^{k, \ell} \bm{e}_i^{\textnormal{q}, \ell}\cdot \mathbf{K}^{k, \ell}\bm{e}_j^{\textnormal{sub}}}{\sqrt{d_k}}\bigg).
\end{align}

\noindent This is concatenated with the contextualized representation of the source entity in the subgraph to output the feature vector for predicting the plausibility scores.
For a link prediction query \eg, $(h, r, ?)$, the model predicts a score distribution over all candidate entities. The model is trained using cross-entropy loss, framing it as a multi-class classification problem. Figure~\ref{fig:model_overall} illustrates the overall model architecture of the Transformer-based reader.
\begin{table}[htbp]
\centering
\caption{Dataset Statistics}
\label{tab:dataset_stat}
\begin{tabular}{cccccc}
\toprule
\bfseries Dataset   & $\bm{|\mathcal{E}|}$      & \bfseries $\bm{|\mathcal{R}|}$   & \multicolumn{3}{c}{\# \bfseries Facts} \\ \cmidrule{4-6}
          &        &     & \bfseries Train    & \bfseries Valid   & \bfseries Test                      \\ \midrule
FB15K-237 & 14,541 & 237 & 272,115  & 17,535  & 20,466            \\
WN18RR    & 40,943 & 11  & 86,835   & 3,034   & 3,134       \\ \bottomrule        
\end{tabular}
\end{table} \begin{table*}[htbp]
\centering
\caption{Comparison of our framework with baseline methods. For all metrics, higher is better. Missing values are denoted by --. Results of RESCAL, TransE, DistMult, ComplEx, and ConvE correspond to the best results obtained after extensive hyperparameter tuning \citep{Ruffinelli2020You}.
Result of Neural LP and DRUM are taken from \citet{DBLP:conf/iclr/QuCXBT21} following the standard evaluation setting.
Results of other methods are taken from their original papers.
}
\label{tab:results_overall}
\begin{tabular}{llcccc|cccc}
\toprule
& & \multicolumn{4}{c}{\bfseries FB15K-237} & \multicolumn{4}{c}{\bfseries WN18RR} \\ \cmidrule{2-10}
\bfseries Framework & \bfseries Model                                  & \bfseries MRR        & \bfseries Hits@1    & \bfseries Hits@3    & \bfseries Hits@10   & \bfseries MRR      & \bfseries Hits@1   & \bfseries Hits@3   & \bfseries Hits@10   \\ \midrule
\textbf{Embedding-} & RESCAL \citep{nickel2011three} & .356	& .263 & .393 & .541 & .467	& .439 & .480 & .517 \\
\textbf{based} & TransE \citep{bordes2013translating} & .313 & .221 & .347 & .497 & .228 & .053 & .368 & .520  \\
& DistMult \citep{yang2014embedding} & .343	& .250 & .378 & .531 & .452	& .413 & .466 & .530\\
& ComplEx \citep{trouillon2016complex} & .348 & .253 & .384 & .536 & .475 & .438 & .490 & .547\\
& ComplEx-N3 \citep{Lacroix2018CanonicalTD} & .37 & -- & -- & .56 & .48 & -- & -- & .57 \\
& RotatE \citep{sun2018rotate} & .338 & .241 & .375 & .533 & .476 & .428 & .492 & .571\\ \midrule
\textbf{CNN-based} & ConvKB \citep{nguyen2017novel} & .243 & .155 & .371 & .421 & .249 & .057 & .417 & .524 \\
& ConvE \citep{dettmers2018convolutional} & .339 & .248	& .369 & .521 & .442 & .411 & .451 & .504\\
\midrule
\textbf{Path-based} & MINERVA \citep{das2018go} & .293 & .217 & .329 & .456 & .448 & .413 & .456 & .513\\
& Neural LP \citep{yang2017differentiable} &  .237 & .173 & .259 & .361 & .381 & .368 & .386 & .408\\
& DRUM \citep{sadeghian2019drum} & .238 & .174 & .261 & .364 & .382 & .369 & .388 & .410\\
\midrule
\textbf{GNN-based} & R-GCN \citep{schlichtkrull2018modeling} & .248 & .151 & -- & .417 & -- & -- & -- & -- \\
& CompGCN \citep{vashishth2020compositionbased} & .355 & .264 & .390 & .535 & .479 & .443 & .494 & .546 \\
\midrule
\textbf{Transformer-based} & HittER \citep{chen-etal-2021-hitter} & .373 & .279 & .409 & \textbf{.558} & \textbf{.503} & \textbf{.462} & \textbf{.516} & \textbf{.584} \\
 \midrule
& \textbf{KG-R3} (this work) & \textbf{.390} & \textbf{.315}  & \textbf{.413}  & .539 & .472 & .439 & .481 & .537 \\ \bottomrule
\end{tabular}
\end{table*} 