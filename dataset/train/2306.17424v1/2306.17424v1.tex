



\documentclass{article}
\usepackage[T1]{fontenc} \usepackage[utf8]{inputenc} \usepackage{ismir,amsmath,cite,url}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{subfiles}
\usepackage{amsfonts}
\usepackage{float}
\usepackage{paralist}
\usepackage{units}
\usepackage{microtype}
\usepackage{caption, subcaption}
\usepackage{multirow}
\usepackage{boldline}

\usepackage{lineno}


\title{Audio Embeddings as Teachers for Music Classification}




\twoauthors
 {Yiwei Ding} {Music Informatics Group \\ Georgia Institute of Technology \\ {\tt yding402@gatech.edu}}
 {Alexander Lerch} {Music Informatics Group \\ Georgia Institute of Technology \\ {\tt alexander.lerch@gatech.edu}}







\def\authorname{Y. Ding and A. Lerch}



\sloppy 

\begin{document}

\maketitle
\begin{abstract}
Music classification has been one of the most popular tasks in the field of music information retrieval.
With the development of deep learning models, the last decade has seen impressive improvements in a wide range of classification tasks.
However, the increasing model complexity makes both training and inference computationally expensive.
In this paper, we integrate the ideas of transfer learning and feature-based knowledge distillation and systematically investigate using pre-trained audio embeddings as teachers to guide the training of low-complexity student networks.
By regularizing the feature space of the student networks with the pre-trained embeddings, the knowledge in the teacher embeddings can be transferred to the students.
We use various pre-trained audio embeddings and test the effectiveness of the method on the tasks of musical instrument classification and music auto-tagging.
Results show that our method significantly improves the results in comparison to the identical model trained without the teacher's knowledge.
This technique can also be combined with classical knowledge distillation approaches to further improve the model's performance.


\end{abstract}


\section{Introduction}\label{sec:introduction}
\subfile{sections/introduction}

\section{Related Work}\label{sec:related_works}
\subfile{sections/related_works}


    \begin{figure*}[htbp]
     \centering
     \includegraphics[width=0.95\linewidth]{figs/pipeline.pdf}
     \caption{Overall pipeline of training a model by using pre-trained embeddings as teachers. The training loss is a weighted sum (weighting factor omitted in the figure) of prediction loss and regularization loss. The regularization loss measures the distance between pre-trained embedding and the output feature map after the feature alignment. During inference, only the bottom part with the blue background is used.}\label{fig:pipeline}
    \end{figure*}
    
\section{Methods}\label{sec:proposed_methods}
\subfile{sections/proposed_methods}

\section{Experimental Setup}\label{sec:experiments}
\subfile{sections/experiments}

\section{Results}\label{sec:results}
\subfile{sections/results}

\section{Conclusion and Future Work}\label{sec:conclusion}
\subfile{sections/conclusion}

\bibliography{ISMIRtemplate}



\end{document}
