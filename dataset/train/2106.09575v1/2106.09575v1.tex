In this section, we begin by presenting our primary results on the Open Catalyst 2020 (OC20) dataset \cite{OC20} and compare against state-of-the-art models. This is followed by results on the smaller datasets of MD17 \cite{chmiela2017machine,chmiela2018towards} and QM9 \cite{ramakrishnan2014quantum} for additional model comparison.

\begin{table*}[t]
    \centering
        
    
    \renewcommand{\arraystretch}{1.0}
    \setlength{\tabcolsep}{5pt}
    \resizebox{0.97\linewidth}{!}{
    \begin{tabular}{lrrrrrccccc}
      \toprule
         \mr{2}{\textbf{Model}} & \textbf{Hidden} & \textbf{\#Msg} & \mr{2}{\textbf{\#Params}} & \textbf{Train}  & \textbf{Inference} & \mc{4}{c}{\textbf{\ocd{} Test}} \\
        & \textbf{dim} & \textbf{layers} & & \textbf{time} & \textbf{time} & Energy MAE [eV] $\downarrow$ & Force MAE [eV/\AA] $\downarrow$ & Force Cos $\uparrow$ & EFwT [\%] $\uparrow$  \\
      \midrule
        Median & -- & -- & -- & & & 2.258 & 0.08438 & 0.0156 & 0.005 \\
      \midrule
        SchNet\cite{schutt2018schnet,OC20}  &  1024 & 5 & 9.1M & 194d & 0.8h &  -- & 0.04903	& 0.3413 & 0   \\
        DimeNet++\cite{klicpera_dimenetpp_2020,OC20} & 192 & 3 & 1.8M &  587d & 8.5h & 0.5343  & 0.04758 & 0.3560 & 0.05  \\
        DimeNet++ energy-only\cite{klicpera_dimenetpp_2020, OC20} & 192 & 3 & 1.8M &  587d & 8.5h & 0.4802 & 0.3459 & 0.1021 & 0.0  \\
        DimeNet++ force-only\cite{klicpera_dimenetpp_2020, OC20} & 192 & 3 & 1.8M &  587d & 8.5h & -- & 0.03573 & 0.4785 &	-- \\
        DimeNet++-large\cite{klicpera_dimenetpp_2020, OC20}  & 512 & 3 & 10.7M & 1600d & 27.0h  & --  & 0.03275 & \textbf{0.5408} & -- \\
        ForceNet\cite{hu2021forcenet} & 512 & 5 & 11.3M & 31d & 1.3h &  -- & 0.03432 & 0.4770 & --  \\
        ForceNet-large\cite{hu2021forcenet} & 768 & 7 & 34.8M & 194d & 3.5h &  - & 0.03113 & 0.5195 & - \\
        \midrule
        {\bf \model{} (energy-centric)}  & 256 & 3 & 6.1M & 275d & 22.7h & 0.4114 & 0.03888 & 0.4299  & 0.16  \\
        {\bf \model{} (energy-centric) force-only }  & 256 & 3 & 6.1M & 380d & 22.7h & -- & 0.03258 & 0.4976 & -- \\
        {\bf \model{} (force-centric)}  & 256 & 3 & 8.5M & 275d & 9.1h & \textbf{0.3363} & \textbf{0.02966} & 0.5391 & \textbf{0.45}  \\
      \bottomrule
    \end{tabular}
    }
    \caption{Comparison of \model{} to existing GNN models on the S2EF task. Average results across all four test splits are reported. We mark as bold the best performance and close ones, \ie, within 0.0005 MAE, which according to our preliminary experiments, is a good threshold to meaningfully distinguish model performance. Training time is in GPU days, and inference time is in GPU hours. Median represents the trivial baseline of always predicting the median training force across all the validation atoms.}
    \vspace{-0.35cm}
    \label{tab:comp-s2ef}
\end{table*}


\begin{table*}[t]
    \centering
        
    
    \renewcommand{\arraystretch}{1.0}
    \setlength{\tabcolsep}{5pt}
    \resizebox{0.96\linewidth}{!}{
    \begin{tabular}{lcccccccc}
      \toprule
         \mr{2}{\textbf{Model}} & \mc{4}{c}{\textbf{Energy MAE (eV)} $\downarrow$} &  \mc{4}{c}{\textbf{Force MAE (eV/\AA)} $\downarrow$} \\
        & ID & OOD Ads. & OOD Cat. & OOD Both & ID & OOD Ads. & OOD Cat. & OOD Both \\
      \midrule
        Median & 2.043 & 2.420 & 1.992 & 2.577 & 0.0809 & 0.0801 & 0.0787 & 0.0978 \\
      \midrule
      & \mc{8}{c}{Energy Loss Only} \\
        SchNet  & 0.395 & 0.446 & 0.551 & 0.703 & - & - & - & - \\
        DimeNet++ & 0.359 & 0.402 & 0.506 & 0.654 & - & - & - & -  \\
      \midrule
      & \mc{8}{c}{Force Loss Only} \\
        SchNet  & - & - & - & - & 0.0443 & 0.0469 & 0.0459 & 0.0590   \\
        DimeNet++ & - & - & - & - & 0.0331 & 0.0341 & 0.0340 & 0.0417 \\
        DimeNet++-large  & - & - & - & - & 0.0281 & 0.0289 & 0.0312 & 0.0371  \\
        ForceNet & - & - & - & - & 0.0313 & 0.0320 & 0.0331 & 0.0409   \\
        ForceNet-large & - & - & - & - & 0.0278 & 0.0283 & 0.0309 & 0.0375  \\
        {\bf \model{} (energy-centric)}  & - &	- &	- &	- &	0.0309&	0.0321&	0.0315&	0.0393   \\
      \midrule
      & \mc{8}{c}{Energy and Force Loss} \\
        SchNet  & 0.443 & 0.491 & 0.529 & 0.716 & 0.0493 & 0.0527 & 0.0508 & 0.0652   \\
        DimeNet++ &  0.486 & 0.470 & 0.533 & 0.648 & 0.0443 & 0.0458 & 0.0444 & 0.0558 \\
        {\bf \model{} (energy-centric)}  & 0.351 &	0.367 &	0.411 & 0.517 & 0.0358 & 0.0374 & 0.0364 & 0.0460    \\
        {\bf \model{} (force-centric)}  & \textbf{0.261} & \textbf{0.275} & \textbf{0.350} & \textbf{0.459} & \textbf{0.0269} & \textbf{0.0277} & \textbf{0.0285} & \textbf{0.0356}  \\
        \bottomrule
    \end{tabular}
    }
    \caption{Comparison of \model{} to existing GNN models on different test splits.
    We mark as bold the best performance and close ones, \ie, within 0.0005 MAE, which according to our preliminary experiments, is a good threshold to meaningfully distinguish model performance. Training time is in GPU days, and inference time is in GPU hours.
    Median represents the trivial baseline of always predicting the median training force across all the validation atoms. }
    \vspace{-0.35cm}
    \label{tab:comp-splits}
\end{table*}

\paragraph{Implementation details.}

For all models, the edge messages have size $M=32$ with $K=3$ layers, the hidden dimension $D=256$ and embedding dimension $B=8$. Unless otherwise stated, the convolutional filters are of size 16x12 and 12x8 for the force-centric and energy-centric models respectively. A smaller filter size was used for the energy-centric model due to memory constraints. GroupNorm \cite{wu2018group} is applied after the spin convolution with group size 4. An L1 loss is used for all experiments. The force loss was weighed by 100 with respect to the energy loss, except for the force-only model where the energy loss is set to 0. All models were trained with Adam (amsgrad) to convergence with the learning rate multiplied by 0.8 when the validation error plateaus. Training was performed using batch sizes ranging from 64 to 96 samples across 32 Volta 32GB GPUs. The Swish \cite{ramachandran2017searching} function is used for all non-linear activation functions. The neighbors $s\in N_t$ of each atom $t$ are found using a distance threshold of $\delta = 6$\AA. If more than 30 atoms are within the distance threshold, only the closest 30 are used.  The distance block uses 256 to 512 Gaussian basis functions with $\sigma$'s equal to three times the distance between Gaussian means. 

\subsection{OC20}

The OC20 dataset \cite{OC20} contains over 130 million structures used to train models for predicting forces and energies during structure relaxations that is released under a CC Attribution 4.0 License. Since the goal of a structure relaxation is to find a local energy minimum, energy conservation in optional for this task. We report results for the Structure to Energy and Forces (S2EF), the Initial Structure to Relaxed Energy (IS2RE) and the Initial Structure to Relaxed Structure (IS2RS) tasks. 

\subsubsection{Structure to Energy and Forces (S2EF)}

There are four metrics for the S2EF task, the energy and force Mean Absolute Error (MAE), the Force Cosine similarity, and the Energy and Forces within a Threshold (EFwT). The EFwT metric is meant to indicate the percentage of energy and force predictions that would be useful in practice. Results for three model variants are shown in Table \ref{tab:comp-s2ef} on the test set. The \model~force-centric approach has the lowest energy MAE and force MAE of all models.  While still low in absolute terms, the \model~models are improving over other models on the EFwT metric. DimeNet++-large slightly out performs \model~on the force cosine metric. The training time for the \model~is significantly faster than DimeNet++, while being a little slower than ForceNet \cite{hu2021forcenet} or SchNet \cite{schutt2018schnet}.


\begin{table*}[t]
    \centering
    \renewcommand{\arraystretch}{1.0}
    \setlength{\tabcolsep}{5pt}
    \resizebox{0.97\linewidth}{!}{
    \begin{tabular}{lrrrrcccc}
      \toprule
         \mr{2}{\textbf{Model}} & \textbf{Hidden} & \textbf{\#Msg} & \mr{2}{\textbf{\#Params}} & \textbf{Train}  &  \mc{4}{c}{\textbf{\ocd{} Val ID 30k}} \\
        & \textbf{dim} & \textbf{layers} & & \textbf{time} & Energy MAE [eV] $\downarrow$ & Force MAE [eV/\AA] $\downarrow$ & Force Cos $\uparrow$ & EFwT [\%] $\uparrow$  \\
      \midrule
        Median & & & & & &   &  & \\
      \midrule 
        \mc{1}{c}{\textbf{Energy-Centric}} & & & & & & & & \\
        {\model{} (grid 12x8)}  & 128 & 2 & 1.3M & 54d &  -- & 0.0417 & 0.401 & -- \\
        {\model{} (spherical harmonics, $\ell = 5$)}  & 256 & 3 & 6.4M & 119d & --  & 0.0405 & 0.411 & -- \\
        {\model{} (grid 12x8)}  & 256 & 3 & 6.1M & 87d &  -- & 0.0406 & 0.426 & -- \\
    \midrule
    \mc{1}{c}{\textbf{Force-Centric}} & & & & & & & & \\
        
        {\model{} (grid 12x8)}  & 128 & 2 & 1.8M & 54d & 0.376 & 0.0370 & 0.436 &  0.15\%   \\
        {\model{} (grid no conv 16x12)}  & 256 & 3 & 8.5M & 56d & 0.341 & 0.0348 &  0.462  & \textbf{0.20}\%    \\
        {\model{} (spherical harmonics, $\ell = 5$)} & 256 & 3 & 8.1M & 113d & 0.321 & \textbf{0.0328} & \textbf{0.484} & \textbf{0.22}\%    \\
        {\model{} (grid 16x12)}  & 256 & 3 & 8.5M & 76d & \textbf{0.317} & \textbf{0.0326} & \textbf{0.484} & \textbf{0.20}\%    \\
      \bottomrule
    \end{tabular}}
    \caption{Ablation studies for \model{}~model variations trained for 560k steps (32-48 batch size, 0.2 epochs) with 16 Volta 32 GB GPUs. Training time is in GPU days and the validation set is a 30k random sample of the OC20 ID Validation set.}
    \label{tab:comp-ablation}
    \vspace{-0.35cm}
\end{table*}
 
\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/oc20_ablation_plots_v3.png}
    \caption{Performance of SpinConv ablations on OC20 Val ID $30k$ (Table~\ref{tab:comp-ablation}).
    All models trained for $560k$ steps and plotted against wall-clock training time.
    Note force-centric models and grid-based approaches converge more quickly than energy-centric models
    and those using spherical harmonics.}
    \label{fig:ablationplots}
\end{figure*}






In Table \ref{tab:comp-splits} we examine the performance of \model~across different test splits. Note that the energy prediction of \model~is signficantly better than SchNet or DimeNet++. Across all models the accuracy for the in domain split are highest and decline for the three Out of Domain (OOD Adsorbate, OOD Catalyst, OOD Both) splits. \model~outperforms all models on each of the different domain splits. When comparing energy-centric approaches trained with both force and energy losses (bottom rows), the \model~model does significantly better at predicting both. In fact, the energy-centric approach trained on forces and energy outperforms the DimeNet++ \cite{klicpera_dimenetpp_2020} model when trained on only energy, or energy and forces.

We examine variations of the \model~model in Table \ref{tab:comp-ablation} and Figure \ref{fig:ablationplots} through ablation studies. We trained three variants of the energy-centric model and four variants of the force-centric model. The grid-based and spherical harmonic approaches produced similar accuracies. However, the grid-based approach was significantly faster to train, so it was used in the remaining experiments. Smaller models lead to reduced performance on the OC20 dataset, but we found for smaller datasets such as MD17 or QM9 smaller model sizes can be beneficial to avoid overfitting. Finally, we test the impact of not performing the convolution (no conv) and only applying the filter at a single rotation. Rotation invariance was maintained by orienting the filter based on the mean angle of the neighboring atoms weighted by distance. The result of not performing the convolution is significantly reduced accuracy. However, its faster training time may make it suitable for some applications. 

Finally, for the force-centric \model~model we explore results when varying the number of random rotations used in the force block. The force MAE when using a single random rotation is 0.0276 and improves slightly to 0.0270 when using 5 random rotations. Increasing the number of rotations beyond 5 leads to negligible gains. The standard deviation of the force estimates at different random rotations is 0.004 eV/\AA. This is equal to $15\%$ of the force MAE, which indicates the amount of error due to the model not being strictly rotation equivariant is small relative to the overall error of the model.

\setlength{\tabcolsep}{3pt}
\begin{table*}[t]
    \begin{center}
        \resizebox{\textwidth}{!}{
            \begin{tabular}{l l cccc | cccc  }
            \midrule
            & & \multicolumn{4}{c|}{Energy MAE [eV] $\downarrow$} & \multicolumn{4}{c}{EwT $\uparrow$}  \\
            \cmidrule(l{4pt}r{4pt}){3-6}
            \cmidrule(l{4pt}r{4pt}){7-10}
        Model & Approach & ID &  OOD Ads & OOD Cat & OOD Both & ID &  OOD Ads & OOD Cat & OOD Both \\
            \midrule
                Median baseline & -
                    & $1.7499$ & $1.8793$ & $1.7090$ & $1.6636$
                    & $0.71\%$ & $0.72\%$ & $0.89\%$ & $0.74\%$ \\
                \midrule
                CGCNN~\cite{xie2018crystal} & Direct
                    & $0.6149$ & $0.9155$ & $0.6219$ & $0.8511$
                    & $3.40\%$ & $1.93\%$ & $3.10\%$ & $2.00\%$  \\
                SchNet~\cite{schutt2017schnet} & Direct
                    & $0.6387$ & $0.7342$ & $0.6616$ & $0.7037$
                    & $2.96\%$ & $2.33\%$ & $2.94\%$ & $2.21\%$  \\
DimeNet${+}{+}$~\cite{klicpera2020directional} & Direct
                    & $0.5620$ & $0.7252$ & $0.5756$ & $0.6613$
                    & $4.25\%$ & $2.07\%$ & $4.10\%$ & $2.41\%$ \\
                SpinConv & Direct
                    & 0.5583   &  0.7230   & 0.5687 & 0.6738
                    & 4.08\%   &  2.26\%   & 3.82\% & 2.33\%\\
                \midrule
DimeNet${+}{+}$ & Relaxation
                    & $0.6908$ & $0.6842$ & $0.7027$ & $0.6834$
                    & $4.25\%$ & $3.36\%$ & $3.76\%$ & $3.52\%$ \\
                DimeNet${+}{+}$ -- force-only + energy-only & Relaxation
                    & $0.5124$ & $0.5744$ & $0.5935$ & $0.6126$
                    & $6.12\%$ & $4.29\%$ & $5.07\%$ & $3.85\%$ \\
                DimeNet${+}{+}$ -- large force-only + energy-only & Relaxation
                    & $0.5034$ & $0.5430$ & $0.5789$ & $0.6113$
                    & $6.57\%$ & $4.34\%$ & $5.09\%$ & $3.93\%$ \\
                SpinConv (force-centric) & Relaxation
                    & $\textbf{0.4235}$ & $\textbf{0.4415}$ & $\textbf{0.4572}$ & $\textbf{0.4245}$
                    & $\textbf{9.37\%}$ & $\textbf{6.75\%}$ & $\textbf{8.49\%}$ & $\textbf{6.76\%}$\\
            \bottomrule
            \end{tabular}}
    \end{center}
    \caption{Initial Structure to Relaxed Energy (IS2RE) results on the OC20 test split as evaluated by the Energy MAE (eV) and Energy within Threshold (EwT) \cite{OC20} (see OC20 discussion board). Comparisons made for the direct and relaxation approaches using various models.}
    \label{tab:is2re}
\end{table*}

\begin{table*}[]
  \centering
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{lrrrrrr|rrrrr}
      \toprule
        \mr{2}{\textbf{Model}}  & \textbf{Inference} & \mc{5}{c}{\textbf{AFbT (\%)} $\uparrow$} & \mc{5}{c}{\textbf{ADwT (\%)} $\uparrow$} \\
        & \textbf{time} $\downarrow$ &  ID & OOD Ads. & OOD Cat. & OOD Both & {\bf Average} &  ID & OOD Ads. & OOD Cat. & OOD Both & {\bf Average}  \\
      \midrule
        SchNet~\cite{schutt2017schnet} & 54.1h & 5.28 & 2.82 & 2.62 & 2.73 & 3.36 & 32.49 & 28.59 & 30.99 & 35.08 & 31.79 \\
        DimeNet++~\cite{klicpera_dimenetpp_2020} & 407.6h & 17.52 & 14.67 & 14.32 & 14.43 & 15.23 & 48.76 &  45.19 & 48.59 & 53.14 & 48.92 \\
        DimeNet++-large~\cite{klicpera_dimenetpp_2020} & 814.6h & \textbf{25.65} & \textbf{20.73} & \textbf{20.24} & \textbf{20.67} & \textbf{21.82} & 52.45 & 48.47 & 50.99 & 54.82 & 51.68 \\
        ForceNet~\cite{hu2021forcenet} & 75.1h & 10.75 & 7.74 & 7.54 & 7.78 & 8.45 & 46.83 & 41.26 & 46.45 & 49.60 & 46.04   \\
        ForceNet-large~\cite{hu2021forcenet} & 186.9h & 14.77 & 12.23 & 12.16 & 11.46 & 12.66 & 50.59 & 45.16 & 49.80 & 52.94 & 49.62   \\
        \midrule
\bf \model{} (force-centric) & 263.2h & 21.10 & 15.70 & 15.86 & 14.01 & 16.67
            & \textbf{53.68} & \textbf{48.87} & \textbf{53.92} & \textbf{58.03} & \textbf{53.62} \\  
      \bottomrule
    \end{tabular}}
    \caption{Relaxed structure from initial structure (IS2RS) results on
      the OC20 test split, as evaluated by Average Distance within Threshold (ADwT)
      and Average Forces below Threshold (AFbT). All values in percentages, higher is better.
      Results computed via the OCP evaluation server. Inference times are total across the $4$ splits.}
    \label{tab:results-is2rs}
\end{table*}

\subsubsection{Initial Structure to Relaxed Energy (IS2RE)}
The Initial Structure to Relaxed Energy (IS2RE) task takes an initial atomic structure and attempts to predict the energy of the structure after it has been relaxed. Two approaches may be taken to address this problem, the direct and relaxation approaches \cite{OC20}. The direct treats the task as a standard regression problem and directly estimates the relaxed energy from the initial structure. The relaxation approach computes the relaxed structure using the ML predicted forces to update the atom positions. Next, given the ML relaxed structure the energy is estimated. We show results for both approaches in the OC20 dataset using \model~in Table \ref{tab:is2re}. 

The results of the \model~model significantly outperform all previous approaches using the relaxation approach for both energy MAE and Energy within Threshold (EwT) metrics. DimeNet++ also shows improved results for the relaxation approach with the best approach using two models; DimeNet++-large for force estimation and DimeNet++ (energy-only) for the energy estimation. Note in contrast to other approaches, \model~shows good results across all test splits, including those with out of domain adsorbates and catalysts. Using the direct approach, \model~is comparable to DimeNet++'s direct approach. 






\subsubsection{Initial Structure to Relaxed Structure (IS2RS)}

Our final results on the OC20 dataset are on the IS2RS task where predicted forces are used to relax an atom structure to a local energy minimum. The is performed by iteratively estimating the forces that are in turn used to update the atoms positions. This process is repeated until convergence or 200 iterations. Results are shown in Table~\ref{tab:results-is2rs}. The suggested metrics are  Average Distance within Threshold (ADwT) metric, which measures whether the atom positions are close to those found using DFT
and Average Forces below Threshold (AFbT), which measures whether a true energy minimum was found (i.e., forces are close to zero). On the ADwT metric, \model~outperforms other approaches ($53.62\%$ averaged across splits).
On the AFbT metric, DimeNet++-large outperforms \model~($21.82\%$~\vs $16.67\%$),
but is more than ${\sim}3$x slower ($814.6$h~\vs $263.2$h) during inference. \model~outperforms all other models.

\begin{table*}[t]
    \centering
    \resizebox{0.7\columnwidth}{!}{   
    
    \renewcommand{\arraystretch}{1.0}
    \setlength{\tabcolsep}{5pt}
    \resizebox{0.97\linewidth}{!}{
    \begin{tabular}{lccc|ccc}
      \toprule
        Molecule & GDML &  PhysNet &  PhysNet-ens5 & SchNet & DimeNet* & \model \\
        \midrule
        Aspirin         & 0.02 & 0.06 & 0.04 & 0.33 & 0.09 & \textbf{0.07} \\ 
        Benzene         & 0.24 & 0.15 & 0.14 & 0.17 & \textbf{0.15} & 0.17 \\
        Ethanol         & 0.09 & 0.03 & 0.02 & 0.05 & 0.03 & \textbf{0.02} \\
        Malonaldehyde   & 0.09 & 0.04 & 0.03 & 0.08 & \textbf{0.04} & \textbf{0.04} \\
        Naphthalene     & 0.03 & 0.04 & 0.03 & 0.11 & 0.06 & \textbf{0.04} \\
        Salicylic       & 0.03 & 0.04 & 0.03 & 0.19 & 0.09 & \textbf{0.05} \\
        Toluene         & 0.05 & 0.03 & 0.03 & 0.09 & 0.05 & \textbf{0.03} \\
        Uracil          & 0.03 & 0.03 & 0.03 & 0.11 & 0.04 & \textbf{0.03} \\
        \midrule
        Mean & 0.073 & 0.053 & 0.044 & 0.141 & 0.069 & \textbf{0.058} \\
      \bottomrule
    \end{tabular}
    }}
    \caption{Forces MAE (kcal/mol\AA) on MD17 for models trained using 50k samples. Best results for models not using domain specific information are in bold. {\footnotesize *The DimeNet results were trained in-house as the original authors did not use the 50k dataset. DimeNet was found to outperform DimeNet++ on this task.} 
    }
    \vspace{-0.35cm}
    \label{tab:comp-md17}
\end{table*}

\begin{table*}[]
    \centering
    \renewcommand{\arraystretch}{1.0}
    \resizebox{0.80\linewidth}{!}{
    \begin{tabular}{lcccccccccccc}
        \toprule
        Task & $\alpha$ & $\Delta \epsilon$ & $\epsilon_{\text{HOMO}}$ & $\epsilon_{\text{LUMO}}$ & $\mu$ & $C_{\nu}$ & G &
        H & R$^{2}$ & U & U$_{0}$ & ZPVE \\
        Units & bohr$^{3}$ & meV & meV & meV & D & cal/mol K & meV & 
        meV & bohr$^{3}$ & meV & meV & meV \\
        \midrule
        NMP \cite{gilmer2017neural} &       .092 & 69 & 43 & 38 & .030 & .040 & 19 & 17 & .180 & 20 & 20 & 1.50 \\
        Schnet \cite{schutt2017schnet} &    .235 & 63 & 41 & 34 & .033 & .033 & 14 & 14 & \textbf{.073} & 19 & 14 & 1.70 \\
        Cormorant \cite{anderson2019cormorant} & .085 & 61 & 34 & 38 & .038 & .026 & 20 & 21 & .961 & 21 & 22 & 2.03 \\
        L1Net \cite{miller2020relevance} &     .088 & 68 & 46 & 35 & .043 & .031 & 14 & 14 & .354 & 14 & 13 & 1.56 \\
        LieConv \cite{finzi2020generalizing} &   .084 & 49 & 30 & 25 & .032 & .038 & 22 & 24 & .800 & 19 & 19 & 2.28 \\
        TFN \cite{thomas2018tensor} &       .223 & 58 & 40 & 38 & .064 & .101 & - & - & - & - & - & - \\
        SE(3)-Tr. \cite{fuchs2020se} & .142 & 53 & 35 & 33 & .051 & .054 & - & - & - & - & - & - \\
        EGNN \cite{satorras2021n} &      .071 & 48 & 29 & 25 & .029 & .031 & 12 & 12 & .106 & 12 & 11 & 1.55\\
        DimeNet++ \cite{klicpera_dimenetpp_2020} & \textbf{.044} & 33 & 25 & 20 & .030 & .023 & \textbf{8} & 7 & .331 & \textbf{6} & \textbf{6} & 1.21 \\
        SphereNet \cite{liu2021spherical} & .047 & \textbf{32} & \textbf{24} & \textbf{19} & \textbf{.027} & \textbf{.022} & \textbf{8} & \textbf{6} & .292 & 7 & \textbf{6} & \textbf{1.12} \\
        \midrule
        \textbf{\model{}} & .058 & 47 & 26 & 22 & \textbf{.027} & .028 & 12 & 12 & .156 & 12 & 12 & 1.50\\
\bottomrule
    \end{tabular}
    }
    
    \caption{Mean absolute error results for QM9 dataset \cite{ramakrishnan2014quantum} on 12 properties for small molecules.}
    \label{tab:comp-qm9}
\end{table*}

\subsection{MD17}

The MD17 dataset \cite{chmiela2017machine,chmiela2018towards} contains molecular dynamic simulations for eight small molecules. Two training datasets are commonly used, one containing 1k examples and another containing 50k examples. We found the 1k training dataset to be too small for the \model~model, and may be more appropriate for approaches that incorporate prior chemistry knowledge, such as hand-coded features or force fields \cite{chmiela2017machine,unke2019physnet}. The 50k dataset provides significantly more training data, but the remaining validation and test data are highly similar to those found in training, and may not guarantee independent samples in the test set\cite{christensen2020role}. Nevertheless, we report results on MD17 for comparison to prior work on the molecular dynamics task. Research in this domain would greatly benefit from the generation of a larger dataset. 

Results are shown in Table \ref{tab:comp-md17}. \model~is on par or better for 7 of the 8 molecules when compared to DimeNet \cite{klicpera2020directional}. Both \model~and DimeNet perform well with respect to the GDML \cite{chmiela2017machine} and PhysNet \cite{unke2019physnet} models that take advantage of domain-specific information. Given the smaller dataset size, the \model~model uses a reduced 8x8 grid-based spherical representation. Other model parameters are the same as previously described.






\subsection{QM9}

Our final set of results are on the popular QM9 dataset \cite{ramakrishnan2014quantum} that tests the prediction of numerous properties for small molecules. While the \model~model was designed to estimate energies and per-atom forces, we may use the same model to predict other proprieties. Results are shown in Table \ref{tab:comp-qm9} on a random test split for an energy-centric 8x8 grid-based \model~model. The results of DimeNet++ and the recent SphereNet\cite{liu2021spherical} outperform those of others. However, DimeNet++, SphereNet and \model~perform well with respect to other approaches across many properties. 






