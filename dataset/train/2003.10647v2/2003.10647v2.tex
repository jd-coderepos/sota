

\documentclass[a4paper,11pt]{article}
\usepackage{graphicx}
\usepackage{hyperref}


\usepackage{tikz}
\usepackage{comment}
\usepackage{amsmath,amssymb} \usepackage{color}







\usepackage[margin=1in]{geometry}

\usepackage{bbm}
\usepackage{booktabs}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{multirow}


\usepackage{amsmath,amsfonts,bm}
\usepackage{amssymb}
\makeatletter
\def\blfootnote{\xdef\@thefnmark{}\@footnotetext}
\makeatother


\newcommand{\newterm}[1]{{\bf #1}}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\bb}[1]{\mathbb{#1}}

\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\Span}{\ensuremath{\mathrm{span}}}
\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 



\title{\textbf{Robust and On-the-fly Dataset Denoising \\ for Image Classification}} 













\author{Jiaming Song 
\\ \texttt{tsong@cs.stanford.edu}
\\ Stanford University\and
Lunjia Hu
\\ \texttt{lunjia@stanford.edu}
\\ Stanford University
\and 
Michael Auli \\ \texttt{michaelauli@fb.com}
\\ Facebook AI Research
\and
Yann Dauphin
\\ \texttt{yann@dauphin.io}
\\ Google Brain\and 
Tengyu Ma 
\\ \texttt{tengyuma@stanford.edu}
\\ Stanford University
}
\begin{document}
\maketitle


\begin{abstract}
Memorization in over-parameterized neural networks could severely hurt generalization in the presence of mislabeled examples. However, mislabeled examples are hard to avoid in extremely large datasets collected with weak supervision.
We address this problem by reasoning counterfactually about the loss distribution of examples with uniform random labels had they were trained with the real examples, and use this information to remove noisy examples from the training set.
First, we observe that examples with uniform random labels have higher losses when trained with stochastic gradient descent under large learning rates. 
Then, we propose to model the loss distribution of the counterfactual examples using only the network parameters, which is able to model such examples with remarkable success. Finally, we propose to remove examples whose loss exceeds a certain quantile of the modeled loss distribution.
This leads to {On-the-fly Data Denoising} (\textsc{ODD}), a simple yet effective algorithm that is robust to mislabeled examples, while introducing almost zero computational overhead compared to standard training.
\textsc{ODD} is able to achieve state-of-the-art results on a wide range of datasets including real-world ones such as WebVision and Clothing1M.
\end{abstract} 
\setlength{\tabcolsep}{1.4pt}

\section{Introduction}


\newcommand{\tnote}[1]{{\color{blue}Tengyu notes: #1}}

Over-parametrized deep neural networks have remarkable generalization properties while achieving near-zero training error~\cite{zhang2016understanding}. However, the ability to fit the entire training set is highly undesirable, as a small portion of mislabeled examples in the dataset could severely hurt generalization~\cite{zhang2016understanding,arpit2017a}.\blfootnote{Work done at Facebook AI research.}

Meanwhile, an exponential growth in training data size is required to linearly improve generalization in vision~\cite{sun2017revisiting}; this progress could be hindered if there are mislabeled examples within the dataset.

Mislabeled examples are to be expected in large datasets that contain millions of examples. Web-based supervision produces noisy labels~\cite{li2017webvision,mahajan2018exploring} whereas human labeled datasets sacrifice accuracy for scalability~\cite{krishna2016embracing}. 
Therefore, algorithms that are robust to various levels of mislabeled examples are warranted in order to further improve generalization for very large labeled datasets.

In this paper, we are motivated by the observation that crowd-sourcing or web-supervision could have multiple disagreeing sources; in such cases, noisy labels could exhibit higher conditional entropy than the ground truth labels. 
Since the information about the noisy labels (such as the amount of noise) is often scarce, we pursue \textit{a general approach} by counterfactual reasoning of the behavior of noisy examples with high conditional entropy.
Specifically, we reason about the \textit{counterfactual} case of how examples with uniform random noise would behave \textit{had they appeared in the training dataset}, without actually training on such labels. If a real example has higher loss than what most counterfactual examples with uniform random noise would have, then there is reason to believe that this example is likely to contain a noisy label; removing this example would then improve performance on a clean test set. 


To reason about the \textit{counterfactual loss distribution} of examples with uniform random noise, we first show that training residual networks with \textit{large learning rates} will create a significant gap between the losses of clean examples and noisy examples. The distribution of training loss over clean examples decrease yet that of the uniformly noisy examples does not change, regardless of the proportion of noisy examples in the dataset.
Based on this observation, we propose a distribution that simulates the loss distribution of uniform noisy examples based only on the network parameters. Reasonable thresholds can be derived from percentiles of this distribution, which we can then utilize to denoise the dataset. This is critical in real-world applications, because prior knowledge about the distribution of label noise is often scarce; even if we have such information (such as transition matrices of label noise), algorithms that specifically utilize this information are not scalable when there are thousands of labels.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figs/pipeline.pdf}
    \caption{Pipeline of our method. We utilize the implicit regularization effect of SGD to (counterfactually) reason the loss distribution of examples with uniform label noise. We remove examples that have loss higher than the threshold and train on the remaining examples. There is no assumption that the dataset has to contain uniformly random labels (thus such labels are ``counterfactual''); we empirically validate our method on real-world noisy datasets.}
    \label{fig:pipeline}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figs/cifar-demo.pdf}
    \caption{Mislabeled examples in the CIFAR-100 training set detected by \textsc{ODD}.}
    \label{fig:cifar-demo}
\end{figure*}

We proceed to propose \textit{On-the-fly Data Denoising} (\textsc{ODD}, see Figure~\ref{fig:pipeline}), a simple and robust method for training with noisy examples based on the implicit regularization effect of stochastic gradient descent.
First, we train residual networks with large learning rate schedules and use the resulting losses to separate clean examples from mislabeled ones. 
This is done by identifying examples whose losses exceed a certain threshold. 
Finally, we remove these examples from the dataset and continue training until convergence. \textsc{ODD} is a general approach that can be used to train clean dataset as well as noisy datasets with almost no modifications.




Empirically, \textsc{ODD} performs favorably against previous methods in datasets containing \textit{real-world noisy examples}, such as WebVision~\cite{li2017webvision} and Clothing1M~\cite{xiao2015learning}. \textsc{ODD} also achieves equal or better accuracy than the state-of-the-art \textit{on clean datasets}, such as CIFAR and ImageNet. 
We further conduct ablation studies to demonstrate that \textsc{ODD} is robust to different hyperparameters and artificial noise levels. Qualitatively, we demonstrate the effectiveness of \textsc{ODD} by detecting mislabeled examples in the ``clean'' CIFAR-100 dataset without any supervision other than the training labels (Figure~\ref{fig:cifar-demo}). These results suggest that we can use \textsc{ODD} in both clean and noisy datasets with minimum computational overhead to the training algorithm. 



 \section{Problem setup}

The goal of supervised learning is to find a function  that describes the probability of a random label vector  given a random input vector , which has underlying joint distribution . Given a loss function , one could minimize the average of  over :

which is the basis of empirical risk minimization (\textsc{ERM})
The joint distribution  is usually unknown, but we could gain access to its samples via a potentially noisy labeling process, such as crowdsourcing~\cite{krishna2016embracing} or web queries~\cite{li2017webvision}. 

We denote the training dataset with  examples as .  represents correctly labeled (clean) examples sampled from .  represents mislabeled examples that are not sampled from , but from another distribution ; , as a sample cannot be both correctly labeled and mislabeled. 


We aim to learn the function  from  \emph{without knowledge about ,  or their statistics} (e.g. ). 
A typical approach is to pretend that  --- i.e., all examples are i.i.d. from  --- and minimize the empirical risk:

If  is indeed true, then the empirical risk converges to the population risk:  as . However, if , then  is no longer an unbiased estimator of . 
Moreover, when  contains large neural nets with the number of parameters exceeding , the empirical risk minimizer could fit the entire training dataset, including the mislabeled examples~\cite{zhang2016understanding}. Overfitting to wrong labels empirically causes poor generalization.  For example, training CIFAR-10 with 20\% of uniformly mislabeled examples and a residual network gives a test error of 11.5\%, which is significantly higher than the 4.25\% error obtained with training on the clean examples\footnote{See Table.~\ref{tab:cifar-val}, Section~\ref{sec:exp-cifar} for the exact experiment setup.}.



\subsection{Entropy-based Assumption over Noisy Labels}
Therefore, if we were able to identify the clean examples belonging to , we could vastly improve the generalization on ; this requires us to provide valid prior assumptions that could distinguish clean examples from mislabeled ones. We note that these assumptions have to be general enough so as to \textit{not depend on additional assumptions specific to each dataset}. For example, knowledge about noise transition matrices is not allowed.

We assume that for any example , the entropy of the clean label distribution is smaller than that of the noisy label distribution:

where the randomness of labeling  could arise from noisy labelings, such as Mechanical Turk~\cite{krishna2016embracing}.
Let  be the cross entropy loss, then the ERM objective is essentially trying to minimize the KL divergence between the empirical conditional distribution (denoted as ) and the conditional distribution parametrized by our model (denoted as ):

which is minimized as ; in this case, the cross entropy loss is higher if  has higher entropy, which suggests that the mislabeled examples are likely to have higher loss than correct ones.

\section{Denoising datasets on-the-fly with Counterfactual Thresholds}
In the following section, we study the behavior of samples with uniformly random label noise; this allows us to reason about their loss distribution \textit{counterfactually}, and develop suitable thresholds to remove noisy examples that appear in the training set. 

The conditional distribution  of uniformly random label noise is simply:

We note that  is the distribution that maximizes entropy; therefore, any real-world noise distribution  will have smaller entropy than . 

While it is unreasonable to assume that the label noise is uniformly random in practice, we do not make such assumption over our training set. Instead, we reason about the following counterfactual case: 
\begin{quote}
    \textit{Had the training set contained some examples with uniform random labels, can we characterize the loss distribution of these examples?}
\end{quote}

Then, we illustrate how such a counterfactual analysis allows simple and practical algorithms that work even under real-world noisy datasets.
\begin{itemize}
    \item First, we show that when training ResNets via SGD with large learning rates, the training loss of uniform noisy labels and clean labels can be clearly separated.
    \item Next, we propose an approach to model the (counterfactual) loss distribution 
\textit{by only looking at the weights of the network}. We empirically show that this does not depend on the type or the amount of noisy labels in the dataset, making this approach generalize well to various counterfactual scenarios (such as different portions of uniform random labels in the dataset).
    \item Finally, we can simply remove all examples that perform worse than a certain percentile of the counterfactual distribution. Since higher entropy examples tend to have higher loss than lower entropy ones, the samples we remove are more likely to be more noisy. In Fig.~\ref{fig:cifar-demo}, we empirically demonstrate that the proposed threshold identifies mislabeled samples in CIFAR-100 even without any additional supervision, validating our assumption.
\end{itemize}

\begin{figure*}[t]
\begin{center}
\includegraphics[width=0.9\textwidth]{figs/loss-dist.pdf}
\caption{Histogram of the distributions of losses, where ``normal'', ``noise'', and ``simulated'' denote (real) examples with clean labels, (real) examples with uniform random labels and the counterfactual model  respectively.  matches the loss distribution of noisy examples, which have higher loss than clean ones;  depends only on the network parameters.}
\label{fig:histogram}
\end{center}
\end{figure*}





\subsection{Separating mislabeled examples via SGD}



First, we find that training the model with stochastic gradient descent (SGD) with large learning rates (e.g. ) will result in significant discrepancy between the loss statistics of the clean examples and mislabeled examples. We consider training deep residual networks on CIFAR-100 and ImageNet with different percentages of uniform label noise ( and ), but with large learning rates (close to ), and at specific epochs, we plot the histogram of the loss for each example. 

As demonstrated in Fig.~\ref{fig:histogram}, the loss distributions of clean examples and mislabeled ones have notable statistical distance. Moreover, it seems that the loss distribution of the uniform labeled examples are relatively stable, and \textit{does not depend on the amount of uniform random noise in the training set}.
This is consistent with the obeservations in~\cite{zhang2016understanding}, as the network starts to fit mislabeled examples when learning rate decrease further; decreasing learning rate is crucial for achieving better generalization on clean datasets.

The working of the implicit regularization of stochastic gradient descent is by and large an open question that attracts much recent attentions~\cite{neyshabur2017implicit,li2017algorithmic,du2018algorithmic,mandt2017stochastic}. Empirically, it has been observed that large learning rates are beneficial for generalization~\cite{kleinberg2018an}. Chaudhari and Soatto~\cite{chaudhari2018stochastic} have argued that SGD iterates converge to limit cycles with entropic regularization proportional to the learning rate and inversely proportional to batch size. Training with large learning rates under fixed batch sizes could then encourage solutions that are more robust to large random perturbations in the parameter space and less likely to overfit to mislabeled examples.

Given these empirical and theoretical evidences on large learning rate helps generalization, we propose to classify correct and mislabeled examples through the loss statistics, and achieve better generalization by removing the examples that are potentially mislabeled.









\subsection{Thresholds that classify mislabeled examples}
The above observation suggests that it is possible to distinguish clean and noisy examples \textit{via a threshold over the loss value}. In principle, we can claim an example is noisy if its loss value exceeds a certain threshold; by removing the noisy labels from the training set, we could then improve generalization performance on clean validation sets.

However, to improve generalization in practice, one critical problem is to select a reasonable threshold for classification. High thresholds could include too many examples from  (the mislabeled set), whereas low thresholds could prune too many examples from  (the clean set); reasonable thresholds should also adapt to different ratios of mislabeled examples, which could be unknown to practitioners. 



If we are able to characterize the loss of  (the highest entropy distribution), we can select a reasonable threshold from this loss as any example having higher loss is likely to have high entropy labels (and is possibly mislabeled).
From Fig.~\ref{fig:histogram}, the loss distribution for  is relatively stable with different ratios of ; examples in  are making little progress when learning rate is large. This suggests a threshold selecting criteria that is \textit{independent of the amount of mislabeled examples in the dataset}.

We propose to characterize the loss distribution of (counterfactual) uniform label noise via the following procedure:

We denote this counterfactual distribution model as .

 tries to simulate the behavior of the model (and the loss distribution) with several components.
\begin{itemize}
    \item  represents a random label from  classes. This simulates the case where  has the highest entropy, i.e. uniformly random.
    \item  is the final (fully connected) layer of the network and  is the Rectified Linear Unit. This simulates the behavior at the last layer of the network outputs .
    \item  suggests that the inputs to the last layer has an identity covariance; the scale of the covariance could result from well-conditioned objectives defined via deep residual networks~\cite{he2015deep}, batch normalization~\cite{ioffe2015batch} and careful initialization~\cite{he2015delving}.
\end{itemize}
We qualitatively demonstrate the validity of our characterization on CIFAR-100 and ImageNet datasets in Fig.~\ref{fig:histogram}, where we plot the histogram of the  distribution for CIFAR-100 and ImageNet, and compare then with the empirical distribution of the loss of uniform noisy labeled examples.
The similarities between the noisy loss distribution and simulated loss distribution  demonstrate that an accurate characterization of the loss distribution can be made \textit{without prior knowledge of the mislabeled examples}. 


To effectively trade-off between precision (correctly identifying noisy examples) and recall (identifying more noisy examples), we define a threshold via the -th percentile of  using the samples generated by Equation~\ref{eq:loss-dist}; it relates to approximately how much examples in  we would retain if  is uniform. In Section~\ref{sec:exp-ablation}, we show that this method is able to identify different percentages of uniform label noise with high precision.



\subsection{A Practical Algorithm for Robust Training}
We can utilize this to remove examples that might harm generalization, leading to \textit{On-the-fly Data Denoising} (\textsc{ODD}), a simple algorithm robust to mislabeled examples.

\begin{algorithm}
  \caption{On-the-fly Data Denoising}
  \label{alg:odd}
\begin{algorithmic}
  \STATE {\bfseries Input:} dataset  of size , model , percentile , epoch , learning rate schedule .
  \FOR{}
  \STATE Train on  with learning rate .
  \ENDFOR
  \STATE  = -th percentile of  in Eq.~(\ref{eq:loss-dist}) 
  \STATE , .
  \FOR{}
  \STATE Train on  with learning rate .
  \ENDFOR
\end{algorithmic}
\end{algorithm}




\subsubsection{Hyperparameter selection} \textsc{ODD} introduces two hyperparameters:  determines the amount of training that separates clean examples from noisy ones;  determines  that specifies the trade-off between less noisy examples and more clean examples. We do not explicitly estimate the portion of noise in the dataset, nor do we assume any specific noise model. Moreover, \textsc{ODD} is compatible with existing practices for learning rate schedules, such as stepwise~\cite{he2015deep} or cosine~\cite{loshchilov2016sgdr}.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figs/hyper.pdf}
    \caption{Hyperparameter selection. (Left) Cosine learning rate schedule across epochs; we wish to select  before learning rate becomes small, and after training over clean labels have converged. (Right) Histogram of the losses; we wish to select  that does not remove too many clean data, but also removes as many (conterfactually) noisy data as possible.}
    \label{fig:hyper}
\end{figure}

In Fig.~\ref{fig:hyper} we demonstrate and discuss how to choose the hyperparameters  and . For , we wish to perform ODD operation at a point not too early (to allow enough time for training on clean labels to converge) and not too late (to prevent overfitting noisy labels with small learning rates). For , we wish to trade-off between keeping as much clean data as possible and removing counterfactually noisy data; selecting  typically works for our case. \section{Experiments}
\label{sec:experiments}
We evaluate our method extensively on several clean and noisy datasets including CIFAR-10, CIFAR-100,  ImageNet~\cite{russakovsky2015imagenet}, WebVision~\cite{li2017webvision} and Clothing1M~\cite{xiao2015learning}. CIFAR-10, CIFAR-100 and ImageNet are clean whereas WebVision and Clothing1M are obtained via web supervision and have more noisy labels. Our experiments consider datasets that are clean, have artificial noise (in CIFAR-10, CIFAR-100 and ImageNet), or have inherent noise from web-supervision (as in the case of WebVision and Clothing1M). 








\begin{table*}[htbp]
\begin{center}
\caption{Validation accuracy (in percentage) with uniform label noise.}
\label{tab:cifar-val}
\begin{tabular}{c|ccc|ccc}
\toprule
& \multicolumn{3}{c|}{CIFAR-10} & \multicolumn{3}{c}{CIFAR-100} \\
 mislabeled                                           & 0                              & 20                              & 40                & 0 & 20 & 40             \\\midrule
\textsc{ERM}                                           &  &  &  &   &  &  \\
\textit{mixup}                                                &                &  &   &  &  &   \\
\textsc{GCE}     &  -  &   &   & - &  &  \\
\textsc{Luo} &  &  &  &  &  &  \\
\midrule
\textsc{Ren}   &    -    &    -     &  & - & - &   \\
\textsc{MentorNet} &   -      & 92.0    & 89.0  & - & 73.0 & 68.0 \\\midrule
\textsc{ODD}                              &  &   &  &  &   &     \\
\textsc{ODD} + \textit{mixup}                              &  &   &  &  &   &     \\
\bottomrule                          
\end{tabular}
\end{center}
\end{table*}

\subsection{CIFAR-10 and CIFAR-100}
\label{sec:exp-cifar}
We first evaluate our method on the CIFAR-10 and CIFAR-100 datasets, which contain 50,000 training images and 10,000 validation images of size  with 10 and 100 labels respectively. 
In our experiments, we train the wide residual network architecture (WRN-28-10) in~\cite{zagoruyko2016wide} for 200 epochs with a minibatch size of 128, momentum  and weight decay . We set  (total number of epochs is 200) and  in our experiments.


\subsubsection{Input-Agnostic Label Noise}

We first consider label noise that are agnostic to inputs. 
Following~\cite{zhang2016understanding}, We randomly replace a ) of the training labels to uniformly random ones, and evaluate generalization error on the clean validation set.
We compare with the following baselines: Empirical Risk Minimization (\textsc{ERM}, Eq.~\ref{eq:erm},~\cite{goyal2017accurate}) which assumes all examples are clean; \textsc{MentorNet}~\cite{jiang2017mentornet}, which pretrains an auxiliary model that predicts weights for each example based on its input features; \textsc{Ren}~\cite{ren2018learning}, which optimizes the weight of examples via meta-learning; \textit{mixup}~\cite{zhang2017mixup}, a data augmentation approach that trains neural networks on convex combinations of pairs of examples and their labels; Generalized Cross Entropy (\textsc{GCE},~\cite{zhang2018generalized}) that includes cross-entropy loss and mean absolute error~\cite{ghosh2017robust}; and \textsc{Luo}~\cite{luo2019simple}, which regularizes the Jacobian of the network. We also consider using \textit{mixup} training after we pruned noisy examples with \textsc{ODD}. 

\begin{figure*}[tbp]
    \centering
    \includegraphics[width=0.9\textwidth]{figs/leopard}
    \caption{Examples with label ``leopard'' that are classified as mislabeled.}
    \label{fig:leopard}
\end{figure*}




\begin{figure*}[tbp]
    \centering
    \includegraphics[width=0.9\textwidth]{figs/noisy-images-cifar100.pdf}
    \caption{Random CIFAR-100 examples that are classified as mislabeled.}
    \label{fig:noisy-images-cifar100-t}
\end{figure*}












We report the top-1 validation error in Table~\ref{tab:cifar-val}, where  denotes methods trained with knowledge of 1000 additional clean labels. Notably, \textsc{ODD} + \textit{mixup} significantly outperforms all other algorithms (except for \textsc{LUO} with 20\% noise on CIFAR-10). On the one hand, this suggests that \textsc{ODD} is able to distinguish the mislabeled examples and improve generalization; on the other hand, it would seem that removing certain examples even in the ``clean'' dataset does not seem to hinder generalization, suggesting that our thresholds works in practice.





\subsubsection{Mislabeled examples in CIFAR-100} 
We display the examples in CIFAR-100 training set for which our \textsc{ODD} methods identify as noise across 3 random seeds. One of the most common label such examples have is ``leopard''; in fact, 21 of 50 ``leopard'' examples in the training set are perceived as hard, and we show some of them in Fig.~\ref{fig:leopard}. It turns out that a lot of the ``leopard'' examples contains images that clearly contains tigers and black panthers (CIFAR-100 has a label corresponding to ``tiger''). We also demonstrate random examples from the CIFAR-100 that are identified as noise in Fig.~\ref{fig:noisy-images-cifar100-t}. The examples identified as noise often contains multiple objects, or are more ambiguous in terms of identity. We include more results in Appendix~\ref{sec:app:noisy-cifar}.









\setlength{\tabcolsep}{5pt}
\begin{table}
\centering
    \caption{Results on non-homogeneous labels.}
\label{tab:cifar-merge}
\begin{tabular}{c|c|cc}
\toprule
    Task & \% samples removed () & \textsc{ERM} & \textsc{ODD} \\\midrule
  \multirow{3}{*}{CIFAR-50} 
  & 30 &  &  \\
  & 50 &  &  \\
  & 70 &  &   \\\midrule
  \multirow{3}{*}{CIFAR-20}  
  & 30 &  &  \\
  & 50 &  &  \\
  & 70 &  &  \\
  \bottomrule
\end{tabular}
\end{table}



\begin{table}
\begin{center}
\caption{Top-1 (top-5) accuracy on ImageNet.}
\label{tab:imagenet-uniform-noise}
\begin{tabular}{c|ccc}
\toprule
 mislabeled &  0 & 20 & 40 \\\midrule
\textsc{ERM} & \textbf{78.7} (94.3) & 72.6 (90.2) & 61.2 (84.4) \\
\textsc{Luo} & 76.7 (93.3) & 75.2 (92.3) & 73.2 (91.0) \\
\textsc{MentorNet} & -  & -  & 65.1 (85.9) \\\midrule
\textsc{ODD} () & \textbf{78.7} (94.0) & \textbf{77.5} (93.5) & \textbf{74.8} (92.1)
  \\\bottomrule
\end{tabular}
\end{center}
\end{table}


\subsubsection{Non-Homogeneous Labels}
We evaluate \textsc{ERM} and \textsc{ODD} on a setting without mislabeled examples, but the ratio of classes could vary. To prevent the model from utilizing the number of examples in a class, we combine multiple classes of CIFAR-100 into a single class, creating the CIFAR-20 and CIFAR-50 tasks. In CIFAR-50, we combine an even class with an odd class while we remove  of the examples in the odd class. In CIFAR-20, we combine 5 classes in CIFAR-100 that belong to the same super-classwhile we remove  of the examples in 4 out of 5 classes. This is performed for both training and validation datasets. Results for \textsc{ERM} and \textsc{ODD} with  and  are shown in Table~\ref{tab:cifar-merge}, where \textsc{ODD} is able to outperform \textsc{ERM} in these settings where the input examples are not uniformly distributed.





\subsection{ImageNet}
\label{sec:exp-imagenet}
We consider experiments on the ImageNet-2012 classification dataset~\cite{russakovsky2015imagenet}.  Input-agnostic random noise of  are considered. We only use the center  crop for validation. 
We train ResNet-152 models~\cite{he2015deep} for  epochs and report top-1 and top-5 validation errors in Table~\ref{tab:imagenet-uniform-noise}. \textsc{ODD} significantly outperforms \textsc{ERM} and \textsc{Luo}~\cite{luo2019simple} in terms of both top-1 and top-5 errors with 20\% and 40\% label noise, while being comparable to \textsc{ERM} on the clean dataset.







\subsection{WebVision}
\label{sec:exp-webvision}

\begin{table}
\centering
\caption{Top-1 (top-5) accuracy on WebVision and ImageNet validation sets when trained on WebVision. }
\label{tab:webvision}
    \begin{tabular}{c | c c}
    \toprule
     Method    & WebVision & ImageNet \\\midrule


        LASS~\cite{arpit2017a} & 66.6 (85.6) & 59.0 (80.8) \\
        CleanNet~\cite{lee2018cleannet}  & 68.5 (86.5) & 60.2 (81.1) \\
        \textsc{ERM} & 69.7 (87.0) & 62.9 (83.6) \\
        \textsc{MentorNet}~\cite{jiang2017mentornet} & 70.8 (88.0) & 62.5 (83.0) \\
CurriculumNet~\cite{guo2018curriculumnet} & 73.1 (89.2) & 64.7 (84.9) \\
        \textsc{Luo}~\cite{luo2019simple} & 73.4 (89.5) & 65.9 (85.7) \\
        \midrule
        \textsc{ODD} &  \textbf{74.6} (90.6) & \textbf{66.7} (86.3)  \\\bottomrule
    \end{tabular}
\end{table}

We further verify the effectiveness of our method on a real-world noisy dataset. The WebVision-2017 dataset~\cite{li2017webvision} contains 2.4 million of real-world noisy labels, that are crawled from Google and Flickr using the 1,000 labels from the ImageNet-2012 dataset. 
We consider training Inception ResNet-v2~\cite{szegedy2016inception} for 50 epochs and use input images of size . 
We use both WebVision and ImageNet validation sets for 1-crop validation, following the settings in~\cite{jiang2017mentornet}. We do not use a pretrained model or additional labeled data from ImageNet.
In Table~\ref{tab:webvision}, we demonstrate superior results than other competitive methods tailored for learning with noisy labels. 

Our \textsc{ODD} method with  removes  of the total examples with Inception ResNet-v2~\cite{szegedy2016inception}. Table~\ref{tab:webvision} suggests that our method is able to outperform the baseline methods when the training dataset is noisy, even as we remove a notable portion of examples. In comparison, we removed around  of examples in ImageNet;  this suggest that WebVision labels are indeed much noisier than the ImageNet labels since there are more examples removed by the (counterfactual) threshold. 






\subsection{Clothing1M}
\label{sec:clothing1m}

\begin{table}
\centering
 \caption{Validation accuracy on Clothing1M.}
    \begin{tabular}{c|c|c}
    \toprule
        Method & Setting & Accuracy \\\midrule
        \textsc{ERM} & noisy & 68.9 \\
        GCE & noisy & 69.1 \\
        Loss Correction~\cite{patrini2017making} & noisy & 69.2 \\
        LCCN~\cite{yao2019safeguarded} & noisy & 71.6 \\
        Joint Opt.~\cite{tanaka2018joint} & noisy & 72.2 \\
        DMI~\cite{xu2019l_dmi} & noisy & 72.5 \\
        \textsc{ODD} & noisy & \textbf{73.5} \\\midrule
        \textsc{ERM} & clean & 75.2 \\
        Loss Correction & noisy + clean & \textbf{80.4} \\
        \textsc{ODD} & noisy + clean & \textbf{80.3} \\
        \bottomrule
    \end{tabular}
    \label{tab:clothing1m}
\end{table}

Clothing1M~\cite{xiao2015learning} contains 1 million examples with noisy labels and 50,000 examples with clean labels 
of 14 classes. Following procedures from previous work, we use the ResNet-50 architecture pre-trained on ImageNet, with a starting learning rate of 0.001 trained with 10 epochs. We consider three settings, where the dataset contains \textit{clean} labels only, \textit{noisy} labels only, or both types of labels. For \textsc{ODD}, we set  for the noisy dataset ( because we fine-tune from ImageNet pre-trained model); we then fine-tune on the clean labels if they are available. 

Table~\ref{tab:clothing1m} suggests our method compares favorably against existing methods such as GCE, Joint Optimization~\cite{tanaka2018joint}, latent class-conditional noise
model (LCCN,~\cite{yao2019safeguarded}) and Determinant based Mutual
Information (DMI,~\cite{xu2019l_dmi}) on the \textit{noisy} dataset, and is comparable to Loss Correction (LC, ~\cite{patrini2017making}) on the \textit{noisy + clean dataset}. We note that LC estimates the label confusion matrix using examples with both clean and noisy labels; the complexity of LC scales quadratically in the number of classes, and it would not be feasible for ImageNet or WebVision.


\subsection{Ablation Studies}
\label{sec:exp-ablation}



We include additional ablation studies in Appendix~\ref{sec:exp-ablation}.

\begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{figs/ablations.pdf}
    \caption{(Left) ablating  on ImageNet. (Right) ablating  on CIFAR10.}
    \label{fig:ablations}
\end{figure}


\subsubsection{Sensitivity to } We first evaluate noisy ImageNet classification with varying . A higher  includes more clean examples at the cost of involving more noisy examples. 
From Fig.~\ref{fig:ablations} (left), \textsc{ODD} is not very sensitive to , and empirically  represents the best trade-off. 


\subsubsection{Sensitivity to } We evaluate the validation error of \textsc{ODD} on CIFAR with  and  input-agnoistic label noise where  ( is equivalent to \textsc{ERM}). The results in Fig.~\ref{fig:ablations} (right) demonstrate that the effect of  on final performance behaves according to our suggestion. 













%
 \section{Related work}
\subsubsection{Generalization of SGD Training} The generalization of neural networks trained with SGD depend heavily on learning rate schedules~\cite{loshchilov2016sgdr}. It has been proposed that wide local minima could result in better generalization~\cite{hochreiter1995simplifying,chaudhari2016entropy,keskar2016on}. Several factors could contribute to wider local optima and better generalization, such as smaller minibatch sizes~\cite{keskar2016on}, reasonable learning rates~\cite{kleinberg2018an}, longer training time~\cite{hoffer2017train}, or distance from the initialization point~\cite{hoffer2017train}. 
In the presence of mislabeled examples, changes in optimization landscape~\cite{arpit2017a} could result in bad local minima~\cite{zhang2016understanding}, although it is argued that larger batch sizes could mitigate this effect~\cite{rolnick2017deep}.

\subsubsection{Training with Mislabeled Examples} One paradigm involves estimating the noise distribution~\cite{liu2014classification} or confusion matrix~\cite{sukhbaatar2014training}. 
Another line of methods propose to identify and clean the noisy examples~\cite{cretu2008casting} through predictions of auxillary networks~\cite{veit2017learning,patrini2017making} or via binary predictions~\cite{northcutt2017learning}; the noisy labels are either pruned~\cite{brodley1996identifying} or replaced with model predictions~\cite{reed2014training}. 
Our method is comparable to these approaches, but the key difference is that we leverage the implicit regularization of SGD to identify noisy examples. We note that \textsc{ODD} is different from hard example mining~\cite{shrivastava2016training} which prunes ``easier'' examples with lower loss; this does not remove mislabeled examples effectively. The method proposed in~\cite{northcutt2017learning} is most similar to ours in principle, but is restricted to binary classification settings.
Other approaches propose to balance the examples via a pretrained network~\cite{jiang2017mentornet}, meta learning~\cite{ren2018learning}, or surrogate loss functions~\cite{ghosh2017robust,zhang2018generalized,tanaka2018joint}. Some methods require a set of trusted examples~\cite{xiao2015learning,hendrycks2018using}. 

\textsc{ODD} has several appealing properties compared to existing methods. First, the thresholds for classifying mislabeled examples from \textsc{ODD} do not rely on estimations of the noise confusion matrix. Next, \textsc{ODD} does not require additional trusted examples. Finally, \textsc{ODD} removes potentially noisy examples on-the-fly; it has little computational overhead compared to standard SGD training. \section{Discussion}
We have proposed \textsc{ODD}, a straightforward method for robust training with mislabeled examples. \textsc{ODD} utilizes the implicit regularization effect of stochastic gradient descent, which allows us to reason counterfactually about the loss distribution of examples with uniform label noise. Based on quantiles of this (counterfactual) distribution, we can then prune examples that would potentially harm generalization.
Empirical results demonstrate that \textsc{ODD} is able to significantly outperform related methods on a wide range of datasets with artificial and real-world mislabeled examples, maintain competitiveness with \textsc{ERM} on clean datasets, as well as detecting mislabeled examples automatically in CIFAR-100. 

The implicit regularization of stochastic gradient descent opens up other research directions for implementing robust algorithms. For example, we could consider removing examples not only once but multiple times, retraining from scratch with the denoised dataset, or other data-augmentation approaches such as \textit{mixup}~\cite{zhang2017mixup}. Moreover, it would be interesting to understand the \textsc{ODD} from additional theoretical viewpoints, such as the effects of large learning rates. 
\clearpage
\bibliographystyle{splncs04}
\bibliography{reference}

\appendix
\newpage
\onecolumn
\appendix




\section{Additional Experimental Results}
\subsection{Ablation Studies}
\label{sec:exp-ablation}



\subsubsection{Sensitivity to } We first evaluate noisy ImageNet classification with varying . A higher  includes more clean examples at the cost of involving more noisy examples. 
From Figure~\ref{fig:app:ablation-imagenet-p}, \textsc{ODD} is not very sensitive to , and empirically  represents the best trade-off. \begin{figure}[h]
    \centering
    \includegraphics[width=0.4\textwidth]{figs/imagenet_p_ablation.pdf}
    \caption{Ablation studies over the hyperparameter  on ImageNet under different levels of mislabeled examples.}
    \label{fig:app:ablation-imagenet-p}
\end{figure}



\subsubsection{Sensitivity to } We evaluate the validation error of \textsc{ODD} on CIFAR with  and  input-agnoistic label noise where  ( is equivalent to \textsc{ERM}). The results in Figure~\ref{fig:app:epoch-vs-val-error} suggest that our method is able to separate noisy and clean examples if  is relatively small where the learning rate is high, but is unable to perform well when the learning rate decreases.

\begin{figure}[h]
\centering
\includegraphics[width=0.50\textwidth]{figs/epoch-vs-val-err-cifar10}
\caption{Validation errors of \textsc{ODD} on CIFAR10 with different values of .
}
\label{fig:app:epoch-vs-val-error}
\end{figure}

\subsubsection{Sensitivity to the amount of noise} Finally, we evaluate the training error of \textsc{ODD} on CIFAR under input-agnostic label noise of  with ,  or . This reflects how much examples exceed the threshold and are identified as noise at epoch . From Figure~\ref{fig:app:noise-vs-train-error}, we observe that the training error is almost exactly the amount of noise in the dataset, which demonstrates that the loss distribution of noise can be characterized by our threshold regardless of the percentage of noise in the dataset.

\begin{figure}
\centering
\includegraphics[width=0.50\textwidth]{figs/noise-vs-train-err-v2-50}
\caption{Training errors of \textsc{ODD} on CIFAR10 with different amount of uniform noise.
}
\label{fig:app:noise-vs-train-error}
\end{figure}















\subsubsection{Precision and recall for classifying noise} We evaluate precision and recall for examples classified as noise on CIFAR10 and CIFAR100 for different noise levels (1, 5, 10, 20, 30, 40) in Figure~\ref{fig:app:cifar-prec-recall}. The recall values are around 0.84 to 0.88 where as the precision values range from 0.88 to 0.92. This demonstrates that \textsc{ODD} is able to achieve good precision/recall with default hyperparameters even at different noise levels.

\begin{figure*}[htbp]
\centering
\includegraphics[width=0.50\textwidth]{figs/recall.pdf}
\hspace*{0.05\textwidth}
\includegraphics[width=0.50\textwidth]{figs/prec}
\caption{Recall and precision for \textsc{ODD} on CIFAR10 and CIFAR100 with different levels of uniform random noise. }
\label{fig:app:cifar-prec-recall}
\end{figure*}





\subsubsection{Percentage of samples discared by \textsc{ODD}} We show the percentage of examples discarded by \textsc{Noise Classifier} in Table~\ref{tab:ablation-imagenet-discard}; the percentage of discarded examples by  is very close to the actual noise level, suggesting that it is a reasonable setting. 

\begin{table*}[htbp]
\begin{center}
\caption{Percentage of example discraded by \textsc{ODD} on ImageNet-2012.}
\label{tab:ablation-imagenet-discard}
\begin{tabular}{c|ccccc|c}
\toprule
\multirow{2}{*}{\% Mislabeled}  & \multicolumn{5}{|c|}{Hyperparameter } & \multirow{2}{*}{Network}
\\
& 1 & 10 & 30 & 50 & 80 & \\\midrule
 & 5.5 & 2.3 & 1.1 & 0.7 & 0.4 & \multirow{3}{*}{ResNet-152} \\
 & 23.8 & 20.8 & 19.2 & 17.5 & 0.7 &\\
 & 44.1 & 40.2 & 36.2 & 27.6 & 0.6  & \\
\bottomrule
\end{tabular}
\end{center}
\end{table*}







\subsubsection{Ablation studies on WebVision} We include additional ablation on  for WebVision (Table~\ref{tab:wv}). While the results for  is slightly better, our method outperforms other methods (\textsc{Luo}) even with worse hyperparameters.

\begin{table}
\centering
\caption{Additional results on WebVision with varying .}
\vspace{1em}
\begin{tabular}{cllll}
\toprule
  & \multicolumn{2}{l}{Webvision}      & \multicolumn{2}{l}{ImageNet}                 \\ \midrule
                   & Top1      & Top5  & Top1  & Top 5\\\midrule
1               & 74.01     & 89.93 & 65.77    & 85.40        \\
10              & 74.31     & 90.55 & 66.09    & 85.86       \\
30              & 74.62     & 90.63 & 66.73    & 86.32        \\
50              & 74.43     & 90.78 & 66.58    & 86.21        \\
80              & 74.33      & 90.30 & 66.23    & 86.24       \\ \bottomrule
\end{tabular}
\label{tab:wv}
\end{table}


\subsection{Images in CIFAR-100 Classified as Noise}
\label{sec:app:noisy-cifar}
We display the examples in CIFAR-100 training set for which our \textsc{ODD} methods identify as noise across 3 random seeds. One of the most common label such examples have is ``leopard''; in fact, 21 of 50 ``leopard'' examples in the training set are perceived as hard, and we show some of them in Figure~\ref{fig:app:leopard}. It turns out that a lot of the ``leopard'' examples contains images that clearly contains tigers and black panthers (CIFAR-100 has a label corresponding to ``tiger'').

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{figs/leopard}
    \caption{Examples with label ``leopard'' that are classified as noise.}
    \label{fig:app:leopard}
\end{figure*}

We also demonstrate random examples from the CIFAR-100 that are identified as noise in Figure~\ref{fig:app:noisy-images-cifar100} and those that are not identified as noise in Figure~\ref{fig:app:clean-images-cifar100}. The examples identified as noise often contains multiple objects, and those not identified as noise often contains only one object that is less ambiguous in terms of identity.


\begin{figure*}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{figs/noisy-images-cifar100.pdf}
    \caption{Random CIFAR-100 examples that are classified as noise.}
    \label{fig:app:noisy-images-cifar100}
\end{figure*}

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{figs/clean-images-cifar100.pdf}
    \caption{Random CIFAR-100 examples that are not classified as noise.}
    \label{fig:app:clean-images-cifar100}
\end{figure*} 
\end{document}
