\documentclass[11pt, letterpaper]{article}
\usepackage{fullpage}
\usepackage[utf8]{inputenc} \usepackage{ae}
\usepackage{enumerate}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{stmaryrd}
\usepackage[breaklinks,bookmarks=false]{hyperref}
\usepackage{enumerate}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}


\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{warning}[theorem]{Warning}
\newenvironment{proof}{\noindent{\bf Proof:}\hspace*{1em}}{\qed\bigskip}
 
\clubpenalty=10000
\widowpenalty = 10000
\newcommand{\qed}{\hfill\ensuremath{\square}}

\DeclareMathOperator{\age}{age}
\DeclareMathOperator{\dist}{dist}

\newcommand{\cC}{\mathcal{C}}
\newcommand{\ocC}{\bar{\mathcal{C}}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\tO}{\widetilde{O}}
\newcommand{\Dstar}{\gamma^*}
\newcommand{\cT}{\mathcal{T}}
\newcommand{\alphaB}{\alpha_B}
\newcommand{\Reff}{R_{\mathrm{eff}}}
\newcommand{\diameff}{\gamma_{\mathrm{eff}}}
\newcommand{\diamdist}{\Delta}
\newcommand{\energy}{\mathcal{E}}
\newcommand{\proj}[2]{#1\llbracket #2\rrbracket}
\newcommand{\hX}{\widehat{X}}
\newcommand{\tX}{\widetilde{X}}
\newcommand{\he}{\hat{e}}
\newcommand{\hi}{\hat{i}}
\newcommand{\te}{\tilde{e}}
\newcommand{\ti}{\tilde{i}}
\newcommand{\cut}{{K}}
\newcommand{\wit}[1]{\mathop{W}(#1)}
\newcommand{\cR}{\mathcal{R}}
\newcommand{\of}{\bar{f}}
\newcommand{\od}{\hat{d}}
\newcommand{\bE}{\mathbb{E}}

\newcommand{\todo}[1]{\noindent \colorbox{red}{\begin{minipage}{\linewidth}{\bf TODO:} #1\end{minipage}}}
\newcommand{\ceil}[1]{\lceil #1 \rceil}
\newcommand{\floor}[1]{\lfloor #1 \rfloor} 




\begin{document}

\title{Fast Generation of Random Spanning Trees and the Effective Resistance Metric}

\author{ Aleksander Mądry\thanks{aleksander.madry@epfl.ch} \\ EPFL \and Damian Straszak\thanks{damian.straszak@gmail.com} \\ University of Wrocław\and Jakub Tarnawski\thanks{jakub.tarnawski@gmail.com} \\ University of Wrocław}

\date{}
 
\maketitle

\begin{abstract}
We present a new algorithm for generating a uniformly random spanning tree in an undirected graph. Our algorithm samples such a tree in expected  time. This improves over the best previously known bound of  -- that follows from the work of Kelner and Mądry [FOCS'09] and of Colbourn et al. [J. Algorithms'96] -- whenever the input graph is sufficiently sparse.

At a high level, our result stems from carefully exploiting the interplay of random spanning trees, random walks, and the notion of effective resistance, as well as from devising a way to algorithmically relate these concepts to the combinatorial structure of the graph. This involves, in particular, establishing a new connection between the effective resistance metric and the cut structure of the underlying graph.  
\end{abstract}







\section{Introduction}\label{sec:introduction}

Random spanning trees are among the oldest and most extensively investigated probabilistic objects in graph theory (see, e.g., \cite{LyonsP13}). Their study goes back as far as to the works of Kirchhoff in the 1840s \cite{Kirchhoff47}. Since then, it has spawned a large number of surprising connections to a variety of topics in theoretical computer science, as well as in discrete probability and other areas of mathematics. For example, random spanning trees are intimately related to the task of spectral graph sparsification \cite{SpielmanS08,BatsonSS09} and Goyal et al. \cite{GoyalRV09} showed how to directly use them to generate an efficient cut sparsifier. They are also closely tied to the notion of electrical flows, a powerful primitive that was instrumental in some of the recent progress on the maximum flow problem \cite{ChristianoKMST11,Madry13}. Finally, random spanning trees were at the heart of the recent results that broke certain long-standing approximation barriers for both the symmetric and the asymmetric version of the Traveling Salesman problem \cite{AsadpourGMOS10,OveisGharanSS11}. 

It is, therefore, not surprising that the task of developing fast algorithms for generating random spanning trees has received considerable attention over the years (\cite{Guenoche83,Kulkarni90,ColbournDM88,ColbournDN89,ColbournMN96,Aldous90,Broder89,Wilson96,KelnerM09}). Broadly, these algorithms can be divided into two categories: determinant-based ones and random walk--based ones. 

The determinant-based algorithms rely on the celebrated Kirchhoff's Matrix Tree Theorem \cite{Kirchhoff47} (also see, e.g., Ch.3, Thm.8 in \cite{Bollobas79}) that reduces counting of the spanning trees of a graph to computing a determinant of a certain associated matrix. This connection was first used in the works of Gu\'enoche \cite{Guenoche83} and of Kulkarni \cite{Kulkarni90} and resulted in -time algorithms. Then, after a sequence of improvements \cite{ColbournDM88,ColbournDN89}, this line of research was culminated with the result of Colbourn et al. \cite{ColbournMN96} that generates a random spanning tree in time , where  is the exponent of the fastest algorithm for square matrix multiplication \cite{CoppersmithW90,Vassilevska12}.

On the other hand, the random walk--based approach to random spanning tree generation is based on the following striking theorem due to Broder \cite{Broder89} and Aldous \cite{Aldous90}:


\begin{theorem}[\cite{Aldous90,Broder89}] \label{thm:rand_tree_via_rand_walk}
Suppose you simulate a random walk in an undirected graph , starting from an arbitrary vertex  and continuing until this walk covers the whole graph, i.e., until every vertex has been visited. For each vertex  other than , let  be the edge through which  was visited for the first time in this walk. Then,  is a uniformly random spanning tree of .
\end{theorem}

In light of this theorem, we can focus our attention on simulating a covering random walk in the input graph  and then just reading off our sampled spanning tree out of the first-visit edges of that walk. The running time of the resulting procedure is proportional to the cover time of the input graph . If  has  vertices and  edges, it is known that this cover time is always  \cite{AleliunasKLLR79}. This improves upon the  bound corresponding to the determinant-based approach whenever  is sparse enough. 

Unfortunately, there are examples of graphs for which the above  bound on the cover time is tight. Therefore, it is natural to wonder if one can obtain a random walk--based algorithm for random spanning tree generation that runs faster than the cover time. Wilson \cite{Wilson96} was the first one to show that this is indeed the case. He put forth a certain random process that is a bit different than random walk, but related. Using this process one is able to generate a random spanning tree in time proportional to the mean hitting time of the graph. This quantity turns out to never be much larger than the cover time and is often much smaller. However, its worst-case asymptotics is still .

Later on,  Kelner and Mądry \cite{KelnerM09} showed that this worst-case bound of  can nevertheless be improved upon. (Initially, this improvement was only for approximate sampling, but later Propp \cite{Propp10} showed how to make it work for the exact case too -- see Lemma 7.3.2 in \cite{Madry11}.) They observed that to use Theorem~\ref{thm:rand_tree_via_rand_walk}, one does not need to simulate the covering random walk in full. It suffices to generate its shortcut transcript which retains enough information to allow us to recover what the first-visit edges were. 

 Following this observation, they put forth such a shortcutting technique that relied on a simple diameter--based graph partitioning primitive of Leighton and Rao \cite{LeightonR99}. This technique yields a transcript which is significantly shorter than the full transcript of the covering walk and can be efficiently generated using fast (approximate) Laplacian system solvers \cite{SpielmanT03,SpielmanT04,KoutisMP10,KoutisMP11,KelnerOSZ13}.  This resulted in a generation procedure with  total (expected) running time, which is the best one known for sparse graphs.


\subsection{Our Contribution}

In this paper, we present a new algorithm that makes progress on the sparse-graph regime of fast random spanning tree generation. More precisely, we prove the following theorem. 

\begin{theorem}
\label{thm:main}
For any connected graph  with  edges, we can generate a uniformly random spanning tree of  in expected  time.
\end{theorem}
This improves over the  bound of Kelner and Mądry \cite{KelnerM09} whenever the graph is sufficiently sparse.

\subsection{Our Approach}\label{sec:our_approach}

The starting point of our approach is the Kelner-Mądry algorithm \cite{KelnerM09} we mentioned above. Recall that this algorithm generates a random spanning tree via simulation of a shortcut transcript of the covering random walk. That transcript retains enough information to allow us to recover what the first-visit edges were while being relatively short and possible to generate quickly. This results in an improved  running time. 

Unfortunately, their technique has certain limitations. Specifically, there are graphs for which the length of the shortcut transcript they aim to construct is inherently . (See Section \ref{sec:overview} for such an example.) This makes the  running time a barrier for their approach.

Our algorithm broadly follows the theme put forth by Kelner-Mądry. However, to obtain an improved running time and, in particular, to overcome this  barrier, we depart from this theme in a number of ways. 

The key new element of our approach is making the effective resistance metric, and its connections to random spanning trees and random walks, the central driver of our algorithm. Compared to the traditional graph distance--based optics employed by Kelner and Mądry, using such an effective resistance--based view gives us a much tighter grasp on the behavior of random walks. It also enables us to tie this behavior to the cut structure of the graph in a new way. 

Specifically, we develop a relationship between the fact that two vertex sets are relatively far away from each other in the effective resistance metric and the existence of a small cut separating them. (See Lemma \ref{lem:good_cut} in Section \ref{sec:parititioning}.)  It can be seen as a certain minimum - cut--based analogue of the celebrated Cheeger's inequality \cite{AlonM85,Alon86} (see also \cite{KwokLLOT13}) and the role it plays in the context of the sparsest cut problem. We believe this relationship to be of independent interest.

Furthermore, to take full advantage of this effective resistance--based view, we also change the way we approach the task of generation of a random spanning tree. Namely, instead of focusing on efficient generation of a single shortcut transcript and then using it to recover the corresponding random spanning tree in full, we generate a number of carefully crafted transcripts, one after another. None of these transcripts alone is enough to extract the whole random spanning tree from it. More precisely, each one of them provides us only with a sample from the marginal distribution corresponding to the intersection of the random spanning tree with some subset of edges of the graph. However, by making sure that all these samples are consistent with each other (which is achieved by running each consecutive sampling procedure in a version of the graph that incorporates the choices made in previous iterations), we can combine all of them to recover the entire random spanning tree in the end. 
 
Finally, to make the above approach algorithmically efficient, we utilize a diverse toolkit of modern tools such as Laplacian system solvers, fast approximation algorithms for cut problems, and efficient metric embedding results. 
 
 
   




\subsection{Organization of the Paper}
We begin the technical part of the paper in Section \ref{sec:preliminaries}, where we present some preliminaries on the tools and notions we will need later. Next, in Section \ref{sec:overview}, we provide an overview of our algorithm and the description of its main components. We develop these components in subsequent sections. Specifically, in Section \ref{sec:parititioning}, we describe our graph-cutting procedure and, in particular, establish the connection between the effective resistance metric and graph cuts. Then, in Section \ref{sec:sampling}, we conclude by providing and analyzing our sampling procedure that generates the random spanning tree.
  
  









   \section{Preliminaries} \label{sec:preliminaries}
 

Throughout the paper,  denotes an undirected connected graph with vertex set  and edge set  (we allow parallel edges here). Let  be two subsets of vertices of . We define  to be the set of all the edges in  whose one endpoint is in  and the other one is in . Also, we define the {\em interior } of  to be the set of all the edges in  with both their endpoints in , i.e., . Further, the {\em boundary } of the set  is the set of all the edges in  with exactly one endpoint in . We say that an edge  is a {\em boundary edge} of  if . Finally, for a collection  of subsets of , we say that an edge  is a boundary edge of  iff  is a boundary edge of some set .

\subsection{Random Spanning Trees and Random Walks}

Our objective is to sample a random spanning tree  from the uniform distribution, that is, so that for every possible spanning tree  of , the probability that we output  is , where  is the set of all spanning trees of .

In light of the connection that Theorem~\ref{thm:rand_tree_via_rand_walk} establishes between random spanning trees and random walks, the latter notion will be at the center of our considerations.

By a random walk on  we mean the stochastic process corresponding to a walk that traverses the vertices of the graph by moving, in each step, from the current vertex  to one of its neighbors , chosen uniformly at random, over the edge . It is well-known that this stochastic process has a stationary distribution described by the formula , where  is the degree of  (see, e.g., \cite{Lovasz93}).

Now, by the {\em cover time}  of a graph  we mean the expected time it takes for a random walk started at a vertex  to visit (cover) every vertex of . (We take a worst-case bound here over all the possible choices of .)

\subsection{Effective Resistances and the Effective Resistance Metric}
\label{sec:effective_resistance}

Another key notion we will use to establish our result is that of effective resistance and the effective resistance metric. This notion arose originally in the context of study of electrical circuits in physics, but since then it has found a number of equivalent characterizations that highlight its intimate connection to the behavior of random walks and also to random spanning trees themselves (see \cite{DoyleS84,Bollobas98,LyonsP13,Lovasz93}). The characterization that will be especially useful to us was proved in \cite{ChandraRRS89} (see also, e.g., \cite{Lovasz93}). It states that the {\em effective resistance} distance  between two points  and  in  is equal to , where  is the {\em commute time} between  and , i.e., the expected number of steps that a random walk started at  takes before visiting  and returning back to .  Also, even more interestingly in our context, it is known that for any edge  of , the probability that this edge is included in a random spanning tree is exactly . 

Given the above characterization in terms of the commute time, it is not hard to see that the distances  give rise to a metric -- called the {\em effective resistance metric}. We may thus define the {\em effective resistance diameter} of a subset of vertices  to be . We also set .




\subsection{Low-Dimensional Embedding of the Effective Resistance Metric} \label{sec:reff_embedding}

In our algorithm we will often be interested in computing the effective resistance distance between pairs of vertices. It is well-known that, for any two vertices  and , one can obtain a very good estimate of the effective resistance  in  time using the fast (approximate) Laplacian system solvers \cite{SpielmanT03,SpielmanT04,KoutisMP10,KoutisMP11,KelnerOSZ13}. Unfortunately, as we will be interested in distances between many different pairs of points, this running time bound is still prohibitive. However, Spielman and Srivastava \cite{SpielmanS08} designed a way for computing (approximate) effective resistances distances that has significantly better per-query performance. 
\begin{theorem}
\label{thm:SpielmanSrivastava}
Let  be a graph with  edges. For every  we can find in  time a low-dimensional embedding  of the effective resistance metric into  such that with high probability

and each value of  can be computed in  time.
\end{theorem} 
We note that the way we employ the above theorem in our algorithm ensures that even when the guarantee \eqref{eq:SpielmanSrivastava} fails, this does not affect the algorithm's correctness -- just the running time. So, as this guarantee holds with high probability, we can assume from now on that \eqref{eq:SpielmanSrivastava} always holds. 



\subsection{Bounding the Cover Time via Effective Resistance}
\label{sec:cover_time_and_effective_resistance}

In Section \ref{sec:effective_resistance}, we have seen the tight connection between effective resistance and the commute time of random walks. As it turns out, drawing on this connection we can relate effective resistance to another key characteristic of random walks: the cover time. To describe this, let us recall the classic results of \cite{AleliunasKLLR79} (see also \cite{Lovasz93}) that the cover time  of a (connected) graph with  edges can be bounded from above by  as well as by , where  is the graph-distance diameter.


It turns out that by working with the effective resistance-based -- instead of the more traditional graph distance-based -- notion of diameter, one is able to obtain a much tighter characterization of the cover time by replacing  by  in the bound. It is tighter, as it is not hard to see that the effective resistance distance is always upper-bounded by the graph distance and thus, in particular,  for any . Moreover, it is also the case that the resulting upper bound on the cover time is close to being a lower bound. The following lemma, proved in Appendix~\ref{app:cover_weighted}, demonstrates both bounds. It stems from the work of Matthews \cite{Matthews88}. (Also, see \cite{KahnKLV00,DingLP12} for even tighter characterizations of the cover time.) 

\begin{lemma}\label{lem:cover_weighted}
For any graph  we have .
\end{lemma}

In Section~\ref{sec:random_walks_restricted_to_subgraphs} we generalize the above lemma to deal with cover times of {\em subgraphs}. This may be of independent interest.

\subsection{Ball-growing Partitioning} \label{sec:ball_growing}

An important primitive in our algorithm is the following simple but powerful graph partitioning scheme based on the ball-growing technique of Leighton and Rao \cite{LeightonR99}. It is in fact a simplified version of the -decomposition used in~\cite{KelnerM09}.

Let us first introduce the following definition.
\begin{definition}\label{def:ball_decomp}
We say that a partition of a graph  with  edges into disjoint components  is a -decomposition of  iff
\vspace{-7pt}
\begin{enumerate}[(1)]\addtolength{\itemsep}{-.5\baselineskip}
\item the graph distance--based diameter  of each  is at most ,
\item the total number of boundary edges of  is at most .
\end{enumerate}
\end{definition}
As it turns out -- see Appendix~\ref{app:ball_growing} -- we can always find the following -decomposition fast.
\begin{lemma}\label{lem:ball_growing}
For every  and every graph  with  edges, we can find a -decomposition in  time.
\end{lemma}

\subsection{Approximating Minimum - Cuts} \label{sec:approximating_minimum_cuts}

One of the key parts of our algorithm relies on being able to quickly identify an approximate minimum - cut in an undirected graph. To do that we use the recent fast -approximate maximum - flow results of Sherman \cite{Sherman13}, Kelner et al. \cite{KelnerLOS14} and Peng \cite{Peng14}.



\begin{theorem}
For any graph , any two vertices  and , and any , one can obtain a -approximation to the minimum - cut problem in  in  time.
\end{theorem}

 \section{The Overview of the Algorithm} \label{sec:overview}

The starting point of our algorithm is the random walk--based approach proposed by Theorem \ref{thm:rand_tree_via_rand_walk} and the Kelner-Mądry technique \cite{KelnerM09} of speeding it up. 

Recall that the bottleneck of the vanilla random walk--based approach is that performing a step-by-step simulation of a covering random walk can take  time. To circumvent this problem,  Kelner-Mądry observed that we actually never need to simulate this covering random walk in full. We just need to be able to recover all its first-visit edges . As it turns out, the latter can be done more efficiently by considering a certain ``shortcut'' version of that walk and simulating this shortcutting instead.

Roughly speaking, the design of this Kelner-Mądry shortcutting boils down to finding a partition of the graph  into regions that have diameter at most  each while cutting at most a -fraction of edges, for some choice of parameters  and . (One can obtain such a partition using the standard ball-growing technique of Leighton and Rao \cite{LeightonR99}.) Then, one starts a step-by-step simulation of a covering random walk in . The crucial difference is that whenever some region is fully covered by the walk, one starts shortcutting any subsequent visits to it. More precisely, upon future revisits to such an already-covered region, one never simulates the walk inside it anymore, but instead immediately jumps out of it according to an appropriate exit distribution. As \cite{KelnerM09} shows, these exit distributions can be computed (approximately) using fast Laplacian system solvers \cite{SpielmanT03,SpielmanT04,KoutisMP10,KoutisMP11,KelnerOSZ13}. Further, the resulting overall (expected) time needed to perform the whole shortcut simulation is at most  (with the optimal choice of parameters being  and ).


The above technique can be further refined to yield a total running time of , which is an improvement for non-sparse graphs (see \cite{KelnerM09} for details). However,  time turns out to be a barrier for this approach, even in the sparse-graph regime. 

\begin{figure}[ht]
\centering
\vspace{8pt}
\includegraphics[width=0.5\textwidth]{files/barrier_example}
\vspace{8pt}
\caption{An example of a graph on which Kelner-Mądry algorithm runs in  time. It consists of two expanders  and , each having  vertices, connected by  disjoint paths of length  each.} 
\label{fig:barrier_example}
\end{figure}


To understand why this is the case, consider the graph depicted in Figure \ref{fig:barrier_example}. This graph has an  diameter. Therefore, if one is interested in generating a random spanning tree only in  running time, one can simply simulate its covering random walk faithfully -- by a well-known result \cite{AleliunasKLLR79}, this walk will have an expected length of .

However, if one wanted to use the Kelner-Mądry approach to get an improved running time here, one hits a bottleneck. The difficulty is that the graph cannot be partitioned in a way that makes the diameter  of each piece, as well as the number  of edges cut, both be . (In particular, the two expander-like parts  and  of this graph are especially problematic due to their size and being far away from each other as well as having large boundaries.) So, the running time of the Kelner-Mądry algorithm is always  here.

\subsection{Breaking the  Barrier}

Our improved -time algorithm that, in particular, breaks this  running time barrier, stems from a number of new observations. We present them here in the context of the example graph depicted in Figure \ref{fig:barrier_example}. This will serve as an informal motivation for the general techniques that we introduce in the remainder of the paper.


The key observation to make is that random walks are a spectral notion, not a combinatorial one. Therefore, basing our analysis on the effective resistance metric -- instead of applying the traditional graph distance--based optics -- turns out to be a better choice. 

In particular, if one considers our graph in Figure \ref{fig:barrier_example} from this perspective, one can notice that the effective resistance--based geometry of that graph differs from its graph distance--based geometry in a very crucial way. Namely, the effective resistance--based diameter of the whole graph is still . (This can be seen by looking at the middle vertices of any two different paths in .) But, the two expander-like parts  and  -- which were far away from each other in the graph distance--based sense -- are actually very close from the point of view of effective resistance distance! (This difference stems from the fact that the latter notion depends not only on the length of the shortest path but also on the number of disjoint paths.) 

As a result, the union  of these two parts forms a region that can be covered relatively efficiently by a random walk. More precisely, as we show in Lemma~\ref{lem:ultimate}, a random walk in  needs only a small number of steps {inside}  before covering this region in full. 

Now, to turn this observation into a fast routine for random spanning tree generation, we need to cope with two issues. Firstly, Lemma~\ref{lem:ultimate} bounds only the number of steps {\em inside} the region . Secondly, it measures the number of steps needed before the region is covered -- the number of steps needed to cover the whole graph  might (and actually will) be much larger. 

Dealing with the former issue is relatively straightforward in our example. After all, the outside of our region in  is just a collection of paths. So, it is easy to partition it into small sets with relatively sparse boundaries and then use the shortcutting technique of Kelner-Mądry to simulate the random walk over these sets efficiently. 

To deal with the latter issue, we abandon the original desire of sampling the whole random spanning tree ``in one shot'', i.e., from a trajectory of a single covering random walk. Instead, we first sample only a part of the random spanning tree: its intersection with our region . This corresponds to simulating a random walk that covers  (but not necessarily the whole graph ), and we already know how to do this fast. 

Once this intersection of our region and the random spanning tree is sampled, one can simply encode this sample in  by contracting the edges of our region that were chosen and removing the remaining ones. As  constituted a significant fraction of the edges of  (and was also the most troublesome structure in ), this sampling makes large progress and thus allows us to simply recurse on the updated (and smaller) version of .  

Now, even though the above discussion was focused on our example graph, it actually already covers almost all the crucial insights behind our general algorithm. 

In particular, at a high level, the main goal of our algorithm is exactly to try to identify in the input graph  some large region that, on one hand, has a small effective resistance diameter and, on the other hand, has an exterior ``shatterable'' enough to enable efficient Kelner-Mądry-type shortcutting. Once such a region is identified, we can make sufficient progress by sampling from its intersection with the random spanning tree, as described above. (Observe that one of the benefits of this approach is that we never need to shortcut the random walk over this region.)

At this point, the only major missing element is a method of dealing with situations when such a large small-radius region with ``shatterable'' exterior does not exist. (For instance, such a situation arises when we remove all but one path connecting  and  in our example graph.\footnote{In this case,  and  are no longer close in the effective resistance distance, and neither  nor  can be partitioned into small sets with sparse boundary.}) To cope with such cases, we establish a relationship between the effective resistance distance and the cut structure of the graph. Specifically, we prove that in such cases there has to exist a small cut that separates two large parts of the graph. (See Lemma~\ref{lem:good_cut} in Section~\ref{sec:parititioning} for details.) Therefore, in lieu of advancing our sampling, we can make progress by refining our grasp on the cut structure of the input graph. 

In the sequel, we proceed to making the above ideas rigorous and introducing the key technical notions behind our algorithm.









\subsection{Subdivisions, Overlays, and Covering Families}\label{sec:covering_families}

The graph-theoretic notions that will be key to our algorithm are subdivisions and covering families. A {\em subdivision}  is any collection of { disjoint} subsets of vertices. Then, a {\em covering family}  is any collection of  subdivisions  -- where  --  such that (1) ; and (2) for each , and any set  in , the total number of edges in the interior  of  is at most , i.e., for each  and all ,

where  is the number of edges of our input graph . (Condition (1) in the above definition is just a convention that we employ to ensure that each vertex always belongs to at least one subdivision in the covering family. We also assume  to be integral for simplicity.) It is important to observe here that even though sets contained in the same subdivision  need to be disjoint, sets belonging to different subdivisions can (and often will) overlap.

Now, the key reason for our use of covering families is that, even though they do not directly constitute a partition of the graph , they still provide an implicit -- and much more convenient to deal with from the computational point of view -- representation of certain partitions of . These partitions will play an important role in our algorithm. To define them formally, consider a covering family . Then, the {\em overlay  of } is the partition of  resulting from superimposing all the subdivisions  of  on top of each other. That is,  is a partition of  in which, for any two vertices  and  of , these vertices are in the same component of  iff they are not separated by any set in , i.e., for every set  in each subdivision  of , it is the case that either  and  both belong to  or both are not a part of .



\subsection{Shattering and -bounded Covering Families}\label{sec:shattering_alpha_bounded_families}

Our goal will be to obtain covering families  that, on one hand, have their overlay  partition the graph  into components of relatively small effective resistance diameter and, on the other hand, have a relatively small number of boundary edges in each of their subdivisions.

To make this precise, let us fix some covering family . We will call  {\em shattering} iff all the components in its overlay  have effective resistance diameter at most . Also, let us call  {\em -bounded}, for some , iff for each , the number of -based boundary edges in  is at most . Here, an edge  is an {\em -based boundary edge} of , for some , iff  belongs to the boundary  of some set , but is not part of the boundary of any set  in  with .

Before proceeding further, let us observe in passing that the fact that any boundary edge in  has to be an -based boundary edge for some  makes the -boundedness property also give us a bound on the total number of all boundary edges of each subdivision . Namely, the following simple fact holds.

\begin{fact}
\label{fa:alpha_boundness_and_boundary_edges}
If  is an -bounded covering family then, for each , the total number of boundary edges of the subdivision  is at most . Furthermore, the total number of all boundary edges in the whole  is at most .
\end{fact}

Now, in the language of the above definitions, our interest will be in finding covering families that are shattering and -bounded for some  being polylogarithmic in .\footnote{The exact value of  is irrelevant for us -- we treat it as a factor hidden under .} As we show in Section \ref{sec:parititioning}, such covering families can indeed be found efficiently. In particular, we prove there the following lemma.

\begin{lemma}
\label{lem:partitioning_main}
For any  and any -bounded covering family , we can, in  time, extend this  -- by only adding new sets to its  subdivisions or subdividing the existing ones -- to a covering family  that is shattering and -bounded with  and  being a fixed parameter that is polylogarithmic in . 
\end{lemma}

Note that the claimed existence of a -bounded and shattering covering family follows directly from this lemma once one takes  to be a trivial -bounded covering family with  and all other  being empty. However, as we will see shortly, our algorithm will actually need the above, more general, statement.

To try to motivate the definition of a covering family and its -boundedness, let us get a little ahead and say that
each boundary edge of  will incur a preprocessing time cost proportional to the maximum size of a set in  (cf. Lemma~\ref{lem:cost_shortcutting_sampling}).
For an -bounded covering family , the sum of these quantities is bounded by  for every . Thus, we will be able to support our running time bound by maintaining the trade-off between sizes of sets in  and the number of boundary edges of .






\subsection{-Marginals, -Conditioned Graphs, and Minimum-Age Interiors}\label{sec:marginals_overview}

As we mentioned earlier, our algorithm, instead of trying to sample the whole random spanning tree at once, will perform a sequence of samplings from certain marginal distributions. Namely, for a given subset of edges  of the graph , we will be interested in sampling from the marginal distribution -- that we will call the {\em -marginal of } -- describing the intersection of the random spanning tree with the set . 

Observe that the connection between random walks and sampling random spanning trees provided by Theorem \ref{thm:rand_tree_via_rand_walk} can be straightforwardly extended to give a sampling procedure for any -marginal. We can just simulate a covering random walk  in , starting from an arbitrary vertex , and take as our sample the intersection  of  and all the first-visit edges  to all the vertices of  other than  in that walk. 

Now, the key observation to make here is that to sample from the -marginal of , not only do we not need to simulate such a covering random walk  in full, but also we do not even need to recover all the first-visit edges . All we need is just to be able to extract all the edges  that could belong to . So, as trivially , we do not ever have to keep exact track of the movement of the walk  over edges which have no common endpoint with an edge of .

It turns out that having such ability to choose some  and then mostly ignore the parts of the random walks that do not touch  is very convenient and our algorithm takes advantage of that in a crucial way. Namely, as we show below, in any graph, there is always a choice of an appropriate set  so that sampling from the corresponding -marginal can be performed efficiently and it non-trivially advances our progress on recovering the whole random spanning tree.

To formalize this, let us consider some covering family  and let  be the corresponding overlay. Given some component  of that overlay, we say that  has {\em age} equal to , for some , -- we will denote this by  -- iff
\begin{itemize}\addtolength{\itemsep}{-.5\baselineskip}
\item  is a contained in some set  -- that we will call the {\em age witness } of  -- from the subdivision  of ;
\item and  is not contained in any set  from a subdivision  with .
\end{itemize}
(Note that due to the definition of the overlay it must be that each component  of that overlay can only be either fully contained in or completely disjoint from each set  in the covering family.) Also, observe that, as in our convention each covering family has , the age of all components of the overlay is well-defined. 

Finally, given a covering family , we define its {\em age}  to be the smallest age of all the components of its overlay , and its {\em minimum-age interior}  to be the union of interiors  of all the components of its overlay  that have such minimum age .  

\subsection{Efficient Sampling from Minimum-Age Interiors}

Now, one of the crucial properties of the minimum-age interior  of a covering family  is that as long as  is shattering and -bounded, we can efficiently sample from the corresponding -marginal. More specifically, in Section \ref{sec:sampling} we establish the following lemma.


\begin{lemma}
\label{lem:conditioning_main}
Given any covering family  that is shattering and -bounded, for some  being polylogarithmic in , let  be its minimum-age interior. We can sample from the corresponding -marginal in (expected)  time.
\end{lemma}

Once we have obtained a sample  from our -marginal, as in the lemma above, it is important to ensure that this sample remains consistent with all the subsequent samplings that we perform. A simple way to achieve this is by encoding this random choice of  into the graph . Namely, given the graph , the subset  of its edges and the sample  from the corresponding -marginal, let us define the {\em -conditioned graph}  to be  after we contract in it all the edges in  and remove all the edges in . It is not hard to see that sampling a random spanning tree of  that is consistent with our sample , i.e., finding a random spanning tree  conditioned on the fact that the intersection of  and  is exactly , boils down to finding a random spanning tree in the -conditioned graph . So, to ensure consistency across all our samplings from consecutive marginal distributions, we just need to make sure to always work with the corresponding conditioned graph. And it is easy to keep track of all edge contractions so as to be able to recover the whole random spanning tree in the original graph at the end.

The key consequence of sampling from the -marginal, with  being the minimum-age interior, is that it allows us to make non-trivial progress, as measured by the structure of the covering family that we maintain. That is, either the age of that covering family increases, or the interiors of all the components of its overlay partitioning become empty (which by Fact \ref{fa:alpha_boundness_and_boundary_edges} means that there are few edges left overall). This is made precise in the following lemma, whose proof appears in Appendix~\ref{app:conditioning_progress}.

\begin{lemma}
\label{lem:conditioning_progress}
Let  be a graph and let  be an age- covering family in  that is -bounded, with its minimum-age interior being . Given a sample  from the -marginal, we can construct, in  time, the corresponding -conditioned graph , as well as a covering family  in that graph. Furthermore, this  will be -bounded and, if , then the age of  will be at least ; otherwise, i.e., if , then each edge of  will be in the boundary  of some set  in .
\end{lemma}
An important technical convention in this context is that the value of  that we use in the definitions of covering family (see \eqref{eq:def_size_bound_covering_family}) and of -boundedness always refers to the number of edges of the {\em original} input graph, i.e., the one before we start removing and contracting edges. (This, of course, means that the running times that we cite in all lemmas are also always proportional to the number of edges of original graph, not the conditioned graph we might be applying them to.)

\subsection{Our Algorithm}\label{sec:algorithm_wrapup}

Having described above all the key ideas and tools behind our approach, we are finally ready to present our algorithm. In fact, at this point, it boils down to a simple iterative procedure.

Namely, our algorithm maintains a graph  and an -bounded covering family  in that graph, for some  being polylogarithmic in . Initially,  is the original input graph and  is the trivial -bounded and age- covering family with  and all other  being empty. 

In each iteration, given  and , we first apply Lemma \ref{lem:partitioning_main} to  to obtain a covering family  that is shattering and -bounded with . Next, we use Lemma \ref{lem:conditioning_main} to obtain a sample  from the -marginal corresponding to the minimum-age interior  of  and then employ the procedure from Lemma \ref{lem:conditioning_progress} (with ) to obtain the corresponding -bounded covering family  in the -conditioned graph . (Note that even though  was shattering, this  might no longer be such, as the removal of edges during construction of  can significantly increase the effective resistance distances.) 

If it is {\em not} the case at this point that all the edges of  are boundary edges of some set in , we just proceed to the next iteration with  being equal to  and  equal to . 

Otherwise, i.e., if indeed all the edges of  are boundary edges for some set in , we can stop our algorithm and note that this condition together with the -boundedness of  and Fact \ref{fa:alpha_boundness_and_boundary_edges} implies that  has at most  edges. (Recall that in our convention  denotes the number of edges of the original input graph.) So, this graph is small enough that we can afford to just simulate the whole covering random walk in it and use Theorem~\ref{thm:rand_tree_via_rand_walk} to recover the corresponding random spanning tree. (Observe that the expected length of this covering walk can be easily bounded in terms of the square of the number of edges of our graph -- see Section~\ref{sec:cover_time_and_effective_resistance} -- which results in a bound of .) 

By combining the edges of this tree with the edges contracted during our repeated constructions of conditioned graphs, we obtain the desired random spanning tree in the original graph. It is not hard to see that the above finishing procedure can be implemented in total  (expected) time, as needed.

Now, to analyze the iterative part of our algorithm, observe that, by Lemma \ref{lem:partitioning_main}, the only operations used to obtain  from  are adding new sets and subdividing the existing ones. As these operations cannot decrease the age of any component in the overlay (but could increase it), we can conclude that the age of  is at least the age of . As a result, Lemma \ref{lem:conditioning_progress} ensures that with each iteration of our algorithm, the age of the maintained covering family  increases by at least one. 

Therefore, there can be at most  such iterations before the algorithm terminates. This observation implies that, on one hand, our maintained covering family  is indeed always -bounded for some  being polylogarithmic in . (This follows since  is initially  and then, by Lemma \ref{lem:partitioning_main}, each iteration increases it by an additive term of at most  that is polylogarithmic.) On the other hand, this observation allows us to conclude that by Lemmas \ref{lem:partitioning_main}, \ref{lem:conditioning_main} and \ref{lem:conditioning_progress}, the total (expected) running time of our algorithm is at most . 

In light of all the above, Theorem \ref{thm:main} follows.



 \section{Cutting the Graph} \label{sec:parititioning}

In this section, we prove Lemma \ref{lem:partitioning_main}. That is, we show how to expand a given -bounded covering family  of the graph  to a covering family  that is still -bounded for  that is not much bigger than  and, furthermore, is shattering, i.e., has its overlay  partition  into components of small effective resistance diameter. (Consult Section \ref{sec:overview} for necessary definitions.)

At a very high level, our approach is a simple iterative procedure that, as long as  is not yet shattering, considers one-by-one all the components  of the current overlay partition  whose effective resistance diameter may still be too large and applies a fixing routine to them. This fixing routine boils down to first identifying good cuts in  that divide the component  and then adding to  carefully-crafted sets (or subdividing the existing ones) based on these identified cuts. (This results in subdivision of  in the resulting overlay  and thus in reduction of its diameter.)

Roughly speaking, the way such good cuts for  are identified is via a combination of two partitioning techniques: one purely combinatorial and the other one relying on a certain new connection between the effective resistance metric and graph cut structure. More precisely, in our fixing routine, we first apply the standard ball-growing primitive, as described in Section \ref{sec:ball_growing}, to . If this results in a partition of  into pieces that have sufficiently small diameter and size (while not cutting too many edges), we use the resulting sets to update  accordingly. On the other hand, if that fails, we show that it must be that  contains two large parts that are far away from each other in effective resistance metric. As we prove then, these parts need to have a relatively small cut separating them in the graph  (see Lemma \ref{lem:good_cut} below) and we (approximately) recover this cut using the recent close-to-linear-time minimum - cut algorithms -- see Section \ref{sec:approximating_minimum_cuts}. Based on that cut, we update  in a very careful manner. 
  
We proceed now to describing our procedure in more detail.

\subsection{The Graph-Cutting Procedure}

Let us first recall the setting of Lemma~\ref{lem:partitioning_main} more formally. We are given an -bounded covering family  and we want to expand it so that it would become shattering and still be -bounded with . In this expanding, we are only allowed to add new sets to  and to subdivide the existing ones.

To facilitate our analysis, we also maintain an additional constraint during our procedure. Namely, for each such subdivision  with , we will only be adding new sets -- never subdividing them. (So, subdivision of existing sets can happen only in the subdivision .) Also, we will never modify subdivision .

Before we start our procedure, we construct, in nearly-linear time, the -approximate low-dimensional embedding  of the effective resistance metric of  -- as described in Theorem \ref{thm:SpielmanSrivastava} in Section~\ref{sec:reff_embedding} -- with . This way, we will be always able to compute quickly (i.e., in  time), for any two vertices  and  in , an approximation  of the effective resistance distance  between them such that




Now, our procedure operates by iterating over all the yet-unseen vertices  of . (Initially, all vertices are unseen.) For each such , we identify the component  in the overlay  of the current  that contains  and run a cutting procedure -- denoted by  -- on it. The objective of this procedure is to subdivide  (by adding new sets to , or subdividing existing ones, in order to refine ) into smaller components, all of effective resistance diameter . At the end of , we mark off all vertices of  as seen. We now describe this procedure.


To this end, let us fix some . We start by applying the ball-growing primitive -- as described by Lemma \ref{lem:ball_growing} in  Section~\ref{sec:ball_growing} -- to it to obtain a -decomposition of  into pieces of effective resistance diameter at most . (Recall that the graph-diameter distance always upper-bounds the effective resistance diameter -- see Section \ref{sec:cover_time_and_effective_resistance}.) For a given piece  in this decomposition, we call it {\em large} if its interior  is of size greater than ; otherwise,  we call it \emph{small}. Let  (possibly ) be all the large pieces and let  be their union. 

We now choose an arbitrary vertex , for each such large , and then use our low-dimensional embedding  to (approximately) compute the effective resistance distances between  and all the other s. (This can be done in  time.) 

If it turns out that at least one of these distances is larger than , then we say that the ball-growing procedure {\em failed} and proceed to our alternative, more sophisticated, cutting method -- described in Section \ref{sec:ball_growing_fails} below -- that deals with this case. This method encapsulates the key advantage of our choice to employ the effective resistance-based optics instead of the usual graph distance-based one. (Intuitively, the failure of the ball-growing procedure corresponds to the graph structures -- such as the graph in Figure \ref{fig:barrier_example} -- that give rise to the  barrier faced by the Kelner-Mądry method \cite{KelnerM09}.)

On the other hand, if ball-growing procedure does not fail, i.e., all these distances  are at most , we are able to deal with the component  relatively easily. To see that, observe first that in this case all the large pieces have to be within a ball of small effective resistance diameter, i.e., we can bound the effective resistance distance  between any two vertices  and  as 

where we used~\eqref{eq:embedding_bound} and the fact that each  has a diameter of at most . In other words, .

Then, we update the covering family  by just adding all the pieces that are {\em small} (not large!) to the subdivision . By definition, all these small pieces are of size at most , as needed -- cf. \eqref{eq:def_size_bound_covering_family}. Also, note that if  and thus  is already contained in some existing , adding these pieces is not valid as they would be contained in . In this case, we just modify  by removing all the pieces from it -- effectively, this corresponds to subdividing  and thus is a valid operation. 

Finally, we note that after the above update, the component  is split into components that correspond to each of the small pieces and the union  of all the large pieces. Each of these components -- in particular,  -- have effective resistance diameter of at most  (see \eqref{eq:bound_diam_large_pieces}). So they satisfy the diameter condition needed in the definition of a shattering covering family -- see Section \ref{sec:shattering_alpha_bounded_families} -- and thus we can mark all the vertices of  as seen and our cutting procedure can proceed to the next unseen vertex. 


\subsection{Cutting the Component  when Ball-growing Fails}\label{sec:ball_growing_fails}

We describe now how to deal with the case when ball-growing fails, i.e., if we have that for at least one large piece  with  that was produced by running the ball-growing primitive on , . Let us assume wlog that . 

At this point, we can forget about all the other large and small pieces that our ball-growing procedure found in  -- from now on we focus exclusively on the large pieces  and . The first observation to make in this situation is that the fact that  implies that  and  are relatively far from each other in the effective resistance metric. In particular, by~\eqref{eq:embedding_bound}, the effective resistance distance  between  and  can be lower-bounded as

where we used the fact that the diameter of  and  is at most . 

Now, the crucial insight here -- and, in fact, one of the key new ideas behind our improvement -- is that distances in the effective resistance metric between vertices are related to the sizes of minimum cut separating them. In particular, as we show in the lemma below, one can relate the fact that  and  are far away in the effective resistance metric to existence of a small cut separating these sets in . (This connection can be viewed as a certain analogue -- for the minimum - cut problem -- of the role Cheeger's inequality \cite{AlonM85,Alon86} and its higher-eigenvalue generalization \cite{KwokLLOT13} play in the context of the sparsest cut problem. In both cases, one is tying certain algebraic properties of the Laplacian -- in the case of Cheeger's inequality, these are its eigenvalues, here it is its full (pseudo-)inverse -- to the quality of the corresponding cut in the graph.)


\begin{lemma} \label{lem:good_cut}
Let  be disjoint sets in a graph  having at most  edges and let  (resp., ) be a vertex in  (resp., in ). There exists a cut  in  that separates  and  and has size at most
 
where we define  and assume it to be strictly positive.
\end{lemma}

\begin{proof}
Let us define . We claim in the lemma that there is a cut of size at most  that separates  from  in . Assume for the sake of contradiction that there is no such cut of size less than . This means that in a graph  which is created from  by contracting  into a vertex  and  into a vertex  there exists no --cut of that size too. As a result, there exist  edge-disjoint paths between  and  in . Once we lift these paths to , we will obtain  paths , with each  connecting a vertex  to a vertex  and being of some length .

Our goal is to derive a contradiction by showing that . (Note that once we use the definition of  and rearrange the terms, this indeed will be a contradiction.) 

To this end, we will use an alternative characterization of effective resistance. According to this characterization, for any two vertices  and , the effective resistance distance  between these vertices is equal to the energy  of the minimum-energy flow  that sends one unit of flow from  to  in . Here, the energy  of a flow  is equal to  and corresponds to  being treated as an electric circuit with all edges being unit resistors. (See, e.g., \cite{LyonsP13,Bollobas98} for a proof of this equivalence.)

In light of the above characterization, we can prove upper bounds on  by bounding the energy of some (not necessarily optimal) flow  that pushes one unit of flow from  to  in . To specify the flow  that we want to use for this purpose, let us first define, for every ,  to be a minimum-energy unit flow from  to . Note that the energy  of  can be at most  -- this is so as, by the above characterization,  is the upper bound on the energy of a minimum-energy unit flow between any two vertices in . Similarly, let  be a minimum-energy unit flow from  to  -- its energy is at most . Finally, let  be the unit flow from  to  that simply pushes the whole flow via the path  (of length ). Observe that, for each , we have . 

Now, our desired flow  is defined as the following convex combination:

Note that each sum of the flows ,  and  gives rise to a unit flow from  to . As a result, their convex combination  is also such a unit flow, as desired.

To bound the energy of , we observe that by virtue of edge-disjointness of the paths  we have

where we also used the fact that , as  has at most  edges.

Next, we note that the energy is a sum of quadratic functions and thus Jensen's inequality implies that for any set of flows  and parameters  we have 
 
Using this, our energy-based characterization of effective resistance and the bound \eqref{eq:bound_on_p_i_energy}, we obtain

which is the desired contradiction.
\end{proof}

Once we have established the above lemma, we can apply it in our context and notice that, by \eqref{eq:before_good_cut_lemma}, , which gives us that there exists a cut  in  that separates  from  and is of size at most . As a result, by contracting  and  into single vertices  and  and then using the close-to-linear time approximate minimum - cut algorithms -- see Section \ref{sec:approximating_minimum_cuts} -- on the resulting graph, we can recover, in total  time, such a cut  that separates  from  in  and is still of size at most . (It is worth noting here that the cut  is a global cut in , i.e., it is not contained in the component  that we are processing and, in fact, it can intersect all the sets in our covering family .)  

Our goal now is to use this cut  to carefully craft a new set  to be added to one of the subdivisions  for . Adding this set will ensure that  and  become separated in the resulting new overlay . To achieve this, we need to ensure, in particular, that, on one hand, this set does not intersect any existing sets in  and, on the other hand, that it does not contain too many boundary edges. As the following lemma (proved in Appendix~\ref{app:massaging}) shows, such a set  can indeed be found and, additionally, it satisfies all the other constraints that we imposed on our algorithm. 

\begin{lemma} \label{lem:massaging}
Let  be a covering family of a graph , and let  be a component of its overlay partition . Also, let  and  be two subsets of  satisfying . Given a cut-set of edges  which separates  and  in , we can, in  time, produce a set of vertices  and an  such that:
\vspace{-7pt}
\begin{enumerate}[(1)]\addtolength{\itemsep}{-.5\baselineskip}
	\item , \label{cond:i_0_ell}
	\item the size of the interior of  satisfies , \label{cond:size}
	\item  is disjoint from all sets in subdivision , \label{cond:disjoint}
	\item exactly one of the sets ,  is contained in , and the other one is disjoint from , \label{cond:separation}
	\item every boundary edge  is either a boundary edge of  with  or it comes from the cut , i.e., . \label{cond:boundary_edges}
\end{enumerate}
\end{lemma}

At this point, we can use Lemma~\ref{lem:massaging} above to produce the set  that we then add to the subdivision  with  (this is valid thanks to conditions \eqref{cond:disjoint} and \eqref{cond:size}).
As a result of this update, in the new overlay , the component  has been separated into two components  and  (see condition~\eqref{cond:separation} in the Lemma \ref{lem:massaging}), both of which might still have too high effective resistance diameter. For this reason, we still need to recurse on both these pieces, i.e., we call  and . As we will shortly see, even though our whole cutting procedure that added this new set  was computationally very expensive, i.e., it took  time, which might be very large compared to the size of , we can justify this cost by proving that we do not need to run this procedure too often. (Roughly speaking, this stems from an observation that the sparsity of the cut that  constitutes is relatively small and thus an appropriate charging argument can be applied.)

\subsection{Analysis of the Cutting Procedure}

We now proceed to the analysis which shows that our cutting procedure fulfills all its promises.

Observe that, from the point of view of the running time analysis, our main concern is that whenever ball-growing fails, the runtime of the cutting procedure handling this situation is as large as . Fortunately, as the following lemma shows, this does not happen too often. 

\begin{lemma} \label{lem:number_of_insertions}
The number of times a new set is inserted into  is less than  for .
\label{lem:number_of_insertions_of_set}
\end{lemma}

To see how this provides a bound on the total running time of the cutting procedure from Section \ref{sec:ball_growing_fails}, recall that these expensive cut computations are always followed by an insertion of a new set into  with . By the above lemma, summing over all , we have a total of at most  of such insertions. The resulting running time bound is , as desired.

\begin{proof}[of Lemma~\ref{lem:number_of_insertions}]
Note that a new set  may be inserted into  with  only when ball-growing fails, i.e., only during the cutting procedure described above in Section \ref{sec:ball_growing_fails}. By condition~\eqref{cond:size} of Lemma~\ref{lem:massaging}, the size of  satisfies . Moreover, by condition \eqref{cond:disjoint}, it is disjoint from all other sets in , thus any edge of  can be covered by a set from  only once. So, as the insertion of one set  covers more than  new edges, there must be less than  such insertions.
\end{proof}

The rest of the running time analysis and the adequacy of our cutting procedure is summarized in the following two statements, whose proofs may be found in Appendices \ref{app:partitioning_running_time} and \ref{app:partitioning_alpha_boundedness}, respectively.

\begin{lemma} \label{lem:partitioning_running_time}
Our whole cutting procedure runs in  time.
\end{lemma}

\begin{lemma} \label{lem:partitioning_alpha_boundedness}
The new covering family  is -bounded with .
\end{lemma}


We are now in a better position to understand the idea behind the notion of -bounded covering families. Namely, if we have a set  that we want to insert into our covering family , we should strive to do so at the lowest level possible (i.e., the lowest level  where the condition  is satisfied). By doing so, we will guarantee the -boundedness of the resulting covering family (as above), which encodes the trade-off between the number of boundary edges on any level  and sizes of sets on that level (see the discussion in Section~\ref{sec:shattering_alpha_bounded_families}). For instance, if we have a large set , we should place it on a low level  of the covering family; this, through the -boundedness property, guarantees that there will be few boundary edges on that level (and such edges are expensive, as each one incurs a preprocessing time cost proportional to the maximum size of sets in  -- which is roughly the size of ). On the other hand, we should place small sets on high levels of the family; because they are small, we can afford to have many more boundary edges incident to them.
In this way, one can view the -level covering family as a way of making sure that we have a right level  on which to insert any set  that we might want to use for shortcutting our random walk.
 \section{Fast Sampling from Marginal Distributions} \label{sec:sampling} 

In this section, we prove Lemma \ref{lem:conditioning_main}. That is, we show how, given a covering family  in a graph  that is shattering and -bounded, for some  being polylogarithmic in , one can sample from the minimum-age interior  of  in (expected)  time. (See Section \ref{sec:overview} for all the necessary definitions, and recall that in our convention  refers to the number of edges of our original input graph and thus our graph  here can have potentially much less than  edges.)

As the covering family  that we will be dealing here is fixed, let us simply use  to denote the overlay partition  of  and fix  to be the age of , i.e., the age  of a minimum-age component  in . Also, let us denote by  the collection of all such minimum-age components  of , and make  be the union of the vertices of all the components in . 

Our goal is to sample from the -marginal distribution of  (cf.~Section \ref{sec:marginals_overview}). This means that we want to sample a random subset of  according to the distribution that returns  with probability 

where  denotes the set of all spanning trees of . In other words, we want to find the intersection of  with a random spanning tree.

As we already discussed, in light of Theorem \ref{thm:rand_tree_via_rand_walk}, this could be done by, first, simulating the random walk starting at some vertex  until all the first-visit edges  to  are known and, then, returning the intersection  of  and all the edges  found. 

Unfortunately, even though we do not need to learn here what the first-visit edges  are for all vertices of  -- only the ones that could be in , the expected length of the necessary random walk might still be comparable to the cover time of the whole graph. Therefore, we cannot afford to simply execute this simulation. 


\subsection{Shortcutting}\label{sec:shortcutting}
To cope with the above-mentioned problem and obtain the desired efficient sampling procedure, we build on the approach developed by Kelner and Mądry \cite{KelnerM09} to sample from the whole random spanning tree distribution. 

Namely, the key observation to make here is the following: even though the transcript of the whole walk defining the first-edges  that we need, might be quite long, i.e., even , in the end, we only utilize at most  of its entries. Therefore, one might hope that there is a way of directly generating a  shortcut transcript of that walk, that is much shorter (and, hopefully, possible to generate much more efficiently) while still retaining the information we need.

To get an intuition how that shortcutting could be implemented, let us consider some subset  of vertices of . (For now, one can think that  corresponds to one of the components of .) Imagine that we are faithfully simulating the random walk in  step-by-step, except that once we cover  we start shortcutting our trajectories inside this set. Namely, whenever we enter some vertex  after  was already covered, we would like to immediately leave  by jumping out over one of the edges of its boundary . Observe that by doing so we are not losing any information that we need later -- at this point, we already know all the first-visit edges  for all the vertices in . So, the only challenge here is to ensure that the edge through which we leave  is sampled consistently with the distribution induced by the random walk.  

More precisely, to implement the above shortcutting strategy, we need to be able, for each vertex , to sample from the distribution , where  is the probability that a random walk started at  exits  for the first time via the edge . 


Fortunately, as shown by Kelner and Mądry \cite{KelnerM09} and Propp \cite{Propp10}, by exploiting the connection between random walks, electrical flows and the fast (approximate) Laplacian system solvers \cite{SpielmanT03,SpielmanT04,KoutisMP10,KoutisMP11,KelnerOSZ13}, one can indeed construct a data structure -- from now on, called {\em -shortcut sampler} -- that allows one to quickly sample from such edge-exit distribution. Moreover, as made precise in the lemma below, this construction is efficient provided that the size of the boundary of  is relatively small compared to the size of its interior. 

\begin{lemma}\label{lem:cost_shortcutting_sampling}
Given a subset , we can construct a data structure -- called {\em -shortcut sampler} -- that allows one to sample, for a given vertex , an edge  according to the distribution  in  time. The total expected time spent on constructing and maintaining this data structure is .
\end{lemma}
This lemma follows from a simple adaptation of the techniques of Kelner and Mądry (see Lemma 7 and 9 in \cite{KelnerM09}) and the observation of Propp \cite{Propp10} (also see Lemma~7.3.2 in \cite{Madry11}). We sketch its proof in Appendix~\ref{app:cost_shortcutting_sampling}. An interesting feature of this construction is that it never computes the exact values of the probabilities  (this is a consequence of fast Laplacian system solvers being only approximate). Nonetheless, it still allows one to obtain an {\em exact} sample from the corresponding distribution.

\subsection{Our Sampling Algorithm}

Having introduced the shortcutting technique, we can proceed to describing our sampling algorithm. For now, we do it assuming a (technical) condition that the overall effective resistance diameter  of our graph is relatively small, i.e., . Later, in Section \ref{sec:deadling_with_large_diameter_case}, we deal with this assumption. 

Recall that in \cite{KelnerM09}, Kelner and Mądry obtain their shortcut transcript of a covering random walk in  by, first, finding a simple ball-growing-based partition of the graph into pieces that have small graph diameter and, then, running simulation of the walk in  while implementing the shortcutting procedure (as described above) for each such piece. As this partition cuts relatively small number of edges and -- due to their small diameter -- the individual cover time of each piece is small too, they are able to recover the whole random spanning tree within their claimed running time bounds. 

Given all of that, it might be tempting to follow this approach also in our case. Namely, we could simulate a random walk in  until it covers all the vertices in , while applying the shortcutting procedure to each component of the overlay partition  of our covering family . In this case, we know that all these components have small effective resistance diameter (as  is shattering). So, it follows (cf. Section~\ref{sec:random_walks_restricted_to_subgraphs}) that the individual cover time of each is sufficiently small, as needed. Furthermore, one might hope that the fact that  is -bounded implies that the size of boundaries of each of these components is also sufficiently small (compared to their size) and thus the construction times of corresponding -shortcut samplers (see Lemma~\ref{lem:cost_shortcutting_sampling}) are acceptable. 

Unfortunately, in general, this is not the case. It might actually happen that the sizes of boundaries of the components of  are prohibitively large and thus we are still hitting here the same bottleneck as the original Kelner-Mądry approach faces -- see our discussion in Section \ref{sec:our_approach}. (In fact, the graph from Figure \ref{fig:barrier_example} is exactly the example showing that.)

In light of the above, we have to design our shortcutting strategy in a different and much more careful way. This will crucially exploit our lack of need of knowing what the first-visit edges are for the components of  that are not in . Namely, the key realization here is that if all we care about is preserving the first-visit information only for the components of , there is much more flexibility in the way we implement the shortcutting of our walk. In particular, for a given component  of  we do not really need to take  and construct the corresponding -shortcut sampler (cf. Lemma~\ref{lem:cost_shortcutting_sampling}); it suffices to find some different -- {\em larger} set  that, on one hand, contains  and has small number of boundary edges and, on the other hand, does not overlap any other component in  (but still can overlap components that are components of ). In fact, a similar observation applies also to other components of  (that might not be in ).

To make this precise, for a given component  of the overlay  (that might or might not be in ), let us call a set  an {\em escape set} for  iff  and  does not intersect any component of  other than  (if  is in ).

Now, it turns out that once we find for each component  its escape set  which is not too big and has sufficiently small boundary, we can use these sets to generate a shortcut version of our random walk. This shortcutting is based on applying a procedure that, whenever our walk enters a vertex  belonging to some component  of , does the following:
\begin{itemize}
	\item if :
	\begin{itemize}
		\item if  has already been covered, perform a jump out of ;
		\item otherwise: faithfully simulate a single step of the walk;
	\end{itemize}
	\item otherwise: perform a jump out of .
\end{itemize}
We stop the random walk at the moment we have covered all components . Note that this procedure always faithfully simulates the walk inside a component from  until this component is fully covered. Also, whenever we perfom a jump over an escape set and land directly inside a component of , we know what the last traversed edge (through which we entered this component) was. So this shortcutting still retains all the information needed to recover the sample from the -marginal.

Now, of course, the crucial part is choosing the sets  appropriately, so that the resulting running time of our simulation is within the desired bounds. This is the aim of the next section. 

\subsection{Choosing Appropriate Escape Sets}
Our goal here is to define the escape sets  for  that we use for shortcutting the walk, as described above. To this end, let us introduce a new piece of notation.

For a given , let us define  to be the partition resulting from superimposing on top of each other all the sets from subdivisions . (So, we have that ,  and each  is a coarsening of .)

 Furthermore, for a component , let us define  to be the component of  that contains . Finally, we say that a component  of  is \emph{modest} if it is contained in a set . The reason why we want to distinguish modest components is because they are small enough for our purposes. Namely, if  is modest on level  and , then , which might not be true for an arbitrary component.

\begin{lemma}
\label{lem:different_Pp_components}
Let  and  be different components of  with . Then they belong to different components of : .
\end{lemma}
\begin{proof}
Since , for some  there must exist a set  such that exactly one of  is contained in . But because of age, neither  nor  is contained in any set  for . Therefore, .
\end{proof}

We infer from the lemma that  and  are disjoint and are both modest whenever  are two different pieces in . It follows that  satisfies all the required conditions for an escape set of , hence we define . 

For the remaining components  we choose  -- the witness set of . Then we have ,  and . So clearly, none of the components in  intersects .

Let us now look more closely at the exceptional situation when . One may see that in this case there is exactly one component in  of age , say . It appears that according to our definition, , which makes little sense (we cannot jump out of ). Fortunately, in this special case we stop the walk immediately after the single component gets covered. This means there is no use for the  set. Note that this issue concerns only the border case ; if , every  is a proper subset of  and has some boundary edges through which we may jump out of it.

In Section~\ref{sec:analysis_sampling} we will see that such a choice of escape sets allows us to prepare all -shortcut samplers in reasonable time. Now we wish to proceed to the runtime analysis of our entire sampling procedure. We start with some preparations in the next section.

\subsection{Random Walks Restricted To Subgraphs} \label{sec:random_walks_restricted_to_subgraphs}

In this section we state a lemma which will be instrumental to our analysis of the random walk simulation. It bounds the expected length of a random walk which covers a given subgraph.

\begin{lemma} \label{lem:ultimate}
Let  be an unweighted graph and . Also let  be any set of edges inside . Suppose we run a random walk in  from an arbitrary vertex (not necessarily from ) and stop it once all vertices from  have been visited. Then the expected number of traversals of edges from the set  is 
\end{lemma}

We prove this lemma in Appendix~\ref{app:proof_of_ultimate}. It may be viewed as a generalization of Lemma~\ref{lem:cover_weighted} in two aspects. First, it lets us look only at traversals of a subset of edges of the graph (for example, in the analysis of our algorithm this might be the set of boundary edges of ). Second, we can restrict our attention to a subgraph induced by a vertex subset  and answer the question: what is the {\em subgraph cover time}, i.e., how many traversals of edges between vertices of  will there be if we decide to stop the walk as soon as  is covered? 

Note that this improves upon Lemma~6 of~\cite{KelnerM09}, not only by considering  rather than , but also by dropping the assumption that the number of boundary edges of  is at most , and by letting us only consider a subset of edges. It also subsumes Fact~5 of~\cite{KelnerM09}. We thus see that both directions of generalizing the upper bound of  for the cover time were known previously, but in weaker versions, separately and not using effective resistance. We believe that Lemma~\ref{lem:ultimate} is a robust and useful statement which is of independent interest.

We now apply the above lemma to analyze our algorithm.

\subsection{Analysis of the Sampling Algorithm}\label{sec:analysis_sampling}

If we recall our simulation of the random walk that we described in the previous section, there are two main ingredients in the running time of that procedure: the expected time needed to prepare the data structures for implementing shortcutting as in Section \ref{sec:shortcutting}, and the expected running time of the actual simulation of the shortcut random walk. Below, we analyze both of these separately.


\begin{lemma}\label{lem:preparation_cost}
The total expected time spent on preparing the structures for shortcutting sets  for  is .
\end{lemma}
\begin{proof}
To initialize our data structures, we need to construct the shortcut samplers for all components in  for  and modest components of . By Lemma \ref{lem:cost_shortcutting_sampling}, computing and maintaining a shortcut sampler for a set  can be done in time , where  is the set of boundary edges of .

Consider first all sets , for some . By Fact~\ref{fa:alpha_boundness_and_boundary_edges} and the bound , the time needed to maintain the corresponding data structures is at most

Then, summing this time over all , we obtain that the overall time is  as well.

Secondly, let us consider all modest components . First notice that  is a boundary edge of  iff it is a boundary edge of  for some . Hence, the number of such edges is, by Fact~\ref{fa:alpha_boundness_and_boundary_edges},

as well. Also, for a modest component  there exists an  with , which implies that . As a result, we can see that the previous calculation (that we made for the sets ) holds also in this case. We conclude that the total time spent on maintaining shortcut samplers is , as desired.
\end{proof}


\begin{lemma}\label{lem:walking_cost}
The expected time of performing the shortcut random walk simulation is .
\end{lemma}

\begin{proof}
Observe that there are two types of steps in our procedure:
\begin{enumerate}[(a)]
\item regular, single steps inside some age- component, before it is covered,
\item jumps over a  set (for some ) or over a component in .
\end{enumerate}
Consider a component . The expected time spent on steps of type (a) within  is bounded by  by Lemma~\ref{lem:ultimate} (used with  and ). Since the effective resistance diameter of  is , this is . Thus, the expected total time spent on type-(a) steps is 


Let us now concentrate on steps of type (b). Recall that every step of this kind is carried out using one query to the corresponding shortcutting structure and thus it takes  time. Each such jump ends on a boundary edge (either of some  set or some component in ). So we can think that every jump corresponds to a boundary edge traversal in the underlying unshortcut random walk.

 We can now bound the time required for type-(b) steps simply by the expected number of times a boundary edge is traversed during our simulation. Note that by the time the whole graph is covered for the first time we are certain to be done with our simulation. Therefore we may use Lemma~\ref{lem:ultimate} to bound the time spent on type-(b) steps. We set  to be the set of all boundary edges, , and we recall that  (the small-diameter assumption) and . Thus we obtain the desired bound of . 

\end{proof}

By Lemmas~\ref{lem:preparation_cost}~and~\ref{lem:walking_cost} the total expected running time of sampling (in the small-diameter case) is .

\subsection{Dealing with the Large-Diameter Case}\label{sec:deadling_with_large_diameter_case}

Recall that so far we were assuming for simplicity that the effective resistance diameter of  is small, i.e.,  . We will show how to deal with this assumption by slightly changing our algorithm. 

We have a graph  and a collection of disjoint subgraphs . We intend to sample a set according to the -marginal distribution, where . 
We already know how to simulate the random walk in  in such a way that we pay only for:
\begin{enumerate}[(a)]
\item traversing interior edges of every component  until it is covered,
\item traversing edges coming from a set  of size .
\end{enumerate}
As seen in the proof of Lemma~\ref{lem:walking_cost}, the time complexity of (a) is  irrespective of , but the time complexity of (b) is , which is too much if we are not able to bound the diameter of  by . To circumvent this problem, we use the ball-growing procedure to cut  into small-diameter pieces (cf.~\ref{sec:ball_growing}). Namely, we obtain a partition  satisfying  and , where  is the set of boundary edges of this partition. Take  to be the set of all boundary edges (of all subdivisions ). By -boundedness of  we have .

In the new algorithm, whenever we cover all interesting vertices (those from ) within some , we start shortcutting the walk over the whole . This allows us to treat every edge  as part of a small-diameter graph  and pay for traversing it only until  is covered. In the next subsection we make this intuition formal.

We now describe the strategy used to simulate the random walk. Suppose we are at a vertex  and want to make a step in our simulation. Let  be the small-diameter piece that  belongs to. Then:
\begin{enumerate}
	\item if all vertices in  have already been covered: jump over  in one step,
	\item otherwise:
	\begin{enumerate}
		\item let  be the component in  such that ,
		\item if :
		\begin{enumerate}
			\item if  is covered: jump over the  set,
			\item else: perform one regular step of the random walk,
		\end{enumerate}
		\item else: jump over .
	\end{enumerate}
\end{enumerate}
Note that case 2. is exactly the same as our simplified strategy from the previous subsections.


\subsection{Analysis of Sampling in the Large-Diameter Case}
As in the small-diameter case, we need to bound two quantities: time spent on preparing the shortcutting structures and the expected time of simulation.
\begin{lemma}\label{lem:preparation_cost_general}
The total expected time spent on preparing the structures for shortcutting sets  for , modest components of  and  for  is .
\end{lemma}
\begin{proof}
An additional part compared to the simplified case are the s. The expected time needed to maintain all the shortcut samplers for them is (use Lemma~\ref{lem:cost_shortcutting_sampling}): .
\end{proof}

\begin{lemma}\label{lem:walking_cost_general}
The expected time of performing the random walk simulation is .
\end{lemma}
\begin{proof}
We remarked previously that the expected time spent on covering 's is bounded by . Thus we only need to estimate the expected number of jumps: those from step 1. and those from step 2. Like in the small-diameter case, we argue that each jump in 1. corresponds to an -edge traversal. Since , we may easily bound the total number of -edge traversals during a covering walk of  by  (use Lemma~\ref{lem:ultimate} with  and ). Similarly, for step 2., each jump ends with an -edge traversal, so it remains only to deal with jumping over -edges. There are  of them, and each lies inside a component .

Fix  and let . Our aim is to bound the number of -traversals until  is covered. This will also be a bound for the number of jumps ending with an -edge. To this end, we use Lemma~\ref{lem:ultimate} with  and the edge set being . Because , this yields a bound of .

Summing this over all 's we obtain that the total time spent on -edge traversals caused by jumps is at most . This completes our argument. 
\end{proof}

The two lemmas above let us conclude that the sampling can be performed in total expected time . \bibliographystyle{abbrv}
\bibliography{files/references}
\appendix
\section{Appendix}

\subsection{Proof of Lemma~\ref{lem:cover_weighted}}\label{app:cover_weighted}

We prove the lower bound only; the upper bound is a special case of Lemma~\ref{lem:ultimate}.

Let us denote by  the expected number of steps before a random walk starting at  visits . We have  and hence . Now one just needs to perform the simple estimation that

where the first inequality follows as covering a graph  starting from  requires hitting all the other vertices first, and the first equality is by definition of effective resistance -- see Section \ref{sec:effective_resistance}.

\subsection{Proof of Lemma~\ref{lem:ball_growing}}\label{app:ball_growing}

We employ here an adaptation of the partitioning procedure from \cite{KelnerM09} that is itself based on the ball-growing technique of Leighton and Rao \cite{LeightonR99}. 

For a subset of vertices  let us define the set of neighbors  to be . Also recall that  denotes the set of edges with exactly one endpoint in .

We start with the original ,  and perform the following procedure until  is empty:
\begin{itemize}\addtolength{\itemsep}{-.5\baselineskip}
	\item choose any vertex  and set ;
	\item while  do: set ;
	\item add  as a new ;
	\item remove  and all incident edges from ;
	\item .
\end{itemize}
Obviously  is a partition of . One can also easily prove that the condition (2) in Definition \ref{def:ball_decomp} is satisfied.  It remains to show that each piece has a small diameter, i.e., establish condition (1). 

To this end, note that the radius of  at the moment it is added as a new piece is bounded by the number of the while-loop executions. We also see that each execution increases  at least by a factor of . This means that after  steps, . Since  for , we deduce that the radius of  is bounded by  and thus the diameter of  is at most . It is not hard to see that the presented algorithm can be implemented to run in  time.

\subsection{Proof of Lemma~\ref{lem:conditioning_progress}} \label{app:conditioning_progress}

It is not hard to see that given  and , the construction of the corresponding -conditioned graph  can be done in  time. Also, note that as edges of  (and thus also edges of ) are always interior edges of components of the overlay  of the covering family , removing or contracting them translates to removing or contracting the interior edges of the sets in . So, doing that (which can again be performed in  time, as each edge can be in at most  different sets) makes the resulting covering family  remain valid for  and still -bounded. (As mentioned before, the value of  used in the definition of the covering family and -boundedness always refers to the number of edges of the original graph and thus remains unchanged here.)

Now, observe that the only difference between the overlay  of  and the overlay  of  is that each minimum-age component  of  has its interior edges removed and/or contracted in . This means that each such  corresponds to a minimum-age component  in  that is a collection of vertices with empty interior (the only edges incident to these vertices are the boundary edges of , which now also form the boundary of ). 

So, if the age  of  (and thus that of ) was equal to , then all the components of  must have been minimum-age and thus all of them have empty interiors now. Given the way the overlay components are defined, this means that each edge remaining in  is part of the boundary of some set in . Thus, in that case, we simply take  equal to  and the lemma follows. 

We can, therefore, restrict ourselves to the case when  is actually strictly smaller than . In this case, let us take  to be a covering family in  that arises from adding, for each vertex of each minimum-age component  of , a singleton set containing only that vertex to the upper-most subdivision  of . (Note that as  and  is minimum-age, it does not intersect any of the sets in  and thus adding these singleton sets to  is valid.)

Observe that performing this operation results in each component  being split into single-vertex sets in the overlay  of  and each of these sets being of age . So, as , this increases the age of all the minimum-age components of  and thus the age of  is at least  now.

In light of the above, all that remains to be done is to argue that  is still -bounded. To this end, note that adding our singleton sets does not introduce any new boundary edges. This is so since, as we already noted above, the only edges incident to vertices of minimum-age components  of  were boundary edges. Furthermore, as these singleton sets are added to , it cannot affect these edges' status as -based boundary edges. That is, if such an edge was an -based boundary edge in  then it remains such in . Thus, as the -boundedness property boils down to bounding the number of -based boundary edges for each , it must be preserved by our operation and thus the covering family  is indeed -bounded, as desired.

\subsection{Proof of Lemma \ref{lem:massaging}} \label{app:massaging}

The cut  separates  into connected components (possibly more than two) -- some of them intersect , some intersect  and some intersect neither (but none can intersect both). Let  be the union of those that intersect  and  be the union of the others; we thus obtain a decomposition  of  where ,  and all edges between  and  come from the cut , i.e., .

Our plan will be to choose the set  as a subset of  of at most half its size and thus set  to be larger than . (Recall that  denotes the age witness of  -- see Section \ref{sec:marginals_overview}.) Because no set in  with  intersects , this will make it easier for us to guarantee condition~\eqref{cond:disjoint}.

So let us define ,  and assume wlog that . 
Since  and  are disjoint and , we get that

where we used the bound \eqref{eq:def_size_bound_covering_family}.

The set  is our candidate for the set . We will now modify it appropriately (by shrinking) in order to satisfy all the required conditions.

Recall that we want to enforce . Let  be the right level for , that is, the smallest  such that . By~\eqref{eq:fresh_color} we know that . We would ideally like to return  as a candidate for insertion into . However,  may intersect some sets in , which violates our condition~\eqref{cond:disjoint}. To deal with this problem, note that since , no set in  intersects  (or ), so we can remove the intersection of  with all the sets in , and we still have that . 

Of course, now -- after removing this intersection with  -- the size  of the interior of  might have decreased considerably.  still contains , which is a large piece, so it cannot be too small, i.e., it has size at least , but it might no longer hold that . If this indeed happens, then we appropriately increase , and repeat.

The resulting set-refining routine works as follows. We start with  and then:
\begin{enumerate}[(a)]\addtolength{\itemsep}{-.3\baselineskip}
	\item if , increment  and repeat;
	\item otherwise, if the intersection of all sets in  with  is nonempty, remove it from  and repeat (starting from step (a));
	\item otherwise, we can stop and output the desired  and .
\end{enumerate}

Let us now verify the satisfaction of all conditions. Conditions~\eqref{cond:size}, \eqref{cond:disjoint} and \eqref{cond:separation} are clear given the design of our set-refining routine. To establish condition~\eqref{cond:i_0_ell}, notice that  because even after all the removals,  still contains , hence . Also .

Regarding condition~\eqref{cond:boundary_edges}, observe that all boundary edges of  are either boundary edges of  (and they come from the cut ), or arise as a result of intersecting  with the age witness  and taking a set difference with some number of sets from subdivisions  with . In the latter case, they are already boundary edges of  on the respective levels  and .

\subsection{Proof of Lemma \ref{lem:partitioning_running_time}} \label{app:partitioning_running_time}

Note that by Theorem \ref{thm:SpielmanSrivastava}, the construction of our low-dimensional embedding  can be carried out in  time.

Let us now consider the runtime of the  procedure for one component . If the ball-growing does not fail, then, using Lemma \ref{lem:ball_growing}, we can bound this running time by . On the other hand, if ball-growing fails, then this running time is equal to  plus the time needed to execute the recursive calls to  and . 

Therefore, to bound the total running time , we model it using a binary tree whose leaves correspond to ball-growing succeeding and internal nodes describe handling ball-growing failures and subsequent recursive calls. By summing the contributions of all the nodes of this tree, we can conclude that the total time spent on all calls to the  procedure is at most
 where  is the total number of cuts computed during handling ball-growing failures (the sum is taken over the final overlay ).

Now, every time we compute a cut, we end up adding a new set to  with . By Lemma~\ref{lem:number_of_insertions_of_set}, this can happen at most  times. Hence we get a runtime of .

\subsection{Proof of Lemma \ref{lem:partitioning_alpha_boundedness}} \label{app:partitioning_alpha_boundedness}

We need to prove that, for every , the number of -based boundary edges is at most  with . To this end, we will show that the number of new -based boundary edges (ones that were not present in the original ) is .

\textbf{Case :} every new -based boundary edge is the boundary edge of a new set  (added in the cutting case). Therefore we only need to bound the number of -based boundary edges introduced by all new sets . If we can prove that adding a new set  introduces at most  new -based boundary edges, we will be done by Lemma~\ref{lem:number_of_insertions_of_set}. But condition~\eqref{cond:boundary_edges} of Lemma~\ref{lem:massaging} means exactly that all new -based boundary edges of such a set  must come from the cut  of the phase when  was added. And recall that Lemma~\ref{lem:good_cut} applied to the components  and , of low diameter  and high effective resistance of  between them, gave us exactly the bound  for the cardinality of the cut , as computed in~\eqref{eq:before_good_cut_lemma}.

\textbf{Case :} every new -based boundary edge is a boundary edge of a new small piece (added when ball-growing succeeds). During the entire partitioning procedure, all the calls to the ball-growing primitive that introduce new -based boundary edges concern disjoint subgraphs  of . Each such call yields a set of cut edges of size , so all the calls result in at most  cut edges.


\subsection{Proof of Lemma \ref{lem:cost_shortcutting_sampling}}\label{app:cost_shortcutting_sampling}

Let us first note that if the interior  of  has more than one connected component, then the problem decomposes into completely independent subproblems corresponding to each of these connected components. So, from now on, we can assume that  forms a connected graph and, in particular, . 

Fix  (with  and ) and suppose we want to calculate the probabilities  for all . Let us construct an auxiliary graph  as follows. The vertex set is , where  is a dummy vertex. We include in  all edges from  and the edge ; we also include all boundary edges of  (those going from  to ) except , but we change their endpoint that originally belonged to   to be  now.

Now, it is not hard to see that  is equal to the probability  that the random walk in this auxiliary graph starting at  will visit  before hitting . The latter probability turns out, however, to be closely related to the electrical structure of the graph . 

More precisely, let us treat  as an electrical network, where every edge has unit resistance. It is known (see Chapter~4 in~\cite{Lovasz93}) that when we impose a voltage of  and  at  and  respectively, the voltage at  becomes exactly  (and thus ). 

Furthermore, this probability  can be computed by solving a Laplacian linear system of the form , where  is the discrete Laplacian of . In fact, in this way we can get such probabilities  for all possible starting vertices  simultaneously. For details, refer to Lemma~9 in \cite{KelnerM09}.

Therefore, we see that computing all the probabilities  boils down to solving a Laplacian linear system  for all the graphs  corresponding to each edge .

As shown first by Spielman and Teng~\cite{SpielmanT04} (see also \cite{KoutisMP10,KoutisMP11,KelnerOSZ13}), solving such a system is possible in time that is nearly-linear with respect to the number of edges in . However, this method does not provide us with exact values of 's -- it outputs only approximate values with prespecified precision . So, in our case, for a given , the total computation takes  time and computes the values of all s up to an additive error of .

Now, suppose for a moment that all these computed probabilities were exact. Then, for a fixed , it is easy to build a simple array structure which allows us to sample edges from  with the desired distribution. It boils down to constructing first an array of prefix sums of size  and then, whenever a sample is needed, performing  a simple binary search in this array, which takes time . (See Lemma~7 in \cite{KelnerM09} for details.)

Finally, the way of coping with the precision issue is based on an idea due to Propp \cite{Propp10}. Here, we only explain the idea on a simpler example and refer the reader to Lemma~7.3.2 in~\cite{Madry11} for a complete description. 

In this example, assume we want to take a sample from a -biased Bernoulli distribution, i.e., sample a - random variable that is  with probability  and  with probability . If we knew the value of , we could perform this sampling by simply choosing a value  uniformly at random and then outputting  if  and  otherwise.

Assume now that instead of the exact value of  we only have access to an algorithm which, given , outputs  with precision  in time proportional to , for some . To perform our sampling in this case, we again choose a uniformly random  and then compare it to the current approximation  of the value of , for some . 

Observe that as long as  or , we can simply output  and  respectively, as we are certain that  or, respectively, . Now, in the remaining case of , we simply keep halving the value of  (and computing the corresponding refinement of our approximation to the value of ) until we are able to conclude that either  or  (in which case we can output an appropriate value).

Clearly, this procedure terminates (with probability ) and outputs the correct sample. It thus remains to analyze the expected time spent on recomputing . To this end, note that the probability that during our sampling the required precision will be less than  is at most . So, the expected time spent on recomputing  is bounded by

Similarly, one can easily show that if we needed  samples (instead of only one) then this expected time would become .

\subsection{Proof of Lemma~\ref{lem:ultimate}} \label{app:proof_of_ultimate}

Let us define the hitting time  between two vertices  to be the expected number of steps before a random walk starting at  visits . We have .

We break the proof down into a sequence of lemmas.

\begin{lemma}[cf. Lemma~2.8 of~\cite{Lovasz93}] \label{lem:LovaszCoverLog}
Let  be a subset of vertices,  -- a~starting vertex, and let  be the maximum hitting time between  and a~vertex in . Suppose we run a random walk from  until more than half of the vertices from  are visited. The expected length of this walk is at most .
\end{lemma}
\begin{proof}
We assume first that  is odd. Let  be the time when vertex  is first visited. Then the length of the walk, , is the -th largest of the . Hence

and

If  is even, we proceed analogously and obtain  at the end.
\end{proof}

\begin{lemma} \label{lem:preUltimate}
Let . Suppose we run a random walk in  from a vertex  and stop it on the first visit to  after all vertices from  have been visited. Then the expected number of steps is 
\end{lemma}
\begin{proof}
The proof is similar to that of Theorem~2.7 of~\cite{Lovasz93}. We divide the walk into phases. The first one ends when more than half of vertices from  have been visited. By Lemma~\ref{lem:LovaszCoverLog} (with ), we know that its expected length is at most  
(recall that we defined  to be ).

The second phase ends when at least half of the remaining vertices of  have been covered, and the lemma applied to that set of vertices (and the starting vertex  being the vertex where the first phase ended) again bounds the expected length of the second phase by the same quantity. (It is important here that , as the first phase must have ended with a visit to a vertex of .) We proceed in this fashion until  is covered. By linearity of expectation, the expected length of the walk up to this point is  times the number of phases, which is clearly . (Note that the number of phases is a fixed quantity here.) Finally, the remaining (expected) time needed to return to  from the last visited vertex of  is at most .
\end{proof}

Now, we will argue that every edge in  has exactly the same contribution to the expected length of the random walk of Lemma~\ref{lem:preUltimate}. Intuitively, this is because the (infinite) stationary random walk on  (where, in the long run, every edge has the same contribution) can be viewed as an (infinite) sequence of samples from the trajectory of our random walk (plus an initial fragment of bounded expected length). Thus, if any edge contributed more than a -fraction to the length of our walk, then this discrepancy would also emerge in the stationary walk, which is a contradiction.

We proceed to formalizing the above intuition. To this end, let us state the following lemma from renewal theory. It appears as Proposition~7.3 in~\cite{Ross09}.
\begin{lemma} \label{lem:renewal_theory}
Let  be an infinite sequence of nonnegative i.i.d. random variables with finite expectation, i.e., , . Let  be another such sequence. (Each sequence is independent, but  may depend on  for every .) For any , set , where  is the maximum integer such that . Then

\end{lemma}

We are now ready to analyze the number of traversals over a given edge  in our random walk. 

\begin{lemma} \label{lem:ultimate_for_one_edge}
Let  and . Also, fix any edge . Consider again the random walk from the statement of Lemma~\ref{lem:preUltimate}. The expected number of times   will be traversed during this walk is .
\end{lemma}
\begin{proof}
Consider an infinite random walk starting at . We will divide this walk into phases, each of which starts at  and ends on the first visit to  after all vertices from  have been visited. In this way we get an i.i.d. sequence of copies of the random walk of Lemma~\ref{lem:preUltimate}. For every  let  be the length of the -th phase, and let  be the number of times the edge  was traversed during this phase. By Lemma~\ref{lem:renewal_theory} we obtain that

where  is the number of traversals of  during all phases which have finished by time . We need to show that this quantity is equal to .

Indeed, if we define  to be the number of traversals of  up to time , then it is well-known
that  (which is the stationary probability of traversing ). Furthermore, the difference between  and  is just the number of traversals of  during the current phase at time , and we can upper-bound  by . Hence, we have that

Taking a limit of  of both sides concludes the proof.
\end{proof}

Now Lemma~\ref{lem:ultimate} follows easily. Namely, let us consider a random walk started at an arbitrary vertex (possibly outside of ) and let  be the first vertex in  on that walk. (No edges from  will be traversed before  is reached, as .) Applying Lemma~\ref{lem:ultimate_for_one_edge} with  to each edge , we obtain that the total number of traversals of edges in  in this random walk until the set  is covered is , as we wanted to show. 



\end{document}
