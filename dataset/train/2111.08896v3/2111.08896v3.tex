\subsection{Data}

\paragraph{Pre-training Data}
The same in-domain data is used as in LXMERT~\citep{tan2019lxmert} for pre-training. It consists of the image caption data from MS COCO~\citep{lin2014microsoft}, Visual Genome~\citep{krishna2017visual}, image question answering data from VQA 2.0~\citep{antol2015vqa}, GQA balanced version~\citep{hudson2019gqa} and VG-QA~\citep{zhu2016visual7w}. The total amount of the dataset is 9.18M image-and-sentence pairs on 180K distinct images. Also, additional out-of-domain data from Conceptual Captions~\citep{sharma2018conceptual} for model pre-training, which consists of about 3M image-text pairs on 3M images.

For Text Reading Expert, the StructuralLM~\citep{structurallm} is used as the base model, which is pre-trained on the IIT-CDIP Test Collection 1.0~\citep{Lewis}. It is a large-scale scanned document image dataset containing more than 6 million documents, with more than 11 million scanned document images.

\paragraph{Fine-tuning Data}
Visual Question Answering (VQA) is a dataset containing open-ended questions about images~\citep{antol2015vqa}. These questions require understanding of vision, language and commonsense knowledge to answer. It contains a large number of labeled question-image-answer triplets with 10 human annotators for each question. The detailed statistics for VQA training/validation/test data splits is shown in Table~\ref{tab:vqa_stat}. 

\begin{table}[t]
\caption{VQA Data Statistics.}
\centering
\begin{tabular}{c|c|cccc|c}
\toprule
 & Images & Questions & Yes/No & Number & Other & Answers  \\
\midrule
Training                                                    & 80K & 443K & 169K & 58K & 219K & 4.4M  \\
Validation                                                 & 40K & 214K & 81K & 28K & 106K & 2.1M  \\
Test & 80K    & 447K & - & - & - & - \\
\bottomrule
\end{tabular}
\label{tab:vqa_stat}
\end{table}

For training Text Reading Expert, three text-reading VQA datasets is used including a subset of VQA data~\citep{antol2015vqa}, TextVQA~\citep{textvqa} and ST-VQA~\citep{stvqa}. A classification model is trained to extract text-reading samples from VQA data. The questions of TextVQA and ST-VQA are treated as positive samples, and the questions on images without text in VQA are treated as negative samples. The detailed statistics for the three text-reading VQA datasets is shown in Table \ref{tab:textvqa_stat}.

\begin{table}[ht]
\caption{\label{table:compare} Text-reading VQA Data Statistics.}
\centering
\begin{tabular}{l|c|c}
\toprule 
\textbf{Dataset} & \textbf{Images} & \textbf{Questions} \\
\midrule
VQA-Subset & 20k & 21k  \\
TextVQA & 25k & 39k  \\
ST-VQA & 19k & 26k  \\
\bottomrule
\end{tabular}
\label{tab:textvqa_stat}
\end{table}

For training Clock Reading Expert, the images are collected from two sources. One is from \textit{open-access datasets}. Specifically, a total of 4863 images are collected from COCO2017~\citep{lin2014microsoft}\footnote{The COCO images used in the VQA test set are left unlabeled and excluded from our training data.} and 2691 images from ImageNet~\citep{deng2009imagenet} for clock labeling, both of which are widely used open-access datasets. Annotators are required to give the bounding boxes and the precision time of clocks in images. After labeling, 4236 and 3271 valid clock bounding boxes are obtained from COCO2017~\citep{lin2014microsoft} and ImageNet~\citep{deng2009imagenet} respectively. 785 clock bounding boxes are randomly sampled from COCO2017 images for validation.
The other source is \textit{Internet images}. To further increase the generalization and capacity of our clock reader, 2878 images from internet with various clocks are collected. After careful annotation, 2314 valid clocks are obtained. Note that this data is only used for the training of the clock reader.

\paragraph{Evaluation Metric}
Following \citep{antol2015vqa}, an evaluation metric robust to inter-human variability is used in phrasing the answers: 
\begin{equation}\label{eq:evaluation}
\text{Acc({\it ans})} = \text{min} \Big\lbrace \frac{\text{\# human that said {\it ans}}}{3}, 1 \Big\rbrace
\end{equation}
In order to be consistent with ``human accuracies'', machine accuracies are averaged over all 10-choose-9 sets of human annotators.

\subsection{Experimental Setup}
\paragraph{VLP} The maximum sequence length for the sentence is set as 20. For the VLP models, the pre-trained Transformer encoder with 12 layers is used as our base architecture, and the one with 24 layers as the large architecture. The basic settings of the Transformer are the same as BERT~\citep{devlin2018bert}, and the Transformer encoder is initialized with StructBERT~\citep{wang2019structbert} for its good performance. For the method of \emph{learning to attend}, the two learnable parameters are initialized with $init\_value_1=1.0$ and $init\_value_2=L_s/L$, where $L$ is the number of Transformer layers and $L_s$ is the corresponding layer number. The base model is pre-trained with a total batch size of 512 for 30 epochs on 8 A100 GPUs and the AdamW optimizer with the initial learning rate of 1e-4. The 24-layer large architecture is pre-trained with the total batch size of 512 on 8 A100 GPUs. To deal with over-fitting, two-stage pre-training strategy is employed as in LXMERT~\citep{tan2019lxmert}. Specifically, the model is first pre-trained without the question answering task with the initial learning rate of 5e-5 for 20 epochs, and then pre-trained with all the tasks together with the initial learning rate of 2e-5 for another 10 epochs. The detailed settings for the three VLP methods are listed as below:
\begin{enumerate}
    \item \textbf{Region-VLP}: The detection model is used in VinVL~\citep{zhang2021vinvl} to detect objects and extract region features. It is a large-scale object-attribute detection model based on the ResNeXt-152 C4 architecture. 100 objects is retained for each image to maximize the pre-training compute utilization by avoiding padding.
    
    \item \textbf{Grid-VLP}: It follows the basic settings in Grid-VLP~\citep{yan2021gridvlp}
    . ResNeXt is chosen to be the visual encoder with different sizes~\citep{xie2017aggregated} as in~\citep{jiang2020defense,huang2020pixel}. The shorter side of every input image is resized to 600, and the longer side is limit to at most 1000. A fixed number of 100 grids are randomly selected each time during pre-training~\footnote{\small{We also tested with 64 and 128 selected grids. It did not lead to significantly different results.}}. 
    
    \item \textbf{Patch-VLP}: It uses the visual Transformer encoder of the Swin detector~\citep{liu2021swin} and CLIP~\citep{radford2021learning}. The ViT-B/32 pre-trained model is chosen, which has 12 Transformers layers with input patches of size $32 \times 32$. Every input images is resized to $224 \times 224$ as CLIP does, resulting in $7\times7=49$ patches.
    
    \item \textbf{Fusion-VLP}: It fuses the three classes of image features (Region, Grid and Patch) by concatenating them together as the visual input to the Transformer. A two-stage strategy is employed to pre-train Fusion-VLP, which first trains the region-grid model initialized with Region-VLP, and then continues to train the region-grid-patch model.
\end{enumerate}

\paragraph{Fine-tuning on VQA}
Following~\citep{anderson2018bottom}, our architecture treats VQA as a multi-class classification task by picking an answer from a shared set of 3,129 answers. The hidden state of $h^L_{CLS}$ is used to map the representation into 3,129 possible answers with an additional MLP layer. The model is trained with a binary cross-entropy loss on the soft target scores. The pre-trained models are fine-tuned based on the three classes of features on the VQA training data for 3 epochs with the batch size of 32, and the BERT Adam optimizer is employed with the initial learning rate of 1e-4 for base models and 2e-5 for large models. At inference, a \emph{softmax} function is used for prediction.

\paragraph{Text Reading Expert} The text reading expert follows the basic settings in StructuralLM ~\citep{structurallm} and uses the pre-trained StructuralLM-large as the backbone model. In particular, StructuralLM is pre-trained with a batch size of 16 for 50K steps. The question tokens and the OCR tokens of an image are concatenated as an input sequence, of which the maximum length is set as 128. For fine-tuning, the three kinds of text-reading VQA datasets are merged and split with 10-fold cross-validation. The StructuralLM is fine-tuned with the total batch size of 16 for 4 epochs, and the AdamW optimizer is employed with the initial learning rate of 3e-5. Accuracy and ANLS (Average Normalized Levenshtein Similarity) are used as the metrics to evaluate the text reading expert.

\paragraph{Clock Reading Expert}
The clock detector of the clock reading expert is trained following the basic settings of Cascade-RCNN~\citep{cai2018cascade}. The clock reader is trained with the batch size of 96 in 2 GPUs, and the initial learning rate is set as 0.02. It is trained for 150 epochs with the learning rate multiplied by 0.1 at 90-th and 120-th epochs. The data augmentation pipeline consists of $256 \times 256$ random resized cropping, random color jittering, random gray-scale conversion, Gaussian blurring and random rotation within $\pm 45^{\circ}$.
        

\paragraph{Visual Understanding Expert} The visual understanding expert ensembles 46 models in total, including 14 Region-VLP models, 21 Grid-VLP models, 4 Patch-VLP models and 7 Fusion-VLP models. Simple maximum voting is adopted to ensemble all the models based on their prediction scores.

\paragraph{Mixture of Experts} The MoE adopts Multi-layer Perceptron (MLP) as the gating network to determine experts for given questions. The MLP has two hidden layers of 100 neurons and 50 neurons, respectively. It uses \emph{tanh} as the activation function, and the Adam optimizer with the initial learning rate of 1e-3. The network is trained for 5 epochs with the batch size of 256.



\begin{table}[t]
\caption{VQA Challenge Leaderboard.}\label{tab:main_results}
\centering
\begin{tabular}{l|c|ccc}
\toprule
\multicolumn{5}{c}{VQA Challenge Leaderboard (Test-std)}                                                             \\
\midrule
Models                                                    &  Overall & Yes/No & Number & Other \\
\midrule
Human    &  80.83 & \bf 95.48 & \bf 81.29 & 67.97\\
\midrule
LXMERT (\cite{tan2019lxmert})                   & 74.34    & 89.45  & 56.69  & 65.22 \\
MCAN (\cite{yu2019mcan})                      & 75.23    & 90.36  & 59.17  & 65.75 \\
VILLA (\cite{gan2020large})                     & 75.85    & 91.30  & 59.23  & 66.20  \\
BGN (\cite{guo2019bilinear})                      & 75.92    & 90.89  & 61.13  & 66.28 \\
InterBERT (\cite{lin2020interbert})          & 76.10    & 91.67  & 59.24  & 66.40  \\
GridFeat+MoVie (\cite{jiang2020defense})           & 76.29    & 90.81  & 61.53  & 67.04 \\
VinVL (\cite{zhang2021vinvl})                     & 77.45    & 92.38  & 62.55  & 67.87 \\
ROSITA (\cite{cui2021rosita})                   & 78.34    & 92.66  & 63.24  & 69.33 \\
UNIMO (\cite{li2020unimo})  & 78.40  & 93.10  & 63.06  & 69.12 \\
VQA Challenge 2021 winner & 79.34    & 93.28  & 65.36  & 70.40  \\
PASH-SFE                  & 79.47    & 92.45  & 76.57  & 68.82 \\
SimVLM (\cite{wang2021simvlm})                   & 80.34    & 93.29  & 66.54  & 72.23 \\ 
\midrule
\modelname                 & \textbf{81.26}    & 93.55  & 72.01  & \bf 72.67 \\
\bottomrule
\end{tabular}
\end{table}





\begin{table}[b]
\setlength{\tabcolsep}{0.6mm}
\caption{Performance comparison with other single models.} \label{tab:single_results}
\centering
\begin{tabular}{lc|ccc|ccc}
\toprule
\multicolumn{8}{c}{Performance of Single Models}                                                                                           \\
\midrule
\multirow{2}{*}{Models}                             & \multirow{2}{*}{Feature Type} & \multicolumn{3}{c|}{BASE}     & \multicolumn{3}{c}{LARGE}    \\
                                                    &                        & Params & Test-dev & Test-std & Params & Test-dev & Test-std \\
\midrule
VLBERT (\cite{su2019vl})           & Region                 & 110M   & 71.16    & -        & 345M   & 71.79    & 72.22    \\
UNITER (\cite{chen2020uniter})     & Region                 & 110M   & 72.70    & 72.91    & 345M   & 73.82    & 74.02    \\
OSCAR (\cite{li2020oscar})         & Region                 & 110M   & 73.16    & 73.44    & 345M   & 73.61    & 73.82    \\
UNIMO (\cite{li2020unimo})         & Region                 & 110M   & 73.79    & 74.02    & 345M   & 75.06    & 75.27    \\
VinVL (\cite{zhang2021vinvl} )     & Region                 & 110M   & 75.95    & 76.12    & 345M   & 76.52    & 76.60    \\
ViLBERT (\cite{lu2019vilbert})     & Region                 & 221M   & 70.55    & 70.92    & -      & -        & -        \\
12-in-1 (\cite{Lu_2020_CVPR})    & Region                 & 221M   & 73.15    & -        & -      & -        & -        \\
LXMERT (\cite{tan2019lxmert}  )    & Region                 & 183M   & 72.42    & 72.54    & -      & -        &          \\
ERNIE-ViL (\cite{yu2021ernie} )    & Region                 & 250M   & 73.18    & 73.36    & 510M   & 74.95    & 75.10    \\
PixelBERT (\cite{huang2020pixel}) & Grid                   & 170M   & 74.45    & 74.55    & -      & -        & -        \\
ViLT (\cite{kim2021vilt}) & Patch                   & 110M   & 71.26   & -    & -      & -        & -        \\
\midrule
Region-VLP                & Region                                            & 110M   &  76.25 &  -  & 345M &  77.17 &  - \\
Grid-VLP                  & Grid                                              & 110M   &  76.50 &  -  & 345M &  77.13 &  - \\
Patch-VLP                 & Patch                                             & 110M   &  71.61 &  -  & 345M &  - &  - \\
Fusion-VLP                                           & Region+Grid+Patch      & 110M   & \textbf{76.80}        & \textbf{76.78}        & 345M   & \textbf{77.59}   & \textbf{77.61}  \\ 
\bottomrule
\end{tabular}
\end{table}

\subsection{Main Results} \label{sec:mr}




Table~\ref{tab:main_results} presents our main results compared with all the previous public and unpublic best results on the VQA Challenge Leaderboard. From the results, it can be observed that: 1) Our VQA architecture \modelname represents the first to achieve human parity on VQA Challenge Leaderboard outperforming all the previous state-of-the-art methods by a large margin, which demonstrates the effectiveness of our framework. 2) With regard to a breakdown of performance on different question types, \modelname performs much better on the ``Other'' type than human do, and gives comparable results on ``Yes/No'' questions. \modelname performs worse than human do on type ``Number'' for the two reasons: a) in the ``Number'' type, there are many questions about reading OCR text, which are easier for human to answer; and b) there are many object counting questions that are more difficult for \modelname to answer.

Table \ref{tab:single_results} presents the detailed results of our single VLP models compared with other state-of-the-art methods. From the results, it is observed that: 1) the proposed VLP model outperforms the others on every kind of visual feature (region / grid / patch), respectively. It demonstrates the effectiveness of the proposed cross-modal interaction with learning to attend mechanism. 2) The methods with self-attention on patch feature perform worse than the region-based and grid-based methods do. There are two weaknesses of patch-based methods: a) the visual semantic information is not well-captured in existing patch-based VLP methods. How to inject visual semantics into patch representation remains largely unexplored; b) the image-text pre-training data is not enough for large-scale patch-based pre-training; 3) Fusion-VLP gives the best performance by fusing all the three classes of visual features as input, which validates the effectiveness of comprehensive feature representation.


\subsection{Model Analysis by Modules} \label{sec:model_analysis}

\paragraph{Visual Feature Importance}
Here presents the ablation study to assess the importance of different visual features for VLP on the VQA test-dev set. The results shown in Table \ref{table:vf} indicate that: 1) The VLP methods based on region and grid features achieve much better performance than the ones based on patch feature do, as stated in Section~\ref{sec:mr}. When examining by individual question types, Region-VLP performs better on the ``Number'' type, while Grid-VLP does better on the ``Yes/No'' and ``Other'' types. The difference can be attributed to the fact that region feature captures more local information of an image at the object level, and thus is more effective in address the visual counting problem by identifying local objects in an image. On the other hand, grid feature captures globally visual context in an image, which helps to answer the ``Yes/No'' and ``Other'' questions; 2) by combining the three classes of features in the way of early fusion, Fusion-VLP performs the best among all the single models. It shows that the different kinds of features can complement well with each other.

\begin{table}[t]
\centering
\caption{Ablation study of visual features on VQA Test-dev.} \label{table:vf}
\begin{tabular}{lcccc}
\toprule
           & Overall & Yes/No & Number & Other \\
\midrule
Fusion-VLP &  \textbf{77.59}    &  91.91    &   \textbf{64.29}    &    \textbf{68.33}   \\
Region-VLP & 77.17 & 91.62  & 63.69 &  67.84     \\
Grid-VLP   &  77.13  &  \textbf{92.20}    &   59.99     &   68.15    \\
Patch-VLP  &    71.61     &  88.17      &   49.44     &   62.54    \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Learning to Attend}
Here presents the ablation study to assess the importance of the \emph{learning to attend} mechanism on the VQA test-dev set. The 24-layer Region-VLP is used as the baseline model, which is pre-trained and fine-tuned based on the original Transformer. The ablation applies the same pre-training and fine-tuning settings, and only modifies the self-attention block with the two ways of \emph{learning to attend} stated in Section~\ref{sec:l2a}. From Table \ref{table:l2a}, it can be seen that the model with either way of \emph{learning to attend} outperforms the best Region-VLP baseline. Among the two different ways, the one with two learnable parameters performs slightly better than the other one. The reason may lie in: 1) learning two unrestricted parameters for each layer allowing for more parameter freedom, so as to better align cross-modal semantics, 2) the discrepancy between pre-training and fine-tuning, where the representation of [CLS] is learnt to model the semantic relation between a caption and an image during pre-training, while it is repurposed for question answering in fine-tuning.



\begin{table}[b]
\centering
\caption{Ablation study of \emph{learning to attend} on VQA Test-dev.}
\begin{tabular}{lcccc}
\toprule
                  & Overall & Yes/no & Number & Other \\
\midrule
Region-VLP (Baseline)       &   76.75 & 91.28 &  63.31 &  67.34  \\
\quad + Learning to Attend (FFN) &   77.09 &  91.58 &  63.54 & 67.74  \\
\quad + Learning to Attend (Param) &  \textbf{77.17} & \textbf{91.62}  & \textbf{63.69}  &  \textbf{67.84} \\

\bottomrule
\label{table:l2a}
\end{tabular}
\end{table}

\begin{comment}
\begin{table}[!h]
\centering
\caption{Ablation study of text reading expert on the VQA Test-dev.}
\begin{tabular}{lcccc}
\toprule
           & Overall & Yes/No & Number & Other \\
MoE (Text Reading \& Visual Understanding) & 80.63 & 93.31 & 69.97 & 72.01 \\
w/o Text Reading Expert &  79.44  & 93.31 &  65.70 & 71.16   \\

w/o TextVQA \& ST-VQA   &  80.28   &  93.31  & 69.43   & 71.47 \\
w/o add separator &   80.57 & 93.31 & 69.95 & 71.86 \\
w/o continue pre-training  &  80.41 &   93.31  &    69.82   &   71.64    \\
\bottomrule
\label{tab:structlm}
\end{tabular}
\end{table}
\end{comment}


\begin{table}[!h]
\centering
\caption{Ablation study of text reading expert on the VQA Test-dev.}
\begin{tabular}{lcccc|c}
\toprule
           & Overall & Yes/No & Number & Other & ANLS \\
\midrule
Visual Understanding Expert & 79.44  & 93.31 &  65.70 & 71.16 & - \\
\: + Text-reading VQA data & 80.35 & 93.31 & 69.81 & 71.49 & 79.85 \\
 \: \: + add separator &  80.41 &   93.31  & 69.82   &   71.64 & 79.96 \\
 \: \: \: + continue pre-training  & \textbf{80.63} & 93.31 & \textbf{69.97} & \textbf{72.01} & \textbf{80.33} \\
\bottomrule
\label{tab:structlm}
\end{tabular}
\end{table}

\paragraph{Knowledge Mining}
Figure~\ref{fig:kmeans_example} illustrates the clustering result. We choose the number of the clusters as 5, which gives the best performance on our quantitative test. For each cluster, we showcase two examples in the cluster and the percentage of the examples in this cluster. From the results, we can see that the proposed knowledge mining can actually mine certain meaningful topic clusters, where similar examples are clustered together. For example, Cluster~1 is about asking questions about time and clocks. Cluster~2 is about counting problems. Cluster~3 is about reading texts from the wall or the clothes. Cluster~4 is about reading number texts on the vehicles. We found that there are two new tasks: clock-reading task (Cluster~1) and text-reading task (Cluster~3,4), both of which require specific prior knowledge. We also use a three-class classifier in Section~\ref{sec:moe} to classify the filtered candidate examples, so as to measure the consistency between the clustering result and classification result.
Figure~\ref{fig:tsne} gives t-SNE visualization of the clustering result and classification result. From the result, we can see high consistency between the clustering and classification result on the measured topics. The knowledge mining method can properly separate part of the clock-reading examples and OCR-reading examples from the other examples, although for the OCR-related exampled there still exist limited examples mixed up in the common vision category.

\begin{figure} \centering
    \includegraphics[width=\textwidth]{figures/kmeans-example.pdf}
    \caption{Illustration examples of clustering results. Percent is the percentage number of each cluster's examples.}
    \label{fig:kmeans_example}
\end{figure}

\begin{figure} \centering
    \begin{subfigure}[]{}
         \centering
         \includegraphics[width=0.45\textwidth]{figures/TSNE-multicore-11-2.pdf}
\end{subfigure}
     \hfill
     \begin{subfigure}[]{}
         \centering
         \includegraphics[width=0.45\textwidth]{figures/TSNE-multicore-11.pdf}
     \end{subfigure}
    \caption{The t-SNE visualization of clustering results. Figure (a) shows the clustering results and the label 1/2/3/4/5 is cluster id. Figure (b) shows the classification results and the label ocr/clock/vision is classification label. The classifier is manually built and the accuracy of it is 95.0\%.}
    \label{fig:tsne}
\end{figure}


To measure the consistency of the clustering result to the classification labels, we also provide detailed quantitative analysis on different clustering methods. We manually build a three-label classifier (OCR, clock and vision) with 95\% accuracy as in Section~\ref{sec:moe} and apply it to evaluate the consistence of each cluster. We project each cluster to the corresponding label heuristically. For example, in Figure~\ref{fig:kmeans_example}, Cluster~1 is assigned to clock label, Cluster~3 and Cluster~4 are to OCR label, and Cluster~5 is to vision label. We then compare the assigned label of each cluster to that of the classification label (95\% accuracy). We use accuracy, macro-precision, macro-recall and macro-f1 to measure how consistent the compared label in each cluster is. As list in Table~\ref{tab:cluster}, K-Means (K=5) achieves the best performance with 0.8448 accuracy and 0.8739 macro-F1, which shows that the clustering result is highly consistent with the assumed classification labels on OCR/clock/vision. 




\begin{table}[h]
\centering
\caption{Quantitative analysis on the clustering results of different clustering methods.}\label{tab:cluster}
\begin{tabular}{l|c|ccc}
\toprule
                  & Acc    & P      & R      & Macro-F1 \\ 
\midrule
DBSCAN (eps=0.5)  & 0.1544 & 0.4605 & 0.3861 & 0.1466   \\
K-Means (K=3)       & 0.4969 & 0.6487 & 0.662  & 0.6163   \\ 
K-Means (K=4)       & 0.7894 & 0.8239 & 0.8219 & 0.8195   \\ 
K-Means (K=5)       & \textbf{0.8448} & 0.8659 & 0.8898 & 0.8739   \\ 
K-Means (K=6)       & 0.8443 & \textbf{0.8668} & \textbf{0.8918} & \textbf{0.8740}   \\ 
\bottomrule
\end{tabular}
\end{table}

\paragraph{Text Reading Expert}
As stated in Section~\ref{structlm}, the pre-trained StructuralLM model is adapted on text-reading VQA samples in different ways. Table \ref{tab:structlm} shows the ablation results on the VQA test-dev set. The visual understanding expert~\footnote{\small An early version of visual understanding expert for VQA Challenge 2021 is used as the baseline.} is used as the baseline method, upon which all the ablation experiments for text reading expert is conducted. First, it is observed that text reading expert greatly improves the performance on the ``Number'' type by over 6\%, where many questions are asked about reading numbers from OCR text, such as a bus number and a football player number. On the ``Other'' type, the performance can be improved by over 1\%. Answering many questions of this type requires the ability to reason with both visual and textual OCR information. Adding the separator between textual bounding boxes and continual pre-training on domain data can lead to further improvement, demonstrating the effectiveness of adapting the pre-trained StructuralLM for text-reading VQA. 


\paragraph{Clock Reading Expert}
The ablation study of clock reading expert is shown in Table~\ref{tab:clock}, where only the results on the ``Number'' and ``Overall'' types are given, because questions on reading clocks are only present in the ``Number'' type. Adding clock reading expert results in more than 4.5\% performance improvement on the ``Number'' type (from 59.93 to 62.65), which demonstrates the effectiveness of proposed ideas in the clock reading expert.
Specifically, the proposed regression loss is prone to provide a larger gradient when there is a bigger difference between the predicted time and the ground truth, which benefits prediction of the clock reader. Moreover, it can be observed that the self-supervised loss boosts the performance significantly, as the relationship prior constrains hour and minute branches both, which eliminates the confusion of hour and minute hands.

\begin{table}[t]
\caption{Ablation study of clock reading expert. } \centering
\begin{tabular}{c|ccc|c|cc}
\toprule
Clock Detector         & \multicolumn{4}{c|}{Clock Reader}                        & \multicolumn{2}{c}{VQA Test-dev} \\ \midrule
Detection(mAP)         & Cls& Regression Loss & Self-supervised Loss & Clock Accuracy & Number    & Overall   \\ \midrule
Baseline                     & --       & --       & --               & --         & 59.93        & 76.51          \\
\multirow{3}{*}{79.30} &\checkmark&          &                  & 72.5       & 62.52        & 76.79          \\
                       &\checkmark&\checkmark&                  & 73.0       & 62.59        & 76.80           \\
                       &\checkmark&\checkmark&\checkmark        & \textbf{74.7}       & \textbf{62.65}        & \textbf{76.81}        \\
\bottomrule
\end{tabular}
\label{tab:clock}
\end{table}

\begin{table}[t]
\centering
\caption{Ablation study of MoE on the VQA Test-dev.}\label{tab:moe_result}
\begin{tabular}{lcccc}
\toprule
           & Overall & Yes/No & Number & Other \\
\midrule
Visual Understanding Expert &  80.05    &   93.67     &   66.78     &  71.40     \\
\quad + Text Reading Expert (MoE) &  81.00  &   93.67  & 69.75     &  \bf 72.69     \\
\quad \quad + Clock Reading Expert (MoE)   &  \bf 81.27   & 93.67  & \bf 72.55    &  72.60  \\
\bottomrule
\end{tabular}
\end{table}







\paragraph{Mixture of Experts (MoE)}
The ablation study of MoE is shown in Table \ref{tab:moe_result}. With only visual understanding expert, the model gives a strong performance of accuracy 80.05 on the VQA Test-dev set. Adding text reading expert increases the overall performance by more than 1\%, which already achieves human parity of 80.83. Adding clock reading expert further boosts the performance to 81.27, where the performance on the ``Number'' type increases by more than 4\%. The gating network of MoE mimics human who is able to identify domain experts based on the nature of tasks. This knowledge-guided MoE framework can also be easily extended to incorporate more specialized experts for continual self-evolution.

\begin{figure}[b]
\includegraphics[width=\textwidth]{figures/type_pie.pdf}
\caption{The distribution of abilities in each answer type.}
\label{fig:vqa_answertype}
\end{figure}

\subsection{VQA Dataset Analysis}

This subsection provides more detailed analysis of our VQA results. To gain an understanding of types of questions and answers, 1000 examples are randomly sampled from the validation set for analysis.
The 1000 examples are classified into the categories listed below by manual examination based on the abilities required. The categorization is multi-label in the sense that every example is classified into all applicable categories. Figure~\ref{fig:vqa_answertype} provides an estimate of the proportion for each category. Commonsense Knowledge, Relational Reasoning and Object Counting are the top three categories in the overall distribution. Commonsense Knowledge accounts for over 80\% of the \emph{Yes/No} type. In the \emph{Number} type, Object Counting and Textual Recognition are the two most popular categories compared with the other two types. The type \emph{Other} has a similar distribution as that of the overall distribution. Figure \ref{fig:examples} presents representative examples from each category.

\begin{itemize}
    \item \textbf{Commonsense Knowledge} This category contains questions inquiring commonsense knowledge from our daily life, such as colors, weathers, food and furniture.
    \item \textbf{Visual Recognition} This category requires the ability to acquire specialized knowledge with visual recognition to answer questions in this category.
    \item \textbf{Relational Reasoning} This category requires understanding and reasoning over certain relationships of objects in an image, such as positional relationship, comparison relationship, etc.    \item \textbf{Textual Recognition (OCR)} This category requires the ability to recognize and utilize text together with the positions or visual information in an image (e.g., road signs, ads on a bus).
    \item \textbf{Object Counting} This category contains the examples that test the ability of counting objects in an image.
    \item \textbf{Clock Reading} This category contains the examples that test the ability of reading a clock.
    \item \textbf{Other} This category contains the questions that are ambiguous or cannot be answered based on given images.
\end{itemize}

\begin{figure}[t] \centering
    \includegraphics[width=\textwidth]{figures/examples.pdf}
    \caption{Representative examples from each category.}
    \label{fig:examples}
\end{figure}










\begin{table}[t]
\caption{The overall performance of \modelname and human on val split.}\label{tab:human_overall}
\setlength{\tabcolsep}{0.6mm}
\centering
\begin{tabular}{l|c|cccc}
\toprule
\multirow{2}{*}{} & Test-std & \multicolumn{4}{c}{Val}           \\
\cmidrule{2-6}
                  & Overall        & Overall & Yes/no & Number & Other \\
\midrule
VLP     &    \bf 81.26    &  79.54       & 92.47      & 70.63      & \bf 72.00     \\
Human                                 & 80.83          & 78.69   & \bf 94.87  & \bf 78.79  &  66.34\\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[t]
\centering
\setlength{\tabcolsep}{0.6mm}
\caption{The performance of \modelname and human by category.}\label{tab:human_category}
\begin{tabular}{c|ccccccc}
\toprule
      & \tabincell{c}{Commonsense \\Knowledge} & \tabincell{c}{Relational \\Reasoning} & Object Counting & Visual Recognition & \tabincell{c}{Textual Recognition \\ (OCR)} & Clock Reading & Other \\
      \cmidrule{2-8}
& 767 & 159  & 103 & 70  & 74 & 7  & 5  \\
\midrule
VLP   & \bf 83.60   & \bf 71.19 & 77.76  & \bf 68.14     & 52.03 & \bf 86.00 & \bf  70.00 \\
Human & 80.04                  & 70.20     & \bf  81.29           & 59.76              & \bf 76.62                     & 60.66 & 49.52 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{\modelname vs. Human}




A comparative study of \modelname and human on visual question answering has been conducted. Table \ref{tab:human_overall} and Table \ref{tab:human_category} show the overall and per-category performance of \modelname and human on the val split, respectively, from which there are the following observations: 
(i) \modelname outperforms human annotators on the two largest categories, Commonsense Knowledge and Relational Reasoning. It shows \modelname's superiority of identifying common scene objects in daily life and leveraging commonsense knowledge such as colors and weathers. This result also demonstrates the power of \modelname in reasoning over relative positions, such as the left sign on a wall, to answer a spatial reasoning question. Besides, it is surprising that \modelname can reason over simple comparison, such as which object is the tallest.
(ii) The questions in the Object Counting category seem rather difficult for \modelname to answer. \modelname is found to be good at counting a small number (<10) of objects. It would give an incorrect count when encountering a large number of small objects and/or requiring reasoning over them.
(iii) \modelname significantly surpasses human performance on Visual Recognition which requires specialized knowledge. It is expected that \modelname, as a machine learner trained with large data, is skilled in memorizing specialized/professional knowledge with visual recognition, compared with non-professional human annotators.
(iv) \modelname is more capable of reading time shown in a clock than human, as demonstrated by the result of Clock Reading. On text reading, however, there is still a big gap between \modelname and human in recognizing and understanding text in an image, as shown by the result of Textual Recognition. Some research progress has been made on text-reading VQA tasks, such as TextVQA~\citep{textvqa}.





Figure~\ref{fig:case_study_1}, Figure~\ref{fig:case_study_2} and Figure~\ref{fig:case_study_3} present \modelname's predictions together with ground truth on each category for case study. In particular, a couple of representative examples are listed for each category, each containing a question, an image and the answer predicted by \modelname. The scores of human annotators and \modelname, as well as the top three ground-truth annotations are also given for comparison. The scores of Human and \modelname are calculated based on Equation~\ref{eq:evaluation}. These examples are studied by category as follows:

\paragraph{Commonsense Knowledge}
\modelname is knowledgeable in many aspects of daily life. As shown in Figure~\ref{fig:case_study_1}, \modelname is able to tell not only weather and the sentiments of people, but also classic sports and electronic products as an ordinary person does. Also, it is skilled in geography and understands the food around the world. For example, \modelname recognizes the small English flag and Big Ben in the image, by which the country is identified as England. As another example, based on the rice and dishes from the image, Chinese food can be identified by \modelname and people familiar with Chinese cuisine. The other people may not tell cuisine of the food .
Our experiments show that \modelname trained on adequate data can capture the commonsense knowledge (the largest category in the VQA dataset) in our daily life.

\begin{figure}[t] \centering
    \includegraphics[width=\textwidth]{figures/case_study_2.pdf}
    \caption{Case Study for Commonsense Knowledge and Visual Recognition. The scores of Human and \modelname are calculated with Equation~\ref{eq:evaluation}. Ground Truth gives the top three annotations.}
    \label{fig:case_study_1}
\end{figure}

\paragraph{Visual Recognition}
Except for Clock Reading, \modelname shows much better performance than an ordinary person does in this category. As shown in Figure~\ref{fig:case_study_1}, it is relatively easy for an AI model trained adequately to memorize specialized knowledge, while rather difficult for people unfamiliar with the specific domain. For example, \modelname can better identify the specific categories of the animals, such as dog and bird, and the historical style of the furniture, which requires specialized knowledge. By locating and recognizing the barely visible logo from the motor bike, \modelname correctly recognizes its brand, while human may miss the details in the image and give an answer based on their best guesses. On the other hand, \modelname has a slim chance of being fooled by the activity present in the image. It incorrectly identifies that the boy is playing baseball based on his motion, while it is actually a Wii game. As a result, by incorporating relevant specialized knowledge and visual recognition capability, \modelname can outperform human by a large margin in this category.

\paragraph{Object Counting}
As shown in Figure~\ref{fig:case_study_2}, \modelname achieves human parity when counting a small number of objects, but fails in more complicated cases where there are occlusions or a great quantity of objects. It surprises us that \modelname can give a count very close to the correct answer in the example of counting trees. However, the object counting ability is still quite limited compared with ordinary people. One reason may lie in that the visual detection is too weak to detect all the objects in an image when the number of objects is large. There are few cases with more than 10 objects in the training set, and thus the model is not fully trained with sufficient data. This is shown in the third example where \modelname fails to identify all people from the image. Another possible reason is that the object detector is difficult to count in the presence of occlusion. The second example shows that \modelname counts the racing people incorrectly due to the occluded person.

\begin{figure}[t] \centering
    \includegraphics[width=\textwidth]{figures/case_study_3.pdf}
    \caption{Case Study for Object Counting and Relational Reasoning.}
    \label{fig:case_study_2}
\end{figure}

\paragraph{Relational Reasoning}
Figure~\ref{fig:case_study_2} shows that \modelname has the abilities to reason over the relationship of positions, comparison and exclusion. It is observed that \modelname may be more capable than human in precise position identification and knowledge reasoning for relational reasoning questions. Specifically, 1) \emph{position}: the first two examples show the power of \modelname in distinguishing the positions of the left-right and front-back, and conducting one-step reasoning over the positional relationship; 2) \emph{comparison}: the third example demonstrates that \modelname can even compare the colors of two objects, which is a simple one-step reasoning over the comparison between attributes of two objects; 3) \emph{exclusion}: the last example shows that \modelname is able to identify the exclusion relationship, and reason over it with commonsense knowledge.

\paragraph{Textual Recognition (OCR)}
As shown in Figure~\ref{fig:case_study_3}, the text reading expert (StructuralLM) is able to identify text and layout in simple cases. In the first example, the model correctly answers with the words displayed on the man's shirt. In addition, StructuralLM is capable of learning the interactions between text and layout, which makes StructuralLM aware of location of text present in an image. This is shown in the second example, where the model predicts the answer correctly when asked about the sign on the left. However, the model fails in the two cases: 1) OCR errors; 2) answering complex questions which requires visual feature and reasoning abilities. As shown in the third example, when asked the words on the man's shirt, the model can predict only ``3'' because the OCR tool cannot recognize the word ``cardinals''. In the fourth example, given the question about the number present on the white shirt, the model answers incorrectly due to the lack of visual feature of colors and the reasoning ability. Currently, the text reading expert utilizes only the layout and textual information to answer a text-reading question, without leveraging visual signals. There is an urgent need for deep interaction between visual information and OCR textual information in images, which is left for future work.

\paragraph{Clock Reading}
As shown in Figure~\ref{fig:case_study_3}, the clock reading expert is able to read the clock time accurately at a five-minute level. One important problem to be addressed is distinguishing the hour hand (generally shorter) from the minute hand (generally longer). The clock reading expert is trained well on this objective. Therefore, in the first example, the model predicts the correct time ``8:05'', while some human annotators misread the hour hand and minute hand, and thus give the wrong time reading ``1:40''. In the second example, the clock reading expert can tell the time accurately even when the hour hand and minute hand overlap. There also exist limitations for the current clock reading expert, that it can't tell more accurate time at a minute level. In the fourth example, the model only recognizes the time is about 12:10, but cannot tell the exact time of 12:12. The reason comes from casting the problem as detection and then classification, of which the clock training data is not adequate to support training of 1-minute level clock reading.

\begin{figure}[t] \centering
    \includegraphics[width=\textwidth]{figures/case_study_4.pdf}
    \caption{Case Study for Textural Recognition (OCR) and Clock Reading.}
    \label{fig:case_study_3}
\end{figure}

