\section{Experiments}
\label{sec:experiments}
We examine our method across a diverse set of input source meshes and target text prompts. We consider a variety of sources including: COSEG~\cite{coseg}, Thingi10K~\cite{Thingi10K}, Shapenet~\cite{chang2015shapenet}, Turbo Squid~\cite{turbosquid}, and ModelNet~\cite{modelnet}. Our method requires no particular quality constraints or preprocessing of inputs, and the breadth of shapes we stylize in this paper and in our project webpage  illustrates its ability to handle low-quality meshes. Meshes used in the main paper and the project webpage contain an average of 79,366 faces, 16\% non-manifold edges, 0.2\% non-manifold vertices, and 12\% boundaries. Our method takes less than 25 minutes to train on a single GPU, and high quality results usually appear in less than 10 minutes. 

In \cref{sec:control}, we demonstrate the multiple control mechanisms enabled by our method. In \cref{sec:priors}, we conduct a series of ablations on the key priors in our method.
We further explore the synergy between learning color and geometry in tandem. We introduce a user study in \cref{sec:baseline} where our stylization is compared to a baseline method. 
In \cref{sec:target}, we show that our method can easily generalize to other target modalities beyond text, such as images or 3D shapes. Finally, we discuss limitations in \cref{sec:limitations}. 

\subsection{Neural Stylization and Controls}
\label{sec:control}
Our method generates details with high granularity while still maintaining global semantics and preserving the underlying content. For example in \cref{fig:gallery}, given a vase mesh and target text `colorful crochet', the stylized output includes knit patterns with different colors, while preserving the structure of the vase. In \cref{fig:humans}, our method demonstrates a global semantic understanding of humans. Different body parts such legs, head and muscles are stylized appropriately in accordance with their semantic role, and these styles are blended seamlessly across the surface to form a cohesive texture. Moreover, our neural style field network generates structured textures which are aligned to sharp curves and features (see bricks in \cref{fig:teaser,fig:gallery} and in the project webpage). We show in \cref{fig:viewconsistency} and in the project webpage that our method styles the entire mesh in a consistent manner that is part-aware and exhibits natural variation in texture. 

\begin{figure}[h]
    \vspace{5pt}
    \centering
    \newcommand{\pl}{-3.5}
    \begin{overpic}[width=\columnwidth]{figures/freqcontrolv2.jpg}
    \put(20,  \pl){\textcolor{black}{}}
    \put(45,  \pl){\textcolor{black}{}}
    \put(75,  \pl){\textcolor{black}{}}
    \put(36,  27){\textcolor{black}{`Stained glass donut'}}
    \end{overpic}
    \caption{Increasing the range of input frequencies in the positional encoding using increasing SD  for matrix  in \cref{eq:positional}. }
    \label{fig:frequency}
\end{figure}
\noindent\textbf{Fine Grained Controls.}
Our network leverages a positional encoding where the range of frequencies can be directly controlled by the standard deviation  of the  matrix \cref{eq:positional}. In \cref{fig:frequency}, we show the results of three different frequency values when stylizing a source mesh of a torus towards the target text `stained glass donut'. Increasing the frequency value increases the frequency of style details on the mesh and produces sharper and more frequent displacements along the normal direction.
We further demonstrate our method's ability to successfully synthesize styles of varying levels of specificity. \cref{fig:textspec} displays styles of increasing detail and specificity for two input shapes. Note the retention of the style details from each level of target granularity to the next. Though the primary mode of style control is through the text prompt, we explore the way the network adapts to the geometry of the source shape. In \cref{fig:geoprior}, the target text prompt is fixed to `cactus'. We consider different input source spheres with increasing protrusion frequency. Observe that both the frequency and structure of the generated style changes to align with the pre-existing structure of the input surface. This shows that our method has the ability to preserve the content of the input mesh without compromising the quality of the stylization. 
\begin{figure}
    \centering
    \newcommand{\pl}{-3}
    \small
    \begin{overpic}[width=\columnwidth]{figures/textprogression.jpg}
    \put(16,  \pl){\textcolor{black}{(a)}}
    \put(39,  \pl){\textcolor{black}{(b)}}
    \put(62,  \pl){\textcolor{black}{(c)}}
    \put(84,  \pl){\textcolor{black}{(d)}}
\end{overpic}
    \caption{Increasing the target text prompt granularity for a source mesh of a lamp and iron. Top row targets: (a). `Lamp', (b). `Luxo lamp', (c). `Blue steel luxo lamp', (d). `Blue steel luxo lamp with corrugated metal. Bottom row targets: (a). `Clothes iron', (b). `Clothes iron made of crochet', (c). `Golden clothes iron made of crochet', (d). `Shiny golden clothes iron made of crochet'.}
    \label{fig:textspec}
\end{figure}


Meshes with corresponding connectivity can be used to \textit{morph} between two surfaces~\cite{meshmorph}.
Thus, our ability to modify style while preserving the input mesh enables morphing (see \cref{fig:morph}). To morph between meshes, we apply linear interpolation between the style value (RGB and displacement) of every point on the mesh, for each instance of the stylized mesh.
\begin{figure}[h]
    \centering
    \newcommand{\pl}{-3}
    \begin{overpic}[width=\columnwidth]{figures/morph.jpg}
    \end{overpic} 
    \caption{Morphing between two different stylizations (geometry and color). Left: `wooden chair', right: `colorful crochet chair'.}
    \label{fig:morph}
\end{figure}

\subsection{\ourmethod{} Priors}
\label{sec:priors}

Our method incorporates a number of priors that allow us to perform stylization without a pre-trained GAN. We show an ablation where each prior is removed in \cref{fig:ablations}. Removing the style field network (\textit{net}), and instead directly optimizing the vertex colors and displacements, results in noisy and arbitrary displacements over the surface. In \cite{clipdraw} random 2D augmentations are necessary to generate meaningful CLIP-guided drawings. We observe the same phenomena in our method, whereby removing 2D augmentations results in a stylization completely unrelated to the target text prompt. Without Fourier feature encoding (\textit{FFN}), the generated style loses all fine-grained details.
With the cropping augmentation removed (\textit{crop}), the output is similarly unable to synthesize the fine-grained style details that define the target. Removing the \textit{geometry-only} component of  (\textit{displ}) hinders geometric refinement, and the network instead compensates by simulating geometry through shading (see also \cref{fig:synergy}).
Without a geometric prior (\textit{3D}) there is no source mesh to impose global structure, thus, the 2D plane in 3D space is treated as an image canvas. For each result in \cref{fig:ablations}, we report the CLIP similarity score, , as defined in Sec.~\ref{sec:method}.
Our method obtains the highest score across different ablations, see \cref{fig:ablations}.  Ideally, there is a correlation between visual quality and CLIP scores. However, \textit{-3D} manages to achieve a high CLIP similarity, despite its zero regard for global content semantics. This shows an example of how CLIP may naively prefer degenerate solutions, while our geometric prior steers our method away from these solutions.
\begin{figure}[h]
    \centering
    \newcommand{\pl}{-4}
    \begin{overpic}[width=\columnwidth]{figures/geometricprior.jpg}
\end{overpic}
    \caption{Texturing input source spheres (yellow) with protrusions of increasing frequency and with a fixed target of a `Cactus'. As can be seen, the final style frequency increases accordingly.}
    \label{fig:geoprior}
\end{figure}

\noindent\textbf{Interplay of Geometry and Color.}\quad 
Our method utilizes the interplay between geometry and color for effective stylization, as shown in \cref{fig:synergy}. Learning to predict only geometric manipulations produces inferior geometry compared to learning geometry and color together, as the network attempts to simulate shading by generating displacements for self shadowing. An extreme case of this can be seen with the ``Batman" in \cref{fig:humans}, where the bat symbol on the chest is the result of a deep concavity formed through displacements alone. Similarly learning to predict only color results in the network attempting to hallucinate geometric detail through shading, leading to a flat and unrealistic texture that nonetheless is capable of achieving a relatively high CLIP score when projected to 2D. \cref{fig:synergy} illustrates this adversarial solution, where the ``Color" mode achieves a similar CLIP score as our ``Full" method.  
\begin{figure}[h]
    \centering
    \newcommand{\pl}{-3}
    \newcommand{\vl}{-8}
    \begin{overpic}[width=\columnwidth]{figures/geocolor_interplay.jpg}
    \put(35,  50){\textcolor{black}{`Alien made of bark'}}
    \put(26,  \pl){\textcolor{black}{Full}}
    \put(50,  \pl){\textcolor{black}{Geometry}}
    \put(81,  \pl){\textcolor{black}{Color}}
\put(25,  \vl){\clipcolor{}}
    \put(52,  \vl){\clipcolor{}}
    \put(81,  \vl){\clipcolor{}}
    \end{overpic}
    \vspace{1pt}
    \caption{Interplay between geometry and color for stylization. 
\textit{Full} - our method, \textit{Color} - only color changes, and \textit{Geometry} - only geometric changes. We also display the {\clipcolor{CLIP similarity}}. }
    \label{fig:synergy}
\end{figure}


\subsection{Stylization Fidelity}
\label{sec:baseline}

Our method performs the task of general text-driven stylization of meshes. 
Given that no approaches exist for this task, we evaluate our method's performance by extending VQGAN-CLIP~\cite{vqganclipnotebook}. 
This baseline synthesizes color inside a binary 2D mask projected from the 3D source shape (without 3D deformations) guided by CLIP. Further, the baseline is initialized with a rendered view of the 3D source.
We conduct a user study to evaluate the perceived quality of the generated outputs, the degree to which they preserve the source content, and how well they match the target style.
 

\begin{table}[h]
\newcommand{\allcolor}{\color[rgb]{0.4,0.4,0.95}}
\centering
\begin{tabular}{lccc} \\ 
\toprule
& (Q1): Overall & (Q2): Content & (Q3): Style  \\
\toprule
VQGAN & 2.83 \small{()}&  3.60 \small{()} & 2.59 \small{()} \\
Ours & \textbf{3.90} \small{()} &  \textbf{4.04} \small{()} & \textbf{3.91} \small{()} \\
\bottomrule
\end{tabular}
    \caption{Mean opinion scores (1-5) for Q1-Q3 (see~\cref{sec:baseline}), for our method and baseline (control score: 1.16).
    }
\label{tab:user}
\end{table}
We had 57 users evaluate  random source meshes and style text prompt combinations. For each combination, we display the target text and the stylized output in pairs. The users are then asked to assign a score (1-5) to three factors: (Q1) ``How natural is the output depiction of \{\textit{content}\} + \{\textit{style}\}?" (Q2) ``How well does the output match the original \{\textit{content}\}?" (Q3) ``How well does the output match the target \{\textit{style}\}?" We report the mean opinion scores with standard deviations in parentheses for each factor averaged across all style outputs for our method and the baseline in \cref{tab:user}.  
We include three control questions where the images and target text do not match, and obtain a mean control score of 1.16. Our method outperforms the VQGAN baseline across all questions, with a difference of , , and  for Q1-Q3, respectively. 
Though VQGAN is somewhat effective at representing the natural content in our prompts, perhaps due to the implicit content signal it receives from the mask, it struggles to synthesize these representations with style in a meaningful way. 
Examples of our baseline outputs are provided in the  \cref{sec:baselinecomp}.
Visual examples of generated styles and screenshots of the user study are also discussed in \cref{sec:baselinecomp}.

\label{sec:modality}
\begin{figure}[h]
    \centering
    \newcommand{\pl}{-3}
    \newcommand{\targetcolor}{\color[rgb]{0.1,0.33,0.57}}
    \begin{overpic}[width=\columnwidth]{figures/imagetarget.jpg}
    \end{overpic} 
    \caption{Stylization driven by an {\targetcolor image target}. Our method can stylize meshes using an image to describe the desired style.}
    \label{fig:target}
\end{figure}
\subsection{Beyond Textual Stylization}
\label{sec:target}
Beyond text-based stylization, our method can be used to stylize a mesh toward different target modalities such as a 2D image or even a 3D object. For a target 2D image ,  in \cref{eq:fullloss}, represents the image-based CLIP embedding of . For a target mesh ,  is the average embedding, in CLIP space, of the 2D renderings of , where the views are the same as those sampled for the source mesh. 
Beyond different modalities, we can combine targets across different modalities by simply summing  over each target. In Fig.~\ref{fig:target} we consider a source mesh of a pig with different image targets. In Fig.~\ref{fig:mesh2mesh}(a-b), we consider stylization using a target mesh and in Fig.~\ref{fig:mesh2mesh}(c-d), we combine both a target mesh and a target text. Our method successfully adheres to the target style. 

\begin{figure}[h]
    \centering
    \vspace{-5pt}
    \newcommand{\pl}{-2}
    \begin{overpic}[width=\columnwidth]{figures/m2m.png}
    \put(10,  \pl){\textcolor{black}{(a)}}
    \put(29,  \pl){\textcolor{black}{(b)}}
    \put(40,  \pl){\textcolor{black}{Target 1}}
    \put(60,  \pl){\textcolor{black}{(c)}}
    \put(75,  \pl){\textcolor{black}{(d)}}
    \put(88,  \pl){\textcolor{black}{Target 2}}
    \end{overpic}
    \caption{Neural stylization driven by mesh targets. (a) \& (c) are styled using Targets 1 \& 2, respectively. (b) \& (d) are styled with text in addition to the mesh targets: (b) `a cactus that looks like a cow', (d) `a mouse that looks like a duck'. }
    \label{fig:mesh2mesh}
\end{figure}

\begin{figure}[h]
    \centering
    \newcommand{\pl}{-2}
    \begin{overpic}[width=\columnwidth]{figures/symmetry.jpg}
    \put(12,  \pl){\textcolor{black}{Input}}
    \put(32,  \pl){\textcolor{black}{No Symmetry Prior}}
    \put(70,  \pl){\textcolor{black}{Symmetry Prior}}
    \end{overpic}
    \caption{Effect of the symmetry prior on a UFO mesh input with text prompt: `colorful UFO'. }
    \label{fig:symmetry}
\end{figure}

\subsection{Incorporating Symmetries}
We can make use of prior knowledge of the input shape symmetry to enforce style consistency across the axis of symmetry. Such symmetries can be introduced into our model by modifying the input to our positional encoding in \cref{eq:positional}. For instance, given a point  and a shape with bilateral symmetry across the X-Y plane, one can apply a function prior to the the positional encoding such that . We show the effect of this symmetry prior on a UFO mesh in \cref{fig:symmetry}. This prior is effective even when the triangulation is not perfectly symmetrical, since the function is applied in Euclidean space. A full investigation into incorporating additional symmetries within positional encoding is an interesting direction for future work. 

\subsection{Limitations}
\label{sec:limitations}
Our method implicitly assumes there exists a synergy between the input 3D geometry and the target style prompt (see \cref{fig:limitations}). However, stylizing a 3D mesh (\eg, dragon) towards an unrelated/unnatural prompt (\eg, \textit{stained glass}) may result in a stylization that ignores the geometric prior and effectively erases the source shape content. Therefore, in order to preserve the original content when editing towards a mismatched target prompt, we simply include the object category in the text prompt (\eg, \textit{stained glass dragon}) which adds a content preservation constraint into the target. 
\begin{figure}
    \centering
    \newcommand{\pl}{-3}
    \newcommand{\tl}{1}
    \begin{overpic}[width=\columnwidth]{figures/limitations_fig.png}
        \put(5,  \pl){\textcolor{black}{`Stained glass'}}
        \put(35,  \tl){\textcolor{black}{`Stained glass}}
        \put(39,  \pl){\textcolor{black}{{dragon'}}}
        \put(63,  \pl){\textcolor{black}{`Candy'}}
        \put(82,  \tl){\textcolor{black}{`Candy}}
        \put(80,  \pl){\textcolor{black}{Aramdillo'}}
    \end{overpic} 
    \caption{Geometric content and target style synergy. If the target style is unrelated to the 3D mesh content, the stylization may ignore the 3D content. Results are improved when including the content in the target text prompt.}
    \label{fig:limitations}
\end{figure}