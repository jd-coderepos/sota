\subsection{Textual workflow languages}
\label{Textual}

This class of SWfMS encompasses textual languages which facilitate the enactment of large workflows in distributed compute infrastructures. In contrast to batch scripts, these languages provide support for highly parallel computation. As they were mostly designed to run on heterogeneous, geographically distributed compute resources, considerable administration effort is required for installation and utilization. Hence, the focus of these systems lies less on ease of use and more on efficient computing of high workloads. Like in a batch script, workflows are specified in a text file (like XML) according to a proprietary notation defined by the SWfMS.

\subsubsection{Swift}
Among the pioneers of parallel workflow execution is the Swift parallel scripting language~\citep{Zhao07}. Swift provides a functional language in which workflows are modeled as a set of program invocations with their associated command-line arguments as well as input and output files. Swift scripts are oblivious to the runtime environment and can be executed on local multicore computers, clusters, grids, or clouds. The execution engine of Swift supports task parallelism, as a task is dispatched for execution in a distributed environment as soon as all its input parameters are available. As data dependencies are resolved, the workflow is expanded dynamically at runtime.

For each known compute resource, the Swift execution engine maintains a score which increases with each successful task execution and decreases with each failed attempt~\citep{Wilde2011}. Tasks are assigned reactively to compute resources at runtime and the higher the score of a resource the more tasks will be assigned to it. Hence, changes in the performance of compute nodes are reflected in the score and taken into account by the scheduler. While scheduling reacts to alterations in the workload of a resource, it can't utilize elasticity in the form of provisioning of additional compute resources at runtime. Swift implements capabilities for failure recovery (retry, restart, and replication) as well as provenance tracking.

In summary, Swift implements adaptive scheduling and task parallelism on arbitrary compositions of local machines and clusters as well as grid and cloud infrastructures, as shown in Table~\ref{parallelism}. Swift has been utilized for computationally intensive applications from various fields of science, including physics and the life sciences. For an overview of computationally intensive scientific applications implemented in a SWfMS discussed in this survey, see Table~\ref{applications}.

\begin{table*}[hbtp]
	\caption[Scientific applications implemented SWfMS]{Scientific applications implemented in a SWfMS}\label{applications}
\medskip
\centering
	\begin{tabular}{llllll}
		\toprule
		SWfMS&life sciences&physics&astronomy&geography\\
		\midrule
		Swift&\citep{Stef-Praun2007,Hasson2008,DeBartolo2010,Lee2010,Adhikari2012}&&&\citep{Agarwal2011,Woitaszek2011}\\
		Pegasus&\citep{Wang2011,Mehta2011}&\citep{Brown2006}&\citep{Berriman2004}&\citep{Deelman2006}\\
		Taverna&\citep{Kell2009,Maleki-Dizaji2009,Zhou2009}&&&\\
		Kepler&\citep{Michener2007}&\citep{Podhorszki2007}&&\citep{Barseghian2010}\\
		\bottomrule
	\end{tabular}
\end{table*}

\subsubsection{Condor DAGMan}
Condor~\citep{Litzkow88} is a batch job scheduler for high-throughput computing on distributed resources. It was originally designed to scavenge idle workstations for CPU cycles, yet has been extended with functionality to interface grid resources via Condor-G. Condor puts a strong emphasis on reliability of execution in the form of job checkpointing, recovery, and migration. Condor provides a resource allocation language called \textit{ClassAds}~\citep{Thain2005} (``classified advertisements'') to describe requirements and preferences for matchings between tasks and compute nodes. For instance, ClassAds can be utilized to describe that a machine only accepts tasks if its current workload is low and it has been idle for an hour or that a task prefers to be executed on a machine with good floating point performance. Condor maintains a queue of tasks in which new tasks are scheduled on the compute resource with the best matching ClassAd. Scheduling in Condor can therefore be considered adaptive, provided the ClassAds are specified such that machines with a high workload are less likely to be assigned new tasks.


Condor's Directed Acyclic Graph Manager (DAGMan)~\citep{Couvares2007} provides the means to textually specify a workflow as a DAG describing a set of tasks along with their data interdependencies. DAGMan operates as a Condor job and supervises workflow execution, submitting workflow tasks which are ready for execution to Condor one at a time. Data parallelism and pipelining are not natively supported since a task is not submitted for execution until all of its input data is available. DAGMan does not communicate with the Condor scheduler, hence advanced workflow characteristics such as task runtime estimates are not considered in scheduling. Since DAGMan does not manage the movement of intermediate data products between jobs, data movements have to be explicitly specified within the workflow DAG. Similar to Condor, DAGMan emphasizes reliability. In case of failure, DAGMan compiles a rescue DAG from which execution can be resumed.

As shown in Table~\ref{parallelism}, the SWfMS DAGMan supports adaptive scheduling and parallel task execution on a grid infrastructure. Since a compute grid might not be available to some users, efforts have been made to execute DAGMan workflows on local compute infrastructure without Condor (e.g., \citep{Schmidt2012}).

\subsubsection{Pegasus}
DAGMan does not provide the means to automatically set up auxiliary tasks, such as data movement, cleanup, or workflow optimization. Therefore, \citet{Deelman05} developed Pegasus as a data-aware layer on top of DAGMan, which introduces capabilities for provenance tracking, execution monitoring, and failure recovery. The SWfMS Pegasus can be viewed as a collection of DAG transformers which iteratively translate a concrete workflow into an executable DAGMan workflow. To this end, Pegasus queries and maintains catalogs of available computational resources, data replicates, and data processing software/services. Pegasus also prunes and optimizes workflow structure by omitting tasks for which output data has been computed previously and by clustering short-running tasks into joint DAGMan jobs.

Tasks in the DAGMan input file generated by Pegasus are location-specific, i.e.,~Pegasus overrides the default scheduling mechanism of Condor (\textit{ClassAds}). Instead, Pegasus provides four different scheduling strategies by default:

\begin{itemize}
	\item \textit{Random}: Tasks are randomly assigned to compute resources able to execute them.
	\item \textit{Round-Robin}: Tasks are evenly distributed among resources, independent of the associated computational cost.
	\item \textit{Group}: Tasks can be put into user-defined groups. Each group of tasks is scheduled to run on the same compute resource.
	\item \textit{HEFT}: The Heterogeneous Earliest Finishing Time scheduling heuristic~\citep{Topcuoglu2002} described in Section~\ref{Scheduling}. Pegasus assumes default costs for data communication, whereas the runtime for tasks has to be specified by the user.
\end{itemize}

All these scheduling strategies result in a static schedule. Since the workload on the compute infrastructure might be subject to change, \citet{Lee2009} developed an adaptive scheduling mechanism for Pegasus. Here, job queues on execution sites are observed and compared. In case of sustained discrepancies, Pegasus re-schedules the workflow from the current point of execution.

Pegasus was originally designed to distribute computationally intensive workflows across grid infrastructures. However, the growing interest in cloud computing has led to efforts to efficiently run Pegasus in a cloud infrastructure~\citep{Hoffa08,Juve09,Juve11}. In its productive version, Pegasus supports task parallel execution of scientific workflows on grid and cloud infrastructures using a static schedule (see Table~\ref{parallelism}). Several computationally intensive workflows from different scientific domains have been implemented and executed in Pegasus (see Table~\ref{applications}).

\subsubsection{Other textual workflow languages}
While aforementioned textual workflow languages can manage execution of thousands of tasks in parallel, evaluation of workflow files (e.g., Swift scripts) is performed on a single node. In the light of ever-increasing concurrency in today's computing systems, this may constitute a bottleneck for workflows consisting of very large numbers of short-running tasks. For this reason, \citet{Wozniak2012} implemented Turbine, an extreme-scale workflow management system with an execution engine distributed over multiple compute nodes, which is responsible for resolving data dependencies, managing task distribution and load balancing, and providing global data storage. Turbine can compile Swift scripts and has been shown to be able to distribute more than 20,000 workflow tasks per second.

\citet{Islam2012} found many established workflow management systems to lack in scalability. Hence, they developed Oozie, a scalable workflow scheduler which is built on top of Hadoop~\citep{White10}, the open source implementation of the MapReduce~\citep{Dean08} programming paradigm. The Oozie server accepts textually specified workflow DAGs submitted by multiple Oozie clients, splits these workflows into sub-tasks, and dispatches the sub-tasks to a Hadoop cluster for processing. Oozie is highly scalable and has been utilized by Yahoo! for execution of more than 770,000 workflows.

\citet{Ogasawara11} recently presented a relational algebra that facilitates highly scalable scientific workflow execution. In their work, data is represented in the form of relations. Workflow tasks, such as external program or service invocations, are interpreted as one of four operators. These operators -- \textit{Map}, \textit{SplitMap}, \textit{Reduce}, and \textit{Filter} -- are characterized by the ratio at which they consume and produce tuples of input and output data. Workflows modeled as compositions of these operators can be structurally optimized by applying concepts of database query optimization, such as predicate pushdown. The relational view on data enables the scheduler to automatically distribute fragments of the workflow and parts of the data among several processing nodes, resulting in a task and data parallel execution. Different scheduling strategies have been shown to yield runtime gains, depending on workflow structure and its opportunities for structural optimization.

\begin{figure*}[t]
  \centering
  \includegraphics[width=\textwidth]{Dataflow.pdf}
  \caption[...]{Stack architecture of the dataflow languages DryadLINQ, Meteor, Pig, Hive, and AQL.}
  \label{dataflow}
\end{figure*}

Instead of designing workflows \textit{ad hoc} in a way that allows for data parallel execution, \citet{DeOliveira2010} proposed to outsource only the most computationally taxing workflow tasks to external resources. Following this rationale, they developed \mbox{SciCumulus}, which can be described as a cloud-based scientific workflow middleware. Scientists can wrap computationally intensive tasks of their existing workflows into \mbox{SciCumulus} cloud activities. To this end, \mbox{SciCumulus} provides components for upload, dispatch, download, and provenance capture. These components can be interfaced from within a SWfMS like Taverna, Pegasus, or Kepler. By using predefined or custom cartridges, users can specify how data is to be fragmented before and merged after processing for data parallel computation. \mbox{SciCumulus} can also be employed to perform parameter sweeps on workflow components in parallel.

\citet{Cieslik11} developed PaPy (``Parallel pipelines in Python''), a light-weight and modular textual SWfMS in which workflow tasks are specified as Python functions. PaPy supports task parallelism in the form of a worker pool distributed among local and remote resources. Furthermore, by allowing input data to be split and processed in chunks, PaPy can be configured to perform data parallel computation. 



\subsubsection{Massively data parallel query languages}
Subsequent to the publication and wide-spread adoption of Google's MapReduce~\citep{Dean08} programming model and its open source implementation Hadoop~\citep{White10}, a class of systems that can be summarized under the term ``dataflow languages'' have emerged. These textual languages including DryadLINQ~\citep{Yu08} from Microsoft, Stratosphere's Meteor~\citep{Heise2012}, Pig~\citep{Olston08} from Yahoo!, Hive~\citep{Thusoo2009} from Facebook, and the Asterix Query Language (AQL)~\citep{Behm2011} from UC Irvine were designed to efficiently perform query-style dataflow programs over extremely large data in parallel.

Dataflow programs specified in these languages are translated into DAGs of Dryad vertices, Stratosphere parallelization contracts (PACTs~\citep{Battre10}), Hadoop \textit{map} and \textit{reduce} jobs, or Hyracks~\citep{Borkar2011} operators. Similar to scientific workflows, these DAGs describe data dependencies between separate data processing steps. Data parallel execution of these DAGs is handled by the execution engines underlying the Dryad, Hadoop, and Hyracks implementation or -- in the case of Stratosphere -- the Nephele~\citep{Warneke09} scheduler. See Figure~\ref{dataflow} for a graphical overview of the stack architecture of aforementioned systems.



While all of these dataflow languages provide the means to model computationally intensive problems, they were not primarily designed for scientific data. Thus, in contrast to most SWfMS, they neither provide domain-specific software libraries nor are there any published workflows designed by domain scientists. Furthermore, while each task in a scientific workflow is treated as a black box, dataflow languages often require the user to specify the task according to a restrictive query-based syntax, which is typically not easily accessible by non-computer-savvy users. For these reasons, while MapReduce and similar frameworks have an undeniable influence on parallelization in SWfMS, they are out of scope for this survey.

\looseness=-1However, Dryad~\citep{Isard07} and Nephele~\citep{Warneke09}, the execution engines underlying the dataflow languages DryadLINQ and Meteor, respectively, can both be utilized for execution of arbitrary workflows. Similar to DAGMan or Pegasus, Dryad maps abstract workflows specified as DAGs to their concrete compute resources. However, while in Pegasus tasks can only exchange data in the form of files, Dryad also supports network or shared memory communication. These features enable Dryad to perform a pipeline parallel execution in which tasks start execution as soon as at least one input record is available at each input port. Dryad tasks are written in C++ and tasks can be annotated with preferences or constraints towards the resources on which they are to be executed. This puts data-awareness into the hands of the user, since the default scheduling strategy of Dryad is a job queue in which new tasks are greedily assigned to the first available resource. 

The Dryad framework was developed with a static computation environments in mind. In contrast, Nephele~\citep{Warneke09} also supports distribution of tasks on dynamically scalable compute infrastructures, such as compute clouds. Similar to Dryad, Nephele supports task communication in the form of shared memory and TCP network connections. Users can also specify to cluster several short-running tasks on a single compute resource or split long-running tasks into subtasks for data parallel execution. If no annotations are provided by the user, the Nephele scheduler assigns each task to its own compute resource. 

In summary, Dryad and Nephele support task, data, and pipeline parallelism on multicore architectures and compute clouds using a job queue for just-in-time scheduling.













