\documentclass[a4paper]{article}


\usepackage[textsize=tiny]{todonotes}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{breqn,bm}
\usepackage{lipsum}

\usepackage{booktabs} 




\usepackage{xcolor} \usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{eso-pic} \usepackage{forloop}
\usepackage{url}


\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm,nicefrac}



\renewcommand{\topfraction}{0.9}\renewcommand{\bottomfraction}{0.8}\renewcommand{\textfraction}{0.05}
\renewcommand{\floatpagefraction}{0.99}

\usepackage[capitalize,noabbrev]{cleveref}


\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{loss}[theorem]{Loss}
\newtheorem{scc}[theorem]{Stopping Criterion}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\allowdisplaybreaks


\usepackage{INTERSPEECH2022}[accepted]

\title{SepIt: Approaching a Single Channel Speech Separation Bound}
\name{Shahar Lutati, Eliya Nachmani, Lior Wolf}
\address{
  Tel-Aviv University\quad Meta AI Research}
\email{shahar761@gmail.com, enk100@gmail.com, wolf@cs.tau.ac.il}

\begin{document}

\maketitle
\begin{abstract}
We present an upper bound for the Single Channel Speech Separation task, which is based on an assumption regarding the nature of short segments of speech. Using the bound, we are able to show that while the recent methods have made great progress for a few speakers, there is room for improvement for five and ten speakers. We then introduce a Deep neural network, SepIt, that iteratively improves the different speakers' estimation. At test time, SpeIt has a varying number of iterations per test sample, based on a mutual information criterion that arises from our analysis. In an extensive set of experiments, SepIt outperforms the state of the art neural networks for 2, 3, 5, and 10 speakers.
\end{abstract}
\noindent\textbf{Index Terms}: speech separation, single channel, deep learning

\section{Introduction}
Single Channel Speech Separation (SCSS) problem is a specific setting of the general Blind Source Separation (BSS) problem. By employing deep neural networks, great improvement has been achieved in the last few years in separating two and three speakers~\cite{luo2020dualpath,nachmani2020voice}, leading to the very recent state of the art \cite{subakan2021attention}. Somewhat disappointingly, in terms of model sizes versus improvement obtained, the model size has more than tripled ( \cite{subakan2021attention} vs  \cite{nachmani2020voice}), while the improvement as measured when training on same dataset without dynamic mixing is only . One may, therefore, wonder whether additional improvement in performance is possible. This drives the need for a theoretical upper bound for SCSS. 

SCSS has unique characteristics in comparison to other BSS problems. The speech signal is not stationary, unless short segments are considered. Jensen et al. \cite{Jensen} have shown empirically that when the length of the speech segment is longer than , the distribution is closer to the Laplace distribution than to the normal distribution. As a result, the known bounds for BSS~\cite{4202618} do not hold. In this work, we derive such a bound by employing the assumption of a Laplacian distribution. Treating the speech mixture as a random process, we derive a bound that expresses the maximum achievable Signal to Distortion Ratio (SDR) by any neural network. We then present a deep learning method called SepIt, which uses the bound during its training. 

In an extensive set of experiments, we validate the assumption made in our analysis, as well as the bound itself. We then show that the SepIt model outperforms the state of the art in separating two and three speakers for the WSJ \cite{garofolo1993csr} benchmark and five and ten speakers for the LibriMix \cite{cosentino2020librimix} benchmark.

\smallskip
\noindent{\bf Related Work\quad} SCSS using deep learning techniques has been explored intensively in the last years. Erdogan et al. \cite{7178061} introduced a phase-sensitive loss function trained using a LSTM neural network. Hershey et al. \cite{hershey2016deep} developed the WSJ-2mix benchmark and proposed a neural separation network with a clustering-based embedding. The TasNet architecture \cite{luo2018tasnet} employs a time domain encoder-decoder. Subsequently, Luo et al. \cite{luo2019conv}  introduced the ConvTasNet architecture based on a convolutional neural network with masking. ConvTasNet was further improved with the dual-path recurrent neural network (DPRNN) architecture by \cite{luo2019dual}. Zeghidour et al. \cite{zeghidour2020wavesplit} introduced Wavesplit, which uses clustering on speaker representation to separate the mixture. Nachmani et al. \cite{nachmani2020voice} proposed the VSUNS model, which is based on the DPRNN model, but removes the masking sub-network. The SepFormer architecture is a transformer-based architecture that captures both short- and long-term dependencies \cite{subakan2021attention}. Yao et al. \cite{yao2022stepwise} introduced a coarse-to-fine framework called the Stepwise-Refining Speech Separation Network (SRSSN). Hu et al. \cite{hu2021speech} presented the Fully Recurrent Convolutional Neural Network (FRCNN) architecture, which uses lateral connections to fuse information processed at various timescales. 

Independently, upper bounds have been developed for the BSS problem. Sahlin et al. \cite{534883} derived an asymptotic Cramer Rao bound for the MIMO case, which assumes a full-rank channel matrix. Doron et al. \cite{4202618} presented a Cramer Rao bound for Gaussian sources. Kautsky et al. \cite{2020} perform an analysis for both the Gaussian and non-Gaussian sources, presenting a Cramer Rao induced bound. However, the analysis only addresses linear operations over the mixture signal. As far as we can ascertain, no upper bound for non-Gaussian sources exists for non-linear methods such as Deep Neural Networks.

\section{SCSS Upper bound}
\label{sec:upper_bound}

We start by formulating the required lemmas and stating the assumption we employ. \vspace{-2mm}
\begin{lemma}[Fisher Information Upper Bound]
From \cite{Brunel} the Fisher Information  is upper bounded by the Mutual Information :

\label{lemma:fisher_bound}
\end{lemma}
\vspace{-6mm}
Where  are random variables as defined in \cite{Brunel}.


\begin{lemma}[Joint Estimation Lemma]
For  i.i.d random variables with parameter , the joint Fisher Information is

\label{lemma:joint_fisher}
\end{lemma}
\vspace{-3mm}
Jensen et al. \cite{Jensen} demonstrated empirically that speech, divided into short segments, neglecting quiet segments, is well modeled as a stationary process. Furthermore, short segments of a single speaker follow a Laplace random variable. 
\vspace{-2mm}
\begin{assumption}Short speech segments can be captured by a Laplace random variable with zero mean and scale parameter .

\label{asmp:laplace}
\end{assumption}
\vspace{-4mm}



\noindent \textbf{Mixture Model\quad }
Let the different speakers' signals in the mixture compose a matrix 
where  is a single speaker signal in the time domain, with length . To maintain the stationary requirements we use windowing of size  as in Assumption ~\ref{asmp:laplace}.
The mixture  is modeled as a linear combination of the columns of , 
where  are the mixture coefficients.
\begin{definition}[Signal to Distortion Ratio (SDR)]
Let the error  be defined as .
The Signal to Distortion Ratio (SDR) is,


\label{defintion:sdr}
\end{definition}
Denote a single segment of size  from  as , and a single segment of  as  , where . Without loss of generality, assume that the goal is to separate  from the mixture . Denote by  the estimation of , using deep neural network, .









\begin{lemma}
Given a single segment , and deep neural network, , the following inequality holds,

\end{lemma}
\begin{proof}

Considering the Markov chain  and using the Data Processing Theorem,

For neural network  that estimates jointly all of the segments we have from Lemma ~\ref{lemma:joint_fisher},

Recall that  is the estimator of  from . Referring  as  in Lemma ~\ref{lemma:fisher_bound} gives,

Plugging Eq.\ref{eq:fisher_ineq}, and Eq.\ref{eq:seg_fisher} gives,

\end{proof}

Based on this lemma, we obtain the following bound.
\begin{theorem}[SDR upper bound]
\label{thm:main}

Let , , ,  be defined as previously stated.
For any neural network, the SDR upper bound that separates a mixture of  speakers is given by:

\label{thrm:uppr_bnd}
\end{theorem}
\begin{proof}

The Cramer Rao Lower Bound \cite{Rao1992} states that for any unbiased estimator there is a lower bound on the estimator square error i.e. for estimator of 


Applying Eq.~\ref{eq:crb_v} to Definition.~\ref{defintion:sdr} gives an upper bound for estimating the single segment:
\begin{dmath}
    SDR(v_{0,r},\bar{v}_{0,r}) \leq 10log10(Var(v_{0,r})J(\bar{v}_{0,r}))
    \label{sdr_term}
\end{dmath}
Using the upper bound for the Fisher information Lemma.\ref{lemma:fisher_bound} yields:

The data processing inequality states the the mutual information is reduced after each processing, thus we obtain
the following:

Using Lemma.\ref{lemma:joint_fisher} gives the upper bound:


\end{proof}
Theorem~\ref{thrm:uppr_bnd} states that the upper bound for separation algorithm, is a function of: (i) the number of speakers , (ii) the number of segments that jointly estimated , (iii) the mutual information . 

Specifically, the bound is linear with , the length of the sequence. Furthermore, it is inversely proportional to the segment length . There is a trade-off between  and the lowest frequencies in each segment, thus, decreasing  also decreases .


A clear limitation of the bound is that it holds only for a network  that jointly process i.i.d stationary segments. This is not the case if a neural network processes the entire signal without segmentation. However, current architectures, including the attention architecture, tend to be myopic.


\section{Method}
\label{sec:method}
Our method, denoted as as SepIt, consists of sequential processing of the estimated signals, where each iteration contains a replica of the basic model.
Consider again the mixture signal, , consisting of  speakers and of length .
First, a backbone network, , is used for initial separation estimation. The backbone can change between different data sets, and it does not learn during the training phase of the SepIt model. 
Expanding the notation in Sec.~\ref{sec:upper_bound}, denote the estimation of the -th speaker in the -th iteration as . Collectively, we denote the list of obtained speakers as . The output of the backbone, , is .

Where  is all speaker estimation.

Both the current estimation, , and the mixture, , pass through an encoder, , of a 1-D convolution with  channels, kernel size  and stride , followed by a ReLU activation function:
\begin{dmath}
    \Acute{m} = E(m)
\end{dmath}
\begin{dmath}
    \Acute{v}^{j-1} = E(\bar{v}^{j-1})
\end{dmath}
\begin{figure}[b]
    \centering
     \includegraphics[width=1\linewidth]{sepit_arch.PNG}
    \caption{The SepIt block architecture.
}
    \label{fig:sepitblock}
\end{figure}
Where   are the latent space for the mixture and current speech estimations, respectively, and  is the strided latent dimension.
The encoder of  shares its weights with the encoder of .

The latent estimation  is treated as a prior on the latent masking. A deep neural network, , is used to evaluate a latent space mask.
 consists of 3 ResBlocks, each a 1-D version of the Residual block from ResNet, each with  channels, kernel size of 3, and ReLU activation function. 
The latent mask is multiplied element-wise by the mixture latent representation. 
To subtract residuals from other sources, a  Convolution layer  with  channels is then introduced. The procedure is:

where  is the latent space residual, and  is the element-wise product operator.
The latent residual  is passed to a decoder layer, . Later, a skip-connection is introduced, such that the -th iteration estimation is the summation of the decoded  and the previous estimation,

A single SepIt block is depicted in Fig~\ref{fig:sepitblock}.
The training algorithm is depicted in Alg.~\ref{alg:diffalg}. The algorithm starts with the initialization of the threshold  to 1. The zero-th iteration is set using the backbone model as in line~\ref{lst1}. Next, the algorithm evaluates the SepIt neural network over the previous iteration, and takes a gradient descent step over the loss function. The threshold  is updated by the difference between  and . 

The algorithm proceeds to apply the SepIt neural network until the maximum number of iterations is reached or the threshold is lower or equal to zero. This stopping criterion follows the derivation of Thm.\ref{thrm:uppr_bnd}, which indicates that the mutual information between the speaker estimation  and the mixture  can indicates the maximal achievable SDR. Thus, a per-sample stopping criterion is derived when  stops increasing relative to previous iteration . This stopping criterion can be computed also on the test-set samples since it does not require the ground truth.

\textbf{Loss Function} Following \cite{roux2018sdr}, a Scale Invariant Signal to Distortion Ratio (SI-SDR) is used as the loss function, which is a similar loss to SDR, but demonstrated better convergence rate. The overall loss function is given by:

where  is the estimated source, , and .


\begin{algorithm}[h]
\caption{The SepIt algorithm. \\ \textbf{Input:}  - mixture signal,  - number of speakers,  - Backbone network, MAXITER - maximum number of iterations \\
\textbf{Output:}  - estimated separated speakers.}
\label{alg:diffalg}
\begin{algorithmic}[1]
\STATE   
\STATE  
\STATE  \label{lst1}
\STATE 
\WHILE{ and } \label{lst:line:blah2}
\STATE  
\STATE  \STATE Take gradient descent step on 
\STATE 
\STATE 
\ENDWHILE\label{diffsepdwhile}
\STATE \textbf{return} 
\end{algorithmic}
\end{algorithm}

\section{Experiments}
\label{sec:exp}


\noindent{\bf Validating Assumption \ref{asmp:laplace}\quad} This assumption is extensively validated by \cite{Jensen}, yet we re-evaluated it again over the LibriMix training set. The LibriMix dataset containing over  hours of speech is investigated. Each speaker signal is split into different non-overlapping segments, each  long. Fig.~\ref{fig:dist_compare} compares the empirical PDF with the best fitted Laplace distribution and normal distribution. Evidently, the Laplacian distribution provides a much better fit, The Kullback–Leibler that presented in the figure supports this conclusion too. 


\begin{figure}[h]
     \includegraphics[width=0.85\linewidth,height=0.6\linewidth]{check_for_laplace_distributed_voice_1.png}
    \caption{Distribution comparison of empirical PDF, best fitted normal and Laplace distribution. X axis - the signal sample amplitude, Y-axis the PDF of the sample amplitude. The Laplace distribution is the best fit to the empirical PDF}
    \label{fig:dist_compare}
\end{figure}





\begin{figure*}[t]
    \centering
     \includegraphics[width=0.8\linewidth,height=0.38\linewidth]{sdr_res.png}
    \caption{The upper bound for single channel source separation (blue curve).
SepIt, denoted by a pink triangle, produces state-of-the-art results.}
    \label{fig:sdr_res}
\end{figure*}














\noindent\textbf{Separation results\quad}
For all experiments, the Adam \cite{kingma2017adam} optimizer is used with a learning rate of  and a decay factor of 0.95 every epoch. The window size  and speech length . Other hyperparameters are summarized in Tab.~\ref{tab:model_size}.
The experiments are divided into two categories based on their Backbone architecture: (i) Transformer -based and (ii) LSTM-based. Each backbone is used in the range for which it is currently the state of the art.
We use dynamic mixing augmentation, introduced in \cite{zeghidour2020wavesplit}, which consist of creating new mixtures in run time, noted as \textbf{DM}.

\begin{table}[h!]
\caption{Hyperparameters for the different number of speakers.}
    \label{tab:model_size}
\centering \begin{tabular}{@{}l@{~}c@{~}c@{~}c@{~}|l@{~}c@{~}c@{~}c@{}} 
 \toprule
No. speakers &  N & K & Params[M]& No. speakers &  N & K & Params\\
\midrule
2 & 256& 4 & 4.6& 5 & 128 & 4& 1.95\\
\midrule
3 & 256& 4 & 7.2&10 & 128& 4 & 2.7\\
\bottomrule
 \vspace{-7mm}
\end{tabular}


\end{table}
For all experiments, 5 iterations are conducted, where \textbf{SC} implies that the stopping criterion per sample is activated. Fig.~\ref{fig:iter_stop} depicts the behavior of the criterion on  a typical sample from the test set.
 \begin{figure}
 \centering
\begin{tabular}{c}
\includegraphics[width=0.85\linewidth]{early_stopping.png}
 \end{tabular}
 \vspace{-1mm}
 \caption{SDR improvement for  speakers and the Mutual information between the mixture and the estimated speaker.}
\label{fig:iter_stop}
\end{figure}

\textbf{Transformer Backbone}
As seen from the upper bound, the current state-of-the-art results are close to the bound. Thus, only a small improvement is expected when experimenting above it with SepIt. The results using a current state-of-the-art Transformer network \cite{subakan2021attention} as  over the Wall Street Journal Mix  (WSJ0-2/3 Mix) are summarized in Tab.~\ref{tab:results_2_3spkrs}.

\begin{table}[h!]
\caption{The performance obtained by our method, as well as the Transformer backbone network. See text for details.}
    \label{tab:results_2_3spkrs}
\centering \begin{tabular}{@{}l@{~}cc@{}} 
 \toprule
& \multicolumn{2}{c}{SI-SDRi {}}\\
 \cmidrule(lr){2-3}Method &  WSJ0-2Mix  & WSJ0-3Mix \\
\midrule
Upper Bound & 23.1 & 21.2\\
 \midrule
(i) SepFormer &20.4 & 17.6\\
(ii) SepFormer + DM &22.3 & 19.8\\
\midrule
{\bf(iii) SepIt + SepFormer + DM} &{\bf22.4} & {\bf20.1}\\
{\bf(iv) SepIt + SepFormer + DM + SC} &{\bf22.4} & {\bf20.1}\\
 \bottomrule
 \vspace{-7mm}
\end{tabular}

\end{table}

First, we observe that the current state of the art is approaching the upper bound. For 2 and 3 speakers, the gap between the upper bound and current results is 0.8dB and 1.4dB, respectively. Second, as expected, our SepIt model is able to improve only slightly the result with  speakers, with an improvement of  over the current state of the art, while for  speakers, SepIt improves the result by 0.3dB. We note that for 2/3 speakers only few examples had early stopping with the stopping criterion ({\bf SC}), which resulted in similar results.

\textbf{LSTM Backbone}
In LSTM-based architectures the state-of-the-art network for a large number of speakers was provided by \cite{dovrat2021manyspeakers}. We apply the SepIt network over the Libri5Mix and Libri10Mix datasets. The results are summarized in Tab.~\ref{tab:results_5_10spkrs}. As expected, here the improvement over the current state-of-the-art architecture is more prominent than in the Transformer case, where for  speakers, SepIt improves results by 1.0dB, and for  speakers by 0.5dB.
\begin{table}[h!]
\caption{The performance obtained by our method, as well as the LSTM backbone network. See text for details.}
\label{tab:results_5_10spkrs}

\centering \begin{tabular}{@{}l@{~}cc@{}} 
 \toprule
& \multicolumn{2}{c}{SI-SDRi  {}}\\
 \cmidrule(lr){2-3}Method &  Libri5Mix  & Libri10Mix \\
\midrule
Upper Bound & 14.5 & 12.0\\
Gated LSTM~\cite{dovrat2021manyspeakers} &12.7 & 7.7\\
SepIt+ Gated LSTM &13.2 & 8.0\\
SepIt+ Gated LSTM + DM & 13.6 & 8.1\\
\textbf{SepIt+ Gated LSTM + DM +SC} &{\bf13.7} & {\bf8.2}\\
 \bottomrule
 \vspace{-12.5mm}
\end{tabular}


\end{table}










\section{Summary}
In this work, a general upper bound for the Single Channel Speech Separation (SCSS) problem is derived. The upper bound is obtained using the Cramer-Rao bound, with fundamental modeling of the speech signal.
We show that the gap between the current state-of-the-art network for 2 and 3 speakers and the obtained bound is relatively small, 0.8[dB] and 1.4[dB] respectively. This may indicate that future research should focus more on reducing the model size or the required training set and less on improving overall accuracy. The gap for 5 and 10 speakers is larger, and there is still room for substantial improvement in separation accuracy. Using the upper bound, a new neural network named SepIt is introduced. SepIt takes a backbone network estimation, and iteratively improves the estimation, until a stopping criterion based on the upper bound is met. SepIt is shown to outperform current state-of-the-art networks for 2, 3, 5 and 10 speakers. 



\section{Acknowledgments}
The contribution of Shahar Lutati is part of a Ph.D. thesis
research conducted at Tel Aviv University.
This project has received funding from the European Research Council (ERC) under the European Unions Horizon 2020 research and innovation programme (grant ERC CoG 725974). 

\bibliography{mybib}
\bibliographystyle{IEEEtran}



\end{document}
