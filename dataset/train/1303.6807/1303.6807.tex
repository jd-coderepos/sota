For the purposes of this paper a topology is defined as a graph of the connections
in the peer-to-peer network annotated with the number of connections between each
pair of peers and the upload bandwidth of each peer.  No peer is ``special" apart
from the peercaster.  The peers have no characteristics apart from an upload
bandwidth and a position which gives rise to a fixed delay between each pair of
peers.

\subsection{Topology definitions}

\begin{definition} \label{defn:feasible}
A {\em feasible topology\/} is one where
\begin{enumerate}
\item all peers have $M$ connections from which they download,
\item no peers exceed their upload bandwidth,
\item all peers can find $M$ edge distinct paths from the peercaster to themselves.
\end{enumerate}
A {\em feasible connection policy\/} is a policy for making connections which,
if followed repeatedly, will connect a set of nodes into a topology which obeys
the conditions above.  A {\em feasible connection\/} is a connection made
according to a feasible connection policy.
\end{definition}

\begin{remark}
Requirement 3 arises because it is necessary to ensure that, for example, in a
substreaming system each peer can download $M$ substreams from the peercaster.
This requirement is equivalent (by the max-flow/min-cut theorem) to requiring
that the minimum cut set to cut each peer from the peercaster is at least $M$ 
edges.
Without this requirement, a policy where node A and node B each send $M$ substreams
to the other and neither connect to the peercaster would be a feasible topology. 
\end{remark}

\begin{definition}\label{defn:thisfeasible}
The feasible connection policy used in this paper is as follows.
\begin{enumerate}
\item Initialise the system assuming only the peercaster is connected.  Let
$F:= u_0 - M$ be the spare upload bandwidth which will remain in the system
after the next peer joins.
\item Choose a peer $i$ which has $u_i$ 
such that $u_i + F \geq M$.  The choice is made according to some topology
policy (see next section).   This  guarantees that the system will have sufficient 
free bandwidth to make all $M$ connections required by the next peer.
\item Make all $M$ upload connections to peer $i$ from already connected peers (with
remaining upload capacity) according to some topology
policy (see next section).
\item Let $F:=F + u_i -M$.
\item If more peers remain to be connected then go to step (2) above.
\end{enumerate}
\end{definition}

It is easy to show that this policy will meet the requirements of
definition \ref{defn:feasible}.  Steps (2) and (4) ensure that requirement (2)
is met by checking that the new peer has sufficient upload bandwidth.  
Step (3) ensures that requirement (1) is met.  


Requirement (3) must be met by step (3) of the algorithm.  The proof is
by induction.  Requirement (3) is clearly satisfied when only the peercaster
is connected.
Assume that requirement (3)
is true of the first $n$ peers to be connected.  When the next peer $n+1$
is connected by step (3) then each of the peers connected has $M$ distinct edge
paths to it.  Is it possible to form $M$ edge distinct paths
to node $n+1$?  If this were not the case then there must be some cut-set with less
than $M$ members between the peercaster and node $n+1$.  Let $U_i$ ($i \in \{1,\ldots,M\}$)
be the set of uploaders to $n+1$.  It is impossible to cut the connection to any of
the $U_i$ by removing fewer than $M$ edges by the induction hypothesis.  By construction
there must be 
exactly $M$
connections between nodes in the set $\{U_1, \ldots, U_M\}$ 
and node $n+1$ so to cut between this set
and $n+1$ obviously all $M$ connections
would need to be removed.  No cut set of less than $M$ members exists between the
peercaster and $n+1$ exists and hence requirement (3) of definition \ref{defn:feasible}
is met.

\subsection{Topology policies}
\label{sec:toppolicy}

In this paper a {\em fixed\/} policy is one where the whole ``universe" of
peers is available from the start and connections can choose from this
universe.  Conversely, a {\em growing\/} policy is one where peers 
arrive one by one and each peer makes all its connects when it arrives.
In earlier work on this subject 
\cite{ukpew} 
the terms global and local were used instead.  
A topology which connects {\em closest\/} peers is one which chooses 
the {\em feasible connection\/} which has least delay between the two
peers being connected.  A topology which
connects {\em least delay\/} peers is one which chooses the
{\em feasible connection\/} which has the smallest value for the
shortest delay path from the peercaster to the peer on the download
end of the new connection.

\begin{remark}
The real difference between fixed and growing topologies is that a growing 
topology connects nodes in the order in which they appear.  A fixed topology
is allowed to choose which node to connect.  In theory, a fixed
topology has much more freedom and could perform much better.
\end{remark}

In this paper {\em connection diversity\/} refers to topologies which attempt to upload 
from distinct peers wherever possible.  If a topology naively selects the {\em closest\/}
peer for example then it is likely to make multiple connections to the same peer (indeed 
this will happen unless that peer has its upload bandwidth exhausted).  With {\em
connection diversity\/} then a peer will have more than one connection to the same uploader
if and only if no other connection is available.  A {\em small world\/} topology is one which 
makes $N-1$ connections with connection diversity and the final connection completely at random.

The policies for the fixed topologies are as follows.
\begin{itemize}
\item FR -- Fixed random.
\item FCD -- Fixed closest, with connection diversity.
\item FCN -- Fixed closest, no diversity.
\item FCS -- Fixed closest, small world.
\item FDD -- Fixed least delay, with connection diversity.
\item FDN -- Fixed least delay, no diversity.
\item FDS -- Fixed least delay, small world.
\end{itemize}

GR, GCD, GCN, GCS, GDD, GDN and GDS are the equivalent topologies for the
``growing" peer sets.

It will help the reader's understanding to describe two of these policies more fully.
The policy GR (growing random) is implemented using definition \ref{defn:thisfeasible}
as follows.  In step (2) of the policy, only one peer (call it peer $i$) 
is available at a time
and therefore this choice is fixed.  In step (3) of the policy, a random peer is
chosen from the set of peers which are already connected and which have spare
upload capacity.  This peer is connected as an uploader to peer $i$ and its upload
capacity is reduced accordingly.  This is repeated $M$ times.

The policy FDD is implemented using definition \ref{defn:thisfeasible} as follows.
Let $d_j$ be the shortest path delay from the peercaster
to node $j$ or $\infty$ if node $j$ is not
yet connected.  Let $d(i,j)$ be the delay from peer $i$ to peer $j$.
In step (2) of the policy, the peer $i$ chosen is the peer with the
smallest value for $d_j + d(i,j)$ which has a sufficiently large $u_i$ to meet the
condition of step (2) (note that $u_i = 0$ is large enough if $F = M$).  It is now
necessary to make $M$ connections (with diversity) to peer $i$.  This is achieved
by connecting to the peer with the smallest value of $d_j + d(i,j)$ and then setting
$d_j:= d_j + L$ where $L$ is some ``large" number\footnote{$L$ should be large
enough that a second connection to $j$ will only be made if no non-penalised node is
available -- $N \max(d(i,j))$ is sufficient.}.  This is repeated until $M$ connections
are made.

\begin{remark}
It should be noticed that in the FR topology the nodes are selected in 
a random order and it is, therefore, effectively the same as the GR topology.
\end{remark}

\subsection{Metrics for topologies}

Because each node has $M$ independent connections, variants on more 
usual network metrics are used here.  For example, it is not 
simply the shortest path
from a node to the peercaster to the node which is of interest but the 
path length along all paths.

The metrics listed in this session have been created with several
considerations in mind.  A ``good" topology should have all or
most of the following properties.
\begin{itemize}
\item Low delay to end nodes -- this translates to nodes being able to
view streams with good ``liveness".
\item High resilience to churn -- a peer-to-peer network is, by its
nature, highly dynamic.  The loss of any single node should
not greatly affect the network.  
\item Diversity of paths -- related to the above,
an individual peer would want a diverse
set of connections so that the loss of a single intermediate 
node will not affect every substream it is downloading.
\end{itemize}

Let $D_k(i)$ be the shortest path from the node $i$ to the peercaster if the
first hop is the $k$th uploader to node $i$.

\begin{definition}
The {\em minimum delay\/} of a node is the shortest path distance from the 
peercaster to the node -- it is the minimum over $k$ of $D_k(i)$.  
The minimum delay of a system is the mean of this 
taken over all nodes.  This metric gives an estimate of the minimum possible
end-to-end liveness that any of the substreams a node gets will experience.
\end{definition}

\begin{definition}
The {\em tree delay\/} of a node is the distance from the peercaster to
the node after the removal of $M-1$ peercasted rooted shortest
path trees (that is to say,
remove the shortest path from the peercaster to each peer and repeat
this operation $M-1$ times).  This metric
estimates a pessimistic end-to-end delay if packets took an extremely
favourable path.
The tree delay of the system is the mean of this taken over all nodes.
\end{definition}

Let $V(i)$ be
the substreams connecting node $i$ to the peercaster which could potentially 
be disrupted by the removal of a single node (not including the
peercaster).  It is zero if and only if every node 
is directly connected to the peercaster.  It is $M$ if all of the paths
$D_k(i)$ go through a single node (that is every path to $i$ could be
cut by the removal of a single node).

\begin{definition}
The {\em mean node vulnerability\/} for the
system is $\sum_{i=1}^N (V_i)/(NM)$ -- this
is one if every node has a single node which could cut all $M$ substreams 
(every node is vulnerable to the loss of all substreams) and
is zero if every node has all connections directly to the peercaster
(every node cannot be cut off).
\end{definition}

Let $S_i$ be the vulnerability of the system to the removal of node $i$.
It is, in a sense, the dual of $V_i$.  It is the total number of streams
$D_j(k)$ (where $j \neq i$) which could be broken if node $i$ were
removed from the system.

\begin{definition}
The {\em maximum system vulnerability\/} is
given by $\max_i S_i/(NM)$ -- 
this is the proportion of paths which could potentially be damaged by the 
removal of a
single node.  It will be one if there is a single node (apart from the peercaster)
which can disrupt every transmission path and zero if there are no nodes which
can damage paths (only possible if every node connects directly to the peercaster).
This measure is similar to finding the node with maximum
Betweenness-Centrality \cite{betweenness}.  It is a measure of the worst case
damage a single peer leaving the network can cause (one meaning a single peer leaving
can disconnect every path and zero meaning no peer leaving can disconnect any paths).
\end{definition}


