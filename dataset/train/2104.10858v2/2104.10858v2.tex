\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{pifont}
\usepackage{overpic}
\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}

\newcommand{\highlight}[1]{\textcolor{ForestGreen}{\textbf{#1}}}

\usepackage{float} 
\usepackage[dvipsnames]{xcolor}


\definecolor{mygreen}{RGB}{0,100,0}
\definecolor{myblue}{RGB}{0,0,240}
\definecolor{myred}{RGB}{200,0,0}

\usepackage{hyperref}
\hypersetup{pagebackref=false,colorlinks=true,linkcolor=myred,citecolor=mygreen,bookmarks=false,urlcolor=black}

\newcommand{\myPara}[1]{\vspace{.05in}\noindent\textbf{#1:}}

\newcommand{\nameofmethod}{LV-ViT}

\newcommand{\cmarkg}{\textcolor{lightgray}{\ding{51}}\xspace}\newcommand{\cmarkgreen}{\textcolor{mygreen}{\ding{51}}\xspace}\newcommand{\xmarkg}{\textcolor{lightgray}{\ding{55}}\xspace}\newcommand{\mycolrule}{\arrayrulecolor{black!30} \midrule \arrayrulecolor{black}}

\newcommand{\stdminus}[1]{\scalebox{0.65}{}}
\newcommand{\bigfigno}{1}





\def \pzo {\phantom{0}} 
\def \dzo {\phantom{00}} 
\def \tzo {\phantom{000}}
\def \qzo {\phantom{0000}} 

\def \OURS {LV-ViT\xspace}
\def \etal {\textit{et al.}\xspace}
\def \ours {\OURS}

\iccvfinalcopy 

\def\iccvPaperID{***} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}





\begin{document}

\title{Token Labeling: Training an 85.4\% Top-1 Accuracy Vision Transformer \\
with 56M Parameters on ImageNet}

\author{Zihang Jiang\thanks{Work done as an intern at ByteDance AI Lab.} \quad Qibin Hou \quad Li Yuan \quad Daquan Zhou  \quad Xiaojie Jin \quad Anran Wang \quad Jiashi Feng\\
National University of Singapore  \quad ByteDance \\
{\tt\small \{jzh0103,andrewhoux,ylustcnus,zhoudaquan21,xjjin0731\}@gmail.com} \\
{\tt\small anran.wang@bytedance.com, elefjia@nus.edu.sg}
}

\maketitle
\ificcvfinal\thispagestyle{empty}\fi

\begin{abstract}
This paper provides a strong baseline for vision transformers on the ImageNet classification task.
While recent vision transformers have demonstrated promising results in ImageNet classification, 
their performance still lags behind powerful convolutional neural networks (CNNs)
with approximately the same model size.
In this work, instead of describing a novel transformer architecture,
we explore the potential of vision transformers in ImageNet classification
by developing a bag of training techniques.
We show that by slightly tuning the structure of vision transformers and introducing token labeling---a
new training objective, our models are able to achieve better results than the CNN counterparts 
and other transformer-based classification models with similar amount of training parameters and computations.
Taking a vision transformer with 26M learnable parameters as an example, we can achieve 
an 84.4\% Top-1 accuracy on ImageNet.
When the model size is scaled up to 56M/150M, the result can be further increased to 85.4\%/86.2\% without extra data. 
We hope this study could provide researchers with useful techniques to train powerful vision transformers.
Our code and all the training details will be made publicly available at \url{https://github.com/zihangJiang/TokenLabeling}.

\end{abstract}



\begin{table}[t]
  \centering
  \small
  \setlength\tabcolsep{2.6mm}
  \renewcommand\arraystretch{1}
  \caption{Comparison with CaiT \cite{touvron2021going}. Our model exploits less training techniques,
  model size, and computations but achieve identical result to CaiT.}
  \label{tab:comp_cait}
  \begin{tabular}{lcc} \toprule[0.5pt]
    Settings & LV-ViT (Ours) & CaiT~\cite{touvron2021going} \\ \midrule[0.5pt] 
    Transformer Blocks    & 20   & 36 \\
    \#Head in Self-attention & 8 & 12 \\
    MLP Expansion Ratio & 3 & 4 \\
    Embedding Dimension & 512 & 384 \\
    Stochastic Depth \cite{huang2016deep} & 0.2 (Linear) & 0.2 (Fixed) \\
    Rand Augmentation \cite{cubuk2020randaugment} & \cmarkgreen & \cmarkgreen \\
    CutMix Augmentation \cite{yun2019cutmix} &  & \cmarkgreen \\
    MixUp Augmentation \cite{zhang2017mixup} & & \cmarkgreen \\
    LayerScaling \cite{touvron2021going} & & \cmarkgreen \\
    Class Attention \cite{touvron2021going} & & \cmarkgreen \\
    Knowledge Distillation & & \cmarkgreen \\
    Enhanced Residuals (Ours) & \cmarkgreen & \\
    MixToken (Ours) & \cmarkgreen & \\
    Token Labeling (Ours) & \cmarkgreen & \\ \midrule[0.5pt]
    Test Resolution &  &  \\
    Model Size & 56M & 69M \\
    Computations & 42B & 48B \\
    Training Epoch & 300 & 400 \\ \midrule[0.5pt]
    ImageNet Top-1 Acc. & 85.4 & 85.4 \\
    \bottomrule[0.5pt]
  \end{tabular}
\end{table}

\section{Introduction}

Transformers, originally designed for the machine translation task \cite{vaswani2017attention},
have achieved great performance for almost all natural language processing (NLP) tasks 
over the past years \cite{brown2020language,devlin2018bert,liu2019roberta}.
Motivated by the success of transforms on the NLP tasks, very recently, many researchers attempt to 
build pure or hybrid transformer models for vision tasks and show the potential of transformer based models 
for image classification \cite{dosovitskiy2020image,yuan2021tokens,touvron2020training,wang2021pyramid,liu2021swin,chen2021crossvit}.
However, as interpreted in \cite{dosovitskiy2020image}, purely transformer-based models need to pretrain
on very large datasets (e.g., ImageNet-22k) with tens of millions of training images to achieve a
decent performance.
Later, DeiT~\cite{touvron2020training} and T2T-ViT~\cite{yuan2021tokens} demonstrate
that by exploiting proper data augmentation policies,
it is possible to train a vision transformer achieving nearly 80\% Top-1 accuracy on ImageNet
with 22M learnable parameters using only 1.2M ImageNet training data \cite{deng2009imagenet}.
This success delivers a promising way to help vision transformers to surpass the canonical
convolutional neural networks \cite{simonyan2014very,he2016deep,tan2019efficientnet}, 
which have dominated ImageNet classification for years. 

\begin{figure*}[t]
    \centering
    \small
\begin{overpic}[width=0.95\linewidth]{figures/TeaserComp.png}
    \end{overpic}
    \caption{Comparison between the proposed \nameofmethod{} and other recent works based on transformers.
    Note that we only show models whose model sizes are under 100M. As can be seen, our \nameofmethod{}
    achieves the best results using the least amount of learnable parameters. The default test resolution
    is  unless clearly shown after @.}
    \label{fig:res_fig}
\end{figure*}

Inspired by DeiT \cite{touvron2020training} and T2T-ViT~\cite{yuan2021tokens}, in this paper, 
we are also interested in investigating the potential of vision transformers in ImageNet classification 
relying on pure ImageNet-1k data.
Our goal is to provide the vision community a strong baseline for vision transformers 
without changing the transformer structures.
To achieve this, we first rethink the way of performing patch embedding and propose to 
explicitly introduce inductive bias as done in \cite{yuan2021tokens}.
Furthermore, besides connecting a linear layer to the class token for score prediction,
we present a \emph{token labeling objective loss} by taking a -dimensional score map as supervision 
to densely supervise all the tokens except the class one, where  is the number of categories for the target dataset.
The -dimensional score map can be easily generated by exploiting the relabeling strategy as described
in \cite{yun2021relabel}.
In addition, we also provide practical advice on adjusting the vision transformer structures.

Based on the aforementioned training strategies and objectives, we present an improved version of vision transformer, 
termed \nameofmethod{}.
As shown in Figure~\ref{fig:res_fig}, our \nameofmethod{} with 56M parameters performs better than
most of the transformer-based models having no more than 100M parameters.
We can also see from Table~\ref{tab:comp_cait} that using less training techniques, model size, and computations,
our \nameofmethod{} achieves better result than the recent state-of-the-art CaiT model \cite{touvron2021going}.
We hope this paper could be regarded as an instructional work to provide researchers or engineers
a strong baseline for training vision transformers.

\section{Related Work}

Transformers~\cite{vaswani2017attention} refer to the models that entirely rely on the self-attention mechanism 
to building global dependencies, which are originally designed for natural language processing tasks.
Due to the strong capability of capturing spatial information, transformers have been successfully applied
into a variety of vision problems,  including low-level vision tasks, such as image enhancement~\cite{chen2020pre,yang2020learning}, as well as more challenging tasks,
such as image classification~\cite{chen2020generative, dosovitskiy2020image}, object 
detection~\cite{carion2020end,dai2020up,zhu2020deformable,zheng2020end},
segmentation~\cite{chen2020pre,sun2020rethinking,wang2020end} and image generation~\cite{parmar2018image}. 
Some work also extend transformers for video and 3D point cloud processing~\cite{zeng2020learning,zhao2020point,zhou2018end}.
As the goal of this paper is to introduce a bag of training techniques for ViTs, in what follows, 
we briefly give a review of transformer models that are closely related to this work.

\begin{figure*}[t]
    \centering
    \small
    \includegraphics[width=0.8\linewidth]{figures/arch.pdf}
    \caption{Training pipeline comparison between the original vision transformer (a) and the proposed approach (b).
    After patch embedding, a MixToken method is introduced to mix the tokens from two different images. A token labeling
    objective is also added along with the original classification loss.}
    \label{fig:arch}
\end{figure*}

Among the recent vision transformers, Vision Transformer (ViT) is one of the earlier attempts 
that achieve state-of-the-art performance on ImageNet classification using pure transformers
as basic building blocks.
However, ViTs need pre-training on very large datasets, such as ImageNet-22k and JFT-300M,
and huge computation resources to achieve comparable performance to ResNet~\cite{he2016deep} with similar model size
trained on ImageNet.
Later, DeiT~\cite{touvron2020training} manages to tackle the data-inefficiency problem 
by simply adjusting the network architecture and adding an additional token along with the class token 
for Knowledge Distillation~\cite{hinton2015distilling, yuan2020revisiting} to improve model performance.

Some recent work~\cite{yuan2021tokens,chen2021crossvit,wu2021cvt,han2021transformer} also attempt
to introduce the local dependency into vision transformers by modifying the patch embedding block 
or the transformer block or both, leading to significant performance gains.
For example, Tokens-to-Token ViT~\cite{yuan2021tokens} presents a T2T module to progressively
aggregate the information of the input image to tokens.
CvT~\cite{wu2021cvt} proposes a convolutional token embedding layer for patch embedding 
as well as a convolutional projection for self-attention layer to extract local information.
Transformer iN Transformer~\cite{han2021transformer} designs a TNT block to replace 
the original transformer block for better modeling both patch-level and pixel-level representations.
ConViT~\cite{d2021convit} uses a gated positional self-attention as a substitution 
for the self-attention to deal with low-level local information.
Moreover, there are also some work~\cite{wang2021pyramid,heo2021rethinking,liu2021swin} adopting
the pyramid structure to reduce the overall computations to maintain the ability to capture low-level features.


Unlike most of the aforementioned work that target at designing new transformer blocks or transformer architectures, 
we aim to describe a bag of interesting training techniques to
improve the performance of vision transformers without changing the architecture. We demonstrate that by adding the proposed techniques to the training recipe of vision transformers,
we can achieve strong baselines for transformer models at multiple different model size levels.





\section{A Bag of Training Techniques for ViT}

In this section, we first briefly review the structure of the vision transformer and then
describe how to improve vision transformers by introducing a bag of training techniques.

\subsection{Vision Transformer}

A typical vision transformer \cite{dosovitskiy2020image} includes five primary components:
patch embedding, position encoding, multi-head self-attention, feed-forward layers, and a score prediction layer.
Both multi-head self-attention and feed-forward comprise a typical transformer block.
In what follows, we briefly describe the functionality of these components.

\myPara{Patch Embedding}
In a vision transformer, the fixed-size input image is first decomposed into a sequence of small patches,
each of which is with a predefined size, say .
For example, when the input resolution is set to , there would be 
small patches.
Each patch is then projected with a linear layer that maps its overall dimension 
to a feature vector, or called a token.
All these tokens concatenated with a class token for classification score prediction will be sent into
a transformer backbone for feature encoding.

\myPara{Positional Encoding}
Due to the permutation invariant propriety of transformers, an additional learnable positional encoding
is often added to the input token embeddings to couple each patch with positional information.
This is proven to be necessary and aid the model to better learn the visual structures.
Empirically, either fixed sinusoidal positional encoding \cite{vaswani2017attention} 
or learnable encoding can be used as they result in similar classification performance. 

\myPara{Multi-Head Self-Attention}
Multi-head self-attention, is introduced in~\cite{vaswani2017attention},
aiming at building long-range dependencies globally.
Given an input tensor ,  the multi-head self-attention applies linear transformations on  and 
embeds them to the key , query  and value , respectively.
Suppose there are  self-attention heads.
The key, query and value embeddings are uniformly split into  segments  along channel dimension, where  is the dimension of each head which satisfies . 
The attention module gives outputs in the form:

The outputs of all heads are then again concatenated 
along the channel dimension and a linear projection is finally applied to produce the final output
as follows:




\myPara{Feed-Forward} As described in~\cite{vaswani2017attention}, the feed-forward layer consists of two linear layers with a non-linear activation function, which can be denoted as:

where  and  are learnable weights and  and  are learnable biases.

\subsection{Training Techniques} \label{sec:train_techniques}

In this subsection, we introduce a bag of simple yet effective techniques to 
improve vision transformers, which will be specifically described in the following. 

\myPara{Network depth}
It is known that increasing the depth of networks can help improve the generalization ability 
as well as the capacity of models~\cite{he2016deep,tan2019efficientnet}. 
We investigate how network depth influences the performance of vision transformers 
by gradually increasing the depth of the baseline vision transformers.
As adding more transformer blocks would inevitably introduce more model parameters, 
we also decrease the hidden dimension size of the feed-forward layer, which we found
has nearly no negative effect on the model performance.

\myPara{Explicit inductive bias}
Vision transformers aim at global information aggregation via self-attention and do not
introduce inductive bias explicitly as in CNNs.
Nevertheless, as demonstrated in DeepVit \cite{zhou2021deepvit}, even without explicit
inductive bias, the lower transformer blocks can still learn it implicitly.
However, a multi-head self-attention layer is much more expensive than a simple convolutional
layer in computation.
Moreover, it is pointed out in~\cite{yuan2021tokens} that the tokenization operation in patch embedding
fails to capture the low-level and local structures, such as edges and lines, 
leading to low sample efficiency while training.
Taking these into account, we propose to directly add more convolutional layers to 
enhance the capability of the patch embedding module to explicitly introduce inductive bias.
In addition, to gather information from a larger receptive field, we also use convolutions 
with a smaller stride to provide an overlapped information for each nearby tokens.
More settings can be found in our experiment section.

\myPara{Rethinking residual connection}
Denoting layer normalization as LN, we can rewrite the forward pass of a transformer block as follows:

We can interpret going through multiple transformer blocks as gradually adding information 
gathered from other tokens and modifying the input.
Original transformers use a ratio of  to add the modification to the input 
while we propose to use a smaller ratio  to re-scale the residual feature values:

This can help enhance the residual connection since less information will go to the residual branch. It is also found in~\cite{liu2020rethinking,liu2019self} that adjusting the scaling factor of residual connection can improve the generalization ability of the model and we extend it to the case of vision transformer. 

\myPara{Re-labeling}
At the training phase, random crop is often used as a data augmentation method. 
However, as pointed out in~\cite{yun2021relabel}, although ImageNet~\cite{deng2009imagenet} 
is a single-label benchmark, the label is not always accurate after cropping
as many images in ImageNet include multiple objects and the objects with the ground-truth label
may not remain in the cropped image.
This problem can be worse when training images with smaller size 
in that the cropped image may be a much smaller region of the original image.
To handle this situation, we apply the re-labeling strategy~\cite{yun2021relabel} 
on the training set and re-assign each image a -dimensional score map where 
for our target ImageNet-1k dataset to get more accurate label while training. 
Unlike knowledge distillation which needs a teacher model to generate supervision labels online, 
the re-labeling technique is a cheap operation that can be viewed as a preprocessing step. 
During training, we only need to calculate the label for the random cropped image 
from the -dimensional dense score map.
Thus, we can easily improve the quality of the label with negligible additional computations.
More details can be found in \cite{yun2021relabel}.

\myPara{Token Labeling}
It is quite common to add a special [cls] token used for classification to transformer based models
since it can better gather the global information from all tokens. 
However, previous works, such as \cite{clark2020electra}, suggest that defining training objects
among all tokens can help to improve the sample efficiency while training.
Meanwhile, based on the dense score map provided by the re-labeling technique, 
we can further use it to assign each patch as well as its corresponding token an individual label. 
We call this operation \emph{token labeling}.
Specifically, we propose to add a token labeling loss function, 
which leverages the dense score map for each training image and uses the cross entropy loss 
between each token and the corresponding label in the dense score map as an auxiliary loss
at the training phase.
Denote the output of the vision transformer as , 
the -dimensional score map as , and the label for the whole image as .
Then, the auxiliary token labeling loss is defined as:

where  is the cross entropy loss.
Therefore, the total loss function can be written as:

where  is a hyper-parameter to balance the two terms.
In our experiment, we empirically set it to 0.5.

\begin{figure}[t]
    \centering
    \small
    \setlength\tabcolsep{0.5mm}
    \begin{overpic}[width=\linewidth]{figures/mix_token.pdf}
\end{overpic}
    \caption{Comparison between CutMix~\cite{yun2019cutmix} (\textbf{Left}) and our proposed MixToken (\textbf{Right}).
    CutMix is operated on the input images. This results in patches containing regions from the mixed two images
    (see the patches enclosed by \textcolor{red}{red} bounding boxes).
    Differently, MixToken targets at mixing tokens after patch embedding. This enables each token
    after patch embedding to have clean content as shown in the right part of this figure. The detailed advantage
    of MixToken can be found in Sec.~\ref{sec:train_techniques} and Sec.~\ref{sec:ablation}.}
    \label{fig:mix_token}
\end{figure}



\myPara{MixToken}
While training vision transformer, previous studies~\cite{touvron2020training, yuan2021tokens} 
have shown that augmentation methods, like MixUp~\cite{zhang2017mixup} and CutMix~\cite{yun2019cutmix},
can boost the performance and improve the robustness of the models.
However, vision transformers rely on patch based tokenization to map each input image to a sequence of tokens
and our token labeling strategy also operates on patch based token labels.
If we apply CutMix directly on the raw image, some of the resulting patches may contain 
content from two images, leading to mixed regions within a small patch as shown in Figure~\ref{fig:mix_token}.
When performing token labeling, it is difficult to assign each output token a clean and correct label.
Taking this situation into account, we rethink the CutMix augmentation method and present MixToken, 
which can be viewed as an modified version of CutMix operating on the tokens after patch embedding
as illustrated in the right part of Figure~\ref{fig:mix_token}. 

To be specific, for two images denoted as  and their corresponding token labels 
as well as , we first feed the two images into the patch embedding module to 
tokenize each image as a sequence of tokens, resulting in  and . 
Then, we produce a new sequence of tokens by applying MixToken using a binary mask  as follows:

where  is element-wise multiplication. We use the same way to generate the mask 
as in~\cite{yun2019cutmix}.
For the corresponding label, we also mix them using the same mask : 
The label for the [cls] token can be written as:

where  is the average of all element values of .



\section{Experiments}

\begin{table}[t]
  \centering
  \small
  \setlength\tabcolsep{1.2mm}
  \renewcommand\arraystretch{1}
  \caption{Default hyper-parameters for our experiments. Note that we do not use the MixUp augmentation method
  when re-labeling and token labeling are used.}
  \label{tab:hyper_p}
  \begin{tabular}{lccccc} \toprule[0.5pt]
    Supervision & Standard & Re-labeling & Token labeling \\ \midrule[0.5pt] \midrule[0.5pt]
    Epoch & 300 & 300 & 300\\ \midrule[0.5pt]
    Batch size & 1024 & 1024 & 1024\\
    LR & 1e-3  &  1e-3  &  1e-3 \\
    LR decay & cosine & cosine & cosine\\
    Weight decay & 0.05& 0.05& 0.05\\
    Warmup epochs& 5 & 5 & 5 \\\midrule[0.5pt]
    Dropout & 0 & 0& 0\\
    Stoch. Depth & 0.1 & 0.1 & 0.1 \\
    MixUp alpha & 0.8 & - & -\\
Erasing prob. & 0.25 & 0.25 &0.25\\
    RandAug & 9/0.5 & 9/0.5 & 9/0.5\\
    \bottomrule[0.5pt]
  \end{tabular}
\end{table}

In this section, we first describe the experiment setup and the default setting of hyper-parameters.
Then, we give an in-depth study of the training techniques proposed in this paper and 
conduct ablation analysis to evaluate the effectiveness of each training techniques.
Finally, we compare our proposed \nameofmethod{} with previous methods.

\subsection{Experiment Setup}

We evaluate our training techniques to improve vision transformers for image classification on ImageNet \cite{deng2009imagenet}.
All experiments are built and conducted upon PyTorch~\cite{paszke2019pytorch} and the timm~\cite{rw2019timm} library.
We follow the standard training schedule and train our models on the ImageNet dataset for 300 epochs.
Besides normal augmentations like CutOut~\cite{zhong2020random} and RandAug~\cite{cubuk2020randaugment}, 
we also explore the effect of applying MixUp~\cite{zhang2017mixup} and CutMix~\cite{yun2019cutmix} together
with our training techniques.
Empirically, we found that using MixUp together with relabeling or token labeling brings no benefit to
the performance and would even degrade the models' generalization ability.
Thus, we do not apply it for these two settings in our experiments.

For optimization, by default, we use the AdamW optimizer~\cite{loshchilov2017decoupled} 
with a linear learning rate scaling strategy  and 
weight decay rate. 
While training with token labeling, we found that using a slightly larger learning rate strategy of 
 gives better performance.
For Dropout regularization, we observe that for small models, using dropout hurts the performance.
This was also observed in a few other works related to training vision transformers~\cite{touvron2020training, touvron2021going, yuan2021tokens}.
As a result, we do not apply Dropout~\cite{srivastava2014dropout} and use 
Stochastic Depth~\cite{huang2016deep} instead.
More details on hyper-parameters are presented in Table~\ref{tab:hyper_p}.


\subsection{Training Technique Analysis} \label{sec:ablation}
\begin{table}[t]
  \centering
  \small
  \setlength\tabcolsep{1mm}
  \renewcommand\arraystretch{1}
  \caption{Ablation path from the DeiT-Small \cite{touvron2020training} baseline to our \nameofmethod{}-S.
  All experiments expect for larger input resolution can be finished within 3 days using a single server node
  with 8 V100 GPUs. Clearly, with only 26M learnable parameters, the performance can be boosted from 79.9
  to 84.4 (\textcolor{ForestGreen}{\textbf{+4.5}}) using the proposed training techniques.}
  \label{tab:tricks}
  \begin{tabular}{lccccc} \toprule[0.5pt]
    Training techniques & \#Param. & Top-1 Acc. (\%) \\ \midrule[0.5pt] \midrule[0.5pt]
    Baseline (DeiT-Small \cite{touvron2020training}) &22M &  79.9\\
    + More transformers () & 28M & 81.2 (\textcolor{ForestGreen}{\textbf{+1.2}}) \\
    + Less MLP expansion ratio () & 25M & 81.1 (\textcolor{ForestGreen}{\textbf{+1.1}})\\
    + More convs for patch embedding &26M& 82.2  (\textcolor{ForestGreen}{\textbf{+2.3}})\\
    + Enhanced residual connection &26M& 82.4
    (\textcolor{ForestGreen}{\textbf{+2.5}}) \\

    + Re-labeling &26M& 82.8 (\textcolor{ForestGreen}{\textbf{+2.9}})\\
    + Token labeling &26M & 83.3
    (\textcolor{ForestGreen}{\textbf{+3.4}})\\
    + Input resolution () & 26M & 84.4 (\textcolor{ForestGreen}{\textbf{+4.5}})\\
    \bottomrule[0.5pt]
  \end{tabular}
\end{table}

We present a summary of our proposed techniques to improve vision transformer models in Table~\ref{tab:tricks}.
We take the DeiT-Small \cite{touvron2020training} model as our baseline and show the performance increment
as more training techniqeus are added.
In this subsection, we will ablate each proposed training technique and 
evaluate the effectiveness of them. 

\myPara{Explicit inductive bias for patch embedding}
Ablation analysis of patch embedding is presented in Table~\ref{tab:abl_conv}. 
The baseline is set to the same as the setting as presented in the third row of Table~\ref{tab:tricks}.
Clearly, by adding more convolutional layers and narrow the kernel size in the patch embedding,
we can see a consistent increase in the performance comparing to the original single-layer patch embedding.
However, when further increasing the number of convolutional layer in patch embedding to 6, 
we do not observe any performance gain.
This indicates that using 4-layer convolutions in patch embedding is enough.
Meanwhile, if we use a larger stride to reduce the size of the feature map,
we can largely reduce the computation cost, but the performance also drops. 
Thus, we only apply a convolution of stride 2 and kernel size 7 at the beginning of 
the patch embedding module, followed by two convolutional layers with stride 1 and kernel size 3.
The feature map is finally tokenized to a sequence of tokens using a convolutional layer of stride 8 
and kernel size 8 (see the fifth line in Table~\ref{tab:abl_conv}).

\begin{table}[t]
  \centering
  \small
  \setlength\tabcolsep{1.1mm}
  \renewcommand\arraystretch{1.1}
  \caption{Ablation on patch embedding. Baseline is set as 16 layer ViT with embedding size 384 and MLP expansion ratio of 3.
  All convolution layers except the last block have 64 filters. \#Convs indicatie the total number of convolution for patch embedding, while the kernel size and stride correspond to each layer are shown as a list in the table.}
  
  \label{tab:abl_conv}
  \begin{tabular}{cccccc} \toprule[0.5pt]
    \#Convs & Kerenl size & Stride & Params & Top-1 Acc. (\%) \\ \midrule[0.5pt] \midrule[0.5pt]
    1 & [16] & [16] &25M &  81.1\\
    2 & [7,8] & [2,8] &25M &  81.4\\ 
    3 & [7,3,8] & [2,2,4] &25M &  81.4\\
    3 & [7,3,8] & [2,1,8] &26M & 81.9\\
    4 & [7,3,3,8] & [2,1,1,8] &26M & \highlight{82.2}\\4 & [7,3,3,8] & [2,2,1,4] &26M &  81.5\\
    6 & [7,3,3,3,3,8] & [2,1,1,1,1,8] &26M & \highlight{82.2}\\

    \bottomrule[0.5pt]
  \end{tabular}
\end{table}

\myPara{Enhanced residual connection}
We found that introducing a residual scaling factor can also bring benefit 
as shown in Table~\ref{tab:abl_res}.
We found that using smaller scaling factor can lead to better performance and faster convergence.
Part of the reason is that the weight can get larger gradients with the applied small scaling factor.
In the meantime, more information can be preserved in the main branch, leading to less information loss 
and better performance.

\begin{table}[t]
  \centering
  \small
  \setlength\tabcolsep{2.6mm}
  \renewcommand\arraystretch{1}
  \caption{Ablation on enhancing residual connection by applying a scaling factor. 
  Baseline is a 16-layer vision transformer with 4-layer convolutional patch embedding. 
  Here, function  represents either self-attention (SA) or feed forward (FF).}
  
  \label{tab:abl_res}
  \begin{tabular}{lccccc} \toprule[0.5pt]
    &Forward Function & \#Parameters & Top-1 Acc. (\%) \\ \midrule[0.5pt] \midrule[0.5pt]
      &   &26M &  82.2\\
     &  &26M &  \highlight{\textbf{82.4}}\\ 
     &  &26M &  \highlight{\textbf{82.4}}\\

    \bottomrule[0.5pt]
  \end{tabular}
\end{table}
\myPara{Re-labeling}
We use the NFNet-F6~\cite{brock2021high} trained on ImageNet with an  Top-1 accuracy 
as the machine annotator to re-label the ImageNet dataset and obtain the 1000-dimensional score map
for each image for training.
The re-labeling procedure is similar to~\cite{yun2021relabel}, but we limit our experiment setting
by training all models from scratch on ImageNet without extra data support, such as JFT-300M and ImageNet-22K.
This is different from the original Re-labeling paper \cite{yun2021relabel}, in which the Efficient-L2
model pre-trained on JFT-300M is used.
The input resolution for NFNet-F6 is , and the dimension of the corresponding output score map
for each image is . 
In practice, we only store the top-5 score maps for each position in half precision to save space 
as storing the entire score map for all the images will result in 2TB of storage.
In our experiment, we only need 10GB storage to store all the score maps.
As shown in Table~\ref{tab:tricks}, re-labeling gives a performance gain of , bringing the performance
from  to .

\myPara{MixToken}
In our experiments, we propose to use MixToken as a substitution for CutMix 
while applying token labeling. 
Our experiments show that MixToken performer better than CutMix.
As shown in Table~\ref{tab:abl_mixtoken}, using the same supervision (Re-labeling),
we can see an improvement of  compared to the CutMix baseline. 


\begin{table}[H]
  \centering
  \small
  \setlength\tabcolsep{2.5mm}
  \renewcommand\arraystretch{1}
  \caption{Ablation on the proposed MixToken and token labeling augmentations.}
  
  \label{tab:abl_mixtoken}
  \begin{tabular}{cccccc} \toprule[0.5pt]
    Augmentation Method & Supervision &Top-1 Acc. (\%) \\ \midrule[0.5pt] \midrule[0.5pt]
     MixToken & Token labeling & \highlight{83.3} \\
     MixToken & Re-labeling & 83.0\\
     CutMix & Re-labeling & 82.8\\

    \bottomrule[0.5pt]
  \end{tabular}
\end{table}


\myPara{Token labeling}
Since augmentations like rotation, shear, and translation change the spatial information of the image, 
we need to make corresponding changes in the dense score map to make token labeling work in a right way.
Comparing the top two lines in Table~\ref{tab:abl_mixtoken}, we can see that using our token labeling as
supervision results in better performance than re-labeling~\cite{yun2021relabel}.
This means that involving all the tokens in the loss calculation does matter for training vision
transformers.

Other than the above experiments, we also study the effect of different augmentation techniques,
such as CutOut and MixUp, while applying token labeling.
The ablation results are shown in Table~\ref{tab:abl_aug}.
We can see that when all the four augmentation methods are used, we obtain a top-1 accuracy of 
83.1.
Interestingly, when the MixUp augmentation is removed, the performance can be slightly improved to 83.3.
This may be due to the fact that use MixToken as well as MixUp at the same time would bring 
too much noise in the label, leading to the confusion of the model.
Moreover, the CutOut augmentation, which randomly erases some part of the image, is also effective 
and removing it brings a performance drop of  point.
Similarly, the RandAug augmentation also contributes to the performance.

\begin{table}[t]
  \centering
  \small
  \setlength\tabcolsep{2mm}
  \renewcommand\arraystretch{1}
  \caption{Ablation on different widely-used data augmentations. We empirically found that our proposed
  MixToken performs even better than the combination of MixUp and CutMix in vision transformers.}
  
  \label{tab:abl_aug}
  \begin{tabular}{cccccc} \toprule[0.5pt]
    MixToken & MixUp & CutOut & RandAug & Top-1 Acc. (\%) \\ \midrule[0.5pt] \midrule[0.5pt]
     \checkmark & \xmark  &\checkmark &\checkmark  &  \highlight{83.3} \\
     \xmark & \xmark  &\checkmark &\checkmark  &   81.3\\
     \checkmark & \checkmark  &\checkmark &\checkmark  &  83.1 \\
     \checkmark & \xmark  &\xmark &\checkmark  &  83.0\\ 
     \checkmark & \xmark  &\checkmark &\xmark  & 82.8\\

    \bottomrule[0.5pt]
  \end{tabular}
\end{table}



\myPara{Model Scaling} In Table~\ref{tab:model_size}, we show the performance
of our proposed approach under different network settings.
By using 16 transformer blocks and setting the embedding dimension to 192, we
can achieve a top-1 accuracy of 79.1.
Increasing the embedding dimension and network depth can further boost the performance.
More experiments compared to other methods can be found in Sec~\ref{sec:comp_others}.

\begin{table}[t]
  \centering
  \small
  \setlength\tabcolsep{1.1mm}
  \renewcommand\arraystretch{1.1}
  \caption{Performance of the proposed \nameofmethod{} with different model sizes. Here, `depth' denotes
  the number of transformer blocks used in different models. By default, the test resolution is set
  to  except the last one which is .}
  \label{tab:model_size}
  \begin{tabular}{cccccc} \toprule[0.5pt]
    Name & Depth & Embed dim. & \#Parameters & Top-1 Acc. (\%) \\ \midrule[0.5pt] \midrule[0.5pt]
    \nameofmethod{}-T & 12 & 240 & 8.5M &  79.1\\
    \nameofmethod{}-S & 16 & 384 & 26M &  83.3\\ 
    \nameofmethod{}-M & 20 & 512 & 56M & 84.0\\
    \nameofmethod{}-L & 24 & 768 & 150M & \highlight{85.3}\\
    \bottomrule[0.5pt]
  \end{tabular}
\end{table}

\begin{table}[t]
  \centering
  \small
  \setlength\tabcolsep{2.6mm}
  \renewcommand\arraystretch{1}
  \caption{Ablation on normalization layer. The baseline is set as 16 layer vision transformer with 384 embedding dimension and 4 layer convolutional patch embedding.}
  
  \label{tab:abl_norm}
  \begin{tabular}{lccccc} \toprule[0.5pt]
    Normalization & Groups & \#Parameters & Top-1 Acc. (\%) \\ \midrule[0.5pt] \midrule[0.5pt]
     LayerNorm & - &26M &  82.2\\
     GroupNorm& 1 &26M &  82.2\\
     GroupNorm&2 & 26M & 82.2 \\
     GroupNorm&3 & 26M & 81.9 \\
     GroupNorm& 6 &26M &  81.8\\

    \bottomrule[0.5pt]
  \end{tabular}
\end{table}


\begin{table*}
    \centering
    \caption{Top-1 accuracy comparison with other methods on ImageNet \cite{deng2009imagenet}
    and ImageNet Real~\cite{beyer2020we}. All models are trained without external data. 
    With the same computation and parameter constrain, our model consistently outperforms
    other CNN-based and transformer-based counterparts. The results of CNNs and ViT are referenced from~\cite{touvron2021going}.}
    \label{tab:sota}
    \def \mysp {\hspace{7pt}}
    \centering \scalebox{1.0}
    {\small 
    \begin{tabular}{@{\ }lccccccc}
    \toprule
    Network  & Params & FLOPs & Train size & Test size  &  Top-1(\%)  & Real Top-1 (\%) \\
    \toprule
    \multicolumn{7}{c}{CNNs}\\
    \midrule

    EfficientNet-B5~\cite{tan2019efficientnet}    & \pzo30M & \dzo9.9B  &  &   & 83.6 & 88.3  \\
    EfficientNet-B7~\cite{tan2019efficientnet}    & \pzo66M & \pzo37.0B &  &   & 84.3 & \_      \\
    Fix-EfficientNet-B8~\cite{tan2019efficientnet, touvron2019fixing} & \pzo87M & \pzo89.5B &  &   & 85.7 & 90.0  \\
    \midrule
    NFNet-F0~\cite{brock2021high}           & \pzo72M & \pzo12.4B &  &  & 83.6 & 88.1  \\
    NFNet-F1~\cite{brock2021high}           & 133M    & \pzo35.5B &  &   & 84.7 & 88.9  \\
    NFNet-F2~\cite{brock2021high}           & 194M    & \pzo62.6B &  &   & 85.1 & 88.9  \\
    NFNet-F3~\cite{brock2021high}           & 255M    & 114.8B    &  &   & 85.7 & 89.4  \\
    NFNet-F4~\cite{brock2021high}           & 316M    & 215.3B    &  &    & 85.9 & 89.4  \\
    NFNet-F5~\cite{brock2021high}           & 377M    & 289.8B    &  &    & 86.0 & 89.2  \\
    \toprule
    \multicolumn{7}{c}{Transformers}\\
    \midrule
    ViT-B/16~\cite{dosovitskiy2020image}           & \pzo86M & \pzo55.4B &  &  & 77.9 & 83.6 \\
    ViT-L/16~\cite{dosovitskiy2020image}           & 307M    & 190.7B    &  &   & 76.5 & 82.2 \\
    \midrule
    T2T-ViT-14~\cite{yuan2021tokens}       & \pzo22M & \dzo5.2B  &  &         & 81.5 & \_  \\
    T2T-ViT-14384~\cite{yuan2021tokens} & \pzo22M & \pzo 17.1B  &  &         & 83.3 & \_ \\
    \midrule
    CrossViT~\cite{chen2021crossvit}           & \pzo45M & \pzo56.6B &  &         & 84.1 & \_ \\
    Swin-B~\cite{liu2021swin}             & \pzo88M & \pzo47.0B   &  &         & 84.2 & \_ \\
    TNT-B~\cite{han2021transformer}              & \pzo66M & \pzo14.1B &  &        & 82.8 & \_   \\
    \midrule
    DeepViT-S~\cite{zhou2021deepvit}          & \pzo27M & \dzo6.2B &  &        & 82.3 & \_   \\
    DeepViT-L~\cite{zhou2021deepvit}          & \pzo55M & \pzo12.5B &  &        & 83.1 & \_   \\         
    \midrule
    DeiT-S~\cite{touvron2020training}             & \pzo22M & \dzo4.6B  &  &   & 79.9  & 85.7  \\
    DeiT-B~\cite{touvron2020training}              & \pzo86M & \pzo17.5B &  &  &  81.8 &   86.7 \\
    DeiT-B384~\cite{touvron2020training}          & \pzo86M & \pzo55.4B &   &   &  83.1 & 87.7 \\
    \midrule
    CaiT-S36384~\cite{touvron2021going}  & \pzo68M & \pzo48.0B &   &   &  85.4 & 89.8 \\
    CaiT-M36~\cite{touvron2021going} & 271M & \pzo53.7B &   &   &  85.1 & 89.3 \\
    CaiT-M36384~\cite{touvron2021going} & 271M & 173.3B &      &  &  86.1 & 90.0 \\
    CaiT-M36448~\cite{touvron2021going} & 271M & 247.8B &  &   & 86.3 & 90.2\\
    \toprule
    
    \multicolumn{7}{c}{Hybrid} \\
    \midrule
    BoTNet-S1-59~\cite{srinivas2021bottleneck} & 33.5M & \dzo7.3B & 224 & 224 & 81.7 & -\\
    BoTNet-S1-110~\cite{srinivas2021bottleneck} & 54.7M & \pzo10.9B & 224 & 224 & 82.8 & - \\
    BoTNet-S1-128~\cite{srinivas2021bottleneck} & 79.1M & \pzo19.3B & 256 & 256 & 84.2 & - \\
    BoTNet-S1-128384~\cite{srinivas2021bottleneck} & 79.1M & \pzo45.8B & 256 & 384 & 84.7 & - \\ \toprule
    \multicolumn{7}{c}{Our LV-ViT} \\
    \midrule
    \OURS-S & \pzo26M & \dzo6.6B &   &    &  83.3 & 88.1 \\
    \OURS-S384 & \pzo26M & \pzo22.2B &   &  384  &  84.4 & 88.9 \\
    \OURS-M & \pzo56M & \pzo16.0B &   &    &  84.0 & 88.4 \\
    \OURS-M384  & \pzo56M & \pzo42.2B &   &   &  85.4 & 89.5 \\
    \OURS-L & 150M & \pzo59.0B &   &   &  85.3 & 89.3 \\
    \OURS-L448 & 150M & 157.2B &   &   &  85.9 & 89.7  \\
    \OURS-L448 & 150M & 157.2B &   &   &  86.2 & 89.9  \\
    \bottomrule
    \end{tabular}}
\end{table*}


\subsection{More Experiments}

\myPara{Normalization at different positions} 
Inspired by~\cite{xiong2020layer}, we attempt to utilize both post-LayerNorm and pre-LayerNorm transformer blocks 
for our models.
While the post-LayerNorm is widely used in NLP, it fails to converge for the hyper-parameters
we use in the image classification task. 
Thus, we only use the pre-LayerNorm transformer block in all of our experiments.

\myPara{Replacing LayerNorm with GroupNorm}
We explore with different normalization layers by replacing the LayerNorm in the transformer block 
with the GroupNorm layer with different group sizes.
Results are shown in Table~\ref{tab:abl_norm}. 
We empirically found that using GroupNorm with larger group size would slow down the convergence 
and does not bring benefit to the final accuracy in the current setting.

\subsection{Comparison to Other Methods} \label{sec:comp_others}
We compare our proposed model \nameofmethod{} with previous method as well as concurrent state-of-the-art methods in Table~\ref{tab:sota}. For small-sized model, when test resolution is set to , we achieves  accuracy on ImageNet, which is  point higher than the strong baseline DeiT-S with only 26M parameters. For medium-sized model, when test resolution is set to  we also achieve same performance of  as CaiT-S36 with much less computation cost and parameters. Note that they use knowledge distillation based method to improve their model, which will introduce much more computations. However, we do not require any extra computation while training and only have to compute and store the dense score map offline. 
For large-sized model, we also test on input resolution of , which achieves 86.2\%, comparable with CaiT-M36 with about half FLOPs and parameters. It's also better than a few other models including CNN based model such as EfficientNet and other transformer based model such as T2T-ViT and Swin transformer.

\section{Conclusions}

In this paper, we have introduced a few techniques to improve the performance of a vision transformer model. We have also analyzed each component individually to evaluate its contribution. Combining them together, we were able to obtain a performant model , termed \nameofmethod{} that achieves 84.4\% Top-1 accuracy on ImageNet with only 26M parameters, which demonstrate the effectiveness of our proposed training techniques.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
