
\documentclass[runningheads, hyperfootnotes=false]{article}
\usepackage[usenames,dvipsnames,table]{xcolor}

\usepackage{iclr2023_conference,times}

\usepackage[pdftex]{graphicx}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage{wrapfig}
\usepackage{booktabs}       




\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{$\eta$}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}
\usepackage{array}
\usepackage{booktabs}
\definecolor{ForestGreen}{RGB}{34,139,34}

\newcommand{\cfx}[1]{{\color{ForestGreen} #1 - cfxu}}
\newcommand{\by}[1]{{\color{purple} #1 - yang}}
\newcommand{\PreserveBackslash}[1]{\let\temp=\\#1\let\\=\temp}
\newcolumntype{C}[1]{>{\PreserveBackslash\centering}p{#1}}

\usepackage{amssymb}\usepackage{pifont}\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}

\title{Time Will Tell: New Outlooks and A Baseline for Temporal Multi-View 3D Object Detection}

\author{Jinhyung Park\textsuperscript{1}\textsuperscript{*}\quad Chenfeng Xu\textsuperscript{2}\textsuperscript{*} \quad Shijia Yang\textsuperscript{2} \quad Kurt Keutzer\textsuperscript{2} \quad Kris Kitani\textsuperscript{1} \\ \textbf{Masayoshi Tomizuka\textsuperscript{2} \quad Wei Zhan\textsuperscript{2}}\\
\textsuperscript{1}Carnegie Mellon University \quad \textsuperscript{2}University of California, Berkeley
\\
\texttt{\{jinhyun1, kkitani\}@cs.cmu.edu} \\
\texttt{\{xuchenfeng, shijiayang, keutzer, tomizuka, wzhan\}@berkeley.edu}
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}


\maketitle
\let\thefootnote\relax\footnote{{$^*$Equal contribution.}}

\begin{abstract}
While recent camera-only 3D detection methods leverage multiple timesteps, the limited history they use significantly hampers the extent to which temporal fusion can improve object perception. Observing that existing works' fusion of multi-frame images are instances of temporal stereo matching, we find that performance is hindered by the interplay between 1) the low granularity of matching resolution and 2) the sub-optimal multi-view setup produced by limited history usage. Our theoretical and empirical analysis demonstrates that the optimal temporal difference between views varies significantly for different pixels and depths, making it necessary to fuse many timesteps over long-term history. Building on our investigation, we propose to generate a cost volume from a long history of image observations, compensating for the coarse but efficient matching resolution with a more optimal multi-view matching setup. Further, we augment the per-frame monocular depth predictions used for long-term, coarse matching with short-term, fine-grained matching and find that long and short term temporal fusion are highly complementary. While maintaining high efficiency, our framework sets new state-of-the-art on nuScenes, achieving first place on the test set and outperforming previous best art by 5.2\% mAP and 3.7\% NDS on the validation set. Code will be released here: \href{https://github.com/Divadi/SOLOFusion}{\color{RubineRed}https://github.com/Divadi/SOLOFusion}.
\end{abstract} \section{Introduction}
Recent advances in camera-only 3D detection have alleviated monocular ambiguities by leveraging a short history. Despite their improvements, these outdoor works neglect the majority of past observations, limiting their temporal fusion to a few frames in a short 2-3 second window. These long-term past observations are critical for better depth estimation - the main bottleneck of camera-only works.

Although existing methods aggregate temporal features differently, in essence, these works all consider regions in 3D space and consider image features corresponding to these hypothesis locations from multiple timesteps. Then, they use this temporal information to determine the occupancy of or the existence of an object at those regions. As such, these works are instances of \textit{temporal stereo matching}. To quantify the quality of multi-view (temporal) depth estimation possible in these methods, we define \textit{localization potential} at a 3D location as the magnitude of the change in the source-view projection induced by a change in depth in the reference view. As shown in Figure \ref{fig:localization_potential}, a larger localization potential causes depth hypotheses \cite{yao2018mvsnet} for a reference view pixel to be projected further apart, giving them more distinct source view features. Then, the correct depth hypothesis with a stronger match with the source view feature can more easily suppress incorrect depth hypotheses with clearly unrelated features, allowing for more accurate depth estimation.

We evaluate the localization potential in driving scenarios and find that only using a few recent frames heavily limits the localization potential, and thus the depth estimation potential, of existing methods. Distinct from the intuition in both indoor works, which select frames with above a minimum translation and rotation \citep{Hou2019MultiViewSB,Sun2021NeuralReconRC}, and outdoor works, which often empirically select a single historical frame \citep{huang2022bevdet4d,wang2022monocular,liu2022petrv2}, we find that \textit{the optimal rotation and temporal difference} between the reference and source frame \textit{varies significantly over different pixels, depths, cameras, and ego-motion}. Hence, it is necessary to utilize many timesteps over a long history for each pixel and depth to have access to a setup that maximizes its localization potential. Further, we find that localization potential is not only decreased by fewer timesteps but is also hurt by the lower image feature resolution used in existing methods. Both factors significantly hinder the benefits of temporal fusion in prior works.
\begin{figure*}[t]
  \centering
  \resizebox{.95\textwidth}{!}{\includegraphics[width=\linewidth]{iclr2023/images/localization_potential_final.pdf}}
  \captionsetup{aboveskip=0pt}\captionsetup{belowskip=0pt}\caption{The depth hypothesis projections onto the $t = T-16$ source view are further apart, making multi-view depth estimation easier when compared to the $t = T-1$ source view.}
  \label{fig:localization_potential}
\end{figure*} 
We verify our theoretical analysis by designing a model that naturally follows from our findings. Although existing methods' usage of low-resolution image feature maps for multi-view stereo limits matching quality, our proposed long-term temporal fusion's dramatic increase in localization potential can offset this limitation. Our model adopts the coarse but efficient low-resolution feature maps and leverages a 16-frame BEV cost volume. We find that such a framework already outperforms prior-arts, highlighting the significant gap in utilizing temporal information in existing literature. We extend our model by further exploiting short-term temporal fusion with an efficient sampling module, replacing monocular depth priors in the 16-frame BEV cost volume with a two-view depth prior. This time offsetting the temporal decrease in localization potential with an increase in feature map resolution, we observe a further boost in performance, demonstrating that \textbf{short-term and long-term temporal fusion are highly complementary}. Our main contributions are as follows:
\begin{itemize}[itemsep=1pt,topsep=0pt]
    \item We define \textit{localization potential} to measure the ease of multi-view depth estimation and use it to theoretically and empirically demonstrate that the optimal rotation and temporal difference between reference and source cameras for multi-view stereo varies significantly over pixels and depths. This runs contrary to intuition in existing works that impose a minimum view change threshold or empirically search for a single past frame to fuse.
    \item We verify our theoretical analysis by designing a model, \textbf{SOLOFusion}, that leverages both \textbf{S}h\textbf{O}rt-term, high-resolution and \textbf{LO}ng-term, low-resolution temporal stereo for depth estimation. Critically, we are the first, to the best of our knowledge, to balance the impacts of spatial resolution and temporal difference on localization potential and use it to design an efficient but strong temporal multi-view 3D detector in the autonomous driving task.
    \item Our framework significantly outperforms state-of-the-art methods in utilizing temporal information, demonstrating considerable improvement in mAP and mATE over a strong non-temporal baseline as shown in Figure \ref{fig:teaser}. SOLOFusion achieves first on the nuScenes test set and outperforms previous best art by 5.2\% mAP and 3.7\% NDS on the validation set.
\end{itemize} \section{Related Work}
\begin{figure*}[t]
  \centering
  \includegraphics[width=\linewidth]{iclr2023/images/teaser_change_v3-cropped.pdf}
  \captionsetup{aboveskip=0pt}\captionsetup{belowskip=0pt}\caption{We visualize the improvement of temporal models over their non-temporal baselines, using results from publications and released code. With short-term and long-term temporal fusion, our method best leverages temporal information, demonstrating 9.4\% and 12.9\% improvements in precision-recall (mAP) and localization quality (mATE), respectively.}
  \label{fig:teaser}
\end{figure*} \subsection{Single-View Camera-Only 3D Object Detection}
Many single-view methods use mature 2D CNNs and predict 3D boxes from the image \citep{Mousavian2017deep3dbox,brazil2019m3drpn,Qin2019MonoGRNetAG,Xu2018MultilevelFB,Zhou2019ObjectsAP}. Some works leverage CAD models \citep{liu2021autoshape,Manhardt2019ROI10DML,Barabanau2020Monocular3O} while others set prediction targets as keypoints \citep{Li2022DiversityMF,Zhang2021ObjectsAD} or disentangled 3D parameters \citep{Simonelli2019DisentanglingM3,wang2021fcos3d}. Another line of work predicts in 3D, using monocular depth prediction networks \citep{FuCVPR18DORN,Godard2017UnsupervisedMD} to generate pseudo-LiDAR \citep{wang2019pseudo,weng2019mono3dplidar} and applying LiDAR-based 3D detection frameworks. 
Our paper addresses monocular 3D ambiguity through temporal fusion and is perpendicular to these works.

\subsection{Multi-View Camera-only 3D Object Detection}
Most multi-camera works operate in 3D space. Following LSS \citep{philion2020lift}, some methods \citep{Reading2021CaDDN,huang2021bevdet,li2022unifying} predict a distribution over depth bins and generate a point cloud with probability-weighted image features to be used for BEV detection. Followup works \citep{Liu2022BEVFusionMM,li2022bevdepth} speed up voxelization and introduce depth supervision \citep{li2022bevdepth}. Another branch of works follow DETR3D \cite{wang2022detr3d} and adopt a queries. These works use object queries  \citep{wang2022detr3d,liu2022petr,chen2022polar,jiang2022polarformer} or BEV grid queries \citep{li2022bevformer} and project them to get image features.

Recent works extend LSS and query frameworks to process several frames. Most LSS methods align and concatenate volumes from multiple timesteps \citep{huang2022bevdet4d,li2022bevdepth,li2022unifying}, and query-based methods sample from past images through projection \citep{chen2022polar,jiang2022polarformer} or attention \citep{liu2022petrv2,li2022bevformer}. However, these methods demonstrate limited improvement from temporal fusion. Most of these works fuse multi-frame features at low resolution and use a limited number of timesteps over a short time window - both factors that we find severely decrease localization potential. In our work, we quantify these limitations and propose a strong framework that explicitly considers the relationship between spatial resolution and temporal history.

\subsection{Multi-View Stereo}
Recent multi-view stereo focus on using depth maps \citep{kang2001handling} and 3D volumes \citep{kutulakos2000theory} to conduct depth, mesh, and point-cloud reconstruction tasks. Several methods \citep{galliani2015massively,ji2017surfacenet} use a depth map fusion algorithm for large-scale scenes. Other works \citep{kar2017learning,yao2018mvsnet,yang2020cost,zbontar2016stereo,yao2019recurrent} generate a 3D volume by scattering pixel features to the 3D grid and estimating occupancy probability or cost for each voxel. A few methods apply multi-view stereo works to 3D detection. DfM \citep{wang2022dfm} generates a plane-sweep volume from consecutive frames. STS \citep{wang2022sts} uses spatially-increasing discretization (SID) depth bins \citep{FuCVPR18DORN} for stereo matching, and BEVStereo \citep{li2022bevstereo} adapts MaGNet \citep{Bae2021MultiViewDE} and dynamically selects candidates for iterative multi-view matching. 

Although these methods demonstrate further improvement, the short history they use for detection limits their gain from temporal fusion as shown in Figure \ref{fig:teaser}. We formulate and analyze the connection between temporal camera-only 3D detection and multi-view stereo and verify our analysis by leveraging the synergy of short-term and long-term fusion. Further, our proposed short-term temporal fusion is more efficient and extensible, also demonstrates larger improvement.
 \section{Unified Stereo Temporal Formulation for Camera-Only 3D Detection}\label{sec:formulation}
\subsection{Components of a Unified Formulation}
Camera-only 3D detection methods using multiple frames each propose their own method for temporal feature aggregation. However, we find that these works are mostly grounded in the same core formulation of multi-view stereo matching. We identify the main components of such a formulation:
\begin{itemize}[noitemsep,topsep=0pt]
    \item Candidate Locations - the locations in 3D space considered for matching.
    \item Sampling Operation - the method used to obtain image features for a candidate region.
    \item Sampling Resolution - the spatial resolution of the image features used for sampling.
    \item Temporal Aggregation - method of fusing features from different frames.
    \item Timesteps Used - number and temporal difference of timesteps used in aggregation.
    \item Candidate Location Processing - the modules used for processing the multi-timestep features aggregated to candidate locations.
    \item Task/Supervision - the task the candidate locations are used for and the corresponding loss.
\end{itemize}
\begin{table*}
\centering
\captionsetup{aboveskip=0pt}\captionsetup{belowskip=0pt}\captionof{table}{${}^*$Sampled image features are features weighted by probability prediction for that depth. ${}^\dagger$STS and BEVStereo also perform the BEV temporal stereo done in BEVDepth. ${}^\ddag$BEVFormer explicitly aggregates only the previous timestep, but the previous timestep features themselves have aggregated earlier features. They set the previous time window to be 3 timesteps during training and maintain a running BEV feature map during inference.}
\label{tab:formulation}
\tiny
\resizebox{\textwidth}{!}{
\setlength{\tabcolsep}{1pt}
\begin{tabular}{c|l|c|c|c|c|c|c|c} 

\toprule
\textbf{Type} & \textbf{Method} & \textbf{Candidate Loc.} & \textbf{Sampling Op.} & \textbf{Sampling Res.} &  \textbf{Temporal Agg.} & \textbf{Prev Time} & \textbf{Cand. Loc. Proc.} & \textbf{Task/Supervision} \\
\midrule
\multirow{3}{*}{\rotatebox[origin=c]{90}{MVS}}& MVSNet & plane-sweep volume & projection \& bilinear & 1/4 & variance & 2; - & 3D conv & depth estimation/L1 \\
& MaGNet & \begin{tabular}{@{}c@{}}predicted gaussian \\ confidence interval\end{tabular} & projection \& bilinear & 1/4 & dot product & 2 or 4; - & 2D conv & depth estimation/L2 \\
\midrule
\multirow{5}{*}{\rotatebox[origin=c]{90}{LSS-Based}} & BEVDet4D & BEV grid cells & image feats${}^*$ BEV pool & 1/16 & align \& concat & 1; 2.5s & 2D conv & obj. pred. \& localization \\
& BEVDepth & BEV grid cells & image feats${}^*$ BEV pool & 1/16 & align \& concat & 1; 0.5s & 2D conv & obj. pred. \& localization \\
& STS${}^\dagger$ & SID plane-sweep vol. & projection \& bilinear & 1/4 & groupwise corr. & 1; 0.5s & MLP & depth estimation/BCE \\
& BEVStereo${}^\dagger$ & \begin{tabular}{@{}c@{}}predicted gaussian \\ confidence interval\end{tabular} & projection \& bilinear & 1/4 & groupwise corr. & 1; 0.5s & MLP & depth estimation/BCE \\
\midrule
\multirow{7}{*}{\rotatebox[origin=c]{90}{Query-Based}}& BEVFormer & BEV query loc. & projection \& deform attn & 1/16 - 1/64 & align \& deform attn & 3; 0 - 2s${}^\ddag$  & trans. decoder & obj. pred. \& localization \\
& PolarDETR & object query loc. & projection \& deform attn & unspecified & concat & 1; 0.5s & trans. decoder & obj. pred. \& localization \\
& PolarFormer & object query loc. & \begin{tabular}{@{}c@{}}deform attn onto image \\feature polar BEV map\end{tabular} & unspecified & align \& concat & 1; 2.5s & trans. decoder & obj pred. \& localization \\
& UVTR & object query loc. &  \begin{tabular}{@{}c@{}}deform attn onto image \\feature${}^*$ polar 3D volume\end{tabular} & 1/4 - 1/32 & align \& concat & 5; 0 - 1s & obj. trans. decoder & obj. pred. \& localization\\
& PETRv2 & object query loc. & cross-attention & 1/16 & align \& cross-attn & 1; 2.5s & trans. decoder & obj. pred. \& localization \\
\bottomrule
\end{tabular}}
\end{table*} Table \ref{tab:formulation} distills temporal works into these components. Full details and justifications are in Appendix \ref{app:formulation}, and we outline the key points in the main paper.

\subsection{Connecting Multi-View Stereo and Temporal 3D Detection}\label{sec:formulation_connect}
To connect multi-view stereo to temporal 3D detection, we note the following key point: In multi-view stereo, the model predicts the probability that a candidate location is occupied \textit{by anything} while temporal 3D detection models predict whether the location is occupied by \textit{a certain object}. At their core, both methods have the same goal - given a candidate 3D location, they both consider how that location is captured in multiple 2D views to determine whether something of interest is there.

This connection holds for both LSS-based and query-based methods. LSS-based methods generate \textit{dense} candidate locations that cover the entire 3D or BEV region. Supervised by a heatmap loss \cite{Yin2021Centerbased3O}, a candidate location predicts high probability if an object exists at that location. This is similar to MVSNet which maximizes probabilities at candidate locations where anything exists. On the other hand, query-based methods use \textit{sparse} candidate locations around where objects are likely to exist. These queries are supervised by L1 to predict offsets that move query locations towards object centers. This can be formulated as the query predicting a Laplacian distribution center in 3D space with unit scale, which is directly analogous to the predicted and iteratively refined L2-supervised Gaussian distribution over depths in MaGNet. Overall, only the specific details of the various components change between models - in essence, both temporal LSS-based and temporal query-based methods aggregate or compare multi-view 2D features to make occupancy predictions, which is intrinsically analogous to multi-view stereo matching \citep{yang2020cost}.


\subsection{Analysis of Components}
Having represented 3D detection methods as instances of temporal stereo under our framework, we examine their various components to identify key points hindering their temporal fusion. From Table \ref{tab:formulation}, we observe that most 3D detection methods are significantly limited in the number of frames and the time window they fuse, with only BEVFormer allowing for longer fusion. However, BEVFormer demonstrates no additional improvement from training on more than 3 timesteps of history, which shows that their sequential fusion framework hinders the potential for long-term temporal fusion. Further, compared to MVS works that use a high 1/4 resolution, most detection methods use low 1/16 resolution feature maps for object detection temporal stereo, with only UVTR using higher resolution features. We note that STS and BEVStereo, as methods that add an MVS module to BEVDepth, do utilize higher-resolution feature maps for MVS depth estimation. However, they inherit the use of short temporal history for detection temporal stereo which limits their localization quality. We will show in Section \ref{sec:theory} that such limited temporal fusion and low resolution features significantly limits the localization potential of existing methods.  \section{Theoretical Analysis}\label{sec:theory}
\begin{wrapfigure}[]{r}{0.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{iclr2023/images/coords_final.pdf}
  \captionsetup{aboveskip=0pt}\captionsetup{belowskip=0pt}\caption{Reference and source coordinate systems. Arrows indicate positive angle direction.}
  \label{fig:coords}
\end{wrapfigure}
 In the previous section, we formulated temporal camera-only 3D detection works as instances of multi-view stereo. This allows us to analyze the object multi-view localization ability of these methods in context of a multi-view stereo setup. In this section, we focus on the general two-view setting and perform a theoretical analysis on how realistic changes between views affect the ease of multi-view depth estimation. Specifically, we derive a formulation for our defined localization potential and examine how \textit{temporal differences} between the views and \textit{image resolution} affect the potential. 

\subsection{Derivation of Localization Potential}
Let image A be the reference view we predict depth in and let image B be the source view we leverage for multi-view depth estimation. Further let $(x_a, y_a, d_a)$ denote a pixel $(x_a, y_a)$ in image A with depth $d_a$, our candidate point, and let $(x_b, y_b, d_b)$ be its corresponding projection onto image B. As shown in Figure \ref{fig:coords}, we define the camera intrinsics, inverse intrinsics, and camera A to camera B transformation as:
\begin{equation}\label{eq:def}
K = \begin{bmatrix}
f & 0 & c_x \\
0 & f & c_y \\
0 & 0 & 1
\end{bmatrix}, 
K^{-1} = \begin{bmatrix}
1/f & 0 & -c_x/f \\
0 & 1/f & -c_y/f \\
0 & 0 & 1
\end{bmatrix},
Rt_{A\rightarrow B} = \left[\begin{array}{ccc|c}
\cos{\theta} & 0 & -\sin{\theta} & t_x \\
0 & 1 & 0 & 0 \\
\sin{\theta} & 0 & \cos{\theta} & t_z
\end{array}\right]
\end{equation}
where $\theta$ is the rotation from camera A to camera B in the XZ plane and $t_x$, $t_z$ are translations from camera A to B as shown in Figure \ref{fig:coords}. We exclude transformations involving the vertical Y axis as they are minimal in driving scenarios. Then, the projection of $(x_a, y_a, d_a)$ onto image B is:
\begin{equation}\label{eq:proj}
[x_b, y_b] = 
\Bigg[
\frac{d_a x_a' \cos{\theta} - d_a f \sin{\theta} + t_x f}{\frac{d_a x_a' \sin{\theta}}{f} + d_a \cos{\theta} + t_z} + c_x, \frac{d_a y_a'}{\frac{d_a x_a' \sin{\theta}}{f} + d_a \cos{\theta} + t_z} + c_y
\Bigg]
\end{equation}
where $x_a' = x_a - c_x, y_a' = y_a - c_y$. The full derivation is in Appendix \ref{app:theory_proj}

We previously defined localization potential as the magnitude of the change in the source-view projection induced by a change in the depth in the reference view. Focusing our analysis on $x_b$ as $y_b$ is a subcase of $x_b$ with $\theta,  t_x = 0$, we represent localization potential at $(x_a, y_a, d_a)$ as:
\begin{equation}\label{eq:math_localization_potential}
\text{Localization Potential} = \left\lvert \frac{\partial x_b}{\partial d_a} \right\rvert = \frac{f \bar{t} \cos(\alpha) |\sin(\alpha - (\theta + \beta))|}{(d_a \cos(\alpha - \theta) + t_z\cos(\alpha))^2}
\end{equation}
where $\alpha, \beta, \theta$ are the angles of image A x-coordinate, translation, and rotation between views shown in Figure \ref{fig:coords}, and $\bar{t}$ is the magnitude of translation $\sqrt{t_x^2 + t_y^2}$.
The full proof is in Appendix \ref{app:theory_loc_potential}.

\subsection{Effect of Temporal Difference on Localization Potential}
We consider how the temporal difference between the source image A and the target image B effects localization potential. To do so, we re-write Equation \ref{eq:math_localization_potential} by introducing time offset $t$\footnote{In the main paper, we limit the effect of time to translation and exclude $\theta$ because it while it is common for a vehicle to maintain the same rate of translation over long periods of time, rotation often varies from timestep to timestep as the vehicle makes small corrections. Over multiple timesteps, the aggregate $\theta$ is close to 0, and we adopt this setting in the main paper as we now focus on the impact of temporal difference through translation. We do, however, analyze effects of rotation on localization potential in Appendix \ref{app:theory_rot} and also examine situations such as turns where $\theta$ varies with time in Appendix \ref{app:theory_turns}.}
\begin{equation}\label{eq:time_localization_potential}
\left\lvert \frac{\partial x_b}{\partial d_a} \right\rvert = \frac{f \bar{t} t \cos(\alpha) |\sin(\alpha - (\theta + \beta))|}{(d_a \cos(\alpha - \theta) + t_z t \cos(\alpha))^2}
\end{equation}

\begin{figure*}[t]
  \centering
  \includegraphics[width=\linewidth]{iclr2023/images_downsample/optimal_time.png}
  \captionsetup{aboveskip=0pt}\captionsetup{belowskip=0pt}\caption{Optimal time difference over candidate locations. White regions indicate that there is no valid projection. }
  \label{fig:optimal_time}
\end{figure*} We empirically study the optimal temporal frames that maximize localization potential over various depths and X-axis Image A Coordinates for the six cameras in nuScenes. For each candidate location, we find the timestep difference between the source and target cameras that maximizes its localization potential. First only allowing projections within the same image, the results are shown in Figure \ref{fig:optimal_time}. Generally, we observe that closer depths prefer smaller time difference while further depths prefer a larger time difference. A more thorough breakdown also considering projections between different cameras is in the Appendix \ref{app:theory_optimal_time_cam}. To summarize those findings, we observe that as different cameras interact, the optimal time difference between views varies wildly over even neighboring candidate points. When considering turns where $\theta$ varies with time, the situation is made even more complex. As such, it is significantly sub-optimal to determine a single, globally optimal temporal difference over which to do matching as many prior methods do \citep{huang2021bevdet,liu2022petrv2}. Furthermore, \textbf{we have determined empirically that considering many timesteps over longer history significantly increases the localization potential over candidate locations}.

\subsection{Impact of Resolution on Localization Potential}\label{sec:theory_res}
\begin{figure*}[t]
  \centering
  \vspace{-3.5em}
  \includegraphics[width=\linewidth]{iclr2023/images_downsample/increase_from_multitimestep.png}
  \captionsetup{aboveskip=0pt}\captionsetup{belowskip=0pt}\caption{Visualization of relative increase in localization potential from using multiple timesteps. Note that each camera heatmap has a different scale.}
  \label{fig:increase_from_multitimestep}
\end{figure*} We now consider the impact of image feature resolution on localization potential. Looking at Equation \ref{eq:time_localization_potential}, we find that since downsampling the image features by a factor of 4 also decreases the effective focal length by a factor of 4, then the localization potential is also divided by 4. We give 4 as an example as the standard multi-view matching resolution is 1/4 \citep{yao2018mvsnet,Bae2021MultiViewDE} and the common detection temporal stereo resolution from Section \ref{sec:formulation} is 1/16. Although deep features can maintain some intra-pixel localization information even at a lower resolution, downsampling still makes multi-view matching more difficult. As analyzed in Section \ref{sec:formulation}, image feature resolution is often constrained by computational limitations. However, we find that the increased localization potential from aggregating more timesteps can compensate for the decrease in potential caused by downsampling. In Figure \ref{fig:increase_from_multitimestep}, we show the relative increase in localization potential from aggregating more timesteps. We find that for most of the candidate locations, the localization potential is improved by far more than a factor of 4. Furthermore, the locations with less than a factor of 4 improvement are generally close depths for which monocular depth estimation has good performance and localization potential is already high enough. Thus, our analysis shows that \textbf{aggregating more timesteps can more than compensate for a lower image feature resolution, maintaining high efficiency with same localization potential as high-resolution feature matching.} 

\subsection{Effect of Temporal Difference on Multi-View Depth Ambiguity}
\begin{figure*}[t]
  \centering
  \includegraphics[width=\linewidth]{iclr2023/images/percent_effective_disparities.pdf}
  \captionsetup{aboveskip=0pt}\captionsetup{belowskip=0pt}\caption{Visualization of \% of objects that can benefit from multi-view stereo.}
  \label{fig:percent_effective_disparities}
\end{figure*}
 So far, we have shown theoretically that aggregating multiple timesteps allows for more diverse rotations and greater localization potential for various candidate locations. We conclude our analysis by directly verifying the impact of aggregating multiple timesteps on multi-view depth ambiguity. More specifically, we consider the projection difference induced by varying the 3D centers of objects in the nuScenes training set by 0.5m\footnote{Most methods adopt this granularity for depth prediction \citep{li2022bevdepth, li2022bevstereo, Liu2022BEVFusionMM}}. Using frame-to-frame ego-motion provided by nuScenes, Figure \ref{fig:percent_effective_disparities} shows the percent of 3D centers with a projection difference of at least one pixel, which is the minimum necessary to accurately localize a point in 3D using multiple views. A more thorough analysis is in Appendix \ref{app:theory_ambig}, but we conclude that \textbf{our theoretical analysis holds true empirically - with a larger number of aggregated timesteps, the percent of objects that can benefit from multi-view stereo dramatically increase over all cameras and depths.}

\noindent\textit{Summary.} We theoretically and empirically confirm that the optimal time difference between two views for maximizing localization potential at a candidate location changes over different image locations and timesteps. Due to the complex relationships governing optimal time difference, it is hard to use a single globally optimal past timestep for multi-view stereo. Instead, we find that allowing candidate locations access to its optimal setup by aggregating many timesteps can significantly improve localization potential. Further, we observe that this increases in localization potential is large enough to offset the severe drop in potential caused by the lower-resolution image features. Finally, we conclude that our analysis holds true empirically - aggregating more timesteps can substantially increase the proportion of objects that can benefit from multi-view depth estimation. \section{Method}
\begin{figure*}[t]
  \centering
  \includegraphics[width=\linewidth]{iclr2023/images/method-cropped.pdf}
  \captionsetup{aboveskip=0pt}\captionsetup{belowskip=0pt}\caption{The framework of SOLOFusion.}
  \label{fig:method_overview}
\end{figure*} We propose SOLOFusion, which is a natural extension of our theoretical analysis. Core to our method and design choices is balancing the impacts of image resolution and temporal aggregation on localization potential. SOLOFusion has two main streams: 1) LSS-based object detection temporal stereo pipeline for coarse image features by using a long history; 2) MVS-based depth prediction temporal framework for short history by using high resolution image features. The framework of SOLOFusion is presented in Figure \ref{fig:method_overview}. 

\subsection{Low-Resolution, Long-Term Object Detection Temporal Stereo}\label{sec:method_long}
Using image features and depth prediction at a coarse 1/16 resolution, we densely generate a point cloud and voxelize into a BEV feature map similar to \citep{huang2021bevdet,li2022bevdepth}. Different from these methods, we offset the deterioration in localization potential caused by the low image resolution by generating a BEV cost volume leveraging long history. Specifically, we align the BEV feature maps from the previous $T$ timesteps to the current timestep and concatenate them. Then, we use the resulting long-term BEV cost volume for detection. We find that this straightforward and efficient pipeline already outperforms existing methods by a large margin.

\subsection{High-Resolution, Short-Term Depth Estimation Temporal Stereo}\label{sec:method_short}
We then consider using high-resolution image features to offset short temporal history. To improve the per-frame depth estimation used for object detection temporal stereo in Section \ref{sec:method_long}, SOLOFusion leverages an MVS temporal stereo pipeline using two consecutive frames. We consider a plane-sweep volume in the reference view at 1/4 resolution and perform stereo matching with the previous frame image features. Opting for a straightforward pipeline to test our analysis, we simply add the cross-view correlations with the monocular depth predictions, supervising them together.

As this naive setup is computationally expensive, we propose to use monocular depth prediction to guide the stereo matching. We draw inspiration from the exploration and exploitation trade-off common in reinforcement learning. Stereo matching a candidate depth is expensive and is analogous to collecting an observation in RL. We hypothesize that balancing exploitation, using the monocular prior, and exploration, considering other candidate locations for stereo, when generating depth hypotheses can best leverage the expensive temporal matching. Hence, we propose Gaussian-Spaced Top-k sampling, an iterative framework to generate sampling points. We first choose the depth hypothesis with the highest monocular probability, exploiting our prior. Then, to force exploration, we down-weight the monocular probability of its neighbors, forcing exploration beyond simple prior maxima. We repeat this process, yielding a set of $k$ candidate locations for each pixel, and perform temporal stereo on these points. Additional details can be found in Appendix \ref{app:sampling}.

We find that this pipeline is able to cover multi-modal depth distributions and far outperforms simply top-k sampling with minimal computational cost. Our high-resolution, short-term depth estimation module further improves on the strong low-resolution, long-term temporal stereo model, demonstrating that short and long-term temporal stereo are complementary.

\noindent\textit{Necessity of Balance.}
Indeed - localization potential is maximized if we use high-resolution, long-term fusion. However, such a pipeline is infeasible in practice. We find that even high-resolution, short-term temporal stereo imposes a significant increase in GPU memory and runtime without our proposed matching point sampling method, making extension to high-resolution, long-term intractable. Our pipeline is built on our theoretical analysis to find a good trade-off between resolution and time difference, using improvements in one to compensate for deterioration in the other, to maximize localization potential under computational constraints.  \section{Experiments}
\subsection{Dataset and Metrics}
We use the large scale nuScenes dataset \cite{nuscenes2019}, containing 750, 150, and 150 scenes for training, validation, and testing, respectively. Each sequence is 20s long and is captured using 6 cameras with resolution $900 \times 1600$. 3D box annotations are provided at key frames every 0.5s. We refer readers to the official paper \cite{nuscenes2019} for additional regarding metrics. The full experimental details are in Appendix \ref{app:exp}.

\subsection{Main Results}
\begin{table*}
\centering
\captionsetup{aboveskip=2pt}\captionsetup{belowskip=0pt}\captionof{table}{Comparison on the nuScenes \texttt{val} set. Note that in each category, we evaluate against methods with a comparable or stronger setting than us. ${}^\dag$Initialized from FCOS3D backbone.}
\label{tab:main_val_set}
\tiny
\resizebox{\textwidth}{!}{
\begin{tabular}{l|c|c|c|c|c|c@{\hspace{1.0\tabcolsep}}c@{\hspace{1.0\tabcolsep}}c@{\hspace{1.0\tabcolsep}}c@{\hspace{1.0\tabcolsep}}c} 


\toprule
\textbf{Methods} & \textbf{Backbone} & \textbf{Image Size} & \textbf{CBGS} & \textbf{mAP}$\uparrow$  &\textbf{NDS}$\uparrow$  & \textbf{mATE}$\downarrow$ & \textbf{mASE}$\downarrow$   &\textbf{mAOE}$\downarrow$   &\textbf{mAVE}$\downarrow$   &\textbf{mAAE}$\downarrow$  \\
\midrule
BEVDet & ResNet50 & 256 $\times$ 704                                      & \cmark & 0.298 & 0.379 & 0.725 & 0.279 & 0.589 & 0.860 & 0.245 \\ PETR & ResNet50 & 384 $\times$ 1056                                       & \cmark & 0.313 & 0.381 & 0.768 & 0.278 & 0.564 & 0.923 & 0.225 \\ BEVDet4D & ResNet50 & 256 $\times$ 704                                    & \cmark & 0.322 & 0.457 & 0.703 & 0.278 & 0.495 & 0.354 & 0.206 \\ BEVDepth & ResNet50 & 256 $\times$ 704                                    & \cmark & 0.351 & 0.475 & 0.639 & \textbf{0.267} & 0.479 & 0.428 & 0.198 \\ STS & ResNet50      & 256 $\times$ 704                                    & \cmark & 0.377 & 0.489 & 0.601 & 0.275 & 0.450 & 0.446 & 0.212 \\ BEVStereo & ResNet50 & 256 $\times$ 704                                   & \cmark & 0.372 & 0.500 & 0.598 & 0.270 & 0.438 & 0.367 & 0.190 \\ \rowcolor[gray]{.9} 
SOLOFusion & ResNet50 & 256 $\times$ 704                                  & \cmark & \textbf{0.427} & \textbf{0.534} & \textbf{0.567} & 0.274 & \textbf{0.411} & \textbf{0.252} & \textbf{0.188} \\ \midrule 
FCOS3D & ResNet101-DCN & 900 $\times$ 1600                                & \xmark & 0.295 & 0.372 & 0.806 & 0.268 & 0.511 & 1.131 & 0.170 \\ BEVFormer${}^\dag$ & ResNet101-DCN & 900 $\times$ 1600                    & \xmark & 0.416 & 0.517 & 0.673 & 0.274 & 0.372 & 0.394 & 0.198 \\ PolarDETR-T${}^\dag$ & ResNet101-DCN & 900 $\times$ 1600                  & \xmark & 0.383 & 0.488 & 0.707 & 0.269 & \textbf{0.344} & 0.518 & \textbf{0.196} \\ UVTR${}^\dag$ & ResNet101-DCN & 900 $\times$ 1600                         & \xmark & 0.379 & 0.483 & 0.731 & \textbf{0.267} & 0.350 & 0.510 & 0.200 \\ PolarFormer${}^\dag$ & ResNet101-DCN & 900 $\times$ 1600                  & \xmark & 0.432 & 0.528 & 0.648 & 0.270 & 0.348 & 0.409 & 0.201 \\ \rowcolor[gray]{.9} 
SOLOFusion & ResNet101 & 512 $\times$ 1408                                & \xmark & \textbf{0.472} & \textbf{0.544} & \textbf{0.518} & 0.275 & 0.604 & \textbf{0.310} & 0.210 \\ 
\midrule
DETR3D${}^\dag$ & ResNet101-DCN & 900 $\times$ 1600                   & \cmark & 0.349 & 0.434 & 0.716 & 0.268 & 0.379 & 0.842 & 0.200 \\ PETR & ResNet101 & 512 $\times$ 1408                                & \cmark & 0.357 & 0.421 & 0.710 & 0.270 & 0.490 & 0.885 & 0.224 \\ BEVDepth & ResNet101 & 512 $\times$ 1408                            & \cmark & 0.412 & 0.535 & 0.565 & 0.266 & 0.358 & 0.331 & \textbf{0.190} \\ STS & ResNet101      & 512 $\times$ 1408                            & \cmark & 0.431 & 0.542 & 0.525 & \textbf{0.262} & \textbf{0.380} & 0.369 & 0.204 \\ \rowcolor[gray]{.9} 
SOLOFusion & ResNet101 & 512 $\times$ 1408                          & \cmark & \textbf{0.483} & \textbf{0.582} & \textbf{0.503} & 0.264 & 0.381 & \textbf{0.246} & 0.207 \\ 

\bottomrule
\end{tabular}}
\end{table*}
 \begin{table*}
\centering
\captionsetup{aboveskip=2pt}\captionsetup{belowskip=0pt}\captionof{table}{Comparison on the nuScenes \texttt{test} set. Extra data is depth pretraining \citep{packnet}.}
\label{tab:main_test_set}
\tiny
\resizebox{\textwidth}{!}{
\begin{tabular}{l|c|c|c|c|c|c|c@{\hspace{1.0\tabcolsep}}c@{\hspace{1.0\tabcolsep}}c@{\hspace{1.0\tabcolsep}}c@{\hspace{1.0\tabcolsep}}c} 


\toprule
\textbf{Methods} & \textbf{Backbone} & \textbf{Image Size} & \textbf{Extra Data} & \textbf{Test-Time Aug} & \textbf{mAP}$\uparrow$  &\textbf{NDS}$\uparrow$  & \textbf{mATE}$\downarrow$ & \textbf{mASE}$\downarrow$   &\textbf{mAOE}$\downarrow$   &\textbf{mAVE}$\downarrow$   &\textbf{mAAE}$\downarrow$  \\
\midrule
FCOS3D         & R101-DCN   & 900 $\times$ 1600 & \xmark & \cmark & 0.358 & 0.428 & 0.690 & 0.249 & 0.452 & 1.434 & 0.124 \\
DETR3D         & V2-99      & 900 $\times$ 1600 & \cmark & \cmark & 0.412 & 0.479 & 0.641 & 0.255 & 0.394 & 0.845 & 0.133 \\
UVTR           & V2-99      & 900 $\times$ 1600 & \cmark & \xmark & 0.472 & 0.551 & 0.577 & 0.253 & 0.391 & 0.508 & 0.123 \\
BEVFormer      & V2-99      & 900 $\times$ 1600 & \cmark & \xmark & 0.481 & 0.569 & 0.582 & 0.256 & 0.375 & 0.378 & 0.126 \\
BEVDet4D       & Swin-B     & 900 $\times$ 1600 & \xmark & \cmark & 0.451 & 0.569 & 0.511 & \textbf{0.241} & 0.386 & 0.301 & \textbf{0.121} \\
PolarFormer    & V2-99      & 900 $\times$ 1600 & \cmark & \xmark & 0.493 & 0.572 & 0.556 & 0.256 & 0.364 & 0.439 & 0.127 \\
PETRv2         & GLOM-like  & 640 $\times$ 1600 & \xmark & \xmark & 0.512 & 0.592 & 0.547 & 0.242 & 0.360 & 0.367 & 0.126 \\
BEVDepth       & ConvNeXt-B & 640 $\times$ 1600 & \xmark & \xmark & 0.520 & 0.609 & 0.445 & 0.243 & \textbf{0.352} & 0.347 & 0.127 \\
BEVStereo      & V2-99      & 640 $\times$ 1600 & \cmark & \xmark & 0.525 & 0.610 & \textbf{0.431} & 0.246 & 0.358 & 0.357 & 0.138 \\
\rowcolor[gray]{.9} 
SOLOFusion     & ConvNeXt-B & 640 $\times$ 1600 & \xmark & \xmark & \textbf{0.540} & \textbf{0.619} & 0.453 & 0.257 & 0.376 & \textbf{0.276} & 0.148 \\
\bottomrule
\end{tabular}}
\end{table*} We report our results on the nuScenes \texttt{val} and \texttt{test} sets in Tables \ref{tab:main_val_set} and \ref{tab:main_test_set}. Despite often operating under weaker training and inference settings, SOLOFusion far outperforms prior state-of-the-art under every setup. Notably, in the ResNet101, CBGS setting, our framework outperforms STS, which extends BEVDepth to incorporate multi-view stereo, by a substantial 5.2\% mAP. This improvement, coupled with an increase in mATE which is also a object localization metric, verifies our theoretical analysis that longer temporal fusion can indeed allow for better multi-view localization. Our method is also much better at predicting velocity, demonstrating a 12.3\% improvement in mAVE, which shows that long-term temporal fusion is not just better for localization - it also improves velocity estimation by observing dynamic objects for longer. SOLOFusion obtain 1st place on the nuScenes \texttt{test} set camera-only track at time of submission. Our framework achieves this by training without CBGS, with a shorter cycle, and without large-scale depth training or test-time augmentation, demonstrating the importance of using both short-term and long-term temporal fusion.

\subsection{Ablation Study \& Analysis}
\noindent\textbf{Ablation of Time Window for Temporal Fusion.}
\begin{table*}
\centering
\captionsetup{aboveskip=0pt}\captionsetup{belowskip=0pt}\captionof{table}{Ablation of \# of previous timesteps for low-res, long-term fusion.}
\label{tab:ablation_time_window}
\tiny
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{c|c|c|c|c} 

\toprule
\textbf{\# of Prev. Timesteps} & \textbf{mAP}$\uparrow$  &\textbf{NDS}$\uparrow$ & \textbf{mATE}$\downarrow$ & \textbf{mAVE}$\downarrow$  \\
\toprule
0  & 0.307 & 0.347 & 0.743 & 1.148 \\
1  & 0.316 & 0.423 & 0.734 & 0.456 \\
2  & 0.326 & 0.434 & 0.736 & 0.382 \\
4  & 0.349 & 0.452 & 0.701 & 0.332 \\
8  & 0.366 & 0.465 & 0.686 & 0.317 \\
16 & 0.377 & 0.474 & 0.655 & 0.307 \\
41 & 0.367 & 0.467 & 0.650 & 0.314 \\
\bottomrule
\end{tabular}}
\end{table*} We start with a BEVDepth baseline with no temporal fusion and ablate the addition of low-resolution, long-term fusion in Table \ref{tab:ablation_time_window}. We find that although fusing a single timestep dramatically improves velocity prediction, which in turn improves NDS, the localization metrics mAP and mATE improve only slightly. As we fuse more timesteps into our BEV cost volue, the mAP and mATE dramatically increases, improving by 6.1\% and and 7.9\% from 1 to 16 timesteps. This supports our analysis that compared to using a single timestep, leveraging many timesteps over a larger time window substantially increases localization potential. We find that performance saturates at 16 timesteps as there is little overlap in the visible region beyond 16 timesteps.

\noindent\textbf{Ablation of Depth Hypothesis Sampling.}
\begin{table*}
\centering
\captionsetup{aboveskip=0pt}\captionsetup{belowskip=0pt}\captionof{table}{Ablation of depth hypothesis sampling methods}
\label{tab:ablation_hypo}
\tiny
\resizebox{0.9\textwidth}{!}{
\begin{tabular}{c|c|cc|ccc} 

\toprule
\textbf{Method of Depth Sampling} & \textbf{\# of Depth Hypotheses} & \textbf{FPS} & \textbf{Memory} & \textbf{mAP}$\uparrow$  &\textbf{NDS}$\uparrow$  & \textbf{mATE}$\downarrow$  \\
\toprule
None; Single-Frame BEVDepth & - & 17.6 & 3.3 GB & 0.321 & 0.349 & 0.722 \\
Uniform Sampling & 112 (all) & 2.9 & 8.5 GB & - & - & - \\
Uniform Sampling & 28 & 7.4 & 4.2 GB & 0.345 & 0.377 & 0.692 \\
Uniform Sampling & 7 & 12.0 & 3.3 GB & 0.319 & 0.359 & 0.727 \\
Top-k Sampling & 7 & 11.9 & 3.3 GB & 0.336 & 0.390 & 0.674 \\
Gaussian-Spaced Top-k Sampling & 7 & 11.4 & 3.3 GB & 0.343 & 0.389 & 0.670 \\
\bottomrule
\end{tabular}}
\end{table*} In Table \ref{tab:ablation_hypo} we ablate various methods for choosing the depth hypotheses for high-resolution, short-term temporal stereo. Starting with the single-frame baseline, we find that attempting to match all depth hypotheses (112 for each pixel) increases runtime by 6x and significantly increases GPU memory usage. The training time and memory cost increase is even more significant, and we find it impossible to train this model properly. A model with 28 uniformly sampled hypotheses is able to improve performance, but it imposes a 2.4x slowdown and an increase in GPU memory. Although uniformly sampling 7 candidates instead improves runtime, it demonstrates no improvement from stereo. Naively leveraging the monocular depth prior with top-k sampling, we do see improvement in all metrics. Interestingly, we find that top-k sampling performs even better than the 28 uniformly sampled hypotheses for localization (mATE), demonstrating that monocular priors are useful for guiding stereo matching. Replacing naive top-k with our proposed Gaussian-Spaced Top-k Sampling improves mAP and mATE with a minimal decrease in FPS.

\noindent\textbf{Ablation of High-Res, Short-Term Fusion and Low-Res, Long-Term Fusion.}
\begin{table*}
\centering
\captionsetup{aboveskip=0pt}\captionsetup{belowskip=0pt}\captionof{table}{Ablation of short-term and long-term fusion components}
\label{tab:ablation_hrst_lrlt}
\tiny
\resizebox{0.9\textwidth}{!}{
\begin{tabular}{c|cc|cc|ccc} 

\toprule
 & \textbf{High-Res, Short-Term} & \textbf{Low-Res, Long-Term} & \textbf{FPS} & \textbf{Memory} & \textbf{mAP}$\uparrow$  &\textbf{NDS}$\uparrow$  & \textbf{mATE}$\downarrow$  \\
\toprule
(a) & \xmark & \xmark & 17.6 & 3.3GB & 0.321 & 0.349 & 0.722 \\
(b) & \cmark & \xmark & 12.2 & 3.3GB & 0.343 & 0.389 & 0.670 \\
(c) & \xmark & \cmark & 15.9 & 3.6GB & 0.386 & 0.479 & 0.650 \\
(d) & \cmark & \cmark & 11.4 & 3.6GB & 0.404 & 0.495 & 0.605 \\
\bottomrule
\end{tabular}}
\end{table*} Starting from a baseline BEVDepth model, we ablate the addition of our short-term and long-term fusion components in Table \ref{tab:ablation_hrst_lrlt}. We find that although the addition of the high-res, short-term matching from (a) to (b) slows down runtime from 17.6 to 12.2 FPS, there no increase in GPU memory cost and the localization mATE is significantly improved by 5.2\%. Instead adding the low-res, long-term matching from (a) to (c), the improvement in mAP is huge with a minimal drop in FPS. We note that purely considering localization improvement, adding high-res, short-term improved by mATE by 5.2\% while low-res, long-term improved by 7.2\%. The comparatively similar increase shows that both modules have a similar amount of localization potential, illustrating the trade-off we analyzed in Section \ref{sec:theory_res}. Finally, adding both modules further improves performance for all metrics. We observe that the the individual improvements from the two modules, $\thicksim$5\% and $\thicksim$7\%, yield a combined improvement of $\thicksim$12\%, demonstrating that the two sides of the trade-off are highly complementary.

\noindent\textbf{Balancing Temporal Fusion and Resolution.}
\begin{table*}
\centering
\captionsetup{aboveskip=0pt}\captionsetup{belowskip=0pt}\captionof{table}{Analysis of whether many-frame temporal fusion can compensate for low-resolution.}
\label{tab:ablation_long_term_resolution}
\tiny
\resizebox{0.9\textwidth}{!}{
\begin{tabular}{l|c|c|cc|ccc} 

\toprule
\textbf{Method} & \textbf{Backbone} & \textbf{Image Size} & \textbf{FPS} & \textbf{Memory} & \textbf{mAP}$\uparrow$  &\textbf{NDS}$\uparrow$  & \textbf{mATE}$\downarrow$  \\
\toprule
BEVDepth & ResNet50 & 512 $\times$ 1408 & 2.3 & 7.3 GB & 0.405 & 0.523 & 0.570 \\
SOLOFusion & ResNet50 & 256 $\times$ 704 & 11.4 & 3.6 GB & 0.427 & 0.534 & 0.567 \\
\bottomrule
\end{tabular}}
\end{table*} Based on our investigation in Section \ref{sec:theory_res}, we found that extensive temporal fusion can compensate for lower resolution in terms of localization potential. We verify this in Table \ref{tab:ablation_long_term_resolution} by comparing half-resolution SOLOFusion, designed to offset low resolution with long-term temporal fusion, with full-resolution BEVDepth. SOLOFusion outperforms BEVDepth by 2.2\% mAP and 1.1\% NDS. Furthermore, SOLOFusion demonstrates comparable object localization (mATE) despite operating at half resolution, verifying our theoretical analysis. This experiment also demonstrates an important property of SOLOFusion. By compensating for a decrease in resolution, which dramatically decreases inference cost, with an increase in temporal fusion, which has minimal impact on latency, low-resolution SOLOFusion achieves similar performance as high-resolution BEVDepth at roughly 5x the FPS and half the memory cost. 

\noindent\textbf{Runtime Analysis of SOLOFusion.}
\begin{table*}
\centering
\captionsetup{aboveskip=0pt}\captionsetup{belowskip=0pt}\captionof{table}{Runtime and improvement of methods that utilize short-term temporal stereo with ResNet50. ${}^*$ FPS and Memory are based on our reproduction. Although STS does not provide details, we reduce the matching feature dimension, as SOLOFusion does, for fair comparison. ${}^\dag$Only the short-term fusion module is used here.}
\label{tab:ablation_runtime}
\tiny
\resizebox{0.6\textwidth}{!}{
\begin{tabular}{c|cc|ccc} 

\toprule
\textbf{Method} & \textbf{FPS} & \textbf{Memory} & \textbf{$\Delta$mAP}  & \textbf{$\Delta$mATE} \\
\toprule
BEVStereo & 1.8 & 4.8 GB & 1.9 & 4.8 \\
STS${}^*$ & 4.9 & 5.5 GB & 2.4 & 4.4\\
SOLOFusion${}^\dag$ & 12.2 & 3.3 GB & 2.2 & 5.2 \\
\bottomrule
\end{tabular}}
\end{table*}
 In this section, we compare the runtime and memory costs of SOLOFusion with STS and BEVStereo, two prior works that also leverage short-term stereo for 3D detection. The results, as well as each method's improvement over the baseline, are shown in Table \ref{tab:ablation_runtime}. Although STS has a slightly larger improvement in mAP, SOLOFusion has runs 2.5x faster and has a larger improvement in localization. Compared to the short-term temporal fusion methods used in existing works, our proposed module is straightforward and efficient with strong performance. \section{Conclusion}
In this paper, we provide new outlooks and a baseline for temporal 3D object detection. We re-formulate the multi-view 3D object detection as a instance of temporal stereo matching, and define \textit{localization potential} to measure the ease of multi-view depth estimation, which is the bottleneck in current multi-view 3D detection. Through detailed theoretical and empirical analysis, we demonstrate that temporal differences and granularity of features are most important under the formulation of temporal stereo. We find that using long temporal history significantly increase the localization potential, even with low-resolution features. Such investigation leads to a natural baseline, termed SOLOFusion, which takes advantage of the synergy of short-term and long-term temporal information as well as the trade-off between time and resolution. Extensive experiments demonstrate the effectiveness of the proposed method. Specifically, SOLOFusion achieves a new state-of-the-art on the nuScenes dataset, outperforming prior-arts by 5.2\% mAP and 3.7\% NDS and placing the top in the leaderboard.  
\bibliography{iclr2023_conference}
\bibliographystyle{iclr2023_conference}

\newpage
\appendix
\section{Additional Details for the Unified Temporal Stereo Formulation for Camera-Only 3D Detection}\label{app:formulation}
In this section, we explain how the various temporal methods can be organized into our framework in Section \ref{sec:formulation}. We group methods by their type.
\subsection{Multi-View Stereo Methods}
\noindent\textbf{MVSNet.} As the seminal multi-view stereo work, MVSNet fits directly into our framework. Its candidate locations considered for matching is every $(x, y, d)$ point in the reference camera view. Image features are obtained for these points via simple projection to each view at a fine 1/4 resolution and subsequent bilinear sampling. Having obtained image features from multiple views - two source views and one reference view - for each candidate location, these features are aggregated through a variance metric. Finally, the candidate locations in the plane-sweep volume are processed using 3D convolutions, and MVSNet predicts a probability for every $(x, y, d)$ candidate location using the aggregated temporal information. Intuitively, this probability value is a prediction of \textit{whether the candidate location is occupied in 3D space}. Finally, this probability distribution over depths for every pixel is used to take a weighted sum over depth locations, yielding a single depth prediction for every pixel, which is then supervised by L1 loss. Our formulation is able to encapsulate every component of a representative multi-view/temporal stereo framework, and we find it generalizes naturally to temporal 3D detection.

\noindent\textbf{MaGNet.} As another MVS work, MaGNet can also be deconstructed into our components. Its candidate locations are $(x, y, d)$ points within the confidence interval of the predicted depth Gaussian. The rest of the pipeline is similar to that of MVSNet, only different in that it uses dot product instead of variance to aggregate temporal features and uses a 2D CNN to process the temporal features at the candidate locations. Using these processed features, MaGNet iteratively updates the depth Gaussian and is supervised via L2 to align the Gaussian with the true ground truth depth. 

\subsection{LSS-Based 3D Detection Methods}
\noindent\textbf{BEVDet4D.} The candidate locations are BEV grid cells, and each cell samples features by pooling over $(x, y, d)$ image points that fall within each cell. As pooling is done over densely generated depth hypotheses, the out-projected image feature resolution is kept coarse at 1/16. BEVDet4D aligns a previous BEV feature map with the current one using ego-motion and simply concatenates them. Considering this alignment in reverse, a grid location receives features from $(x, y, d)$ past image points that were within that grid cell location before. In this manner, each grid cell receives image features of multiple timesteps from regions in 2D views around the grid cell's 2D projection. For inference, they use a single historical observation at 2.5s and process the fused BEV map with 2D convolutions for object prediction. 

\noindent\textbf{BEVDepth.} BEVDepth, as an extension of BEVDet, maintains the exact same main components as BEVDet4D. It does, however, use a more recent frame of 0.5s for aggregation.

\noindent\textbf{STS.} Already briefly discussed in the main paper, STS extends the BEVDepth pipeline, and hence the BEVDet4D pipeline, by adding an MVS depth estimation temporal stereo component. The components that make up the MVS depth estimation component of STS are mostly identical to that of MVSNet. Although STS appears to leverage both low and high resolution temporal stereo, their low-resolution temporal stereo inherited from BEVDepth is still severely hindered by the single temporal frame they use, limiting the improvements STS can obtain from utilizing temporal information.

\noindent\textbf{BEVStereo.} Similar to STS, BEVStereo extends the BEVDepth pipeline by adding an MVS depth estimation temporal stereo module. However, instead of drawing from MVSNet, BEVStereo instead adopts intuitions from MaGNet.

\subsection{Query-Based 3D Detection Methods}
\noindent\textbf{BEVFormer.} The candidate locations are queries aligned to BEV cell locations, and each query has fixed 3D sampling locations along the z-axis which are projected onto images. BEVFormer uses several coarse image feature maps from 1/16 to 1/64 resolution. The previous timestep BEV queries are aligned to the current frame using ego-motion their features are aggregated to current queries using deformable attention. Similar to the formulation of BEVDet4D, this temporal alignment allows current BEV query candidate locations to aggregate image features that correspond to it in a previous timestep. We note that BEVFormer continuously saves the past BEV queries for the next timestep, allowing for continuous temporal fusion. However, they observe no benefit beyond training using three historical timesteps, limiting their potential for long-term temporal fusion. Finally, a transformer decoder is used on the BEV queries for object prediction.

\noindent\textbf{PolarDETR.} Although similar to BEVFormer, PolarDETR instead uses moveable object queries as candidate sampling locations. Projecting the predicted object query locations on to images, using temporal ego-motion transformation to previous timesteps, and aggregating features using deformable attention, PolarDETR combines features from different timesteps though concatenation. PolarDETR only utilizes a single frame at 0.5s history and processes the object queries with a transformer decoder for object prediction. As explained in Section \ref{sec:formulation_connect} in the main paper, the supervised offsets of the object query location that moves it towards where objects are likely to exist causes PolarDETR to be a sparse candidate location version of multi-view stereo, localizing object occupancy regions in 3D space using temporal information.

\noindent\textbf{PolarFormer.} Like other query-based methods, the candidate locations are object queries. However, different from other works that directly aggregate image features to to object queries, PolarFormer generates an intermediate Polar BEV representation that image features are first projected to. The candidate locations then aggregate features from this Polar BEV feature map. Despite this intermediate step, the key points are still similar to other query-based works. As the object queries perform deformable attention on the Polar BEV representation, there is still a significant prior that forces queries to aggregate features from BEV locations close to it. Since this Polar BEV representation was generated using projection and alignment of image locations, there is still a strong spatial connection between regions where the object query's aggregated features come from and the object query's projection onto the image. Unfortunately, the paper does not specify the resolution of image features used. The object query also has access to multiple views of it over time as previous Polar BEV representations are aligned and concatenated. The remainder of the pipeline is standard for query-based methods, with a transformer decoder then predicting objects from the candidate locations.

\noindent\textbf{UVTR.} UVTR is similar to PolarFormer in its use of an intermediate representation. Instead of a Polar BEV feature map, UVTR generates a 3D volume whose voxels are projected onto images to get their timesteps. Past 3D volumes are aligned and concatenated, and object queries, which are the candidate locations, aggregate image features through deformable attention onto this 3D volume, allowing our formulation for PolarFormer to be directly applied to UVTR.

\noindent\textbf{PETRv2.} PETRv2 is different from previous works in that candidate locations aggregate image features not through projection or deformable attention but through unconstrained cross-attention over both previous and current image features. However, by out-projecting image features and decorating them with 3D positional emebddings, PETRv2 encourages object queries to attend to image features that are spatially relevant to it. Furthermore, PETR shows that object queries attend most to image locations that they are projected to, making this cross-attention operation a form of "soft" projection and sampling. From this observation, the rest of the analysis is identical to that of PolarDETR. \section{Additional Details for Theoretical Analysis}\label{app:theory}
\subsection{Derivation and Analysis of Image A to Image B Projection}\label{app:theory_proj}
In this section, we derive our formulation for the projection $(x_b, y_b, d_b)$ in Equation \ref{eq:proj} and connect it to the familiar standard stereo case. 

Starting from Image A coordinates $(x_a, y_a, d_a)$ and applying homographic transforms:
\begin{align}
    [x_a, y_a, d_a] &\implies [x_a d_a, y_a d_a, d_a] \tag{Image A Image Hom Coords} \\
    &\implies \left[ \frac{d_a (x_a - c_x)}{f}, \frac{d_a (y_a - c_y)}{f}, d_a \right] \tag{Image A Camera Coords} \\
    &\implies \left[ \frac{d_a x_a'}{f}, \frac{d_a y_a'}{f}, d_a \right] \tag{Let $x_a' = x_a - c_x, y_a' = y_a - c_y$} \\
    &\implies \left[ 
        \frac{d_a x_a' \cos{\theta}}{f} - d_a \sin{\theta} + t_x,
        \frac{d_a y_a'}{f},
        \frac{d_a x_a' \sin{\theta}}{f} + d_a \cos{\theta} + t_z
    \right] \tag{Image B Camera Coords} \\
    &\implies \begin{aligned}\Bigg[&
        \frac{d_a x_a' \cos{\theta} - d_a f \sin{\theta} + t_x f}{\frac{d_a x_a' \sin{\theta}}{f} + d_a \cos{\theta} + t_z} + c_x, \\
        &~~~~~\frac{d_a y_a'}{\frac{d_a x_a' \sin{\theta}}{f} + d_a \cos{\theta} + t_z} + c_y, \\
        &~~~~~\frac{d_a x_a' \sin{\theta}}{f} + d_a \cos{\theta} + t_z
    \Bigg]\end{aligned} \tag{Image B Image Coords} \\
    &~~= [x_b, y_b, d_b] \tag{Image B Image Coords}
\end{align}



This formulation is applicable to any two-camera system with rotation and translation along the XZ axis. For instance, consider a standard stereo setup with cameras $A$ and $B$ as the left and right cameras, respectively: $\theta = 0$, $t_z = 0$, baseline $-t_x > 0$. Then, the above reduces to:
\[
[x_b, y_b, d_b] = \left[\frac{d_a x_a' + t_x f}{d_a} + c_x, \frac{d_a y_a'}{d_a} + c_y, d_a\right] = \left[x_a - \frac{t_x f}{d_a}, y_a, d_a \right]
\]
This yields the standard stereo disparity formula $disparity = \frac{t_x f}{d_a}$. For depth estimation in stereo or temporal stereo, we project multiple depth hypotheses for a pixel in image A onto image B and find the pixel along the epipolar line in image B that matches best with the original pixel in image A. Given such a matching, we can derive the depth using the transformation matrix between the two images/cameras. For such a formulation to work well, it is beneficial for the image B projections of nearby depth hypotheses for a pixel in image A to be as far apart from one another as possible. For instance, if two candidate depths are projected to the same pixel in image B, it is impossible to determine which of the two candidates are a better match. Even beyond same-pixel projections, with downsampled feature maps and local homogeneity of features extracted from CNN backbones, more separated depth hypothesis projections allows for easier stereo depth estimation. To quantify changes in projection from changes in depth, we defined in the main paper localization potential, which is closely tied to the ease of depth estimation.

We then examine localization potential in this standard stereo case by finding $\lvert \frac{\partial x_b}{\partial d_a} \rvert$. For the standard stereo setup, $\lvert \frac{\partial x_b}{\partial d_a} \rvert = \frac{(-t_x) f}{d_a^2}$. This partial derivative tells us that localization potential is larger, which means depth estimation is easier, if:
\begin{itemize}
    \item The baseline $-t_x$ is larger. This is in-line with our intuition; the further apart the cameras are, smaller depth changes can be captured. 
    \item The focal length $f$ is larger. Intuitively, if we downsample the image resolution, the focal length decreases, causing more different depth hypotheses to project to the now ''larger'' pixels.
    \item The depth at which we evaluate localization potential is smaller. Indeed, the projected difference between 1m and 2m is larger than the difference between 59m and 60m.
\end{itemize}

We do comment, however, that these observations do not mean the depth estimation quality in standard stereo can be simply improved by adopting a larger baseline and focal length. A larger baseline significantly decreases the overlapping region in standard stereo while a larger focal length limits the scene captured. In addition, we observe that unlike the general two-view case in Equation \ref{eq:math_localization_potential}, the localization potential does not vary over different image A x-coordinates for standard stereo. This is because the stereo cameras are aligned, causing the epipolar lines to be parallel to the x axis. As such, choosing an optimal setup for standard stereo is much simpler compared to the more general multi-view, temporal stereo case.

\subsection{Full Proof of Formulation of Localization Potential}\label{app:theory_loc_potential}
In this section, we detail the steps we took to derive our formulation for localization potential in Equation \ref{eq:math_localization_potential}. We first reparameterize $x_b$ using $\alpha$ as defined in Figure \ref{fig:coords}. Let $r_a' = \sqrt{(x_a')^2 + f^2}$ and note that we have $\sin{\alpha} = \frac{x_a'}{r_a'}, \cos{\alpha} = \frac{f}{r_a'}$. 
\begin{align*}
x_b &= \frac{d_a x_a' \cos{\theta} - d_a f \sin{\theta} + t_x f}{\frac{d_a x_a' \sin{\theta}}{f} + d_a \cos{\theta} + t_z} + c_x \\
    &= f \frac{d_a x_a' \cos{\theta} - d_a f \sin{\theta} + t_x f}{d_a x_a' \sin{\theta} + d_a f \cos{\theta} + t_z f} + c_x \tag{multiply top \& bottom with $f$} \\
    &= f \frac{d_a r_a' \left(\frac{x_a'}{r_a'} \cos{\theta} - \frac{f}{r_a'} \sin{\theta}\right) + t_x f}{d_a r_a' \left( \frac{x_a'}{r_a'} \sin{\theta} + \frac{f}{r_a'} \cos{\theta} \right) + t_z f} + c_x \\
    &= f \frac{d_a r_a' \left(\sin{\alpha} \cos{\theta} - \cos{\alpha}  \sin{\theta}\right) + t_x f}{d_a r_a' \left( \sin{\alpha} \sin{\theta} + \cos{\alpha} \cos{\theta} \right) + t_z f} + c_x \tag{substitute $\sin{\alpha}, \cos{\alpha}$}\\
    &= f \frac{d_a r_a' \sin(\alpha - \theta) + t_x f}{d_a r_a' \cos(\alpha - \theta) + t_z f} + c_x \tag{sin \& cos identities} \\
    &= f \frac{d_a \sin(\alpha - \theta) + t_x \frac{f}{r_a'}}{d_a \cos(\alpha - \theta) + t_z \frac{f}{r_a'}} + c_x \tag{divide top \& bottom with $r_a'$} \\
    &= \frac{d_a f \sin(\alpha - \theta) + t_x f \cos{\alpha}}{d_a \cos(\alpha - \theta) + t_z \cos{\alpha}} + c_x \tag{substitute $\cos{\alpha}$} \\
\end{align*}
Then, deriving $\frac{\partial x_b}{\partial d_a}$ from this formulation, we get:
\[
\left\lvert \frac{\partial x_b}{\partial d_a} \right\rvert = \frac{f \cos(\alpha) |t_z \sin(\alpha - \theta) - t_x \cos(\alpha - \theta)|}{(d_a \cos(\alpha - \theta) + t_z\cos(\alpha))^2}
\]
Note that $\cos(\alpha) > 0$ for all values of $\alpha$. We can then rewrite the above further by using the angular direction of translation $\beta$ as defined in Figure \ref{fig:coords}. The total magnitude of translation is $\bar{t} = \sqrt{t_x^2 + t_y^2}$, giving us $\sin{\beta} = t_x/\bar{t}, \cos{\beta} = t_z/\bar{t}$. Then, we get:
\begin{align*}
    \left\lvert \frac{\partial x_b}{\partial d_a} \right\rvert &= \frac{f \cos(\alpha) |t_z \sin(\alpha - \theta) - t_x \cos(\alpha - \theta)|}{(d_a \cos(\alpha - \theta) + t_z\cos(\alpha))^2} \\
    &= \frac{f \bar{t} \cos(\alpha) |\cos(\beta) \sin(\alpha - \theta) - \sin(\beta) \cos(\alpha - \theta)|}{(d_a \cos(\alpha - \theta) + t_z\cos(\alpha))^2} \\
    &= \frac{f \bar{t} \cos(\alpha) |\sin(\alpha - (\theta + \beta))|}{(d_a \cos(\alpha - \theta) + t_z\cos(\alpha))^2}
\end{align*}
which is our formulation in Equation \ref{eq:math_localization_potential}.

\subsection{Effect of View Rotation $\theta$ on Localization Potential}\label{app:theory_rot}
In this section, we identify the impact of $\theta$ on localization potential formulation in Equation \ref{eq:math_localization_potential}. Again, recall that a larger value of $\left\lvert \frac{\partial x_b}{\partial d_a} \right\rvert$ means easier depth estimation. \textit{Critically}, we first observe that $\theta$ only affects the ease of depth estimation of a pixel through its \textit{relative} angular difference with $\alpha$ of that pixel and $\beta$. This is important because it means there is no singular camera rotation that is best for all pixels or all camera translations. The optimal rotation maximizing localization potential changes based on the pixel and the current ego-motion.

Analyzing Equation \ref{eq:math_localization_potential}, we find that if $\theta$ is close to $\alpha$, the $d_a\cos(\alpha - \theta)$ term increases, making the depth estimation more difficult by a factor of $d_a^2$. This is in-line with our intuition; if the camera rotation is such that the resulting camera B's principal axis is in-line with the pixel ray, the depth hypotheses along that pixel ray be projected close together. Further, we also want $\theta + \beta$ to be different from $\alpha$ (otherwise the numerator decreases). This follows a similar intuition as before - for depth hypotheses along a pixel ray to be projected further apart, the ego-vehicle should not rotate towards or move in the same direction as the pixel ray. 

\begin{figure*}[t]
  \centering
  \includegraphics[width=\linewidth]{iclr2023/images_downsample/optimal_theta.png}
  \captionsetup{aboveskip=0pt}\captionsetup{belowskip=0pt}
  \caption{We visualize the optimal theta for various candidate locations.}
  \label{fig:optimal_theta}
\end{figure*} 
We empirically verify our analysis by visualizing the optimal $\theta$ that maximizes localization potential over various depths and X-axis Image A Coordinates for the six cameras in nuScenes. For translational movement, we take the average ego-motion for moving scenes in nuScenes, which yields approximately $tx = 0.05m, tz = 3.19m$ between consecutive frames (0.5s difference) in the front camera coordinates. The results are shown in Figure \ref{fig:optimal_theta}. Indeed, we find that \textbf{the optimal $\theta$ various over different pixel locations, depths, and cameras}, varying most significantly over image location. As the translation direction $\beta$ is different for each camera, each with its own coordinate system, by observing varied rotation values over different cameras, we verify that translation direction significantly affects the optimal $\theta$ as well. Examining the front camera with translation direction $\beta$ close to 0, we find that the $\theta$ seeks to maximize the difference between $\alpha - \theta$ while keeping the candidate point in-view. Furthermore, the optimal $\theta$ does change over depth as well. For instance, the optimal $\theta$ along the center ray in the back right camera changes from 0 degrees at 10m to 30 degrees at 50m. That some \textbf{candidate locations prefer smaller rotations} runs contrary to methods used to choose matching frames in indoor temporal stereo, which impose a minimum rotation and translation \citep{Hou2019MultiViewSB,Sun2021NeuralReconRC} between frames to be used to for matching. Hence, there is no globally optimal rotation between views. \textbf{To allow different candidate locations to maximize their localization potential, it is important to utilize many views with different rotations irrespective of their magnitude.} In practice, we can obtain diverse rotations by utilizing many timesteps over long history.

\subsection{Optimal Time Difference Considering Multi-Camera Projection}\label{app:theory_optimal_time_cam}
We first further analyze trends in optimal time difference in \ref{fig:optimal_time}
Intuitively, for the forward-facing cameras where a larger time difference increases the distance between a 3D point and the vehicle, this is a trade-off between the tendency of further depth points to be projected closer together (the denominator) and the larger difference in views generated through ego-motion (the numerator). The former wins out for closer points and the latter for further points. The different trends over various cameras are representative of both their orientation w.r.t ego-vehicle movement (we see tilted trends for the left/right slanted cameras) as well as their general forward/backward facing orientation.
\begin{figure*}[t]
  \centering
  \includegraphics[width=\linewidth]{iclr2023/images_downsample/optimal_time_all_cams.png}
  \captionsetup{aboveskip=0pt}\captionsetup{belowskip=0pt}\caption{Allowing projections of depth hypotheses between different cameras, we visualize the optimal time difference in the first row. The second row shows the target camera that the depth hypothesis was projected onto to maximize localization potential. The cameras are labeled 0 to 5 in the order of front-left to back-left.}
  \label{fig:optimal_time_all_cams}
\end{figure*} \begin{figure*}[t]
  \centering
  \includegraphics[width=\linewidth]{iclr2023/images_downsample/optimal_values_comparison_regularlog.png}
  \captionsetup{aboveskip=0pt}\captionsetup{belowskip=0pt}\caption{Log of maximized localization potential values are visualized for single-timestep with same-camera projections in row 1. Row 2 shows multi-timestep same camera projections, and row 3 shows multi-timestep, multi-camera projections. Note each vertical column has same scale}
  \label{fig:optimal_values_comparison}
\end{figure*} Next, we visualize the optimal time difference for candidate locations when allowing for projections different cameras. We also visualize the optimal target camera that the depth hypothesis is projected onto. The results are in Figure \ref{fig:optimal_time_all_cams}. Further, we also maximized log localization potential values at these optimal locations in Figure \ref{fig:optimal_values_comparison}. First, we notice that in the multi-camera setup, all depth hypotheses have valid projections. This is important for two reasons. First, this allows all pixel locations and depths to benefit from multi-view depth estimation. Second, it allows the multi-camera setting to exploit larger temporal differences without worrying about non-overlapping regions. To see this, consider the back camera in Figure \ref{fig:optimal_values_comparison}. When considering multiple timesteps (row 1 to row 2), we see that the localization potential dramatically increases for regions where same-camera projections are valid. However, the close-depth regions are unable to make use of the larger temporal differences due to invalid same-camera projection. However, when considering all cameras, we are able to leverage larger temporal differences for both these close-depth and previously invalid regions. In standard stereo, a larger baseline, despite the easier depth estimation, causes large portions of the left \& right images to not overlap. However, via our formulation, in multi-timestep temporal stereo with multiple cameras, we can leverage larger temporal differences without worrying about lack of overlap. Finally, we also notice that the multi-camera setup, although better for localization potential, has much more complex patterns for optimal time difference compared to the single-camera setup. Similar to our conclusions when analyzing rotation, we find that the optimal time difference various for different pixels, cameras, ego-motion, depth, and camera setup.


\subsection{Optimal Time Difference during Ego-Vehicle Rotation}\label{app:theory_turns}
\begin{figure*}[t]
  \centering
  \includegraphics[width=\linewidth]{iclr2023/images_downsample/optimal_time_all_cams_30deg4seconds.png}
  \includegraphics[width=\linewidth]{iclr2023/images_downsample/optimal_time_all_cams_60deg4seconds.png}
  \includegraphics[width=\linewidth]{iclr2023/images_downsample/optimal_time_all_cams_90deg4seconds.png}
  \captionsetup{aboveskip=0pt}\captionsetup{belowskip=0pt}\caption{Visualization of the optimal time difference for every candidate location when the ego-vehicle turns 30, 60, or 90 degrees over the first 8 timesteps.}
  \label{fig:optimal_time_all_cams_rot}
\end{figure*} In this section, we consider the case where theta varies with time. This happens during ego-vehicle turns, and we visualize the optimal time difference over the candidate locations in realistic scenarios of 30, 60, and 90 degree turns in Figure \ref{fig:optimal_time_all_cams_rot}. We find that when $\theta$ varies with time, the optimal time difference and optimal projected camera varies wildly over different candidate locations. This shows that it is suboptimal to choose just a few temporal differences for multi-view stereo - a past frame that worked well when the vehicle simply moved forward might fail drastically in more complex ego-motion scenarios such as turns. As such, we conclude it is not only optimal but also necessary to leverage many past timesteps over a long time window for multi-view stereo.

\subsection{Additional Analysis on Effects of Temporal Difference on Multi-View Depth Ambiguity}\label{app:theory_ambig}
\begin{figure*}[t]
  \centering
  \includegraphics[width=\linewidth]{iclr2023/images/boxplot_effective_disparities.pdf}
  \captionsetup{aboveskip=0pt}\captionsetup{belowskip=0pt}\caption{Visualization of absolute value of change in projected location for object centers induced by a 0.5m change in depth.}
  \label{fig:boxplot_effective_disparities}
\end{figure*} We find that with the single timestep aggregation used in many methods, less than 20 \% of change in object center projection is larger than 1 pixel for objects at 40m - 60m, making accurate multi-view localization impossible. By leveraging 16 past timesteps, we significantly ease multi-view depth estimation (note that for frames with less than 16 timesteps of history, we use as many is available). We do note, however, that the critical front camera is the most difficult view. This is because as seen in Figure \ref{fig:optimal_time_all_cams}, points in the front camera can only be projected to itself and are unable to leverage multi-camera depth estimation. However, we find that multi-timestep aggregation can bring \% of change in object center projection $>$ 1px from 17\% and 0.4\% to 53\% and 22\% for objects at 20m\-40m and 40m\-60m, respectively, significantly decreasing the safety risk of front depth estimation. The numerical values of changes in projected location can be seen in Figure \ref{fig:boxplot_effective_disparities}. As ease of depth estimation isn't simply a binary indicator of "possible" or "not possible", the actual distance between projected locations of two depth hypotheses matters as well. We find that these values increase over various cameras and depths with more temporal aggregation, demonstrating that increased temporal history significantly eases multi-view depth estimation. \section{Additional Details for High-Resolution, Short-Term Depth Hypothesis Sampling}\label{app:sampling}
In this section, we provide more details for our Gaussian-Spaced Top-k depth candidate sampling method. More specifically, we outline the Gaussian-based down-weighting in greater detail. Let $\sigma$ be a the standard deviation over depth used for Gaussian-base down-weighting, and let $P_l(d)$ as the monocular depth probability of some depth candidate $d$ at sampling iteration $l$. Note that the monocular depth probability is updated over multiple iterations via our down-weighting method. 

Suppose $d_l$ is the depth candidate chosen at sampling iteration $l$. Write $P_{\mathcal{N}(\mu, \sigma)}(x)$ as the probability mass at location $x$ in a normal distribution with mean $\mu$ and standard deviation $\sigma$. Then, the monocular depth probability at some depth location $d$ is updated to be:
\begin{equation}\label{eq:gauss}
    P_{l+1}(d) = P_l(d) * (1 - P_{\mathcal{N}(d_l, \sigma)}(d) * \sigma \sqrt{2\pi})
\end{equation}
This down weights the monocular probability at $d$ by normalized inverse Gaussian factor, with the decrease being larger the closer $d$ is to $d_l$. We find that this simple formulation is enough to force the model to choose depth hypotheses that maintain an effective trade-off between ''exploiting'' the monocular prior and ''exploring'' other depth candidates. \section{Additional Details for Experimental Setting}\label{app:exp}
\subsection{Implementation Details}
We adopt state-of-the-art BEVDepth \citep{li2022bevdepth} as our baseline model and conduct experiments with ImageNet pretrained ResNet50 and ResNet101 backbones \citep{He2016DeepRL}. We use $T = 16$ timesteps for long-term fusion and $k = 7$ depth hypotheses. During both training and inference, we save past BEV feature maps and use them for later timesteps, keeping our pipeline efficient despite using long-term temporal fusion. For short-term, high-resolution stereo matching, we use a small FPN to reduce the matching channel dimension to 64. Group correlation \citep{Guo2019GroupWiseCS} is used for matching. For BEV pooling, we adopt the fast implementation from \citep{Liu2022BEVFusionMM}.

As our method processes frames sequentially, the commonly used CBGS \citep{Zhu2019ClassbalancedGA} training scheme is not readily applicable to our framework. As such, to compare with methods that use CBGS, we simply increase the number of training iterations to match the CBGS cycle without other changes. We emphasize our setting is a \textit{significantly disadvantaged} setting as CBGS is known to substantially boost performance in rarer categories.   \end{document}
