

\documentclass[runningheads]{llncs}
\usepackage{graphicx}
\usepackage{comment}
\usepackage{amsmath,amssymb} \usepackage{color}


\usepackage{siunitx}
\usepackage{cite}
\usepackage{ctable}
\usepackage{hhline}
\usepackage{booktabs}
\usepackage{csquotes}
\usepackage{pifont}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage[super]{nth}

\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}

\renewcommand{\labelitemi}{}

\begin{document}
\pagestyle{headings}
\mainmatter

\title{I2L-MeshNet: Image-to-Lixel Prediction Network for Accurate 3D Human Pose and Mesh Estimation from a Single RGB Image} 

\begin{comment}
\titlerunning{ECCV-20 submission ID \ECCVSubNumber} 
\authorrunning{ECCV-20 submission ID \ECCVSubNumber} 
\author{Anonymous ECCV submission}
\institute{Paper ID \ECCVSubNumber}
\end{comment}


\titlerunning{I2L-MeshNet}
\author{Gyeongsik Moon \and
Kyoung Mu Lee}
\authorrunning{G. Moon and K. M. Lee}
\institute{ECE \& ASRI, Seoul National University, Korea \\
\email{\{mks0601,kyoungmu\}@snu.ac.kr}}
\maketitle

\begin{abstract}
Most of the previous image-based 3D human pose and mesh estimation methods estimate parameters of the human mesh model from an input image.
However, directly regressing the parameters from the input image is a highly non-linear mapping because it breaks the spatial relationship between pixels in the input image. 
In addition, it cannot model the prediction uncertainty, which can make training harder.
To resolve the above issues, we propose I2L-MeshNet, an image-to-lixel (line+pixel) prediction network.
The proposed I2L-MeshNet predicts the per-lixel likelihood on 1D heatmaps for each mesh vertex coordinate instead of directly regressing the parameters.
Our lixel-based 1D heatmap preserves the spatial relationship in the input image and models the prediction uncertainty.
We demonstrate the benefit of the image-to-lixel prediction and show that the proposed I2L-MeshNet outperforms previous methods.
The code is publicly available \footnote{\url{https://github.com/mks0601/I2L-MeshNet_RELEASE}}.
\end{abstract}

\section{Introduction}

\begin{figure}[t]
\begin{center}
\includegraphics[width=1.0\linewidth]{fig/intro_qualitative.pdf}
\end{center}
   \caption{
   Qualitative results of the proposed I2L-MeshNet on MSCOCO~\cite{lin2014microsoft} and FreiHAND~\cite{Freihand2019} datasets.
   }
\label{fig:intro_qualitative}
\end{figure}

3D human pose and mesh estimation aims to simultaneously recover 3D semantic human joint and 3D human mesh vertex locations. 
This is a very challenging task because of complicated human articulation and 2D-to-3D ambiguity.
It can be used in many applications such as virtual/augmented reality and human action recognition.

SMPL~\cite{loper2015smpl} and MANO~\cite{romero2017embodied} are the most widely used parametric human body and hand mesh models, respectively, which can represent various human poses and identities.
They produce 3D human joint and mesh coordinates from pose and identity parameters.
Recent deep convolutional neural network (CNN)-based studies~\cite{kanazawa2018end,pavlakos2018learning,kolotouros2019learning} for the 3D human pose and mesh estimation are based on the model-based approach, which trains a network to estimate SMPL/MANO parameters from an input image. 
On the other hand, there have been few methods based on model-free approach~\cite{ge20193d,kolotouros2019convolutional}, which estimates mesh vertex coordinates directly.
They obtain the 3D pose by multiplying a joint regression matrix, included in the human mesh model, to the estimated mesh.

Although the recent deep CNN-based methods perform impressive, when estimating the target (\textit{i.e.}, SMPL/MANO parameters or mesh vertex coordinates), all of the previous 3D human pose and mesh estimation works break the spatial relationship among pixels in the input image because of the fully-connected layers at the output stage.
In addition, their target representations cannot model the uncertainty of the prediction.
The above limitations can make training harder, and as a result, reduce the test accuracy as addressed in~\cite{moon2018v2v,tompson2014joint}.
To address the limitations, recent state-of-the-art 3D human pose estimation methods~\cite{moon2018v2v,moon2019camera,sun2018integral}, which localize 3D human joint coordinates without mesh vertex coordinates, utilize the \emph{heatmap} as the target representation of their networks.
Each value of one heatmap represents the likelihood of the existence of a human joint at the corresponding pixel positions of the input image and discretized depth value.
Therefore, it preserves the spatial relationship between pixels in the input image and models the prediction uncertainty.


Inspired by the recent state-of-the-art heatmap-based 3D human pose estimation methods, we propose I2L-MeshNet, image-to-lixel prediction network that naturally extends heatmap-based 3D human pose to heatmap-based 3D human pose and mesh.
Likewise voxel (volume+pixel) is defined as a quantized cell in three-dimensional space, we define \emph{lixel (line+pixel)} as a quantized cell in one-dimensional space.
Our I2L-MeshNet estimates per-lixel likelihood on 1D heatmaps for each mesh vertex coordinates, therefore it is based on the model-free approach.
The previous state-of-the-art heatmap-based 3D human pose estimation methods predict 3D heatmap of each human joint.
Unlike the number of human joints, which is around 20, the number of mesh vertex is much larger (\textit{e.g.}, 6980 for SMPL and 776 for MANO).
As a result, predicting 3D heatmaps of all mesh vertices becomes computationally infeasible, which is beyond the limit of modern GPU memory.
In contrast, the proposed lixel-based 1D heatmap has an efficient memory complexity, which has a linear relationship with the heatmap resolution.
Thus, it allows our system to predict heatmaps with sufficient resolution, which is essential for dense mesh vertex localization.


For more accurate 3D human pose and mesh estimation, we design the I2L-MeshNet as a cascaded network architecture, which consists of PoseNet and MeshNet. 
The PoseNet predicts the lixel-based 1D heatmaps of each 3D human joint coordinate.
Then, the MeshNet utilizes the output of the PoseNet as an additional input along with the image feature to predict the lixel-based 1D heatmaps of each 3D human mesh vertex coordinate.
As the locations of the human joints provide coarse but important information about the human mesh vertex locations, utilizing it for 3D mesh estimation is natural and can increase accuracy substantially.


Our I2L-MeshNet outperforms previous 3D human pose and mesh estimation methods on various 3D human pose and mesh benchmark datasets.
Figure~\ref{fig:intro_qualitative} shows 3D human body and hand mesh estimation results on publicly available datasets.


Our contributions can be summarized as follows.
\begin{itemize}
\item We propose I2L-MeshNet, a novel image-to-lixel prediction network for 3D human pose and mesh estimation from a single RGB image.
Our system predicts lixel-based 1D heatmap that preserves the spatial relationship in the input image and models the uncertainty of the prediction.
\item Our efficient lixel-based 1D heatmap allows our system to predict heatmaps with sufficient resolution, which is essential for dense mesh vertex localization.
\item We show that our I2L-MeshNet outperforms previous state-of-the-art methods on various 3D human pose and mesh datasets.
\end{itemize}
 \section{Related works}

\iffalse
\noindent \textbf{3D human body pose estimation.}
Current 3D human body pose estimation methods can be categorized into RGB-based and 2D pose-based approaches.
The RGB-based approach takes an RGB image as an input. 
Tekin~et al.~\cite{tekin2016structured} modeled high-dimensional joint dependencies by adopting an auto-encoder structure. 
Pavlakos~et al.~\cite{pavlakos2017coarse} extended a U-net shaped network to estimate a 3D heatmap for each joint. They used a coarse-to-fine approach to boost performance.
Zhou~et al.~\cite{zhou2017weaklysupervised} proposed a geometric loss to facilitate weakly-supervised learning of the depth regression module with images in the wild. 
Sun~et al.~\cite{sun2017compositional} introduced compositional loss to consider the joint connection structure.
Yang~et al.~\cite{yang20183d} utilized adversarial loss to handle the 3D human pose estimation in the wild.
Sun~et al.~\cite{sun2018integral} used soft-argmax operation to obtain the 3D coordinates of body joints in a differentiable manner from the 3D heatmaps.
Kocabas~et al.~\cite{kocabas2019self} designed a self-supervised 3D human pose estimation system using epipolar geometry.
Moon~et al.~\cite{moon2019camera} proposed a general 3D multi-person pose estimation framework that can predict the distance between the camera and human root joint. 

The 2D pose-based approach takes a 2D pose as an input, which is obtained from off-the-shelf 2D human pose estimation methods. 
Martinez~et al.~\cite{martinez2017simple} proposed a fully-connected layer-based simple network that directly regresses the 3D coordinates of body joints from 2D coordinates. 
Zhao~et al.~\cite{zhao2019semantic} considered human joint coordinates as a graph structure and processed 2D human pose using their proposed semantic graph CNN. 
Cai~et al.~\cite{cai2019exploiting} also used graph CNN to process both spatial and temporal information. 
Chang~et al.~\cite{chang2019absposelifter} introduced an absolute 3D pose estimation network, which additionally utilizes mean and standard deviation of the 2D pose coordinates. 


\noindent \textbf{3D human hand pose estimation.}
Current 3D human hand pose estimation methods can be categorized into depth map-based and RGB-based approaches.
The depth map-based approach takes a depth map as an input.
Early depth map-based 3D hand pose estimation methods are mainly based on a generative approach, which fits a pre-defined hand model to the input depth image by minimizing hand-crafted cost functions~\cite{sharp2015accurate, tang2015opening}. 
Recent deep CNN-based methods are mainly based on a discriminative approach, which directly localizes hand joints from an input depth map.
Ge~et al.~\cite{ge2016robust} predicted multi-view 2D heatmaps of each hand joint.
Guo~et al.~\cite{guo2017ren} proposed a region ensemble network to estimate the 3D coordinates of hand keypoints accurately.
Moon~et al.~\cite{moon2018v2v} designed a 3D CNN model that takes voxel input and outputs a voxel-based 3D heatmap for each hand joint.
Wan~et al.~\cite{wan2019self} proposed a self-supervised system, which can be trained only from an input depth map. 

The RGB-based approach takes an RGB image as an input.
Zimmermann~et al.~\cite{zimmermann2017learning} proposed a deep neural network that learns a network-implicit 3D articulation prior.
Mueller~et al.~\cite{mueller2018ganerated} used an image-to-image translation model to generate a realistic hand pose dataset from a synthetic dataset.
Cai~et al.~\cite{cai2018weakly} and Iqbal~et al.~\cite{iqbal2018hand} implicitly reconstruct depth map and estimate 3D hand keypoint coordinates from it.
Spurr~et al.~\cite{spurr2018cross} and Yang~et al.~\cite{yang2019disentangling} proposed deep generative models to learn latent space for hand.
Yang~et al.~\cite{yang2019aligning} modeled multi-modal hand distribution using variational auto-encoder.
\fi

\noindent \textbf{3D human body and hand pose and mesh estimation.}
Most of the current 3D human pose and mesh estimation methods are based on the model-based approach, which predict parameters of pre-defined human body and hand mesh models (\textit{i.e.}, SMPL and MANO, respectively).
The model-based methods can be trained only from groundtruth human joint coordinates without mesh vertex coordinates because the model parameters are embedded in low dimensional space.
Early model-based methods~\cite{bogo2016keep} iteratively fit the SMPL parameters to estimated 2D human joint locations. 
More recent model-based methods regress the body model parameters from an input image using CNN. 
Kanazawa~et al.~\cite{kanazawa2018end} proposed an end-to-end trainable human mesh recovery (HMR) system that uses the adversarial loss to make their output human shape is anatomically plausible. 
Pavlakos~et al.~\cite{pavlakos2018learning} used 2D joint heatmaps and silhouette as cues for predicting accurate SMPL parameters. 
Omran~et al.~\cite{omran2018neural} proposed a similar system, which exploits human part segmentation as a cue for regressing SMPL parameters. 
Xu~et al.~\cite{xu2019denserac} used differentiable rendering to supervise human mesh in the 2D image space. 
Pavlakos~et al.~\cite{pavlakos2019texturepose} proposed a system that uses multi-view color consistency to supervise a network using multi-view geometry. 
Baek~et al.~\cite{baek2019pushing} trained their network to estimate the MANO parameters using a differentiable renderer. 
Boukhayma~et al.~\cite{boukhayma20193d} trained their network that takes a single RGB image and estimates MANO parameters by minimizing the distance of the estimated hand joint locations and groundtruth.
Kolotouros~et al.~\cite{kolotouros2019learning} introduced a self-improving system consists of SMPL parameter regressor and iterative fitting framework~\cite{bogo2016keep}. 


On the other hand, the model-free approach estimates the mesh vertex coordinates directly instead of regressing the model parameters. 
Due to the recent advancement of the iterative human body and hand model fitting frameworks~\cite{bogo2016keep,pavlakos2019expressive,Freihand2019}, pseudo-groundtruth mesh vertex annotation on large-scale datasets~\cite{ionescu2014human3,lin2014microsoft,Freihand2019,von2018recovering} became available.
Those datasets with mesh vertex annotation motivated several model-free methods that require mesh supervision.
Kolotouros~et al.~\cite{kolotouros2019convolutional} designed a graph convolutional human mesh regression system.
Their graph convolutional network takes a template human mesh in a rest pose as input and outputs mesh vertex coordinates using image feature from ResNet~\cite{he2016deep}.
Ge~et al.~\cite{ge20193d} proposed a graph convolution-based network which directly estimates vertices of hand mesh.
Recently, Choi~et al.~\cite{choi2020p2m} proposed a graph convolutional network that recovers 3D human pose and mesh from a 2D human pose.


Unlike all the above model-based and model-free 3D human pose and mesh estimation methods, the proposed I2L-MeshNet outputs 3D human pose and mesh by preserving the spatial relationship between pixels in the input image and modeling uncertainty of the prediction.
Those two main advantageous are brought by designing the target of our network to the lixel-based 1D heatmap.
This can make training much stable, and the system achieves much lower test error. 

\begin{figure}[t]
\begin{center}
\includegraphics[width=0.6\linewidth]{fig/overall_pipeline.pdf}
\end{center}
   \caption{
   Overall pipeline of the proposed I2L-MeshNet. 
   }
\label{fig:overall_pipeline}
\end{figure}


\noindent \textbf{Heatmap-based 3D human pose estimation.}
Most of the recent state-of-the-art 2D and 3D human pose estimation methods use heatmap as a prediction target, which preserves the spatial relationship in the input image and models the uncertainty of the prediction. 
Tompson~et al.~\cite{tompson2014joint} proposed to estimate the Gaussian heatmap instead of directly regressing coordinates of human body joints. 
Their heatmap representation helps their model to perform 2D human pose estimation more accurate and motivated many heatmap-based 2D human pose methods~\cite{newell2016stacked,chen2018cascaded,xiao2018simple}. 
Pavlakos~et al.~\cite{pavlakos2017coarse} and Moon~et al.~\cite{moon2018v2v} firstly proposed to use 3D heatmaps as a prediction target for 3D human body pose and 3D hand pose estimation, respectively. 
Especially, Moon~et al.~\cite{moon2018v2v} demonstrated that under the same setting, changing prediction target from coordinates to heatmap significantly improves the 3D hand pose accuracy while requires much less amount of the learnable parameters.
Recently, Moon~et al.~\cite{moon2019camera} achieved significantly better 3D multi-person pose estimation accuracy using 3D heatmap compared with previous coordinate regression-based methods~\cite{rogez2017lcr}.


 

\begin{figure}[t]
\begin{center}
\includegraphics[width=1.0\linewidth]{fig/lixel_heatmap.pdf}
\end{center}
   \caption{
   Network architecture to predict lixel-based 1D heatmaps and visualized examples of feature maps and the 1D heatmaps.
   }
\label{fig:lixel_1d_heatmap}
\end{figure}


\section{I2L-MeshNet}
Figure~\ref{fig:overall_pipeline} shows the overall pipeline of the proposed I2L-MeshNet.
I2L-MeshNet consists of PoseNet and MeshNet, which will be described in the following subsections.

\subsection{PoseNet}
The PoseNet estimates three lixel-based 1D heatmaps of all human joints  from the input image . 
 and  are defined in - and -axis of the image space, while  is defined in root joint (\textit{i.e.}, pelvis or wrist)-relative depth space.
For this, PoseNet extracts image feature  from the input image by ResNet~\cite{he2016deep}.
Then, three upsampling modules increases the spatial size of  by 8 times, while changing channel dimension from  to .
Each upsampling module consists of deconvolutional layer, 2D batch normalization layer~\cite{ioffe2015batch}, and ReLU function.
The upsampled features are used to compute lixel-based 1D human pose heatmaps, as illustrated in Figure~\ref{fig:lixel_1d_heatmap} (a).
We obtain - and -axis 1D human pose heatmaps as follows:

where  denotes the three upsampling modules of the PoseNet.
 and  denote -axis marginalization by averaging and a 1-by-1 1D convolution that changes channel dimension from  to  for -axis 1D human pose heatmap estimation, respectively.


We obtain -axis 1D human pose heatmaps as follows:

where  and  denote a building block and and reshape function, respectively.
The building block consists of a fully-connected layer, 1D batch normalization layer, and ReLU function, and it changes the activation size from  to .
 denotes depth discretization size and is equal to .
We convert the discretized heatmaps of  to continuous coordinates  by soft-argmax~\cite{sun2018integral}.

\subsection{MeshNet}
The MeshNet has a similar network architecture with that of the PoseNet.
Instead of taking the input image , MeshNet takes a pre-computed image feature from the PoseNet  and 3D Gaussian heatmap .
 is the input of the first residual block of the PoseNet whose spatial dimension is .
 is obtained from  as follows:

where ,  and  are th joint -, -, and -axis coordinates from , respectively.
 is set to 2.5.

From  and , we obtain image feature  as follows:

where  and  denote reshape function and concatenation along the channel dimension, respectively.
 is a convolutional block that consists of a 3-by-3 convolutional layer, 2D batch normalization layer, and ReLU function.
It changes the channel dimension of the input to the input channel dimension of the first residual block of the ResNet.
 is the ResNet starting from the first residual block.

From the , MeshNet outputs three lixel-based 1D heatmaps of all mesh vertices  in an exactly the same manner with that of PoseNet, as illustrated in Figure~\ref{fig:lixel_1d_heatmap} (a).
Likewise heatmaps of PoseNet,  and  are defined in - and -axis of the image space, while  is defined in root joint-relative depth space.
We obtain - and -axis 1D human mesh heatmaps as follows:

where  denotes the three upsampling modules of the MeshNet.
 denote a 1-by-1 1D convolution that changes channel dimension from  to  for -axis 1D human mesh heatmap estimation, respectively.
Figure~\ref{fig:lixel_1d_heatmap} (b) shows visualized , , and .

We obtain -axis 1D human mesh heatmaps as follows:

where  and  denote a building block and and reshape function, respectively.
The building block consists of a fully-connected layer, 1D batch normalization layer, and ReLU function, and it changes the activation size from  to .
Likewise we did in the PoseNet, we convert the discretized heatmaps of  to continuous coordinates  by soft-argmax~\cite{sun2018integral}.

\subsection{Final 3D human pose and mesh}
The final 3D human mesh  and pose  are obtained as follows:

\noindent where , , and  denote camera back-projection, inverse affine transformation (\textit{i.e.}, 2D crop and resize), and -axis offset whose element is a depth of the root joint, respectively.
 is obtained from RootNet~\cite{moon2019camera}.
We use normalized camera intrinsic parameters if not available following Moon~et al.~\cite{moon2019camera}.
 is a joint regression matrix defined in SMPL or MANO model.

\subsection{Loss functions} ~\label{section:loss_functions}

\textbf{PoseNet pose loss.}
To train the PoseNet, we use  loss function defined as follows:

where  indicates groundtruth. 
-axis loss becomes zero if -axis groundtruth is unavailable.

\textbf{MeshNet pose loss.}
To train the MeshNet to predict mesh vertex aligned with body joint locations, we use  loss function defined as follows:

where  indicates groundtruth. 
-axis loss becomes zero if -axis groundtruth is unavailable.

\textbf{Mesh vertex loss.}
To train the MeshNet to output mesh vertex heatmaps, we use  loss function defined as follows:

where  indicates groundtruth. 
-axis loss becomes zero if -axis groundtruth is unavailable.

\textbf{Mesh normal vector loss.}
Following Wang~et al.~\cite{wang2018pixel2mesh}, we supervise normal vector of predicted mesh to get visually pleasing mesh result. The  loss function for normal vector supervision is defined as follows:

where  and  indicate a mesh face and unit normal vector of face , respectively. 
 and  denote th and th vertex coordinates of , respectively.
 is computed from , where  denotes groundtruth. 
The loss becomes zero if groundtruth 3D mesh is unavailable.

\textbf{Mesh edge length loss.}
Following Wang~et al.~\cite{wang2018pixel2mesh}, we supervise edge length of predicted mesh to get visually pleasing mesh result. The  loss function for edge length supervision is defined as follows:

where  and  indicate mesh face and groundtruth, respectively. 
 and  denote th and th vertex coordinates of , respectively.
The loss becomes zero if groundtruth 3D mesh is unavailable.

We train our I2L-MeshNet in an end-to-end manner using all the five loss functions as follows:

where  is a weight of .
For the stable training, we do not back-propagate gradients before .

 \section{Implementation details}~\label{section:implementation_details}

PyTorch~\cite{paszke2017automatic} is used for implementation. 
The backbone part is initialized with the publicly released ResNet-50~\cite{he2016deep} pre-trained on the ImageNet dataset~\cite{russakovsky2015imagenet}, and the weights of the remaining part are initialized by Gaussian distribution with . 
The weights are updated by the Adam optimizer~\cite{kingma2014adam} with a mini-batch size of 48. 
To crop the human region from the input image, we use groundtruth bounding box in both of training and testing stages following previous works~\cite{kanazawa2018end,kolotouros2019convolutional,kolotouros2019learning}.
When the bounding box is not available in the testing stage, we trained and tested Mask R-CNN~\cite{he2017mask} to get the bounding box.
The cropped human image is resized to 256256, thus  and .
Data augmentations including scaling (25\%), rotation (\ang{60}), random horizontal flip, and color jittering (20\%) is performed in training.
The initial learning rate is set to  and reduced by a factor of 10 at the \nth{10} epoch.
We train our model for 12 epochs with three NVIDIA RTX 2080Ti GPUs, which takes 36 hours for training. 
Our I2L-MeshNet runs at a speed of 25 frames per second (fps).
 \section{Experiment}

\subsection{Datasets and evaluation metrics} ~\label{section:dataset}

\noindent \textbf{Human3.6M.}
Human3.6M~\cite{ionescu2014human3} contains 3.6M video frames with 3D joint coordinate annotations. 
Because of the license problem, previously used groundtruth SMPL parameters of the Human3.6M are inaccessible.
Alternatively, we used SMPLify-X~\cite{pavlakos2019expressive} to obtain groundtruth SMPL parameters.
Please see the supplementary material for a detailed description of SMPL parameters of the Human3.6M.
MPJPE and PA MPJPE are used for the evaluation~\cite{moon2019camera}, which is Euclidean distance (mm) between predicted and groundtruth 3D joint coordinates after root joint alignment and further rigid alignment, respectively.


\noindent \textbf{3DPW.}
3DPW~\cite{von2018recovering} contains 60 video sequences captured mostly in outdoor conditions. 
We use this dataset only for evaluation on its defined test set following Kolotouros~et al.~\cite{kolotouros2019learning}.
The same evaluation metrics with Human3.6M (\textit{i.e.}, MPJPE and PA MPJPE) are used, following Kolotouros~et al.~\cite{kolotouros2019learning}.


\noindent \textbf{FreiHAND.}
FreiHAND~\cite{Freihand2019} contains real-captured 130K training images and 4K test images with MANO pose and shape parameters.
The evaluation is performed at an online server. 
Following Zimmermann~et al.~\cite{Freihand2019}, we report PA MPVPE, PA MPJPE, and F-scores.


\noindent \textbf{MSCOCO.}
MSCOCO~\cite{lin2014microsoft} contains large-scale in-the-wild images with 2D bounding box and human joint coordinates annotations.
We fit SMPL using SMPLify-X~\cite{pavlakos2019expressive} on the groundtruth 2D poses, and used the fitted meshes as groundtruth 3D meshes.
This dataset is used only for the training.


\noindent \textbf{MuCo-3DHP.}
MuCo-3DHP~\cite{mehta2018single} is generated by compositing the existing MPI-INF-3DHP 3D~\cite{mehta2017monocular}.
200K frames are composited, and half of them have augmented backgrounds.
We used images of MSCOCO dataset that do not include humans to augment the backgrounds following Moon~et al.~\cite{moon2019camera}.
This dataset is used only for the training.


\subsection{Ablation study}
All models for the ablation study are trained and tested on Human3.6M. 
As Human3.6M is the most widely used large-scale benchmark, we believe this dataset is suitable for the ablation study. 

\noindent \textbf{Benefit of the heatmap-based mesh estimation.}
To demonstrate the benefit of the heatmap-based mesh estimation, we compare models with various target representations of the human mesh, such as SMPL parameters, vertex coordinates, and heatmap. 
Table~\ref{table:ablation_hm} shows MPJPE, the number of parameters, and the GPU memory usage comparison between models with different targets.
The table shows that our heatmap-based mesh estimation network achieves the lowest errors while using the smallest number of the parameters and consuming small GPU memory.


The superiority of our heatmap-based mesh estimation network is in two folds.
First, it can model the uncertainty of the prediction.
To validate this, we trained two models that estimate the camera-centered mesh vertex coordinates directly and estimates lixel-based 1D heatmap of the coordinates using two fully-connected layers.
Note that the targets of the two models are the same, but their representations are different.
As the first network regresses the coordinates directly, it cannot model the uncertainty on the prediction, while the latter one can because of the heatmap target representation.
However, both do not preserve the spatial relationship in the input image because of the global average pooling and the fully-connected layers.
As the second and third row of the table show, modeling uncertainty on the prediction significantly decreases the errors while using a smaller number of parameters.
In addition, it achieves lower errors than the SMPL parameter regression model, which is the most widely used target representation but cannot model the uncertainty.

\begin{table}[t]
\centering
\setlength\tabcolsep{1.0pt}
\def\arraystretch{1.1}
\scalebox{1.0}{
\begin{tabular}{C{4.0cm}|C{1.0cm}C{2.0cm}C{1.5cm}C{1.8cm}C{1.6cm}}
\specialrule{.1em}{.05em}{.05em}
targets & spatial & uncertainty & MPJPE & no. param. & GPU mem.\\ \hline
SMPL param. & \xmark & \xmark & 100.3 & 91M & 4.3 GB \\
xyz coord. & \xmark & \xmark & 114.3 & 117M & 5.4 GB\\
xyz lixel hm. wo. spatial & \xmark & \cmark & 92.6 & 82M & 4.5 GB\\
\textbf{xyz lixel hm. (ours)} & \cmark & \cmark & \textbf{86.2} & \textbf{73M} & \textbf{4.6 GB} \\
 \specialrule{.1em}{.05em}{.05em}
\end{tabular}
}
\caption{The MPJPE, the number of parameters, and the GPU memory usage comparison between various target representations on Human3.6M.}
\label{table:ablation_hm}
\end{table}

\begin{table}[t]
\centering
\setlength\tabcolsep{1.0pt}
\def\arraystretch{1.1}
\scalebox{1.0}{
\begin{tabular}{C{4.3cm}|C{2.2cm}C{1.8cm}C{1.3cm}C{1.8cm}}
\specialrule{.1em}{.05em}{.05em}
targets & mem. complx. & resolution & MPJPE & GPU mem.\\ \hline
\multirow{2}{*}{xyz voxel hm.} & \multirow{2}{*}{} & 888 & 102.8 & 4.3 GB\\
& & 161616 & - & OOM \\ \hline
\multirow{3}{*}{xy pixel hm. + z lixel hm.} & \multirow{3}{*}{} & 88, 8 & 97.9 & 3.5 GB\\
& & 3232, 32 & 89.4 & 5.7 GB\\
& & 6464, 64 & - & OOM \\ \hline
\multirow{3}{*}{\textbf{xyz lixel hm. (ours)}} & \multirow{3}{*}{} & 8, 8, 8 & 100.2 & 3.4 GB\\
& & 32, 32 ,32 & 94.8 & 4.0 GB\\
& & 64, 64, 64 & \textbf{86.2} & \textbf{4.6 GB} \\
 \specialrule{.1em}{.05em}{.05em}
\end{tabular}
}
\caption{The MPJPE and the GPU memory usage comparison between various heatmap representations on Human3.6M.}
\label{table:ablation_lixel}
\end{table}

Second, it preserves the spatial relationship between pixels in the input image.
The final model estimates the - and -axis heatmaps of each mesh vertex in a fully-convolutional way, thus preserves the spatial relationship.
It achieves the best performance with the smallest number of the parameters while consuming similar GPU memory usage compared with SMPL parameter regression method that requires the least amount of GPU memory.


In Table~\ref{table:ablation_hm}, all models have the same network architecture with our I2L-MeshNet except for the final output prediction part.
We removed PoseNet from all models, and the remaining MeshNet directly estimates targets from the input image .
Except for the last row (ours), all settings output targets using two fully-connected layers.
We followed the training details of ~\cite{kanazawa2018end,kolotouros2019learning} for the SMPL parameter estimation.

\begin{table}[t]
\centering
\setlength\tabcolsep{1.0pt}
\def\arraystretch{1.1}
\scalebox{1.0}{
\begin{tabular}{C{4.6cm}|C{1.3cm}C{1.5cm}C{1.7cm}}
\specialrule{.1em}{.05em}{.05em}
settings & 3D pose & MPJPE & PA MPJPE  \\ \hline
MeshNet & \xmark & 86.2 & 59.8 \\
\textbf{PoseNet+MeshNet (ours)} & \cmark & \textbf{81.8} & \textbf{58.0} \\ \hline
MeshNet & GT & 25.5 & 17.1 \\
 \specialrule{.1em}{.05em}{.05em}
\end{tabular}
}
\caption{The MPJPE and PA MPJPE comparison between various network cascading strategies on Human3.6M.}
\label{table:ablation_3dpose}
\end{table}


\noindent \textbf{Lixel-based vs. pixel-based vs. voxel-based heatmap.}
To demonstrate the effectiveness of the lixel-based 1D heatmap over other heatmap representations, we train three models that predict lixel-based, pixel-based, and voxel-based heatmap, respectively.
We used the same network architecture (\textit{i.e.}, MeshNet of the I2L-MeshNet) for all settings except for the final prediction part.
Their networks directly predict the heatmaps from the input image.
-, -, and -axis of each heatmap represents the same coordinates.
Table~\ref{table:ablation_lixel} shows memory complexity, heatmap resolution, MPJPE and GPU memory usage comparison between models that predict different target representations of human mesh. 
The table shows that our lixel-based one achieves the lowest error while consuming small GPU memory usage.

Compared with the pixel-based and voxel-based heatmap, our lixel-based one consumes much less amount of GPU memory under the same resolution.
The  voxel-based heatmap requires similar GPU memory usage with that of  lixel-based one, and we found that enlarging the voxel-based heatmap size from it is not allowed in current GPU memory limit (\textit{i.e.}, 12 GB).
The pixel-based heatmap is more efficient than the voxel-based one; however still much inefficient than our lixel-based one, which makes enlarging from  impossible.
This inefficient memory usage limits the heatmap resolution; however, we found that the heatmap resolution is critical for dense mesh vertex localization.
On the other hand, the memory complexity of our lixel-based heatmap is a linear function with respect to ; thus, we can predict high-resolution heatmap for each mesh vertex.
The memory efficiency will be more important when a high-resolution human mesh model is used.


Under the same resolution, the combination of pixel-based heatmap and lixel-based heatmap achieves the best performance.
We think that estimating the voxel-based heatmap involves too many parameters at a single output layer, which makes it produce high errors.
In addition, lixel-based heatmap inherently involves spatial ambiguity arises from marginalizing the 2D feature map to 1D, which can be a possible reason for worse performance than the combined one.

\begin{table}[t]
\setlength{\tabcolsep}{1pt}
\centering
\scalebox{1.0}{
\begin{tabular}{C{4.0cm}|C{1.5cm}C{1.7cm}|C{1.5cm}C{1.7cm}}
\specialrule{.1em}{.05em}{.05em}
\multirow{ 2}{*}{methods} & \multicolumn{2}{c|}{Human3.6M} & \multicolumn{2}{c}{3DPW} \\
                       & MPJPE & PA MPJPE & MPJPE & PA MPJPE\\ \hline
HMR~\cite{kanazawa2018end} & 153.2 & 85.5 & 300.4 & 137.2 \\
GraphCMR~\cite{kolotouros2019convolutional} & 78.3 & 59.5 & 126.5 & 80.1 \\
SPIN~\cite{kolotouros2019learning} & 72.9 & 51.9 & 113.1 & 71.7 \\
\textbf{I2L-MeshNet (Ours)} & \textbf{55.7} & \textbf{41.7} & \textbf{95.4} & \textbf{60.8} \\
 \specialrule{.1em}{.05em}{.05em}
\end{tabular}
}
\caption{The MPJPE and PA MPJPE comparison on Human3.6M and 3DPW. All methods are trained on Human3.6M and MSCOCO.}
\label{table:compare_h36m_3dpw_same_dataset}
\end{table}

\begin{table}[t]
\setlength{\tabcolsep}{1pt}
\begin{minipage}{.45\linewidth}
\centering

\scalebox{1.0}{
\begin{tabular}{C{2.5cm}|C{1.3cm}C{1.7cm}}
\specialrule{.1em}{.05em}{.05em}
methods &  MPJPE & PA MPJPE \\ \hline
SMPLify~\cite{bogo2016keep} & - & 82.3 \\
Lassner~\cite{lassner2017unite} &- & 93.9 \\
HMR~\cite{kanazawa2018end} & 88.0 & 56.8  \\
NBF~\cite{omran2018neural} & - & 59.9 \\
Pavlakos~\cite{pavlakos2018learning} & - & 75.9  \\
Kanazawa~\cite{kanazawa2019learning} & - & 56.9  \\
GraphCMR~\cite{kolotouros2019convolutional} & - & 50.1  \\
Arnab~\cite{arnab2019exploiting} &77.8 & 54.3  \\
SPIN~\cite{kolotouros2019learning} & - & \textbf{41.1}  \\
\textbf{I2L-MeshNet (Ours)} & \textbf{55.7} & \textbf{41.1} \\
 \specialrule{.1em}{.05em}{.05em}
\end{tabular}
}
\caption{The MPJPE and PA MPJPE comparison on Human3.6M. Each method is trained on different datasets.}
\label{table:compare_h36m_different_dataset}
\end{minipage}\hfill
\begin{minipage}{.50\linewidth}
\centering
\scalebox{1.0}{
\begin{tabular}{C{2.5cm}|C{1.3cm}C{1.7cm}}
\specialrule{.1em}{.05em}{.05em}
methods & MPJPE & PA MPJPE \\ \hline
HMR~\cite{kanazawa2018end} & - & 81.3 \\
Kanazawa~\cite{kanazawa2019learning}  & - & 72.6 \\
GraphCMR~\cite{kolotouros2019convolutional} & - & 70.2 \\
Arnab~\cite{arnab2019exploiting}  & - & 72.2 \\
SPIN~\cite{kolotouros2019learning}  & - & 59.2 \\
\textbf{I2L-MeshNet (Ours)} & \textbf{93.2} & \textbf{57.7} \\
\textbf{I2L-MeshNet (Ours) + SMPL regress} & 100.0 & 60.0 \\
 \specialrule{.1em}{.05em}{.05em}
\end{tabular}
}
\caption{The MPJPE and PA MPJPE comparison on 3DPW. Each method is trained on different datasets.}
\label{table:compare_3dpw_different_dataset}
\end{minipage}
\end{table}

\noindent \textbf{Benefit of the cascaded PoseNet and MeshNet.}
To demonstrate the benefit of the cascaded PoseNet and MeshNet, we trained and tested three networks using various network cascading strategy.
First, we removed PoseNet from the I2L-MeshNet. 
The remaining MeshNet directly predicts lixel-based 1D heatmap of each mesh vertex from the input image.
Second, we trained I2L-MeshNet, which has cascaded PoseNet and MeshNet architecture.
Third, to check the upper bound accuracy with respect to the output of the PoseNet, we fed the groundtruth 3D human pose instead of the output of the PoseNet to the MeshNet in both training and testing stage.
Table~\ref{table:ablation_3dpose} shows utilizing the output of the PoseNet (the second row) achieves better accuracy compared with using only MeshNet (the first row) to estimate the human mesh.
Interestingly, passing the groundtruth 3D human pose to the MeshNet (the last row) significantly improves the performance compared with all the other settings.
This indicates that improving the 3D human pose estimation network can be one important way to improve 3D human mesh estimation accuracy.


\subsection{Comparison with state-of-the-art methods}


\begin{table}[t]
\centering
\setlength\tabcolsep{1.0pt}
\def\arraystretch{1.1}
\scalebox{1.0}{
\begin{tabular}{C{3.5cm}|C{1.8cm}C{1.8cm}C{1.5cm}C{1.5cm}C{1.5cm}}
\specialrule{.1em}{.05em}{.05em}
 methods &  PA MPVPE &  PA MPJPE &  F@5 mm &  F@15 mm & GT scale \\ \hline
 Hasson~et al.~\cite{hasson2019learning} & 13.2 & - & 0.436 & 0.908 & \cmark \\
Boukhayma~et al.~\cite{boukhayma20193d} & 13.0 & - & 0.435 & 0.898 & \cmark \\
FreiHAND~\cite{Freihand2019} & 10.7 & - & \scriptsize0.529 & 0.935 & \cmark \\
\textbf{I2L-MeshNet (Ours)} & \textbf{7.6} & \textbf{7.4} & \textbf{0.681} & \textbf{0.973} & \xmark \\
 \specialrule{.1em}{.05em}{.05em}
\end{tabular}
}
\caption{
The PA MPVPE, PA MPJPE, and F-scores comparison between state-of-the-art methods and the proposed I2L-MeshNet on FreiHAND.
The checkmark denotes a method use groundtruth information during inference time.
}
\label{table:compare_freihand}
\end{table}


\noindent \textbf{Human3.6M and 3DPW.}
We compare the MPJPE and PA MPJPE of our I2L-MeshNet with previous state-of-the-art 3D human body pose and mesh estimation methods on Human3.6M and 3DPW test set.
As each previous work trained their network on different training sets, we report the 3D errors in two ways.


First, we train all methods on Human3.6M and MSCOCO and report the errors in Table~\ref{table:compare_h36m_3dpw_same_dataset}.
The previous state-of-the-art methods~\cite{kanazawa2018end,kolotouros2019convolutional,kolotouros2019learning} are trained from their officially released codes.
The table shows that our I2L-MeshNet significantly outperforms previous methods by a large margin on both datasets.


Second, we report the 3D errors of previous methods from their papers and ours in Table~\ref{table:compare_h36m_different_dataset} and Table~\ref{table:compare_3dpw_different_dataset}. 
Each network of the previous method is trained on the different combinations of datasets, which include Human3.6M, MSCOCO, MPII~\cite{andriluka20142d}, LSP~\cite{johnson2010clustered}, LSP-Extended~\cite{johnson2011learning}, UP~\cite{lassner2017unite}, and MPI-INF-3DHP~\cite{mehta2017monocular}.
We used MuCo-3DHP for the additional training dataset for the evaluation on 3DPW dataset.
We also report the 3D errors from a additional SMPL parameter regression module following Kolotouros~et al.~\cite{kolotouros2019convolutional}.
The tables show that the performance gap between ours and the previous state-of-the-art method~\cite{kolotouros2019learning} is significantly reduced.


The reason for the reduced performance gap is that previous model-based state-of-the-art methods~\cite{kanazawa2018end,kolotouros2019learning} can get benefit from many in-the-wild 2D human pose datasets~\cite{lin2014microsoft,johnson2010clustered,johnson2011learning} by a 2D pose-based weak supervision.
As the human body or hand model assumes a prior distribution between the human model parameters (\textit{i.e.}, 3D joint rotations and identity vector) and 3D joint/mesh coordinates, the 2D pose-based weak supervision can provide gradients in depth axis, calculated from the prior distribution.
Although the weak supervision still suffers from the depth ambiguity, utilizing in-the-wild images can be highly beneficial because the images have diverse appearances compared with those of the lab-recorded 3D datasets~\cite{ionescu2014human3,mehta2017monocular,mehta2018single}.
On the other hand, model-free approaches, including the proposed I2L-MeshNet, do not assume any prior distribution, therefore hard to get benefit from the weak supervision.
Based on the two comparisons, we can draw two important conclusions.
\begin{itemize}
\item I2L-MeshNet achieve much higher accuracy than the model-based methods when trained on the same datasets that provide groundtruth 3D human poses and meshes.
\item The model-based approaches can achieve comparable or higher accuracy by utilizing additional in-the-wild 2D pose data without requiring the 3D supervisions.
\end{itemize}


We think that a larger number of accurately aligned in-the-wild image-3D mesh data can significantly boost the accuracy of I2L-MeshNet.
The iterative fitting~\cite{bogo2016keep,pavlakos2019expressive}, neural network~\cite{joo2020exemplar}, or their combination~\cite{kolotouros2019learning} can be used to obtain more data.
This can be an important future research direction, and we leave this as future work.


\noindent \textbf{FreiHAND.}
We compare MPVPE and F-scores of our I2L-MeshNet with previous state-of-the-art 3D human hand pose and mesh estimation methods~\cite{boukhayma20193d,hasson2019learning,Freihand2019}.
We trained Mask R-CNN~\cite{he2017mask} on FreiHAND train images to get the hand bounding box of test images.
Table~\ref{table:compare_freihand} shows that the proposed I2L-MeshNet significantly outperforms all previous works without groundtruth scale information during the inference time.
We additionally report MPJPE in the table.

 \section{Conclusion}

We propose a I2L-MeshNet, image-to-lixel prediction network for accurate 3D human pose and mesh estimation from a single RGB image.
We convert the output of the network to the lixel-based 1D heatmap, which preserves the spatial relationship in the input image and models uncertainty of the prediction.
Our lixel-based 1D heatmap requires much less GPU memory usage under the same heatmap resolution while producing better accuracy compared with a widely used voxel-based 3D heatmap.
Our I2L-MeshNet outperforms previous 3D human pose and mesh estimation methods on various 3D human pose and mesh datasets.
We hope our method can give useful insight to the following model-free 3D human pose and mesh estimation approaches.
 
\section*{Acknowledgments}
This work was supported by IITP grant funded by the Ministry of Science and ICT of Korea (No. 2017-0-01780), and Hyundai Motor Group through HMG-SNU AI Consortium fund (No. 5264-20190101).

\clearpage

\begin{center}
\textbf{\large Supplementary Material of \enquote{I2L-MeshNet: Image-to-Lixel Prediction Network for \\ Accurate 3D Human Pose and Mesh Estimation \\ from a Single RGB Image}}
\end{center}

In this supplementary material, we present more experimental results that could not be included in the main manuscript due to the lack of space.

\section{Various settings of I2L-MeshNet}
\subsection{When to marginalize 2D to 1D?}
We report how the MPJPE, PA MPJPE, and GPU memory usage change when the marginalization takes place on the ResNet output (\textit{i.e.},  or ), which is the input of the first upsampling module, instead of the output of the last upsampling module (\textit{i.e.},  or ) in Table~\ref{table:when_margi}.
For the convenience, we removed PoseNet from our I2L-MeshNet and changed MeshNet to take the input image.
The table shows that the early marginalization increases the errors while requiring less amount of GPU memory.
This is because the marginalized two 1D feature maps can be generated from multiple 2D feature map, which results in spatial ambiguity.
To reduce the effect of this spatial ambiguity, we designed our I2L-MeshNet to extract a sufficient amount of 2D information and then apply the marginalization at the last part of the network instead of applying it in the early stage.

When the marginalization is applied on the ResNet output , all 2D layers (\textit{i.e.}, deconvolutional layers and batch normalization layers) in the upsampling modules are converted to the 1D layers.
All models are trained on Human3.6M dataset.
The -axis heatmap prediction part is not changed.

\begin{table}
\centering
\setlength\tabcolsep{1.0pt}
\def\arraystretch{1.1}
\scalebox{1.0}{
\begin{tabular}{C{4.0cm}|C{2.0cm}C{2.0cm}C{2.0cm}}
\specialrule{.1em}{.05em}{.05em}
settings & MPJPE & PA MPJPE & GPU mem.\\ \hline
avg on  & 93.5 & 64.1 & \textbf{4.4 GB} \\
\textbf{avg on  (ours)} & \textbf{86.2} & \textbf{59.8} & 4.6 GB\\ \hline
 \specialrule{.1em}{.05em}{.05em}
\end{tabular}
}
\caption{The MPJPE, PA MPJPE, and GPU memory usage comparison between various marginalization settings on Human3.6M dataset.}
\vspace*{-7mm}
\label{table:when_margi}
\end{table}



\subsection{How to marginalize 2D to 1D?}
We report how the MPJPE and PA MPJPE change when different marginalization methods are used in Table~\ref{table:how_margi}.
For the convenience, we removed PoseNet from our I2L-MeshNet and changed MeshNet to take the input image.
The table shows that our average pooling achieves the lowest errors.
Compared with the max pooling that provides the gradients to one pixel position per one  or  position, our average pooling provides the gradients to all pixel positions, which is much richer ones.
We implemented the weighted sum by constructing a convolutional layer whose kernel size is  and  for - and -axis lixel-based 1D heatmap prediction, respectively, without padding.
The weighted sum provides lower error than that of the max pooling, however still worse than our average pooling.
We believe the large size of a kernel of the convolutional layer (\textit{i.e.},  and ) is hard to be optimized, which results in higher error than ours.
For all settings, models are trained on Human3.6M dataset, and the -axis heatmap prediction part is not changed.

\begin{table}
\centering
\setlength\tabcolsep{1.0pt}
\def\arraystretch{1.1}
\scalebox{1.0}{
\begin{tabular}{C{4.0cm}|C{2.0cm}C{2.0cm}}
\specialrule{.1em}{.05em}{.05em}
settings & MPJPE & PA MPJPE \\ \hline
max pooling & 93.5 & 64.1  \\
weighted sum & 89.4 & 61.4 \\
\textbf{avg pooling (ours)} & \textbf{86.2} & \textbf{59.8} \\ \hline
 \specialrule{.1em}{.05em}{.05em}
\end{tabular}
}
\caption{The MPJPE and PA MPJPE comparison between various marginalization settings on Human3.6M dataset.}
\vspace*{-7mm}
\label{table:how_margi}
\end{table}


\section{Comparison with previous 2.5D heatmap regression}
We compare the MPJPE and GPU memory usage between a model that predicts our lixel-based 1D heatmap and a model that predicts the 2.5D heatmap~\cite{iqbal2018hand} in Table~\ref{table:iqbal_lixel}.
The 2.5D heatmap~\cite{iqbal2018hand} consists of  heatmap and  heatmap, where  one is the pixel-based 2D heatmap and  one has the same spatial size with that of  heatmap and contains root joint-relative depth on the activated  position for all mesh vertices.
They predict the depth values on  heatmap, not the likelihood, thus cannot model uncertainty of the -axis prediction.
As the table shows, our lixel-based one achieves significantly lower error under the same resolution while requiring a much smaller amount of GPU memory.
We think that this is because the 2.5D heatmap of Iqbal~et al.~\cite{iqbal2018hand} cannot model uncertainty of the prediction in -axis, while ours can.
For all settings, models are trained on Human3.6M dataset, and we removed PoseNet and changed MeshNet to take an input image and predict the heatmap.


\begin{table}
\centering
\setlength\tabcolsep{1.0pt}
\def\arraystretch{1.1}
\scalebox{0.8}{
\begin{tabular}{C{3.5cm}C{2.5cm}C{3.0cm}|C{1.4cm}C{1.8cm}}
\specialrule{.1em}{.05em}{.05em}
settings & resolution & uncertainty in -axis & MPJPE & GPU mem. \\ \hline
2.5D heatmap~\cite{iqbal2018hand} & 8  8, 8  8 & \xmark & 107.4 & 3.6GB  \\
2.5D heatmap~\cite{iqbal2018hand} & 32  32, 32  32 & \xmark & 100.4 & 8.4GB  \\
lixel-based 1D heatmap & 8, 8, 8 & \cmark &  100.2 & 3.4GB  \\
lixel-based 1D heatmap & 32, 32, 32 & \cmark & 94.8 & 4.0GB  \\
lixel-based 1D heatmap & 64, 64, 64 & \cmark &  86.2 & 4.6GB  \\ \hline
 \specialrule{.1em}{.05em}{.05em}
\end{tabular}
}
\caption{The MPJPE and GPU memory usage comparison between various marginalization settings on Human3.6M dataset.}
\vspace*{-7mm}
\label{table:iqbal_lixel}
\end{table}

\section{Effect of each loss function}
We show the effectiveness of the MeshNet pose loss  in Table~\ref{table:meshnet_pose_loss}.
Although we supervise mesh vertices by the mesh vertex loss , additional  is helpful for human joint-aligned mesh prediction.
Both models are trained on Human3.6M dataset.


For visually pleasant mesh estimation, we use normal vector loss  and edge length loss .
We show the effectiveness of the two loss functions in Figure~\ref{fig:normal_edge_loss}.
As the figure shows, the two loss functions improves visual quality of output meshes.
We checked that  and  marginally affect the MPJPE and PA MPJPE.
For all settings, all models are trained on Human3.6M dataset and MSCOCO dataset.

\begin{table}
\centering
\setlength\tabcolsep{1.0pt}
\def\arraystretch{1.1}
\scalebox{1.0}{
\begin{tabular}{C{3.5cm}|C{2.0cm}C{2.0cm}}
\specialrule{.1em}{.05em}{.05em}
settings & MPJPE & PA MPJPE \\ \hline
wo.  & 84.5 & 58.5  \\
w.  & 81.8 & 58.0  \\ \hline
 \specialrule{.1em}{.05em}{.05em}
\end{tabular}
}
\caption{The MPJPE and PA MPJPE comparison between models trained with and without  on Human3.6M dataset.}
\vspace*{-7mm}
\label{table:meshnet_pose_loss}
\end{table}

\vspace*{-10mm}


\begin{figure}
\begin{center}
\includegraphics[width=1.0\linewidth]{fig/normal_edge_loss.pdf}
\end{center}
\vspace*{-7mm}
   \caption{
Estimated meshes from models trained with different combinations of loss functions.
   }
\vspace*{-8mm}
\label{fig:normal_edge_loss}
\end{figure}


\section{Accuracy of PoseNet}
We provide the MPJPE and PA MPJPE of PoseNet from I2L-MeshNet in Table~\ref{table:posenet_error}.
The PoseNet is trained with MeshNet by minimizing the loss function .
As our PoseNet predicts 3D joint coordinates of the SMPL body joint set or MANO hand joint set, we calculate the errors using groundtruth SMPL or MANO 3D joint coordinates.
We could not calculate the MPJPE on FreiHAND dataset because the official evaluation server does not support it.

\begin{table}
\centering
\setlength\tabcolsep{1.0pt}
\def\arraystretch{1.1}
\scalebox{0.8}{
\begin{tabular}{C{3.5cm}|C{2.0cm}C{2.0cm}}
\specialrule{.1em}{.05em}{.05em}
datasets & MPJPE & PA MPJPE \\ \hline
Human3.6M & 62.2 & 47.2  \\
3DPW & 112.2 & 72.3  \\ 
SURREAL & 40.0 & 29.5  \\
FreiHAND & n/a & 8.0 \\ \hline
 \specialrule{.1em}{.05em}{.05em}
\end{tabular}
}
\caption{The MPJPE and PA MPJPE of PoseNet on each dataset.}
\vspace*{-7mm}
\label{table:posenet_error}
\end{table}

\newpage

\section{Pseudo-groundtruth SMPL parameters of Human3.6M dataset}
All the previous works~\cite{kanazawa2018end,pavlakos2018learning,kolotouros2019convolutional,kolotouros2019learning} used SMPL parameters obtained by applying Mosh~\cite{loper2014mosh} on the marker data of Human3.6M dataset as the groundtruth parameters.
However, currently, the distribution of the SMPL parameters from Mosh is disallowed because of the license problem.
In addition, the source code of Mosh is not publicly released.
Alternatively, we obtain groundtruth SMPL parameters by applying SMPLify-X~\cite{pavlakos2019expressive} on the groundtruth 3D joint coordinates of Human3.6M dataset.
Although the obtained SMPL parameters are not perfectly aligned to the groundtruth 3D joint coordinates, we checked that the error of the SMPLify-X is much less than those of current state-of-the-art 3D human pose estimation methods, as shown in Table~\ref{table:smplify-x_error}. 
Therefore, we think using SMPL parameters from SMPLify-X as groundtruth is reasonable.
Note that for a fair comparison, all the experimental results of previous works are reported by training and testing them on our SMPL parameters from SMPLify-X.
When fitting, we used neutral gender SMPL body model.
However, we found that it produces gender-specific body shapes, although we did not specify gender for each subject.
As most of the subjects of the training set in Human3.6M dataset are female, we found that our I2L-MeshNet trained on Human3.6M dataset tends to produce female body shape meshes.
We tried to fix the identity code of the SMPL body model obtained from the T-pose; however it produces higher errors.
Thus, we did not fix the identity code for each subject.

\begin{table}
\centering
\setlength\tabcolsep{1.0pt}
\def\arraystretch{1.1}
\scalebox{1.0}{
\begin{tabular}{C{4.5cm}|C{2.0cm}C{2.0cm}}
\specialrule{.1em}{.05em}{.05em}
methods & MPJPE  \\ \hline
Moon~et al.~\cite{moon2019camera} & 53.3  \\ 
Sun~et al.~\cite{sun2018integral} & 49.6  \\
Iskakov~et al.~\cite{iskakov2019learnable}*  & 20.8 \\
SMPLify-X from GT 3D pose & \textbf{13.1}  \\ \hline
 \specialrule{.1em}{.05em}{.05em}
\end{tabular}
}
\caption{The MPJPE comparison between SMPLify-X fitting results and state-of-the-art 3D human pose estimation methods. \enquote{*} takes multi-view RGB images as inputs.}
\vspace*{-7mm}
\label{table:smplify-x_error}
\end{table}

\newpage

\begin{table}[t]
\centering
\setlength\tabcolsep{1.0pt}
\def\arraystretch{1.1}
\begin{tabular}{C{3.5cm}|C{1.3cm}C{1.2cm}}
\specialrule{.1em}{.05em}{.05em}
methods & MPVPE & MPJPE \\ \hline
SMPLify~\cite{bogo2016keep} & 75.3 & - \\
BodyNet~\cite{varol2018bodynet} & 65.8 & 40.8 \\
\textbf{I2L-MeshNet (Ours)} & \textbf{44.7} & \textbf{37.7} \\
 \specialrule{.1em}{.05em}{.05em}
\end{tabular}
\caption{The MPVPE and MPJPE comparison between state-of-the-art methods and the proposed I2L-MeshNet on SURREAL.}
\vspace*{-5mm}
\label{table:compare_surreal}
\end{table}

\section{Evaluation on SURREAL}
We additionally provide evaluation results on SURREAL~\cite{varol17_surreal} that contains 67K clips synthesized by animating SMPL body model. 
We followed the same training and test set split of BodyNet~\cite{varol2018bodynet}.
For evaluation, mean per-vertex position error (MPVPE), which is averaged per-vertex Euclidean distance error (mm) between predicted and groundtruth 3D mesh coordinates, and MPJPE are used after root joint alignment.
We compare MPVPE and MPJPE of our I2L-MeshNet with previous state-of-the-art 3D human body pose and mesh estimation methods~\cite{bogo2016keep,kanazawa2018end,varol2018bodynet} on the SURREAL test set.
To this end, we reduced the clips in the training set to 1 fps to make the training image set.
Table~\ref{table:compare_surreal} shows that the proposed I2L-MeshNet significantly outperforms all previous state-of-the-art methods.
Especially, it achieves much lower test error compared with BodyNet~\cite{varol2018bodynet}, model-free approach.


\section{Qualitative results}
We provide qualitative results comparison between ours and previous state-of-the-art model-free method (\textit{i.e.}, GraphCMR~\cite{kolotouros2019convolutional}) in Figure~\ref{fig:qualitative}.
As the figure shows, our I2L-MeshNet provides much more visually pleasant mesh results than GraphCMR.
We think this is because the graph convolutional network (GraphCNN) often tends to smooth the meshes by averaging the vertex feature with that of neighboring vertices.


\begin{figure}
\begin{center}
\includegraphics[width=1.0\linewidth]{fig/qualitative_comparison.pdf}
\end{center}
\vspace*{-7mm}
   \caption{
    Estimated meshes comparisons between our I2L-MeshNet and GraphCMR~\cite{kolotouros2019convolutional}.
   }
\vspace*{-8mm}
\label{fig:qualitative}
\end{figure}


\clearpage
 
\bibliographystyle{splncs04}
\bibliography{main}
\end{document}
