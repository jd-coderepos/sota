\section{Experiments}
We evaluate the effectiveness of our new scene graph benchmark through one intrinsic evaluation and two extrinsic evaluation tasks.
\subsection{Textual Scene Graph Parsing}
\label{sec:text_parser}
\paragraph{Task Setting.}
Following~\citet{schuster2015generating,wang2018scene,choi-etal-2022-scene}, we construct scene graph parsers to translate textual descriptions of image regions into scene graphs, which are then compared against their respective ground truth scene graphs.
\paragraph{Datasets.} In terms of datasets, our evaluations are conducted on the VG~\cite{krishna2017visual}, CDP~\cite{wang2018scene}, and FACTUAL dataset. The VG dataset comprises 108,077 images and 5.4 million region captions. The CDP dataset converts all scene graphs in VG into a customized dependency graph, which has a one-to-one mapping to the original scene graphs.

We report the performance of the parsers on two data splits for each dataset representation. For the FACTUAL dataset, we consider a random split (Random), which includes 37,861 training, 1,000 validation, and 1,508 test examples. Additionally, we also evaluate a more challenging split (Length) to assess the parsers' compositional generalization abilities. The benchmark test set for this split comprises 1,053 examples. The caption of each example includes more than ten caption tokens and three facts in the corresponding scene graphs. The remaining examples are split into 38,316 training and 1,000 validation examples. The test examples for VG and CDP consist of captions from the Random and Length splits of FACTUAL, while the remaining examples are divided into a validation set of 1,000 and a training set of over 2 million.

\paragraph{Baselines.} In this study, we evaluated the performance of five parsers: \textbf{SPICE-Parser}~\cite{anderson2016spice}, \textbf{AMR-SG-T5}~\cite{choi-etal-2022-scene}, \textbf{CDP-T5}~\cite{choi-etal-2022-scene}, \textbf{VG-T5}~\cite{sharifzadeh2022improving}, and \textbf{FACTUAL-T5}. SPICE utilizes a set of rules to convert dependency graphs of captions into scene graphs. AMR-SG-T5 converts captions into AMRs through the use of AMR-BART~\cite{bai-etal-2022-graph}, and subsequently converts the AMRs into CDP-SG format by using a T5~\cite{raffel2020exploring} model. CDP-T5 directly converts captions into CDP-SGs without the intermediate steps. In contrast to the original CDP-to-SG parser~\cite{wang2018scene}, which relies on intermediate representation, CDP-T5 demonstrates significantly better performance~\cite{choi-etal-2022-scene}. VG-T5, trained on the VG, parses captions into VG-SGs. FACTUAL-T5 parses captions into FACTUAL-SGs and maps them into scene graphs in a collective way. FACTUAL-T5 (pre) was first pre-trained on the VG dataset and then fine-tuned on FACTUAL. As different datasets use different annotations, SPICE\footnote{It is worth noting that SPICE-Parser utilizes a dependency parser trained on a general domain instead of on the CDP dataset. However, it is also based on a dependency parser, and thus we compare its output scene graphs with the ground truth CDP-SGs.}, AMR-SG-T5 and CDP-T5 are evaluated against the ground truth of the CDP dataset, while VG-T5 and FACTUAL-T5 are evaluated against the ground truth VG-SGs and FACTUAL-SGs.

\paragraph{Evaluation.} Following~\citet{schuster2015generating,wang2018scene,choi-etal-2022-scene}, we evaluate scene graph parsers utilizing the SPICE metric~\cite{anderson2016spice}. The SPICE F-score measures the similarity between the candidate and ground truth graph representations extracted from captions by the parsers. In addition, we also employ the Exact Set Match metric~\cite{yu2019sparc}, which assesses the accuracy of the parsers by determining whether the strings of the parsed facts match the ground truth facts while disregarding the order of the facts. During the evaluation, all intermediate representations are converted into scene graphs.

We also evaluate the faithfulness and consistency of parser outputs by human evaluation and automatic lexical diversity metrics, respectively. Specifically, three students manually examine the rates of correctness and completeness of the parsing outputs, and we report the average scores. We employ Yules I~\cite{yule2014statistical}, TTR~\cite{templin1957certain}, and MTLD~\cite{koehn2005europarl} to evaluate the lexical diversity of objects, attributes, and predicates, which indicate consistency of the output scene graphs.
\paragraph{Discussion.}
\begin{table}[t]
\centering
  \resizebox{\textwidth}{!}{\begin{tabular}{|c||cc|cc|}
    \toprule
     \multirow{2}{*}{Parser} &
      \multicolumn{2}{c|}{Random} &
      \multicolumn{2}{c|}{Length}\\
       & Set Match  & SPICE & Set Match & SPICE  \\
      \midrule 
\hline    
   SPICE-Parser & 13.00 & 56.15 & 0.94 & 38.04   \\
   AMR-SG-T5 & 28.45 & 64.82 & 12.16 & 51.71   \\
      CDP-T5 &  46.15 & 73.56 & 26.50 & 61.21  \\
     VG-T5 &  11.54 & 47.46 & 2.94 & 42.98  \\
     \hline    
    FACTUAL-T5 (pre) & \textbf{79.77} & \textbf{92.91} & \textbf{42.35} &  \textbf{82.43}  \\     
           FACTUAL-T5 & 79.44 & 92.23 & 38.65 &  80.76  \\     
    \bottomrule
  \end{tabular}}
    \caption{Intrinsic evaluation results of two metrics for various textual scene graph parsers across two test set splits.
     \vspace{-2mm} }
  \label{tab:parser}
    \vspace{-2mm}
\end{table} As shown in Table~\ref{tab:parser}, the FACTUAL-T5 and FACTUAL-T5 (pre) models demonstrate a clear superiority over other parsers regarding Set Match and SPICE scores. Notably, the FACTUAL-T5 model, which utilizes the T5 architecture, outperforms other T5-based baselines trained on millions of data points with different annotations. This highlights the effectiveness of the FACTUAL benchmark in generating outputs that are well-aligned with ground truth annotations. In the more challenging Length setting, all parsers experience a decline regarding parsing text into ground truth scene graphs. However, the FACTUAL-T5 model has the least drop among all parsers. Furthermore, pre-training the FACTUAL-T5 model on millions of VG data points only results in a slight improvement in the Length split. This indicates that a dataset as small as 40,000 high-quality examples is sufficient to yield a competent parser.

\begin{table}[t]
\centering
  \resizebox{\textwidth}{!}{\begin{tabular}{|c|cc|ccc|}
    \toprule
      & \multicolumn{2}{c|}{Faithfulness $\uparrow$} & \multicolumn{3}{c|}{Consistency $\downarrow$}\\
     & Completeness   & Correctness  &  Yules I & TTR & MTLD\\ \hline\midrule 
     SPICE-Parser & 49\% & 57\% & 1.56 & 10.26 &14.87\\
     AMR-SG-T5 & 31\% & 71\% & 2.85 & 15.45 &22.56\\
     CDP-T5 & 28\% & 86\% & 3.64 &16.57 &23.96\\
     VG-T5 & 51\% & 47\% & \textbf{0.37} &\textbf{5.27} &\textbf{10.59}\\
     \textbf{FACTUAL-T5 (pre)} & \textbf{92\%} &  \textbf{93\%} &2.76 & 13.55 & 15.30\\
    \bottomrule
  \end{tabular}}
    \caption{Evaluation of faithfulness and consistency across outputs from various scene graph parsers. }
  \label{tab:parser_faith_consistency}
    \vspace{-3mm}
\end{table} The SPICE-Parser has become the most frequently utilized parser in vision-language tasks. However, as shown in Table~\ref{tab:parser}, it is unable to align with the CDP-SG in either of the two settings. However, this does not necessarily imply that the SPICE-Parser is the worst among the parsers, as the oracle CDP-SGs have a high degree of noise as well, as demonstrated in Table~\ref{tab:oracle_statistics}. Our human evaluation of the faithfulness of the parsing results, as presented in Table~\ref{tab:parser_faith_consistency}, indicates that the SPICE-Parser can perform comparably with the VG-T5 model and outperform the CDP-T5 model in terms of completeness. Furthermore, our subsequent extrinsic evaluation also shows that the SPICE-Parser is the second-best parser among the parsers evaluated. Table~\ref{tab:parser_faith_consistency} also illustrates that our parser performs much better than the other baselines in terms of faithfulness while ranking second in terms of consistency. Interestingly, the VG-T5 model exhibits the best performance in consistency. However, its ORACLE annotations are more inconsistent than ours. Our analysis reveals that the VG-T5 prioritizes predicting scene graphs with simple lexicons and discards more complex patterns, resulting in its strong performance in consistency but much weaker performance in faithfulness metrics.

\subsection{Image Caption Evaluation}
\begin{table}[t]
\centering
  \resizebox{\textwidth}{!}{\begin{tabular}{|cc||cc|cc|}
    \toprule
    \multirow{2}{*}{Metric}& \multirow{2}{*}{Parser} &
      \multicolumn{2}{c|}{Flicker8K} &
      FOIL (1-ref)& FOIL (4-ref)\\
      & & $\tau_c\uparrow$  & $\rho\uparrow$ & $Acc\uparrow$  & $Acc\uparrow$  \\
      \midrule  \midrule
   \multirow{3}{*}{SPICE} & SPICE-Parser & 44.77 & 60.11 & 76.31 & \textbf{87.02}  \\
        & CDP-T5 & 33.50 & 49.50 & 65.66 & 72.76  \\
       & VG-T5 & 37.18 & 51.94 & 68.43 & 76.12 \\

    & \textbf{FACTUAL-T5(pre)}  & \textbf{45.12} & \textbf{60.78} & \textbf{76.69} & 86.88  \\
\hline    
 \multirow{3}{*}{SoftSPICE} &  SPICE-Parser & 51.897 & 68.118 & 78.53 & 86.77   \\
     & CDP-T5 & 45.54 & 59.64 &53.58 & 59.49   \\
    & VG-T5 & 39.66 & 53.05 &70.80 & 76.77   \\
   & \textbf{FACTUAL-T5(pre)} & \textbf{53.35} & \textbf{69.52} & \textbf{85.66} & \textbf{91.61}   \\     
    \bottomrule
  \end{tabular}}
    \caption{ (Left) The correlation scores between SPICE or SoftSPICE with the human judgment. (Right) The accuracies of the metrics w.r.t. detecting the hallucinated sentences.
     \vspace{-3mm} }
  \label{tab:img_eval}
    \vspace{-3mm}
\end{table}
 \paragraph{Task Setting.} To assess the quality of the model-generated captions regarding a set of reference captions and an image, we adopt the SPICE and SoftSPICE metrics to calculate a graph similarity between graphs extracted from the candidate and reference captions. As these metrics are based on the parser outputs, a \textit{better} parser will result in scores that more closely align with human judgment. 
\paragraph{Evaluation.} Following~\citet{hessel2021clipscore}, we employ two evaluation settings. The first setting involves calculating the correlation of the scores with human judgment utilizing Kendall's $\tau$ and Pearson correlation on the Flicker8K dataset~\cite{hodosh2013framing}. The Flicker8K dataset includes 17k "expert" human judgments for 5664 images, with each caption being rated on a scale of 1 to 4 against five reference captions. In the second setting, we utilize one (1-ref) or four (4-ref) reference captions sourced from the FOIL dataset~\cite{shekhar2017foil}. This dataset consists of 32k pairs of true captions and their corresponding corrupted versions, where a single word is replaced with an incorrect one. The objective is to assess the accuracy of each image caption evaluation metric in identifying and assigning higher scores to the uncorrupted captions. This setting aims to evaluate the metric's ability to detect instances of sentence hallucination effectively.


\paragraph{SoftSPICE.} SPICE calculates the similarity between two graphs by matching strings of sub-components within the graphs. These sub-components include \textit{objects}, tuples \textit{\{object, attribute\}} and triples \textit{\{object, predicate, object\}}. To improve SPICE, we propose an alternative method that utilizes embedding-based techniques to calculate string similarity. This approach involves decomposing each graph into the aforementioned sub-components and encoding the text of each component using the Sentence-BERT~\cite{reimers2019sentence}. The resulting similarity score, coined SoftSPICE, is as follows:

\begin{small}
\begin{align}
    \phi_s(G_c,G_r) = \frac{1}{|\mathcal{V}_c|}\sum_{\ve_c\in\mathcal{V}_c}\max_{\ve_r\in\mathcal{V}_{r}}(cos(\ve_c,\ve_r))
\end{align}
\end{small}
\noindent where $\ve$ denotes the embedding of each component, $\mathcal{V}_{r}$ and $\mathcal{V}_{c}$ denote the sets of embeddings encoding components within the candidate and reference graphs, respectively. Additionally, we can also use the image $I$ to compute a \textbf{SoftSPICE(img)} score, denoted as $\phi_i(G_c,I)$. This score is computed by combining the embeddings of the graph components and the image:

\vspace{-2mm}
\begin{small}
\begin{align}
    \phi'_i(G_c,I) &= \frac{1}{|\mathcal{V}_c|}\sum_{\ve_c\in\mathcal{V}_c}cos(\ve_c,\ve_I)\\
    \phi_i(G_c,I) &= \frac{2\cdot\phi_s(G_c,I)\cdot\phi'_i(G_c,I)}{\phi_s(G_c,G_r)+\phi'_i(G_c,G_r)}
\end{align}
\end{small}

\noindent
where $e_c$ and $e_I$ are obtained by encoding the sub-components and the images with CLIP.

\paragraph{Discussion.} Table~\ref{tab:img_eval} illustrates that FACTUAL-T5 demonstrates improvement over other parsers in terms of enhancing the correlation of SPICE and SoftSPICE scores with human judgments. However, when using SPICE to detect hallucinated instances, our parser performs comparably to the SPICE-Parser. We attribute this to the fact that approximately one-third of the pairs will have tied SPICE scores due to the use of exact string matching. On the other hand, when using the embedding-based metric, SoftSPICE, the superiority of our parser on FOIL is revealed. Currently, the SPICE utilizing the SPICE-Parser has been a common standard in image caption evaluation settings. We are confident that our parser can be a suitable replacement for SPICE-Parser. \begin{table}[t]
\centering
  \resizebox{\textwidth}{!}{\begin{tabular}{|c||cc|cc|}
    \toprule
    \multirow{2}{*}{Metric} &
      \multicolumn{2}{c|}{Flicker8K} &
      FOIL (1-ref)& FOIL (4-ref)\\
       & $\tau_c\uparrow$  & $\rho\uparrow$ & $Acc\uparrow$  & $Acc\uparrow$  \\
      \midrule  \midrule
 SoftSPICE & 53.35 & 69.52 & 85.66 & 91.61  \\
         SoftSPICE(img) & 54.85 & 70.55 & 88.12 & 92.31   \\
         \hline
         BERTScore & 36.71 & 49.81 & 86.70  & 90.49  \\
      BERTScore + SoftSPICE(img)& 51.08 & 65.80 & 90.50  &  \textbf{94.64} \\
          \hline
        CLIPScore & 51.44 & 64.86 & 86.85  & 86.85  \\
        RefCLIPScore & 53.00 & 67.67 & \textbf{90.94}  & 92.40  \\
                RefCLIPScore + SoftSPICE(img) & \textbf{57.37} & \textbf{73.40} & 90.69  &94.01\\

    \bottomrule
  \end{tabular}}
    \caption{ The results comparing SoftSPICE with current SOTA image caption evaluation metrics. We use FACTUAL-T5 as the parser for SoftSPICE. 
     \vspace{-2mm} }
  \label{tab:img_eval_sota}
    \vspace{-3mm}
\end{table}
 
We also compare SoftSPICE with current SOTA image evaluation metrics, namely BERTScore~\cite{zhang2019bertscore}, CLIPScore, and RefCLIPScore. These metrics calculate the similarity between the embeddings of the candidate caption with the embeddings of the reference captions, the image, and both reference captions and images, respectively. As in Table~\ref{tab:img_eval_sota}, SoftSPICE performs comparably with all the SOTA methods \textit{when there are over four reference captions}, and with the inclusion of image information, SoftSPICE(img) can even outperform SOTA results on Flicker8K. We also observed that the scene graph feature could be a useful supplement to caption-level features. By taking the harmonic mean of SoftSPICE(img) with BERTScore and RefCLIPScore, the performance of both metrics achieve new SOTA results.
\subsection{Zero-shot Image Retrieval}
\paragraph{Task Setting.} The goal of image retrieval is to identify and retrieve an image that precisely corresponds to a given textual query description. This is typically accomplished by allocating scores to images based on their relevance to the query and selecting the top $k$ images. 

Following the setting from~\citet{johnson2015image,wang2018scene}, we have selected 456 captions and their corresponding images from the Random and Length test sets, initially prepared for intrinsic evaluation. These captions serve as queries to retrieve their associated images, forming the basis for evaluating the performance of our image retrieval system. We proceed under the assumption that an oracle scene graph corresponding to each selected image is available. Furthermore, we introduce a '\textit{Local}' setting, which provides access to the coordinates of a bounding box within each image that corresponds to each caption and the ground truth scene graph aligned with this bounding box region. 

\paragraph{Evaluation.} During the evaluation, the scene graph of the captions is generated using various baseline parsing methods. The 456 images are ranked according to the similarity scores computed using either the SoftSPICE or CLIPScore between each image and the caption. Notably, the representation encoders employed in both similarity measurements are not fine-tuned on the in-domain dataset. The performance of various methods is assessed using the Recall@k metric. The performance of different methods is assessed using the Recall@k metric, which indicates the percentage of caption queries where the top $k$ retrieved images, given a specific query, include the ground truth.

\paragraph{Discussion.} 
As observed in Table~\ref{tab:img_retrieval}, FACTUAL-T5 consistently outperforms other baselines in zero-shot image retrieval tasks, highlighting the superiority of our dataset and parser. The performance of both SoftSPICE and CLIPScore is generally enhanced by incorporating location information of the bounding boxes, depicting that more accurate information could boost image retrieval. Moreover, when combined with all available parsers, SoftSPICE demonstrates significantly superior performance compared to CLIPScore, emphasizing the substantial potential benefits of utilizing structured information for image retrieval. \begin{table}[t]
\centering
  \resizebox{\textwidth}{!}{\begin{tabular}{|ccc||cc|cc|}
    \toprule
    & \multirow{2}{*}{Method} & \multirow{2}{*}{Parser} &
      \multicolumn{2}{c|}{Random} &
      \multicolumn{2}{c|}{Length}\\
      & & & R@1  & R@5 & R@1 & R@5  \\
      \midrule  \midrule
   \multirow{4}{*}{Local.} &\multirow{3}{*}{SoftSPICE} &  SPICE-Parser & 67.76 & 84.87 & 67.54 & 81.80  \\
   & &  CDP-T5 & 72.59 & 88.16 & 62.28 & 80.70  \\
   & & VG-T5 &49.56  & 68.86 & 58.77 & 74.34 \\

&    & FACTUAL-T5  & \textbf{79.39} & \textbf{92.32} & \textbf{75} & \textbf{87.06}  \\

    &\multirow{1}{*}{CLIPScore} & N/A  & 31.58 & 58.77 & 45.61 & 66.01  \\
        \hline
\hline    
 \multirow{4}{*}{No Local.} &\multirow{4}{*}{SoftSPICE} &   SPICE-Parser & 47.81 & 71.05  & 57.01 & 78.07   \\
 &&   CDP-T5 & 57.02 & 76.54 & 51.54 & 71.27   \\
    && VG-T5 & 38.38 & 58.11 &51.54 & 70.61   \\
   && FACTUAL-T5 & \textbf{66.45} & \textbf{83.99} & \textbf{68.42} & \textbf{85.53}   \\   
      
       &\multirow{1}{*}{CLIPScore} & N/A  & 23.02 & 47.37 & 34.65 & 55.26  \\
    \bottomrule
  \end{tabular}}
    \caption{ Zero-shot image retrieval evaluation on two sets of image-caption pairs that utilize localization or do not use localization information during image retrieval.
     \vspace{-3mm} }
  \label{tab:img_retrieval}
    \vspace{-3mm}
\end{table} 