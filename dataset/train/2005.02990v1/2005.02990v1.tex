\pdfoutput=1
\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2020}
\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}
\usepackage{graphicx}
\usepackage{url}
\usepackage{subcaption}
\usepackage{amsmath,bm,amsfonts,dsfont,bbm}
\usepackage{paralist}
\usepackage{hyperref}
\usepackage{multicol}
\usepackage{color}
\usepackage{booktabs}
\usepackage[inline]{enumitem}
\usepackage{xspace}
\usepackage{mdframed}

\usepackage[activate={true,nocompatibility}, kerning=true,spacing=true, stretch=30,shrink=30]{microtype}
\microtypecontext{spacing=nonfrench}
\aclfinalcopy 

\newcommand\BibTeX{B\textsc{ib}\TeX}
\newcommand{\actignore}{\textsc{ignore}\xspace}
\newcommand{\actcoref}{\textsc{coref}\xspace}
\newcommand{\actoverwrite}{\textsc{overwrite}\xspace}
\newcommand{\mysim}{\mathit{sim}}
\newcommand{\mlp}{\mathrm{MLP}}
\newcommand{\cs}{\mathit{cs}}

\newcommand{\bertbase}{BERT\textsubscript{BASE}\xspace}
\newcommand{\bertlarge}{BERT\textsubscript{LARGE}\xspace}
\newcommand{\modelname}{PeTra\xspace}
\newcommand\Mark[1]{\textsuperscript#1}
\newcommand{\hlent}[2]{\colorbox{gray!30}{#1\textsubscript{#2}}}

\definecolor{dkgreen}{RGB}{0,130,0}
\definecolor{aqua}{rgb}{0.0, 0.4, 1.0}

\def\vec#1{\ensuremath{\boldsymbol{{#1}}}}
\def\mat#1{\vec{#1}}


\title{PeTra: A Sparsely Supervised Memory Model for People Tracking}

\author{Shubham Toshniwal\Mark{1},
Allyson Ettinger\Mark{2},
Kevin Gimpel\Mark{1},
Karen Livescu\Mark{1}\\
\Mark{1}Toyota Technological Institute at Chicago\\
\Mark{2}Department of Linguistics, University of Chicago\ \label{ent_eqn}
e_t = \sigma(\mlp_1(\vec{h}_t))
 \label{sim_eqn}
\mysim_{t}^{i} = \mlp_2([\vec{h}_t; \vec{m}_{t-1}^{i};
                \vec{h}_t \odot \vec{m}_{t-1}^{i}; u_{t-1}^i])
 \label{coref_score_eqn}
\cs_{t}^{i} = \mysim_{t}^{i} - \infty \cdot \mathds{1} [u^i_{t-1} = 0]

\label{coref_over_eqn}
          \begin{pmatrix}
           c_t^1 \\
           \vdots \\
           c_t^N \\
           n_t
          \end{pmatrix} = e_t \cdot \text{ softmax}
          \begin{pmatrix}
           \cs_{t}^{1} \\
           \vdots \\
           \cs_{t}^{N} \\
           0
         \end{pmatrix}
\label{over_eqn_inf}
o_t^{i} = n_t \cdot \mathbbm{1}_{i = \arg\min_j u^j_{t-1}}
\label{over_eqn_train}
o_t^{i} = n_t \cdot \text{GS}\left(\frac{1 - u_{t-1}^{i}}{\tau}\right)_i

\begin{aligned}\label{memory_up}
    \vec{m}_t^{i}  = &\ \overbrace{(1 - (o_t^{i} + c_t^{i})) \vec{m}_{t-1}^{i}}^{\mbox{\actignore}}  \,+\!\!\!\!\!\! \overbrace{o_t^{i}\cdot \vec{h}_t}^{\mbox{\actoverwrite}} \
In this expression, the coreference term takes into account both the previous cell vector  and the current token representation , while the overwrite term is based only on .  In contrast to a similar
 memory update equation in the Referential Reader which employs a pair of GRUs and MLPs for each memory cell, our update parameter uses just  which is memory cell-agnostic. 

Finally, the memory usage scalar is
updated as

where  is the decay rate for the usage scalar.
Thus the usage scalar  keeps decaying with time unless the memory is updated via \actoverwrite or \actcoref in which case the value is increased to reflect the memory cell's recent use.
 



\paragraph{Memory Variants}
In vanilla \modelname, each memory cell is represented as a single vector and the memory is parameter-free,
so the total number of model parameters is independent of memory size.  This is a property that is shared with, for example, differentiable neural computers \citep{graves2016hybrid}.
On the other hand, recent models for entity tracking, such as the EntNet~\cite{henaff2016tracking} and the Referential Reader~\cite{liu2019referential}, learn memory initialization parameters and separate the memory cell into key-value pairs.
To compare these memory cell architectures, we investigate the following two variants of \modelname:
\begin{enumerate}
    \item \emph{\modelname + Learned Initialization}: memory cells are initialized at  to learned parameter vectors.
    \item \emph{\modelname + Fixed Key}: a fixed dimensions of each memory cell are initialized with  learned parameters and kept fixed throughout the document read, as in EntNet~\cite{henaff2016tracking}.
\end{enumerate}
Apart from initialization, the initial cell vectors are also used to break ties for overwrites in Eqs.~\eqref{over_eqn_inf} and \eqref{over_eqn_train} when deciding among unused cells (with ). The criterion for breaking the tie is the similarity score computed using Eq.~\eqref{sim_eqn}.

\subsection{Coreference Link Probability}
\label{sec:coref_link_prob}
The probability that the tokens  and  are coreferential according to, say, cell  of the memory depends on three things:
\begin{enumerate*}[label=(\alph*)]
    \item  is identified as part of an entity mention and is either overwritten to cell  or is part of an earlier coreference chain for an entity tracked by cell ,
    \item Cell  is not overwritten by any other entity mention from  to , and
    \item  is also predicted to be part of an entity mention and  is coreferential with cell .
\end{enumerate*}
Combining these factors and marginalizing over the cell index results in the
following expression for the {\bf coreference link probability}: 



\subsection{Losses}
The GAP~\cite{webster2018gap} training dataset is small and provides sparse supervision with labels for only two coreference links per instance.
In order to compensate for this lack of supervision, we use a heuristic loss  over entity mention probabilities  in combination with the end task loss  for coreference. The two losses are combined with a tunable hyperparameter  resulting in the following total loss: . 

\subsubsection{Coreference Loss}
\label{sec:coref_loss}
The coreference loss is the binary cross entropy between the ground truth labels for mention pairs and the coreference link probability  in 
Eq.~\eqref{prob_eqn}. 
Eq.~\eqref{prob_eqn} expects 
a pair of tokens while the annotations are on pairs of spans, so we compute the loss for all ground truth token pairs: 

where  is the set of annotated span pairs and  represents the cross entropy of the distribution  relative to distribution .

 
Apart from the ground truth labels, we use 
``implied labels" in the coreference loss calculation.
 For handling multi-token spans, we assume that all tokens following the head token are coreferential with the head token (self-links). 
 We infer more supervision based on knowledge of the setup of the GAP task. Each GAP instance has two candidate names and a pronoun mention with supervision provided for the \{name, pronoun\} pairs. By design the two names are different, and therefore we use them as a negative coreference pair.
 
 Even after the addition of this implied supervision, 
 our coreference loss calculation is restricted to the three mention spans in each training instance; therefore, the running time is  for finite-sized mention spans. 
In contrast, \citet{liu2019referential} compute the above coreference loss for all token pairs (assuming a negative label for all pairs outside of the mentions), which results in a runtime of  due to the    pairs and  computation per pair, and thus will scale poorly to long documents.

\subsubsection{Entity Mention Loss}
\label{sec:ent_pred_loss}
We use the inductive bias that most tokens do not correspond to entities by imposing a loss on the average of the entity mention probabilities predicted across time steps, after masking out the labeled entity spans. 
For a training instance where spans  and  correspond to the person mentions and span  is a pronoun, the entity mention loss is \vspace{-0.05in}
 
where  if  and  otherwise. 

Each GAP instance has only 3 labeled entity mention spans, but the text typically has other entity mentions that are not labeled. 
Unlabeled entity mentions will be 
inhibited by this loss. However, on average there are far more tokens outside entity spans than inside the spans. 
In experiments without this loss, we observed that the model is susceptible to 
predicting a high entity probability for all tokens while still performing well on the end task of pronoun resolution. We are interested in tracking people beyond just the entities that are labeled in the GAP task, 
for which this loss is very helpful. 


 
 \section{Experimental Setup}
\subsection{Data}
GAP is a gender-balanced pronoun resolution dataset
introduced by \citet{webster2018gap}.
Each instance consists of a small snippet of text from Wikipedia, two spans corresponding to candidate names along with a pronoun span, and two binary labels indicating the coreference relationship between the pronoun and the two candidate names. Relative to other popular coreference datasets~\cite{pradhan2012conll, chen-etal-2018-preco},
GAP is comparatively small and sparsely annotated. We choose GAP because its small size allows us to do extensive experiments.






\subsection{Model Details}
For the input BERT embeddings, we concatenate either the last four layers of \bertbase, or layers 19--22 of \bertlarge since those layers have been found to carry the most information related to coreference~\cite{liu2019linguistic}. The BERT embeddings are fed to a 300-dimensional GRU model, which matches the dimensionality of the memory vectors.

We vary the number of memory cells 
from 2 to 20. The decay rate for the memory usage scalar  is 0.98. The MLPs used for predicting the entity probability and similarity score consist of two 300-dimensional ReLU hidden layers.
For the \emph{Fixed Key} variant of \modelname we use 20 dimensions for the learned key vector and the remaining 280 dimensions as the value vector.


\begin{figure*}[ht]
\centering
\begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/base/ref_reader_comp_orig_val.jpg}
        \caption{\bertbase}
        \label{fig:gap_val_small}
    \end{subfigure}~
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/large/ref_reader_comp_orig_val.jpg}
        \caption{\bertlarge}
        \label{fig:gap_val_large}
    \end{subfigure}
    \caption{Mean F1 score on the GAP validation set as a function of the number of memory cells.}
    \label{fig:gap_val}
\end{figure*}


\subsection{Training}
All models are trained for a maximum of 100 epochs with the Adam optimizer~\cite{Kingma2015AdamAM}.
The learning rate is initialized to 
and is reduced by half, until a minimum of ,
whenever there is no improvement on the validation performance for the last 5 epochs.
Training stops when there is no improvement in validation performance for the last 15 epochs.
The temperature  of the Gumbel-Softmax distribution used in the \actoverwrite operation is initialized to  and halved every 10 epochs.
The coreference loss terms in Section~\ref{sec:coref_loss} are weighted differently for different coreference links: \begin{enumerate*}[label=(\alph*)]
    \item self-link losses for multi-token spans are given a weight of 1,
    \item positive coreference link losses are weighted by 5, and
    \item negative coreference link losses are multiplied by 50.
\end{enumerate*}
To prevent overfitting: \begin{enumerate*}[label=(\alph*)]
    \item we use early stopping based on validation performance, and
    \item apply dropout at a rate of 0.5 on the output of the GRU model.
\end{enumerate*}
Finally, we choose  to weight the entity prediction loss described in Section~\ref{sec:ent_pred_loss}.


\subsection{People Tracking Evaluation}
\label{sec:exp_people_tracking_eval}
One of the goals of this work is to develop memory models that not only do well on the coreference resolution task, but also are interpretable in the sense that the memory cells actually track entities. Hence in addition to reporting the standard metrics on GAP, we consider two other ways to evaluate memory models. 

  As our first task, we propose an auxiliary entity-counting task. We take 100 examples from the GAP validation set and annotate them
with the number of unique people mentioned in them.\footnote{In the GAP dataset, the only relevant entities are people.}
We test the models by predicting the number of people from their memory logs as explained in Section~\ref{sec:inference}.
The motivation behind this exercise is that if a memory model is truly tracking entities, then its memory usage logs should allow us to recover  this information.

To assess the people tracking performance more holistically, we conduct a human evaluation in which we ask annotators to assess the memory models on people tracking performance, defined as:(a) detecting references to people including pronouns, and (b) maintaining a 1-to-1 correspondence between people and memory cells.
For this study, we pick the best run (among 5 runs) of \modelname and the Referential Reader for the 8-cell configuration using \bertbase (\modelname: 81 F1; Referential Reader: 79 F1).
Next we randomly pick 50 documents (without replacement) from the GAP dev set and split those into groups of 10 to get 5 evaluation sets.
We shuffle the original 50 documents and follow the same steps to get another 5 evaluation sets.
In the end, we have a total of 10 evaluation sets with 10 documents each, where each unique document belongs to exactly 2 evaluation sets.

We recruit 10 annotators for the 10 evaluation sets.
The annotators are shown memory log visualizations as in Figure~\ref{fig:visualize}, and instructed to compare the models on their people tracking performance (detailed instructions in Appendix~\ref{sec:app_hum_eval}). For each document the annotators are presented memory logs of the two models (ordered randomly) and asked whether they prefer the first model, prefer the second model, or have no preference
(neutral). 


\subsection{Inference}
\label{sec:inference}
\paragraph{GAP} Given a pronoun span  and two candidate name spans  \&  , we
have to predict binary labels for potential coreference links between (, ) and (, ). Thus, for a pair of entity spans, say  and , we predict the coreference link probability as:

where  is calculated using the procedure described in Section~\ref{sec:coref_link_prob}\footnote{The computation of this probability includes the mention detection steps required by\citet{webster2018gap}.}. The final binary prediction is made by comparing the probability against a threshold.


\begin{figure*}[t]
\begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/base/err_in_people_count.jpg}
        \caption{\bertbase}
        \label{fig:count_uniq_people_small}
    \end{subfigure}~
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/large/err_in_people_count.jpg}
        \caption{\bertlarge}
        \label{fig:count_uniq_people_large}
    \end{subfigure}
    \caption{Error in counting unique people as a function of number of memory cells; lower is better. }
    \label{fig:count_uniq_people}
    \vspace{-0.1in}
\end{figure*}


\paragraph{Counting unique people}
For the test of unique
people counting, we discretize the overwrite operation, which corresponds to new entities, against a threshold  and sum over all
tokens and all
memory cells to predict the count as follows:\vspace{-0.02in}





\subsection{Evaluation Metrics}
For GAP we evaluate models using F-score.\footnote{GAP also includes evaluation related to gender bias, but this is not a focus of this paper so we do not report it.}
First, we pick a threshold from the set \{0.01, 0.02, , 1.00\} which maximizes the validation F-score.
This threshold is then used to evaluate performance on the GAP test set.





For the interpretability task of counting unique people, we
choose a threshold that minimizes the absolute difference between ground truth count and predicted count summed over the 100 annotated examples.
We select the best threshold from the set \{0.01, 0.02, , 1.00\}.
The metric is then the number of errors corresponding to the best threshold.\footnote{Note that the error we report is therefore a best-case result.  We are not proposing a way of counting unique people in new test data, but rather using this task for analysis.}

\subsection{Baselines}
The Referential Reader~\cite{liu2019referential} is the most relevant baseline in the literature, and the most similar to \modelname.
The numbers reported by~\citet{liu2019referential} are obtained by a version of the model using \bertbase, with only two memory cells.
To compare against \modelname for other configurations, we retrain the Referential Reader using the code made available by the authors.\footnote{\url{https://github.com/liufly/refreader}}

We also report the results of \citet{joshi-etal-2019-bert} and \citet{wu2019coreference}, although these numbers are not comparable since both of them train on the much larger OntoNotes corpus and just test on GAP.
 \section{Results}

\begin{figure*}[!ht]
    \centering

    \begin{subfigure}[t]{\textwidth}
    \begin{mdframed}
        \footnotesize{
        \hlent{Amelia Shepherd}{1}, M.D. is a fictional character on the ABC American television medical drama Private Practice, and the spinoff series' progenitor show, Grey's Anatomy, portrayed by \hlent{\textcolor{red}{\it Caterina Scorsone}}{2}. In \hlent{\textcolor{aqua}{\bf her}}{1} debut appearance in season three, \hlent{\textcolor{aqua}{\bf Amelia}}{1} visited her former sister-in-law, \hlent{Addison Montgomery}{3}, and became a partner at the Oceanside Wellness Group.
        }
    \end{mdframed}
    \end{subfigure}
    \begin{subfigure}[t]{\textwidth}
    \includegraphics[width=\textwidth]{figures/model_runs/petra_positive.pdf}
    \caption{A successful run of \modelname with 4 memory cells. The model accurately links all the mentions of ``Amelia" to the same memory cell while also detecting other people in the discourse.}
    \label{fig:success}
    \end{subfigure}
    \begin{subfigure}[t]{\textwidth}
    \begin{mdframed}
        \footnotesize{
        \hlent{Bethenny}{1} calls a meeting to get everyone on the same page, but \hlent{Jason}{2} is hostile with the group, making things worse and forcing \hlent{Bethenny}{1} to play referee. Emotions are running high with \hlent{\textcolor{red}{\it Bethenny}}{1}'s assistant, \hlent{\textcolor{aqua}{\bf Julie}}{3}, who breaks down at a lunch meeting when asked if \hlent{\textcolor{aqua}{\bf she}}{3} is committed to the company for the long haul.
        }
    \end{mdframed}
    \end{subfigure}
    \begin{subfigure}[t]{\textwidth}
    \includegraphics[width=\textwidth]{figures/model_runs/petra_negative_1.pdf}
    \caption{Memory log of \modelname with 8 memory cells. The model correctly links ``she" and ``Julie" but fails at linking the three ``Bethenny" mentions, and also fails at detecting ``Jason".}
    \label{fig:failure}
    \end{subfigure}
    \caption{Visualization of memory logs for different configurations of \modelname.
    The documents have their GAP annotations highlighted in red (italics) and blue (bold), with blue (bold) corresponding to the right answer. For illustration purposes only, we highlight all the spans corresponding to mentions of people and mark cluster indices as subscript.
    In the plot, X-axis corresponds to document tokens, and Y-axis corresponds to memory cells. Each memory cell has the OW=\actoverwrite and CR=\actcoref labels. Darker color implies higher value.
    We skip text, indicated via ellipsis, when the model doesn't detect people for extended lengths of text.}
    \label{fig:visualize}
\end{figure*}


\subsection{GAP results}
\begin{table}[h]
\centering{
\small{
\begin{tabular}{lll}
\toprule
 & \bertbase  & \bertlarge \\\midrule
\modelname                   & {\bf 81.5  0.6} &  {\bf 85.3  0.6} \\
\hspace{0.1in} + Learned Init. & 80.9  0.7 &  84.4  1.2\\
\hspace{0.1in} + Fixed Key & 81.1  0.7 & 85.1  0.8\\
Ref. Reader & 78.9  1.3 & 83.7  0.8\\
Ref. Reader ~\shortcite{liu2019referential} & 78.8 & - \\\midrule
\citet{joshi-etal-2019-bert} & 82.8 & 85.0 \\
\citet{wu2019coreference} & - & 87.5 (SpanBERT)\\
\bottomrule
\end{tabular}
}
}
\caption{Results (\%F1) on the GAP test set.}
\label{tab:gap_res}
\end{table}




We train all the memory models, including the Referential Reader, with memory size varying from \{2, 4, , 20\} memory cells for both \bertbase and \bertlarge, with each configuration being trained 5 times.
Figure~\ref{fig:gap_val} shows the performance of the
models on the GAP validation set as a function of memory size.
The Referential Reader outperforms
\modelname (and its memory variants) when using a small number of memory cells, but its performance starts degrading after 4 and 6 memory cells for \bertbase and \bertlarge respectively. \modelname and its memory variants, in contrast, keep improving with increased memory size (before saturation at a higher number of cells) and outperform the best Referential Reader performance for all memory sizes  6 cells. With larger numbers of memory cells, we see a higher variance, but the curves for \modelname and its memory variants are still consistently higher than those of the Referential Reader.

Among different memory variants of \modelname  , when using \bertbase the performances are comparable with no clear advantage for any particular choice. For \bertlarge, however, vanilla \modelname has a clear edge for almost all memory sizes, suggesting the limited utility of initialization.
The results show that \modelname works well without learning vectors for initializing the key or memory cell contents. Rather, we can remove the key/value distinction and simply initialize all memory cells with the zero vector.


To evaluate on the GAP test set, we pick the memory size corresponding to the best validation performance for all memory models. Table~\ref{tab:gap_res} shows that the trends from validation hold true for test as well, with \modelname outperforming the Referential Reader and the other memory variants of \modelname.








\subsection{Counting unique people}
Figure~\ref{fig:count_uniq_people} shows the results for the proposed interpretability task of counting unique people.
For both \bertbase and \bertlarge, \modelname achieves the lowest error count.
Interestingly, from Figure~\ref{fig:count_uniq_people_large} we can see that for  memory cells, the other memory variants of \modelname perform worse than the Referential Reader while being better at the GAP validation task (see Figure~\ref{fig:gap_val_large}). This shows that a better performing model is not necessarily better at tracking people.



\begin{table}[h]
\setlength{\tabcolsep}{5pt}
\small
\centering{
    \begin{tabular}{lcc}
    \toprule
         & \bertbase & \bertlarge \\
        \toprule
        \modelname & \textbf{0.76} & \textbf{0.69} \\
        \hspace{0.1in} + Learned Init & 0.72 & 0.60 \\
        \hspace{0.1in} + Fixed Key & 0.72 & 0.65 \\
        Ref. Reader & 0.49 &  0.54 \\
        \bottomrule
    \end{tabular}
}
\caption{Spearman's correlation between GAP validation F1 and negative error count for unique people.} \label{tab:correlation}
\vspace{-0.1in}
\end{table}
To test the relationship between the GAP task and the proposed interpretability task, we compute the correlation between the GAP F-score and the negative count of unique people for each model separately.\footnote{Each correlation is computed over 50 runs (5 runs each for 10 memory sizes).}
Table \ref{tab:correlation} shows the Spearman's correlation between these measures. For all models we see a positive correlation, indicating that a dip in coreference performance corresponds to an increase in error on counting unique people. The correlations for \modelname are especially high, again suggesting it's greater interpretability.
 







\subsection{Human Evaluation for People Tracking}
\label{sec:hum_eval}

\begin{table}[h]
\centering{
\small{
\begin{tabular}{lc}
\toprule
Model & Preference (in \%) \\
\midrule
\modelname        & \textbf{74} \\
Ref. Reader &  08\\
Neutral & 18 \\\bottomrule
\end{tabular}
}
}
\caption{Human Evaluation results for people tracking.}
\label{tab:hum_eval}
\vspace{-0.1in}
\end{table}


Table~\ref{tab:hum_eval} summarizes the results of the human evaluation for people tracking. The annotators prefer \modelname in 74\% cases while the Referential Reader for only 8\% instances (see Appendix~\ref{sec:app_visualizations} for visualizations comparing the two). Thus, \modelname easily outperforms the Referential Reader on this task even though they are quite close on the GAP evaluation.
The annotators agree on 68\% of the documents, disagree between \modelname and Neutral for 24\% of the documents, and disagree between \modelname and the Referential Reader for  the remaining 8\%    documents. For more details, see Appendix~\ref{sec:app_hum_eval_res}.

\subsection{Model Runs}
We visualize two runs of \modelname with different configurations in Figure~\ref{fig:visualize}. For both instances the model gets the right pronoun resolution, but clearly in Figure~\ref{fig:failure} the model fails at correctly tracking repeated mentions of ``Bethenny". We believe these errors happen because (a) GAP supervision is limited to pronoun-proper name pairs, so the model is never explicitly supervised to link proper names, and (b) there is a lack of span-level features, which hurts the model when a name is split across multiple tokens.  \section{Related Work}

There are several strands of related work, including prior work in developing neural models with external memory as well as variants that focus on modeling entities and entity relations, and neural models for coreference resolution.

\paragraph{Memory-augmented models.}

Neural network architectures with external memory include memory networks~\citep{weston-15,sukhbaatar-15}, neural Turing machines~\citep{graves2014neural}, and differentiable neural computers~\citep{graves2016hybrid}. This paper focuses on models with inductive biases that produce particular structures in the memory, specifically those related to entities.

\paragraph{Models for tracking and relating entities.}
A number of existing models have targeted entity tracking and coreference links for a variety of tasks.
EntNet~\citep{henaff2016tracking} aims to track entities via a memory model.
EntityNLM~\citep{D17-1195} represents entities dynamically within a neural language model. \citet{hoang-etal-2018-entity} augment a reading comprehension model to track entities, incorporating a set of auxiliary losses to encourage capturing of reference relations in the text. \citet{dhingra-etal-2018-neural} introduce a modified GRU layer designed to aggregate information across coreferent mentions.




\paragraph{Memory models for NLP tasks.}
Memory models have been applied to several other NLP tasks in addition to coreference resolution, including targeted aspect-based sentiment analysis~\citep{liu-etal-2018-recurrent}, machine translation~\citep{maruf-haffari-2018-document}, narrative modeling~\citep{liu-etal-2018-narrative}, and dialog state tracking~\citep{perez-liu-2017-dialog}. Our study of architectural choices for memory may also be relevant to models for these tasks.

\paragraph{Neural models for coreference resolution.}
Several neural models have been developed for coreference resolution, most of them focused on modeling pairwise interactions among mentions or spans in a document~\citep{wiseman-etal-2015-learning,clark-manning-2016-deep,lee-etal-2017-end,lee-etal-2018-higher}. These models use heuristics to avoid computing scores for all possible span pairs in a document, an operation which is quadratic in the document length  assuming a maximum span length.
Memory models for coreference resolution, including our model, differ by seeking to store information about entities in memory cells and then modeling the relationship between a token and a memory cell. This reduces computation from  to , where  is the number of memory cells, allowing memory models to be applied to longer texts by using the global entity information. Past work ~\citep{wiseman-etal-2016-learning} have used global features, but in conjunction with other features to score span pairs.


\paragraph{Referential Reader.} Most closely related to the present work is the Referential Reader~\citep{liu2019referential}, which uses a memory model to perform coreference resolution incrementally. We significantly simplify this model to accomplish the same goal with far fewer parameters.
 \section{Conclusion and Future Work}
We propose a new memory model for entity tracking, which is trained using sparse coreference resolution supervision.
The proposed model outperforms a previous approach with far fewer parameters and a simpler architecture.
We propose a new diagnostic evaluation and conduct a human evaluation to test the interpretability of the model, and find that
our model again does better on this evaluation.
In future work, we plan to extend this work to longer documents such as the recently released dataset of~\citet{bamman2019annotated}.
 
\section*{Acknowledgments}
{\footnotesize
This material is based upon work supported by the National Science Foundation under Award Nos.~1941178 and 1941160.
We thank the ACL reviewers, Sam Wiseman, and Mrinmaya Sachan for their valuable feedback.
We thank Fei Liu and Jacob Eisenstein for answering questions regarding the Referential Reader.
Finally, we want to thank all the annotators at TTIC who participated in the human evaluation study.}

\bibliography{acl2020}
\bibliographystyle{acl_natbib}
\appendix
\section{Appendix}

\subsection{Best Runs vs.~Worst Runs}
As Table~\ref{tab:gap_res} shows, there is significant variance in the performance of these memory models.
To analyze how the best runs diverge from the worst runs, we analyze how the controller network is using the different memory cells in terms of overwrites.
For this analysis, we choose the best and worst among the 5 runs for each configuration, as determined by GAP validation performance. For the selected runs, we calculate the KL-divergence of the average overwrite probability distribution from the uniform distribution and average it for each model type. Table~\ref{tab:best_vs_worst} shows that for the memory variants {\it Learned Init} and {\it Fixed Key}, the worst runs overwrite more to some memory cells than others (high average KL-divergence). Note that both \modelname and Referential Reader are by design intended to have no preference for any particular memory cell (which the numbers support), hence the low KL-divergence.
\begin{table}[h]
\small
\centering{
\begin{tabular}{lll}
\toprule
 & \multicolumn{2}{c}{Avg KL-div} \\
 & Best run & Worst run \\\midrule
\modelname & 0.00 & 0.01 \\
\hspace{0.1in} + Learned Init. & 0.3 & {\bf 0.83} \\
\hspace{0.1in} + Fixed Key & 0.2 & {\bf 0.8} \\
Ref. Reader & 0.05 & 0.04 \\
\bottomrule
\end{tabular}
}
\caption{A comparison of best runs vs.~worst runs. }
\label{tab:best_vs_worst}
\vspace{-0.1in}
\end{table}

\subsection{Human Evaluation Results}
\label{sec:app_hum_eval_res}
The agreement matrix for the human evaluation study described in Section~\ref{sec:hum_eval} is shown in Figure~\ref{fig:agreement}.
This agreement matrix is a result of the two annotations per document that we get as per the setup described in Section~\ref{sec:exp_people_tracking_eval}.
Note that the annotations are coming from two sets of annotators rather than two individual annotators. This is also the reason why we don't report standard inter-annotator agreement coefficients.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.45\textwidth]{figures/resp_agreement.pdf}
    \caption{Agreement matrix for human evaluation study.}
    \label{fig:agreement}
\end{figure}




\subsection{Instructions for Human Evaluation}
The detailed instructions for the human evaluation study described in Section~\ref{sec:hum_eval} are shown in Figure~\ref{fig:instructions}.
We simplified certain memory model specific terms such as ``overwrite" to ``new person" since the study was really about people tracking.
\label{sec:app_hum_eval}
\begin{figure*}[ht]
\begin{mdframed}\begin{itemize}
    \item In this user study we will be comparing memory models at tracking people.
    \item What are memory models? Memory models are neural networks coupled with an external memory which can be used for reading/writing.
    \item \textcolor{red}{(IMPORTANT)} What does it mean to track people for memory models?
    \begin{itemize}
        \item Detect all references to people which includes pronouns.
        \item A 1-to-1 correspondence between people and memory cells i.e. all references corresponding to a person should be associated with the same memory cell AND each memory cell should be associated with at most 1 person.
    \end{itemize}

    \item The memory models use the following scores (which are visualized) to indicate the tracking decisions:
    \begin{itemize}
    \item New Person Probability (Cell ): Probability that the token refers to a new person (not introduced in the text till now) and we start tracking it in cell .
    \item Coreference Probability (Cell ): Probability that the token refers to a person already being tracked in cell .
    \end{itemize}
    \item The objective of this study is to compare the models on the interpretability of their memory logs i.e.\ are the models actually tracking entities or not. You can choose how you weigh the different requirements for tracking people (from 3).
    \item For this study, you will compare two memory models with 8 memory cells (represented via 8 rows). The models are ordered randomly for each instance.
    \item For each document, you can choose model A or model B, or stay neutral in case both the models perform similarly.
    \end{itemize}
\end{mdframed}
\caption{Instructions for the human evaluation study.}
\label{fig:instructions}
\end{figure*}



\subsection{Comparative visualization of memory logs of \modelname and the Referential Reader}
\label{sec:app_visualizations}
Figure~\ref{fig:petra_vs_ref_1} and ~\ref{fig:petra_vs_ref_2} compare the memory logs of \modelname and the Referential Reader.

\begin{figure*}[h]
    \begin{subfigure}[t]{\textwidth}
    \begin{mdframed}
        \hlent{\textcolor{aqua}{\bf Neef}}{1} took an individual silver medal at the 1994 European Cup behind Russia's \hlent{\textcolor{red}{\it Svetlana Goncharenko}}{2} and returned the following year to win gold. \hlent{\textcolor{aqua}{\bf She}}{1} was a finalist individually at the 1994 European Championships and came sixth for Scotland at the 1994 Commonwealth Games.
    \end{mdframed}
    \caption{GAP validation instance 293. The ground truth GAP annotation is indicated via colors.}
    \end{subfigure}
    ~\vspace{0.2in}
    \centering
    \begin{subfigure}[t]{\textwidth}
    \includegraphics[width=\textwidth]{figures/model_runs/petra_positive_1.pdf}
    \caption{Memory log of \modelname with 8 memory cells. \modelname uses only 2 memory cells for the 2 unique people, namely Neef and Svetlana Goncharenko, and correctly resolves the pronoun.}
    \end{subfigure}
    \begin{subfigure}[t]{\textwidth}
    \includegraphics[width=\textwidth]{figures/model_runs/ref_0.pdf}
    \caption{Memory log of the Referential Reader with 8-memory cells. The Referential Reader does successfully resolve the pronoun in the topmost memory cell but it ends up tracking Neef in as many as 4 memory cells.}
    \end{subfigure}
    \caption{
    Both the models only weakly detect ``Svetlana Goncharenko" which could be due to lack of span modeling.}
    \label{fig:petra_vs_ref_1}
\end{figure*}


\begin{figure*}[h]
    \begin{subfigure}[t]{\textwidth}
    \begin{mdframed}
        \hlent{Fripp}{1} has performed Soundscapes in several situations: * \hlent{Fripp}{1} has featured Soundscapes on various King Crimson albums. \hlent{He}{1} has also released pure Soundscape recordings as well: * On May 4, 2006, \hlent{\textcolor{red}{\it Steve Ball}}{2} invited \hlent{\textcolor{aqua}{\bf Robert Fripp}}{1} back to the Microsoft campus for a second full day of work on Windows Vista following up on \hlent{\textcolor{aqua}{\bf his}}{1} first visit in the Fall of 2005.
    \end{mdframed}
    \caption{GAP validation instance 17. The ground truth GAP annotation is indicated via colors.}
    \end{subfigure}
    \centering
    \begin{subfigure}[t]{\textwidth}
    \includegraphics[width=\textwidth]{figures/model_runs/petra_positive_2.pdf}
    \caption{Memory log of \modelname with 8-memory cells. \modelname is pretty accurate at tracking Robert Fripp but it misses out on connecting ``Fripp" from the earlier part of the document to ``Robert Fripp". }
    \end{subfigure}
    \begin{subfigure}[t]{\textwidth}
    \includegraphics[width=\textwidth]{figures/model_runs/ref_1.pdf}
    \caption{Memory log of the Referential Reader with 8-memory cells. The Referential Reader completely misses out on all the mentions in the first half of the document (which is not penalized in GAP evaluations where the relevant annotations are typically towards the end of the document). Apart from this, the model ends up tracking Robert Fripp in as many as 6 memory cells, and Steve Ball in 3 memory cells.}
    \end{subfigure}
    \caption{\modelname clearly performs better than the Referential Reader at people tracking for this instance. \modelname's output is more sparse, detects more relevant mentions, and is better at maintaining a 1-to-1 correspondence between memory cells and people.}
    \label{fig:petra_vs_ref_2}
\end{figure*}
 \end{document}
