\vspace{-1mm}
\section{Experiments}

\vspace{-1mm}
\subsection{Datasets}
\label{exp:datasets}
\vspace{-1mm}

We evaluate our model on three publicly-available datasets: \textbf{THUMOS'14}~\cite{THUMOS14}, \textbf{TVSeries}~\cite{de2016online} and \textbf{HACS Segment}~\cite{Zhao_2019_ICCV}.
THUMOS'14 includes over 20 hours of sports video annotated with 20 actions.
We follow prior work~\cite{xu2019temporal,eun2020learning} and train on the validation set (200 untrimmed videos) and evaluate on the test set (213 untrimmed videos).
TVSeries contains 27 episodes of 6 popular TV series, totaling 16 hours of video.
The dataset is annotated with 30 realistic, everyday actions (\eg, open door).
HACS Segment is a large-scale dataset of web videos. It contains 35,300 untrimmed videos over 200 human action classes for training and 5,530 untrimmed videos for validation.

\vspace{-2mm}
\subsection{Settings}
\label{exp:settings}
\vspace{-1mm}

\textbf{Feature Encoding.}
We follow the experimental settings of state-of-the-art methods~\cite{xu2019temporal,eun2020learning}.
We extract video frames at 24 FPS and set the video chunk size to 6.
Decisions are made at the chunk level, and thus accuracy is evaluated at every 0.25 second.
For feature encoding, we adopt the TSN~\cite{wang2016temporal} models implemented in an open-source toolbox~\cite{2020mmaction2}.
Specifically, the features are extracted by one visual model with the ResNet-50~\cite{he2016deep} architecture from the central frame of each chunk and one motion model with the BN-Inception~\cite{ioffe2015batch} architecture from the stacked optical flow fields between 6 consecutive frames~\cite{wang2016temporal}.
The visual and motion features are concatenated along the channel dimension as the final feature .
We experiment with feature extractors pretrained on two datasets, ActivityNet and Kinetics.

\textbf{Implementation Details.}
We implemented our proposed model in PyTorch~\cite{pytorch},
and performed all experiments on a system with 8 Nvidia V100 graphics cards.
For all Transformer units, we set their number of heads as 16 and hidden units as 1024 dimensions.
To learn model weights, we used the Adam~\cite{kingma2014adam} optimizer with weight decay .
The learning rate was linearly increased from zero to  in the first  of training iterations and then reduced to zero following a cosine function.
Our models were optimized with batch size of , and the training was terminated after  epochs.

\textbf{Evaluation Protocols.}
We follow prior work and use per-frame \textbf{mean average precision (mAP)} to evaluate the performance of online action detection.
We also use per-frame \textbf{calibrated average precision (cAP)}~\cite{de2016online} that was proposed for TVSeries to correct the imbalance between positive and negative samples,
,
where ,  is  if frame  is a true positive,  is the number of true positives, and  is the negative and positive ratio.

\vspace{-1.5mm}
\subsection{Comparison with the State-of-the-art Methods}
\label{exp:sota}
\vspace{-2mm}

\begin{table}[!htp]
\vspace{-3mm}
    \caption{
        \textbf{Online action detection and anticipation results} on THUMOS'14 and TVSeries in terms of mAP and cAP, respectively.
        For online action detection, LSTR outperforms the state-of-the-art methods on THUMOSâ€™14 by 3.7\% and 2.4\% in mAP and on TVSeries by 2.8\% and 2.7\% in cAP, using ActivityNet and Kinetics pretrained features, respectively. LSTR also achieves promising results for action anticipation.
        *Results are reproduced by us using their papers' default settings.
    }
    \setlength\tabcolsep{1.0pt}
    \vspace{-2pt}
    \begin{minipage}{0.315\linewidth}
        \footnotesize
        \subcaption{\textbf{Results of online action detection using ActivityNet features}}
        \vspace{-2pt}
        \resizebox{0.97\textwidth}{!}{
            \begin{tabular}{lcc}
                \toprule
                \multirow{2}{*}{} & \multicolumn{1}{c}{THUMOS'14} & \multicolumn{1}{c}{TVSeries} \\
                \cmidrule(lr){2-2} \cmidrule(lr){3-3}
                & mAP (\%) & mcAP (\%) \\
                \midrule
                CDC~\cite{shou2017cdc}        & 44.4 &  -   \\
                RED~\cite{gao2017red}         & 45.3 & 79.2 \\
                TRN~\cite{xu2019temporal}     & 47.2 & 83.7 \\
                FATS~\cite{kim2021temporally} & 51.6 & 81.7 \\
                IDN~\cite{eun2020learning}    & 50.0 & 84.7 \\
                LAP~\cite{qu2020lap}          & 53.3 & 85.3 \\
                TFN~\cite{eun2021temporal}    & 55.7 & 85.0 \\
                LFB*~\cite{wu2019long}        & 61.6 & 84.8 \\
                \textbf{LSTR} (ours)          & \textbf{65.3} & \textbf{88.1} \\
                \bottomrule
            \end{tabular}
        }
        \label{table:sota:activityNet}
    \end{minipage}
    \hfill
    \begin{minipage}{0.315\linewidth}
        \footnotesize
        \subcaption{\textbf{Results of online action detection using Kinetics features}}
        \vspace{-2pt}
        \resizebox{0.97\textwidth}{!}{
            \begin{tabular}{lccc}
                \toprule
                \multirow{2}{*}{} & \multicolumn{1}{c}{THUMOS'14} & \multicolumn{1}{c}{TVSeries} \\
                \cmidrule(lr){2-2} \cmidrule(lr){3-3}
                & mAP (\%) & mcAP (\%) \\
                \midrule
                FATS~\cite{kim2021temporally} & 59.0 & 84.6 \\
                IDN~\cite{eun2020learning}    & 60.3 & 86.1 \\
                TRN~\cite{xu2019temporal}     & 62.1 & 86.2 \\
                PKD~\cite{zhao2020privileged} & 64.5 & 86.4 \\
                WOAD~\cite{gao2020woad}       & 67.1 &  -   \\
                LFB*~\cite{wu2019long}        & 64.8 & 85.8 \\
                \textbf{LSTR} (ours)          & \textbf{69.5} & \textbf{89.1} \\  
                \bottomrule
                \\ \\
            \end{tabular}
        }
        \label{table:sota:kinetics}
    \end{minipage}
    \hfill
    \begin{minipage}{0.315\linewidth}
        \footnotesize
        \subcaption{\textbf{Results of action anticipation using ActivityNet features}}
        \vspace{-2pt}
        \resizebox{0.97\textwidth}{!}{
            \begin{tabular}{lccc}
                \toprule
                \multirow{2}{*}{} & \multicolumn{1}{c}{THUMOS'14} & \multicolumn{1}{c}{TVSeries} \\
                \cmidrule(lr){2-2} \cmidrule(lr){3-3}
                & mAP (\%) & mcAP (\%) \\
                \midrule
                EFC~\cite{gao2017red}         & 34.4 & 72.5 \\
                ED~\cite{gao2017red}          & 36.6 & 74.5 \\
                RED~\cite{gao2017red}         & 37.5 & 75.1 \\
                TRN~\cite{xu2019temporal}     & 38.9 & 75.7 \\
                TTM~\cite{wang2021ttpp}   & 40.9 & 77.9 \\
                LAP~\cite{qu2020lap}          & 42.6 & 78.7 \\
                \textbf{LSTR} (ours)          & \textbf{50.1} & \textbf{80.8} \\  
                \bottomrule
                \\ \\
            \end{tabular}
        }
        \label{table:sota:anticipation}
    \end{minipage}
    \label{table:sota}
\vspace{-2.5mm}
\end{table} \begin{table*}[!htp]
\vspace{-1mm}
    \caption{
        \textbf{Online action detection results when only portions of videos are considered}
        in cAP (\%) on TVSeries (\eg, 80\%-90\% means only frames of this range of action instances were evaluated).
        LSTR outperforms existing methods at every time stage, especially on boundary locations.
    }
    \vspace{-2pt}
    \footnotesize
    \centering
    \setlength\tabcolsep{2.5pt}
    \resizebox{1.0\textwidth}{!}{
        \begin{tabular}{lccccccccccc}
            \toprule
            & \multirow{2}{*}{\makecell{Features}} & \multicolumn{10}{c}{Portion of Video} \\
            \cmidrule{3-12} 
            & & \makecell{0-10\%} & \makecell{10-20\%} & \makecell{20-30\%} & \makecell{30-40\%} & \makecell{40-50\%} & \makecell{50-60\%} & \makecell{60-70\%} & \makecell{70-80\%} & \makecell{80-90\%} & \makecell{90-100\%} \\
            \midrule
            
            TRN~\cite{xu2019temporal} & \multirow{4}{*}{\makecell{ActivityNet}} & 78.8 & 79.6 & 80.4 & 81.0 & 81.6 & 81.9 & 82.3 & 82.7 & 82.9 & 83.3 \\
            IDN~\cite{eun2020learning} &                                        & 80.6 & 81.1 & 81.9 & 82.3 & 82.6 & 82.8 & 82.6 & 82.9 & 83.0 & 83.9 \\
            TFN~\cite{eun2021temporal} &                                        & 83.1 & 84.4 & 85.4 & 85.8 & 87.1 & 88.4 & 87.6 & 87.0 & 86.7 & 85.6 \\
            \textbf{LSTR} (ours) &                                              & \textbf{83.6} & \textbf{85.0} & \textbf{86.3} & \textbf{87.0} & \textbf{87.8} & \textbf{88.5} & \textbf{88.6} & \textbf{88.9} & \textbf{89.0} & \textbf{88.9} \\ [0.2ex]
            \midrule
            IDN~\cite{eun2020learning} & \multirow{3}{*}{Kinetics} & 81.7 & 81.9 & 83.1 & 82.9 & 83.2 & 83.2 & 83.2 & 83.0 & 83.3 & 86.6 \\
            PKD~\cite{zhao2020privileged} &                        & 82.1 & 83.5 & 86.1 & 87.2 & 88.3 & 88.4 & 89.0 & 88.7 & 88.9 & 87.7 \\
            \textbf{LSTR} (ours) &                                        & \textbf{84.4} & \textbf{85.6} & \textbf{87.2} & \textbf{87.8} & \textbf{88.8} & \textbf{89.4} & \textbf{89.6} & \textbf{89.9} & \textbf{90.0} & \textbf{90.1} \\
            \bottomrule
        \end{tabular}
    }
    \vspace{-5pt}
    \label{table:early}
\end{table*}
 
We compare LSTR against other state-of-the-art methods~\cite{xu2019temporal,eun2020learning,gao2020woad} on THUMOS'14, TVSeries, and HACS Segment.
Specifically, on THUMOS'14 and TVSeries, we implement LSTR with the long- and short-term memories of 512 and 8 seconds, respectively.
On HACS Segment, we reduce the long-term memory to 256 seconds, considering that its videos are strictly shorter than 4 minutes.
For LSTR, we implement the two-stage memory compression using Transformer decoder units.
We set the token numbers to  and  and the Transformer layers to  and .

\vspace{-1mm}
\subsubsection{Online Action Detection}
\vspace{-1mm}

\textbf{THUMOS'14.}
We compare LSTR with recent work on THUMOS'14, including methods that use 3D ConvNets~\cite{shou2017cdc} and RNNs~\cite{xu2017end,eun2020learning,gao2020woad}, reinforcement learning~\cite{gao2017red}, and curriculum learning~\cite{zhao2020privileged}.
Table~\ref{table:sota:activityNet} and~\ref{table:sota:kinetics} shows that LSTR significantly outperforms the the state-of-the-art methods~\cite{wu2019long,gao2020woad} by 3.7\% and 2.4\% in terms of mAP using ActivityNet and Kinetics pretrained features, respectively.

\textbf{TVSeries.}
Table~\ref{table:sota:activityNet} and~\ref{table:sota:kinetics} show the online action detection results that LSTR outperforms the state-of-the-art methods~\cite{eun2021temporal,zhao2020privileged} by 2.8\% and 2.7\% in terms of cAP using ActivityNet and Kinetics pretrained features, respectively.
Following prior work~\cite{de2016online}, we also investigate LSTR's performance at different action stages by evaluating each decile (ten-percent interval) of the video frames separately.
Table~\ref{table:early} shows that LSTR outperforms existing methods at every stage of action instances.

\textbf{HACS Segment.}
LSTR achieves 82.6\% on HACS Segment in term of mAP using Kinetics pretrained features.
Note that HACS Segment is a new large-scale dataset with only a few previous results.
LSTR outperforms existing methods RNN~\cite{hochreiter1997long} (77.6\%) by 5.0\% and TRN~\cite{xu2019temporal} (78.9\%) by 3.7\%. 

\vspace{-1mm}
\subsubsection{Action Anticipation}
\vspace{-1mm}

We extend the idea of LSTR to action anticipation for up to 2 seconds (\ie, 8 steps in 4 FPS) into the future.
Specifically, we concatenate another 8 learnable output tokens (with positional embedding) after the short-term memory in the LSTR decoder to produce the prediction results accordingly.
Table~\ref{table:sota:anticipation} shows that LSTR significantly outperforms the state-of-the-art methods~\cite{xu2019temporal,qu2020lap} by 7.5\% mAP on THUMOS and 2.1\% cAP on TVSeries, using ActivityNet pretrained features.

\vspace{-1mm}
\subsection{Design Choices of Long- and Short-Term Memories}
\label{exp:memory}
\vspace{-2mm}

\begin{table}[!htp]
\vspace{-3mm}
    \caption{
        \textbf{Results of LSTR using downsampled long-term memory} on THUMOS'14 in mAP (\%).
        In particular, we use long-term memory of 512 seconds and short-term memory as 8 seconds.
    }
    \vspace{3pt}
    \begin{minipage}{1.0\linewidth}
        \centering
        \footnotesize
        \begin{tabular}{lccccccccccc}
            \toprule
            Temporal Stride & 1 & 2 & 4 & 8 & 16 & 32 & 64 & 128 \\
            \midrule
            LSTR \quad\quad\quad\quad\quad\quad\quad
            & 69.5 & 69.5 & 69.5 & 69.2 & 68.7 & 67.3 & 66.6 & 65.9 \\
            \bottomrule
        \end{tabular}
    \end{minipage}
    \vspace{-5pt}
    \label{table:memory}
\end{table}


 
We experiment for design choices of long- and short-term memories.
Unless noted otherwise, we use THUMOS'14, which contains various video lengths, and Kinetics pretrained features.

\textbf{Lengths of long- and short-term memories.}
We first analyze the effect of different lengths of long-term  and short-term  memory.
In particular, we test  seconds with  starting from 0 second (no long-term memory).
Note that we choose the max length (1024 seconds for THUMOS'14 and 256 seconds for HACS Segment) to cover lengths of 98\% videos, and do not have proper datasets to test longer .
Fig.~\ref{fig:memory} shows that LSTR is beneficial from larger  in most cases.
In addition, when  is short ( in our cases), using larger  obtains better results and when  is sufficient ( in our cases), increasing  does not always guarantee better performance.

\vspace{-3mm}
\begin{figure*}[!htp]
    \begin{center}
        \includegraphics[width=1.0\linewidth]{figures/memory_lengths.pdf}
    \end{center}
    \vspace{-6mm}
    \caption{
        {\textbf{Effect of using different lengths of long- and short-term memories}.}
    }
    \vspace{-3mm}
    \label{fig:memory}
\end{figure*}

\textbf{Can we downsample long-term memory?}
We implement LSTR with  as 8 seconds and  as 512 seconds, and test the effect of downsampling long-term memory.
Table~\ref{table:memory} shows the results that downsampling with strides smaller than 4 does not cause performance drop, but more aggressive strides dramatically decrease the detection accuracy.
Note that, when extracting frame features in 4 FPS, both LSTR encoder () and downsampling with stride  compress the long-term memory to 16 features,
but LSTR achieves much better performance ( vs.  in mAP).
This demonstrates the effectiveness of our ``adaptive compression'' compared to heuristics downsampling. 

\textbf{Can we compensate reduced memory length with RNN?}
We note that LSTR's performance notably decreases when it can only access very limited memory (\eg,  seconds).
Here we test if RNN can compensate LSTR's reduced memory or even fully replace the LSTR encoder.
We implement LSTR using  seconds with an extra Gated Recurrent Unit (GRU)~\cite{chung2014empirical} (its architecture is visualized in the Supplementary Material)
to capture all history outside the long- and short-term memories.
The dashed line in Fig.~\ref{fig:memory} shows the results.
Plugging-in RNNs indeed improves the performance when  is small, but when  is large ( seconds), it does not improve the accuracy anymore.
Note that RNNs are not used in any other experiments in this paper.

\vspace{-2mm}
\subsection{Design Choices of LSTR}
\label{exp:lstr_design}
\vspace{-2mm}

\begin{table}[!htp]
\vspace{-3mm}
    \caption{
        \textbf{Results of different designs of the LSTR encoder and decoder}.
        The length of short-term memory is set to 8 seconds. ``TR'' denotes Transformer.
        The last row is our proposed LSTR design.
    }
    \vspace{5pt}
    \begin{minipage}{1.0\linewidth}
        \centering
        \setlength\tabcolsep{4.5pt}
        \footnotesize
        \vspace{-2pt}
        \begin{tabular}{cccccccccccc}
            \toprule
            \multirow{2}{*}{\makecell{LSTR Encoder}} & \multirow{2}{*}{\makecell{LSTR Decoder}} & \multicolumn{8}{c}{Length of Long-Term Memory  (secs)} \\
            \cmidrule(lr){3-10}
            & & 8 & 16 & 32 & 64 & 128 & 256 & 512 & 1024 \\
            \midrule
            \textit{N/A}            & TR Encoder & 65.7 & 66.8 & 67.1 & 67.2 & 67.3 & 66.8 & 66.5 & 66.2 \\
            \textit{N/A}            & TR Decoder & 66.5 & 67.3 & 67.7 & 68.1 & 68.3 & 67.9 & 67.0 & 66.5 \\
            TR Encoder              & TR Decoder & 65.9 & 66.4 & 66.7 & 67.4 & 67.5 & 67.2 & 67.0 & 66.6 \\
            Projection Layer        & TR Decoder & 66.2	& 67.1 & 67.4 & 67.7 & 67.5 & 67.2 & 66.9 & 66.8 \\
            TR Decoder              & TR Decoder & 66.1 & 67.1 & 67.4 & 68.0 & 68.5 & 68.6 & 68.7 & 68.7 \\
            TR Decoder + TR Encoder & TR Decoder & 66.2 & 67.3 & 67.6 & 68.4 & 68.6 & 68.8 & 68.9 & 69.0 \\
            TR Decoder + TR Decoder & \textit{N/A} & 64.0 & 64.7 & 65.9 & 66.1 & 66.5 & 66.2 & 65.4 & 65.2 \\
            \textBF{TR Decoder} + \textBF{TR Decoder} & \textBF{TR Decoder} & \textbf{66.6} & \textbf{67.8} & \textbf{68.2} & \textbf{68.8} & \textbf{69.2} & \textbf{69.4} & \textbf{69.5} & \textbf{69.5} \\
            \bottomrule
        \end{tabular}
    \end{minipage}
    \vspace{-5pt}
    \label{table:lstr}
\end{table}
 
We continue to explore the design trade-offs of LSTR.
Unless noted otherwise, we use short-term memory of 8 seconds, long-term memory of 512 seconds, and Kinetics pretrained features.

\textbf{Number of layers and tokens.}
First, we assess using different numbers of token embeddings (\ie,  and ) in LSTR encoder.
Fig~\ref{fig:hyperparameter} (left) shows that LSTR is quite robust to different choices (the best and worst performance gap is only about ), but using  and  gets highest accuracy.
Second, we experiment for the effect of using different numbers of Transformer decoder units (\ie,  and ).
As shown in Fig~\ref{fig:hyperparameter} (right), LSTR does not need a large model to get the best performance, and in practice, using more layers can  cause  overfitting.

\begin{figure*}[!htp]
    \vspace{-3mm}
    \begin{center}
        \includegraphics[width=1.0\linewidth]{figures/hyperparameter.pdf}
    \end{center}
    \vspace{-20pt}
    \caption{
        \textbf{Left}: Results of different number of token embeddings for our two-stage memory compression.
        \textbf{Right}: Results of different number of Transformer decoder units for  and .
    }
    \vspace{-5pt}
    \label{fig:hyperparameter}
\end{figure*}

\textbf{Can we unify the temporal modeling using only self-attention models?}
We test if long-term  and short-term  memory can be learned as a whole using self-attention models.
Specifically, we concatenate  and  and feed them into a standard Transformer Encoder~\cite{vaswani2017attention} with a similar model size to LSTR.
Table~\ref{table:lstr} (row 8 vs.~row 1) shows that LSTR achieves better performance especially when  is large (\eg,  vs.  when  and  vs.  when ).
This shows the advantage of LSTR for temporal modeling on long- and short-term context.

\textbf{Can we remove the LSTR encoder?}
We explore this by directly feeding  into the LSTR decoder, and using  as tokens to reference useful information.
Table~\ref{table:lstr} shows that LSTR outperforms this baseline (row 8 vs. row 2), especially when  is large.
We also compare it with self-attention models (row 1) and observe that although neither of them can effectively model prolonged memory, this baseline outperforms the Transformer Encoder.
This also demonstrates the effectiveness of our idea of using short-term memory to query related context from long-range context.

\textbf{Can the LSTR encoder learn effectively using self-attention?}
To evaluate the ``bottleneck'' design with cross-attention in LSTR encoder,
we try modeling  using standard Transformer Encoder units~\cite{vaswani2017attention}.
Note that this still captures  and  with the similar workflow of LSTR,
but does not compress and encode  using learnable tokens.
Table~\ref{table:lstr} (row 8 vs. row 3) shows that LSTR outperforms this baseline with all  settings.
In addition, the performance of this baseline decreases when  gets larger, which suggests the superior ability of LSTR for modeling long-range patterns.

\textbf{How to design the memory compression for the LSTR encoder?}
First, we use a projection layer, consisting of a learnable matrix of size  followed by MLP layers, to compress the long-term memory along the temporal dimension.
Table~\ref{table:lstr} shows that using this simple projection layer (row 4) slightly outperforms the model without long-term memory (row 1), but is worse than attention-based compression methods.
Second, we evaluate the one-stage design with  Transformer decoder units.
Table~\ref{table:lstr} (row 8 vs. row 5) shows that two-stage compression is stably better than one-stage, and their performance gap gets larger when using larger  ( when  and  when ).
Third, we compare cross-attention and self-attention for two-stage compression
by replacing the second Transformer decoder with Transformer encoder.
LSTR stably outperforms this baseline (row 6) by about  in mAP. However, its performance is still better than models with one-stage compression of about  (row 6 vs. row 3) and  (row 6 vs. row 5) on average.

\textbf{Can we remove the LSTR decoder?}
We remove the LSTR decoder to evaluate its contribution.
Specifically, we feed the entire memory to LSTR encoder and attach a multi-layer (MLP) classifier on its output tokens embeddings.
Similar to the above experiments, we increase the model size to ensure a fair comparison.
Table~\ref{table:lstr} shows that LSTR outperforms this baseline (row 7) by about  on large  (\eg, 512 and 1024) and about  on relative small  (\eg, 8 and 16).

\begin{table}[!htp]
    \vspace{-3.5mm}
    \caption{
        \textbf{Results of LSTR using different memory integration methods} in mAP (\%).
        Our proposed integration method using cross-attention stably outperforms the heuristic methods.
    }
    \vspace{1.5mm}
    \begin{minipage}{1.0\linewidth}
        \centering
        \setlength\tabcolsep{4.5pt}
        \footnotesize
        \vspace{-2pt}
        \begin{tabular}{lcccccccccc}
            \toprule
            \multirow{2}{*}{\makecell{Memory Integration Methods}} & \multicolumn{8}{c}{Length of Long-Term Memory  (secs)} \\
            \cmidrule(lr){2-9}
            & 8 & 16 & 32 & 64 & 128 & 256 & 512 & 1024 \\
            \midrule
            Average Pooling & 66.1 & 67.0 & 67.3 & 67.5 & 68.2 & 68.4 & 68.6 & 68.6 \\
            Concatenation   & 65.9 & 67.2 & 67.5 & 67.7 & 68.4 & 68.5 & 68.7 & 68.6 \\
            \textbf{Cross-Attention} (ours) & \textbf{66.6} & \textbf{67.8} & \textbf{68.2} & \textbf{68.8} & \textbf{69.2} & \textbf{69.4} & \textbf{69.5} & \textbf{69.5} \\
            \bottomrule
        \end{tabular}
    \end{minipage}
    \vspace{-1.5mm}
    \label{table:integration}
\end{table} 
\textbf{Cross-attention vs. heuristics for integrating long- and short-term memories.}
We explore integrating the long- and short-term memories by using average pooling and concatenation.
Specifically, the encoded long-term features of size  is converted to a vector of  elements by channel-wise averaging, and the short-term memory of size  is encoded by  Transformer encoder units.
Then, each slot of the short-term features is either averaged or concatenated with the long-term feature vector for action classification.
Note that these models still benefit from LSTR's effectiveness for long-term modeling.
Table~\ref{table:integration} shows that using average pooling and concatenation obtain comparable results, but LSTR with cross-attention stably outperforms these baselines.

\vspace{-2mm}
\subsection{Runtime}
\label{exp:runtime}
\vspace{-5mm}

\begin{table}[!htp]
    \caption{
        \textBF{Runtime of LSTR with different design choices}. The last row is our proposed LSTR design.
    }
    \vspace{1mm}
    \begin{minipage}{1.0\linewidth}
        \centering
        \setlength\tabcolsep{3.5pt}
        \footnotesize
        \begin{tabular}{cc@{\quad}ccccc}
            \toprule
            \multirow{3}{*}{\makecell{LSTR Encoder}} & \multirow{3}{*}{\makecell{LSTR Decoder}} & \multicolumn{4}{c}{Frames Per Second (FPS)} \\
            \cmidrule{3-6} 
            & & \makecell{OptFlow\\Computation} & \makecell{RGB Feature\\Extraction} & \makecell{OptFlow Feature\\Extraction} & \makecell{LSTR} \\
            \midrule
            \textit{N/A}            & TR Encoder & \multirow{4}{*}{8.1} & \multirow{4}{*}{70.5} & \multirow{4}{*}{14.6} & 43.2 \\
            TR Encoder              & TR Decoder & & & & 50.2 \\
            TR Decoder              & TR Decoder & & & & 59.5 \\
            \textBF{TR Decoder} + \textBF{TR Decoder} & \textBF{TR Decoder} & & & & 91.6 \\
            \bottomrule
        \end{tabular}
    \end{minipage}
    \label{table:runtime}
    \vspace{-2mm}
\end{table} 
We report LSTR's runtime in frames per second (FPS) on a system with a single V100 GPU, and use the videos from THUMOS'14 dataset.
The results are shown in Table~\ref{table:runtime}.

We start by comparing the runtime between LSTR's different design choices without considering the pre-processing (\eg, feature extraction).
First, LSTR runs at 91.6 FPS using our two-stage memory compression (row 4), whereas using the one-stage design runs at a slower 59.5 FPS (row 3).
Our two-stage design is more efficient because it does not need to reference information from the long-term memory multiple times, and can be further accelerated during online inference (see Sec.~\ref{lstr:inference}).
Second, we test the LSTR encoder using self-attention mechanisms (row 2).
This design does not compress the long-term memory, thus increasing the computational cost of both the LSTR encoder and decoder, leading to a slower speed of 50.2 FPS.
Third, we test the standard Transformer Encoder~\cite{vaswani2017attention} (row 1), whose runtime speed, 43.2 FPS, is about 2 slower than LSTR.

We also compare LSTR with state-of-the-art recurrent models.
As we are not aware of any prior work that reports their runtime, we test TRN~\cite{xu2019temporal} using their official open-source code~\cite{trn}.
The result shows that TRN runs at 123.3 FPS, which is faster than LSTR.
This is because recurrent models abstract the entire history as a compact representation but LSTR needs to process much more information.
On the other hand, LSTR achieves much higher performance, outperforming TRN by about  in mAP on THUMOS'14 and about  in cAP on TVSeries.

For end-to-end online inference, we follow the state-of-the-art methods~\cite{xu2019temporal,eun2020learning,gao2020woad}
and build LSTR on two-stream features~\cite{wang2016temporal}.
LSTR together with pre-processing techniques run at 4.6 FPS.
Table~\ref{table:runtime} shows that the speed bottleneck is the motion feature extraction --- it accounts for about 90\% of the total runtime including the optical flow computation with DenseFlow~\cite{denseflow}.
One can improve the efficiency largely by using real-time optical flow extractors (\eg, PWC-Net~\cite{sun2018pwc}) or using only visual features extracted by a light-weight backbone (\eg, MobileNet~\cite{howard2017mobilenets} and FBNet~\cite{wu2019fbnet}).

\vspace{-2mm}
\subsection{Error Analysis}
\label{exp:error}
\vspace{-2mm}

\begin{table}[!htp]
    \vspace{-3.5mm}
    \caption{
        \textbf{Action classes with highest and lowest performance} on THUMOS'14.
    }
    \vspace{1mm}
    \begin{minipage}{1.0\linewidth}
        \centering
        \setlength\tabcolsep{3.5pt}
        \resizebox{\textwidth}{!}{
        \begin{tabular}{lcccccccc}
            \toprule
            Action Classes & \textcolor{green}{HammerThrow} & \textcolor{green}{PoleVault} & \textcolor{green}{LongJump} & \textcolor{green}{Diving} & \textcolor{red}{BaseballPitch} & \textcolor{red}{FrisbeeCatch} & \textcolor{red}{Billiards} & \textcolor{red}{CricketShot} \\
            \midrule
            AP (\%) & 92.8 & 89.7 & 86.9 & 86.7 & 55.4 & 49.4 & 39.8 & 38.6 \\
            \bottomrule
        \end{tabular}
        }
    \end{minipage}
    \label{table:action_classes}
    \vspace{-1mm}
\end{table}

\begin{figure*}[!htp]
    \vspace{-2mm}
    \begin{center}
        \includegraphics[width=1.0\linewidth]{figures/failure_cases.pdf}
    \end{center}
    \vspace{-2mm}
    \caption{\textbf{Failure cases} on THUMOS'14. Action classes from left to right are ``BaseballPitch'', ``FrisbeeCatch'', ``Billiards'', and ``CricketShot''. Red circle indicates where the action is happening.}
    \label{fig:failure_cases}
    \vspace{-2mm}
\end{figure*}

In Table~\ref{table:action_classes}, we list the action classes from THUMOS'14 where LSTR gets the highest (color green) and the lowest (color red) per-frame APs. 
In Fig.~\ref{fig:failure_cases}, we illustrate four sample frames with incorrect predictions.
More visualizations are included in the Supplementary Material.
We observe that LSTR sees a decrease in detection accuracy when the action incurs only tiny motion or the subject is very far away from the camera, but excels at recognizing actions with long temporal span and multiple stages, such as ``PoleVault'' and ``Long Jump''. 
This suggests we may explore extending the temporal modeling capability of LSTR to both spatial and temporal domains.