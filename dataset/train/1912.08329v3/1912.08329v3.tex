\section{Experiments}
In this section, we demonstrate the performance of our framework for MVS with a comprehensive set of experiments in standard benchmarks. Below, we first describe the datasets and benchmarks and then analyze our results.
\subsection{Datasets}
 \noindent\textbf{DTU Dataset}~\cite{aanaes2016large} is a large-scale MVS dataset with 124 scenes scanned from 49 or 64 views under 7 different lighting conditions. DTU provides 3D point clouds acquired using structured-light sensors.~Each view consists of an image and the calibrated camera parameters.~To train our model, we generate a  depth map for each view by using the method provided by MVSNet~\cite{yao2018mvsnet}. We use the same training, validation and evaluation sets as defined in~\cite{yao2018mvsnet,yao2019recurrent}.
 
 \noindent\textbf{Tanks and Temples}~\cite{knapitsch2017tanks} contains both indoor and outdoor scenes under realistic lighting conditions with large scale variations. For comparison with other approaches, we evaluate our results on the \emph{intermediate set}.
\subsection{Implementation}
\noindent\textbf{Training} We train our CVP-MVSNet on DTU training set.  
 Unlike previous methods~\cite{yao2018mvsnet,yao2019recurrent} that take high resolution image as input but estimate a depth map of smaller size, our method produces the same size depth map as the input image.
 For training, we match the ground-truth depth map by downsampling the high resolution image into a smaller one of size .~Then, we build the image and ground truth depth pyramid with 2 levels.~To construct the~\emph{cost volume pyramid}, we uniformly sample  depth hypotheses across entire depth range at the coarsest (nd) level.~Then, each pixel has  depth residual hypotheses at the next level for the refinement of the depth estimation.~Following MVSNet~\cite{yao2018mvsnet}, we adopt 3 views for training. We implemented our network using Pytorch~\cite{paszke2017automatic}, and we used ADAM~\cite{kingma2014adam} to train our model. The batch size is set to 16 and the network is end-to-end trained on a NVIDIA TITAN RTX graphics card for 27 epochs. The learning rate is initially set to 0.001 and divided by 2 iteratively at the ,, and  epoch.

\noindent\textbf{Metrics}. We follow the standard evaluation protocol as in~\cite{aanaes2016large,yao2018mvsnet}. We report the~\emph{accuracy},~\emph{completeness} and~\emph{overall score} of the reconstructed point clouds. \emph{Accuracy} is measured as the distance from  estimated point clouds to the ground truth ones in millimeter and~\emph{completeness} is defined as the distance from ground truth point clouds to the estimated ones~\cite{aanaes2016large}. The~\emph{overall score} is the average of accuracy and completeness~\cite{yao2018mvsnet}. 

\vspace{0.1cm}
\noindent\textbf{Evaluation} As the parameters are shared across the~\emph{cost volume pyramid}, we can evaluate our model with different number of cost volumes and input views.~For the evaluation, we set the number of depth sampling,  for the coarsest depth estimation (same as~\cite{chen2019point}. We also provide results of  in the Supp. Mat.) and  for the following depth residual inference levels. Similar to previous methods~\cite{chen2019point,yao2018mvsnet,yao2019recurrent}, we use 5 views and apply the same depth map fusion method to obtain the point clouds. We evaluate our model with images of different size and set the pyramid levels accordingly to maintain a similar size as the input image () at coarsest level. For instance, for an input size of , the pyramid has 5 levels and 4 levels for an input size of  and .

\begin{table}[!t]
\begin{center}
\footnotesize
\begin{tabular}{ll|ccc}
\hline
\multicolumn{2}{c|}{Method} & Acc. & Comp. & Overall (\textit{mm}) \\
\hline\hline
\parbox[t]{2mm}{\multirow{5}{*}{\rotatebox[origin=c]{90}{Geometic}}}
& Furu\cite{furu2010} & 0.613 & 0.941 & 0.777 \\
&Tola\cite{tola2012} & 0.342 & 1.190 & 0.766 \\
&Camp\cite{comp2008} & 0.835 & 0.554 & 0.695 \\
&Gipuma\cite{galliani2016gipuma} & \textbf{0.283} & 0.873 & 0.578 \\
&Colmap\cite{schoenberger2016sfm,schoenberger2016mvs} & 0.400 & 0.664 & 0.532 \\
\hline
\parbox[t]{2mm}{\multirow{5}{*}{\rotatebox[origin=c]{90}{Learning}}}
&SurfaceNet\cite{ji2017surfacenet} & 0.450 & 1.040 & 0.745 \\
&MVSNet\cite{yao2018mvsnet} & 0.396 & 0.527 & 0.462 \\
&P-MVSNet\cite{luo2019p} & 0.406 & 0.434 & 0.420 \\
&R-MVSNet\cite{yao2019recurrent} & 0.383 & 0.452 & 0.417 \\
&MVSCRF\cite{xue2019mvscrf} & 0.371 & 0.426 & 0.398 \\
&Point-MVSNet\cite{chen2019point} & 0.342 & \underline{0.411} & \underline{0.376} \\\hline
\multicolumn{2}{c|}{Ours} & \underline{0.296} & \textbf{0.406} & \textbf{0.351} \\
\hline
\end{tabular}
\end{center}
\vspace{-0.3cm}
\caption{Quantitative results of reconstruction quality on DTU dataset (lower is better). Our method outperforms all methods on Mean Completeness and Overall reconstruction quality and achieved second best on Mean Accuracy.
}
\label{table:quatitative}
\vspace{-0.4cm}
\end{table}
\subsection{Results on DTU dataset}

We first compare our results to those reported by traditional geometric-based methods and other learning-based baseline methods. As summarized in Table~\ref{table:quatitative}, our method outperforms all current learning-based methods in terms of~\emph{accuracy},~\emph{completeness} and~\emph{overall score}. Compared to geometric-based approaches, only the method proposed by Galliani \etal~\cite{galliani2016gipuma} provides slightly better results in terms of mean~\emph{accuracy}.

We now compare our results to related learning based methods in terms of GPU memory usage and runtime for different input resolution.~The summary of these results is listed in Table \ref{table:dtu-speed}.~As shown, our network, with a similar memory usage (bottom row), is able to produce better point clouds with \comment{significantly} lower runtime. In addition, compared to Point-MVSNet~\cite{chen2019point} on the same size of depth map output (top rows), our approach is six times faster and consumes six times less memory \comment{provides} with similar accuracy. We can output high resolution depth map with better accuracy, less memory usage and shorter runtime than Point-MVSNet~\cite{chen2019point}.

  \begin{figure*}[!t]
    \begin{center}
    \setlength\tabcolsep{1pt}
    \begin{tabular}{cccc}
          \includegraphics[width=0.225\linewidth]{Figs/rmvsnet_pc_l.jpg}
          & \includegraphics[width=0.225\linewidth]{Figs/point_mvs_pc_l.jpg}
          & \includegraphics[width=0.225\linewidth]{Figs/ours_pc_l.jpg}
          & \includegraphics[width=0.225\linewidth]{Figs/gt_pc_l.jpg}\\
          \includegraphics[width=0.225\linewidth]{Figs/rmvsnet_norm_s.pdf}
          & \includegraphics[width=0.225\linewidth]{Figs/point_mvs_norm_l_s.pdf}
          & \includegraphics[width=0.225\linewidth]{Figs/ours_norm_l1_s.pdf}
          & \includegraphics[width=0.225\linewidth]{Figs/gt_norm_l_s.pdf}\\
          R-MVSNet~\cite{yao2019recurrent} & Point-MVSNet~\cite{chen2019point} & Ours & Ground truth
    \end{tabular}
    \end{center}
    \vspace{-0.5cm}
    \caption{Qualitative results of scan 9 of DTU dataset. The upper row shows the point clouds and the bottom row shows the normal map corresponding to the orange rectangle. As highlighted in the blue rectangle, the completeness of our results is better than those provided by Point-MVSNet\cite{chen2019point}. The normal map (orange rectangle) further shows that our results are smoother on surfaces while maintaining more high-frequency details. 
    }
    \label{fig:normDTU}
    \vspace{-0.1cm}
\end{figure*}
 
\begin{table*}[!t]
\begin{center}
\resizebox{0.925\linewidth}{!}{
\footnotesize
\begin{tabular}{l|cccccccc}
\hline
Method       & Input Size & Depth Map Size & Acc.\scriptsize(mm) & Comp.\scriptsize{(mm)} & Overall\scriptsize{(mm)} & \textit{f-score}\scriptsize{(0.5mm)} & GPU Mem\scriptsize{(MB)} & Runtime\scriptsize{(s)} \\
\hline\hline
Point-MVSNet\cite{chen2019point} & 1280x960 & 640x480        & 0.361     & 0.421      & 0.391 & 84.27       & 8989         & 2.03            \\
Ours-640     & 640x480    & 640x480         & 0.372     & 0.434      & 0.403       & 82.44 & \textbf{1416}         & \textbf{0.37}        \\
\hline
Point-MVSNet\cite{chen2019point} & 1600x1152  & 800x576        & 0.342     & 0.411      & 0.376  & -      & 13081             & 3.04            \\
Ours-800     & 800x576    & 800x576         & 0.340    & 0.418     & 0.379    & 86.82    & \textbf{2207}         & \textbf{0.49}        \\
\hline
MVSNet\cite{yao2018mvsnet}       & 1600x1152  & 400x288        & 0.396     & 0.527      & 0.462   & 78.10     & 22511        & 2.76            \\
R-MVSNet\cite{yao2019recurrent}     & 1600x1152  & 400x288        & 0.383     & 0.452      & 0.417  & 83.96      & \textbf{6915}         & 5.09            \\
Point-MVSNet\cite{chen2019point} & 1600x1152  & 800x576        & 0.342     & 0.411      & 0.376  & -      & 13081             & 3.04            \\
Ours         & 1600x1152  & 1600x1152      & \textbf{0.296}     & \textbf{0.406}     & \textbf{0.351}  & \textbf{88.61}     & 8795         & \textbf{1.72}        \\
\hline
\end{tabular}
}
\end{center}
\vspace{-0.5cm}
\caption{Comparison of reconstruction quality, GPU memory usage and runtime on DTU dataset for different input sizes. GPU memory usage and runtime are obtained by running the official evaluation code of baselines on a same machine with a NVIDIA TITAN RTX graphics card. For the same size of depth maps (Ours-640, Ours-800) and a performance similar to Point-MVSNet~\cite{chen2019point}, our method is 6 times faster and consumes 6 times smaller GPU memory. For the same size of input images (Ours), our method achieves the best reconstruction with the shortest time and a reasonable GPU memory usage. 
}

\label{table:dtu-speed}
\vspace{-0.14cm}
\end{table*}



\begin{table*}[!t]
\begin{center}
\footnotesize
\begin{tabular}{l|cccccccccc}
\hline
Method       & Rank       & Mean  & Family & Francis & Horse & Lighthouse & M60   & Panther & Playground & Train \\
\hline\hline
P-MVSNet~\cite{luo2019p} & \textbf{11.72} & \textbf{55.62} & \underline{70.04} & \underline{44.64} & \textbf{40.22} & \textbf{65.20} & \underline{55.08} & \textbf{55.17} & \textbf{60.37} & \textbf{54.29} \\
Ours         & 12.75 & \underline{54.03} & \textbf{76.5}   & \textbf{47.74}   & \underline{36.34} & \underline{55.12}      & \textbf{57.28} & \underline{54.28}   & \underline{57.43}      & \underline{47.54} \\
Point-MVSNet\cite{chen2019point} & 29.25      & 48.27 & 61.79  & 41.15   & 34.20  & 50.79      & 51.97 & 50.85   & 52.38      & 43.06 \\
R-MVSNet\cite{yao2019recurrent}     & 31.75      & 48.40  & 69.96  & 46.65   & 32.59 & 42.95      & 51.88 & 48.80    & 52.00         & 42.38 \\
MVSNet\cite{yao2018mvsnet}       & 42.75      & 43.48 & 55.99  & 28.55   & 25.07 & 50.79      & 53.96 & 50.86   & 47.90       & 34.69 \\
\hline
\end{tabular}
\end{center}
\vspace{-0.4cm}
\caption{Performance on Tanks and Temples~\cite{knapitsch2017tanks} on November 12, 2019. Our results outperform Point-MVSNet~\cite{chen2019point}, which is the strongest baseline on DTU dataset, and are competitive compared to P-MVSNet~\cite{luo2019p}.}
\label{tanks}
\vspace{-0.4cm}
\end{table*}

Figures~\ref{fig:normDTU} and~\ref{fig:qualityDTU} show some qualitative results. As shown, our method is able to reconstruct more details than Point-MVSNet~\cite{chen2019point}, see for instance, the details highlighted in blue box of the roof behind the front building. Compared to R-MVSNet~\cite{yao2019recurrent} and Point-MVSNet~\cite{chen2019point}, as we can see in the normal maps, our results are smoother on the surfaces while capturing more high-frequency details in edgy areas. 
\begin{figure*}[!ht]
    \begin{center}
    \setlength\tabcolsep{3pt}
    \begin{tabular}{cccc}
          \includegraphics[width=0.18\linewidth]{Figs/dtu_vis/scan13_r.pdf}
          & \includegraphics[width=0.18\linewidth]{Figs/dtu_vis/scan13_point.pdf}
          & \includegraphics[width=0.18\linewidth]{Figs/dtu_vis/scan13_ours.pdf}
          & \includegraphics[width=0.18\linewidth]{Figs/dtu_vis/scan13_gt.pdf}\\
          \includegraphics[width=0.18\linewidth]{Figs/dtu_vis/scan49_r.pdf}
          & \includegraphics[width=0.18\linewidth]{Figs/dtu_vis/scan49_point.pdf}
          & \includegraphics[width=0.18\linewidth]{Figs/dtu_vis/scan49_ours.pdf}
          & \includegraphics[width=0.18\linewidth]{Figs/dtu_vis/scan49_gt.pdf}\\
          R-MVSNet~\cite{yao2019recurrent} & Point-MVSNet~\cite{chen2019point} & Ours & Ground truth
    \end{tabular}
    \end{center}
    \vspace{-0.5cm}
    \caption{Additional results from DTU dataset. Best viewed on screen.}
    \label{fig:qualityDTU}
    \vspace{-0.4cm}
\end{figure*}
\begin{figure*}[!ht]
    \begin{center}
    \resizebox{0.7\linewidth}{!}{
    \setlength\tabcolsep{2pt}
    \begin{tabular}{cc}
          \includegraphics[width=0.405\linewidth]{Figs/tank/train.pdf}
          & \includegraphics[width=0.3650\linewidth]{Figs/tank/panther.pdf}\\
         (a) Train & (b) Panther
    \end{tabular}
    }
    \resizebox{0.781\linewidth}{!}{
    \setlength\tabcolsep{1pt}
    \begin{tabular}{ccc}
          \includegraphics[width=0.284\linewidth]{Figs/tank/lighthouse.pdf}
          & \includegraphics[width=0.226\linewidth]{Figs/tank/family.pdf}
          & \includegraphics[width=0.284\linewidth]{Figs/tank/horse.pdf}\\
         (c) Lighthouse & (d) Family & (e) Horse
    \end{tabular}
    }
    \end{center}
    \vspace{-0.5cm}
    \caption{Point cloud reconstruction of Tanks and Temples dataset~\cite{knapitsch2017tanks}. Best viewed on screen.}
    \label{fig:qualityTanks}
    \vspace{-0.25cm}
\end{figure*}

\subsection{Results on Tanks and Temples}
\vspace{-0.1cm}
We now evaluate the generalization ability of our method. To this end, we use the model trained on DTU \textbf{without any fine-tuning} to reconstruct point clouds for scenes in Tanks and Temples dataset. For fair comparison, we use the same camera parameters, depth range and view selection of MVSNet~\cite{yao2018mvsnet}. For comparison, we consider four baselines~\cite{chen2019point,luo2019p,yao2018mvsnet,yao2019recurrent} and evaluate the \textit{f-score} on Tanks and Temples. Table~\ref{tanks} summarizes these results. As shown, our method yielded a mean \textit{f-score} 5\% higher than Point-MVSNet~\cite{chen2019point}, which is the best baseline on DTU dataset, and only 1\% lower than P-MVSNet~\cite{luo2019p}. Note that P-MVSNet~\cite{luo2019p} applies more depth filtering process for point cloud fusion than ours which just follows the simple fusion process provided by MVSNet~\cite{yao2018mvsnet}. Qualitative results of our point cloud reconstructions are shown in Fig.~\ref{fig:qualityTanks}. 

\begin{table*}[!t]
\footnotesize
\begin{center}
\begin{tabular}{cc}
\resizebox{0.38\linewidth}{!}{
\begin{tabular}{c|c|ccc}
\hline
Levels & Coarsest Img. Size & Acc. & Comp. & Overall \\
\hline\hline
\comment{1 & 128x160 & 0.483 & 0.526 & 0.5045  \\}
2 & 80x64 & \textbf{0.296} & \textbf{0.406} & \textbf{0.351}  \\
3 & 40x32 & 0.326 & 0.407 & 0.366 \\
4 & 20x16 & 0.339 & 0.411 & 0.375  \\
5 & 10x8  & 0.341 & 0.412 & 0.376 \\
\hline
\end{tabular} 
}
&
\resizebox{0.4\linewidth}{!}{
\begin{tabular}{c|ccc} 
\hline
Pixel Interval & Acc. (mm) & Comp. (mm) & Overall (mm) \\
\hline\hline
2              & 0.299     & 0.413      & 0.356        \\
1              & 0.299     & \textbf{0.403}      & \textbf{0.351}        \\
0.5            & \textbf{0.296}     & 0.406      & \textbf{0.351}        \\
0.25           & 0.313     & 0.482      & 0.397        \\
\hline
\end{tabular} 
} \a)&(b)
\end{tabular}
\end{center}
\vspace{-0.55cm}
\caption{Parameter sensitivity on DTU dataset. a) Accuracy as a function of the number of pyramid levels. b) Accuracy as a function of the interval setting.}\label{table:interval}
\vspace{-0.4cm}
\end{table*}

\subsection{Ablation study}
\noindent\textbf{Training pyramid levels.} We first analyze the effect of the number of pyramid levels on the quality of the reconstruction. To this end, we downsample the images to form pyramids with four different levels. Results of this analysis are summarized in Table~\ref{table:interval}a. As shown, the proposed 2-level pyramid is the best. As the level of pyramid increases, the image resolution of the coarsest level decreases. For more than 2-levels, this resolution is too small to produce a good initial depth map to be refined.


\noindent\textbf{Evaluation pixel interval settings.} We now analyze the effect of varying the pixel interval setting for depth refinement. As discussed in section~\ref{sec:depthEstimator}, the depth sampling is determined by the corresponding pixel offset in source views, hence, it is important to set a suitable pixel interval. Table \ref{table:interval}b summarizes the effect of varying the interval from depth ranges corresponding to 0.25 pixel to 2 pixels during \mbox{evaluation}. As shown, the performance drops when the interval is too small (0.25 pixel) or too large (2 pixels).