\newcommand{\simplify}{\textsc{Simplify}}
\newcommand{\E}{\text{\bf E}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\qavg}{\hat{q}}
\newcommand{\alphavg}{\hat{\alpha}}

Recall the setup described in Section~\ref{subsec:dep-round}, and properties (A1)-(A4) in particular. For the $k$-median application, we will actually need a weighted generalization of this, as mentioned briefly in Section~\ref{subsubsec:depround-kmedian}. The basic change is that we now have positive weights $a_1, a_2, \ldots, a_n$, and want to preserve the \emph{weighted} sum $\sum_i a_i p_i$, instead of $\sum_i p_i$ as in (A2). Such preservation may not be possible (no matter what the rounding), so we will leave at most one variable un-rounded at the end, as specified by property (A0') next. Let us describe our main problem, and then discuss the results we obtain. Our \textbf{main problem},  
given $P=(p_1,\dots,p_n)\in[0,1]^n$ and positive weights $A=(a_1\dots,a_n)\in\mathbb{R}_{>0}^n$, is to efficiently sample a vector $(X_1,X_2,\dots,X_n)$ from a distribution on $[0,1]^n$ which satisfies the following properties:
\begin{description}
\item[(A0')] ``almost-integrality": all but at most one of the $X_i$ lies in $\{0,1\}$ (with the remaining at most one element lying in $[0,1]$); 
\item[(A1')] $\forall i, \E[X_i]=p_i$;
\item[(A2')] $\Pr[\sum_i a_iX_i = \sum_i a_ip_i]=1$;
\item[(A3')] $\forall S \subseteq [n]$, $\E[\prod_{i \in S} (1 - X_i)] \leq \prod_{i \in S} (1 - p_i)$, and 
$\E[\prod_{i \in S} X_i] \leq \prod_{i \in S} p_i$;
\item[(A4': informal)] if the weights $a_i$ are ``not too far apart", there is near-independence for subsets of $\{X_1, X_2, \ldots, X_n\}$ that are of cardinality at most $t$, 
analogously to (A4). 
\end{description}

In this section we will give an $O(n)$-time algorithm (called {\sc DepRound}) for sampling from such a distribution; see Section~\ref{sec:depround}.
In particular, this shows that such distributions exist. This algorithm is a generalization of the unweighted version given in \cite{srin:level-sets}, with a specific (random) ordering of operations, leading to the added property (A4') of near-independence. 
Our main theorem is Theorem~\ref{thm:limited-dep-general}. 
It basically says, in the notation of (\ref{eqn:desired-goal}), that we can achieve $\beta_1, \beta_2 = O(t^2/(n \alpha^3))$ when 
$t \leq O(\sqrt{n \alpha^3})$. Thus, we obtain near-independence up to fairly large sizes $t$. This bound is further improved in 
Section~\ref{sec:dep-round-special}. Let us also remark about the possible (sole) index $i$ that is left unrounded, as in (A0'). Three simple ways to round this are to round down, round up, or round randomly; these can be chosen in a problem-specific manner. None of these three fits the $k$-median application perfectly; a different probabilistic handling of this index $i$ is done in Section~\ref{sec:bipoint-main-case}. 

Note that {\sc DepRound} could be described more simply as a form of pipage rounding (with an added, crucial, element of processing the variables in random order), with flow adjusted proportionally to the weights. However, in order to facilitate the analysis of (A4'), we give the following, less compact description of the algorithm.
Also note that we will apply this method to $k$-median in Section~\ref{sec:bipoint-main-case}, but here it is described as a general-purpose rounding procedure, independent of $k$-median; this is because we believe that this method is of independent interest since it goes much beyond negative correlation. Thus, the reader is asked to note that the notation defined in this section is largely separate from that of the other sections.

\subsection{The {\sc Simplify} subroutine}
\label{sec:simplify}
As in \cite{srin:level-sets}, our main subroutine is a procedure called $\simplify(a_1,a_2,\beta_1,\beta_2)$. It takes as input two fractional values $\beta_1,\beta_2\in(0,1)$, and corresponding positive weights $a_1,a_2\in\mathbb{R}_{>0}$. The subroutine outputs a random pair of values $(\gamma_1,\gamma_2)\in[0,1]^2$, with the following properties:

\begin{description}
\item[(B0)] $\gamma_1,\gamma_2\in[0,1]$, and at least one of the two variables is integral (0 or 1);
\item[(B1)] $\E[\gamma_1]=\beta_1$ and $\E[\gamma_2]=\beta_2$;
\item[(B2)] $\Pr\left[a_1\gamma_1+a_2\gamma_2=a_1\beta_1+a_2\beta_2 \right]=1$; and
\item[(B3)]
$\E[\gamma_1\gamma_2]\le\beta_1\beta_2$, and $\E[(1-\gamma_1)(1-\gamma_2)\le(1-\beta_1)(1-\beta_2)$.
\end{description}

Now define $\text{\sc simplify}(a_1,a_2,\beta_1,\beta_2)$ as follows. 
There are four cases:
\begin{enumerate}[\bf\text{Case} I:]
\item $0\le a_1\beta_1+a_2\beta_2 \le\min\{a_1,a_2\}$. With probability $a_2\beta_2/(a_1\beta_1+a_2\beta_2 )$ set $\gamma_1= 0$. With remaining probability set $\gamma_2= 0$.
\item $a_1<a_1\beta_1+a_2\beta_2 <a_2$. With probability $\beta_1$ set $\gamma_1=1$. With remaining probability set $\gamma_1=0$.
\item $a_2<a_1\beta_1+a_2\beta_2 <a_1$. With probability $\beta_2$ set $\gamma_2=1$. With remaining probability set $\gamma_2=0$.
\item $\max\{a_1,a_2\}\le a_1\beta_1+a_2\beta_2 \le a_1+a_2$. With probability $a_2(1-\beta_2)/(a_1(1-\beta_1)+a_2(1-\beta_2))$ set $\gamma_1=1$. With remaining probability set $\gamma_2=1$.
\end{enumerate}

If we set $\gamma_1=0$, then set $\gamma_2=\beta_2+\beta_1\frac{a_1}{a_2}$.

If we set $\gamma_1=1$, then set $\gamma_2=\beta_2-(1-\beta_1)\frac{a_1}{a_2}$.

If we set $\gamma_2=0$, then set $\gamma_1=\beta_1+\beta_2\frac{a_2}{a_1}$.

If we set $\gamma_2=1$, then set $\gamma_1=\beta_1-(1-\beta_2)\frac{a_2}{a_1}$.

\begin{lemma}\label{lemma:b-props}
$\simplify(a_1,a_2,\beta_1,\beta_2)$ outputs $(\gamma_1,\gamma_2)$ with properties (B0), (B1), (B2), and (B3).
\end{lemma}
This is straightforward to show; we provide a partial proof in Appendix \ref{apdx:depround}.

\subsection{Main algorithm: {\sc DepRound}} \label{sec:depround}
We now describe the full dependent rounding algorithm, which we denote {\sc DepRound}. 


\begin{algorithm}[h]
\caption{$\textsc{DepRound}(P,A)$}
\begin{algorithmic}[1]
\STATE $X\gets P$
\STATE Let $\pi\in S_n$ be a random permutation.
\WHILE{$X$ contains at least two fractional elements} 
\STATE Let $X_i$ and $X_j$ be the two left-most fractional elements in $\pi(X)$.\label{step:pick_ij}
\STATE $(X_i, X_j)\gets \simplify(a_i,a_j,X_i,X_j)$
\ENDWHILE
\STATE \textbf{Return} $X^{(t)}$.
\end{algorithmic} 
\label{algo:A}
\end{algorithm}
Define $X^{(s)}=(X^{(s)}_1,X^{(s)}_2,\ldots,X^{(s)}_n)$ to be the value of $X$ after the step $s$ of the rounding process, with $X^{(0)}=P$, and $X^{(T)}=X$ if the algorithm halts after $T$ steps. 
We say $X_i$ is \emph{fixed} during step $s$ if $X_i^{(s-1)}$ is fractional but $X_i^{(s)}$ is integral, since this implies $X_i$ will not change in any future steps.

It is not hard to follow the proof of \cite{srin:level-sets} and validate properties (A0'), (A1'), (A2'), and (A3'); we present a proof sketch below. 
\begin{lemma}
{\sc DepRound} samples a vector in $O(n)$ time; this vector satisfies properties (A0'), (A1'), (A2'), and (A3').
\end{lemma}
\begin{proof}
By definition, {\sc DepRound} ends only when there is at most one fractional variable remaining, and by definition of $\simplify$, the remaining variables are either 0 or 1, showing (A0'). Since at least one variable is fixed in each constant-time step, the algorithm takes at most $n-1$ steps, and thus runs in linear time. (The random permutation $\pi$ may also be generated in linear time.)

(B1) implies that for each $i$ and $s$, $\E[X_i^{(s)}]=X_i^{(s-1)}$. By induction, this implies (A1'). (B2) implies that $\sum_i a_i X^{(T)}_i =\sum_i a_i X^{(T-1)}_i =\ldots=\sum_i a_i X^{(0)}=\sum_i a_i p_i$, thus implying (A2'). Finally, let $X_i$ and $X_j$ be the variables chosen in line \ref{step:pick_ij} during step $t$. If $i,j\in S$, then (B3) implies $\E[\prod_{i\in S}X^{(s)}_i]\le \prod_{i\in S}X^{(s-1)}_i$ and $\E[\prod_{i\in S}(1-X^{(s)}_i)]\le\prod_{i\in S}(1-X^{(s-1)}_i)$ (all other terms in the product are constant). If only one or neither of $i,j$ are in $S$, then the same hold with equality. By induction, these imply (A3').
\end{proof}
Note that the above properties hold under any ordering $\pi$. The additional element of using $\pi$ to process the indices in random order is needed only for the new property (A4').

\subsection{Limited dependence} 
In this section we will prove the limited dependence property (A4').  Consider a subset $I\subseteq[n]$ of $t$ indices, with corresponding ``target'' bits $(b_i)_{i\in I}\in\{0,1\}^t$.  For each $i\in I$, define $Y_i:=X_i$ if $b_i=1$, or $Y_i:=1-X_i$ if $b_i=0$. We are interested in the value of $\E\left[\prod_{i\in I}Y_i\right]$, which is essentially equal to the joint probability $\bigwedge_{i\in I}(X_i=b_i)$. Note these are not exactly equivalent, because of the one fractional variable. This variable must be handled in a domain-specific way to ensure this property holds, as we will do when applying it to $k$-median.

From property (A1'), we have $\E[X_i]=p_i$. Similarly, $\forall i\in I$, define $q_i$ such that $\E[Y_i]=q_i$. That is, let $q_i$ be either $p_i$ or $1-p_i$ if $b_i$ is 1 or 0, respectively. If we independently rounded each variable, $\E[\prod_{i\in I} Y_i]$ would be exactly $\prod_{i\in I}q_i$ (but (A2') would be violated). We will show that when set $I$ is not too large, the product is still very close to $\prod_{i\in I}q_i$ in expectation (i.e., that the variables $\{X_i\}_{i\in I}$ are near-independent). 

During a run of {\sc DepRound}, we say that two variables $X_i$ and $X_j$ are \emph{co-rounded} if they are both changed in the same step. We will first show that if two variables are far apart in $\pi(X)$, then they are unlikely to be co-rounded. Then we will show that a group of variables is near-independent if the probability of any two of them being co-rounded is small. Finally, we will show that for a small enough set $I$ variables are very likely to be far apart, and thus very likely to remain independent.

\subsubsection{Distant variables are seldom co-rounded}
Recall that {\sc DepRound} always calls $\simplify$ on the two left-most fractional variables in $\pi(X)$, fixing at least one of them (to 0 or 1). Thus, once a variable \emph{survives} (i.e., remains fractional after) one step it will continue to be included in all subsequent $\simplify$ steps until it gets fixed (or becomes the last remaining fractional variable). We want to upper bound the probability that a variable survives for too long. We first show that in any two consecutive steps involving $X_i$, there is a minimum probability that $X_i$ gets fixed (if the weights are not too different).
\begin{lemma}\label{lemma:pair-prob}
Let $a_{min}:=\min_i\{a_i\}$ and $a_{max}:=\max_i\{a_i\}$ be the minimum and maximum weights. Suppose $\frac{a_{max}}{a_{min}}\le 2$. 
Suppose $X_i$ is co-rounded with variable $X_j$ in step $s$, and if it survives it will be co-rounded with variable $X_k$ in step $s+1$. Let $\beta_j:=X_j^{(s-1)}$ and $\beta_k:=X_k^{(s)}$. 
Then $X_i$ will be fixed in one of these two steps with probability at least $p=\min\{\beta_j,1-\beta_j\}\cdot\min\{\beta_k,1-\beta_k\}$.
\end{lemma}
\begin{proof}
What is the probability that $X_i$ is fixed in the first step? It depends on which of the four cases occur. In case I, it is $\frac{a_j\beta_j}{a_i\beta_i+a_j\beta_j}\ge \frac{a_j\beta_j}{a_j}=\beta_j$. In case II it is 1. In case IV it is $\frac{a_j(1-\beta_j)}{a_i(1-\beta_i)+a_j(1-\beta_j)}
= \frac{a_j(1-\beta_j)}{a_i+a_j - (a_i\beta_i+a_j\beta_j)}
\ge  \frac{a_j(1-\beta_j)}{a_i+a_j-a_i}=1-\beta_j$. In these three cases, $X_i$ is fixed with probability at least $\min\{\beta_j,1-\beta_j\}\ge p$. However, in the remaining case III, $X_i$ will be fixed with probability 0. 

Given that case III occurs in the first step, what is the probability that $X_i$ gets fixed in the second step? It is at least the probability that one of cases I, II or IV occur in the second step, times $\min\{\beta_k,1-\beta_k\}$ (by the same reasoning as above). When case III occurs in the first step, $X_i^{(s)}$ gets set randomly to one of two values: $\beta_i+\beta_j\frac{a_j}{a_i}$ or $\beta_i-(1-\beta_j)\frac{a_j}{a_i}$, with probability $1-\beta_j$ or $\beta_j$, respectively.  These values differ by exactly $\frac{a_j}{a_i} \ge\frac{a_{min}}{a_{max}}\ge\frac12$. 
Now, in the second step, case III only occurs if $X_i^{(s)}$ lies in the open interval $(\frac{a_k}{a_i}-\frac{a_k}{a_i}\beta_k, 1-\frac{a_k}{a_i}\beta_k)$. But the distance between any two numbers in this interval is strictly less than $1-\frac{a_k}{a_i}\le1-\frac{a_{min}} {a_{max}}\le\frac12$. Therefore, the two possible values of $X_i^{(s)}$ cannot both lie in the interval required for case III, so with probability at least $\min\{\beta_j,1-\beta_j\}$, the second step will be a case other than III. 
\end{proof} 

\smallskip \noindent \textbf{Important remark on notation:} In the following paragraph, by overloading notation, we fix the random permutation $\pi$ to be some arbitrary but fixed $\pi$. Several pieces of notation such as $\sigma, J_i$, and, most importantly, 
$\delta_k:=\prod_{i=0}^{\lfloor |J_k|/2\rfloor}(1-\alpha_{j_{k,2i}}\alpha_{j_{k,2i+1}})$, are functions of this $\pi$. All statements and proofs \emph{until the end of the proof of Lemma~\ref{lemma:sandwich1}}, are \emph{conditional on the random permutation equaling $\pi$} (this is sometimes stated explicitly, sometimes not). 

\smallskip
Lemma~\ref{lemma:pair-prob} implies that the probability of a variable's survival decays exponentially with the number of steps survived. Now, given a permutation $\pi\in S_n$, let $\sigma:[t]\to I$ be the bijection such that $\pi^{-1}(\sigma(1))<\pi^{-1}(\sigma(2))<\cdots<\pi^{-1}(\sigma(t))$. 
Let 
\[ J:=[n]\setminus I \] 
be the set of the indices not in $I$. Now partition $J$ into sequences $(J_0,J_1,\ldots,J_t)$, using elements of $I$ as dividers. Formally, for $k=0,\ldots,t$, let $J_k$ be the maximal sequence $(j_{k,1},j_{k,2},\ldots)$ satisfying $\pi^{-1}(\sigma(k))<\pi^{-1}(j_{k,1})<\pi^{-1}(j_{k,2})<\cdots<\pi^{-1}(\sigma(k+1))$, letting $\pi^{-1}(\sigma(0)):=0$ and $\pi^{-1}(\sigma(i+1)):=n+1$. Note that if $\sigma(k)$ and $\sigma(k+1)$ are directly adjacent in $\pi$, then $J_k$ will be empty, as seen in the example below. 
\[ I=\{2,3,8\}
\qquad\pi([n])=(\underbrace{12,11,5}_{J_0},\mathbf{3},\underbrace{4,1,9,6}_{J_1},\underset{J_2=\emptyset}{\mathbf{8},\mathbf{2}},\underbrace{7,10}_{J_3})
 \qquad  (\sigma(1),\sigma(2),\sigma(3))=(3,8,2)
\]

For $k\in[t-1]$, let $Z_k$ be the ``bad'' event that {\sc DepRound} co-rounds $X_{\sigma(k)}$ and $X_{\sigma(k+1)}$. For $Z_k$ to occur, it is necessary that $X_{\sigma(k)}$ be co-rounded with all variables inbetween as well. For example, in the above sequence, $Z_1$ means that $X_{\sigma(1)}=X_3$ must be co-rounded with $X_4, X_1, X_9, X_6$, (surviving each round), and finally $X_8$. The next lemma bounds $\Pr[Z_k]$ in terms of the set $J_k$.

\begin{lemma}\label{lemma:zbound}
Let $\alpha_i:=\min\{p_i,1-p_i\}$. If $\frac{a_{max}}{a_{min}}\le 2$, then $\forall k\in[t-1]$, we have $\Pr[Z_k]\le \delta_k$, where $\delta_k:=\prod_{i=1}^{\lfloor |J_k|/2\rfloor}(1-\alpha_{j_{k,2i-1}}\alpha_{j_{k,2i}})$.
\end{lemma}

\begin{proof}
Assume $|J_k|\ge2$ (else the lemma is trivially true).
Let $E_{0}$ be the event that $X_{\sigma(k)}$ is co-rounded with $X_{j_{k,1}}$ (i.e. is not fixed earlier in the algorithm).
For $\ell\in[|J_k|]$, let $E_\ell$ be the event that $X_{\sigma(k)}$ is corounded with $X_{j_{k,\ell}}$ \emph{and} survives. To apply Lemma 2.3, we first express the probability of $Z_k$ in terms of consecutive pairs of events.
\begin{align}
\Pr[Z_k]=\Pr[E_0\land E_1\land\ldots\land E_{|J_k|}]
&\le \Pr\left[E_0\land \left(\bigwedge_{i=1}^{\lfloor |J_k|/2\rfloor}(E_{2i-1}\land E_{2i})\right)\right] 
\nonumber\\&= Pr[E_0]\prod_{i=1}^{\lfloor |J_k|/2\rfloor}\Pr\left[ E_{2i-1}\land E_{2i}\,\Big\vert \bigwedge_{j=0}^{2i-2} E_j\right]
\nonumber\\&\le \prod_{i=1}^{\lfloor |J_k|/2\rfloor}\Pr\left[ E_{2i-1}\land E_{2i}\,\Big\vert \bigwedge_{j=0}^{2i-2} E_j\right].
\label{eq:pair_prod}
\end{align}


Note that $E_{\ell}$ implies $E_{\ell-1}\land E_{\ell-2}\land\ldots\land E_0$. So $\Pr\left[E_{2i-1}\land E_{2i}\mid \bigwedge_{i=1}^{2i-2} E_i\right]=\Pr\left[E_{2i-1}\land E_{2i}\mid E_{2i-2}\right]$.
Observe that event $E_{2i-2}$ is equivalent to the event that $X_{\sigma(k)}$ is co-rounded with $X_{j_{k,2i-1}}$. Also, observe that if $E_\ell$ occurs during step $s$ (for $\ell\in[|J_k|]$), then $s$ is the first step involving $X_{j_{k,\ell}}$, so the input value to $\simplify$ is $X_{j_{k,\ell}}^{(s-1)}=p_{j_{k,\ell}}$. These observations together with Lemma 2.3 imply that $\Pr\left[E_{2i-1}\land E_{2i}\mid E_{2i-2}\right]\le 1-\alpha_{j_{k,2i-1}}\alpha_{j_{k,2i}}$. Then (\ref{eq:pair_prod}) is bounded by $\delta_k$ as defined in the lemma.
\end{proof}

\subsubsection{Seldom co-rounded variables are near-independent}

The following lemmas show that if the probability of variables in $\{X_i\}_{i\in I}$ being co-rounded is low, then $\E[\prod_{i\in I} Y_i]\approx \prod_{i\in I} q_i$. For notational convenience, define $\delta_t:=0$.

\begin{lemma}\label{lemma:sandwich1} Let $I_k:=\{\sigma(k),\sigma(k+1),\ldots,\sigma(t)\}$.
Let $\mathcal{E}_k$ denote a set of events which consists of exactly one of $Z_i$ or $(\bar Z_i\land Y_{\sigma(i)}=y_i)$ for each $i=1\ldots k$, where $y_i$ is some attainable value of $Y_{\sigma(i)}$. Then, conditioned on a fixed permutation $\pi$, the following holds for all $k\in[t]$: 
\begin{equation} 
\prod_{i\in I_k} \max\left\{q_i - \delta_{\sigma^{-1}(i)},0\right\}
\le \E\bigg[\prod_{i\in I_k} Y_i \,\Big\rvert\, \mathcal{E}_{k-1}
\bigg]
\le \prod_{i\in I_k} \left(q_i + \delta_{\sigma^{-1}(i)}\right).
\label{eq:sandwich-ih}\end{equation}
\end{lemma}
\begin{proof}
Recall $\pi\in S_n$ is a fixed permutation; all probabilities and expectations in this proof are conditioned on $\pi$. This proof formalizes the idea that if $Z_k$ doesn't occur, then $X_{\sigma(k)}$ and $X_{\sigma(k+1)}$ remain independent. If it does occur, the effect on the expected value is limited by $\delta_k$.

After step $s$ of {\sc DepRound}, we may consider the remainder of the algorithm as simply a recursive call on vector $X^{(s)}$ (using the same permutation $\pi$). Let $D_k$ be the first such call where $X_{\sigma(k)}$ is one of the two left-most fractional variables to be co-rounded. Then there is at most one fractional variable to the left of $X_{\sigma(k)}$, say $X_{i_0}$, and all variables to the right still have their initial values from $P$.

The key observation is that while events in $\mathcal{E}_{k-1}$ do affect the identity and initial value of $X_{i_0}$, they do not further influence the outcome of $D_k$. The only way $D_k$ could be further influenced is if $\mathcal{E}_{k-1}$ contains the event $Y_{\sigma(j)}=y_j$, where $X_{i_0}$ is with some probability the variable $X_{\sigma(j)}$. However, for each $j=1\ldots k-1$, $\mathcal{E}_{k-1}$ either contains $\bar Z_j$ (which means $X_{\sigma(j)}$ was fixed earlier and cannot be $X_{i_0}$), or it lacks $Y_{\sigma(j)}=y_j$.

This means that all the properties of {\sc DepRound} shown so far (which hold for a fixed $\pi$) still hold for $D_k$ when conditioned on $\mathcal{E}_{k-1}$. Namely, we have $E[X_i\mid \mathcal{E}_{k-1}]=p_i$ (for all $X_i$ to the right of and including $X_{\sigma(k)}$), $\Pr[Z_k|\mathcal{E}_{k-1}]\le \delta_k$, and -- as we will claim by induction --  (\ref{eq:sandwich-ih}) for $I_k$.
The bounds derived below handle the problematic compound event $Z_k\land Y_{\sigma(k)}=y_k$ explicitly by assuming that when $Z_k$ occurs, $Y_{\sigma(k)}$ always attains its worst-case value (1 for the upper bound, or 0 for the lower bound).

As a base case, consider the singleton set $I_t=\{\sigma(t)\}$. As just described, we have $\E[X_{\sigma(t)}\mid\mathcal{E}_{k-1}]=p_{\sigma(t)}$ , so $\E[Y_{\sigma(t)}\mid\mathcal{E}_{k-1}]=q_{\sigma(t)}$, and (\ref{eq:sandwich-ih}) follows from $\delta_t=0$.
We now proceed by induction on $k$, counting backward from $t$. Let $W_k:=\prod_{i\in{I_k}} Y_i=\prod_{j=k}^t Y_{\sigma(j)}$. Let $\bar Z_k$ be the complement of event $Z_k$.  For some $k<t$, assume that (\ref{eq:sandwich-ih}) holds for $D_{k+1}$ with set $I_{k+1}$. Then, using the independence properties just mentioned, we have that $\E[W_k\mid\mathcal{E}_{k-1}]$ is
\begin{align*}
=&\E[Y_{\sigma(k)} W_{k+1}\mid\mathcal{E}_{k-1}] 
\\=& \E[Y_{\sigma(k)} W_{k+1}\mid\bar Z_k\land\mathcal{E}_{k-1}]\Pr[\bar Z_k\mid\mathcal{E}_{k-1}]
	+\E[Y_{\sigma(k)} W_{k+1}\mid Z_k\land\mathcal{E}_{k-1}]\Pr[Z_k\mid\mathcal{E}_{k-1}] 
\\\le& \E[Y_{\sigma(k)} W_{k+1}\mid\bar Z_k\land\mathcal{E}_{k-1}]\Pr[\bar Z_k\mid\mathcal{E}_{k-1}]
	+\E[W_{k+1}\mid Z_k\land\mathcal{E}_{k-1}]\delta_k 
\\ =&\textstyle\sum_{y_k}\left(y_k\E[W_{k+1}\mid Y_{\sigma(k)}=y_k\land \bar Z_k\land 
\mathcal{E}_{k-1}]
	\Pr[Y_{\sigma(k)}=y_k\mid\bar Z_k\land \mathcal{E}_{k-1}]
	\Pr[\bar Z_k\mid\mathcal{E}_{k-1}]\right)
	+ \E[W_{k+1}\mid\mathcal{E}_k']\delta_k 
\\ =&\textstyle\sum_{y_k}\left(y_k\E[W_{k+1}\mid \mathcal{E}_k'']
	\Pr[Y_{\sigma(k)}=y_k\mid\bar Z_k\land\mathcal{E}_{k-1}]
	\Pr[\bar Z_k\mid\mathcal{E}_{k-1}]\right)
	+ \E[W_{k+1}\mid\mathcal{E}_k']\delta_k 
\\ \le& \textstyle\prod_{i\in I_{k+1}}(q_i+\delta_{\sigma^{-1}(i)})
	\big(\sum_{y_k}(y_k\Pr[Y_{\sigma(k)}=y_k \land\bar Z_k\mid \mathcal{E}_{k-1}]
	) +\delta_k \big) 
\\ \le& \textstyle\prod_{i\in I_{k+1}}(q_i+\delta_{\sigma^{-1}(i)})
	\big(\sum_{y_k}(y_k\Pr[Y_{\sigma(k)}=y_k \mid \mathcal{E}_{k-1}]
	) +\delta_k \big) 
\\ =& \textstyle\prod_{i\in I_{k+1}}(q_i+\delta_{\sigma^{-1}(i)})
	\big(\E[Y_{\sigma(k)}\mid \mathcal{E}_{k-1}] +\delta_k\big) 
\\ =& \textstyle\prod_{i\in I_{k+1}}(q_i+\delta_{\sigma^{-1}(i)})
	\big(q_{\sigma(k)} + \delta_{\sigma^{-1}(\sigma(k))}\big) 
= \textstyle\prod_{i\in I_{k}}(q_i+\delta_{\sigma^{-1}(i)}).
\end{align*}
Similarly, we have that $\E[W_k\mid\mathcal{E}_{k-1}]$ is
\begin{align*}
=&\E[Y_{\sigma(k)} W_{k+1}\mid\mathcal{E}_{k-1}] 
\\ \ge& \E\left[Y_{\sigma(k)} W_{k+1}\mid \bar Z_k\land\mathcal{E}_{k-1}\right] 
	\Pr[\bar Z_k\mid\mathcal{E}_{k-1}] 
\\ =& \textstyle\sum_{y_k} y_k \E\left[W_{k+1}\mid Y_{\sigma(k)}=y_k\land\bar Z_k\land\mathcal{E}_{k-1}\right]
	\Pr[Y_{\sigma(k)}=y_k\mid \bar Z_k\land \mathcal{E}_{k-1}] 
	\Pr[\bar Z_k\mid\mathcal{E}_{k-1}] 
\\ =& \textstyle\sum_{y_k} y_k \E\left[W_{k+1}\mid \mathcal{E}_{k}'\right] 
	\Pr[Y_{\sigma(k)}=y_k\mid \bar Z_k\land\mathcal{E}_{k-1}]
	\Pr[\bar Z_k\mid\mathcal{E}_{k-1}] 
\\ \ge& \textstyle\prod_{i\in I_{k+1}}(q_i-\delta_{\sigma^{-1}(i)})
	\E[Y_{\sigma(k)} \mid \bar Z_k\land\mathcal{E}_{k-1}]\Pr[\bar Z_k\mid\mathcal{E}_{k-1}] 
\\ =& \textstyle\prod_{i\in I_{k+1}}(q_i-\delta_{\sigma^{-1}(i)})
	\big(\E[Y_{\sigma(k)}\mid\mathcal{E}_{k-1}]-\E[Y_{\sigma(k)}\mid  Z_k\land\mathcal{E}_{k-1}]\Pr[ Z_k\mid\mathcal{E}_{k-1}] \,\big)
\\ \ge& \textstyle\prod_{i\in I_{k+1}}(q_i-\delta_{\sigma^{-1}(i)})
	\big(q_{\sigma(k)}-\delta_k\,\big) 
=\textstyle\prod_{i\in I_{k}}(q_i-\delta_{\sigma^{-1}(i)}).
\end{align*}
But also $\E[W_{k}|\mathcal{E}_{k-1}]\ge 0\cdot\prod_{i\in I_{k+1}}(q_i-\delta_{\sigma^{-1}(i)})$ so we use the better of the two lower bounds.
\end{proof} 

\smallskip \noindent \textbf{Remark.} From now on, we will no longer take $\pi$ as fixed, and hence the $\delta_k$ (which are functions of the random permutation) will be viewed as random variables.

\smallskip
\begin{lemma} \label{lemma:sandwich}
If $\frac{a_{max}}{a_{min}}\le2$, we have 
\[ \prod_{i\in I} q_i \cdot\E\left[\prod_{k=1}^{t-1} \max\left\{1 - \frac{\delta_k}{q_{\sigma(k)}},0\right\}\right]
\le \E\left[\prod_{i\in I} Y_i \right]
\le \prod_{i\in I} q_i \cdot\E\left[\prod_{k=1}^{t-1} \left(1 + \frac{\delta_k}{q_{\sigma(k)}}\right)\right]
.\] 
\end{lemma}
\begin{proof}
Apply Lemma \ref{lemma:sandwich1} with $k=1$ and $\mathcal{E}=\emptyset$. Recall $\delta_t:=0$. Take the expectation over all permutations $\pi$, and then factor out the constant $\prod_{i\in I} q_i$.
\end{proof}
\subsubsection{Small subsets are spread out}

The following lemma gives a very useful combinatorial characterization of the distribution of $\{J_k\}$.
\begin{lemma}\label{lemma:bijection}
Let $g=(g_1,\dots,g_{t+1})$ be a sequence of nonnegative integers which sum to $n-t$, picked uniformly at random from all such possible sequences. Then the distribution of $g$ is equal to the distribution of $(|J_0|,\dots,|J_t|)$. Both distributions are symmetric.
\end{lemma}
\begin{proof}Consider the mapping $\Phi_I:S_n\to\{0,1\}^n$ from permutations on $[n]$ to binary strings of length $n$, in which we replace each index in $I$ with a 1, and the rest with a 0. Also define $\Theta$ to be the following standard combinatorial bijection from binary strings to arrangements of balls in boxes: given a binary string $s$, first add a 1 to the beginning and end of the string; then, viewing the space between each nearest pair of 1's as a `box', and the zeros between each pair as `balls' in that box, let $\Theta(s)$ be the sequence which counts the number of balls in each box, from left to right. For an arbitrary permutation $\pi$, and the corresponding sets $\{J_k\}$, we see that $|J_k|$ corresponds to the number of zeros between the $k$'th and the $(k+1)$'th 1 in $\Phi_I(\pi)$, and thus to the number of balls in the corresponding box in $(\Theta\circ\Phi_I)(\pi)$. Therefore we have that $(\Theta\circ\Phi_I)(\pi)=(|J_0|,\ldots,|J_t|)$.
\[I=\{2,3,8\}\quad\pi([n])=(\underbrace{12,11,5}_{J_0},\mathbf{3},\underbrace{4,1,9,6}_{J_1},\underset{J_2=\emptyset}{\mathbf{8},\mathbf{2}},\underbrace{7,10}_{J_3})
\;\implies
\begin{array}{rl}
	\Phi_I(\pi)=&000100001100 \\
	(\Theta\circ\Phi_I)(\pi)=&(3,4,0,2)
\end{array}\] 

Now recall that {\sc DepRound} chooses a uniformly random permutation $\pi\in S_n$. Notice that $\Phi_I(\pi)$ only maps to binary strings of length $n$ with exactly $|I|=t$ ones. Furthermore, for each such binary string, there are exactly $t!(n-t)!$ permutations which map to it. Thus, $\Phi_I(\pi)$ is uniformly distributed over all $\binom{n}{t}$ such binary strings. $\Theta$ provides an exact bijection between binary strings of length $n$ with $t$ ones, and sequences $g$ as defined in the lemma. This implies that $(\Theta\circ\Phi_I)(\pi)$ -- and thus $(|J_0|,\ldots,|J_t|)$ -- is uniformly distributed over all such possible sequences $g$. 

Furthermore, by definition of $g$, all permutations of a sequence $g$ would be equally likely. Therefore the distribution of $g$, and thus $(|J_0|,\ldots,|J_t|)$, is symmetric.
\end{proof}

\smallskip \noindent \textbf{Remark.} Note that the above distribution over balls-in-boxes is such that each possible arrangement is equally likely. This is not to be confused with distributions obtained by randomly and independently throwing the balls into the boxes. 

\begin{lemma}\label{lemma:exjc}
 Consider a subset $C\subseteq[t]$ of size $c$, and let $J_C:=\bigcup_{k\in C}J_k$. Then for any constant $0<x<1$,
\[\E\left[x^{|J_C|}\right]\le\left(\frac{t}{n(1-x)}\right)^c .\]
\end{lemma}
\begin{proof}
From Lemma \ref{lemma:bijection} the distribution of $(|J_0|,|J_1|,\ldots |J_t|)$ is symmetric. Thus, when considering the distribution of a function of the sizes $\{|J_k|\}_{k\in C}$, we may w.l.o.g.\ assume that $J_C=J_0\cup J_1\cup\ldots\cup J_{c-1}$. Notice since $\{J_k\}$ are all disjoint, we have $|J_C|=\sum_{k\in C}|J_k|$; also, $|J_C|\le|J|=n-t$.
\begin{equation}\label{eq:ejc}
\E\left[x^{|J_C|}\right]
=\sum_{m=0}^{n-t} \Pr[|J_C|=m] x^m
=\sum_{m=0}^{n-t} \Pr\left[\sum_{k=0}^{c-1}|J_k|=m\right] x^m.
\end{equation}
Now for a quick exercise in counting. From the previous proof, $\Phi_I(\pi)$ maps permutations uniformly to $n$-digit binary strings with $t$ 1's. For a given permutation $\pi$, we observe that $\sum_{k=0}^{c-1} |J_k|=m$ iff the binary string $\Phi_I(\pi)$ has exactly $m$ zeros before the $c$'th 1 (i.e., there are $m$ total balls in the first $c$ boxes). How many of the $\binom{n}{t}$ possible strings have this property? It is the number of ways to put $(c-1)$ 1's in the first $(m+c-1)$ digits, a $1$ in the $(m+c)$'th digit, and $(t-c)$ 1's in the remaining $(n-m-c)$ digits. Thus,
\begin{equation}\label{eq:prjkm}
\Pr\left[\sum_{k=1}^{c} |J_k|=m\right] 
= \frac{\binom{m+c-1}{c-1}\binom{n-m-c}{t-c}}{\binom{n}{t}}
\le \frac{\binom{m+c-1}{c-1}\binom{n-c}{t-c}}{\binom{n}{t}}
= \binom{m+c-1}{c-1}\cdot\frac{t^{\underline c}}{n^{\underline c}}
\le \binom{m+c-1}{c-1} \left(\frac t n\right)^c
\!\!\!,\end{equation}
where $n^{\underline c}:=n\cdot(n-1)\cdots(n-c+1)$ denotes the falling factorial. Now we combine (\ref{eq:ejc}) and (\ref{eq:prjkm}), and relax the bound by allowing $m$ to go up to infinity. The resulting series converges when $0<x<1$.
\begin{align*}
\E\left[x^{|J_C|}\right]
\le \left(\frac{t}{n}\right)^c\sum_{m=0}^{\infty}\binom{m+c-1}{c-1}x^m
=  \left(\frac{t}{n}\right)^c\frac{1}{(1-x)^c}
= \left(\frac{t}{n(1-x)}\right)^c.
\end{align*}
A quick proof of the series' convergence is to start with $\sum_{m=0}^{\infty}x^m=1/(1-x)$, and take the $(c-1)$'th derivative of both sides, with respect to $x$.
\end{proof}

\begin{lemma}\label{lemma:exprod}
Let $\delta_k:=\prod_{i=0}^{\lfloor |J_k|/2\rfloor}(1-\alpha_{j_{k,2i}}\alpha_{j_{k,2i+1}})$, and $\alpha:=\min_j\{\alpha_j\}$. Let $C\subseteq[t]$ of size $c$. If $\frac{a_{max}}{a_{min}}\le2$, 
\[ \E\left[ \prod_{k\in C} \frac{\delta_k}{q_{\sigma(k)}} \right] 
\le \left(\frac{16}{7}\cdot\frac{t}{n\alpha^3}\right)^c . \]
\end{lemma}

\begin{proof} First, recall by definition that $q_i$ is either $p_i$ or $1-p_i$, so $q_i\ge \min\{p_i,1-p_i\} =\alpha_i\ge \alpha$. Then
\begin{align} 
\E\hspace{-1pt}\left[ \prod_{k\in C} \frac{\delta_k}{q_{\sigma(k)}} \right] 
&\le \frac{1}{\alpha^c}\E\left[ \prod_{k\in C} \delta_k \right] 
=\frac{1}{\alpha^c} \E\left[ \prod_{k\in C} \prod_{i=1}^{\lfloor |J_k|/2\rfloor}(1-\alpha_{j_{k,2i}}\alpha_{j_{k,2i+1}})\right] 
\le \frac{1}{\alpha^c}\E\left[ \prod_{k\in C} (1-\alpha^2)^{(|J_k|-1)/2}\right] \label{eq:am-gm}
\\&= \frac{1}{\alpha^c}\E\left[(1-\alpha^2)^{(|J_C|-c)/2}\right]
\le \frac{1}{\alpha^c}\E\left[\left(1-{\textstyle\frac{\alpha^2}{2}}\right)^{|J_C|-c}\right] \label{step:sqrt-bound}
\\&\le \frac{1}{\alpha^c}\cdot \left(1-\frac{\alpha^2}{2}\right)^{-c}\left(\frac{t}{n(\alpha^2/2)}\right)^c
\le \left(1-\frac{(1/2)^2}{2}\right)^{-c} \left(\frac{2t}{n\alpha^3}\right)^c
=\left(\frac{16}{7}\cdot\frac{t}{n\alpha^3}\right)^c . \label{step:lemma-exjc}
\end{align}
In the first line we used $\lfloor x/2\rfloor\ge (x-1)/2$. In (\ref{step:sqrt-bound}) we used $\sqrt{1-x^2}\le\sqrt{1-x^2+x^4/4}=1-x^2/2$. In (\ref{step:lemma-exjc}) we applied Lemma \ref{lemma:exjc} and then used $\alpha\le1/2$.
\end{proof}
Now we can complete the bound given in Lemma \ref{lemma:sandwich}. The upper bound follows by expanding the binomial, bounding the expected value of each term, and then refactoring. The lower bound follows by the Weierstrass product inequality.
\begin{align}
\E\left[\prod_{k=1}^{t-1} \left(1 + \frac{\delta_k}{q_{\sigma(k)}}\right)\right] 
 &=1+\sum_{1\le i<t}\E\left[\frac{\delta_i}{q_{\sigma(i)}}\right]
	+\sum_{1\le i<j<t}\E\left[\frac{\delta_i\delta_j}{q_{\sigma(i)}q_{\sigma(j)}}\right]
	+\cdots+\E\left[\frac{\delta_1\cdots \delta_{t-1}}{q_{\sigma(1)}\cdots q_{\sigma(t-1)}}\right]
\nonumber\\ &\le1+\!\sum_{1\le i<t}\left(\frac{16t}{7 n\alpha^3}\right)
	+\!\!\sum_{1\le i<j<t} \left(\frac{16t}{7 n\alpha^3}\right)^2 
	\!\!\!+\cdots+\left(\frac{16t}{7 n\alpha^3}\right)^{\!t-1}
\!\!\!\!= \left(1+\frac{16t}{7 n\alpha^3}\right)^{\!t-1}.
\label{eq:exp-up}\end{align}
\begin{align}
\E\left[\prod_{k=1}^{t-1} \max\left\{1 - \frac{\delta_k}{q_{\sigma(k)}},0\right\} \right]
&= \E\left[\prod_{k=1}^{t-1} \left(1-\min\left\{\frac{\delta_k}{q_{\sigma(k)}},1\right\}\right)\right]
\ge \E\left[1-\sum_{k=1}^{t-1} \min\left\{\frac{\delta_k}{q_{\sigma(k)}},1\right\}\right]
\nonumber\\ &\ge 1-\sum_{k=1}^{t-1} \E\left[\frac{\delta_k}{q_{\sigma(k)}}\right]
\ge 1-\sum_{k=1}^{t-1} \frac{16t}{7n\alpha^3}
= 1-\frac{16t(t-1)}{7n\alpha^3}.
\label{eq:exp-down}\end{align}

Thus we are led to our main theorem on dependent rounding (which in turn is improved upon, with further work, in Section~\ref{sec:dep-round-special}): 

\begin{theorem}\label{thm:limited-dep-general}
Let $(X_1,\ldots,X_n)$ be the vector returned by running {\sc DepRound} with probabilities $(p_1,\ldots,\allowbreak p_n)$ and positive weights $(a_1,\ldots,a_n)$. Let $I^+$ and $I^-$ be disjoint subsets of $[n]$. Define $\alpha:=\min_i\{p_i,1-p_i\}$, $I:=I^+\cup I^-$, $t = |I|$, and $\lambda:=\displaystyle\prod_{i\in I^+}p_i\prod_{i\in I^-}(1-p_i)$. Then if $\displaystyle \max_{i,j}\left\{\frac{a_i}{a_j}\right\}\le 2$, we have
\[ \left(1-\frac{16t(t-1)}{7n\alpha^3}\right) \lambda 
\le \E\left[\prod_{i\in I^+} X_i \prod_{i \in I^-}(1-X_i) \right] 
\le\left(1+\frac{16t}{7n\alpha^3}\right)^{t-1} \lambda.\]
\end{theorem}
\begin{proof}
For all $i\in I^+$, set $b_i=1$; for all $i\in I^-$, set $b_i=0$. Then apply (\ref{eq:exp-up}) and (\ref{eq:exp-down}) to Lemma \ref{lemma:sandwich}. The theorem follows by recognizing that
\begin{equation*}
 \prod_{i\in I^+} X_i \prod_{i \in I^-}(1-X_i) = \prod_{i\in I} Y_i \qquad\text{and}\qquad
	\prod_{i\in I^+}p_i\prod_{i\in I^-}(1-p_i) = \prod_{i\in I}q_i
.\end{equation*}
\end{proof}
Note that $\left(1+\frac{16t}{7n\alpha^3}\right)^{t-1}\le\exp\left(\frac{16t^2}{7n\alpha^3}\right)$. Thus we see that Theorem \ref{thm:limited-dep-general} allows us to bound the dependence among groups of variables as large as $O(\sqrt{n})$ when $\alpha = \Theta(1)$.
\subsection{Improvements and Special Cases}
\label{sec:dep-round-special}
In this section we present several refinements of Theorem \ref{thm:limited-dep-general}. The proofs all follow the same outline as that of the main result; we describe only the places where they differ.  We reuse the same definitions unless stated otherwise.

In our $k$-median application we will have that all $p_i$ are uniform. In this case, if the maximum ratio of weights is sufficiently small, we can tighten the bound to show a weaker dependency on $\alpha$.
\begin{theorem}\label{cor:dep-uniform-p}
Suppose $p_1=p_2=\cdots=p_n=p$ and $\alpha = \min\{p, 1 - p\}$. Then if $\frac{a_{max}}{a_{min}}\le 1+\alpha$, we have  
\[ \left(1-\frac{8t(t-1)}{3n\alpha^2}\right) \lambda
\le \E\left[\prod_{i\in I^+} X_i \prod_{i \in I^-}(1-X_i) \right]
\le\left(1+\frac{8t}{3n\alpha^2}\right)^{t-1} \lambda  .\]
\end{theorem}

\begin{proof}
The improvement comes from strengthening the result of Lemma \ref{lemma:pair-prob}: Suppose $X_i$ is co-rounded with variable $X_j$ in step $s$ and then (if it survives) variable $X_k$ in step $s+1$, where $X_j^{(s-1)}=X_k^{(s)}=p$. Then we can show $X_i$ will be fixed in one of these two steps with probability at least $\alpha$.


First assume that during both steps Case III occurs. 
By requirements for Case III, we have $\frac{a_j}{a_i}(1-p)<X_i^{(s-1)}<1-\frac{a_j}{a_i}p$ and $\frac{a_k}{a_i}(1-p)<X_i^{(s)}<1-\frac{a_k}{a_i}p$. 
Suppose $X_j$ is fixed to 0 in step $s$. Then 
$$X_i^{(s)}=X_i^{(s-1)}+p\frac{a_j}{a_i}>\frac{a_j}{a_i}(1-p)+p\frac{a_j}{a_i}=\frac{a_j}{a_i}\ge\frac{1}{1+\alpha}=1-\frac{1}{1+\alpha}\cdot\alpha\ge 1-\frac{a_k}{a_i} p>X_i^{(s)}.$$ 
Else suppose $X_j$ is fixed to 1. Then
$$X_i^{(s)}=X_i^{(s-1)}-(1-p)\frac{a_j}{a_i}<1-\frac{a_j}{a_i}p-(1-p)\frac{a_j}{a_i}=1-\frac{a_j}{a_i}<1-\frac{1}{1-\alpha}
=\frac{1}{1-\alpha}\cdot\alpha<\frac{a_k}{a_i}(1-p)<X_i^{(s)}.$$

In either outcome, we have a contradiction. Therefore in at least one of the two steps, a case other than III must occur. As shown in the proof of Lemma \ref{lemma:pair-prob}, in the other 3 cases $X_i$ will be fixed with probability at least $\min\{p,1-p\}=\alpha$.

This stronger bound carries through the remaining lemmas in a straightforward way. Following the proof for Lemma \ref{lemma:zbound}, starting from (\ref{eq:pair_prod}), we get
$$\Pr[Z_k]
\le\prod_{i=1}^{\lfloor |J_k|/2\rfloor}\Pr\left[ E_{2i-1}\land E_{2i}\,\Big\vert E_{2i-2}\right]
\le \prod_{i=1}^{\lfloor |J_k|/2\rfloor} (1-\alpha)=(1-\alpha)^{\lfloor |J_k|/2\rfloor}.
$$
So Lemmas \ref{lemma:zbound}, \ref{lemma:sandwich1} and \ref{lemma:sandwich} will now hold with the new definition $\delta_k:=(1-\alpha)^{\lfloor |J_k|/2\rfloor}$. Then in Lemma \ref{lemma:exprod}, we get
\begin{align*} 
\E\hspace{-1pt}\left[ \prod_{k\in C} \frac{\delta_k}{q_{\sigma(k)}} \right] 
&\le\frac{1}{\alpha^c} \E\left[ \prod_{k\in C} (1-\alpha)^{\lfloor |J_k|/2\rfloor}\right] 
\le \frac{1}{\alpha^c}\E\left[ \prod_{k\in C} (1-\alpha)^{(|J_k|-1)/2}\right] 
\\&= \frac{1}{\alpha^c}\E\left[(1-\alpha)^{(|J_C|-c)/2}\right]
\le \frac{1}{\alpha^c}\E\left[\left(1-{\textstyle\frac{\alpha}{2}}\right)^{|J_C|-c}\right]
\\&\le \frac{1}{\alpha^c}\cdot \left(1-\frac{\alpha}{2}\right)^{-c}\left(\frac{t}{n(\alpha/2)}\right)^c
\le \left(1-\frac{(1/2)}{2}\right)^{-c} \left(\frac{2t}{n\alpha^2}\right)^c
=\left(\frac{8}{3}\cdot\frac{t}{n\alpha^2}\right)^c . \label{step:lemma-exjc}
\end{align*}
The theorem follows as before.
\end{proof}

In the unweighted case (where all $a_i=1$), we can similarly tighten the bound. We can also refine the bound to be in terms of a sort of average of the probabilities instead of just the most extreme. 
\begin{theorem}
\label{thm:limited-dep-unweighted}
Let $\X:=(X_1,\ldots,X_n)$ be the vector returned by running {\sc DepRound} with probabilities $(p_1,\ldots,p_n)$ and unit weights $(1,\ldots,1)$. Let $\alpha_i=\min\{p_i,1-p_i\}$. Let $I^+$ and $I^-$ be disjoint subsets of $[n]$.  Let $q_i=p_i$ for $i\in I^+$, and let $q_i=1-p_i$ for $i\in I^-$; let
$I = I^+ \cup I^-$. Define
\[
J = ([n] \setminus I), ~~~\lambda:=\displaystyle\prod_{i\in I^+}p_i\prod_{i\in I^-}(1-p_i), ~~~
\alphavg:=\frac{1}{|J|}\sum_{j\in J}\alpha_j, ~~~
\text{and}~~ \frac1{\qavg}:=\frac{1}{|I|}\sum_{i\in I} \frac{1}{q_i}
.\] 
Then,
\[ \left(1-\frac{t(t-1)}{n\qavg\alphavg}\right) \lambda
\le \E\left[\prod_{i\in I^+} X_i \prod_{i \in I^-}(1-X_i) \right]
\le\left(1+\frac{t}{n\qavg\alphavg}\right)^{t-1} \lambda  .\]
Furthermore, if $\sum_i p_i$ is an integer, then $\X$ has no fractional elements.
\end{theorem}

\begin{proof}
Uniform weights allow us to strengthen Lemma \ref{lemma:pair-prob} even further; in particular, we no longer need to consider pairs of steps. Suppose $X_i$ is co-rounded with $X_j$ during step $s$. Since all $a_i=1$, Cases II and III cannot occur. Thus, $X_i$ will be fixed with probability at least $\min\{X_j^{(s-1)},1-X_j^{(s-1)}\}=\alpha_j$ (as in proof of Lemma \ref{lemma:pair-prob}).

If we follow the proof of Lemma \ref{lemma:zbound}, but without splitting events into pairs, we can show
$$\Pr[Z_k]
\le\prod_{i=1}^{|J_k|}\Pr\left[ E_i\,\Big\vert E_{i-1}\right]
\le \prod_{i=1}^{|J_k|} (1-\alpha_{j_{k,i}})=\prod_{j\in J_k}(1-\alpha_j).
$$
So Lemmas \ref{lemma:zbound}, \ref{lemma:sandwich1} and \ref{lemma:sandwich} will now hold with the new definition $\delta_k:=\prod_{j\in J_k}(1-\alpha_j)$. Now (as in Lemma \ref{lemma:exprod}), we wish to upper bound $\E[\prod_{k\in C} \frac{\delta_k}{q_{\sigma(k)}}]$. Recall the expectation here is conditioned on the random permutation $\pi$. We may decompose $\pi$ into 3 independent components. First, recall $\Phi_I(\pi)=:\phi$ is the binary string corresponding to $\pi$ with $t$ 1's representing the locations of indices in $I$. Second, let $\pi_I\in S_t$ be the permutation representing the ordering of $I$ over the 1's in $\phi$. Third, let $\pi_J\in S_{n-t}$ be the permutation representing the ordering of $J$ over the 0's in $\phi$. Then $\pi$ is uniquely defined by the tuple ($\phi$, $\pi_I$, $\pi_J$) and vice versa. So we can think of $\pi$ as being generated by choosing each element of the tuple uniformly at random. 
Thus, the value of $q_{\sigma(k)}$ depends only on $\pi_I$; the sizes $\{|J_k|\}$ depend only on $\phi$; and the elements of $\{J_k\}$ (conditioned on a particular set of sizes) depend only on $\pi_J$. This shows that some of the variables are independent, so we may separate the terms. Here we are explicit over which random variable we take each expectation:
\begin{align} 
\E_\pi\hspace{-2pt}\bigg[ \prod_{k\in C} \frac{\delta_k}{q_{\sigma(k)}} \bigg] 
&= \E_{\pi_I}\hspace{-2pt}\bigg[\prod_{k\in C}\frac{1}{q_{\sigma(k)}}\bigg] 
\E_{\pi_J,\phi}\hspace{-2pt}\bigg[ \prod_{k\in C} \prod_{j\in J_k}(1-\alpha_j)\bigg] \nonumber
\\&= \E_{\pi_I}\hspace{-2pt}\bigg[\prod_{k\in C}\frac{1}{q_{\sigma(k)}}\bigg] 
\sum_{\phi}\Pr[\phi]\cdot\E_{\pi_J}\hspace{-2pt}\bigg[ \prod_{j\in J_C}(1-\alpha_j)\Big\vert \phi\bigg].\label{eq:exp_decompose} 
\end{align}
The following lemma is basically a restatement of Maclaurin's inequality for symmetric polynomials:
\begin{lemma}\label{lemma:maclaurin}
Given a vector of positive reals $\mathbf{x}=x_1,x_2,\ldots,x_n$, with average value $\bar x$, let $S\subseteq[n]$ be a subset chosen uniformly at random from all such subsets of size $k$. Then $\E_S[\prod_{i\in S} x_i]\le {\bar x}^k$.
\end{lemma}
The first expectation in (\ref{eq:exp_decompose}) is over a product of $c$ random terms from $\{1/q_j\}_{j\in I}$. The second expectation is a product over $|J_C|$ (which as a function of $\phi$ is fixed for each term) random terms from $\{1-\alpha_j\}_{j\in J}$. So both expectations may be bounded by Lemma \ref{lemma:maclaurin}:
\begin{align*} 
\E_\pi\hspace{-2pt}\bigg[ \prod_{k\in C} \frac{\delta_k}{q_{\sigma(k)}} \bigg] 
&\le \frac{1}{\hat q^c}
\sum_{\phi}\Pr[\phi]\cdot (1-\hat\alpha)^{|J_C(\phi)|}
=\frac{1}{\hat q^c}\E_\phi\big[(1-\hat\alpha)^{|J_C|}\big]
\le\frac{1}{\hat q^c}\left(\frac{t}{n\hat\alpha}\right)^c
=\left(\frac{t}{n\hat q\hat\alpha}\right)^c.
\end{align*}
The theorem follows as before.
\end{proof}

\subsubsection{An alternative lower bound}
All the lower bounds given thus far become negative for $t$ larger than $O(\sqrt{n})$. We now derive an alternative lower bound which remains positive even for larger values of $t$. We will do this for the uniform weight case for simplicity, but it may be adapted in a straightforward manner to the weighted case. 

\begin{theorem}\label{thm:alt-lower-bound}
Suppose $a_1=a_2=\cdots=a_n=1$. Let $d$ be an integer which satisfies $(1-\alpha)^d\le \alpha$ and $d\le (n-t)/t$. Then
\[ \left(1-\frac{td}{n-t}\right)^t \left(1 - \frac{ (1-\alpha)^d}{\alpha}\right)^{t-1} \lambda \le \E\left[\prod_{i\in I^+} X_i \prod_{i \in I^-}(1-X_i) \right]. \]
\end{theorem}
\begin{proof}
Start with the lower bound given by Lemma \ref{lemma:sandwich}. As shown in the proof of Theorem \ref{thm:limited-dep-unweighted}, if all $a_i=1$, we may use $\delta_k:= \prod_{i\in J_k}(1-\alpha_i)\le (1-\alpha)^{|J_k|}$. To lower bound the expression, we will focus on the event that sets $J_1,\ldots,J_{t-1}$ all have at least $d$ elements, and use the trivial bound of 0 if this event does not occur. This event is useful because it implies that $\{X_i\}_{i\in I}$ are all far away in $\pi(X)$.
\begin{align*}
\E\left[\prod_{k=1}^{t-1} \max\left\{1 - \frac{\delta_k}{q_{\sigma(k)}},0\right\} \right]
\ge \E\left[\max\left\{1 - \frac{ (1-\alpha)^{|J_k|}}{\alpha},0\right\}^{t-1} \right]
\ge \Pr\left[\bigwedge_{k=1}^{t-1} |J_k|\ge d\right] 
\left(1 - \frac{ (1-\alpha)^d}{\alpha}\right)^{t-1}.
\end{align*}
To calculate this probability, recall that in Lemma \ref{lemma:bijection}, we showed that the distribution of $(|J_i|,\ldots,|J_t|)$ is equivalent to the uniform distribution over unique arrangements of $n-t$ identical balls into $t+1$ boxes. Note there are $\binom{n}{t}$ such arrangements. How many of these arrangements have at least $d$ balls in the middle $t-1$ boxes? To count these arrangements, we suppose that there are already exactly $d$ balls in each of the middle $t-1$ boxes and then count how many ways there are to add the remaining $n-t-(t-1)d$ balls to $t+1$ boxes, which is $\binom{n-(t-1)d}{t}$. So,
\begin{align*}
\Pr\left[\bigwedge_{k=1}^{t-1} |J_k|\ge d\right] 
= \frac{\binom{n-(t-1)d}{t}}{\binom{n}{t}}
> \left(\frac{n-(t-1)d-t}{n-t}\right)^t
> \left(1-\frac{td}{n-t}\right)^t.
\end{align*}
\end{proof}

We show that if $\alpha=\Theta(1)$, and $n$ is sufficiently large, then Theorem \ref{thm:alt-lower-bound} gives a nontrivial bound for $t=O(n/\ln n)$ and a tight bound (close to $\lambda$) for some $t=O(\sqrt{n/\ln n})$.

First suppose $t\le \kappa \frac{n}{\ln n}$ and set $d=\lceil\frac{\ln n}{2\kappa}\rceil$, for some $\kappa>0$. Assume $\ln n> \frac{2\kappa}{\alpha}(\alpha+\ln(1/\alpha))>2\kappa$. Then we have $(1-\alpha)^d\le e^{-\alpha d}\le e^{-\alpha(\frac{\ln n}{2\kappa}-1)}< e^{-(\alpha+\ln(1/\alpha))+\alpha}=\alpha$ and $d\le\frac{\ln n}{2\kappa}=\frac{\ln n}{\kappa}-\frac{\ln n}{2\kappa}< \frac{\ln n}{\kappa}-1\le\frac{n}{t}-1=\frac{n-t}{t}$, so $d$ is valid. These two inequalities also imply that the bound is positive.



Now suppose for some $\epsilon\in(0,1]$ that $t\le \sqrt{\frac{\alpha\epsilon n}{4\ln n}}$ and set $d=\lceil \frac{\ln n}{\alpha}\rceil$. 
Assume $n>\max\{\frac{2\epsilon}{\alpha}\ln\frac{2\epsilon}{\alpha},\frac{e^{2\alpha}}{\alpha\epsilon}\}$, which implies $\frac{n}{\ln n}>\frac{(2\epsilon/\alpha)\ln(2\epsilon/\alpha)}{\ln(2\epsilon/\alpha)+\ln\ln(2\epsilon/\alpha)}>\frac{(2\epsilon/\alpha)\ln(2\epsilon/\alpha)}{2\ln(2\epsilon/\alpha)}=\frac{\epsilon}{\alpha}$. 
For simplicity of the argument, observe that
$n-t\ge n-\sqrt{\frac{\alpha\epsilon n}{4\ln n}}\ge n-\sqrt{\frac{n\cdot n}{4\cdot 1}}=\frac{n}{2}$.
Then we have 
$(1-\alpha)^d\le e^{-\alpha d}
\le e^{-\alpha(\frac{\ln n}{\alpha}-1)}
= \frac{e^\alpha}{n}
<\frac{\alpha\epsilon}{e^\alpha}\le\alpha$ 
and 
$d\le\frac{\ln n}{\alpha}
=\sqrt{\frac{\ln n}{n}\cdot\frac{n\ln n}{\alpha^2}}
\le\sqrt{\frac{n\ln n}{\alpha\epsilon}}\le\frac{n}{2t}\le\frac{(n-t)}{t}$, so $d$ is valid. Then
\begin{align*}
\left(1-\frac{td}{n-t}\right)^t 
&\ge \left(1-\frac{t \frac{\ln n}{\alpha}}{n/2}\right)^t 
\ge 1-\frac{2t^2 \ln n}{\alpha n} 
\ge 1-\frac{\alpha\epsilon n}{4\ln n}\cdot\frac{2 \ln n}{\alpha n}
= 1-\frac{\epsilon}{2},
\\
\left(1 - \frac{ (1-\alpha)^d}{\alpha}\right)^{t-1}
&\ge \left(1 - \frac{1}{\alpha}\cdot\frac{e^\alpha}{n}\right)^t
\ge 1 - t\frac{e^\alpha}{\alpha n}
\ge 1 - \sqrt{\frac{\alpha\epsilon n}{4\ln n}}\cdot\frac{e^\alpha}{\alpha n}
> 1-\sqrt{\frac{e^{2\alpha} \epsilon}{4\alpha n}}
> 1- \frac{\epsilon}{2}.
\end{align*}
This implies the bound is at least $(1-\epsilon)\lambda$.


