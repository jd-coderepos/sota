\section{Experiments}\label{sec::experiment}

\begin{table*}[t!]

\scalebox{1.0}
{
    \begin{tabular}{lcccccc}
    \toprule
    \multirow{2}{*}{Model} & \multicolumn{2}{c}{Last.FM} & \multicolumn{2}{c}{MovieLens-1M} &  \multicolumn{2}{c}{DBbook2014} \\
    & HR@10 & NDCG@10 & HR@10 & NDCG@10 & HR@10 & NDCG@10 \\
    \midrule
ItemKNN & 0.0605 & 0.0511 & 0.0738 & 0.2273 & 0.0702 & 0.0665 \\
BPR-MF & 0.1199 & 0.0916 & 0.0895 & 0.1914 & 0.0829 & 0.0565 \\
\midrule
    RippleNet & 0.1008 & 0.0641 & 0.1269 & 0.2516 & 0.0763 & 0.0571 \\
    CFKG & 0.1781 & 0.1226 & 0.1393 & 0.2512 & 0.1428 & 0.1036 \\
    MKR & 0.1447 & 0.0850 & 0.1073 & 0.2245 & 0.0863 & 0.0575 \\
    KTUP & 0.1891 & 0.1566 & 0.1579 & 0.3230 & 0.1761 & 0.1299 \\\vspace{3pt}
    ConvE-Rec & 0.2426 & 0.1742 & 0.1993 & 0.3676 & 0.1850 & 0.1357 \\
\name & 0.2201 & 0.1552 & 0.1889 & 0.3543 & 0.1716 & 0.1266\\
    \name* & \textbf{0.2483} & \textbf{0.1766} & \textbf{0.1994} & \textbf{0.3699} & \textbf{0.1874} & \textbf{0.1371}\\
\midrule
Gain over KTUP & 31.31\% & 13.41\% & 26.28\% & 14.52\% & 6.41\% & 5.54\%\\  \bottomrule 
    \end{tabular}
}
\centering\caption{Performance of different models on three datasets.
The gain of \name* over KTUP is statistically significant at the 0.001 level according to t-test.
}\label{tab::comparision}
\end{table*}

In this section, we evaluate \name\ on three real-world datasets\footnote{We will make the code and processed datasets public if our paper gets accepted.}. Compared to other state-of-the-art methods, our proposed approach has the following advantages:
\begin{itemize}[leftmargin=*]
    \item \textbf{Effectiveness}. \name\ significantly outperforms existing state-of-the-art KG-based recommendation methods in terms of recommendation accuracy.
    \item \textbf{Explainability}. Case studies on generated paths demonstrate that \name\ can offer good explanations for recommended items.
\end{itemize}



Next, we first describe the evaluation datasets and experimental set-ups.


\subsection{Data and Experiment Settings}



\subsubsection{Data}
We test \name\ on three benchmark datasets for KG-based recommendation: 
\begin{itemize}[leftmargin=*]
\item \textit{Last.FM}\footnote{https://grouplens.org/datasets/hetrec-2011/}. 
This dataset contains a set of music artist listening information from a popular online music system Last.Fm. 

\item \textit{MovieLens-1M}\footnote{https://grouplens.org/datasets/movielens/1m/}. 
MovieLens-1M provides users' ratings towards thousands of movies. For these two datasets, we convert the explicit ratings into implicit feedback where each observed rating is treated as ``1'', and unobserved ratings are marked as ``0''s. Following \cite{Wang_2019}, we use Microsoft Satori to construct knowledge graphs for Last.FM and MovieLens-1M datasets respectively.
\item \textit{DBbook2014}\footnote{http://2014.eswc-conferences.org/important-dates/call-RecSys.html}. 
This dataset provides users' reading history in the book domain. Its supporting knowledge graph is extracted from DBpedia.
\end{itemize}

As we focus on KG-based recommendation, we remove items that have no matching entities in the corresponding knowledge graph. The statistics of processed datasets are presented in Table~\ref{tab::dataset}. Following \cite{Cao_2019,wang2018ripplenet,Wang2018ExplainableRO}, we randomly split the interactions of each user into training, validation, and test set with ratio 6:2:2.





\subsubsection{Baseline Methods} 
We compare \name\ with two kinds of methods: 
1) Classical similarity-based methods including:  
\textbf{ItemKNN}, which recommends items that are most similar to target user's historical items; 
\textbf{BPR-MF}~\cite{yu2014personalized}, which is a widely-used matrix factorization method using Bayesian Personalized Ranking (BPR) loss. 
2) KG-based recommendation methods including: 
\textbf{RippleNet}~\cite{wang2018ripplenet}, which propagates users' interests over knowledge graph with attention mechanism;  
\textbf{CFKG}~\cite{zhang2018learning}, which learns users' and items' representations by applying TransE~\cite{bordes2013translating} on the graph ; 
\textbf{MKR}~\cite{Wang_2019}, which learns both user-item matching task and knowledge graph embedding task under multi-task learning framework; 
\textbf{KTUP}~\cite{Cao_2019}, which is a state-of-the-art KG-based recommender that jointly learns translation-based recommendation~\cite{he2017translation} and translation-based knowledge graph embedding;
\textbf{ConvE-Rec}, which learns users' and items' embeddings based on integrated graph  with ConvE~\cite{dettmers2018convolutional}. As we use ConvE model for reward shaping, we treat ConvE-Rec as a special recommender.  


\subsubsection{Evaluation Metrics} Following~\cite{Cao_2019,he2015trirank,song2019session}, we adopt \textit{Hit Ratio} (HR) and \textit{Normalized Discounted Cumulative Gain} (NDCG) to evaluate the effectiveness of proposed \name\ and baseline methods. We use the same definition of HR and NDCG in \cite{he2015trirank}, where HR measures whether test items are present in the recommendation list and NDCG assesses the ranking quality of test items respectively.
In our study, we always report the averaged HR@K and NDCG@K scores across all users over five runs.



\subsubsection{Implementation Details} First of all, we add  into  if a triplet  exists to enhance the connectivity of graph, where  is the inverse of relation .
Following~\cite{Cao_2019}, we only preserve those triplets that are directly connected to items in each supporting knowledge graph.
Since the volume of action space  can be quite large for some nodes, it's time- and memory-consuming to maintain the size of action space based on the largest action space for all states. To simply implementation and computation, we only preserve a fixed number (256 in our experiments) of ``important'' outgoing edges for each node, and the importance of edges are decided by the PageRank scores~\cite{Pageetal98} of their end nodes. For nodes that have less than 256 outgoing edges, we up-sampling their outgoing edges to 256. 

We implement \name\ with Pytorch~\cite{paszke2017automatic}. 
Entity and relation embeddings are pre-trained by applying ConvE~\cite{dettmers2018convolutional}
on graph ,
and the embedding size is set to 32 for all methods except for ItemKNN, which has no latent representations. Meanwhile, we use the score function of ConvE to compute augmented rewards (i.e., Equation~\ref{eq:reward}). 
From Figure~\ref{fig:example}, we can see path patterns ``User->Item->Entity->item'' and ``User->Item->User->Item'' are more probable to be meaningful, so we empirically set the maximum path length  to 3 as our default setting. We select action dropout rate from \{0.1-0.9\}, dropout rate for entity/relation embeddings from \{0.1-0.9\} using grid search.
Meanwhile, grid search is also applied to select the optimal hyper-parameters for other baseline methods based on the performance on validation data.
For training, we use Adam~\cite{kingma2014adam} optimizer for all models with batch size of 512.
For recommendation, we use beam search with beam size 64 to generate paths for target users. For duplicate paths leading to the same item, we keep the one with the highest probability. 
Finally, we adopt two different ranking strategies to generate final top-K recommendation list: (1) ranks the searched items according to the path probabilities and we denote it as \name, 2) ranks the searched items based on ``rewards'' defined by  in Equation~\ref{eq:reward} and we denote it as \name*.



\subsection{Analysis of Recommendation Performance}
We report the recommendation accuracy of different methods in Table~\ref{tab::comparision}. We can see that KG-based recommendation methods consistently outperform classical similarity-based methods, which indicates that knowledge graphs indeed help to alleviate the problem of data sparsity in the recommendation. Among KG-based recommendation methods, the RippleNet performs worst, which may be attributed to representing users with multi-hop away entities. KTUP performs strongly because it takes the advantages of both translation-based recommendation and multi-task learning. Note that the only difference between ConvE-Rec and CFKG is the used knowledge graph embedding methods; however, ConvE-Rec achieves much better performance. The reason behind this is that ConvE is a state-of-the-art knowledge graph embedding method, which outperforms TransE used in CFKG. By using pre-trained ConvE embeddings to augment rewards, our \name\ and \name* significantly outperform existing state-of-the-art KG-based recommendation methods in most cases and perform comparably to the unexplainable ConvE-Rec model, which shows that our proposed methods are quite effective.  





\begin{table*}[tb]
\centering
\scalebox{0.93}
{
\begin{tabular}{lcccccc}
\toprule
\multirow{2}{*}{Model} & \multicolumn{2}{c}{Last.FM} & \multicolumn{2}{c}{MovieLens-1M} & \multicolumn{2}{c}{DBbook2014} \\
& HR@10 & NDCG@10 & HR@10 & NDCG@10 & HR@10 & NDCG@10 \\
\midrule 
\vspace{3pt}
\name\ & \textbf{0.2201} & \textbf{0.1552} & \textbf{0.1889} & \textbf{0.3543} & \textbf{0.1716} & \textbf{0.1266} \\
\name-KG & 0.2061 & 0.1466 & 0.1869 & 0.3489 & 0.0802 & 0.0525 \\
\name-RS & 0.0614 & 0.0349 & 0.0654 & 0.1132 & 0.1174 & 0.0867 \\
\name-AD & 0.1350 & 0.0827 & 0.1715 & 0.3217 & 0.1449 & 0.1083 \\ 
\name(T=5) & 0.2108 & 0.1505 & 0.1859 & 0.3500 & 0.1524 & 0.1125 \\
\bottomrule
\end{tabular}
}
\centering\caption{Performance w.r.t. model variants, where [-] means removing that component from the \name.}\label{tab::ablation}

\end{table*}


\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/case_movie_multi_path.pdf}
\caption{Recommendations and explanations for user \#4229 in MovieLens-1M dataset. ``Toy Story'' is recommended because 1) it shares the same movie genre(i.e., Children's) with ``Mouse Hount``, and 2) it is watched by another user who also watched ``Wrong Trousers''. These explainable paths are automatically discovered by \name\ in a generative manner. 
    }
    \label{fig::case}
\end{figure}


\subsection{Analysis of Explainability}
After demonstrating the effectiveness of \name, we now illustrate its explainability, which is the main contribution of this work to recommender systems. As introduced in previous sections, \name\ provides recommendations by generating meaningful paths from users to items, where paths serve as explanations for recommended items. To give you an intuitive example, we randomly select a real user from MovieLens-1M dataset and search preference paths for them with \name.
As shown in Figure~\ref{fig::case}, we can easily understand that ``Toy Story'' is recommended because it shares the same genre (i.e., Children's) with ``Mouse Hunt'', which the user watched before.
What'more, we find that \name\ model tends to provide multiple explanations for the recommendations. For example, movie ``Toy Story'' is also liked by another user (\#1088) who watched ``Wrong Trousers''. Such diverse explanations may better motivate users to accept the corresponding recommendations. 

 
Beyond explanations for an individual user, we are also interested in global preference path patterns discovered by \name. More specifically, we try to figure out what are the typical path patterns w.r.t. different datasets. 
From Table~\ref{tab::case}, we can see that \name\ relies more on the path pattern ``UserMovieUserMovie'' on MovieLens-1M dataset.
Interestingly, we find that \name\ learns relatively diverse path patterns on DBbook2014 dataset such as ``UserBookWikiPageBook'' and \\
 ``UserBookTypebook'', which lead to new books that share the same WikiPage or Type with users' historical books respectively. The reason behind the discrepancy of path patterns on two datasets may be two folds. First, note that the average number of interactions for each user in MovieLens-1M data is about 100 while this number is 12 in DBbook2014 data. Therefore it is easier to find a user sharing similar movie preference in MovieLens-1M data than to find a user with similar reading taste in DBbook2014 data. 
Second, the size of supporting knowledge graph for DBbook2014 data is much larger than the number of user-item interactions, so our \name\ learns to make more use of external knowledge when the user-item interactions are sparse.











\iffalse
\begin{table*}
\centering\caption{Performance w.r.t. variants of our model.  is the full model, and [-] means removing that component from . We omit the similar results of .}\label{tab::ablation}
\begin{tabular}{lcccccc}
\toprule
\multirow{2}{*}{Model} & \multicolumn{2}{c}{Last.FM} & \multicolumn{2}{c}{MovieLens-1M} & \multicolumn{2}{c}{DBbook2014} \\
& HR@10 & NDCG@10 & HR@10 & NDCG@10 & HR@10 & NDCG@10 \\
\midrule 
\vspace{3pt}
 & 0.2504 & 0.1812 & 0.1994 & 0.3699 & 0.1874 & 0.1371 \\
-KG & 0.2193 & 0.1553 & 0.1983 & 0.3634 & 0.0924 & 0.0626 \\
-rs & 0.0673 & 0.0380 & 0.0338 & 0.0645 & 0.0514 & 0.0284 \\
-AD & 0.1976 & 0.1482 & 0.1953 & 0.3639 & 0.1814 & 0.1340 \\ 
(T=5) & 0.2322 & 0.1703 & 0.1986 & 0.3696 & 0.1707 & 0.1275 \\
\bottomrule
\end{tabular}
\end{table*}
\fi




\begin{table}[tb]
    \centering
    \begin{tabular}{cc}
  \toprule
  Explainable preference paths & Pct. (\%)  \\ 
\midrule
  \text{U}\text{M}\text{U}\text{M} & 78.6 \\
  \text{U}\text{M}\text{C}\text{M} & 15.4 \\
  \text{U}\text{M}\text{G}\text{M} & 2.3 \\
   \midrule
   \text{U}\text{B}\text{T}\text{B} & 64.9 \\
   \text{U}\text{B}\text{WP}\text{B} & 21.7 \\
  \text{U}\text{B}\text{U}\text{B} & 7.4 \\

   \bottomrule
  \end{tabular}
\caption{Most frequent path patterns of Ekar during inference on MovieLens-1M (top) and DBbook2014 (bottom) datasets. ``U'', ``M'', ``C'', ``G'', ``B'', ``T'' and ``WP'' represent User, Movie, Country, Genre, Book, Type and WikiPage respectively. ``'' and ``'' are ``Interact'' and  ``Interacted'' in short respectively.}
    \label{tab::case}
\end{table}



\subsection{Ablation Study}\label{subsec:ablation}
In this section, we compare different variants of \name\ to show the influences of some essential components such as KG, reward shaping, action dropout and maximum path length , which are denoted as \name-KG, \name-RS, \name-AD and \name(T=5) respectively in Table~\ref{tab::ablation}. 
We find the influence of KG is very significant on Last.FM and DBbook2014 datasets. This is because these two datasets are extremely sparse, while the MovieLens-1M data is relatively dense. 
Removing reward shaping leads to a severe performance drop on all datasets because \name\ without reward shaping assigns zero rewards to all items that a user has not interacted with. In this way, the agent is penalized for exploring potential items that are of interest to users and therefore cannot effectively generate recommendations.
Besides reward shaping, we also use action dropout to further encourage the agent to explore diverse paths, and we can see that \name-AD performs worse than the full model \name. 
At last, we try a larger maximum path length  to enable the agent to explore longer paths. We find that \name(T=5) performs worse than \name\ on all datasets.
This is because long paths may introduce more noise and thus be less meaningful. However, thanks to the stop mechanism, the performance drop is not significant. 

To better understand the role of stop action, we present the details about \name(T=5)'s paths patterns in Table~\ref{tab::T5_case}. We have two observations. First, there are many paths of length five, which means that our agent is recommending items that have high-order similarity to users' historical items. 
Second, since the supporting knowledge graph is very large on DBbook2014 dataset and paths with length five may not be always useful, our agent learns to infer path with length of three or four by taking stop actions.




\begin{figure*}[th]
\includegraphics[width=0.98\textwidth]{figures/time_reward_2.pdf}
\caption{Results of running time with batch size of 512 and maximum path length of 3. The x-axis is the training time in minutes, and the y-axis is the average rewards over training samples.
}\label{fig::time_reward}
\end{figure*}















\begin{table}[tb]
\centering
  \begin{tabular}{cc}
  \toprule
  Explainable preference paths & Pct. (\%)  \\ 
\midrule
  \text{U}\text{M}\text{U}\text{M}UM & 58.7 \\
  \text{U}\text{M}\text{U}\text{M}\text{C}\text{M} & 25.7 \\  
\midrule
  \text{U}\text{B}\text{U}\text{B}TB & 52.3 \\
  UUBTBB & 13.2 \\
  UUBTBB  & 9.9 \\
  \bottomrule
  \end{tabular}
  \caption{Most frequent path patterns of \name(T=5) during inference on MovieLens-1M (top) and Last.FM (bottom) datasets. 
``U'', ``M'', ``C'', ``B'' and ``T'' represent User, Movie, Country, Book and Type respectively. ``'' and ``'' are ``Interact'' and  ``Interacted'' in short respectively.
}\label{tab::T5_case}
\end{table}


\subsection{\name\ with Different KGE Methods}\label{sec::distmult}
Although \name\ model has been utilizing ConvE model so far to pre-train entity and relation embeddings for both initialization and reward shaping, it is notable that \name\ is independent of any specific knowledge graph embedding methods. 
Therefore we also test our model with another widely-used knowledge graph embedding method DistMult~\cite{yang2014embedding}. The score functions of DistMult and ConvE are presented in Table~\ref{tab::score_func}. For a fair comparison, we use the same experimental settings and just substitute DistMult for ConvE in our experiments. Following ConvE-Rec, we denote recommendation with DistMult as DistMult-Rec.



\begin{table}[t]
    \centering
    \begin{tabular}{cc}
    \toprule
        Model & Score function   \\
        \midrule
        DistMult &  \\
        ConvE &  \\
    \bottomrule
    \end{tabular}
    \caption{Score functions w.r.t. different KGE methods, where  denotes generalized inner product of three vectors,  denotes a 2D shaping of vectors,  is the convolution operator,  denotes filters in convolutional layers,  is a non-linear activation function and \cdot converts a tensor to a vector. ,  and  are embeddings of user , entity  and relation ``Interact'' respectively.}
\label{tab::score_func}
\end{table}

The results of using different knowledge graph embedding methods on MovieLens-1M and DBbook2014 are presented in Table~\ref{tab::distmult}. First, we can see that ConvE-Rec outperforms DistMult-Rec on these two datasets. This is because ConvE has proven more effective than DistMult for knowledge graph completion, as a result of which our \name\ with ConvE also outperforms \name\ with DistMult. Second, \name\ (DistMult) and \name* (DistMult) outperform DistMult-Rec on MovieLens-1M dataset and perform comparably to DistMult-Rec on DBbook2014 dataset, which is consistent with the observations of \name\ (ConvE) and \name* (ConvE). We omit the results on Last.FM dataset because they show similar trends and lead to the same conclusion.










\begin{table}[!tb]
\scalebox{0.95}
{
\begin{tabular}{lcccc}
\toprule
\multirow{2}{*}{Model} & \multicolumn{2}{c}{MovieLens-1M} & \multicolumn{2}{c}{DBbook2014} \\
 & HR@10 & NDCG@10 & HR@10 & NDCG@10 \\
\midrule 
\vspace{3pt}
ConvE-Rec &  0.1993 & 0.3676 & 0.1850 & 0.1357 \\
\vspace{3pt}
DistMult-Rec  & 0.1773 & 0.3341 & 0.1535 & 0.1090 \\
\name\ (ConvE)  & 0.1889 & 0.3543 & 0.1716 & 0.1266\\\vspace{3pt}
\name\ (DistMult) & 0.1761 & 0.3352 & 0.1367 & 0.0958 \\
\name* (ConvE) & {0.1994} & {0.3699} & {0.1874} & {0.1371}\\
\name* (DistMult) & 0.1774 & 0.3343 & 0.1482 & 0.1061 \\
\bottomrule
\end{tabular}
}
\centering\caption{Effectiveness comparison of using different knowledge graph methods for entity/relation initialization and reward shaping.
The trends on Last.FM dataset are similar hence omited. 
}
\label{tab::distmult}
\end{table}




\subsection{Convergence Analysis}
We present the running time of \name\ in Figure~\ref{fig::time_reward}. As can be seen, \name\ converges fast on MovieLens-1M dataset with less than ten minutes, while it takes a bit more time to converge on the other two datasets. The reason for different convergence behaviors is that it is easier to walk to correct items on dense datasets (e.g., MovieLens-1M) than on sparse datasets (e.g., DBbook2014). Overall, our \name\ is efficient because we initialize entity/relation embeddings with pre-trained knowledge graph embeddings, and we use reward shaping to augment reward signals.



