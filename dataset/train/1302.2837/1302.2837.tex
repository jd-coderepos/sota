
\newcommand{\spc}{\hspace{1ex}}
\newcommand{\ctr}[1]{#1~~}
\begin{table*}[htb]
  \centering
{\footnotesize\noindent
  \begin{tabular}[htb]{@{}l@{\spc}l@{\spc}||@{\spc}r@{\spc}r@{\spc}r@{\spc}r@{\spc}|@{\spc}r@{\spc}r@{\spc}r@{\spc}r@{\spc}|@{\spc}r@{\spc}r@{\spc}r@{\spc}r@{\spc}|@{\spc}r@{\spc}r@{\spc}r@{\spc}r@{\spc}|@{\spc}r@{\spc}r@{\spc}r@{\spc}r@{\spc}|@{\spc}r@{\spc}r@{\spc}r@{\spc}r@{\spc}}
  &Problem& \multicolumn{4}{c@{\spc}|@{\spc}}{randmat} & \multicolumn{4}{c@{\spc}|@{\spc}}{thresh} & \multicolumn{4}{c@{\spc}|@{\spc}}{winnow} & \multicolumn{4}{c@{\spc}|@{\spc}}{outer} & \multicolumn{4}{c@{\spc}|@{\spc}}{product} & \multicolumn{4}{c@{\spc}}{chain} \\
  &Version& \ctr{s} & ex-s & \ctr{p} & ex-p & \ctr{s} & ex-s & \ctr{p} & ex-p & \ctr{s} & ex-s & \ctr{p} & ex-p & \ctr{s} & ex-s & \ctr{p} & ex-p & \ctr{s} & ex-s & \ctr{p} & ex-p & \ctr{s} & ex-s & \ctr{p} & ex-p \\
  \hline\hline

\multirow{4}{*}{
  \begin{sideways}
    \begin{minipage}{1.3cm}
      \centering
      Source code size
    \end{minipage}
  \end{sideways}
} 
& Chapel & 33 & 32 & 33 & 32 & 58 & 51 & 58 & 61 & 71 & 61 & 72 & 74 & 55 & 58 & 55 & 58 & 34 & 31 & 34 & 36 & 145 & 130 & 145 & 159\\
& Cilk & 39 & 39 & 48 & 40 & 69 & 72 & 119 & 95 & 88 & 93 & 146 & 139 & 76 & 79 & 83 & 72 & 58 & 61 & 65 & 58 & 187 & 190 & 320 & 251\\
& Go & 37 & 54 & 52 & 71 & 73 & 77 & 141 & 118 & 116 & 112 & 144 & 191 & 88 & 78 & 103 & 98 & 74 & 68 & 89 & 86 & 204 & 160 & 345 & 330\\
& TBB & 38 & 38 & 52 & 53 & 69 & 69 & 110 & 98 & 78 & 78 & 142 & 137 & 72 & 69 & 83 & 81 & 49 & 50 & 63 & 62 & 172 & 171 & 302 & 302\\
\hline

\multirow{4}{*}{
  \begin{sideways}
  \begin{minipage}{1.3cm}
    \centering
    Coding time (min)
  \end{minipage}
  \end{sideways}
} 
& Chapel & 58 & 81 & 76 & 100 & 25 & 25 & 121 & 156 & 21 & 21 & 134 & 155 & 19 & 21 & 55 & 64 & 8 & 8 & 43 & 45 & 6 & 12 & 76 & 137 \\
& Cilk & 18 & 18 & 101 & 154 & 24 & 24 & 251 & 294 & 19 & 19 & 112 & 121 & 17 & 17 & 26 & 39 & 6 & 6 & 12 & 15 & 21 & 21 & 77 & 118 \\
& Go & 25 & 42 & 45 & 76 & 20 & 63 & 132 & 163 & 48 & 129 & 92 & 163 & 19 & 67 & 24 & 31 & 6 & 12 & 18 & 21 & 44 & 103 & 56 & 91 \\
& TBB & 15 & 15 & 35 & 37 & 31 & 31 & 196 & 207 & 25 & 25 & 41 & 43 & 28 & 28 & 32 & 43 & 15 & 15 & 23 & 23 & 12 & 12 & 24 & 26 \\
\hline

\multirow{4}{*}{
  \begin{sideways}
  \begin{minipage}{1.28cm}
    \centering
    Execution time (sec)
  \end{minipage}
  \end{sideways}
} 
& Chapel & 23.3 & 11.4 & 18.7 & 3.1 & 22.2 & 36.7 & 7.8 & 13.1 & 49.4 & 45.7 & 21.4 & 21.3 & 5.8 & 5.6 & 1.6 & 1.6 & 2.5 & 2.5 & 1.4 & 1.4 & 71.8 & 97.3 & 36.0 & 36.0 \\
& Cilk & 6.7 & 6.7 & 0.5 & 0.4 & 11.9 & 11.9 & 0.9 & 0.8 & 16.0 & 16.0 & 0.8 & 0.7 & 2.7 & 2.7 & 0.3 & 0.2 & 1.3 & 1.3 & 0.3 & 0.2 & 41.2 & 36.1 & 2.4 & 1.7 \\
& Go & 11.8 & 10.5 & 2.9 & 0.5 & 18.9 & 16.7 & 2.1 & 1.6 & 18.2 & 15.5 & 2.0 & 1.3 & 15.3 & 11.3 & 1.5 & 2.4 & 2.2 & 2.2 & 1.1 & 0.3 & 107.2 & 75.2 & 177.7 & 38.4 \\
& TBB & 5.3 & 5.3 & 0.3 & 0.2 & 9.3 & 9.4 & 1.2 & 0.6 & 9.7 & 9.7 & 1.0 & 1.0 & 1.9 & 1.9 & 0.3 & 0.3 & 1.3 & 1.3 & 0.2 & 0.2 & 35.2 & 35.5 & 2.8 & 2.8 \\
\hline

\multirow{4}{*}{
  \begin{sideways}
  \begin{minipage}{1.3cm}
    \centering
    Speedup
  \end{minipage}
  \end{sideways}
} 
& Chapel & - & - & 1.2 & 2.8 & - & - & 2.8 & 2.8 & - & - & 2.3 & 2.1 & - & - & 3.4 & 3.5 & - & - & 1.7 & 1.7 & - & - & 2.0 & 2.1 \\
& Cilk & - & - & 13.6 & 16.8 & - & - & 13.4 & 14.9 & - & - & 19.1 & 20.2 & - & - & 8.1 & 8.1 & - & - & 4.2 & 5.8 & - & - & 17.3 & 20.2 \\
& Go & - & - & 4.1 & 21.2 & - & - & 8.9 & 8.1 & - & - & 8.0 & 11.5 & - & - & 10.4 & 4.7 & - & - & 1.9 & 7.5 & - & - & 0.6 & 1.9 \\
& TBB & - & - & 20.7 & 21.2 & - & - & 8.1 & 14.8 & - & - & 9.4 & 9.5 & - & - & 7.4 & 7.4 & - & - & 7.2 & 7.3 & - & - & 12.5 & 12.6 \\
  \end{tabular}
}

{\footnotesize s: sequential; ex-s: expert-sequential; p: parallel; ex-p: expert-parallel}
{\footnotesize  average times and speedups are given \hfill}
  \vspace{2ex}
  \caption{Measurements for all metrics, across all languages, problems, and versions}
  \label{tab:data}
\end{table*}


\section{Results}
\label{sec:results}

This section presents and discusses the data collected in the experiment as defined in \secref{sec:experimental-design}. To facilitate replication of the results, an online repository\footnote{\url{https://bitbucket.org/nanzs/multicore-languages}} provides all the code as well as the analysis scripts. 

\subsection{Preliminaries}
\label{sec:preliminaries}

\tabref{tab:data} provides absolute numbers for all versions of the code, before and after expert review. 
Unless stated otherwise, the discussion of the data in \secref{sec:results} refers to the expert-parallel versions, i.e.\ the parallel versions obtained after expert review.

To facilitate comparison, all figures display the data in value-normalized form, namely relative to the smallest/fastest/etc. measurement per problem (which itself gets the value 1.0).

\subsubsection{Statistical evaluation} The results are statistically evaluated using the Wilcoxon signed-rank test (two-sided variant), a non-parametric test for paired samples. Specifically, for all metrics, each language is compared with each other language across all problems. We will say that `` is significantly different from '' regarding a specific metric if ; we will say that `` \emph{tends to be} different from '' if . 

We will represent the language relationships using graphs, where a solid arrow is drawn from  to  if  is significantly better than  in a certain metric; a dotted arrow is drawn if  tends to be better than . The ordering relations are transitive, but this will not explicitly be shown in the figures for clarity.

\subsubsection{Rating function} The statistical evaluation states the difference of two languages in qualitative terms, but does not expose the magnitude of this difference. The magnitude is important, however, because although two languages are significantly different regarding a certain metric, the relative difference might be small enough to be negligible in certain use cases. To address this, we define the \emph{average relative rating} of each language amongst the other languages, for a specific metric :

\vspace{-2ex}


For each language, the rating function calculates the average of the language's relative performance in each problem compared to the best performance of any language in the same problem. Thus, if the language was the best in all problems in a given metric, the result will be 1.0; a value of 2.0 for a given language and metric means that, on average, the language was 2 times ``worse'' (slower or larger, etc., depending on the metric) than the best language for that metric in each problem; and so on.

\subsection{Source code size}
\label{sec:source-code-size}

The graph in \figref{fig:loc} shows the relative number of lines of source code (LoC) across all languages and problems, normalized to the smallest size in each problem.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth+0.3cm]{images/bargraph-loc-var-norm-expertpar.pdf}
  \caption{Source code size (LoC)}
  \label{fig:loc}
\end{figure}

Chapel shows the smallest code size in all of the problems, which can be explained by the conciseness of its language-integrated parallel directives. All the other languages are typically around 1.5-2.0 times larger, relative to Chapel's code size. Go's code size is the largest in all of the problems; reasons for this are the space taken for setting up the goroutines whenever a parallel operation is needed, and for synchronization with channels. Cilk and TBB hold a middle ground and are often comparable in code size.

Results of the Wilcoxon test and of the rating function are combined in \figref{fig:ord:size} (\secref{sec:preliminaries} explains how to interpret the graph). The placement of a language along the x-axis reflects its rating according to the rating function.

\begin{figure}[htbp]
  \centering
  \begin{tikzpicture}[scale=3]
    \node (chapel) at (1,0) {\chapel};
    \node (cilk)   at (1.5,0) {\cilk};
    \node (go)     at (2.1,0) {\go};
    \node (tbb)    at (1.7,-0.2) {\tbb};
    \path[->] (go) edge (cilk);
    \path[->] (go) edge (tbb);
    \path[->] (tbb) edge[dotted] (cilk);
    \path[->] (tbb) edge (chapel);
    \path[->] (cilk) edge (chapel);

    \draw[very thin,color=gray,step=0.3] (1,-0.3) grid (2.3,-0.3);
    \foreach \pos in {1, 1.5, 1.7, 2.1}
      \draw[shift={(\pos,-0.3)}] (0pt,1pt) -- (0pt,-1pt) node[below] {\stext{\pos}};
  \end{tikzpicture}
  \vspace{-2ex}
  \caption{Source code size: statistical ordering and rating}
  \label{fig:ord:size}
\end{figure}

This confirms statistically the visual interpretation of \figref{fig:loc}. Chapel provides the most concise code overall, and Go the largest code size (on average about 2.1 times as large as Chapel's code). Cilk and TBB are in between, with Cilk tending to have smaller code sizes than TBB.

\subsection{Coding time}
\label{sec:coding-time}

\figref{fig:time} shows the relative time to code for each problem in each language. Note that times are cumulative in the following way: the coding time in the reference versions (expert-parallel time) is the sum of the time used to obtain the initial parallel version (parallel time) plus the time needed to refine it after the expert comments; as the parallel versions were based on the sequential ones, the parallel coding time itself includes the sequential coding time (sequential and expert-sequential time, respectively) in all cases. 

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth+0.3cm]{images/bargraph-codingtime-var-norm-expertpar.pdf}
  \caption{Coding time}
  \label{fig:time}
\end{figure}

In contrast to the lines of code metric, the figure does not suggest any immediate conclusions. No clear ordering is visible, although TBB seems to have consistently low (but not always lowest) coding times. This is confirmed by the statistical evaluation, which yields no significant differences (displayed again as graph in \figref{fig:ord:codingtime}). The individual ratings show a clearer picture: coding in TBB takes on average only 1.2 times longer to code than the other three approaches, placing it at the top; Go, Cilk, and Chapel take on average at least 2.1 times longer, with Chapel taking 3.0 times longer. 

\begin{figure}[htbp]
  \centering
  \begin{tikzpicture}[scale=3]
    \node (chapel) at (3.0,0) {\chapel};
    \node (cilk)   at (2.6,0) {\cilk};
    \node (go)     at (2.1,0) {\go};
    \node (tbb)    at (1.2,0) {\tbb};

    \draw[very thin,color=gray,step=0.1] (1,-0.1) grid (3.2,-0.1);
    \foreach \pos in {1, 1.2, 2.1, 2.6, 3.0}
      \draw[shift={(\pos,-0.1)}] (0pt,1pt) -- (0pt,-1pt) node[below] {\stext{\pos}};
  \end{tikzpicture}
  \vspace{-2ex}
  \caption{Coding time: statistical ordering and rating}
  \label{fig:ord:codingtime}
\end{figure}

\subsection{Execution time}
\label{sec:execution-time}

\subsubsection{Measurement} The performance tests were run on a 4  Intel Xeon Processor E7-4830 (2.13 GHz, 8 cores; total 32 physical cores) server with 256 GB of RAM, running Red Hat Enterprise Linux Server release 6.3. Language and compiler versions used were: chapel-1.6.0 with gcc-4.4.6, for Chapel; Intel C++ Compiler XE 13.0 for Linux, for both Cilk and TBB; go-1.0.3, for Go.

Each performance test was repeated 30 times, and the mean of the results was taken. All tests use the same inputs, the size-dominant of which is a  matrix (about 12 GB of RAM). This size, which is the largest input size all languages could handle, was chosen to test scalability. The language Go provided the tightest constraint, while the other languages would have been able to scale to even larger sizes. 

An important factor in the measurement is that for all problems the I/O time is significant, since they involve reading/writing matrices to/from the disk. In order for the measurements to not be dominated by I/O, a special flag \lstinl{is_bench} was added to every solution. This flag means that neither input nor output should occur and that the input matrices should be generated on-the-fly instead. All performance tests were run with the \lstinl{is_bench} flag set.

\subsubsection{Observations} \figref{fig:exec:time} shows the relative execution time on 32 cores for each language and problem. The error bars show the 99.9\% confidence interval for the mean.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\columnwidth+0.3cm]{images/bargraph-executiontime-var-norm-expertpar.pdf}
  \caption{Execution time}
  \label{fig:exec:time}
\end{figure}

Chapel took the most time to execute in almost all problems. As mentioned in \secref{sec:chapel}, this reflects that the Chapel compiler is missing important optimizations, which were deferred to give first more attention to correctness. Also, all Chapel variables are default initialized; in particular the large matrix in the experiment will be zeroed, causing additional delay.  The distance to the other languages decreases significantly as the input size is decreased, hinting at the fact that the main problem is scalability (Chapel's speedup reaches a plateau early, as discussed in \secref{sec:speedup}).

Go shows uneven execution times across the problems, which might be explained by the language's lack of maturity (only 3 years old); the performance might show more stable results in the future. 
In particular, the execution time for the chain and outer problems are 
much higher than expected, 
they should be on the same order of magnitude as the other subproblems.
Chain additionally has a much higher variance than expected.

TBB and Cilk show consistently low execution times.

This impression is confirmed statistically, as shown in \figref{fig:ord:exectime}. Both Chapel and Go exhibit a significantly slower execution time than Cilk and TBB. Considering the rating, TBB and Cilk are on par with a score of 1.2, followed by Go at 6.9 and Chapel at 17.0.

\begin{figure}[htbp]
  \centering
  \begin{tikzpicture}[scale=0.4]
    \node (chapel) at (17.0,1.5) {\chapel};
    \node (cilk)   at (1.2,1.5) {\cilk};
    \node (go)     at (6.9,-0.2) {\go};
    \node (tbb)    at (1.2,-0.2) {\tbb};

    \path[->] (chapel) edge (cilk);
    \path[->] (chapel) edge (tbb);
    \path[->] (go) edge (cilk);
    \path[->] (go) edge (tbb);

    \draw[very thin,color=gray,step=1] (1,-1) grid (17.2,-1);
    \foreach \pos in {1.2, 6.9, 17.0}
      \draw[shift={(\pos,-1)}] (0pt,6pt) -- (0pt,-6pt) node[below] {\stext{\pos}};
  \end{tikzpicture}
  \vspace{-2ex}
  \caption{Execution time: statistical ordering and rating}
  \label{fig:ord:exectime}
\end{figure}

\subsection{Speedup}
\label{sec:speedup}

\begin{figure*}[htb]
  \centering
  \includegraphics[width=\textwidth-1.5cm]{images/speedup-expertpar-all.pdf}
  \caption{Speedup per problem}
  \label{fig:speedup:problem}
\end{figure*}

\subsubsection{Measurement} Speedup was measured across 1, 2, 4, 8, 16, and 32 cores, with respect to the \emph{fastest single thread implementation} in the respective language; this is the fastest implementation when executed on a single logical thread, i.e.\ either the sequential version itself, or the parallel version restricted to run on a single thread. 



\subsubsection{Observations} \figref{fig:speedup:problem} shows the speedup graphs per problem. The values are accurate within a 99.9\% confidence interval (error bars would not be visible on the plot).

For the problems product and outer, the speedup in all languages tends to plateau starting from about 16 cores. This can be partly attributed to the fact that the sequential versions already take very little time to execute on these problems; the input size would have to be further increased (but cannot without losing the ability to compare amongst all approaches, as discussed above). In all other problems, at least one language shows good scalability; as the number of cores increase, the speedup lines fan out, showing that there are significant differences.

Cilk and TBB show good scalability on these problems, with speedups of about 15--20 and 10--21 on 32 cores, respectively. 

Go's scalability is more uneven: in product and randmat it keeps up with the top performers; a plateau is visible in thresh at 16 cores; and speedup deterioration is detected in chain and outer. The deterioration might be caused by excessive creation of goroutines, generating scheduling and communication overheads. 

Chapel's speedup consistently plateaus early from around 4-8 cores and at a speedup of around 2-3 in all problems, but does not specifically underperform in any of them. This shows the need of an an overall improvement in the Chapel compiler's implementation (see discussion in \secref{sec:chapel}).

\figref{fig:ord:speedup} shows the results of the statistical tests and the application of the rating function, for the speedup at 32 cores. We opted for using the speedup at 32 cores, as it represents the best approximation available to the asymptotic speedup. Note that the rating function has to be modified slightly: since in the speedup measure larger is better, the inverse of the metric value is used.

\begin{figure}[htbp]
  \centering
  \begin{tikzpicture}[scale=1.4]
    \node (chapel) at (6.4,0.3) {\chapel};
    \node (cilk)   at (1.0,0.3) {\cilk};
    \node (go)     at (2.9,-0.3) {\go};
    \node (tbb)    at (1.4,-0.3) {\tbb};

    \path[->] (chapel) edge (cilk);
    \path[->] (chapel) edge (tbb);
    \path[->] (chapel) edge[dotted] (go);
    \path[->] (go) edge[dotted] (tbb);

    \draw[very thin,color=gray,step=0.5] (1,-0.5) grid (6.6,-0.5);
    \foreach \pos in {2.9, 6.4}
      \draw[shift={(\pos,-0.5)}] (0pt,2pt) -- (0pt,-2pt) node[below] {\stext{\pos}};
    \foreach \pos in {1.0}
      \draw[shift={(\pos,-0.5)}] (0pt,2pt) -- (0pt,-2pt) node[below] {\stext{1.1}};    
    \foreach \pos in {1.4}
      \draw[shift={(\pos,-0.5)}] (0pt,2pt) -- (0pt,-2pt) node[below] {\stext{1.3}};    
  \end{tikzpicture}
  \vspace{-2ex}
  \caption{(Inverse of) speedup: statistical ordering and rating}
  \label{fig:ord:speedup}
\end{figure}

Confirming the expectation from the speedup graphs, Chapel shows significantly worse speedup than Cilk and TBB, and tends to show worse speedup than Go. Cilk and TBB show no significant difference, while Go tends to show worse speedup than TBB.

\subsection{Lessons learned from the expert review}
\label{sec:influence-expert}

In the previous sections, the results were computed from the study's reference versions of the benchmark (i.e.\ after expert review). However, numerous lessons can also be drawn from the experts' suggestions on how to \emph{change} the original implementations. For space reasons, we present only two examples of such change requests (for Cilk and Go); a more complete analysis is available in a related publication~\cite{nanz-et-al:2013:examining}.





\codecilk{}{}
After expert review, all Cilk solutions decreased in source code size by up to about 20\%. This change can be traced back to one of the expert comments to replace \lstinline[basicstyle=\normalsize]{cilk_spawn}/\lstinline[basicstyle=\normalsize]{cilk_sync} style code (see \lstref{lst:cilk:spawnsync}), an idiom that Cilk has been known for originally.

\codecilk{Cilk: divide-and-conquer}{lst:cilk:spawnsync}
\begin{lstlisting}
void do_work(int begin, int end) {
  int middle = begin + (end - begin) / 2;
  if (begin + 1 == end) {
    work (begin);
  } else {
    cilk_spawn do_work (begin, middle);
    cilk_spawn do_work (middle, end);
  }
  cilk_sync;
}
. . .
cilk_spawn do_work (0, n);
\end{lstlisting}

The expert suggested to use \mbox{\lstinline[basicstyle=\normalsize]{cilk_for}} (\lstref{lst:cilk:parfor}) as it simplifies the code while doing the same recursive divide-and-conquer underneath, and should therefore be preferred. 

















Strong execution time improvements (up to about 80\% decrease) after expert review were visible for Go in many of the problems. This can be attributed to a single change in the way parallelism was achieved. In the non-expert versions, a divide-and-conquer pattern of the form displayed in \lstref{lst:go:divideandconquer} was used. Instead, the expert recommended the distribute-work-synchronize pattern of \lstref{lst:go:parfor}. While the divide-and-conquer approach creates one goroutine per row of the matrix, the distribute-work-synchronize approach creates one for each processor core; for large matrices, the overhead of the excessive creation of goroutines then causes a performance hit.




\codego{Go: divide-and-conquer}{lst:go:divideandconquer}
\begin{lstlisting}
func do_work(begin, end, done chan bool) {
  if (begin + 1 == end) {
    work (begin, done)
  } else {
    middle := begin + (end - begin) / 2
    go do_work (begin, middle, done)
    do_work (middle, end, done)
  }
}
. . .
done := make(chan bool)
go do_work(0, n, done)

for i := 0; i < nrows; i++ {
  <-done
}
\end{lstlisting}








\subsection{Threats to validity}
\label{sec:threats-to-validity}



As a threat to external validity, it is arguable whether the study results transfer to large applications, due to the size of the programs used. The modest problem size is intrinsic to the study: the use of top experts is crucial to reliably answer the research questions and, unfortunately, this also means that the program size has to remain reasonable to fit within the review time the experts were able to donate. However, a recent study~\cite{okur:2012:libraries} confirms that the amount of code dedicated to parallel constructs for 10K and 100K LOC programs is between 12 and 60 lines of code on average; this makes our study programs representative of the parallel portions of larger programs. 

Furthermore, only one developer is used to provide the base versions for all languages. While this would be a serious objection in other contexts, this a lesser threat to validity in our experiment. This is because the experiment builds on the concept of single \emph{reference} benchmark programs rather than groups of average-quality programs. This is indeed one of the innovations of our experimental setup, and avoids problems with using non-expert study participants, which are discussed in \secref{sec:related-work}. The base versions were only provided to enable the expert review step; while alternatively, we could have asked experts to implement the benchmark problems themselves from scratch, this would certainly have exceeded the time the experts were able to donate for the study. 

On the other hand, the choice of using expert-checked reference benchmark programs may threaten external validity in the sense that the results only hold for developers which are highly skilled in the approaches. Also, the influence of a learning effect when a single developer solves the same problem in different languages remains as a threat, which could be mitigated by using a group of developers.

Problem selection bias, a threat to internal validity, is avoided in part by using an existing problem set, instead of creating a new one. The threat that specific problems could be better suited to some languages than others remains, as it could already be present in the existing problem set. As a positive point, none of the experts criticized the choice of problems for evaluating their language.

Since the languages are based on very different fundamental designs, it is not immediately clear whether or not they are actually comparable. But as long as it is possible to solve a problem in all languages the metric comparison seems to be meaningful. Again, the experts were aware of the competing languages and did not challenge the choice of languages.
  




\subsection{Discussion}
\label{sec:discussion}

Considering all four metrics together (see the summary of the ratings in \tabref{tab:overallranks}), it becomes apparent that all four languages have individual strengths and weaknesses.

\begin{table}[htb]
  \centering
\def\arraystretch{1.1}
{\footnotesize
  \begin{tabular}{l|cccc}
         & Source       & Coding       & Execution    & (Inverse of) \\
         & code size    & time         & time         & speedup\\
\hline
Chapel   & \textbf{1.0} & 3.0          & 17.0         & 6.4 \\
Cilk     & 1.5          & 2.6          & \textbf{1.2} & \textbf{1.1} \\
Go       & 2.1          & 2.1          & 6.9          & 2.9 \\
TBB      & 1.7          & \textbf{1.2} & \textbf{1.2} & 1.3 \\
  \end{tabular}
}
  \vspace{2ex}
  \caption{Ratings (smaller is better; best in bold)}
  \label{tab:overallranks}
  \vspace{-2ex}
\end{table}

Chapel has incorporated parallel directives at the language level. This has an apparent advantage as the code size is consistently the smallest of all problems: there is a clear benefit to having language-level support for high-level operations.  However, the performance rates quite low, though this does not appear to be an inherent property of the language, but rather that the focus of the compiler implementation has been on other issues (see discussion in \secref{sec:chapel}).

Cilk's initial claim to fame was in lightweight tasks, which could quickly be balanced among many threads. Consequently, the language shows very strong performance results. Since then Cilk has gained a new keyword (\cilkinline{cilk_for}), giving it also some advantage on the code size metric.

Go has been designed for more general forms of concurrency; for example, using channels for communication but allowing shared memory where necessary is very flexible. Consequently, Go does not have extensive support for structured parallel computations, such as fork-join or parallel-for. This is evident in the source code size, which is often the largest. Go does an acceptable job on the performance measures, although some problems have been detected. Since the language is the youngest in the study (it appeared in 2009), the compiler is expected to mature in this respect.

TBB has no language-level support, it is strictly a library approach. However the library it provides is the most comprehensive of the four languages, containing algorithmic skeletons, task groups, synchronization and message passing facilities. The high level parallel algorithms were sufficient to implement every task in the benchmark set without dropping down to lower level primitives such as manual task creation and synchronization. TBB provides together with Cilk the best performance. Being a library for a well known language, it also has the fastest coding times.

Although none of the languages have any mechanisms to ensure freedom from concurrency issues such as data races or deadlocks, their common aim is to provide the ability to use built-in functionality to make the common cases easy and as safe as they can be.

