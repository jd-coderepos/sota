

In this section, we present experimental settings and results. We primarily conduct experiments on (detailed) image/grounded captioning, vision question answering, and visual grounding tasks, including referring expression comprehension. We present both quantitative and qualitative results. 









\begin{table*}[t!]
\small
\centering \resizebox{\textwidth}{!}{
\begin{tabular}{@{}lc ccccccc @{}} \toprule
\multirow{2}{*}{Method} & \multirow{2}{*}{Grounding} &  \multirow{2}{*}{OKVQA}  & \multirow{2}{*}{GQA} & VSR  & IconVQA & VizWiz & HM  \\  & & & & (zero-shot) & (zero-shot)  &  (zero-shot) & (zero-shot)  \\
\midrule
Flamingo-9B & \xmark & 44.7  & - & 31.8 & -  & 28.8 & 57.0  \\
BLIP-2 (13B) & \xmark & 45.9  & 41.0 & 50.9  & 40.6 & 19.6 & 53.7  \\
InstructBLIP (13B) & \xmark & - & 49.5 & 52.1  & 44.8 & 33.4 & 57.5  \\ 
MiniGPT-4 (13B) & \xmark & 37.5   & 30.8 &  41.6  & 37.6 & - & - \\
LLaVA (13B) & \xmark & 54.4 &  41.3 & 51.2  & 43.0 & - & - \\
Shikra (13B)  & \cmark & 47.2  & - & - &  - & - & - \\
\rowcolor{Gray} 
\rowcolor{Gray}
\rowcolor{Gray}
Ours (7B)& \cmark & 56.9  & \textbf{60.3} & 60.6 & 47.7 & 32.9  & 58.2  \\
\rowcolor{Gray}




Ours (7B)-chat & \cmark & \textbf{57.8} & 60.1 & \textbf{62.9} & \textbf{51.5} & \textbf{53.6} & \textbf{58.8} \\


\bottomrule
\end{tabular}
}
\caption{\textbf{Results on multiple VQA tasks.} We report top-1 accuracy for each task. Grounding column indicates whether the model incorporates visual localization capability. The best performance for each benchmark is indicated in \textbf{bold}.}
\label{vqa_results}
\end{table*}


\begin{table*}[t!]
\small
\centering \resizebox{\textwidth}{!}{
\begin{tabular}{@{}lc| cccccccc c  @{}} \toprule
 \multirow{2}{*}{Method}&  \multirow{2}{*}{Model types} &  \multicolumn{3}{c}{RefCOCO} & \multicolumn{3}{c}{RefCOCO+} &\multicolumn{2}{c}{RefCOCOg} & \multirow{2}{*}{Avg}\\
&&  val & test-A & test-B & val & test-A & test-B &  val & test  & \\  [0.5ex] \midrule
UNINEXT & \multirow{2}{*}{Specialist models} & 92.64 & 94.33 & 91.46 & 85.24 & 89.63 & 79.79 & 88.73 & 89.37  &88.90 \\
G-DINO-L & & 90.56 & 93.19 &  88.24 & 82.75 & 88.95 & 75.92 & 86.13 & 87.02 & 86.60\\
\midrule
VisionLLM-H & \multirow{6}{*}{Generalist models} & - & 86.70 & -& -& - &- &- &- & -\\
OFA-L & & 79.96 & 83.67 & 76.39 & 68.29 & 76.00 & 61.75 & 67.57 & 67.58 &  72.65\\
Shikra (7B) & &  87.01 & 90.61 & 80.24 & 81.60 & 87.36 & 72.12 & 82.27 & 82.19 & 82.93 \\ 
Shikra (13B) & &  87.83 & 91.11 & 81.81 & \textbf{82.89} & \textbf{87.79} & 74.41 & 82.64 & 83.16 & 83.96\\ 



\rowcolor{Gray}
Ours (7B) &  & \textbf{88.69} & \textbf{91.65}  & \textbf{85.33} & 79.97 & 85.12 & \textbf{74.45}& \textbf{84.44}  & \textbf{84.66} & \textbf{84.29} \\

\rowcolor{Gray}

Ours (7B)-chat & & 88.06 & 91.29 & 84.30 & 79.58 & 85.52 & 73.32 & 84.19 & 84.31 & 83.70 \\
\bottomrule
\end{tabular}
}
\caption{\textbf{Results on referring expression comprehension tasks.} Our MiniGPT-v2 outperforms many VL-generalist models including VisionLLM~\citep{wang2023visionllm}, OFA~\citep{ofa} and Shikra~\citep{shikra} and reduces the accuracy gap comparing to specialist models including UNINEXT~\citep{yan2023universal} and G-DINO~\citep{liu2023grounding}.}
\label{refcoco}
\end{table*}







\noindent \textbf{Implementation details.} Throughout the entire training process, the visual backbone of MiniGPT-v2 remains frozen. We focus on training the linear projection layer and efficient finetuning the language model using LoRA~\citep{lora}. With LoRA, we finetune  and  via low-rank adaptation. In our implementation, we set the rank, . We trained the model with an image resolution of 448x448 during all stages. During each stage, we use our designed multi-modal instructional templates for various vision-language tasks during the model training.


\textbf{Training and hyperparameters.} We use AdamW optimizer with a cosine learning rate scheduler to train our model.  In the initial stage, we train on 8xA100 GPUs for 400,000 steps with a global batch size of 96 and an maximum learning rate of 1e-4. This stage takes around 90 hours. During the second stage, the model is trained for 50,000 steps on 4xA100 GPUs with a maximum learning rate of 1e-5, adopting a global batch size of 64, and this training stage lasts roughly 20 hours. For the last stage, training is executed for another 35,000 steps on 4xA100 GPUs, using a global batch size of 24 and this training stage took around 7 hours, maintaining the same maximum learning rate of 1e-5.














\subsection{Quantitative Evaluation}

\noindent \textbf{Dataset and evaluation metrics.} We  evaluate our model across a range of VQA and visual grounding benchmarks. For VQA benchmarks, we consider OKVQA~\citep{schwenk2022okvqa}, GQA~\citep{hudson2019gqa}, visual spatial reasoning (VSR)~\citep{liu2023visual}, IconVQA~\citep{iconqa}, VizWiz~\citep{vizwiz}, HatefulMemes and (HM)~\citep{hateful}. For visual grounding, we evaluate our model on  RefCOCO~\citep{kazemzadeh2014referitgame} and RefCOCO+\citep{yu2016modeling}, and RefCOCOg\citep{mao2016generation} benchmarks. 



To evaluate VQA benchmarks, we use an open-ended approach with a greedy decoding strategy. We evaluate each VQA question with the following instruction template: \textit{``[vqa] {question}"}. Following the previous  method~\citep{instructblip}, we evaluate the performance by matching the model's response to the ground-truth and reporting top-1 accuracy. For visual grounding benchmarks, we use the template \textit{``[refer] give me the location of {Referring expression}"} for each referring expression comprehension question, and a predicted bounding box is considered as correct for reporting accuracy if its IOU between prediction and ground-truth is higher than 0.5.





\begin{table*}[t!]
\small
\centering \resizebox{0.95\textwidth}{!}{
\begin{tabular}{@{}lccccccccc @{}} \toprule
&  OKVQA  & GQA & WizViz & VSR & IconVQA & HM &  Average\\  \midrule
Ours w/o task identifier  & 50.5  & 53.4 &  28.6 & 57.5 & 44.8 & 56.8 & 48.6 \\
Ours  & \textbf{52.1} & \textbf{54.6} & \textbf{29.4}& \textbf{59.9} &  \textbf{45.6} & \textbf{57.4} & \textbf{49.8}  \\
\bottomrule
\end{tabular}
}
\caption{Task identifier ablation study on VQA benchmarks. With task identifier during the model training can overall improve VQA performances from multiple VQA benchmarks}
\label{ablation}
\end{table*}



\begin{table*}[t!]
\centering \resizebox{0.6\textwidth}{!}{
\begin{tabular}{@{}lc cc @{}} \toprule
Method &   &    & Len  \\  \midrule
MiniGPT-4 & 9.2& 31.5 & 116.2 \\
mPLUG-Owl  & 30.2 & 76.8  & 98.5 \\
LLaVA & 18.8 &  62.7&  90.7 \\
MultiModal-GPT & 18.2 & 36.2 & 45.7 \\
\rowcolor{Gray}
MiniGPT-v2 (long) & 8.7 & 25.3 & 56.5 \\
\rowcolor{Gray}
MiniGPT-v2 (grounded) & 7.6  & 12.5 & 18.9 \\
\rowcolor{Gray}
MiniGPT-v2 (short) & \textbf{4.4} & \textbf{7.1} & \textbf{10.3}\\
\bottomrule
\end{tabular}
}
\caption{\textbf{Results on hallucination.} We evaluate the hallucination of MiniGPT-v2 with different instructional templates and output three versions of captions for evaluation. For the ``long" version, we use the prompt \textit{generate a brief description of the given image}. For the ``grounded" version, the instruction is \textit{[grounding] describe this image in as detailed as possible}. For the ``short" version, the prompt is \textit{[caption] briefly describe the image}. 
}
\label{halluciniation}
\end{table*}




\noindent \textbf{Visual question answering results.} Table~\ref{vqa_results} presents our experimental results on multiple VQA benchmarks. Our results compare favorably to baselines including MiniGPT-4~\citep{zhu2023minigpt}, Shikra~\citep{shikra}, LLaVA~\citep{llava}, and InstructBLIP~\citep{instructblip} across all the VQA tasks. For example, on QKVQA, our MiniGPT-v2 outperforms MiniGPT-4, Shikra, LLaVA, and BLIP-2 by 20.3\%, 10.6\%, 3.4\%, and 11.9\%. These results indicate the strong visual question answering capabilities of our model. Furthermore, we find that our MiniGPT-v2 (chat) variant shows higher performance than the version trained after the second stage. On OKVQA,  VSR, IconVQA, VizWiz, and HM, MiniGPT-v2 (chat) outperforms MiniGPT-v2 by 0.9\%,  2.3\%, 4.2\%, 20.7\%, and 0.6\%. We believe that the better performance can be attributed to the improved language skills during the third-stage training, which is able to benefit visual question comprehension and response, especially on VizWiz with 20.7\% top-1 accuracy increase.


\noindent \textbf{Referring expression comprehension results.} Table \ref{refcoco} compares our model to baselines on REC benchmarks. Our MiniGPT-v2 shows strong REC performance on RefCOCO, RefCOCO+, and RefCOCOg, performing better than  other vision-language generalist models. MiniGPT-v2 outperforms OFA-L~\citep{ofa} by over 8\% accuracy across all tasks of RefCOCO/RefCOCO+/RefCOCOg. Compared with a strong baseline, Shikra (13B) ~\citep{shikra}, our model still shows better results, e.g., 84.29\% vs 83.96\% accuracy in average. These results provide direct evidence for  the competing visual grounding capabilities of MiniGPT-v2. Although our model underperforms specialist models, the promising performance indicates its growing competence in visual grounding.



\textbf{Ablation on task identifier.} We conduct ablation studies on the effect of the task identifier on the performance of MiniGPT-v2. We compare our model with the variant without using task identifiers on VQA benchmarks. Both models were trained on 4xA100 GPUs for 24 hours with an equal number of training steps for multiple vision-language tasks. Results in Table \ref{ablation} demonstrate the performance on multiple VQA benchmarks and consistently show that token identifier training benefits the overall performance of MiniGPT-v2. Specifically, our MiniGPT-v2 with task-oriented instruction training achieves 1.2\% top-1  accuracy improvement on average. 
These ablation results can validate the clear advantage of adding task identifier tokens and support the use of multi-task identifiers for multi-task learning efficiency.


\begin{figure}[t!]
    \centering
    \includegraphics[width=\textwidth]{image/demo.pdf}
    \caption{\textbf{Examples for various multi-modal capabilities of MiniGPT-v2.} We showcase that our model is capable of completing multiple tasks such as referring expression comprehension, referring expression generation, detailed grounded image caption, visual question answering, detailed image description, and directly parsing phrase and grounding from a given input text. }
    \label{fig:demo}
\end{figure}





















\noindent \textbf{Hallucination.} We measure the hallucination of our model on image description generation and compare the results with other vision-language baselines, including MiniGPT-4~\citep{zhu2023minigpt}, mPLUG-Owl~\citep{mplugowl}, LLaVA~\citep{llava}, and MultiModal-GPT~\citep{gong2023multimodal}. Following the methodology from \citep{li2023evaluating}, we use CHAIR~\citep{rohrbach2018object} to assess hallucination at both object and sentence levels. As shown in Table \ref{halluciniation}, we find that our MiniGPT-v2 tends to generate the image description with reduced hallucination compared to other baselines. We have evaluated three types of prompts in MiniGPT-v2. 
First, we use the prompt \textit{generate a brief description of the given image} without any specific task identifier which tends to produce more detailed image descriptions. Then we provide the instruction prompt \textit{[grounding] describe this image in as detailed as possible} for evaluating grounded image captions. Lastly, we prompt our model with \textit{[caption] briefly describe the image}. With these task identifiers, MiniGPT-v2 is able to produce a variety of image descriptions with different levels of hallucination. As a result, all these three instruction variants have lower hallucination than our baseline, especially with the task specifiers of \textit{[caption]} and \textit{[grounding]}.















\subsection{Qualitative Results}
We now provide the qualitative results for a complementary understanding of our model's multi-modal capabilities. Some examples can be seen in Fig. \ref{fig:demo}. Specifically, we demonstrated various abilities in the examples including a) object identification; b) detailed grounded image captioning; c) visual question answering; d) referring expression comprehension; e) visual question answering under task identifier; f) detailed image description; g) object parsing and grounding from an input text. More qualitative results can be found in the Appendix. These results demonstrate that our model has competing vision-language understanding capabilities. Moreover, notice that we train our model only with a few thousand of instruction samples on object parsing and grounding tasks at the third-stage, and our model can effectively follow the instructions and generalize on the new task. This indicates that our model has the flexibility to adapt on many new tasks.


Note that our model still occasionally shows hallucinations when generating the image description or visual grounding. e.g., our model may sometimes produce descriptions of non-existent visual objects or generate inaccurate visual locations of grounded objects. We believe training with more high-quality image-text aligned data and integrating with a stronger vision backbone or large language model hold the potential for alleviating this issue. 





















































%
