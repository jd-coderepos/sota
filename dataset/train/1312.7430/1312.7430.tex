\documentclass[11pt,letterpaper,english]{article}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[margin=1in]{geometry}
\usepackage[numbers]{natbib}
\setcitestyle{nocompress}
\bibpunct{(}{)}{;}{a}{}{,}

\usepackage{graphicx}
\usepackage[cmex10]{amsmath}
\usepackage{authblk}
\usepackage{macros}

\begin{document}
\title{Simultaneous Perturbation Methods for Adaptive Labor Staffing in
Service Systems}
\author[]{Prashanth L.A.}
\author[]{H.L. Prasad}
\author[\sharp\
\label{eqn:c-mp}
\begin{array}{l}
\textrm{Find } \min\limits_{\theta} J(\theta) \stackrel{\triangle}{=} \lim\limits_{n \rightarrow \infty}\frac{1}{n} \sum\limits_{m=0}^{n-1} c(X_m)\\
\textrm{subject to}\\
G_{i,j}(\theta) \stackrel{\triangle}{=} \lim\limits_{n \rightarrow \infty}\frac{1}{n} \sum\limits_{m=0}^{n-1} g_{i,j}(X_m) \le 0, \\\qquad\qquad\qquad\quad\forall i=1,\ldots,|C|, j=1,\ldots,|P|,\\
H(\theta) \stackrel{\triangle}{=} \lim\limits_{n \rightarrow \infty}\frac{1}{n} \sum\limits_{m=0}^{n-1} h(X_m) \le 0.
\end{array}

\label{eq:optimal-parameter-set}
\theta^* = \mathop{{\rm argmin}} \bigg\{ J(\theta) \text{ s.t. } \theta \in \D, G_{i,j}(\theta) \le 0, i=1,\ldots,|C|,j=1,\ldots,|P|,H(\theta) \le 0\bigg\},

X_{n}  = ( \N(n), u(n), \gamma'(n), q(n)),

\label{eqn:singlestagecost}
\begin{array}{l}
c(X_n)  =    r \times \left( 1 -  \sum_{i=1}^{|A|}\sum_{j=1}^{|B|} \alpha_{i,j} \times u_{i,j}(n) \right) + s \times \left( \dfrac{\sum_{i=1}^{|C|}\sum_{j=1}^{|P|} \left | \gamma'_{i,j}(n) - \gamma_{i,j} \right |}{|C|\times|P|}\right),
\end{array}
0 \le \alpha_{i,j} \le 1, \textrm{ and }\sum_{i=1}^{|A|}\sum_{j=1}^{|B|} \alpha_{i,j} = 1,
g_{i,j}(X_n) & = \gamma_{i,j} - \gamma'_{i,j}(n), \forall i=1,\ldots,|C|, j=1,\ldots,|P|,\label{eqn:sla-constraints}\
Here (\ref{eqn:sla-constraints}) specifies that the attained SLA levels should be equal to or above the contractual SLA levels for each customer-priority tuple. Further, (\ref{eqn:feasibility-constraint}) ensures that the SR queues for each complexity in the system stay bounded. In the constrained optimization problem formulated below, we attempt to satisfy these constraints in the long-run average sense (see (\ref{eqn:c-mp})).

The SASOC algorithms treat the parameter as continuous-valued and tune it accordingly. Let us denote this continuous version of the worker parameter by . Note that . We now design a smooth projection operator  that projects  on to the discrete space  so that the same can be used for performing the simulation of the service system. We call the -operator as a generalized projection scheme as it lies in between a fully deterministic projection scheme based on mere rounding off and a completely randomized scheme, whereby depending on the value of  (for any ) one can find points  and  with ,  such that  and  are the immediate neighbours of  in the set . Then, one sets the corresponding discrete parameter as

where, w.p.~ stands for `with probability'.

\subsection{A Generalized Projection Operator}
\label{sec:gammaproj}
For any  with , we define a projection operator
 which projects any  onto the discrete set  as follows:

For convenience, lets enumerate the elements of  as
 for some .
Let  be a fixed real number and  be such that  for some .
Let us consider an interval of length  around the midpoint of  and denote it as , where  and .
 Then, \\ for  is defined by

Further,  for  is
given by

In the above,  is any continuously
differentiable function defined on  such that  and . Note
that we deterministically project onto either  or  if
 is outside of the interval . Further,
for , we project randomly using a
smooth function . It is necessary to have a smooth projection operator to
ensure convergence of our SASOC algorithms as opposed to a deterministic
projection operator that would project  to  and  to . The problem with a
deterministic projection operator is that there is a jump at the midpoint of the interval and hence, when
extended for any  in the convex hull , the transition dynamics
of the process   is not continuously
differentiable. A non-smooth projection operator makes the dynamics non-smooth at the boundary
points.

The SASOC algorithms that we present subsequently tune the worker parameter in
the convex hull of , denoted by , a set that can be defined as
. This idea has been used in
\citep{shalabh2011stochastic} for an unconstrained discrete
optimization
problem. However, the projection operator used there was a fully randomized
operator. The generalized projection scheme that we incorporate has the
advantage that while it ensures that the transition dynamics of the parameter
extended Markov process is smooth (as desired), it requires a lower
computational effort because in a large portion of the parameter space (assuming
 is small), the projection operator is essentially deterministic.

We also require another projection operator  that projects any
 onto the set  and is defined as , where
, .
Thus,  keeps the parameter updates within the set 
and  projects them to the discrete set . The projected
updates are then used as the parameter values for conducting the simulation of
the service system.

\subsection{Assumptions}
We now make the following standard assumptions: One amongst (A2) and (A2')
will be assumed for the algorithms that follow.

\begin{description}
\item[\textbf{(A1)}] The Markov process  under a given dispatching policy and parameter  is ergodic.
\item[\textbf{(A2)}] The single-stage cost functions ,
 and  are all continuous. The long-run average cost
 and constraint functions  are twice
continuously differentiable with bounded third derivative.
\item[\textbf{(A2')}] The single-stage cost functions ,
 and  are all continuous. The long-run average cost
  and constraint functions  are continuously
differentiable with bounded second derivative .
\item[\textbf{(A3)}] The step-sizes ,  and 
satisfy
2ex]
\dfrac{b(n)}{d(n)}, \dfrac{a(n)}{b(n)} \rightarrow 0 \text{ as } n \rightarrow
\infty.
\end{array}

\nonumber
\max_{\lambda} \min_{\theta} L(\theta, \lambda) &\stackrel{\triangle}{=}& \lim_{n \rightarrow \infty}\frac{1}{n} \sum_{m=0}^{n-1} E\left \{ c(X_m) + \sum\limits_{i = 1}^{|C|} \sum\limits_{j = 1}^{|P|}\lambda_{i,j} g_{i,j}(X_m) + \lambda_f h(X_m) \right \}\\
\label{eqn:Lagrangian}
&=& J(\theta) + \sum\limits_{i = 1}^{|C|} \sum\limits_{j = 1}^{|P|}\lambda_{i,j} G_{i,j}(\theta)
+ \lambda_f H(\theta),

\label{eqn:gradestimate}
 \nabla_\theta L(\theta,\lambda) = \lim_{\delta\downarrow 0} E\left[ \left(
\frac{L(\theta +\delta\Delta,\lambda) -
L(\theta,\lambda)}{\delta}\right) \Delta^{-1}\right], 

\label{eqn:spsa-update-rule}
\left .\scalebox{0.96}{m=0,1,\ldots,K-1} \right \}
\theta^* = \theta_0 -[\nabla^2_\theta L (\theta_0)]^{-1} \nabla_\theta L(\theta_0),  \nabla_\theta L(\theta,\lambda) = \lim_{\delta_1,\delta_2\downarrow 0} E\left[ \left(
\frac{L(\theta +\delta_1\Delta+\delta_2\widehat{\Delta},\lambda) -
L(\theta,\lambda)}{\delta_2}\right) \widehat{\Delta}^{-1}\right],  \nabla_\theta^2 L(\theta,\lambda) = \lim_{\delta_1,\delta_2\downarrow 0}
E\left[ \Delta^{-1}
\left( \frac{L(\theta +\delta_1\Delta+\delta_2\widehat{\Delta},\lambda) -
L(\theta,\lambda)}{\delta_1\delta_2}\right) \left(\widehat{\Delta}^{-1}\right)^T\right], 
\label{eqn:hessian-update-rule}
\theta_{i}(n+1)  =  \bar\Gamma_i \left( \theta_{i}(n) + b(n)
\sum\limits_{j = 1}^{N} M_{i, j}(n) \left(\dfrac{\bar{L}(nK) - \bar{L}'(nK)}{\delta_2 \widehat\triangle_{j}(n)}
\right) \right),\\
H_{i, j}(n + 1) = H_{i, j}(n) + b(n) \left ( \dfrac{\bar{L}'(nK) - \bar{L}(nK)}{\delta_1 \triangle_{j}(n) \delta_2 \widehat\triangle_{i}(n)} - H_{i, j}(n) \right ),
(A + BCD)^{-1} = A^{-1} - A^{-1} B \left ( C^{-1} + D A^{-1} B \right )^{-1} D A^{-1}H(n + 1) = (1 - b(n)) H(n) +  P(n) Z(nK) Q(n)M(n + 1) = \left (\dfrac{M(n)}{1 - b(n)} \left [ I - \dfrac{b(n) \left ( \bar{L}'(nK) - \bar{L}(nK) \right ) P(n) Q(n) M(n)}{1 - b(n) + b(n) \left ( \bar{L}'(nK) - \bar{L}(nK) \right ) Q(n) M(n) P(n) } \right ] \right ),
\label{eqn:wudbury-update-rule}
\theta_{i}(n+1)  = & \bar\Gamma_i \left( \theta_{i}(n) + b(n)
\sum\limits_{j = 1}^{N} M_{i, j}(n) \left(\dfrac{\bar{L}(nK) - \bar{L}'(nK)}{\delta_1 \widehat\triangle_{j}(n)}
\right) \right),\\\nonumber
M(n + 1) = &\Upsilon \left (\dfrac{M(n)}{1 - b(n)} \left [ I - \dfrac{b(n) \left ( \bar{L}'(nK) - \bar{L}(nK) \right ) P(n) Q(n) M(n)}{1 - b(n) + b(n) \left ( \bar{L}'(nK) - \bar{L}(nK) \right ) Q(n) M(n) P(n) } \right ] \right ).

     p_\theta(i,j) = \sum\limits_{k=1}^{p} \beta_k(\theta) p_{D^k}(i,j), \quad \forall \theta \in \bar\D, i,j \in S,
\label{eq:pthetabar}

 (\beta_j(\theta_1),\beta_{j+1}(\theta_1)) =
  \begin{cases}
   (1,0) &  \text{if } \theta_1 \in\left[D^j,\tilde D_1 \right] \\
   (f(\frac{\tilde D_2 - \theta_1}{2\zeta}), 1- f(\frac{\tilde D_2 - \theta_1}{2\zeta})) & \text{if } \theta_1 \in \left[\tilde D_1, \tilde D_2\right]  \\
   (0,1) & \text{if } \theta_1 \in \left[\tilde D_2,D^{j+1}\right]
  \end{cases}

\label{eqn:sasoc-g:theta-ode}
\dot{\theta}(t) = \check{\Gamma}\left ( -\nabla_\theta L(\theta(t), \lambda)
\right
),

\label{eqn:Pi-bar-operator}
\check{\Gamma}(\epsilon(\theta(t))) = \lim\limits_{\eta \downarrow 0}
\dfrac{\Gamma(\theta(t) + \eta \epsilon(\theta(t))) - \theta(t)}{\eta}.

\label{eqn:sasoc-h:theta-ode}
\dot{\theta}(t) = \check{\Gamma}\left ( - \Upsilon(\nabla^2_\theta L(\theta(t),
\lambda))^{-1} \nabla_\theta L(\theta(t), \lambda) \right ).
\bar{K}^\lambda = \left \{ \theta \in S:
\dfrac{d L (\theta(t),
\lambda)}{dt} = - \nabla_{\theta} L (\theta(t), \lambda)^T
\Upsilon(\nabla^2_\theta L(\theta(t), \lambda))^{-1} \nabla_\theta L(\theta(t),
\lambda) = 0 \right \}.\begin{array}{l}
\dot{\lambda}_{i,j}(t) = \check\Pi \left ( G_{i, j}(\theta^*) \right ), \forall
i
=
1, 2, \dots, |C|, j = 1, 2, \dots, |P|,\
where  is the converged parameter value of SASOC-G/H corresponding to
Lagrange parameter , and for any bounded continuous
functions ,  
Here again, the projection
operator  ensures that the evolution of each component
of  stays non-negative.
From the definition of the Lagrangian given in (\ref{eqn:Lagrangian}),
the gradient of the Lagrangian w.r.t.  can be seen to  be
 and that w.r.t.  is . Thus, the
above ODEs suggest that in SASOC-G/H s' and  are
ascending in the Lagrangian value and converge to a local maximum point.
We now have the following result:
 
\begin{theorem}
\label{theorem:sasoc-g-lambda}
Let  Then,  for some  w.p. 1 as .
\end{theorem}

\subsection*{Step 5: Convergence to a locally saddle point}

Finally, we argue that the algorithm indeed converges to a (local) saddle
point of the Lagrangian.
Suppose  denote a local neighborhood in which  is
a minimum. Then, through an application of the envelope theorem of mathematical economics \citep[pp.
964-966]{mas1995microeconomic}, applied in the `Caratheodory sense' \citep[Lemma 4.3, pp.211]{borkar2005actor},
it can be seen that

where  is some local neighborhood that contains .
The SASOC algorithms thus converge to a locally saddle point. As mentioned
at the beginning of this section, the detailed proofs of the above results
are available in an attached supplementary file.

\section{Simulation Experiments}
\label{sec:simulation}


\begin{figure}
\centering
\begin{tabular}{l}
    \subfigure[Total work volume statistics for each SS]
    {
	
	\tabl{c}{\scalebox{1.0}{\begin{tikzpicture}
	\begin{axis}[
ybar stacked,
	legend style={at={(0.5,-0.15)},anchor=north,legend columns=-1},
	legend image code/.code={\path[fill=white,white] (-2mm,-2mm) rectangle (-3mm,2mm); \path[fill=white,white] (-2mm,-2mm) rectangle (2mm,-3mm); \draw (-2mm,-2mm) rectangle (2mm,2mm);},
	ylabel={Total Hours/ SW/ Day},
	symbolic x coords={1, SS1, SS2, SS3, SS4, SS5, 2},
	xmin={1},
	xmax={2},
	xtick=data,
	ytick align=outside,
	bar width=18pt,
grid,
	grid style={gray!20},
	width=10cm,
	height=8cm,
	]
	\addplot+[ybar,pattern=horizontal lines] coordinates {(SS1,3.8) (SS2,5.3) (SS3,2.7) (SS4,0.7) (SS5,7.6)}; \addplot+[ybar,pattern=north east lines] coordinates {(SS1,2.7) (SS2,7) (SS3,5.8) (SS4,4.9) (SS5,5.8)}; \legend{Customer SRs, Internal SRs}
	\end{axis}
	\end{tikzpicture}}\1ex]}
	\label{fig_servicetime}
    } 
\end{tabular}
\caption{Characteristics of the service systems used for simulation}
\end{figure}
 \begin{figure}
    \centering
    \begin{tabular}{c}
    \subfigure[SS1 and SS2 work arrival pattern]
    {
                \begin{tabular}{l}
            \tabl{c}{\includegraphics[width=4.5in]{images/arrivals_ss12.png}\2ex]}
            \end{tabular}
            \label{fig_arrivals_ss45}
    }
    \end{tabular}
\caption{Work arrival patterns over a week for each SS}
    \label{fig_arrivals}
\end{figure}


We use the simulation framework developed in
\citep{banerjee2011simulation} for implementing all our algorithms. 
A number of dispatching policies have been developed in
\citep{banerjee2011simulation}. In particular, we study the PRIO-PULL
and EDF policies for performance comparisons of the various
algorithms. In addition to the three SASOC algorithms, we implemented an
algorithm that uses the state-of-the-art optimization tool-kit OptQuest, for the sake of comparison. 
OptQuest employs an
array of techniques including scatter and tabu search, genetic
algorithms, and other meta-heuristics for the purpose of optimization
and is quite well-known as a hybrid search tool for solving simulation optimization
problems (\citep{laguna1998optimization}). In particular, we have
used the scatter search variant of OptQuest for our experiments.


We choose five real-life SS from two different countries providing
server support to IBM's customers. The five SS cover a
variety of characteristics such as high vs. low workload, small
vs. large number of customers to be supported, small vs. big staffing
levels, and stringent vs. lenient SLA constraints. Collectively, these five SS staff more than 200 SWs with , , and  of them having low, medium, and high skill level, respectively. Also, these SS support more than  customers each, who make more than  SRs every week with each customer having a distinct pattern of arrival depending on its business hours and seasonality of business domain.
 Figure \ref{fig_average_workload} shows the total work hours per SW per day
for each of the SS. The bottom part of the bars denotes customer SR work,
i.e., the SRs raised by the customers whereas the top part of the bars
denotes internal SR work, i.e, the SRs raised internally for overhead work
such as meetings, report generation, and HR activities.  This
segregation is important because the SLAs apply only to customer SRs. Internal SRs
do not have deadlines but they may contribute to queue growth.  Note
that while average work volumes are significant, they may not directly
correlate to SLA attainment. Figure \ref{fig_servicetime} shows the effort data,
i.e., the mean time taken to resolve an SR (a lognormal distributed random variable in our setting)
across priority and complexity classes.
 As shown in Figures \ref{fig_arrivals}, the
arrival rates for SS4 and SS5 show much higher peaks than SS1, SS2,
and SS3, respectively, although their average work volumes are comparable. The
variations are significant because during the peak periods, many SRs
may miss their SLA deadlines and influence the optimal staffing
result.

Some of the specific details of the service system
setting (see Section \ref{sec:formulation}) are as follows:
, the set of time intervals, contains one element for each hour of the
week. Hence,  = . 
The set of priority levels, , where, . The set of skill levels  is ,
where, High  Medium  Low. The simulation framework also involves the
swing and preemption policies and the reader is referred to
\citep{banerjee2011simulation} for a detailed description of this.

We implemented our
SASOC algorithms on the simulation framework from
\citep{banerjee2011simulation} for both the perturbed and the
unperturbed simulations (see  and  computations in
Algorithm~\ref{algorithm:sasoc-g-complete-algorithm}).
For our SASOC algorithms, the
simulations were conducted for  iterations, with each iteration
having  simulation replications - ten each with unperturbed
parameter  and perturbed parameter ,
respectively.  Each replication simulated the operations of the
respective SS for a  day period.  Thus, we set  and 
for SASOC algorithms. On the other hand, for the OptQuest algorithm,
simulations were conducted for  iterations, with each iteration
of  replications of the SS.

For all the SASOC algorithms, we set the weights
in the single-stage cost function , see
(\ref{eqn:singlestagecost}), as . We thus give equal
weightage to both the worker utilization and the SLA over-achievement
components. The indicator variable  used in the constraint
(\ref{eqn:feasibility-constraint}) was set to  (i.e., infeasible)
if the queues were found to grow by  over a two-week period
during simulation. We performed a sensitivity study for the paramter 
and found that the choice of  gave the best results. For the second order
methods, the perturbation control parameters  and  were
both set to . The function  in the generalized projection operator was
set as , with the parameter . Each of the experiments were run on a machine with dual core
Intel
 GHz processor and  GB RAM.

The
 operator implemented for SASOC-W can be described as
follows. Let  be the Hessian update which needs to be
projected. The following sequence of operations represent this
projection. \begin{inparaenum}[(i)] \item ; \item Perform eigen-decomposition
  on  to get all eigen-values and corresponding eigen-vectors; \item Project each eigen-value to  where . 
  is chosen to be a small number so as to allow for larger range of
  values, but not too small to avoid singularity. The upper limit in
  the projection range is to avoid singularity of the inverse of the
  Hessian estimate; and \item Reconstruct  using the
  projected eigen-values but with same eigen-vectors. \end{inparaenum}
The  operator in the case of SASOC-H
with diagonal Hessian is one that simply projects each diagonal entry to . It is easy to see that the
 operator satisfies assumption \textit{(A4)}. For a closely related
modification of the Hessian, the reader is referred to \citep{gill1981practical}.
In our experiments, we
set .

On each SS, we compare our SASOC algorithms with the OptQuest
algorithm using  and mean utilization as the performance metrics. Here  is
the sum of workers across shifts and skill levels. The mean utilization here refers to a weighted average of the utilization percentage achieved for each skill level, with the weights being the fraction of the workload corresponding to each skill level.

As evident in Figures \ref{fig_arrivals_ss123} and \ref{fig_arrivals_ss45}, the SS pools SS1, SS2 and SS3 are characterized by a flat SR arrival pattern, whereas SS4 and SS5 are characterized by a bursty SR arrival pattern. We present and analyze the results on these pools separately, starting with the flat arrival pools in the next section.

\subsection{Flat-Arrival SS pools}


\begin{figure}
\begin{minipage}[c][\textheight]{\textwidth}
    \centering
    \begin{tabular}{c}
    \subfigure[ for PRIO-PULL]
    {
\tabl{c}{\scalebox{1.0}{\begin{tikzpicture}
\begin{axis}[
ybar={2pt},
legend style={at={(0.5,-0.15)},anchor=north,legend columns=-1},
legend image code/.code={\path[fill=white,white] (-2mm,-2mm) rectangle (-3mm,2mm); \path[fill=white,white] (-2mm,-2mm) rectangle (2mm,-3mm); \draw (-2mm,-2mm) rectangle (2mm,2mm);},
ylabel={},
symbolic x coords={1, SS1, SS2, SS3, 2},
xmin={1},
xmax={2},
xtick=data,
ytick align=outside,
bar width=16pt,
nodes near coords,
grid,
grid style={gray!20},
width=11cm,
height=9cm,
]
\addplot[pattern=horizontal lines] coordinates {(SS1,124) (SS2,0) (SS3,74)}; \addplot[pattern=vertical lines]   coordinates {(SS1,68) (SS2,67) (SS3,76)}; \addplot[pattern=north east lines] coordinates {(SS1,49) (SS2,63) (SS3,76)};
\legend{OptQuest, SASOC-SPSA, SASOC-H}
\end{axis}
\end{tikzpicture}}\1ex]}

                                \label{fig_wsum_edf_ss123}
    }    \end{tabular}
    \caption[Performance of OptQuest and SASOC algorithms]{Performance of OptQuest and SASOC algorithms on SS1, SS2 and SS3\footnote{Note: OptQuest is infeasible over SS2}}
    \label{fig_priopull_ss123}
\end{minipage}
\end{figure}
\begin{figure}
    \centering
    \begin{tabular}{c}
    \subfigure[ for PRIO-PULL]
    {
\tabl{c}{\scalebox{1.0}{\begin{tikzpicture}
\begin{axis}[
ybar={2pt},
legend style={at={(0.5,-0.15)},anchor=north,legend columns=-1},
legend image code/.code={\path[fill=white,white] (-2mm,-2mm) rectangle (-3mm,2mm); \path[fill=white,white] (-2mm,-2mm) rectangle (2mm,-3mm); \draw (-2mm,-2mm) rectangle (2mm,2mm);},
ylabel={},
symbolic x coords={1, SS4, SS5, 2},
xmin={1},
xmax={2},
xtick=data,
ytick align=outside,
bar width=16pt,
nodes near coords,
grid,
grid style={gray!20},
width=11cm,
height=9cm,
]
\addplot[pattern=horizontal lines] coordinates {(SS4,35) (SS5,39) }; \addplot[pattern=vertical lines]   coordinates {(SS4,44) (SS5,46) }; \addplot[pattern=north east lines] coordinates {(SS4,53) (SS5,55) }; \legend{OptQuest, SASOC-SPSA, SASOC-H}
\end{axis}
\end{tikzpicture}}\1ex]}

                                \label{fig_wsum_edf_ss45}
    }    \end{tabular}
    \caption{Performance of OptQuest and SASOC for two different dispatching
policies on SS4 and SS5}
    \label{fig_priopull_ss45}
\end{figure}

\begin{figure}
\begin{minipage}[c][\textheight]{\textwidth}
    \centering
\tabl{c}{\scalebox{0.8}{\begin{tikzpicture}
\begin{axis}[
ybar={2pt},
legend style={at={(0.5,-0.15)},anchor=north,legend columns=-1},
legend image code/.code={\path[fill=white,white] (-2mm,-2mm) rectangle
(-3mm,2mm); \path[fill=white,white] (-2mm,-2mm) rectangle (2mm,-3mm); \draw
(-2mm,-2mm) rectangle (2mm,2mm);},
ylabel={},
symbolic x coords={1, SS1, SS4, 2},
xmin={1},
xmax={2},
xtick=data,
ytick align=outside,
bar width=16pt,
nodes near coords,
grid,
grid style={gray!20},
width=16cm,
height=11cm,
]
\addplot[pattern=horizontal lines]   coordinates {(SS1,68) (SS4,45) }; \addplot[pattern=vertical lines] coordinates {(SS1,63) (SS4,56) }; \addplot[pattern=grid] coordinates {(SS1,67) (SS4,80) }; \addplot[pattern=north east lines] coordinates {(SS1,63) (SS4,74) }; 

\legend{SASOC-SPSA, SASOC-H, SASOC-SF-N, SASOC-SF-C}
\end{axis}
\end{tikzpicture}}\1ex]}
\label{fig:mean-util}
\caption{Performance of OptQuest and SASOC for EDF dispatching policy. The mean utilization values have been rounded to nearest integer.}
\end{figure}

Figures \ref{fig_wsum_priopull_ss123} and \ref{fig_wsum_edf_ss123} compare the
 achieved for OptQuest and SASOC algorithms using PRIO-PULL
and EDF on three real life SS with a flat SR arrival pattern (see Figure \ref{fig_arrivals_ss123}). 
Here  denotes the value
obtained upon convergence of . On these SS pools, namely
SS1, SS2 and SS3, respectively, we observe that our SASOC algorithms
find a better value of  as compared to OptQuest.
Note in particular that on SS1, SASOC algorithms perform significantly better than OptQuest with an improvement of nearly .
Further, on SS2, OptQuest is seen to be infeasible whereas all the
SASOC algorithms obtain a feasible and good allocation.

It is evident that SASOC algorithms consistently outperform the OptQuest algorithm on these SS pools.
Further, among the SASOC algorithms, we observe that SASOC-W finds
better solutions in general as compared to the other two SASOC
algorithms. Further, we observe that in all our experiments that include both flat as well as bursty arrival pools, the optimal worker parameter obtained by all our SASOC algorithms is feasible, i.e., satisfies both the SLA as well as the queue stability constraints.

Figure \ref{fig_wsum_edf_ss123} presents similar results for the case of the
EDF dispatching policy. The behavior of OptQuest and SASOC algorithms
was found to be similar to that of PRIO-PULL with SASOC showing
performance improvements over OptQuest here as well.


\subsection{Bursty-Arrival SS pools}

Figures \ref{fig_wsum_priopull_ss45} and \ref{fig_wsum_edf_ss45} compare the
 achieved for OptQuest and SASOC algorithms using PRIO-PULL
and EDF on two real life SS with a bursty SR arrival pattern (see Figure \ref{fig_arrivals_ss45}).
From these performance plots, we observe that OptQuest is seen to be slightly better than SASOC-G and
SASOC-W when the underlying dispatching policy is PRIO-PULL, whereas in the case of EDF dispatching policy,
the SASOC algorithms clearly outperform OptQuest. The execution time advantage of SASOC algorithms over OptQuest hold in the case of these pools as well.

Computational efficiency is a significant factor for any adaptive labor staffing algorithm. For instance, if a candidate labor staffing algorithm takes too long to find the optimal staffing levels, it is not amenable for making staffing changes in a real SS. Both from the number of simulations required as well as the wall clock run time standpoints, SASOC algorithms are better than OptQuest. This is because
OptQuest requires  iterations with each iteration of 
replications, whereas the SASOC algorithms require 
iterations of  replications each in order to find .
This results in a  speedup for SASOC algorithms and also manifests in the wall clock runtimes of SASOC algorithms because simulation run-times are
proportional to the number of SS simulations. We observe that the SASOC
algorithms result in at least  to  times improvement as compared to OptQuest from the wall clock runtimes perspective. For
instance, on SS1 the typical run-time of OptQuest was found to be 
hours, whereas SASOC algorithms took less than  hours each to converge.

In fact, we observed in the case of SS2, OptQuest does not find a feasible solution even after repeated runs  for  search iterations. Also, because OptQuest
depends heavily on SLA attainments and respective confidence intervals
of previous iterations, it requires higher number of replications than
SASOC.  Further, we observed that SASOC algorithms converge within  iterations in all our experiments. Thus, SASOC algorithms require  times less number of simulations as compared to OptQuest, while searching for the optimal SS
configuration. This runtime advantage ensures that an SS manager can make staffing changes even at the granularity of every week by making use of SASOC algorithms and the same may not be possible with OptQuest due to its longer runtimes. 

\subsection{Comparsion with SF approaches}
Figure \ref{fig:sf-compare} compares the  achieved with EDF as the
dispatching policy for the SASOC algorithms with the smoothed functional (SF)
based schemes from \citep{prashanth2011ss}. We observe that the SASOC algorithms
perform on par with the Cauchy variant (SASOC-SF-C), while performing better
than the Gaussian variant of the algorithm from \citep{prashanth2011ss}. An
important advantage with our SASOC algorithms in comparison with the SF based
approaches, especially the Cauchy variant, is the low computational overhead. While
our algorithms require Bernoulli random variable for perturbing the worker
parameter, the SF approaches require Gaussian or Cauchy random variables for
the same. Further, the second order method that we propose here (SASOC-H) is
more robust in comparison to the first order SF approaches and through the use
of Woodbury's identity, we also achieve low computational overhead as well.

\subsection{Empirical Convergence of }
We observe that the parameter  (and hence )
converges to the optimum value for each of the SS pools
considered. This is illustrated by the convergence plots in
Figures \ref{fig_bra06_pp} and \ref{fig_arg05_edf}. This is a
significant feature of SASOC as our algorithms are seen to converge analytically
(see Section \ref{sec:convergence}) and the plots confirm the
same. In contrast, the OptQuest algorithm is not proven to converge to
the optimum even after repeated runs, as illustrated in the case of SS2 in Figure
\ref{fig_wsum_priopull_ss123}.

\subsection{Mean utilization results}
We present the
utilization percentages across different skill levels (low, medium and high) in 
Figure \ref{fig:mean-util}. The underlying dispatching policy here is EDF. The results for the case of PRIO-PULL are similar. We observe mean utilization of workers is a crucial factor for a labor staffing algorithm and it is evident from Figure \ref{fig:mean-util} that SASOC algorithms exhibit a higher mean utilization of workers and hence, better overall performance in comparison to the OptQuest algorithm.

From the above performance comparisons over SS pools with flat as well as bursty SR arrival patterns,
it is evident that our SASOC
algorithms, which converge to a local saddle point, show overall better performance in comparison with the scatter search-based algorithm of OptQuest. Among the SASOC algorithms, we observe that the second order algorithms (SASOC-H and SASOC-W) perform
better than the first order algorithm (SASOC-G) in many cases, with
SASOC-W being marginally better than SASOC-H.

\section{Conclusions}
\label{sec:conclusion}
We motivated the discrete optimization problem of adaptively determining optimal staffing
levels in SS and proposed two novel SASOC algorithms for
solving this problem.  The aim was to find an optimum
worker parameter that minimizes a certain long-run cost objective, while
adhering to a set of constraint functions, which are also long run averages.
All SASOC algorithms are simulation-based optimization methods as the
single-stage cost and constraint functions are observable only via simulation
and no closed form expressions are available.
For solving
the constrained optimization problem, we applied the Lagrange
relaxation procedure and used an SPSA based scheme for performing
gradient descent in the primal and at the same time, ascent in the
dual, for the Lagrange multipliers. All SASOC algorithms also incorporated a smooth (generalized) projection operator that helped imitate a continuous parameter system with suitably defined transition dynamics. Using the theory of
multi-timescale stochastic approximation, we presented the convergence
proof of our algorithms.  Numerical experiments were performed to
evaluate each of the algorithms based on real-life SS data against the
state-of-the-art simulation optimization toolkit OptQuest in the
current context.  SASOC algorithms in general showed overall superior performance
compared to OptQuest, as they (a) exhibited more than an order of
magnitude faster convergence than OptQuest, (b) consistently found
solutions of good quality and in most cases better than those found by
OptQuest, and (c) showed guaranteed convergence even in scenarios
where OptQuest did not find feasibility even after repeated runs for  iterations.
Given the quick convergence of SASOC algorithms (in minutes), they are
particularly suitable for adaptive labor staffing where a few days of
optimization run like in OptQuest would fail to keep up with the
changes.  By comparing the results of the SASOC algorithms on two
independent dispatching policies, we showed that SASOC's performance
is independent of the operational model of SS.

As future work, one may consider single-stage cost function enhancements that
include worker salaries as well as other relevant monetary costs, apart from staff
utilization and SLA attainment factors. 
An orthogonal direction of future work in this context is to
develop skill updation algorithms, i.e., derive novel work dispatch policies
that improve the skills of the workers beyond their current levels by way of
assigning work of a higher complexity. However, the setting is still constrained
and the SLAs would need to be met while improving the skill levels of the
workers. The skill updation scheme could then be combined  with the SASOC
algorithms presented in this paper to optimize the
staffing levels on a slower timescale.

\newpage
\appendix
\section{Appendix: Convergence Analysis}
Here we provide a sketch of the convergence of the SASOC-G and SASOC-H algorithms. The first step in the convergence analysis is common to all the SASOC algorithms and involves the extension of the transition dynamics  of the constrained parameterized hidden Markov process to the convex hull .

\subsection*{Extension of the transition dynamics }
Recall that the discrete parameter  of the Markov process  takes values in the set  defined earlier. Using the members of , one can extend the transition dynamics  of the underlying Markov process to any  in the convex hull  as follows:

where the weights  satisfy  and .  can be seen to satisfy the properties of transition probabilities. It is worth noting here that the weights  must be continuously differentiable in order to ensure that the extended transition probabilities are continuously differentiable as well and our SASOC algorithms converge. Moreover, in the SASOC algorithms, we do not require an explicit computation of these weights while trying to solve the constrained optimization problem equation (1) of the main paper. Consider the case when  and suppose  lies between  and  (both members of ). By construction,  will correspond to the probability with which projection is done on  and is obtained using the -projection operator as follows:  Let us consider an interval of length  around the midpoint of  and denote it as , where  and .
Then, the weights  are set in the following manner:
 and  is given by: 


In the above,  is obtained from the definition of -projection and hence, is a continuously differentiable function defined on  such that  and . The above can be similarly extended when the parameter  has  components. It can thus be seen that  are continuously differentiable functions of . Thus, from  \eqref{eq:pthetabar} and the fact that  are continuously differentiable, it can be seen that the extended transition dynamics  are continuously differentiable.

We now claim the following:


\begin{lemma}
\label{lemma:markov}
    For any ,  is ergodic Markov.
\end{lemma}
\begin{proof}
    Follows in a similar manner as Lemma 2 of \cite{shalabh2011stochastic}.
\end{proof}

Now, define analogues of the long-run average cost and constraint functions for any  as follows:

The difference between the above and the corresponding entitites defined in equation (1) of the main paper is that  can take values in  in the above. In lieu of Lemma 2, the above limits are well-defined for all .

\begin{lemma}
 and  are continuously differentiable in .
\end{lemma}
\begin{proof}
    Follows in a similar manner as Lemma 3 of \cite{shalabh2011stochastic}.
\end{proof}

We now prove the SASOC algorithms described previously are equivalent to their analogous continuous parameter  counterparts under the extended Markov process dynamics.
 
\begin{lemma}
\label{lemma:sasocequivalence}
Under the extended dynamics  of the Markov process  defined over all , we have
\begin{enumerate}[(i)]
    \item SASOC-G algorithm is analogous to its continuous counterpart where  and  are replaced by  and  respectively.
    \item SASOC-H algorithm is analogous to its continuous counterparts where  and  are replaced by  and  respectively.
\end{enumerate}

\end{lemma}
\begin{proof}
\textbf{(i)}:
Consider the SASOC-G algorithm which updates according to  equation (11) of the main paper. Let 
be a given parameter update that lies in  (where  denotes
the interior of the set ). Let  be sufficiently small so that

.

Consider now the -projected parameters
 and
, respectively.
By the construction of the generalized projection operator,
these parameters are equal to  with probabilities

and
, respectively.
When the operative parameter is , the transition probabilities are
, . Thus with probabilities

and
, respectively,
the transition probabilities in the two simulations equal ,
.

Next, consider the alternative (extended) system with parameters 
and , respectively. The transition probabilities are now given by

, . Thus with probability , a transition
probability of  is obtained in the th system.
Thus the two systems (original and the one with extended dynamics) are analogous.

Now consider the case
when , i.e., is a point on the boundary of ).
Then, one or more components of  are extreme points. For simplicity,
assume that only one component (say the th component) is an extreme point as the
same argument carries over if there are more parameter components that are extreme
points. By
the th component of  being an extreme
point, we mean that  is either  or .
The other components  are not extreme.
Thus,  can lie outside of the interval . For instance, suppose
that  and that 
(which will happen if ). In such a case,
 with probability one.
Then, as before,  can be written as the convex combination
 and
the rest follows as before.

\textbf{(ii)}: Follows in a similar manner as part (i) above. 
\end{proof}

As a consequence of Lemma \ref{lemma:sasocequivalence}, we can analyze the SASOC algorithms with the continuous parameter  used in place of  and under the extended transition dynamics \eqref{eq:pthetabar}. By an abuse of notation, we shall henceforth use  to refer to the latter. 

\subsection*{SASOC-G}
 The convergence analysis of SASOC-G can be split into four stages:

\begin{inparaenum}[\bfseries (I)]
\hspace{-1em}\item The fastest time-scale in SASOC-G is  which is used to update the Lagrangian estimates  and  corresponding to simulations with  and  respectively. Firstly, we show that these estimates indeed converge to the Lagrangian values  and  defined in equation (9) of the main paper. Note that the  and  which are updated on slower time-scales, can be assumed to be time invariant quantities for the purpose of analysis of these Lagrangian estimates.\\
\item Next, we show that  the parameter updates  using SASOC-G converge to a limit point of the ODE

where  is defined as follows: For any bounded continuous function ,

The projection operator  ensures that the evolution of  stays within the bounded set . Again for the analysis of the -update, the value of  which is updated on the slowest time-scale is assumed constant.\\
\item We show that
s and  converge respectively
to the limit points of the ODEs 1ex]
\dot{\lambda}_f(t) = \check\Pi \left ( H(\theta^*) \right ),
\end{array}\check\Pi(\bar{\epsilon}(\lambda(t))) =
\lim\limits_{\eta \downarrow 0} \dfrac{(\lambda(t) + \eta
\bar{\epsilon}(\lambda(t)))^+ - \lambda(t)}{\eta}.l(X_m) \stackrel{\triangle}{=} c(X_{nK+m}) + \sum\limits_{i=1}^{|C|}\sum\limits_{j=1}^{|P|} \lambda_{i,j}(nK) g_{i,j}(X_{nK+m}) + \lambda_f h(X_{nK+m}).\bar{L}(m + 1) = \bar{L}(m) + d(m)\left ( L(\theta(m),\lambda(m)) + \xi_1(m) - \bar{L}(m) + M_{m + 1} \right ), 
W_{i}(n + 1) = W_{i}(n) - b(n) \left ( \frac{L(\theta(n) + \delta \Delta_{i}(n), \lambda) - L(\theta(n), \lambda)}{\delta \Delta_{i}(n)}  \right) + b(n) \chi_{n+1} ,
\label{weq}
F^{\theta^*} = \left \{ \lambda \ge 0 : \check\Pi \left ( G_{i, j}(\theta^*) \right ) = 0, \forall i = 1, 2, \dots, |C|, j = 1, 2, \dots, |P|; \check\Pi \left ( H(\theta^*) \right ) = 0 \right \}.\lambda_{i, j}(n + 1) = \lambda_{i, j}(n) + a(n) \left [ G_{i, j}(\theta^*) + N_{n + 1} + M_{n + 1} \right ],
\hspace{-2em}\|\bar{L}(n) - L(\theta(n), \lambda(n)) \|, \|\bar{L}'(n) - L(\theta(n) + \delta_1 \Delta(n) + \delta_2 \widehat\Delta(n), \lambda(n)) \| \rightarrow 0 \textrm{ as } n \rightarrow \infty.

\label{eqn:sasoc-h:theta-ode}
\dot{\theta}(t) = \check{\Gamma}\left ( - \Upsilon(\nabla^2_\theta L(\theta(t), \lambda))^{-1} \nabla_\theta L(\theta(t), \lambda) \right ),
\left \| \dfrac{L(\theta(n) + \delta_1 \Delta(n) + \delta_2 \widehat\Delta(n), \lambda(n)) - L(\theta(n),\lambda(n))}{\delta_2 \widehat\Delta_i(n)} - \nabla_{\theta_{i}} L(\theta(n), \lambda(n)) \right \| \rightarrow 0\textrm{ w.p. 1},
\left \| \dfrac{L(\theta(n) + \delta_1 \Delta(n) + \delta_2 \widehat\Delta(n), \lambda(n)) - L(\theta(n),\lambda(n))}{\delta_1 \Delta_i(n) \delta_2 \widehat\Delta_j(n)} - \nabla^2_{\theta_{i, j}} L(\theta(n), \lambda(n)) \right \| \rightarrow 0\textrm{ w.p. 1},
\left \| H_{i, j}(n) - \nabla^2_{\theta_{i, j}} L(\theta(n), \lambda(n)) \right \| \rightarrow 0\textrm{ w.p. 1},
\left \| M(n) - \Upsilon(\nabla^2_{\theta} L(\theta(n), \lambda(n)))^{-1} \right \| \rightarrow 0\textrm{ w.p. 1},
\bar{K}^\lambda = \left \{ \theta \in S: \dfrac{d L (\theta(t), \lambda)}{dt} = - \nabla_{\theta} L (\theta(t), \lambda)^T \Upsilon(\nabla^2_\theta L(\theta(t), \lambda))^{-1} \nabla_\theta L(\theta(t), \lambda) = 0 \right \}. \theta_{n + 1} = \Pi \left ( \theta_n - b(n) \Upsilon(\nabla^2_\theta L(\theta(t), \lambda))^{-1} \nabla_\theta L(\theta(t), \lambda) + b(n) \chi_n \right )\left \| M(n) - \Upsilon(\nabla^2_{\theta} L(\theta(n), \lambda(n)))^{-1} \right \| \rightarrow 0\textrm{ w.p. 1},

with  as .
\begin{proof}
{\rm
From Woodbury's identity, since  sequence of SASOC-W is identical to the  sequence of SASOC-H, the result follows from Lemma \ref{lemma:inverse-hessian}.
}
\end{proof}
\end{lemma}
 
\bibliographystyle{plainnat}

\bibliography{sasoc-simulation}

\end{document}
