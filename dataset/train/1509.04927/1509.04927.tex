\documentclass[12pt,twoside,a4paper]{article}
\ProvidesPackage{dina4}
\usepackage{rawfonts,amsmath,amsfonts,amssymb,epsfig,color}
\textwidth14.8cm
\oddsidemargin1.2cm
\evensidemargin0.4cm
\catcode`\@=11
\def\QED{\hbox{\hskip 1pt \vrule width4pt height 6pt depth 1.5pt \hskip 1pt}}




\newtheorem{theo}{Theorem}
\newtheorem{algo}{Algorithm}
\newtheorem{lemma}{Lemma}
\def\DOM{{\rm DOM}}
\def\lev{{\rm level}}
\def\plev{{\rm potlevel}}
\def\pred{{\rm Pred}}
\def\POS{{\rm POS}}

\title{Maximum Matching in General Graphs Without Explicit Consideration 
of Blossoms Revisited}

\author{Norbert Blum\thanks{Institut f\"ur Informatik, Universit\"at Bonn, Friedrich-Ebert-Allee 144, D-53113 Bonn, Germany.
    e-mail:{ \tt blum@cs.uni-bonn.de}}}
\begin{document}
\date{}
\maketitle
\begin{abstract}
\noindent 
We reduce the problem of finding an augmenting path in a general
graph to a reachability problem in a directed bipartite graph. 
A slight modification of depth-first search leads to an algorithm for 
finding such paths.
Although this setting is equivalent to the traditional terminology of 
blossoms due to Edmonds, there are some advantages. Mainly, 
this point of view enables the description of algorithms
for the solution of matching problems without explicit analysis of
blossoms, nested blossoms, and so on. Exemplary, we describe an efficient
realization of the Hopcroft-Karp approach for the computation of a 
maximum cardinality matching in general graphs and a variant of 
Edmonds' primal-dual algorithm for the maximum weighted matching problem.
\end{abstract}


\section{Introduction and motivation}

Since Berge's theorem in 1957 \cite{Be} it has been well known that
for constructing a maximum matching, it suffices to search for augmenting
paths. But until 1965, only exponential algorithms for finding a maximum 
cardinality matching in non-bipartite graphs have been known. The reason was that 
one did not know how to treat odd cycles, the so-called ``blossoms'', in 
alternating paths.
In his pioneering work, Edmonds \cite{Ed1} solved this problem by shrinking
these odd cycles. Because each shrinking reduces the number of nodes in the
current graph at least by two, the total number of shrinkage's is bounded by
 where  is the number of nodes in the graphs. Hence, Edmonds
original algorithm uses only  time.
In \cite{Bal,Gab,La,WZ}, it is shown how to avoid explicit
shrinking of odd cycles. All these algorithms use  or  
time where  is the number of edges in the graph and  is the functional 
inverse of Ackermann's function.

In 1973, Hopcroft and Karp \cite{HK} proved the following fact. If one computes
in one phase a maximal set of pairwise disjoint shortest augmenting paths and augments 
these paths then  such
phases would be sufficient. For the bipartite case they showed that a phase can
be implemented using a breadth-first search followed by a depth-first search.
This led to an  implementation of one phase and hence, to an 
algorithm for the computation of a maximum matching in bipartite graphs.
The implementation of a phase for non-bipartite graphs is much harder.
In 1975, Even and Kariv \cite{EK,Ka} presented an 
implementation of a phase leading to an  
algorithm for the computation of a maximum matching in general graphs.
Galil \cite{Gal} called the full paper \cite{Ka} ``a strong contender for the ACM 
Longest Paper Award''. Tarjan \cite{Ta} called their paper ``a remarkable 
tour-de-force''.
In 1978, Bartnik \cite{Bar} gave an alternative  implementation
in his unpublished Ph.D.~thesis (see \cite{GoMi}).
In 1980, Micali and Vijay Vazirani \cite{MV} have presented an 
implementation of a phase without the presentation of a correctness proof. 
The algorithm as presented in \cite{MV} is not correct since their definition
of ``tenacity'' does not work in all cases. In 1988, Peterson and Loui
have given an informal correctness proof of the incorrect algorithm of Micali
and Vazirani. Fourteen years later, Vijay Vazirani \cite{Va1} has repaired the 
mistake by changing the definition of ``tenacity'' and provided a correctness proof. 
Recently \cite{Va2}, Vazirani has presented a new version of a correctness proof.
With respect to his proof in \cite{Va1}, he writes: ``Although the statements of these theorems
were largely correct, their proofs, which involved low level arguments about individual
paths and their complicated intersections with other structures, were, in retrospect,
incorrect.'' In 1999 \cite{Bl3}, I have tried to
combine ideas of Micali and Vazirani and my framework. As pointed out by Ross McConnell
\cite{Mc}, the algorithm, as described in \cite{Bl3}, is not correct. In 1991,
Gabow and Tarjan \cite{GaTa2} have given an efficient scaling algorithm for general
graph-matching problems. As pointed out in their paper, by a slight modification of
their algorithm, they have obtained another  implementation of a phase.
The history of efficient implementations of a phase of the Hopcroft-Karp approach
for general graphs illustrates the need of a framework which allows a clear description 
and an elaborated correctness proof of matching algorithms.

The first polynomial algorithm for the maximum weighted matching problem has been also 
given by Edmonds \cite{Ed2}. A straightforward implementation of his algorithm has run 
time . Gabow \cite{Ga1} and Lawler \cite{La} have developed  
implementations of Edmonds' algorithm. Galil, Micali and Gabow \cite{GaMiGa} have given 
an  implementation. At the first SODA, Gabow \cite{Ga2} has
presented an implementation of Edmonds algorithm which uses complicated data structures
and stated that the time complexity of his implementation is . 

Our goal is to avoid sophisticated explicit analysis of (nested) blossoms.
Hence, we reduce the problem of finding an augmenting path to a reachability 
problem in a directed, bipartite graph. We show, how to solve this reachability 
problem by a modified depth-first search. This approach yields an algorithm which is not
fundamentally different from previous algorithms which use Edmonds' traditional
terminology of blossoms. But if Edmonds' algorithm is used as a subroutine with respect
to the solution of a more involved problem, using the framework of the reachability
problem which avoids the explicit consideration of blossoms can simplify the situation 
considerably. To illustrate this, we describe a realization of the Hopcroft-Karp approach 
\cite{HK} for the 
computation of a maximum cardinality matching in general graphs. Furthermore, we show how 
to use the modified depth-first search algorithm in the primal step of Edmonds' maximum 
weighted matching algorithm. For the description of the primal-dual method, we use no 
linear program and no duality theory for linear programs. 
A straightforward  implementation will be described as well.

In Section~2, the basic algorithm is presented. After the description of
the reduction to a reachability problem in a directed, bipartite graph, this 
reachability problem is solved by a modification of depth-first search.
The correctness of the algorithm is proved and an efficient implementation
is given. In Section 3, a realization of the Hopcroft-Karp approach for general graphs
is described and its correctness is proved using the framework of the reachability 
problem. Furthermore, an efficient implementation is given.
In Section 4, we show how to use the modified depth-first search as a subroutine in Edmonds' 
maximum weighted matching algorithm. Furthermore, we describe an 
implementation of this approach.

\section{The basic algorithm}

After the reduction of the problem of finding an augmenting path to a reachability
problem, we shall describe the solution of the reachability problem by a modification
of depth-first search. The resulting algorithm is equivalent to Edmonds' algorithm
since to each run of Edmonds' algorithm there corresponds a run of the modified
depth-first search and vice versa \cite{Ro}. Hence, the correctness of the modified
depth-first search follows directly from the correctness of Edmonds' algorithm.
Nevertheless, we shall prove the correctness of the algorithm directly without
the use of the correctness of Edmonds' algorithm. Then we shall describe an efficient
implementation of the algorithm. The description of Edmonds' algorithm within
the framework of the reachability problem seems not to be simpler than an
elaborated description of Edmonds' algorithm within the traditional framework
of blossoms. However, as we shall see later, if Edmonds' algorithm is used as a 
subroutine with respect to the solution of a more involved problem, using the framework
of the reachability problem can simplify the situation considerably.

\subsection{Definitions and the general method}

A {\em graph}  consists of a finite, nonempty set of {\em nodes\/}
 and a set of {\em edges\/} .  is either {\em directed\/} or
{\em undirected\/}. In the (un-)directed case, each edge is an (un-)ordered
pair of distinct nodes. 
A graph  is {\em bipartite\/} if  can be
partitioned into disjoint nonempty sets  and  such that for all
,  and  or vice versa. Then we often write 
.
A {\em path\/}  from  to  is a sequence of
nodes , which satisfies , for
.
The {\em length\/}  of  is the number  of edges on .  is
{\em simple\/} if , for . For conveniences,
 will denote the path , the set of nodes
, and the set of edges .
If there exists a path from  to  (of length ) then  is called a
{\em (direct) predecessor\/} of , and  is called a {\em (direct) 
successor\/} of .
Let  be an undirected graph.  is a
{\em matching\/} of  if no two edges in  have a node in common.
A matching  is {\em maximal\/} if there exists no  such
that  is a matching. A matching  is {\em maximum\/} if there
exists no matching  of larger size.
Given an undirected graph , the {\em maximum matching problem\/} is
finding a maximum matching .
A path  is {\em M-alternating\/}, if it contains 
alternately edges in  and in .
A node  is {\em M-free\/} if  is not incident to any edge in . 
Let  be a simple -alternating path.  is
{\em -augmenting} if  and  are -free.
Let  be an -augmenting path in . Then  denotes the
{\em symmetric difference\/} of  and ; i.e., .
It is easy to see that  is a matching of , and .

The key to most algorithms for finding a maximum matching in a graph is the
following theorem of Berge \cite{Be}.

\begin{theo}
Let  be an undirected graph and  be a matching. Then
 is maximum if and only if there exists no -augmenting path in .
\end{theo}
Berge's theorem directly implies the following general method for
finding a maximum matching in a graph .


\medskip
\noindent
{\bf Algorithm 1} \\
\noindent {\bf Input:} An undirected graph , and a matching
. \\
\noindent {\bf Output:} A maximum matching . \\
\noindent {\bf Method:}
\vspace{-0.3cm}
\begin{tabbing}
AA \= AA \= \kill
{\bf while} there exists an -augmenting path \\
\> {\bf do} \\
        \>  \> construct such a path ; \\
        \>  \>  \\
\> {\bf od}; \\
 := .
\end{tabbing}

The key problem is now this: How to find an -augmenting path , if such 
a path exists?
We solve this key problem in the following way.
\begin{enumerate}
\item We reduce the key problem to a reachability problem in a directed,
bipartite graph .
\item We solve this reachability problem constructively.
\end{enumerate}

\subsection{The reduction to a reachability problem}

In the bipartite case, we construct from  and a matching 
 a directed graph  by directing 
the edges in  from~ to~,
and directing the edges in  from ~to~. Additionally, we add
two new
nodes~ and  to~, add for each -free node  the edge
 to~, and add for each -free node~ the edge~
to~.
It is easy to prove that there is an -augmenting path in  if and
only if there is a simple path from~ to~ in .
This reachability problem can be solved by performing a depth-first search 
of  with start node . Now we shall consider the general case.

Let  be an undirected graph and  be a matching. Let
M. For the definition of
 we have the following difficulty. Let us consider the graph described in
Figure 1 where edges in  are wavy. The -augmenting path from  to  enters 
the edge  in  and
leaves the edge from . The -augmenting path from  to  enters 
in  and leaves  from .
\begin{figure}[t]
\begin{picture}(0,0)\includegraphics{nmbild1_pspdftex}\end{picture}\setlength{\unitlength}{3947sp}\begingroup\makeatletter\ifx\SetFigFont\undefined \gdef\SetFigFont#1#2#3#4#5{\reset@font\fontsize{#1}{#2pt}\fontfamily{#3}\fontseries{#4}\fontshape{#5}\selectfont}\fi\endgroup \begin{picture}(4966,3464)(593,-2918)
\put(4801,-436){\makebox(0,0)[b]{\smash{{\SetFigFont{10}{12.0}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(751,-1911){\makebox(0,0)[b]{\smash{{\SetFigFont{10}{12.0}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(4201,329){\makebox(0,0)[b]{\smash{{\SetFigFont{10}{12.0}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(4201,-571){\makebox(0,0)[b]{\smash{{\SetFigFont{10}{12.0}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(4201,-2811){\makebox(0,0)[b]{\smash{{\SetFigFont{10}{12.0}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(5401,-571){\makebox(0,0)[b]{\smash{{\SetFigFont{10}{12.0}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(751,-711){\makebox(0,0)[b]{\smash{{\SetFigFont{10}{12.0}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\end{picture} \caption{Difficulty with respect to the definition of .}
\end{figure}
A priori, we cannot divide the set of nodes  into two sets  and  such
that an -augmenting path exists in  if and
only if there exists an -augmenting path, using alternately nodes
from  and from . 
Hence, for defining , we introduce for each node
 two nodes  and  such that an analogous construction of 
a graph  is possible.
Both edges  and  are in  if and only if
. Both edges  and  
are in  if and only if . Additionally, we add
for each -free node  the edges  and  to
, where  and  are two new distinct nodes.
More formally, let  where


\noindent
Analogously to the bipartite case, we have directed the edges in 
``from  to '' and the edges in  ``from  to ''.
Since the distinct nodes  and  in  correspond to the same
node  in , it does not suffice to construct a simple path from  to 
in  for finding an -augmenting path in . Hence, we define strongly
simple paths in  which cannot contain both nodes  and , for
all . A path  in  is {\em strongly simple} if
\begin{enumerate}
\item[a)] 
 is simple, and 
\item[b)] 
.
\end{enumerate}
Now we can formulate the reachability problem in  which is equivalent to 
the problem of finding an -augmenting path in .

\begin{theo} \label{theo1}
Let  be an undirected graph,  be a matching, and
 be defined as above. Then there exists an -augmenting path
in  if and only if there exists a
strongly simple path from  to  in .
\end{theo}
{\bf Proof:}
``'': Let  be an
-augmenting path in . Then , ,
and . Hence, by the construction of ,

is a strongly simple path in .

\medskip
\noindent 
``'':
Let  be a strongly simple path in . Then
, , and . Hence,
 is an -augmenting path in .
\QED

\medskip
Because of Theorem \ref{theo1}, the reachability problem equivalent to the problem
of finding an -augmenting path in  is to find a 
strongly simple path from  to  in .

\subsection{The solution of the reachability problem}

Depth-first search (DFS) finds simple paths in a directed graph. Hence, we 
cannot
use DFS directly for the solution of the reachability problem in . We will
modify the usual DFS such that the modified depth-first search (MDFS) finds
precisely the strongly simple paths in .
Let  and . Remember that
a DFS partitions the edges of the graph into four categories \cite{AHU}.
Similarly, the edges of  are partitioned into five categories by an
MDFS of :
\begin{enumerate}
\item {\em Tree edges\/}, which are edges leading to new nodes ,
 for which
 is not a predecessor during the search.
\item {\em Weak back edges\/}, which are edges leading to new nodes  for
which  is a predecessor during the search.
\item {\em Back edges\/}, which go from descendants to ancestors during the 
search.
\item {\em Forward edges\/}, which go from ancestors to proper descendants but
are not tree edges.
\item {\em Cross edges\/}, which go between nodes that are neither ancestors
nor descendants of one another during the search.
\end{enumerate}
Like DFS, MDFS uses a stack  for the organization of the search. Analogously
to DFS, the MDFS-stack  defines a tree, the {\em MDFS-tree} . Before
describing MDFS in detail, we shall describe the algorithm informally. TOP()
denotes the last node added to the MDFS-stack . In each step, MDFS considers
an edge  which has not been considered previously.
Let  be the edge under consideration. We 
distinguish two cases.

\begin{itemize}
\item[1.] , i.e.\ .\hfill {\em tree edge}
\item[2.] , i.e.\ 
      \vspace{-0.2cm}  
      \begin{itemize}
        \item[2.1]  \hfill {\em back edge}
        \item[2.2]  but 
                \begin{itemize}
                \item[i)]  has been in  previously \hfill {\em cross edge}
                \item[ii)]  has not been in  previously \hfill {\em weak back edge}
                \end{itemize} 
        \item[2.3]  and 
                \begin{itemize}
                \item[i)]  has been in  previously \hfill {\em
forward} or {\em cross edge}
                \item[ii)]  has not been in  previously \hfill {\em tree edge}
                \end{itemize}
        \end{itemize}
\end{itemize}
MDFS differs from DFS only in Cases 2.2.ii and 2.3.i. Next, we shall discuss
both of these cases.

\medskip
\noindent
{\bf Case 2.2.ii:} Since  has not been in  previously, DFS would 
perform the operation PUSH(). Since  and
MDFS should only construct strongly simple paths in , the operation PUSH() is 
not performed by MDFS.

Note that the path  defined by the MDFS-stack  is strongly
simple. Hence, the path  where  
is strongly simple for each node  on . We say then that MDFS has {\em found\/}
the strongly simple path  from  to .
Since the path  is above the node  with respect to the MDFS-stack , 
after the execution of the operation ,
no node on  is in the MDFS-stack . Moreover, as we shall prove later, for all nodes 
 on  the operations , , 
and  have been performed before the operation .

\medskip
\noindent
{\bf Case 2.3.i:} Since  has been in  before, DFS would perform no 
PUSH-operation. But the different treatment of Case 2.2.ii can cause the
following situation: 
MDFS has found a strongly simple path  from the node 
to a node  but at that moment, the node  was below  in the MDFS-stack
 such that the operation PUSH() has not been performed. But now, 
.

As we shall prove later, the paths  from  to  and  from
 to  are {\em strongly disjoint\/}; i.e.\ there is no node 
on ,  such that . 
Since MDFS has found a strongly simple path  from  to , MDFS now performs 
the operation PUSH().

\medskip
Note that with respect to depth-first search, the DFS-stack contains exactly
the current search path. With respect to the modified depth-first search,
the situation is different. In Case 2.3.i, the node  is pushed. But
to obtain a current search path, between the nodes  and ,
we have to insert any path  which has been found by MDFS.
Since we do not want to forget the information about the first node 
on the path which we add between the nodes  and , we 
create the artificial tree edge . Such an edge is
called {\em extensible edge}. It is possible that there exists various such
paths . Hence, after the execution of PUSH, the number of
corresponding current search paths can increase. 
Whenever we say that we consider {\em a current search path\/} we mean that we can
take an arbitrary corresponding current search path. If we add to the
constructed MDFS-tree  all forward, back, cross and weak back edges and  
every extensible edge  is replaced by all strongly simple 
paths , then
we obtain the {\em expanded current MDFS-tree\/} .

We say that MDFS has {\em constructed\/} a path  if the MDFS-stack  contains the 
path  where each extensible edge  in  is replaced by one
of the strongly simple paths  which replace this extensible
edge in . 
We say that MDFS has {\em formed\/} a strongly simple path  if  contains 
. We say that MDFS has {\em found\/} a strongly simple path  if 
the path  is formed by MDFS and the edge  is a considered 
weak back edge.
Next we shall describe MDFS in detail. 
We have to solve the following
problem: How to find the node  in Case 2.3.i?
For the solution of this problem, we assume that MDFS is organized such that 
for all nodes , the following holds true:

\medskip
After performing the operation POP(), MDFS has always computed a
set  of nodes such that  contains exactly those
nodes  satisfying the requirements that  \label{WM1}
\begin{enumerate}
\item MDFS has found a path  with ,
\item PUSH() has never been performed, and
\item POP() has been performed.
\end{enumerate}
Before the execution of POP, we fix . 

\medskip
In the description of MDFS we assume for all  that
 is computed correctly. As we shall prove later, it will always hold
that .
The computation of  as well
as an efficient implementation of MDFS, can be found in Section~\ref{MDFSim}.
For ,  denotes the adjacency list of .
Note that after the POP of the head  of a matched edge ,
the POP of the tail  of this edge has also to be performed. Hence, at the
end of the procedure SEARCH, we shall have two POP-operations.

\medskip
\noindent
{\bf Algorithm 2 (MDFS)} \\
{\bf Input:} . \\
{\bf Output:} A strongly simple path  from  to , if such a path exists.\\
{\bf Method:}
\vspace{-0.3cm}
\begin{tabbing}
AA \= AA \= \= \kill
Initialize the stack  to be empty; \\
PUSH(); \\
{\bf while}  {\bf and} no path from  to  is constructed \\
\> {\bf do} \\
\> \> SEARCH \\
\> {\bf od}.
\end{tabbing}
\noindent
SEARCH is a call of the following procedure. 

\begin{tabbing}
(Case 2.3 ii)\= AA \= AA \= AA \= AA \= AA \= AA \= AA \= AA \= \kill \> {\bf procedure} SEARCH; \\
\> {\bf if} TOP =  \\
\> {\bf then} \\
\> \> reconstruct a strongly simple path  from  to  \\ 
\> \> which has been constructed by the algorithm \\
\> {\bf else} \\ 
\> \> mark TOP ``pushed''; \\
\> \> {\bf for} all nodes  \\
\> \> {\bf do} \\
\> \> \> {\bf if}  =  \\
(Case 1) \> \> \> {\bf then} \\
\> \> \> \> PUSH(); \\
\> \> \> \> SEARCH \\
(Case 2) \> \> \> {\bf else} \\
\> \> \> \> {\bf if}  \\
(Case 2.1) \> \> \> \> {\bf then} \\
\> \> \> \> \> no PUSH-operation is performed \\
\> \> \> \> {\bf else} \\
\> \> \> \> \>{\bf if}  \\
(Case 2.2) \> \> \> \> \> {\bf then} \\
\> \> \> \> \> \>no PUSH-operation is performed \\
(Case 2.3) \> \> \> \> \> {\bf else} \\
\> \> \> \> \> \> {\bf if}  is marked ``pushed''  \\
(Case 2.3.i) \> \> \> \> \> \> {\bf then} \\
\> \> \> \> \> \> \>{\bf while}   \\
\> \> \> \> \> \> \> {\bf do} \\
\> \> \> \> \> \> \> \> choose any ; \\
\> \> \> \> \> \> \> \> PUSH(); \\
\> \> \> \> \> \> \> \> SEARCH \\
\> \> \> \> \> \> \> {\bf od} \\
(Case 2.3.ii)\> \> \> \> \> \> {\bf else} \\
\> \> \> \> \> \> \> PUSH(); \\
\> \> \> \> \> \> \> SEARCH \\
\> \> \> \> \> \> {\bf fi} \\
\> \> \> \> \> {\bf fi} \\
\> \> \> \> {\bf fi} \\
\> \> \> {\bf fi} \\
\> \> {\bf od};\\
\> \> POP; \\
\> \> POP \\
\> {\bf fi}.
\end{tabbing}

\subsection{The correctness proof of MDFS}

The correctness proof of MDFS is inspired by the correctness proof of DFS.
But in contrast to DFS, the proof is difficult. The difficulties come from the 
fact that the MDFS-stack does not contain the whole current search path and the 
decisions taken by the algorithm only depend on the content of the current stack.
Hence, the proof that the algorithm constructs only strongly simple paths is
involved. First we shall prove some lemmas. The first lemma implies that the first
PUSH-operation which destroys the property ``strongly simple'' must push 
a node with second component .
\begin{lemma}
As long as MDFS constructs only strongly simple paths, the following holds true:
After the operation PUSH where  is not -free, the
operation PUSH where  always follows without 
destroying the property ``strongly simple''.
\end{lemma}
{\bf Proof:} 
After the execution of the operation PUSH, always the unique edge 
 is considered and the operation PUSH is
performed. If this operation would destroy the property ``strongly simple'',
then  and hence,  would be on a current search path. But then,
already the operation PUSH would have destroyed the property ``strongly
simple'', a contradiction. 
\QED

\medskip
The next lemma shows that in a certain situation MDFS constructs a path from  to a 
node .
\begin{lemma} \label{Lemma1}
Let  be a node for which MDFS performs the operation
PUSH. Furthermore, at the moment when POP is performed, only strongly 
simple paths have been constructed by MDFS.
Let  be a node such that at the moment when PUSH is 
performed, there is a strongly simple path
 with 
for all . Then PUSH has been performed before POP.
\end{lemma}
{\bf Remark:} Lemma~\ref{Lemma1} implies that either PUSH() 
and POP() have been performed before the execution of PUSH(), 
or both operations have been performed between the operations PUSH() 
and POP().

\medskip
\noindent 
{\bf Proof:}
Assume that the assertion does not hold. We consider a path
 of shortest length such that 
PUSH has {\em not\/} been performed before POP.
It is clear that the edge  has been considered before the 
execution of POP. By assumption,  and 
at the moment when  is considered. Hence, MDFS is in Case 2.3.

If the operation PUSH is performed according to this consideration of 
edge , then PUSH would be the next operation performed by the
algorithm. By assumption,  is a shortest path such that the assertion
is not fulfilled. Therefore, PUSH has been performed before POP, 
and hence, before POP, a contradiction. 

If the operation PUSH is not performed 
according to this consideration of edge , then MDFS is in Case 2.3.i and performs 
the corresponding while-statement. Consider the moment when MDFS terminates this 
while-statement; i.e., .
Let  be the first node on  for which PUSH has not been
performed. Since  has this property, the node  exists.
Let . By construction, each node on
 is pushed and each edge on  is considered.
Hence, all these nodes and edges are in  such that  is formed by MDFS.
Furthermore, the edge  is a considered weak back edge. Therefore,
MDFS has found the path . By assumption, PUSH has never been 
performed. Since  when PUSH is performed it holds that
POP has been performed.
Hence, , and hence, . But this 
contradicts  such that the lemma is proved.
\QED

\medskip
The following lemma shows that in a certain situation some nodes are 
already pushed.
For the proof of the lemma, we need the notation of the so-called 
back-path  of a path  in . Essentially, the back-path of  is
obtained by changing the direction of the edges on  and running  backwards. 
To get a formal definition for , we denote for 

Let  be a path in . The {\em back-path\/}
 of  is defined by

The following lemma is a direct consequence of Lemma \ref{Lemma1}.
\begin{lemma} \label{Lemma3}
Let  be a node for which MDFS performs the operation
PUSH. 
Furthermore, at the moment when POP is performed, only strongly simple 
paths have been constructed by MDFS. If there exists a strongly simple path
 such that at the moment when PUSH is performed,  
, for all , and 
 then for all , the 
operations PUSH and PUSH have been performed 
before the execution of the operation POP.
\end{lemma}
{\bf Proof:} 
For , the assertion follows by an application of Lemma \ref{Lemma1}
to the path . With respect to  where , 
we apply Lemma \ref{Lemma1} to the path .
\QED

\medskip
Note that by the definition of ,  implies that PUSH 
and POP have been performed. The following lemma extracts properties of
the algorithm MDFS which enable us to prove the correctness and to develop an efficient 
implementation of MDFS.

\begin{lemma} \label{Lemma4}
MDFS maintains the following invariants:
\vspace{-0.2cm}
\begin{enumerate}
\item 
MDFS constructs only strongly simple paths.
\item 
, for all .
\item
Assume that the algorithm performs the assignment . 
Then after the execution of PUSH always . 
\end{enumerate}
\end{lemma}
{\bf Remark:} Invariant 2 and Invariant 3 are not needed for the correctness
proof. But we shall need these invariants for an efficient 
implementation of the algorithm. Moreover, the proof of Invariant 1 is easier 
if we prove all invariants simultaneously.

\medskip
\noindent
{\bf Proof:} Consider the first situation in which one of the three invariants
is not maintained. Three cases have to be considered.

\medskip
\noindent
{\bf Case 1:} Invariant 1 is not maintained.

\medskip
Only a PUSH-operation can destroy the property ``strongly simple''. Note that
a PUSH-operation cannot affect Invariant 2 or Invariant 3.
Lemma 1 implies that this PUSH-operation occurs during the consideration
of an edge . 

If  is not marked ``pushed'', then Case 2.3.ii applies and
PUSH is performed. Since , 
the only possible situation in which this PUSH-operation destroys the
property ``strongly simple'' is the following:
On a current search path there is a subpath  which is caused by an 
application of Case 2.3.i of MDFS such that .
Hence, there exists  such that the addition of  to this 
current search path is caused by the operation PUSH. By construction, 
the assumptions of Lemma \ref{Lemma3} are fulfilled with respect to  and  on . 
Hence, by Lemma \ref{Lemma3}, PUSH has been performed {\em before\/} 
POP, and hence, {\em before\/} PUSH, a contradiction. 

Hence,  is marked ``pushed'' such that Case 2.3.i of MDFS 
applies. By Invariant 2, . We thus write 
 instead of . 
By Case 2.3.i, for the node , the operation PUSH
is performed such that MDFS extends the current search paths by a path , 
but only  is pushed. 
By the definition of  and by Lemma 3, the operations PUSH,
POP, PUSH, and POP have been 
performed, for all  such that none of these nodes is contained
in the current MDFS-stack . Hence, the only possible situation in which
PUSH destroys the property ``strongly simple'' is the following:
There is a node , and a subpath  of a current
search path which is caused by an application of Case 2.3.i such that
 or . Since one end node of an edge in 
the current matching uniquely determines the other end node, we can choose 
 such that .

Consider the node  such that PUSH is the operation which adds 
the subpath  to this current search path. Therefore, immediately before the execution
of PUSH, . Hence, by Invariant 3,
after the execution of PUSH, always . By
the choice of , , and hence,  in 
the situation under consideration. Hence, 
POP has been performed such that , a contradiction. 

\medskip
\noindent
{\bf Case 2:} Invariant 2 is not maintained.

\medskip
Then there exists  such that 
 before the execution of POP and 
  after the execution of POP.
Hence, MDFS has found a path  with  and 
found a path  with .

If MDFS has found the path  after the execution of POP, then
 can only be added to  in the following way:
An operation PUSH, caused by an application of Case 2.3.i
with respect to a node  (i.e., ) is performed 
such that the current search path is extended by a path 
with .
But then,  before the execution of PUSH. 
PUSH is performed after POP. Hence,  between the execution of these two operations.
This contradicts the assumption that we consider the situation in
which Invariant 2 is not maintained {\em for the first time\/}.

Hence, MDFS has found the path  before the execution of POP. 
Note that . Otherwise, by Lemma 3, 
PUSH is performed before POP, and hence,  after POP. 
Let  be the first node on  such that  or . Since node  has this property, the node  exists. Let 
   
Consider the path

Then Lemma 2 applies with respect to ,  and the
strongly simple path . Hence, PUSH is performed
before POP, and hence, 
after POP, a contradiction.

\medskip
\noindent 
{\bf Case 3:} Invariant 3 is not maintained.

\medskip
After the execution of PUSH, it holds that
. 
We shall prove that  after the next 
POP-operation which changes  or . Then, the assertion 
follows because of Invariant 2 and the transitivity of the relation .
Let POP be the next POP-operation which enlarges  or
.  denotes the current MDFS-stack, directly after the 
execution of PUSH. Let .
Note that .
According to the location of  with respect to  and to
, we have to discuss three cases.

By construction, . Otherwise, 
POP would be performed before PUSH.

Assume that . Let  be the first node 
in  such that 
. Node  exists since 
has the property that .
Consider the back-path of the path from node  to node . This 
back-path implies that  and  fulfill the assumptions of
Lemma 2. Hence, PUSH occurs before POP. Since , the operation PUSH is also performed before
POP. Hence, POP can enlarge neither  nor
. 

It remains to consider . Let  be the node 
nearest to the top of  for which PUSH has not been performed at 
the moment when MDFS performs PUSH. Since  has this property,
 exists.
By consideration of the back-path of the path from  to , 
it is easy to prove that MDFS finds a path from  to 
not containing . Hence,  after the 
execution of POP, and hence, 
. Since MDFS has found a path from  to  which 
does not contain , it holds that .  
\QED

\medskip
Now, the correctness of the algorithm MDFS can easily be derived from Lemma \ref{Lemma1}
and Lemma \ref{Lemma4}.
\begin{theo}
MDFS constructs a strongly simple path from  to  iff such a path exists.
\end{theo}
{\bf Proof:}
Assume  is a strongly simple path 
from  to . It is clear that MDFS considers the
edge  and performs the operation PUSH(). (Note that 
 is -free.)
Hence, ,  fulfill the assumptions of Lemma~\ref{Lemma1}
with respect to the path .
By Lemma~\ref{Lemma1}, MDFS performs PUSH() and hence, 
PUSH(). Therefore, MDFS constructs a path from  to .
By Invariant 1 of Lemma \ref{Lemma4}, MDFS constructs only strongly simple paths.
\QED

\subsection{An implementation of MDFS} \label{MDFSim}

Now we shall describe how to get an efficient implementation of MDFS. Only 
two parts of the algorithm are nontrivial to implement.
\begin{enumerate}
\item The manipulation of .
\item The reconstruction of a strongly simple path~ from  to  which
 is constructed by the algorithm.
\end{enumerate}
For the solution of both subproblems it is useful to perform the
POP-operations not explicitly and to maintain the whole MDFS-tree .
This can be done as follows:
The data structure is a tree . A pointer  always points to TOP 
in . The current MDFS-stack  is represented by the unique path from the 
root  of  to TOP in . For performing the operation POP, the
pointer  is changed such that it points to the unique direct predecessor 
in . When we perform a PUSH-operation, the node in  to which TOP 
points obtains a new leaf. After the PUSH-operation TOP points to this
new leaf.

Invariant 2 and Invariant 3 are the key for the efficient implementation of 
MDFS. Now we shall describe the update of . By the definition of
, we only have to change  after a PUSH- or after
a POP-operation. More exactly, we have to perform after PUSH() the 
operation  if  and after POP()
the operation  if PUSH() has never been performed and 
MDFS has found a path  for which .

After the execution of POP, if PUSH has never been performed, 
MDFS needs all nodes  for which a path  such that 
 has been found by MDFS.
This can easily be done by any graph search method like depth-first search on the
current , starting at node  and running the considered edges backwards. 
When the node  is reached, a backtrack is performed. But with respect
to efficiency, it is useful to investigate the properties of MDFS and to
refine the backward graph search. 

First, we shall characterize the paths  with , found by MDFS. Let . Then, the following 
properties are fulfilled:
\begin{enumerate}
\item 
 is a weak back edge.
\item
If we start in edge  and consider  backwards, then we see a nonempty sequence
of tree edges followed by a single cross, forward or back edge, followed by a 
nonempty sequence of tree edges followed by a single cross, forward or 
back edge, and so on.
\end{enumerate}
Hence, after the execution of POP, we need the following sets
of nodes:

and for some 

According to Invariant 3, during the backward search, some subpaths can be
skipped over. Therefore, we need the following set of nodes

By Invariant 3,  implies . 
We say that  is {\em current\/} if , for all .
According to Invariant 3, we can compute  in the following way.
\begin{enumerate}
\item Compute  such that , and  is 
current.
\item If  does not exist, then . Otherwise, 

\end{enumerate} 
As described above, a correct manipulation of the current sets  
allows the solution of the first subproblem. Note that by Invariant 2
of Lemma \ref{Lemma4}, each 
 is contained in at most one current set . 
If during the backward search a node  is met for which 
previously, some nodes can be skipped over. Hence, we have also to know 
if  previously. This will be realized by the correct 
update of the following set

Now we can give a detailed description of the backward search which will be 
performed after POP.
The consideration of those paths  with 
is done in several rounds.
In the first round, we construct backwards all paths {\em without any\/} cross,
forward, or back edge.
In the second round, all paths with {\em exactly one} such edge are constructed
implicitly, and so on.
Let  denote the nodes in  which have been already considered during 
the backward search. If a node in  is considered again, the search has not 
to be continued at that node.
In the first round, we consider the weak back edges . In the th
round, , we consider those edges  for which
 is computed in the st round.
Starting in node , we follow backwards the tree edges as long as 
a node in  is reached. If we reach a node , then 
we compute the current  such that  and we jump to
 for the continuation of the backward search. Since we perform a backward
search,  is switched to . According to 
Invariant 3,  and hence,  for all 
. 

For the organization of the backward search, we use a queue  which contains
the start nodes of the next round. During Round , the start nodes of Round  
are added to the end of the queue. Since the data structure is a queue, all start
nodes of a round are treated before the first start node of the next round is taken
away from .

For the reconstruction of a strongly simple path from  to  constructed
by the algorithm, we have to know the non-tree edges used on the path. Hence, 
for all  we use a variable  to store the needed information
with respect to the node . This means, we store in variable  that non-tree
edge which concludes the block of tree edges which contains the tree edge with end 
node  at the moment when  for the first time 
during the backward search.

The implementation of MDFS must be done with attention to the correct 
manipulation of the
sets , and . The following table describes in
terms of the case of MDFS, and in terms of the operation which is 
performed, how MDFS has to update these sets.
\begin{center}
\begin{tabular}{|l|l|} \hline
{\em case, operation\/}  & {\em set updating\/} \\ \hline
Case 1  &  no update \\
Case 2.1  &   :=   \\
Case 2.2.i  &  :=  \\
Case 2.2.ii  &   :=   \\
Case 2.3.i  &                                          \\
  & no update  \\
 &  :=  if  \\
Case 2.3.ii  & no update   \\
PUSH & no update \\
POP  &  :=  MDFS has found a path from \\
  &   to  not containing   \\
\hline
\end{tabular}
\end{center}
In Case 2.1, it is clear that  since POP has not been 
performed. In Case 2.2.i,  follows directly from  and Lemma 1.
Note that in Case 2.3.i, subcase , we have to 
store the information that edge  is used. In the 
implementation, we accomplish this by adding the edge  to the node 
 in .
Then we obtain the {\em expanded node} .
The considerations above lead to the following implementation of the procedure
SEARCH.

\medskip

\begin{tabbing}
(Case 2.3.ii)\= AA \= AA \= AA \= AA \= AA \= AA \= AA \= AA \= AA \= \kill
\> {\bf procedure} SEARCH; \\
\> {\bf if} TOP =  \\
\> {\bf then} \\
\> \> reconstruct a strongly simple path  from  to  \\
\> \> which has been constructed by the algorithm \\
\> {\bf else} \\
\> \> mark TOP ``pushed''; \\
\> \> {\bf for} all nodes  \\
\> \> {\bf do} \\
\> \> \> {\bf if}  =  \\
(Case 1) \> \> \> {\bf then} \\
\> \> \> \> PUSH; \\
\> \> \> \> SEARCH \\
(Case 2) \> \> \> {\bf else} \\
\> \> \> \> {\bf if}  \\
(Case 2.1) \> \> \> \> {\bf then} \\
\> \> \> \> \>  :=  \\
\> \> \> \> {\bf else} \\
\> \> \> \> \> {\bf if}  \\
(Case 2.2) \> \> \> \> \> {\bf then} \\
\> \> \> \> \> \> {\bf if}  is marked ``pushed'' \\
(Case 2.2.i) \> \> \> \> \> \> {\bf then} \\
\> \> \> \> \> \> \>  \\
(Case 2.2.ii) \> \> \> \> \> \> {\bf else} \\
\> \> \> \> \> \> \>  :=  \\
\> \> \> \> \> \> {\bf fi} \\
(Case 2.3) \> \> \> \> \> {\bf else} \\
\> \> \> \> \> \> {\bf if}  is marked ``pushed'' \\ 
(Case 2.3.i) \> \> \> \> \> \> {\bf then}\\
\> \> \> \> \> \> \> {\bf if}  \\
\> \> \> \> \> \> \> {\bf then} \\
\> \> \> \> \> \> \> \> expand TOP in  to \\
\> \> \> \> \> \> \> \> ; \\
\> \> \> \> \> \> \> \> PUSH; ;\\
\> \> \> \> \> \> \> \> SEARCH \\
\> \> \> \> \> \> \> {\bf else} \\
\> \> \> \> \> \> \> \> {\bf if}  \\
\> \> \> \> \> \> \> \> {\bf then} \\
\> \> \> \> \> \> \> \> \>  :=  \\
\> \> \> \> \> \> \> \> {\bf fi} \\
\> \> \> \> \> \> \> {\bf fi} \\
(Case 2.3.ii) \> \> \> \> \> \> {\bf else} \\
\> \> \> \> \> \> \> PUSH; \\
\> \> \> \> \> \> \> SEARCH \\
\> \> \> \> \> \> {\bf fi} \\
\> \> \> \> \> {\bf fi} \\
\> \> \> \> {\bf fi} \\
\> \> \> {\bf fi} \\
\> \> {\bf od}; \\ 
\> \>  let TOP =  \\
\> \>  := ; \\
\> \>  := ; \\
\> \>  := ; \\
\> \> {\bf for} all  \\
\> \> {\bf do} \\
\> \> \> CONSTRL; \\
\> \> {\bf od}; \\
\> \> {\bf while}  \\
\> \> {\bf do} \\
\> \> \> remove the front node  from ; \\
\> \> \> {\bf for} all  which have not already been \\
\> \> \> \> considered during the backward search \\
\> \> \> {\bf do} \\
\> \> \> \> CONSTRL \\
\> \> \> {\bf od} \\
\> \> {\bf od;} \\
\> POP; \\
\> POP \\
{\bf fi}. \\
\end{tabbing}
\vspace{-.3cm}
\noindent
CONSTRL is a call of the following procedure. The variable  contains 
always the non-tree edge which concludes the current block of tree edges.

\begin{tabbing}
(Case 2.3.ii)\= AA \= AA \= AA \= AA \= AA \= AA \= AA \= AA \= AA \= \kill
\> {\bf procedure} CONSTRL; \\
\>  := ; \\
\>  := ; \\
\> {\bf while} no node in  is reached \\
\> {\bf do} \\
\> \> starting in the node , perform a backward search until \\ 
\> \> a node in  is reached on the tree edges; \\
\> \> {\bf if}  is met during the backward search \\ 
\> \> {\bf then} \\
\> \> \>  := ; \\
\> \> \>  := ; \\
\> \> \>  := ; \\
\> \> \> add the node  at the end of  \\
\> \> {\bf fi}; \\
\> \> {\bf if}  is met by the backward search \\
\> \> {\bf then} \\
\> \> \>  Let  be the current set containing . \\
\> \> \>  := ; \\
\> \> \>  :=  \\
\> \> {\bf fi} \\
\> {\bf od}. \\
\end{tabbing}

\noindent
The reconstruction of a strongly simple path  from  to  constructed by
the algorithm remains to be explained. 
Beginning at the end of , such a path  can be reconstructed by traversing
the MDFS-tree  backwards. Note that  points to the end of ,
and that the father of each node in  is always unique.
As long as we traverse tree edges of the algorithm MDFS, we have no difficulty.
But every time when we meet a node  which has been added to  by an 
application of Case 2.3.i, we have to reconstruct a subpath  
which has been joined to . In this situation, the considered portion of 
is the expanded node ;
i.e., the structure of  tells us that MDFS has applied Case 2.3.i.
It remains the reconstruction of the subpath . For doing this, 
we start in the node . We use  to obtain the non-tree edge of MDFS, 
which finishes the block containing the tree edge with end node .
Let . Then  denotes , and
 denotes . First we reconstruct the block from the node
 to the node . Then we reconstruct the block from the node
 to the node , and so on until the node
 is met. Each block can be reconstructed as the path  itself.
These considerations lead to the following procedure for the reconstruction of
an augmenting path, constructed by the algorithm.
\begin{tabbing}
(Case 2.3.ii)\= AA \= AA \= AA \= AA \= AA \= AA \= AA \= AA \= AA \= \kill
\> {\bf procedure} RECONSTRPATH; \\
\>  := ; \\
\> {\bf while}  \\
\> {\bf do} \\ 
\> \> {\bf if}  is not expanded \\
\> \> {\bf then} \\
\> \> \>  :=  \\
\> \> {\bf else}  let  \\
\> \> \> RECONSTRQ; \\
\> \> \>  :=  \\
\> \> {\bf fi} \\
\> {\bf od}. \\
\end{tabbing}
\noindent
RECONSTRQ is a call of the following procedure.
\begin{tabbing}
(Case 2.3.ii)\= AA \= AA \= AA \= AA \= AA \= AA \= AA \= AA \= AA \= \kill
\> {\bf procedure} RECONSTRQ; \\
\>  := ; \\
\> RECONSTRPATH; \\ 
\> {\bf while}  \\
\> {\bf do} \\
\> \>  := ; \\
\> \> RECONSTRPATH  \\
\> {\bf od}. \\
\end{tabbing}

\medskip
\noindent
The correctness of the manipulation of , 
, and the correctness of the reconstruction of the -augmenting
path  follow from Lemma \ref{Lemma4}, and are straightforward to prove.
The procedure RECONSTRPATH resembles standard recursive methods used for
the reconstruction of augmenting paths (see e.g. \cite{Ta}).
The time and space complexity of our implementation of MDFS remain to be
considered. It is easy to see that the time used by the algorithm MDFS is bounded
by  plus the total time needed for the manipulation of the sets
, .
If we use linear lists for the realization of the sets  with a 
pointer to the node  for each element of , the 
execution time for each union operation is bounded by . Following the 
pointer corresponding to , we can find the set containing  
in constant time.
At most  union operations are performed by MDFS. 
Hence the total time used for the manipulation of the sets  is 
bounded by . The time needed for the  union operations can be 
reduced to  if we use the following standard trick,
the so-called {\em weighted union heuristic\/}:

\medskip
We store with each set the number of elements of the set. A union operation 
is performed by changing the pointer of the smaller of the two sets which are
involved and updating the number of elements. Every time when the pointer with 
respect to an element is changed,
the size of the set containing this element is at least twice of the size of
its previous set. Hence, for each element, its pointer is changed at most
-times. Hence, the total time used for all union operations is 
. Altogether, the total time used for the augmentation of one 
augmenting path is .

\medskip
If we use for the update of the sets  disjoint set 
union \cite{Ta}, the total time can be bounded to be 
where  is the inverse Ackermann function.
Note that for each node  one find operation suffices for the decision 
of .
Furthermore, we can reduce these bounds to  using incremental tree set
union \cite{GaTa}.
The space complexity of MDFS is bounded by . The considerations above 
lead to the following theorem.
\begin{theo}
MDFS can be implemented such that it uses only  time and  
space.
\end{theo}

\section{The Hopcroft-Karp approach for general graphs}

In 1973, Hopcroft and Karp \cite{HK} proved the following fact. If one computes in 
one phase a maximal set of shortest pairwise disjoint augmenting paths
and augments these paths then  such phases would suffice. 
In the bipartite case, they have described an elegant simple  implementation 
of an entire phase. Let us sketch this implementation. First they have reduced 
the problem of finding augmenting paths to a reachability problem in a directed
graph  with two additional nodes  and .
Then, by performing a breadth-first search on  with start node 
until the target node  is reached, they have obtained a layered, directed
graph  for which the paths from~ to~ correspond exactly to
the shortest -augmenting paths in . Using depth-first search, they find a
maximal set of pairwise disjoint -augmenting paths. Whenever an -augmenting path
is found, the symmetric difference is applied to the path and the current matching, 
the path and all incident edges are deleted and the depth-first search is continued. 
Breadth-first search and depth-first search
take  time. Hence, the implementation of Hopcroft and Karp  of a phase has 
time complexity .
Since -augmenting paths can be found in general graphs by a slightly modified 
depth-first search (MDFS), the following question suggests itself:
Can we get an implementation of an entire phase by performing something like
breadth-first search followed by MDFS?
We shall give an affirmative answer to this question.

\subsection{The description of a phase}

Let  be an undirected graph,  be a matching of , and 
 be the directed graph as defined in Section 2.2.
Our goal is to construct from  a layered directed graph
 such that
\begin{enumerate}
\item the th layer contains exactly those nodes  such that a
      shortest strongly simple path from~ to~ in  has length~, and
\item  contains all shortest strongly simple paths from~ to~ 
      in .
\end{enumerate}
The {\em level} of a node  is the length of a shortest strongly simple path 
from  to . In , the th layer contains exactly the nodes of level
. It is clear that  is the only node in Layer 0. 
By the structure of~,  is odd and  is even for all 
. Since breadth-first search (BFS) on  with start node~ finds shortest 
simple distances from~ and not shortest strongly simple distances, BFS cannot be
used directly for the construction of . But we can modify BFS 
such that the modified breadth-first search (MBFS) finds shortest strongly 
simple distances.
Remember that for the construction of the st layer, BFS needs only
to consider the nodes in Layer , and to insert into the st layer 
all nodes~  which fulfill the following properties:
\begin{enumerate}
\item There is a node~ in the th layer with .
\item  has not been defined.
\end{enumerate}
With respect to finding strongly simple distances from~, the construction of
the st layer is a bit more difficult.
By the structure of , the level of a free node  is one and
the level of a non-free node~ is
well-defined by the level of the unique node~ with
. Hence, the construction of odd layers is trivial.
For odd , we shall describe the construction of the st layer under 
the assumption that Layers~ are constructed.
It is clear that similar to BFS, MBFS has to insert into the st layer 
all nodes  which fulfill the following properties:
\begin{enumerate}
\item There is a node~ in Layer  with , and there is a strongly simple path
        from~ to~ of length~ which does not contain the node .
\item  has not been defined.
\end{enumerate}
But these are not all nodes which MBFS has to insert into Layer .
Consider the example described by Figure 2.
\begin{figure}[t]
\setlength{\unitlength}{0.0094in}\begin{picture}(345,346)(165,370)
\thicklines
\multiput(510,585)(0.00000,10.00000){3}{\makebox(0.5926,0.8889){\tenrm .}}
\put(260,570){\vector(-1, 1){ 60}}
\put(355,626){\vector( 0,-1){ 55}}
\multiput(335,690)(0.55556,-0.55556){10}{\makebox(0.5926,0.8889){\sevrm .}}
\put(340,685){\line( 0,-1){ 10}}
\multiput(340,675)(0.55556,-0.55556){10}{\makebox(0.5926,0.8889){\sevrm .}}
\put(345,670){\line( 0,-1){ 10}}
\multiput(345,660)(0.55556,-0.55556){10}{\makebox(0.5926,0.8889){\sevrm .}}
\put(350,655){\vector( 1,-1){0}}
\put(465,705){\vector(-1, 0){ 70}}
\put(224,705){\vector( 1, 0){ 75}}
\multiput(508,657)(-0.55556,0.55556){10}{\makebox(0.5926,0.8889){\sevrm .}}
\put(503,662){\line( 0, 1){ 10}}
\multiput(503,672)(-0.55556,0.55556){10}{\makebox(0.5926,0.8889){\sevrm .}}
\put(498,677){\line( 0, 1){ 10}}
\multiput(498,687)(-0.55556,0.55556){10}{\makebox(0.5926,0.8889){\sevrm .}}
\put(493,692){\vector(-1, 1){0}}
\put(510,614){\vector( 0, 1){ 25}}
\put(510,552){\vector( 0, 1){ 25}}
\multiput(495,500)(0.55556,0.55556){10}{\makebox(0.5926,0.8889){\sevrm .}}
\put(500,505){\line( 0, 1){ 10}}
\multiput(500,515)(0.55556,0.55556){10}{\makebox(0.5926,0.8889){\sevrm .}}
\put(505,520){\line( 0, 1){ 10}}
\multiput(505,530)(0.55556,0.55556){10}{\makebox(0.5926,0.8889){\sevrm .}}
\put(510,535){\vector( 1, 1){0}}
\put(315,435){\vector( 4, 1){180}}
\multiput(200,655)(-0.55556,0.55556){10}{\makebox(0.5926,0.8889){\sevrm .}}
\put(195,660){\line( 0, 1){ 10}}
\multiput(195,670)(-0.55556,0.55556){10}{\makebox(0.5926,0.8889){\sevrm .}}
\put(190,675){\line( 0, 1){ 10}}
\multiput(190,685)(-0.55556,0.55556){10}{\makebox(0.5926,0.8889){\sevrm .}}
\put(185,690){\vector(-1, 1){0}}
\multiput(280,510)(-0.55556,0.55556){10}{\makebox(0.5926,0.8889){\sevrm .}}
\put(275,515){\line( 0, 1){ 10}}
\multiput(275,525)(-0.55556,0.55556){10}{\makebox(0.5926,0.8889){\sevrm .}}
\put(270,530){\line( 0, 1){ 10}}
\multiput(270,540)(-0.55556,0.55556){10}{\makebox(0.5926,0.8889){\sevrm .}}
\put(265,545){\vector(-1, 1){0}}
\put(280,440){\vector( 0, 1){ 45}}
\multiput(280,385)(0.00000,8.57143){4}{\line( 0, 1){  4.286}}
\put(280,415){\vector( 0, 1){0}}
\put(275,370){\makebox(0,0)[lb]{\raisebox{0pt}[0pt][0pt]{\egtrm }}}
\put(470,700){\makebox(0,0)[lb]{\raisebox{0pt}[0pt][0pt]{\egtrm }}}
\put(335,555){\makebox(0,0)[lb]{\raisebox{0pt}[0pt][0pt]{\egtrm }}}
\put(335,635){\makebox(0,0)[lb]{\raisebox{0pt}[0pt][0pt]{\egtrm }}}
\put(315,700){\makebox(0,0)[lb]{\raisebox{0pt}[0pt][0pt]{\egtrm }}}
\put(485,645){\makebox(0,0)[lb]{\raisebox{0pt}[0pt][0pt]{\egtrm }}}
\put(490,540){\makebox(0,0)[lb]{\raisebox{0pt}[0pt][0pt]{\egtrm }}}
\put(475,485){\makebox(0,0)[lb]{\raisebox{0pt}[0pt][0pt]{\egtrm }}}
\put(165,700){\makebox(0,0)[lb]{\raisebox{0pt}[0pt][0pt]{\egtrm }}}
\put(180,640){\makebox(0,0)[lb]{\raisebox{0pt}[0pt][0pt]{\egtrm }}}
\put(245,555){\makebox(0,0)[lb]{\raisebox{0pt}[0pt][0pt]{\egtrm }}}
\put(265,495){\makebox(0,0)[lb]{\raisebox{0pt}[0pt][0pt]{\egtrm }}}
\put(265,420){\makebox(0,0)[lb]{\raisebox{0pt}[0pt][0pt]{\egtrm }}}
\end{picture}
 \caption{Further node which has to be inserted into Layer .}
\end{figure}
Note that  but , 
since the unique shortest strongly simple path from~ to~
contains . The unique strongly simple path~ from~ to~
has length~14. Hence, . Therefore, MBFS has to insert nodes 
 into Layer  for which there is a shortest 
strongly simple path~ with 
. For the treatment of these nodes and for the knowledge if
there is a strongly simple path from  to  of length  which does
not contain the node , the following notation is useful.

Let  such that  is defined for all .
We denote by  the set of those nodes 
 which satisfy:
\begin{itemize}
\item[a)] For all , all shortest strongly simple paths from~ to
 contain ,
\item[b)]  has not been defined, and
\item[c)]  for all  satisfying a) and b).
\end{itemize}
If such a node  does not exist then  denotes the node .
Furthermore,  denotes the node .
We shall use  only for subsets  of  of size at most two.
Next we shall show that always . 
This will be a direct consequence of the following lemma.
\begin{lemma} \label{hk1}
Let  be any shortest strongly simple
path from  to , i.e., . Let  be a node with . Then  
and  for all .
\end{lemma}
{\bf Proof:} 
Since  is the th node on the strongly simple path  it holds that
level.
Suppose  and let  be any shortest strongly simple path
from  to . Let . 
Note that all nodes on  are not -free. Let .
By construction, . Since  is a shortest strongly simple path from  
to , the paths  and  cannot be strongly disjoint. Let  be the
first node on  such that  or  is on . 
If  is on  then . If  then the unique node  with
 has to be the direct predecessor of the node  on both 
paths  and . This contradicts the choice of the node . 
If  is on  then also . If  then the node  
such that  has to be on  the direct predecessor of  
and  has to be on  the direct successor of .
But this would also contradict the choice of the node . This shows
that in any case . Let

If , then  would be a strongly 
simple path from  to  shorter than , a contradiction. 
Hence, . But then

would be a strongly simple path from  to  shorter than . This
contradicts . Hence, level.

Since  is the th node on  it holds that .
Hence,  implies .
\QED

\medskip
The following lemma is a simple consequence of Lemma \ref{hk1}.
\begin{lemma} \label{hk1a}
Let ,  such that  is defined for all 
. Then the following statements hold true:
\begin{itemize}
\item[a)] 
.
\item[b)]
Let . Then after the definition of , always
.
\end{itemize}
\end{lemma}
{\bf Proof:} a) Assume that . Let  and  be two
distinct elements of . By the consideration of any shortest strongly 
simple path from  to  for any , applying Lemma \ref{hk1}, we obtain 
 or . This
contradicts Part c) of the definition of .

\medskip
\noindent
b)
After the definition of ,  decreases. Hence,
by Lemma \ref{hk1}, 
 has to be below  on all shortest strongly simple paths from
 to  for all . Hence,  is on all shortest strongly 
simple paths from  to  for all . Assume that 
. Then . 
By the definition of , there is a shortest strongly simple path  
from  to  
which does not contain . Hence,  cannot be a subpath of a shortest 
strongly simple path from  to  for a node . Let  
be any shortest strongly simple path from  
to . Lemma \ref{hk1} implies that . Since 
 is not a shortest strongly simple path, the paths  and  are 
not strongly disjoint. Let  be the first node on  such that  or  
is on . Exactly as in the proof of Lemma \ref{hk1}, we prove that such a node cannot 
exist. Hence, . Therefore, .
\QED

\medskip
Now we shall describe a phase in detail. Note that to each node  there correspond 
two different levels, namely  and . The {\em first level} of  
denoted by  is the smaller one of these two levels. The other level denoted by 
 is the {\em second level} of .

In \cite{Bl1,Bl4}, we have constructed during the
st phase exactly the shortest strongly simple paths of length . Especially for
the treatment of the shortest strongly simple paths 
with , we have used some sophisticated data structures to achieve a 
certain time bound. In \cite{Bl4}, we have used the dynamic nearest common ancestor algorithm 
of \cite{Ga2} in combination with Fibonacci heaps \cite{FrTa} to get an
 implementation of a phase. We have also described an alternative to the
use of Fibonacci heaps which uses two-dimensional arrays to get an 
implementation. A better way is to use an idea of Micali and Vazirani \cite{MV,Va1}. 
For all nodes , they compute the first level during Phase  and the second level 
during Phase . We shall incorporate this idea into 
our framework.

Each phase separates into two parts. Both parts use breadth-first search.
The first level of each node  is computed during Part 1 of Phase .
The second level of each node  is computed during Part 2 of Phase
. Before describing the two parts of a phase,
we shall investigate the structure of shortest strongly simple paths defining the first 
and the second level of a node.
First we characterize exactly those shortest strongly simple paths which define
the first level of a node to be . 

\medskip
If  is even then  for a node  iff for the unique node 
 such that  it holds that . 
Note that  implies that  such that  cannot 
be the first level of the node . Each shortest strongly simple path from  to the
node  followed by the edge  is a shortest strongly simple path
from  to . Furthermore, there is no other shortest strongly simple path from
 to .

If  is odd then  for a node  iff  and 
 are larger than  and there is a node  with 
and the edge  is in . Exactly the shortest strongly simple paths
from  to such a node  followed by the edge  are
the shortest strongly simple paths from  to .

\medskip
Next, we shall characterize exactly those shortest strongly simple paths which define
the second level of a node to be . 

\medskip
If  is even then  for a node  iff ,
, and for the unique node  such that  
it holds that . Note that the node  cannot be on a strongly simple
path from  to  since the unique direct successor of  has to be the node
. With respect to the node , two cases
are possible,  and . Exactly the shortest 
strongly simple paths from  to the node  followed by the edge 
are the shortest strongly simple paths from  to .

If  is odd and  for a node  then  and 
. With respect to a shortest strongly simple path  from  to ,
for the direct predecessor  of , exactly two situations are possible,
 and .

In the case that  two subcases can happen,  and
. Exactly the shortest strongly simple paths  from  to such a 
node  with the property that  is not on  followed by the edge 
 are the shortest strongly simple paths from  to .

If  then consider any shortest strongly simple path\\  from 
 to . Let  be the last node on  such that the length of the subpath
from  to  of  is equal . Since ,
the node  has to be on . Since the level of the direct successor of  on  is
one, the node  on  exists. 
Let . As we shall prove later,
 has to be on the back-path of a shortest strongly simple path from 
to the node .

\medskip
Next, we shall describe MBFS in detail.
At the beginning,  and .
In the first phase, the node  is inserted into Layer 0. For each -free node ,
the node  is inserted into Layer 1 and the edge  is inserted into
. Assume that  and Phase  is finished. 
We shall give a detailed description of the two parts of Phase .

\medskip
\noindent
{\em Part 1 of Phase }

\medskip
If  is even then MBFS adds for all nodes  with  
the unique edge  to  and inserts the node  into 
the st layer. Note that  implies that the first level of
node  is already defined.

If  is odd then MBFS considers all edges 
 with level and  has not been considered 
during Part 2 of a previous phase. If the edge  has been considered during
Part 2 of a previous phase then  is already defined. Three cases are possible.

\medskip
\noindent
{\bf Case 1:}  and .

\medskip
MBFS inserts the node  into the st layer and adds the edge
 to .

\medskip
\noindent
{\bf Case 2:}  and .

\medskip
Then the first level of the node  is already defined. But for the computation of the
second level of  and some other nodes, the edge  is needed. Hence, MBFS 
inserts the pair
 into the set  where .

\medskip
\noindent
{\bf Case 3:} .

\medskip
Then the first level of the node  is also defined. 
But the edge  might be necessary for the computation of the second level of 
some nodes. Hence, if  is already computed then the pair  is 
inserted into  where . Otherwise, this pair 
is inserted into  directly after the computation of .

\medskip
These are all cases. Next we shall describe the second part of Phase .

\medskip
\noindent
{\em Part 2 of Phase }

\medskip
During Part 2 of Phase ,  will be computed for all nodes  with
. 
MBFS considers all pairs of nodes  where  if  is even and 
 if  is odd such that
\begin{enumerate}
\item
 and  have been defined,
\item
, and
\item
.
\end{enumerate}
Note that these are exactly the pairs in .
Starting in  and , MBFS performs a breadth-first search on the back-paths 
of the current layered network until  is reached.
All visited nodes  such that  has not been defined
and  are inserted into Layer  where
. 
The edges  and  are inserted into 
. Furthermore, MBFS adds the traversed edges which are not in  
to .

\subsection{The correctness proof of MBFS}

We say that a path  {\em is constructed} by MBFS if all edges on  are inserted into
. We have to prove the following: 
\begin{enumerate}
\item
For all nodes , MBFS computes both levels  and  correctly.
\item
The layered network  computed by MBFS contains all shortest
strongly simple paths from  to .
\end{enumerate}
The correctness of MBFS is a direct consequence of the following theorem.

\begin{theo} \label{hk4}
Let  and . Then the following holds:
\vspace{-0.2cm}
\begin{itemize}
\item[a)]
MBFS defines  correctly during Part 1 of Phase . All 
shortest strongly simple paths from  to  have been constructed after the 
termination of Part 1 of Phase . 
\vspace{-0.2cm}
\item[b)]
Level has been defined and all shortest strongly simple paths from  to 
 have been constructed after the termination of Part 2 of Phase 
.
\end{itemize}
\end{theo}

\noindent
{\bf Proof:} 
We prove the theorem by induction on the number of performed phases. 
By construction, it is clear that the assertion of the theorem is fulfilled after the 
termination of Phase 1. Note that no second level of a node has to be computed during
Phase 1. Assume that  and that the assertion of the theorem is fulfilled after
the termination of Phase . We shall prove that the assertion also holds
after the termination of Phase .

First we shall prove that for all nodes with first level , their first level is computed 
correctly during Part 1 of Phase . Furthermore, the corresponding shortest strongly simple 
paths are constructed after termination of Part 1 of Phase . No level of any other node
is determined during Part 1 of Phase .

If the assertion of the theorem does not hold for a node  then the assertion also
does not hold for the unique node  with . Hence, if  is
even, the induction hypothesis implies that the assertion is fulfilled after the termination
of Part 1 of Phase . Therefore, we have only to consider the case that  is odd.

Consider any node  with . Let  be any shortest 
strongly simple path from  to . Let  be the direct predecessor of  
on . Since , the node  cannot be a node on a shortest 
strongly simple path from  to . Hence, .
Since the assertion of the lemma has been maintained in previous phases, the node 
and the edge  have been considered during Part 1 of Phase . Therefore,
the node  is inserted into Layer  and the edge  is inserted into 
. Since each shortest strongly simple path from  to  is contained in the 
current layered network, the path  is constructed as well.
On the other hand, if we have a shortest strongly simple path  from  to a node  
of length  and
an edge , where  is not defined, then the path  is a shortest
strongly simple path from  to . Hence, . Therefore, only for nodes 
with first level , the first level is computed during Part 1 of Phase . 
This shows that the assertion is fulfilled after the termination of Part 1 of Phase 
such that Part a) of the theorem is proved.

\medskip
Next we shall show that the assertion of the theorem is fulfilled after the termination
of Part 2 of Phase . It is clear by construction that during Part 2 of a phase only 
the second level of some nodes is computed. We have to prove that after the termination 
of Part 2 of Phase , the following properties are fulfilled.
\begin{enumerate}
\item
For all nodes  such that  is computed during Part 2 of Phase  
it holds that  and  is computed
correctly.
\item
For all nodes  with , 
the second level of  is computed correctly during Part 2 of Phase  and
all shortest strongly simple paths from  to  with  have
been constructed after the termination of Part 2 of Phase .
\end{enumerate}
First, we shall prove that the first property is fulfilled and then we shall consider the
second property. 

\medskip
Assume that the first property is not fulfilled. Consider the first situation such that
the first property is not fulfilled. Assume that this situation occurs during the backward 
search with respect to the pair  of nodes because of the assignment 
. Since a breadth-first search on the back-paths is performed and this is 
the first time that the first property is not fulfilled,  has to be larger than 
. Furthermore, for each predecessor of  on the back-path, its second level is computed
correctly. 
By the construction of the algorithm MBFS, there also holds .
First we shall prove that  and then that 
there is a shortest strongly simple path from  to  of length .

By construction, . 
Hence,

and therefore,

Assume that no shortest strongly simple path from  to  of length  exists. 
By Lemma \ref{hk1}, on all shortest strongly simple paths which contain the node 
, the length of the subpath from  to  is equal
. Furthermore, since  is computed during the backward 
search, . Moreover, 
.

Since no shortest strongly simple path from  to  of length  exists, the node
 has to be on the all shortest strongly simple paths from  to the direct
predecessor of  during the backward search. Each such strongly simple path consists of
a shortest strongly path from  to  or  and the backpath of a shortest strongly 
simple path from  to  and , respectively until the node  is reached 
without the node . Therefore, the subpath from  to  and , 
respectively of these paths contains the node .
Since , there is a shortest strongly simple path 
from  to at least one of the nodes in  which does not contain the node 
. W.l.o.g., let  be such a path.
Let  and  be any 
shortest strongly simple paths from  to  and to , respectively such that
the path  
is strongly simple and constructed during the backward search. Consider the path

By construction, . Since , the paths  and  and hence, the
paths  and , are not strongly disjoint. Let  be the first node on  
such that one of the nodes  or  is on . Let

If  is on  then  would be a strongly simple path from
 to  shorter than , a contradiction. Hence,  is a node on .
By construction,  is strongly simple. Hence, 

If this inequality is strict then .
Hence, the paths  and  are not strongly disjoint. 
Let  be the first node on  such that one of the nodes
 or  is on . If  is on  then a strongly simple path from 
to  shorter than  can easily be constructed, a contradiction. Hence,  is 
a node on . Let .  We distinguish two cases.

\medskip
\noindent
{\em Case 1:}  is on .

\medskip
Let . Consider the path 

By construction,  is a path from  to  shorter than .
By the choice of the nodes  and , the path  is also strongly simple, a 
contradiction. Hence, Case 1 cannot happen.

\medskip
\noindent
{\em Case 2:}  is on .

\medskip
Let . Consider the path

Note that . Hence,  cannot be strongly simple. By the choice of the nodes
 and , the paths  and  are not strongly disjoint. Let
 be the first node on  such that one of the nodes  or  is a node on 
. 
If  is on  then a strongly simple path from  to  shorter than  
can easily be constructed, a contradiction. Hence,  is a node on . 
Let  and . Consider the path

By construction,  is a path from  to  shorter than .
By the choice of the nodes  and , the path  is also strongly simple, a contradiction. 
Hence, also Case 2 cannot happen.

\medskip
Altogether, we have proved . Consider the path

By construction, . Since , the path  cannot be strongly simple. By
construction, the paths  and  are not strongly disjoint. Let  be the first
node on  such that one of the nodes  or  is on . 
If  is on  then a strongly simple path from  to  shorter than  
can easily be constructed, a contradiction. Hence,  is a node on  and hence, 
is on .  Let
 and . Consider the path
 
By construction, the path  is strongly simple. Since  is a shortest strongly simple
path from  to  it holds that .
In the same way as we have proved , we prove 
 . Hence, . This contradicts our assumption 
that . 
Altogether, the first property is proved.

\medskip
Assume that the second property is not fulfilled. Let  be a node with smallest second
level such that the second property is not fulfilled with respect to the node . This means 
that
\begin{enumerate}
\item
 and there is a shortest strongly simple path 
 with  but 
\item
 has not been constructed after the termination of Part 2 of Phase  or 
 has not been computed correctly. 
\end{enumerate}
With respect to , the situations  and  are possible. We shall investigate 
both cases one after the other.

\medskip
\noindent
{\em Case 1:} .

\medskip
Two subcases are to consider,  and
.

\medskip
\noindent
{\em Subcase 1.1:} .

\medskip
First we shall prove that . 
If  then  and
 imply .
Let  be any shortest strongly simple path from  to
. Since , the nodes  and  cannot be on . 
Hence,  would be a strongly simple path from  to  shorter than 
, a contradiction. Furthermore,  and
 imply  and also
. Hence, during Part 1 of Phase , Case 2 applies such
that the pair  is inserted into . Therefore,
during Part 2 of Phase , the backward search with respect to the pair 
 has been performed.

By the induction hypothesis, at the beginning of Part 2 of Phase , all shortest strongly
simple paths from  to  and all shortest strongly simple paths from 
to  are constructed. Since during Part 2 of Phase , the backward search with 
respect to the pair  is performed, the edges
 and  are inserted into the layered graph. 
Hence, after the termination of Part 2 of Phase , the path 
 is constructed. By construction of the algorithm MBFS,
 has been computed correctly as well. This contradicts our assumption such that
Subcase 1.1 cannot happen.

\medskip
\noindent
{\em Subcase 1.2:} .

\medskip
Let  be any shortest strongly simple path from  to 
. Consider the path . 

If  is a shortest strongly simple path from  to  then 
. 
By the choice of the node , all shortest strongly simple paths from  to 
 has been constructed. By the induction hypothesis,  is also constructed. 
Hence, during the backward search which constructs the path ,
the edge  is added to the layered network. Hence, the path
 is also constructed, a contradiction.
Therefore,  is not a shortest strongly simple path from  to . 

If  is strongly simple
then . Hence, . 
By the induction hypothesis,  has been computed and all shortest strongly 
simple paths from  to  have been constructed. Furthermore, 
. Hence, by the induction hypothesis, all shortest 
strongly simple paths from  to  have been constructed and the edge 
 has been considered during Part 1 of Phase
. 
Therefore, during Part 2 of a Phase  where , the backward search has been performed
with respect to the pair  and the edge 
 has been inserted into the layered network such that the path 
has been constructed after the termination of Part 2 of Phase , a contradiction.

Hence, the path  is not strongly simple. Since 
is strongly simple, the node  or the node  has to be on the path . Let

If  is on  then . Exactly as in the case `` is strongly simple but
'', we prove that the path  has been constructed after the termination of 
Part 2 of Phase  getting a contradiction. Hence,  is on . But then, 
 would be a strongly simple path from  to  shorter than 
, a contradiction. Therefore, Subcase 1.2 and hence, Case 1 cannot happen.

\medskip
\noindent
{\em Case 2:} .

\medskip
Let  be any shortest strongly simple path from  to . 
Let  be the last node on  such that the length of the subpath from  
to  of  is equal to . Let ,
let  and let 
.

First we shall show that for all nodes  on , the level of  is not larger than
the length of the subpath  of . Then we shall use 
this fact to prove that all edges on the path  are contained in the current layered graph. 
Since , the pair  has been considered during Part 2
of a Phase  where . Hence, the path  is constructed after the execution of 
Part 2 of Phase , a contradiction.

To prove  assume that  for a node 
on . Then  cannot be strongly simple such that  and  are
not strongly disjoint. Let  be the first node on  such that one of the nodes
 or  is a node on . Let 

If  then  would be a strongly simple path from  to  
shorter than , a contradiction. Hence, . But then,
 would be a strongly simple path from  to  shorter than
, a contradiction. Altogether, we have proved that .

Assume that an edge  is not contained in the current layered network
after the execution of Part 2 of Phase . Since , we obtain
. Hence,
. Therefore, the pair  has been
considered and the edge  has been inserted into the current layered 
graph during Part 2 of a Phase  where , a contradiction. Altogether we have shown 
that the path  is constructed after the execution of Part 2 of Phase .
Moreover, since the pair  has been considered during Part 2 of Phase ,
 has been computed correctly because of the consideration of this pair of nodes.
This shows that Case 2 cannot happen as well. This proves the second property.

\medskip
Altogether, the theorem is proved.
\QED

\subsection{An implementation of an entire phase}

First we shall describe the implementation of MBFS, and then we shall show how to 
combine MBFS and MDFS to get an implementation of an entire phase.

Obviously, Part 1 of all phases can be implemented in such a way that the 
total time is bounded by . For the implementation of Part 2 of all 
phases, we have to describe how to implement the search on the back-paths,
starting in  and , until  is reached.
Most importantly, since we do not know  in advance, 
meaning that  has to be computed simultaneously, we have to 
take care that the search does not continue beyond .
Note that by Lemma \ref{hk1}, the subpaths from  to 
are always shortest strongly simple paths from  to .
Hence, we can perform a breadth-first search on the back-paths until the current 
level contains exactly one node. By Lemma \ref{hk1}, this node has to be 
. 

With respect to the efficiency, at the moment when the search meets a node
 for which  has been defined, we have to compute efficiently
the next node of the search having the property that its level is not defined. 
By definition, this node is . As a consequence of Lemma \ref{hk1a}, we can 
maintain these nodes by a data structure for disjoint set union such that for the computation 
of 
one {\em find\/} operation would suffice. In that case, an extensible edge would be stored.
Using disjoint set union \cite{Ta}, the total time can be bounded to be  
where  is the inverse Ackermann function. Using incremental tree set union 
\cite{GaTa}, we obtain a total time bound of  for the computation of 
the next node such that its level is not defined.

The levels of the nodes  and  have not to be equal. If the level of the 
two nodes are different then we start the breadth-first search at the node with 
larger level. We always continue the search at the nodes with largest level
until all nodes in the front of the search are on the same level. But we do not need
to perform a precise breadth-first search. Hence, we can organize the search in the following way:
\begin{enumerate}
\item
In the front of the search, continue the search always in a node  such that
there is another node  in the front of the search with .
\item
If the front of the search contains exactly one node then stop the search.
\end{enumerate}
Next, we shall combine MBFS and MDFS for the implementation of an entire phase.
Knowing , a maximal set of up to  and  pairwise disjoint shortest
strongly simple paths from  to  in  can be computed using MDFS in  
time. Every time, a strongly simple path~ from~ to~ is found, all
nodes ,  with  or  and all incident edges
are deleted from~. If a node gets zero indegree or zero
outdegree, then also this node, and all incident edges, are deleted.
Altogether, we have obtained the following theorem.

\begin{theo}
A maximum matching in a general graph  can be computed in
 time and  space, where 
 and .
\end{theo}

\section{The primal-dual method for the \\maximum weighted matching problem}

Let  be an undirected graph. 
If we associate with each edge  a weight  then we
obtain a {\em weighted undirected graph\/} .
The weight  of a matching  is the sum of the weights of the edges in
. A matching  has {\em maximum weight\/} iff   for all matchings
.
Given a weighted undirected graph , the {\em maximum weighted
matching problem\/} is finding a matching  of maximum weight.
Our goal is to develop a method for the computation of a maximum weighted 
matching in a given weighted undirected graph.

\subsection{The description of the method}

Let  be a weighted undirected graph. Let ,  be a family of pairwise distinct subsets of 
. With each node  we associate a {\em node weight\/} . Furthermore, with each edge set , we associate a 
{\em set weight\/} . These new variables are called {\em dual 
variables}.
The values of the dual variables are treated in such a way that the following 
invariant is always maintained.
\begin{itemize}
\item
 for all .
\end{itemize}
The right side of this inequality is the {\em dual weight } of
the edge .
We define the {\em dual weight } of a matching  by

Note that because of the invariant, always  for all matchings 
.  
With respect to an arbitrary matching , the maximal possible 
contribution of the node weight  to  is  since  is 
adjacent to at most one edge in . Note that  where
 is the size of a maximum cardinality matching with respect to . 
Hence, the maximal possible contribution of the set weight  to  is
. Hence,

is always an upper bound for the dual weight of any matching of .
Therefore, with respect to a matching , 
 
implies that the matching  has maximum weight.
The question is now: When with respect to a matching , this equation holds?

The right side of the equation contains for each edge in  its dual weight.
Since the dual weight of an edge is at least as large as its weight, we obtain 
the necessary condition  for all edges .
Since all summands in both sums of the right side of the equation are non-negative, 
the node weight  has to be zero for all -free nodes .
Furthermore, for all  such that , the set weight
 has also to be zero. On the other hand, these conditions are fulfilled with
respect to a matching  for which . Altogether, we have obtained the 
following necessary and sufficient {\em optimality conditions\/}:
\begin{enumerate}
\item
 for all ,
\item
 for all -free nodes , and
\item 
 for  implies .
\end{enumerate}
The value  is called the {\em reduced cost\/} of the edge . Because of
the invariant which is maintained, the reduced cost of an edge is always non-negative.

\medskip
The primal-dual method for the weighted matching problem can be separated into 
rounds. The input of every round will be a matching , a set  of pairwise 
distinct subsets of  such that  for all , and values 
for the dual variables which fulfill the first and third optimality conditions with 
respect to the matching . The second optimality condition can be unsatisfied with 
respect to some free nodes. Our goal within a round is to modify  and the 
values of the dual variables such that Conditions 1 and 3 remain valid
and the number of nodes violating Condition 2 is strictly decreased.

A round divides into two steps, the {\em search step\/} and the
{\em extension step}. The search step tries to improve
the current matching by finding an augmenting path  such that the number
of free nodes with node weight larger than zero can be decreased by the 
augmentation of . Since Condition 1 has to be maintained, the search step
can only be performed on edges with reduced cost zero. If the search step cannot
improve the current matching then the extension step
changes the values of some dual variables using an appropriate value . 
The extension step can decrease the reduced cost of some edges to zero. Hence, 
it is possible that the next search step will find an augmenting path.

During the search step, we use MDFS. Hence, we define with respect to the current 
matching  the weighted directed bipartite graph  where

This means that  contains for each edge in  two copies.
Both copies of the edge  obtain weight , dual weight , and hence, 
reduced cost .
We arrange that edges with tail  or head  have always reduced cost zero. 
According to Condition 1,
it is only allowed to consider augmenting paths where all edges on these
paths have reduced cost zero. Hence, the input graph  of the
search step will be the subgraph of  containing exactly those edges in  
having reduced cost zero; i.e., 

By definition, for each edge  it holds that 
. We start with the empty matching  
and define the graph  as described above.
Let . We initialize all node weights 
by . Altogether, we obtain the input graph 
 for the first search step where

At the beginning, the family  of subsets of  will be empty such 
that no set weight has to be defined. 
During the execution of the algorithm, the needed elements of  and the corresponding 
set weights will be defined. 
Whenever to some edge set  a value
 is associated,  enters . 
As soon as  gets to be zero, the set  is removed from .

A search step terminates with a matching , a weighted directed graph
, a current subgraph  of  such that
no -augmenting path  is contained in , a current set  where
the set weight of each set in  is positive, and values 
for the dual variables. This is the input of the next extension step.

For the treatment of the extension step consider the expanded MDFS-tree 
, computed by the last MDFS on . Note that this MDFS was 
unsuccessful; i.e., no path from the start node  to the target node  has been 
found. The goal is to add edges to  such that 
possibly an augmenting path can be found. Therefore, we have to decrease the 
reduced cost of edges with positive reduced cost. Such an edge  has to 
be in  and  has to be in . Let

Furthermore, let

Later, we shall see that according to the optimality
conditions which we have to maintain, some nodes in  are
not allowed to be a node in . Hence, we shall modify the definition of
 by removing exactly these nodes.

The idea is to decrease the reduced cost  of all edges  with 
positive reduced cost and  by an appropriate value .
This is done by decreasing the node weight  by  for all nodes 
 with .
As a consequence of the decrease of the node weight , the reduced cost
of each edge  in  with end node  or  becomes negative. 
Because of the invariant maintained by the method, such a reduced cost has to be increased 
until its value is zero again. We distinguish two cases:
\begin{enumerate}
\item
The other end node of  is in  but not in .
\item
The other end node of  is in .
\end{enumerate}
If Case 1 is fulfilled then we can increase the reduced cost of the edge  by 
increasing the node weight of the other end node of  by ; i.e.,
we increase  by  for all nodes  such that 
and .
Note that increasing the node weight  of a node  increases the reduced 
cost of each edge .
If  then  has only to be non-negative. Since
 was non-negative before the increase of , it is also non-negative after 
the increase. If  then  has to be zero. Note that 
implies for the unique node  with  that .
Hence,  has been decreased by the same value such that  
before the change of the dual variables implies that  after the
change.

If Case 2 is fulfilled then the reduced cost  is decreased by 
. This can be corrected by increasing the set weight  of
exactly one set  containing the edge  by .
Two questions have to be answered.
\begin{enumerate}
\item
What is the accurate edge set  for increasing its set weight?
\item
What is the appropriate value ?
\end{enumerate}
To answer the first question, let us consider MDFS which is used as a subroutine 
during the search step. 
Review the definitions and properties of the sets  and  as given 
on Pages \pageref{WM1} and \pageref{WM2}, respectively. At the moment when an edge set  
is chosen to obtain a positive set weight , the set  corresponds to the 
current set  as defined on Page \pageref{WM2} and  enters .
Note that  can contain edge sets which are generated during a previous run
of MDFS. The corresponding set  has not to be current with respect to the
current run of MDFS. Hence, we introduce an analogous terminology to ``current'' with
respect to the edge sets in .
A set  is called {\em maximal\/} iff  for all
.
Now, it is useful to investigate the structure of a set . Let

and

The node  is the unique node  such that  is end node of an
edge  with . 
If a path  runs through a set  then there is an edge such
that  enters  using this edge and also an edge such that  
leaves  using that edge. 
We say that a path  {\em enters\/} or {\em leaves\/}  
{\em via an edge\/} {\em in\/}  if the used edge  
corresponds to an edge .
Let  be the unique matched edge with end node . 
During the execution of MDFS, for an -augmenting path , there are three 
possibilities to run through a set .
\begin{enumerate}
\item
 enters  via the matched edge  and leaves
 via an edge in .
\item
 enters  via an edge in  and leaves 
 via the matched edge .
\item
 enters and leaves  via an edge in .
\end{enumerate}
If an -alternating path  enters  via the edge 
 then, by Lemma \ref{Lemma3}, for all , 
. Therefore, with respect to each edge  
in  with both end nodes in , the node weights  and
 have been decreased by  such that we have to increase the set
weight of exactly one edge set containing the edge  by .
Note that for all  there exists at most one current 
such that . Hence, we define the edge set  
corresponding to  where  current by

If we have to increase the set weight with respect to an edge  
then we choose the edge set  where  is the current set such that 
. Hence, for all current sets , we perform the following
operation:
\begin{itemize}
\item
If  is not already in  then  is inserted into  with 
. Otherwise,  is increased by . 
\end{itemize}
The following lemma shows that in this situation, the current set  always exists.
\begin{lemma} \label{pd1}
Let  and . Then there exists 
a current set  such that .
\end{lemma} 
{\bf Proof:} 
Note that  iff .
With respect to the positions of the nodes  and  in the MDFS-tree , exactly
the following two situations are possible: 
\begin{enumerate}
\item
There is no path  from the root  to a leaf of  such that both
nodes are on .
\item
There is a path  from the root  to a leaf of  such that both
nodes are on .
\end{enumerate}
If the first situation arises then there exists a unique node in  such that both nodes 
 and  are successors of this node in  and no successor of this node in  
has this property. Since no -augmenting path has been found during the last MDFS, this 
node cannot be the node . Hence, this node has to be a node .
Let  be the path from  to the node  in . Furthermore,
let  and  be the paths in  from  to  and 
, respectively. 
By the definition of  and ,
because of the paths  and
,  if . 
If , i.e., PUSH has been performed, then, by Lemma \ref{Lemma4},
. By the definitions of the current set  and
the set , the assertion follows directly.

Assume that the second situation arises. W.l.o.g., let  be a successor of  
in . Let  be the path from  to  in . 

If  then, because of the path  and
the definition of , it follows . By the definition
of , the node  is contained . The definition of  implies 
 such that the assertion follows directly.

If  then, by Lemma \ref{Lemma4}, . Hence, the assertion 
follows directly.
\QED

\medskip
Let us examine the effect of the augmentation of the path  to the number of edges
in the current matching with both end nodes in .
If  enters and leaves  via an edge in , then this number  
decreases by one. In the other two cases, this number does not change.
Hence, the augmentation of an augmenting path which goes through 
using the unique matched edge with one end node in  and the other end node 
not in  is always allowed. But the augmentation of an augmenting path which
enters and leaves  via an edge in 
is only allowed if ; i.e.,  is not contained in .
Now, we shall determine the accurate value for .

Since all node weights have to be non-negative,  cannot be larger 
than the node weight of any -free node . Note that with respect to an -free
node , always  is fulfilled. Since all nodes are initialized with the same 
node weight and always the node weights of all nodes in  are decreased using the same 
value, all free nodes have the same node weight and no non-free node has a smaller node weight.
Hence,  where  is -free implies that after the 
change of the dual values, all node weights are non-negative.

Let . Then  will be decreased by . For , 
in dependence of the status of the other end node  with respect to  and , 
three situations can arise:

If  and  then  will not be changed by the
extension step. Hence, we have to choose  
to decrease the reduced cost  of the edge  to zero.
 
If  and  then  will be increased by .
Hence, independently of the choice of , the reduced cost  does not change.

If  then  will be also decreased by . Hence, we have to 
choose  to decrease  to zero.

Note that  has to be chosen in such a way that after the extension step
 for all edges . Hence,  should not be 
larger than the minimal reduced cost with respect to edges  with
,  and , and also not larger than 
the half of the minimal reduced cost with respect to edges  with 
, , and there is no current  such that 
. Note that  with respect to a current set
 implies that  does not change since  is increased by 
. Because of Lemma \ref{pd1},  implies that there is a current 
 such that .

Since MDFS obtains in addition to the input graph the current set  and the  
corresponding set weights, because of the maintenance of the third optimality 
condition, the following holds true with respect to a maximal :

Assume that during the MDFS no path uses the edge  to enter ; 
i.e., . If there is a path  entering  via an edge 
in  then  implies that  has to leave  via 
the edge , independently if the node  is already pushed or not. Since
 and therefore , this is always possible.
Note that more than one such a path can run through .
Depending on the nodes in  which are entering nodes of such a path ,
the following situations can happen with respect to a node .
\begin{itemize}
\item[a)]
,
\item[b)]
 but , or
\item[c)]
 and .
\end{itemize}
The problem to solve is the following: How to change the node weights of the
nodes in ?
Each entering node  of  is the head of an edge 
where . Hence,  is decreased by . According to 
the first optimality condition,   has to be increased by . 
Possibly, there are edges in  with exactly one end node is an entering node, 
with both end nodes are entering nodes or with no end node is an entering node. 
With respect to all these cases, the node weights and  have to be changed 
in such a way that the optimality conditions remain to be valid.
For doing this, we increase  by  for all .
Since we have increased the reduced cost of all edges in  by ,
we decrease  by . Since  has to be non-negative, 
 has to be chosen such that before the change of the dual variables,
. 

Remember that we consider the situation that .
Possibly, there exist nodes  which are
in . Since we cannot decrease  by  and increase  by
 at the same time, it is not allowed for such a node to be in . 
Hence, we define 

and we redefine

Altogether,  can be defined in the following way.

Then we define

Altogether, we have obtained the following extension step.

\begin{tabbing}
AA \= AA \= \kill
 for an -free node ; \\
;\\
   \\
 ; \\
; \\
; \\
{\bf for} all  \\
\> {\bf do} \\
\> \>  \\
\> {\bf od}; \\ 
{\bf for} all (, ) {\bf or} (,  
maximal with \\
\> \> \qquad \qquad \qquad \qquad \qquad \qquad  but ) \\
\> {\bf do} \\
\> \>  \\
\> {\bf od}; \\ 
{\bf for} all current  and  \\
\> {\bf do} \\
\> \>  \\
\> {\bf od}; \\
{\bf for} all maximal ,  and  \\
\> {\bf do} \\
\> \>  \\
\> {\bf od}.
\end{tabbing}

The correctness of the described primal-dual method follows from the 
discussion done during the development of the method. Note that the primal-dual method described 
above is equivalent to the method developed by Edmonds \cite{Ed2}.

\subsection{An implementation of the primal-dual method}

First we shall determine an upper bound for the number of dual changes which can occur between 
two augmentations in the worst case. Then we shall describe the implementation of the search
steps between two augmentations. Finally, we shall describe an implementation of the
computation of  and the update of the dual variables. With respect to the determination
of the upper bound, we have to consider four cases.

\medskip
\noindent
{\em Case 1:\/} 

\medskip
Then after the change of the dual variables,  for all -free nodes
. Therefore, the current matching  is of maximum weight and the 
algorithm terminates. Hence, Case 1 occurs at most once.

\medskip
\noindent
{\em Case 2:\/} 

\medskip
Then during the next search step, at least one new node  enters .
Hence, Case 2 occurs at most  times.

\medskip
\noindent
{\em Case 3:\/} 

\medskip
Then during the next search step, at least one new edge  enters 
. Furthermore, at the moment of the definition of the value ,
there is no current  with . Lemma \ref{pd1}
shows that after the next search step there exists a current set 
such that  contains both nodes  and . This means, by the
union of two smaller current sets, a larger current set is obtained. 
Between two augmentations, the number of such unions is bounded by .
Hence, Case 3 occurs at most  times. 

\medskip
\noindent
{\em Case 4:\/} 

\medskip
Then at least one edge set  leaves the family . As long as 
, no edge set  can enter the family .
But if  is pushed, a corresponding edge set  cannot contribute to 
the definition of the value  before  leaves  again. 
This cannot happen before the next augmentation. Hence, Case 4 occurs at most 
 times.

\medskip
Altogether we have shown that the number of dual changes between two augmentations
is bounded by .

\medskip
With respect to the implementation of the search steps between two augmentations,
MDFS has to be adapted to the situation that the current search path enters
the set  via an edge in 
with respect to a maximal edge set .
According to the discussion during the development of the primal-dual method,
we have to jump to the node  and to continue the search using the unique edge
. This can be organized using a data structure for
disjoint set union. But this data structure has also to support a further operation.

If according to an extension step,  becomes zero, the corresponding edge
set leaves the family . Note that  is a maximal set in . 
If there is a set  in  then another set in  becomes 
to be maximal. This means that we have to undo the union operation performed with respect 
to the set . 

With respect to the implementation of MDFS, we have introduced a data structure for
disjoint set union which uses the weighted union heuristic. Our goal is now to extend
this data structure to support also deunion operations such that the time used for the 
deunion operations will be, up to
a small constant factor, the same as the time used for the union operations and each 
find operation uses only constant time. This can be done in the following way:

During the execution of an union operation, instead of changing a pointer, we add a
new pointer. The current pointer of an element will be always the last created pointer.
Since the pointer of any element is changed at most  times, for each element, 
at most  extra pointer are used. The time used for the union operations
remains essentially the same. A deunion can be performed by the deletion of the current 
pointers created during the corresponding union operation, the update of the set sizes 
and the update of the name of the larger subset. The time used for a find operation
remains to be constant. Altogether, the data structure for 
union-find-deunion can be implemented such that the time used for at most  find,
 union and  deunion operations is .

After an extension step, the last MDFS can be continued instead of
to start a new MDFS. 
Next we shall describe an implementation of the computation of the value  and
the update of the dual variables.

\medskip
Since all -free nodes  have the same dual weight, 
can be computed by the consideration of any -free node in constant time.

For the computation of , we maintain a priority queue 
which contains for all  with  an item
which points to a list containing exactly the edges  with 
of minimum reduced cost within all such edges, if such an edge exists. The 
associated key with this item is the minimum reduced cost of these edges.
Furthermore, using an array of size , we have direct access to the list in  
containing all edges with end node , if such an edge in  exists.
Note that each extension step decreases the keys of all items in the
priority queue by the current . It is useful to maintain the property
that always the key of all elements in  has to be decreased by the same
amount. Hence, we maintain the sum  of all dual changes
done so far. If we add an edge  to the priority queue then we define
the modified reduced cost of the edge to be .
At the moment when a node  enters , it is possible that we have to insert some
edges into . Hence, we update  with respect to  
at that moment in the following way:

\medskip
For all edges  with  and  perform 
the following update operations: 
\begin{itemize}
\item[(1)]
If no element with respect to  is contained in the priority queue then
we create a list which contains the element  with modified reduced cost 
. We insert a new item which points to this list with associated
key  into .
\item[(2)]
If  contains an item which points to a list containing edges with end node
 with larger associated key than  
then delete all edges from the list, insert the edge by  into the list and 
decrease the key of the item such that its value becomes .
\item[(3)]
If  contains an item which points to a list containing edges with end node
 with associated key  then add the edge  to the 
list.
\item[(4)]
In all other cases do nothing.
\item[(5)]
If  contains some edges with respect to the node  then 
delete the corresponding item and the list of edges to which this item points.
\end{itemize}
The number of deletions performed in Step 5 is bounded by the number of nodes
in . If , we have to delete at least one minimum
key from . Altogether, the total number of deletions is bounded by .
For the implementation of the priority queue  we can use
Fibonacci heaps \cite{FrTa} or strict Fibonacci heaps \cite{BrLaTa} such that
with respect to the computation of all 's between two
augmentations, the used time is .

For the computation of all 's, we maintain a priority queue 
 which contains all edges  such that  
and .
Again, we can use Fibonacci heaps or strict Fibonacci heaps for the realization 
of the priority queue.
Note that each extension step decreases all weights of the elements  
in  with the property that there is no current  such that 
 by  where  is the value chosen for the 
current extension step.
Analogously to the manipulation of , we maintain with respect to 
 the sum  of all dual changes done so far with respect to 
edges in  and modify the weights in the appropriate manner.
We update  with respect to  
at the moment when  is added to  in the following way:
\begin{itemize}
\item
For all  with  add the edge  with modified
reduced cost  to the priority queue .
\end{itemize}
Since at most  edges are inserted, the used time is .
 
For the computation of , we have to find the smallest element
 in  which has the property that there exists no current 
such that . We use the priority queue  and
perform the following procedure:

\begin{tabbing}
AA \= AA \= \kill
(1) findmin; \\ 
\> (* Let  be the output of findmin.*) \\
(2) {\bf if} there exists  current with  \\
\> {\bf then} \\
\> \> deletemin; \\
\> \> {\bf goto} (1) \\
\> {\bf else} \\ 
\> \>  \\
\> {\bf fi}.
\end{tabbing}

If  then we have to delete at least one minimal
element from the priority queue . The number of deletemin operations
performed during all computations of  between two augmentations is bounded by 
the number of edges in .
Altogether, the number of deletions performed during the computation of
 is . Each deletion can be performed in  time.
Hence, the total time used for deletions is .
Altogether, with respect to the computation of all 's between two
augmentations, the used time is .

For the computation of all 's, we maintain a priority queue 
 which contains for all maximal  with  and  the value .
We use a heap, a Fibonacci heap or a strict Fibonacci heap for the realization of 
the priority queue .
Each extension step decreases all keys of the elements in . The amount
is two times the current . Hence, we can use the value 
defined above and modify the keys in the appropriate manner.
We update  before the computation of . We have to insert 
for all  such that  is maximal and  was inserted 
into  after the last dual change and the last augmentation, respectively 
but  the value . 
We have to delete for all  which are inserted to 
after the last dual change and the last augmentation, respectively 
the corresponding value if in  such a value exists. 
Since the number of dual changes between two augmentations is bounded by  and
 can only leave  because of an augmentation, the number
of insertions and deletions is at most . 
If , we have to delete at least one minimal
element from . As observed above, the number of such deletions is
bounded by . If there exists some sets  in ,
some other edge sets in  become to be maximal. Hence, we have to perform
the deunion operation which corresponds to the union operation performed with
respect to the construction of  and to insert the resulting maximal sets  of
 with key . As observed above, the total number of such 
insertions is also bounded by . 
Each deletion and each insertion can be performed in  time.
Altogether, with respect to the computation of all 's between two
augmentations, the used time is .

We have proved the following theorem.
\begin{theo}
The primal-dual method can be implemented such that its time complexity is .
\end{theo}
To get an implementation of time complexity , we can use ordinary heaps
instead of Fibonacci heaps or strict Fibonacci heaps. Using Fibonacci heaps or strict
Fibonacci heaps, the value  can be computed such that the used time between
two augmentations is . To get an implementation of time complexity
 the only critical point is the computation of the values 
such that the time used between two augmentations is bounded by .
Gabow \cite{Ga2} has presented an implementation of Edmonds maximum weighted matching 
algorithm which uses complicated data structures at the first SODA. The stated time 
complexity is .

The description of some recent programs which implements Edmonds' maximum weighted 
matching algorithm can be found in \cite{CoRo, MeSc, Ko}.

\medskip
\noindent
{\bf Acknowledgments:} 
I thank Ross McConnell for pointing out a mistake in the description of the
algorithm in \cite{Bl3} and Ari Freund for indicating some points in 
\cite{Bl2} which have to be clarified. These hints
have caused myself to revise my whole work on graph matching algorithms.
I thank Mathias Hauptmann for a careful reading of a preliminary version of the paper.

\begin{thebibliography}{99}
\frenchspacing

\bibitem{AHU} Aho A. V., Hopcroft J. E, and Ullman J. D.,
{\em The Design and Analysis of Computer Algorithms\/},
Addison-Wesley (1974), 187--189.

\bibitem{Bal} Balinski M. L.,
Labelling to obtain a maximum matching, in {\em Combinatorial Mathematics
and its Applications\/} (R. C. Bose and T. A. Dowling, eds.),
University of North Carolina Press, Chapel Hill (1969), 585--602.



\bibitem{Bar} Bartnik G.W.,
Algorithmes de couplages dans les graphes,
Th\'ese Doctorat  cycle, Universit\'e Paris VI (1978).

\bibitem{Be} Berge C.,
Two theorems in graph theory, {\em Proc. Nat. Acad. Sci. U.S.A.\/},
{\bf 43} (1957), 842--844.

\bibitem{Bl1} Blum N., A new approach to maximum matching in general graphs,
{\em 17th ICALP\/} (1990), LNCS 443, 586--597.

\bibitem{Bl4} Blum N., A new approach to maximum matching in general graphs,
Research report, Universit\"at Bonn (1993).

\bibitem{Bl2} Blum N., Maximum matching in general graphs without explicit
consideration of blossoms, Research report, Universit\"at Bonn (1999).

\bibitem{Bl3} Blum N., A simplified realization of the Hopcroft-Karp approach
to maximum matching in general graphs, Research report, Universit\"at
Bonn (1999).

\bibitem{CoRo} Cook W., and Rohe A., Computing minimum-weight perfect matchings,
{\em INFORMS Journal on Computing\/} {\bf 11} (1999), 138--148.

\bibitem{BrLaTa} Brodal G. S., Lagogiannis G., and Tarjan R. E.,
Strict Fibonacci heaps, {\em 44th STOC\/} (2012), 1177--1184.



\bibitem{EK} Even S, and Kariv O.,
An  algorithm for maximum matching in general graphs,
{\em 16th FOCS\/} (1975), 100--112.

\bibitem{Ed1} Edmonds J.,
Paths, trees, and flowers, {\em Canad. J. Math\/}, {\bf 17} (1965), 449--467.

\bibitem{Ed2} Edmonds J.,
Maximum matching and a polyhedron with 0,1-vertices, {\em J. Res. Nat. Bur.
Standards\/} {\bf 69 B} (1965), 125--130.

\bibitem{FrTa} Fredman M. L., and Tarjan R. E.,
Fibonacci heaps and their uses in improved network optimization algorithms,
{\em J. ACM\/} {\bf 34} (1987),596--615.

\bibitem{Fre} Freund A., private communication 2013.

\bibitem{Ga1} Gabow H. N.,
Implementations of algorithms for maximum matching on nonbipartite graphs,
Doctoral thesis, Comp. Sci. Dept., Stanford Univ., Stanford, Calif.
(1973).

\bibitem{Gab} Gabow H. N.,
An efficient implementation of Edmonds algorithm for maximum matching
on Graph, {\em J. ACM\/}, {\bf 23} (1976), 221--234.

\bibitem{Ga2} Gabow H. N.,
Data structures for weighted matching and nearest common ancestors with
linking, {\em 1st SODA\/} (1990), 434--443.

\bibitem{GaTa} Gabow H.~N., and Tarjan R.~E., 
A linear-time algorithm for a special case of disjoint set union, 
{\em J.~Comput.~Syst.~Sci.\/}, {\bf 30} (1985), 209--221.


\bibitem{GaTa2} Gabow H. N., and Tarjan R. E.,
Faster scaling algorithms for general graph-matching problems, {\em J. ACM\/}
{\bf 38} (1991), 815--853.

\bibitem{Gal} Galil Z., Efficient algorithms for finding maximum matching in graphs,
Computing Surveys, 1986, 23--38.

\bibitem{GaMiGa} Galil Z., Micali S., and Gabow H. N., An  algorithm for
finding a maximal weighted matching in general graphs, {\em SIAM J. Comput.\/} {\bf 15}
(1986), 120--130.

\bibitem{GoMi} Gondran M., and Minoux M.,
{\em Graphs and Algorithms}, Wiley \& Sons, (1984).

\bibitem{HK} Hopcroft J. E., and Karp R. M.,
An  algorithm for maximum matching in bipartite graphs,
{\em SIAM J. Comput.\/}, {\bf 2} (1973), 225--231.

\bibitem{Ka} Kariv O.,
An  algorithm for maximum matching in general graphs,
Ph.D. thesis, Dept. of Applied Mathematics, Weizmann Institute of Science,
Rehovort, Israel (1976).

\bibitem{Ko} Kolmogorov V., Blossom V: A new implementation of a minimum cost
perfect matching algorithm, {\em Mathematical Programming Computation\/} {\bf 1} 
(2009), 43--67.

\bibitem{La} Lawler E.:
{\em Combinatorial Optimization, Networks and Matroids}, Holt, Rinehart and
Winston, (1976).

\bibitem{LoPl} Lov\'{a}sz L., and Plummer M. D.,
{\em Matching Theory\/}, North-Holland Mathematics Studies 121, North-Holland,
New York (1986).

\bibitem{Mc} McConnell R., private communication 2011.

\bibitem{MeSc} Mehlhorn K., and Sch\"{a}fer G., Implementation of  weighted
matchings in general graphs: The power of data structures, {\em ACM Journal of Experimental
Algorithms\/} {\bf 7} (2002).

\bibitem{MV} Micali S., and Vazirani V. V.,
An  algorithm for finding maximum matching in
general graphs, {\em 21st FOCS\/} (1980), 12--27.

\bibitem{PeLo} Peterson P. A., and Loui M. C., The general matching algorithm
of Micali and Vazirani, Algorithmica, {\bf 3} (1988), 511--533.

\bibitem{Ro} Rochow H., Analyse und algorithmische Optimierung des MDFS
Maximum-Matching Algorithmus, diploma thesis, Universit\"at Bonn (1991).

\bibitem{Ta} Tarjan J. E.,
{\em Data Structures and Network Algorithms}, SIAM (1983).

\bibitem{Va1} Vazirani V. V.,
A theory of alternating paths and blossoms for proving correctness of
the  general graph maximum matching algorithm, 
{\em COMBINATORICA\/} {\bf 14} (1994), 71--109.

\bibitem{Va2} Vazirani V. V., A proof of the MV matching algorithm (2014).

\bibitem{WZ} Witzgall C., and Zahn C. T. Jr.,
Modification of Edmonds maximum matching algorithm,
{\em J.~Res.~Nat.~Bur.~Standards\/}, {\bf 69 B} (1965), 91--98.
\end{thebibliography}

\end{document}
