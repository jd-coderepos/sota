\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{booktabs}
\usepackage{verbatim}
\usepackage{wrapfig,lipsum,booktabs}
\usepackage[dvipsnames]{xcolor}
\usepackage{mathtools} 
\usepackage{multirow}
\usepackage{subfigure}
\usepackage{graphicx} 
\usepackage{color}
\newcommand{\hlrow}{\rowcolor{black!6}}
\usepackage{arydshln}
\newcommand{\hdline}{\hdashline\addlinespace[0.6ex]}
\newcommand{\ours}{CAT-Seg\xspace}

\usepackage{colortbl}
\usepackage{comment}
\usepackage{caption}
\usepackage{float}

\newcommand{\figref}[1]{Fig. \ref{#1}}
\newcommand{\tabref}[1]{Table \ref{#1}}
\newcommand{\equref}[1]{(\ref{#1})}
\newcommand{\secref}[1]{Sec. \ref{#1}}
\newcommand{\algref}[1]{Alg. \ref{#1}}

\makeatletter
\def\hlinewd#1{\noalign{\ifnum0=`}\fi\hrule \@height #1 \futurelet
\reserved@a\@xhline}
\newcommand{\floor}[1]{\lfloor #1 \rfloor}
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}

\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\iccvfinalcopy 

\def\iccvPaperID{****} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}



\begin{document}

 \title{CAT-Seg \makebox[14pt][r]{\raisebox{-0.55ex}{\includegraphics[scale=0.08]{iccv2023AuthorKit/figure/cats.png}}}: Cost Aggregation for Open-Vocabulary Semantic Segmentation}


\author{
Seokju Cho\textsuperscript{1} \quad Heeseong Shin\textsuperscript{1} \quad Sunghwan Hong\textsuperscript{1} \quad Seungjun An\textsuperscript{1} \quad Seungjun Lee\textsuperscript{1}, \\
Anurag Arnab\textsuperscript{2} \quad Paul Hongsuck Seo\textsuperscript{2} \quad Seungryong Kim\textsuperscript{1} \\
\textsuperscript{1}Korea University \quad \textsuperscript{2}Google Research\\
{\tt\small \{seokju\_cho, hsshin98, sung\_hwan, dkstmdwns, 9penguin9, seungryong\_kim\}@korea.ac.kr}\\
{\tt\small \{aarnab, phseo\}@google.com}
}


\twocolumn[{\maketitle
\centering
\vspace{-20pt}
\begin{figure}[H]
    \centering
    \vspace{-15pt}
    \hsize=1.0\textwidth
    \includegraphics[width=0.93\textwidth]{iccv2023AuthorKit/figure/pdf/fig1.pdf}
    \vspace{-10pt}
    \caption{\textbf{Teaser.} For open-vocabulary semantic segmentation, we propose to aggregate a matching cost derived from dense image and text embeddings of CLIP~\cite{radford2021learning}, resulting in state-of-the-art performance across all benchmarks.}
    \label{fig:intuition}
    \vspace{-3pt}
\end{figure}}] \maketitle


\begin{abstract}
Existing works on open-vocabulary semantic segmentation have utilized large-scale vision-language models, such as CLIP, to leverage their exceptional open-vocabulary recognition capabilities. However,  the problem of transferring these capabilities learned from image-level supervision to the pixel-level task of segmentation and addressing arbitrary unseen categories at inference makes this task challenging.  
To address these issues, we aim to attentively relate objects within an image to given categories by leveraging relational information among class categories and visual semantics through aggregation, while also adapting the CLIP representations to the pixel-level task. 
However, we observe that direct optimization of the CLIP embeddings can harm its open-vocabulary capabilities. In this regard, we propose an alternative approach to optimize the image-text similarity map, i.e. the cost map, using a novel cost aggregation-based method. Our framework, namely \ours, achieves state-of-the-art performance across all benchmarks. We provide extensive ablation studies to validate our choices.  Project page:~\url{https://ku-cvlab.github.io/CAT-Seg/}.
\let\thefootnote\relax\footnotetext{Equal contribution}

\end{abstract}
 

\begin{figure*}[t]
  \centering
  \renewcommand{\thesubfigure}{}

     \subfigure[(a) mIoU scores of \textbf{seen} classes]
{\includegraphics[width=0.28\linewidth]{iccv2023AuthorKit/figure/pdf/plot1.pdf}}\hfill
     \subfigure[(b) mIoU scores of \textbf{unseen} classes]
{\includegraphics[width=0.28\linewidth]{iccv2023AuthorKit/figure/pdf/plot2.pdf}}\hfill
     \subfigure[(c) Qualitative comparisons]
{\includegraphics[width=0.440\linewidth]{iccv2023AuthorKit/figure/pdf/fig2.pdf}}\hfill\\


\vspace{-10pt}
\caption{\textbf{Comparison between feature and cost aggregation.} To validate our framework, we consider two approaches: direct optimization of CLIP embeddings through feature aggregation and indirect optimization through \textbf{cost aggregation}. 
(a): Both approaches achieves performance gains for \textbf{seen} classes from fine-tuning the CLIP image encoder. (b): Feature aggregation fails to generalize to \textbf{unseen} classes, while cost aggregation achieves a large performance gains, highlighting the effectiveness of this approach for open-vocabulary segmentation. (c): Qualitative results where \textbf{(IV)} successfully segments the previously unseen class, e.g., `\textit{birdcage}', whereas \textbf{(III)} fails.
}
\vspace{-5pt}
  \label{fig:conceptual_qual}
\end{figure*} \section{Introduction}

Open-vocabulary semantic segmentation aims to label each pixel in an image with class  categories given as textual descriptions, which may include classes that were not seen during training. To address this task, recent works~\cite{li2022language,ghiasi2022scaling,ding2022decoupling,liang2022open,xu2022simple} typically attempted to leverage large-scale pre-trained vision-language models, e.g., CLIP~\cite{radford2021learning} and ALIGN~\cite{jia2021scaling}, for their remarkable open-vocabulary recognition capabilities achieved from extensive training process involving large image-caption pairs scaling from millions to billions~\cite{schuhmann2022laion}. However, transferring the knowledge of these models trained with image-level supervision to the pixel-level prediction task is highly challenging~\cite{xu2022simple} due to the difference in granularity between image and pixel.


To address this challenge, recent methods~\cite{liang2022open,ding2022decoupling,xu2022simple} generate class-agnostic region proposals and use these regions as inputs to the CLIP encoder. Despite their appreciable performance, these proposals not only exhibit bias towards the training dataset~\cite{liang2022open} but are also processed independently by CLIP disregarding the global context within the image. Alternatively, MaskCLIP~\cite{zhou2022extract} modifies the last pooling layer within the CLIP encoder~\cite{radford2021learning} to obtain dense image embeddings. However, this approach can introduce substantial noise without a means for refinement, which may lead to erroneous predictions, as shown in Fig.~\ref{fig:intuition}. While it is a common practice to fine-tune the encoder to obtain task-specific features~\cite{long2015fully, he2017mask, chen2017deeplab} for mitigating this issue, it has been shown that using the conventional fine-tuning approach harms the capabilities of CLIP~\cite{zhou2022extract}. 

Moreover, existing methods~\cite{li2022language, ding2022decoupling, ghiasi2022scaling, liang2022open} often do not account for the additional challenges presented by open-vocabulary setups. In contrast to conventional segmentation settings~\cite{long2015fully, he2017mask, he2019adaptive, yu2020context,zhou2022rethinking}, where relational information among predefined classes can be used to better understand the scene during inference~\cite{zhou2022rethinking, he2019adaptive, jin2021mining, yuan2020object}, open-vocabulary setups assume arbitrary classes at inference time, preventing the models from learning such relational information without an explicit means for consideration.


To this end, given that the encoders of CLIP have been trained on a large-scale data that enables remarkable open-vocabulary capabilities~\cite{li2022language, ghiasi2022scaling, zhou2022extract,xu2022simple,liang2022open,ding2022decoupling}, we aim to effectively transfer their knowledge for the pixel-level prediction task, and attentively relate objects within the image to given class categories provided as textual descriptions by joint aggregation of both modalities. Furthermore, we aim to fine-tune the CLIP image encoder to enable the model to adapt to the pixel-level prediction task at hand.
However, due to the aforementioned issue that direct optimization degrades the open-vocabulary capabilities~\cite{zhou2022extract}, it is important to explore an alternative to optimize the image encoder of CLIP without hurting its pre-trained knowledge. 

Upon revisiting the training process of CLIP, we find that the cost volume, often defined as cosine-similarity map,
constructed between the image and text embeddings~\cite{radford2021learning} is used for loss computation. From this, based on an insight that preserving the use of cost volume could serve as an alternative approach, we propose to construct an image-text matching cost to explicitly represent their relations, and introduce a cost aggregation-based framework to process the cost volume as shown in Fig.~\ref{fig:intuition}. We demonstrate that this approach can effectively transfer the knowledge of CLIP without diminishing its open-vocabulary capabilities as shown in Fig.~\ref{fig:conceptual_qual}. 

Our network, namely \ours, consists of three components: first, we construct a cost volume using image and text embeddings; second, we employ a Transformer ~\cite{vaswani2017attention} based module that decomposes the aggregation process into spatial and class aggregation, aided by an additional technique called embedding guidance, to effectively aggregate the cost volume; and finally, we use a decoder to process the aggregated cost volume while capturing fine details. In addition, we introduce an efficient approach for training our network, where we only fine-tune the attention layers within the Transformer architecture. Our approach is not only efficient, but also more effective compared to the conventional method of fine-tuning all parameters.

In experiments, we evaluate \ours on various datasets, including ADE20K~\cite{zhou2019semantic}, PASCAL VOC~\cite{everingham2009pascal} and PASCAL-Context~\cite{mottaghi2014role}. \ours achieves state-of-the-art results, with a 22\% improvement in mIoU over the previous best result on ADE20K with 150 categories and a 73\% improvement on PASCAL-Context with 459 categories. We also provide extensive ablation studies and analyses.

 
 \begin{figure}[t]
  \centering
  \renewcommand{\thesubfigure}{}
     \subfigure[(a) Raw cost of \textbf{``cat"}]
    {\includegraphics[width=0.49\linewidth]{iccv2023AuthorKit/figure/pdf/refine/refine_a.pdf}}\hfill
     \subfigure[(b) Aggregated cost of \textbf{``cat"}]
    {\includegraphics[width=0.49\linewidth]{iccv2023AuthorKit/figure/pdf/refine/refine_b.pdf}}\hfill\\
    \vspace{-5pt}
     \subfigure[(c) Raw cost of \textbf{``guitar"}]
    {\includegraphics[width=0.49\linewidth]{iccv2023AuthorKit/figure/pdf/refine/refine_c.pdf}}\hfill
     \subfigure[(d) Aggregated cost of \textbf{``guitar"}]
    {\includegraphics[width=0.49\linewidth]{iccv2023AuthorKit/figure/pdf/refine/refine_d.pdf}}\hfill\\

    \vspace{-10pt}
    \caption{\textbf{Visualization of cost volumes.} The proposed cost aggregation framework effectively refines the noisy costs in (a) and (c), as exemplified in (b) and (d). Note that we overlay the costs with the input image for visualization.}
    \vspace{-10pt}
  \label{fig:refinement_qual}
\end{figure} 

\begin{figure*}
    \centering
    \includegraphics[width=1.0\linewidth]{iccv2023AuthorKit/figure/pdf/main_architecture.pdf}\hfill\\
    \vspace{-10pt}
    \caption{\textbf{Overall architecture of \ours.} Our network consists of a cost computation, aggregation module consisting of spatial aggregation and inter-class aggregation and an upsampling decoder.}
    \label{fig:overall}\vspace{-10pt}
\end{figure*} 
\section{Related Work}

\paragraph{Open-vocabulary semantic segmentation.} Classical approaches to the task~\cite{zhao2017open, bucher2019zero, xian2019semantic} attempt to learn visual embeddings that align with pre-defined text embeddings~\cite{miller1998wordnet, mikolov2013efficient}. However, the domain difference, as well as the limited vocabulary of the words have been the major bottlenecks. To address this, LSeg~\cite{li2022language} leveraged CLIP for learning pixel-level visual embeddings aligned with the text embeddings of CLIP. Alternatively, OpenSeg~\cite{ghiasi2022scaling} proposed to identify local regions within the image and correlate with the text embeddings with class-agnostic region proposals.

More recently, ZegFormer~\cite{ding2022decoupling} and ZSseg~\cite{xu2022simple} proposed two-stage frameworks. Typically, they first learn to predict class-agnostic region proposals similar to~\cite{ghiasi2022scaling}, and feed them to CLIP for final predictions. To better recognize these regions, OVSeg~\cite{liang2022open} collects region-text pairs to fine-tune the CLIP encoder. A notable work, MaskCLIP~\cite{zhou2022extract} attempts to obtain pixel-level embeddings from CLIP that can be directly utilized for segmentation. However, based on their findings~\cite{zhou2022extract} that conventional fine-tuning impedes the open-vocabulary capability of CLIP, they instead propose to freeze the CLIP encoder and apply non-learnable methods to mitigate its noise. Unlike the aforementioned works, we introduce a cost aggregation method to benefit from fine-tuning the CLIP encoder and obtain accurate pixel-level predictions.



\vspace{-10pt}
\paragraph{Cost aggregation.} Cost aggregation is a popular technique adopted for the process of establishing correspondence between visually or semantically similar images~\cite{kendall2017end,guo2019group,yang2019hierarchical,cho2021cats,hong2022cost} by reducing the impact of errors and inconsistencies in the matching process. A matching cost, an input to cost aggregation, is typically constructed between dense features extracted from a pair of images~\cite{rocco2017convolutional}, and often cosine-similarity~\cite{liu2022graftnet,rocco2017convolutional} is used. In matching literature, numerous works~\cite{kendall2017end, chang2018pyramid, guo2019group, yang2019hierarchical, song2021adastereo,hong2022neural,huang2022flowformer,cho2022cats++} have proposed cost aggregation modules and demonstrated its importance, owing to its favorable generalization ability~\cite{song2021adastereo,liu2022graftnet}. In this work, we leverage the cost volume constructed between image and text embeddings from CLIP encoders to promote accurate segmentation through cost aggregation.

 
\section{Methodology}
\subsection{Motivation and Overview}\label{motivation}
Given an image  and a set of candidate class categories , where  denotes textual description of -th category and  is the number of classes, open-vocabulary semantic segmentation aims to assign a class label to each pixel in . Different from classical semantic segmentation tasks~\cite{long2015fully, he2017mask, zhou2022rethinking, he2019adaptive, jin2021mining, yu2020context, yuan2020object}, open-vocabulary segmentation is additionally challenged by varying  at inference, which includes classes that were not observed during training. 

In this paper, we aim to transfer the open-vocabulary capabilities of CLIP~\cite{radford2021learning} to enable our model to reason about relations between image semantics and arbitrary class labels, addressing the challenges of open-vocabulary semantic segmentation. To achieve this goal, we propose to jointly aggregate the image and text embeddings of CLIP and further enhance the aggregation by fine-tuning the image encoder. While it is a common approach to transfer the knowledge within large-scale models to tackle downstream tasks through fine-tuning the parameters~\cite{he2017mask,mensink2021factors}, existing approaches in this task have failed to effectively transfer their knowledge~\cite{zhou2022extract} due to several reasons. This includes the granularity gap between image-level supervision and pixel-level prediction, and catastrophic forgetting caused by direct optimization through feature embeddings~\cite{zhou2022extract}.

To address the issues, we leverage insights from the training process of CLIP~\cite{radford2021learning} which uses a cosine-similarity map, \ie cost map.
Our pilot experiment showed that using cost volume derived from dense image embeddings and text embeddings of CLIP as an alternative is effective, as demonstrated in Fig.~\ref{fig:conceptual_qual}, and thus  we introduce cost volume to our framework.
Subsequently, we propose an aggregation module that incorporates both spatial and class aggregation to effectively capture relational information among inputs while promoting accurate matching between text and objects in the image, thereby reducing errors and inconsistencies in the matching costs, as shown in Fig.~\ref{fig:refinement_qual}. Finally, we introduce an efficient fine-tuning approach that not only provides improved performance, but also enhances accessibility. All these combined, we present \ours, \textbf{C}ost \textbf{A}ggrega\textbf{T}ion approach for open-vocabulary semantic \textbf{Seg}mentation.

\subsection{Cost Computation and Embedding}
To obtain dense CLIP features, we follow \cite{zhou2022extract} and modify the last layer of the image encoder.
Given the modified CLIP image encoder  and the frozen text encoder , we extract the dense image features  and the text features , respectively. We use the image and text features  and , where  denotes 2D spatial positions of feature embedding and  denotes an index for a class, to compute a cost volume  by cosine similarity~\cite{rocco2017convolutional}. 
Formally, this is defined as:

To enhance the processing of cost in high dimensional feature space, we feed the cost volume to a single convolution layer that processes each cost slice  independently to obtain initial cost volume embedding , where  is the cost embedding dimension, as shown in Fig.~\ref{fig:overall}. 

\subsection{Cost Aggregation}
Based on the insights from above, we use the constructed cost volume embedding and feed it to aggregation modules. 
As it has been shown in previous studies that learning based on a matching cost within a regularized representation, such as the cosine-similarity map, can improve generalization ability and robustness to unseen domains \cite{cai2020matching,song2021adastereo,liu2022graftnet}, we find this approach valid. 

Within our cost aggregation module, as shown in Fig.~\ref{fig:overall}, two separate aggregation strategies, \ie spatial and class aggregation, are designed to model interrelations across pixels and classes. This considers the distinct characteristic of each modality, such as spatial smoothness in an image or the permutation invariance among classes. Specifically, we first perform spatial aggregation and then class aggregation takes place, and interleave both aggregations  times. In addition, we facilitate the cost aggregation process with embedding guidance that provide contextual information from two modalities. In the following, we explain each in detail.
\vspace{-10pt}
\paragraph{Spatial aggregation.} Here, we reason about the spatial relations based on pixel-wise similarities computed between image and text embeddings. 
For this, we adopt Transformer~\cite{vaswani2017attention,liu2021swin} over CNNs for its adaptability to input tokens~\cite{dai2021coatnet}, while also having global~\cite{vaswani2017attention} or semi-global~\cite{liu2021swin,hong2022cost} receptive fields, which is more favorable to our goal to learn relations among all tokens.  In practice, we employ Swin Transformer~\cite{liu2021swin} for computational efficiency. We define this process as:

where , and  denotes a pair of two consecutive Swin transformer block for spatial aggregation, where the first block features self-attention within a local window, followed by the second block with self-attention within shifted window. Note that we treat  as channel dimensions for each token, and attention is computed within individual classes separately. Intuitively, we perform spatial aggregation for each class to locate the features that will guide to accurate segmentation outputs. 

\vspace{-10pt}
\paragraph{Class aggregation.}
Subsequent to spatial aggregation, class aggregation is designed to explicitly capture relationships between different class categories. However, this task presents two challenges that need to be addressed: the variable number of categories  and their unordered input arrangement. To address these challenges, we employ a Transformer~\cite{vaswani2017attention} model without position embedding for aggregation.
This approach enables the handling of sequences of arbitrary length and provides the model with permutation invariance to inputs. This process is defined as: 

where , and  denotes a transformer block for class aggregation. Although we can employ the same Swin Transformer~\cite{liu2021swin} as for the spatial aggregation, we instead employ a linear transformer~\cite{katharopoulos2020transformers} as we do not need to consider spatial structure of the input tokens in this aggregation. Also, it offers a linear computational complexity with respect to the number of the tokens, allowing efficient computation. 

\vspace{-10pt}
\paragraph{Embedding guidance.}
As a means to enhance cost aggregation process, we additionally leverage feature embeddings to provide spatial structure or contextual information of the inputs. Intuitively, we aim to guide the process with feature embeddings, based on the assumption that visually or semantically similar input tokens, e.g., color or category, have similar matching costs, inspired by cost volume filtering~\cite{hosni2012fast,sun2018pwc} in stereo matching literature~\cite{scharstein2002taxonomy}.
Specifically, the visual embeddings  and the text embedding  are extracted from the additional feature backbone and CLIP text encoder, respectively, followed by a linear projection and concatenation to the input sequence. When extracting , we avoid using the CLIP image encoder because of the potential hindrance to generalization ability when directly utilized, as discussed in Fig.~\ref{fig:conceptual_qual}. Accordingly, we redefine Eq.~\ref{eq:spatial-aggregation} and Eq.~\ref{eq:class-aggregation} as:  

where  denotes concatenation,  and  denote linear projection layer, , and , where  denotes the feature dimension. Notably, we only provide  feature embeddings to query and key as we find this is sufficient for embedding guidance. 

\subsection{Upsampling Decoder}
Given the aggregated cost volume, we aim to generate the final segmentation mask that captures fine-details via upsampling. The simplest approach would be using handcrafted upsamplers, \ie bilinear upsampling, but we propose to conduct further aggregation within the
decoder with light-weight convolution layers. Additionally, we provide feature embeddings from the feature backbone 
that act as an effective guide to filter out the noises in the cost volume and exploit the higher-resolution spatial structure for preserving fine-details.

Specifically, we leverage the multi-level feature embeddings readily extracted in embedding guidance for cost aggregation. Following the approach presented in~\cite{hong2022cost}, we employ bilinear upsampling on the cost volume and concatenate it with the corresponding level of feature map, followed by a convolutional layer. We iterate this process  times, generating a high-resolution output that we feed into the prediction head for final inference.

\subsection{Efficient Fine-Tuning of CLIP}
During training, we train our model in an end-to-end manner, including the CLIP image encoder. However, fine-tuning the encoder, which can scale up to hundreds of millions of parameters, can be computationally expensive and memory-intensive. Moreover, since our objective is to effectively transfer the open-vocabulary capability of CLIP, performance should not be compromised as well. To this end, we freeze the MLP layers in the encoder, also known as Feed-Forward Networks (FFNs), and only optimize the layers responsible for spatial interaction, such as the attention layers and positional embeddings~\cite{vaswani2017attention}, based on the insight that tuning these layers is sufficient for transferring image-level representations to pixel-level. With this approach, we achieve higher efficiency and improved performance over full fine-tuning.
 \begin{table*}[!t]
    \begin{center}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{l|cccc|cccccc}
    \toprule
        Methods & VLM & Feature backbone & Training dataset & Additional dataset & A-847 & PC-459 & A-150 & PC-59 & PAS-20 & 
        \\
        \midrule\midrule
        SPNet~\cite{xian2019semantic} & - & ResNet-101 & PASCAL VOC & - & - & - & - & 24.3 & 18.3 & - \\
        ZS3Net~\cite{bucher2019zero} & - & ResNet-101 & PASCAL VOC & - & - & - & - & 19.4 & 38.3 & - \\
        LSeg~\cite{li2022language} & ViT-B/32 & ResNet-101 & PASCAL VOC-15  & - & - & - & - & - & 47.4 & - \\
        LSeg+~\cite{ghiasi2022scaling} & ALIGN & ResNet-101 & COCO-Stuff & - & 2.5 & 5.2 & 13.0 & 36.0 & - & 59.0 \\
        ZegFormer~\cite{ding2022decoupling} & ViT-B/16 & ResNet-101 & COCO-Stuff-156 & - & 4.9 & 9.1 & 16.9 & 42.8 & 86.2 & 62.7 \\
        ZegFormer{}~\cite{ding2022decoupling} & ViT-B/16 & ResNet-101 & COCO-Stuff & - & 5.6 & 10.4 & 18.0 & 45.5 & 89.5 & 65.5 \\
        ZSseg~\cite{xu2022simple} & ViT-B/16 & ResNet-101 & COCO-Stuff & - & 7.0 & - & 20.5 & 47.7 & 88.4 & - \\
        OpenSeg~\cite{ghiasi2022scaling} & ALIGN & ResNet-101 & COCO Panoptic~\cite{kirillov2019panoptic} & Localized Narrative & 4.4 & 7.9 & 17.5 & 40.1 & - & 63.8 \\
        OVSeg~\cite{liang2022open} & ViT-B/16 & ResNet-101c & COCO-Stuff & COCO Caption & \underline{7.1} & \underline{11.0} & \underline{24.8} & \underline{53.3} & \underline{92.6} & - \\
        \hlrow 
        \ours (ours) & ViT-B/16 & ResNet-101 & COCO-Stuff & - & \textbf{8.4} \textcolor{ForestGreen}{(+1.3)} &\textbf{16.6} \textcolor{ForestGreen}{(+5.6)} &\textbf{27.2} \textcolor{ForestGreen}{(+2.4)} &\textbf{57.5} \textcolor{ForestGreen}{(+4.2)} & \textbf{93.7} \textcolor{ForestGreen}{(+1.1)} & \textbf{78.3} \textcolor{ForestGreen}{(+12.8)} \\
        \midrule
        LSeg~\cite{li2022language} & ViT-B/32 & ViT-L/16 & PASCAL VOC-15  & - & - & - & - & - & 52.3 & - \\
        OpenSeg~\cite{ghiasi2022scaling} & ALIGN & Eff-B7~\cite{tan2019efficientnet} & COCO Panoptic~\cite{kirillov2019panoptic} & Localized Narrative & 8.1 & 11.5 & 26.4 & 44.8 & - & 70.2\\
        OVSeg~\cite{liang2022open} & ViT-L/14 & Swin-B & COCO-Stuff & COCO Caption & \underline{9.0} & \underline{12.4} & \underline{29.6} & \underline{55.7} & \underline{94.5} & - \\
        \hlrow 
        & ViT-L/14 & Swin-B & COCO-Stuff & -& \textbf{10.8} \textcolor{ForestGreen}{(+1.8)} & \textbf{20.4} \textcolor{ForestGreen}{(+8.0)} & \textbf{31.5} \textcolor{ForestGreen}{(+1.9)} & \textbf{62.0} \textcolor{ForestGreen}{(+6.3)} & \textbf{96.6} \textcolor{ForestGreen}{(+2.1)} & \textbf{81.8} \textcolor{ForestGreen}{(+11.6)}\\
        \hlrow & ViT-H/14* & Swin-B & COCO-Stuff & - & \textbf{12.4} \textcolor{ForestGreen}{(+3.4)} & \textbf{20.1} \textcolor{ForestGreen}{(+7.7)} & \textbf{34.4} \textcolor{ForestGreen}{(+4.8)} & \textbf{61.2} \textcolor{ForestGreen}{(+5.5)} & \textbf{96.7} \textcolor{ForestGreen}{(+2.2)} & \textbf{80.2} \textcolor{ForestGreen}{(+10.0)}\\

        \hlrow \multirow{-3}{*}{\ours (ours)} & ViT-G/14* & Swin-B & COCO-Stuff & - & \textbf{13.3} \textcolor{ForestGreen}{(+4.3)} & \textbf{21.4} \textcolor{ForestGreen}{(+9.0)} & \textbf{36.2} \textcolor{ForestGreen}{(+6.6)} & \textbf{61.5} \textcolor{ForestGreen}{(+5.8)} & \textbf{97.1} \textcolor{ForestGreen}{(+2.6)} & \textbf{81.4} \textcolor{ForestGreen}{(+11.2)}\\
        \bottomrule
    \end{tabular}
    }
    \vspace{-5pt}
    \caption{\textbf{Quantitative evaluation.} The best-performing results are presented in bold, while the second-best results are underlined. Improvements over the second-best methods are highlighted in \textcolor{ForestGreen}{green}. mIoU is adopted for evaluation metric. :~Re-implemented to train with full COCO-Stuff~\cite{caesar2018coco} dataset. *: Model trained on LAION-2B~\cite{schuhmann2022laion} dataset introduced in~\cite{cherti2022reproducible}.
    }\label{tab:main_table}
    \end{center}\vspace{-5pt}
\end{table*}
 
\begin{table}[t]
    \centering
    \resizebox{\linewidth}{!}{
   \begin{tabular}{ll|cccccc}
        \toprule
        &Components & A-847 & PC-459 & A-150 & PC-59 & PAS-20 & 
         \\
        \midrule\midrule
        \textbf{(I)} & Feature Agg. & 3.8 & 10.9 & 19.1 & 53.5 & 96.2 & 74.2\\
        \midrule
        \textbf{(II)} & Cost Agg.  & 9.2& 14.6& 27.4& 58.6& \underline{96.8}&78.2\\
        \textbf{(III)} &\textcolor{gray}{\textbf{(II)}} + \textcolor{gray}{Transformer} & 9.9& 15.1& 28.5& 57.0& 96.2& 76.4\\
        \textbf{(IV)} &\textbf{(II)} + Spatial agg.  & 9.4& 15.1& 28.1& 58.3& \textbf{97.0}&78.5\\
        \textbf{(V)} &\textbf{(IV)} + Inter-class agg.  & \underline{10.6}& \underline{16.1}& \underline{30.9}& \underline{61.7}& \underline{96.8}&\underline{81.4}\\
        \textbf{(VI)} &\textbf{(V)} + Embedding guidance  & \textbf{10.8} & \textbf{20.4}& \textbf{31.5}& \textbf{62.0}& 96.6&\textbf{81.8}\\
        \bottomrule
\end{tabular}}\vspace{-5pt}
        \caption{\textbf{Ablation study for \ours.}}
    \label{tab:clipcats-ablation}
    \vspace{-5pt}
\end{table}
 \begin{table}[t]
    \centering
    \resizebox{\linewidth}{!}{
   \begin{tabular}{ll|cccccc|cc}
        \toprule
        &\multirow{2}{*}{Methods} & \multirow{2}{*}{A-847} & \multirow{2}{*}{PC-459} & \multirow{2}{*}{A-150} & \multirow{2}{*}{PC-59}& \multirow{2}{*}{PAS-20} & \multirow{2}{*}{} &\#param.  & Memory
         \\
         &&&&&&&&(M)&(GiB)
         \\
        \midrule\midrule
        \textbf{(I)} &Freeze & 4.8& 8.7& 20.8& 52.5& 84.5& 76.7& 0 & 1.9\\
        \textbf{(II)} &VPT~\cite{jia2022visual}  & 8.3& 13.7& 29.4& 59.5 & 90.8 & 81.0 &2.3 & 2.0\\
        \textbf{(III)} &Full F.T.  & \underline{9.4}& \underline{18.5}& \underline{30.2}& \underline{61.0}& \textbf{96.8}& \underline{81.5} & 290.0 & 5.4\\
        \hlrow \textbf{(IV)} &Ours  & \textbf{10.8}& \textbf{20.4}& \textbf{31.5}& \textbf{62.0}& \underline{96.6}& \textbf{81.8} & 96.7 & 3.1\\
        \bottomrule
\end{tabular}}\vspace{-5pt}
        \caption{\textbf{Ablation study of fine-tuning methods for CLIP image encoder.} We additionally note the number of learnable parameters of CLIP and memory consumption during training. Our method not only outperforms full fine-tuning, but also requires a smaller computational footprint. \textit{F.T.: Fine-Tune}}
    \label{tab:finetuning-ablation}
    \vspace{-5pt}
\end{table}
 
\begin{table}[t]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{l|ccccccc}
    \toprule
        Methods & A-847 & PC-459 & A-150 & PC-59 & PAS-20 & \\
        \midrule\midrule
        CLIP image enc.~\cite{radford2021learning} & \underline{10.6} & \underline{15.6} & \underline{30.1} & \underline{60.2} &\underline{95.9} & \underline{80.5}\\
        Swin-B~\cite{liu2021swin} & \textbf{10.8} & \textbf{20.4} & \textbf{31.5} & \textbf{62.0} & \textbf{96.6} & \textbf{81.8}\\
        \midrule
        CLIP image enc.~\cite{radford2021learning} & 7.7 & 14.4 & 25.7 & 55.8 &85.1 & 78.4\\
    \bottomrule
    \end{tabular}
    }
    
    \vspace{-5pt}
    \caption{\textbf{Ablation on feature backbone.} CLIP with ViT-L is used for all methods. : Stop-gradient operation is applied to cost computation.}
    \label{tab:affinity-backbone}
    \vspace{-5pt}
\end{table}
 
\section{Experiments}
\subsection{Experimental Setup}
\paragraph{Datasets and evaluation metric.} 
We train our model on the COCO-Stuff dataset, which has 118k densely annotated training images with 171 categories, following \cite{liang2022open}. We evaluate our model on ADE20K~\cite{zhou2019semantic}, PASCAL VOC~\cite{everingham2009pascal}, and PASCAL-Context~\cite{mottaghi2014role} datasets. ADE20K has 20k training and 2k validation images, with two sets of categories: A-150 with 150 frequent classes and A-847 with 847 classes. PASCAL-Context contains 5k training and validation images, with 459 classes in the full version (PC-459) and the most frequent 59 classes in the PC-59 version. PASCAL VOC has 20 object classes and a background class, with 1.5k training and validation images. We report PAS-20 using 20 object classes. We also report the score for PAS-, which defines the ``background" as classes present in PC-59 but not in PAS-20, as in \cite{ghiasi2022scaling}. We adopt mean Intersection over Union (mIoU) as evaluation metric for all experiments.

\vspace{-10pt}
\paragraph{Implementation details.}
We train the CLIP image encoder and the cost aggregation module with per-pixel binary cross-entropy loss and freeze the CLIP text encoder for all settings.
Feature embeddings  are obtained from ResNet-101~\cite{he2016deep} pre-trained on the ImageNet-1k~\cite{deng2009imagenet} for model that uses CLIP with ViT-B~\cite{dosovitskiy2020image} and Swin-B~\cite{liu2021swin} pre-trained on the ImageNet-21k for models that use CLIP with either ViT-L, H or G~\cite{zhai2022scaling}.
We set , ,  for all of our models.
We implement our work using PyTorch~\cite{paszke2019pytorch} and Detectron2~\cite{wu2019detectron2}. AdamW~\cite{loshchilov2017decoupled} 
optimizer is used with a learning rate of  for our model  and  for the CLIP image encoder, with weight decay set to . The batch size is set to 4. We use 4 NVIDIA RTX 3090 GPUs for the model variant that utilizes ViT-B, L, H~\cite{dosovitskiy2020image} as the CLIP encoder, and use a single NVIDIA A100 for ViT-G~\cite{zhai2022scaling}. All of the models are trained for 80k iterations, which takes about 8 hours for model using ViT-L. Further details can be found in supplementary material. Our code and pre-trained weights will be made publicly available.




\subsection{Main Results}

In Table~\ref{tab:main_table}, we provide quantitative comparisons to competitors. For a fair comparison, we note the type of the vision-language model (VLM), the feature backbone, and the training dataset. We also specify the additional dataset if it is used. We first compare our method with those that employ ResNet-101 as the feature backbone and ViT-B as vision-language models. 

Overall, our method significantly outperforms all the other competitors, including  those~\cite{ghiasi2022scaling,liang2022open} that leverage additional dataset~\cite{chen2015microsoft,pont2020connecting} for further performance improvements. For methods that employ Swin-B as the feature backbone and ViT-L as vision-language models, our method surpasses the closest competitor by 20\% in A-847 and 65\% in PC-459.
Furthermore, if we choose to scale our VLM model to a larger variant, performance is further boosted. For CLIP variants introduced in~\cite{cherti2022reproducible}, \ie ViT-H and ViT-G, which are trained on LAION-2B~\cite{schuhmann2022laion}, our method enjoys further performance gains. These results confirm the validity of our approach to leverage cost aggregation, which offers an effective means of transferring knowledge from large-scale vision-language models.

We also present qualitative results of PASCAL-Context with 459 categories in Fig.~\ref{fig:qualitative}, demonstrating the efficacy of our proposed approach in comparison to the current state-of-the-art methods~\cite{ding2022decoupling, xu2022simple,liang2022open}. 

\subsection{Analysis and Ablation Study}
In this section, we provide ablation study and analysis to validate our choices. For all the experiments, we employ ViT-L variant for the CLIP encoder~\cite{radford2021learning} and Swin-B~\cite{liu2021swin} for the feature backbone if not mentioned.

\vspace{-10pt}
\paragraph{Component analysis.}
Table~\ref{tab:clipcats-ablation} shows the effectiveness of the main components within our architecture through quantitative results. 
First, we introduce the baseline models in \textbf{(I)} and \textbf{(II)}, which simply feed the feature embeddings or the cost volume to the proposed upsampling decoder. We then progressively add each proposed components from \textbf{(IV)} to \textbf{(VI}) to validate our approach to factorize the aggregation process into spatial and class aggregations, and the proposed embedding guidance. Note that for the design of \textbf{(III)}, we employ a Linear Transformer~\cite{katharopoulos2020transformers} that aggregates the entire input without separately processing spatial and class dimensions. For this analysis, we control the number of parameters to be similar across all the variants from \textbf{(III)} to \textbf{(VI}). 

As shown, we stress the gap between \textbf{(I)} and  \textbf{(II)}, which supports the findings presented in Fig.~\ref{fig:conceptual_qual}. 
We also highlight that as we gradually incorporate the proposed spatial and class aggregation techniques, our approach \textbf{(V)} outperforms \textbf{(III)}, demonstrating the effectiveness of our design.
Finally, \textbf{(VI)} shows that our embedding guidance further improves performance across most of the benchmarks, with the exception of PAS-20, where the error levels have become saturated as its class categories mostly overlap with the training dataset.


\begin{figure}[t]
    \centering
  \renewcommand{\thesubfigure}{}
     \subfigure[(a)]
    {\includegraphics[width=0.497\linewidth]{iccv2023AuthorKit/figure/pdf/training_time.pdf}}\hfill
     \subfigure[(b)]
    {\includegraphics[width=0.497\linewidth]{iccv2023AuthorKit/figure/pdf/inference_time.pdf}}\hfill\\
\vspace{-10pt}
\caption{\textbf{Run-time comparison.} (a) Comparison of training time among different fine-tuning choices. (b) Comparison of inference time to~\cite{ding2022decoupling}. \textit{F.T.: Fine-Tuning.}}
\vspace{-10pt}
  \label{fig:inference-time}
\end{figure} 
\begin{table}[t]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{l|c|ccccccc}
    \toprule
        Methods & CLIP variants & A-847 & PC-459 & A-150 & PC-59 & PAS-20 & 
        \\
        \midrule\midrule
        ZegFormer~\cite{ding2022decoupling} & ViT-B/16 & 5.6 & \underline{10.4} & 18.0 & 45.5 & \underline{89.5} & 65.5 \\
        ZSseg~\cite{xu2022simple} & ViT-B/16 & \underline{7.0} & 9.0 & \underline{20.5} & \underline{47.7} & 88.4 & \underline{67.9}\\
        \hlrow \ours (ours) & ViT-B/16 & \textbf{8.4} & \textbf{16.6} & \textbf{27.2} & \textbf{57.5} & \textbf{93.7} & \textbf{78.3} \\
        \midrule
        ZegFormer~\cite{ding2022decoupling} & ViT-L/14 & 6.5 & \underline{12.2} & 21.1 & 50.1 & 93.2 & 67.9\\
        ZSseg~\cite{xu2022simple} & ViT-L/14 & \underline{6.8} & 10.6 & \underline{22.3} & \underline{53.8} & \underline{94.7} & \underline{72.2}\\
        \hlrow \ours (ours) & ViT-L/14 & \textbf{10.8} & \textbf{20.4} & \textbf{31.5} & \textbf{62.0} & \textbf{96.6} & \textbf{81.8} \\
        \midrule
        ZegFormer~\cite{ding2022decoupling} & ViT-H/14* & \underline{5.8} & 9.0 & 17.7 & 43.5 & 86.3 & 65.4 \\ 
        ZSseg~\cite{xu2022simple} & ViT-H/14* & 5.7 & \underline{9.1} & \underline{18.4} & \underline{48.8} & \underline{90.5} & \underline{70.4}\\
        \hlrow \ours (ours) & ViT-H/14* & \textbf{12.4} & \textbf{20.1} & \textbf{34.4} & \textbf{61.2} & \textbf{96.7} & \textbf{80.2} \\
        \midrule
        ZegFormer~\cite{ding2022decoupling} & ViT-G/14* & 6.4 & 10.1 & 20.3 & 45.9 & 89.8 & 66.8 \\
        ZSseg~\cite{xu2022simple} & ViT-G/14* & \underline{6.7} & \underline{10.5} & \underline{20.9} & \underline{51.7} & \underline{93.7} & \underline{73.0}\\
        \hlrow \ours (ours) & ViT-G/14* & \textbf{13.3} & \textbf{21.4} & \textbf{36.2} & \textbf{61.5} & \textbf{97.1} & \textbf{81.4} \\
        \bottomrule
    \end{tabular}
    } 

    \vspace{-5pt}
    \caption{\textbf{Scalability comparison.} *: Model trained on LAION-2B~\cite{schuhmann2022laion} dataset introduced in~\cite{cherti2022reproducible}.}
    \label{tab:scaling-ablation}
    \vspace{-5pt}
\end{table}
 \begin{table}[t]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{l|c|ccccccc}
    \toprule
        Methods & Training dataset & A-847 & PC-459 & A-150 & PC-59 & PAS-20 & 
        \\
        \midrule\midrule
        ZegFormer~\cite{ding2022decoupling} & COCO-Stuff & 5.6 & \underline{10.4} & 18.0 & 45.5 & \underline{89.5} & 65.5\\
        ZSseg~\cite{xu2022simple} & COCO-Stuff & \underline{7.0} & 9.0 & \underline{20.5} & \underline{47.7} & 88.4 & \underline{67.9}\\
        \hlrow \ours (ours) & COCO-Stuff & \textbf{8.4} & \textbf{16.6} & \textbf{27.2} & \textbf{57.5} & \textbf{93.7} & \textbf{78.3}\\
        \midrule
        ZegFormer~\cite{ding2022decoupling} & A-150 & 6.8 & \underline{7.1} & \color{gray}{33.1} & 34.7 & 77.2 & 53.6 \\
        ZSseg~\cite{xu2022simple} & A-150 & \underline{7.6} & \underline{7.1} & \color{gray}{40.3} & \underline{39.7} & \underline{80.9} & \underline{61.1}\\
        \hlrow \ours (ours) & A-150 & \textbf{10.6} & \textbf{14.5} & \color{gray}{46.8} & \textbf{46.7} & \textbf{85.5} & \textbf{70.3} \\
        \midrule
        ZegFormer~\cite{ding2022decoupling} & PC-59 & \underline{3.8} & \underline{8.2} & \underline{13.1} & \color{gray}{48.7} & 86.5 & 66.8 \\
        ZSseg~\cite{xu2022simple} & PC-59 & 3.0 & 7.6 & 11.9 & \color{gray}{54.7} & \underline{87.7} & \underline{71.7}\\
        \hlrow \ours (ours) & PC-59 & \textbf{5.6} & \textbf{12.9} & \textbf{23.0} & \color{gray}{62.4} & \textbf{87.3} & \textbf{79.0} \\
        \bottomrule
    \end{tabular}
    }
    
    \vspace{-5pt}
    \caption{\textbf{Training on various datasets.} CLIP with ViT-B is used for all methods. Our model demonstrates remarkable generalization capabilities even on relatively smaller datasets. The scores evaluated on the same dataset used for training are colored in \textcolor{gray}{gray} for clarity.}
    \label{tab:cross-dataset-ablation}
    \vspace{-5pt}
\end{table}
 
\vspace{-10pt}
\label{finetune}
\paragraph{Comparison among different fine-tuning approaches.}
In this ablation study, we examine both effectiveness and efficiency of the proposed approach to fine-tune the CLIP image encoder. In Table~\ref{tab:finetuning-ablation}, we report the results of different approaches, which include the variant without fine-tuning, adopting VPT~\cite{jia2022visual}, fine-tuning the entire network and our approach. We additionally show the comparison of time taken for training in Fig.~\ref{fig:inference-time}~(a). In short, we find that our proposed approach not only yields the best performance, but also requires less memory and training time than full fine-tuning, which highlights the effectiveness and efficiency of our approach.


\vspace{-10pt}
\paragraph{Feature backbone.}
Table~\ref{tab:affinity-backbone} presents a comparison of variants with different feature backbones for providing feature embeddings . The first row reports the results using the CLIP image encoder~\cite{radford2021learning} whereas the second row reports the results using Swin-B~\cite{liu2021swin} as the feature backbone. The results indicate that the use of the CLIP image encoder generally reduces performance across all benchmarks, although it still outperforms other approaches~\cite{li2022language,xu2022simple, ding2022decoupling, liang2022open, ghiasi2022scaling}. In contrast, using a separate network, Swin-B~\cite{liu2021swin}, leads to the best performance. We attribute this trend to the same observation shown in Fig.~\ref{fig:conceptual_qual}, where the direct optimization of the CLIP image embeddings harms its open-vocabulary capability. However, we find that optimizing the CLIP image embedding through the cost volume can mitigate the performance degradation. To verify this, we include additional results in the last row that employ the CLIP image encoder as feature backbone, but prohibit gradient flow through the cost volume, allowing the gradient to flow only through the feature embedding to the encoder. The degradation in performance is found to be significant, indicating that using the cost volume can serve as an effective means to prevent such degradation.

\begin{figure*}[t]
  \centering
  \renewcommand{\thesubfigure}{}
    \subfigure[(a) Input]
{\includegraphics[width=0.166\linewidth]{iccv2023AuthorKit/figure/pdf/image.pdf}}\hfill
    \subfigure[(b) ZegFormer~\cite{ding2022decoupling}]
{\includegraphics[width=0.166\linewidth]{iccv2023AuthorKit/figure/pdf/zegformer.pdf}}\hfill
     \subfigure[(c) ZSSeg~\cite{xu2022simple}]
 {\includegraphics[width=0.166\linewidth]{iccv2023AuthorKit/figure/pdf/zsseg.pdf}}\hfill
    \subfigure[(d) OVSeg~\cite{liang2022open}]
{\includegraphics[width=0.166\linewidth]{iccv2023AuthorKit/figure/pdf/ovseg.pdf}}\hfill
    \subfigure[(e) \textbf{\ours (ours)}]
{\includegraphics[width=0.166\linewidth]{iccv2023AuthorKit/figure/pdf/ours.pdf}}\hfill
    \subfigure[(f) Ground-truth]
{\includegraphics[width=0.166\linewidth]{iccv2023AuthorKit/figure/pdf/gt.pdf}}\hfill
\\
\vspace{-10pt}
\caption{\textbf{Qualitative results on PASCAL-Context with 459 categories.} In comparison to the two-stage approach~\cite{liang2022open}, where each region of an image is independently classified using CLIP, our model excels at capturing the context of an image, as the CLIP image encoder can process the image as a whole, thereby allowing the CLIP to better capture the relationships and contextual information between objects.} 
\vspace{-10pt}

  \label{fig:qualitative}
\end{figure*} \begin{figure}[t]
  \centering
  \renewcommand{\thesubfigure}{}
     \subfigure[(a) OVSeg~\cite{liang2022open}]
    {\includegraphics[width=0.497\linewidth]{iccv2023AuthorKit/figure/pdf/part/part_ac.pdf}}\hfill
     \subfigure[(b) \textbf{\ours (ours)}]
    {\includegraphics[width=0.497\linewidth]{iccv2023AuthorKit/figure/pdf/part/part_bd.pdf}}\hfill\\

\vspace{-10pt}
\caption{\textbf{Visualization of part segmentation.} Our method can identify different parts of a person, whereas OVSeg~\cite{liang2022open} can only identify person as a whole, visualizing the bias of region proposals in two-stage methods.}
\vspace{-10pt}
  \label{fig:partseg}
\end{figure} 
\vspace{-10pt}
\paragraph{Scaling comparison.}
In this analysis, we examine the scaling property of our method and compare with other recent methods~\cite{ding2022decoupling,xu2022simple} in Table~\ref{tab:scaling-ablation}. We use four variants of CLIP model, ViT-B, L, H and ViT-G, where ViT-B model has the least number of learnable parameters and ViT-G model has the most. As shown, we observe only marginal performance gains or even severe drops for competitors when we progressively increase the model capacity. In contrast, our proposed framework benefits and shows apparent improvements on almost all the datasets, which indicate the favorable scaling capacity of our work. Notably, on A-150 and A-847, our work achieves mIoU improvements of +9.0 and +4.9 over the ViT-B model, respectively.
\vspace{-10pt}
\paragraph{Training with various datasets.}
In this experiment, we further examine the generalization power of our method in comparison to other methods~\cite{ding2022decoupling, xu2022simple} by training our model on smaller-scale datasets, which include A-150 and PC-59, that poses additional challenges to achieve good performance.  The results are shown in Table~\ref{tab:cross-dataset-ablation}. As shown, we find that although we observe some performance drops, which seems quite natural when smaller dataset is used, our work significantly outperforms other competitors. These results highlight the strong generalization power of our framework, which is a favorable characteristic that suggests practicality of our approach.

\vspace{-10pt}
\paragraph{Inference speed comparison.}
In Fig.~\ref{fig:inference-time}~(b), we visualize the run-time comparison of different CLIP ViT variants~\cite{radford2021learning}, and compare our results with a recent  two-stage framework~\cite{ding2022decoupling}. Note that as the run-time of our method resort on the number of classes at inference, which results in different run-time for each dataset, we report the mean value of the run-time. As illustrated,  our approach generally runs faster than the competitor. We highlight that as we scale the model size, our approach increasingly benefits from fast inference time, which is favorable for practicality.


\subsection{Generalization to Part Segmentation} In Fig.~\ref{fig:partseg},  we show that our framework can generalize to part segmentation task, even when fine-grained categories that were not observed during training are given as inputs.
We compare with state-of-the-art two-stage approach~\cite{liang2022open}.
Interestingly, \cite{liang2022open} fails to account for fine-grained, unseen categories, as its mask proposals exhibit bias towards its training dataset. For example, as ``person" seen during training, it can only identify person as a whole, failing to segment fine-grained categories such as ``arm" and ``leg". In contrast, our framework does not suffer from such bias, enabling robust recognition of unseen objects of different granularity. \section{Conclusion}
In this paper, we have introduced cost aggregation to open-vocabulary semantic segmentation, which jointly aggregates both image and text modalities within the matching cost. Our framework, namely \ours, not only effectively transfers the knowledge of CLIP to unseen classes, but also leverages the relations between image semantics and class labels through carefully designed spatial and class aggregation module aided by the additional technique called embedding guidance. With this approach, new state-of-the-art is set on every benchmarks, which highlights the superior generalization ability of our model.
 
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\clearpage
\twocolumn[{\centering
\vspace{-20pt}
\begin{figure}[H]
    \centering
    \hsize=1.0\textwidth
\includegraphics[width=\textwidth]{iccv2023AuthorKit/figure/pdf/architecture.pdf}
    \vspace{-5pt}
    \caption{\textbf{More architectural details of \ours:} (a) overall architecture. (b) embedding guidance. Note that a generalized embedding guidance is illustrated to include different attention designs, \ie shifted window attention~\cite{liu2021swin} or linear attention~\cite{katharopoulos2020transformers}. (c) upsampling decoder layer. GN: Group Normalization~\cite{wu2018group}. LN: Layer Normalization~\cite{ba2016layer}.}
    \label{fig:guidance-architecture}
    \vspace{-5pt}
\end{figure}}]
 \renewcommand{\thesection}{\Alph{section}}
\setcounter{section}{0}
\section*{Appendix}
In the following, we first provide more implementation details in Section~\ref{A}. We then provide additional experimental results and ablation study in Section~\ref{B}. Finally, we present qualitative results for all the benchmarks and human part segmentation in Section~\ref{C} and a discussion of limitations in Section~\ref{D}.


\section{More Details}\label{A}
\subsection{Architectural Details}

In the following, we provide more architectural details. Our overall architecture is first illustrated in Fig.~\ref{fig:guidance-architecture} (a).

\smallbreak
\noindent\textbf{Embedding guidance.}
In this paragraph, we provide more details of embedding guidance, which is designed to facilitate the cost aggregation process by exploiting its rich semantics for a guidance.  We first extract visual and text embeddings from feature backbone and frozen CLIP text encoder~\cite{radford2021learning}, respectively. The embeddings then undergo linear projection and concatenated to the cost volume before query and key projections in aggregation layer. The design is illustrated in Fig.~\ref{fig:guidance-architecture}~(b). In each case, for employing Swin Transformer~\cite{liu2021swin} as a feature backbone, the stage 3 output is used, whereas for ResNet-101~\cite{he2016deep}, the output from the last conv layer of \texttt{conv4\_x} is used as our guidance.

\smallbreak
\noindent\textbf{Upsampling decoder.}
The detailed architecture is illustrated in Fig.~\ref{fig:guidance-architecture}~(c). For multi-level features , we leverage the features of last layers from \texttt{conv2\_x} and \texttt{conv3\_x} when ResNet-101~\cite{he2016deep} is used, whereas the output features of stages 1 and 2 are leveraged when Swin Transformer~\cite{liu2021swin} is used. 





\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{iccv2023AuthorKit/figure/pdf/fine-tuning2.pdf}\hfill\\
    \vspace{-5pt}
    \caption{\textbf{Illustration of the proposed fine-tuning approach.} To fine-tune the CLIP image encoder, we optimize the attention layers and position embeddings exclusively. This approach not only improves efficiency but also enhances performance. Here,  refers to the number of layers in the CLIP image encoder.}
    \label{fig:fine-tune}\vspace{-5pt}
\end{figure} 
\subsection{Other Implementation Details}
\smallbreak
\noindent\textbf{Training details.}
In Fig.~\ref{fig:fine-tune}, we visualize the proposed approach to fine-tune the CLIP image encoder, which was introduced in Section 3.5 in the main paper. A cost volume resolution of  is used for training. The position embeddings of the CLIP image encoder is initialized with bicubic interpolation~\cite{touvron2021training}, and we set training resolution as . For ViT-B and ViT-L variants, we initialize CLIP~\cite{radford2021learning} with official weights of ViT-B/16 and ViT-L/14@336px respectively. For ViT-H and ViT-G variants, we initialize with OpenCLIP~\cite{cherti2022reproducible} weights trained with LAION-2B~\cite{schuhmann2022laion}. The CLIP text encoder remains frozen across all experiments. The feature backbone networks are pre-trained on ImageNet-1k~\cite{deng2009imagenet} in  resolution for ResNet-101~\cite{he2016deep}, or on ImageNet-21k in  resolution for Swin Transformer~\cite{liu2021swin}. All hyperparameters are kept constant across the evaluation datasets.  


\begin{figure}[t]
  \centering
  \renewcommand{\thesubfigure}{}
     \subfigure[(a) Feature Aggregation]
    {\includegraphics[width=0.49\linewidth]{iccv2023AuthorKit/figure/pdf/decoder_a.pdf}}\hfill
     \subfigure[(b) Cost Aggregation]
    {\includegraphics[width=0.49\linewidth]{iccv2023AuthorKit/figure/pdf/decoder_b.pdf}}\hfill\\

\vspace{-10pt}
\caption{\textbf{Visualization of aggregation baselines.} (a) concatenates the features extracted from CLIP image and text encoders to feed into the upsampling decoder, while (b) constructs a cost volume using the image and text features from CLIP. }
\vspace{-5pt}
  \label{fig:baseline}
\end{figure} \smallbreak
\noindent\textbf{Text prompt templates.}
To obtain text embeddings from the text encoder, we follow CLIP~\cite{radford2021learning} and form sentences with the class names, such as \texttt{"a photo of a \{class\}"}. We ensemble 80 text prompts originally used in CLIP for ImageNet classification, without additionally curating text prompts or synonyms.

\smallbreak
\noindent\textbf{Feature and cost aggregation baselines.}
In this paragraph, we provide more details of the architecture of two models introduced in Fig.~\ref{fig:conceptual_qual}: One is feature aggregation method and the other is cost aggregation method. As shown in Fig.~\ref{fig:baseline} (a), the feature aggregation method directly leverages the features extracted from CLIP image encoder by feeding the concatenated image and text embeddings into the upsampling decoder. Fig.~\ref{fig:baseline} (b) shows the cost aggregation approach that constructs cost volume instead, and subsequent embedding layer processes it to feed into upsampling decoder. 


\subsection{Patch Inference}
The practicality of Vision Transformer (ViT)~\cite{dosovitskiy2020image} for high-resolution image processing has been limited due to its quadratic complexity with respect to the sequence length. As our model leverages ViT to extract image embeddings, \ours may struggle to output to the conventional image resolutions commonly employed in semantic segmentation literature, such as ~\cite{cheng2021per,ghiasi2022scaling}, without sacrificing some accuracy made by losing some fine-details. Although we can adopt the same approach proposed in~\cite{zhou2022extract} to upsample the positional embedding~\cite{zhou2022extract}, we ought to avoid introducing excessive computational burdens, and thus adopt an effective inference strategy without requiring additional training which is illustrated in Fig.~\ref{fig:patch-inference}.

To this end, we begin by partitioning the input image into overlapping patches of size . Intuitively, given an image size of , we partition the image to sub-images of size , which matches the image resolution at training phase, and each sub-images has overlapping regions . Subsequently, we feed these sub-images and the original image that is resized to  into the model. Given the results for each patches and the image, we merge the obtained prediction, while the overlapping regions are averaged to obtain the final prediction. In practice, we employ ,  while adjusting the overlapping region to match the effective resolution of .

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{iccv2023AuthorKit/figure/pdf/patch_inference.pdf}\hfill\\
    \vspace{-5pt}
    \caption{\textbf{Illustration of the patch inference.} During inference, we divide the input image into patches, thereby increasing the effective resolution.}
    \label{fig:patch-inference}\vspace{-5pt}
\end{figure} 
\section{Additional Ablation Study}\label{B}
\subsection{Comparison of Aggregation Baselines}\vspace{-10pt}
\begin{table}[H]
    \centering
    \resizebox{0.48\textwidth}{!}{\begin{tabular}{cl|cccccc}
    \toprule
        & Methods & A-847 & PC-459 & A-150 & PC-59 & PAS-20 & 
        \\
        \midrule\midrule
        \textbf{(I)} & Feature agg. + Freeze & 3.1 & 8.7 & 16.6 & 46.8 & 92.3 & 69.7\\
        \textbf{(II)} & Feature agg. + F.T. & \underline{3.8} & \underline{10.9} & \underline{19.1} & \underline{53.5} & \underline{96.2} & \underline{74.2}\\
        \midrule
        \textbf{(III)} & Cost agg. + Freeze& 2.7 & 5.4 & 11.0 & 27.7 & 65.1 & 44.9\\
        \textbf{(IV)} & Cost agg. + F.T. & \textbf{9.0} & \textbf{17.2} & \textbf{26.9} & \textbf{57.0} & \textbf{96.9} & \textbf{76.8}\\
        \bottomrule
    \end{tabular}}
    \vspace{-5pt}
    \caption{\textbf{Quantitative comparison between feature and cost aggregation.} Cost aggregation acts as an effective alternative to direct fine-tuning of CLIP image encoder. \textit{F.T.: Fine-Tuning.}
    }
    \label{tab:feature-vs-cost}
    \vspace{-10pt}

\end{table}
 In this ablation study, we provide a quantitative comparison of two aggregation baselines, feature aggregation and cost aggregation, in Table~\ref{tab:feature-vs-cost}. We freeze the CLIP image encoder and only optimize the upsampling decoder, and the results are summarized in \textbf{(I)} and \textbf{(III)}. Subsequently, in \textbf{(II)} and \textbf{(IV)}, we fine-tune the CLIP image encoder on top of \textbf{(I)} and \textbf{(III)}. Our results show that feature aggregation  can benefit from fine-tuning, but the gain is only marginal. On the other hand, cost aggregation benefits significantly from fine-tuning, highlighting the effectiveness of cost aggregation to transfer knowledge in CLIP encoder. 

\subsection{Ablation Study of the Number of Layers  }\vspace{-10pt}
\begin{table}[H]
    \centering
    \resizebox{0.48\textwidth}{!}{\begin{tabular}{c|cccccc}
    \toprule
        \# of layers & A-847 & PC-459 & A-150 & PC-59 & PAS-20 & 
        \\
        \midrule\midrule
        1 & \underline{10.7} & 16.3 & \underline{30.6} & 61.2 & \textbf{96.6} & 81.7\\
        2 & \textbf{10.8} & \textbf{20.4} & \textbf{31.5} & \underline{62.0} & \textbf{96.6} & \underline{81.8}\\
        3 & \textbf{10.8} & \underline{20.0} & \textbf{31.5} & 61.9 & \underline{96.5} & \underline{81.8}\\
        4 & \textbf{10.8} & \textbf{20.4} & \textbf{31.5} & \textbf{62.1} & \textbf{96.6} & \textbf{82.2}\\
        \bottomrule
    \end{tabular}}
    \vspace{-5pt}
    \caption{\textbf{Effects of varying .}
    }
    \label{tab:layer-ablation}
    \vspace{-10pt}

\end{table} Table~\ref{tab:layer-ablation} summarizes the effects of varying . From the results, we find that choosing higher  does not always lead to performance improvements.
Note that we chose  to balance between performance and model capacity.

\subsection{Ablation Study of Inference Strategy}\vspace{-10pt}
\begin{table}[H]
    \centering
    \resizebox{\linewidth}{!}{
   \begin{tabular}{l|cccccc}
        \toprule
        Methods & A-847 & PC-459 & A-150 & PC-59 & PAS-20 & 
         \\
        \midrule\midrule
        \ours w/ training reso. & \underline{10.1}& \underline{18.9}& 29.8& \underline{59.8}& \underline{96.1}& 79.7\\
        \ours & OOM & 14.8& \underline{30.4}& 59.7& 95.9& \underline{80.3}\\
        \hlrow\ours (ours) & \textbf{10.8}& \textbf{20.4}& \textbf{31.5}& \textbf{62.0}& \textbf{96.6}& \textbf{81.8}\\
        \bottomrule
\end{tabular}}\vspace{-5pt}
        \caption{\textbf{Ablation study of inference strategy.} CLIP with ViT-L is used for ablation. : Process  by upsampling position embedding of CLIP ViT. \textit{OOM: Out-of-memory.}}
    \label{tab:inference-ablation}
    \vspace{-10pt}
\end{table} Table~\ref{tab:inference-ablation} presents effects of different inference strategies for our model. The first row shows the results using the training resolution at inference time. The second row represents a variant that upsamples the positional embedding within the CLIP image encoder, allowing the encoder to process higher resolution images. Finally, the last row adopts the proposed patch inference strategy. It is shown that increasing the positional embedding introduces substantial memory overhead. Moreover, for some datasets, using training resolution yields better performance. On the other hand, our proposed approach can bring large performance gains, while also ensuring high efficiency.

\subsection{Effects of Upsampling Decoder}\vspace{-10pt}
\begin{table}[H]
    \centering
    \resizebox{\linewidth}{!}{
   \begin{tabular}{l|cccccc}
        \toprule
        Methods & A-847 & PC-459 & A-150 & PC-59 & PAS-20 & 
         \\
        \midrule\midrule
        \ours w/o upsampling decoder & \underline{10.1}& \underline{18.9}& \underline{29.8}& \underline{59.8}& \underline{96.1}& \underline{79.7}\\
        \hlrow\ours (ours) & \textbf{10.8} & \textbf{20.4}& \textbf{31.5}& \textbf{62.0}& \textbf{96.6}& \textbf{81.8}\\
        \bottomrule
\end{tabular}}\vspace{-5pt}
        \caption{\textbf{Ablation study of upsampling decoder.} CLIP with ViT-L is used for ablation.}
    \label{tab:conv-decoder}
    \vspace{-10pt}
\end{table} We provide an quantitative results of adopting the proposed upsampling decoder in Table~\ref{tab:conv-decoder}. The results show consistent improvements across all the benchmarks.

\subsection{Comparison of Inference Time}\vspace{-10pt}
\begin{table}[H]
    \centering
    \resizebox{\linewidth}{!}{\begin{tabular}{lc|ccccccc}
    \toprule
        Methods & VLM & Mean & A-847 & PC-459 & A-150 & PC-59 & PAS-20
        \\
        \midrule\midrule
        ZegFormer~\cite{ding2022decoupling} & ViT-B/16 & 0.85&-&-&-&-&-\\
        \hlrow \ours (ours) & ViT-B/16 &0.73 \textcolor{ForestGreen}{(-0.12)}&1.83 \textcolor{red}{(+0.98)}&1.04 \textcolor{red}{(+0.19)}&0.40 \textcolor{ForestGreen}{(-0.45)}&0.22 \textcolor{ForestGreen}{(-0.63)}&0.14 \textcolor{ForestGreen}{(-0.71)}\\
        \midrule
        ZegFormer~\cite{ding2022decoupling} & ViT-L/14 & 2.70&-&-&-&-&-\\
        \hlrow \ours (ours) & ViT-L/14 & 0.79 \textcolor{ForestGreen}{(-1.91)}&1.91 \textcolor{ForestGreen}{(-0.79)}&1.09 \textcolor{ForestGreen}{(-1.61)}&0.46 \textcolor{ForestGreen}{(-2.24)}&0.28 \textcolor{ForestGreen}{(-2.42)}&0.21 \textcolor{ForestGreen}{(-2.49)}\\
        \midrule
        ZegFormer~\cite{ding2022decoupling} & ViT-H/14 & 5.01&-&-&-&-&-\\
        \hlrow \ours (ours) & ViT-H/14 & 0.88 \textcolor{ForestGreen}{(-4.13)}&2.01 \textcolor{ForestGreen}{(-3.00)}&1.21 \textcolor{ForestGreen}{(-3.80)}&0.54 \textcolor{ForestGreen}{(-4.47)}&0.35 \textcolor{ForestGreen}{(-4.66)}&0.28 \textcolor{ForestGreen}{(-4.73)}\\
        \midrule
        ZegFormer~\cite{ding2022decoupling} & ViT-G/14 & 13.40&-&-&-&-&-\\
        \hlrow \ours (ours) & ViT-G/14 & 1.14 \textcolor{ForestGreen}{(-12.26)}&2.32 \textcolor{ForestGreen}{(-11.08)}&1.51 \textcolor{ForestGreen}{(-11.89)}&0.77 \textcolor{ForestGreen}{(-12.63)}&0.58 \textcolor{ForestGreen}{(-12.82)}&0.51 \textcolor{ForestGreen}{(-12.89)}\\
        \bottomrule
    \end{tabular}}
    \vspace{-5pt}
    \caption{\textbf{Comparison of inference time in seconds.} Faster scores are highlighted in \textcolor{ForestGreen}{green}, while slower scores are displayed in \textcolor{red}{red}. For ZegFormer~\cite{ding2022decoupling}, we report only the mean inference time, as the method exhibits minimal variance across different datasets.
    }
    \label{tab:inference-time}
    \vspace{-5pt}

\end{table} In Table~\ref{tab:inference-time}, we show run-time comparison at inference phase to the recent two-stage approach~\cite{ding2022decoupling}. We report the results with different VLM, \textit{e.g.,} ViT-B, L, H and G.   Note that the inference speed of the proposed method differs depending on the number of categories, resulting in different run-times across the evaluation datasets. In comparison to~\cite{ding2022decoupling}, we report the mean run-time, as its performance demonstrates minimal variation with respect to the number of categories.

When utilizing ViT-B for vision-language models, our method exhibits a relatively slower inference time for some scenarios, including A-847 and PC-459.  However, as we scale the VLM to larger variants, such as ViT-L, H, and G, our approach enjoys significantly faster inference by almost over 20 times faster, highlighting the efficiency of the proposed method.



\section{More Qualitative Results}\label{C}
We provide more qualitative results on A-847~\cite{zhou2019semantic} in Fig.~\ref{fig:ade847}, PC-459~\cite{mottaghi2014role} in Fig.~\ref{fig:pc459}, A-150~\cite{zhou2019semantic} in Fig.~\ref{fig:ade150}, and PC-59~\cite{mottaghi2014role} in Fig.~\ref{fig:pc59}. We also further compare the results in A-847~\cite{zhou2019semantic} with other methods~\cite{ding2022decoupling, xu2022simple, liang2022open} in Fig.~\ref{fig:ade847-comparison}.

\smallbreak
\noindent\textbf{Qualitative results on part segmentation.}
We further compare human part segmentation results with \cite{liang2022open} in Fig.~\ref{fig:partseg-supple}. 


\section{Limitations}\label{D}
To evaluate open-vocabulary semantic segmentation results, we follow ~\cite{ghiasi2022scaling, liang2022open} and compute the metrics using the other segmentation datasets. However, since the ground-truth segmentation maps involve some ambiguities, the reliability of  the evaluation dataset is somewhat questionable. For example, the last row of Fig.~\ref{fig:qualitative}~(e) exemplifies how our predictions in the mirror, ``sky" and ``car", as well as ``plant" in between the ``fence", are classified as wrong segmentation as the ground truth classes are ``mirror" and ``fence". Constructing a more reliable dataset including ground-truths accounting for above issue for accurate evaluation is an intriguing topic. 


\begin{figure*}
    \centering
    \includegraphics[width=0.95\linewidth]{iccv2023AuthorKit/figure/pdf/qualitative_supp/ade847.pdf}\hfill\\
    \vspace{-5pt}
    \caption{\textbf{Qualitative results on ADE20K~\cite{zhou2019semantic} with 847 categories.}}
    \label{fig:ade847}\vspace{-5pt}
\end{figure*} \begin{figure*}
    \centering
    \includegraphics[width=0.95\linewidth]{iccv2023AuthorKit/figure/pdf/qualitative_supp/pc459.pdf}\hfill\\
    \vspace{-5pt}
    \caption{\textbf{Qualitative results on PASCAL Context~\cite{mottaghi2014role} with  459 categories.}}
    \label{fig:pc459}\vspace{-5pt}
\end{figure*} \begin{figure*}
    \centering
    \includegraphics[width=0.95\linewidth]{iccv2023AuthorKit/figure/pdf/qualitative_supp/ade150.pdf}\hfill\\
    \vspace{-5pt}
    \caption{\textbf{Qualitative results on ADE20K~\cite{zhou2019semantic} with 150 categories.}}
    \label{fig:ade150}\vspace{-5pt}
\end{figure*} \begin{figure*}
    \centering
    \includegraphics[width=0.95\linewidth]{iccv2023AuthorKit/figure/pdf/qualitative_supp/pc59.pdf}\hfill\\
    \vspace{-5pt}
    \caption{\textbf{Qualitative results on PASCAL Context~\cite{mottaghi2014role} with 59 categories.}}
    \label{fig:pc59}\vspace{-5pt}
\end{figure*} \begin{figure*}
    \centering
    \includegraphics[width=1.0\linewidth]{iccv2023AuthorKit/figure/pdf/qualitative_supp/supp_comparison.pdf}\hfill\\
    \vspace{-5pt}
    \caption{\textbf{Comparison of qualitative results on ADE20K~\cite{zhou2019semantic} with 847 categories.} We compare CAT-Seg with ZegFormer~\cite{ding2022decoupling}, ZSseg~\cite{xu2022simple}, and OVSeg~\cite{liang2022open} on A-847 dataset.}
    \label{fig:ade847-comparison}\vspace{-5pt}
\end{figure*} 




\begin{figure*}
    \centering
    \includegraphics[width=0.95\linewidth]{iccv2023AuthorKit/figure/pdf/qualitative_supp/part_supp.pdf}\hfill\\
    \vspace{-5pt}
    \caption{\textbf{Qualitative results on human part segmentation.} We compare CAT-Seg with OVSeg~\cite{liang2022open}.}
    \label{fig:partseg-supple}\vspace{-5pt}
\end{figure*} 
\clearpage
 

\end{document}
