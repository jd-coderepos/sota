\UseRawInputEncoding 
\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}              

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}
\usepackage{array}
\usepackage{color}


\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


\def\cvprPaperID{8372} \def\confName{CVPR}
\def\confYear{2022}


\begin{document}

\title{Cost Function Unrolling in Unsupervised Optical Flow}

\author{Gal Lifshitz\\
Tel Aviv University\\
{\tt\small lifshitz@mail.tau.ac.il}
\and
Dan Raviv\\
Tel Aviv University\\
{\tt\small darav@tauex.tau.ac.il}
}
\maketitle

\begin{abstract}

Steepest descent algorithms, which are commonly used in deep learning, use the gradient as the descent direction, either as-is or after a direction shift using preconditioning. 
In many scenarios calculating the gradient is numerically hard due to complex or non-differentiable cost functions, specifically next to singular points. 
In this work we focus on the derivation of the Total Variation semi-norm commonly used in unsupervised cost functions.
Specifically, we derive a differentiable proxy to the hard $L^1$ smoothness constraint in a novel iterative scheme which we refer to as Cost Unrolling.
Producing more accurate gradients during training, our method enables finer predictions of a given DNN model through improved convergence, without modifying its architecture or increasing computational complexity.
We demonstrate our method in the unsupervised optical flow task. Replacing the $L^1$ smoothness constraint with our unrolled cost during the training of a well known baseline, we report improved results on both MPI Sintel and KITTI 2015 unsupervised optical flow benchmarks.
Particularly, we report EPE reduced by up to 15.82\% on occluded pixels, where the smoothness constraint is dominant, enabling the detection of much sharper motion edges.
\footnote{Under review. Our code will be publicly available upon publication.}






\end{abstract}

 \section{Introduction}

The $L^1$ norm of the gradients of a given function, also known as Total Variation (TV) semi-norm, and more specifically its estimation, has been a significant field of study in robust statistics \cite{Huber.Wiley.ea1981Robuststatistics}. Even prior to the sweeping AI era, many approaches to Computer Vision problems, such as image restoration, denoising and registration \cite{RUDIN1992259,doi:10.1080/00207160500069904,zach2007duality}, have used a TV regularizer, as it represents the prior distribution of pixel intensities of natural images \cite{786990}. Its main  advantage is its robustness to small oscillations such as noise while preserving sharp discontinuities such as edges.

Historically, solving the TV problem has been a challenging task, mainly due to the non-differenetiabilty of the $L^1$ norm at zero. 
As a result, differentiable relaxations have been proposed, such as the Huber \cite{huber1964robust} and Charbonnier \cite{charbonnier1997deterministic} loss functions. Further approaches involved iterative variational methods and have shown superior results in many tasks \cite{admm0,zach2007duality,RUDIN1992259}.
Introducing trainable Deep Neural Networks (DNNs) to Computer Vision has brought a significant performance boost, and specifically the commonly used auto-derivation frameworks \cite{pytorch,jia2014caffe,tensorflow2015-whitepaper} have provided quick and easy tools to solve complex functions. Indeed, these frameworks commonly bypass the $L^1$ non-differentiability either by a differentiable proxy or using its sub-gradients. We claim that non-differentiable cost functions should be dealt with greater care, as was done for many years before the deep learning era.

\begin{figure}
\begin{center}
\includegraphics[width=1\linewidth]{figures/files/teaser_sintel.png}
\end{center}
   \caption{\textbf{Sintel Final benchmark qualitative example.}
Introducing our method to the unsupervised optical flow domain, our unrolled cost function enables the detection of sharper motion boundaries through improved convergence, without modifying the model's architecture or increasing complexity.
Displayed are the predicted Sintel Final benchmark 'Market 1' flows (bottom) and errors (top) of both our method and the ARFlow \cite{liu2020learning} baseline, with close-ups on specific regions.
   White regions feature measured high errors.
   }
\label{fig:teaser}
\end{figure}
 
Proximal iterative methods for learning complex functions have shown superiority in many Image Processing, as well as Computer Vision tasks \cite{wang2016proximal,monga2019algorithm,zhang2020deep}. In these works, axiomatic optimization algorithms are unrolled and each iteration is mapped to a sub-network having its own learnable parameters. The resulting network architectures consist of task specific stacked modules which may be trained end-to-end in both supervised and unsupervised manners. Performance boost has been achieved either by increasing the number of stacked modules \cite{wang2016proximal} or introducing intermediate modules  \cite{luo2021upflow,wang2020cot}, both resulting in increased model complexity and size.

In this paper, we shift unrolling to the cost function, improving DNN model optimization while preserving its architecture as opposed to standard unrolling. Specifically, we present a novel pipeline we refer to as Cost Unrolling, in which we unroll the commonly used unsupervised cost function consisting of a data term and TV smoothness regularization, to obtain an iterative proxy. Following the well known Alternating Direction Method of Multiplies (ADMM) \cite{admm0} algorithm, our initial optimization problem is iteratively decomposed into a set of sub-problems, each one featuring a differentiable cost function. Gradients computed using all sub-cost functions at each training step are shown to be more accurate in the regions where the gradients of the original cost function are hard to evaluate or undefined, improving convergence. 


We demonstrate the effectiveness of our unrolled cost function in synthetic experiments, as well as the unsupervised optical flow domain. The lack of available labeled data has made unsupervised optical flow learning a significant field of interest. Used cost functions commonly consist of a measure for image consistency and TV smoothness regularization. The occluded pixels, pixels within a reference image with no correspondence at the target image, are masked out when measuring image consistency. As a result, occluded regions, which are highly correlated to motion boundaries, are mostly dominated by the gradients of the smoothness constraint.

Our unrolled cost function is introduced to the recently published unsupervised optical flow pipeline ARFlow \cite{liu2020learning}. Replacing their used TV smoothness constraint with our unrolled cost during all phases of training produces improved results on both MPI Sintel \cite{Butler:ECCV:2012} and KITTI 2015 \cite{Menze2015CVPR} unsupervised optical flow benchmarks.
Particularly, we report EPE reduced by up to 15.82\% on occluded pixels, where the smoothness constraint is dominant, allowing the detection of much sharper motion edges, as is highly visible in figures \ref{fig:teaser} and \ref{fig:l1vsunroll}.



\subsection{Contributions}
We summarize our contributions as follows:
\begin{itemize}
    \item Present the first unrolled cost function in deep learning for TV regularized problems.
    \item Demonstrate our proposed pipeline in the real-life unsupervised optical flow domain.
    \item Report improved results on unsupervised optical flow benchmarks, achieved without modifying model architecture or increasing complexity.
\end{itemize}
 \section{Related Work}

Total Variation (TV) minimization has been a significant field of research in robust statistics \cite{Huber.Wiley.ea1981Robuststatistics}. It was introduced to Image Processing and Computer Vision tasks by \cite{RUDIN1992259} and showed superior performance over previous methods mainly due to its edge preserving property. However, solving TV regularized problems has shown to be non-trivial, mainly as the $L^1$ norm is non-differentiable at zero. Differentiable relaxations were proposed by \cite{huber1964robust,charbonnier1997deterministic,Chambolle1997ImageRV}, however iterative variational methods have proven more effective \cite{doi:10.1080/00207160500069904,zach2007duality,admm0}.

Introducing trainable DNN models to Computer Vision tasks has brought a significant performance boost. The lack of labeled data available for training has pushed many DNN algorithms towards unsupervised learning. Not surprisingly, the TV smoothness regularization can be found in the cost functions of many unsupervised works both past and recent. As DNN algorithms use gradient descent algorithms for optimization, the non-differentiability of the $L^1$ norm at zero has been commonly bypassed either by differentiable approximations \cite{meister2017unflow} or by using its sub-gradients \nolinebreak \cite{kim2020unsupervised,jonschkowski2020matters,luo2021upflow}.

Algorithm unrolling is an iterative structured approach for learning complex functions, in which iterative operations derived from axiomatic algorithms determine the model architecture. A vast review of the topic is presented in \cite{monga2019algorithm}. Axiomatic algorithms, such as the Alternating Direction Method of Multipliers (ADMM) \cite{admm0}, Half Quadratic Splitting (HQS) \cite{5445028} or Primal-Dual method \cite{chambolle2011first}, have inspired many deep algorithm unrolling frameworks. Indeed, ADMM-Net \cite{sun2016deep} introduced a DNN model for compressed sensing with architecture derived from ADMM, USRNet \cite{zhang2020deep} learned Single Image Super-Resolution (SISR) using a deep framework inspired by HQS and \cite{wang2016proximal} demonstrated an iterative structure derived from a Primal-Dual solver, aimed to refine the optical flow predicted by a FlowNet \cite{dosovitskiy2015flownet} model. 

However, these frameworks feature closed form update steps derived from the learned objective function, which are mapped into task specific sub-networks trained in a supervised manner, increasing model size and complexity. As opposed to standard unrolling, we demonstrate improving the optimization of a given DNN model while preserving its architecture. We unroll the commonly used unsupervised cost function consisting of a data term and TV smoothness regularization, to obtain an iterative differentiable proxy which produces more accurate gradients during training (demonstrated in the experimentation section). While previous works designed dedicated unrolled learnable units, we present a pipeline that can be applied to any model architecture.


\begin{figure*}
\begin{center}
\includegraphics[trim=0 0 0 0,  width=1\linewidth]{figures/files/cvpr_unrolling.png}
   \caption{\textbf{Unrolled cost block diagram for unsupervised optical flow.} 
   In each training iteration, given a flow prediction, its weighted spatial gradients $\mathbf{C}$ are computed.
   Once obtained, the Soft Thresholding and Multipliers Update operations are carried for update steps $t \in \{0,...,T-1\}$ to produce $\{\mathbf{Q}^{(t)},\boldsymbol{\beta}^{(t)} \}_{t=0}^{T-1}$, which are then used together with $\mathbf{C}$ to construct the unrolled loss function in \nolinebreak(\ref{eq:loss_f}). As our unrolled cost is computed only during training, our method does not affect inference.}
   
\label{fig:unrol_arch}
\end{center}
\end{figure*}
 
Learning in iterations has shown to be very effective at both image registration and optical flow tasks. While FlowNet and FlowNet 2.0 \cite{dosovitskiy2015flownet,ilg2017flownet} were the first to learn optical flow, they were surpassed in both supervised and unsupervised settings, by the well known PWC-Net \cite{Sun2018PWC-Net} which featured a course-to-fine iterative scheme. RAFT \cite{teed2020raft} introduced a fully iterative architecture which enabled a significant performance boost, as well as its expansion to the unsupervised settings \cite{stone2021smurf}. Further approaches, such as \cite{wang2016proximal,wang2020cot,luo2021upflow}, managed to improve the results of existing frameworks by adding intermediate modules and parameters for better flow refinement, which increase model size and complexity. 

\emph{Here we demonstrate that unlike all other methods, improving the ability of a model to predict a finer flow can be achieved simply through improving the computed gradients during training.}













  \section{Cost Function Unrolling}
A general formulation of the cost function used for unsupervised learning consists of a data term, measuring the likelihood of a given prediction over the given data, as well as a prior term, constraining probable predictions. In this work we consider the TV regularized unsupervised cost function case and unroll it to obtain our novel smoothness regularizer. Denote by $\Theta$ the set of trainable parameters of a DNN model, its predicted output $\mathbf{F}$ and the set of unlabeled training data $\mathcal{I}$. The unsupervised TV regularized cost function takes the form:
\begin{equation} \label{eq:loss}
    \mathcal{L}(\mathcal{I},\Theta) = \mathbf{\Phi}\left( \mathbf{F},\mathcal{I} \right)+\lambda\|\nabla \mathbf{F} \|_1
\end{equation}
where $\mathbf{\Phi(\cdot)}$ is a differentiable function measuring the likelihood of $\mathbf{F}$ over the training data $\mathcal{I}$, $\nabla \mathbf{F}$ are its spatial gradients, $\|\cdot\|_1$ is the $L^1$ norm and $\lambda$ is a hyperparameter controlling regularization.

\subsection{Unrolling the Unsupervised Cost Function}
Our goal is to minimize the objective function in (\ref{eq:loss}). Following the ADMM \cite{admm0} algorithm, we derive the iterative update steps minimizing (\ref{eq:loss}), which are then used to construct our unrolled cost function.
Let us examine the following optimization problem:
\begin{equation} \label{eq: opt}
    \hat{\mathbf{F}}=\arg\min_\mathbf{F} \left\{ \mathbf{\Phi}\left( \mathbf{F},\mathcal{I} \right)+\lambda\|\nabla \mathbf{F} \|_1 \right\}
\end{equation}
We rewrite (\ref{eq: opt}), introducing an auxiliary variable $\mathbf{Q}$:
\begin{equation}
\begin{aligned}
    \mathbf{\hat{F},\hat{Q}}=&\arg\min_{\mathbf{F,Q}} \left\{ \mathbf{\Phi}\left( \mathbf{F},\mathcal{I} \right)+\lambda\|\mathbf{Q}\|_1 \right\} \\ &\text{s.t.} \quad \mathbf{Q=\nabla F}
\end{aligned}
\end{equation}
The augmented Lagrangian is then constructed as follows:
\begin{equation}
\begin{aligned}
     &\mathcal{L}_{\rho}(\mathbf{F,Q},\boldsymbol{\alpha}) = \mathbf{\Phi}\left( \mathbf{F},\mathcal{I} \right)+\lambda\|\mathbf{Q}\|_1\\ 
     &+ \mathbf{\left<\boldsymbol{\alpha},Q - \nabla F \right>}_2 + \frac{\rho}{2} \mathbf{\|Q - \nabla F\|}_2^2
\end{aligned}
\end{equation}
where $\boldsymbol{\alpha}$ is the Lagrange Multipliers matrix and $\rho$ is a penalty parameter. Substituting $\boldsymbol{\beta}=\frac{\boldsymbol{\alpha}}{\rho}$ for simplicity, we iteratively optimize $\mathbf{\{F,Q,\boldsymbol{\beta}\}}$ through solving the sub-problems (\ref{eq:admm_f}), (\ref{eq:admm_q}) and update the multipliers (\ref{eq:admm_b}) in each iteration:
\begin{subequations} 
\begin{align}
    \hat{\mathbf{F}}=&\min_{\mathbf{F}} \left\{\mathbf{\Phi}\left( \mathbf{F},\mathcal{I} \right) + \frac{\rho}{2} \mathbf{\|Q - \nabla F + \boldsymbol{\beta}\|}_2^2 \right\} \label{eq:admm_f} \\ 
    \hat{\mathbf{Q}}=&\min_{\mathbf{Q}} \left\{\mathbf{\lambda\|Q\|_1} + \frac{\rho}{2} \mathbf{\|Q - \nabla F + \boldsymbol{\beta}\|}_2^2 \right\} \label{eq:admm_q} \\ 
    \boldsymbol{\beta} \leftarrow &\boldsymbol{\beta} + \eta \mathbf{\left( Q - \nabla F \right)} \label{eq:admm_b}
\end{align}
\end{subequations}
with $\eta$ an update rate. The solutions to the problems in (\ref{eq:admm_f}) and (\ref{eq:admm_q}) are referred to as the ADMM update steps for $\mathbf{F}$ and $\mathbf{Q}$, respectively.

\subsection{Solving the Sub-Optimization Problems}
Solutions to the sub-optimization problems in equations (\ref{eq:admm_f}) and (\ref{eq:admm_q}) are now discussed. 
While the problem in equation (\ref{eq:admm_q}) has a closed form solution, deriving a closed form solution for the problem in (\ref{eq:admm_f}) is not trivial, as $\mathbf{\Phi}$ can be any function.
Substituting $\mathbf{C=\nabla F}$, (\ref{eq:admm_q}) reduces to:
\begin{equation}
\begin{aligned}
    &\min_{\mathbf{Q}} \left\{\mathbf{\lambda\|Q\|}_1 + \frac{\rho}{2} \mathbf{\|Q - C + \boldsymbol{\beta}\|}_2^2 \right\} \\
    &= \min_{\mathbf{Q}} \left\{\lambda \sum_{i}|Q_i| + \frac{\rho}{2} \sum_{i} |Q_i - C_i + \beta_i|^2 \right\} \\
    &= \sum_i \min_{Q_i} \left\{ \lambda |Q_i| + \frac{\rho}{2}|Q_i - C_i + \beta_i|^2 \right\}
    \label{eq:admm_qsol1}
    \end{aligned}
\end{equation}
where $A_i$ are the matrix elements of $\mathbf{A}$, i.e. the problem is separable and we may solve independently for each matrix element. The objective in (\ref{eq:admm_qsol1}) is convex and its solution is the well known Soft Thresholding operator $\hat{Q}_i = \mathcal{S}_{\lambda/\rho} \left( C_i-\beta_i \right)$, defined:
\begin{equation} \label{eq:S}
\begin{aligned}
    \mathcal{S}_{\lambda/\rho} (x) =
    \begin{cases}
    0, & |x| < \lambda / \rho \\
    x - \frac{\lambda}{\rho} \text{sign}(x), & |x| \geq \lambda / \rho
    \end{cases}
\end{aligned}
\end{equation}
The Soft Thresholding operator performs shrinkage of the input signal, thus it promotes sparse solutions.

We now consider the sub-problem in equation (\ref{eq:admm_f}). Using the same substitution as in (\ref{eq:admm_qsol1}), we wish to optimize:
\begin{equation}
    \min_{\mathbf{F}} \left\{\mathbf{\Phi}\left( \mathbf{F},\mathcal{I} \right) + \frac{\rho}{2} \mathbf{\|Q - C + \boldsymbol{\beta}\|}_2^2 \right\} \label{eq:admm_fsol1}
\end{equation}
Deriving a closed form solution is not trivial, as we wish to optimize for any $\mathbf{\Phi(\cdot)}$. However, note that the problem in (\ref{eq:admm_fsol1}) consists of the same data term as in (\ref{eq: opt}) with the TV smoothness regularizer replaced by a softer, differentiable constraint. Re-substituting into eq. (\ref{eq:admm_fsol1}), the constraint takes the form:
\begin{equation}
    \frac{\rho}{2} \|\mathcal{S}_{\lambda/\rho} \left(\nabla \mathbf{F}-\boldsymbol{\beta} \right) - \left(\nabla \mathbf{F} - \boldsymbol{\beta}\right)\|_2^2 \label{eq:smoothness}
\end{equation}
Recall that minimizing the TV of a function promotes sparse output gradients. In fact, (\ref{eq:smoothness}) yields a differentiable alternative for TV minimization, as it suggests minimizing the $L^2$ distance between the true and sparsified output gradients in each ADMM iteration. This realization stands in the core of our approach.



\begin{figure*}
\begin{center}
\includegraphics[trim=10 10 0 0, width=1\linewidth]{figures/files/tv_graphs.png}
   \caption{\textbf{PC signal prediction - training.} We train a simple model to predict a full PC 1D function given a small set of sampled points and TV regularization, using our unrolled cost, as well as TV cost function and common TV relaxation methods. From right to left: displayed are the validation errors and gradient norms, recorded during training, as well as gradients measured at $x=-1$ and $x=1$, respectively. As can be seen, our unrolled cost not only achieves the best results, but it also converges the fastest and the smoothest.}
\label{fig:toy_graphs}
\end{center}
\end{figure*} 
\subsection{Unrolled Cost Function}
Our unrolled cost function is inspired by the ADMM update steps given in equations (\ref{eq:admm_f}),(\ref{eq:admm_q}),(\ref{eq:admm_b}). However, instead of explicitly refining $\mathbf{F}$ in each iteration as in standard unrolling, we aim to solve the iterative sub-problems given in (\ref{eq:admm_f}) simultaneously, updating $\mathbf{Q},\boldsymbol{\beta}$ alone given a fixed model prediction $\mathbf{F}$.

Our proposed unrolled cost function takes the form:
\begin{equation}
\begin{aligned}
    \mathcal{L}^{\mathbf{F}}(\mathcal{I},\Theta)=&\mathbf{\Phi}\left( \mathbf{F},\mathcal{I} \right) \\
    +&\frac{\rho}{2} \frac{1}{T} \sum_{t=0}^{T-1} \|\mathbf{Q}^{(t)} + \boldsymbol{\beta}^{(t)} - \mathbf{C} \|_2^2 \label{eq:loss_f}
\end{aligned}
\end{equation}
where:
\begin{subequations} 
\begin{align}
    \mathbf{Q}^{(t)}&=\mathcal{S}_{\lambda/\rho} \left( \mathbf{\nabla F}-\boldsymbol{\beta}^{(t-1)} \right) \label{eq:update_q} \\ 
    \boldsymbol{\beta}^{(t)}&= \boldsymbol{\beta}^{(t-1)} + \eta \mathbf{\left( Q^{(t)} - \nabla F \right)} \label{eq:update_b}
\end{align}
\end{subequations}
$Ö¿\mathbf{C} =\nabla\mathbf{F}$ and $T$ is a hyperparameter stating the number of update steps carried. Thus, at each training iteration, given a model prediction $\mathbf{F}$, its spatial gradients $\mathbf{C}$ are calculated and used to iteratively compute $\{\mathbf{Q}^{(t)},\boldsymbol{\beta}^{(t)}\}_{t=0}^{T-1}$. All update terms are then accumulated to construct the smoothness constraint in (\ref{eq:loss_f}), which is used for model training. 
A block diagram for the unsupervised optical flow setting (described in section \ref{sec:of}) is shown in figure \ref{fig:unrol_arch}. Being fully differentiable, specifically around its optimum, our unrolled smoothness constraint produces more accurate  gradients, converges more efficiently and improves performance, as is shown in the experimentation section. \section{Experimentation}
\begin{figure}
\begin{center}
\includegraphics[trim= 0 0 10 10, width=1\linewidth]{figures/files/1234_tv_signal.png}
\end{center}
   \caption{\textbf{PC signal prediction - target vs. predicted signals.}
   Displayed are the generated target signal, its samples and signals predicted by models trained using either standard TV regularization or our unrolled cost function. As can be seen, our unrolled cost overcomes the staircasing effect, promoting a solution which is both smooth and edge preserving.}
\label{fig:toy_sigs}
\end{figure} We demonstrate the effectiveness of our method through conducting experiments in two domains: piece-wise constant (PC) signal prediction (section \ref{sec:synthtetic}) and unsupervised optical flow (section \ref{sec:of}). 

We further compare our unrolled cost to standard TV smoothness regularization, as well as $L^1$ relaxation baselines: Huber \cite{huber1964robust} and Charbonnier \cite{charbonnier1997deterministic} loss functions.
The Huber regularizer over a prediction $\mathbf{x}$ is defined as $\mathcal{R}_H(\mathbf{x};k)=\sum_i r_H (x_i;k)$, where:
\begin{equation} \label{eq:huber}
\begin{aligned}
    r_H(x_i;k) =
    \begin{cases}
    \frac{1}{2}x_i^2, & |x_i| < k \\
    k|x_i| -\frac{1}{2}k^2 & |x_i| \geq k
    \end{cases}
\end{aligned}
\end{equation}
and $x_i$ are the elements of $\mathbf{x}$. It aims to take advantage of the $L^1$ norm robustness to outliers, but suggests better convergence replacing $L^1$ by $L^2$ for points near its optimum. The Charbonnier regularizer is defined as $\mathcal{R}_C(\mathbf{x})=\sum_i r_C(x_i)$, where:
\begin{equation} \label{eq:charb}
    r_C(x_i)=\sqrt{x_i^2+\epsilon^2}
\end{equation}
We show that our iterative approach is superior to these $L^1$ relaxations, achieving improved results and faster convergence.



\subsection{PC Signal Prediction} \label{sec:synthtetic}
We randomly generate a 1D Piece-wise Constant (PC) signal $y(x),x\in[a,b]$ and sample it uniformly to generate our training data $\{(x_i,y_i)\}_{i=0}^{N-1}$ where $y_i=y(x_i)$. We over-fit a small NN model, consisting of 4 FC layers to predict the full signal given its samples, using GD iterations (see figure \ref{fig:toy_sigs}). We define our data term:
\begin{table}
\begin{center}
\begin{tabular}{l c c} 
    \toprule
    \textbf{Method} & \textbf{Grad Norm} & \textbf{Error} \\
    \midrule
    Standard TV     & $3.86\times10^{-5}$   & $2.29\times10^{-2}$\\
    Charbonnier \cite{charbonnier1997deterministic}    & $4.00\times10^{-9}$   & $2.07\times10^{-2}$\\
    Huber \cite{huber1964robust}           & $3.11\times10^{-9}$   & $1.84\times10^{-2}$\\    
    \textbf{Unrolled Cost}   & $\mathbf{6.38\times10^{-10}}$   & $\mathbf{1.65\times10^{-2}}$\\
    \bottomrule
\end{tabular}
\end{center}
\caption{\textbf{PC signal prediction - measured results.} Presented are the measured data losses and prediction errors of models trained using our unrolled cost, standard TV regularization and TV relaxations. Training using our unrolled cost decreases TV prediction error by 28.2\%.}
\label{ta:toy}
\end{table} \begin{equation}
    \mathbf{\Phi}\left( \mathbf{f},\mathcal{I} \right)=\frac{1}{N}\sum_{i=0}^{N-1} \left(f_i - y_i\right)^2
\label{eq:toy_data}
\end{equation}
where $\{f_i\}_{i=0}^{N-1}$ are the corresponding samples of the predicted output. We train our model using our unrolled cost and compare to standard TV, Huber \cite{huber1964robust} and Charbonnier \nolinebreak \cite{charbonnier1997deterministic} baselines, measuring the prediction error as the mean absolute difference between target and predicted signals. We perform independent hyperparameter optimization prior to comparing both methods. 
We present here results of one scenario, further results as well as implementation details are available in the supplementary material. 

\begin{figure*}
\begin{center}
\includegraphics[width=1\linewidth]{figures/files/ours_vs_sota_2.png}
   \caption{\textbf{Qualitative benchmark results.}
   We compare qualitative flow benchmark results of ours, the ARFlow \cite{liu2020learning} and UFlow \cite{jonschkowski2020matters} baselines. Both ARFlow and UFlow are methods adopting a PWC-Net \cite{Sun2018PWC-Net} based backbone, reporting the best results on the MPI Sintel \cite{Butler:ECCV:2012} and KITTI 2015 \cite{Menze2015CVPR} benchmarks, respectively. 
   We find adapting our unrolled cost to a PWC-Net based backbone outperforms both baselines, particularly at the motion boundaries.}
\label{fig:l1vsunroll}
\end{center}
\end{figure*} 
\subsubsection{Prediction Error}
We compare the signals predicted by our models. Prediction errors are given in table \ref{ta:toy}.
The target, its samples and predicted signals are shown in figure \ref{fig:toy_sigs}. We find that training using our unrolled cost decreases TV prediction error by 28.2\%, enabling a smoother but edge preserving solution (see fig. \ref{fig:toy_sigs}).

\subsubsection{Convergence}
We study the convergence of all methods. Figure \ref{fig:toy_graphs} displays prediction errors and gradient norms recorded during training, as well as gradients recorded at selected points (other points may be viewed in the supplementary material). We find our method not only achieves better results, but it also converges nearly two times faster than TV. Further inspecting the gradients generated during training reveals that gradients of the TV suffer from severe oscillations as a result of its non-differentiabilty at zero. These oscillations cause the measured gradient norms to increase rather than decrease, suggesting an unstable optimum. While $L^1$ relaxation baselines feature improved results and convergence compared to TV, our method achieves the best results and converges the fastest with its gradients smoothly decreasing to zero.

\subsection{Unsupervised Optical Flow} \label{sec:of}
We next test our proposed unrolled cost on the real-world unsupervised optical flow problem. We introduce our unrolled cost to the recently published ARFlow \cite{liu2020learning} baseline, rigorously following their proposed training scheme and model architecture, yet replacing their used TV regularization with our method. Optical flow is defined as a mapping $\mathbf{F}:\mathbb{R}^2 \mapsto \mathbb{R}^2$, which aligns pixels given a pair of images $I_1,I_2$. In the unsupervised optical flow setting, our data term takes the form:
\begin{equation}
\begin{aligned}
    \mathbf{\Phi}\left( \mathbf{F},\mathcal{I} \right) = \mathbf{\Phi}\left( I_1,\hat{I}_2^{\mathbf{F}} \right) \label{eq:data_flow}
\end{aligned}
\end{equation}
i.e. $\mathbf{\Phi}$ is a differetiable photometric loss, measuring the consistency of a reference image and a target image warped using $\mathbf{F}$. In order to remain consistent with the ARFlow baseline, we use masked TV regularization, substituting $\mathbf{C=W\odot\nabla F}$ into equation (\ref{eq:loss_f}), where $\mathbf{W}$ is a deterministic importance matrix, aiming to increase the penalty on the boundaries, defined: 
\begin{equation} \label{eq:mask}
\begin{aligned}
    \mathbf{W}&=\exp\left\{-\alpha|\nabla I_1|\right\} \\
    &=\exp\left\{-\alpha\left[\left|\frac{\partial  I_1}{\partial x}\right|,\left|\frac{\partial I_1}{\partial y} \right|\right]\right\}
\end{aligned}
\end{equation}
and $\alpha$ is an hyperparameter controlling the masking operation. A block diagram of our method for the unsupervised optical flow setting is shown in figure \ref{fig:unrol_arch}.

\begin{table*}
\begin{center}
\begin{tabular*}{.9\linewidth}{@{\extracolsep{\fill}}l | c c c | c c c | c c c}
    \toprule
\multirow{2}{*}{\textbf{Method}} & \multicolumn{3}{c}{\textbf{\underline{Sintel Clean}}} & \multicolumn{3}{c}{\textbf{\underline{Sintel Final}}} & \multicolumn{3}{c}{\textbf{\underline{KITTI 2015}}} \\
    & \textbf{\emph{Occ.}} & \emph{Noc.} & \textbf{\emph{All}} & \textbf{\emph{Occ.}} & \emph{Noc.} & \textbf{\emph{All}} & \textbf{\emph{Occ.}} & \emph{Noc.} & \textbf{\emph{All}} \\
    \midrule
    Standard TV                                     & 8.77 & 1.27 & 2.29 & 9.97 & 2.19 & 3.22 & 6.51 & 2.16 & 2.91\\
    Charbonnier \cite{charbonnier1997deterministic} & 8.96 & 1.31 & 2.34 & 9.98 & 2.20 & 3.22 & 6.23 & 2.13 & 2.89\\
    Huber \cite{huber1964robust}                    & 8.80 & 1.26 & 2.27 & 10.10 & 2.22 & 3.27 & 6.60 & 2.11 & 2.93 \\
    \textbf{Unrolled Cost (Ours)}  & \textbf{7.91} & 1.28 & \textbf{2.17} & \textbf{9.16} & 2.13 & \textbf{3.06} & \textbf{5.48} & 2.14 & \textbf{2.82}\\
    \bottomrule
\end{tabular*}
\end{center}
\caption{\textbf{Unrolled cost vs. TV relaxations - unsupervised optical flow.} Our method outperforms all tested $L^1$ relaxation baselines. Furthermore, our method decreases the TV EPE averaged over the occluded regions, which are highly affected by the smoothness constraint, by up to 15.82\%, enabling the detection of sharper motion boundaries without any modification to the model architecture or complexity.
}
\label{ta:l1vsunrolled}
\end{table*} 
\subsubsection{Model}
Following the ARFlow \cite{liu2020learning} baseline, we use a PWC-Net based model consisting of a 7 level multi-resolution pyramid. The model is trained end-to-end in a fully unsupervised manner. The photometric loss term is evaluated on non-occluded regions only, thus occluded regions are optimized using only the smoothness regularization. The training scheme consists of both pre-training and finetuning. During finetuning, self-supervision is acquired through appearance, spatial and occlusion pseudo-random deformations, as elaborated next. Further implementation details are available in the supplementary material.

\paragraph{Self-supervision from Augmentations}
As carried by ARFlow, self-supervision from pseudo-random augmentations is applied during finetunine. Image and corresponding flow augmentations, $\mathcal{T}^{\text{img}}:I_k \mapsto \tilde{I}_k$ and $\mathcal{T}^{\text{flow}}: \mathbf{F \mapsto \tilde{F}}$ respectively, perform a combination of appearance, spatial and occlusion pseudo-random deformations. An initial flow estimate $\mathbf{F}_1$ is computed given a pair of images $I_1,I_2$. The image pair then undergoes an image augmentation $\mathcal{T}^{\text{img}}$ resulting in a deformed pair of images $\tilde{I}_1,\tilde{I}_2$. A second flow estimate $\mathbf{F}_2$ is generated next over the deformed images $\tilde{I}_1,\tilde{I}_2$. Finally, we acquire self-supervision obtaining $\mathbf{\tilde{F}}_1$ through applying the flow augmentation $\mathcal{T}^{\text{flow}}$ over $\mathbf{F}_1$ and measuring the error $\mathbf{F}_2-\mathbf{\tilde{F}}_1$.








\subsubsection{Datasets} \label{datasets}
We evaluate our method on well-known optical flow benchmarks: the synthetic MPI Sintel \cite{Butler:ECCV:2012} and real-world autonomous driving KITTI 2015 \cite{Menze2015CVPR}. 

We follow the ARFlow \cite{liu2020learning} training scheme. For the MPI Sintel benchmark, we first pretrain our model on the raw Sintel movie. $12,466$ image pairs are extracted and split according to scene shots. We then finetune our model on the official training set, consisting of $1,041$ image pairs, each rendered with $2$ levels of difficulty. 
For the KITTI 2015 benchmark, we first pretrain our model on the KITTI raw dataset \cite{Geiger2013IJRR}, which consists of real road scenes captured by a car-mounted stereo camera rig. 
We then use the KITTI 2015 multi-view extension for finetuning. We exclude frames related to validation from our training set, i.e. use only frames below 9 or above 12 in each scene. Further details regarding the train-test data splitting used in our experiments are given in the supplementary.






\subsubsection{Flow in Occluded Regions}
Occluded pixels, i.e. pixels with no correspondence, are highly correlated with motion boundaries. Although crucial to many CV applications, performance on the occluded regions minorly affects the errors averaged over full images. 
We test our method particularly on occluded pixels as they are mostly affected by the gradients of the smoothness constraint during training. As official benchmark results don't report errors in occluded regions, we validate our models on the available official training data using train-test splitting (see supplementary). 
Measured validation AEPE rates of our unrolled cost, as well as the standard TV, Huber \cite{huber1964robust} and Charbonnier \cite{charbonnier1997deterministic} baselines are given in table \ref{ta:l1vsunrolled}. Our method outperforms all tested baselines. As expected, modifying the smoothness regularizer has little effect on the performance in the non-occluded regions (\emph{Noc.}). However, measuring the performance on the occluded regions, our method reduces TV EPE rates by up to 15.82\% enabling the detection of sharper motion boundaries with no added complexity.
Qualitative flow visualizations are displayed in figures \ref{fig:teaser} and \ref{fig:l1vsunroll}.

\begin{table*}
\begin{center}
\begin{tabular*}{.8\linewidth}{@{\extracolsep{\fill}}l | c c | c c | c c}
    \toprule
    \multirow{2}{*}{\textbf{Method}} & \multicolumn{2}{c}{\textbf{\underline{Sintel Clean}}}& \multicolumn{2}{c}{\textbf{\underline{Sintel Final}}}  &
    \multicolumn{2}{c}{\textbf{\underline{KITTI 2015}}}\\
    &\emph{Train} & \emph{Test} & \emph{Train} & \emph{Test} & \emph{Train} & \emph{Test (F1)} \\
    \midrule
    CoT-AMFlow $^\star$ \cite{wang2020cot}              & - & 3.96 & - & 5.14 & - & 10.34\% \\
    STFlow $^\star$ \cite{tian2020unsupervised}         & (2.91) & 6.12 & (3.59) & 6.63 & 3.56 & 13.83\% \\
    UPFlow $^\star$ \cite{luo2021upflow}                & (2.33) & 4.68 & (2.67) & 5.32 & 2.45 & 9.38\% \\
    SMURF $^\star$ \cite{stone2021smurf}                & (1.71) & 3.15 & (2.58) & 4.18 & (2.00) & 6.83\% \\
    \midrule
    DDFlow \cite{liu2019ddflow}                         & (2.92) & 6.18 & (3.98) & 7.40 & 5.72 & 14.29\% \\
    UFlow-train \cite{jonschkowski2020matters}          & (\textbf{2.50}) & 5.21 & (\textbf{3.39}) & 6.50 & (\textbf{2.71}) & 11.13\% \\
    SimFlow \cite{im2020unsupervised}                   & (2.86) & 5.92 & (3.57) & 6.92 & 5.19 & 13.38\% \\
    ARFlow \cite{liu2020learning}                       & (2.79) & 4.78 & (3.73) & 5.89 & 2.85 & 11.80\% \\
    \textbf{UnrolledCost (Ours)}                        & (2.75) & \textbf{4.69} & (3.61) & \textbf{5.80} & 2.87 & \textbf{10.81\%} \\
    \bottomrule
\end{tabular*}
\end{center}
\caption{\textbf{MPI Sintel \& KITTI 2015 official unsupervised optical flow benchmark results.} We report AEPE and F1 rates for recently published unsupervised methods on both the MPI Sintel \cite{Butler:ECCV:2012} and KITTI 2015 \cite{Menze2015CVPR} optical flow benchmarks. Methods featuring a different backbone are marked with $\star$. Brackets "()" indicate results of models trained using their validation set. Best methods in each category are in bold. Missing results are marked as "-". 
By adapting our unrolled cost to the ARFlow \cite{liu2020learning} baseline we not only achieve improved results, but also report the best results for a PWC-Net based backbone on both benchmarks at the time of submission. Moreover, we find our method highly effective on the motion boundaries, as shown in the qualitative results (figures \ref{fig:teaser},\ref{fig:l1vsunroll}).
}
\label{ta:sota_all}
\end{table*}

 
\subsubsection{Comparison with State of the Art}
We compare our trained model with recently published unsupervised methods on the MPI Sintel and KITTI 2015 optical flow benchmarks in table \ref{ta:sota_all}. The used measures are the Average End-Point-Error (AEPE) and F1 score, measuring outliers percentage (error $>$ 3px). 
Our method managed to improve the reported results of the ARFlow \cite{liu2020learning} baseline on both MPI Sintel and the KITTI 2015 unsupervised optical flow benchmarks, particularly at the motion boundaries, as is highly visible in figures \ref{fig:l1vsunroll} and \ref{fig:teaser}. Moreover, our method reports the best results using a PWC-Net based backbone on dual images at the time of submission.





\subsubsection{Number of Cost Function Update Steps}
\begin{table}
\begin{center}
\begin{tabular}{l c c} 
    \toprule
    \textbf{$\#$ Update Steps} & \textbf{\underline{Sintel Clean}} & \textbf{\underline{Sintel Final}} \\
    \midrule
    $T=1$               & 2.43   & 3.25 \\
    $T=2$               & 2.35   & 3.19 \\
    $\mathbf{T=4}$      & \textbf{2.35}   & \textbf{3.16} \\
    \bottomrule
\end{tabular}
\end{center}
\caption{\textbf{Number of cost function update steps.} 
    Finetuning using several $T$ values, we find that increasing the number of update steps brings a performance boost.
}
\label{ta:admm_iters}
\end{table} \begin{table}
\begin{center}
\begin{tabular}{l c c} 
    \toprule
    \textbf{Method} & \textbf{Memory} & \textbf{Runtime} \\
    \midrule
    Standard TV                 & 4.326Gi   & 51ms \\
    \textbf{Unrolled Cost (Ours)}      & \textbf{4.285Gi}   & 51ms \\
    \bottomrule
\end{tabular}
\end{center}
\caption{\textbf{Computational complexity.} 
    Our method consumes slightly less memory during training and preserves runtime, hence 
    replacing TV regularization with our unrolled cost enables improved results with no added complexity.
}
\label{ta:comp}
\end{table} \begin{figure}
\begin{center}
\includegraphics[width=1\linewidth]{figures/files/ablations_T_4.png}
\end{center}
   \caption{\textbf{Qualitative example - number of update steps.}
   We find increasing the number of update steps improves the ability of our model to capture finer flow details.}
\label{fig:ab_T}
\end{figure}
 
We study the effective number of cost function update steps $T$ required for training. We use a model which has been pretrained on the MPI Sintel raw movie and set various $T$ values during finetuning. Several $T$ values are compared in table \ref{ta:admm_iters} and a qualitative example is shown in figure \ref{fig:ab_T}. We find  that increasing $T$ brings a performance boost, enabling our flow estimator to capture finer motion details. 
However, further investigating the convergence of the cost function update steps during training shows that convergence is achieved at $T=2$ making the gain for setting setting $T>2$ relatively small. Additional study regarding update steps convergence is given in the supplementary material.
Substituting $T=2$ in eq. (\ref{eq:loss_f}), we conclude that our effective TV regularization proxy consists of the $L^2$ norm of the spatial gradients, as well as one update step.


\subsubsection{Computational Complexity}
As our method deals with model training alone, it does not affects inference. We compare both memory consumption and training time of our method to the ARFlow \cite{liu2020learning} baseline in table \ref{ta:comp}. All measurements are carried using a batch size of 4 image pairs and an image size of $384\times832$ over an RTX2080 GPU. Our method does not increase model size, as no learned parameters are added. Furthermore, as we set our unrolled cost unit to operate at flow original resolution, which is 1/4 image resolution, our method consumes slightly less memory and preserves training time. 
 \section{Conclusions}
We introduced the concept of Cost Unrolling, shifting algorithm unrolling to the cost function, while preserving model architecture. Our method enables improved training of a TV regularized model as a result of more accurate gradients, thanks to its differentiability around its optimum, preserving complexity.
We have demonstrated the effectiveness of our method in a synthetic signal prediction problem as well as real-world unsupervised optical flow. Our unrolled cost achieved superior results in all tested scenarios.
We believe that the proposed framework can be applied on top of other model architectures for boosting their results next to non-differentiable optimum solutions.

 {\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
