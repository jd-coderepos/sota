

\documentclass[journal,twoside,web]{IEEEtran}
\usepackage{url}
















\usepackage[caption=false,font=footnotesize,labelfont=sf,textfont=sf]{subfig}
\let\proof\relax
\let\endproof\relax
\usepackage{graphicx}
\usepackage{titling}
\usepackage{epsfig}
\usepackage{amsmath,siunitx}
\usepackage{units}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{booktabs}
\usepackage{setspace}
\usepackage{nth}
\usepackage{xfrac}
\usepackage{dblfloatfix}
\usepackage{lipsum}\usepackage{multicol}\usepackage[export]{adjustbox}
\usepackage{textcomp}
\usepackage{graphicx}  \usepackage{booktabs}


\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 \renewcommand{\textrightarrow}{}
\usepackage{pdfpages}
\newcommand{\subf}[2]{{\small\begin{tabular}[t]{@{}c@{}}
  #1\\#2
  \end{tabular}}}
\newcommand{\highlight}[1]{\colorbox{yellow}{#1}}


\urlstyle{rm}
\def\UrlFont{\rm}

\usepackage{amsthm}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{soul}
 
\newtheorem{lemma}{Lemma}

\makeatletter



\def\rvz{{\mathbf{z}}}
\def\rvx{\mathbf{x}}
\def\rvl{{\rm l}}
\def\sR{{\mathbb{R}}}
\renewcommand{\algorithmicforall}{\textbf{for each}}

\usepackage{algcompatible}
\usepackage[noadjust]{cite}
\renewcommand{\citepunct}{,\penalty\citepunctpenalty\,}
\renewcommand{\citedash}{--}

\newcommand\blfootnote[1]{\begingroup
  \renewcommand\thefootnote{}\footnote{#1}\addtocounter{footnote}{-1}\endgroup
}
\DeclareMathOperator*{\argminB}{argmin} 

\renewcommand\topfraction{0.85}
\renewcommand\bottomfraction{0.85}
\renewcommand\textfraction{0.1}
\renewcommand\floatpagefraction{0.85}

\newcommand{\var}[1]{\text{\texttt{#1}}}
\newcommand{\func}[1]{\text{\textsl{#1}}}

\makeatletter
\newcounter{phase}[algorithm]
\newlength{\phaserulewidth}
\newcommand{\setphaserulewidth}{\setlength{\phaserulewidth}}
\newcommand{\phase}[1]{\vspace{-1.9ex}
\Statex\leavevmode\llap{\rule{\dimexpr\labelwidth+\labelsep}{\phaserulewidth}}\rule{\linewidth}{\phaserulewidth}
  \Statex\strut\refstepcounter{phase}\textbf{#1}\vspace{-1.9ex}\Statex\leavevmode\llap{\rule{\dimexpr\labelwidth+\labelsep}{\phaserulewidth}}\rule{\linewidth}{\phaserulewidth}}
\makeatother

\setphaserulewidth{.35pt}

\algrenewcommand\algorithmicindent{0.9em}
\usepackage{setspace}

\newcommand{\cc}{\textcolor{black}}
\newcommand{\bb}{\textcolor{black}}

\usepackage{array}

\usepackage{url}

\begin{document}
\title{Target-Independent Domain Adaptation for WBC Classification using Generative Latent Search}
 \author{
Prashant Pandey, Prathosh AP, Vinay Kyatham, Deepak Mishra and 
Tathagato Rai Dastidar
}
\date{}
\markboth{IEEE Transactions on Medical Imaging}{Prashant \MakeLowercase{\textit{et al.}}: Target-independent Domain Adaptation for WBC Classification using Generative Latent Search}
\maketitle


\setlength{\dbltextfloatsep}{0.3cm}
\begin{abstract}

Automating the classification of camera-obtained microscopic images of White Blood Cells (WBCs) and related cell subtypes has assumed importance since it aids the laborious manual process of review and diagnosis. Several State-Of-The-Art (SOTA) methods developed using Deep Convolutional Neural Networks suffer from the problem of domain shift - severe performance degradation  when they are tested on data (target) obtained in a setting different from that of the training (source). The change in the target data might be caused by factors such as differences in camera/microscope types, lenses, lighting-conditions etc. This problem can potentially be solved using Unsupervised Domain Adaptation (UDA) techniques albeit standard algorithms presuppose the existence of a sufficient amount of unlabelled target data which is not always the case with medical images. In this paper, we propose a method for UDA that is devoid of the need for target data.
Given a test image from the target data, we obtain its `closest-clone' from the source data that is used as a proxy in the classifier. We prove the existence of such a clone given that infinite number of data points can be sampled from the source distribution.
We propose a method in which a latent-variable generative model based on variational inference is used to simultaneously sample and find the `closest-clone' from the source distribution through an optimization procedure in the latent space. \bb{We demonstrate the efficacy of the proposed method over several SOTA UDA methods for WBC classification on datasets captured using different imaging modalities under multiple settings.}
\blfootnote
{Copyright (c) 2019 IEEE. Personal use of this material is permitted. However, permission to use this material for any other purposes must be obtained from the IEEE by sending a request to pubs-permissions@ieee.org. 

Prashant Pandey and Prathosh AP are with Department of Electrical Engineering, Indian Institute of Technology Delhi, New Delhi 110016, India. Deepak Mishra is with Department of Computer Science and Engineering, Indian Institute of Technology Jodhpur, Rajasthan 342037, India. Vinay Kyatham and Tathagato Rai Dastidar are with SigTuple Technologies Pvt. Ltd., Bangalore 560102, India. Email: getprashant57@gmail.com, prathoshap@iitd.ac.in, dmishra@iitj.ac.in, trd@sigtuple.com, vinay.k@sigtuple.com. \bb{The code for our implementation is available at https://github.com/prinshul/WBC-Classification-UDA.}
}


\end{abstract}

\begin{IEEEkeywords} 
WBC, Microscopic imaging, Unsupervised domain adaptation, Generative models, VAE.
\end{IEEEkeywords}
\IEEEpeerreviewmaketitle
\section{Introduction}
\subsection{Background}
\IEEEPARstart{M}{icroscopic} review of Peripheral Blood Smear (PBS) slides by clinical pathologists is considered as the gold standard for detection of various disorders~\cite{bloodbook}. {This requires manual counting and classification of various types of cells, including White Blood Cells} (WBCs or leukocytes) and analysing their morphological characteristics in PBS slides.
The presence, absence, or relative counts of these cells help in the diagnosis of several types of diseases, including different forms of blood cancer, anaemia, and presence of parasites like in malaria.
This process of manual review is both laborious and error prone.
In addition, due to variations in stain, smearing process, the differentiation between various subclasses of cells is often blurry.
It takes significant expertise and experience to correctly classify all types of cells.
Lack of qualified medical professionals, especially in non-urban areas of developing countries, accentuates the problem.
Furthermore, the misdiagnosis, often caused by lack of adequate time to examine a slide thoroughly, can even lead to fatalities.
Thus, automating and standardising this process is a pressing need.

Several attempts have been made to automate some of these manual processes using methods ranging from classical computer vision~\cite{lee2013performance,young1972classification,bikhet2000segmentation} to image cytometry~\cite{hagwood2011evaluation, lippeveld2019classification}. While classical vision techniques suffer from issues like poor-generalization, image cytometry is limited by its operational speed and inability to engineer complex features~\cite{chen2016deep}. 
An alternative is to harness the power of Deep Convolutional Neural Networks (CNNs) in addressing some of these issues~\cite{qin2018fine}. In SC-CNN~\cite{sirinukunwattana2016locality}, a weighted sum of multiple classifiers is used to predict the class label of cell nuclei detected with a Spatially Constrained CNN. In \cite{mahmood2019deep}, a Conditional Generative Adversarial Network (cGAN)~\cite{mirza2014conditional} is used for nuclei segmentation, a fundamental task for cell classification. MGCNN~\cite{huang2019blood} is a White Blood Cells classification framework that combines modulated Gabor wavelet~\cite{lee1996image} and deep CNN kernels. A few commercial products too have been built utilizing some of these techniques. CellaVision~\cite{cellavision}, Shonit~\cite{shonit}, etc., automate the counting and classification of leukocytes and other blood cells. These systems consist of an automated microscope equipped with a digital camera, which captures the images of a biological sample on a glass slide. A software based analysis system, built using CNN models, is then used to localise and classify different types of cells in the sample.

\subsection{Motivation and Problem setting}

Even though the aforementioned models and systems are effective in their own ways, they suffer from certain issues that may limit their utility. For instance, Deep CNN models used for microscopic image classification are typically trained using proprietary datasets. These datasets tend to be homogeneous in terms of the capture device -- microscopes, lens and cameras used. This homogeneity and limited number of images in the training dataset cause the models trained on them to over-fit on specific characteristics of the image capturing device. As a result, when images captured with a different device or camera are presented to these models, they often wrongly classify new images, even though the trained human observers will have no difficulty in classification (images shown in Figure \ref{fig:wbctypes}). Hence, as the image capturing device changes, these models fail to adapt to the new input data distribution. This is known as the domain shift problem. Domain shift also occurs when the underlying imaging modality itself changes. For instance, a \cc{deep learning} model trained on Flow Cytometry images \cite{lippeveld2019classification} will not readily generalize for microscopic PBS images even though both capture WBCs. The problem of domain shift exists not only for medical images, but for any \cc{deep learning} system trained with single image source \cite{tzeng2017adversarial}.
\par A natural solution to this problem is to (re)-train the model with large amount of data obtained from the new device. However, generating sufficient quantity of annotated medical data is a time consuming and costly process. In addition, bottlenecks such as regulatory clearances, cause a large development cycle and delay in building such systems. We consider one such problem in this paper, where performance of CNNs trained on a dataset from a single source camera for automatic classification of images of WBCs taken from PBS, degrade when tested on unseen target dataset collected from different cameras. This falls within the ambit of a well-known computer vision problem known as \cc{Unsupervised Domain Adaptation} (UDA). \bb{However, almost all the SOTA methods on UDA~\cite{tzeng2017adversarial, ganin2017domain, sankaranarayanan2018generate} need access to the unlabelled target data during the time of training. While it may be feasible to obtain unlabelled target data, retraining of the UDA model for every newly emerging target domain might be infeasible, post their deployment in the field. Therefore, an unsupervised domain adaptation method that can operate without target data is desirable \cite{pandey2020skin}.} Motivated by these observations, in this paper we propose a UDA technique for WBC classification with following core contributions:

\begin{enumerate}
 
    \item We propose a UDA technique that does not require access to the target data \bb{during the time of training.}   
    \item We cast the problem of UDA as finding the `closest-clone' in the source domain for a given target image that is used as a proxy for the target image in the classifier trained on the source data.
    \item We theoretically prove the existence of the `closest-clone' given that infinite data points can be sampled from the source distribution. 
    \item We propose an optimization method over the latent space of variational inference based Deep generative model, to find the aforementioned clone through implicit sampling. 
    \item We demonstrate through extensive experimentation, the efficacy of the proposed method over several state-of-the-art UDA techniques for WBC classification \bb{on several datasets obtained using different imaging modalities with multiple domain shifts. We also validate our algorithm on the standard datasets used for UDA.}
\end{enumerate}















































\section{Related work}
\cc{Unsupervised Domain Adaptation} (UDA) refers to the design of techniques aimed at improving the performance of machine learning tasks such as classification and segmentation when the classifier is trained using labels only from a source domain and tested on data \cc{from related} but a shifted target domain. In this section, we present a review of the state-of-the-art UDA techniques based on their principle of operation and their use in the medical imaging community. 







\subsubsection{Adversarial-learning} These methods~\cite{tzeng2017adversarial, ganin2017domain, sankaranarayanan2018generate} learn domain-invariant representations using the principles of adversarial learning. 
ADDA~\cite{tzeng2017adversarial} employs a source network, pre-trained with labeled source data. Adversarial adaptation is performed by learning a target network such that a domain discriminator fails to predict the domain labels of the source and target features. During inference, the target images are mapped to the shared feature space by using the target network which are predicted by the source classifier. Generate To Adapt (GTA)~\cite{sankaranarayanan2018generate} learns domain invariant embeddings using a joint generative-discriminative set-up. During training, a feature extraction network outputs embeddings that are used by label prediction network for classification with a \cc{Generative Adversarial Network} (GAN) framework to generate realistic source images. DIRT-T~\cite{shu2018dirt} employs a Virtual Adversarial Domain Adaptation (VADA) model that pushes the decision boundaries away from regions of high data density by penalizing violation of the cluster assumption in the target domain. Transferable Adversarial Training (TAT)~\cite{liu2019transferable} generates transferable examples to fill in the gap between the source and target domains without distorting feature distributions. Domain Agnostic Learning (DAL)~\cite{peng2019domain} uses Deep Adversarial Disentangled Auto-Encoders (DADA) to disentangle domain-invariant features in the latent space by minimizing  the mutual information between domain-invariant and domain-specific features. The principles of adversarial feature learning has been used in \cite{mahmood2018unsupervised,gadermayr2019generative} to transform real images to a synthetic-like representation using unlabeled  synthetic endoscopy images and achieve stain independence. In~\cite{bertinetto2016fully}, a siamese architecture with adversarial training is used to improve the classification performance of target prostate histopathology whole-slide images. Zhang et al.~\cite{zhang2019noise} used adversarial learning for a noise adaptation task that allows a trained model to work effectively for medical images with different noise patterns.
\subsubsection{Target Reconstruction}These approaches for UDA reconstructs source or target samples as an auxiliary task that simultaneously focuses on creating a shared representation between the two domains while keeping the individual characteristics of each domain intact.
CyCADA~\cite{hoffman2017cycada} adapts between domains by aligning 
both generative and latent space
representations, with cycle and semantic consistency loss. PixelDA~\cite{bousmalis2017unsupervised} learns transformation in the pixel space from one domain to the other using task-specific and content–similarity losses. SBADA-GAN~\cite{russo2018source} maps source samples into the target domain and vice versa by imposing a class consistency loss to improve the quality of reconstructed images. I2I Adapt~\cite{murez2018image} is a framework that learns from the source domain and adapt to the target domain by extraction of domain agnostic features, domain specific reconstruction with cycle consistency losses. Tulder et al.~\cite{van2018learning} proposed a representation learning method that transforms data from different sources to a shared feature representation using per-feature normalization, a cross-modality based objective function. Goetz et al.~\cite{goetz2015dalsa} used domain adaptation to correct the sampling bias introduced with sparsely labeled MR images for tissue classification.
\subsubsection{Divergence Minimization} In these methods, source and target distributions are aligned by minimizing a divergence measure between the two distributions. Joint Adaptation Networks (JAN)~\cite{long2017deep} learns a transfer network by aligning the joint distributions of multiple domain-specific layers across domains based on a \cc{Joint Maximum Mean Discrepancy} (JMMD) criterion. Maximum Classifier Discrepancy (MCD)~\cite{saito2018maximum} aligns distributions of source and target by utilizing the task-specific decision boundaries. Task-specific classifiers are trained to detect the target samples that are far from the support of the source.
Contrastive Adaptation Network (CAN)~\cite{kang2019contrastive} estimates
the underlying label hypothesis of target samples through clustering and adapts the feature representations according to the  Contrastive Domain Discrepancy (CDD) metric. Pacheco et al.~\cite{pacheco2019unsupervised} addressed the discrepancies related to the stem cell differentiation process by minimizing a \cc{Maximum Mean Discrepancy} (MMD) based loss function in a Recurrent Neural Network (RNN) classifier.
\subsubsection{Domain Randomization}
\bb{Domain Randomization~\cite{tobin2017domain} (DR) is another class of methods related to UDA that are used to improve the generalization of classifiers. The idea is to reduce the domain shift by randomizing properties in the training environment (like source domain). Every data point in the source domain is perturbed randomly during training while assigning the same ground truth to the perturbed samples.  In methods such as~\cite{mahmood2018deep}, cinematically rendered source domain images are varied in color and texture.
For RGB images, such transformations can be obtained by varying hue, saturation, contrast and brightness.
In~\cite{toth2018training}, source images intensity is divided into multiple non-overlapping ranges. A random perturbation is added to the start/end pixel values by sampling from a Gaussian distribution. Finally, one of the following randomisation is applied to each range, a) shift the intensity values by adding a random value from a uniform distribution or b) transform the intensity values using cumulative distribution function of beta distribution or
c) simply invert the intensity range. \cite{zakharov2019deceptionnet} varies source images background color, add uniform noise, change the illumination and distort source images with different scaling factors.}

\section{Proposed Method}
\subsection{Motivation}
All the UDA methods mentioned in the previous section assumes that one has access to images from the target distribution. These images are either used to retrain the original classifier in a domain-invariant way \cite{tzeng2017adversarial,ganin2017domain,sankaranarayanan2018generate} or to align the target distribution to the source distribution \cite{hoffman2017cycada,bousmalis2017unsupervised,long2017deep,kang2019contrastive}. Also, in most of the methods \cite{tzeng2017adversarial,ganin2017domain,sankaranarayanan2018generate,kang2019contrastive}, the original classifier trained on the source data is altered, so that a new decision boundary is learned using the images from the target data in an unsupervised manner. However, in many practical situations, such as the current one, there would neither be access to the target data nor the scope to retrain the classifier. Further, a new unseen target domain may arise in the field which was not used during adaptation. \par We propose to address these issues in this paper by first assuming that the classifier learned on the source data (Oracle classifier) will perform well as long as the data comes from the source distribution. Subsequently, (i) we learn to sample from the source distribution and (ii) given an image from the target distribution, we find an image from the source distribution that is arbitrarily close (`closest-clone') to the given target image, under some distance metric. Finally, the target image is replaced with its `closest-clone' from the source distribution before its class is inferred by the Oracle classifier.

\setlength{\dbltextfloatsep}{0.2cm}
\begin{figure*}
\includegraphics[width=1.0\textwidth,height=.358\textwidth]{train_tmi1.pdf}
\caption{\cc{The architecture for the Variational Auto-Encoder in the proposed method (TIGDA). Edges of the input microscopic image is concatenated with the features from the decoder . The encoder and decoder parameters ,  are optimized with reconstruction loss , KL-divergence loss  and the perceptual loss . The perceptual model  outputs  layer features of VGG-16 (or ResNet-50) classifier trained on source data. A zero mean and unit variance isotropic Gaussian prior is imposed over the latent space .}}
\label{fig:vaetrain}
\end{figure*}
\subsection{Existence of closest source `clone' }
To begin with, we prove that given an image from the target distribution, there exists an arbitrarily close image in the source distribution (named as `closest-clone'), provided infinite data can be sampled from the source distribution \cite{cover1967nearest}. \par Let  and  denote the source and the target distributions, respectively. We assume that the the underlying random variable on which   and  are defined, forms a separable metric space  where  is some distance metric. Let  be i.i.d. points drawn from  and  be any point drawn from . The following lemma asserts that as , there exists a point in  that it arbitrarily close to , with probability one.

\begin{lemma}

If  is the point such that , then as ,\   converges to   with probability   \bb{(Refer supplementary material for proof).} 
\end{lemma} 

Lemma 1 guarantees that given an image from the target distribution, an image from the source distribution, that is arbitrarily close to the given target image can be found out given the following requirements are met:
 \begin{itemize}
    \item Given a few images from the source distribution , one can sample infinite images from it. 
    \item Given infinite samples from , it is possible to find the `closest-clone' (under ) in , to the target image .
 \end{itemize}
To satisfy the above requirements, in subsequent sections, we employ variational inference based sampling methods on the source distribution with which one can implicitly sample and find the `closest-clone' simultaneously. 
\subsection{Variational inference for source sampling}
In variational inference based generative models \cite{kingma2013auto}, it is assumed that the data or the observed variable (in this case images from ) is generated via a two step process: (i) sample from the distribution  of an unobserved or latent variable , (ii) given a data point from the latent variable, sample from the conditional distribution  to obtain the data. Owing to the fact that the parameters of the true latent prior  and data conditional  are unknown, and  the posterior  is intractable, a variational distribution,  is used to approximate the true posterior. With this, it can be shown that the log-likelihood of the observed data will decompose into two terms (Eq. \ref{elbo1}), an irreducible non-negative KL-divergence between   and   and the Evidence Lower Bound (ELBO) given by Eq. \ref{elbo}. 

 Here,  represents ELBO which is given by,


In Eq. \ref{elbo1}, the KL-term is irreducible and non-negative and thus,   serves as a lower bound on the data log-likelihood which is optimized. In deep generative model frameworks,  and  are parameterized using probabilistic encoder  (that outputs the parameters  and  of a distribution)  and decoder  neural networks with parameters  and  respectively, that maps the data space into latent space and vice-versa. Additionally,   is taken to be an arbitrary prior on  which is usually a  mean and unit variance Gaussian distribution. The first term in Eq. \ref{elbo} is approximated using a norm-based divergence metric between the input and the output of the decoder as below:

\cc{
}

Note that Eq. \ref{reconst} can be seen as `reconstruction' or `Auto-Encoding' of the data. Further, the second term in ELBO employs a variational approximation to the true posterior . Thus, the aforementioned method is famously referred to as the Variational Auto-Encoder (VAE) \cite{kingma2013auto}. For the current problem of interest, a VAE is trained using the images from the source distribution  and once trained,  the decoder network serves as a sampler for the source distribution using a two step process: (i) sample , (ii) sample   as the output of the decoder .

VAEs are know to produce blurred images in their conventional formulation with norm-based losses. To address this, we use the edge information (extracted using standard edge detectors) of the input image by passing it to the decoder via a skip connection, as shown in Figure~\ref{fig:vaetrain}. Rationale behind this is that unlike features such as colour and contrast, edges are in general invariant to the changes in camera characteristics. Edge information reduces the blurring due to the decoder as shown in Figure \ref{fig:ablation_imgs} and ablation studies in Table \ref{tab:ablation}. \par \bb{Further, we also incorporate the perceptual loss, which is known to enhance the generation quality of VAEs, along with the standard norm-based losses. Perceptual loss  between two images  and  is defined as the Euclidean distance between the representations or the features obtained under a pre-trained classifier model . Mathematically,

The idea behind  is that the distance metrics in a representational space learned by a classifier model trained on large scale data are better than on raw image space. This is shown to enhance image quality in several applications \cite{yang2019unsupervised}.} Figure \ref{fig:vaetrain} depicts the network diagram of the VAE on the source data with the proposed edge concatenation. 
\setlength{\textfloatsep}{0pt}

\setlength{\textfloatsep}{0pt}
\subsection{Finding `closest-clone' through Latent Search}
As mentioned in the previous sections, the objective is to simultaneously sample and search for the `closest-clone' in the source distribution, given a sample from target distribution.  Suppose a VAE has been trained on the source distribution , the decoder  of which outputs a `de-novo' image from  by taking a normally distributed latent variable as input. That is, 
\setlength{\textfloatsep}{0pt}


Our goal is to find the `closest-clone' under some distance metric , for any given image from the target distribution. Mathematically, given a , find  as follows:



Since  is computable and  is a neural network that outputs a sample from  as a function of the latent variable , finding  (Eq. \ref{obj}) can be cast an optimization problem over  with minimization of  as the objective:


The optimization problem is Eq. \ref{obj} can be solved using gradient descent based techniques on the decoder network   are the parameters of the decoder network trained only on the source images   with respect to . This implies that given any input image, the optimization problem in Eq. \ref{obj} will be solved to find its `closest-clone' in the source distribution which is used as a proxy in the original classifier trained only on . We call the iterative procedure of finding   through optimization using  as the Latent Search (LS).

Finally, inspired by the observations made  in~\cite{zhao2016loss,mishra2018ultrasound}, we propose
to use \cc{Structural Similarity Index} (SSIM) loss for  to conduct the Latent Search. Unlike norm-based losses, SSIM loss helps in preservation of structural information as compared to discrete pixel level information. SSIM  is defined in~\cite{wang2004image} using the three aspects of similarities, luminance , contrast  and structure  that are measured for a pair of images  as follows:\setlength{\textfloatsep}{0pt}

\setlength{\textfloatsep}{0pt}

\setlength{\textfloatsep}{0pt}

where 's denote sample means and 's denote variances.  and  are constants as defined in \cite{wang2004image}. With these, SSIM and the corresponding loss function , for a pair of images  are defined as: 
\setlength{\textfloatsep}{0pt}

where ,  and  are parameters used to adjust the relative importance of the three components.
\setlength{\textfloatsep}{0pt}

Since our method does not utilize target images and employs generative Latent Search, we call our method Target-Independent Generative Domain Adaptation (TIGDA). \bb{The target independence of our method refers to the fact that we do not use target data during training, unlike SOTA UDA methods.} The inference for TIGDA is shown in Figure \ref{fig:inf}.
\begin{figure}
\includegraphics[width=.48\textwidth,height=.297\textwidth]{inf_cond3.pdf}
\caption{Latent Search procedure during inference with TIGDA. The latent vector  is initialized with a random sample drawn from . Iterations over the latent space  are performed to minimize the Structural Similarity loss  between the input target image  and the predicted target image , which is the output of the trained decoder (blue dotted lines).  After convergence of  loss, the optimal latent vector , generates the `closest-clone'  which is used to predict the class of  using the classifier  trained on source samples.}
\label{fig:inf}
\end{figure}
\begin{figure*}
\includegraphics[width=1.0\textwidth,height=.272\textwidth]{classes_camera.pdf}
\caption{Samples of White Blood Cells and related microscopic images (categorized into 11 classes) taken from three different cameras A, B and C. (IG=Immature granulocytes, NRBC=Nucleated red blood cells, GP=Giant platelets, PC=Platelet clumps). It is to be noted that there are no visually distinctive features across cameras but it is easy for a human-pathologist to correctly classify despite camera changes. On the other hand, \cc{deep learning} models fail to generalize across cameras.}
\label{fig:wbctypes}
\end{figure*}
\bgroup
\def\arraystretch{1.3}
\begin{table*}[hbt!]
\caption{Number of White Blood Cells and related microscopic images for each subtype (class) captured with three different cameras A, B and C. (NE=Neutrophil, LY=Lymphocyte, MO=Monocyte, EO=Eosinophil, BA=Basophil, IG=Immature granulocytes, NRBC=Nucleated red blood cells, GP=Giant platelets, PC=Platelet clumps).}
\centering
\scalebox{0.96}{
  \begin{tabular}{c|ccccccccccc|c}
    \toprule
         \textbf{Camera} & \textbf{NE} & \textbf{LY} & \textbf{MO} & \textbf{EO} & \textbf{BA} & \textbf{IG} & \textbf{Atypical} & \textbf{NRBC} & \textbf{GP} & \textbf{PC} & \textbf{Artefact} & \textbf{\scalebox{1.4}{}}
        \\
    \midrule
    A&3,885&1,507&2,224&2,076&65&863&984&651&486&138&2,550&\scalebox{1.4}{}\\
    B&2,045&1,840&612&373&67&1,073&2,257&97&918&796&1,437&\scalebox{1.4}{}\\
    C&85&43&144&85&12&323&861&321&303&11&16&\scalebox{1.4}{}\\
    \bottomrule
  \end{tabular}
  }
  \label{table:datasets}
\end{table*}
\egroup




\section {Implementation Details}
\subsection{Training of the VAE}
The Encoder  and Decoder  network architectures for the VAE are shown in Figure \ref{fig:vaetrain}.
We use Sobel Edge operator for Edge concatenation. Edges of the input image are concatenated with the output of \textit{tanh} nonlinearity as shown in Figure \ref{fig:vaetrain}. The VAE is trained using (i) the Mean squared error reconstruction loss  between the real and VAE reconstructed images and (ii) the perceptual loss  for which the features are taken from the  layer of the VGG-16 ( layer) or RestNet-50 ( layer) classifier trained on source images for WBC classification task.  The hidden layers of Encoder and Decoder networks use Leaky ReLU and \textit{tanh} as activation functions with the dimensionality of the latent space being 64. VAE is trained using a standard gradient descent procedure with RMSprop optimizer.

\subsection{Inference through Latent Search}
Once the VAE is trained, given an image  from the target distribution,
the Latent Search algorithm searches for an optimal latent vector  that generates its `closest-clone'  from . The search is performed by minimizing the SSIM loss  between the input target image  and VAE reconstructed target image. The latent vector is optimized using a gradient-based optimization procedure, performed for  (a hyper-parameter) iterations over the latent space of the VAE for every target image. The gradient based optimization is implemented with Nesterov Accelerated Gradient method with a momentum of 0.5. Finally, the class for the input target image is assigned the same as the one given by the source classifier  on .  is a VGG-16 or RestNet-50 classifier  trained  on  source images.  Note that our algorithm solves an optimization problem before predicting class for every input target image. However, since it involves only a forward-pass through a trained neural network (decoder ), the time taken is only of the order of few milliseconds on standard CPUs. \bb{The complete algorithmic steps and the architectural details for TIGDA are given in the supplementary material.}

\section{Dataset details}
The datasets used in this study will be described in this section. Peripheral blood smear (PBS) consists primarily of three cell types -- RBC (Red Blood Cell or erythrocyte), WBC (White Blood Cell or leukocyte) and platelet (or thrombocyte). Each of these primary classes have subclasses. The subclasses of WBCs are: neutrophil, lymphocyte, monocyte, eosinophil, basophil, immature granulocytes and atypical/blast cells. Apart from these, there are other types of cells and artefacts which can have appearance similar to leukocytes.
These are -- nucleated red blood cell (NRBC), large platelets, platelet clumps, and stain artefacts \cite{bloodbook}. In the current study, we consider classification of 11 categories of which seven are subtypes of WBCs and rest four are NRBC, large platelets, platelet clumps, and stain artefacts (images shown in Figure~\ref{fig:wbctypes}). 
\begin{table*}
\caption{ Accuracy (mean  std\%) values for UDA tasks on WBC and related microscopic images captured with three different cameras A, B and C. X\textrightarrow Y indicates model trained on images from source Camera X and tested on images from target Camera Y. 
Results are reported as an average over five independent runs using various state-of-the-art UDA \bb{and Domain Randomization methods}. Note that while all UDA methods perform better than the source only model, TIGDA offers the best performance despite not using the target images.}
\begin{center}
\scalebox{0.74}{
\begin{tabular}{l|cccccc|ccccccc}
    \toprule


    \multicolumn{1}{c|}{} & \multicolumn{6}{c|}{ResNet-50} & \multicolumn{6}{c}{VGG-16}\\
    Models & {A\textrightarrow B} &  {A\textrightarrow C} & {B\textrightarrow A} & {B\textrightarrow C} & {C\textrightarrow A} & {C\textrightarrow B} & {A\textrightarrow B} &  {A\textrightarrow C} & {B\textrightarrow A} & {B\textrightarrow C} & {C\textrightarrow A} & {C\textrightarrow B} \\
    \midrule
    {Source Only}&42.70.5&51.30.4&35.80.6&46.20.2&22.80.6&26.90.4&37.40.5&47.60.4&31.20.3&40.10.5&17.60.6&22.70.2\\
    
   {DR1~\cite{mahmood2018deep}}&52.50.3&57.70.1&43.60.2&51.70.4&34.50.3&36.20.2&44.60.1&50.90.2&38.20.4&46.50.3&27.30.3&30.80.2\\
    
    {DR2~\cite{toth2018training}}&60.30.2&65.40.3&55.90.2&64.20.4&44.60.3&49.80.4&54.10.1&59.60.2&48.70.1&60.50.4&41.30.3&45.20.1\\
    
    {DR3~\cite{zakharov2019deceptionnet}}&50.40.2&53.40.4&40.50.2&49.80.3&29.50.3&32.70.4&41.80.3&47.50.2&35.90.1&42.10.2&23.60.2&28.30.3\\

    {ADDA~\cite{tzeng2017adversarial}}&43.50.1&52.70.2&37.30.1&48.10.5&24.90.4&29.10.5&39.30.2&50.10.3&33.60.4&43.30.2&19.80.4&25.20.5\\
   


   {GTA~\cite{sankaranarayanan2018generate}}&56.20.4&66.30.5&48.10.2&56.70.6&35.50.4&37.80.1&52.60.7&62.10.3&41.90.6&50.70.3&30.10.1&33.70.6\\
   


   {TAT~\cite{liu2019transferable}}&65.80.5&70.50.4&54.80.3&63.10.7&44.70.2&48.20.3&61.70.5&67.30.4&50.60.4&58.30.6&40.30.1&42.50.1\\
    

   {DIRT-T~\cite{shu2018dirt}}&55.70.5&65.10.6&49.20.2&55.40.3&34.20.3&37.50.4&53.10.8&61.90.7&40.70.5&50.30.5&31.30.4&32.90.7\\

    {DAL~\cite{peng2019domain}}&64.70.2&69.40.3&56.30.2&62.70.4&43.50.1&47.50.5&60.80.2&66.50.5&51.80.4&59.10.3&39.70.1&41.10.2\\
    


    {CyCADA~\cite{hoffman2017cycada}}&67.20.5&73.70.1&58.20.2&64.50.6&48.40.4&50.20.3&62.30.3&70.20.2&53.40.4&59.70.2&42.60.6&43.90.7\\
   


   {PixelDA~\cite{bousmalis2017unsupervised}}&65.90.2&71.80.7&59.10.8&66.20.5&47.80.4&50.60.5&61.50.3&68.40.4&54.60.7&58.80.6&41.30.6&42.50.4\\

    {SBADA-GAN~\cite{russo2018source}}&66.30.2&70.50.2&60.30.3&65.60.4&46.40.7&51.10.1&62.70.6&67.90.8&53.80.7&58.70.2&42.70.4&44.60.7\\
    

    {I2IAdapt~\cite{murez2018image}}&64.40.6&68.70.5&61.20.3&65.40.4&45.20.1&49.70.6&63.90.8&65.10.1&52.50.7&55.60.4&43.80.8&45.30.3\\
    

    {JAN~\cite{long2017deep}}&49.60.2&58.20.5&43.30.2&54.70.4&30.20.7&35.40.8&43.50.6&54.20.4&39.10.3&47.50.3&26.30.4&31.40.6\\

   {MCD~\cite{saito2018maximum}}&55.40.4&67.10.8&49.20.7&55.80.6&36.10.2&39.20.5&50.90.7&63.20.4&42.30.3&50.40.5&31.90.8&34.80.5\\
   

  {CAN~\cite{kang2019contrastive}}&67.80.4&71.30.5&63.40.5&65.40.3&47.30.2&51.20.4&61.90.8&68.10.3&54.60.6&59.30.4&40.90.2&45.70.8\\
  
  {TIGDA (Ours)}&\textbf{76.20.3}&\textbf{80.10.4}&\textbf{72.30.5}&\textbf{74.80.6}&\textbf{53.50.4}&\textbf{56.20.3}&\textbf{71.80.5}&\textbf{76.70.2}&\textbf{63.20.5}&\textbf{68.60.7}&\textbf{50.80.2}&\textbf{55.10.4}\\
    
    \bottomrule
\end{tabular}
}
\end{center}
\label{tab:comp}
\end{table*} 
\begin{figure*}
\centering
    \subfloat[ADDA]{\includegraphics[width=0.16\linewidth,height=0.14\linewidth]{adda_tsne1.png}
        \label{fig:adda}}
     \subfloat[GTA]{\includegraphics[width=0.16\linewidth,height=0.14\linewidth]{gta_tsne2.png}
      \label{fig:gta}}
         \subfloat[DIRT-T]{\includegraphics[width=0.16\linewidth,height=0.14\linewidth]{dirtt_tsne1.png}
      \label{fig:dirtt}}
    \subfloat[TAT]{\includegraphics[width=0.16\linewidth,height=0.14\linewidth]{tat_tsne1.png}
      \label{fig:tat}} 
     \subfloat[DAL]{\includegraphics[width=0.16\linewidth,height=0.14\linewidth]{dal_tsne1.png}
      \label{fig:dal}} 
     \subfloat[TIGDA]{\includegraphics[width=0.16\linewidth,height=0.14\linewidth]{tigda_tsne1.png}
      \label{fig:ours_tsne}} 
 \caption{\bb{t-SNE plots of features generated by ADDA~\cite{tzeng2017adversarial}, GTA~\cite{sankaranarayanan2018generate}, DIRT-T~\cite{shu2018dirt}, TAT~\cite{liu2019transferable}, DAL~\cite{peng2019domain} and TIGDA on domain adaptation task A\textrightarrow C. We used different markers and different colors to denote 11 categories. It is seen that TIGDA offers better clustering as compared to the rest.}}
\label{fig:tsne}
\end{figure*}\bb{Data used in our experiments comprises images from the PBS slides processed after complete de-identification to remove all the patient information, including age and gender. These were collected from two large clinical laboratories in Bangalore, India. The internal ethics committee of the respective laboratories approved the study. The samples were collected retrospectively 
without prospective patient recruitment.} \par\bb{The hardware consists of the following components,
(a) Optical system: Consists of an optical tube (40X or 100X Plan Achromat objective and 10X eyepiece) and Abbe Condenser with white LED source,
(b) Camera: The system is built such that either a mobile phone or a USB camera can be fitted on top of the eyepiece with a 3D printed attachment, aligning the optical axis of the tube/eyepiece with the camera,
(c) Hardware control: A small PCB designed to receive USB commands and drive motors and LED,
(d) XYZ slide stage: The XYZ platform is built using commercially available low-cost ball screws and stepper motors, along with some machined parts \cite{dastidar2020whole}.
The images used in this work are captured through 3 different cameras -- One cell phone make (iPhone 6s) and two brands of USB camera (from e-con systems~\cite{econ} and das-Cam \cite{dascam}). All cameras had resolution of at least 13MP with varying hardware and optical designs that induce the domain shift. For example, econ camera has an AR1335 CMOS image sensor and lens with 1/3.2'' optical format while das-Cam contains an OV13850 CMOS sensor with a lens of 1/3.06'' form factor.} \par Images are collected only from the `monolayer' region of the slides -- where the red blood cells are just touching each other. This is the area of the slide which is typically used for manual analysis~\cite{bloodbook}.
Slides prepared using varied staining types were used.
The images are of size approximately 13MP, with a spatial resolution of around 5.5 pixels per micron.
WBC and other similar looking cells (as described above) are localised in these images using a U-Net~\cite{unet} based technique described in~\cite{shonit}.
Each sample slides can potentially yield hundreds of unique WBC candidates. For annotation, we cropped  area around the WBCs identified by the extraction model.
These cells are then presented to three different certified medical professionals for annotating into different subtypes, using an in-house web based annotation tool.
There is usually a high degree (as high as 20\%) of inter-observer variability in the data annotation process.
Therefore, we use only those images where at least 2 out of 3 clinical pathologists agree on the class while the rest of the images are rejected. Table \ref{table:datasets} describes the summary of the datasets named as A, B and C corresponding to three cameras used. 
\section{Experiments and Results}
\subsection{Benchmarking Experiments}
\bb{In the first set of experiments, we benchmark performance of the baseline classifier with the following experiments: (a) Train and test on the same dataset type (A/B/C), (b) Train and test by combining images from all dataset types (A+B+C), (c) Train on one dataset and test on the other (all six combinations) with and without class balancing. The notation {X\textrightarrow Y} symbolizes training on a dataset X and testing on Y.}

\bb{Table \ref{table:sametasks} lists the results of experiment (a) which establishes an upper bound on the performance and (b) where it is seen that the performance degrades when all images from all three datasets are combined. This is due to the existence of domain shift between the datasets that makes learning difficult even with supervision. Moreover, combining datasets is not possible in the UDA setting where the labels are not known for the target data. Results of experiment (c) are shown in Table \ref{table:samesamples} where it is seen that the accuracy severely degrades when train and test sets are from different domains despite inducing an artificial class balance. The goal of UDA techniques is to improve the accuracies reported in Table \ref{table:samesamples}.} 
\begin{table}[hbt!]
\caption{\bb{Benchmarking A,B and C datasets using ResNet-50 classifier with different train and test sets. It is seen that combining all datasets makes learning difficult because of domain shift.}}
\centering
\scalebox{0.79}{
 \color{black} \begin{tabular}{ccccc}
    \toprule
         Measure & {A\textrightarrow A} & {B\textrightarrow B} & {C\textrightarrow C} & {(A+B+C)\textrightarrow (A+B+C)}
\\
    \midrule
     Train Acc. &98.60.1&99.30.2&100.00.0&98.70.2\\
    Test Acc. &95.20.2&94.00.3&92.50.1&84.40.3\\
\bottomrule
  \end{tabular}
  }
  \label{table:sametasks}
\end{table}
\setlength{\textfloatsep}{0pt}
\begin{table}[hbt!]
\caption{\bb{Accuracy on Resnet-50 classifiers for different Adaptation tasks. In the second row, all the three datasets are made to have same size by randomly subsampling the datasets}.}
\centering
\scalebox{0.67}{
  \color{black}\begin{tabular}{ccccccc}
    \toprule
         Measure & {A\textrightarrow B} &   {A\textrightarrow C} & {B\textrightarrow A}  & {B\textrightarrow C} & {C\textrightarrow A} & {C\textrightarrow B}
        \\
    \midrule
   W/o Balance&  42.70.5&51.30.4&35.80.6&46.20.2&22.80.6&26.90.4\\
   With Balance &  40.40.1&36.20.4&38.90.2&30.50.2&24.50.4&28.20.3\\
    
\bottomrule
  \end{tabular}
  }
  \label{table:samesamples}
\end{table}
\subsection{Baseline Experiments}
\bb{The first set of task is of classification across 11 classes with classifiers trained on one (source) dataset and tested on another (target) dataset. We report average classification accuracies with standard-deviation (averaged over five independent runs) with two backbone architectures for the source classifier: ResNet-50 and VGG-16.} \bb{For all the UDA tasks, the VAE is trained with the entire source data and tested on the entire target data.} \bb{Table \ref{tab:comp} compares the performance of TIGDA with 12 SOTA UDA baselines, along with the accuracy without any UDA (called Source Only). It is seen that although all the UDA methods improve upon the Source Only performance, TIGDA offers the best performance despite not using any data from the target distribution. The confusion matrix for a few methods is given in the Figure 2 of the Supplementary material.  We also compare with three Domain Randomization (DR) techniques, DR1 \cite{mahmood2018deep}, DR2 \cite{toth2018training} and DR3 \cite{zakharov2019deceptionnet}. While DR provides performance boost, they have poorer performance as compared to TIGDA. This is because DR methods typically work well when the unseen target is within the scope of the class of random perturbations that are made on the source} \bb{which is not the case always. In TIGDA on the other hand, every target image is made to resemble the source image through implicit sampling. Since VAE learns to sample from the entire source domain, the domain shift is implicitly reduced during inference without explicitly assuming any form for the shift.}
\bb{It is also observed that the performance of the classifier when trained and tested on single source domain (around 92-95\% for all the datasets) do not degrade with TIGDA.} 
\setlength{\textfloatsep}{0pt}
\subsubsection{t-SNE}
\bb{To further examine our hypothesis, in Figure \ref{fig:tsne} we depict the t-SNE~\cite{maaten2008visualizing} 
plots of features generated by adversarial based UDA methods (ADDA~\cite{tzeng2017adversarial}, GTA~\cite{sankaranarayanan2018generate}, DIRT-T~\cite{shu2018dirt}, TAT~\cite{liu2019transferable} and DAL~\cite{peng2019domain}) for the domain adaptation task A\textrightarrow C. For TIGDA, we plot the embeddings of the latent variable  obtained through the LS on the target images. It is seen that the representation generated by the LS of TIGDA is more separated compared to those generated by adversarial training based UDA methods.} \bb{A similar observation is made on the first two principal component plots of the latent representations (Please refer to Figure 1 in supplementary material).}
\setlength{\textfloatsep}{0pt}
\begin{figure}
\includegraphics[width=.48\textwidth,height=.20\textwidth]{discrepancy_based_adist.png}
\caption{-Distance (lower is better) of JAN~\cite{long2017deep}, MCD~\cite{saito2018maximum}, CAN~\cite{kang2019contrastive} and TIGDA.}
\label{fig:adist}
\end{figure}



\setlength{\textfloatsep}{0pt}
\begin{figure}
\centering
\includegraphics[width=0.48\textwidth,height=.350\textwidth]{recons_comp_2.pdf}
\caption{Translation of images from one domain (Camera B) to other (Camera A) using reconstruction based domain adaptation methods: PixelDA~\cite{bousmalis2017unsupervised}, I2IAdapt~\cite{murez2018image}, CyCADA~\cite{hoffman2017cycada}, SBADA-GAN~\cite{russo2018source}. In TIGDA, we depict the `closest-clones' of Camera B (target) images in the Camera A (source) domain. It is seen that TIGDA preserves the edges, perceptual quality and structural details in the generated clones. }
\label{fig:recons_comp}
\end{figure}
\setlength{\textfloatsep}{0pt}
\begin{figure}
\includegraphics[width=0.488\textwidth,height=.265\textwidth]{latent_transform_3.pdf}
\caption{Illustration of Latent Search in TIDGA. VAE reconstructs images prior to LS. The closest-clones obtained after every 200 iterations are shown. A transformation is observed from the target to the source domain as the LS progresses.}
\label{fig:transform}
\end{figure}
\setlength{\textfloatsep}{0pt}
\subsubsection{-Distance}
\bb{To ascertain the closeness of the `closest-clones' obtained through the LS, to the source distribution, we compute the -distance~\cite{ben2007analysis}, which is a measure of similarity between two probability distributions. Similar feature distributions will have lower -distance between them as compared to dissimilar feature distributions. -distance is given by  where  is the generalization error  of a linear SVM classifier trained to discriminate between the source and target domains.} \bb{Figure \ref{fig:adist} displays  for the four domain adaptation tasks with JAN~\cite{long2017deep} features, MCD~\cite{saito2018maximum} features and CAN~\cite{kang2019contrastive} features, respectively. In our case,  is measured between the latent vectors (produced by the Encoder of the VAE) of the source images and the latent vectors of the `closest-clones' for target images obtained from Latent Search. We  observe that  is smallest in our case as compared to other methods for all the tasks. This implies that the features obtained using TIGDA are transferable between the source and target domains, aiding better adaptation.}
\subsubsection{Qualitative examination}
\bb{To qualitatively examine the performance of the reconstruction-based methods, we plot the transformed target samples from (source) Camera B to (target) Camera A for different methods as shown in Figure \ref{fig:recons_comp}. It is seen that I2IAdapt~\cite{murez2018image} and SBADA-GAN~\cite{russo2018source} are not able to capture fine subtleties of partially visible White Blood Cells in microscopic images that results in poor performance. PixelDA~\cite{bousmalis2017unsupervised} and CyCADA~\cite{hoffman2017cycada} result in blurry images while TIGDA generated images are better where it is seen that the subtleties like edge information are well-preserved. In summary, we have demonstrated that TIGDA achieves better performance over the SOTA adversarial, divergence and reconstruction based UDA methods without any requirement for target images.}
\bb{\subsubsection{One-shot learning}
Even though TIGDA does not utilize the target data during training, target image is used for LS during inference. Therefore, we also compare TIGDA with SOTA one-shot learning techniques in Table \ref{tab:oneshot}. In one-shot learning methods, a single target image is used during training for adaptation. It is seen that TIGDA outperforms such techniques. This is because, in one-shot learning methods, the target image that is used for training is fixed which restricts the learnability. However in TIGDA, no target image is used during training but a fresh latent search is conducted on each input target image during inference.}
\setlength{\textfloatsep}{0pt}
\begin{table}
\caption{\bb{Comparison of TIGDA with One-shot learning methods.}}
\begin{center}
\color{black}\begin{tabular}{l|c|c}
    \toprule
    Method  & {A\textrightarrow B} &  {C\textrightarrow B} \\
    \midrule
    ProtoNet~\cite{chen2019closer}&61.90.1&49.60.3\\
     MatchingNet~\cite{chen2019closer}&57.60.2&43.70.1\\
     DAPN~\cite{zhao2020domain}&68.90.2&51.90.2\\
     DN4~\cite{li2019revisiting}&55.40.1&44.60.2\\
      FADA~\cite{motiian2017few}&60.60.3&45.90.3\\
      {TIGDA (Ours)}&76.20.3&56.20.3\\
\bottomrule
\end{tabular}
\end{center}
\label{tab:oneshot}
\end{table} 
\setlength{\textfloatsep}{0pt}
\subsection{Ablation studies}
\bb{To examine the contributions made by each of the proposed components, we conduct several ablation experiments} \bb{on TIGDA in this section.} 
\subsubsection{Effect of number of iterations on LS}
\bb{The inference of TIGDA involves a gradient-based optimization through the decoder network  to generate the `closest-clone' for a given target image. In Figure \ref{fig:transform},  we show the transformation of a few target images after every 200 iterations. It can be seen that as the number of iterations increase, the target images change their characteristics to move towards the source distribution.}
\setlength{\textfloatsep}{0pt}
\begin{figure}[hbt!]
\subfloat[Inference on camera C microscopic images when the model is trained on camera A images.]{\includegraphics[width=0.470\linewidth,height=0.41\linewidth]{plot_wbc_loss_1}
        \label{fig:alossacc}}
\hfill
    \hfill
\subfloat[Inference on camera C microscopic images when the model is trained on camera B images.]{\includegraphics[width=0.470\linewidth,height=0.41\linewidth]{plot_wbc_loss_2}
        \label{fig:blossacc}}
\caption{Performance of gradient-based Latent Search during inference on target microscopic images for two domain adaptation tasks using different objective functions; MSE=Mean Squared Error, MAE=Mean Absolute Error, SSIM=Structural Similarity Index. It is seen that the loss saturates around 500-600 iterations.}
\label{fig:lossacc}
\end{figure}
\bb{Quantitatively, we plot the accuracy as a function of number} of iterations in Figure \ref{fig:lossacc} where it is seen that it saturates around 500-600 iterations. We thus used 600 iterations in all the previous experiments in Table \ref{tab:comp}. 
\subsubsection{Effect of the Edge concatenation}
As described earlier, the edge-map of the input image is concatenated with one of the layers of decoder both while training and inference. Figure \ref{fig:wec} shows the quality of image generated after Latent Search when the model was trained without edge concatenation (wEc). It can be observed that edge information of the nucleus and surrounding cells is lost resulting in a blurry image. Further, the accuracy drops to 57.6\% if edge concatenation is removed from VAE for the task A\textrightarrow B as evident from Table \ref{tab:ablation}, whereas the accuracy for TIGDA is 76.2\% for the same task. Similarly, the accuracy drops to 60.3\% for the task B\textrightarrow C without edge concatenation while it is 74.8\% for TIGDA.
\setlength{\textfloatsep}{0pt}
\begin{figure}[hbt!]
\centering
\scalebox{.99}{
    \subfloat[real target]{\frame{\includegraphics[width=0.18\linewidth,height=0.18\linewidth]{abl_real}}
        \label{fig:ab_realt}}
        \hfill
         \hfill
         \subfloat[wEc]{\frame{\includegraphics[width=0.18\linewidth,height=0.18\linewidth]{abl_no_edge.png}}
      \label{fig:wec}}
      \hfill
       \hfill
         \subfloat[wPl]{\frame{\includegraphics[width=0.18\linewidth,height=0.18\linewidth]{abl_no_pl.png}}
      \label{fig:wpl}}
      \hfill
       \hfill
    \subfloat[wLS]{\frame{\includegraphics[width=0.18\linewidth,height=0.18\linewidth]{abl_no_ls.jpg}}
      \label{fig:wls}} 
      \hfill
       \hfill
     \subfloat[TIGDA]{\frame{\includegraphics[width=0.18\linewidth,height=0.18\linewidth]{abl_with_ls.jpg}}
      \label{fig:ab_ours}} 
      }
\caption{Ablation of TIGDA for task C\textrightarrow A. 
(wEc=without Edge concatenation, wPl=without Perceptual loss, wLS=without Latent Search). The best source-like features are observed in the image with all the components of TIGDA.}
\label{fig:ablation_imgs}
\end{figure}
\setlength{\textfloatsep}{0pt}
\subsubsection{Effect of Perceptual loss } We have used a perceptual model  trained on source samples while training the VAE.
Perceptual loss minimizes the Euclidean distance between the (perceptual) feature vectors of input and reconstructed source images. 
It measures image similarities more robustly than per-pixel losses (e.g., Mean squared error). It ensures that the VAE reconstructed image is semantically similar to the input. We can observe from Figure \ref{fig:wpl} that VAE reconstructed image without perceptual loss (wPl) during training, has different color and texture patterns from the real target image shown in Figure \ref{fig:ab_realt}. The finer background details are missing in Figure \ref{fig:wpl}. Such images will result in a poor latent space and the performance on target images will drop during inference. Table \ref{tab:ablation} shows that the accuracy drops to 53.4\% for the task A\textrightarrow B without perceptual loss while it is 76.2\% for TIGDA that uses perceptual loss during training. Similarly the accuracy drops to 57.1\% for the task B\textrightarrow C when perceptual loss was not employed during training but the accuracy on the same task is 74.8\% with perceptual loss.
\setlength{\textfloatsep}{0pt}
\begin{table}
\caption{Ablation of different components of TIGDA during training and inference; Edge, perceptual loss  and Latent Search (LS). Accuracy (mean  std\%) values are reported as an average over five independent runs for two tasks.}
\begin{center}
\scalebox{1.0}{
\begin{tabular}{ccc|c|c}
    \toprule
    Edge &  & LS & {A\textrightarrow B} &  {B\textrightarrow C} \\
    \midrule
    &&&35.80.2&39.50.1\\
    \checkmark&&&39.70.4&42.20.3\\
    &\checkmark&&38.90.5&43.40.3\\
    &&\checkmark&50.20.3&52.80.2\\
    \checkmark&\checkmark&&43.70.2&46.90.5\\
    &\checkmark&\checkmark&57.60.4&60.30.2\\
    \checkmark&&\checkmark&53.40.3&57.10.4\\
    \checkmark&\checkmark&\checkmark&76.20.3&74.80.6\\
\bottomrule
\end{tabular}
}
\end{center}
\label{tab:ablation}
\end{table} 
\setlength{\textfloatsep}{0pt}
\subsubsection{Effect of Latent Search and other Loss functions}
To validate the importance of the Latent Search procedure, in Figure \ref{fig:wls} we show the VAE reconstructed images  without Latent Search for the target image shown in Figure \ref{fig:ab_realt}. 
Figure \ref{fig:ab_ours} shows the generated image after Latent Search for the task C\textrightarrow A. It is observed (empirically) that the `closest-clone' obtained through TIGDA shown in Figure \ref{fig:ab_ours} is visually more closer to the source domain  as compared to VAE reconstructed image shown in Figure \ref{fig:wls}. When no  Latent Search is employed, the accuracy for the tasks A\textrightarrow B and B\textrightarrow C drops to 43.7\% and 46.9\% respectively as shown in Table \ref{tab:ablation}. 
\setlength{\textfloatsep}{0pt}
\begin{figure*}
\centering
     \subfloat[Accu. vs. SSIM window sizes.]{\includegraphics[width=0.2434\linewidth,height=0.165\linewidth]{ssim_wind.png}
      \label{fig:window}}
         \subfloat[Accu. vs. Edge concat. position.]{\includegraphics[width=0.2434\linewidth,height=0.165\linewidth]{edge_sweep1.jpg}
      \label{fig:dge_sweep}}
    \subfloat[Accuracy vs Skip connections. ]{\includegraphics[width=0.2434\linewidth,height=0.165\linewidth]{skip.jpg}
      \label{fig:skip}} 
      \subfloat[FID vs. no. of training images.]{\includegraphics[width=0.2434\linewidth,height=0.165\linewidth]{fid.jpg}
        \label{fig:fid}}
 \caption{\bb{(a) Accuracy of TIGDA on task A\textrightarrow B by selecting different window sizes in SSIM during Latent Search (b) Performance of TIGDA when the edges of input images are concatenated with different convolutional layers in decoder  (c) Performance of TIGDA when edge concatenation is replaced with different types of skip connections between encoder  and decoder  layers. Window size of 11 gives the best performance. For the same task, edge concatenation is better than skip connections. (d) FID of VAE generated images when TIGDA is trained on dataset A with different number of images ranging from 2,000 (2K) to 10,000 (10K). }}
\label{fig:f_w_e_s}
\end{figure*}
To affirm the usefulness of the choice of SSIM as loss for the Latent Search, we implemented Latent Search with three different losses, Mean Squared Error (MSE), Mean Absolute Error (MAE) and Structural Similarity Index (SSIM) loss and found that SSIM loss is the best performing among the three. SSIM loss compares pixels and their corresponding neighborhoods in two images, preserving the luminance, contrast and structure information. On the other hand, MSE or MAE measures only the absolute pixel differences rather than the structural differences. Figure \ref{fig:alossacc} and \ref{fig:blossacc} depict the outcome of these ablation studies where the superiority of the SSIM loss  is seen over MSE and MAE for the tasks A\textrightarrow C and B\textrightarrow C respectively. Table \ref{tab:ablation} summarizes all the ablation studies conducted  on two domain adaptation tasks with different combinations of the components. It can be noted that the best performance is observed by utilizing all the three components: Edge concatenation, perceptual loss and Latent Search procedure. Thus, with all the aforementioned studies, we have demonstrated the utility of all the individual components used in TIGDA for UDA task on WBC classification.  

\subsubsection{Effect of other hyperparameters} \bb{In this section, we study the effect of four hyperparameters: (a) the window size for the SSIM loss used for Latent Search, (b) the position of the Edge-operator in the decoder network, (c) use of Skip connection as in \cite{unet} instead of edge concatenation, (d) number of source samples required to generate high-fidelity images using VAE. Figure \ref{fig:f_w_e_s}(a) depicts the change in the performance for A\textrightarrow B with varying window sizes of SSIM. While the performance varies with different window sizes, the best accuracy is observed with the default choice of 11 that is used in all our experiments.}\par \bb{Next, in Figure \ref{fig:f_w_e_s}(b), we vary the layer of the decoder to concatenate edges. It is seen that the performance is best at the penultimate layers since the edges are used only to reduce the blurriness of the generated image that occurs near the last few layers of the decoder. Providing the edge information at initial layers of the decoder, regularizes more than required, thus degrading the quality of the generated image.} \par \bb{To further quantify the effect of edge concatenation as a regularizer, we replace it with another type of spatial contiguity in the form of skip connections as in a segmentation network such as UNet \cite{unet}.  We have used five different types of skip connections. Type-1 refers to no skip connection. Type-2 connects FC1 layer (Refer to Supplementary material for the names of the layers in the architecture) of the encoder with FC2 layer of the decoder network. Type-3 connects all the layers in the encoder with layers of corresponding dimensions in the decoder (like a U-Net). Type-4 connects Conv1 layer in the encoder with Conv9 layer of the decoder. Type-5 is combination of Type-2 and Type-4 skip connections. We observe in Figure \ref{fig:f_w_e_s}(c) that having skip connection is better than not having it since it regularizes the network. Further, Type-4, that connects the initial layers of the encoder with final layers of the decoder, has the best performance. This can be explained by the fact that initial layers of the CNNs are known to extract edge-like features which is shown to enhance the performance in the given task. Connecting more layers as in Type-3 and Type-5 leads to over regularization and degrades the performance. However, explicit edge concatenation still provides the best performance.} \par \bb{In the final plot Figure \ref{fig:f_w_e_s}(d), we report the Fréchet Inception Distance (FID)~\cite{heusel2017gans}, that quantifies the quality of the generated data (lower the better) for any generative model, as a function of the number of source samples used to train the VAE. It is seen that with the increase in number of images for training VAE, the quality of generated images improve as shown by the FID values. Therefore, with about 10K samples, one can expect the VAE to sample high-fidelity source images.}

\begin{table*}[t]
\caption{ \bb{Accuracy (mean  std\%) values for UDA tasks on Office-31 and Imaging Flow Cytometry (Cyto.) and grayscale Peripheral Blood Smear (gray-PBS) White Blood Cell datasets. 
Results are reported as an average over five independent runs using various SOTA UDA methods using ResNet-50 classifier. Note that while all UDA methods perform better than the source only model, TIGDA offers significant performance enhancement despite not using the target images during training.}}
 \begin{center}
\color{black}\scalebox{0.8}{
\begin{tabular}{l|ccccccc|ccc}
    \toprule


    \multicolumn{1}{c|}{} & \multicolumn{7}{c|}{Office-31} & \multicolumn{3}{c}{WBC}\\
    Models & {A\textrightarrow W} &  {D\textrightarrow W} & {W\textrightarrow D} & {A\textrightarrow D} & {D\textrightarrow A} & {W\textrightarrow A} & {Avg} & {gray-PBS\textrightarrow Cyto.} &  {Cyto.\textrightarrow gray-PBS}  & {Avg}\\
\midrule
    {Source Only}&68.40.2&96.70.1&99.30.1&68.90.2&62.50.3&60.70.3&76.1&42.60.1&22.20.2& 32.4\\

    {JAN~\cite{long2017deep}}&85.40.3&97.40.2&99.80.2&84.70.3&68.60.3&70.00.4&84.3&67.50.2&57.20.3&62.3\\
   


   {MADA~\cite{pei2018multi}}&90.00.2&97.40.1&99.60.1&87.80.2&70.30.4&66.30.1&85.2&73.30.2&61.80.3&67.5\\
   


   {SimNet~\cite{pinheiro2018unsupervised}}&88.60.5&98.20.2&99.70.2&85.30.3&73.40.8&71.80.6&86.2&76.40.2&66.80.2&71.6\\
    

   {GTA~\cite{sankaranarayanan2018generate}}&89.50.5&97.90.3&99.80.4&87.70.5&72.80.3&71.40.4&86.5&75.20.4&66.50.3&70.8\\

    {DAAA~\cite{kang2018deep}}&86.80.2&99.30.1&100.00.0&88.80.4&74.30.2&73.90.2&87.2&75.80.3&68.20.1&72.0\\
    


    {CDAN~\cite{long2018conditional}}&94.10.1&98.60.1&100.00.0&92.90.2&71.00.3&69.30.3&87.7&78.60.2&67.10.1&72.8\\
   




    {CAN~\cite{kang2019contrastive}}&94.50.3&99.10.2&99.80.2&95.00.3&78.00.3&77.00.3&90.6&79.40.3&68.90.2&74.1\\
    











  {TIGDA (Ours)}&93.20.2&99.40.4&99.80.1&93.60.3&76.70.2&75.70.3&89.7&80.30.4&71.40.3&75.8\\
    
    \bottomrule
\end{tabular}
}
\end{center}
\label{tab:compoffice}
\end{table*}



\section{TIGDA beyond PBS}
\bb{
In this section, we examine the effectiveness of the proposed method TIGDA on two datasets, Imaging Flow Cytometry~\cite{lippeveld2019classification} and Office-31~\cite{saenko2010adapting}, apart from PBS. In Cytometry dataset, WBCs from whole blood samples were stained using a ImageStream-X MK II imaging flow cytometer. A three channel image is extracted with two bright-field (at wavelengths of 420 nm - 480 nm and 570 nm - 595 nm) and a dark-field channel. Four classes of WBCs are employed in this study: Eosinophil (1470 images), Neutrophil (4809 images), Lymphocyte (4570 images) and Monocyte (1239 images). The objective of this experiment is to examine if TIGDA can perform domain adaptation when the source is Cytometry data and the target is PBS and vice versa. Since Cytometry data doesn't have the notion of color, we take the grayscale version of the PBS dataset with a  central crop (in all the images) representing the nucleus. Figure \ref{fig:cytofig} depicts a sample image from each class of the Cytometry dataset and the PBS dataset which apparently shows a significant domain shift.} \bb{Office-31~\cite{saenko2010adapting}, a publicly available standard dataset for UDA tasks (some sample images are given in the supplementary material), contains images from 31 common object types taken with three different imaging sources namely Dslr (D), Webcam (W) and Amazon (A). The objective of UDA is to adapt between these three domains.}

\bb{
Table \ref{tab:compoffice} lists the results of TIGDA along with some SOTA UDA methods for domain adaptation tasks on both the Office-31 and Cytometry datasets. It is seen that on Cytometry and gray-PBS datasets, TIGDA performs the best by significantly improving upon the Source Only model for gray-PBS\textrightarrow Cyto. and Cyto.\textrightarrow gray-PBS tasks. Whereas, on the Office-31 dataset, TIGDA's average performance is comparable (less than a percent) to the best SOTA method. All these experiments firmly demonstrate the effectiveness of TIGDA in UDA despite not using the target data during training.}
\begin{figure}[!t]
\includegraphics[width=.48\textwidth,height=.236\textwidth]{cyto_pbs2}
\caption{\bb{Imaging Flow Cytometry~\cite{lippeveld2019classification} and grayscale Peripheral Blood Smear (gray-PBS) White Blood Cell datasets.}}
\label{fig:cytofig}
\end{figure}


















\section{Conclusion}
\bb{In this work, we have considered the problem of domain shift occurring with the CNN-based classifiers for WBC classification. The performance of the existing \cc{deep learning} based techniques is known to degrade with the change in camera characteristics. We cast the problem of performance degradation of WBC classifiers with the change in camera as that of \cc{Unsupervised Domain Adaptation} (UDA) and propose a method that is devoid of need for access to the target data during training. We have demonstrated the efficacy of the proposed method for UDA with experiments on multiple datasets acquired under different settings. A few possible future directions can be: (i) extension of TIGDA for medical data beyond WBC, (ii) combining multiple sources for UDA.} 


\section{Acknowledgements}
\bb{We sincerely thank the Associate Editor and the Anonymous Reviewers for their thoughtful comments that helped to significantly improve our paper. We also thank Maxim Lippeveld, Ghent University for his generous help in providing and navigating through the Cytometry dataset. We thank Sameer Ambekar and Aayush Tyagi for their help in experiments.} 


\bibliographystyle{ieeetr}
\bibliography{bibtex}












\clearpage


\title{Target-Independent Domain Adaptation for WBC Classification using Generative Latent Search \\--Supplementary--}
 \author{
Prashant Pandey, Prathosh AP, Vinay Kyatham, Deepak Mishra and 
Tathagato Rai Dastidar
}
\maketitle
\setlength{\dbltextfloatsep}{0.2cm}
\section{Proof for Lemma 1}:
\begin{lemma}

If  is the point such that , then as ,\   converges to   with probability . 
\end{lemma}

\begin{proof}
Let   be a closed ball of radius  around  under the metric . That is,  . Since  is a separable metric space, ,  has non-zero probability measure [40]. That is, 

For any  , the probability that  none of the points in  are within the ball  of radius  is given by:
\setlength{\textfloatsep}{1pt}

Therefore, the probability of , lying within  is given by: 
\setlength{\textfloatsep}{1pt}

Thus, given any , with probability ,  that is within   distance from  as 
\end{proof}

\vspace{2.9 in}
\section{Algorithm for TIGDA}



\begin{algorithm}
    \caption{\textbf{Target-Independent Generative Domain Adaptation (TIGDA)}}
    \label{alg:trainalgo}


    \hspace*{\algorithmicindent} 
    
    \begin{algorithmic}[1] \phase{{Training VAE on source data}}
    \textbf{Input}: Source dataset , Number of source images , Encoder , Decoder , Trained Perceptual Model , Learning rate , Batchsize . \textbf{Output}: Optimal parameters , .
    \State Initialize parameters , 
    \REPEAT
\State sample batch  from dataset  for   
\State 
    \State sample  
\State  \State  
    \vspace*{0.08cm}
    \State 
    
    \State 
    \State  
    \State  
    \State  
\UNTIL{convergence of ,  }
    \phase{Inference - Latent Search for target images }
     \textbf{Input}: Target image , Trained decoder , Learning rate . \textbf{Output}: `closest-clone'   for the target image .  
    \State sample  from 
    \REPEAT


\State 
    \State 
    \UNTIL{convergence of  }
    \State 
    
    \State 
    \end{algorithmic}
\end{algorithm}




\begin{figure*}[th]
\centering
    \subfloat[ADDA]{\includegraphics[width=0.16\linewidth,height=0.14\linewidth]{adda_pca1.png}
        \label{fig:adda}}
     \subfloat[GTA]{\includegraphics[width=0.16\linewidth,height=0.14\linewidth]{gta_pca1.png}
      \label{fig:gta}}
         \subfloat[DIRT-T]{\includegraphics[width=0.16\linewidth,height=0.14\linewidth]{dirtt_pca1.png}
      \label{fig:dirtt}}
    \subfloat[TAT]{\includegraphics[width=0.16\linewidth,height=0.14\linewidth]{tat_pca.png}
      \label{fig:tat}} 
     \subfloat[DAL]{\includegraphics[width=0.16\linewidth,height=0.14\linewidth]{dal_pca.png}
      \label{fig:dal}} 
     \subfloat[TIGDA]{\includegraphics[width=0.16\linewidth,height=0.14\linewidth]{tigda_pca.png}
      \label{fig:ours_tsne}} 
 \caption{PCA plots of the first two principal component using features generated by ADDA, GTA, DIRT-T, TAT, DAL and TIGDA on task A\textrightarrow C.}
\label{fig:pca}
\end{figure*}


\begin{figure*}[th]
\centering
    \subfloat[ADDA]{\includegraphics[width=0.30\linewidth,height=0.30\linewidth]{adda_cm1.png}
        \label{fig:adda}}
     \subfloat[GTA]{\includegraphics[width=0.30\linewidth,height=0.30\linewidth]{gta_cm1.png}
      \label{fig:gta}}
         \subfloat[DIRT-T]{\includegraphics[width=0.30\linewidth,height=0.30\linewidth]{dirtt_cm1.png}
      \label{fig:dirtt}}
      \\
    \subfloat[TAT]{\includegraphics[width=0.30\linewidth,height=0.30\linewidth]{tat_cm1.png}
      \label{fig:tat}} 
     \subfloat[DAL]{\includegraphics[width=0.30\linewidth,height=0.30\linewidth]{dal_cm1.png}
      \label{fig:dal}} 
     \subfloat[TIGDA]{\includegraphics[width=0.30\linewidth,height=0.30\linewidth]{tigda_cm1.png}
      \label{fig:ours_tsne}} 
 \caption{Confusion Matrices for ADDA, GTA, DIRT-T, TAT, DAL and TIGDA on task A\textrightarrow C. Classes are Neutrophil (NE), Lymphocyte (LY), Monocyte (MO), Eosinophil (EO), Basophil (BA), Immature granulocytes (IG), Atypical (AT), Nucleated red blood cells (NR), Giant platelets (GP), Platelet clumps (PC), Artefact (AF).}
\label{fig:pca}
\end{figure*}

\begin{table*}
\caption{Encoder architecture for the Variational Auto-Encoder (VAE) in the proposed method (TIGDA). Convolution kernel is  and for Leaky ReLU .}
\centering
  \begin{tabular}{cc}
    \toprule
        {Layer (type)} & Output shape 
        \\
    \midrule
    encoder\_input (InputLayer) & (128, 128, 3) \\
    Conv1 (Convolution) & (128, 128, 128) \\
    leakyReLU1 (Activation) & (128, 128, 128) \\
    Conv2 (Convolution) & (64, 64, 128) \\
    leakyReLU2 (Activation) & (64, 64, 128)\\
    Conv3 (Convolution) & (32, 32, 128) \\
    leakyReLU3 (Activation) & (32, 32, 128) \\
    Conv4 (Convolution) & (16, 16, 128) \\
    leakyReLU4 (Activation) & (16, 16, 128) \\
    Conv5 (Convolution) & (8, 8, 128) \\
    leakyReLU5 (Activation) & (8, 8, 128) \\
    Conv6 (Convolution) & (4, 4, 128) \\
    leakyReLU6 (Activation) & (4, 4, 128) \\
    FC1 (Dense) & (1024) \\
    Z (Dense) & (64) \\
    \bottomrule
  \end{tabular}
  \label{table:enc}
\end{table*}

\begin{table*}
\caption{Decoder architecture for the VAE in TIGDA. Convolution kernel is  and for Leaky ReLU .}
\centering
  \begin{tabular}{cc}
    \toprule
        {Layer (type)} & Output shape 
        \\
    \midrule
    decoder\_input (InputLayer) & (64) \\
    FC2 (Dense) & (1024) \\
    leakyReLU7 (Activation) & (1024) \\
    FC3 (Dense) & (2048) \\
    leakyReLU8 (Activation) & (2048) \\
    Deconv1 (Deconvolution) & (4, 4, 128) \\
    leakyReLU9 (Activation) & (4, 4, 128) \\
     Deconv2 (Deconvolution) & (8, 8, 128) \\
     leakyReLU10 (Activation) & (8, 8, 128) \\
     Deconv3 (Deconvolution) & (16, 16, 128) \\
     leakyReLU11 (Activation) & (16, 16, 128) \\
     Deconv4 (Deconvolution) & (32, 32, 128) \\
     leakyReLU12 (Activation) & (32, 32, 128) \\
     Deconv5 (Deconvolution) & (64, 64, 128) \\
     leakyReLU13 (Activation) & (64, 64, 128) \\
     Deconv6 (Deconvolution) & (128, 128, 3) \\
     tanh1 (Activation) & (128, 128, 3) \\
     edge\_input (InputLayer) & (128, 128, 6)\\
     edge\_concat (Concatenate) & (128, 128, 9)\\
  Conv7 (Convolution) & (128, 128, 128) \\
  leakyReLU14 (Activation) & (128, 128, 128) \\
    Conv8 (Convolution) & (128, 128, 128) \\
    leakyReLU15 (Activation) & (128, 128, 128) \\
    Conv9 (Convolution) & (128, 128, 3) \\
    tanh2 (Activation) & (128, 128, 3) \\
    \bottomrule
  \end{tabular}
  \label{table:enc}
\end{table*}


\begin{figure*}[t]

\centering
 \includegraphics[width=0.81\textwidth,height=0.294\textwidth]{office_31.pdf}
\caption{Samples from the Office-31 dataset from the three sources, Amazon, Dslr and Webcam.}
\label{fig:office31}
\end{figure*}



\end{document}
