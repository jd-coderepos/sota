


\documentclass[10pt,journal]{IEEEtran}

\usepackage[cmex10]{amsmath}


\usepackage[]{cite}
\usepackage{url}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{marvosym}
\usepackage{epsfig}
\usepackage{amssymb}
\usepackage{caption}
\usepackage[svgnames,table]{xcolor}
\usepackage{comment}
\usepackage{algorithm}
\usepackage[algo2e,vlined,algoruled,resetcount]{algorithm2e}
\captionsetup[algorithm]{font=footnotesize}
\usepackage{enumitem}
\usepackage{multirow}
\usepackage{subfig}
\captionsetup[figure]  {labelfont=bf,font=small}
\captionsetup[subfloat]{labelfont=bf,font=small}
\captionsetup[table]   {labelfont=bf,font=small}

\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\usepackage{cases}
\usepackage{wrapfig}

\usepackage{textcomp}



\newcommand{\be}{\mathbf e}
\newcommand{\bu}{\mathbf u}
\newcommand{\bx}{\mathbf x}
\newcommand{\bt}{\mathbf t}
\newcommand{\bh}{\mathbf h}
\newcommand{\by}{\mathbf y}
\newcommand{\bc}{\mathbf c}
\newcommand{\bv}{\mathbf v}
\newcommand{\bb}{\mathbf b}
\newcommand{\bp}{\mathbf p}
\newcommand{\bw}{\mathbf w}

\newcommand{\bX}{\mathbf X}
\newcommand{\bT}{\mathbf T}
\newcommand{\bP}{\mathbf P}
\newcommand{\bD}{\mathbf D}
\newcommand{\bW}{\mathbf W}
\newcommand{\bA}{\mathbf A}
\newcommand{\bI}{\mathbf I}
\newcommand{\bK}{\mathbf K}
\newcommand{\bH}{\mathbf H}
\newcommand{\bC}{\mathbf C}
\newcommand{\bN}{\mathbf N}
\newcommand{\bM}{\mathbf M}
\newcommand{\bR}{\mathbf R}
\newcommand{\bL}{\mathbf L}
\newcommand{\bB}{\mathbf B}
\newcommand{\bZ}{\mathbf Z}
\newcommand{\bY}{\mathbf Y}
\newcommand{\bU}{\mathbf U}

\newcommand{\fY}{\mathrm{Y}}
\newcommand{\fy}{\mathrm{y}}
\newcommand{\fE}{\mathrm{E}}
\newcommand{\fD}{\mathrm{D}}
\newcommand{\fd}{\mathrm{d}}
\newcommand{\ftheta}{\mathrm{\theta}}
\newcommand{\frank}{\mathrm{rank}}
\newcommand{\fdiag}{\mathbf{diag}}
\newcommand{\fbound}{\mathrm{bound}}
\newcommand{\fbranch}{\mathrm{split}}
\newcommand{\ftrace}{\mathrm{trace}}

\newcommand{\setD}{\mathcal{D}}
\newcommand{\setV}{\mathcal{V}}
\newcommand{\setE}{\mathcal{E}}
\newcommand{\setF}{\mathcal{F}}
\newcommand{\setH}{\mathcal{H}}
\newcommand{\setN}{\mathcal{N}}
\newcommand{\setZ}{\mathcal{Z}}
\newcommand{\setI}{\mathcal{I}}
\newcommand{\setJ}{\mathcal{J}}
\newcommand{\setJbar}{\bar{\setJ}}
\newcommand{\setQ}{\mathcal{Q}}
\newcommand{\setG}{\mathcal{G}}

\newcommand{\real}{\mathbb{R}}
\newcommand{\psdd}{\succcurlyeq}

\newcommand{\sst}{\mathrm{s.t.}}

\newcommand{\GLB}{\mathrm{glb}}
\newcommand{\GUB}{\mathrm{gub}}
\newcommand{\LB}{\mathrm{lb}}
\newcommand{\UB}{\mathrm{ub}}


\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}

\def\eg{\emph{e.g}\onedot} \def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot} \def\Ie{\emph{I.e}\onedot}
\def\cf{\emph{c.f}\onedot} \def\Cf{\emph{C.f}\onedot}
\def\etc{\emph{etc}\onedot} \def\vs{\emph{vs}\onedot}
\def\wrt{with respect to\xspace} \def\dof{d.o.f\onedot}
\def\etal{\emph{et al}\onedot}
\def\pw{P\onedot Wang\xspace}

\makeatother

\def\sexyname{Ahab\xspace}
\def\KBKnowl{KB-knowledge\xspace} \def\KBName{FVQA\xspace}
\def\ModParms{{\boldsymbol \theta}}

\def\bluetext{}
\def\bluett{}
\def\bluettt{\textcolor{black}}



\begin{document}
\sloppy

\title{\KBName: Fact-based Visual Question Answering }
\author{Peng Wang,
        Qi Wu,
        Chunhua Shen,
        Anthony Dick,
        Anton~van~den~Hengel
        \IEEEcompsocitemizethanks{\IEEEcompsocthanksitem
P. Wang is with Northwestern Polytechnical University, China, and The University of Adelaide, Australia.
Q. Wu, C. Shen, A. Dick and A. van den Hengel are with The University of Adelaide, Australia.
(E-mail: peng.wang@nwpu.edu.cn, \{qi.wu01, chunhua.shen, anthony.dick, anton.vandenhengel\}@adelaide.edu.au).
  Correspondence should be addressed to C. Shen.
The first two authors contributed to this work equally.
}
}


\maketitle



\begin{abstract}

Visual Question Answering (VQA) has attracted much attention in both computer vision and natural language processing communities, not least because it offers insight into the relationships between two important sources of information.
Current datasets, and the models built upon them, have focused on questions which are answerable by direct analysis of the question and image alone.  The set of such questions that require no external information to answer is interesting, but very limited.  It excludes questions which require common sense, or basic factual knowledge to answer, for example.
Here we introduce \KBName (Fact-based VQA), a VQA dataset which requires, and supports, much deeper reasoning.
\KBName primarily contains questions that require external information to answer.
We thus extend a conventional visual question answering dataset, which contains {image-question-answer} triplets, through additional {image-question-answer-supporting fact} tuples.
Each supporting-fact is represented as a structural triplet, such as \texttt{<Cat,CapableOf,ClimbingTrees>}.

We evaluate several baseline models on the \KBName dataset, and describe a novel model which is capable of reasoning about an image on the basis of supporting-facts.

\end{abstract}

\begin{IEEEkeywords}
  Visual Question Answering, Knowledge Base, Recurrent Neural Networks.
\end{IEEEkeywords}




\tableofcontents
\clearpage




\section{Introduction}
\label{intro}
Visual Question Answering (VQA) can be seen as a proxy task for evaluating a vision system's capacity for deeper image understanding. It requires elements of image analysis, natural language processing, and a means by which to relate images and text. Distinct from many perceptual visual tasks such as image classification, object detection and recognition~\cite{krizhevsky2012imagenet, lin2014microsoft, deng2009imagenet, simonyan2014very}, however, VQA requires that a method be prepared to answer a question that has never been seen before.  In object detection the set of objects of interest are specified at training time, for example, whereas in VQA the set of questions which may be asked inevitably extend beyond those in the training set.

The set of questions that a VQA method is able to answer are one of its key features, and limitations. Asking a method a question that is outside its scope will lead to a failure to answer, or worse, to a random answer.
Much of the existing VQA effort has been focused on questions which can be answered by the direct analysis of the question and image, on the basis of a large training set~\cite{antol2015vqa,malinowski2014towards,gao2015you,Yu_2015_ICCV,ren2015image,zhu2015visual7w}.
This is a restricted set of questions, which require only relatively shallow image understanding to answer.  It is possible, for example, to answer `How many giraffes are in the image?' without understanding any non-visual knowledge about giraffes.

The number of VQA datasets available has grown as the field progresses~\cite{antol2015vqa,malinowski2014towards,gao2015you,Yu_2015_ICCV,ren2015image,zhu2015visual7w}. They have contributed valuable large-scale data for training neural-network based VQA models and introduced various question types, and tasks, from global association between QA pairs and images~\cite{antol2015vqa,malinowski2014towards,ren2015image} to grounded QA in image regions~\cite{zhu2015visual7w}; from free-from answer generation~\cite{antol2015vqa,gao2015you,ren2015image,zhu2015visual7w} to multiple-choice picking~\cite{antol2015vqa,malinowski2014towards} and blank filling~\cite{Yu_2015_ICCV}.
For example, the questions defined in DAQUAR~\cite{malinowski2014towards} are almost exclusively
``Visual'' questions, referring to ``color'', ``number'' and ``physical location of the object''. In the COCO-QA dataset~\cite{ren2015image}, questions are generated automatically from image captions which describe the major visible content of the image.

\bluetext{
The VQA dataset in~\cite{antol2015vqa}, for example, has been very well studied, yet only 5.5\% of questions require adult-level (18+) knowledge (28.4\% and 11.2\% questions require older child (9-12) and teenager (13-17) knowledge). }
This limitation means that this is not a truly ``AI-complete'' problem, because this is not a realistic test for human beings. Humans inevitably use their knowledge to answer questions, even visual ones.
For example, to answer the question given in  Fig. \ref{fig:t_figure}, one not only needs to visually recognize the `red object' as a `fire hydrant', but also to know that `{\em a fire hydrant can be used for fighting fires}'.

\begin{figure}[t!]
\begin{center}
	\includegraphics[width=0.95\columnwidth]{t_fig.pdf}
\end{center}
\vspace{-4pt}
\caption{An example visual-based question from our \KBName dataset that requires both visual and common-sense knowledge to answer. The answer and mined knowledge are generated by our proposed method.}
\label{fig:t_figure}
\vspace{-5pt}
\end{figure}

Developing methods that are capable of deeper image understanding demands a more challenging set of questions.
We consider here the set of questions which may be answered on the basis of an external source of information, such as Wikipedia.
This reflects our belief that reference to an external source of knowledge is essential to general VQA.  This belief is based on the observation that the number of \textit{image-question-answer} training examples that would be required to provide the background information necessary to answer general questions about images would be completely prohibitive.
The number of concepts that would need to be illustrated is too high, and scales combinatorially.

In contrast to previous VQA datasets which only contain \textit{question-answer} pairs for an image, we additionally provide a \textit{supporting-fact} for each \textit{question-answer} pair, as shown in Fig.~\ref{fig:t_figure}. The \textit{\bf supporting-fact} is a structural representation of knowledge that is stored in external KBs and indispensable for answering a given visual question.
For example, given an image with a cat and a dog, and the question  \texttt{`Which animal in the image is able to climb trees?'},  the answer is \texttt{`cat'}. The required supporting-fact for answering this question is \texttt{<Cat,CapableOf,ClimbingTrees>}, which is extracted from an existing knowledge base. By providing supporting-facts, the dataset supports answering complex questions, even if all of the information required to answer the question is not depicted in the image. Moreover, it supports explicit reasoning in visual question answering, \ie, it gives an indication  as to how a method might derive an answer. This information can be used in answer inference, to search for other appropriate facts, or to evaluate answers which include an inference chain.


In demonstrating the value of the dataset in driving deeper levels of image understanding in VQA, we
examine on our \KBName dataset 
the performance of the state-of-the-art RNN (Recurrent Neural Network) based approaches~\cite{antol2015vqa,malinowski2014towards,ren2015image}.  We find that there are a number of limitations with these approaches. Firstly, there is no explicit reasoning process in these methods. This means that it is impossible to tell whether the method is answering the question based on image information or merely the prevalence of a particular answer in the training set. The second problem is that, because the model is trained on individual question-answer pairs, the range of questions that can be accurately answered is limited. It can only answer questions about concepts that have been observed in the training set, but there are millions of possible concepts and hundreds of millions relationships between them. 

Our main contributions are as follows.
A new VQA dataset (\KBName) with additional supporting-facts is introduced in Sec.~\ref{dataset}, which requires and supports deeper reasoning.
In response to the observed limitations of RNN-based approaches, we propose a method which is based on explicit reasoning about the visual concepts detected from images, in Sec.~\ref{method}. The proposed method first detects relevant content in the image, and relates it to information available in a pre-constructed knowledge base (we combine several publicly available large-scale knowledge bases). A natural language question is then automatically classified and mapped to a query which runs over the combined image and knowledge base information. The response of the query leads to the supporting-fact, which is then processed so as to form the final answer to the question. 
\bluettt{Our approach achieves the Top-1 accuracy of , outperforming existing baseline VQA methods.}









\section{Related Work}
\label{relwork}

\subsection{Visual Question Answering Datasets}

Several datasets designed for Visual Question Answering have been proposed. The DAQUAR \cite{malinowski2014towards}
dataset is the first small benchmark dataset built upon indoor scene RGB-D images, which is mostly composed of questions requiring only visual knowledge. Most of the other datasets \cite{antol2015vqa,gao2015you,Yu_2015_ICCV,ren2015image,zhu2015visual7w} represent question-answer pairs for Microsoft COCO images \cite{lin2014microsoft}, either generated automatically by NLP tools \cite{ren2015image} or written by human workers \cite{antol2015vqa,gao2015you}. \bluetext{The Visual Genome dataset~\cite{krishnavisualgenome} contains 1.7 million questions, which are asked by human workers based on region descriptions. }The MadLibs dataset \cite{Yu_2015_ICCV} provides a large number of template based text descriptions of images, which are used to answer multiple choice questions about the images. Visual 7W  \cite{zhu2015visual7w} established a semantic link between textual descriptions and image regions by object-level grounding and the questions are asked based on groundings.

\subsection{Visual Question Answering Methods}
Malinowski \etal \cite{malinowski2014multi}
were
the first to study the VQA problem. They proposed a method that combines image segmentation and semantic parsing with a Bayesian approach to sample from nearest neighbors in the training set. This approach requires human defined relationships, which are inevitably dataset-specific. Tu \etal~\cite{tu2014joint} built a query answering system based on a joint parse graph from text and videos. Geman \etal~\cite{geman2015visual} proposed an automatic `query generator' that is trained on annotated images and produces a sequence of binary questions from any given test image.

The current dominant trend within VQA is to combine convolutional neural networks and recurrent neural networks to learn the mapping from input images and questions, to answers. Both Gao \etal~\cite{gao2015you} and Malinowski \etal \cite{malinowski2015ask} used RNNs to encode the question and generate the answer.  Whereas Gao \etal~\cite{gao2015you} used two networks, a separate encoder and decoder, Malinowski \etal \cite{malinowski2015ask} used a single network for both encoding and decoding. Ren \etal \cite{ren2015image} focused on questions with a single-word answer and formulated the task as a classification problem using an LSTM (Long Short Term Memory) network. Inspired by Xu \etal \cite{xu2015show} who encoded visual attention in image captioning, authors of \cite{zhu2015visual7w,Chen2015ABC,Jiang2015compositional,andreas2015deep,yang2015stacked} proposed to use spatial attention to help answer visual questions. 
\bluettt{Most of existing methods formulated the VQA as a classification problem and restrict that the answer only can be drawn from a fixed answer space.} 
In other words, they cannot generate open-ended answers. 
Zhu~\etal~\cite{zhu2015uncovering} investigated the video question answering problem using `fill-in-the-blank' questions. However, either an LSTM or a GRU (Gated Recurrent Unit) is still applied in these methods to model the questions. Irrespective of the finer details, we refer to them as the \bluettt{RNN} approaches.

\subsection{Knowledge-bases and VQA}

Answering general questions posed by humans about images inevitably requires reference to information not contained in the image itself. To an extent this information may be provided by an existing training set such as ImageNet~\cite{deng2009imagenet}, or Microsoft COCO~\cite{lin2014microsoft} as class labels or image captions.
There are a number of forms of such auxiliary information, including, for instance, question/answer pairs
which refer to objects that are not depicted (\eg, which reference people waiting for a train, when the train is not visible in the image) and provide external knowledge that cannot be derived directly from the image (\eg, the person depicted is Mona Lisa).

Large-scale {\em structured} Knowledge Bases
(KBs)~\cite{auer2007dbpedia,banko2007open,bollacker2008freebase,carlson2010toward,chen2013neil,mahdisoltani2014yago3,vrandevcic2014wikidata}
in contrast, offer an explicit, and typically larger-scale, representation of
such external information. In structured KBs, knowledge is typically represented by a large number of triples of the form \texttt{(arg1,rel,arg2)}, which we refer to as {\bf Facts} in this paper.
\texttt{arg1} and \texttt{arg2} denote two {\bf Concepts} in the KB,
each describing a concrete or abstract entity with specific characteristics.
\texttt{rel} represents a specific {\bf Relationship} between them.
A collection of such triples form a large interlinked graph. Such triples are often described according to a Resource Description Framework~\cite{rdf2014resource} (RDF) specification, and housed in a relational database management system (RDBMS), or triple-store, which allows queries over the data.
The information in KBs can be accessed efficiently using a query language.
In this work we use SPARQL Protocol~\cite{prud2008sparql} to query the OpenLink Virtuoso~\cite{erling2012virtuoso} RDBMS.

Large-scale structured KBs are constructed either by manual annotation
(\eg, DBpedia~\cite{auer2007dbpedia}, Freebase~\cite{bollacker2008freebase} and Wikidata~\cite{vrandevcic2014wikidata}), or by automatic extraction from unstructured/semi-structured data (\eg, YAGO~\cite{hoffart2013yago2,mahdisoltani2014yago3}, OpenIE~\cite{banko2007open,etzioni2011open,fader2011identifying}, NELL~\cite{carlson2010toward}, NEIL~\cite{chen2013neil},
WebChild~\cite{tandon2014acquiring}, ConceptNet~\cite{liu2004conceptnet}). The KB that we use here is the combination of DBpedia, WebChild and ConceptNet, which contains structured information extracted from Wikipedia and unstructured online articles.

\bluett{
In the NLP and AI communities, there is an increasing interest in the problem of natural language question answering over structured KBs (referred to as KB-QA). KB-QA approaches can be divided into two categories: {\em semantic parsing} methods~\cite{zettlemoyer2012learning,zettlemoyer2009learning,berant2013semantic,cai2013large,liang2013learning,kwiatkowski2013scaling,berant2014semantic,fader2014open,yih2015semantic,reddy2016transforming,xiao2016sequence} that
try to translate questions into accurate logical expressions and then map to KB queries, and
{\em information retrieval} methods~\cite{unger2012template,kolomiyets2011survey,yao2014information,bordes2014question,bordes2014open,dong2015question,bordes2015large} that coarsely retrieve a set of answer candidates and then perform ranking.
Similar to information retrieval methods, the QA systems using memory networks~\cite{weston2014memory,sukhbaatar2015end,bordes2015large,kumar2016ask,xiong2016dynamic} record
supporting-fact candidates in a memory module that can be read and written to.
The memory networks are trained to find the supporting-fact that leads to the answer by performing lookups in memory.
The ranking of stored facts can be implemented by measuring the similarity between facts and the question 
using attention mechanisms.
}

\bluett{
VQA systems that exploit KBs are still relatively rare.
Our proposed approach, which generates KB queries to obtain answers, is similar to semantic-parsing based methods.  
Alternatively, information-retrieval based methods, or memory networks, can be also used for the problem
posed in this paper. We leave this for future work.
The work in \cite{zettlemoyer2012learning} maps natural language queries to structured expressions in the lambda calculus.
Similarly, our VQA dataset also provides the supporting fact for a visual question, which, however, is not necessarily an equivalent translation of the given question.
}






 Zhu \etal~\cite{zhu2015building} used a KB and RDBMS to answer image-based queries. However, in contrast to our approach, they build a KB for the purpose, using an MRF model, with image features and scene/attribute/affordance labels as nodes. The  links between nodes represent mutual compatibility relationships. The KB thus relates specific images to specified image-based quantities, which are all that exists in the database schema. This prohibits question answering that relies on general knowledge about the world. Most recently, Wu \etal \cite{wu2015ask} encoded the text mined from DBpedia to a vector with the Word2Vec model which they combined with visual features to generate answers using an LSTM model. However, their proposed method only extracts discrete pieces of text from the knowledge base, thus ignoring the power of its structural representation. Neither \cite{zhu2015building} nor \cite{wu2015ask} are capable of explicit reasoning, in contrast to the method we propose here.

The approach closest to that we propose here is that of Wang \etal~\cite{wang2015explicit}, as it is capable of reasoning about an image based on information extracted from a knowledge base. However, their method largely relies on the pre-defined template, which only accepts questions in a pre-defined format. Our method here does not suffer this constraint. Moreover, their proposed model used only a single manually annotated knowledge source whereas the method here uses this plus two additional automatically-learned knowledge bases.
This is critical because manually constructing such KBs does not scale well, and using automatically generated KBs thus enables the proposed method to answer more general questions.

\bluett{
Krishnamurthy and Kollar~\cite{krishnamurthy2013jointly} 
proposed a semantic-parsing based method to find the grounding of a natural language query in a physical environment (such as image scene or geographical data).
Similar to our approach, a structured, closed-domain KB is firstly constructed for a specific environment, 
then the natural language query is parsed into a logical form and its grounding is predicted.
However, our approach differs from~\cite{krishnamurthy2013jointly} in that it focuses on 
visual questions requiring support from both physical environment and external commonsense knowledge,
which predicts over a much larger, multi-domain KB. }



\bluett{
	Similarly to our approach, 
	Narasimhan \etal \cite{narasimhan2016improving} also query data from external sources when
	information in the existing data is incomplete.
	The work in \cite{narasimhan2016improving} performs queries over unstructured Web articles, while ours 
	performs queries over structured relational KBs.
	Furthermore, the query templates used in \cite{narasimhan2016improving} are much simpler: only the article’s title
	needs to be replaced.}



\section{Creating the \KBName Dataset}
\label{dataset}
Different from previous VQA datasets \cite{antol2015vqa,gao2015you,Yu_2015_ICCV,ren2015image,zhu2015visual7w} that only ask annotators to provide {\em question-answer} pairs without any restrictions, 
{the questions in our dataset are expected to be answered with support of some commonsense knowledge.}
This means that we cannot simply distribute only images to questioners like others \cite{antol2015vqa,zhu2015visual7w}. 
\bluett{
We need to provide a large number of supporting-facts (commonsense knowledge) which are linked to 
concepts that can be grounded in images (we refer to as {\bf Visual Concepts}). }
We build our own on-line question collection system and allow users to choose images, visual concepts and candidate supporting-facts freely. Then users can ask questions based on their previous choices (all choices will be recorded). We provide users with a tutorial and restrict them to ask questions that only to be answered with both visual concept in the image and the provided external commonsense knowledge. In the following sections, we provide more details about images, visual concepts, knowledge bases and our question collection system and procedures. We also compare with other VQA datasets with data statistics.

\subsection{Images and Visual Concepts}
\label{sec:imgvc}

We sample  images from the Microsoft COCO~\cite{lin2014microsoft} validation set and ImageNet~\cite{deng2009imagenet} test set for collecting questions. Images from Microsoft COCO can provide more context because they have more complicated scenes. Scenes of ImageNet images are simpler but there are more object categories ( in ImageNet vs.  in Microsoft COCO).



Three types of visual concepts are extracted in this work:

\noindent{\bf  Object:}
Instances of real-world object classes with certain semantic meaning (such as humans, cars, dogs)
are detected by two Fast-RCNN~\cite{girshick2015fast} models that are trained respectively on 
Microsoft COCO -object (train split) and ImageNet -object datasets.
The image attribute model~\cite{qi2015caption} also predicts the existence 
of  objects without localisation information.
Overall, there are  distinct object classes to be extracted.

\noindent{\bf  Scene:}
The image scene (such as office, bedroom, beach, forest) 
information is extracted by combining 
the VGG-16 model trained on MIT Places -class dataset~\cite{zhou2014learning} and
the attribute classifier~\cite{qi2015caption} including  scene classes. 
 distinct scene classes are obtained after the combination.


\noindent{\bf  Action:}
The attribute model~\cite{qi2015caption} provides  classes of actions of humans or animals,
such as walking, jumping, surfing, swimming. 

A full list of extracted visual concepts can be found in the Appendix.
In the next section, these visual concepts are further linked to a variety of external knowledge bases.


\subsection{Knowledge Bases}

\begin{table*}[tpb!]
  \centering
\centering
\footnotesize
{
\resizebox{0.95\linewidth}{!}{
\begin{tabular}{l|p{2.4cm}|c|l}
\hline
& & & \\ [-2ex]
KB &{Relationship} &Facts &{Examples} \\
& & & \\ [-2ex]
\hline
& & & \\ [-2ex]
DBpedia
&\texttt{Category} 	&
&\texttt{(\underline{Wii},Category,VideoGameConsole)}  \\
& & & \\ [-2ex]
\hline
& & & \\ [-2ex]
\multirow{10}{*}{ConceptNet}
&\texttt{RelatedTo} 	&
&\texttt{(\underline{Horse},RelatedTo,\underline{Zebra})}, \texttt{(\underline{Wine},RelatedTo,Goblet)},
\bluett{\texttt{(\underline{Surfing},RelatedTo,Ocean)}} \\
&\texttt{AtLocation} 	&
&\texttt{(Bikini,AtLocation,\underline{Beach})}, \texttt{(Tap,AtLocation,\underline{Bathroom})} \\
&\texttt{IsA} 		&
&\texttt{(\underline{Broccoli},IsA,GreenVegetable)} \\
&\texttt{CapableOf} 	&
&\texttt{(\underline{Monitor},CapableOf,DisplayImages)} \\
&\texttt{UsedFor}   	&
&\texttt{(\underline{Lighthouse},UsedFor,SignalingDanger)}\\ &\texttt{Desires} 	&
&\texttt{(\underline{Dog},Desires,PlayFrisbee)}, \texttt{(\underline{Bee},Desires,Flower)} \\
&\texttt{HasProperty} 	&
&\texttt{(\underline{Wedding},HasProperty,Romantic)}  \\
&\texttt{HasA} 	&
&\texttt{(\underline{Giraffe},HasA,LongTongue)}, \texttt{(\underline{Cat},HasA,Claw)} \\
&\texttt{PartOf} 	&
&\texttt{(RAM,PartOf,\underline{Computer})}, \texttt{(Tail,PartOf,\underline{Zebra})} \\
&\texttt{ReceivesAction} 	&
&\texttt{(\underline{Books},ReceivesAction,bought at a bookshop)} \\
&\texttt{CreatedBy} 	&
&\texttt{(\underline{Bread},CreatedBy,Flour)}, \texttt{(\underline{Cheese},CreatedBy,Milk)} \\
& & & \\ [-2ex]
\hline
& & & \\ [-2ex]
\multirow{3}{*}{WebChild}
&\texttt{Smaller}, \texttt{Better}, & \multirow{3}{*}{}
&\texttt{(\underline{Motorcycle},Smaller,\underline{Car})}, \texttt{(\underline{Apple},Better,VitaminPill)},\\
&\texttt{Slower}, \texttt{Bigger},  &
&\texttt{(\underline{Train},Slower,\underline{Plane})}, \texttt{(Watermelon,Bigger,\underline{Orange})}, \\
&\texttt{Taller},	      &
&\texttt{(\underline{Giraffe},Taller,Rhino)}, \bluett{\texttt{(\underline{Skating},Faster,\underline{Walking}})}  
\
 \mathcal{C}=-\frac{1}{N}\sum_{i=1}^N\log p(T^{(i)})+\lambda_\ModParms\cdot||\ModParms||_2^2
=\langle\rangleT_{\mbox{REL}}T_{\mbox{KVC}}\{ \mbox{\texttt{?X,?Y}} \} = \mbox{\texttt{Query(,,)}}IT_{\mbox{KVC}}T_{\mbox{REL}}IIIT_{\mbox{KVC}}T_{\mbox{KVC}}T_{\mbox{REL}}T_{\mbox{REL}}\{\}T_{\mbox{REL}}4216T_{\mbox{REL}}\{\}3T_{\mbox{AS}}T_{\mbox{AS}}T_{\mbox{AS}}T_{\mbox{AS}}sT_{\mbox{REL}}\mathcal{W}_{\mathrm{Y}}\mathcal{W}_{\mathrm{Q}}T_{\mbox{AS}}T_{\mbox{KVC}}=T_{\mbox{KVC}}=1001280.001100.550<>82.42\pm0.56\pm61.73\pm1.8385.56\pm2.1464.10\pm1.1080.85\pm0.8783.15\pm3.0395.24\pm1.2364.94\pm1.0882.42\pm0.56131310101010130.90.050.0\pm10.37\pm0.8020.72\pm0.5834.63\pm1.1918.41\pm1.0732.42\pm1.0647.53\pm1.0218.89\pm0.9132.78\pm0.9048.13\pm0.7310.45\pm0.5719.02\pm0.7431.64\pm0.9320.55\pm0.8136.01\pm1.4555.74\pm2.28 22.97\pm0.6436.76\pm1.2254.19\pm2.4524.98\pm0.6040.40\pm1.0557.27\pm1.2933.70\pm1.1850.00\pm0.7864.08\pm0.5743.14\pm0.6159.44\pm0.34\mathbf{72.20\pm0.39}\ddagger63.63\pm0.7371.30\pm0.7872.55\pm0.7952.56\pm1.0359.72\pm0.8260.58\pm0.86\mathbf{56.91\pm0.99}\mathbf{64.65\pm1.05}{65.54\pm1.06}58.76\pm0.9277.99\pm0.75\ddagger\pm17.06\pm1.0929.43\pm0.6244.73\pm1.0724.73\pm1.2940.95\pm1.3456.33\pm0.6325.30\pm1.0941.37\pm1.1956.78\pm0.6415.82\pm0.5726.45\pm0.6140.99\pm1.0226.78\pm1.0244.00\pm1.6162.86\pm2.2329.08\pm0.9144.36\pm1.2961.71\pm2.8231.96\pm0.6548.55\pm0.9764.73\pm1.3839.75\pm0.7856.48\pm0.6869.78\pm0.5148.93\pm0.7164.75\pm0.44\mathbf{76.73\pm0.38}\ddagger65.51\pm0.8272.37\pm0.8973.55\pm0.8754.79\pm0.9161.41\pm0.7162.22\pm0.70\mathbf{59.67\pm0.90}\mathbf{66.89\pm1.01}{67.77\pm1.04}82.47\pm0.71\ddagger\pm56.88\pm2.5768.45\pm0.6776.93\pm0.7559.64\pm0.7273.30\pm0.9681.77\pm0.3359.97\pm0.6373.39\pm0.8881.93\pm0.3951.45\pm1.4165.65\pm0.6675.76\pm0.6259.53\pm1.5273.83\pm0.5083.53\pm0.8961.86\pm0.8174.45\pm0.5983.54\pm0.9463.42\pm0.3576.63\pm0.50{84.94\pm0.57}66.11\pm0.6278.90\pm0.5086.32\pm0.5571.51\pm0.16\mathbf{82.71\pm0.39}\mathbf{89.01\pm0.50}\ddagger73.98\pm0.7778.67\pm0.7879.98\pm0.7964.96\pm0.7069.57\pm0.6570.64\pm0.59\mathbf{72.34\pm0.65}{77.52\pm0.70}78.69\pm0.6387.30\pm0.54\ddagger409630050093.68\%1310C1C50040965120.00150.51310015310513\pm4.13\pm0.899.94\pm1.4420.91\pm2.0811.03\pm0.7621.84\pm0.8335.89\pm1.3116.38\pm4.3831.40\pm0.9350.17\pm2.987.11\pm0.7319.40\pm2.5039.75\pm2.2320.23\pm1.1433.91\pm1.5748.15\pm1.1820.95\pm2.1743.32\pm1.2357.31\pm2.717.35\pm0.7320.43\pm2.3240.38\pm2.4620.76\pm0.9534.18\pm1.3548.64\pm1.0821.40\pm2.0143.33\pm1.4559.58\pm2.925.08\pm0.5411.17\pm0.2123.68\pm2.4210.96\pm0.6019.70\pm0.8732.02\pm0.9616.37\pm2.8728.31\pm2.1144.91\pm1.3214.62\pm2.3829.68\pm4.3549.74\pm2.5620.96\pm0.9636.54\pm1.2156.32\pm2.3128.82\pm2.7943.64\pm5.4962.04\pm3.8715.77\pm2.0728.30\pm3.5649.45\pm7.2523.57\pm0.5237.36\pm1.4354.30\pm2.9931.74\pm3.6948.18\pm5.8063.59\pm5.6015.38\pm1.1132.64\pm2.5851.80\pm3.1225.97\pm0.7041.02\pm1.3157.35\pm1.4034.42\pm2.6850.27\pm2.22{68.56\pm3.19}27.65\pm1.3445.71\pm1.1662.44\pm1.6334.00\pm1.3350.01\pm0.5364.05\pm0.4643.74\pm4.4859.46\pm4.6067.95\pm4.4338.39\pm1.3356.07\pm1.05\mathbf{71.60\pm1.82}43.23\pm0.7659.47\pm0.43\mathbf{72.21\pm0.77}\mathbf{52.85\pm5.10}\mathbf{66.49\pm3.77}\mathbf{73.40\pm2.35}\ddagger65.96\pm2.0678.80\pm1.0879.43\pm1.1164.62\pm0.8271.75\pm0.9473.17\pm0.9445.55\pm1.1448.29\pm1.0848.86\pm1.2951.25\pm2.2163.07\pm1.2363.13\pm1.2153.50\pm1.1460.16\pm0.8761.20\pm0.9643.54\pm2.1446.58\pm1.8047.03\pm2.08\mathbf{56.67\pm1.68}\mathbf{69.31\pm1.10}{69.36\pm1.03}\mathbf{57.60\pm1.29}\mathbf{64.70\pm1.24}{65.77\pm1.28}{48.74\pm2.47}{53.45\pm1.99}53.90\pm2.2657.08\pm2.0258.98\pm1.0459.77\pm5.5274.41\pm1.1378.32\pm0.7681.95\pm1.83\ddagger\pm11.39\pm0.8422.72\pm0.5337.89\pm1.060.68\pm0.241.98\pm0.833.97\pm0.740.00\pm0.000.00\pm0.000.00\pm0.0020.08\pm1.2535.33\pm1.1351.60\pm1.112.88\pm0.465.22\pm0.859.60\pm0.360.00\pm0.000.00\pm0.000.00\pm0.0020.59\pm1.0435.73\pm1.0652.24\pm0.793.02\pm0.425.22\pm0.849.83\pm0.190.00\pm0.000.00\pm0.000.00\pm0.0011.39\pm0.5220.70\pm0.6334.34\pm0.921.58\pm1.073.05\pm0.676.03\pm1.090.00\pm0.003.53\pm4.714.71\pm5.7622.34\pm0.9839.09\pm1.4960.13\pm2.183.80\pm0.927.07\pm1.4914.94\pm1.430.00\pm0.001.05\pm2.111.05\pm2.1124.92\pm0.8839.71\pm1.4858.32\pm3.824.56\pm0.729.21\pm2.0015.56\pm1.625.99\pm6.645.99\pm6.649.52\pm6.1526.97\pm0.5343.40\pm0.7761.30\pm0.956.46\pm1.0112.27\pm1.1719.98\pm2.102.35\pm2.888.35\pm7.208.34\pm7.2036.35\pm1.2553.83\pm1.0068.51\pm0.619.09\pm0.4614.36\pm1.3022.54\pm1.271.18\pm2.356.52\pm5.4217.38\pm3.8146.38\pm0.8163.65\pm0.55\mathbf{76.81\pm0.48}12.72\pm0.20\mathbf{19.64\pm1.53}\mathbf{28.92\pm2.74}8.11\pm8.0219.08\pm5.0721.43\pm3.67\ddagger68.70\pm0.7776.50\pm0.6977.11\pm0.7114.81\pm0.8320.93\pm1.5427.75\pm2.0328.78\pm5.7239.76\pm6.6153.32\pm10.4756.75\pm1.4864.11\pm1.3364.47\pm1.3512.81\pm1.2918.36\pm2.6824.19\pm3.5216.58\pm10.1419.57\pm12.3822.57\pm14.32\mathbf{61.53\pm1.31}\mathbf{69.51\pm1.35}{69.90\pm1.20}\mathbf{12.89\pm1.29}{18.51\pm2.72}{24.34\pm3.57}\mathbf{19.75\pm6.16}\mathbf{22.73\pm8.34}\mathbf{25.72\pm10.12}63.38\pm1.1915.13\pm1.2414.39\pm3.4180.58\pm0.4252.70\pm2.0668.23\pm4.90\ddagger\pm12.38\pm0.8824.68\pm0.4741.05\pm1.050.78\pm0.232.00\pm0.474.16\pm0.7921.81\pm1.3038.27\pm1.3355.93\pm1.292.30\pm0.254.73\pm0.627.74\pm0.5122.38\pm1.0938.72\pm1.2056.63\pm0.972.38\pm0.304.65\pm0.637.86\pm0.4412.35\pm0.5722.45\pm0.6837.06\pm0.981.45\pm0.682.72\pm0.635.89\pm0.9524.19\pm0.9842.40\pm1.7364.92\pm2.443.31\pm0.745.69\pm0.8711.87\pm0.7126.98\pm1.0842.90\pm1.5862.87\pm4.104.03\pm0.957.70\pm1.3013.11\pm1.5128.97\pm0.6446.62\pm0/8565.83\pm0.976.13\pm1.0110.94\pm1.2416.73\pm1.2339.20\pm1.3657.80\pm1.0773.41\pm0.558.16\pm0.3913.81\pm1.4620.78\pm1.1149.93\pm1.0868.21\pm0.67\mathbf{82.08\pm0.56}11.61\pm0.9518.69\pm1.55\mathbf{26.25\pm1.51}\ddagger73.69\pm0.6781.04\pm0.5181.04\pm0.5115.95\pm0.6425.17\pm0.8932.39\pm1.1661.11\pm1.4868.31\pm1.2468.34\pm1.2212.12\pm1.0119.13\pm1.3023.91\pm1.39\mathbf{66.32\pm1.20}\mathbf{74.11\pm1.21}{74.15\pm1.18}\mathbf{12.39\pm1.09}\mathbf{19.87\pm1.35}{24.81\pm1.31}68.15\pm0.9715.18\pm1.2282.97\pm0.3854.47\pm1.77\ddagger3T_{\mbox{REL}}T_{\mbox{KVC}}T_{\mbox{AS}}3T_{\mbox{REL}}T_{\mbox{KVC}}T_{\mbox{AS}}158.76\%31010\pm\ddagger56.31\pm0.8962.55\pm0.9663.55\pm0.9638.76\pm0.8842.96\pm0.7843.60\pm0.77\mathbf{41.12\pm0.74}\mathbf{45.49\pm0.89}\mathbf{46.13\pm0.87}\ddagger51N41\%$ chance to predict the correct one from millions of facts in the incorporated Knowledge Bases.







\section{Conclusion}
\label{conclusion}

In this work, we have built a new dataset and an approach for the task of visual question answering with external commonsense knowledge.
The proposed \KBName dataset differs from existing VQA datasets in that it provides a supporting-fact which is critical for answering each visual question.
We have also developed a novel VQA approach, which is able to automatically find the supporting-fact
for a visual question from large-scale structured knowledge bases.
Instead of directly learning the mapping from questions to answers, our approach learns the mapping from questions to KB-queries,
so it is much more scalable to the diversity of answers.
Not only give the answer to a visual question, the proposed method also provides the supporting-fact based on which it arrives at the answer,
which uncovers the reasoning process.










{
\bibliographystyle{IEEEtran}
\bibliography{CSRef}
}


\begin{IEEEbiographynophoto}{Peng Wang}
	is a Professor at Northwestern Polytechnical University, China. He was with 
	the Australian Centre for Visual Technologies (ACVT) of the University of Adelaide from 2012 to 2017. He received a Bachelor in electrical engineering and automation, and a PhD in control science and engineering from Beihang University (China) in 2004 and 2011, respectively.
\end{IEEEbiographynophoto}


\begin{IEEEbiographynophoto}{Qi Wu}
	is a postdoctoral researcher at the University of Adelaide. His research interests include cross-depiction object detection and classification, attributes learning, neural networks, and image captioning. He received a Bachelor in mathematical sciences from China Jiliang University, a Masters in Computer Science, and a PhD in computer vision from the University of Bath (UK) in 2012 and 2015, respectively.
\end{IEEEbiographynophoto}

\begin{IEEEbiographynophoto}{Chunhua Shen}
	is a Professor of computer science at the University of Adelaide. He was with the computer vision program at NICTA (National ICT Australia) in Canberra for six years before moving back to Adelaide. He studied at Nanjing University (China), at the Australian National University, and received his PhD degree from the University of Adelaide. In 2012, he was awarded the Australian Research Council Future Fellowship.
\end{IEEEbiographynophoto}

\begin{IEEEbiographynophoto}{Anthony Dick}
	is an Associate Professor at the University of Adelaide. He
	received a PhD degree from the University of Cambridge in 2002, where he worked on 3D reconstruction of architecture from images. His research interests include image-based modeling, automated video surveillance, and image search.
\end{IEEEbiographynophoto}

\begin{IEEEbiographynophoto}{Anton van den Hengel}
	is a Professor at the University of Adelaide and the founding Director of The Australian Centre for Visual Technologies (ACVT). He received a PhD in Computer Vision in 2000, a Master Degree in Computer Science in 1994, a Bachelor of Laws in 1993, and a Bachelor of Mathematical Science in 1991, all from The University of Adelaide.
\end{IEEEbiographynophoto}








\end{document}
