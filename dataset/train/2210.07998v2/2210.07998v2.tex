
\documentclass{article} \usepackage{fancyhdr, iclr2023_conference, times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{$\eta$}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{wrapfig}
\usepackage{xspace} \usepackage[subtle]{savetrees}


\newcommand{\aiTodo}[1]{\textcolor{blue}{[AI:] #1}}
\newcommand{\md}[1]{\textcolor{red}{[MD:] #1}}
\newcommand{\saj}[1]{\textcolor{violet}{[SAJ:] #1}}
\newcommand{\mydarts}{$\Lambda$-DARTS\xspace}


\newtheorem{prop}{Proposition}

\title{\mydarts: Mitigating Performance Collapse by Harmonizing Operation Selection among Cells}

\newcommand{\lambdafn}{layer alignment\xspace}





\author{\normalfont \hspace{-5pt}\begin{tabular}{@{}lll@{}}
\\
\textbf{Sajad Movahedi}\textsuperscript{1}\textbf{, Melika Adabinejad}\textsuperscript{1}\textbf{, Ayyoob Imani}\textsuperscript{2}\textbf{, Arezou Keshavarz}\textsuperscript{1}\textbf{, Mostafa Dehghani}\textsuperscript{3}\\
\textbf{Azadeh Shakery}\textsuperscript{1, 4}\textbf{ and Babak N. Araabi}\textsuperscript{1}\\
\textsuperscript{1}University of Tehran, \textsuperscript{2}LMU Munich, \textsuperscript{3}Google Brain, \\\textsuperscript{4} Institute for Research in Fundamental Sciences (IPM)
\\
\texttt{\{s.movahedi, melika.adabi\}@ut.ac.ir, ayyoob.imani@cis.lmu.de}\\
\texttt{arezou@keshavarz.net,
dehghani@google.com,
\{shakery, araabi\}@ut.ac.ir}
\end{tabular}
}





\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}
\setlength\parfillskip{0pt plus .75\textwidth}
\setlength\emergencystretch{1pt}

\maketitle

\begin{abstract}
Differentiable neural architecture search (DARTS) is a popular method for neural architecture search (NAS), which performs cell-search and utilizes continuous relaxation to improve the search efficiency via gradient-based optimization. The main shortcoming of DARTS is performance collapse, where the discovered architecture suffers from a pattern of declining quality during search. Performance collapse has become an important topic of research, with many methods trying to solve the issue through either regularization or fundamental changes to DARTS.
However, the weight-sharing framework used for cell-search in DARTS and the convergence of architecture parameters has not been analyzed yet. In this paper, we provide a thorough and novel theoretical and empirical analysis on DARTS and its point of convergence.
We show that DARTS suffers from a specific structural flaw due to its weight-sharing framework that limits the convergence of DARTS to saturation points of the softmax function. This point of convergence gives an unfair advantage to layers closer to the output in choosing the optimal architecture, causing performance collapse. We then propose two new regularization terms that aim to prevent performance collapse by harmonizing operation selection via aligning gradients of layers. 
Experimental results on six different search spaces and three different datasets show that our method (\mydarts) does indeed prevent performance collapse, providing justification for our theoretical analysis and the proposed remedy. We have published our code at \href{https://github.com/dr-faustus/Lambda-DARTS}{\texttt{https://github.com/dr-faustus/Lambda-DARTS}}.
\end{abstract}
\section{Introduction}
\par With the growth of the popularity of deep learning models, neural architecture design has become one of the most important challenges of machine learning. Neural architecture search (NAS), a now prominent branch of AutoML, aims to perform neural architecture design in an automatic way~\citep{DBLP:journals/jmlr/ElskenMH19, DBLP:journals/csur/RenXCHLCW21}. Initially, the problem of NAS was addressed through reinforcement learning or evolutionary algorithms by the seminal works of~\citep{DBLP:conf/iclr/ZophL17, DBLP:conf/iclr/BakerGNR17, DBLP:journals/ec/StanleyM02, DBLP:conf/icml/RealMSSSTLK17}. But despite the recent advancements in efficiency ~\citep{DBLP:conf/iclr/BakerGRN18, DBLP:conf/eccv/LiuZNSHLFYHM18, DBLP:conf/aaai/CaiCZYW18}, these methods remain impractical and not widely accessible. One-shot methods aim to address this impracticality by performing the architecture search in a one-shot and end-to-end manner~\citep{DBLP:conf/iclr/LiuSY19,DBLP:conf/eccv/GuoZMHLWS20,DBLP:conf/icml/PhamGZLD18,DBLP:conf/iclr/BrockLRW18,DBLP:conf/icml/BenderKZVL18}. Another technique for increasing the efficiency of search is cell-search~\citep{DBLP:conf/cvpr/ZophVSL18}, which performs NAS for a set of cells stacked on top of each other according to a pre-defined macro-architecture. 
Differentiable neural architecture search (DARTS)~\citep{DBLP:conf/iclr/LiuSY19} is a one-shot method that performs cell-search using gradient descent and continuous relaxation. It performs the cell-search using a weight-sharing framework. As a result of these innovations, DARTS is one of the most efficient methods of architecture search~\citep{DBLP:journals/jmlr/ElskenMH19}.
\par One of the most severe issues of DARTS is the performance collapse problem~\citep{DBLP:conf/iclr/ZelaESMBH20}, which states that the quality of the architecture selected by DARTS gradually degenerates into an architecture solely consisting of skip-connections. This issue is usually attributed to gradient vanishing~\citep{DBLP:conf/nips/ZhouXSH20} or large Hessians of the loss function~\citep{DBLP:conf/iclr/ZelaESMBH20}, and mostly addressed by heavily modifying DARTS~\citep{DBLP:conf/iclr/ChuW0LWY21,DBLP:conf/iclr/WangCCTH21, DBLP:conf/cvpr/GuW0YWLC21,DBLP:journals/corr/abs-2203-01665}.




\begin{figure*}[tb]
\centering
\vspace{-15pt}
\includegraphics[width=0.95\linewidth]{figures/teaser_final.pdf}
\vspace{-5pt}
\caption{An illustration of the differentiable formulation for NAS by DARTS, and the effects of \lambdafn over the trajectory of the performance of the discovered architectures by DARTS and $\Lambda$-DARTS. The experiments are performed on the NAS-Bench-201 search space, averaged over 4 runs with a $95\%$ confidence interval.}
\vspace{-15pt}
\label{fig:teaser}
\end{figure*}


In this paper, we argue that these theories fail to grasp the main reason behind the performance collapse: the conditions imposed on the convergence of DARTS by the weight-sharing framework. Specifically, we first define a measure to show the correlation between the gradient of each layer corresponding to the architecture parameters - dubbed \textit{\lambdafn} ($\Lambda$). Then, following a careful analysis of the convergence conditions of DARTS and the effects of weight-sharing, we find that:
\begin{enumerate}[itemsep=0em]
    \item Due to the value of $\Lambda$, DARTS can only achieve convergence by reaching the saturation point of the softmax function, where the normalized architecture parameters become almost one-hot vectors. This convergence point does not depend directly on the loss function, giving an unfair advantage to the layers that do not suffer from gradient vanishing.
    \item Low value for $\Lambda$ means that the optimal architecture corresponding to each layer varies wildly, with layers that are closer to the output - and therefore not suffering from gradient vanishing - mainly preferring non-parametric operations.
    \item The aforementioned issues are direct contributors to the performance collapse problem. Furthermore, increasing the depth of the search model or the number of iterations used for the search increases the severity of performance collapse.
\end{enumerate}
In Figure-\ref{fig:teaser}, we can see an illustration of DARTS and the concept of \lambdafn (which we will define in Section-\ref{sec:wsi}), and observe a clear correlation between the \lambdafn and the performance of the discovered architecture on the NAS-Bench-201 search space~\citep{DBLP:conf/iclr/Dong020}. In this paper, we will use a combination of analytical and empirical evidence to support the claim that this relationship is in fact a causal relationship, resulting from the weight-sharing framework used in DARTS. 


\par Following these conclusions, we introduce a regularization term that aims to alleviate the problem of performance collapse by increasing the \lambdafn. 
Then, through a comprehensive empirical examination, we show that our method - dubbed \mydarts~- significantly improves the performance of DARTS on various search spaces and datasets without any modification to the algorithm of DARTS or the structure of the cells. Specifically, our method achieves an average accuracy of $96.57\%$ and $83.85\%$ on the CIFAR-10 and CIFAR-100 datasets on the DARTS search space, improving upon the current state-of-the-art by $0.06\%$ and $0.39\%$, respectively. To the best of our knowledge, our work is the first to investigate DARTS from the point-of-view of its \textit{wight-sharing framework} and \textit{convergence conditions}.


\section{Related Work}
DARTS~\citep{DBLP:conf/iclr/LiuSY19} proposed a continuous and differentiable search space through weighting a fixed set of operations to make NAS more scalable. It trains a super-graph with gradient descent and chooses the sub-graph consisted of weightiest operation edges. Its simplicity made it very popular and many variations emerged to address its theoretical and empirical setbacks:

\textbf{Overlooked effects of design decisions in DARTS.}
\citep{DBLP:conf/iclr/WangCCTH21} and~\citep{DBLP:conf/cvpr/GuW0YWLC21} claim that DARTS chooses the final architecture with regard to operation weights, whereas these magnitudes do not necessarily correlate with the performance.~\citep{DBLP:conf/iclr/WangCCTH21} tries to measure operation impact directly and DOTS~\citep{DBLP:conf/cvpr/GuW0YWLC21} tries to decouple topology from operation during search. IDARTS~\citep{DBLP:conf/iccv/XueW0WGD21} posits that the innate relationship between the architectureâ€™s parameters is ignored by the gradient descent method used in DARTS and formulates their optimization as a bi-linear problem.

\textbf{Generalization ability.} The differentiable NAS methods are often executed on a smaller dataset of a certain task to reduce the required computational resources. AdaptNAS~\citep{DBLP:conf/nips/LiYW020}, MixSearch~\citep{DBLP:journals/corr/abs-2102-13280}, P-DARTS~\citep{DBLP:journals/ijcv/ChenXWT21}, and DrNAS~\citep{DBLP:conf/iclr/ChenWCTH21} show that the discovered architecture does not always perform as well as on the proxy task on another more challenging dataset. They solve this issue by providing the network with domain information, making the search network progressively deeper, and reformulating the search algorithm.
\par \textbf{Performance collapse.} \citet{DBLP:conf/iclr/ZelaESMBH20,DBLP:conf/iclr/ChuW0LWY21} point out the performance collapse problem of DARTS. They show that DARTS consistently discovers networks with mainly skip-connection operations, which causes severe performance declination.~\citet{DBLP:conf/iclr/ZelaESMBH20} monitors the eigenvalues of the Hessian of the architecture parameters and performs early stopping. Based on their work, SmoothDARTS~\citep{DBLP:conf/icml/ChenH20} performs Hessian regularization to smooth the architecture Hessian. PC-DARTS~\citep{DBLP:conf/iclr/XuX0CQ0X20} reduces the number of output channels of each operation, for the sake of lower memory consumption and a larger batch size during search. iDARTS~\citep{DBLP:conf/icml/ZhangSPCAH21} focuses on improving the optimization of the architecture parameters by estimating the descent direction more accurately. DARTS-~\citep{DBLP:conf/iclr/ChuW0LWY21} argues that skip-connection operations have a potentially sharp loss curvature that leads to their advantage during search, and tries to solve it by adding auxiliary skip-connections to cell structure. $\beta$-DARTS~\citep{DBLP:journals/corr/abs-2203-01665} replaces the $\ell^2$-regularization on the architecture parameters with a new regularization term to alleviate the unfair advantage of skip-connections. 
In this paper, we provide a novel analysis of DARTS from the point-of-view of its convergence, diagnosing an overlooked issue of its weight-sharing system that sends convergence down an irrelevant path and may be the root cause of performance collapse. We propose a solution for the matter without modifying the core formulation of DARTS.

\section{An Analysis on the Convergence of DARTS}
We start with an analysis of the weight-sharing framework used for cell-search in DARTS, and the required conditions for its convergence. We then empirically investigate whether DARTS can satisfy any of these conditions, and show that it is only capable of reaching convergence under one of them. Finally, using these observations, we will show that this type of convergence causes performance collapse, providing a motivation for \mydarts. For the uninitiated reader, we provide a brief explanation of DARTS and the weight-sharing framework used by it for cell-search in Appendix-\ref{appndx:DARTS}.
\subsection{The convergence of DARTS}
Assuming the primary loss function $\mathcal{L}(., .)$ to be differentiable, we can write the necessary and sufficient condition for optimality of the architecture parameters, ($\alpha$) as~\citep{DBLP:books/cu/BV2014}:
\begin{equation}
\label{eqn:optimality}
    \nabla_\alpha \mathcal{L}^{val}(\omega, \alpha^*)=0,
\end{equation}
where $\omega$ represents the parameters corresponding to the operations, and $\mathcal{L}^{val}(., .)$ is the loss function calculated over the validation set. Let $\vp=\sigma(\alpha)$ be the softmax normalized architecture parameters, where $\sigma(.)$ corresponds to the softmax function. Then we can re-write (\ref{eqn:optimality}) as:
\begin{equation}
    \mJ_\sigma (\alpha^*) \nabla_\vp \mathcal{L}(\omega, \alpha^*) = 0,
\end{equation}
where $\mJ_\sigma(\alpha)=\text{diag}(\sigma(\alpha))-\sigma(\alpha)\sigma(\alpha)^T$ corresponds to the Jacobian of the softmax function on all edges, and $\text{diag}(.)$ being the diagonal matrix. Note that given the weight-sharing framework, we can write $\nabla_\vp\mathcal{L}(\omega, \alpha^*)=\frac{\partial \mathcal{L}}{\partial p}(\omega, \alpha^*)$ as the sum of gradients received by each layer:
\begin{equation}
    \mJ_\sigma (\alpha^*) \left(\sum_{\ell=1}^L \nabla_{{}^\ell \vp}\mathcal{L}(\omega, \alpha^*)\right) = 0.
\end{equation}
Now note that $\mJ_\sigma(.)$ is a block-diagonal matrix, with each block corresponding to one of the edges in the architecture. Therefore, $\mJ_\sigma(.)$ is a positive semi-definite matrix with its null-space containing vectors of the following form:
\begin{equation}
\label{eqn:null-space}
    \vv=\begin{bmatrix}
    \vv_1^T&\vv_2^T&\dots&\vv_{\vert E\vert}^T
    \end{bmatrix}^T,
\end{equation} 
where the vector $\vv$ consists of concatenated vectors $\vv_i\in \mathbb{R}^{|E|}$ of the form $\vv_i^T\in \{c_i\cdot\begin{bmatrix}1&1&\dots&1\end{bmatrix}^T~|~c_i\in \mathbb{R}\}$~\citep{DBLP:journals/corr/abs-1704-00805}. Considering the diverse set of operations used in DARTS, it is not an unfair assumption that $\nabla_\vp\mathcal{L}(\omega, \alpha)$ won't be in the form presented in (\ref{eqn:null-space}). In fact, we have seen empirical evidence to support this claim, which will be discussed in Section-\ref{sec:wsi}. So assuming that $\nabla_\vp\mathcal{L}(\omega, \alpha)$ does not belong to the null space of $\mJ_\sigma(\alpha)$, satisfying (\ref{eqn:optimality}) can only be achieved in two ways:
\begin{enumerate}
    \item $\nabla_\vp \mathcal{L}(\omega, \alpha^*)$ approaching $0$.
    \item The Jacobian $\mJ_\sigma(\alpha^*)$ approaching $0$.
\end{enumerate}
\subsection{The \lambdafn issue}
\label{sec:wsi}
\begin{figure}[t]
    \vspace{-10pt}
    \centering
    \includegraphics[width=0.75\textwidth]{figures/grad_plot.jpg}
    \vspace{-5pt}
    \caption{The exponential average (with decay rate $0.999$) of the sum of the gradients of the $3\times3$ convolution and skip-connection operations for the first 5 layers and the last 5 layers averaged over all 6 edges of the cell on the NAS-Bench-201 search space, averaged over 4 runs with a $95\%$ confidence interval. Note that negative values in the gradient mean the value of the parameter corresponding to the operation will be increasing.}
    \label{fig:grads-b}
    \vspace{-10pt}
\end{figure}

Assuming that $\forall \ell:\quad \nabla_{{}^\ell \vp}\mathcal{L}(\omega, \alpha^*)\neq0$, a necessary condition for $\nabla_\vp\mathcal{L}(\omega, \alpha^*)$ to be able to approach zero is that for each $\ell$ we have $\exists {\ell'\neq \ell}:\quad \nabla_{{}^\ell \vp}\mathcal{L}(\omega, \alpha^*) \not\perp \nabla_{{}^{\ell'} \vp}\mathcal{L}(\omega, \alpha^*)$. Because otherwise, we can write $\nabla_\vp \mathcal{L}(\omega, \alpha^*)^T\nabla_{{}^\ell \vp}\mathcal{L}(\omega, \alpha^*)=\Vert \nabla_{{}^\ell \vp}\mathcal{L}(\omega, \alpha^*)\Vert_2^2\neq 0$. Empirically, we noticed that this is not the case in DARTS. More specifically, we define the function $\Lambda$ - which we dub \textit{\lambdafn} - as:
\begin{equation}
    \Lambda(\omega, \alpha)=\frac{1}{\binom{L}{2}} \sum_{\ell< \ell'} \frac{\nabla_{{}^\ell \vp}\mathcal{L}(\omega, \alpha)^T\nabla_{{}^{\ell'} \vp}\mathcal{L}(\omega, \alpha)}{\Vert \nabla_{{}^\ell \vp}\mathcal{L}(\omega, \alpha)\Vert_2 \Vert \nabla_{{}^{\ell'} \vp}\mathcal{L}(\omega, \alpha) \Vert_2}\text{.}
\end{equation}
We noticed that regardless of the search space, the \lambdafn is always near zero, i.e. the layer gradients are almost orthogonal. In Figure~\ref{fig:teaser}, we can see the value of $\Lambda(., .)$ on the NAS-Bench-201 search space~\citep{DBLP:conf/iclr/Dong020} for both DARTS and \mydarts. In Appendix-\ref{appndx:cause-of-lambdafn}, we try to detect the reason behind the low layer alignment issue.
\par As a result of this orthogonality, we expect that under the aforementioned assumptions DARTS is incapable of reaching convergence the first way, since it is likely incapable of satisfying the necessary condition for it. So we will propose a lower-bound on the value $\Vert \nabla_\alpha \mathcal{L}(\omega, \alpha)\Vert_2^2$, which formalizes our claim:
\begin{prop}
\label{prop:1}
Let $\nabla_\alpha \mathcal{L}(\omega, \alpha)$ be orthogonal to the null space of $\mJ_\sigma(\alpha)$, and assuming we have $L$ layers of cells in our search network, we have $\forall \ell\neq\ell'\quad \nabla_{{}^\ell \vp}\mathcal{L}(\omega, \alpha)\perp\nabla_{{}^{\ell'} \vp}\mathcal{L}(\omega, \alpha)$. Let $\lambda_i(\alpha)>0$ be the set of eigenvalues corresponding to $\mJ_\sigma(\alpha)$. Then the square norm of the gradient $\nabla_\alpha \mathcal{L}(\omega, \alpha)$ can be bounded by:
\begin{equation}
\label{eqn:prop}
    \Vert \nabla_\alpha \mathcal{L}(\omega, \alpha)\Vert_2^2\geq \min_{i, \lambda_i(\alpha)\neq 0} \lambda_i(\alpha)^2\cdot L\cdot \min_\ell \Vert \nabla_{{}^\ell \vp}\mathcal{L}(\omega, \alpha)\Vert_2^2.
\end{equation}
\end{prop}
We provide a proof for this proposition in Appendix-\ref{appndx:proof}. This proposition shows that for any value of $\min_\ell \Vert \nabla_{{}^\ell \vp}\mathcal{L}(\omega, \alpha)\Vert_2^2$, we have a large enough value for $L$ that prevents the right-hand-side of (\ref{eqn:prop}) to become too small. As a result, for a deep enough search network, the only way for $\alpha$ to reach convergence is through reaching the softmax saturation point, wherein we have near zero eigenvectors corresponding to $\mJ_\sigma(\alpha)$, and the normalized architecture parameters become close to one-hot vectors. This phenomenon has been observed in the literature~\citep{DBLP:journals/corr/abs-1910-11831, DBLP:conf/iclr/Dong020}, where DARTS usually converges to a softmax saturation point with all of the edges selecting the skip-connection or none operations, depending on the search space. In Figure-\ref{fig:abschange}, we can see this effect, where the absolute changes in the softmax normalized architecture weights of DARTS during search is rapidly growing, compared to \mydarts which reaches a plateau during search. Note that converging to a softmax saturation point is not an inherently bad outcome, because it corresponds to near zero optimality gap when using continuous relaxation~\citep{DBLP:books/cu/BV2014}. But as we will see, this characteristic gives an advantage to layers closer to the output in selecting the optimal architecture, contributing to performance collapse~\citep{DBLP:conf/iclr/ZelaESMBH20}.
\subsection{The performance collapse}
\par Another issue caused by low \lambdafn is that the optimal architectures corresponding to each layer are going to be vastly different. In Figure~\ref{fig:grads-b} we can see the exponential average of the sum of the gradients for convolution and skip-connection operations in the first 5 layers (furthest from the output of the network) and the last 5 layers (closest to the output of the network) averaged over all edges in a search cell on the NAS-Bench-201 search space~\citep{DBLP:conf/iclr/Dong020}. As evident, the optimal cell corresponding to the shallower layers (closer to the input) mostly contains convolution operations, while the optimal cell corresponding to the deeper layers (closer to the output) mostly contains skip-connection operations. We attribute this observation to skip-connections having a proclivity towards allowing the gradients to flow further into the network, while convolutions provide more complex features for upper layers. Furthermore, due to gradient vanishing, the gradients corresponding to the skip-connection operations on average have larger magnitudes, with the magnitude of the layer gradients gradually becoming smaller the deeper we go. As a result, $\nabla_\alpha \mathcal{L}(\omega, \alpha)$ will have much smaller negative values corresponding to the skip-connection operations.
\par \begin{wrapfigure}{r}{0.4\textwidth}
  \vspace{-23pt}
  \begin{center}
    \includegraphics[width=0.4\textwidth]{figures/abschange_plot.pdf}
  \end{center}
  \vspace{-15pt}
  \caption{The $\ell^1$ norm of changes in softmax-normalized architecture weights, averaged over 4 runs with a $95\%$ confidence interval. The search is performed on CIFAR-10.}
  \vspace{-10pt}
  \label{fig:abschange}
\end{wrapfigure}Using these observations, we can clearly bridge the analytical gap between the performance collapse problem and the \lambdafn. Due to the weight-sharing structure, the value of the function $\Lambda(\omega, \alpha)$ is close to zero. As a result, DARTS can only achieve convergence in the softmax saturation points. Furthermore, given the low \lambdafn, the optimal architecture from the perspective of the layers are going to be extremely varied, with deeper layers having a strong inclination towards skip-connections. This gives skip-connections a clear advantage due to the larger magnitude of their gradients. So as the search progresses, the architecture parameters corresponding to the skip-connection operations become larger than the convolution operations, eventually resulting in a saturation point that overwhelmingly selects skip-connection operations~\citep{DBLP:journals/corr/abs-1910-11831, DBLP:conf/iclr/ChuW0LWY21}. As the depth of the search network increases, vanishing gradient becomes more severe, further exacerbating the issue~\citep{DBLP:conf/cvpr/HeZRS16}. This is evident when comparing the results of DARTS on its original search space and the NAS-Bench-201 search space, which utilizes deeper search networks~\citep{DBLP:conf/iclr/Dong020}. Similarly, increasing the number of iterations used for the search results in further exacerbation of the performance collapse problem~\citep{DBLP:journals/corr/abs-1910-11831}, which can be attributed to the severity of vanishing gradient increasing as the loss function approaches its optimal point~\citep{DBLP:conf/cvpr/HeZRS16}. These observations motivate us to alleviate both of these issues caused by low \lambdafn through increasing the value of $\Lambda(\omega, \alpha)$. In Appendix-\ref{sec:appndx-priorwork} we provide a thorough analysis that connects our method to some of the prior works.
\section{The Proposed Method to Mitigate Performance Collapse}
In the previous segment, we saw that the main contributor to the performance collapse problem is low \lambdafn (i.e. $\Lambda(\omega, \alpha)\approx 0$). Now we will introduce two regularization terms based on this value to solve performance collapse. Furthermore, we will introduce a method to estimate the gradient corresponding to the regularization terms that grow linearly with the size of the network.
\subsection{The regularization term}
For the sake of simplicity, let's assume we aim to search for a single cell. To increase the value of $\Lambda(\omega, \alpha)$, we can directly use the function itself, since it is a differentiable function. So we define our main regularization term as the function $\Lambda(\omega, \alpha)$, which we aim to maximize.
\par In our preliminary experiments, we noticed that in case the dimension of $\alpha$ is large, $\Lambda(\omega, \alpha)$ only concentrates on the operations with relatively larger gradients. This means that while the angle between the gradients of the layers is reduced, only the gradient of a small portion of the operations will have the same sign, resulting in lower performance. So given ${}^\ell g=\nabla_{{}^\ell \vp}\mathcal{L}(\omega, \alpha)$ and ${}^{\ell'}g=\nabla_{{}^{\ell'} \vp}\mathcal{L}(\omega, \alpha)$, we will also introduce another regularization term that aims to increase the correlation between the signs of the gradients of each layer:
\begin{equation}
    \Lambda_\pm (\omega, \alpha)=\frac{1}{\binom{L}{2}} \sum_{\ell<\ell'}\frac{{}^\ell g^T{}^{\ell'}g}{\vert {}^\ell g\vert ^T \vert{}^{\ell'}g\vert},
\end{equation}
where $\vert .\vert$ corresponds to the element-wise absolute value function. Note that while the optimal point of $\Lambda(\omega, \alpha)$ corresponds to $\nabla_{{}^\ell \vp}\mathcal{L}(\omega, \alpha)=c\nabla_{{}^{\ell'} \vp}\mathcal{L}(\omega, \alpha)$, where $c$ is a constant, the optimal point of $\Lambda_{\pm}(\omega, \alpha)$ corresponds to $\sign(\nabla_{{}^\ell \vp}\mathcal{L}(\omega, \alpha))=\sign(\nabla_{{}^{\ell'} \vp}\mathcal{L}(\omega, \alpha))$, where $\sign(.)$ is the element-wise signum function.
\par Given one of the regularization terms above, the bi-level optimization problem of \mydarts will become:
\begin{equation}
    \begin{aligned}
        \min_\alpha \quad & \mathcal{L}^{val} (\omega^*(\alpha), \alpha)
        \\\textrm{s.t.} \quad&\omega^*(\alpha)=\argmin_\omega~\mathcal{L}^{train} (\omega, \alpha) - \lambda \Lambda^{train}(\omega, \alpha),
    \end{aligned}
\end{equation}
where we insert the regularization term in the inner objective. In Appendix-\ref{appndx:outer-obj}, we show that optimizing the regularization term w.r.t. the outer objective performs poorly.
Similar to most variants of DARTS, we use a linear scheduling for $\lambda$~\citep{DBLP:conf/iclr/ZelaESMBH20, DBLP:conf/icml/ChenH20}: i.e. $\lambda_t=\lambda \times \frac{t}{T}$, where $t$ is the number of current epoch and $T$ is the total number of epochs used for search.
\subsection{Estimating the gradients of the regularization terms}
Assuming the gradients corresponding to the operations are non-zero, both $\Lambda(\omega, \alpha)$ and $\Lambda_{\pm}(\omega, \alpha)$ are differentiable. So their gradient with respect to $\omega$ can be written as:
\begin{equation}
\label{eqn:grad}
    \nabla_\omega \Lambda(\omega, \alpha)=\frac{1}{\binom{L}{2}}\sum_{\ell=1}^L\nabla_{\omega, {}^\ell p}^2\mathcal{L}(\omega, \alpha) \sum_{\ell'\neq\ell} \delta^{\ell, \ell'},
\end{equation}
where we have:
\begin{equation}
\begin{aligned}
    \delta_{\Lambda}^{\ell, \ell'}&=\left(\mI-\frac{{}^\ell g{}^\ell g^T}{\Vert{}^\ell g\Vert_2^2}\right)\frac{{}^{\ell'} g}{\Vert{}^\ell g \Vert_2 \Vert {}^{\ell'} g \Vert_2},\\
    \delta_{\Lambda_\pm}^{\ell, \ell'}&=\left(\mI-\frac{{}^\ell g^T{}^{\ell'}g}{\vert{}^\ell g\vert^T\vert{}^{\ell'}g\vert}\text{diag}(\sign({}^\ell g)) \text{diag}(\sign({}^{\ell'}g))\right)\frac{{}^{\ell'}g}{\vert{}^\ell g \vert^T \vert{}^{\ell'}g\vert},
\end{aligned}
\end{equation}
with $\mI$ corresponding to the identity matrix. Note that calculating $\nabla_\omega \Lambda(\omega, \alpha)$ will be of $O(L\cdot \vert \omega\vert \cdot \vert \alpha\vert)$ due to the existence of second derivative matrices for each layer. So a direct calculation of (\ref{eqn:grad}) is out of the question. But similar to~\citep{DBLP:conf/iclr/LiuSY19}, we can use finite difference approximation to estimate $\nabla_\omega \Lambda(\omega, \alpha)$.
\par Let $\mathcal{L}(\omega, \mP)$ be the loss function as a function of $\omega$ and $\mP\in \mathbb{R}^{L\times \vert \alpha\vert}$, where:
\begin{equation}
    \mP=\begin{bmatrix}\sigma(\alpha)&\sigma(\alpha)&\dots & \sigma(\alpha)\end{bmatrix}^T
\end{equation}
corresponds to the softmax normalized architecture weights used for each layer in the search network. Furthermore, let $\Delta\in \mathbb{R}^{L\times \vert \alpha \vert}$ be:
\begin{equation}
    \Delta=\begin{bmatrix}\sum_{\ell'\neq1} \delta^{1, \ell'}&\sum_{\ell'\neq2} \delta^{2, \ell'}&\dots & \sum_{\ell'\neq L} \delta^{L, \ell'}\end{bmatrix}^T.
\end{equation}
Then we can write:
\begin{equation}
    \nabla_\omega \Lambda(\omega, \alpha)=\frac{1}{\binom{L}{2}} \left[\frac{\nabla_\omega \mathcal{L}(\omega, \mP+\epsilon \Delta)-\nabla_\omega \mathcal{L}(\omega, \mP-\epsilon \Delta)}{2\epsilon}\right]+O(\epsilon^2),
    \label{eqn:approx}
\end{equation}
where we set $\epsilon=\frac{\epsilon_0}{\Vert \Delta\Vert_F}$. The cost of using this approximation technique is two extra forward-backward pass, which is of $O(\vert \omega\vert +\vert \alpha \vert)=O(\vert\omega\vert)$. So we have eliminated the $L\cdot \vert \alpha\vert$ factor from the complexity of \mydarts, which significantly reduces the computational complexity, while completely eliminating the memory overhead. In Appendix-\ref{sec:appndx-search-cost} we provide the cost of our method in terms of GPU days along with another approximation technique which can reduce the overhead of \mydarts significantly.
\section{Experimental Results and Discussions}
In this section, we will show the effectiveness of \mydarts by performing experiments on the NAS-Bench-201~\citep{DBLP:conf/iclr/Dong020} and DARTS~\citep{DBLP:conf/iclr/LiuSY19} search spaces. We will then demonstrate the robustness of \mydarts by performing more experiments on the reduced DARTS search spaces proposed by~\citep{DBLP:conf/iclr/ZelaESMBH20} on different datasets. Finally, by performing an ablation study, we will analyze the effects of the hyper-parameters of \mydarts over the performance of DARTS. For all experiments, we report the mean and the standard deviation of \mydarts for 4 independent searches with different random seeds, as well as the result of the best performing architecture, except for the experiments on the reduced DARTS search spaces, wherein only the best performance is reported. In all experiments, unless explicitly mentioned otherwise, the search is performed on CIFAR-10. For details of the experiment settings, we direct the reader to Appendix-\ref{appndx:exp-setting}. In Appendix-\ref{appndx:archs} we provide the architectures discovered by \mydarts on these search spaces and datasets. For further experiments, we refer the reader to Appendix-\ref{appndx:orthogonal-vars}, Appendix-\ref{appndx:more-epochs}, and Appendix-\ref{appndx:imagenet-search}.
\subsection{NAS-Bench-201 search space}
\label{sec:nas-bench}
\begin{table}[t]
\vspace{-10pt}
\setlength\tabcolsep{5pt}
\caption{Comparison with state-of-the-art on the NAS-Bench-201 search space. Mean and standard deviation of the accuracy were calculated over 4 independent runs. The search is performed on the CIFAR-10 dataset. The best performance is boldface. (1st): first-order, (2nd): second-order}
\label{table:nasbench}
\vspace{-10pt}
\begin{center}
\scalebox{0.78}{
\begin{tabular}{l l l l l l l}
\multirow{2}{*}{\bf Method} &\multicolumn{2}{c}{\bf CIFAR-10}&\multicolumn{2}{c}{\bf CIFAR-100}&\multicolumn{2}{c}{\bf ImageNet16-120}
\\ \cline{2-7}
& Valid & Test & Valid & Test & Valid & Test \\ \hline
DARTS (1st)~\citep{DBLP:conf/iclr/LiuSY19} & $39.77\pm 0.00$ & $54.30\pm 0.00$ & $15.03\pm0.00$ & $15.61\pm0.00$ & $16.43\pm0.00$ & $16.32\pm 0.00$\\
DARTS (2nd)~\citep{DBLP:conf/iclr/LiuSY19} & $39.77\pm 0.00$ & $54.30\pm 0.00$ & $15.03\pm0.00$ & $15.61\pm0.00$ & $16.43\pm0.00$ & $16.32\pm 0.00$\\
GDAS~\citep{DBLP:conf/cvpr/DongY19} & $89.89\pm 0.08$ & $93.61\pm 0.09$ & $71.34\pm0.04$ & $70.70\pm0.30$ & $41.59\pm1.33$ & $41.71\pm 0.98$\\
SNAS~\citep{DBLP:conf/iclr/XieZLL19} & $90.10\pm 1.04$ & $92.77\pm 0.83$ & $69.69\pm2.39$ & $69.34\pm1.98$ & $42.84\pm1.79$ & $43.16\pm 2.64$\\
DSNAS~\citep{DBLP:conf/cvpr/HuXZLSLL20} & $89.66\pm 0.29$ & $93.08\pm 0.13$ & $30.87\pm16.40$ & $31.01\pm16.38$ & $40.61\pm0.09$ & $41.07\pm 0.09$\\
PC-DARTS~\citep{DBLP:conf/iclr/XuX0CQ0X20} & $89.96\pm 0.15$ & $93.41\pm 0.30$ & $67.12\pm0.39$ & $67.48\pm0.89$ & $40.83\pm0.08$ & $41.31\pm 0.22$\\
iDARTS~\citep{DBLP:conf/icml/ZhangSPCAH21} & $89.86\pm 0.60$ & $93.58\pm 0.32$ & $70.57\pm0.24$ & $70.83\pm0.48$ & $40.38\pm0.59$ & $40.89\pm 0.68$\\
DARTS-~\citep{DBLP:conf/iclr/ChuW0LWY21} & $91.03\pm 0.44$ & $93.80\pm 0.40$ & $71.36\pm1.51$ & $71.53\pm1.51$ & $44.87\pm1.46$ & $45.12\pm 0.82$\\
$\beta$-DARTS~\citep{DBLP:journals/corr/abs-2203-01665} & $\mathbf{91.55\pm 0.00}$ & $\mathbf{94.36\pm 0.00}$ & $\mathbf{73.49\pm0.00}$ & $\mathbf{73.51\pm0.00}$ & $\mathbf{46.37\pm0.00}$ & $\mathbf{46.34\pm 0.00}$\\
\hline
\mydarts~-~$\Lambda(\omega, \alpha)$ & $\mathbf{91.55\pm 0.00}$ & $\mathbf{94.36\pm 0.00}$ & $\mathbf{73.49\pm0.00}$ & $\mathbf{73.51\pm0.00}$ & $\mathbf{46.37\pm0.00}$ & $\mathbf{46.34\pm 0.00}$\\
\hline
Optimal~\citep{DBLP:conf/iclr/Dong020} & $91.61$ & $94.37$ & $73.49$ & $73.51$ & $46.77$ & $47.31$
\end{tabular}
}
\end{center}
\vspace{-10pt}
\end{table}
\par In order to demonstrate the effectiveness of \mydarts in a reproducible setting, we used the NAS-Bench-201 search space for our first set of experiments. In Table-\ref{table:nasbench} we can see the results of \mydarts compared to other DARTS-based methods. We omit from reporting the results of $\Lambda_\pm(\omega, \alpha)$ here, since it performed similar to $\Lambda(\omega, \alpha)$. As evident, \mydarts performs exceptionally well compared to the baselines, surpassing all but one of them in terms of the mean and standard deviation of the performance. Compared to the current state-of-the-art~\citep{DBLP:journals/corr/abs-2203-01665}, \mydarts discovers the same architecture in all 4 experiments. Considering how close to the optimal the performance of \mydarts is, we attribute the slightly less than optimal performance to the high variance of the CIFAR-10 dataset. Furthermore, we note that the method proposed in~\citep{DBLP:journals/corr/abs-2203-01665} converges to its optimal architecture very early in the search process (the first 10 epochs), which may indicate a premature convergence to a sub-optimal architecture. 
\par In comparison, \mydarts does not reach its optimal performance until late in the search procedure, where the performance of the search model is close to optimal and the correlation between the gradient of the layers is very high. In fact, in the DARTS search space we will show further evidence for this claim, where \mydarts is capable of surpassing the performance of~\citep{DBLP:journals/corr/abs-2203-01665} on the DARTS search space by a large margin on two datasets. In Figure-\ref{fig:teaser}, we can see a clear correlation between the \lambdafn and the accuracy of the architecture on CIFAR-10, which reaches its optimal point at around the $40^{th}$ search epoch. Comparing the results to DARTS, we see that the performance of the discovered architecture declines consistently, eventually reaching an extremely low point. 
\subsection{DARTS search space}
\label{sec:DARTS-exp}
\begin{table}[t]
\vspace{-10pt}
\caption{Comparison with state-of-the-art on the DARTS search space. The first block contains methods that report the best performing architecture, while the second block contains methods that report the average of multiple searches (except for results on ImageNet, where always the best performance is reported). Our reported mean and standard deviation of the accuracy are calculated over 4 independent runs. We perform the search on the CIFAR-10 dataset. The best performance is boldface on each block. (1st): first-order, (2nd): second-order, (avg): average performance, (best): best performance. \\$^\dagger$ Different search space used for ImageNet.}
\label{table:darts}
\vspace{-10pt}
\begin{center}
\scalebox{0.72}{
\begin{tabular}{l l l l l l l}
\multirow{2}{*}{\bf Method} &\multicolumn{2}{c}{\bf CIFAR-10}&\multicolumn{2}{c}{\bf CIFAR-100}&\multicolumn{2}{c}{\bf ImageNet}
\\ \cline{2-7}
& Params (M) & Test Acc (\%) & Params (M) & Test Acc (\%) & Params (M) & Test Acc (\%) \\ \hline
NASNet-A~\citep{DBLP:conf/cvpr/ZophVSL18} & $3.3$ & $97.35$ & $3.3$ & $83.18$ & $5.3$ & $74.0$\\
DARTS (1st)~\citep{DBLP:conf/iclr/LiuSY19} & $3.4$ & $97.00\pm 0.14$ & $3.4$ & $82.46$ & - & - \\
DARTS (2nd)~\citep{DBLP:conf/iclr/LiuSY19} & $3.3$ & $97.24\pm 0.09$ & - & - & $4.7$ & $73.3$ \\
SNAS~\citep{DBLP:conf/iclr/XieZLL19} & $2.8$ & $97.15\pm 0.02$ & $2.8$ & $82.45$ & $4.3$ & $72.7$ \\
GDAS~\citep{DBLP:conf/cvpr/DongY19} & $3.4$ & $97.07$ & $3.4$ & $81.62$ & $5.3$ & $74.0$ \\
P-DARTS~\citep{DBLP:conf/eccv/LiuZNSHLFYHM18} & $3.4$ & $\mathbf{97.50}$ & $3.6$ & $82.51$ & $5.1$ & $75.3$ \\
PC-DARTS~\citep{DBLP:conf/iclr/XuX0CQ0X20} & $3.6$ & $97.43\pm 0.07$ & $3.6$ & $\mathbf{83.10}$ & $5.3$ & $\mathbf{75.8}$ \\
DrNAS~\citep{DBLP:conf/iclr/ChenWCTH21} & $4.0$ & $97.46\pm 0.03$ & - & - & $5.2$ & $\mathbf{75.8}$ \\
\hline
R-DARTS~\citep{DBLP:conf/iclr/ZelaESMBH20} & - & $97.05\pm 0.21$ & - & $81.99\pm 0.26$ & - & - \\
P-DARTS~\citep{DBLP:conf/eccv/LiuZNSHLFYHM18} & $3.3\pm 0.21$ & $97.19\pm 0.14$ & - & - & - & - \\
SDARTS-ADV~\citep{DBLP:conf/icml/ChenH20} & $3.3$ & $97.39\pm 0.02$ & - & - & $5.4$ & $74.8$ \\
DOTS~\citep{DBLP:conf/cvpr/GuW0YWLC21} & $3.5$ & $\mathbf{97.51\pm 0.06}$ & $4.1$ & $\mathbf{83.52\pm 0.13}$ & $5.2$ & $75.7$ \\
DARTS+PT~\citep{DBLP:conf/iclr/WangCCTH21} & $3.0$ & $97.39\pm 0.08$ & - & - & $4.6$ & $74.5$ \\
DARTS-~\citep{DBLP:conf/iclr/ChuW0LWY21}$^\dagger$ & $3.5\pm0.13$ & $97.41\pm 0.08$ & $3.4$ & $82.49\pm 0.25$ & $4.9$ & $\mathbf{76.2}$ \\
$\beta$-DARTS~\citep{DBLP:journals/corr/abs-2203-01665} & $3.8\pm 0.08$ & $97.49\pm 0.07$ & $3.8\pm 0.08$ & $83.48\pm 0.03$ & $5.4$ & $75.8$\\\hline
\mydarts~-~$\Lambda(\omega, \alpha)$ (avg) & $3.5\pm 0.13$ & $97.48\pm0.11$ & $3.6\pm 0.13$ & $83.47\pm 0.20$ & - & - \\
\mydarts~-~$\Lambda(\omega, \alpha)$ (best) & $3.5$ & $97.59$ & $3.6$ & $83.64$ & $5.1$ & $75.4$ \\

\mydarts~-~$\Lambda_\pm(\omega, \alpha)$ (avg) & $3.6\pm 0.13$ & $\mathbf{97.57\pm 0.05}$ & $3.6\pm0.1$ & $\mathbf{83.85\pm0.38}$ & - & -\\
\mydarts~-~$\Lambda_\pm(\omega, \alpha)$ (best) & $3.8$ & $\mathbf{97.65}$ & $3.8$ & $\mathbf{84.20}$ & $5.2$ & $\mathbf{75.7}$\\
\end{tabular}
}
\end{center}
\end{table}
\par In order to show that \mydarts is effective in mitigating the performance collapse problem in a complex search space, we used the original search space of DARTS proposed in~\citep{DBLP:conf/iclr/LiuSY19} for our second experiment. In Table-\ref{table:darts}, we can see the results of \mydarts compared to the baselines. \mydarts is able to achieve state-of-the-art results on the CIFAR-10 and CIFAR-100 datasets, surpassing the baseline models by a large margin on the more complex dataset of CIFAR-100. Furthermore, \mydarts achieves comparable results on ImageNet, with about $0.1\%$ difference in accuracy compared to the state-of-the-art results on the same macro-architecture. Furthermore, compared to~\citep{DBLP:journals/corr/abs-2203-01665}, we see that \mydarts achieves better performance on two of the three datasets, supporting our claim that $\beta$-DARTS may be converging to sub-optimal architectures due to pre-mature convergence. Interestingly, compared to most baselines, \mydarts does not select unnecessarily complex architectures, with the number of parameters averaging around $3.5$ on $\Lambda(\omega, \alpha)$ and $3.6$ on $\Lambda_\pm(\omega, \alpha)$ on CIFAR-10. This means that the architectures are much more efficient in terms of performance, further proving the superiority of \mydarts. This is especially the case on the ImageNet search space, where our model is well within the boundaries of the mobile setting introduced in~\citep{DBLP:conf/iclr/LiuSY19}. 
\par Comparing $\Lambda(\omega, \alpha)$ with $\Lambda_\pm(\omega, \alpha)$, we see that the second function achieves a much better performance. Comparing the discovered cells, we noticed that $\Lambda_\pm(\omega, \alpha)$ usually discovers deeper cells with more parametric operations, a quality that highly correlates with the performance of the model. This observation can be attributed to the large dimension of $\alpha$ on this search space.
\subsection{Reduced search spaces}
\begin{table}[t]
\vspace{-10pt}
\caption{Comparison with state-of-the-art on the reduced DARTS search spaces. The test error rate ($\%$) of the best performing architecture from $4$ independent runs is reported. The best performance is boldface.}
\label{table:reduced-darts}
\vspace{-10pt}
\begin{center}
\scalebox{0.73}{
\begin{tabular}{c c c c c c c c}
\textbf{Dataset} & \textbf{Search Space} & \textbf{DARTS} & \textbf{PC-DARTS} & \textbf{R-DARTS} & \textbf{SDARTS-RS} & \textbf{DARTS+PT} & \textbf{\mydarts} \\ \hline
\multirow{4}{*}{\textbf{CIFAR-10}} & \textbf{S1} & $3.84$ & $3.11$ & $\mathbf{2.78}$ & $\mathbf{2.78}$ & $3.50$ & $2.83$ \\ 
& \textbf{S2} & $4.85$ & $3.02$ & $3.31$ & $2.75$ & $2.79$ & $\mathbf{2.56}$ \\ 
& \textbf{S3} & $3.34$ & $2.51$ & $2.51$ & $2.53$ & $2.49$ & $\mathbf{2.38}$ \\ 
& \textbf{S4} & $7.20$ & $3.02$ & $3.56$ & $2.93$ & $2.64$ & $\mathbf{2.46}$ \\ \hline
\multirow{4}{*}{\textbf{CIFAR-100}} & \textbf{S1} & $29.46$ & $24.69$ & $24.25$ & $23.51$ & $24.48$ & $\mathbf{22.79}$ \\ 
& \textbf{S2} & $26.05$ & $22.48$ & $22.44$ & $22.28$ & $23.16$ & $\mathbf{21.68}$ \\ 
& \textbf{S3} & $28.90$ & $21.69$ & $23.99$ & $21.09$ & $22.03$ & $\mathbf{21.03}$ \\ 
& \textbf{S4} & $22.85$ & $21.50$ & $21.94$ & $21.46$ & $20.80$ & $\mathbf{20.65}$ \\ \hline
\multirow{4}{*}{\textbf{SVHN}} & \textbf{S1} & $4.58$ & $2.47$ & $4.79$ & $\mathbf{2.35}$ & $2.62$ & $2.39$ \\ 
& \textbf{S2} & $3.53$ & $2.42$ & $2.51$ & $2.39$ & $2.53$ & $\mathbf{2.37}$ \\ 
& \textbf{S3} & $3.41$ & $2.41$ & $2.48$ & $2.36$ & $2.42$ & $\mathbf{2.31}$ \\ 
& \textbf{S4} & $3.05$ & $2.43$ & $2.50$ & $2.46$ & $2.42$ & $\mathbf{2.34}$ \\ \hline
\end{tabular}
}
\end{center}
\vspace{-10pt}
\end{table}
\par In order to show the effectiveness of \mydarts in alleviating the performance collapse problem even in the most extreme circumstances, we used the reduced search spaces proposed in~\citep{DBLP:conf/iclr/ZelaESMBH20} for further experimentation. In Table-\ref{table:reduced-darts} we can see the results of \mydarts compared to the baselines. Out of 12 different experiments performed, \mydarts performs better than the baselines in 10 of them, with its performance in the other two experiments being comparable to the baselines. Following these observations, we can claim that our model is robustly stepping out of the performance collapse trap without any modifications to the original formulation of DARTS. 
\par Comparing the baselines, we can see that while DARTS+PT performs better on the S4 search space, SDARTS performs mostly better on the other search spaces. Furthermore, we see similar inconsistencies in PC-DARTS and R-DARTS, where they both perform well on S3 but usually not as well on the other search spaces. On the other hand, \mydarts consistently performs well across all search spaces and datasets, which further shows the robustness of our method. 
\subsection{Ablation study}
\begin{figure}[t]
\centering
\begin{subfigure}[]{0.3\textwidth}
    \includegraphics[width=\textwidth]{figures/eps_plot.pdf}
    \caption{Effects of $\epsilon_0$.}
    \label{fig:ablation-a}
\end{subfigure}\begin{subfigure}[]{0.3\textwidth}
    \includegraphics[width=\textwidth]{figures/lambda_plot.pdf}
    \caption{Effects of $\lambda$.}
    \label{fig:ablation-b}
\end{subfigure}\begin{subfigure}[]{0.3\textwidth}
    \includegraphics[width=\textwidth]{figures/epochs_plot.pdf}
    \caption{Effects of number of epochs.}
    \label{fig:ablation-c}
\end{subfigure}
\vspace{-10pt}
\caption{An analysis of the effects of hyper-parameters on \mydarts. We report the mean and $95\%$ confidence interval of CIFAR-10 validation accuracy, calculated over 4 runs. (a) Effects of $\epsilon_0$ over the accuracy of the approximation and the performance of discovered architectures. (b) Effects of $\lambda$ over the performance of the discovered architectures. (c) Effects of the number of search epochs on the performance of the discovered architectures. The search is performed on CIFAR-10.}
\vspace{-10pt}
\end{figure}
\par In order to assess how the hyper-parameters affect \mydarts, we conducted an ablation study on $\epsilon_0$ and $\lambda$ on the NAS-Bench-201 search space. Furthermore, to show that \mydarts has solved the performance collapse issue, we perform the search for longer epochs (starting from $50$, up to $400$ epochs) following~\citep{DBLP:conf/iclr/ChuW0LWY21, DBLP:journals/corr/abs-2203-01665}. In Figure~\ref{fig:ablation-a}, we can see the effect of $\epsilon_0$ over the performance of the discovered architecture. We observe that smaller values of $\epsilon_0$ correlate positively with the quality of the discovered architecture. Considering that lower values of $\epsilon_0$ correspond to lower approximation error in (\ref{eqn:approx}), this is expected behavior. But we note that due to the problem of round-off error, we expect this trend to reverse for smaller values of $\epsilon_0$~\citep{DBLP:journals/corr/abs-2009-07098}. In fact, we can see that the variance of the performance is already starting to increase for the smallest value of $\epsilon_0$.
\par Figure~\ref{fig:ablation-b} shows the effect of $\lambda$ over the performance of the discovered architecture. Clearly, the model is not very sensitive to the value of $\lambda$, with the best performing value and the worst performing value having a difference of about $2\%$ in their mean accuracy. Furthermore, the best performing value at $\lambda=0.125$ corresponds to an average length normalized dot-product of about $0.7$, while the worst performing value at $\lambda=0.5$ is around $0.9$. So we are well within the realm of diminishing returns for larger $\lambda$s, and as a result of increased emphasis on the regularization term in the Pareto-optimal point, we observe negative effects from the main objective being sidetracked in the form of lower quality architectures.
\par We can see the performance of \mydarts for different numbers of search epochs in Figure-\ref{fig:ablation-c}. Unlike other methods that try to address the issue of performance collapse for larger number of epochs~\citep{DBLP:journals/corr/abs-1910-11831, DBLP:conf/iclr/ChuW0LWY21}, we see a negligible drop in performance (about $0.4\%$) when comparing the best performing and the worst performing setting. This is a clear indication that the models have achieved convergence at around $100$ epochs, reaching a local-minima that corresponds to near-optimal architectures. In fact, we can observe further proof for the convergence of \mydarts in Figure-\ref{fig:abschange}, where clearly the absolute changes in the softmax normalized architecture weights has reached a plateau in $50$ epochs of search, resulting in a concave curvature. 
\section{Conclusion}
\par In this paper, we use a set of analytical and empirical tools to show that the main reason behind performance collapse in DARTS is the lack of correlation between the gradient of its layers, caused by the multi-layer structure and the use of weight-sharing for cell-search. Based on the aforementioned analysis, we propose \mydarts which manages to achieve better or comparable performance when compared to state-of-the-art baselines.
Here, we focused our discussion and experiments on DARTS, however the analysis and ideas are extendable to any other type of differentiable NAS model that performs cell search using weight sharing, which we plan to study in future works. Furthermore, we point out to the cost of estimating the gradient of the regularization terms proposed here as another potential subject for further research.

\subsubsection*{Acknowledgments}
We thank colleagues and students at the University of Tehran for all the useful discussions. We would like to also thank researchers at Google for feedback, in particular Hanxiao Liu, Xuanyi Dong, and Avital Oliver for their detailed and valuable comments on the paper. We also thank the reviewers for their constructive and helpful feedback, which helped improving the quality of the paper.

\bibliography{ref}
\bibliographystyle{iclr2023_conference}

\appendix
\section{Appendix}
\subsection{Review on differentiable neural architecture search (DARTS)}
\label{appndx:DARTS}
\subsubsection{Continuous relaxation}
\par For simplicity, let's assume we are searching for a single cell. Let the vector $\alpha\in \mathbb{R}^{\vert E\vert \cdot \vert \mathcal{O}\vert}$ be the set of parameters corresponding to the architecture, where $E$ is the set of all the possible edges in the computational graph of the cell, and $\mathcal{O}$ is a set of pre-defined operations. The goal is to find the optimal topology and operations in the continuous space using gradient-descent, and then transfer it from the continuous space to the discrete space using the $\argmax$ operator to get a near-optimal architecture. 
\par Specifically, let $x_j$ be the output of the $j^{th}$ node in the computational graph; We can write $x_j$ as:
\begin{equation}
    x_j=\sum_{i<j}\sum_{o\in \mathcal{O}} ~p_o^{(i, j)}o(x_i),
\end{equation}
where $p_o^{(i, j)}$ is the softmax normalized architecture weights: $p_o^{(i, j)}=\frac{\exp(\alpha_o^{(i, j)})}{\sum_{o'\in \mathcal{O}}\exp(\alpha_{o'}^{(i, j)})}$. DARTS then solves the following bi-level optimization problem to find the optimal value for $\alpha$:
\begin{equation}
    \label{eqn:darts-opt-problem}
    \begin{aligned}
        \min_\alpha \quad & \mathcal{L}^{val} (\omega^*(\alpha), \alpha)
        \\\textrm{s.t.} \quad&\omega^*(\alpha)=\argmin_\omega~\mathcal{L}^{train} (\omega, \alpha),
    \end{aligned}
\end{equation}
where $\mathcal{L}^{train}(., .)$ and $\mathcal{L}^{val}(., .)$ correspond to the loss function calculated on the train and validation set, and $\omega$ is the set of parameters corresponding to the operations. There are two methods proposed by the original paper to solve the problem above using gradient descent. The first method - dubbed first-order - performs gradient descent on $\alpha$ and $\omega$ in an interleaved fashion, while the second method - dubbed second-order - considers the dependency between $\omega^*(\alpha)$ and $\alpha$ when calculating the descent direction corresponding to $\alpha$. In this paper, we focus on the first-order method since it is more widely used~\citep{DBLP:journals/jmlr/ElskenMH19}. For further information, we direct the reader to the original paper~\citep{DBLP:conf/iclr/LiuSY19}.
\subsubsection{Weight-sharing for cell-search}
\par In order to perform cell-search, DARTS utilizes a weight-sharing framework. Specifically, in a cell-search scheme, $L$ layers of the same cell are stacked on top of each other according to a pre-defined macro architecture. These cells share the same $\alpha$ - i.e. share the same architecture parameter - but have different parameters corresponding to the operations. So they have the same structure, but different operation weights. As a result, we can write the gradient corresponding to the architecture parameter as the sum of the gradients received by each layer.
\par In order to define this weight-sharing framework concretely, let ${}^\ell x_j$ correspond to the output of the $j^{th}$ node of the architecture graph produced by the $\ell^{th}$ layer. Then we can write the gradient received by $p_{o}^{(i, j)}$ from the $\ell^{th}$ layer as:
\begin{equation}
    \label{eqn:archgrad-layerwise}
    \frac{\partial \mathcal{L}}{\partial {}^\ell p_{o}^{(i, j)}}=\frac{\partial \mathcal{L}}{\partial {}^\ell x_j}\cdot o({}^\ell x_i).
\end{equation}
In other words, the gradient received by $p_{o}^{(i, j)}$ from the $\ell^{th}$ layer corresponds to the dot product of the output of the $i^{th}$ node in $\ell^{th}$ layer and the gradient of the $j^{th}$ node in $\ell^{th}$ layer. Since we have $L$ layers in the macro-architecture, we can write the gradient corresponding to $p_{o}^{(i, j)}$ as the sum of the gradients received by each layer:
\begin{equation}
    \frac{\partial \mathcal{L}}{\partial p_{o}^{(i, j)}}=\sum_{\ell=1}^L \frac{\partial \mathcal{L}}{\partial {}^\ell p_{o}^{(i, j)}}.
\end{equation}
\par Now, by writing the gradient for all edges and operations for $p$ in vector form, we have:
\begin{equation}
    \nabla_\vp \mathcal{L}(\omega, \alpha)=\sum_{\ell=1}^L \nabla_{{}^\ell \vp} \mathcal{L}(\omega, \alpha).
\end{equation}
\subsection{The root cause of low \lambdafn}
\label{appndx:cause-of-lambdafn}
\par An important matter to consider is the root cause of low \lambdafn, which can help us further understand the reason behind the performance collapse, and the superior performance of \mydarts. Here, we will consider two likely candidates that may have caused this issue:
\begin{itemize}
    \item High level of approximation in the formulation of first-order DARTS~\citep{DBLP:conf/icml/ZhangSPCAH21}.
    \item Learning unrolled iterative estimates in the ResNet family of networks~\citep{DBLP:conf/iclr/GreffSS17}.
\end{itemize}
In the following, we will further investigate these candidates and their effects over performance collapse.
\subsubsection{High level of approximation in first-order DARTS}
\par In (\ref{eqn:darts-opt-problem}), the optimality of $\omega^*(\alpha)$ w.r.t. the train loss is added to the formulation of the optimization problem of DARTS in the form of a constraint. Using the implicit function theorem, we can write the gradient corresponding to the outer optimization problem as~\citep{DBLP:conf/icml/ZhangSPCAH21}:
\begin{equation}
    \nabla_\alpha \mathcal{L}^{val}(\omega^*(\alpha), \alpha)=\nabla_\alpha \mathcal{L}^{val}(\omega, \alpha)-\nabla_{\alpha, \omega}^2\mathcal{L}^{train}(\omega, \alpha)\left(\nabla_\omega^2 \mathcal{L}^{train}(\omega, \alpha)\right)^{-1}\nabla_\omega \mathcal{L}^{val}(\omega, \alpha)\text{,}
\end{equation}
where $\omega=\omega^*(\alpha)$ and $\left(\nabla_\omega^2 \mathcal{L}^{train}(\omega, \alpha)\right)^{-1}$ is the inverse of the Hessian w.r.t. $\omega$. In second-order DARTS, the inverse Hessian is estimated using the identity matrix, while in iDARTS the inverse Hessian is estimated using Neumann series. Note that it is entirely plausible for these approximations to be the cause of low \lambdafn, and by extention, the performance collapse problem. So it may be the case that by eliminating at least one of these approximations in DARTS, the \lambdafn may get improved. 
\par In order to investigate this claim, we apply the \lambdafn function $\Lambda(\omega, \alpha)$ over the layer gradients with the approximations removed:
\begin{equation}
    \label{eqn:exact-grad}
    \nabla_{{}^\ell p} \mathcal{L}^{val}(\omega^*(p), \alpha)=\nabla_{{}^\ell p} \mathcal{L}^{val}(\omega, \alpha)-\nabla_{{}^\ell p, \omega}^2\mathcal{L}^{train}(\omega, \alpha)\left(\nabla_\omega^2 \mathcal{L}^{train}(\omega, \alpha)\right)^{-1}\nabla_\omega \mathcal{L}^{val}(\omega, \alpha)\text{,}
\end{equation}
where we can equivalently see $\omega^*(\alpha)$ as a function of $\omega^*(p)$ with the softmax function removed from the abstraction. Since we need to calculate (\ref{eqn:exact-grad}) for each layer, we estimate the inverse Hessian with the identity function similar to second-order DARTS. Note that estimating (\ref{eqn:exact-grad}) with this simplification can be performed similar to estimating the gradient corresponding $\Lambda(\omega, \alpha)$, by using finite difference approximation and perturbing the value of $p$ for each layer separately.
\par \begin{wrapfigure}{r}{0.4\textwidth}
  \vspace{-23pt}
  \begin{center}
    \includegraphics[width=0.4\textwidth]{figures/corr_first_second_order.pdf}
  \end{center}
  \vspace{-15pt}
  \caption{The average \lambdafn for the first and second-order estimation of gradients of the layers, compared to \mydarts.}
  \vspace{-10pt}
  \label{fig:second-order-corr}
\end{wrapfigure}Comparing the average value for \lambdafn for first-order and second-order estimation, we did not notice any meaningful differences. In both cases, the value of $\Lambda(\omega, \alpha)$ would vary between $0.06$ to $0.08$ on the NAS-Bench-201 search space, as can be observed in Figure-\ref{fig:second-order-corr}, with the variation resembling the noise caused by mini-batching. Considering the performance of \mydarts doesn't improve significantly until the \lambdafn reaches a value larger than $0.2$ (as observed in Figure-\ref{fig:teaser}), we can confidently assert that the low \lambdafn is not caused by the approximation of the gradient corresponding to architecture parameters.
\subsubsection{The iterative estimation theory}
\par An interesting observation to note is that the value of \lambdafn is much lower for parametric operations compared to non-parametric operations. So it is completely plausible that the low \lambdafn problem is somehow caused by the parametric operations (specifically, convolution operations) and their dynamic with the network in some way. Here, we will show that at according to the iterative estimation theory in ResNets~\citep{DBLP:conf/iclr/GreffSS17}, this is indeed the case.
\par According to (\ref{eqn:archgrad-layerwise}), we can write the gradient corresponding to each operation on each edge for each layer as the dot product between the gradient of the node receiving the edge in that layer and the value of the operation itself. So we can write the correlation between the gradients of the layers $\ell$ and $\ell'$ for operation $o$ on edge $(i, j)$ as:
\begin{equation}
    \frac{\partial \mathcal{L}}{\partial {}^\ell p_{o}^{(i, j)}}\cdot \frac{\partial \mathcal{L}}{\partial {}^{\ell'} p_{o}^{(i, j)}}=\frac{\partial \mathcal{L}}{\partial {}^\ell x_j}^To({}^\ell x_i)\cdot \frac{\partial \mathcal{L}}{\partial {}^{\ell'} x_j}^To({}^{\ell'} x_i)=o({}^\ell x_i)^T\left[\frac{\partial \mathcal{L}}{\partial {}^\ell x_j} \frac{\partial \mathcal{L}}{\partial {}^{\ell'} x_j}^T\right]o({}^{\ell'} x_i)\text{,}
\end{equation}
which implicitly assumes the two layers work on the same number of height, width, and channels. Assuming $\frac{\partial \mathcal{L}}{\partial {}^\ell x_j}$ and $\frac{\partial \mathcal{L}}{\partial {}^{\ell'} x_j}$ are very similar (an assumption we will confirm empirically), then their outer product is very close to a positive semi-definite matrix. As a result, the correlation between layer gradients mainly depends on whether the value of the operations $o({}^\ell x_i)$ and $o({}^{\ell'} x_i)$ highly correlate or not. 
\par Interestingly, the iterative estimation theory provides a framework for us to answer this question. According to this theory, the layers of a ResNet model try to iteratively refine an estimate of an ideal output that will ultimately result in an accurate prediction at the classification level~\citep{DBLP:conf/iclr/GreffSS17, DBLP:conf/iclr/JastrzebskiABVC18}. As a result, we can see the output of each ResNet block as a representation in the same vector-space, with each block trying to move this representation in a direction that better estimates the ideal representation using its corresponding convolution operations.
\par Note that in this case, an optimal iterative estimation would correspond to a set of orthogonal updates, since undoing the work of previous blocks in undesirable. Furthermore, since most of the search spaces used for DARTS (and generally, NAS) are from the family of ResNets, we can apply these findings to DARTS as well. Therefore, under the iterative estimation theory, we can claim that the reason behind low \lambdafn in convolution operations is the orthogonality of their output. This is inline with the motivation behind DARTS-PT in~\citep{DBLP:conf/iclr/WangCCTH21}, which showed a direct connection between performance collapse and the iterative estimation theory. In this paper, we provide further evidence for this claim, but also provide a solution to the problem that does not require significant changes to the DARTS framework.
\begin{figure}[h]
    \centering
    \begin{subfigure}[h]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/cosine_op.pdf}
        \caption{Cosine Similarity, Convolution Operations}
    \end{subfigure}\begin{subfigure}[h]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/cosine_grad.pdf}
        \caption{Cosine Similarity, Node Gradients}
    \end{subfigure}
    \caption{The cosine similarity between (a) the convolution operations, (b) gradients received by each node, averaged over each pair of layers and each edge. We report the mean and the $95\%$ confidence interval for four experiments on the NAS-Bench-201 search space and the CIFAR-10 dataset.}
    \label{fig:cosine-sim}
\end{figure}
\par In order to confirm our assumptions and provide empirical evidence for our analysis, here we will provide a visualization of the cosine similarity between $\frac{\partial \mathcal{L}}{\partial {}^\ell x_j}$ and $\frac{\partial \mathcal{L}}{\partial {}^{\ell'} x_j}$, and the convolution operation outputs $o({}^\ell x_i)$ and $o({}^{\ell'} x_i)$, averaged over each pair of layers on the NAS-Bench-201 search space. Since DARTS works in three stages with respect to input scale, we will provide the aforementioned values for the three stages separately. You can observe the results in Figure-\ref{fig:cosine-sim}.
\par The first thing to note is the large correlation between node gradients, which increases in higher stages. On the otherhand, we can see that the correlation between the output of the convolution operations is incredibly low. This observation confirms our assumption that the main reason behind low \lambdafn is low correlation between the output of the convolution operation, which can be attributed to the iterative estimation theory. Furthermore, we can observe that in order to increase the value of \lambdafn, \mydarts focuses on increasing the correlation between the output of convolutions, and does not have a noticeable impact on the correlation between node gradients.
\par Given the analytical and empirical evidence provided here, we can confidently assert that as pointed in~\citep{DBLP:conf/iclr/WangCCTH21}, one of the main reasons behind performance collapse is the residual connections and the ResNet-like structure of most DARTS search spaces. But unlike~\citep{DBLP:conf/iclr/WangCCTH21}, we attribute performance collapse to the discrepancies caused by the residual connections on the optimal architecture from the perspective of each layer, and the resulting problem in the convergence point of DARTS. The superior performance of \mydarts compared to DARTS-PT confirms the validity of this claim, and we hope these findings can provide new directions for further research.
\subsection{Relationship to prior works}
\label{sec:appndx-priorwork}
In this section, we will provide an analysis on the relationship between our work and some of the prior works that aim to solve the performance collapse problem. More specifically, we will show that most of the papers that reduce the severity of performance collapse to some degree can be explained through our analytical framework, suggesting that they may be alleviating the symptoms of low layer alignment indirectly.
\subsubsection{The convergence of DARTS and its relation to $\ell^2$ regularization}
\label{sec:l2-reg}
\par In~\citep{DBLP:journals/corr/abs-2203-01665}, it has been suggested that one of the possible solutions to performance collapse is $\ell^2$ regularization performed on the log-sum-exp of the architecture values: $\mathcal{L}_\beta(\alpha) = \log(\sum_{i<j, o\in \mathcal{O}} \exp(\alpha_o^{(i, j)}))$. The regularization term is then added to the loss function corresponding to the outter optimization problem, changing it to: $\min_\alpha~\mathcal{L}(\omega^*(\alpha), \alpha)+\gamma \mathcal{L}_\beta(\alpha)$, where $\gamma$ is scheduled linearly, starting from $0$ and increased to a very large value (usually around $25$ to $100$). 
\par Note that in the case of $\ell^2$ regularization, the optimality condition in \ref{eqn:optimality} can be re-written as:
\begin{equation}
    \nabla_\alpha \mathcal{L}^{val}(\omega, \alpha^*)=-\gamma \nabla_\alpha \mathcal{L}_\beta(\alpha) \text{,}
\end{equation}
where we have $\nabla_\alpha \mathcal{L}_\beta(\alpha)=\sigma(\alpha)=p$. Given that $p$ is normalized over each edge, we have $\Vert \sigma(\alpha)\Vert_1=\vert E\vert$. So we can given a lower-bound on the $\ell^2$ norm of $\nabla_\alpha \mathcal{L}_\beta(\alpha)$ using the Cauchy-Schwarz inequality:
\begin{equation}
    \Vert \nabla_\alpha \mathcal{L}_\beta(\alpha)\Vert_2\geq \frac{\Vert \nabla_\alpha \mathcal{L}_\beta(\alpha)\Vert_1}{\sqrt{\vert E\vert \cdot \vert \mathcal{O}\vert}}=\frac{\vert E\vert}{\sqrt{\vert E\vert\cdot\vert\mathcal{O}\vert}}=\sqrt{\frac{\vert E\vert}{\vert\mathcal{O}\vert}}\text{.}
\end{equation}
Since the number of edges and operations are usually comparable in most search spaces (for example, in DARTS it is equal to $14$ and $7$, and in NAS-Bench-201 it is $6$ and $5$, respectively), the lower-bound proposed above is very close to $1$. Therefore, as $\gamma$ increases to a very large value, the convergence point of $\beta$-DARTS no longer correspond to the stationary point of the primary loss function. Moreover, in our experiments we noticed that $\Vert\nabla_\alpha \mathcal{L}^{val}(\omega, \alpha)\Vert_2$ usually has a very small value (in the order of $O(10^{-1})$). This means that the convergence point doesn't happen on the saturation point of the softmax function, but instead, becomes dominated by the regularization term, which corresponds to the current value of $p$. So similar to the case of DARTS, the convergence point does not directly depend on the loss function, but is directed by the current value of $\alpha$ which is based on previous information from the gradient of the loss, which may be inaccurate at the current point.
\par This analysis is supported by the experiment results provided by~\citep{DBLP:journals/corr/abs-2203-01665} in its Figure-4c, where at the convergence point the value of $\alpha$ increases monotonically and linearly, while the value of $p$ does not change at all, and indeed does not correspond to the saturation point of softmax. Note that a similar analysis can be done on implicit gradient and second-order methods of DARTS as well, where the convergence point no longer corresponds to a stationary point of the explicit gradient of the architecture parameters
\subsubsection{Vanishing gradient and the low \lambdafn issue}
\par In~\citep{DBLP:conf/nips/ZhouXSH20} it has been observed that the rate of the convergence of the search model with respect to the operation parameters is directly impacted by the architecture weights corresponding to the skip-connection operations. This observation is then used an incentive to propose auxiliary skip-connections by~\citep{DBLP:conf/iclr/ChuW0LWY21} - dubbed DARTS-, where the effect of auxiliary connections is gradually reduced by a linearly scheduling their magnitude to approach zero. Independently, in order to reduce the memory consumption of the search model, PC-DARTS proposes performing the candidate operations on a subset of the channels of the input, thereby implicitly providing an auxiliary skip-connection on a subset of the input channels~\citep{DBLP:conf/iclr/XuX0CQ0X20}. Here, we will argue that both of these methods are implicitly trying to solve a symptom of the lower \lambdafn issue by up-weighting the gradients corresponding to the lower layers.
\par 
\begin{table}[t]
\centering
\caption{The test performance of architectures discovered by PC-DARTS with batch size of $256$ (PC-DARTS-$256$) and $64$ (PC-DARTS-$64$) on NAS-Bench-201.} 
\label{table:pc-darts-bs}
\begin{tabular}{l|c|c|c}
\textbf{Method} & \textbf{CIFAR-10 Acc (\%)} & \textbf{CIFAR-100 Acc (\%)} & \textbf{ImageNet Acc (\%)}\\ \hline
    PC-DARTS-$256$ & $93.41\pm 0.30$ & $67.48\pm 0.89$ & $41.30\pm 0.22$ \\
    PC-DARTS-$64$ & $93.76\pm 0.00$ & $71.11\pm 0.00$ & $41.44\pm 0.00$
\end{tabular}
\end{table}
Firstly, in order to show that the increased batch size doesn't benefit PC-DARTS greatly, we performed the search using PC-DARTS on a batch size of $64$ and compared the results to performing the search with a batch size of $256$ on the NAS-Bench-201 search space in Table-\ref{table:pc-darts-bs}. Clearly, we can claim that the batch size is not the main contributer to the better performance of PC-DARTS, and instead, it is the partial-channeling that appears to be alleviating performance collapse.
\par Note that according to (\ref{eqn:archgrad-layerwise}), we can write the gradient received by each architecture weight as the dot product of the gradient of the receiving node and the corresponding operation. While gradient vanishing does not affect the operation, it does have an impact over the gradient of the loss w.r.t. the receiving node. We can see this effect in Figure-\ref{fig:grads-b}, wherein the gradients corresponding to the first 5 layers generally have smaller magnitudes. Therefore, one way of alleviating the symptoms of low \lambdafn can be reducing the effects of gradient vanishing by providing stronger skip-connections. This is effectively what is done by the auxiliary skip-connection proposed by DARTS- and the partial-channel structure of PC-DARTS, which causes limited improvements in the performance of DARTS.
\par \begin{figure}[t]
    \centering
    \begin{subfigure}[h]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/grad_dartsminu_plot.jpg}
        \caption{DARTS-}
    \end{subfigure}\begin{subfigure}[h]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/grad_pcdarts_plot.jpg}
        \caption{PC-DARTS}
    \end{subfigure}
    \caption{The exponential average of the gradient of the first five layers and the last five layers for DARTS- and PC-DARTS, similar to Figure-\ref{fig:grads-b}.}
    \label{fig:grads-dartsmin-pcdarts}
\end{figure}In order to make our argument stronger, we also provide the gradients corresponding to the first five layers and the last five layers similar to Figure-\ref{fig:grads-b} for both DARTS- and PC-DARTS, which you can see in Figure-\ref{fig:grads-dartsmin-pcdarts}. The first thing to notice is the striking similarity between the two figures, where the gradients corresponding to layers closer to the output have been extremely down-weighted compared to the gradients corresponding to layers closer to the input. This observation proves the validity of our claim that auxiliary skip-connection and partial-channeling significantly up-weights the gradients corresponding to the layers closer to the input. Since these layers prefer parametric operations, we observe better performance by these models. But note that despite the improvements, these methods are merely circumventing the problem of low \lambdafn, which is the reason behind their poor performance compared to \mydarts.
\subsubsection{High \lambdafn as a prior by eliminating weight-sharing in sample-based NAS}
\par One of the popular methods of cell-based NAS is the use of sampling candidate architectures from a pre-determined distribution, wherein the sufficient statistics of the distribution are optimized according to the performance of the sampled architecture. Among these methods, we can mention DrNAS which utilizes a Dirichlet distribution for the search~\citep{DBLP:conf/iclr/ChenWCTH21}, and SNAS which utilizes a concrete distribution~\citep{DBLP:conf/iclr/XieZLL19}. In both of these cases, the sufficient statistics are optimized according to a gradient estimation technique that approximates the gradient corresponding to the sufficient statistics based on the gradients received by the sampled operations.
\par We argue that the mere act of using sample-based NAS is a way to circumvent the problem of low \lambdafn. Firstly, because such a formulation of the NAS problem completely eliminates the issue of convergence at softmax saturation points, thus preventing the optimal architecture to be completely dictated by the layers closer to the input. Secondly, because the sampled architecture in a cell-based NAS method is used on all of the cells, sampling itself comes with an implicit prior that the optimal architecture according to each layer must be the same. Therefore, sample-based NAS can be also viewed as a way to enforce high \lambdafn in an a priori fashion. Therefore, we can see sampled-based differentiable NAS as a way of solving the problem of performance collapse. But we should note that since in these methods only a subset of the supernet is optimized, they usually come with the caveat of low sample efficiency and longer search time~\citep{DBLP:journals/csur/RenXCHLCW21}.
\subsection{Regularization of the outer objective}
\label{appndx:outer-obj}
\par An interesting point to consider is whether we can apply the regularization term of \mydarts over the outer objective, which will directly optimize the architecture parameters $\alpha$. Here, we will first investigate this matter from a theoretical standpoint, and provide two points that shows the sub-optimality of such approach. Then provide empirical results to confirm our analytical findings.
\par The first problem with regularizing the outer objective has to do with the fact that the gradient of $p_o^{(i, j)}$ does not directly depend on $p_o^{(i, j)}$ itself, but the output of the corresponding operation $o(.)$ and the gradient of the receiving node $x_j$, as observed in (\ref{eqn:archgrad-layerwise}). Therefore, increasing the \lambdafn must be done by changing the composition of the operations of the layers in the higher layers to adjust $\frac{\partial \mathcal{L}}{\partial {}^\ell x_j}$, while changing the composition of the operations in the lower layers to adjust $o({}^\ell x_i)$. Considering the small size of $\alpha$, and the weight-sharing framework, this may be an impossible task that may result in overfitting on the regularization term as well.
\par Another issue we need to take note of is the connection between the problem of low \lambdafn and the iterative estimation theory, as noted in Appendix-\ref{appndx:cause-of-lambdafn}. Assuming the problem of low \lambdafn is caused by the orthogonality of the output of convolution operations in each layer, then resolving this problem is only possible through regularizing the convolution operations themselves. While it may be possible to resolve the issue through adjusting the input, we note that as pointed in the previous paragraph, this adjustment may lead to overfitting due to the low complexity of the model with respect to $\alpha$.
\par \begin{table}[t]
\centering
\caption{The test performance of architectures discovered by \mydarts with the regularization term applied to the inner objective (inner), and the regularization term applied to the outer objective (outer). The search is performed on CIFAR-10 and the NAS-Bench-201, and the mean and standard deviation of three runs is reported.} 
\label{table:outer-inner}
\begin{tabular}{l|c|c|c}
\textbf{Model} &\textbf{CIFAR-10 Acc (\%)} & \textbf{CIFAR-100 Acc (\%)} & \textbf{ImageNet Acc (\%)}\\ \hline
    \mydarts (inner) & $93.95\pm 0.15$ & $72.75\pm 0.42$ & $45.49\pm 0.08$\\
    \mydarts (outer) & $91.37\pm1.59$ & $67.87\pm 1.57$ & $41.6\pm 2.11$\\
\end{tabular}
\end{table}In order to provide empirical evidence for our analysis, we tried to perform the regularization on the outer objective. You can see the results of $50$ epochs of search in Table-\ref{table:outer-inner}, where we compared \mydarts with inner objective regularized with \mydarts with outer objective regularized. Clearly, we observe that regularizing the outer objective performs poorly compared to the case where we regularized the inner objective. This observation further supports our analytical reasoning, and shows that performing the regularization on $\omega$ yields more superior and robust performances compared to regularizing $\alpha$.
\subsection{Combination with orthogonal variants of DARTS}
\label{appndx:orthogonal-vars}
\par In this section, we will empirically demonstrate that our method can be extended to orthogonal variants of DARTS to yield better and more robust results. Specifically, we will discuss two popular variants of DARTS, P-DARTS and SDARTS, and how \mydarts can eliminate the need for strong prior assumptions or costly regularization in these methods.
\subsubsection{Progressive-DARTS and the problem of strong prior assumptions}
\par \begin{table}[t]
\centering
\caption{The test performance of P-DARTS, P-DARTS- (P-DARTS with auxiliary connections), and P-DARTS with the proposed regularization term. In all experiments, the prior assumptions of P-DARTS are removed. The search is performed on CIFAR-10 and the DARTS search space, and the mean and standard deviation of three runs is reported.} 
\label{table:p-darts}
\begin{tabular}{l|c}
\textbf{Model} &\textbf{CIFAR-10 Acc (\%)} \\ \hline
    P-DARTS (w/o priors)~\citep{DBLP:journals/ijcv/ChenXWT21} & $96.48\pm 0.55$\\
    P-DARTS- (w/o priors)~\citep{DBLP:journals/ijcv/ChenXWT21} & $97.28\pm 0.04$\\\hline
    $\Lambda$-P-DARTS (w/o prios) & $\mathbf{97.34\pm 0.06}$
\end{tabular}
\end{table}Progressive-DARTS (P-DARTS) is a low-cost variant of DARTS that is based on performing the search in a progressive manner. P-DARTS performs the search in three stages, with each stage eliminating some of the candidate operations while also making the search model deeper and more similar to the evaluation architecture~\citep{DBLP:journals/ijcv/ChenXWT21}. One of the issues with PC-DARTS is its reliance on two very strong priors in the search process: 1) limiting the number of skip-connections to two, which severely reduces the size of the search space, 2) using dropout on skip-connections to limit their utility during search. Following~\citep{DBLP:conf/iclr/ChuW0LWY21}, here we will show that \mydarts can eliminate the need for both of these strong priors, which further validates the effectiveness of our method. You can see the results in Table-\ref{table:p-darts}.
\par We can see that our method successfully removes any need for strong prior assumptions or auxiliary connections in the progressive search format, achieving superior performance to both cases. We note that these results are attained without any adjustment to the hyperparameters of \mydarts, which is likely going to further benefit the results, considering that the progressive framework changes the macro-architecture and the search space significantly in each stage.
\subsubsection{SDARTS and the problem of costly regularization}
\par SDARTS is a variant of DARTS that aims to improve generalization by regularizing the Hessian of the loss function corresponding to the architecture parameters~\citep{DBLP:conf/icml/ChenH20}. The regularization is done through additive noise to the architecture parameters during the optimization of operation parameters ($\omega$). There are tow methods proposed by~\citep{DBLP:conf/icml/ChenH20} based on this idea: 1) SDARTS-RS and 2) SDARTS-ADV. SDARTS-RS is based on random perturbations sampled from a uniform distribution, while SDARTS-ADV utilizes the costly method of projected gradient descent to produce adversarial perturbations. One of the issues with SDARTS-RS is that it is not as effective as the costly method of SDARTS-ADV, which needs more than four forward-backward propagations to estimate an effective perturbation. Here, we will show that by combining our proposed method with SDARTS-RS, we will achieve better results than both SDARTS-RS and SDARTS-ADV, at the cost of only two forward-backward propagations.
\par \begin{table}[t]
\centering
\caption{The test performance of SDARTS-RS, SDARTS-ADV, and SDARTS-RS with the proposed regularization term. The search and evaluation is performed on CIFAR-10 and the DARTS search space, and the mean and standard deviation of three runs is reported. The search cost is reported based on GPU days on a 1080 Ti.} 
\label{table:sdarts}
\begin{tabular}{l|c|c}
\textbf{Model} &\textbf{Test Acc} & \textbf{Search Cost} \\ 
& \textbf{(\%)} & \textbf{(GPU days)} \\\hline
    SDARTS-RS~\citep{DBLP:conf/icml/ChenH20} & $97.33\pm 0.03$ & $0.4$\\
    SDARTS-ADV~\citep{DBLP:journals/ijcv/ChenXWT21} & $97.39\pm 0.02$ & $1.3$\\\hline
    $\Lambda$-SDARTS-RS & $\mathbf{97.42\pm 0.04}$ & $0.8$
\end{tabular}
\end{table} In Table-\ref{table:sdarts}, you can see the performance of SDARTS with and without adversarial perturbations, and compare them to that of $\Lambda$-SDARTS-RS, which utilizes our proposed regularization term along with the random perturbation regularization of SDARTS-RS. Clearly, our proposed regularization completely eliminates the need for costly adversarial training, reducing the cost to almost half of what SDARTS-ADV requires. This observation further proves the importance of increasing the \lambdafn in DARTS, regardless of the type of DARTS variant used during search. We note that these results are attained without any adjustment to the hyperparameters of \mydarts, which is likely going to further benefit the results, considering that the Hessian regularization is likely to change the optimal $\epsilon_0$ due to changes in the loss landscape.
\subsection{Performance collapse with more epochs}
\label{appndx:more-epochs}
\par \begin{table}[t]
\centering
\caption{The test performance of Random Search, DARTS (first and second order), P-DARTS, PC-DARTS, Amended-DARTS, and \mydarts after $200$ epochs of search on CIFAR-10. All results, except for \mydarts, are borrowed from~\citep{DBLP:journals/ijcv/ChenXWT21}. The search cost is reported based on GPU days on a 1080 Ti, and inferred from reported costs for the baselines. Note that here, (\#P) corresponds to the number of parametric operations in the normal cell.} 
\label{table:200epochs}
\begin{tabular}{l|c|c|c}
\textbf{Model} & \textbf{Test Acc (\%)} & \textbf{\#P} & \textbf{Search Cost} \\ 
&\textbf{(\%)} & & \textbf{(GPU days)}\\\hline
    Random Search~\citep{DBLP:journals/corr/abs-1910-11831} & $96.71$ & - & - \\\hline 
    DARTS (1st)~\citep{DBLP:conf/iclr/LiuSY19} & $93.82$ & $0$ & $1.6$ \\
    DARTS (2nd)~\citep{DBLP:conf/iclr/LiuSY19} & $94.85$ & $0$ & $4.0$ \\
    P-DARTS~\citep{DBLP:journals/ijcv/ChenXWT21} & $94.62$ & $0$ & $1.2$ \\
    PC-DARTS~\citep{DBLP:conf/iclr/XuX0CQ0X20} & $96.85$ & $3$ & $0.4$\\
    Amended-DARTS~\citep{DBLP:journals/corr/abs-1910-11831} & $97.29$ & $7$ & $4.8$ \\ \hline
    \mydarts & $\mathbf{97.34}$ & $5$ & $3.2$ \\
\end{tabular}
\end{table}
Recently, it has been shown that performance collapse happens more consistently and frequently the longer the search process is performed~\citep{DBLP:journals/corr/abs-1910-11831}. This observation is inline with our analysis, which shows the negative impacts of low \lambdafn becomes more pronounced with longer epochs of search, which is attributed to the softmax saturation point and vanishing gradients. In order to show that \mydarts has successfully alleviated this problem even in the most complex search spaces, we performed the search on the DARTS search space for $200$ epochs following~\citep{DBLP:journals/corr/abs-1910-11831}. You can see the results in Table-\ref{table:200epochs}.
\par Clearly, our method successfully mitigates performance collapse even in the most complex search spaces. Among the baselines, the only method with comparable performance to \mydarts is Amended-DARTS, which performs the search at $50\%$ more search cost due to its second-order nature. Furthermore, comparing the number of parametric operations, we can see that our method is much better at discovering efficient architecture that are not unnecessarily over-parameterized. On the otherhand, Amended-DARTS perform poorly despite the fact that it has two more parametric operations. Furthermore, we can see that the only models performing better than random baseline are PC-DARTS, Amended-DARTS, and \mydarts. Considering the connection between \mydarts and PC-DARTS, as pointed out at Appendix-\ref{sec:appndx-priorwork}, this observation further proves the role of vanishing gradient and low \lambdafn in performance collapse.
\subsection{Search results on ImageNet}
\label{appndx:imagenet-search}
\begin{table}[t]
\centering
\caption{The test performance of \mydarts, compared with baselines, with the search and evaluation performed on ImageNet and the DARTS search space.} 
\label{table:imagenet-search}
\begin{tabular}{l|c|c}
\textbf{Model} &\textbf{ImageNet Top-1 Acc} & \textbf{ImageNet Top-5 Acc} \\ 
& \textbf{(\%)} & \textbf{(\%)} \\\hline
NASNet-A~\citep{DBLP:conf/cvpr/ZophVSL18} &	$74.0$ &	$91.6$\\
DARTS (2nd)~\citep{DBLP:conf/iclr/LiuSY19}	& $73.3$ &	$91.3$\\
SNAS~\citep{DBLP:conf/iclr/XieZLL19} &	$72.7$ &	$90.8$ \\
GDAS~\citep{DBLP:conf/cvpr/DongY19} &	$74.0$ &	$91.5$ \\
P-DARTS~\citep{DBLP:journals/ijcv/ChenXWT21} &	$75.3$ &	$92.5$ \\
PC-DARTS~\citep{DBLP:conf/iclr/XuX0CQ0X20} &	$75.8$ &	$92.7$ \\
DrNAS~\citep{DBLP:conf/iclr/ChenWCTH21} &	$75.8$ &	$92.7$\\
SDARTS-ADV~\citep{DBLP:conf/icml/ChenH20} &	$74.8$ &	$92.2$\\
DOTS~\citep{DBLP:conf/cvpr/GuW0YWLC21} &	$75.7$ &	$92.6$ \\
DARTS+PT~\citep{DBLP:conf/iclr/WangCCTH21} &	$74.5$ &	$92.0$ \\
$\beta$-DARTS~\citep{DBLP:journals/corr/abs-2203-01665} &	$75.8$ &	$92.9$ \\\hline
\mydarts-$\Lambda(\omega, \alpha)$ &	$\mathbf{75.9}$ &	$\mathbf{93.0}$

\end{tabular}
\end{table}
\par In this section, we provide the search and evaluation results for the ImageNet dataset. The search was performed on the setting proposed by PC-DARTS~\citep{DBLP:conf/iclr/XuX0CQ0X20}, on a search model with 8 layers (6 normal and 2 reduction cells) and 16 initial channels. Following PC-DARTS we randomly sampled $10\%$ and $2.5\%$ of the ImageNet dataset for optimizing 
$\omega$ and $\alpha$, respectively. We used a batch size of $384$ and a learning rate of $0.5$ which is linearly decayed to $0.0$. The evaluation was performed with a similar setting, for $250$ epochs with a model with $14$ layers and $48$ initial channels. You can see the results in Table-\ref{table:imagenet-search}.
\par We can see that our proposed method improves upon all baselines both in terms of top-1 and top-5 accuracy. We improve upon the performance of DARTS by $2.5\%$ in terms of top-1 accuracy and $1.7\%$ in terms of top-5 accuracy. Furthermore, we beat the best-performing baseline, namely $\beta$-DARTS, by a margin of $0.1\%$ in terms of top-1 accuracy and $0.1\%$ in terms of top-5 accuracy. We also note that DrNAS utilizes a slightly different setting for search, which performs the search on a search model with 14 layers and 48 initial channels, similar to the evaluation model. Despite this advantage, our proposed method beats the performance of DrNAS by a margin of $0.1\%$ on top-1 accuracy and $0.3\%$ on top-5 accuracy. The discovered architectures can be seen in Figure-\ref{fig:imagenet-cells}.
\subsection{Search cost}
\label{sec:appndx-search-cost}
\par One of the important factors in NAS is the cost of performing the search on a large-scale dataset, which is usually measured in terms of GPU hours or GPU days on the DARTS search space and the CIFAR-10 dataset~\citep{DBLP:journals/jmlr/ElskenMH19}. Since \mydarts requires two extra forward-backward passes to estimate the gradients of the regularization term, it costs twice as much as DARTS to perform the search on the same dataset. Therefore, the cost of our method in terms of GPU days is $0.8$ days on a GTX 1080 Ti GPU on the DARTS search space and CIFAR-10 dataset, which is $20\%$ less than the second-order DARTS (at $1.0$ GPU days) and about $40\%$ less than SDARTS-ADV (at about $1.3$ GPU days), both of which perform inferior to our model.
\par In order to reduce the search cost and make our method comparable to most baselines, we can perform forward or backward finite difference approximation instead of a central difference approximation~\citep{DBLP:journals/corr/abs-2009-07098}:
\begin{equation}
    \nabla_\omega^{fwd} \Lambda(\omega, \alpha)=\frac{1}{\binom{L}{2}} \left[\frac{\nabla_\omega \mathcal{L}(\omega, \mP+\epsilon \Delta)-\nabla_\omega \mathcal{L}(\omega, \mP)}{\epsilon}\right]+O(\epsilon),
\end{equation}
\begin{equation}
    \nabla_\omega^{bwd} \Lambda(\omega, \alpha)=\frac{1}{\binom{L}{2}} \left[\frac{\nabla_\omega \mathcal{L}(\omega, \mP)-\nabla_\omega \mathcal{L}(\omega, \mP-\epsilon \Delta)}{\epsilon}\right]+O(\epsilon).
\end{equation}
\begin{table}[t!]
\centering
\caption{The test performance of architectures discovered by \mydarts with central finite difference approximation (ctr) and forward/backward finite difference approximation (f/b) for $50$ epochs of search on CIFAR-10.} 
\label{table:lambda-darts-est}
\begin{tabular}{l|c|c|c}
\textbf{Method} & \textbf{CIFAR-10 Acc (\%)} & \textbf{CIFAR-100 Acc (\%)} & \textbf{ImageNet Acc (\%)}\\ \hline
    \mydarts (ctr) & $94.04\pm 0.03$ & $72.75\pm 0.42$ & $45.49\pm 0.08$ \\
    \mydarts (f/b) & $93.95\pm 0.15$ & $72.33\pm 0.71$ & $44.98\pm 0.90$
\end{tabular}
\end{table}Note that since $\nabla_\omega^{fwd} \Lambda(\omega, \alpha)$ and $\nabla_\omega^{bwd} \Lambda(\omega, \alpha)$ are biased estimations, we can use them in an interleaved fashion, where we perform forward estimation for one mini-batch, and a backward estimation in the next. This will reduce the cost by $25\%$, putting our method at around $0.6$ GPU days, which is only $50\%$ more than the original first-order DARTS (at $0.4$ GPU days), and comparable to DrNAS with progressive search. You can see a thorough comparison with state-of-the-art NAS models in terms of performance on CIFAR-10 and search cost in Table-\ref{table:search-cost}.
\par In order to show the reduction in the accuracy of the estimation does not reduce the effectiveness of \mydarts, we have provided the results of discovered architectures by this low-cost method compared to the central method for $50$ epochs on the NAS-Bench-201 search space on CIFAR-10 in Table-\ref{table:lambda-darts-est}. We can see that in all three datasets, the drop in average performance is negligible (less than $0.1\%$ in CIFAR-10 and less than $0.5\%$ in CIFAR-100 and ImageNet). Therefore, it is clear that using forward/backward finite difference approximation can be a good technique to reduce the cost of searching on large datasets, which makes our method comparable to DrNAS in terms of cost for large scale applications.
\begin{table}[t!]
\centering
\caption{Comparison with state-of-the-art NAS models on the CIFAR-10 dataset. We provide the search cost based on GPU days on a 1080 Ti.} 
\label{table:search-cost}
\begin{tabular}{l|c|c|c}
\textbf{Method} & \textbf{Test Acc} & \textbf{Params} & \textbf{Search Cost}\\
&  \textbf{(\%)} & \textbf{(M)} & \textbf{(GPU days)} \\\hline
    NASNet-A~\citep{DBLP:conf/cvpr/ZophVSL18} & $97.35$ & $3.3$ & 2000 \\
    ENAS~\citep{DBLP:conf/icml/PhamGZLD18} & $97.11$ & $4.6$ & $0.5$ \\
    DARTS (1st)~\citep{DBLP:conf/iclr/LiuSY19} & $97.00\pm 0.14$ & $3.4$ & 0.4 \\
    DARTS (2nd)~\citep{DBLP:conf/iclr/LiuSY19} & $97.24\pm 0.09$  & $3.3$ & $1.0$ \\
    SNAS~\citep{DBLP:conf/iclr/XieZLL19} & $97.15\pm 0.02$ &  $2.8$ & $1.5$ \\
    GDAS~\citep{DBLP:conf/cvpr/DongY19} & $97.07$ & $3.4$ & $0.3$ \\
    P-DARTS~\citep{DBLP:conf/eccv/LiuZNSHLFYHM18} & $97.50$ & $3.4$ & $0.3$ \\
    R-DARTS~\citep{DBLP:conf/iclr/ZelaESMBH20} & $97.05\pm 0.21$ & - & $1.6$ \\
    PC-DARTS~\citep{DBLP:conf/iclr/XuX0CQ0X20} & $97.43\pm 0.07$ & $3.6$ & $0.1$ \\
    DrNAS~\citep{DBLP:conf/iclr/ChenWCTH21} & $97.46\pm 0.03$ & $4.0$ & $0.4$ \\
    SDARTS-ADV~\citep{DBLP:conf/icml/ChenH20} & $97.39\pm 0.02$ & $3.3$ & $1.3$ \\
    DARTS-~\citep{DBLP:conf/iclr/ChuW0LWY21} & $97.41\pm 0.08$ & $3.5$ & $0.4$ \\
    DARTS+PT~\citep{DBLP:conf/iclr/WangCCTH21} & $97.39\pm 0.08$ & $3.0$ & $0.8$ \\
    
    \hline
    \mydarts~-~$\Lambda(\omega, \alpha)$ & $97.48\pm0.11$ & $3.5$ & $0.8$ \\
    \mydarts~-~$\Lambda_\pm(\omega, \alpha)$ & $\mathbf{97.57\pm 0.05}$ & $3.6$ & $0.8$\\
\end{tabular}
\end{table}
\subsection{Experiment settings}
\label{appndx:exp-setting}
\subsubsection{NAS-Bench-201 search space}
 NAS-Bench-201 provides a DARTS-like cell-search framework, where we search for a single cell containing 3 nodes and 5 operations. The performance of the discovered architecture on CIFAR-10, CIFAR-100, and ImageNet16-120 datasets can be attained by querrying the database provided by~\citep{DBLP:conf/iclr/Dong020}. We used the implementation of~\citep{DBLP:conf/iclr/Dong020} for our search. The operation parameters ($\omega$) are optimized using stochastic gradient descent with Nestrov momentum. The learning rate is set to $0.025$, gradually reduced to $0.001$ using cosine scheduling. The weight decay and momentum are set to $0.0005$ and $0.9$, respectively.  
 \par For architecture parameters ($\alpha$), we use Adam with the learning rate set to $10^{-4}$ and the weight decay rate set to $0.001$. The momentum values $\beta_1$ and $\beta_2$ are set to $0.5$ and $0.999$, respectively. We perform the search on CIFAR-10 for $100$ epochs, and we set $\epsilon_0=0.0001$ and $\lambda=0.125$ for this experiment. 
 \subsubsection{DARTS search space}
 The search space of DARTS contains two types of cells: Normal and Reduction, corresponding to layers with stride of 1 and 2, respectively. Each cell has 4 intermediate nodes and 7 operations. Here, we only apply the regularization term to the normal cell, since the performance collapse is not usually associated with the reduction cell~\citep{DBLP:conf/iclr/ZelaESMBH20}. We used the implementation provided by~\citep{DBLP:conf/iclr/WangCCTH21, DBLP:conf/icml/ChenH20}. The operation parameters ($\omega$) are optimized using stochastic gradient descent with momentum. The learning rate is set to $0.025$, gradually reduced to $0.001$ using cosine scheduling. The weight decay and momentum are set to $0.0003$ and $0.9$, respectively.  
 \par For architecture parameters ($\alpha$), we use Adam with the learning rate set to $3\times 10^{-4}$ and the weight decay rate set to $0.001$. The momentum values $\beta_1$ and $\beta_2$ are set to $0.5$ and $0.999$, respectively. We perform the search on CIFAR-10 for $50$ epochs, and we set the value for $\lambda$ to $0.25$ for $\Lambda(\omega, \alpha)$ and $0.125$ for $\Lambda_\pm(\omega, \alpha)$. In both cases, we set the value of $\epsilon_0$ to $0.0001$.
 \subsubsection{Reduced search spaces}
 The four search spaces proposed by~\citep{DBLP:conf/iclr/ZelaESMBH20} contain some subset of the operations used in the DARTS search space, except for the fourth search space which contains convolutions and random noise as operations. Otherwise, the cells have the same structure as in the DARTS search space.
\par We perform the search and evaluation on three different datasets: CIFAR-10, CIFAR-100, and SVHN. For CIFAR-10, the search setting is the same as in Section-\ref{sec:DARTS-exp}. For SVHN and CIFAR-100, we increase the batch size and the value of $\epsilon_0$ to $96$ and $0.001$, respectively. The evaluation setting for these datasets is the same as in~\citep{DBLP:conf/iclr/ZelaESMBH20}. For all experiments, we only report the results of $\Lambda(\omega, \alpha)$, since both methods perform similarly.
\subsubsection{Ablation study}
 For this experiment, we used the exact same setting as in Section-\ref{sec:nas-bench}, except for the architecture learning rate. For the experiments on $\epsilon_0$ and $\lambda$, the architecture learning rate is set to $3\times 10^{-4}$. For the experiments on the number of epochs, the architecture learning rate is set to $3\times 10^{-4}$ when performing the search for $50$ epochs, and reduced to $10^{-4}$ when performing the search for more than $50$ epochs.
 \par For experiments corresponding to $\epsilon_0$, we set $\lambda=0.125$, while for experiments corresponding to $\lambda$, we set $\epsilon_0=0.0001$, and for experiments corresponding to the number of epochs, we set $\epsilon_0=0.0001$ and $\lambda=0.125$.
 \subsection{Proof of Proposition \ref{prop:1}}
 \label{appndx:proof}
 \textbf{Proof:} Using simple linear algebra, we can re-write the squared norm of the gradient $\nabla_\alpha \mathcal{L}(\omega, \alpha)$ as:
 \begin{equation}
     \Vert \nabla_\alpha \mathcal{L}(\omega, \alpha) \Vert_2^2
     =\Vert \mJ_\sigma (\alpha)\nabla_\vp \mathcal{L}(\omega, \alpha)\Vert_2^2=
     \nabla_\vp \mathcal{L}(\omega, \alpha)^T \mJ_\sigma (\alpha)^T \mJ_\sigma (\alpha) \nabla_\vp \mathcal{L}(\omega, \alpha).
 \end{equation}
 Now using the assumption that $\nabla_\vp \mathcal{L}(\omega, \alpha)$ is the sum of vectors orthogonal to the null-space of the Jacobian matrix and the fact that $\mJ_\sigma (\alpha)^T \mJ_\sigma (\alpha)$ is a positive semi-definite matrix, we can give the following lower-bound for this term~\citep{DBLP:books/siam/Meyer00}:
\begin{equation}
\label{eqn:eigen-norm}
    \nabla_\vp \mathcal{L}(\omega, \alpha)^T \mJ_\sigma (\alpha)^T \mJ_\sigma (\alpha) \nabla_\vp \mathcal{L}(\omega, \alpha)\geq \min_{i, \lambda_i(\alpha)\neq 0} \lambda_i^2 \cdot \Vert \nabla_\vp \mathcal{L}(\omega, \alpha)\Vert_2^2.
\end{equation}
Since we assume the gradient $\nabla_\vp \mathcal{L}(\omega, \alpha)$ can be written as the sum of orthogonal layer gradients, its norm can be lower bounded by the smallest gradient~\citep{DBLP:books/siam/Meyer00}:

\begin{align}
    \Vert \nabla_\vp \mathcal{L}(\omega, \alpha)\Vert_2^2
    &=\Vert\sum_{\ell=1}^L \nabla_{{}^\ell \vp} \mathcal{L}(\omega, \alpha)\Vert_2^2
    \label{eqn:s1}\\
    &=\sum_{\ell=1}^L \Vert\nabla_{{}^\ell \vp} \mathcal{L}(\omega, \alpha)\Vert_2^2
    \label{eqn:s2}\\
    &\geq L \cdot \min_\ell  \Vert\nabla_{{}^\ell \vp} \mathcal{L}(\omega, \alpha)\Vert_2^2,
    \label{eqn:sum-norm}
\end{align}
where \ref{eqn:s2} follows from the orthogonality. So using (\ref{eqn:eigen-norm}) and (\ref{eqn:sum-norm}), we can get:
\begin{equation}
    \Vert \nabla_\alpha \mathcal{L}(\omega, \alpha) \Vert_2^2\geq \min_{i, \lambda_i(\alpha)\neq 0} \lambda_i^2 \cdot L \cdot \min_\ell  \Vert\nabla_{{}^\ell \vp} \mathcal{L}(\omega, \alpha)\Vert_2^2.
\end{equation}

\subsection{Discovered architectures}
\label{appndx:archs}
\begin{figure}[h]
    \centering
    \begin{subfigure}[h]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/archs/corrdarts_corr_3_normal.pdf}
        \caption{Normal Cell}
    \end{subfigure}\begin{subfigure}[h]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/archs/corrdarts_corr_3_reduction.pdf}
        \caption{Reduction Cell}
    \end{subfigure}
    \caption{Best normal and reduction cells discovered by $\Lambda(\omega, \alpha)$ on DARTS search space and CIFAR-10 dataset.}
\end{figure}
\begin{figure}[h]
    \centering
    \begin{subfigure}[h]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/archs/corrdarts_sign_2_normal.pdf}
        \caption{Normal Cell}
    \end{subfigure}\begin{subfigure}[h]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/archs/corrdarts_sign_2_reduction.pdf}
        \caption{Reduction Cell}
    \end{subfigure}
    \caption{Best normal and reduction cells discovered by $\Lambda_\pm(\omega, \alpha)$ on DARTS search space and CIFAR-10 dataset.}
\end{figure}
\begin{figure}[h]
    \centering
    \begin{subfigure}[h]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/archs/c10_s1_corrdarts_corr_normal.pdf}
        \caption{Normal Cell}
    \end{subfigure}\begin{subfigure}[h]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/archs/c10_s1_corrdarts_corr_reduction.pdf}
        \caption{Reduction Cell}
    \end{subfigure}
    \caption{Best normal and reduction cells discovered by $\Lambda(\omega, \alpha)$ on S1 and CIFAR-10 dataset.}
\end{figure}
\begin{figure}[h]
    \centering
    \begin{subfigure}[h]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/archs/c10_s2_corrdarts_corr_normal.pdf}
        \caption{Normal Cell}
    \end{subfigure}\begin{subfigure}[h]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/archs/c10_s2_corrdarts_corr_reduction.pdf}
        \caption{Reduction Cell}
    \end{subfigure}
    \caption{Best normal and reduction cells discovered by $\Lambda(\omega, \alpha)$ on S2 and CIFAR-10 dataset.}
\end{figure}
\begin{figure}[h]
    \centering
    \begin{subfigure}[h]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/archs/c10_s3_corrdarts_corr_normal.pdf}
        \caption{Normal Cell}
    \end{subfigure}\begin{subfigure}[h]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/archs/c10_s3_corrdarts_corr_reduction.pdf}
        \caption{Reduction Cell}
    \end{subfigure}
    \caption{Best normal and reduction cells discovered by $\Lambda(\omega, \alpha)$ on S3 and CIFAR-10 dataset.}
\end{figure}
\begin{figure}[h]
    \centering
    \begin{subfigure}[h]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/archs/c10_s4_corrdarts_corr_normal.pdf}
        \caption{Normal Cell}
    \end{subfigure}\begin{subfigure}[h]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/archs/c10_s4_corrdarts_corr_reduction.pdf}
        \caption{Reduction Cell}
    \end{subfigure}
    \caption{Best normal and reduction cells discovered by $\Lambda(\omega, \alpha)$ on S4 and CIFAR-10 dataset.}
\end{figure}
\begin{figure}[h]
    \centering
    \begin{subfigure}[h]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/archs/c100_s1_corrdarts_corr_normal.pdf}
        \caption{Normal Cell}
    \end{subfigure}\begin{subfigure}[h]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/archs/c100_s1_corrdarts_corr_reduction.pdf}
        \caption{Reduction Cell}
    \end{subfigure}
    \caption{Best normal and reduction cells discovered by $\Lambda(\omega, \alpha)$ on S1 and CIFAR-100 dataset.}
\end{figure}
\begin{figure}[h]
    \centering
    \begin{subfigure}[h]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/archs/c100_s2_corrdarts_corr_normal.pdf}
        \caption{Normal Cell}
    \end{subfigure}\begin{subfigure}[h]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/archs/c100_s2_corrdarts_corr_reduction.pdf}
        \caption{Reduction Cell}
    \end{subfigure}
    \caption{Best normal and reduction cells discovered by $\Lambda(\omega, \alpha)$ on S2 and CIFAR-100 dataset.}
\end{figure}
\begin{figure}[h]
    \centering
    \begin{subfigure}[h]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/archs/c100_s3_corrdarts_corr_normal.pdf}
        \caption{Normal Cell}
    \end{subfigure}\begin{subfigure}[h]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/archs/c100_s3_corrdarts_corr_reduction.pdf}
        \caption{Reduction Cell}
    \end{subfigure}
    \caption{Best normal and reduction cells discovered by $\Lambda(\omega, \alpha)$ on S3 and CIFAR-100 dataset.}
\end{figure}
\begin{figure}[h]
    \centering
    \begin{subfigure}[h]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/archs/c100_s4_corrdarts_corr_normal.pdf}
        \caption{Normal Cell}
    \end{subfigure}\begin{subfigure}[h]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/archs/c100_s4_corrdarts_corr_reduction.pdf}
        \caption{Reduction Cell}
    \end{subfigure}
    \caption{Best normal and reduction cells discovered by $\Lambda(\omega, \alpha)$ on S4 and CIFAR-100 dataset.}
\end{figure}
\begin{figure}[h]
    \centering
    \begin{subfigure}[h]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/archs/svhn_s1_corrdarts_corr_normal.pdf}
        \caption{Normal Cell}
    \end{subfigure}\begin{subfigure}[h]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/archs/svhn_s1_corrdarts_corr_reduction.pdf}
        \caption{Reduction Cell}
    \end{subfigure}
    \caption{Best normal and reduction cells discovered by $\Lambda(\omega, \alpha)$ on S1 and SVHN dataset.}
\end{figure}
\begin{figure}[h]
    \centering
    \begin{subfigure}[h]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/archs/svhn_s2_corrdarts_corr_normal.pdf}
        \caption{Normal Cell}
    \end{subfigure}\begin{subfigure}[h]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/archs/svhn_s2_corrdarts_corr_reduction.pdf}
        \caption{Reduction Cell}
    \end{subfigure}
    \caption{Best normal and reduction cells discovered by $\Lambda(\omega, \alpha)$ on S2 and SVHN dataset.}
\end{figure}
\begin{figure}[h]
    \centering
    \begin{subfigure}[h]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/archs/svhn_s3_corrdarts_corr_normal.pdf}
        \caption{Normal Cell}
    \end{subfigure}\begin{subfigure}[h]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/archs/svhn_s3_corrdarts_corr_reduction.pdf}
        \caption{Reduction Cell}
    \end{subfigure}
    \caption{Best normal and reduction cells discovered by $\Lambda(\omega, \alpha)$ on S3 and SVHN dataset.}
\end{figure}
\begin{figure}[h]
    \centering
    \begin{subfigure}[h]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/archs/svhn_s4_corrdarts_corr_normal.pdf}
        \caption{Normal Cell}
    \end{subfigure}\begin{subfigure}[h]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/archs/svhn_s4_corrdarts_corr_reduction.pdf}
        \caption{Reduction Cell}
    \end{subfigure}
    \caption{Best normal and reduction cells discovered by $\Lambda(\omega, \alpha)$ on S4 and SVHN dataset.}
\end{figure}
\begin{figure}[h]
    \centering
    \begin{subfigure}[h]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/archs/imagenet_normal.pdf}
        \caption{Normal Cell}
    \end{subfigure}\begin{subfigure}[h]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/archs/imagenet_reduction.pdf}
        \caption{Reduction Cell}
    \end{subfigure}
    \caption{Best normal and reduction cells discovered by $\Lambda(\omega, \alpha)$ on DARTS search space and the ImageNet dataset.}
    \label{fig:imagenet-cells}
\end{figure}

\end{document}
