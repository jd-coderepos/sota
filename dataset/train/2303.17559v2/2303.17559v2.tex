\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{lipsum}
\usepackage{marvosym}

\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}
\hypersetup{citecolor=[RGB]{119,185,0}}


\iccvfinalcopy 

\def\iccvPaperID{7009} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ificcvfinal\pagestyle{empty}\fi

\usepackage[mathscr]{eucal}
\usepackage{amssymb,amsmath,amsfonts,latexsym}
\usepackage{amsmath,graphicx,bm,xcolor,url}
\usepackage[caption=false]{subfig} 
\usepackage{bm}
\usepackage{mathrsfs}
\usepackage{amsthm}



\catcode`~=11 \def\UrlSpecials{\do\~{\kern -.15em\lower .7ex\hbox{~}\kern .04em}} \catcode`~=13 

\allowdisplaybreaks[3]
 
\newcommand{\Iff}{if and only if\;\,} 
\newcommand{\norm}[1]{\left\Vert#1\right\Vert}
\newcommand{\as}{{\rm \;\;a.s.}}
\newcommand{\nn}{\nonumber} 
\newcommand{\cequal}{\stackrel{\mathrm{c}}{=}}
\newcommand{\dequal}{\stackrel{\mathrm{d}}{=}}
\newcommand{\St}{\mathrm{subject\; to}}

\newcommand{\calA}{\mathcal{A}}
\newcommand{\calB}{\mathcal{B}}
\newcommand{\calC}{\mathcal{C}}
\newcommand{\calD}{\mathcal{D}}
\newcommand{\calE}{\mathcal{E}}
\newcommand{\calF}{\mathcal{F}}
\newcommand{\calG}{\mathcal{G}}
\newcommand{\calH}{\mathcal{H}}
\newcommand{\calI}{\mathcal{I}}
\newcommand{\calJ}{\mathcal{J}}
\newcommand{\calK}{\mathcal{K}}
\newcommand{\calL}{\mathcal{L}}
\newcommand{\calM}{\mathcal{M}}
\newcommand{\calN}{\mathcal{N}}
\newcommand{\calO}{\mathcal{O}}
\newcommand{\calP}{\mathcal{P}}
\newcommand{\calQ}{\mathcal{Q}}
\newcommand{\calR}{\mathcal{R}}
\newcommand{\calS}{\mathcal{S}}
\newcommand{\calT}{\mathcal{T}}
\newcommand{\calU}{\mathcal{U}}
\newcommand{\calV}{\mathcal{V}}
\newcommand{\calW}{\mathcal{W}}
\newcommand{\calX}{\mathcal{X}}
\newcommand{\calY}{\mathcal{Y}}
\newcommand{\calZ}{\mathcal{Z}}

\newcommand{\hatcalA}{\hat{\calA}} 
\newcommand{\hatcalB}{\hat{\calB}} 
\newcommand{\hatcalC}{\hat{\calC}} 
\newcommand{\hatcalD}{\hat{\calD}} 
\newcommand{\hatcalE}{\hat{\calE}} 
\newcommand{\hatcalF}{\hat{\calF}} 
\newcommand{\hatcalG}{\hat{\calG}} 
\newcommand{\hatcalH}{\hat{\calH}} 
\newcommand{\hatcalI}{\hat{\calI}} 
\newcommand{\hatcalJ}{\hat{\calJ}} 
\newcommand{\hatcalK}{\hat{\calK}} 
\newcommand{\hatcalL}{\hat{\calL}} 
\newcommand{\hatcalM}{\hat{\calM}} 
\newcommand{\hatcalN}{\hat{\calN}} 
\newcommand{\hatcalO}{\hat{\calO}} 
\newcommand{\hatcalP}{\hat{\calP}} 
\newcommand{\hatcalQ}{\hat{\calQ}} 
\newcommand{\hatcalR}{\hat{\calR}} 
\newcommand{\hatcalS}{\hat{\calS}} 
\newcommand{\hatcalT}{\hat{\calT}} 
\newcommand{\hatcalU}{\hat{\calU}} 
\newcommand{\hatcalV}{\hat{\calV}} 
\newcommand{\hatcalW}{\hat{\calW}} 
\newcommand{\hatcalX}{\hat{\calX}} 
\newcommand{\hatcalY}{\hat{\calY}} 
\newcommand{\hatcalZ}{\hat{\calZ}} 

\newcommand{\ba}{\mathbf{a}}
\newcommand{\bA}{\mathbf{A}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bB}{\mathbf{B}}
\newcommand{\bc}{\mathbf{c}}
\newcommand{\bC}{\mathbf{C}}
\newcommand{\bd}{\mathbf{d}}
\newcommand{\bD}{\mathbf{D}}
\newcommand{\be}{\mathbf{e}}
\newcommand{\bE}{\mathbf{E}}
\newcommand{\boldf}{\mathbf{f}}
\newcommand{\bF}{\mathbf{F}}
\newcommand{\bg}{\mathbf{g}}
\newcommand{\bG}{\mathbf{G}}
\newcommand{\bh}{\mathbf{h}}
\newcommand{\bH}{\mathbf{H}}
\newcommand{\bi}{\mathbf{i}}
\newcommand{\bI}{\mathbf{I}}
\newcommand{\bj}{\mathbf{j}}
\newcommand{\bJ}{\mathbf{J}}
\newcommand{\bk}{\mathbf{k}}
\newcommand{\bK}{\mathbf{K}}
\newcommand{\bl}{\mathbf{l}}
\newcommand{\bL}{\mathbf{L}}
\newcommand{\boldm}{\mathbf{m}}
\newcommand{\bM}{\mathbf{M}}
\newcommand{\bn}{\mathbf{n}}
\newcommand{\bN}{\mathbf{N}}
\newcommand{\bo}{\mathbf{o}}
\newcommand{\bO}{\mathbf{O}}
\newcommand{\bp}{\mathbf{p}}
\newcommand{\bP}{\mathbf{P}}
\newcommand{\bq}{\mathbf{q}}
\newcommand{\bQ}{\mathbf{Q}}
\newcommand{\br}{\mathbf{r}}
\newcommand{\bR}{\mathbf{R}}
\newcommand{\bs}{\mathbf{s}}
\newcommand{\bS}{\mathbf{S}}
\newcommand{\bt}{\mathbf{t}}
\newcommand{\bT}{\mathbf{T}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bU}{\mathbf{U}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bV}{\mathbf{V}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bW}{\mathbf{W}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bY}{\mathbf{Y}}
\newcommand{\bz}{\mathbf{z}}
\newcommand{\bZ}{\mathbf{Z}}

\newcommand{\rma}{\mathrm{a}}
\newcommand{\rmA}{\mathrm{A}}
\newcommand{\rmb}{\mathrm{b}}
\newcommand{\rmB}{\mathrm{B}}
\newcommand{\rmc}{\mathrm{c}}
\newcommand{\rmC}{\mathrm{C}}
\newcommand{\rmd}{\mathrm{d}}
\newcommand{\rmD}{\mathrm{D}}
\newcommand{\rme}{\mathrm{e}}
\newcommand{\rmE}{\mathrm{E}}
\newcommand{\rmf}{\mathrm{f}}
\newcommand{\rmF}{\mathrm{F}}
\newcommand{\rmg}{\mathrm{g}}
\newcommand{\rmG}{\mathrm{G}}
\newcommand{\rmh}{\mathrm{h}}
\newcommand{\rmH}{\mathrm{H}}
\newcommand{\rmi}{\mathrm{i}}
\newcommand{\rmI}{\mathrm{I}}
\newcommand{\rmj}{\mathrm{j}}
\newcommand{\rmJ}{\mathrm{J}}
\newcommand{\rmk}{\mathrm{k}}
\newcommand{\rmK}{\mathrm{K}}
\newcommand{\rml}{\mathrm{l}}
\newcommand{\rmL}{\mathrm{L}}
\newcommand{\rmm}{\mathrm{m}}
\newcommand{\rmM}{\mathrm{M}}
\newcommand{\rmn}{\mathrm{n}}
\newcommand{\rmN}{\mathrm{N}}
\newcommand{\rmo}{\mathrm{o}}
\newcommand{\rmO}{\mathrm{O}}
\newcommand{\rmp}{\mathrm{p}}
\newcommand{\rmP}{\mathrm{P}}
\newcommand{\rmq}{\mathrm{q}}
\newcommand{\rmQ}{\mathrm{Q}}
\newcommand{\rmr}{\mathrm{r}}
\newcommand{\rmR}{\mathrm{R}}
\newcommand{\rms}{\mathrm{s}}
\newcommand{\rmS}{\mathrm{S}}
\newcommand{\rmt}{\mathrm{t}}
\newcommand{\rmT}{\mathrm{T}}
\newcommand{\rmu}{\mathrm{u}}
\newcommand{\rmU}{\mathrm{U}}
\newcommand{\rmv}{\mathrm{v}}
\newcommand{\rmV}{\mathrm{V}}
\newcommand{\rmw}{\mathrm{w}}
\newcommand{\rmW}{\mathrm{W}}
\newcommand{\rmx}{\mathrm{x}}
\newcommand{\rmX}{\mathrm{X}}
\newcommand{\rmy}{\mathrm{y}}
\newcommand{\rmY}{\mathrm{Y}}
\newcommand{\rmz}{\mathrm{z}}
\newcommand{\rmZ}{\mathrm{Z}}


\newcommand{\bbA}{\mathbb{A}}
\newcommand{\bbB}{\mathbb{B}}
\newcommand{\bbC}{\mathbb{C}}
\newcommand{\bbD}{\mathbb{D}}
\newcommand{\bbE}{\mathbb{E}}
\newcommand{\bbF}{\mathbb{F}}
\newcommand{\bbG}{\mathbb{G}}
\newcommand{\bbH}{\mathbb{H}}
\newcommand{\bbI}{\mathbb{I}}
\newcommand{\bbJ}{\mathbb{J}}
\newcommand{\bbK}{\mathbb{K}}
\newcommand{\bbL}{\mathbb{L}}
\newcommand{\bbM}{\mathbb{M}}
\newcommand{\bbN}{\mathbb{N}}
\newcommand{\bbO}{\mathbb{O}}
\newcommand{\bbP}{\mathbb{P}}
\newcommand{\bbQ}{\mathbb{Q}}
\newcommand{\bbR}{\mathbb{R}}
\newcommand{\bbS}{\mathbb{S}}
\newcommand{\bbT}{\mathbb{T}}
\newcommand{\bbU}{\mathbb{U}}
\newcommand{\bbV}{\mathbb{V}}
\newcommand{\bbW}{\mathbb{W}}
\newcommand{\bbX}{\mathbb{X}}
\newcommand{\bbY}{\mathbb{Y}}
\newcommand{\bbZ}{\mathbb{Z}}


\newcommand{\frakA}{\mathfrak{A}}
\newcommand{\frakB}{\mathfrak{B}}
\newcommand{\frakC}{\mathfrak{C}}
\newcommand{\frakD}{\mathfrak{D}}
\newcommand{\frakE}{\mathfrak{E}}
\newcommand{\frakF}{\mathfrak{F}}
\newcommand{\frakG}{\mathfrak{G}}
\newcommand{\frakH}{\mathfrak{H}}
\newcommand{\frakI}{\mathfrak{I}}
\newcommand{\frakJ}{\mathfrak{J}}
\newcommand{\frakK}{\mathfrak{K}}
\newcommand{\frakL}{\mathfrak{L}}
\newcommand{\frakM}{\mathfrak{M}}
\newcommand{\frakN}{\mathfrak{N}}
\newcommand{\frakO}{\mathfrak{O}}
\newcommand{\frakP}{\mathfrak{P}}
\newcommand{\frakQ}{\mathfrak{Q}}
\newcommand{\frakR}{\mathfrak{R}}
\newcommand{\frakS}{\mathfrak{S}}
\newcommand{\frakT}{\mathfrak{T}}
\newcommand{\frakU}{\mathfrak{U}}
\newcommand{\frakV}{\mathfrak{V}}
\newcommand{\frakW}{\mathfrak{W}}
\newcommand{\frakX}{\mathfrak{X}}
\newcommand{\frakY}{\mathfrak{Y}}
\newcommand{\frakZ}{\mathfrak{Z}}


\newcommand{\scA}{\mathscr{A}}
\newcommand{\scB}{\mathscr{B}}
\newcommand{\scC}{\mathscr{C}}
\newcommand{\scD}{\mathscr{D}}
\newcommand{\scE}{\mathscr{E}}
\newcommand{\scF}{\mathscr{F}}
\newcommand{\scG}{\mathscr{G}}
\newcommand{\scH}{\mathscr{H}}
\newcommand{\scI}{\mathscr{I}}
\newcommand{\scJ}{\mathscr{J}}
\newcommand{\scK}{\mathscr{K}}
\newcommand{\scL}{\mathscr{L}}
\newcommand{\scM}{\mathscr{M}}
\newcommand{\scN}{\mathscr{N}}
\newcommand{\scO}{\mathscr{O}}
\newcommand{\scP}{\mathscr{P}}
\newcommand{\scQ}{\mathscr{Q}}
\newcommand{\scR}{\mathscr{R}}
\newcommand{\scS}{\mathscr{S}}
\newcommand{\scT}{\mathscr{T}}
\newcommand{\scU}{\mathscr{U}}
\newcommand{\scV}{\mathscr{V}}
\newcommand{\scW}{\mathscr{W}}
\newcommand{\scX}{\mathscr{X}}
\newcommand{\scY}{\mathscr{Y}}
\newcommand{\scZ}{\mathscr{Z}}

\DeclareMathAlphabet{\mathbsf}{OT1}{cmss}{bx}{n}
\DeclareMathAlphabet{\mathssf}{OT1}{cmss}{m}{sl}\newcommand{\rva}{\mathsf{a}}
\newcommand{\rvA}{\mathsf{A}}
\newcommand{\rvba}{\mathbsf{a}}
\newcommand{\rvbA}{\mathbsf{A}}
\newcommand{\rvb}{\mathsf{b}}
\newcommand{\rvB}{\mathsf{B}}
\newcommand{\rvbb}{\mathbsf{b}}
\newcommand{\rvbB}{\mathbsf{B}}
\newcommand{\rvc}{\mathsf{c}}
\newcommand{\rvC}{\mathsf{C}}
\newcommand{\rvbc}{\mathbsf{c}}
\newcommand{\rvbC}{\mathbsf{C}}
\newcommand{\rvd}{\mathsf{d}}
\newcommand{\rvD}{\mathsf{D}}
\newcommand{\rvbd}{\mathbsf{d}}
\newcommand{\rvbD}{\mathbsf{D}}
\newcommand{\rve}{\mathsf{e}}
\newcommand{\rvE}{\mathsf{E}}
\newcommand{\rvbe}{\mathbsf{e}}
\newcommand{\rvbE}{\mathbsf{E}}
\newcommand{\rvf}{\mathsf{f}}
\newcommand{\rvF}{\mathsf{F}}
\newcommand{\rvbf}{\mathbsf{f}}
\newcommand{\rvbF}{\mathbsf{F}}
\newcommand{\rvg}{\mathsf{g}}
\newcommand{\rvG}{\mathsf{G}}
\newcommand{\rvbg}{\mathbsf{g}}
\newcommand{\rvbG}{\mathbsf{G}}
\newcommand{\rvh}{\mathsf{h}}
\newcommand{\rvH}{\mathsf{H}}
\newcommand{\rvbh}{\mathbsf{h}}
\newcommand{\rvbH}{\mathbsf{H}}
\newcommand{\rvi}{\mathsf{i}}
\newcommand{\rvI}{\mathsf{I}}
\newcommand{\rvbi}{\mathbsf{i}}
\newcommand{\rvbI}{\mathbsf{I}}
\newcommand{\rvj}{\mathsf{j}}
\newcommand{\rvJ}{\mathsf{J}}
\newcommand{\rvbj}{\mathbsf{j}}
\newcommand{\rvbJ}{\mathbsf{J}}
\newcommand{\rvk}{\mathsf{k}}
\newcommand{\rvK}{\mathsf{K}}
\newcommand{\rvbk}{\mathbsf{k}}
\newcommand{\rvbK}{\mathbsf{K}}
\newcommand{\rvl}{\mathsf{l}}
\newcommand{\rvL}{\mathsf{L}}
\newcommand{\rvbl}{\mathbsf{l}}
\newcommand{\rvbL}{\mathbsf{L}}
\newcommand{\rvm}{\mathsf{m}}
\newcommand{\rvM}{\mathsf{M}}
\newcommand{\rvbm}{\mathbsf{m}}
\newcommand{\rvbM}{\mathbsf{M}}
\newcommand{\rvn}{\mathsf{n}}
\newcommand{\rvN}{\mathsf{N}}
\newcommand{\rvbn}{\mathbsf{n}}
\newcommand{\rvbN}{\mathbsf{N}}
\newcommand{\rvo}{\mathsf{o}}
\newcommand{\rvO}{\mathsf{O}}
\newcommand{\rvbo}{\mathbsf{o}}
\newcommand{\rvbO}{\mathbsf{O}}
\newcommand{\rvp}{\mathsf{p}}
\newcommand{\rvP}{\mathsf{P}}
\newcommand{\rvbp}{\mathbsf{p}}
\newcommand{\rvbP}{\mathbsf{P}}
\newcommand{\rvq}{\mathsf{q}}
\newcommand{\rvQ}{\mathsf{Q}}
\newcommand{\rvbq}{\mathbsf{q}}
\newcommand{\rvbQ}{\mathbsf{Q}}
\newcommand{\rvr}{\mathsf{r}}
\newcommand{\rvR}{\mathsf{R}}
\newcommand{\rvbr}{\mathbsf{r}}
\newcommand{\rvbR}{\mathbsf{R}}
\newcommand{\rvs}{\mathsf{s}}
\newcommand{\rvS}{\mathsf{S}}
\newcommand{\rvbs}{\mathbsf{s}}
\newcommand{\rvbS}{\mathbsf{S}}
\newcommand{\rvt}{\mathsf{t}}
\newcommand{\rvT}{\mathsf{T}}
\newcommand{\rvbt}{\mathbsf{t}}
\newcommand{\rvbT}{\mathbsf{T}}
\newcommand{\rvu}{\mathsf{u}}
\newcommand{\rvU}{\mathsf{U}}
\newcommand{\rvbu}{\mathbsf{u}}
\newcommand{\rvbU}{\mathbsf{U}}
\newcommand{\rvv}{\mathsf{v}}
\newcommand{\rvV}{\mathsf{V}}
\newcommand{\rvbv}{\mathbsf{v}}
\newcommand{\rvbV}{\mathbsf{V}}
\newcommand{\rvw}{\mathsf{w}}
\newcommand{\rvW}{\mathsf{W}}
\newcommand{\rvbw}{\mathbsf{w}}
\newcommand{\rvbW}{\mathbsf{W}}
\newcommand{\rvx}{\mathsf{x}}
\newcommand{\rvX}{\mathsf{X}}
\newcommand{\rvbx}{\mathbsf{x}}
\newcommand{\rvbX}{\mathbsf{X}}
\newcommand{\rvy}{\mathsf{y}}
\newcommand{\rvY}{\mathsf{Y}}
\newcommand{\rvby}{\mathbsf{y}}
\newcommand{\rvbY}{\mathbsf{Y}}
\newcommand{\rvz}{\mathsf{z}}
\newcommand{\rvZ}{\mathsf{Z}}
\newcommand{\rvbz}{\mathbsf{z}}
\newcommand{\rvbZ}{\mathbsf{Z}}

\newcommand{\rvTh}{\ssfTheta}
\newcommand{\svTh}{\Theta}
\newcommand{\rvbTh}{\bsfTheta}
\newcommand{\svbTh}{\boldsymbol{\Theta}}
\newcommand{\rvPh}{\ssfPhi}
\newcommand{\svPh}{\Phi}
\newcommand{\rvbPh}{\bsfPhi}
\newcommand{\svbPh}{\boldsymbol{\Phi}}

\DeclareSymbolFont{bsfletters}{OT1}{cmss}{bx}{n}  
\DeclareSymbolFont{ssfletters}{OT1}{cmss}{m}{n}
\DeclareMathSymbol{\bsfGamma}{0}{bsfletters}{'000}
\DeclareMathSymbol{\ssfGamma}{0}{ssfletters}{'000}
\DeclareMathSymbol{\bsfDelta}{0}{bsfletters}{'001}
\DeclareMathSymbol{\ssfDelta}{0}{ssfletters}{'001}
\DeclareMathSymbol{\bsfTheta}{0}{bsfletters}{'002}
\DeclareMathSymbol{\ssfTheta}{0}{ssfletters}{'002}
\DeclareMathSymbol{\bsfLambda}{0}{bsfletters}{'003}
\DeclareMathSymbol{\ssfLambda}{0}{ssfletters}{'003}
\DeclareMathSymbol{\bsfXi}{0}{bsfletters}{'004}
\DeclareMathSymbol{\ssfXi}{0}{ssfletters}{'004}
\DeclareMathSymbol{\bsfPi}{0}{bsfletters}{'005}
\DeclareMathSymbol{\ssfPi}{0}{ssfletters}{'005}
\DeclareMathSymbol{\bsfSigma}{0}{bsfletters}{'006}
\DeclareMathSymbol{\ssfSigma}{0}{ssfletters}{'006}
\DeclareMathSymbol{\bsfUpsilon}{0}{bsfletters}{'007}
\DeclareMathSymbol{\ssfUpsilon}{0}{ssfletters}{'007}
\DeclareMathSymbol{\bsfPhi}{0}{bsfletters}{'010}
\DeclareMathSymbol{\ssfPhi}{0}{ssfletters}{'010}
\DeclareMathSymbol{\bsfPsi}{0}{bsfletters}{'011}
\DeclareMathSymbol{\ssfPsi}{0}{ssfletters}{'011}
\DeclareMathSymbol{\bsfOmega}{0}{bsfletters}{'012}
\DeclareMathSymbol{\ssfOmega}{0}{ssfletters}{'012}

\newcommand{\hata}{\hat{a}}
\newcommand{\hatA}{\hat{A}}
\newcommand{\tila}{\tilde{a}}
\newcommand{\tilA}{\tilde{A}}
\newcommand{\hatba}{\hat{\ba}}
\newcommand{\hatbA}{\hat{\bA}}
\newcommand{\tilba}{\tilde{\ba}}
\newcommand{\tilbA}{\tilde{\bA}}

\newcommand{\hatb}{\hat{b}}
\newcommand{\hatB}{\hat{B}}
\newcommand{\tilb}{\tilde{b}}
\newcommand{\tilB}{\tilde{B}}
\newcommand{\hatbb}{\hat{\bb}}
\newcommand{\hatbB}{\hat{\bB}}
\newcommand{\tilbb}{\tilde{\bb}}
\newcommand{\tilbB}{\tilde{\bB}}

\newcommand{\hatc}{\hat{c}}
\newcommand{\hatC}{\hat{C}}
\newcommand{\tilc}{\tilde{c}}
\newcommand{\tilC}{\tilde{C}}
\newcommand{\hatbc}{\hat{\bc}}
\newcommand{\hatbC}{\hat{\bC}}
\newcommand{\tilbc}{\tilde{\bc}}
\newcommand{\tilbC}{\tilde{\bC}}

\newcommand{\hatd}{\hat{d}}
\newcommand{\hatD}{\hat{D}}
\newcommand{\tild}{\tilde{d}}
\newcommand{\tilD}{\tilde{D}}
\newcommand{\hatbd}{\hat{\bd}}
\newcommand{\hatbD}{\hat{\bD}}
\newcommand{\tilbd}{\tilde{\bd}}
\newcommand{\tilbD}{\tilde{\bD}}

\newcommand{\hate}{\hat{e}}
\newcommand{\hatE}{\hat{E}}
\newcommand{\tile}{\tilde{e}}
\newcommand{\tilE}{\tilde{E}}
\newcommand{\hatbe}{\hat{\be}}
\newcommand{\hatbE}{\hat{\bE}}
\newcommand{\tilbe}{\tilde{\be}}
\newcommand{\tilbE}{\tilde{\bE}}

\newcommand{\hatf}{\hat{f}}
\newcommand{\hatF}{\hat{F}}
\newcommand{\tilf}{\tilde{f}}
\newcommand{\tilF}{\tilde{F}}
\newcommand{\hatbf}{\hat{\boldf}}
\newcommand{\hatbF}{\hat{\bF}}
\newcommand{\tilbf}{\tilde{\boldf}}
\newcommand{\tilbF}{\tilde{\bF}}

\newcommand{\hatg}{\hat{g}}
\newcommand{\hatG}{\hat{G}}
\newcommand{\tilg}{\tilde{g}}
\newcommand{\tilG}{\tilde{G}}
\newcommand{\hatbg}{\hat{\bg}}
\newcommand{\hatbG}{\hat{\bG}}
\newcommand{\tilbg}{\tilde{\bg}}
\newcommand{\tilbG}{\tilde{\bG}}

\newcommand{\hath}{\hat{h}}
\newcommand{\hatH}{\hat{H}}
\newcommand{\tilh}{\tilde{h}}
\newcommand{\tilH}{\tilde{H}}
\newcommand{\hatbh}{\hat{\bh}}
\newcommand{\hatbH}{\hat{\bH}}
\newcommand{\tilbh}{\tilde{\bh}}
\newcommand{\tilbH}{\tilde{\bH}}

\newcommand{\hati}{\hat{i}}
\newcommand{\hatI}{\hat{I}}
\newcommand{\tili}{\tilde{i}}
\newcommand{\tilI}{\tilde{I}}
\newcommand{\hatbi}{\hat{\bi}}
\newcommand{\hatbI}{\hat{\bI}}
\newcommand{\tilbi}{\tilde{\bi}}
\newcommand{\tilbI}{\tilde{\bI}}

\newcommand{\hatj}{\hat{j}}
\newcommand{\hatJ}{\hat{J}}
\newcommand{\tilj}{\tilde{j}}
\newcommand{\tilJ}{\tilde{J}}
\newcommand{\hatbj}{\hat{\bj}}
\newcommand{\hatbJ}{\hat{\bJ}}
\newcommand{\tilbj}{\tilde{\bj}}
\newcommand{\tilbJ}{\tilde{\bJ}}

\newcommand{\hatk}{\hat{k}}
\newcommand{\hatK}{\hat{K}}
\newcommand{\tilk}{\tilde{k}}
\newcommand{\tilK}{\tilde{K}}
\newcommand{\hatbk}{\hat{\bk}}
\newcommand{\hatbK}{\hat{\bK}}
\newcommand{\tilbk}{\tilde{\bk}}
\newcommand{\tilbK}{\tilde{\bK}}

\newcommand{\hatl}{\hat{l}}
\newcommand{\hatL}{\hat{L}}
\newcommand{\till}{\tilde{l}}
\newcommand{\tilL}{\tilde{L}}
\newcommand{\hatbl}{\hat{\bl}}
\newcommand{\hatbL}{\hat{\bL}}
\newcommand{\tilbl}{\tilde{\bl}}
\newcommand{\tilbL}{\tilde{\bL}}

\newcommand{\hatm}{\hat{m}}
\newcommand{\hatM}{\hat{M}}
\newcommand{\tilm}{\tilde{m}}
\newcommand{\tilM}{\tilde{M}}
\newcommand{\hatbm}{\hat{\boldm}}
\newcommand{\hatbM}{\hat{\bM}}
\newcommand{\tilbm}{\tilde{\boldm}}
\newcommand{\tilbM}{\tilde{\bM}}

\newcommand{\hatn}{\hat{n}}
\newcommand{\hatN}{\hat{N}}
\newcommand{\tiln}{\tilde{n}}
\newcommand{\tilN}{\tilde{N}}
\newcommand{\hatbn}{\hat{\bn}}
\newcommand{\hatbN}{\hat{\bN}}
\newcommand{\tilbn}{\tilde{\bn}}
\newcommand{\tilbN}{\tilde{\bN}}

\newcommand{\hato}{\hat{o}}
\newcommand{\hatO}{\hat{O}}
\newcommand{\tilo}{\tilde{o}}
\newcommand{\tilO}{\tilde{O}}
\newcommand{\hatbo}{\hat{\bo}}
\newcommand{\hatbO}{\hat{\bO}}
\newcommand{\tilbo}{\tilde{\bo}}
\newcommand{\tilbO}{\tilde{\bO}}

\newcommand{\hatp}{\hat{p}}
\newcommand{\hatP}{\hat{P}}
\newcommand{\tilp}{\tilde{p}}
\newcommand{\tilP}{\tilde{P}}
\newcommand{\hatbp}{\hat{\bp}}
\newcommand{\hatbP}{\hat{\bP}}
\newcommand{\tilbp}{\tilde{\bp}}
\newcommand{\tilbP}{\tilde{\bP}}

\newcommand{\hatq}{\hat{q}}
\newcommand{\hatQ}{\hat{Q}}
\newcommand{\tilq}{\tilde{q}}
\newcommand{\tilQ}{\tilde{Q}}
\newcommand{\hatbq}{\hat{\bq}}
\newcommand{\hatbQ}{\hat{\bQ}}
\newcommand{\tilbq}{\tilde{\bq}}
\newcommand{\tilbQ}{\tilde{\bQ}}

\newcommand{\hatr}{\hat{r}}
\newcommand{\hatR}{\hat{R}}
\newcommand{\tilr}{\tilde{r}}
\newcommand{\tilR}{\tilde{R}}
\newcommand{\hatbr}{\hat{\br}}
\newcommand{\hatbR}{\hat{\bR}}
\newcommand{\tilbr}{\tilde{\br}}
\newcommand{\tilbR}{\tilde{\bR}}

\newcommand{\hats}{\hat{s}}
\newcommand{\hatS}{\hat{S}}
\newcommand{\tils}{\tilde{s}}
\newcommand{\tilS}{\tilde{S}}
\newcommand{\hatbs}{\hat{\bs}}
\newcommand{\hatbS}{\hat{\bS}}
\newcommand{\tilbs}{\tilde{\bs}}
\newcommand{\tilbS}{\tilde{\bS}}

\newcommand{\hatt}{\hat{t}}
\newcommand{\hatT}{\hat{T}}
\newcommand{\tilt}{\tilde{t}}
\newcommand{\tilT}{\tilde{T}}
\newcommand{\hatbt}{\hat{\bt}}
\newcommand{\hatbT}{\hat{\bT}}
\newcommand{\tilbt}{\tilde{\bt}}
\newcommand{\tilbT}{\tilde{\bT}}

\newcommand{\hatu}{\hat{u}}
\newcommand{\hatU}{\hat{U}}
\newcommand{\tilu}{\tilde{u}}
\newcommand{\tilU}{\tilde{U}}
\newcommand{\hatbu}{\hat{\bu}}
\newcommand{\hatbU}{\hat{\bU}}
\newcommand{\tilbu}{\tilde{\bu}}
\newcommand{\tilbU}{\tilde{\bU}}

\newcommand{\hatv}{\hat{v}}
\newcommand{\hatV}{\hat{V}}
\newcommand{\tilv}{\tilde{v}}
\newcommand{\tilV}{\tilde{V}}
\newcommand{\hatbv}{\hat{\bv}}
\newcommand{\hatbV}{\hat{\bV}}
\newcommand{\tilbv}{\tilde{\bv}}
\newcommand{\tilbV}{\tilde{\bV}}

\newcommand{\hatw}{\hat{w}}
\newcommand{\hatW}{\hat{W}}
\newcommand{\tilw}{\tilde{w}}
\newcommand{\tilW}{\tilde{W}}
\newcommand{\hatbw}{\hat{\bw}}
\newcommand{\hatbW}{\hat{\bW}}
\newcommand{\tilbw}{\tilde{\bw}}
\newcommand{\tilbW}{\tilde{\bW}}

\newcommand{\hatx}{\hat{x}}
\newcommand{\hatX}{\hat{X}}
\newcommand{\tilx}{\tilde{x}}
\newcommand{\tilX}{\tilde{X}}
\newcommand{\hatbx}{\hat{\bx}}
\newcommand{\hatbX}{\hat{\bX}}
\newcommand{\tilbx}{\tilde{\bx}}
\newcommand{\tilbX}{\tilde{\bX}}

\newcommand{\haty}{\hat{y}}
\newcommand{\hatY}{\hat{Y}}
\newcommand{\tily}{\tilde{y}}
\newcommand{\tilY}{\tilde{Y}}
\newcommand{\hatby}{\hat{\by}}
\newcommand{\hatbY}{\hat{\bY}}
\newcommand{\tilby}{\tilde{\by}}
\newcommand{\tilbY}{\tilde{\bY}}

\newcommand{\hatz}{\hat{z}}
\newcommand{\hatZ}{\hat{Z}}
\newcommand{\tilz}{\tilde{z}}
\newcommand{\tilZ}{\tilde{Z}}
\newcommand{\hatbz}{\hat{\bz}}
\newcommand{\hatbZ}{\hat{\bZ}}
\newcommand{\tilbz}{\tilde{\bz}}
\newcommand{\tilbZ}{\tilde{\bZ}}

\newcommand{\bara}{\bar{a}}
\newcommand{\barb}{\bar{b}}
\newcommand{\barc}{\bar{c}}
\newcommand{\bard}{\bar{d}}
\newcommand{\bare}{\bar{e}}
\newcommand{\barf}{\bar{f}}
\newcommand{\barg}{\bar{g}}
\newcommand{\bah}{\bar{h}}
\newcommand{\bari}{\bar{i}}
\newcommand{\barj}{\bar{j}}
\newcommand{\bark}{\bar{k}}
\newcommand{\barl}{\bar{l}}
\newcommand{\barm}{\bar{m}}
\newcommand{\barn}{\bar{n}}
\newcommand{\baro}{\bar{o}}
\newcommand{\barp}{\bar{p}}
\newcommand{\barq}{\bar{q}}
\newcommand{\barr}{\bar{r}}
\newcommand{\bars}{\bar{s}}
\newcommand{\bart}{\bar{t}}
\newcommand{\baru}{\bar{u}}
\newcommand{\barv}{\bar{v}}
\newcommand{\barw}{\bar{w}}
\newcommand{\barx}{\bar{x}}
\newcommand{\bary}{\bar{y}}
\newcommand{\barz}{\bar{z}}

\newcommand{\barA}{\bar{A}}
\newcommand{\barB}{\bar{B}}
\newcommand{\barC}{\bar{C}}
\newcommand{\barD}{\bar{D}}
\newcommand{\barE}{\bar{E}}
\newcommand{\barF}{\bar{F}}
\newcommand{\barG}{\bar{G}}
\newcommand{\barh}{\bar{H}}
\newcommand{\barI}{\bar{I}}
\newcommand{\barJ}{\bar{J}}
\newcommand{\barK}{\bar{K}}
\newcommand{\barL}{\bar{L}}
\newcommand{\barM}{\bar{M}}
\newcommand{\barN}{\bar{N}}
\newcommand{\barO}{\bar{O}}
\newcommand{\barP}{\bar{P}}
\newcommand{\barQ}{\bar{Q}}
\newcommand{\barR}{\bar{R}}
\newcommand{\barS}{\bar{S}}
\newcommand{\barT}{\bar{T}}
\newcommand{\barU}{\bar{U}}
\newcommand{\barV}{\bar{V}}
\newcommand{\barW}{\bar{W}}
\newcommand{\barX}{\bar{X}}
\newcommand{\barY}{\bar{Y}}
\newcommand{\barZ}{\bar{Z}}

\newcommand{\balpha}{\bm{\alpha}}
\newcommand{\bbeta}{\bm{\beta}}
\newcommand{\bgamma}{\bm{\gamma}}
\newcommand{\bdelta}{\bm{\delta}}
\newcommand{\btheta}{\bm{\theta}}
\newcommand{\btau}{\bm{\tau}}
\newcommand{\bpi}{\bm{\pi}}
\newcommand{\bepsilon}{\bm{\epsilon}}
\newcommand{\veps}{\varepsilon}
\newcommand{\bvarepsilon}{\bm{\varepsilon}}
\newcommand{\bsigma}{\bm{\sigma}}
\newcommand{\bzeta}{\bm{\zeta}}
\newcommand{\bmeta}{\bm{\eta}}
\newcommand{\bkappa}{\bm{\kappa}}
\newcommand{\bchi}{\bm{\chi}}
\newcommand{\bphi}{\bm{\phi}}
\newcommand{\bpsi}{\bm{\psi}}
\newcommand{\bomega}{\bm{\omega}}
\newcommand{\bxi}{\bm{\xi}}
\newcommand{\blambda}{\bm{\lambda}}
\newcommand{\brho}{\bm{\rho}}

\newcommand{\bGamma}{\bm{\Gamma}}
\newcommand{\bLambda}{\bm{\Lambda}}
\newcommand{\bSigma	}{\bm{\Sigma}}
\newcommand{\bPsi}{\bm{\Psi}}
\newcommand{\bDelta}{\bm{\Delta}}
\newcommand{\bXi}{\bm{\Xi}}
\newcommand{\bUpsilon}{\bm{\Upsilon}}
\newcommand{\bOmega}{\bm{\Omega}}
\newcommand{\bPhi}{\bm{\Phi}}
\newcommand{\bPi}{\bm{\Pi}}
\newcommand{\bTheta}{\bm{\Theta}}

\newcommand{\talpha}{\tilde{\alpha}}
\newcommand{\tbeta}{\tilde{\beta}}
\newcommand{\tgamma}{\tilde{\gamma}}
\newcommand{\tdelta}{\tilde{\delta}}
\newcommand{\ttheta}{\tilde{\theta}}
\newcommand{\ttau}{\tilde{\tau}}
\newcommand{\tpi}{\tilde{\pi}}
\newcommand{\tepsilon}{\tilde{\epsilon}}
\newcommand{\tvarepsilon}{\tilde{\varepsilon}}
\newcommand{\tsigma}{\tilde{\sigma}}
\newcommand{\tzeta}{\tilde{\zeta}}
\newcommand{\tmeta}{\tilde{\eta}}
\newcommand{\tkappa}{\tilde{\kappa}}
\newcommand{\tchi}{\tilde{\chi}}
\newcommand{\tphi}{\tilde{\phi}}
\newcommand{\tpsi}{\tilde{\psi}}
\newcommand{\tomega}{\tilde{\omega}}
\newcommand{\txi}{\tilde{\xi}}
\newcommand{\tlambda}{\tilde{\lambda}}
\newcommand{\trho}{\tilde{\rho}}

\newcommand{\halpha}{\hat{\alpha}}
\newcommand{\hbeta}{\hat{\beta}}
\newcommand{\hgamma}{\hat{\gamma}}
\newcommand{\hdelta}{\hat{\delta}}
\newcommand{\htheta}{\hat{\theta}}
\newcommand{\htau}{\hat{\tau}}
\newcommand{\hpi}{\hat{\pi}}
\newcommand{\hepsilon}{\hat{\epsilon}}
\newcommand{\hvarepsilon}{\hat{\varepsilon}}
\newcommand{\hsigma}{\hat{\sigma}}
\newcommand{\hzeta}{\hat{\zeta}}
\newcommand{\hmeta}{\hat{\eta}}
\newcommand{\hkappa}{\hat{\kappa}}
\newcommand{\hchi}{\hat{\chi}}
\newcommand{\hphi}{\hat{\phi}}
\newcommand{\hpsi}{\hat{\psi}}
\newcommand{\homega}{\hat{\omega}}
\newcommand{\hxi}{\hat{\xi}}
\newcommand{\hlambda}{\hat{\lambda}}
\newcommand{\hrho}{\hat{\rho}}

\newcommand{\ua}{\underline{a}}
\newcommand{\uA}{\underline{A}}
\newcommand{\ub}{\underline{b}}
\newcommand{\uB}{\underline{B}}
\newcommand{\uc}{\underline{c}}
\newcommand{\uC}{\underline{C}}
\newcommand{\ud}{\underline{d}}
\newcommand{\uD}{\underline{D}}
\newcommand{\ue}{\underline{e}}
\newcommand{\uE}{\underline{E}}
\newcommand{\uf}{\underline{f}}
\newcommand{\uF}{\underline{F}}
\newcommand{\ug}{\underline{g}}
\newcommand{\uG}{\underline{G}}
\newcommand{\uh}{\underline{h}}
\newcommand{\uH}{\underline{H}}
\newcommand{\ui}{\underline{i}}
\newcommand{\uI}{\underline{I}}
\newcommand{\uj}{\underline{j}}
\newcommand{\uJ}{\underline{J}}
\newcommand{\uk}{\underline{k}}
\newcommand{\uK}{\underline{K}}
\newcommand{\ul}{\underline{l}}
\newcommand{\uL}{\underline{L}}
\newcommand{\um}{\underline{m}}
\newcommand{\uM}{\underline{M}}
\newcommand{\un}{\underline{n}}
\newcommand{\uN}{\underline{N}}
\newcommand{\uo}{\underline{o}}
\newcommand{\uO}{\underline{O}}
\newcommand{\up}{\underline{p}}
\newcommand{\uP}{\underline{P}}
\newcommand{\uq}{\underline{q}}
\newcommand{\uQ}{\underline{Q}}
\newcommand{\ur}{\underline{r}}
\newcommand{\uR}{\underline{R}}
\newcommand{\us}{\underline{s}}
\newcommand{\uS}{\underline{S}}
\newcommand{\ut}{\underline{t}}
\newcommand{\uT}{\underline{T}}
\newcommand{\uu}{\underline{u}}
\newcommand{\uU}{\underline{U}}
\newcommand{\uv}{\underline{v}}
\newcommand{\uV}{\underline{V}}
\newcommand{\uw}{\underline{w}}
\newcommand{\uW}{\underline{W}}
\newcommand{\ux}{\underline{x}}
\newcommand{\uX}{\underline{X}}
\newcommand{\uy}{\underline{y}}
\newcommand{\uY}{\underline{Y}}
\newcommand{\uz}{\underline{z}}
\newcommand{\uZ}{\underline{Z}}




\def\fndot{\, \cdot \,}
\def\fndia{\, \diamond \,}
\def\fnstar{\, \star \,}



\newcommand{\eexp}[1]{e^{#1}}



\newcommand{\iid}{i.i.d.\ }



\newcommand{\convp}{\stackrel{\mathrm{p}}{\longrightarrow}}
\newcommand{\convwpone}{\stackrel{\mathrm{w.p.1}}{\longrightarrow}}
\newcommand{\convd}{\stackrel{\mathrm{d}}{\longrightarrow}}
\newcommand{\convD}{\stackrel{\mathrm{D}}{\longrightarrow}}

\newcommand{\ceil}[1]{\lceil{#1}\rceil}
\newcommand{\floor}[1]{\lfloor{#1}\rfloor}
\newcommand{\lrangle}[2]{\langle{#1},{#2}\rangle}


\newcommand{\dotleq}{\stackrel{.}{\leq}}
\newcommand{\dotlt}{\stackrel{.}{<}}
\newcommand{\dotgeq}{\stackrel{.}{\geq}}
\newcommand{\dotgt}{\stackrel{.}{>}}
\newcommand{\dotdoteq}{\stackrel{\,..}{=}}
\newcommand{\eqa}{\stackrel{(a)}{=}}
\newcommand{\eqb}{\stackrel{(b)}{=}}
\newcommand{\eqc}{\stackrel{(c)}{=}}
\newcommand{\eqd}{\stackrel{(d)}{=}}
\newcommand{\eqe}{\stackrel{(e)}{=}}
\newcommand{\eqf}{\stackrel{(f)}{=}}
\newcommand{\eqg}{\stackrel{(g)}{=}}
\newcommand{\eqh}{\stackrel{(h)}{=}}
\newcommand{\lea}{\stackrel{(a)}{\le}}
\newcommand{\leb}{\stackrel{(b)}{\le}}
\newcommand{\lec}{\stackrel{(c)}{\le}}
\newcommand{\led}{\stackrel{(d)}{\le}}
\newcommand{\lee}{\stackrel{(e)}{\le}}
\newcommand{\lef}{\stackrel{(f)}{\le}}
\newcommand{\leg}{\stackrel{(g)}{\le}}
\newcommand{\leh}{\stackrel{(h)}{\le}}
\newcommand{\gea}{\stackrel{(a)}{\ge}}
\newcommand{\geb}{\stackrel{(b)}{\ge}}
\newcommand{\gec}{\stackrel{(c)}{\ge}}
\newcommand{\ged}{\stackrel{(d)}{\ge}}
\newcommand{\gee}{\stackrel{(e)}{\ge}}
\newcommand{\gef}{\stackrel{(f)}{\ge}}
\newcommand{\geg}{\stackrel{(g)}{\ge}}
\newcommand{\geh}{\stackrel{(h)}{\ge}}
 
\newcommand{\Pen}{P_{\mathrm{e}}^{(n)}}
\newcommand{\Penone}{P_{\mathrm{e}, 1}^{(n)}}
\newcommand{\Pentwo}{P_{\mathrm{e}, 2}^{(n)}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argsup}{arg\,sup}
\DeclareMathOperator*{\arginf}{arg\,inf}
\DeclareMathOperator{\minimize}{minimize}
\DeclareMathOperator{\maximize}{maximize}
\DeclareMathOperator{\st}{s.t.\;}
\DeclareMathOperator{\erfc}{erfc}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\cum}{cum}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\spn}{span}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\adj}{adj}
\DeclareMathOperator{\var}{\mathsf{Var}}
\DeclareMathOperator{\Vol}{Vol}
\DeclareMathOperator{\cov}{\mathsf{Cov}}
\DeclareMathOperator{\sech}{sech}
\DeclareMathOperator{\sinc}{sinc}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\poly}{poly}
\DeclareMathOperator{\vect}{vec}
\newcommand{\Hb}{H_{\mathrm{b}}}\newcommand{\Ber}{\mathrm{Bern}}
\DeclareMathOperator*{\lms}{l.i.m.\,}
\newcommand{\varop}[1]{\var\left[{#1}\right]}
\newcommand{\covop}[2]{\cov\left({#1},{#2}\right)} 
\newcommand{\bzero}{\mathbf{0}}
\newcommand{\bone}{\mathbf{1}}

\def\independenT#1#2{\mathrel{\rlap{}\mkern5mu{#1#2}}}
\newcommand\indep{\protect\mathpalette{\protect\independenT}{\perp}}
 
\newtheorem{theorem}{Theorem} 
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{assumption}{Assumption}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition} 
\newtheorem{example}{Example} 
\newtheorem{remark}{Remark}




\newcommand{\qednew}{\nobreak \ifvmode \relax \else
      \ifdim\lastskip<1.5em \hskip-\lastskip
      \hskip1.5em plus0em minus0.5em \fi \nobreak
      \vrule height0.75em width0.5em depth0.25em\fi}

\newcommand{\bfA}{\mathbf{A}}
\newcommand{\bfB}{\mathbf{B}}
\newcommand{\bfC}{\mathbf{C}}
\newcommand{\bfD}{\mathbf{D}}
\newcommand{\bfE}{\mathbf{E}}
\newcommand{\bfF}{\mathbf{F}}
\newcommand{\bfG}{\mathbf{G}}
\newcommand{\bfH}{\mathbf{H}}
\newcommand{\bfI}{\mathbf{I}}
\newcommand{\bfJ}{\mathbf{J}}
\newcommand{\bfK}{\mathbf{K}}
\newcommand{\bfL}{\mathbf{L}}
\newcommand{\bfM}{\mathbf{M}}
\newcommand{\bfN}{\mathbf{N}}
\newcommand{\bfO}{\mathbf{O}}
\newcommand{\bfP}{\mathbf{P}}
\newcommand{\bfQ}{\mathbf{Q}}
\newcommand{\bfR}{\mathbf{R}}
\newcommand{\bfS}{\mathbf{S}}
\newcommand{\bfT}{\mathbf{T}}
\newcommand{\bfU}{\mathbf{U}}
\newcommand{\bfV}{\mathbf{V}}
\newcommand{\bfW}{\mathbf{W}}
\newcommand{\bfX}{\mathbf{X}}
\newcommand{\bfY}{\mathbf{Y}}
\newcommand{\bfZ}{\mathbf{Z}}
\newcommand{\bfSigma}{\mathbf{\Sigma}}

\newcommand{\scrA}{\mathscr{A}}
\newcommand{\scrB}{\mathscr{B}}
\newcommand{\scrC}{\mathscr{C}}
\newcommand{\scrD}{\mathscr{D}}
\newcommand{\scrE}{\mathscr{E}}
\newcommand{\scrF}{\mathscr{F}}
\newcommand{\scrG}{\mathscr{G}}
\newcommand{\scrH}{\mathscr{H}}
\newcommand{\scrI}{\mathscr{I}}
\newcommand{\scrJ}{\mathscr{J}}
\newcommand{\scrK}{\mathscr{K}}
\newcommand{\scrL}{\mathscr{L}}
\newcommand{\scrM}{\mathscr{M}}
\newcommand{\scrN}{\mathscr{N}}
\newcommand{\scrO}{\mathscr{O}}
\newcommand{\scrP}{\mathscr{P}}
\newcommand{\scrQ}{\mathscr{Q}}
\newcommand{\scrR}{\mathscr{R}}
\newcommand{\scrS}{\mathscr{S}}
\newcommand{\scrT}{\mathscr{T}}
\newcommand{\scrU}{\mathscr{U}}
\newcommand{\scrV}{\mathscr{V}}
\newcommand{\scrW}{\mathscr{W}}
\newcommand{\scrX}{\mathscr{X}}
\newcommand{\scrY}{\mathscr{Y}}
\newcommand{\scrZ}{\mathscr{Z}}
 
\usepackage{hyperref}
\usepackage{stfloats}
\usepackage{subfig}
\usepackage{xcolor}
\usepackage{makecell}
\usepackage{xspace}
\usepackage{url}
\usepackage{wrapfig}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{booktabs, colortbl}
\usepackage{longtable}
\usepackage{hhline}
\usepackage{color}
\usepackage{algpseudocode}
\usepackage{listings}
\usepackage[ruled]{algorithm2e}

\definecolor{myblue}{HTML}{0072C6}
\definecolor{myyellow}{HTML}{FFFADF}
\definecolor{myred}{HTML}{FF0000}

\lstset{
    language=Python,
    basicstyle=\ttfamily\footnotesize,
    keywordstyle=\color{myblue}\ttfamily\footnotesize,
    stringstyle=\color{myred}\ttfamily\footnotesize,
    commentstyle=\color{myyellow}\ttfamily\footnotesize,
    morecomment=[l][\color{myyellow}\ttfamily\footnotesize]{\#},
    backgroundcolor=\color{white},
    frame=none,
    rulecolor=\color{gray},
    showstringspaces=false
}

\usepackage[capitalize,noabbrev]{cleveref}
\usepackage{appendix}
\usepackage{cleveref}
\crefname{section}{Section}{Sections}
\crefname{theorem}{Theorem}{Theorems}
\crefname{lemma}{Lemma}{Lemmas}
\crefname{equation}{Equation}{Equations}
\crefname{proposition}{Proposition}{Propositions}
\crefname{claim}{Claim}{Claims}
\crefname{appendix}{Appendix}{Appendices}
\crefname{algorithm}{Algorithm}{Algorithms}
\crefname{figure}{Figure}{Figs}
\crefname{table}{Table}{Tables}
\crefname{remark}{Remark}{Remarks}
\crefname{definition}{Definition}{Definitions}
\crefname{equation}{Equation}{Equations}
\crefname{corollary}{Corollary}{Corollaries}
\crefalias{section}{appendix}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\definecolor{lightgreen}{HTML}{30E1C8}
\definecolor{lightblue}{HTML}{0254D6}
\definecolor{cite_color}{HTML}{114083}
\definecolor{link_color}{RGB}{153, 0,0}  
\definecolor{url_color}{RGB}{153, 102,  0}
\definecolor{emp_color}{RGB}{0,0,255}

\usepackage{xspace}
\newcommand{\ours}[0]{DDP\xspace}
\newcommand{\yf}[1]{\textcolor{blue}{yf: #1}}
\newcommand{\enze}[1]{\textcolor{red}{[enze: #1]}}
\newcommand{\todo}[1]{\textcolor{red}{[todo: #1]}}
\newcommand{\xihui}[1]{\textcolor{purple}{Xihui: #1}}
\newcommand{\zhaoqiang}[1]{\textcolor{cyan}{Zhaoqiang: #1}}
\newcommand{\tablestyle}[2]{\setlength{\tabcolsep}
{#1}\renewcommand{\arraystretch}{#2}\centering\small}
\newlength\savewidth\newcommand\shline{\noalign{\global\savewidth\arrayrulewidth\global\arrayrulewidth 1pt}\hline\noalign{\global\arrayrulewidth\savewidth}}
\begin{document}
\definecolor{baselinecolor}{gray}{.9}
\newcommand{\baseline}[1]{\cellcolor{baselinecolor}{#1}}
\newcommand{\gray}[1]{\textcolor{gray}{#1}}
\definecolor{mygray}{gray}{.8}

\makeatletter
\renewcommand{\paragraph}{\@startsection{paragraph}{4}{\z@}{0.2\baselineskip}{-1em}{\normalfont\normalsize\bfseries}}
\makeatother

\newcommand{\algcomment}[1]{\vspace{-\baselineskip}\noindent {\footnotesize #1\par}\vspace{\baselineskip}}
\renewcommand{\thefootnote}{}

    
\title{DDP: Diffusion Model for Dense Visual Prediction}

\author{
    Yuanfeng Ji, 
    Zhe Chen,
    Enze Xie,
    Lanqing Hong, 
    Xihui Liu, \\
    Zhaoqiang Liu,
    Tong Lu,
    Zhenguo Li,
    Ping Luo\\
    The University of Hong Kong~~~
    Huawei Noah’s Ark Lab~~~
    Nanjing University~~~ \\
    {\small \url{https://github.com/JiYuanFeng/DDP}}
}

\maketitle

\ificcvfinal\thispagestyle{empty}\fi

\footnotetext{ Equal contribution.}
\footnotetext{ Corresponding author.}

\begin{abstract}
We propose a simple, efficient, yet powerful framework for dense visual predictions based on the conditional diffusion pipeline.
Our approach follows a ``noise-to-map" generative paradigm for prediction by progressively removing noise from a random Gaussian distribution, guided by the image.
The method, called DDP, efficiently extends the denoising diffusion process into the modern perception pipeline.
Without task-specific design and architecture customization, \ours is easy to generalize to most dense prediction tasks, e.g., semantic segmentation and depth estimation.
In addition, \ours shows attractive properties such as dynamic inference and uncertainty awareness, in contrast to previous single-step discriminative methods.
We show top results on three representative tasks with six diverse benchmarks, without tricks, \ours achieves state-of-the-art or competitive performance on each task compared to the specialist counterparts.
For example, semantic segmentation (83.9 mIoU on Cityscapes), BEV map segmentation (70.6 mIoU on nuScenes), and depth estimation (0.05 REL on KITTI).
We hope that our approach will serve as a solid baseline and facilitate future research.
\end{abstract} \section{Introduction}
Dense prediction tasks are the foundation of computer vision research, including a wide range of perceptual tasks such as semantic segmentation \cite{Cordts_2016_CVPR,zhou2017scene}, depth estimation \cite{geiger2013vision,silberman2012indoor,song2015sun}, and optical flow \cite{fischer2015flownet,geiger2013vision}.
These tasks require correctly predicting the discrete labels or continuous values for all pixels in the image, which provides detailed contextual understanding and enables various applications.

Numerous methods have rapidly improved the result of perception tasks over a short period of time.
In general terms, these methods can be divided into two paradigms: \textit{discriminative-based} 
 \cite{fu2018deep,zhao2017pyramid,xie2021segformer,cheng2022masked} and \textit{generative-based} 
\cite{xie2017adversarial,hendrik2017universal,isola2017image,li2021semantic,yeh2017semantic}. 
The former approach, which directly learns the mapping between input-output pairs and predicts in a single forward step, has become the current de-facto choice due to its simplicity and efficiency.
Whereas, generative models aim at modeling the underlying distribution of the data, conceptually having a greater capacity to handle challenging tasks.
However, they are often restricted by complex architecture customization as well as various training difficulties \cite{salimans2016improved,karras2018progressive,brock2019large}.
\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{src/figures/ddp_task_illustration.pdf}
    \caption{\textbf{Conditional diffusion pipeline for dense visual predictions}. Specifically, a conditional diffusion model is employed, where  is the forward diffusion process and  is the inverse process. The framework iteratively transforms the noise sample , drawn from a standard Gaussian distribution, into the desired target prediction  under the guidance of the input image .}
    \label{fig_intro_task}
\end{figure}


These challenges have been largely addressed by the diffusion and score-based models~\cite{ho2020denoising,sohl2015deep,song2020improved}.
The solutions, based on \textit{denosing diffusion process}, are conceptually simple:
they apply a continuous diffusion process to transform data into noise and generate new samples by simulating the time-reversed diffusion process.
These methods now enable easy training and achieve superior results on various generative tasks~\cite{nichol2021glide,saharia2022photorealistic,rombach2022high,ramesh2022hierarchical}.
Witnessing these great successes, there has been a recent surge of interest to introduce diffusion models to dense prediction tasks, including semantic segmentation \cite{amit2021segdiff,chen2022generalist,wu2022medsegdiff,wolleb2022diffusion} and depth estimation \cite{saxena2023depthgen}.
However, these methods simply transfer the heavy frameworks from image generation tasks to dense prediction, resulting in low efficiency, slow convergence, and sub-optimal performance.

In this paper, we introduce a general, simple, yet effective diffusion framework for dense visual prediction.
Our method named as DDP, which extends the denoising diffusion process into
the modern perception pipeline effectively (see \cref{fig_intro_overview}).
During training, the Gaussian noise controlled by a noise schedule \cite{nichol2021improved} is added to the encoded ground truth to obtain the noisy maps.
Then these noisy maps are fused with the conditional features from the image encoder, \eg, Swin Transformer \cite{liu2021swin}.
Finally, these fused features are fed to a lightweight map decoder to produce the predictions without noise.
At the inference phase, \ours generates predictions by reversing the learned diffusion process, which adjusts a noisy Gaussian distribution to the learned map distribution under the guidance of the test images (see \cref{fig_intro_task}).


\begin{figure*}[t!]
    \centering
    \includegraphics[width=\linewidth]{src/figures/ddp_overview.pdf}
    \caption{\textbf{The proposed \ours framework}. The image encoder extracts feature representation from the input image  as the condition. The map decoder takes the noisy map  as input and produces the denoised prediction under the guidance. During training, the noisy map  is constructed by adding Gaussian noise to the encoded ground truth. In inference, the noisy map  is randomly sampled from the Gaussian distribution and iteratively refined to obtain the desired prediction .
     }
    \label{fig_intro_overview}
\end{figure*}

Compared to previous cumbersome diffusion perception models \cite{wu2022medsegdiff,wolleb2022diffusion,saxena2023depthgen}, \ours decouples the image encoder and map decoder. The image encoder runs only once, while the diffusion process is performed only in the lightweight decoder head.
With this efficient design, our proposed method can easily be applied to modern perception tasks.
Furthermore, unlike previous single-step discriminative models, \ours is capable of performing iterative inference multiple times using the shared parameters and exhibits the following appealing properties: (1) dynamic inference to trade off computation and prediction quality and (2) natural awareness of the prediction uncertainty.

We evaluate \ours on three representative dense prediction tasks, including semantic segmentation, BEV map segmentation, and depth estimation, using six popular datasets (ADE20K \cite{zhou2017scene}, Cityscapes \cite{Cordts_2016_CVPR}, nuScenes~\cite{caesar2020nuscenes}, KITTI \cite{geiger2013vision}, NYU-DepthV2 \cite{silberman2012indoor}, and SUN RGB-D \cite{song2015sun}).
Our experimental results demonstrate that DDP significantly outperforms existing state-of-the-art methods. Specifically, on ADE20K, DDP achieves 46.1 mIoU with a single sampling step, which is significantly better than UperNet \cite{xiao2018unified} and K-Net \cite{zhang2021k}. On nuScenes, DDP yields an mIoU of 70.3, which is clearly better than the BEVFusion~\cite{liu2022bevfusion} baseline that achieves an mIoU of 62.7.
Furthermore, by increasing the sampling steps, DDP can achieve even higher performance on both ADE20K and nuScenes, reaching anmIoU of 47.0 and 70.6, respectively. 
Moreover, the gains are more versatile for different model architectures as well as model sizes. \ours achieves 83.9 mIoU on Cityscapes with the ConvNeXt-L backbone and produces a leading REL of 0.05 on KITTI with the Swin-L backbone.

Overall, our contributions in this work are three-fold. 
\begin{itemize}
  \item We formulate the dense visual prediction tasks as a general conditional denoising process, with simple yet highly effective designs.
  \item Our ``noise-to-map" generative paradigm offers several appealing properties, such as the ability to perform dynamic inference and uncertain awareness.
  \item We conduct extensive experiments on three representative tasks with six diverse benchmarks. The results demonstrate that our method, which we refer to as \ours, achieves competitive performance when compared to previous discriminative methods.
\end{itemize}
 \section{Related Work}

\paragraph{Diffusion Model.}
Diffusion~\cite{ho2020denoising, sohl2015deep} and score-based generative models~\cite{songdenoising} have been particularly successful as generative models and achieve impressive results across various modalities, including images~\cite{ramesh2022hierarchical,saharia2022image,dhariwal2021diffusion,nichol2021glide,daras2022multiresolution,daras2022multiresolution}, video~\cite{ho2022video,hong2022cogvideo}, audio~\cite{kolesnikov2020big}, and biomedical~\cite{anand2022protein, trippe2022diffusion,schneuing2022structure,corso2022diffdock}.
Given the notable achievements of diffusion models in these respective domains, leveraging such models to develop generation-based perceptual models would prove to be a highly promising avenue to push the boundaries of perceptual tasks to newer heights.

\paragraph{Dense Prediction.}
The perception of real-world scenes via pixel-by-pixel classification or regression is commonly formulated as dense prediction tasks, such as semantic segmentation \cite{Cordts_2016_CVPR,zhou2017scene}, depth estimation \cite{geiger2013vision,silberman2012indoor,song2015sun}, and optical flow \cite{fischer2015flownet,geiger2013vision}.
Numerous methods have emerged and achieved tremendous progress, and these advances can be roughly divided to:
multi-scale feature aggregation \cite{chen2017deeplab, chen2018encoder,xiao2018unified}, high-capacity backbone \cite{xie2021segformer,zheng2021rethinking,ranftl2021vision} and powerful decoder head \cite{strudel2021segmenter,zhang2021k,cheng2021per,jain2022oneformer}.
In this paper, as shown in \cref{fig_intro_task}, which differs from previous discriminative-based methods, we explore a generative ``noise-to-map" paradigm for general dense prediction tasks.

\paragraph{Diffusion Models for Dense Prediction.}
With the recent success of diffusion models in generation tasks, there has been a noticeable rise in interest to incorporate them into dense visual prediction tasks.
Several pioneering works~\cite{wu2022medsegdiff,amit2021segdiff,wolleb2022diffusion,chen2022generalist,saxena2023depthgen,chen2022diffusiondet} attempted to apply the diffusion model to visual perception tasks, \eg image segmentation or depth estimation task. For example, Wolleb \etal \cite{wolleb2022diffusion} explore the diffusion model for medical image segmentation. Pix2Seq-D~\cite{chen2022generalist} applies the bit diffusion model~\cite{chen2023analog} for panoptic segmentation. Our concurrent work DepthGen~\cite{saxena2023depthgen} involves diffusion pipeline to the task of depth estimation.
For all the diffusion models listed above, one or two parameter-heavy convolutional U-Nets~\cite{ronneberger2015u} are adopted, leading to low efficiency, slow convergence, and sub-optimal performance.
In this work, as illustrated in \cref{fig_intro_overview}, we introduce a simple yet effective diffusion framework, which extends the denoising diffusion process into the modern perception pipeline while maintaining accuracy and efficiency.

 \section{Methodology}
\subsection{Preliminaries}

\paragraph{Dense Prediction.} The objective of dense prediction tasks is to predict discrete labels or continuous values, denoted as , for every pixel present in the input image .


\paragraph{Conditional Diffusion Model.}
The conditional diffusion model, which is an extension of the diffusion model \cite{ho2020denoising,sohl2015deep,song2020improved}, belongs to the category of likelihood-based models inspired by non-equilibrium thermodynamics.
The conditional diffusion model assumes a forward noising process by gradually adding noise to the data sample, which is defined as:

which transforms the data sample  to a latent noisy sample  for . The constants   and  represents the noise schedule~\cite{nichol2021improved,ho2020denoising}.
During training, the reverse process model  is trained to predict  from  under the guidance of condition  by minimizing the training objective function (\ie,  loss).
At the inference stage, predicted data sample  is reconstructed from a random noise  with the model , conditional input , and a translation rule~\cite{ho2020denoising,song2020denoising} in a markovian way, \ie, , which can be formulated as:

In this paper, our goal is to solve dense prediction tasks via the conditional diffusion model.
In our setting, the data samples are the ground truth map , and a neural network  is trained to predict  from random noise  conditioned on the corresponding image .

\subsection{Architecture}
Since the diffusion model generates samples progressively, it requires multiple runs of the model in the inference stage.
Previous methods~\cite{wu2022medsegdiff,saxena2023depthgen, wolleb2022diffusion} apply the model  in multiple steps on the raw image , which significantly increases the computational overhead.
To alleviate this issue, we separate the entire model into two parts: image encoder and map decoder, as shown in \cref{fig_intro_overview}.
The image encoder forwards only once to extract the feature map from the input image . Then the map decoder employs it as the condition rather than the raw image , to gradually refine the prediction from the noisy map .
\paragraph{Image Encoder.}
The image encoder receives the raw image  as input and generates multi-scale features at 4 different resolutions.
Subsequently, these multi-scale features are fused using the FPN \cite{lin2017feature} and aggregated by a 11 convolution.
The produced feature map, with the resolution of , is employed as the condition for the map decoder.
In contrast to the previous methods~\cite{amit2021segdiff, wu2022medsegdiff, saxena2023depthgen}, \ours is able to work with modern network architectures such as ConvNext \cite{liu2022convnet} and Swin Transformer \cite{liu2021swin}.


\paragraph{Map Decoder.}
The map decoder  takes as input the noisy map  and the feature map from the image encoder via concatenation and performs a pixel-by-pixel classification or regression.
Following the common practice \cite{cheng2022masked,zhu2020deformable,zhang2022dino} in modern perception pipelines, we simply stack six layers of deformable attention as the map decoder.
Compared to previous works \cite{amit2021segdiff, wu2022medsegdiff, saxena2023depthgen, chen2022generalist, wolleb2022diffusion} that use the parameter-intensive U-Nets, our map decoder is lightweight and compact, allowing efficient reuse of the shared parameters during the multi-step reverse diffusion process.


\begin{algorithm}[t!]
\caption{\ours Training}
\label{algo:ddp:training}
\definecolor{codeblue}{HTML}{2E8B57} 
\definecolor{codekw}{HTML}{DC143C} 
\lstset{
  backgroundcolor=\color{white},
  columns=fullflexible,
  breaklines=true,
  captionpos=b,
  commentstyle=\fontsize{7.2pt}{7.2pt}\color{codeblue},
  keywordstyle=\fontsize{7.2pt}{7.2pt}\color{codekw},
  escapechar={|}, 
}
\lstset{language=Python}
\begin{lstlisting}[xleftmargin=-1em]
def train(images, maps):
  """images: [b, 3, h, w], maps: [b, 1, h, w]"""
  img_enc = image_encoder(images) # encode image
  map_enc = encoding(maps) # encode gt
  map_enc = (sigmoid(map_enc) * 2 - 1) * scale
  # corrupt gt
  t, eps = uniform(0, 1), normal(mean=0, std=1)
  map_crpt = sqrt(alpha_cumprod(t)) * map_enc +
              sqrt(1 - alpha_cumprod(t)) * eps
  # predict and backward
  map_pred = map_decoder(map_crpt, img_enc, t)
  loss = objective_func(map_pred, maps)
  return loss
\end{lstlisting}
\end{algorithm}



\subsection{Training}
During training, we first construct a diffusion process from the ground truth  to the noisy map  and then train the model to reverse this process.
The training procedure for \ours is provided in \cref{algo:ddp:training} (for more details please refer to \cref{supp:diffusion}).



\paragraph{Label Encoding.}
\label{para:label_encoding}
Standard diffusion models assume continuous data, which makes them a convenient choice for regression tasks with continuous values (\eg, depth estimation). However, existing studies~\cite{chen2022generalist,chen2023analog} show that they are unsuitable for discrete labels (\eg, semantic segmentation).
Therefore, we explore several encoding strategies for the discrete labels, including:
(1) One-hot encoding, which represents categorical labels as binary vectors of 0 and 1;
(2) Analog bits encoding~\cite{chen2022generalist}, which first converts discrete integers into bit strings, and then casts them as real numbers;
(3) Class embedding, which uses a learnable embedding layer to project discrete labels into a high-dimensional continuous space, with a sigmoid function for normalization.
For all of these strategies, we normalize and scale the range of encoded labels within , as shown in \cref{algo:ddp:training}.
Notably, the scaling factor  controls the signal-to-noise ratio (SNR) \cite{chen2022generalist,chen2023importance}, which is an important hyper-parameter for diffusion models.
We compare these strategies in \cref{tab:label_encoding} and find class embedding work best.
More discussions are in \cref{sec:exp:abla}.


\paragraph{Map Corruption.}
We add Gaussian noise to corrupt the encoded ground truth, obtaining the noisy map .
As shown in \cref{eq:forward}, the intensity of corruption noise is controlled by , which adopts the monotonically decreasing schedule for  in different time steps .
Different noise scheduling strategies, including cosine schedule \cite{nichol2021improved} and linear schedule \cite{ho2020denoising},
are compared and discussed in \cref{sec:exp:abla}.
We found that the cosine schedule usually worked best in our benchmark tasks.


\paragraph{Objective Function.}
Standard diffusion models are trained with  loss, which is reasonable for dense prediction tasks, but we found that adopting a task-specific loss works better for supervision, \eg, cross-entropy loss for semantic segmentation, sigloss for depth estimation.

\begin{algorithm}[t!]
\caption{\ours Sampling}
\label{algo:ddp:sampling}
\definecolor{codeblue}{HTML}{2E8B57}
\definecolor{codekw}{HTML}{DC143C}
\lstset{
  backgroundcolor=\color{white},
  columns=fullflexible,
  breaklines=true,
  captionpos=b,
  commentstyle=\fontsize{7.5pt}{7.5pt}\color{codeblue},
  keywordstyle=\fontsize{7.5pt}{7.5pt}\color{codekw},
  escapechar={|}, 
}
\lstset{language=Python}
\begin{lstlisting}[xleftmargin=-1em]
def sample(images, steps, td=1):
  """steps: sample steps, td: time difference"""
  img_enc = image_encoder(images)
  map_t = normal(0, 1) # [b, 256, h/4, w/4]
  for step in range(steps):
    # time intervals
    t_now = 1 - step / steps
    t_next = max(1 - (step + 1 + td) / steps, 0)
    # predict map_0 from map_t
    map_pred = map_decoder(map_t, img_enc, t_now)
    # estimate map_t at t_next
    map_t = ddim(map_t, map_pred, t_now, t_next)
  return map_pred
\end{lstlisting}
\end{algorithm}



\subsection{Inference}
\label{sec:inference}
Given a test image as condition input, the model starts with a random noise map sampled from a Gaussian distribution and gradually refines the prediction, we summarize the inference procedure in \cref{algo:ddp:sampling}.


\paragraph{Sampling Rule.}
We choose the DDIM update rule~\cite{song2020denoising} for the sampling.
In each sampling step , the random noise  or the predicted noisy map  from the last step is fused with the conditional feature map, and sent to the map decoder  for map prediction.
After getting the predicted result of the current step, we compute the noisy map  for the next step using the reparameterization trick.
Following~\cite{chen2022analog,chen2022generalist,chen2022diffusiondet}, we use the asymmetric time intervals (controlled by a hyper-parameter ) during the inference stage, and  works best in our method.


\paragraph{Sampling Drift.}
As displayed in \cref{fig:multiple_inference}, we empirically observe that the model performance improves in a few sampling steps and then declines slightly as the number of steps increases.
Similar observations can also be found in \cite{chen2022diffusiondet,chen2022sampling,saxena2023depthgen}.
This performance decline can be attributed to the ``sampling drift'' challenge, which refers to the discrepancy between the distribution of training and sampling data.
During training, the model is trained to inverse the noisy ground truth map, while during testing, the model is inferred to remove noise from its ``imperfect" prediction, which drifts away from the underlying corrupted distributions.
This drift becomes pronounced with smaller time steps ,  owing to the compounded errors, and is further intensified when a sample deviates more substantially from the distribution of ground truth~\cite{daras2023consistent}.


To verify our hypothesis, in the last 5k iterations of training, we construct  using the model's prediction rather than the ground truth.
The approach transforms the training target to remove the added noise on its own predictions, thereby aligning the data distribution of training and testing. 
We name this approach ``\emph{self-aligned denoising}."
As revealed in \cref{fig:multiple_inference}, this approach tends to produce saturation instead of performance degradation. 
Our findings suggest that incorporating the diffusion process into perception tasks could enhance efficacy compared to image generation (\eg, about 50 DDIM steps for image generation).
In other words, the proposed DDP can improve efficiency (\eg, satisfied results in 3 iterative steps) while retaining the benefits of the diffusion model.
More discussions can be found in \cref{supp:diffusion}.



\paragraph{Multiple Inference.}
By virtue of the multi-step sampling procedure, our method supports dynamic inference, which has the flexibility to trade compute for prediction quality.
Besides, it naturally enables the assessment of the reliability and uncertainty of model predictions.
 \section{Experiment}
We first present the appealing properties of our \ours, followed by empirical evaluations of its performance against leading methods on several representative  tasks, including semantic segmentation, BEV map segmentation, and monocular depth estimation. 
Finally, we provide ablation studies on the \ours components. 
Due to space limitations, 
more implementation details and experimental results are provided in \cref{supp:implement} and \cref{supp:experiment}, respectively.


\subsection{Main Properties}
We explore and show properties of \ours in \cref{fig:properties} using the default setting in \cref{sec:semantic_settings}.
With such a multi-step sampling procedure, we have the flexibility to trade computational cost for prediction quality.
Furthermore, the stochastic sampling process allows the computing of pixel-wise uncertainty maps of the prediction.

\begin{figure}
  \centering
  \subfloat[\textbf{Dynamic inference}.~The results of multiple inference on Cityscapes.]{\includegraphics[width=0.95\linewidth]{src/figures/ddp_multiple.pdf}
  \label{fig:multiple_inference}}
  \vspace{-0.5em}
  \\
  \subfloat[\textbf{Inference trajectory}.~Predicted mask results on different time steps.]{\includegraphics[width=0.95\linewidth]{src/figures/refine.pdf}
  \label{fig:refine}}
  \vspace{-0.5em}
  \\
  \subfloat[\textbf{Uncertainty awareness}. High response areas in the uncertainty map indicate high estimated uncertainty and are highly positively correlated with white areas in the error map, which indicate misclassified points. Zoom in for better visualization.]{\includegraphics[width=0.95\linewidth]{src/figures/ddp_uncertainty_aware.pdf}
  \label{fig:uncertainty}}
  \vspace{1em}
  \caption{\textbf{DDP enjoys two appealing properties:} dynamic inference to trading off computation and prediction quality and the natural awareness of the prediction uncertainty.}
  \label{fig:properties}
\end{figure}

\paragraph{Dynamic Inference.}
We evaluate \ours with ConvNext-T and ConvNext-L backbones by increasing their sampling steps from 1 to 10.
The results are presented in \cref{fig:multiple_inference}.
It can be seen that the DDP can continuously improve its performance by using more sampling steps.
For example, DDP with ConvNext-T shows an increase from 82.33 mIoU (1 step) to 82.60 mIoU (3 steps), and we visualize the inference trajectory in \cref{fig:refine}.
In comparison to the previous single-step method, our approach boasts the flexibility to balance computational cost against accuracy. This means our method can be adapted to different trade-offs between speed and accuracy under various scenarios without the need to retrain the network.

\paragraph{Uncertainty Awareness.}
In addition to the performance gains, the proposed \ours can naturally provide uncertainty estimates.
In the multi-step sampling process, we can simply count the pixels where the predicted result of each step differs from the result of the previous step, and finally, we simply normalize this change count map to 0-1 and obtain an uncertainty map.
In comparison, DDP is naturally and easily capable of estimating uncertainty, whereas previous methods \cite{loquercio2020general,harakeh2020bayesod} require complicated modeling such as Bayesian networks.


\subsection{Semantic Segmentation}

\paragraph{Datasets.}
We evaluate the proposed \ours using two widely used datasets: ADE20K \cite{zhou2017scene} and Cityscapes \cite{Cordts_2016_CVPR}.
ADE20K is a large-scale scene parsing dataset with over 20,000 images, and Cityscapes is a street scene dataset with high-quality pixel-level annotations for 5,000 images.

\paragraph{Settings.}
\label{sec:semantic_settings}
In the training phase, following common practices \cite{wang2021pyramid,chen2022vitadapter,xie2021segformer,wang2022internimage}, the crop size is set to 512512 for ADE20K, and 5121024 for Cityscapes.
We optimize our \ours models using the AdamW~\cite{loshchilov2017decoupled} optimizer, with an initial learning rate of  and a weight decay of 0.01.
All models are trained for 160k iterations and compared fairly with previous non-diffusion methods.

\begin{table}[t]
\centering
\footnotesize
\renewcommand\arraystretch{1.05}
\setlength{\tabcolsep}{1.35mm}
\begin{tabular}{l|l|c|c|c|c}
    Method & Backbone  & \#Param & FLOPs  & mIoU & +MS \\
    \hline
    UperNet~\cite{xiao2018unified} & Swin-T & 60M & 236G & 44.5 & 45.8\\
    Region Rebalance~\cite{cui2022region} & Swin-T & 60M & 236G & 45.0 & 46.5\\
    MaskFormer~\cite{cheng2021per}  & Swin-T & 42M & 55G & 46.7 & 48.8 \\
    Mask2Former~\cite{cheng2022masked} & Swin-T & 47M & 74G & 47.7 & 49.6 \\    
    K-Net~\cite{zhang2021k} & Swin-T & 73M & 256G & 45.8 & 46.3\\
    SenFormer~\cite{bousselham2021efficient} & Swin-T & 144M & 179G & 46.0 & 46.4\\
    \rowcolor{gray!10} 
    Non-diffusion Baseline & Swin-T & 35M & 111G & 44.9 & 46.1 \\  
    \rowcolor{gray!10} 
    \ours (step 1) & Swin-T & 40M & 113G & 46.1 & 47.6 \\
    \rowcolor{gray!10} 
    \ours (step 3) & Swin-T & 40M & 252G & 47.0 & 47.8\\
    \hline
    UperNet~\cite{xiao2018unified} & Swin-S & 81M & 259G & 47.6 & 49.5\\ 
    \rowcolor{gray!10} 
    \ours (step 1) & Swin-S & 61M & 136G & 48.4 & 49.7 \\
    \rowcolor{gray!10} 
    \ours (step 3) & Swin-S & 61M & 276G & 48.7 & 49.7 \\
    \hline
    UperNet~\cite{xiao2018unified} & Swin-B & 121M & 297G & 48.1 & 49.7\\
    \rowcolor{gray!10} 
    \ours (step 1) & Swin-B & 99M & 173G & 49.2 & 50.8\\
    \rowcolor{gray!10} 
    \ours (step 3) & Swin-B & 99M & 312G & 49.4 & 50.8 \\
    \hline
    UperNet~\cite{xiao2018unified} & Swin-L & 234M & 411G & 52.1 & 53.5\\
    \rowcolor{gray!10} 
    \ours (step 1) & Swin-L & 207M & 285G & 53.1 & 54.4 \\
    \rowcolor{gray!10} 
    \ours (step 3) & Swin-L & 207M & 425G & 53.2 & 54.4 \\
    \end{tabular}
    \vspace{1em}
    \caption{\textbf{Semantic segmentation on ADE20K val set.}
    We report single-scale (SS) and multi-scale (MS) mIoU.
    The FLOPs are measured with 512512 inputs.
    Backbones pre-trained on ImageNet-22K are marked with .
    }
    \label{tab:result_ade20k}
\end{table}



\paragraph{Results on ADE20K.}
Table \ref{tab:result_ade20k} presents the semantic segmentation performance of \ours on  ADE20K \cite{zhou2017scene}, which shows that our method consistently outperforms many representative methods \cite{xiao2018unified,cui2022region,zhang2021k,bousselham2021efficient} and the non-diffusion baseline across different backbones.
For instance, when using Swin-T \cite{liu2021swin} as the backbone, our \ours (step 1) yields a promising result of 46.1 mIoU, surpassing the non-diffusion baseline (\ours \emph{w/o} diffusion process)  by 1.2 points (46.1 \emph{vs.}~44.9).
Moreover, our \ours (step 3) can further enhance the performance to 47.0 mIoU, attaining a remarkable gain of 0.9 points by multi-steps of denoising diffusion.
With the Swin-L backbone, our \ours (step 3) achieves the best performance of 53.2 mIoU, which is 1.1 points (53.2 \emph{vs.}~52.1) better than UperNet with comparable FLOPs.
These results suggest that our \ours not only achieves a performance gain but also offers more flexibility than previous methods.


\begin{table}[t!]
\centering
\footnotesize
\renewcommand\arraystretch{1.05}
\setlength{\tabcolsep}{1.25mm}
    \begin{tabular}{l|l|c|c|c|c}
Method & Backbone & \#Param  & FLOPs & mIoU & +MS \\
        \hline
        Segmenter~\cite{strudel2021segmenter} & ViT-L & 333M & 2685G & 79.10 & 81.30\\
        SETR-PUP~\cite{zheng2021rethinking} & ViT-L  & 318M & 2955G &  79.34 & 82.15\\
        StructToken~\cite{lin2022structtoken} & ViT-L  & 364M  & 2913G & 80.05 & 82.07 \\
        OCRNet~\cite{yuan2020object,yuan2021hrformer} & HRFormer-B & 56M & 2240G & 81.90 & 82.60 \\
        SegFormer-B5~\cite{xie2021segformer} & MiT-B5 & 85M  & 1448G & 82.25 & 83.48  \\
        DiversePatch~\cite{gong2021vision} & Swin-L & 234M & 3190G  & 82.70 & 83.60 \\ 
        Mask2Former~\cite{cheng2022masked} & Swin-L & 216M   & 2113G & 83.30 & 84.30 \\
        
        \hline
        \ours (step 1) & Swin-T & 39M &885G  & 80.96 & 82.25 \\
        \ours (step 3) & Swin-T & 39M &1992G & 81.24 & 82.46 \\
        \ours (step 1) & Swin-S & 61M &1067G & 82.17 & 83.06 \\
        \ours (step 3) & Swin-S & 61M &2174G & 82.41 & 83.21 \\
        \rowcolor{gray!10} 
        \ours (step 1) & Swin-B & 99M &1357G & 82.37 & 83.36 \\
        \rowcolor{gray!10} 
        \ours (step 3) & Swin-B & 99M &2464G & 82.54 & 83.42    \\   
        \hline
        \ours (step 1) & ConvNext-T  & 40M  & 883G  &82.33 & 83.00 \\
        \ours (step 3) & ConvNext-T  & 40M  & 1989G &82.60 & 83.15 \\
        \ours (step 1) & ConvNext-S  & 62M  & 1059G &82.37 & 83.38 \\
        \ours (step 3) & ConvNext-S  & 62M  & 2166G &82.69 & 83.58 \\
        \ours (step 1) & ConvNext-B  & 100M & 1340G &82.59 & 83.47 \\
        \ours (step 3) & ConvNext-B  & 100M & 2447G &82.78 & 83.49 \\
        \rowcolor{gray!10} 
        \ours (step 1) & ConvNext-L & 209M & 2139G &82.95 & 83.76 \\
        \rowcolor{gray!10} 
        \ours (step 3) & ConvNext-L & 209M & 3245G &
        83.21 & 83.92\\
    \end{tabular}
    \vspace{1em}
\caption{\textbf{Semantic segmentation on Cityscapes val set.}
We report single-scale (SS) and multi-scale (MS) mIoU.
The FLOPs are measured with 10242048 inputs.
Backbones pre-trained on ImageNet-22K are marked with .
}
\label{tab:result_cityscapes}
\end{table}

\paragraph{Results on Cityscapes.}
We compare our \ours with various representative models on Cityscapes~\cite{Cordts_2016_CVPR} in Table \ref{tab:result_cityscapes}, such as Segmenter \cite{strudel2021segmenter}, SETR \cite{zheng2021rethinking}, SegFormer \cite{xie2021segformer}, DiversePatch \cite{gong2021vision}, and Mask2Former \cite{cheng2022masked}, and so on.
As shown, we conduct extensive experiments based on ConvNeXt \cite{liu2022convnet} and Swin \cite{liu2021swin} with different model sizes. 
When using ConvNeXt-L as the backbone, our \ours (step 1) produces a competitive result of 82.95 mIoU, and it can be further boosted to 83.21 mIoU (step 3). 
This phenomenon was also observed when taking Swin-T as the backbone, and the mIoU increased from 80.96 to 81.24 through additional 2 sampling steps.
These experimental results demonstrate the scalability of our methodology, which can be applied to different model structures of arbitrary size.
Moreover, once again, the experimental results show that \ours achieves progressive improvements through multi-step denoising diffusion while keeping comparable computational overhead.

\paragraph{Discussion.}  
The original intention of DDP is to design a diffusion-based general framework for various dense prediction tasks. 
Although its segmentation performance is slightly lower than its specialized counterpart Mask2Former \cite{cheng2022masked}, 
it remains highly competitive and has several attractive features.
How to design a segmentation-specific diffusion framework to achieve better performance than Mask2Former is left for future research.


\subsection{BEV Map Segmentation}

\paragraph{Dataset.} 
We conduct our experiments of BEV map segmentation on the nuScenes~\cite{caesar2020nuscenes} dataset. 
It is a large-scale autonomous driving perception dataset, which includes over 1000 urban road scenes covering different time periods and weather conditions in two cities, Boston and Singapore.

\paragraph{Settings.}
We further verify the \ours framework on the BEV map segmentation task.
Specifically, we equip our method with the representative method BEVFusion \cite{liu2022bevfusion}, where we directly replace its segmentation head with the proposed map decoder for the diffusion process.
We follow evaluation protocol from~\cite{liu2022bevfusion} and compare the results with state-of-the-art methods~\cite{xie2022m2bev, yin2021multimodal, liu2022bevfusion, borse2023x}.
We report the IoU of 6 background classes, including drivable space (Dri), pedestrian crossing (Ped), walk-way (Wal), stop line (Sto), car-parking area (Car), and lane divider (Div), and use the mean IoU as the primary evaluation metric.
Other training settings are kept the same as \cite{liu2022bevfusion} for fair comparisons.

\paragraph{Results.} 
We show the results of our BEV map segmentation experiments in Table \ref{tab:exp:bev:nuscenes}, which exhibit the superior performance of our approach, over existing state-of-the-art methods.
Specifically, in the camera-only scenario, our \ours (step 1) attains a 59.3 mIoU score on the nuScenes validation dataset, which surpasses the previous best method X-Align \cite{borse2023x} by 1.3 mIoU (59.3 \emph{vs.} 58.0). 
By iteratively refining the output of the model, \ours (step 3) sets a new state-of-the-art record of 59.4 mIoU solely based on camera modality.
In the multi-modality setting, we improve the segmentation results of our \ours (step 1) to 70.3 mIoU by combining LiDAR information, significantly higher than the current state-of-the-art methods \cite{liu2022bevfusion, borse2023x} by at least 4.6 mIoU. Remarkably, this performance can be further enhanced to a maximum of 70.6 mIoU by leveraging the benefits of iterative denoising diffusion.
In summary, these results demonstrate that \ours can be easily generalized to other tasks and obtain performance gains, proving the effectiveness and generalization of our approach.


\begin{table}[t]
    \centering
    \footnotesize
    \renewcommand\arraystretch{1.05}
    \setlength{\tabcolsep}{0.88mm}
    \begin{tabular}{l|c|cccccc|c}
Method & Modality & Dri & Ped & Wal & Sto & Car & Div & Mean \\
        \hline 
        OFT~\cite{roddick2018orthographic} & C & 74.0 & 35.3 & 45.9 & 27.5 & 35.9 & 33.9 & 42.1 \\
        LSS~\cite{philion2020lift}  & C & 75.4 & 38.8 & 46.3 & 30.3 & 39.1 & 36.5 & 44.4 \\
        CVT~\cite{zhou2022cross}  & C &  74.3 & 36.8 & 39.9 & 25.8 & 35.0 & 29.4 & 40.2 \\
        MBEV~\cite{xie2022m2bev} & C & 77.2 & - & - & - & - & 40.5 & - \\
        BEVFusion~\cite{liu2022bevfusion} & C & 81.7 & 54.8 & 58.4 & 47.4 & 50.7 & 46.4 & 56.6 \\
        X-Align~\cite{borse2023x} & C & 82.4 & 55.6 & 59.3 & 49.6 & 53.8 & 47.4 & 58.0 \\ 
        \rowcolor{gray!10} 
        \ours (step 1) & C &83.2  &58.5 &61.6 &52.4 &51.1 &48.9 &59.3 \\
        \rowcolor{gray!10} 
        \ours (step 3) & C & 83.6 &58.3 &61.8 &52.3 &51.4 &49.2 &\textbf{59.4} \\
        \hline 
        PointPainting~\cite{vora2020pointpainting}  & C+L & 75.9 & 48.5 & 57.1 & 36.9 & 34.5 & 41.9 & 49.1 \\
        MVP~\cite{yin2021multimodal} & C+L & 76.1 & 48.7 & 57.0 & 36.9 & 33.0 & 42.2 & 49.0 \\
        BEVFusion~\cite{liu2022bevfusion} & C+L & 85.5 & 60.5 & 67.6 & 52.0 & 57.0 & 53.7 & 62.7 \\
        X-Align~\cite{borse2023x} & C+L & 86.8 & 65.2 & 70.0 & 58.3 & 57.1 & 58.2 & 65.7 \\
        \rowcolor{gray!10} 
        \ours (step 1)  & C+L & 89.3   &69.5   &74.8  &62.5 &63.5   & 62.3    &70.3   \\
        \rowcolor{gray!10} 
        \ours (step 3)  & C+L &89.4   &69.8 &75.0   &63.0   & 63.8   &62.6   &\textbf{70.6} \\
    \end{tabular}
    \vspace{1em}
    \caption{\textbf{BEV map segmentation on nuScenes val set.} 
    We report the IoU of 6 background classes and the mean IoU.
    ``C" and ``L" denotes the camera modality and LiDAR modality, respectively.
    }
    \label{tab:exp:bev:nuscenes}
\end{table}





\subsection{Depth Estimation}
\paragraph{Datasets.}
We evaluate the depth estimation performance of \ours on three prominent datasets, namely KITTI \cite{geiger2013vision}, NYU-DepthV2 \cite{silberman2012indoor}, and SUN RGB-D \cite{song2015sun}.
(1) The KITTI dataset encompasses stereo image pairs and corresponding ground truth depth maps for outdoor scenes captured by a car-mounted camera.
Following common practices \cite{eigen2014depth,li2022depthformer}, we use about 26K left-view images for training and 697 images for testing.
(2) The NYU dataset contains RGB-Depth images for indoor scenes captured at a resolution of 640480.
Similar to prior research \cite{li2022depthformer}, the model is trained on 24K train images and evaluated on the reserved 652 images.
(3) The SUN RGB-D dataset is a vast collection of around 10K indoor images. 
We employ it to evaluate the generalization abilities of our NYU pre-trained models.
The results on KITTI are shown in the main paper, while others will be provided in the supplementary material.
 

\paragraph{Settings.}
We incorporate the \ours model into the codebase developed by \cite{li2022depthformer} for depth estimation experiments.
We excluded the discrete label encoding module as the task requires continuous value regression.
All experimental settings are the same as \cite{li2022depthformer} for a fair comparison.

\paragraph{Metrics.}
Typically, the evaluation of depth estimation methods employs the following metrics: accuracy under threshold (), mean absolute relative error (REL), mean squared relative error (SqRel), root mean squared error (RMSE), root mean squared log error (RMSE log), and mean log10 error (log10).



\begin{table*}[ht]
\centering
\footnotesize
\renewcommand\arraystretch{1.05}
\setlength{\tabcolsep}{2.2mm}
\begin{tabular}{l|l|ccc|cccc}
Method & Backbone &  &  &  & REL  & SqRel  & RMSE  & RMSE  \\
\hline
DORN~\cite{fu2018deep}  & ResNet-101 & 0.932 & 0.984 & 0.994 & 0.072 & 0.307 & 2.727 & 0.120 \\
VNL~\cite{yin2019enforcing} & ResNeXt-101 & 0.938 & 0.990 & \underline{0.998} & 0.072 & - & 3.258 & 0.117 \\
BTS~\cite{lee2019big}  & DenseNet-161 & 0.956 & 0.993 & \underline{0.998} & 0.059 & 0.245 & 2.756 & 0.096 \\
TransDepth~\cite{yang2021transformer}  & ResNet-50 + ViT-B & 0.956 & 0.994 & \textbf{0.999} & 0.064 & 0.252 & 2.755 & 0.098 \\
DPT~\cite{ranftl2021vision}  & ResNet-50 + ViT-B & 0.959 & 0.995 & \textbf{0.999} & 0.062 & - & 2.573 & 0.092 \\
AdaBins~\cite{bhat2021adabins}  & EfficientNet-B5 + Mini-ViT & 0.964 & 0.995 & \textbf{0.999} & 0.058 & 0.190 & 2.360 & 0.088 \\
DepthFormer~\cite{li2022depthformer} & ResNet-50 + Swin-T & 0.966 & 0.995 & \textbf{0.999} & 0.056 & 0.177 & 2.252 & 0.086 \\
DepthFormer~\cite{li2022depthformer} & ResNet-50 + Swin-L  & \textbf{0.975} & \textbf{0.997} & \textbf{0.999} & 0.052 & 0.158 & 2.143 & 0.079 \\
BinsFormer~\cite{li2022binsformer} & Swin-L  & \underline{0.974} & \textbf{0.997} & \textbf{0.999} & 0.052 & \underline{0.151} & \underline{2.098} & 0.079 \\
\hline
DepthGen (step 8)*~\cite{saxena2023depthgen} &  Efficient U-Net & 0.953 & 0.991 & \underline{0.998} & 0.064 & 0.356 & 2.985 & 0.100\\
\rowcolor{gray!10} 
\ours (step 3) & Swin-T &0.969 & \underline{0.996} & \textbf{0.999} & 0.054 & 0.168&2.172&0.083 \\ 
\rowcolor{gray!10} 
\ours (step 3) & Swin-S &0.970 & \underline{0.996} & \textbf{0.999} & 0.053 & 0.167 &2.171 & 0.082 \\
\rowcolor{gray!10} 
\ours (step 3) & Swin-B & 0.973 & \textbf{0.997} & \textbf{0.999} & \underline{0.051} & 0.155 & 2.119 & \underline{0.078} \\
\rowcolor{gray!20} 
\ours (step 3) & Swin-L & \textbf{0.975} & \textbf{0.997} & \textbf{0.999} & \textbf{0.050} & \textbf{0.148} & \textbf{2.072} & \textbf{0.076} \\
\end{tabular}
\vspace{1em}
\caption{\textbf{Depth estimation on the KITTI val set.}
Backbones pre-trained on ImageNet-22K are marked with .
We report the performance of \ours with 3 diffusion steps.
The best and second-best results are bolded or underlined, respectively. ↓ means lower is better, and ↑ means higher is better. * denotes best results of our concurrent work~\cite{saxena2023depthgen}.}
\vspace{-1em}
\label{tab:exp:depth:kitti}
\end{table*}

\paragraph{Results.}
Table~\ref{tab:exp:depth:kitti} shows the depth estimation results on the KITTI dataset.
We compare the proposed \ours models with state-of-the-art depth estimators. 
Specifically, we choose DepthFormer \cite{li2022depthformer} and DepthGen \cite{saxena2023depthgen} as our main competitors, in which DepthFormer is a strong counterpart and achieved leading performance, while DepthGen is a concurrent work of ours and is also a diffusion-based depth estimator.
As we can observe, although the performance on this benchmark tends to be saturated,
our \ours models still outperform all the competitors with clear margins in most metrics, such as REL, SqRel, and RMSE.
For instance, equipped with Swin-L, our method achieves a state-of-the-art RMSE log of 0.076 by 3 steps of denoising diffusion. 
Compared with the concurrent diffusion-based model~\cite{saxena2023depthgen}, we find that:
(1) \ours outperforms DepthGen with clear margins, particularly in regards to the RMSE metric (2.072 \emph{vs.} 2.985), which can be contributed by the equipped advanced pipeline design (e.g., Swin Transformer \emph{vs.} U-Net).
(2) \ours is more lightweight and efficient compared to DepthGen, as the denoising diffusion process occurs solely on the decoder head, whereas with DepthGen, the process occurs on the entire model.

\begin{table*}[t]
\centering
\hspace{-0.5em}
\subfloat[
    \textbf{Label encoding}. We find class embedding works best.
    \label{tab:label_encoding}
]{
    \begin{minipage}{0.18\linewidth}{
        \begin{center}
            \tablestyle{1pt}{1.0}
            \begin{tabular}{l|cc}
            \footnotesize{Type} & \footnotesize{mAcc} & \footnotesize{mIoU}  \\
            \hline
            \footnotesize{analog bits} & \footnotesize{57.6} & \footnotesize{46.2}  \\
            \footnotesize{onehot} & \footnotesize{56.8} & \footnotesize{46.2} \\
            \rowcolor{gray!10} 
            \footnotesize{\textbf{embedding}} &  \footnotesize{\textbf{58.4}} & \footnotesize{\textbf{47.0}}  \\
            \multicolumn{3}{c}{~}\\
            \multicolumn{3}{c}{~}\\
            \end{tabular}
        \end{center}}
    \end{minipage}
}
\hspace{1em}
\subfloat[
    \textbf{Scaling factor}. The best scaling factor is 0.01.
    \label{tab:scaling_factor}
]{
    \centering
    \begin{minipage}{0.155\linewidth}{
        \begin{center}
            \tablestyle{2pt}{1.0}
            \begin{tabular}{l|cc}
            \footnotesize{Scale} & \footnotesize{mAcc} & \footnotesize{mIoU} \\
            \hline
            \footnotesize{0.001} & \footnotesize{56.6} & \footnotesize{45.4} \\
            \rowcolor{gray!10} 
            \footnotesize{\textbf{0.01}} & \footnotesize{\textbf{58.4}} & \footnotesize{\textbf{47.0}} \\
            \footnotesize{0.02} & \footnotesize{57.5} & \footnotesize{46.8} \\
            \footnotesize{0.04} & \footnotesize{56.8} & \footnotesize{45.9} \\
            \footnotesize{0.1}  & \footnotesize{55.0} & \footnotesize{44.0} \\
            \end{tabular}
        \end{center}}
    \end{minipage}
}
\hspace{1em}
\subfloat[
    \textbf{Noise schedule}. Cosine works best.
    \label{tab:noise_schedule}
]{
    \begin{minipage}{0.13\linewidth}{
        \begin{center}
            \tablestyle{1pt}{1.0}
            \begin{tabular}{l|cc}
            \footnotesize{Type} & \footnotesize{mAcc} & \footnotesize{mIoU} \\
            \hline
            \rowcolor{gray!10} 
            \footnotesize{\textbf{cosine}} & \footnotesize{\textbf{58.4}} & \footnotesize{\textbf{47.0}} \\
            \footnotesize{linear} & \footnotesize{56.3} & \footnotesize{45.1} \\
            \multicolumn{3}{c}{~}\\
            \multicolumn{3}{c}{~}\\
            \multicolumn{3}{c}{~}\\
            \end{tabular}
        \end{center}}
    \end{minipage}
}
\hspace{1em}
\subfloat[
    \textbf{Decoder depth }. Six blocks work best.
    \label{tab:decoder_depth}
]{
    \begin{minipage}{0.15\linewidth}{
        \begin{center}
            \tablestyle{1pt}{1.0}
            \begin{tabular}{l|ccc}
            \footnotesize{} & \footnotesize{mAcc} & \footnotesize{mIoU}   & \footnotesize{\#Param} \\
            \hline
            \footnotesize{1} & \footnotesize{56.1} & \footnotesize{44.5} & \footnotesize{2.4M}  \\
            \footnotesize{2} & \footnotesize{56.5} & \footnotesize{45.0}& \footnotesize{3.6M}   \\
            \footnotesize{4} & \footnotesize{57.2} & \footnotesize{45.7} & \footnotesize{6.0M}  \\
            \rowcolor{gray!10} 
            \footnotesize{\textbf{6}} & \footnotesize{\textbf{58.4}} & \footnotesize{\textbf{47.0}} & \footnotesize{\textbf{8.4M}}  \\
            \footnotesize{12} & \footnotesize{55.7} & \footnotesize{46.0} & \footnotesize{15.6M}\\
            \end{tabular}
        \end{center}}
    \end{minipage}
}
\hspace{1em}
\subfloat[
    \textbf{Accuracy \emph{vs.}~Efficiency}. \colorbox{myyellow}{Yellow} denotes K-Net \cite{zhang2021k}.
    \label{tab:efficiency}
]{
    \begin{minipage}{0.18\linewidth}{
        \begin{center}
            \tablestyle{1.5pt}{1.0}
            \begin{tabular}{l|ccc}
            \footnotesize{Step} & \footnotesize{mIoU} &\footnotesize{FLOPs} & \footnotesize{FPS}  \\
            \hline
            \footnotesize{1}\cellcolor[HTML]{FFFADF} & \footnotesize{45.8}\cellcolor[HTML]{FFFADF} & \footnotesize{256G}\cellcolor[HTML]{FFFADF} & \footnotesize{18}\cellcolor[HTML]{FFFADF} \\
            \footnotesize{1} & \footnotesize{46.1} &\footnotesize{113G} & \footnotesize{19}  \\
            \footnotesize{2} & \footnotesize{46.8} & \footnotesize{182G} & \footnotesize{15}  \\
            \rowcolor{gray!10} 
            \footnotesize{\textbf{3}} & 
            \footnotesize{\textbf{47.0}} & \footnotesize{\textbf{252G}} & \footnotesize{\textbf{13}} \\
            
            \footnotesize{4} & \footnotesize{46.8} & \footnotesize{322G} &\footnotesize{11}  \\
            \end{tabular}
        \end{center}}
    \end{minipage}
}
\\
\caption{\textbf{\ours ablation experiments} with Swin-T~\cite{liu2021swin} on ADE20K semantic segmentation. We report the performance with 3 sampling steps in (a), (b), (c), and (d).
If not specified, the default settings are: 
the label encoding strategy is class embedding, the scaling factor is set to 0.01, the noise schedule is cosine, and the map decoder has a depth of 6. Default settings are marked in \colorbox{baselinecolor}{gray}.}

\label{tab:ablations}
\end{table*}

\subsection{Ablation Study}
We conduct ablation studies on the ADE20K semantic segmentation.
All models are trained using our \ours with Swin-T~\cite{liu2021swin} backbone for 160k iterations.
Other settings are the same as the settings in \cref{sec:semantic_settings}.
\label{sec:exp:abla}

\paragraph{Label Encoding.}
Since the labels of semantic segmentation are discrete, we need to encode them first. As shown in \cref{tab:label_encoding}, here we study the effect of three different strategies. For each of them, we search the optimal scaling factor. The results show that class embedding is a better strategy to encode semantic labels than one-hot and analog bits \cite{chen2022generalist}.

\paragraph{Signal Scale.}
As shown in \cref{tab:scaling_factor}, we search for the best scaling factor for the class embedding strategy. As can be seen, when we use a larger scaling factor than 0.01, the performance degraded significantly. This is because using a larger scaling factor, more easy cases are reserved with the same time step .
In addition, we found the best scaling factor (\ie, 0.01) for class embedding is typically smaller than analog bits \cite{chen2022generalist} and one-hot (\ie, 0.1). 

\paragraph{Noise Schedule.} 
As shown in \cref{tab:noise_schedule}, we compare the effectiveness of the cosine schedule \cite{nichol2021improved} and linear schedule \cite{ho2020denoising} in \ours for semantic segmentation, and find that the model using the cosine schedule achieves notably better performance (47.0 \emph{vs.} 45.1).
This is attributed to the cosine schedule's mechanism of simulating the realistic scenario of gradually weakening signal influence, which prompts the model to learn stronger denoising capabilities, in contrast to the simple linear schedule. 






\paragraph{Decoder Depth.}
We study the effect of decoder depth in \cref{tab:decoder_depth} and observe that the map decoder requires a suitable depth.
Initially, the model accuracy improves as the depth increases, but eventually decreases. 
Therefore, we finally adopted a map decoder with 6 blocks, which only has 8.4M parameters.
Overall, the map decoder is lightweight and efficient, compared with representative methods K-Net \cite{zhang2021k} (41.5M) and UperNet \cite{xiao2018unified} (31.5M).




\paragraph{Accuracy \emph{vs.} Efficiency.}

We show the dynamic trade-off of \ours between accuracy and efficiency  in \cref{tab:efficiency}. 
Compared with the representative discriminative method K-Net \cite{zhang2021k}, \ours yields a better mIoU when using only one sampling step, with fewer FLOPs and higher FPS. When adopting three sampling steps, the performance is further boosted to 47.0 mIoU, while maintaining comparable FLOPs and FPS.
These results show that \ours can iteratively infer multiple times with reasonable time cost.
 \vspace{0.7em}
\section{Conclusion}
This paper introduced DDP, a simple, efficient, yet powerful framework for dense visual predictions based on conditional diffusion.
It extends the denoising diffusion process into modern perception pipelines, without requiring architectural customization or task-specific design.
We demonstrate DDP's effectiveness through state-of-the-art or competitive performance on three representative tasks and six diverse benchmarks.
Moreover, it additionally exhibits multiple inference and uncertainty awareness, which contrasts with previous single-step discriminative methods. 
These results indicate that DDP can serve as an important baseline for future research in dense prediction tasks.
One potential drawback of DDP is its non-negligible additional computational cost for multi-step inference. 
Besides, while DDP has demonstrated excellent improvement on several benchmark datasets for dense visual prediction tasks, further research is necessary to determine its efficacy in other domains.


\paragraph{Acknowledgement.}
We gratefully acknowledge the support of MindSpore, CANN (Compute Architecture for Neural Networks) and Ascend AI Processor used for this research. 
{\small
\bibliographystyle{ieee_fullname}
\bibliography{src/bibs/egbib,src/bibs/diffusion}
}


\clearpage

\appendix

\section{Diffusion Model}
\label{supp:diffusion}

\subsection{Algorithm details}

As a supplement to \cref{algo:ddp:training} and \cref{algo:ddp:sampling}  described in the main paper, we provide the implementation details in \cref{algo:ddp:ddim} for better clarity.
Additionally, we introduce the implementation of the ``\emph{self-aligned denoising}" procedure in \cref{algo:ddp:self}, used in the last 5K iteration training to address the sampling drift problem (see \cref{sec:inference}).
We provide an example  in \cref{fig:dis:shift} to illustrate the  gap between the training and inference denoising targets.


\begin{algorithm}[t!]
\caption{DDIM Update}
\label{algo:ddp:ddim}
\definecolor{codeblue}{HTML}{2E8B57} \definecolor{codekw}{HTML}{DC143C} \lstset{
  backgroundcolor=\color{white},
columns=fullflexible,
  breaklines=true,
  captionpos=b,
  commentstyle=\fontsize{7.5pt}{7.5pt}\color{codeblue},
  keywordstyle=\fontsize{7.5pt}{7.5pt}\color{codekw},
  escapechar={|}, 
}
\lstset{language=Python}
\begin{lstlisting}[mathescape,xleftmargin=-1em]
def alpha_cumprod(t, ns=0.0002, ds=0.00025):
 """cosine noise schedule"""
 n = torch.cos((t + ns) / (1 + ds)
     * math.pi / 2) ** -2
 return -torch.log(n - 1, eps=1e-5)
 
def ddim(map_t, map_pred, t_now, t_next):
  """
  estimate x at t_next with DDIM update rule.
  """
   = alpha_cumprod(t_now)
   = alpha_cumprod(t_next)
  map_enc = encoding(map_pred)
  map_enc = (sigmoid(map_enc) * 2 - 1) * scale
  eps =  * (map_t -  * map_enc)
  map_next =  * x_pred +  * eps
  return map_next
\end{lstlisting}
\vspace{0.475em}
\end{algorithm}




\subsection{More Discussions}

As illustrated in \cref{fig:multiple_inference}, diffusion models for perceptual tasks tend to reach a saturation point within the first few steps, usually between 3-5 steps, making additional diffusion less advantageous. This is in contrast to the requirements of generative models for image generation, where multiple iterations over many steps (from 10 to 50) are often necessary.
Intuitively, in generative tasks such as image generation, the goal is to produce complete and high-quality results by progressively incorporating more information at each time step, thus gradually accumulating and improving the overall result.
Therefore, it may take more time steps to reach convergence in order to fully accumulate the necessary information.
In perceptual tasks, such as semantic segmentation and object detection, the process from image to label is a gradual reduction of information, and critical information sufficient to make a decision needs to be obtained in only a few steps. Therefore, further diffusion has a limited role in improving the accuracy of predictions, leading to an early peak within three to five steps.
In short, the diffusion process in a perception task can make decisions by accumulating the most important information. Therefore, DDP can achieve high accuracy in perception tasks with minimal computational cost.

\begin{algorithm}[t!]
\caption{ \ours Self-aligned Denoising}
\label{algo:ddp:self}
\definecolor{codeblue}{HTML}{2E8B57} 
\definecolor{codekw}{HTML}{DC143C}
\lstset{
  backgroundcolor=\color{white},
  columns=fullflexible,
  breaklines=true,
  captionpos=b,
  commentstyle=\fontsize{7.2pt}{7.2pt}\color{codeblue},
  keywordstyle=\fontsize{7.2pt}{7.2pt}\color{codekw},
  escapechar={|}, 
}
\lstset{language=Python}
\begin{lstlisting}[mathescape,xleftmargin=-1em]
def train(images, maps):
  """
  images: [b, 3, h, w], maps: [b, 1, h, w]
  """
  img_enc = image_encoder(images)
  map_t = normal(mean=0, std=1)
  map_pred = map_decoder(map_t, img_enc, t=1)
  # encode map_pred
  map_enc = encoding(map_pred.detach())
  map_enc = (sigmoid(map_enc) * 2 - 1) * scale
  # corrupt the map_enc
  t, eps = uniform(0, 1), normal(mean=0, std=1)
  map_crpt = sqrt(alpha_cumprod(t)) * map_enc +
              sqrt(1 - alpha_cumprod(t)) * eps
  # predict
  map_pred = map_decoder(map_crpt, img_enc, t)
  loss = objective_func(map_pred, maps)
  return loss
\end{lstlisting}
\end{algorithm}

\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{src/figures/dis_shift.pdf}
    \caption{\textbf{Sampling drift}. Denoising targets differ from the training process and inference process. }
    \label{fig:dis:shift}
\end{figure}

\section{Implementation Details}
\label{supp:implement}
\subsection{Semantic Segmentation}
\paragraph{ADE20K.} We conduct the experiments of ADE20K \cite{zhou2017scene} semantic segmentation based on MMSegmentation~\cite{mmseg2020}.
In the training phase, the backbone is initialized with the ImageNet \cite{deng2009imagenet} pre-trained weights.
We optimize our \ours models using AdamW~\cite{loshchilov2017decoupled} optimizer with an initial learning rate of 6, and a weight decay of 0.01.
The learning rate is decayed following the polynomial decay schedule with a power of 1.0.
Besides, we randomly resize and crop the image to 512512 for training, and rescale to have a shorter side of 512 pixels during testing.
All models are trained for 160k iterations with a batch size of 16 and compared fairly with previous discriminative-based and non-diffusion methods.


\paragraph{Cityscapes.}
The Cityscape dataset includes 5000 high-resolution images, which contain  2,975 training images, 500 validation images, and 1525 testing samples.
The images are captured from 50 different cities in Germany, covering various environments such as highways, city centers, and suburbs.
Similar to ADE20K, during training, we load the ImageNet pre-trained weights and employ the AdamW optimizer.
Following common practice, we randomly resize and crop the image to 5121024 for training, and take the original images of 10242048 for testing.
We 
Other hyper-parameters are kept the same as our ADE20K experiments.

\subsection{BEV Map Segmentation}
\paragraph{nuScenes.}
We conduct our experiments of BEV map segmentation on nuScenes~\cite{caesar2020nuscenes}, a large-scale multi-modal dataset for 3D detection and map segmentation.
The dataset is split into 700/150/150 scenes for training/validation/testing.
It contains data from multiple sensors, including six cameras, one LIDAR, and five radars.
For camera inputs, each frame consists of six views of the surrounding  environment at the same timestamps.
We resize the input views to 256704 and voxelize the point cloud to 0.1m.
Our evaluation metrics align with \cite{liu2022bevfusion} and report the IoU of 6 background classes, including drivable space, pedestrian crossing, walk-way, stop line, car-parking area, and lane divider, and use the mean IoU as the primary evaluation metric.
We adopt the image and LiDAR data augmentation strategies from \cite{mmdetection} for training.
AdamW is utilized with a weight decay of 0.01 and a learning rate of 5e-5.
We take overall 20 training epochs on 8 A100 GPUs with a batch size of 32.
Other training settings are kept the same as \cite{liu2022bevfusion} for fair comparisons.
\subsection{Depth Estimation}
\paragraph{KITTI.}
The KITTI depth estimation dataset is a widely used benchmark dataset for monocular depth estimation with a depth range from 0-80m.
The stereo images of the dataset have a resolution of 1242375, while the corresponding GT depth map has a low density of 3.75\% to 5.0\%. 
Following the standard Eigen training/testing split~\cite{eigen2014depth}, we use around 26K left view images for training and 697 frames for testing.
We incorporate the \ours model into the codebase developed by \cite{li2022depthformer} for KITTI depth estimation experiments.
We excluded the discrete label encoding module as the task requires continuous value regression
All experimental settings are the same as \cite{li2022depthformer} for a fair comparison.



\begin{table}[t!]
\centering
\footnotesize
\setlength{\tabcolsep}{1.25mm}
\begin{tabular}{l|ccc|ccc}
Method &  &  &  & REL  & RMS  &  \\
\hline
Chen et al. & 0.757 & 0.943 & 0.984 & 0.166 & 0.494 & 0.071 \\
Yin et al.~\cite{yin2019enforcing}  & 0.696 & 0.912 & 0.973 & 0.183 & 0.541 & 0.082 \\
BTS~\cite{lee2019big}  & 0.740 & 0.933 & 0.980 & 0.172 & 0.515 & 0.075 \\
AdaBins~\cite{bhat2021adabins}  & 0.771 & 0.944 & 0.983 & 0.159 & 0.476 & 0.068 \\
DepthFormer~\cite{li2022depthformer} & \underline{0.815} & \underline{0.970} & \underline{0.993} & \underline{0.137} & \underline{0.408} & \underline{0.059} \\
\rowcolor{gray!10} 
\ours (step 3) & \textbf{0.825} & \textbf{0.973} & \textbf{0.994} & \textbf{0.128} & \textbf{0.397} & \textbf{0.056} \\
\end{tabular}
\vspace{0.5em}
\caption{\textbf{Depth estimation on the SUN RGB-D dataset.}
We report the result of the model trained on the NYU-DepthV2 dataset and tested on the SUN RGB-D dataset without fine-tuning.}
\label{tab:exp:depth:sun}
\end{table}

\paragraph{NYU-DepthV2.}
The NYU-DepthV2 is an indoor scene dataset that consists of RGB and depth images captured at a resolution of 640480 pixels. 
The dataset contains over 1,449 pairs of aligned indoor scenes, captured from 464 different indoor areas.
We train \ours using image pairs with a resolution of 320240 and with varying depths up to approximately 10 meters.
Following previous work, we evaluate the results on the predefined center cropping by \cite{eigen2014depth}.
To be fair, all experimental configurations were aligned with the previous method \cite{li2022depthformer}.


\paragraph{SUN RGB-D.}
We use this dataset \cite{song2015sun} to evaluate generalization. 
To be specific, we assess the performance of our NYU pre-trained models on the official test set, which includes 5,050 images, without any additional fine-tuning. 
The maximum depth is restricted to 10 meters. 
Please note that this dataset is solely intended for evaluation purposes and is not utilized for training.

\begin{table}[t!]
\centering
\footnotesize
\setlength{\tabcolsep}{1.25mm}
\begin{tabular}{l|ccc|ccc}
Method &  &  &  & REL  & RMSE  &   \\
\hline 
StructDepth~\cite{li2021structdepth} & 0.817 & 0.955 & 0.988 & 0.140 & 0.534 & 0.060  \\
MonoIndoor~\cite{ji2021monoindoor} & 0.823 & 0.958 & 0.989 & 0.134 & 0.526 & -  \\
DORN~\cite{fu2018deep} & 0.828 & 0.965 & 0.992 & 0.115 & 0.509 & 0.051  \\
BTS~\cite{lee2019big} & 0.885 & 0.978 & 0.994 & 0.110 & 0.392 & 0.047  \\
DAV~\cite{huynh2020guiding} & 0.882 & 0.980 & 0.996 & 0.108 & 0.412 & -  \\
TransDepth~\cite{yang2021transformer} & 0.900 & 0.983 & 0.996 & 0.106 & 0.365 & 0.045  \\
DPT-Hybrid~\cite{ranftl2021vision} & \underline{0.904} & 0.988 & \textbf{0.998} & 0.110 & 0.357 & 0.045  \\
AdaBins~\cite{bhat2021adabins} & 0.903 & 0.984 & \underline{0.997} & 0.103 & 0.364 & 0.044  \\
DepthFormer~\cite{li2022depthformer} &  \textbf{0.921} & \underline{0.989} & \textbf{0.998} & \underline{0.096} & \underline{0.339} & \underline{0.041}  \\
\rowcolor{gray!10} 
\ours (step 3) & \textbf{0.921} & \textbf{0.990} & \textbf{0.998} &\textbf{0.094} & \textbf{0.329} & \textbf{0.040}   \\ 
\end{tabular}
\vspace{0.5em}
\caption{\textbf{Depth estimation on the NYU-DepthV2 val set.}
We report the performance of \ours with 3 diffusion steps.
The best and second-best results are bolded or underlined, respectively. ↓ means lower is better, and ↑ means higher is better.}
\label{tab:exp:depth:nyu}
\end{table}

\section{Experimental Results}
In \cref{tab:exp:depth:nyu}, we provide the depth estimation performance of DDP on the NYU-V2 dataset, in addition, in \cref{tab:exp:depth:sun}, we provide the generalization performance results of DDP on the SUN-RGBD dataset.

\label{supp:experiment}





\section{Visualization}
\label{supp:visualization}

\cref{fig:vis:city_multi_steps} and \cref{fig:vis:ade_multi_steps} visualize the ``multiple inference" property of \ours on the validation sets of Cityscapes and ADE20K, respectively.
These inference trajectories show that DDP can enhance its performance continuously and produce smoother segmentation maps by using more sampling steps.
\cref{fig:vis:bev} presents the BEV map segmentation results of \ours (step 3) with the ground truths and multi-view images.
\cref{fig:vis:kitti} and \cref{fig:vis:nyu} compare the generated depth estimation results of \ours (step 3) with the ground truths on the validation sets of KITTI and NYU-DepthV2, respectively. 
These results indicate that our method can be easily generalized to most dense prediction tasks.






\newpage

\begin{figure*}[h]
    \centering
    \includegraphics[width=\linewidth]{src/figures/city.pdf}
    \caption{\textbf{Visualization of multiple inference on Cityscapes val set.}
     }
    \label{fig:vis:city_multi_steps}
\end{figure*}

\newpage
\begin{figure*}[h]
    \centering
    \includegraphics[width=\linewidth]{src/figures/ade.pdf}
    \caption{\textbf{Visualization of multiple inference on ADE20K val set.}
     }
    \label{fig:vis:ade_multi_steps}
\end{figure*}

\newpage
\begin{figure*}[h]
    \centering
    \includegraphics[width=\linewidth]{src/figures/ddp_bev_vis.pdf}
    \caption{\textbf{Visualization of predicted BEV map segmentation results on nuScenes val set.}
     }
    \label{fig:vis:bev}
\end{figure*}

\newpage
\begin{figure*}[h]
    \centering
    \includegraphics[width=\linewidth]{src/figures/ddp_kitti_vis.pdf}
    \caption{\textbf{Visualization of predicted depth estimation results on KITTI val set.}
     }
    \label{fig:vis:kitti}
\end{figure*}

\newpage
\begin{figure*}[h]
    \centering
    \includegraphics[width=\linewidth]{src/figures/ddp_nyu_vis.pdf}
    \caption{\textbf{Visualization of predicted depth estimation results on NYU-DepthV2 val set.}
     }
    \label{fig:vis:nyu}
\end{figure*}

\newpage

\section{More Applications}
\subsection{Combine DDP with ControlNet}
\paragraph{Setup.}
It has been found that compared to the previous single-shot model, DDP can achieve more continuous and semantic consistency prediction results. To demonstrate the benefits of this pixel clustering property, we combined DDP with the recently popular segmentation mask condition generation model: ControlNet. We followed the official implementation of ControlNet for all hyperparameters, including input resolution and DDIM sampling steps.


\paragraph{Implementation}
ControlNet~\cite{zhang2023adding} improves upon the original Stable Diffusion (SD) model by adding extra conditions, which is done by incorporating a conditioning network.
In the mask-conditional ControlNet, the map generated by the segmentation model is used as input for image synthesis.
The original segmentation model was adopted from Uniformaer-S~\cite{li2022uniformer} with UperNetHead, which has 52M parameters and achieves 47.6 mIoU (ss) on the ADE20K dataset.
To make a fair comparison, we replaced the original segmentation model in the mask-conditional ControlNet with DDP using the Swin-T backbone, which has 40M parameters and achieves 47.0 mIoU (ss) on the ADE20K dataset.
Note that all results were obtained with the default prompt.

\paragraph{Results}
We select images from the PEXEL website \url{https://www.pexels.com/} for testing in different scenarios.
The results from the original ControlNet and the combination of DDP with ControlNet are shown in \cref{fig:vis:ddp_control}.
ControlNet is designed to achieve fine-grained, controllable image generation, our experiments show that DDP can produce more consistent results and has advantages in various scenarios. Moreover, when combined with DDP, ControlNet produces visually satisfying and well-composed results, surpassing those of the original ControlNet.
Our experimental results suggest that DDP has great potential to improve cooperation with other types of foundation models.


\begin{figure*}[h]
    \centering
    \includegraphics[width=0.775\linewidth]{src/figures/ddp_control.pdf}
    \caption{\textbf{Control Stable Diffusion with Semantic Map}, the Uniformer-UnperNet, and DDP segmentation models are used to predict segmentation maps as condition input. All results were achieved using the default prompt.
     }
    \label{fig:vis:ddp_control}
\end{figure*}


\newpage
\clearpage

 
\end{document}