\let\accentvec\vec \documentclass{llncs}
\let\spvec\vec     \let\vec\accentvec 

\pagestyle{plain}



\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc} 

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}


\usepackage[ruled,vlined,linesnumberedhidden]{algorithm2e}
\setlength{\algomargin}{1em}
\DontPrintSemicolon

\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{xcolor}
\usepackage{tikz}

\definecolor{darkblue}{rgb}{0,0,.5}
\definecolor{darkred}{rgb}{0.5,0,0}
\definecolor{lightgray}{rgb}{0.8,0.8,0.8}
\definecolor{lightlightgray}{rgb}{0.9,0.9,0.9}

\usepackage{paralist}
\usepackage{colortbl}
\usepackage{placeins} \usepackage{framed}
\usepackage{wrapfig}

\usepackage{url}
\usepackage{hyperref}
\hypersetup{colorlinks=true, breaklinks=true, linkcolor=darkred, menucolor=darkred, urlcolor=darkred, citecolor=darkred}


\newcommand{\cost}{\gamma}
\newcommand{\mcost}{\hat{\gamma}}
\newcommand{\nprim}{{n_{\mathrm{p}}}}
\newcommand{\nback}{{n_{\mathrm{b}}}}
\newcommand{\fr}{\lambda}
\newcommand{\npn}{r_p}
\newcommand{\Enpn}{\overline{r_p}}
\newcommand{\npm}{\alpha_p}
\newcommand{\Enpm}{\overline{\alpha_p}}
\newcommand{\pr}{{\scriptstyle \#}\mathrm{pr}}
\newcommand{\Epr}{\overline{{\scriptstyle \#}\mathrm{pr}}}
\newcommand{\Vpr}{S^2[{\scriptstyle \#}\mathrm{pr}]}
\newcommand{\st}{{\scriptstyle \#}\mathrm{st}}
\newcommand{\Est}{\overline{{\scriptstyle \#}\mathrm{st}}}
\renewcommand{\Vst}{S^2[{\scriptstyle \#}\mathrm{st}]}
\newcommand{\res}{\sum_{\mathrm{res}}}
\newcommand{\kprim}{{k_{\mathrm{p}}}}
\newcommand{\kback}{{k_{\mathrm{b}}}}
\newcommand{\gc}{\mathrm{globalCounter}}
\newcommand{\palpha}{\mathfrak{a}}
\newcommand{\pbeta}{\mathfrak{b}}
\newcommand{\wb}{w}
\newcommand{\Ewb}{\overline{w}}
\newcommand{\Vwb}{S^2[w]}
\newcommand{\tnb}{{c^{\scriptscriptstyle-}_{s,m}}}
\newcommand{\tb}{{c^{\scriptscriptstyle+}_{s,m}}}

\newcommand{\cs}{c^{\scriptstyle\mathrm{start}}}
\newcommand{\ce}{c^{\scriptstyle\mathrm{end}}}


\newcommand{\ie}{i.e.}
\newcommand{\eg}{e.g.}

\newcommand{\imgScale}{0.58}
\newcommand{\tabScale}{0.75}
\newcommand{\figPath}{.}

\renewcommand{\arraystretch}{1.2}
\setlength{\tabcolsep}{4pt}

\setlength{\intextsep}{0.2cm}


\spnewtheorem*{prob}{Problem}{\itshape}{\upshape}


\author{Martin Dietzfelbinger\inst{1}\thanks{Research supported by DFG grant DI 412/10-1.}
\and Michael Mitzenmacher \inst{2}\thanks{Research supported by NSF grants IIS-0964473 and CCF-0915922.}
\and Michael Rink\inst{1}
}

\institute{Fakult\"at f\"ur Informatik und Automatisierung, Technische Universit\"at Ilmenau
\email{\{martin.dietzfelbinger,michael.rink\}@tu-ilmenau.de}
\and School of Engineering and Applied Sciences, Harvard University
\email{michaelm@eecs.harvard.edu}
}

\title{Cuckoo Hashing with Pages}

\begin{document}
\maketitle
\begin{abstract}  
Although cuckoo hashing has significant applications in both
theoretical and practical settings, a relevant downside is that it
requires lookups to multiple locations.  In many settings, where
lookups are expensive, cuckoo hashing becomes a less compelling
alternative.  One such standard setting is when memory is arranged in
large pages, and a major cost is the number of page accesses.  We
propose the study of cuckoo hashing with pages, advocating approaches
where each key has several possible locations, or cells, on a single
page, and additional choices on a second backup page.  We show
experimentally that with  cell choices on one page and a single
backup cell choice, one can achieve nearly the same loads as when each
key has  random cells to choose from, with most lookups requiring
just one page access, even when keys are placed online using 
a simple algorithm.  While our results are currently
experimental, they suggest several interesting new open theoretical
questions for cuckoo hashing with pages.
\end{abstract}  
\section{Introduction}
Standard cuckoo hashing places keys into a hash table by providing
each key with  cells determined by hash functions.  Each cell can
hold one key, and each key must be located in one of its cells.  As
new keys are inserted, keys may have to move from one alternative to
another to make room for the new key. Cuckoo hashing provides high
space utilization and worst-case constant-time lookups, making it an
attractive hashing variant, with useful applications in both
theoretical and practical settings, \eg, \cite{alcantara2009real,erlingsson2006cool,FotakisPSS2005,PaghR2004,ross2007efficient}.  

Perhaps the most significant downside of cuckoo hashing, however, is
that it potentially requires checking multiple cells randomly
distributed throughout the table.  In many settings, such random
access lookups are expensive, making cuckoo hashing a less compelling
alternative.  As a comparison, standard linear probing works
well for many settings where memory is split into (not too small)
chunks, such as cache lines; in such settings, with suitably small
loads, the average number of memory accesses is usually very close to
1.

In this paper, we consider cuckoo hashing under a setting where memory
is arranged into pages, and the primary cost is the number of
page accesses.  In such a setting, a natural scheme to minimize this
number might be to first hash each key to a page, and
then keep a separate cuckoo hash table in each page.  This limits the
number of pages examined to one, and maintains the constant lookup
time once the page is loaded.  Such a scheme has been utilized in
previous work (\eg, \cite{alcantara2009real}).  However, a problem
with such a scheme is that the most overloaded page limits the load
utilization of the entire table.  As we show later, the random
fluctuations in the distribution of keys per page can significantly
affect the maximum achievable load.

We generalize the above approach by placing most of the cell choices
associated with a key on the same primary page.  We then allow a
backup page to contain secondary choices of possible locations for a
key (usually just one).  In the worst case we now must access two
pages, but we demonstrate experimentally that we can arrange so that
for most keys we only access the primary page, leading to close to one page
accesses on average.  Intuitively, the secondary page for each key
allows overloaded pages to slough off load constructively to
underloaded pages, this distributing the load.  We show that we
can do this effectively offline as well as online by evaluating an
algorithm that we find performs well even when keys are deleted as
well as inserted into the table.

We note that it is simple to show that using a pure splitting scheme,
with no backup page, and page sizes , , where
 is the number of memory cells, the load thresholds obtained are
asymptotically provably the same as for cuckoo hashing without pages.
Analysis using such a parametrization does not seem suitable to
describe real-world page and memory sizes.  While we \emph{conjecture}
that the load thresholds obtained using the backup approach, for
reasonable parameters for memory and page sizes, match this bound, at
this point our \emph{work is entirely experimental}.  We believe this
work introduces interesting new theoretical problems for cuckoo
hashing that merit further study.
\subsection{Related Work}
The issue of coping with pages for hash-based data structures is not
new.  An early reference is the work of Manber and Wu, who consider
the effects of pages for Bloom filters
\cite{manber1994algorithm}. Their approach is the simple splitting approach we
described above; they first hash a key to a page, and each page then
corresponds to a separate Bloom filter.  The deviations in the number
of keys hashed to a page yield only small increases in the overall
probability of a false positive for the Bloom filter, making this
approach effective.  As we show below, such deviations
have more significant effects on the acceptable load for cuckoo
hash tables, leading to our suggested use of backup pages.  More recent
work includes that of Woelfel \cite{woelfel2006maintaining}, who
focuses on perfect external memory dictionaries that require
additional space for the hash function and for handling insertions and
deletions.

Our work is perhaps superficially related to the body of literature on
cuckoo hashing where cells (or buckets) can hold multiple keys. 
Here for searching the whole page or bucket has to be scanned.
This topic was first examined by Dietzfelbinger and Weidling
\cite{dietzfelbinger2007balanced}; other notable work in this area
includes that of Lehman and Panigrahy, who prove that ``overlapping''
buckets can yield improved space utilization \cite{LehmanP2009}.  Here
we don't think of the entire page as a bucket, as we are thinking of
pages as being sufficiently large that we may want to avoid searching
through a page for a key.  Our work can also be considered as related
to work on using stashes, or extra locations when keys cannot be
placed normally, with cuckoo hashing \cite{kirsch2008more}.  Here,
each key can be thought of as having an individualized stash corresponding
to one or more cells on a separate page.

A number of papers have recently resolved the longstanding issue regarding the load
threshold for standard cuckoo hash tables where each key obtains 
choices
\cite{dietzfelbinger2010tight,fountoulakis2011multiple,fountoulakis2010orientability,frieze2009maximum,GaoWormald2010}.
Our work re-opens the issue, as we consider the question of the effect
of pages on these thresholds, if the pages are smaller then
, such as for example .

Practical motivation for this approach includes recent work on
real-world implementations of cuckoo hashing \cite{alcantara2009real,ross2007efficient}.
In \cite{alcantara2009real}, where cuckoo
hashing algorithms are implemented on graphical processing units, the
question of how to maintain page-level locality for cuckoo hash tables
arises.  Even though work for lookups can be done in parallel, the
overall communication bandwidth can be a limitation in this setting.
Ross examines cuckoo hashing on modern processors, showing they
can be quite effective by taking advantage of available parallelism
for accessing cache lines \cite{ross2007efficient}.  Our approach 
can be seen as attempting to extend this performance, from cache lines
to pages, by minimizing the amount of additional parallelism required.
\subsection{Our Results}
We give a short summary of the results in the paper. (All results are
experimental.)  Our presented results focus on the setting of four
location choices per key.  The maximum load factor  of keys
with four hash functions and no paging is known.  With small pages and
each key confined to one page, we find using an optimal offline
algorithm that the maximum achievable load factor is quite low, well
below .  However, if each key is given three choices on a
primary page and a fourth on a backup page, the load factor is quite
close to , even while
placing most keys in their primary page, so that most keys can be
accessed with a single page access.  With three primary choices, a
single backup choice and filling up the table to 95 percent,
we find that only about 3 percent of keys need
to be placed on a backup page (with suitable page sizes).  We show
that a simple variation of the well-known random walk insertion
procedure allows nearly the same level of performance with online,
dynamic placement of keys (including scenarios with alternating insertion and deletions).
Our experiments consistently yield that at
most 5 percent of keys needs to be placed on a backup page with these
parameters. This provides a tremendous reduction of the number of
page accesses required for successful searches. 
For unsuccessful searches, spending a little more space for
Bloom filters on each page leads to an even smaller number of accesses to backup pages.

\section{Problem Description}

We want to place  keys into  memory (table) cells where each cell can hold a fixed number of  keys.  The value  is referred to 
as the load factor.  
The memory is subdivided into  pages (sub-tables) of equal size .  (Throughout the paper we assume
 is divisible by .)  Each key is associated with a \emph{primary page} 
and a \emph{backup page} distinct from the primary page, as well as a set of  distinct table cells,  on the primary page and  on the backup page.  The pages and keys are chosen according to hash functions on the key, and it is useful to think of them as being chosen uniformly at random in each case. 
For a given assignment let  be the number of keys that are placed in their primary page and let  be the number of keys that are placed in their backup page. We can state the \emph{cuckoo paging problem as} follows.

\begin{prob}[Cuckoo Paging]
Find a placement of the  keys such that the fraction  is maximized.
\end{prob}
\begin{remark}
Note that under the standard model, with no backup pages and all key locations
assumed to be chosen uniformly at random, there is threshold load factor  such that whenever  a placement exists with probability . The recent paper \cite{fountoulakis2011multiple} gives the complete picture for all reasonable values of  and .
\end{remark}


\begin{remark}As mentioned, if  and page sizes are ,
the asymptotic threshold load factor is the same as in the setting without pages.
This is easily proven using tight concentration bounds on the number of keys per page.
Our interest, however, is in ranges for  that are realistic and not too
large page sizes , so that this asymptotic behavior is not an adequate description of performance.
\end{remark}

The aim of the paper is to experimentally investigate the potential
for saving access cost by using primary and backup pages. Appropriate
algorithms are presented in the next section. For ease of description
of the algorithms we also use the following bipartite cuckoo graph model as well as the
hashing model.



\subsection{Cuckoo Graph Model}
\label{sec:graphModel}
We consider random bipartite graphs  with left node set  and right node set .
The left nodes correspond to the keys, the right nodes correspond to the memory cells of capacity . The set  is subdivided into  segments , each of size , which correspond to the separate pages.
Each left node  is incident to  edges where its neighborhood  consists of two disjoint sets  and  determined according to the following scheme (all choices are fully random): choose  from  (the index of the primary page); then choose  different right nodes from  to build the set ; next choose  (the index of the backup page) from ; and finally choose  different right nodes from  to build the set . Let  be an edge where  and . We call  a \emph{primary edge} if  and call  a \emph{backup edge} if .

\section{Algorithms}
Using the cuckoo graph we can restate the problem of inserting the keys as finding an \emph{orientation} of the edge set of the cuckoo graph  such that the indegree of all left nodes is exactly 1 and the outdegree of all right nodes is at most .
We call such an orientation \emph{legal}. An edge  with  from  and  from  is interpreted as ``storing key  in memory cell .'' If  is from  we call  a \emph{primary key} and otherwise we call  a \emph{backup key}.
Each legal orientation which has a maximum number of primary keys is called \emph{optimal}.

\subsection{Static Case}
In the static case, \ie, if the cuckoo graph  is given in advance, 
there are well-known efficient (but not linear time) algorithms to find an optimal orientation of . One possibility is 
to consider a corresponding \emph{minimum cost matching problem}:
Assign costs to each edge from  where primary edges get cost  and backup edges get cost .
Then replace each node  from  with  copies and replace each edge to which  is incident with  copies as well.
Initially direct all edges from left to right. Edges from right to left are \emph{matching edges}. The minimum cost matching problem is to find a left-perfect matching (legal orientation) with minimum cost (minimum number of backup keys).
The algorithm we used to determine such a matching is a variant of the Successive Shortest Path Algorithm \cite{network_flows} but uses a modified Hopcroft-Karp Algorithm instead of Dijkstra's Algorithm for finding augmenting paths of minimal cost. 

Given a bipartite graph with - edge costs the modified Hopcroft-Karp Algorithm finds a left-maximum matching of minimum cost
as follows.
Initially let . The algorithm works in rounds. In each round we try to find node disjoint augmenting paths (directed paths with free start node from  and free end node from ) of cost exactly . Consider round number .
For each augmenting path found in round  flip the edge orientations and the edge costs along the path, and then go to round . If in  round  no such path exists but there is an augmenting path of larger costs, increment  by one and go to round ; otherwise stop the algorithm. Augmenting paths with fixed costs  are found via a combination of a modified breadth first search (BFS) and depth first search (DFS). 
The BFS starts from all left nodes with in-degree zero  (unmatched nodes) at the beginning of a round.
The search partitions the nodes into layers. For each explored node the layer and the costs of the path to this node are stored.
A node can be explored twice if it is reached by a path of lesser cost. The BFS stops at the first level where one or more free nodes of  are reached by a path of cost exactly . Let  be the set of these right nodes. The algorithm tries to find node disjoint path between  and  via DFS, where the search can only follow edges between two successive layers.
During the recursive descent the costs of the path are accumulated. If the DFS reaches a free node and the costs are exactly ,
the recursive ascent removes the node labels, flips the edge costs and orientations along the path.
The algorithm is optimal in the sense that it finds a left-maximum matching of minimum costs since we have only integer weights and the costs of the minimum cost augmenting paths are monotonically non-decreasing.

\begin{algorithm}[!ht] 
\small
\caption{\texttt{ModifiedHopcroftKarp(\textsf{bipartite\_graph} )}\label{algo:modHK}}


\SetKw{KwFalse}{false}
\SetKw{KwNot}{not}

\;
\While{\upshape{an augmenting path exists}}
{
 x\;
 \texttt{BFS}()\;
\textrm{atLeastOnePathFound} \KwFalse\;
    \ForEach{}
    {
        \textrm{atLeastOnePathFound}\texttt{DFS}\;
    }
    \lIf{\KwNot \upshape{\textrm{atLeastOnePathFound}}}
    {
        \;
    }
}
\texttt{BFS}(\textsf{max\_cost} ):
\begin{compactitem}
 \item partitions the nodes into layers, starting from 
 \item stops at the first layer  with path to a right node with cost equals 
 \item returns the set of free nodes at layer  with path cost equals 
\end{compactitem}
 
\texttt{DFS}(\textsf{node} , \textsf{current\_cost} , \textsf{max\_cost} ):
\begin{compactitem}
 \item recursive descent through the layers given by \texttt{BFS}
 \item the current costs of the path are accumulated in 
 \item if a free node  is reached and  then the recursive ascent removes the node labels, flips the edge costs and orientations along the path and returns true; otherwise returns false
\end{compactitem}

\end{algorithm}


\subsection{Dynamic Case}
\label{sec:randomWalk}
In the online scenario the cuckoo graph initially consists only of the right nodes. To begin let us consider the case of insertions only.  The keys arrive and are inserted one by one,
and with each new key the graph grows by one left node and  edges. To find an appropriate orientation of the edges in each insertion step, we use a random walk algorithm, which is a modification of the common random walk for -ary cuckoo hashing \cite{FotakisPSS2005} but with two additional constraints:
\begin{compactenum}
 \item avoid creating backup keys at the beginning of the insertion process, and
 \item keep the number of backup keys below a small fixed fraction.
\end{compactenum}
For the description of the algorithm we use a dual approach. The pseudocode (Algorithm \ref{algo:random_walk})
refers to the graph model and the following explanation uses the hashing model.
We refer to a key's  cells on its primary page as primary positions, and the
 cells on its backup page as backup positions.
The insertion of an arbitrary key  takes one or more basic steps of the random walk,
which can be separated into the following sub-steps.
\begin{algorithm}[!ht] 

\small
\caption{\texttt{RandomWalkInsert(\textsf{node} )}\label{algo:random_walk}}

\SetKw{KwTrue}{true}
\SetKw{KwFalse}{false}
\SetKw{KwAnd}{and}
\SetKw{KwNot}{not}
\SetKw{KwBreak}{break}

\newcommand{\od}{\mathrm{outdeg}}
\newcommand{\suc}{\mathrm{success}}

\newcommand{\rn}{\texttt{randomNumber()}}



\;
\While{ \KwAnd \KwNot }
{
    \If{ with }
    {
            flip edge ; \;
        
    }

    \If{\KwNot }
    {
        \eIf{ \emph{\rn}}
        {
                choose random ;
                choose random \;
                flip edge ; flip edge ; \;
        }
        {
            \eIf{ with }
            {
                flip edge ; \;
            }
            {
                choose random ;
                choose random \;
                flip edge ; flip edge ; \;
            }
        }
    }
\;
}
\KwRet \;
{\scriptsize \textrm{\texttt{(*}The modification to avoid unnecessary back steps is not shown for the sake of clarity.\texttt{*)}}}
\end{algorithm}

Let  be the key that is currently ``nestless'', \ie,  is not stored in the memory.
First check if one of its primary positions is free. If this is the case store  in such a free cell and stop successfully. Otherwise toss a biased coin to decide whether the insertion of  should be proceed on its primary page or on its backup page.
\begin{compactitem}\item If the insertion of  is restricted to the primary page, randomly choose one of its primary positions .
Let  be the key which is stored in cell . Store  in , replace  with , and start the next step of the random walk.
\item  If  is to be stored on its backup page, first check if 
one of the backup positions of  is free. If this is the case store  in such a free cell and stop successfully. Otherwise randomly choose one of the backup positions  on this page and proceed as in the previous case.
\end{compactitem}

The matching procedure is slightly modified to avoid unnecessary back steps. That is, if a key  displaces a key  and in the next step  displaces  then  is forbidden as long as  has another option on this page.

The algorithm uses two parameters. 
 \begin{compactitem}
 \item [] - the bias of the virtual coin. This influences the fraction of backup keys. 
 \item []  - controls the terminating condition. A global counter is initialized with value , which is the maximum number of total steps of the random walk summed over all keys. For each basic step the global counter is decremented by one. If the limit is exceeded the algorithm stops with ``failure''.
 \end{compactitem}

Deletions are carried out in a straightforward fashion. To remove a
key , first the primary page is checked for  in its possible
cells, and if needed the backup page can then be checked as well.  The
cell containing  is marked as empty, which can be interpreted as
removing the left node  and its  incident edges from . The
global counter is ignored in this setting ().

\section{Experiments}
For each of the following experiments we consider cuckoo graphs  randomly generated according to some configuration 
 where  is the quotient of left nodes (keys) and right nodes (table cells),  is the total number of right nodes,  is the page size, and  are the number of primary and backup edges of each left node.
In the implementation the left and right nodes were simply the number sets  and . All random choices were made via
the pseudo random number generator {} ``Mersenne Twister'' of the GNU Scientific Library~\cite{GNU_Scientific}.

If not stated otherwise the total number of cells is  and pages are of size . Our main focus is on situations where , \ie, each cell can hold one key. Moreover we restrict ourselves to the cases  and (just for comparison)  and .  While we have done experiments with other parameter values, we believe these settings portray the main points.  Also, while we have computed sample variances, in many cases they are small;  this should be assumed when they are not discussed.  

\subsection{Static Case}
Experimental results for the static case determine the limits of our approach and serve as a basis of comparison for the dynamic case.
\subsubsection{Setup and Measurements.}
First of all we want to see the limits of cuckoo hashing with pages if there are no backup options at all.
Note that for fixed page size  and larger and larger table size  the fraction of keys that can be placed decreases.
For  keys the load of each page is approximately Poisson distributed with parameter ; 
asymptotically the success probability
can be estimated as

for , which approaches  for .

For the case with backup options we try to get an approximation for possible threshold densities.
Let  and  be the loads  that identify the transition from where there is a feasible orientation and where there is no feasible orientation of  without and with the backup option respectively.
To get approximations for  and  we study different ranges of load factors .
Specifically, for all  where , and   we construct  random graphs 
and measure the failure rate  at . We fit the sigmoid function 

to the data points (, ) using the method of least squares. The parameter  (inflection point) is an approximation of  and  respectively. With  we denote the sum of squares of the residuals.

Furthermore, for different  and page sizes , we are interested in the maximum ratio  or load  of primary keys, respectively.

For a fixed page  let  be the number of keys that have primary page  but are inserted on their backup page.
Since the number of potential primary keys for a page follows a binomial distribution, 
some pages will be lightly loaded and therefore have a small value of  or even .
Some pages will be overloaded and have to shed load, yielding a large value of .
We want to study the relative frequency of the values .

\subsubsection{Results.}  Here we consider results from an optimal placement algorithm.


\paragraph{I.}Table \ref{tab:thresholds} gives approximations of the loads where cuckoo hashing with paging and  hash functions has failure rate  in the case of  or  backup pages. With no backup pages the number of keys that can be stored decreases with decreasing page size and the success probability around  converges less rapidly, as demonstrated clearly in Fig.~\ref{fig:threshold_curves}. This effect becomes stronger as the pages get smaller. For this reason the range of load factors  of sub-table~(a) grows with decreasing page size. Using only one backup edge per key almost eliminates this effect.
In this case the values  seem to be stable for varying  and are very near to the theoretical threshold of standard -ary cuckoo hashing, which is ; only in the case of very small pages  can a minor shift of  be observed. 
The position of  as well as the slope of the fitting function appear to be quite stable for all considered page sizes. 

\newcommand{\ws}[1]{\hspace{1.2em}#1\hspace{1.2em}}
\begin{table}
\scalebox{0.90}{
\subtable[ data points, ]{
\begin{tabular}{lccl}
       & \ws{} &      &      \\\hline
    &  & 0.976794  & 0.014874  \\
    &  & 0.971982  & 0.096023 \\
    &  & 0.952213  & 0.299843  \\
    &  & 0.879309  & 0.653894  \\
 \rowcolor{lightlightgray}   &  & 0.648756  & 1.382760   \\
    &  & 0.188029  & 1.809620   \\
\end{tabular}
}

\subtable[ data points, ]{
\begin{tabular}{lccl}
       & \ws{} &      &      \\\hline
\\
    &   &  0.976774 & 0.010523 \\  
    &      ''         &  0.976760 & 0.014250 \\
    &      ''         &  0.976765 & 0.002811 \\
 \rowcolor{lightlightgray}   &      ''         &  0.976611 & 0.007172 \\
   &  &   0.973178 & 0.008917 \\
\end{tabular}
}
}

\caption{\label{tab:thresholds}Approximations of the load factors that are the midpoints of the transition from failure rate 0 to failure rate 1 (without and with backup option) via fitting function
\eqref{eq:sigmoid} to a series of data points. For each data point the failure rate among  random graphs was measured.
The grey rows correspond to the plots of Fig.~\ref{fig:threshold_curves}.}

\end{table}

\begin{figure}
\subfigure[ ,  data points,  ]{\scalebox{\imgScale}{\input{\figPath/maxCardMatch_m10E6_t10E4_dp4_ds0_a10E2_TEX.tex}}}
\subfigure[ ,   data points,  ]{\scalebox{\imgScale}{\input{\figPath/maxCardMatch_m10E6_t10E4_dp3_ds1_a10E2_TEX.tex}}}
\caption{\label{fig:threshold_curves}Point of transition (a) without and (b) with backup pages.}
\end{figure}

\paragraph{II.}The average of the {maximum fraction of primary keys}, allowing one backup option, is shown in Table \ref{tab:opt_frac_primary}.
The fraction decreases with increasing load factor  and decreases with decreasing page size  as well.
Interestingly, for several parameters,
we found that an optimal algorithm finds placements with more than  keys sitting in one of their  primary positions, where  is the threshold for standard -ary cuckoo-hashing.
That is, more keys obtain one of their primary three choices with three primary and one backup choice than what could be reached using just three primary choices even without paging.   

\newcommand{\col}{\cellcolor{lightgray}}
\begin{table}[ht]
\vspace{0.3cm}
\scalebox{\tabScale}{
 \begin{tabular}{c||cc|cc|cc|cc|cc}
      & \multicolumn{2}{c|}{} &\multicolumn{2}{c|}{} &\multicolumn{2}{c|}{} &\multicolumn{2}{c|}{} &\multicolumn{2}{c}{} \\
  &    &    &    &    &    &    &    &    &    &          \\\hline
0.90 & 1.000000 & 0.900000 & 0.999881 & 0.899893 & 0.995650 & 0.896085 & 0.975070 & 0.877563 & 0.902733 & 0.812460 \\
0.91 & 0.999997 & 0.909997 & 0.999093 & 0.909175 & 0.993008 & 0.903638 & 0.971556 & 0.884116 & 0.898281 & 0.817436 \\
0.92 & 0.998136 &\col 0.918286 & 0.996111 &     0.916422 & 0.989452 & 0.910296 & 0.967781 & 0.890358 & 0.893546 & 0.822062 \\
0.93 & 0.990957 &\col 0.921510 & 0.990467 &\col 0.921134 & 0.985015 & 0.916064 & 0.963723 & 0.896263 & 0.888041 & 0.825878 \\
0.94 & 0.983443 &\col 0.924436 & 0.983422 &\col 0.924416 & 0.979730 &\col 0.920946 & 0.959429 & 0.901863 & 0.880848 & 0.827997 \\
0.95 & 0.975952 &\col 0.927154 & 0.975961 &\col 0.927163 & 0.973744 &\col 0.925057 & 0.954876 & 0.907132 & 0.872427 & 0.828805 \\
0.96 & 0.968578 &\col 0.929835 & 0.968524 &\col 0.929783 & 0.967224 &\col 0.928535 & 0.947650 & 0.909744 & 0.862883 & 0.828367 \\
0.97 & 0.961112 &\col 0.932279 & 0.961157 &\col 0.932323 & 0.956892 &\col 0.928185 & 0.935928 & 0.907850 & 0.850154 & 0.824650 \\
 \end{tabular}
}
\vspace{0.2cm}            
\caption{\label{tab:opt_frac_primary}Average (among 100 random graphs) of the fraction of keys that can be placed on their primary page for different page sizes  and , . The failure rate is . For  the random graph did not admit a solution anymore. The entries
of the grey cells are larger than .}
\end{table}               

\paragraph{III.}
Figure~\ref{fig:moved_keys_per_page} depicts the relative frequency of the values  among  pages for selected parameters . In this case about 17 percent of all pages do not need backup pages, \ie, . This is consistent with the idea that pages with a load below  will generally not need backup pages. The mean  is about 2.5 percent of the page size  and for about  percent of the pages the value  is at most  percent of the page size. The relative frequency of  being greater than  is very small, about .
\begin{figure}
\centering
\scalebox{\imgScale}{\input{\figPath/modHK_m10E6_t10E3_dp3_ds1_a10E2_backup_per_pageTEX.tex}}
\vspace{-0.1cm}
\caption{\label{fig:moved_keys_per_page}frequency of  is , , , }
\end{figure}
 
\subsubsection{Summary.}
\label{sec:page_request}
We observed that using pages with  we achieve loads very close to the  threshold ().
Moreover the load  from keys placed on their primary page  is quite large, near or even
above .

Let  be the average (over all keys that have been inserted) number of page requests needed in a search for a key ,
where naturally we first check the primary page.
If  and a key was equally likely to be in any of its locations, 
the \emph{expected number of page requests}  would satisfy .
If  and  is near  then we have roughly .
For example, for , using the values of Table \ref{tab:thresholds} we find .



Now assume we perform a lookup for a key  not in the table. The
disadvantage of using two pages per key is that now we always require two
page requests, \ie, . This can be circumvented by
storing an additional set membership data structure, such as a Bloom
filter \cite{Bloom1970}, for each page  representing the  many
keys that have primary page  but are inserted on their backup page.

One can trade off space, computation, and the false positive
probability of the Bloom filter as desired.  As an example, suppose
the Bloom filters use  hash functions and their size corresponds to
just one bit per page cell. In this case, we can in fact use the same
hash functions that map keys to cell locations for our Bloom filter.
Bounding the fraction of 1 bits of a Bloom Filter
from above via , the distribution of  as in
Fig.~\ref{fig:moved_keys_per_page} leads to an average false positive
rate of less than  percent and therefore an expected number of page
requests  of less than  for unsuccessful searches.  One could
reduce false positives even further using more hash functions, or use
less space.

\subsection{Dynamic Case}
We have seen the effectiveness of optimal offline cuckoo hashing with paging.
We now investigate whether similar placements can be found online, by considering the simple random walk algorithm
from Sect.~\ref{sec:randomWalk}.  We begin with the case of insertions only.

\subsubsection{Setup and Measurements.}
Along with the failure rate , 
the fraction of primary keys  and corresponding load , 
and the distribution of the number of keys  inserted on their backup page,
we consider two more performance characteristics:
\begin{compactitem}
\item[] - the average number of steps of the random walk insertion procedure. A step is either storing a key  in a free cell  or replacing an already stored key with the current ``nestless'' key.
\item[] - the average number of page requests over all inserted items. Here each new key  requires at least one page request, and every time we move an item to its backup page, that requires another page request. 
\end{compactitem}


We focus on characteristics of the algorithm with loads near , varying the number of table cells  and page sizes . The performance of the algorithm heavily depends on the choice of parameters  and . Instead of covering the complete parameter space we first set  to infinity and use the measurements to give insight into the performance of the algorithm for selected values of .

In addition we want to explore whether we can expect a sufficiently low failure probability of the random walk algorithm, at least for some selected sets of parameters  including practical values of . For this we tested the following null hypothesis 
``If one uses parameter set  then Algorithm \ref{algo:random_walk} fails with probability at least .''
To test the null hypothesis for a specific  we performed the random experiment ``insertion of  keys with Algorithm \ref{algo:random_walk}''  times.
Let  be the event that all of the  many random experiments for a given  ended successfully. Then we have:

For example if  and  we have . 
Hence if we observe  we may reject the null hypothesis with high confidence.

We also study the influence of  for a fixed configuration.
We vary  to see qualitatively how the number of primary
keys as well as the number of steps and page requests depend on this
parameter.

It is well known that hashing schemes can perform differently in
settings with insertions and deletions rather than insertions alone,
so we investigate whether there are substantial differences in this
setting.  Specifically, we consider the table under a constant load by
alternating insertion and deletion steps.

\subsubsection{Results.}  Here we consider results from the random walk algorithm.
\paragraph{I.} Tables \ref{tab:random_walk_0.95} and \ref{tab:random_walk_0.97} show the behavior of the random walk algorithm with loads near  for  and . The number of allowed steps for the insertion of  keys is set to infinity via . The number of trials  per configuration is chosen such that  (keeping the running time for each configuration approximately constant).

We first note that with these parameters the algorithm found a placement for the keys in all experiments; failure did not occur.  For fixed page size the sample means are almost constant; for growing page size the load  increases, while  and  decrease, with a significant drop from page size  to .
For our choices of  the random walk insertion procedure missed the maximum fraction of primary keys by up to  percent for  and by up to  percent for  and needs roughly the same average number of steps (for fixed page size).

\begin{table}[!hb]
\centering
\scalebox{0.90}{
\begin{tabular}{c|ccccccccc}
 &     &     &        &   &   &      &      &     &    \\ \hline\hline
 &  &  &  & 0.860248 & 0.817236 & 158.707669 & 114.072760 & 10.327258 & 0.409334 \\
 ''    &  &  &  & 0.860219 & 0.817208 & 158.618752 &  11.405092 & 10.321981 & 0.040869 \\
 ''    &  &  &  & 0.860217 & 0.817206 & 158.645056 &   1.092781 & 10.323417 & 0.003914 \\ \hline
 &  &  &  & 0.938431 & 0.891509 & 22.807328 & 1.081478 & 2.248953 & 0.003760 \\
 ''    &  &  &  & 0.938424 & 0.891503 & 22.813986 & 0.104012 & 2.249273 & 0.000366 \\
 ''    &  &  &  & 0.938412 & 0.891491 & 22.813905 & 0.010862 & 2.249201 & 0.000038 \\ \hline
 &  &  &  & 0.955773 & 0.907985 & 16.580150 & 0.512018 & 1.892190 & 0.001779\\
 ''    &  &  &  & 0.955737 & 0.907950 & 16.603145 & 0.052386 & 1.893515 & 0.000182\\
 ''    &  &  &  & 0.955730 & 0.907943 & 16.598381 & 0.005534 & 1.893248 & 0.000019\\
\end{tabular}
}
\vspace{0.3cm}
\caption{\label{tab:random_walk_0.95}Characteristics of Algorithm \ref{algo:random_walk} for .  .}
\end{table}


 
\begin{table}[!ht]
\centering
\scalebox{0.90}{
\begin{tabular}{c|cccccccccc}
    &     &     &     &   &    &      &       &     &    \\\hline\hline
 &  &  &   & 0.816795 & 0.792291 & 158.506335 & 1222.640379 & 32.336112 & 48.892079  \\
''     &  &  &   & 0.816790 & 0.792286 & 153.645339 &   78.581917 & 31.363876 &  3.142566  \\
''     &  &  &   & 0.816802 & 0.792298 & 152.873602 &   10.759338 & 31.209210 &  0.430827  \\\hline
 &  &  &  & 0.886997 & 0.860387  & 23.320507 & 2.731285 & 5.361922 & 0.108700\\
''     &  &  &  & 0.886992 & 0.860382  & 23.289233 & 0.256942 & 5.355625 & 0.010218\\
''     &  &  &  & 0.886985 & 0.860375  & 23.268641 & 0.024796 & 5.351518 & 0.000986\\\hline
 &  &  &  & 0.898281 & 0.871332  & 19.497032 & 1.550490 & 4.607751 & 0.061739 \\
''     &  &  &  & 0.898232 & 0.871285  & 19.486312 & 0.146267 & 4.605481 & 0.005816 \\
''     &  &  &  & 0.898235 & 0.871288  & 19.493215 & 0.012744 & 4.606893 & 0.000507 \\
\end{tabular}
}
\vspace{0.3cm}
\caption{\label{tab:random_walk_0.97}Characteristics of Algorithm \ref{algo:random_walk} for .  .}
\end{table}


\paragraph{II.}To get more practical values for  we scaled up the values  from Tables \ref{tab:random_walk_0.95} and \ref{tab:random_walk_0.97} and {estimated the failure probability} for suitable parameter sets .
For all these parameter sets we observed a failure rate of zero among  attempts (event ). We can conclude at a level of significance of at least  that for these sets the failure probability of the random walk algorithm is at most .

\paragraph{III.}Figure \ref{fig:parameter_alpha} shows how parameter  influences the 
ratio of primary keys , the number of insertion steps  and the number of page requests .

\begin{figure}[htb]
\centering
\subfigure{\scalebox{0.58}{\input{\figPath/randWalk_m10E6_t10E3_dp3_ds1_a10E3_fac30_thresh070-099_loadTEX.tex}}}
 \hspace{0.2cm}
\subfigure{\scalebox{0.58}{\input{\figPath/randWalk_m10E6_t10E3_dp3_ds1_a10E3_fac30_thresh070-099_stepTEX.tex}}}
 \hspace{0.2cm}
\subfigure{\scalebox{0.58}{\input{\figPath/randWalk_m10E6_t10E3_dp3_ds1_a10E3_fac30_thresh070-099_preqTEX.tex}}}
\caption{\label{fig:parameter_alpha} }
\end{figure}

The mean fraction of primary keys  grows linearly and  grows nonlinearly with growing .
For  the gap between the optimal fraction of primary keys and the fraction reached by the random walk procedure is about  percent. The value of  also depends nonlinearly on  and reaches a local minimum at . The sample variances are quite small and stable except for  and large  (near ).

\paragraph{IV.}
The results for alternating insertions and deletions for parameters  and  are shown in Fig.~\ref{fig:number_of_steps}. We measured the current fraction of primary keys  and the number of insertion steps with respect to each key . Recall that  is the average number of insertion steps concerning all keys.
\begin{figure}[ht]
\centering
{
\scalebox{0.7}{
\input{\figPath/del_2n_randWalk_m10E6_t10E3_dp3_ds1_a10E3_fac30_thresh097_numOfOpsPerKeyTEX.tex}
}
\caption{\label{fig:number_of_steps}, , , The ordinate of the right half of the upper plot is in log scale.}
}
\end{figure}
In the first phase (insertions only) the average number of steps per key grows very slowly at the beginning and is below  when reaching a load where about  percent of current keys are backup keys.  After that  grows very fast up to almost  (for the last few keys), which is the page size.
The sample mean of the average number of steps  up to this point is about . Similarly the sample mean of the fraction of primary keys  decreases very slowly at the beginning and decreases faster at the end of the first phase.
Up to load about  percent the fraction of backup keys is below  percent. 
In the second phase (deletions and insertions alternate)  and  decrease and quickly reach a steady state. Since the decrease of  is marginal but the drop  is significant we may conclude that the overall behavior is better in steady state than at the end of the insertion only phase. Moreover in an extended experiment with  insertions and  delete-insert pairs the observed equilibrium remains the same and therefore underpins the conjecture that Fig.~\ref{fig:number_of_steps} really shows a ``convergence point'' for alternating deletions and insertions.

\paragraph{V.}
Figure~\ref{fig:moved_keys_per_page_random_walk} shows the relative frequency of the values  among  pages for  and  at the end of the insertion only phase, given by Fig.~\ref{fig:moved_keys_per_page_random_walk}~(a), and
at the end of the alternation phase, given by Fig.~\ref{fig:moved_keys_per_page_random_walk}~(b).
\begin{figure}
\centering
\hspace{0.7cm}
\subfigure[insertion only phase]{\scalebox{\imgScale}{\input{\figPath/randWalk_m10E6_t10E3_dp3_ds1_a10E2_fac30_thresh097_backup_per_pageTEX.tex}}}
 \hspace{0.9cm}
\subfigure[insertion and deletion phase]{\scalebox{\imgScale}{\input{\figPath/del_2n_randWalk_m10E6_t10E3_dp3_ds1_a10E2_fac30_thresh097_backup_per_pageTEX.tex}}}
\caption{\label{fig:moved_keys_per_page_random_walk}frequency of , , , }
\end{figure}
Note that Fig.~\ref{fig:moved_keys_per_page_random_walk}~(a) corresponds to Fig.~\ref{fig:moved_keys_per_page} with respect to the graph parameters. 
The shapes of the distributions differ only slightly, except that in the second phase the number of backup keys is larger.
In comparison with the values given by the optimal algorithm in Fig.~\ref{fig:moved_keys_per_page} the distribution of the
 values is more skewed and shifted to the right. 



\subsubsection{Summary.}
A simple online random-walk algorithm, with appropriately chosen parameters, can perform quite close to the optimal algorithm for cuckoo hashing with paging, even in settings where deletions occur.  

With parameters  and  the expected number
of page requests  for a successful search is about , using the values from
Table \ref{tab:random_walk_0.95}.
With the Bloom filter approach described in Sect.~\ref{sec:page_request} (which can be done only after finishing the insertion of all keys), the distribution from Fig.~\ref{fig:moved_keys_per_page_random_walk}~(a) gives an expected number of page requests for an unsuccessful search of less than .
Both values are only slightly higher than those resulting from an optimal solution.  One can instead use counting Bloom filters \cite{broder2004network} to improve performance for unsuccessful searches with online insertions and deletions, at the cost of more space.

\subsection{Small Pages}
We have seen that if one uses one backup option then the page size has only marginal influence on the existence of a legal orientation of  but heavily influences the maximum fraction of primary keys (in the dynamic case as well as in the static case). 
Tables \ref{tab:opt_frac_primary}, \ref{tab:random_walk_0.95} and \ref{tab:random_walk_0.97} show that the smaller the page the 
smaller the fraction of primary keys, with a significant decrease from page size  to .
In order to attenuate this downside one can use the following variant.
Let  and . We use the idea of blocked cuckoo hashing \cite{CainSW2007,dietzfelbinger2007balanced,FernholzR2007}
where each table cell gets capacity  for some constant . One can think of pages of size exactly one cell. Some reference (theoretical) thresholds  for the existence of a legal orientation of the corresponding cuckoo graphs are given in Table \ref{tab:2,l-ary} \cite{CainSW2007,FernholzR2007}; and experimental threshold values are given in Figure \ref{fig:2,l-ary}. (Note that  is the number of table cells of capacity  and we refer to the normalized values .)

\begin{table}[htb]
\centering
\begin{tabular}{c|ccccccc}
                    &  2        & 3        & 4        & 5        & 8        & 10       & 16    \\\hline
 &  0.897012 & 0.959154 & 0.980370 & 0.989551 & 0.997853 & 0.999143 & 0.999928
\end{tabular}
\vspace{0.2cm}
\caption{\label{tab:2,l-ary}Theoretical thresholds values .}
\end{table}






\begin{figure}
\subfigure[]{\scalebox{\imgScale}{\input{\figPath/maxCardMatch_m10E6_t10E0_dp4_ds4_a10E2_b=4_TEX.tex}}}
\subfigure[]{\scalebox{\imgScale}{\input{\figPath/maxCardMatch_m10E6_t10E0_dp16_ds16_a10E2_b16_TEX.tex}}}
\caption{\label{fig:2,l-ary} , ,  }
\end{figure}

Our aim remains to store as many keys as possible in their primary cell while keeping the load  near the threshold . 
Table \ref{tab:2,l-ary_opt_prim} gives optimal (offline) results.
\begin{table}
\vspace{0.3cm}
\centering
\scalebox{0.9}{
\begin{tabular}{c||cc|cc|cc|cc}
     & \multicolumn{2}{c|}{} &\multicolumn{2}{c|}{} &\multicolumn{2}{c}{}&\multicolumn{2}{c}{} \\
 &    &  &    &    &    &    &    &  \\\hline
0.90 & 0.822251 & 0.740026    & 0.898282 & 0.808454 &    0.913798 & 0.822418      & 0.940022 & 0.846020\\
0.91 & 0.815652 & 0.742244    & 0.894259 & 0.813776 &    0.910014 & 0.828113      & 0.936509 & 0.852223\\
0.92 & 0.808867 & 0.744157    & 0.890040 & 0.818837 &    0.906196 & 0.833700      & 0.932937 & 0.858302\\
0.93 & 0.801424 & 0.745325    & 0.885679 & 0.823682 &    0.902177 & 0.839024      & 0.929188 & 0.864145\\
0.94 & 0.793452 & 0.745845    & 0.881098 & 0.828232 &    0.898052 & 0.844169      & 0.925333 & 0.869813\\
0.95 & 0.784526 & 0.745300    & 0.876222 & 0.832411 &    0.893687 & 0.849003      & 0.921360 & 0.875292\\
0.96 & 0.774254 & 0.743283    & 0.870778 & 0.835947 &    0.889150 & 0.853584      & 0.917347 & 0.880653\\
0.97 & 0.761745 & 0.738893    & 0.864615 & 0.838676 &    0.884317 & 0.857787      & 0.913244 & 0.885846\\
0.98 & 0.743799 & 0.728923    & 0.857017 & 0.839876 &    0.878957 & 0.861378      & 0.908929 & 0.890751\\
0.99 & \multicolumn{2}{c|}{no solution}          & 0.847632 & 0.839156 &    0.870738 & 0.862030      & 0.904474 & 0.895429\\
\end{tabular}
}
\vspace{0.2cm}
\caption{\label{tab:2,l-ary_opt_prim}Maximum fraction of primary keys among 100 random graphs, for different block sizes , . For  the failure rate is  and for  we have ; otherwise .  
}
\end{table}
They indicate that with respect to the ratio of primary keys the variant 
is slightly better than  . An advantage is that for  the (known) thresholds  are higher than the values  which are near  .
For example, with  and load factor  a fraction  of the keys can be stored in their
primary cell, thus reducing the expected number of cell requests for successful searches from 
when each key is equally likely to be in either location to less than . 

\section{Conclusion}
Our results suggest that cuckoo hashing with paging may prove useful in a number of settings where the cost of multiple lookups might otherwise prove prohibitive. Perhaps the most interesting aspect for continuing work is to obtain
provable performance bounds for cuckoo hashing with pages.  Even in
the case of offline key distribution with one additional choice on a
second page we do not have a formal result proving the threshold
behavior we see in experiments.     

\begin{thebibliography}{10}
\providecommand{\url}[1]{\texttt{#1}}
\providecommand{\urlprefix}{URL }

\bibitem{network_flows}
Ahuja, R.K., Magnanti, T.L., Orlin, J.B.: {Network Flows: Theory, Algorithms,
  and Applications}. Prentice-Hall, Upper Saddle River, New Jersey 07458, USA
  (1993)

\bibitem{alcantara2009real}
Alcantara, D.A., Sharf, A., Abbasinejad, F., Sengupta, S., Mitzenmacher, M.,
  Owens, J.D., Amenta, N.: {Real-time parallel hashing on the GPU}. ACM Trans.
  Graph.  28(5) (2009)

\bibitem{Bloom1970}
Bloom, B.H.: {Space/Time Trade-offs in Hash Coding with Allowable Errors}.
  Commun. ACM  13(7),  422--426 (1970)

\bibitem{broder2004network}
Broder, A., Mitzenmacher, M.: {Network applications of Bloom filters: A
  survey}. Internet Mathematics  1(4),  485--509 (2004)

\bibitem{CainSW2007}
Cain, J.A., Sanders, P., Wormald, N.C.: {The Random Graph Threshold for
  -orientiability and a Fast Algorithm for Optimal Multiple-Choice
  Allocation}. In: Proc. 18th SODA. pp. 469--476. SIAM (2007)

\bibitem{dietzfelbinger2010tight}
Dietzfelbinger, M., Goerdt, A., Mitzenmacher, M., Montanari, A., Pagh, R.,
  Rink, M.: {Tight Thresholds for Cuckoo Hashing via XORSAT}. In: Proc. 37th
  ICALP (1). pp. 213--225 (2010)

\bibitem{dietzfelbinger2007balanced}
Dietzfelbinger, M., Weidling, C.: {Balanced allocation and dictionaries with
  tightly packed constant size bins}. Theor. Comput. Sci.  380(1-2),  47--68
  (2007)

\bibitem{erlingsson2006cool}
Erlingsson, {\'U}., Manasse, M., McSherry, F.: {A cool and practical
  alternative to traditional hash tables}. In: Proc. 7th WDAS (2006)

\bibitem{FernholzR2007}
Fernholz, D., Ramachandran, V.: {The -orientability Thresholds for 
  }. In: Proc. 18th SODA. pp. 459--468. SIAM (2007)

\bibitem{FotakisPSS2005}
Fotakis, D., Pagh, R., Sanders, P., Spirakis, P.G.: {Space Efficient Hash
  Tables with Worst Case Constant Access Time}. Theory Comput. Syst.  38(2),
  229--248 (2005)

\bibitem{fountoulakis2011multiple}
Fountoulakis, N., Khosla, M., Panagioutou, K.: {The Multiple-orientability
  Thresholds for Random Hypergraphs}. In: Proc. 22nd SODA. SIAM (2011)

\bibitem{fountoulakis2010orientability}
Fountoulakis, N., Panagiotou, K.: {Orientability of Random Hypergraphs and the
  Power of Multiple Choices}. In: Proc. 37th ICALP (1). pp. 348--359. Springer
  (2010)

\bibitem{frieze2009maximum}
Frieze, A.M., Melsted, P.: {Maximum Matchings in Random Bipartite Graphs and
  the Space Utilization of Cuckoo Hashtables}. CoRR  abs/0910.5535 (2009)

\bibitem{GaoWormald2010}
Gao, P., Wormald, N.C.: {Load balancing and orientability thresholds for random
  hypergraphs}. In: Proc. 42nd STOC. pp. 97--104. ACM (2010)

\bibitem{GNU_Scientific}
Gough, B., (editor): {GNU Scientific Library Reference Manual - Third Edition}.
  Network Theory Ltd. (2009), online:
  \url{http://www.gnu.org/software/gsl/manual/}

\bibitem{kirsch2008more}
Kirsch, A., Mitzenmacher, M., Wieder, U.: {More Robust Hashing: Cuckoo Hashing
  with a Stash}. In: Proc. 16th ESA. pp. 611--622. Springer (2008)

\bibitem{LehmanP2009}
Lehman, E., Panigrahy, R.: {3.5-Way Cuckoo Hashing for the Price of
  2-and-a-Bit}. In: Proc. 17th ESA. pp. 671--681. Springer (2009)

\bibitem{manber1994algorithm}
Manber, U., Wu, S.: {An Algorithm for Approximate Membership checking with
  Application to Password Security}. Inf. Process. Lett.  50(4),  191--197
  (1994)

\bibitem{PaghR2004}
Pagh, R., Rodler, F.F.: {Cuckoo hashing}. J. Algorithms  51(2),  122--144
  (2004)

\bibitem{ross2007efficient}
Ross, K.A.: {Efficient Hash Probes on Modern Processors}. In: Proc. 23rd ICDE.
  pp. 1297--1301. IEEE (2007)

\bibitem{woelfel2006maintaining}
Woelfel, P.: {Maintaining External Memory Efficient Hash Tables}. In:
  APPROX-RANDOM. pp. 508--519. Springer (2006)

\end{thebibliography}



\end{document}
