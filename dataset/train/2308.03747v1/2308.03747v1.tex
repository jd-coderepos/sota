\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{pbox}
\usepackage{makecell}

\usepackage{algorithm}\usepackage{algpseudocode}

\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{subcaption}

\usepackage{booktabs}
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}

\usepackage[table,x11names]{xcolor}
\def\methods{CASeg}
\usepackage{multicol}
\usepackage{multirow}
\usepackage[misc]{ifsym}


\newcommand\todo[1]{\textcolor{red}{TODO: #1}}
\newcommand\toedit[1]{\textcolor{orange}{TOEDIT: #1}}
\newcommand{\comment}[1]{}
\newcommand{\tsdagger}{{\textsuperscript{\textdagger}}}

\newcommand{\mbf}[1]{\mathbf{#1}}

\newcommand\blfootnote[1]{\begingroup
  \renewcommand\thefootnote{}\footnote{#1}\addtocounter{footnote}{-1}\endgroup
}

\definecolor{LightCyan}{rgb}{0.88,1,1}


\newcommand{\xiangyu}[1]{{\textcolor{blue}{[Xiangyu: #1]}}}
\newcommand{\mayan}[1]{{\textcolor[rgb]{1,0.545,0.545}{[Mayan: #1]}}}
\newcommand{\yuhui}[1]{{\textcolor[rgb]{0.59, 0.44, 0.84}{[Yuhui: #1]}}}
\newcommand{\haodi}[1]{{\textcolor[rgb]{1,0.545,0.545}{[Haodi: #1]}}}
\newcommand{\bluetext}[1]{\noindent \textcolor{blue}{{#1}}}
\newcommand{\yellowtext}[1]{\noindent \textcolor[rgb]{0.95, 0.52, 0.0}{{#1}}}
\newcommand{\greentext}[1]{\noindent \textcolor[rgb]{0.0, 0.62, 0.38}{{#1}}}
\newcommand{\redtext}[1]{\noindent \textcolor{red}{#1}}

\newlength\savewidth\newcommand\shline{\noalign{\global\savewidth\arrayrulewidth
  \global\arrayrulewidth 1pt}\hline\noalign{\global\arrayrulewidth\savewidth}}

\renewcommand{\ttdefault}{ptm}  \newcommand{\tablestyle}[2]{\ttfamily\setlength{\tabcolsep}{#1}\renewcommand{\arraystretch}{#2}\centering\footnotesize}

\makeatletter
\def\@fnsymbol#1{\ensuremath{\ifcase#1\or \dagger\or \dagger\or
\mathsection\or \mathparagraph\or \|\or **\or \dagger\dagger
\or \ddagger\ddagger \else\@ctrerr\fi}}
\makeatother


\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\usepackage{algorithm}
\usepackage[]{algpseudocode}

\newcommand{\norm}[1]{\|#1\|}


\cvprfinalcopy 

\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\begin{document}


\title{Mask Frozen-DETR: High Quality Instance Segmentation with One GPU}



\author{
  Zhanhao Liang \quad\quad\quad\quad\quad\quad\quad\quad\quad Yuhui Yuan\thanks{Corresponding author. \Letter\space \texttt{yuhui.yuan@microsoft.com}} \\
  The Australian National University\quad\quad\quad
  Microsoft Research Asia \\
}

\maketitle

\begin{abstract}
In this paper, we aim to study how to build a strong instance segmenter with minimal training time and GPUs, as opposed to the majority of current approaches that pursue more accurate instance segmenter by building more advanced frameworks at the cost of longer training time and higher GPU requirements. To achieve this, we introduce a simple and general framework, termed Mask Frozen-DETR, which can convert any existing DETR-based object detection model into a powerful instance segmentation model. Our method only requires training an additional lightweight mask network that predicts instance masks within the bounding boxes given by a frozen DETR-based object detector. Remarkably, our method outperforms the state-of-the-art instance segmentation method Mask DINO in terms of performance on the COCO test-dev split (55.3\% vs. 54.7\%) while being over 10 times faster to train. Furthermore, all of our experiments can be trained using only one Tesla V100 GPU with 16 GB of memory, demonstrating the significant efficiency of our proposed framework.
\end{abstract}


\section{Introduction}

Instance segmentation is one of the most fundamental but difficult computer vision tasks requiring pixel-level localization and recognition in the given input image. Most advances in modern image instance segmentation methods are heavily influenced by the latest state-of-the-art 2D object detection systems. For example, two representative leading instance segmentation approaches, including Cascade Mask R-CNN~\cite{cai2019cascade} and Mask DINO~\cite{li2022mask}, are built by adding a parallel segmentation branch to the strong object detection systems, specifically Cascade R-CNN~\cite{cai2018cascade} and DINO~\cite{zhang2022dino}.


\begin{figure}[t]
\centering
\begin{minipage}[t]{1\linewidth}
\centering
\begin{subfigure}[b]{\linewidth}
\centering
\includegraphics[height=0.3\columnwidth]{img/intro_map_v3.pdf}
\caption{mask AP}
\end{subfigure}
\hfill
\begin{subfigure}[b]{\linewidth}
\centering
\includegraphics[height=0.3\columnwidth]{img/intro_gpu_hours_v2.pdf}
\caption{GPU Hours}
\end{subfigure}
\end{minipage}
\caption{\small{Illustrating the comparison results with Mask2Former and Mask DINO on COCO instance segmentation task. (a) our Mask Frozen-DETR outperforms the previous SOTA Mask DINO by +. (b) our Mask Frozen-DETR speeds up the training by more than  times compared to Mask DINO.
}}
\label{fig:attention_maps_intro}
\end{figure}




\begin{figure*}[t]
\begin{minipage}[t]{1\linewidth}
\centering
\begin{subfigure}[b]{1\textwidth}
\centering
\includegraphics[width=1\textwidth]{img/BoxDetector_v5.pdf}
\vspace{-3mm}
\caption{\footnotesize{DETR-based object detection framework}}
\end{subfigure}
\begin{subfigure}[b]{1\textwidth}
\centering
\vspace{3mm}
\includegraphics[width=1\textwidth]{img/MaskSegmenter_v5.pdf}
\vspace{-5mm}
\caption{\footnotesize{DETR-based instance segmentation framework}}
\end{subfigure}
\begin{subfigure}[b]{1\textwidth}
\centering
\vspace{3mm}
\includegraphics[width=1\textwidth]{img/BoxDetector2MaskSegmenter_v3.pdf}
\vspace{-3mm}
\caption{\footnotesize{Our approach: Mask Frozen-DETR}}
\end{subfigure}
\end{minipage}
\caption{\small{Illustrating the overall pipelines of DETR-based object detectors (-st row), instance segmenters (-ed row) based on DETR, and the proposed approach. Instead of training the instance segmentation models from scratch, we propose a Mask Frozen-DETR that uses a frozen DETR-based object detector to generate the bounding boxes and then trains a mask head to produce instance segmentation masks.}}
\label{fig:intro}
\end{figure*}


Despite the convergence of deep neural network architectures between object detection and instance segmentation, most existing efforts still need independent training based on supervision signals of different granularity, i.e., bounding boxes vs. instance masks. Training modern instance segmentation models from scratch is resource-intensive and time-consuming. For example, Using ResNet-~\cite{He2016ResNet} and Swin-L~\cite{liu2021swin} as backbone networks, Mask2Former~\cite{cheng2022masked} requires over  and  V100 GPU hours for training, respectively.

We show that the existing DETR-based object detection models can be efficiently converted into strong instance segmentation models, unlike the previous efforts that train the instance segmentation models from scratch. We start from the recent powerful 2D object detection models, i.e., -DETR~\cite{jia2022detrs} and DINO-DETR~\cite{zhang2022dino}. We propose two key innovations: (i) design a light-weight instance segmentation network that effectively uses the output of a frozen DETR-based object detector, including the object query and the encoder feature map, to predict instance masks, and (ii) demonstrate the high training efficiency of our approach compared to the previous state-of-the-art instance segmentation approaches while achieving competitive performance under different model scales.


We conduct comprehensive comparison experiments on the COCO instance segmentation benchmark to verify the effectiveness of our approach. Our approach achieves strong results with a very short learning time. For instance, with only  training epochs, our approach slightly surpasses the state-of-the-art method Mask DINO. Remarkably, the entire training process of our approach takes less than  GPU hours, while Mask DINO takes nearly  GPU hours. Consequently, our approach improves the training efficiency by more than . We hope our simple approach can enable broader research communities to contribute to advancing stronger instance segmentation models.


\section{Related Work}

\vspace{1mm}
\noindent\textbf{Object Detection.}
Object detection is a fundamental research area that has produced a lot of excellent work, such as Faster R-CNN~\cite{ren2015faster}, Cascade R-CNN~\cite{cai2018cascade}, YOLO~\cite{redmon2016you}, DETR~\cite{carion2020end}, and Deformable DETR~\cite{zhu2020deformable}.
Recently, most of the exciting progress in object detection mainly comes from the developments of various DETR-based approaches, including DINO-DETR~\cite{zhang2022dino} and -DETR~\cite{jia2022detrs}.
The current state-of-the-art methods~\cite{wang2022internimage,lin2023detr,ma2023revisiting,liang2022expediting,he2022rankseg} are also built based on them.
In general, we can easily access a lot of object detection model weights based on DETR framework as most of them are open-sourced.
We show our approach can be extended to these modern DETR-based detectors easily and achieves strong performance while being more than  faster to train by exploiting the off-the-shelf pre-trained weights on object detection tasks.


\vspace{1mm}
\noindent\textbf{Instance Segmentation.}
Instance segmentation is a computer vision task that requires an algorithm to assign a pixel-level or point-level mask with a category label for each instance of interest in an image, video or point cloud. Most existing methods follow the R-CNN~\cite{girshick2014rich} paradigm, which first detects objects and then segments them. For example, Mask R-CNN~\cite{he2017mask} extends Faster R-CNN~\cite{ren2015faster} with a fully convolutional mask head, Casacde Mask R-CNN combines Casacde R-CNN~\cite{cai2018cascade} with Mask R-CNN, and HTC~\cite{chen2019hybrid} improves the performance with interleaved execution and mask information flow. Some recent methods propose more concise designs such as SOLO~\cite{xwang20} that segments objects by locations without bounding boxes or embedding learning, QueryInst~\cite{FangQueryInst} that performs end-to-end instance segmentation based on Sparse RCNN~\cite{sun2021sparse}, MaskFormer~\cite{cheng2021per} and Mask2Former\cite{cheng2022masked} that use a simple mask classification based on DETR~\cite{carion2020end}, and Mask-DINO~\cite{li2022mask} that extends DINO by adding a mask prediction branch that supports all image segmentation tasks using query embeddings and pixel embeddings.
Moreover, the very recent Mask3D~\cite{schult2022mask3d} and SPFormer~\cite{sun2022superpoint} have built state-of-the-art 3D instance segmentation systems following the design of Mask2Former.


\noindent\textbf{Discussion.}
Most of the existing efforts train the instance segmentation models from scratch without using the off-the-shelf object detection model weights, thus requiring very expensive training.
Our approach uses the weights of frozen DETR-based models and introduces a very efficient instance segmentation head design with high training efficiency.
Figure~\ref{fig:intro} illustrates the differences between the existing methods and our approach.
For example, the very recent state-of-the-art Mask DINO is trained from scratch while we simply freeze the existing object detector and train a very light instance mask decoder.
We empirically show the great advantages of our approach on the COCO instance segmentation task with experiments under various settings.
Notably, our approach sets new records on the challenging COCO instance segmentation task while accerating the training speed by more than .


\section{Our Approach}

\subsection{Baseline Setup}
We use a strong object detector -DETR+ResNet with AP= as our baseline for the following ablation experiments and report the results based on stronger -DETR+Swin-L and DINO-DETR+FocalNet-L for the system-level comparisons. The entire -DETR+ResNet model is pre-trained on Object~\cite{shao2019objects365} and then fine-tuned on COCO for higher performance.

To build a simple baseline for instance segmentation without extra training, we first sort the object queries output by the last Transformer decoder layer of the object detection model according to the decreasing order of their classification scores, and select the top  object queries for mask prediction. Then we multiply the object queries  and the image features  at /-resolution to get the instance segmentation masks as follows:

where  represents the feature map output by the first stage of the backbone.  represents the 1/8-resolution feature map from the Transformer encoder. 
, , and  represent the image height, image width, and feature hidden dimension, respectively.  represents the final predicted probability mask with the same resolution as the input image.
We illustrate the overall pipeline in Figure~\ref{fig:frozen_detr_pipeline_0}.

Next, we compute the confidence scores that reflects the quality of masks following:

where  represents the classification score associated with the -th object query predicted by the last Transformer decoder layer of the object detection model.

\begin{figure}[t]
\centering
\includegraphics[width=0.5\textwidth]{img/FrozenDETR_baseline.pdf}
\caption{\small{Mask Frozen-DETR Baseline: both RoIAlign and MaskHead are non-parametric operations.}}
\label{fig:frozen_detr_pipeline_0}
\end{figure}


\begin{table}[t]
\begin{minipage}[t]{1\linewidth}
\vspace{2mm}
\centering
\setlength{\tabcolsep}{1pt}
\footnotesize
\renewcommand{\arraystretch}{1.2}
\resizebox{1.0\linewidth}{!}
{
\begin{tabular}{c|c|c|c|cccccc}
RoIAlign & 1/4 feat.  & \#FLOPs & \#params & AP & AP & AP & AP & AP & AP \\
\shline
\xmark & \xmark  &  G &  M &  &  &  &  &   &  \\
\cmark & \xmark  &  G &  M &  &  &  &  &  &  \\
\xmark & \cmark  &  G &  M &  &  &  &  &  &  \\
\rowcolor{gray!10}\cmark & \cmark  &  G &  M &  &  &  &  &   &  \\
\end{tabular}
}
\caption{\small{\textbf{
Effect of each factor within our baseline that requires no training.}}
RoIAlign: use RoIAlign to pool the region features according to the predicted boxes.
1/4 feat.: fuse the /-resolution feature maps output by the stage- of backbone with the up-sampled /-resolution feature maps output by the Transformer encoder.
We use  to mark the additional increased number of parameters and FLOPs in all the following tables.
}
\label{tab:baseline_ablate}
\end{minipage}
\end{table}


We illustrate the modifications when using RoIAlign operation~\cite{he2017mask} as follows. Instead of using the entire image features , we use the RoIAlign to gather the region feature maps located within the predicted bounding boxes:

where ,  represents the predicted bounding box of .
We set  and  as  by default.
Then we compute the instance segmentation masks as follows:

where we first reshape and interpolate the predicted regional instance masks to be the same size as the real bounding box size in the original image and then paste the resized ones to an empty instance mask of the same size as the original image.
We compute the confidence scores based on  following a similar manner.

\vspace{1mm}
\noindent\textbf{Results.}
Table~\ref{tab:baseline_ablate} shows the comparison results on the effect of fusing the /-resolution feature maps output by the first stage of the backbone and using the RoIAlign to constrain the computation focus only within the predicted bounding boxes.
According to the reported results,
we find that directly multiplying the object queries with the image feature maps performs very poorly, i.e., the best setting achieves a mask AP score .
Notably, we also report the additional increased computational cost and number of parameters in all ablation experiments by default.

We aim to make minimal modifications to boost the segmentation performance based on the above baseline setting.
We show how to improve the system from three main aspects, including image feature encoder design, box region feature encoder design, and query feature encoder design, in the following discussions.
{We conduct all the following ablation experiments by freezing the entire object detection network and only fine-tuning the additional introduced parameters for  epochs on COCO instance segmentation task.}

\noindent\textbf{Experiment Setup.}
We use AdamW optimizer with an initial learning rate , ,  and a weight decay of  is employed. We train the models for  iterations (i.e.  epochs), and divide the learning rate by  at  and  fractions of the total number of the training iterations.
We follow the data pre-processing scheme of Deformable DETR \cite{zhu2020deformable}. We set the batch size as  and run all experiments with V100 GPUs with GB memory.
We report a set of COCO metrics including AP, AP, AP, AP, AP and AP.


\begin{figure}[t]
\centering
\includegraphics[width=0.5\textwidth]{img/baseline_seg_visualzation.pdf}
\caption{\small{Coarse instance segmentation with the Mask frozen-DETR baseline. We visualize the predicted probability maps and the color indicates the confidence scores: red for high and blue for low.}}
\label{fig:baseline_seg}
\vspace{-3mm}
\end{figure}


\subsection{Image Feature Encoder}

We first study how to improve the previous baseline setting by introducing a trainable image feature encoder to transform the image feature maps into a more suitable feature space for instance segmentation tasks.
Figure~\ref{fig:frozen_detr_pipeline_2} shows the overall pipeline. We apply the image feature encoder on feature map  from the Transformer encoder. We simply modify Equation~\ref{eq.mask1} as follows:


where the  represents the image feature encoder that refines the image feature map for all object queries simultaneously.
We study the following three kinds of modern convolution or transformer blocks.


\begin{figure}[t]
\centering
\includegraphics[width=0.5\textwidth]{img/FrozenDETR_imgenc.pdf}
\caption{\small{Add image feature encoder to Mask Frozen-DETR. We insert an additional image feature encoder to enhance the image feature maps.}}
\label{fig:frozen_detr_pipeline_2}
\end{figure}


\vspace{1mm}
\noindent\textbf{Deformable encoder block~\cite{zhu2020deformable}.}
We follow the multi-scale deformable encoder design and stack multiple multi-scale deformable encoder blocks to enhance the multi-scale feature map  following:

where , , , and  represent the feature maps of different scales from the Transformer encoder of the object detection system.
Each multi-scale deformable encoder block is implemented with .
 is implemented as  by default.


\vspace{1mm}
\noindent\textbf{Swin Transformer encoder block~\cite{liu2021swin}}
We follow the Swin Transformer to apply a stack of multiple Swin Transformer blocks on the feature map  with highest resolution as follows:

where each Swin Transformer block is implemented as .  represents the window multi-head self-attention operation.
We apply shifted  in the successive block to propogate information across windows following~\cite{liu2021swin}.

\vspace{1mm}
\noindent\textbf{ConvNext encoder block~\cite{liu2022convnet}}
We follow the ConvNext to apply the proposed combination of large-kernel convolution and inverted bottleneck on the Transformer encoder feature map  following:

where each ConvNext block is implemented as .  represents a depth-wise convolution with large kernel size, i.e., .

\begin{table}[t]
\begin{minipage}[t]{1\linewidth}
\vspace{2mm}
\centering
\setlength{\tabcolsep}{2pt}
\footnotesize
\renewcommand{\arraystretch}{1.2}
\resizebox{1.0\linewidth}{!}
{
\begin{tabular}{l|c|c|c|cccccc}
block type & \# layers  & \#FLOPs & \#params & AP & AP & AP & AP & AP & AP \\
\shline
None &   &  G &  M &  &  &  &  &   &  \\\hline
deformable. &   &  G &  M &  &  &  &  &  &   \\
\rowcolor{gray!10}deformable. &   &  G &  M &  &  &  &  &  &   \\
deformable. &   &  G &  M &  &  &  &  &  &  \\\hline
Swin Trans. &    &  G &  M &  &  &  &  &  &   \\
Swin Trans. &    &  G &  M &  &  &  &  &  &  \\
Swin Trans. &    &  G &  M & &  &  &  &  &  \\\hline
ConvNext. &   &  G &  M &  &  &  &  &  &    \\
ConvNext. &   &  G &  M &  &  &  &  &  &   \\
ConvNext. &   &  G &  M &  &  &  &  &  &  \\
\end{tabular}
}
\caption{\small{{
Effect of image feature encoder design.}}
}
\label{tab:feat_enc_ablate}
\end{minipage}
\end{table}

\vspace{1mm}
\noindent\textbf{Results.}
In Table~\ref{tab:feat_enc_ablate}, we compare different choices of the image feature encoder architecture design. We observe that: (i) All three image feature encoder implementations improve the instance segmentation performance. (ii) More image feature encoder layers leads to better performance, e.g.,  deformable encoder layers: AP= vs.  deformable encoder layer: AP=. (iii) Under similar computation budget, Deformable encoder block performs better, e.g.,  deformable encoder layers: AP=/FLOPs=G vs.  Swin transformer encoder layers: AP=/FLOPs=G vs.  ConvNext encoder layers: AP=/FLOPs=G. We use  deformable encoder layers as the image feature encoder in the following experiments as it has a better trade-off between performance and computational cost.

\subsection{Box Feature Encoder}
Now we study the influence of improving the box region features with an additional box feature encoder design.
We illustrate the modification in Figure~\ref{fig:frozen_detr_pipeline_3}.
we simply apply additional transformation  on  before Equation~\ref{eq.paste_mask} following:

where we study different choices of the box feature encoder implementation following the study on the image feature encoder design.

\begin{figure}[t]
\centering
\includegraphics[width=0.5\textwidth]{img/FrozenDETR_imgenc_boxenc.pdf}
\caption{\small{Add box feature encoder to Mask Frozen-DETR. We insert a feature encoder to enhance the bounding box region features.}}
\label{fig:frozen_detr_pipeline_3}
\end{figure}

\begin{table}[t]
\begin{minipage}[t]{1\linewidth}
\vspace{2mm}
\centering
\setlength{\tabcolsep}{1.2pt}
\footnotesize
\renewcommand{\arraystretch}{1.2}
\resizebox{1.0\linewidth}{!}
{
\begin{tabular}{l|c|c|c|cccccc}
block type & \# layers & \#FLOPs & \#params & AP & AP & AP & AP & AP & AP  \\
\shline
None &   &  G &  M &  &  &  &  &  &  \\\hline
deformable. &   &  G &  M &  &  &  &  &  &  \\
\rowcolor{gray!10}deformable. &   &  G &  M &  &  &  &  &  &  \\
deformable. &   & G &  M &  &  &  &  &  &  \\\hline
Swin Trans. &    &  G &  M &  &  &  &  &  &  \\
Swin Trans. &    &  G &  M &  &  &  &  &  &  \\
Swin Trans. &    &  G &  M &  &  &  &  &  &  \\\hline
ConvNext. &   &  G &  M &  &  &  &  &  &    \\
ConvNext. &   &  G &  M &  &  &  &  &  &   \\
ConvNext. &   &  G &  M &  &  &  &  &  &   \\
\end{tabular}
}
\caption{\small{{
Effect of box feature encoder design.}}
}
\label{tab:box_enc_ablate}
\end{minipage}
\end{table}

\begin{table}[t]
\begin{minipage}[t]{1\linewidth}
\vspace{2mm}
\centering
\setlength{\tabcolsep}{2pt}
\footnotesize
\renewcommand{\arraystretch}{1.2}
\resizebox{1.0\linewidth}{!}
{
\begin{tabular}{c|c|c|cccccc}
\# hidden dim. & \#FLOPs & \#params & AP & AP & AP & AP & AP & AP  \\
\shline
 &  G &  M &  &  &  &  &  &  \\
\rowcolor{gray!10} &  G &  M &  &  &  &  &  &  \\
 &  G &  M &  &  &  &  &  &  \\
 &  G &  M &  &  &  &  &  &  \\
\end{tabular}
}
\caption{\small{{
Effect of hidden dimension after channel mapper.}}
}
\label{tab:box_enc_ablate_channel_deformable}
\end{minipage}
\end{table}


\vspace{1mm}
\noindent\textbf{Channel mapper.}
To build an efficient box feature encoder, we propose to use a channel mapper as a simple  layer to decrease the channel dimension of  and apply the box region feature encoder on the updated feature map with smaller channels.


\vspace{1mm}
\noindent\textbf{Results.}
We compare the effect of box feature encoder architecture design in Table~\ref{tab:box_enc_ablate}. We notice significant computational cost increase for all settings, mainly due to two reasons: (i) the large number of bounding box region features, i.e.,  box predictions during inference; and (ii) the high-resolution of the bounding box feature map, i.e., . We study the impact of different resolutions of the region feature maps in the ablation experiments. We find similar conclusions as Table~\ref{tab:feat_enc_ablate}: all methods improve the performance, more encoder layers lead to better performance, and deformable encoder block performs the best. Therefore, we use deformable encoder blocks for the box feature encoder. To reduce the expensive computational cost, we use the channel mapper to decrease the hidden dimension. The results are in Table~\ref{tab:box_enc_ablate_channel_deformable}. We observe that lower hidden dimension reduces computational cost with little performance loss. Considering the trade-off between performance and overhead, we use  hidden dimensions for subsequent experiments.

\subsection{Query Feature Encoder}
After studying the influence of adding the image feature encoder and box region feature encoder, we further investigate how to design a suitable query feature encoder to refine the object queries originally designed for detecting the boxes for instance segmentation tasks.

\vspace{1mm}
\noindent\textbf{Object-to-object attention.}
The proximity of objects to each other may results in a situation where multiple instances lie within one bounding box. We add object-to-object attention to help object query representations distinguish instances. Specifically, we use multi-head self-attention mechanism to process the queries as follows:



\vspace{1mm}
\noindent\textbf{Box-to-object attention.}
In the frozen DETR-based object detector, the object queries are used to perform object detection and process the whole image feature instead of a box region feature. The discrepancies between the usage of object queries in the frozen detector and MaskHead may lead to sub-optimal segmentation results. Therefore, we introduce box-to-object attention to transform the queries and adapt them to the segmentation task as follows:

where the object queries are updated by refering to the information in the box region feature.

\vspace{1mm}
\noindent\textbf{FFN.}
The feed forward network (FFN) block is a widely employed element in Transformers. It is usually integrated after an attention layer to transform individual tokens. In Table \ref{tab:query_enc_ablate}, we investigate the efficacy of this block in adjusting object query representations for segmentation. 

\vspace{1mm}
\noindent\textbf{Results.}
Table~\ref{tab:query_enc_ablate} shows the comparison results on the effect of different modifications to the query feature encoder architecture design.
We find no performance improvement when only using FFN to enhance the object queries and a minor gain (+) when only using the box-to-object attention module. We think this is because the enhanced box features are already strong for instance segmentation, so using the original object query representation from the frozen detection model is enough. Moreover, using object-to-object attention reduces the performance, as the interaction between object queries might mix the semantic information of different objects. In general, we conclude that transforming the object queries is unnecessary and simply using the original ones to interact with the refined box region features achieves strong results.
We only adopt the box-to-object attention design in the qualitative analysis experiments.

\begin{figure}[t]
\centering
\includegraphics[width=0.5\textwidth]{img/FrozenDETR_imgenc_boxenc_queryenc.pdf}
\caption{\small{Add object query encoder to Frozen DETR.} We apply a query feature encoder to enhance the object query representations. This is the complete framework of our Mask Frozen-DETR.}
\label{fig:frozen_detr_pipeline}
\end{figure}


\begin{table}[t]
\begin{minipage}[t]{1\linewidth}
\vspace{2mm}
\centering
\setlength{\tabcolsep}{1.5pt}
\footnotesize
\renewcommand{\arraystretch}{1.2}
\resizebox{1.0\linewidth}{!}
{
\begin{tabular}{c|c|c|c|c|cccccc}
FFN & O2O & B2O  & \#FLOPs & \#params & AP & AP & AP & AP & AP & AP \\
\shline
\xmark & \xmark & \xmark  &  G &  M &  &  &  &  &  &  \\
\cmark & \xmark & \xmark  &  G &  M &  &  &  &  &  &  \\
\cmark & \cmark & \xmark  &  G &  M &  &  &  &  &  &  \\
\rowcolor{gray!10}\cmark  & \xmark & \cmark  &  G &  M &  &  &  &  &  &  \\
\end{tabular}
}
\caption{\small{{
Effect of each factor within the query feature encoder.}
O2O: object-to-object attention module.
B2O: box-to-object attention module.
}
}
\label{tab:query_enc_ablate}
\end{minipage}
\end{table}


\subsection{Other Improvements}

\vspace{1mm}
\noindent\textbf{Mask loss on sampled pixel points.}
Inspired by implicit PointRend~\cite{cheng2022pointly} that shows a segmentation model can be trained with its mask loss computed on  sampled points instead of the entire mask, we compute the mask loss with sampled points in the  the final loss calculation. Specifically, 
given number of points , oversample ratio  ( and importance sample ratio  (), we randomly sample  points from the output mask and select  most uncertain points from the sampled points. Then we randomly sample other  points from the output mask and compute loss only on these  points.

\begin{table}[t]
\begin{minipage}[t]{1\linewidth}
\vspace{2mm}
\centering
\setlength{\tabcolsep}{0.5pt}
\footnotesize
\renewcommand{\arraystretch}{1.2}
\resizebox{1.0\linewidth}{!}
{
\begin{tabular}{ccc|c|c|cccccc}
   neck & \thead{samp. pixel \\ sup.} & \thead{mask \\ scoring} & \#FLOPs & \#params & AP & AP & AP & AP & AP & AP  \\
    \shline
   \xmark & \xmark & \xmark &  G &  M &  &  &  &  &  &  \\
   \cmark & \xmark & \xmark &  G &  M &  &  &  &  &  &  \\
   \cmark & \cmark & \xmark &  G &  M &  &  &  &  &  &  \\ 
  \rowcolor{gray!10}\cmark & \cmark & \cmark &  G&  M &  &  &  &  &  &  \\
\end{tabular}
}
\caption{\small{{
Effect of other improvements including sampled pixel supervision, mask scoring, and neck design.}}
}
\label{tab:other_improvement_ablate}
\end{minipage}
\end{table}



\begin{table*}[t]
\begin{minipage}[t]{1\linewidth}
\centering\setlength{\tabcolsep}{5pt}
\footnotesize
\renewcommand{\arraystretch}{1.2}
\label{tab:coco_instance_exp}
\resizebox{\linewidth}{!}
{
\begin{tabular}{l|l|c|c|c|c|cccccc}
method & backbone & \#epochs  & Object & AP & AP & AP & AP & AP & AP & AP & GPU Hours \\
\shline
K-Net-N~\cite{zhang2021k} & R &  & \xmark &  &  &  &  &  &  &  & \\
QueryInst~\cite{FangQueryInst} & Swin-L &  & \xmark &  &  &  &  &  &  &  &  \\

MaskFormer~\cite{cheng2021masked} & R &  & \xmark &  &  &  &  &  &  &  &    \\
MaskFormer~\cite{cheng2021masked} & Swin-L &  & \xmark &  &  &  &  &  &  &  &    \\ 
Mask DINO~\cite{li2022mask} & R &  & \xmark &  &  &  &  &  &  &  &  \\
Mask DINO~\cite{li2022mask} & Swin-L &  & \xmark &  &  &  &  &  &  &  &  \\
\rowcolor{gray!8}Mask Frozen--DETR &R &  & \xmark&  &  &  &  &  &  &  &  \\
\rowcolor{gray!8}Mask Frozen--DETR & Swin-L &  & \xmark &  &  &  &  &  &  &  & \\
\hline
ViT-Adapter-L~\cite{chen2022vitadapter} & ViT-L &  & \cmark &  &  &  &  &  &  &  &  \\
Mask DINO~\cite{li2022mask} & Swin-L &  & \cmark &  &  &  &  &  &  &  &  \\
\rowcolor{gray!8}Mask Frozen--DETR &R &  & \cmark &  &  &  &  &  &  &  & \\
\rowcolor{gray!8}Mask Frozen--DETR & Swin-L &  & \cmark &  &  &  &  &  &  &  & \\
\rowcolor{gray!8}Mask Frozen-DINO-DETR & FocalNet-L &  & \cmark&  &  &  &  &  &  &  & \\
\end{tabular}
}
\caption{\small{Comparison with SOTA instance segmentation methods on COCO val.}
}
\label{coco_instance_exp}
\vspace{2mm}
\end{minipage}
\begin{minipage}[t]{1\linewidth}
\vspace{2mm}
\centering
\setlength{\tabcolsep}{7pt}
\footnotesize
\renewcommand{\arraystretch}{1.35}
\resizebox{1.0\linewidth}{!}
{
\begin{tabular}{l|l|c|c|c|cccccc}
method & backbone & \#epochs  & Object & AP & AP & AP & AP & AP & AP & AP \\
\shline
K-Net-N~\cite{zhang2021k} & R &   & \xmark & - &  &  &  &  &  &  \\
SOLQ~\cite{dong2021solq} & Swin-L &  & \xmark &  &  & - & - &  &  &  \\
SOIT~\cite{yu2022soit} & Swin-L &  & \xmark & &  &  &  &  &  &  \\
QueryInst~\cite{FangQueryInst} & Swin-L &  & \xmark &  &  &  &  &  &  &  \\
MaskFormer~\cite{cheng2021masked} & Swin-L &  & \xmark & - &  &  &  &  &  &  \\
Mask DINO~\cite{li2022mask} & Swin-L &  & \cmark & - &  & - & - & - & - & - \\
\rowcolor{gray!8}Mask Frozen-DINO-DETR & FocalNet-L &  & \cmark &  &  &  &  &  &  &  \\
\end{tabular}
}
\caption{\small{{
Comparison with SOTA instance segmentation methods on COCO test-dev.}}
}
\label{tab:coco_test}
\end{minipage}
\end{table*}


\vspace{1mm}
\noindent\textbf{Mask scoring.}
Since the classification scores predicted by the frozen DETR cannot reflect the quality of segmentation masks, we introduce mask scoring \cite{huang2019mask} to our method to adjust the score, making it able to describe the quality of segmentation masks more precisely. Specifically, the mask scoring head takes the output mask and box region features as input and uses them to predict the iou score between the output mask and ground truth following:

where ,  and  refer to concatenation, covolution layers and multi layer perceptron, respectively.
The iou score predicted by the mask scoring head is then used to adjust the classification score as following:

where  is the confidence score of the output mask.


\vspace{1mm}
\noindent\textbf{Neck for backbone feature.}
The feature map output by the first stage of backbone   may not contain sufficient semantic information for accurate instance segmentation. Therefore, we introduce a simple neck block to encode more semantic information into the high resolution feature map  . The neck block can be described using the following formula:

where  and  refer to group normalization and point-wise convolution, respectively.


\vspace{1mm}
\noindent\textbf{Results.}
In Table \ref{tab:other_improvement_ablate}, we attempt to further improve the results by integrating a neck block design, using sampled pixel supervision, and using mask scoring.
We observe that all three designs bring consistent gains in AP scores. For example, using the neck block improves AP from  to  and using mask scoring improves AP from  to . Notably, while sampled pixel supervision reduces the number of the points for training supervisions by , it still brings a slight gain in AP (+). Therefore, we use all three designs in the following experiments.


\section{Comparison with SOTA Systems}
To compare our system with state-of-the-art instance segmentation methods, we construct a series of strong Mask Frozen-DETR models based on different Frozen DETR-based detector weights. These include Mask Frozen--DETR + ResNet-, which uses -DETR + ResNet- with AP of ; Mask Frozen--DETR + Swin-L, which uses -DETR + Swin-L with AP of ; and Mask Frozen-DINO-DETR + Swin-L, which uses DINO-DETR + FocalNet-L with AP of .


Table~\ref{coco_instance_exp} presents the detailed comparison results on COCO \texttt{val} set. We can see that, with Object object detection pre-training, our approach surpasses the very recent state-of-the-art Mask-DINO by a clear margin (ours:  vs. Mask DINO: ). This result is remarkable considering that the strong object detector DINO-DETR achieves even better object detection performance, i.e., , than the DINO-DETR + FocalNet-L that we use. The most important advantage of our approach is the significantly reduced training time, e.g., we can complete the training of DINO-DETR + FocalNet-L within \textbf{ hours} while training a Mask DINO + Swin-L takes more than \textbf{ days} when using  V100 GPUs.


We also provide the detailed comparison results for Mask Frozen--DETR + ResNet- and Mask Frozen--DETR + Swin-L. In general, our approach achieves competitive results across various model sizes and different DETR-based frameworks.
We further compare Mask Frozen-DINO-DETR to the state-of-the-art methods in instance segmentation on COCO test-dev in Table \ref{tab:coco_test}. Frozen-DINO-DETR with FocalNet-L achieves an AP of . This result surpasses the recent state-of-the-art method, Mask DINO, by + AP.


\section{Ablation Experiments and Analysis}

\begin{table}[t]
\begin{minipage}[t]{1\linewidth}
\vspace{2mm}
\centering
\setlength{\tabcolsep}{3pt}
\footnotesize
\renewcommand{\arraystretch}{1.2}
\resizebox{1.0\linewidth}{!}
{
\begin{tabular}{l|c|c|cccccc}
   output size & \#FLOPs & \#params & AP & AP & AP & AP & AP & AP  \\
    \shline
    &  G &  M &  &  &  &  &  & \\ 
   \rowcolor{gray!10} &  G &  M &  &  &  &  &  &  \\
    &  G &  M &  &  &  &  &  &  \\
\end{tabular}
}
\caption{\small{{
Effect of RoIAlign output size.}}
}
\label{tab:roi_output_size}
\end{minipage}
\end{table}




\begin{table}[t]
\begin{minipage}[t]{1\linewidth}
\vspace{2mm}
\centering
\setlength{\tabcolsep}{9pt}
\footnotesize
\renewcommand{\arraystretch}{1.2}
\resizebox{1.0\linewidth}{!}
{
\begin{tabular}{c|cccccc}
   epoch  & AP & AP & AP & AP & AP & AP  \\
    \shline
   \rowcolor{gray!10} &  &  &  &  &  &  \\
     &  &  &  &  &  &  \\ 
\end{tabular}
}
\caption{\small{{
Effect of training epochs.}}
}
\label{tab:effect_training_epochs}
\end{minipage}
\end{table}




\begin{table}[t]
\begin{minipage}[t]{1\linewidth}
\vspace{2mm}
\centering
\setlength{\tabcolsep}{7pt}
\footnotesize
\renewcommand{\arraystretch}{1.2}
\resizebox{1.0\linewidth}{!}
{
\begin{tabular}{c|c|cccccc}
   LSJ   & GPU Hour& AP & AP & AP & AP & AP & AP  \\
    \shline
   \rowcolor{gray!10}\xmark &  &  &  &  &  &  &  \\
  \cmark  &  &  &  &  &  &  &  \\
\end{tabular}
}
\caption{\small{{
Effect of large scale jittering.}}
}
\label{tab:effect_lsj}
\end{minipage}
\end{table}




\vspace{1mm}
\noindent\textbf{RoIAlign output size.}
Table \ref{tab:roi_output_size} shows the the influence of RoIAlign output size. We observe that (i) Enlarging the RoIAlign output size significantly increases GFLOPs. (ii) Increasing RoIAlign output size can improve instance segmentation performance. Specifically, we observe a 0.9 improvement in AP by increasing the output size from  to . (iii) Instance segmentation performance saturates beyond an RoIAlign output size of . Taking into account the trade-off between performance and computational cost, we set RoIAlign output size as  by default.

\vspace{1mm}
\noindent\textbf{Training epochs.}
Table \ref{tab:effect_training_epochs} shows the effect of training epochs. We notice that doubling the number of training epochs only brings 0.3 gain in AP. This result suggests that our model achieves convergence promptly, which can effectively reduce the required training time.

\vspace{1mm}
\noindent\textbf{Large scale jittering.}
Table \ref{tab:effect_lsj} shows the effect of large scale jittering. We observe that using large scale jittering achieves a 0.3 AP improvement and increase the training GPU hours by 28.6\%. In light of the trade-off between training time and performance, we do not utilize large scale jittering in our following experiments.

\vspace{1mm}
\noindent\textbf{Instance mask head design.}
We compare the effect of mask head design in Table \ref{tab:effect_instance_mask_head}. Compared with segmenter head \cite{strudel2021segmenter} that contains linear projection and normalization, our simple dot product design achieves comparable AP scores with lower FLOPs and fewer number of parameters.


\begin{table}[t]
\begin{minipage}[t]{1\linewidth}
\vspace{2mm}
\centering
\setlength{\tabcolsep}{2pt}
\footnotesize
\renewcommand{\arraystretch}{1.3}
\resizebox{1.0\linewidth}{!}
{
\begin{tabular}{l|c|c|cccccc}
   mask head  & \#FLOPs & \#params & AP & AP & AP & AP & AP & AP  \\
    \shline
   \rowcolor{gray!10}dot product &  G &  M &  &  &  &  &  &  \\
   segmenter head  &  G &  M &  &  &  &  &  &  \\ 
\end{tabular}
}
\caption{\small{{
Effect of instance mask head design.}}
}
\label{tab:effect_instance_mask_head}
\end{minipage}
\end{table}


\begin{table}[t]
\begin{minipage}[t]{1\linewidth}
\centering
\setlength{\tabcolsep}{8pt}
\footnotesize
\renewcommand{\arraystretch}{1.3}
\resizebox{1.0\linewidth}{!}
{
\begin{tabular}{c|cccccc}
     batch size & AP & AP & AP & AP & AP & AP  \\
    \shline
    &  &  &  &  &  &  \\ 
    &  &  &  &  &  &  \\
    &  &  &  &  &  &  \\
\end{tabular}

}
\caption{\small{{
Effect of batch size.}}
}
\label{tab:effect_batch_size}
\end{minipage}
\end{table}

\begin{table}[t]
\begin{minipage}[t]{1\linewidth}
\centering
\setlength{\tabcolsep}{8pt}
\footnotesize
\renewcommand{\arraystretch}{1.2}
\resizebox{1.0\linewidth}{!}
{
\begin{tabular}{c|cccccc}
     layer\# & AP & AP & AP & AP & AP & AP  \\
    \shline
    &  &  &  &  &  &  \\ 
    &  &  &  &  &  &  \\
    &  &  &  &  &  &  \\
\end{tabular}

}
\caption{\small{{
Layer index of the encoder feature map .
}}
}
\label{tab:feat_fetch_layer}
\end{minipage}
\end{table}


\vspace{1mm}
\noindent\textbf{Batch size.}
Table \ref{tab:effect_batch_size} shows the effect of batch size. We observe that reducing the batch size from 8 to 4 or 2 even brings slightly improvements in performance (+0.2 AP). It is worth noting that our method can run on a single V100 GPU with 16G memory when the batch size is 2.
This highlights the effectiveness and computational resource efficiency of our approach.

\vspace{1mm}
\noindent\textbf{Layer index of the encoder feature map.}
We compare the encoder feature map  from different layers of the Transformer encoder in Table \ref{tab:feat_fetch_layer}. We notice that using the encoder feature map from shallower layers outperforms using that from the last layer of the encoder. We think this is because the feature map from the last layer of the encoder contains task-specific information relevant to detection, while feature maps from shallower layers contain more generalized object information that may facilitate segmentation. Therefore, we use the encoder feature map from layer\# by default.


\begin{table*}[h]
\begin{minipage}[t]{1\linewidth}
\vspace{2mm}
\centering
\setlength{\tabcolsep}{12pt}
\footnotesize
\renewcommand{\arraystretch}{1.35}
\resizebox{1.0\linewidth}{!}
{
\begin{tabular}{c|c|c|c|cccccc}
\# img. enc. layers & \# box enc. layers  & \#FLOPs & \#params & AP & AP & AP & AP & AP & AP \\
\shline
 &   &  G &  M &  &  &  &  &  &  \\
 &   &  G &  M &  &  &  &  &  &   \\
\rowcolor{gray!10} &   &  G &  M &  &  &  &  &  &   \\
 &  &  G &  M &  &  &  &  &  &  \\
 &  &  G &  M &  &  &  &  &  &  \\
\end{tabular}
}
\caption{\small{{
Effect of the depth of the image feature encoder and the box feature encoder on DINO + FocalNet-L.}}
}
\label{tab:feat_enc_depth_ablate}
\end{minipage}
\end{table*}


\begin{table}[t]
\begin{minipage}[t]{1\linewidth}
\vspace{-3mm}
\centering
\setlength{\tabcolsep}{0.25pt}
\footnotesize
\renewcommand{\arraystretch}{1.2}
\resizebox{1.0\linewidth}{!}
{
\begin{tabular}{l|ccc|cc|c|c}
detector method & \thead{image feat.\\ enc.} & \thead{box feat.\\ enc.} & \thead{query feat. \\ enc.} & partial finetune &  finetune & AP & GPU Hours \\
\shline
\multirow{5}{*}{-DETR+R50} & \cmark & \cmark & \cmark & \xmark & \xmark & 45.7 & 49 \\ 
& \xmark & \xmark & \xmark & \cmark & \xmark &  43.8 & 92 \\
&\xmark & \xmark & \xmark & \xmark & \cmark & 43.9 & 99 \\
&\cmark & \cmark & \cmark & \cmark & \xmark & 45.6 & 100 \\
&\cmark & \cmark & \cmark & \xmark & \cmark & 46.0 & 108\\
\hline
\multirow{3}{*}{-DETR+Swin-L}  & \cmark & \cmark & \cmark & \xmark & \xmark & 54.0 & 172\\
& \cmark & \cmark & \cmark & \cmark & \xmark & 54.1 & 406\\
& \cmark & \cmark & \cmark & \xmark & \cmark & 54.1 & 532\\\hline
\multirow{3}{*}{DINO-DETR+FocalNet}  & \cmark & \cmark & \cmark & \xmark & \xmark & 54.9 & 136\\
& \cmark & \cmark & \cmark & \cmark & \xmark & 54.9 & 218 \\
& \cmark & \cmark & \cmark & \xmark & \cmark & 55.0 & 319\\
\end{tabular}
}
\caption{\footnotesize{
Effect of fine-tuning the whole DETR (fine-tune) or only the transformer encoder \& decoder within DETR (partial fine-tune). During fine-tuning, we set the learning rate of the original DETR parameters as  of the ones of the additional new parameters.}}
\label{tab:finetune_detr}
\end{minipage}
\end{table}



\vspace{1mm}
\noindent\textbf{Effect of the depth of the image feature encoder and the box feature encoder}
Table \ref{tab:feat_enc_depth_ablate} shows the influence of the depth of image feature encoder and the box feature encoder on Mask Frozen-DINO-DETR with FocalNet-L as backbone. We observe that: (i) The instance segmentation performance of Frozen-DINO-DETR reaches saturation when the depth of the box feature encoder is 2, and further increasing its depth does not result in a performance gain. (ii) Increasing the depth of image feature encoder from 2 to 3 leads to a 0.1 increase in AP. Nevertheless, the performance saturates when the depth of image feature encoder is 3. Given these findings, we select the depth of the image feature encoder to be 3 and the depth of the box feature encoder to be 2 for the comparisons with state-of-the-art instance segmentation methods on the COCO test-dev.

\vspace{1mm}
\noindent\textbf{Effect of fine-tuning DETR}:
We further ablate the effect of fine-tuning the DETR-based object detector either entirely or partially, as outlined in Table~\ref{tab:finetune_detr}.
Accordingly,
we observe that (i) fine-tuning DETR brings consistent, albeit marginal, gains while significantly increasing the overall training GPU hours; (ii) our method achieves the best trade-off between performance and training cost.

\vspace{1mm}
\noindent\textbf{Qualitative results.}
Figure~\ref{fig:box2q_attn} shows the instance segmentation probability maps based on our approach. We notice that the probability maps precisely capture the object boundaries, which support the strong performance of our approach on instance segmentation tasks.


\begin{figure}[t]
\centering
\includegraphics[width=0.5\textwidth]{img/mask_heat_out.pdf}
\caption{\small{Visualizing the segmentation probability maps of our approach.}}
\label{fig:box2q_attn}
\vspace{-3mm}
\end{figure}

\section{Conclusion}
In this work, we have presented the detailed techniques for converting an existing off-the-shelf DETR-based object detector into a strong instance segmentation model with minimal training time and resources. Our approach is remarkably simple yet effective. 
We verify the effectiveness of our approach by reporting state-of-the-art instance segmentation results while accelerating the training by more than  times.
We believe our simple approach can inspire more research on advancing the state-of-the-art in instance segmentation model design.


{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}


\end{document}
