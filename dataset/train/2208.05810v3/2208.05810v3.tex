

\section{Experiments}\label{sec:experiments}
This section presents the effectiveness of the proposed sequencel-level training using four baseline trackers, SiamRPN++~\cite{siamrpnpp}, SiamAttn~\cite{siamattn}, TransT~\cite{transt}, and TrDiMP~\cite{trdimp}, 
on three standard benchmarks, LaSOT~\cite{lasot}, TrackingNet~\cite{trackingnet}, and GOT-10k~\cite{got10k}.


\subsection{Implementation Details}
As widely adopted in deep RL~\cite{deepqlearning,scst,adnet}, we pre-train the trackers with supervised learning, which is done with frame-level training in our case, to stabilize and speed up the subsequent SLT. 
For each training iteration,  tracking episodes are randomly sampled from training datasets, where  is set to 8 for SiamRPN++, TransT, and TrDiMP and 12 for SiamAttn, respectively.
Each episode is composed of  video frames and a single template frame and  is a hyper-parameter for training.
For frame-interval augmentation, the interval is randomly chosen every time sampling the video frames and its maximum is set to 7 for SiamRPN++ and SiamAttn, and 10 for TransT and TrDiMP, respectively.
We use the average overlap (AO) score for the reward function  in Eq.~\ref{eq:slt_loss_siamrpnpp}.



Many recent trackers have post-processing strategies based on geometric priors~\cite{siamrpnpp,siamattn,dimp,trdimp,transt}.
For example, SiamRPN++ has the cosine-window penalty and the shape penalty, which prevent drastic updates in target bounding box estimation.
These penalties are typically not applied during frame-level training, which also brings inconsistency between training and testing.
However, our sequence-level training also resolves this inconsistency problem. 
Note that  in Eq.~\ref{eq:sampling_prob} is the anchor score after post-processing.


The argmax and sampling trackers in our SLT framework share all weights for training, and our tracker behaves like the argmax tracker during inference.
Thus, the additional memory cost is marginal in training, and the test-time efficiency of the original tracker is not affected by SLT.
Our algorithm is implemented in Python using PyTorch with NVIDIA RTX A6000 2GPUs.


\subsection{Training Dataset}

For a fair comparison, we aligned the pre-training datasets for the baseline with the fine-tuning datasets for SLT as similar as possible.
1) For SiamRPN++, we adopt LaSOT, TrackingNet, and GOT-10k for both pre-training and fine-tuning.
2) Following the original paper, SiamAttn is trained on LaSOT, TrackingNet, COCO~\cite{coco}, and YouTube-VOS~\cite{ytvos}.
Since COCO is an image dataset, a data augmentation scheme such as shift, scale, and blur is adopted to extend the image to compose an episode.
Note that the data augmentation strategy except for frame-interval augmentation is used only for COCO.
3) Finally, TransT and TrDiMP are both pre-trained using  LaSOT, TrackingNet, GOT-10k, and COCO, as same as the original papers, and then fine-tuned on three video datasets, LaSOT, TrackingNet, and GOT-10k.







\subsection{Evaluation}

We compare the performance of SLT with four baseline trackers. Note that for a fair comparison, we strictly maintain the same test-time hyper-parameters for each method for all datasets.
Only TrDiMP uses a different hyper-parameter setting for evaluation in LaSOT following the baseline paper~\cite{trdimp}.


\begin{table*}[t]
\centering
\caption{
Performance of sequence-level training on LaSOT, TrackingNet, and GOT-10k.
}
\label{table:eval_base_slt}
\scalebox{0.83}{
\setlength{\tabcolsep}{6pt}
\begin{tabular}{cc|cc|ccc|ccc}
\Xhline{2\arrayrulewidth}
\multicolumn{2}{c|}{\multirow{2}{*}{Method}} & \multicolumn{2}{c|}{LaSOT} & \multicolumn{3}{c|}{TrackingNet} & \multicolumn{3}{c}{GOT-10k}\\  
   &        & AUC () &  & AUC () &  & P & AO () & & \\ \hline
\multirow{2}{*}{SiamRPN++} & Base & 51.0 & 60.3 & 68.2 & 78.3 & 68.9  & 49.5 & 58.0 & 30.5\\
                           & +SLT & 58.4 (+7.4) & 66.6 & 75.8 (+7.6) & 81.0 & 71.3 & 62.1 (+12.6) & 74.9 & 49.0\\ 
                          \hline
\multirow{2}{*}{SiamAttn}  & Base  & 54.8 & 63.5 & 74.3 & 80.9 & 70.6 & 53.4 & 61.8 & 36.4\\
                           & +SLT & 57.4 (+2.6) & 66.2 & 76.9 (+2.6) & 82.3 & 72.6 & 62.5 (+9.1) & 75.4 & 50.2\\ 
                          \hline
\multirow{2}{*}{TrDiMP}    & Base & 63.3 & 72.3 & 78.1 & 83.3 & 73.1 & 67.1 & 77.4 & 58.5\\
                           & +SLT & 64.4 (+1.1) & 73.5 & 78.1 (+0.0) & 83.1 & 73.1 & 67.5 (+0.4) & 78.8 & 58.7 \\ 
                          \hline
\multirow{2}{*}{TransT}    & Base & 64.2 & 73.7 & 81.1 & 86.8 & 80.1 & 66.2 & 75.5 & 58.7\\
                           & +SLT & 66.8 (+2.6) & 75.5 & 82.8 (+1.7) & 87.5 & 81.4 & 67.5 (+1.3) & 76.5 & 60.3 \\ 
\Xhline{2\arrayrulewidth}
\end{tabular}
}
\end{table*}


\subsubsection{LaSOT \cite{lasot}} is a recently published dataset that consists of 1,400 videos with more than 3.5M frames in total.
This benchmark is widely used to measure the long-term capability of trackers. 
The average video length of LaSOT is more than 2,500 frames, and each sequence comprises various challenging attributes.
The one-pass evaluation (OPE) protocol is used to measure the normalized precision () and the area under curve (AUC) of the success plot.
Table \ref{table:eval_base_slt} shows that the proposed method consistently improves all baseline trackers.


\subsubsection{TrackingNet~\cite{trackingnet}} is a large-scale dataset that provides 30K videos in training split and 511 videos in test split.
We evaluate our trackers on the test split of TrackingNet through the evaluation server.
Table \ref{table:eval_base_slt} shows that our SLT improves the AUC score by 7.6\%p, 2.6\%p, and 1.7\%p for SiamRPN++, SiamAttn, and TransT, respectively. It is noteworthy that the simple convolutional tracker SiamRPN++ shows competitive performance with SiamAttn with the power of SLT.



\subsubsection{GOT-10k~\cite{got10k}}
is a large-scale dataset that contains 10k sequences for training and 180 videos for testing.
For evaluation metrics, the average overlap (AO) and the success rate (SR) at overlap thresholds 0.5 and 0.75 are adopted.
Following the evaluation protocol of GOT-10k, we retrain our models using only the GOT-10k train split and submit the tracking results to the evaluation server.
Since the GOT-10k benchmark does not provide mask annotations, SLT-SiamAttn is trained without the mask branch.
Table \ref{table:eval_base_slt} shows that our SLT successfully improves all the baseline trackers in all evaluation metrics.
Baseline models are reproduced using only the GOT-10k train split.



\begin{table*}[t]
\centering
\caption{
Comparison with the state-of-the-art trackers on LaSOT.
}
\label{table:comparison_lasot}
\scalebox{0.75}{
\setlength{\tabcolsep}{2pt}
\begin{tabular}{c|cccccccccccc}
\Xhline{2\arrayrulewidth} 
\multirow{2}{*}{} & PACNet & Ocean & DiMP50 & PrDiMP50 & TransT & STARK- & STARK- & \textbf{SLT-} & \textbf{SLT-} & \textbf{SLT-} & \textbf{SLT-} \\
 & \cite{pacnet} & \cite{ocean} & \cite{dimp} & \cite{prdimp} & \cite{transt} & ST50 \cite{stark} & ST101 \cite{stark} & \textbf{SiamRPN++} & \textbf{SiamAttn} & \textbf{TrDiMP} & \textbf{TransT} \\ 
\hline
AUC (\%) & 55.3 & 56.0 & 56.9  & 59.8 & 64.2 & 66.4 & \textbf{67.1} & 58.4 & 57.4 & 64.4 & 66.8\\
 (\%) & 62.8 & 65.1 & 64.3 & 68.0 & 73.7 & 76.3 & \textbf{77.0} & 66.6 & 66.2 & 73.5 & 75.5\\
\Xhline{2\arrayrulewidth}
\end{tabular}
}
\setlength{\abovecaptionskip}{10pt}
\setlength{\belowcaptionskip}{0pt}
\caption{
Comparison with the state-of-the-art trackers on TrackingNet.
}
\label{table:comparison_trackingnet}
\scalebox{0.73}{
\setlength{\tabcolsep}{2pt}
\begin{tabular}{c|cccccccccccc}
\Xhline{2\arrayrulewidth} 
\multirow{2}{*}{}  & DiMP50 & SiamFC++ & MAML & PrDiMP50 & TransT & STARK- & STARK- & \textbf{SLT-} & \textbf{SLT-} & \textbf{SLT-} & \textbf{SLT-}\\
 & \cite{dimp} & \cite{siamfcpp} & \cite{mamltrack} & \cite{prdimp}  & \cite{transt} & ST50 \cite{stark} & ST101 \cite{stark} & \textbf{SiamRPN++} &  \textbf{SiamAttn} & \textbf{TrDiMP} & \textbf{TransT}\\ 
\hline
AUC (\%) & 74.0 & 75.4 & 75.7 & 75.8 & 81.1  & 81.3 & 82.0 & 75.8 & 76.9 & 78.1 & \textbf{82.8} \\
 (\%) & 80.1 & 80.0 & 82.2 & 81.6 & 86.8 & 86.1 & 86.9 & 81.0 & 82.3 & 83.1 &\textbf{87.5}\\
\Xhline{2\arrayrulewidth}
\end{tabular}
}
\centering
\caption{
Comparison with the state-of-the-art trackers on GOT-10k. `Add. data' denotes that trackers are trained using additional training datasets other than GOT-10k.
}
\label{table:comparison_got10k}
\scalebox{0.71}{
\setlength{\tabcolsep}{2pt}
\begin{tabular}{c|c|ccccccccccc}
\Xhline{2\arrayrulewidth} 
\multirow{2}{*}{} & Add. & SiamFC++ & DiMP50 & Ocean & PrDiMP50 & TransT & TrDiMP & STARK- & \textbf{SLT-} & \textbf{SLT-} & \textbf{SLT-} & \textbf{SLT-}  \\
& data & \cite{siamfcpp} & \cite{dimp} & \cite{ocean} & \cite{prdimp} & \cite{transt} & \cite{trdimp} & ST50 \cite{stark} & \textbf{SiamRPN++} & \textbf{SiamAttn} & \textbf{TrDiMP} & \textbf{TransT} \\ 
\hline
AO (\%) & \multirow{3}{*}{-} & 59.5 & 61.1 & 61.1 & 63.4 & 66.2 & 67.1 & \textbf{68.0} & 62.1 & 62.5 & 67.5 & 67.5  \\
 (\%) &  & 69.5 & 71.7 & 72.1 & 73.8 & 75.5 & 77.4 & 77.7 & 74.9 & 75.4 & \textbf{78.8} & 76.5  \\
 (\%) &  & 47.9 & 49.2 & 47.3 & 54.3 & 58.7 & 58.5 & \textbf{62.3} & 49.0 & 50.2 & 58.7 & 60.3 \\ \hline
AO (\%) & \checkmark  & - & 60.4 & - & 65.2 & 71.9 & 68.6 & 71.5 & 56.9 & 62.8 & 69.0 & \textbf{72.5}  \\
\Xhline{2\arrayrulewidth}
\end{tabular}
}
\end{table*}



\subsubsection{Comparison with SOTA trackers.}
We compare the performance of the proposed SLT family with the other state-of-the-art trackers on LaSOT and TrackingNet as shown in Table~\ref{table:comparison_lasot} and \ref{table:comparison_trackingnet}.
When compared to the recently proposed RL-based tracker PACNet~\cite{pacnet}, all four SLT trackers are showing superior performance by a large margin.
SLT-TransT, which is our best model, achieves state-of-the-art performance in both benchmarks.
Note that STARK-ST101~\cite{stark} uses deeper backbone (ResNet101) than TransT, which use ResNet50 backbone.
SLT-TransT thus needs to be compared with STARK-ST50 for fairness.
Table~\ref{table:comparison_got10k} also shows that both SLT-TrDiMP and SLT-TransT achieve comparable performance with state-of-the-art trackers on GOT-10k.



\subsection{Analysis}\label{sec:experiment_analysis}
We also analyze the effects of SLT using SiamRPN++ as the base tracker. The experimental analyses in this subsection are done on the \textit{validation} split of GOT-10k and the \textit{test} splits of LaSOT and TrackingNet.



\subsubsection{Sequence-level training components.}
The benefit of SLT comes from sequence-level sampling, sequence-level objective, and sequence-level data augmentation.
We validate the components of SLT by measuring the accuracy gains on the three benchmarks (Table~\ref{table:ssvsso}) and performing an attribute-based analysis on LaSOT (Fig.~\ref{fig:small_attr}).

\textit{Sequence-level sampling (SS).}
To measure the net effect of SS, we train a tracker with SS but use the frame-level objective.
As shown at `+SS' in Table~\ref{table:ssvsso}, by learning more accurate input data distribution, the tracker with SS outperforms the baseline with frame-level sampling by 4.1\%p, 5.3\%p, and 3.8\%p, respectively, on the three benchmarks. SS allows the tracker to observe realistic appearance variations of target objects during tracking, making itself more robust to variations of aspect ratio, scale, rotation, and illumination (ARC, SC, R, IV) as seen in Fig.~\ref{fig:small_attr}.



\begin{table*}[t]
\centering
\caption{
Effect of sequence-level training components.
}
\label{table:ssvsso}
\scalebox{0.85}{
\setlength{\tabcolsep}{5pt}
\begin{tabular}{c|cccc}
\Xhline{2\arrayrulewidth}
\multirow{2}{*}{Benchmark} & \multicolumn{4}{c}{SiamRPN++} \\  
 & Baseline & +SS () & +SS+SO () & +SS+SO+SA () \\ \hline
 LaSOT (AUC) & 51.0 & 55.1 (+4.1)& 57.3 (+6.3) & 58.4 (+7.4) \\
 TrackingNet (AUC) & 68.2 & 73.5 (+5.3) & 75.0 (+6.8) & 75.8 (+7.6) \\
 GOT-10k (AO)   & 66.4 & 70.2 (+3.8) & 73.8 (+7.4) & 74.3 (+7.9) \\
\Xhline{2\arrayrulewidth}
\end{tabular}
}
\end{table*}


\begin{figure}[t]
\centering
\includegraphics[width=1.0\linewidth]{./images/lasot_attr_plot.pdf}
\caption{
Benefits of sequence-level training components to individual attributes on the LaSOT dataset.
The baseline tracker is SiamRPN++, and the y-axis is performance (AUC) gain compared with the baseline tracker.
}
\label{fig:small_attr}
\end{figure}


\textit{Sequence-level objective (SO).}
As shown at `+SS+SO' in Table~\ref{table:ssvsso}, SO additionally improves the performance by 2.2\%p, 1.5\%p, and 3.6\%p on three benchmarks, respectively.
SO enables the tracker to reflect accumulated localization errors, preventing it from losing the target in challenging situations such as full occlusion, background clutters, and motion blur (FO, BC, MB) as seen in Fig.~\ref{fig:small_attr}.

\textit{Sequence-level augmentation (SA).}
As shown at `+SS+SO+SA' in Table~\ref{table:ssvsso}, SA further improves the performance by 1.1\%p, 0.8\%p, and 0.5\%p, respectively, resulting in the significant gain of SLT in total as 7.4\%p, 7.6\%p, and 7.9\%p. The effectiveness of SA is also evident from the improvement in the overall attributes in Fig.~\ref{fig:small_attr}.

To show that the frame-interval augmentation strategy of SA also potentially helps in adapting to videos with diverse frame rates, we set up tracking scenarios with lower frame rates (\ie, faster motion).
In the evaluation protocol, we track objects  every th frame only, skipping all the other frames; 
when the interval is 1, the evaluation protocol is the same as the original benchmark.
Table~\ref{table:frame_interval} shows that the frame-interval augmentation strategy not only improves the performance in normal videos, but also makes the tracker more robust to videos with lower frame rates.

\begin{table*}[t]
\centering
\caption{
Effect of sequence-level augmentation (SA) in terms of video frame interval with the low frame rate protocol. The frame interval is denoted by .
}
\label{table:frame_interval}
\scalebox{0.85}{
\setlength{\tabcolsep}{6pt}
\begin{tabular}{c|c|ccc|cccc}
\Xhline{2\arrayrulewidth}
\multirow{2}{*}{Method} & \multirow{2}{*}{SA} & \multicolumn{3}{c|}{GOT-10k (AO)} & \multicolumn{4}{c}{LaSOT (AUC)} \\  
   &         &  & &&  &  &  &   \\ \hline
SiamRPN++       & -          & 66.4 & 63.1 & 60.8 & 51.0 & 50.0 & 50.2 & 48.8\\
SLT-SiamRPN++   & -          & 73.8 & 67.9 & 65.5 & 57.3 & 55.1 & 54.1 & 52.6\\
SLT-SiamRPN++   & \checkmark & 74.3 & 70.8 & 67.8 & 58.4 & 56.9 & 56.2 & 54.6\\
\Xhline{2\arrayrulewidth}
\end{tabular}
}
\setlength{\abovecaptionskip}{10pt}
\setlength{\belowcaptionskip}{0pt}
\centering
\caption{
Effect of training sequence length.
}
\label{table:seq_len}
\scalebox{0.85}{
\setlength{\tabcolsep}{8pt}
\begin{tabular}{c|cccccc}
\Xhline{2\arrayrulewidth}
\multirow{2}{*}{Benchmark} & \multicolumn{6}{c}{Training sequence length ()} \\ 
 & 1 & 4 & 8 & 16 & 24 & 32 \\ \hline
GOT-10k (AO)   & 65.8 & 69.6 & 70.1 & 73.0 & \textbf{73.8} & 73.4\\
\Xhline{2\arrayrulewidth}
\end{tabular}
}


\centering
\caption{
Effect of frame-level pre-training. The zero (0) epoch stands for random initialization.
}
\label{table:pretrain_flt}
\scalebox{0.85}{
\setlength{\tabcolsep}{8pt}
\begin{tabular}{c|ccccc}
\Xhline{2\arrayrulewidth}
\multirow{2}{*}{Method} & \multicolumn{5}{c}{Frame-level pre-training (epoch)} \\ 
 & 0 & 1 & 5 & 10 & 20  \\ \hline
SiamRPN++       & - & 62.0 & 64.2 & 64.9 & 66.4\\ 
SLT-SiamRPN++   & 60.3 & 68.3 & 70.6 & 72.1 & 74.3\\ 
\Xhline{2\arrayrulewidth}
\end{tabular}
}
\end{table*}



\subsubsection{Length of training episodes.}
In training time, we randomly sample training episodes of pre-defined sequence length .
Learning temporal dependency may be affected by the length of training sequences.
We thus experiment with varying  while fixing the sampled frame interval to 1.
The best result is obtained with , as can be seen in Table~\ref{table:seq_len}.
When , the performance does not improve over the pre-trained tracker. 

\subsubsection{Frame-level pre-training.}
In our experiments, we perform SLT from a model pre-trained by frame-level training (FLT).
We analyze the effect of  warm-up FLT in Table~\ref{table:pretrain_flt}, showing that it improves tracking performance indeed, and FLT with only a few epochs is sufficient for the warm-up.
We also observed that the gain from SLT easily disappears by another few epochs of FLT (\ie, FLT  SLT  FLT).
In particular, the AO score of SiamRPN++ on the GOT-10k validation split reverted from 74.3\% to 66.2\% after 5 epochs of FLT.
This indicates that SLT grants a unique gain, which FLT cannot provide.


For additional analysis and qualitative results, see the supplementary material.
