

\section{Results}
In this section, we evaluate multilingual fine-tuning  for the baselines for the tasks enumerated above. 

\subsection{Code Summarization}
\label{csumr}


We apply multilingual fine-tuning on the \cxglue dataset. We first \emph{replicate} the summarization task by (monolingually) 
fine-tuning the available pre-trained \cbert 
model  for six languages\footnote{We use the publicly available \cbert implementation and dataset, \url{https://github.com/microsoft/CodeXGLUE/tree/main/Code-Text/code-to-text}}. 
We replicate the fine-tuning stage for 2 reasons: 

\begin{enumerate}
\item We want to account for any hardware or environmental bias (\eg we have a different set of GPUs than the original paper. We fine-tune with NVIDIA TITAN RTX, while Feng \etal~\cite{feng2020codebert} use NVIDIA Tesla V100). 
\item We use a pairwise two-sample statistical test (as described in~\cite{roy2021reassessing}, it is more precise than just comparing test-set summary statistics) to gauge differences. 
This requires a performance measurement for each test sample, which the repository did not include.
\end{enumerate}  
Our BLEU-4 numbers  for monolingual training
were close to reported numbers, with some differences; but we do obtain the same overall score (17.83) (\cref{tbl:csum}, leftmost 2 columns). 

We use the same, per-language test sets to compare  monolingual and multilingual fine-tuning. 
The validation set, however, is a single multilingual one combining all the monolingual validation sets. 
\Cref{tbl:csum} shows that multilingual fine-tuning improves performance, even for high-resource languages (with more than 100K training instances). 
With \cbert, multilingual fine-tuning gains 2.5\%-17.5\% over monolingual fine-tuning, 
for all languages,  yielding a  6.90\% overall improvement (4.48\% weighted improvement)\footnote{The \cbert paper simply averages the BLEU across languages to report the ``overall'' number; our \emph{weighted average}  weights each BLEU by the number of samples in that language.}. 
 With the more advanced \gcbert, we  see  smaller gains, although the relative gains span a wide range. 




\begin{table*}[h]


\centering

\resizebox{\textwidth}{!}{\renewcommand{\arraystretch}{1.2}

\begin{tabular}{lccccccccccc}

\hline
\multicolumn{1}{c}{Language}                                                            & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\cbert 			\\ (reported)\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\cbert 			\\ (re-trained)\end{tabular}} & \multicolumn{1}{c}{\mlcbert } & \multicolumn{1}{c}{Improvement} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Effect 			\\ Size\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}p-value 			\\ (adjusted)\end{tabular}} & \multicolumn{1}{c}{\gcbert} & \multicolumn{1}{c}{\mlgcbert} & \multicolumn{1}{c}{Improvement} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Effect 			\\ Size\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}p-value 			\\ (adjusted)\end{tabular}} \\ \hline
Ruby                                                                                      & 12.16                                                                                  & 12.53                                                                                    & 14.75                                  & +17.72\%                         & 0.055 & \textless{}0.001                                                                      & 12.62                       & \textbf{14.95}                                     & +18.46\%                         & 0.055                                                                          & \textless{}0.001                                                                      \\
JS                                                                                        & 14.90                                                                                  & 13.86                                                                                    & \textbf{15.80                                 } & +14.00\%                         & 0.016 & \textless{}0.001                                                                      & 14.79                       & 15.79                                     & +6.76\%                          & 0.016                                                                          & 0.014                                                                                 \\
Java                                                                                      & 17.65                                                                                  & 18.72                                                                                    & \textbf{20.11                                 } & +7.43\%                          & 0.016 & \textless{}0.001                                                                      & 19.22                       & 19.91                                     & +3.59\%                          & 0.016                                                                          & \textless{}0.001                                                                      \\
Go                                                                                        & 18.07                                                                                  & 18.15                                                                                    & 18.77                                  & +3.42\%                          & 0.010 & \textless{}0.001                                                                      & 18.40                       & \textbf{18.92 }                                    & +2.83\%                          & 0.010                                                                          & \textless{}0.001                                                                      \\
PHP                                                                                       & 25.16                                                                                  & 25.48                                                                                    & \textbf{26.23                                 } & +2.94\%                          & 0.012 & \textless{}0.001                                                                      & 25.45                       & 26.15                                     & +2.75\%                          & 0.012                                                                          & \textless{}0.001                                                                      \\
Python                                                                                    & \textbf{19.06                                                                                 } & 18.25                                                                                    & 18.71                                  & +2.52\%                          & 0.022 & \textless{}0.001                                                                      & 18.02                       & 18.90                                     & +4.88\%                          & 0.022                                                                          & \textless{}0.001                                                                      \\ \hline
\multicolumn{1}{l}{Overall}                                                      & \multicolumn{1}{c}{17.83}                                                             & \multicolumn{1}{c}{17.83}                                                               & \multicolumn{1}{c}{19.06}             & \multicolumn{1}{c}{+6.90\%}     & \multicolumn{1}{c}{\multirow{2}{*}{0.016}}                                & \multicolumn{1}{c}{\multirow{2}{*}{\textless{}0.001}}                                & \multicolumn{1}{c}{18.08}  & \multicolumn{1}{c}{\textbf{19.10}}                & \multicolumn{1}{c}{+5.64\%}     & \multicolumn{1}{c}{\multirow{2}{*}{0.016}}                                    & \multicolumn{1}{c}{\multirow{2}{*}{\textless{}0.001}}                                \\ \multicolumn{1}{l}{\begin{tabular}[c]{@{}l@{}}Overall\\ (weighted)\end{tabular}} & \multicolumn{1}{l}{\begin{tabular}[c]{@{}l@{}}Not\\ Reported\end{tabular}}            & \multicolumn{1}{c}{19.85}                                                               & \multicolumn{1}{c}{20.74}             & \multicolumn{1}{c}{+4.48\%}     & \multicolumn{1}{c}{}                                                          & \multicolumn{1}{c}{}                                                                 & \multicolumn{1}{c}{19.98}  & \multicolumn{1}{c}{\textbf{20.76}}                & \multicolumn{1}{c}{+3.90\%}     & \multicolumn{1}{l}{}                                                          & \multicolumn{1}{c}{}                                                                 \\ \hline
\multicolumn{12}{l}{*Evaluation criteria followed by \cxglue~\cite{DBLP:journals/corr/abs-2102-04664} and \cbert~\citep{feng2020codebert}}  
\end{tabular}

}
\vspace{0.05in}
\caption{{\em {\small Effectiveness of multi-lingual fine-tuning for code summarization task. Note that p-values are B-H corrected}}}
\vspace{-0.2in}
\label{tbl:csum}
\end{table*}




We use a one-sided (AH: monolingual < multilingual) \underline{pairwise} Wilcoxon signed-rank test
(thus avoiding the \underline{corpus-level} measurement pitfalls noted in~\cite{roy2021reassessing}). 
Null hypothesis is rejected for all six languages, for \cbert. For \gcbert, it's rejected overall, and for every language;
except for Javascript, where the p-value is 0.014 (all after B-H correction). 

Thus our measurement indicates that multilingual fine-tuning 
provides a statistically significant improvement over monolingual training. We find rather low  effect sizes using Cliff's Delta~\cite{macbeth2011cliff}. 
While we report the effect size for the sake of completeness, this is not a major concern: 
we note that \emph{all gains are statistically highly significant}. 
We also emphasize that even the minor 
improvements provided here by multilingual training (which is broadly compatible with a range of settings) constitute a relevant and potentially widely useful result. 
Roy \emph{et al}~\cite{roy2021reassessing} have previously noted that small gains in BLEU-4 may not be perceptible to humans as
increased text quality; nevertheless, we note that  natural language translation (which is now widely used) attained
 high performance levels based on decades of incremental progress; this result and others below
provide evidence that  multilingual training could be an important step in the progress towards more useful automated
tools.  Finally, we note 
 that BLEU-4 gains are higher for low-resource language (\eg 17.7\% for Ruby), and lower for high-resource languages (\eg 2.5\% for Python), as expected. 




\noindent{\underline{\em Comparing multi-lingual \cbert with other models}}
Code summarization is widely studied---there are many models for this task; our specific focus 
here is to understand if multilingual fine-tuning provides benefits, using a high-quality token-sequence model and dataset. 
So we focus comparisons on the papers which report performance on \cxglue dataset, and use a token-sequence inductive bias:
comparing against all models is beyond the scope of this paper. 
We compare multi-lingual \cbert (\mlcbert) and \gcbert (\mlgcbert) with other models that  have been published in peer-reviewed venues; 
among them, four apply pre-training strategies~\cite{liu2019roberta,feng2020codebert,ahmad-etal-2021-unified,qi2021prophetnet}. 
We achieve the best overall performance (table~\ref{comp1}), outperforming all the models, and  for four specific languages (\ie Ruby, Java, Go and PHP). 


There is one other system, CoTexT~\cite{phan2021cotext}  which claims (in an unpublished, non-peer-reviewed report) 
 better performance than us for just Python~\cite{phan2021cotext}, but is worse overall. We will include it for
comparison once it is published in a peer-reviewed venue.

This table also provides evidence supporting the effectiveness of multilingual fine-tuning.   







\begin{table}[b]

\centering
\resizebox{\columnwidth}{!}{\renewcommand{\arraystretch}{1.2}\begin{tabular}{lccccccc}
\hline
\multicolumn{1}{c}{Models}  & \multicolumn{1}{c}{Overall} & \multicolumn{1}{c}{Ruby} & \multicolumn{1}{c}{JavaScript}    & \multicolumn{1}{c}{Go}    & \multicolumn{1}{c}{Python} & \multicolumn{1}{c}{Java}  & \multicolumn{1}{c}{PHP}   \\ \hline

\mlgcbert           & \textbf{19.10}                        & \textbf{14.95}                     & 15.79 & \textbf{18.92 }                     & 18.90                       & 19.91 & 26.15\\

\mlcbert           & 19.06                        & 14.75                     & 15.80                     & 18.77                      & 18.71                       & \textbf{20.11                     } & \textbf{26.23}                      \\
ProphetNet-X~\cite{qi2021prophetnet}                  & 18.54                        & 14.37                     & \textbf{16.60                      } & 18.43                      & 17.87                       & 19.39                      & 24.57                      \\
PLBART~\cite{ahmad-etal-2021-unified}                        & 18.32                        & 14.11                     & 15.56                      & 18.91                      & \textbf{19.30}                       & 18.45                      & 23.58                      \\

\gcbert~\cite{guo2020graphcodebert}                        & 18.08                        & 12.62 & 14.79 & 18.40                      & 18.02                       & 19.22                      & 25.45\\ 




\cbert~\cite{feng2020codebert}                         & 17.83                        & 12.16                     & 14.90                      & 18.07                      & 19.06                       & 17.65                      & 25.16                      \\
RoBERTa~\cite{liu2019roberta}                       & 16.57                        & 11.17                     & 11.90                      & 17.72                      & 18.14                       & 16.47                      & 24.02                      \\
Transformer~\cite{vaswani2017attention}                   & 15.56                        & 11.18                     & 11.59                      & 16.38                      & 15.81                       & 16.26                      & 22.12                      \\ \multicolumn{1}{l}{Seq2Seq~\cite{sutskever2014sequence}} & \multicolumn{1}{c}{14.32}   & \multicolumn{1}{c}{9.64} & \multicolumn{1}{c}{10.21} & \multicolumn{1}{c}{13.98} & \multicolumn{1}{c}{15.93}  & \multicolumn{1}{c}{15.09} & \multicolumn{1}{c}{21.08} \\ \hline
\end{tabular}
}
\vspace{0.05in}

\caption{{\em {\small Comparison to existing models, on \cxglue dataset}}}
\vspace{-0.2in}
\label{comp1}
\end{table}



\subsection{Code Search}
We study the gains from multilingual fine-tuning using two  pre-trained models (\ie \cbert \& \gcbert). We multilingually fine-tune both models using the publicly available code \& dataset~\footnote{https://github.com/microsoft/CodeBERT/tree/master/GraphCodeBERT/codesearch}. As we did for code summarization, we re-trained the baseline models, to get performance numbers
for each case in the test set (to enable pairwise two-sample testing).
We use the same test sets for both monolingual and multilingual training to evaluate our approach. During the training, \gcbert uses a matrix of dimension . We could not use the full merged validation set (as we did for the code summarization task) because that makes the query and 
candidate code sets too large; the resulting matrix could not fit on our GPU server.
We used a down-sampled validation set comprising six monolingual validation sets with 10K query and 50K candidate codes each. However, we did not face any issue while testing because we did not merge the test sets. 

We report both the published values, and our replication; we need the replication to measure pairwise  gains. 
Though \cbert and \gcbert both work on sequence of code tokens, \gcbert creates a rudimentary data-flow graph, once it's told the programming language. 

Table~\ref{csearch} shows that multilingual fine-tuning improves the mean reciprocal rank for all languages except Go with \cbert. The improvement for Ruby, JavaScript, and Java are statistically significant. We found similar results for \gcbert exhibiting improvement for Ruby, JavaScript, Java, and Python; but with \gcbert both Go and PHP showed performance declines. 
However, overall, both showed statistically signficant improvements (p < 0.001); but the 
 improvement for \gcbert(1.54\%) is lower than \cbert(2.74\%). 
Finally, we note that our numbers for \cbert differ from the performance reported for  on the \cxglue leaderboard. 
This is because \cxglue benchmark uses only Python, and is based on
a restricted setting where identifier names are left out. \cxglue team argues that this abstraction enables them to stress-test the generalization ability of a model; however,
here we consider an unmodified setting where someone gives an natural language query and wishes to find
``natural'' code with variable names intact. 




\begin{table*}[h]


\centering
\resizebox{\textwidth}{!}{\renewcommand{\arraystretch}{1.2}

\begin{tabular}{lcccccccccccc}
\hline
\multicolumn{1}{c}{Language}                                                      & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\cbert\\ (published)~\cite{guo2020graphcodebert}\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\cbert \\ (re-trained)\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\mlcbert\end{tabular}} & \multicolumn{1}{c}{Improvement} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Effect \\ Size\end{tabular}}                          & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}p-value\\ (adjusted)\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\gcbert\\ (published)~\cite{guo2020graphcodebert}\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\gcbert\\ (re-trained)\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\mlgcbert\end{tabular}} & \multicolumn{1}{c}{Improvement} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Effect \\ Size\end{tabular}}                         & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}p-value\\ (adjusted)\end{tabular}} \\ \hline
Ruby                                                                                & 0.679                                                                           & 0.677                                                                              & 0.732                                                                         & +8.12\%                          & 0.072                                           & \textless{}0.001                                                                  & 0.703                                                                            & 0.708                                                                              & \textbf{0.738 }                                                                         & +4.24\%                          & 0.039                                           & \textless{}0.001                                                                  \\
JavaScript                                                                          & 0.620                                                                           & 0.616                                                                              & 0.643                                                                         & +4.38\%                          &  0.034                                          & \textless{}0.001                                                                  & 0.644                                                                            & 0.644                                                                              & \textbf{0.660}                                                                          & +2.48\%                          &  0.019                                          & 0.004 \\
Java                                                                                & 0.676                                                                           & 0.676                                                                              & 0.697                                                                         & +3.11\%                          & 0.026                                         & \textless{}0.001                                                                  & 0.691                                                                            & 0.693                                                                              & \textbf{0.710 }                                                                         & +2.45\%                          &  0.022                                        & \textless{}0.001                                                                  \\
Go                                                                                  & 0.882                                                                           & 0.885                                                                              & 0.885                                                                         & 0\%                              & -0.003                                          & 0.550 & \textbf{0.897                                                                           } & 0.894                                                                              & 0.894                                                                          & 0\%                              & -0.002                                          & 0.724                                                                           \\
PHP                                                                                 & 0.628                                                                           & 0.629                                                                              & 0.635                                                                         & +0.95\%                          & 0.009                                     & 0.003 & \textbf{0.649                                                                           } & 0.648                                                                              & 0.646                                                                          & -0.31\%                          &  -0.002                                        & 0.904 \\
Python                                                                              & 0.672                                                                           & 0.676                                                                              & 0.678                                                                         & +0.30\%                          &  0.004                                        & 0.050 & 0.692                                                                            & 0.692                                                                              & \textbf{0.695                                                                         } & +0.43\%                          & 0.005                                       & 0.300 \\ \hline
\multicolumn{1}{l}{Overall*}                                                      & \multicolumn{1}{c}{0.693}                                                      & \multicolumn{1}{c}{0.693}                                                         & \multicolumn{1}{c}{0.712}                                                    & \multicolumn{1}{c}{+2.74\%}     & \multicolumn{1}{c}{\multirow{2}{*}{ 0.013}} & \multicolumn{1}{c}{\multirow{2}{*}{\textless{}0.001}}                            & \multicolumn{1}{c}{0.713}                                                       & \multicolumn{1}{c}{0.713}                                                         & \multicolumn{1}{c}{\textbf{0.724}}                                                     & \multicolumn{1}{c}{+1.54\%}     & \multicolumn{1}{c}{\multirow{2}{*}{0.007}} & \multicolumn{1}{c}{\multirow{2}{*}{\textless{}0.001}}                            \\ \multicolumn{1}{l}{\begin{tabular}[c]{@{}l@{}}Overall \\ (weighted)\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}l@{}}Not \\ Reported\end{tabular}}    & \multicolumn{1}{c}{0.692}                                                         & \multicolumn{1}{c}{0.702}                                                    & \multicolumn{1}{c}{+1.42\%}     & \multicolumn{1}{c}{}                           & \multicolumn{1}{c}{}                                                             & \multicolumn{1}{c}{\begin{tabular}[c]{@{}l@{}}Not \\ Reported\end{tabular}}     & \multicolumn{1}{c}{0.709}                                                         & \multicolumn{1}{c}{\textbf{0.715}}                                                     & \multicolumn{1}{c}{+0.80\%}     & \multicolumn{1}{c}{}                           & \multicolumn{1}{c}{}                                                             \\ \hline
\multicolumn{13}{l}{*Evaluation criteria followed by \gcbert~\cite{guo2020graphcodebert}}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                
\end{tabular}
}
\vspace{0.05in}
\caption{{\em {\small Effectiveness of multi-lingual fine-tuning for code search task. Note that p-values are BH-corrected}}}

\label{csearch}
\vspace{-0.2in}
\end{table*}





\subsection{Method Name Prediction}

As for the previous two tasks, we try multilingual fine-tuning for method name prediction for \cbert.   
Here, too, we find evidence supporting the conclusion that multilingual training provides improvement for all the languages (Table~\ref{mpred}). Non-parametric pairwise improvements are significant for Ruby, JavaScript, and Java. We also  note observe relatively greater effect size for Ruby and JavaScript. 
Note that we achieve highest improvement for JavaScript because many functions therein are anonymous lambdas, 
since these functions have no names, they are not useful, and this diminishes available the JavaScript training set  relative to other tasks 
(lambdas still have summaries, and can be used for other tasks). Therefore, multilingual fine-tuning increases the dataset diversity and boosts JavaScript method name prediction performance. 



\begin{table}[h]
\centering
\resizebox{\columnwidth}{!}{\renewcommand{\arraystretch}{1.2}

\begin{tabular}{lccccccccc}
\hline
\multicolumn{1}{c}{\multirow{2}{*}{Language}}                                    & \multicolumn{3}{c}{\cbert}                                                               & \multicolumn{3}{c}{\mlcbert}                                                      & \multicolumn{1}{c}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}F-Score \\ Improvement\end{tabular}}} & \multicolumn{1}{c}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Effect \\ Size\end{tabular}} }         & \multicolumn{1}{c}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}p-value \\ (adjusted)\end{tabular}}} \\ \multicolumn{1}{c}{}                                                             & \multicolumn{1}{c}{Precision} & \multicolumn{1}{c}{Recall} & \multicolumn{1}{c}{F-Score} & \multicolumn{1}{c}{Precision} & \multicolumn{1}{c}{Recall} & \multicolumn{1}{c}{F-Score} & \multicolumn{1}{c}{}                                                                                & \multicolumn{1}{c}{}                           & \multicolumn{1}{c}{}                                                                               \\ \hline
Ruby                                                                               & 0.44                           & 0.40                       & 0.41                        & 0.53                          & 0.49                       & \textbf{0.49 }                       & 20.59\%                                                                                              & 0.112                                          & \textless{}0.001                                                                                    \\
JavaScript                                                                         & 0.30                          & 0.24                       & 0.26                        & 0.45                          & 0.40                       & \textbf{0.41}                        & 59.00\%                                                                                              & 0.215                                          & \textless{}0.001                                                                                    \\
Java                                                                               & 0.54                          & 0.51                       & 0.51                        & 0.56                          & 0.52                       & \textbf{0.52}                        & 2.22\%                                                                                               & 0.016                                         & \textless{}0.001                                                                                    \\
Go                                                                                 & 0.54                          & 0.52                       & 0.52                        & 0.56                          & 0.53                       & 0.52                        & 1.67\%                                                                                               &  0.015                                         & 0.004 \\
PHP                                                                                & 0.56                          & 0.53                       & 0.52                        & 0.57                          & 0.53                       & \textbf{0.53 }                       & 1.30\%                                                                                               & 0.009                                        & 0.004 \\
Python                                                                             & 0.49                          & 0.45                       & 0.45                        & 0.50                          & 0.45                       & \textbf{0.46 }                       & 1.60\%                                                                                               & 0.011                                         & 0.002 \\ \hline
\multicolumn{1}{l}{Overall}                                                      & \multicolumn{1}{c}{0. 48}     & \multicolumn{1}{c}{0.44}  & \multicolumn{1}{c}{0.44}   & \multicolumn{1}{c}{0.53}     & \multicolumn{1}{c}{0.49}  & \multicolumn{1}{c}{\textbf{0.49}}   & \multicolumn{1}{c}{10.09\%}                                                                         & \multicolumn{1}{c}{\multirow{2}{*}{0.024}} & \multicolumn{1}{c}{\multirow{2}{*}{\textless{}0.001}}                                              \\ \multicolumn{1}{l}{\begin{tabular}[c]{@{}l@{}}Overall\\ (weighted)\end{tabular}} & \multicolumn{1}{c}{0. 52}     & \multicolumn{1}{c}{0.48}  & \multicolumn{1}{c}{0.48}   & \multicolumn{1}{c}{0.54}     & \multicolumn{1}{c}{0.50}  & \multicolumn{1}{c}{\textbf{0.50}}   & \multicolumn{1}{c}{3.37\%}                                                                          & \multicolumn{1}{c}{}                           & \multicolumn{1}{c}{}                                                                               \\ \hline
\end{tabular}
}
\vspace{0.05in}
\caption{{\em {\small Effectiveness of multi-lingual fine-tuning for method naming task. Note that p-values are adjusted using Benjamini-Hochberg}}}
\label{mpred}
\vspace{-0.2in}
\end{table}




\begin{table}[h]


\centering
\resizebox{\columnwidth}{!}{\renewcommand{\arraystretch}{1.2}

\begin{tabular}{lll}
\hline




\multicolumn{3}{l}{\begin{tabular}[c]{@{}l@{}}
\textbf{Example:1}\\
\textit{//set the values from an Array}\\

public void setValues* ( Array arr ) \{\\
\hspace{.5cm}	\textit{//we omit intermediate lines to fit in the paper}\\
\hspace{.5cm}	\textit{//original code \href{https://github.com/Unidata/thredds/blob/d2d68f9eee87f345625211324d71d5dc3e162ee1/cdm/src/main/java/ucar/nc2/Attribute.java\#L548-L596
}{\link{here}}}\\
\}

\end{tabular}} \\ 

\hline
\multicolumn{3}{c}{\emph{Code Summarization}}                                                                                                                                                            \\
\multicolumn{2}{l}{Models \& comments}                                                                                                    & BLEU-4                                                \\ \hline
\multicolumn{2}{l}{Gold: set the values from an Array}                                                      & NA                                                    \\
\multicolumn{2}{l}{\cbert: Sets the values of the array .}                                                                            & 25                                                  \\
\multicolumn{2}{l}{\mlcbert: Set the values from an array .}                                                                      & 84                                                \\ \hline
\multicolumn{3}{c}{\emph{Code Search}}                                                                                                                                                                 \\ \multicolumn{2}{l}{Models}                                                                                                                & MRR                                                   \\ \hline
\multicolumn{2}{l}{\gcbert}                                                                                                                 & 0.33                                                  \\
\multicolumn{2}{l}{\mlgcbert}                                                                                                   & 1.00                                                  \\ \hline
\multicolumn{3}{c}{\emph{Method Name Prediction}}                                                                                                                                                      \\ \multicolumn{1}{l}{Models \& method name}                                   & \multicolumn{1}{l}{Sub tokens}                           & \multicolumn{1}{l}{F-Score}                          \\ \hline
Gold: setValues                                                                 & set Values                                                 & NA                                                    \\
\cbert: setArrayValue                                                                  & set Array Value                                                     & 0.40                                                  \\ \multicolumn{1}{l}{\mlcbert: setValue}                           & \multicolumn{1}{l}{set Value}                           & \multicolumn{1}{l}{0.50}                             \\ \hline

\multicolumn{3}{l}{\begin{tabular}[c]{@{}l@{}}
\textbf{Example:2}\\

\textit{//Registers set injection point .}\\
public void registerPetiteSetInjectionPoint* ( final String beanName, final String property ) \{\\
\hspace{.5cm}	\textit{//we omit intermediate lines to fit in the paper}\\
\hspace{.5cm}	\textit{//original code \href{https://github.com/oblac/jodd/blob/85ad7f813ec0e07ecd27042aeb47ff2047631fa5/jodd-petite/src/main/java/jodd/petite/PetiteBeans.java\#L585-L598
}{\link{here}}}\\
\}



\end{tabular}} \\ 

\hline
\multicolumn{3}{c}{\emph{Code Summarization}}                                                                                                                                                            \\
\multicolumn{2}{l}{Models \& comments}                                                                                                    & BLEU-4                                                \\ \hline
\multicolumn{2}{l}{Gold: Registers set injection point .}                                                      & NA                                                    \\
\multicolumn{2}{l}{\cbert: Register a set of set InjectionPoint .}                                                                            & 19                                                 \\
\multicolumn{2}{l}{\mlcbert: Register a set injection point .}                                                                      & 60                                                  \\ \hline
\multicolumn{3}{c}{\emph{Code Search}}                                                                                                                                                                 \\ \multicolumn{2}{l}{Models}                                                                                                                & MRR                                                   \\ \hline
\multicolumn{2}{l}{\gcbert}                                                                                                                 & 0.50                                                  \\
\multicolumn{2}{l}{\mlgcbert}                                                                                                   & 1.00                                                  \\ \hline
\multicolumn{3}{c}{\emph{Method Name Prediction}}                                                                                                                                                      \\ \multicolumn{1}{l}{Models \& method name}                                   & \multicolumn{1}{l}{Sub tokens}                           & \multicolumn{1}{l}{F-Score}                          \\ \hline
Gold: registerPetiteSetInjectionPoint                                                                 & register Pet ite Set In jection Point                                                 & NA                                                    \\
\cbert: addPropertyInjectionPoint                                                                  & add Property In jection Point                                                     & 0.50 \\ \multicolumn{1}{l}{\mlcbert: setPropertyInjectionPoint}                           & \multicolumn{1}{l}{set Property In jection Point}                           & \multicolumn{1}{l}{0.57}                             \\ \hline






\multicolumn{3}{l}{\emph{*``registerPetiteSetInjectionPoint'' \& ``setValues'' tokens are abstracted for method name prediction task}}                                                                                                                                                           


\end{tabular}
}


\vspace{0.05in}
\caption{{\em {\small Examples exhibiting the effectiveness of multilingual training}}}
\label{exmp}
\vspace{-0.2in}

\end{table}



\subsection{Two Illustrative Examples}
We used the same dataset for all tasks; for illustration, we show 
(Table~\ref{exmp})  two test instances where all the tasks show improved performance  from multilingual fine-tuning. In code summarization task, the monolingual fine-tuning scores 25 BLEU-4 in Example 1. \cbert produces a semantically wrong comment where multilingual fine-tuning generates the semantically correct solution. Note that the BLEU-4 is 84 for the second example because of the missing period in the gold standard (BLEU-4 is case-insensitive). Multilingual fine-tuning also helps the code search problem by increasing the MRR from 0.33 (Rank:3) to 1.00 (Rank:1). We also observe performance improvement from the method name prediction task. The gold standard consists of two sub tokens (\ie set and Values), and mono-lingual fine-tuning generates three (\ie set, Array, and Value), one of them is exact match. On the other hand, multilingual fine-tuning removes the extra ``Array'' subtoken and produces two subtokens(\ie set and Value) resulting in the F-score 0.50. We observe a similar result in example 2. Note that like BLEU-4, our method name prediction metric is also case-insensitive.  








\takeaway{3}{Multilingual fine-tuning is likely to increase diversity and help the models perform better than those trained with smaller mono-lingual datasets, especially for low-resource languages, irrespective of the task.}