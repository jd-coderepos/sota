\documentclass{article}
\PassOptionsToPackage{numbers, compress}{natbib}





\usepackage[preprint]{neurips_2023}








\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         \usepackage{graphicx}
\usepackage{subfigure}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{tcolorbox}
\usepackage{makecell}
\usepackage{caption}
\usepackage{wrapfig}
\usepackage{enumitem}


\newcommand{\red}[1]{{\color{red}#1}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{constraint}{Constraint}[section]


\title{Spectral Heterogeneous Graph Convolutions via Positive Noncommutative Polynomials}

\author{Mingguo He \\
  Renmin University of China\\
  \texttt{mingguo@ruc.edu.cn}
  \And
  Zhewei Wei \\
  Renmin University of China\\
  \texttt{zhewei@ruc.edu.cn}
  \AND
  Shikun Feng \\
  Baidu Inc.\\
  \texttt{fengshikun01@baidu.com}
  \And
  Zhengjie Huang\\
  Baidu Inc.\\
  \texttt{huangzhengjie@baidu.com}
  \AND
  Weibin Li\\
  Baidu Inc.\\
  \texttt{liweibin02@baidu.com}
  \And
  Yu Sun\\
  Baidu Inc.\\
  \texttt{sunyu02@baidu.com}
  \And
  Dianhai Yu\\
  Baidu Inc.\\
  \texttt{yudianhai@baidu.com}
  \And  
}




\begin{document}
\maketitle

\begin{abstract}
Heterogeneous Graph Neural Networks (HGNNs) have gained significant popularity in various heterogeneous graph learning tasks. However, most HGNNs rely on spatial domain-based message passing and attention modules for information propagation and aggregation. These spatial-based HGNNs neglect the utilization of spectral graph convolutions, which are the foundation of Graph Convolutional Networks (GCN) on homogeneous graphs. Inspired by the effectiveness and scalability of spectral-based GNNs on homogeneous graphs, this paper explores the extension of spectral-based GNNs to heterogeneous graphs. We propose PSHGCN, a novel heterogeneous convolutional network based on positive noncommutative polynomials. PSHGCN 
provides a simple yet effective approach for learning spectral graph convolutions on heterogeneous graphs. Moreover, we demonstrate the rationale of PSHGCN in graph optimization. We conducted an extensive experimental study to show that PSHGCN can learn diverse spectral heterogeneous graph convolutions and achieve superior performance in node classification tasks. Our code is available at \url{https://github.com/ivam-he/PSHGCN}.
\end{abstract}

\section{Introduction}
In recent years, there has been a significant surge of interest in graph neural networks (GNNs) due to their remarkable performance in tackling diverse graph learning tasks, including but not limited to node classification~\cite{gcn,gat,Chebnet}, link prediction~\cite{zhang2018link,cai2021line,zhu2021neural}, and graph property prediction~\cite{gin,liu2019hyperbolic,hao2020asgn}. While earlier versions of GNNs~\cite{gcn,gat} were primarily developed for homogeneous graphs, which consist of only one type of node and edge, real-world graphs typically comprise a diverse range of nodes and edges known as heterogeneous graphs. For instance, an academic graph may include multiple nodes such as "author," "paper," and "conference," as well as several edges such as "cite," "write," and "publish." Due to the extensive and diverse information in heterogeneous graphs, specialized models are necessary to analyze them effectively.

In response to the challenge of heterogeneity, numerous Heterogeneous Graph Neural Networks (HGNNs) have been proposed, achieving significant performance~\cite{han,magnn,mhgcn,halo}.  The majority of these HGNNs depend on spatial domain-based message passing and attention modules for information propagation and aggregation.
Following~\cite{sehgnn}, we can broadly classify these spatial-based HGNNs into two categories based on whether they use manually-selected meta-paths or use meta-path-free techniques to aggregate information.







\begin{enumerate}
    \item[$\bullet$] \textbf{The HGNNs driven by predetermined meta-paths:} 
     HAN~\cite{han} utilizes a hierarchical attention mechanism to aggregate both node features and semantic information with multiple meta-paths selected by human experts. 
     HetGNN~\cite{hetgnn} first employs random walks to generate node neighbors and subsequently aggregates the features of these neighbors.
     MAGNN~\cite{magnn}, similar to HAN, uses manually-selected meta-paths. It encodes all the information from the paths rather than solely focusing on the endpoints.
     SeHGNN~\cite{sehgnn} utilizes predetermined meta-paths to pre-compute neighbor aggregations and uses a transformer-based approach to fuse features.
     


   
    \item[$\bullet$] \textbf{The HGNNs driven by meta-path-free methods:} 
    RGCN~\cite{rgcn} extends GCN~\cite{gcn} to heterogeneous graphs by devising graph convolutions on subgraphs that consist of a single unique edge type. 
    GTN~\cite{gtn} employs a combination of soft sub-graph selection and matrix multiplication steps to generate meta-path neighbor graphs. It then applies GCN's convolution to encode these graphs.
    HGB~\cite{hgb} utilizes a multi-layer GAT network as its backbone and incorporates both node features and learnable edge-type embeddings to generate attention. 
    MHGCN~\cite{mhgcn} learns meta-paths through multi-layer convolution aggregation and uses GCN's convolution for feature aggregation.
    EMRGNN~\cite{emrgnn} and HALO~\cite{halo} propose graph optimization objectives specifically tailored for heterogeneous graphs and design new HGNN architectures by solving these optimization problems.
\end{enumerate}
Although the above HGNNs have shown promising results in various heterogeneous graph learning tasks, they still have two significant limitations. First, these HGNNs either use manually-selected meta-paths or design some heuristic modules to aggregate information, which lacks theoretical explanation and controllability. For example, HAN~\cite{han}, MAGNN~\cite{magnn}, and SeHGNN~\cite{sehgnn} need selected meta-paths, and different selections would significantly affect the model's performance. MHGCN~\cite{mhgcn} directly learns the summation weights of each adjacency matrix with only one type of edge to obtain the graph convolution. As a result, this approach lacks a theoretical explanation regarding the effectiveness or validity of the learned convolution. Second, we can observe that these HGNNs design their propagation and aggregation modules from the spatial domain. They neglected to develop the graph convolution from the spectral domain, which is the foundation of GCN~\cite{gcn}. On homogeneous graphs, GNNs designed from the spectral domain are known as spectral-based GNNs, which have remarkable effectiveness and scalability~\cite{gprgnn,bernnet,jacobi,chebnetii}.

To address these issues, we explore the extension of spectral-based GNNs to heterogeneous graphs. We propose a novel heterogeneous graph neural network called PSHGCN, which provides a simple yet effective approach for learning spectral heterogeneous graph convolutions. Specifically, our PSHGCN uses a positive noncommutative polynomial to constrain the learned graph convolutions to be valid, i.e., those convolutions' outputs are positive semidefinite. Furthermore, we propose a generalized graph optimization framework and demonstrate the rationale of our PSHGCN from this framework. Finally, we conduct an extensive experimental study to show that PSHGCN can learn various spectral heterogeneous graph convolutions and achieve superior performance in node classification tasks. Notably, our PSHGCN has desirable scalability and obtains a state-of-art result on the large heterogeneous graph ogbn-mag.

We summarize the contributions of this paper as follows:
\begin{enumerate}[label=\alph*)]
    \item We are the first to explore the extension of spectral-based GNNs to heterogeneous graphs, providing a new direction for HGNNs' research.  
    \item We propose PSHGCN, utilizing positive noncommutative polynomials to effectively learn spectral graph convolutions on heterogeneous graphs. 
    \item We propose a generalized graph optimization framework and demonstrate the rationale of our PSHGCN from this framework.
    \item Our PSHGCN achieves superior performance in the node classification task and has desirable scalability.
\end{enumerate}











\section{Preliminaries}
\subsection{Spectral Graph Convolution}\label{se:spectral_g_conv}
Recent studies suggest that many popular spectral-based GNNs utilize a polynomial of the Laplacian matrix to approximate spectral graph convolutions or, equivalently, spectral graph filters~\cite{gcn,Chebnet,bernnet,jacobi,chebnetii}. In particular,
we denote an undirected homogeneous graph with node set $V$ and edge set $E$ as $G(V,E)$, whose adjacency matrix is  $\mathbf{A}$. Let $\mathbf{L}= \mathbf{I}-\mathbf{D}^{-1/2}\mathbf{A}\mathbf{D}^{-1/2}$ denote the normalized Laplacian matrix, where $\mathbf{D}$ is the diagonal degree matrix of $\mathbf{A}$. We use $\mathbf{L}=\mathbf{U}\mathbf{\Lambda}\mathbf{U}^{\top}$ to represent the eigendecomposition of $\mathbf{L}$, where $\mathbf{U}$ denotes the matrix of eigenvectors and $\mathbf{\Lambda}={\rm diag}[\lambda_1,...,\lambda_{|V|}]$ is the diagonal matrix of eigenvalues. Given a graph signal vector $\mathbf{x} \in \mathbb{R}^{|V|}$, we can formulate its spectral graph convolution operator as
\begin{equation}\label{spec_gnn}
h(\mathbf{L})\mathbf{x}=\mathbf{U}h \left(\mathbf{\Lambda}\right)\mathbf{U}^{\top}\mathbf{x}=\mathbf{U} {\rm diag}\left[h(\lambda_1),...,h(\lambda_{|V|})\right]\mathbf{U}^{\top}\mathbf{x}.
\end{equation}
The function $h(\mathbf{L})$ (or, equivalently, $h(\lambda)$) is the \textbf{ spectral graph convolution/filter}. To avoid the expansive eigendecomposition, spectral-based GNNs use a polynomial to approximate $h(\mathbf{L})$.
\begin{equation}\label{eq:h(L)}
    h(\mathbf{L}) \approx \sum\limits_{k=0}^K w_k \mathbf{L}^k,
\end{equation}
where $w_k$ is the polynomial convolution weight. We can also use the normalized adjacency matrix $\hat{\mathbf{A}}=\mathbf{D}^{-1/2}\mathbf{A}\mathbf{D}^{-1/2}$ to replace the Laplacian matrix $\mathbf{L}$, i.e., the spectral graph convolution can be equivalently denoted as $\sum\nolimits_{k=0}^K w_k \hat{\mathbf{A}}^k$~\cite{gprgnn,bernnet}. Importantly, recent research on spectral-based GNNs~\cite{bernnet} highlights the constraint that a valid graph convolution $h(\lambda)$ must be non-negative, i.e., $h(\lambda) \geq 0$ or in terms of the Laplacian matrix, $h(\mathbf{L})$ should be \textbf{positive semidefinite}. 









\subsection{Heterogeneous Graph}
A heterogeneous graph~\cite{hetero_graph} is defined as $G=(V,E,\phi , \psi)$, where $V$ is the set of nodes and $E$ is the set of edges. Let $n=\lvert V \rvert$ denote the number of nodes.
Each node $v \in V$ is attached with a node type $\phi(v)$ and each edge $e \in E$ is attached with an edge type $\psi(e)$. We use $\mathcal{T}_v =\{\phi(v): \forall v \in V\}$ to denote the set of possible node types and $\mathcal{T}_e = \{\psi(e):\forall e \in E\}$ to denote the set of possible edge types. When $\lvert \mathcal{T}_v \rvert=\lvert \mathcal{T}_e \rvert=1 $, the graph becomes an ordinary homogeneous graph. For  convenience, we use $\mathcal{R}=\lvert \mathcal{T}_e \rvert$ to denote the number of edge types.

For a heterogeneous graph $G$, we denote the sub-graph generated by differentiating the types of edges between all nodes as $\{G_r|r=1,2,\cdots,\mathcal{R}\}$. Each $G_r$ includes $n$ nodes but only contains one type of edge. Let $\mathbf{A}_r$ denote the adjacency matrix of the sub-graph $G_r$, where $\mathbf{A}_r[i,j]$ is non-zero if there exists an $r$-th type edge from $i$ to $j$. Notably, in the general case of heterogeneous graphs, the sub-graph $G_r$ is a directed graph. Hence, we use $\mathbf{D}_r$ to represent the diagonal out-degree matrix of $\mathbf{A}_r$, i.e., $\mathbf{D}_{r}[i,i]=\sum\nolimits_{j}^n \mathbf{A}_r[i,j]$. We use $\hat{\mathbf{A}}_r=\mathbf{D}_{r}^{-1}\mathbf{A}_r$ to denote the normalized adjacency matrix and use $\mathbf{L}_r=\mathbf{I}-\hat{\mathbf{A}}_r$ to denote the normalized Laplacian matrix. For brevity, we assume that all nodes possess the same dimensional features and denote the node features as $\mathbf{X}\in \mathbb{R}^{n \times f}$, where $f$ represents the dimensionality of the node features. 





\section{PSHGCN}
In this section, we will first introduce the definition of the spectral heterogeneous graph convolution/filter. Then we will propose PSHGCN, a novel \underline{P}ositive \underline{S}pectral \underline{H}eterogeneous \underline{G}raph \underline{C}onvolutional \underline{N}etwork via positive noncommutative polynomials~\cite{positive-poly}.



\subsection{Spectral Heterogeneous Graph Convolution}
Based on the definition of spectral graph convolution~\eqref{eq:h(L)}, we can extend it to define spectral heterogeneous graph convolution on either $\hat{\mathbf{A}}_r$ or $\mathbf{L}_r$. For the sake of simplicity, we denote the matrix $\hat{\mathbf{A}}_r$ or $\mathbf{L}_r$ as $\mathbf{P}_r$, which is commonly referred to as the shift operator in graph signal processing~\cite{graphsignal}.


\begin{definition}\label{h_conv}
Consider a heterogeneous graph $G=(V,E,\phi , \psi)$ with shift operators $\{\mathbf{P}_r\}_{r=1}^{R}$. A spectral heterogeneous graph convolution/filter on $G$ is a noncommutative polynomial function $h$ that takes the shift operators $\{\mathbf{P}_r\}_{r=1}^{R}$ as independent variables, denoted as $h(\mathbf{P}_1,\mathbf{P}_2,\cdots,\mathbf{P}_R)$.
\end{definition}
We can formalize the spectral heterogeneous graph convolution $h$ as
\begin{equation}\label{eq:g}
h(\mathbf{P}_1,\mathbf{P}_2,\cdots,\mathbf{P}_R) = w_0\mathbf{I}+\sum\limits_{k=1}^{K}\sum\limits_{r_1,r_2\cdots,r_k}w_{r_1,r_2,\cdots,r_k}\left(\mathbf{P}_{r_1}\mathbf{P}_{r_2}\cdots\mathbf{P}_{r_k}\right),
\end{equation}
where $w_{r_1,r_2,\cdots,r_k} \in \mathbb{R}$ denote the polynomial coefficients, $K \in \mathbb{Z}^+$ is the order of the polynomial, and $r_i \in \{1,2,\cdots,R\}$ for each $i \in \{1,2,\cdots,k\}$.
For example, a $2$-order polynomial $h$ with two variables can be denoted as $h(\mathbf{P}_1,\mathbf{P}_2)=w_0\mathbf{I}+w_1\mathbf{P}_{1}+w_2\mathbf{P}_{2}+w_{1,1}\mathbf{P}_{1}\mathbf{P}_{1}+w_{1,2}\mathbf{P}_{1}\mathbf{P}_{2}+w_{2,1}\mathbf{P}_{2}\mathbf{P}_{1}+w_{2,2}\mathbf{P}_{2}\mathbf{P}_{2}$.  In Table~\ref{tb:hgnn_with_filter}, we show several existing HGNNs that can be perceived as attempts to design the spectral heterogeneous graph convolution $h$.
\begin{enumerate} \item[$\bullet$] \textbf{GTN}~\cite{gtn} introduces a Graph Transformer layer to perform the graph convolution, which can be represented as  $\mathbf{D}^{-1}\prod\nolimits_{k=0}^{K}\sum\nolimits_{r=0}^{R}\alpha_{r}^{(k)}\mathbf{A}_r$. In this expression, $\mathbf{D}$ denotes the degree matrix for normalization, and $\mathbf{A}_0$ denotes the identity matrix. GTN obtains graph convolutions by learning the weights $\alpha_{r}^{(k)}$.
\item[$\bullet$] \textbf{EMRGNN}~\cite{emrgnn} utilizes the multi-relational Personalized PageRank~\cite{appnp} to design the graph convolutions, which can be expressed as $\sum\nolimits_{k=0}^{K}\alpha(1-\alpha)^k \left(\sum\nolimits_{r=1}^{R}\mu_r\Tilde{\mathbf{A}}_{r} \right)^k$. Here, $\Tilde{\mathbf{A}}$ denotes the normalized adjacency matrix with self-loops. EMRGNN approximates a graph convolution by learning the weight $\mu_r$ and setting $\alpha$ as a non-negative trade-off parameter.
\item[$\bullet$]\textbf{MHGCN}~\cite{mhgcn} 
applies a straightforward approach by aggregating the adjacency matrix $\mathbf{A}_r$ with learnable weights $\beta_r$, and uses $K$ GCN-layers to achieve the graph convolution. This can be expressed as  $\left( 
\sum\nolimits_{r=1}^{R}\beta_r\mathbf{A}_r \right)^{K}$.
\end{enumerate}


\begin{table}[t]
\centering
\caption{Existing HGNNs that attempt to design the spectral graph convolution.}
\vspace{1mm}
\begin{tabular}{@{}lll@{}}
\toprule
Method &Operator $\mathbf{P}_r$  &Convolution $h$  \\ \midrule
GTN~\cite{gtn}    &$\mathbf{A}_r$  &$\mathbf{D}^{-1}\prod\nolimits_{k=0}^{K}\sum\nolimits_{r=0}^{R}\alpha_{r}^{(k)}\mathbf{A}_r$  \\
EMRGNN~\cite{emrgnn} &$\Tilde{\mathbf{A}}_r$  &$\sum\nolimits_{k=0}^{K}\alpha(1-\alpha)^k \left(\sum\nolimits_{r=1}^{R}\mu_r\Tilde{\mathbf{A}}_{r} \right)^k$ \\
MHGCN~\cite{mhgcn}  &$\mathbf{A}_r$  &$\left( 
\sum\nolimits_{r=1}^{R}\beta_r\mathbf{A}_r \right)^{K}$   \\ 
\bottomrule
\end{tabular}
\label{tb:hgnn_with_filter}
\vspace{-4mm}
\end{table}

We can observe that the above methods try to approximate the spectral heterogeneous graph convolution $h$ by learning different weights. 
However, all of these methods exhibit comparatively lower expressiveness compared to the spectral graph convolution defined in Equation~\eqref{eq:g}. 
In fact, the spectral graph convolution has the ability to approximate arbitrary convolution functions, given a sufficiently large value of $K$, whereas the above methods lack this capability.


























\subsection{Positive Spectral Heterogeneous Graph Convolutional Network}
Although using the spectral heterogeneous graph convolution as defined in Definition~\ref{h_conv} appears promising, it still has some drawbacks: 1) It is hard to guarantee the convolution $h(\mathbf{P}_1,\mathbf{P}_2,\cdots,\mathbf{P}_R)$ to be \textbf{positive semidefinite}, i.e, the constraint of valid graph convolutions described in Section~\ref{se:spectral_g_conv}. 2) Using the monomial basis to learn graph convolutions is inefficient, which has been extensively studied in spectral-based GNNs for homogeneous graphs~\cite{bernnet,jacobi,chebnetii}.

To ensure the learned graph convolution is positive semidefinite, spectral-based GNNs employ techniques such as Bernstein Approximation or Polynomial Interpolation to constrain the filters $h(\lambda)$ to be non-negative~\cite{bernnet,chebnetii}. However, directly extending these methods to the heterogeneous convolution $h(\mathbf{P}_1,\mathbf{P}_2,\cdots,\mathbf{P}_R)$ is infeasible since the shift operators $\{\mathbf{P}_{r}\}_{r=1}^R$ share different eigenvectors. Consequently, obtaining valid heterogeneous graph convolutions becomes a nontrivial and challenging task.




\begin{definition}\label{sos}
{\rm \cite{positive-poly}}
A noncommutative polynomial $h(\mathbf{P}_1,\mathbf{P}_2,\cdots,\mathbf{P}_R)$ is a Sum of Squares if it satisfies the following condition:
\begin{equation}
    h(\mathbf{P}_1,\mathbf{P}_2,\cdots,\mathbf{P}_R) = \sum\nolimits_{i}g_i(\mathbf{P}_1,\mathbf{P}_2,\cdots,\mathbf{P}_R)^{\top}g_i(\mathbf{P}_1,\mathbf{P}_2,\cdots,\mathbf{P}_R),
\end{equation}
where each $g_i$ is an arbitrary polynomial and $g_i^{\top}$ denotes its transpose matrix.
\end{definition}


\begin{theorem}\label{th_sos_posi}
{\rm \cite{positive-poly}} Let $h(\mathbf{P}_1,\mathbf{P}_2,\cdots,\mathbf{P}_R)$ be a noncommutative polynomial. If $h(\mathbf{P}_1,\mathbf{P}_2,\cdots,\mathbf{P}_R)$ is a Sum of Squares, then the output of $h(\mathbf{P}_1,\mathbf{P}_2,\cdots,\mathbf{P}_R)$ is positive semidefinite. Conversely,   If the output of $h(\mathbf{P}_1,\mathbf{P}_2,\cdots,\mathbf{P}_R)$ is positive semidefinite, then $h(\mathbf{P}_1,\mathbf{P}_2,\cdots,\mathbf{P}_R)$ is a Sum of Squares.
\end{theorem}

Theorem~\ref{th_sos_posi} establishes the necessity of using the Sum of Squares to ensure that the output of $h(\mathbf{P}_1,\mathbf{P}_2,\cdots,\mathbf{P}_R)$ is positive semidefinite.  While any noncommutative polynomial with a positive semidefinite output can be expressed in the form of Sum of Squares, finding suitable polynomials $g_i$ is challenging. Therefore, we simplify the Sum of Squares form by utilizing only one polynomial $g(\mathbf{P}_1,\mathbf{P}_2,\cdots,\mathbf{P}_R)$, which is a monomial noncommutative polynomial with coefficients $w$. Based on this simplified form, we propose the Positive Spectral Heterogeneous Graph Convolutional Network (PSHGCN), with its convolution operator denoted as
\begin{equation}
\mathbf{y}= g(\mathbf{P}_1,\mathbf{P}_2,\cdots,\mathbf{P}_R)^{\top}g(\mathbf{P}_1,\mathbf{P}_2,\cdots,\mathbf{P}_R)\mathbf{x},
\end{equation}
where $\mathbf{x} \in \mathbb{R}^n$  represents a graph signal vector and we treat it as a column of the node features $\mathbf{X}$. As illustrated in Figure~\ref{fig:scheme}, our PSHGCN approximated a positive semidefinite heterogeneous graph convolution by the multiplication of  $g(\mathbf{P}_1,\mathbf{P}_2,\cdots,\mathbf{P}_R)^{\top} g(\mathbf{P}_1,\mathbf{P}_2,\cdots,\mathbf{P}_R)$. In practice, we employ two Multi-layer Perceptrons (MLPs) for feature extraction and downstream learning tasks, with one preceding and another following the convolution operator. More precisely, the PSHGCN model can be formulated as
\begin{tcolorbox}[height=13mm,boxrule=0.5pt,valign=center]
\begin{equation}
\label{eq:pshgcn}
    \mathbf{Z}=f^{\prime}_{\theta}(\mathbf{Y}), \mathbf{Y}=g(\mathbf{P}_1,\mathbf{P}_2,\cdots,\mathbf{P}_R)^{\top} g(\mathbf{P}_1,\mathbf{P}_2,\cdots,\mathbf{P}_R)f_{\theta}(\mathbf{X}), 
\end{equation}
\end{tcolorbox}
where $f_{\theta}(\cdot)$ and $f^{\prime}_{\theta}(\cdot)$ denote MLPs. Interestingly, despite PSHGCN not being as powerful as the full Sum of Squares form, this simple approach demonstrates state-of-the-art (SOTA) performance in node classification tasks on heterogeneous graphs, as showcased in Section~\ref{sec:experiments}.




 














\begin{figure*}[t]
    \centering
    \includegraphics[width=13cm]{pshgcn_f.pdf}
\caption{An illustration of the proposed PSHGCN.}
    \label{fig:scheme}
    \vspace{-3mm}
\end{figure*}






\section{Analysis of PSHGCN}
In this section, we delve into the underlying rationale of PSHGCN, with a particular emphasis on graph optimization. We will establish that PSHGCN represents the essential structure of any valid heterogeneous convolution. Additionally, we will analyze the complexity and scalability of our PSHGCN approach.




\subsection{Generalized Heterogeneous Graph Optimization Framework}
The utilization of graph optimization in designing GNNs for homogeneous graphs has been extensively explored and has led to remarkable performance achievements~\cite{zhou2003learning,twirs,bernnet,HF_HL}. However, there have been limited efforts to extend the graph optimization framework to heterogeneous graphs. Given an $n$-dimensional graph signal $\mathbf{x}$, we consider a generalized heterogeneous graph optimization problem
\begin{equation}
\label{eq_opt}
    \min_{\mathbf{y}} 
    \mathcal{F}(\mathbf{y})= \min_{\mathbf{y}} \left\{(1-\alpha)\mathbf{y}^{\top}\gamma(\mathbf{P}_1,\mathbf{P}_2,\cdots,\mathbf{P}_R)\mathbf{y}+ \alpha\parallel \mathbf{y}-\mathbf{x}\parallel_{2}^{2}\right\},
\end{equation}
where $\alpha \in (0,1)$ is a trade-off parameter, $\mathbf{y}$ denotes the resulting representation of the input signal $\mathbf{x}$, and $\gamma(\mathbf{P}_1,\mathbf{P}_2,\cdots,\mathbf{P}_R)$ is an energy function determining the rate of propagation~\cite{spielman2019spectral}. Generally, $\gamma(\cdot)$ takes the shift operators $\{\mathbf{P}_r\}_{r=1}^R$ as inputs and produces a real $n \times n$ matrix. We can express certain existing heterogeneous graph optimization objectives with the generalized graph optimization framework. For example, by setting $\gamma(\mathbf{P}_1,\mathbf{P}_2,\cdots,\mathbf{P}_R) = \sum\nolimits_{r=1}^{R}\mu_r\mathbf{P}_r$ subject to $\sum\nolimits_{r=1}^{R} \mu_r=1$ and $\mu_r \geq 0$, the optimization function~\eqref{eq_opt} corresponds to the objective used in EMRGNN~\cite{emrgnn}.



We establish a "minimum requirement" for the energy function $\gamma(\mathbf{P}_1,\mathbf{P}_2,\cdots,\mathbf{P}_R)$, demanding that its output matrix must be positive semidefinite. Failure to meet this requirement renders the optimization function $\mathcal{F}(\mathbf{y})$ non-convex. Conversely, if the output of $\gamma(\mathbf{P}_1,\mathbf{P}_2,\cdots,\mathbf{P}_R)$ is positive semidefinite, problem~\eqref{eq_opt} admits a closed-form solution. We compute the derivative of $\mathcal{F}(\mathbf{y})$ with respect to $\mathbf{y}$ and we have
\begin{equation}
    \begin{split}
        \frac{\partial\mathcal{F}}{\partial\mathbf{y}}= 2(1-\alpha) \gamma(\mathbf{P}_1,\mathbf{P}_2,\cdots,\mathbf{P}_R) \mathbf{y}+2\alpha \left(\mathbf{y}-\mathbf{x} \right).
    \end{split}
\end{equation}
Set the derivative to zero and we obtain the optimal solution
\begin{equation}
\label{opt-re}
    \mathbf{y}^{\ast}=\alpha \left[ \alpha\mathbf{I}+(1-\alpha)\gamma(\mathbf{P}_1,\mathbf{P}_2,\cdots,\mathbf{P}_R) \right]^{-1}\mathbf{x}.
\end{equation}
We can observe that $\alpha \left[ \alpha\mathbf{I}+(1-\alpha)\gamma(\mathbf{P}_1,\mathbf{P}_2,\cdots,\mathbf{P}_R) \right]^{-1}$ is the heterogeneous graph convolution, and we denote it as $\mathbf{H}$, i.e., $\mathbf{H} = \alpha \left[ \alpha\mathbf{I}+(1-\alpha)\gamma(\mathbf{P}_1,\mathbf{P}_2,\cdots,\mathbf{P}_R) \right]^{-1}$.

\subsection{Positive Semidefinite Constraint on Graph Convolution}
Given an arbitrary energy function $\gamma(\mathbf{P}_1,\mathbf{P}_2,\cdots,\mathbf{P}_R)$ whose output is positive semidefinite, we now consider how the corresponding heterogeneous graph convolution $\mathbf{H}$ should look like. Specifically, we present the following lemma.
\begin{lemma}
\label{lemma}
Consider an arbitrary energy function $\gamma(\mathbf{P}_1,\mathbf{P}_2,\cdots,\mathbf{P}_R)$ whose output is a real positive semidefinite $n \times n$ matrix and $\alpha \in (0,1)$, then the matrix $\alpha \left[ \alpha\mathbf{I}+(1-\alpha)\gamma(\mathbf{P}_1,\mathbf{P}_2,\cdots,\mathbf{P}_R) \right]^{-1}$ is also a real positive semidefinite  matrix.
\end{lemma}

The derivation of this Lemma is shown in the supplementary materials. Lemma~\ref{lemma} demonstrates that, within our generalized heterogeneous graph optimization framework, the heterogeneous graph convolution $\mathbf{H}$ must be positive semidefinite. This observation motivates our PSHGCN, whose learned convolution is ensured to be positive semidefinite. Specifically, according to Taylor's theorem for multivariate function~\cite{taylor_mul_fun}, we approximate the heterogeneous graph convolution $\mathbf{H}$ with a multivariate polynomial, denoted as $\mathbf{H} \approx h(\mathbf{P}_1,\mathbf{P}_2,\cdots,\mathbf{P}_R)$. Theorem~\ref{th_sos_posi} implies that we should employ the Sum of Squares form to obtain a positive semidefinite polynomial for the heterogeneous graph convolution $h(\mathbf{P}_1,\mathbf{P}_2,\cdots,\mathbf{P}_R)$.




\subsection{Complexity and Scalability}\label{se:scale}
In the Equation~\eqref{eq:pshgcn}, we assume the 
dimension of $f_\theta(\mathbf{X})$'s output is $d$ and $g(\mathbf{P}_1,\mathbf{P}_2,\cdots,\mathbf{P}_R)$ represents a $K$-order monomial noncommutative polynomial similar to Equation~\eqref{eq:g}. Theoretically, the polynomial $g$ has $\frac{R^{K+1}-1}{R-1}$ terms. However, in practice, many terms $\mathbf{P}_i\mathbf{P}_j$ in $g$ are zero matrices due to various types of nodes have no direct connections from each other in heterogeneous graphs, such as authors and conferences in DBLP.
Consequently, we can eliminate these terms. Let $\mathcal{C}$ represent the count of non-zero terms with order $K$ in polynomial $g$, and let $m$ represent the maximum number of edges in $\{\mathbf{P}_1,\mathbf{P}_2,\cdots,\mathbf{P}_R\}$. We can get the time complexity of PSHGCN as $O(\mathcal{C}Kmd)$. This complexity is comparable to that of many meta-path based HGNNs. The constant $\mathcal{C}$ can be interpreted as the number of selected meta-paths, and $K$ can be seen as the maximum length of meta-paths.

Similar to many popular spectral-based GNNs, our PSHGCN can be extended to large-scale graphs by decoupling the feature transformation and propagation processes. In particular, we first calculate and store $\mathbf{P}_{r_1}\mathbf{P}_{r_2}\cdots\mathbf{P}_{r_k}\mathbf{X}$ for $k \in \{1,2,\cdots K\}$ in the preprocessing, where $\mathbf{P}_{r_1}\mathbf{P}_{r_2}\cdots\mathbf{P}_{r_k}$ is defined in Equation~\eqref{eq:g}. Then we have the following propagation process:
\begin{equation} \label{eq:pshgcn_scale}
    \mathbf{Z}=f_{\theta}(\mathbf{Y}), \mathbf{Y} = c_0\mathbf{X}+\sum\limits_{k=1}^{K}\sum c_i\mathbf{P}_{r_1}\mathbf{P}_{r_2}\cdots\mathbf{P}_{r_k}\mathbf{X},
\end{equation}
where $c_i$ denotes the multiplication between the coefficients in $g(\mathbf{P}_1,\mathbf{P}_2,\cdots,\mathbf{P}_R)^{\top}$ and $g(\mathbf{P}_1,\mathbf{P}_2,\cdots,\mathbf{P}_R)$ when we expand their multiplication, such as $c_0 = w_0^2$. The pre-computed $\mathbf{P}_{r_1}\mathbf{P}_{r_2}\cdots\mathbf{P}_{r_k}\mathbf{X}$ allows us to train PSHGCN in a mini-batch manner.  In our experiments, we assess the scalability of PSHGCN on ogbn-mag and find that PSHGCN achieves a new SOTA result. 











\section{Related Work}
\textbf{Graph Neural Networks}. Graph neural networks (GNNs) are machine-learning techniques designed specifically for graph data. These methods aim to find a low-dimensional vector representation for each node, enabling efficient processing for various network mining tasks. GNNs can be broadly classified into two categories: spatial-based and spectral-based approaches~\cite{wu2020comprehensive}. 

Spatial-based GNNs employ direct propagation and aggregation of information within the spatial domain. From this viewpoint, GCN~\cite{gcn} can be interpreted as aggregating one-hop neighbor information in the graph. GAT~\cite{gat} leverages attention mechanisms to learn aggregation weights. 


Spectral-based GNNs utilize spectral graph convolutions/filters designed in the spectral domain. ChebNet~\cite{Chebnet} employs Chebyshev polynomials to approximate filters. GCN~\cite{gcn} simplifies the Chebyshev filter by utilizing a first-order approximation. APPNP~\cite{appnp} uses Personalized PageRank (PPR) to determine the filter weights.  GPR-GNN~\cite{gprgnn} learns polynomial filters by employing gradient descent on the polynomial weights. BernNet~\cite{bernnet} utilizes the Bernstein basis for approximating graph convolutions, enabling the learning of arbitrary graph filters.
JacobiConv~\cite{jacobi} and ChebNetII~\cite{chebnetii} use Jacobi polynomials and Chebyshev interpolation, respectively, to learn filters. These methods are designed for homogeneous graphs and may not perform optimally on heterogeneous graphs.

\textbf{Heterogeneous Graph Neural Networks}. 
Heterogeneous graphs encompass various types of nodes and edges, offering rich semantic information alongside structural details. Heterogeneous graph neural networks (HGNNs) are explicitly developed to address the challenges posed by heterogeneity. HGNNs can be broadly categorized into meta-path-based and meta-path-free HGNNs~\cite{sehgnn}. 

Meta-path-based HGNNs propagate and aggregate neighbor features using selected meta-paths. HAN~\cite{han} uses a hierarchical attention mechanism with multiple meta-paths for aggregating node features and semantic information. HetGNN~\cite{hetgnn} employs random walks to generate node neighbors and aggregates their features. MAGNN~\cite{magnn} encodes information from manually-selected meta-paths instead of just focusing on endpoints. SeHGNN~\cite{sehgnn} utilizes predetermined meta-paths for neighbor aggregations and applies a transformer-based approach for feature fusion. 

Meta-path-free HGNNs propagate and aggregate messages from neighboring nodes in a manner similar to GNNs, without requiring a selected meta-path. RGCN~\cite{rgcn} extends GCN~\cite{gcn} to heterogeneous graphs with edge type-specific graph convolutions. GTN~\cite{gtn} utilizes soft sub-graph selection and matrix multiplication to generate meta-path neighbor graphs. HGB~\cite{hgb} incorporates a multi-layer GAT network with attention based on node features and learnable edge-type embeddings. MHGCN~\cite{mhgcn} learns meta-paths through multi-layer convolution aggregation and employs GCN's convolution for feature aggregation. EMRGNN~\cite{emrgnn} and HALO~\cite{halo} propose optimization objectives tailored for heterogeneous graphs and design new HGNN architectures based on solving these optimization problems. All the HGNNs design their propagation modules within the spatial domain, which leads to a deficiency in theoretical explanation and controllability. This motivates the design of the proposed PSHGCN, which extends spectral-based GNNs to heterogeneous graphs, enabling the learning of spectral heterogeneous graph convolutions.



\begin{table}[t]
\vspace{-3mm}
\caption{Statistics of datasets.}
\centering
\begin{tabular}{@{}lrcrclc@{}}
\toprule
Dataset  & \#Nodes & \begin{tabular}[c]{@{}c@{}}\#Node\\ Types\end{tabular} & \#Edges & \begin{tabular}[c]{@{}c@{}}\#Edges\\ Types\end{tabular} &Target & \#Classes \\ \midrule
DBLP &26,128 &4 &239,566 &6 &author &4 \\
ACM  &10,942 &4 &547,872 &8 &paper &3 \\
IMDB &21,420 &4 &86,642 &6 &movie &5 \\
AMiner &55,783 &3 &153,676 &4 &paper &4\\  
Ogbn-mag &1,939,743 &4  &21,111,007 &4 &paper &349 \\ 
\bottomrule
\end{tabular}
\label{dataset}
\vspace{-4mm}
\end{table}


\section{Experiments}\label{sec:experiments}
In this section, we conduct experiments to evaluate the performance of SHGCN against the state-of-the-art heterogeneous graph neural networks on a wide variety of open graph datasets.

\textbf{Datasets and Experimental setup.}
We evaluate PSHGCN on five widely-used heterogeneous graphs for node classification tasks. The datasets include three academic citation heterogeneous graphs DBLP~\cite{hgb}, ACM~\cite{hgb} and AMiner~\cite{HeCo}, a movie rating heterogeneous graph IMDB~\cite{hgb}, and a large-scale academic citation heterogeneous graph ogbn-mag from OGB benchmark~\cite{ogb}. We list the details of the datasets in Table~\ref{dataset}. All the experiments are carried out on a machine with an NVIDIA Tesla A100 GPU (80 GB memory), Intel Xeon CPU (2.30 GHz) with 64 cores, and 512 GB of RAM.










\begin{table}[t]
\centering
\small
\caption{Node classification performance (Mean F1 scores $\pm$ standard errors) comparison of different methods on four datasets. The best-performing two results are highlighted.}
\resizebox{\textwidth}{!}{
\begin{tabular}{@{}lcccccccc@{}}
\toprule
       & \multicolumn{2}{c}{DBLP} & \multicolumn{2}{c}{ACM} & \multicolumn{2}{c}{IMDB} & \multicolumn{2}{c}{AMiner} 
\\ \midrule
       &Macro-F1 &Micro-F1 &Macro-F1 &Micro-F1 &Macro-F1 &Micro-F1 &Macro-F1 &Micro-F1 
       \\ \midrule
GCN   &90.84$_{\pm\text{0.32}}$ &91.47$_{\pm\text{0.34}}$ & 92.17$_{\pm\text{0.24}}$ & 92.12$_{\pm\text{0.23}}$ & 57.88$_{\pm\text{1.18}}$ & 64.82$_{\pm\text{0.64}}$  &75.63$_{\pm\text{1.08}}$ &85.77$_{\pm\text{0.43}}$
\\
GAT   &93.83$_{\pm\text{0.27}}$ &93.39$_{\pm\text{0.30}}$ & 92.26$_{\pm\text{0.94}}$ & 92.19$_{\pm\text{0.93}}$ & 58.94$_{\pm\text{1.35}}$ & 64.86$_{\pm\text{0.43}}$  &75.23$_{\pm\text{0.60}}$ &85.56$_{\pm\text{0.65}}$
\\
GPRGNN   &91.66$_{\pm\text{1.01}}$ &92.45$_{\pm\text{0.76}}$ & 92.36$_{\pm\text{0.28}}$ & 92.28$_{\pm\text{0.27}}$ & 58.90$_{\pm\text{1.15}}$ & 64.84$_{\pm\text{0.81}}$ &75.32$_{\pm\text{0.67}}$ &86.13$_{\pm\text{0.58}}$
\\
ChebNetII &92.05$_{\pm\text{0.53}}$ &92.97$_{\pm\text{0.48}}$ & 92.45$_{\pm\text{0.37}}$ & 92.33$_{\pm\text{0.38}}$ & 58.07$_{\pm\text{1.34}}$ & 64.79$_{\pm\text{0.89}}$ &75.59$_{\pm\text{0.73}}$ &85.82$_{\pm\text{0.52}}$
\\
\midrule
RGCN   &91.52$_{\pm\text{0.50}}$  &92.07$_{\pm\text{0.50}}$ & 91.55$_{\pm\text{0.74}}$ & 91.41$_{\pm\text{0.75}}$ & 58.85$_{\pm\text{0.26}}$ & 62.05$_{\pm\text{0.15}}$  &63.03$_{\pm\text{2.27}}$ &82.79$_{\pm\text{1.12}}$\\

HAN   &91.67$_{\pm\text{0.49}}$ &92.05$_{\pm\text{0.62}}$ & 90.89$_{\pm\text{0.43}}$ & 90.79$_{\pm\text{0.43}}$ & 57.74$_{\pm\text{0.96}}$ & 64.63$_{\pm\text{0.58}}$ &63.86$_{\pm\text{2.15}}$ &82.95$_{\pm\text{1.33}}$ \\

GTN   &93.52$_{\pm\text{0.55}}$ &93.97$_{\pm\text{0.54}}$ & 91.31$_{\pm\text{0.70}}$ & 91.20$_{\pm\text{0.71}}$ & 60.47$_{\pm\text{0.98}}$ & 65.14$_{\pm\text{0.45}}$ &72.39$_{\pm\text{1.79}}$ &84.74$_{\pm\text{1.24}}$ 
\\
MAGNN &93.28$_{\pm\text{0.51}}$ &93.76$_{\pm\text{0.45}}$ & 90.88$_{\pm\text{0.64}}$ & 90.77$_{\pm\text{0.65}}$ & 56.49$_{\pm\text{3.20}}$ & 64.67$_{\pm\text{1.67}}$ &71.56$_{\pm\text{1.63}}$ &83.48$_{\pm\text{1.37}}$ \\

EMRGNN &92.19$_{\pm\text{0.38}}$ &92.57$_{\pm\text{0.37}}$ & 92.93$_{\pm\text{0.34}}$ & 93.85$_{\pm\text{0.33}}$ & 61.87$_{\pm\text{2.03}}$ & 65.86$_{\pm\text{0.81}}$ &73.74$_{\pm\text{1.25}}$ &85.46$_{\pm\text{0.74}}$\\

MHGCN &93.56$_{\pm\text{0.41}}$ &94.03$_{\pm\text{0.43}}$ & 92.12$_{\pm\text{0.66}}$ & 91.97$_{\pm\text{0.68}}$ & 62.85$_{\pm\text{1.11}}$ & 66.57$_{\pm\text{0.63}}$ &73.56$_{\pm\text{1.75}}$ &85.18$_{\pm\text{1.28}}$\\

HGB   &94.01$_{\pm\text{0.24}}$ &94.46$_{\pm\text{0.22}}$ & 93.42$_{\pm\text{0.44}}$ & 93.35$_{\pm\text{0.45}}$ & 63.53$_{\pm\text{1.36}}$ & 67.36$_{\pm\text{0.57}}$ &75.43$_{\pm\text{0.88}}$ &86.52$_{\pm\text{0.73}}$ 
\\

HALO   &92.37$_{\pm\text{0.32}}$ &92.84$_{\pm\text{0.34}}$ & 93.05$_{\pm\text{0.31}}$ & 92.96$_{\pm\text{0.33}}$ & \underline{71.63$_{\pm\text{0.77}}$} & \underline{73.81$_{\pm\text{0.72}}$} &74.91$_{\pm\text{1.23}}$
&\underline{87.25$_{\pm\text{0.89}}$} 
\\
SeHGNN &\underline{95.06$_{\pm\text{0.17}}$} &\underline{95.42$_{\pm\text{0.17}}$} & \underline{94.05$_{\pm\text{0.35}}$} & \underline{93.98$_{\pm\text{0.36}}$} & 67.11$_{\pm\text{0.25}}$ & 69.17$_{\pm\text{0.43}}$
&\underline{76.83$_{\pm\text{0.57}}$}
&86.96$_{\pm\text{0.64}}$
\\
PSHGCN &\textbf{95.21}$_{\pm\textbf{0.15}}$ &\textbf{95.53}$_{\pm\textbf{0.14}}$ & \textbf{94.35}$_{\pm\textbf{0.23}}$ & \textbf{94.27}$_{\pm\textbf{0.23}}$ & \textbf{72.33}$_{\pm\textbf{0.57}}$ & \textbf{74.46}$_{\pm\textbf{0.32}}$ &\textbf{77.26}$_{\pm\textbf{0.75}}$
&\textbf{88.21}$_{\pm\textbf{0.31}}$ \\

\bottomrule
\end{tabular}}
\vspace{-6mm}
\label{tb:no_class_res}
\end{table}




\subsection{Node Classification}\label{se:node}
\textbf{Setting and baselines.} For the node classification task, we first compare PSHGCN to 
four popular homogeneous GNNs, including GCN\cite{gcn}, GAT~\cite{gat}, GPRGNN~\cite{gprgnn} and ChebNetII~\cite{chebnetii}, where GPRGNN and ChebNetII are two famous spectral-based GNNs. Additionally, we also compare PSHGCN to nine competitive HGNNs, including RGCN~\cite{rgcn}, HAN~\cite{han}, GTN~\cite{gtn}, MAGNN~\cite{magnn}, EMRGNN~\cite{emrgnn}, MHGCN~\cite{mhgcn}, HGB~\cite{hgb}, HALO~\cite{halo} and SeHGNN~\cite{sehgnn}. To ensure a fair comparison, we adopt the experimental setup used in the HGB benchmark~\cite{hgb}. Specifically, we follow the standard split with the training/validation/test sets accounting for 24\%/6\%/70\%. We use the existing baseline results provided by HGB~\cite{hgb}. For results that are not available, we use the officially released code and conduct a hyperparameter search based on the guidelines presented in their respective paper.


For PSHGCN, we use Equation~\eqref{eq:pshgcn} as the propagation process and set $\mathbf{P}_r$ to normalized adjacency matrix $\hat{\mathbf{A}}_r$. Since these datasets have diverse node types with varying dimensional features, we initially use feature projection to align them into the same data space, following a common practice in HGNNs~\cite{sehgnn,hgb,emrgnn}. We optimize the order $K$ from 1 to 5 in the polynomial $g$. We use a uniform distribution to randomly initialize the weights $w$ in $g$ and optimize them using gradient descent, consistent with spectral-based GNNs~\cite{gprgnn,bernnet,chebnetii}. More details of hyper-parameters and settings are listed in supplementary materials. 



\textbf{Results.} We use the mean F1 scores with standard errors over five runs as the evaluation metric. Table~\ref{tb:no_class_res} reports the results, and the best-performing two results are highlighted by boldface letters and underlining, respectively. We observe that the spectral-based GNNs, specially designed for homogeneous graphs, outperform certain HGNNs, indicating their promising effectiveness on heterogeneous graphs. Additionally, PSHGCN outperforms other methods on all datasets. The superiority of PSHGCN can be attributed to its capability of approximating various valid graph convolutions through learning the weights $w$ in $g$ and imposing the constraint of positive semidefiniteness on the learned convolution.




\begin{wraptable}{r}{65mm}
\centering
\small
\vspace{-4mm}
\caption{The experimental results (Mean accuracies $\pm$ standard errors) on ogbn-mag compared with baselines on the OGB leaderboard, where the symbol "$^\ast$" denotes the usage of extra embeddings and multi-stage training.}
\vspace{-2mm}
\setlength{\tabcolsep}{0.85mm}{
\begin{tabular}{lcc}
\toprule
Methods & Validation accuracy & Test accuracy \\ \midrule
RGCN    &48.35$_{\pm\text{0.36}}$    & 47.37$_{\pm\text{0.48}}$    \\
HGT     &49.89$_{\pm\text{0.47}}$    & 49.27$_{\pm\text{0.61}}$    \\
NARS    &51.85$_{\pm\text{0.08}}$    & 50.88$_{\pm\text{0.12}}$    \\
SAGN    &52.25$_{\pm\text{0.30}}$    & 51.17$_{\pm\text{0.32}}$    \\
GAMLP   &53.23$_{\pm\text{0.41}}$    & 51.63$_{\pm\text{0.22}}$    \\
SeHGNN   &55.95$_{\pm\text{0.11}}$    & 53.99$_{\pm\text{0.18}}$    \\
PSHGCN  &\textbf{56.16}$_{\pm\textbf{0.21}}$    & \textbf{54.57}$_{\pm\textbf{0.16}}$    \\ 
\midrule
SAGN$^\ast$    &55.91$_{\pm\text{0.17}}$    & 54.40$_{\pm\text{0.15}}$ \\
GAMLP$^\ast$   &57.02$_{\pm\text{0.41}}$    & 55.90$_{\pm\text{ 0.27}}$    \\
SeHGNN$^\ast$  &59.17$_{\pm\text{0.09}}$    & 57.19$_{\pm\text{0.12}}$    \\
PSHGCN$^\ast$  &\textbf{59.43}$_{\pm\textbf{0.15}}$     & \textbf{57.52}$_{\pm\textbf{0.11}}$ \\
\bottomrule
\end{tabular}}
\vspace{-4mm}
\label{tb:mag_res}
\end{wraptable}


\subsection{Scalability}\label{se:scale}
To evaluate the scalability of PSHGCN, we conduct a node classification task on the large-scale heterogeneous graph ogbn-mag from the Open Graph Benchmark (OGB). We compare six  baselines listed on the OGB leaderboard: RGCN~\cite{rgcn}, HGT~\cite{hgt}, NARS~\cite{nars}, SAGN~\cite{sagn}, GAMLP~\cite{gamlp} and SeHGNN~\cite{sehgnn}. We use results on the leaderboard for these baselines. For PSHGCN, we use the process discussed in Section\ref{se:scale}, and more settings details are listed in supplementary materials.


Table~\ref{tb:mag_res} shows the mean accuracies with standard errors over five runs. We use the symbol $^{\ast}$ to denote the usage of extra embeddings (e.g., ComplEx embedding) and multi-stage training, which are commonly used in the baselines. We observe that PSHGCN achieves a new SOTA result on ogbn-mag, highlighting its strong scalability and effectiveness.


\subsection{Ablation Study}\label{se:ab}
We investigate the impact of the order $K$ in the polynomial $g$ on the performance of PSGCN. Figure~\ref{fig:f1-K} displays the node classification F1 scores with respect to the order $K$ on ACM and AMiner (more results are listed in supplementary materials). We find that the performance of PSHGCN increases gradually with increasing $K$, which is consistent with the theory of polynomial approximation in graph convolution. Notably, using excessively $K$ may result in too many weights, which can make the learning process challenging.

We explore the impact of the positive semidefinite constraint on PSHGCN by removing $g(\mathbf{P}_1,\cdots,\mathbf{P}_R)^{\top}$ in Equation~\eqref{eq:pshgcn}. For this variant, we search the optimal order $K$ from 2 to 10 and optimize the hyperparameters following the same procedure as PSHGCN. Table~\ref{tb:ab_res} presents the classification results of PSHGCN and the variant. We observe that PSHGCN outperforms this variant, particularly with lower standard errors. These findings highlight the effectiveness of our proposed positive semidefinite constraint in enhancing the capability of learning the convolution weights.


\begin{figure}[t]
    \centering
    \vspace{-6mm}
\subfigure[ACM]{
   \includegraphics[width=64.5mm]{acm.pdf}\label{fig:acm}
   }
\subfigure[AMiner]{
   \includegraphics[width=64.5mm]{aminer.pdf}\label{fig:aminer}}
   \vspace{-2mm}
   \caption{Classification performance of PSHGCN with respect to the order $K$.}
   \vspace{-5mm}
   \label{fig:f1-K}
\end{figure}

\begin{table}[t]
\centering
\small
\caption{Node classification performance comparison of PSHGCN and its variant on four datasets.}
\resizebox{\textwidth}{!}{
\begin{tabular}{@{}lcccccccc@{}}
\toprule
       & \multicolumn{2}{c}{DBLP} & \multicolumn{2}{c}{ACM} & \multicolumn{2}{c}{IMDB} & \multicolumn{2}{c}{AMiner} 
\\ \midrule
       &Macro-F1 &Micro-F1 &Macro-F1 &Micro-F1 &Macro-F1 &Micro-F1 &Macro-F1 &Micro-F1 
       \\ \midrule

PSHGCN &\textbf{95.21}$_{\pm\textbf{0.15}}$ &\textbf{95.53}$_{\pm\textbf{0.14}}$ & \textbf{94.35}$_{\pm\textbf{0.23}}$ & \textbf{94.27}$_{\pm\textbf{0.23}}$ & \textbf{72.33}$_{\pm\textbf{0.57}}$ & \textbf{74.46}$_{\pm\textbf{0.32}}$ &\textbf{77.26}$_{\pm\textbf{0.75}}$
&\textbf{88.21}$_{\pm\textbf{0.31}}$ \\

VARIANT  &94.14$_{\pm\text{0.41}}$ &94.57$_{\pm\text{0.39}}$ & 94.03$_{\pm\text{0.43}}$ & 93.95$_{\pm\text{0.43}}$ & 71.35$_{\pm\text{1.07}}$ & 73.71$_{\pm\text{0.81}}$ &75.80$_{\pm\text{1.79}}$ &87.26$_{\pm\text{0.35}}$ 
\\

\bottomrule
\end{tabular}}
\vspace{-4mm}
\label{tb:ab_res}
\end{table}



\section{Discussion \& Conclusion}
This paper introduces PSHGCN, a novel heterogeneous convolutional network that extends spectral-based GNNs to heterogeneous graphs. Through the utilization of positive noncommutative polynomials, PSHGCN enables effective learning of diverse spectral heterogeneous graph convolutions. Experimental results demonstrate that PSHGCN achieves superior performance in node classification tasks compared to existing methods. Notably, PSHGCN is the first model that attempts to learn polynomial spectral graph convolutions on heterogeneous graphs, opening up various avenues for future exploration. For example, challenges remain when using PSHGCN to handle heterogeneous graphs with numerous node types and edge types, a predicament faced by most existing HGNNs. There are also problems regarding the definition of the Fourier transform on heterogeneous graphs and the utilization of other multivariable polynomial bases to learn the heterogeneous convolutions. 



\newpage
\clearpage
\bibliography{ref}
\bibliographystyle{plain}

\end{document}