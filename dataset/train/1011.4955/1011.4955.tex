\section{Introduction}


Proximity queries are ubiquitous in science and engineering, and given
their natural importance they have received a lot of attention from
the computer science community~\citep{Clarkson99, Clarkson06, Indyk04,
  SDI05}. {\em Nearest Neighbor} (\nn) search is certainly among the
most popular ones. Given a finite set  with  points sitting in
some metric space , the goal is to preprocess  in such a
way that, for any query point , a nearest neighbor of 
among the set  can be found quickly.  The \nn query
can be easily answered in linear time by brute force search, so the
algorithmic challenge is to preprocess the data points so as to find
the answer in sub-linear time. Numerous methods have been proposed,
however their performances degrade significantly when the
dimensionality  of the data increases --- a phenomenon known as the
{\em curse of dimensionality}.  Typically, they suffer
from either space or query time that is exponential in  , and so
they become no better than brute-force search when  becomes higher
than a few dozens or hundreds~\cite{WSB98}.

In light of the apparent hardness of \nn search, an approximate
version of the problem called -\nn has been considered, where the
answer can be any point of  whose distance to  is
within a given factor  of the true nearest neighbor
distance~\citep{AMNSW98, Clarkson94, IM98, Kleinberg97,
  KOR98}. Inspired from the random projection techniques developed by
\citet{Kleinberg97}, \citet{IM98} and \citet{KOR98} proposed data
structures to answer -\nn queries with truly sublinear runtime and
fully polynomial space complexity. The approach developped
in~\cite{IM98} is based on the idea of {\em Locality-Sensitive
  Hashing} (LSH), which consists in hashing the data and query points
into a collection of tables indexed by random hash functions, such
that the query point  has more chance to collide with nearby data
points than with data points lying far away. This technique solves a
decision version of the -\nn problem called {\em Point Location
  among Equal Balls} (-\pleb), which asks to decide whether
the distance of  to  is below a given threshold
 or above .  The output is proven correct with high
probability, and the query time is bounded by 
for some constant . Moreover,
\citet{IM98} proposed a reduction of -\nn search to a
poly-logarithmic number of -\pleb queries, thus providing a
fully sublinear-time and polynomial-space procedure for solving
-\nn. Although originally designed for the Hamming cube, LSH was
later extended~\cite{AI06, DIIM04, HarPeledIndykMotwani} to affine
spaces  equipped with -norms, .


In this paper we mainly focus on the reverse problem, known as {\em
  Reverse Nearest Neighbors} (\rnn) search. Given a finite set 
with  points sitting in some metric space , the goal is to
preprocess  in such a way that, for any query point , one
can find the {\em influence set} of , i.e. the set 
formed by the points  that are closer to 
than to . Such points are called {\em reverse
  nearest neighbors} of .  \rnn queries arise in many different
contexts, and it is no surprise that they have
received a lot of attention since their formal introduction
by~\citet{KM00}. A wealth of methods have been
proposed~\citep{ABKKPR06,BJKS06,Clarkson03,FP09,
  KMSXZ07,KM00,KJG08,SFT03,SAA00,TPL04,TYM06}, which behave well in
practice on some classes of inputs. However, these methods are mostly
heuristic, and to date very little is known about the theoretical
complexity of \rnn search, except in low~\citep{CDLSV09,MVZ02} or
fixed~\citep{CVY09} dimensions, where the dimensionality of the data
can be considered as a mere constant. The crux of the matter is that,
in contrast to (-)\nn search, the answer to an \rnn query is not a
single point but a set of points, whose size can be up to exponential
in the ambient dimension \citep{PZ04}, so there is no way to achieve a
systematic sub-linear query time. Ideally, one would like to achieve a
query time of the form , where
 is a constant less than  and  is the size
of the reverse nearest neighbors set. The big- notation may
hide extra factors that are polynomial in  and
poly-logarithmic in .  Intuitively, the first term in the bound
would represent the incompressible time needed to locate the query
point  with respect to the point cloud , as in a standard \nn
query, while the second term would represent the size of the
sought-for answer.

\paragraph*{Our contributions.}
Our main contribution (see Section~\ref{sec:rnn}) is a reduction of
\rnn search to one instance of -\nn search plus a poly-logarithmic
number of instances of {\em exhaustive -\pleb}, 
a set-theoretic version of \pleb where not only one -ball
containing the query point  is sought for, but all such balls. Our
reduction is based on a partitioning of the data points into buckets
according to their nearest neighbor distances, combined with a pruning
strategy that prevents the inspection of too many buckets at query
time.

Turning our reduction into an effective algorithm for \rnn search
requires to adapt the LSH scheme to solve exhaustive -\pleb
queries. Such an adatptation was proposed in~\cite[Chapter~1]{SDI05},
with expected query time , where  and where  is a user-defined
parameter. Even though the ouput of the query is the set
, the query time depends on the size of the superset
, and when choosing  the user must find a
trade-off between increasing the size of  and
increasing the average retrieval cost  per point of
. In Section~\ref{sec:all-re-nn} we revisit the
analysis of~\cite[Chapter~1]{SDI05} using a somewhat finer concept of
locality-sensitive hashing (see
Definition~\ref{def:stronglshashfunc}), which enables us to quantify
more precisely the amount of collisions with the query point that may
occur within the hash tables stored in the LSH data structure. Taking
advantage of this refined analysis, we propose a simple extra
preprocessing step that reduces the average retrieval cost per point
of  down to  for some constant
, thereby making the previous trade-off no
longer necessary. The price to pay is a slight degradation of the
absolute term  in the complexity bound, which rises to
 where 
(Theorem~\ref{thm:allnn_Rd}). All in all, the query time bound becomes
 and
therefore remains sublinear in  as long as .  Intuitively, our extra preprocessing step consists
in lifting the point cloud  and query point  one dimension
higher through some highly non-isometric embedding, so that the
induced metric distortion moves  away from  and further
concentrates the distribution of the distances to  around the
parameter value , thereby reducing the total number of collisions
with  within the hash tables. The output of the query can still be
proven correct thanks to the fact that the embedding preserves the
order of the distances to . This approach stands in contrast to the
general trend of applying low-distortion embeddings to solve proximity
queries.

Down the road, these advances lead to an algorithm for solving \rnn
queries with high probability in expected  time using fully polynomial space, where 
is a user-defined parameter and  is a superset
of  whose points are -close to being true
reverse nearest neighbors of  (Theorem~\ref{thm:rnnquerytime}). To
the best of our knowledge, this is the first algorithm for answering
\rnn queries that is provably correct and efficient in
all dimensions. Furthermore, the algorithm and its analysis extend
naturally to the bichromatic setting where the data points are split
into two disjoint categories, e.g. clients and servers, a scenario
that is encountered in various applications~\cite{KM00}.

Along the way, in Section~\ref{sec:exact-nn} we obtain a simple
algorithm that can answer exact \nn queries in expected  time using
fully polynomial space, where  is a user-defined parameter and
 is a set of approximate nearest neighbors of 
(Theorem~\ref{thm:exact-nn}). The first term in the running time bound
corresponds to a standard -\nn query, while the second term is
parametrized by the size of , which thereby plays
the role of a {\em condition number} measuring the discrepancy in
difficulty between the exact and approximate \nn queries on a given
instance. Note that our algorithm is not expected to perform as well
as state-of-the-art techniques in growth-restricted
spaces~\cite{Clarkson99, hkmr-nnngrm-04, kr-fnngrm-02, kl-ns-04},
however its complexity bounds hold in a more general setting and its
sublinear behavior on a particular instance relies on the weaker
hypothesis that the condition number of this instance lies below the
threshold . In the same spirit, \citet{DIIM04} designed a
lightweight version of our algorithm that only works in Euclidean
spaces but is competitive with~\cite{Clarkson99, hkmr-nnngrm-04,
  kr-fnngrm-02, kl-ns-04}.

Throughout the paper, the analysis is carried out either in full
generality in metric spaces that admit locality-sensitive families of
hash functions, or more precisely in  when liftings of
the data one dimension higher come into play.  The case of the
-dimensional Hamming cube is also encompassed by our analysis since
this space embeds itself isometrically into .


\section{Preliminaries}\label{sec:prelim}

In Section~\ref{sec:defs} we introduce some useful notation and state
the nearest neighbor and reverse nearest neighbors problems
formally. In Sections~\ref{sec:renn-to-enn}
through~\ref{sec:pleb_affine} we give an overview of LSH and its
application to approximate nearest neighbor search, with a special
emphasis on the case of affine spaces  equipped with
-norms in Section~\ref{sec:pleb_affine}. The data structures
and algorithms introduced in this section are used as black-boxes in
the rest of the paper.

\subsection{Problem statements and notations}
\label{sec:defs}

Throughout the paper,  denotes a metric space and  a
finite subset of .  Given a point , let  denote
the distance of  to , that is:  Given a
parameter , let  denote the metric ball of
center  and radius , and let  be the set of
points of  that lie within this ball.  Then,
 is the set of {\em nearest neighbors} of
 among , noted . By analogy, given a
parameter ,  denotes the set
 of {\em -nearest neigbors} of 
among . The usual convention is that point  itself
is excluded from these sets, which is not mentioned explicitly in our
notations for simplicity but will be admitted implicitly throughout the paper.
\begin{problem}[\nn] \label{problem:nn}
  Given a query point , the {\em nearest neighbor query} asks
  to return any point of .
\end{problem}
\begin{problem}[-\nn] \label{problem:enn}
  Given a query point , the {\em -nearest neighbor query}
  asks to return any point of .
\end{problem}
Given now a point , let  denote the set of {\em
  reverse nearest neighbors} of  among , which by
definition are the points  such that . By analogy, let  denote the set of
     {\em reverse -nearest neighbors} of  among
     , which by definition are the points  such that . Here
     again, point  itself is excluded from the various sets, a fact
     omitted in our notations for simplicity but admitted
     implicitly.
\begin{problem}[\rnn] \label{problem:rnn}
  Given a query point , the {\em reverse nearest neighbors
    query} asks to retrieve the set .
\end{problem}



\subsection{Reducing approximate nearest neighbor search to its decision version}
\label{sec:renn-to-enn}

Given a parameter , the decision version of
Problem~\ref{problem:nn} consists in deciding whether  is
smaller or larger than . This problem is also known as {\em Point
  Location among Equal -Balls} (-\pleb) in the literature, because
it is equivalent to deciding whether  lies inside the union of
balls of same radius  about the points of . It is
formalized as follows:
\begin{problem}[-\pleb] \label{problem:pleb}
  Given a query point , the {\em -\pleb} query asks
  the following:
  \begin{slist}
  \item[] if , then return {\em
    YES} and any point  such that ;
  \item[] else {\rm ()}, return {\em NO}.
  \end{slist}
\end{problem}
By analogy, the decision version of Problem~\ref{problem:enn} consists
in deciding whether  is smaller than  or larger than
. If it lies between these two bounds, then any answer is
acceptable.  The formal statement is the following:
\begin{problem}[-\pleb] \label{problem:epleb}
  Given a query point , the
  {\em -\pleb}  query asks the following:
  \begin{slist}
  \item[] if , then return {\em
    YES} and any point  such that ;
  \item[] if , then return {\em NO};
  \item[] else {\rm ()},
    return any of the above answers.
  \end{slist}
\end{problem}


The original LSH paper \citep{IM98} showed a construction that reduces
the -\nn problem to a logarithmic number of -\pleb
queries. Other reductions have since been proposed, and in this paper
we will make use of the following one, introduced by \citet{HarPeled01},
which is simple and works in any metric space. It is based on
a divide-and-conquer strategy, building a tree  of height
, such that each node  is assigned a subset
 and an interval  of possible values for
parameter .  Each -\nn query is performed by traversing down
the search tree , and by answering two -\pleb
queries at each node  to decide (approximately) whether
 belongs to the interval  or not: in the
former case, a simple dichotomy on a geometric progression of values
of  within the interval makes it possible to determine within a
relative error of  where  lies in the interval, and
to return a point of , with a total number of -\pleb queries bounded by ;
in the latter case, the choice of the child of  in which to
continue the search is determined from the output of the two -\pleb queries. In this construction, the ratio 
is guaranteed to be at most a polynomial in , with
bounded degree, so we have .  Thus,
\begin{thm}[see~\cite{HarPeled01}] \label{thm:cnn} 
Given a finite set  with  points, the tree 
stores  data structures for
-\pleb queries per node, and it reduces every -\nn query
to a set of  queries of type
-\pleb.
 \end{thm}

\subsection{Solving -\pleb queries by means of Locality-Sensitive Hashing}
\label{sec:lsh-ernn}

\begin{defn}\label{def:lshashfunc}
  Given a metric space  and two radii , a family
   of hash functions is called {\em
    -sensitive} if there exist quantities  such that ,
  \begin{slist}
  \item[]   ,
  \item[]   ,
  \end{slist}
  where probabilities are given for a random choice of hash
  function  according to some probability distribution
  over the family.
\end{defn}
Intuitively, a -sensitive family of hash functions
distinguishes points that are close together from points
that are far apart. 

Assuming that a -sensitive family  of hash
functions is given, it is possible to answer -\pleb queries in
sub-linear time \citep{GIM99,IM98}. The algorithm proceeds as follows:
\begin{itemize}
\item In the pre-processing phase, it {\em boosts} the sensitivity of
  the family  by building -dimensional vectors  whose coordinate functions  are drawn
  independently at random from . The hash key of a point 
  is now a -dimensional vector , and
  two keys  and  are equal if and only if
   for all . Call  the family of
  such random hash vectors. The algorithm draws  elements  independently from , and it builds the 
  corresponding hash tables . It then hashes each
  data point  into every hash table  using vector
   as the hash key.

\item In the online query phase, the algorithm hashes the query point
   into each of the  hash tables, and it collects all the
  points colliding with  therein, until either some point  has been found or more than  points
  (including duplicates) have been collected in total. In the former
  case the algorithm answers YES and returns , while in the latter
  case it answers NO. It also answers NO if no point of
   has been found after visiting all the hash
  tables.
\end{itemize}

Letting  and
, where ,
one can prove that this procedure gives the correct answer with
constant probability~\cite{GIM99,IM98}. By repeating it 
times, for a fixed constant , one can increase the probability of
success to at least .  Thus,
\begin{thm}[see ~\citep{GIM99,IM98}]\label{th:lsh4renn}
  Given a finite set  with  points in , two parameters
  , and a -sensitive family  of hash
  functions for some constants , the LSH data structure has
  size  and answers -\pleb
  queries correctly with high probability in
  
  time, where .
\end{thm}
Note that the running time bound ignores the time needed to compute
distances and to evaluate hash functions. These typically depend on
the metric space  and hash family  considered.
The probabilities  also depend on
, therefore they may vary with  and .

\subsection{The case of affine spaces}
\label{sec:pleb_affine}

In most of the paper the ambient space  will be the affine
space  equipped with some -norm, , and  will denote the induced distance: , , where  stand for the -th
coordinates of .

In  we use the families of hash functions introduced
by~\citet{DIIM04}\footnote{A possible improvement would be to use the
  hash functions defined by~\citet{AI06} instead, which are known to
  give better complexity bounds. For now we leave this as future
  work.}, which are derived from so-called {\em -stable
  distributions}. A distribution  over the reals is called
-stable if any linear combination  of
finitely many independent variables  with distribution  has
the same distribution as , where  is
a random variable with distribution .  Given such a distribution
, one can build -sensitive families of hash
functions in  for any radius  and any
approximation parameter  as follows. First, rescale the data and
query points so that . Then, choose a real value  and define
a two-parameters family of hash functions
 by
  , where 
  stands for the inner product in . The probability distribution
  over the family is not uniform: the coordinates of vector
   are chosen independently according to , while  is drawn
  uniformly at random from the interval .  The local
    sensitivity of this family depends on the choice of parameter
    . More precisely, according to~\citet{DIIM04}, given two points
    at distance  of each other, the probability (over a random
    choice of hash function) that these points collide is

 where  denotes the probability density function of the
 absolute value of . The probabilities  in
 Theorem~\ref{th:lsh4renn} are then obtained as  and 
 respectively. They do not depend on , thanks to the
 rescaling. Note that they do note depend on the dimension 
 either.

Focusing back on Har-Peled's construction, recall from
Theorem~\ref{thm:cnn} that each node  of the tree  stores
 data structures for answering
-\pleb queries, each of size . Let us point out that by construction the subsets of 
assigned to the sons of  form a partition of . Then, a
recursion gives the following bounds on the size of  and on
the query time\footnote{Our complexity bounds differ from
  the ones of~\citet{HarPeledIndykMotwani} in that the 
  factor in their bounds is replaced by a  factor in ours. This
  difference comes from the fact that we run the LSH procedure
   times, for a fixed constant , to make
  its output correct with probability at least ,
  so the full -\nn algorithm can be correct with probability at
  least , which will be useful in the rest of the
  paper. By contrast, the analysis in~\cite{HarPeledIndykMotwani} only
  runs the LSH procedure  times, to make the -\nn
  algorithm correct with constant probability.}:
\begin{cor}[see~\cite{HarPeledIndykMotwani}]\label{cor:lsh4enn}
  Given a finite set  with  points in , , and a parameter , the tree structure  and
  its associated -\pleb data structures can answer -\nn
  queries correctly with high probability in
   time using
  
  space, where , the quantities
   and  being derived from some -stable
  distribution  according to Eq.~(\ref{eq:proba_collision_ls}).
\end{cor}
Here again the running time bound ignores the time needed to compute
distances and to evaluate hash functions, which is  per
operation (distance computation or hash function evaluation) in
. From now on we will also ignore poly-logarithmic factors in
 and hide them within big- notations for the
sake of simplicity. Thus, the time and space complexities given in
Theorem~\ref{th:lsh4renn} become respectively  and , while those given in Corollary~\ref{cor:lsh4enn}
become respectively 
and .

The challenge now is to choose a value for parameter  that makes
 as small as
possible. The {\em best} value for  heavily depends on  and
, and it may be difficult to find for some values of ,
especially when no closed form solution to
Eq.~(\ref{eq:proba_collision_ls}) is known. Two special cases of
practical interest ( and ) are analyzed
in~\cite{DIIM04}:
\begin{itemize}
\item In the case , one can use the Cauchy distribution (which is
  -stable) to derive a family of hash functions, and the
  probability of collision becomes . The ratio  lies then strictly above , yet
  larger and larger values of parameter  make it closer and
  closer to .
\item In the case , one can use the normal distribution
   (which is -stable), and the probability of
  collision becomes , where
   stands for the cumulative
  distribution function of .  The ratio
   lies then below 
  for reasonably small values of parameter .
\end{itemize}
The results obtained by~\citet{DIIM04} can be extended to any
 via low-distortion embeddings~\cite{js-elpilo}.  In the
rest of the paper we will follow~\cite{DIIM04} and use respectively
the Cauchy distribution and the normal distribution in the cases 
and . An analysis of the influence of the choice of
parameter  on the quantities ,  and
 will be provided in
Section~\ref{sec:erpleb_Rd}.




\section{Exhaustive -\pleb}
\label{sec:all-re-nn}

Let  be a metric space and  a finite subset of . The
following variant of -\pleb, where all the -balls containing the
query point are asked to be retrieved, will play a central part in the
rest of the paper:
\begin{problem}[Exhaustive -\pleb] \label{problem:allnnexact}
  Given a query point , the {\em exhaustive -\pleb} query
  asks to return the set .
\end{problem}
This problem is introduced under the name {\em near-neighbors
  reporting} in previous literature~\cite[Chapter~1]{SDI05}, where a
variant of the LSH scheme of Section~\ref{sec:lsh-ernn} is proposed
for solving it. The difference with the original LSH scheme is that
the query procedure does not stop when  collisions with the query
point  have been found, but instead it continues until all the
points colliding with  in the  hash tables have been
collected. The output is then the subset of these points that lie
within . The details of the pre-processing and query
phases are given in Algorithms~\ref{alg:nnpre} and~\ref{alg:nnquery}
respectively, where the data structure is called . Note
that parameter  no longer controls the quality of the output,
which is shown to coincide with the set  with high
probability, but instead it influences the average complexity of the
procedure, as we will see later on.


\begin{algorithm}[!htb]
\LinesNumbered
  \SetKwInOut{Input}{Input}
  \SetKwInOut{Output}{Output}


  \Input{metric space , finite set  with  points in , parameters
    }
  \Output{ data structure}
  \BlankLine

  Take an -sensitive LSH family \; 
  Let  and , where \;
  Create the -dimensional hash family  as described in Section~\ref{sec:lsh-ernn}\;
  \For{ to   \tcp{ is a constant to be explicited later} } {  
    pick  functions  independently at random from \;
    Create the corresponding hash tables \;
    \ForAll{} {
      \For{ to } {
        Insert  into  using the key \;
      }
    }
    Store the data structure \;
  }
  Output \;


  \caption{\em Pre-processing phase for exhaustive -\pleb}
  \label{alg:nnpre}
\end{algorithm}

\begin{algorithm}[!hbt]
\LinesNumbered
  \SetKwInOut{Input}{Input}
  \SetKwInOut{Output}{Output}


  \Input{metric space ,  data structure, query point } 
\BlankLine

  Let , ,   and  be defined as in Algorithm~\ref{alg:nnpre}\;
  Initialize the output set: \;
  \For{ to } {
    Let   be the functions and
     the tables contained in \;
    \For{ to } {
      Compute  and retrieve the set  of the points colliding with  in \;
      \ForAll{} {
        \If{\nllabel{line:approxnn}} {
          Update the output set:
          \;
        }
      }
    }\nllabel{line:breakjump}
  }
  Return \;


  \caption{\em Online query phase for exhaustive -\pleb}
  \label{alg:nnquery}
\end{algorithm}


In Section~\ref{sec:erpleb_general} we revisit the analysis
of~\cite[Chapters~1 and~3]{SDI05} and quantify more precisely the
amount of collisions with the query point that may occur within the
hash tables. To this end we use the following refined concept of
locality-sensitive family of hash functions\footnote{An even finer
  concept, proposed in~\cite[\textsection~3.3]{SDI05}, makes the
  probability of having  a function of the distance between
   and . However, for our purpose it is not necessary to go to
  this level of refinement.}:
\begin{defn}\label{def:stronglshashfunc}
  Given a metric space  and positive radii , a family  of hash functions is
  called {\em -sensitive} if there exist
  quantities  such that ,
  \begin{slist}
  \item[\rm (i)]   ,
  \item[\rm (ii)]   ,
  \item[\rm (iii)]   ,
  \end{slist}
  where probabilities are given for a random choice of hash function
   according to some probability distribution
  over the family.
\end{defn}
Axioms (i) and (ii) correspond to the classical notion of
locality-sensitive family of hash functions
(Definition~\ref{def:lshashfunc}). They do not make it possible to
limit the number of collisions between the query point  and the
points of  in the analysis of exhaustive
-\pleb queries. Specifically, every point of 
might collide with  in every hash table in theory, thus raising the
cost of an exhaustive -\pleb query to  per
point of . This is in fact all theoretical, since
in practice the hash functions are likely to make a difference between
those points of  that are really close to  and
those that are farther away. This is the reason for introducing the
third axiom (iii), which will prove its usefulness in
Section~\ref{sec:erpleb_Rd}, where we concentrate on the case where
the ambient space is , , and show that a
non-isometric embedding of the data into  enables
us to move the sets of data and query points away from each other.


\subsection{Revisiting the analysis in the general case}
\label{sec:erpleb_general}

\begin{thm} \label{thm:allnn}
  Given a finite set  with  points and two parameters
  , if  admits a -sensitive family  of hash functions with  and
  , then Algorithm~\ref{alg:nnquery} answers
  exhaustive -\pleb queries correctly with high probability in
  expected  time,
  involving  distance computations and  hash function
  evaluations only, and using  space, where
  . If moreover the family  is
  -sensitive for some ,
  then for any query point  the algorithm answers the exhaustive
  -\pleb query in expected 
  time, where .
\end{thm}

The first term () in the
running time bound corresponds to the complexity of a standard
-\pleb query and can be viewed as the incompressible time
needed to locate the query point  in the data structure. The second
term () bounds the total number of collisions
of  with data points lying outside .  The third
term () arises from the
fact that a data point lying within distance  of  may
collide in every single hash table with . Finally, the last term
()
arises from the fact that the points of  that
lie farther than  can only collide up to 
times with  each, for some . Note that the less
sensitive the family  between radii  and , the closer to
 the ratio , and therefore the smaller
 compared to . By contrast, the more sensitive the
family between radii  and , the smaller the ratio
 compared to .

Our proof of Theorem~\ref{thm:allnn} follows previous
literature~\cite{HarPeledIndykMotwani} and is divided into three
parts: (1) proving the correctness of the output of
Algorithm~\ref{alg:nnquery} with high probability, (2) bounding the
expected query time, and (3) bounding the size of the data
structure. The novelty resides in Lemma~\ref{lem:realpos}, which
exploits the axiom (iii) of Definition~\ref{def:stronglshashfunc} to
bound the number of collisions of  with points of
.

\paragraph{Correctness of the output.}
\label{sec:A_correct}
Note that the test on line~\ref{line:approxnn} of
Algorithm~\ref{alg:nnquery} ensures that the output set  is
always a subset of . Thus, we only need to show that
 contains all the points of  with high probability
at the end of the query.
\begin{lem} \label{lem:success}
 with probability at least
  . 
\end{lem}
This result means that the probability of success of the query is
high, even for small values of . For instance, it is at least
 for , and more
generally it is at least  for .
\begin{proof}[Proof of the lemma]
  Let  be a point of . Consider a single iteration
   of the main loop of Algorithm~\ref{alg:nnquery}, and let us show
  that  is inserted in the output set  during this iteration
  with constant probability.  This is equivalent to showing that, with
  constant probability, there exists some function  that
  hashes  and  to the same location (). Since
  , the probability of a collision for a fixed  is at
  least . Therefore, the
  probability that no hash function  generates a collision is at
  most  since  functions are picked from 
  at iteration . Thus, the probability that this iteration inserts
   into the output set  is at least
  .

Now, there are  iterations in total, with
independent hash functions, so the probability that  at the
end of the query is at most . Applying the union bound on the set
, we obtain that the probability that all points of
 belong to  at the end of the query is at least
.
\end{proof}

\begin{remark}\label{rem:increase_n}
It is easily seen from the final paragraph of the proof of
Lemma~\ref{lem:success} that the correctness of the output can be
guaranteed with probability  for any given
. Indeed, by running  iterations of the
main loops of Algorithms~\ref{alg:nnpre} and~\ref{alg:nnquery} instead
of  iterations, we obtain that each point of
 belongs to  at the end of the query with probability
at least , and thus that  with probability at least . This
remark will be useful when dealing with \rnn queries in
Section~\ref{sec:rnn}.
\end{remark}

\paragraph{Expected query time.}
First of all, the query point  is hashed into  hash tables in total, and each hashing
operation involves  hash function evaluations,  being a constant
here. Thus, the total number of hash function evaluations is , and so is the total time
spent hashing  (modulo the time needed to do a hash function
evaluation, which is ignored here as in the previous sections). There
remains to bound the expected number of colllisions of  with points
of  in the hash tables.
\begin{lem} \label{lem:falsepos}
The expected total number of collisions of  with points of
 is .
\end{lem}
\begin{proof}
 Take an arbitrary iteration  of the main loop of
 Algorithm~\ref{alg:nnquery}, and an arbitrary hash table 
 considered during that iteration. Recall that the hash family  is
 constructed in Algorithm~\ref{alg:nnpre} by concatenating
  functions drawn
 from a -sensitive family . Therefore, the
 probability that a given point of 
 collides with  in  is at most .  It follows that the expected number of points
 of  that collide with  in  is
 at most , from which we conclude that the expected
 total number of such collisions in all the hash tables at all
 iterations is at most .
\end{proof}

Without any further assumptions on the family  of hash functions,
each point of  might collide with  in every
hash table. The number of collisions of  with points of
 is therefore . Combined with
Lemma~\ref{lem:falsepos}, this bound implies that the expected running
time of the algorithm is ,
as claimed in the theorem.
For every collision considered, a test is made on the distance between
 and the colliding point of  (see line~\ref{line:approxnn} of
Algorithm~\ref{alg:nnquery}). With a simple book-keeping, e.g. by
marking the points of  that have already been considered during the
query, we can afford to do the test at most once per point of ,
thus yielding a total number of distance computations of the order of
.

Consider now the stronger hypothesis that the family  of hash
functions is -sensitive for some
.
\begin{lem} \label{lem:realpos}
Assuming that  is -sensitive,
the expected total number of collisions of  with points of
 is , where .
\end{lem}
\begin{proof}
 Take an arbitrary iteration  of the main loop of
 Algorithm~\ref{alg:nnquery}, and an arbitrary hash table 
 considered during that iteration. The probability that a given point
  collides with 
 in  is at most .  It follows that the expected total
 number of collisions between  and  during the execution of the
 algorithm is at most , where . We conclude that the
 expected total number of collisions of  with points of
  during the course of
 the algorithm is .
\end{proof}

It follows from Lemma~\ref{lem:realpos} that the expected query time
becomes  when the
family  of hash functions is -sensitive, as claimed in the theorem.

\paragraph{Size of the data structure.}
Each hash table contains one pointer per point of , and there are
 such hash
tables in total, so we need to store  pointers in total. In addition, we need
to store the 
vectors of hash functions corresponding to the hash tables, but this
term is dominated by the previous one. Thus, in total our data
structure has a space complexity of . This bound ignores the costs of storing
the input point cloud and the selected hash functions, which depend on
the type of data representation.



\subsection{Affine case: the non-isometric embedding trick}
\label{sec:erpleb_Rd}

Assume from now on that the ambient space is , where
, and note that axiom~(iii) of
Definition~\ref{def:stronglshashfunc} is satisfied by the families of
hash functions introduced in Section~\ref{sec:pleb_affine} since the
probability  defined in Eq.~(\ref{eq:proba_collision_ls})
decreases as the distance  increases. In order to prevent the
points of  from getting too close to the query point , so
axiom~(iii) can be exploited, our strategy is to 
apply a non-isometric embedding into  that moves
 away from , while preserving the order of the distances to .

At preprocessing time, we lift the points of  to  by adding one coordinate equal to  to every point. We then
build an  data structure using
Algorithm~\ref{alg:nnpre}, where  denotes the image of  through
the embedding, , and
.  In effect, right before
building the data structure we follow
Section~\ref{sec:pleb_affine} and rescale  by a factor of ,
to get a normalized point cloud  on top of which we build
an  data structure using Algorithm~\ref{alg:nnpre}.

At query time, we lift  to  by adding one coordinate
equal to , then we answer an exhaustive
-\pleb query in  by running Algorithm~\ref{alg:nnquery}
with the  data structure, and then we return the
pre-image of the output set through the embedding. Once again, in
effect we rescale the image of the query point in  by a
factor of , so Algorithm~\ref{alg:nnquery} is actually run
with .

Note that the embedding into  is not isometric since it
does not preserve the distances of  to the data points. However, it
does preserve their order. Indeed, for every point  the distance
 becomes 
after the embedding. Since the map
 is monotonically
increasing with , the embedding preserves the order of
distances to . We then have the following easy properties, where  denotes the image of any point  through the
embedding:
\begin{itemize}
\item[\rm (i)] , ;
\item[\rm (ii)] , ;
\item[\rm (iii)] , \\\\\\.
\end{itemize}
It follows from (i) that  is the image of
 through the embedding. Hence, by
Lemma~\ref{lem:success}, with high probability the output set of the
exhaustive -\pleb query in  is the image of
 through the embedding. Thus, our output is correct
with high probability. In the meantime, the embedding has the
following impact on the complexity bounds of Theorem~\ref{thm:allnn}:
\begin{itemize}
\item On the negative side, parameter  is now replaced by
  , which
  increases  from  to . This means
  that the ratio  becomes
   and thus gets closer to , even though it still
  remains strictly below . Furthermore, the term  grows from
   to .
\item On the positive side, we know from (ii) that the points of 
  lie at least  away from the query point , so by
  Lemma~\ref{lem:realpos} they cannot collide with  more than
   times each in expectation, where
  , , and
  .
\end{itemize}
For the rest, the embedding is a neutral operation. Indeed, even
though the complexity now depends on the size of
 instead of the size of
, we know from (iii) that the preimage of the
former set through the embedding is contained within the latter set,
so we have .
In addition, the fact that the query now takes place in 
instead of , with a radius parameter that grew from  to ,
does not affect the probabilities , which depend neither on
the ambient dimension as pointed out after
Eq.~(\ref{eq:proba_collision_ls}), nor on the radius thanks to the
rescaling of the data. It also does not affect the asymptotic complexities
of distance computations and hash function evaluations, which remain
.

All in all, we obtain the following complexity bounds for the
exhaustive -\pleb query in , where 
denotes the full data structure built at preprocessing time, which
contains the embedding and rescaling information together with the
 data structure:
\begin{thm} \label{thm:allnn_Rd}
  Given a finite set  with  points in , , and two parameters , the  data
    structure answers exhaustive -\pleb queries correctly with high
    probability in expected  time using  space, where 
    and , the
    quantities ,  and
     being derived from
    some -stable distribution  according to
    Eq.~(\ref{eq:proba_collision_ls}).
\end{thm}
Quantifying precisely the amounts by which the quantities ,
 and  are affected by the
embedding, what the corresponding {\em best} choice of parameter 
is, and how this choice impacts , are the main
questions at this point. Because Eq~(\ref{eq:proba_collision_ls}) may
not always have a closed form solution, it is difficult to provide an
answer in full generality for all values . We will
  nevertheless investigate two special cases that are of practical
  interest:  and .

\begin{figure}[!htb]
\centering
\includegraphics[height=0.48\textwidth]{images/l1_rho_theorique_epsilon}
\includegraphics[height=0.48\textwidth]{images/l1_rho_theorique_epsilon2}
\caption{\em Behavior of  in . Left: plots
  of  (blue) and 
  (red) versus  and .  Right: plots of  (blue) and
   (red) versus  and .}
\label{fig:l1_rho_theorique}
\end{figure}

\begin{figure}[!htb]
\centering
\includegraphics[height=0.48\textwidth]{images/l1_rho_theorique_epsilon2_fixedw}
\includegraphics[height=0.48\textwidth]{images/l1_alpha_theorique_fixedw}
\caption{\em Behaviors of  and  in  after letting . From left to right, in
  blue: plots of  and
  . Both plots are versus
   on a logarithmic scale (). The red lines have
  equation .}
\label{fig:l1_rho_alpha}
\end{figure}

\begin{figure}[!htb]
\centering
\includegraphics[height=0.48\textwidth]{images/l1_p1_theorique_fixedw}
\includegraphics[height=0.48\textwidth]{images/l1_k_theorique_fixedw}
\caption{\em Behaviors of  and
   in  after
  letting . From left to right, in blue: plots of
   and . Both
  plots are versus  on a logarithmic scale (). The
  red lines have equation .}
\label{fig:l1_p2_k}
\end{figure}

\paragraph{Case .}
The definition of  gives  in this
case. The formula for  is then the same as in , with
 replaced by . As reported in~\cite{DIIM04} and
illustrated in Figure~\ref{fig:l1_rho_theorique} (left), 
remains above , even though it seems to
converge to this quantity as  tends to infinity. Letting
, we found experimentally that  is dominated
by  when  and by
 when , as shown in
Figures~\ref{fig:l1_rho_theorique} (right) and~\ref{fig:l1_rho_alpha}
(left). In the meantime,  is less than , as
can be seen from Figure~\ref{fig:l1_rho_alpha} (right), while
 and  are less
than  and  respectively, as shown in Figure~\ref{fig:l1_p2_k}.
All in all, Theorem~\ref{thm:allnn_Rd} can be re-written as follows:
\addtocounter{thm}{-1}
\begin{thm}[case ] \label{thm:allnn_Rd_l1}
  Given a finite set  with  points in , and two
  parameters , the  data
    structure answers exhaustive -\pleb
  queries correctly with high probability in expected  time using
     space, where  and .
\end{thm}


\begin{figure}[!htb]
\centering
\includegraphics[height=0.48\textwidth]{images/l2_rho_theorique_epsilon}
\includegraphics[height=0.48\textwidth]{images/l2_rho_theorique_epsilon2}
\caption{\em Behavior of  in . Left: plots
  of  (blue) and
   (red) versus 
  and .  Right: plots of  (blue) and
   (red) versus  and .}
\label{fig:l2_rho_theorique}
\end{figure}

\begin{figure}[!htb]
\centering
\includegraphics[height=0.48\textwidth]{images/l2_rho_theorique_epsilon2_fixedw}
\includegraphics[height=0.48\textwidth]{images/l2_alpha_theorique_fixedw}
\caption{\em Behaviors of  and  in  after letting . From left to right, in
  blue: plots of  and
  . Both plots are versus
   on a logarithmic scale (). The red lines have
  equation .}
\label{fig:l2_rho_alpha}
\end{figure}

\begin{figure}[!htb]
\centering
\includegraphics[height=0.48\textwidth]{images/l2_p1_theorique_fixedw}
\includegraphics[height=0.48\textwidth]{images/l2_k_theorique_fixedw}
\caption{\em Behaviors of  and
   in  after
  letting . From left to right, in blue: plots of
   and
  . Both plots are versus
   on a logarithmic scale (). The red lines have
  equation .}
\label{fig:l2_p2_k}
\end{figure}

\paragraph{Case .}
The definition of  gives 
in this case. The formula for  is then the same as in ,
with  replaced by . As pointed
out in~\cite{DIIM04} and illustrated in
Figure~\ref{fig:l2_rho_theorique} (left),  goes below
 at reasonably small values of
parameter . Since this bound is not quite evocative, we used a
slightly different bound, namely , and we
found experimentally that 
whenever , as shown in
Figures~\ref{fig:l2_rho_theorique} (right) and~\ref{fig:l2_rho_alpha}
(left). In the meantime,  is less than , as can be
seen from Figure~\ref{fig:l2_rho_alpha} (right), while the terms
 and  are bounded
by small constants, as shown in Figure~\ref{fig:l2_p2_k}. All in all,
Theorem~\ref{thm:allnn_Rd} can be re-written as follows:
\addtocounter{thm}{-1}
\begin{thm}[case ] \label{thm:allnn_Rd_l2}
  Given a finite set  with  points in , and two
  parameters , the  data
    structure answers exhaustive -\pleb
  queries correctly with high probability in expected  time using
     space, where  and .
\end{thm}




\section{Interlude: from exhaustive -\pleb to exact \nn}
\label{sec:exact-nn}

Before dealing with \rnn queries (the main topic of the paper),
let us show a simple but pedagogical application of exhaustive
-\pleb queries to exact  search. Given a set  with  points
and a user-defined parameter , we will show that \nn queries can
be solved exactly with high probability on any query point  in
expected  time
using  space, for some quantities
 and 
(Theorem~\ref{thm:exact-nn}). The running time bound is composed of
two terms: the first one is sublinear in  and corresponds to a
standard approximate -\nn query using locality-sensitive hashing;
the second one depends on the size of the approximate nearest
neighbors set  and indicates that the solution to
the exact query is sought for among this set. Whether the bound will
be sublinear in  or not in the end depends on the size of the set
compared to the quantity . This follows the intuition
that finding the exact nearest neighbor of  is easy when  does
not have too many approximate nearest neighbors, and in this respect
the quantity  plays the role of a {\em
  condition number} measuring the inherent difficulty of a given
instance of the exact  problem. The interesting point to raise
here is that the limit on this number for our algorithm to be
sublinear is at least of the order of  since we
have .


Let us point out that the above bounds are for the ambient space
 equipped with the - or -norm. Our analysis will
be carried out in the more general setting of an -norm, with
, where we will derive more general complexity bounds.
  The choice of  is mainly for ease of
  exposition, since the algorithm can actually be applied in arbitrary
  metric spaces that admit locality-sensitive families of hash
  functions, where its analysis extends in a straightforward manner
  (see Remark~\ref{rem:exact-NN_general} at the end of the section).


\paragraph{The algorithm.} Let  be a finite set of  points in , , and let  be a parameter. The preprocessing phase
consists of the following steps:
\begin{slist}
\item[i.] Build the tree structure  of
  Section~\ref{sec:renn-to-enn} and its associated -\pleb data
  structures.
\item[ii.] For every -\pleb data structure built on some subset
  of  at step i, build an  data structure using the
  procedure of Section~\ref{sec:erpleb_Rd}.
\end{slist}
Then, given a query point , we proceed as follows:
\begin{slist}
\item[1.] Answer an -\nn query using the tree structure ,
  and let  be the output value.
\item[2.] Answer an exhaustive -\pleb query using the 
  data structure, and let  be the output set.
\item[3.] Iterate over the points of  and return the one that is
  closest to . If  is empty, then return any arbitrary point of .
\end{slist}
Note that the execution of step 2 is
made possible by the fact that the algorithm solving the -\nn
query at step 1 returns a radius  that is stored in one of the
 data structures built during the preprocessing
phase. For any other value  we would not be able to perform step 2
because we would not have the corresponding  data
structure at hand.

\paragraph{Analysis.} We begin by showing the correctness of the 
query procedure:

\begin{lem}\label{lem:exact-nn_correct}
The query procedure returns a point of  with high probability.
\end{lem}
\begin{proof}
Corollary~\ref{cor:lsh4enn} guarantees that the radius  computed at
step 1 satisfies  with high
probability. Under this condition, we have , and so Theorem~\ref{thm:allnn_Rd} guarantees that
the set  computed at step 2 contains  with high
probability. It follows that the point returned at step 3 belongs to
 with high probability.
\end{proof}

We will now analyze the expected running time of the query. Let  be
the -stable distribution used by the algorithm, and let
, ,  and
 be derived from  according
to Eq.~(\ref{eq:proba_collision_ls}). By Corollary~\ref{cor:lsh4enn},
the running time of step 1 is , where . The
running time of step 3 is , so it is dominated by the running
time of step 2.
\begin{lem}\label{lem:exact-nn_runtime}
The expected running time of step 2 is , where
 and .
\end{lem}
\begin{proof}
Let  be the radius computed at step 1. By
Theorem~\ref{thm:allnn_Rd}, the expected running time of step 2 is
. If ,
then we have 
and so the expected running time becomes .  By contrast, if , then we have no bound on the size of
 other than , so the expected running time of
step 2 becomes . Now, recall from Section~\ref{sec:prelim} that the
event that  only occurs with very low
probability, more precisely with probability at most
. Therefore, in total the expected running time of step 2
is bounded by , which is  since the set
 contains at least one point, namely the
nearest neighbor of .
\end{proof}

Let us now focus on the size of the data structure. By
Corollary~\ref{cor:lsh4enn}, the total size of the tree  and
associated -\pleb data structures is . In addition, since  has  nodes
in total, each one storing  data structures
for -\pleb, the total number of  data structures
built at step ii of the preprocessing phase is . Therefore, by Theorem~\ref{thm:allnn_Rd}, the total
memory usage of the  data structures is . 

Observing now that we have  and 
since , we conclude that our
procedure has the following space and time complexities (where 
and  have been renamed respectively  and  for
convenience):
\begin{thm}\label{thm:exact-nn}
  Given a finite set  with  points in , , and a user-defined parameter , our procedure
    answers exact \nn queries with high probability in expected
     time using  space, where  and ,
    the quantities ,  and
     being derived from some
    -stable distribution  according to
    Eq.~(\ref{eq:proba_collision_ls}).
\end{thm}
Replacing Theorem~\ref{thm:allnn_Rd} by its specialized versions for
 and  in the analysis immediately gives the following complexity
bounds:
\addtocounter{thm}{-1}
\begin{thm}[case ]\label{thm:exact-nn_l1}
  Given a finite set  with  points in , and a
  user-defined parameter , our procedure answers exact \nn
  queries with high probability in expected  time using  space, where  and .
\end{thm}
\addtocounter{thm}{-1}
\begin{thm}[case ]\label{thm:exact-nn_l2}
  Given a finite set  with  points in , and a
  user-defined parameter , our procedure answers exact \nn
  queries with high probability in expected  time using  space, where  and .
\end{thm}

Note that in practice a trade-off must be made by the user when
choosing parameter . Indeed, the smaller , the smaller
the set  and the smaller  compared to
, but on the other hand the higher  itself.

\begin{remark}\label{rem:exact-NN_general}
In our analysis we traded optimality for simplicity since we applied
the results from Section~\ref{sec:erpleb_Rd} verbatim. In fact, a
closer look at the problem reveals that the points of  lie at least
 away from the query point  with high
probability at step 2 of the query phase. This means that no
lifting of the data into  is actually needed. We then
have , , and a careful analysis shows that
relevant choices of parameter  reduce  down to (or at
least close to) . In addition and more importantly,
not having to re-embed the data means that the algorithm can be
applied in arbitrary metric spaces  that admit
locality-sensitive families of hash functions, where the analysis
extends in a straightforward manner.
\end{remark}



\section{From exhaustive -\pleb to exact \rnn}
\label{sec:rnn}

In this section we focus on our main problem (\rnn) and show how
it can be reduced to a single instance of -\nn search plus a
controlled number of instances of exhaustive -\pleb. Although the
reduction is applicable in any metric space, we will restrict our
study to the case of  equipped with an -norm, , where the non-isometric embedding trick of
  Section~\ref{sec:erpleb_Rd} can be used to speed-up the process. The
  details of the reduction are given in Section~\ref{sec:rnn_alg}, its
  output proven correct in Section~\ref{rnn_correctness}, and its
  complexity analyzed in Section~\ref{rnn_complexity}. The reduction
  and analysis are then extended to the bichromatic setting in
  Section~\ref{sec:rnn_bichro}.  For now we begin with an overview of
  the reduction and of its key ingredients in
  Section~\ref{sec:rnn_overview}.


\subsection{Overview of the reduction}
\label{sec:rnn_overview}

Let  be a finite set with  points in , .  Suppose the distance of every point  to its nearest
  neighbor in  has been pre-computed. Then, given a query point
  , computing a solution to the \rnn query amounts to
  checking, for every point , whether  or : in the first case,  must be
  included in the solution, whereas in the second case it must
  not. This check for point  can be done by computing the solution
   of the exaustive -\pleb query on input , with
  , and by including  in the answer if and only if it
  belongs to~. Indeed, 
 
Thus, computing the set  boils down to locating  among
the set of balls .  This
observation was exploited in previous work~\cite{KM00} and serves as
the starting point of our approach.  The main problem is that the ball
radius  changes with each data point  considered, so the
total number of exhaustive -\pleb queries to be solved can be up to
linear in . To reduce this number, we allow some degree of
fuzziness and use a bucketing strategy. Given a user-defined parameter
, at pre-processing time we compute and store  for
every point  and then we hash the data points into buckets
according to their nearest neighbor distances, so that bucket 
contains the points  such that
. At query time, we
  solve an exhaustive -\pleb query with  on each
  bucket  separately, then we consider the union  of the
  solutions and prune out those points  such that
  . Since the points  satisfy
  , it is easily seen that
   and that our output
  is an admissible solution to the \rnn query.

A remaining issue is that we do not impose any constraints on
parameter , so at query time we need to inspect every single
non-empty bucket . As a result, in pathological cases such as
when all non-empty buckets are singletons, we will end up considering
a linear number of buckets, even though the set  itself
might be small or even empty. To avoid this pitfall, we limit the
range of values of  to be considered thanks to the following
observations, where  is an arbitrary point of :
\begin{observation}\label{claim:closeby_pts}
  Every point  satisfies .
\end{observation}
\begin{proof}
   Since , we have  and . Moreover, since  and , we
   have . It
   follows that .
 \end{proof}
\begin{observation}\label{claim:far-away_pts}
  Every point  such that  belongs to .
\end{observation}
\begin{proof}
   Since , we have . In addition, we have  by
   hypothesis. Hence, , which means that either  or .
 \end{proof}
Assuming that we have precomputed a data structure that enables us to
find some , Observation~\ref{claim:closeby_pts}
ensures that we can safely ignore the buckets  with
. Furthermore, assuming that
the set  has been precomputed,
Observation~\ref{claim:far-away_pts} ensures that the reverse
nearest neighbors of  that belong to the buckets  with
 can simply be looked for
among the points of . Thus, the total number
of buckets to be inspected is reduced to 
.


\subsection{Details of the reduction}
\label{sec:rnn_alg}

Given a finite set  with  points in , , and a parameter , our pre-computation phase builds a
data structure  that stores the following pieces of
information:
\begin{slist}
\item[i.] A collection of buckets  that
  partition . Each bucket  contains those points  such
  that . To fill in the
  buckets, we iterate over the points , we compute the
  distance  exactly\footnote{This can be done either by
    brute-force or using the algorithm of
    Section~\ref{sec:exact-nn}.} and store it, and then we assign 
  to its corresponding bucket.
Once this is done, the empty buckets are discarded and the non-empty
 buckets are stored in a hash table to ensure constant look-up
 time. On each non-empty bucket  we build an  data structure using the procedure of
 Section~\ref{sec:erpleb_Rd}.
Note that when applying Algorithm~\ref{alg:nnpre} we increase the
 number of iterations of the main loop from 
 to , where .
\item[ii.] For each point , an array  containing the
  points , sorted by increasing
  distances . Building the array takes  time once
   has been computed for all .
\item[iii.] The tree  of
  Section~\ref{sec:renn-to-enn} and its associated -\pleb data
  structures.
\end{slist}

\smallskip

\noindent Given a point , we answer the \rnn
query using the  data structure as follows:
\begin{slist}
\item[1.] We use the tree  and its
  associated -\pleb data structures to answer an -\nn
  query, and we let  be the output point.
\item[2.] We use the  data structure to
  answer an exhaustive -\pleb query on each bucket 
  separately, for  lying in the range prescribed by
  Observations~\ref{claim:closeby_pts} and~\ref{claim:far-away_pts},
and then we merge the output sets into a single set .
Note that when applying Algorithm~\ref{alg:nnquery} on  we
 increase the number of iterations of the main loop from
  to , where
 , which raises the probability of success
 of the query from  (which can be as low as
  when   is a singleton) to .
\item[3.] We add to  the points  s.t.
  . These are found by looking up
  the value  in the sorted array  by
  binary search, and then by iterating until the end of the array.
\item[4.] We iterate over the points  and remove the ones
  that do not satisfy .
\end{slist}
\smallskip
Upon termination, we return the set .  The pseudo-codes of the
preprocessing and query procedures are given in Algorithms~\ref{alg:rnnpre}
and~\ref{alg:rnnquery}.

\begin{algorithm}[!htb]
\LinesNumbered
  \SetKwInOut{Input}{Input}
  \SetKwInOut{Output}{Output}


  \Input{point cloud , 
    parameter }
  \Output{ data structure}
  \BlankLine

  Initialize  for \;
  \ForEach{} {
    Compute  exactly and store it\;
    Find  s.t.  and
    update \;
  }
  \ForEach{} {
    Build an  data structure\;
  }
  \ForEach{} {
    Build the set  and store it in an array  \;
    \mbox{Sort the points  by increasing distances } \;
  }
  Build the tree  of
  Section~\ref{sec:renn-to-enn} and its associated -\pleb data
  structures \;
\caption{\small Pre-processing phase for \rnn.}
  \label{alg:rnnpre}
\end{algorithm}
\begin{algorithm}[!htb]
\LinesNumbered
  \SetKwInOut{Input}{Input}
  \SetKwInOut{Output}{Output}


  \Input{ data structure, query point }
  \BlankLine

Answer an -\nn query on input , and let   be the output \;
  \For{} { 
    \If{}{ Answer an exhaustive -\pleb query on input , and let  be the output \; 
    } 
  }

  Let  \; 


  Look up the value  in the sorted array  by binary search \;

  Iterate from the  value  to the end of the array  and insert all the visited points into  \;


  \ForEach{ } {
    \If{} {
        Remove  from \;
    }
  }

  Return  \;
\caption{\small Online query phase for \rnn.}
  \label{alg:rnnquery}
\end{algorithm}




\subsection{Correctness of the output}
\label{rnn_correctness}

Corollary~\ref{cor:lsh4enn} guarantees that step 1 of the query
procedure retrieves a point  with high
probability. Let us show that, given that , the
final set  output by the query procedure satisfies  with high
probability.  For clarity, we let  be the set of points inserted
in  at step 2 of the procedure, and  be the set of points
inserted at step 3. The output of the algorithm is then .
Let  for  to .
\begin{lem} \label{lem:allnnred}
   with high probability.
\end{lem}
\begin{proof}
  Step 2 of the query procedure builds  by taking the union of the
  sets  generated by answering exhaustive -\pleb
  queries on the non-empty buckets  with query point . For
  each such , we have  since by definition every point  satisfies . Now, by Theorem~\ref{thm:allnn}, we have  with probability at least
  .  Thus,  with
  probability at least . Since the total number of
  non-empty buckets is at most , the union bound tells us that
   with probability
  at least .
\end{proof}
\begin{lem}\label{lem:rnnofann}
  Given that , we have  with high probability.
\end{lem}
\begin{proof}
  The result follows from Observations~\ref{claim:closeby_pts}
  and~\ref{claim:far-away_pts}. Indeed, every point  with
   satisfies
   and therefore
  cannot belong to , by Observation~\ref{claim:closeby_pts}.
  In addition, the points  with  satisfy  and therefore belong to
  , by Observation~\ref{claim:far-away_pts}.
  Hence, all such points  are inserted in  at step 3 of the
  query procedure. It follows that .
\end{proof}

It follows from Lemmas~\ref{lem:allnnred} and~\ref{lem:rnnofann} that
 with high probability.  In other
words, the set  returned after step 4 of the query procedure
coincides with  with high probability.

\subsection{Complexity}
\label{rnn_complexity}

Let  be the -stable distribution used by the algorithm, and let
, ,  and
 be derived from 
according to Eq.~(\ref{eq:proba_collision_ls}). By
Corollary~\ref{cor:lsh4enn}, the running time of the -\nn query at
step 1 is ,
where . Then, for  ranging from
 to
, the
exhaustive -\pleb query on the set  takes  time in
expectation, where  and , by Theorem~\ref{thm:allnn_Rd}.
Observe that the points  satisfy
, so we have
. Furthermore, since the buckets  are
pairwise disjoint, so are the sets . It
follows that the total expected time spent at step 2 is , the factor
 in the first term coming from the fact that there are
 iterations of the loop.  Considering now step
3, the binary search takes  time.  For
every point  such that , we have , so  since . It follows that . Hence,
the total time spent at step 3 is  and is therefore dominated by the time spent
at step 2.  Finally, the time spent at step 4 is dominated by the
times spent at steps 2 and 3. Combining these bounds together and
using the fact that  and  since
, we obtain the following
query time bound (where  and  are renamed respectively
 and  for convenience):
\begin{thm}\label{thm:rnnquerytime}
Given , the expected query time is
, where  and
, the quantities
,  and
 being derived from some
-stable distribution  according to
Eq.~(\ref{eq:proba_collision_ls}).
\end{thm}
Replacing Theorem~\ref{thm:allnn_Rd} by its specialized versions for
 and  in the analysis immediately gives the following
running time bounds:
\addtocounter{thm}{-1}
\begin{thm}[case ]\label{thm:rnnquerytime_l1}
Given a query point , the expected running time
of Algorithm~\ref{alg:rnnquery} is ,
where  and
.
\end{thm}
\addtocounter{thm}{-1}
\begin{thm}[case ]\label{thm:rnnquerytime_l2}
Given a query point , the expected running time
of Algorithm~\ref{alg:rnnquery} is ,
where  and
.
\end{thm}


\begin{comment}

\subsection{Correctness of the output}
\label{rnn_correctness}

Corollary~\ref{cor:lsh4enn} guarantees that
Line~\ref{line:rnnquery_NN} of Algorithm~\ref{alg:rnnquery} will
retrieve a point  with high probability. Let us show
that, given that , the set  output by the
algorithm satisfies  with
high probability.
For clarity, we let  be the set of points inserted in  on
lines~\ref{line:rnnquery_i_begin}-\ref{line:rnnquery_S} of
Algorithm~\ref{alg:rnnquery}, and  be the set of points inserted
in  on
lines~\ref{line:rnnquery_far_begin}-\ref{line:rnnquery_far_end}. The
output of the algorithm is . Our plan is to show that
 is correct with high probability (Lemma~\ref{lem:allnnred}),
then that  is also correct with high probability
(Lemma~\ref{lem:rnnofann}), from which we will deduce that the output
 itself is correct with high probability
(Theorem~\ref{thm:rnn_output}).
Let  for  to .
\begin{lem} \label{lem:allnnred}
  Given that , we have  with high probability.
\end{lem}
\begin{proof}
  Line~\ref{line:rnnquery_S} of Algorithm~\ref{alg:rnnquery} builds
   by taking the union of the sets  generated by answering
  exhaustive -\pleb queries on the point sets  with
  query point . Recall from line~\ref{line:rnnpre_i} of
  Algorithm~\ref{alg:rnnpre} that every point  satisfies
  .  Recall also that the
  test on line~\ref{line:approxnn} of Algorithm~\ref{alg:nnquery}
  ensures that every point  belongs to
  , which implies that . Thus, . In addition,
  the points  satisfy  and therefore belong to
  . Since  iterations
  of the main loops of Algorithms~\ref{alg:nnpre}
  and~\ref{alg:nnquery} are run, with ,
  Lemma~\ref{lem:success} and Remark~\ref{rem:increase_n} guarantee
  that  is included in  with
  probability at least . Hence, with the same
  probability we have .

  Since the above inclusions hold with probability at least
   for each non-empty set  taken separately, and
  since the total number of non-empty sets  is at most , we
  conclude by the union bound that  with probability at least ,
  where  is the union of the  and  is the union of
  the  for  ranging from  to .
\end{proof}
\begin{lem}\label{lem:rnnofann}
  Given that , we have
   with high probability.
\end{lem}
\begin{proof}
  The first inclusion follows from
  Observations~\ref{claim:closeby_pts}
  and~\ref{claim:far-away_pts}. Indeed, recall from
  line~\ref{line:rnnpre_i} of Algorithm~\ref{alg:rnnpre} that every
  point  with  satisfies
   and therefore
  cannot belong to , by
  Observation~\ref{claim:closeby_pts}. 
In addition, the points  with  satisfy
   and
  therefore belong to , by
  Observation~\ref{claim:far-away_pts}.
Hence, all such points  are considered in the loop
  of lines~\ref{line:rnnquery_far_begin}-\ref{line:rnnquery_far_end}
  of Algorithm~\ref{alg:rnnquery} and are therefore inserted in
  . It follows that .

  The second inclusion is straightforward: since by construction we
  have , every
  point  satisfies , which means that . 
\end{proof}

Our correctness guarantee follows directly from Lemmas~\ref{lem:allnnred}
and~\ref{lem:rnnofann}, using the union bound:
\begin{thm}\label{thm:rnn_output}
  Given a query point , Algorithm~\ref{alg:rnnquery} outputs a
  set  such that  with high probability.
\end{thm}



\subsection{Complexity}
\label{rnn_complexity}

Let  be the -stable distribution used by the algorithm, and let
, ,  and
 be dervied from 
according to Eq.~(\ref{eq:proba_collision_ls}). By
Corollary~\ref{cor:lsh4enn}, the running time of the -\nn query on
line~\ref{line:rnnquery_NN} of Algorithm~\ref{alg:rnnquery} is , where . Then, for  ranging from  to , the exhaustive
-\pleb query on the set  takes  time in
expectation, where  and , by Theorem~\ref{thm:allnn_Rd}.
Observe that the points  satisfy
, so we have
. Furthermore, since the buckets  are
pairwise disjoint, so are the sets . It
follows that the total expected time spent in the loop of
lines~\ref{line:rnnquery_i_begin}-\ref{line:rnnquery_S} of
Algorithm~\ref{alg:rnnquery} is , the factor
 in the first term coming from the fact that there are
 iterations of the loop.  In addition, since
the points  have been sorted by increasing distances
 (line~\ref{line:rnnpre_sort} of
Algorithm~\ref{alg:rnnpre}), locating the subset of those points that
satisfy  takes  time. Now, for every such point  we have
, so
 since
. It follows that
. Hence, the total time spent in
the loop of
lines~\ref{line:rnnquery_far_begin}-\ref{line:rnnquery_far_end} of
Algorithm~\ref{alg:rnnquery} is , a term that is dominated by the
previous one since .

Combining these bounds and using the fact that  and
 since ,
we obtain the following bound on the total expected query time (where
 and  have been renamed respectively  and
 for convenience):
\begin{thm}\label{thm:rnnquerytime}
Given a query point , the expected running time
of Algorithm~\ref{alg:rnnquery} is , where  and , the
quantities ,  and
 being derived from some
-stable distribution  according to
Eq.~(\ref{eq:proba_collision_ls}).
\end{thm}
Replacing Theorem~\ref{thm:allnn_Rd} by its specialized versions for
 and  in the analysis immediately gives the following
running time bounds:
\addtocounter{thm}{-1}
\begin{thm}[case ]\label{thm:rnnquerytime_l1}
Given a query point , the expected running time
of Algorithm~\ref{alg:rnnquery} is ,
where  and
.
\end{thm}
\addtocounter{thm}{-1}
\begin{thm}[case ]\label{thm:rnnquerytime_l2}
Given a query point , the expected running time
of Algorithm~\ref{alg:rnnquery} is ,
where  and
.
\end{thm}

\end{comment}

As mentioned in Section~\ref{sec:rnn_alg}, the  data
structure consists mainly of a collection of pairwise-disjoint
non-empty buckets, of total cardinality , and for each bucket 
an  data structure of size  where , by
Theorem~\ref{thm:allnn_Rd}. This gives a total size of .  In addition,  stores
the tree structure  and its associated -\pleb data
structures, whose total size is , by
Corollary~\ref{cor:lsh4enn}. Finally,  stores a vector
 for each point , which requires a total space of , where .
Combining these bounds and using the fact that ,
we obtain the following bound on the size of the data structure (where
 and  have been renamed respectively  and
 for convenience):
\begin{thm}\label{thm:rnnspaceusage}
  The size of the data structure  built by
  Algorithm~\ref{alg:rnnpre} is , where , the quantities  and
   being derived from some
  -stable distribution  according to
  Eq.~(\ref{eq:proba_collision_ls}).
\end{thm}





\subsection{Bichromatic \rnn}
\label{sec:rnn_bichro}

Let  be a metric space, and let  be two finite
subsets of , respectively referred to as the blue and yellow sets
in the following. Given a point , a {\em reverse nearest neighbor}
of  in this bichromatic setting is a point 
such that . Let  denote the
set of all such points. By analogy, given a parameter ,
a {\em reverse -nearest neighbor of } is a point  such that , and let
 denote the set of all such points. The bichromatic
version of Problem~\ref{problem:rnn} is
stated as follows:
\begin{problem}[Bichromatic \rnn] \label{problem:bichro-rnn}
  Given a query point , the {\em bichromatic reverse nearest
    neighbors query} asks to retrieve the set .
\end{problem}


\begin{comment}
What makes bichromatic \rnn search generally more difficult than
the monochromatic version is that arbitrarily many blue points may lie
close to the query point , whereas in the monochromatic setting
only a limited number of data points can lie close to , even though
this limit is exponential in the ambient dimension~\cite{PZ04}. This
means that the output set in the bichromatic setting can be
arbitrarily larger, which makes it even more important to work out
output-sensitive running time bounds.
\end{comment}


\begin{algorithm}[!htb]
\LinesNumbered
  \SetKwInOut{Input}{Input}
  \SetKwInOut{Output}{Output}


  \Input{point clouds , 
    parameter }
  \Output{ data structure}
  \BlankLine

  Initialize  for \;
  \ForEach{} {
    Compute  exactly and store it\;
    Find  s.t.  and
    update \;
  }
  \ForEach{} {
    Build an  data structure\;
  }
  \ForEach{} {
    Build the set  and store it 
    in an array  \;
    Sort the points  by increasing distances \;
  }
  Build the tree structure  of
  Section~\ref{sec:renn-to-enn} and its associated -\pleb data
  structures  \;
\caption{\small Pre-processing phase for bichromatic \rnn.}
  \label{alg:rnnpre_bichro}
\end{algorithm}
\begin{algorithm}[!htb]
\LinesNumbered
  \SetKwInOut{Input}{Input}
  \SetKwInOut{Output}{Output}


  \Input{ data structure,
    query point }
  \BlankLine

Answer an -\nn query on input , and let   be the output \;
  \For{ } { 
    \If{}{ Answer an exhaustive -\pleb query on input , and let  be the output \; 
    } 
  }

  Let \; 

  Look up the value  in the sorted array  by binary search \;

  Iterate from the value  to the end of the array  and insert the visited points into  \;


  \ForEach{ } {
    \If{} {
        Remove  from \;
    }
  } 

  Return  \;
\caption{\small Online query phase for bichromatic \rnn.}
  \label{alg:rnnquery_bichro}
\end{algorithm}


Our strategy for answering reverse nearest neighbors queries extends
quite naturally to the bichromatic setting when the ambient space is
 equipped with an -norm, . Given two finite
  subsets  of , and a parameter , the data
  structure and algorithms are the same as in
  Section~\ref{sec:rnn_alg}, modulo the following minor changes:
\begin{itemize}
\item the buckets  now partition the blue point
  set , and each bucket  gathers the points  such that
  ,
\item the tree structure of Section~\ref{sec:renn-to-enn} is now built
  on top of the yellow set , so we can find approximate nearest
  neighbors among the yellow points efficiently,
\item for each point , we now store the set
   in vector , to which we add  itself
  only if the latter coincides with a point of . The points in
   are then sorted by increasing distances to .
\end{itemize}
The details of the preprocessing and query procedures are given in
Algorithms~\ref{alg:rnnpre_bichro} and~\ref{alg:rnnquery_bichro} for
completeness. The proof of correctness with high probability and the
complexity analysis extend verbatim to the bichromatic setting, modulo
the systematic replacement of point set  by either  or . We
thus obtain the following guarantees:
\begin{thm}\label{thm:rnnquery_bichro}
Given a query point ,
Algorithm~\ref{alg:rnnquery_bichro} answers bichromatic \rnn
queries correctly with high probability in expected  time using  space, where
,  and
, the quantities
,  and
 being derived from some
-stable distribution  according to
Eq.~(\ref{eq:proba_collision_ls}).
\end{thm}
\addtocounter{thm}{-1}
\begin{thm}[case ]\label{thm:rnnquery_bichro_l1}
Given a query point ,
Algorithm~\ref{alg:rnnquery_bichro} answers bichromatic \rnn queries
correctly with high probability in expected  time
using 
space, where ,  and
.
\end{thm}
\addtocounter{thm}{-1}
\begin{thm}[case ]\label{thm:rnnquery_bichro_l2}
Given a query point ,
Algorithm~\ref{alg:rnnquery_bichro} answers bichromatic \rnn queries
correctly with high probability in expected  time
using 
space, where ,  and .
\end{thm}


\section{Conclusion}


We have introduced a novel algorithm for answering (monochromatic or
bichromatic) \rnn queries that is both provably correct and 
efficient in all dimensions. Our approach is based on a reduction of
the problem to standard -\nn search plus a controlled number of
exhaustive -\pleb queries, for which we propose a speed-up of the
original LSH scheme based on a non-isometric lifting of the
data. Along the way, we obtain a new method for answering exact \nn
queries, whose complexity bounds reflect the gap in difficulty that
exists between exact and approximate queries on a given instance.

Note that the non-isometric lifting trick can be used in a more
aggressive way by applying liftings with ever more distortion, so as
to reduce the exponent  to arbitrarily small positive
constants. However, this comes at the price of a steady degradation of
the exponent , which gets closer and closer to . The
question is how far up in distortion one can go before the increase of
 starts compensating for the reduction of .  Another
question in the same vein is whether  can be made dependent on
. For instance, can  be reduced to , so the output-sensitive term in the query time depends on  instead of ?  More generally, how far from the
optimal do our complexity bounds stand?

In this paper we only cared about sublinear query time and polynomial
space usage. In practice the degree of the polynomial in the space
bound matters, and in this respect the almost-cubic bound of
Theorem~\ref{thm:exact-nn} for exact \nn search is not quite
satisfactory. Moreover, the current preprocessing time may not be so good
due to the fact that some proximity sets, such as 
in step ii of the \rnn procedure, are computed
exactly. To speed up the process one could compute them approximately,
like in previous
literature~\cite{HarPeledIndykMotwani}. Then, the outcome of the
query would likely not be exact, however it might still be
approximately correct. In other words, solving approximate \nn and
\rnn queries might help speed up the preprocessing times and
reduce the size of the data structures.

\section*{Acknowledgements}
A preliminary version of the paper was written in collaboration with
Aneesh Sharma, and some exploratory experiments were undertaken by
Maxime Br\'enon. The authors wish to acknowledge their substantial
contributions to this work. They also wish to thank Piotr Indyk for
helpful discussions about previous work in the area.

