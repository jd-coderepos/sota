\def\year{2021}\relax
\documentclass[letterpaper]{article} \usepackage{style/aaai21}  \usepackage{times}  \usepackage{helvet} \usepackage{courier}  \usepackage[hyphens]{url}  \usepackage{graphicx} \usepackage{color}
\usepackage{bbm}
\urlstyle{rm} \def\UrlFont{\rm}  \usepackage{natbib}  \usepackage{caption} \frenchspacing  \setlength{\pdfpagewidth}{8.5in}  \setlength{\pdfpageheight}{11in}  

\usepackage{algorithm}  \usepackage{algorithmic}  \usepackage{amsfonts} \usepackage{amsmath} \usepackage{multirow} \usepackage{subfigure} \usepackage{url} 

\usepackage[colorlinks,
            linkcolor=black, 
            anchorcolor=black,  
            citecolor=black,      
            ]{hyperref}


\usepackage{makecell}  \usepackage{cleveref}
\usepackage[switch]{lineno}  

\usepackage{eqparbox}
\usepackage{amssymb}

\usepackage{breakurl}
\renewcommand{\algorithmiccomment}[1]{\hfill\eqparbox{COMMENT}{{\#}~#1}}
\renewcommand{\algorithmicrequire}{ \textbf{Input:}} \renewcommand{\algorithmicensure}{ \textbf{Output:}} 

\newcommand{\ying}[1]{\textcolor{magenta}{{[Ying: #1]}}}
\newcommand{\y}[1]{\textcolor{blue}{#1}}

\newcommand{\xuhan}[1]{\textcolor{red}{{[XuHan: #1]}}}
\newcommand{\xh}[1]{\textcolor{green}{#1}}

\newcommand{\anxiang}[1]{\textcolor{cyan}{{[AnXiang: #1]}}}
\newcommand{\ax}[1]{\textcolor{cyan}{#1}}

\pdfinfo{
/Title (Face Paper)
/Author (Anonymous AAAI submission)
/TemplateVersion (2021.1)
} 



\setcounter{secnumdepth}{0} 







\title{ 
Partial FC: Training 10 Million Identities on a Single Machine
}


\author {
Xiang An,\textsuperscript{\rm 1}
         Xuhan Zhu, \textsuperscript{\rm 2}
         Yang Xiao \textsuperscript{\rm 3} 
         Lan Wu \textsuperscript{\rm 1} \\
         Ming Zhang \textsuperscript{\rm 1}
         Yuan Gao \textsuperscript{\rm 1} 
         Bin Qin \textsuperscript{\rm 1} 
         Debing Zhang \textsuperscript{\rm 1} 
         Ying Fu  \textsuperscript{\rm 4} \\
 }
 \affiliations {
\textsuperscript{\rm 1}  DeepGlint 
     \textsuperscript{\rm 2}  Beijing University of Posts and Telecommunications 
     \textsuperscript{\rm 3}  Xiangtan University  
     \textsuperscript{\rm 4}  Beijing Institute of Technology \\
     \{xiangan, lanwu, mingzhang, yuangao, binqin\}@deepglint.com, zhuxuhan@bupt.edu.cn, \\
     yangxiao\_xtu@foxmail.com, debingzhangchina@gmail.com, fuying@bit.edu.cn \\
 }


\begin{document}
\maketitle


\begin{abstract}
Face recognition has been an active and vital topic among computer vision community for a long time. Previous researches mainly focus on loss functions used for facial feature extraction network, among which the improvements of softmax-based loss functions greatly promote the performance of face recognition. However, the contradiction between the drastically increasing number of face identities and the shortage of GPU memories is gradually becoming irreconcilable. 
In this paper, we thoroughly analyze the optimization goal of softmax-based loss functions and the difficulty of training massive identities. We find that the importance of negative classes in softmax function in face representation learning is not as high as we previously thought. The experiment demonstrates no loss of accuracy when training with only 10\% randomly sampled classes for the softmax-based loss functions, compared with training with full classes using state-of-the-art models on mainstream benchmarks. We also implement a very efficient distributed sampling algorithm, taking into account model accuracy and training efficiency, which uses only eight NVIDIA RTX2080Ti to complete classification tasks with tens of millions of identities. The code of this paper has been made available \url{https://github.com/deepinsight/insightface/tree/master/recognition/partial_fc}. 
\end{abstract}

\section{Introduction}

Face recognition is playing an increasingly important role in modern life and has been widely used in residential security, face authentication \cite{wang2015face}, and criminal investigation. During the learning process of face recognition models, the features of each person in the dataset are mapped to so-called embedding space, where the features belonging to the same person are pulled together and the features belonging to different persons are pushed away on the Euclidean distance basis. A golden rule is that the more identities the dataset provides, the more information the model can learn, and, further, the stronger the ability can be acquired to distinguish these features \cite{cao2018celeb,deng2019arcface}. Many companies have training sets with millions and even tens of millions of face identities. For instance, Google’s face dataset in 2015 already had 200 million images consisting of 8 million different identities \cite{schroff2015facenet}.
\begin{figure}[t]
	\centering
	\subfigure[]
	{
		\centering         
		\includegraphics[width=0.22\textwidth]{figure/a2.eps} }
		\subfigure[]
	{
		\centering         
		\includegraphics[width=0.22\textwidth]{figure/a3.eps} }
	\caption{ (a) By using hash forest, the complexity of finding active classes is reduced from O(N) to O(logN). (b) Positive class is selected and negative classes are selected randomly, the complexity of sampling is O(1).}
	\label{fig1}
\end{figure}


The softmax loss and its variants \cite{wang2018cosface,deng2019arcface,wang2018additive,liu2017sphereface} are widely used as objectives for face recognition. In general, they make global feature-to-class comparisons during the multiplication between the embedding features and the linear transformation matrix. In spite of that, when there are huge number of identities in the training set, the cost of storage and calculation of the final linear matrix easily exceed the current GPU capabilities, resulting in failure to train.
  
Zhang~\emph{et al.} reduces the amount of calculation by dynamically selecting active classes in each mini-batch and using only a subset of the classes (partial-classes) to approximate full-class softmax \cite{zhang2018accelerated}. Deng~\emph{et al.}  mitigate the memory pressure of each GPU through model parallel, and calculate the full-class softmax with very little communication \cite{deng2019arcface}. The problem with selecting active classes is that when the number of identites is large to a certain extent, such as 10 million, the time consumption cannot be ignored when retrieving the active classes through features. As for model parallel, the merit of memory savings of distributed GPUs has its bottleneck. When the number of identities grows, the increase of GPU amount can indeed alleviate the problem of storing weight matrix $W$, whereas the storage of final $logits$ will put new burden on GPU memories.


In this paper, we propose an efficient face recognition training strategy that can accomplish ultra-large-scale face recognition training. Specifically, we first equally store the non-overlapping subsets of softmax linear transformation matrix on all GPUs in order. Then each GPU is accountable for calculating the sum of the dot product of sampled sub-matrix that is stored on its own and input features. After that each GPU gathers the local sum from other GPUs to approximate the full-class softmax function. By only communicating the sampled local sum, we approximate full-class softmax with only a small amount of communication. This method greatly reduces the communication, calculation, and storage costs on each GPU. Furthermore, we demonstrate the effectiveness of our strategy, which can promote the training efficiency several times that of the previous practice. By using 8 NVIDIA RTX2080Ti, datasets with 10 million of identities can be trained and 64 GPUs are able to train 100 millions identities. In order to verify the usefulness and robustness of our algorithm in academia, we clean and merge existing public face recognition dataset to obtain the largest publicly available face recognition training set Glint360K, which will be released. Experiment results from multiple datasets show that our method using only 10\% classes to calculate softmax, can achieve on par accuracy with state-of-the-art works.

In summary, our contributions are mainly as follows:
\begin{itemize}
	\item[1)] We propose a softmax approximation algorithm, which can maintain the accuracy when using only 10\% of the class centers.
	\item[2)] We propose an efficient distributed training strategy that can easily train classification tasks with massive number of classes.
	\item[3)] We clean, merge, and release the largest and cleanest face recognition dataset Glint360K. Baseline models trained on Glint360K with our proposed training strategy can easily achieve state-of-the-art.
\end{itemize}


\section{Related Work}

\subsection{Face Recognition}
With the development of deep learning, deep neural networks has been playing an increasingly important role in the field of face recognition. The general pipeline is that the deep neural network extracts a feature for each input image. The learning process gradually narrows the gaps within the classes, and widens the gaps between the classes. At present, the most successful classifiers distinguish different identities by using the softmax classifier or its variants \cite{wang2018cosface,deng2019arcface,liu2017sphereface}. The field of face recognition now requires massive identities to train a model, \emph{e.g.}, 10 millions. For the methods based on softmax loss, the linear transformation matrix $W$ will increase linearly followed the increase of the number of classes. When the number of identities is large to an extent, a single GPU cannot even carry such a weight matrix.  

    \subsection{Acceleration for Softmax} For accelerating large-scale softmax in face recognition, there have been some approaches. HF-softmax \cite{goodman2001classes} dynamically selects a subset of active class centers for each mini-batch. The active class centers are selected by constructing a random hash forest in the embedding space and retrieving the approximate nearest class centers by features. However, all the class centers of this method are stored in RAM, the time cost for calculation in feature retrieval cannot be ignored. Softmax Dissection \cite{he2020softmax}  separates softmax loss into intra class objective and inter class objective and reduces the calculation of the redundancy of the inter class objective, but it can not be extended to other softmax-based losses. 

These methods are based on data parallel when using multi-GPU training. Even though only part of class centers are used to approximate softmax loss function, the inter-GPU communication is still costly, when applying gradients average for synchronizing SGD \cite{li2014scaling}. Besides, the number of selectable class centers are limited by the memory capacity of a single GPU. ArcFace \cite{deng2019arcface} proposes model parallel, which separates softmax weight matrix to different GPUs, then calculates full-class softmax loss with very little communication cost. They successfully trains 1 million identities with eight GPUs on a single machine. However, this method still has memory limitations. When the number of identities keeps increasing, the GPU memory consumption 
will finally exceed its capacity limit, although the number of GPUs increases in the same proportion. We will analyze the GPU memory usage of model parallel in detail in subsequent section. 

\section{Method}


\begin{figure}[t]
	\centering
	\subfigure[]
	{
		\centering         
		\includegraphics[width=0.22\textwidth]{figure/gpu1} }
	\subfigure[]
	{
		\centering         
		\includegraphics[width=0.22\textwidth]{figure/gpu2} }
	\caption{(a) Increasing the number of classes and GPU, the total memory usage increases with the power of the number of GPU. (b) the $W$ memory cost is constant, but logits will increase linearly, when in 16 servers with 8 GPUs case, the percentage of total GPU memory, is as high as 90\%.}
	\label{fig9}
\end{figure}


\begin{figure}[t]
	\centering
	\subfigure[]
	{
        \centering         
        \includegraphics[width=0.22\textwidth]{figure/Random_P} 
	}
	\subfigure[]
	{
        \centering          
        \includegraphics[width=0.22\textwidth]{figure/Positive_P}  
	}
 	\caption{(a) The curves $CA_{pcc}$ values. Solid lines means obtaining the whole positive class centers and sampling partial negative class centers at rate 0.5 and 0.1 (PPRN). Dotted lines means randomly sampling all class centers at rate 0.1 and 0.5. (b) The curves $CA_{pcc}$ values when sampling partial negative class centers at rate 0.1, 0.5 and 1.0 respectively (PPRN). }
	\label{PositiveRandomP}
\end{figure}
In this section, we first detail the existing model parallel, analyzing its inter-device communication overhead, storage cost and memory limitations. Then we introduce our performance-lossless approximation method, and explain how this method works. Finally, we propose our distribution approximation method with the implementation details.

\subsection{Problem Formulation}
\subsubsection{Model parallel}

\begin{figure*}[t]
	\centering
	\includegraphics[width=0.80 \textwidth]{figure/particl_fc.eps} \caption{The structure of  distributed implementation of our method. $k$  means the number of GPUs. Allgather: Gather data from all GPUs and distribute the combined data to all GPUs. Allreduce: Sum up the data and distribute the results to all GPUs.}
	\label{fig2}
\end{figure*}
It is painful to train models with massive identities without using model parallel, subject to the memory capacity of a single graphics card. The bottleneck exists in storing the matrix of softmax weight $W$ $\in \mathbb{R}^{d\times C}$, where $d$ denotes embedding feature dimension and $C$ denotes the number of classes. A natural and straightforward approach to break the bottleneck is to partition $W$ into $k$ sub-matrices $w$ of size $d \times \frac{C}{k} $  and places the $i$-th sub-matrices on the $i$-th GPU. Consequently, to calculate the final softmax outputs, each GPU has to gather features from all other GPUs, as the weights are split up among different GPUs. The definition of softmax function is \begin{equation}
\sigma(X, i) = \frac{e^{w_{i}^TX}}{\sum\nolimits_{j=1}^{C}e^{w_{j}^TX}}.\label{softmax}
\end{equation}
The calculation of the numerator can be done independently by each GPU as input feature $X$ and corresponding weight sub-matrix $w_{i}$ are stored locally.

To calculate the denominator of the softmax function, sum of all $e^{w_{j}^TX}$ to be specific, information from all other GPUs have to be collected. Naturally, we can first calculate the local sum of each GPU, and then compute the global sum through communication. Compared with the naive data parallel, this implementation has negligible communication cost. The difference lies in the data to be communicated changed. Data parallel has to transmit the gradients of whole $W$ to get all weights updated, whereas model parallel only communicates the local sum, whose cost can be ignored. To be specific, the size of communication overhead is equal to batch size multiplied by 4 bytes (Float32). We use collective communication primitives and matrix operations to describe the calculation process of the model parallel on the $i$-th GPU including forward as well as backward propagation, as shown in Algorithm 1. This method can greatly reduce inter-worker communication. Because the sizes of $W$, $x_i$, and $\nabla {X}$ are $d*C$, $N*d$ and $N*d*k$ respectively, and on large-scale classification tasks, we typically assume $C \gg N*(k+1)$, where $N$ represents the mini-batch size on each GPU.
\begin{algorithm}[t]
 \caption{The Model Parallel on the $i$-th GPU}
 \begin{algorithmic}[1]
  \REQUIRE ~~\\ 
  	{$ x_i$}  : features, located on $i$-th GPU;                \\
	{$ w_i$}  : i-th part matrix of $W$, located on $i$-th GPU;          \\
  	{$ onehot_i$}  : onehot of $x_i$, located on $i$-th GPU;   \\
  \ENSURE ~~\\ 
  	$\nabla {x_i}$: the gradient of $x_i$;\quad $\nabla {w_i}$: the gradient of $w_i$;\\
    \STATE $/*$ collect features across all GPUs $*/$
    \STATE $   {X} = \textbf{allgather}(x_i)$  \\
    \STATE $   {logits_i = X*w_{i}}$ \\  
    \STATE $/*$ calculate the local softmax denominator $*/$
    \STATE $   den_i = \text{sum} (e^{logits_i})$ \\  
    \STATE $/*$ get the global denominator across all GPUs $*/$
    \STATE $   den = \textbf{allreduce}(den_i)$\\ \STATE $   prob_i = e^{logits_i} / {den} $ 
    \STATE $   \nabla {logits_i} = prob_i-onehot_i$ \\
    \STATE $   \nabla {w_i} = X^T*\nabla {logits_i}$ \\ 
    \STATE $/*$ sync the gradients of features across all GPUs $*/$
    \STATE $  {\nabla {X}} = \textbf{allreduce}(\nabla {logits_i}*w_i^T)$ \\
    \STATE $   \nabla {x_i} = {get\_submatrix}(i,\nabla {X})$\\
    \RETURN $   \nabla {x_i} $, \quad $\nabla {w_i}$; 
  \end{algorithmic}
\end{algorithm}

\subsubsection{Memory Limits Of Model Parallel}
The model parallel can completely solve  the storage and communication problems of $w$, since no matter how big $C$ is, we can easily add more GPUs. So that each GPU's memory size storing sub-matrix $w$ remains unchanged, \emph{i.e.},  \begin{equation}
Mem_w = d \times \frac{C \uparrow}{k \uparrow} \times 4 \; bytes. \label{memory_weight}
\end{equation}

However, $w$ is not the only one stored on GPU memories. The storage of predicted logits suffers from the increase of total batch size. We denote the $logits$ storage on each GPU as $logits=Xw$, and then the memory consumption storing $logits$ on each GPU is therefore equal to 
\begin{equation}
Mem_{logits} = Nk \times \frac{C}{k} \times 4 \; bytes
\end{equation} 
where $N$ is the mini-batch size on each GPU, and $k$ is the number of GPUs. Assuming that the batch size of each GPU is constant, when $C$ increases, in order to keep $\frac{C}{k}$ unchanged, we have to increase $k$ at the same time. Hence, the GPU memory occupied by $logits$ will continue to increase, because the batch size of features increase synchronously with $k$. 
Assuming that only the classification layer is considered, each parameter will occupy 12 bytes, as we use momentum SGD optimization algorithm during training. In case CosFace \cite{wang2018cosface} or ArcFace \cite{deng2019arcface} is used, each element in $logits$ occupies 8 bytes. Hence, the overall GPU memory occupied by classification layer is calculated as  
\begin{equation}
Mem_{FC} = 3 \times Mem_{W}+2\times Mem_{logits}.
\end{equation}
As shown in Figure~\ref{fig9}, suppose the mini-batch size on each GPU is 64 and the embedding feature dimension is 512, then 1 million classification task requires 8 GPUs and training 10 millions classification task requires at least 80 GPUs. We find that $logits$ will take up ten times as much memory cost as $w$, which makes storing logtis new bottleneck to model parallel. The result shows that training tasks with massive identities cannot be solved by simply adding GPUs.


\subsection{Approximate Strategy}

\subsubsection{Roles of positive and negative classes}
The most widely used classification loss function, softmax loss, can be described as 

\begin{equation}
L = - \frac{1}{N}{\sum\limits_{i=1}^{N}log\frac{e^{w_{y_i}^Tx_i  + b_{y_i}}}{\sum\nolimits_{j=1}^{C}e^{w_{j}^Tx_i  + b_j}}} = - \frac{1}{N}{\sum\limits_{i=1}^{N}log\frac{e^{f_{y_i}}}{\sum\nolimits_{j=1}^{C}e^{f_j}}}, 
\end{equation}
where $x_i \in \mathbb{R}^{d}$ denotes the deep feature of the $i$-th sample, belonging to the $y_i$-th class. $w_j \in \mathbb{R}^{d}$ denotes the $j$-th column of the weight $W \in \mathbb{R}^{d\times C}$. The batch size and the class number are $N$ and $C$, respectively.

$f_j $ is usually denoted as activation of a fully-connected layer with weight vector $w_j$ and bias $b_j$. We fix the bias $bj=0$ for simplicity, and as a result  $f_j$ is given by 
\begin{equation}
f_j = w_{j}^T x = \left \|w_{j}^T\right \| \left \|x\right \|\cos\theta_j, 
\end{equation} 
where $\theta_j$ is the angle between the weight $w_j$ and the feature $x_i$. Following \cite{wang2018cosface,liu2017sphereface,deng2019arcface,wang2018additive}, we
fix the individual weight $\left\|w_{j}\right\|$ by $l_2$ normalisation, we also fix the feature $\left\|x_{i}\right\|$ by $l_2$ normalisation
and rescale it to $s$.  The normalisation step on features and weights makes the predictions only depend on the angle between the feature and the weight.

Naturally, each column of the linear transformation matrix is viewed as a class center, and the $j$-th column of the matrix corresponds to the class center of class $j$. we denote $w_{y_i}$ as positive class center of $x_i$, and the others are negative class centers. 

Through the analysis of the softmax equation, we arrive at the following assumption. If we want to select a subset of class centers to approximate the softmax, positive class centers must be selected, whereas negative class centers only need to be selected from a subset of all. By doing so, the performance of the model can be maintained.

We use two experiments to prove this hypothesis. In each experiment, only a certain percentage of the class centers will be sampled to calculate the approximated softmax loss in each iteration.
The first experiment will primarily select all positive classes corresponding to input features in the current batch, and then randomly sample the negative class centers. We call this sampling strategy as Positive Plus Randomly Negative (PPRN) for short in the follow section. The second is just making random selection from all class centers. Sampling rate is set to 0.1 and 0.5 for both experiments. We define the average cosine distance between $x_i$ and $w_{y_i}$ as $CA_{pcc}$ during the training process, \emph{i.e.}, 
\begin{equation}
CA_{pcc} = \frac{1}{n}\sum\limits_{i=1}^{n}\cos\theta_{i}, \text{with} \cos\theta_{i}\in [0, 1].
\end{equation}

The results of the experiments are shown in Figure~\ref{PositiveRandomP}. In Figure~\ref{PositiveRandomP} (a), we can find that at sampling rate of 0.1, fully random sampling results in an inferior model performance compared to PPRN. Because the averaged cosine angle between positive centers and features is our optimization goal. When training without sampling positive centers, the gradients of $x_i$ only learn the direction to push the sample away from negative centers but lack of the intra-class 
clustering objective. Nonetheless, this performance degradation will gradually decline as the sampling rate increases, since the probability of positive class is sampled is also increasing.

According to Figure~\ref{PositiveRandomP} (b), model trained with PPRN with
sampling rate of 0.1, 0.5 and 1.0 respectively 
 have similar performance. To explain this phenomenon, one key point is that the predicted probability without sampling $P_i$ and the predicted probability with PPRN sampling strategy $\hat{P_i}$ are very similar under certain circumstances. \emph{i.e.} 
 
\begin{equation}
    P_i=\frac{e^{f_i}}{\sum_{j=0}^{C}{e^{f_j}}},
\end{equation}
\begin{equation}
\hat{P_i}=\frac{e^{f_i}}{\sum_{j\in S}{e^{f_j}}}=\frac{P_i}{\sum_{j \in S}{P_j}}, |S|=C*r,
\end{equation}
where $S$ denotes the set of sampled classes and $r$ denotes the sampling rate.

As the sampling strategy PPRN continuously optimizes the positive class centers, the probability of the positive class $P_{gt}$  and the sum of sampled classes $\sum{P_j}$ continues to increase. This makes the gap between the probability of any negative classes $\hat{P_i}$ and $P_i$ smaller and smaller. That is to say in the later stage of the training process, the optimization direction and amplitude of the negative class centers has little relationship with the sampling rate. Therefore, this is also the reason the sampling rates of 0.1, 0.5 and 1.0 can achieve very similar results.

\begin{table}[t]
	\centering
	\label{table2}
	\begin{tabular}{c|c| c| c }
		Method               & LFW        & CFP-FP    & AgeDB-30     \cr
		\hline
		CosFace(0.35)        & 99.51      & 95.44     & 94.56        \cr
		ArcFace(0.5)         & 99.53      & 95.56     & 95.15        \cr
		\hline
		CosFace, Ours(r=1.0) &   \bf99.57 &  \bf95.77 & 95.30        \cr
		CosFace, Ours(r=0.1) &   99.50    &  95.51    & 94.63        \cr
		\hline
		ArcFace, Ours(r=1.0) &   99.52    & 95.74     & \bf95.32     \cr
		ArcFace, Ours(r=0.1) &   99.50    & 95.52     & 94.69        \cr
		\hline
	\end{tabular}
	\caption{Verification performance(\%) small models on LFW, CFP-FP and AgeDB-30.}
	\label{webface}
\end{table}
\begin{figure}[t]
	\centering
	\subfigure[]
	{
		\centering          
		\includegraphics[width=0.22\textwidth]{figure/ver_webface_lfw} 
	}
	\subfigure[]
	{
		\centering         
		\includegraphics[width=0.22\textwidth]{figure/ver_webface_agedb} 
	}
	\caption{Verification results of PPRN (ours) and random sampling of all class centers on different val datasets. We employ ResNet50 as the backbone and CosFace loss on trainning dataset CASIA. (a) Verification result on LFW.  (b) Verification result on AgeDB-30.}
	\label{ver_webface}
\end{figure}

\begin{table*}[t]
	\centering
	\resizebox{\linewidth}{!}{
	\begin{tabular}{ c | c | c | c | c | c | c | c | c | c  }
	    \multirow{2}{*}{Method} &
	      \multicolumn{2}{c| } {IJB} &  
	      \multicolumn{2}{c| } {MegaFace} &
		  \multicolumn{5}{c  } {Verification Accuracy } \cr
		\cline{2-10}
                                                      & IJB-B    & IJB-C    & Id       & Ver    & LFW      &  AgeDB   & CALFW    & CPLFW    & CFP-FP \cr      
        \hline                                                                                                                                                
        CosFace(0.35)                                 & -        &  -       & 97.91    & 97.91  & 99.43    & -        & 90.57    & 84.00    & -      \cr      
        ArcFace(0.5)                                  & 0.942    & 0.956    & 98.35    & 98.48  & 99.82    & -        & 95.45    & 92.08    & 98.27  \cr      
        GroupFace  \cite{kim2020groupface}            & 0.949    & 0.963    & 98.74    & 98.79  & \bf99.85 & 98.28    & 96.20    & 93.17    & 98.63  \cr      
        CircleLoss \cite{sun2020circle}               & -        & 0.940    & 98.50    & 98.73  & 99.73    & -        & -        & -        & -      \cr      
        CurricularFace \cite{huang2020curricularface} & 0.948    & 0.961    & 98.71    & 98.64  & 99.80    & 98.32    & 96.20    & 93.13    & 98.37  \cr      
        \hline                                                                                                                                                
        \hline                                                                                                                                                
        MS1MV2, CosFace, Ours(r=1.0)                  & 0.950    & 0.964    & 98.36    & 98.58  & 99.83    & 98.03    & 96.20    & 93.10    & 98.51  \cr      
        MS1MV2, CosFace, Ours(r=0.1)                  & 0.946    & 0.960    & 98.04    & 98.49  & 99.82    & 98.13    & 96.12    & 92.90    & 98.60  \cr      
        \hline                                                                                                                                                
        \hline                                                                                                                                                
        MS1MV2, ArcFace, Ours(r=1.0)                  & 0.948    & 0.962    & 98.31    & 98.59  & 99.83    &  98.20   & 96.18    & 93.00    & 98.45  \cr      
        MS1MV2, ArcFace, Ours(r=0.1)                  & 0.944    & 0.958    & 98.25    & 98.03  & 99.83    &  98.15   & 96.15    & 92.95    & 98.48  \cr      
        \hline                                                                                                                                                
        \hline                                                                                                                                                
        Glint360K, CosFace, Ours(r=1.0)               & \bf0.961 & \bf0.973 & \bf99.13 &98.98   & 99.83    & 98.55    & \bf96.21 & 94.78    & \bf99.33 \cr      
        Glint360K, CosFace, Ours(r=0.1)               & \bf0.961 & 0.972    & 98.94    &\bf99.10& 99.83    & \bf98.57 & 96.20    & \bf94.83 & \bf99.33 \cr      
	   \hline
	\end{tabular}
	}
	\caption{The 1:1 verification accuracy on the LFW, AgeDB-30, CALFW, CPLFW, CFP-FP datasets. TAR@FAR=1e-4 is reported on the IJB-B and IJB-C datasets. Identification and verification evaluation on MegaFace Challenge1 using FaceScrub as the probe set. “Id” refers to the rank-1 face identification accuracy with 1M distractors, and “Ver” refers to the face verification TAR@FPR=1e-6. $r$ means the sampling rate.}
	\label{all}
\end{table*}


\begin{table}[t]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c|c|c|c|c|c}
         Dataset              & African  & Caucasian & Indian   & Asian    & All        \\
         \hline
    CASIA-R100                & 39.67    & 53.93     & 47.81    & 16.17    & 37.53      \\
    VGG2-R50                  & 49.20    & 65.93     & 56.22    & 27.15    & 47.13      \\
    MS1MV2-R50                & 71.97    & 83.24     & 79.66    & 22.94    & 56.20      \\
    MS1MV3-R134               & 81.08    & 89.06     & 87.53    & 38.40    & 74.76      \\
    \hline                                                                                                                                                
    \hline              
    Glint360K-R100(r=1.0)     & 89.50    & 94.23     & 93.54    & \bf65.07 & \bf88.67   \\
    Glint360K-R100(r=0.1)     & \bf90.45 & \bf94.60  & \bf93.96 & 63.91    & 88.23      \\
         \hline
    \end{tabular}
    }
    \caption{The 1:1 verification accuracy on InsightFace Recognition Test (IFRT), TAR@FAR=1e-6 is measured on all-to-all 1:1 protocal. $r$ means the sampling rate.}
    \label{tab:IFRT}
\end{table}



\subsubsection{Distributed Approximation}
As mentioned in the previous section, only a subset of the class centers can achieve a comparable performance. In order to train a training set with a larger number of identities, we propose a distributed approximation. The process of sampling subset class centers is  straightforward: 1) First select the positive class centers; 2) Randomly sample negative class centers. In the case of model parallel, in order to balance the calculation and storage of each GPU, the number of class centers sampled on each GPU should be equal, so the sampling process has changed as follows: 

\noindent \textbf{1. Obtain the positive class centers on this GPU}  

$W$ will be evenly divided into different GPUs according to the order, such as $W=[w_{1}, w_{2}, ..., w_{k}]$, $k$ is the number of GPUs. When we know the label $y_i$ of the sample $x_i$, its positive class center is the $y_i$-th column of the $W$ linear matrix. Therefore, the positive class centers $w_i^p$ on this current GPU can be easily obtained by the label $y$ of the features in current batch.

\noindent \textbf{2. Calculate the number of negative class centers} 
  
According to the previous information, the number of class centers stored on this GPU is $|w_i|$, the number of positive class centers is $|w_i^p|$, then the number of negative class centers that need to be randomly sampled on this GPU is $s_i = (|w_i| - |w_i^p|) * r$, where $r$ is the sampling rate for PPRN.

\noindent \textbf{3. Randomly sample negative classes}  

By randomly sampling $s_i$ negative class centers in the difference set between $w_i$ and $w_i^p$, we get the negative class centers $w_i^n = \textbf{random}(w_i - w_i^p, s_i)$ 

Finally, we get all the class centers to participate in the softmax calculation, $W^s = [W^p, W^n]$, where $W^p = [w_1^p, ... , w_k^p]$, $W^n = [w_1^n, ... , w_k^n]$. In fact, this methon is an approximate method to obtain the load balance of each GPU.

\begin{small} 
\begin{equation}
\begin{split}
W^s = & \textbf{random}(W - {W^p}) \\
\approx & [\textbf{random}(w_1 - w^p_1, s_1), ..., \textbf{random}(w_k - w^p_k, s_k) ]
\end{split}
\end{equation}
\end{small}


\section{Experiment}
\subsection{Datasets and Settings}
\subsubsection{Training Dataset}
Our training datasets include CASIA \cite{liu2015deep} and MS1MV2 \cite{deng2019arcface}. Furthermore, we clean Celeb-500k \cite{cao2018celeb} and MS1MV2 to merge into a new training set, which we call \textbf{Glint360K}. The released dataset contains 17 million images of 360K individuals, which is the largest and cleanest training set by far in academia.

\subsubsection{Testing Dataset}
We explore efficient face verification datasets (\emph{e.g.}, LFW \cite{huang2008labeled}, CFP-FP \cite{sengupta2016frontal}, AgeDB-30 \cite{moschoglou2017agedb}) to check the improvement from different settings. Besides, we report the performance of our method on the large-pose and large-age datasets (\emph{e.g.}, CPLFW \cite{CPLFWTech} and CFLFW \cite{zheng2017cross}). In addition, we extensively test the proposed method on large-scale image datasets (\emph{e.g.}, MegaFace \cite{kemelmacher-shlizerman2016the}, IJB-B \cite{whitelam2017iarpa}, IJB-C \cite{maze2018iarpa}) and InsightFace Recognition Test (IFRT)\footnote{\url{https://github.com/deepinsight/insightface/tree/master/IFRT}}.
 


\subsubsection{Training Settings}

We use ResNet50 and ResNet100 \cite{deng2019arcface, he2016deep}, as our backbone network, and use two margin-base loss functions (\emph{i.e.}, CosFace and ArcFace). We set the feature scale $s$ to 64 and cosine margin $m$ of CosFace at 0.4 and arccos margin $m$ of ArcFace at 0.5. We use a mini-batch size of 512 across 8 NVIDIA RTX2080Ti. The learning rate starts from 0.1. For CASIA, the learning rate is divided by 10 at 20K, 28K iterations and the training process is finished at 32K iterations. For MS1MV2, we divide the learning rate at 100K, 160K iterations and finish at 180K iterations. For Glint360K, the learning rate is divided by 10 at 200k, 400k, 500k, 550k iterations and finish at 600K iterations.


\begin{table*}[t]
    \centering
    \resizebox{!}{22mm}{
    \begin{tabular}{c|c|c|c|c|c|c|c}
         Method & Sample Rate & GPUs & BatchSize & Identites & Memory/M & Throughput img/sec &W \\
    \hline
         Model Parallel& -  & 8  & 1024 & 1M      & 10408     & 2390       & GPU \\
         Ours & 0.1         & 8  & 1024 & 1M      & \bf 8100  & \bf 2780   & GPU \\
    \hline
    \hline
         Model Parallel& -  & 8  & -     & 10M     & OOM       & -          & -   \\
         Ours & 0.1         & 8  & 1024 & 10M     & \bf10400     & \bf900        & RAM \\
    \hline
    \hline
        Model Parallel & -  & 64 & 2048 & 10M     & 9684      & 4483       & GPU \\
        Ours & 0.1          & 64 & 4096 & 10M     & \bf 6722  & \bf 12600  & GPU \\
    \hline
    \hline
        Ours & 0.1          & 64 & 4096 & \bf 20M     & 8702  &  10790  & GPU \\
        Ours & 0.1          & 64 & 4096 & \bf 30M     & 9873  &  8600   & GPU \\
        Ours & 0.05         & 64 & 4096 & \bf 100M    & 7068  &  2000   & RAM \\
    \hline
    \end{tabular}
    }
    \caption{Large-scale classification training comparison. The less memory occupied and the larger throughput, the better. OOM means the GPU memory overflows and the model cannot be trained. When model parallel is applied, storing weight matrices in RAM is useless, because all class centers still need to be loaded back into GPUs when calculating softmax loss.}
    \label{distributed}
\end{table*}

\subsection{Effectiveness and Robustness}
\subsubsection{Effects on positive class centers}
We compare the results of PPRN and random sampling of all class centers under different sampling rates, as shown in Figure~\ref{ver_webface}. The experiment shows that the accuracy of all random sampling of all class centers will plumb drastically when the sampling rate is small, while our method will maintain.
\subsubsection{Effects on small-scaled trainset}
As shown in Table~\ref{webface}. On a small-scaled training set, PPRN with sampling rate of 10\%, has almost no adverse effect on accuracy, which proves that our method is effective even on a small data set.
\subsubsection{Robustness on the number of identities}
As shown in Table~\ref{all}, We use two large training sets MS1MV2 and Glint360K to verify the number of identities in the training set effects on our sampling method. 
For IJB-B and IJB-C, when using MS1MV2, the accuracy difference between 10\% sampling and full softmax in IJB-B and IJB-C is 0.4\% and 0.4\%, when using Glint360K, 10\% sampling has no difference in IJB-B, and has only 0.1\% difference in IJB-C. For MegaFace, when using MS1MV2, the identification and verification accuracy difference between 10\% and full softmax are 0.24\% and 0.09\%, when using Glint360K, the performance of 10\% sampling rate and full softmax are comparable, in verification evaluation, 10\% even outperforms full softmax, surpasses full softmax by +0.12\%. This conclusion shows that our method is also 
work in larger-scale training sets, and if the number of identites increases greater than or equal to 300K, the performance of 10\% sampling is comparable to full softmax.

\subsection{Benchmark Results}
\subsubsection{Results on IJB-B and IJB-C}
We follow the testing protocol in ArcFace, and employ the face detection scores and the feature norms to re-weight faces within templates. The experiments on these two datasets (MS1MV2 and Glint360K) are used to prove that PPRN with sampling rate of 10\% for softmax calculation has little lost on performance. As shown in Table~\ref{all}, when we apply PPRN with sampling rate of 10\% on our large-scale training data (Glint360K), further improve the TAR (@FAR=1e-4) to 0.961 and 0.972 on IJB-B and IJB-C respectively.

\subsubsection{Results on MegaFace}
We adopt the refined version of MegaFace \cite{deng2019arcface} to give a fair evaluation, we use MS1MV2 and Glint360K under the large protocol. As shown in Table~\ref{all}. Finally, Our method using our large-scale Glint360K dataset with PPRN with sampling rate of 10\% achieves state-of-the-art verification accuracy of 99.13\% on the MegaFace dataset.

\subsubsection{Results on IFRT}
IFRT is a globalised fair benchmark for face recognition algorithms, this test dataset contains 242143 identities and 1624305 images. IFRT evaluates the algorithm performance on worldwide web pictures which contains various sex, age and race groups. In Table~\ref{tab:IFRT}, we compare the performance of our method train on Glint360k. The proposed Glint360k dataset obviously boosts the performance compared to MS1MV3. Furthermore, the performance of PPRN with sampling rate of 10\% and full softmax are still comparable.

\subsection{Training 100 millions identities}
We set different number of identities and GPUs to test the training speed of our method and model parallel. In all experiments, we remove the influence of IO. We compare four settings, as shown in Table~\ref{distributed}:  

\noindent \textbf{1. 1 Million identities on 8 GPUs}  
8 GPU is more than enough to store one million class centers, we store $W$ on the GPU. Because of the reduction in calculations brought by logits, our speed is 30\% faster than model parallel.    

\noindent \textbf{2. 10 Million identities on 8 GPUs}  
When the number of idintities is as large as 10 millions, the model parallel method can not work. We can still continue training, the training speed of our method is 900 images per second.

\noindent \textbf{3. 10 Million identities on 64 GPUs}  
When using model parallel, 64 GPUs will bring a large global batch size, which will increase the GPU memory of $logits$. 2048 is the largest batch size for model parallel. Compared with model parallel, the memory consumption of our method on each GPU is reduced from 9.6G to 6.7G, and training speed is 12600 images per second, which is 3 times faster than model parallel.  

\noindent \textbf{4. 100 Million identities on 64 GPUs}
With 64 GPUs, training 10 million identities is already at the limit of model parallel, but ours can easily expand to 20 million, 30 million, or even 100 million identities. when training 20 million, 30 million and 100 million identities, the training speed of our method are 10790, 8600 and 2000 images per second. We are the first to propose how to training 100 millions classes. 

\begin{table}[t]
    \centering
    \resizebox{!}{!}{
    \begin{tabular}{c|c|c|c|c|c}
         Method & Ids & r & LFW & CFP & AgeDB  \\
         \hline
         HF-Softmax   & 1.3K & 1/64    & 99.18     & 86.11 & 91.55 \\
         D-Softmax-K  & 1.3K & 1/64    & 99.55     & 89.77 & 95.02 \\
         Ours & 1.3K  & 1/64 &\bf99.60 & \bf95.52  & \bf95.63      \\
         \hline
    \end{tabular}
    }
    \caption{Comparison of our method and other existing sampling-based methods in terms of face verification accuracy on LFW, CFP and AgeDB. $r$ means the sampling rate.}
    \label{tab:accuracy}
\end{table}

\begin{table}[t]
    \centering
    \resizebox{!}{12mm}{
    \begin{tabular}{c|c|c|c}
         Method        &   Ids & r & Total Avg. Time(s)                      \\
        \hline      
         Softmax            & 750K &  1/64 & 3.96                            \\
         HF-Softmax         & 750K &  1/64 & 2.88                            \\
         D-Softmax & 750K &  1/64 & 1.05                            \\
         Ours               & 750K &  1    & \bf 0.46                        \\
         Ours               & 750K &  1/10 & \bf 0.32                        \\  
         Ours               & 750K &  1/64 & \bf 0.31                        \\
         \hline   
    \end{tabular}
    }
    \caption{Comparison of our method and other existing sampling-based methods in terms of training speed, the Total average time is computed as the average time for one forward-backward pass through the entire model.}
    \label{tab:my_label}
\end{table}

\subsection{Compare with other sampling-based methods.}

We compare our method with some current sampling-based methods.  The methods are HF-Softmax proposed in \cite{zhang2018accelerated} and D-Softmax proposed in \cite{he2020softmax}. We adopt same dataset which is merged by MS1MV2 \cite{deng2019arcface} and MegaFace2 \cite{nech2017level} and same network as the work done by \cite{zhang2018accelerated,he2020softmax} for fair comparison. By using the same 1/64 sampling rate, our method outperforms all the methods in accuracy and speed, as shown in Table~\ref{tab:accuracy} and Table~\ref{tab:my_label}.


\section{Conclusion}
In this paper, we first systematically analyse the pros and cons of model parallel. Following this, for the issue that model parallel cannot train models with massive number of classes, we introduce PPRN sampling strategy. On one hand, by training only a subset of all classes in each iteration, the training speed can be very fast. More importantly, this training on partial classes method makes GPU memory no longer bottleneck in model parallel, which means we can make the e of massive identities from impossible to possible. Next we make broad experiment on verifying the effectiveness and robustness of PPRN across different models, loss functions, training sets and test sets. Last but not least, we release by far the largest and cleanest face recognition dataset Glint360K to accelerate the development in the field. When training on Glint360K, we achieve state-of-the-art performance with only 10\% of classes used for training.

\bibliography{partical_fc}

\end{document}
