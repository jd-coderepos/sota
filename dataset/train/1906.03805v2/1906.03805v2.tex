\documentclass{article} 

\usepackage[accepted]{icml2019}

\usepackage{times}
\usepackage{multirow}
\usepackage{url}
\usepackage{qiangstyle}
\usepackage{mathrsfs}

\usepackage{booktabs}

\usepackage{lipsum}  

\icmltitlerunning{Improving Neural Language Modeling via Adversarial Training}

\newcommand{\softmax}{{\mathrm{Softmax}}}
\newcommand{\ours}{{\mathrm{AdvSoft}}}
\newcommand{\advsoft}{\ours}


\begin{document}

\twocolumn[
\icmltitle{Improving Neural Language Modeling via Adversarial Training}









\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Dilin Wang}{equal,ut}
\icmlauthor{Chengyue Gong}{equal,ut}
\icmlauthor{Qiang Liu}{ut}
\end{icmlauthorlist}
\icmlaffiliation{ut}{Department of Computer Science, UT Austin}
\icmlcorrespondingauthor{Dilin Wang}{dilin@cs.utexas.edu}
\icmlcorrespondingauthor{Chengyue Gong}{cygong@cs.utexas.edu}
\vskip 0.2in
]

\printAffiliationsAndNotice{\icmlEqualContribution}

\begin{abstract}
Recently, substantial progress has been made in language modeling by using deep neural networks. 
However, in practice, 
large scale neural language models have been shown to be prone to overfitting.
In this paper, 
we present a simple yet highly effective   
adversarial training mechanism for regularizing neural language models.
The idea is to introduce adversarial noise to the output embedding layer while training the models. We show that the optimal adversarial noise yields a simple closed form solution, thus allowing us to develop a simple and time efficient algorithm. 
Theoretically, we show that our adversarial mechanism effectively encourages the diversity of the embedding vectors, helping to increase the robustness of models. 
Empirically, we show that our method improves on the single model state-of-the-art results for language modeling on Penn Treebank (PTB) and Wikitext-2,  
achieving test perplexity scores of 46.01 and 38.07, respectively. 
When applied to machine translation, 
our method improves over 
various transformer-based translation baselines 
in BLEU scores on the WMT14 English-German and IWSLT14 German-English tasks.
\end{abstract}
\vspace{-2em}




\section{Introduction}

Statistical language modeling is a 
fundamental task in machine learning, with wide applications in various areas, 
including automatic speech  recognition  \citep[e.g.,][]{yu2016automatic}, machine translation \citep[e.g.,][]{koehn2009statistical} and computer vision \citep[e.g.,][]{xu2015show},
to name a few. 
Recently, deep neural network models, especially
recurrent neural networks (RNN) based models,  have emerged to be one of the most powerful 
approaches for language modeling  
\citep[e.g.,][]{merity2017regularizing, yang2017breaking, vaswani2017attention, anderson2018bottom}. 

Unfortunately, a major challenge in  training large scale RNN-based language models is  their tendency to overfit; 
this is caused by the 
high complexity of RNN models 
and the discrete nature of language inputs.  
Although various regularization techniques, such as early stop and  dropout \citep[e.g.,][]{gal2016theoretically}, have been investigated, severe overfitting is still widely observed in state-of-the-art benchmarks, as evidenced by the large gap between training and testing performance. 




In this paper, we develop
a simple yet surprisingly efficient minimax training strategy for regularization.
Our idea is to inject an adversarial perturbation  
on the word embedding vectors in the softmax layer of the language models,     
and seek to find the optimal parameters that  maximize the worst-case performance subject to the adversarial perturbation.   
Importantly, we show that the optimal perturbation vectors yield a simple and computationally efficient form under our construction, allowing us to derive a simple 
and fast training algorithm (see Algorithm~\ref{alg:main}),  
which can be easily implemented based a minor modification of the standard maximum likelihood  training and does not introduce additional training parameters.  

An intriguing theoretical property of our method is that it provides an 
effective mechanism to encourage
diversity of word embedding vectors,  
which is widely observed to yield 
better generalization performance  
in neural language models \citep[e.g.,][]{
mu2017all, gao2018representation, 
liu2018learning, cogswell2015reducing, khodak2018carte}. 
In previous works, the diversity is often enforced explicitly by adding additional diversity penalty terms \citep[e.g.,][]{gao2018representation}, 
which may impact the likelihood optimization and are computationally expensive when the vocabulary size is large. 
Interestingly, we show that our adversarial training  
effectively enforces diversity without explicitly introducing the additional diversity penalty,  and is significantly more  computationally efficient than direct regularizations. 


Empirically, 
we find that our adversarial method 
can significantly improve the performance of 
state-of-the-art large-scale neural language modeling  and machine translation. 
For language modeling, 
we establish a new single model state-of-the-art result
for the Penn Treebank (PTB) and WikiText-2 (WT2) datasets to the best of our knowledge, achieving 46.01 and 38.07 test perplexity scores, respectively.  
On the large scale WikiText-103 (WT3) dataset, our method improves the Quasi-recurrent neural networks (QRNNs) \citep{merity2018analysis} 
baseline. 


To demonstrate the broad applicability of the method, 
we also apply our method to improve machine translation, using 
Transformer \citep{vaswani2017attention} as our base model. 
By incorporating our adversarial training,  
we improve a variety of Transformer-based translation baselines on the WMT2014 English-German and IWSTL2014 German-English translations.








 
\section{Background: Neural Language Modeling}
Typical word-level language models are specified as a product of conditional probabilities using the chain rule: 

where  denotes a sentence of length , with 
 the -th word and  the 
vocabulary set. 
In modern deep language models, the conditional probabilities   are often specified using recurrent neural networks (RNNs), 
in which  the context  
at each time  is represented 
using a hidden state vector  defined recursively via 

where  is a nonlinear map with a trainable parameter . 
The conditional probabilities are then defined using a softmax function: 
where  is the coefficient of softmax;  
 can be viewed as an embedding vector for word 
and  the embedding vector of context .  
The inner product  measures the similarity between word  and context , which is converted into a probability using the softmax function. 

In practice, the nonlinear map  is specified by typical RNN units, 
such as LSTM \citep{hochreiter1997long} or GRU \citep{chung2014empirical}, 
applied on another set of embedding vectors  of the words, 
that is, 

where  is the weight of the RNN unit , 
and   is trained jointly with . 
Here,  is the embedding vector of word , fed into the model from the input side (and hence called the \emph{input embedding}), while  is the embedding vector from the output side (called the \emph{output embedding}). 
It has been found that it is often useful to tie the input and output embeddings, that is, setting  (known as the weight-tying trick), 
which reduces the total number of free parameters and yields significant improvement of performance   \citep[e.g.,][]{press2016using, inan2016tying}. 




Given a set of sentences , 
the parameters  and  are jointly trained by maximizing the log-likelihood:  

This optimization involves joint training of 
a large number of parameters , including both the neural weights and word embedding vectors,  and is hence highly prone to overfitting in practice. 






\iffalse 
Further,  is specified by a softmax function 

where  is the  coefficients of the softmax and 
 is its -th column. 
The bias term is dropped for simplicity. 
It has been found useful to make the weight coefficient  equal the embedding vector  , that is, , if the hidden state dimension  equals the embedding dimension . This yields the  
weight tying softmax function \citep{cite},

Training language models with weight-tying could reduce the total free parameters
substantially and yield significant improvements to the standard neural language model \citep{press2016using, inan2016tying}. 
Then the netowrk is typically trained by minimising the negative log-likelihood:

where  are model parameters.
\fi  
\section{Main Method} \label{sec:main}
We propose a simple 
algorithm that 
effectively  
alleviates overfitting in deep neural language models, 
based on injecting adversarial perturbation on the output embedding vectors  in  the softmax function (Eqn. \eqref{equ:softmax}).  
Our method is \emph{embarrassingly simple}, adding virtually no additional computational overhead over standard maximum likelihood training, 
while achieving substantial improvement on challenging benchmarks (see Section~\ref{sec:experiement}). 
We also draw theoretical insights on this simple mechanism,  
showing that it implicitly 
promotes diversity among the output embedding vectors ,
which is widely believed to increase robustness of the results \citep[e.g.,][]{cortes1995support, liu2018learning, gao2018representation}. 




\subsection{Adversarial MLE} 

Our idea is to introduce an adversarial noise on the output embedding vectors  in maximum likelihood training \eqref{equ:mle}: 

where  is an adversarial perturbation applied on 
the embedding vector  of word , in the -th sentence at the -th location.  
We use  to denote the L2 norm throughout this paper; 
 controls the magnitude of the adversarial perturbation.   




A key property of this formulation is that, with fixed model parameters , 
the adversarial perturbation  has an elementary closed form solution, 
which allows us to derive a simple and efficient algorithm (Algorithm~\ref{alg:main}) by optimizing  and  alternately. 


\begin{thm}\label{thm:advh}
For each conditional probability term
  in \eqref{equ:softmax},
the optimization of the adversarial perturbation in \eqref{equ:advsoft0} is formulated as  

This is equivalent to just adding adversarial perturbation on   
with magnitude :

which is further equivalent to 

As a result, we have 
where . 
\end{thm}


In practice, we propose to optimize  and  alternatively.  Fixing , the models parameters   
are updated using  gradient descent as standard maximum likelihood training. 
Fixing , 
the adversarial noise  is updated using the elementary solution in 
\eqref{equ:deltaistar}, which introduces almost no additional computational cost. 
See Algorithm~\ref{alg:main}. 
Our algorithm can be viewed as an approximate gradient descent optimization of    
, but without back-propagating through the norm term . Empirically, we note that back-propagating through  
seems to make the performance worse, as
the training error would diverge within a few epochs.
This is maybe because the gradient of  forces  to be large in order to increase , which is not encouraged in our setting.






\iffalse 
\subsection{Adversarial-Softmax}
In this section, we use  to refer to the hidden state and 
use  to denote word embeddings, 
consisting of the stacked real vectors , where .
Our adversarial softmax function with weigth-tying 
is defined as 

where  and the injected noise 
is chosen to decrease the logit value of the target word to the utmost
within a small neighborhood,

where  denotes the the amount of noise added to the softmax function.
It is easy to derive the optimal noise direction, which can be written as

In this way,
our adversarial softmax yields a more difficult target, to see that, 

\fi 

\subsection{Diversity of Embedding Vectors} 

An interesting property of our adversarial strategy is that  
it can be viewed as a mechanism to encourage diversity among word embedding vectors: we show that an embedding vector  is guaranteed 
to be separated from the embedding vectors of all the other words by at least distance ,
once there exists a context vector  
with which  dominates the other words according to .  
This is a simple property implied by the definition of the adversarial setting:  
if there exists an  within the -ball of , 
then  (and ) can never dominate the other, 
because the winner is always penalized by the adversarial perturbation.   




\begin{mydef}\label{def:word}
Given a set of embedding vectors , 
a word  is said to be \emph{-recognizable} 
if there exists a vector   
on which  dominates 
all the other words under  -adversarial perturbation, in that 

In this case, 
we have , and 
 would be classified to be the target word of context , despite the adversarial perturbation. 
\end{mydef}




\begin{algorithm}[t] \begin{algorithmic} \STATE \textbf{Input} Training data , model parameters 
    \WHILE {not converge} 
    \STATE Sample a mini-batch  from the data  
    \STATE For each sentence  in the minibatch and , 
set the adversarial noise on  to be  
    
    where  is the RNN hidden state related to , define in \eqref{equ:fht}.  
\STATE Update  using gradient ascent of log-likelihood~\eqref{equ:mle} on minibatch , 
\ENDWHILE 
\STATE 
\emph{Remark}. We find it is practically useful to choose  to adapt with the norm of , that is, 
 for each word, and  is a hyperparameter.     
\end{algorithmic}
\caption{Adversarial MLE Training} 
\label{alg:main} 
\end{algorithm}



\begin{thm}
\label{thm:diversity}
Given a set of embedding vectors , 
 if a word  is -recognizable, then we must have 

that is, 
 is separated from the embedding vectors of all other words by at least  distance. 
\end{thm}
\begin{proof}
If there exists  such that , 
following the adversarial optimization, we must have 

which forms a contradiction. 
\end{proof}
Note that maximizing the adversarial training objective function 
can be viewed as enforcing each  to be -recognized by 
its corresponding context vector , and hence implicitly enforces diversity between the recognized words and the other words.   
We should remark that the context vector  in Definition~\ref{def:word} does not have to exist in the training set, although it will more likely happen in the training set due to the training. 
 
In fact, we can draw a more explicit connection between pairwise distance and adversarial softmax function. \begin{thm} 
Following the definition in \eqref{equ:advsoft}, we have  

where  is the sigmoid function and 

is an ``energy function''
that measures the distance from  to the other words , : 

\label{thm:diversity}
\end{thm}
\vspace{-3em}
\begin{proof}
We have 

where 

Note that , we have

\vspace{-1.0em}
\iffalse 
Assume , we have 

This is equivalent to 

Further

Define . We have 

This shows that 

\fi 
\end{proof}
\vspace{-0.5em}

Therefore, maximizing 
, as our algorithm advocates, 
also maximizes the energy function  to enforce  larger than  by placing a higher penalty 
on cases in which this is violated. 



























\begin{figure*}
\centering
\begin{tabular}{ccc}
\raisebox{3em}{\rotatebox{90}{ Density}}
\includegraphics[height=0.2\textwidth]{./figures/w_nbrs_top_ratio} &
\raisebox{1.4em}{\rotatebox{90}{ Log Eigenvalues}}
\includegraphics[height=0.2\textwidth]{./figures/w_eig} & 
\raisebox{3em}{\rotatebox{90}{ Perplexity}}
\includegraphics[height=0.2\textwidth]{./figures/wt2_loss}  
\\
(a) L2 Distance & (b) Index &  (c) Epochs (Wiki2)  \\
\end{tabular}
\caption{
(a) Kernel density estimation of the Euclidean distance to the nearest neighbor for each word;
(b) Logarithmic scale singular values of embedding
matrix. We normalize the singular values of each matrix so that the largest one is 1;
(c) Training and validation perplexities vs. training epochs
for AWD-LSTM \citep{merity2017regularizing} and our approach 
on the Wikitext-2(WT2) datasets. 
We follow the training settings reported in \citet{merity2017regularizing}.
The kink in the middle represents the start of fine-tuning.}
\label{fig:ptb_wt_ppl}
\end{figure*} 

\section{Related Works and Discussions}
\label{sec:related} 



















\paragraph{Adversarial training} 
Adversarial machine learning has been an active 
research area recently \citep{szegedy2013intriguing, goodfellow2015explaining, athalye2018obfuscated},
in which algorithms are developed to 
either attack existing models by constructing adversarial examples, or 
 train robust models to defend adversarial attacks. 
More related to our work, 
\citep{sankaranarayanan2018regularizing} proposes a layer-wise adversarial training method to regularize deep neural networks. 
In statistics learning and robust statistics, 
various adversarial-like ideas are also leveraged to construct efficient and robust estimators, mostly for preventing model specification or data corruption \citep[e.g.,][]{maronna2018robust, duchi2016statistics}.     
Compared to these works, our work  
leverages the adversarial idea
as a regularization technique specifically for neural language models and focuses on introducing adversarial noise only on the softmax layers, so that a simple closed form solution can be obtained.


\paragraph{Direct Diversity Regularization}  There has been a body of literature on
increasing the robustness by 
directly 
adding various forms of diversity-enforcing penalty functions \citep[e.g.,][]{elsayed2018large, xie2016diverse, liu2016large, liu2017sphereface, chen2017noisy, wang2018additive}. In the particular setting of enforcing diversity of word embeddings, 
\citet{gao2018representation} show 
that adding a cosine similarity regularizer improves language modeling performance, which has the form 

However, in language modeling, 
one disadvantage of the direct diversity regularization approach  is that 
the vocabulary size  can be huge, and
calculating the summation term exactly at each step is not feasible, while 
approximation with mini-batch samples may  make it ineffective. Our method promotes diversity implicitly with theoretical guarantees and does not introduce  computational overhead. 






\paragraph{Large-margin classification}
In a general sense, our method can be seen as an instance of
 constructing
 large-margin classifiers by enforcing the distance of a word to its neighbors larger than a margin if it's recognized by any context.
Learning large-margin classifiers has been extensively studied in the literature; 
 see e.g., \citet{weston1999support, tsochantaridis2005large, jiang2018predicting, elsayed2018large, liu2016large, liu2017sphereface}. 



\paragraph{Other Regularization Techniques for  
Language Models} 
Various other techniques have been also developed to address overfitting in RNN language models. 
For example, 
\citet{gal2016theoretically} propose to use variational inference-based dropout 
\citep{srivastava2014dropout} on recurrent neural networks, 
 in which the same dropout mask
 is repeated  at each time step for inputs, outputs, and recurrent layers  
for regularizing RNN models.  
\citet{merity2017regularizing} suggest to use DropConnect \citep{wan2013regularization} 
on the recurrent weight matrices and report a series of encouraging benchmark results.
Other types of regularization include 
activation regularization \citep{merity2017revisiting},
layer normalization \citep{ba2016layer}, 
and frequency agnostic training \citep{gong2018frage}, etc.
Our work is orthogonal to these regularization and optimization techniques
and can be easily combined with them to achieve further improvements, as we demonstrate in our experiments. 





%
 
\section{Empirical Results} 
\label{sec:experiement}
We demonstrate the effectiveness of our method
in two applications: neural language modeling and 
neural machine translation, and compare them with state-of-the-art architectures and learning methods.
All models are trained with the weight-tying trick \citep{press2016using, inan2016tying}. 
Our code is available at:~ \url{https://github.com/ChengyueGongR/advsoft}.



\begin{table*}[ht]
\begin{center}
    \setlength{\tabcolsep}{1.2em}
        \begin{tabular}{l|c||cc}
            \toprule
            Method & \bf Params & Valid & Test \\
            \hline
            Variational LSTM~\cite{gal2016theoretically} & 19M & - & 73.4 \\
            Variational LSTM + weight tying~\cite{inan2016tying} & 51M & 71.1 & 68.5 \\
            NAS-RNN~\cite{zoph2016neural} & 54M & - & 62.4 \\
            DARTS~\cite{liu2018darts} & 23M & 58.3 & 56.1 \\
            \hline
            \multicolumn{4}{l}{w/o dynamic evaluation} \\ 
            \hline
            AWD-LSTM ~\citep{merity2017regularizing} & 24M & 60.00 & 57.30\\
            \bf{AWD-LSTM + Ours} & 24M & \bf{57.15} & \bf{55.01}\\
AWD-LSTM + MoS \citep{yang2017breaking} & 22M & 56.54 & 54.44 \\
            \bf AWD-LSTM + MoS + Ours & 22M & \bf{54.98} & \bf{52.87} \\
            AWD-LSTM + MoS + Partial Shuffled \citep{press2019partially} & 22M & 55.89 & 53.92 \\
            \bf AWD-LSTM + MoS + Partial Shuffled + Ours & 22M & \bf{54.10} & \bf{52.20} \\
\hline
            \multicolumn{4}{l}{+~~dynamic evaluation \citep{krause2017dynamic}} \\ 
            \hline
            AWD-LSTM 
            ~\citep{merity2017regularizing}   & 24M & 51.60 & 51.10\\
            \bf{AWD-LSTM +Ours}  & 24M & \bf{49.31} & \bf{48.72}\\
AWD-LSTM + MoS 
            \citep{yang2017breaking}  & 22M & 48.33 & 47.69 \\
            \bf AWD-LSTM + MoS + Ours & 22M & \bf{47.15} & \bf{46.52} \\
            AWD-LSTM + MoS + Partial Shuffled \citep{press2019partially} & 22M & 47.93 & 47.49 \\
            \bf AWD-LSTM + MoS + Partial Shuffled + Ours & 22M & \bf{46.63} & \bf{46.01} \\
            \bottomrule
        \end{tabular}
    \end{center}
\caption{\label{PTB-table}
Perplexities on the validation and test sets on the Penn Treebank dataset.  Smaller perplexities refer to better language modeling performance. {\tt Params} denotes the number of model parameters. 
}
\end{table*}

\begin{table*}[ht]
\begin{center}
    \setlength{\tabcolsep}{1.2em}
        \begin{tabular}{l|c||cc}
            \toprule
            Method & \bf Params & Valid & Test\\
            \hline
            Variational LSTM~\cite{inan2016tying} (h = 650)  &28M &92.3& 87.7\\
			Variational LSTM~\cite{inan2016tying} (h = 650) + weight tying &28M& 91.5& 87.0\\
1-layer LSTM~\cite{mandt2017stochastic} &24M& 69.3 &65.9\\
			2-layer skip connection LSTM ~\cite{mandt2017stochastic} (tied) &24M &69.1& 65.9\\
			DARTS~\cite{liu2018darts} & 33M & 69.5 & 66.9 \\
			\hline
\hline
\multicolumn{4}{l}{w/o dynamic evaluation} \\
            \hline
            AWD-LSTM ~\citep{merity2017regularizing} & 33M & 68.60 & 65.80\\
           \bf{AWD-LSTM + Ours} & 33M & \bf{64.01} & \bf{61.56}\\
            AWD-LSTM + MoS \citep{yang2017breaking} & 35M & 63.88 & 61.45 \\
            \bf AWD-LSTM + MoS + Ours & 35M & \bf{61.93} & \bf{59.62} \\
            AWD-LSTM + MoS + Partial Shuffled \citep{press2019partially} & 35M & 62.38 & 59.98 \\
            \bf AWD-LSTM + MoS + Partial Shuffled + Ours & 35M & \bf{61.10} & \bf{58.95} \\
            \hline
            \multicolumn{4}{l}{+~~dynamic evaluation \citep{krause2017dynamic}} \\ 
            \hline
             AWD-LSTM ~\citep{merity2017regularizing}  & 33M & 46.40 & 44.30\\
            \bf{AWD-LSTM + Ours}  & 33M & \bf{42.48} & \bf{40.71}\\
            AWD-LSTM + MoS \citep{yang2017breaking}  & 35M & 42.41 & 40.68 \\
            \bf AWD-LSTM + MoS + Ours  & 35M & \bf{40.27} & \bf{38.65} \\
            AWD-LSTM + MoS + Partial Shuffled \citep{press2019partially} & 35M & 40.75 & 39.03 \\
            \bf AWD-LSTM + MoS + Partial Shuffled + Ours & 35M & \bf 39.58 & \bf 38.07 \\
            \bottomrule
        \end{tabular}
    \end{center}
\caption{\label{WT2-table} Perplexities on validation and test sets on the Wikitext-2 dataset.} 
\end{table*}


\begin{table*}[ht]
\begin{center}
    \setlength{\tabcolsep}{1.5em}
        \begin{tabular}{l|cc}
            \toprule
            Method & Valid & Test\\
            \hline
            LSTM~\cite{grave2016efficient} & - & 48.7 \\
            Temporal CNN~\cite{bai2018convolutional} & - & 45.2 \\
            GCNN~\cite{dauphin2016language}  & - & 37.2\\
LSTM + Hebbian~\citep{rae2018fast} & 34.1 &  34.3 \\
            \hline
             4 layer QRNN~\cite{merity2018analysis} & 32.0 & 33.0\\
             \bf{4 layer QRNN + Ours}  & \bf{30.6} & \bf{31.6}\\
             \hline \multicolumn{3}{l}{+~~post process \cite{rae2018fast}} \\
             \hline
             LSTM + Hebbian + Cache + MbPA~\cite{rae2018fast} & 29.0 & 29.2\\
             \bf{4 layer QRNN + Ours + dynamic evaluation}  & \bf{27.2} & \bf{28.0}\\
            \bottomrule
        \end{tabular}
    \end{center}
\caption{\label{WT103-table} Perplexities on validation and test sets on the  Wikitext-103 dataset. 
}
\end{table*}

\subsection{Experiments on Language Modeling}
We test our method on three benchmark datasets: 
Penn Treebank (PTB), Wikitext-2 (WT2) and Wikitext-103 (WT103). 



\paragraph{PTB
}
The PTB corpus~\citep{marcus1993building} 
has been a standard dataset used for benchmarking language models. It consists of 923k training, 73k validation and 82k test words.
We use the processed version provided by \citet{mikolov2010recurrent}
that is widely used for this dataset \citep[e.g.,][]{merity2017regularizing, yang2017breaking, kanai2018sigsoftmax, gong2018sentence}.


\paragraph{WT2 and WT103
} 
The WT2 and WT103 datasets are introduced in \citet{merity2016pointer} as an alternative to the PTB dataset, 
and which contain lightly pre-possessed Wikipedia articles. 
The WT2 and WT103 contain approximately 2 million and 103 million words, respectively. 


\paragraph{Experimental settings}
For the PTB and WT2 datasets, 
we closely follow the regularization and optimization techniques introduced in
AWD-LSTM~\citep{merity2017regularizing},
which stacks a three-layer LSTM and 
performs optimization with a bag of tricks.







The WT103 corpus contains around 103 million tokens, which is significantly larger than 
the PTB and WT2 datasets. 
In this case, we use Quasi-Recurrent neural networks (QRNN)-based language models \citep{merity2018analysis, bradbury2016quasi} as our base model for efficiency. 
QRNN allows for parallel computation across both time-step and minibatch dimensions,
enabling high throughput and good scaling for long sequences and large datasets. 


\citet{yang2017breaking} show that softmax-based language models yield low-rank
approximations and do not have enough capacity to model complex natural language.
They propose a mixture of softmax (MoS) to break the softmax bottleneck and achieve
significant improvements. 
We also evaluated our method within the MoS framework by 
directly following the experimental settings in \citet{yang2017breaking}, 
except we replace the original softmax function with our adversarial softmax function.

The training procedure of AWD-LSTM-based language models can be decoupled into two stages: 1) optimizing the model with SGD and averaged SGD (ASGD); 
2) restarting ASGD for fine-tuning.
We report the perplexity scores at the end of both stages.
We also report the perplexity scores with a recent proposed post-process method, dynamical evaluation \citep{krause2017dynamic}
after fine-tuning.



\paragraph{Applying Adversarial MLE training} 
To investigate the effectiveness of our approach, we simply replace the softmax layer of baseline methods with our adversarial softmax function, 
with all other the parameters and architectures untouched. 
We empirically found that adding small annealed Gaussian noise in the input embedding layer makes our noisy model converge more quickly.
We experimented with different ways of scaling the Gaussian dropout level and
found that a small Gaussian noise with zero mean and a small variance,
such that it decreases from 0.2 to 0.0 over the duration of the run, 
works well for all the tasks.

Note the optimal adversarial noise  (see Algorithm~\ref{alg:main}) given RNN
prediction  associated with a target word .
Here,   controls the magnitude of of the noise level. 
When , our approach reduces to the original MLE training.  
We propose setting the noise level adaptive 
that proportional to the L2 norm of the target word embeddings, namely, by setting  with  as a hyperparameter. 


Figure~\ref{fig:adversarial_noise} shows the training and validation perplexities 
on the PTB dataset with different choices of . We find that  in the range of  perform similarly well.   
 Larger values (e.g., ) causes more difficult optimization and hence underfitting, 
 while smaller values (e.g.,  (the baseline approach))
 tends to overfit as we observe from standard MLE training. 
We set  for the rest of experiments  unless otherwise specified. 


\begin{figure}[ht]
    \centering
     \setlength\tabcolsep{2pt} \begin{tabular}{cc}
    \raisebox{1em}{\rotatebox{90}{\small Training Perplexity}}
    \includegraphics[width=0.21\textwidth]{./figures/ptb_train} &
    \raisebox{1em}{\rotatebox{90}{\small Validation Perplexity}}
    \includegraphics[width=0.21\textwidth]{./figures/ptb_valid}\\
     Epochs & Epochs \\
    \end{tabular}
    \caption{Training and validation perplexities on the PTB dataset with different choices of adversarial perturbation magnitude.}
    \label{fig:adversarial_noise}
\end{figure}




\paragraph{Results on PTB and WT2}
The results on the PTB and WT2 corpus are illustrated in Tables~\ref{PTB-table}
and \ref{WT2-table}, respectively. 
Methods with our adversarial softmax outperform
the baselines in all settings.  
Our results establish a new single model state-of-the-art
on PTB and WT2, achieving perplexity scores of 46.01 and 38.07, respectively.
Specifically, our approach significantly improves AWD-LSTM by a margin of  
2.29/2.38 and 3.92/3.59 in validation and test perplexity on the PTB and WT2 dataset.
We also improve the AWD-LSTM-MoS baseline 
by an amount of  1.18/1.17 and 2.14/2.03 in perplexity
for both datasets.




\paragraph{Results on WT103}
Table~\ref{WT103-table} shows that
on the large-scale WT103 dataset,
we improve the QRNN baseline with 1.4/1.4 points in perplexity on validation and test sets, respectively. 
With dynamic evaluation,
our method can achieve a test perplexity of 28.0,
which is,
to the authors' knowledge,
better than all existing CNN- or RNN-based models 
with similar numbers of model parameters.





\paragraph{Analysis}
We further analyze the properties of the learned word embeddings on the WT2 dataset.
Figure~\ref{fig:ptb_wt_ppl} (a) shows the distribution (via kernel density estimation) of the L2 distance between each word and its nearest neighbor learned by our method and the baseline, 
which verifies the diversity promoting property of our method. 
Figure~\ref{fig:ptb_wt_ppl} (b) shows the  
singular values of word embedding matrix learned by our model and that by the baseline model. 
We can see that, when trained with our method, the singular values distribute more uniformly,
an indication that our embedding vectors fills a higher dimensional subspace. 


 Figure~\ref{fig:ptb_wt_ppl} (c) 
 shows the training and validation 
 perplexities  of our method and baseline on AWD-LSTM. 
 We can see that our method is less prone to overfitting. While the baseline model reaches a smaller training error quickly, our method has a larger training error at the same stage because it optimizes a more difficult adversarial  objective, yet yields a significantly lower 
 validation error. 







\subsection{Experiments on Machine Translation}
We apply our method on machine translation tasks. Neural machine translation aims at building a single neural network
that maximize translation performance. 
Given a source sentence , translation is equivalent to finding a target 
sentence  by maximizing the conditional probability .
Here, we fit a parametrized model to maximize the conditional probability
using a parallel training corpus. Specifically, we use an RNN encoder-decoder framework \citep{cho2014learning, gehring2017convolutional, vaswani2017attention}, upon which we apply 
our adversarial MLE training that learns to translate. 




\paragraph{Datasets} 
We evaluate the proposed method on two translation tasks:
WMT2014 English  German (EnDe) and IWSLT2014 German  English (DeEn) translation. 
We use the parallel corpora publicly available at WMT 2014  and IWSLT 2014, 
which have been widely used for benchmark neural machine translation tasks~\cite{vaswani2017attention,gehring2017convolutional}. 
For fair comparison, we follow the standard data pre-processing procedures described in \citet{ranzato2015sequence, bahdanau2016actor}.


\noindent\emph{WMT2014 EnDe}
We use the original training set for model training, which consists of 
4.5 million sentence pairs. Source and target sentences are encoded by 
37K shared sub-word tokens based on byte-pair encoding (BPE) \citep{sennrich2015neural}.
We use the concatenation of newstest2012 and newstest2013 
as the validation set and test on newstest2014. 

\noindent\emph{IWSLT2014 DeEn}
This dataset contains 160K training sequences pairs and 7K validation sentence
pairs. Sentences are encoded using BPE with a shared vocabulary of about 33K tokens.
We use the concatenation of dev2010, tst2010, tst2011 and tst2011 as the test set, which is widely used in prior works~\citep{bahdanau2016actor}.

\paragraph{Experimental settings}
We choose the Transformer-based state-of-the-art machine translation model 
~\citep{vaswani2017attention} 
as our base model and use \textit{Tensor2Tensor}~\citep{tensor2tensor}~\footnote{https://github.com/tensorflow/tensor2tensor}
for implementation.
Specifically, 
to be consistent with prior works,
we closely follow the settings reported in \citet{vaswani2017attention}.
We use the Adam optimizer~\cite{kingma2014adam}
and follow the learning rate warm-up strategy in~\citet{vaswani2017attention}.
Sentences are pre-processed using byte-pair encoding~\citep{BPE} into subword tokens before training, 
and we measure the final performance with the BLEU score.



For the WMT2014 DeEn task,
we evaluate on the \textit{Transformer-Base} and \textit{Transformer-Big} architectures,
which consist of a 6-layer encoder and a 6-layer decoder with 512-dimensional and 1024-dimensional hidden units per layer, respectively. 
For the IWSLT2014 DeEn task, 
we evaluate on two standard configurations: \textit{Transformer-Small} and \textit{Transformer-Base}.
For \textit{Transformer-Small}, we stack a 4-layer encoder and a 4-layer decoder with 256-dimensional hidden units per layer. For \textit{Transformer-Base}, we set the batch size to 6400 and the dropout rate to 0.4 following \citet{wang2018multiagent}. For both tasks, we share the BPE subword vocabulary for decoder and encoder.


\paragraph{Results}
From Table~\ref{WMT-table} and Table~\ref{IWSLT-table}, 
we can see that our method
improves over the baseline algorithms for all settings.  
On the WMT2014 DeEn translation task, 
our method reaches 28.43 and 29.52 in BLEU score 
with the \textit{Transformer Base} and \textit{Transformer Big} architectures,  respectively; 
this yields an 1.13/1.12 improvement over their corresponding baseline models. 
On the IWSLT2014 DeEn dataset,
our method improves the BLEU score from 32.47 to  33.61 and 34.43 to 35.18 for the  \textit{Transformer-Small} and \textit{Transformer-Base} configurations, respectively.




















\begin{table}[ht]
  	\begin{center}
  	\setlength\extrarowheight{1.5pt}
	\begin{tabular}{lc}
\toprule 
\bf  Method & \bf  BLEU \\
\hline
Local Attention~\cite{luong2015effective} &  20.90 \\
ByteNet~\cite{kalchbrenner2016neural} &  23.75 \\
ConvS2S~\cite{gehring2017convolutional} &  25.16 \\
\hline
Transformer Base ~\cite{vaswani2017attention}&  27.30 \\
\bf{Transformer Base + Ours} &  \bf {28.43} \\
\hline
Transformer Big ~\cite{vaswani2017attention}&  28.40 \\
Transformer Big + ~\cite{gao2018representation} &  28.94 \\
\bf{Transformer Big + Ours} &  \bf{29.52} \\
\bottomrule 
\end{tabular}
\end{center}
\caption{\label{WMT-table}  BLEU scores on the WMT2014 EeDe machine translation task. }
\end{table}

\begin{table}[ht]
  	\begin{center}
  	\setlength\extrarowheight{1.5pt}
	\begin{tabular}{lc}
\toprule 
\bf  Method & \bf  BLEU \\
\hline
Actor-critic~\cite{bahdanau2016actor}  &  28.53 \\
CNN-a~\cite{gehring2016convolutional}  &  30.04\\
\hline
Transformer Small~\cite{vaswani2017attention} &  32.47 \\
\bf{Transformer Small + Ours} &  \bf 33.61 \\
\hline
Transformer Base + \cite{wang2018multiagent} &  34.43 \\
\bf{Transformer Base + Ours} &  \bf 35.18 \\
\bottomrule
\end{tabular}
\end{center}
\caption{\label{IWSLT-table} BLEU scores on the IWSLT2014 DeEn machine translation task. }
\end{table}


 
\section{Conclusion}
In this work, we present an adversarial MLE training strategy for  neural language modeling,  which promotes diversity in the embedding space and improves the generalization performance.
Our approach can be easily used as a drop-in  replacement for standard MLE-based model
with no additional training parameters and  computational overhead. 
Applying this approach to a variety of language modeling and machine translation tasks, we achieve improvements over state-of-the-art baseline models on standard benchmarks. 

\section*{Acknowledgement}
This work is supported in part by NSF CRII 1830161 and NSF  CAREER 1846421.
We would like to acknowledge Google Cloud for their support.


\bibliographystyle{iclr2019_conference}
\bibliography{advsoft}

\end{document}
