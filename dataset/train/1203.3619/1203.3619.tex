We have implemented both the HWM and SHALE  algorithms described in
Section~\ref{sec:solution} and benchmarked their performance against
the full solution approach (known hereafter as XPRESS) on
historical booked contract sets. 
We have extensively tuned the parameters for XPRESS, so it is much
faster than just using it ``off-the-shelf.''
First we describe these datasets
and our chosen performance metrics and then present our evaluation
results.

\subsection{Experimental setup}
\label{experimental setup}
In order to test the ``real-world'' performance of all three
algorithms we considered 6 sets of real GD contracts booked and
active in the recent past. In particular, we chose three periods of
time, each for one to two weeks, 
and two ad positions
LREC and SKY for each of these time periods. 


We considered US region contracts booked to the aforementioned
positions and time periods and also excluded all frequency capped
contracts and all contracts with time-of-day and other custom targets.
Also, all remaining contracts that were
active for longer than the specified date ranges were truncated and
their demands were proportionally reduced. Next, 
we generated a bipartite graph for each contract set as in
Figure~\ref{fig:bipartite1}; by sampling $50$ eligible impressions for each
contract in the set. This sampling procedure is described in detail in
\cite{vvs10}. We then ran HWM, SHALE and XPRESS on each of the 6 graphs
and evaluated the following metrics.
\begin{enumerate}
\item  {\bf Under-delivery Rate} : This represents the total
  under-delivered impressions as a proportion of the booked demand,
  i.e.,
\begin{equation}
\label{eq:under-delivery rate}
U \ = \ \frac{\sum_{j} u_j}{\sum_{j} d_j}
\end{equation}
\item {\bf Penalty Cost} : This represents the penalty incurred
  by the publisher for failing to deliver the guaranteed number of
  impressions to booked GD contracts. Note that the true long-term penalty 
  due to under-delivery  is not known since we cannot easily forecast
  how an advertiser's future business with the publisher will change due to
  under-delivery on a booked contract. Here we define the total
  penalty cost to be
\begin{equation}
\label{eq: penalty cost def}
P \ = \ \sum_{j}p_ju_j
\end{equation}
where $u_j$ is the number of under-delivered impressions to contract
$j$ and $p_j$ is the cost for each under-delivered
impression. For our experiments, we set $p_j$ to be $p_j =
0.005 \ + \ q_j$ where $q_j$ is the revenue per delivered
impression from contract $j$. Indeed, it is intuitive and reasonable
to expect that contracts that are more valuable to the advertiser
incur larger penalties for under-delivery. The offset (here $\$5 CPM$)
serves to ensure that our algorithms attempt to fully deliver even the
contracts with low booking prices.

\item {\bf L2 Distance} : This metric shows how much the generated
  allocation deviates from a desired allocation (for example a
  perfectly representative one). In particular, the L2 distance is
  the non-representativeness function 
  $\frac{1}{2}\sum_{i\in\neij} s_i\frac{V_j}{\theta_{ij}}(x_{ij} - \theta_{ij})^2$,
  the first term of the objective
  function in Section~\ref{sec:preliminaries}, corresponding to the
weighted $\ell_2^2$ distance between target and allocation.
\end{enumerate}

\subsection{Experiment 1}
\label{subsec:expt 1}
As we mentioned earlier, SHALE was designed to provide a trade-off
between the speed of execution of HWM and the quality of solutions
output by XPRESS. Accordingly in our first experiment we measured the
performance of SHALE (run for 0, 5, 10, 20 and 50 iterations)
as compared to XPRESS against our chosen metrics.  Since SHALE at 0 iterations
is the same as HWM, we label it as such.
\begin{figure}[h!]
\centering
\includegraphics*[viewport=37 155 760 575,scale=0.32]{experiment1.pdf}
\caption{Performance Vs. Completion time}
\label{fig:expt 1}
\end{figure}
Figure~\ref{fig:expt 1} shows the penalty cost, under-delivery rate,
L2 distance and completion  for HWM and SHALE run for 5, 10, 20 and 50
iterations respectively as a percentage of the corresponding metric
for XPRESS, averaged over our 6 chosen contract sets. 
Note that the y-axis labels for the under-delivery
rate and penalty cost are on the left, while the labels for the L2 distance and completion time are on the right.

It is immediately clear that SHALE after only 10 iterations is within  2\% of XPRESS with respect to penalty cost and
under-delivery rate. Further, note that SHALE after 10 iterations is able to provide an
allocation whose L2 distance is less than half that of
XPRESS.  (Recall smaller L2 distance means the solution is more representative, so SHALE is doing twice as well on this metric.) 
This somewhat surprising result seems to be an artifact of the SHALE algorithm: The functional form of $\g$ is determined 
by the representativeness objective, so we can think of representativeness as ``driving'' the algorithm.

Even at 50 iterations, SHALE is more than 5 times as fast as XPRESS.  Remarkably, its penalty and under-delivery
are almost equal to XPRESS (less than 1\% different), yet the L2 distance is still much better.
At 20 iterations, we see SHALE gives a very high-quality solution, despite being about an order of
magnitude faster than the commercial solver.


\subsection{Experiment 2}
\label{subsection:expt_serv}
We next study how SHALE performs compared to the optimal 
algorithm when used to serve real world sampled impressions from 
actual server logs. This experiment uses real contracts and real adserver logs 
(downsampled) for performing the complete offline simulation.

\subsubsection{Setup}
Here we take three new datasets which consists of real guaranteed delivery contracts 
from Yahoo! active during different one to two week periods in the past year. We 
run our optimization algorithms and serve real downsampled serving logs for 
each of the one-to-two week periods, reoptimizing every two hours.  That is,
the offline optimizer creates an allocation plan to serve the contracts for the remaining
duration; we serve for two hours using that plan; collect the delivery stats so far;
then re-optimize for the rest of the duration using the updated stats.  Note that the
two-hours corresponds to two hours of serving logs.  Our actual simulation is somewhat
faster due to the downsampling.

\subsubsection{Algorithms compared}
At the end of the simulation, we look at the contracts that start and end within 
the simulation period and compare how metrics of under-delivery and penalty across 
HWM, SHALE and DUAL algorithms.  
Our DUAL solution is obtained by running a coordinate gradient descent algorithm 
till convergencence; if our forecasts had been perfect, this would have produced optimal
delivery.
The SHALE algorithms are run with setting of 0, 5, 10 and 20 iterations, with the 0-iteration
version labeled as HWM.

We performed serving using the reconstruction algorithm described in Section
\ref{subsec:online_serving}. 

\subsubsection{Metrics}
The metrics include the underdelivery metric and penalty metrics as defined in 
Equation \ref{eq:under-delivery rate} and in Equation \ref{eq: penalty cost def}
For these set of experiments, we set $p_j$ to be $p_j =
0.002 \ + \ 4*q_j$ where $q_j$ is the revenue per delivered
impression from contract $j$. 

We also compare another metric called {\em pacing} between these algorithms. This captures 
how representative contracts are with respect to time during the delivery of these contracts.
The {\em linear goal} of a contract at a given time is the amount of delivery was perfectly
smooth with respect to time.  For example, a 7 day contract with demand of 14 million has a linear
goal of 6 milion on day 3.
In this experiment, pacing is defined as the percentage of contracts that are within 12\% of the linear 
delivery goal at least 80\% of their active duration. 

\subsubsection{Results}
Figures \ref{fig:dataset1}, \ref{fig:dataset2} and \ref{fig:dataset3} show that the 
under-delivery and penalty cost for HWM (SHALE with 0 iterations) algorithm 
is the worst. Further, as the number of SHALE iterations increase it gets very close to the DUAL
algorithm. Note that even SHALE with 5 or 10 iterations performs as well or sometimes 
slightly better than the DUAL algorithm. This can be attributed to different reasons;
one being the fact that there are forecasting errors intrinsic to using real serving logs. 
Another contributing factor is the fact that the DUAL algorithm does not
directly optimize for either of these metrics.
In addition, Stage Two
attempts to fulfill the delivery of every contract, even if it is not optimal according to the objective
function.  This heuristic aspect of SHALE actually appears to aid in its performance when judged by
simple metrics like delivery.

\begin{figure}[h!]
\centering
\includegraphics*[scale=0.32]{dataset1.pdf}
\caption{Dataset 1: Under Delivery and Penalty Cost Comparison}
\label{fig:dataset1}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics*[scale=0.32]{dataset2.pdf}
\caption{Dataset 2: Under Delivery and Penalty Cost Comparison}
\label{fig:dataset2}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics*[scale=0.32]{dataset3.pdf}
\caption{Dataset 3: Under Delivery and Penalty Cost Comparison}
\label{fig:dataset3}
\end{figure}

Figure \ref{fig:pacing} shows how these algorithm perform with respect to pacing.
The pacing is similar for all three datasets for SHALE with 5, 10 and 20 iterations when compared with 
the DUAL algorithm.  Surprisingly, HWM has better pacing than SHALE and DUAL for two of the datasets.
One possible reason for this is that SHALE and DUAL algorithm gives better under-delivery and 
penalty cost, compromising some pacing. Note that the time dimension is just one of the many dimensions
that the representativeness portion of the objective function.
This may also be an artifact of forecasting errors.  In real systems, certain additional modifications are employed
to ensure good pacing.  For these experiments, we have removed those modifications to give a clearer picture of
how the base algorithms perform.

\begin{figure}[h!]
\centering
\includegraphics*[scale=0.32]{pacing.pdf}
\caption{Pacing Comparisons on all three datasets}
\label{fig:pacing}
\end{figure}


\subsection{Experiment 3}
\label{subsec:expt 2}
Superficially, HWM and SHALE both perform well.  In this experiment, we do a  more detailed
simulation to compare HWM and SHALE.
We fix the iteration count for SHALE at 20 and test
its performance under varying supply levels. Specifically, for each of our 6 contract sets, we
artificially reduced the supply weight on each of the supply nodes
while keeping the graph structure fixed in order to simulate the
increasing scarcity of supply. We define the average supply contention (ASC)
metric to represent the scarcity of supply, as follows
\begin{equation}
\label{eqn:avg supply cont}
\textrm{ASC} \ = \ \frac{\sum_i s_i \left(\sum_{j\in
      i}\frac{d_j}{S_j}\right)}{\sum_i s_i}
\end{equation}
where $s_i$ represents the supply weight and $d_j$ and $S_j$ represent
the demand and eligible supply for contract $j$.
In Figure~\ref{fig:expt 2}, we show the
under-delivery rate, penalty cost and L2 distance for SHALE as a
percentage of the corresponding metric for HWM for various levels of
ASC.
\begin{figure}[h!]
\centering
\includegraphics*[viewport=37 155 760 575,scale=0.32]{experiment2.pdf}
\caption{SHALE Vs. HWM}
\label{fig:expt 2}
\end{figure}
First we note that each of our metrics for SHALE is better than the
corresponding metric for HWM for all values of ASC. Indeed, the SHALE
L2 distance is less than 50\% of that for HWM. Also note 
that the SHALE penalty cost consistently improves compared to HWM as
the ASC increases.  This indicates that even though HWM appears to have better
pacing for some data sets, SHALE is still a more robust algorithm and is likely
preferrable in most situations.  (Indeed, we see very consistently that its
under-delivery penalty and revenue are both clearly better.)


