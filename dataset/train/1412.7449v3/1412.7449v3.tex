\documentclass{article} \usepackage{nips15submit_e,times}
\usepackage{url}
\usepackage{tikz}
\usepackage[draft]{fixme}
\usepackage[small,tight]{bibhacks}
\usepackage[colorlinks=true, linkcolor=black, citecolor=black, filecolor=black,
  urlcolor=black]{hyperref} \usepackage{epstopdf}
\usepackage{enumerate}
\usepackage{booktabs}

\newcommand{\xxcomment}[4]{\textcolor{#1}{[ #4]}}
\newcommand{\spcomment}[1]{\xxcomment{blue}{S}{P}{#1}}
\newcommand{\iscomment}[1]{\xxcomment{red}{I}{S}{#1}}
\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}

\title{Grammar as a Foreign Language}


\author{
Oriol Vinyals\thanks{Equal contribution}\\
Google\\
\texttt{vinyals@google.com}
\And
Lukasz Kaiser\samethanks \\
Google\\
\texttt{lukaszkaiser@google.com}
\AND
Terry Koo \\
Google\\
\texttt{terrykoo@google.com}
\And
Slav Petrov \\
Google\\
\texttt{slav@google.com}
\And
Ilya Sutskever \\
Google\\
\texttt{ilyasu@google.com}
\And
Geoffrey Hinton \\
Google\\
\texttt{geoffhinton@google.com}
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\newcommand\citet\cite
\newcommand\citep\cite

\nipsfinalcopy 

\begin{document}


\maketitle
\vspace{-4mm}

\begin{abstract}
Syntactic constituency parsing is a fundamental problem in natural
language processing and has been the subject of intensive research
and engineering for decades.  As a result, the most accurate parsers
are domain specific, complex, and inefficient.  In this paper we show
that the domain agnostic attention-enhanced sequence-to-sequence model
achieves state-of-the-art results on the most widely used syntactic
constituency parsing dataset, when trained on a large synthetic
corpus that was annotated using existing parsers.  It also matches the
performance of standard parsers when trained only on a small
human-annotated dataset, which shows that this model is highly
data-efficient, in contrast to sequence-to-sequence models without the
attention mechanism.  Our parser is also fast, processing over a
hundred sentences per second with an unoptimized CPU implementation.
\end{abstract}
\vspace{-0mm}
\section{Introduction}
\label{sec:intro}

Syntactic constituency parsing is a fundamental problem in linguistics
and natural language processing that has a wide range of applications.
This problem has been the subject of intense research for decades, and
as a result, there exist highly accurate domain-specific parsers.
The computational requirements of traditional parsers are cubic in
sentence length, and while linear-time shift-reduce constituency parsers
improved in accuracy in recent years, they never matched state-of-the-art.
Furthermore, standard parsers have been designed with parsing in mind;
the concept of a parse tree is deeply ingrained into these systems,
which makes these methods inapplicable to other problems.

Recently, Sutskever et al.~\cite{sutskever14} introduced a neural
network model for solving the general sequence-to-sequence problem,
and Bahdanau et al.~\cite{bahdanau2014neural} proposed a related model
with an attention mechanism that makes it capable of handling long
sequences well.  Both models achieve state-of-the-art results on large
scale machine translation tasks (e.g., \cite{thang14,jean2014using}).
Syntactic constituency parsing can be formulated as a
sequence-to-sequence problem if we linearize the parse tree
(cf. Figure ~\ref{fig:task-example}), so we can apply these models to
parsing as well.

Our early experiments focused on the sequence-to-sequence model of
Sutskever et al.~\cite{sutskever14}.  We found this model to work poorly
when we trained it on standard human-annotated parsing datasets (1M tokens),
so we constructed an artificial dataset by labelling a large corpus with
the BerkeleyParser.  To our surprise, the sequence-to-sequence model
matched the BerkeleyParser that produced the annotation, having
achieved an F1 score of 90.5 on the test set (section 23 of the WSJ).

We suspected that the attention model of
Bahdanau et al.~\cite{bahdanau2014neural} might be more data efficient
and we found that it is indeed the case.  We trained a sequence-to-sequence
model with attention on the small human-annotated parsing dataset and were
able to achieve an F1 score of 88.3 on section 23 of the WSJ
without the use of an ensemble and 90.5 with an ensemble, which matches
the performance of the BerkeleyParser (90.4) when trained on the same data.

Finally, we constructed a second artificial dataset consisting of only
high-confidence parse trees, as measured by the agreement of two
parsers.  We trained a sequence-to-sequence model with attention on this
data and achieved an F1 score of 92.5 on section 23 of the WSJ -- a new
state-of-the-art.  This result did not require an ensemble, and as a result,
the parser is also very fast. An ensemble further improves the score to 92.8.
 \vspace{-0mm}
\section{LSTM+A Parsing Model}
\label{sec:model}

\newcommand\cb[1]{)}
\newcommand\pair[2]{#1{}#2}
\newcommand\eos{{\small \textsc{end}}}

\begin{figure}
\begin{center}
\begin{tikzpicture}[xscale=0.64,yscale=0.75]
\node (A) at (0, -1.7) {.};
\node (B) at (2, -1.7) {Go};

\draw[thick,->] (A) -- (0, -1.05);
\draw[thick,->] (B) -- (2, -1.05);

\draw (-0.8, -1.0) rectangle (2.8, 0.0);
\node (lstm) at (1, -0.5) {LSTM};
\draw[thick,->] (0, 0.05) -- (0, 0.45);
\draw[thick,->] (2, 0.05) -- (2, 0.45);
\draw (-0.8, 0.5) rectangle (2.8, 1.5);
\node (lstm) at (1, 1) {LSTM};
\draw[thick,->] (0, 1.55) -- (0, 1.95);
\draw[thick,->] (2, 1.55) -- (2, 1.95);
\draw (-0.8, 2) rectangle (2.8, 3);
\node (lstm) at (1, 2.5) {LSTM};

\draw[thick,->] (2.85, -0.5) -- (4.15, -0.5);
\draw[thick,->] (2.85, 1) -- (4.15, 1);
\draw[thick,->] (2.85, 2.5) -- (4.15, 2.5);

\begin{scope}[xshift=1cm]
\node (end) at (4, -1.7) {\eos};
\node (S) at (6, -1.7) {(S};
\node (VP) at (8, -1.7) {(VP};
\node (VB) at (10, -1.7) {XX};
\node (cVP) at (12, -1.7) {\cb{VP}};
\node (dot) at (14, -1.7) {.};
\node (cS) at (16, -1.7) {\cb{S}};

\draw[thick,->] (end) -- (4, -1.05);
\draw[thick,->] (S) -- (6, -1.05);
\draw[thick,->] (VP) -- (8, -1.05);
\draw[thick,->] (VB) -- (10, -1.05);
\draw[thick,->] (cVP) -- (12, -1.05);
\draw[thick,->] (dot) -- (14, -1.05);
\draw[thick,->] (cS) -- (16, -1.05);

\draw (3.2, -1.0) rectangle (16.8, 0.0);
\node (lstm) at (10, -0.5) {LSTM};
\draw[thick,->] (4, 0.05) -- (4, 0.45);
\draw[thick,->] (6, 0.05) -- (6, 0.45);
\draw[thick,->] (8, 0.05) -- (8, 0.45);
\draw[thick,->] (10, 0.05) -- (10, 0.45);
\draw[thick,->] (12, 0.05) -- (12, 0.45);
\draw[thick,->] (14, 0.05) -- (14, 0.45);
\draw[thick,->] (16, 0.05) -- (16, 0.45);
\draw (3.2, 0.5) rectangle (16.8, 1.5);
\node (lstm) at (10, 1) {LSTM};
\draw[thick,->] (4, 1.55) -- (4, 1.95);
\draw[thick,->] (6, 1.55) -- (6, 1.95);
\draw[thick,->] (8, 1.55) -- (8, 1.95);
\draw[thick,->] (10, 1.55) -- (10, 1.95);
\draw[thick,->] (12, 1.55) -- (12, 1.95);
\draw[thick,->] (14, 1.55) -- (14, 1.95);
\draw[thick,->] (16, 1.55) -- (16, 1.95);
\draw (3.2, 2) rectangle (16.8, 3);
\node (lstm) at (10, 2.5) {LSTM};

\node (S) at (4, 3.7) {(S};
\node (VP) at (6, 3.7) {(VP};
\node (VB) at (8, 3.7) {XX};
\node (cVP) at (10, 3.7) {\cb{VP}};
\node (dot) at (12, 3.7) {.};
\node (cS) at (14, 3.7) {\cb{S}};
\node (end) at (16, 3.7) {\eos};
\draw[thick,->] (4, 3.05) -- (S);
\path (4, 3.05) edge [out=165,in=15,->,color=gray] (0, 3.05);
\draw[thick,->] (6, 3.05) -- (VP);
\path (6, 3.05) edge [out=167,in=13,->,color=gray] (0, 3.05);
\draw[thick,->] (8, 3.05) -- (VB);
\path (8, 3.05) edge [out=169,in=11,->,color=gray] (0, 3.05);
\draw[thick,->] (10, 3.05) -- (cVP);
\path (10, 3.05) edge [out=171,in=9,->,color=gray] (0, 3.05);
\draw[thick,->] (12, 3.05) -- (dot);
\path (12, 3.05) edge [out=173,in=7,->,color=gray] (0, 3.05);
\draw[thick,->] (14, 3.05) -- (cS);
\path (14, 3.05) edge [out=175,in=5,->,color=gray] (0, 3.05);
\draw[thick,->] (16, 3.05) -- (end);
\path (16, 3.05) edge [out=177,in=3,->,color=gray] (0, 3.05);
\end{scope}
\end{tikzpicture}
\end{center}
\caption{\small A schematic outline of a run of our LSTM+A model on
the sentence ``Go.''. See text for details.}
\label{fig:model}
\end{figure}

Let us first recall the sequence-to-sequence LSTM model.
The Long Short-Term Memory model of \citet{hochreiter1997}
is defined as follows.  Let , , and  be the input, control
state, and memory state at timestep .
Given a sequence of inputs , the LSTM computes
the -sequence  and
the -sequence  as follows.

The operator  denotes element-wise multiplication,
the matrices  and the vector  are the parameters
of the model, and all the nonlinearities are computed element-wise.

In a deep LSTM, each subsequent layer uses the -sequence of the previous
layer for its input sequence . The deep LSTM defines a distribution over
output sequences given an input sequence:

The above equation assumes a deep LSTM whose input sequence is
, so  denotes -th element
of the -sequence of topmost LSTM.
The matrix  consists of the vector representations of
each output symbol and the symbol  is a Kronecker
delta with a dimension for each output symbol,
so  is precisely
the 'th element of the distribution defined by the softmax.
Every output sequence terminates with a special end-of-sequence token which is
necessary in order to define a distribution over sequences of variable lengths.
We use two different sets of LSTM parameters, one for the input sequence and
one for the output sequence, as shown in Figure \ref{fig:model}.
Stochastic gradient descent is used to maximize the training objective
which is the average over the training set of the log probability
of the correct output sequence given the input sequence.

\subsection{Attention Mechanism}
\label{sec:attention}
An important extension of the sequence-to-sequence model is by adding an
attention mechanism. We adapted the attention model from \cite{bahdanau2014neural}
which, to produce each output symbol , uses an attention mechanism over
the encoder LSTM states. Similar to our sequence-to-sequence model described
in the previous section, we use two separate LSTMs (one to encode the sequence
of input words , and another one to produce or decode the output
symbols ). Recall that the encoder hidden states are denoted
 and we denote the hidden states of the decoder by
.

To compute the attention vector at each output time  over the input
words  we define:

The vector  and matrices  are learnable parameters of the model.
The vector  has length  and its -th item contains a score of
how much attention should be put on the -th hidden encoder state .
These scores are normalized by softmax to create the attention mask  over
encoder hidden states.
In all our experiments, we use the same hidden dimensionality (256) at
the encoder and the decoder, so  is a vector and  and  are
square matrices. Lastly, we concatenate  with , which becomes
the new hidden state from which we make predictions, and which is fed to
the next time step in our recurrent model.

In Section~\ref{sec:analysis} we provide an analysis of what the attention
mechanism learned, and we visualize the normalized attention vector  for
all  in Figure~\ref{fig:attention}.

\subsection{Linearizing Parsing Trees}

To apply the model described above to parsing, we need to design an
invertible way of converting the parse tree into a sequence (linearization).
We do this in a very simple way following a depth-first
traversal order, as depicted in Figure~\ref{fig:task-example}.

\begin{figure}
  \begin{center}
  \begin{tikzpicture}[growth parent anchor=north,
      level distance=2em, xscale=1.5, yscale=1.3]
    \node (tokens) at (-2, 0) {John has a dog .};
    \node (arrow) at (-0.7, 0) {};
    \node (S) at (2, 0.5) {S}
      child { node {NP}
        child { node {NNP} }
      }
      child { node {VP}
        child { node {VBZ} }
        child { node {NP}
          child { node {DT} }
          child { node {NN} }
        }
      }
      child { node {.} }
    ;
    \node (tokens) at (-2, -1.7) {John has a dog .};
    \node (arrow) at (-0.7, -1.7) {};
    \node[anchor=west] (result) at (0, -1.7)
      {(S (NP NNP \cb{NP} (VP VBZ (NP DT NN \cb{NP} \cb{VP} . \cb{S}};
  \end{tikzpicture}
  \end{center}
  \caption{Example parsing task and its linearization.}
  \label{fig:task-example}
\end{figure}

We use the above model for parsing in the following way.
First, the network consumes the sentence in a left-to-right sweep,
creating vectors in memory. Then, it outputs the linearized parse tree
using information in these vectors. As described below, we use 3 LSTM layers,
reverse the input sentence and normalize part-of-speech tags.
An example run of our LSTM+A model on the sentence ``Go.'' is depicted in
Figure \ref{fig:model} (top gray edges illustrate attention).

\subsection{Parameters and Initialization}

\paragraph{Sizes.}
In our experiments we used a model with 3 LSTM layers and 256 units
in each layer, which we call LSTM+A. Our input vocabulary size was 90K
and we output 128 symbols.

\paragraph{Dropout.}
Training on a small dataset we additionally used 2 dropout layers, one
between LSTM and LSTM, and one between LSTM and LSTM.
We call this model LSTM+A+D.

\paragraph{POS-tag normalization.}
Since part-of-speech (POS) tags are not evaluated in the syntactic parsing F1
score, we replaced all of them by ``XX'' in the training data. This improved
our F1 score by about 1 point, which is surprising: For standard parsers,
including POS tags in training data helps significantly. All experiments
reported below are performed with normalized POS tags.

\paragraph{Input reversing.}
We also found it useful to reverse the input sentences but not their parse
trees, similarly to \cite{sutskever14}.
Not reversing the input had a small negative impact on the F1 score on our
development set (about  absolute).
All experiments reported below are performed with input reversing.

\paragraph{Pre-training word vectors.}
The embedding layer for our 90K vocabulary can be initialized randomly or
using pre-trained word-vector embeddings. We pre-trained skip-gram embeddings
of size 512 using \texttt{word2vec} \citep{mikolov2013efficient} on a 10B-word
corpus. These embeddings were used to initialize our network but not fixed,
they were later modified during training. We discuss the impact of pre-training
in the experimental section.

We do not apply any other special preprocessing to the data.
In particular, we do not binarize the parse trees or handle unaries
in any specific way. We also treat unknown words in a naive way:
we map all words beyond our 90K vocabulary to a single UNK token.
This potentially underestimates our final results, but keeps our
framework task-independent.
 \vspace{-3mm}
\section{Experiments}
\label{sec:exp}

\subsection{Training Data}

We trained the model described above on 2 different datasets.
For one, we trained on the standard WSJ training dataset.
This is a very small training set by neural network standards, as it contains
only 40K sentences (compared to 60K examples even in MNIST).
Still, even training on this set, we managed to get results that
match those obtained by domain-specific parsers.

To exceed the previous state-of-the-art, we created another, larger training
set of 11M parsed sentences (250M tokens). First, we collected all
publicly available treebanks.
We used the OntoNotes corpus version 5~\citep{hovy-EtAl:2006:NAACL},
the English Web Treebank~\citep{petrov-mcdonald:2012:SANCL} and the updated
and corrected Question Treebank~\citep{judge-etAl:2006:ACL}.\footnote{All
treebanks are available through the Linguistic Data Consortium
(LDC): OntoNotes (LDC2013T19), English Web Treebank (LDC2012T13) and
Question Treebank (LDC2012R121).}
Note that the popular Wall Street Journal section of the Penn
Treebank~\citep{marcus:1993:CL} is part of the OntoNotes corpus.
In total, these corpora give us 90K training sentences (we held out
certain sections for evaluation, as described below).

In addition to this gold standard data, we use a corpus parsed with existing
parsers using the ``tri-training'' approach of \cite{Li0C14}. In this approach,
two parsers, our reimplementation of BerkeleyParser~\cite{petrov-EtAl:2006:ACL}
and a reimplementation of ZPar~\cite{zhu-EtAl:2013:ACL}, are used to process
unlabeled sentences sampled from news appearing on the web. We select only
sentences for which both parsers produced the same parse tree and re-sample
to match the distribution of sentence lengths of the WSJ training corpus.
Re-sampling is useful because parsers agree much more often on short sentences.
We call the set of 11 million sentences selected in this way,
together with the 90K golden sentences described above,
the \emph{high-confidence corpus}.

In earlier experiments, we only used one parser, our reimplementation of BerkeleyParser,
to create a corpus of parsed sentences. In that case we just parsed 7 million
senteces from news appearing on the web and combined these parsed sentences with the
90K golden corpus described above. We call this the \emph{BerkeleyParser corpus}.

\subsection{Evaluation}

\begin{table}\centering
\small{
\begin{tabular}{|c|c|c|c|c|c|}
\hline
{\bf Parser}  & {\bf Training Set} & {\bf WSJ 22} & {\bf WSJ 23} \\ \hline
baseline LSTM+D & WSJ only &  70 &  70 \\
LSTM+A+D        & WSJ only & 88.7 & 88.3 \\
LSTM+A+D ensemble    & WSJ only & 90.7 & 90.5 \\
\hline
baseline LSTM   & BerkeleyParser corpus & 91.0 & 90.5 \\
LSTM+A          & high-confidence corpus & 93.3 & 92.5 \\
LSTM+A ensemble & high-confidence corpus & {\bf 93.5} & {\bf 92.8} \\
\specialrule{1pt}{-1pt}{0pt}
Petrov et al. (2006) \cite{petrov-EtAl:2006:ACL}
  & WSJ only & 91.1 & 90.4 \\
Zhu et al. (2013) \cite{zhu-EtAl:2013:ACL}
  & WSJ only & N/A & 90.4   \\
Petrov et al. (2010) ensemble \cite{petrov:2010:NAACL}
  & WSJ only & 92.5 & 91.8 \\
\hline
Zhu et al. (2013) \cite{zhu-EtAl:2013:ACL}
  & semi-supervised & N/A & 91.3 \\
Huang \& Harper (2009) \cite{huang-harper:2009:EMNLP}
  & semi-supervised & N/A & 91.3 \\
McClosky et al. (2006) \cite{mcclosky-etAl:2006:NAACL}
  & semi-supervised & 92.4 & 92.1 \\
Huang \& Harper (2010) ensemble \cite{huang-etAl:2010:EMNLP}
  & semi-supervised & 92.8 & 92.4 \\ \hline
\end{tabular}
}
\caption{F1 scores of various parsers on the development and test set.
  See text for discussion.}
\label{tab:res}
\end{table}

We use the standard EVALB tool\footnote{\url{http://nlp.cs.nyu.edu/evalb/}} for
evaluation and report F1 scores on our developments set (section 22 of
the Penn Treebank) and the final test set (section 23) in Table~\ref{tab:res}.

First, let us remark that our training setup differs from those reported
in previous works. To the best of our knowledge, no standard parsers have
ever been trained on datasets numbering in the hundreds of millions of tokens,
and it would be hard to do due to efficiency problems. We therefore cite
the semi-supervised results, which are analogous in spirit but use less data.

Table \ref{tab:res} shows performance of our models on the top and results
from other papers at the bottom.
We compare to variants of the BerkeleyParser that use
self-training on unlabeled data \citep{huang-harper:2009:EMNLP}, or built an ensemble
of multiple parsers \citep{petrov:2010:NAACL}, or combine both techniques.
We also include the best linear-time parser in the literature,
the transition-based parser of \citet{zhu-EtAl:2013:ACL}.

It can be seen that, when training on WSJ only, a baseline LSTM does not
achieve any reasonable score, even with dropout and early stopping.
But a single attention model gets to  and an ensemble of 5 LSTM+A+D models
achieves  matching a single-model BerkeleyParser on WSJ~23.
When trained on the large high-confidence corpus, a single LSTM+A model
achieves  and so outperforms not only the best single model, but also
the best ensemble result reported previously. An ensemble of 5 LSTM+A models
further improves this score to .

\paragraph{Generating well-formed trees.}
The LSTM+A model trained on WSJ dataset only produced malformed trees
for 25 of the 1700 sentences in our development set (\% of all cases),
and the model trained on full high-confidence dataset did this for 14
sentences (\%). In these few cases where LSTM+A outputs a malformed tree,
we simply add brackets to either the beginning or the end of the tree
in order to make it balanced. It is worth noting that all 14 cases where
LSTM+A produced unbalanced trees were sentences or sentence fragments that did not
end with proper punctuation. There were very few such sentences in the training data,
so it is not a surprise that our model cannot deal with them very well.


\paragraph{Score by sentence length.}
An important concern with the sequence-to-sequence LSTM was that it may not
be able to handle long sentences well.  We determine the extent of this problem
by partitioning the development set by length, and evaluating BerkeleyParser,
a baseline LSTM model without attention, and LSTM+A on sentences of each length.
The results, presented in Figure~\ref{fig:len}, are surprising. The difference
between the F1 score on sentences of length upto 30 and that upto 70 is 
for the BerkeleyParser,  for the baseline LSTM, and  for LSTM+A.
So already the baseline LSTM has similar performance to the BerkeleyParser,
it degrades with length only slightly. Surprisingly, LSTM+A shows \emph{less}
degradation with length than BerkeleyParser -- a full  chart parser
that uses a lot more memory.
\begin{figure}[h]
\begin{tikzpicture}[xscale=0.18,yscale=0.5]
\draw[xstep=10cm,ystep=1cm,color=lightgray,thin] (10,90) grid (71,96);
\foreach \x in {10, 20, ..., 70} {
  \node (x) at (\x, 89.8) {\tiny \x};
}
\foreach \y in {90, ..., 96} {
  \node (x) at (8, \y) {\tiny \y};
}
\node (bottom) at (40, 89.3) {\small Sentence length};
\node[rotate=90] (left) at (5, 92.5) {\small F1 score};
\draw[color=blue, fill=blue] (72, 93.85) rectangle (73, 94.15);
\node[anchor=west] (l) at (73, 94)   {\tiny BerkeleyParser};
\draw[color=red, fill=red] (72, 93.35) rectangle (73, 93.65);
\node[anchor=west] (l) at (73, 93.5) {\tiny baseline LSTM};
\draw[color=orange, fill=orange] (72, 92.85) rectangle (73, 93.15);
\node[anchor=west] (l) at (73, 93)   {\tiny LSTM+A};
\draw[thick,color=blue] plot coordinates
{ (10, 92.7)
  (11, 92.94)
  (12, 92.95)
  (13, 93.49)
  (14, 93.69)
  (15, 93.73)
  (16, 93.75)
  (17, 93.83)
  (18, 93.69)
  (19, 93.57)
  (20, 93.33)
  (21, 93.17)
  (22, 93.12)
  (23, 93.34)
  (24, 93.2)
  (25, 93.05)
  (26, 92.78)
  (27, 92.74)
  (28, 92.64)
  (29, 92.54)
  (30, 92.37)
  (31, 92.34)
  (32, 92.3)
  (33, 92.29)
  (34, 92.15)
  (35, 92.04)
  (36, 91.99)
  (37, 91.97)
  (38, 91.91)
  (39, 91.89)
  (40, 91.8)
  (41, 91.72)
  (42, 91.69)
  (43, 91.67)
  (44, 91.55)
  (45, 91.55)
  (46, 91.47)
  (47, 91.48)
  (48, 91.45)
  (49, 91.47)
  (50, 91.43)
  (51, 91.37)
  (52, 91.34)
  (53, 91.3)
  (54, 91.3)
  (55, 91.3)
  (56, 91.26)
  (57, 91.14)
  (58, 91.15)
  (59, 91.15)
  (60, 91.15)
  (61, 91.14)
  (62, 91.14)
  (63, 91.14)
  (64, 91.09)
  (65, 91.1)
  (66, 91.1)
  (67, 91.1)
  (68, 91.1)
  (69, 91.1)
  (70, 91.1)
  (71, 91.1)
};
\draw[thick,color=red] plot coordinates
{ (10, 92.36)
  (11, 93.05)
  (12, 93.33)
  (13, 93.47)
  (14, 93.88)
  (15, 94.12)
  (16, 93.77)
  (17, 93.83)
  (18, 93.76)
  (19, 93.84)
  (20, 93.67)
  (21, 93.53)
  (22, 93.63)
  (23, 93.76)
  (24, 93.38)
  (25, 93.3)
  (26, 93.21)
  (27, 92.98)
  (28, 92.81)
  (29, 92.72)
  (30, 92.69)
  (31, 92.54)
  (32, 92.57)
  (33, 92.61)
  (34, 92.57)
  (35, 92.44)
  (36, 92.31)
  (37, 92.33)
  (38, 92.3)
  (39, 92.18)
  (40, 92.09)
  (41, 92.04)
  (42, 92.01)
  (43, 91.99)
  (44, 91.92)
  (45, 91.9)
  (46, 91.75)
  (47, 91.74)
  (48, 91.71)
  (49, 91.7)
  (50, 91.67)
  (51, 91.57)
  (52, 91.53)
  (53, 91.52)
  (54, 91.52)
  (55, 91.52)
  (56, 91.46)
  (57, 91.23)
  (58, 91.14)
  (59, 91.11)
  (60, 91.11)
  (61, 91.06)
  (62, 91.06)
  (63, 90.99)
  (64, 90.99)
  (65, 90.99)
  (66, 90.99)
  (67, 90.99)
  (68, 90.99)
  (69, 90.99)
  (70, 90.99)
  (71, 90.99)
};
\draw[thick,color=orange] plot coordinates
{ (10, 95.41)
  (11, 95.07)
  (12, 95.57)
  (13, 95.16)
  (14, 95.39)
  (15, 95.24)
  (16, 95.44)
  (17, 95.5)
  (18, 95.5)
  (19, 95.39)
  (20, 95.42)
  (21, 95.1)
  (22, 94.85)
  (23, 94.76)
  (24, 94.83)
  (25, 94.58)
  (26, 94.54)
  (27, 94.37)
  (28, 94.18)
  (29, 94.06)
  (30, 94.03)
  (31, 94.03)
  (32, 93.91)
  (33, 93.9)
  (34, 93.99)
  (35, 93.94)
  (36, 93.85)
  (37, 93.86)
  (38, 93.9)
  (39, 93.87)
  (40, 93.87)
  (41, 93.84)
  (42, 93.77)
  (43, 93.77)
  (44, 93.77)
  (45, 93.75)
  (46, 93.72)
  (47, 93.62)
  (48, 93.61)
  (49, 93.59)
  (50, 93.6)
  (51, 93.6)
  (52, 93.54)
  (53, 93.49)
  (54, 93.49)
  (55, 93.49)
  (56, 93.5)
  (57, 93.43)
  (58, 93.42)
  (59, 93.42)
  (60, 93.43)
  (61, 93.43)
  (62, 93.42)
  (63, 93.42)
  (64, 93.42)
  (65, 93.36)
  (66, 93.37)
  (67, 93.37)
  (68, 93.37)
  (69, 93.37)
  (70, 93.37)
  (71, 93.37)
};
\end{tikzpicture}
 \caption{Effect of sentence length on the F1 score on WSJ 22.}
\label{fig:len}
\end{figure}

\paragraph{Beam size influence.}
Our decoder uses a beam of a fixed size to calculate the output sequence of
labels. We experimented with different settings for the beam size. It turns
out that it is almost irrelevant. We report report results that use beam size
10, but using beam size 2 only lowers the F1 score of LSTM+A on the
development set by , and using beam size 1 lowers it by 
(to ). Beam sizes above 10 do not give any additional improvements.

\paragraph{Dropout influence.}
We only used dropout when training on the small WSJ dataset and its
influence was significant. A single LSTM+A model only achieved
an F1 score of  on our development set, that is over  points
lower than the  of a LSTM+A+D model.

\paragraph{Pre-training influence.}
As described in the previous section, we initialized the word-vector
embedding with pre-trained word vectors obtained from \texttt{word2vec}.
To test the influence of this initialization, we trained a LSTM+A model
on the high-confidence corpus, and a LSTM+A+D model on the WSJ corpus,
starting with randomly initialized word-vector embeddings.
The F1 score on our development set was  lower for the LSTM+A model
( vs ) and  lower for the LSTM+A+D model ( vs ).
So the effect of pre-training is consistent but small.

\paragraph{Performance on other datasets.}
The WSJ evaluation set has been in use for 20 years and is commonly used
to compare syntactic parsers. But it is not representative for text
encountered on the web~\citep{petrov-mcdonald:2012:SANCL}. Even though
our model was trained on a news corpus, we wanted to check how well it
generalizes to other forms of text. To this end, we evaluated it on two
additional datasets:
\begin{itemize}
\item[QTB] 1000 held-out sentences from the Question
  Treebank~\cite{judge-etAl:2006:ACL};
\item[WEB] the first half of each domain from the English Web
  Treebank~\cite{petrov-mcdonald:2012:SANCL} (8310 sentences).
\end{itemize}
LSTM+A trained on the high-confidence corpus (which only includes text from
news) achieved an F1 score of  on QTB and  on WEB.
Our score on WEB is higher both than the best score reported
in \cite{petrov-mcdonald:2012:SANCL} () and the best score
we achieved with an in-house reimplementation of BerkeleyParser trained
on human-annotated data (). We managed to achieve a slightly higher
score () with the in-house BerkeleyParser trained on a large corpus.
On QTB, the  score of LSTM+A is also lower than the best score of
our in-house BerkeleyParser (). Still, taking into account that there
were only few questions in the training data, these scores show that
LSTM+A managed to generalize well beyond the news language it was trained on.

\paragraph{Parsing speed.}
Our LSTM+A model, running on a multi-core CPU using batches of 128 sentences
on a generic unoptimized decoder, can parse over 120 sentences from WSJ per second
for sentences of all lengths (using beam-size 1). This is better than the speed
reported for this batch size in Figure 4 of \citep{hall2014sparser} at 100 sentences
per second, even though they run on a GPU and only on sentences of under 40 words.
Note that they achieve  F1 score on this subset of sentences of section 22,
while our model at beam-size 1 achieves a score of  on this subset.
 \vspace{-0mm}
\section{Analysis}
\label{sec:analysis}

As shown in this paper, the attention mechanism was a key component especially when learning from a relatively small dataset. We found that the model did not overfit and learned the parsing function from scratch much faster, which resulted in a model which generalized much better than the plain LSTM without attention.

One of the most interesting aspects of attention is that it allows us to visualize to interpret what the model has learned from the data. For example, in \cite{bahdanau2014neural} it is shown that for translation, attention learns an alignment function, which certainly should help translating from English to French.

Figure~\ref{fig:attention} shows an example of the attention model trained only on the WSJ dataset. From the attention matrix, where each column is the attention vector over the inputs, it is clear that the model focuses quite sharply on one word as it produces the parse tree. It is also clear that the focus moves from the first word to the last monotonically, and steps to the right deterministically when a word is consumed.

On the bottom of Figure~\ref{fig:attention} we see where the model attends (black arrow), and the current output being decoded in the tree (black circle). This stack procedure is learned from data (as all the parameters are randomly initialized), but is not quite a simple stack decoding. Indeed, at the input side, if the model focuses on position , that state has information for all words after  (since we also reverse the inputs). It is worth noting that, in some examples (not shown here), the model does skip words.

\begin{figure}[t]
\centering
\includegraphics[width=1.1\textwidth]{attention_figure}
\caption{Attention matrix. Shown on top is the attention matrix where each column is the attention vector over the inputs. On the bottom, we show outputs for four consecutive time steps, where the attention mask moves to the right. As can be seen, every time a terminal node is consumed, the attention pointer moves to the right.}
\label{fig:attention}
\end{figure} \vspace{-0mm}
\section{Related Work}
\label{sec:rel}

The task of syntactic constituency parsing  
has received a tremendous amount of attention in the last 20 years.
Traditional approaches to constituency parsing rely on probabilistic
context-free grammars (CFGs).
The focus in these approaches is on devising appropriate smoothing
techniques for highly lexicalized and thus rare 
events \citep{collins:1997:ACL} or carefully crafting the
model structure \citep{klein-manning:2003:ACL}.
\cite{petrov-EtAl:2006:ACL} partially alleviate the heavy reliance on
manual modeling of linguistic structure by using latent variables to learn a
more articulated model.
However, their model still depends on a CFG backbone and is thereby potentially
restricted in its capacity. 

Early neural network approaches to parsing, for example by
\cite{henderson:2003:NAACL,henderson:2004:ACL} also relied on strong linguistic insights.
\cite{titov-henderson:2007:ACL} introduced Incremental Sigmoid Belief Networks for syntactic parsing.
By constructing the model structure incrementally, they are able to avoid 
making strong independence assumptions but inference becomes intractable.
To avoid complex inference methods, 
\cite{collobert2011deep} propose a recurrent neural network
where parse trees are decomposed into a stack of independent levels.
Unfortunately, this decomposition breaks for long sentences and
their accuracy on longer sentences falls quite significantly behind
the state-of-the-art.
\cite{socher2011parsing} used a tree-structured neural network to score candidate parse trees.
Their model however relies again on the CFG assumption and
furthermore can only be used to score candidate trees rather than for full inference.

Our LSTM model significantly differs from all these models, as it makes no assumptions about
the task. As a sequence-to-sequence prediction model it is somewhat
related to the incremental parsing models, pioneered by \cite{ratnaparkhi}
and extended by \cite{collins-roark:2004:ACL}.
Such linear time parsers however typically need some task-specific constraints
and might build up the parse in multiple passes. Relatedly, \citet{zhu-EtAl:2013:ACL}
present excellent parsing results with a single left-to-right pass, but require a stack
to explicitly delay making decisions and a parsing-specific transition strategy in order
to achieve good parsing accuracies.
The LSTM in contrast uses its short term memory to model the complex underlying structure
that connects the input-output pairs.

Recently, researchers have developed a number of neural network models that
can be applied to general sequence-to-sequence problems.
\cite{graves2013generating} was the first to propose a differentiable attention
mechanism for the general problem of handwritten text synthesis, although his
approach assumed a monotonic alignment between the input and output sequences.
Later, \cite{bahdanau2014neural}
introduced a more general attention model that does not assume a monotonic
alignment, and applied it to machine translation, and
\cite{chorowski2014end} applied the same model to speech
recognition.  \cite{kalchbrenner2013recurrent} used a convolutional
neural network to encode a variable-sized input sentence into
a vector of a fixed dimension and used a RNN to produce the output sentence.
Essentially the same model has been used by \cite{vinyals2014show} to
successfully learn to generate image captions.
Finally, already in 1990 \cite{zoubin} experimented with
applying recurrent neural networks to the problem of syntactic parsing.
 \vspace{-0mm}
\section{Conclusions}
\label{sec:concl}

In this work, we have shown that generic sequence-to-sequence
approaches can achieve excellent results on syntactic constituency
parsing with relatively little effort or tuning.  In addition, while
we found the model of Sutskever et al.~\cite{sutskever14} to not be
particularly data efficient, the attention model of Bahdanau et
al.~\cite{bahdanau2014neural} was found to be highly data efficient,
as it has matched the performance of the BerkeleyParser when trained
on a small human-annotated parsing dataset.  Finally, we showed that
synthetic datasets with imperfect labels can be highly useful, as our
models have substantially outperformed the models that have been used
to create their training data.  We suspect it is the case due to the
different natures of the teacher model and the student model: the
student model has likely viewed the teacher's errors as noise which it
has been able to ignore.  This approach was so successful that we
obtained a new state-of-the-art result in syntactic constituency
parsing with a single attention model, which also means that the model
is exceedingly fast.  This work shows that domain independent models
with excellent learning algorithms can match and even outperform
domain specific models. 
\vspace{-0mm}
{\bf Acknowledgement.} We would like to thank Amin Ahmad, Dan Bikel and Jonni Kanerva.

\vspace{-3mm}
\bibliographystyle{unsrt}
\bibliography{nips2015}

\end{document}
