
\documentclass[12pt,a4paper]{article}
\pdfoutput=1
\usepackage{amsmath,amssymb}
\usepackage[pdftex]{graphicx}
\usepackage{mathptmx}


\newcommand{\ignore}[1]{}


\newcommand{\keywordsNarrow}[1]{\centerline{\parbox{5.5in}
                {{\bf Keywords.~}{\em #1\\}}}}
\newcommand{\keywords}[1]{{{\bf Keywords.~}{\em #1}}}


\newenvironment{program}
{
\begin{list}
{} {  \setlength{\leftmargin}{20pt}
\setlength{\rightmargin}{0pt} \setlength{\parsep}{0pt}
\setlength{\itemsep}{0pt}
\setlength{\topsep}{0pt}
\setlength{\parskip}{0pt}
\setlength{\baselineskip}{14pt}
\setlength{\labelsep}{3pt}
}
}
{\end{list}}
\def\bp{\begin{program}}
\def\ep{\end{program}}
\def\set{{\bf set~}}
\def\for{{\bf for~}}
\def\doo{{\bf ~do}}
\def\enddo{{\bf enddo}}
\def\balg{{\bf begin algorithm}}
\def\endalg{{\bf end algorithm}}
\def\note{{\bf note:~}}

\renewenvironment{itemize}
{
\begin{list}
{}
{
\setlength{\leftmargin}{25pt}
\setlength{\rightmargin}{0pt}
\setlength{\parsep}{0pt}
\setlength{\itemsep}{0pt}
\setlength{\topsep}{\parskip}
\setlength{\parskip}{0pt}
\setlength{\baselineskip}{14pt}
\setlength{\labelsep}{3pt}
}
}
{\end{list}}

\newcounter{i}
\newenvironment{enumerat}
{
\setcounter{i}{0}
\begin{list}
{\arabic{i}.}
{
\usecounter{i}
\setlength{\leftmargin}{25pt}
\setlength{\parsep}{0pt}
\setlength{\itemsep}{0pt}
\setlength{\topsep}{\parskip}
\setlength{\parskip}{0pt}
\setlength{\baselineskip}{14pt}
\setlength{\labelsep}{5pt}
\setlength{\itemindent}{0pt}
}
}
{\end{list}}
  
\textwidth=16cm
\hoffset=-1.25cm
\voffset=-2cm
\parindent=0cm
\setlength{\textheight}{24.5cm}
\parskip=9pt

\renewcommand{\theequation}{\arabic{section}.\arabic{equation}}
\renewcommand{\thetable}{\arabic{section}.\arabic{table}}
\renewcommand{\thefigure}{\arabic{section}.\arabic{figure}}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{algorithm}{Algorithm}[section]

\def\proof{\noindent{\bf Proof:~}}
\def\qed{\hfill\rule{1ex}{2ex}}
\def\real{\mathbb R}
\def\diag{{\rm diag}}
\def\deq{\buildrel\Delta\over=}
\def\rar{\!\!\rightarrow\!\!}
\def\rul{\rule{0mm}{2ex}}
\def\CONV{\small CONV}
\def\beq{}
\def\bi{\begin{itemize}}
\def\ei{\end{itemize}}
\def\be{\begin{enumerat}}
\def\ee{\end{enumerat}}
\def\time{\!\times\!}
\def\ldot{\put(2.5,2.5){\circle*{2.5}}\hspace{5pt}}
\def\ldott{\put(3.5,2.5){\circle*{2.5}}\hspace{5pt}}
\def\qed{\hfill\rule{1ex}{2ex}}
\def\real{\mathbb R}
\def\diag{{\rm diag}}
\def\deq{\buildrel\Delta\over=}
\def\rul{\rule{0mm}{2ex}}
\def\CONV{\small CONV}
\def\beq{}
\def\mns{\!-\!}
\def\half{\scriptstyle \frac{1}{2}}
\def\noconv{\multicolumn{2}{|c|}{no conv.}}



\begin{document}

\baselineskip 14pt

\title{\vspace{-1.5cm}
\bf\Large
Geometric scaling: a simple preconditioner for certain\\
linear systems with discontinuous coefficients}

\author{Dan Gordon\thanks{Corresponding author. 
	Dept.\ of Computer Science,
        University of Haifa, Haifa 31905, Israel.
        \newline\hspace*{18pt}{\tt gordon@cs.haifa.ac.il}} 
	\and
        Rachel Gordon\thanks{Dept.\ of Aerospace Engineering,
        The Technion--Israel Inst.\ of Technology, Haifa 32000,
        Israel.\newline\hspace*{18pt}{\tt rgordon@tx.technion.ac.il}}}

\date{April 17, 2009}

\maketitle
\vspace{-1cm}
\begin{abstract}\noindent
Linear systems with large differences between coefficients 
(``discontinuous coefficients'') arise in many cases in which 
partial differential equations (PDEs) model physical phenomena 
involving heterogeneous media.  The standard approach to solving 
such problems is to use domain decomposition (DD) techniques, 
with domain boundaries conforming to the boundaries between the 
different media.  This approach can be difficult to implement 
when the geometry of the domain boundaries is complicated or 
the grid is unstructured.  This work examines the simple 
preconditioning technique of scaling the equations by dividing 
each equation by the -norm of its coefficients.  This 
preconditioning is called geometric scaling (GS).  
It has long been known that diagonal scaling can be useful in 
improving convergence, but there is no study on the general 
usefulness of this approach for discontinuous coefficients.  
GS was tested on several nonsymmetric linear systems with 
discontinuous coefficients derived from convection-diffusion 
elliptic PDEs with small to moderate convection terms.
It is shown that GS improved the convergence properties of 
restarted GMRES and Bi-CGSTAB, with and without the ILUT 
preconditioner.
GS was also shown to improve the distribution of the 
eigenvalues by reducing their concentration around the 
origin very significantly.
\end{abstract}
\keywords{Bi-CGSTAB; diagonal scaling; discontinuous coefficients; 
domain decomposition; geometric scaling; GMRES; GS; Lp-norm; linear 
equations; nonsymmetric equations; parallel processing; partial 
differential equations.}



\section{Introduction}

Many physical phenomena are modeled by partial differential equations 
(PDEs) which describe the relations between one or more scalar or vector 
fields and the surrounding media.  When boundary conditions are prescribed, 
a common approach to achieving a numerical solution is to impose a grid 
and discretize the equations, thus getting a system of linear equations.  
In some cases, this approach yields a system of equations with very large 
differences between coefficients of the equations.  Examples of such 
systems arise in modeling flow through heterogeneous media with 
widely-varying permeability, oil reservoir simulation, electromagnetics 
and semiconductor modeling.  Such systems are commonly referred to as 
systems with ``discontinuous coefficients''.  

One of the most common methods for tackling such problems is 
the domain-decomposition (DD) approach, in which the domain 
is partitioned into subdomains, with the subdomain boundaries 
conforming to the boundaries between the different media.  
DD techniques typically operate as follows: some boundary 
conditions are assumed to exist on the interfaces between 
subdomains, and a solution of the equations in each subdomain 
is obtained, often with only low accuracy.  The boundary 
conditions at the interfaces are then updated according to 
these solutions, and the process is repeated until convergence 
is achieved.  There exists a vast literature on this subject, 
and a review of the area is beyond the scope of this work; 
see, for example, \cite{Glowinski88,Smith96,Quarteroni99,Rice00}.  
DD techniques may be difficult to implement on unstructured 
grids or when the boundaries between domains have a 
complicated geometry.

We consider nonsymmetric linear systems with discontinuous 
coefficients derived from convection-diffusion elliptic PDEs 
with small to moderate convection terms.  It is shown that a 
simple preconditioning technique, which we call {\em geometric 
scaling} (GS), can be applied to the system of equations in 
order to improve the convergence properties of algorithms
applied to the system.  GS(), with an integer parameter 
, consists of dividing each equation by the -norm 
of its vector of coefficients.  GS, which is a particular form 
of diagonal scaling on the left, is geometric in the following
sense: after applying GS() to a linear system, all the rows
of the system matrix have a unity -norm.

In order to examine the general usefulness of GS in conjunction with
various solution methods, we tested it with the two leading Krylov 
subspace solvers for nonsymmetric systems: GMRES \cite{Saad86} and 
Bi-CGSTAB \cite{Vorst92}.  Both algorithms were tested with and 
without the ILUT preconditioner \cite{Saad94} on several nonsymmetric 
problems taken from (or based on) Saad \cite{Saad03}, van der Vorst
\cite{Vorst92}, and Graham and Hagger \cite{Graham99}.  Four basic 
problems were considered, as well as several variations of the 
problems, such as modifications of the differences between the 
coefficients and various grid sizes.  Results are provided for 
three different levels of accuracy.  

Our experiments used the -norm, but the -norm yielded
similar results.  Our results can be characterized as follows:
\bi
\item In most cases, when the tested method (algorithm/preconditioner
combination) converges to the specified accuracy criterion, GS speeds
up the convergence time.
\item In many cases, when the tested method stagnates or diverges
on the original system, it converges on the scaled system.
\item GS was not particularly useful on problems without discontinuous
coefficients.
\item GS was not helpful on problems derived from PDEs with large
convection terms.  Such PDEs produce linear systems with very large
off-diagonal elements, and these require other solution methods.
\ei

We also provide data on the effect of GS on the distribution of the 
eigenvalues.  It is generally considered to be detrimental to the
convergence of Krylov subspace methods to have a large accumulation
of eigenvalues near the origin.
In the cases examined here, there is a large accumulation of 
eigenvalues near the origin, and GS appears to ``push'' many 
eigenvalues away from the origin.  Also, the eigenvalues of 
the scaled system appear to be distributed similarly to those 
obtained without scaling but with equal-sized coefficients.  

Note that the above choice of algorithm/preconditioner combinations 
does not imply that these methods are optimal for the above problems. 
Finding an optimal method is problem-dependent and a topic for
further research.

Diagonal scaling is not new, and its usefulness for discontinuous
coefficients has also been noted.  Van der Sluis \cite{Sluis69} 
deals with the effect of scaling on the condition number of matrices.  
Widlund \cite[p.\ 34-35]{Widlund71} writes that well-scaled ADI 
methods give good rates of convergence when the coefficients of 
elliptic problems vary very much in magnitude (ADI stands for the 
alternating direction implicit method \cite{Peaceman55}).  Graham 
and Hagger \cite[p.\ 2042-2043]{Graham99} write that diagonal 
scaling has been observed in practical computations to be very 
effective as a preconditioner for problems with discontinuous 
coefficients.  Duff and van der Vorst \cite{Duff98} write that 
on vector machines, diagonal scaling is often competitive with 
other approaches.  Regarding diagonal scaling of symmetric matrices, 
see Meurant \cite[Th. 8.1, 8.2]{Meurant99}, who also has some 
reservations about the usefulness of diagonal scaling for parallel 
processing \cite[p.\ 401]{Meurant99}.  Gambolati et al.\ 
\cite{Gambolati03} use the least square logarithm (LSL) scaling 
on the rows and the columns of the system matrix for a certain 
problem in geomechanics with discontinuous coefficients.

However, most previous works are concerned with symmetric systems
and diagonal scaling refers to multiplying the system matrix on 
the left and on the right by the same diagonal matrix, in order 
to preserve symmetry.  Also, there is no study on the general 
usefulness of geometric scaling for nonsymmetric problems with 
discontinuous coefficients.  The idea of geometric scaling was 
found to be useful for certain problems in image reconstruction 
from projections; see \cite{Gordon07}.

DD methods are often mentioned in connection with parallel 
processing.  The equations of different domains can, in 
principle, be solved in parallel by different processors.  
However, the different domains that arise in many practical 
situations do not necessarily lead to an optimal load-balancing 
assignment of equations to processors.  With the GS approach, 
inherently parallel algorithms such as Bi-CGSTAB and GMRES can 
be implemented efficiently in parallel without regard to 
subdomain boundaries.  Needless to say, GS is inherently 
parallel.

The rest of this paper is organized as follows.  \S \ref{back} 
presents some essential background.  \S \ref{prob1}--\S \ref{prob4} 
deal with the four different problems, and \S \ref{conclusions} 
concludes with a summary and some future research directions.



\section{General background}
\label{back}

Throughout the rest of the paper, we assume that all vectors are column
vectors, and we use the following notation:   
denotes the dot product of two vectors  and , which is also 
.  Given a vector , we denote 
its -norm by .  
For , we will omit the index and just write 
.
If  is an  matrix, we denote by  the th 
row-vector of ; i.e., .  

Consider a system of  linear equations in  variables:


We shall assume throughout that (\ref{linear}) is consistent and that 
 does not contain a row of zeros.  For , we define a diagonal 
matrix .  The 
geometrically-scaled system is defined as 


In some algorithms, GS() is an inherent step in the following 
sense: either the scaling is executed at the beginning as an 
intrinsic part of the algorithm, or, executing the algorithm 
produces identical results to those obtained when GS() is 
done at the beginning.  As an example, it is easy to see that 
GS(2) is inherent in the Kaczmarz algorithm (KACZ) 
\cite{Kaczmarz37}.  KACZ can be described geometrically as 
follows: starting from an arbitrary point  
as the initial iterate, KACZ projects the current iterate 
orthogonally towards a hyperplane defined by one of the 
equations.  The hyperplanes are chosen in cyclic order.  

The tests were run using the AZTEC software library \cite{Tuminaro99}, 
which includes a wide range of algorithms and preconditioners, suitable 
for sequential and parallel implementations.  Geometric scaling with 
the -norm is a built-in option in AZTEC (where it is called 
``row-sum scaling'').  As mentioned we used Bi-CGSTAB and GMRES, 
with and without ILUT.  Restarted GMRES was used with Krylov 
subspace size of 10.  In AZTEC, GMRES is implemented with a double 
classical Gram-Schmidt orthogonalization step.  ILUT was implemented 
with AZTEC's default parameters of drop tolerance = 0 and fill-in = 1.  
These parameters are not necessarily optimal for the tested examples, 
but since our purpose was to demonstrate the general usefulness of GS, 
we stuck with a commonly-used restart value for GMRES and AZTEC's 
default values for ILUT.  No doubt, better convergence properties and 
runtimes would be obtained if these parameters were fine-tuned to each 
problem.  The eigenvalue computations were done with the LAPACK linear 
algebra package \cite{lapack}.

We also experimented with the option of multiplying the system 
matrix  by  on the left and on the right, resulting 
in the system , with 
.  In one problem, the results were somewhat
poorer than those obtained with GS, but otherwise, the results
were similar.  Note that this option is computationally more
expensive.
Gambolati et al.\ \cite{Gambolati03} remark that when using ILU(0),
the iteration matrix of Bi-CGSTAB on a diagonally scaled system
(on the left and/or the right) is theoretically the same as the
one with ILU(0) on the original matrix.  In our experiments, the
effect of ILUT was identical to that of ILU(0), so in theory, 
Bi-CGSTAB with ILUT on the original and the scaled systems should 
have behaved similarly.  However, our experiments indicated that 
using GS produces much better numerical results, and this also 
holds for GMRES with ILUT.  This is probably due to the fact that 
the scaled system does not have very large differences between 
coefficients, so it is much less prone to roundoff errors.




\subsection{Setup of the numerical experiments}
\label{setup}

In two dimensions, the general form of the second-order differential 
equations in this study was

where  and  are given functions of  and , ``'' 
stands for lower-order derivatives, and  is a prescribed RHS.  
In three dimensions, there are three given functions  of 
, and a second order partial derivative w.r.t.\  was 
also included.  Boundary conditions were either Dirichlet or mixed 
Dirichlet and Neumann.  The regions were taken as the unit square 
or the unit cube.  The discretization of the second-order 
derivatives at a given grid point  was done using central 
differences; e.g., 
 was approximated as
5pt]
& = & \left(a_{i+\half,j}(u_{i+1,j}-u_{i,j}) / \Delta x -
            a_{i-\half,j}(u_{i,j}-u_{i-1,j}) / \Delta x \right) / \Delta x \
All problems were discretized with equally-spaced grids, and the initial
value was taken as .
The tests were run on a Pentium IV 2.8GHz processor with 3GB memory,
running Linux. 



\subsection{Stopping tests}

There are several stopping criteria which one may apply to iterative
systems.  Our stopping criterion was to use the relative residual:
, where  was taken as
,  and .  In some of the cases, this was
not attainable.  Since this stopping criterion depends on the scaling
of the equations, we always made this test on the geometrically-scaled
system using the -norm.
In order to limit the time taken by the methods implemented in AZTEC, 
the maximum number of iterations was set to 10,000.  The AZTEC library 
has several other built-in stopping criteria:  numerical breakdown, 
numerical loss of precision and numerical ill-conditioning.
In the results, we denote the relative residual by rel-res, and 
non-convergence by ``no conv.''

One should note that the test for numerical breakdown in AZTEC uses 
the machine precision {\tt DBL\_EPSILON}, and this may result in a
premature notice of numerical breakdown in some cases.  To get around 
this problem, we multiplied the variable {\tt brkdown\_tol} in the 
Bi-CGSTAB algorithm by some small factor, such as .
({\tt brkdown\_tol} is normally set equal to {\tt DBL\_EPSILON}, 
which is approximately  on our machine).  




\section{Problem 1}
\label{prob1}

Problem 1 is taken from Saad \cite[\S 3.7, problem F2DB]{Saad03}.  
It is a two-dimensional PDE

where 
5pt]
	1 & \mbox{otherwise}
       \end{array} \right.
-12pt]
&\multicolumn{6}{|c|}{\bf No.\ of iterations and time (in sec.)}\\
\hline &&&&&&\-12pt]
Bi-CGSTAB       & \noconv       & \noconv       & \noconv       \\
with GS         & 91 & (0.30)   & 299 & (0.99)  & 361 & (1.19)  \\
\hline &&&&&&\-12pt]
GMRES & \multicolumn{6}{|c|}{converged to } \\
with GS & 265 & (0.85) &
\multicolumn{4}{|c|}{converged to } \\
\hline 
&\multicolumn{6}{|c|}{}\-12pt]
&\multicolumn{6}{|c|}{\bf No.\ of iterations and time (in sec.)}\\
\hline&&&&&&\-12pt]
Bi-CGSTAB        & 121 & (0.41)	& 224 & (0.76)	& 286 & (0.96)  \\
with GS         & 123 & (0.42) & 217 & (0.74) & 261 & (0.88)  \\
\hline&&&&&&\-12pt]
GMRES           & 1365 & (4.31) & 3704 & (11.69) & 6045 & (19.19) \\
with GS         & 1356 & (4.28) & 3582 & (11.45) & 5722 & (18.16) \\
\hline&&&&&&\-12pt]
{\bf Matrix} &  &  &
 &
\parbox[c]{1.25in}{No.\ of eigenvalues in first interval\vspace{2pt}} \\
\hline&&&&\-12pt]
With GS & 7.07E-6 & 1.78E+0 & 2.52E+5 & 4 \\
\hline&&&&\
-~ \frac{\partial(D(x,y)u_x)}{\partial x} ~-~
   \frac{\partial(D(x,y)u_y)}{\partial y} ~+~
   a u_x ~+~ b u_y ~=~ 1,
\label{eq2}
-12pt]
&\multicolumn{6}{|c|}{\bf No.\ of iterations and time (in sec.)}\\
\hline&&&&&&\-12pt]
Bi-CGSTAB       & \noconv      & \noconv       & \noconv       \\
with GS         & 591 & (2.13) & 1025 & (3.69) & 
\multicolumn{2}{|c|}{conv.\ to } \\
\hline&&&&&&\-12pt]
GMRES & \multicolumn{6}{|c|}{converged to 6.15}  \\
with GS & \multicolumn{6}{|c|}{converged to } \\
\hline
& \multicolumn{6}{|c|}{}\-12pt]
&\multicolumn{6}{|c|}{\bf No.\ of iterations and time (in sec.)}\\
\hline&&&&&&\-12pt]
Bi-CGSTAB       & \noconv      & \noconv      & \noconv      \\
with GS         & 589 & (2.01) & 707 & (2.41) & 1493 & (5.37) \\
\hline&&&&&&\-12pt]
GMRES & \multicolumn{6}{|c|}{converged to 1.84} \\
with GS & 
\multicolumn{6}{|c|}{converged to } \\
\hline
&\multicolumn{6}{|c|}{}\-12pt]
&\multicolumn{6}{|c|}{\bf No.\ of iterations and time (in sec.)}\\
\hline&&&&&&\-12pt]
Bi-CGSTAB       & \noconv      & \noconv      & \noconv      \\
with GS         & 224 & (0.82) & 730 & (2.65) & 
\multicolumn{2}{|c|}{conv.\ to } \\
\hline&&&&&&\-12pt]
GMRES & \multicolumn{6}{|c|}{converged to 1.37} \\
with GS & 357 & (1.38) & 
\multicolumn{4}{|c|}{converged to } \\
\hline
&\multicolumn{6}{|c|}{}\-11pt]
{\bf Matrix} &  &  &
 &
\parbox[c]{.8in}{Eigenvalues around \vspace{3.5pt}} \\
\hline&&&&\-12pt]
With GS      & 9.15E-5 & 1.78E0 & 1.95E+4 & 8 \\
\hline&&&&\
 -~ \frac{\partial}{\partial x} \left(A(x,y)u_x\right) 
~-~ \frac{\partial}{\partial y} \left(A(x,y)u_y\right) 
~+~ B(x,y)u_x ~=~ F,
-12pt]
&\multicolumn{6}{|c|}{\bf No.\ of iterations and time (in sec.)}\\
\hline&&&&&&\-12pt]
Bi-CGSTAB+ILUT   & 93  & (0.60)  & 124 & (0.79)  & 152 & (0.95)  \\
with GS         & 67  & (0.44)  & 90  & (0.59)  & 112 & (0.72)  \\
\hline&&&&&&\-12pt]
{\bf Matrix} &  &  &
 &
\parbox[c]{1.25in}{No.\ of eigenvalues in first interval\vspace{2pt}} \\
\hline&&&&\-12pt]
With GS      & 1.21E-4 & 1.78E+0 & 1.47E+4  & 167 \\
\hline
\end{tabular}
\caption{Basic eigenvalue information for Problem 3.}
\label{tbl3a}
\end{table}

Figure \ref{eigen3} shows the distribution of the eigenvalues 
for the original (with a zoom) and the scaled matrices of 
Problem 3.

\begin{figure}[!h]
\begin{tabular}{@{}c@{}c@{}}
\hspace{.1in}
\includegraphics[width=3in]{eigen3}
&
\includegraphics[width=3in]{eigen3_zoom} \\
\multicolumn{2}{c}{\includegraphics[width=3in]{eigen3_L2}} \\
\end{tabular}
\caption{Eigenvalue distribution for Problem 3, with a zoom to 
the region 0--800, and the distribution for the scaled matrix.} 
\label{eigen3}
\end{figure}

Figure \ref{dist3} is a histogram of the eigenvalues of the 
original and the scaled matrices.  We can see from Figures 
\ref{eigen3} and \ref{dist3} that in the original matrix, 
the eigenvalues are very concentrated around the origin, 
while in the scaled matrix, many are ``pushed'' away from 
the origin.

\begin{figure}[!h]
\centering
\includegraphics[width=4.5in]{distrib3}
\vspace{-.1in}
\caption{Histogram of the eigenvalues for Problem 3, for the original
and the scaled cases.}
\label{dist3}
\end{figure}





\section{Problem 4}
\label{prob4}

This problem is based on a three-dimensional problem from Graham 
and Hagger \cite{Graham99}, to which we added convection terms.  
The differential equation is the following:

where the domain is the unit cube, and  is defined as 
2pt]
	 1 & \mbox{otherwise.} 
		 \end{array} \right.
-11pt]
& \multicolumn{2}{|c|}{rel-res }
& \multicolumn{2}{|c|}{rel-res }
& \multicolumn{2}{|c|}{rel-res } \\
\hline&&&&&&\-12pt]
Bi-CGSTAB &---&---&---&---&---&--- \\
with GS        & 112 & 111 & 262 & 160 & 406 & 374 \\
\hline&&&&&&\-12pt]
GMRES &---&---&---&---&---&--- \\
with GS & 208 & 207 &---& 286 &---&--- \\
\hline
&&&&&&\-12pt]
\multicolumn{7}{|l|}{Note: ``---'' denotes no convergence 
to the prescribed accuracy.} \\
\hline
\end{tabular}
\caption{No.\ of iterations for Problem 4, with , ,
and grid .}
\label{tbl4}
\end{table}

Figures \ref{run4} and \ref{run4a} show bar-plots of the runtimes 
for the two grid sizes, for  and , for the three 
levels of accuracy.  In cases of stagnation, the figures show (in 
parentheses) the relative residual achieved before stagnation.

\begin{figure}[!h]
\hspace{5pt}
\begin{tabular}{@{}c@{}c@{}}
Results for  & Results for 
\\ \hline
&\-.5in]
\includegraphics[width=3.15in]{ex18_40_J04_E7_conv_100}
&
\includegraphics[width=3.15in]{ex18_40_J06_E7_conv_100}
\-.5in]
\includegraphics[width=3.15in]{ex18_80_J04_E7_conv_100}
&
\includegraphics[width=3.15in]{ex18_80_J06_E7_conv_100}
\-12pt]
{\bf Matrix} &  &  &
 &
\parbox[c]{1.25in}{No.\ of eigenvalues around \vspace{2pt}} \\
\hline&&&&\-12pt]
With GS      & 2.41E-6 & 1.00E+0 & 4.16E+5 & 25 \\
\hline&&&&\-12pt]
{\bf Method / Convection:} & {\bf 100} & {\bf 200} &
{\bf 500} & {\bf 1000} \\
\hline
&&&&\-12pt]
Bi-CGSTAB+ILUT &  &  & --- & --- \\
with GS        &  &  & --- & --- \\
\hline
&&&&\-12pt]
GMRES+ILUT & --- & --- & --- & --- \\
with GS    &  & --- & --- & --- \\

\hline
\multicolumn{5}{|l|}{}\-12pt]
{\bf Method} & {\bf Problem 1} & {\bf Problem 2} & 
{\bf Problem 3} & {\bf Problem 4} \\ 
\hline 
&&&&\-12pt]
Bi-CGSTAB+ILUT &  &  &  &  \\
with GS        &  &  &  &  \\
\hline
&&&&\-12pt]
GMRES+ILUT &  & 0.90 &  & 0.29 \\
with GS    &  &  &  & 
 \\
\hline
\multicolumn{5}{|l|}{}\-12pt]
\multicolumn{5}{|l|}{Note: `' means no convergence, `' 
means convergence, `' means} \\
\multicolumn{5}{|l|}{better convergence.  The numbers indicate the 
best relative error obtained.} \\
\hline
\end{tabular}
\caption{Summary of convergence of the different methods on the four 
problems, for the three convergence goals 
(, and ).}
\label{tbl_sum}
\end{table}

This study is only a first report on this subject, and much work 
remains to be done.  On the theoretical side, we have seen that 
GS improves the eigenvalue distribution, but further analysis is
still needed.  On the practical side, GS needs to be tested in 
conjunction with other algorithms and preconditioners, on various 
other difficult problems.

A natural question that arises is how does GS compare with DD
techniques?  Over the years, these methods have achieved a high 
level of efficiency, and a head-on runtime comparison is very 
much problem-dependent and a topic for further research.
However, an even more interesting topic is the combination of
the two methods: apply GS to the equations and then apply some 
standard DD method; hopefully, on some problems, GS could also 
improve the convergence properties of some DD approaches.

GS can also help the parallelization of solution methods in 
several ways.  Firstly, GS itself is a parallel computational 
step.  Secondly, it enables the partitioning of a domain into 
subdomains along boundaries that do not necessarily follow the 
physical boundaries of the problem; this way, better load 
balancing can be achieved.  Thirdly, as seen in some of the 
cases, GS enabled the convergence of algorithms that are 
inherently parallel without the need for a preconditioner 
(such as ILUT) that may not be ideal for parallelism.

Another topic for further research is the application of the
(sequential) CGMN algorithm \cite{Bjorck79,Gordon08} and its
block-parallel version CARP-CG \cite{Gordon08a} on problems 
that have discontinuous coefficients and are also strongly 
convection-dominated.  These algorithms have already been 
shown to be very effective on convection-dominated problems 
and initial experiments with discontinuous coefficients have 
yielded good results.  The reason for this may be that these 
two methods are essentially conjugate-gradient acceleration
of the Kaczmarz algorithm, and as such, GS (with the 
-norm) is already inherent in them.  



\bibliography{}
\end{document}
