\section{Experiments}
\label{experiment}
\begin{table*}[t]
	\centering
	\scalebox{0.9}{	
		\begin{tabular}{lccccccc}
\toprule
			Method    &\# Frames & Calendar (Y) & City (Y) & Foliage (Y)  & Walk (Y) &Average (Y)  & Average (RGB)  
			\\
			\midrule    	
			Bicubic & 1 &18.83/0.4936  &23.84/0.5234	&21.52/0.4438  &23.01/0.7096  						 	    & 21.80/0.5426   & 20.37/0.5106	
			\\	
			SPMC ~\cite{tao2017detail}  & 3 & -  &- &-  &- & 25.52/0.76~~~~	&-
			\\
			Liu~\cite{liu2017robust} & 5  &21.61/~~~~~-~~~~~ &26.29/~~~~~-~~~~~ &24.99/~~~~~-~~~~~ &28.06/~~~~~-~~~~~ &25.23/~~~~~-~~~~~  & -
			\\
			TOFlow~\cite{xue2019video} & 7 & 22.29/0.7273  & 26.79/0.7446 & 25.31/0.7118 & 29.02/0.8799 & 25.85/0.7659 &24.39/0.7438	
			\\
			FRVSR~\cite{sajjadi2018frame} &recurrent  & - &- &-  &- & 26.69/0.822~~ &-	
			\\
			DUF-52L~\cite{jo2018deep} & 7 &24.17/0.8161 &28.05/0.8235 &26.42/0.7758 & 30.91/ \textbf{\color{blue}0.9165}	&27.38/0.8329 &\textbf{\color{blue} 25.91}/\textbf{\color{blue}0.8166}
			\\
			RBPN~\cite{haris2019recurrent} & 7 &24.02/0.8088 &27.83/0.8045 &26.21/0.7579 &30.62/0.9111 &27.17/0.8205  &25.65/0.7997 
			\\
			EDVR-L~\cite{wang2019edvr} & 7 & 24.05/0.8147 &28.00/0.8122	 &26.34/0.7635  &\textbf{\color{red}31.02}/0.9152  & 27.35/0.8264  &25.83/0.8077   	
			\\		
			PFNL~\cite{yi2019progressive} &7 &\textbf{\color{blue} 24.37}/\textbf{\color{blue}0.8246} &\textbf{\color{blue} 28.09}/\textbf{\color{blue}0.8385} &\textbf{\color{blue}26.51}/\textbf{\color{blue}0.7768} &30.65/0.9135 &\textbf{\color{blue}27.40}/\textbf{\color{blue}0.8384} &-
			\\
			TGA (Ours)  & 7 &\textbf{\color{red}24.47}/\textbf{\color{red}0.8286} &\textbf{\color{red}28.37}/\textbf{\color{red}0.8419} & \textbf{\color{red}26.59}/\textbf{\color{red}0.7793} &\textbf{\color{blue}30.96}/\textbf{\color{red}0.9181} &\textbf{\color{red}27.59}/\textbf{\color{red}0.8419} &\textbf{\color{red}26.10}/\textbf{\color{red}0.8254}
			\\
			\bottomrule
		\end{tabular}
	}
\vspace{3mm}
	\caption{Quantitative comparison (PSNR(dB) and SSIM) on \textbf{Vid4} for  video super-resolution. {\color{red}Red} text indicates the best and {\color{blue} blue} text indicates the second best performance. Y and RGB indicate the luminance and RGB channels, respectively. `' means the values are taken from original publications or calculated by provided models. Best view in color.}
	\label{vid4_table}
\end{table*}

\begin{table*}[t]
	\centering
	\scalebox{0.92}{	
		\begin{tabular}{lcccccc}
\toprule
			&Bicubic   &TOFlow~\cite{xue2019video}  &DUF-52L ~\cite{jo2018deep}   &RBPN~\cite{haris2019recurrent}  &EDVR-L~\cite{wang2019edvr}  &TGA(Ours)
			\\
			\midrule 
			\# Param.  &N/A &1.4M  &5.8M  &12.1M &20.6M &5.8M
			\\
			FLOPs    &N/A  &0.27T &0.20T & 3.08T  &0.30T  &0.07T
			\\   	
			Y Channel  &31.30/0.8687 &34.62/0.9212    &36.87/0.9447 &37.20/0.9458   &\textbf{\color{red} 37.61}/\textbf{\color{blue}0.9489} &\textbf{\color{blue}37.59}/\textbf{\color{red}0.9516}
			\\
			RGB Channels &29.77/0.8490 &32.78/0.9040    &34.96/0.9313 &35.39/0.9340	&\textbf{\color{red} 35.79}/\textbf{\color{blue} 0.9374}  &\textbf{\color{blue}35.57}/\textbf{\color{red} 0.9387}
			\\	
			\bottomrule
		\end{tabular}
	}
\vspace{3mm}
	\caption{Quantitative comparison (PSNR(dB) and SSIM) on \textbf{Vimeo-90K-T} for  video super-resolution. {\color{red}Red} text indicaktes the best result and {\color{blue} blue} text indicates the second best. FLOPs are calculated on an LR image of size 11264. `' means the values are taken from original publications. Note that the deformation convolution and offline pre-alignment are not included in calculating FLOPs. Best view in color.}
	\vspace{-5mm}
	\label{vimeo_table}
\end{table*}

\begin{figure*}[thbp]
	\centering
	\includegraphics[width=\textwidth]{img/vid4_v2.jpg}
	\caption{Qualitative comparison on the \textbf{Vid4} for 4SR. Zoom in for better visualization.}
	\vspace{-2mm}
	\label{vid_figure}
\end{figure*} 

\begin{figure*}[thbp]
	\centering
	\includegraphics[width=\textwidth]{img/vimeo.jpg}
	\caption{Qualitative comparison on the \textbf{Vimeo-90K-T} for 4SR. Zoom in for better visualization.}
	\vspace{-3mm}
	\label{vimeo_figure}
\end{figure*} 


To evaluate the proposed method, a series of experiments are conducted and results are compared with existing state-of-the-art methods. Subsequently, a detailed ablation study is conducted to analyze the effectiveness of the proposed temporal grouping, group attention and fast spatial alignment. Results demonstrate the effectiveness and superiority of the proposed method.

\subsection{Implementation Details}
\textbf{Dataset.}
Similar to \cite{haris2019recurrent,xue2019video}, we adopt Vimeo-90k~\cite{xue2019video} as our training set, which is a widely used for the task of video super-resolution. We sample regions with spatial resolution 256256 from high resolution video clips. Similar to \cite{jo2018deep,xue2019video,yi2019progressive} low-resolution patches of  are generated by applying a Gaussian blur with a standard deviation of  and  downsampling. We evaluate the proposed method on two popular benchmarks: Vid4~\cite{liu2013bayesian} and Vimeo-90K-T\cite{xue2019video}. Vid4 consists of four scenes with various motion and occlusion. Vimeo-90K-T contains about 7 high-quality frames and diverse motion types.

\textbf{Implementation details.} 
In the intra-group fusion module, three 2D units are used for spatial features extractor, which is followed by a 3D convolution and eighteen 2D units in the 2D dense block to integrate information within each group.
For the inter-group fusion module, we use four 3D units in the 3D dense block and twenty-one 2D units in the 2D dense block. The channel size is set to 16 for convolutional layers in the 2D and 3D units. Unless specified otherwise, our network takes seven low resolution frames as input. 
The model is supervised by pixel-wise  loss and optimized with Adam \cite{kingma2014adam} optimizer in which  and . Weight decay is set to  during training. 
The learning rate is initially set to  and later down-scaled by a factor of 0.1 every 10 epoches until 30 epochs. 
The size of mini-batch is set to 64. The training data is augmented by flipping and rotating with a probability of 0.5.
All experiments are conducted on a server with Python 3.6.4, PyTorch 1.1 and Nvidia Tesla V100 GPUs.
\subsection{Comparison with State-of-the-arts}



\begin{figure}[t]
	\centering
	\includegraphics[width=1\columnwidth]{img/temporal_profile.pdf}
	\caption{Visualization of temporal consistency for \textit{calendar} sequence. Temporal profile is produced by recording a single pixel line ({\color{green}green} line) spanning time and stacked vertically.
	}
	\vspace{-5mm}
	\label{temporal-profile}
\end{figure} 

We compare the proposed method with six state-of-the-art VSR approaches, including TOFlow~\cite{xue2019video}, SPMC~\cite{tao2017detail}, Liu~\cite{liu2017robust}, DUF~\cite{jo2018deep}, RBPN~\cite{haris2019recurrent}, EDVR~\cite{wang2019edvr} and PFNL~\cite{yi2019progressive}. Both TOFlow and SPMC apply 
explicit pixel-level motion compensation with optical flow estimation, while RBPN uses pre-computed optical flow as additional input. DUF, EDVR and PFNL conduct VSR with implicit motion compensation. We carefully implement TOFlow and DUF on our own, and rebuild RBPN and EDVR based on the publicly available code. We reproduce the performance of most of these methods as reported in the paper except for EDVR. 
Tab.~\ref{vid4_table} and Tab.~\ref{vimeo_table} give quantitative results of state-of-the-art methods on Vid4 and Vimeo-90K-T, which are either reported in the original papers or computed by us. In the evaluation, we take all frames into account except for the DUF method~\cite{jo2018deep} which crop 8 pixels on four borders of each frame since it suffer from severe border artifacts. In addition, we also include the number of parameters and FLOPs for most methods on an LR image of size  in Tab.~\ref{vimeo_table}. 
On Vid4 test set, the proposed method achieves a result of 27.59dB PSNR in the Y channel and 26.10dB PSNR in RGB channel, which outperforms other state-of-the-art methods by a large margin. 
Qualitative result in Fig.~\ref{vid_figure} also validates the superiority of the proposed method. Attributed to the proposed temporal group attention, which is able to make full use of complementary information among frames, our model produces sharper edges and finer detailed texture than other methods.
In addition, we extract temporal profiles in order to evaluate the performance on temporal consistency in Fig.\ref{temporal-profile}. A temporal profile is produced by taking the same horizontal row of pixels from consecutive frames and stacking them vertically. The temporal profiles show that the proposed method gives temporally consistent results, which suffer less flickering artifacts than other approaches.

Vimeo-90K-T is a large and challenging dataset covering scenes with large motion and complicated illumination changes. The proposed method is compared with several methods including TOFlow, DUF, RBPN and EDVR. 
As shown in Tab.~\ref{vimeo_table} and Fig.~\ref{vimeo_figure}, the proposed method also achieves very good performance on this challenging dataset. It outperforms most state-of-the-art methods such as TOFlow, DUF and RBPN by a large margin both in PSNR and SSIM. The only exception is EDVR-L whose model size and computation is about four times larger than our method. In spite of this, our method is still rather comparable in PSNR and a little better in SSIM.

\subsection{Ablation Study}
In this section, we conduct several ablation study on the proposed temporal group attention and fast spatial alignment to further demonstrate the effectiveness of our method.

\textbf{Temporal Group Attention.} First we experiment with different ways of organizing the input sequence. 
One baseline method is to simply stack input frames along temporal axis and directly feed that to several 3D convolutional layers, similar to DUF~\cite{jo2018deep}. Apart from our grouping method , we also experiment with other ways of grouping:  and . As shown in Tab.~\ref{group_mannar}, DUF-like input performs worst among these methods. That illustrate that integrating temporal information in a hierarchical manner is a more effective way in integrating information across frames. Both  and  are better than , which implies the advantage of adding the reference frame in each group. Having the reference in the group encourages the model to extract complementary information that is missing in the reference frame. Another 0.05dB improvement of our grouping method  could be attributed to the effectiveness of motion-based grouping in employing temporal information.
\begin{table}[t]
	
	\centering
	\scalebox{0.65}{	
\begin{tabular}{lcccc}
			
			\toprule
			Model   & DUF-like & & &  
			\\
			TG?   &\XSolidBrush &\Checkmark  &\Checkmark &\Checkmark
			\\
			\midrule
			Vid4          &27.18/0.8258   &27.47/0.8384  &27.54/0.8409   & \textbf{27.59}/\textbf{0.8419}
			\\
			Vimeo-90K-T   &37.06/0.9465 &37.46/0.9487  &37.51/0.9509  &\textbf{37.59}/\textbf{0.9516}
			\\
			\bottomrule
		\end{tabular}
	}
\vspace{1mm}
	\caption{Ablation on: different grouping strategies.}
	\vspace{-5mm}
	\label{group_mannar}
\end{table}

In addition, we also evaluate a model which removes the attention module from our whole model. 
As shown in Tab.~\ref{frames}, this model performs a little worse than our full model. 
We also train our full model with a sequence of 5 frames as input. The result in Tab.~\ref{frames} shows that the proposed method can effectively borrow information from additional frames. We notice that the proposed method outperforms DUF even with 2 fewer frames in the input.
In addition, we conduct a toy experiment where a part of a neighboring frame is occluded and visualize the maps of temporal group attention. As shown in Fig.~\ref{occlusion}, the model does attempt to borrow more information from other groups when a group can not provide complementary information to recover the details of that region. 
\begin{table}[th]
	\begin{center}
		\scalebox{0.8}{	
\begin{tabular}{lccc}
				
				\toprule
				Model   & Model 1 & Model 2 & Model 3
				\\
				\midrule
				\# Frames  & 7   & 5 & 7
				\\
GA?       &\XSolidBrush &\Checkmark  &\Checkmark
				\\
				\midrule  	
				Vid4       & 27.51/0.8394       &27.39/0.8337   & \textbf{27.59}/\textbf{0.8419}
				\\
				Vimeo-90K-T  & 37.43/0.9506      & 37.34/0.9491   &\textbf{37.59}/\textbf{0.9516}  
				\\
				\bottomrule
			\end{tabular}
		}
	\end{center}
\vspace{1mm}
	\caption{Ablations on: group attention (GA) module and the influence of the different input frames in our hierarchical information aggregation way.}
    \vspace{-5mm}
	\label{frames}
\end{table}

~~\\
\textbf{Fast Spatial Alignment.} To investigate the effectiveness and efficiency of the proposed fast spatial alignment, we equip the proposed TGA model with three different pre-alignment strategies: TGA without alignment, TGA with PyFlow~\cite{pathak2017learning}, and TGA with FSA. The evaluation is conducted on Vimeo-90K-T where there is various motion in the video clips.
Tab.~\ref{pre-align} shows the performance of TGA with PyFlow is significantly inferior than the TGA model without any pre-alignment. It implies that imperfect optical flow estimation leads to inaccurate motion compensation such as distortion on the regions with large motion (see the green box in Fig.~\ref{flow_homo}), which confuses the model during training and hurts the final video super-resolution performance. In contrast, the proposed FSA boosts the performance of the TGA model from 37.32dB to 37.59dB. This demonstrates that the proposed FSA, which although does not perfectly align frames, is capable of reducing appearance differences among frames in a proper way. We also compute time cost of this module on Vimeo-90K-T dataset and present it in Tab.~\ref{pre-align}. Our FSA method is much more efficient than the PyFlow method. Note that since every sequence in Vimeo-90K-T only contains 7 frames, the advantage of FSA in reducing redundant computation is not fully exployed. Both PyFlow and our FSA are run on CPU, and FSA could be further accelerated with optimized GPU implementation.\\ 
\begin{table}[t]
	\centering
	\scalebox{0.8}{	
\begin{tabular}{lccc}
			
			\toprule
			Pre-alignment & w/o & w/ PyFlow~\cite{pathak2017learning}  & w/ FSA
			\\
			\midrule
			PSNR/SSIM & 37.32/0.9482 &35.14/0.9222   &\textbf{37.59}/\textbf{0.9516}
			\\
			Time (CPU+GPU) & 0+70.8ms &760.2+70.8ms & 18.6+70.8ms
			\\
			\bottomrule
		\end{tabular}
	}
     \vspace{1mm}
	\caption{Ablation on: the effectiveness and efficiency of the fast spatial alignment module. The elapsed time are calculated on processing a seven frame sequence with LR size of 11264.}
  \vspace{-1mm}
	\label{pre-align}
\end{table}

 \begin{figure}[t]
 	\centering
 	\includegraphics[width=1\columnwidth]{img/occlusion_2_v2.jpg}
 	\caption{Visualization of group attention masks under occlusion settings.  and  denote three groups. 
 	}
  	\vspace{-5mm}
 	\label{occlusion}
 \end{figure} 
















%
