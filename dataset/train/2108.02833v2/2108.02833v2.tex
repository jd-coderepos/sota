\section{Experiments}


\subsection{Datasets and Splits}
\noindent\textbf{Existing ZSAR Benchmarks.}
Olympic Sports~\cite{niebles2010modeling}, HMDB51~\cite{kuehne2011hmdb} and UCF101~\cite{soomro2012ucf101} are the three most popular datasets used in existing ZSAR papers \cite{junior2019zero}, which contain 783, 6766 and 13320 videos of 16, 51, 101 action categories respectively.
For robust evaluation, Xu \etal \cite{xu2017transductive} proposed to evaluate on 50 independent data splits and report the average accuracy and standard deviation.
In each split, videos of 50\% randomly selected classes are used for training and the remaining 50\% classes are held unseen for testing.
We adopt the same data splits as \cite{xu2017transductive} for fair comparison.

There are two major limitations in the above ZSAR protocols.
Firstly, it is problematic to use deep features pre-trained on other large-scale supervised video datasets because there exist overlapped action classes between pre-training classes and testing classes.
Secondly, the size of training and testing data is small which leads to large variations among different data splits, so that abundant numbers of experiments are necessary to evaluate a model.
To address these limitations, Brattoli \etal \cite{brattoli2020rethinking} proposed another setting which excludes classes overlapped with the above testing dataset in pre-training dataset Kinetics.
Nevertheless, their overlapped class selection algorithm is too tender, leaving the testing classes still seen in the training. Moreover, new end-to-end training of video backbones is needed because this setting does not follow the official Kinetics data split.
Therefore, in this work, we propose a more \emph{realistic, convenient and clean ZSAR protocol}.

\vspace{0.2em}
\noindent\textbf{Our Proposed Kinetics ZSAR Benchmark.}
The evolution of the Kinetics dataset \cite{carreira2018short,carreira2017quo} naturally involves increment of new action classes: Kinetics-400 and Kinetics-600 datasets contains 400 and 600 action classes, respectively. Due to some renamed, removed or split classes in Kinetics-600, we obtain 220 new action classes outside of Kinetics-400 after cleaning.
Therefore, we use 400 action classes in Kinetics-400 as seen classes for training. We randomly split the 220 new classes in Kinetics-600 into 60 validation classes and 160 testing classes respectively.
We independently split the classes for three times for robustness evaluation. 
As shown in our experiments, due to the large-size training and testing sets, the variations of different splits are significantly smaller than previous ZSAR benchmarks. In summary, our benchmark contains 212,577 training videos from Kinetics-400 training set, 2,682 validation videos from Kinetics-600 validation set and 14,125 testing videos from Kinetics-600 testing set on average of the three splits.
More details of our evaluation protocol are in the supplementary material.

\subsection{Implementation Details}
For action class embedding, we use a pretrained 12-layer BERT model \cite{devlin2018bert}, and finetune the last two layers if not specified.
For video embedding, we use TSM \cite{lin2019tsm} pretrained on Kinetics-400 in the spatio-temporal stream for Kinetics benchmark, and BiT image model \cite{kolesnikov2019big} pretrained on ImageNet for the other three benchmarks to avoid overlapped action classes in Kinetics; the object stream uses BiT image model~\cite{kolesnikov2019big} pretrained on ImageNet21k \cite{deng2009imagenet} and top-5 objects are selected for each video.
The above backbones are fixed for fast training.
We use one Nvidia TITAN RTX GPU for the experiments.
More details are presented in the supplementary material.
We set the dimensionality  of the common semantic space,  in the loss and use top-5 objects in the ER loss.
We use ADAM algorithm to train the model with weight decay of 1e-4. The base learning rate is 1e-4  with warm-up and cosine annealing. The model was trained for 10 epochs except on the Olympic Sports dataset where we train 100 epochs due to its small training size. The best epoch is selected according to performance on the validation set.
Top-1 and top-5 accuracies (\%) are used to evaluate all models.

\begin{table}
	\centering
	\footnotesize
	\tabcolsep=0.1cm
	\begin{tabular}{lccccccc} \toprule
		Method & Video & Class & Olympics & HMDB51 & UCF101 \\ \midrule
		DAP \cite{lampert2009learning}   & FV & A & 45.4  12.8 & N/A & 15.9  1.2 \\
		IAP  \cite{lampert2009learning}  & FV & A & 42.3  12.5 & N/A & 16.7  1.1 \\
		HAA \cite{liu2011recognizing}  & FV & A & 46.1  12.4 & N/A & 14.9  0.8 \\
		SVE \cite{xu2015semantic}  & BoW & W & N/A & 13.0  2.7 & 10.9  1.5 \\
		ESZSL  \cite{romera2015embarrassingly} & FV & W & 39.6  9.6 & 18.5  2.0 & 15.0  1.3 \\
		SJE \cite{akata2015evaluation}  & FV & W & 28.6  4.9 & 13.3  2.4 & 9.9  1.4 \\
		SJE  \cite{akata2015evaluation} & FV & A & 47.5  14.8 & N/A & 12.0  1.2 \\
		MTE  \cite{xu2016multi} & FV & W & 44.3  8.1 & 19.7  1.6 & 15.8  1.3 \\
		ZSECOC \cite{qin2017zero} & FV & W & 59.8  5.6 & 22.6  1.2 & 15.1  1.7 \\
		UR \cite{zhu2018towards} & FV & W & N/A & 24.4  1.6 & 17.5  1.6 \\
		O2A \cite{jain2015objects2action}  & Obj & W & N/A & 15.6 & 30.3 \\
		ASR \cite{wang2017alternative} & C3D & W & N/A & 21.8  0.9 & 24.4  1.0 \\
		TS-GCN \cite{gao2019know} & Obj & W & 56.5  6.6 & 23.2  3.0 & 34.2  3.1 \\
		E2E \cite{brattoli2020rethinking} & r(2+1)d & W & N/A & 32.7 & 48 \\ \midrule
		Ours  & (S+Obj) & ED & \textbf{60.2  8.9} & \textbf{35.3  4.6} & \textbf{51.8  2.9}  \\
	\bottomrule
	\end{tabular}
	\caption{ZSAR performances on the three existing benchmarks. Video: fisher vector (FV), bag of words (BoW), object (Obj), image spatial feature (S), (trained on video datasets), (trained on ImageNet dataset); Class: attribute (A), word embedding of class names (W), word embedding of class texts (W), elaborative description (ED). The average top-1 accuracy (\%)  standard deviation are reported.}
	\label{tab:three_datasets_results}
\end{table} 
\subsection{Evaluation on Existing ZSAR Benchmarks}
We compare our model with:
(1) Direct/Indirect Attribute Prediction (DAP, IAP) \cite{lampert2009learning};
(2) Human Actions by Attribute (HAA) \cite{liu2011recognizing};
(3) Self-training method with SVM and semantic Embedding (SVE) \cite{xu2015semantic};
(4) Embarrassingly Simple Zero-Shot Learning (ESZSL) \cite{romera2015embarrassingly};
(5) Structured Joint Embedding (SJE) \cite{akata2015evaluation};
(6) Multi-Task Embedding (MTE) \cite{xu2016multi};
(7) Zero-Shot with Error-Correcting Output Codes (ZSECOC) \cite{qin2017zero};
(8) Universal Representation (UR) model \cite{zhu2018towards};
(9) Objects2Action (O2A) \cite{jain2015objects2action};
(10) Alternative Semantic Representation (ASR) \cite{wang2017alternative}, which uses text descriptions and images as alternative class embedding;
(11) TS-GCN \cite{gao2019know} which builds graphs among action and object classes with ConceptNet for better action class embedding;
(12) End-to-End Training (E2E) \cite{brattoli2020rethinking} which uses a reduced Kinetics training set by excluding part of action classes overlapped with testset.
All above methods are evaluated on the inductive ZSL setting, where the videos of unseen action classes are unavailable during training. The unseen action classes are not used in training except \cite{gao2019know}.

Table~\ref{tab:three_datasets_results} presents the comparison.
To avoid leaking information from features pretrained on Kinetics video dataset, we only use image features and predicted objects from a 2D network pretrained on ImageNet \cite{kolesnikov2019big} for video semantic representation learning.
The proposed ER-enhanced ZSAR model achieves consistent improvements over state-of-the-art approaches on three benchmarks.
Our model outperforms previous best performances (without using pretrained video features) with 0.4, 10.9 and 17.6 absolute gains on OlympicSports16, HMDB51 and UCF101 respectively, and achieves even better performance than E2E trained on large-scale Kinetics dataset with 2.6 and 3.8 gains on HMDB51 and UCF101 datasets\footnote{We observe large performance variations with different random weight initialization, which mainly results from the small training set.}.
This demonstrates the effectiveness of our ED as action semantic representation and the ER objective to improve generalization ability of the model.


\begin{table}
	\centering
	\small
	\begin{tabular}{lcccc} \toprule
		Method & Video & Class & top-1 & top-5 \\ \midrule
		DEVISE \cite{frome2013devise} & \multirow{6}{*}{ST} & \multirow{6}{*}{W} & 23.8  0.3 & 51.0  0.6 \\
		ALE \cite{akata2015label} &  &  & 23.4  0.8 & 50.3  1.4 \\
		SJE \cite{akata2015evaluation} &  &  & 22.3  0.6 & 48.2  0.4 \\
		DEM \cite{zhang2017learning} &  &  & 23.6  0.7 & 49.5  0.4 \\
		ESZSL \cite{romera2015embarrassingly} &  &  & 22.9  1.2 & 48.3  0.8 \\
		GCN \cite{ghosh2020all} &  &  & 22.3  0.6 & 49.7  0.6 \\  \midrule
		\multirow{2}{*}{Ours} & ST & \multirow{2}{*}{ED} & 37.1  1.7 & 69.3  0.8 \\
		& ST+Obj &  & \textbf{42.1  1.4} & \textbf{73.1  0.3} \\ \bottomrule
	\end{tabular}
	\caption{ZSAR performance on the proposed Kinetics benchmark. Notations are the same as Table~\ref{tab:three_datasets_results}; ST: spatio-temporal feature.}
	\label{tab:kinetics_sota_cmpr}
\end{table}

\subsection{Evaluation on Kinetics ZSAR Benchmark}
Due to limitations of existing benchmarks, we further carry out extensive experiments on more realistic Kinetics ZSAR setting to evaluate the effectiveness of our model.

\subsubsection{Comparison with State of the Arts}
We re-implement state-of-the-art ZSL algorithms on the proposed benchmark, including:
(1) DEVISE \cite{frome2013devise}; (2) ALE \cite{akata2015label}; (3) SJE \cite{akata2015evaluation}; (4) DEM \cite{zhang2017learning}; (5) ESZSL \cite{romera2015embarrassingly}; and (6) GCN \cite{ghosh2020all}: a very recent ZSAR work leveraging knowledge graphs of action classes to predict classification weights as \cite{wang2018zero}.
Details are in the supplementary material.  

Table~\ref{tab:kinetics_sota_cmpr} shows the ZSAR performances of above methods.
When using the same Spatio-Temporal(ST) features extracted from TSM network, our ER-enhanced model with ED and ER loss significantly outperforms previous works with 13.3 and 18.3 absolute gains on top-1 and top-5 accuracies respectively.
The existing methods however achieves similar performances, which might due to ambiguous word embedding representations.
After fusing with object semantics in video semantic representation, the performance of our model gets another boost, demonstrating that ST visual features and object textual features are complementary.
Moreover, compared with the results in Table~\ref{tab:three_datasets_results}, the performance variations on different splits are much lower than those in previous benchmarks, which further proves the superiority of our benchmark for future ZSAR research.

\subsubsection{Ablation Studies}
\begin{table*}[ht]
    \small
    \begin{subtable}[h]{0.23\linewidth}
        \centering
        \begin{tabular}{c|cc} \toprule
        Class Rep & top-1 & top-5 \\ \midrule
        W & 26.5  0.4 & 54.7  1.2 \\
        Wiki & 25.8  1.1 & 50.4  1.6 \\
        Dict & 22.3  0.4 & 49.7  0.6 \\
        ED & \textbf{31.0  1.2}  & \textbf{63.2  0.4} \\ \bottomrule
       \end{tabular}
       \caption{Comparing action class texts.}
       \label{tab:action_class_text_ablation}
    \end{subtable}
    \hfill
    \begin{subtable}[h]{0.24\linewidth}
        \centering
        \begin{tabular}{c|cc} \toprule
        Class Enc & top-1 & top-5 \\ \midrule
        AvgPool & 25.3  1.2 & 54.7  0.6 \\
        AttnPool & 28.2  1.0 & 56.9  0.2 \\
        RNN & 25.4  1.1 & 53.7  1.1 \\
        BERT & \textbf{31.0  1.2} & \textbf{63.2  0.4} \\ \bottomrule
        \end{tabular}
        \caption{Comparing action class encoders.}
        \label{tab:action_class_encoder_ablation}
    \end{subtable}
    \hfill
    \begin{subtable}[h]{0.33\linewidth}
        \centering
        \begin{tabular}{cc|cc} \toprule
        Video & ER & top-1 & top-5 \\ \midrule
        ST & w/o & 31.0  1.2 & 63.2  0.4 \\
        ST & w/ & 37.1  1.7 & 69.3  0.8 \\
        Obj & w/o & 34.6  1.4 & 60.6  1.1 \\
        Obj & w/ & 36.7  1.0 & 63.2  0.5 \\ \bottomrule
        \end{tabular}
        \caption{Comparing models with or without ER loss.}
        \label{tab:er_loss_ablation}
    \end{subtable}
    \hfill
    \hfill
    \begin{subtable}[h]{0.4\linewidth}
        \centering
        \begin{tabular}{c|cc} \toprule
        Video & top-1 & top-5 \\ \midrule
        ST & 37.1  1.7 & 69.3  0.8 \\
        Obj & 36.7  1.0 & 63.2  0.5 \\
        ST + Obj & \textbf{42.1  1.4} & \textbf{73.1  0.3} \\ \bottomrule
        \end{tabular}
        \caption{Comparing video representations.}
        \label{tab:video_st_ablation}
    \end{subtable}
    \hfill
    \begin{subtable}[h]{0.55\linewidth}
        \centering
    	\begin{tabular}{c|c|cc} \toprule
    		Video & Loss & top-1 & top-5 \\ \midrule
    		Obj (Name) & ER (Name) & 34.5  1.6 & 61.4  1.2\\
    		Obj (ED) & ER (Name) & 36.3  1.3 & 62.8 	0.9  \\
    		Obj (ED) & ER (ED) & 36.7  1.0 & 63.2  0.5 \\ \bottomrule
    	\end{tabular}
    	\caption{Comparing EDs and class names to represent object classes.}
    	\label{tab:ed_obj_ablation}
    \end{subtable}
    \caption{Ablation studies on the Kinetics ZSAR benchmark.}
    \label{tab:kinetics_ablation_results}
\end{table*}


%
 
We present the following \emph{Q\&As} to prove the effectiveness of our proposed semantic representations and the ER training objectives. More hyper-parameter ablation and analysis are in the supplementary material.
All the ablation studies below are carried out on the Kinetics ZSAR benchmark.

\textbf{Is human involvement necessary for action class representation?}
In Table~\ref{tab:action_class_text_ablation}, we compare different action class representations including action class names (W), Wikipedia entries (Wiki), Dictionary definitions (Dict) and the manually modified EDs.
All the models use TSM video features and the  objective in Eq.~\ref{eqn:act_loss} for training. The W is encoded with pre-trained Glove word embedding while others are encoded by BERT because we observe that BERT is not suitable to encode short text such as the class names.
We can see that the automatically crawled texts of the action class are very noisy which are even inferior to the ambiguous class names.
However, with a minimal manual clean of crawled descriptions, we achieve significant improvements such as 8.5\% absolute gains on top-5 accuracy compared to W. 
This proves that even such easy human involvement is beneficial to the class representation quality as justified in Section~\ref{sec:elaborative_description}, and ED is more discriminative action class prototype than word embedding.


\textbf{How much improvements are from the pre-trained BERT model?}
In Table~\ref{tab:action_class_encoder_ablation}, we compare different action class encoding modules for EDs.
AvgPool, AttnPool and RNN all transfer knowledge from a pre-trained Glove word embedding, and apply average pooling, attentive weighted pooling and bi-directional GRU respectively to encode the ED sentence.
Similar to Table~\ref{tab:action_class_text_ablation}, all the models use TSM video features and are trained with .
The pretrained BERT significantly boosts the performance over the other three encoding modules, demonstrating its effectiveness to understand action descriptions.

\textbf{Is the ER loss beneficial?}
Table~\ref{tab:er_loss_ablation} compares models trained with or without ER loss.
The generalization ability on unseen actions is boosted by a large margin through the ER-enhanced training for both ST and object features.
The ER loss augments the semantic labels for videos from automatic elaborative concepts, making the features more generalizable to unseen classes.


\textbf{Whether ST features and object features are complementary?}
The object features alone ``Obj" in Table~\ref{tab:video_st_ablation} are comparable with ST features on top-1 accuracy, but are worse than ST on top-5 accuracy. Their combination ``ST+Obj" via the proposed multimodal channel attention achieves the best performance on the Kinetics ZASR setting.
This shows that object features alone are not discriminative enough, compared to ST features, to differentiate actions. But adding object features enriches ST with the shared semantic embedding among the action classes.

\textbf{Whether EDs are universal representations for both actions and objects?}
Though we show that ED is beneficial to represent action classes, it remains a question whether ED also improves semantic representation for objects.
To be noted, the EDs for objects are automatically extracted from WordNet thanks to the good correspondence between ImageNet classes and WordNet concepts.
Therefore, we replace the ED with the class name of the object in Eq.~\ref{eqn:video_obj_embed} for video object embedding, and in Eq.~\ref{eqn:er_loss} for the ER training objective.
From Table~\ref{tab:ed_obj_ablation}, we see that even though objects are less ambiguous than actions, it is still beneficial to use its ED instead of its class name.

We provide more hyper-parameter ablations and analysis in the supplementary material.

\subsection{Comparison with Supervised Learning}
Previous ZSAR works mainly benchmark the progress with respect to zero-shot methods. However, it is interesting to know how well the state-of-the-arts ZSAR methods really work from a practical prospective of video action recognition. We present one of the first attempts for this purpose. 

In Table~\ref{tab:few_shot_comparison}, we compare our ZSAR model with supervised models trained with different numbers of labeled videos of unseen classes in our Kinetics ZSAR benchmark.
To avoid overfitting on few training samples, we use the same fixed ST features from TSM and only train a linear classification layer. 
It servers as a simple but strong baseline for few-shot learning as suggested in \cite{tian2020rethinking}.
Our ER-enhanced ZSAR model improves over the 1-shot baseline by a large margin, but is still inferior to the model using 2 labeled videos per classes. 
Although our work is the new state-of-the-arts in Table~\ref{tab:three_datasets_results} and \ref{tab:kinetics_sota_cmpr}, it only establishes a starting point from which ZSAR models are comparable with supervised models trained on very few samples.



\begin{table}
	\centering
	\small
	\begin{tabular}{cccc} \toprule
		& \#videos per class & Top-1 (\%) & Top-5 (\%) \\ \midrule
		ER-ZSAR & 0 &42.1  1.4 & 73.1  0.3  \\ \midrule
		\multirow{4}{*}{supervised} & 1 & 31.8  0.8 & 60.2  2.5 \\
		& 2 & 45.0  0.9 & 73.2  0.6 \\
		& 5 & 56.5  1.5 & 83.4  0.8 \\
		& 100 & 72.7  1.4 & 93.3  0.5 \\ \bottomrule
\end{tabular}
	\caption{Comparison of our ER-enhanced ZSAR model and supervised few-shot baselines on the Kinetics benchmark.}
	\label{tab:few_shot_comparison}
	\vspace{-0.2cm}
\end{table}
