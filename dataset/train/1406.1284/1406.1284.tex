\documentclass[conference]{IEEEtran}

\usepackage{cite}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{epsfig}
\usepackage[latin1]{inputenc}
\usepackage{amsthm}
\usepackage{subfigure}

\hyphenation{op-tical net-works semi-conduc-tor}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}

\theoremstyle{remark}
\newtheorem{lemma}{Lemma}

\begin{document}

\title{The Price of Updating the Control Plane in Information-Centric Networks}

\author{\,}

\author{Bita Azimdoost, Cedric Westphal, and Hamid R. Sadjadpour\\
Department of Electrical and Computer Engineering, 
University of California Santa Cruz\\
 Huawei Innovation Center, Santa Clara, CA
}

\maketitle

\begin{abstract}
We are studying some fundamental properties of the interface between control and data planes in Information-Centric Networks.We try to evaluate the traffic between these two planes based on allowing a minimum level of acceptable distortion in the network state representation in the control plane. We apply our framework to content distribution, and see how we can compute the overhead of maintaining the location of content in the control plane. This is of importance to evaluate content-oriented network architectures: we identify scenarios where the cost of updating the control plane for content routing overwhelms the benefit of fetching a nearby copy. We also show how to minimize the cost of this overhead when associating costs to peering traffic and to internal traffic for operator-driven CDNs.
\end{abstract}

\section{Introduction}

Routing in Information-Centric Networks (ICNs) is performed based upon content names. Name Resolution Service-based ICNs \cite{Ahlgren2012Survey} require the routing mechanism to be dynamically updated with the content location. Content location from the forwarding plane needs to be delivered to the control plane. This raises the following question: depending on the size of the domain being controlled, of the underlying state space, of the dynamics of the evolution of the state in the forwarding plane, how much data is required to keep the ICN control plane up to date of the content location?

We consider the issue of maintaining a consistent view of the underlying state at the control layer, through an abstracted mechanism, which can be applied to a wide range of scenarios: we consider the underlying state as an evolving random  process, and calculate the information theoretic rate that this process would create to keep the representation of this state up-to-date in the control plane. This provides a lower bound on the bandwidth overhead required for the control plane to have an accurate view of the forwarding plane.

We then illustrate the power of our model by focusing on the specific case of locating content in ICNs. Enabling content routing has attracted a lot of attention recently, and thus we are able to shed some light on its feasibility.  In this case, the underlying state depends on the size and number of caches, on the request for content process and on the caching policy. We  apply our framework to derive the bandwidth needed to accurately locate a specific piece of content. We observe that there is a  trade-off for keeping an up-to-date view of the network at the cost of  significant  bandwidth utilization, versus the gain achieved by fetching the nearest copy of the content. We consider some simple scenarios to illustrate this trade-off.

Our contribution is as follows:\\
 A framework is presented to quantify the minimal amount of information required to keep a (logical) control plane aware of the state of the forwarding plane. We believe this framework to be useful in many distributed systems contexts (Lemma \ref{lem:01});\\
 This framework is applied to the specific case of locating content, and the effect of the availability of caches, the caching policy and the content popularity on content location is studied. We can thus apply our results to some of the content-oriented architectures (Theorems \ref{thm:1}, \ref{thm:2});\\
 We see how our framework allows to define some optimal policies with respect to the content that should be cached for an Information-Centric Network (Section\ref{sec:costanalysis}).

We quickly note that our framework does not debate the merit of centralized vs distributed, as the control layer we consider could be either. For a routing example, our model would provide a lower-bound estimate of the bandwidth for, say OpenFlow to update a centralized SDN controller, or for a BGP-like mechanism to update distributed routing instances.

Our results are theoretic in nature, and provide a lower bound on the overhead. We hope they will provide a practical guideline for protocol designers to optimize the protocols which synchronize the network state and the control plane.

The rest of the paper is organized as follows. After going over some related work in section \ref{sec:related}, we  introduce our framework to model the protocol overhead in section \ref{sec:protocol}. Section \ref{sec:scenarios} utilizes the derived model to study some simple caching networks. We show the power of the model in the protocol design by computing the cost of content routing in Section~\ref{sec:costanalysis} and suggesting a cache management policy. Finally, section \ref{sec:conclusion} concludes the paper.

\section{Related Work}
\label{sec:related}

SDN makes the separation explicit between the control and forwarding layer. Interactions between the control and forwarding planes has been pointed out as one of the bottlenecks of OpenFlow~\cite{McKeown2008OpenFlow}. As a consequence, \cite{Curtis2011Devo} or \cite{Yu2010Difane} attempt to reduce the amount of interaction in between the switches and the control layer. \cite{Levin2012Logically} studies the gap between the state of the actual system and the view of the (logically centralized) controller, focusing on consistency. There has been no attempt to model the interaction between the control and forwarding layers to our knowledge.

The control plane needs to obtain adequate information about the underlying states so that the network can perform within a satisfactory range of distortion. The first theoretical study of this information was conducted by Gallager in \cite{Gallager1976Basic}, which utilized rate distortion theory to calculate the information required to extract network parameters. \cite{Wang2012Cost} applied these ideas to mobile wireless networks. An information-theoretic framework to model the relationship between network information and network performance was derived in \cite{Hong2009Impact}.

One impetus to study the relationship between the control layer and the network layer comes from the increased network state complexity from trying to route directly to content. Request-routing mechanisms have been in place for a while\cite{Barbir2003Requestrouting} and proposals~\cite{Davie2012Framework} have been suggested to share information between different CDNs, in essence enabling the control planes of two domains to interact. Many architectures have been proposed that are oriented around content\cite{Gritter2001Architecture,Koponen2007Dataoriented,Jacobson2009Networking,Zhang2010Named,Pursuit,Ahlgren2012Survey} and some have raised concerns about the scalability of properly identifying the location of up to  pieces of content\cite{Ghodsi2011InformationCentric}. Our model presents a mathematical foundation to study the pros and cons of such architecture.

Cache management goes jointly with content routing. \cite{Tang2008BenefitBased}\cite{Bhattacharjee1998Self}\cite{Cho2012WAVE} present cache management policies. Some cooperative cache management algorithms have been developed in \cite{Borst2010Distributed} which attempt to maximize the traffic volume served from cache and minimize the bandwidth cost in content distribution networks. \cite{Sourlas2012Autonomic} proposes some online cache management algorithms for Information Centric Networks (ICNs) where all the contents are available by caching in the network. \cite{Chai2012Cache} investigates if caching only in a subset of nodes along the path in ICNs can achieve better performance in terms of cache hit rate. None of these work study the overhead required to learn the content location. 

\section{Protocol Overhead Model}
\label{sec:protocol}

We now turn our attention to the mechanism to synchronize the view at the control layer with the underlying network state. Assume that  describes the state of random process  in a network at time . 

In order to update the control plane's information about the states of  in the network, the forwarding plane must send update packets regarding those states to the control plane whenever some change occurs. Let  denote the control plane's perceived state of  at time . It is obvious that no change in  will happen before  changes, and if  changes, the Control plane may or may not be notified of that change. Therefore,  there are some instances of time where . 

In this paper, we consider that the state can have two values  and . For instance, a link can be up or down; or a piece of content can be present at a node, or not. 

Please note: It is easy to see that such boolean state space can be generalized to other possible values for . For instance, if one wanted to measure the congestion on a link, one could quantize the link congestion into bins (say bins  to  for normalized link utilization between 0 and 0.1, 0.1 to 0.2, , 0.9 to 1) and map a the link utilization to a 0-1 variable such that  if the current link utilization is in  and 0 otherwise. Therefore, there is no loss of generality of selecting a 0-1 variable, and it greatly simplifies the exposition of the results. 

Let  and  denote the sequences of s and s time durations of  respectively, and  denote the times of changes. We assume that  is an i.i.d sequence with probability density function (pdf)  and mean , and  is another i.i.d. sequence with pdf  and mean . We also assume that any two  and  are mutually independent\footnote{There is also no loss of generality in assuming independence of these processes for the following reason: we consider large distributed systems, where the input is driven by a large population of users (smaller systems offer no difficulty in tracking in the control plane what is happening in the data plane). It is a well known result that the aggregated process resulting from a large population of uncoordinated users will converge to a Poisson process, and therefore the events in the future are independent of the events in the past and depend only on the current state.}.

Fig. \ref{fig:statetime} illustrates the time diagram of state changes of such random process which is the state of the forwarding plane in the network being announced to the control plane. 

\begin{figure}[http]
    \center
      \includegraphics[scale=0.55,angle=0]{gennetmodel}\\
      \caption{\textit{Time diagram of : the state of random process  at time .}}
    \label{fig:statetime}
\end{figure}


 and  may differ in two cases; first, when the state of  is changed from  to  (change type I) but the control plane is not notified (); second, when the state of  is changed from  to  (change type II) and the control plane still has the old information about it (). Here we calculate the minimum rate at which the underlying plane has to update the state of  so that the mentioned errors are less than some values  for the first type of error, and  for the second type, respectively.  and  can be viewed as probability of false negative and false positive alarms at the controller.

We make an additional assumption that the delay of the network is negligible with respect to the time scale of the changes in the state of the system, and the control plane will be aware of the announced state immediately (the alternative - that the state of the system changes as fast or faster as the control plane can be notified of these changes - is obviously unmanageable). Thus, the above errors may occur just when the forwarding plane does not send an update about a change.

The main result now can be stated as a Lemma (with the proof in Appendix).

\begin{lemma}\label{lem:01}
	If the ups and downs in the state of  follow some distributions with means  and , respectively, then the minimum update rate  (number of update packets per second) satisfying the mentioned distortion criterion is given by

if  and . Otherwise an update of rate zero can satisfy the distortion criteria.
\end{lemma}

Equation \ref{eq:rX} shows the minimum update rate for state of a single random variable X in the underlying plane so that an accepted amount of distortion is satisfied. The total rate and consequently the total protocol overhead for keeping the control layer informed of the forwarding layer is the combination of all the overheads needed for all the random processes of the underlying layer, which may be independent of each other or have some impact on each other.
For example, the locations and velocities of different nodes in a mobile wireless network may be considered independent of each other, thus the total overhead will be simply the sum of the individual overheads. But in a caching network, the existence of a content in a specific cache strongly depends on the positions of the other contents if the cache storage sizes are limited.

In the following sections, we apply our model to cache networks and study the total data retrieval cost, including the protocol overhead.

\section{Content Location in ICNs}
\label{sec:scenarios}

Information-centric networks (ICN) require the control layer to know at least one location for each piece of data. However, many ICNs (for instance~\cite{Pursuit}), attempt to set up a route to a nearby copy by requesting the content from a pub/sub mechanism. The pub/sub rendez-vous point needs to know the location of the content. This is highly dynamic, as content can be cached, or expunged from the cache at any time. NDN~\cite{Zhang2010Named} also assumes that the routing plane is aware of multiple locations for a piece of content~\footnote{The routing (in NDN in particular) could know only one route to the content publisher or to an origin server and find cached copies opportunistically on the path to this server. But Fayazbakhsh et al~\cite{Fayazbakhsh2013Less} have demonstrated that the performance of such an ICN architecture would bring little benefit over that of strict edge caching.}.

The request process impacts the cache state, and we make the usual assumption that the items are requested according to a Zipf distribution with parameter ; meaning that the popularity of an item  is , where  is the size of the content set, and can be arbitrarily large. We also assume from now on that the Least-Recently-Used (LRU) replacement policy is used in the caches, as it is a common policy and has been suggested in some ICN architectures~\cite{Jacobson2009Networking}. (Other caching policies can be handled in a similar manner.) We denote the cache size by . From the popularity , Dan and Towsley \cite{Dan1990Approximate}  and Che et al|~\cite{Che2002Hierarchical} provide a model to calculate the probability  of an item being in the cache under the Independent Reference Model (IRM). Since we know the  distribution, we consider that  is also given.

In the following sections we use the results of section \ref{sec:protocol} together with LRU policy results to study the total control packet rates needed to keep the control plane updated about the items in the caches in two scenarios: 
\begin{itemize}
\item in scenario , we consider nodes updating the control plane of a domain (say, an AS) so as to route content to a copy of the cache within this domain if its available. We denote the control plane function which locates the content for each request as the Content Resolution System (CRS);
\item in scenario , we consider two controllers over two neighboring domains updating each other.
\end{itemize}

\subsection{Scenario : Intra-AS Cache-Controller Interaction}
\label{subsec:scenario1}

We will consider an autonomous network containing  nodes (terminals), each sending requests for items  with sizes  according to a Poisson distributed process with rate of . The total request rate for all the items from each node is denoted by . Note that the total request rate of each terminal is a fixed rate independent of the total number of nodes and items while the total requests for all the nodes is a function of  (namely ).

Suppose that there are  caches in the system () each with size  that can keep (and serve) any item  for some limited amount of time , which depends on the cache replacement policy. For simplicity, we assume that all the caches are similar to each other~\footnote{We can easily extend to the case of heterogenous caches at the cost of notation complexity. For instance, Theorem~\ref{thm:1} below can be stated as a sum over all  possible types of caches with  different 's for each type of cache, instead of a product by  of identical terms. Our purpose is to describe the homogenous case, and let the reader adapt the heterogenous case to suit her/his specific needs.} and the rate of requests for item  received by each cache is . Assume that  caches store item  during each download ( could be a single copy near the requester, or multiple copies at different caches).

Whenever a client has a request for an item, it needs to discover a location of that item, preferentially within the AS, and it downloads it from there. To do so, it will ask a (logically) centralized \emph{Content Resolution System (CRS)} or will locate the content by any other non-centralized locating protocol.

If the network domain is equipped with a CRS, it is supposed to have the knowledge of all the caches, meaning that each cache sends its item states (local presence or absence of each item) to the CRS whenever some state changes. 

Depending on the caching policy, whenever a piece of content is being downloaded, either no cache, all the intermediate caches on the path, or just the cache directly connected to the requester stores it in its content store independently of the content state in the other caches, or refresh it if it already contains it.

We want to compute the update rate for this system assuming that each downloaded piece of content is stored only at the cache directly connected to the requester, and demonstrate the following theorem.

\begin{theorem}\label{thm:1}
The total update rate is the summation of the rate  for all  with


if  and . Otherwise no update is needed.
\end{theorem}

\begin{proof}
Let the random process  in the forwarding plane denote the existence of item  in cache  at time , which is needed to be announced to the control plane (CRS). We assume that all the caches have the same characteristics resulting in the same average up and down duration times for each item in all the caches. Let  denote the mean duration item  spends in any cache , and  denote the mean duration of item  not being in the cache. We define .

It can be seen that at the steady-state, the probability of cache  containing item  will be .

In order to keep the CRS updated about the content states in the network, all the nodes have to send update packets regarding their changed items to the CRS. All the assumptions of section \ref{sec:protocol} are valid here. Thus, by replacing  and  in equation \ref{eq:rX} with  and  respectively, the result () shows the minimum rate at which each cache   has to send information about item  to the CRS.
	
Since we assumed that each cache stores items independent of the items in other caches, the total update rate for item , is the sum of the update rates in all caches which is , where  is the update rate obtained through equation \ref{eq:rX} for item  at cache .

The total rate of generating (or refreshing) copies of item  at each cache is , which equals to .
Replacing the values of  and  in  with  and  respectively, we can express the total update rate of item  in terms of the probability of this item being in a cache.

This yields the result of equation~\ref{eq:ri} and the total update rate for all the items is the summation of these rates.
\end{proof}

One important consequence of Theorem~\ref{thm:1} is that for large , due to the heavy tail of the popularity distribution, a significant number of the requests will be for items with low , thus creating an update rate that, according to the first line of equation~\ref{eq:ri}, will not vanish. This means that updating the control plane of an ICN architecture would create a significant amount of traffic with little or not benefits.

Fig. \ref{fig:updatemiss} illustrates  versus . The only parameters that can change this graph are  and . The higher distortion we tolerate, the less update announcements we need to handle. As can be seen the update rate starts from zero for those items which are for sure in the cache, and does not need any CRS update. At the other end of the graph, for the items which are almost surely not in the cache, again no update is needed. The number of items which need some updates is decreasing when higher distortions are accepted.

\begin{figure}[http]
    \center
      \includegraphics[scale=0.18,angle=0]{update_miss}\\
      \caption{\textit{Total item  cache-CRS update rate versus  for different distortion criteria.}}
    \label{fig:updatemiss}
\end{figure}

Fig. \ref{fig:lruLc} shows the changes of the total update rate (scaled by ) versus the cache storage size, such that the distortion criteria defined by  is satisfied. In this simulation . Note that each change in a cache consists of one item entering into and one other item being expunged from the cache, therefore if no distortion is tolerable, this rate will be  updates per change per cache.

\begin{figure}[http]
    \center
      \includegraphics[scale=0.18,angle=0]{ratevsLc1ASM1000}\\
      \caption{\textit{Total cache-CRS update rate (Updates per new arrival per cache) for different cache storage capacities () and (dashed), (solid), (dash-dotted)).}}
    \label{fig:lruLc}
\end{figure}

It can be observed that for very small storage sizes and small popularity index, almost each incoming item will change the status of the cache and a need for an update arises. When the storage size is still very small, the caches do not provide enough space for storing the items and reusing them when needed, so increasing the size will increase the update rate. At some point, the items will move down and up in the cache before going out, so increasing the storage size more than that will reduce the need to update. However, if the popularity index is large, then increasing cache size from the very small sizes will decrease the need to update since there are just a few most popular items which are being requested.

Moreover, as it is expected, the more distortion is tolerable, the CRS needs fewer change notifications. However, if the cache size is too big, or the popularity exponent is too high, fewer changes will occur, but almost all the changes are needed to be announced to the CRS. On the other hand, for small cache sizes accepting a little distortion will significantly decrease the update rate.

To figure out how the calculated rates perform in practice, we simulate an LRU cache with capacity  items, which are selected from a catalog of size  items and are requested according to Zipf distribution with parameter . In these simulations we first estimate the item availability in the cache , then using these estimated  and according to equations \ref{eq:U1} and \ref{eq:U2}, we calculate the update probability in case of a change. We then run the simulation for  Poisson requests and measure the average generated distortion during 20 rounds of simulation. Fig. \ref{fig:EvalDist1} illustrates the results for the case where .

\begin{figure}[http]
    \center
      \includegraphics[scale=0.18,angle=0]{EvalD1M103eps01Lc20a7}\\
			\includegraphics[scale=0.18,angle=0]{EvalD2M103eps01Lc20a7}\\
      \caption{\textit{Measured distortion type I () and II () for , , , and .}}
    \label{fig:EvalDist1}
\end{figure}

It is observed that for a large portion of the items the distortion type I satisfies the distortion criteria. Distortion type II, however, has more unsatisfied distortion. The reason is that the calculated update rates are strongly dependent on the availability of the items in the cache and any small error in the estimation of  may lead to some extra distortions. Since the 's are mostly very small, not updating just one type II change may cause an error which remain in the system for a long time, and thus creating a large distortion.



\subsection{Scenario : Interaction Between Two ASs}
\label{subsec:scenario2}

In this section, we assume two separate neighboring Autonomous Systems , ) similar to the one discussed in section \ref{subsec:scenario1}. Each AS can be considered as a big storage containing all the items of its caches. Let  and  denote the number and size of the caches inside , and  denote the probability of item  being in at least one cache in  for .


The control plane in each AS is informed about all the contents of the caches inside that AS through the mechanism described earlier in previous sections. The control planes (CRSs) of two separate ASs also need to be informed of the information stored in the other AS so that they can forward the requests to the proper AS in case of not being able to serve that request using the local caches.

Here we try to derive the minimum rate at which one AS () needs to send information about its stored items to the other AS () such that some distortion criteria is satisfied. Similar to the case of information updates inside one AS, two types of errors may happen; first, when  contains item  and  is not aware of that; second, when  does not contain item  and  assumes otherwise. Since each control plane first tries to find a copy of the requested data in some caches inside its own sub-network, none of these errors are  important if  itself contains item . Therefore, the distortion happens only when item  is not stored in any cache inside , and the distortion criteria is defined based on these errors.
 and .

 is the state of item  in  , and  denotes the state of item  in  perceived by the other AS ().

We assume that the existence of item  in different ASs are independent of each other. Consequently, the perceived state of item  in one AS by the other AS is independent of the existence of that item in the latter AS. Therefore,




Replacing  by  we will have


 is the probability of item  not being available in  and is equal to , where  denotes the probability of item  in each cache of . Let  and .
According to Lemma \ref{lem:01} the total information rate regarding item  in  reported to  is given by


if  and . Otherwise an update rate of zero can satisfy the distortion criteria.
	
In the above equations  and  are the average durations where item  is available in  and the durations where it is not, respectively.

Assume that the rate of generating or refreshing item  in at least one cache of  is denoted by . This rate is equal to the total rate of requests for item  from all the users connected to  which equals to . Similar to the reasoning in the proof of Theorem \ref{thm:1} we can derive the results which can now be stated as the following Theorem:

\begin{theorem}\label{thm:2}
The total information rate  can be written as:


In these equations 
and  and  can be calculated as in\cite{Dan1990Approximate}.
\end{theorem}

According to the conditions where nonzero update rates are required in the statement of Theorem~\ref{thm:2}, it can be observed that if an item is in a cache in an AS with high probability (large ), very few  control packets are needed to inform it of the same item in another AS. In other words, when an AS contains an item with high probability, it does not need to know about that item's status in the other AS and the distortion criteria is satisfied with very low rate or even no updates.

\begin{figure}[http]
    \center
      \includegraphics[scale=0.18,angle=0]{updatemiss2AS}\\
      \caption{\textit{Total item  AS-AS update rate versus  for different distortion criteria. (First AS contains  caches, and item  is in the second AS with low probability ())}}
    \label{fig:updatemiss2AS}
\end{figure}

In Fig. \ref{fig:updatemiss2AS} we fix the probability of  being in  to a low value () and plot the changes of the ratio of this rate to the request rate versus the probability of item  not being in each cache in , which we assume contains  caches.
If the item is not in a cache in an AS with high probability, but the probability of that item being in another cache is high  (low  resulting in high ), very few updates will be needed. Increasing  will decrease , so a higher update rate is needed. When this probability is higher than some point, then  is very low and the probability of any change in this item is very low, thus again very few updates may be needed.

\section{Application to Cost Analysis}
\label{sec:costanalysis}



Fig. \ref{fig:netmodel} illustrates the network model studied in this section. This model consists of entities in three substrates: users are located on the first layer; a network of caches with the CRS on the second level; external resources (caches in other networks, Internet, etc.) on the third.

\begin{figure}[http]
    \center
      \includegraphics[scale=0.4,angle=0]{netmodel}\\
      \caption{\textit{Network Model.}}
    \label{fig:netmodel}
\end{figure}

In this section, we try to calculate the total cost (download cost+update cost) in scenario  and look at the trade-offs between the cost, the number of caches, and the size of caches. We need to define the relative costs of the different actions. We assume that the state update process for item  has a per bit cost of  for sending data from the cache to CRS. On the other hand, the requested piece of content  may be downloaded from the local cache with cost 0 (with probability  of being in this cache), from another cache inside the same network with some per bit cost  (with a probability we denote by , where  is the probability that content  is within the AS's domain), or it must be downloaded from an external server with some other cost  (with probability ). Obviously, . 



The total download cost of item  with size  bits in the sub-network is


Each cache sends update packets to provide its CRS with the state of item  in its local content store. Each update packet contains the ID of the cache issuing the query, the ID of the updated item and the new state.  bits are needed to represent the cache. Item  is updated with probability , which results in a code length of at least  bits. Thus, the length of each update packet is . Hence, the total cost for updating information about item  in the sub-network is , where  is the minimum rate at which the update state of item  must be announced to CRS so that a distortion criteria defined by  is satisfied.

The total cost for item  is the sum of the update and download costs: . Therefore, is


Then the total cost for all the items is

We now compute some bounds on  based upon the allowed distortion.

Recall that   is the set of caches,  denotes the probability that a specific cache contains item ,  represents the state of an item  at a node , which is  if cache  contains item , and  otherwise, and  denotes the corresponding state perceived by the CRS. A request from a user is not served internally (by a cache in second layer) either if no cache contains it 
or if there are some caches containing it but the CRS is not aware of that.

where  is the probability that  exists in cache  and the CRS does not know about it.

Thus the probability that a request is served externally is  which equals

where under the independent cache assumption, the state of an item in a cache is independent of the state in another cache. The probability  is always less than the probability of  being in cache  (). If the state updates are done at rate greater than , it will be less than . Hence, denoting , 




Fig. \ref{fig:lruFixLcNc} illustrates the changes of update and total cost when the size of each cache is limited to . The request rate received by each cache is inversely proportional  (the request rate per user is assumed to be fixed and independent of ), and the update packet length increases logarithmically with the number of caches. The total update rate per cache is almost linearly decreasing with , hence the total update rate will almost be stable when  varies (changes are in the order of ). Increasing , however, increases the probability of an item being served internally and thus decreases the download, and consequently the total cost.
\begin{figure}[http]
    \center
      \includegraphics[scale=0.18,angle=0]{lruFixLcNc}\\
      \caption{\textit{Scenario , Total update cost () and Total Cost (lower  and upper bounds ), when the storage size per cache is fixed (), vs. the number of caches ().}}
        \label{fig:lruFixLcNc}
\includegraphics[scale=0.18,angle=0]{lruFixNcLc}\\
      \caption{\textit{Scenario , Total update cost () and Total Cost (lower  and upper bounds ),when the number of caches is fixed (), vs. the size of caches ().}}
    \label{fig:lruFixNcLc}
\end{figure}

In Fig. \ref{fig:lruFixNcLc}, we fix the number of caches in the AS () and study the effects of cache storage size on the update and total cost. Increasing the cache size simply increases the probability of an item being served internally and decreases the download cost. However, the update cost shows more complicated behavior when changing the storage size. Looking at each cache, very small cache size leads to very large durations where that item is not in that cache and consequently, the update rate would be low. Increasing the storage size will increase the probability of that item being in the cache, and thus increases the update rate. This increase will reach its highest value for a certain value of cache size.
For larger values of cache beyond a threshold, the item is in the cache most of the time. Therefore, we need less updates and increasing the cache size will increase the duration of the item being in the cache leading to lower update messages. Since the total cost mostly depends on the download cost, by increasing the cache size, this value reaches its minimum value.










\subsection{Optimized Cache Management}
\label{subsec:optimumi}

We now turn our attention to minimizing the total cost for given  and .


 Under a Zipf popularity distribution, many rare items will not be requested again while they are in the cache under the LRU policy. We can rewrite the total cost if the caches only keep the items with popularity from 1 up to . 




Now just  different pieces of content may be stored in each cache, so probability of an item  being in a cache () is changed, which in turn changes  and .

Fig. \ref{fig:optIstar} demonstrates the total cost versus the caching popularity threshold , for different number and size of content stores, and acceptable distortions.

If just a very small number of items (small ) are kept inside cache layer, then the download cost for those which are not allowed to be inside caches will be the dominant factor in the total cost and will increase it. On the other hand, if a lot of popularity classes are allowed to be kept internally, then the update rate is increased and also the probability of the most popular items being served internally decreases, so the total cost will increase. There is some optimum caching popularity threshold where the total cost is minimized. This optimum threshold is a function of the number and size of the stores, distortion criteria, per bit cost of downloads and updates.

\begin{figure}[http]
    \center
      \includegraphics[scale=0.18,angle=0]{optIstar1}\\
      \includegraphics[scale=0.18,angle=0]{optIstar2}\\
      \caption{\textit{Update, Download, and Total cost when only the  most popular items are allowed to be stored inside caches ().}}
    \label{fig:optIstar}
\end{figure}

To find the optimal , assume that all the items have the same size () and the per bit costs is fixed for all popularity classes (, , ). We can rearrange equation \ref{eq:phiwthr}:

where  is the total cost if no  cache  exists and all the  requests are served externally; ) corresponds to the benefit of caching (cost reduction due to caching); and  is the caching overhead cost due to the updates.  We need to calculate the value of  such that the cost of caching is dominated by its advantage; i.e. we need to maximize .

This can be done using numerical methods which will lead to a unique  for each network setup (fixed parameters). However, the network characteristics and the request pattern are changing over time, so it seems that it is better to have a mechanism to dynamically optimize the cost by selecting the caching threshold () according to the varying network features.





In such a mechanism, the CRS can keep track of requests and have an estimation of their popularity. For those requests which are served locally the CRS can have an idea of the popularity based on the updates that receives from all the caches; i.e. the longer an item stays in a cache, the more popular it is. It can also take into account the local popularity of the items. The CRS can then dynamically search for the caching threshold which minimizes the total cost by solving equation~\ref{eq:phiministar}. Once the CRS determines which items to keep internally, it will set/reset a flag in each CRRep so that the local cache knows to store or not to store the requested piece of content.




\subsection{Total Cost in the AS-AS Scenario }

The AS-AS interaction to keep each other updated will add an overhead to the total cost calculated in equation \ref{eq:totcost}. So assuming  for the per bit cost of the control packets between the ASs, the total cost is

where  is the length of the control packet sent from  to . Let  denote the probability of a change in item 's status in . This probability equals the probability of an item  change in an  cache while no other caches contains it.

Assuming  number of ASs, the inter-AS control packet length for updating item 's status is .

Fig. \ref{fig:cost2ASvsLc} shows the internal update cost (cache-CRS update cost), external update cost (AS-AS update cost), and total cost versus the storage size for fixed number of caches (). In this Figure we assume , and . Increasing the storage size of each cache increases the probability of an item being in at least a cache in an AS and decreases the rate of updates and the external update cost. For low storage size, the AS-AS update rate will be almost the same as cache-CRS update rate, thus the external update cost will be higher than the internal update cost by a factor proportional to  and . For large enough storage size, the probability of an item being in at least one cache is much more than the probability of it being in a specific cache, so the AS-AS update rate will be lower than the cache-CRS update rate and the corresponding costs are being closer to each other.

\begin{figure}[http]
    \center
      \includegraphics[scale=0.18,angle=0]{2ASFixNc}\\
      \caption{\textit{Scenario , (a) Total Cost, (b) Total update cost (intra-AS and inter-AS) vs. the size of caches (Lc).}}
    \label{fig:cost2ASvsLc}
\end{figure}





\section{Conclusions}
\label{sec:conclusion}

We formulated a distortion-based protocol overhead model for ICNs. Some simple content distribution networks were then considered as examples to show how this framework can be used. We calculated the overhead of keeping the control plane informed about the state of the content. We also studied the total cost of data retrieval and observe that with limited cache storage sizes, allowing all the items to have the opportunity to be stored inside the network's caches is not efficient. For the case with a central resolution system in each sub-network and with LRU cache replacement policy, an algorithm has been proposed that can dynamically determine which items should be cached inside the AS at any time such that the total cost of data retrieval is minimized.

\bibliographystyle{abbrv}
\bibliography{acmicn14}

\appendix
{\textbf{Proof of Lemma~\ref{lem:01}: }}The distortion criteria is defined as  and . It can be seen that

There are three cases where a zero update rate can satisfy the distortion criteria.\\
 1) If , then keeping  constantly equal to  will result in  and .\\
 2) If , then keeping  constantly equal to  will result in  and .\\
 3) If , then we can find some probability , such that assigning '1' to  randomly  and independently of  results in , and .

Thus in the following we concentrate on the cases where , , and . Note that we assume that , then , and the first two regions can be summarized in the region where .

Let  and   denote the ratio of updated type I and II changes to the total number of corresponding changes, respectively, such that the distortion criteria is satisfied. The false negative alarm is generated during the  'up' period () if a type I change in the state of  at time  is not announced to the control plane while the previous state ('0') was correctly perceived by the control plane; we show this event by , and its probability is given by


In this case  during the time where . So assuming that the  such change is perceived wrong by the control plane,  is the time interval where the control plane has the type I wrong information about the state of . Thus, the probability of type I error, and consequently type I distortion can be calculated as the ratio of total time of type I error over some time interval  when .


Similarly, a false positive alarm is generated when a type II change is not announced while the previous perceived state ('1') was correct, and assuming that this is the  such change,  is the time interval that the control plane has type II wrong information about ; let  denote this event. Thus,


To satisfy the distortion criteria we need  and . The update rates  and  then can be written as


Thus, the total number of updates announced to the control plane divided by the total number of changes is given by .

Note that the total rate of type I changes, which is equal to the rate of type II changes in average is given by  changes per second, thus total number of updates per second is given by


Combining equations \ref{eq:U1}-\ref{eq:RX} the Lemma is proved.


\end{document}
