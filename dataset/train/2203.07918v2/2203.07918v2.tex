

\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}              

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{bm}
\usepackage{soul}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{tabularx}
\usepackage[accsupp]{axessibility}  \newcommand\todo[1]{\textbf{TODO: }{\color{red}{#1}}}
\usepackage{color}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\renewcommand{\ll}{\mathcal{L}}

\newlength\savewidth\newcommand\shline{\noalign{\global\savewidth\arrayrulewidth
  \global\arrayrulewidth 1pt}\hline\noalign{\global\arrayrulewidth\savewidth}}
\newcommand{\tablestyle}[2]{\setlength{\tabcolsep}{#1}\renewcommand{\arraystretch}{#2}\centering\footnotesize}

\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}

\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


\def\cvprPaperID{3841} \def\confName{CVPR}
\def\confYear{2022}
\renewcommand\footnotemark{}


\begin{document}

\title{GPV-Pose: Category-level Object Pose Estimation via Geometry-guided Point-wise Voting}

\author{Yan Di, Ruida Zhang, Zhiqiang Lou, Fabian Manhardt,\\
Xiangyang Ji, Nassir Navab and Federico Tombari\\
\textsuperscript{1}Technical University of Munich, \textsuperscript{2}Tsinghua University, \textsuperscript{3}Google\\
\tt\small{\{\textsuperscript{*}zhangrd21@mails. lzq20@mails. xyji@\}tsinghua.edu.cn},
\tt\small{shangbuhuan13@gmail.com}
\\
\tt\small{fabianmanhardt@google.com}, \tt\small{tombari@in.tum.de}
\thanks{\textsuperscript{*}Authors with equal contributions.}
\thanks{Codes: \url{https://github.com/lolrudy/GPV_Pose}}
}
\maketitle

\begin{abstract}
While 6D object pose estimation has recently made a huge leap forward, most methods can still only handle a single or a handful of different objects, which limits their applications. 
To circumvent this problem, category-level object pose estimation has recently been revamped, which aims at predicting the 6D pose as well as the 3D metric size for previously unseen instances from a given set of object classes.
This is, however, a much more challenging task due to severe intra-class shape variations.
To address this issue, we propose GPV-Pose, a novel framework for robust category-level pose estimation, harnessing geometric insights to enhance the learning of category-level pose-sensitive features. 
First, we introduce a decoupled confidence-driven rotation representation, which allows geometry-aware recovery of the associated rotation matrix.
Second, we propose a novel geometry-guided point-wise voting paradigm for robust retrieval of the 3D
object bounding box.
Finally, leveraging these different output streams, we can enforce several geometric consistency terms, further increasing performance, especially for non-symmetric categories.
GPV-Pose produces superior results to state-of-the-art competitors on common public benchmarks, whilst almost achieving real-time inference speed at 20 FPS. 




\iffalse Category-level object pose estimation aims at predicting the 6D pose as well as the metric 3D size of previously unseen objects from a known set of classes.
Compared to common instance-level pose estimation, this is a much more challenging problem due to significant intra-class shape variations and lack of appropriate 3D CAD models.
To address this issue, we propose GPV-Pose, a novel framework for category-level pose estimation, which harnesses three kinds of geometric constraints to enhance the learning of category-level pose-sensitive features. 
First, we decompose the rotation matrix into the plane normals of the object bounding box, and utilize two decoders are employed to predict the disentangled rotation normals along with their confidence for geometry-aware rotation matrix recovery.
Second, we introduce a geometry-guided point-wise voting strategy for the object bounding box prediction.
Finally, two parallel streams of geometric information, Point Cloud - Pose, and Point Cloud - Bounding Box - Pose, are naturally exploited as geometric consistency terms to further boost our performance especially for non-symmetric categories.
GPV-Pose produces superior performance to state-of-the-art methods on common public benchmarks, whilst almost achieving real-time inference speed at 20 FPS. Code and trained models will be be made publicly available.
\fi




\end{abstract}

\section{Introduction}

\begin{figure}[t]
  \centering
  \includegraphics[width=0.99\linewidth]{pic/teaser_name.pdf}
  \caption{\textbf{GPV-Pose} consists of three individual branches for pose estimation, symmetry-aware reconstruction and point-wise bounding box voting.
  To enhance the learning of pose-sensitive features, we make use of two different types of geometric relations, \ie \textbf{Point Cloud - Pose} (\textbf{PP}) and \textbf{Point Cloud - Bounding Box - Pose} (\textbf{PBP}). 
  In the \textbf{PP} stream, we explicitly analyze how to directly retrieve pose from the point cloud, while in the \textbf{PBP} stream, we first introduce a geometry-guided point-wise bounding box voting approach and then establish geometric correspondences  between pose and the bounding box.
  }
  \label{teaser}
\end{figure}

Being able to reliably estimate the 6D object pose, \ie its 3D rotation and translation, is a fundamental problem in computer vision, as it enables a wide range of applications within AR\&VR~\cite{arvr}, robotics~\cite{robotics} and 3D understanding~\cite{nie2020total3dunderstanding, zhang2021holistic}. 
Hence, a lot of research has recently been devoted to the domain of pose estimation, producing methods that can reliably estimate the pose at real-time, even under severe occlusion~\cite{hybridpose,densefusion,GDRN,sopose}. 
Nevertheless, the majority of these methods can only deal with a few objects, in fact, sometimes even just a single instance at a time~\cite{labbe2020cosypose,Kehl2017}.
In addition, a high-quality CAD model is usually required during training and/or inference~\cite{Kehl2017,zakharov2019dpod}, which clearly limits the use of such methods in real applications. 
To deal with this issue, category-level pose estimation attempts to go beyond the instance-level scenario and estimate the pose together with the object's scale for previously unseen objects from known classes~\cite{NOCS,cass}. 
The category-level task is inherently more challenging due to the lack of respective CAD models as well as the large intra-class variations among different objects. 




Despite category-level pose estimation being a well-established field~\cite{lopez2011deformable,ozuysal2009pose, savarese20073d}, it has very recently started to enjoy again increasing popularity, thanks to a new line of works based on deep learning~\cite{NOCS,cass,shape_deform}. 
Interestingly, most of these works resort to a learned or manually-designed canonical object space to recover the pose~\cite{NOCS, cass} and, in parts, additionally leverage point-cloud based shape priors~\cite{shape_deform,sgpa} or pose consistency terms~\cite{dualposenet} to better deal with intra-class shape variations. Nonetheless, despite these methods achieving remarkable improvements on the benchmarks, their performance is still far from satisfactory due to their insufficient ability to extract pose-sensitive features, as they do not explicitly harness the geometric relationships between pose and point cloud.


In this paper, we propose GPV-Pose, a novel category-level pose estimation framework that leverages geometric constraints to enhance the learning of intra-class object shape characteristics.
GPV-Pose stacks three individual branches on top of a 3D graph convolution (3DGC) based encoder~\cite{3DGC, fs-net} for direct pose regression, symmetry-aware reconstruction and point-wise bounding box voting. 
In particular, we introduce a novel decoupled confidence-driven rotation representation, in which the rotation matrix is decomposed as the plane normals of the object bounding box.
We demonstrate that our estimated confidence enables closed-form recovery of the 3D rotation and is capable of capturing geometric characteristics of each class. Further, besides enhancing the feature quality by means of symmetry-aware reconstruction, we additionally propose a novel \textbf{G}eometry-guided \textbf{P}oint-wise \textbf{V}oting paradigm (GPV), enabling the robust recovery of the object's 3D bounding box.
As constituted in Fig.~\ref{teaser}, leveraging these three individual estimates, two streams of geometric relationships, \ie \textbf{Point Cloud - Pose} (\textbf{PP}) and \textbf{Point Cloud - Bounding Box - Pose} (\textbf{PBP}), can be exploited to serve as geometric consistency terms, further improving the learning of category-level shape characteristics and in turn enhancing the overall performance.


In summary, our main contributions are as follows:
\begin{itemize}
\setlength{\itemsep}{0pt}
\setlength{\parsep}{0pt}
\setlength{\parskip}{0pt}
\item 
We propose a novel geometry-guided category-level pose estimation framework GPV-Pose, which consists of three respective branches for direct pose regression, symmetry-aware reconstruction, and point-wise bounding box voting, giving superior results on common public benchmarks at a high framerate of 20 FPS.
\item 
To enhance the learning of intra-class shape characteristics, we introduce confidence-aware predictions of rotation and the object bounding box, and two parallel streams of geometric constraints, \textbf{PP} and \textbf{PBP}, are naturally converted into geometric consistency terms.
\item 
We propose a novel point-wise object bounding box voting mechanism that aggregates direction, distance and confidence predictions of all points with the confidence-weighted least square algorithm.
\end{itemize} \section{Related Works}
\textbf{Instance-level 6D Pose Estimation.}
Instance-level pose estimation describes the task of estimating 6D pose for known object instances. 
For monocular methods, there are three different lines of works. 
Whereas a few methods directly estimate the pose~\cite{Kehl2017, xiang2017posecnn, manhardt2018deep, manhardt2019explaining, li2019deepim, labbe2020cosypose}, other approaches either learn a latent embedding for latter pose retrieval~\cite{Sundermeyer_2018_ECCV,sundermeyer2020multi} or predict 2D-3D correspondence and use a variant of the P\textit{n}P paradigm to obtain the final pose~\cite{li2019cdpn,hybridpose,peng2019pvnet,zakharov2019dpod,hodan2020epos,park2019pix2pose}. 
Interestingly, a handful methods combine direct regression with correspondence-driven methods, in particular, they first estimate 2D-3D correspondences and then make use of another network to learn the P\textit{n}P step~\cite{hu2020single, sopose, GDRN}.
A similar division can be also made for RGB-D methods. While a few methods again learn a latent embedding for retrieval purposes~\cite{wohlhart2015learning, Kehl2016a}, most methods aim at directly estimating the final 6D object pose~\cite{6drgbd, wang2019densefusion, he2020pvn3d, FFB6D}. The main difference lies thereby in the way how the two different modalities are fused together. 

Despite great progress in the recent years, instance-level methods can typically only deal with a single or a handful of objects and require an object CAD model for training and testing, limiting its use in practical applications such as autonomous driving.


\textbf{Category-level Pose Estimation.}
Category-level approaches aim at predicting the pose of previously unseen objects~\cite{manhardt2020cps,NOCS}.
Exemplary, Sahin \etal~\cite{9d1} introduce \textit{"Intrinsic Structure Adaptor"} to adapt the distribution shifts arising from shape discrepancies.
Wang~\etal~\cite{NOCS} derive a \textit{Normalized Object Coordinate Space} (NOCS) and recover pose using the Umeyama's algorithm~\cite{umeyama1991least}, while CASS~\cite{cass} introduces a learned canonical shape space.
6D-PACK~\cite{6dpack} computes the pose by means of tracking.
To alleviate the influence of intra-class shape variations, a few methods incorporate point-based object shape priors~\cite{shape_deform, sgpa, donet}.
DualPoseNet~\cite{dualposenet} instead utilizes a dual network for explicit and implicit pose prediction, and introduces a pose refinement strategy by means of pose consistency within both branches.
While FS-Net~\cite{fs-net} proposes to represent rotation using decoupled vectors, DO-Net~\cite{donet} makes use of symmetry for pose optimization.
Additionally, SGPA~\cite{sgpa} adopt visual transformers~\cite{transformer} for pose estimation.
Interestingly, a handful of methods have recently started to investigate pose estimation for articulated objects~\cite{aticategory, captra}.
Noteworthy, none of the aforementioned works harness different geometric cues to strengthen the model's prediction capabilities, which in turns mitigates the overall performance. 

On the contrary, in this paper we propose a novel geometry-guided category-level pose estimation framework, leveraging two different streams of geometric information based on the sampled point cloud and the object bounding box, which we refer to as \textbf{PP} and \textbf{PBP}.

\iffalse
\textbf{Category-level 3D bounding box detection.}
Category-level 3D bounding box detection mainly develops in two streams.
Methods of the first stream~\cite{bbox1, bbox2} mainly leverage off-the-shelf 2D object detector to first segment the objects in RGB images and then regress the bounding box based on features extracted in the object frustums.
The other branch~\cite{bb3, bb4,bb5, bb6} directly predicts the 7DoF bounding box from point observations.
Recently, RGB and point cloud based holistic 3D understanding methods~\cite{posegraph1, posegraph2, posegraph3} arouse wide research interest, which first predict per-object bounding box and then analyze their geometric relations.
In this paper, we propose a novel point-wise voting based bounding box predict method, which enables geometric analysis between pose and the object bounding box.
\fi
 \begin{figure*}[h]
  \centering
  \includegraphics[width=0.99\linewidth]{pic/pipeline_final_crop.pdf}
  \vspace{-0.3cm}
  \caption{\textbf{Schematic overview of our architecture for GPV-Pose.} 
  We first employ an off-the-shelf object detector (\eg Mask-RCNN~\cite{maskrcnn}) to segment the object of interest from the depth map, and sample 1028 points from the backprojected depth map as input to GPV-Pose (a-d).
  We then extract global and per-point features with the help of 3DGC~\cite{3DGC}, processed with three respective branches.
  The first branch (yellow) outputs confidence-aware rotation , residual translation  and size  parameters, from which we recover the pose  in closed form.
  Moreover, while the second branch (green) reconstructs the input in a symmetry-aware manner (f), the last branch (blue) conducts point-wise bounding box voting.
  For each point in the point cloud (g), we predict its direction, distance and confidence towards each bounding box face, moving the points to the respective bounding box faces (h).
  By employing confidence-weighted least squares we can recover the plane parameters (i). Both branches are then further leveraged to serve as additional geometric guidance during optimization.
  Note that, at inference, we only need the pose estimation part, with benefits in efficiency.
  }
  \label{pipeline}
\end{figure*}


\section{GPV-Pose}
Given a RGB-D image, we first employ an off-the-shelf object detector (\eg Mask-RCNN~\cite{maskrcnn}) to segment the object of interest from the depth map.
We then sample 1028 points from the backprojected 3D point cloud and feed them as input to our proposed GPV-Pose network.
Since 3DGC~\cite{3DGC} is insensitive to shift and scale of the given point cloud, we employ 3DGC as the backbone to extract global and per-point features.
We further attach three parallel branches for direct confidence-aware pose prediction, symmetry-aware point cloud reconstruction and point-wise bounding box voting.
Leveraging these different outputs, we can naturally enforce several geometric constraints, leading to superior performance. A schematic overview is provided in Fig~\ref{pipeline}.
In the following subsections, we will explain in detail each branch as well as our geometric constraints. 
Note that throughout this paper we use  to refer to .

\subsection{Confidence-aware Pose Regression}
Recent works~\cite{zhou2019continuity, fs-net} have shown that predicting the rotation in the form of bounding box plane normals, \ie equivalent to considering two columns of the rotation matrix, can benefit learning as it resolves discontinuities in SO(3).
Hence following these works, as shown in Fig.~\ref{pipeline} (e), we also predict rotation as two plane normals of the 3D bounding box.
However, since certain normals are naturally easier to recover, we additionally estimate a confidence value for each normal in an effort to increase robustness when recovering the final 3D rotation.
As an example, imagine predicting the rotation for a \textit{laptop}, as one plane normal is typically perpendicular to the keyboard surface, it is thus an easier target to compute than the remaining one.
Therefore, this normal should receive higher confidence to, in turn, stabilize rotation recovery.
Overall, we want to enforce that a higher confidence is accompanied by a more accurate rotation normal prediction. 
Thus, we define,

where  is a constant,  is the corresponding ground truth plane normal and
 denotes the -loss.


As shown in Fig.~\ref{voteandR} (b), given predicted plane normals , , and their confidence , , we minimize the following cost function to calibrate the plane normals to be perpendicular normals  and ,

where  denotes the angle between  and ,
From Eq.~\ref{minimum} we then obtain


\noindent 
The calibrated plane normals  and  can be calculated from  using the Rodrigues Rotation Formula.
Eventually, the rotation matrix is obtained as .
As for the translation , provided the predicted residual translation  and the mean of the input point cloud , we compute .
Similarly, given the estimated residual size  and the pre-computed category mean size , we obtain the scale .
Please refer to the supplementary material for all derivations. 


\subsection{Symmetry Analysis}
We utilize two types of symmetries to extract more effective per-point features: \textbf{Reflection Symmetry} and \textbf{Rotational Symmetry} following~\cite{donet}.
For reflection symmetry categories (\textit{mug}, \textit{laptop}), we predict the corresponding point cloud  of the input point cloud  w.r.t. the reflection plane, while for rotational symmetry categories (\textit{can, bowl, bottle}), we predict  that is symmetrical to   w.r.t the symmetry-axis, otherwise we directly reconstruct  with .
The symmetry-aware reconstruction is supervised by the following loss term

where  depends on the symmetry type.
Due to space limit, please refer to the supplementary material and~\cite{donet} for detailed derivations.




\subsection{Point-wise Voting for Bounding Box}
\label{BBoxVote}
Another major contribution of GPV-Pose resides in our novel confidence-weighted point-wise voting strategy for robust bounding box prediction.
To this end, for each observed point , we predict its direction , distance  and confidence  towards each of the six bounding box faces with  and , as shown in Fig.~\ref{voteandR} (a).
We then compute the bounding box of the observed object using weighted averaging of all spatial cues.
As depicted in Fig.~\ref{voteandR} (a), consider the top bounding box face , we obtain the corresponding point  on the top face  for  as

\noindent The plane parameters, described as the normal  and origin-to-plane distance  for , can be estimated using weighted least squares, where the weight of point  amounts to its confidence . 
All other faces in  follow the same calculations.


Since we can easily obtain the ground truth vote for each predicted value , we directly supervise it by means of the  loss.
As for confidence , similar to Eq.~\ref{basic_c}, we define the following loss term

where  is a constant hyper-parameter and  denotes the ground truth distance of  to the face  in .
Considering now again the top face , we accordingly obtain 

with  being the size along  direction, and  and  referring to the ground truth rotation and translation.


Noteworthy, the contributions of our confidence-weighted bounding box voting are two-fold. 
First, it enforces geometric consistency terms within the \textbf{PBP} stream. Secondly, it also enhances the learning of intra-category geometric features as experimentally proven in the evaluation section.
In general, the closer a point is to a bounding box plane, the more confidence it should have to accurately vote for the bounding box plane, as it is less affected by intra-class variations.
As a consequence, these geometric guidance enforces the learning of global (green) and per-point (blue) features in Fig.~\ref{pipeline} to capture more specific category-level traits.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.99\linewidth]{pic/outlier_rm_bin.pdf}
  \caption{Illustration of how we predict  based on the point cloud only.
  Given all points projected along the y positive axis, we aim to approximate the ideal distribution of size  with combined exponential functions in Eq.~\ref{fs}, as shown in (b).
  }
  \label{outlier}
\end{figure}

\subsection{Point Cloud - Pose Geometric Consistency}
\label{PPGC}
While most category-level methods~\cite{fs-net, dualposenet, sgpa} predict the pose from uniformly sampled point clouds, they do not consider the geometric relation between pose and the point cloud explicitly. Instead they resort to individual loss terms for each pose parameter to learn such geometric relations implicitly~\cite{fs-net,dualposenet,sgpa}. 
On another note, it is not difficult to deduce that, when transforming the point cloud to the canonical view via the inverse of  and removing all outliers, the size  can be simply computed as the distance between the outermost points along each axis.
In this paper, we turn the above transformation into supervision terms to explicitly enforce these geometric relations and, thus, enhance performance.

To this end, we first transform the point cloud from the camera view to a predefined canonical view. Similar to the point matching loss used in instance-level pose prediction~\cite{GDRN, sopose, li2019deepim}, we then minimize

with  being the sampled point cloud and  being the corresponding ground truth of  under the canonical view.

Next, estimating the correct metric scale can be generally considered as the task of finding the 3D bounding box which encapsulates all object points whilst having minimal volume. 
Unfortunately, this is only true as long as we do not observe any outliers. 
Since this is rarely the case when dealing with real data, we instead relax this constraint and try to enforce having as \textit{many} points as possible in the bounding box of minimal volume. 
Given sampled points with a few outliers, we first conduct bin sampling to ensure that the points follow an approximate uniform distribution.
To this end, we segment the points into 64 bins. Further, whenever one bin overflows we uniformly sub-sample a fixed number of points. The final obtained point set is denoted as . This process is also demonstrated in Fig.~\ref{outlier} (b).
Then given an estimate of , its probability to be the expected size is calculated as follows,




where  are constant hyper-parameters.
Further,  is a normalization parameter and  if , otherwise .
From this, our geometric loss term for the scale  can be derived as

where .
Note that this term is only meaningful when the corresponding bounding box face in  is visible from the given viewpoint, hence, we turn off  for occluded faces.

To summarize, our geometry-aware loss terms between the estimated pose and the corresponding object point cloud are defined as

where  denote the balancing weights of the two terms.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.9\linewidth]{pic/vote_final.pdf}
  \caption{Confidence-aware predictions of the bounding box (a) and rotation (b).
  In (a), for each point, we predict its direction , distance  and confidence  to each of the six faces, \eg for face , we predict the  of each point.
  In (b), we calibrate the predicted rotation normals  to be perpendicular normals  by minimizing  Eq.~\ref{minimum}.
   }
  \label{voteandR}
\end{figure}

\subsection{Bounding Box - Pose Geometric Consistency}
\label{BPGC}
From Sec.~\ref{BBoxVote}, we take  as the center and \{length, width, height\} of the bounding box from the point-wise voting.
Further, as aforementioned, we decompose the rotation  into its 3 columns , yet, only predict  and  together with the associated confidence, since they already fully describe the 3D rotation, \ie .
We have also shown that  and  correspond to the plane normals of the bounding box.
Thus given the estimated six bounding box faces , , we can define the following \textbf{Bounding box - Pose} consistencies
 

The overall geometric consistency between pose and the estimated bounding box is then computed as the weighted sum of these individual terms



\subsection{Overall Training Objective}
In summary, GPV-Pose employs the following loss function

where  contains all loss terms for fully supervising the learning of pose, rotation confidence, symmetrical reconstruction and point-wise bounding box voting.
Further,  denote our geometric consistency terms as described in Sec.~\ref{PPGC} and Sec.~\ref{BPGC}, respectively.


 \begin{table*}[ht]
\centering
\begin{tabular}{c|c|ccc|cccc|c}
\shline
Method & P.E. Setting &  &  &  &  & & &  & Speed(FPS)\\
\hline
NOCS~\cite{NOCS} & RGB-D & \textbf{84.9} & 80.5 & 30.1 & 7.2 & 10.0 & 25.2 & 26.7 & 5 \\
CASS~\cite{cass} & RGB-D & 84.2 & 77.7 & - & - & 23.5 & 58.0 & 58.3 & - \\
SPD~\cite{shape_deform} & RGB-D & 83.4 & 77.3 & 53.2 & 19.3 & 21.4 & 54.1 & - & 4 \\
CR-Net~\cite{cr-net} & RGB-D & - & 79.3 & 55.9 & 27.8 & 34.3 & 60.8 & - & - \\
SGPA~\cite{sgpa} & RGB-D & - & 80.1 & 61.9 & \textbf{35.9} & 39.6 & 70.7 & - & - \\
DualPoseNet~\cite{dualposenet} & RGB-D & - & 79.8 & 62.2  & 29.3 & 35.9 & 66.8 & - & 2 \\
\hline
DO-Net~\cite{donet} & D & - & 80.4 & \underline{63.7} & 24.1 & 34.8 & 67.4 & - & 10 \\
FS-Net~\cite{fs-net} & D & -& - & - & - & 28.2 & 60.8 & 64.6 & 20
\\
\hline
Ours(FS-Net) & D & &  &  &  & 33.9 & 69.1 & 71.0 & 20 \\
Ours(M) & D & 84.1 &  \underline{82.0} &  63.2 & 30.6 & \textbf{44.2} & \underline{72.1} & \underline{73.8} & 20 \\
Ours & D & \underline{84.2} &  \textbf{83.0} &  \textbf{64.4} & \underline{32.0} & \underline{42.9} & \textbf{73.3} & \textbf{74.6} & \textbf{20} \\
\shline
\end{tabular}
\vspace{-0.3cm}
\caption{\textbf{Comparison with state-of-the-art methods on REAL275 dataset.} Overall best results are in bold and the second best results are underlined.  
\textbf{P.E. Setting} lists the input data type for pose estimation.
Since FS-Net uses different detection results under 3DIoU, we reimplement it as \textbf{Ours(FS-Net)}.
Here \textbf{Ours(FS-Net)} inherits all loss terms of FS-Net but uses our pose decoder for fair comparison.
}
\label{tab_real275}
\end{table*}

\section{Experiments}
\textbf{Datasets.}
We evaluate GPV-Pose on the synthetic CAMERA25 and real REAL275~\cite{NOCS} benchmarks for category-level pose estimation and the real LineMod~\cite{Hinterstoisser2012a} dataset for instance-level pose estimation.
CAMERA25 contains 300K synthetic RGB-D images with rendered objects on top of virtual backgrounds, among which 25K images are withhold for testing.
The objects cover 6 categories, \ie \textit{bottle, bowl, camera, can, laptop} and \textit{mug}.
REAL275 is a more challenging real-world dataset with 13 different scenes. Thereby, 7 scenes with 4.3k images are provided for training, while the remaining 6 scenes with 2.7k images are employed for testing. 
REAL275 possesses the same categories as CAMERA25.
LineMod~\cite{Hinterstoisser2012a} is a widely-used instance-level 6D pose estimation benchmark, consisting of individual sequences with 13 objects undergoing mild occlusion.
We follow \cite{GDRN,li2019cdpn} and employ 15\% of the RGB-D images for training and the utilize the rest for testing.

\textbf{Implementation Details.}
We train our method purely on real data and follow~\cite{shape_deform,sgpa} to generate instance segmentation masks with an off-the-shelf object detector, \ie Mask-RCNN~\cite{maskrcnn}.
We uniformly sample 1028 points from the back-projected object depth map and feed them as input to GPV-Pose.
We employ several strategies for data augmentation including random scaling, random uniform noise, random rotational and translational perturbations, and bounding box based adjustment similar to FS-Net~\cite{fs-net}.
The parameters for all loss terms are kept unchanged during experimentation unless specified, with .
We further set ,  and keep  fixed at 1.0, assuming an approximate point number of about 300 after bin sampling.
We employ the Ranger optimizer~\cite{ranger1,ranger2,ranger3} and run all our experiments on a single TITAN X GPU with batch size of 24 and base learning rate of 1e-4.
The learning rate is annealed at  of the training phase using a cosine schedule.
The total training epoch is set to 70 for \textbf{Ours(M)}, in which we train a separate model for each category and 150 for \textbf{Ours} in which we train a single model for all categories.

\textbf{Evaluation Metrics.}
We follow~\cite{NOCS, sgpa, dualposenet} and report the mean precision of 3D intersection over union (IoU) at thresholds of 25, 50, 75 to jointly evaluate rotation, translation and size.
To directly compare errors in rotation and translation, we also adopt the , , ,  metrics. 
A pose is thereby considered correct if the translation and rotation errors are both below the given thresholds.
For instance-level pose estimation task on LineMOD, we report the commonly employed ADD(-S) metric.

\subsection{Comparison with State-of-the-Art Methods}

\begin{table*}[t]
\centering
\scalebox{0.95}{
\begin{tabular}{c|ccc|c|ccc|cc|c|ccc}
\shline
\multirow{3}{0.3cm}{}&\multicolumn{3}{c|}{Network} & \multicolumn{6}{c|}{Loss Terms} & \multirow{3}{*}{} & \multirow{3}{*}{} & \multirow{3}{*}{} & \multirow{3}{*}{} \\
\cline{2-10}
& Conf. & B.Box & Symm. &\multirow{2}{*}{} & \multicolumn{3}{c|}{} & \multicolumn{2}{c|}{} & &  & & \\
\cline{6-10}
& Rot. & Voting & Recon. & & &  & \multicolumn{1}{c|}{} & &  & & & & \\
\hline
&  &  &  & \checkmark & & & & & & 52.0 & 19.9 & 33.9 & 69.1 \\
& \checkmark &  &  & \checkmark & & & & & & 56.9 & 22.7 & 35.7 & 70.0 \\
& \checkmark &  & \checkmark & \checkmark & & & & & & 60.4 & 25.3 & 37.0 & 70.6 \\
\hline
& \checkmark & \checkmark & \checkmark & \checkmark & \checkmark& & & & & 60.7 & 28.8 & 39.6 & 73.1 \\
& \checkmark & \checkmark & \checkmark & \checkmark & &\checkmark & & & & 62.0 & 29.1 & 38.4 & 73.0 \\
& \checkmark & \checkmark & \checkmark & \checkmark & & & \checkmark& & & 61.0 & 26.3 & 37.7 & 73.1 \\
& \checkmark & \checkmark &  \checkmark& \checkmark &\checkmark & \checkmark& \checkmark& & & 63.2 & 29.9 & 39.7 & \textbf{73.3} \\
\hline
& \checkmark &  & \checkmark & \checkmark & & & & \checkmark& & 61.5 & 27.4 & 40.3 & 73.0 \\
& \checkmark &  & \checkmark & \checkmark & & & & \checkmark& \checkmark& 61.7 & 27.6 & 41.0 & 73.0 \\
\hline
& \checkmark & \checkmark & \checkmark & \checkmark & \checkmark& \checkmark& \checkmark&\checkmark &\checkmark & \textbf{64.4} & \textbf{32.0} & \textbf{42.9} & \textbf{73.3}\\
\shline
\end{tabular} }
\vspace{-0.3cm}
\caption{\textbf{Ablation studies on different configurations of network architectures and loss terms on REAL275 datasets.}
\textbf{Conf. Rot.} refers to confidence-aware rotation representation. 
Without this term, we follow FS-Net to recover rotation matrix with SVD.
\textbf{B.Box Voting} refers to point-wise bounding box voting.
\textbf{Symm. Recon.} is symmetry-aware reconstruction.
Without this term, we directly reconstruct the input point cloud and  is replaced by .
Overall best results are in bold.
}
\label{tab-abs}
\end{table*}

\begin{table}[t!]
\centering
\scalebox{0.99}{
\begin{tabular}{c|c|c|c}
\shline
Method & C.L. & ADD-(S) & speed(FPS) \\
\hline
PVNet~\cite{peng2019pvnet}  &  & 86.3 & 25 \\
G2L-Net~\cite{g2lnet}  &  & 98.7 & 23 \\
DenseFusion~\cite{densefusion}  &  & 94.3 & 16 \\
PVN3D~\cite{pvn3d}  &  & \textbf{99.4} & 5 \\
\hline
DualPoseNet~\cite{dualposenet}  & \checkmark & \textbf{98.2} & 2 \\
FS-Net~\cite{fs-net}  & \checkmark & 97.6 & 20 \\
Ours & \checkmark & \textbf{98.2} & 20 \\
\shline 
\end{tabular} }
\vspace{-0.3cm}
\caption{\textbf{Performance on LineMod.} 
We compare our method with competitors on the instance-level 6D pose estimation task on LineMod.
\textbf{C.L.} refers to category-level method.}
\label{tab-lm}
\end{table}



\textbf{Overall Performance on REAL275.}
In Tab.~\ref{tab_real275}, we compare GPV-Pose with state-of-the-art competitors on REAL275~\cite{NOCS}. 
In line with our work, also DO-Net~\cite{donet} and FS-Net~\cite{fs-net} both use only depth observations for pose estimation, whereas all other methods rely on RGB-D data during pose inference.
Notice that we provide three variants of our method.
\textbf{Ours(FS-Net)} inherits the loss terms of FS-Net but uses our pose estimation network architecture and serves as the baseline for our method.
\textbf{Ours(M)} trains a separate model for each category, while \textbf{Ours} trains a single model for all categories.
From Tab.~\ref{tab_real275}, it can be deduced that GPV-Pose achieves state-of-the-art performance w.r.t 5 out of total 7 metrics.
Specifically, for , GPV-Pose outperforms the previous best method NOCS with 83.0 \textit{vs.} 80.5. Furthermore, for the  metric, we surpass SGPA~\cite{sgpa} with 42.9 compared to 39.6, which increases by .
Interestingly, the best performance for  is achieved by \textbf{Ours(M)} with 44.2.
In terms of , our method is inferior to SGPA~\cite{sgpa}.
The main reason may be that SGPA employs a point cloud based shape prior, while we only rely on the mean size of each category as prior.
As for our baseline method \textbf{Ours(FS-Net)}, we outperform it by a large margin w.r.t all metrics, clearly proving the effectiveness of our proposed contributions.
In Fig.~\ref{PDC_line}, we additionally present a detailed per-category comparison of our method with DualPoseNet~\cite{dualposenet}.
As one can easily deduce, we outperform DualPoseNet by a large margin, especially when viewing the 3D rotation results for non-symmetrical objects such as \textit{camera}. 
Moreover, Fig.~\ref{res_big} presents a qualitative comparison with DualPoseNet on REAL275.
GPV-Pose is capable of predicting accurate rotation and translation estimations, even when the objects are only partially detected, as shown in Fig.~\ref{res_big} (a) \textit{camera}, 
while DualPoseNet tends to fail on such challenging cases.
In addition, to get a better grasp of our methodology, in Fig.~\ref{res_small}, we demonstrate the bounding box voting results. It can be noted that the computed bounding box faces are accurate and the estimated confidence is reasonable (blue: low - yellow: high). 
Due to space limitations, we present our results on CAMERA25 in the supplementary material. In general, similar to REAL275, we again achieve comparable or superior performance compared to the state-of-the-art.

As speed is a vital factor for many applications, in the last column of Tab.~\ref{tab_real275}, we show the framerate of each method. 
Regardless of the object detection time, our method can infer at a real-time speed 50 FPS, and is thus very suitable for real applications with time requirements.
When using Yolo-V3~\cite{redmon2018yolov3} + ATSA~\cite{atsa} to extract object instances, the whole pipeline runs at about 20 FPS, faster than most other methods.

\textbf{Performance on Instance-level 6D Pose Estimation.}
We also apply GPV-Pose to the instance-level 6D pose estimation scenario and compare with state-of-the-art instance-level and category-level methods.
As shown in Tab.~\ref{tab-lm}, GPV-Pose achieves a comparable performance under the ADD(-S) metric (98.2\% for GPV-Pose compared to 98.2\% for DualPoseNet and 99.4\% for PVN3D~\cite{pvn3d}), whilst running almost in real time.

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.99\linewidth]{pic/res_big_crop.pdf}
  \caption{Qualitative results of our method (green line) and DualPoseNet (blue line). 
  Image (a)-(d) demonstrate 2D segmentation results.
  }
  \vspace{-0.5cm}
  \label{res_big}
\end{figure*}

\begin{figure}[t]
  \centering
  \includegraphics[width=0.99\linewidth]{pic/res_small_crop.pdf}
  \caption{Demonstration of the point-wise bounding box voting results w.r.t three planes .
  GPV-Pose predicts point-wise directions, distances and confidences, moving the input points to the bounding box planes (the red points). Plane parameters are then calculated via confidence-weighted least squares.
  The color of each point (blue: low, yellow: right) reflects its confidence to support the target plane.
We normalize the confidence for better visualization.
  In general, the closer a point is to the target plane, the higher its voting confidence for the plane.
  }
  \label{res_small}
\end{figure}


\begin{figure}[t]
  \centering
  \includegraphics[width=0.99\linewidth]{pic/curve-crop.pdf}
  \caption{Per-category camparison between our method and DualPoseNet.
  We demonstrate average precision \vs different error thresholds on REAL275. 
  }
  \vspace{-0.3cm}
  \label{PDC_line}
\end{figure}



\subsection{Ablation Studies}


\textbf{Effect of Geometric Terms.} 
In Tab.~\ref{tab-abs} (B) and (C), we evaluate the performance w.r.t. different configurations of network architectures and geometric loss terms.
First, we gradually add each loss term (from  to ) , capturing the geometric relations between pose and the voted bounding box.
As can be observed, when adding these terms, the performance consistently improves from 60.4 to 63.2 referring to , and 25.3 to 29.9 w.r.t , which demonstrates the efficacy of the geometric consistencies.
Second, from  to  we evaluate the effectiveness of our loss terms  and , which are derived from the geometric relationship between pose and point cloud.
As before, the enforced consistency terms help again improve the estimator's performance (\ie we achieve an increase from 60.4 to 61.7 for ).

\textbf{Effect of Confidence-Aware Rotation Prediction.}
When incorporating confidence into the rotation prediction, from  to  in Tab.~\ref{tab-abs}, the overall performance shows a significant leap forward from 52.0 to 56.9 referring to  and 19.9 to 22.7 for .
As aforementioned, for several categories one rotation vector is much easier to predict than the other and thus enables higher accuracy.
Exemplary, for \textit{laptop}, the average confidence for the  normal lies around 0.88, whereas the average confidence for  only amounts to 0.45. 
Since  is typically perpendicular to the keyboard surface, it is a much easier target to estimate.
Therefore, exerting higher weight on  benefits the performance.


 \section{Conclusion}
In this paper, we propose a novel category-level pose estimation network, which we dub GPV-Pose.
GVP-Pose jointly predicts the confidence-driven pose, symmetry-aware reconstruction and point-wise bounding box voting.
Two parallel streams of geometric consistencies, \textbf{Point Cloud - Pose} and \textbf{Point Cloud - Bounding Box - Pose}, are derived and converted into supervision terms to improve the pose accuracy.
GPV-Pose achieves superior performance on public datasets at a fast inference speed of 20 FPS, enabling real-time applications. 
In the future, we plan to incorporate GPV-Pose into holistic 3D understanding via first predicting the pose of each object and, secondly, describing the object interactions by means of a corresponding 3D scene graph. {\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
