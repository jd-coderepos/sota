
\section{Experiments}

\subsection{Dataset}

The training dataset consists of the popular Microsoft COCO benchmark \cite{lin2014microsoft} split according to \cite{karpathy2015deep} resulting in 113.287 training images and 5000 validation images and 5000 testing cases. Each reference caption is pre-processed by a simple pipeline that consists of lowering casing, punctuation removal and filtering out words that do not occur at least 5 times (vocabulary of size 10.000). Additionally, the final model is evaluated over the Novel Object Captioning at Scale dataset (nocaps) validation set \cite{agrawal2019nocaps} which consists of three class of images called in-domain, near-domain and out-domain, according to the familiarity of the classes with respect to those contained in the training set. This dataset is subject to the same pre-processing of MS-COCO and serves the purpose of further challenging the model in unfavourable conditions.
 

\subsection{Model details}

Three models are implemented for the experiment results. Each one rely on top of the same backbone, the Swin-Transformer in the Large configuration \cite{liu2021swin} pre-trained on ImageNet \cite{deng2009imagenet}. All images are subject to a minimal pre-processing: first they are resized into a  tensor, then RGB values are converted into a  range and further normalized using = and =. The baseline, which is the Base Transformer, and our main model, referred as ``ExpansionNet v2", are implemented with the following configurations =, =, ==. In the latter, the dynamic expansion coefficients is set to 16 and the improved static expansion coefficients consist of = (more details in Section \ref{section_ablation}).

\begin{table}[ht]
\caption{Comparison between our training strategy and the standard one using a single Nvidia A100 GPU.}
\center
\footnotesize
\vspace{-12.0pt}
\begin{tabular}{| c | c | c | c |} 
 \hline
 Training & Standard & Ours & Speed-up \\
 \hline
 Cross-Entropy & 10 days & 1.5 days & 6.7  \\
 \hline
 Reinforcement & 5 days & 1 day & 5 \\
 \hline
 Complete & 15 days & 2.5 days & 6 \\
 \hline
 \end{tabular}
  \label{tab:train_speed_gpu}
\end{table}



\begin{table*}[htb!]
  \centering
  \footnotesize
  \caption{Ablation study in the first stage of Cross-Entropy training using beam size 3 over the Karpathy test split.}
  \begin{tabular}{| c c | c  c  c  c  c  c  c  c |} 
 \hline
 Encoder & Decoder & B1 & B2 & B3 & B4 & M & R & C & S \\
 \hline
 Baseline & Baseline & 74.8 & 58.4 & 44.8 & 34.2 & 28.3 & 56.7 & 115.8 & 21.5 \\
 \hline
 Static Exp. G= & Dyn. Exp. & 76.9 & 61.1 & 47.5 & 36.6 & 28.8 & 57.6 & 121.9 & 22.3 \\
 \hline
 Static Exp G= & Dyn. Exp. & 76.4 & 60.8 & 47.2 & 36.5 & 29.2 & 57.8 & 122.6 & 22.6 \\
 \hline
 Static Exp G= & Dyn. Exp. & 77.3 & 61.4 & 47.8 & 36.8 & 29.1 & 57.9 & 122.7 & 22.6 \\
 \hline
 Static Exp G= & Dyn. Exp. & 76.9 & 60.9 & 47.1 & 36.1 & 29.2 & 57.6 & 122.9 & 22.7 \\
 \hline
 Static Exp G= & Dyn. Exp. &  77.1 & 61.4 & 47.6 & 36.5 & 29.0 & 57.8 & 123.0 & 22.5 \\
 \hline
 Static Exp G= & Dyn. Exp. & 77.0 & 61.2 & 47.6 & 36.6 & 29.2 & 57.7 & 123.3 & 22.6 \\
 \hline
 Static Exp G= & Dyn. Exp. & 77.4 & 61.6 & 47.8 & 36.8 & 29.1 & 57.8 & 123.4 & 22.6 \\
 \hline
 Static Exp. G= & Dyn. Exp. & 77.4 & 61.6 & 47.8 & 36.8 & 29.3 & 58.0 &  123.8 & 22.7 \\
 \hline
 \end{tabular}
 \label{tab:ablation_1}
\end{table*} 


\subsection{Training algorithm}
\label{training_section}

From the sheer number of parameters perspective alone it can be observed that the Swin-Transformer backbone is the biggest part of the system, hence it represents the most computationally expensive portion of the training phase.
Following this observation, in order to enable the end-to-end training step to a broader number of computational architectures, our training is divided in four steps, in particular each phase, during either the pre-training and the reinforcement stages, consists of an initial training in which the backbone's weights are freezed and a fine-tuning step during which gradients flow throughout the whole system:
\begin{enumerate}
    \item \textbf{Cross-Entropy -- Freezed backbone.} The model is trained using batch size 48, an initial learning rate of 2e-4, a warmup of 10.000 and is annealed by 0.8 every 2 epochs for 8 epochs;
    \item \textbf{Cross Entropy -- End to end.} The whole system is trained for 2 additional epochs, using batch size 48 and an initial learning rate of 3e-5 annealed by 0.55 every epoch;
    \item \textbf{CIDEr-D optmization -- Freezed backbone.} Reinforcement phase adopt a batch size of 48, an initial learning rate of 1e-4, no warmup, annealed by 0.8 every epoch for 9 epochs;
    \item \textbf{CIDEr-D optimization -- End to end.} The whole system is fine-tuned for few more iterations up to an additional epoch using a batch size of 48 and fixed learning rate 2e-6. This step is optional since it only slightly contribute to the final performances and can be skipped in case no improvements are observed.
\end{enumerate}


\noindent
Despite its apparent complexity, it is much more computational friendly compared to the standard method consisting of a small batch size of 10 for 30 epochs for both optimization steps. As a matter of fact, only a much smaller number of training epochs are dedicated for the fine-tuning of the whole system. 
This not only means that most of the time the cost of computing the required information related to  backbone's gradient is avoided, but the cost of the forward operation can be drastically reduced as well. In particular, in our implementation, during step 1 and 3 the backbone's forward pass is performed only once for each image in the data set therefore the forward cost is replaced by a memory read and copy. This configuration not only results in a significant decrease of the training time, as show in Table \ref{tab:train_speed_gpu}, but it is able to achieve even higher performances compared to the end-to-end training showcased in \cite{wang2022end} (see Table \ref{tab:offline_table_eval}). All steps are trained using the RAdam optimizer \cite{liu2019variance} ().




\begin{table}[ht!]
 \centering
 \scriptsize
 \caption{Two examples of captions sampled from the Karpathy test split.}
 \begin{tabular}{ @{}c@{} | >{\RaggedRight\arraybackslash}p{4.5cm} }
 Image & Captions \\
 \hline
 \includegraphics[height=0.26\textwidth, width=0.17\textwidth,  valign=m]{img/small_img_example_1.png}
& \vspace{-2.0cm}{\textbf{Baseline:} A cat looking in front of a mirror. \leavevmode\newline \leavevmode\newline
\textbf{ExpansionNet v2:} A cat looking at its reflection in a mirror.
\leavevmode\newline \leavevmode\newline
\textbf{Gt:} \{ A cat looking at his reflection in the mirror. ; A cat that is looking in a mirror. ; A cat looking at itself in a mirror. ;  A cat looking at itself adoringly in a mirror. ;
A cat stares at itself in a mirror. \} } \\
\hline
 \includegraphics[height=0.26\textwidth, width=0.17\textwidth,  valign=m]{img/small_img_example_2.png}
& \vspace{-2.0cm}{\textbf{Baseline:} A cat on a leash next to a water bottle. \leavevmode\newline \leavevmode\newline
\textbf{ExpansionNet v2:} Two pictures of a cat and a dog on a leash.
\leavevmode\newline \leavevmode\newline
\textbf{Gt:} \{ A gray and white cat sitting on top of a table. ; A double picture with one featuring a dog and the other a cat. ; A dog is shown above a cat picture,  A dog and cat in a coupe of photos. ; 
Two pictures of a dog and a cat sitting. \} } \\
\hline
\end{tabular}
\label{small_table_img}
\end{table}


\subsection{Ablation Study}
\label{section_ablation}

In Table \ref{tab:ablation_1} we compare the results of the improved Static Expansion over the previous one. We adopt seve\-ral configurations covering a wide range of lengths. First of all, there is no apparent relation between the size of the length groups and the performance, as more targets does not necessarily yield better performances, in contrast to the diversity, where instances like 128, 128, 128, 128, 128, 256, 256, 256, 256, 256 and 512, 512, 512, 512, 512 lead to a less significant increase in performance compared to 8, 16, 32, 64, 96, 128, 256 or 16, 32, 64, 128, 196, 256, 320. The most successful configuration in terms of CIDEr, which will be adopted in the remaining experiments, is 32, 64, 128, 256, 512  and it suggests that in addition to the diversity, the model benefits from adopting both short and long expansion sequences. For instance, removing either the smallest or greatest end from the targets length interval, like in the case of 128, 256, 384, 512, leads to a performance downgrade. Remarkably, all instances perform better than the baseline across all metrics.


\begin{table*}[htb!]
  \centering
  \footnotesize
  \caption{Offline comparison of state of art models over the Karpathy test split.}
  \begin{tabular}{| c | c  c  c  c  c  c | c  c  c  c  c  c  |} 
 \hline
 \multicolumn{1}{|c|}{} & \multicolumn{6}{c|}{Cross-Entropy} & \multicolumn{6}{c|}{CIDEr-D optimization}\\
 \hline
 Model & B@1 & B@4 & M & R & C & S & B@1 & B@4 & M & R & C & S \\
 \hline
 \multicolumn{13}{c}{\textcolor{white}{XXX}}\\
 \hline
 \multicolumn{13}{|c|}{Single Model}\\
 \hline
 Up-Down \cite{anderson2018bottom} & 77.2 & 36.2 & 27.0 & 56.4 & 113.5 & 20.3 & 79.8 & 36.3 & 27.7 & 56.9 & 120.1 & 21.4\\
 \hline
 GCN-LSTM \cite{yao2018exploring} & 77.3 & 36.8 & 27.9 & 57.0 & 116.3 & 20.9 & 80.5 & 38.2 & 28.5 & 58.3 & 127.6 & 22.0\\
 \hline
 SGAE \cite{yang2019auto} & - & - & - & - & - & - & 80.8 & 38.4 & 28.4 & 58.6 & 127.8 & 22.1\\
 \hline
 AoANet \cite{huang2019attention}& 77.4 & 37.2 & 28.4 & 57.5 & 119.8 & 21.3 & 80.2 & 38.9 & 29.2 & 58.8 & 129.8 & 22.4\\
 \hline
 X-Transformer \cite{pan2020x} & 77.3 & 37.0 & 28.7 & 57.5 & 120.0 & 21.8 & 80.9 & 39.7 & 29.5 & 59.1 & 132.8 & 23.4 \\
 \hline
 GET \cite{ji2021improving} & - & - & - & - & - & - & 81.5 & 39.5 & 29.3 & 58.9 & 131.6 & 22.8 \\
 \hline
 DLCT \cite{luo2021dual} & - & - & - & - & - & - & 81.4 & 39.8 & 29.5 & 59.1 & 133.8 & 23.0 \\
 \hline
 RSTNet \cite{zhang2021rstnet} & - & - & - & - & - & - & 81.8 & 40.1 & 29.8 & 59.5 & 135.6 & 23.3 \\
 \hline
 PureT \cite{wang2022end} & - & - & - & - & - & - & 82.1 & 40.9 & 30.2 & 60.1 & 138.2 & 24.2 \\

 \Xhline{1.5\arrayrulewidth}
 ExpansionNet v2 & 78.1 & 38.1 & 30.1 & 58.9 & 128.2 & 23.5 & \textbf{82.8} & \textbf{41.5} & \textbf{30.3} & \textbf{60.5} & \textbf{140.4} & \textbf{24.5} \\
 \hline
 \multicolumn{13}{c}{\textcolor{white}{XXX}}\\
 \hline
 \multicolumn{13}{|c|}{Ensemble Model}\\
 \hline
 GCN-LSTM \cite{yao2018exploring} & 77.4 & 37.1 & 28.1 & 57.2 & 117.1 & 21.1 & 80.9 & 38.3 & 28.6 & 58.5 & 128.7 & 22.1\\
 \hline
 SGAE \cite{yang2019auto} & - & - & - & - & - & - & 81.0 & 39.0 & 28.4 & 58.9 & 129.1 & 22.2\\
 \hline
 AoANet \cite{huang2019attention} & 78.7 & 38.1 & 28.5 & 58.2 & 122.7 & 21.7 & 81.6 & 40.2 & 29.3 & 59.4 & 132.0 & 22.8\\
 \hline
 X-Transformer \cite{pan2020x} & 77.8 & 37.7 & 29.0 & 58.0 & 122.1 & 21.9 & 81.7 & 40.7 & 29.9 & 59.7 & 135.3 & 23.8 \\
 \hline
 GET \cite{ji2021improving} & - & - & - & - & - & - & 82.1 & 40.6 & 29.8 & 59.6 & 135.1 & 23.8 \\
 \hline
 DLCT \cite{luo2021dual} & - & - & - & - & - & - & 82.2 & 40.8 & 29.9 & 59.8 & 137.5 & 23.3 \\
\hline
 PureT \cite{wang2022end} & - & - & - & - & - & - & 83.4 & 42.1 & 30.4 & 60.8 & 141.0 & 24.3 \\
 \Xhline{1.5\arrayrulewidth}
 ExpansionNet v2  & 78.5 & 38.5 & 29.9 & 58.8 & 128.7 & 23.6 & \textbf{83.5} & \textbf{42.7} & \textbf{30.6} & \textbf{61.1} & \textbf{143.7} & \textbf{24.7} \\
 \hline
 
 \end{tabular}
  \label{tab:offline_table_eval}
\end{table*} 

\begin{table*}[htb!]
  \scriptsize
  \center
  \caption{Online server results on the MS-COCO 2014 test set which ground truth are unknown.}
  \begin{tabular}{| c | c  c |  c  c |  c  c | c  c | c  c | c  c | c  c |}
 \hline
 {Model} & \multicolumn{2}{c|}{B1} & \multicolumn{2}{c|}{B2} &\multicolumn{2}{c|}{ B3} & \multicolumn{2}{c|}{B4} & \multicolumn{2}{c|}{METEOR} & \multicolumn{2}{c|}{ROUGE-L} & \multicolumn{2}{c|}{CIDEr-D}\\
 \hline
 {} & c5 & c40 & c5 & c40 & c5 & c40 & c5 & c40 & c5 & c40 & c5 & c40 & c5 & c40\\
 \hline
 SCST \cite{rennie2017self} & 78.1 & 93.7 & 61.9 & 86.0 & 47.0 & 75.9 & 35.2 & 64.5 & 27.0 & 35.5 & 56.3 & 70.7 & 114.7 & 116.0 \\
 \hline
 Up-Down \cite{anderson2018bottom}   & 80.2 & 95.2 & 64.1 & 88.8 & 49.1 & 79.4 & 36.9 & 68.5 & 27.6 & 36.7 & 57.1 & 72.4 & 117.9 & 120.5\\
 \hline
 GCN-LSTM \cite{yao2018exploring} & - & - & 
 65.5 & 89.3 & 50.8 & 80.3 & 38.7 & 69.7 & 28.5 & 37.6 & 58.5 &
 73.4 & 125.3 & 126.5 \\
 \hline
 SGAE \cite{yang2019auto}& 81.0 & 95.3 &
 65.6 & 89.5 & 50.7 & 80.4 & 38.5 & 69.7 & 28.2 & 37.2 & 58.6 & 73.6 & 123.8 & 126.5 \\
 \hline
 AoANet \cite{huang2019attention}& 81.0 & 95.0 &
 65.8 & 89.6 & 51.4 & 81.3 & 39.4 & 71.2 & 29.1 & 38.5 & 58.9 & 74.5 & 126.9 & 129.6 \\
 \hline
 X-Transformer \cite{pan2020x} & 81.9 & 95.7 & 66.9 & 90.5 & 52.4 & 82.5 & 40.3 & 72.4 & 29.6 & 39.2 & 59.5 & 75.0 & 131.1 & 133.5 \\
 \hline
 RSTNet \cite{zhang2021rstnet} & 82.1 & 96.4 & 67.0 & 91.3 & 52.2 & 83.0 & 40.0 & 73.1 & 29.6 & 39.1 & 59.5 & 74.6 & 131.9 & 134.0 \\
 \hline
 GET \cite{ji2021improving} & 81.6 & 96.1 & 66.5 & 90.9 & 51.9 & 82.8 & 39.7 & 72.9 & 29.4 & 38.8 & 59.1 & 74.4 & 130.3 & 132.5 \\
 \hline
 DLCT \cite{luo2021dual} & 82.4 & 96.6 & 67.4 & 91.7 & 52.8 & 83.8 & 40.6 & 74.0 & 29.8 & 39.6 & 59.8 & 75.3 & 133.3 & 135.4 \\
 \hline
 PureT \cite{wang2022end} & 82.8 & 96.5 & 68.1 & 91.8 & 53.6 & 83.9 & 41.4 & 74.1 & 30.1 & 39.9 & 60.4 & 75.9 & 136.0 & 138.3 \\
 \Xhline{1.5\arrayrulewidth}
ExpansionNet v2 & \textbf{83.3} & \textbf{96.9} & \textbf{68.8} & \textbf{92.6} & \textbf{54.4} & \textbf{85.0} & \textbf{42.1} & \textbf{75.3} & \textbf{30.4} & \textbf{40.1} & \textbf{60.8} & \textbf{76.4} & \textbf{138.5} & \textbf{140.8} \\
 \hline
 \end{tabular}
  \label{tab:online_table_eval}
\end{table*} 


\subsection{Performance comparison}

\textbf{COCO Offline Evaluation}. Table \ref{tab:offline_table_eval} reports the score comparison between ExpansionNet v2 and the best performing models in recent years. Up-Down \cite{anderson2018bottom} introduced the idea of extracting a collection of features from the images using an object detector like Faster-RCNN \cite{ren2015faster} in contrast to the classification backbone \cite{xu2015show}. The idea was adopted in most of the following architectures as well, for instance in case of GCN-LSTM \cite{yao2018exploring} and SGAE \cite{yang2019auto}, which, additionally, implemented a graph convolutional network on top of it in order to exploit the information provided by a scene graph. AoANet \cite{huang2019attention} instead, relied on a Transformer based architecture and improved the attentive components with two gates serving the purpose of simulating an additional level of attention over the inputs and augmented the language modeling part with a LSTM. X-Transformer \cite{pan2020x} on the other hand, adopted a fully attentive architecture and further refined the attentive blocks by means of bilinear pooling techniques. The most recent and performing architectures redefined the best way of feeding the visual input into the network, for instance RSTNet \cite{zhang2021rstnet} showcased the effectiveness of grid features over regions, GET \cite{ji2021improving} processed the images using a global representation as well in addition to the local ones, DLCT \cite{luo2021dual} instead exploited the advantages of both regions and grid visual features. Finally
PureT \cite{wang2022end} removed the convolutional backbone and implemented an entirely attentive model based upon the novel Swin-Transformer, hence applying Window / Shifted-Window MHA in both encoder and decoder.
ExpansionNet v2 outperforms all the previous models across all metrics in both single and ensemble configuration, in particular it achieves the new state of art performance, outperforming the previous one by 0.1 BLEU1, 0.6 BLEU4, 0.3 ROUGE-L, 2.7 CIDEr-D and 0.4 SPICE in the ensemble configuration. \\

\noindent
\textbf{COCO Online Evaluation}. We evaluate ExpansionNet v2 using the ensemble configuration and adopting the standard Beam Search (beam size 5) over the official testing set of 40.775 images, submitting the predictions to the online testing server. Results are reported in Table \ref{tab:online_table_eval}, c5 and c40 represent the scores in case of 5 and 40 reference captions (unknown to the user) respectively. Similarly to the offline case, it achieves the new state of art performance across all metrics (2 July 2022), in particular it outperforms the previous best performing model by a margin of 1.2 BLEU4 (c40), 0.2 METEOR (c40), 0.5 ROUGE-L (c40) and 2.5 CIDEr-D in both c5 and c40 instances. \\



\begin{figure*}[ht!]
\centering
\includegraphics[width=1.0\textwidth]{img/double_example_head_row.png}
\caption{\label{fig:attention_vis.png} Attention visualization of one head in one of our main model layers.}
\end{figure*}

\begin{table}[ht!]
  \caption{Performances comparison on nocaps validation set.}
  \centering
  \footnotesize
  \begin{tabular}{| c c | c c c |} 
 \hline
Domain & Metric & Enc-Dec\cite{changpinyo2021conceptual} & Up-Down\cite{anderson2018bottom} & Ours \\
 \hline
 \multirow{2}{*}{In} & C & 72.8 & 78.1 & \textbf{83.8} \\
  & S & 11.1 & 11.6 & \textbf{12.6} \\
 \hline
 \multirow{2}{*}{Near} & C & 57.1 & 57.7 & \textbf{79.2} \\
 & S & 10.2 & 10.3 & \textbf{12.4} \\
 \hline
 \multirow{2}{*}{Out} & C & 34.1 & 31.3 & \textbf{54.0} \\
 & S & 8.3 & 8.3 & \textbf{9.3} \\
 \hline
 \multirow{2}{*}{All} & C & 54.7 & 55.3 & \textbf{72.9} \\
 & S & 10.0 & 10.1 & \textbf{11.4} \\
 \hline
 \end{tabular}
  \label{tab:no_caps_eval}
\end{table}


\begin{table}[ht!]
 \centering
 \scriptsize
 \caption{Three examples of nocaps out-of-domain images, for the sake of brevity only 3 ground truth descriptions are reported.}
 \begin{tabular}{  @{}c@{} | >{\RaggedRight\arraybackslash}p{4.5cm}  }
 Image & Captions \\
 \hline
 \includegraphics[height=0.18\textwidth, width=0.17\textwidth,  valign=m]{img/nocaps_img_1.png}
& \vspace{-1.5cm}{\textbf{Pred:} A close up of a fish in a body of water.
\leavevmode\newline \leavevmode\newline
\textbf{Gt:} \{ A seahorse in an aquarium full of water with some plants growing in the background. ; A blue seahorse is swimming near sea plants on back. ; A very small seahorse is in the water along with other pieces. \} } \\

\hline
 \includegraphics[height=0.18\textwidth, width=0.17\textwidth,  valign=m]{img/nocaps_img_3.png}
& \vspace{-1.5cm}{\textbf{Pred:} Three pictures of a blender with red liquid in it.
\leavevmode\newline \leavevmode\newline
\textbf{Gt:} \{ A picture of three blenders with a strawberry looking beverage inside. ; A white mixer in the process of making a smoothie. ; The steps of making a smoothie in a blender are shown. \} } \\

\hline
 \includegraphics[height=0.18\textwidth, width=0.17\textwidth,  valign=m]{img/nocaps_img_2.png}
& \vspace{-1.5cm}{\textbf{Pred:} 
A birthday cake is decorated with a house.
\leavevmode\newline \leavevmode\newline
\textbf{Gt:} \{ A gingerbread house has a red frosting roof and several candy pieces. ; A gingerbread house that is red, brown, white, and green. ; A log cabin is made out of dessert treats.  \} } \\
\hline
\end{tabular}
\label{nocaps_img}
\end{table}

\medskip
\noindent
\textbf{Nocaps Evaluation}. We evaluate ExpansionNet v2 over the nocaps validation set. In particular, we adopt a single model trained exclusively on Cross-Entropy Loss, using no additional pre-training data sets and the predictions are generated by the standard Beam Search algorithm (beam size 3) in contrast to the CBS \cite{anderson2016guided}. A limited comparison is reported in Table \ref{tab:no_caps_eval} which showcases that our model achieves very competitive results among the architectures trained in similar configurations, with a overall lead of 17.6 CIDEr and 1.4 SPICE over the Up-Down model \cite{anderson2018bottom}. However, it's worth noting that it is still ultimately outperformed by many recent works such as \cite{hu2022scaling, zhang2021vinvl, li2022blip, li2020oscar} which results are omitted because of a different experiment setup.


\subsection{Qualitative Analysis}

Table \ref{tab:example_captions_1} and \ref{tab:example_captions_2} provide some examples of captions. Regardless of the complexity, ExpansionNet v2 appears not only able to correctly describe the subjects depicted in the scenes but also showcases a good level of semantic understanding by describing also the goals and interactions (Images 1, 8 and 12). Unfortunately, our model seem to struggle with out-of-domain objects as showcased in Table \ref{nocaps_img} where, because of objects and terms unknown to the model, predictions are either imprecise (2\textsuperscript{nd} image) or incorrect (1\textsuperscript{st} and 3\textsuperscript{rd} image). Nonetheless, it seems to be able of providing a rough description of the images context. Finally, we showcase an example of attention visualization in Figure \ref{fig:attention_vis.png}, despite the absence of an object detector, the scattered focus correctly outlines the main subjects.


