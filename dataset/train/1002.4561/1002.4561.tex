\documentclass[letterpaper,11pt]{article}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath}
\usepackage[normalbibnotes,normallooseness,normalindent,normallists,normalbib,normalleading,normaltitle]{savetrees}

\usepackage{xspace}
\usepackage{sober}
\usepackage{mdwlist}
\usepackage{graphicx}



\newcommand{\old}[1]{}

\newcommand{\jared}[1]{[{\bf Jared:\ } {\em #1}]}
\newcommand{\val}[1]{[{\bf Val:\ } {\em #1}]}
\textwidth6.7in \textheight9in \oddsidemargin 0pt \evensidemargin 0pt
\topmargin -47pt


\newcounter{algorithmLine}
\newcommand{\algline}{\\\thealgorithmLine\hfil\>\stepcounter{algorithmLine}}
\newcommand{\algnono}{\\ \>}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{observation}{Observation}
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}
\newtheorem{claim}{Claim}
\newcommand{\sq}{\hbox{\rlap{}}}
\newcommand{\qed}{\hspace*{\fill}\sq}
\newenvironment{proof}{\noindent {\bf Proof:}}{\qed\par\vskip 4mm\par}


\newenvironment{tabAlgorithm}[2]{
\setcounter{algorithmLine}{1} \samepage
\begin{tabbing}
999\=\kill #1 \ \ \ \ \parbox{3.8in}{\it #2} }{
\end{tabbing}
}

  
\newtheorem{con}{Conjecture}

\title{Breaking the  Bit Barrier: Scalable Byzantine agreement with an Adaptive Adversary}
\author{Valerie King \thanks{val@cs.uvic.ca; Department of Computer Science, University of Victoria, P.O.
Box 3055, Victoria, BC, Canada V8W 3P6.  This research was supported by an NSERC grant.} \and Jared Saia \thanks{saia@cs.unm.edu; Department of Computer Science, University of New Mexico, Albuquerque, NM 87131-1386. This research was partially supported by NSF CAREER Award 0644058, NSF CCR-0313160, and an AFOSR MURI grant.}}

\date{}                                           


\begin{document}
\maketitle

\thispagestyle{empty}


\begin{abstract}

We describe an algorithm for Byzantine agreement that is scalable in the sense that each processor sends only  bits, where  is the total number of processors.  Our algorithm succeeds with high probability against an \emph{adaptive adversary}, which can take over processors at any time during the protocol, up to the point of taking over arbitrarily close to a  fraction.  We assume synchronous communication but a \emph{rushing} adversary.  Moreover, our algorithm works in the presence of flooding: processors controlled by the adversary can send out any number of messages.  We assume the existence of private channels between all pairs of processors but make no other cryptographic assumptions.  Finally, our algorithm has latency that is polylogarithmic in .  To the best of our knowledge, ours is the first algorithm to solve Byzantine agreement against an adaptive adversary, while requiring  total bits of communication.
\end{abstract}


\ \\ \ \\ \ \\ \ \\ \ \\ \ \\ \ \\ \ \\ \ \\ \ \\ \ \\ \ \\ \ \\ \ \\ \ \\ \ \\ \ \\ \ \\ \ \\

This paper should \emph{not} be considered for the best student paper award.


\pagebreak
\setcounter{page}{1}

\section{Introduction}

Recent years have seen a rapid increase in the number of networks that are characterized by large sizes and little admission control.  Such networks are open to attacks by malicious users, who may subvert the network for their own gain.  To address this problem, the research community has been recently revisiting techniques for dealing with nodes under the control of a malicious adversary~\cite{kotla2007zyzzyva,clement-making,1529992,anderson2002worldwide}.

The Byzantine agreement problem, defined in 1982, is the \emph{sine qua non} of handling malicious nodes.   With a solution to Byzantine agreement, it is possible to create a network that is reliable, even when its components are not.  Without a solution, a compromised network cannot perform even the most basic computations reliably.  A testament to the continued importance of the problem is its appearance in modern domains such as sensor networks~\cite{shi2004designing}; mediation in game theory~\cite{ADGH,ADH}; grid computing~\cite{anderson2002worldwide}; peer-to-peer networks~\cite{rhea2003pond}; and cloud computing~\cite{wright2009contemporary}.  However, despite decades of work and thousands of papers, we still have no practical solution to Byzantine agreement for large networks.  One impediment to practicality is suggested by the following quotes from recent systems papers (see also~\cite{castro2002practical,Malkhi97unreliableintrusion,amir2006scaling,agbaria2003overcoming,1098025}):

\begin{itemize}
\item \emph{``Unfortunately, Byzantine agreement requires a number of messages quadratic in the number of participants, so it is infeasible for use in synchronizing a large number of replicas''}~\cite{rhea2003pond}

\item \emph{``Eventually batching cannot compensate for the quadratic number of messages [of Practical Byzantine Fault Tolerance (PBFT)]''}~\cite{CMLRS}

\item \emph{``The communication overhead of Byzantine Agreement is inherently large''}~\cite{Cheng2009219}

\end{itemize}

In this paper, we describe an algorithm for Byzantine agreement with only  bit communication per processor overhead.  
Our  techniques also lead to solutions with  bit complexity for universe reduction and a problem we call  the {\it  global coin subsequence problem},  generating a polylogarithmic length string, most of which are global coinflips generated uniformly and independently at random and agreed upon by all the good processors .  Our protocols are polylogarithmic in time and, succeed with high probability.\footnote{That is probability  for any fixed }

We overcome the lower bound of~\cite{DR} by allowing for a small probability of error.  This is necessary since this lower bound also implies that any randomized algorithm which always uses no more than  messages must necessarily err with positive probability, since the adversary can guess the random coinflips
and achieve the lower bound if the guess is correct.




\subsection{Model and Problem Definition}
 We assume a fully connected network of  processors, whose IDs are common knowledge.  Each processor has a private coin.  We assume that all communication channels are \emph{private} and that whenever a processor sends a message directly to another, the identity of the sender is known to the recipient, but we otherwise make no cryptographic assumptions.  We assume an {\it adaptive} adversary. That is, the adversary can take over processors at any point during the protocol up to the point of taking over up to a  fraction of the processors for any positive constant .  The adversary is malicious: it  chooses the input bits of every processor,  bad processors can engage in any kind of deviations from the protocol, including false messages and collusion, or crash failures, while the remaining processors are good and follow the protocol. Bad processors can send {\it any} number of messages.

We assume a synchronous model of communication.  In particular, we assume there is a known upper bound on the transit time of any message and communication proceeds in rounds determined by this transit time.  The time complexity of our protocols are given in the number of rounds.  However, we assume a \emph{rushing} adversary that gets to control the order in which messages are delivered in each round.  In particular, the adversary can receive all the messages sent by good processors before sending out its own messages.

In the {\it  Byzantine agreement}  problem, each processor begins with either a 0 or 1. An execution of a protocol is {\it successful} if all processors  terminate and, upon termination, agree on a bit held by at least one good  processor at the start.  

The  {\it  global coin subsequence}   problem  generates a string of length  words,  of  which are global coinflips generated uniformly and independently at random and agreed upon by all the good processors . We call  an  {\it unreliable global coin sequence}.


\subsection{ Results}

We use the phrase {\it with high probability} ({\it w.h.p.}) to mean that an event happens with probability at least 
for every constant  and sufficiently large . For readability, we treat  as an integer throughout. 

In all of our results,  is the number of processors in a synchronous message passing model with an adaptive, rushing adversary that controls less than  fraction of processors, for any positive constant   We have three main results.  The first result makes use of the second and third ones, but these latter two results may be of independent interest. First, we show:

\begin{theorem} \label{t:main} {\sc [Byzantine agreement]}
There exists a protocol which w.h.p. computes Byzantine agreement, runs in polylogarithmic time,  and uses  bits of communication.
\end{theorem}

Our second result concerns \emph{almost-everywhere} Byzantine agreement and,  \emph{almost-everywhere} global coin subsequence where a  fraction of the good processors come to agreement on a good processor's input bit, or the random coin flip, resp.

\begin{theorem} \label{t:ae} {\sc [Almost Everywhere Byzantine agreement]}
For any , there exists a protocol which w.h.p. computes almost -everywhere Byzantine agreement, runs in time  and uses  bits of communication per processor. In addition, this protocol can be used to solve an almost everywhere global coin subsequence  problem for an additional cost of  time and  bits of communication per bit of .   
\end{theorem}

Our third result is used as a subroutine of the previous protocol. 
\begin{theorem} \label{t:aebasparse}{\sc[Almost Everywhere Byzantine Agreement with Unreliable Global Coins}]
Let  be a sequence of length   containing a subsequence of uniformly and independent random coinflips of length   known to 
  good processors.  Let  and  be any positive constants.  Then there is a protocol which runs in time  with bit complexity  such that with probability at least , all but  of the good processors commit to the same vote , where  was the input of at least one good processor.   
\end{theorem}



Our final result concerns going from almost-everywhere Byzantine agreement to everywhere Byzantine agreement.  It makes use of a simple consequence of our first result which is that not only can almost all of the processors reach agreement on a bit, but they can also generate a random bit.  We actually prove a result below that is stronger than what is necessary to establish Theorem~\ref{t:main}.

\begin{theorem}\label{t:ae2e}
Assume  good processors agree on a message  and there is an oracle which can generate each bit of a global coin subsequence  in  time where . Then  there is a protocol that ensures with probability   that all good processors output  and n.  Moreover this protocol runs in  time and uses  bits of communication per processor.
\end{theorem} 


\subsection{Techniques}

Our protocol uses a sparse network construction and tournament tree similar to the network and tournament network in \cite{KSSV}.  This past result gives a bandwidth efficient Byzantine agreement algorithm for a \emph{non-adaptive adversary},  which must take over all its processors at the start of the algorithm.  The basic idea of the algorithm from~\cite{KSSV} is that processors compete in local elections in a tournament network, where the winners advance to the next highest level, until finally a small set is elected that is representative in the sense that the fraction of bad processors in this set is not much more than the fraction of bad processors in the general population.

This election approach is \emph{prima facie} impossible with an adaptive adversary, which can simply wait until a small set is elected and then can take over all processors in that set.  To avoid this problem, we make use of two novel techniques.  First, instead of electing processors, we elect \emph{arrays} of random numbers, each generated initially by a processor.  Second, we use secret sharing on these arrays to make sure that 1) the arrays are split among increasingly larger numbers of processors as the array is elected higher up in the tournament; and 2) the secrets in the arrays cannot be reconstructed except at the appropriate time in the protocol.  Critical to our approach is the need to iteratively reapply secret sharing on shares of secrets that were computed previously, in order to increase the number of shares when necessary in the protocol.

Another contribution of this paper is the algorithm we use to run an election.  In~\cite{KSSV}, elections were run by participants.  These elections used Feige's bin selection protocol~\cite{Feige} and a Byzantine agreement algorithm run among the small group of participants to agree on the bin selected by everyone.  Because we are now faced with an adaptive adversary, this approach fails.  In particular, we must now have a much larger sets of processors which come to agreement on the bins selected in Feige's protocol.  To achieve this, we make use of Rabin's algorithm~\cite{rabin1983randomized} run on a sparse network.  
\old{To ensure that Rabin's algorithm is bandwidth efficient, not only do we run it on a sparse network, but we also}
To run Rabin's algorithm, we supply it with an almost everywhere global coin sequence, where coinflips are generated from the arrays described above.  

Our final new technique is a simple but not obvious protocol for going from almost-everywhere Byzantine agreement and the global coin subsequence problem to everywhere Byzantine agreement with an adaptive adversary.  A past result~\cite{KSDISC09} shows that it is possible to do this with a non-adaptive adversary, even without private channels.  However, the technique presented in this paper for solving the problem with an adaptive adversary is significantly different than the approach from~\cite{KSDISC09}.

 \medskip
 
In Section~\ref{s:aeprotocol}, we describe the almost everywhere Byzantine agreement  and global coin subsequence protocols. The scalable version of Rabin's algorithm is in Section \ref{AEBACC}.  In Section~\ref{s:AE2E}, we describe the almost everywhere to everywhere protocol. 

\section{Related work}

As mentioned previously, this paper builds on a result from~\cite{KSSV} that gives a polylogarithmic time protocol with polylogarithmic bits of communication per processor for almost everywhere Byzantine agreement, leader election, and universe reduction in the synchronous full information message passing model with a \emph{nonadaptive} rushing adversary. 
 
Almost everywhere agreement in sparse networks has been studied since 1986. See \cite{KSSV,KSSV2} for references. The problem of almost everywhere agreement for secure multiparty computation on a partially connected network was defined and solved in 2008 in \cite{GO}, albeit with  message cost.

In \cite{KSSV2}, the authors give a sparse network implementation of their protocols from \cite{KSSV}.   It is easy to see that everywhere agreement is impossible in a sparse network where the number of faulty processors  is sufficient to surround  a good processor.  A protocol in which processors use  bits may seem as vulnerable to being isolated as in a sparse network, but the difference is that without access to private random bits, the adversary can't anticipate at the start of the protocol where communication will occur.   In \cite{HKK}, it is shown that even with private channels, if a processor must pre-specify the set of processors it is willing to listen to at the start of a round,  where its choice in each round can depend on the outcome of its random coin tosses, at least one processor must send  messages to compute Byzantine agreement with probability at least .  Hence the only hope for a protocol where every processor sends  messages is to design outside this constraint.  Note that the Almost Everywhere Byzantine Agreement protocol falls within this restrictive model, but the Almost Everywhere to Everywhere protocol does not, as the decision of whether a message is listened to  (or acted upon) depends on how many messages carrying a certain value are received so far. 

\section{Almost everywhere protocol} \label{s:aeprotocol}

We first outline the protocol.  We label the processors .  The processors are arranged into nodes in a -ary tree. Each processor appears in polylogarithmic places in each level of the tree, in a manner that will be described below. The levels of the tree are numbered from the leaf nodes (level 1) to the root (level ).  In addition, each processor, , generates an \emph{array} of random bits, consisting of one \emph{block} for each level of the network and secret shares this with the processors in the  node on level 1. 

Each node in the tree runs an election among  arrays whereby a subset of  arrays are selected.  In order to run this election at level ,
the  block of each array supplies a random bin choice and random bits to run almost everywhere Byzantine agreement with common global coins to agree on each bin choice of every competing array.  It suffices that some of these coins are random and known almost everywhere. The  shares of  the remaining blocks of arrays  which remain in the competition are further subdivided into more shares and sent to the parent  (and erased from the current processors'
memories.) In this way, the more important the arrays, the more processors  need to be taken over to prevent its correct operation.

 Random bits are revealed as needed by sending the iterated shares of secrets down to {\it all}  the leaves of the subtree rooted where the election is taking place, collecting -shares at each level  to reconstruct  shares.  In the level 1 nodes, each processor sends the other processors its share.

The winning arrays of a node's election compete in elections at the next higher level.
At the root there are a small number of arrays left to run almost everywhere Byzantine Agreement with a global coin.

The method of secret sharing and iterative secret sharing is described in Section \ref{secretsharing}.
Networks and communication protocols are described in Section \ref{network}; the election routine is described in Section \ref{election}.
The procedure for running almost everywhere Byzantine Agreement with unreliable coins is described in Section~\ref{AEBACC}.  The main procedure for almost everywhere Byzantine agreement is in \ref{main}.  The extension of the almost everywhere Byzantine Agreement protocol to a solution for  the global coin subsequence problem is in Section~\ref{GCS}.  Finally the analysis and correctness proof can be found in Sections \ref{analysis} and \ref{correct}, respectively.



\subsection{Secret sharing} \label{secretsharing}

We assume any (non-verifiable) secret sharing scheme which is a   threshold scheme. That is, each of  players are given shares of size
proportional to the message  and  shares are required to reconstruct . Every message which is the size of  is consistent with any subset of  or fewer shares, so no information as to the contents of  is gained about the secret from holding fewer than  shares. See \cite{crypto} for details on constructing such a scheme.  We will make extensive use of the following definition.

\begin{definition}
{\it secretShare(s)}:   To share a secret sequence of words  with  processes (including itself)  of which  may be corrupt, a processor (dealer) creates and distributes shares of each of the words using a  secret sharing mechanism. 
Note that if a processor knows a share of a secret, it can treat that share as a secret. To share that share with   processors of which at most  processors are corrupt,  it creates and  distributes shares of the share
using a  mechanism and deletes its original share from memory. This can be iterated many times.
We define a {\it 1-share} of a secret to be a share of a secret and an {\it -share} of a secret to be a share of an -share of a secret.
\end{definition}
To reveal a secret sequence , all processors which receive a share of  from a dealer sends this shares to a processor   which computes the secret. This also may be iterated to first reconstruct   shares from  shares, etc.,  and eventually the secret sequence. 
In this paper we assume secret sharing schemes with . (This is quite robust, as any  would work.)


\begin{lemma} \label{secret}  If a secret  is shared in this manner  up to  iterations, then an adversary which possesses   shares of each -share learns no information about the secret.  \end{lemma}
\begin{proof}
The proof is by induction. For level 1, it is true by definition of secret sharing.
Suppose it is true up to  iterations. 

 Let  be any value. By induction, it is consistent
with the  known  shares  on all levels  and some assignment  of values to sets of unknown  -shares. Then
consider the shares of these shares that have been spread to level .  For each  value of an -share given by ,  there is an assignment  of values to the unknown   shares  consistent with the  - known shares.  Hence knowing in addition the
 -shares of each
-share does not reveal any information about  the secret. 
\end{proof}





\subsection{Network and Communication}\label{network}
We first describe the topology of the network and then the communications protoocols. 

\subsubsection{Samplers}
Key to the construction of the network is the definition of an averaging sampler which was also used heavily in \cite{KKKSS-TALG,KSSV2}.
We repeat the definition here for convenience. 

Our protocols rely on the use of averaging (or
oblivious) samplers, families of bipartite graphs which define
subsets of elements such that all but a small number contain at
most a fraction of ``bad'' elements close to the fraction of bad
elements of the entire set.  We assume either a nonuniform model in
which each processor has a copy of the required samplers for a
given input size, or else that each processor initializes by
constructing the required samplers in exponential time.

\begin{definition}
Let , denote the set of integers , and

the multisets of size  consisting of elements of .
Let  be a function assigning
multisets 
of size  to integers. We define the intersection of a multiset  and a set  to be the number of 
elements of  which are in .


 is a  sampler if
for every set 
at most a  fraction of all inputs  have
.
\end{definition}

\medskip

The following lemma establishing the existence of samplers
can be shown using the probabilistic method.  For , 
let .  A slight modification of Lemma 2 in \cite{KKKSS-TALG} yields:

\begin{lemma} \label{l:samp2}
For every  
such that ,
there exists a 
sampler  and for all , .
 \end{lemma}

\medskip

For this paper we will use the term {\it sampler} to refer to a   sampler, where .




\subsubsection{Network structure}

Let  be the set of all  processors. 
The network is structured as a complete -ary  tree. The level 1 nodes (leaves) contain  processors.  Each node at height  contains  processors; there are  nodes on level ; and the root node at height  contains all the processors.  There are  leaves, each assigned to a different processor.  The contents of each node on level  is determined by  a sampler where  is the set of nodes,  and . 

The edges in the network are of three types:
\begin{enumerate}
\item
{\it Uplinks:}   The {\it uplinks}  from  processors in a child node on level   to processors in a parent node on level  are determined by a sampler
of degree  ,  is the set of processors in the child node and  is the set of processors in the parent node.  
\item
:  The   between  processors in a node  at level  to 's descendants at level  is determined by a sampler with  the set of processor in the node ,  's level 1 descendants, and  a subset of size .  Here, ; ; 
and the maximum number of  incident to  a level 1 node is .

\item
{\it Links between processors in a node} are also determined by a sampler of polylogarithmic degree. These are described in the Almost Everywhere Byzantine Agreement with Global Coin protocol. 
\end{enumerate}


From the properties of samplers,  we have: 
\begin{enumerate}
\item Fewer than a  fraction of the nodes on any level contain less than a  fraction of good processors (we call such nodes {\it bad nodes}). 
\item  There are fewer than a  fraction of processors in every node whose uplinks are connected to fewer than a  fraction of good processors, unless the parent or child, resp. is a bad node.
\item
There are fewer than a  fraction of processors in a node which are connected  through  to a majority of bad nodes on level 1, in any subtree which has fewer than a  fraction of bad level 1 nodes. 
\end{enumerate}


\subsubsection{Communication protocols}


We use the following three subroutines for communication.  Initially each processor  shares its secret with all the processors in the  node at level 1.
 
\smallskip
\noindent
: To  send up a secret sequence , a  processor in a node uses   to  send to each of its neighbors in its parent node (those connected by ) a share of .  Then the processor erases  from its own memory.

\smallskip
\noindent
: After a secret sequence has been passed up a path to a node , the secret sequence is passed down to  the processors in the 1-nodes in the subtree.  To send a secret sequence  down the tree, each processor in a node  on level  sends its -shares of   {\it down} the uplinks it came from {\it plus the  corresponding uplinks from each of its other children}.  The processors  on level  receiving the -shares use these shares to reconstruct  -shares of . This is repeated for lower levels until  all the 2-shares are received by the processors in all the level 1 nodes in  's subtree. The processors in the 1-node  each send each other all their shares and reconstruct the secrets received. Note that a processor may have received an -share generated from more than one  share because of the overlapping of sets  (of uplinks) in the sampler.

\smallskip
\noindent
 This procedure is used by a node  on any level   to learn a sequence  held by the set of level 1 nodes in 's subtree.  Each processor in the level 1 node   sends  up the  from  to a subset of processors in . 
 A processor in  receiving  from each of the processors in a level 1 node takes a majority to determine the node's  version of  .
Then it takes a majority over  the values obtained from each of the level 1 nodes it is linked to.  

\subsubsection{Correctness of communications}\label{s:correct-comm}

\begin{definition} A {\it good node} is a node with at least  fraction of good processors and a bad node is a node which is not good.
(Note that for the lemma below, it suffices that that a good node contain a  fraction of good processors)
A {\it good path} up the tree is a path from leaf to root which has no nodes which become bad during the protocol. 
\end{definition}

\begin{lemma}
\enumerate
\item If   is executed up a path in the tree and if the adversary learns the secret , there must be at least one bad node on that path. 
\item Assume that  is generated by a good processor and  is executed up a good path in a tree to a node  on level , followed by   and then .  Further assume there are at least a  fraction of nodes among 's descendants on level 1 which are good, and whose paths to  are good.  Then a  fraction of the good processors  in  learn .
\end{lemma}

\begin{proof}
In the protocol any secret shared to a good node on level 1 remains hidden from the adversary which receives no more than a  fraction of the shares.  If it is passed to a good node on level 2, then since the uplinks are determined by a -sampler, no more than  fraction of the uplink sets contain more than  fraction of bad processors. Hence   
no more than an additional  fraction of the 1-shares are revealed because the adversary has too many 2-shares.  Similarly, no more than 
an additional  fraction of the 2-shares are revealed because the adversary has too many 3-shares. Hence if the secret is passed up a good path,
the adversary does not gain more than  additional shares of the secret, or  of the shares, for a total less than  fraction. Thus by Lemma \ref{secret} the adversary has no knowledge of any secret that is sent up a good path until that secret is released. 


We consider a secret released  when it is first sent down from a node . A secret will be reconstructed  by a processor when it is passed down good paths along the uplinks as   of all its shares are returned down the good paths to the leaves. If there are at least  fraction of level 1 nodes which are good and whose paths to  are good, then a  fraction of the good processors in  have  from a majority of 1-level nodes which have received the correct sequence. 
\end{proof}



\subsection{Election}\label{election}
Here we describe Feige's election procedure \cite{Feige}, adapted to this context.  We assume  candidates are competing in the election.  The election algorithm is given below.

\begin{definition}
Let , and let a \emph{word} consist of  bits.  A {\it block}  is a sequence of bits, beginning with an initial word (bin choice)  followed by  words , which will be used as coins in running Byzantine agreement on each bit of the bin choices for each of the  candidates.  The input to an election is a set of  candidate blocks labelled . The output is 
a set of  indices .  Let .
  
 \end{definition}

\begin{algorithm}\label{a:ep}
\caption{Election Protocol}
\begin{enumerate}
\item In parallel, for , the processors run almost everywhere Byzantine agreement on the bin choice of each of the  candidate blocks.  Round   of the Byzantine agreement protocol to determine  's bin choice  is run  using the  word of the   processor's block .   Let  be the decided bin choices.
\item
Let  . Then   .
If  then  is augmented by adding the first   indices that would otherwise be omitted. 
\end{enumerate} 
\end{algorithm}

If we assume that the bin choices are agreed upon by all processors then Feige's result for the atomic broadcast model holds:

\begin{lemma} \cite{Feige} \label{Feige}
Let  be the set of bin choices generated independently at random. Then even if the adversary sets the remaining bits after seeing the bin choices of , with probability at least   there are at least    winners from .  E.g., if   and
  then with probability  the fraction of winners from  is at least . 
\end{lemma}


\subsection{Main protocol for Almost Everywhere Byzantine Agreement}\label{main}

The main protocol for Almost Everywhere Byzantine Agreement is given as Algorithm 2.  Figure 1, which we now describe, outlines the main ideas behind the algorithm.  The left part of Figure 1 illustrates the algorithm when run on a 3-ary network tree.  The processors are represented with the numbers  through  and the ovals represent the nodes of the network, where a link between a pair of nodes illustrates a parent child relationship.  The numbers in the bottom part of each node are the processors contained in that nodes.  Note that the size of these sets increase as we go up tree.  Further note that each processor is contained in more than one node at a given level.  The numbers in the top part of each node represent the processors whose blocks are candidates at that node.  Note that the size of this set remains constant () as we go from level 2 to level 3.  Further note that each processor is a candidate in at most one node at a given level.

The right part of Figure 1 illustrates communication in Algorithm 2 for an election occurring at a fixed node at level .  Time moves from left to right in this figure and the levels of the network are increasing from bottom to top.  Salient points in this figure are as follows.   First, bin choices are revealed by (1) communication in the  protocol that moves hop by hop from level  down to level  in the network and (2) communication in the  protocol that proceeds directly from the level  leaf nodes to the level  nodes.  Second, Byzantine agreement occurs at level , via communication between the processors in the node at level  \emph{and} communication down and up the network in order to expose the coins, one after another, as needed in the course of the Byzantine agreement algorithm.  Finally, shares of the blocks of the winners of the election at the node at level  are sent up to the parent node at level .

\begin{figure}[t]
\begin{center}
\begin{tabular}{ll}
\includegraphics[scale=0.32]{network.pdf} &
\includegraphics[scale=0.32]{alg.pdf}
\end{tabular}
\caption{Left: Example run of Algorithm 2 on a small tree; Right: Communication in different phases of Algorithm 2 for a fixed level . }
\end{center}
\label{f:aeBA}
\end{figure}



\begin{algorithm} \label{aeBA}
\caption {Almost Everywhere Byzantine Agreement}
\begin{enumerate}
\item
For all  in parallel
\begin{enumerate}
\item
 Each processor  generates an array of   blocks   and  uses  to share its array with the  level 1 node;
\item
Each processor in the   level 1 node uses   to share its 1-share of  with its parent node
and then erases its shares from memory.  \\
\end{enumerate}
\item
Repeat for  to \\
\begin{enumerate}
\item
For each processor in each node  on level :\\
   for  and , let   be the  array sent up from child . 
( If  then  ) \\
  \\

Let  be the sequence of first blocks of the arrays of , i.e., the  array of  is the first block of  array of .  Let  be the sequence of the remaining blocks  of each array of .\\

 {\bf Expose bin choices:}\\
In parallel,  for all candidates 

 \begin{enumerate}
\item
 ;
  \item
 . \\
\end{enumerate} 

\item {\bf Agree on bin choices:}\\
If  then for rounds   \\
\begin{enumerate}
\item
{\bf Expose coin flips}: Generate  coinflips for the  round of Byzantine agreement to decide each of  bin choices.\\
In parallel,  for all contestants  
\begin{enumerate}
\item
; upon receiving all -shares, level 1 processors compute the secret bits ;
\item
 .
\end{enumerate}
\item
Run the  round of a.e. Byzantine agreement in parallel to decide the bin choice of all contestants. \\
\end{enumerate}

\item {\bf Send Shares of Winners}: 
Let  be the winners of the election decided from the previous step (the lightest bin).  Let  be the subsequence of  from ;   All processors in a node at level  use  to  send  to its parent node
and erase  from memory. \\
   \\
\end{enumerate}
\item
All  processes in the single node on level  run a.e. Byzantine agreement once using their initial inputs  as inputs to the protocol  (instead of bin choices) and the remaining block of each contestant. (Note that only two bits of  this block are needed.) \\
For rounds , 

 \begin{enumerate}
\item
 ;

 \item
 . 
\item
Use  to run the  round of a.e. Byzantine agreement. 
\end{enumerate}


\end{enumerate}
\end{algorithm}

\subsection{Modification to output a sequence of mostly random bits}\label{GCS}

The Almost Everywhere Byzantine Agreement can be modified easily to solve the global coin subsequence  for  a sequence of  words.  Add one more block of the desired length to each processor's array at the start. At level , use sendDown and sendOpen to recover each word, one from each of the  contestants. The time and bit complexity is given in Theorem \ref{t:ae}. 

\subsection{Bit complexity and running time analysis}\label{analysis}

The proof of the following lemma is given in Section~\ref{s:pf-analysis}

\begin{lemma}\label{l:analysis}
For any , Almost Everywhere Byzantine Agreement protocol runs in  bits  and time .
\end{lemma}


\subsection{Proof of correctness}\label{correct}

An election is {\it good}  at a node   if the processors can carry out  a.e.\ Byzantine agreement and a  fraction of good processors agree on the result. Recall that the a.e. Byzantine agreement with common coins protocol succeeds w.h.p., if  fraction of processors in the node are good
and bits can be generated so that at least  are random and for each of these, there is a fraction of  fraction of good processors which agree on it. 
Then an election is good if  (1)  is a good node; (2) at least  contestants are good processors  with good paths from 
the assigned  level 1 node to   so that that secrets are correctly transmitted up the tree without the adversary learning the secret until it it released; and (3) there must be a  fraction of level 1 nodes in 's subtree which have good paths to , so that a  fraction of good processors in  learn the random bin selections and random bits of the good arrays that are competing.  Finally, if fewer than  good arrays compete in an election, the probability of correctness of the election is diminished, see Section~\ref{election}. 
  
 Condition (3) is sufficient to show that  between the time the secret is released and the time the processors in the node   learn the secret, the adversary can not selectively decide to prevent the secret from being learned by taking over the processors which know the secret.  As all the secrets are sent down together to all the descendants of , to prevent learning of a secret at , the adversary must prevent a majority of level 1 nodes from hearing from . This would require taking over enough nodes so that half the paths from  to the leaves have at least one bad node in them, in which case we would view the election as bad and all arrays contending in it  bad.  While the adversary does have the ability to selectively make bad an entire election, this does not significantly affect the number of good arrays, since with high probability all elections return a representative fraction of good arrays.  See  Section \ref{election}. Nor does it affect the random bits which are later to be revealed, as these remain hidden from the adversary.  See  Section \ref{s:correct-comm}.

We  now lower  bound the fraction of arrays which remain which are good.

\begin{lemma}
At least a  fraction of  winning arrays are good on every level , that is, they are generated by good processors and
they are known by  fraction of good processors in their election node. 
In particular, the protocol can be used to generate a sequence of random words, of length  of which a  fraction are random and known to  fraction of good processors. 
\end{lemma}


\begin{proof}
With high probability, each good election causes an increase of  no more than a  in the fraction of bad arrays, unless
there are too few () good arrays participating.   But the latter cannot happen too often.  Let  be the number of contestants.
 If there are  bad arrays overall on level  then the total number of such lop-sided elections  is less than . A representative set
 of winners would have  fraction of good arrays. Since the number of candidates , the fraction of good arrays lost this way is less  than .
   So in total, the fraction of arrays which are good decreases by no more than  on a given level because of good election results.

 We now examine the effect of bad nodes. Each node makes bad any paths that run through it.  A fraction of  level 1 nodes are bad. In the worst case, all arrays that pass through bad nodes and bad elections are good. Hence  a  fraction of  bad nodes may eliminate a
 fraction of good arrays on level 1 and be responsible for making a  fraction of elections bad in any level  by making bad half the paths of those elections, thus eliminating an additional fraction of  good arrays.  On level 2, an additional  fraction of elections may be made bad by the bad nodes in that level and so on.  Each bad election eliminates all good arrays which pass through it. Note that a bad election does not make additional paths bad, as information and secrets can still be passed through a good node that holds a bad election. 

 Initially a  fraction of the arrays are good. Assuming this is true,  the bad elections  may eliminate no more than  fraction of good arrays.  Thus the fraction of elections which become bad on level   is no more than a total of less than .  Hence the fraction of good arrays at the root node is as stated. 
\end{proof}  

\section{Almost Everywhere to Everywhere}\label{s:AE2E}
We call a processor {\it knowledgeable} if it is good and agrees on a message .   Otherwise, if it is good, it is {\it confused}.  We assume that almost all , i.e., ,  of the processors are  knowledgeable and can come to agreement on a random number   in . We assume private channels.  Here is the protocol.

\begin{algorithm}

\caption{Almost Everywhere To Everywhere with Global Coin}

\begin{enumerate}

\item
Each processor  does the following in parallel:  \\
Randomly pick   and ,  and send  a {\it request label}  to processor .
\item \label{random}
Almost all good processors agree on a random number   in .

\item \label{received}
For each processor , if  receives request label  from   and  then  
if  has not received more than  such messages (it is not {\it overloaded}) ,  
 returns  a message to , 
\item \label{majority}
Let   be the number of messages returned to  by processors sent the request label . Let  be an  such that  . If  the same message   is  returned by   processors which  were sent the request label  then  decides  .


\end{enumerate}
\end{algorithm}

\subsection{Proof of correctness}

\begin{lemma}\label{singleLoop}
Assume at the start of the protocol  good processors agree on a message  and can generate a  random bit.  Let  be any constant > 0. Then after a single execution of the loop:
\begin{enumerate}
\item
With probability , this protocol results in agreement on . 
\item
With probability , every processor either agrees on  or is undecided.
\end{enumerate}
\end{lemma} 


To prove Lemma \ref{singleLoop} we first prove two other lemmas.
 
 \begin{lemma} \label{requests} Suppose there are  knowledgeable processors.  W.h.p., for any one loop, for every processor  and every request label ,
 at least  processors which are sent  by  are knowledgeable and fewer than  processors which are sent   by  are corrupt or confused.
 
 \end{lemma}
 \begin{proof}
 Since there are private channels, the adversary does not know 's requests other than those sent to bad processors .  Hence the choice of the set  of processors which are not knowledgeable is independent of the queries,  and each  event  consisting of a processor querying a knowledgeable processor is an independent random variable. 
 
 Let  be the number of knowledgeable processors sent a value  by processor . .
 Since  is the sum of independent random variables we use
 Chernoff bounds:   which is less than 
  for .
 
Taking a union bound over all  and  processors ,  for all ,  
is less than . The second part of Lemma \ref{requests} is shown similarly.
\end{proof}

Lemma \ref{requests} immediately implies statement (2) of Lemma \ref{singleLoop}.


We now show Lemma \ref{singleLoop} (1). A knowledgeable processor  which is sent  will respond unless overloaded. Each processor can receive no more than  requests, or the sender is evidently corrupt.
Then there can be no more than   values of   for which there are more than   requests labelled . Then we claim:

 
 \begin{lemma}\label{overload}
The probability that more than  knowledgeable processors are overloaded is less than .
\end{lemma}

\begin{proof} 
We call a value  for a processor overloaded if  request labels equal .
A processor is only overloaded if  and  is overloaded.  Since  is randomly chosen, each processor has 
at most a  chance of being overloaded.  Let  be the number of overloaded knowledgeable processors and
 be the number of knowledgeable processors.
Then . Using Markov's Inequality, . 
\end{proof}
Similar to the argument above, because the adversary does not know the requests and request labels of the requests 
sent to knowledgeable processors, the event sof choosing knowledgeable  processors which are not overloaded are independent random variables and Chernoff bounds apply. With probability , there are  knowledgeable processors which are not overloaded.
Setting  to  in Lemma~ \ref{requests},  we  have w.h.p., for every processor and request label  that  processor
and .  Therefore, with probability , one loop of this protocol results in agreement on .   As each repetitions of the loop are independent, the probability that they all fail is the product of their individual failure
probabilities, implying the following.
 
\begin{lemma}
Repeating the protocol  times,  the probability that all processors agree on  and no processor outputs a different message is . 

\end{lemma}



\section{Everywhere Byzantine Agreement}
We run the Almost Everywhere Agreement  protocol modified as in Section \ref{GCS}  to solve the global coin subsequence problem, i.e., it generates a polylogarithmic length sequence containing a subsequence of   bits  random numbers  generated uniformly and independently at random which are
known to  processors and are in the range .
At each step below,  generates the  number in the sequence.  Since the number of  good
random numbers is greater than , the protocol is successful with probability . 

\begin{algorithm}
\caption{Everywhere Byzantine Agreement}

\begin{enumerate}
\item
Run   to come to almost everywhere consensus on a bit;
\item
For  to   do
\begin{enumerate}
\item
 GenerateSecretNumber
\item
Run 
\end{enumerate}
\end{enumerate}
\end{algorithm}

Finally,  it is easy to see that each execution of the  takes  bits per
processor, which dominates the cost per processor. As there are polylogarithmic number () of rounds, the communication cost  of  per processor remains   bits while the time is polylogarithmic. 
\old{

\subsection{If the confused processors don't know who they are}
Here we allow the good processors to choose different values of  as determined by their ID and a commonly agreed upon string of  bits. 

We assume almost all good processors agree on a string  of  random bits, and  each processor  uses these bits and its ID to map to an , i.e.,i the ID's are , then the set of possible mappings  is given by  . We denote this  by .
A processor   is considered {\it overloaded} if it receives more than  queries labelled . We would like to show that 
with high probability, no more than  processors who are not confused are overloaded.
 
  Each processor can receive less than   queries partitioned over   values, so that  the adversary can overload a processor with requests with probability .  If the queries are set randomly and independently by each processor, the  expected number of overloaded processors is . Using Chernoff bounds we can show that the probability of overloading a constant fraction of
processors in less than .  

However, a random function would require  bits. Instead, we need to show there exists a function with a similar property. Let , such that for all , .
A mapping  is bad for   and  if for all , .
   We show that there exists a collection of     such that for all    and all  with , there are fewer than  mappings are bad for  and .

 
Suppose we randomly pick  mappings . 

Fix a subset   of nodes of size ,  and an . The probability that
a single mapping   is bad for  and  is   since there is a  chance that
 for each  and there are .
 Let  be the number of  which are bad for  and . Then  
. Then using Chernoff bounds  .



Taking the union bound, the probability  mappings are bad for any  and  is  bounded above by the sum over the number of ways to choose  times the number of ways to choose  so as to make the mapping bad for  and :



Thus the probability that  mappings are bad for any  and  is less than 1, which implies there exists
a collection of   mappings such that with probability , the adversary cannot overload more than  fraction of knowledgeable processors.  We also note that the probability that good processors overload a processor with queries for any particular value
  is less than , since the number would have to exceed the expected number of  by
a factor of . the expected number of such queries is , the choices are independent and uniformly random less than 
 using Chernoff bounds as it would imply that the number of queries for any given value received by the processor is
greater than a factor of  times the expected number and
Hence we  modfiy the line \ref{received} as follows:

\indent{
For each processor , if  receives  from   and  then  
{\it if  has not received more than  such messages (it is not {\it overloaded} ),  }
 returns the  message to  along with .}
Now, confused processors will not send too many messages. 

The decision procedure (line \label{majority} is modified as follows:
\indent{
Let   be the number of messages returned by processors  sending the string  such that . Let  be an  such that  .
Then  takes the majority of messages returned to it by processors to which it sent the value .}
}


\section{Conclusion}
We have described an algorithm that solves the Byzantine agreement problem with each processor sending only  bits.  Our algorithm succeeds against an adaptive, rushing adversary in the synchronous communication model.  It assumes private communication channels but makes no other cryptographic assumptions.  Our algorithm succeeds with high probability and has latency that is polylogarithmic in .  Several important problems remain including the following:  Can we use  bits per processor, or alternatively prove that  bits are necessary for agreement against an adaptive adversary?  Can we adapt our results to the asynchronous communication model?  Can we use the ideas in this paper to perform scalable, secure multi-party computation for other functions?  Finally, can the techniques in this paper be used to create a practical Byzantine agreement algorithm for real-world, large networks?

\pagebreak

\bibliography{security}
\bibliographystyle{plain}

\appendix

\section{Appendix}

\subsection{Proof of Lemma~\ref{l:analysis}} \label{s:pf-analysis}

\begin{proof}
We first analyze running time. There are   rounds in the first execution of  Step 2(c)  and  rounds on the second and later executions of
Step 2(c) and Step 2.  Each round takes the time needed to traverse up and down to the node running the election or . 
The total running time is .

We now consider the number of bits communicated per processor. We note that each processor appears in all node only polylogarithmic number of times. Hence it suffices to bound the cost per appearance of processor in a node to get  a  result. 
Step 1 requires each processor to  generate   words. Each share takes the same number of bits as the secret shared,  and there are
 shares. When a processor in a level 1 node receives its share,  it  shares it with its parent node via  uplinks, for a total of  words.  sent by each processor. 

In the first execution of Step 2 (a) and (b) , every processor in every node  on level 2 has  2-shares of the  first blocks from its children, each containing  words. These are  sent from every node  on level 2 down its uplinks
to processors in all its  level 1 children,  so that each processor in   sends down  words in total. Here,  is the maximum number of uplinks from a single child that a processor in a node is incident to. The 1-shares are reconstructed from the 2-shares and then the 1-shares are shared with the other processors in the level 1 node, with each level 1 processor sending  words in total. 

 Step 2(c) requires  shares of arrays from  winners, or a total of  blocks to be sent secretly to the next level.  Each block has size  and is shared among  processors, where  is the number of a processor's uplinks. 
 for a total of  words sent. 
 
In the second execution of Step 2(b) and (c) all shares of all  blocks of all  candidates are sent down from  at level 3.  Each processor has received 
2-shares from   candidates, hence it sends down  shares of blocks of size  or  a total of 
words.
On level  the 2-shares are converted to 1-shares and each 1-share is sent to  processors, for a total of  words sent to  processors or
 words sent. 
The processors in level 1 nodes each determine  numbers which they communicate back to the level 3 nodes via the  to their neighbors in .
Since each level 1 node is incident to   , for any level  node, the cost of this is  words.


In the third execution of Step 2(b) and (c), again all shares of  blocks of all  candidates are sent down. Each has size  and the analysis is similar, except for one item. As the levels increase to level , the number of candidates from which a candidate has received -shares
increases by a  factor. Hence, each processor at level  sends down  words. 

Step 3 is dominated by Step 2.

The total number of bits sent per processor is the number of times a processor appears in a node on any level times the number of levels times
the costs described above. These additional factors  for appearances and levels are subsumed by the  notation as they are polylog .
That is, the cost is determined by summing up the above amounts with the exception of one term which increases per level, that is, . Hence the cost is 

 

.

Since , ,  ,   and  to , then setting  , ,   we have that  the total cost is dominated by the last term and .

I.e., there is a running time of  and a bit complexity per processor of
. 
\end{proof}


\subsection{Almost Everywhere Byzantine Agreement (AEBA) with Unreliable Global Coins}\label{AEBACC}

\begin{algorithm}
\caption{AEBA with Unreliable Coins} \label{alg:aebasparse}
Set ; For each round do the following:
\begin{enumerate*}
\item Send  to all neighbors in ;
      \label {step_a}
\item Collect votes from neighbors in ;
      \label {step_b}
\item  majority bit among votes received;
\item  fraction of votes received for ;
\item  result of call to algorithm GetGlobalCoin;
\item If  then 
\item else
\begin{enumerate}
\item If  = ``heads'', then , else ;
\end{enumerate}
\end{enumerate*}

At the end of all rounds, commit to  as the output bit;
\end{algorithm}

\subsection{Analysis}

We assume here that the fraction of bad processors is no more than  for some fixed .  For a processor , let  be the set of neighbors of  in the sparse graph  and let  be the number of nodes in this graph.  We say that a call to  \emph{succeeds}, when it selects a bit  with uniform probability and independently from all past events, and that all but  processors learn .

\begin{theorem} Assume there are at least  rounds in Algorithm~\ref{alg:aebasparse} where the call to  succeeds.  Let  and  be any positive constants and  depend only on  and .  Then, at the end of the algorithm, for any positive constants  and  with probability at least , all but  of the good processors commit to the same vote , where  was the input of at least one good processor.   This occurs provided that the graph  is a random  regular graph.
\end{theorem}

Before proving this theorem we establish the following lemmas.

For a fixed round, let  be the bit that the majority of good processors vote for in that round.  Let  be the set of good processors that will vote for  and let .  Let  be a fixed positive constant to be determined later.  We call a processor \emph{informed} for the round if the fraction value for that processor obeys the following inequalities:



\begin{lemma}
For any fixed  and , with probability at least , in any given round of Algorithm~\ref{alg:aebasparse}, all but  of the good processors are informed, for  a  regular graph where  depends only on ,  and . 
\end{lemma}

\begin{proof}
Fix the set , we know that  is of size at least  since at least half of the good processors must vote for the majority bit.  Let . We will also fix a set  which consists of all the processors that have .  We will first show that the probability that  is of size  for some constant  is very small for fixed  and , and will then show, via a union bound, that with high probability, for any  there is no set of  processors with .  Finally, we will use a similar technique to show that with high probability, no more than  processors have .  This will complete the proof.

To begin, we fix the set  of size at least  and fix  of size .  Let  be the event that all processors in  have .  Let  be the number of edges from  to .  Since the graph  is  regular, we know that .  We will find an upper-bound on the latter probability by using a random variable  that gives the number of edges from  to  if the graph  were generated by having  edges from each vertex with endpoint selected uniformly at random.  In particular,  is the number of edges between the two sets if  is a random regular graph, and  is the number of edges if  is a graph where the out degree of each node is the same but the in-degrees may differ.  We know that  since the model for generating  assumes sampling without replacement and that for  assumes sampling with replacement.  We will thus bound the probability of deviation for .  Note that , and so by Chernoff bounds, we can say that 



Where the last step holds since .  Let  be the union of events  for all possible values of  and .  Then we know by union bounds that



Where the last equation holds for any constant , provided that  is sufficiently large but depends only on the constants  and .  We have thus shown that with high probability, the number of processors with  is no more than .  

By a similar analysis, letting  consist of the union of  and the set of bad processors, we can show that with high probability, the number of processors with  is no more than .  These two results together establish that with high probability, in any round of the algorithm, all but  processors are informed for any constant , provided that  is chosen sufficiently large with respect to  and .
\end{proof}


The following Lemma establishes validity (that the output bit will be the same as the input bit of one good processor) and will also be helpful in establishing consistency (that all but  good processors will output the same bit).

\begin{lemma} \label{l:validity}
If in any given round all but  good processors vote for the same value , for some constant , then for every remaining round, all but  good processors will vote for .
\end{lemma}

\begin{proof}
We will show that if all but  good processors vote for the same value  in some round , then in round , all but  good processors will vote for .  Consider what happens after the votes are received in round .  We know that for this round,  for  sufficiently large.  Thus, every informed processor in that round will have , and so every informed processor will set its vote value, at the end of the round, to .  It follows that all the processors that were informed in round  will vote for  in round .  Note that this result holds irrespective of the outcome of GetGlobalCoin for the round, even including the case where different processors receive different outcomes from that subroutine.
\end{proof}

\begin{lemma} \label{l:term}
If the call to GetGlobalCoin succeeds in some round (i.e. the same unbiased coin toss is returned to all but  good players), then with probability at least , at the end of that round, all but  good processors will have a vote value equal to the same bit.
\end{lemma}

\begin{proof}
Fix a round where the call to GetGlobalCoin succeeds.  There are two main cases\\
\noindent
Case 1: No informed processor has .  In this case, at the end of the round, with probability , all but  processors will set their vote to the same bit.
\noindent
Case 2: At least one informed processor has .  We first show that in this case, all informed processors that have 
 will set their vote to the same value at the end of the round.  We show this by contradiction.  Assume there are two processors,  and , where  () are the fraction values of  (), such that both  and  are greater than or equal to , and  sets its vote to  at the end of the round, while  sets its vote to .

Let  () be the fraction of good processors that vote for  () during the round.  Then we have that .  By the definition of informed, we also know that .  This implies that
  Isolating  in this inequality, we get that


A similar analysis for  implies that


But then, for  sufficiently small, we have , which is a contradiction.  

Now, let  be the value that all good and informed processors with  set their value to at the end of the round.   With probability , the outcome of the GetGlobalCoin is equal to  and in this case, all but  informed processors will set their vote value to the same bit  at the end of the round.
\end{proof}


We can now prove Theorem~\ref{t:aebasparse}.\\
\begin{proof}
Lemma~\ref{l:validity} establishes validity: if all processor initially start with the same input bit, then all but  of the processors will eventually commit to that bit, with probability at least .  Lemmas~\ref{l:term} and~\ref{l:validity} together establish that the probability of having a round in which all but  processors come to agreement (and after which all but  processors  will stay in agreement) is at least  where  is the number of rounds in which  succeeds.  A simple union bound on the probabilities of error then establishes the result of the theorem.
\end{proof}

\end{document}
