


\begin{abstract}
Monocular depth estimation is fundamental for 3D scene understanding and downstream applications. However, even under the supervised setup, it is still challenging and ill-posed due to the lack of full geometric constraints. Although a scene can consist of millions of pixels, there are fewer high-level patterns. We propose \ourmodel to learn those patterns with internal discretized representations. The method implicitly partitions the scene into a set of high-level patterns. 
In particular, our new module, \ourmodulename (\ourmodule), implements a continuous-discrete-continuous bottleneck to learn those concepts without supervision. In contrast to state-of-the-art methods, the proposed model does not enforce any explicit constraints or priors on the depth output. 
The whole network with the \ourmodule module can be trained end-to-end, thanks to the bottleneck module based on attention. Our method sets the new state of the art with significant improvements on NYU-Depth v2 and KITTI, outperforming all published methods on the official KITTI benchmark. \ourmodel can also achieve state-of-the-art results on surface normal estimation. 
Further, we explore the model generalization capability via zero-shot testing. We observe the compelling need to promote diversification in the outdoor scenario. Hence, we introduce splits of two autonomous driving datasets, DDAD and Argoverse. Code is available at \url{http://vis.xyz/pub/idisc}.
\end{abstract}

\vspace{-10pt}
\section{Introduction}
\label{sec:intro}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.495\columnwidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/teaser/image_or.pdf}
        \caption{Input image}
    \end{subfigure}
    \begin{subfigure}[b]{0.495\columnwidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/teaser/preds.pdf}
        \caption{Output depth}
    \end{subfigure}
    \par\smallskip
    \begin{subfigure}[b]{0.495\columnwidth}
        \centering
            \includegraphics[width=1\linewidth]{figures/teaser/resols_overlay.pdf}
        \caption{Intermediate representations}
         \label{fig:intermediate_repr}
    \end{subfigure}
    \begin{subfigure}[b]{0.495\columnwidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/teaser/mask_smooth.pdf}
        \caption{Internal discretization}
        \label{fig:internal_disc}
    \end{subfigure}\\
    \vspace{-5pt}
    \caption{
    We propose \ourmodel which implicitly enforces an internal discretization of the scene via a continuous-discrete-continuous bottleneck. Supervision is applied to the output depth only, \ie, the fused intermediate representations in \subref{fig:intermediate_repr}, while the internal discrete representations are implicitly learned by the model. \subref{fig:internal_disc} displays some actual internal discretization patterns captured from the input, \eg, foreground, object relationships, and 3D planes. Our \ourmodel model is able to predict high-quality depth maps by capturing scene interactions and structure.}
\label{fig:teaser}
\vspace{-15pt}
\end{figure}

Depth estimation is essential in computer vision, especially for understanding geometric relations in a scene. This task consists in predicting the distance between the projection center and the 3D point corresponding to each pixel. Depth estimation finds direct significance in downstream applications such as 3D modeling, robotics, and autonomous cars. Some research~\cite{Zhou2019} shows that depth estimation is a crucial prompt to be leveraged for action reasoning and execution. In particular, we tackle the task of monocular depth estimation (MDE). MDE is an ill-posed problem due to its inherent scale ambiguity: the same 2D input image can correspond to an infinite number of 3D scenes.

State-of-the-art (SotA) methods typically involve convolutional networks~\cite{Eigen2014, Fu2018, Lee2019} or, since the advent of vision Transformer~\cite{Dosovitskiy2020VIT}, transformer architectures~\cite{Bhat2020, Yang2021, Ranftl2021, Yuan2022}. Most methods either impose geometric constraints on the image~\cite{Yin2019, Huynh2020, Long2021, Patil2022}, namely, planarity priors or explicitly discretize the continuous depth range~\cite{Fu2018, Bhat2020, Bhat2022}. The latter can be viewed as learning frontoparallel planes. These imposed priors inherently limit the expressiveness of the respective models, as they cannot model \emph{arbitrary} depth patterns, ubiquitous in real-world scenes.

We instead propose a more general depth estimation model, called \ourmodel, which does not explicitly impose any constraint on the final prediction. We design an \ourmodulename(\ourmodule) of the scene which is in principle depth-agnostic. Our assumption behind this \ourmodule is that each scene can be implicitly described by a set of concepts or patterns, such as objects, planes, edges, and perspectivity relationships. The specific training signal determines which patterns to learn (see \cref{fig:teaser}). 

We design a continuous-to-discrete bottleneck through which the information is passed in order to obtain such internal scene discretization, namely the underlying patterns. In the bottleneck, the scene feature space is partitioned via learnable and input-dependent quantizers, which in turn transfer the information onto the continuous output space. The \ourmodule bottleneck introduced in this work is a general concept and can be implemented in several ways. Our particular \ourmodule implementation employs attention-based operators, leading to an end-to-end trainable architecture and input-dependent framework. More specifically, we implement the continuous-to-discrete operation via ``transposed'' cross-attention, where transposed refers to applying  on the output dimension. This  formulation enforces the input features to be routed to the internal discrete representations (IDRs) in an exclusive fashion, thus defining an input-dependent soft clustering of the feature space. The discrete-to-continuous transformation is implemented via cross-attention. Supervision is only applied to the final output, without any assumptions or regularization on the IDRs. 

We test \ourmodel on multiple indoor and outdoor datasets and probe its robustness via zero-shot testing. As of today, there is too little variety in MDE benchmarks for the outdoor scenario, since the only established benchmark is KITTI~\cite{Geiger2012}. Moreover, we observe that all methods fail on outdoor zero-shot testing, suggesting that the KITTI dataset is not diverse enough and leads to overfitting, thus implying that it is not indicative of generalized performance. Hence, we find it compelling to establish a new benchmark setup for the MDE community by proposing two new train-test splits of more diverse and challenging high-quality outdoor datasets: Argoverse1.1~\cite{Chang2019} and DDAD~\cite{Guizilini2020}. 

Our main contributions are as follows: (i) we introduce the \ourmodulename module, a novel architectural component that adeptly represents a scene by combining underlying patterns; (ii) we show that it is a generalization of SotA methods involving depth ordinal regression~\cite{Fu2018, Bhat2020}; (iii) we propose splits of two raw outdoor datasets~\cite{Chang2019, Guizilini2020} with high-quality LiDAR measurements. We extensively test \ourmodel on six diverse datasets and, owing to the \ourmodule design, our model consistently outperforms SotA methods and presents better transferability. Moreover, we apply \ourmodel to surface normal estimation showing that the proposed module is general enough to tackle generic real-valued dense prediction tasks.

\section{Related Work}
\label{sec:relwork}

\begin{figure*}[t]
    \centering
    \includegraphics[width=1\textwidth]{figures/model_up.pdf}
    \vspace{-15pt}
    \caption{\textbf{Model Architecture.} The \ourmodulename Module imposes an information bottleneck via two consecutive stages: continuous-to-discrete (C2D) and discrete-to-continuous (D2C). The module processes multiple resolutions, \ie, , independently in parallel. The bottleneck embodies our assumption that a scene can be represented as a set of patterns. The C2D stage aggregates information, given a learnable prior (), from the -th resolution feature maps () to a finite set of IDRs (). In particular, it learns how to define a partition function that is dependent on the input  via transposed cross-attention, as in \eqref{eqn:slotattn}. The second stage (D2C) transfers the IDRs on the original continuous space using layers of cross-attention as in \eqref{eqn:crossattn}, for sake of simplicity, we depict only a generic -th layer. Cross-attention is guided by the similarity between decoded pixel embeddings () and . The final prediction () is the fusion, \ie, mean, of the intermediate representations .}
    \label{fig:model}
    \vspace{-15pt}
\end{figure*}

The supervised setting of MDE assumes that pixel-wise depth annotations are available at training time and depth inference is performed on single images. The coarse-to-fine network introduced in Eigen~\etal\cite{Eigen2014} is the cornerstone in MDE with end-to-end neural networks. The work established the optimization process via the Scale-Invariant log loss (). Since then, the three main directions evolve: new architectures, such as residual networks~\cite{Laina2016}, neural fields~\cite{Liu2015, Xu2018}, multi-scale fusion~\cite{Lee2018, Miangoleh2021}, transformers~\cite{Yang2021, Bhat2020, Yuan2022}; improved optimization schemes, such as reverse-Huber loss~\cite{Laina2016}, classification~\cite{Cao2016}, or ordinal regression~\cite{Fu2018, Bhat2020}; multi-task learning to leverage ancillary information from the related task, such as surface normals estimation or semantic segmentation~\cite{Eigen2014, Qi2018, Xu2018tasks}.

\noindent{}\textbf{Geometric priors} have been widely utilized in the literature, particularly the piecewise planarity prior~\cite{Gallup2010, Chauve2010, Szomoru2014}, serving as a proper real-world approximation. The geometric priors are usually incorporated by explicitly treating the image as a set of planes~\cite{Liu2018PlaneNet, Liu2018PlaneRCNN, Yu2019, Li2021}, using a plane-inducing loss~\cite{Yu2020p2net}, forcing pixels to attend to the planar representation of other pixels~\cite{Lee2019, Patil2022}, or imposing consistency with other tasks' output~\cite{Yin2019, Long2021, Bae2022}, like surface normals. Priors can focus on a more holistic scene representation by dividing the whole scene into 3D planes without dependence on intrinsic camera parameters~\cite{Yang2018, Zhang2020}, aiming at partitioning the scene into dominant depth planes. In contrast to geometric prior-based works, our method lifts any explicit geometric constraints on the scene. Instead, \ourmodel implicitly enforces the representation of scenes as a set of high-level patterns.

\noindent{}\textbf{Ordinal regression} methods~\cite{Fu2018, Bhat2020, Bhat2022} have proven to be a promising alternative to other geometry-driven approaches. The difference with classification models is that class ``values'' are learnable and are real numbers, thus the problem falls into the regression category. The typical SotA rationale is to explicitly discretize the continuous output depth range, rendering the approach similar to mask-based segmentation. Each of the scalar depth values is associated with a confidence mask which describes the probability of each pixel presenting such a depth value. Hence, SotA methods inherently assume that depth can be represented as a set of frontoparallel planes, that is, depth ``masks''. 

The main paradigm of ordinal regression methods is to first obtain hidden representations and scalar values of discrete depth values. The dot-product similarity between the feature maps and the depth representations is treated as logits and  is applied to extract confidence masks (in Fu~\etal~\cite{Fu2018} this degenerates to ). Finally, the final  prediction is defined as the per-pixel weighted average of the discrete depth values, with the confidence values serving as the weights. \ourmodel draws connections with the idea of depth discretization. However, our \ourmodule module is designed to be depth-agnostic. The discretization occurs at the abstract level of \emph{internal} features from the \ourmodule bottleneck instead of the output depth level, unlike other methods.

\noindent{}\textbf{Iterative routing} is related to our ``transposed'' cross-attention. The first approach of this kind was Capsule Networks and their variants \cite{Sabour2017, Hinton2018capsule}. Some formulations~\cite{Tsai2020capsules, Locatello2020} employ different kinds of attention mechanisms. Our attention mechanism draws connections with~\cite{Locatello2020}. However, we do not allow permutation invariance, since our assumption is that each discrete representation internally describes a particular kind of pattern. In addition, we do not introduce any other architectural components such as gated recurrent units (GRU). In contrast to other methods, our attention is employed at a higher abstraction level, namely in the decoder.

\section{Method}
\label{sec:method}

We propose an \ourmodulename (\ourmodule) module, to discretize the internal feature representation of encoder-decoder network architectures. We hypothesize that the module can break down the scenes into coherent concepts without semantic supervision. This section will first describe the module design and then discuss the network architecture. \cref{sec:method_idd_afp} defines the formulation of ``transposed'' cross-attention outlined in \cref{sec:intro} and describes the main difference with previous formulations from \cref{sec:relwork}. Moreover, we derive in \cref{sec:method_idd_sd} how the \ourmodel formulation can be interpreted as a generalization of SotA ordinal regression methods by reframing their original formulation. Eventually, \cref{sec:method_arch} presents the optimization problem and the overall architecture.

\subsection{\ourmodulename Module}
\label{sec:method_idd}
The \ourmodule module involves a continuous-discrete-continuous bottleneck composed of two main consecutive stages. The overall module is based on our hypothesis that scenes can be represented as a finite set of patterns. The first stage consists in a continuous-to-discrete component, namely soft-exclusive discretization of the feature space. More specifically, it enforces an input-dependent soft clustering on the feature maps in an image-to-set fashion. The second stage completes the internal scene discretization by mapping the learned IDRs onto the continuous output space. IDRs are not bounded to focus exclusively on depth planes but are allowed to represent any high-level pattern or concept, such as objects, relative locations, and planes in the 3D space. In contrast with SotA ordinal regression methods~\cite{Fu2018, Bhat2020, Bhat2022}, the IDRs are neither explicitly tied to depth values nor directly tied to the output. Moreover, our module operates at multiple intermediate resolutions and merges them only in the last layer. The overall architecture of \ourmodel, particularly our \ourmodule module, is shown in Fig.~\ref{fig:model}.

\subsubsection{Adaptive Feature Partitioning}
\label{sec:method_idd_afp}
The first stage of our \ourmodule module, \emph{Adaptive Feature Partitioning} (AFP), generates proper discrete representations () that quantize the feature space () at each resolution . We drop the resolution superscript  since resolutions are independently processed and only one generic resolution is treated here. \ourmodel does not simply learn fixed centroids, as in standard clustering, but rather learns how to define a partition function in an input-dependent fashion. More specifically, an iterative transposed cross-attention module is utilized. Given the specific input feature maps (), the iteration process refines (learnable) IDR priors () over  iterations. 

More specifically, the term ``transposed'' refers to the different axis along which the  operation is applied, namely  instead of the canonical dot-product attention , with  as query, key and value tensors, respectively. In particular, the tensors are obtained as projections of feature maps and IDR priors, , , . The -th iteration out of  can be formulated as follows:

where  are query, key and value respectively,  is the number of IDRs, nameley, clusters, and  is the number of pixels. The weights  may be normalized to 1 along the  dimension to avoid vanishing or exploding quantities due to the summation of un-normalized distribution. 

The quantization stems from the inherent behavior of . In particular,  forces competition among outputs: one output can be large only to the detriment of others. Therefore, when fixing , namely, given a feature, only a few attention weights () may be significantly greater than zero. Hence, the content  is routed only to a few IDRs at the successive iteration. Feature maps are fixed during the process and weights are shared by design, thus  are the same across iterations. The induced competition enforces a soft clustering of the input feature space, where the last-iteration IDR represents the actual partition function (). The probabilities of belonging to one partition are the attention weights, namely  with -th query fixed. Since attention weights are inherently dependent on the input, the specific partitioning also depends on the input and takes place at inference time. The entire process of AFP leads to (soft) mutually exclusive IDRs.

As far as the partitioning rationale is concerned, the proposed AFP draws connections with iterative routing methods described in \cref{sec:relwork}. However, important distinctions apply. First, IDRs are not randomly initialized as the ``slots'' in Locatello~\etal~\cite{Locatello2020} but present a learnable prior. Priors can be seen as learnable positional embeddings in the attention context, thus we do not allow a permutation-invariant set of representations. Moreover, non-adaptive partitioning can still take place via the learnable priors if the iterations are zero. Second, the overall architecture differs noticeably as described in \cref{sec:relwork}, and in addition, \ourmodel partitions feature space at the decoder level, corresponding to more abstract, high-level concepts, while the SotA formulations focus on clustering at an abstraction level close to the input image.


One possible alternative approach to obtaining the aforementioned IDRs is the well-known image-to-set proposed in DETR~\cite{Carion2020}, namely via classic cross-attention between representations and image feature maps. However, the corresponding representations might redundantly aggregate features, where the extreme corresponds to each output being the mean of the input. Studies~\cite{Gao2021, Sun2020} have shown that slow convergence in transformer-based architectures may be due to the non-localized context in cross-attention. The exclusiveness of the IDRs discourages the redundancy of information in different IDRs. We argue that exclusiveness allows the utilization of fewer representations (32 against the 256 utilized in \cite{Bhat2020} and \cite{Fu2018}), and can improve both the interpretability of what IDRs are responsible for and training convergence. 


\subsubsection{Internal Scene Discretization}
\label{sec:method_idd_sd}
In the second stage of the \ourmodule module, \emph{Internal Scene Discretization} (ISD), the module ingests pixel embeddings () from the decoder and IDRs  from the first stage, both at different resolutions , as shown in \cref{fig:model}. Each discrete representation carries both the signature, as the \textit{key}, and the output-related content, as the \textit{value}, of the pattern it represents. The similarity between IDRs and pixel embeddings is computed in order to spatially localize in the continuous output space where to transfer the information of each IDR. We utilize the dot-product similarity function. 

Furthermore, the kind of information to transfer onto the final prediction is not constrained, as we never explicitly handle depth values, usually called bins, until the final output. Thus, the IDRs are completely free to carry generic high-level concepts (such as object-ness, relative positioning, and geometric structures). This approach is in stark contrast with SotA methods \cite{Fu2018, Bhat2020, Li2022DF, Bhat2022}, which explicitly constrain what the representations are about: scalar depth values. Instead, \ourmodel learns to generate unconstrained representations in an input-dependent fashion. The effective discretization of the scene occurs in the second stage thanks to the information transfer from the set of exclusive concepts () from AFP to the continuous space defined by . We show that our method is not bounded to depth estimation, but can be applied to generic continuous dense tasks, for instance, surface normal estimation. Consequently, we argue that the training signal of the task at hand determines how to internally discretize the scene, rendering our \ourmodule module general and usable in settings other than depth estimation.

From a practical point of view, the whole second stage consists in cross-attention layers applied to IDRs and pixel embeddings. As described in \cref{sec:method_idd_afp}, we drop the resolution superscript . After that, the final depth maps are projected onto the output space and the multi-resolution depth predictions are combined. The -th layer is defined as:

where ,  are pixel embeddings with shape , and  are the  IDRs under linear transformations , . The term  determines the spatial location for which each specific IDR is responsible, while  carries the semantic content to be transferred in the proper spatial locations.

Our approach constitutes a generalization of depth estimation methods that involve (hybrid) ordinal regression. As described in \cref{sec:relwork}, the common paradigm in ordinal regression methods is to explicitly discretize depth in a set of masks with a scalar depth value associated with it. Then, they predict the likelihood that each pixel belongs to such masks. Our change of paradigm stems from the reinterpretation of the mentioned ordinal regression pipeline which we translate into the following mathematical expression:

where  are the pixel embeddings at maximum resolution and  is the  temperature.  are  depth scalar values and  are their hidden representations, both processed as a unique stacked tensor (). From the reformulation in \eqref{eqn:explicit_disc}, one can observe that \eqref{eqn:explicit_disc} is a degenerate case of \eqref{eqn:crossattn}. In particular,  degenerates to the identity function.  and  degenerate to selector functions: the former function selects up to the  dimensions and the latter selects the last dimension only. Moreover, the hidden representations are refined pixel embeddings (), and  in \eqref{eqn:explicit_disc} is the final output, namely no multiple iterations are performed as in \eqref{eqn:crossattn}. The explicit entanglement between the semantic content of the hidden representations and the final output is due to hard-coding  as depth scalar values.


\subsection{Network Architecture}
\label{sec:method_arch}
Our network described in \cref{fig:model} comprises first an encoder backbone, interchangeably convolutional or attention-based, producing features at different scales. The encoded features at different resolutions are refined, and information between resolutions is shared, both via four multi-scale deformable attention (MSDA) blocks~\cite{Zhu2021DefDETR}. The feature maps from MSDA at different scales are fed into the AFP module to extract IDRs (), and into the decoder to extract pixel embeddings in the continuous space (). Pixel embeddings at different resolutions are combined with the respective IDRs in the ISD stage of the \ourmodule module to extract the depth maps. The final depth prediction corresponds to the mean of the interpolated intermediate representations. 
The optimization process is guided only by the established  loss defined in~\cite{Eigen2014}, and no other regularization is exploited.  is defined as:

where  is the predicted depth and  is the ground-truth (GT) value.  and  are computed as the empirical variance and expected value over all pixels, namely, .  is the purely scale-invariant loss, while  fosters a proper scale.  and  are set to  and , as customary. 

\section{Experiments}
\label{sec:experiments}
\begin{figure}
    \centering
    \includegraphics[width=1\columnwidth]{figures/nyu_results_short.pdf}\\
    \footnotesize
    \begin{tabularx}{\columnwidth}{l}
        \hspace{0.02\columnwidth}\textbf{Image+GT}\hspace{0.09\columnwidth}\textbf{AdaBins}\cite{Bhat2020}\hspace{0.055\columnwidth}\textbf{NeWCRF}\cite{Yuan2022}\hspace{0.11\columnwidth}\textbf{Ours}
    \end{tabularx}\\
    \vspace{-5pt}
    \caption{\textbf{Qualitative results on NYU.} Each pair of consecutive rows corresponds to one test sample. Each odd row shows the input RGB image and depth predictions for the selected methods. Each even row shows GT depth and the prediction errors of the selected methods clipped at 0.5 meters. The error color map is \textit{coolwarm}: blue corresponds to lower error values and red to higher values.}
    \label{fig:nyu_results}
    \vspace{-10pt}
\end{figure}

\subsection{Experimental Setup}
\label{sec:experiments_expsetup}
\subsubsection{Datasets}

\noindent{}\textbf{NYU-Depth V2.} NYU-Depth V2 (NYU)~\cite{Silberman:ECCV12} is a dataset consisting of 464 indoor scenes with RGB images and quasi-dense depth images with 640480 resolution. Our models are trained on the train-test split proposed by previous methods~\cite{Lee2019}, corresponding to 24,231 samples for training and 654 for testing. In addition to depth, the dataset provides surface normal data utilized for normal estimation. The train split used for normal estimation is the one proposed in~\cite{Yin2019}.

\noindent{}\textbf{Zero-shot testing datasets.} We evaluate the generalizability of indoor models on two indoor datasets which are not seen during training. The selected datasets are SUN-RGBD~\cite{Song2015} and DIODE-Indoor~\cite{Vasiljevic2019}. For both datasets, the resolution is reduced to match that of NYU, which is 640480.

\noindent{}\textbf{KITTI.} The KITTI dataset provides stereo images and corresponding Velodyne LiDAR scans of outdoor scenes captured from a moving vehicle~\cite{Geiger2012}. RGB and depth images have (mean) resolution of 1241376. The split proposed by~\cite{Eigen2014} (Eigen-split) with corrected depth is utilized as training and testing set, namely, 23,158 and 652 samples. The evaluation crop corresponds to the crop defined by~\cite{Garg2016}. All methods in \cref{sec:experiments_compsota} that have source code and pre-trained models available are re-evaluated on KITTI with the evaluation mask from~\cite{Garg2016} to have consistent results.

\noindent{}\textbf{Argoverse1.1 and DDAD.} We propose splits of two autonomous driving datasets, Argoverse1.1 (Argoverse)~\cite{Chang2019} and DDAD~\cite{Guizilini2020}, for depth estimation. Argoverse and DDAD are both outdoor datasets that provide 360 HD images and the corresponding LiDAR scans from moving vehicles. We pre-process the original datasets to extract depth maps and avoid redundancy. Training set scenes are sampled when the vehicle has been displaced by at least 2 meters from the previous sample. For the testing set scenes, we increase this threshold to 50 meters to further diminish redundancy. Our Argoverse split accounts for 21,672 training samples and 476 test samples, while DDAD for 18,380 training and 860 testing samples. Samples in Argoverse are taken from the 6 cameras covering the full 360 panorama. For DDAD, we exclude 2 out of the 6 cameras since they have more than 30\% pixels occluded by the camera capture system. 
We crop both RGB images and depth maps to have 1920870 resolution that is 180px and 210px cropped from the top for Argoverse and DDAD, respectively, to crop out a large portion of the sky and regions occluded by the ego-vehicle. For both datasets, we clip the maximum depth at 150m. 


\subsubsection{Implementation Details}
\begin{figure}[]
    \centering
    \includegraphics[width=\columnwidth]{figures/nyu_attns.pdf}\\
    \vspace{-10pt}
    \caption{\textbf{Attention maps on NYU for three different IDRs.} Each row presents the attention map of a specific IDR for four test images. Each discrete representation focuses on a specific high-level concept. The first two rows pertain to IDRs at the lowest resolution while the last corresponds to the highest resolution. Best viewed on a screen and zoomed in.}
    \label{fig:nyu_attns}
    \vspace{-10pt}
\end{figure}
\noindent{}\textbf{Evaluation Details.} In all experiments, we do not exploit any test-time augmentations (TTA), camera parameters, or other tricks and regularizations, in contrast to many previous methods~\cite{Fu2018, Lee2019, Bhat2020, Patil2022, Yuan2022}. This provides a more challenging setup, which allows us to show the effectiveness of \ourmodel. 
As depth estimation metrics, we utilize root mean square error () and its log variant (), absolute error in log-scale (), absolute () and squared () mean relative error, the percentage of inlier pixels () with threshold , and scale-invariant error in log-scale (): . The maximum depth for NYU and all zero-shot testing in indoor datasets, specifically SUN-RGBD and Diode Indoor, is set to 10m, while for KITTI it is set to 80m and for Argoverse and DDAD to 150m. Zero-shot testing is performed by evaluating a model trained on either KITTI or NYU and tested on either outdoor or indoor datasets, respectively, without additional fine-tuning. For surface normals estimation, the metrics are mean () and median () absolute error,  angular error, and percentages of inlier pixels with thresholds at , , and . GT-based mean depth rescaling is applied only on Diode Indoor for all methods since the dataset presents largely scale-equivariant scenes, such as plain walls with tiny details.

\noindent{}\textbf{Training Details.} We implement \ourmodel in PyTorch~\cite{pytorch}. For training, we use the AdamW~\cite{Loshchilov2017} optimizer (, ) with an initial learning rate of  for every experiment, and weight decay set to . As a scheduler, we exploit Cosine Annealing starting from 30\% of the training, with final learning rate of . We run 45k optimization iterations with a batch size of 16. 
All backbones are initialized with weights from ImageNet-pretrained models. The augmentations include both geometric (random rotation and scale) and appearance (random brightness, gamma, saturation, hue shift) augmentations. The required training time amounts to 20 hours on 4 NVidia Titan RTX.

\subsection{Comparison with the State of the Art}
\label{sec:experiments_compsota}

\begin{table}[]
    \centering
    \caption{\textbf{Comparison on NYU official test set.} R101: ResNet-101~\cite{He2015}, D161: DenseNet-161~\cite{Huang2016}, EB5: EfficientNet-B5~\cite{Tan2019}, HR48: HRNet-48~\cite{Wang2019}, DD22: DRN-D-22~\cite{Yu2017}, ViTB: ViT-B/16+Resnet-50~\cite{Dosovitskiy2020VIT}, MViT: EfficientNet-B5-AP~\cite{Xie2019}+MiniViT, Swin\{L, B, T\}: Swin-\{Large, Base, Tiny\}~\cite{Liu2021}. (\dag): ImageNet-22k~\cite{Deng2010} pretraining, (\ddag): non-standard training set, (): in-house dataset pretraining, (\textsection): re-evaluated without GT-based rescaling.}\vspace{-10pt}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{ll|ccc|ccc}
        \toprule
        \multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Encoder}} &  &  &  &  &  & \\
        & & \multicolumn{3}{c|}{\textit{Higher is better}} & \multicolumn{3}{c}{\textit{Lower is better}}\\
        \toprule
        Eigen \etal~\cite{Eigen2014} & - &  &  &  &  &   & \\
        DORN~\cite{Fu2018} & R101 &  &  &  &  &   & \\
        VNL~\cite{Yin2019} & - &  &  &  &  &  & \\
        BTS~\cite{Lee2019} & D161 &  &  &  &  &  & \\
        AdaBins\textsuperscript{\ddag}~\cite{Bhat2020} & MViT &  &  &  &  &  & \\
        DAV~\cite{Huynh2020} & DD22 &  &  &  &  &  &  \\
        Long \etal~\cite{Long2021} & HR48 &  &  &  &  &  &  \\
        TransDepth~\cite{Yang2021} & ViTB &  &  &  &  &  & \\
        DPT*\cite{Ranftl2021} & ViTB  &  &  &  &  &  & \\
P3Depth\textsuperscript{\textsection}~\cite{Patil2022} & R101 &  &  &  &  &  & \\NeWCRF~\cite{Yuan2022} & SwinL\textsuperscript{\dag} &  &  &  &  &  & \\
        LocalBins\textsuperscript{\ddag}~\cite{Bhat2022} & MViT &  &  &  &  &  & \\
        \midrule
        \midrule
        Ours 
        & R101 &  &  &  &  &  & \\
        & EB5 &  &  &  &  &  & \\
        & SwinT &  &  &  &  &  & \\
& SwinB &   &  &  &  &  & \\
        & SwinL\textsuperscript{\dag} &  &  &  &  &  & \\
        \bottomrule
    \end{tabular}}
    \label{tab:nyu_res}
    \vspace{-10pt}
\end{table}
\noindent{}\textbf{Indoor Datasets.} Results on NYU are presented in \Cref{tab:nyu_res}. The results show that we set the new state of the art on the benchmark, improving by more than 6\% on  and 9\% on  over the previous SotA. Moreover, results highlight how \ourmodel is more sample-efficient than other transformer-based architectures~\cite{Ranftl2021, Yang2021, Bhat2020, Yuan2022, Bhat2022} since we achieve better results even when employing smaller and less heavily pre-trained backbone architectures. In addition, results show a significant improvement in performance with our model instantiated with a full-convolutional backbone over other full-convolutional-based models~\cite{Eigen2014, Fu2018, Lee2019, Huynh2020, Patil2022}. \Cref{tab:nyu_zeroshot} presents zero-shot testing of NYU models on SUN-RGBD and Diode. In both cases, \ourmodel exhibits a compelling generalization performance, which we argue is due to implicitly learning the underlying patterns, namely, IDRs, of indoor scene structure via the \ourmodule module. 
\begin{table}[]
    \centering
    \caption{\textbf{Zero-shot testing of models trained on NYU.} All methods are trained on NYU and tested without further fine-tuning on the official validation set of SUN-RGBD and Diode Indoor.}
    \vspace{-10pt}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{llcccc}
        \toprule
        \textbf{Test set} & \textbf{Method} &  &  &  &  \\
        \toprule
        SUN-RGBD
         & BTS~\cite{Lee2019} &  &  &  & \\
         & AdaBins~\cite{Bhat2020} &  &  &  & \\
         & P3Depth~\cite{Patil2022} &  &  &  & \\
         & NeWCRF~\cite{Yuan2022} &  &  &  & \\ 
         \cmidrule{2-6}
         & Ours &  &  &  & \\
        \midrule
        Diode  
         & BTS~\cite{Lee2019} &  &  &  & \\
         & AdaBins~\cite{Bhat2020} &  &  &  & \\
         & P3Depth~\cite{Patil2022} &  &  &  & \\
         & NeWCRF~\cite{Yuan2022} &  &  &  & \\
         \cmidrule{2-6}
         & Ours &  &  &  & \\
        \bottomrule
    \end{tabular}}
    \label{tab:nyu_zeroshot}
    \vspace{-10pt}
\end{table}

Qualitative results in \cref{fig:nyu_results} emphasize how the method excels in capturing the overall scene complexity. In particular, \ourmodel correctly captures discontinuities without depth over-excitation due to chromatic edges, such as the sink in row 1, and captures the right perspectivity between foreground and background depth planes such as between the bed (row 2) or sofa (row 3) and the walls behind. In addition, the model presents a reduced error around edges, even when compared to higher-resolution models such as~\cite{Bhat2020}. We argue that \ourmodel actually reasons at the pattern level, thus capturing better the structure of the scene. This is particularly appreciable in indoor scenes, since these are usually populated by a multitude of objects. This behavior is displayed in the attention maps of \cref{fig:nyu_attns}. \cref{fig:nyu_attns} shows how IDRs at lower resolution capture specific components, such as the relative position of the background (row 1) and foreground objects (row 2), while IDRs at higher resolution behave as depth refiners, attending typically to high-frequency features, such as upper (row 3) or lower borders of objects. It is worth noting that an IDR attends to the image borders when the particular concept it looks for is not present in the image. That is, the borders are the last resort in which the IDR tries to find its corresponding pattern (\eg, row 2, col.~1).

\begin{figure*}[]
    \centering
    \includegraphics[width=1\textwidth]{figures/kitti_results_short.pdf}\\
    \footnotesize
    \begin{tabularx}{\textwidth}{l}
        \hspace{0.085\textwidth}\textbf{Image}\hspace{0.2\textwidth}\textbf{AdaBins}\cite{Bhat2020}\hspace{0.16\textwidth}\textbf{NeWCRF}\cite{Yuan2022}\hspace{0.18\textwidth}\textbf{Ours}
    \end{tabularx}\\
    \vspace{-5pt}
    \caption{\textbf{Qualitative results on KITTI.} Three zoomed-in crops of different test images are shown.
The comparisons show the ability of \ourmodel to capture small details, proper background transition, and fine-grained variations in, \eg, crowded scenes. Best viewed on a screen.
}
    \label{fig:kitti_results}
    \vspace{-10pt}
\end{figure*}

\begin{table}[]
    \caption{\textbf{Comparison on KITTI Eigen-split test set.} Models without  have implementation (partially) unavailable. R101: ResNet-101~\cite{He2015}, D161: DenseNet-161~\cite{Huang2016}, EB5: EfficientNet-B5\cite{Tan2019}, ViTB: ViT-B/16+Resnet-50~\cite{Dosovitskiy2020VIT}, MViT: EfficientNet-B5-AP~\cite{Xie2019}+MiniViT, Swin\{L, B, T\}: Swin-\{Large, Base, Tiny\}~\cite{Liu2021}. (\dag): ImageNet-22k~\cite{Deng2010} pretraining, (\ddag): non-standard training set, (): in-house dataset pretraining, (\textsection): re-evaluated without GT-based rescaling.}\vspace{-10pt}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{ll|ccc|cccc}
        \toprule
        \multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Encoder}} &  &  &  &  &  &  & \\
        & & \multicolumn{3}{c|}{\textit{Higher is better}} & \multicolumn{4}{c}{\textit{Lower is better}}\\
        \toprule
        Eigen\etal~\cite{Eigen2014} &  &  &  &  &  &  &  & \\
        DORN~\cite{Fu2018} & R101 &  &  &  &  &  &  & \\
        BTS~\cite{Lee2019} & D161 &  &  &  &  &  &  & \\
        AdaBins\textsuperscript{\ddag}~\cite{Bhat2020} & MViT &  &  &  &  &   &  & \\
        TransDepth~\cite{Yang2021} & ViTB &  &  &  &  &  &  & \\
        DPT*~\cite{Ranftl2021} & ViTB &  &  &  &  &  &  & \\
P3Depth\textsuperscript{\textsection}~\cite{Patil2022} & R101 &  &  &  &  &  &  & \\NeWCRF~\cite{Yuan2022} & SwinL\textsuperscript{\dag} &  &  &  &  &  &  & \\
        \midrule
        \midrule
        Ours
        & R101 &  &  &  &  &  &  & \\
        & EB5 &  &  &  &  &  &  & \\
        & SwinT &  &  &  &  &  &  & \\
& SwinB &  &  &  &  &  &  & \\
        & SwinL\textsuperscript{\dag} &  &  &  &  &  &  & \\
        \bottomrule
    \end{tabular}}
    \label{tab:kitti_res}
    \vspace{-0pt}
\end{table}
\noindent{}\textbf{Outdoor Datasets.} Results on KITTI in \Cref{tab:kitti_res} demonstrate that \ourmodel sets the new SotA for this primary outdoor dataset, improving by more than 3\% in  and by 0.9\% in  over the previous SotA. However, KITTI results present saturated metrics. For instance,  is not reported since every method scores , with recent ones scoring . Therefore, we propose to utilize the metric , to better convey meaningful evaluation information. In addition, \ourmodel performs remarkably well on the highly competitive official KITTI benchmark, ranking 3\textsuperscript{rd} among all methods and 1\textsuperscript{st} among all published MDE methods.

Moreover, \Cref{tab:ddad_argo_results} shows the results of methods trained and evaluated on the splits from Argoverse and DDAD proposed in this work. All methods have been trained with the same architecture and pipeline utilized for training on KITTI. We argue that the high degree of sparseness in GT of the two proposed datasets, in contrast to KITTI, deeply affects windowed methods such as~\cite{Bhat2020, Yuan2022}. Qualitative results in \cref{fig:kitti_results} suggest that the scene level discretization leads to retaining small objects and sharp transitions between foreground objects and background: background in row 1, and boxes in row 2. These results show the better ability of \ourmodel to capture fine-grained depth variations on close-by and similar objects, including crowd in row 3. Zero-shot testing from KITTI to DDAD and Argoverse are presented in Supplement.


\begin{table}[]
    \centering
    \caption{\textbf{Comparison on Argoverse and DDAD proposed splits.} Comparison of performance of methods trained on either Argoverse or DDAD and tested on the same dataset.}
    \vspace{-10pt}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{ll|ccc|cccc}
        \toprule
        \multirow{2}{*}{\textbf{Dataset}} & \multirow{2}{*}{\textbf{Method}} &  &  &  &  &  &  & \\
        & & \multicolumn{3}{c|}{\textit{Higher is better}} & \multicolumn{4}{c}{\textit{Lower is better}}\\
        \toprule
        Argoverse
         & BTS~\cite{Lee2019} &  &  &  &  &  &  & \\
         & AdaBins~\cite{Bhat2020} &  &  &  &  &  &  & \\
         & NeWCRF~\cite{Yuan2022} &  &  &  &  &  &  & \\
         \cmidrule{2-9}
         & Ours &  &  &  &  &  &  & \\
        \midrule
        DDAD
         & BTS~\cite{Lee2019} &  &  &  &  &  &  & \\
         & AdaBins~\cite{Bhat2020} &  &  &  &  &  &  & \\
         & NeWCRF~\cite{Yuan2022} &  &  &  &  &  &  & \\
         \cmidrule{2-9}
         & Ours &  &  &  &  &  &  & \\
        \bottomrule
    \end{tabular}}
    \label{tab:ddad_argo_results}
    \vspace{-10pt}
\end{table}


\noindent{}\textbf{Surface Normals Estimation.} We emphasize that the proposed method has more general applications by testing \ourmodel on a different continuous dense prediction task such as surface normals estimation. Results in \Cref{tab:nyu_normals_res} evidence that we set the new state of the art on surface normals estimation. It is worth mentioning that all other methods are specifically designed for normals estimation, while we keep the same architecture and framework from indoor depth estimation.
\begin{table}[]
    \centering
    \caption{\textbf{Comparison of surface normals estimation methods on NYU official test set.} \ourmodel architecture and training pipeline is the same as the one utilized for indoor depth estimation.}
    \vspace{-10pt}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{l|ccc|ccc}
        \toprule
        \multirow{2}{*}{\textbf{Method}} &  &  &  &  &  & \\
        & \multicolumn{3}{c|}{\textit{Higher is better}} & \multicolumn{3}{c}{\textit{Lower is better}}\\
        \toprule
SURGE~\cite{Wang2016} &  &  &  &  &  &  \\
        GeoNet~\cite{Qi2018} &  &  &  &  &  & \\
        PAP~\cite{Zhang2019} &  &  &  &  &  & \\
        GeoNet++~\cite{Qi2022} &  &  &  &  &  & \\
        Bae \etal~\cite{Bae2021} &  &  &  &  &  & \\
        \midrule
        \midrule
        Ours &  &  &  &  &  & \\
        \bottomrule
    \end{tabular}}
    \label{tab:nyu_normals_res}
    \vspace{-10pt}
\end{table}

\subsection{Ablation study}
\label{sec:experiments_ablations}
The importance of each component introduced in \ourmodel is evaluated by ablating the method in \Cref{tab:ablations}. 

\noindent{}\textbf{Depth Discretization.} Internal scene discretization provides a clear improvement over its explicit counterpart (row 3 vs.\ 2), which is already beneficial in terms of robustness. Adding the MSDA module on top of explicit discretization (row 5) recovers part of the performance gap between the latter and our full method (row 8). We argue that MSDA recovers a better scene scale by refining feature maps at different scales at once, which is helpful for higher-resolution feature maps.

\noindent{}\textbf{Component Interactions.} Using either the MSDA module or the AFP module together with internal scene discretization results in similar performance (rows 4 and 6). We argue that the two modules are complementary, and they synergize when combined (row 8). The complementarity can be explained as follows: in the former scenario (row 4), MSDA preemptively refines feature maps to be partitioned by the non-adaptive clustering, that is, by the IDR priors described in \cref{sec:method}, while on latter one (row 6), AFP allows the IDRs to adapt themselves to partition the unrefined feature space properly. Row 7 shows that the architecture closer to the one in~\cite{Locatello2020}, particularly random initialization, hurts performance since the internal representations do not embody any domain-specific prior information.

\begin{table}[]
    \centering
    \caption{\textbf{Ablation of \ourmodel.} EDD: Explicit Depth Discretization \cite{Fu2018, Bhat2020}, ISD: Internal Scene discretization, AFP: Adaptive Feature Partitioning, MSDA: MultiScale Deformable Attention. The EDD module, used in SotA methods, and our ISD module are mutually exclusive. AFP with (\cmark) refers to random initialization of IDRs and architecture similar to~\cite{Locatello2020}. The last row corresponds to our complete \ourmodel model.}
    \vspace{-10pt}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{ccccc|ccc}
        \toprule
        & \textbf{EDD} & \textbf{ISD} & \textbf{AFP} & \textbf{MSDA} &  &  & \\
        \toprule    
        1 & \xmark & \xmark & \xmark & \xmark &  &  & \\
        2 & \cmark & \xmark & \xmark & \xmark &  &  & \\
        3 & \xmark & \cmark & \xmark & \xmark &  &  & \\
        4 & \xmark & \cmark & \cmark & \xmark &  &  & \\
        \midrule
        5 & \cmark & \xmark & \xmark & \cmark &  &  & \\
        6 & \xmark & \cmark & \xmark & \cmark &  &  & \\
        \midrule
        7 & \xmark & \cmark & \cmark & \cmark &  &  & \\
        8 & \xmark & \cmark & \cmark & \cmark &  &  & \\
        \bottomrule
    \end{tabular}
    }
    \label{tab:ablations}
    \vspace{-10pt}
\end{table}
\section{Conclusion}
We have introduced a new module, called \ourmodulename, for MDE. The module represents the assumption that scenes can be represented as a finite set of patterns. Hence, \ourmodel leverages an internally discretized representation of the scene that is enforced via a continuous-discrete-continuous bottleneck, namely \ourmodule module. We have validated the proposed method, without any TTA or tricks, on the primary indoor and outdoor benchmarks for MDE, and have set the new state of the art among supervised approaches. Results showed that learning the underlying patterns, while not imposing any explicit constraints or regularization on the output, is beneficial for performance and generalization. \ourmodel also works out-of-the-box for normal estimation, beating all specialized SotA methods. In addition, we propose two new challenging outdoor dataset splits, aiming to benefit the community with more general and diverse benchmarks.

\vfill
\noindent{}\textbf{Acknowledgment.} This work is funded by Toyota Motor Europe via the research project TRACE-Z\"urich.