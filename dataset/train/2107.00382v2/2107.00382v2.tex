

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  



\IEEEoverridecommandlockouts                              

\overrideIEEEmargins                                      \usepackage[pdftex]{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{cite}
\usepackage{epsfig}
\usepackage{caption}
\usepackage{color}
\usepackage[dvipsnames]{xcolor}
\usepackage{psfrag}
\usepackage{cases}
\usepackage{acronym}
\usepackage{setspace}
\usepackage{times}
\usepackage{latexsym}
\usepackage{dblfloatfix}
\usepackage[ruled,linesnumbered]{algorithm2e}
\usepackage{metalogo}
\usepackage{tabularx}
\usepackage{array}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{subfigure}
\let\labelindent\relax
\usepackage{enumitem}
\renewcommand{\arraystretch}{1.1}
\usepackage{hyperref}
 \usepackage{threeparttable}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=blue,      
    urlcolor=blue,
    citecolor=cyan,
}
\setlength{\textfloatsep}{3pt}
\setlength{\floatsep}{3pt}
\setlength{\intextsep}{3pt}
\usepackage{geometry}
\geometry{top=1.91cm, bottom=1.91cm, left=1.91cm, right=1.91cm}

\title{\LARGE \bf
SSC: Semantic Scan Context for Large-Scale Place Recognition
}


\author{Lin Li, Xin Kong, Xiangrui Zhao, Tianxin Huang and Yong Liu\thanks{Lin Li, Xin Kong, Xiangrui Zhao, Tianxin Huang and Yong Liu are with the Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou 310027, P. R. China. (*Yong Liu is the corresponding author, email: yongliu@iipc.zju.edu.cn).}
}


\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


\begin{abstract}

Place recognition gives a SLAM system the ability to correct cumulative errors. Unlike images that contain rich texture features, point clouds are almost pure geometric information which makes place recognition based on point clouds challenging. Existing works usually encode low-level features such as coordinate, normal, reflection intensity, etc., as local or global descriptors to represent scenes. Besides, they often ignore the translation between point clouds when matching descriptors. Different from most existing methods, we explore the use of high-level features, namely semantics, to improve the descriptor's representation ability. Also, when matching descriptors, we try to correct the translation between point clouds to improve accuracy. Concretely, we propose a novel global descriptor, Semantic Scan Context, which explores semantic information to represent scenes more effectively. We also present a two-step global semantic ICP to obtain the 3D pose (, ,  used to align the point cloud to improve matching performance. Our experiments on the KITTI dataset show that our approach outperforms the state-of-the-art methods with a large margin. Our code is available at: \url{https://github.com/lilin-hitcrt/SSC}.

\end{abstract}


\section{INTRODUCTION}

Simultaneous Localization and Mapping (SLAM) has rapidly developed in recent decades as critical technologies for autonomous vehicles and robots. Place recognition represents the ability of robots to recognize previously visited places, which can build global constraints for the SLAM system to eliminate the odometry's cumulative errors and establish a globally consistent map\cite{4633680}. Place recognition is usually conducted by using images or point clouds. Since point cloud data is rarely affected by environmental factors such as illumination and seasonal changes, LiDAR-based methods have received widespread attention in recent years.

Most existing works on LiDAR-based place recognition are achieved by encoding the point cloud into global or local descriptors and then matching the descriptors. They usually use low-level features such as coordinates\cite{spin,SC,SC1,M2DP,locnet}, normal\cite{ON}, reflection intensity\cite{delight,ISHOT,ISC,ON}, etc. In recent years, with the development of point cloud deep learning, many LiDAR-based object detection\cite{pvrcnn} and semantic segmentation\cite{cylinder3d,spvnas} methods have been proposed, making it possible to obtain semantic information from point clouds. However, there are still only a few LiDAR-based works trying to use semantic information\cite{SGPR,gosmatch,ON}.
  
  \begin{figure}[t]
    \centering
\includegraphics[width=0.99\columnwidth]{demo.png}
\caption{An example of place recognition using semantic scan context. It is a partial map of the KITTI sequence 08, where the frames 720 and 1500 form a reverse loop. The lower part of the figure is the semantic scan context corresponding to the two frames. Since the directions of them are opposite, the descriptors are quite different, while the aligned one shown in Fig.~\ref{pic:pipeline} is easy to distinguish.}
    \label{pic:demo}
 \end{figure}

 \begin{figure*}[t]
    \centering
\includegraphics[width=2\columnwidth]{pipeline.png}
\caption{The pipeline of our approach. It mainly consists of two parts: two-step global semantic ICP and Semantic Scan Context. First, we conduct semantic segmentation on the raw point cloud. Then we use semantic information to retain representative objects and project them onto the x-y plane. The two-step global semantic ICP is performed on the projected cloud to get the 3D pose (. Finally, we use the 3D pose to align the original clouds and generate global descriptors (Semantic Scan Context). The similarity score is obtained by matching SSC.}
    \label{pic:pipeline}
 \end{figure*}

For place recognition, when a robot passes through a place visited before, it does not mean that the two poses are the same. Instead, the robot may walk through the original area from any direction, and there may be a small amount of translation from the original position. Many existing works consider the robot's orientation, namely rotation, and realize the invariance of rotation\cite{SGPR,SC,SC1,ISC}. They may think that the small translation will not strongly impact the recognition result and therefore ignore it. However, we find that simply ignoring the translation for the scan context-based methods will greatly reduce the similarity of the positive samples, making them difficult to identify. 

In this paper, we propose a novel global descriptor named Semantic Scan Context (SSC), which explores semantic information to enhance the expressive power of descriptors. We also propose a two-step global semantic ICP that can produce reliable results regardless of the pose initialization, to obtain the 3D pose  of the point cloud. The pose is then used to align the point clouds to reduce the influence of rotation and translation on the similarity of the descriptors. Furthermore, it can also provide good initial values for 6D ICP algorithms to refine the global pose further. Fig.~\ref{pic:demo} is a demonstration of our results. The main contribution is summarized as follows:
\begin{itemize}
    \item We propose a novel global descriptor for LiDAR-based place recognition, which exploits semantic information to encode the 3D scenes effectively.
\item We propose a two-step global semantic ICP, which doesn't require any initial values, to obtain the 3D pose  of the point clouds.
    \item We align point clouds with the obtained 3D poses to eliminate the influence of rotation and translation error on the similarity of the descriptors, which can also further benefit the SLAM system as good initial poses.
    \item Exhaustive experiments on the KITTI odometry dataset show that our approach achieves state-of-the-art performance both in place recognition and pose estimation.
\end{itemize}



\section{RELATED WORK}

According to the features used, we can divide the place recognition methods into three categories: geometry-based, semi-semantic-based, semantic-based.

\textbf{Geometry-based methods}: Spin image~\cite{spin} establishes a local coordinate system for each point, then projects the point into the 2D space and counts the number of points in different areas in the 2D space to form a spin image. ESF~\cite{ESF} proposes a shape descriptor that combines angle, point-distance, and area to boost the recognition rate. M2DP~\cite{M2DP} projects the point cloud into multiple 2D planes and generates a density signature for each plane's points. The left and right singular vectors of those signatures are used as the global descriptors. Scan context~\cite{SC,SC1} converts the point cloud to polar coordinates and then divides it into blocks along the azimuth and radial directions. Lastly, it encodes the z coordinate of the highest point in each block as a 2D global descriptor. LocNet~\cite{locnet} divides a point cloud into rings, generates a distance histogram for each ring, and stitches all histograms to form a global descriptor. Then a siamese network is used to score the similarity between the descriptors. LiDAR Iris~\cite{LI} extracts a binary signature image for each point cloud then uses the Hamming distance of two corresponding binary signature images as the similarity. Seed~\cite{seed} segments the point cloud into different objects and encodes the topological information of the segmented objects into the global descriptor. The above methods have achieved good results by encoding low-level geometric structures into descriptors. It can be expected that integrating more advanced features can further enhance the discriminative power of descriptors.


\textbf{Semi-semantic-based methods}: Some methods use non-geometric information to construct descriptors, such as reflection intensity or learning-based features extracted by neural networks. Such features are related to the object type but do not clearly indicate the semantic category, so we classify these methods as semi-semantic based. ISHOT~\cite{ISHOT} and ISC~\cite{ISC} exploit the intensity information of the point cloud for place recognition. SegMatch~\cite{segmatch} and SegMap~\cite{segmap2} cluster a point cloud into segments. Then they extract features for each segment and use the kNN algorithm to identify corresponds. PointNetVLAD~\cite{PV} combines PointNet~\cite{pointnet} and NetVLAD~\cite{netvlad} to extract global descriptors from the 3D point clouds end-to-end. -Net~\cite{l3net} selects key points from the given point cloud then uses a PointNet to learn local descriptors for each key point. OREOS~\cite{oreos} projects the 3D point cloud into a 2D range image and proposes a convolutional neural network to extract the global descriptor. DH3D~\cite{dh3d} designs a siamese network to learn 3D local features from the raw 3D point clouds, then use an attention mechanism to aggregate these local features as the global descriptor. LPD-Net~\cite{lpdnet} proposes the adaptive local feature extraction module and the graph-based neighborhood aggregation module to extract local features of the point cloud; then, as the PointNetVLAD, they use the NetVLAD to generate the global descriptor. MinkLoc3D~\cite{minlock3d} uses a sparse voxelized point cloud representation and sparse 3D convolutions to compute a discriminative 3D point cloud descriptor. SeqSphereVLAD~\cite{SeqSphereVLAD} projects the point cloud onto a spherical view, extracts features on it and sequences those features to form a descriptor. SpoxelNet~\cite{spoxenet} voxelized the point cloud in spherical coordinates and defines the occupancy of each voxel in ternary values. Then they use a neural network to extract the global descriptor. The above methods combine more advanced features with geometric features. However, most of them use neural networks to extract abstract features, which are more complicated and not well interpretable.


\textbf{Semantic-based methods}: SGPR~\cite{SGPR} represents the scene as a semantic graph then uses a graph similarity network to score the similarity of the graphs. GOSMatch~\cite{gosmatch} proposes a new global descriptor that is generated from the spatial relationship between semantics. It also proposes a coarse-to-fine strategy to efficiently search loop closures and gives an accurate 6-DOF initial pose estimation. The two methods represent the scene as a graph and abstract the object as a node in the graph, which will cause the loss of features such as the size of each object. OverlapNet~\cite{ON} designed a deep neural network that uses different types of information, such as intensity, normal, and semantics generated from LiDAR scans, to provide overlap and relative yaw angle estimates between paired 3D scans. However, it is too slow in preprocessing due to the need to calculating the normal and inferring the complex network backbone. To use the semantic information more effectively, we propose our Semantic Scan Context approach.



\section{METHODOLOGY}

In this section, we present our semantic scan context approach. Different from other scan context-based methods that use incomplete semantic information and ignore small translations between point clouds, we explore to exploit full semantic information and emphasize that the small translation between point cloud pairs has a significant influence on the accuracy of recognition.


As shown in Fig.~\ref{pic:pipeline}, our method consists of two main parts: two-step global semantic ICP and Semantic Scan Context. The two-step global semantic ICP is divided into Fast Yaw Angle Calculate and Fast Semantic ICP. First, we define a point cloud frame as , with each point ,  represent the semantic label of . Given a pair of point clouds , we first use our Fast Yaw Angle Calculate method to get the relative yaw angle  between them. Then we use the Fast Semantic ICP to calculate their relative translation  in the x-y plane. Through the above two steps, we get the relative poses  of the two frames of point clouds in 3D pose space. In order to eliminate the influence of rotation (e.g., reverse loop closures) and small translation on recognition, we use the obtained relative pose to align point cloud . We mark the aligned point cloud as . Finally, we use our global descriptor -- the Semantic Scan Context to describe  as . The similarity score is obtained by comparing  and .

\begin{figure}[t]
    \centering
\subfigure[Yaw Aligned]{
\includegraphics[width=0.46\columnwidth]{angle_aligned.png}
        }
        \subfigure[Translation Aligned]{
\includegraphics[width=0.46\columnwidth]{icp_aligned.png}
        }
    \caption{An illustration of the two-step global semantic ICP.}
    \label{pic:yaw_icp}
 \end{figure}

\subsection{Global Semantic ICP}
It is known that the general ICP algorithm based on local iterative optimization is susceptible to local minimums~\cite{goicp}. For place recognition, we usually cannot get a valid initial value, which leads to the failure of the general ICP algorithm. To solve this, we propose the two-step global semantic ICP algorithm consisting of Fast Yaw Angle Calculate and Fast Semantic ICP. Benefited from the use of semantic information, our algorithm does not require any initial values to get satisfactory results.

\textbf{Fast Yaw Angle Calculate.}\label{ssc:yaw}
For scan context based methods, columns of their descriptor represent the yaw angle. The pure rotation of the LiDAR in the horizontal plane will cause the column shift of their descriptor. Scan context and Intensity Scan Context get the similarity score and the yaw angle at the same time. Specifically, they calculate similarity (or distance) with all possible column-shifted descriptors and find the maximum similarity (or minimum distance). However, there are two main disadvantages. Firstly, it's inefficient to compare the whole 2D descriptors by shifting. Secondly, they still try to get the maximum score for point clouds from different places (not loop closure). This obviously makes it more prone to false positives. To draw the above issues, we propose the semantic-based fast yaw angle calculate method.

Given a point cloud pair , we select representative objects such as buildings, tree trunks, and traffic signs based on semantic information. Then we convert the filtered clouds to polar coordinate in  the x-y plane:


where  is the  point in each converted cloud,  and  represent polar diameter and polar angle, respectively. Each converted cloud is then segmented to  sectors by yaw angle. We only keep the point with the smallest polar diameter in each sector. Finally, we get two clouds  and , with  elements. We sort the points in  and  according to the azimuth angle and save their corresponding polar diameters as vectors  and . Similar to the scan context, the shift of the column vector is related to the yaw angle:
 
where  is  shifted by  element and  is defined as:


Compared with Scan Context and Intensity Scan Context, our method only needs to compare one-dimensional vectors; therefore, it is more efficient. Moreover, our method does not obtain the angle via maximizing the score, which is helpful to identify non-loop-closure point-cloud pairs. Fig.~\ref{pic:yaw_icp} shows the result of Fast Yaw Angle Calculate.

\begin{figure}[t]
    \centering
\includegraphics[width=0.99\columnwidth]{ssc.png}\vspace{-2mm}
\caption{An example of generating SSC.  and  represent the polar diameter and polar angle, respectively. A sector corresponds to a descriptor column, while a ring corresponds to a row of the descriptor.}
    \label{pic:ssc}
 \end{figure}

\textbf{Fast Semantic ICP.}\label{ssc:icp}
Though most works ignore translation between point clouds, ignoring the translation causes considerable declines in our experiments. In fact, for methods based on scan context, translation will affect both the row and column of the descriptor. We can’t get the best result just by the column-shifted descriptor. Therefore, we propose a fast semantic ICP algorithm to correct the translation between point clouds.



To find the relative translation, we firstly rotate  to the same direction as , and the rotated point cloud is , which is defined as:
 
where  and  represent the  point in  and  respectively. Our ICP problem can be defined as:
 
 where  represents the corresponding point of , which is the point closest to  in ,  and  are semantic labels of the points. If  is equal to , then the output of  is ; otherwise, . As our point clouds are ordered, we can search for the corresponding points near the position where the yaw angle is consistent with the target point. Specifically, our search interval for the  target point is:
 
 where  is the length of search interval and  is defined in Eq.~\ref{equ:shift}. After a certain number of iterations, we can get the relative translation between the input point clouds, shown in Fig.~\ref{pic:yaw_icp}.

\begin{figure*}[htb]
    \centering
        \subfigure[00]{
        \centering
        \includegraphics[width=0.64\columnwidth]{00.png}
        }
        \subfigure[02]{
        \centering
        \includegraphics[width=0.64\columnwidth]{02.png}
        }
        \subfigure[05]{
        \centering
        \includegraphics[width=0.64\columnwidth]{05.png}
        }\vspace{-3mm}
        \subfigure[06]{
        \centering
        \includegraphics[width=0.64\columnwidth]{06.png}
        }
        \subfigure[07]{
        \centering
        \includegraphics[width=0.64\columnwidth]{07.png}
        }
        \subfigure[08]{
        \centering
        \includegraphics[width=0.64\columnwidth]{08.png}
        }\vspace{-3mm}
    \caption{Precision-Recall curves on KITTI dataset.}
    \label{pic:pr_curvs}
 \end{figure*}
 
  \begin{table*}[htb]\footnotesize
    \caption{\centering  max scores and Extended Precision on KITTI dataset}\vspace{-3mm}
    \label{table:F1}
    \begin{center}
    \begin{threeparttable}
        {
    \begin{tabular}{c c c c c c c c}
    \hline
    Methods & 00 & 02 & 05 & 06 & 07 &08&Mean\\ 
    \hline
    SC\cite{SC} & 0.750/0.609 & 0.782/0.632 &0.895/0.797&0.968/0.924&0.662/0.554&0.607/0.569&0.777/0.681\\
    ISC\cite{ISC}&0.657/0.627&0.705/0.613&0.771/0.727&0.842/0.816&0.636/0.638&0.408/0.543&0.670/0.661\\
    M2DP\cite{M2DP}&0.708/0.616&0.717/0.603&0.602/0.611&0.787/0.681&0.560/0.586&0.073/0.500&0.575/0.600\\
    LI\cite{LI}&0.668/0.626&0.762/0.666&0.768/0.747&0.913/0.791&0.629/0.651&0.478/0.562&0.703/0.674\\
    PV\cite{PV}&0.779/0.641&0.727/0.691&0.541/0.536&0.852/0.767&0.631/0.591&0.037/0.500&0.595/0.621\\
    ON\cite{ON}&0.869/0.555&0.827/0.639&0.924/0.796&0.930/0.744&0.818/0.586&0.374/0.500&0.790/0.637\\
    SGPR\cite{SGPR}&0.820/0.500&0.751/0.500&0.751/0.531&0.655/0.500&0.868/0.721&0.750/0.520&0.766/0.545\\
    Ours-RN&\underline{0.939}/\underline{0.826}&\underline{0.890}/\underline{0.745}&\underline{0.941}/\underline{0.900}&\textbf{0.986}/\textbf{0.973}&\underline{0.870}/\underline{0.773}&\underline{0.881}/\underline{0.732}&\underline{0.918}/\underline{0.825}\\
    Ours-SK&\textbf{0.951}/\textbf{0.849}&\textbf{0.891}/\textbf{0.748}&\textbf{0.951}/\textbf{0.903}&\underline{0.985}/\underline{0.969}&\textbf{0.875}/\textbf{0.805}&\textbf{0.940}/\textbf{0.932}&\textbf{0.932}/\textbf{0.868}\\
    \hline
    \end{tabular}
    }
    \begin{tablenotes} 
\footnotesize
        \item  max scores and Extended Precision:  max scores / Extended Precision. The best scores are marked in bold and the second best scores are underlined.
\end{tablenotes}
    \end{threeparttable}
    \end{center}
    \end{table*}

\subsection{Semantic Scan Context}
Scan Context and Intensity Scan Context uses the points' height and reflection intensity as features, respectively. Their methods essentially take advantage of the different characteristics of different objects in the scene. However, height and reflection intensity is only low-level features of the object which are not representative enough. We explore to use the high-level semantic features to represent scenes and thus propose the Semantic Scan Context descriptor.

\smallskip\noindent\textbf{Descriptor definition.} Given a point cloud , we first convert it to the polar coordinate system as we did in Section~\ref{ssc:yaw}. Then, like scan context, we divide the point cloud into  blocks along the azimuthal and radial directions. Each block is represented by:

where  is the the maximum effective measurement distance of LiDAR,  and . Our descriptor can be defined by:

 is an encoding function to encode features of . Note that if . We manually set the priority of different semantics in function  to show their representativeness. We believe objects that appear less frequently in the scene are more representative (e.g., traffic signs are more representative than roads). 

\smallskip\noindent\textbf{Similarity Scoring.} Given aligned clouds  and , we can get their descriptors  and  by Eq.~\ref{eq:descriptor}. Then the similarity score between them can be calculated by:
\begin{small}

\end{small}
where  is the indicator function, defined by:

Fig.~\ref{pic:ssc} shows Semantic Scan Context creation. 



    \begin{figure*}[htb]
        \centering
            \subfigure[Average  max scores]{
            \centering
            \includegraphics[width=0.95\columnwidth]{F1.png}
            }
            \subfigure[Average ]{
            \centering
            \includegraphics[width=0.95\columnwidth]{EP.png}
            }\vspace{-3mm}
        \caption{Average  max score and Average Extended Precision corresponding to different .}
        \label{pic:F1}
     \end{figure*}

\section{EXPERIMENTS}


\subsection{Experiment Setup}\label{exp:setup}
We conduct experiments on the KITTI odometry dataset\cite{kitti} collected by a 64-ring LiDAR, which contains 11 training sequences (00-10) with ground truth poses. We choose sequences with loop-closure (00,02,05,06,07,08) for evaluation and note that sequence 08 has reverse loops while others are in the same direction. Similar to SGPR\cite{SGPR}, we regard the point cloud pair with a relative distance less (greater) than 3m (20m) as a positive (negative) sample. Since there are too many negative samples, we only select a part of the negative samples for evaluation. Specifically, if there are  positive samples in a sequence, we will randomly select  negative samples. We can adjust the proportion of negative samples by changing the coefficient .

The ground-truth semantic labels are from the SemanticKITTI dataset\cite{semkitti}. We also test our method with the semantic segmentation algorithm (RangeNet++~\cite{rangenet++})  to prove that our method can be applied to noisy predictions in real situations. In our experiments, we set . All experiments are done on the same system with an Intel i7-9750H @3.00GHz CPU with 16 GB RAM.

\subsection{Place Recognition Performance}\label{seu:pr}
As mentioned in Section~\ref{exp:setup}, we use both ground-truth semantic labels (Ours-SK) and predicted semantic labels (Ours-RN) for testing. We compare our approach with the state-of-the-art methods, including Scan Context\cite{SC} (SC), Intensity Scan Context\cite{ISC} (ISC), M2DP\cite{M2DP}, LiDAR Iris\cite{LI} (LI), PointNetVLAD\cite{PV} (PV), OverlapNet\cite{ON} (ON), and SGPR\cite{SGPR}. For SGPR, we use their pre-trained models trained with the 1-fold strategy. As we cannot reproduce the results of OverlapNet, we use the pre-trained model provided by the author. The model is trained on sequences 03-10, so sequences 05, 06, 07, 08 are included in the training set.

\textbf{Fixed }. In this experiment, we set  to 100, which means the number of negative samples is . Fig.~\ref{pic:pr_curvs} shows the precision-recall curve of each method. Additionally, we also use the maximum  score and Extended Precision\cite{EP} (EP) shown in Tab.~\ref{table:F1} to analyze the performance. The  score is defined as:

where  and  represent the Precision and Recall, respectively;  is the harmonic mean of  and . It treats  and  as equally important and measures the overall performance of classification. The Extended Precision is defined as:

where  is the precision at minimum recall, and  is the max recall at  precision.  is specifically designed metrics for place recognition algorithms. 

As shown in Fig.~\ref{pic:pr_curvs} and Tab.~\ref{table:F1}, Ours-SK surpasses other methods in all indicators of all sequences with a large margin. Especially in sequence 08, which has only reverse loops, the performance of other methods drops significantly while our method still performs well. This indicates that our method is robust to view angle changes. OverlapNet performs well on most sequences except 08. We guess this is because it uses the normal of the point cloud, which will change as the point cloud rotates. Therefore, this method cannot robustly handle reverse loops. SGPR works well on indicator the  max score but poorly on the Extended Precision. We find that it gives some negative samples a huge score, which causes the recall to be almost zero when the accuracy reaches . The result of Ours-RN is slightly worse than Ours-SK as expected. As the difference is not obvious, it means that our approach can adapt to semantic segmentation algorithms for actual systems.

\textbf{Change }. In this experiment, we change the value of  to analyze the influence of the number of negative samples on those algorithms. Fig.~\ref{pic:F1} shows the Average  max score and Average Extended Precision corresponding to different . It clearly shows that our method performs better than others no matter how much  is taken. As  increases, the performance of all methods gradually decreases, but our method is less affected, showing that our method can effectively identify negative samples. For place recognition, negative samples are generally far more than positive samples, which is one key reason why our method leads in metrics far ahead. Moreover, identifying negative samples is significant as false positives will bring fatal crashes to the SLAM system.

    \begin{table}[t]\footnotesize
        \caption{\centering Yaw error on KITTI dataset}\vspace{-3mm}
        \label{table:yaw}
        \begin{center}
        \begin{threeparttable}
            {
        \begin{tabular}{c c c c c c c c}
        \hline
        sequences & SC (deg) & ISC (deg) & ON (deg) & Ours-SK (deg) \\ 
        \hline
        00 & 11.526&\textbf{0.829}&2.595&0.891\\
        02 &11.301&1.343&4.911&\textbf{1.142}\\
        05 &18.394&0.904&3.329&\textbf{0.653}\\
        06 &4.074&\textbf{0.534}&1.124&0.759\\
        07 &21.862&0.684&2.233&\textbf{0.512}\\
        08 &49.170&3.856&68.622&\textbf{1.878}\\
        Average &19.388&1.358&13.802&\textbf{0.973}\\
        \hline
        \end{tabular}
        }
\end{threeparttable}
        \end{center}
        \end{table}
     
\subsection{Pose Accuracy}
As described in Section~\ref{ssc:icp}, our approach can estimate the 3D relative pose , while most other methods cannot estimate pose or can only estimate 1D pose (yaw). We compare our method with Scan Context, Intensity Scan Context, and Overlap. The ground-truth pose is calculated by:

where  and  represent the pose of  and , respectively. Since the pitch and roll angles are hardly changed in autonomous vehicles, we ignore them.

Tab.~\ref{table:yaw} shows the relative yaw error on the KITTI dataset. We can see that our method outperforms other methods in terms of the average relative yaw error. Especially in the challenging sequence 08, affected by the reverse loop, most methods perform poorly, while our method can still accurately estimate the yaw angle. This again shows that our method can handle the reverse loop well. As mentioned in Section~\ref{seu:pr}, OverlapNet performs poorly due to its inability to handle reverse loops.

Fig.~\ref{pic:trans} shows the relative translation error of our approach on the KITTI dataset. As shown, our method can estimate accurate relative translation, which is currently not possible with other methods to our knowledge. Thus, our Fast Yaw Angle Calculate and Fast Semantic ICP approaches can give accurate 3D pose estimation. This can provide a good initial value for the ICP algorithm to obtain a 6D pose or directly serve as a global constraint in the SLAM system. 








    \begin{figure}[t]
        \centering
\includegraphics[width=0.95\columnwidth]{box_trans.png}
\caption{Translation error.}
        \label{pic:trans}
     \end{figure}
     
\begin{table}[t]\footnotesize
    \caption{\centering Contribution of individual components}\vspace{-3mm}
    \label{table:Ablation}
    \begin{center}
\begin{tabular}{c c c c c c c c}
    \hline
    Yaw & ICP & Semantic & &Decrease\\ 
    \hline
    ~&&&0.896/0.820&3.6\%/4.8\%\\
    &~&&0.757/0.685&17.5\%/18.3\%\\
    &&~&0.775/0.762&15.7\%/10.6\%\\
    &&&0.932/0.868&0.0\%/0.0\%\\
    \hline
    \end{tabular}
\end{center}
    \end{table}

\subsection{Ablation Study}
We design an ablation study to investigate the contribution of each component. Specifically, we remove or replace a module at a time and then calculate the  max scores and Extended Precision. To show the contribution of our Fast Yaw Angle Calculate method, we replace this module with the method used in scan context -- shift the column of descriptors and calculate the maximum similarity score while obtaining the yaw angle. Similarly, we replace the semantic label in the descriptor by maximum  to see semantic contribution. To evaluate the contribution of our Fast Semantic ICP approach, we directly set  and  to 0. As shown in Tab.~\ref{table:Ablation}, after removing Yaw, ICP, and Semantic, the average  max score decrease by , , , and the average Extended Precision decrease by , , . Therefore, the following conclusions can be drawn:
\begin{itemize}
     \item Compared with other methods, our approach can get a more accurate yaw angle and translation.
     \item As we emphasized, the small translation has a significant impact on scan context-based methods. Simply ignoring the translation will greatly weaken the performance.
     \item High-level features, like semantics, can bring considerable improvements in the scene description.
\end{itemize}

\begin{table}[t]\footnotesize
    \caption{\centering Average time cost on KITTI 08}\vspace{-3mm}
    \label{table:time}
    \begin{center}
    \begin{threeparttable}
    {
    \begin{tabular}{c c c c c c c}
    \hline
    Methods &Size& Description & Retrieval & ICP & Total\\ 
    \hline
    SC&&4.825&0.158&-&4.983\\
    ISC&&3.094&0.800&-&\textbf{3.894}\\
    Ours&&\textbf{2.563}&\textbf{0.066}&2.126&4.755\\
    \hline
    \end{tabular}
    }
    \begin{tablenotes} 
\footnotesize
            \item The unit of time in the table is milliseconds.
         \end{tablenotes}
    \end{threeparttable}
    \end{center}
    \end{table}

\subsection{Efficiency}
To evaluate the efficiency, we set  to  and compare the average time cost of our method with Scan Context and Intensity Scan Context on sequence 08. As shown in Tab.~\ref{table:time}, the total time cost of our approach is acceptable. As we use the obtained 3D pose to align the point clouds in advance, we don't need to shift the column of descriptors during the matching stage, so our retrieval speed is extremely fast. Our two-step global semantic ICP only takes 2.126 milliseconds on average. This algorithm is fast due to the following reasons. Firstly, since we only keep  (360 taken in our experiments) points, the computational cost is greatly reduced compared to the original point cloud (about 120,000 points). Secondly, We divide the algorithm into two steps, first calculate the yaw angle, and then iteratively calculate  and , which simplifies the algorithm and speeds up the calculation. Thirdly, when calculating  and , we use the yaw angle to align the input clouds in advance. Therefore we don’t need to traverse the entire point cloud when looking for the corresponding points. Instead, we can find them near the corresponding positions, which greatly reduces the number of searches.


\section{CONCLUSION}
In this paper, we propose a novel semantic-based global descriptor for place recognition. We propose a two-step global semantic ICP to obtain the 3D pose  of the point cloud pair, aligning the point clouds to improve the descriptor matching accuracy. In addition, it can provide good initial values for point cloud registration. We achieve leading performance on the KITTI odometry dataset compared to the state-of-the-art methods. 

Our method also has some limitations. Like most place recognition methods, our method does not consider pitch angle and roll angle. Therefore, our method may fail in some extreme scenarios.

In the future work, we will try to solve the above problems and further explore the application of semantic information in LiDAR-based SLAM systems.







\bibliographystyle{ieeetr}
\bibliography{main}



\end{document}
