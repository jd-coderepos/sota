

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{eacl2021}
\usepackage{times}
\usepackage{latexsym}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{todonotes}
\usepackage{stfloats} \renewcommand{\UrlFont}{\ttfamily\small}

\usepackage{microtype}

\usepackage{graphicx}


\aclfinalcopy 



\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{FLERT: Document-Level Features for Named Entity Recognition}

\author{
Stefan Schweter \\
	schweter.ml\\
	{\tt stefan@schweter.eu}\\\And
Alan Akbik \\
	Humboldt-Universit√§t zu Berlin\\
	{\tt alan.akbik@hu-berlin.de}\\
\\}

\date{}

\begin{document}
\maketitle
\begin{abstract}

Current state-of-the-art approaches for named entity recognition (NER) using BERT-style transformers typically use one of two different approaches: (1) The first \textit{fine-tunes} the transformer itself on the NER task and adds only a simple linear layer for word-level predictions. (2) The second uses the transformer only to provide \textit{features} to a standard LSTM-CRF sequence labeling architecture and thus performs no fine-tuning. In this paper, we perform a comparative analysis of both approaches in a variety of settings currently considered in the literature. In particular, we evaluate how well they work when \textit{document-level} features are leveraged. Our evaluation on the classic CoNLL benchmark datasets for 4 languages shows that document-level features significantly improve NER quality and that fine-tuning generally outperforms the feature-based approaches. We present recommendations for parameters as well as several new state-of-the-art numbers. Our approach is integrated into the \textsc{Flair} framework to facilitate reproduction of our experiments.  

\end{abstract}

\section{Introduction}

Named entity recognition (NER) is the well-studied NLP task of predicting shallow semantic labels for sequences of words, used for instance for identifying the names of persons, locations and organizations in text. Current approaches for NER often leverage pre-trained transformer architectures such as BERT \citep{devlin-etal-2019-bert} or XLM \citep{lample2019cross}.

\noindent 
\textbf{Document-level features.} While NER is traditionally modeled at the sentence-level, transformer-based models offer a natural option for capturing document-level features by passing a sentence with its surrounding context. As Figure~\ref{overview-bert-fine-tuning-document-level} shows, this context can then influence the word representations of a sentence: The example sentence "I love Paris" is passed through the transformer together with the next sentence that begins with "The city is", potentially helping to resolve the ambiguity of the word "Paris".  
A number of prior works have employed such document-level features~\cite{devlin-etal-2019-bert, virtanen2019multilingual,yu-etal-2020-named} but only in combination with other contributions and thus have not evaluated the impact of using document-level features in isolation. 

\noindent
\textbf{Contributions.}
With this paper, we close this experimental gap and present an evaluation of document-level features for NER. As there are two conceptually very different approaches for transformer-based NER that are currently used across the literature, we evaluate document-level features in both: 

\begin{enumerate}
    \item In the first, we \textit{fine-tune} the transformer itself on the NER task and only add a simple linear layer for word-level predictions~\cite{devlin-etal-2019-bert}.
    \item In the second, we use the transformer only to provide \textit{features} to a standard LSTM-CRF sequence labeling architecture~\cite{2015arXiv150801991H} and thus perform no fine-tuning.
\end{enumerate}
We discuss the differences between both approaches and explore best hyperparameters for each. In their best determined setup, we then perform a comparative evaluation using document-level features from different transformer models.
We find that document-level features significantly improve NER quality and that fine-tuning generally outperforms feature-based approaches. 

We use document-level features in the best determined fine-tuning setup and report a number of new state-of-the-art scores on the classic CoNLL benchmark datasets.  Our approach is integrated into the \textsc{Flair} framework~\cite{akbik-etal-2019-flair} to facilitate reproduction of our experiments.


















\begin{figure*}
 \centering
 \includegraphics[width=\linewidth]{figures/bert_document_level_3.pdf}
\vspace{-6mm}
 \caption{To obtain document-level features for a sentence that we wish to tag ("I love Paris", shaded green), we add 64 tokens of left and right tokens each (shaded blue). As self-attention is calculated over all input tokens, the representations for the sentence's tokens are influenced by the left and right context.}
 \label{overview-bert-fine-tuning-document-level}
\end{figure*}















\section{Document-Level Features}

In a transformer-based architecture, document-level features can easily be realized by passing a sentence with its surrounding context to obtain word embeddings, as illustrated in Figure~\ref{overview-bert-fine-tuning-document-level}. 

\noindent
\textbf{Prior approaches.}
This approach was first employed by~\citet{devlin-etal-2019-bert} with what they described as a "maximal document context", though technical details were not listed. Subsequent work has used variants of this approach. For instance, \citet{virtanen2019multilingual} experiment with adding the following (but not preceding) sentence as context to each sentence. \citet{yu-etal-2020-named} instead use a 64 surrounding token window for each token in a sentence, thus calculating a large context on a per-token basis. By contrast, \citet{luoma2020exploring} adopt a multi-sentence view in which they evaluate different ways for passing sequences of multiple complete sentences through a transformer, evaluate the impact of the position of a sentence in this multi-sentence view and present ways to combine predictions from different windows and sentence positions. 

\noindent
\textbf{Our approach.}
In this paper, we instead use a conceptually simple variant in which we create context on a per-sentence basis. That is, for each sentence we wish to classify, we add 64 subtokens of left and right context, as shown in Figure~\ref{overview-bert-fine-tuning-document-level}. This has computational and implementation advantages in that each sentence and its context need only be passed through the transformer once and that added context is limited to a relatively small window of 64 subtokens. Furthermore, we can still follow standard procedure in shuffling  sentences at each epoch during training, since context is encoded on a per-sentence level. We use this approach throughout this paper. 




















\begin{table*}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{lccccc}
\hline
 Fine-tuning Approach &  \textsc{En} & \textsc{De}      &  & \textsc{Nl} & \textsc{Es} \\ \hline
 \textit{XLM-R}       &      96.64  0.14        & 89.06  0.18 & 91.86  0.41   &    93.41  0.19         & 88.95  0.19 \\
 + WE                 &     96.82  0.13         & 88.96  0.10 & 92.12  0.10   &    93.51  0.09         & 89.09  0.36 \\
 + Context            &   96.82  0.07           & \textbf{89.79}  0.13 & \textbf{93.09}  0.06   &     94.19  0.14        & 90.34  0.27 \\ 
 + WE + Context       &    \textbf{97.02  0.09}          & 89.74  0.46 & 92.83  0.12   &   94.01  0.27          & 90.17  0.25 \\ \hline
 \textit{XLM-R-CRF}   &    96.79  0.11          & 88.52  0.10 & 92.21  0.07   &    93.61  0.15         & 88.77  0.20 \\ 
 + WE                 &   96.79  0.15           & 88.84  0.15 & 91.97  0.09   &    93.36  0.04         & 88.63  0.47 \\
 + Context            &    96.90  0.06          & 89.67  0.24 & 92.87  0.21   &     94.16  0.07        & \textbf{90.56}  0.09 \\ 
 + WE + Context       &   96.87  0.00           & 89.69  0.22 & 92.88  0.26   &     \textbf{94.34  0.13}        & 90.37  0.14 \\\hline
\end{tabular}}
\caption{Evaluation of different variants using the fine-tuning approach. The evaluation is performed against the \textbf{development set} of all 4 languages of the CoNLL-03 shared task for NER.}
\label{tab:finetuning_development_dataset}
\end{table*}

\section{Baseline Parameter Experiments}
\label{sec:baseline_experiments}

As mentioned in the introduction, there are two common architectures for transformer-based NER, namely fine-tuning and feature-based approaches. In this section, we briefly introduce the differences between both approaches and conduct a study to identify best hyperparameters for each. The best respective setups are then used in the final comparative evaluation in Section~\ref{sec:evaluation}. 





\subsection{Setup}

\noindent
\textbf{Data set.} We use the development datasets of the CoNLL shared tasks \citep{tjong-kim-sang-de-meulder-2003-introduction,tjong-kim-sang-2002-introduction} for NER on four languages (English, German, Dutch and Spanish). Following \citet{yu-etal-2020-named} we report results for both original and revisited dataset for German (denoted as ).

\noindent
\textbf{Transformer model.}
In all experiments in this section, we employ the multilingual XLM-RoBERTa (XLM-R) transformer model proposed by \citet{conneau2019unsupervised}. We use the \texttt{xlm-roberta-large} model in our experiments, trained on 2.5TB of data from a cleaned Common Crawl corpus \cite{wenzek-etal-2020-ccnet} for 100 different languages



\noindent
\textbf{Embeddings (+WE).}
For each setup we experiment with adding classic word embeddings that are concatenated to the word-level representations obtained from the transformer model. Following~\citet{akbik-etal-2018-contextual}, we use \textsc{GloVe} embeddings \citep{pennington-etal-2014-glove} for the English tasks and \textsc{FastText} embeddings \citep{bojanowski-etal-2017-enriching} for all other languages.






\subsection{First Approach: Fine-Tuning}
\label{sec:fine_tuning}

Fine-tuning approaches typically only add a single linear layer to a transformer and fine-tune the entire architecture on the NER task. To bridge the difference between subtoken modeling and token-level predictions, they apply \textit{subword pooling} to create token-level representations which are then passed to the final linear layer. A common subword pooling strategy is "first"~\cite{devlin-etal-2019-bert} which uses the representation of the first subtoken for the entire token. See Figure~\ref{fig:subword_pooling} for an illustration. 

\begin{table}[h]
\begin{center}
\begin{tabular}{ l l }
\toprule
Parameter & Value \\
\midrule
Transformer layers & last \\
Learning rate & 5e-6 \\
Mini batch size & 4 \\
Max epochs & 20 \\
Optimizer & AdamW \\ Scheduler & One-cycle LR \\
Subword pooling & first \\
\bottomrule
\end{tabular}
\end{center}
\vspace{-2mm}
\caption{\label{tab:finetuning_params} Parameters used for fine-tuning.}
\end{table}

To train this architecture, prior works typically use the AdamW~\cite{loshchilov2018decoupled} optimizer, a very small learning rate and a small, fixed number of epochs as a hard-coded stopping criterion~\cite{conneau2019unsupervised}. We adopt a one-cycle training strategy~\cite{2018arXiv180309820S}, inspired from the HuggingFace transformers~\cite{wolf2019huggingface} implementation, in which the learning rate linearly decreases until it reaches  by the end of the training. Table~\ref{tab:finetuning_params} lists the architecture parameters we use across all our experiments. 

Conceptually, fine-tuning approaches have the advantage that everything is modeled in a single architecture that is fine-tuned as a whole. 

\begin{figure}[t!]
 \centering
 \includegraphics[width=\linewidth]{figures/bert_first_subword_pooling.pdf}
 \captionof{figure}{
 Illustration of first subword pooling. The input "The Eiffel Tower" is subword-tokenized, splitting "Eiffel" into three subwords (shaded green). Only the first ("E") is used as representation for "Eiffel".
}
 \label{fig:subword_pooling}
\end{figure}

\subsubsection{Fine-Tuning Variants}
\label{sec:baseline_setup}

We compare two variants of fine-tuning: 

\begin{description}
\item[XLM-R] In the first, we use the standard approach of adding a simple linear classifier on top of the transformer to directly predict tags. 
\item[XLM-R-CRF] In the second, we evaluate if it is helpful to add a conditional random fields (CRF) decoder between the transformer and the linear classifier~\cite{souza2019portuguese}. 
\end{description}

\noindent
We evaluate both in all possible combinations of adding standard word embeddings ("+WE") and using document-level features ("+Context"). Results are listed in Table~\ref{tab:finetuning_development_dataset}.


\subsubsection{Best Fine-Tuning Variant}

As Table~\ref{tab:finetuning_development_dataset} shows, the best results are observed with different configurations on different datasets. In particular, we find that using additional word embeddings and using a CRF decoder improves results only for some languages, and often only minimally so. On the other hand, the results clearly show document-level context to be of significant importance. We thus select the standard fine-tuning approach with document features (i.e.~without word embeddings or the CRF) as fine-tuning architecture for our comparative evaluation in Section~\ref{sec:evaluation}.









\begin{table}
\centering
\begin{tabular}{lc}
\hline
 Feature-based Approach             & \textsc{En}                \\ \hline \textsc{XLM-R} (last-four-layers)  & 91.17  0.29           \\        + WE                               & 92.19  0.46           \\        + Context                          & 94.23  0.19           \\        + WE + Context                     & 94.61  0.10           \\ \hline \textsc{XLM-R} (all-layer-mean)    & 94.37  0.06           \\        + WE                               & 95.63  0.04           \\        + Context                          & 96.09  0.07           \\        + WE + Context                     & \textbf{96.53}  0.10  \\ \hline \end{tabular}\caption{Evaluation of feature-based approach (development set).}
\label{tab:feature_based_development_dataset}
\end{table}


\subsection{Second Approach: Feature-Based}
\label{sec:feature_based}

Feature-based approaches instead use the transformer only to generate embeddings for each word in a sentence and use these as input into a standard sequence labeling architecture, most commonly a LSTM-CRF~\cite{2015arXiv150801991H}. The transformer weights remain frozen so that training is limited to the LSTM-CRF. Training typically uses SGD with a larger learning rate that is annealed against the development data. Training terminates when the learning rate becomes too small. Advantages of this approach include the use of a real stopping criterion and the relative ease of combining BERT embeddings with other types of word embeddings (e.g. "embedding \textit{stacking}"~\cite{akbik-etal-2019-flair}). 

\begin{table}[h!]
\begin{center}
\begin{tabular}{ l l }
\toprule
Parameter & Value \\
\midrule
LSTM hidden size & 256 \\
Learning rate & 0.1 \\
Mini batch size & 16 \\
Max epochs & 500 \\
Optimizer & SGD \\
Subword pooling & first \\
\bottomrule
\end{tabular}
\end{center}
\caption{\label{ner-training-parameters-feature-based} Parameters for feature-based approach.}
\end{table}

The parameters used for training a feature-based model are shown in Table~\ref{ner-training-parameters-feature-based}. 


\begin{table*}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{lllllll}
\hline
 Approach &  Context? & \textsc{En} & \textsc{De} &  & \textsc{Nl} & \textsc{Es} \\ \hline
 \textsc{\textit{XLM-R}}  & & & & & & \\ 
 
  - Feature-based & no & 91.83  0.06 & 82.88  0.28 & 87.35  0.17 & 89.87  0.45 & 88.78  0.08\\ 
   - Feature-based & yes & 93.12  0.14 & 84.86  0.11 & 89.88  0.26 & 91.73  0.21 & 88.98  0.11\\ 
   
  - Fine-tuning  & no &92.79  0.10 & 86.60  0.43 & 90.04  0.37 & 93.50  0.15 & 89.94  0.24 \\ 
 - Fine-tuning & yes &  93.64  0.05 & 86.99  0.24 & 91.55  0.07 & 93.99  0.16 & \textbf{90.14  0.14} \\ \hline 



 \textit{XLM-R (+ Dev) } & & & & & & \\ 
  - Fine-tuning & yes &  \textbf{93.96  0.11} & \textbf{88.21  0.43} & \textbf{92.29  0.18} & \textbf{94.66  0.12} & 89.93  0.03 \\ \hline
  
  
  \textsc{\textit{Monolingual*}}   & & & & & \\ 
 - Fine-tuning & yes & 93.84  0.12  & 85.77  0.13 & 89.56  0.55    &  92.15  0.12  &  89.16  0.11  \\ \hline
 
 \textit{Best published} & & & & & & \\ 
 \citet{akbik2019naacl} & pooling & 93.18  0.09 & - & 88.27  0.30 & 90.44  0.20 & - \\
 \citet{yu-etal-2020-named} & yes & 93.5 & 86.4 & 90.3 & 93.7 & 90.3 \\
 \citet{strakova-etal-2019-neural} & yes & 93.38 & 85.10 & - & 92.69 & 88.81 \\ \hline
 
\end{tabular}}
\caption{Comparative evaluation of best configurations of fine-tuning and feature-based approaches on test data. * corresponds to the following monolingual models: for English we use RoBERTa \citep{liu2019roberta},  DBMDZ\footnote{\url{https://github.com/dbmdz/berts}}-BERT for German, BERTje \citep{vries_bertje_2019} for Dutch and BETO \citep{CaneteCFP2020} for Spanish.}
\label{tab:stats_best_configurations}
\end{table*}


\subsubsection{Feature-Based Variants}



We compare two variants to produce token features:
\begin{description}
\item[All-layer-mean] In the first, we obtain embeddings for each token using mean pooling across all transformer layers, including the word embedding layer. This representation has the same length as the hidden size for each transformer layer. This approach is inspired by the ELMO-style~\cite{peters-etal-2018-deep} "scalar mix", that was shown to achieve better results than the best individual layer on most tasks~\cite{liu-etal-2019-linguistic,DBLP:conf/iclr/TenneyXCWPMKDBD19}.
\item[Last-four-layers] In the second, we follow~\citet{devlin-etal-2019-bert} to only use the last four transformer layers for each token and concatenate their representations into a final representation for each token. This representation thus has four times the length of the transformer layer hidden size.
\end{description}
We again evaluate both in all possible combinations of adding standard word embeddings "(+WE)" and using document-level features "(+Context)".


\subsubsection{Best Feature-Based Variant}


The results of parameter combinations for English\footnote{Other languages omitted to preserve space, but they show the same clear results.} are shown in Table~\ref{tab:feature_based_development_dataset}. The results clearly show that averaging over all layers outperforms using only the last four layers. In addition, the results clearly show that additionally using word embeddings as well as document-level context improves development set F1 score. For this reason, we chose a setup with all-layer-mean, word embeddings and document features for the feature-based experiments. 

\subsection{Summary of Best Configurations}

This section identified the best configurations for the fine-tuning and feature-based approaches to NER using development data. In both cases, experiments clearly showed that using document-level features improves NER quality. For the feature-based approach, we also find that an all-layer-mean strategy as well as the addition of word embeddings yields the best results. We employ these configurations in our final evaluation in the next section.

\section{Comparative Evaluation}
\label{sec:evaluation}

Using the best identified configurations,  
we conduct a final comparative evaluation on the test splits of the CoNLL-03 shared task datasets. The results of this evaluation are listed in Table~\ref{tab:stats_best_configurations}. For a meaningful comparison we also list previous state-of-the-art results. We additionally report two ablations: (1)~fine-tuning with both development and test data, and (2)~fine-tuning monolingual transformers. 





\subsection{Main Results}



We make the following observations:

\noindent
\textbf{Fine-tuning document-level features best.} As Table~\ref{tab:stats_best_configurations} shows, we find that fine-tuning outperforms the feature-based approach across all experiments (2 pp on average). 
Similarly, we find that document-level features clearly outperform sentence-level features (1.15 pp on average). Similar to our results on development data, we thus find fine-tuning with document-level features to work best across all languages. 

\noindent 
\textbf{New state-of-the-art results.} For the best approach, we also report results obtained with a training strategy in which we train over both train and development splits,  anneal against training loss (instead of development F1) and perform no model selection using development data. This yields new state-of-the-art F1 scores that in most cases outperform all currently published numbers: For German (original), our approach outperforms the best previous approach~\cite{yu-etal-2020-named} by 1.81 pp, and 2 pp on the revised German dataset. For Dutch and English, our approach outperforms the previous best by 0.96 pp and 0.46 pp respectively. For Spanish, however, our results lie slightly below those reported by \citep{yu-etal-2020-named}, with a difference of 0.16 pp.  


\begin{table}
\centering
\resizebox{220pt}{!}{
\begin{tabular}{lccccc}
\hline
 Entity &  \textsc{En} & \textsc{De}       &  & \textsc{Nl}       & \textsc{Es} \\ \hline
 LOC    & +0.44        & +0.23             & +1.97              & \underline{-0.74} & +0.17 \\
 MISC   & +0.22        & \underline{-0.90} & +1.66              & +1.16             & +0.72 \\
 ORG    & +1.21        & +0.56             & +0.74              & +1.66             & +0.11 \\
 PER    & +1.19        & +1.15             & +1.50              & \underline{-0.34} & +0.14 \\
 \hline
\end{tabular}}
\caption{Relative change in F1 for different entity types and languages when adding document-level context.}
\label{tab:comparison_context_vs_no_context}
\end{table}

\subsection{Analysis}

To get a better understanding of the impact of document-level features on different entity types, we perform a per-type analysis to compare average results across entity types with and without document context.  Results are shown in Table \ref{tab:comparison_context_vs_no_context}. 

We find that overall, the {\tt ORG} (organization) and {\tt PER} (person) entity types benefit the most from document-level context. In particular, the  {\tt ORG} type consistently improves across all languages. For other entity types, we find a mix of results depending on the language: For instance, document-level context reduces F1 score for {\tt LOC} (location) and {\tt PER} entities in Dutch, whereas these entities improve the most in all other languages. For German (original), document-features improve all entity types except for {\tt MISC} (miscellaneous, 0.9 pp) whereas for Spanish all entity types improve, albeit only slightly.






\subsection{Ablation: Monolingual Transformers}

As Table~\ref{tab:comparison_context_vs_no_context} also shows, we surprisingly find monolingual, language-specific transformers to underperform the multilingual XLM-R for most languages: For German, Dutch and Spanish the average F1 score is lower by 1.22 pp, 1.84 pp and 0.98 pp respectively when compared against XLM-R. This could be due to the fact that XML-R was trained over significantly larger data sets. Only for English we find that using RoBERTa \citep{liu2019roberta} instead of XLM-R leads to a slight increase in F1-score (0.2 pp).

\section{Conclusion}

In this paper we evaluated document-level features in two commonly used NER architectures. For each architecture, we conducted an evaluation of different variants to determine the best setup. We derive the following recommendations for transformer-based NER: 

\begin{itemize}
    \item For \textit{fine-tuning} approaches, we recommend a straightforward architecture without CRF and additional word embeddings with the hyperparameters as listed in Section~\ref{sec:fine_tuning}. 
    \item For \textit{feature-based} approaches, we recommend an all-layer-mean with additionally stacking classic word embeddings and the hyperparameters as listed in Section~\ref{sec:feature_based}.
\end{itemize}

\noindent
For both architectures, we find document-level features to significantly improve overall F1 score for all four languages in the CoNLL-03 benchmark. We also find fine-tuning to outperform the feature-based approach. 

We thus recommend the combination of document-level features and fine-tuning for NER. In our experiments, this yields new state-of-the-art F1 scores for English, German and Dutch and competitive numbers for Spanish. We integrate our approach into the \textsc{Flair} framework and share the recommended parameter settings to enable the research community to reproduce our results and apply our new state-of-the-art models in their tasks.


\bibliography{anthology,eacl2021}
\bibliographystyle{acl_natbib}
\newpage
\appendix
\section{Appendix: Figures} 

Figure~\ref{overview-bert-feature-based-document-level} gives an overview of the feature-based approach: Word representations are extracted from the transformer by either averaging over all layers (all-layer-mean) or by concatenating the representations of the last four layers (last-four-layers). These are then input into a standard sequence labeling model as features. 

\begin{figure*}[b]
 \centering
 \includegraphics[width=\linewidth]{figures/bert_feature_based.pdf}
 \captionof{figure}{
 Overview of Feature-based approach. Self-attention is calculated over all input tokens (incl. left and right context). The final representation for each token in the sentence ("I love Paris", shaded green) can be calculated as a) mean over all layers of transformer-based model or b) concatenating the last four layers. 
}
 \label{overview-bert-feature-based-document-level}
\end{figure*}


\end{document}
