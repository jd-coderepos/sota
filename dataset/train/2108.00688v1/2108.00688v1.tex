\documentclass[journal]{IEEEtran}
\usepackage{ifpdf}
\ifCLASSINFOpdf
  \usepackage[pdftex]{graphicx}
  \usepackage{graphbox}
  \graphicspath{{figures/}}
  \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  \usepackage[dvips]{graphicx}
  \graphicspath{{figures/}}
  \DeclareGraphicsExtensions{.eps}
\fi
\usepackage{amsmath,amsfonts}
\usepackage[bookmarks=false]{hyperref}
\usepackage{colortbl}
\usepackage{booktabs}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{siunitx}
\usepackage[table,dvipsnames]{xcolor}
\usepackage{cite}
\usepackage{nicefrac}
\usepackage{bbding}

\usepackage{algorithm}
\usepackage{listings}

\newcommand{\ie}{i.\,e.\ }
\newcommand{\eg}{e.\,g.\ }

\newcommand{\new}[1]{{\color{blue}#1}}


\hyphenation{op-tical net-works semi-conduc-tor}

\begin{document}
\title{Self-supervised Audiovisual Representation Learning for Remote Sensing Data}

\author{Konrad~Heidler,~\IEEEmembership{Student Member,~IEEE},
        Lichao~Mou,
        Di~Hu,
        Pu~Jin,
        Guangyao~Li,
        Chuang~Gan,
        Ji-Rong~Wen,~\IEEEmembership{Senior Member,~IEEE}
        and~Xiao~Xiang~Zhu,~\IEEEmembership{Fellow,~IEEE}\thanks{
  This work is supported by Helmholtz Association’s Initiative and
  Networking Fund through Helmholtz AI [grant number: ZT-I-PF-5-01] --
  Local Unit ``Munich Unit @Aeronautics, Space and Transport (MASTr)'',
  by the German Federal Ministry of Education and Research (BMBF)
  in the framework of the international future AI lab
  ``AI4EO -- Artificial Intelligence for Earth Observation: Reasoning,
  Uncertainties, Ethics and Beyond'' (Grant number: 01DD20001),
  by the Fundamental Research Funds for the Central Universities, 
  by the Research Funds of Renmin University of China (NO. 2021030200), 
  and by the Beijing Outstanding Young Scientist Program (NO. BJJWZYJH012019100020098), Public Computing Cloud, Renmin University of China.
}\thanks{K. Heidler, L. Mou and X. Zhu are with the Remote Sensing Technology Institute (IMF),
  German Aerospace Center (DLR), 82234 Wessling, Germany,
  and also with the Data Science in Earth Observation
  (SiPEO, formerly Signal Processing in Earth Observation),
  Technical University of Munich (TUM),
  80333 Munich, Germany.
  E-mails: konrad.heidler@dlr.de; lichao.mou@dlr.de; xiaoxiang.zhu@dlr.de}\thanks{D. Hu, G. Li, and J.-R. Wen are with the
  Gaoling School of Artificial Intelligence and the
  Beijing Key Laboratory of Big Data Management and Analysis Methods,
  Renmin University of China,
  100872 Beijing, China.
  E-mails: dihu@ruc.edu.cn; guangyaoli@ruc.edu.cn; jrwen@ruc.edu.cn
}\thanks{P. Jin is with the Technical University of Munich (TUM),
    80333 Munich, Germany.
    E-mail: pu.jin@tum.de
}
\thanks{C. Gan is with the MIT-IBM Watson AI Lab, Cambridge, MA 02142 USA.
    E-mail: ganchuang1990@gmail.com
}
\thanks{Corresponding author: Di Hu.
}
}
\markboth{}{Heidler \MakeLowercase{\textit{et al.}}: Self-supervised Audiovisual Representation Learning for Remote Sensing Imagery}

\maketitle
\begin{abstract}
  Many current deep learning approaches make
  extensive use of backbone networks pre-trained on large datasets like ImageNet,
  which are then fine-tuned to perform a certain task.
  In remote sensing, the lack of comparable large annotated datasets
  and the wide diversity of sensing platforms impedes similar developments.
In order to contribute towards the availability of pre-trained backbone networks in remote sensing, 
  we devise a self-supervised approach for pre-training deep neural networks.
  By exploiting the correspondence between geo-tagged audio recordings and remote sensing imagery,
  this is done in a completely label-free manner,
  eliminating the need for laborious manual annotation.
  For this purpose, we introduce the \emph{SoundingEarth} dataset,
  which consists of co-located aerial imagery and audio samples all around the world.
  Using this dataset, we then pre-train ResNet models to map samples from both modalities into
  a common embedding space,
  which encourages the models to understand key properties of a scene that influence
  both visual and auditory appearance.
To validate the usefulness of the proposed approach,
  we evaluate the transfer learning performance of pre-trained weights obtained
  against weights obtained through other means.
  By fine-tuning the models on a number of commonly used remote sensing datasets,
  we show that our approach outperforms existing pre-training strategies for remote sensing imagery.
The dataset, code and pre-trained model
  weights will be available at \url{https://github.com/khdlr/SoundingEarth}
\end{abstract}
\begin{IEEEkeywords}
Self-supervised learning, multi-modal learning, representation learning, audiovisual dataset
\end{IEEEkeywords}

\IEEEpeerreviewmaketitle
\vspace{1cm}
\section{Introduction}



\IEEEPARstart{I}{magine} yourself standing in a lush green forest. You can see the green of the trees around you, maybe a brown, muddy path below your feet. At the same time, you can hear the leaves rustling in the wind, and the songs of some birds nearby.
Now try to imagine one without the other, the same forest scenery but completely silent, or the soundscape without any visual context. Chances are, you will find it \emph{hard to clearly separate these impressions completely}.

In most situations, our mind makes use of multiple of our senses to perceive the scenery around us.
By basing our perception of the world on multiple senses, we get a more robust impression of our surroundings than if we were to rely on a single sense.
In fact, phenomena like the \emph{McGurk effect}~\cite{mcgurk_hearing_1976}
suggest that the distinction between human vision and hearing might not even be as clear as we think.
\begin{figure}
    \centering
    \setlength\tabcolsep{2pt}
    \begin{tabular}{cc}
\includegraphics[width=0.47\linewidth]{example/53254.jpg}&\includegraphics[width=0.47\linewidth]{example/47342.jpg}\\\includegraphics[width=0.47\linewidth]{example/53254_waveform.png}&\includegraphics[width=0.47\linewidth]{example/47342_waveform.png}\\\includegraphics[width=0.47\linewidth]{example/53254_logmel.jpg}&\includegraphics[width=0.47\linewidth]{example/47342_logmel.jpg}\\\end{tabular}
    \caption{Two examples of corresponding imagery and audio.
    From top to bottom: Aerial image, waveform, log-mel spectrogram.
    For visualization purposes, the audio data was clipped to the first 20 seconds,
    even though the full samples are much longer.
    They can be found at
    \href{https://archive.org/details/aporee_46891_53254}{\footnotesize{\texttt{archive.org/details/aporee\_46891\_53254}}},
    and
    \href{https://archive.org/details/aporee_41512_47342}{\footnotesize{\texttt{archive.org/details/aporee\_41512\_47342}}},
    respectively.
    }
    \label{fig:example}
\end{figure}

Given the great added value of combining our vision and hearing as stated above, the simultaneous processing of visual imagery and sounds is something that comes very natural to us as humans.
Recent studies have shown remarkable advances in audiovisual machine learning~\cite{arandjelovic_look_2017,petridis2018end,tzirakis2017end}.
However, there still remains a paucity of research on understanding the earth in an audiovisual way~\cite{hu_cross-task_2020}.

The advances in airborne and spaceborne platforms open up new possibilities for sensing and understanding our planet from a bird’s eye view,
and plenty of new overhead images are acquired daily.
However, most existing works focus on processing and analyzing inputs from a single modality (\ie vision).

In this paper, we demonstrate a way of exploiting combined audiovisual earth observation data.
To facilitate the above research, we built a large-scale \emph{SoundingEarth} dataset that consists of pairwise audio and imagery captured at the same geographical location, and is tailored towards audiovisual learning in the context of remote sensing. 
 
It turns out that the task of matching imagery and audio is
instructive for neural networks in the sense that it teaches the networks
how to learn useful and general features without the need for labels.
To our knowledge, this is the first work to introduce self-supervised
pre-training from scratch on audiovisual remote sensing data.
As we show in our experiments, network weights trained in this way are
better suited for the evaluated tasks
than those obtained by self-supervised pre-training on the single modality of
aerial imagery or the commonly used ImageNet weights.



To summarize, this work's contributions are threefold.
\begin{itemize}
    \item In Section~\ref{sect:dataset}, we build a large-scale dataset to facilitate this task, called \emph{SoundingEarth}, which consists of more than 50K co-localized field recordings and overhead imagery pairs, collected from a publicly available audio source.
    \item In Section~\ref{sect:methodology},
    we describe a framework for the pre-training of
    deep neural network models based on the audiovisual correspondence of
    aerial imagery and field recordings. 
    This framework is trained using a batch-wise triplet loss,
    which we propose to combine the benefits of classical triplet loss training
    with those of recent contrastive learning methods.
    \item In Section~\ref{sect:experiments}, we report and discuss the results
    of our extensive experiments on downstream
    tasks that demonstrate the effectiveness of our approach with superior performance over state-of-the-art methods.
\end{itemize}

\section{Related Work}
\subsection{Audiovisual Learning}
Exploiting the relationship between audio and imagery is an emerging topic that has enjoyed increased attention
in the machine learning community.
In the early stage, their relationship was first explored and exploited in audiovisual speech recognition~\cite{petridis2018end} and affect classification~\cite{soleymani2011multimodal,tzirakis2017end}, where the visual and audio modalities are considered to have influences on each other due to the findings of the \emph{McGurk} effect~\cite{mcgurk_hearing_1976}. 
In the deep learning era, their relationship is further investigated in cross-modal transfer learning, where the predictions of a well-trained visual (audio) network are employed as the supervision for training a student network for the audio (visual) modality~\cite{aytar2016soundnet,owens2016ambient}. 

Perhaps the most prominent type of audiovisual data is given by videos. Millions of hours of video content are readily available through online video platforms, and can be filtered by topics and keywords.
Therefore, a few approaches are proposed to directly use massive unlabeled video datasets for self-supervised model training.
In \cite{arandjelovic_look_2017}, the authors use a large dataset of unlabeled videos to train a model on the frame-to-sound correspondence. Without any additional supervision, this model gains the ability to discern semantic concepts in both modalities.
Similarly, deep clustering approaches can also learn meaningful representations when the clustering information is cross-fed and used as supervision for the other modality~\cite{alwassel_self-supervised_2020}.
Other possible tasks include
temporal alignment~\cite{korbar_cooperative_2018,owens_audio-visual_2018},
audiovisual scenario analysis,
\eg sound source localization~\cite{senocak2018learning,qian2020multiple}
and sound separation~\cite{zhao2018sound,gao2019co}.

Collecting video data tailored towards a specific task is also an option. Owens et al.~\cite{owens_visually_2016} recorded short video clips where they hit or scratched a large variety of objects with a drumstick. They then trained a network to predict the resulting sounds from just the visual video data, as well as to predict the material of the probed object from both video and audio. 
Apart from directly using both modalities,
derived data can also be used as a target.
For example, in \cite{owens2016ambient}, the authors
predict audio statistics for the ambient sound from an image.

To take it a step further, text data can be added as a third input mode. In~\cite{aytar_see_2017}, the authors train student networks on triplets of image, sound and text to match the output distribution of a pre-trained ResNet model. By sharing the final layers between the modalities, the internal feature representations of the models become aligned and allow for cross-modal analysis.

Different from these works, this paper explores the audiovisual relationship in terms of geographical location, based on which we target to achieve reasonable geo-understanding in an audiovisual way.  

A few existing works address audiovisual machine learning in
the context of remote sensing.
In~\cite{salem2018multimodal}, the authors propose to
combine the audiovisual correspondence with a clustering algorithm
to build an ``aural atlas''.
Another study~\cite{hu_ambient_2020} shows that
fusing audiovisual information can greatly benefit
the task of crowd counting.
Finally, cross-modal retrieval is a recently popular task in
remote sensing~\cite{mao_deep_2018,chen_deep_2020,chen_deep_2020-1}.

\subsection{Self-supervised Model Pre-training}
Self-supervised learning is a subset of unsupervised learning methods~\cite{jing2020self}.
It is usually implemented in one of three ways,
either based on contextual similarity, temporal similarity
or contrastive methods.
Early work in self-supervised pre-training for deep neural networks aimed to effectively train stacked auto-encoders~\cite{bengio2007greedy} and deep belief networks (DBNs)~\cite{hinton2006reducing} without labels.
Recently, self-supervised learning methods like MoCo~\cite{he_momentum_2020}, SimCLR~\cite{simclr}, BYOL~\cite{grill2020bootstrap} and SwAV~\cite{caron2020unsupervised}
have significantly reduced the gap with supervised methods.
The most recent self-supervised models pre-trained on ImageNet even surpass supervised pre-trained models on multiple downstream tasks~\cite{he_momentum_2020}.
There is also some work focused on pre-training audio representations~\cite{tagliasacchi2020pre}.
The success of self-supervised learning in the pre-training of models
makes researchers feel very excited,
but also encourages more inspiration.
Especially in the field of remote sensing image analysis, research based on self-supervised learning has gradually attracted attention.
Some researchers\cite{stojnic2018analysis,stojnic2018evaluation} used split-brain autoencoders
to analyze aerial images, and explored the number of images used for self-supervised learning and the influence of the use of different color channels on aerial image classification.
Ayush et al.\cite{ayush_geography-aware_2020} introduced a contrastive loss and a loss term
based on image geolocation classification to enhance the aerial image.
Tao et al.\cite{tao2020remote} analyzed the possibility of using different self-supervised 
methods, especially based on image restoration,
context prediction and the use of different enhanced contrast learning methods,
and conducted training on a small remote sensing image dataset of 30,000 images.
Additionally, Kang et al.\cite{kang2020deep} trained on 100,000 remote sensing image patches
based on comparative learning with different enhancements and tested them
on the NAIP~\cite{tile2vec} and EuroSAT~\cite{helber2019eurosat} tasks.
These self-supervised learning methods have made a series of achievements
in remote sensing data analysis,
but they only consider information from the visual mode,
and do not use the sound information corresponding to the visual scene.
In this work, we utilize self-supervised audiovisual representation
learning for downstream tasks on aerial imagery.

\subsection{Pre-trained Models in Remote Sensing}
There are two prevailing methods of initializing deep learning models
for remote sensing tasks before training~\cite{zhu_deep_2017}:
\begin{enumerate}
\item Training models from scratch,
\ie initializing the weights in a completely random fashion
and only training the model on the given dataset.
\item Using models pre-trained on natural imagery tasks,
like the ImageNet dataset~\cite{imagenet}.
\end{enumerate}

Both practices have their respective issues.
While random model initialization can be used for data from all
sensing platforms, it requires a large amount of
labelled data to converge to satisfactory results,
and can lead to overfitting and poor generalization~\cite{zhu_deep_2017}.
On the other hand, weights pre-trained on natural images
will only work for RGB imagery.
While the modalities of ground-level and overhead imagery
are very different from each other,
this approach sometimes works surprisingly well~\cite{guo_crossdomain_2020}.

A central issue with remote sensing data is the large number of different
sensing platforms.
For example, a model trained on aerial imagery cannot be applied to optical multispectral imagery.
Other acquisition methods like SAR and hyperspectral imagery
further complicate things.
This might be the reason for the scarcity of
large annotated remote sensing datasets available for pre-training.
Seeing this predicament, quite a number of self-supervised pre-training tasks have
been studied in the context of remote sensing.

Early works in this field
were using techniques like
greedy layer-wise unsupervised pre-training~\cite{romero_unsupervised_2016}.
Another notable early work is Tile2Vec~\cite{tile2vec},
where the model was trained to match imagery patches
based on their spatial proximity,
inspired by a similar pre-training task for natural language.
More recent studies in pre-training for remote sensing usually involve some
pre-text tasks like the colorization of images~\cite{vincenzi_color_2021},
super-resolution of imagery~\cite{peng_pre-training_2021},
or classifying whether two patches overlap~\cite{leenstra_self-supervised_2021}.
It is also possible to pre-pre-train a network on natural imagery
before pre-training on aerial imagery in a second step~\cite{reed_self-supervised_2021}.

Exploiting particular properties of remote sensing data
for self-supervised learning
is another promising, active area of research
that is quickly gaining traction.
Recently, Ayush et al.~\cite{ayush_geography-aware_2020}
suggested a way of extending
the MoCo~\cite{he_momentum_2020} framework to include
a geography-aware loss term,
which improves the learned representations
compared to training using regular MoCo.
Going in a different direction, Ma\~nas et al.~\cite{manas_seasonal_2021}
combine acquisitions from different seasons with
traditional image augmentations,
and encode the information in multiple orthogonal subspaces.
This effectively separates seasonal variations from
other transformations in the encodings.
Much in the same vein as these recent works,
our goal is to learn features from the information contained in
the colocation of audio and imagery.

\section{The \emph{SoundingEarth} Dataset}\label{sect:dataset}
\begin{figure}
  \center
  \includegraphics[width=0.99\linewidth]{distribution.jpg}
  \caption{Spatial distribution of samples in
      our SoundingEarth dataset.
  }
  \label{fig:distribution}
\end{figure}

\begin{table*}[t]
  \centering
  \caption{Comparison of Audiovisual datasets focusing on Remote Sensing Imagery}
  \begin{tabular}{cccccc}
    \toprule[1pt]
    Dataset & Audio Source & Audio Duration & Audio Content & Image Source & Amount \\
    \midrule[1pt]
    CVS~\cite{salem2018multimodal}& Freesound & - & unspecified & Bing Maps & 23,308\\
    ADVANCE~\cite{hu_cross-task_2020}& Freesound & $\sim$14h& unspecified & Google Earth & 5,075\\
    SoundingEarth & Radio Aporee :::~Maps &  $\sim$3500h & field recording & Google Earth & 50,545\\
    \bottomrule[1pt]
  \end{tabular}
  \vspace{6pt}
  \label{tabel:compare}
\end{table*}
\begin{figure}
  \includegraphics[width=0.9\linewidth]{duration}
  \caption{Histogram of the audio durations in the dataset. The rightmost bin sums up all durations longer than 10 minutes.
  }
  \label{fig:duration}
\end{figure}
To enable audiovisual pre-training in the context of remote sensing,
we introduce a new dataset for geo-aware audiovisual learning in remote sensing, which we call the \emph{SoundingEarth} Dataset. We will outline the process of building the dataset as well as the statistical analysis on it. The development of the dataset is split in two steps, first the acquisition and cataloging of geo-tagged audio data, and then the extraction of corresponding overhead imagery.

\subsection{Collection of Geo-tagged Audio}
Sources for representative and geo-tagged audio are rare.
Among the few public audio libraries that include geo-tags,
most contain samples that have little connection to their geographical surroundings.
In contrast, collecting audio samples that capture a local ambience well
is the central point of the
Radio~Aporee~:::~Maps project~\cite{noll_radio_2019}.
Started in 2006 by Udo Noll,
the project represents a collective effort of gathering a global soundmap from many geo-tagged \emph{field recordings},
which refer to any audio recordings made ``in the field''.

Anyone can contribute to this soundmap by uploading their own recordings,
which leads to a nearly global coverage of the samples,
even though a large fraction of the samples is clustered in areas like Europe
or the United States (see~Fig.~\ref{fig:distribution}).
The guidelines for uploading sounds to the site include requirements for quality, length, and a focus on local ambience. 
Upon uploading, the creators either put their recordings under one of the creative commons licenses,
or release them into the public domain,
making the audio data fit as a training set for machine learning approaches.
All of Radio Aporee :::~Maps's audio data is mirrored on the \emph{Internet Archive}\footnote{It can be found under the following link:
    \url{https://archive.org/details/radio-aporee-maps}
}.
As the clear orientation of Radio Aporee :::~Maps towards field recordings
imposes a homogeneous composition of the collected audio samples,
most of the recorded audio samples give the listener a vivid impression of the recorded scene.
For geospatial analysis, this project therefore constitutes a treasure trove of audio data.

At the time of our download,
the database contained about 435GB of high quality audio data,
with metadata for each sample including the geographical coordinates,
the creator's name necessary for correct attribution,
and in many cases a short textual description of the audio.

\subsection{Collection of Aerial Imagery}
Using the geographical coordinates from the audio samples, we matched the audio samples with corresponding imagery by extracting the image tiles from Google Earth
in an automated fashion.
Given the longitude and latitude where the audio was recorded, a tile of $1024 \times 1024$ pixels is extracted
at the highest available resolution from Google Earth.
This implies a spatial resolution of approximately \SI{0.2}{\metre} per pixel.

\subsection{Data Cleaning}
As already mentioned, the audio recordings in the dataset have an exceptional level of quality, both regarding audio fidelity and the recorded content. Therefore, very few manual corrections were applied. As it is infeasible to listen to the thousands of hours of audio content, our data cleaning routine was limited to a full-text search over the recordings' filenames and textual descriptions to filter out nondescript audio samples like ``testsound.mp3''. During this semi-automated cleaning process, 621 samples were excluded from the dataset.

\subsection{Dataset Overview}\label{subsec:data_overview}
As of June 2020, Radio Aporee :::~Maps has collected over 50,000 geo-tagged
field recordings from 136 countries all over the world,
as can be seen in Figure~\ref{fig:distribution}.
As a result, our built \emph{SoundingEarth} dataset consists of 50,545 image-audio pairs.
The total length of the audio amounts to more than 3,500 hours of ambient sounds.
This makes the dataset much larger than existing audiovisual datasets focusing
on aerial imagery (see~Table~\ref{tabel:compare}).

One notable property of the dataset is its extreme skew of audio length values.
While the median duration is about 3 minutes, the longest 1\% of the audio samples exceed half an hour in duration. The general distribution of the duration in minutes can be seen in Figure~\ref{fig:duration}.
To facilitate further research in audiovisual based geo-understanding,
the built \emph{SoundingEarth} dataset will be publicly available.

\section{Audiovisual Model Pre-Training}\label{sect:methodology}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/framework}
    \caption{Overview of the proposed pre-training method. After sampling a batch,
        the corresponding images and spectrograms are augmented and then embedded into the
        representation space by the Image and Audio CNNs.
        The loss function then causes corresponding
        image and audio embeddings to be drawn together,
        while samples from different locations are pushed away from each other.
    }
    \label{fig:framework}
\end{figure}
Following recent advances in self-supervised representation learning
for images \cite{simclr,he_momentum_2020},
we develop a framework to automatically learn representations from the paired audiovisual data.
The goal of this framework is to build a common embedding space for imagery and audio,
where the embeddings of corresponding audiovisual pairs are close together while
the embeddings for distinct pairs are farther apart from each other.
For both modalities, we train a CNN to perform this projection.
The underlying assumption of this methodology is the idea that the networks will learn features that
represent the commonalities between the visual imagery and the sound recorded at the scene.
In turn, these features need to be of a high abstraction level,
and will therefore be useful for a number of downstream tasks.

\subsection{Data Preparation and Augmentation}
Before training the networks,
the input data needs to be transformed into a suitable format for the CNNs.
While imagery is the natural input domain for CNNs,
digital audio is represented quite differently,
namely as a waveform, which consists of a sequence of samples.
To get the audio into a more serviceable representation,
we first apply a short-time Fourier transform (STFT).
This converts the audio into a two-dimensional representation,
with the added second dimension representing frequency or pitch.
The squared absolute values of these coefficients are then mapped to mel-scale using 128 filter bands.
Finally, the logarithm is taken to arrive at a log-mel spectrogram.
After this conversion process, the audio representation is equivalent to that of a grayscale
image with size $128 \times T$,
with the second dimension $T$ depending on the duration of the audio sample.

In order to prevent pure memorization of the input data and
introduce more variety into the training samples,
a number of data augmentation techniques are applied both to the
imagery as well as the audio spectrograms.

Given the arbitrary length of the audio,
a random sample of 128 consecutive spectrogram frames is extracted
from the original spectrogram, resulting in a square spectrogram of size $128 \times 128$.
Further audio augmentations like random volume adjustments or frequency shifts did not
bring any further improvements,
likely because of the translation-invariance of CNNs,
as well as the scale-invariance introduced by batch normalization layers.

For the training images,
we first cropped the central half of the image to ensure that the augmented scenes
do not deviate too far from the true location.
Then, a square crop sized randomly between 192 and 384 pixels was extracted
and scaled to 192 pixels in size.
Finally, random adjustments were done with regard to
rotation, blur, hue, saturation and value (lightness).
Efficient data augmentations were enabled by the \emph{albumentations}
python library~\cite{albumentations}.

\subsection{Embedding Networks}
Pixel images and sound waveforms exist in distinct representation spaces and have different
statistical properties~\cite{srivastava2014multimodal}.
In order to still derive common features and represent these highly non-linear
semantic correlations across modalities,
image and sound are encoded with modality-specific networks to represent them in
the common embedding space.

\subsubsection{Visual Subnet}
The visual pathway adopts a ResNet architecture~\cite{he2016deep}.
It is working on inputs of size $3 \times 192 \times 192$ at training time.
To better assess the transferability of the framework to different network architectures,
both ResNet-18 and ResNet-50 are evaluated as visual encoders.
After the convolutional stages of the ResNet,
the data is transformed into rich feature maps.
In order to get a single vector representing the entire
image, these feature maps are merged using
global average pooling,
followed by a final fully connected layer.

\subsubsection{Audio Subnet}
The audio pathway operates on log-mel spectrograms of size $1 \times 128 \times 128$.
Given the reduced complexity of the spectrograms compared to RGB imagery,
we only employ a ResNet-18 encoder for this subnet.
Just like with the visual encoder,
the convolutional features of the ResNet encoder are
globally averaged and fed into a final fully connected layer,
so that the final representation for both modalities is
given by a single vector for each.

\subsection{Batch Triplet Loss}
After acquiring representations for visual and audio inputs,
we train the two networks in a way that encourages corresponding
bimodal inputs to match each other closely in the embedding space.

Conventional representation learning methods
compare the embeddings of two~\cite{hadsell_dimensionality_2006}
or three~\cite{weinberger_distance_2009,schroff_facenet_2015} samples,
which discards a lot of possible learning feedback.
Therefore, a key idea in recent contrastive learning techniques
is to use all possible pairings in a training batch~\cite{simclr}.

We combine this idea with triplet loss,
resulting in a \emph{batch triplet loss} objective.
For visual embeddings $v_i$ and corresponding audio embeddings $a_i$,
we first calculate the matrix of pairwise distances $D(a, v)$ as
\begin{equation}
    D = \begin{pmatrix}
        \|a_1 - v_1\|_2 & \dots & \|a_1 - v_n\|_2 \\
        \vdots & \ddots & \vdots \\
        \|a_n - v_1\|_2 & \dots & \|a_n - v_n\|_2 \\
    \end{pmatrix}\,.
\end{equation}
The objective of the representation learning procedure should then
be to minimize the diagonal entries of that matrix
while keeping all other values above a certain margin.
Keeping in mind the original formulation of the triplet margin loss function~\cite{weinberger_distance_2009}
as
\begin{equation}\label{eq:naive_tl}
    \mathop{\mathcal{L}}(x, y^+, y^-) = \max{(0, \|x - y^+\|_2 - \|x - y^-\|_2 + 1)},
\end{equation}
we apply this to all possible pairings of diagonal elements and off-diagonal elements for each
row and column:
\begin{equation}\label{eq:fullbatch_tl}
    \arraycolsep=1.4pt
    \begin{array}{rl}
    \displaystyle
    \mathcal{L}(D) =&
    \displaystyle
    \sum_i\sum_{j \neq i} \max{(0, D_{ii} - D_{ij} + 1)}\,+\\
    +&\displaystyle
    \sum_j\sum_{i \neq j} \max{(0, D_{ii} - D_{ij} + 1)}\,.
    \end{array}
\end{equation}

\begin{algorithm}[t]
\caption{Batch Triplet Loss in PyTorch-like pseudocode.}
\label{alg:fullbatch_tl}
\definecolor{codeblue}{rgb}{0.25,0.5,0.5}
\lstset{
  backgroundcolor=\color{white},
  basicstyle=\fontsize{7pt}{7pt}\ttfamily\selectfont,
  columns=fullflexible,
  breaklines=true,
  captionpos=b,
  commentstyle=\fontsize{7pt}{7pt}\color{codeblue},
  keywordstyle=\fontsize{7pt}{7pt}\bfseries,
}
\begin{lstlisting}[language=python]
def batch_triplet_loss(v, a):
  diff = v.unsqueeze(1) - a.unsqueeze(0)  # pairwise diff's
  D = norm(diff, dim=2)  # distance matrix
  D_true = D.diagonal()  # distances of the true pairings
  
  d_col = sum(relu(D_true.unsqueeze(0) - D + 1.0))
  d_row = sum(relu(D_true.unsqueeze(1) - D + 1.0))
  
  return d_col + d_row
\end{lstlisting}
\end{algorithm}
%
 Alg.~\ref{alg:fullbatch_tl} shows the pseudocode for this loss function.
Our experiments in section~\ref{sect:ablation} show that this approach outperforms the naive triplet
loss formulation from Eq.~\ref{eq:naive_tl} as well as
the contrastive loss used in~\cite{simclr}.

\section{Transfer Learning Experiments}\label{sect:experiments}
The penultimate goal of this work is to provide pre-trained networks for downstream applications.
In order to confirm the hypothesis that our weights are indeed better suited for remote sensing
tasks than other sets of weights, we evaluate them against a number of competitors on
several downstream tasks.

\subsection{Competing pre-training schemes}
\subsubsection{Random}
The predominant method of initializing backbone weights in remote sensing
is to initialize them completely at random.
To quantify the benefit of pre-trained weights,
we evaluate random weight initialization as a baseline.
\subsubsection{ImageNet}
The first actual pre-training method for RGB imagery is to use weights
trained on the classification task in the ImageNet dataset~\cite{imagenet}.
As these are readily available in most deep learning frameworks,
this method is very common and has proven successful on ground-level imagery and some remote sensing
tasks as well.
However, due to the different nature of ImageNet images and remote-sensing overhead images,
we speculate that this might not be the optimal strategy.
\subsubsection{Tile2Vec}
This method learns weights in a self-supervised fashion from the spatial relations
of overhead imagery~\cite{tile2vec}.
In the original paper,
the weights were trained on NAIP imagery which includes not only RGB but an additional NIR channel
as well.
For our experiments on RGB imagery,
we have to impute this fourth channel by the mean value of the other channels,
which may lead to decreased performance.
As the authors provide only pre-trained ResNet-18 weights,
this approach is not included in the ResNet-50 evaluations.
\subsubsection{Contrastive}
Plain contrastive learning without an additional projecting head,
as outlined in~\cite{simclr}.
\subsubsection{SimCLR}
A recent advance in self-supervised learning was given by SimCLR~\cite{simclr},
which combines extensive data augmentation strategies
with the contrastive loss objective function.
\subsubsection{Momentum Contrast (MoCo)}
Another fairly recent addition to the family of self-supervised learning methods,
MoCo~\cite{he_momentum_2020} aims to align the representations
of different image augmentations between the model and a \emph{momentum encoder},
which is a copy of the model that is updated via exponential moving average.

For ImageNet weights, we used the ones distributed by the torchvision python package,
while for Tile2Vec we used the weights made available by the authors~\cite{tile2vec}.
All other methods were trained by us on the previously introduced dataset.
Naturally the image-only pre-training methods were applied to only the visual part of the data.

\subsection{Aerial Image Classification}
A common task in remote sensing is to categorize scenes into one of several pre-defined classes.
Due to the importance of this task, a great number of available datasets exist.
For our comparisons, we have evaluated the models on three such datasets.
The datasets and the obtained results are discussed in the following paragraphs.
As the evaluated networks are already pre-trained,
we follow the evaluation protocol from~\cite{xia_aid_2017},
where only 50\% of the data are used for training
and the other 50\% are used for evaluation purposes.

\subsubsection{UC Merced Land Use}
\begin{table}
  \center
  \caption{
    Results on UC Merced Land Use~\cite{ucmerced}, values displayed in \%.
  }\label{table:ucmerced}
  \begin{tabular}{llrrr}
    \toprule
    &&\multicolumn{3}{c}{Accuracy after}\\
    Weights&Backbone&1 epoch&2 epochs&5 epochs\\
    \midrule
    Random                        & ResNet-18 & 12.10 & 42.57 & 45.81 \\
    ImageNet                      & ResNet-18 & 46.29 & 59.24 & 82.10 \\
    Tile2Vec~\cite{tile2vec}      & ResNet-18 & 38.67 & 59.05 & 74.38 \\
    Contrastive~\cite{simclr}     & ResNet-18 & 39.90 & 63.43 & 80.95 \\
    SimCLR~\cite{simclr}          & ResNet-18 & 58.95 & 77.33 & 88.48 \\
    MoCo~\cite{he_momentum_2020}  & ResNet-18 & 50.86 & 67.05 & 77.33 \\
    Ours                          & ResNet-18 & \textbf{71.33} & \textbf{85.81} & \textbf{90.19} \\
    \midrule
    Random                        & ResNet-50 &  9.24 & 19.71 & 44.95 \\
    ImageNet                      & ResNet-50 & 24.29 & 37.52 & 80.19 \\
    Contrastive~\cite{simclr}     & ResNet-50 & 39.81 & 67.52 & 84.57 \\
    SimCLR~\cite{simclr}          & ResNet-50 & 56.48 & 75.71 & 85.43 \\
    MoCo~\cite{he_momentum_2020}  & ResNet-50 & 53.71 & 64.29 & 78.95 \\
    Ours                          & ResNet-50 & \textbf{72.29} & \textbf{87.24} & \textbf{89.71} \\
    \bottomrule
  \end{tabular}
  \vspace{6pt}
\end{table}
 The first dataset~\cite{ucmerced} contains 2,100 overhead images
with each belonging to one of 21 land-use classes.
The images in this dataset are 256$\times$256 pixels in size
and come at a spatial resolution of $\sim$0.3m.
Extracted from the USGS National Map Urban Area Imagery collection,
they cover various regions in the United States.
Results for this dataset are presented in table~\ref{table:ucmerced}.
Here, our pre-training method clearly shows superior results compared to the other evaluated methods,
especially on the more light-weight ResNet-18 architecture.
However, this dataset is sometimes criticized for
being both very small and simple to solve~\cite{resisc45,xia_aid_2017}.
Therefore, we conduct further evaluations on two other datasets
which both set out to address these two issues.

\subsubsection{NWPU-RESISC45}
\begin{table}
  \center
  \caption{
    Results on NWPU-RESISC45~\cite{resisc45}, values displayed in \%.
  }\label{table:resisc45}
  \begin{tabular}{llrrr}
    \toprule
    &&\multicolumn{3}{c}{Accuracy after}\\
    Weights&Backbone&1 epoch&2 epochs&5 epochs\\
    \midrule
    Random                        & ResNet-18 & 31.21 & 42.32 & 57.65 \\
    ImageNet                      & ResNet-18 & 69.83 & \textbf{77.89} & \textbf{83.76} \\
    Tile2Vec~\cite{tile2vec}      & ResNet-18 & 52.44 & 58.03 & 69.73 \\
    Contrastive~\cite{simclr}     & ResNet-18 & 59.41 & 67.75 & 81.49 \\
    SimCLR~\cite{simclr}          & ResNet-18 & 69.77 & 73.68 & 80.36 \\
    MoCo~\cite{he_momentum_2020}  & ResNet-18 & 51.94 & 64.09 & 78.28 \\
    Ours                          & ResNet-18 & \textbf{73.82} & 76.30 & 81.71 \\
    \midrule
    Random                        & ResNet-50 & 25.96 & 36.89 & 42.48 \\
    ImageNet                      & ResNet-50 & 68.49 & 72.36 & 83.06 \\
    Contrastive~\cite{simclr}     & ResNet-50 & 63.55 & 70.60 & 81.34 \\
    SimCLR~\cite{simclr}          & ResNet-50 & 68.14 & 75.16 & 80.69 \\
    MoCo~\cite{he_momentum_2020}  & ResNet-50 & 56.39 & 64.70 & 76.98 \\
    Ours                          & ResNet-50 & \textbf{77.17} & \textbf{79.82} & \textbf{84.88} \\
    \bottomrule
  \end{tabular}
\end{table}
 Created in an attempt to improve upon the size and diversity of the UC Merced dataset,
the Remote Sensing Image Scene Classification dataset by the
Northwestern Polytechnical University (NWPU-RESISC45)~\cite{resisc45}
consists of 31,500 images from 45 categories.
These images are taken from Google Earth and also have a size of 256$\times$256.
Other than with the UC Merced dataset,
these scenes are of varying resolution (between 0.2 and \SI{30}{\metre} per pixel)
and are taken from locations all around the world.
As can be seen in table~\ref{table:resisc45},
this benchmark task does indeed pose a bigger challenge to the models
than the previous one.
Out of the competing methods, both the ImageNet weights and
the SimCLR weights are strong contenders on this dataset.
However, our method performs on par with these approaches,
and even has a slight advantage on the ResNet-50 evaluation.

\subsubsection{AID}
Much like the NWPU-RESISC45 dataset,
the Aerial Image Dataset (AID)~\cite{xia_aid_2017} aims to provide an aerial scene classification dataset
that is both large and diverse.
It is composed of 10000 images from 30 categories,
which were acquired from Google Earth at varying resolution levels between 0.5 and 8 meters per pixel,
making it comparable to NWPU-RESISC45 in terms of data modality and size.
The main difference here is the fact that the images in AID are 600$\times$600 pixels in size,
allowing for a larger spatial context window for the scenes.

Also for this benchmark, our method outperforms the competing methods.
The ImageNet weights are very far behind on this evaluation,
which is surprising given their very good performance on the previous NWPU-RESISC45 dataset.
We speculate that the larger image size in this dataset
favors those methods actually pre-trained on remote sensing imagery,
whereas ImageNet consists of ground-level imagery.

\begin{table}
  \center
  \caption{
    Results on AID~\cite{xia_aid_2017}, values displayed in \%.
  }\label{table:aid}
  \begin{tabular}{llrrr}
    \toprule
    &&\multicolumn{3}{c}{Accuracy after}\\
    Weights&Backbone&1 epoch&2 epochs&5 epochs\\
    \midrule
    Random                        & ResNet-18 & 16.32 & 34.04 & 47.24 \\
    ImageNet                      & ResNet-18 & 38.66 & 53.12 & 70.72 \\
    Tile2Vec~\cite{tile2vec}      & ResNet-18 & 40.60 & 52.22 & 65.46 \\
    Contrastive~\cite{simclr}     & ResNet-18 & 54.52 & 64.94 & 80.56 \\
    SimCLR~\cite{simclr}          & ResNet-18 & 66.70 & 75.94 & 81.24 \\
    MoCo~\cite{he_momentum_2020}  & ResNet-18 & 57.64 & 65.70 & 81.02 \\
    Ours                          & ResNet-18 & \textbf{67.62} & \textbf{76.52} & \textbf{81.78} \\
    \midrule
    Random                        & ResNet-50 & 21.28 & 26.82 & 41.80 \\
    ImageNet                      & ResNet-50 & 32.52 & 40.64 & 57.22 \\
    Contrastive~\cite{simclr}     & ResNet-50 & 57.00 & 67.76 & 76.30 \\
    SimCLR~\cite{simclr}          & ResNet-50 & 64.41 & 72.94 & 79.62 \\
    MoCo~\cite{he_momentum_2020}  & ResNet-50 & 55.32 & 62.28 & 82.42 \\
    Ours                          & ResNet-50 & \textbf{71.90} & \textbf{77.62} & \textbf{84.44} \\
    \bottomrule
  \end{tabular}
\end{table}
 
\subsection{Aerial Image Segmentation}
The need for pre-trained networks is especially
strong in fields like image segmentation.
The recent state-of-the-art approaches in this field make ample
use of the deep features provided by backbone networks
in order to outperform less sophisticated methods that initialize
their weights at random.
To demonstrate that the learned weights in our models do not only capture
information from the entire scene,
but local information needed for accurate segmentation,
we evaluate a semantic segmentation benchmark as well.
The DeepGlobe Land Cover Classification Challenge~\cite{deepglobe}
aims to provide a benchmark for this task.
It consists of 1146 satellite images that are 2448$\times$2448
pixels in size at a pixel resolution of 0.5 meters per pixel,
covering an area of around \SI{1700}{\square\km}.
Again, we conduct a fine-tuning benchmark on this dataset
where the pre-trained models are used as backbones for
a DeepLabv3+ model~\cite{ferrari_encoder-decoder_2018}
for 5 epochs.

For this segmentation task, the modern self-supervised methods
are all very close to each other
in terms of performance (see Table~\ref{table:deepglobe}).
Even though the self-supervised pre-training tasks are all
on a scene-level,
the learned weights generalize well to a segmentation task,
outperforming the random and ImageNet baselines.

Surprisingly, the Contrastive pre-training method without an additional
projection shows a strong performance,
and even wins over all other methods in the mIoU evaluation for ResNet-50.
Upon closer inspection, this is in line with the findings of~\cite{simclr},
where the authors show that the additional projection head tends to
discard spatial information like rotation of an image.
This could explain why the Contrastive method performs so well
on the segmentation task, 
where fine-grained spatial analysis is needed.

As can be seen from Figure~\ref{fig:deepglobe},
the visual quality of the prediction results varies a lot
between the different evaluations.
Small structures like scattered houses
are not captured well by the methods that have never seen
aerial imagery before (Random, ImageNet).
The self-supervised methods trained on aerial imagery
on the other hand have no issues picking up these structures.

\begin{table}
  \center
  \caption{Segmentation results on DeepGlobe Land Cover Classification~\cite{deepglobe}.
  }\label{table:deepglobe}
  \begin{tabular}{lrrrr}
    \toprule
    &\multicolumn{2}{c}{ResNet-18}&\multicolumn{2}{c}{ResNet-50}\\
    Weights&OA&mIoU&OA&mIoU\\
    \midrule
    Random                        & 81.09 & 55.38 & 80.81 & 54.42 \\
    ImageNet                      & 83.27 & 61.95 & 82.27 & 59.31 \\
    Tile2Vec~\cite{tile2vec}      & 80.50 & 56.93 & --- & ---\\
    Contrastive~\cite{simclr}     & 85.25 & 64.85 & 86.06 & \textbf{68.46} \\
    SimCLR~\cite{simclr}          & 85.65 & 66.15 & 83.80 & 63.97 \\
    MoCo~\cite{he_momentum_2020}  & 84.79 & 65.28 & 85.07 & 66.17 \\
    Ours                          & \textbf{86.11} & \textbf{67.07} & \textbf{86.58} & 67.87 \\
    \bottomrule
  \end{tabular}
  \vspace{6pt}
\end{table}
 \begin{figure*}
    \newlength{\imgwidth}
    \setlength{\imgwidth}{1.8cm}
    \setlength{\tabcolsep}{2pt}
    \center
    \newcommand{\sample}[1]{\includegraphics[width=\imgwidth]{figures/deepglobe-images/#1-_RGB.jpg}&
  \includegraphics[width=\imgwidth]{figures/deepglobe-images/#1-_GT.png}&
  \includegraphics[width=\imgwidth]{figures/deepglobe-images/#1-Random_RN50.png}&
  \includegraphics[width=\imgwidth]{figures/deepglobe-images/#1-ImageNet_RN50.png}&
  \includegraphics[width=\imgwidth]{figures/deepglobe-images/#1-Tile2Vec_RN18.png}&
  \includegraphics[width=\imgwidth]{figures/deepglobe-images/#1-ImgCL_RN50.png}&
  \includegraphics[width=\imgwidth]{figures/deepglobe-images/#1-SimCLR_RN50.png}&
  \includegraphics[width=\imgwidth]{figures/deepglobe-images/#1-MoCo_RN50.png}&
  \includegraphics[width=\imgwidth]{figures/deepglobe-images/#1-TL_RN50.png}&
}

\begin{tabular}{cccccccccc}
  \small{}RGB&\small{}Ground Truth&\small{}Random&\small{}ImageNet&\small{}Tile2Vec~\cite{tile2vec}&\small{}Contrast.~\cite{simclr}&\small{}SimCLR~\cite{simclr}&\small{}MoCo~\cite{he_momentum_2020}&\small{}Ours\\
  \sample{9}\\
  \sample{17}\\
  \sample{115}\\
  \sample{161}\\
  \sample{189}\\
  \sample{254}\\
  \sample{371}\\
  \sample{325}\\
\end{tabular}

\definecolor{00ffff}{HTML}{00ffff}
\definecolor{ffff00}{HTML}{ffff00}
\definecolor{ff00ff}{HTML}{ff00ff}
\definecolor{00ff00}{HTML}{00ff00}
\definecolor{0000ff}{HTML}{0000ff}
\definecolor{ffffff}{HTML}{ffffff}
\fcolorbox{black}{00ffff}{\rule{0pt}{6pt}\rule{6pt}{0pt}}\quad Urban \qquad
\fcolorbox{black}{ffff00}{\rule{0pt}{6pt}\rule{6pt}{0pt}}\quad Agriculture \qquad
\fcolorbox{black}{ff00ff}{\rule{0pt}{6pt}\rule{6pt}{0pt}}\quad Rangeland \qquad
\fcolorbox{black}{00ff00}{\rule{0pt}{6pt}\rule{6pt}{0pt}}\quad Forest \qquad
\fcolorbox{black}{0000ff}{\rule{0pt}{6pt}\rule{6pt}{0pt}}\quad Water \qquad
\fcolorbox{black}{ffffff}{\rule{0pt}{6pt}\rule{6pt}{0pt}}\quad Barren Land
     \caption{Predictions of the different models on randomly selected
        validation tiles from the DeepGlobe
        Land Cover Classification dataset~\cite{deepglobe}.
        For all models, the ResNet-50 version was used,
        with the exception for Tile2Vec, for which only the ResNet-18 weights are available.
        Best viewed in color.
    }\label{fig:deepglobe}
\end{figure*}
\begin{table}
  \center
  \caption{
    Results on the ADVANCE dataset~\cite{hu_cross-task_2020},
    values displayed in \%.
  }\label{table:advance}
  \begin{tabular}{lccccc}
    \toprule
    Model&Imagery&Audio&Precision&Recall&F$_1$\\
    \midrule
    Audio Baseline~\cite{hu_cross-task_2020}&\XSolidBrush&\CheckmarkBold&
    30.46&32.99&28.99\\
    Visual Baseline~\cite{hu_cross-task_2020}&\CheckmarkBold&\XSolidBrush&
    74.05&72.79&72.85\\
    AV Baseline~\cite{hu_cross-task_2020}&\CheckmarkBold&\CheckmarkBold&
    75.25&74.79&74.58\\
    \midrule
    Ours (ResNet-18)&\XSolidBrush&\CheckmarkBold&
    37.91&38.36&37.69\\
    Ours (ResNet-18)&\CheckmarkBold&\XSolidBrush&
    \textbf{87.09}&\textbf{87.07}&\textbf{86.92}\\
    Ours (ResNet-18)&\CheckmarkBold&\CheckmarkBold&
    \textbf{89.59}&\textbf{89.52}&\textbf{89.50}\\
    \midrule
    Ours (ResNet-50)&\XSolidBrush&\CheckmarkBold&
    \textbf{39.13}&\textbf{39.96}&\textbf{39.01}\\
    Ours (ResNet-50)&\CheckmarkBold&\XSolidBrush&
    83.97&83.88&83.84\\
    Ours (ResNet-50)&\CheckmarkBold&\CheckmarkBold&
    88.90&88.85&88.83\\
    \bottomrule
  \end{tabular}
  \vspace{6pt}
\end{table}
 \iffalse
\begin{table}
  \center
  \caption{
    Results of the Ablation Study, values displayed in \%.
  }\label{table:ablation}
  \begin{tabular}{llrrrrrr}
    \toprule
    &&\multicolumn{2}{c}{Naïve TL}&\multicolumn{2}{c}{Contrastive Loss}&\multicolumn{2}{c}{Batch TL}\\
    \cmidrule(lr){4-5}
    \cmidrule(lr){6-7}
    \cmidrule(lr){8-9}
    Benchmark&Metric&RN-18&RN-50&RN-18&RN-50&RN-18&RN-50\\
    \midrule
    UC Merced~\cite{ucmerced}&Accuracy&85.14&77.43&86.48&88.19&90.19&89.71\\
    RESISC45~\cite{resisc45}&Accuracy&76.11&72.15&80.65&82.41&81.71&84.88\\
    AID~\cite{xia_aid_2017}&Accuracy&78.70&75.64&77.18&81.08&81.78&84.44\\
    \midrule
    \multirow{2}{*}{DeepGlobe~\cite{deepglobe}}&Accuracy&83.96&85.40&80.72&85.96&86.11&86.58
    \\
    &mIoU&63.14&65.18&57.26&67.28&67.07&67.87
    \\
    \midrule
    ADVANCE~\cite{hu_cross-task_2020}&F-Score&88.51&87.61&79.42&80.84&89.46&88.83\\
    \bottomrule
  \end{tabular}
\end{table}
\fi

\begin{table*}
  \center
  \caption{
    Results of the Ablation Study, values displayed in \%.
  }\label{table:ablation}
  \begin{tabular}{lllrrrrrr}
    \toprule
    &&&\multicolumn{2}{c}{Naive TL}&\multicolumn{2}{c}{Contrastive Loss}&\multicolumn{2}{c}{Batch TL}\\
    \cmidrule(lr){4-5}
    \cmidrule(lr){6-7}
    \cmidrule(lr){8-9}
    Task&Benchmark&Metric&RN-18&RN-50&RN-18&RN-50&RN-18&RN-50\\
    \midrule
    \multirow{3}{*}{Scene Classification}&UC Merced Land Use~\cite{ucmerced}&Accuracy&85.14&77.43&86.48&88.19&\textbf{90.19}&\textbf{89.71}\\
    &NWPU-RESISC45~\cite{resisc45}&Accuracy&76.11&72.15&80.65&82.41&\textbf{81.71}&\textbf{84.88}\\
    &AID~\cite{xia_aid_2017}&Accuracy&78.70&75.64&77.18&81.08&\textbf{81.78}&\textbf{84.44}\\
    \midrule
    \multirow{2}{*}{Semantic Segmentation}&\multirow{2}{*}{DeepGlobe Land Cover~\cite{deepglobe}}&Accuracy&83.96&85.40&80.72&85.96&\textbf{86.11}&\textbf{86.58}
    \\
    &&mIoU&63.14&65.18&57.26&67.28&\textbf{67.07}&\textbf{67.87}
    \\
    \midrule
    Audiovisual Scene Classification&ADVANCE~\cite{hu_cross-task_2020}&F-Score&88.51&87.61&79.42&80.84&\textbf{89.46}&\textbf{88.83}\\
    \midrule
    \multirow{2}{*}{Cross-Modal Retrieval}&\multirow{2}{*}{SoundingEarth (ours)}&Recall @ 100&18.59&13.41&\textbf{29.12}&\textbf{28.35}&19.01&15.28\\
    &&Median Rank&749&951&\textbf{565}&\textbf{580}&744&836\\
    \bottomrule
  \end{tabular}
\end{table*} 
\subsection{Audiovisual Scene Classification}
One application that has not received too much attention from the research community
is audiovisual scene classification,
where locally sourced audio data is combined with overhead imagery.
Given that our framework exploits these very two modalities as well,
we also include this task as a possible downstream task in our experiments.
The \emph{ADVANCE Dataset}~\cite{hu_cross-task_2020} poses a benchmark
for audiovisual scene classification,
and the accompanying research is a large source of inspiration for our work.
On this dataset, our model outperforms the baseline set in~\cite{hu_cross-task_2020} by a large margin,
as can be seen in table~\ref{table:advance}.
These results suggest that for this task,
self-supervised training on a large dataset
beats direct, supervised training on a smaller dataset.

\subsection{Cross-Modal Retrieval}
As a final application of our pre-trained models,
we evaluate the task of cross-modal retrieval.
Given an input image,
we try to predict the corresponding audio sample by
retrieving the closest audio samples in the shared embedding space.
Good performance in this task should imply
high semantic similarity for neighboring points in this space.

It turns out that this task is really hard to perform on the given dataset.
To understand this difficulty, imagine seeing an overhead image of city streets,
which needs to be matched to
exactly one out of hundreds of audio clips containing car and traffic sounds.
This explains why in quantitative evaluations, the scores for our models look rather low.
For the ResNet-18 model,
\SI{19.01}{\percent} of all testing samples had the correct audio sample among the top 100 retrievals,
while the median rank of the correct audio clip was at 744.
The model based on ResNet-50 scores a bit lower on these metrics,
reaching \SI{15.28}{\percent} and 836, respectively.

To put the retrieval results into perspective,
we asked participants to assess the model performance in a kind of ``Turing Test''.
In this human evaluation,
we mixed up two kinds of sound-image pairs (35 pairs for each, 70 in total).
The first kind is an image paired with the original sound
while the other one is an image paired with the top-1 audio retrieved.
These predicted pairs do not share the same overhead image.
Given 70 pairs each, 15 participants were then asked to answer
``Was the sound clip recorded somewhere within the image?''.
Then, we calculated the percentage of ``Yes'' answers for each kind of pair.
For true pairings, the participants correctly answered ``Yes''
for \SI{71.6}{\percent} (\SI{\pm12.1}{\percent}) of the samples.
Surprisingly, the participants considered nearly the same proportion
(\SI{69.5}{\percent} \SI{\pm13.1}{\percent})
of the pairings suggested by our model to be true pairings.

First, the classification rate of original sound-image pairs is
significantly higher than the chance level of 0.5,
which confirms our assumption that co-located sounds and overhead images share similar semantics.
Second, and to our surprise,
the results on predicted pairs are just slightly worse than the original ones,
which validates the quality of retrieved sounds. 

\subsection{Ablation Study}\label{sect:ablation}
Finally, we conduct an ablation study to provide evidence
that our Batch Triplet Loss function actually improves
the quality of the learned representations over the other loss functions.
Therefore, we compare the performance of models trained with our Batch Triplet Loss
to models trained with plain Triplet Loss
and the Contrastive Loss used in recent methods like SimCLR~\cite{simclr}.
 
Table~\ref{table:ablation} shows the results for the ablation study.
First, and most importantly, we notice that Batch Triplet Loss outperforms the
underlying naive Triplet Loss in all benchmarks.
What is more, it also outperforms the strong competitor given by the Contrastive Loss
in all tasks with the exception of the retrieval task,
where Contrastive Loss outperforms the Triplet Loss-based models by a large margin.

We speculate that this might be due to the different
embedding manifolds induced by the loss functions.
While the Triplet Losses embed the samples into the full vector space $\mathbb{R}^d$,
Contrastive Loss restricts the embeddings to the hypersphere $S^{d-1}\!\subset\!\mathbb{R}^d$.
The results suggest that for the evaluated downstream tasks,
the full space might be the better embedding manifold.
However, for multi-modal retrieval,
the more compact representation on the hypersphere
appears to be better suited.

\section{Conclusion}
With this work, we showed how the recent ideas in
self-supervised learning can contribute to the improvement of
deep learning models in remote sensing.
By exploiting the strong connections between audio and imagery,
our models can learn semantic representations of both modalities,
without the need for a laborious manual annotation process.
The resulting models outperform competing methods
on a number of benchmark datasets,
covering the tasks of
aerial image classification, audiovisual scene classification,
aerial image segmentation and cross-modal retrieval.

We hope that by making our code and pre-trained weights available,
further research on aerial imagery can profit directly from this
pre-training method.

Further, the multimodal dataset that we built should open up interesting
possibilities for further research in this direction,
including more sophisticated multimodal representation learning methods.

\section*{Acknowledgment}
This research would not have been possible without the
countless contributors to the Radio Aporee :::~Maps project.

Further, we acknowledge Google for providing imagery from Google Earth for research purposes.





\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,references}

\end{document}
