









\documentclass[a4paper,conference]{IEEEtran}











\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{comment}
\usepackage{color}
\usepackage{hyperref}
\usepackage{tikz}
\usepackage{pgfplots}
\usetikzlibrary{arrows}










\ifCLASSINFOpdf
\else
\fi






\usepackage{amsmath, amssymb}











\usepackage{array, multirow}





























\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
\title{Graph-based Interpolation of Feature Vectors for Accurate Few-Shot Classification}


\author{\IEEEauthorblockN{Yuqing Hu}
\IEEEauthorblockA{Electronics Dept., IMT Atlantique\\
Orange Labs\\
France\\
Email: yuqing.hu@imt-atlantique.fr}
\and
\IEEEauthorblockN{Vincent Gripon}
\IEEEauthorblockA{Electronics Dept., IMT Atlantique\\
Brest, France\\
Email: vincent.gripon@imt-atlantique.fr}
\and
\IEEEauthorblockN{St\'ephane Pateux}
\IEEEauthorblockA{Orange Labs\\
Cesson-S\'evign\'e, France\\
Email: stephane.pateux@orange.com\\
}}













\maketitle

\begin{abstract}
In few-shot classification, the aim is to learn models able to discriminate classes using only a small number of labeled examples. In this context, works have proposed to introduce Graph Neural Networks (GNNs) aiming at exploiting the information contained in other samples treated concurrently, what is commonly referred to as the transductive setting in the literature. These GNNs are trained all together with a backbone feature extractor. In this paper, we propose a new method that relies on graphs only to interpolate feature vectors instead, resulting in a transductive learning setting with no additional parameters to train. Our proposed method thus exploits two levels of information: a) transfer features obtained on generic datasets, b) transductive information obtained from other samples to be classified. Using standard few-shot vision classification datasets, we demonstrate its ability to bring significant gains compared to other works.
\end{abstract}






\IEEEpeerreviewmaketitle




\section{Introduction}
\label{section:introduction}

Deep learning is the state-of-the-art solution for many problems in machine learning, specifically in the domain of computer vision. Relying on a huge number of tunable parameters, these systems are able to absorb subtle dependencies in the distribution of data in such a way that it can later generalize to unseen inputs. Numerous experiments in the field of vision suggest that there is a trade-off between the size of the model (for example expressed as the number of parameters~\cite{tan2019efficientnet}) and its performance on the considered task. As such, reaching state-of-the-art performance often requires to deploy complex architectures. On the other hand, using large models in the case of data-thrifty settings would lead to a case of an underdetermined system. This is why few-shot learning is particularly challenging in the field. 

In order to overcome this limitation of deep learning models, several works propose to use Graph Neural Networks (GNNs)~\cite{garcia2017few,kim2019edge,gidaris2019generating,liu2018learning}. GNNs are a natural way to exploit information available in other samples to classify, a setting often referred to as transductive in the literature. However, most often introduced GNNs come with their own set of parameters to be added to the already numerous parameters to tune to solve the considered task.  As a consequence, many of these methods do not achieve top-tier results when compared to state-of-the-art solutions.

\begin{figure*}
    \centering
    \begin{tikzpicture}[thick]
    \draw[fill=black,fill opacity=0.1,draw=black,draw opacity=0.1]
    (0,0) rectangle (4,4)
    (4.2,0) rectangle (8.2,4)
    (8.4,0) rectangle (12.4,4)
(12.6,0) rectangle (17.,1.9)
    (12.6,2) rectangle (17.,4);
    \draw[]
    (-0.05,-0.05) rectangle (4.05,4.5)
    (4.15,-0.05) rectangle (12.45,4.5)
    (12.55,-0.05) rectangle (17.05,4.5)
;
    \node at (2,4.25) {\textbf{Pretraining}};
    \node at (8.4,4.25) {\textbf{Graph-based feature interpolation}};
    \node at (14.8,4.25) {\textbf{Logistic regression}};
    
    \begin{scope}[yshift=-0.2cm]
    \node at (2,3.8) {using lots of training data};
    \setlength{\fboxsep}{1pt}\node at (1,3) {\fcolorbox{black}{white}{\includegraphics[width=1cm,height=1cm]{images/dog1.jpg}}};
    \node at (0.95,2.95) {\fcolorbox{black}{white}{\includegraphics[width=1cm,height=1cm]{images/dog2.jpg}}};
    \node at (0.90,2.90) {\fcolorbox{black}{white}{\includegraphics[width=1cm,height=1cm]{images/dog3.jpg}}};
    \node at (0.85,2.85) {\fcolorbox{black}{white}{\includegraphics[width=1cm,height=1cm]{images/dog4.jpg}}};
    \node at (0.80,2.80) {\fcolorbox{black}{white}{\includegraphics[width=1cm,height=1cm]{images/dog5.jpg}}};
    \node at (3,3) {\fcolorbox{black}{white}{\includegraphics[width=1cm,height=1cm]{images/dinosaur1.jpg}}};
    \node at (2.95,2.95) {\fcolorbox{black}{white}{\includegraphics[width=1cm,height=1cm]{images/dinosaur2.jpg}}};
    \node at (2.90,2.90) {\fcolorbox{black}{white}{\includegraphics[width=1cm,height=1cm]{images/dinosaur3.jpg}}};
    \node at (2.85,2.85) {\fcolorbox{black}{white}{\includegraphics[width=1cm,height=1cm]{images/dinosaur4.jpg}}};
    \node at (2.80,2.80) {\fcolorbox{black}{white}{\includegraphics[width=1cm,height=1cm]{images/dinosaur5.jpg}}};
    \node at (2,2.7) {\fcolorbox{black}{white}{\includegraphics[width=1cm,height=1cm]{images/bird1.jpg}}};
    \node at (1.95,2.65) {\fcolorbox{black}{white}{\includegraphics[width=1cm,height=1cm]{images/bird2.jpg}}};
    \node at (1.90,2.60) {\fcolorbox{black}{white}{\includegraphics[width=1cm,height=1cm]{images/bird3.jpg}}};
    \node at (1.85,2.55) {\fcolorbox{black}{white}{\includegraphics[width=1cm,height=1cm]{images/bird4.jpg}}};
    \node at (1.80,2.50) {\fcolorbox{black}{white}{\includegraphics[width=1cm,height=1cm]{images/bird5.jpg}}};
    \end{scope}
    \node[blue] at (2,1.2) {1.train feature extractor};
    \node at (2,0.7) {};
    \node at (0.8,0.2) {\tiny{novel input}};
    \node at (3.15,0.2) {\tiny{feature vector}};
    \path[->,>=stealth]
    (3.1,0.3) edge (3.1,0.6)
    (0.85,0.3) edge (0.85,0.6);
    
    \begin{scope}[yshift=-0.1cm]
    \tikzstyle{every node} = [draw,circle,fill=white, scale=0.71];
    \node(v1) at (4.8, 3.3) {};
    \node(v2) at (7.5, 2.8) {};
    \node(v3) at (6.2,3.2) {};
    \node(v4) at (6.2,1.7) {};
    \node(v5) at (4.8,1.4) {};
    \node(v6) at (7.5,1.4) {};
    \tikzstyle{every node}=[scale=0.71];
    \path
    (v1) edge (v3)
    (v2) edge (v3)
    edge (v6)
    (v3) edge (v4)
    (v4) edge (v5)
    edge (v6);
    \path[->,>=stealth']
    (6.7,3.6) edge (6.85,3.0);
    \node at (6.8,3.8) {based on };
    \end{scope}
    \node[blue] at (6.2, 0.5) {2.construct similarity graph};

    
    \begin{scope}[xshift=4.2cm,yshift=0.3cm]
    \tikzstyle{every node} = [draw,circle,fill=white,minimum width=18pt,scale=0.71];
    \node(v1) at (4.8, 3.3) {};
    \node(v2) at (7.5, 2.8) {};
    \node(v3) at (6.2,3.2) {};
    \node(v4) at (6.2,1.7) {};
    \node(v5) at (4.8,1.4) {};
    \node(v6) at (7.5,1.4) {};
    \tikzstyle{every node}=[scale=0.71];
    \path
    (v1) edge (v3)
    (v2) edge (v3)
    edge (v6)
    (v3) edge (v4)
    (v4) edge (v5)
    edge (v6);
    \path[>=stealth',<->]
    (v1) edge (v3)
    (v2) edge (v3)
    edge (v6)
    (v3) edge (v4)
    (v4) edge (v5)
    edge (v6);
    \end{scope}
    \node at (10.4,1.) {};
    \node[blue] at (10.4,0.4) {3.feature propagation};
    
    


    
    \node[blue] at (14.8, 2.2) {\small{4.training with labeled inputs}};
    \tikzstyle{every node} = [draw,circle,minimum width=7pt,inner sep =0pt,scale=0.71];
    \begin{scope}[xshift=0.8cm,yshift=-0.1cm]
    \foreach \i in {0,...,6}{
        \node(a\i) at (13.+0.3*\i,3.5) {};
    }
    \foreach \i in {0,...,3}{
        \node(b\i) at (13.45+0.3*\i,2.7) {};
    }
    \path[opacity=0.8]
    \foreach \i in {0,...,6}{
        \foreach \j in {0,...,3}{
            (a\i) edge (b\j)
        }
    };
\tikzstyle{every node}=[scale=0.65];
    \node at (14.,3.9) {\footnotesize{train mapping  to labels}};
    \end{scope}
    
    \tikzstyle{every node} = [draw,circle,minimum width=7pt,inner sep =0pt,scale=0.71];
    \begin{scope}[xshift=0.8cm,yshift=-2.2cm]
    \foreach \i in {0,...,6}{
        \node(a\i) at (13.+0.3*\i,3.5) {};
    }
    \foreach \i in {0,...,3}{
        \node(b\i) at (13.45+0.3*\i,2.7) {};
    }
    \path[opacity=0.8]
    \foreach \i in {0,...,6}{
        \foreach \j in {0,...,3}{
            (a\i) edge (b\j)
        }
    };
\tikzstyle{every node}=[scale=0.62];
    \node at (14,3.9) {\footnotesize{predict  associated labels}};
    \end{scope}
    \tikzstyle{every node}=[scale=0.71];
    \node[blue] at (14.8, 0.15) {\small{5.prediction on other inputs}};
    
    \end{tikzpicture}
    \caption{Illustration of the proposed method. The proposed method is composed of three stages. During the pretraining stage, a classical backbone is trained using large datasets (step 1.). This trained backbone is then used to extract features of a novel dataset, comprising few supervised inputs. During feature interpolation, first is built a similarity graph depending on the cosine similarity between extracted features of both labeled and unlabeled available data (step 2.). Then this graph is used to diffuse (i.e. interpolate) features of similar (neighbor) samples (step 3.). The obtained representations are used to train a simple logistic classifier (step 4.) using the supervised data. Finally, in step 5., the trained classifier is used to perform predictions on unlabeled data.}
    \label{fig:illustration}
\end{figure*}

In this work, we propose to incorporate a graph-based method with no additional parameters, as a way to naturally bring transductive information in solving the considered task. The first step of the method consists in training a feature extractor with abundant data, followed by an interpolation strategy using well designed graphs. The graphs considered in this paper use vertices to represent each sample of the batch, and their edges are weighted depending on the similarity of corresponding feature vectors. The graph is thus used to interpolate features and thus share information between inputs. Once the features have been interpolated, we simply use a classical Logistic Regression (LR) to classify them. This work comes with the following claims:
\begin{itemize}
    \item We introduce a three-stage method for few-shot classification of input images that combines state-of-the-art transfer learning~\cite{mangla2020charting}, a graph-based interpolation technique and logistic regression.
    \item We empirically demonstrate that the proposed method reaches competitive accuracy on standardized benchmarks in the field of few-shot learning and largely surpasses the current works using GNNs.
    \item We analyze the importance of each step of the method and discuss hyperparameters influence.
\end{itemize}

The paper is organized as follows. In Section~\ref{sec:rw}, we present related works. In Section~\ref{sec:met} we introduce our proposed methodology. In Section~\ref{sec:exp}, we show experimental results on standard vision datasets and discuss hyperparameters influence. Finally, Section~\ref{sec:con} is a conclusion.
The source code can be found at~\url{https://github.com/yhu01/transfer-sgc}.














\section{Related Work}
\label{sec:rw}
\textbf{Optimization based methods:} Recent work on few-shot classification contains a variety of approaches, some of which can be categorized as meta-learning~\cite{thrun2012learning} where the goal is to train an optimizer that initializes the network parameters using a first generic dataset, so that the model is able to reach good performance with only a few more steps on actual considered data. The well-known MAML method~\cite{finn2017model} trains on different tasks with a basic stochastic gradient decent optimizer~\cite{chen2019closer} and Meta-LSTM~\cite{ravi2016optimization} utilizes a LSTM-based meta-learner that is thus memory-augmented. Meta-learning can be thought of as a refined transfer method, where the few-shot setting is taken into consideration directly when training on the generic dataset. Although both MAML and Meta-LSTM achieve good performance with quick adaptation, this type of solution suffers from the domain shift problem~\cite{chen2019closer} as well as the sensitivity of hyperparameters.

\textbf{Embedding based methods:} Another popular approach aims at finding compact embedding for the input data by learning a metric that measures the distance in a low-dimensional way. Matching Nets~\cite{vinyals2016matching} and Proto Nets~\cite{snell2017prototypical} learn a nearest-neighbor classifier by comparing the distance between the query inputs and labeled inputs with a certain metric, while Relation Nets~\cite{sung2018learning} construct a new neural network that learns the metric itself. If some of these methods are able to outperform MAML, they mainly suffer from over-fitting and a lack of task specific information. 

Therefore, ideas have been proposed to address these issues. For example in~\cite{li2019finding}, a plug network is added to find task-relevant features inside embeddings so that the model can tell the inter-class uniqueness and intra-class commonality for a specific task. In~\cite{lee2019meta} and~\cite{bertinetto2018meta}, the authors create a class-weight generator by training the model with a linear classifier (e.g. SVM) in order for the model to minimize generalization error across a distribution of tasks. More recently, the use of graph methods~\cite{gori2005new}~\cite{koch2015siamese} starts to gain momentum in the few-shot learning problems. For example, in~\cite{garcia2017few,kim2019edge,gidaris2019generating,liu2018learning}, the authors incorporate the idea of semi-supervised learning~\cite{chapelle2009semi} as a mean to benefit from the unlabeled query input data when solving a task, what is referred to as the transductive setting. Many recent works propose neural networks able to handle inputs supported on graphs~\cite{hamilton2017representation}. For example, in GCN~\cite{kipf2016semi}, the authors introduce a graph convolution operator, that can be used in cascade to generate deep learning architectures. In GAT~\cite{velivckovic2017graph}, the authors enrich GCN with additional learnable attention kernels. In SGC~\cite{wu2019simplifying}, the authors propose to simplify GCN by using only one-layer systems on powers of the adjacency matrix of considered graphs. Interestingly, they reach state-of-the-art accuracy with fewer parameters.

\textbf{Hallucination based methods:} Other methods propose to augment the training sets by learning a generator that can hallucinate novel class data using data-augmentation techniques~\cite{chen2019closer}. In~\cite{zhang2019few}, the authors extract labeled data into different components and then combine them using learned transformations, while in~\cite{chen2019image}, the authors aim at constructively deforming original samples with new samples drawn from another dataset. However, these methods lack precision as in the way the data is generated, which results in coarse and low-quality synthesized data that can sometimes lead to unsignificant gains in performance~\cite{wang2019few}. 

\textbf{Transfer based methods:} As in our work, transfer learning is another possible solution to solve few-shot classification problems. The main idea is to first train a feature extractor using a generic dataset~\cite{torrey2010transfer,das2019two}, then process these features directly when solving the new task. In~\cite{chen2019closer} a distance-based classifier is applied to train the backbone (i.e. the feature extractor), and in~\cite{mangla2020charting}, the authors aims at improving the feature quality by adding self-supervised learning and data-augmentation techniques during training. These methods have been proven to perform generally well, yet the challenge remains to fine-tune using the limited amount of labeled data.

In our work, we propose to align multiple ingredients that have been introduced in this section. Namely, we use transfer with graph-based interpolation. We mainly use transfer to exploit information contained in massive generic datasets, and we use a graph method to leverage the additional information available in both labeled and unlabeled inputs. Following the transductive setting, our proposed method can be considered as similar to~\cite{liu2018learning,garcia2017few,kim2019edge,gidaris2019generating}, but contrary to their works, we adopt a strategy in which the considered graph-based method contains no additional parameters to be trained. Our method can also be seen as a modification of Simplified Graph Convolutions~\cite{wu2019simplifying}, where contrary to their work we infer a graph structure from the latent representations of data.

\section{Methodology}
\label{sec:met}

\subsection{Problem statement}
Consider the following problem. We are given two datasets, termed  and  with disjoint classes. The first one (called ``base'') contains a large number of labeled examples from  different classes. The second one (called ``novel'') contains a small number of labeled examples, along with some unlabeled ones, all from  new classes. Our aim is to accurately predict the class of the unlabeled inputs of the novel dataset. There are a few important parameters to this problem: the number of classes in the novel dataset , the number of training samples  for each corresponding class, and the total number of unlabeled inputs .

Note that in previous works~\cite{liu2018learning}, authors consider that there are exactly  unlabeled inputs for each class. We consider that this is non-practical, since in most applications there is no reason to think that this holds. We shall see in Section~\ref{sec:exp} that this has strong implications in terms of performance, especially when  is small. Indeed, in practice the  unlabeled examples are drawn uniformly at random in a pool containing the same amount of unlabeled inputs for each class. So, when  is large, the central limit theorem tells us that the number of drawn inputs from each class should be similar, whereas it can be highly contrasted when  is small, leading to an imbalanced case.

\subsection{Proposed solution}

Our method is illustrated in Figure~\ref{fig:illustration}.
We first train a backbone deep neural network able to discriminate inputs from the base dataset , where  and . The proposed methodology builds upon using this pretrained architecture as a generic feature extractor, what is referred to as \emph{transfer} in the literature~\cite{torrey2010transfer}. Usually, a common way to extract features is to process data belonging to the novel dataset using the penultimate activation layer. Here, we obtain the extractor , where  are the learnable parameters trained using only the base dataset.

We then directly make use of the transferred representations . Based on these, we build a  nearest neighbor graph using cosine similarity: 
This graph contains as many vertices as the total number of inputs in the novel dataset (both labeled and unlabeled ones).
Then, we train a model of simplified graph convolution model, that is supervised only for labeled inputs.

The rationale behind this method is twofold:  the pretrained backbone should be able to find good discriminative features since it is trained on a sufficiently large labeled dataset  the graph-based interpolation technique should be able to benefit from both the supervised inputs and the unlabeled ones, resulting in significant gains in accuracy when compared to methods that would ignore the unlabeled data. 

We show in the experiments that this method is also able to outperform other methods that use the unlabeled data especially when the number of labeled inputs is very limited.

The details of the proposed method are provided in the following paragraphs, first the pre-training stage (i.e. training the generic backbone), followed by the feature interpolation and logistic regression stages.

\textbf{Pre-training:} We follow the methodology introduced in~\cite{mangla2020charting}. In more details the feature extractor  and a distance-based classifier (parametrized by )~\cite{mensink2012metric} are trained on , where we compute the cosine distance between an input feature  and each weight vector in  in order to reduce the intra-class variations~\cite{chen2019closer}. The training process consists of two sub-stages: the first sub-stage utilizes rotation-based self-supervised learning technique~\cite{gidaris2018unsupervised} where each input image is randomly rotated by a multiple of 90 degrees. We then co-train a linear classifier to tell which rotation was applied. Therefore, the total loss function of this sub-stage is given by:

The second sub-stage fine-tunes the model with Manifold Mixup~\cite{verma2018manifold} technique for a few more epochs, where the outputs of hidden layers in the neural network are linearly combined to help the trained model generalize better. The total loss in this sub-stage is given by:

With this training process, we are able to obtain robust input representations that generalize well to novel classes.

\textbf{Feature interpolation:} We consider fixed the pretrained parameters  of . Before training a new classifier  on the transferred representations of the novel dataset, we propose to interpolate features using a graph.

In details, we define a graph ~\cite{kipf2016semi} where vertices matrix  contains the stacked features of labeled and unlabeled inputs~\cite{garcia2017few}. To build the adjacency matrix , we first compute:

where  denotes the -th row of . Note that in all backbone architectures we use in the experiments, the penultimate layers are obtained by applying a ReLU function, so that all coefficients in  are nonnegative. As a result, coefficients in  are nonnegative as well. Also, note that  is symmetric.

Then, we only keep the value  if it is one of the  largest values on the corresponding row or on the corresponding column in . So, as soon as , all values are kept. Otherwise,  contains many 0s.

Finally, we apply normalization on the resulting matrix: 

where  is the degree diagonal matrix defined as: 
Therefore, the graph vertices represent all inputs (both labeled and unlabeled) of the novel dataset. Its nonzero weights are based on the cosine similarity between corresponding transferred representations.

We then apply feature propagation~\cite{wu2019simplifying} to obtain new features for each vertex. The formula is:

in which  and  are both hyperparameters, and  is the identity matrix. The role of  is important: providing  is too small, the new feature of a vertex will only depend on its direct neighbors in the graph. Using larger values of  allows to encompass for more indirect relationships. Using a too large value of  might drown out the information by averaging over all inputs. Similarly,  allows to balance between the neighbors representations and self-ones.

\textbf{Logistic regression:} Finally, a softmax classifier is trained using only the labeled vertices. We denote by  the subset of  corresponding to labeled vertices, then the predicted results  can be written following this formula:

where ,  and  denotes the probability of vertex  being categorized as being in the -th class. 

Prediction is performed using the same principle, but using unlabeled inputs instead: denote by  the subset of  corresponding to unlabeled inputs, then we have the decision:


In Table~\ref{tab:hyperparams} we summarize the main parameters and hyperparameters of the considered problem and proposed solution. Let us point out that the proposed graph-based method does not contain any parameter to train.

\begin{table}[h]
    \caption{Parameters and hyperparameters of the considered problem and proposed solution (\# stands for ``number'').}
    \centering
    \scalebox{1.2}{
    \begin{tabular}{|c|l|}
    \hline
    \multicolumn{2}{|c|}{Novel dataset parameters}\\
    \hline
         & \# classes\\
         \hline
         & \# supervised inputs per class\\
         \hline
          & total \# of unsupervised inputs\\
         \hline
         \hline

    \multicolumn{2}{|c|}{Proposed method hyperparameters}\\
\hline
          & \# nearest neighbors to keep\\
         \hline
          & power of the diffusion matrix\\
         \hline
          & strength of self-representations\\
         \hline
    \end{tabular}
    }
    \label{tab:hyperparams}
\end{table}

\section{Experimental Validation}
\label{sec:exp}
\subsection{Datasets}

We perform our experiments on 3 standardized few-shot classification datasets: miniImageNet~\cite{vinyals2016matching}, CUB~\cite{wah2011caltech} and CIFAR-FS~\cite{bertinetto2018meta}. These datasets are split into two parts: a)  classes are chosen to train the backbone, called base classes, b)  classes are drawn uniformly in the remaining classes to form the novel dataset, called novel classes. Among the  drawn novel classes,  labeled inputs per class and a total of  unlabeled inputs are drawn uniformly at random. As in most related works, unless mentioned otherwise all our experiments are performed using  and . We perform a run of 10,000 random draws to obtain an accuracy score and indicate confidence scores (95\%) when relevant.

\textbf{miniImageNet:} It consists of a subset of ImageNet~\cite{russakovsky2015imagenet} that contains 100 classes and 600 images of size  pixels per class. According to the standard~\cite{ravi2016optimization}, we use 64 base classes to train the backbone and 20 novel classes to draw the novel datasets from. So, for each run, 5 classes are drawn uniformly at random among these 20 classes.

\textbf{CUB:} The dataset contains 200 classes and has a total of 11,788 images of size  pixels. We split it into 100 base classes to train the backbone and 50 novel classes to draw the novel datasets from.

\textbf{CIFAR-FS:} This dataset has 100 classes, each class contains 600 images of size  pixels. We use the same numbers as for the miniImageNet dataset.

\subsection{Backbone models and implementation details}
We perform experiments using 2 different backbones as the structure of feature extractor .

\textbf{Wide residual networks (WRN)}~\cite{zagoruyko2016wide}\textbf{:} We follow the settings in~\cite{mangla2020charting} by choosing a WRN with 28 convolutional layers and a widening factor of 10. The output feature size  is 640.

\textbf{Residual networks (ResNet18)}~\cite{he2016deep}\textbf{:} Our ResNet18 contains a total of 18 convolutional layers grouped into 8 blocks. Following the settings in~\cite{wang2019simpleshot}, we remove the first two down-sampling layers and change the kernel size of the first convolutional layer to  pixels instead of  pixels. Here, .

For the pre-training stage and miniImageNet, we train all backbones for a total of  epochs from scratch using Adam optimizer~\cite{kingma2014adam} and cross-entropy loss, including  epochs on the first sub-stage and  epochs on the second sub-stage. For the logistic regression, we train with the same optimizer and loss function for  epochs with learning rate being  and weight decay being , which typically requires of the order of one second of computation on a modern GPU. Note that we observed that convergence usually occurs much quicker than 1000 epochs. In the In-Domain settings two stages are trained on the same dataset with base classes and novel classes respectively, while in the Cross-Domain settings we use these splits from two different datasets (e.g. base classes from miniImageNet and novel classes from CUB).


\subsection{Comparison with state-of-the-art methods}

As a first experiment, we compare the raw performance of the proposed method with state-of-the-art solutions with WRN and ResNet18 as backbones. The results are presented in Table~\ref{tab:results}. We fixed ,  and  respectfully with  and  for the proposed method, as it empirically gave the best results. Note that the sensitivity of these hyperparameters is discussed later in this section.

We point out that the proposed method reaches state-of-the-art performance in both case of 1-shot and 5-shot classification for most of the time, whatever the choice of all considered datasets. Note that the gain we observe is higher in the 1-shot case than in the 5-shot case, this is expected as in the case of 1-shot, the unlabeled samples bring proportionally more information compared to the case of 5-shot. In the extreme case of -shot, with  large enough, we expect the unlabeled samples to be almost useless.

We also perform experiments where the backbone has been trained using the base classes of miniImageNet but the few-shot task is performed using the novel classes of the CUB dataset. According to the results, we can draw conclusions very similar to the previous study, where the proposed method performs well for this specific task.

\begin{table*}
    \caption{1-shot and 5-shot accuracy of state-of-the-art methods in the literature, compared with the proposed solution. We present results using WRN and ResNet18 as backbones. For the proposed solution, we use the hyperparameters ,  and  for ; ,  and  for .}
    \centering
    \scalebox{1.0}{
    \begin{tabular}{l|l|l|l}
         \toprule
         &          & \multicolumn{2}{c}{\textbf{miniImageNet}} \\
         Method & Backbone & 1-shot & 5-shot \\
         \midrule
         MAML~\cite{finn2017model} & ResNet18 &  & \\
         Baseline++~\cite{chen2019closer} & ResNet18 &  & \\
         Matching Networks~\cite{vinyals2016matching} & ResNet18 &  & \\ 
         ProtoNet~\cite{snell2017prototypical} & ResNet18 &  & \\
         SimpleShot~\cite{wang2019simpleshot} & ResNet18 &  & \\
         S2M2\_R~\cite{mangla2020charting} & ResNet18 &  & \\
         LaplacianShot~\cite{ziko2020laplacian} & ResNet18 &  & \\ 
         Transfer+Graph Interpolation (ours) & ResNet18 &  & \\
         \midrule
         ProtoNet~\cite{snell2017prototypical} & WRN &  & \\
         Matching Networks~\cite{vinyals2016matching} & WRN &  & \\
         S2M2\_R~\cite{mangla2020charting} & WRN &  & \\
         SimpleShot~\cite{wang2019simpleshot} & WRN &  & \\
         SIB~\cite{hu2020empirical} & WRN &  & \\
         BD-CSPN~\cite{liu2019prototype} & WRN &  & \\
         LaplacianShot~\cite{ziko2020laplacian} & WRN &  & \\
         Transfer+Graph Interpolation (ours) & WRN &  & \\
         \bottomrule
         


         \toprule
         &          & \multicolumn{2}{c}{\textbf{CUB}} \\
         Method & Backbone & 1-shot & 5-shot \\       
         \midrule
S2M2\_R~\cite{mangla2020charting} & ResNet18 &  & \\
         ProtoNet~\cite{snell2017prototypical} & ResNet18 &  & \\
         Matching Networks~\cite{vinyals2016matching} & ResNet18 &  & \\ 
         LaplacianShot~\cite{ziko2020laplacian} & ResNet18 &  & \\
         Transfer+Graph Interpolation (ours) & ResNet18 &  & \\
         \midrule
         S2M2\_R~\cite{mangla2020charting} & WRN &  & \\
         Transfer+Graph Interpolation (ours) & WRN &  & \\
         \bottomrule
         
         \toprule
         &          & \multicolumn{2}{c}{\textbf{miniImageNet}\textbf{CUB}} \\
         Method & Backbone & 1-shot & 5-shot \\
         \midrule
         Baseline++~\cite{chen2019closer} & ResNet18 &  & \\
         SimpleShot~\cite{wang2019simpleshot} & ResNet18 &  & \\
         LaplacianShot~\cite{ziko2020laplacian} & ResNet18 &  & \\
         Transfer+Graph Interpolation (ours) & ResNet18 &  & \\
         \midrule
         Manifold Mixup~\cite{verma2018manifold} & WRN &  & \\
         S2M2\_R~\cite{mangla2020charting} & WRN &  & \\
         Transfer+Graph Interpolation (ours) & WRN &  & \\
         \bottomrule
         
         \toprule
         &          & \multicolumn{2}{c}{\textbf{CIFAR-FS}} \\
         Method & Backbone & 1-shot & 5-shot \\
         \midrule
BD-CSPN~\cite{liu2019prototype} & WRN &  & \\
         S2M2\_R~\cite{mangla2020charting} & WRN &  & \\
         SIB~\cite{hu2020empirical} & WRN &  & \\
         Transfer+Graph Interpolation (ours) & WRN &  & \\
         \bottomrule
         
    \end{tabular}
    }
    \label{tab:results}
\end{table*}

\subsection{Comparaison with other GNN methods}

In this experiment we compare our performance on miniImageNet with others that use Graph Neural Network to address the few-shot classification. As we can see in Table~\ref{tab:results_compareGNN}, with a three-stage training strategy, our proposed method has largely surpassed the current GNN based methods that train an entire model at once, given the transductive setting. 

\begin{table}
    \caption{1-shot and 5-shot performance (on miniImageNet) comparison with other GNN based methods. In our experiment we use the same hyperparameters as Table~\ref{tab:results}.}
    \centering
    \scalebox{0.9}{
    \begin{tabular}{l|l|l}
         \toprule
         Method & 1-shot & 5-shot \\
         \midrule
         GNN~\cite{garcia2017few} &  & \\
         TPN~\cite{liu2018learning} &  & \\
         wDAE-GNN~\cite{gidaris2019generating} &  & \\
         Transfer+Graph Interpolation (ours) &  & \\
         \bottomrule
         
    \end{tabular}
    }

    \label{tab:results_compareGNN}
\end{table}

\subsection{Importance of the parameter-free graph interpolation}

In our work, we considered using a parameter-free graph interpolation technique to diffuse features between inputs. As mentioned in the related work section, there are many alternatives, but they come with additional parameters. In the next experiment, we compare the accuracy of the method when using GCN~\cite{kipf2016semi} and GAT~\cite{velivckovic2017graph}, instead of a simple interpolation. Results are presented in Table~\ref{tab:results_compare}. We note that the best results are obtained using our designed graph interpolation, which we believe to be due to the fact we use fewer parameters in total. Graph interpolation also has the interest of being many times faster to train. In our experiments, each run took about  seconds to train using graph interpolation versus  seconds for GCN and  seconds with GAT, which happens to lead to the worst performance of our considered methods.

It is worth pointing out that a drawback of the proposed method is that it requires to train a logistic regression model each time a batch prediction is required. In other words, it can be limiting in settings where predictions to make are streamed. However, the time required to train the logistic regression model remains very small in our experiments (less than one second).

\begin{table}
    \caption{1-shot and 5-shot accuracy on miniImageNet, when using the WRN backbone and various Graph Neural Networks. We use the same hyperparameters as Table~\ref{tab:results} and apply them to all methods (with the exception of  for GCN and GAT).}
    \centering
    \scalebox{1.0}{
    \begin{tabular}{l|l|l}
         \toprule
         Method & 1-shot & 5-shot \\
         \midrule
         Transfer+GAT &  & \\
         Transfer+GCN &  & \\
         Transfer+Graph Interpolation &  & \\
         \bottomrule
         \multicolumn{3}{l}{\begin{minipage}{6.5cm}\tiny *GAT is evaluated with 600 test runs.\end{minipage}}
    \end{tabular}
    }

    \label{tab:results_compare}
\end{table}

\subsection{Influence of Parameters}

We then inquire the importance of various parameters of the task to the performance of the proposed method. We begin by varying the number of supervised inputs , and consider two settings: one where we dispose of an average of  unsupervised inputs for each class and one where we dispose of  of them. Results are depicted in Figure~\ref{fig:functionofs}. As we can see, the performance of the method is highly influenced by the number of supervised inputs, as expected. Interestingly, there is a significant gap in accuracy between  and  for 1-shot setting, even if this gap diminishes as the number of supervised inputs is increased.

\begin{figure}[h]
  \begin{center}
    \begin{tikzpicture}
       \begin{scope}[xscale=0.9, yshift=6cm]
        \begin{axis}[
            xlabel=,
            ylabel=Accuracy,
            height=4cm,
            width=.5\textwidth,
            legend pos={south east,legend cell align=left}
            ]
          




          \addlegendentry{}
          \addplot coordinates
{(1,66.70) (2,74.56) (3,78.19) (4,80.75) (5,82.18)};




          \addlegendentry{}
          \addplot coordinates
          {(1,79.62) (2,83.57) (3,85.20) (4,86.21) (5,86.95)};
        \end{axis}
      \end{scope}
    \end{tikzpicture}
  \end{center}
  \vspace{-.5cm}
  \caption{Evolution of the accuracy of few-shot classification with miniImageNet (backbone: WRN) as a function of the number of supervised inputs , and for various number of unsupervised queries . We use ,  and .}
  \label{fig:functionofs}
\end{figure}

In the next experiment, we draw in Figure~\ref{fig:functionofq} the evolution of the performance of the method as a function of the number of unsupervised inputs , for 1-shot, 3-shot and 5-shot settings. This curve confirms two observations: a) in the case of 5-shot setting, the influence of the number of unsupervised inputs is little, and the accuracy of the method quickly reaches its pick and b) in the case of 1-shot setting, the number of unsupervised inputs significantly influences accuracy up to a few dozens. It is interesting to point out that about the same accuracy is achieved for 5-shot using  and 1-shot using , suggesting that  unsupervised inputs bring about the same usable information as 4 labeled inputs per class.

\begin{figure}[h]
  \begin{center}
    \begin{tikzpicture}
       \begin{scope}[xscale=0.9, yshift=6cm]
        \begin{axis}[
            xlabel=,
            ylabel=Accuracy,
            height=4cm,
            width=.5\textwidth,
            legend pos=south east
            ]
            
          \addlegendentry{}
          \addplot coordinates
          {(1,63.36) (25,66.72) (50,74.32) (75,76.33) (100,77.41) (250,79.00) (500,79.75)};
          
          \addlegendentry{}
          \addplot coordinates
          {(1,70.68) (25,78.23) (50,81.55) (75,82.51) (100,83.16) (250,84.42) (500,85.38)};
          
          \addlegendentry{}
          \addplot coordinates
          {(1, 77.00) (25,82.21) (50,84.02) (75,84.59) (100,85.15) (250,86.10) (500,86.93)};
        \end{axis}
      \end{scope}
    \end{tikzpicture}
  \end{center}
  \vspace{-.5cm}
  \caption{Evolution of the accuracy of few-shot classification with miniImageNet (backbone: WRN) as a function of the number of query inputs , and for various number of unsupervised inputs . We use ,  and .}
  \label{fig:functionofq}
\end{figure}



In the next experiment we look at the influence of the parameters  and  which respectively control to which power the diffusion matrix is taken and the importance of self-representations. In Figure~\ref{fig:functionofkappa}, we draw the obtained mean accuracy as a function of ,  and . We use  and  in this experiment. There are multiple interesting conclusions to draw from this figure. 

\begin{enumerate}
    \item This curve justifies the previously mentioned choice of parameters, leading to the best performance.
    \item We observe that when  is large and  is small, it is better not to use powers of the diffusion matrix. This is the only setting where this statement holds, emphasizing the fact that if the graph is not sparse and self-importance is low, powers of the diffusion matrix are likely to over-smooth the representations of neighbors.
    \item When  is small (here:  or ), there is little sensitivity to both  and  (for ). This is an asset as it makes it simpler to find good hyperparameters.
    \item The best results are achieved for smaller values of , suggesting that cosine similarity between distant representations can be noisy and damaging to the performance of the method.
    \item Note that in this experiment . So using  would ideally select exactly 15 neighbors of the same class for each input. Interestingly, this choice of  does not lead to the best performance, showing the graph structure is not perfectly aligned with classes.
\end{enumerate}

\begin{figure}[h]
  \begin{center}
    \begin{tikzpicture}
       \begin{scope}[xscale=0.9, yshift=3cm]
        \begin{axis}[
ylabel={\begin{tabular}{c}\% accuracy\\\end{tabular}},
            height=4cm,
            width=.5\textwidth,
            ymin=60,
            ymax=77,
            xticklabels=[]
            ]
            
\addplot coordinates
          {(1, 75.16) (2, 76.12) (3, 74.83) (4, 73.46) (5, 71.75)};
          
          
\addplot coordinates
          {(1, 75.50) (2, 76.32) (3, 76.09) (4, 75.53) (5, 74.74)};
          
\addplot coordinates
          {(1, 75.05) (2, 76.03) (3, 76.32) (4, 76.08) (5, 75.71)};
          
\addplot coordinates
          {(1, 74.52) (2, 75.83) (3, 76.31) (4, 76.26) (5, 75.97)};
          
\addplot coordinates
          {(1, 73.96) (2, 75.75) (3, 76.24) (4, 76.27) (5, 76.13)};
        \end{axis}
      \end{scope}
       \begin{scope}[xscale=0.9, yshift=5.7cm]
        \begin{axis}[
legend style={at={(0.48,0.0)},anchor=south, legend cell align=left},
            ylabel={\begin{tabular}{c}\% accuracy\\\end{tabular}},
            height=4cm,
            width=.5\textwidth,
            ymin=60,
            ymax=77,
            xticklabels=[]]
            
          \addlegendentry{\footnotesize{}}
          \addplot coordinates
          {(1, 73.71) (2, 75.67) (3, 75.14) (4, 75.17) (5, 74.63)};
          
          
          \addlegendentry{\footnotesize{}}
          \addplot coordinates
          {(1, 74.56) (2, 75.43) (3, 75.51) (4, 75.48) (5, 75.34) };
          
          \addlegendentry{\footnotesize{}}
          \addplot coordinates
          {(1, 74.55) (2, 75.41) (3, 75.59) (4, 75.52) (5, 75.39)};
          
          \addlegendentry{\footnotesize{}}
          \addplot coordinates
          {(1, 74.27) (2, 75.36) (3, 75.59) (4, 75.57) (5, 75.43)};
          
          \addlegendentry{\footnotesize{}}
          \addplot coordinates
          {(1, 73.85) (2, 75.29) (3, 75.59) (4, 75.60) (5, 75.49)};
        \end{axis}
      \end{scope}
       \begin{scope}[xscale=0.9, yshift=0.3cm]
        \begin{axis}[
ylabel={\begin{tabular}{c}\% accuracy\\\end{tabular}},
            height=4cm,
            width=.5\textwidth,
            ymin=60,
            ymax=77,
            xticklabels=[],
            legend pos=south east
            ]
            
\addplot coordinates
          {(1, 74.13) (2, 74.14) (3, 71.44) (4, 68.78) (5, 66.30)};
          
          
\addplot coordinates
          {(1, 74.92) (2, 75.24) (3, 74.29) (4, 72.84) (5, 71.25)};
          
\addplot coordinates
          {(1, 74.40) (2, 75.37) (3, 75.20) (4, 74.54) (5, 73.65)};
          
\addplot coordinates
          {(1, 73.74) (2, 75.20) (3, 75.35) (4, 75.13) (5, 74.60)};
          
\addplot coordinates
          {(1, 73.14) (2, 74.90) (3, 75.32) (4, 75.27) (5, 75.01)};
        \end{axis}
      \end{scope}
       \begin{scope}[xscale=0.9, yshift=-2.4cm]
        \begin{axis}[
            xlabel=,
            ylabel={\begin{tabular}{c}\% accuracy\\\end{tabular}},
            height=4cm,
            ymin=60,
            ymax=77,
            width=.5\textwidth,
            legend pos=south east
            ]
            
\addplot coordinates
          {(1, 71.53) (2, 70.38) (3, 66.44) (4, 63.32) (5, 60.74)};
          
          
\addplot coordinates
          {(1, 73.40) (2, 72.92) (3, 70.98) (4, 68.66) (5, 66.53)};
          
\addplot coordinates
          {(1, 72.93) (2, 73.54) (3, 72.88) (4, 71.70) (5, 70.32)};
          
\addplot coordinates
          {(1, 72.34) (2, 73.50) (3, 73.45) (4, 72.89) (5, 71.99)};
          
\addplot coordinates
          {(1, 71.80) (2, 73.24) (3, 73.55) (4, 73.32) (5, 72.83)};
        \end{axis}
      \end{scope}
    \end{tikzpicture}
  \end{center}
  \vspace{-.5cm}
  \caption{Evolution of the accuracy of few-shot classification with miniImageNet (backbone: WRN) as a function of ,  and .}
  \label{fig:functionofkappa}
\end{figure}

It is often disregarded the impact of class imbalance in the context of few-shot learning. As a matter of fact, since we only consider very few labeled examples, it does not make much sense to consider such a scenario. But in the context of transductive setting, it is highly probable that unlabeled inputs are imbalanced between classes. So we perform the next experiment by varying the number of examples chosen in two random classes from miniImageNet. We always make sure that the total number of queries to classify remains the same, that is 100. But we select  of them in class 1 and  of them in class 2.

In Figure~\ref{fig:imbalanced}, we depict the evolution of the accuracy of the proposed method, as a function of . As one can clearly see from this figure, there is an important influence of class imbalance towards the performance of the proposed method. This is expected as the generated graphs will have imbalanced communities as a consequence. This could be problematic to some application domains where such imbalance is expected to happen in considered datasets, as there is no direct way of correcting it. Obviously, if one has insights about the relative distribution between classes, simple data augmentation or sampling could be used for mitigation.

\begin{figure}[h]
  \begin{center}
    \begin{tikzpicture}
       \begin{scope}[xscale=0.9, yshift=6cm]
        \begin{axis}[
            xlabel=,
            ylabel=Accuracy,
            height=4cm,
            width=.5\textwidth,
            legend pos=south east
            ]
            
          \addlegendentry{1-shot}
          \addplot coordinates
          {(1,51.02) (10,77.54) (20,88.69) (30,92.33) (40,93.58) (50,94.00)};
          
          \addlegendentry{5-shot}
          \addplot coordinates
          {(1,76.02) (10,88.72) (20,93.34) (30,94.88) (40,95.41) (50,95.50)};
        \end{axis}
      \end{scope}
    \end{tikzpicture}
  \end{center}
  \vspace{-.5cm}
  \caption{Accuracy of 2-ways classification with unevenly distributed query data for each class, where the total number of query inputs remains constant. When , we obtain the most imbalanced case, whereas  corresponds to a balanced case. We use ,  and .}
  \label{fig:imbalanced}
\end{figure}
However, this could be problematic to some application domains where such imbalance is expected to happen in considered datasets, as there is no direct way of correcting it. Obviously, if one has insights about the relative distribution between classes, simple data augmentation or sampling could be used for balancing this negative effect.

Finally, in Figure~\ref{graphvisu}, we draw a representation of a typical graph obtained with the miniImageNet dataset, using Laplacian embedding~\cite{horaud2009short,shuman2013emerging}. On this figure, we colored vertices depending on which class they belong to. Interestingly, this figure shows that some classes are easily separated in the graph, whereas others are much harder to discriminate. We believe that the main reason why these graphs are not perfectly segregating classes is because some dimensions obtained using the backbone are specialized on features completely irrelevant for the novel task.

\begin{figure}[h]
\begin{center}
\begin{tikzpicture}[xscale=20,yscale=8]
\tikzstyle{every node} = [circle, draw]
\node[fill=blue](0) at (0.045680106307013336,-0.15944062638357112) {};
\node[fill=blue](1) at (0.005795768814802205,-0.15185095713528257) {};
\node[fill=blue](2) at (0.03463846860889502,-0.2110892481243941) {};
\node[fill=blue](3) at (0.06579803803191195,-0.2051698618809592) {};
\node[fill=blue](4) at (0.03510376101988162,-0.1524208974077208) {};
\node[fill=green](5) at (-0.12223756962994375,-0.004875656341539053) {};
\node[fill=green](6) at (-0.09294142330772531,-0.0037330623280312364) {};
\node[fill=green](7) at (-0.08118991426078026,-0.03974422991990645) {};
\node[fill=green](8) at (-0.0912337660777498,-0.021272203605895115) {};
\node[fill=green](9) at (-0.12331059700548036,-0.008604068331404577) {};
\node[fill=cyan](10) at (0.09150783364461061,0.09451427103269636) {};
\node[fill=cyan](11) at (0.09721522869932414,0.10285940530770542) {};
\node[fill=cyan](12) at (0.08663011121818587,0.08378056606101787) {};
\node[fill=cyan](13) at (0.09297082015007248,0.09935849885229188) {};
\node[fill=cyan](14) at (0.10508624908952474,0.10586678090724996) {};
\node[fill=orange](15) at (-0.13205851027855717,0.03716700817903457) {};
\node[fill=orange](16) at (-0.11899280991198748,0.03613495199043308) {};
\node[fill=orange](17) at (-0.10851559445142425,0.037857019577786574) {};
\node[fill=orange](18) at (-0.10653092664743519,0.02076141025731176) {};
\node[fill=orange](19) at (-0.16053915923055537,0.04120600742452389) {};
\node[fill=magenta](20) at (0.09179943072273908,0.061911913917469086) {};
\node[fill=magenta](21) at (0.09771416381267159,0.0683329982718129) {};
\node[fill=magenta](22) at (0.08365061126396928,0.05613146767544901) {};
\node[fill=magenta](23) at (0.1185739609011659,0.061777875114200895) {};
\node[fill=magenta](24) at (0.10036771156647388,0.07379741517943594) {};
\node[fill=blue](25) at (0.033153027791940994,-0.2220536260316069) {};
\node[fill=blue](26) at (0.05067007948230345,-0.23578184464944213) {};
\node[fill=blue](27) at (0.04664711863391337,-0.24511707325278492) {};
\node[fill=blue](28) at (0.04044576373695165,-0.22123819250010235) {};
\node[fill=blue](29) at (0.041128402691482914,-0.2080563988723915) {};
\node[fill=blue](30) at (0.045091408004321556,-0.22521561870600532) {};
\node[fill=blue](31) at (0.04053143687310179,-0.14815479236631182) {};
\node[fill=blue](32) at (0.0508515614600885,-0.2509020057882185) {};
\node[fill=blue](33) at (0.05110040178177802,-0.2363169111843277) {};
\node[fill=blue](34) at (-0.03348445560830227,-0.08278578644557111) {};
\node[fill=blue](35) at (0.052890327746633936,-0.12786178045450736) {};
\node[fill=blue](36) at (0.05620659705713664,-0.2233136032098087) {};
\node[fill=blue](37) at (0.05320283605517201,-0.1848374687520992) {};
\node[fill=blue](38) at (0.03893892284687967,-0.1917405733000982) {};
\node[fill=blue](39) at (-0.04245496021167464,-0.08804175272721608) {};
\node[fill=green](40) at (-0.1064825977134248,-0.006826339044408989) {};
\node[fill=green](41) at (-0.13389232272383758,0.015130925319067892) {};
\node[fill=green](42) at (-0.11111941340836073,-0.011877867366608893) {};
\node[fill=green](43) at (-0.12154741827105724,0.0009072327355212306) {};
\node[fill=green](44) at (-0.16000862810804425,0.021283395392449615) {};
\node[fill=green](45) at (-0.11563117306504925,-0.0048550536600447955) {};
\node[fill=green](46) at (-0.10843569174027456,-0.003636968726271712) {};
\node[fill=green](47) at (-0.10651348022577209,-0.009104519220707605) {};
\node[fill=green](48) at (-0.10933153063529844,-0.0023292833464828946) {};
\node[fill=green](49) at (-0.03420085760023503,0.011440096862787573) {};
\node[fill=green](50) at (-0.11537767802840303,-0.009417894080083913) {};
\node[fill=green](51) at (-0.10385142776396412,-0.007681413446208618) {};
\node[fill=green](52) at (-0.11497231710032586,-0.01916181563943129) {};
\node[fill=green](53) at (-0.11384345284875037,-0.008915496149130514) {};
\node[fill=green](54) at (-0.1418522019271105,0.009945806537128354) {};
\node[fill=cyan](55) at (0.11229536240357657,0.10390337649180861) {};
\node[fill=cyan](56) at (0.09621785016278052,0.10012164096018901) {};
\node[fill=cyan](57) at (0.07730970115967592,0.04879081646215706) {};
\node[fill=cyan](58) at (0.10237443367578684,0.07359200054195886) {};
\node[fill=cyan](59) at (0.07293958489288799,0.0633828903303368) {};
\node[fill=cyan](60) at (0.09953512785490022,0.10907071134428713) {};
\node[fill=cyan](61) at (0.10317298259433377,0.09887743706397407) {};
\node[fill=cyan](62) at (0.0354606525430728,-0.028089637501751002) {};
\node[fill=cyan](63) at (0.10317846587665176,0.08722158606946856) {};
\node[fill=cyan](64) at (0.12281783265898168,0.10633538455777025) {};
\node[fill=cyan](65) at (0.08719864995528398,0.0864995821615455) {};
\node[fill=cyan](66) at (0.08067846163421083,0.07564350639996877) {};
\node[fill=cyan](67) at (0.07925716154758809,0.05732825030326772) {};
\node[fill=cyan](68) at (0.11960459948419798,0.10715921278308423) {};
\node[fill=cyan](69) at (0.10327334871339962,0.052246401064706405) {};
\node[fill=orange](70) at (-0.12190417588752714,0.03865263912721711) {};
\node[fill=orange](71) at (-0.14996144226294963,0.04529653968988584) {};
\node[fill=orange](72) at (-0.14555395285224051,0.033391922992644825) {};
\node[fill=orange](73) at (-0.11793869648222371,0.023711100008302965) {};
\node[fill=orange](74) at (-0.13697998287141003,0.046470808422414245) {};
\node[fill=orange](75) at (-0.12600263063739156,0.03651652156636198) {};
\node[fill=orange](76) at (-0.15475682311207992,0.04566264029338459) {};
\node[fill=orange](77) at (-0.1220450725897264,0.03660284109597463) {};
\node[fill=orange](78) at (-0.13130032640922876,0.0389486885091815) {};
\node[fill=orange](79) at (-0.10025984810609924,0.043211029194900545) {};
\node[fill=orange](80) at (-0.15193823788427652,0.03334508745280716) {};
\node[fill=orange](81) at (-0.12560308045845167,0.041171034832980735) {};
\node[fill=orange](82) at (-0.11116180811348121,0.034543150155311494) {};
\node[fill=orange](83) at (-0.12635197223379688,0.03811424634051897) {};
\node[fill=orange](84) at (-0.15954550735059964,0.04353627285376779) {};
\node[fill=magenta](85) at (0.10609925940044689,0.053266710271951345) {};
\node[fill=magenta](86) at (0.09754658993683786,0.014829879501092663) {};
\node[fill=magenta](87) at (0.03490782804988936,0.020169530978496098) {};
\node[fill=magenta](88) at (0.10757745177128425,-0.02926038536465539) {};
\node[fill=magenta](89) at (0.08309116683417309,0.039500480226552954) {};
\node[fill=magenta](90) at (0.07861474204937015,0.042873062357695356) {};
\node[fill=magenta](91) at (0.09358537958816211,0.03489218074493471) {};
\node[fill=magenta](92) at (0.08417419750372704,0.03979533895727236) {};
\node[fill=magenta](93) at (0.12201833670371279,0.10250606382194034) {};
\node[fill=magenta](94) at (0.07587625888810613,0.04065742215759498) {};
\node[fill=magenta](95) at (0.08519874450891896,0.05609531953515105) {};
\node[fill=magenta](96) at (0.09236414600441839,0.06911664965333096) {};
\node[fill=magenta](97) at (0.10095686759384923,0.05060722636149858) {};
\node[fill=magenta](98) at (0.10827771798028502,-0.024326793118052303) {};
\node[fill=magenta](99) at (0.07931728250555724,0.03332919328368493) {};
\path[opacity=0.2]
(0) edge (2)
(0) edge (3)
(0) edge (26)
(0) edge (27)
(0) edge (28)
(0) edge (29)
(0) edge (32)
(0) edge (33)
(0) edge (34)
(0) edge (36)
(0) edge (38)
(0) edge (58)
(0) edge (62)
(0) edge (69)
(0) edge (94)
(0) edge (98)
(1) edge (2)
(1) edge (4)
(1) edge (25)
(1) edge (26)
(1) edge (27)
(1) edge (28)
(1) edge (29)
(1) edge (30)
(1) edge (32)
(1) edge (34)
(1) edge (36)
(1) edge (37)
(1) edge (41)
(1) edge (51)
(1) edge (73)
(2) edge (3)
(2) edge (7)
(2) edge (25)
(2) edge (26)
(2) edge (27)
(2) edge (28)
(2) edge (29)
(2) edge (30)
(2) edge (32)
(2) edge (33)
(2) edge (34)
(2) edge (36)
(2) edge (37)
(2) edge (38)
(2) edge (62)
(2) edge (98)
(3) edge (4)
(3) edge (23)
(3) edge (25)
(3) edge (26)
(3) edge (27)
(3) edge (28)
(3) edge (29)
(3) edge (30)
(3) edge (31)
(3) edge (32)
(3) edge (33)
(3) edge (35)
(3) edge (36)
(3) edge (37)
(3) edge (38)
(3) edge (85)
(3) edge (86)
(3) edge (88)
(3) edge (89)
(3) edge (98)
(4) edge (25)
(4) edge (26)
(4) edge (27)
(4) edge (28)
(4) edge (30)
(4) edge (32)
(4) edge (33)
(4) edge (37)
(4) edge (39)
(4) edge (72)
(4) edge (88)
(4) edge (91)
(4) edge (92)
(4) edge (98)
(5) edge (6)
(5) edge (7)
(5) edge (8)
(5) edge (9)
(5) edge (39)
(5) edge (40)
(5) edge (42)
(5) edge (44)
(5) edge (45)
(5) edge (46)
(5) edge (47)
(5) edge (48)
(5) edge (49)
(5) edge (50)
(5) edge (51)
(5) edge (52)
(5) edge (53)
(5) edge (54)
(5) edge (70)
(5) edge (72)
(5) edge (73)
(5) edge (80)
(5) edge (87)
(6) edge (7)
(6) edge (8)
(6) edge (15)
(6) edge (34)
(6) edge (39)
(6) edge (41)
(6) edge (43)
(6) edge (46)
(6) edge (47)
(6) edge (49)
(6) edge (52)
(6) edge (71)
(6) edge (75)
(6) edge (76)
(6) edge (83)
(6) edge (84)
(7) edge (8)
(7) edge (9)
(7) edge (25)
(7) edge (34)
(7) edge (42)
(7) edge (43)
(7) edge (45)
(7) edge (47)
(7) edge (50)
(7) edge (52)
(7) edge (53)
(7) edge (54)
(7) edge (62)
(7) edge (80)
(8) edge (9)
(8) edge (34)
(8) edge (39)
(8) edge (40)
(8) edge (42)
(8) edge (43)
(8) edge (44)
(8) edge (46)
(8) edge (47)
(8) edge (48)
(8) edge (49)
(8) edge (50)
(8) edge (52)
(8) edge (53)
(8) edge (62)
(9) edge (19)
(9) edge (39)
(9) edge (40)
(9) edge (41)
(9) edge (42)
(9) edge (43)
(9) edge (44)
(9) edge (45)
(9) edge (46)
(9) edge (47)
(9) edge (48)
(9) edge (49)
(9) edge (50)
(9) edge (51)
(9) edge (52)
(9) edge (53)
(9) edge (54)
(9) edge (80)
(10) edge (11)
(10) edge (13)
(10) edge (14)
(10) edge (23)
(10) edge (55)
(10) edge (56)
(10) edge (60)
(10) edge (61)
(10) edge (63)
(10) edge (64)
(10) edge (65)
(10) edge (66)
(10) edge (67)
(10) edge (68)
(10) edge (69)
(10) edge (93)
(10) edge (94)
(11) edge (12)
(11) edge (13)
(11) edge (14)
(11) edge (24)
(11) edge (55)
(11) edge (56)
(11) edge (58)
(11) edge (60)
(11) edge (61)
(11) edge (63)
(11) edge (64)
(11) edge (65)
(11) edge (66)
(11) edge (67)
(11) edge (68)
(11) edge (69)
(11) edge (93)
(12) edge (13)
(12) edge (14)
(12) edge (24)
(12) edge (55)
(12) edge (56)
(12) edge (58)
(12) edge (60)
(12) edge (61)
(12) edge (62)
(12) edge (63)
(12) edge (64)
(12) edge (67)
(12) edge (68)
(12) edge (87)
(12) edge (89)
(12) edge (93)
(12) edge (97)
(13) edge (14)
(13) edge (23)
(13) edge (55)
(13) edge (56)
(13) edge (59)
(13) edge (60)
(13) edge (61)
(13) edge (63)
(13) edge (64)
(13) edge (65)
(13) edge (66)
(13) edge (68)
(13) edge (69)
(13) edge (93)
(14) edge (22)
(14) edge (24)
(14) edge (49)
(14) edge (55)
(14) edge (56)
(14) edge (57)
(14) edge (58)
(14) edge (59)
(14) edge (60)
(14) edge (61)
(14) edge (62)
(14) edge (63)
(14) edge (64)
(14) edge (65)
(14) edge (66)
(14) edge (67)
(14) edge (68)
(14) edge (69)
(14) edge (93)
(14) edge (95)
(14) edge (96)
(15) edge (16)
(15) edge (17)
(15) edge (18)
(15) edge (19)
(15) edge (41)
(15) edge (44)
(15) edge (54)
(15) edge (71)
(15) edge (72)
(15) edge (73)
(15) edge (74)
(15) edge (76)
(15) edge (77)
(15) edge (78)
(15) edge (80)
(15) edge (81)
(15) edge (84)
(16) edge (17)
(16) edge (19)
(16) edge (44)
(16) edge (51)
(16) edge (70)
(16) edge (71)
(16) edge (72)
(16) edge (73)
(16) edge (74)
(16) edge (75)
(16) edge (76)
(16) edge (78)
(16) edge (80)
(16) edge (81)
(16) edge (84)
(17) edge (19)
(17) edge (34)
(17) edge (60)
(17) edge (70)
(17) edge (71)
(17) edge (72)
(17) edge (73)
(17) edge (74)
(17) edge (76)
(17) edge (77)
(17) edge (79)
(17) edge (80)
(17) edge (81)
(17) edge (84)
(18) edge (19)
(18) edge (28)
(18) edge (41)
(18) edge (43)
(18) edge (71)
(18) edge (72)
(18) edge (74)
(18) edge (75)
(18) edge (76)
(18) edge (78)
(18) edge (79)
(18) edge (82)
(18) edge (83)
(18) edge (84)
(19) edge (41)
(19) edge (43)
(19) edge (44)
(19) edge (46)
(19) edge (48)
(19) edge (50)
(19) edge (54)
(19) edge (70)
(19) edge (71)
(19) edge (72)
(19) edge (73)
(19) edge (74)
(19) edge (75)
(19) edge (76)
(19) edge (77)
(19) edge (78)
(19) edge (79)
(19) edge (80)
(19) edge (81)
(19) edge (82)
(19) edge (83)
(19) edge (84)
(20) edge (21)
(20) edge (22)
(20) edge (23)
(20) edge (24)
(20) edge (56)
(20) edge (60)
(20) edge (64)
(20) edge (68)
(20) edge (85)
(20) edge (86)
(20) edge (88)
(20) edge (89)
(20) edge (91)
(20) edge (93)
(20) edge (95)
(20) edge (96)
(20) edge (98)
(21) edge (22)
(21) edge (23)
(21) edge (24)
(21) edge (55)
(21) edge (60)
(21) edge (64)
(21) edge (68)
(21) edge (85)
(21) edge (86)
(21) edge (88)
(21) edge (89)
(21) edge (90)
(21) edge (91)
(21) edge (92)
(21) edge (93)
(21) edge (94)
(21) edge (95)
(21) edge (96)
(21) edge (99)
(22) edge (23)
(22) edge (24)
(22) edge (58)
(22) edge (85)
(22) edge (86)
(22) edge (89)
(22) edge (90)
(22) edge (93)
(22) edge (94)
(22) edge (95)
(22) edge (96)
(22) edge (97)
(22) edge (98)
(23) edge (24)
(23) edge (26)
(23) edge (36)
(23) edge (55)
(23) edge (56)
(23) edge (60)
(23) edge (61)
(23) edge (63)
(23) edge (64)
(23) edge (65)
(23) edge (68)
(23) edge (85)
(23) edge (86)
(23) edge (88)
(23) edge (89)
(23) edge (90)
(23) edge (91)
(23) edge (92)
(23) edge (93)
(23) edge (94)
(23) edge (95)
(23) edge (96)
(23) edge (97)
(23) edge (99)
(24) edge (55)
(24) edge (58)
(24) edge (64)
(24) edge (67)
(24) edge (68)
(24) edge (85)
(24) edge (87)
(24) edge (88)
(24) edge (89)
(24) edge (91)
(24) edge (93)
(24) edge (95)
(24) edge (97)
(24) edge (98)
(24) edge (99)
(25) edge (26)
(25) edge (27)
(25) edge (28)
(25) edge (29)
(25) edge (30)
(25) edge (31)
(25) edge (32)
(25) edge (33)
(25) edge (36)
(25) edge (37)
(25) edge (38)
(25) edge (39)
(25) edge (62)
(26) edge (27)
(26) edge (28)
(26) edge (29)
(26) edge (30)
(26) edge (31)
(26) edge (32)
(26) edge (33)
(26) edge (36)
(26) edge (37)
(26) edge (38)
(26) edge (88)
(27) edge (28)
(27) edge (29)
(27) edge (30)
(27) edge (31)
(27) edge (32)
(27) edge (33)
(27) edge (35)
(27) edge (36)
(27) edge (37)
(27) edge (38)
(27) edge (39)
(27) edge (88)
(28) edge (29)
(28) edge (30)
(28) edge (31)
(28) edge (32)
(28) edge (33)
(28) edge (35)
(28) edge (36)
(28) edge (37)
(28) edge (38)
(28) edge (39)
(28) edge (62)
(28) edge (87)
(28) edge (98)
(29) edge (30)
(29) edge (32)
(29) edge (33)
(29) edge (34)
(29) edge (36)
(29) edge (38)
(29) edge (59)
(29) edge (98)
(30) edge (31)
(30) edge (32)
(30) edge (33)
(30) edge (35)
(30) edge (36)
(30) edge (37)
(30) edge (38)
(30) edge (39)
(30) edge (62)
(30) edge (69)
(31) edge (32)
(31) edge (33)
(31) edge (35)
(31) edge (36)
(31) edge (67)
(31) edge (69)
(31) edge (70)
(31) edge (88)
(31) edge (98)
(32) edge (33)
(32) edge (35)
(32) edge (36)
(32) edge (37)
(32) edge (38)
(32) edge (88)
(33) edge (35)
(33) edge (36)
(33) edge (37)
(33) edge (38)
(33) edge (39)
(33) edge (88)
(33) edge (97)
(34) edge (36)
(34) edge (38)
(34) edge (43)
(34) edge (50)
(34) edge (52)
(34) edge (53)
(34) edge (73)
(34) edge (98)
(35) edge (36)
(35) edge (37)
(35) edge (63)
(35) edge (67)
(35) edge (69)
(35) edge (86)
(35) edge (88)
(35) edge (98)
(36) edge (37)
(36) edge (38)
(36) edge (86)
(36) edge (88)
(36) edge (98)
(37) edge (86)
(37) edge (88)
(37) edge (91)
(37) edge (97)
(37) edge (98)
(38) edge (98)
(38) edge (99)
(39) edge (42)
(39) edge (43)
(39) edge (46)
(39) edge (49)
(39) edge (52)
(39) edge (54)
(39) edge (62)
(40) edge (42)
(40) edge (43)
(40) edge (44)
(40) edge (45)
(40) edge (46)
(40) edge (47)
(40) edge (48)
(40) edge (50)
(40) edge (51)
(40) edge (52)
(40) edge (53)
(40) edge (54)
(41) edge (42)
(41) edge (43)
(41) edge (44)
(41) edge (45)
(41) edge (47)
(41) edge (49)
(41) edge (51)
(41) edge (53)
(41) edge (54)
(41) edge (71)
(41) edge (72)
(41) edge (75)
(41) edge (76)
(41) edge (77)
(41) edge (78)
(41) edge (80)
(41) edge (84)
(42) edge (44)
(42) edge (45)
(42) edge (46)
(42) edge (47)
(42) edge (48)
(42) edge (50)
(42) edge (52)
(42) edge (53)
(42) edge (54)
(42) edge (80)
(43) edge (44)
(43) edge (46)
(43) edge (47)
(43) edge (49)
(43) edge (51)
(43) edge (52)
(43) edge (53)
(43) edge (62)
(43) edge (71)
(43) edge (72)
(43) edge (75)
(43) edge (76)
(43) edge (82)
(43) edge (83)
(43) edge (84)
(44) edge (45)
(44) edge (46)
(44) edge (47)
(44) edge (48)
(44) edge (50)
(44) edge (51)
(44) edge (52)
(44) edge (53)
(44) edge (54)
(44) edge (71)
(44) edge (72)
(44) edge (75)
(44) edge (76)
(44) edge (77)
(44) edge (78)
(44) edge (80)
(44) edge (82)
(44) edge (83)
(44) edge (84)
(45) edge (46)
(45) edge (47)
(45) edge (48)
(45) edge (49)
(45) edge (50)
(45) edge (51)
(45) edge (52)
(45) edge (53)
(45) edge (54)
(45) edge (80)
(46) edge (48)
(46) edge (49)
(46) edge (50)
(46) edge (52)
(46) edge (54)
(46) edge (72)
(46) edge (84)
(47) edge (50)
(47) edge (51)
(47) edge (52)
(47) edge (53)
(47) edge (54)
(48) edge (50)
(48) edge (51)
(48) edge (52)
(48) edge (53)
(48) edge (54)
(48) edge (80)
(49) edge (51)
(49) edge (57)
(49) edge (59)
(49) edge (63)
(49) edge (97)
(50) edge (51)
(50) edge (52)
(50) edge (53)
(50) edge (54)
(50) edge (84)
(51) edge (52)
(51) edge (53)
(51) edge (54)
(51) edge (80)
(52) edge (53)
(52) edge (54)
(53) edge (78)
(53) edge (80)
(54) edge (70)
(54) edge (72)
(54) edge (73)
(54) edge (76)
(54) edge (77)
(54) edge (80)
(54) edge (81)
(54) edge (84)
(55) edge (56)
(55) edge (58)
(55) edge (59)
(55) edge (60)
(55) edge (61)
(55) edge (63)
(55) edge (64)
(55) edge (65)
(55) edge (66)
(55) edge (67)
(55) edge (68)
(55) edge (69)
(55) edge (88)
(55) edge (93)
(55) edge (95)
(55) edge (96)
(55) edge (98)
(56) edge (59)
(56) edge (60)
(56) edge (61)
(56) edge (63)
(56) edge (64)
(56) edge (65)
(56) edge (68)
(56) edge (89)
(56) edge (93)
(56) edge (96)
(57) edge (58)
(57) edge (60)
(57) edge (61)
(57) edge (62)
(57) edge (67)
(57) edge (68)
(57) edge (69)
(57) edge (85)
(57) edge (88)
(57) edge (90)
(57) edge (91)
(57) edge (92)
(57) edge (94)
(57) edge (97)
(57) edge (98)
(57) edge (99)
(58) edge (59)
(58) edge (60)
(58) edge (61)
(58) edge (62)
(58) edge (63)
(58) edge (64)
(58) edge (66)
(58) edge (67)
(58) edge (68)
(58) edge (69)
(58) edge (85)
(58) edge (88)
(58) edge (91)
(58) edge (93)
(58) edge (97)
(58) edge (98)
(59) edge (60)
(59) edge (61)
(59) edge (63)
(59) edge (64)
(59) edge (65)
(59) edge (68)
(59) edge (69)
(59) edge (93)
(59) edge (97)
(60) edge (61)
(60) edge (63)
(60) edge (64)
(60) edge (65)
(60) edge (66)
(60) edge (67)
(60) edge (68)
(60) edge (69)
(60) edge (70)
(60) edge (79)
(60) edge (92)
(60) edge (93)
(60) edge (94)
(60) edge (96)
(60) edge (97)
(60) edge (98)
(61) edge (63)
(61) edge (64)
(61) edge (66)
(61) edge (68)
(61) edge (69)
(61) edge (85)
(61) edge (89)
(61) edge (93)
(61) edge (97)
(61) edge (99)
(62) edge (63)
(62) edge (64)
(62) edge (66)
(62) edge (67)
(62) edge (69)
(62) edge (79)
(62) edge (94)
(62) edge (98)
(63) edge (64)
(63) edge (65)
(63) edge (66)
(63) edge (67)
(63) edge (68)
(63) edge (69)
(63) edge (88)
(63) edge (93)
(63) edge (96)
(63) edge (98)
(64) edge (65)
(64) edge (66)
(64) edge (67)
(64) edge (68)
(64) edge (69)
(64) edge (86)
(64) edge (87)
(64) edge (88)
(64) edge (89)
(64) edge (91)
(64) edge (92)
(64) edge (93)
(64) edge (95)
(64) edge (96)
(64) edge (97)
(65) edge (66)
(65) edge (68)
(65) edge (69)
(65) edge (88)
(65) edge (93)
(66) edge (67)
(66) edge (69)
(66) edge (87)
(66) edge (96)
(66) edge (98)
(67) edge (68)
(67) edge (69)
(67) edge (79)
(67) edge (87)
(67) edge (94)
(67) edge (98)
(68) edge (69)
(68) edge (85)
(68) edge (86)
(68) edge (90)
(68) edge (91)
(68) edge (92)
(68) edge (93)
(68) edge (94)
(68) edge (97)
(69) edge (85)
(69) edge (90)
(69) edge (94)
(69) edge (97)
(69) edge (98)
(69) edge (99)
(70) edge (71)
(70) edge (72)
(70) edge (73)
(70) edge (74)
(70) edge (75)
(70) edge (76)
(70) edge (77)
(70) edge (78)
(70) edge (79)
(70) edge (80)
(70) edge (81)
(70) edge (82)
(70) edge (83)
(70) edge (84)
(70) edge (87)
(71) edge (72)
(71) edge (73)
(71) edge (74)
(71) edge (75)
(71) edge (76)
(71) edge (77)
(71) edge (78)
(71) edge (79)
(71) edge (80)
(71) edge (81)
(71) edge (82)
(71) edge (83)
(71) edge (84)
(72) edge (73)
(72) edge (74)
(72) edge (75)
(72) edge (76)
(72) edge (78)
(72) edge (79)
(72) edge (80)
(72) edge (81)
(72) edge (82)
(72) edge (83)
(72) edge (84)
(73) edge (74)
(73) edge (76)
(73) edge (77)
(73) edge (79)
(73) edge (80)
(73) edge (81)
(73) edge (84)
(74) edge (75)
(74) edge (76)
(74) edge (77)
(74) edge (78)
(74) edge (79)
(74) edge (80)
(74) edge (81)
(74) edge (82)
(74) edge (83)
(74) edge (84)
(75) edge (76)
(75) edge (78)
(75) edge (79)
(75) edge (82)
(75) edge (83)
(75) edge (84)
(76) edge (77)
(76) edge (78)
(76) edge (79)
(76) edge (80)
(76) edge (81)
(76) edge (82)
(76) edge (83)
(76) edge (84)
(77) edge (80)
(77) edge (81)
(77) edge (82)
(77) edge (83)
(77) edge (84)
(78) edge (79)
(78) edge (80)
(78) edge (82)
(78) edge (83)
(78) edge (84)
(79) edge (80)
(79) edge (81)
(79) edge (82)
(79) edge (83)
(79) edge (84)
(80) edge (81)
(80) edge (83)
(80) edge (84)
(80) edge (87)
(81) edge (83)
(81) edge (84)
(82) edge (83)
(82) edge (84)
(83) edge (84)
(85) edge (86)
(85) edge (87)
(85) edge (88)
(85) edge (89)
(85) edge (90)
(85) edge (91)
(85) edge (92)
(85) edge (94)
(85) edge (95)
(85) edge (96)
(85) edge (97)
(85) edge (98)
(85) edge (99)
(86) edge (88)
(86) edge (89)
(86) edge (90)
(86) edge (91)
(86) edge (92)
(86) edge (93)
(86) edge (95)
(86) edge (96)
(86) edge (97)
(86) edge (98)
(86) edge (99)
(87) edge (88)
(87) edge (89)
(87) edge (91)
(87) edge (94)
(87) edge (98)
(88) edge (89)
(88) edge (90)
(88) edge (91)
(88) edge (92)
(88) edge (93)
(88) edge (95)
(88) edge (96)
(88) edge (98)
(88) edge (99)
(89) edge (94)
(89) edge (97)
(89) edge (98)
(90) edge (91)
(90) edge (92)
(90) edge (93)
(90) edge (97)
(90) edge (98)
(90) edge (99)
(91) edge (92)
(91) edge (93)
(91) edge (95)
(91) edge (97)
(91) edge (98)
(91) edge (99)
(92) edge (93)
(92) edge (97)
(92) edge (98)
(92) edge (99)
(93) edge (95)
(93) edge (96)
(93) edge (97)
(93) edge (98)
(93) edge (99)
(94) edge (96)
(94) edge (97)
(94) edge (98)
(95) edge (96)
(95) edge (98)
(96) edge (98)
(97) edge (98)
(97) edge (99)
(98) edge (99);
\node[fill=blue](0) at (0.045680106307013336,-0.15944062638357112) {};
\node[fill=blue](1) at (0.005795768814802205,-0.15185095713528257) {};
\node[fill=blue](2) at (0.03463846860889502,-0.2110892481243941) {};
\node[fill=blue](3) at (0.06579803803191195,-0.2051698618809592) {};
\node[fill=blue](4) at (0.03510376101988162,-0.1524208974077208) {};
\node[fill=green](5) at (-0.12223756962994375,-0.004875656341539053) {};
\node[fill=green](6) at (-0.09294142330772531,-0.0037330623280312364) {};
\node[fill=green](7) at (-0.08118991426078026,-0.03974422991990645) {};
\node[fill=green](8) at (-0.0912337660777498,-0.021272203605895115) {};
\node[fill=green](9) at (-0.12331059700548036,-0.008604068331404577) {};
\node[fill=cyan](10) at (0.09150783364461061,0.09451427103269636) {};
\node[fill=cyan](11) at (0.09721522869932414,0.10285940530770542) {};
\node[fill=cyan](12) at (0.08663011121818587,0.08378056606101787) {};
\node[fill=cyan](13) at (0.09297082015007248,0.09935849885229188) {};
\node[fill=cyan](14) at (0.10508624908952474,0.10586678090724996) {};
\node[fill=orange](15) at (-0.13205851027855717,0.03716700817903457) {};
\node[fill=orange](16) at (-0.11899280991198748,0.03613495199043308) {};
\node[fill=orange](17) at (-0.10851559445142425,0.037857019577786574) {};
\node[fill=orange](18) at (-0.10653092664743519,0.02076141025731176) {};
\node[fill=orange](19) at (-0.16053915923055537,0.04120600742452389) {};
\node[fill=magenta](20) at (0.09179943072273908,0.061911913917469086) {};
\node[fill=magenta](21) at (0.09771416381267159,0.0683329982718129) {};
\node[fill=magenta](22) at (0.08365061126396928,0.05613146767544901) {};
\node[fill=magenta](23) at (0.1185739609011659,0.061777875114200895) {};
\node[fill=magenta](24) at (0.10036771156647388,0.07379741517943594) {};
\node[fill=blue](25) at (0.033153027791940994,-0.2220536260316069) {};
\node[fill=blue](26) at (0.05067007948230345,-0.23578184464944213) {};
\node[fill=blue](27) at (0.04664711863391337,-0.24511707325278492) {};
\node[fill=blue](28) at (0.04044576373695165,-0.22123819250010235) {};
\node[fill=blue](29) at (0.041128402691482914,-0.2080563988723915) {};
\node[fill=blue](30) at (0.045091408004321556,-0.22521561870600532) {};
\node[fill=blue](31) at (0.04053143687310179,-0.14815479236631182) {};
\node[fill=blue](32) at (0.0508515614600885,-0.2509020057882185) {};
\node[fill=blue](33) at (0.05110040178177802,-0.2363169111843277) {};
\node[fill=blue](34) at (-0.03348445560830227,-0.08278578644557111) {};
\node[fill=blue](35) at (0.052890327746633936,-0.12786178045450736) {};
\node[fill=blue](36) at (0.05620659705713664,-0.2233136032098087) {};
\node[fill=blue](37) at (0.05320283605517201,-0.1848374687520992) {};
\node[fill=blue](38) at (0.03893892284687967,-0.1917405733000982) {};
\node[fill=blue](39) at (-0.04245496021167464,-0.08804175272721608) {};
\node[fill=green](40) at (-0.1064825977134248,-0.006826339044408989) {};
\node[fill=green](41) at (-0.13389232272383758,0.015130925319067892) {};
\node[fill=green](42) at (-0.11111941340836073,-0.011877867366608893) {};
\node[fill=green](43) at (-0.12154741827105724,0.0009072327355212306) {};
\node[fill=green](44) at (-0.16000862810804425,0.021283395392449615) {};
\node[fill=green](45) at (-0.11563117306504925,-0.0048550536600447955) {};
\node[fill=green](46) at (-0.10843569174027456,-0.003636968726271712) {};
\node[fill=green](47) at (-0.10651348022577209,-0.009104519220707605) {};
\node[fill=green](48) at (-0.10933153063529844,-0.0023292833464828946) {};
\node[fill=green](49) at (-0.03420085760023503,0.011440096862787573) {};
\node[fill=green](50) at (-0.11537767802840303,-0.009417894080083913) {};
\node[fill=green](51) at (-0.10385142776396412,-0.007681413446208618) {};
\node[fill=green](52) at (-0.11497231710032586,-0.01916181563943129) {};
\node[fill=green](53) at (-0.11384345284875037,-0.008915496149130514) {};
\node[fill=green](54) at (-0.1418522019271105,0.009945806537128354) {};
\node[fill=cyan](55) at (0.11229536240357657,0.10390337649180861) {};
\node[fill=cyan](56) at (0.09621785016278052,0.10012164096018901) {};
\node[fill=cyan](57) at (0.07730970115967592,0.04879081646215706) {};
\node[fill=cyan](58) at (0.10237443367578684,0.07359200054195886) {};
\node[fill=cyan](59) at (0.07293958489288799,0.0633828903303368) {};
\node[fill=cyan](60) at (0.09953512785490022,0.10907071134428713) {};
\node[fill=cyan](61) at (0.10317298259433377,0.09887743706397407) {};
\node[fill=cyan](62) at (0.0354606525430728,-0.028089637501751002) {};
\node[fill=cyan](63) at (0.10317846587665176,0.08722158606946856) {};
\node[fill=cyan](64) at (0.12281783265898168,0.10633538455777025) {};
\node[fill=cyan](65) at (0.08719864995528398,0.0864995821615455) {};
\node[fill=cyan](66) at (0.08067846163421083,0.07564350639996877) {};
\node[fill=cyan](67) at (0.07925716154758809,0.05732825030326772) {};
\node[fill=cyan](68) at (0.11960459948419798,0.10715921278308423) {};
\node[fill=cyan](69) at (0.10327334871339962,0.052246401064706405) {};
\node[fill=orange](70) at (-0.12190417588752714,0.03865263912721711) {};
\node[fill=orange](71) at (-0.14996144226294963,0.04529653968988584) {};
\node[fill=orange](72) at (-0.14555395285224051,0.033391922992644825) {};
\node[fill=orange](73) at (-0.11793869648222371,0.023711100008302965) {};
\node[fill=orange](74) at (-0.13697998287141003,0.046470808422414245) {};
\node[fill=orange](75) at (-0.12600263063739156,0.03651652156636198) {};
\node[fill=orange](76) at (-0.15475682311207992,0.04566264029338459) {};
\node[fill=orange](77) at (-0.1220450725897264,0.03660284109597463) {};
\node[fill=orange](78) at (-0.13130032640922876,0.0389486885091815) {};
\node[fill=orange](79) at (-0.10025984810609924,0.043211029194900545) {};
\node[fill=orange](80) at (-0.15193823788427652,0.03334508745280716) {};
\node[fill=orange](81) at (-0.12560308045845167,0.041171034832980735) {};
\node[fill=orange](82) at (-0.11116180811348121,0.034543150155311494) {};
\node[fill=orange](83) at (-0.12635197223379688,0.03811424634051897) {};
\node[fill=orange](84) at (-0.15954550735059964,0.04353627285376779) {};
\node[fill=magenta](85) at (0.10609925940044689,0.053266710271951345) {};
\node[fill=magenta](86) at (0.09754658993683786,0.014829879501092663) {};
\node[fill=magenta](87) at (0.03490782804988936,0.020169530978496098) {};
\node[fill=magenta](88) at (0.10757745177128425,-0.02926038536465539) {};
\node[fill=magenta](89) at (0.08309116683417309,0.039500480226552954) {};
\node[fill=magenta](90) at (0.07861474204937015,0.042873062357695356) {};
\node[fill=magenta](91) at (0.09358537958816211,0.03489218074493471) {};
\node[fill=magenta](92) at (0.08417419750372704,0.03979533895727236) {};
\node[fill=magenta](93) at (0.12201833670371279,0.10250606382194034) {};
\node[fill=magenta](94) at (0.07587625888810613,0.04065742215759498) {};
\node[fill=magenta](95) at (0.08519874450891896,0.05609531953515105) {};
\node[fill=magenta](96) at (0.09236414600441839,0.06911664965333096) {};
\node[fill=magenta](97) at (0.10095686759384923,0.05060722636149858) {};
\node[fill=magenta](98) at (0.10827771798028502,-0.024326793118052303) {};
\node[fill=magenta](99) at (0.07931728250555724,0.03332919328368493) {};
 \end{tikzpicture}
\end{center}
\caption{Visualisation of a graph obtained using miniImageNet. Colors represent various classes. Vertices are placed close if they share many connections.}
\label{graphvisu}
\end{figure}






\section{Conclusion}
\label{sec:con}
In this paper we introduced a novel method to solve the few-shot classification problem. It consists in combining three steps: a pretrained transfer, a graph-based interpolation technique and a logistic regression.

By performing experiments on standardized vision datasets, we obtained state-of-the-art results, with the most important gains in the case of 1-shot classification.

Interestingly, the proposed method requires to tune few hyperparameters, and these have a little impact on accuracy. We thus believe that it is an applicable solution to many practical problems.

There are still open questions to be addressed, such as the case of imbalanced classes, or settings where prediction must be performed on streaming data, one input at a time.

















\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,egbib}









\end{document}
