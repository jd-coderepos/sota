






\begin{table*}[!ht]
\centering
\scalebox{0.88}{
\begin{tabular}{l|llllllllll}\hline
 \multicolumn{2}{c}{}& \multicolumn{3}{c}{MSCOCO-1K}&  \multicolumn{3}{c}{MSCOCO-5K}& \multicolumn{3}{c}{Flickr 30K} \\ 

 \multicolumn{2}{c}{} & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 \\ \hline 
 
 \multirow{2}{1.5cm}{Query-dependent} & Unicoder-VL~\cite{li2020unicoder} & 69.7 & 93.5 & 97.2 & 46.7 & 76.0 & 85.3 & 71.5 & 90.9 & 94.9 \\ 
& Oscar ~\cite{li2020oscar} & 75.7 & 95.2 & 98.3 & 54.0 & 80.8 & 88.5 & - & - & - \\\hline


\multirow{8}{1.5cm}{Query-agnostic} & SM-LSTM~\cite{huang2017instance} & 40.7 & 75.8 & 87.4 & - & - & - & 30.2 & 60.4 & 72.3\\
& DAN~\cite{nam2017dual} & - & - & - & - & - & - & 39.4 & 69.2 & 79.1\\
& VSE++~\cite{faghri2017vse++} & 52.0 & - & 92.0 & 30.3 & - & 72.4 & 39.6 & - & 79.5\\
& CAMP~\cite{wang2019camp} & 58.5 & 87.9 & 95.0 & 39.0 & 68.9 & 80.2 & 51.5 & 77.1 & 85.3\\
& SCAN~\cite{lee2018stacked} & 58.8 & 88.4 & 94.8 & 38.6 & 69.3 & 80.4 & 48.6 & 77.7 & 85.2\\
& PFAN~\cite{wang2019position} & 61.6 & 89.6 & 95.2 & - & - & - & 50.4 & 78.7 & 86.1 \\ 
& CVSE~\cite{wang2020consensus} & 59.9 & 89.4 & 95.2 & 35.3 & 66.4 & 78.4 & 52.9 & 80.4 & 87.8 \\

& \textbf{VisualSparta (ours)} & \textbf{68.7} & \textbf{91.2} & \textbf{96.2} & \textbf{45.1} & \textbf{73.0} & \textbf{82.5} &  \textbf{57.1} & \textbf{82.6} & \textbf{88.2} \\ \hline

\end{tabular}}
\caption{Detailed comparisons of text-to-image retrieval results in MSCOCO (1K/5K) and Flickr30K datasets}
\label{tbl:visualsparta-score}

\end{table*}

\subsection{Datasets}
In this paper, we use MSCOCO~\cite{lin2014microsoft}\footnote{\url{https://cocodataset.org}} and Flickr30K~\cite{plummer2015flickr30k}\footnote{\url{http://bryanplummer.com/Flickr30kEntities}} datasets for the training and evaluation of text-to-image retrieval tasks. MSCOCO is a large-scale multi-task dataset including object detection, semantic segmentation, and image captioning data. In this experiment, we follow the previous work and use the image captioning data split for text-to-image model training and evaluation. Following the experimental settings from~\citet{karpathy2015deep}, we split the data into 113,287 images for training, 5,000 images for validation, and 5,000 images for testing. Each image is paired with 5 different captions. The performance of 1,000 (1K) and 5,000 (5K) test splits are reported and compared with previous results.

Flickr30K~\cite{plummer2015flickr30k} is another publicly available image captioning dataset, which contains 31,783 images in total. Following the split from ~\citet{karpathy2015deep}, 29,783 images are used for training, and 1,000 images are used for validation. Scores are reported based on results from 1,000 test images. 

For speed experiments, in addition to MSCOCO 1K and 5K splits, we create 113K split and 1M split, two new data splits to test the performance in the large-scale retrieval setting. Since these splits are only used for speed experiments, we directly reuse the training data from the existing dataset without the concern of data leaking between training and testing phases. Specifically, the 113K split refers to the MSCOCO training set, which contains 113,287 images, 23 times larger than the MSCOCO 5K test set. The 1M split consists of one million images randomly sampled from the MSCOCO training set. Speed experiments are done on these four splits to give comprehensive comparisons under different sizes of image index.


\subsection{Evaluation Metrics}
\label{sec:visualsparta-metrics}
Following previous works, we use recall rate as our accuracy evaluation metrics. In both MSCOCO and Flikr30K datasets, we report Recall@\textit{t}, \textit{t}=[1, 5, 10] and compare with previous works. 

For speed performance evaluation, we choose query per second and latency(ms) as the evaluation metric to test how each model performs in terms of speed under different sizes of image index.

\subsection{Implementation Details}
All experiments are done using the PyTorch library. During training, one NVIDIA Titan X GPU is used. During speed performance evaluation, one NVIDIA Titan X GPU is used for models that need GPU acceleration. One 10-core Intel 9820X CPU is used for models that needs CPU acceleration. For the image encoder, we initialize the model weights from Oscar-base model~\cite{li2020oscar} with 12 layers, 768 hidden dimensions, and 110M parameters. For the query embedding, we initialize it from the Oscar-base token embedding. The Adam optimizer~\cite{kingma2014adam} is used with the learning rate set to 5e-5. The number of training epochs is set to 20. The input sequence length is set to 120, with 70 for labels with attributes features and 50 for deep visual features. We search on batch sizes (96, 128, 160) with Recall@1 validation accuracy, and set the batch size to 160.


\subsection{Experimental Results}
We compare both recall and speed performance with the current state-of-the-art retrieval model in text-to-image search. Query-dependent model refers to models in which image information cannot be encoded offline, because each image encoding is dependent on the query information. These models usually achieve promising performance in recall but suffer from prohibitively slow inference speed. Query-agnostic model refers to models in which image information can be encoded offline and is independent of query information. In section~\ref{sec:recall-perf} and ~\ref{sec:speed-perf}, we evaluate accuracy and speed performance respectively for both lines of methods.




\subsubsection{Recall Performance}
\label{sec:recall-perf}

As shown in Table~\ref{tbl:visualsparta-score}, the results reveal that our model is competitive compared with previous methods. Among query-agnostic methods, our model is significantly superior to the state-of-the-art results in all evaluation metrics over both MSCOCO and Flickr30K datasets and outperforms previous methods by a large margin. Specifically, in MSCOCO 1K test set, our model outperforms the previously best query-agnostic method~\cite{wang2019position} by 7.1\%, 1.6\%, 1.0\% for Recall@1, 5, 10 respectively. In Flickr30K dataset, VisualSparta also shows strong improvement compared with the previous best method: in Recall@1,5,10, our model gets 4.2\%, 2.2\%, 0.4\% improvement respectively.

We also observe that VisualSparta reduces the gap by a large margin between query-agnostic and query-dependent methods. In MSCOCO-1K split, the performance of VisualSparta is only 1.0\%, 2.3\%, 1.0\% lower than Unicoder-VL method~\cite{li2020unicoder} for Recall@1,5,10 respectively. Compared to Oscar~\cite{li2020oscar}, the current state-of-the-art query-dependent model, our model is 7\% lower than the Oscar model in MSCOCO-1K Recall@1. This shows that there is still room for improvement in terms of accuracy for query-agnostic model.






\subsubsection{Speed Performance}
\label{sec:speed-perf}
\begin{table}[ht]
\centering
\scalebox{0.9}{
\begin{tabular}{P{0.16\textwidth}p{0.06\textwidth}p{0.06\textwidth}p{0.06\textwidth}p{0.06\textwidth}}
 


& \multicolumn{2}{c}{GPU}& \multicolumn{2}{c}{CPU} \\ 
\cmidrule(lr){2-3}\cmidrule(lr){4-5}

Index Size vs. Query/s & Oscar & CVSE & CVSE & Visual Sparta \\ \hline

1K        & 0.4    & 195.1 & 177.4& \textbf{451.4}        \\
5K        & 0.06   & 191.0 & 162.0& \textbf{390.5}        \\
113K      & 0.003  & 101.2 & 5.4   & \textbf{275.5}        \\
1M        & 0.0003 & 21.7 & 0.3   & \textbf{117.3}        \\ \hline
\end{tabular}}
\caption{\textbf{Model Speed vs. Index Size}: VisualSparta experiments are done under setting top-\textit{n} term scores to 1000. Detailed settings are reported in section \ref{sec:speed-perf}.}
\label{tbl:visualsparta-speed}
\end{table}
To show the efficiency of VisualSparta model in both small-scale and large-scale settings, we create 113K dataset and 1M dataset in addition to the original 1K and 5K test split, as discussed in section~\ref{sec:visualsparta-metrics}. Speed experiments are done using these four splits as testbeds. 


\begin{table*}[ht!]
\centering
\small
\begin{tabular}{ccc@{\hskip 0.6cm}ccc@{\hskip 0.6cm}ccc}\hline
 & & & \multicolumn{3}{c}{MSCOCO-1k}&  \multicolumn{3}{c}{MSCOCO-5k} \\ \hline 

 \textit{n} & Inf. time (ms) & query/s & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 \\ \hline  

50 & 1.9 & 537.0 & 54.6	& 82.8	& 90.7	& 33.0	& 60.0	& 71.1\\
100	& 1.9 & 514.7	& 60.1	& 86.2	& 92.8	& 37.1	& 64.6	& 75.3\\
500	& 2.1 & 477.7 & 65.5	& 90.3	& 95.1	& 42.5	& 70.6	& 80.4\\
1000 & 2.4 & 414.5 & 67.5	& 90.9	& 95.8	& 43.7	& 71.7	& 81.5\\
2000 & 3.9 & 256.3 & 68.5	& 91.1	& 96.0	& 44.4	& 72.5	& 82.1\\
all & 6.9 & 144.1 & 68.7	& 91.2	& 96.2	& 45.1	& 73.0	& 82.5\\ \hline

\end{tabular}
\caption{Effect of top-\textit{n} term scores in terms of speed and accuracy tested in MSCOCO dataset;  means higher the better, and  means lower the better.}
\label{tbl:visualsparta-tscore}
\end{table*}



To make a fair comparison, we benchmark each method with its preferred hardware and software for speed acceleration. Specifically, For CVSE model~\cite{wang2020consensus}, both CPU and GPU inference time are recorded. For CPU setting, the Maximum Inner Product Search (MIPS) is performed using their original code based on Numpy~\cite{harris2020array}. For GPU setting, we adopt the model and use FAISS~\cite{johnson2019billion}, an optimized MIPS library, to test the speed performance. For Oscar model~\cite{li2020oscar}, since the query-dependent method cannot be formulated as a MIPS problem, we run the original model using GPU acceleration and record the speed. For VisualSparta, we use the top-1000 term scores settings for the experiment. Since VisualSparta can be fit into an inverted-index architecture, GPU acceleration is not required. For all experiments, we use 5000 queries from MSCOCO-1K split as query input to test the speed performance.







As we can see from Table~\ref{tbl:visualsparta-speed}, in all four data splits (1K, 5K, 113K, 1M), VisualSparta significantly outperforms both the best query-agnostic model (CVSE~\cite{wang2020consensus}) and the best query-dependent model (Oscar~\cite{li2020oscar}). Under CPU comparison, the speed of VisualSparta is 2.5, 2.4, 51, and 391 times faster than that of the CVSE model in 1K, 5K, 113K, and 1M splits respectively.

This speed advantage also holds even if previous models are accelerated with GPU acceleration. To apply the latest MIPS progress to the comparison, we adopt the CVSE model to use FAISS~\cite{johnson2019billion} for better speed acceleration. Results in the table reveal that the speed of VisualSparta can also beat that of CVSE by 2.5X in the 1K setting, and this speed advantage increases to 5.4X when the index size increases to 1M. 

Our model holds an absolute advantage when comparing speed to query-dependent models such as Oscar~\cite{li2020oscar}. Since the image encoding is dependent on the query information, no offline indexing can be done for the query-dependent model. As shown in Table~\ref{tbl:visualsparta-speed}, even with GPU acceleration, Oscar model is prohibitively slow: In the 1K setting, Oscar is 1128 times slower than VisualSparta. The number increases to 391,000 when index size increases to 1M. 








