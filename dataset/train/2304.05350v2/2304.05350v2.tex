
\documentclass{article} \usepackage{iclr2023_conference,times}
\usepackage{inputenc}
\usepackage{graphicx}
\graphicspath{ {./media/} }
\usepackage{multirow}


\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 \usepackage [autostyle, english = american]{csquotes}
\MakeOuterQuote{"}
\usepackage{hyperref}
\usepackage{url}
\usepackage{subcaption}
\usepackage{amsbsy}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{pifont}
\usepackage{adjustbox}
\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\title{Astroformer: More Data Might not be all you need for Classification}



\author{Rishit Dagli \\
University of Toronto\\
\texttt{rishit@cs.toronto.edu}
}

\author{
Rishit Dagli\\
University of Toronto\\
\texttt{rishit@cs.toronto.edu}
}




\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\rulesep}{\unskip\ \vrule\ }

\iclrfinalcopy \begin{document}


\maketitle

\begin{abstract}
Recent advancements in areas such as natural language processing and computer vision rely on intricate and massive models that have been trained using vast amounts of unlabelled or partly labeled data and training or deploying these state-of-the-art methods to resource constraint environments has been a challenge. Galaxy morphologies are crucial to understanding the processes by which galaxies form and evolve. Efficient methods to classify galaxy morphologies are required to extract physical information from modern-day astronomy surveys. In this paper, we introduce Astroformer, a method to learn from less amount of data. We propose using a hybrid transformer-convolutional architecture drawing much inspiration from the success of CoAtNet and MaxViT. Concretely, we use the transformer-convolutional hybrid with a new stack design for the network, a different way of creating a relative self-attention layer, and pair it with a careful selection of data augmentation and regularization techniques. Our approach sets a new state-of-the-art on predicting galaxy morphologies from images on the Galaxy10 DECals dataset, a science objective, which consists of 17736 labeled images achieving  top- accuracy, beating the current state-of-the-art for this task by . Furthermore, this approach also sets a new state-of-the-art on CIFAR-100 and Tiny ImageNet. We also find that models and training methods used for larger datasets would often not work very well in the low-data regime.
\end{abstract}

\section{Introduction}
\label{introduction}

Recently, many hybrid transformer-convolutional models have gained a lot of popularity in vision tasks, especially with the success of models like MobileViT \citep{mehta2022mobilevit}, ResNet-ViT \citep{dosovitskiy2021an}, ConViT \citep{d2021convit}, PatchConvNet \citep{https://doi.org/10.48550/arxiv.2112.13692}, and CoAtNet \citep{dai2021coatnet}. On one hand, the spatial inductive biases of convolutional neural networks allow them to learn representations with fewer parameters across different vision tasks as well as enable sample-efficient learning; however, they often have a potentially lower performance. On the other hand, transformers learn global representations and have shown a lot of success in vision tasks, and have outperformed convolutional neural nets for image classification. Though transformers tend to have larger model capacities, their generalization can be worse than convolutional neural networks. However, transformers are very data hungry \citep{https://doi.org/10.48550/arxiv.2104.05704} and often require pre-training on large datasets. Typically Vision Transformers (ViTs) show great performance when trained on ImageNet-k/k \citep{deng2009imagenet} or JFT-M \citep{sun2017revisiting} datasets; however, the absence of such large-scale pre-training is very detrimental to its performance as we show in this paper.

In practice, collecting high-quality labeled data or human annotators is very expensive. Synthetic data though shows significant promise but is often a distorted version of the real data and any modeling or inference performed on synthetic data comes with additional risks. Models trained on synthetic data often need to be fine-tuned with real data before being deployed \citep{Tremblay_2018_CVPR_Workshops, https://doi.org/10.48550/arxiv.2205.03257}. Furthermore, methods such as transfer learning will not fully solve the problem often due to bias in pre-training datasets that do not reflect environments \citep{salman2023when}. We also explored semi-supervised learning approaches for this task using labeled data from the Galaxy10 DECals dataset \citep{2019MNRAS.483.3255L} and unlabelled galaxy images from the GalaxyZoo dataset \citep{2011MNRAS}, this approach is promising and we believe is most certainly an interesting direction. We however focused our efforts on building models when obtaining unlabeled data is costly as well Thus, in this paper, we focus on training in a supervised setting from scratch.

A galaxy's morphology describes its physical appearance and encodes important information about the physical processes that have shaped its growth \citep{10.1111/j.1365-2966.2009.14921.x}. Galaxy morphologies are influenced heavily by the physical properties of galaxies, like star formation \citep{hashimoto1998influence, sandage1986star, lotz2006rest} and galaxy mergers \citep{rodriguez2017role} among others \citep{https://doi.org/10.48550/arxiv.1102.0550}. Thus understanding galaxy morphologies leads to a better understanding on these fronts as well. One such instance is where scientists have used the disc and spheroid structures that make up a galaxy to figure out crucial components of its formation \citep{10.1007/978-3-540-28642-4_2}. As such, a galaxy's morphology is a culmination of internal physical processes (e.g., star formation, stellar dynamics such as active galactic nuclei and dark matter, feedback), as well as its environment and interactions with other galaxies. Thus, to fully comprehend the formation and evolution of galaxies, it is essential to accurately classify galaxy morphologies.

Our contributions can be summarized as: \textbf{(1)} We propose a hybrid transformer-convolutional architecture comparable to the approach employed by CoAtNet \citep{dai2021coatnet} with a different stack design and pair it with a careful selection of augmentation and regularization techniques which is able to learn and generalize well \footnote{By generalization, we mean that we measured the gap between the training loss and the evaluation accuracy that describes how well the model can generalize to unseen data.} in the low-data regime. \textbf{(2)} With our approach we establish a new state-of-the-art for the task of identifying galaxy morphologies from images on the Galaxy10 DECals dataset, and to the best of our knowledge, we believe this is the first work to use a hybrid transformer-convolutional model to improve models in this domain. We also set the new state-of-the-art for Tiny ImageNet and CIFAR-100. \textbf{(3)} We explore approaches and techniques for designing non-transfer learned models for the low-data regime in general which can be applied to tasks other than the one we explore.

\section{Data}
\label{Data}

We use the Galaxy10 DECals dataset introduced by \cite{2019MNRAS.483.3255L} which contains k labeled images (see raw images in Appendix \ref{Supplemental Figures}, Figure \ref{fig:rawimages}). These images come from the DESI Legacy Imaging Surveys which include: two DECals campaign \citep{dey2019overview, 2022MNRAS}, the Beijing-Arizona Sky Survey \citep{Zou_2019} as well as the Mayall z-band Legacy Survey \citep{dey2019overview}. The labels for the galaxy morphologies come from the GalaxyZoo Data Release 2 \citep{2008MNRAS, 2011MNRAS}. This dataset includes 10 strictly exclusive morphology classes: disturbed galaxies, merging galaxies, round smooth galaxies, in-between round smooth galaxies, cigar shaped smooth galaxies, barred spiral galaxies, unbarred tight spiral galaxies, unbarred loose spiral galaxies, edge-on galaxies without bulge, edge-on galaxies with bulge. The dataset is imbalanced, in that not all classes have a similar number of images. In particular, there are only 334 labeled images present for "cigar shaped smooth galaxies", so during training, we use stratified sampling. The size of this dataset also allows us to explore training techniques and develop approaches that work well in the low-data regime and generalize well.

The data collected for the DECals campaign uses Dark Energy Camera \cite{Flaugher_2015} on the Blanco m telescope and covers both the North Galactic Cap region at Dec  and the South Galactic Cap region at Dec . The DECals survey utilizes a method of tiling the sky that involves three separate passes. These passes are slightly shifted in relation to one another, with an approximate offset range of . The specific pass and duration of exposure for each observation are determined in real-time based on multiple factors allowing for a nearly uniform level of depth across the survey. The Beijing-Arizona Sky Survey covers  square degrees of the Northern Galactic Cap, using Steward Observatory's m Bok Telescope. The Mayall -band Legacy Survey imaged the Dec  region using the -m Mayall telescope \citep{dey2019overview}.

we also perform our experiments on the Tiny ImageNet \citep{le2015tiny}, CIFAR-100 \citep{krizhevsky2009learning},a dn CIFAR-10 \citep{krizhevsky2009learning}. These datasets are very popularly used benchmarks for image classification in the low-data regime. Tiny ImageNet contains  images of  classes ( for each class) downsized to  colored images which are a subset of the ImageNet dataset \citep{deng2009imagenet}. Each class has  training images,  validation images, and  test images. The CIFAR-10 dataset consists of   color images in  classes, with  images per class. The CIFAR-100 dataset is just like the CIFAR-10, except it has  classes containing  images each.

\section{Related Work}
\label{Related Work}

Classifying galaxy morphologies on the Galxy10 DECals is a rather well-established task and there have been multiple works in the past by \cite{venn2019lrp2020, blancato2020decoding, chen2021classifying, 10.1007/978-3-031-23092-9_1, Hui_2022, 9990776, holandainfluence, https://doi.org/10.48550/arxiv.2211.00677, https://doi.org/10.48550/arxiv.2210.05484} have employed multiple techniques for this task. However, no work has explored using hybrid models for this task which allowed us to set a new state-of-the-art for Galaxy10 DECals.

Transformer-convolutional hybrids are a recent innovation and in the past multiple models have proposed different approaches to constructing such hybrids namely: MobileViT \citep{mehta2022mobilevit}, ResNet-ViT \citep{dosovitskiy2021an}, ConViT \citep{d2021convit}, PatchConvNet \citep{https://doi.org/10.48550/arxiv.2112.13692}, and CoAtNet \citep{dai2021coatnet}. However, they do not explore the performance or modification that could be made to these methods to perform well for lower amounts of data with training from scratch. The work by \cite{https://doi.org/10.48550/arxiv.2210.07240} extensively explores training ViTs in the low-data regime and have shown success on small datasets. Their work based on learning self-supervised inductive biases from small-scale datasets use these biases as a weight initialization scheme for fine-tuning. The work by \cite{https://doi.org/10.48550/arxiv.2112.13492} explores modifying ViTs to learn locality inductive bias. In this paper, we explore training in low-data regimes with hybrid models and have motivated the use of hybrid models in the low-data regime, unlike these works.

\section{Methodology}
\label{Methodology}

We develop a variant of the CoAtNet \citep{dai2021coatnet} model using a different stack design and careful selection of augmentation and regularization techniques. The Transformer block makes use of relative attention which efficiently combines depthwise convolutions \citep{sandler2018mobilenetv2} and self-attention \citep{vaswani2017attention}. A depthwise convolution uses a fixed kernel to extract features from a local region of the input data whereas self-attention  allows the receptive field to be the global spatial space. Relative attention allows us to combine convolutions and self-attention.



where ,  are the input and output at position ,  represents the depthwise convolution kernel and  represents the global spatial space. Here, the attention weight  is decided by both  and . The update made to the attention weight is rather intuitive by simply summing a global static convolution kernel



To construct a network that uses relative attention, we adopt an approach similar to CoAtNets by first down-sampling the feature map via a multi-stage network with gradual pooling to reduce the spatial size and then employing the global relative attention. To do so, CoAtNets propose using a network of 5 stages (\texttt{S0}, \texttt{S1}, \texttt{S2}, \texttt{S3}, \texttt{S4}) where \texttt{S0} is a simple 2-layer convolutional Stem and \texttt{S1} employs Inverted Residual blocks \citep{sandler2018mobilenetv2} with squeeze-excitation. In the work by \cite{dai2021coatnet} they eliminate the possibility of using a C-C-C-T stack i.e. \texttt{S1}, \texttt{S2}, and \texttt{S3} employ Inverted Residual blocks \citep{sandler2018mobilenetv2} and \texttt{S4} employs a Transformer block, due to supposedly low model performance. However, in our experiments, we find that a C-C-C-T design works much better than C-C-T-T which was adapted as the layout for CoAtNet. This is precisely due to \textbf{(1)} higher generalization capability of C-C-C-T and \textbf{(2)} highly unstable training of C-C-T-T and C-T-T-T architectures. These proposed architectural choices are summarized in Figure \ref{fig:architecture}.

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{architecture}
    \caption{Overview of the proposed model, notice the stack-design design we employ.}
    \label{fig:architecture}
\end{figure}

To evaluate the benefits of partially-occluded augmentation methods like CutMix \citep{yun2019cutmix}, DropBlock \citep{ghiasi2018dropblock}, and Cutout \citep{https://doi.org/10.48550/arxiv.1708.04552} hold for this task we run some experiments. We find that these regional dropout-based augmentation techniques have a strong detrimental effect on datasets such as the one we use \footnote{We also explore Attentive CutMix \citep{https://doi.org/10.48550/arxiv.2003.13048} and though it identifies the most discriminative regions based on the intermediate attention maps from a feature extractor, we do not observe a significantly higher generalization capability.} like the Galaxy10 DECals, this is mainly due to the nature of the task, one example we observe is that even some minor augmentations on an image with the class "edge-on galaxies with bulge" could cause the ground-truth (as identified by a human) of the augmented image to shift to the class "edge-on galaxies without bulge" \footnote{More information on how galaxy morphologies are manually classified can be found at \url{https://data.galaxyzoo.org/gz_trees/gz_trees.html}} which is detrimental to the model performance. This example is visually in Appendix \ref{Supplemental Figures}, Figure \ref{fig:visualizeaugmentations}.

Finally, for augmentation, we consider a combination of Mixup \citep{zhang2017mixup} and RandAugment \citep{cubuk2020randaugment}. As for regularization strategies, we make use of stochastic depth regularization, weight decay, and label smoothing. The hyperparameters values for these regularizations are listed in Appendix \ref{Implementation Details}. Surprisingly, we find that strong augmentations techniques give much higher performance gains than stronger regularization. Overall we believe, that judiciously choosing augmentation and regularization strategies is crucial to model performance in low-data regimes and careful selection of the associated hyperparameters for augmentation and regularization is equally important.

 In brief, the reasons these models perform so well on low-data regime tasks, even when trained from scratch, and we believe it would work for other tasks in the low-data regime are: \textbf{(1)} Careful selection of augmentation and regularization is very important, especially in smaller datasets. \textbf{(2)} Our approach to train a hybrid transformer-convolutional model shows great generalizability and does not face the problem of highly unstable training and \textbf{(3)} The inherent translational equivalence helps  make the training less prone to overfitting in the low-data regime, more experiments on this front were done by \cite{https://doi.org/10.48550/arxiv.2004.09691}. We postpone the proofs for this to Appendix \ref{proofs}.

\section{Results}
\label{Results}

\begin{table}[ht]
    \caption{Model Performance on Galaxy10 DECals dataset. \texttt{Galaxy10 only} denotes training on the Galaxy10 DECals dataset only; \texttt{Galaxy10 + Zoo} denotes the use of extra unlabeled data from GalaxyZoo, though the models that use extra unlabeled data are not the focus of this work, we present those results as well. The rows under the horizontal line in this table represent experiments performed in this work.}
    \centering
    \begin{adjustbox}{width=\textwidth,center}
    \begin{tabular}{lcc}
    \toprule
    \textbf{Method Description} & \multicolumn{2}{c}{\textbf{Galaxy10 DECals top-1 accuracy()}}\\
    & Galaxy10 only & Galaxy10 + Zoo\\
    \midrule
    Random Baseline & 14.77 & -\\
    Fractal Analysis \citep{9990776} & 73.45 & -\\
    Architectural Optimization Over Subgroups \citep{https://doi.org/10.48550/arxiv.2210.05484} & 77.00 & -\\
    DeepAstroUDA \citep{https://doi.org/10.48550/arxiv.2211.00677} & 79.00 & -\\
    Deep Galaxies CNN \citep{10.1007/978-3-031-23092-9_1} & 84.04 & -\\
    EfficientNet \citep{tan2019efficientnet} & 86.00 & - \\
    Luma \citep{holandainfluence} & 86.20 & -\\
    DenseNet 121 \citep{iandola2014densenet} & 88.64 & - \\
    EfficientNetv2 \citep{tan2021efficientnetv2} & 90.24 & -\\
    \midrule
    Noisy Student \citep{xie2020self} & - & \textbf{91.80}\\
    SimCLRv2 \citep{NEURIPS2020_fcbc95cc} & - & 90.24\\
    Standard CoAtNet-4 \citep{dai2021coatnet} & 81.55 & -\\
    \textbf{Ours (Astroformer)} & \textbf{94.86} & -\\
    \bottomrule
    \end{tabular}
    \end{adjustbox}
    \label{tab:results}
\end{table}

\begin{figure}[ht]
  \centering
  \begin{subfigure}[t]{0.23\textwidth}
    \centering
    \includegraphics[width=\textwidth]{example_1}
    \caption{}
    \label{fig:example1}
  \end{subfigure}
  \begin{subfigure}[t]{0.23\textwidth}
    \centering
    \includegraphics[width=\textwidth]{example_2}
    \caption{}
    \label{fig:example2}
  \end{subfigure}
  \begin{subfigure}[t]{0.23\textwidth}
    \centering
    \includegraphics[width=\textwidth]{example_3}
    \caption{}
    \label{fig:example3}
  \end{subfigure}
  \begin{subfigure}[t]{0.23\textwidth}
    \centering
    \includegraphics[width=\textwidth]{example_4}
    \caption{}
    \label{fig:example4}
  \end{subfigure}
  \caption{Random selection of ground truth comparison against our model predictions on the hold-out test set. We also include one example where the model predicted incorrectly (c). The acronyms "GT" and "P" in this figure refer to the ground truth and predicted labels.}
  \label{fig:examples}
\end{figure}

We report all results and compare them against previous models and a random baseline (equivalent to making a guess) in Table \ref{tab:results}. We also provide a few random ground truth comparisons to our predictions in Figure \ref{fig:examples}. The performance of models is calculated using the metrics that are typical for an image classification problem, top-1 accuracy. Additionally, we find that using any of the other larger variants of CoAtNet is detrimental for this task and the model starts heavily overfitting even with our design changes, augmentation, and regularization. We also explore semi-supervised learning for this task, our experiments with semi-supervised learning techniques included applying Noisy Student \citep{xie2020self}, SimCLRv2 \citep{NEURIPS2020_fcbc95cc}, and Meta Pseudo Labels \citep{Pham_2021_CVPR} to the Galaxy10 DECals dataset as well as using unlabeled images from the GalaxyZoo Dataset though. These results are also summarized in Table \ref{tab:results}.

\section{Discussion}
\label{Discussion}

In this paper, we developed a supervised model to train from scratch on the Galaxy10 DECals dataset and explored other methods to train from scratch in the low-data regime. We train a CoAtNet model and pair it with a careful selection of augmentation and regularization strategies as well as use a different stack design which was earlier thought to have severely low model performance and a novel way of creating relative self-attention layers on top of the CoAtNet model \citep{dai2021coatnet}. We apply this proposed model in the low-data regime and achieve state-of-the-art performance on the Galaxy10 DECals dataset. Furthermore, with this approach, we also establish new state-of-the-art without using extra training data on popular low-data regime image classification datasets, CIFAR-100 \citep{krizhevsky2009learning} and Tiny ImageNet \citep{le2015tiny}, and competitive results on CIFAR-10 \citep{krizhevsky2009learning} as indicated in Appendix \ref{other datasets}. From the perspective of science objectives, we believe this model will enable more precise studies of galaxy morphology. In the future, we hope to see more methods for training models with low data from scratch and we believe our model is a potential choice for a variety of other low-data regime tasks. We also hope that this model could potentially be used as a backbone for other vision tasks in the low-data regime.

\section*{Acknowledgements}

The authors would like to thank Google for supporting this work by providing Google Cloud credits. The authors would also like to thank Google TPU Research Cloud (TRC) program \footnote{https://sites.research.google/trc/about/} for providing access to TPUs.

We thank David Lindell of the University of Toronto for insightful conversations. We thank Caleb Lammers, Jo Bovy, and Henry Leung of the University of Toronto for insightful discussions and their domain expertise in the area of galaxy morphologies and evolution.

\bibliography{iclr2023_conference}
\bibliographystyle{iclr2023_conference}

\newpage
\appendix
\section{Appendix}

\subsection{Extended Related Work}
\label{extended related work}

\paragraph{Self-Attention for Vision.} Self Attention has extensively been applied to various computer vision tasks, such as image classification, object detection, and semantic segmentation. The idea of self-attention was first introduced by \cite{vaswani2017attention}, where they proposed the Transformer model for neural machine translation. The Transformer model consists of an encoder and a decoder, each composed of multiple layers of self-attention and feed-forward networks. The self-attention mechanism can be seen as a generalization of the attention mechanism that was previously used in conjunction with RNNs for sequence modeling. The application of self-attention and transformers to computer vision tasks faces some challenges due to the high-resolution and spatial structure of visual data. One challenge is the quadratic computational complexity of self-attention, which limits its scalability to large inputs. Another challenge is the lack of local information in self-attention, which may hinder its ability to capture fine-grained details and local patterns in images. To address these challenges, several variants and extensions of self-attention and transformers have been proposed for computer vision tasks. For example, \cite{dosovitskiy2021an} proposed Vision Transformer (ViT), which applies a vanilla Transformer to image classification by dividing an image into patches and treating them as tokens.

These are some examples of how self-attention and transformers can be adapted and improved for computer vision tasks. For a more comprehensive review of vision transformers, we refer readers to the dedicated surveys by \cite{10.1145/3505244} and \cite{9716741}.

\paragraph{Hybrid Models.} Hybrid convolution-attention models are a recent trend in computer vision that aim to combine the advantages of CNNs and self-attention mechanisms. CNNs are known for their ability to capture local features and spatial invariance, while self-attention can model long-range dependencies and global context. However, CNNs and self-attention have different strengths and weaknesses, and finding the optimal balance between them is not trivial.

One approach to hybridize convolution and attention is to augment the CNN backbone with explicit self-attention or non-local modules \citep{Wang_2018_CVPR}, or to replace certain convolution layers with standard self-attention \citep{Bello_2019_ICCV} or a more flexible mix of linear attention and convolution. These methods often improve the accuracy of CNNs, but they also increase the computational cost and complexity. Moreover, they do not exploit the natural connection between depthwise convolution and relative attention, which can be fused together to form a more efficient and effective hybrid layer \citep{dai2021coatnet}. Another approach to hybridize convolution and attention is to start with a Transformer backbone and try to incorporate explicit convolution or some desirable properties of convolution into the Transformer layers \citep{Liu_2021_ICCV}. These methods aim to overcome the limitations of vanilla Transformers, such as the lack of inductive biases, the quadratic complexity, and the dependence on large-scale pre-training. However, they also face challenges such as how to design convolutional embeddings, how to integrate convolutional operations into the attention mechanism, and how to balance the trade-off between model capacity and generalization.

A recent work that proposes a novel hybrid convolution-attention model is CoAtNet \citep{dai2021coatnet}, which is based on two key insights: (1) by using simple relative attention, depthwise convolution, and self-attention can be naturally fused together to form a hybrid layer called CoAt layer; (2) by stacking CoAt layers and standard convolution layers in a principled manner, generalization, capacity, and efficiency can be dramatically improved.

\subsection{Model Details}
\label{proofs}

\paragraph{Inherent Translation Equivariance.}
\begin{theorem}
A variant of relative attention:

where ,  are the input and output at position ,  represents the depthwise convolution kernel and  represents the global spatial space preserves translational equivariance.
\end{theorem}
\begin{proof}
Analyzing the groups for which the version of relative attention shown in Equation \ref{eq:relativeattentionthm} is equivariant, we will just focus our efforts on proving equivariance for the translational group. To do so, we need to show that if we translate the input  and  by the same vector , the output  will also be translated by the same vector .

Informally we can understand this proof as identifying that the depthwise convolution kernel for any position pair  is only dependent on , the relative positions rather than the values of  and  individually.

We will also use the well-known result that convolutions enjoy translational equivariance, this also aligns well with the general nature of tasks in vision and also partly helps generalize the model to different positions or to images of different sizes.

Formally we can define , to be the translation operator that shifts the input sequence  by  positions, that is, . Similarly, let  denote the output of the relative attention mechanism when the input is shifted by  positions. We then want to show that  for all . That is a shift in the input sequence results in an equivalent shift in the output sequence. Equivalently for this proof, we use the simpler notation where we let  and  be the translated input vectors. We want to show that .

Substituting the translated input vectors into Equation \ref{eq:relativeattentionthm}, we have:



Expanding the dot product, we get:



Substituting this back into Equation \ref{eq:relativeattention_translate}, we get:



Thus, we can write this as:



Notice that the term inside the sum on the right-hand side of Equation \ref{eq:relativeattention_translation_result} is equivalent to the attention weight between  and . Therefore, we can rewrite the sum as:



where  and  are the relative positions between  and  and , respectively.

Therefore, we can rewrite Equation \ref{eq:relativeattention_translation_result} as:



which shows that the output  is translated by the same vector  as the input  and , and therefore the relative attention in Equation \ref{eq:relativeattentionthm} enjoys the property of translational equivariance.
\end{proof}

\begin{theorem}
The variant of relative attention described in Equation \ref{eq:relativeattentionthm} has the ability to do input-adaptive weighting.
\end{theorem}
\begin{proof}
To show this, in this proof, we will show that the form in Equation \ref{eq:relativeattentionthm} that the attention weight  is influenced by the input  in a way that adapts to the input.

We can rewrite the numerator of the attention weight as:



Since the depthwise convolution kernel  is fixed for all inputs, it does not adapt to the input. Therefore, the input-adaptive property of the attention weight must come from the term .

Now, we can write the denominator of the attention weight as:



Again, the term  is fixed and does not adapt to the input. Therefore, we only need to focus on the term  to see if it adapts to the input.



Now, we can see that the term  represents the similarity between the input vectors  and  relative to the similarity between  and . This relative similarity term ensures that the attention weight adapts to the input, as it depends on the relationship between the input vectors rather than their absolute values.
\end{proof}

\paragraph{Pre-Activation.} We follow the same pre-activation structure as demonstrated by \cite{dai2021coatnet} for both the Inverted Residual blocks and Transformer blocks:



where \texttt{Module} denotes the Inverted Residual, Self-Attention, or FFN module, while \texttt{Norm} corresponds to \texttt{BatchNorm} for Inverted Residual block and \texttt{LayerNorm} for Self-Attention and FFN.

\paragraph{Down-Sampling.} We follow the same down-sampling structure as demonstrated by \cite{dai2021coatnet}. For the first block inside each stage from S1 to S4, down-sampling is performed independently for the residual branch and the identity branch. The down-sampling self-attention module can is expressed as:



As for the Inverted Residual block, the down-sampling in the residual branch is instead achieved by using a stride-2 convolution to the normalized inputs



\subsection{Model Performance on other low-data regime datasets}
\label{other datasets}

In this section, we explore applying the proposed model to other low-data regime image classification tasks namely CIFAR-100 \citep{krizhevsky2009learning}, Tiny ImageNet \citep{le2015tiny}, and CIFAR-10 \citep{krizhevsky2009learning} without extra training data. Similar to the experiments for Galaxy10 DECals, the hyperparameters for these tasks were also found through a naive hyperparameter search, however, any of the models trained on these datasets do not use stratified sampling. In Table \ref{tab:cifar100results} we report our results for CIFAR-100, in Table \ref{tab:tiresults} for Tiny ImageNet, and in Table \ref{tab:cifar10results} for CIFAR-10 datasets.

\begin{table}[ht]
    \caption{Model Performance on the CIFAR-100 dataset. We only present models that do not use extra unlabeled data which can be compared with these experiments. We notice that even without using extra training data our approach beats multiple well-established models which use extra-training data.}
    \centering
    \resizebox{\textwidth}{!}{\begin{tabular}{lcc}
        \toprule
        \textbf{Method Description} & \textbf{CIFAR-100 top-1 accuracy()} & \textbf{Extra Training Data}\\
        \midrule
        EfficientNetV2 \citep{tan2021efficientnetv2} & 92.30 & \cmark\\
        TResNet \citep{ridnik2021tresnet} & 93.00 & \cmark\\
        CaiT \citep{touvron2021going} & 93.10 & \cmark\\
        µ2Net \citep{https://doi.org/10.48550/arxiv.2205.12755} & 94.95 & \cmark\\
        ML-Decoder \citep{ridnik2023ml} & 95.10 & \cmark\\
        EffNet-L2 (SAM) \citep{chen2021vision} & \textbf{96.08} & \cmark\\
        \midrule
        CoAtNet-5 \citep{dai2021coatnet} & 81.21 & \xmark\\
        CoAtNet-4 \citep{dai2021coatnet} & 84.60 & \xmark\\
        WRN \citep{Zhao_2022} & 86.90 & \xmark\\
        DenseNet \citep{iandola2014densenet} & 87.44 & \xmark\\
        ColorNet \citep{gowda2019colornet} & 88.40 & \xmark\\
        ShakeDrop \citep{cubuk2018autoaugment} & 89.30 & \xmark\\
        PyramidNet \citep{zhao2022toward} & 89.90 & \xmark\\        
        \textbf{Ours (Astroformer)} & \textbf{93.36} & \xmark\\
        \bottomrule
    \end{tabular}}
    \label{tab:cifar100results}
\end{table}

\begin{table}[ht]
    \caption{Model Performance on the Tiny ImageNet dataset. We only present models that do not use extra unlabeled data which can be compared with these experiments. We notice that even without using extra training data our approach beats multiple well-established models which use extra-training data.}
    \centering
    \begin{tabular}{lcc}
        \toprule
        \textbf{Method Description} & \textbf{TI top-1 accuracy()} & \textbf{Extra Training Data}\\
        \midrule
        EfficientNet (DCL) \citep{luo2019direction} & 84.39 & \cmark\\
        ViT (PUGD) \citep{tseng2022perturbed} & 90.74 & \cmark\\
        DeiT (PUGD) \citep{tseng2022perturbed} & 91.02 & \cmark\\
        Swin-L \citep{huynh2022vision} & \textbf{91.35} & \cmark\\
        \midrule
        PreActResNet \citep{rame2021mixmo} & 70.24 & \xmark\\
        ResNeXt-50 (SAMix+DM) \citep{https://doi.org/10.48550/arxiv.2203.10761} & 72.39 & \xmark\\
        Context-Aware Pipeline \citep{yao2021context} & 73.60 & \xmark\\
        WaveMixLite \citep{viswanathanwavemix} & 77.47 & \xmark\\
        DeiT \citep{https://doi.org/10.48550/arxiv.2210.00471} & 92.00 & \xmark\\
        \textbf{Ours (Astroformer)} & \textbf{92.98} & \xmark\\
        \bottomrule
    \end{tabular}
    \label{tab:tiresults}
\end{table}

\begin{table}[ht]
    \caption{Model Performance on the CIFAR-10 dataset. We only present models that do not use extra unlabeled data which can be compared with these experiments. We notice that even without using extra training data our approach beats multiple well-established models which use extra-training data.}
    \centering
    \resizebox{\textwidth}{!}{\begin{tabular}{lcc}
        \toprule
        \textbf{Method Description} & \textbf{CIFAR-10 top-1 accuracy()} & \textbf{Extra Training Data}\\
        \midrule
        CeiT \citep{Yuan_2021_ICCV} & 99.10 & \cmark\\
        ViT (PUGD) \citep{tseng2022perturbed} & 99.13 & \cmark\\
        BIT-L \citep{10.1007/978-3-030-58558-7_29} & 99.37 & \cmark\\
        CaiT \citep{touvron2021going} & 98.40 & \cmark\\ 
        CvT \citep{Wu_2021_ICCV} & \textbf{98.39} & \cmark\\
        \midrule
        ViT-SAM \citep{chen2021vision} & 98.60 & \xmark\\
        PyramidNet \citep{zhao2022toward} & 98.71 & \xmark\\
        LaNet \citep{wang2021sample} & 99.03 & \xmark\\
        \textbf{Ours (Astroformer)} & 99.12 & \xmark\\
        µ2Net \citep{https://doi.org/10.48550/arxiv.2205.12755} & 99.49 & \xmark\\
        ViT \citep{dosovitskiy2021an} & \textbf{99.50} & \xmark\\
        \bottomrule
    \end{tabular}}
    \label{tab:cifar10results}
\end{table}

\subsection{Implementation Details}
\label{Implementation Details}

In this section, we explain the implementation details of the experiments and our proposed model.

\paragraph{Sampling.} There is a class imbalance in the dataset, meaning that not all classes have a similar number of images. For this reason, we follow a stratified sampling strategy during data loading to ensure each batch contains  instances of each label class.

\paragraph{Model backbone. } Throughout this work we use CoAtNet as the model backbone due to their success in efficiently unifying depthwise convolutions and self-attention through relative attention and their approach of vertically stacking convolution layers and attention layers. Since the Galaxy10 DECals dataset does not contain a large amount of data, other transformer-based models did not show great results for this task whereas a hybrid model with regularization and augmentation techniques, was able to generalize well and achieved better results.

\paragraph{Baseline model. } We established a naive baseline with random guesses. The baseline model we chose was training CoAtNet-4 without any design modifications. This gets to a top-1 accuracy of 81.55\% on the Galaxy10 DECals dataset. To train this model, we employ standard augmentations (MixUp and CutMix) and train the network for 300 epochs using the default settings in \texttt{timm} with a batch size of 256.

\paragraph{Loss function.} We adopt the standard cross entropy loss with smoothing. We also performed some preliminary experiments using Dense Relative Localization Loss \citep{liu2021efficient} and we believe this might be a potentially promising direction as well.

\paragraph{Code. } Our code is in PyTorch 1.10 \citep{paszke2019pytorch}. We use a number of open-source packages to develop our training workflows. Most of our experiments and models were trained with \texttt{timm} \citep{rw2019timm} and we also use \texttt{mmclassify} \citep{2020mmclassification} for some of the experiments. Our hardware setup for the experiments included either four NVIDIA Tesla V100 GPUs or a TPUv3-8 cluster. We utilized mixed-precision training with PyTorch's native AMP (through \texttt{torch.cuda.amp}) for mixed-precision training and a distributed training setup (through \texttt{torch.distributed.launch}) which allowed us to obtain significant boosts in the overall model training time. We report the number of parameters and FLOPS of the final model in Table \ref{tab:model_comparison_sorted}.

\paragraph{Hyperparameters. } The choice of hyperparameters for training the Astroformer model is shown in Table \ref{tab:hyperparams}. The rest of the hyperparameters were kept to their defaults as provided in \texttt{timm} \citep{rw2019timm}. The hyperparameters related to Lookahead \citep{NEURIPS2019_90fd4f88} are used at their default values as suggested.

\begin{table}[!ht]
    \caption{Hyper-parameters used to train Astroformer for Galaxy10 DECals.}
    \centering
    \begin{tabular}{ll}
        \toprule
        \textbf{Hyper-parameter} & \textbf{Values} \\
        \midrule
        Stochastic depth rate & 0.2\\
        Center crop & False\\
        Mixup Alpha & 0.8\\
        Train epochs & 300\\
        Label smoothing & 0.1 (\texttt{timm} default)\\
        Train batch size & 256\\
        Optimizer type & Lookahead \citep{NEURIPS2019_90fd4f88} \\ & \quad + RAdam \citep{Liu2020On}\\
        LR decay schedule & Cosine\\
        Base learning rate & 2e-5\\
        Warmup learning rate & 1e-5 (\texttt{timm} default)\\
        Warmup & 5 epochs\\
        Weight decay rate & 1e-2\\
        Gradient clip & None\\
        EMA decay rate & None\\
        RandAugment layers & 2\\
        \bottomrule
    \end{tabular}
    \label{tab:hyperparams}
\end{table}

\subsection{Full Tabular Results}

We provide Tables \ref{tab:model_comparison_sorted}-\ref{tab:cifar10scaling} that provides all the numerical results.

\begin{table}[ht]
\begin{minipage}[t]{0.62\textwidth}
\centering
\caption{Sorted table based on the number of parameters in each family for top-1 accuracy and scaling curves of multiple models on the Galaxy10 DECals dataset shown in Figure \ref{fig:image_ab}.}
\label{tab:model_comparison_sorted}
\resizebox{\columnwidth}{!}{
\begin{tabular}{crrr}
\toprule
\textbf{Model family} & \textbf{Params (M)} & \textbf{FLOPs (G)} & \textbf{Top-1 Accuracy ()} \\ \midrule
\multirow{3}{*}{\textbf{Astroformer}} & 655.04 & 115.97 & 75.27 \\
& 271.54 & 60.54 & 94.86 \\
& 161.75 & 31.36 & 92.39 \\ \midrule
\multirow{3}{*}{\textbf{SwinV2}} & 274.06 & 81.55 & 81.55 \\
& 195.16 & 35.09 & 91.23 \\
& 86.86 & 15.86 & 84.58 \\ \midrule
\multirow{6}{*}{\textbf{ViT}} & 1011.22 & 267.18 & 78.27 \\
& 630.78 & 167.40 & 78.46 \\
& 303.31 & 61.60 & 80.25 \\
& 87.46 & 4.41 & 84.36 \\
& 85.81 & 17.58 & 84.35 \\
& 5.53 & 1.26 & 74.21 \\ \midrule
\multirow{3}{*}{\textbf{EfficientNet v2}} & 117.25 & 12.40 & 90.12 \\
& 52.87 & 5.46 & 85.67 \\
& 20.19 & 2.91 & 84.37 \\ \midrule
\multirow{3}{*}{\textbf{CoAtNet}} & 163.08 & 36.69 & 82.39 \\
& 72.62 & 16.58 & 87.45 \\
& 40.87 & 8.76 & 85.24 \\ \midrule
\multirow{3}{*}{\textbf{Swin}} & 195.01 & 34.53 & 93.21 \\
& 86.75 & 15.47 & 82.38 \\
& 23.68 & 4.63 & 81.57 \\ \bottomrule
\end{tabular}}
\end{minipage}
\hfill
\begin{minipage}[t]{0.32\textwidth}
\centering
\caption{Contribution to the error rate for Astroformer-4 on the Galaxy10 Dataset according to the classes in the test set.}
\label{tab:galaxy10error}
\resizebox{\columnwidth}{!}{
\begin{tabular}{rr}
     \toprule
     \textbf{Class} & \textbf{Contribution to}\\
     & \textbf{error rate (\%)}\\
     \midrule
     0 & 0.31\\
     1 & 0.23\\
     2 & 1.28\\
     3 &0.3\\
     4 &1.02\\
     5 &0.14\\
     6 &0.31\\
     7 &0.12\\
     8 &1.16\\
     9 &0.27\\
     \midrule
     &5.14\\
     \bottomrule
\end{tabular}}
\end{minipage}
\end{table}

\begin{table}[ht]
\centering
\caption{Sorted table based on the number of parameters in each family for top-1 accuracy and scaling curves of multiple models on the CIAFR-100 shown in Figure \ref{fig:cifar100scalingfig}.}
\label{tab:cifar100scaling}
\begin{tabularx}{\textwidth}{>{\centering\arraybackslash}X*{3}{>{\raggedleft\arraybackslash}X}}
\toprule
\textbf{Model family} & \textbf{Params (M)} & \textbf{FLOPs (G)} & \textbf{Top-1 Accuracy ()} \\ \midrule
\multirow{3}{*}{\textbf{EfficientNetV2}} & 117.36 & 12.4 & 92.3 \\
& 52.98 & 5.45 & 92.2 \\
& 20.3 & 2.91 & 91.5 \\ \midrule
\multirow{4}{*}{\textbf{ViT}} & 305.61 & 15.38 & 74.26 \\
& 303.4 & 61.6 & 75.28 \\
& 87.53 & 4.41 & 86.3 \\
& 85.87 & 17.58 & 87.1 \\ \midrule
\multirow{5}{*}{\textbf{EfficientNet}} & 84.87 & 7.21 & 92.32 \\
& 64.04 & 5.36 & 91.7 \\
& 40.96 & 3.5 & 89.96 \\
& 28.54 & 2.47 & 88.76 \\
& 17.72 & 1.58 & 88.72 \\ \midrule
\multirow{3}{*}{\textbf{CoAtNet}} & 271.68 & 62.65 & 92.13 \\
& 163.22 & 36.68 & 89.45 \\
& 72.71 & 16.58 & 87.9 \\ \midrule
\multirow{3}{*}{\textbf{Astroformer}} & 655.34 & 115.97 & 89.38 \\
& 271.68 & 60.54 & 93.36 \\
& 161.95 & 31.36 & 87.65 \\ \bottomrule
\end{tabularx}
\end{table}

\begin{table}[ht]
\centering
\caption{Sorted table based on the number of parameters in each family for top-1 accuracy and scaling curves of multiple models on the CIAFR-10 shown in Figure \ref{fig:cifarscaling}.}
\label{tab:cifar10scaling}
\begin{tabularx}{\textwidth}{>{\centering\arraybackslash}X*{3}{>{\raggedleft\arraybackslash}X}}
\toprule
\textbf{Model family} & \textbf{Params (M)} & \textbf{FLOPs (G)} & \textbf{Top-1 Accuracy ()} \\ \midrule
\multirow{3}{*}{\textbf{EfficientNetV2}} & 117.25 & 12.40 & 99.1 \\
& 52.87 & 5.46 & 99.0 \\
& 20.19 & 2.91 & 98.7 \\ \midrule
\multirow{4}{*}{\textbf{ViT}} & 305.52 & 15.39 & 77.8 \\
& 303.31 & 61.60 & 76.5 \\
& 85.81 & 17.58 & 74.9 \\
& 87.46 & 4.41 & 73.4 \\ \midrule
\multirow{5}{*}{\textbf{EfficientNet}} & 63.81 & 5.37 & 99.0 \\
& 40.76 & 3.51 & 97.23 \\
& 28.36 & 2.47 & 95.43 \\
& 17.57 & 1.59 & 93.89 \\
& 10.71 & 1.03 & 93.45 \\ \midrule
\multirow{3}{*}{\textbf{CoAtNet}} & 72.62 & 16.58 & 92.17 \\
& 163.08 & 36.69 & 91.43 \\
& 271.54 & 62.65 & 91.34 \\ \midrule
\multirow{3}{*}{\textbf{Astroformer}} & 161.75 & 31.36 & 99.12 \\
& 271.54 & 60.54 & 98.93 \\
& 655.04 & 115.97 & 93.23 \\ \bottomrule
\end{tabularx}
\end{table}

\subsection{Supplemental Figures}
\label{Supplemental Figures}

We provide Figures \ref{fig:image_ab}-\ref{fig:examples} that provides supplementary images.

\begin{figure}[ht]
    \centering
    \begin{tabular}{cc}
        \includegraphics[width=0.5\textwidth]{galaxy10params} & 
        \includegraphics[width=0.5\textwidth]{galaxy10flops} \\
        (a) & (b)
    \end{tabular}
    \caption{(a) The top-1 accuracy to parameter scaling curves for multiple models on the Galaxy10 DECals dataset. (b) The top-1 accuracy to FLOPs scaling curves for multiple models on the Galaxy10 DECals dataset. All these scaling curves are for the evaluation size of . The data for this graph can be found in Table \ref{tab:model_comparison_sorted}}
    \label{fig:image_ab}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{tabular}{cc}
        \includegraphics[width=0.5\textwidth]{cifar100params} & 
        \includegraphics[width=0.5\textwidth]{cifar100flops} \\
        (a) & (b)
    \end{tabular}
    \caption{(a) The top-1 accuracy to parameter scaling curves for multiple models on the CIFAR-100 dataset. (b) The top-1 accuracy to FLOPs scaling curves for multiple models on the CIFAR-100 dataset. All these scaling curves are for the evaluation size of . The data for this graph can be found in Table \ref{tab:cifar100scaling}}
    \label{fig:cifar100scalingfig}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{tabular}{cc}
        \includegraphics[width=0.5\textwidth]{cifar10params} & 
        \includegraphics[width=0.5\textwidth]{cifar10flops} \\
        (a) & (b)
    \end{tabular}
    \caption{(a) The top-1 accuracy to parameter scaling curves for multiple models on the CIFAR-10 dataset. (b) The top-1 accuracy to FLOPs scaling curves for multiple models on the CIFAR-10 dataset. All these scaling curves are for the evaluation size of . The data for this graph can be found in Table \ref{tab:cifar10scaling}}
    \label{fig:cifarscaling}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{visualize_augmentations}
    \caption{Random selection of augmented images after applying regional dropout-based augmentation techniques. We note the visible differences in the galaxy morphologies as well as observe visually why partial-occluded augmentation techniques do not work well for this dataset.}
    \label{fig:visualizeaugmentations}
\end{figure}

\newpage
\begin{figure}[ht]
    \centering
    \includegraphics[height=0.95\textheight]{raw_images}
    \caption{Raw data (galaxy images and labels) from the Galaxy10 DECals dataset. All images are a random sample from the training set.}
    \label{fig:rawimages}
\end{figure}

\end{document}
