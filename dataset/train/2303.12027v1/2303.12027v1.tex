\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}

\subsection{Experimental setup}
\noindent{\textbf{Implementation details.}}
We use Swin-B~\cite{SWIN} pre-trained on ImageNet~\cite{imagenet} as our vision encoder and use the base-uncased version of BERT~\cite{devlin2018bert} as our language encoder. 
For the vision input, the test image for grounding is resized such that its long edge is 320. No cropping or rotating is used to avoid breaking the alignment between the language and vision signal. The sizes of the template image and search image are set to $128\times128$ and $320\times320$, respectively. 
The size of historical target features for temporal modeling is set to $6\times 6$.
For the language input, the max length of the language is set to 40, including a CLS and a SEP token. 

We use the training splits of RefCOCOg-google~\cite{REFCOCO}, OTB99~\cite{li2017tracking}, LaSOT~\cite{LASOT}, and TNL2K~\cite{TNL2K} to train our model. 
We train our model for 300 epochs, and the warm-up strategy is adopted. 
The learning rates for the vision and language encoders and the other parameters first increase to $10^{-5}$ and $10^{-4}$ linearly at the first 30 epochs, respectively. 
The learning rates drop by 10 after the $200^{th}$ epoch every 50 epochs. 
We test the proposed algorithm on an NVIDIA 3090 GPU, and the tracking speed is about 39 FPS.

\noindent{\textbf{Datasets and metrics.}}
We evaluate our method for natural language tracking on the tracking benchmarks with natural language descriptions, including OTB99~\cite{li2017tracking}, LaSOT\cite{LASOT}, TNL2K~\cite{TNL2K}. 
All these datasets adopt success and precision to measure tracking performance. 
We report the Area Under the Curve (AUC) of the success plot and the precision score when the center location error threshold is set to 20 pixels.
Besides, we also evaluate our model for visual grounding on the Google-split val set of RefCOCOg~\cite{REFCOCO} using the standard protocol~\cite{2019onestagevg, transvg, VLTVG} and report the Top-1 accuracy. Herein the predicted box whose IoU with grounding truth is over 0.5 is considered as a correct prediction.

\begin{table}[t!]
\centering
\setlength{\tabcolsep}{8pt}
\renewcommand{\arraystretch}{1.05}
\caption{AUC and Precision (PRE) for four variants of our model on the LaSOT and TNL2K datasets.}
\vspace{-1mm}
\resizebox{0.8\linewidth}{!}{
\begin{tabular}{lccccc}
\toprule
\multirow{2}{*}{Variants} & \multicolumn{2}{c}{LaSOT} && \multicolumn{2}{c}{TNL2K} \\ 
\cline{2-3} \cline{5-6}  
                        & AUC         & PRE           && AUC         & PRE           \\ 
\midrule
SepRM                   & 0.518       & 0.512       && 0.491       & 0.471       \\
MSRM                    & 0.536       & 0.550       && 0.511       & 0.500       \\
MSRM-TDec               & 0.549       & 0.567       && 0.524       & 0.514       \\
MSRM-TM                 & 0.561       & 0.581       && 0.541       & 0.540       \\
\textbf{Our model}      & 0.569       & 0.593       && 0.546       & 0.550       \\ 
\bottomrule
\end{tabular}}
\label{Tab:ablation}
\end{table}


\begin{table}[t]
\centering
\setlength{\tabcolsep}{2.5pt}
\renewcommand{\arraystretch}{1.05}
\caption{Comparisons between separated and joint methods. We report FLOPs and inference time for grounding and tracking separately. Note that the time for all methods is tested on RTX 3090.}
\resizebox{1.0\linewidth}{!}{
\begin{tabular}{lcccccc}
\toprule
\multicolumn{2}{c}{\multirow{2}{*}{}} & \multicolumn{3}{c}{Separated Model} &\!& Joint Model \\
\multicolumn{2}{c}{}  & VLTVG+STARK    & VLTVG+OSTrack  & \multicolumn{1}{c}{SepRM} && Ours \\ 
\midrule
\multirow{2}{*}{FLOPs} & Grounding & 39.6G & 39.6G & 34.7G      && 34.9G \\
                       & Tracking & 20.4G & 48.3G & 38.5G       && 42.0G  \\ 
\midrule
\multirow{2}{*}{Time} & Grounding & 28.2ms & 28.2ms &  26.4ms     && 34.8ms \\
                      &Tracking   & 22.9ms & 8.3ms  &  20.6ms     && 25.3ms \\ 
\midrule
Params &  Total  & 169.8M & 214.7M &214.4M  && 153.0M \\ 
\midrule
\multicolumn{1}{l}{\multirow{2}{*}{AUC}} & LaSOT   & 0.446  & 0.524  &0.518  && 0.569  \\
\multicolumn{1}{l}{}                       & TNL2K  & 0.373  & 0.399  &0.491 && 0.546 \\
\bottomrule
\end{tabular}}
\label{Tab:flops_comparsion}
\end{table}


\subsection{Ablation study}
We first conduct experiments to analyze the effect of each main component in our model. To this end, we perform ablation studies on five variants of our model:

\noindent1) \textbf{SepRM}, which deploys \textbf{Sep}arated \textbf{R}elation \textbf{M}odeling modules, i.e., two independent transformer encoders, to perform visual grounding and tracking. In this variant, the target decoder is removed, and two localization heads are directly constructed on the outputs of the two separated relation modeling modules, respectively.

\noindent2) \textbf{MSRM}, which replaces the separated relation modeling modules with the \textbf{M}ulti-\textbf{S}ource \textbf{R}elation \textbf{M}odeling module in SepRM, and only deploys a single localization head.

\noindent3) \textbf{MSRM-TDec}, which introduces a \textbf{T}arget \textbf{Dec}oder into the MSRM variant.

\noindent4) \textbf{MSRM-TM}, which introduces our proposed semantic-guided temporal modeling into the MSRM-TDec variant but only the historical target patches as inputs.

\noindent5) \textbf{Our model}, which introduces the NL CLS tokens into SGTM module, further exploits language information to improve the adaptability to the target appearance variations.

Table~\ref{Tab:ablation} presents the experimental results of these variants on LaSOT~\cite{LASOT} and TNL2K~\cite{TNL2K}.

\noindent\textbf{Effect of multi-source relation modeling.}
The performance gap between SepRM and MSRM demonstrates the effectiveness of multi-sources relation modeling to embed the reference information from the natural language and template into the feature of the test image for both grounding and tracking.

\noindent\textbf{Effect of target encoder.}
The performance comparison between MSRM and MSRM-TDec shows that the target decoder plays an important role in decoding the target state information from the test feature after relation modeling. 
Similar performance gains from the target decoder are also observed in STARK~\cite{stark}. 

\noindent\textbf{Effect of temporal modeling.}
A large performance boost arises at the comparison from MSRM-TDec to our intact model, which is benefited from temporal modeling. 
The temporal modeling module provides effective temporal clues for the decoder to adapt to the appearance variations of the target, thus greatly improving tracking performance.

\noindent\textbf{Effect of semantics-guided temporal modeling.}
We can observe that incorporating the NL CLS token brings performance gains, especially in the precision metric.
With the NL CLS token, our model predicts the target location more precisely, demonstrating the effectiveness of the language information in SGTM.

We further compare our joint visual grounding and tracking model with the separated methods in FLOPs, inference time, params, and AUC, as shown in Table~\ref{Tab:flops_comparsion}. Since the codes of existing separated methods have not been released, we construct two separated methods using state-of-the-art grounding (VLTVG~\cite{VLTVG}) and tracking (STARK~\cite{stark} and OSTrack-384~\cite{ostrack}) models.
Our proposed variant SepRM is also involved in the comparison. Compared with the separated methods, our approach performs comparably in computation, speed, and params, while performing substantially better in AUC on LaSOT and TNL2K.

\begin{table}[t]
\centering
\caption{AUC and Precision (PRE) of different methods on the OTB99, LaSOT, and TNL2K datasets. The best and second-best results are marked in \textbf{bold} and \underline{underline}. BB and NL denote the Bounding Box and Natural Language, respectively.}
\vspace{-1mm}
\setlength{\tabcolsep}{3.5pt}
\renewcommand{\arraystretch}{1.05}
\label{Tab:Tracking_results}
\resizebox{1.0\linewidth}{!}{
\begin{tabular}{lcccccccc}
\toprule
\multirow{2}{*}{Algorithms}  & \multirow{2}{*}{Initialize} & OTB99       & LaSOT    & TNL2K \\  
& & AUC \textbar\ \ PRE  & AUC \textbar\ \  PRE & AUC \textbar \ \ PRE \\
\midrule
AutoMatch     & BB         & --         & 0.583 \textbar\ 0.599 & 0.472 \textbar\ 0.435 \\
TrDiMP       & BB    & -- & 0.639 \textbar\  0.663 & \underline{0.523} \textbar\ \textbf{0.528} \\
TransT        & BB    & -- & 0.649 \textbar\ 0.690 & 0.507 \textbar\ 0.517 \\
STARK      & BB    & -- & 0.671 \textbar\ 0.712 & --      \\
KeepTrack     & BB    & -- & 0.671 \textbar\ 0.702 & --        \\
SwinTrack-B & BB    & -- & \underline{0.696}  \textbar\  \underline{0.741} & --       \\
OSTrack-384   & BB    & -- & \textbf{0.711} \textbar\ \textbf{0.776} & \textbf{0.559} \textbar\ \  \ --\ \   \\ \midrule
TNLS-II     & NL      & 0.250 \textbar\ 0.290     & --     & --        \\
RTTNLD   & NL      & 0.540 \textbar\ \textbf{0.780}     & 0.280 \textbar\ 0.280 & --       \\
GTI          & NL      & \underline{0.581} \textbar\ 0.732     & 0.478 \textbar\ 0.476 & --   \\
TNL2K-1       & NL      & 0.190 \textbar\ 0.240     & 0.510 \textbar\ 0.490 & 0.110 \textbar\ 0.060  \\
CTRNLT   & NL      & 0.530 \textbar\ 0.720     & \underline{0.520} \textbar\  \underline{0.510} & \underline{0.140}  \textbar\  \underline{0.090}  \\
\textbf{Ours}              & NL      & \textbf{0.592}  \textbar\  \underline{0.776}  & \textbf{0.569}  \textbar\  \textbf{0.593} & \textbf{0.546}  \textbar\  \textbf{0.550} \\ \midrule
TNLS-III     & NL+BB & 0.550  \textbar\  0.720     & --    & --     \\
RTTNLD   & NL+BB & 0.610  \textbar\  0.790     & 0.350 \textbar\ 0.350 & 0.250  \textbar\  0.270 \\
TNL2K-2      & NL+BB & \underline{0.680} \textbar\ \underline{0.880}     & 0.510 \textbar\ 0.550 & 0.420 \textbar\ 0.420 \\
SNLT        & NL+BB & 0.666 \textbar\ 0.804     & 0.540 \textbar\ 0.576 & 0.276 \textbar\ 0.419 \\
VLTTT         & NL+BB & \textbf{0.764} \textbar\ \textbf{0.931}     & \textbf{0.673} \textbar\ \textbf{0.721} & \underline{0.531} \textbar\ \underline{0.533} \\
\textbf{Ours}             & NL+BB & 0.653 \textbar\  0.856          & \underline{0.604} \textbar\ \underline{0.636}      & \textbf{0.569} \textbar\ \textbf{0.581}    \\ \bottomrule
\end{tabular}}
\vspace{-2mm}
\end{table}
\subsection{Tracking by only language specification}
In this section, we compare our approach with state-of-the-art trackers using only language for initialization including TNLS-II~\cite{li2017tracking}, RTTNLD~\cite{feng2020real}, GTI~\cite{GTI}, TNL2K-1~\cite{TNL2K}, CTRNLT~\cite{li2022cross} on TNL2K~\cite{TNL2K}, LaSOT~\cite{LASOT}, and OTB99~\cite{li2017tracking}.  
Table~\ref{Tab:Tracking_results} reports the experimental results of different algorithms on these three datasets.
The results of the classical trackers (AutoMatch~\cite{AutoMatch}, TiDiMP~\cite{TransMeetTracker}, TransT~\cite{chen2021transformer}, STARK~\cite{stark}, KeepTrack~\cite{keeptrack}, SwinTrack-B~\cite{swintrack}, and OSTrack-384\cite{ostrack}) are also presented.


\vspace{1mm}
\noindent\textbf{TNL2K.}
Compared with the TNL2K-1 and CTRNLT methods that use separated grounding and tracking models, our method achieves substantial performance gains of 40.6\% and 46.0\% in AUC, precision respectively, which manifests the effectiveness of the proposed joint visual grounding and tracking framework. 
Besides, our method also performs marginally better than the VLTTT method that uses both natural language and a bounding box for initialization on the TNL2K dataset.

\noindent\textbf{LaSOT.}
Compared with CTRNLT, our method improves tracking performance by 4.9\% in AUC and 8.3\% in precision. Besides, our approach also achieves substantial performance gains in comparison with TNL2K-1. 

\noindent\textbf{OTB99.} 
Compared with recently proposed state-of-the-art natural language trackers, our method achieves better performance (an AUC score of 59.2\%) on OTB99, which validates the effectiveness of our method.

\begin{figure}[t]
\centering
\includegraphics[width=1.0\columnwidth]{Figures/vis_temporal.pdf}
\caption{Visualization for revealing what temporal clue SGTM learns. 
``Only TQ'' denotes only Target Query is used as the query in the target decoder. 
``Only TC'' denotes only Temporal Clue is used as the query. 
``TQ+TC'' denotes the summation of the Target Query and Temporal Clue is used as the query.
}
\label{Fig:vis_temporal}
\end{figure}
\noindent\textbf{Qualitative study.} 
To further investigate the effectiveness of the SGTM module, we visualize the cross-attention maps in the target decoder and the prediction results in Figure~\ref{Fig:vis_temporal}.
The attention map w/o TC (only TQ) has high responses on the distractor, and the variant MSRM-TDec loses the target, as the target appearance varies a lot compared to the template. 
By contrast, the learned TC attends to the real target, and it helps our model (TQ+TC) focus on the real target and predicts a precise bounding box.

As shown in Figure~\ref{Fig:qualitative}, we visualize the tracking results on three challenging sequences in TNL2K, in which the main challenges are: viewpoint change, appearance variation, and out-of-view, respectively. 
We can observe that our model is more robust than other trackers. 
For example, in the second sequence, TNL2K-1 cannot predict the target box accurately at the beginning, and VLTTT and OSTrack gradually predict inaccurate target boxes as the target appearance varies. 
By contrast, our method predicts the target box more accurately.
These comparisons validate the effectiveness of the SGTM module.



\begin{figure*}[t]
\centering
\includegraphics[width=0.965\textwidth]{Figures/TNL_qualitative.pdf}
\vspace{-1mm}
\caption{Qualitative comparison on three challenging sequences. From top to bottom, the main challenge factors are viewpoint change, appearance variation, and out-of-view, respectively. Our model is more robust than other trackers.}
\label{Fig:qualitative}
\end{figure*}


\begin{figure*}[t]
\centering
\includegraphics[width=0.965\textwidth]{Figures/Limitations.pdf}
\vspace{-1mm}
\caption{Analysis about the effect of ambiguous natural language (NL) on a zebra sequence. 
Given the original NL description (a) with ambiguity from LaSOT, our method localizes the wrong target at the first frame and consequently fails in the whole sequence. 
By contrast, given a clear NL description (b) or providing a bounding box (c) to eliminate ambiguity, our method can successfully locate the target.
} 
\label{Fig:casestudy2}
\vspace{-1mm}
\end{figure*}


\begin{table}[t]
\centering
\setlength{\tabcolsep}{3pt}
\renewcommand{\arraystretch}{1.15}
\caption{Comparison of our method with state-of-the-art algorithms for visual grounding on the val set of RefCOCOg.}
\vspace{-1mm}
\resizebox{1.0\linewidth}{!}{
\begin{tabular}{lcccccc}
\toprule
Algorithms & NMTree & LBYL-Net & ReSC-Large & TransVG & VLTVG  & Ours   \\
&~\cite{NMTree} & ~\cite{LBYL-Net}& ~\cite{ReSC-Large} &~\cite{transvg} & ~\cite{VLTVG} \\
\midrule
Accuracy & 0.6178 & 0.6270   & 0.6312     & 0.6702  & \textbf{0.7298} & \underline{0.7007} \\
\bottomrule
\end{tabular}}
\label{Tab:groundingresult}
\vspace{-1mm}
\end{table}

\subsection{Tracking by language and box specification}
We also compare our approach with state-of-the-art trackers in the same settings that use both natural language and a bounding box for initialization. 
The trackers involved in comparisons include TNLS-III~\cite{li2017tracking}, RTTNLD~\cite{feng2020real}, TNL2K-2~\cite{TNL2K}, SNLT~\cite{feng2021siamese}, VLTTT~\cite{guo2022divert}. Table~\ref{Tab:Tracking_results} presents the experimental results of these methods.

\noindent\textbf{TNL2K.}
Compare with VLTTT, our model improves performance by 3.8\% and 4.8 \% in AUC and precision, respectively, demonstrating the effectiveness of our method.

\noindent\textbf{LaSOT.}
The proposed method obtains an AUC score of 0.604 and a precision score of  0.636 on LaSOT.
The VLTTT method is designed based on the classical tracker TransT~\cite{chen2021transformer} that already achieves a favorable performance on LaSOT and performs better than our method. 
Compared to the other trackers with language and box specifications, our method achieves substantial performance gains.

\noindent\textbf{OTB99.}
VLTTT achieves the best performance with an AUC score of 0.653 and a precision score of 0.856, which is higher than ours. 
Nevertheless, our method performs on par with SNLT and TNL2K-2.

\subsection{Performance on visual grounding}\label{grounding}
We also report the visual grounding performance of our framework on the val set of RefCOCOg split by Google, as shown in Table~\ref{Tab:groundingresult}.
Although not originally designed for visual grounding, our method performs comparably with the state-of-the-art visual grounding algorithm VLTVG~\cite{VLTVG}.

\subsection{Limitations}
Herein we discuss the limitations of our algorithm. 
Our method is designed for tracking the target of interest only based on the natural language description. 
Inevitably, it is sensitive to ambiguous natural language descriptions to some extent. As shown in Figure~\ref{Fig:casestudy2}(a), given the ambiguous description ``Zebra running on the grass with other zebras" that does not clearly specify which zebra is the target one, our method fails to locate the target zebra at the beginning. 
One solution for this issue is to provide a clear description. 
As shown in Figure~\ref{Fig:casestudy2}(b), given a clear location description of the target zebra, our algorithm successfully locates the target and keeps tracking it. 
Another alternative solution is to provide a bounding box for disambiguation, as shown in Figure~\ref{Fig:casestudy2}(c). With the correction of the additional bounding box, our method can also successfully track the target zebra.

