\documentclass[lettersize,journal]{IEEEtran}
\usepackage[linesnumbered, ruled]{algorithm2e}
\usepackage{amsthm,amsmath,amssymb}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{booktabs,color}
\usepackage{cite}
\usepackage[affil-it]{authblk}
\usepackage{stackengine} 
\usepackage{mathtools}
\usepackage{mathrsfs}

\usepackage{indentfirst} 
\usepackage{multirow}
\usepackage[backref=page]{hyperref}
\usepackage{cleveref}
\usepackage{amssymb}
\usepackage{xcolor,eucal}
\usepackage{makecell}

\def\BL#1{{\color{blue}{\bf [BoLi:} {{#1}}{ ]}}}
\graphicspath{{figs}}
\let\oldnl\nl \newcommand{\nonl}{\renewcommand{\nl}{\let\nl\oldnl}}\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
\newcommand{\hanxiargmax}[1]{\mathop{\arg\max}\limits_{#1}}
\newcommand{\hanxiargmin}[1]{\mathop{\arg\min}\limits_{#1}}
\newcommand{\hanxiR}{\mathbb{R}}
\newcommand{\II}{\text{I}}
\newcommand{\vv}[1]{\vec{\mathbf{#1}}}
\newcommand{\NL}{\\}

\newcommand{\TODO}[3]{{\color{#1}\bf{TODO: #2, please #3}}}
\newcommand\oast{\stackMath\mathbin{\stackinset{c}{0ex}{c}{0ex}{\ast}{\bigcirc}}}
\newcommand\osquare{\stackMath\mathbin{\stackinset{c}{0ex}{c}{0ex}{2}{\bigcirc}}}
\DeclareMathOperator*{\argminB}{argmin}   

\SetKwRepeat{Do}{do}{while}\begin{document}

\title{Target before Shooting: Accurate Anomaly Detection and Localization under One
  Millisecond via Cascade Patch Retrieval}

\author[1,2,]{Hanxi Li\thanks{This work was mainly completed during Hanxi Li's visit to
    Zhejiang University.}}
\author[1,]{Jianfei Hu}
\author[3,]{Bo Li}
\author[2]{Hao Chen}
\author[4,]{Yongbin Zheng}
\author[2,]{Chunhua Shen}
\affil[1]{Jiangxi Normal University, Jiangxi, China}
\affil[2]{Zhejiang University, Zhejiang, China}
\affil[3]{Northwestern Polytechnical University, Shaanxi, China}
\affil[4]{National University of Defense Technology}
\affil[]{Corresponding author}
\affil[]{These authors contributed equally to this work}

\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, June~2023}{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

\maketitle

\begin{abstract}

In this work, by re-examining the ``matching'' nature of Anomaly Detection (AD), we propose a
  new AD framework that simultaneously enjoys new records of AD accuracy and dramatically high running speed. In this framework, the anomaly detection problem is solved via a
  cascade patch retrieval procedure that retrieves the nearest neighbors for each test
  image patch in a coarse-to-fine fashion.
Given a test sample, the top- most similar training images are first selected
  based on a robust histogram matching process. Secondly, the
  nearest neighbor of each test patch is retrieved over the similar geometrical locations on
  those ``global nearest neighbors'', by using a carefully trained local metric. Finally,
  the anomaly score of each test image patch is calculated based on the distance to its
  ``local nearest neighbor'' and the ``non-background'' probability. The proposed method is termed ``Cascade Patch Retrieval'' (CPR) in this work. Different from the conventional
  patch-matching-based AD algorithms, CPR selects proper ``targets'' (reference images and locations)
  before ``shooting'' (patch-matching).
On the well-acknowledged MVTec AD, BTAD and MVTec-3D AD datasets, the proposed
  algorithm consistently outperforms all the comparing SOTA methods by remarkable margins, measured by various AD metrics.
Furthermore, CPR is extremely efficient. It runs at the
  speed of  FPS with the standard setting while its simplified version only
  requires less than  ms to process an image at the cost of a trivial accuracy drop. The code of CPR is available at \href{https://github.com/flyinghu123/CPR}{https://github.com/flyinghu123/CPR}.

\end{abstract}

\begin{IEEEkeywords}
  Anomaly detection, image patch retrieval, metric learning.
\end{IEEEkeywords}

\section{Introduction}
\label{sec:intro}
To achieve industrial-level anomaly detection (AD) is challenging as the demanding
accuracy is high to ensure the reliability of defect inspection while the time budget is
limited on a running assemble-line.
Most of the recently proposed AD methods focus on increasing the recognition accuracy as it is difficult already, especially in
the standard setting where only normal samples are available for training.

The most straightforward way to realize the ``unsupervised'' AD is the ``one-class'' classification strategy
\cite{scholkopf2000support, scholkopf2001estimating}: by considering normal images or
patches as a single class and then the anomalous ones can be detected as outliers
\cite{ruff2018deep, yi2020patch, liznerskiexplainable2020, massoli2021mocca,
  defard2021padim, zhang2022pedenet, chen2022deep, dinh2014nice, rudolph2021same}.
Similarly, the Normalized-Flow-based approaches inherit the one-class assumption but
additionally impose a Gaussian distribution onto the class for better performance
\cite{yu2021fastflow, tailanian2022u, gudovskiy2022cflow, lei2023pyramidflow}.
In contrast to the single-class setting, some discriminative anomaly detectors
\cite{li2021cutpaste, zavrtanik2021draem, yang2023memseg} learn the pixel-wise binary
classifier with genuine normal samples and synthetic anomalous samples and can usually
lift the AD accuracy to some extent.
On the other hand, the AD methods based on distillation extract the ``knowledge'' of the
``teacher'' network, which is pre-trained on an AD-irrelevant dataset, to a ``student''
network, on only normal samples. The anomaly score of each image region is then
calculated based on the response difference between the two networks
\cite{salehi2021multiresolution, deng2022anomaly, bergmann2022beyond, zhang2023destseg}.
By considering the anomalous regions as ``occlusions'' or ''noises'', some researchers
propose to detect anomalies via image reconstruction and the anomaly scores are
positively related to the reconstruction residuals \cite{zong2018deep,
  dehaene2020anomaly, shi2021unsupervised, hou2021divide, wu2021learning}.
Compared with the sophisticated framework of other anomaly detectors, PatchCore
\cite{roth2022towards} offers a much simpler alternative for AD: it can achieve the
state-of-the-art AD accuracy via a plain patch matching/retrieval process. The success
of PatchCore inspires a number of its variants \cite{kim2022fapm, bae2022image,
  xie2023pushing, saiku2022enhancing, zhu2022anomaly} which try to increase the AD
performance mainly based on more informative patch features.
More recently, some AD methods propose to conduct an image alignment, explicitly or
implicitly, before detecting the anomaly \cite{huang2022registration, zheng2022focus,
  liu2023diversity}. In this way, the image patches can be only compared with the
(original or reconstructed) normal patches in a similar geometrical location. This
geometrical constraint usually leads to more reasonable matching distances and better AD
performances.

\begin{figure*}[t!]
  \begin{center}
    \includegraphics[width=0.95\textwidth]{main_idea.pdf}
    \caption{The main scheme of the proposed CPR algorithm. Left-top (a): the vanilla nearest
      neighbor searching method that searches the neighbors in a brute-force fashion;
      left-bottom (b): the ``align \& match'' strategy that perform an implicit alignment
      process before matching; right (c): the proposed ``target \& shoot'' scheme that realize
      AD within a retrieving cascade.}
    \label{fig:main_idea}
  \end{center}
\end{figure*}

In this work, by investigating the inherent ``matching'' character of AD, we propose a
more elegant framework to solve the AD problem. We found that a properly selected
reference set can significantly benefit the matching process, in terms of both
efficiency and accuracy. The main scheme of the proposed framework is shown in
\Cref{fig:main_idea}. As it is illustrated in the left part of the figure, the
vanilla nearest-neighbor searching methods retrieve the closest prototype for the test
patch in a brute-force way over the whole reference ``bank'' \cite{roth2022towards,
  kim2022fapm, bae2022image, xie2023pushing, saiku2022enhancing, zhu2022anomaly}. In this
way, an anomalous patch could be ``perfectly'' matched with a normal patch, which is
totally geometrically irrelevant to the query sample. Consequently, this unconvincing
patch matching/retrieval usually implies a failure of AD at the corresponding local
region.
The ``align \& match'' strategy \cite{huang2022registration, zheng2022focus,
  liu2023diversity} seems a better alternative as the images are firstly aligned to
generate geometrically meaningful matching pairs. However, when the alignment fails on
the test image, the geometrical constraint could cause even worse AD results.
On the contrary, we perform the retrieval in a ``target \& shoot'' fashion. As we can
see in the right part of \Cref{fig:main_idea}, instead of aligning the test image, we
select proper reference ``targets'' for each test patch. In specific, the qualified target samples should locate at similar image coordinates to the test patch and should be only extracted from the
reference images obtained in the global retrieval stage. The local retrieval is conducted in the feature space which is learned
using a carefully customized contrastive loss and enjoys high accuracy. In addition,
differing from the ``align \& match'' strategy, the cascade scheme is naturally
robust to global retrieval errors, and thus the high recall rate of retrieval is ensured.

We term the proposed method
``Cascade Patch Retrieval'' (CPR) because the retrieval is
realized with a two-stage cascade process. For each test patch, our CPR model first
selects qualified reference samples in the oracle and then matches the query patch with
the selected references. In metaphorical words, the CPR ``targets'' on the proper
references before ``shooting''. In the extensive experiments of this work, the proposed
``target \& shoot'' methodology illustrates remarkably high AD accuracy as well as time
efficiency. In summary, the contribution of this work is threefold, as listed below.
\begin{itemize}
  \item
        Firstly, we cast the AD task as a cascade retrieval problem. Instead of brute force
        searching over the patch bank or aligning the test image to a ``standard'' pose, the
        cascade retrieval strategy naturally possesses a high retrieving recall rate and
        offers sufficient room for learning better retrieval metrics.
  \item
        Secondly, to address the over-fitting problem under the few sample condition for
        most AD datasets, we propose a novel metric learning framework with a customized
        contrastive loss and a more conservative learning strategy. The yielded CPR model
        outperforms existing SOTA algorithms by large margins, in three
        well-acknowledged AD datasets, namely MVTec AD, MVTec-3D AD and BTAD.
  \item
        Finally, in this work, the original CPR algorithm is smartly simplified for higher
        efficiency. The fastest versions of CPR, \emph{i.e.}, CPR-Faster runs at a speed
        over  FPS while still maintaining accuracy superiority over most current SOTA
        methods.
\end{itemize}

The rest of this paper is organized as follows: in \Cref{sec:related}, the related work
of the proposed algorithm is introduced; the details of the CPR algorithm are represented
in \Cref{sec:method} and we report all the experiment results in \Cref{sec:experiments};
the \Cref{sec:conclusion} summarizes this paper and discusses the future work of CPR.

\section{Related work}
\label{sec:related}

\subsection{Anomaly Detection via Patch Matching}
\label{subsec:matching}

A simple way to detect and localize anomalies is to find a normal prototype for each test
patch and assign high anomaly scores to those with low similarities. As a typical and
simple example, the PatchCore algorithm \cite{roth2022towards} proposes to build a
``memory bank'' of patch features based on the coreset-subsampling algorithm and the anomaly
score is then estimated according to the matching distance between the test patch
and its nearest neighbor. Interestingly, merely with this simple matching strategy, PatchCore
achieves dramatically high performance on the well-acknowledged MVTech-AD dataset
\cite{bergmann2019mvtec}.
The success of PatchCore inspired a number of following works: the FAPM algorithm
\cite{kim2022fapm} designs patch-wise adaptive coreset sampling to improve the matching
speed.  \cite{bae2022image} improves the matching quality by employing the position and
neighborhood information of the test patch. Graphcore \cite{xie2023pushing} proposes a
graph representation to adapt PatchCore to the few-shot scenario.
Apart from the vanilla brute-force matching, some recently proposed methods
\cite{huang2022registration, zheng2022focus, liu2023diversity} try to conduct the patch
matching between the test patch and the reference set with similar geometrical
properties. In a straightforward way, the test image is firstly aligned to a
``standard'' pose using rigid \cite{huang2022registration, zheng2022focus} or nonrigid
\cite{liu2023diversity} geometrical deformations which could be explicitly realized via
one or more learnable neural network layers. The aligned deep feature tensor are matched
with a reconstructed prototype \cite{liu2023diversity} or a position-conditioned
distribution \cite{huang2022registration, zheng2022focus} and the anomaly score is
estimated according to the deformation magnitude from the prototype
\cite{liu2023diversity} or the Mahalanobis distances from each test patch to the
corresponding distribution \cite{huang2022registration, zheng2022focus}.

Despite the well-designed framework, the ``align \& matching'' strategy does not
illustrate significant improvement over the non-aligned patch matching methods on the
well-acknowledged AD datasets, partially due to the failure cases of image alignment. In
this work, we impose the geometrical constraints in another way. Instead of aligning
the image, we directly match each test patch with its nearest neighbor, given certain
constraints. The constrained searching process is realized in a two-stage
retrieval cascade and new SOTA AD performance is obtained.

\subsection{Deep Learning based Image Retrieval}
\label{subsec:retrieval}

Image retrieval is a long-standing and fundamental task in computer vision and
multimedia.  Typically, one needs to find the nearest neighbor(s) of a ``query'' image
over the image ``gallery'', which is an image oracle for matching in fact. In this way,
the unknown property (\emph{e.g.} category label or instance identity) of the query
image can be determined by the matched nearest neighbors \cite{chen-survey-2022deep}. A great
amount of effort has been devoted to realizing better image retrieval algorithms. Most
existing image retrieval methods focus on obtaining semantic-aware global features which
could be aggregated from the off-the-shelf models \cite{2015Cross, 2015Exploiting,
  2017Unsupervised} or specifically learned models \cite{2015NetVLAD, 2016Deep,
  2021Instance}. In particular, to ensure a swift image retrieval process on large-scale
datasets, many approaches learn binary codes, instead of conventional real-valued
features, to realize the hashing retrieval \cite{2016Accurate, Mor2016Nested, 2017Deep,
  2018Supervised}, which is highly efficient in terms of both memory usage and time
consumption.

A frequently employed strategy of state-of-the-art retrieval algorithms is the
``re-ranking'' scheme that consists two phases, \emph{i.e.}, the initial ranking stage
and re-ranking stage respectively \cite{2016Image, 2018Detect, 2021Instance}. In
specific, the top-k nearest neighbors are first selected for the query image from a
global view in the former stage and the retrieval order is then modified according to
the matching state between the local descriptors extracted from the query image and its
neighbors. In this work, inspired by the high-level concept of this sophisticated
strategy, we smartly transfer the conventional matching operation of anomaly detection
into a cascade retrieval problem which is also solved in a coarse to fine manner.

\section{The proposed method}
\label{sec:method}


\subsection{The overview of the proposed method}
\label{subsec:alg_overview}
The detailed inference scheme of the proposed Cascade Patch Retrieval (CPR) algorithm is
shown in \Cref{fig:our_method}.
According to the illustration, the input of the CPR algorithm includes a test image
 and a reference image set
. The CNN-based CPR model consists of  subnetworks, namely
the DenseNet \cite{huang2018densely} backbone, the Global Retrieval Branch (GRB), the
Local Retrieval Branch (LRB) and the Foreground Estimation Branch (FEB), respectively. As
the raw feature generator, the DenseNet can be denoted as the following function

where  denotes the generated raw feature which will be fed into the
succeeding three branches. In a similar manner, the global retrieval branch
, the local retrieval branch  and the foreground
estimation branch  are defined as follows

note that the outputs of  is a collection of  histograms with 
bins,  generates a -D tensor while the output of  is
a -D confidence map .

The first stage of the cascade retrieval is conducted based on the ``global feature''
 and the top- nearest neighbors of the test image  is
stored into the image set .


The second stage of the cascade retrieval is performed based on the local feature
 which can be viewed as a collection
of  vector features with each element corresponding to an image patch.
The primitive anomaly score of each test patch is estimated according to the patch feature distance between itself and its nearest neighbor retrieved from  and at similar image locations.

Finally, under the assumption that the anomalies only take place in the foreground part,
the primitive anomaly scores are corrected via using the foreground confidence map
 predicted by the foreground estimation branch
.

\begin{figure*}[htbp]
  \centering
  \includegraphics[width=6.8in]{our_method}
  \caption{CPR train procedure for detecting and localizing anomalies on
    images. The whole process could be divided into  sectors, namely the DenseNet
    backbone, the global retrieval branch, the local retrieval branch and the foreground
    estimation branch, respectively.}\label{fig:our_method}
\end{figure*}

\subsection{The global retrieval branch}
\label{subsec:grb}
The Global Retrieval Branch (GRB) is actually a statistical feature generator. Given 
reference (anomaly-free) images , and the corresponding DenseNet feature tensors
, we
firstly flatten each tensor to build the ``raw patch feature'' set as

Then suppose all the raw patch features belonging to different reference images are
collected together into the feature set , a
-means clustering method is performed on  to obtain 
clustering centers .

The block-wise statistics of the test image  can be obtained based
on the raw feature tensor  and the cluster center set
. In specific, the tensor  is evenly
divided into  sub-tensors as

For each sub-tensor, we extract its statistics in the ``Bag of Words'' (BoW) style as

where  denotes a BoW histogram of 
with respect to the codebook  and this histogram is normalized
so that . The histograms
of a reference image  can also be obtained in a similar manner as
\Cref{equ:statistic} and let us denote them as . Let

denote the Kullback-Leibler (KL) divergence between two block histograms with the same
block ID, one can get the following sorted (in increasing order) block divergences

Then the global features distance between  and  is
estimated as

where  is a small number for ignoring large block-wise KL divergences so that the
overall distance estimation is robust to partial image contamination.

Based on the global feature distance, the top- neighbor reference images are retrieved
for  and stored into the image set  which are employed
as the reference images in the following patch retrieval process.

In practice, this searching process over the reference images usually lead to a set of
``pseudo-aligned'' nearest neighbors (see \Cref{subsec:qualitative} for details). Thus our
global retrieval branch offers an alternative to the image alignment approaches
\cite{huang2022registration, zheng2022focus, liu2023diversity}, while with higher
simplicity and accuracy.

\subsection{The local retrieval branch}
The Local Retrieval Branch (LRB) plays the most important role in the CPR algorithm.
\subsubsection{The structure of LRB}
As to the branch structure, inspired by the pioneering work \cite{szegedy2015rethinking},
we modify the well-known inception block to build our LRB. As it is shown in
\Cref{fig:our_method}, the input feature (\emph{i.e.}, the raw feature tensor
 of \Cref{equ:raw_fea}) are processed by the block via four main paths, with
different combinations of the ``Conv-BN-ReLU'' blocks and the average pooling layers.  The
output tensors of these four paths are firstly concatenated together and then further
processed by a  convolutional layer. According to \Cref{equ:branch_functions},
the final output tensor of the test image  is given by


\subsubsection{The inference process of LRB}
Let us flatten the tensor  into a set of feature vectors with
the associated row-column coordinates as

where , .

Given the nearest neighbor set  obtained by GRB, one can calculate the
corresponding local feature tensors as . In a similar way to \Cref{equ:local_patch_feature}, the feature vectors of
 writes

The local patch retrieval is then conducted between each feature vector in
 and the patch reference set . In
particular, given a test patch feature , its nearest neighbor
is retrieved as

where  stands for the function of cosine similarity between two
vectors. Accordingly, the minimal distance is defined as:

where  denotes the map of anomaly
score predicted by the LRB.

\subsubsection{The training strategy of LRB}
\label{subsubsec:lrb_train}
To achieve a good metric function  for patch retrieval, we learn the
parameters of LRB using a sophisticated training scheme and a modified contrastive loss
\cite{1640964}.

In particular, firstly, a query image  and a reference
image  randomly selected from the -NN set 
are employed as the input of the metric learning process. Following the data
augmentation strategy of \cite{yang2023memseg}, pseudo anomalies are randomly
merged with  to generate the synthetically defective image
.
Secondly,  and  are processed by
\Cref{equ:image_to_L} to obtain the feature tensors  and
 which are further flattened to the vector feature sets
 and  respectively.

Thirdly, the training samples of the metric learning, \emph{i.e.}, the feature pairs are
sampled using Algorithm~\ref{alg:sample}. Note that three kinds of feature pairs are
sampled, namely the positive pairs, the remote pairs and the anomalous pairs. The former
one is positive () and the latter two kinds are both negative () but
differ in the reason for being negative.



\begin{algorithm}[ht]
  \caption{Feature-pair sampling strategy of CPR}  
    \label{alg:sample}
    \LinesNumbered  
    \KwIn{
      Query feature set ; reference feature set
      ; binary mask  of the synthetic anomaly of ; number of
      samples ; distance threshold ; vector normalization function
      .
    }  
    \KwOut{
    The set of query-reference feature pairs ; Binary labels .
    }
    \textbf{Algorithm:}\\
    , \\
    \While{}
    {
        \tcc{{positive pairs sampling}}
        \\
        \\
        ,  \\

        \tcc{{remote pairs sampling}}
        \\
,  \\


        \tcc{{anomalous pairs sampling}}
        \\
        \\
        ,  \\

         \\
    }
\end{algorithm}


Fourthly, in Algorithm~\ref{alg:sample}, a remote pair is considered as negative only
because of the coordinate distance between the two involved features. In this way,
unnecessary ambiguities could be brought into the training process due to the existence of
the ``remote but similar'' feature pairs.
In this work, we propose to reduce the training sample weights of those ambiguous pairs.
Concretely, given the row-column coordinates of the two involved features in a pair
 writes ,  and the raw feature tensors
of query and reference images are  and  \footnote{Note that here the tensors 
and  are resized so that their height and width are identical to
 and }. The weight of  is
denoted as  which is calculated as

where  and  are the
sliced feature vectors from  and ,
according to the coordinates ,   stands
for the  norm function and  is the pre-estimated average raw feature distance
between randomly selected reference and query images.

Finally, the modified contrastive loss is defined as

where the inner product between  and
 reflects the pair similarity,  and
 are the hyper-parameters of ``similarity threshold'' for positive pairs and
negative pairs,  is a constant set larger than  to emphasis the hard pair samples in
training.

\begin{figure*}[htbp]
  \centering
  \includegraphics[width=7in]{train}
  \caption{
    CPR inference procedure for detecting and localizing anomalies on images. Note that
    the remote feature pairs (green) and the anomalous feature pairs (blue) are labeled as
    negative while the positive feature pairs (red) are extracted from the identical and
    defect-free location on two feature tensors.}
  \label{fig:train}
\end{figure*}

\subsection{The foreground estimation branch}
\label{subsec:foreground}
In many real-life scenarios, the inspected object (\emph{e.g.} a screw or a hazelnut
\cite{bergmann2019mvtec}) is photographed with a large surrounding background region and
without explicit annotations. Recalling that the concerned anomalies are all located on the
object part, removing the ``false alarms'' on the background region can therefore
effectively increase the AD accuracy. Consequently, in some recently proposed works
\cite{yang2023memseg, zhang2022prototypical, wang2023multimodal, yao2022explicit},
foreground/background information is well exploited for anomaly detection.

In this paper, we employ a Foreground Estimating Branch (FEB) to classify
the pixels as foreground or background. The workflow of FEB is depicted in
\Cref{fig:foreground}. As it is shown in the figure, firstly, each training image
 is processed by DenseNet backbone as \Cref{equ:raw_fea} to
generate the raw feature tensor . Secondly, all the vector feature  of this tensor are coded with the codebook
 defined in \Cref{subsec:grb} and then the codes are recombined
into a code map  (shown as numbers on the right column of \Cref{fig:foreground}), which is mainly used to select training samples for FEB.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=3.2in]{foreground}
  \caption{The foreground estimation strategy of the proposed CPR method. The deep
    features obtained by DenseNet and the pseudo-labels are used to learn a  convolutional layer.}
  \label{fig:foreground}
\end{figure}

In this work, some of the vector features  of this tensor are selected as training features of the classifier. As there is
no explicit foreground/background label in most AD datasets \cite{bergmann2019mvtec,
  Bergmann_2022}, we propose to generate the pseudo-labels for the selected deep features.
In general, the surrounding area (shown in blue in the top-right block of
\Cref{fig:foreground}) are treated as negative (background) regions and the center area
(shown in red in the two blocks) are the positive (foreground) region. However, to further
remove the potential class ambiguity, only the surrounding features with the majority code
(cluster- in the example of \Cref{fig:foreground}) are selected as negative samples and
the center samples with this majority code are abandoned. The training samples of the
FEB is selected over all the reference images and a 
convolutional layer is employed to classify the vector features
 into the two categories, as shown in the
bottom-left corner of \Cref{fig:foreground}.

In practice, suppose that the foreground prediction map of the test image  is denoted as  and those maps
of 's  nearest neighbors are collected in the set , each element of the final foreground estimation map  is given by

where  and .
The final anomaly prediction map  is then
obtained as

where  denotes the element-wise multiplication and function
 stands for the up-sampling operation as  and usually .
Note that the proposed FEB is not compatible to all the AD datasets, \emph{e.g.} the
texture sub-categories of MVTec AD \cite{Bergmann_2019_CVPR}. In these scenarios, we just
disable the FEB for the whole learning-inference process.

\subsection{The end-to-end inference process of CPR}
\label{subsec:cpr_learn}
The end-to-end inference process of the proposed method is shown in
Algorithm~\ref{alg:infer}. Note that in practice the proposed method is designed in a
multi-scale style. In particular, two raw feature tensors 
are extracted from different layers of DenseNet. The GRB and the FEB are only
conducted on  with higher resolution while the local patch retrieval is
performed on both of the two tensors. The yielded two anomaly score maps
 and  are firstly aggregated and
then fused with the foreground estimation  to generate the final score
map .
In addition, the image-level anomaly score  is estimated using the similar
method as \cite{zhang2022prototypical, zhang2023destseg}.


\begin{algorithm}[htbp]
  \caption{Inference pseudo-code of CPR}
  \label{alg:infer}
  \LinesNumbered
  \KwIn{Two pre-trained feature extractors of DenseNet: ; GRB , global feature oracle ;
  LRB ; local feature oracles with multi-scale
  ; FEB ; foreground
  estimation map set ; number of nearest neighbors ; test
  image .
  }
  \KwOut{Anomaly score map  and anomaly score  for .}  \textbf{Algorithm:}\\
  \tcc{{Feature extraction}}
  \For{}{
  
  }
   \\
  \tcc{{Find the -NNs of  in
  }}
  \\

  \\

   \\
  \tcc{{multiscale local retrieval}}
  \\


  \For{}
  {

   \\

   \\

   \\

  \For{ and }
  {
   as \ref{equ:loal_retrieval}
  \\

  }
  \\
  }
  \tcc{{foreground filter}}
  \\
  \tcc{{image-level anomaly score}}
  
\end{algorithm}


\subsection{Implementation details}
\label{subsec:cpr_detail}
The input images of our CPR model are consistently resized to , in both
training and test procedures.
We adopt the ``denseblock-'' and ``denseblock-'' of the DenseNet model
\cite{huang2018densely} pre-trained on ImageNet \cite{russakovsky2015imagenet} to obtain
the raw feature tensors with the sizes  and  respectively and freeze these blocks during training. In particular, GRB and FEB only
performed based on denseblock- and LRB uses both denseblock- and denseblock- in a
multi-scale manner.

The dimension of the feature space for local retrieval is , following the setting in
\cite{roth2022towards}. In the inference stage, the number of  nearest neighbors is ,
the size of the sub-tensors  is set to 5, the number of clusters  is
, the parameter for robust KL divergences  is set to , the parameter  for
calculating image-level anomaly score is  and the local retrieval region size
for denseblock- and denseblock- are  and  respectively.

As to the training setup, we use the AdamW optimizer \cite{loshchilov2018decoupled} to
update the model parameters with a default learning rate  and default
weight decay rate . The training is conducted for  iterations
with the batch size  for each sub-category and we empirically select \textbf{A FIXED}
iteration number for \textbf{ALL THE SUB-CATEGORIES} of one dataset \footnote{Note that
  for the most comprehensive MVTec AD dataset \cite{Bergmann_2019_CVPR}, we treat it as a
  combination of a texture subset and an object subset.}. In this paper, the data augmentation only contains
the salt-and-pepper noise and the brightness changes.

We adopt off-the-shelf defect generation/augmentation approaches in this paper. For the
anomaly-free training sets, synthetic anomalies are randomly generated and added onto the
normal samples, mainly following the pseudo-defect generation method proposed in
\cite{yang2023memseg}. For the supervised scenarios, we employ a similar strategy as the
PRN algorithm \cite{zhang2022prototypical} to ``extend'' the anomalous patterns for
achieving better performance based on limited samples.


\section{Experiments}
\label{sec:experiments}
In this section, a series of experiments are conducted to verify the proposed method,
comparing with other state-of-the-art methods. Three well-acknowledged benchmark datasets, namely the MVTec AD \cite{bergmann2019mvtec}, MVTec-3D AD \cite{Bergmann_2022} and BTAD
\cite{mishra2021vt} are involved in the experiments. The anomaly detection and
localization performances are evaluated using four popular AD metrics, \emph{i.e.},
Image-AUC, Pixel-AUC, Per-Region Overlap (PRO), and Average Precision (AP), under both the
supervised and unsupervised scenarios.

\subsection{Experimental settings}
\label{subsec:exp_setting}
\subsubsection{Three involved datasets}
\begin{itemize}
  \item
        \textbf{MVTec AD} \cite{Bergmann_2019_CVPR} contains  images belonging to 
        sub-datasets which can be further split into  object sub-categories and 
        texture sub-categories. Each sub-category involves a nominal-only training set and a
        test set with various types of anomalies.
  \item
        \textbf{MVTec-3D AD} \cite{Bergmann_2022} is a 3D anomaly detection dataset
        comprising over  RGB images and the corresponding high-resolution 3D point
        cloud data. Similar to MVTec AD, each of the  sub-categories of MVTec-3D AD are
        divided into a defect-free training set and a test set containing various kinds of
        defects. Though MVTec-3D AD provides accurate point cloud information, only RGB
        images are utilized for predicting anomalies in this work.
  \item
        \textbf{BTAD} \cite{mishra2021vt} comprises  images from three categories of
        real-world industrial products with different body and surface defects. It is
        usually considered as a complementary dataset to the MVTec AD when evaluating an AD
        algorithm.
\end{itemize}

\subsubsection{Four evaluation metrics}
\begin{itemize}
  \item
        \textbf{Image-AUC}: this image-level AD performance is measured by calculating the
        area under the ``Receiver Operating Characteristic'' curve (AUROC) of the predicted
        image-level anomaly scores with increasing thresholds.
  \item
        \textbf{Pixel-AUC}: this pixel-level AD performance is similar to the image-AUC
        except that the unit to be classified is pixel rather than image.
  \item
        \textbf{PRO}: the ``Per-Region Overlap'' metric is a pixel-level metric that evaluates
        the AD performance on connected anomaly regions. It is more robust to the dataset
        with significantly different sizes of anomaly regions \cite{bergmann2020uninformed}.
  \item
        \textbf{AP}: the average precision (AP) is the most popular metric for semantic
        segmentation tasks. It treats each pixel independently when measuring the
        segmentation performance \cite{saito2015precision}.
\end{itemize}

\subsubsection{Two supervision scenarios}
\begin{itemize}
  \item
        \textbf{The unsupervised scenario.} This is the conventional supervision condition
        in the literature of AD \cite{Bergmann_2019_CVPR, Bergmann_2022, mishra2021vt}. In
        this scenario, \textbf{all} the training images are anomaly free while various
        anomalies exist in the test set. To achieve a better AD model, we train our CPR
        algorithm by using synthetic defects as it is introduced in \Cref{subsec:cpr_learn}.
  \item
        \textbf{The supervised scenario.} To mimic the real-life situation where a limited
        number of defective samples are available, some researchers \cite{ding2022catching,
          zhang2022prototypical} propose to randomly select some anomalous test samples and
        add them to the original training set. The yielded AD models usually perform better
        on the remaining test images. In this work, we employ the ``Extended Anomalies''
        strategy \cite{zhang2022prototypical} to further exploit the information of the
        genuine defects.
\end{itemize}

\subsubsection{Hardware setting}
We conduct all the mentioned experiments in this paper using a machine equipped with an
Intel i-KF CPU, 32G DDR4 RAM and an NVIDIA RTX  GPU.

\subsection{Quantitative Results}
\label{subsec:quantitative}

\subsubsection{Results in unsupervised setting}
We firstly evaluate the proposed method and the  comparing SOTA methods (PatchCore
\cite{roth2022towards}, DRAEM \cite{zavrtanik2021draem}, RD \cite{deng2022anomaly}, SSPCAB
\cite{ristea2022self}, DeSTSeg \cite{zhang2023destseg}, PyramidFlow
\cite{lei2023pyramidflow}, CDO \cite{cao2023collaborative} and SemiREST \cite{li2023efficient}) on the
MVTec AD dataset \cite{Bergmann_2019_CVPR} and within the unsupervised condition.
\Cref{table:result} shows the AP, PRO and Pixel-AUC results of the involved methods.
According to the table, our CPR algorithm outperforms other competitors by large margins.
The most recently proposed SemiREST algorithm \cite{li2023efficient} performs slightly
better than our method on the texture sub-categories while CPR beats it on the object
sub-categories with significant superiority (\emph{e.g.}, the nearly  gain on AP). In
general, the proposed CPR method ranks first for the AP and PRO metrics and ranks
second for the Pixel-AUC metric. We report the Image-AUC scores of the comparing methods
in \Cref{table:image_auc_result}, from which one can see a new Image-AUC record of
 is achieved by the proposed method. This performance is achieved by selecting the
best training iteration number independently for each sub-category. Although contradicting
the principle of model selection described in \Cref{subsec:cpr_detail}, this model
selection trick is common in the literature of anomaly detection
\cite{gudovskiy2022cflow, Liu_2023_CVPR, cao2023collaborative}. When strictly following the model
selection principle in \Cref{subsec:cpr_detail}, the Image-AUC drops marginally to
 which is also higher than the existing SOTA performances.

\begin{table*}[htbp]
\centering
  \caption{The AP, PRO and Pixel-AUROC scores on MVTec AD in the unsupervised settings.}
  \label{table:result}
  \resizebox{\textwidth}{!}{
    \begin{tabular}{lccccccccc}
\toprule
      Category     & \makecell{PatchCore \cite{roth2022towards} \NL (CVPR2022)} & \makecell{DRAEM \cite{zavrtanik2021draem} \NL (ICCV2021)}                                     & \makecell{RD \cite{deng2022anomaly} \NL (CVPR2022)}                   & \makecell{SSPCAB \cite{ristea2022self} \NL (CVPR2022)}                                        & \makecell{DeSTSeg \cite{zhang2023destseg} \NL (CVPR2023)}            & \makecell{PyramidFlow \cite{lei2023pyramidflow} \NL (CVPR2023)} & \makecell{CDO \cite{cao2023collaborative} \NL (TII2023)} & \makecell{SemiREST \cite{li2023efficient} \NL (arXiv2023)}                                      & Ours                                                                                            \\\hline
      Carpet       & //{\color{blue}{}}              & //                                                                          & //                                                  & //                                                                          & //                                                 & //                                            & //{\color{blue}{}}            & {\color{red}{}}/{\color{red}{}}/{\color{red}{}}    & {\color{blue}{}}/{\color{blue}{}}/                          \\
      Grid         & //                                       & {\color{red}{}}/{\color{red}{}}/{\color{red}{}}  & //                                                  & /{\color{blue}{}}/{\color{blue}{}}                        & //                                                 & //                                            & //                                     & {\color{blue}{}}//{\color{blue}{}}                          & //{\color{blue}{}}                                                   \\
      Leather      & //                                       & //                                                                          & //                                                  & //                                                                          & //{\color{blue}{}}                        & //                                            & //                                     & {\color{red}{}}/{\color{blue}{}}/{\color{red}{}}   & {\color{blue}{}}/{\color{red}{}}/{\color{red}{}}   \\
      Tile         & //                                       & /{\color{blue}{}}/                                                 & //                                                  & {\color{blue}{}}//{\color{blue}{}}                        & //                                                 & //                                            & //                                     & {\color{red}{}}/{\color{red}{}}/{\color{red}{}}    & //                                                                            \\
      Wood         & //                                       & //                                                                          & //                                                  & //                                                                          & {\color{red}{}}//{\color{red}{}} & /{\color{red}{}}/                    & //                                     & //{\color{red}{}}                                                    & {\color{blue}{}}/{\color{blue}{}}/{\color{blue}{}} \\\hline
      Average      & //                                       & //                                                                          & //                                                  & //                                                                          & //                                                 & /{\color{blue}{}}/                   & //                                     & {\color{red}{}}/{\color{red}{}}/{\color{red}{}}    & {\color{blue}{}}/{\color{red}{}}/{\color{blue}{}}  \\\hline
      Bottle       & //                                       & //                                                                          & //                                                  & //                                                                          & //                                                 & //                                            & //                                     & {\color{red}{}}/{\color{red}{}}/{\color{red}{}}    & {\color{blue}{}}/{\color{blue}{}}/{\color{blue}{}} \\
      Cable        & //                                       & //                                                                          & //                                                  & //                                                                          & //                                                 & //                                            & //                                     & {\color{blue}{}}/{\color{red}{}}/{\color{blue}{}}  & {\color{red}{}}/{\color{blue}{}}/{\color{red}{}}   \\
      Capsule      & //                                       & //                                                                          & //                                                  & //                                                                          & //{\color{blue}{}}                        & /{\color{red}{}}/                    & //                                     & {\color{blue}{}}/{\color{blue}{}}/                          & {\color{red}{}}//{\color{red}{}}                            \\
      Hazelnut     & //                                       & {\color{blue}{}}/{\color{red}{}}/{\color{red}{}} & //                                                  & {\color{red}{}}/{\color{blue}{}}/{\color{red}{}} & //{\color{blue}{}}                        & //                                            & //                                     & //{\color{blue}{}}                                                   & //{\color{blue}{}}                                                   \\
      Metal Nut    & //                                       & {\color{blue}{}}//{\color{red}{}}                         & //                                                  & /{\color{red}{}}/{\color{blue}{}}                         & //                                                 & //                                            & //                                     & {\color{red}{}}/{\color{blue}{}}/{\color{red}{}}   & /{\color{blue}{}}/                                                   \\
      Pill         & //                                       & //                                                                          & //                                                  & //                                                                          & //                                                 & //                                            & //                                     & {\color{blue}{}}/{\color{blue}{}}/{\color{blue}{}} & {\color{red}{}}/{\color{red}{}}/{\color{red}{}}    \\
      Screw        & //{\color{blue}{}}              & //                                                                          & /{\color{blue}{}}/{\color{red}{}} & //                                                                          & //                                                 & //                                            & //                                     & {\color{blue}{}}//{\color{red}{}}                           & {\color{red}{}}/{\color{red}{}}/{\color{red}{}}    \\
      Toothbrush   & //                                       & //                                                                          & //                                                  & //                                                                          & {\color{blue}{}}//                        & /{\color{blue}{}}/                   & //                                     & //{\color{blue}{}}                                                   & {\color{red}{}}/{\color{red}{}}/{\color{red}{}}    \\
      Transistor   & //                                       & //                                                                          & //                                                  & //                                                                          & //                                                 & //{\color{blue}{}}                   & //                                     & {\color{blue}{}}/{\color{blue}{}}/{\color{red}{}}  & {\color{red}{}}/{\color{red}{}}/{\color{red}{}}    \\
      Zipper       & //                                       & //                                                                          & //                                                  & //                                                                          & //                                                 & //                                            & //                                     & {\color{red}{}}/{\color{red}{}}/{\color{red}{}}    & {\color{blue}{}}/{\color{blue}{}}/{\color{blue}{}} \\\hline
      Average      & //                                       & //                                                                          & //                                                  & //                                                                          & //                                                 & //                                            & //                                     & {\color{blue}{}}/{\color{blue}{}}/{\color{blue}{}} & {\color{red}{}}/{\color{red}{}}/{\color{red}{}}    \\\hline
      TotalAverage & //                                       & //                                                                          & //                                                  & //                                                                          & //                                                 & //                                            & //                                     & {\color{blue}{}}/{\color{blue}{}}/{\color{red}{}}  & {\color{red}{}}/{\color{red}{}}/{\color{blue}{}}   \\
\bottomrule
    \end{tabular}
  }
\end{table*}

\begin{table*}[htbp]
\centering
  \caption{
    Image-level anomaly detection AUC (\%) on MVTec AD. Results are averaged over
    all categories.
  }
  \label{table:image_auc_result}
\begin{tabular}{ccccccccc}
    \toprule
    \makecell{PatchCore \cite{roth2022towards} \NL (CVPR2022)} & \makecell{DRAEM \cite{zavrtanik2021draem} \NL (ICCV2021)} & \makecell{RD \cite{deng2022anomaly} \NL (CVPR2022)} & \makecell{SSPCAB \cite{ristea2022self} \NL (CVPR2022)} & \makecell{DeSTSeg \cite{zhang2023destseg} \NL (CVPR2023)} & \makecell{CDO \cite{cao2023collaborative} \NL (TII2023)} & \makecell{SimpleNet \cite{Liu_2023_CVPR} \NL (CVPR2023)} & Ours                            & Ours                       \\ \hline
                                                         &                                                     &                                               &                                                  &                                                     &                                                    &                                                    & {\color{blue}{}} & {\color{red}{}} \\
    \bottomrule
  \end{tabular}
\end{table*}

To achieve a more comprehensive comparison, the proposed method is also evaluated on other
two widely-used datasets, namely MVTec-3D AD \cite{Bergmann_2022} and BTAD
\cite{mishra2021vt}. The corresponding results of CPR and those SOTA algorithms on these
two datasets are shown in \Cref{table:3d_result} and \Cref{table:btad_supervised_result}.
Unsurprisingly, the proposed CPR maintains the accuracy superiority over the existing SOTA
methods.

\begin{table}[htbp]
\centering
  \caption{
    AP, PRO and Pixel-AUROC scores on MVTec-3D
    AD \cite{Bergmann_2022} with pure RGB inputs.
  }
  \label{table:3d_result}
  \resizebox{\linewidth}{!}{
    \begin{tabular}{lcccc}
\toprule
      Category   & \makecell{AST \cite{rudolph2022asymmetric} \NL (WACV2023)} & \makecell{CDO \cite{cao2023collaborative} \NL (TII2023)}                                       & \makecell{M3DM \cite{wang2023multimodal} \NL (CVPR2023)}                                        & Ours                                                                                         \\\hline
      Bagel      & //                                       & /{\color{blue}{}}/{\color{blue}{}}                         & {\color{blue}{}}//                                                   & {\color{red}{}}/{\color{red}{}}/{\color{red}{}} \\
      CableGland & //                                       & {\color{blue}{}}/{\color{red}{}}/{\color{blue}{}} & /{\color{blue}{}}/{\color{blue}{}}                          & {\color{red}{}}/{\color{red}{}}/{\color{red}{}} \\
      Carrot     & //                                        & /{\color{red}{}}/{\color{red}{}}                           & {\color{blue}{}}/{\color{blue}{}}/{\color{red}{}}  & {\color{red}{}}//{\color{blue}{}}                        \\
      Cookie     & //                                       & /{\color{blue}{}}/{\color{blue}{}}                         & {\color{blue}{}}//                                                   & {\color{red}{}}/{\color{red}{}}/{\color{red}{}} \\
      Dowel      & //                                       & //{\color{blue}{}}                                                  & {\color{blue}{}}/{\color{blue}{}}/{\color{red}{}}  & {\color{red}{}}/{\color{red}{}}/{\color{red}{}} \\
      Foam       & //                                        & //                                                                           & {\color{blue}{}}/{\color{blue}{}}/{\color{blue}{}} & {\color{red}{}}/{\color{red}{}}/{\color{red}{}} \\
      Peach      & //                                       & {\color{blue}{}}/{\color{red}{}}/{\color{blue}{}} & /{\color{blue}{}}/                                                   & {\color{red}{}}/{\color{red}{}}/{\color{red}{}} \\
      Potato     & 1//                                       & /{\color{red}{}}/{\color{red}{}}                           & {\color{blue}{}}/{\color{red}{}}/{\color{blue}{}}  & {\color{red}{}}/{\color{blue}{}}/                        \\
      Rope       & //                                       & /{\color{blue}{}}/{\color{blue}{}}                         & {\color{blue}{}}//                                                   & {\color{red}{}}/{\color{red}{}}/{\color{red}{}} \\
      Tire       & //                                        & /{\color{blue}{}}/{\color{blue}{}}                         & {\color{blue}{}}//{\color{blue}{}}                          & {\color{red}{}}/{\color{red}{}}/{\color{red}{}} \\\hline
      Average    & //                                       & //                                                                           & {\color{blue}{}}/{\color{blue}{}}/{\color{blue}{}} & {\color{red}{}}/{\color{red}{}}/{\color{red}{}} \\
\bottomrule
    \end{tabular}
  }
\end{table}


\begin{table*}[htbp]
\centering
  \caption{
    Results of the AP, PRO and Pixel-AUC metrics for unsupervised anomaly localization
    performance on BTAD \cite{mishra2021vt}.
  }
  \label{table:btad_result}
\begin{tabular}{lcccccc}
\toprule
    Category & \makecell{PatchCore \cite{roth2022towards} \NL (CVPR2022)} & \makecell{SSPCAB \cite{ristea2022self} \NL (CVPR2022)} & \makecell{RD \cite{deng2022anomaly} \NL (CVPR2022)} & \makecell{NFAD \cite{yao2022explicit} \NL (CVPR2023)}                                        & \makecell{SemiREST \cite{li2023efficient} \NL (arXiv2023)}                                      & Ours                                                                                           \\\hline
         & //                                       & //                                   & //                                & //                                                                         & {\color{blue}{}}/{\color{blue}{}}/{\color{blue}{}} & {\color{red}{}}/{\color{red}{}}/{\color{red}{}}   \\
         & //                                       & //                                   & {\color{blue}{}}//       & //                                                                         & /{\color{blue}{}}/{\color{blue}{}}                          & {\color{red}{}}/{\color{red}{}}/{\color{red}{}}   \\
         & //{\color{blue}{}}              & //                                    & /{\color{red}{}}/        & {\color{red}{}}/{\color{red}{}}/{\color{red}{}} & /{\color{red}{}}/{\color{red}{}}                            & {\color{blue}{}}/{\color{blue}{}}/{\color{red}{}} \\\hline
    Average  & //                                       & //                                   & //                                & {\color{blue}{}}//                                                & /{\color{blue}{}}/{\color{blue}{}}                          & {\color{red}{}}/{\color{red}{}}/{\color{red}{}}   \\
\bottomrule
  \end{tabular}
\end{table*}

\subsubsection{Results in supervised setting}

On the other hand, the proposed CPR method also performs well in the supervised scenario.
According to the results reported in \Cref{table:supervise_result} and
\Cref{table:btad_supervised_result}, the CPR method achieves the consistently highest
scores with Pixel-AUC, PRO and AP metrics. In particular, the overall AP of the proposed
algorithm is  on MVTec AD and  on BTAD, exceeding the current SOTA
method SemiREST \cite{li2023efficient} by  and , respectively.

\begin{table*}[htbp]
\centering
  \caption{
    Results of the AP, PRO and Pixel-AUROC metrics for anomaly localization performance on
    MVTec AD in a supervised setting. In accordance with the general framework proposed by
    DRA \cite{ding2022catching}, we sampled  labeled anomaly samples from all anomaly
    samples in the test set for each sub-dataset.
  }
  \label{table:supervise_result}
  \begin{tabular}{lccccc}
\toprule
    Category     & \makecell{DevNet \cite{pang2021explainable} \NL (arXiv2021)} & \makecell{DRA \cite{ding2022catching} \NL (CVPR2022)} & \makecell{PRN \cite{zhang2022prototypical} \NL (CVPR2023)}                                    & \makecell{SemiREST \cite{li2023efficient} \NL (arXiv2023)}                                      & Ours                                                                                            \\\hline
    Carpet       & //                                         & //                                  & //                                                                          & {\color{red}{}}/{\color{red}{}}/{\color{red}{}}    & {\color{blue}{}}/{\color{blue}{}}/{\color{blue}{}} \\
    Grid         & //                                         & //                                  & //                                                                          & {\color{blue}{}}/{\color{blue}{}}/{\color{blue}{}} & {\color{red}{}}/{\color{red}{}}/{\color{red}{}}    \\
    Leather      & //                                          & //                                   & //                                                                          & {\color{red}{}}/{\color{red}{}}/{\color{red}{}}    & {\color{blue}{}}/{\color{blue}{}}/{\color{blue}{}} \\
    Tile         & //                                         & //                                  & //{\color{blue}{}}                                                 & {\color{blue}{}}/{\color{blue}{}}/{\color{red}{}}  & {\color{red}{}}/{\color{red}{}}/{\color{red}{}}    \\
    Wood         & //                                         & //                                  & //                                                                          & {\color{blue}{}}/{\color{blue}{}}/{\color{blue}{}} & {\color{red}{}}/{\color{red}{}}/{\color{red}{}}    \\\hline
    Average      & //                                         & //                                  & //                                                                          & {\color{red}{}}/{\color{blue}{}}/{\color{blue}{}}  & {\color{blue}{}}/{\color{red}{}}/{\color{red}{}}   \\\hline
    Bottle       & //                                         & //                                  & {\color{blue}{}}/{\color{blue}{}}/                        & {\color{red}{}}/{\color{red}{}}/{\color{blue}{}}   & {\color{red}{}}/{\color{red}{}}/{\color{red}{}}    \\
    Cable        & //                                         & //                                  & /{\color{red}{}}/                                                  & {\color{red}{}}/{\color{blue}{}}/{\color{blue}{}}  & {\color{blue}{}}//{\color{red}{}}                           \\
    Capsule      & //                                         & //                                  & {\color{blue}{}}//                                                 & /{\color{red}{}}/{\color{blue}{}}                           & {\color{red}{}}/{\color{blue}{}}/{\color{red}{}}   \\
    Hazelnut     & //                                         & //                                  & {\color{blue}{}}//{\color{blue}{}}                        & /{\color{blue}{}}/{\color{red}{}}                           & {\color{red}{}}/{\color{red}{}}/{\color{red}{}}    \\
    Metal Nut    & //                                         & //                                  & //                                                                          & {\color{red}{}}/{\color{blue}{}}/{\color{red}{}}   & {\color{blue}{}}/{\color{red}{}}/{\color{blue}{}}  \\
    Pill         & //                                         & //                                  & {\color{red}{}}/{\color{blue}{}}/{\color{red}{}} & /{\color{red}{}}/{\color{blue}{}}                           & {\color{blue}{}}/{\color{red}{}}/{\color{red}{}}   \\
    Screw        & //                                          & //                                   & //{\color{blue}{}}                                                 & {\color{blue}{}}/{\color{blue}{}}/{\color{red}{}}  & {\color{red}{}}/{\color{red}{}}/{\color{red}{}}    \\
    Toothbrush   & //                                          & //                                   & {\color{blue}{}}//{\color{blue}{}}                        & /{\color{blue}{}}/{\color{blue}{}}                          & {\color{red}{}}/{\color{red}{}}/{\color{red}{}}    \\
    Transistor   & //                                          & //                                  & {\color{blue}{}}//{\color{blue}{}}                        & /{\color{blue}{}}/{\color{red}{}}                           & {\color{red}{}}/{\color{red}{}}/{\color{blue}{}}   \\
    Zipper       & //                                         & //                                  & //                                                                          & {\color{blue}{}}/{\color{red}{}}/{\color{blue}{}}  & {\color{red}{}}/{\color{blue}{}}/{\color{red}{}}   \\\hline
    Average      & //                                         & //                                  & //                                                                          & {\color{blue}{}}/{\color{red}{}}/{\color{blue}{}}  & {\color{red}{}}/{\color{blue}{}}/{\color{red}{}}   \\\hline
    TotalAverage & //                                         & //                                  & //                                                                          & {\color{blue}{}}/{\color{blue}{}}/{\color{blue}{}} & {\color{red}{}}/{\color{red}{}}/{\color{red}{}}    \\
\bottomrule
  \end{tabular}
\end{table*}

\begin{table}[htbp]
\centering
  \caption{
    The AP, PRO and Pixel-AUC scores on BTAD  \cite{mishra2021vt} in the supervised setting.
  }
  \label{table:btad_supervised_result}
  \resizebox{\linewidth}{!}{
    \begin{tabular}{lcccc}
\toprule
      Category & \makecell{BGAD \cite{yao2022explicit} \NL (CVPR2023)}                & \makecell{PRN \cite{zhang2022prototypical} \NL (CVPR2023)} & \makecell{SemiREST \cite{li2023efficient} \NL (arXiv2023)}                                      & Ours                                                                                         \\\hline
           & /                                                       & //                                       & {\color{blue}{}}/{\color{blue}{}}/{\color{blue}{}} & {\color{red}{}}/{\color{red}{}}/{\color{red}{}} \\
           & /{\color{blue}{}}/                        & //                                       & {\color{blue}{}}/{\color{red}{}}/{\color{blue}{}}  & {\color{red}{}}/{\color{red}{}}/{\color{red}{}} \\
           & /{\color{red}{}}/{\color{red}{}} & //{\color{blue}{}}              & {\color{blue}{}}/{\color{blue}{}}/{\color{red}{}}  & {\color{red}{}}//{\color{red}{}}                         \\\hline
      Average  & //                                                 & //                                       & {\color{blue}{}}/{\color{blue}{}}/{\color{blue}{}} & {\color{red}{}}/{\color{red}{}}/{\color{red}{}} \\
\bottomrule
    \end{tabular}
  }
  \vspace{-1.0em}
\end{table}

\subsection{Ablation Study}

To understand the success of CPR, we conducted a detailed ablation study to analyze the
major contributing components of the proposed method. These components are listed as
follows:
\begin{itemize}
  \item
        \textbf{Global Retrieval} process. It is the most basic component of the proposed method.
        If it is not conducted, the algorithm reduces to a standard PatchCore
        \cite{roth2022towards} algorithm.
  \item
        \textbf{FEB} module. Without this module, the final predicted anomaly score map is
         instead of , introduced in
        Algorithm~\ref{alg:infer}.
  \item
        \textbf{Learned LRB}. If it is not employed, one performs the local retrieval on the
        raw feature tensors generated by using DenseNet, as shown in \Cref{equ:raw_fea}.
  \item
        \textbf{Multiscale} strategy. If remove this component from CPR model, only the raw
        feature tensor from ``denseblock-'' of DenseNet model is used. See
        Algorithm~\ref{alg:infer} for details.
  \item
        \textbf{Remote pair weighting} strategy which is introduced in
        \Cref{subsubsec:lrb_train}. The CPR without this component treats all the sampled
        negative pairs equally.
\end{itemize}
The ablation results are summarized in \Cref{table:ablation}. One can see an obvious
increasing trend as the modules are added to the CPR model one by one. The accumulated
performance gains contributed by the modules are  (Image-AUC),  (Pixel-AUC),
 (PRO) and  (AP) respectively.


\begin{table}[htbp]  \centering
  \caption{
    Ablation studies on our main designs: global retrieval, foreground filter, trainable
    LRB, local region retrieval, multiscale and features weight. I, P, R, and O
    respectively refer to the five metrics of Image-AUC, Pixel-AUC, PRO, and AP.
  }
  \label{table:ablation}
  \resizebox{\columnwidth}{!}{
    \begin{tabular}{ccccccccc}
      \toprule
      \multicolumn{5}{c}{Module} & \multicolumn{4}{c}{Performance}                                                                                                                                       \\
      \cmidrule(lr){1-5} \cmidrule(lr){6-9}
      \makecell{GRB}             & \makecell{FEB}                  & \makecell{LRB} & \makecell{Mul. \NL Scale} & \makecell{Pair \NL Weight} & I  & P  & O  & A  \\
\midrule
                                 &                                 &                &                           &                            &        &        &        &        \\
                     &                                 &                &                           &                            &        &        &        &        \\
                     &                     &                &                           &                            &        &        &        &        \\
                     &                                 &    &                           &                            &        &        &        &        \\
                     &                     &    &                           &                            &        &        &        &        \\                 &                     &    &               &                            &        &        &        &        \\
                     &                     &    &               &                &        &        &        &        \\
      \bottomrule
    \end{tabular}
  }
\end{table}

\subsection{Algorithm Speed}

In real-world AD applications, the inspection time budget is usually limited and thus the
running speed is a crucial property of an AD algorithm. In this part of the paper, we test
the time efficiency of the proposed method as well as its two accelerated variations,
namely the ``CPR-Fast'' algorithm and the ``CPR-Faster'' algorithm, respectively. In
specific, CPR-Fast keeps employing DenseNet as the backbone while the dimension of
LRB feature is reduced to  from the original ; CPR-Faster utilizes EfficientNet
\cite{tan2020efficientnet} as its backbone and further reduces the dimension of the LRB feature to .
The running speeds of  SOTA algorithms are also tested and to make a fair comparison,
all the tests are conducted on one machine and mostly based on the official code
published by the authors. In particular, we run each AD algorithm  times on the
image with the fixed size  and only the last  times are used to
estimate the average speed. To further exploit the speed potential of the proposed method, we
also adopt the TensorRT SDK \cite{migacz20178} to reimplement the CPR which is originally coded based
on Pytorch \cite{paszke2019pytorch} and the running speeds of the two frameworks are both measured.

The speeds of different algorithms are presented in \Cref{table:speed}, along with the
corresponding AD performances on the MVTec AD dataset. As we can see, CPR achieves the
highest Pixel-AUC, PRO and AP scores while maintaining a high speed (over  FPS). The
CPR-Fast algorithm reaches the inference speed over  FPS at a trivial cost of accuracy
drop.  Most remarkably, the CPR-Faster method achieves a speed of  FPS and still
outperform other compared SOTA methods except SemiREST \cite{li2023efficient} which requires more than  ms to process an image. In other words, the CPR-Faster algorithm can
realize highly accurate anomaly detection/localization on an image using less than 
millisecond.

\begin{table}[htbp]
\centering  \caption{
    Speed Comparison between the proposed method and current SOTA algorithms on MVTec
    AD.  I, P, R and O respectively refer to the Image-AUC, Pixel-AUC, PRO and AP. The number with symbol  stands for the result obtained by a re-implementation of the algorithm. Note that for CPR and its variations, we show the results with both the PyTorch \cite{paszke2019pytorch} implementation and the TensorRT \cite{migacz20178} version.}
  \label{table:speed}
  \resizebox{\columnwidth}{!}{
    \begin{tabular}{ c | c | c | c | c | c  }
      \hline
      Methods                                     & I                     & P                     & O                     & A                     & \makecell{FPS  \NL (PyTorch/TensorRT)}             \\
      \hline
PatchCore-WRes \cite{roth2022towards}       &                           &                           &                           &                           & /                                                  \\
      DREAM \cite{zavrtanik2021draem}             &                           &                           &                           &                           & /                                                  \\
      RD \cite{deng2022anomaly}                   &                           &                           &                           &                           & /                                                  \\
      NFAD \cite{yao2022explicit}                 &                           &                           &                           &                           & /                                                  \\
      DMAD \cite{liu2023diversity}                &                           &                           &                           &                           & /                                                  \\
      CDO \cite{cao2023collaborative}             &                           &                           &                           &                           & 9/                                                     \\
      SimpleNet \cite{Liu_2023_CVPR}              & {\color{blue}{}} &                           &                           &                           & /                                                  \\
      DeSTSeg \cite{zhang2023destseg}             &                           &                           &                           &                           & /                                                 \\
      EfficientAD-S \cite{batzner2023efficientad} &                           &                           &                           &                           & {\color{blue}{}}/                      \\
      EfficientAD-M \cite{batzner2023efficientad} &                           &                           &                           &                           & /                                               \\
      SemiREST \cite{li2023efficient}             &                           & {\color{red}{}}  &                           &                           & /                                                   \\
      \hline
      CPR                                         & {\color{red}{}}  & {\color{red}{}}  & {\color{red}{}}  & {\color{red}{}}  & /                                                  \\
      CPR-fast                                    & {\color{red}{}}  & {\color{red}{}}  & {\color{blue}{}} & {\color{blue}{}} & /{\color{blue}{}}                         \\
      CPR-faster                                  &                           & {\color{blue}{}} &                           &                           & {\color{red}{}}/{\color{red}{}} \\
      \hline
    \end{tabular}
  }
\end{table}

To better illustrate the advantage of the proposed method, \Cref{fig:speed} depicts the AD
involved AD methods as differently shaped markers with the marker locations indicating the
accuracies and the marker colors indicating the algorithm speeds.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=3.5in]{speed}
  \caption{Inference speed (FPS) versus AP and PRO on MVTec AD benchmark. The algorithm speeds are indicated by the colors (the redder the faster) and sizes (the larger the faster) of the markers. }
  \label{fig:speed}
  \vspace{-2.0em}
\end{figure}

\subsection{Qualitative Comparisons}
\label{subsec:qualitative}

In this section, qualitative AD results are shown for offering more intuitive
understanding of the proposed method.

As a key step of CPR, the global retrieval procedure
select the ``similarly posed'' reference images for the query. \Cref{fig:retrieval}
illustrate the retrieved  nearest neighbors of a typical test image (inside the red rectangle) for each category.
All the object categories in the MVTec AD dataset \cite{Bergmann_2019_CVPR} are involved in this comparison except those sub-categories with well-aligned images.
In \Cref{fig:retrieval}, our CPR method (top row) are compared with the SOTA image retrieval algorithm Unicom \cite{an2023unicom} (middle row) and RegAD \cite{huang2022registration} (bottow row) which is a SOTA AD approach employing STN layers for image alignment.
Note that RegAD is not retreival-based and performs an implicit image alignment for each test image. As a result, we show the test image (inside the red rectangle) and  randomly selected training images, all aligned using the affine transform parameters predicted by the STN layers of RegAD \cite{huang2022registration}.
One can see that global retrieval process of CPR usually leads to a set of
samples which seem ``aligned'' to the test image. In practice, this merit significantly benefits the proposed CPR algorithm for both training and inference. In contrast, the general retrieval algorithm Unicom \cite{Bergmann_2019_CVPR} performs worse than CPR and the alignment-based RegAD \cite{huang2022registration} fails to align all the images to similar poses.

\begin{figure*}[htbp]
  \centering
  \includegraphics[width=6in]{retrieval}
  \caption{
    Global retrieval results on MVTec AD. The proposed GRB (top row) achieves more robust
    retrieval results for various categories of defective objects, comparing with the Unicom
    algorithm \cite{an2023unicom} (middle row) and the RegAD method \cite{huang2022registration} (bottom row).
    The images inside the red rectangles are the test images while the others are the retrieved -NN images.
    Note that for RegAD, the  training image are randomly selected and all the  images are supposed to be aligned to similar poses according to the original paper \cite{huang2022registration}.
  }
  \label{fig:retrieval}
\end{figure*}

Secondly, in \Cref{fig:foreground_vis} we demonstrate the foreground estimation of the proposed FEB module. As it can be seen, the FEB can accurately predict the pixel labels (foreground \emph{vs.} background) even though no manual annotation is available in the training stage.

\begin{figure*}[htbp]
  \centering
  \includegraphics[width=5in]{foreground_vis.pdf}
  \caption{
    The demonstration of the proposed FEB module. All the object categories in MVTec AD (top row) and MVTec-3D AD (bottom row) are illustrated in this figure. Note that instead of the binary map, CPR utilizes the foreground confidence to reduce the influence of the ``false alarms'' in the background region.
  }
  \label{fig:foreground_vis}
\end{figure*}

Finally, the anomaly score maps of our method and  representative AD algorithms are
shown in \Cref{fig:vis}. As we can see from the image, CPR predicts more accurate
score maps in most scenarios.
\begin{figure*}[htbp]
  \centering
  \includegraphics[width=7in]{vis}
  \caption{
    Visualization comparisons examples of anomaly localization on MVTec AD. Our method
    accurately localizes anomalous regions and shows more similar predictions to the ground
    truth.
  }
  \label{fig:vis}
\end{figure*}

\section{Conclusion \& future work}
\label{sec:conclusion}
In this paper, we rethink the inherent ``matching'' nature of anomaly detection and consequently
propose to perform AD in a cascade manner. The former layer of the cascade filters out
most improper reference images while the test patch is matched with the reference set in
the latter layer. This coarse-to-fine retrieval strategy is proved to be remarkably useful
and yields new SOTA AD accuracy as well as a running speed over  FPS with a
moderate simplification. Besides the good performances, this work also sheds light on
the understanding of all the matching-based AD algorithms. According to the empirical
study of this work, the high accuracy usually stems from two crucial factors, namely the
reference set selection and the metric feature learning. In the future, algorithms
can be improved
by finding more sophisticated alternatives to those two key components.
In addition, the running speed of the retrieval-based AD process could be further
increased by introducing the hashing techniques which have wide applications in
fast
retrieval.

\bibliography{strings,ref}
\bibliographystyle{IEEEtran}


\end{document}
