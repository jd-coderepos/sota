\documentclass[11pt]{article}

\usepackage{cite}
\usepackage{fullpage}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{multirow}

\newcommand{\str}{\ensuremath{S} }
\newcommand{\tree}{\ensuremath{T}}
\newcommand{\slp}{\ensuremath{\mathcal{S}} }
\newcommand{\kq}{\ensuremath{k_{\str, q}} }
\newcommand{\qg}{\ensuremath{G_q(\str)} }
\newcommand{\fca}{\ensuremath{\textsc{firstcolor}}}
\newcommand{\lca}{\ensuremath{\textsc{lastcolor}}}
\newcommand{\labsuc}{\ensuremath{\textsc{ls}}}
\newcommand{\la}{\ensuremath{\textsc{la}}}
\newcommand{\leftc}{\ensuremath{\text{\textit{left}}}}
\newcommand{\rightc}{\ensuremath{\text{\textit{right}}}}
\newcommand{\band}{\ensuremath{\wedge}}
\newcommand{\bor}{\ensuremath{\vee}}
\newcommand{\bxor}{\ensuremath{\oplus}}
\newcommand{\hf}{\ensuremath{\mathcal{H}}}
\newcommand{\pat}{\ensuremath{P}}










\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{problem}{Problem}
\newtheorem{prop}{Proposition}
\newcommand{\qed}{\hfill\ensuremath{\Box}\medskip\\\noindent}
\newenvironment{proof}{\noindent\emph{Proof. }}





\title{Compressed Subsequence Matching and Packed~Tree~Coloring}
\author{Philip Bille\thanks{Supported in part by the The Danish Council for Independent Research  Natural Sciences grant DFF Â­1323--00178.} \and Patrick Hagge Cording\0.4em]
\texttt{\small{\{phbi,phaco,inge\}@dtu.dk}} \and Inge Li G\o rtz\footnotemark[1]}




\begin{document}
	
\maketitle

\begin{abstract}
\noindent We present a new algorithm for subsequence matching in grammar compressed strings. Given a grammar of size  compressing a string of size  and a pattern string of size  over an alphabet of size , our algorithm uses  space and  or  time. Here  is the word size and  is the number of occurrences of the pattern. Our algorithm uses less space than previous algorithms and is also faster for  occurrences. The algorithm uses a new data structure that allows us to efficiently find the next occurrence of a given character after a given position in a compressed string. This data structure in turn is based on a new data structure for the tree color problem, where the node colors are packed in bit strings.
\end{abstract}


\section{Introduction}


In the  \textit{compressed subsequence matching problem} we are given a grammar \slp of size  compressing a string \str of size  and a pattern string  of size  over an alphabet of size , and the goal is to find and report the index of all minimal substrings of  that contain  as a subsequence. A substring is said to be minimal if shortening it implies that  is no longer a subsequence of that substring. In this paper we present a new algorithm for compressed subsequence matching which is space efficient and is faster than the previously fastest algorithm for a bounded number of occurrences. Our algorithm relies on a method that is different from the ones used by previous algorithms.



Subsequence matching is useful when searching sequential log data for a sequence of events that may be separated by other events. Say for instance that we are running a webserver and we want to know how often a visitor has found her way to subpage  through page  and then . We then set  and apply a subsequence matching algorithm to the contents of the log file. Many applications will automatically compress log data to save space, and so the bottleneck of the procedure becomes decompression of the data. In this case, processing the data without fully decompressing it, is crucial. Subsequence matching was also considered in relation to knowledge discovery and data mining \cite{mannila1997discovery}.




Several algorithms have been presented for uncompressed strings \cite{mannila1997discovery,das1997episode,boasson1999window,cegielski2006multiple,baeza1991searching,tronicek2001episode,crochemore2003directed}. The fastest of these is due to Das et al.~\cite{das1997episode}. Since it is an online algorithm we may apply it to the compressed version without having to store the entire decompressed string, and we get an algorithm with running time  that uses  space. The first algorithm with time complexity independent of the size of the string was presented by Cegielski~et~al.~\cite{cegielski2006window} in 2006. Its runnning time is  time and it uses  space. Using a different approach, Tiskin improved the running time to  \cite{tiskin2009faster} and later even further to ~\cite{tiskin2011towards}. The space usage of his algorithms is . The most recent improvement is due to Yamamoto~et~al.~\cite{yamamoto2011faster} who present an algorithm based on the ideas of Cegielski~et~al. that runs in  time and  space. All results are summarized in Table 1.

\begin{table}[h]\label{tab:results}
\begin{center}
\begin{tabular}{lll}
	\hline
	Time complexity & Space complexity & Author(s) \\
	\hline
	 &  & Das et al.~\cite{das1997episode} \\
	 &  & Cegielski~et~al.~\cite{cegielski2006window} \\
	 &  & Tiskin~\cite{tiskin2009faster} \\
	 &  & Tiskin~\cite{tiskin2011towards} \\
	 &  & Yamamoto~et~al.~\cite{yamamoto2011faster} \\
	\hline
	 & \multirow{2}{*}{} & \multirow{2}{*}{This paper} \\	
	 & & \\
	\hline
\end{tabular}
\caption{Time and space complexities of algorithms for compressed subsequence matching.}
\end{center}
\end{table}

\noindent Assume without loss of generality that the compressed string is given as a Straight Line Program (SLP). An SLP is an acyclic grammar in Chomsky normal form, i.e., a grammar where each nonterminal production rule expands to two other rules and  generates one string only. SLPs are widely studied because they model many well-known compression schemes, such as LZ77 \cite{lz77}, LZ78 \cite{lz78}, and Re-Pair \cite{larsson2000off}  with little overhead \cite{charikar2005smallest,rytter2003application}. The following theorem is the main result of this work.



\begin{theorem}\label{thm:SCSM}
Given an SLP  of size  compressing a string  of size  and a pattern  of size  over an alphabet of size , compressed subsequence matching can be solved in  words of space and time
\begin{enumerate}
\item[\textit{(i)}] , or
\item[\textit{(ii)}] 
\end{enumerate}
in the word RAM model with word size , and where  is the number of minimal occurrences of  in .
\end{theorem}

\noindent Our new algorithm uses less space (linear in  if ) and is also faster than the previously fastest algorithm for  occurrences when . Note that we can guarantee that the latter requirement always holds by bounding  using hashing in return for using  additional extra space.

The algorithm is based on the idea of a simple algorithm for subsequence matching in uncompressed strings which basically scans the string for occurrences of the pattern. We speed up the scanning on compressed strings by introducing the first data structure for SLPs that supports labelled successor queries. The answer to a labelled succesor query  on a string is the index of the first character  occurring after position  in the string. An essential part of this data structure is a new data structure for the tree color problem. This problem is to preprocess a tree where each node is colored by zero or more colors, such that given a node  and a color , we may efficiently answer a first colored ancestor query, i.e., compute the lowest ancestor of  with color . Additionally, this data structure also supports a new type of query we call the last colored ancestor. Here the query is two nodes  and  and a color , and the answer is the highest node on the path from  to  with color . These results may be of independent interest.





This paper is organized such that we start by describing our new result for the tree color problem (after a section of preliminaries), then move on to the labelled successor data structure, and finally describe the algorithm for subsequence matching.

\section{Preliminaries}



\paragraph{Bit Strings.}
We will use bit strings to represent sets. In a bit string  representing a set  of elements from a universe of size ,  iff element  is in .  denotes the empty set. The operators , , and  denote the bitwise AND, OR, and exclusive OR (XOR) of two bit strings. The negation of a bit string  is . A \emph{summary}  of  bit strings  of equal length is . For a bit string of length  we assume that the mask of any constant can be computed in  time. Given a bit string ,  is the most significant bit. The index of the least significant set bit can be found in  time from . Finding the most significant set bit is more elaborate, but can also be done  time~\cite{fredman1993surpassing}. An  bit matrix may be transposed in  time if  and ~\cite{thorup2002randomized}.

\paragraph{Trees.}
In this paper all trees are rooted, ordered, and have labels on the nodes. The number of nodes in a tree  is . We denote by  the subtree rooted in  containing all descendants of . The size  is the number of nodes in the subtree  including . If  is a node in the subtree  we write . If  is a binary tree we denote the left and right child of a node  by  and . 

A heavy path decomposition \cite{sleator1983data} decomposes  into disjoint paths. Nodes are classified as either heavy or light and the decomposition is defined as follows. The root is light. For each internal node , its heavy child  is the node for which  is of maximum size among the subtrees rooted in children of . The other children of  are light. Edges are also classified as heavy and light. An edge going into a heavy node is heavy and likewise for light nodes. The heavy path decomposition ensures the property that  for any light child  of . This means that there are  light edges on any path from the root to a leaf. The heavy path decomposition can be computed in  time and space.

Given a binary tree  rooted in a node , , and a parameter , we may partition  into at most  clusters such that for a fixed constant , the size of any cluster is at most  \cite{alstrup1997optimal,alstrup1997minimizing} (see also \cite{abiteboul2006compact} for a full proof). Two clusters overlap in at most one node, and a node is called a boundary node if it is part of more than one cluster. Any cluster has at most two boundary nodes, and a boundary node is either a leaf or the root in the subtree that is the cluster. The tree obtained by repeatedly contracting edges between two nodes if one of them is not a boundary node is called the macro tree. In other words, the macro tree is the tree consisting only of boundary nodes. A cluster partition can be found in  time.

The answer to a level ancestor query  on  is the ancestor of  with depth . A linear space data structure that answers an  query in  time can be computed for  in  time~\cite{dietz1991finding} (see also \cite{berkman1994finding,alstrup2000improved,bender2004level}).


\paragraph{Straight Line Programs.}
A Straight Line Program \slp is a context-free grammar in Chomsky normal form with  production rules that unambigously derives a string  of length . We represent the SLP as a rooted, ordered, and node-labelled directed acyclic graph (DAG) with outdegree  and we will refer to production rules as nodes in the DAG. A depth-first left-to-right traversal starting from a node  in the DAG produces the string  of length . The tree that emerges from the traversal we call the derivation tree. We denote the left and right children of  for  and , respectively. Furthermore, the height of the SLP is the length of the longest path going from the root to a terminal node and is denoted by .




We may access a character  in  time by storing  for each node  in the SLP, and simulate a top-down search of the derivation tree. Doing so yields a unique path from the root of  to the terminal node labelled . There is also a linear space data structure that supports random access in SLPs in  time~\cite{bille2011random}. A key technique used in this data structure is the extension of the heavy path decomposition of trees to SLPs which we will also use in our data structure. For each node , we select the child of  that derives the longest string to be a heavy node. The other child is light. Heavy and light edges are defined as in the decomposition of trees. Whereas applying this technique to a tree results in a decomposition into disjoint paths, it will result in a decomposition into disjoint trees when applied to an SLP. We denote this set of trees by the heavy forest  of the SLP. This decomposition ensures that the number of light edges on any path from the root to a terminal node is . Hence, on any path from the root of the SLP to a terminal node, we visit at most  trees from . When accessing a character using the data structure of \cite{bille2011random} we may also report the entry and exit nodes for each tree visited on the unique root-to-terminal path that emerges from the query.

\section{Packed Tree Color Problems}
In a colored tree, each node is colored by zero or more colors from the set . A packed colored tree is a colored tree where the colors of each node  is given as a bit string  where  iff  is colored . In this section we consider the \textit{packed tree color problem} which is to preprocess a packed colored tree  to support  first and last colored ancestor queries. The answer to a first colored ancestor query  is the lowest ancestor of  with color , and the answer to a last colored ancestor query  is the highest node with color  on the path from  to , where we always assume that  is an ancestor of . Throughout this section we will use the following notation to distinguish results. If a data structure requires  time to build, uses  space, and supports  and  queries in  time, then the the triple  refers to the solution.

Solutions to the tree color problem for trees that are not packed may be applied to packed trees. All known solutions focus entirely on supporting  queries \cite{dietz1991finding,muthukrishnan1996time,ferragina1996efficient,alstrup1998marked}. A simple solution that supports  queries in  time is to store the answer for every color in every node. This yields a  solution. The currently best known trade-off for the tree color problem is   \cite{muthukrishnan1996time}, where  is the accumulated number of colors used. 

Our motivation for revisiting this problem is twofold. First we have that  in our application and we are striving for a space bound that is in . Second we want to support  queries. 

In this section we present three solutions to the packed tree coloring problem and combine them to a data structure with a new and desireable time-space trade-off.



\subsection{A  Solution}


We store the result of a  query for every node and color. For each color, let the induced -colored subtree be the tree obtained by deleting all nodes that are not colored by color  except the root. Build a levelled ancestor data structure for each induced colored subtree.

The result of a  query is precomputed. A  query is answered as follows. If  then there is not a node with color  on the path from  to . If  then let  and  be the nodes corresponding to  and  in the induced -colored subtree. The answer to  is then the answer to  in the induced -colored subtree.

The results of  queries can be found and stored using  time and space. The induced colored subtrees can be computed in  time and use  space. A  query clearly takes  time. For a  query, we perform two  queries and one  query, each of which takes constant time.

\begin{lemma}
The packed tree color problem can be solved using  preprocessing time and space, and  query time.
\end{lemma}


\subsection{A  Solution}




We fix a heavy path decomposition of . For each path  in the heavy path decomposition of  we build a balanced binary tree  having the nodes of  as leaves. For each node  in  we store a summary  of the colors of its children. For each heavy path , where  is the highest node on the path, we store a summary  of colors on the path prefix  for every  on . 

For answering a  query, let  be the heavy path containing  and let  for some . If  we find the lowest ancestor  of  in  for which  and . The answer to the query is then the rightmost leaf in  with color . If  we repeat the procedure with , i.e., we jump to the previous heavy path, until we find the first colored ancestor or we reach the root of .

A  query is handled in a similar way. We first find the highest light node  on the path from  to  for which . Let  be the heavy path containing . Now there are three cases. If  is not on , the answer to the query is the leftmost leaf in  that has color . If  contains , the answer is the leftmost leaf with color  to the right of  in , if such a node exists. If it does not exist, we repeat the first step for the second highest light node  between  and  for which .

The heavy path decomposition of  can be found and stored in  time and space. Since the paths of the heavy path decomposition are disjoint, the total number of leaves in the binary summary trees is , so the total number of nodes in the trees is . We store  summary bit vectors of size  using a total of  space. We use  bitwise OR operations to create the summaries in a bottom up fashion. In total, preprocessing time and space usage is .

For both queries we visit at most  heavy paths. When the path with the answer has been found we walk up the binary tree and then down again. Since the tree is balanced and has at most  leaves, this takes  time. For  queries we do this at most twice. The query time for  and  queries is therefore  time.

\begin{lemma}
The packed tree color problem can be solved using  preprocessing time and space, and  query time.
\end{lemma}

\subsection{A  Solution}



Let  be the nodes of  in pre-order. We will represent  as a  bit matrix . Let  be a color from the set of colors . In row  of  we store a bit string where bit  is \texttt{1} iff  has color . For each node  we also store a bit string  where bit  is \texttt{1} iff  is an ancestor of .

We construct this data structure from a packed colored tree as follows. Assume that the bit strings representing the node colorings form a  matrix where row  is the colorings of node . We transpose this matrix to get . To do this we partition the matrix into a  matrix (assume w.l.o.g. that  divides  and ), transpose each  submatrix as described in \cite{thorup2002randomized}, and transpose the  matrix to get . To compute the ancestor bit strings first set . For all other nodes , where  is the parent of , set .

We answer a  as follows. Let . Now  is a bit string representing the set of ancestors of  with color . Since the nodes have pre-order indices, the answer to the query is , where  is the index of the least significant set bit in . 

To answer a  query we start by computing  the same way as above. We then set the first  bits of  to \texttt{0}, where  is the index of . The answer to the query is the most significant set bit of .

The  bit matrix  can be packed in words and therefore uses  space. The same is evident for the ancestor bit strings. Transposing a  matrix takes  time, and since there are  submatrices of this size in the color bit matrix, the total time spent for all submatrices is . Transposing the  matrix takes  time. Computing the ancestor bit strings clearly takes  time.

The size of  is , so finding the first non-zero word takes  time. Determining the least or most significant set bit of a word is done in  time. Thus, the query time for both a  and a  query is .

\begin{lemma}
The packed tree color problem can be solved using  preprocessing time,   space, and  query time.
\end{lemma}

\subsection{Combining the Solutions}
We now show how to combine the previously described solutions to get  and  trade-offs. This is achieved by doing a cluster partioning of the tree.

First we convert  to a binary tree . Then we partition  into  clusters, i.e., each cluster has size . For each cluster , where one boundary node is a leaf in the cluster and the other is the root of the cluster, we make a summary of the colors of the nodes on the path from the root to the leaf. The summary is stored in the macro tree node that corresponds to the leaf boundary node of . Apply the  solution to the macro tree, and apply either the  solution or the  solution to each cluster using the original colors.

Here is how we answer a  query. Let  be the cluster containing . First we ask for  in . If the answer is a node in , we are done. If it is undefined, we find the node  in the macro tree corresponding to the root of . We check if  has color  in the macro tree and otherwise ask for  in the macro tree. In the cluster  having  as a leaf boundary node we then check if  has color  and otherwise ask for  in .

We answer a  query as follows. Assume that  and let  and  be the clusters containing  and . If  then the answer is  in the cluster containing  and . If , let  be the leaf boundary node of  where . We now proceed in three steps. First, we ask for  in . If the query returns a node, this is also the answer to the  query. If the answer in the first step is undefined we ask for  in the macro tree to locate the highest cluster with a node with color  between  and . The answer to the query is then  on . If the first two steps fail, the answer to a query is .

The cluster partition can be computed in linear time, and the cluster path summaries are computed in  time. Since the macro tree has  nodes the preprocessing time and space to apply the  solution becomes . To answer a query we perform a constant number of  and  queries on the macro tree and clusters. Therefore the total time to perform queries on the macro tree is  time. To get  we apply the  solution to clusters. Since a cluster has size  we use a total of  time performing queries on clusters. To get  we apply the   solution to clusters. Again, since clusters have size  we use a total of  time performing queries on clusters. Preprocessing time and space for the cluster data structures follow because .

\begin{theorem}\label{thm:PTC}
The packed tree color problem can be solved using  space,
\begin{enumerate}
	\item[\textit{(i)}]  preprocessing time, and  query time, or
	\item[\textit{(ii)}]  preprocessing time, and  query time.
\end{enumerate}
\end{theorem}


\section{Labelled Successor Data Structure for SLPs}
The answer to a labelled successor \labsuc query on a string  is the index of the first occurrence of the character  after position  in . More formally, the answer to \labsuc is an index  such that , , and  for . 

In this section we present a data structure that supports  queries on an SLP.  This is the first data structure dedicated to solving this problem on SLPs. Alternatively, we may build the random access data structure of \cite{bille2011random} and then answer an  query by doing a random access query for position  followed by a linear scan to find the first occurrence of . This yields a query time of  while using  space for the data structure. 

Our data structure combines the random access data structure of \cite{bille2011random} with a new way of navigating the SLP based on the characters of substrings. For the latter we will utilize our result for the packed tree color problem described in the previous section.

The basic idea is to store a bit string for each node  that summarizes which characters that are generated by . We first seach for position  in  and let  be the unique path in  defining . We then walk up  until reaching a node  where  generates a string that contains  and  is not on . Then we walk down from  using the summaries to locate the leftmost terminal descending from  that generates . This algorithm requires  space and  time to find .

To speed things up we fix a heavy path decomposition of the SLP to get a heavy forest and build the random access data structure of \cite{bille2011random}. Now  is a sequence of entry and exit points in the trees of the heavy forest. When we walk up  we enter a tree in an exit node and have to walk away from the root to the first node whose right child generates a string that contains  before reaching the entry node. This is equivalent to a  query. When we walk down to find  we enter a tree and have to walk towards the root to find either the first ancestor whose left child generates a string that contains  or the highest ancestor whose right child generates . This is equivalent to a  and a  query, respectively.

In the remainder of this section we give the details of the data structure.


\begin{theorem}\label{thm:LSDS}
There is a data structure supporting labelled successor (and predecessor) queries on a string of size  over an alphabet of size  compressed by an SLP of size  in the word RAM model with word size  using  space and

\begin{enumerate}
	\item[\textit{(i)}]  preprocessing time, and  query time, or
	\item[\textit{(ii)}]  preprocessing time, and  query time.
\end{enumerate}
\end{theorem}

\begin{proof} We first apply the construction of \cite{bille2011random}, and let  be the heavy forest obtained from the heavy path decomposition of . For each node  in  with children  and  we store two bit strings  and  summarizing the characters in  and . If  and  are in the same tree in  then  and similarly for  and . For each tree in  we build two data structures for the packed tree color problem.  One where the  bit strings serve as colors and one where the  bit strings serve as colors.

We answer an  query as follows. First we access the character  using the random access data structure. We now have the entry and exit points of the heavy trees in  on the unique path  describing . Let  be a sequence of trees on  in the order they are visited when starting from the root and ending in the terminal generating , and let  be the entry and exit nodes for each tree in the sequence. Using the packed tree color data structure for the  colors, we repeat  for  down to some  until  is not undefined. Let . We now search for the first occurrence of  in . Let  be the tree in  that contains the node , then the search proceeds in three steps. First, we ask for  in  in the data structure for  colors and restart the search from . If the query  is undefined we continue to the next step. In the second step we check if  generates . If it does, we now have a unique set of entry and exit nodes in the trees of  that constitutes a path to a terminal that generates the first  after position . The answer to the  query is the index of this  which we retrieve using the random access data structure. Finally, if  does not generate  we ask for  in  in the data structure for  colors, and restart the search from .

The data structure uses  space because the random access data structure uses linear space and the bit strings  and  use  space. The random access data structure, including the heavy path decomposition, takes  time to compute and the  and  values are computed using  OR operations in a bottom up fashion. Therefore, this part of the data structure is computed in  time.

To get Theorem \ref{thm:LSDS} (i) we use the packed tree color data structure of Theorem \ref{thm:PTC} (i) for the trees in  and likewise for (ii). Since the trees are disjoint, the preprocessing time and space becomes as in the Theorem \ref{thm:LSDS}.

For the query, we first do one random access query that takes  time, then we perform at most   queries walking up the SLP and at most   and  queries locating the labelled successor. Finally, retrieving the index also takes  time using the random access data structure.   \qed
\end{proof}

\section{Subsequence Matching}
We will now use the labelled successor data structure to obtain a subsequence matching algorithm for SLPs. Our algorithm is based on the folklore algorithm for subsequence matching  which works as follows (see also \cite{mannila1997discovery,das1997episode}). First we find the minimal prefix  that contains  as a subsequence. This is done by reading  left to right while searching for the characters of  one at a time. We then find the minimal suffix  of the prefix  that contains . Similarly, this is done by scanning the prefix right to left. Now  is the first minimal occurrence of . To find the next minimal occurrence we repeat this process for the suffix . It can be shown that this algorithm finds all minimal occurrences of  in  time.

By using our labelled successor data structure described in the previous section we speed up the procedure of finding some specific character of . Assume we have matched  to  such that . Instead of doing a linear scan of  to find  we ask for the next occurrence of  using .

For each occurrence of  we perform  labelled successor (and labelled predecessor) queries, and we also have to construct the data structures to support these. By applying the results of Theorem \ref{thm:LSDS} we get Theorem \ref{thm:SCSM}.




\bibliographystyle{abbrv}
\bibliography{references}


\end{document}