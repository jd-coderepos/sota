\documentclass{llncs}
\usepackage{xcolor} 
\usepackage{tikz-qtree}
\usepackage{etex}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{proof}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{comment}
\usepackage{subcaption}
\usepackage{fancybox}
\usepackage{stmaryrd}

\usepackage{todonotes}
\newcommand{\knote}[1]{\todo[inline, color=blue!20]{#1}}
\newcommand{\tom}[1]{\todo[inline, color=red!30]{#1}}
\definecolor{light-gray}{gray}{0.86}
\newcommand{\nil}[0]{\mathsf{nil}} 
\newcommand{\cons}[0]{\mathsf{cons}} 
\newcommand{\vecc}[0]{\mathsf{vec}} 
\newcommand{\suc}[0]{\mathsf{S}} 
\newcommand{\nat}[0]{\mathsf{Nat}} 
\newcommand{\ind}[0]{\mathsf{Ind}} 
\newcommand{\app}[0]{\mathsf{app}} 
\newcommand{\selfstar}[0]{} 
\newcommand{\self}[0]{\mathbf{S}} 
\newcommand{\fomega}[0]{\mathbf{F}_{\omega}} 
\newcommand{\cc}[0]{\mathbf{CC}} 
\newcommand{\gray}[1]{\colorbox{light-gray}{#1}}
\newcommand{\mgray}[1]{\colorbox{light-gray}{}}
\newcommand{\M}[3]{
\{#1_i \mapsto #2_i\}_{i \in #3}} 
\newcommand{\Mb}[4]{
\{#1_i:#4_i \mapsto #2_i\}_{i \in #3}} 
\newcommand{\interp}[1]{\llbracket #1 \rrbracket} 
\newcommand{\mylet}[2]{\mathbf{let}\ #1 \ \mathbf{in} \ #2} 
\newcommand{\fix}[1]{\mathbf{fix}(#1)} 
\newcommand{\mycase}[2]{\mathbf{case}\ #1 \ \mathbf{of} \ \{ #2 \}_{i\in {N}}} 
\newcommand{\bm}[4]{
\{(#1_i:#2_i) \mapsto #3_i\}_{i \in #4}} 
\newcommand{\frank}[1]{\textcolor{blue}{\textbf{[#1 ---F.P.]}}}
\newcommand{\remph}[1]{\textcolor{blue}{#1}}


\makeatletter
\@ifundefined{lhs2tex.lhs2tex.sty.read}{\@namedef{lhs2tex.lhs2tex.sty.read}{}\newcommand\SkipToFmtEnd{}\newcommand\EndFmtInput{}\long\def\SkipToFmtEnd#1\EndFmtInput{}}\SkipToFmtEnd

\newcommand\ReadOnlyOnce[1]{\@ifundefined{#1}{\@namedef{#1}{}}\SkipToFmtEnd}
\usepackage{amstext}
\usepackage{amssymb}
\usepackage{stmaryrd}
\DeclareFontFamily{OT1}{cmtex}{}
\DeclareFontShape{OT1}{cmtex}{m}{n}
  {<5><6><7><8>cmtex8
   <9>cmtex9
   <10><10.95><12><14.4><17.28><20.74><24.88>cmtex10}{}
\DeclareFontShape{OT1}{cmtex}{m}{it}
  {<-> ssub * cmtt/m/it}{}
\newcommand{\texfamily}{\fontfamily{cmtex}\selectfont}
\DeclareFontShape{OT1}{cmtt}{bx}{n}
  {<5><6><7><8>cmtt8
   <9>cmbtt9
   <10><10.95><12><14.4><17.28><20.74><24.88>cmbtt10}{}
\DeclareFontShape{OT1}{cmtex}{bx}{n}
  {<-> ssub * cmtt/bx/n}{}
\newcommand{\tex}[1]{\text{\texfamily#1}}	

\newcommand{\Sp}{\hskip.33334em\relax}


\newcommand{\Conid}[1]{\mathit{#1}}
\newcommand{\Varid}[1]{\mathit{#1}}
\newcommand{\anonymous}{\kern0.06em \vbox{\hrule\@width.5em}}
\newcommand{\plus}{\mathbin{+\!\!\!+}}
\newcommand{\bind}{\mathbin{>\!\!\!>\mkern-6.7mu=}}
\newcommand{\rbind}{\mathbin{=\mkern-6.7mu<\!\!\!<}}\newcommand{\sequ}{\mathbin{>\!\!\!>}}
\renewcommand{\leq}{\leqslant}
\renewcommand{\geq}{\geqslant}
\usepackage{polytable}

\@ifundefined{mathindent}{\newdimen\mathindent\mathindent\leftmargini}{}

\def\resethooks{\global\let\SaveRestoreHook\empty
  \global\let\ColumnHook\empty}
\newcommand*{\savecolumns}[1][default]{\g@addto@macro\SaveRestoreHook{\savecolumns[#1]}}
\newcommand*{\restorecolumns}[1][default]{\g@addto@macro\SaveRestoreHook{\restorecolumns[#1]}}
\newcommand*{\aligncolumn}[2]{\g@addto@macro\ColumnHook{\column{#1}{#2}}}

\resethooks

\newcommand{\onelinecommentchars}{\quad-{}- }
\newcommand{\commentbeginchars}{\enskip\{-}
\newcommand{\commentendchars}{-\}\enskip}

\newcommand{\visiblecomments}{\let\onelinecomment=\onelinecommentchars
  \let\commentbegin=\commentbeginchars
  \let\commentend=\commentendchars}

\newcommand{\invisiblecomments}{\let\onelinecomment=\empty
  \let\commentbegin=\empty
  \let\commentend=\empty}

\visiblecomments

\newlength{\blanklineskip}
\setlength{\blanklineskip}{0.66084ex}

\newcommand{\hsindent}[1]{\quad}\let\hspre\empty
\let\hspost\empty
\newcommand{\NB}{\textbf{NB}}
\newcommand{\Todo}[1]{\textbf{To do:}~#1}

\EndFmtInput
\makeatother
\ReadOnlyOnce{polycode.fmt}\makeatletter

\newcommand{\hsnewpar}[1]{{\parskip=0pt\parindent=0pt\par\vskip #1\noindent}}

\newcommand{\hscodestyle}{}



\newcommand{\sethscode}[1]{\expandafter\let\expandafter\hscode\csname #1\endcsname
   \expandafter\let\expandafter\endhscode\csname end#1\endcsname}



\newenvironment{compathscode}{\par\noindent
   \advance\leftskip\mathindent
   \hscodestyle
   \let\\=\@normalcr
   \par\noindent
   \ignorespacesafterend}

\newcommand{\compaths}{\sethscode{compathscode}}



\newenvironment{plainhscode}{\hsnewpar\abovedisplayskip
   \advance\leftskip\mathindent
   \hscodestyle
   \let\\=\@normalcr
   \hsnewpar\belowdisplayskip
   \ignorespacesafterend}



\newcommand{\plainhs}{\sethscode{plainhscode}}
\plainhs



\newenvironment{arrayhscode}{\hsnewpar\abovedisplayskip
   \advance\leftskip\mathindent
   \hscodestyle
   \let\\=\@normalcr
   \hsnewpar\belowdisplayskip
   \ignorespacesafterend}

\newcommand{\arrayhs}{\sethscode{arrayhscode}}



\newenvironment{mathhscode}{\parray}{\endparray}

\newcommand{\mathhs}{\sethscode{mathhscode}}



\newenvironment{texthscode}{}

\newcommand{\texths}{\sethscode{texthscode}}



\def\codeframewidth{\arrayrulewidth}
\RequirePackage{calc}

\newenvironment{framedhscode}{\parskip=\abovedisplayskip\par\noindent
   \hscodestyle
   \arrayrulewidth=\codeframewidth
   \tabular{@{}|p{\linewidth-2\arraycolsep-2\arrayrulewidth-2pt}|@{}}\hline\framedhslinecorrect\\{-1.5ex}\let\endoflinesave=\\
   \let\\=\@normalcr
   \framedhslinecorrect\endoflinesave{.5ex}\hline
   \endtabular
   \parskip=\belowdisplayskip\par\noindent
   \ignorespacesafterend}

\newcommand{\framedhslinecorrect}[2]{#1[#2]}

\newcommand{\framedhs}{\sethscode{framedhscode}}



\newenvironment{inlinehscode}{1][]{}\def\fromto##1##2##3{##3}\def\nextline{}}{\blanklineskip]\>[3]{}\mathbf{data}\;\Conid{HPTree}\;\Varid{f}\;\Varid{a}\mathrel{=}\Conid{HPLeaf}\;\Varid{a}\mid \Conid{HPNode}\;(\Varid{f}\;(\Varid{a},\Varid{a})){}\<[E]\ColumnHook
\end{hscode}\resethooks
These two datatypes give rise to the following \ensuremath{\Conid{Eq}} type class instances.
\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}\column{3}{@{}>{\hspre}l<{\hspost}@{}}\column{5}{@{}>{\hspre}l<{\hspost}@{}}\column{16}{@{}>{\hspre}c<{\hspost}@{}}\column{16E}{@{}l@{}}\column{19}{@{}>{\hspre}c<{\hspost}@{}}\column{19E}{@{}l@{}}\column{20}{@{}>{\hspre}l<{\hspost}@{}}\column{22}{@{}>{\hspre}l<{\hspost}@{}}\column{31}{@{}>{\hspre}c<{\hspost}@{}}\column{31E}{@{}l@{}}\column{34}{@{}>{\hspre}l<{\hspost}@{}}\column{E}{@{}>{\hspre}l<{\hspost}@{}}\>[3]{}\mathbf{instance}\;\Conid{Eq}\;(\Varid{h}\;(\Conid{Mu}\;\Varid{h})\;\Varid{a})\Rightarrow \Conid{Eq}\;(\Conid{Mu}\;\Varid{h}\;\Varid{a})\;\mathbf{where}{}\<[E]\\
\>[3]{}\hsindent{2}{}\<[5]\>[5]{}\Conid{In}\;\Varid{x}\equiv \Conid{In}\;\Varid{y}{}\<[19]\>[19]{}\mathrel{=}{}\<[19E]\>[22]{}\Varid{x}\equiv \Varid{y}{}\<[E]\\blanklineskip]\>[3]{}\kappa_\Conid{PTree}\mathbin{::}(\Conid{Eq}\;\Varid{a},\Conid{Eq}\;(\Conid{PTree}\;(\Conid{Pair}\;\Varid{a}\;\Varid{a}))\Rightarrow \Conid{Eq}\;(\Conid{PTree}\;\Varid{a}){}\<[E]\\
\>[3]{}\kappa_\Conid{Pair}\mathbin{::}(\Conid{Eq}\;\Varid{a},\Conid{Eq}\;\Varid{b})\Rightarrow \Conid{Eq}\;(\Conid{Pair}\;\Varid{a}\;\Varid{b}){}\<[E]\ColumnHook
\end{hscode}\resethooks
 In Haskell, the resolution of \texttt{Eq (DList Int)} requires the resolution of \texttt{Eq
(DList (DList Int))}, which requires the resolution of \texttt{Eq (DList (DList
   (DList Int)))} and so on.

 In LP, this example would correspond to 
 program  below:\\ \\
 .


 Likewise, program   will give rise to infinite derivations in CoLP:
  \\
 Here, cycle detection will not help, as there will be a derivation branch
 
 that does not exhibit a cycle.

 These few examples illustrate that, in order to make further progress with richer cases of
 corecursive computations by resolution, we must first establish a more general understanding
 of the coinductive proof principles for resolution, and in particular the methods of formation of coinductive hypotheses.

In order to develop and implement this method, we have to introduce several new methods. Firstly, we still need to 
answer the question (ii). In Section~\ref{inv}, we introduce a heuristic method for generating the coinductive hypotheses, by observing invariant patterns in a resolution tree arising from a given
query. In Section~\ref{corec}, we introduce a method that proves the generated coinductive lemmas coinductively sound.
Finally, in order to apply these methods in type class resolution in Haskell, we develop, in Sections~\ref{}, a method for viewing derivations by resolution
through the prism of simply-typed calculus.

 It has been noticed  by Girard~\cite{Girard:1989}, that resolution rule   can be expressed by means of 
the cut rule in  intuitionistic sequent calculus:
.
Although the resolution rule is classically equivalent to the cut rule, 
the cut rule is better suited for performing computation 
and at the same time preserving constructive content. In Section\ref{} we present
a type system reflecting this intuition: if  is a proof of   and  is a proof of , then  is a proof of .  Thus, a proof can be recorded alongside with each cut rule. 

*For Fu Peng to tie it up with dictionary construction... Perhaps re-phrase the above*

*Check all sections are accounted for once we are done*

*Also, make sure to explain the difference between the STLC used here and the one introduced in our LOPSTR15 paper*













Haskell's type class mechanism was initially conceived to provide a structured
approach to adhoc overloading. However, over the years and aided by various
extensions they have exceeded this initial scope to become a powerful mechanism
for type-directed computation and implicit code generation.

The standard implementation of type classes consists of two phases. \emph{At compile
time}, the type checker derives a so-called type class constraint for every use
of a type class method. Such a constraint is in fact a proof obligation that
is discharged by means of resolution against Horn clauses that come in the form of
type class instances. The type checker distills witnesses from the proofs in the form
of function dictionaries. These dictionaries are passed \emph{at runtime} to provide
implementations for the overloaded type class methods.

Of course it is essential that the resolution process, and thus the compiler,
terminates. To ensure this property stringent conditions have been
imposed on type class instances~\cite{SulzmannDJS07}. Unfortunately, these were found
to be overly restrictive as they rule out many interesting applications.
Hence, the GHC Haskell compiler provides various language extensions (e.g.,
{\tt UndecidableInstances} and {\tt FlexibleContexts}) to lift the stringent
conditions. With these extensions type inference is no longer guaranteed to
terminate and becomes the programmer's responsability.

In order to relax a little the termination requirement and to support their
approach to datatype-generic programming, L{\"a}mmel and Peyton
Jones~\cite{Lammel:2005} have adapted the type class resolution mechanism to
permit cycles. Consider the following mutually recursive definitions of even
and odd lists and their \texttt{Eq} type class instances.
\begin{tabbing}\tt
~~~~~data~OddList~a~~~\char61{}~~Nil~\char124{}~OCons~a~\char40{}EvenList~a\char41{}\\
\tt ~~~~~data~EvenList~a~~\char61{}~~ECons~a~\char40{}OddList~a\char41{}\\
\tt ~\\
\tt ~~~~~instance~\char40{}Eq~a\char44{}~Eq~\char40{}EvenList~a\char41{}\char41{}~\char61{}\char62{}~~Eq~\char40{}OddList~a\char41{}~where\\
\tt ~~~~~~~Nil~~~~~~~~~\char61{}\char61{}~~Nil~~~~~~~~~~\char61{}~~True\\
\tt ~~~~~~~OCons~x~xs~~\char61{}\char61{}~~OCons~y~ys~~~\char61{}~~x~\char61{}\char61{}~y~\char38{}\char38{}~xs~\char61{}\char61{}~ys\\
\tt ~~~~~~~\char95{}~~~~~~~~~~~\char61{}\char61{}~~\char95{}~~~~~~~~~~~~\char61{}~~False\\
\tt ~\\
\tt ~~~~~instance~\char40{}Eq~a\char44{}~Eq~\char40{}OddList~a\char41{}\char41{}~\char61{}\char62{}~Eq~\char40{}EvenList~a\char41{}~where\\
\tt ~~~~~~~ECons~x~xs~~\char61{}\char61{}~~ECons~y~ys~~~\char61{}~~x~\char61{}\char61{}~y~\char38{}\char38{}~xs~\char61{}\char61{}~ys\char58{}
\end{tabbing}
Thanks to cycle detection the resolution of \texttt{Eq (OddList Int)}
terminates and yields a recursive dictionary as witness.


Yet, in more advanced scenarios, cycle detection is not strong enough to obtain
termination.  Indeed, consider the following datatype and equality instance:
\begin{tabbing}\tt
~~~~~data~DList~a~\char61{}~Nil~\char124{}~Cons~a~\char40{}DList~\char40{}DList~a\char41{}\char41{}\\
\tt ~\\
\tt ~~~~~instance~\char40{}Eq~a\char44{}~Eq~\char40{}DList~\char40{}DList~a\char41{}\char41{}\char41{}~\char61{}\char62{}~Eq~\char40{}DList~a\char41{}~where\\
\tt ~~~~~~~Nil~~~~~~~\char61{}\char61{}~Nil~~~~~~~~~~\char61{}~True\\
\tt ~~~~~~~Cons~x~xs~\char61{}\char61{}~Cons~y~ys~~~~\char61{}~x~\char61{}\char61{}~y~\char38{}\char38{}~xs~\char61{}\char61{}~ys\\
\tt ~~~~~~~\char95{}~~~~~~~~~\char61{}\char61{}~\char95{}~~~~~~~~~~~~\char61{}~False
\end{tabbing}
The resolution of \texttt{Eq (DList Int)} requires the resolution of \texttt{Eq
(DList (DList Int))}, which requires the resolution of \texttt{Eq (DList (DList
(DList Int)))} and so on. The process diverges without cycling. In other words, the
type checker does not terminate.

In this paper we address the above divergence issue. We show that there is a
perfectly reasonable corecursive dictionary that witnesses the \texttt{DList}
proof obligation. Moreover, we provide a general resolution mechanism, based on the techniques for co-inductive
proof search introduced by Fu and Komendantskaya
\cite{DBLP:journals/corr/FuK15} and Komendantskaya \etal
\cite{KomendantskayaEtAl15}, that derives the witness in finite time. Key to our approach is to infer
a co-inductive hypothesis, e.g., in the case of the \texttt{DList} example, we infer:

which we use to discharge the constraint {\tt Eq (DList (DList (DList
a)))}. 



Specifically, our contributions are:\todo{reformulate contributions}
\begin{itemize}
	\item We separate the concerns of type checker in the simplification
		phase into three stages; we identify co-inductive invariant
		search, co-inductive hypothesis synthesis; and hypothesis proof.
	\item We describe a new technique for each of the stages that is
		improving existing technology.
	\item We combine the presented techniques such that type checing is
		sound and terminating as opposed to the previous
		state-of-the-art where the termination is not guaranteed.
\end{itemize}

\end{comment}



















\begin{comment}
\section{Old Overview}
\knote{I have started to clean up formatting and grammar. E.g. citations were given inconsistently with and without spaces}

Type class and its extensions~\cite{wadler1989make,kaes1988parametric,jones2003qualified}, have found many useful applications, describing monad~\cite{peyton1993imperative},
implementing quickcheck~\cite{claessen2011quickcheck}. It is very desirable to construct all the evidences at compile time,
\knote{desirable for whom? do something with this sentence}
and report unconstructable evidence as error. To achieve
this while maintaining reasonable flexiblity requires a lot of
careful considerations on the syntactic form of instance declaration and the evidence construction process. The subtleties of the issues and design decisions of 
type class in Haskell are well documented in \cite{Jones97}. For example, Haskell 98 imposes the restriction that each instance declaration must be obviously terminates,
\knote{firstly -- grammar: do you mean ``must obvioulsy terminate''? but obviously is a strange term in this context... ``Should be structurally terminating''?}
i.e. for each instance declaration \texttt{C1, ..., Cm => H},
the \texttt{Ci} must be of the form \texttt{C a1 ... an}, where \texttt{ai} are type variables. With all the good intentions of this restriction, sometimes these restrictions still gets in the way of programming,
\knote{what does it mean -- ``get on the way of programming''? -- reformulate}
for example, a declaration \texttt{Eq a, Eq (List (Tree a)) => Eq (Tree a)} will be banned
\knote{what does it mean banned? rejected by compiler? -- be precise}
even though it looks like  a natural declaration for equality class for 
the type \texttt{data Tree a = Node a (List (Tree a))}. There are extensions that aim to relax some part of the Haskell 98 requirements and to extend the evidence construction mechanism to support circular evidence, the following is an example (From \cite{Lammel:2005}).

{\footnotesize
\begin{tabbing}\tt
~instance~Data~SizeD~t~\char61{}\char62{}~Size~t~where~\char46{}\char46{}\char46{}\\
\tt ~instance~Sat~\char40{}cxt~Char\char41{}~\char61{}\char62{}~Data~cxt~Char~where~\char46{}\char46{}\char46{}\\
\tt ~instance~Size~t~\char61{}\char62{}~Sat~\char40{}SizeD~t\char41{}~where~\char46{}\char46{}\char46{}
\end{tabbing}
}

\noindent If we want to see whether the evidence of \texttt{Size Char} is constructable, we
need to make sure evidence for \texttt{Data SizeD Char} is constructable, which requires
evidence for \texttt{Sat (SizeD Char)}, which requires \texttt{Size Char}. Now we are back
to where we started.
\knote{back where we started is confusing -- as it makes one feel we have made a circle in argumentation, but that is in fact a cycle in inference... }
In \cite{Lammel:2005}, evidence construction process is extended with cycle
aware mechanism.  The evidence to be constructed is assumed to be already given, and
the construction process proceeds as usual, possibly using the assumed ``hypothesis'' coinductively.
\knote{changed the half sentence above, -- check me if ok?}
In the example above, the evidence of \texttt{Size Char} will be assumed  already given, when the evidence construction process reach \texttt{Size Char} again,  the construction process will terminate and construct a circular evidence (the knot is tied)\footnote{This approach is implemented by GHC(Glasgow Haskell Compiler) in the \texttt{UndecidableInstances} flag. }. 

\begin{figure}[htbp]
  \makebox[\textwidth]{\hrulefill}{
    \footnotesize
\begin{tabbing}\tt
~data~Comp~f~g~a~\char61{}~Comp~\char40{}f~\char40{}g~a\char41{}\char41{}\\
\tt ~data~Pair~a~\char61{}~Pair~a~a\\
\tt ~data~GSeqF~a~r~\char61{}~Nil~\char124{}\char124{}~Cons~a~r\\
\tt ~data~Fix~f~g~\char61{}~Fix~\char40{}f~\char40{}Fix~\char40{}Comp~g~f\char41{}~g\char41{}\char41{}\\
\tt ~\\
\tt ~instance~Eq~\char40{}\char41{}~where~\char46{}\char46{}\char46{}\\
\tt ~instance~Eq~\char40{}f~\char40{}g~a\char41{}\char41{}~\char61{}\char62{}~Eq~\char40{}Comp~f~g~a\char41{}~where\\
\tt ~~~Comp~s~~\char61{}\char61{}~~Comp~t~~\char61{}~~s~\char61{}\char61{}~t\\
\tt ~\\
\tt ~instance~Eq~a~\char61{}\char62{}~Eq~\char40{}Pair~a\char41{}~where\\
\tt ~~~Pair~x1~y1~~\char61{}\char61{}~~Pair~x2~y2~~\char61{}~~x1~\char61{}\char61{}~x2~\char38{}\char38{}~y1~\char61{}\char61{}~y2\\
\tt ~\\
\tt ~instance~\char40{}Eq~a\char44{}~Eq~r\char41{}~\char61{}\char62{}~Eq~\char40{}GSeqF~a~r\char41{}~where\\
\tt ~~~Nil~~~~~~~~\char61{}\char61{}~~Nil~~~~~~~~\char61{}~~True\\
\tt ~~~Cons~x~xs~~\char61{}\char61{}~~Cons~y~ys~~\char61{}~~x~\char61{}\char61{}~y~\char38{}\char38{}~ys~\char61{}\char61{}~ys\\
\tt ~~~\char95{}~~~~~~~~~~\char61{}\char61{}~\char95{}~~~~~~~~~~~\char61{}~~False\\
\tt ~\\
\tt ~instance~Eq~\char40{}f~\char40{}Fix~\char40{}Comp~g~f\char41{}~g\char41{}\char41{}~\char61{}\char62{}~Eq~\char40{}Fix~f~g\char41{}~where\\
\tt ~~~Fix~s~~\char61{}\char61{}~~Fix~t~~\char61{}~~s~\char61{}\char61{}~t\\
\tt ~\\
\tt ~test~\char61{}~Fix~\char40{}Cons~\char40{}\char41{}~\char40{}Fix~\char40{}Comp~\char40{}Pair~Nil~Nil\char41{}\char41{}\char41{}\char41{}~\char61{}\char61{}~Fix~Nil
\end{tabbing}
	\footnotesize
\hrulefill}
	\caption{Equality instances for a generic multi-way tree}
	\label{fig:ex1}
\end{figure}

Unfortunitely, circularity is just one way for the evidence construction to be diverging,
\knote{diverging occurs for the first time here. If different from non-termination -- explain how, if not different -- just use non-termination consistently }
there are nontrivial code that will result in divergent without forming any cycle.
\knote{I do not understand that sentence as given}
Fig. \ref{fig:ex1} is a generic definition of multi-way trees where every node at level  has  or  subtrees. The instance declarations is what one would write given the data type declarations. However, the function \texttt{test} will make the type checker diverge, since it gives rise to the constraint \texttt{Eq (Fix (GSeqF ()) Pair)}, which has the following context reduction trace: 

\knote{check the above sentence: again diverge versus never terminate, does test on its own gives divergence? in which sense   \texttt{Eq (Fix (GSeqF ()) Pair)} is a constraint? do you really want to call it a constriant -- then this needs some semi-formal explanation...  }

\begin{center}
  {\footnotesize
    \noindent  
  }
\end{center}

\noindent We can see the constraint is periodically increased by \texttt{(Comp Pair)}. But if we look at the definition of \texttt{test}, it is comparing two finite data, we would want it to be type checked and evaluated to \texttt{False}! 
Internally, \texttt{test} is represented as:

{\footnotesize
\noindent \texttt{test = (==) d (Fix (Cons () (Fix (Comp (Pair Nil Nil))))) (Fix Nil)}
}

\noindent where \texttt{d} is some corecursive evidence that is used finitely by the context

\noindent \texttt{ (==) * (Fix (Cons () (Fix (Comp (Pair Nil Nil))))) (Fix Nil)}. The divergent of GHC in this case means the compiler can not construct such corecursive evidence. 

\end{comment}
\section{Preliminaries: Resolution with Evidence}
\label{preliminary}

This section provides a standard formalisation of resolution with evidence
together with two derived forms: a
\emph{small-step} variant of resolution and a reification of resolution in a
resolution tree.

We consider the following syntax.
\begin{definition}[Basic syntax]
{\small
}
\end{definition}

We consider first-order applicative terms, where  stands for some 
constant symbol. Atomic formulas are predicates
on terms, and Horn formulas are defined as usual. We assume that 
all variables  in Horn formulas are implicitly universally quantified.
There are no \textit{existential variables} in the Horn formulas, i.e.,
 for . 
The axiom environment  is a set of Horn formulas labelled with distinct
evidence constants . Evidence terms  are made of evidence
constants  and their applications. Finally, 
we often use  to abbreviate  when the number  is
unimportant. 







The above syntax can be used to model the Haskell type class setting as follows.
Terms denote Haskell types like \ensuremath{\Conid{Int}} or \ensuremath{(\Varid{x},\Varid{y})}, and atomic formulas
denote Haskell type class constraints on types like \ensuremath{\Conid{Eq}\;(\Conid{Int},\Conid{Int})}. Horn formulas
correspond to the type-level information of type class instances. 






Our evidence  models type class dictionaries, following Wadler and Blott's
dictionary-passing elaboration of type classes \cite{wadler1989make}.  In particular the constants
 refer to dictionaries that capture the term-level information of type
class instances, i.e., the implementations of the type class methods.  Evidence
application  accounts for dictionaries that are parametrised by other
dictionaries.  Horn formulas in turn represent type class instance
declarations. The axiom environment  corresponds to Haskell's global
environment of type class instances. Note that the treatment of type class instance declaration
and their corresponding evidence construction here
are based on our own understanding of many related works (\cite{Jones97,jones2003qualified}), which are also discussed in Section \ref{relwork}.  






In order to define resolution  together with evidence generation, we use resolution judgement  to state that the
atomic formula  is entailed by the axioms , and that the proof
term  witnesses this entailment. It is defined by means of the following
 inference rule.
\begin{definition}[Resolution]\label{def:resolution}
\fbox{}
\label{context-red}
{\small
}
\end{definition}
Using this definition we can show .


In case  resolution is diverging, it is often more convenient to consider
a \emph{small-step} resolution judgement (in analogy to the small step
operational semantics) that performs one resolution step at a time and allows
us to observe the intermediate states.

The basic idea is to rewrite the initial query  step by step into its
evidence~. This involves \emph{mixed terms} on the way that consist partly
of evidence, and partly of formulas that are not yet resolved.
\begin{definition}[Mixed Terms]
{\small
}
\end{definition}
At the same time we have defined mixed term contexts  as mixed
terms with a hole , where  substitutes the hole with  in the usual way.



\begin{definition}[Small-Step Resolution]
\fbox{}
\label{context-red}
{\small
}
\end{definition}
For instance, we resolve \ensuremath{\Conid{Eq}\;(\Conid{Int},\Conid{Int})} in three small
steps: ,
        and
       .
We write   to denote the transitive closure of
small-step resolution.





The following theorem formalizes the intuition that resolution and small-step resolution coincide.
\begin{theorem}
\label{func:eq}
  iff .
\end{theorem}


The proof tree for a judgement  is called a
\emph{resolution tree}.  It conveniently records the history of resolution and,
for instance, it is easy to observe the ancestors of a node. This last feature
is useful for our heuristic loop invariant analysis in Section \ref{inv}.    
 
Our formalisation of trees in general is as follows: We use  to denote
positions  in a tree, where 
for .  Let  denote the empty position or
\emph{root}. We also define  and . We write  if there exists a non-empty  such that . For a tree ,  refers to the node at position , and
 refers to the edge between  and . We use  as a special proposition to denote
success.   


 Resolution trees are defined as follows, note that they are a special case of \textit{rewriting
 trees}~\cite{KomendantskayaEtAl15,KJ15}: 
\begin{definition}[Resolution Tree]
The resolution tree for atomic formula  is
a tree  satisfying:
\begin{itemize}
\item .
\item  and  with  if  and . When , we write  and  for
any .  
\end{itemize}
\end{definition}




In general, the resolution tree can be infinite, this means that resolution is
non-terminating, which we denote as .
Figure~\ref{fig:ex2} shows a finite fragment of the infinite resolution tree for .









\begin{figure}[tbp]
  \makebox[\textwidth]{

\begin{tikzpicture}[every tree node/.style={},
   level distance=1.20cm,sibling distance=1.00cm, 
   edge from parent path={(\tikzparentnode) -- (\tikzchildnode)}, font=\scriptsize]
\Tree [.\node{\underline{\ensuremath{\Conid{Eq}\;(\Conid{Mu}\;\Conid{HPTree}\;\Conid{Int})}}};
        \edge node[auto=right]{};      
         [.\node{\ensuremath{\Conid{Eq}\;(\Conid{HPTree}\;(\Conid{Mu}\;\Conid{HPTree})\;\Conid{Int})}};  
        \edge node[auto=right]{};      
         [.\node{\ensuremath{\Conid{Eq}\;\Conid{Int}}};  
           \edge node[auto=right]{};      
         [.\node{};  
         ]
         ]
         \edge node[auto=left]{};      
         [.\node{\underline{\ensuremath{\Conid{Eq}\;(\Conid{Mu}\;\Conid{HPTree}\;(\Conid{Int},\Conid{Int}))}}};  
           \edge node[auto=right]{};
                 [.\node{\ensuremath{\Conid{Eq}\;(\Conid{HPTree}\;(\Conid{Mu}\;\Conid{HPTree})\;(\Conid{Int},\Conid{Int}))}};
                   \edge node[auto=right]{};
                   [.\node{...};]
                   \edge node[auto=left]{};
                   [.\node{...};]
                 ]
         ]
         ]
         ]
\end{tikzpicture}
}
	\caption{The infinite resolution tree for }
	\label{fig:ex2}
\end{figure}

We note that Definitions~~\ref{def:resolution} and~\ref{context-red} describe a special case of SLD-resolution in which unification taking place in derivations is restricted to term-matching.
This restriction is motivated by two considerations. The first one comes directly from the application area of our results: type class resolution uses exactly this restricted version of SLD-resolution.
The second reason is of more general nature. As discussed in detail in~\cite{FuK15,KJ15}, SLD-derivations restricted to term-matching reflect the \emph{theorem proving} content of a proof by SLD-resolution. That is,
if  can be derived  from  by SLD-resolution with term-matching only, then  is inductvely entailed by . If, on the other hand,  is derived from  by SLD-resolution with unification and computes a substitution ,
then  is inductively entailed by . In this sense, SLD-resolution with unification additionally has a \emph{problem-solving} aspect.
In developing proof-theoretic approach to resolution here, we thus focus on resolution by term-matching.



The resolution rule of Definition~\ref{def:resolution} resembles the definition of the consequence operator~\cite{Llo87} used to
define declarative semantics of Horn clause Logic.
In fact,  the forward and backward closure of the  rule of Definition~\ref{def:resolution} can be directly used to construct the usual least and greatest Herbrand models
for Horn clause logic, as shown in~\cite{KJ15}. There, it was also shown that SLD-resolution by term-matching is sound but incomplete relative to the least Herbrand models.







\section{Candidate Lemma Generation}
\label{inv}

This section explains how we generate candidate lemma from a
 potentially infinite resolution tree. Based on Paterson's condition we obtain
a finite pruned approximation (Definition~\ref{closed}) of this resolution tree.
Anti-unification on this approximation yields an abstract atomic formula and
 the corresponding abstract approximated resolution tree. It is from this abstract
tree that we read off the candidate lemma (Definition~\ref{invariant}).




We use  and  to denote the
multi-sets of respectively function symbols and variables in .

\begin{definition}[Paterson's Condition]
  For , we say 
satisfies Paterson's condition if  for each . 
\end{definition}

Paterson's condition is used in Glasgow Haskell Compiler to enforce termination of context
reduction \cite{SulzmannDJS07}. In this paper, we use it as a practical
criterion to detect problematic instance declarations. Any declarations that do
not satisfy the condition could potentially introduce diverging behavior in
the resolution tree. 



If , then we have  for projection on index .

\begin{definition}[Critical Triple]
\label{loop}
Let  for some . A critical triple in  is a triple  such that , and  does not satisfy Paterson's condition. 
\end{definition}

We will omit  from the triple and write  when it is not important. Intuitively, it means the nodes  and  are
using the same problematic projection , which could give rise to infinite resolution. 

The absence of a critical triple in a resolution tree means
that it has to be finite \cite{SulzmannDJS07}, while the presence of
a critical triple only means that the tree is \textit{possibly} infinite. In
general the infiniteness of a resolution tree is undecidable and
the critical triples provide a convenient over-approximation.


\begin{definition}[Closed Subtree]
  \label{closed}
  A closed subtree  is a subtree of a resolution tree such that for all leaves ,
the root  and  form a critical triple. 
\end{definition}

The critical triple in Figure \ref{fig:ex2} is formed by the underlined nodes. The
closed subtree in that figure is the subtree without the infinite branch below node
\ensuremath{\Conid{Eq}\;(\Conid{Mu}\;\Conid{HPTree}\;(\Conid{Int},\Conid{Int}))}. A closed subtree can intuitively be
understood as a finite approximation of an infinite resolution tree. We use it
as the basis for generating candidate lemma by means of 
anti-unification \cite{plotkin1970note}. 




\begin{definition}[Anti-Unifier]
We define the least general anti-unifier of atomic formulas  and  (denoted by ) and 
the least general anti-unifier of the terms  and  (denoted by ) as: 

  \begin{itemize}
  \item 
  \item 
  \item Otherwise, , where  is an injective function from a pair of terms (atomic formulas) to a set of fresh variables. 
  \end{itemize}
\end{definition}

Anti-unification allows us to extract the common pattern from different ground atomic formulas.

\begin{definition}[Abstract Representation]
  \label{abs:rep}
  Let  be all the critical triples in a closed subtree . 
  Let  , then the abstract
  representation  of the closed subtree  is a tree such that:
  \begin{itemize}
  \item 
  \item  and  with  if  and . When , we write  and  for
any .  
  \item  is undefined if  for some .
  \end{itemize}

\end{definition}

The abstract representation unfolds the anti-unifier of all the critical
triples. Thus the  abstract representation can always be embedded into the
original closed subtree. It is an abstract form of the closed subtree, and we
can extract the candidate lemma from the abstract representation.  
  
\begin{definition}[Candidate Lemma]
  \label{invariant}
  Let  be an abstract representation of a closed subtree, then the candidate lemma
  induced by this abstract representation is , 
  where the  are all the leaves for which  satisfies Paterson's
  condition.
\end{definition}

Figure \ref{fig:ex3} shows the abstract representation of the closed subtree of Figure \ref{fig:ex2}.
We read off the candidate lemma as \ensuremath{\Conid{Eq}\;\Varid{x}\Rightarrow \Conid{Eq}\;(\Conid{Mu}\;\Conid{HPTree}\;\Varid{x})}. 

\begin{figure}[tbp]
  \makebox[\textwidth]{

\begin{tikzpicture}[every tree node/.style={},
   level distance=1.20cm,sibling distance=1.00cm, 
   edge from parent path={(\tikzparentnode) -- (\tikzchildnode)}, font=\scriptsize]
\Tree [.\node{};
        \edge node[auto=right]{};      
         [.\node{\ensuremath{\Conid{Eq}\;(\Conid{HPTree}\;(\Conid{Mu}\;\Conid{HPTree})\;\Varid{x})}};  
        \edge node[auto=right]{};      
         [.\node{\ensuremath{\Conid{Eq}\;\Varid{x}}};  
         ]
         \edge node[auto=left]{};      
         [.\node{\underline{\ensuremath{\Conid{Eq}\;(\Conid{Mu}\;\Conid{HPTree}\;(\Varid{x},\Varid{x}))}}};  
         ]
         ]
         ]
         ]
\end{tikzpicture}
}
	\caption{The abstract representation of the closed subtree of Figure~\ref{fig:ex2}}
	\label{fig:ex3}
\end{figure}

The candidate lemma plays a double role. Firstly, it allows us to
construct a finite resolution tree. For example, we know that \ensuremath{\Conid{Eq}\;(\Conid{Mu}\;\Conid{HPTree}\;\Conid{Int})} gives rise to infinite tree with the axiom environment . However, a finite
tree can be constructed with \ensuremath{\Conid{Eq}\;\Varid{x}\Rightarrow \Conid{Eq}\;(\Conid{Mu}\;\Conid{HPTree}\;\Varid{x})},
since it reduces \ensuremath{\Conid{Eq}\;(\Conid{Mu}\;\Conid{HPTree}\;\Conid{Int})} to \ensuremath{\Conid{Eq}\;\Conid{Int}}, which succeeds trivially
with \ensuremath{\kappa_\Varid{Int}}. Next we show how to prove the candidate lemma coinductively, and
such proofs will encapsulate the
infinite aspect of the resolution tree. Since an infinite resolution tree gives
rise to infinite evidence, the finite proof of the lemma has to be
coinductive. We discuss such evidence construction in detail in Section~\ref{corec} and Section~\ref{ob-equiv}.    




\section{Corecursive Resolution}
\label{corec}





In this section, we extend the definition of resolution from  Section~\ref{preliminary}
by introducing two additional rules: one to handle coinductive proofs, and another -- to allow Horn formula goals, rather than atomic goals, in the derivations.
We call the resulting calculus \textit{corecursive resolution}.


\begin{definition}[Extended Syntax]
\label{syntax}
{\small
}
\end{definition}

To support coinductive proofs, we extend the syntax of evidence with functions
, variables  and fixed point  (which models the corecursive equation  in which  occurs in ). Also
we allow the Horn clauses  in the axiom environment  to be supported by any
form of evidence  (and not necessarily by constants ).



\begin{definition}[Corecursive Resolution]\label{def:cresolution}
  The following judgement for corecursive resolution extends the resolution in Definition~\ref{def:resolution}. {\small
}

\end{definition}
Note that  means  has to be in \textit{head normal form}
. This requirement is essential to ensure
the corecursive evidence satisfies the \textit{guardedness} condition.\footnote{See the extended version for a detailed discussion.}  The \textsc{Lam} rule implicitly assumes the treatment of \textit{eigenvariables},
i.e. we instantiate all the free variables in 
with fresh term constants. 

We implicitly assume that axiom environments are well-formed. 
\begin{definition}[Well-formedness of environment]
{\small
  }
\end{definition}

 
As an example, let us consider resolving the candidate lemma \ensuremath{\Conid{Eq}\;\Varid{x}\Rightarrow \Conid{Eq}\;(\Conid{My}\;\Conid{HPTree}\;\Varid{x})} against
the axiom environment . This yields the following derivation, where
 and :


{\small
\begin{center}
  \infer{\Phi_{\ensuremath{\Conid{HPTree}}}\vdash \ensuremath{\Conid{Eq}\;\Varid{x}\Rightarrow \Conid{Eq}\;(\Conid{Mu}\;\Conid{HPTree}\;\Varid{x})} \Downarrow \mu \alpha . \lambda \alpha_1 . \kappa_{\ensuremath{\Conid{Mu}}}\ (\kappa_{\ensuremath{\Conid{HPTree}}}\ \alpha_1\ (\alpha\
  (\kappa_{\ensuremath{\Conid{Pair}}}\ \alpha_1\ \alpha_1)))}
   {\infer{\Phi_1 \vdash \ensuremath{\Conid{Eq}\;\Varid{x}\Rightarrow \Conid{Eq}\;(\Conid{Mu}\;\Conid{HPTree}\;\Varid{x})} \Downarrow \lambda \alpha_1 . \kappa_{\ensuremath{\Conid{Mu}}}
\ (\kappa_{\ensuremath{\Conid{HPTree}}}\ \alpha_1\ (\alpha\
  (\kappa_{\ensuremath{\Conid{Pair}}}\ \alpha_1\ \alpha_1)))}
   {\infer{(\Phi_2 = \Phi_1, \alpha_1 : \ensuremath{\Conid{Eq}\;\Conid{C}}) \vdash \ensuremath{\Conid{Eq}\;(\Conid{Mu}\;\Conid{HPTree}\;\Conid{C})} \Downarrow \kappa_{\ensuremath{\Conid{Mu}}}\ (\kappa_{\ensuremath{\Conid{HPTree}}}\ \alpha_1\ (\alpha\
  (\kappa_{\ensuremath{\Conid{Pair}}}\ \alpha_1\ \alpha_1)))}
{\infer{\Phi_2 \vdash \ensuremath{\Conid{Eq}\;(\Conid{HPTree}\;(\Conid{Mu}\;\Conid{HPTree}\;\Conid{C}))} \Downarrow \kappa_{\ensuremath{\Conid{HPTree}}}\ \alpha_1\ (\alpha\
  (\kappa_{\ensuremath{\Conid{Pair}}}\ \alpha_1\ \alpha_1))}{
   \infer{\Phi_2 \vdash \ensuremath{\Conid{Eq}\;\Conid{C}} \Downarrow \alpha_1}{} & \infer{\Phi_2 \vdash \ensuremath{\Conid{Eq}\;(\Conid{HPTree}\;(\Conid{C},\Conid{C}))} \Downarrow \alpha\
  (\kappa_{\ensuremath{\Conid{Pair}}}\ \alpha_1\ \alpha_1)}{
   \infer{\Phi_2 \vdash \ensuremath{\Conid{Eq}\;(\Conid{C},\Conid{C})} \Downarrow (\kappa_{\ensuremath{\Conid{Pair}}}\ \alpha_1\ \alpha_1)}
{\infer{\Phi_2 \vdash \ensuremath{\Conid{Eq}\;\Conid{C}} \Downarrow \alpha_1}{} & \infer{\Phi_2 \vdash \ensuremath{\Conid{Eq}\;\Conid{C}} \Downarrow \alpha_1}{}}}}} }}
\end{center}
}

\noindent Once we prove \ensuremath{\Conid{Eq}\;\Varid{x}\Rightarrow \Conid{Eq}\;(\Conid{Mu}\;\Conid{HPTree}\;\Varid{x})} from  by corecursive resolution, we can add it to the
axiom environment and use it to prove the ground query \ensuremath{\Conid{Eq}\;(\Conid{Mu}\;\Conid{HPTree}\;\Conid{Int})}. Let . We have the following derivation. 

{\small

}



\begin{comment}
  We begin with initial environment . By preprocessing, we have , where 
  and  is an constant. Thus we have the following corecursive
  resolution.


  {\footnotesize
    \begin{center}
      
    \end{center}
  }

  \noindent By theorem \ref{corec:thm}, we obtain the following
  corecursive evidence:


  \noindent .

\end{comment}

\section{Operational Semantics of Corecursive Evidence}
\label{ob-equiv}



The purpose of this section is to give operational
semantics to corecursive resolution, and in particular, we are interested in giving 
operational interpretation to the
corecursive evidence constructed as a result of applying corecursive resolution.
In type class applications, for example, the evidence constructed for a query will be run as a program.
It is therefore important to establish the exact relationship
between the non-terminating resolution as a process and the proof-term that we obtain via corecursive resolution. We prove that corecursive evidence indeed faithfully
captures the otherwise infinite resolution process of Section~\ref{preliminary}.




In general, we know that if , then we can observe the following looping infinite reduction trace:

Each iteration of the loop gives rise to repeatedly applying
substitution  to the reduction context . 

In principle, this mixed term context  may contain an atomic
formula  that itself is normalizing, where  spawns another loop. 
Clearly this is a complicating factor. For instance, a
loop can spawn off additional loops in each iteration. Alternatively, a loop
can have multiple iteration points such as 
.\footnote{Note that we abuse notation here to denote contexts with multiple holes.
Also we abbreviate identical instantiation of  those multiple holes  to .} 
These complicating factors are beyond the scope of this section. We focus
only on \textit{simple loops}. These are loops with a single iteration point
that does not spawn additional loops.

We use  to denote the set of atomic formulas in the context
. If all atomic formulas  are irreducible
with respect to , then we call  a \textit{normal context}. 
\begin{definition}[Simple Loop]
Let , where  is
normal. If for all , we have that  with , then we call  a simple loop.  
 \end{definition}
In the above definition, the normality of  ensures that the loop
has a single iteration point. Likewise the condition , which implies that , guarantees that each iteration of the loop
spawns no further loops.

 \begin{definition}[Observational Point]
\label{observe}
   Let  be a simple loop and .   We call  an observational point if it is of the form . We use  to denote the set of observational points in the simple loop. 
 \end{definition}

For example, we have the following infinite resolution trace generated by the simple loop (with the subterms of observational points underlined). 
 {\scriptsize
 \begin{center}

    
 \end{center}
 }
\noindent In this case, we have  and . 




The corecursive evidence encapsulates an infinite derivation in a finite fixpoint
expression. We can recover the infinite resolution by reducing the
corecursive expression.
To define small-step \textit{evidence reduction}, we first extend
 mixed terms to cope with richer
corecursive evidence.
\begin{definition}
Mixed term 
\end{definition}

Now we define the small-step evidence reduction relation .
\begin{definition}[Small Step Evidence Reduction]
  \label{label}
  \fbox{}
{\small
}

\end{definition}
Note that for simplicity we still use the mixed term context  as defined in
Section \ref{preliminary}, but we only allow the reduction of an outermost redex, i.e.,
a redex that is not a subterm of some other redex. In other words, 
reduction unfolds the evidence term strictly downwards from the
root, this follows closely the way evidence is constructed during
resolution.

We call the states where we perform a -transition \emph{corecursive points}.
Note that -transitions unfold
 a corecursive definition.
These
correspond closely to the observational points in resolution.

\begin{definition}[Corecursive Point]
Let . We call  a corecursive point if it is of the form . We use  to denote the set of corecursive points in .  
 \end{definition}


Let . We have the following evidence reduction trace (with the subterms of corecursive points underlined):

{\scriptsize
  \begin{center}

      
      
  \end{center}
}

Observe that the mixed term contexts of the observational points and the
corecursive points in the above traces coincide. This allows us to show
observational equivalence of resolution and evidence reduction without
explicitly introducing actual infinite evidence.

The following theorem shows that if resolution gives rise to a simple loop,
then we can obtain a corecursive evidence  (Theorem \ref{equiv} (1)) such that
the infinite resolution trace is observational equivalent to 's evidence
reduction trace (Theorem \ref{equiv} (2)). 

\begin{theorem}[Observational Equivalence]
\label{equiv}
Let  be a simple loop and .
Then:

\noindent \textit{1.} We have   for some . 

\noindent \textit{2.}  iff .
\end{theorem}

The proof can be found in the extended version.













\section{Related Work}
\label{relwork}
\emph{Calculus of Coinductive Constructions.} Interactive theorem prover Coq pioneered implementation of the \textit{guarded coinduction principle} (\cite{coquand1994infinite,gimenez1996}).  
The Coq termination checker may prevent some nested uses of coinduction, e.g. a proof term such as  is not accepted by Coq, while from the outermost reduction point of view, this proof term is productive. 

\emph{Loop detection in term rewriting.} Distinctions between cycle, loop and non-looping
 has long been established in term rewriting research (\cite{dershowitz1987,zantema1996non}). For us, detecting loop is the first step of invariant analysis, but we also want to extract corecursive evidence such that it captures the infinite reduction trace. 


\emph{Restricted datatypes.} Hughes (Section 4 \cite{hughes1999restricted})
observed the cyclic nature of the instance declarations \ensuremath{\mathbf{instance}\;\Conid{Sat}\;(\Conid{EqD}\;\Varid{a})\Rightarrow \Conid{Eq}\;\Varid{a}} and
\ensuremath{\mathbf{instance}\;\Conid{Eq}\;\Varid{a}\Rightarrow \Conid{Sat}\;(\Conid{EqD}\;\Varid{a})}. He proposed to treat the looping context
reduction as failure, in which case the compiler would need to search for an alternative
reduction. 


\emph{Scrap Your Boilerplate with Class.} The cycle
detection method \cite{Lammel:2005} was proposed to
generate corecursive evidence for a restricted
class of non-terminating resolution. It is supported by the current Glasgow Haskell Compiler. 


\emph{Derivable type classes.} Hinze and Peyton Jones \cite{hinze2001}
wanted to use an instance of the form
\noindent \ensuremath{\mathbf{instance}\;(\Conid{Binary}\;\Varid{a},\Conid{Binary}\;(\Varid{f}\;(\Conid{GRose}\;\Varid{f}\;\Varid{a})))\Rightarrow \Conid{Binary}\;(\Conid{GRose}\;\Varid{f}\;\Varid{a})}, but discovered that it causes resolution to diverge.
They  suggested the following as a replacement:
\noindent \ensuremath{\mathbf{instance}\;(\Conid{Binary}\;\Varid{a},\forall \Varid{b}\hsforall \hsdot{\mathrel{\,\cdot\,}}{\;.\;}\Conid{Binary}\;\Varid{b}\Rightarrow \Conid{Binary}\;\Varid{f}\;\Varid{b})\Rightarrow \Conid{Binary}\;(\Conid{GRose}\;\Varid{f}\;\Varid{a})}. 
Unfortunately, Haskell does not support instances with \textit{polymorphic higher-order instance contexts}.
Nevertheless, allowing such implication constraints  
would greatly increase the expressitivity of corecursive resolution. In the terminology of 
our paper, it amounts to extending Horn formulas to intuitionistic formulas. 
Working with intuitionistic formulas would require a certain amount of searching, 
as the non-overlapping condition for Horn formulas is not enough to ensure uniqueness of 
the evidence. For example, consider the following axioms: 

  \begin{center}
    

    
    
    
  \end{center}





\noindent  We have two distinct proof terms for :

\begin{center}
  

  
\end{center}
\noindent This is 
 undesirable from the perspective of generating evidence for type class.  

\emph{Instance declarations and (Horn Clause) logic programs}. The process of simplifying type class
constraints is formally described as the notion of \textit{context reduction}
by Peyton Jones et. al.~\cite{Jones97}. In Section 3.2 of the same paper also
describes the form and the requirements of instance declarations. Type class
evidence and their connection to type system are studied in Mark Jones's
thesis~\cite[Chapter 4.2]{jones2003qualified}. Context reduction,
instance declaration and their connection to proof relevant resolution are also
discussed under the name of \textit{LP-TM} (logic programming with
term-matching) in Fu and Komendantskaya~\cite[Section 4.1]{FuK15}.        






\section{Conclusion and Future Work}\label{fwork}

We have introduced a novel approach to  non-terminating resolution.
Firstly, we have shown that the popular cycle detection methods employed for logic programming or type class resolution
can be understood via more general coinductive proof principles (\cite{coquand1994infinite,gimenez1996}).
Secondly, we have shown that 
 resolution can be enriched with rules that capture the intuition of richer coinductive hypothesis formation.
 This extension allows to provide corecursive evidence to some derivations that could not be handled by previous methods.
 Moreover, corecursive resolution is formulated in a proof-relevant way, i.e. proof-evidence construction is an essential part
 of  corecursive resolution. This makes it easier to integrate it directly into type class inference.



\begin{comment}


To obtain corecursive evidence for a query that has non-terminating resolution,
our approach is to first generate a richer coinductive hypothesis in the form of
an invariant formula, and to prove this formula automatically by corecursive
resolution, then use the proven invariant formula as a lemma to facilitate the construction
of finite evidence for the initial query. 

We first describe a heuristic method to generate invariant formulas in Section
\ref{inv}. This method exploits Paterson's condition and anti-unification. It
is essentially a form of loop detection for the resolution trace generated by
the ground query. The head of the invariant formula generated by our method is
least general due to the use of anti-unification. This is a desirable property
in practice since it avoids unnecessary overlap with other axioms. 
To construct corecursive evidence for the invariant formulas, in Section 
\ref{corec}, we extend resolution to allow corecursion and Horn formula queries. The corecursive resolution has 
  a direct type-theoretic interpretation and the corecursive evidence it generates is weak-head
  normalizing.\footnote{See Appendix \ref{ap:guard}.} In 
Section \ref{ob-equiv}, divergent resolution due to simple loops is shown to be
observational equivalent to the infinite unfolding of its corecursive evidence.

\end{comment}



We have implemented the techniques of Sections \ref{inv} and \ref{corec}, and have incorporated
them as part of the evidence construction process for a simple language that 
admits previously non-terminating examples.\footnote{See the extended version for more examples and information about the implementation. Extended version is available from authors' homepages.} 




\textbf{Future Work} In general, the interactions between different loops
can be complicated. Consider  with the following
declarations (denoted by ):
\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}\column{3}{@{}>{\hspre}l<{\hspost}@{}}\column{E}{@{}>{\hspre}l<{\hspost}@{}}\>[3]{}\kappa_\Conid{M}\mathbin{:}\Conid{Eq}\;(\Varid{h}_{1}\;(\Conid{M}\;\Varid{h}_{1}\;\Varid{h}_{2})\;(\Conid{M}\;\Varid{h}_{2}\;\Varid{h}_{1})\;\Varid{a})\Rightarrow \Conid{Eq}\;(\Conid{M}\;\Varid{h}_{1}\;\Varid{h}_{2}\;\Varid{a}){}\<[E]\\
\>[3]{}\kappa_\Conid{H}\mathbin{:}(\Conid{Eq}\;\Varid{a},\Conid{Eq}\;((\Varid{f}_{1}\;\Varid{a}),(\Varid{f}_{2}\;\Varid{a})))\Rightarrow \Conid{Eq}\;(\Conid{H}\;\Varid{f}_{1}\;\Varid{f}_{2}\;\Varid{a}){}\<[E]\\
\>[3]{}\kappa_\Conid{G}\mathbin{:}\Conid{Eq}\;((\Varid{g}\;\Varid{a}),(\Varid{f}\;(\Varid{g}\;\Varid{a})))\Rightarrow \Conid{Eq}\;(\Conid{G}\;\Varid{f}\;\Varid{g}\;\Varid{a}){}\<[E]\ColumnHook
\end{hscode}\resethooks
\begin{figure}[tbp]
  \makebox[\textwidth]{
\begin{tikzpicture}[every tree node/.style={},
   level distance=1.00cm,sibling distance=.40cm, 
   edge from parent path={(\tikzparentnode) -- (\tikzchildnode)}, font=\scriptsize]
\Tree [.\node{};
        \edge node[auto=right]{};      
         [.\node{\ensuremath{\Conid{Eq}\;(\Conid{H}\;(\Conid{M}\;\Conid{H}\;\Conid{G})\;(\Conid{M}\;\Conid{G}\;\Conid{H})\;\Conid{Int}}};  
        \edge node[auto=right]{};      
         [.\node{\ensuremath{\Conid{Eq}\;\Conid{Int}}};  
           \edge node[auto=right]{};      
         [.\node{};  
         ]
         ]
         \edge node[auto=right]{};      
         [.\node{\ensuremath{\Conid{Eq}\;((\Conid{M}\;\Conid{H}\;\Conid{G}\;\Conid{Int}),(\Conid{M}\;\Conid{G}\;\Conid{H}\;\Conid{Int}))}};  
           \edge node[auto=right]{};      
         [.\node{};  
         ]
         \edge node[auto=right]{};      
         [.\node{};  
           \edge node[auto=right]{};      
         [.\node{\ensuremath{\Conid{Eq}\;(\Conid{G}\;(\Conid{M}\;\Conid{G}\;\Conid{H})\;(\Conid{M}\;\Conid{H}\;\Conid{G})\;\Conid{Int})}};  
         \edge node[auto=right]{};      
         [.\node{\ensuremath{\Conid{Eq}\;((\Conid{M}\;\Conid{H}\;\Conid{G}\;\Conid{Int}),(\Conid{M}\;\Conid{G}\;\Conid{H}\;((\Conid{M}\;\Conid{H}\;\Conid{G})\;\Conid{Int})))}};  
           \edge node[auto=right]{};      
         [.\node{};  
         ]
         \edge node[auto=right]{};      
         [.\node{};  
         ]
         ]
         ]
         ]
         ]
         ]
         ]
\end{tikzpicture}
}
	\caption{A Partial Resolution tree for }
	\label{fig:ex4}
\end{figure}
\noindent A partial resolution tree generated by the query \ensuremath{\Conid{Eq}\;(\Conid{M}\;\Conid{H}\;\Conid{G}\;\Conid{Int})} is
described in Figure \ref{fig:ex4}. In this case the cycle (underlined with the
index ) is mutually nested with a loop (underlined with index ). Our method in
Section \ref{inv} is not able to generate any candidate lemmas. Yet there are two
candidate lemmas for this case (with the proof of 
refer to ): 

\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}\column{3}{@{}>{\hspre}l<{\hspost}@{}}\column{E}{@{}>{\hspre}l<{\hspost}@{}}\>[3]{}\Varid{e}_{1}\mathbin{:}(\Conid{Eq}\;\Varid{x},\Conid{Eq}\;(\Conid{M}\;\Conid{G}\;\Conid{H}\;\Varid{x}))\Rightarrow \Conid{Eq}\;(\Conid{M}\;\Conid{H}\;\Conid{G}\;\Varid{x}){}\<[E]\\
\>[3]{}\Varid{e}_{2}\mathbin{:}\Conid{Eq}\;\Varid{x}\Rightarrow \Conid{Eq}\;(\Conid{M}\;\Conid{G}\;\Conid{H}\;\Varid{x}){}\<[E]\ColumnHook
\end{hscode}\resethooks
\noindent We would
like to improve our heuristics to allow generating multiple candidate lemmas, where
their corecursive evidences mutually refer to each other. 

There are situations where resolution is non-terminating but does not form any loop such
as . Consider the following program : 
\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}\column{3}{@{}>{\hspre}l<{\hspost}@{}}\column{E}{@{}>{\hspre}l<{\hspost}@{}}\>[3]{}\kappa_\mathrm{1}\mathbin{:}\Conid{D}\;\Varid{n}\;(\Conid{S}\;\Varid{m})\Rightarrow \Conid{D}\;(\Conid{S}\;\Varid{n})\;\Varid{m}{}\<[E]\\
\>[3]{}\kappa_\mathrm{2}\mathbin{:}\Conid{D}\;(\Conid{S}\;\Varid{m})\;\Conid{Z}\Rightarrow \Conid{D}\;\Conid{Z}\;\Varid{m}{}\<[E]\ColumnHook
\end{hscode}\resethooks
\noindent For query \ensuremath{\Conid{D}\;\Conid{Z}\;\Conid{Z}}, the resolution diverges without forming any loop. We would have
to introduce extra equality axioms in order to obtain finite corecursive evidence.\footnote{See the extended version for a solution in Haskell using type family and more discussion.} We would like
to investigate the ramifications of incorporating equality axioms in the corecursive resolution in the future. 


We plan to extend the observational equivalence result of Section
\ref{ob-equiv} to cope with more general notions of loop and extend our approach to 
allow intuitionistic formulas as candidate lemmas. 



Another avenue for future work is a formal proof that  the calculus of Definition~\ref{def:cresolution} is sound relative to the the greatest Herbrand models~\cite{Llo87},
 and therefore reflects the broader notion of the coinductive entailment for Horn clause logic as discussed in the introduction.



\subsection*{Acknowledgements} We want to thank Patricia Johann and the FLOPS reviewers for their helpful 
comments, Franti\v sek Farka for many discussions. Part of this work was funded
by the Flemish Fund for Scientific Research.

\bibliographystyle{plain}
\bibliography{paper}

\newpage
\appendix



\section{Proof of Theorem \ref{equiv}}\label{app:proof}















\begin{theorem}
Let 
with  and  are normal for all . Suppose , where  for any . We have the following: 

\begin{enumerate}
\item .
\item Let . We have: 

 iff . 
\end{enumerate}
\end{theorem}
\begin{proof}

  \begin{enumerate}
  \item We have the following finite derivation. 

    \begin{center}

    \end{center}

\noindent By Theorem \ref{func:eq}, we just need to reduce  to a proof term using
    the rules . 
We have the following reduction:

    \begin{center}
      
    \end{center}
    
    Thus we have the corecursive evidence 
     for , and .
  \item Using the same notation in (1), let , we can observe following equivalence reduction traces:
 
  \begin{center}
  
   
\

    
  \end{center}
We proceed by induction on . When , it is obvious. Let . By IH, 
we know that  iff . We have 

.

On the other hand, 

. Thus we have the observational equivalence. 
  \end{enumerate}
\end{proof}

\section{Weak Head Normalization of Corecursive Evidence}
\label{ap:guard}
\begin{definition}
\label{syntax}

\end{definition}

We write  as a shorthand for . Note that Horn formulas are special case of formulas. 

\begin{definition}
  Weak Head reduction context 
\end{definition}


\begin{definition}[Weak Head Reduction]
  \label{label}
  \fbox{}


\end{definition}

Note that weak head reduction context do not allow reduction under the constant . It is 
more restricted than the context in Section \ref{preliminary}.


\begin{definition}[General Corecursive Resolution]

  

\end{definition}




\begin{definition}[Howard's Type System]
\label{proofsystem}

\end{definition}


\begin{theorem}
  If , then . 
\end{theorem}
\begin{proof}
  By induction on the derivation of . 
\end{proof}

\begin{comment}
  \begin{proof}
    By induction on the derivation of .

    \noindent \textit{Case.}

    \

    \infer[\text{if}~(\kappa :\ \Rightarrow A) \in \Phi] {\Phi \vdash
      \sigma A \Downarrow \kappa} {}


    Obvious.

    \noindent \textit{Case.}

    \

    \infer[\text{if}~(\kappa : G_1,..., G_m \Rightarrow A) \in \Phi, m
    \geq 1] {\Phi \vdash \sigma A \Downarrow \kappa~e_1 \cdots e_n} {
      \Phi \vdash \sigma G_1 \Downarrow e_1 \quad \cdots \quad \Phi
      \vdash \sigma G_n \Downarrow e_n }


    By IH, we have . Thus by \textsc{Inst} and \textsc{App}, we have
    .

    \noindent \textit{Case.}

    \

    \infer{\Phi \vdash F \Downarrow \mylet{\alpha = e}{\alpha} }{\Phi,
      \alpha : F \vdash F \Downarrow e & \mathrm{HNF}(e)}

    By IH, we have , thus by
    \textsc{Let} rule, we have .

    \noindent \textit{Case.}

    \

    \infer {\Phi \vdash \underline{G} \Rightarrow B \Downarrow \lambda
      \underline{\alpha} . e} {\Phi, \underline{\alpha} :
      \underline{G} \vdash B \Downarrow e}

    By IH, we have , thus we can apply \textsc{Abs} rule, which give us .
  \end{proof}


  \begin{definition}
  
    \

    

    

    

    

    
  \end{definition}
\end{comment}
\begin{theorem}
  If , then  is terminating with respect to weak head reduction. 
\end{theorem}
\begin{proof} Sketch. The proof is very similar to standard normalization proof for simply typed
lambda calculus. The only tricky rule is the \textsc{Mu} rule: 

\

\infer{\Phi \vdash \mu \alpha. e : \underline{G} \Rightarrow A}{\Phi, \alpha :  \underline{G} \Rightarrow A \vdash e : \underline{G} \Rightarrow A & \mathrm{HNF}(e)}

\

\noindent We want to show  is in the reducible set of type . Since , we just need to show for any reducible  of type , we have  is terminating. This is the case due to the expression is
guarded by . 
\end{proof}

\section{Examples}
\label{examples}

In this section, we show several examples with the prototype that we developed. They are
also available from the \texttt{examples} directory in \url{https://github.com/Fermat/corecursive-type-class}.

\subsection{Example 1}
\begin{verbatim}
module bush where
axiom (Eq a, Eq (f (f a))) => Eq (HBush f a)
axiom Eq (f (Mu f) a) => Eq (Mu f a)
axiom Eq Unit
auto Eq (Mu HBush Unit)
\end{verbatim}

The keyword \texttt{axiom} introduces an axiom and the keyword \texttt{auto}
requests the system to prove the formula automatically using the heuristic
corecursive hypothesis generation that we described in Section \ref{inv}. If we
save the above code in the \texttt{bush.asl} file, and, at the command line, type
\texttt{asl bush.asl}, then we get the following output:

\begin{verbatim}
Parsing success! 
Type Checking success! 
Program Definitions
  Ax0 :: (Eq (f (Mu f) a)) => Eq (Mu f a)
  = Ax0 
  Ax1 :: (Eq a, Eq (f (f a))) => Eq (HBush f a)
  = Ax1 
  Ax2 :: Eq Unit
  = Ax2 
  genLemm4 :: (Eq var_1) => Eq (Mu HBush var_1)
  = \ b0 . Ax0 (Ax1 b0 (genLemm4 (genLemm4 b0))) 
  goalLem3 :: Eq (Mu HBush Unit)
  = genLemm4 Ax2 
Axioms
  Ax2 :: Eq Unit
  Ax1 :: (Eq a, Eq (f (f a))) => Eq (HBush f a)
  Ax0 :: (Eq (f (Mu f) a)) => Eq (Mu f a)
Lemmas
  goalLem3 :: Eq (Mu HBush Unit)
  genLemm4 :: (Eq var_1) => Eq (Mu HBush var_1)
\end{verbatim}

The corecursive hypothesis generated is  \texttt{genLemm4 :: (Eq var\string_1) => Eq (Mu HBush var\string_1)}, its proof is \texttt{\string\ b0 . Ax0 (Ax1 b0 (genLemm4 (genLemm4 b0)))}. The proof
for \texttt{Eq (Mu HBush Unit)} is \texttt{genLemm4 Ax2}. 

Using \texttt{axiom} and \texttt{auto} allows us to quickly experiment with
different small examples. Here is the corresponding type-class code. 
\begin{verbatim}
module bush where
data Unit where
  Unit :: Unit

data Maybe a where
  Nothing :: Maybe a
  Just :: a -> Maybe a

data Pair a b where
  Pair :: a -> b -> Pair a b

data HBush f a where
  HBLeaf :: HBush f a
  HBNode ::  a -> (f (f a)) -> HBush f a

data Mu f a where
  In :: f (Mu f) a -> Mu f a
  
data Bool where
     True :: Bool
     False :: Bool

class Eq a where
   eq :: Eq a => a -> a -> Bool

and = \ x y . case x of
                True -> y
                False -> False

instance  => Eq Unit where
   eq = \ x y . case x of
                   Unit -> case y of 
                              Unit -> True

instance Eq a, Eq (f (f a)) => Eq (HBush f a) where
  eq = \ x y . case x of
                 HBLeaf -> case y of
                            HBLeaf -> True
                            HBNode a c -> False
                 HBNode a c1 -> case y of
                              HBLeaf -> False
                              HBNode b c2  -> and (eq a b) (eq c1 c2)

instance Eq (f (Mu f) a) => Eq (Mu f a) where
   eq = \ x y . case x of
                  In s -> case y of
 		            In t -> eq s t

term1 = In HBLeaf
term2 = In (HBNode Unit term1)
test = eq term2 term1
test1 = eq term2 term2
reduce test
reduce test1
\end{verbatim}

Inspecting the output of this code, we see that the result of the evaluation of 
\texttt{test} (resp. \texttt{test1}) is \texttt{False} (resp. \texttt{True}). It is quite
verbose and probably irrelevant to see the type-class code, so in the following
we will show examples using only the \texttt{axiom}, \texttt{auto} and \texttt{lemma} keywords.

\subsection{Example 2}

\begin{verbatim}
module lam where
axiom Eq (f (Mu f) a) => Eq (Mu f a)
axiom (Eq a, Eq (f a), Eq (f a), Eq (f (Maybe a))) => Eq (HLam f a)
axiom Eq Unit
axiom Eq a => Eq (Maybe a)
lemma (Eq x) => Eq (Mu HLam x)
lemma Eq (Mu HLam Unit)
\end{verbatim}

Of course, our heuristic \texttt{auto Eq (Mu HLam Unit)} also works for this case. But
we can specify the corecursive hypothesis as lemma and guided our mini-prover to prove the 
final goal \texttt{Eq (Mu HLam Unit)}.

\subsection{Example 3}
Are there any examples where \texttt{auto} fails, but where we can come to the rescue with a \texttt{lemma}? 
The answer is yes. 

\begin{verbatim}
axiom (Eq a, Eq (Pair (f1 a) (f2 a))) => Eq (H1 f1 f2 a)
axiom Eq (Pair (g a) (f (g a))) => Eq (H2 f g a)
axiom Eq (h1 (Mu h1 h2) (Mu h2 h1) a) => Eq (Mu h1 h2 a)
axiom (Eq a, Eq b) => Eq (Pair a b)
axiom Eq Unit
auto Eq (Mu H1 H2 Unit)
\end{verbatim}
If we run this code, our mini-prover diverges, since this example require multiple lemmas
in order to prove the final goal \texttt{Eq (Mu H1 H2 Unit)}. 

\begin{verbatim}
axiom (Eq a, Eq (Pair (f1 a) (f2 a))) => Eq (H1 f1 f2 a)
axiom Eq (Pair (g a) (f (g a))) => Eq (H2 f g a)
axiom Eq (h1 (Mu h1 h2) (Mu h2 h1) a) => Eq (Mu h1 h2 a)
axiom (Eq a, Eq b) => Eq (Pair a b)
axiom Eq Unit
lemma (Eq x, Eq (Mu H2 H1 x)) => Eq (Mu H1 H2 x)
lemma Eq x => Eq (Mu H2 H1 x)
lemma Eq (Mu H1 H2 Unit)
\end{verbatim}

\subsection{Example 4}
\label{DZ}
Are there any examples where even \texttt{lemma} does not work? We believe that
the following is such an example.
\begin{verbatim}
axiom D n (S m) => D (S n) m
axiom D (S m) Z => D Z m
\end{verbatim}

Note that \texttt{auto D Z Z} will not work because the corecursive hypothesis
generated by our method is not provable. The following is a solution in Haskell
using type families. We want to point out that using type families is a way to
introduce equality axioms for addition, and these equality axioms are not
derivable from the original axioms. The ability to learn addition seems to
require a higher notion of intelligence.    

\begin{verbatim}
{-# LANGUAGE Rank2Types, TypeFamilies, UndecidableInstances #-}
data Z
data S n
data D a b
type family Add m n
type instance Add n Z  =  n
type instance Add m (S n) = Add (S m) n

k1 :: D n (S m) -> D (S n) m
k1 = undefined
k2 :: D (S m) Z -> D Z m
k2 = undefined

f :: (forall n. D n (S m) -> D (S (Add m n)) Z) -> D Z m
f g = k2 (g (f (g . k1)))
\end{verbatim}
 


\begin{comment}
\section{Corecursive Evidence Construction(will be moved to appendix)}
\label{app:corec}
This section describes our procedure to automatically construct corecursive
evidence for invariant formulas.  We first generalize Horn formulas to
implicational intuitionistic formulas and then formalize the evidence
construction procedure in terms of Howard's simple type system~\cite{Howard80}.
It is a generalization of resolution by term-matching in
\cite{DBLP:journals/corr/FuK15}. The procedure is shown to be sound with
respect to the simple type system. 


The sort of axioms is extended with proof variable annotations  and corecursive definitions
. The latter means that  of type  is corecursively defined as
. We write  to mean  or . We write  to
mean ,  or .


We use Howard's type system because it expresses evidence construction
for a sufficiently rich notion of formulas (e.g., in contrast with the atomic
intuitionistic sequent calculus~\cite{DBLP:journals/corr/FuK15}). We can
easily introduce corecursive evidence with the following permissive
judgement for well-formed contexts.


Here  means that  is in (head normal) form . This requirement ensures a notion of
productiveness for the corecursive evidence, which is captured in the normalization theorem.






\begin{theorem}
  If  and , then  is terminating w.r.t. weak head reduction. 
\end{theorem}
The proof makes use of standard Tait-Girard reducibility; we elide the details.
Note that the type system does not have the strong normalization property due
to the corecursive definition.  



The following definition generalizes the notion of type class context
reduction~\cite{Jones97} to queries in implicational form. 

\begin{definition}[Generalized Resolution]\label{context-red}

\end{definition}

When the query is an atomic formula, andt the axioms are Horn formulas, then
the first reduction rule above coincides with the usual notion of type class
context reduction. The second reduction rule allows the reduction of
implicational formulas. Note that  in the second reduction rule
replaces the quantified variables  with fresh term constants
. This corresponds to the treatment of \textit{eigenvariables} in
theorem proving. 

The following theorem ensures that successful context reduction gives rise to valid evidence. 
\begin{theorem}[Soundness]
  \label{soundness}
If , then there exist  such that  ().
\end{theorem}



The generalized context reduction of Definition \ref{context-red} does not
directly generate corecursive evidence. This is tackled in the following
reduction strategy.  
\begin{definition}[Corecursive Strategy]
The reduction beginning with  , where  is fresh, is corecursive if it is of the
form  where  are fresh and  does not appear in . \end{definition}

Hence to obtain corecursive evidence for  in , we can use the
corecursive strategy to reduce , where  is
fresh. The following theorem shows that 
the corecursive strategy indeed yields well-formed corecursive evidence.

\begin{theorem}
  If the reduction path  is corecursive, then
   for some . 
\end{theorem}



As an example, let us consider the following axioms : 



\noindent Note that  are variables. The concretization of the invariant formula  as query (resp. as axiom) is  (resp. ), where  is a dummy constant.  Let . We have the following terminating context reduction: 

{\footnotesize
\begin{center}
 














\end{center}
}

\noindent We obtain the following corecursive evidence: 


\noindent .


\noindent Thus we have evidence  for query .

\section{Observational Equivalence}
\label{rewrite}



In this section we show that context reduction can be expressed as first-order term rewriting
of the query  into its proof term. The process involves \emph{mixed} terms, that are
part query and part proof term.

\begin{definition}[Mixed terms]
Mixed terms 
\end{definition}
Note that  is a function symbol uniquely identified by . 



The following definition shows how to construct a term rewrite system on these mixed
terms from a set of axioms. 
  
\begin{definition}[Functionalisation]
  \label{label}
We construct a set of rewrite rules  from  as follows:
\begin{itemize}
\item For each , we
  have .
\item For each , we have  and . 
\end{itemize}
\end{definition}


 Thus  is a set of first order rewrite rules on mixed terms. If  does not contain any corecursive definition, then  is called \textit{pure}. 
 We define \textit{mixed term context}  to be the mixed term extended with the symbol  to denote a hole. The notation  means replacing the holes in  from left to right with . We conveniently remove duplications in this representation, i.e.  will be represented as . We write 
 if ,  and  with . 


The following theorem formalize the intuition that context reduction can be viewed as a rewriting process. 
\begin{theorem}
\label{func:eq}
Assume  is pure and non-overlapping.  iff  where  is a proof term.
\end{theorem}

We know that if , then the reduction loops. We use  to denote the set of atomic formulas in the context . If
 for any ,  is in normal form, then we call  a \textit{normal context}.

 \begin{definition}[Simple Loop]
 Let  be a normal context and . If for any ,  with , then we call (or just  ) a simple loop.  
 \end{definition}



We use  to denote the set of mixed terms reachable from , i.e.  iff .   

 \begin{definition}[Observational Point]
\label{observe}
   Let  be a simple loop and . 
   We call  an observational point if it is of the form , for some  such that  and . 
 \end{definition}

We use  to denote the set of observational points in . 
Intuitively, the infinite rewrite process using the pure rules means the proof evidence for the query will be an infinite first order proof term, while most of the time the computation (weak head reduction) only uses/observes finite part of this infinite term, these finite parts are formalized as contexts  of the observational points in Definitition \ref{observe}.

\begin{definition}[Corecursive Point]
Let . We call  a corecursive point if it is of the form , where  is normal, , and there
is a rule  with  occurs in . 
 \end{definition}

We use  to denote the set of corecursive points in . If we
have a corecursive rewrite rule , we can applying
this rule infinitely, thus for the same intuition as observational points, we can define corecursive points
to capture the finite contexts.
 




The following theorem shows that if the pure rewrite rules give rise to a simple loop, then we can obtain a 
corecursive proof term (Theorem \ref{ob:equiv} (1)) such that the original infinite trace of reduction is observationally equivalent to the infinite trace generated by the corecursive proof term (Theorem \ref{ob:equiv} (2)). 

\begin{theorem}
\label{ob:equiv}
Assume  is pure and non-overlapping. Let 
with  and  are normal for all . Suppose , where  for any . We have the following: 

\noindent \textit{1.}  is corecursively provable by some corecursive definition . 

\noindent \textit{2.} Let
 , where  with . We have:  iff , where .  

\end{theorem}


 For example, consider the following rewrite rule :
 
 \begin{center}





\end{center}

\noindent We can dectect the following simple loop (with subterm of observational points underlined):
 {\scriptsize
 \begin{center}

   
 \end{center}
 }
\noindent In this case, we have  and . 
Thus  is provable with the corecursive evidence . Thus we can obtain two rewrite rules:  and . Let . We can observe the following equivalent reduction trace (with corecursive point underlined):

{\scriptsize
\begin{center}
  
\end{center}
}


\section{Proof of Theorem \ref{soundness}}

\begin{theorem}
If , then .
\end{theorem}

\begin{proof}
By induction on the length of .

\begin{itemize}
\item Base Case:  with  and . In this case, by rules \textit{assump}, \textit{inst}, we have .

\item Step Case: 
  
\noindent 

\noindent , with  and . By IH, 
we have . Since  by rule \textit{inst}, we have  by applying rule \textit{app} multiple times.

\item Step Case: 

\noindent 

\noindent . Let . By IH, we have . We apply 
rule \textit{abs} and rule \textit{gen}, we get .
\end{itemize}
\end{proof}



\section{Multi-Loop and Experiments(Will not be included in submission.)}

\begin{theorem}
  \label{provability}
Assume  is pure and non-overlapping. Let  and , where  are normal for all . Assume  with , let . For any , , we have  with . Note that  may contain .  We have the following: 

\noindent  is corecursively provable by some corecursive definition . 

\end{theorem}

\begin{proof}
we just need to corecursively reduce  to a proof term by
    the rewrite rules .  We have the following reduction:

    \begin{center}
      
    \end{center}
So we have       
\end{proof}

\begin{definition}[Finitely descends]
We say  can be (finitely) descended to  (by ) if there exist a finite
  sequence of syntatic equalities: , ,   ...,  such that  is identity. We call such  the descending depth. 
\end{definition}

\begin{definition}[Multi-Loop]
 Let  be a normal context and . If for any , , either  can be descended to  by , or , with , then we call (or just  ) a multi-loop.  
 \end{definition}


 \begin{definition}[Generalized Observational Point]
   Let  be a loop and . 
   We call  an observational point if it is of the form , where  is a substitution and . 
 \end{definition}

We use  to denote the set of observational points in . 


Note that in this section we assume outer-most reduction strategy, the trace equivalence will 
fail if we use innermost strategy, a counter example is \texttt{DList}. 
 \begin{definition}[Generalized Corecursive Point]

Let . We call  a corecursive point if it is of the form

\noindent , where  is normal, , and there
is a rule  with  occurs in . 
 \end{definition}

We use  to denote the set of corecursive points in .

\begin{theorem}

Assume  is pure and non-overlapping. Let  and , where  are normal for all . Assume  with , For any , , either  with , or  can be descended to  by . We have the following: 

\

\noindent  is corecursively provable by some corecursive definition . 

\end{theorem}


\begin{proof}

Let 
and .  
We want to show that if  can be descended to  by  with depth , then


\end{proof}

 \begin{definition}[Generalized Observational Point]
   Let  be a loop and . 
   We call  an observational point if it is of the form , where  is a substitution and . 
 \end{definition}



Note that in this section we assume outer-most reduction strategy, the trace equivalence will 
fail if we use innermost strategy, a counter example is \texttt{DList}. 
 \begin{definition}[Generalized Corecursive Point]

Let . We call  a corecursive point if it is of the form

\noindent , where  is normal, , and there
is a rule  with  occurs in . 
 \end{definition}

We use  to denote the set of corecursive points in .

\begin{conjecture}
  \
  
\noindent In Theorem \ref{provability}, the observational points  and the corecusive points  coincide, where . 
\end{conjecture}
\end{comment}
\end{document}
