\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{bm}
\usepackage{subfigure}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{multirow}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}



\newcommand{\cA}{\mathcal{A}}
\newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\cE}{\mathcal{E}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\cG}{\mathcal{G}}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\cI}{\mathcal{I}}
\newcommand{\cJ}{\mathcal{J}}
\newcommand{\cK}{\mathcal{K}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cM}{\mathcal{M}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\cO}{\mathcal{O}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\cQ}{\mathcal{Q}}
\newcommand{\cR}{\mathcal{R}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\cT}{\mathcal{T}}
\newcommand{\cU}{\mathcal{U}}
\newcommand{\cV}{\mathcal{V}}
\newcommand{\cW}{\mathcal{W}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}}
\newcommand{\cZ}{\mathcal{Z}}
\usepackage{graphicx}
\usepackage{soul}
\begin{document}

\title{Heterogeneous Molecular Graph Neural Networks for Predicting Molecule Properties
}

\author{
\IEEEauthorblockN{Zeren Shui}
\IEEEauthorblockA{Computer Science \& Engineering\\
University of Minnesota\\
Minneapolis, USA \\
shuix007@umn.edu}
\and
\IEEEauthorblockN{George Karypis}
\IEEEauthorblockA{Computer Science \& Engineering \\
University of Minnesota\\
Minneapolis, USA \\
karypis@umn.edu}
}

\maketitle

\begin{abstract}
As they carry great potential for modeling complex interactions, graph neural network (GNN)-based methods have been widely used to predict quantum mechanical properties of molecules. Most of the existing methods treat molecules as molecular graphs in which atoms are modeled as nodes. They characterize each atom's chemical environment by modeling its pairwise interactions with other atoms in the molecule. Although these methods achieve a great success, limited amount of works explicitly take many-body interactions, i.e., interactions between three and more atoms, into consideration. In this paper, we introduce a novel graph representation of molecules, heterogeneous molecular graph (HMG) in which nodes and edges are of various types, to model many-body interactions. HMGs have the potential to carry complex geometric information. To leverage the rich information stored in HMGs for chemical prediction problems, we build heterogeneous molecular graph neural networks (HMGNN) on the basis of a neural message passing scheme. HMGNN incorporates global molecule representations and an attention mechanism into the prediction process. The predictions of HMGNN are invariant to translation and rotation of atom coordinates, and permutation of atom indices. Our model achieves state-of-the-art performance in 9 out of 12 tasks on the QM9 dataset.
\end{abstract}

\begin{IEEEkeywords}
Heterogeneous molecular graphs, many-body interactions, graph neural networks, molecular property prediction
\end{IEEEkeywords}

\section{Introduction}
Predicting quantum mechanical properties of molecules based on their structures is important for molecule screening and drug design. We can compute exact molecular properties by solving the many-body Schr\"{o}dinger equation. However, closed form solution to this equation is only available for simple systems. Although researchers developed methods such as Density Functional Theory (DFT) \cite{hohenberg1964inhomogeneous} to approximate the solution, the computational cost of these methods scales poorly and is worse than  w.r.t. the number of electrons. 

Recently, researchers have been developing machine learning methods that are orders of magnitude faster with a moderate compromise in prediction accuracy. Among the machine learning approaches, graph neural network (GNN)-based methods attract a lot of research attention as their ability to model complex interactions among atoms. These methods treat molecules as molecular graphs (e.g., distance graphs \cite{unke2019physnet, SchNet-1, SchNet-2, MGCN}, chemical graphs \cite{MPNN}, -nearest neighbor graphs \cite{NMP-edge}) in which atoms are modeled as nodes. They compute an atom's low-dimensional representation as a function of its feature and characteristics of its graph neighbors. The low-dimensional representations are then used to estimate the local contribution of the atoms to the desired property, or to compute a global representation of the molecule for downstream predictions.





The many-body expansion (MBE)~\cite{MBE-1, MBE-2, MBE-NN} is an important scheme that computes the energy of an -particle system as the sum of the contributions of many-body terms

where  is the local energy contribution of a single atom,  is the energy contribution of a two-body (a group of two atoms),  is the energy contribution of a three-body, and eventually  is the contribution of the body formed by all the atoms in the molecule. Note that, the local contribution to the total energy decreases fast with the number of atoms in the many-body. 
As most of the existing GNN-based methods are developed on molecular graphs, they focus mainly on modeling atom-based representations, interactions, and predictions which correspond to the first two terms of the series and do not have an explicit characterization of the higher order terms. This may compromise their accuracy in the chemical prediction problems.

In this paper, we introduce a novel graph representation of molecules, \emph{heterogeneous molecular graph} (HMG), to explicitly model many-body interactions. A -body (the value of  is called the \emph{order} of the many-body) is a group of  atoms that functions as a whole entity. In HMGs, a -body is modeled as a node of order . Nodes connect to nodes of the same or different order via different types of edges. This heterogeneous structure allows us to explicitly model interactions, representations, and predictions associated with many-bodies. Moreover, edges between nodes of the same order carry the potential of incorporating complex geometric information (e.g., bond angles and dihedral angles) into node embeddings. 

To leverage the rich information stored in HMG for tasks of molecular property predictions, we design heterogeneous molecular graph neural networks (HMGNN) by following a message passing framework. In the message passing framework \cite{MPNN}, nodes send and receive messages from their neighbors and update their low-dimensional representations using the received messages. HMGNN is a multi-task learning \cite{MTL} model whose design is inspired by the MBE of energy surfaces. In HMGNN, each many-body order possesses its own set of parameters and shares computations with other orders. In the prediction phase, HMGNN computes one estimation for each many-body and aggregates them based on their orders. It uses an attention-based model that takes into account a global representation of the molecule to fuse the prediction of different orders, which correspond to different terms in Eq~\ref{eq:MBE}. We design a multi-task learning loss that enforces the prediction of each order and the fused prediction to be close to the true target. Experimental results show that the fused prediction is better than any of the standalone predictions. The fusing weight of the predictions are also consistent with the convergence assumption in the many-body expansion. 

The main contribution of this work lies in two folds. First, we propose HMG which allows graph learning methods to explicitly model many-body representation, interaction, and prediction. Second, we develop a multi-task learning method HMGNN for the task of molecule property prediction. HMGNN explicitly incorporates many-body interaction and a global molecule representation into the prediction process and achieves state-of-the-art performance on the QM9 dataset \cite{QM9-1, QM9-2}. The code of HMGNN is available online\footnote{https://github.com/shuix007/HMGNN}.

\section{Review of relevant prior works}

Traditionally, prediction of many important molecular properties such as atomization energies relies on methods that approximate the solution of the many-body Schr\"{o}dinger equation such as density function theory (DFT) and its variants \cite{DFT}. This class of methods involves solving complex linear systems and has a computational complexity worse than  where  is the number of atoms. 

Recent years have seen a surge in data-driven methods that train machine learning models to learn patterns from molecule databases. The learned patterns are assumed to be general in chemical space and can be used to estimate properties of unknown compounds. These attempts started from \cite{faber2017machine, bartok2017machine} which feed hand-crafted molecule descriptors (e.g., Coulomb matrix, bag of bonds) into regression models such as linear regression and random forests. These methods rely heavily on the quality of the crafted descriptors and have limited representation power. 

Recently, graph neural networks (GNN) have been achieving a great success in graph-related applications \cite{GCN, GAT, SAGE, DiffPool}. In chemistry, researchers developed GNN-based method for learning tasks over graph represented molecules. The authors of \cite{MPNN} introduced a generic framework over chemical graphs that models interactions between atoms in a message passing fashion. In \cite{DTNN, SchNet-1, SchNet-2}, the authors designed neural network structures that have no dependency on hand-crafted features but learn molecule representations from only atom types and coordinates. Since GNNs possess a hierarchical structure, i.e., they iteratively apply GNN layers on graphs to encode each node's multi-hop neighbors into its embedding, GNN-based methods \cite{HIP-NN} and \cite{unke2019physnet} further decompose atom-wise prediction to layer-wise atom prediction to fit in the MBE framework. Although these methods include many-body contributions into final predictions, they do not have an explicit modeling of many-body representations and interactions. Some recent works have incorporated many-body interactions and representations by updating edge embeddings along message passing \cite{NMP-edge} or by passing messages on line graphs of the corresponding molecular graphs \cite{DimeNet}. However, these methods capture only partial many-body interactions and lack many-body predictions.

Equivariant neural network is another class of neural network methods that has been applied in chemical prediction problems. The notion of group equivariant neural network was first introduced by \cite{cohen2016group} in the domain of image processing. Later, researchers developed neural network methods that are equivariant to continuous rotations for learning representations for 3D objects, including molecules \cite{anderson2019cormorant, thomas2018tensor, kondor2018covariant}. These methods achieve rotation invariance by transforming objects from Euclidean space to Fourier space and conducting computations in Fourier space. In these methods, each many-body interacts only with itself but not other many-bodies. Thus, they are not optimal in predicting molecule properties.



\begin{figure*}[t]
\centering
\subfigure[A formaldehyde molecule.]{
\includegraphics[width=0.2\textwidth]{CH2O_molecule.png}
\label{subfig:molecule}}
\subfigure[A molecular graph of the formaldehyde molecule.]{
\includegraphics[width=0.3\textwidth]{ch2o_MG.pdf}
\label{subfig:mg}}
\subfigure[A heterogeneous molecular graph of the molecular graph.]{
\includegraphics[width=0.35\textwidth]{ch2o_HMG.pdf}
\label{subfig:hmg}}

\caption{An example of heterogeneous molecular graph (HMG). Figure \ref{subfig:molecule} is a spatial structure of a formaldehyde () molecule. Each atom in the molecule is associated with a three-dimensional coordinates in the Euclidean space. Figure \ref{subfig:mg} is the molecular graph of the methanol molecule with a cutoff distance . We convert atom coordinates to pair-wise distances to guarantee translation and rotation invariance of the representation. We denote edges whose distances are less than  using black solid lines, and edges that are broke by the cutoff using black dotted lines. Figure \ref{subfig:hmg} is a HMG of order two constructed from the molecular graph. There are two types of nodes (-bodies and -bodies denoted by yellow and blue circles, respectively) and three types of edges (- and - denoted by yellow and blue lines, respectively, - denoted by black dashed lines) in the HMG. Edges between nodes of the same order are associated with features that depict the geometric relation between the nodes (distance for - edges, angle for - edges). }
\label{fig:HMG}
\end{figure*}

\section{Notations and Definitions}
We denote matrices by bold upper-case letters (e.g., ), and vectors by bold lower-case letters (e.g., ). We denote entries of a matrix/vector by lower-case letter with subscripts (e.g., /). We use superscripts to indicate variables at the -th message passing layer (e.g., ). We denote \emph{molecular graphs} by  where  and  represent the set of nodes (atoms) and edges, respectively. Two atoms are connected in a molecular graph when the Euclidean distance between them is less than a cutoff threshold . Each edge in the graph is associated with a distance to store the geometric structure of the molecule. We define a -body in a molecular graph  as a -clique of the graph. We refer to the value of  as the order of the many-body.





\section{Heterogeneous Molecular Graph and Many-Body Interactions}

In this section, we illustrate the construction of \emph{heterogeneous molecular graphs} (HMG) and how we leverage the heterogeneous structure of HMGs to model many-body representations and interactions. 

\subsection{Heterogeneous Molecular Graph} \label{Sec:HMG}

An HMG is a graph in which nodes are many-bodies and edges are defined by various types of geometric and set relations. HMGs are constructed from molecular graphs. We denote an HMG of order  of a molecular graph  as  where ,  is the set of -bodies in  (i.e., all -cliques of ), and  is the set of edges between  and . We denote the order  of -bodies as the node type and - as the type of the edges that connect nodes of order  and nodes of order . Given two nodes  and , when they are of the same order, i.e., ,  and  are connected if they share  atoms. A special case is when , instead of building a complete graph, we use the edge set  of the molecular graph to define connections. When the two nodes are of different orders, presumably ,  if  is a sub-graph of . An example HMG is shown in Figure \ref{fig:HMG}. With this formulation, we can explicitly model up to -body representations by node embeddings and -body interactions by message passing.  

In an HMG, each node  of order  is associated with a discrete feature  that indicates its atomic composition, and a continuous feature  that describes aspects of its geometry. Note that, nodes of order  do not have continuous features since they are points in the Euclidean space and do not have geometric structure. Each edge  is associated with an edge feature  when  and  are of the same order . The edge feature characterizes the geometric relation between the two nodes, e.g., distance between atoms, angles between bonds. In this paper, we use a hash function to map the set of atomic numbers of the atoms to . Construction of continuous node features and edge features requires feature engineering especially when order of the many-bodies are high. We will illustrate how we convert geometric information to feature vectors up to the second order in Section \ref{Sec:implement}.

\subsection{Message Passing on Heterogeneous Molecular Graphs} 
The message passing framework consists of two phases, message passing and node update. On molecular graphs, each node (atom)  sends/receives messages to/from its neighbors and uses the received messages to update its embedding

In Eq-\ref{eq:MP},  is the set of neighbor nodes of ,  is the node (atom) embedding of ,  is the aggregation of messages from 's neighbor nodes,   is the edge feature associated with the edge between  and ,  is a message function that maps embeddings of the sender and the receiver and the corresponding edge feature to a message vector,  is a node update function that combines the incoming message and the old embedding to be the new node embedding. Both  and  are learnable. Message passing on HMGs is different from that on molecular graphs due to the heterogeneous property of HMGs. Nodes in HMGs are of different orders and they pass messages through edges of different types. A message passing framework needs to learn edge type specific message functions and order specific node update functions to capture this heterogeneous structure. Moreover, the framework should allow inter-order message passing such that the node embeddings can capture information from other orders. For example, by passing messages from -bodies, -bodies can encode edge angle information into their embeddings. Let  be a node of order  in a HMG and  be its embedding at the -th layer, we design the message passing framework as

where  the set of nodes of order  that are connected to ,  denotes the aggregated messages from nodes 's neighbor nodes of order ,  denotes the edge feature between  and  if they are of the same order,  and  are learnable functions specific to edge type  and node type (order) , respectively. Compare to the message passing framework on molecular graphs which has two functions to learn, this framework possesses larger model capacity and is able to model many-body interactions explicitly.

\section{Heterogeneous Molecular Graph Neural Networks.}

\begin{figure*}[t]
\centering

\includegraphics[width=0.97 \textwidth]{HMGNN.pdf}
\caption{Computation flow of heterogeneous molecular graph neural networks (HMGNN) for many-bodies up to order two. We use  to represent the input to the function. The activation function is set to be the shifted softplus function, i.e., . Each many-body order  owns its input module, interaction module, and output module. For each node  of order , an input module converts the discrete and continuous feature of the node to an initial node embedding . HMGNN passes the initial embeddings through a stack of  interaction modules to encode information from its neighbor nodes of different orders to the node embedding. The outputs of the last interaction module, the final node embedding , are then fed into a fusion module and an output module to compute a weight vector  and prediction , respectively. HMGNN sums the predictions per many-body order and computes the final prediction as a weighted sum of these summed predictions.}
\label{fig:HMGNN}
\end{figure*}

We present Heterogeneous Molecular Graph Neural Networks (HMGNN) for the purpose of predicting molecule properties. An HMGNN contains four types of modules, input module, interaction module, output module, and fusion module. All the modules except the fusion module are order specific. HMGNNs learn functions for message passing on heterogeneous molecular graphs to compute local node representations, and uses a readout function to combine the representations to form a global molecule representation. HMGNNs compute node-wise contributions to the target property and aggregates them based on their orders. The final prediction is a weighted combination of the predictions of all orders where the weights are computed by an attention mechanism from the global molecule representation. An HMGNN is learned by optimizing a loss function which forces predictions of each order and the fused prediction to be close to the true target. Since the construction of heterogeneous molecular graphs and associated features rely on atom pairwise distances and atomic numbers but not atom coordinates, HMGNNs are invariant under both translations and rotations. HMGNNs are also permutation invariant to atom indices as the message aggregation function in Eq-\ref{Eq:HOMP} and the readout function are permutation invariant \cite{xu2018how}. Figure \ref{fig:HMGNN} shows an overview of the architecture of HMGNN.



\subsection{Input Module}

The input module of HMGNN converts raw features of nodes to latent embeddings. As we described in Section \ref{Sec:HMG}, each node  in a HMG is associated with a discrete feature  and a continuous feature . We use an embedding lookup table to map the discrete feature  to a real value vector  and apply a fully connected layer to the concatenation of the latent vector  and the continuous feature  to get the initial node embedding

where  and  are learnable parameters for nodes of order  (-bodies),  is an element-wise activation function,  denotes concatenation of vectors.

\subsection{Interaction Module}

HMGNN stacks  interaction modules to encode information across far reaches of the heterogeneous molecular graph into node embeddings. Each interaction module takes the output embeddings of the previous module and update the embeddings. Note that, edges between nodes of the same orders have features while other edges do not. As a result, we paramatrize the message functions between nodes of the same order as

and the message functions along edges without features as

In Eq-\ref{eq:mp1} and Eq-\ref{eq:mp2},  and  denotes the set of neighbor nodes of order  and order  of node , respectively,  denotes the Hadamard product, , , and  are learnable parameters. A node embedding  is then updated as a function of its old embedding and the incoming messages,

where  denotes concatenation of vectors. The interaction module then refines the node embeddings with two consecutive fully connected layers with residual connections \cite{ResNet}. 

\subsection{Output Module}

Each many-body order  possesses a specific output module that passes the output of its interaction module, final node embeddings , through a sequence of linear mappings and a aggregation process to compute the estimated value of the target property. First, we use a fully connected layer to convert the node embeddings to node predictions

where  and  are learnable parameters for nodes of order . Then we follow \cite{unke2019physnet} and scale the predictions with scaling parameters that are specific to the discrete feature  of the nodes

where  and  are learnable embedding lookup tables that map  to the corresponding scaling factors and shifts. The goal of the scaling layer is to adapt the magnitude of the predictions to different unit systems of the target property.

\subsection{Fusion Module} \label{Sec:fuse}

The fusion module computes a global molecule representation out of the final node embeddings and uses the global representation to weigh the prediction of different orders. We sum the final node embeddings  of each -body to form an order specific representation and concatenate them to be an intermediate representation

Since node embeddings of different orders are computed by different parameters and the number of nodes of the orders also varies, the distributions of the order specific representations could be dramatically different from each other. In order to unify the distributions of the representations and to accelerate training, we apply batch normalization \cite{batchnorm} followed by a fully connected layer on the intermediate representation to obtain the global representation

Then we pass the global representation through an attention layer to compute the weight  that measures the importance of the predictions of order 

where  are learnable vectors, and . We can understand the global representation as a query to the knowledge-base distilled in  for assigning contributions to predictions of different orders. This gives the model better flexibility and explainability in dealing with different molecules.

\subsection{Final prediction}

Inspired by the many-body expansion, we decompose the final prediction as a weighted sum of the prediction of different orders

where the weights  are computed by the fusion module.

\subsection{Model Training}

Since all the modules in HMGNNs except for the fusion module are order specific, and the final prediction is a weighted average of the predictions per order, training HMGNNs by optimizing objective functions that only depend on the final prediction (the fused prediction) may cause gradient vanishing issues for parameters of some orders so that these parameters do not learn enough and lose their prediction utilities. To avoid this issue, we treat the computation of each order as a separate prediction task and propose a multi-task objective function that forces the prediction of all orders together with the final prediction to be close to the true target

where  is the node order specific prediction,  denotes all trainable parameters of the model,  is a hyper-parameter that controls the strength of  normalization to prevent the model overfits. This objective function preserves gradient flow for parameters of each order and gives higher training importance to orders that the fussing module assigning larger weights to.

\subsection{Complexity Analysis}

The time and space complexity of HMGNN depends linearly on the number of nodes and edges in a HMG. The number of nodes determines the complexity of the input module and the output module while the number of edges determines the complexity of message passing. 

Let  be a molecular graph with  atoms and  be its HMG that explicitly models up to -bodies. We assume  is a complete graph for the worst case scenario. The number of nodes of order  in  is . Let  be a node of order  (i.e., a -body),  is connected to nodes that are of order  where . When , the number of  order neighbors of node  is  as  is connected to all -bodies who are sub-graphs of ; when , the number of order  neighbors of  is  since  is connected to -bodies who share  atoms with ; When , the number of -body neighbors of  is . As a result, the complexity of message passing is 

and the complexity of the input/output module of HMGNN is .

In this paper, we experiment with HMGs and HMGNNs for up to -bodies, consequently, the time complexity and space complexity of our model are both . Modern computing architectures such as graphics processing unit (GPU) and tensor processing unit (TPU) are optimized to accelerate this computation. Empirically, HMGNNs can generate property predictions for 10000 randomly drawn molecules from the QM9 dataset in 4 seconds.

\section{Experiments}

We conduct experiments to investigate three research problems in regards of many-body modeling and the HMGNN model
\begin{itemize}
    \item How does HMGNN perform in the molecule property prediction tasks compared against the current state-of-the-art methods?
    \item How does many-body representation, interaction, and prediction contribute to the prediction?
    \item What is the utility of the components of HMGNN?
\end{itemize}

\subsection{Implementation Details} \label{Sec:implement}

We experiment with HMGs and HMGNNs for many-bodies up to order two. There are two types of nodes (-bodies and -bodies), two types of edges with edge features (- and - edges), and one type of edge without edge features (- edges). Since -bodies are atoms, they only have discrete features. Each -body  is determined by its two end atoms and the distance between them . 

There are three types of geometries that we need to model, distance  between -bodies  and , length  of -bodies, and angle  between -bodies  and . We use a set of  radial basis functions (RBF) to convert the scalar geometries to real valued vector features. Let  be a scalar input and  be the real valued output of the RBFs, the -th entry of  is computed as

where  and  specify the center and width of . For distance  between -bodies, we multiply its feature vector by a continuous monotonic decreasing function  that has  and . With this formulation, an -body node will have less influence to/from its distant order  neighbors. We follow \cite{unke2019physnet} and set the value of  to be equally spaced between  and  while . The goal of using RBFs is to decorrelate the scalar features to accelerate training \cite{SchNet-1}. We apply three different sets of RBFs to convert the distance , the length , and the angle  to the corresponding features , , and , respectively. 

We set the latent dimension to be  and use  interaction modules for our experiments. We use the shifted softplus function as the activation function. For ZPVE, , , ,  and , the cutoff distances  while for other targets . We initialize the weights of fully connected layers with random orthogonal matrices scaled by the glorot initialization scheme \cite{glorot} and the bias to zero. For learning the parameters of HMGNN, we run the AMSGrad algorithm \cite{amsgrad} with a batch size of  for up to  steps and set the  regularizer  to be . We initialize the learning rate to be  and multiply it with  every  gradient steps. The training algorithm stops if the MAE on the validation set does not decrease for  steps. We implement HMGNN using the Deep Graph Library (DGL) \cite{dgl-1, dgl-2}. 

\subsection{Experimental Setting}

\begin{table}[t]
\captionof{table}{Target properties in the QM9 dataset.}
\begin{center}
\begin{tabular}{l|l}
\toprule
Target & Description \\
\midrule
 & Dipole moment \\
 & Isotropic polarizability \\
 & Energy of Highest occupied molecular orbital (HOMO) \\
 & Energy of Lowest occupied molecular orbital (LUMO) \\
 & Gap, difference between LUMO and HOMO \\
 & Electronic spatial extent \\
ZPVE & Zero point vibrational energy \\
 & Internal energy at  K \\
 & Internal energy at  K \\
 & Enthalpy at  K \\
 & Free energy at  K \\
 & Heat capacity at  K \\
\bottomrule
\end{tabular}
\label{Table:Property}
\end{center}
\end{table}

We evaluate the performance of the proposed model on the QM9 dataset \cite{QM9-1, QM9-2}. QM9 is a widely used benchmark for evaluating models that predict molecule properties. It consists of around K equilibrium molecules associated with  geometric, energetic, electronic, and thermodynamic properties. The properties are described in Table \ref{Table:Property}. These molecules contain up to nine heavy atoms (C, O, N, and F). We randomly select  molecules for training,  molecules for validation, and  molecules as the test set. We conduct model selection for different targets on the validation set and report the mean absolute error (MAE) of the best performing models. For properties with atomic reference values (, , , , ), we subtract the original value by the per-atom-type reference values to be the target. Since  is defined as the gap between  and , we predict it as . In our experiments, we convert the units of , , , ZPVE, , , ,  to eV.

We compare the performance of HMGNN with six state-of-the-art methods, enn-s2s \cite{MPNN}, SchNet \cite{SchNet-1}, neural message passing with edge updates (NMP-edge) \cite{NMP-edge}, Cormorant \cite{anderson2019cormorant}, PhysNet \cite{unke2019physnet}, and directional message passing neural network (DimeNet) \cite{DimeNet}. Results of enn-s2s, SchNet, NMP-edge, Cormorant, and DimeNet are from the corresponding papers. We take the results of PhysNet from \cite{DimeNet}.

\subsection{Prediction Performance}

\begin{table*}[t]
\captionof{table}{Mean absolute error on QM9 with 110K training molecules. In each row, we use boldface for the best performance method. Column HMGNN- and HMGNN- correspond to the performance of summing over predictions of -bodies and -bodies, respectively.}
\begin{center}
\begin{tabular}{l|l|rrrrrr|rrr}
\toprule
Target & Unit & enn-s2s & SchNet & NMP-edge & Cormorant & PhysNet & DimeNet & HMGNN- & HMGNN- & HMGNN \\
\midrule
 & D & 0.030 & 0.033 & 0.029 & 0.038 & 0.0529 & 0.0286 & 0.0276	& 0.0283 & \textbf{0.0272} \\
 &  & 0.092 & 0.235 & 0.077 & 0.085 & 0.0615 & \textbf{0.0469} & 0.0571 & 0.0647 & 0.0561 \\
 &  meV & 43 & 41 & 36.7 & 34 & 32.9 & 27.8 & 24.94 & 26.31 & \textbf{24.78}  \\
 & meV & 37 & 34 & 30.8 & 38 & 27.4 & \textbf{19.7} & 20.72 & 21.42 & 20.61 \\
 &  meV & 69 & 63 & 58.0 & 61 & 42.5 & 34.8 & 33.44  & 35.02 & \textbf{33.31} \\
 &   & 0.180 & 0.073 & \textbf{0.072} & 0.961 & 0.765 & 0.331 & 0.43 & 0.6 & 0.416 \\
ZPVE & meV & 1.5 & 1.7 & 1.49 & 2.03 & 1.39 & 1.29 & 1.24 & 1.34 &	\textbf{1.18} \\
 & meV & 19 & 14 & 10.5 & 22 & 8.15 & 8.02 & 6.19 & 9.06 & \textbf{5.92} \\
 & meV & 19 & 19 & 10.6 & 21 & 8.34 & 7.89 & 7.22 & 11 & \textbf{6.85} \\
 & meV & 17 & 14 & 11.3 & 21 & 8.42 & 8.11 & 6.35 & 8.37 & \textbf{6.08} \\
 & meV & 19 & 14 & 12.2 & 20 & 9.40 & 8.98 & 7.95 & 11.06 & \textbf{7.61} \\
 &  & 0.040 & 0.033 & 0.032 & 0.026 & 0.0280 & 0.0249 & 0.0241 & 0.025 & \textbf{0.0233} \\
\bottomrule
\end{tabular}
\label{Table:Result}
\end{center}
\end{table*}

\begin{figure*}[t]
\centering
\subfigure[.]{
\includegraphics[width=0.23 \textwidth]{cut_off_U0.pdf}
\label{subfig:U0}}
\subfigure[.]{
\includegraphics[width=0.23 \textwidth]{cut_off_Cv.pdf}
\label{subfig:Cv}}
\subfigure[ZPVE.]{
\includegraphics[width=0.23 \textwidth]{cut_off_ZPVE.pdf}
\label{subfig:zpve}}
\subfigure[.]{
\includegraphics[width=0.23 \textwidth]{cut_off_mu.pdf}
\label{subfig:mu}}

\caption{Effect of the cutoff distance  on prediction performance on four target properties. }
\label{fig:cut_r}
\end{figure*}

We show the prediction performance of HMGNN and the competing methods on the 12 properties of QM9 in Table \ref{Table:Result}. Our proposed method sets the new state-of-the-art on 9 out of the 12 target properties. HMGNN's performance aligns with the best results on the remaining targets with an exception of . We also present the performance of summing over predictions over -bodies (HMGNN-) and -bodies (HMGNN-), respectively. Although the performance of HMGNN- is consistently worse than HMGNN-, their weighted combination outperforms any of the standalone prediction. This demonstrates the effectiveness of the fusion module driven by the global molecule representations and the attention mechanism, and that explicitly modeling and computing predictions of many-bodies can be beneficial for chemical prediction tasks.

We analyze the effect of a critical hyper-parameter, the cutoff distance , on prediction performances of four types of properties. We choose  to represent properties related to atomization energies (, , , ),  to represent thermodynamic properties (), ZPVE to represent properties related to fundamental vibrations of the molecule (ZPVE), and  to represent electronic properties (, , , , , ) \cite{MPNN}. We present the training and test mean absolute error (MAE) of HMGNNs on HMGs constructed with  in Figure \ref{fig:cut_r}. 

When constructing molecular graphs as well as HMGs, the larger the cutoff distance we choose, the less geometric information about the molecules that we lose. However, a large cutoff value does not always lead to better performance. In Figure \ref{fig:cut_r}, despite the training error decreases across all the four targets as the cutoff value increases, the test error shows an increasing trend for three properties. This is a signal that the model over-fits the training set on the three properties. This is because of the large model capacity of HMGNNs as they have one set of parameters for each many-body order. An HMGNN of order  possesses  times the number of parameters of a normal GNN-based model. 



\subsection{Ablation Study}

\begin{table}[t]
\captionof{table}{Ablation study on  and .}
\begin{center}
\begin{tabular}{llccc}
\toprule
Target & Architecture & HMGNN-1 & HMGNN-2 & HMGNN \\
\midrule
\multirow{4}{*}{} & Default & 6.19 & 9.06 & 5.92 \\
& Remove MTL & 8.22 & 9716.95 & 8.22 \\
& Remove IOMP & 10.26 & 8.18 & 7.88 \\
& Remove HO & 10.08 & - &  - \\
\midrule
\multirow{4}{*}{} & Default & 0.0241 & 0.0250 & 0.0233 \\
& Remove MTL & 0.0247 & 1.4022 & 0.0247\\
& Remove IOMP & 0.0297 & 0.0275 & 0.0244 \\
& Remove HO & 0.0289 & - &  - \\
\bottomrule
\end{tabular}
\label{Table:mtl}
\end{center}
\end{table}

In this section, we conduct ablation study on two targets (i.e., , ) to demonstrate the importance of the multi-task learning loss, inter-order message passing, and explicit modeling of high-order bodies in improving the performance of molecular property prediction. We propose three variants of the HMGNN model and show their results in Table \ref{Table:mtl}.

\subsubsection{Remove MTL (Multi-Task Learning loss)} This variant has the same specification with the default model. It differs with the default model in that it is trained by minimizing the naive loss  instead of the multi-task learning loss that we proposed in Eq-\ref{eq:mtlloss}. As shown in Table \ref{Table:mtl}, the -bodies of this variant lose their prediction power while the fusion module gives all attention weights to the -bodies, and as a result, the performance of this variant is worse than the default HMGNN. Furthermore, the prediction of -bodies (i.e., HMGNN-1) is also less accurate than the default model. 

\subsubsection{Remove IOMP (Inter-Order Message Passing)} This variant removes edges/messages between -bodies and -bodies, as a result, information of the two orders are not shared. We can see that the performance of HMGNN- and HMGNN drops in the prediction of both  and . This demonstrates the importance of inter-order message passing. However, the prediction accuracy of HMGNN- on  is better than models with inter-order message passing. This might because -bodies (both distance and angle) contain more geometric information than -bodies (only distance). 

\subsubsection{Remove HO (High-Order modeling)} This variant removes high-order related modeling (-body interaction, representation, and prediction) and is similar to existing GNN-based prediction methods (i.e., PhysNet). As shown in Table \ref{Table:mtl}, this method performs worse than HMGNN-1 of the variant that removes multi-task learning loss. This shows another evidence of the effectiveness of inter-order message passing. 











\subsection{Visualization of Attention weights}

\begin{figure*}[t]
\centering
\subfigure[.]{
\includegraphics[width=0.33 \textwidth]{hist_U0.pdf}
\label{subfig:U0}}
\subfigure[.]{
\includegraphics[width=0.33 \textwidth]{hist_Cv.pdf}
\label{subfig:Cv}}
\subfigure[ZPVE.]{
\includegraphics[width=0.33 \textwidth]{hist_zpve.pdf}
\label{subfig:zpve}}
\subfigure[.]{
\includegraphics[width=0.33 \textwidth]{hist_mu.pdf}
\label{subfig:mu}}

\caption{Attention weights generate by the fusion module for predicting the four properties. }
\label{fig:att}
\end{figure*}

In Figure \ref{fig:att}, we show the attention scores of the -body predictions generated by the fusion module for predicting , , , and ZPVE on the test set. Since we only experiment with many-bodies up to the second order, the attention weights of the -bodies is one minus that of the -bodies. On the four types of chemical properties, -body contribution dominates the prediction of most of the molecules. However, -body predictions also take a considerable amount of attention. 

\section{Conclusion}

We propose a novel heterogeneous graph based molecule representation, heterogeneous molecular graph (HMG), to model many-body representations and interactions. Inspired by the many-body expansion of energy surfaces, we design a heterogeneous molecular graph neural network (HMGNN) to leverage the rich information stored in HMGs for molecular prediction tasks. HMGNN follows a message passing paradigm and leverages global molecule representations using an attention mechanism. We propose to train HMGNNs by optimizing a multi-task learning loss. HMGNN achieves state-of-the-art performance on 9 out of 12 properties on the QM9 dataset. Experiments also show that the multi-task learning loss improves the generalization of the model. In this paper, we only model many-bodies up to the second order, future works should aim to model many-bodies of higher than third orders and also to enable HMGNNs for another important chemical prediction tasks, molecular dynamics simulations.

\section{Acknowledgement}

This work was supported in part by NSF (1447788, 1704074, 1757916, 1834251), Army
Research Office (W911NF1810344), Intel Corp, and the Digital Technology Center at the
University of Minnesota. Access to research and computing facilities was provided by
the Digital Technology Center and the Minnesota Supercomputing Institute. We are grateful to Mingjian Wen for his fruitful comments, corrections and inspiration.

\bibliographystyle{IEEEtran}
\bibliography{conference_101719}

\end{document}
