\documentclass[]{elsarticle}




\usepackage{lineno,hyperref}


\usepackage[utf8]{inputenc}
\usepackage{booktabs,tabularx}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{pifont}
\usepackage[table,usenames,dvipsnames,svgnames]{xcolor} \usepackage{colortbl}
\usepackage{wrapfig}
\usepackage{tabu}
\usepackage{bm}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{upgreek}
\usepackage{relsize}
\graphicspath{{figures/}}

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\newcommand{\algorithmicbreak}{\textbf{break}}
\newcommand{\BREAK}{\STATE \algorithmicbreak}
\makeatother

\newcommand{\sgn}{\operatorname{sgn}}
\newcommand{\setHelper}[1]{\left\lbrace #1 \right\rbrace}
\newcommand{\Omicron}{\mathcal{O}}


\usepackage{blindtext}
\newcommand{\todo}[1]{\textcolor{red}{\textbf{[[#1]]}}}
\newcommand{\phony}[1]{\textcolor{gray}{#1}}
\newcommand{\blind}[1][1]{\textcolor{gray}{\Blindtext[#1][1]}}
\newcommand{\spanhel}[1]{\textcolor{blue}{#1}}

\newcommand{\dashrule}[1][black]{\color{#1}\rule[\dimexpr.5ex-.2pt]{4pt}{.4pt}\xleaders\hbox{\rule{4pt}{0pt}\rule[\dimexpr.5ex-.2pt]{4pt}{.4pt}}\hfill\kern0pt}
\newcommand{\rulecolor}[1]{\def\CT@arc@{\color{#1}}}

\setlength{\fboxsep}{0.005pt}
\newcommand{\tmpframe}[1]{\fbox{#1}}
\renewcommand{\tmpframe}[1]{#1}



\usepackage{booktabs}
\usepackage{multirow}

\usepackage{stackengine}
\usepackage{scalerel}
\newlength\lthk
\setlength\lthk{.1ex}
\def\bline{\rule{2ex}{\lthk}}
\def\slash{\rotatebox{60}{\bline}}
\def\parallelogram{\stackMath\scalerel*{\def\stackalignment{l}{\stackunder[-.5\lthk]{\def\stackalignment{r}\stackon[-.5\lthk]{\slash\rule{.866ex}{0ex}\slash}{\bline}}{\bline}}}{\square}}

\definecolor{verylightgray}{gray}{0.95}
\newlength{\tabwidth}

\newcommand{\proc}[1]{\textcolor[rgb]{0.64,0,0}{#1}}
\newcommand{\abso}[1]{\textcolor[rgb]{0,0.6875,0.93}{#1}}
\newcommand{\gt}[1]{\hat{#1}}





\hyphenation{op-tical net-works semi-conduc-tor}





\newcommand{\ITSCalib}{ITS15}
\newcommand{\EdgeLetsCalib}{Edgelets}
\newcommand{\ManualCalib}{ManualCalib}

\newcommand{\BMVCScale}{BMVC14}
\newcommand{\BBScale}{BBScale}
\newcommand{\BBScaleReg}{BBScale + reg}
\newcommand{\ManualScale}{ManualScale}
\newcommand{\SpeedScale}{SpeedScale}

\hypersetup{
	colorlinks=true,
	linkcolor=BrickRed,
	citecolor=OliveGreen,
	filecolor=magenta,
	urlcolor=cyan
}

\journal{Computer Vision and Image Understanding.}
\usepackage{numcompress}\bibliographystyle{model4-names}\biboptions{authoryear}

\begin{document}
\begin{frontmatter}

\title{Traffic Surveillance Camera Calibration\\by 3D Model Bounding Box Alignment\\for Accurate Vehicle Speed Measurement}


\cortext[fn:BrnoPhdTalent]{Accepted to CVIU. DOI: 10.1016/j.cviu.2017.05.015. Corresponding author at BUT FIT, Božetěchova 2, 612 66 Brno, Czech Republic. Jakub Sochor is a Brno Ph.D. Talent Scholarship Holder --- Funded by the Brno City Municipality.}
\author[AddressIT4I]{Jakub Sochor\corref{fn:BrnoPhdTalent}}
\ead{isochor@fit.vutbr.cz}
\author[AddressIT4I]{Roman Juránek}
\ead{ijuranek@fit.vutbr.cz}
\author[AddressIT4I]{Adam Herout}
\ead{herout@fit.vutbr.cz}
\address[AddressIT4I]{Brno University of Technology,
	Faculty of Information Technology,\\
	Centre of Excellence IT4Innovations,
	Božetěchova 2,
	612 66 Brno,
	Czech Republic
}


\begin{abstract}
In this paper, we focus on fully automatic traffic surveillance camera calibration, which we use for speed measurement of passing vehicles. We improve over a recent state-of-the-art camera calibration method for  traffic surveillance based on two detected vanishing points. More importantly, we propose a novel automatic scene scale inference method. The method is based on matching bounding boxes of rendered 3D models of vehicles with detected bounding boxes in the image. The proposed method can be used from arbitrary viewpoints, since it has no constraints on camera placement. 
We evaluate our method on the recent comprehensive dataset for speed measurement BrnoCompSpeed. Experiments show that our automatic camera calibration method by detection of two vanishing points reduces error by 50\,\% (mean distance ratio error reduced from 0.18 to 0.09) compared to the previous state-of-the-art method. We also show that our scene scale inference method is more precise, outperforming both state-of-the-art automatic calibration method for speed measurement (error reduction by 86\,\% -- 7.98\,km/h to 1.10\,km/h) and manual calibration (error reduction by 19\,\% -- 1.35\,km/h to 1.10\,km/h). 
We also present qualitative results of the proposed automatic camera calibration method on video sequences obtained from real surveillance cameras in various places, and under different lighting conditions (night, dawn, day).
\end{abstract}

\begin{keyword}
speed measurement \sep camera calibration \sep fully automatic \sep traffic surveillance \sep bounding box alignment \sep vanishing point detection
\end{keyword}



\end{frontmatter}


\section{Introduction}

Surveillance systems pose specific requirements on camera calibration.  Their cameras are typically placed in hardly accessible locations and optics are focused at longer distances, making the common pattern-based calibration approaches unusable (such as classical \cite{Zhang2000}). That is why many solutions place markers to the observed scene and/or measure existing geometric features \citep{Sina2013,Do2015,You2016,Luvizon2016}.  These approaches are laborious and inconvenient both in terms of camera setup (manually clicking on the measured features in the image) and in terms of physically visiting the scene and measuring the distances.

In our paper, we focus on \emph{precise} and at the same time \emph{fully automatic} traffic surveillance camera calibration including scene scale for speed measurement. The proposed speed measurement method needs to be able to deal with significant viewpoint variation, different zoom factors, various roads and densities of traffic. If the method should be applicable for large-scale deployment, it needs to run fully automatically without the necessity to stop traffic for installation or for performing calibration measurements.

\begin{figure}[h]
	\centering
	\fbox{\includegraphics[width=0.497\linewidth]{images_render_real_real_1640_185288.png}\includegraphics[width=0.497\linewidth]{images_render_real_render_1640_185288.png}}\
	\label{eq:focal}
	f &=& \sqrt{-\mathbf{u}^T \cdot \mathbf{v}}\\
	\mathbf{\overline{u}} &=& [u_x, u_y, f]^T\\
	\mathbf{\overline{v}} &=& [v_x, v_y, f]^T\\
	\mathbf{\overline{w}} &=& \mathbf{\overline{u}} \times \mathbf{\overline{v}} \\
	\mathbf{n} &=& \frac{\mathbf{\overline{w}}}{\|\mathbf{\overline{w}}\|}\\
	\boldsymbol{\rho} &=& \left[\mathbf{n}^T,\delta\right]^T \label{eq:RoadPlane}

\mathbf{\overline{p}} &=& [p_x, p_y, f]^T\\
\mathbf{P} &=&  - \frac{\delta}{\left[\mathbf{\overline{p}}^T, 0\right] \cdot \boldsymbol{\rho}} \mathbf{\overline{p}}  \label{eq:3DCoords}

	&\mathbf{X}_i =
	\left[
	\begin{array}{cc}
		w_1 (m_1 - x_i) & w_1 (n_1 - y_i) \\
		w_2 (m_2 - x_i) & w_2 (n_2 - y_i) \\
		\vdots & \vdots \\
		w_k (m_k - x_i) & w_k (n_k - y_i) \\
	\end{array}
	\right]&

	\label{eq:svd}
	\mathbf{W}_i \mathbf{\Sigma}_i^{2} \mathbf{W}_i^{T}  &=& \mathrm{SVD}\left(\mathbf{X}_i^{T}\mathbf{X}_i\right), \\
    \mathrm{where}&& \nonumber \\
	\label{eq:eigenvectors}
	\mathbf{W}_i &=& \left[ \mathbf{a}_1, \mathbf{a}_2 \right] \\
	\label{eq:eigenvalues}
	\mathbf{\Sigma}_i &=&
	\left(
	\begin{array}{cc}
		\lambda_1 & 0 \\
		0 & \lambda_2 \\
	\end{array}
	\right).

&\lambda_{ij} = \cfrac{l_{t_i}}{\|\mathbf{F}_{ij}-\mathbf{R}_{ij}\|} &\label{eq:BBScaleComputationOne}

&\lambda^* = \arg \max_\lambda~p\left(\lambda\,|\,(\lambda_{ij}, m_{ij}) \right) &\label{eq:BBScaleAll}

&v = \underset{i=1\dots N-\tau}{\mathrm{median}} \left( \frac{\displaystyle\lambda^*_{reg} \| \mathbf{P}_{i+\tau} - \mathbf{P}_i \|}{\displaystyle t_{i+\tau} - t_i} \right)&\label{eq:SpeedComputation}

		\mathbf{v^*} = \arg \min_\mathbf{v} \left( \sum_{(\mathbf{p}_1, \mathbf{p}_2, d) \in \mathcal{D}_2} \left| \lambda \| \mathbf{P}_1 - \mathbf{P}_2 \| - d\right| \right) \label{eq:ManualCalib}, 
	
		\lambda = \mathbb{E}\left[ \frac{d_i}{\| \mathbf{P}_{i,1} - \mathbf{P}_{i,2} \|} \right] \label{eq:ManualScale}
	
	\lambda =  \mathbb{E} \left[ \frac{\gt{v}_i}{v_i} \right] \label{eq:SpeedScale}
	-\jot]			    \multicolumn{4}{@{}c@{}}{\makebox[\tabwidth]{\dashrule[lightgray]}} \-\jot]			    \multicolumn{4}{@{}c@{}}{\makebox[\tabwidth]{\dashrule[lightgray]}} \-\jot]			    \multicolumn{4}{@{}c@{}}{\makebox[\tabwidth]{\dashrule[lightgray]}} \-\jot]			    \multicolumn{4}{@{}c@{}}{\makebox[\tabwidth]{\dashrule[lightgray]}} \-\jot]			    \multicolumn{4}{@{}c@{}}{\makebox[\tabwidth]{\dashrule[lightgray]}} \-\jot]			    \multicolumn{4}{@{}c@{}}{\makebox[\tabwidth]{\dashrule[lightgray]}} \-\jot]			    \multicolumn{4}{@{}c@{}}{\makebox[\tabwidth]{\dashrule[lightgray]}} \-\jot]			    
		\multicolumn{4}{@{}c@{}}{\makebox[\tabwidth]{\dashrule[lightgray]}} \-\jot]			    \multicolumn{4}{@{}c@{}}{\makebox[\tabwidth]{\dashrule[lightgray]}} \-\jot]			    \multicolumn{4}{@{}c@{}}{\makebox[\tabwidth]{\dashrule[lightgray]}} \-\jot]			    \multicolumn{4}{@{}c@{}}{\makebox[\tabwidth]{\dashrule[lightgray]}} \-\jot]			    \multicolumn{4}{@{}c@{}}{\makebox[\tabwidth]{\dashrule[lightgray]}} \-\jot]			    \multicolumn{4}{@{}c@{}}{\makebox[\tabwidth]{\dashrule[lightgray]}} \-\jot]			    \multicolumn{4}{@{}c@{}}{\makebox[\tabwidth]{\dashrule[lightgray]}} \-\jot]			    \multicolumn{4}{@{}c@{}}{\makebox[\tabwidth]{\dashrule[lightgray]}} \-\jot]			    \multicolumn{4}{@{}c@{}}{\makebox[\tabwidth]{\dashrule[lightgray]}} \-\jot]
		
		\multirow{2}{*}{Combi + Sedan + reg} & \abso{1.10} & \abso{0.97} & \abso{3.05}\\
		& \proc{1.39} & \proc{1.22} & \proc{4.13}\\	
		
		\bottomrule
	\end{tabular}
\end{table}

The second segment of the table shows the performance of the system with scale correction regression to overcome the inaccuracies of the 3D models. The results show that for model Combi, the error significantly decreases. However, for the Sedan model, the results stay more or less the same. This paradox is caused by the smaller number of training data for Sedan version as for some training videos, no Sedan vehicle was detected.
The results also show that if we use both models, the performance drop is not that significant (1.10\,km/h to 1.38\,km/h) and therefore, it is possible to use the scale inference without the scale correction regression.


\subsection{Vehicle Detection and Tracking Evaluation} \label{sec:TrackingEval}

\begin{table}[t]
	\centering
	\caption{Evaluation of differences between vehicle detection and tracking proposed by  \cite{Dubska2014} and our detection and tracking method. FPPM denotes the number of False Positives Per Minute, recall was computed as mean recall across all videos and speed error denotes mean speed measurement error.}  \label{tab:CountingResults}
	\vspace{2mm}
	\setlength{\tabwidth}{0.90\linewidth}
	\small
	\begin{tabular}{l r r r}
		\toprule
		\textbf{method} & \textbf{FPPM} & \textbf{recall} & \textbf{speed error [km/h]}\\
		\midrule
		\cite{Dubska2014} & 9.77 & 0.885 & 1.46\\
		ours & 1.91 & 0.863 & 1.21\\	
		\bottomrule
	\end{tabular}
\end{table}


Since we use a different vehicle detection and tracking method from \cite{Dubska2014}, we also evaluate this part of the solution. We compare the methods on all videos of BrnoCompSpeed (including extra session0) with exactly the same calibration (\ManualCalib\ + \ManualScale) to isolate the influence of vehicle detection and tracking. 

We report the number of False Positives Per Minute and mean recall in vehicle counting. The results can be found in Table~\ref{tab:CountingResults}, and as the table shows, our method considerably reduces the number of false positives with essentially the same recall.

A tracked vehicle is matched to the ground truth if it passes through the correct lane and the time difference of pass through the measurement line (yellow line in Figure~\ref{fig:RoadMarkings} which is closest to the camera) compared to the ground truth is less than . This threshold is used by \cite{BrnoCompSpeed} to correctly match the vehicles, as a higher threshold could lead to mismatches between the detected track and ground truth.

As we use the same calibration, we can also compare directly the speed measurement error which is influenced (with the same calibration) only by the tracking. As the table shows, our tracking method yields slightly reduced speed measurement error for the same scale and camera calibration.



For the tracking and speed measurement, we use the point at the front of the vehicle on the road plane (using the 3D bounding box), which is geometrically correct, as the point is on the road plane. We evaluated how the choice of the tracking point influences the measurement error, comparing to a naive solution which takes the center of the bottom edge of the 2D bounding box for the tracking, and we found out that the difference to the correct solution was negligible.


\subsection{Camera Calibration on Real Surveillance Cameras}
\label{sec:RealCameras}

The automatic calibration from vehicle movement can be justifiably suspected of requiring idealized conditions and to be sensitive to bad lighting, etc.
In order to verify the usability of our camera calibration method in real-world conditions, we obtained data from surveillance cameras in production use at 9 different locations. The videos were captured both at day and night conditions. The data are of rather poor quality ( or ) with 6 frames per second and a mean length of 40s.
As the ground truth calibration is not available for the data, we report only qualitative results in the form of equilateral grid projected on the road plane.
Despite the challenging character of the sequences (poor video quality and lighting conditions), we were able to correctly detect the vanishing points, as can be seen in Figure~\ref{fig:RealWorldCalib} on a few examples, and thus find the camera parameters and its orientation, which is important in many real-world surveillance applications (e.g estimation of vehicle viewpoints or image rectification).

\begin{figure*}[t]
	\centering
	\makebox[\textwidth][c]{\includegraphics[width=0.45\linewidth]{images_camea_data_301.pdf}\hspace{2pt}\includegraphics[width=0.45\linewidth]{images_camea_data_287.pdf}\hspace{2pt}\includegraphics[width=0.45\linewidth]{images_camea_data_12.pdf}}
	\makebox[\textwidth][c]{\includegraphics[width=0.45\linewidth]{images_camea_data_179.pdf}\hspace{2pt}\includegraphics[width=0.45\linewidth]{images_camea_data_166.pdf}\hspace{2pt}\includegraphics[width=0.45\linewidth]{images_camea_data_210.pdf}}
	\caption{Example of camera calibration (two vanishing points) for real world surveillance cameras. The first row shows different locations, while the second one show the same locations at night, dawn, and during daylight. The yellow line denotes the detected horizon (if present inside the frames) and red-green grid is formed by lines going to the first vanishing point (red) and to the second one (green). In an ideal case the grid is perpendicular in the real world and the lines are parallel to the features which define the vanishing points on the ground (e.g. line marking). It should also be noted that the method is able to work even on an intersection (top center). }
	\label{fig:RealWorldCalib}
\end{figure*}

\section{Conclusions}

We propose a fully automatic method for traffic surveillance camera calibration. It does not have any constraints on camera placement and does not require any manual input whatsoever. The results show that our system decreases the mean speed measurement error by  (7.98\,km/h to 1.10\,km/h) compared to the previous automatic state-of-the-art method and by  (1.35\,km/h to 1.10\,km/h) compared to the manual calibration method. This improvement is important, as in the previous approaches, automation always compromised accuracy, forcing the system developer to trade off between them.  Our work shows that fully automatic calibration methods may produce better results than manual calibration. This result can be important beyond the field of traffic surveillance, since different forms of manual camera calibration are often considered the ``ground truth'', but our work shows that automatic calibration from statistics of repeated inaccurate measurements can be more precise, despite requiring no user input.
Our method removes the necessity of per-camera setting or calibration, but it still requires some human annotations per coarse geographic region (e.g. European Union or the USA) and per time period when the car models get vastly replaced (e.g. per decade). 

In the experiments, we also showed that our method is able to calibrate real world traffic surveillance cameras and our proposed method for vehicle detection and tracking significantly reduces the number of false positives compared to the previous method.
In future work, we would like to simplify the system and remove the necessity to render the vehicles by approximation of the bounding box size with a function parametrized by viewpoint and image location. 




\section*{Acknowledgments}
This work was supported by The Ministry of Education, Youth and Sports of the Czech Republic from the National Programme of Sustainability (NPU II); project IT4Innovations excellence in science -- LQ1602.

We would also like to thank to company CAMEA for providing us data from industrial surveillance cameras.


\section*{References}
\bibliography{2016-CVIU-RobustTraffic-bibliography}


\end{document}
