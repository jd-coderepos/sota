
\documentclass{article} \usepackage{iclr2021_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{tikz}
\usepackage{pgfplots}
\usepackage{chngpage}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage{multicol}
 \usepackage{float}
 \usepackage{multirow}
 \usepackage{makecell}
 \usepackage{caption}
 \usepackage{stmaryrd,scalerel}
  \usepackage{enumitem}
\usepackage{dsfont}


\usepackage{todonotes}

\makeatletter
\newcommand\footnoteref[1]{\protected@xdef\@thefnmark{\ref{#1}}\@footnotemark}
\makeatother
\newcommand{\eqname}[1]{\tag*{#1}}


\usepackage{amsmath}

\usepackage{amsthm}
\newtheorem{definition}{Definition}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{lemma}{Lemma}[section]



\newcommand{\modelname}{GSN}





\title{Improving Graph Neural Network Expressivity via Subgraph Isomorphism Counting}



\author{Giorgos Bouritsas, Fabrizio Frasca, Stefanos Zafeiriou, Michael M. Bronstein
  \\
  Imperial College London, United Kingdom, Twitter, United Kingdom\\
 \texttt{\{g.bouritsas, s.zafeiriou\}@imperial.ac.uk, } \\
 \texttt{\{ffrasca, mbronstein\}@twitter.com} \\
}




\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}


\maketitle

\begin{abstract}
While Graph Neural Networks (GNNs) have achieved remarkable results in a variety of applications, recent studies exposed important shortcomings in their ability to capture the structure of the underlying graph. It has been shown that the expressive power of standard GNNs is bounded by the Weisfeiler-Leman (WL) graph isomorphism test, from which they inherit proven limitations such as the inability to detect and count graph substructures. On the other hand, there is significant empirical evidence, e.g. in network science and bioinformatics, that substructures are often informative for downstream tasks, suggesting that it is desirable to design GNNs capable of leveraging this important source of information. To this end, we propose a novel topologically-aware message passing scheme based on substructure encoding. We show that our architecture allows incorporating domain-specific inductive biases and that it is strictly more expressive than the WL test. Importantly, in contrast to recent works on the expressivity of GNNs, we do not attempt to adhere to the WL hierarchy; this allows us to retain multiple attractive properties of standard GNNs such as locality and linear network complexity, while being able to disambiguate even hard instances of graph isomorphism. We extensively evaluate our method on graph classification and regression tasks and show state-of-the-art results on multiple datasets including molecular graphs and social networks\footnote{The implementation is publicly available at \url{https://github.com/gbouritsas/graph-substructure-networks}}.  

\end{abstract}

\section{Introduction}

The field of graph representation learning has undergone a rapid growth in the past few years. 
In particular, Graph Neural Networks (GNNs), a family of neural architectures designed for irregularly structured data, have been successfully applied to problems 
ranging from social networks and recommender systems \citep{ying2018graph} to bioinformatics \citep{fout2017protein, gainza2020deciphering}, chemistry \citep{duvenaud2015convolutional, gilmer2017neural, sanchez2019machine} and physics \citep{DBLP:conf/icml/KipfFWWZ18, battaglia2016interaction}, to name a few. 
Most GNN architectures are based on message passing \citep{gilmer2017neural}, where the representation of each node is iteratively updated by aggregating information from its neighbours. 

A crucial difference from traditional neural networks operating on grid-structured data is the absence of canonical ordering of the nodes in a graph. To address this, the aggregation function is constructed to be invariant to neighbourhood permutations and, as a consequence, to graph isomorphism. This kind of symmetry is not always desirable and thus different inductive biases that disambiguate the neighbours have been proposed. For instance, in geometric graphs, such as 3D molecular graphs and meshes, directional biases are usually employed in order to model the positional information of the nodes  \citep{masci2015geodesic,monti2017geometric,bouritsas2019neural,klicpera_dimenet_2020, de2020gauge}; for proteins, ordering information is used to disambiguate amino-acids at different positions in the sequence \citep{ingraham2019generative}; in multi-relational knowledge graphs, a different aggregation is performed for each relation type \citep{schlichtkrull2018modeling}. 

The structure of the graph itself does not usually explicitly take part in the aggregation function. In fact, most models rely on multiple message passing steps as a means for each node to discover the global structure of the graph. However, since message-passing GNNs are at most as powerful as the Weisfeiler Leman test (WL) \citep{xu2018how, morris2019weisfeiler}, they are limited in their abilities to adequately exploit the graph structure, e.g. by  counting  substructures \citep{DBLP:conf/fct/ArvindFKV19, chen2020can}. This uncovers a crucial limitation of GNNs, as substructures have been widely recognised as important in the study of complex networks. For example, in molecular chemistry, functional groups and rings are related to a plethora of chemical properties, while cliques are related to protein complexes in Protein-Protein Interaction networks and community structure in social networks, respectively \citep{Granovetter82thestrength, girvan2002community}. 

Motivated by this observation, in this work we propose \textit{Graph Substructure Network (GSN)}, a new symmetry breaking mechanism for GNNs based on introducing structural biases in the aggregation function. In particular, each message is transformed differently depending on the topological relationship between the endpoint nodes. This relationship is expressed by counting the appearance of certain substructures, the choice of which allows us to provide the model with different inductive biases, based on the graph distribution at hand. The substructures are encoded as structural identifiers that are assigned to either the nodes or edges of the graph and can thus disambiguate the neighbouring nodes that take part in the aggregation. 

We characterise the expressivity of substructure encoding in GNNs, showing that GSNs are strictly more expressive than traditional GNNs for the vast majority of substructures while retaining the locality of message passing, as opposed to higher-order methods \citep{DBLP:conf/iclr/MaronBSL19,pmlr-v97-maron19a, maron2019provably, morris2019weisfeiler} that follow the WL hierarchy (see Section \ref{sec:prelims}). 
In the limit, our model can yield a unique representation for every isomorphism class and is thus universal. 
We provide an extensive experimental evaluation on hard instances of graph isomorphism testing (strongly regular graphs), as well as on real-world networks from the social and biological domains, including the recently introduced large-scale benchmarks \citep{dwivedi2020benchmarking, DBLP:ogb}. We observe that when choosing the structural inductive biases based on domain-specific knowledge, \modelname\ achieves state-of-the-art results.



\section{Preliminaries}\label{sec:prelims}

Let  be a graph with vertex set  and edge set , directed or undirected. A subgraph  of  is any graph with . When  includes all the edges of  with endpoints in , i.e.  ,
the subgraph is said to be \textit{induced}. 

\noindent\textbf{Isomorphisms}
Two graphs  are {\em isomorphic} (denoted ), if there exists an adjacency-preserving bijective mapping ({\em isomorphism}) , i.e.  iff . Given some small graph , the \textit{subgraph isomorphism} problem amounts to finding a subgraph  of  such that . An \textit{automorphism} of  is an isomorphism that maps  onto itself. The set of all the unique automorphisms form the \textit{automorphism group} of the graph, denoted as , which contains all the possible symmetries of the graph. The automorphism group yields a partition of the vertices into disjoint subsets of  called \textit{orbits}. Intuitively, this concept allows us to group the vertices based on their \textit{structural roles}, e.g. the end vertices of a path, or all the vertices of a cycle (see Figure \ref{fig:method_illustration}). Formally, the 
orbit of a vertex  is the set of vertices to which it can be mapped via an automorphism: , and the set of all orbits  is usually called the \textit{quotient} of the automorphism when it acts on the graph . We are interested in the unique elements of this set that we will denote as , where  is the cardinality of the quotient.

Analogously, we define edge structural roles via \textit{edge automorphisms}, i.e. bijective mappings from the edge set onto itself, that preserve edge adjacency (two edges are adjacent if they share a common endpoint). In particular, every vertex automorphism  induces an edge automorphism by mapping each edge  to . \footnote{Note that the edge automorphism group is larger than that of induced automorphisms, but strictly larger only for 3 trivial cases \citep{Whitney1932}. However, induced automorphisms provide a more natural way to express edge structural roles.} In the same way as before, we construct the edge automorphism group, from which we deduce the partition of the edge set in {\em edge orbits} .


\noindent\textbf{Weisfeiler-Leman tests:} The {\em Weisfeiler-Leman graph-isomorphism test} \citep{weisfeilerreduction}, also known as naive vertex refinement, \textit{1-WL}, or just \textit{WL}), is a fast heuristic to decide if two graphs are isomorphic or not. The WL test proceeds as follows: every vertex  is initially assigned a colour  that is later iteratively refined by aggregating neighbouring information: , where  denotes a multiset (a set that allows element repetitions) and  is the neighbourhood of . The WL algorithm terminates when the colours stop changing, and outputs a histogram of colours. Two graphs with different histograms are non-isomorphic; if the histograms are identical, the graphs are possibly, but not necessarily, isomorphic. Note that the neighbour aggregation in the WL test is a form of message passing, and GNNs are the learnable analogue. 

A series of works on improving GNN expressivity mimic the higher-order generalisations of WL, known as -WL and -Folklore WL (WL hierarchy) and operate on -tuples of nodes (see Appendix \ref{k-wl}). The -FWL is strictly stronger than -FWL, -FWL is as strong as -WL and 2-FWL is strictly stronger than the simple 1-WL test. 

\section{Graph Substructure Networks}

Graphs consist of nodes (or edges) with repeated structural roles. Thus, it is natural for a neural network to treat them in a similar manner, akin to weight sharing between local patches in CNNs for images \citep{lecun1989backpropagation} or positional encodings in language models for sequential data \citep{DBLP:conf/nips/SukhbaatarSWF15, DBLP:conf/icml/GehringAGYD17, vaswani2017attention}. 
Nevertheless, GNNs are usually unaware of the nodes' different structural roles, since all nodes are treated equally when performing local operations. Despite the initial intuition that the neural network would be able to discover these roles by constructing deeper architectures, it has been shown that GNNs are ill-suited for this purpose and are blind to the existence of structural properties, e.g. triangles or larger cycles \citep{chen2020can, DBLP:conf/fct/ArvindFKV19}. 

To this end, we propose to explicitly encode structural roles as part of message passing, in order to capture richer topological properties. Our method draws inspiration from \cite{Loukas2020What}, where it was shown that GNNs become universal when the nodes in the graph are uniquely identified, i.e when they are equipped with different features. However, it is not clear how to choose these identifiers in a way that can allow the neural network to generalise. Structural roles, when treated as identifiers, although not necessarily unique, are more amenable to generalisation due to their repetition across different graphs. Thus, they can constitute a trade-off between uniqueness and generalisation.


\begin{wrapfigure}{R}{0.44\textwidth}\vspace{-3mm}
\centering
      \includegraphics[width=0.44\textwidth]{figures/matching_orbits_thin.png}
        \captionsetup[figure]{skip=\abovecaptionskip}
      \captionof{figure}{\emph{Node} (left) and \emph{edge} (right) induced subgraph counting for a 3-cycle and a 3-path. Counts are reported for the blue node on the left and for the blue edge on the right. Different colors depict orbits.}
      \label{fig:method_illustration}
\end{wrapfigure}

\noindent\textbf{Structural features:} Structural roles are encoded into features by counting the appearance of certain substructures. The larger the substructure collection, the more fine-grained the partition of nodes into roles will be.
Let  be a set of small (connected) graphs, for example cycles of fixed length or cliques.  For each graph , we first find its isomorphic subgraphs in  denoted as . For each node  we infer its role w.r.t.  by obtaining the orbit of its mapping  in , . By counting all the possible appearances of different orbits in , we obtain the {\em vertex structural feature}  of , defined as follows:



Note that there exist  different functions  that can map a subgraph  to , but any of those can be used to determine the orbit mapping of each node .
By combining the counts from different substructures in  and different orbits, we obtain the feature vector 
 of dimension . 

Similarly, we can define {\em edge structural features}  by counting occurrences of edge automorphism orbits:


and the combined edge features . An example of vertex and edge structural features is illustrated in Figure \ref{fig:method_illustration}.


\paragraph{Structure-aware message passing:} The key building block of our architecture is the graph substructure layer, defined in a general manner as a Message Passing Neural Network (MPNN) \citep{gilmer2017neural}, where now the messages from the neighbouring nodes also contain the structural information. In particular, each node  updates its state  by combining its previous state with the aggregated messages: , where the  function is a multilayer perceptron (MLP) and the message aggregation is a summation of features transformed by an MLP  as follows:

where the two variants, named \textit{GSN-v} and \textit{GSN-e}, correspond to vertex- or edge-counts, respectively, and  denotes edge features. The two variants are analogous to absolute and relative positional encodings in language models \citep{DBLP:conf/naacl/ShawUV18, DBLP:conf/acl/DaiYYCLS19}.  

It is important to note here that contrary to identifier-based GNNs \citep{Loukas2020What, sato2020random, clip_ijcai20} that obtain universality at the expense of permutation equivariance (since the identifiers are arbitrarily chosen with the sole requirement of being unique), GSNs retain the attractive inductive bias of the permutation equivariant property. This stems from the fact that the process generating our structural identifiers (i.e. subgraph isomorphism) is permutation equivariant itself (proof provided in the Appendix \ref{proof:perm_equiv}). 




\begin{figure}[t]\vspace{-5mm}
\centering
\begin{subfigure}{0.4\textwidth}
  \centering
  \includegraphics[width=\textwidth]{figures/mols.png}
  \label{fig:molecules}
\end{subfigure}
\hspace{1cm}
\begin{subfigure}{0.4\textwidth}
  \centering
     \includegraphics[width=\textwidth]{figures/SR_graphs_example.png}
\end{subfigure}
    \captionsetup[figure]{skip=\abovecaptionskip}
  \caption{(Left) \emph{Decalin} and \emph{Bicyclopentyl}: Non-isomorphic molecular graphs than can be distinguished by \modelname, but not the by the WL test \citep{sato2020survey} (nodes represent carbon atoms and edges chemical bonds).
  (Right) \emph{Rook's 4x4 graph} and the \emph{Shrikhande graph}: the smallest pair of strongly regular non-isomorphic graphs with the same parameters SR(16,6,2,2). \modelname\ can distinguish them with 4-clique counts, while 2-FWL fails.\vspace{-3mm}}
 \label{fig:graph_examples}
\end{figure}





\paragraph{How powerful are \modelname s?}


We now turn to the expressive power of \modelname s in comparison to MPNNs and the WL tests, a key tool for the theoretical analysis of the expressivity of graph neural networks so far. Since GSN is a generalisation of MPNNs, it is easy to see that it is at least as powerful. Importantly, \modelname s have the capacity to learn functions that traditional MPNNs cannot learn. The following observation derives directly from the analysis of the counting abilities of the 1-WL test  \citep{DBLP:conf/fct/ArvindFKV19} and its extension to MPNNs \citep{chen2020can} (for proofs, see Appendix \ref{proof}). 

\begin{proposition}\label{as_expressive}
\modelname\ is strictly more powerful than MPNN and the 1-WL test when one of the following holds:
\begin{itemize}
    \item  is any graph except for star graphs of any size, and structural features are inferred by subgraph matching, i.e. we count all subgraphs  for which it holds that . Or,
    \item  is any graph except for single edges and single nodes,     and structural features are inferred by \textbf{induced} subgraph matching, i.e. we count all subgraphs  for which it holds that .
\end{itemize}
\end{proposition}

\noindent\textbf{Universality:} A natural question that emerges is under which conditions a GSN can solve graph isomorphism. This would entail that GSN is a universal approximator of functions defined on graphs \citep{clip_ijcai20, chen2019equivalence}. To address this, we can examine whether there exists a specific substructure collection that can completely characterise each graph. As of today, we are not aware of any results in graph theory that can guarantee the reconstruction of a graph from a smaller collection of its subgraphs. However, the \textit{Reconstruction Conjecture} \citep{kelly1957congruence, ulam1960collection}, states that a graph with size  can be reconstructed from its vertex-deleted subgraphs. Consequently, (proof in the Appendix \ref{proof_universality}):
\begin{corollary}\label{recon_conj}
If the Reconstruction Conjecture holds and the substructure collection  contains all graphs of size , then GSN can distinguish all non-isomorphic graphs of size  and is therefore universal. 
\end{corollary}

\noindent\textbf{GSN-v vs GSN-e}: Finally, we examine the expressive power of the two proposed variants. A crucial observation that we make is that for each graph  in the collection, the vertex structural identifiers can be reconstructed by the corresponding edge identifiers. Thus, we can show that for every GSN-v there exists a GSN-e that can simulate the behaviour of the former (proof provided in the Appendix \ref{proof_GSNv_GSNe}).

\begin{proposition}\label{GSN-v vs GSN-e}
For a given subgraph collection , let  the set of functions that can be expressed by a GSN-v with arbitrary depth and with, and  the set of functions that can be expressed by a GSN-e with the same properties. Then, it holds that , or in other words GSN-e is at least as expressive as GSN-v.
\end{proposition}


\paragraph {How to choose the substructures?}
The Reconstruction Conjecture has only been proven for  \citep{mckay1997small} and still remains open for larger graphs, while to the best of our knowledge, there is no similar hypothesis for smaller values of . However, in practice we observe that small substructures of size , i.e. independent of the size of the graph, are sufficient both for resolving hard instances of graph isomorphism as well as for tasks related to real-world networks. In particular, although our method does not attempt to align with the WL hierarchy, an interesting observation that we make is that small substructures have the capacity to distinguish \textit{strongly regular} graphs, a family of graphs on which 2-FWL provably fails (proven in Appendix~\ref{sr_graphs_wl}). This is illustrated in Figure \ref{fig:graph_examples} (right), where counting 4-cliques is sufficient to disambiguate this exemplary pair, and in Section \ref{sec:experiments}, where a large scale experimental study is conducted. 

In real-world scenarios, one can avoid the computational burden of counting many large subgraphs, by choosing only the most discriminative ones, i.e. the ones that can achieve the maximum possible node disambiguation, similarly to identifier-based approaches. In Figure \ref{fig:zinc-substructures}, we show that certain small substructures (with size independent of the size of the target graph),  such as paths and trees, significantly improve node disambiguation, compared to the initial node features (see also Table \ref{tab:uniqueness} in the Appendix), and as expected allow for better fitting of the training data, which validates our claim that GNN expressivity improves. 


Note that this approach solely takes into account expressivity and does not provide any guarantee about generalisation. This is also empirically validated in Figure \ref{fig:zinc-substructures}, where we observe that the test set performance does not necessarily improve when choosing substructures with strong node disambiguation. In practice, aiming at better generalisation, it is desirable to make use of substructures for which there is substantial prior knowledge of their importance in certain network distributions and have been observed to be intimately related to various properties. For example, small substructures (graphlets) have been extensively analysed in protein-protein interaction networks \citep{prvzulj2004modeling}, triangles and cliques characterise the structure of ego-nets and social networks in general \citep{Granovetter82thestrength}, simple cycles (rings) are central in molecular distributions, directed and temporal motifs have been shown to explain the working mechanisms of gene regulatory networks, biological neural networks, transportation networks and food webs \citep{milo2002network, DBLP:conf/wsdm/ParanjapeBL17, benson2016higher}. In Figure \ref{fig:generalisation} in the Appendix, we showcase the importance of these inductive biases: a cycle-based GSN predicting molecular properties achieves smaller generalisation gap compared to a traditional MPNN, while at the same time generalising better with less training data. Choosing the best substructure collection is still an open problem; various heuristics can be used (motif frequencies, feature selection strategies) or ideally by learning the substructure dictionary in an end-to-end manner. Answering this question is left for future work.


\noindent\textbf{Complexity:} 
The complexity of GSN comprises two parts: precomputation (substructure counting) and training/testing. The key appealing property is that training and inference are linear w.r.t the number of edges, , as opposed to higher-order methods \citep{maron2019provably, morris2019weisfeiler} with  training complexity and relational pooling \citep{DBLP:conf/icml/Murphy0R019} with  training complexity in absence of approximations. 

The worst-case complexity of subgraph isomorphism of fixed size  is , by examining all the possible -tuples in the graph. However, for specific types of subgraphs, such as paths and cycles, the problem can be solved even faster (see e.g. \cite{DBLP:journals/algorithmica/GiscardKW19}). Approximate counting algorithms are also widely used, especially for counting frequent network motifs \citep{DBLP:journals/bioinformatics/KashtanIMA04, DBLP:conf/wabi/Wernicke05, DBLP:journals/tcbb/Wernicke06, DBLP:journals/bioinformatics/WernickeR06}, and can provide a considerable speed-up. Furthermore, recent neural approaches \citep{ying2020frequent, ying2020neural} provide fast approximate counting. 

In our experiments, we performed exact counting using the common isomorphism algorithm VF2 \citep{DBLP:journals/pami/CordellaFSV04}. Although its worst case complexity is , it scales better in practice, for instance when the candidate subgraph is infrequently matched or when the graphs are sparse, and is also trivially parallelisable. In Figure \ref{fig:complexity}, we show a quantitative analysis of the empirical runtime of the counting algorithm against the worst case, for three different graph distributions: molecules, protein contact maps, social networks. It is easy to see that when the graphs are sparse (for the first two cases) and the number of matches is small, the algorithm is significantly faster than the worst case, while it scales better with the size of the graph . Even, in the case of social networks, where several examples are near-complete graphs, both the runtime and the growth w.r.t both  and  are  better than the worst case. Overall, the preprocessing computational burden in most of the cases remains negligible for relatively small and sparse graphs, as it is the case of molecules. 

\begin{figure}\vspace{-2mm}
     \centering
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/zinc_temp.png}\vspace{-4mm}
\label{fig:y equals x}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/proteins_temp.png}\vspace{-4mm}
\label{fig:three sin x}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/imdbm.png}\vspace{-4mm}
\label{fig:five over x}
     \end{subfigure}
        \caption{Empirical (solid) vs worst case (dashed) runtime for different graph distributions (in seconds, log scale). For each distribution we count the best performing (and frequent) substructures of increasing sizes .\vspace{-3mm}}
        \label{fig:complexity}
\end{figure}


 
 \section{Related Work}

\noindent\textbf{Expressive power of GNNs and the WL hierarchy}: The seminal results in the theoretical analysis of the expressivity of GNNs \citep{xu2018how} and k-GNNs \citep{morris2019weisfeiler} established that traditional message passing-based GNNs are at most as powerful as the 1-WL test. \citet{chen2019equivalence} showed that graph isomorphism is equivalent to universal invariant function approximation. Higher-order Invariant Graph Networks (IGNs) have been studied in a series of works \citep{DBLP:conf/iclr/MaronBSL19, pmlr-v97-maron19a, maron2019provably, chen2019equivalence, keriven2019universal, puny2020graph}, establishing connections with the WL hierarchy, similarly to \cite{morris2019weisfeiler, MorrisNeurips2020}. The main drawback of these methods is the training and inference time complexity and memory requirements of  as well as the super-exponential number of parameters for linear IGNs making them impractical. 

From a different perspective, \cite{sato2019approximation} and \cite{Loukas2020What} showed the connections between GNNs and distributed local algorithms \citep{angluin1980local, linial1992locality, DBLP:conf/stoc/NaorS93} and suggested more powerful alternatives based on either local orderings or unique global identifiers (in the form of random features in \cite{sato2020random}) that make GNNs universal. Similarly, \cite{clip_ijcai20} propose to use random colorings in order to uniquely identify the nodes. However, these methods lack a principled way to choose orderings/identifiers that can be shared across graphs (this would require a graph canonisation procedure).   \cite{DBLP:conf/icml/Murphy0R019} take into account all possible node permutations and can therefore be intractable. \cite{DBLP:Garg2020icml} also analysed the expressive power of MPNNs and other more powerful variants and provided generalisation bounds. Concurrently with our work, more expressive GNNs have been proposed 
using equivariant message passing with different kernels for each edge based on their isomorphism classes \citep{de2020natural}, message passing with matrices of order equal to the size of the graph instead of vectors \citep{vignac2020building}, or by enhancing node features with distance encodings \citep{li2020distance}.

Solely quantifying the  expressive power of GNNs in terms of their ability to distinguish non-isomorphic graphs does not provide the necessary granularity: even the 1-WL test can distinguish almost all (in the probabilistic sense) non-isomorphic graphs \citep{babai1980random}. As a result, there have been several efforts to analyse the power of -WL tests in comparison to other graph invariants \citep{furer2010power, furer2017combinatorial, DBLP:conf/fct/ArvindFKV19, DBLP:conf/icalp/DellGR18}, while recently \cite{chen2020can} approached GNN expressivity by studying their ability to count substructures.

\noindent\textbf{Substructures in Complex Networks: }The idea of analysing complex networks based on small-scale topological characteristics dates back to the 1970's and the notion of triad census for directed graphs \citep{holland1976local}. The seminal paper of \cite{milo2002network} coined the term \textit{network motifs} as over-represented subgraph patterns that were shown to characterise certain functional properties of complex networks in systems biology.  The closely related concept of  \textit{graphlets} \citep{prvzulj2004modeling, prvzulj2007biological, milenkovic2008uncovering, sarajlic2016graphlet}, different from motifs in being induced subgraphs, has been used to analyse the distribution of real-world networks and as a topological signature for network similarity. Our work is similar in spirit with the {\em graphlet degree vector} (GDV) \citep{prvzulj2007biological}, a node-wise descriptor based on graphlet counting. 

Substructures have been also used in the context of ML. In particular, subgraph patterns have been used to define Graph Kernels (GKs) \citep{horvath2004cyclic, shervashidze2009efficient, costa2010fast, DBLP:conf/icml/KriegeM12, nt2020graph}, with the most prominent being the graphlet kernel \citep{shervashidze2009efficient}. Motif-based node embeddings \citep{motif2vec, rossi-HONE-arxiv} and diffusion operators  \citep{motifnet, sankar2019meta, lee19-motif-attention}  that employ adjacency matrices weighted according to motif occurrences, have recently been proposed for graph representation learning. Our formulation provides a unifying framework for these methods and it is the first to analyse their expressive power. Finally, GNNs that operate in larger induced neighbourhoods \citep{li2019hierarchy, kim2019near} or higher-order paths \citep{flam2020neural} have prohibitive complexity since the size of these neighbourhoods typically grows exponentially. 



\section{Experimental Evaluation}\label{sec:experiments}


In the following section we evaluate \modelname\ in comparison to the state-of-the-art in a variety of datasets from different domains.  We are interested in practical scenarios where the collection of subgraphs, as well as their size, are kept small. Depending on the dataset domain we experimented with typical substructure families (\textit{cycles, paths} and \textit{cliques}) and maximum substructure size  (note that for each setting, our substructure collection consists of all the substructures of the family with size ). We also experimented with both graphlets and motifs and observed similar performance in most cases. 
Appendix~\ref{experiments_supp} provides details and shows additional experiments.

\noindent\textbf{Synthetic Graph Isomorphism test:}\label{synthetic}
We tested the ability of GSNs to decide if two graphs are non-isomorphic on a collection of Strongly Regular graphs of size up to 35 nodes,  attempting to disambiguate pairs with the same number of nodes (for different sizes the problem becomes trivial). As we are only interested in the bias of the architecture itself, we use GSN with random weights to compute graph representations. Two graphs are deemed isomorphic if the Euclidean distance of their representations is smaller than a predefined threshold . Figure \ref{fig:sr_plot} shows the failure percentage of our isomorphism test when using different graphlet substructures (\textit{cycles, paths}, and \textit{cliques}) of varying size . Interestingly, the number of failure cases of GSN decreases rapidly as we increase ; cycles and paths of maximum length  are enough to tell apart all the graphs in the dataset. Note that the performance of cliques saturates, possibly because the largest clique in our dataset has 5 nodes. Observe also the discrepancy between \modelname-v and \modelname-e. In particular, vertex-wise counts do not manage to distinguish all graphs, although missing only a few instances, which is in accordance with Proposition \ref{GSN-v vs GSN-e}. Finally, 1-WL \citep{xu2018how} and 2-FWL \citep{maron2019provably} equivalent models demonstrate 100\% failure, as expected from theory.



\begin{figure}[t]
    \begin{minipage}[t]{0.48\linewidth}\vspace{-3mm}
        \centering
          \includegraphics[width=\linewidth]{figures/uniqueness.pdf}
           \captionsetup[figure]{skip=\abovecaptionskip}
        \captionof{figure}{Train (dashed) and test (solid) MAEs for Path-, Tree- and Cycle-GSN\textit{-EF} as a function of the maximum substructure size . Vertical bars indicate standard deviation; horizontal bars depict disambiguation/uniqueness scores .}
        \label{fig:zinc-substructures}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.48\linewidth}\vspace{-3mm}
        \centering
         \includegraphics[width=\linewidth]{figures/bar.png}
         \captionsetup[figure]{skip=\abovecaptionskip}
        \captionof{figure}{SR graphs isomorphism test (log scale, smaller values are better). Different colours indicate different substructure sizes.}\vspace{-3mm}
        \label{fig:sr_plot} 
    \end{minipage}\vspace{-4mm}
\end{figure}

\noindent\textbf{TUD Graph Classification Benchmarks:}\label{TUD}
We evaluate GSN on datasets from the classical TUD benchmarks. We use seven datasets from the  domains of bioinformatics and computational social science and compare against various GNNs and Graph Kernels. Our main GNN baselines are GIN \cite{xu2018how} and PPGNs \cite{maron2019provably}. We follow the same evaluation protocol of \cite{xu2018how}, performing 10-fold cross-validation and then reporting the performance at the epoch with the best average accuracy across the 10 folds. Table \ref{tab:tud_datasets} lists all the methods evaluated with the split of \cite{zhang2018end}. We select our model by tuning architecture and optimisation hyperparameters and substructure related parameters, that is: (i) , (ii) motifs against graphlets. Following domain evidence we choose the following substructure families: \textit{cycles} for molecules, \textit{cliques} for social networks. Best performing substructures both for GSN-e and GSN-v are reported. As can be seen, our model obtains state-of-the-art performance in most of the datasets, with a considerable margin against the main GNN baselines in some cases. 
    
\begin{table}[h]
    \centering
\caption{Graph classification accuracy on TUD Dataset.  {\bf \bf \color{red} First}, {\bf \bf \color{violet} Second}, {\bf Third} best methods are highlighted. For GSN, the best performing structure is shown. 
Graph Kernel methods.  }
\label{tab:tud_datasets}
    \resizebox{\columnwidth}{!}{\begin{tabular}{l|cccc ccc}
Dataset & MUTAG & PTC & Proteins & NCI1 & Collab & IMDB-B & IMDB-M \\
\hline        
RWK* \citep{DBLP:conf/colt/GartnerFW03} & 
         79.22.1 & 
         55.90.3 & 
         59.60.1 & 
         3 days & N/A & N/A &
         N/A\\
        
        GK* (k=3) \citep{shervashidze2009efficient} & 81.41.7& 
        55.70.5 &
        71.40.31 & 
        62.50.3 & 
        N/A & 
        N/A &
        N/A\\

         
        PK* \citep{DBLP:journals/ml/NeumannGBK16} & 
         76.02.7& 
         59.52.4 &
         73.70.7 & 
         82.50.5 & 
         N/A & 
         N/A & 
         N/A\\
         
         
          WL kernel* \cite{shervashidze2011weisfeiler} & \textbf{90.45.7} & 59.94.3 & 
         75.03.1 & \textcolor{red}{\textbf{86.0}\textbf{1.8}} & 
         78.91.9 &
         73.83.9 &
         50.93.8 \\
         
        GNTK* \citep{DBLP:conf/nips/DuHSPWX19} & 
        90.08.5 & \textbf{\textcolor{violet}{67.96.9}} & 75.64.2 & \textbf{\textcolor{violet}{84.21.5}} & \textbf{\textcolor{violet}{83.61.0}} & \textbf{\textcolor{violet}{76.93.6}} & \textbf{\textcolor{violet}{52.84.6}}\\ 
         
        DCNN \citep{DBLP:conf/nips/AtwoodT16}& 
          N/A&  N/A &
          61.31.6
          & 56.61.0 &
          52.10.7 &
          49.11.4 &
          33.51.4\\


         DGCNN \citep{zhang2018end} & 85.81.8 & 
        58.62.5 & 
        75.50.9 & 
        74.40.5 & 
        73.80.5 &
        70.00.9 & 
        47.80.9\\
        

        
        IGN \citep{DBLP:conf/iclr/MaronBSL19} & 83.913.0 &
        58.56.9 &
        \textbf{\textcolor{violet}{76.65.5}} &
        74.32.7 & 
        78.32.5 & 
        72.05.5 & 
        48.73.4\\
        
        GIN \citep{xu2018how} & 
        89.45.6 & 
        64.67.0	& 
        \textbf{76.22.8} &
        82.71.7 &
        80.21.9 & 
        75.15.1 &
        52.32.8\\
        

        
        PPGNs \citep{maron2019provably} &
        \textbf{\textcolor{violet}{90.68.7}} &
        66.26.6 &
        \textbf{\textcolor{red}{77.24.7}} & 
        83.21.1 & 
        81.41.4 &
        73.05.8 & 
        50.53.6\\

        
        Natural GN \citep{de2020natural} &
        89.41.60 &
        66.81.79 &
        71.71.04 &
        82.41.35 &
        N/A &
        73.52.01 &
        51.31.50 \\
        
        
            
        \hline
        {\bf GSN-e} & \textbf{\textcolor{violet}{90.67.5}}  &
        \textbf{\textcolor{red}{68.27.2}}& 
        \textbf{\textcolor{violet}{76.65.0 }} & 
        \textbf{83.5 2.3}&
        \textbf{\textcolor{red}{85.51.2}} &
        \textbf{\textcolor{red}{77.83.3}} & 
        \textbf{\textcolor{red}{54.33.3}}\\
         &  6 (cycles) 
         & 6 (cycles)  
         & 4 (cliques)
         & 15 (cycles) 
         & 3 (triangles)
         & 5 (cliques)  
         & 5 (cliques)   \\
        \hline
        {\bf GSN-v}  &
        \textbf{\textcolor{red}{92.27.5}} & 
        \textbf{67.45.7} & 
        74.595.0 &
        \textbf{ 83.52.0} &
        \textbf{82.71.5}  &
        \textbf{{76.82.0}}& 
        \textbf{52.63.6}\\
            &  12 (cycles) 
            &  10 (cycles) 
            &  4 (cliques)
            & 3 (triangles) 
            & 3 (triangles)
            &   4 (cliques) 
            &   3 (triangles) \\
\end{tabular}
    }
\end{table}


\noindent\textbf{ZINC Molecular graphs:} We evaluate GSN on the task of regressing the penalized water-octanol
partition coefficient - logP - (see \cite{gomez2018automatic, DBLP:conf/icml/KusnerPH17, DBLP:conf/icml/JinBJ18} for details) of molecules from the ZINC database \citep{DBLP:journals/jcisd/IrwinSMBC12, dwivedi2020benchmarking}. Our main baselines are GIN (which we re-implement), as well as a stronger baseline (as in Eq. \ref{eq:gsn-v} and \ref{eq:gsn-e} without structural features - see Appendix \ref{zinc_appendix} for details) that can also take into account edge features (MPNN-\textit{EF}). We then extend both baselines with structural features obtained with -cycle counting (models denoted as GSN and GSN\textit{-EF}) and report the result of the best performing substructure w.r.t. the validation set. The data split into training, validation and test set is obtained from \cite{dwivedi2020benchmarking}. The evaluation metric and the training loss is the Mean Absolute Error (MAE). Table \ref{tab:zinc_dataset} shows that our model significantly outperforms all the baselines. 

In Figure \ref{fig:zinc-substructures}, we compare the training and test error for different substructure families (cycles, paths and non-isomorphic trees -- for each experiment we use all the substructures of size  in the family). Additionally, we measure the ``uniqueness'' of the identifiers each substructure yields as follows: for each graph  in the dataset, we measure the number of unique node features  (initial node features concatenated with node structural identifiers). Then, we sum them up over the entire training set and divide by the total number of nodes, yielding the disambiguation score , depicted with horizontal bars in Figure \ref{fig:zinc-substructures}.  A first thing to notice is that the training error is tightly related to the disambiguation score. As identifiers become more discriminative, the model gains expressive power. On the other hand, the test error is not guaranteed to decrease when the identifiers become more discriminative. For example, although cycles have smaller disambiguation scores, they manage to generalise much better than the other substructures, the performance of which is similar to the baseline architecture. This is also observed when comparing against \cite{sato2020random} (\textit{-r} methods in Table~\ref{tab:zinc_dataset}), where, akin to unique identifiers, random features are used to strengthen the expressivity of GNN architectures. This approach also fails to improve the baseline architectures in terms of mean test error. This validates our intuition that unique identifiers can be hard to generalise when chosen arbitrarily and motivates once more the importance of choosing the identifiers not only based on their discriminative power, but also in a way that allows incorporating the appropriate inductive biases. Finally, we observe a substantial jump in performance when using GSN with cycles of size . This is not surprising, as cyclical patterns of such sizes (e.g. aromatic rings) are very common in organic molecules.


\begin{table}[t!]
    \begin{minipage}[t]{0.38\linewidth}
        \centering
\caption{MAE on \textit{ZINC}. `\textit{-EF}': Additional Edge Features, `\textit{-r}': random features.  Our implementation.  }
           \resizebox{\columnwidth}{!}{
          \begin{tabular}{l | l}
Method & Test MAE \\
            \hline
            
            GCN 
& 0.4690.002 \\
            
                
            GatedGCN\textit{-EF} & 0.3630.009 \\
        
            GAT 
& 0.4630.002 \\

            
            GIN \citep{xu2018how} & 0.4080.008\\
            
            GIN & 0.2880.011\\
            
             GIN\textit{-r} &  0.2820.021 \\
            
            MPNN\textit{-EF} & 0.1840.012\\
            
            MPNN\textit{-EF-r}&  0.1930.009\\
            \hline
            
            \textbf{GSN} & 0.1390.007\\
            
            \textbf{GSN}\textit{-EF} & \textbf{0.108}0.018\\
\end{tabular}}
        \label{tab:zinc_dataset}
    \end{minipage}
    \hspace{0.3cm}
    \begin{minipage}[t]{0.58\linewidth}
\caption{ROC-AUC on \texttt{ogbg-molhiv}. `-\textit{AF}': Additional Features,  `-\textit{VN}': message passing with a virtual node.}
    \resizebox{\columnwidth}{!}{
        \begin{tabular}{l|lll}


        Method & Training & Validation & Test\\
        \hline
        GCN-\textit{VN} & 
        88.651.01 &
        83.730.78 & 
        74.181.22\\
        
        GCN-\textit{AF}&
        88.652.19 & 
        82.04 1.41 & 
        76.060.97\\
        
        GCN-\textit{VN}-{\em AF} &
        90.074.69 &
        83.840.91 &
        75.991.19\\
        \hline
        
        GIN-\textit{VN} &
        93.892.96 & 
        84.101.05  & 
        75.201.30\\
        
        GIN-\textit{AF} &
        88.642.54 & 
        82.320.90 & 
        75.581.40\\
        
        GIN-\textit{VN}-{\em AF} &
        92.733.80 & 
        84.790.68 & 
        77.071.49\\
        \hline
        
        
        {\bf GSN}-\textit{VN} & 
        93.611.85 &
        84.450.97 &
        75.881.86 \\
        
        {\bf GSN}-\textit{AF} & 
        88.673.26 &
        85.170.90 &
        76.061.74\\
        
        {\bf GSN}-\textit{VN}-{\em AF}&
         \textbf{94.30}3.38&
         \textbf{86.58}0.84&
        \textbf{77.99}1.00\\


      \end{tabular}
      }\label{tab:molhiv_dataset}
     \centering 
    \end{minipage}
\end{table}


\noindent\textbf{OGB-MOLHIV:} 
We use \texttt{ogbg-molhiv} from the Open Graph Benchmark - OGB - \citep{DBLP:ogb} as a 
graph-level binary classification task, where the aim is to predict if a molecule inhibits HIV replication or not. The baseline architecture provided by the authors is a variation of GIN that allows for edge features and is extended with a \textit{virtual node}, \textit{GIN-VN}, or with additional node/edge features, \textit{GIN-AF}, or both, \textit{GIN-VN-AF} (more information in the supplementary material). Similarly to the experiment on ZINC, we extend the baseline settings with cyclical substructure features by treating them in a similar way as node and edge features (\textit{GSN-VN, GSN-AF, GSN-VN-AF}). Using the evaluator provided by the authors, we report the ROC-AUC metric at the epoch with the best validation performance (substructures are also chosen based on the validation set).  As shown in Table~\ref{tab:molhiv_dataset}, considerable improvement in the performance of the model in all splits is obtained, thus demonstrating strong generalisation capabilities. Additional results obtained with different substructures are provided in the Appendix \ref{app: molhiv}. Tests with additional chemical features such as formal charge, hybridisation, etc. (`\textit{-AF}' setting in Table~\ref{tab:molhiv_dataset}) show that structure provides important  complementary information. 


\noindent\textbf{Structural Features \& Message Passing:} We perform an ablation study on the abilities of the structural features to predict the task at the hand, when given as input to a graph-agnostic network. In particular, we compare our best performing GSN with a DeepSets model \citep{zaheer2017deep} that treats the input features and the structural identifiers as a set. For fairness of evaluation the same hyperparameter search is performed for both models (see Appendix \ref{deepset_appendix}). Interestingly, as we show in Table \ref{tab:deepset_vs_gsn}, our baseline attains particularly strong performance across a variety of datasets and often outperforms other traditional message passing baselines. This demonstrates the benefits of these additional features and motivates their introduction in GNNs, which are unable to compute them. As expected, we observe that applying message passing on top of these features, brings performance improvements in the vast majority of the cases, sometimes considerably, as in the ZINC dataset.



\begin{table}[h]
    \centering
\caption{Comparison between DeepSets and GSN with the same structural features}
\label{tab:deepset_vs_gsn}
    \resizebox{\columnwidth}{!}{\begin{tabular}{l|ccccccc|c|c}
    
        Dataset &
        MUTAG &
        PTC &
        Proteins &
        NCI1 &
        Collab &
        IMDB-B &
        IMDB-M &
        ZINC &
        MOL-HIV \\
        \hline

        DeepSets & 
        \textbf{93.36.9} & 
        66.46.7 & 
        \textbf{ 77.84.2} & 
        80.3 2.4 & 
        80.9 1.6 &
        77.1 3.7 &
        53.3  &
        0.288 0.003 &
        77.341.46 \\
         
        \# params &
        3K & 
        2K &
        3K & 
        10K & 
        30K & 
        51K &
        68K &
        366K &
        3.4M \\
        \hline

        GSN & 
        92.87.0 & 
        \textbf{68.27.2} & 
        \textbf{77.85.6} & 
        \textbf{83.5 2.0} &
        \textbf{85.51.2} &
        \textbf{77.83.3} & 
        \textbf{54.33.3} &
        \textbf{0.108 0.018} &
        \textbf{77.991.00} \\
        
        \# params
        & 3K
        & 3K
        & 3K
        & 10K
        & 52K
        & 65K  
        & 66K
        & 385K
        & 3.3M \\
         
      
    \end{tabular}
    }
\end{table}





\section{Conclusion}

In this paper, we propose a novel way to design structure-aware graph neural networks. Motivated by the limitations of traditional GNNs to capture important topological properties of the graph, we formulate a message passing scheme enhanced with structural features that are extracted by counting the appearances of prominent substructures, as domain knowledge suggests. We show both theoretically and empirically that our construction leads to improved expressive power and attains state-of-the-art performance in real-world scenarios. In future work, we will further explore the expressivity of GSNs as an alternative to the -WL tests, as well as their generalisation capabilities. Another important direction is to infer prominent substructures directly from the data and explore the ability of graph neural networks to compose substructures.


\bibliography{iclr2021_conference}
\bibliographystyle{iclr2021_conference}

\appendix

\section{Deferred Proofs} 


\subsection{GSN is permutation equivariant}\label{proof:perm_equiv}



\begin{proof}
Let  the adjacency matrix of the graph,  the input vertex features,  the input edge features and  the edge features at dimension . Let  the functions generating the vertex and edge structural identifiers respectively. It is obvious that subgraph isomorphism is invariant to the ordering of the vertices, i.e. we will always obtain the same matching between subgraphs  and graphs  in the subgraph collection. Thus, each vertex  (edge ) in the graph will be assigned the same structural identifiers  () regarless of the vertex ordering. Thus  and  are permutation equivariant, i.e. for any permutation matrix  it holds that , , where the permutation is applied at each slice  of the tensor .



Let  a GSN layer. We will show that  is permutation equivariant.  We need to show that if  the output of the GSN layer, then  for any permutation matrix . It is easy to see that GSN-v can can be expressed as a traditional MPNN  (similar to equations 3 and 4 of the main paper) by replacing the vertex features  with the concatenation of the input vertex features and the vertex structural identifiers, i.e. . Thus, 


where in the last step we used the permutation equivariant property of MPNNs. Similarly, we can show that a GSN-e layer is permutation equivariant, since it can be expressed as a traditional MPNN layer by replacing the edge features with the concatenation of the original edge features and the edge structural identifiers .

Overall, a GSN network is permutation equivariant as composition of permutation equivariant functions, or permutation invariant when composed with a permutation invariant layer at the end, i.e. the READOUT function.

\end{proof}

\subsection{Proof of Proposition 3.1}\label{proof}

\begin{proof}

\noindent\textbf{{\modelname\ is at least as powerful as MPNNs and the 1-WL test:}} It is easy to see that \modelname\ model class contains MPNNs and is thus at least as expressive.  To show that it is at least as expressive as the 1-WL test, one can repurpose the proof of Theorem 3 in \cite{xu2018how}  and demand the injectivity of the update function (w.r.t. both the hidden state  and the message aggregation ), and the injectivity of the message aggregation w.r.t. the multiset of the hidden states of the neighbours . It suffices then to show that if injectivity is preserved then \modelname s are at least as powerful as the 1-WL.



We will show the above statement for node-labelled graphs, since traditionally the 1-WL test does not take into account edge labels.\footnote{if one considers a simple 1-WL extension that concatenates edge labels to neighbour colours, then the same proof applies.} We can rephrase the statement as follows: \textit{If GSN deems two graphs ,  as isomorphic, then also 1-WL deems them isomorphic}. Given that the graph-level representation is extracted by a readout function that receives the multiset of the node colours in its input (i.e. the graph-level representation is the node colour histogram at some iteration ), then it suffices to show that if for the two graphs the multiset of the node colours that GSN infers is the same, then also 1-WL will infer the same multiset for the two graphs. 

Consider the case where the two multisets that GSN extracts are the same:  i.e. . Then both multisets contain the same distinct colour/hidden representations with the exact same multiplicity. Thus, it further suffices to show that
if two nodes  (that may belong to the same or to different graphs) have the same GSN hidden representations  at any iteration , then they will also have the same colours , extracted by 1-WL. Intuitively, this means that GSN creates a partition of the nodes of each graph that is at least as fine-grained as the one created by 1-WL. We prove by induction (similarly to \cite{xu2018how}) that GSN model class contains a model where this holds (w.l.o.g. we show that for GSN-v; same proof applies to GSN-e).

For  the statement holds since the initial node features are the same for both GSN and 1-WL, i.e. . Suppose the statement holds for , i.e. . Then we show that it also holds for . Every node hidden representation at step  is updated as follows: . Assuming that the update function  is injective, we have the following: if , then:
\begin{itemize}[wide, labelwidth=!, labelindent=0pt]
\item , which from the induction hypothesis implies that  
\item , where the message function is defined as in Eq. \ref{eq:gsn-v} of the main paper: . Additionally here we require  to be injective w.r.t. the multiset of the hidden representations of the neighbours. In fact, using Lemma 5 from \cite{xu2018how} we know that there always exists a function , such that  is unique for each multiset , assuming that the domain from where the elements of the multiset originate is countable. Thus, 



From the induction hypothesis we know that   implies that  for any  , thus .
\end{itemize}
Concluding, given the update rule of 1-WL: , it holds that 

\noindent\textbf{{When are GSNs strictly more powerful than the 1-WL test?}} \cite{DBLP:conf/fct/ArvindFKV19} showed that 1-WL, and consequently MPNNs, can count only \textit{forests of stars}. Thus, if the subgraphs are required to be connected, then they can only be star graphs of any size (note that this contains single nodes and single edges). In addition, \cite{chen2020can} showed that 1-WL, and consequently MPNNs, cannot count any connected \textbf{induced} subgraph with 3 or more nodes, i.e. any connected subgraph apart from single nodes and single edges. 
 
 Given the first part of the proposition, in order to show that \modelname s are strictly more expressive than the 1-WL test, it suffices to show that \modelname\ can distinguish a pair of graphs that 1-WL deems isomorphic. If  is a substructure that 1-WL cannot learn to count, i.e. the ones mentioned above, then there is at least one pair of graphs with different number of counts of , that 1-WL deems isomorphic. Thus, by assigning counting features to the nodes/edges of the two graphs based on appearances of , a \modelname \ can obtain different representations for  and  by summing up the features. Hence, ,  are deemed non-isomorphic. An example is depicted in Figure \ref{fig:graph_examples} (left), where the two  non-isomorphic graphs are distinguishable by GSN via e.g. cycle counting, but not by 1-WL.
\end{proof}
\subsection{Proof of Corollary 3.1}\label{proof_universality}
\begin{proof}
In order to show the universality of GSN, we will show that when the substructure collection contains all graphs of size , then there exists a parametrisation of GSN that can infer the isomorphism classes of all vertex-deleted subgraphs of the graph at hand  (the \textit{deck} of ). 

The reconstruction conjectures states that two graphs with at least three vertices are isomorphic if and only if they have the same deck. Thus, the deck is sufficient to distinguish all non-isomorphic graphs. The deck can be defined as follows:

Let  the set of all possible graphs of size . The vertex-deleted subgraphs of  are by definition all the induced subgraphs of  with size , which we denote as . Then, the deck  can be defined as a vector of size , where at the -th dimension , where  the indicator function. 
The structural feature   for each substructure  and orbit  are computed as follows:

where  if , otherwise it is the bijective mapping function. The deck can be inferred as follows:

where we used that  = 1, since each vertex can be mapped to a single orbit only. Thus, the deck can be inferred by a simple GSN-v parametrisation (a linear layer with depth equal to  that performs orbit-wise summation and division by the constant  for each node separately, followed by a sum readout). Since GSN-v can be inferred by GSN-e (Proposition \ref{GSN-v vs GSN-e}), then GSN-e is also universal.
\end{proof}


\subsection{Proof of Proposition 3.2}\label{proof_GSNv_GSNe}


\begin{proof}
Without loss of generality we will show Proposition 3.2 for the case of a single substructure . In order to show that GSN-e can express GSN-v, we will first prove that \textit{the vertex identifier of each vertex  can be inferred by the edge identifiers of its incident edges }.

We first introduce the following definitions to simplify notation:  Recall the definition of a vertex orbit . For an orbit , denote the \textit{orbit neighbourhood}  as the multiset of the orbits of the neighbours of : . Note that the orbit neighbourhood is the same for any vertex  and that the neighbourhood is a multiset, since different neighbours might belong to the same orbit. Denote the \textit{orbit degree}  as the degree of any vertex in  as . We will be indexing different vertex orbits as follows: .

Regarding the edge orbits, recall that each edge orbit is induced by the orbits of the vertex automorphism group, i.e. , or alternatively . Observe here, that we define edge orbits for directed edges. Let ,   and . 


Recall that vertex structural identifiers are defined as follows (we drop the subscript  for ease of notation):



and edge structural identifiers for directed edges:




Let's assume that there exists only one matched subgraph  and the bijection between  and  is denoted as . Then, concerning the vertex orbit , one of the following holds:
\begin{itemize}
    \item . Then  and ,
    \item  and . Then,  and , otherwise the orbit of  would be . Note that here the directionality of the edge is important, otherwise an ambiguity would be created and  could have been equal to 1 if the orbit of  is .
    \item  and . Then,  and since  has exactly  neighbours in , then  has exactly  neighbours that belong to  as well and the set of their vertex orbits is . In other words, 
\end{itemize}

Thus, by induction, we can easily see that for  matched subgraphs  where  and , it holds that  and . Then it follows that:




The rest of the proof is straightforward: we will assume a GSN-v using substructure counts of the graph , with  layers and width  defined as in the main paper, i.e. at each layer vertex hidden states are updated by , where . Then, there exists a GSN-e with  layers, where the first layer has width  and implements the following function: , where:

Note that since  is a universal approximator, then there exist parameters of  with which the above function can be computed. The next  layers of GSN-e can implement a traditional MPNN where now the input vertex features are  (which is exactly the formulation of GSN-v) and this concludes the proof. 

\end{proof}




\section{Comparison with higher-order Weisfeiler-Leman tests}\label{k-wl-comparison}

\subsection{The WL hierarchy}\label{k-wl}

Following the terminology introduced in \cite{maron2019provably}, we describe the so-called {\em Folklore WL family (-FWL)}. Note that, in the majority of papers on GNN expressivity \citep{morris2019weisfeiler, maron2019provably, chen2020can} another family of WL tests is discussed, under the terminology \textit{-WL} with expressive power equal to -FWL. In contrast, in most graph theory papers on graph isomorphism \citep{DBLP:journals/combinatorica/CaiFI92, furer2017combinatorial, DBLP:conf/fct/ArvindFKV19} the -WL term is used to describe the algorithms referred to as -FWL in GNN papers. Here, we follow the -FWL convention to align with the work mostly related to ours.

The -FWL operates on -tuples of nodes  to which an initial colour  is assigned based on their \textit{isomorphism types} (see section \ref{sr_graphs_wl}), which can loosely be thought of as a generalisation of isomorphism that also preserves the ordering of the nodes in the tuple. Then, at each iteration the colour is refined as follows:

where . 

The multiset  can be perceived as a form of generalised neighbourhood. Note here, that information is saved in all possible tuples in the graph, thus each -tuple receives information {\em from the entire graph}, contrary to the {\em local} nature of the 1-WL test. 

\subsection{When can GSN be stronger than 2-FWL?}\label{gsn_vs_2fwl}

In order to give further intuition in the limitations of the 2-FWL, in the following section we describe a specific family of graphs which 2-FWL is known to be unable to disambiguate.

\begin{definition}[Strongly regular graph] 
A {\em SR(,,,)-graph} is a regular graph with  nodes and degree , where every two adjacent vertices have always  mutual neighbours, while every two non-adjacent vertices have always  mutual neighbours. 
\end{definition}

2-FWL will always decide that a pair of SR graphs with the same parameters  are isomorphic, since all 2-tuples of the same isomorphism type have the exact same generalised neighbourhood, leading 2-FWL to converge to its solution at initialisation. On the other hand, as we show in section \ref{synthetic} of the main paper, GSN can tell strongly regular graphs apart by using small-sized substructures. Although it is not clear if there exists a certain substructure collection that results in GSNs that align with the WL hierarchy, we stress that this is not a necessary condition in order to design more powerful GNNs. In particular, the advantages offered by k-WL might not be able to outweigh the disadvantage of the larger computational complexity introduced. For example, a 2-FWL equivalent GNN will still fail to count 4-cliques (a frequent pattern in social and biological networks) or 8 cycles (a common ring structure in organic molecules).  Therefore, we suggest that it might be more appropriate to design powerful GNNs based on the distinct characteristics of the task at hand.


\subsection{Why does 2-FWL fail on strongly regular graphs?}\label{sr_graphs_wl}

\begin{proof}

 We first rigorously describe what an isomorphism type is. Two k-tuples , 
 will have the same isomorphism type iff:

\begin{itemize}
    \item  
    \item  , where  means that the vertices are adjacent.
\end{itemize}

Note that this is a stronger condition than isomorphism, since the mapping between the vertices of the two tuples needs to preserve order. In case the graph is employed with edge and vertex features, they need to be preserved as well (see \cite{chen2020can}) for the extended case). 

For the 2-FWL test, when working with simple undirected graphs without self-loops, we have the following 2-tuple isomorphism types:
\begin{itemize}
    \item : \textit{vertex type}. Mapped to the colour 
    \item  and :\textit{ non-edge type}. Mapped to the colour 
    \item  and : \textit{edge type}. Mapped to the colour 
\end{itemize}
 
 For each 2-tuple , a generalised ``neighbour'' is the following tuple: , where  is an arbitrary vertex in the graph. 

Now, let us consider a strongly regular graph SR(,,,). We have the following cases:

\begin{itemize}
    \item generalised neighbour of a  \textit{vertex type} tuple: . The corresponding neighbour colour tuples are:
    \begin{itemize}
        \item   if  ,
        \item  if  ,
        \item  if .
    \end{itemize}
    The update of the 2-FWL is: n-1-dd same for all \textit{vertex type} 2-tuples.
    
    \item generalised neighbour of a \textit{non-edge type} tuple: . The corresponding neighbour colour tuples are:
    \begin{itemize}
        \item  if ,  
        \item  if , 
        \item  if  and , 
        \item  if  and , 
        \item  if  and , 
        \item  if  and .
    \end{itemize}
     The update of the 2-FWL is:
     
     d-\mud-\mun-2 - (2d - \mu)\mu
same for all \textit{non-edge type} 2-tuples.

    
    \item generalised neighbour of an \textit{edge type} tuple:  
    \begin{itemize}
        \item  if , 
        \item  if , 
        \item  if  and , 
        \item  if  and , 
        \item  if  and , 
        \item  if  and .
    \end{itemize}
    The update of the 2-FWL is:
     
     d-\lambdad-\lambdan-2-(2d - \lambda)\lambda
same for all \textit{edge type} 2-tuples.
\end{itemize}

From the analysis above, it is clear that all 2-tuples in the graph of the same initial type are assigned the same colour in the 1st iteration of 2-FWL. In other words, the vertices cannot be further partitioned, so the algorithm terminates. Therefore, if two SR graphs have the same parameters ,,, then 2-FWL will yield the same colour distribution and thus the graphs will be deemed isomorphic.

\end{proof}

\section{Experimental Settings - Additional Details}\label{experiments_supp}

In this section, we provide additional implementation details of our experiments. All experiments were performed on a server equipped with 8 Tesla V100 16 GB GPUs, except for the Collab dataset where a Tesla V100 GPU with 32 GB RAM was used due to larger memory requirements.
Experimental tracking and hyperparameter optimisation were done via the Weights \& Biases platform (wandb) \citep{wandb}. Our implementation is based on native PyTorch sparse operations \citep{paszke2019pytorch} in order to ensure complete reproducibility of the results. PyTorch Geometric \citep{DBLP:journals/corr/abs-1903-02428} was used for additional operations (such as preprocessing and data loading).

In each one of the different experiments we aim to show that structural identifiers can be used off-the-shelf and are independently of the architecture. At the same time we aim to suppress the effect of other confounding factors in the model performance, thus wherever possible we build our model on top of a baseline architecture. More details in the relevant subsections. Interestingly, we observed that in most of the cases it was sufficient to replace only the first layer of the baseline architecture with a GSN layer, in order to obtain a boost in performance.

Throughout the experimental evaluation the structural identifiers  and  are one-hot encoded, by taking into account the unique count values present in the dataset. Other more sophisticated methods can be used, e.g. transformation to continuous features via a normalisation scheme or binning. However, we found that the number of unique values in our datasets were usually relatively small (which is a good indication of recurrent structural roles) and thus such methods were not necessary.



\subsection{Synthetic Experiment}

For the Strongly Regular graphs dataset (available from \url{http://users.cecs.anu.edu.au/~bdm/data/graphs.html}) we use all the available families of graphs with size of at most 35 nodes: 

\begin{itemize}
    \item SR(16,6,2,2): 2 graphs
    \item SR(25,12,5,6): 15 graphs
    \item SR(26,10,3,4): 10 graphs
    \item SR(28,12,6,4): 4 graphs
    \item SR(29,14,6,7): 41 graphs
    \item SR(35,16,6,8): 3854 graphs
    \item SR(35,18,9,9): 227 graphs
\end{itemize}

The total number of non-isomorphic pairs of the same size is . We used a simple 2-layer architecture with width 64. The message aggregation was performed as in the general formulation of Eq. \ref{eq:gsn-v} and \ref{eq:gsn-e} of the main paper, where the update and the message functions are MLPs. The prediction is inferred by applying a sum readout function in the last layer and then passing the output through a MLP. Regarding the substructures, we use \textit{graphlet} counting, as certain \textit{motifs} (e.g. cycles of length up to 7) are known to be unable to distinguish strongly regular graphs (since they can be counted by the 2-FWL \citep{furer2017combinatorial, DBLP:conf/fct/ArvindFKV19}).

Given the adversities that strongly regular graphs pose in graph isomorphism testing, it would be interesting to see how this method can perform in other categories of hard instances, such as the classical  \textit{CFI} counter-examples for k-WL proposed in \cite{DBLP:journals/combinatorica/CaiFI92}, and explore further its expressive power and combinatorial properties. We leave this direction to future work.


\subsection{TUD Graph Classification Benchmarks}

For this family of experiments, due to the usually small size of the datasets, we choose a parameter-efficient architecture, in order to reduce the risk of overfitting. In particular, we follow the simple GIN architecture \citep{xu2018how} and we concatenate structural identifiers to node or edge features depending on the variant. Then for GSN-v, the hidden representation is updated as follows:

and for GSN-e:

where  is a dummy variable (also one-hot encoded) used to distinguish self-loops from edges. Empirically, we did not find training the  parameter used in GIN to make a difference. Note that this architecture is less expressive than our general formulation. However, we found it to work well in practice for the TUD datasets, possibly due to its simplicity and small number of parameters.
 


\begin{table}[t]
    \centering
\caption{Graph Classification accuracy on various social and biological networks from the TUD Datasets collection \url{https://chrsmrrs.github.io/datasets/}. Graph Kernel methods are denoted with an *. For completeness we also include methods that were evaluated on potentially different splits. The top three performance scores are highlighted as: {\bf \bf \color{red} First}, {\bf \bf \color{violet} Second}, {\bf Third}.}
    \label{tab:tud_datasets_full}
\resizebox{\columnwidth}{!}{
    \begin{tabular}{p{0.01cm}l|llll|lll}
& Dataset & MUTAG & PTC & Proteins & NCI1 & Collab & IMDB-B & IMDB-M \\
        \hline
        \hline
        &size &188 &344 &1113 &4110 &5000 &1000 &1500\\
        &classes &2 &2 &2& 2  &3& 2 &3\\
        &avg num. nodes& 17.9& 25.5 &39.1& 29.8 &74.4& 19.7 &13\\
        
        \hline 
         \parbox[t]{1mm}{\multirow{9}{*}{\rotatebox[origin=c]{90}{different splits}}} & DGK* (best) \citep{DBLP:conf/kdd/YanardagV15} & 
         87.4 2.7 & 
         60.12.6	&
         75.70.5 &
         80.3 0.5 &
         73.1 0.3 &
         67.00.6& 
         44.60.5\\
         
               & FSGD* \citep{DBLP:conf/nips/VermaZ17} & 
         92.1 & 62.8 & 73.4 & 79.8&  80.0 &73.6& 52.4\\
         
     & AWE-FB*\citep{DBLP:conf/icml/IvanovB18} & 
     87.99.8& 
     N/A & 
     N/A &
     N/A & 
     71.01.5 &  
     73.13.3 &  
     51.64.7 \\
        &AWE-DD*\citep{DBLP:conf/icml/IvanovB18} & 
        N/A& 
        N/A & 
        N/A & 
        N/A &
        73.91.9 &  
        74.55.8 & 
        51.53.6 \\
         
         & ECC \citep{DBLP:conf/cvpr/SimonovskyK17} & 
         76.1 & N/A & N/A & 76.8 & N/A &N/A &N/A\\
         
         
        & PSCN k=10\textsuperscript{E} \citep{DBLP:conf/icml/NiepertAK16} &
        92.64.2 &
        60.04.8
        & 75.92.8 & 
        78.61.9 & 
        72.62.2 & 
        71.02.3 
        & 45.22.8\\
        
        & DiffPool \citep{DBLP:conf/nips/YingY0RHL18} & 
        N/A &
        N/A &
        76.2 &
        N/A&
        75.5  & 
        N/A &
        N/A\\
       
        & CCN \citep{DBLP:conf/iclr/KondorSPAT18} &
        91.67.2 &
        70.67.0 &
        N/A &
        76.34.1 & 
        N/A & 
        NA &
        N/A \\
        
        &1-2-3 GNN \citep{morris2019weisfeiler} & 86.1 & 60.9	& 75.5 & 76.2 & N/A & 74.2 & 49.5\\


                
        \hline
        
        \parbox[t]{1mm}{\multirow{10}{*}{\rotatebox[origin=c]{90}{same splits}}} &RWK* \citep{DBLP:conf/colt/GartnerFW03} & 
         79.22.1 & 
         55.90.3 & 
         59.60.1 & 
         >3 days & N/A & N/A &
         N/A\\
        
        &GK* (k=3) \citep{shervashidze2009efficient} & 
        81.41.7& 
        55.70.5 &
        71.40.31 & 
        62.50.3 & 
        N/A & 
        N/A &
        N/A\\

         
        &PK* \citep{DBLP:journals/ml/NeumannGBK16} & 
         76.02.7& 
         59.52.4 &
         73.70.7 & 
         82.50.5 & 
         N/A & 
         N/A & 
         N/A\\
         
         
         & WL kernel* \citep{shervashidze2011weisfeiler} &
         \textbf{90.45.7} & 
         59.94.3 & 
         75.03.1 & 
         \textcolor{red}{\textbf{86.0}\textbf{1.8}} & 
         78.91.9 &
         73.83.9 &
         50.93.8 \\
         
        & GNTK* \citep{DBLP:conf/nips/DuHSPWX19} & 
        90.08.5 &
        \textbf{\textcolor{violet}{67.96.9}} &
        75.64.2 & 
        \textbf{\textcolor{violet}{84.21.5}} &
        \textbf{\textcolor{violet}{83.61.0}} &
        \textbf{\textcolor{violet}{76.93.6}} &
        \textbf{\textcolor{violet}{52.84.6}}\\ 
         
        &DCNN \citep{DBLP:conf/nips/AtwoodT16}& 
          N/A&  N/A &
          61.31.6
          & 56.61.0 &
          52.10.7 &
          49.11.4 & 
          33.51.4\\


        & DGCNN \citep{zhang2018end} & 85.81.8 & 
        58.62.5 & 
        75.50.9 & 
        74.40.5 & 
        73.80.5 &
        70.00.9 & 
        47.80.9\\
        

        
        &IGN \citep{DBLP:conf/iclr/MaronBSL19} & 83.913. &
        58.56.9 &
        \textbf{\textcolor{violet}{76.65.5}} &
        74.32.7 & 
        78.32.5 & 
        72.05.5 & 
        48.73.4\\
        
        &GIN \citep{xu2018how} & 
        89.45.6 & 
        64.67.0	& 
        \textbf{76.22.8} &
        82.71.7 &
        80.21.9 & 
        75.15.1 & 
        52.32.8\\
        

        
        &PPGNs \citep{maron2019provably} &
        \textbf{\textcolor{violet}{90.68.7}} &
        66.26.6 &
        \textbf{\textcolor{red}{77.24.7}} & 
        83.21.1 & 
        81.41.4 &
        73.05.8 & 
        50.53.6\\
        
        &Natural GN \citep{de2020natural} &
        89.391.60 &
        66.841.79 &
        71.711.04 &
        82.371.35 &
        N/A &
        73.502.01 &
        51.271.50 \\
        
        \hline
        &GSN-e (Ours) & \textbf{\textcolor{violet}{90.67.5}}  &
        \textbf{\textcolor{red}{68.27.2}}& 
        \textbf{\textcolor{violet}{76.65.0}}& 
        \textbf{83.5 2.3}&
        \textbf{\textcolor{red}{85.51.2}} &
        \textbf{\textcolor{red}{77.83.3}} & 
        \textbf{\textcolor{red}{54.33.3}}\\
       &  &  6 (cycles)  & 6 (cycles)  & 4 (cliques) & 15 (cycles) & 3 (triangles) & 5 (cliques)    & 5 (cliques)   \\
        \hline
        &GSN-v (Ours) &
        \textbf{\textcolor{red}{92.27.5}} & 
        \textbf{67.45.7} & 
        74.65.0 &
        \textbf{ 83.52.0} &
        \textbf{82.71.5}  &
        \textbf{{76.82.0}}&
        \textbf{52.63.6}\\
        &    &  12 (cycles)   &  10 (cycles)    &  4 (cliques) & 3 (triangles) & 3 (triangles) &   4 (cliques) &   3 (cliques) \\
\end{tabular}}
\end{table}

We implement an architecture similar to GIN \citep{xu2018how}, i.e. \textit{message passing layers}: 4 , \textit{jumping knowledge} from all the layers \citep{DBLP:conf/icml/XuLTSKJ18} (including the input), \textit{transformation of each intermediate graph-level representation}: linear layer, \textit{readout}: sum for biological and mean readout for social networks. Node features are one-hot encodings of the categorical node labels. Similarly to the baseline, the hyperparameters search space is the following: \textit{batch size} in \{32, 128\} (except for Collab where only 32 was searched due to GPU memory limits), \textit{dropout} in \{0,0.5\}, \textit{network width} in \{16,32\} for biological networks, 64 for social networks, \textit{learning rate} in \{0.01, 0.001\}, \textit{decay rate} in \{0.5,0.9\} and \textit{decay steps} in \{10,50\} (number of epochs after which the learning rate is reduced by multiplying with the decay rate). For social networks, since they are not attributed graphs, we also experimented with using the degree as a node feature, but in most cases the structural identifiers were sufficient. 


Model selection is done in two stages. First, we choose a substructure that we perceive as promising based on indications from the specific domain: \textit{triangles} for social networks and Proteins, and \textit{6-cycles (motifs)} for molecules. Under this setting we tune model hyperparameters for a GSN-e model. Then, we extend our search to the parameters related to the substructure collection: i.e. \textit{the maximum size } and \textit{motifs vs graphlets}. In all the molecular datasets we search cycles with , except for NCI1, where we also consider larger sizes due to the presence of large rings in the dataset (\textit{macrocycles} \citep{liu2017surveying}).  For social networks, we searched cliques with .  
In Table \ref{tab:tud_datasets_hyperparams} we report the hyperparameters chosen by our model selection procedure, including the best performing substructures.



The seven datasets\footnote{more details on the description of the datasets and the corresponding tasks can be found at \cite{xu2018how}.} we chose are the intersection of the datasets used by the authors of our main baselines: the Graph Isomorphism Network (GIN) \citep{xu2018how}, a simple, yet powerful GNN with expressive power equal to the 1-WL test, and the Provably Powerful Graph Network (PPGN) \citep{maron2019provably}, a polynomial alternative to the Invariant Graph Network \citep{DBLP:conf/iclr/MaronBSL19}, that increases its expressive power to match the 2-FWL.
We also compare our results to other GNNs as well as Graph Kernel approaches. Our main baseline from the GK family is the Graph Neural Tangent Kernel (GNTK) \citep{DBLP:conf/nips/DuHSPWX19}, which is a kernel obtained from a GNN of infinite width. This operates in the Neural Tangent Kernel regime \citep{DBLP:conf/nips/JacotHG18, DBLP:conf/icml/Allen-ZhuLS19, DBLP:conf/icml/DuLL0Z19}.

Table \ref{tab:tud_datasets_full} is an extended version of Table \ref{tab:tud_datasets} of the main paper, where the most prominent methods are reported, regardless of the splits they were evaluated on. For DGK (best variant) \citep{DBLP:conf/kdd/YanardagV15}, FSGD \citep{DBLP:conf/nips/VermaZ17}, AWE \citep{DBLP:conf/icml/IvanovB18},  ECC \citep{DBLP:conf/cvpr/SimonovskyK17}, PSCN \citep{DBLP:conf/icml/NiepertAK16},  DiffPool \citep{DBLP:conf/nips/YingY0RHL18}, CCN \citep{DBLP:conf/iclr/KondorSPAT18} (slightly different setting since they perform a train, validation, test split), 1-2-3 GNN \citep{morris2019weisfeiler} and GNTK \citep{DBLP:conf/nips/DuHSPWX19}, we obtain the results from the original papers. For RWK \citep{DBLP:conf/colt/GartnerFW03}, GK \citep{shervashidze2009efficient}, PK \citep{DBLP:journals/ml/NeumannGBK16}, DCNN \citep{DBLP:conf/nips/AtwoodT16} and DGCNN \citep{zhang2018end}, we obtain the results from the DGCNN paper, where the authors reimplemented these methods and evaluated them with the same split. Similarly, we obtain the WLK \citep{shervashidze2011weisfeiler} and GIN \citep{xu2018how} results from the GIN paper, and IGN \citep{DBLP:conf/iclr/MaronBSL19} and PPGN \citep{maron2019provably} results from the PPGN paper.


\begin{table}[t]
    \centering
\caption{Chosen hyperparameters for each of the two GSN variants for each dataset.}
    \label{tab:tud_datasets_hyperparams}
     \resizebox{\columnwidth}{!}{
    \begin{tabular}{p{0.8cm}p{3cm}|llll|lll}
& Dataset & MUTAG & PTC & Proteins & NCI1 & Collab & IMDB-B & IMDB-M \\
        
      \hline
        
        \parbox[t]{1mm}{\multirow{9}{*}{GSN-e}} &  batch size & 32 & 128	& 32 & 32 & 32 & 32&  32\\
         
        & width  & 
         32 & 16 &  32 & 32 & 64 & 64 & 64\\
         
     & decay rate  & 0.9 & 0.5 & 0.5 & 0.9 & 0.5 &  0.5&  0.5 \\
     
        & decay steps & 50 & 50 & 10 & 10 & 50 &  10 & 10 \\
         
         & dropout & 0.5 & 0 & 0.5 & 0 & 0 & 0 & 0\\
         
        & lr &  &  &  & & &  & \\
        
        & degree  & No & No & No & No & No & No & Yes\\

         
        & substructure type & 
          graphlets &  motifs & same & graphlets & same & same & same\\

        
        & substrucure family & cycles & cycles & cliques &cycles& clique & clique & cliques\\
        
        & k & 6	& 6 & 4 & 15 & 3 & 5 & 5\\
        
        
        \hline 
         \parbox[t]{1mm}{\multirow{9}{*}{GSN-v}} &  batch size & 32 & 128	& 32 & 32 & 32 & 32&  32\\
         
        & width  & 
         32 & 16 &  32 & 32 & 64 & 64 & 64\\
         
     & decay rate  & 0.9 & 0.5 &0.5 & 0.9 & 0.5 &  0.5&  0.5 \\
     
        & decay steps & 50 & 50 & 10 & 10 & 50 &  10 & 10 \\
         
         & dropout & 0.5 & 0 & 0.5 & 0 & 0 & 0 & 0\\
         
        & lr &  &  &  & & &  & \\
        
        & degree  & No & No & No & No & No & Yes & Yes\\

        
         
        & substructure type & graphlets & graphlets & same& same & same & same & same\\
        

        & substrucure family & cycles & cycles & cliques &cycles& cliques & clique & cliques\\
       

        
        & k  &12 & 10& 4 & 3 & 3 & 4 & 3\\
        
        
         
        

                

        
\end{tabular}}
\end{table}


\subsection{Graph Regression on ZINC}\label{zinc_appendix}

\noindent\textbf{Experimental Details: }The ZINC dataset includes 12k molecular graphs of which 10k form the training set and the remaining 2k are equally split between validation and test (splits obtained from \url{https://github.com/graphdeeplearning/benchmarking-gnns}). Molecule sizes range from 9 to 37 nodes/atoms. Node features encode the type of atoms and edge features the chemical bonds between them. Again, here node and edge features are one-hot encoded. 

We re-implemented the GIN baseline (\textit{GIN} in Table \ref{tab:zinc_dataset} of the main paper). We extended GIN with structural identifiers as in Eq. \ref{GSN-v-gin} and \ref{GSN-e-gin} (\textit{GSN} model in Table \ref{tab:zinc_dataset}). Our stronger baseline (\textit{MPNN-EF} model in Table \ref{tab:zinc_dataset}) updates node representations as follows:  ,     , where  and  functions are MLPs and  are edge features. Our extension with structural identifiers (\textit{GSN-EF} model in Table \ref{tab:zinc_dataset}) is precisely the model of Eq. \ref{eq:gsn-v} or \ref{eq:gsn-e} of the main paper. Observe that, probably due to the fact that the ZINC dataset is larger and more stable, the general MPNN-based formulation performs better than the GIN-based counterpart. 

Following the same rationale as before, the network configuration is minimally modified w.r.t. the baseline of \cite{dwivedi2020benchmarking}, while here no hyperparameter tuning is done, since the best performing hyperparameters are provided by the authors. In particular, the parameters are the following: \textit{message passing layers}: 4, \textit{transformation of the output of the last layer}: MLP, \textit{readout}: sum, \textit{batch size}: 128, \textit{dropout}: 0.0, \textit{network width}: 128, \textit{learning rate}: 0.001. The learning rate is reduced by 0.5 (decay rate) after 5 epochs (\textit{decay rate
patience}) without improvement in the validation loss. Training is stopped when the learning rate reaches the \textit{minimum learning rate} value of .  Validation and test metrics are inferred using the model at the last training epoch. 

We select our best performing substructure related parameters based on the performance in the validation set in the last epoch. We search cycles with  , \textit{graphlets vs motifs}, and \textit{GSN-v vs GSN-e}. The chosen hyperparameters for GSN are:  \textit{GSN-e, cycle graphlets of 10 nodes} and for GSN\textit{-EF}: \textit{GSN-v, cycle motifs of 8 nodes}. Once the model is chosen, we repeat the experiment 10 times with different seeds and report the mean and standard deviation of the test MAE in the last epoch. This is performed for all 4 models that we implemented and compared (\textit{GIN, MPNN\textit{-EF}, GSN, GSN\textit{-EF}}).

        
\noindent\textbf{Fixed parameter budget:}  In Table \ref{tab:zinc_100k} we include an additional comparison between the main methods, where we maintain an approximately constant parameter budget (approx. 100k parameters; we adjust network width to keep this number consistent across different models), as suggested in \cite{dwivedi2020benchmarking}. Again here, GSN significantly outperforms the baselines, hence the improvement is insensitive to the absolute number of the parameters.


\begin{table}[H]
        \centering
\caption{MAE on \textit{ZINC}. Experiments with the same parameter budget (100K)}
          \begin{tabular}{l | l}
Method & Test MAE \\
            \hline
            
            GIN & 0.3880.013\\
            
            MPNN\textit{-EF} & 0.3240.006\\
            \hline
            
            \textbf{GSN} &\textbf{0.1930.010}\\
            
            \textbf{GSN\textit{-EF}} & \textbf{0.1540.002}\\
\end{tabular}\label{tab:zinc_100k}
\end{table}


\noindent\textbf{Generalisation - empirical observations:} We repeat the experimental evaluation on ZINC using different fractions of the training set. We compare our best baseline model (MPNN\textit{-EF} in Table \ref{tab:zinc_dataset}) against GSN with the best performing substructure (GSN\textit{-EF} in Table \ref{tab:zinc_dataset}). In Figure \ref{fig:generalisation}, we plot the training and test errors of both methods. Regarding the training error, GSN consistently performs better, following our theoretical analysis on its expressive power. More importantly, GSN manages to generalise much better even with a small fraction of the training dataset. Observe that GSN requires only 20\% of the samples to achieve approximately the same test error that MPNN achieves when trained on the entire training set.

\begin{figure}[!h]
  \centering
  \includegraphics[width=0.5\textwidth]{figures/generalization.pdf}
    \captionsetup[figure]{skip=\abovecaptionskip}
  \captionof{figure}{Train (dashed) and test (solid) MAEs for GSN\textit{-EF}(blue) and MPNN\textit{-EF}(red) as a function of the dataset fraction employed in training.}
  \label{fig:generalisation}
\end{figure}

\noindent\textbf{Disambiguation Scores:} In Table \ref{tab:uniqueness}, we provide the disambiguation scores  as defined in section \ref{sec:experiments} of the main paper for the different types of substructures. Note that these are computed based on vertex structural identifiers (GSN-v).

\begin{table}[h]
         \centering
\caption{Disambiguation scores  on \textit{ZINC} for different substructure families and their maximum size . Size  refers to the use of the original node features only.}
          \begin{tabular}{r | l | l | l}
k & Cycles & Paths & Trees \\
            \hline
            0 & 0.196 & 0.196 & 0.196\\
            3 & 0.199 & 0.540 & 0.540\\
            4 & 0.200 & 0.746 & 0.762\\
            5 & 0.256 & 0.866 & 0.875\\
            6 & 0.327 & 0.895 & 0.897\\
            7 & 0.330 & 0.900 & 0.900\\
            8 & 0.330 & 0.901 & 0.901\\
            9 & 0.330 & 0.901 & 0.901\\
            10 & 0.330 & 0.901 & 0.901\\
          \end{tabular}\label{tab:uniqueness}
\end{table}




\subsection{Graph Classification on \texttt{ogbg-molhiv}}\label{app: molhiv}

The \texttt{ogbg-molhiv} dataset contains  41K graphs, with 25.5 nodes and 27.5 edges on average. As most molecular graphs, the average degree is small (2.2) and they exhibit a tree-like structure (average clustering coefficient 0.002). The average diameter is 12 (more details in \cite{DBLP:ogb}).

We follow the design choices of the authors of \cite{DBLP:ogb} and extend their architectures to include structural identifiers. Initial node features and edge features are multi-hot encodings passed through linear layers that project them in the same embedding space, i.e. , .
The baseline model is a modification of GIN that allows for edge features: for each neighbour, the hidden representation is added to an embedding of its associated edge feature. Then the result is passed through a ReLU non-linearity which produces the neighbour's message. Formally, the aggregation is as follows:

A stronger baseline is also proposed by the authors: in order to allow global information to be broadcasted to the nodes, a \textit{virtual node} takes part in the message passing (\textit{-VN} setting in Table \ref{tab:molhiv_dataset} of the main paper). The virtual node representation, denoted as , is initialised as a zero vector  and then Message Passing proceeds as follows:
  

We modify this model, as follows: first the substructure counts are embedded into the same embedding space as the rest of the features. Then, for GSN-v, they are added to the corresponding node embeddings:  , or for GSN-e, they are added to the edge embeddings . Interestingly, even with this simple modification we obtain a considerable improvement in the performance of the model in all splits, thus demonstrating strong generalisation capabilities.

\begin{table}[t]
    \centering
\caption{Chosen substructures for \texttt{ogbg-molhiv}}
    \label{tab:ogb_datasets_hyperparams}
    \begin{tabular}{llll}
Model & GSN-AF & GSN-VN & GSN-VN -AF\\
        \hline
        features
        & edges (GSN-e) &  vertices (GSN-v)&  edges (GSN-e) \\
        substructure type
        & graphlets &  graphlets& graphlets \\
        substructure family & cycles & cycles  &cycles \\
        k & 12 & 6 & 6 \\
\end{tabular}
\end{table}


We use the same hyperparameters as the ones provided by the authors, i.e. \textit{message passing layers}: 5, \textit{readout}: mean, \textit{transformation of the output of the last layer}: linear layer, batch size: 32, \textit{batch size
dropout}: 0.5, \textit{network width/embedding dimension}: 300 (in the ogb implementation the hidden layer of each MLP has dimensions equal to 2*network width, contrary to the rest of the experiments where network width and MLP hidden dimensions are equal), \textit{learning rate}: 0.001.

We select our best performing substructure related parameters based on the highest validation ROC-AUC (choosing the best scoring epoch as in \cite{DBLP:ogb}). We search cycles with , \textit{graphlets vs motifs}, and \textit{GSN-v vs GSN-e} (see Table \ref{tab:ogb_datasets_hyperparams} for the chosen parameters).  We repeat the experiment 10 times with different seeds and report the mean and standard deviation of the train, validation and test ROC-AUC, again by choosing the best scoring epoch w.r.t the validation set. We repeat the process for all 3 settings independently (\textit{GSN-VN, GSN-AF, GSN-VN-AF}).

In parallel and following our work, several other methods have been proposed and evaluated on \texttt{ogbg-molhiv}. Some of them are orthogonal to our work and can be used as stronger baselines, in a similar plug-and-play manner (e.g. techniques that facilitate training of deeper GNNs \citep{li2020deepergcn},  Wasserstein graph embeddings \citep{kolouri2020wasserstein}, multiple aggregation functions \citep{corso2020principal}, adversarial data augmentation \citep{kong2020flag}, graph normalisation \citep{cai2020graphnorm} and directional aggregation \citep{beaini2020directional}), while others are domain-specific approaches, specifically designed for molecules \citep{fey2020hierarchical, rogers2010extended}. The interested reader may refer to the OGB leaderboard \url{https://ogb.stanford.edu/docs/leader_graphprop/#ogbg-molhiv} for a detailed list of the state-of-the-art methods.

\noindent\textbf{Impact of the `Scaffold-based' dataset split:}
Motivated by the differences between the train, validation and test distributions in the ogbg-molhiv dataset (the authors of \cite{DBLP:ogb} implement a ``scaffold splitting'' procedure, i.e. molecules with different 2D-structure are assigned to a different partition of the dataset), we studied how different substructures affect the performance on different sets. In particular, as shown in Table \ref{tab:ablation_molhiv_dataset}, we observed that substructures that yielded the best test score did not always attain best validation performance as well, this confirming the characteristic discrepancy between the test and validation distributions. 

Lastly, we note that these results were obtained with a GSN model using the structural features only at its input. In fact, it is possible to 'inject' the structural features at each message passing layer of the architecture, effectively accounting for a skip connection acting on structural identifiers. Although we observe performance improvements, the typical validation/test distribution discrepancy persists. Results are reported in Table \ref{tab:ablation_molhiv_dataset} (``GSN - skip'').

\begin{table}[t!]
\caption{ROC-AUC on \texttt{ogbg-molhiv}. Performance with different substructures and skip connections for the structural identifiers}
        \begin{tabular}{l|lll}


        Method & Training & Validation & Test\\
        \hline
        GSN  ()&
        94.29   3.38&
        \textbf{ 86.58}   0.84 &
        77.99  1.00\\

        GSN  ()&
        94.33    2.38&
        85.59   0.82&
        78.20    1.69\\
        
        
        GSN - skip ()&
        93.77  3.38&
         86.441.09&
        78.070.83\\
        
        GSN - skip ()&
        93.97    2.70 &
        85.30   1.01&
       \textbf{78.55}   1.25\\
\end{tabular}
          \label{tab:ablation_molhiv_dataset}
     \centering 
\end{table}






\subsection{Structural Features \& Message Passing}\label{deepset_appendix}

The baseline architecture treats the input node and edge features, along with the structural identifiers, as a \textit{set}. In particular, we consider each graph as a set of independent edges  endowed with the features of the endpoint nodes , the structural identifiers  and the edge features , and we implement a DeepSets universal set function approximator \citep{zaheer2017deep} to learn a prediction function:
{\small

}
, with  the edge set of the graph and , modelled as MLPs. This baseline is naturally extended to the case where we consider edge structural identifiers by replacing  with . For fairness of evaluation, we follow the exact same parameter tuning procedure as the one we followed for our GSN models for each benchmark, i.e. for the TUD datasets we first tune network and optimisation hyperaparameters (network width was set to be either equal to the ones we tuned for GSN, or such that the absolute number of learnable parameters was equal to those used by GSN; depth of the MLPs was set to 2) and subsequently we choose the substructure related parameters based on the evaluation protocol of \cite{xu2018how}. For ZINC and ogbg-molhiv we perform only substructures selection, based on the performance on the validation set. Using the same widths as in GSN leads to smaller baseline models w.r.t the absolute number of parameters, and we interestingly observed this to lead to particularly strong performance in some cases, especially Proteins and MUTAG, where our DeepSets implementation attains state-of-art results. This finding motivated us to explore `smaller' GSNs (with either reduced layer width or a single message passing layer). These GSN variants exhibited a similar trend, i.e. to perform better than their `larger' counterparts over these two datasets. We hypothesise this phenomenon to be mostly due to the small size of these datasets, which encourages overfitting when using architectures with larger capacity. In Table \ref{tab:deepset_vs_gsn} in the main paper, we report the result for the best performing architectures, along with the number of learnable parameters.



\end{document}
