\section{Experiments}

\subsection{Dataset and evaluation protocols}
\noindent {\bf MPII \cite{andriluka14cvpr} }
This dataset contains in-the-wild images for 2D human pose estimation, which includes 25k training images and 3k validation images. Those images are annotated with 2D joint locations and bounding boxes. In Section \ref{sec:dataset}, we constructed a 3D pose dataset on top of MPII dataset by annotating 3D joints to images. This dataset is used in training.

\noindent {\bf Human3.6M }
Human 3.6M dataset \cite{ionescu2014human3} is used in training and testing. Human 3.6M dataset is a large scale dataset for 3D human pose detection. This dataset contains 3.6 million images of 15 everyday activities, such as walking, sitting and making a phone call, which is performed by 7 professional actors. 3D positions of joint locations captured by motion capture (Mocap) systems are also available in the dataset. In addition, 2D projections of those 3D joint locations into images are available. The images are taken from four different views. As with previous researches, we down-sampled the video from 50fps to 10fps in order to reduce redundancy in video frames. We followed the same evaluation protocol used in previous approaches \cite{PavlakosZDD16,zhou2017weakly} for evaluation, where we use 5 subjects (S1, S5, S6, S7, S8) for training and the rest 2 subjects (S9, S11) for testing.  We used the 3D model of an actor provided in Human3.6 to generate mesh animations but this could be replaced with any 3D models.

The error metric used is called mean per joint position error (MPJPE) in . In the evaluation protocol, the position of the root joints is aligned with the ground truth but the global orientation is kept as is. Following \cite{zhou2017weakly} the output joint positions from ConvNets is scaled so that the sum of all 3D bone lengths is equal to that of a canonical average skeleton. This is done by:

where  is the predicted position,  is the root position,  is the sum of skeleton length of the predicted skeleton and  is the average of sum of skeleton length for all the training subjects in Human 3.6M dataset. 

We also evaluated the method with the error measure called the reconstruction error, where, before calculating the error, the result is aligned to the ground truth with a similarity transformation. 



\subsection{Baselines}
Four baseline methods are implemented to conduct ablation studies. We trained the first three networks using Human3.6M dataset only and the fourth one with both Human3.6M and MPII dataset. 

\noindent {\bf Rotation regress (Rot reg) only } This only uses bone rotation regressor. The position is computed from linear integration of bone rotations. 

\noindent {\bf Heatmap (HM) only } This on the other hand only uses cross heatmap regressor.  

\noindent {\bf Rot reg + HM } This method is our proposed method which combines bone rotation regressor and cross heatmap regressor.

\noindent {\bf All } This is our proposed method trained using Human 3.6M and MPII dataset with 3D annotations obtained using the method presented in Sec \ref{sec:dataset}.   


\subsection{Implementation and training detail}
Our method is implemented using MatConvNet toolbox \cite{vedaldi15matconvnet}. For the bone rotation regressor, we use ResNet50 \cite{DBLP:journals/corr/HeZRS15} as the base network, which is pre-trained on the ImageNet dataset. A single up-sampling layer and a single hourglass module is followed by the base network to predict heatmaps. We also use skip connections to connect skeleton regression layers and up-sampling layers. We used a skeleton with 16 joints and 15 segments each of them have 9 rotational parameters. The definition of joints is same as that of MPII dataset. Training a whole model takes about 1 day using three NVIDIA Quadro P6000 graphics cards with 24 GB memory. The batch size is 30 for each GPU. As for augmentation, we used left/right flip only---no scaling and rotation augmentation is used. We trained a model with SGD for 70 epochs, starting from the learning late of 0.001 and decreasing it to 0.00001. During test time, a single forward pass of our network is approx. 0.12 sec, which means the performance of our method is approx. 8-9 fps.    

We set the parameters in the loss function as  and . When training with both Human 3.6M and MPII datasets, we randomly selected the images from both dataset such that half of the batch is filled with Human 3.6M and the other half by MPII dataset, following \cite{zhou2017weakly}. Since the 3D annotations of MPII are not accurate, we use them for supervising bone rotation regressor only. Thus, when training images are from MPII, we do not back propagate gradients from the losses that include the final 3D pose,   and , (i.e., ,  and the right term of ) to the network. On the other hand, when the training sample is from Human3.6M dataset, which has accurate 3D annotations, we minimize all the losses.


 
\begin{comment}
\begin{table}[hbt]
	\begin{center}
		\caption{Results on Human 3.6M dataset. Only Human 3.6M dataset is used in training. MPJPE [mm] is used for error metric.}
		\label{tab:comparison2}
		\scalebox{0.8}{
			\begin{tabular}{c c c c c c c c c}
				\hline                & Directions  & Discussion  & Eating      & Greeting    & Phoning & Posing & Purchases & Sitting      \\  
				\hline Ours (Rot reg only) & 93.363       & 123.65       & 106.23      & 112.77      & 107.8  & 117.01        & 100.16    & 109.77   \\  
				Ours (Heatmap only)   & 110.39	    & 139.43      & 97.369      & 123.94      & 115.50  & 141.85  & 117.73   & 150.85   \\  
				Ours (Rot reg + HM)    & {\bf 80.59} & {\bf 89.89} & {\bf 74.64} & {\bf 83.96} & {\bf 78.87}   & 102.04  & {\bf 76.94}    & {\bf 81.53}   \\  
				Ours (All)  & 63.33  & 71.59  & 61.39 & 70.40 & 69.90 &  83.17 & 62.98  & 68.77    \\  
				\hline   & SittingDown   & Smoking     & Photo       & Waiting     & WalkDog    & Walking     & WalkPair    & Average \\ 
				\hline Ours (Rot reg only) & 144.27        & 174.95     & 107.13      & 99.808       & 119.94    & 97.891       &  71.699     & 112.43  \\  
				Ours (Heatmap only)   & 131.60        & 187.74      & 115.70      & 121.84      & 144.03     & 122.24      & 107.96      & 128.55 \\  
				Ours (Rot reg + HM)    & {\bf 100.30}  & 132.74      & {\bf 82.24} & {\bf 80.27} & 96.23      & {\bf 77.30} & {\bf 68.20} & {\bf 87.05} \\ 
				Ours (All)  & 76.81 & 98.90 & 68.24 & 67.45 & 73.74 & 57.72 & 57.13 & 69.95  \\  
				\hline 
			\end{tabular}
		}
	\end{center}
\vspace{-10pt}
\end{table}
\end{comment}







\begin{figure}[tb]
	\centering
	\includegraphics[width=1\linewidth]{result_mesh2.jpg}
	\caption{Some results on in-the-wild images. }
	\label{fig:result_mesh}
\end{figure}

\subsection{Results}
\subsubsection{Qualitative results}

In Figs. \ref{fig:result_mesh} and \ref{fig:result_mesh2}, we show the result of our 3D pose prediction method. Our method is able to predict 3D joint positions and bone orientations reasonably accurately even on in-the wild-images. Because our method can predict bone rotations of human body skeletons, we can produce mesh animations from 3D joint positions and rotations using linear blend skinning. Note that the rotations of hands and feet are not supervised.     





\subsubsection{Comparisons to other state-of-the-art}

We have compared our techniques against other state-of-the-art in Table \ref{tab:comparison1}. Our technique is comparable with other state-of-the-art in terms of MPJPE.  Volumetric heatamaps \cite{PavlakosZDD16} can achieve MPJPE approx. 71 mm. However, it got worse results when including MPII (MPJPE 78 mm) with their decoupled structure, whereas we are around MPJPE 70 mm. Also, compared with \cite{PavlakosZDD16} with two stacks of hourglass networks, cross heatmap is more compact, which requires 1/32 of memory spaces to store. 

Compared with the previous techniques that predict angular poses \cite{zhou2016deep,hmrKanazawa17}, SkeletonNet is more accurate. In fact, MPJPE of our result is , whereas that of Kanazawa et al. \cite{hmrKanazawa17} is . In Table \ref{tab:recon_err}, we also compared the reconstruction error with previous approaches. Our technique outperforms previous techniques that iteratively optimizes joint angles \cite{bogo2016keep} and perform regression of joint angles \cite{pavlakos2018humanshape}. The benefit of SkeletonNet is, in addition to estimating 3D joint positions relatively accurately, we can predict 3D bone rotations, which is useful in animating a human body mesh or possibly predicting dynamics such as joint torques.  
















\begin{table}[hbt]
\footnotesize
\begin{center}
\caption{Comparisons to other state-of-the-art. MPJPE [mm] is used for error metric. }
\label{tab:comparison1}
\scalebox{0.85}{
\begin{tabular}{c c c c c c c c c}
\hline & Directions & Discussion & Eating & Greeting & Phoning & Photo & Posing & Purchases \\ 
\hline Zhou et al. \cite{zhou2016deep}    & 91.8        &  102.4      & 97.0        &   98.8      & 113.4     & 90.0        &  93.8    & 132.2  \\  
Tome et al. \cite{tome2017lifting} & 64.98 & 73.47 &  76.82 & 86.43 & 86.28 & 110.67  & 68.93 & 74.79 \\    
Mehta et al. \cite{mehta2016monocular} &  59.69  & 69.74  & 60.55 & 68.77 &  76.36 & 85.42 & 59.05 &  75.04\\      
Pavlakos et al. \cite{PavlakosZDD16} &  67.38 & 71.95 & 66.70 &   69.07 & 71.95 &  76.97 & 65.03 & 68.30 \\   
Ours (All)  & 63.33  & 71.59  & 61.39 & 70.40 & 69.90 &  83.17 & 62.98  & 68.77    \\  



\hline & Sitting & SittingDown & Smoking & Waiting & WalkDog & Walking & WalkPair & Average \\ 
\hline Zhou et al. \cite{zhou2016deep}   &  159.0        &  106.9 & 125.2 & 94.4 & 79.0 & 126.0  & 99.0  & 107.3 \\  
Tome et al. \cite{tome2017lifting} & 110.19  & 172.91  & 84.95 & 85.78 & 86.26 & 71.36 & 73.14 & 88.39 \\      
Mehta et al. \cite{mehta2016monocular} & 96.19 & 122.92 &  70.82 &  68.45 & 54.41 & 82.03  & 59.79 & 74.14\\       
Pavlakos et al. \cite{PavlakosZDD16} & 83.66 & 96.51 &  71.74 & 65.83  & 74.89 & 59.11 & 63.24  & 71.90 \\        
Ours (All)  & 76.81 & 98.90 & 68.24 & 67.45 & 73.74 & 57.72 & 57.13 & 69.95  \\  
\hline 
\end{tabular}
}
\end{center}
\end{table}


\begin{table}[hbt]
	\footnotesize
	\begin{center}
		\caption{Comparison of reconstruction errors on Human 3.6M dataset. }
		\label{tab:recon_err}
		\scalebox{0.85}{
			\begin{tabular}{c c c c c c c c c}
				\hline Zhou et al. \cite{zhou2016sparseness} ~&~  Bogo et al. \cite{bogo2016keep} ~&~ Lassener et al. \cite{Lassner:UP:2017}~ &~ Pavlakos et al. \cite{pavlakos2018humanshape}~ & ~Ours \\ 
				\hline 
				106.7 & 82.3 & 80.7 & 75.9 & 61.4 \\
				\hline   
			\end{tabular}
		}
	\end{center}
\end{table}



\begin{table}[hbt]
	\begin{center}
		\caption{Comparisons between baselines. MPJPE [mm] is used for error metric. }
		\label{tab:comparison2}
		
		\begin{tabular}{c c c c }
			\hline 
			Rot reg only \;\;\;& Heatmap only \;\;\; &	Rot reg + HM \;\;\; & All \\ 
			\hline  112.43 \;\;\;   & 128.55 \;\;\;  & 87.05 \;\;\; & 69.95 \\ 
			\hline 
		\end{tabular}
		
	\end{center}
\end{table}

\subsubsection{Comparisons between baselines}

In  Table \ref{tab:comparison2} and Fig. \ref{fig:Ablation_study}, we show comparisons between the baselines. As can be seen from Fig. \ref{fig:Ablation_study} a, the result of bone rotation regressor preserves skeletal structure, but the joint positions are not accurate enough. With only heatmaps, however, skeletal structure is sometimes destructed e.g., by the left and right flips. By combining our bone rotation and cross heatmap regressor, a more accurate result is produced, while preserving skeletal structure. Note that in previous work this kind of confusions are remedied by incorporating recurrence \cite{belagiannis2016recurrent} or using many stacks of a hourglass module \cite{newell2016stacked}. By training with both Human 3.6M and MPII, we get the best result (Table \ref{tab:comparison2}). In addition, we found that annotating 3D rotations is important for reconstructing human poses from in-the-wild images (Fig. \ref{fig:Ablation_study} b). Thus, the key to our improvements in MPJPE is the use of cross heatmap and the use of MPII dataset in training. Even when MPII dataset is not provided for training, SkeletonNet can predict reasonably accurately 3D human pose by exploiting the combination of skeletal structure and heatmaps. 







\subsubsection{Comparisons between rotation representation}
We have also compared the results of bone rotation regressor by changing its rotation representation. Specifically, we tested the network that 1) regressess axis angles but indirectly supervised with rotation matrices (AA), 2) regressess axis angles but supervised with relative joint rotation matrices and converts them back to the absolute space using forward kinematics, which is equivalent as SMPL \cite{bogo2016keep}  (FKAA), 3) regresses absolute rotations (AbsRotReg), 4) regresses rotations without the GS layer (w/o GS), 5) regresses a global rotation (GlobalReg), 6) classifies a global rotation (GlobalClass) and 7) is same as GlobalClass but aligns rotations with heatmaps (All), respectively. The networks are trained with Human 3.6M, except for All that was trained with both MPII and Human 3.6M. To compare the rotation prediction accuracy, we compute relative rotations between the ground truth and predicted bone rotations, convert them to axis angles and take the norms in degrees, which reflects all three DoFs of rotations.


In Table  \ref{tab:cmp_rotation}, Global Rot. Err. indicates the error of global rotations. Bone Rot. Err. indicates the average error of bone rotations relative to the root. As shown in Table \ref{tab:cmp_rotation}, the proposed method based on the GS layer, which  classifies a global rotation, is the best in terms of MPJPE accuracy. AbsRotReg is also high in accuracy but it produces bone rotations with its determinant of -1, which collapse skeletal structure. The method based on axis angle tends to produce large errors probably because of their high non-linearity, requiring an iterative process \cite{hmrKanazawa17} or a more informative geometric loss, e.g., the one using differences between silhouettes \cite{pavlakos2018humanshape}. In summary, our method can benefit from the use of  rotation matrices, which can probably be modeled as simpler functions than other angle representations, which is more friendly to ConvNets to learn with. As reported in \cite{zhou2016deep}, we could not train a network properly using Euler angles as supervisions, where training and validation losses remained high. In contrast, SkeletonNet can model subtle pose appearances due to e.g., medial and lateral rotations around segment axes by providing supervisions on both rotations and positions. With joint position supervisions only and no rotational supervisions, it is possible to get reasonable results in joint position predictions \cite{zhou2016deep} but is difficult to obtain good results for bone orientations.



\begin{table}[hbt]
	\footnotesize
	\begin{center}
		\caption{Comparison of rotation representation. }
		\label{tab:cmp_rotation}
		\scalebox{0.8}{
			\begin{tabular}{c c c c c c c c c}
				\hline ~&~ ~&~AA ~&~ FKAA ~&~AbsRotReg ~&~ w/o GS ~&~ GlobalReg ~ & ~ GlobalClass ~ & ~ All \\ 
				\hline 
				& MPJPE            & 175.06 & 197.44 & 114.70 & 124.06 & 119.18 & 112.42 & 69.95 \\
				& Global Rot. Err. &  30.46 & 35.81  & 18.83  & 21.28  & 21.64  & 20.81  & 12.93 \\
				& Bone Rot. Err.   & 37.34 & 44.77  & ---    & 28.72  & 29.08  & 29.46  & 21.94 \\
				\hline   
			\end{tabular}
		}
	\end{center}
\end{table}

\begin{comment}
\begin{table}[hbt]
	\footnotesize
	\begin{center}
		\caption{Results on MPII validation dataset. PCK@0.5 is used for the error measure. }
		\label{tab:PCK}
		\scalebox{0.9}{
			\begin{tabular}{c c c c c c c c c}
				\hline & Head & Sho. & Elbow & Wrist & Hip & Knee & Ankle & Mean \\ 
				\hline 
				Pavlakos et al. (4 stacks) \cite{PavlakosZDD16} &  95.86& 93.05  &	85.73  & 80.42 & 81.29 & 79.31 & 75.41 & 84.96 \\   
				Ours ( 2 stacks)  & 93.86 & 89.2 & 71.18  & 79.92  & 74.49  & 72.15 & 79.85 & 79.99 \\
				\hline   
			\end{tabular}
		}
	\end{center}
\end{table}

\end{comment}					











\begin{figure}[tb]
	\centering
	\includegraphics[width=1\linewidth]{ablation_study2.jpg}
	\caption{Comparisons of baselines. a) Bone rotation regressor preserves skeletal structure but the joint positions are not accurate enough. With only heatmaps, skeletal structure is sometimes destructed e.g., with the left and right flips. By combining both, it produces more accurate result while preserving structure. b) The rotation annotation is important for reconstructing a pose from in the wild images. c) With the proposed Gram Schmidt (GS) orthogonalization layer, undesirable deformations such as shears and scalings are removed. }
	\label{fig:Ablation_study}
\end{figure}



\subsubsection{Failure cases and limitations}
In Fig. \ref{fig:failure}, we show failure cases. Our technique fails when there are large self-occlusions and occlusions by objects or other humans. In addition, our network are currently designed for the single-person detection and thus fails when multiple humans exist in the image. Since we scale a skeleton, we are not be able to model absolute bone lengths. Cross heatmap regressor possesses the ability to alter relative bone lengths but our method have generalization issues when the body type is extremely different from the original skeleton, e.g., prediction of small children's poses. Also our network does not take into account hand and foot orientations.



    

\begin{figure}[tb]
	\centering
	\includegraphics[width=1\linewidth]{failure_case.jpg}
	\caption{Failure cases.}
	\label{fig:failure}
\end{figure}

