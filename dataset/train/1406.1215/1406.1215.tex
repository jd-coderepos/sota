\documentclass[conference,letterpaper,10pt]{IEEEtran}



\usepackage[numbers]{natbib}




\usepackage{graphicx}
\RequirePackage[usenames,dvipsnames]{xcolor}

\usepackage[cmex10]{amsmath}
\usepackage{amsfonts}
\interdisplaylinepenalty=2500

\usepackage{amsthm}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}


\usepackage[section]{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{listings}






\usepackage{multicol}
\usepackage{multirow}


\usepackage[font={small,sf},
    labelfont=bf,
    format=hang,    
    format=plain,
    margin=0pt,
    width=0.8\textwidth,
]{caption}
\usepackage[list=true]{subcaption}




\usepackage{url}
\usepackage{hyperref}
\hypersetup{
	colorlinks,
	linktocpage=true,
	linkcolor={red!50!black},
	citecolor={blue!50!black},
	urlcolor={MidnightBlue}
}



\usepackage{xspace}
\newcommand{\Figure}{Fig.\xspace}




\usepackage{cancel}
\renewcommand{\CancelColor}{\color{red}}
\usepackage{soul}
\setstcolor{red}

\definecolor{gray}{rgb}{0.5,0.5,0.5}
\newcommand{\hide}[1]{}
\newcommand{\ignore}[1]{}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\gray}[1]{\textcolor{gray}{#1}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\listspace}{\setlength{\itemsep}{0pt}\setlength{\parskip}{2pt}\setlength{\topsep}{0pt}}

\newcommand{\given}{\mbox{ }|\mbox{ }}



\newcommand{\comment}[1]{{\color{gray} #1}}

\newcommand{\instruction}[1]{\vspace{0.05in}\noindent {\color{blue} {\textbf{Instruction:}} #1} \vspace{0.15in}}

\newcommand{\info}[1]{\vspace{0.05in}\noindent {\color{red} #1} \vspace{0.05in}}

\newcommand{\sinfo}[1]{\vspace{0.2in}\noindent {\color{red} #1} \vspace{0.1in}}




\newcommand{\overbar}[1]{\mkern 1.5mu\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}




\begin{document}

\title{Parallel Algorithms for Generating Random Networks with Given Degree Sequences}

\author{
\IEEEauthorblockN{Maksudul Alam\IEEEauthorrefmark{1}\IEEEauthorrefmark{2} and
Maleq Khan\IEEEauthorrefmark{1}}
\IEEEauthorblockA{
\IEEEauthorblockA{\IEEEauthorrefmark{1}Network Dynamics and Simulation Science Laboratory, Virginia Bioinformatics Institute\\}
\IEEEauthorblockA{\IEEEauthorrefmark{2}Department of Computer Science\\}
Virginia Tech, Blacksburg, Virginia 24061 USA \\
Email: \texttt{\{maksud, maleq\}@vbi.vt.edu}}
}

\date{1 March 2014}

\maketitle

\begin{abstract}
Random networks are widely used for modeling and analyzing complex processes. 
Many mathematical models have been proposed to capture diverse real-world networks. 
One of the most important aspects of these models is degree distribution. 
Chung--Lu (CL) model is a random network model, which can produce networks with any given arbitrary degree distribution. 
The complex systems we deal with nowadays are growing larger and more diverse than ever.
Generating random networks with any given degree distribution consisting of billions of nodes and edges or more has become a necessity, which requires efficient and parallel algorithms. 
We present an MPI-based distributed memory parallel algorithm for generating massive random networks using CL model, which takes  time with high probability and  space per processor, where , , and  are the number of nodes, edges and processors, respectively. The time efficiency is achieved by using a novel load-balancing algorithm. Our algorithms scale very well to a large number of processors and can generate massive power--law networks with one billion nodes and  billion edges in one minute using  processors.
\keywords{massive networks, parallel algorithms, network generator}
\end{abstract}

\begin{IEEEkeywords}
massive networks, parallel algorithms, network generator
\end{IEEEkeywords}





\IEEEpeerreviewmaketitle

\section{Introduction}
The advancements of modern technologies are causing a rapid growth of  complex systems. 
These systems, such as the Internet \cite{Siganos2003}, biological networks \cite{Girvan2002}, social networks \cite{Yang2011,Yang2012}, and various infrastructure networks \cite{Latora2005,Chassin2005}  are sometimes modeled by random graphs for the purpose of studying their behavior.
The study of these complex systems have significantly increased the interest in various random graph models such as Erd\H{o}s--R\'enyi (ER) \cite{Erdos1960}, small-world \cite{Watts1998}, Barab\'{a}si--Albert (BA) \cite{Barabasi1999}, Chung-Lu (CL) \cite{Chung2002}, HOT \cite{Carlson1999}, exponential random graph (ERGM) \cite{Robins2007}, recursive matrix (R-MAT)\cite{Chakrabarti2004}, and stochastic Kronecker graph (SKG) \cite{Leskovec2007,Leskovec2010} models. Among those models, the SKG model has been included in Graph500 supercomputer benchmark \cite{Graph500} due to its simple parallel implementation. The CL model exhibits the similar properties of the SKG model and further has the ability to generate a wider range of degree distributions \cite{Pinar2012}. To the best of our knowledge, there is no parallel algorithm for the CL model.

Analyzing a very large complex system requires generating massive random networks efficiently. As the interactions in a larger network lead to complex collective behavior, a smaller network may not exhibit the same behavior, even if both networks are generated using the same model. In \cite{Leskovec2008}, by experimental analysis, it was shown that the structure of larger networks is fundamentally different from small networks and many patterns emerge only in massive datasets.
Demand for large random networks necessitates efficient algorithms to generate such networks. However, even efficient sequential algorithms for generating such graphs were nonexistent until recently. 
Sequential algorithms are sometimes acceptable in network analysis with tens of thousands of nodes, but they are not appropriate for generating large graphs \cite{Batagelj2005}.
Although, recently some efficient sequential algorithms have been developed \cite{Chakrabarti2004,Batagelj2005,Leskovec2007,Miller2011}, these algorithms can generate networks with only millions of nodes in a reasonable time. But, generating networks with billions of nodes can take an undesirably longer amount of time. Further, a large memory requirement may even prohibit generating such large networks using these sequential algorithms. Thus, distributed-memory parallel algorithms are desirable in dealing with these problems. Shared-memory parallel algorithms also suffer from the memory restriction as these algorithms use the memory of a single machine. Also, most shared-memory systems are limited to only a few parallel processors whereas distributed-memory parallel systems are available with hundreds or thousands of processors.


In this paper, we present a time-efficient  MPI--based distributed memory parallel algorithm for generating random networks from a given sequence of expected degrees using the CL model. To the best of our knowledge, it is the first parallel algorithm for the CL model. The most challenging part of this algorithm is load-balancing. Partitioning the nodes with a balanced computational load is a non trivial problem. In a sequential setting, many algorithms for the load-balancing problem were studied \cite{Manne1995,Olstad1995,Pinar2004}. 
Some of them are exact and some are approximate. 
These algorithms uses many different techniques such as heuristic, iterative refinement, dynamic programming, and parametric search. All of these algorithms require at least  time, where ,  are the number of nodes and processors respectively.  To the best of our knowledge, there is no parallel algorithm for this problem. In this paper, we present a novel and efficient parallel algorithm for computing the balanced partitions in  time. The parallel algorithm for load balancing can be of independent interest and probably could be used in many other problems. Using this load balancing algorithm, the parallel algorithm for the CL model takes an overall runtime of  {w.h.p.}. The algorithm requires  space per processor. Our algorithm scales very well to a large number of processors and can generate a power-law networks with a billion nodes and  billion edges in memory in less than a minute using  processors.
The rest of the paper is organized as follows. 
In Section~\ref{Section:PCL:chung-lu-model} we describe the problem and the efficient sequential algorithm. 
In Section~\ref{Section:TimeEfficient}, we present the parallel algorithm along with analysis of partitioning and load balancing. 
Experimental results showing the performance of our parallel algorithms are presented in Section~\ref{Section:PCL:exp}. We conclude in Section~\ref{Section:PCL:conclusion}.

\section{Preliminaries and Notations} \label{Section:PCL:prelim}
In the rest of the paper we use the following notations. We denote a network by , where  and  are the sets of vertices (nodes) and edges, respectively, with  edges and  vertices labeled as . We use the terms \emph{node} and \emph{vertex} interchangeably. 

We develop parallel algorithm for the message passing interface (MPI) based distributed memory system, where the processors do not have any shared memory and each processor has its own local memory. The processors can exchange data and communicate with each other by exchanging messages. The processors have a shared file system and they read-write data files from the same external memory. However, such reading and writing of the files are done independently.

We use ,  and  to denote thousands, millions and billions, respectively; e.g.,  stands for two billion.


\section{Chung--Lu Model and Efficient Sequential Algorithm}
\label{Section:PCL:chung-lu-model}
Chung--Lu (CL) model \cite{Chung2002} generates random networks from a given sequence of expected degrees. We are given  nodes and a set of non-negative weights  assuming , where  \cite{Chung2002}. For every pair of nodes  and , edge  is added to the graph with probability .
If no self loop is allowed, i.e., , the expected degree of node  is given by . For massive graphs, where  is very large, the average degree converges to , thus  represents the expected degree of node  \cite{Miller2011}.

The na\"{\i}ve algorithm of CL model for an undirected graph with  nodes takes each of the  possible node pairs  and creates the edge with probability , therefore requiring  time. An  algorithm was proposed in \cite{Miller2011}  to generate networks assuming  is sorted in non-increasing order, where  is the number of edges. It is easy to see that  is the best possible runtime to generate  edges. The algorithm is based on the edge skipping technique introduced in \cite{Batagelj2005} for Erd\H{o}s--R\'enyi model. Adaptation of that technique leads to the efficient sequential algorithm in \cite{Miller2011}. The pseudocode of the algorithm is given in Algorithm~\ref{Algorithm:PCL:scl}, consisting of two procedures \Call{Serial--CL}{} and \Call{Create--Edges}{}. Note that we restructured Algorithm~\ref{Algorithm:PCL:scl} by defining procedure \Call{Create--Edges}{} to use it without any changes later in our parallel algorithm. Below we provide an overview and a brief description of the algorithm (for complete explanation and correctness see \cite{Miller2011}). 
\begin{algorithm}[t]
\caption{Sequential Chung--Lu Algorithm}
\label{Algorithm:PCL:scl}
\begin{algorithmic}[1]
\Procedure {Serial--CL}{}
	\State{}\label{Line:PCL:SCL:S}
	\State{ \Call{Create--Edges}{, , }}\label{Line:PCL:SCL:CreateEdges}
\EndProcedure
\Procedure{Create--Edges}{, , }
	\State{}
	\ForAll {  }\label{Line:PCL:SCL:foralli}
		\State \label{Line:PCL:SCL:j}, {}
		\While{ and }\label{Line:PCL:SCL:while}
		    \If{}
			    \State{choose a random }
			    \State{}\label{Line:PCL:SCL:delta}		
			\Else
				\State{}
		    \EndIf
		    \State{}\label{Line:PCL:SCL:v} \Comment{ skip  edges}
		    \If{}
		        \State{ }\label{Line:PCL:SCL:q}
		        \State{choose a random }
		        \If{} 
			        \State{}\label{Line:PCL:SCL:E}
	            \EndIf
	            \State{,\quad}\label{Line:PCL:SCL:jv1}
		    \EndIf
		\EndWhile	
	\EndFor
	\State{\Return {}}
\EndProcedure
\end{algorithmic}
\end{algorithm}

The algorithm starts at  \Call{Serial--CL}{}, which computes the sum  and calls procedure \Call{Create--Edges}{}, where  is the entire set of nodes. For each node , the algorithm selects some random nodes  from , and creates the edges . A na\"{\i}ve way to select the nodes  from  is: for each , select  independently with probability , leading to an algorithm with run time . Instead, the algorithm skips the nodes that are not selected by a random skip length  as follows. 
For each  (Line~\ref{Line:PCL:SCL:foralli}),
the algorithm starts with  and computes a random skip length , where  is a real number in  chosen uniformly at random and . Then node  is selected by skipping the next  nodes (Line~\ref{Line:PCL:SCL:v}), and edge  is selected with probability , where  (Line~\ref{Line:PCL:SCL:q}--\ref{Line:PCL:SCL:E}). Then from the next node , this cycle of skipping and selecting edges is repeated (while loop in Line~\ref{Line:PCL:SCL:while}--\ref{Line:PCL:SCL:jv1}). 
As we always have  and no edge  can be selected more than once, this algorithm does not create any self-loop or parallel edges.
As the set of weights  is sorted in non-increasing order, for any node , the probability  decreases monotonically with the increase of . It is shown in \cite{Miller2011} that for any , edge  is included in  with probability exactly , as desired, and that the algorithm runs in  time.

\section{Parallel Algorithm for the CL Model}
\label{Section:TimeEfficient}
Next we present our distributed memory parallel algorithm for the CL model.  Although our algorithm generates undirected edges, for the ease of discussion we consider  as the \emph{source node} and  as the \emph{destination node} for any edge  generated by the procedure \Call{Create--Edges}{}. 
Let  be the task of generating the edges from source node  (Lines~\ref{Line:PCL:SCL:foralli}--\ref{Line:PCL:SCL:jv1} in Algorithm~\ref{Algorithm:PCL:scl}). It is easy to see that for any  tasks  and  are independent, i.e., tasks  and  can be executed independently by two different processors. Now execution of procedure \Call{Create--Edges}{} is equivalent to executing the set of tasks .  Efficient parallelization of Algorithm~\ref{Algorithm:PCL:scl} requires:

\begin{itemize}
\item Computing the sum  in parallel
\item Dividing the  task of executing \Call{Create--Edges}{} into independent subtasks
\item Accurately estimating the computational cost for each task
\item Balancing load among the processors 
\end{itemize}

To compute the sum  efficiently, a parallel sum operation is performed on   using  processors, which takes  time. To divide the task of executing procedure \Call{Create--Edges}{} into independent subtasks, the set of nodes  is divided into  disjoint subsets ; that is, , such that for any ,  and . Then  is assigned to processor , and  execute the tasks ; that is,  executes \Call{Create--Edges}{}. 

Estimating and balancing computational loads accurately are the most challenging tasks. To achieve good speedup of the parallel algorithm, both tasks must also be done in parallel, which is a non-trivial problem. A good load balancing is achieved by properly partitioning the set of nodes  such that the computational loads are equally distributed among the processors. We use two classes of partitioning schemes named consecutive partitioning (CP) and round-robin partitioning (RRP). In CP scheme consecutive nodes are assigned to each partition, whereas in RRP scheme nodes are assigned to the partitions in a round-robin fashion. 
The use of various partitioning schemes is not only interesting for understanding the performance of the algorithm, but also useful in analyzing the generated networks. It is sometimes desirable to generate networks on the fly and analyze it without performing disk I/O. Different partitioning schemes can be useful for different network analysis algorithms. 
Many network analysis algorithms require partitioning the graph into an equal number of nodes (or edges) per processor. Some algorithms also require the consecutive nodes to be stored in the same processor.
Before discussing the partitioning schemes in detail, we describe some formulations that are applicable to all of these schemes.

Let  be the expected number of edges produced and  be the computational cost in task  for a \textit{source node} . For the sake of simplicity, we assign one unit of  time to process a node or an edge. With , we have:


For two nodes  such that , we have  (see Lemma~\ref{lemma:eugeqev} in Appendix~\ref{Section:Appendix}). The expected number of edges generated by the tasks  is given by . Note that the expected number of edges in the generated graph, i.e., the expected total number of edges generated by all processors is .
The computational cost for processor  is given by:

Therefore, the total cost for all processors is given by:


\subsection{Consecutive Partitioning (CP)}
\label{Section:PCL:consecutive-partitioning}
Let partition  starts at node  and ends at node , where  and , i.e.,  for all . We say  is the \textit{lower boundary} of partition .  
A na\"{\i}ve way for partitioning  is where each partition consists of an equal number of nodes, i.e.,  for all . To keep the discussion neat, we simply use . 
Although the number of nodes in each partition is equal, the computational cost among the processors is very imbalanced. For two consecutive partitions  and ,  for all  and the difference is at least , where , the average weight (degree) of the nodes in  (see  Lemma~\ref{lemma:unp} in Appendix~\ref{Section:Appendix}). Thus  gradually decreases with  by a large amount leading to a very imbalanced distribution of the computational cost. 

To demonstrate that na\"{\i}ve CP scheme leads to imbalanced distribution of computational cost, we generated two networks, both with one billion nodes: ) Erd\H{o}s--R\'enyi network with an average degree of , and ) Power--Law network with an average degree of . We used  processors, which is good enough for this experiment.
\begin{figure*}[t]
\centering
{\includegraphics[width=\textwidth]{Fig1}}
\caption{Computational cost and runtime in na\"{\i}ve CP scheme}
\label{Figure:PCL:UNP-LB}
\end{figure*}
\Figure~\ref{Figure:PCL:UNP-LB} shows the computational cost and runtime per processor. In both cases, the cost is not balanced. For power-law network the imbalance of computational cost is more prominent. Observe that the runtime is almost directly proportional to the cost, which justifies our choice of cost function. That is balancing the cost would also balance the runtime.

We need to find the partitions  such that each partition has equal cost, i.e.,  , where  is the average cost per processor. We refer such partitioning scheme as uniform cost partitioning (UCP).
Although determining the partition boundaries in the na\"{\i}ve scheme is very easy, finding the boundaries in UCP scheme is a non trivial problem and requires: (i) computing the cost  for each node  and (ii) finding the boundaries of the partitions such that every partition has a cost of . 
Na\"{\i}vely computing costs for all nodes takes  as each node independently requires  time using Equation~\ref{Equation:PCL:cu} and \ref{Equation:PCL:eu}. A trivial parallelization achieves  time. 
However, our goal is to parallelize the computation of the costs in  time.

Finding the partition boundaries such that the maximum cost of a partition is minimized is a well-known problem named \textit{chains-on-chains partitioning} (CCP) problem \cite{Pinar2004}. In CCP, a sequence of  separators are determined to divide a chain of  tasks with associated non-negative weights () into  partitions so that the maximum cost in the partitions is minimized.
Sequential algorithms for CCP are studied quite extensively \cite{Manne1995,Olstad1995,Pinar2004}. Since these algorithms take at least  time, using any of these sequential algorithms to find the partitions, along with the parallel algorithm for the CL model, does not scale well. To the best of our knowledge, there is no parallel algorithm for CCP problem. We present a novel parallel algorithm for determining the partition boundaries which takes  time in the worst case.
\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{UEP-Partitions}
\caption{Uniform cost partitioning (UCP) scheme}
\label{Figure:PCL:uep-partitions}
\end{figure*}

To determine the partition boundaries, instead of using  directly, we use the cumulative cost . We call a partition  a \textit{balanced partition} if the computational cost of  is 
. 
Also note that for lower boundary  of partition  we have,  for . Thus, we have:

In other words, a node  with cumulative cost  belongs to partition  such that . The partition scheme is shown visually in \Figure~\ref{Figure:PCL:uep-partitions}.


\begin{algorithm}[t]
\caption{Uniform Consecutive Partition}
\label{Algorithm:PCL:UCP}
\begin{algorithmic}[1]
    \Procedure{\textbf{UCP}}{, , }
    \State{\Call{Calc--Cost}{, , }}
    \State{\Call{Make--Partition}{, , }}\label{Line:PCL:UCP:MakePartition}
    \EndProcedure
    \vspace{0.5em}
    \Procedure{\textbf{Calc--Cost}}{, , }
    \State{ processor id}
    \State{}\label{Line:PCL:UCP:si}
    \State{\textbf{In Parallel:}  }
    \label{Line:PCL:UCP:Si}
    \State{}\label{Line:PCL:UCP:u}
    \State{}
    \State{}\label{Line:PCL:UCP:Cu}
    \For{ to }\label{Line:PCL:UCP:foru1}
    \State{ }\label{Line:PCL:UCP:sigmau}
    \State{}\label{Line:PCL:UCP:eu}
    \State{}\label{Line:PCL:UCP:Cu1}
    \EndFor
    \State{}\label{Line:PCL:UCP:zi}
    
    \State{\textbf{In Parallel:}  }\label{Line:PCL:UCP:Zi}
    \For{ to }\label{Line:PCL:UCP:foru2}
    \State{}\label{Line:PCL:UCP:Cu2}
    \EndFor
    \EndProcedure
		\Procedure{\textbf{Make--Partition}}{, , }
		\State{\textbf{In Parallel:}  }
		\label{Line:PCL:UCP:Z}
		\State{}\label{Line:PCL:UCP:Zbar}
		\State{\Call{Find--Boundaries}{}\label{Line:PCL:UCP:CallBoundary}}
		\ForAll{}\label{Line:PCL:UCP:fornk}
		\State{Send   to  and }
		\EndFor
		
		\State{Receive boundaries  and }\label{Line:PCL:UCP:MSG}
		
		\State{\Return{}}
\EndProcedure
\vspace{0.5em}
\Procedure{\textbf{Find--Boundaries}}{, , , }
		\If{}
		\Return \label{Line:28}
\EndIf
		\State{\label{Line:29}}
		\If{\label{Line:30}}		
		\State{\label{Line:31}}
		\EndIf
		\State{\Call{Find--Boundaries}{}\label{Line:32}}
		\State{\Call{Find--Boundaries}{}\label{Line:33}}
\EndProcedure
\end{algorithmic}
\end{algorithm}

\textbf{Computing  in Parallel}. Computing  has two difficulties: i) for a node , computing  by using Equation~\ref{Equation:PCL:eu} and \ref{Equation:PCL:cu} directly is inefficient and ii)  is dependent on , which is hard to parallelize. To overcome the first difficulty, we use the following form of  to calculate . From Equation~\ref{Equation:PCL:eu} we have:

Therefore,  can be computed by successively updating .

To deal with the second difficulty, we compute  in several steps using procedure \Call{Calc--Cost}{} as shown in Algorithm~\ref{Algorithm:PCL:UCP} (see \Figure~\ref{Figure:PCL:uep-scheme} in Appendix~\ref{Section:Appendix} for a visual representation of the algorithm). In each processor, the partitioning algorithm starts with procedure \Call{UCP}{} that calculates the cumulative costs using procedure  \Call{Calc--Cost}{}. Then procedure \Call{Make--Partition}{} is used to compute the partitioning boundaries.
At the beginning of the \Call{Calc--Cost}{} procedure, the task of computing costs for the  nodes are distributed among the  processors equally, i.e., processor  is responsible for computing costs for the nodes  from  to . Note that these are the nodes that processor  works with while executing the partitioning algorithm to find the boundaries of the partitions. 

In Step \textbf{1} (Line~\ref{Line:PCL:UCP:si}),  computes a partial sum  independently of other processors. In Step~\textbf{2} (Line~\ref{Line:PCL:UCP:Si}),  \emph{exclusive prefix sum}  is calculated for all  where  and . This exclusive prefix sum can be computed in parallel in  time \cite{Sanders2006}. We have:

In Step~\textbf{3},  partially computes , where . By assigning ,   is determined partially using Equation~\ref{Equation:PCL:expected-edges} in constant time (Line~\ref{Line:PCL:UCP:Cu}). For each , values of  ,  and  are also determined in constant time (Line~\ref{Line:PCL:UCP:foru1}--\ref{Line:PCL:UCP:Cu1}), where .  
After Step~\textbf{3}, we have . To get the final value of , the value   needs to be added. For a processor , let . In Step~\textbf{4} (Line~\ref{Line:PCL:UCP:Zi}), another exclusive parallel prefix sum operation is performed on  so that  Note that  is exactly the value required to get the final cumulative cost . In Step~\textbf{5} (Lines~\ref{Line:PCL:UCP:foru2}--\ref{Line:PCL:UCP:Cu2}),  is added to  for . 

\vspace{0.5em}
\noindent\textbf{Finding Partition Boundaries in Parallel}. The partition boundaries are determined using Equation~\ref{Equation:PCL:Partition-Boundary}. The  procedure \Call{Make--Partition}{} generates the partition boundaries. In Line~\ref{Line:PCL:UCP:Z}, parallel sum is performed on  to determine , the total cost and  be the average cost per processor (Line~\ref{Line:PCL:UCP:Zbar}). 
\Call{Find--Boundaries}{} is called to determine the boundaries (Line~\ref{Line:PCL:UCP:CallBoundary}).
From  Equation~\ref{Equation:PCL:Partition-Boundary} it is easy to show that a partition boundary is found between two consecutive nodes  and , such that . Node  is the lower boundary of partition , where .
 executes \Call{Find--Boundaries}{} from nodes  to . 
\Call{Find--Boundaries}{} is a divide \& conquer based algorithm to find all the boundaries in that range efficiently using the cumulative costs . All the found boundaries are stored in a local list. In Line~\ref{Line:28}, it is determined whether the range contains any boundary. If the range does not have any boundary, i.e., if , the algorithm returns immediately. Otherwise, it determines the middle of the range  in Line~\ref{Line:29}. In Line~\ref{Line:30}, the existence of a boundary between  and  is evaluated. If  is indeed a lower partition boundary, it is stored in local list in Line~\ref{Line:31}. In Line~\ref{Line:32} and \ref{Line:33}, \Call{Find--Boundaries}{} is called with the ranges  and  respectively.
Note that the range  may contain none, one or more boundaries. Let  be the set of those boundaries. 
Once the set of boundaries , for all , are determined, the processors exchange these boundaries with each other as follows. Node , in some , is the boundary between the partitions  and , i.e.,  is the upper boundary of , and  is the lower boundary of . In  Line~\ref{Line:PCL:UCP:fornk}, for each  in the range , processor  sends a boundary message containing  to processors  and . Notice that each processor  receives exactly two boundary messages from other processors (Line~\ref{Line:PCL:UCP:MSG}), and these two messages determine the lower and upper boundary of the -th partition . That is, now each processor  has  partition  and is ready to execute the parallel algorithm for the CL model with UCP scheme.

The runtime of parallel Algorithm~\ref{Algorithm:PCL:UCP}  is  as shown in Theorem \ref{thm:ucptime}.

\begin{theorem} \label{thm:ucptime}
The parallel algorithm for determining the partition boundaries of the UCP scheme runs in  time, where  and  are the number of nodes and processors, respectively.
\end{theorem}
\begin{proof}
The parallel algorithm for determining the partition boundaries is shown in  Algorithm~\ref{Algorithm:PCL:UCP}. 
For each processor, Line~\ref{Line:PCL:UCP:si} takes  time. The exclusive parallel prefix sum operation requires  time in Line~\ref{Line:PCL:UCP:Si}. Lines~\ref{Line:PCL:UCP:u}--\ref{Line:PCL:UCP:Cu} take constant time. The for loop at Line~\ref{Line:PCL:UCP:foru1} iterates  times. Each execution of the for loop takes constant time for Lines~\ref{Line:PCL:UCP:sigmau}--\ref{Line:PCL:UCP:Cu1}. Hence, the for loop at Line~\ref{Line:PCL:UCP:foru1} takes  time. The prefix sum in Line~\ref{Line:PCL:UCP:Zi} takes  time. The for loop at Line~\ref{Line:PCL:UCP:foru2} takes  time.

The parallel sum operation in Line~\ref{Line:PCL:UCP:Z} takes  time using \texttt{MPI\_Reduce} function.
For each processor , 's are determined in \Call{Find--Boundaries}{} on the range of . Finding a single partition boundary on these  nodes require  time.
If the range contains  partition boundaries, then it takes   time.
For each partition boundary , processor  sends exactly two messages to the processors  and . Thus each processor receives exactly two messages. There are at most  boundaries in , . Thus, in the worst case, a processor may need to send at most  messages, which takes  time. Therefore, the total time in the worst case is .
\qed
\end{proof}

\begin{figure}[b]
\centering
{\includegraphics[trim=0.6cm 0.6cm 1.1cm 0.65cm,width=\columnwidth]{Partition-Boundary}}
\caption{Maximum number of boundaries in a single processor}
\label{Figure:PCL:FirstBlock}
\end{figure}

Theorem \ref{thm:ucptime} shows the worst case runtime of .
Notice that this bound on time is obtained considering the case that all  partition boundaries  can be in a single processor. However, in most real-world networks, it is an unlikely event, especially when the number of processors  is large. Thus it is safe to say that for most practical cases, this algorithm will scale to a larger number of processors than the runtime analysis suggests. Now we experimentally show the number of partition boundaries found in the first partition for some popular networks.
For the ER networks, the maximum number of boundaries in a processor is  , regardless of the number of processors. Even for the power--law networks, which has very skewed degree distribution, the maximum number of boundaries in a single processor is very small.
\Figure~\ref{Figure:PCL:FirstBlock} shows the maximum number of boundaries found in a single processor. Two fitted plots of  and  is added in the figure for comparison. From the trend, it appears the maximum number of partition boundaries in a processor is somewhere between  and . Since power--law has one of the most skewed degree distribution among real-world networks, we can expect the runtime to find partition boundaries to be approximately  time.


Using the UCP  scheme, our parallel algorithm for generating random networks with the CL model runs in   time as shown in Theorem~\ref{thm:chunglutime}. To prove Theorem~\ref{thm:chunglutime}, we need a bound on computation cost which is shown in Theorem~\ref{thm:comp_load}.

\begin{theorem}\label{thm:comp_load}
The computational cost in each processor is   w.h.p.	
\end{theorem}
\begin{proof}
For each   and ,   is a potential edge in processor , and  creates the edge with probability   where . Let  be the number of potential edges in , and these potential edges are denoted by   (in any arbitrary order). Let   be an indicator random variable such that   if   creates   and   otherwise. Then the number of edges created by   is . 

As discussed in Section~\ref{Section:PCL:chung-lu-model}, generating the edges efficiently by applying the edge skipping technique is stochastically equivalent to generating each edge  independently with probability . Let  be the event that edge  is generated. Regardless of the occurrence of any event  with , we always have . Thus, the events  for all edges  are mutually independent. Following the definitions and formalism given in Section~\ref{Section:PCL:consecutive-partitioning}, we have the expected number of edges created by , denoted by , as 

Now we use the following standard Chernoff  bound for independent indicator random variables and for any , 
 
Using this Chernoff  bound with , we have 
 for any . We assume   and consequently   for all . Now using the union bound, 

for all   simultaneously. 
Then with probability at least , the computation cost  is bounded by . By
construction of the partitions by our algorithm, we have . Thus the computation cost in all processors is  w.h.p.
\qed
\end{proof}

\begin{theorem}\label{thm:chunglutime}
Our parallel algorithm with UCP  scheme for generating random networks with the CL model runs in   time w.h.p.
\end{theorem}

\begin{proof}
Computing the sum   in parallel takes   time. Using the UCP  scheme, node partitioning takes  time (Theorem~\ref{thm:ucptime}). In the UCP  scheme, each partition has   computation cost w.h.p. (Theorem~\ref{thm:comp_load}). Thus creating edges using procedure \Call{Create--Edges}{} requires   time, and the total time is  w.h.p.
\qed
\end{proof}

\subsection{Round-Robin Partitioning (RRP)}
\label{Section:PCL:RRP}
In RRP scheme nodes are distributed in a round robin fashion.  Partition  has the nodes  such that ; i.e., . In other words node  is assigned to . The number of nodes in each partition is almost equal, either  or . 

In order to compare the computational cost, consider two partitions  and  with . Now, for the -th nodes in these two partitions,  we have:    as  (see Lemma~\ref{lemma:eugeqev}). Therefore,  and by the definition of RRP scheme,  The difference in cost between any two partitions is at most , the maximum weight (see Lemma~\ref{lemma:rrp} in Appendix~\ref{Section:Appendix}). Thus RRP scheme provides quite good load balancing. However, it is not as good as the UCP scheme. It is easy to see that in the RRP scheme, for any two partitions  and  such that , we have . But, by design, the UCP scheme makes the partition such that cost are equally distributed among the processors. 
Furthermore, although the RRP scheme is simple to implement and provides quite good load balancing, it has another subtle problem. In this scheme, the nodes of a partition are not consecutive and are scattered in the entire range leading to some serious efficiency issues in accessing these nodes. One major issue is that the locality of reference is not maintained leading to a very high rate of cache miss during the execution of the algorithm. This contrast of performance between UCP and RRP is even more prominent when the goal is to generate massive networks as shown by experimental results in Section~\ref{Section:PCL:exp}.



\section{Experimental Results}
\label{Section:PCL:exp}
In this section, we experimentally show the accuracy and performance of our algorithm. The accuracy of our parallel algorithms is demonstrated by showing that the generated degree distributions closely match the input degree distribution. The strong scaling of our algorithm shows that it scales very well to a large number of processors. We also present experimental results showing the impact of the partitioning schemes on load balancing and performance of the algorithm.

\begin{table*}[t]
\caption{Networks used in the experiments}
\label{Table:Networks}
\centering
\begin{tabular}{|l|l|l|l|}
\hline
Network  & Type & Nodes & Edges \\ 
\hline
PL & Power Law Network &  1\textbf{B} & 249\textbf{B}\\
ER &  Erd\H{o}s--R\'enyi Network &  1\textbf{M} & 200\textbf{M}\\
Miami \cite{Barrett2009} &  Contact Network &  2.1\textbf{M} & 51.4\textbf{B}\\
Twitter \cite{Yang2011} &  Real--World Social Network &  41.65\textbf{M} & 1.37\textbf{B}\\
Friendster \cite{Yang2012} &  Real--World Social Network &  65.61\textbf{M} & 1.81\textbf{B}\\
\hline
\end{tabular}
\end{table*}

\begin{figure*}[t]
\centering
{\includegraphics[width=\textwidth]{Fig4}}
\caption{Degree distributions of input and generated degree sequences}
\label{Figure:PCL:DegreeDistributions}
\end{figure*}

\begin{figure*}[t]
\centering
{\includegraphics[width=\textwidth]{Fig5}}
\caption{Comparison of partitioning schemes}
\label{Figure:PCL:PartitioningSchemes}
\end{figure*}

\begin{figure*}[t]
\centering
{\includegraphics[width=\textwidth]{Fig6}}
\caption{Strong and weak scaling of the parallel algorithms}
\label{Figure:PCL:cl-strongscaling}
\end{figure*}

\textbf{Experimental Setup.}
We used a -node  HPC cluster for the experiments. Each node is powered by two octa-core  SandyBridge E5-2670 2.60GHz (3.3GHz Turbo) processors with  GB memory. The algorithm is developed with MPICH2 (v1.7), optimized for QLogic InfiniBand cards. In the experiments, degree distributions of real-world and artificial random networks were considered. The list of networks is shown in Table~\ref{Table:Networks}. The runtime does not include the I/O time to write the graph into the disk.

\textbf{Degree Distribution of Generated Networks.}
\Figure~\ref{Figure:PCL:DegreeDistributions} shows the input and generated  degree distributions for PL, Miami, and Twitter networks (see Appendix~\ref{Section:OtherNetworks} for other networks).  As observed from the plots, the generated degree distributions closely follow the input degree distributions reassuring that our parallel algorithms generate random networks with given expected degree sequences accurately.

\textbf{Effect of Partitioning Schemes.} As discussed in Section~\ref{Section:PCL:consecutive-partitioning}, partitioning significantly affects load balancing and performance of the algorithm. We demonstrate the effects of  the partitioning schemes in terms of computing time in each processor as shown in  \Figure~\ref{Figure:PCL:PartitioningSchemes} using ER, Twitter, and PL networks.
Computational time fo na\"{\i}ve scheme  is skewed. For all the networks, the computational times for UCP and RRP stay almost constant in all processors, indicating good load-balancing.
RRP is little slower than UCP because the locality of references is not maintained in RRP, leading to high cache miss as discussed in Section~\ref{Section:PCL:RRP}.

\iffalse
\textbf{Memory Requirement.}
Table~\ref{Table:MemoryReq} shows the memory required to generate a power--law network with B nodes and B edges with  processors and the Twitter network. As expected, our space-efficient algorithm requires  times less memory than the time-efficient algorithm to store the given degree sequence. Time-efficient algorithm also requires more memory for computing the partition boundaries.

\begin{table}[h]
\caption{Required memory (in MB) for parallel algorithm}
\label{Table:MemoryReq}
\centering
\begin{tabular}{@{}llll@{}}
\hline
Network  & Algorithm & Weight  & Computation \\ 
\hline
\multirow{2}{*}{Power--Law} & Space Efficient & 1.98 &  0.93\\
& Time Efficient &  1,907.36 &  7.48\\
\hline
\multirow{2}{*}{Twitter} & Space Efficient & 0.077 &  0.9\\
& Time Efficient &  79.44 &  0.31\\
\hline
\end{tabular}
\end{table}
\fi

\textbf{Strong and Weak Scaling.}
Strong scaling of a parallel algorithm shows it's performance with the increasing number of processors while keeping the problem size fixed. \Figure~\ref{Figure:PCL:cl-strongscaling} shows the speedup of na\"{\i}ve, UCP, and RRP partitioning schemes using PL  and Twitter networks. Speedups are measured as , where  and  are the running time of the sequential and the parallel algorithm, respectively. The number of processors were varied from  to . As \Figure~\ref{Figure:PCL:cl-strongscaling} shows, UCP and RRP achieve excellent linear speedups. Na\"{\i}ve scheme performs the worst as expected. The speedup of PL is greater than that of Twitter network. As Twitter is smaller than the PL network, the impact of the parallel communication overheads is higher contributing to decreased speedup. Still the algorithm to generate Twitter network has a speedup of  using  processors. 

The weak scaling measures the performance of a parallel algorithm when the input size per processor remains constant. For this experiment, we varied the number of processors from  to . For  processors, a PL network with  nodes and  edges is generated. Note that weak scaling can only be performed on artificial networks. \Figure~\ref{Figure:PCL:cl-strongscaling}(c) shows the weak scaling for UCP and RRP schemes using PL networks. Both RRP and UCP show very good weak scaling with almost constant runtime.

\textbf{Generating Large Networks.}
The primary objective of the parallel algorithm is to generate massive random networks. Using the algorithm with UCP scheme,  we have generated power law networks with one billion nodes and  billion edges in one minute using  processors with a speedup of about .

\section{Conclusion}
\label{Section:PCL:conclusion}
We have developed an efficient parallel algorithm for generating massive networks with a given degree sequence using the Chung--Lu model. The main challenge in developing this algorithm is load balancing. To overcome this challenge, we have developed a novel parallel algorithm for balancing computational loads that results in a significant improvement in efficiency. We believe that the presented parallel algorithm for the Chung--Lu model will prove useful for modeling and analyzing emerging massive complex systems and uncovering patterns that emerges only in massive networks. As the algorithm can generate networks from any given degree sequence, its application will encompass a wide range of complex systems.


\bibliographystyle{IEEEtranN}
\bibliography{references}

 \section{Appendix}
 \label{Section:Appendix}
 
 \begin{figure*}[ht!]
 \centering
{\includegraphics[width=\textwidth]{UEP-Algorithm}}
 \caption{Steps for determining cumulative cost in UCP}
 \label{Figure:PCL:uep-scheme}
 \end{figure*}
 
 \begin{figure*}[ht!]
 \centering
{\includegraphics[width=\textwidth]{Fig4-A}}
 \caption{Input and generated degree distributions for other networks}
 \label{Figure:PCL:OtherNetworks}
 \end{figure*}
 
\begin{lemma}
\label{lemma:eugeqev}
For any two nodes  such that , .
\end{lemma}
\begin{proof}
Proof omitted. The lemma follows immediately from Equation~\ref{Equation:PCL:cu} and the fact that, the weights are sorted in non-increasing order. 
\qed
\end{proof}


\begin{lemma}
\label{lemma:unp}
Let  be the computational cost for partition . In the na\"{\i}ve partitioning scheme, we have , where , the average weight of the nodes in .
\end{lemma}

\begin{proof}
In the na\"{\i}ve partitioning scheme, each of the partitions has  nodes, except the last partition which can have smaller than  nodes. For the ease of discussion, assume that for ,  and consequently . Now, .  Using Equation \ref{Equation:PCL:pcost}, we have


\qed
\end{proof}

\begin{lemma}
\label{lemma:rrp}
In Round Robin Partitioning (RRP) scheme, for any , we have .
\end{lemma}

\begin{proof}
The difference in  cost between two partitions  and  is given by:

\qed
\end{proof}

\subsection{Visual Representation of Computing Cost in UCP}
\label{Figure:Schematic Diagram}

\Figure~\ref{Figure:PCL:uep-scheme} shows the visual representation of \Call{Calc-Cost}{} procedure of Algorithm~\ref{Algorithm:PCL:UCP}.



\subsection{Other Networks}
\label{Section:OtherNetworks}

\Figure~\ref{Figure:PCL:OtherNetworks} shows input and generated degree distributions for ER and Friendster networks as shown in Table~\ref{Table:Networks}.



\end{document}
