\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{multirow}
\usepackage{iccv}
\usepackage[dvipsnames]{xcolor}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[toc,page]{appendix}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{float}

\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage[export]{adjustbox}
\usepackage{booktabs}
\usepackage{url}
\usepackage[autostyle]{csquotes}
\usepackage{epigraph}
\usepackage{graphicx}
\usepackage{pifont}\usepackage{authblk}
\usepackage[bb=boondox]{mathalfa}
\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}
\newcommand\scalemath[2]{\scalebox{#1}{\mbox{\ensuremath{\displaystyle #2}}}}
\newcommand{\mat}[1]{\textcolor{magenta}{[Matt: #1]}}
\newcommand{\remy}[1]{\textcolor{blue}{[R: #1]}}
\newcommand{\alex}[1]{\textcolor{red}{[A: #1]}}
\newcommand{\rdiff}[2]{\textcolor{Sepia}{[A: #1]}\textcolor{blue}{[R: #2]}}
\newcommand{\adiff}[2]{\textcolor{blue}{[R: #1]}\textcolor{Sepia}{[A: #2]}}

\SetKwInput{KwInput}{Input}                \SetKwInput{KwData}{Data}                \SetKwInput{KwParams}{Parameters}                \SetKwInput{KwOutput}{Output}              \SetKwBlock{KwRepeat}{Repeat}              



\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\iccvfinalcopy \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}



\begin{document}

\title{MixMo: Mixing Multiple Inputs for Multiple Outputs via Deep Subnetworks}
\author[1]{Alexandre Ramé\thanks{Equal contribution.}\thanks{Correspondence to alexandre.rame@lip6.fr}}
\author[1,2]{Rémy Sun\samethanks[1]}
\author[1,3]{Matthieu Cord}
\affil[1]{Sorbonne Université}
\affil[2]{Optronics \& Missile Electronics, Land \& Air Systems, Thales}
\affil[3]{Valeo.ai}
\renewcommand\Authands{ and }
\newcommand\blfootnote[1]{\begingroup
  \renewcommand\thefootnote{}\footnote{#1}\addtocounter{footnote}{-1}\endgroup
}
\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand\mycommfont[1]{\footnotesize\ttfamily\textcolor{blue}{#1}}
\maketitle
\ificcvfinal\thispagestyle{empty}\fi
\begin{abstract}Recent strategies achieved ensembling ``for free'' by fitting concurrently diverse
subnetworks inside a single base network. The main idea during training is that
each subnetwork learns to classify only one of the multiple inputs
simultaneously provided.
However, the question of how to best mix these multiple inputs has not been studied so far.

In this paper, we introduce MixMo, a new generalized framework for learning
multi-input multi-output deep subnetworks. Our key motivation is to replace the
suboptimal summing operation hidden in previous approaches by a more appropriate
mixing mechanism. For that purpose, we draw inspiration from successful mixed
sample data augmentations.
We show that binary mixing in features - particularly with rectangular patches from CutMix - enhances results by making subnetworks stronger and more diverse.

We improve state of the art for image classification on CIFAR-100 and Tiny ImageNet datasets.
Our easy to implement models notably outperform data augmented deep ensembles, without the inference and memory overheads.
As we operate in features and simply better leverage the expressiveness of large networks, we open a new line of research complementary to previous works.%
 \end{abstract}\section{Introduction}

Convolutional Neural Networks (CNNs) have shown exceptional performance in
computer vision tasks, notably classification \cite{krizhevsky2009learning}.
However, among other limitations, obtaining reliable predictions remains
challenging \cite{hendrycks2018benchmarking,ovadia2019can}. For additional
robustness in real-world scenarios or to win Kaggle competitions, CNNs usually
pair up with two practical strategies: data augmentation and ensembling.

Data augmentation reduces overfitting and improves generalization, notably by
diversifying training samples \cite{gontijo2020affinity}. Traditional image
augmentations are label-preserving: \textit{e.g} flipping, cropping, etc
\cite{simard1993efficient}. However, recent \textbf{mixed sample data
augmentation} (MSDA) alters labels: multiple inputs and their labels are mixed
proportionally to a ratio  to create artificial samples. The seminal
work Mixup \cite{zhang2018mixup} linearly interpolates pixels while Manifold
Mixup \cite{manifoldmixup19} interpolates latent features in the network.
Binary masking MSDAs \cite{french2020milking,harris2020mix,kim2020puzzle} such as
CutMix \cite{yun2019cutmix} have since diversified mixed samples by pasting patches from one image onto another in place of interpolation.
\begin{figure}\centering \includegraphics[width=\linewidth]{pictures/model/files/mixmo_intro.png}\caption{\textbf{MixMo overview}. We embed  inputs into a shared space with convolutional layers. The key point of our framework is the subsequent mixing block. Mixing with patches performs better than basic summing:  vs.\  (MIMO \cite{havasi2020raining}) on CIFAR-100 with WRN-28-10.}\label{fig:intro}\end{figure}% 

Aggregating diverse predictions from several neural networks significantly improves
generalization
\cite{dietterich2000ensemble,hansen1990neural,lakshminarayanan2016simple,wolpert1992stacked},
notably uncertainty estimation
\cite{ashukha2020pitfalls,gustafsson2020evaluating,ovadia2019can}.
An ensemble of several small networks usually performs
better than one large network empirically \cite{chirkova2020deep,lobacheva2020power}.
Yet, unfortunately, ensembling is costly in time and memory both at training and inference: this often limits applicability.

In this paper, we propose \textbf{MixMo}, a new generalized multi-input multi-output
framework. To tackle these overheads found in traditional ensembling, we fit  independent
subnetworks within a single base network
\cite{gao2019ntraensemble,havasi2020raining,soflaei2020aggregated}. This is
possible as networks only leverage a subset of their weights. For example,
``winning tickets'' \cite{lottery2019} subnetworks perform similarly to the full
network. Rather than pruning inactive filters
\cite{e9ee2143e19d49cf9cbe8861950b6b2a,li2017pruning,malach2020proving}, we seek
to fully use the available neural units and over parameterization.

The challenge is to prevent homogenization and enforce diversity among
subnetworks with no structural differences. Thus, we consider  (input,
label) pairs simultaneously in training: . \textbf{ images are treated simultaneously}, as shown on Fig.~\ref{fig:intro} with . The  inputs are encoded by  separate convolutional layers  into a shared latent space before being mixed. The representation is
then fed to the core network, which finally branches out into  dense layers .
Diverse subnetworks naturally emerge as  learns to classify  from
input . At inference, the same image is repeated  times: we obtain
ensembling ``for free'' by averaging  predictions.


The key divergent point between MixMo variants lies in the \textbf{multi-input
mixing block}.
Should the merging be a basic summation, we would recover an equivalent
formulation to MIMO \cite{havasi2020raining} (which first featured this multi-input
multi-output strategy).
Contrarily to classical fusion mechanisms that detect tensors' intercorrelations
\cite{ben2017mutan,cadene2019murel}, we seek features independence. This makes
summing suboptimal.

Our main intuition is simple: we see summing as a balanced and restrictive form
of Mixup \cite{zhang2018mixup} where . By analogy,
we draw from the considerable MSDA literature to design a more
appropriate mixing block. In particular, we leverage binary masking methods to
ensure subnetworks diversity. Our framework allows us to create a
new Cut-MixMo variant inspired by CutMix \cite{yun2019cutmix}, and illustrated in
Fig.~\ref{fig:intro}: a patch of features from the first input is pasted into
the features from the second input.


This asymmetrical mixing also raises new questions regarding information flow in
the network's features. We tackle the imbalance between the multiple
classification training tasks via a new weighting scheme.
Conversely, MixMo's double nature as a
new mixing augmentation in features yields important insights on traditional MSDA.

In summary, our contributions are threefold:\begin{enumerate}
\item We propose a general framework, MixMo, connecting two successful fields:
mixing samples data augmentations  multi-input multi-output ensembling.
\item We identify the appropriate mixing block to best tackle
  the diversity/individual accuracy trade-off in subnetworks: our easy to implement Cut-MixMo benefits from the synergy between CutMix and ensembling.\item We design a new weighting of the loss components to properly leverage the asymmetrical inputs mixing.
\end{enumerate}

\begin{figure}[!t]\centering \includegraphics[width=1.0\linewidth]{pictures/experiments/files/fig_main_4.jpg}\caption{\textbf{Main results}. CIFAR-100 with WRN-28-. Our Cut-MixMo variant (patch mixing and ) surpasses CutMix and deep ensembles (with half the parameters) by leveraging over-parameterization in wide networks.}\label{fig:main}\end{figure}%
 We demonstrate excellent accuracy and uncertainty estimation with MixMo on CIFAR-10/100 and Tiny ImageNet.
Specifically, Cut-MixMo with  reaches state of the art on these standard datasets: as exhibited by Fig.~\ref{fig:main}, it outperforms CutMix, MIMO and deep ensembles, at (almost) the same inference cost as a single network.%
 \section{Related work}


\subsection{Data augmentation}

\paragraph{}CNNs are known to memorize the training data \cite{45820} and make
overconfident predictions \cite{guo2017calibration} to the detriment of generalization on new test examples. \textbf{Data Augmentation} (DA)
inflates the training dataset's size by creating artificial samples from
available labeled data. Beyond slight perturbations (\textit{e.g.} rotation), recent works \cite{Cubuk_2020_CVPR_Workshops, hendrycks2019augmix} apply stronger transformations \cite{he19_data_augmen_revis}. CutOut \cite{devries2017improved} and others
\cite{li2020encemask,singh2018hide,zhong2020random} randomly delete regions of
images in training. They prevent models from focusing on a single pixels region,
similarly to how regularizations like Dropout
\cite{srivastava2014dropout} or DropBlock \cite{ghiasi2018dropblock} force
networks to leverage multiple features.


\textbf{Mixed Sample Data Augmentation} (MSDA) recently expanded the notion of
DA. From pairs of labeled samples , they
create virtual samples:  where . 
\cite{liang2018understanding} shows that mixing the targets differently than this linear interpolation may cause
underfitting and unstable learning. Indeed, approaches mainly focus on developing the most effective input mixing . In \cite{inoue18_data_augmen_by_pairin_sampl_images_class,Tokozume_2018_CVPR,tokozume2018learning,zhang2018mixup},  performs a simple linear interpolation between pixels: \textit{e.g} in Mixup \cite{zhang2018mixup}, .  Theoretically, it regularizes outside the training distribution \cite{onmixup2020,guo2019mixup,zhang2020does,zhang2021hen} and applies label smoothing \cite{muller2019does,pereyra2017regularizing}.

CutMix draws from Mixup and CutOut
\cite{devries2017improved} by pasting a patch from  onto : 
 where  represents the element-wise
product and  a binary mask with average
value . CutMix randomly samples squares, which often leads to rectangular masks due to boundary effects. Such
\textbf{non-linear binary masking} improves generalization
\cite{summers2019improved,takahashi2019data} by increasing dataset: it creates new images with usually disjoint patches
\cite{harris2020mix}.
\cite{BAEK2021107594,faramarzi2020patchup,lee2020smoothmix} seek more diverse transformations
via arbitrarily shaped masks: proposals range from
cow-spotted masks \cite{french2020milking} to masks with irregular
edges \cite{harris2020mix}.
As masking of discriminative regions may cause label
misallocation \cite{guo2019mixup},
\cite{kim2021comixup,kim2020puzzle,uddin2020saliencymix,walawalkar2020attentive}
try to alleviate this issue with costly saliency heatmaps
\cite{selvaraju2017grad}. Yet, ResizeMix \cite{qin2020resizemix} shows that they
perform no better than random selection of patch locations.


In addition to Manifold Mixup \cite{manifoldmixup19}, only a few works \cite{faramarzi2020patchup,li2020n,yaguchi2019mixfeat,yun2019cutmix} have
tried to mix intermediate \textbf{latent features} as we do.
Our goals and methods are however quite different, as shown later in Section \ref{sec:msdamixmo}.
In brief, they mix deep features to smooth the decision boundaries, while we mix shallow features only so that inputs can remain distinct.

\subsection{Ensembling}

\paragraph{}Like \cite{wen2021combining}, we explore combining DA with another standard technique in machine learning: ensembling \cite{dietterich2000ensemble,hansen1990neural,krogh1995neural}. For improved performances, aggregated members should be both \textit{accurate} and \textit{diverse} \cite{opitz1999popular,perrone1992networks,rame2021dice}.
Deep ensembles \cite{lakshminarayanan2016simple} (DE) simultaneously train multiple networks with different random initializations \cite{kolen1991back} converging towards different explanations for the
training data \cite{fort2019deep,wilson2020bayesian}.


Ensembling's fundamental drawback is the inherent \textbf{computational and memory overhead}, which
increases linearly with the number of members. This bottleneck is typically addressed by sacrificing either \textit{individual} performance or \textit{diversity} in a complex \textit{trade-off}. Averaging predictions from several checkpoints on the training process, \textit{i.e.}\ snapshot ensembles \cite{garipov2018loss,huang2017snapshot,izmailov2018averaging,maddox2019simple,ritter2018scalable}, fails to explore multiple local optima \cite{ashukha2020pitfalls,fort2019deep,wilson2020bayesian}. So does Monte Carlo Dropout \cite{gal2016dropout}. The recent BatchEnsemble
\cite{dusenberry2020efficient}
is parameter-efficient, yet still requires multiple forward passes.
TreeNets \cite{lee2015m,szegedy2015going} by share low-level layers. MotherNets \cite{wasay2018othernets} share first training epochs between members.
However, sharing reduces diversity.




Very recently, the multi-input multi-output MIMO \cite{havasi2020raining} achieves \textbf{ensemble almost ``for free''}: all of the layers except the first convolutional and last dense layers are shared ( \#parameters). \cite{soflaei2020aggregated} motivated a related Aggregated Learning to learn concise representations with arguments from information bottleneck \cite{alemi2016deep,tishby2000information}. The idea is that over-parameterized CNNs \cite{lottery2019,molchanov2016pruning,pensia2020ptimal} can fit multiple subnetworks \cite{veit2016residual}. The question is how to prevent homogenization among the simultaneously trained subnetworks.
Facing a similar challenge, \cite{gao2019ntraensemble} includes stochastic channel
recombination; \cite{durasov2020asksembles} relies on predefined binary masks; in GradAug \cite{yang2020radaug}, subnetworks only leverage the first channels up to a given percentage. In contrast, MIMO does not need structural differences among subnetworks: they learn to build their own paths while being as diverse as in DE.% \section{MixMo framework}
We first introduce the main components of our MixMo strategy, summarized in Fig.~\ref{fig:archi_network}: we mix multiple inputs to obtain multiple outputs via subnetworks.
We highlight the key mixing block combining information from inputs, and our training loss based on a dedicated weighting scheme.

We mainly study  subnetworks here, both for clarity and as it empirically
performs best in standard parameterization regimes. For completeness, we
straightforwardly generalize to  in Section \ref{model:mheads}.

\subsection{General overview}
We leverage a training classification dataset  of i.i.d. pairs of associated image/label . We randomly sample a subset of  samples  that we randomly shuffle via permutation . Our training batch is . The loss  is averaged over these  samples: the networks' weights are updated through backpropagation and gradient descent.

Let's focus on the training sample . In MixMo, both inputs are \textbf{separately encoded} (see Fig.~\ref{fig:intro}) into the shared latent space with two different convolutional layers (with  input channels each and no bias term):  via  and  via . To recover a strictly equivalent formulation to MIMO \cite{havasi2020raining}, we simply sum the two encodings: . Indeed, MIMO merges inputs through channel-wise concatenation in pixels: MIMO's first convolutional layer (with  input channels and no bias term) hides the summing operation in the output channels.

Explicitly highlighting the underlying mixing leads us to consider a \textbf{generalized multi-input mixing block} . This manifold mixing presents a unique opportunity to tackle the ensemble diversity/individual accuracy trade-off and to improve overall ensemble results (see Section \ref{section:fusion}). The shared representation  feeds the next convolutional layers. We note  the \textbf{mixing ratio} between inputs.

The core network  handles features that represent both inputs simultaneously. The dense layer  predicts  and targets , while  targets . Thus, the \textbf{training loss} is the sum of two cross-entropies  weighted by parametrized function  (defined in Section \ref{section:weight}) to balance the asymmetry when :At inference, the same input  is repeated twice: the core network  is fed the sum  that preserves maximum information from both encodings. Then, the diverse predictions are averaged: . This allows us to benefit from ensembling in a single forward pass.

\subsection{Mixing block }
\label{section:fusion}
The mixing block  - which combines both inputs into a shared representation - is the cornerstone of MixMo.
Our main intuition was to analyze MIMO as a simplified Mixup variant where the mixing ratio  is fixed to .
MixMo generalized framework encompasses a wider range of variants inspired by MSDA mixing methods. 
Our first main variant - Linear-MixMo - fully extends Mixup. The mixing block is , where ,  and  with  the concentration parameter. The second and more effective variant \textbf{Cut-MixMo} adapts the patch mixing from CutMix:
\noindent where  is a binary mask with area ratio , valued at  either on a rectangle or on the complementary of a rectangle.
In brief, a patch from  is pasted onto , or vice versa.
As shown in Section \ref{expe:mixingmechanism}, Cut-MixMo performs better than other strategies.
Specifically, the binary mixing in Cut-MixMo advantageously replaces the linear interpolation in MIMO and Linear-MixMo: subnetworks are more accurate and more diverse.

First, binary mixing in  trains stronger \textbf{individual}
subnetworks for the same reasons why CutMix improves over Mixup.
In a nutshell, linear MSDAs \cite{manifoldmixup19,zhang2018mixup} produce noisy samples \cite{onmixup2020} that lead to robust representations.
As MixMo tends to distribute different inputs on
non-overlapping channels (as discussed later in Fig.~\ref{fig:mixmo_proj}), this regularization hardly takes place anymore in
.
On the contrary, by masking features,
we simulate common object occlusion problems. This spreads
subnetworks' focus across different locations: the two classifiers are forced to
find information relevant to their assigned input at disjoint locations. This
occlusion remains effective as the receptive field in this first shallow latent
space remains small.

Secondly, linear interpolation is fundamentally ill-suited to induce
diversity as
full information is preserved from both inputs.
CutMix on the other hand explicitly increases dataset diversity by presenting
patches of images that do not normally appear together.
Such benefits can be directly transposed to :
binary mixing with patches increases randomness and \textbf{diversity between the subnetworks}.
Indeed, in a similar spirit to bagging
\cite{breiman1996bagging}, different samples are given to the subnetworks. By
deleting asymmetrical complementary locations from the two inputs, subnetworks
will not rely on the same region and information. Overall, they are less likely
to collapse on close solutions.


\begin{figure}\includegraphics[width=\linewidth]{pictures/model/files/archi_network.png}\caption{\textbf{MixMo learning components}. The asymmetry in mixing influences the balance of the training loss.}\label{fig:archi_network}\end{figure}% \subsection{Loss weighting }\label{section:weight}
Asymmetries in the mixing mechanism can cause one
input to overshadow the other. Notably when , the predominant input may be easier to predict.
We seek a weighting function  to \textbf{balance the relative importance} of the two  in
. This weighting modifies the effective learning rate, how gradients flow in the network and overall how mixed information is represented in features.
In this paper, we propose to weight via the parametrized:This defines a family of functions indexed by the parameter , visualized for  in red on Fig.~\ref{fig:archi_network}. See Appendix \ref{app:weighting} for complementary visualizations. This power law provides a natural relaxation between \textbf{two extreme configurations}. The first extreme, , , is in line with linear label interpolation in MSDA. The resulting imbalance in each subnetwork's contribution to  causes lopsided updates. While it promotes diversity, it also reduces regularization: the overshadowed input has a reduced impact on the loss. The opposite extreme, , , removes reweighting. Consequently,  inflates the importance of hard under-represented inputs, \textit{à la} Focal Loss \cite{lin2017focal}. However, minimizing the role of the predominant inputs destabilizes training. Overall, we empirically observe that moderate values of  perform best as they trade off pros and cons from both extremes.


Interestingly, the proper weighting of loss components is also a central theme in \textbf{multi-task learning} \cite{caruana1997multitask,chen2018gradnorm,kendall2018multi,sener2018multi}. While it aims at predicting several
tasks from a shared input, MixMo predicts a shared task from several different inputs.
Beyond this inverted structure, we have similar issues: \textit{e.g.}\ gradients for one task can be detrimental to another conflicting task. Fortunately, MixMo presents an advantage: the exact ratios  and  of each task are known exactly.% \vspace{-0.5em}
\begin{figure}
\centering
\begin{subfigure}[b]{0.48\columnwidth}
\centering
\includegraphics[width=\columnwidth]{pictures/experiments/files/mimo2810L1}
\caption{Filters -norms of the input encoders  and .}
\label{fig:mixmo_proj}
\end{subfigure}
\hfill \begin{subfigure}[b]{0.48\columnwidth}\centering \includegraphics[width=\columnwidth]{pictures/experiments/files/active}\caption{Proportion of active filters in the core network vs.\ width .}\label{fig:active_feats}\end{subfigure}\caption{\textbf{Influence of MixMo on network utilization}.\linebreak(a) The encoders have separate channels: the two subsequent classifiers can differentiate the two inputs. (b) Less filters are strongly active () in wider networks: Cut-MixMo reduces this negative point.}
\label{fig:proj_model}
\vspace{-0.5em}\end{figure}
%
 \vspace{-0.5em}
\subsection{From manifold mixing to MixMo}
\label{sec:msdamixmo}
We have discussed at length how we extend multi-input multi-output frameworks by
borrowing mixing protocols from MSDA. Now we reversely point out how
our \textbf{MixMo diverges from MSDA schemes}. At first glimpse, the idea is the
same as manifold mixing
\cite{faramarzi2020patchup,li2020n,manifoldmixup19}:  inputs
are encoded into a latent space to be mixed before being fed to the rest of the network.
Yet, while they mix at varying depths, we only mix in the shallowest space. Specifically, we only mix in features - and not in pixels - to allow separate encodings of the inputs: they need to remain distinct in the mixed representation for the subsequent classifiers.

Hence our two key differences: \textit{first}, MixMo uses two separated encoders (one for each input), and \textit{second}, it outputs two predictions instead of a single one. Indeed, MSDAs use a single classifier that targets a unique soft label reflecting the different classes via linear interpolation. MixMo instead chooses to fully leverage the composite nature of mixed samples and trains
\textbf{separated dense layers},  and , ensembled ``for free'' at test
time.

Section \ref{expe:differentencdec} demonstrates that MixMo works because it also
uses two \textbf{different encoders}  and .
While training two classifiers may seem straightforward in MSDA, it actually
raises a troubling question: which input should each classifier predicts ?
Having two encoders provides a simple solution: the network is divided in two subnetworks, one for each input. Their separability is easily observed:
Fig.~\ref{fig:mixmo_proj} shows the -norm of the 16 filters
for the two encoders (WRN-28-10 on CIFAR-100). Each filter norm is far from
zero in only one of the two encoders:  and  do not
overlap much in the mixing space and can be treated differently by the
subsequent layers.

This leads MixMo to use most available filters. Following the
structured pruning literature \cite{li2017pruning}, we consider in
Fig.~\ref{fig:active_feats} that a filter (in a layer of the core network) is
active if its -norm is at least  of the -norm from its layer's
most active filter (see Appendix \ref{app:feature}). This proportion of active
filters decreases with width : vanilla and CutMix networks do not fully use
additional parameters. In contrast, MixMo better leverages over-parameterization
thanks to its two encoders, classifiers and subnetworks.

\subsection{Generalization to  subnetworks}\label{model:mheads}Most of the framework is easily extended by optimizing \linebreak 

with  from a Dirichlet distribution (see Appendix \ref{app:mheads}). The key change is that  now needs to
handle more than 2 inputs: . While linear
interpolation is easily generalized, Cut-MixMo has several possible extensions:
in our experiments, we first linearly interpolate between  inputs and then
patch in a region from the -th.% \section{Experiments}
\label{expe:experiments}We evaluate MixMo efficiency on standard image classification datasets: CIFAR-\{10,100\} \cite{krizhevsky2009learning} and Tiny ImageNet \cite{chrabaszcz2017}. We equally track accuracies (Top\{1,5\}, ) and the \textit{calibrated Negative Log-Likelihood} (NLL,
). Indeed, \cite{ashukha2020pitfalls} shows that we should compare in-domain uncertainty estimations after temperature scaling (TS) \cite{guo2017calibration}: we thus split the test set in two and calibrate (after averaging in ensembles) with the temperature optimized on the other half, as in \cite{lobacheva2020power,rame2021dice}. We nonetheless report NLL (without TS) along with the Expected Calibration Error \cite{naeini2015obtaining} in Appendix \ref{app:expeeval}.\subsection{Implementation details}
We mostly study the Linear-MixMo and Cut-MixMo variants with
. We set \textbf{hyper-parameter}  (see Section~\ref{expe:hyperparamablation}).  performs better than  (see Appendix \ref{app:alpha}). In contrast, MIMO \cite{havasi2020raining} refers to linear summing, like Linear-MixMo, but with
 instead of .

Different mixing methods create a strong \textbf{train-test distribution
gap} \cite{onmixup2020,gontijo2020affinity}. Thus, in Cut-MixMo we actually substitute  for 
with probability  to accommodate for the summing in  at inference. We set the probability of patch mixing during training to , with linear descent to  over the last twelfth of training epochs (see pseudocode \ref{pseudocode} in Appendix).

When MixMo is combined with CutMix in pixels, the inputs may be:  with interpolated targets , where  are randomly sampled and .

MIMO duplicates samples  times via \textbf{batch repetition}:
 will be associated with  and  in the same batch
if . As the batch size remains fixed, the count of unique
samples per batch and the learning rate is divided by . Conversely, the
number of steps is multiplied by . Overall, this stabilizes training but multiplies
its cost by . We thus indicate an estimated (training/inference) overhead (wrt. vanilla training) in the \textit{time} column of our tables. Note that some concurrent approaches also lengthen training: \textit{e.g.}\ GradAug \cite{yang2020radaug} via multiple subnetworks predictions ().

We provide more details in Appendix \ref{app:expe} and will open source our PyTorch \cite{NEURIPS2019_9015} implementation. \subsection{Main results on CIFAR-100 and CIFAR-10}\begin{table}[!t]\caption{\textbf{Main results}: WRN-28-10 on CIFAR. \textbf{Bold} highlights best scores,  marks approaches not re-implemented.}\centering \resizebox{0.86\linewidth}{!}{\begin{tabular}{c | c | c c c | c c}
            \toprule
            \multicolumn{2}{c|}{Dataset} & \multicolumn{3}{c|}{CIFAR-100} & \multicolumn{2}{c}{CIFAR-10} \\
            \midrule
            \midrule
            Approach & \begin{tabular}{@{}c@{}}\scalebox{1.0}{Time}\\\scalebox{1.0}{Tr./Inf.}\end{tabular} & \begin{tabular}{@{}c@{}}\scalebox{1.0}{Top1}\\\scalebox{1.0}{}\end{tabular} & \begin{tabular}{@{}c@{}}\scalebox{1.0}{Top5}\\\scalebox{1.0}{}\end{tabular} & \begin{tabular}{@{}c@{}}\scalebox{1.0}{NLL}\\\scalebox{1.0}{}\end{tabular} & \begin{tabular}{@{}c@{}}\scalebox{1.0}{Top1}\\\scalebox{1.0}{}\end{tabular} & \begin{tabular}{@{}c@{}}\scalebox{1.0}{NLL}\\\scalebox{1.0}{}\end{tabular} \\
            \midrule
            \midrule
            Vanilla & \multirow{6}{*}{1/1} & 81.63 & 95.49 & 73.9 & 96.34 & 12.6\\
            Mixup & & 83.44 & 95.92 & 65.7 & 97.07 & 11.2 \\
            Manifold Mixup & & 81.96 & 95.51 & 73.4 & 97.45 & 12.2 \\
            CutMix & &84.05 & 96.09 & 64.8 & 97.23 & 9.9 \\
ResizeMix & & 84.31 & - & - & 97.60 & -  \\
            \midrule
            Puzzle-Mix & 2/1 & 84.31 & 96.46 & 66.8 &-&-\\
            \midrule
            GradAug & \multirow{2}{*}{3/1} & 84.14 & 96.43 & - & -&- \\
            + CutMix & & 85.51 & 96.86 & - &- & - \\
            \midrule
Mixup BA & 7/1 & 84.30 & - & - & \textbf{97.80} &- \\
            \midrule
            DE (2 Nets) & \multirow{2}{*}{2/2} & 83.17 & 96.37 & 66.4 & 96.67 & 11.1 \\
            + CutMix & & 85.74 & 96.82 & 57.1 & 97.52 & 8.6 \\
            \midrule
            \midrule
            \multirow{1}{*}{MIMO} & \multirow{5}{*}{2/1} & 82.40 & 95.78 & 68.8 & 96.38 & 12.1 \\
            \cmidrule{1-1}
            \cmidrule{3-7}
            \multirow{1}{*}{Linear-MixMo} & & 82.54 & 95.99 & 67.6 & 96.56 & 11.4 \\
            + CutMix & & 84.69 & 97.12 & 57.2 &  97.32 & 9.4 \\
            \cmidrule{1-1}
            \cmidrule{3-7}
            \multirow{1}{*}{Cut-MixMo} & & 84.38 & 96.94 & 56.3 & 97.31 & 8.9 \\
            + CutMix & & 85.18 & 97.20 & 54.5 & 97.45 & 8.4 \\
\midrule
            \multirow{1}{*}{MIMO} & \multirow{5}{*}{4/1} & 83.06 & 96.23 & 66.1 & 96.74 & 11.4 \\
            \cmidrule{1-1}
            \cmidrule{3-7}
            \multirow{1}{*}{Linear-MixMo} & & 83.08 & 96.26 & 65.6 & 96.91 & 10.8 \\
            + CutMix & & 85.47 & 97.04 & 55.8 & 97.68 & 8.7 \\
            \cmidrule{1-1}
            \cmidrule{3-7}
            \multirow{1}{*}{Cut-MixMo} &  & 85.40 & 97.22 & 53.5 & 97.51 & 8.1 \\
            + CutMix & & \textbf{85.77} & \textbf{97.42} & \textbf{52.4} & 97.73 & \textbf{7.9} \\
            \bottomrule \end{tabular}}\label{table:cifar10010}\vspace{-0.2em}\end{table}% Tab.~\ref{table:cifar10010} reports averaged scores over 3 runs for our main experiment on CIFAR with WRN-28-10 \cite{BMVC2016_87}. We re-use the hyper-parameters given in MIMO \cite{havasi2020raining}. Cut-MixMo reaches ( Top1,  NLL) on
CIFAR-100 with : it surpasses our Linear-MixMo (, ) and MIMO (, ). Cut-MixMo sets a new state of the art when combined with CutMix (, ).
Results remain strong when : Cut-MixMo (, ) proves better on its own than traditional DE \cite{lakshminarayanan2016simple} and previous MSDAs like Puzzle-Mix \cite{kim2020puzzle} or CutMix \cite{yun2019cutmix}.
On CIFAR-10, we observe similar trends: Cut-MixMo reaches  in NLL,  with CutMix. Yet, the costlier batch augmented Mixup BA \cite{Hoffer_2020_CVPR} edges it out in Top1.

Fig.~\ref{fig:paramefficient} shows how MixMo grows stronger than DE (green curves) as width  in WRN-28- increases. The parameterization becomes
appropriate at :
Cut-MixMo (yellow curves) then matches DE - with half the parameters - in
Fig.~\ref{fig:widthcx} and its subnetworks match a vanilla
network in Fig.~\ref{fig:indacc}. Beyond, MixMo better uses over-parameterization:
Cut-MixMo+CutMix surpasses DE+CutMix in NLL for , and this is true in Top1 for .
Compared to our strong Linear-MixMo+CutMix (purple curves), Cut-MixMo performs similarly in Top1, and better with CutMix for .
While Linear-MixMo and DE learn from occlusion, Cut-MixMo also benefits from CutMix, notably from the induced label smoothing.
Overall, Cut-MixMo, even without CutMix, significantly better estimates uncertainty.\begin{figure}\centering \begin{subfigure}{.65\linewidth}\centering \includegraphics[width=1\linewidth]{pictures/experiments/files/widthfull_onlybest_2.jpg}\caption{Ensemble Top1 and NLL.}\label{fig:widthcx}\end{subfigure}\begin{subfigure}{.35\linewidth}\centering \includegraphics[width=1\linewidth]{pictures/experiments/files/widthacc_ind.jpg}\caption{Individual Top1.}\label{fig:indacc}\end{subfigure}\caption{\textbf{Parameters efficiency} (metrics/\#params). CIFAR-100 with WRN-28-, . Comparisons between (a) ensemble and some of their (b) individual counterparts.}\label{fig:paramefficient}\end{figure}% \subsection{MixMo analysis on CIFAR-100 w/ WRN-28-10}\label{expe:componentanalysis}\subsubsection{Training time}\label{expe:trainingcost}We have just seen that CutMix improves Linear-MixMo at varying widths , but not enough to match Cut-MixMo in NLL: CutMix can not fully compensate for the advantages from patch mixing over linear interpolation. We recover this finding in Fig.~\ref{fig:batchrepet}, this time at varying batch repetition  when . Moreover, Cut-MixMo outperforms DE for the \textbf{same training time}. Indeed, MixMo variants trained with a given  matches the training time of DE with  networks. In the rest of this section, we set .\begin{figure}[!h]\centering \includegraphics[width=0.8\linewidth]{pictures/experiments/files/trainingcost.jpg}\vspace{-0.4em}\caption{\textbf{NLL improves with longer training}, via batch repetitions (MixMo) or additional networks (DE).}\label{fig:batchrepet}\vspace{-0.8em}\end{figure}% \subsubsection{The mixing block }\label{expe:mixingmechanism}Tab.~\ref{table:cifar100_othermixingmethod} compares performance for several mixing blocks
\cite{faramarzi2020patchup,harris2020mix,summers2019improved,yun2019cutmix}. No matter the \textbf{shape} (illustrated in Appendix \ref{app:msda}), binary masks perform better than linear mixing: the cow-spotted mask (, ) \cite{french2019semi,french2020milking} notably performs well. The basic CutMix patching (, ) is nevertheless more accurate and was our main focus.\begin{table}[!h]\caption{ inspired by various \textbf{MSDA approaches}.}\centering \vspace{-0.4em}\resizebox{0.95\linewidth}{!}{\begin{tabular}{c | c | c c c c c c}\toprule \begin{tabular}{@{}c@{}}\\approach\end{tabular} & \begin{tabular}{@{}c@{}}Mixup\\\cite{zhang2018mixup}\end{tabular} & \begin{tabular}{@{}c@{}}Horiz.\\Concat.\end{tabular} & \begin{tabular}{@{}c@{}}Vertical\\Concat.\end{tabular}  & \begin{tabular}{@{}c@{}}PatchUp 2D\\\cite{faramarzi2020patchup}\end{tabular} & \begin{tabular}{@{}c@{}}FMix\\\cite{harris2020mix}\end{tabular} & \begin{tabular}{@{}c@{}}CowMask\\\cite{french2019semi,french2020milking}\end{tabular} & \begin{tabular}{@{}c@{}}CutMix\\\cite{yun2019cutmix}\end{tabular}\\\midrule Top1   &82.5 & 82.78 & 84.00 & 84.16 & 83.76 & 84.17 & \textbf{84.38}\\NLL  & 0.676 & 0.627 & 0.573 & 0.581 & 0.602 & \textbf{0.561} & 0.563\\\bottomrule \end{tabular}}\label{table:cifar100_othermixingmethod}\end{table}% 

We further study the impact of patch mixing through the lens of the
\textit{ensemble diversity/individual accuracy trade off}. As in
\cite{rame2021dice}, we measure diversity via the pairwise ratio-error \cite{aksela2003comparison} (, ), defined as the ratio between the number of different errors and simultaneous errors for two predictors. In Fig.~\ref{fig:tradeoff_probcutmix} and \ref{fig:tradeoff_root}, we average metrics over the last 10 epochs.

As argued in Section \ref{section:fusion}, patch mixing increases diversity compared to linear mixing in Fig.~\ref{fig:tradeoff_probcutmix}. As the probability  of patch mixing grows, so does diversity: from  (Linear-MixMo) to  (Cut-MixMo). We provide associated training dynamics in Appendix \ref{app:td}. In contrast, DE have  while MIMO has  on the same setup. Increasing  past  boosts diversity even more at the cost of subnetworks' accuracies: this is due to underfitting and an increased test-train
distribution gap.  is thus the best trade off.\begin{figure}[!h]\centering \includegraphics[width=0.88\linewidth]{pictures/experiments/files/acc_vs_div_vs_probcutmix.jpg}\vspace{-0.5em}\caption{\textbf{Diversity/accuracy} as function of  with .}\label{fig:tradeoff_probcutmix}\vspace{-1em}\end{figure}% \subsubsection{Weighting function }\label{expe:hyperparamablation}
We analyze the impact of the parameter  in the reweighting function .
Higher values tend to remove reweighting, as shown in Appendix \ref{app:weighting}: they
strongly decrease diversity in Fig.~\ref{fig:tradeoff_root}.
The opposite extreme with  increases diversity via lopsided
gradient updates but it degrades accuracy.
We speculate it under-emphasizes hard samples.
The range  strikes a good balance: results remain high and stable.\begin{figure}[!h]\centering \includegraphics[width=0.88\linewidth]{pictures/experiments/files/root.jpg}\vspace{-0.5em}\caption{\textbf{Diversity/accuracy} as function of  with .}\vspace{-1em}\label{fig:tradeoff_root}\end{figure}% \subsubsection{Generalization to  subnetworks}\label{expe:mheads}We try to generalize MixMo to more than  subnetworks in Fig.~\ref{fig:msubnetworks}. Cut-MixMo's subnetworks perform at 
when  vs.\  when . In MIMO,
it's  vs.\ . Because subnetworks do not share features, higher
 degrades their results: only two can fit seamlessly.
Ensemble Top1 overall decreases in spite of the additional predictions, as already noticed in MIMO \cite{havasi2020raining}.\newpage
\begin{figure}[!h]\centering \includegraphics[width=0.77\linewidth]{pictures/experiments/files/fig_msubnetworks.jpg}\vspace{-1.em}\caption{\textbf{Ensemble/individual} accuracies for .}\label{fig:msubnetworks}\end{figure}% This reflects MixMo's strength in
over-parametrized regimes, but also its limitations with fewer parameters
when subnetworks underfit (recall previous Fig.~\ref{fig:paramefficient}).
Facing similar findings, MIMO \cite{havasi2020raining} introduced input
repetition so that subnetworks share their features, at the cost of drastically
reducing diversity. Our generalization may be extended by future approaches whose mixing blocks (perhaps not inspired by MSDA) would tackle these issues.\subsubsection{Multiple encoders and classifiers}
\begin{wraptable}[10]{hR!}{0.40\linewidth}
\centering
\vspace{-1em}
\caption{\textnormal{Number of encoders/classifiers}.}
\vspace{-0.5em}
\resizebox{\linewidth}{!}{\begin{tabular}{cc |c}
            \toprule
            \# Enc. & \# Clas. & NLL \\
            \midrule
            1 & 1 & 0.604 \\
            2 & 1 & 0.666 \\
            1 & 2 & 0.687 \\
            1 & 2 & 0.598 \\
            \midrule
            2 & 2 & \textbf{0.563} \\
            \bottomrule
        \end{tabular}
}
\label{table:cifar100sh}
\end{wraptable} \label{expe:differentencdec}
In Section \ref{sec:msdamixmo}, we compared MixMo and MSDA. Tab.~\ref{table:cifar100sh} confirms the need for \textbf{2 encoders and 2 classifiers}. With 1 classifier and linearly interpolated labels (in the same spirit as \cite{chen2020mclr}), the 2 encoders perform worse than 1 encoder. With 1 shared encoder and 2 classifiers, it is not clear which input each classifier should target. In the first naive , we randomly associate the 2 classifiers and the 2 inputs (encoded with the same encoder). This  variant yields poor results. In , the first classifier tries to predict the label from the predominant input, the second targets the other input:  reaches  vs.\  for Cut-MixMo.\subsection{Robustness to image corruptions}Deep networks' results decrease when facing unfamiliar samples. To measure
robustness to train-test distribution gaps, \cite{hendrycks2018benchmarking}
corrupted CIFAR-100 test images into CIFAR-100-c (more details in Appendix \ref{app:expe}).
As in Puzzle-Mix \cite{kim2020puzzle}, we report WRN-28-10 results with and
without AugMix \cite{hendrycks2019augmix}, a pixels data augmentation technique
specifically introduced for this task. Tab.~\ref{table:cifar100c} shows that Cut-MixMo () best complements AugMix and reaches  Top1.\begin{table}[!b]\vspace{-0.5em}\caption{\textbf{Robustness comparison on CIFAR-100-c}.}\centering \resizebox{1.0\linewidth}{!}{\begin{tabular}{c | c c |c|c c| c c |c |c c |c c}\toprule Approach & \multicolumn{2}{c|}{1 Net.} & CutMix & \multicolumn{2}{c|}{Puzzle-Mix} & \multicolumn{2}{c|}{DE (2 Nets)} & MIMO & \multicolumn{2}{c|}{Linear-MixMo} & \multicolumn{2}{c}{Cut-MixMo} \\AugMix & - &\checkmark & - & - & \checkmark & - & \checkmark & - & - & \checkmark & - & \checkmark \\\midrule Top1  & 52.2 & 67.8 & 51.93 & 58.09 & 70.46 & 53.8 & 69.9 & 53.6 & 55.6 & 70.4 & 57.0 & \textbf{71.1} \\
            Top5  & 73.7 & 87.5 & 72.03 & 77.3 & 87.7 & 74.9 & 88.9 &74.9 & 76.1 & 89.4 & 77.4 & \textbf{89.5} \\
            NLL  & 2.50 & 1.38 & 2.13 & 1.96 & 1.34 & 2.27 & 1.24 & 2.66 & 2.33 & 1.22 & 2.04 & \textbf{1.16} \\\bottomrule \end{tabular}}\label{table:cifar100c}\end{table}% \subsection{Pushing MixMo further: Tiny ImageNet}At a larger scale and with more varied  images, Cut-MixMo reaches a new state of the art of 70.24\% on
Tiny ImageNet \cite{chrabaszcz2017} in Tab.~\ref{table:tiny200}. We re-use the hyper-parameters given in previous state of the art Puzzle-Mix \cite{kim2020puzzle}. With , PreActResNet-18 \cite{preacthe} is not sufficiently parametrized for MixMo's advantages to express themselves on this challenging dataset. MixMo's full potential shines with wider networks: with  and M parameters, Cut-MixMo reaches (, ) vs.\ (, ) for CutMix. Compared to DE with  networks,
Cut-MixMo performs \{worse, similarly, better\} for width . At (almost) the
same numbers of parameters, Cut-MixMo when  performs better
(, ) than DE with 4 networks when 
(, ).\begin{table}[!h]
\caption{\textbf{Results}: PreActResNet-18- on Tiny ImageNet.}
\centering
\resizebox{1.0\linewidth}{!}{\begin{tabular}{c | c | c c | c c | c c }
            \toprule
            \multicolumn{2}{c|}{Width  (\# params)} & \multicolumn{2}{c|}{ (11.2M)} & \multicolumn{2}{c|}{ (44.9M)} & \multicolumn{2}{c}{ (100.5M)} \\
            \midrule
            \midrule
            Approach & \begin{tabular}{@{}c@{}}\scalebox{0.9}{Time}\\\scalebox{.8}{Tr./Inf.}\end{tabular} & \begin{tabular}{@{}c@{}}\scalebox{0.9}{Top1}\\\scalebox{0.8}{}\end{tabular} & \begin{tabular}{@{}c@{}}\scalebox{0.9}{NLL}\\\scalebox{0.8}{}\end{tabular} &
           \begin{tabular}{@{}c@{}}\scalebox{0.9}{Top1}\\\scalebox{0.8}{}\end{tabular} & \begin{tabular}{@{}c@{}}\scalebox{0.9}{NLL}\\\scalebox{0.8}{}\end{tabular} &
           \begin{tabular}{@{}c@{}}\scalebox{0.9}{Top1}\\\scalebox{0.8}{}\end{tabular} & \begin{tabular}{@{}c@{}}\scalebox{0.9}{NLL}\\\scalebox{0.8}{}\end{tabular}
            \\
            \midrule
            \midrule
            Vanilla & \multirow{5}{*}{1/1} & 62.56 & 1.53 & 64.80 & 1.51 & 65.78 & 1.53\\
            Mixup & & 63.74 & 1.62 & 66.62 & 1.50 & 67.27 & 1.51\\
            Manifold Mixup &  & 58.70 & 1.92 & -&- &- &- \\
            Co-Mixup & & 64.15 & - & - & - &- &- \\
            CutMix & & 65.09 & 1.58 & 67.76 & 1.33 & 68.95 & 1.29 \\
            \midrule
Puzzle-Mix & 2/1 & 64.48 & 1.65 &-&- &- &- \\
\midrule
            DE (2 Nets) & 2/2 & 65.53 & 1.39 & 68.06 & 1.37 & 68.38 & 1.36 \\
            DE (3 Nets) & 3/3 & 66.76 & 1.34 & 69.05 & 1.29 & 69.36 & 1.28 \\
            DE (4 Nets) & 4/4 & \textbf{67.51} & \textbf{1.31} & \textbf{69.94} & \textbf{1.24} & 69.72 & 1.26 \\
            \midrule
            \midrule
            \multirow{1}{*}{Linear-MixMo} & \multirow{2}{*}{2/1} & 61.58 & 1.61 & 66.62 & 1.41 & 68.18 & 1.36 \\
\multirow{1}{*}{Cut-MixMo} & & 63.78 & 1.48 & 68.30 & 1.30 & 69.89 & 1.26 \\
            \midrule
            \multirow{1}{*}{Linear-MixMo} & \multirow{2}{*}{4/1} & 62.91 & 1.51 & 67.03 & 1.41 & 68.38 & 1.38\\
\multirow{1}{*}{Cut-MixMo} & & 64.44 & 1.48 & 69.13 & 1.28 & \textbf{70.24} & \textbf{1.19}\\
            \bottomrule \end{tabular}}
\label{table:tiny200}
\end{table}
 \subsection{Ensemble of MixMo}\label{expe:ensemblemixmo}
Since MixMo adds very little parameters (), we can combine independently trained MixMo like in DE. This ensembling of ensemble of subnetworks leads in practice to the
averaging of  predictions.
Fig.~\ref{fig:splitadvantage} compares ensembling for vanilla networks
and Cut-MixMo on CIFAR-100. We first recover the Memory Split Advantage
\cite{chirkova2020deep,lobacheva2020power,wang2020ultiple,zhao2020plitnet} (MSA): at similar parameter counts,  vanilla
WRN-28-3 do better than a single vanilla WRN-28-7 ( in NLL). \textbf{Cut-MixMo challenges this MSA}: we bridge the gap between using one network or several smaller networks ( on same setup). 
Visually, Cut-MixMo's curves remain closer to the lower envelope: performances are less dependent on how the memory budget is split.
This is because Cut-MixMo is effective mainly for larger architectures by better leveraging their parameters.\begin{figure}[!t]\centering \includegraphics[width=1.0\linewidth]{pictures/experiments/files/smapowerlaw.png}\caption{\textbf{Ensemble effectiveness} (NLL/\#params), for different widths  in WRN-28- and numbers of members . Standard data augmentations on CIFAR-100 with . Curves interpolated through power laws \cite{lobacheva2020power}.}\label{fig:splitadvantage}\end{figure}% 

We also recover that wide vanilla networks tend to be less diverse
\cite{neal2018modern}, and thus gain less from ensembling \cite{lobacheva2020power}:  vanilla
WRN-28-14 ( Top1,  NLL) perform not much better than
 WRN-28-7 (, ). Contrarily,
\textbf{Cut-MixMo facilitates the ensembling of large networks} with
(, )  vs.\ (, ) (more comparisons in Appendix \ref{app:ensmixmo}).

When combined with CutMix \cite{yun2019cutmix}, Cut-MixMo previously set a new state of the art of  with  WRN-28-10. Final Tab.~\ref{table:cifar100sota} shows it further reaches  with  and even  with .
\begin{table}[H]
\caption{\textbf{Best results for WRN-28-10 on CIFAR-100} via Cut-MixMo + CutMix \cite{yun2019cutmix} + -ensembling and .\linebreak Recent Top1 SoTAs: 85.23 \cite{qin2020resizemix}, 85.51 \cite{yang2020radaug}, 85.74 \cite{zhao2020plitnet}.}\centering
\resizebox{0.49\textwidth}{!}{\begin{tabular}{c  c | c c c| c c c}
            \toprule
            \multirow{2}{*}{} & \multirow{2}{*}{\# params} & \multicolumn{3}{c|}{Average} & \multicolumn{3}{c}{Best run} \\
            & & Top1  & Top5  & NLL  & Top1  & Top5  & NLL \\
            \midrule
            1 & 36.6M & 85.77 \scalebox{.7}{ 0.14} & 97.36 \scalebox{.7}{ 0.02} & 0.524 \scalebox{.7}{ 0.005} & 85.92 & 97.36 & 0.518\\
            2 & 73.2M &  86.63 \scalebox{.7}{ 0.19} & 97.73 \scalebox{.7}{ 0.05} & 0.479 \scalebox{.7}{ 0.003} & 86.75 & 97.80 & 0.475\\
            3 & 109.8M & 86.81 \scalebox{.7}{ 0.17} & 97.85 \scalebox{.7}{ 0.04} & 0.464 \scalebox{.7}{ 0.002} & 86.94 & 97.83 & 0.464\\
\bottomrule \end{tabular}}
\label{table:cifar100sota}
\end{table}


 %
 \section{Conclusion}In this paper, we introduce MixMo, a framework that generalizes the multi-input multi-output strategy and draws inspiration from recent data augmentations.
MixMo can be analyzed as either an ensembling method or a new mixed samples data
augmentation, while still remaining complementary to works from both lines of
research. We also introduce a new weighting scheme to properly balance our losses for training.
Overall, we demonstrated that different MixMo variants have excellent performance.
In particular, Cut-MixMo with two subnetworks improves the state of the art on CIFAR-100,
CIFAR-100-c and Tiny ImageNet.

MixMo better exploits large networks capacity and thus may become an important tool for real world projects. In future works, applications beyond vision tasks such as natural language processing could certainly be investigated.






 \clearpage
\subsubsection*{Acknowledgments}
This work was granted access to the HPC resources of IDRIS under the allocation 20XX-AD011012262 made by GENCI. We acknowledge the financial support by the ANR agency in the chair VISA-DEEP (project number ANR-20-CHIA-0022-01).
{\small
\bibliographystyle{ieee_fullname}
\bibliography{main}
}

 
\clearpage


\section{Appendix}

The sections in this Appendix follow a similar order to their related sections in the main paper. 
We first illustrate the reweighting of the loss components in Appendix \ref{app:weighting}.
Appendix \ref{app:feature} elaborates on our analysis of filters activity.
Appendix \ref{app:mheads} clarifies our framework generalization with  subnetworks.
We describe in greater details our implementation in Appendix \ref{app:expe}, and then our evaluation setting in \ref{app:expeeval}.
Appendix \ref{app:td} showcases training dynamics.
We provide a quick refresher on common MSDA techniques in Appendix \ref{app:msda}.
Appendix \ref{app:alpha} studies the importance of .
Appendix \ref{app:ensmixmo} analyzes ensembles of Cut-MixMo with CutMix that reach state of the art. 
Finally, we provide a pseudocode in Algorithm \ref{pseudocode}.

\subsection{Weighting function }
\label{app:weighting}
As outlined in Section \ref{section:weight}, the asymmetry in the mixing mechanism leads to
asymmetry in the relative importance of the two inputs.
Thus we reweight the loss components with function , defined as
. It
rescales the mixing ratio   through the use of a  root
operator. In the main paper, we have focused on .

Fig.~\ref{fig:wr_plots} illustrates how  behaves for 
and . The first extreme  matches the diagonal
, without rescaling of , similarly to what is customary in MSDA.
Our experiments in Section \ref{expe:hyperparamablation} justified the initial idea to shift the weighting function closer to the
horizontal and constant curve  with higher .
In the other experiments, we always set .
\begin{figure}[!h]\centering \includegraphics[width=\columnwidth]{pictures/experiments/root_plots/roots_plot}\caption{\textbf{Curves of the reweighting operation} that projects  to the flattened ratio }\label{fig:wr_plots}\end{figure}%  \subsection{Filters activity}\label{app:feature}
We argued in Section \ref{sec:msdamixmo} that MixMo better leverages additional parameters in wider networks.
Concretely, a larger proportion of filters in large
networks really help for classification as demonstrated in
Fig.~\ref{fig:mixmo_proj} and \ref{fig:active_feats} in the main paper.
Following common practices in the structured pruning literature
\cite{li2017pruning}, we used the -norm of convolutional filters as a proxy
for importance. These 3D filters are of shape  with
 the number of input channels and  the kernel size. In
Fig.~\ref{fig:active_feats}, we arbitrarily defined a filter as active if its
-norm is at least  of the highest filter -norm in that
filter's layer. We report the average percentage of active filters across all filters in the core network , for 3 learning strategies: vanilla, CutMix and Cut-MixMo.

The threshold  was chosen for visualization purposes. Nevertheless, the
observed trend in activity proportions remains for varying thresholds in
Tab.~\ref{table:feat_l1}. For example, for the lax , CutMix uses  of filters vs.\  for Cut-MixMo.\begin{table}[!t]
  \caption{\textbf{Proportion (\%) of active filters} in core network vs.\ width  for a WRN-28- on CIFAR 100 and different activity thresholds .}
\centering
\resizebox{0.8\linewidth}{!}{\begin{tabular}{c c | c c c c}
            \toprule
          Method & Width &  &  &  &  \\
          \midrule
          \multirow{7}{*}{Vanilla} & 2 & 98.9 & 98.8 & 97.8 & 93.3 \\
           & 3 & 97.3 & 96.4 & 93.2 & 87.5 \\
           & 4 & 96.5 & 95.2 & 91.2 & 81.6 \\
           & 5 & 95.1 & 91.7 & 85.7 & 73.3 \\
           & 7 & 92.6 & 88.2 & 81.0 & 69.5 \\
           & 10 & 87.8 & 80.4 & 71.5 & 57.3 \\
                 & 14 & 83.9 & 74.0 & 61.6 & 46.8 \\ \midrule
          \multirow{7}{*}{CutMix} & 2 & 99.2 & 99.0 & 97.8 & 95.3 \\
                 & 3 & 98.7 & 98.5 & 97.2 & 93.4 \\
                 & 4 & 98.1 & 97.4 & 94.0 & 87.3 \\
                 & 5 & 97.0 & 96.1 & 90.7 & 80.6 \\
                 & 7 & 95.8 & 94.0 & 86.2 & 74.6 \\
                 & 10 & 93.5 & 88.4 & 81.3 & 67.0 \\
                 & 14 & 89.4 & 81.9 & 70.3 & 50.9 \\ \midrule
          \multirow{7}{*}{Cut-MixMo} & 2 & 100.0 & 100.0 & 99.4 & 97.3 \\
           & 3 & 99.8 & 99.8 & 99.7 & 98.7 \\
           & 4 & 99.7 & 99.7 & 99.6 & 98.7 \\
           & 5 & 99.3 & 99.3 & 98.9 & 97.4 \\
           & 7 & 98.9 & 98.8 & 98.0 & 95.2 \\
           & 10 & 98.5 & 98.2 & 96.8 & 92.4 \\
           & 14 & 97.5 & 96.3 & 93.1 & 82.6 \\  \bottomrule
        \end{tabular}
    }
\label{table:feat_l1}
\end{table}
 %
 \subsection{Generalization to  heads}
\label{app:mheads}

We have mostly discussed our MixMo framework with  subnetworks. For better
readability, we referred to the mixing ratios 
and  with . It's equivalent to a more
generic formulation  from a symmetric Dirichlet distribution with concentration parameter .
This leads to the alternate equations
,\linebreak
where 

Now generalization to the general case  is straightforward. We draw a
tuple  and optimize the
training loss: 

where the new weighting naturally follows:


The remaining point is the generalization of the mixing block ,
that relies on the existence of MSDA methods for  inputs. The linear
interpolation can be easily expanded as in Mixup:
where . However, extensions for other masking MSDAs have only
recently started to emerge \cite{kim2021comixup}. For example, CutMix is not
trivially generalizable to , as the patches could overlap and hide
important semantic components. In our experiments, a soft extension of Cut-MixMo performs best: it first linearly interpolates  inputs and then patches a region from the -th:

where  is a rectangle of area ratio  and  sampled
uniformly in . However, it has been less successful than
, as only two subnetworks can fit independently in standard parameterization regimes. Future work could design new framework components, such as specific mixing blocks, to tackle these limits.% \subsection{Implementation details}
\label{app:expe}

We first used the popular image classification \textbf{datasets} CIFAR-100 and
CIFAR-10 \cite{krizhevsky2009learning}. They contain 60k  natural
and colored images in respectively  classes and  classes, with 50k
training images and 10k test images. At a larger scale, we study Tiny ImageNet
\cite{chrabaszcz2017}, a downsampled version of ImageNet \cite{imagenet_cvpr09}.
It contains  different categories, 100k  training images
(\textit{i.e.}\ 500 images per class) and 10k test images.

Our code was adapted from the official MIMO \cite{havasi2020raining}
implementation\footnote{\url{https://github.com/google/edward2/}}. For CIFAR,
we re-use the \textbf{hyper-parameters} from MIMO \cite{havasi2020raining}. The
optimizer is SGD with learning rate of , batch size , linear warmup over 1 epoch, decay
rate 0.1 at steps ,  regularization 3e-4. We follow
standard MSDA practices \cite{ashukha2020pitfalls,kim2020puzzle,yun2019cutmix} and
set the maximum number of epochs to . For Tiny ImageNet, we adapt
PreActResNet-18-, with  times more filters. We re-use the
hyper-parameters from Puzzle-Mix \cite{kim2020puzzle}. The optimizer is SGD with
learning rate of , batch size , decay rate  at steps
,  epochs maximum, weight decay 1e-4.
Our experiments ran on a single NVIDIA 12Go-TITAN X Pascal GPU.
All results without a  were obtained with these training configurations.
We will soon release our code and pre-trained models to facilitate reproducibility.

\textbf{Batch repetition} increases performances at the cost of \textbf{longer training}, which may be discouraging for some practitioners. Thus in addition to  as in MIMO \cite{havasi2020raining}, we often consider the quicker . Note that most of our concurrent approaches also increase training time: DE
\cite{lakshminarayanan2016simple} via several independent trainings, Puzzle-Mix
\cite{kim2020puzzle} via saliency detection (), GradAug
\cite{yang2020radaug} via multiple subnetworks predictions () or
Mixup BA \cite{Hoffer_2020_CVPR} via  batch augmentations (
with our hardware on a single GPU).


MixMo operates in the features space and is complementary with \textbf{pixels
augmentations}, \textit{i.e.}\ cropping, AugMix. The standard vanilla pixels
data augmentation \cite{he51deep} consists of  pixels padding, random
cropping and horizontal flipping. When combined with CutMix, notably to benefit
from multilabel smoothing, the input may be of the form: , where  is randomly chosen in the whole dataset, and not
only inside the current batch\footnote{Following \url{https://github.com/ildoonet/cutmix}}. Moreover,
 modifies by  the
visible part from mask  (of area ). We thus modify
targets accordingly:  where
. To fully benefit from , we force the repeated
 to remain predominant in its  appearances: \textit{i.e.}, we swap
 and  if . We see CutMix as a perturbation on the
main batch sample.

Distributional uncertainty measures help when there is a mismatch between train
and test data distributions. Thus \cite{hendrycks2018benchmarking} introduced
\textbf{CIFAR-100-c} on which \textbf{AugMix} performs best. AugMix sums the
pixels from a chain of several augmentations and is complementary to our
approach in features. We use default
parameters\footnote{\url{https://github.com/google-research/augmix/blob/master/cifar.py}}:
the severity is set 3, the mixture's width to 3 and the mixture's depth to 4. We
exclude operations in AugMix which overlap with CIFAR-100-c corruptions: thus,
[equalize, posterize, rotate, solarize, shear\_x, shear\_y, translate\_x,
translate\_y] remain. We disabled the Jensen-Shannon Divergence loss between
predictions for the clean image and for the same image AugMix
augmented: that would otherwise triple the training time. For comparison of out-of-domain uncertainty estimations, we report NLL as in \cite{havasi2020raining,ovadia2019can}: indeed, the recommendation of \cite{ashukha2020pitfalls} to apply TS only stands for in-domain test set.

\begin{table}[!t]
    \caption{\textbf{WRN-28-10 on CIFAR} without early stopping.}\centering
    \resizebox{1.0\linewidth}{!}{\begin{tabular}{c | c | c c c c c | c c c c}
        \toprule
        \multicolumn{2}{c|}{Dataset} & \multicolumn{5}{c|}{CIFAR-100} & \multicolumn{4}{c}{CIFAR-10} \\
        \midrule
        \midrule
        Approach & \begin{tabular}{@{}c@{}}\scalebox{1.0}{Time}\\\scalebox{1.0}{Tr./Inf.}\end{tabular} & \begin{tabular}{@{}c@{}}\scalebox{1.0}{Top1}\\\scalebox{0.9}{}\end{tabular} & \begin{tabular}{@{}c@{}}\scalebox{1.0}{Top5}\\\scalebox{0.9}{}\end{tabular} & \begin{tabular}{@{}c@{}}\scalebox{1.0}{NLL}\\\scalebox{0.9}{}\end{tabular} & \begin{tabular}{@{}c@{}}\scalebox{1.0}{NLL}\\\scalebox{0.9}{}\end{tabular} & \begin{tabular}{@{}c@{}}\scalebox{1.0}{ECE}\\\scalebox{0.9}{}\end{tabular} & \begin{tabular}{@{}c@{}}\scalebox{1.0}{Top1}\\\scalebox{0.9}{}\end{tabular} & \begin{tabular}{@{}c@{}}\scalebox{1.0}{NLL}\\\scalebox{0.9}{}\end{tabular} & \begin{tabular}{@{}c@{}}\scalebox{1.0}{NLL}\\\scalebox{0.9}{}\end{tabular} & \begin{tabular}{@{}c@{}}\scalebox{1.0}{ECE}\\\scalebox{0.9}{}\end{tabular}\\
        \midrule
        \midrule
        Vanilla & \multirow{4}{*}{1/1} & 81.47 & 95.57 & 73.6 & 76.2 & 6.47 & 96.31 & 12.5 & 14.1 & 1.95 \\
        Mixup & & 83.15 & 95.75 & 66.3 & 67.3 & \textbf{1.62} & 97.00 & 11.3 & 11.5 & 0.97 \\
Hard PatchUp & & 83.87 & - & - & 66.0 & - & 97.47 & - & 11.4 & - \\
        CutMix & & 83.74 & 96.18 & 65.4 & 66.1 & 4.95 & 97.21 & 9.7& 10.8 & 1.51 \\
\midrule
        Puzzle-Mix & 2/1 & 84.05 & 96.08& 66.9 & 68.1 & 2.76 & - & -& - & - \\
        \midrule
        GradAug & \multirow{2}{*}{3/1} & 83.98 & 96.28 & -& - & - & - & -& - & - \\
        + CutMix & & 85.25 & 96.85 & -& - & - & - & -& - & - \\
        \midrule
        Mixup BA & 7/1 & 84.30 & - & - & - & - &\textbf{97.80} &- & - &- \\
        \midrule
        DE (2 Nets) & \multirow{2}{*}{2/2} & 83.15 & 96.30 & 66.0 & 67.2 & 5.15 & 96.58 & 11.1& 12.2 & 1.82 \\
        + CutMix & & 85.46 & 96.90 & 57.4& 57.5 & 3.62 & 97.51 & 8.7& 9.0 & 1.16 \\
        \midrule
        \midrule
        MIMO () & \multirow{5}{*}{2/1} & 82.04 & 95.75 & 69.1& 72.4 & 6.32 & 96.33 & 12.1& 13.4 & 1.89 \\
        \cmidrule{1-1}
        \cmidrule{3-11}
        \multirow{1}{*}{Linear-MixMo} & & 81.88 & 95.97 & 67.8& 70.3 & 6.20 & 96.55 & 11.4 & 12.5 & 1.67 \\
        + CutMix & & 84.55 & 96.95 & 57.4& 57.5 & 2.54 & 97.34 & 8.9& 9.3 & 1.34 \\
        \cmidrule{1-1}
        \cmidrule{3-11}
        \multirow{1}{*}{Cut-MixMo} & & 84.07 & 96.97 & 56.6& 57.9 & 4.19 & 97.26 & 8.7& 9.1 & 0.98 \\
        + CutMix & & 85.17 & 97.28 & 54.4& 54.5 & 2.13 & 97.33 & 8.5& 8.6 & \textbf{0.88} \\
        \midrule
        MIMO () & \multirow{6}{*}{4/1} & 82.74 & 95.90 & 67.0& 74.0 & 7.56 & 96.66 & 11.5& 13.6 & 1.98 \\
        MIMO ()& & 82.0 & - & -& 69.0 & 2.2 & 96.4 & -& 12.3 & 1.0 \\
        \cmidrule{1-1}
        \cmidrule{3-11}
        \multirow{1}{*}{Linear-MixMo} & & 82.53 & 96.08 & 65.8& 68.5 & 6.64 & 96.78 & 10.8& 11.8 & 1.80 \\
        + CutMix & & 85.24 & 96.97 & 56.3& 56.4 & 3.53 & 97.53 & 8.8& 8.6 & 1.19 \\
        \cmidrule{1-1}
        \cmidrule{3-11}
        \multirow{1}{*}{Cut-MixMo} &  & 85.32 & 97.12 & 53.6& 54.8 & 4.53 & 97.42 & 8.1& 8.4 & 1.15 \\
        + CutMix & & \textbf{85.59} & \textbf{97.33} & \textbf{53.2} & \textbf{53.3} & 1.95 & 97.70 & \textbf{8.0} & \textbf{8.2} & 0.98 \\
        \bottomrule \end{tabular}}\label{table:cifar10010end}\end{table} \begin{figure*}[!ht]
\centering
\includegraphics[width=0.78\linewidth]{pictures/experiments/files/dynamics_p_accnlldiv.jpg}
\caption{\textbf{Training dynamics}. Higher probability  of binary mixing via patches increases diversity (lower right), and also subnetworks accuracy (lower left) but only up to . Around this value, we obtain best ensemble performances, in terms of accuracy (upper left) or uncertainty estimation (upper right). , ,  with WRN-28-10 on CIFAR-100.}
\label{fig:dynamicsp}
\end{figure*} 

\subsection{Evaluation setting and metrics}
\label{app:expeeval}
We reproduce the \textbf{experimental setting} from CutMix
\cite{yun2019cutmix}, Manifold Mixup \cite{manifoldmixup19} and other works such
as the recent state-of-the-art ResizeMix \cite{qin2020resizemix}: in absence of
a validation dataset, results are reported at the epoch that yields the best
test accuracy. For fair comparison, we apply this early stopping for all concurrent
approaches. Nonetheless, for the sake of completeness, Table \ref{table:cifar10010end}
shows results without early stopping on the main experiment (CIFAR with a
standard WRN-28-10). We recover the exact same ranking among methods as in Table \ref{table:cifar10010}.

Following recent works in ensembling \cite{chirkova2020deep,lobacheva2020power,rame2021dice}, we have mainly focused on the NLL \textbf{metric} for in-domain test set. Indeed, \cite{ashukha2020pitfalls} have shown that \enquote{comparison of \textelp{} ensembling methods without temperature scaling (TS) \cite{guo2017calibration} might not provide a fair ranking}. Nevertheless  in Table \ref{table:cifar10010end}, we found that \textit{Negative Log-Likelihood} (NLL) (without TS) leads to similar conclusions as NLL (after TS).

The TS even mostly seems to benefit to poorly calibrated models, as shown by the calibration criteria \textit{Expected Calibration Error} (ECE, , 15 bins). ECE measures how confidences match accuracies. MixMo attenuates over-confidence in large networks and thus reduces ECE. In our case, combining ensembling and data augmentation improves calibration \cite{wen2021combining}.
Note that the appropriate measure of calibration is still under debate \cite{nixon2019measuring,NEURIPS2019_1c336b80,zhang2020mix}. 
Notably, \cite{ashukha2020pitfalls} have also stated that, despite being widely used, ECE is biased and unreliable: we can confirm that we found ECE to be dependant to hyper-parameters and implementation details. Due to space constraints and these pitfalls, we have not included this controversial metric in the main paper.
%
 \subsection{Training dynamics}
\label{app:td}

Fig.~\ref{fig:dynamicsp} showcases training dynamics for probability 
of patch mixing (see Section \ref{expe:mixingmechanism}). In the remaining , we interpolate features linearly. For , we recover our Linear-MixMo; for , we recover our Cut-MixMo. In all approaches,  is linearly reduced towards  beyond the  of the training epochs, \textit{i.e.}\ from epoch 275 to 300 on CIFAR. As we sum at inference, this reduces the train-test distribution gap and slightly increases individual accuracy during the final epochs (lower left in Fig.~\ref{fig:dynamicsp}).

Diversity is measured by the ratio-error, the ratio between the number of samples on which only one of the two predictor is wrong, divided by the number of samples on which they are both wrong. It is positively correlated with . However, individual accuracies first increase
with  until , then the tendency is reversed.
Overall, best ensemble performances in terms of accuracy (Top1) and uncertainty (NLL)
estimation are obtained with . Most importantly, we note that the performance gaps are consistent and stable along training. \begin{figure}[!b]\includegraphics[width=\linewidth]{pictures/model/files/msda.png}\caption{\textbf{Common MSDA procedures with }.}\label{fig:msda}\end{figure}% \subsection{Mixed sample data augmentations}
\label{app:msda}
We have drawn inspiration from MSDA techniques to design our mixing block
. In particular, Section \ref{expe:mixingmechanism} compared
different  based on recent papers. Fig.~\ref{fig:msda}
provides the reader a visual understanding of their behaviour, which we explain
below.

\textbf{MixUp} \cite{zhang2018mixup} linearly interpolates between pixels
values: . The remaining methods
fall under the label of \textbf{binary MSDA}:  with
 a mask with binary values  and area of ratio .
They diverge in how this mask is created. The \textbf{horizontal
  concatenation}, also found in \cite{summers2019improved}, simply draws a
vertical line such that every pixel to the left belongs to one sample and every
pixel to the right belongs to the other. Similarly, we define a \textbf{vertical
  concatenation} with an horizontal line. \textbf{PatchUp}
\cite{faramarzi2020patchup} adapted DropBlock \cite{ghiasi2018dropblock}: a
canvas  of patches is created by sampling for every spatial coordinate from
the Bernoulli distribution  (where  is a recalibrated
value of ): if the drawn binary value is , a patch around that
coordinate is set to  on the final binary mask . PatchUp was
designed for in-manifold mixing with a different mask by channels. However,
duplicating the same 2D mask in all channels for  performs better in our
experiments. \textbf{FMix} \cite{harris2020mix} selects a large contiguous
region in one image and pastes it onto another. The binary mask is made of the
top- percentile of pixels from a low-pass filtered 2D map  drawn
from an isotropic Gaussian distribution. \textbf{CowMix}
\cite{french2019semi,french2020milking} selects a cow-spotted set of regions,
and is somehow similar to FMix with a Gaussian filtered 2D map .
\textbf{CutMix} \cite{yun2019cutmix} was inspired by CutOut
\cite{devries2017improved}. Formally, we sample a square with edges of length
, where  is the length of an image edge. Note that this
sometimes leads to non square rectangles when the initially sampled square
overlaps with the edge from the original image. We adjust our  a
posteriori to fix this boundary effect. Regarding the hyper-parameters, we
use in  those provided in the seminal papers, except for sampling of  where we set  in all setups.% \subsection{Hyper-parameter }
\label{app:alpha}
In Fig.~\ref{fig:tradeoff_alpha}, we study the impact of different values of
, parameterizing the sampling law for .
For high values of , the interval of  narrows down around .
Diversity is therefore decreased: we speculate this is because we do not benefit
anymore from lopsided updates. The opposite extreme, when , is
equivalent to uniform distribution between  and . Therefore diversity is
increased, at the cost of lower individual accuracy due to less stable training.
For simplicity, we set . Manifold-Mixup \cite{manifoldmixup19} selected the same value on CIFAR-100. However, this value could be fine tuned on the target task: \textit{e.g.}\ in
Fig.~\ref{fig:tradeoff_alpha},  seems to perform best for Cut-MixMo on CIFAR-100 with WRN-28-10 with ,  and .% \begin{figure}[!h]
\centering \includegraphics[width=0.855\linewidth]{pictures/experiments/files/alpha.jpg}\vspace{-0.5em}\caption{\textbf{Diversity/accuracy} as function of .}\label{fig:tradeoff_alpha}\end{figure}% \begin{table*}[!h]\caption{\textbf{Summary}: WRN-28- on CIFAR-100. .}
\centering
\resizebox{0.85\linewidth}{!}{\begin{tabular}{c | c | c c | c c| c c | c c | c c }
    \toprule
    Width & \multirow{1}{*}{Approach}  & \multicolumn{2}{c|}{1-Net} &  \multicolumn{2}{c|}{2-Nets} & \multicolumn{2}{c|}{Linear-MixMo} & \multicolumn{2}{c|}{Cut-MixMo} & \multicolumn{2}{c}{2-Cut-MixMos} \\
     &  CutMix & - & \checkmark & - & \checkmark & - & \checkmark & - & \checkmark & - & \checkmark \\
    \midrule
    \midrule
    \multirow{3}{*}{2} & Top1 & 76.44 & 78.06 & 79.16 & 80.81 & 75.82 & 76.36 & 75.66 & 75.17 & 76.98 & 76.11 \\
    & NLL & 0.921 & 0.815 & 0.776 & 0.695 & 0.841 & 0.824 & 0.824 & 0.846 & 0.7661 & 0.798\\
    & \# params & \multicolumn{2}{c|}{1.48M} & \multicolumn{2}{c|}{2.95M} & \multicolumn{4}{c|}{1.49M} & \multicolumn{2}{c}{2.99M} \\
    \midrule
    \multirow{3}{*}{3} & Top1 & 77.95 & 80.70 & 80.85 & 83.14 & 78.51 & 80.74 & 79.81 & 79.85 & 80.78 & 81.20 \\
    & NLL & 0.862 & 0.750 & 0.738 & 0.644 & 0.760 & 0.696 & 0.693 & 0.702 & 0.635 & 0.650\\
    & \# params & \multicolumn{2}{c|}{3.31M} & \multicolumn{2}{c|}{6.62M} & \multicolumn{4}{c|}{3.33M} & \multicolumn{2}{c}{6.66M} \\
    \midrule
    \multirow{3}{*}{4} & Top1 & 78.84 & 81.55 & 81.48 & 83.93 & 80.43 & 81.66 & 81.68 & 81.69 & 82.57 & 82.58\\
    & NLL & 0.824 & 0.711 & 0.711 & 0.609 & 0.712 & 0.656 & 0.646 & 0.635 & 0.590 & 0.588\\
    & \# params & \multicolumn{2}{c|}{5.87M} & \multicolumn{2}{c|}{11.74M} & \multicolumn{4}{c|}{5.89M} & \multicolumn{2}{c}{11.79M} \\
    \midrule
    \multirow{3}{*}{5} & Top1 & 79.75 & 82.55 & 82.18 & 84.60 & 80.95 & 83.06 & 83.11 & 83.34 & 83.97 & 84.31\\
    & NLL & 0.813 & 0.686 & 0.693 & 0.596 & 0.703 & 0.617 & 0.598 & 0.591 & 0.549 & 0.546\\
    & \# params & \multicolumn{2}{c|}{9.16M} &  \multicolumn{2}{c|}{18.32M} & \multicolumn{4}{c|}{9.19M} &  \multicolumn{2}{c}{18.39M} \\
    \midrule
    \multirow{3}{*}{7} & Top1 & 81.14 & 83.71 & 82.94 & 85.52 & 82.4 & 84.51 & 84.32 & 84.94 & 85.50 & 85.90 \\
    & NLL & 0.764 & 0.648 & 0.673 & 0.573 & 0.675 & 0.581 & 0.562 & 0.543 & 0.516 & 0.498 \\
    & \# params & \multicolumn{2}{c|}{17.92M} & \multicolumn{2}{c|}{35.85M} & \multicolumn{4}{c|}{17.97M}  &  \multicolumn{2}{c}{35.94M} \\
    \midrule
    \multirow{3}{*}{10} & Top1 & 81.63 & 84.05& 83.17 & 85.74 & 83.08 & 85.47 & 85.40 & 85.77 & 86.04 & 86.63 \\
    & NLL & 0.750 & 0.644 & 0.668 & 0.571 & 0.656 & 0.558 & 0.535 & 0.524 & 0.494 & 0.479 \\
    & \# params & \multicolumn{2}{c|}{36.53M} & \multicolumn{2}{c|}{73.07M} & \multicolumn{4}{c|}{36.60M} & \multicolumn{2}{c}{73.21M} \\
    \midrule
    \multirow{3}{*}{14} & Top1 & 82.01 & 84.31 & 83.47 & 85.80 & 83.79 & 86.05 & 85.76 & 86.19 & 86.58 & 87.11  \\
    & NLL & 0.730 & 0.645 & 0.656 & 0.569 & 0.648 & 0.545 & 0.527 & 0.518 & 0.488 & 0.473 \\
    & \# params & \multicolumn{2}{c|}{71.55M} & \multicolumn{2}{c|}{143.1M} & \multicolumn{4}{c|}{71.64M} & \multicolumn{2}{c}{143.28M} \\
    \bottomrule
\end{tabular}
    }
\label{table:splitadvantage}
\end{table*}  \begin{figure}[!b]\centering \includegraphics[width=1.0\linewidth]{pictures/experiments/files/smacxpowerlaw.png}\caption{\textbf{Ensemble effectiveness} (NLL/\#params). We slide the width in WRN-28- and numbers of members . CutMix data augmentation. Interpolations through power laws \cite{lobacheva2020power} when more than 2 points are available.}\label{fig:smacx}\end{figure}% \subsection{Ensemble of Cut-MixMo with CutMix}
\label{app:ensmixmo}
Fig.~\ref{fig:smacx} 
plots performance for different
widths  in WRN-28- and varying number of ensembled networks : two
vertically aligned points have the same parameter budget. Indeed, the total number of
parameters in our architectures has been used as a proxy for model complexity, as in \cite{chirkova2020deep,lobacheva2020power}. We compare
ensembling with CutMix rather than standard pixels data augmentation, as previously done in Fig.~\ref{fig:batchrepet} from Section \ref{expe:ensemblemixmo}. CutMix induces additional regularization and label smoothing: empirically, it improves all our approaches. For a
fixed memory budget, a single network usually performs worse than an ensemble of
several medium-size networks: we recover the \textbf{Memory Split Advantage} even with CutMix.
However, Cut-MixMo challenges this by remaining closer to the
lower envelope. In other words,
parameters allocation (more networks or bigger networks) has less impact on
results. This is due to Cut-MixMo's ability to better use large networks.

In Table \ref{table:splitadvantage}, we summarize several experiments on
CIFAR-100. Among other things, we can observe that large vanilla networks tend
to gain less from ensembling \cite{lobacheva2020power}: \textit{e.g}.\ 2
vanillas WRN-28-10 ( Top1,  NLL) do not perform much better
than 2 WRN-28-7 (, ). This remains true even with CutMix:
(, ) vs.\ (, ). We speculate this is related to
wide networks' tendency to converge to less diverse solutions, as studied in
\cite{neal2018modern}. Contrarily, \textbf{MixMo improves the ensembling of large networks}, with (, ) vs.\ (, ) on the same setup. When additionally combined with CutMix, we obtain state of the art
(, ) vs.\ (, ). 
This demonstrates the importance of Cut-MixMo in cooperation with standard pixels data augmentation.
It attenuates the drawbacks from over-parameterization
This is of great importance for practical efficiency: it modifies
the optimal network width for real-world applications.%
 \subsection{Pseudo Code}Finally, the pseudocode in Algorithm \ref{pseudocode} describes the procedure behind Cut-MixMo with .\begin{algorithm*}
\DontPrintSemicolon
\tcc{Setup}\KwParams{First convolutions , dense layers  and core network , randomly initialized.}\KwInput{Dataset , probability  of applying binary mixing via patches, reweighting coefficient , concentration parameter , batch size , batch repetition , optimizer  , learning rate .}\tcc{Training Procedure}\For{epoch \textbf{from}  \textbf{to} \#epochs} {\For{step \textbf{from}  \textbf{to} } {\tcc{Step 1: Batch creation}\text{Randomly select}  samples \tcp*{Sampling}\text{Duplicate these samples  times} to create batch  of size  \tcp*{Batch repetition}\text{Randomly shuffle  with } to create  \tcp*{Shuffling}\tcc{Step 2: Define the mixing mechanism at the batch level}\uIf{}{
 \tcp*{Linear descent to  over the last twelfth of training}
}
\Else{

}
Sample  from Bernoulli distribution \tcp*{Whether we apply binary or linear mixing}Sample  \tcp*{Whether the first input is inside or outside the rectangle}\tcc{Step 3: Forward and loss}\For{} {Sample \\
     and \\
  \uIf{}{
    Sample  a rectangular binary mask with average  (as in CutMix)\\
    \uIf{}{
     \tcp*{Permute the rectangle and its complementary}
    }
     \tcp*{Apply binary mixing}}
   \Else{
     \tcp*{Apply linear interpolation}}
Extract features  from core network \\
Compute predictions  and  \\
Compute weights \\
Compute loss 
}
Average loss \\
\tcc{Step 4: Back propagation}\\
}}\tcc{Test Procedure}\KwData{Inputs  \tcp*{Test Data}}\For{} {\text{Extract features}  \\\KwOutput{}}\caption{Procedure for Cut-MixMo with  subnetworks}\label{pseudocode}
\end{algorithm*}%
 %
 
\end{document}
