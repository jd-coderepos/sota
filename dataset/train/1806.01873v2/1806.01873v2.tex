\begin{comment}
\begin{table*}[]
\centering
\small
\begin{tabular}{l||c|c|c|c|c|c}
\hline
Method            & how many & what & when & where  & who  & overall \\ 
&(11.8\%)  & (41.9\%)& (16.2\%)  &(17.2\%)&(12.9\%)&
\\\hline \hline
Logistic Regression             & 0.645 & 0.241 & 0.217 & 0.277 & 0.260 & 0.295 \\
Embedding + LSTM             & 0.771 & 0.564& 0.349& 0.314& 0.310& 0.478 \\
Embedding + LSTM + Concat            & 0.776 &	0.668& 	0.398& 	0.433	& 0.409	& 0.563 \\
\hline
MemexNet \cite{jiang2017memexqa}            & 0.668 & 0.448 & \textbf{0.480} & 0.460 & 0.468 & 0.484 \\
DMN+  \cite{xiong2016dynamic}        & 0.792 &	0.625 &	0.342 &	0.252 & 	0.222&	0.483 \\
Multimodal Compact Bilinear Pooling \cite{fukui2016multimodal}& 0.768 &	0.631	 &	0.258& 	0.229		&0.240	 &0.467 \\


Bi-directional Attention Flow    \cite{seo2016bidirectional}    &0.790	 &0.665&	0.354& 	0.556& 	0.453& 	0.583\\
Soft Attention          &  \textbf{0.802} &	0.682	 & 0.384&	0.541 & 	0.479& 	0.598 \\
TGIF Temporal Attention   \cite{jang2017tgif}       & 0.783 &	0.714	 & 	0.469&	0.622	 & 0.558& 		0.647 \\
FVTA          &  0.773	& \textbf{0.725} & 	0.422& \textbf{	0.701}	& \textbf{0.710} &	\textbf{0.676} \\
 \hline
\end{tabular}
\caption{Comparison of different methods on MemexQA by type of question. The first three methods do not use the attention mechanism.}
\vspace{-3mm}
\label{exp-mem}
\end{table*}
\end{comment}

\begin{table*}[]
\centering
\begin{tabular}{l||c|c|c|c|c|c}
\hline
Method            & how many & what & when & where  & who  & overall \\ 
&(11.8\%)  & (41.9\%)& (16.2\%)  &(17.2\%)&(12.9\%)&
\\\hline \hline
Logistic Regression             & 0.645 & 0.241 & 0.217 & 0.277 & 0.260 & 0.295 \\
Embedding + LSTM             & 0.771 & 0.564& 0.349& 0.314& 0.310& 0.478 \\
Embedding + LSTM + Concat            & 0.776 &	0.668& 	0.398& 	0.433	& 0.409	& 0.563 \\
\hline
DMN+  \cite{xiong2016dynamic}        & 0.792 & 0.616 & 0.346 & 0.248 & 0.224 & 0.480 \\
Multimodal Compact Bilinear Pooling \cite{fukui2016multimodal}& 0.773 & 0.618 & 0.250 & 0.229 & 0.248 & 0.462 \\


Bi-directional Attention Flow    \cite{seo2016bidirectional}    &0.790 & 0.689 & 0.356 & 0.567 & 0.468 & 0.598\\
Soft Attention          &  \textbf{0.795} & 0.697 & 0.346 & 0.604 & 0.582 & 0.621 \\
TGIF Temporal Attention   \cite{jang2017tgif}       & 0.761 & 0.700 & \textbf{0.522} & 0.582 & 0.477 & 0.630 \\
FVTA          &  0.761 & \textbf{0.714} & 0.476 & \textbf{0.676} & \textbf{0.668} & \textbf{0.669} \\
 \hline
\end{tabular}
\vspace{1mm}
\caption{Comparison of different methods on MemexQA by question type. The first three methods do not use the attention mechanism.}
\label{exp-mem}
\vspace{-4mm}
\end{table*}






























\section{Experiments}\label{exp}



\subsection{MemexQA}
\noindent\textbf{Dataset} 
MemexQA~\cite{jiang2017memexqa} is a recently proposed visual-text question answering dataset. The dataset consists of 20,860 questions about 13,591 personal photos belonging to 101 real Flickr users. These personal photos capture a variety of key moments of their lives such as a trip to Japan, wedding ceremonies, family parties, etc. Each album and photo come with comprehensive visual-text information, including a timestamp, GPS, a photo title, an album title and description. The metadata is incomplete and GPS, the photo title, the album title and description may not present in every photo.

MemexQA provides 4 answer choices and only one correct answer for each question. The dataset also provides more than one ground truth grounding images for each question. There are five types of questions corresponding to the frequent search terms discovered in the Flickr search logs~\cite{jiang2017delving}.
The input visual-text sequence length varies for questions. Some questions are about images taken on a certain date \eg ``what did we do after 2006 Halloween party?''; others are about all images \eg ``what was the last time we drove to a bar?''.






\noindent\textbf{Baseline Methods} A large proportion of the existing solutions is to project image or videos into an embedding space, and train a classification model using these embeddings. We implement the following methods as baselines: \textbf{\textit{Logistic Regression}} predicts the answer with concatenated image, question and metadata features as reported in \cite{jiang2017memexqa}. \textbf{\textit{Embedding + LSTM}} utilizes word embeddings and character embeddings, along with the same visual embeddings used in FVTA. Embeddings are encoded by LSTM and averaged to get the final context representation. \textbf{\textit{Embedding + LSTM + Concat}} concatenates the last LSTM output from different modalities to produce the final output. On the other hand, we compare the proposed model to a rich collection of VQA attention models: \textbf{\textit{Classic Soft Attention}} uses classic one dimensional question-to-context attention to summarize context for question answering. A correlation matrix between each question word and context is used to compute the attention as in \cite{seo2016bidirectional,xu2016ask}. \textbf{\textit{DMN+}} is the improved dynamic memory networks \cite{xiong2016dynamic}, which is one of the representative architectures that achieve good performance on the VQA Task. We implement the DMN+ network with each sentence and each photo representation used in our proposed network as supporting facts input. \textbf{\textit{Multimodal Compact Bilinear Pooling}}\cite{fukui2016multimodal} is the state-of-the-art method on VQA~\cite{antol2015vqa} dataset. The spatial attention in the original model is directly used on the sequential images input. The hyperparameters including the output dimension of MCB and hidden size of LSTM are selected based on the validation results. 
\textbf{\textit{Bi-directional Attention Flow}} implements the single-modal attention flow model \cite{seo2016bidirectional} over all concatenated context representations with embeddings as in FVTA network. \textbf{\textit{TGIF Temporal Attention}} \cite{jang2017tgif} is a recently proposed spatial-temporal reasoning network on sequential animated image QA. Since other baseline methods do not use spatial attention, we compare the TGIF network with temporal attention only. TGIF temporal attention uses a simple MLP to compute the attention and only the last hidden state of the question is considered. We compute the attention following \cite{jang2017tgif} and use the same output layer in our method.





\noindent\textbf{Implementation Details}
\label{sec-impl}
In MemexQA dataset, each question is asked to a sequence of photos organized in albums. A photo might have 5 types of textual metadata, including the \textit{album title}, \textit{album descriptions},  \textit{GPS Locations}, \textit{timestamp} and a \textit{title}. We use   to denote the maximum number of albums,  for the maximum number of photos in an album and  for the maximum words. For album-level textual sequences like album titles and descriptions, the  dimension only has one item and others are zero-padded. We also use zeros to pad those positions with no word/image.
We encode GPS locations using words.
The photos and their corresponding metadata form the visual-text sequences. All questions, textual context and answers are tokenized using the Stanford word tokenizer. We use pre-trained GloVe word embeddings \cite{pennington2014glove}, which is fixed during training.
For image/video embedding, we extract fixed-size features using the pre-trained CNN model, Inception-ResNet \cite{szegedy2017inception}, by concatenating the pool5 layer and classification layer's output before softmax. We then use a linear transformation to compress the image feature into 100 dimensional.
Then a bi-directional LSTM is used for each modality to obtain contextual representations. Given a hidden state size of ,  which is set to 50, we concatenate the output of both directions of the LSTM and get a question matrix 
and context tensor  for all media documents. 
We reshape the context tensor into . 
To select the best hyperparmeters, we randomly select 20\% of the official training set as the validation set.
We use the AdaDelta \cite{zeiler2012adadelta} optimizer and an initial learning rate of 0.5 to train for 200 epochs with a dropout rate of 0.3.





\subsubsection{Comparison to the state-of-the-art} \label{FVT_comp_exp}

Table~\ref{exp-mem} compares the accuracy on the MemexQA. As we see, the proposed method consistently outperforms the baseline methods and achieves the state-of-the-art accuracy on this dataset. The first 3 methods in the table show the performance of embedding methods without any attentions. Although embedding methods are relatively simple to implement, their performance is much lower than the proposed FVTA model. The experiment results advocate the attention model among images and image sequences. Compare to previous attention models, our FVTA network significantly outperforms other methods, which proves the efficacy of the proposed method.










\begin{table}[ht]
\centering
\begin{tabular}{l||c|c|c}
\hline
               & HIT@1 & HIT@3 & mAP \\ \hline \hline
Soft Attention    & 1.16\%         & 12.60\% & 0.1680.002                      \\
MCB    & 11.98\%         & 30.54\% & 0.2690.005                      \\
TGIF Temporal    & 13.28\%         & 32.83\% & 0.2890.005                      \\
FVTA & \textbf{15.48}\%          & \textbf{35.66}\%    & \textbf{0.3120.005}                  \\
\hline
\end{tabular}
\vspace{1mm}
\caption{The quality comparison of the learned FVTA and classic attention. We compare the image of the highest activation in a leaned attention to the ground truth evidence photos which human used to answer the question. HIT@1 means the rate of the top attended images being found in the ground truth evidence photos. AP is computed on the photo ranked by their attention activation.}
\label{att-acc}
\vspace{-3mm}
\end{table}



The MemexQA dataset provides ground truth evidence photos for every question. We can compare the correlation between the photos of the highest attention weights and the ground truth photos to correctly answer a question. An ideal VQA model should not only enjoy a high accuracy in answering a question (Table~\ref{exp-mem}) but also can find images that are highly correlated to the ground-truth evidence photos. Table~\ref{att-acc} lists the accuracy to examine whether a model puts focus on the correct photos. FVTA outperforms other attention models on finding the relevant photos for the question. The results show that the proposed attention can capture salient information for answering the question. For qualitative comparison, we select some representative questions and show both the answer and the retrieved top images based on the attention weights in Fig.~\ref{FVT_comp_vis}. 
As shown in the first example, the system has to find the correct photo and visually identify the object to answer the question "what did the daughter eat while her dad was watching during the trip in June 2010?". FVTA attention puts a high weight on the correct photo of the girl eating a corn, which leads to correctly answering the question. Whereas for soft attention, the one-dimensional attention network outputs the wrong image and gets the wrong answer.
This example shows the advantage of FVTA modeling the correlation at every time step, across visual-text sequences over the traditional dimensional attention. 

\subsubsection{Ablation Study}
Table \ref{exp-mem-abla} shows the performance of FVTA mechanism and its ablations on the MemexQA dataset. 
To evaluate the FVTA attention mechanism, we first replace our kernel tensor with simple cosine similarity function. Results show that standard cosine similarity is inferior to our similarity function. 
For ablating intra-sequence dependency, we use the representations from the last timestep of each context document. For ablating cross sequence interaction, we average all attended context representation from different modalities to get the final context vector.
Both aspects of correlation of the FVTA attention tensor contribute towards the model's performance, while intra-sequence dependency shows more importance in this experiment. We compare the effectiveness of context-aware question attention by removing the question attention and use the last timestep of the LSTM output from the question as the question representation. It shows the question attention provides slight improvement. Finally, we train FVTA without photos to see the contribution of visual information. The result is quite good but it is perhaps not surprising due to the language bias in the questions and answers of the dataset, which is not uncommon in VQA dataset~\cite{antol2015vqa} and in Visual7W~\cite{zhu2016visual7w}. This also leaves significant rooms of improvement with visual information.

\begin{table}[ht]
\centering
\begin{tabular}{l||c|c}
\hline
Ablations      & Accuracy &  \\ \hline \hline
FVTA w/ Cosine Similarity  & 0.619 & -4.9\% \\
FVTA w/o Intra-seq  &0.569& -10.0\% \\
FVTA w/o Cross-seq  & 0.604& -6.5\%\\
FVTA w/o Question Attention 		&0.629 &-4.0\%\\ 
FVTA w/o Photos &	0.577& -9.1\%\\
\hline
\end{tabular}
\vspace{1mm}
\caption{Ablation studies of the proposed FVTA method on the MemexQA dataset. 
The last column shows the performance drop.}
\label{exp-mem-abla}
\end{table}




\begin{figure*}[!t]
	\centering
		\includegraphics[width=0.99\textwidth]{all4_small_2.png}
	\caption{ 
	Qualitative comparison of FVTA model and other attention models on the MemexQA dataset. For each question, we show the answer and the images of the highest attention weights. Images are ranked from left to right based on the attention weights. The correct images and answers have green border whereas the incorrect ones are surrounded by the red border. }
	\label{FVT_comp_vis}
\end{figure*}

\begin{figure*}[!t]
	\centering
		\includegraphics[width=0.99\textwidth]{movieqaQual.png}
	\caption{ 
	Qualitative analysis of FVTA on the MovieQA dataset. It shows the visual justification (movie clip frames) and text justification (subtitles) based on the top attention activation. Both justifications provide supporting evidence for the system to get the correct answer.
	}
	\label{FVT_comp_movieqa_vis}
\end{figure*}

\subsection{MovieQA}
\noindent\textbf{Dataset}
The MovieQA dataset consists of 140 movies and 6,462 multiple choice QA pair. Each QA pair contains five answer choices with only one correct answer. Systems are required to answer the questions given a number of movie clips from the same movie and the corresponding subtitles. 
More details of the dataset can be viewed in~\cite{tapaswi2016movieqa}.


\noindent\textbf{Implementation Details}
\label{sec-impl2}
In the MovieQA dataset, each QA is given a set of  movie clips of the same movie, and each clip comes with subtitles. We implement FVTA network for MovieQA task with modality number of 2 (video \& text). 
We set the maximum number of movie clips per question to , the maximum number of frames to consider to , the maximum number of subtitle sentences in a clip to  and the maximum words to . Visual and text sequences are encoded the same way as in the MemexQA \cite{jiang2017memexqa} experiment.  
We use the AdaDelta \cite{zeiler2012adadelta} optimizer with a minibatch of 16 and an initial learning rate of 0.5 to trained for 300 epochs. A dropout rate is set at 0.2 during training. 
The official training/validation/test split is used in our experiments.



\begin{table}[]
\centering
\begin{tabular}{l||c|c}
\hline
Method            & Val & Test \\
\hline \hline
SSCB \cite{tapaswi2016movieqa}          & 0.219 & -\\
MemN2N \cite{tapaswi2016movieqa}        & 0.342 & -\\
DEMN \cite{kim2017deepstory} & - & 0.300\\
Soft Attention & 0.321 & -\\
MCB \cite{fukui2016multimodal}& 0.362 & -\\
TGIF Temporal \cite{jang2017tgif} & 0.371 & -\\
RWMN \cite{na2017read} & 0.387 & 0.363 \\
FVTA        & \textbf{0.410} & \textbf{0.373} \\
 \hline
\end{tabular}
\vspace{1mm}
\caption{Accuracy comparison on the test and the validation set of the MovieQA dataset. The test set performance can only be evaluated on the MovieQA server, and thus not all the studies provide the accuracy on Test set.
}
\label{exp-movieqa}
\end{table}

\noindent\textbf{Experimental Results}
We compare FVTA with recent results on MovieQA dataset, including End-to-End Memory Network (MemN2N) \cite{MovieQA}, Deep Embedded Memory Network (DEMN) \cite{kim2017deepstory}, and Read-Write Memory Network (RWMN) \cite{na2017read}. 
Table~\ref{exp-movieqa} shows the detailed comparison of MovieQA results using both videos and subtitles. FVTA model outperforms all baseline methods and achieves comparable performance to the state-of-the-art result \footnote{The best test accuracy on the leaderboard by the time of paper submission (Nov. 2017) is 0.39 (Layered Memory Networks). It is not included in the table as there is no publication to cite.} on the MovieQA test server. Notably, RWMN \cite{na2017read} is a very recent work that uses memory net to cache sequential input, with a high capacity and flexibility due to the read and write networks. Our accuracy is 0.410 (vs 0.387 by RWMN) on the validation set and 0.373 (vs 0.363) on the test set. Benefiting from such modeling ability,
FVTA consistently outperforms the classical attention models including soft attention, MCB \cite{fukui2016multimodal} and TGIF \cite{jang2017tgif}. The result demonstrates the consistent advantages of FVTA over other attention models in question-answering for multiple sequence data.


Fig.~\ref{FVT_comp_movieqa_vis} illustrates the output of our FVTA model. FVTA can not only predict the correct answer, but also identify the most relevant subtitle description as well as the movie clip frames. As shown in Fig.~\ref{FVT_comp_movieqa_vis}, FVTA can provide fine-grained level justifications such as the most informative movie frames or subtitle sentences, whereas most of existing methods cannot find fine-grained justifications from the attention computed at the movie clip level. We believe the results show the benefits and potentials of FVTA model.











\begin{comment}
The level of difficulty of this dataset can also be indicated from the fact that the percentage of answers directly found in text metadata is 32\% on "What" questions and 46\% overall. It suggests that to find the correct answers, the system has to reason and come up with words or phrases that do not exist in the metadata most of the time.
\begin{table}[]
\centering
\caption{Percentage of answers found in text metadata excluding answers from "How many" questions.}
\label{memex-dataset}
\begin{tabular}{l||c}
\hline
Question Type       & Found in anywhere  \\ \hline \hline
what    & 32.61\%                  \\
when    & 70.86\%                    \\
where   & 48.36\%                      \\
who     & 55.82\%                   \\
overall & 46.09\%               \\
\hline
\end{tabular}
\end{table}
\end{comment}