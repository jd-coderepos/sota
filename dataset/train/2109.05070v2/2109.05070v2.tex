We provide additional material to support the main paper. We credit the used assets by citing their web links and licenses in Section~\ref{app:assets}, and continue by describing the experimental setup and used hyperparameters in Section~\ref{app:hyperparam}. We compute Precision and Recall metrics on \ImNet in Section~\ref{app:pr_metrics}, and we further compare BigGAN and StyleGAN2 backbones for IC-GAN on \ImNet in Section~\ref{app:stylegan_biggan}. We provide additional qualitative results for both \ours on \ImNet in Section~\ref{app:add_qualitative} and \ours off-the-shelf transfer results on other datasets in Section~\ref{app:add_transfer}. Moreover, we provide results when training BigGAN with class balancing on \ImNet-LT in Section~\ref{app:class_balacing}. Finally, we show further impact studies such as the choice of feature extractor (Section~\ref{app:feature_extractor}), the number of conditionings used during training (Section~\ref{app:num_cond_train}), matching storage requirements for unconditional counterparts of BigGAN and StyleGAN2 and \ours (Section~\ref{app:fair_comparison}) and the qualitative impact of neighborhood size  for \ImNet, as well as quantitative results for ImageNet-LT and COCO-Stuff (Section~\ref{app:k_size}).




\section{Assets and licensing information}
\label{app:assets}
In Tables~\ref{table:assets} and~\ref{table:licenses}, we provide the links to the used datasets, repositories and their licenses. We use Faiss~\cite{JDH17} for a fast computation of nearest neighbors and k-means algorithm leveraging GPUs, DiffAugment~\cite{zhao2020differentiable} for additional data augmentation when training BigGAN, and the pre-trained SwAV~\cite{caron2020unsupervised} and ResNet50 models on \ImNet-LT~\cite{kang2019decoupling} to extract instance features. 

\begin{table}[h]
\centering
\small
\caption{Links to the assets used in the paper.}
\begin{tabular}{@{}lc@{}}
\toprule
 Name & GitHub link \\
  \midrule
  \ImNet~\cite{ILSVRC15} & \url{https://www.image-net.org} \\ 
  \ImNet-LT~\cite{openlongtailrecognition} & \url{https://github.com/zhmiao/OpenLongTailRecognition-OLTR}\\ 
COCO-Stuff~\cite{caesar2018cvpr} & \url{https://cocodataset.org/} \\ 
Cityscapes~\cite{Cordts2016Cityscapes} & \url{https://www.cityscapes-dataset.com/} \\ 
MetFaces~\cite{karras2020training} & \url{https://github.com/NVlabs/metfaces-dataset}\\
PACS~\cite{li2017deeper} & \url{https://domaingeneralization.github.io/}\\ 
Sketches~\cite{eitz2012hdhso} & \url{http://cybertron.cg.tu-berlin.de/eitz/projects/classifysketch/}\\ 
\midrule
BigGAN~\cite{brock2018large} & \url{https://github.com/ajbrock/BigGAN-PyTorch} \\
StyleGAN2~\cite{karras2020training} & \url{https://github.com/NVlabs/stylegan2-ada-pytorch}\\
Faiss~\cite{JDH17} & \url{https://github.com/facebookresearch/faiss} \\
DiffAugment~\cite{zhao2020differentiable} & \url{https://github.com/mit-han-lab/data-efficient-gans} \\
PRDC~\cite{naeem2020reliable} & \url{https://github.com/clovaai/generative-evaluation-prdc} \\
SwAV~\cite{caron2020unsupervised} & \url{https://github.com/facebookresearch/swav} \\
Pre-trained ResNet50~\cite{kang2019decoupling} & \url{https://github.com/facebookresearch/classifier-balancing} \\
\bottomrule
\end{tabular}
 \label{table:assets}
 \end{table}
 
 
 \begin{table}[h]
 \centering
 \small
 \caption{Assets licensing information.}
\begin{tabular}{@{}lc@{}}
\toprule
 Name & License \\
  \midrule
\ImNet~\cite{ILSVRC15} and \ImNet-LT~\cite{openlongtailrecognition} & Terms of access: \url{https://www.image-net.org/download.php} \\ 
COCO-Stuff~\cite{caesar2018cvpr} & \url{https://www.flickr.com/creativecommons} \\ 
Cityscapes~\cite{Cordts2016Cityscapes} & \url{https://www.cityscapes-dataset.com/license} \\ 
MetFaces~\cite{karras2020training} & Creative Commons BY-NC 2.0 \\
PACS~\cite{li2017deeper} & Unknown \\ 
Sketches~\cite{eitz2012hdhso} & Creative Commons Attribution 4.0 International \\ 
\midrule
BigGAN~\cite{brock2018large} & MIT \\
StyleGAN2~\cite{karras2020training}  & NVIDIA Source Code \\
Faiss~\cite{JDH17}  & MIT \\
DiffAugment~\cite{zhao2020differentiable} & BSD 2-Clause "Simplified" \\
PRDC~\cite{naeem2020reliable} & MIT \\
swAV~\cite{caron2020unsupervised}  & Attribution-NonCommercial 4.0 International \\
Pre-trained ResNet50~\cite{kang2019decoupling} & BSD \\
\bottomrule
\end{tabular}
\label{table:licenses}
 \end{table}

\section{Experimental setup and hyperparameters}
\label{app:hyperparam}
We divide the experimental section into architecture modifications in Subsection~\ref{subsec:mod} and training and hyperparameter details in Subsection~\ref{subsec:tr_details}.
\subsection{Architecture modifications for \ours.}
\label{subsec:mod}
In our \ours experiments, we leveraged BigGAN and  StyleGAN2 backbones, and extended their architectures to handle the introduced instance conditionings. 

When using BigGAN as a base architecture, \ours replaces the class embedding layers in both generator and discriminator by fully connected layers. The fully connected layer in the generator has an input size of  (corresponding to the feature extractor 's dimensionality) and an output size  that can be adjusted. For all our experiments, we used  -- selected out of . The fully connected layer in the discriminator has a variable output size  to match the dimensionality of the intermediate unconditional discriminator feature vector -- following the practice in BigGAN~\cite{brock2018large}. 
For the class-conditional \ours, we use both the class embedding layers as well as the fully connected layers associated with the instance conditioning. In particular, we concatenate class embeddings (of dimensionality ) and instance embeddings (with dimensionality ). To avoid the rapid growth of parameters when using both class and instance embeddings, we use  as the output dimensionality for each of the embeddings in the discriminator, so that the resulting concatenation has a dimensionality of .

When using StyleGAN2 as a base architecture, we modify the class-conditional architecture of~\cite{karras2020training}. In particular, we replace the class embeddings layers with a fully connected layer of output dimensionality  in the generator. The fully connected layer substituting the class embedding in the discriminator is of variable size. In this case, the instance features are concatenated with the noise vector at the input of the StyleGAN2's mapping network, creating a \emph{style vector} for the generator. However, when it comes to the discriminator, the mapping network is only fed with the extracted instance features to obtain a modulating vector that is multiplied by the internal discriminator representation at each block.

All instance feature vectors  are normalized with  norm before computing the neighborhoods and when used as conditioning for the GAN. 


\subsection{Training details and hyperparameters}
\label{subsec:tr_details}
All models were trained while monitoring the training FID, and training was stopped according to either one of the following criteria: (1) early stopping when FID did not improve for  epochs -- or the equivalent number of iterations depending on the batch size --, or (2) when the training FID diverged. For BigGAN, we mainly explored the hyperparameter space around previously known and successful configurations~\cite{brock2018large,noroozi2020self}. Concretely, we focused on finding the following best hyperparameters for each dataset and resolution: the batch size (), model capacity controlled by channel multipliers (), number of discriminator updates versus generator updates (), discriminator learning rate () and generator learning rate (), while keeping all other parameters unchanged~\cite{brock2018large}. For StyleGAN, we also performed a hyperparameter search around previously known successful settings~\cite{karras2020training}. More precisely, we searched for the optimal  and  and R1 regularization weight  and used default values for the other hyperparameters. 


\paragraph{\ImNet.} When using the BigGAN backbone, in the \myres{64} resolution, we followed the experimental setup of~\cite{noroozi2020self}, where: , ,  and found that, although the unconditional BigGAN baseline achieves better metrics with , \ours and BigGAN do so with . Note that we explored additional configurations such as increasing  or  but did not observe any improvement upon the aforementioned setup. In both the \myres{128} and \myres{256} resolutions, BigGAN hyperparameters were borrowed from~\cite{brock2018large}. For \ours, we explored  and . 
For \myres{128}, we used ,  (as in~\cite{brock2018large}), ,  and . 
For \myres{256}, we set  and  (half capacity, therefore faster training) for both BigGAN and \ours, and used  with  for \ours. 
When using the StyleGAN2 architecture both as a baseline and as a backbone, we explored ,  and  and selected  and  and  for all resolutions.

\paragraph{COCO-Stuff.} When using BigGAN architecture, we explored  and   and found  and  to be the best choice. We searched for  and . For both unconditional BigGAN and \ours, we chose  and  in \myres{128} and  in \myres{256}. 
For both resolutions, unconditional BigGAN uses  and \ours, . When using StyleGAN2 architecture, we tried several learning rates  in combination with . For the unconditional StyleGAN2 and IC-GAN trained at resolution \myres{128}, we chose  with . At resolution \myres{256}, we found that  with  were optimal for IC-GAN while we obtained  with  for the unconditional StyleGAN.  

\paragraph{ImageNet-LT.} We explored  and  and found  and  to be the best configuration. We explored  and . In \myres{64}, we used ,  and  for both BigGAN and \ours setup. In \myres{128} and \myres{256}, we used   and .
  
  
\paragraph{Data augmentation.} We use horizontal flips to augment the real data fed to the discriminator in all experiments, unless stated otherwise. For COCO-Stuff and \ImNet-LT, we found that using translations with the DiffAugment framework~\cite{zhao2020differentiable} improves FID scores, as the number of training samples is significantly smaller than \ImNet (5\% and 10\% the size of \ImNet, respectively). However, we did not see any improvement in \ImNet dataset and therefore we do not use DiffAugment in our \ImNet experiments.
For \ImNet and COCO-Stuff, we augment the conditioning instance features  by horizontally flipping all data samples  and obtaining a new  from the flipped image, unless stated otherwise in the tables. This effectively doubles the number of conditionings available at training time, which have the same sample neighborhood  as their non-flipped versions. We tried applying this augmentation technique to \ImNet-LT but found that it degraded the overall metrics, possibly due to the different feature extractor used in these experiments. We hypothesize that the benefits of this technique are dependent on the usage of horizontal flips during the training stage of the feature extractor.
As seen in Table~\ref{table:coco_da_instance}, using data augmentation in the conditioning instance features slightly improves the results for \ours both when coupled with BigGAN and StyleGAN2 backbones in COCO-Stuff.

\paragraph{Compute resources.} We used NVIDIA V100 32GB GPUs to train our models. Given that we used different batch sizes for different experiments, we adapted the resources to each dataset configuration. In particular, \ImNet \myres{64} models were trained using 1 GPU, whereas \ImNet \myres{128} and \myres{256} models required 32 GPUs. \ImNet-LT \myres{64}, \myres{128} and \myres{256} used 1, 2 and 8 GPUs each, respectively. Finally, COCO-Stuff \myres{128} and \myres{256} required 4 and 16 GPUs, respectively, when using the BigGAN backbone, but required 2 and 4 GPUs when leveraging StyleGAN2.

 \begin{table}[h]
\centering
\footnotesize
\caption{Comparison between \ours with and without data augmentation using the  COCO-Stuff dataset. : 50\% chance of horizontally flipping data samples  to later obtain . The backbone for each \ours is indicated with the number of parameters between parentheses. To compute FID in the training split, we use a subset of  training instance features (selected with k-means) as conditionings.}
 \begin{tabular}{@{}lccccc@{}}
\toprule
 & \rotatebox[origin=c]{0}{\textbf{Backbone (M)}}  & \multicolumn{4}{c}{\textbf{FID}} \\
 & &\rotatebox[origin=c]{0}{\textbf{train}}  &  \rotatebox[origin=c]{0}{\textbf{eval}}   &
 \rotatebox[origin=c]{0}{\textbf{eval seen}}
&  \rotatebox[origin=c]{0}{\textbf{eval unseen}}  \\ 
  \midrule
 \multicolumn{3}{l}{128x128} \\ \midrule
\ours & BigGAN (22) & 18.0  0.1 & 45.5  0.7 & 85.0  1.1 & 60.6  0.9 \\
\ours & BigGAN (22) & \textbf{16.8}  0.1 & \textbf{44.9}  0.5 & \textbf{81.5}  1.3 & \textbf{60.5}  0.5 \\
\midrule
\ours  & StyleGAN2 (24) & 8.9  0.0 & 36.2  0.2 & 74.3  0.8 & 50.8  0.3\\
\ours & StyleGAN2 (24) & \textbf{8.7}  0.0  &  \textbf{35.8}  0.1 & \textbf{74.0}  0.7 & \textbf{50.5}  0.6\\
\midrule
\multicolumn{3}{l}{256x256} \\ \midrule
\ours & BigGAN (26) & 25.6  0.1 & 53.2  0.3 & 91.1  3.3 & \textbf{68.3}  0.9 \\
\ours & BigGAN (26) & \textbf{24.6}  0.1 & \textbf{53.1}  0.4 & \textbf{88.5}  1.8 & 69.1  0.6 \\
\midrule
\ours & StyleGAN2 (24.5) & 10.1  0.0 & 41.8  0.3 & 78.5  0.9 & 57.8  0.6 \\
\ours & StyleGAN2 (24.5) & \textbf{9.6}  0.0 & \textbf{41.4}  0.2 & \textbf{76.7}  0.6 & \textbf{57.5}  0.5 \\
\bottomrule
\end{tabular}
 \label{table:coco_da_instance}
 \end{table}

\section{Additional metrics: Precision and Recall}
\label{app:pr_metrics}
As additional measures of visual quality and diversity, we compute Precision (P) and Recall (R)~\cite{kynkaanniemi2019improved} in Table~\ref{table:pr_table}. Results are provided on the \ImNet dataset, following the experimental setup proposed in~\cite{naeem2020reliable}. 
By inspecting the results, we conclude that IC-GAN obtains better Recall (and therefore more diversity) than all the baselines in both the unlabeled and labeled settings, when selecting 10,000 random instances from the training set. Moreover,  when selecting 1,000 instances with k-means, which is the standard experimental setup we used across the paper,  we obtain higher Precision (as a measure of visual quality) than the other baselines in the unlabeled setting. 
In the labeled setting, the Precision is also higher than the one of BigGAN for 64x64 while being  lower for 128128 and 256256.

\begin{table}
\caption{Results for \ImNet in terms of Precision (P) and Recall (R)~\cite{kynkaanniemi2019improved} (bounded between 0 and 100), using 10,000 real and generated images. "Instance selection", only used for \ours, indicates whether 1,000 conditioning instances are selected with k-means (k-means 1,000) or 10,000 conditioning instances are sampled uniformly (random 10,000) from the training set to obtain 10,000 generated images in both cases.
 *: Generated images obtained with the paper's opensourced code.
\looseness-1}\label{table:pr_table}
\footnotesize
\centering
\begin{tabular}{@{}lllll@{}}
\toprule
\textbf{Method}
&  \textbf{Res.} & \textbf{Instance selection} & \textbf{P} & \textbf{R} \\  \midrule
\textit{Unlabeled setting}\\ 
\midrule
Uncond.\ BigGAN & 64 & - & 69.6  1.0 & 63.1  0.0\\
\ours & 64 & k-means 1,000 & \textbf{74.2}  0.8 & 60.2  0.6 \\
\ours & 64 & random 10,000 & 67.5  0.4 & \textbf{68.6}  0.5 \\
\midrule
Self-cond. GAN~\cite{liu2020diverse}* & 128 & - & 66.3  0.5 & 48.4  0.8 \\ 
\ours & 128 & k-means 1,000 & \textbf{78.2}  0.8 & 55.6  0.9 \\ 
\ours & 128 & random 10,000 & 71.7  0.3 & \textbf{69.7}  0.9 \\ 
\midrule
\ours & 256 & k-means 1,000 & \textbf{77.7}  0.5 & 54.3  0.7 \\ 
\ours & 256 & random 10,000 & 70.4  0.7 & \textbf{68.9}  0.3 \\ 
\midrule
\textit{Labeled setting}\\ 
\midrule
BigGAN~\cite{brock2018large} & 64 & - & 72.8  0.4 & 68.6  0.6\\
Class-conditional \ours & 64 & k-means 1,000 & \textbf{76.6}  0.7 & 67.5  0.8 \\ 
Class-conditional \ours & 64 & random 10,000 & 69.6  0.9 & \textbf{74.5}  0.8 \\ 
\midrule
BigGAN~\cite{brock2018large} & 128 & - & \textbf{83.2}  0.7 & 64.2  0.7\\
Class-conditional \ours & 128 & k-means 1,000 & 78.8  0.3 & 64.3  0.7 \\ 
Class-conditional \ours & 128 & random 10,000 & 72.2  0.4 & \textbf{73.6}  0.5 \\ 
\midrule
BigGAN~\cite{brock2018large} & 256 & - & \textbf{83.9}  0.6 & 70.2  0.7\\
Class-conditional \ours & 256 & k-means 1,000 & 82.2  0.3 & 70.4  0.3 \\ 
Class-conditional \ours & 256 & random 10,000 & 73.9  0.6 & \textbf{79.3}  0.2 \\ 

\bottomrule
\end{tabular}
\end{table}

\section{Comparison between StyleGAN2 and BigGAN backbones on \ImNet}
\label{app:stylegan_biggan}
We present additional experiments with \ours using the StyleGAN2 backbone in \ImNet in Table~\ref{table:unsup_in_stylegan}, comparing them to StyleGAN2 across all resolutions. \ours with a StyleGAN2 backbone obtains better FID and IS than StyleGAN2 across all resolutions, further supporting that \ours does not depend on a specific backbone, as already shown in the COCO-Stuff dataset in Table~\ref{table:coco_quantitative}. StyleGAN2, despite being designed for unconditional generation, is outperformed by the unconditional counterpart of BigGAN, that uses a single label for the entire dataset, in \ImNet. We suspect that there might be some biases introduced in the architecture at design time, as BigGAN was proposed for \ImNet and StyleGAN2 was tested on datasets with human faces, cars, and dogs, generally with presumably lower complexity and less number of data points than \ImNet. This intuition is further supported by StyleGAN2 improving over the BigGAN backbone in the COCO-Stuff experiments in Table~\ref{table:coco_quantitative}, as this dataset is much smaller than ImageNet and contains a lot of images where people are depicted. Interestingly, we qualitatively found that people and their faces are better generated with a StyleGAN2 backbone rather than the BigGAN one when trained on COCO-Stuff.


\begin{table}
\caption{Results for  \ImNet in unlabeled setting, comparing BigGAN and StyleGAN backbones.
For fair comparison with~\cite{noroozi2020self} at  resolution, we trained an unconditional BigGAN model and report the non-official FID and IS scores -- computed with Pytorch rather than  TensorFlow -- indicated with *. : increased parameters to match \ours capacity. DA: 50\% horizontal flips in real and fake samples (\textbf{d}), and conditioning instances (\textbf{i}). : Channel multiplier that affects network width.
\looseness-1}\label{table:unsup_in_stylegan}
\footnotesize
\centering
\begin{tabular}{@{}llll@{}}
\toprule
\textbf{Method}
&  \textbf{Res.} & \textbf{FID} & \textbf{IS} \\  \midrule
Uncond.\ BigGAN & 64 & 16.9*  0.0 & 14.6*  0.1 \\
\textbf{StyleGAN2} + DA (\textbf{d}) & 64 & 12.4*  0.0 & 15.4*  0.0 \\ 
\textbf{\ours (BigGAN)} + DA (\textbf{d},\textbf{i}) & 64 & 9.2*  0.0 & \textbf{23.5}*  0.1 \\ 
\textbf{\ours (StyleGAN2)} + DA (\textbf{d},\textbf{i}) & 64 &  \textbf{8.5}*  0.0 & \textbf{23.5}*  0.1 \\ 
\midrule
Uncond.\ BigGAN~\cite{pmlr-v97-lucic19a} & 128 & 25.3 & 20.4 \\
\textbf{StyleGAN2} + DA (\textbf{d}) & 128 &  27.8  0.1 & 18.8  0.1 \\ 
\textbf{\ours (BigGAN)} + DA (\textbf{d},\textbf{i}) & 128 & \textbf{11.7}  0.0 & \textbf{48.7}  0.1 \\ 
\textbf{\ours (StyleGAN2)} + DA (\textbf{d},\textbf{i}) & 128 & 15.2  0.1 & 38.3  0.2 \\ 
\midrule 

\textbf{StyleGAN2} + DA (\textbf{d}) & 256 &  41.3  0.1 & 19.7  0.1 \\ 
\textbf{\ours (BigGAN)} () + DA (\textbf{d},\textbf{i}) & 256 &  17.4  0.1 & 53.5  0.5 \\
\textbf{\ours (BigGAN)} () + DA (\textbf{d}) & 256 &  \textbf{15.6}  0.1 & \textbf{59.0}  0.4 \\
\textbf{\ours (StyleGAN2)} + DA (\textbf{d},\textbf{i}) & 256 &  23.1  0.1 & 42.2  0.2 \\ 
\bottomrule
\end{tabular}
\end{table}


\section{Additional qualitative results for \ours}
\label{app:add_qualitative}
\paragraph{Unlabeled \ImNet.} \ours generates high quality and diverse images that generally preserve the semantics and style of the conditioning. Figure~\ref{fig:icgan_qualitative_app} shows three instances -- a golden retriever in the water, a humming bird on a branch, and a landscape with a castle --, followed by their six closest nearest neighbors in the feature space of SwAV~\cite{caron2020unsupervised}, a ResNet50 model trained with self-supervision. 
Note that, although all neighbors contain somewhat similar semantics to those of the instance, the class labels do not always match. For example, one of the nearest neighbors of a golden retriever depicts a monkey in the water. The generated images depicted in Figure~\ref{fig:icgan_qualitative_app} are obtained by conditioning \ours with a BigGAN backbone on the features of the aforementioned instances. These highlight that generated images preserve the semantic content of the conditioning instance (a dog in the water, a bird with a long beak on a branch, and a landscape containing a water body) and present similarities with the real samples in the neighborhood of the instance. In cases such as the conditioning instance featuring a castle, the corresponding generated samples do not contain buildings; this could be explained by the fact that most of its neighbors do not contain castles either. Moreover, the generated images are not mere memorizations of training examples, as shown by the row of images immediately below, nor are they copies of the conditioning instance. 

\paragraph{Instance feature vector and noise interpolation.} In Figure~\ref{fig:app_interpolation}, we provide the resulting generated images when interpolating between the instance features of two data samples (vertical axis), shown on the left of each generated image grid, and additionally interpolating between two noise vectors in the horizontal axis. The top left quadrant shows generated images when interpolating between conditioning instance features from the class \textit{husky}. The generated dog changes its fur color and camera proximity according to the instance conditioning. At the top right corner, when interpolating between two \textit{mushroom} instance features, generated images change their color and patterns accordingly. Moreover, in the bottom left quadrant, \textit{lorikeet} instance features are interpolated with flying \textit{hummingbird} instance features, and the generated images change their color and appearance accordingly. Finally, in the bottom right grid, we interpolate instance features from a \textit{tiger} and instance features from a \textit{white wolf}, resulting in different blends between the striped pelt of the tiger and the white fur of the wolf. 

\paragraph{Unlabeled COCO-Stuff.} Training \ours with a StyleGAN2 backbone on COCO-Stuff has resulted in quantitative results that surpass those achieved by the state-of-the-art LostGANv2~\cite{sun2020learning} and OC-GAN~\cite{sylvain2020object}, controllable and conditional complex scene generation pipelines that rely on heavily labeled data (bounding boxes and class labels), tailored intermediate steps and somewhat complex architectures. In Figure~\ref{fig:coco_qualitative_sota}, we compare generated images obtained with LostGANv2 and OC-GAN with those generated by \ours with a StyleGAN2 backbone. Note that the two former methods use a bounding box layout with class labels as a conditioning, while we condition on the features extracted from the real samples  depicted in Figure~\ref{subfig:coco_app_input}. We compare the generations obtained with two random seeds for all methods, and observe that \ours generates higher quality images in all cases, especially for the top three instances. Moreover, the diversity in the generations using two random seeds for LostGANv2 and OC-GAN is lower than for \ours. This is not surprising, as the former methods are restricted by their bounding box layout conditioning that specifies the number of objects, their classes and their expected positions in the generated images. By contrast, \ours conditions on an instance feature vector, which does not require any object label, cardinality or position to be satisfied, allowing more freedom in the generations. 


 \begin{figure}
\centering
\begin{subfigure}{1\textwidth}
 \centering
\includegraphics[width=\textwidth]{figures/icgan_qualitative_app_1.png}
\end{subfigure}
\begin{subfigure}{1\textwidth}
 \centering
\includegraphics[width=\textwidth]{figures/icgan_qualitative_app_2.png}
\end{subfigure}
\begin{subfigure}{1\textwidth}
 \centering
\includegraphics[width=\textwidth]{figures/icgan_qualitative_app_3.png}
\end{subfigure}
\caption{Qualitative results on unlabeled \ImNet (\myres{256}). Next to each input sample , used to obtain the instance features , the six nearest neighbors in the feature space of  are displayed. Below the neighbors, generated images sampled from \ours with a BigGAN backbone and conditioned on  are depicted. Immediately below the generated images, the closest image in the \ImNet training set is shown for each example (cosine distance in the feature space of ).
}
\label{fig:icgan_qualitative_app}
\end{figure}


 \begin{figure}
\centering
\begin{subfigure}{0.48\textwidth}
 \centering
\includegraphics[width=\textwidth]{figures/interpolation_app2.png}
\end{subfigure}
\begin{subfigure}{0.48\textwidth}
 \centering
\includegraphics[width=\textwidth]{figures/interpolation_app3.png}
\end{subfigure}
\begin{subfigure}{0.48\textwidth}
 \centering
\includegraphics[width=\textwidth]{figures/interpolation_app1.png}
\end{subfigure}
\begin{subfigure}{0.48\textwidth}
 \centering
\includegraphics[width=\textwidth]{figures/interpolation_app4.png}
\end{subfigure}
\caption{Qualitative results on unlabeled \ImNet (\myres{256}) using \ours (BigGAN backbone) and interpolating between two instance feature vector conditionings (vertical axis) and two input noise vectors (horizontal axis).The two images depicted to the left of the generated image grids are used to extract the instance feature vectors used for the interpolation.}
\label{fig:app_interpolation}
\end{figure}

 \begin{figure}[h]
\centering
\begin{subfigure}[t]{0.1465\textwidth}
 \centering
\includegraphics[width=\textwidth]{figures/scene_gen_all_inputs.png}
\caption{ }
\label{subfig:coco_app_input}
\end{subfigure}
\begin{subfigure}[t]{0.26\textwidth}
 \centering
\includegraphics[width=\textwidth]{figures/lostganv2_gen.png}
\caption{LostGANv2~\cite{sun2020learning}}
\label{subfig:coco_app_lg2}
\end{subfigure}
\begin{subfigure}[t]{0.26\textwidth}
 \centering
\includegraphics[width=\textwidth]{figures/ocgan_gen.png}
\caption{OC-GAN~\cite{sylvain2020object} }
\label{subfig:coco_app_ocgan}
\end{subfigure}
\begin{subfigure}[t]{0.26\textwidth}
 \centering
\includegraphics[width=\textwidth]{figures/icgan_stylegan_gen.png}
\caption{\ours (StyleGAN2) }
\label{subfig:coco_app_input2}
\end{subfigure}

\caption{Qualitative comparison for scene generation on \myres{256} COCO-Stuff with other state-of-the-art scene generation methods. (a) Data samples  from which instance features  are obtained for \ours, and labeled bounding box conditionings are obtained for LostGANv2 and OC-GAN. Images generated with two random seeds with (b) LostGANv2~\cite{sun2020learning}, (c) OC-GAN~\cite{sylvain2020object}, (d) \ours (StyleGAN2).
}
\label{fig:coco_qualitative_sota}
\end{figure}

\paragraph{\ImNet.} Class-conditional \ours with a BigGAN backbone has shown comparable quantitative results to those of BigGAN for \myres{256} resolution in Subsection~\ref{sec:supervised}. In Figure~\ref{fig:icgan_cc_qualitative_app}, we can qualitatively compare BigGAN () (first rows) and \ours () (second and third rows), for three class labels: \textit{goldfish}, \textit{limousine} and \textit{red fox}. By visually inspecting the generated images, we can observe that the generation quality is similar for both BigGAN and \ours in these specific cases. Moreover, \ours allows controllability of the semantics by changing the conditioning instance features. For instance, changing the background in which the goldfish are swimming into lighter colors in Figure~\ref{app:goldfish}, generating limousines in generally dark and uniform backgrounds or, instead, in an urban environment with a road and buildings (Figure~\ref{app:limousine}), or generating red foxes with a close up view or with a full body shot as seen in Figure~\ref{app:red_fox}.

\paragraph{Swapping classes for class-conditional \ours on \ImNet.} 
In Figure~\ref{fig:icgan_cc_qualitative_app}, we show that we can change the appearance of the generated images by leveraging different instances of the same class.
In Figure~\ref{fig:icgan_cc_qualitative_swap_app}, we take a further step and condition on instance features from other classes. More specifically, in Figure~\ref{fig:icgan_cc_qualitative_swap_app} (top), we condition on the instance features of a snowplow in the woods surrounded by snow, and ask to generate snowplows, camels and zebras. Perhaps surprisingly, the generated images effectively get rid of the snowplow, and replace it by camel-looking and zebra-looking objects, respectively, while maintaining a snowy background in the woods. 
Moreover, when comparing the generated images with the closest samples in \ImNet, we see that for generated camels in the snow, the closest images are either a camel standing in dirt or other animals in the snow; for the generated zebras in the snow, we find one sample of a zebra standing in the snow, while others are standing in other locations/backgrounds. 
In Figure~\ref{fig:icgan_cc_qualitative_swap_app} (bottom), we condition on the features of an instance that depicts a golden retriever on a beach with overall purple tones, and ask to generate golden retrievers, camels or zebras. In most cases, generated images contain camels and zebras standing on water, while other generations contain purple or blue tones, similar to the instance used as conditioning. Note that, except one generated zebra image, the closest samples in \ImNet do not depict camels or zebras standing in the water nor on the beach.




 \begin{figure}
\centering
\begin{subfigure}{1\textwidth}
 \centering
\includegraphics[width=\textwidth]{figures/icgan_cc_qualitative_app_text1.png}
\caption{Class label \textit{Goldfish}}
\label{app:goldfish}
\end{subfigure}
\begin{subfigure}{1\textwidth}
 \centering
\includegraphics[width=\textwidth]{figures/icgan_cc_qualitative_app_text2.png}
\caption{Class label \textit{Limousine}}
\label{app:limousine}
\end{subfigure}
\begin{subfigure}{1\textwidth}
 \centering
\includegraphics[width=\textwidth]{figures/icgan_cc_qualitative_app_text3.png}
\caption{Class label \textit{Red fox}}
\label{app:red_fox}
\end{subfigure}
\caption{Qualitative results in \myres{256} \ImNet. For each class, generated images with BigGAN are presented in the first row, while the second and third row show generated images using class-conditional \ours with a BigGAN backbone, conditioned on the instance feature extracted from the data sample to their left () and their corresponding class.}
\label{fig:icgan_cc_qualitative_app}
\end{figure}



 \begin{figure}
\centering
\begin{subfigure}{0.90\textwidth}
 \centering
\includegraphics[width=\textwidth]{figures/cc_icgan_class_swap_app_text1.png}
\end{subfigure}
\begin{subfigure}{0.90\textwidth}
 \centering
\includegraphics[width=\textwidth]{figures/cc_icgan_class_swap_app_text2.png}
\end{subfigure}
\begin{subfigure}{0.90\textwidth}
 \centering
\includegraphics[width=\textwidth]{figures/cc_icgan_class_swap_app_text3.png}
\end{subfigure}
\begin{subfigure}{0.90\textwidth}
 \centering
\includegraphics[width=\textwidth]{figures/cc_icgan_class_swap_app_text4.png}
\end{subfigure}
\begin{subfigure}{0.90\textwidth}
 \centering
\includegraphics[width=\textwidth]{figures/cc_icgan_class_swap_app_text5_blurred.png}
\end{subfigure}
\begin{subfigure}{0.90\textwidth}
 \centering
\includegraphics[width=\textwidth]{figures/cc_icgan_class_swap_app_text6.png}
\end{subfigure}

 
\caption{Generated \myres{256} images with a class-conditional \ours (BigGAN backbone) trained on \ImNet. Next to each data sample , used to obtain the instance features , we find generated images sampled from \ours using  and six sampled noise vectors. Below the generated images, the closest image in the \ImNet training set are shown (Cosine similarity in the feature space of ).}
\label{fig:icgan_cc_qualitative_swap_app}
\end{figure}



\section{Additional off-the-shelf transfer results for \ours}
\label{app:add_transfer}
\paragraph{Is \ours able to shift the generated data distribution by conditioning on different instances?} As discussed in Section~\ref{sec:transfer}, we can transfer an \ours trained on unlabeled \ImNet to COCO-Stuff and obtain better metrics and qualitative results than with the same \ours trained on COCO-Stuff.
We hypothesize that the success of this experiment comes from the flexibility of our conditioning strategy, where the generative model exploits the generalization capabilities of the feature extractor when dealing with unseen instances to shift the distribution of generated images from \ImNet to COCO-Stuff. To test this hypothesis we design the following experiment: we compute FID scores of generated images obtained by conditioning \ours with instance features from either \ImNet or COCO-Stuff and use either COCO-Stuff or \ImNet as a reference dataset to compute FID. In Table~\ref{table:ablation_transfer_shift} (first row) we show that when using COCO-Stuff for both the instance features and the reference dataset, \ours scores 8.5 FID; this is a lower FID score than the 43.6 FID obtained in Table~\ref{table:ablation_transfer_shift} (second row) when conditioning \ours on \ImNet instance features and using COCO-Stuff as reference dataset. Moreover, when using COCO-Stuff instance features and \ImNet as reference dataset, in Table~\ref{table:ablation_transfer_shift} (third row), we obtain 37.2 FID. This shows that, by changing the conditioning instance features, \ours successfully exploits the generalization capabilities of the feature extractor to shift the distribution of generated images to be closer to the COCO-Stuff distribution. Additionally, note that the distance between ImageNet and COCO-Stuff datasets can be quantified with an FID score of 37.2~\footnote{We subsampled  ground-truth images from ImageNet training set and used all COCO-Stuff training ground-truth images.}.

\begin{table}[h!]
\centering
\footnotesize
\caption{FID scores on COCO-Stuff \myres{128}, when using an \ours trained on ImageNet and tested with instance features from either COCO-Stuff or \ImNet and using either of those datasets as reference. The metrics obtained by sampling  instance features (k-means) from either ImageNet or COCO, and generating  samples. As a reference,  real samples from COCO-Stuff or ImageNet training set are used.}
 \begin{tabular}{@{}lcccc@{}}
\toprule
 & train instance dataset & eval instance dataset & FID reference dataset & \textbf{FID} \\  \midrule
\ours & ImageNet & COCO-Stuff & COCO-Stuff & \textbf{8.5}  0.1 \\
\ours & ImageNet & ImageNet & COCO-Stuff  & 43.6  0.1 \\
\ours & ImageNet & COCO-Stuff & ImageNet & 37.2  0.1 \\ 
\bottomrule
\end{tabular}
 \label{table:ablation_transfer_shift}
 \end{table}
\paragraph{What is being transferred when \ours is conditioned on instances other than the ones in the training dataset?} From the point of view of KDE, what is being transferred is the kernel shape, not the kernel location (that is controlled by instances). The kernel shape is predicted using a generative model from each input instance and we probe the kernel via sampling from the generator. Thus, we transfer a function that predicts kernel shape from a conditioning, and this function seems to be robust to diverse instances as shown in the paper (e.g. see Figure~\ref{subfig:teaser_unsup_transfer} and~\ref{subfig:teaser_class_transfer}). 
Moreover, by visually inspecting the generated images in our transfer experiments, we observed that when transferring an IC-GAN trained on ImageNet to COCO-Stuff, if the model is conditioned on images that contain unseen classes in ImageNet, such as “giraffe”, the model will still generate an animal that would look like a giraffe without the skin patterns and characteristic antennae, because ImageNet contains other animals to draw inspiration from. This suggests that the model generates plausible images that have some similar features to those present in the instance conditioning, but adapting it to the training dataset style. Along these lines, we also observed that in some cases, shapes and other object characteristics from one dataset are transferred to another (ImageNet to COCO-Stuff). Moreover, when we conditioned on instances from Cityscapes, the generated images were rather colorful, resembling more the color palette of ImageNet images rather than the Cityscapes one. 

\paragraph{Off-the-shelf transfer results for \ours.} In Figure~\ref{fig:icgan_transfer_app}, we provide additional generated samples and their closest images in the \ImNet training set, when conditioning on unseen instance features from other datasets. 
Generated images often differ substantially from the closest image in \ImNet. Although generated images using a COCO-Stuff and Cityscapes instances may have somewhat similar looking images in \ImNet (for the first and second instances in Figure~\ref{fig:icgan_transfer_app}), the differences accentuate when conditioning on instance features from MetFaces, PACS or Sketch datasets, where, for instance, \ours with a BigGAN backbone generates images resembling sketch strokes in the last row, even if the closest \ImNet samples depict objects that are not sketches.

\paragraph{Off-the-shelf transfer results for class-conditional \ours.} In Figure~\ref{fig:cc_icgan_transfer_app}, we show additional results when transferring a class-conditional \ours with a BigGAN backbone trained on \ImNet to other datasets, using an \ImNet class label but an unseen instance. We are able to generate camels in the grass by conditioning on an image of a cow in the grass from COCO-Stuff, we show generated images with a zebra in an urban environment by conditioning on a Cityscapes instance, and we generate cartoon-ish birdhouses by conditioning on a PACS cartoon house instance. This highlights the ability of class-conditional \ours to transfer to other datasets \emph{styles} while maintaining the class label semantics. 
 
 
 \begin{figure}
\centering
\begin{subfigure}{1\textwidth}
 \centering
\includegraphics[width=\textwidth]{figures/icgan_transfer_app_1.png}
\end{subfigure}
\begin{subfigure}{1\textwidth}
 \centering
\includegraphics[width=\textwidth]{figures/icgan_transfer_app_2.png}
\end{subfigure}
\begin{subfigure}{1\textwidth}
 \centering
\includegraphics[width=\textwidth]{figures/icgan_transfer_app_3.png}
\end{subfigure}
\begin{subfigure}{1\textwidth}
 \centering
\includegraphics[width=\textwidth]{figures/icgan_transfer_app_4.png}
\end{subfigure}
\begin{subfigure}{1\textwidth}
 \centering
\includegraphics[width=\textwidth]{figures/icgan_transfer_app_5.png}
\end{subfigure}
\caption{Qualitative off-the-shelf transfer results in \myres{256}, using an \ours trained on unlabeled \ImNet and conditioning on unseen instances from other datasets. The instances come from the following datasets (top to bottom): COCO-Stuff, Cityscapes, MetFaces, PACS (cartoons), Sketches. 
Next to each data sample , used to obtain the instance features , generated images conditioning on  are displayed. Immediately below each generated image, the closest image in the \ImNet training set is displayed (Euclidean distance in the feature space of ).
}
\label{fig:icgan_transfer_app}
\end{figure}

 \begin{figure}
\centering
\begin{subfigure}{1\textwidth}
 \centering
\includegraphics[width=\textwidth]{figures/cc_icgan_transfer_app_text1.png}
\end{subfigure}
\begin{subfigure}{1\textwidth}
 \centering
\includegraphics[width=\textwidth]{figures/cc_icgan_transfer_app_text2.png}
\end{subfigure}
\begin{subfigure}{1\textwidth}
 \centering
\includegraphics[width=\textwidth]{figures/cc_icgan_transfer_app_text3.png}
\end{subfigure}
\caption{Qualitative off-the-shelf transfer results in \myres{256}, using a class-conditional \ours trained on \ImNet and conditioning on unseen instances from other datasets and a class label. The instances come from the following datasets (top to bottom): COCO-Stuff, Cityscapes, PACS (cartoons).
On the left, a data sample  is depicted, used to obtain the instance features . Next to the data samples, generated images conditioning on  and a class label (under the data samples) are displayed. Just below the generated images, the closest image in the \ImNet training set are shown (Euclidean distance in the feature space of ).
}
\label{fig:cc_icgan_transfer_app}
\end{figure}

\section{Class balancing in \ImNet-LT}
\label{app:class_balacing}
We experimented with class balancing for BigGAN in the \ImNet-LT dataset.
In Table~\ref{table:imagenet_lt_balancing}, we compare (1) \textbf{BigGAN}, where both the class distribution for the generator and the data for the discriminator are long-tailed; (2) \textbf{BigGAN (CB)}, a class-balanced version of BigGAN, where the generator samples class labels from a uniform distribution and the samples fed to the discriminator are also class-balanced; and (3) \textbf{BigGAN (T\,=\,2)} where the class distribution is balanced with a softmax temperature of T\,=\,2 providing a middle ground between the long-tail and the uniform distributions. In the latter case, the probability to sample class  (with a frequency  in the long-tailed training set) during training is given by .

Interestingly, balancing the class distribution (either with uniform class distribution or with T=2) harmed performance in all cases except for the validation Inception Score. We hypothesize that oversampling rare classes, and thus the few images therein, may result in overfitting for the discriminator, leading to low quality image generations. 

\begin{table}[h]
\small
\centering
\caption{ImageNet-LT quantitative results for different class balancing techniques.
"t.": training and "v.": validation.}
 \begin{tabular}{@{}lc|cc|ccc@{}}
\toprule
 & \textbf{Res.} & \textbf{t. FID} & \textbf{t. IS} & \textbf{v. FID} & \textbf{v. [many/med/few] FID} & \textbf{v. IS} \\  \midrule
BigGAN & 64 & \textbf{27.6}  0.1 & \textbf{18.1}  0.2 & \textbf{28.1}  0.1  & \textbf{28.8/32.8/48.4}  0.2 & 16.0  0.1  \\
BigGAN (CB) & 64 & 62.1  0.1 & 10.7  0.2 & 56.2  0.1 & 62.2/59.7/74.7  0.2 & 11.0  0.0  \\
BigGAN (T\,=\,2) & 64 & 30.6  0.1 & 16.8  0.1 & 29.2  0.1  & 30.9/33.3/49.5  0.2 & \textbf{16.4}  0.1 \\
\bottomrule
\end{tabular}

 \label{table:imagenet_lt_balancing}
 \end{table}
 
 
 
\section{Choice of feature extractor}
\label{app:feature_extractor}

We study the choice of the feature extractor used to obtain instance features in Table~\ref{table:ablation_knn_feat}. We compare results using an \ours with a BigGAN backbone when coupling it with a ResNet50 feature extractor trained with either self supervision (SwAV) or with supervision for classification purposes (RN50) on \ImNet dataset. Results highlight similar \ours performances for both feature extractors, suggesting that the choice of feature extractor that does not greatly impact the performance of our method when leveraging unlabeled datasets. Given that for the unlabeled scenario we assume no labels are available, we use the SwAV feature extractor. However, in the class-conditional case, we observe that the \ours coupled with a RN50 feature extractor surpasses \ours coupled with a SwAV feature extractor. Therefore, we choose the RN50 feature extractor for the class-conditional experiments. For \ImNet-LT, we transfer these findings and use a RN50 trained on \ImNet-LT as feature extractor, assuming we do not have access to the entire \ImNet dataset and its labels.
 
\begin{table}[h]
\footnotesize 
\centering
\caption{Feature extractor impact with SwAV (ResNet50 trained with a self-supervised approach) and RN50 (ResNet50 trained for the classification task in ImageNet). Experiments performed in \myres{64} ImageNet, using  training instance features at test time, selected with k-means.}
 \begin{tabular}{@{}lc@{}}
\toprule
  & \textbf{FID} \\ 
 \midrule
\ours + SwAV & 11.7  0.1 \\ 
\ours + RN50  & \textbf{11.3}  0.1 \\ 
\midrule
Class-conditional \ours + SwAV & 9.9  0.1  \\ 
Class-conditional \ours + RN50  & \textbf{8.5}  0.0 \\
\bottomrule
\end{tabular}
 \label{table:ablation_knn_feat}
 \end{table}


\section{Number of conditioning instance features at train time}
\label{app:num_cond_train}
To demonstrate that using many fine-grained overlapping partitions results in better performance than using a few coarser partitions, we trained \ours with a BigGAN backbone by conditioning on all 1.2M training instance features at training time in \ImNet and a neighborhood size of , and compared it quantitatively with an \ours trained by conditioning on only  instance features at training time. In this case, we extend the neighborhood size to  to better cover the training distribution~\footnote{Note that this setup resembles the class partition in \ImNet, where  classes contain approximately  images each.}. Note that using  would result in using at most  training samples during training, an unfair comparison. The  instance features are selected by applying k-means to the entire \ImNet training set. We then use the same instances to generate images. Results are presented in Table~\ref{table:ablation_num_instances_train} and emphasize the importance of training with all available instances, which results in significantly better FID and IS presumably due to the increased number of conditionings and their smaller neighborhoods. 


\begin{table}[h]
\footnotesize 
\centering
\caption{Comparison between training \ours (BigGAN backbone) using only  conditioning instance features (selected with k-means) or all training instance features during training, in \ours \myres{64}. At test time, we condition \ours on  training instance features, selected with k-means.}
 \begin{tabular}{@{}lcc@{}}
\toprule
 \textbf{Method}
&  \textbf{FID} & \textbf{IS} \\  \midrule
\ours (, trained with all 1.2M conditionings)  & \textbf{11.7}  0.1 & \textbf{21.6}  0.1 \\ \ours (, trained with only  conditionings)  & 24.8  0.1 & 16.4  0.1 \\ \bottomrule
\end{tabular}
 \label{table:ablation_num_instances_train}
 \end{table}
 
 
\section{Matching storage requirements for \ours and unconditional models}
\label{app:fair_comparison}
We hypothesize that the good performance of \ours on \ImNet and COCO-Stuff can not solely be attributed to the slight increase of parameters and the extra memory required to store the instance features used at test time, but also to the \ours design, including the finegrained overlapping partitions and the instance conditionings. To test for this hypothesis, we performed experiments with the unconditional BigGAN baseline on \ImNet and COCO-Stuff, training it by setting all labels in the training set to zero, following~\cite{pmlr-v97-lucic19a,noroozi2020self}, and increasing the generator capacity such that it matches the \ours storage requirements. In particular, we not only endowed the unconditional BigGAN with additional parameters to compensate for the capacity mismatch, but also for the instances required by \ours. Moreover, we performed analogous experiments for the unconditional StyleGAN2 in COCO-Stuff.

\paragraph{\ImNet.} Given its instance conditioning, the \ours (BigGAN backbone) generator introduces an additional 4.5M parameters when compared to the unconditional BigGAN generator. Moreover, \ours requires an extra 8MB to store the  instance features used at inference time. This 8MB can be translated into roughly 2M parameters\footnote{We store both parameters and instance features as float32.}. 
Therefore, we compensate for this additional storage in \ours by increasing the unconditional BigGAN capacity by expanding the width of both generator and discriminator hidden layers. We follow the practice in~\cite{brock2018large}, where the generator and discriminator's width are changed together. 
The resulting unconditional BigGAN baseline generator has an additional 6.5M parameters. 
Results are reported in Table~\ref{table:ablation_fair_capacity_imagenet}, showing that adding extra capacity to the unconditional BigGAN leads to an improvement in terms of FID and IS. However, \ours still exhibits significantly better FID and IS, highlighting that the improvements cannot be solely attributed to the increase in parameters nor instance feature storage requirements. 

\paragraph{COCO-Stuff.} Similarly, \ours (BigGAN backbone) trained on COCO-Stuff requires 4M additional parameters on top of the extra storage required by the  stored instance features (8MB again translated into roughly 2M parameters). Therefore, we add 6M extra parameters to the unconditional BigGAN generator. In the case of \ours with a StyleGAN2 backbone, the instance feature conditionings constitute 1M additional parameters. We therefore increase the capacity of the unconditional StyleGAN2 model by 3M to match the storage requirements of \ours (StyleGAN2 backbone). The results are presented in Table~\ref{table:ablation_fair_capacity_coco}, where it is shown that both the unconditional BigGAN and unconditional StyleGAN2 do not take advantage of the additional capacity and achieve poorer performance than the model with lower capacity, possibly due to overfitting. When compared to \ours, the results match the findings in the \ImNet dataset: \ours exhibits lower FID when using BigGAN or StyleGAN2 backbones, compared to their respective unconditional models with the same storage requirements, further highlighting that \ours effectively benefits from its design, finegrained overlapping partitions, and instance conditionings.


\begin{table}[h]
\footnotesize 
\centering
\caption{Comparing \ours with the unconditional counterparts of BigGAN on \myres{64} \ImNet with the same storage requirements. 
Storage-G counts the storage required for the generator, Storage-I the storage required for the training instance features, and Storage-All is the sum of both generator and instance features required storage. FID and IS scores are computed using Pytorch code.}
 \begin{tabular}{@{}lrrrrcc@{}}
\toprule
 \textbf{Method}
&  \textbf{\#prms.} & \textbf{Storage-G} &\textbf{Storage-I} & \textbf{Storage-All}  & \textbf{FID} & \textbf{IS} \\ 
\midrule
Unconditional BigGAN & 32.5M & 124MB & 0MB & 124MB & 30.0  0.1 & 12.1  0.1 \\
Unconditional BigGAN & 39M & 149MB & 0MB & 149MB & 16.9  0.0 & 14.6  0.1 \\
\ours (BigGAN) & 37M & 141MB & 8MB & 149MB & \textbf{10.4}  0.1 & \textbf{21.9}  0.1 \\ \bottomrule
\end{tabular}

 \label{table:ablation_fair_capacity_imagenet}
 \end{table}
 
\begin{table}[h]
\footnotesize 
\centering
\caption{Comparing \ours with the unconditional counterparts on \myres{128} COCO-Stuff, with the same storage requirements. 
Storage-G counts the storage required for the generator, Storage-I the storage required for the training instance features, and Storage-All is the sum of both generator and instance features required storage.
}
 \begin{tabular}{@{}lrrrrcc@{}}
\toprule
 & \textbf{\#prms.} & \textbf{Storage-G} &\textbf{Storage-I} & \textbf{Storage-All} &  \multicolumn{2}{c}{\textbf{FID}} \\ 
  & & & & &  \textbf{train} &  \textbf{eval} \\ 
   \midrule
Unconditional BigGAN & 18M & 68MB & 0MB & 68MB & 17.9  0.1 & 46.9  0.5 \\ 
Unconditional BigGAN & 24M & 92MB & 0MB & 92MB & 28.8  0.1 & 58.1  0.5 \\
\ours (BigGAN) & 22M & 84MB & 8MB & 92MB & \textbf{16.8}  0.1 & \textbf{44.9}  0.5 \\ 
\midrule
Unconditional StyleGAN2 & 23M & 88MB & 0MB & 88MB & 8.8  0.1 & 37.8  0.2 \\
Unconditional StyleGAN2 & 26M & 100MB & 0MB & 100MB & 9.4  0.0 & 38.4  0.3\\
\ours (StyleGAN2) & 24M & 92MB & 8MB & 100MB & \textbf{8.7}  0.0 & \textbf{35.8}  0.1\\
\bottomrule
\end{tabular}
 \label{table:ablation_fair_capacity_coco}
 \end{table}
 
 
 
\section{Additional neighborhood size impact studies}
\label{app:k_size}
We additionally study the impact of the neighborhood size for \ImNet-LT in Table~\ref{table:ablation_lt} and in COCO-Stuff in Table~\ref{table:ablation_coco}, showing that in both cases, \ours with a BigGAN backbone and  achieves the best FID and IS metrics. The choice of a lower neighborhood size  than in the \ImNet case () could suggest that the number of semantically similar neighboring samples is smaller for these two datasets. This wouldn't be completely surprising given that these two datasets are considerably smaller than \ImNet. Increasing the value of  in COCO-Stuff and \ImNet-LT would potentially gather samples with different semantics within a neighborhood, which could potentially harm the training and therefore the generated images quality.
 
Finally, in Figure~\ref{fig:icgan_k_effect}, we qualitatively show generated images in \ImNet when using an \ours trained with varying neighborhood sizes. The findings further support the ones presented in Subsection~\ref{sec:ablation}, showing that smaller neighborhoods result in generated images with less diversity, while bigger neighborhood sizes, for example  result in more varied but lower quality generations, supporting that  controls the smoothing effect. 
 
 
 
\begin{table}[h]
\footnotesize 
\caption{Impact of the number of neighbors () used to train class-conditional \ours (BigGAN backbone) in ImageNet-LT \myres{64}. Reported results in ImageNet validation set. As a feature extractor, a ResNet50 is trained as a classifier on the same dataset is used. 50k instance features are sampled from the training set.}
\centering
\begin{tabular}{@{}lcc@{}}
\toprule
 & \textbf{FID} & \textbf{IS}  \\ \midrule
\,=\,5 & \textbf{23.4}  0.1 & \textbf{17.6}  0.1 \\ 
\,=\,20 & 24.1  0.1 & 16.8  0.1 \\ 
\,=\,50 & 24.1  0.1 & 16.7  0.1 \\ 
\,=\,100 & 25.6  0.1 & 16.3  0.1 \\
\,=\,500 & 27.1  0.1 & 15.3  0.1 \\ 
\bottomrule
\end{tabular}
\label{table:ablation_lt}
\end{table}
 
 
  
\begin{table}[h]
\footnotesize 
\caption{Impact of the number of neighbors () used to train \ours (BigGAN backbone) on COCO-Stuff \myres{128}. Reported results on COCO-Stuff evaluation set. As a feature extractor, a ResNet50 trained with self-supervision (SwAV) is used.}
\centering
 \begin{tabular}{@{}lcc@{}}
\toprule
 & \textbf{FID} \\ \midrule
\,=\,5 & \textbf{44.9}  0.5 \\ 
\,=\,20 & 46.8  0.3 \\ 
\,=\,50 & 45.8  0.4 \\ 
\,=\,100 & 48.4  0.3 \\ 
\,=\,500 & 48.3  0.5 \\
\bottomrule
\end{tabular}
  \label{table:ablation_coco}
 \end{table}
 
 
 
 \begin{figure}[h]
\footnotesize 
\centering
\begin{subfigure}{0.1\textwidth}
 \centering
\includegraphics[width=\textwidth]{figures/bird_input.png}
\caption{}
\label{subfig:icgan_unlabeled_input1}
\end{subfigure}
\begin{subfigure}{0.31\textwidth}
 \centering
\includegraphics[width=\textwidth]{figures/bird_k5_gen.png}
\caption{\ours trained with }
\label{subfig:bird_k5}
\end{subfigure}
\begin{subfigure}{0.31\textwidth}
 \centering
\includegraphics[width=\textwidth]{figures/bird_k20_gen.png}
\caption{\ours trained with }
\label{subfig:bird_k20}
\end{subfigure}
\begin{subfigure}{0.31\textwidth}
 \centering
\includegraphics[width=\textwidth]{figures/bird_k50_gen.png}
\caption{\ours trained with }
\label{subfig:bird_k50}
\end{subfigure}
\begin{subfigure}{0.31\textwidth}
 \centering
\includegraphics[width=\textwidth]{figures/bird_k100_gen.png}
\caption{\ours trained with }
\label{subfig:bird_k100}
\end{subfigure}
\begin{subfigure}{0.31\textwidth}
 \centering
\includegraphics[width=\textwidth]{figures/bird_k500_gen.png}
\caption{\ours trained with }
\label{subfig:bird_k500}
\end{subfigure}
\caption{Qualitative results in \myres{64} unlabeled \ImNet when training \ours (BigGAN backbone) with different neighborhood sizes . (a) Data samples  used to obtain the instance features .  (b-f) Generated images with \ours (BigGAN backbone), sampling different noise vectors, for different neighborhood sizes  used during training.}
\label{fig:icgan_k_effect}
\end{figure}