\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{pifont}
\usepackage{color}
\usepackage{xcolor}

\usepackage{algorithm}
\usepackage{listings}

\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}




\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy 

\def\iccvPaperID{****} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}



\usepackage{booktabs}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\captionsetup[table]{skip=10pt}

\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}

\newcommand{\mbb}[1]{\mathbb{#1}}
\newcommand{\mbf}[1]{\mathbf{#1}}
\newcommand{\op}[1]{\operatorname{#1}}

\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}



%
 
\begin{document}

\title{Escaping the Big Data Paradigm with Compact Transformers}

\author{Ali Hassani\textsuperscript{1}\thanks{Equal contribution. Our code and pre-trained models will be made publicly available at \href{https://github.com/SHI-Labs/Compact-Transformers}{https://github.com/SHI-Labs/Compact-Transformers}}, Steven Walton\textsuperscript{1}\footnotemark[1] \,, Nikhil Shah\textsuperscript{1},\\Abulikemu Abuduweili\textsuperscript{1}, Jiachen Li\textsuperscript{2,1}, Humphrey Shi\textsuperscript{1,2,3} \\
{\small \textsuperscript{1}SHI Lab  University of Oregon, \textsuperscript{2}University of Illinois at Urbana-Champaign, \textsuperscript{3}Picsart AI Research (PAIR)}\\
}

\maketitle

\begin{abstract}
    With the rise of Transformers as the standard for language processing, and their advancements in computer vision, along with their unprecedented size and amounts of training data, many have come to believe that they are not suitable for small sets of data. This trend leads to great concerns, including but not limited to: limited availability of data in certain scientific domains and the exclusion of those with limited resource from research in the field.
    In this paper, we dispel the myth that transformers are ``data hungry'' and therefore can only be applied to large sets of data. 
    We show for the first time that with the right size and tokenization, transformers can perform head-to-head with state-of-the-art CNNs on small datasets.
    Our model eliminates the requirement for class token and positional embeddings through a novel sequence pooling strategy and the use of convolutions. 
    We show that compared to CNNs, our compact transformers have fewer parameters and MACs, while obtaining similar accuracies. 
    Our method is flexible in terms of model size, and can have as little as  M parameters  and achieve reasonable results.
    It can reach an accuracy of  when training from scratch on CIFAR-10, which is comparable with modern CNN based approaches, and a significant improvement over previous Transformer based models.
     Our simple and compact design democratizes transformers by making them accessible to those equipped with basic computing resources and/or dealing with important small datasets.
\end{abstract}

\section{Introduction}

\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{assets/images/model.pdf}
    \caption{Overview of Compact Convolutional Transformer (\textbf{CCT}): the convolutional variant of our compact transformer models. CCT can be quickly be trained from scratch on small datasets, while achieving high accuracy 
    (in under 30 minutes one can get 90\% on an NVIDIA 2080Ti GPU or 80\% on an AMD 5900X CPU).
    With CCT the class token becomes unnecessary and positional embedding becomes optional, resulting in only slightly different accuracy. 
    }
    \label{fig:method_overview}
\end{figure}

Transformers have rapidly been increasing in popularity and become a major focus of modern machine learning research. With the advent of Attention is All You Need\cite{vaswani2017attention}, the community saw an explosion in transformer and attention based research. While this work originated in natural language processing, these models have been applied to other fields, such as computer vision. 
Vision Transformer (ViT) \cite{dosovitskiy2020image} was the first major demonstration of a pure transformer backbone being applied to computer vision tasks. With this explosion in research, we've also seen an explosion in model sizes and datasets. It has become a common understanding that transformers are data hungry models and that they need sufficiently large datasets to perform as well or better than their convolutional counterparts.
ViT authors argued that ``\textit{Transformers lack some of the inductive biases inherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well when trained on insufficient amounts of data.}''.

This ``data hungry'' paradigm has made transformers intractable for many types of pressing problems, where there are typically several orders of magnitude less data. It also limits major contributions in the research to those with vast computational resources. Reducing machine learning's dependence on large sums of data is important, as many domains, such as science and medicine, would hardly have datasets the size of ImageNet. This is because events are far more rare and it would be more difficult to properly assign labels, let alone create a set of data has low bias and is appropriate for conventional deep neural networks. 
In medical research, for instance, it may be difficult to compile positive samples of images for a rare disease without other correlating factors, such as medical equipment being attached to patients who are actively being treated. 
Additionally, for a sufficiently rare disease there may only be a few thousand images for positive samples, which is typically not enough to train a network with good statistical prediction unless it can sufficiently be pre-trained on data with similar attributes.
This inability to handle smaller datasets has impacted the scientific community where they are much more limited in the models and tools that they are able to explore. 
Frequently, problems in scientific domains have little in common with domains of pre-trained models and when domains are sufficiently distinct pre-training can have little to no effect on the performance within a new domain~\cite{zhuang2019a}.
Additionally, it has been shown that performing strongly on ImageNet does not necessarily mean that the model will perform strongly in other domains, such as medicine~\cite{ke2021chextransfer}. 
A non-``data hungry'' transformer would allow these domains to utilize the advantages of long range interdependence that self-attention provides within their research and applications.
This makes it important to build more efficient models that can be effective in less data intensive domains and allow for datasets that are orders of magnitude less than those conventionally seen in computer vision and NLP problems. 

Additionally, the requisite of large data results in a requisite of large computational resources and this subsequently prevents many researchers from being able to provide insight. 
This not only limits the ability to apply models in different domains, but also limits reproducibility, making our field vulnerable to the reproducibility crisis. 
Verification of state of the art machine learning algorithms should not be limited to those with large infrastructures and computational resources. 
Throughout the history of research, we have learned that the speed of scientific advancement is directly proportional to the number of researchers, making it essential to enable researchers to participate in all areas of research. 

On the other end of the spectrum, Convolutional Neural Networks (CNNs) \cite{lecun1989backpropagation} became the standard for computer vision, since the success of AlexNet \cite{krizhevsky2012imagenet}. AlexNet showed that convolutions are adept at vision based problems due to their invariance to spacial translations as well as having low relational inductive bias. He \etal \cite{he2016deep} extended this work by introducing residual connections, allowing for significantly deeper models to perform efficiently. Convolutions leverage three important concepts that lead to their efficiency: \textit{sparse interaction}, \textit{weight sharing}, and \textit{equivariant representations} \cite{goodfellow2016deep}. Translation equivariance and translational invariance are due to the convolutions and pooling layers, respectively \cite{goodfellow2016deep,schmidhuber2015deep}. They allow CNNs to leverage natural image statistics and subsequently allow models to have higher sampling efficiency \cite{ruderman1994statistics,ruderman1994statistics}. 
Conventionally, CNNs have commonly been used for works on smaller datasets because they have been shown to be the best performers in this regime. 
Additionally, CNNs are more efficient, both computationally and in terms of memory, when compared to transformers They require less time and data to train while also requiring a lower number of parameters to accurately fit data. 
The above properties lead them to be effective for these areas, but they come with limitations in long range interdependence that attention mechanisms provide.


Both Transformers and CNNs have highly desirable qualities for statistical inference and prediction, but each comes with their own costs. In this work, we try to bridge the gap between these two architectures and develop an architecture that can both attend to important features within images, while also being spatially invariant, where we have sparse interactions and weight sharing. This allows for a Transformer based model to be trained, from scratch, on small datasets like CIFAR-10, CIFAR-100, and MNIST while providing competitive results.

In this paper we introduce ViT-Lite, a smaller and more compact version of ViT, which can obtain over  accuracy with correct patch sizing.
We expand on ViT-Lite by adding using our novel sequence pooling and forming the Compact Vision Transformer (CVT), and further iterate by adding convolutional blocks to the tokenization step and thus creating the Compact Convolutional Transformer (CCT). 
Both of these additions add to significant increases in performance, leading to a top-1\%accuracy of  on CIFAR-10. 
This is only slightly behind the top performing CNN based algorithm, ResNet1001-v2, which obtains an accuracy of  but with  model size and  GMACs comparing to ours.
Our model outperforms all previous transformer based models within this domain. 
Additionally, we show that our model can be lightweight, only needing  million parameters and still reach close to  top-1\% accuracy on CIFAR-10.

The main contributions of this paper are:
\begin{itemize}
    \item Dispelling the myth of ``data hungry'' transformers by introducing ViT-Lite, which can be trained efficiently with high accuracy on small datasets such as CIFAR-10.
    \item Introducing Compact Vision Transformer (CVT) with a novel sequence pooling stratgey, which removes the need of the conventional class token design in vision transformers and make it more accurate.
    \item Introducing Compact Convolutional Transformer (CCT) to increase performance and provide flexibility for input image sizes while also demonstrating that these variants do not depend as much on Positional Embedding compared to the rest.
\end{itemize}

In addition, we demonstrate that our CCT model is extremely fast, obtaining  accuracy on CIFAR-10 using a single NVIDIA 2080Ti GPU and  when trained on a CPU (AMD 5900X), both in under 30 minutes. 
Additionally, since our model has a relatively small number of parameters we can train on the majority of GPUs, even if researchers do not have access to top of the line hardware.
Through these efforts, we aim to democratise transformer research and make it more accessible to the average person. 

\section{Related Works}
\label{sec:related}

Since the advent of deep neural networks, CNNs have predominantly been used for visual recognition. 
In NLP based research, attention mechanisms~\cite{graves2014neural,bahdanau2016neural,luong2015effective} gained popularity for their ability to weigh different features within sequential data. 
In the past, many researchers leveraged a combination of attention and convolutions in neural networks for visual tasks \cite{bello2019attention, hu2019local, hu2018squeeze, wang2017residual, zhang2019self}. 
Ramachandran \etal \cite{ramachandran2019stand} introduced the first a vision model that relies purely on an attention mechanism. 
Following this success, transformers have been used in a wide variety of tasks, including: machine translation \cite{devlin2019bert, liu2019roberta, yang2019xlnet}, visual question answering \cite{lu2019vilbert, su2019vl}, action recognition \cite{ bertasius2021space, girdhar2019video}, and the like. Dosovitskiy \etal \cite{dosovitskiy2020image} introduced the first stand-alone transformer based model (ViT) for image classification. In the following subsections we briefly revisit ViT and several related works.


\subsection{Vision Transformer}
\label{subsec:vits}
Vision Transformers (ViT) were introduced as a way to compete with CNNs on image classification tasks and utilize the benefits of attention within these networks. 
The motivation was because attention has many desirable properties for a network, specifically its ability to make long range connections.
Dosovitskiy~\etal showed that large scale training triumphs over the advantage of inductive bias that CNNs have, allowing their model to be competitive with CNN based architectures given sufficiently large amount of training data.
ViT is composed of several parts: Image Tokenization, Positional Embedding, Classification Token, the Transformer Encoder, and a Classification Head. 
These subjects are discussed in more detail below. 

\textbf{Image Tokenization:}
A standard transformer takes as input a sequence of vectors, called tokens. 
For traditional NLP based transformers the word ordering provides a natural order to sequence the data, but this is not so obvious for images. 
To tokenize an image, ViT subdivides an image into non-overlapping square patches and orders them from top left to bottom right. 
The sequence of patches,  with patch size , are flattened into 1D vectors and transformed into latent vectors of dimension , using a linear layer to obtain the token embeddings. 
This simple patching method has a few limitations, in particular information is lost along the boundary regions. 

\textbf{Positional Embedding:}
Positional embedding is a method that adds spatial information into the network. 
Since the model does not actually know anything about the spatial relationship between tokens we must add some extra information. 
Typically this is either a learned embedding or tokens are given weights from two sine waves with high frequencies, which is sufficient for the model to learn that there exists a positional relationship between these tokens.


\textbf{Classification Token:}
Vision transformers typically add an extra learnable \verb|[class]| token to the sequence of the embedded patches, representing the class parameter of an entire image and its state after transformer encoder can be used for classification. 
\verb|[class]| token contains the latent information that is later used for classification. 
This \verb|[class]| token design originated from BERT~\cite{devlin2019bert}.


\textbf{Transformer Encoder:}
A transformer encoder consists of a series of stacked encoding layers. Each encoder layer is comprised of two sub-layers: Multi-headed Self-Attention (MSA) and a Multi-Layer Perceptron (MLP) head. Each sub-layer is preceded by a layer normalization (LN), and followed by a residual connection to the next sub-layer. Considering an input sequence , the output , of a single encoder layer (ViT's notation) can be obtained as follows:



\textbf{Classification:}
Similar to BERT~\cite{devlin2019bert}, the position of the \verb|[class]| token yields the location of the image representation. A linear layer is then applied to this representation to obtain a confidence score for each class.

\subsection{Data-Efficient Transformers}

In an effort to reduce the dependence upon data, Data-Efficient Image Transformers (DeiT) \cite{touvron2020training} use knowledge distillation to improve the classification performance of the vision transformer. 
This work builds off of Xu \etal \cite{xu2020optimizing}, which optimizes transformers for smaller linguistic datasets by introducing a novel initialization technique inspired by Huang \etal \cite{huang2020improving}. 
DeiT's work focuses on reducing the data dependence of vision transformers, showing that this model could perform well on mid-sized dataset like ImageNet, as well as fine tune on small datasets like CIFAR-10.
This work pushes forward accessibility of transformers in smaller dataset domains but makes an assumption that pre-trained models are accessible and that the smaller datasets share enough attributes for fine tuning to work. 
However, if a small dataset happens to be sufficiently novel, then the pre-trained model will not help train on that domain and the model will not be appropriate for that dataset.

\begin{figure*}[ht!]
    \centering
    \includegraphics[width=0.95\textwidth]{assets/images/convvit-diagram.pdf}
    \caption{Comparing ViT (top) to CVT (middle) and CCT (bottom)}
    \label{fig:vit_comparison}
\end{figure*}

Yuan \etal \cite{yuan2021tokens} proposed Tokens-to-token ViT (T2T-ViT), which adopts a layer-wise tokenization strategy in which overlapping image patches are flattened, sent through self-attention layers, and reshaped back into patches. 
This strategy allows their network to model local structures, including along the boundaries between patches.
Additionally, this process can be repeated multiple times and the final outputs are flattened into a sequence of vectors.
This repetition of attention allows for a feature rich and learnable tokenization that allows T2T-ViT to obtain high accuracy on ImageNet. 
T2T-ViT differs from our work in that it focuses on medium sized datasets like ImageNet, which are not only far too large for many research problems in science and medicine but also resource demanding.

\subsection{Other Concurrent Works}
Many concurrent works have been recently released on arXiv to improve vision transformers and eliminate the need to pretrain on super large datasets like JFT-300M~\cite{sun2017revisiting} that is not accessible by the large community. ConViT~\cite{d2021convit}, make an effort to combine the strength of CNNs and transformer based models. 
They introduce a \textit{gated positional self-attention} (GPSA) that allows for a ``soft'' convolutional inductive bias within their model. 
This GPSA allows their network to have more flexibility with respect to positional information. 
Since their GPSA is able to be initialized as a convolutional layer, this allows their network to sometimes have the properties of convolutions or alternatively having the properties of attention. 
Their \textit{gating parameter} can be adjusted by the network allowing it to become more expressive and adapt to the needs of the dataset.
Convolution-enhanced image Transformers (Ceit)~\cite{yuan2021incorporating} uses a convolutions to extract low level features. 
They use an image-to-token module to extract this information, introduce a locally enhanced feed-forward layer that processes the spatial information form the extracted tokens, and utilize a layer-wise cross attention mechanism. 
This allows them to create a network that is competitive with DeiT on ImageNet. 
Convolutional vision Transformer (CvT)~\cite{wu2021cvt} also uses a convolutional layer for the tokenization process. 
Wu~\etal uses these convolutional projections instead of the standard linear projections for their embeddings into the encoding layer of the transformer.
This allows them to achieve competitive results to DeiT. 
All of these works report results when trained from scratch on ImageNet or larger datasets.


\subsection{Comparison}

Our work differs from the aforementioned in several ways but our work focuses on answering the following question: \textbf{Can vision transformers be trained from scratch on small datasets?} 
Focusing on a small datasets we seek to create a model that can be trained, from scratch, on datasets that are orders of magnitude smaller than ImageNet.
Having a model that is compact, small in size, and efficient allows greater accessibility, as training on ImageNet is still a difficult and data intensive task for many researchers. 
Thus our focus is on an accessible model, with few parameters, that can quickly and efficiently be trained on smaller platforms while still maintaining SOTA results. 



\section{Methodology}

In order to dispel the myth that vision transformers are only applicable when dealing with large sets of data, we propose three different models: ViT-Lite, \textbf{C}ompact \textbf{V}ision \textbf{T}ransformers~(CVT), and \textbf{C}ompact \textbf{C}onvolutional \textbf{T}ransformers~(CCT). 
ViT-Lite is near identical to the original ViT, but with more suitable smaller patch sizing for small datasets. In other words, \textit{an image is \textbf{not always} worth 16\texttimes16 words}~\cite{dosovitskiy2020image} and a suitable size matters.
CVT builds on this by introducing a sequential pooling method, called SeqPool, that pools the sequential based information that results from the transformer encoder. SeqPool eliminates the need for the extra Classification Token(Sec~\ref{sec:seqpool}).
Lastly, we have CCT which introduces a convolutional based patching method, which preserves local information and is able to encode relationships between patches, unlike the original ViT. A detailed modular-level comparison of these models can be viewed in Figure~\ref{fig:vit_comparison}.

Our convolution based model, CCT, consist of convolutions with small strides that allow for efficient tokenization and preserves local spatial relationships.
Furthermore, we eliminate the need for the \verb|[class]| token in CCT (also eliminated in CVT), allowing for greater flexibility within our model.
While simple in design, we show that our network can be quite effective if composed in the correct manner. 
To further differentiate the subtle differences between CVT and CCT we provide Figure~\ref{fig:cct_vs_cvt} which outlines the similarities and differences of the networks. 
The components of our compact transformers are further discussed in the following subsections. 

\subsection{Convolutional Block}
In order to introduce an inductive bias into the model, we replace the ``image patching'' and ``embedding'' layers in ViT with a simple convolutional block.
Our convolutional block follows conventional design, which consists of a single convolution layer,  activation, and a max pool operation. Given an image :

where the  operation has  filters, same number as the embedding dimension of the transformer backbone.
The number of these blocks can be to more than . When iterating this process, we use  filters for the earlier convolution operations, and  filters for the final one. A single-block convolution block is presented in Algorithm \ref{alg:convblock}.
By using this convolutional block we gain some added flexibility over models like ViT. 
While ViT patches the image, we seek to use convolutions to embed the image into a latent representation that will be more efficient for the transformer. 

\begin{algorithm}[ht]
   \caption{CCT ConvBlock(PyTorch-style)}
   \label{alg:convblock}
   \lstset{
      basicstyle=\fontsize{7.2pt}{7.2pt}\ttfamily\bfseries,
      commentstyle=\fontsize{7.2pt}{7.2pt},
      morekeywords={super, def},
      keywordstyle=\fontsize{7.2pt}{7.2pt}\color{deepblue},
      emph={ConvBlock,__init__, nn, forward},
      emphstyle=\color{deepred},
      stringstyle=\color{deepgreen}
    }
\begin{lstlisting}[language=python]
class ConvBlock(nn.Module):
    def __init__(self, embed_dim=128):
        self.conv_layers = nn.Sequential(
          nn.Conv2d(3, embed_dim,
              kernel_size=7, stride=2, padding=3),
          nn.ReLU(),
          nn.MaxPool2d(kernel_size=3, stride=2,
              padding=1)
          nn.Flatten(2, 3)
        )

    def forward(self, x):
        return self.conv_layers(x).transpose(2, 3)
\end{lstlisting}
\end{algorithm}

Changes in image size do not affect the number of parameters, but do affect the sequence length and subsequently the amount of computation needed. Although, ViT's patching requires that both the image height and width be divisible by the patch size.
This allows us to take as input different size data without the need for cropping or padding. 
In other words, our CCT model does not have the requirement of evenly subdividing an image into patches, as ViT or CVT.
Additionally, the convolution and max pool operations can be overlapping, which also increases the sequence length, but on the other hand, could increase performance by injecting inductive bias.
Having these convolutional patches allows our model to maintain locally spatial information. 
This also  gives us more flexibility toward removing the positional embedding in our model, as it manages to maintain a very good performance. 
This is further discussed in Section~\ref{sec:pe}. 

\subsection{Transformer-based Backbone}
In terms of model design, we follow the original Vision Transformer \cite{dosovitskiy2020image}, and original Transformer \cite{vaswani2017attention}, relatively closely.
As mentioned in Section~\ref{subsec:vits}, the encoder consists of transformer blocks, each including a MSA layer and a MLP head. 
Simillar to ViT, we apply layer norm after the positional embeddings are added. The transformer encoder uses Layer Normalization,  activation, and dropout. The positional embeddings are learnable by default.

\begin{figure*}[ht!]
    \centering
    \includegraphics[width=0.8\textwidth]{assets/images/model_sym.pdf}
    \caption{Overview of CCT vs CVT}
    \label{fig:cct_vs_cvt}
\end{figure*}
\subsection{SeqPool}
\label{sec:seqpool}
In order to map the sequential outputs to a singular class index, instead of using a class token (like BERT \cite{devlin2019bert}, ViT \cite{dosovitskiy2020image}, and most other transformer-based classifiers), we propose \textbf{Seq}uence \textbf{Pool}ing. As the name suggests, we pool the outputs of the transformer backbone across the sequence. 
We pool over the entire sequence of data since it contains relevant information across different parts of the input image, making our model compact. 
We can think of this operation as the mapping transformation . Given:

where  or  is the output of an  layer transformer encoder,  is the mini-batch size,  is the sequence length, and  is the embedding dimension, we feed  to a linear layer  and apply softmax activation:

Then, we simply compute:

and by pooling the second dimension we are left with . Then this output can be sent through a linear classifier, like most other such networks. 

This SeqPool allows for our network to weigh the sequential embeddings of the latent space produced by the transformer encoder and better correlate data across the input data. 
This can be thought of this as attending to the sequential data, where we are assigning importance weights across the sequence of data. 
We have tested several variations of this pooling method, including learnable and static methods, and found that the learnable pooling performs the best. 
We believe that the learnable weighting is more efficient because each embedded patch does not contain the same amount of entropy.
This allows our model to give higher weight to patches that contain more information relevant to the classifier.
Additionally, the sequence pooling allows our model to better utilize information across spatially sparse data. 
We will further study the effects of this pooling in the ablation study (Sec~\ref{sec:ablation}).

\subsection{Small and Compact Models}
We propose some smaller and more compact transformer models compared to ViT, and use them alongside the sizes that they've already proposed. 
In our notation, we use the number of layers to specify size, as well as the kernel size for our convolutional layers: for instance, CCT-12/7\texttimes2 means that the variant's transformer encoder has 12 layers, and uses 2 convolutional blocks with  convolutions.
We follow the same format for other models: ViT-Lite, which is like ViT but with smaller patches and CVT which is like ViT but with SeqPool (making it compact).
We further summarize these models in Appendix \ref{appdx:model_variants}, Table \ref{tab:model_sizes}, and those with ``smaller'' convolutions which are more suitable for small-resolution images are summarized in Table \ref{tab:small_model_sizes}.


\section{Experiments}
\label{sec:experiments}
We conducted image classification experiments using our method on the following datasets: CIFAR-10, CIFAR-100, MNIST, and Fashion-MNIST. These four datasets not only have a small number of training samples, but they are also small in resolution.
Additionally, MNIST and Fashion-MNIST only contain a single channel, greatly reducing the information density.

In all experiments, we conducted a hyperparamter search for every different method and report the best results we were able to achieve. We provide a detailed report on the hyperparamter settings in Appendix \ref{appdx:hyperparam}.
All reported top-1\% accuracy values are best out of 4 runs. Unless stated otherwise, all tests were run for 200 epochs with a batch size of , and the learning rate is reduced per epoch based on cosine annealing \cite{loshchilov2017sgdr}. Label smoothing with a probability of  was also used during training. All methods are warmed up for either 5 or 10 epochs (which will be specified in Appendix~\ref{appdx:hyperparam}). In all experiments (unless stated otherwise), CIFAR-specific augmentations computed by AutoAugment~\cite{cubuk2019autoaugment}
are used for CIFAR-10 and CIFAR-100.

\subsection{Performance Comparison}
In order to argue that vision transformers can be as effective as convolutional neural networks even in settings with small sets of data, we compare our compact transformers to ResNets \cite{he2016deep}, which are still very useful CNNs for small to medium amounts of data, as well as to MobileNetV2 \cite{sandler2018mobilenetv2}, which are very compact and small-sized CNNs. We also compare with results from~\cite{he2016identity} where He \etal designed very deep (up to 1001 layers) CNNs specifically for CIFAR. The results are presented in Table \ref{tab:full_comparsion}.

\begin{figure}[ht]
\centering
\includegraphics[width=0.445\textwidth]{assets/images/cnn_comparison.pdf}
\caption{CIFAR-10 accuracy vs model size for model with size M.}
\label{fig:mobilenet_comparison}
\end{figure}

\begin{table*}[ht]
    \centering
    \begin{tabular}{l|cccc|cr}
        \toprule
        \textbf{Model} & \textbf{CIFAR-10} & \textbf{CIFAR-100} & \textbf{Fashion-MNIST} & \textbf{MNIST} & \textbf{\# Params} & \textbf{MACs}\\
        \midrule
        \multicolumn{3}{l}{\textit{Convolutional Networks (Designed for ImageNet)}}\\
        \midrule
        \textbf{ResNet18} &  &  &  &  &  M &  G \\
        \textbf{ResNet34} &  &  &  &  &  M &  G \\
        \textbf{ResNet50} &  &  &  &  &  M &  G \\
        \midrule
        \textbf{MobileNetV2/0.5} &  &  &  &  &  M &  G \\
        \textbf{MobileNetV2/1.0} &  &  &  &  &  M &  G \\
        \textbf{MobileNetV2/1.25} &  &  &  &  &  M &  G \\
        \textbf{MobileNetV2/2.0} &  &  &  &  &  M &  G \\
        \midrule
        \multicolumn{3}{l}{\textit{Convolutional Networks (Designed for CIFAR)}}\\
        \midrule
\textbf{ResNet164-v1\cite{he2016identity}} &  &  &  &  &  M &  G \\
        \textbf{ResNet164-v2\cite{he2016identity}} &  &  &  &  &  M &  G \\
        \textbf{ResNet1001-v1\cite{he2016identity}} &  &  &  &  &  M &  G \\
        \textbf{ResNet1001-v2\cite{he2016identity}} &  &  &  &  &  M &  G \\
        \textbf{ResNet1001-v2\cite{he2016identity}} &  &  &  &  &  M &  G \\
        \midrule
        \multicolumn{3}{l}{\textit{Vision Transformers}}\\
        \midrule
        \textbf{ViT-12/16} &  &  &  &  &  M &  G \\
        \midrule
        \textbf{ViT-Lite-7/16} &  &  &  &  &  M &  G \\
        \textbf{ViT-Lite-6/16} &  &  &  &  &  M &  G \\
        \midrule
        \textbf{ViT-Lite-7/8} &  &  &  &  &  M &  G \\
        \textbf{ViT-Lite-6/8} &  &  &  &  &  M &  G \\
        \midrule
        \textbf{ViT-Lite-7/4} &  &  &  &  &  M &  G \\
        \textbf{ViT-Lite-6/4} &  &  &  &  &  M &  G \\
        \midrule
        \multicolumn{3}{l}{\textit{Compact Vision Transformers}}\\
        \midrule
        \textbf{CVT-7/8} &  &  &  &  &  M &  G \\
        \textbf{CVT-6/8} &  &  &  &  &  M &  G \\
        \midrule
        \textbf{CVT-7/4} &  &  &  &  &  M &  G \\
        \textbf{CVT-6/4} &  &  &  &  &  M &  G \\
        \midrule
        \multicolumn{3}{l}{\textit{Compact Convolutional Transformers}}\\
        \midrule
        \textbf{CCT-2/3\texttimes2} &  &  &  &  &  M &  G \\
        \textbf{CCT-4/3\texttimes2} &  &  &  &  &  M &  G \\
        \textbf{CCT-6/3\texttimes2} &  &  &  &  &  M &  G \\
        \textbf{CCT-7/3\texttimes2} &  &  &  &  &  M &  G \\
        \midrule
        \textbf{CCT-7/7\texttimes1} &  &  &  &  &  M &  G \\
        \textbf{CCT-7/3\texttimes1} &  &  &  &  &  M &  G \\
        \textbf{CCT-7/3\texttimes1} &  &  &  &  &  M &  G \\
        \bottomrule
    \end{tabular}
    \caption{Top-1 validation accuracy comparisons. The numbers reported are best out of 4 runs. Hyperparamters are mentioned in Appendix \ref{appdx:hyperparam}. Variants with  used a batch size of  instead of the default .}
    \label{tab:full_comparsion}
\end{table*} 
Other than CNNs, we also compare our method to the original ViT \cite{dosovitskiy2020image} in order to express the effectiveness of smaller sized backbones, convolutional layers, as well our pooling technique. 
It should be noted that only the first row represents ViT according to the original paper, and the rest of the rows are simply ``smaller'' ViT variants according to our own models. These smaller ViT variants are denoted as ViT-Lite. Following ViT's original notation, we denote ViT variants with their patch size, but use the number of layers instead of the nominal descriptions (``Base'', ``Large'', ``Huge''). For instance, ViT-12/8 means a ViT with a 12 layer transformer encoder and an  patch size.



\begin{table*}[t]
    \centering
    \begin{tabular}{clccccc|cc|cc}
        \toprule
        W & Model & Pool & \# Conv & Conv Size & Aug & Tuning & CIFAR-10 & CIFAR-100 & \# Params & MACs\\
        \midrule
        \xmark & ViT-12/16 & CT & \xmark & \xmark & \xmark & \xmark &  &  &  M &  G \\
        
        \cmark & ViT-12/16 & CT & \xmark & \xmark & \xmark & \xmark &  &  &  M &  G \\
        \midrule
        
        \cmark & ViT-Lite-7/16 & CT & \xmark & \xmark & \xmark & \xmark &  &  &  M &  G \\
        
        \cmark & ViT-Lite-7/8 & CT & \xmark & \xmark & \xmark & \xmark &  &  &  M &  G \\
        
        \cmark & ViT-Lite-7/4 & CT & \xmark & \xmark & \xmark & \xmark &  &  &  M &  G \\
        
        \midrule
        
        \cmark & CVT-7/16 & SP & \xmark & \xmark & \xmark & \xmark &  &  &  M &  G \\
        
        \cmark & CVT-7/8 & SP & \xmark & \xmark & \xmark & \xmark &  &  &  M &  G \\
        
        \cmark & CVT-7/8 & SP & \xmark & \xmark & \cmark & \xmark &  &  &  M &  G \\
        
        \cmark & CVT-7/4 & SP & \xmark & \xmark & \xmark & \xmark &  &  &  M &  G \\
        
        \cmark & CVT-7/4 & SP & \xmark & \xmark & \cmark & \xmark &  &  &  M &  G \\
        
        \cmark & CVT-7/4 & SP & \xmark & \xmark & \cmark & \cmark &  &  &  M &  G \\
        
        \cmark & CVT-7/2 & SP & \xmark & \xmark & \xmark & \xmark &  &  &  M &  G \\
        
        \midrule
        
        \cmark & CCT-7/7\texttimes1 & SP & 1 &  & \xmark & \xmark &  &  &  M &  G \\
        
        \cmark & CCT-7/7\texttimes1 & SP & 1 &  & \cmark & \xmark &  &  &  M &  G \\
        
        \cmark & CCT-7/7\texttimes1 & SP & 1 &  & \cmark & \cmark &  &  &  M &  G \\
        
        \midrule
        
        \cmark & CCT-7/3\texttimes2 & SP & 2 &  & \cmark & \cmark &  &  &  M &  G \\
        
        \midrule
        
        \cmark & CCT-7/3\texttimes1 & SP & 1 &  & \cmark & \cmark &  &  &  M &  G \\
        \bottomrule
    \end{tabular}
    \caption{Top-1 validation accuracy of CIFAR-10 and CIFAR-100 when transforming ViT into CCT step by step. The numbers reported are best out of 4 runs. Hyperparamters are mentioned in Appendix \ref{appdx:hyperparam}.}
    \label{tab:ablation_summary}
\end{table*} 
\subsection{Model Sizing}
We also evaluate smaller variants of our model. These variants can have as little as  K parameters and still achieve a considerably good performance on these datasets. These results are also presented in Table~\ref{tab:full_comparsion}.

\subsection{Ablation Study}
\label{sec:ablation}
We extend our previous comparisons by doing an ablation study on our method. 
We do so by progressively transforming the original ViT in structure into CCT, with intermediate models called ViT-Lite and CVT, and comparing the accuracy between these models. 
In this particular study, we report the results on CIFAR-10 and CIFAR-100, we summarize the results (best out of 4 runs) in Table \ref{tab:ablation_summary}.

The first column in Table \ref{tab:ablation_summary} represents the usage of the weighted Adam optimizer~\cite{Zhong_2020}, as opposed to Adam~\cite{kingma2017adam}, which was the primary optimizer used for training ViT from scratch. The ``Model'' column refers to the model sizes in Tables \ref{tab:model_sizes} and \ref{tab:small_model_sizes}. ViT-Base is equivalent to our model with size 12 in terms of the transformer encoder, and the number after the forward slash is the patch size (for ours it is the convolution kernel size).
The column ``Conv'' specifies the number of convolutional blocks used, and ``Conv Size'' specifies the kenel size. 
``Aug'' denotes the use of dataset-specific augmentations, which were not reported in ViT, and were therefore excluded from some of the experiments. 
``Tuning'' specifies a minor change in dropout, attention dropout, and/or stochastic depth. 
ViT, by default, uses a dropout for the MLP heads only with a probability of , and no attention dropout. 
Conversely, we do not use MLP dropout and only use attention dropout (). 
We also use stochastic depth with a drop probability of .

The first row in Table~\ref{tab:ablation_summary} are essentially ViT with the same settings from the paper. In the second row, the optimizer has been changed to weighted Adam, which we found more useful for training vision transformers. The next three rows are modified variants of ViT, which are not proposed in the original paper. These variants are more compact and use smaller patch sizes.

\subsection{Positional Embedding}
\label{sec:pe}
\begin{table*}[t]
    \centering
    \begin{tabular}{cc|cc}
        \toprule
        Model & PE & CIFAR-10 & CIFAR-100\\
        \midrule
        \multicolumn{3}{l}{\textit{Conventional Vision Transformers are more dependent on Positional Embedding }}\\
        \midrule
        
        \multirow{3}{*}{ViT-12/16} & Learnable &  {\color{red}\small \textit{()}} &  {\color{red}\small \textit{()}} \\
        & Sinusoidal &  {\color{red}\small \textit{()}} &  {\color{blue}\small \textit{()}}\\
        & None &  {\color{black}(\small \textit{baseline)}} &  {\color{black}(\small \textit{baseline)}}\\
        
        \midrule
        
        \multirow{3}{*}{ViT-Lite-7/8} & Learnable &  {\color{red}\small \textit{()}} &  {\color{red}\small \textit{()}} \\
        & Sinusoidal &  {\color{red}\small \textit{()}} &  {\color{red}\small \textit{()}} \\
        & None &  {\color{black}(\small \textit{baseline)}} &  {\color{black}(\small \textit{baseline)}} \\
        
        \midrule
        
        \multirow{3}{*}{CVT-7/8} & Learnable &  {\color{red}\small \textit{()}} &  {\color{red}\small \textit{()}} \\
        & Sinusoidal &  {\color{red}\small \textit{()}} &  {\color{red}\small \textit{()}} \\
        & None &  {\color{black}(\small \textit{baseline)}} &  {\color{black}(\small \textit{baseline)}} \\
        
        \midrule
        \multicolumn{3}{l}{\textit{Compact Convolutional Transformers are less dependent on Positional Embedding }}\\
        \midrule
        
        \multirow{3}{*}{CCT-7/7} & Learnable &  {\color{red}\small \textit{()}} &  {\color{red}\small \textit{()}} \\
        & Sinusoidal &  {\color{blue}\small \textit{()}} &  {\color{red}\small \textit{()}} \\
        & None &  {\color{black}(\small \textit{baseline)}} &  {\color{black}(\small \textit{baseline)}} \\
        
        
        \midrule
        \multirow{3}{*}{CCT-7/3\texttimes2} & Learnable &  {\color{red}\small \textit{()}} &  {\color{red}\small \textit{()}} \\
        & Sinusoidal &  {\color{red}\small \textit{()}} &  {\color{red}\small \textit{()}} \\
        & None &  {\color{black}(\small \textit{baseline)}} &  {\color{black}(\small \textit{baseline)}} \\
        \midrule
        \multirow{3}{*}{CCT-7/3\texttimes2} & Learnable &  {\color{red}\small \textit{()}} &  {\color{red}\small \textit{()}} \\
        & Sinusoidal &  {\color{red}\small \textit{()}} &  {\color{red}\small \textit{()}} \\
        & None &  {\color{black}(\small \textit{baseline)}} &  {\color{black}(\small \textit{baseline)}} \\
        \midrule
        
        \multirow{3}{*}{CCT-7/7\texttimes1-noSeqPool} & Learnable &  {\color{red}\small \textit{()}} &  {\color{red}\small \textit{()}} \\
        & Sinusoidal &  {\color{blue}\small \textit{()}} &  {\color{red}\small \textit{()}} \\
        & None &  {\color{black}(\small \textit{baseline)}} &  {\color{black}(\small \textit{baseline)}} \\
        
        \midrule
        
        \multirow{3}{*}{CCT-7/3\texttimes2-noSeqPool} & Learnable &  {\color{red}\small \textit{()}} &  {\color{red}\small \textit{()}} \\
        & Sinusoidal &  {\color{red}\small \textit{()}} &  {\color{blue}\small \textit{()}} \\
        & None &  {\color{black}(\small \textit{baseline)}} &  {\color{black}(\small \textit{baseline)}} \\
        \bottomrule
    \end{tabular}
    \caption{Top-1 validation accuracy comparison when changing the positional embedding method. The numbers reported are best out of 4 runs with random initializations.  denotes model trained with extra augmentation and hyperparameter tuning.}
    \label{tab:pe_comparison}
\end{table*}
 To determine the effects of the convolution layers and SeqPool, we perform an ablation study just for the purpose of positional embedding, seen in Table~\ref{tab:pe_comparison}.
In this study we look at ViT, ViT-Lite, CVT, and CCT, and investigate the effects of: a learnable positional embedding, the standard sinusoidal embedding, as well as no positional embedding. 
We finish the table with our best model, which also has augmented training and an optimal tuning (refer to Appendix \ref{appdx:hyperparam}). 
In these experiments we find that positional encoding matters in all variants, but to varying degrees. In particular, CCT relies less on positional encoding and can be safely removed much impact in accuracy.
We also tested our CCT model without SeqPool, in place using the standard class token, and found that there was little to no effect from having a positional encoder or not, depending on model size. 
This demonstrates that the convolutions are the feature that helps provide spatially sparse information to the transformer as well as helps our model overcome some of the previous limitations, allowing for more efficient use of data.
We do find that SeqPool helps slightly in this respect, but overall has a larger effect on increasing total accuracy. 
Lastly, we find that with proper data augmentation and tuning that we can increase the overall performance and maintain a low dependence on positional information.
This demonstrates that our model is more robust to sparse data and that users may not even be required to positionally embed their datasets at all. 



\subsection{Performance vs Dataset Size}\label{sec:dataset}

\begin{figure}[ht]
\centering
\includegraphics[width=0.45\textwidth]{assets/images/reduced_ds_training.pdf}
\caption{CIFAR-10 with Reduced Number of Samples per Class.}
\label{fig:redruns}
\end{figure}

In this experiment we reduce the size of CIFAR-10 to determine the relationship between our model's performance and the number of samples within a dataset. 
This experiment disassociates the dimensionality of the data from the size of the dataset, measuring which metric is the ``data hungry'' aspect of transformers.
To do this we remove samples, uniformly, from each class in CIFAR-10 and measure the performance of both ViT and CCT. 
In Figure~\ref{fig:redruns} we see the comparison of each model's accuracy vs the number of samples per class. 
We show how the model runs when we have 500, 1000, 2000, 3000, 4000, or 5000 (original) samples per class, meaning the total data set ranges from one tenth the size to full. 
We can see here that our model is more robust since it is able to obtain higher accuracy with a lower number of samples per class, especially in the low sample regime.
This allows us to conclude that our model is not data hungry and can still effectively learn on very small datasets, obtaining over 84\% accuracy with a dataset that has only 10k samples and a total of 10 classes. 

\subsection{Performance vs Dimensionality}\label{sec:dimensionality}

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.45\textwidth]{assets/images/resolution_training.pdf}
    \caption{Image Size vs Accuracy}
    \label{fig:sizeruns}
\end{figure}



In order to determine if transformers need highly dimensional data, as opposed to the number of samples (explored in Section~\ref{sec:dataset}) 
we again use CIFAR-10 and downsample or upsample the images to determine this relationship. 
In Figure~\ref{fig:sizeruns}, we show the image dimensionality vs the performance of our networks, where we trained both CCT and ViT-Lite with images of sizes ranging from 16\texttimes16 to 64\texttimes64.
We can see that CCT performs better on all image sizes, with a widening difference as the number of pixels increases.
From this we can infer that our model is able to better utilize the information density of an image, where ViT does not see continued performance increases after the standard 32x32 size.
We go into further detail about the effects of positional embeddings in Appendix~\ref{appdx:dimensionality}, where it should be noted that the difference seen here is exaggerated once positional embeddings are removed.
Additionally, we show the difference between training a model with these parameters and inferring on a model that was trained on 32\texttimes32 sized images, denoting the generalizability of the models. 
We note that the number of parameters for these competing models is roughly the same, see Table~\ref{tab:ablation_summary}.



\section{Conclusion}
The myth of ``data hungry'' transformers has persisted since they were introduced.
We have shown within this paper that this is not necessarily true, and that with proper configuration, a transformer can be just as efficient as state-of-the-art CNN image classifiers. 
We democratise the use of Transformers by providing a framework that allows for these SOTA results on small datasets and minimal hardware. Our method is flexible in size, and the smallest of our variants can be easily loaded on even a minimal GPU, or even a CPU.
While the trend of research has been bigger and bigger transformers, we show that there is still much research to be done to make efficient networks that work on small datasets and less efficient hardware. 
This kind of research is extremely important for many scientific domains where data is far more limited that the conventional datasets we use for general research. 
Continuing research in this direction will help open research up to more people and domain, helping democratize machine learning research.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{references}
}

\appendix

\section{Hyperparameter tuning}
\label{appdx:hyperparam}
We tuned the hyperparamters per experiment and arrived at the following for each table in Sec. \ref{sec:experiments}. It should be noted that we directly report the numbers from the paper ``Identity mappings in deep residual networks'' \cite{he2016identity} for ResNet164 and ResNet1001.

\begin{table}[ht]
    \small
    \centering
    \begin{tabular}{c|cccc}
        \toprule
        \textbf{Model} & \textbf{LR} & \textbf{Optim.} & \textbf{Weight Decay}\\
        \midrule
        \textbf{ResNet18} & \multirow{9}{*}{1e-1} & \multirow{9}{*}{SGD} & \multirow{9}{*}{1e-4} \\
        \textbf{ResNet34} \\
        \textbf{ResNet50} \\
\textbf{MobileNetV2/0.5} \\
        \textbf{MobileNetV2/1.0} \\
        \textbf{MobileNetV2/1.25} \\
        \textbf{MobileNetV2/2.0} \\
        \bottomrule
    \end{tabular}
    \caption{Hyperparamters from Table \ref{tab:full_comparsion} experiments.}
    \label{tab:cnn_comparison_hyperparams}
\end{table} In table \ref{tab:cnn_comparison_hyperparams}, we summarize the different hyperparamters used for different models in table \ref{tab:full_comparsion}. Methods using SGD also used momentum (), and those using AdamW used the default beta values  and . 

\begin{table}[ht]
    \small
    \centering
    \begin{tabular}{c|ccc}
        \toprule
        \textbf{Hyper Param} & \textbf{ViT-12} & \textbf{ViT-7} & \textbf{CCT} \\
        \midrule
        \textbf{Optimizer} & AdamW & AdamW & AdamW \\
        \textbf{LR} & 1e-4 & 5e-4 & 5e-4 \\
        \textbf{Weight Decay} & 0 & 3e-2 & 3e-2 \\
        \bottomrule
    \end{tabular}
    \caption{ViT and CCT hyperparamters.}
    \label{tab:vit_comparison_hyperparams}
\end{table}
\begin{table}[ht]
    \small
    \centering
    \begin{tabular}{c|cc}
        \toprule
        \textbf{Hyper Param} & \textbf{Not Tuned} & \textbf{Tuned} \\
        \midrule
        \textbf{MLP Dropout} & 0.1 & 0 \\
        \textbf{MSA Dropout} & 0 & 0.1 \\
        \textbf{Stochastic Depth} & 0 & 0.1 \\
        \bottomrule
    \end{tabular}
    \caption{Difference between \textbf{tuned} and not tuned runs.}
    \label{tab:tuned_comparison}
\end{table} 
\section{Dimensionality Experiments}
\label{appdx:dimensionality}
Within this appendix we extend the analysis from Section~\ref{sec:dimensionality}, showing the difference in performance when using different types of positional embedding. Figure~\ref{fig:sizeruns_training} shows the difference of the accuracy when models are being trained from scratch. On the other hand, Figure~\ref{fig:sizeruns_inference} shows the performance difference when models are only used in inference and pre-trained on the 32\texttimes32 sized images. We note that in Figure~\ref{fig:sizeruns_inference}(a) that we do not provide inference for image sizes greater than the pre-trained image because the learnable positional embeddings do not allow us to extend in this direction. 
We draw the reader's attention to Figure~\ref{fig:sizeruns_training}(c) and Figure~\ref{fig:sizeruns_inference}(c) to denote the large difference between the models when positional embedding is not used. 
We can see that in training CCT has very little difference when positional embeddings are used.
Additionally, it should be noted that when performing inference our non-positional embedding CCT model has much higher generalizability than its ViT-Lite counterpart.

\begin{figure}[ht!]
     \centering
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{assets/images/resolution_training.pdf}
         \caption{Learnable PEs}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{assets/images/resolution_training_sine.pdf}
         \caption{Sinusoidal Positional Embedding}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{assets/images/resolution_training_nope.pdf}
         \caption{No Positional Embedding}
     \end{subfigure}
    \caption{CIFAR-10 resolution vs top-1\% validation accuracy (training from scratch). Images are square.}
    \label{fig:sizeruns_training}
\end{figure}

\begin{figure}[ht!]
     \centering
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{assets/images/resolution_inference.pdf}
         \caption{Learnable PEs}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{assets/images/resolution_inference_sine.pdf}
         \caption{Sinusoidal PEs}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{assets/images/resolution_inference_nope.pdf}
         \caption{None}
     \end{subfigure}
    \caption{CIFAR-10 resolution vs top-1\% validation accuracy (inference only). Images are square.}
    \label{fig:sizeruns_inference}
\end{figure}

\section{CCT Variants}
\label{appdx:model_variants}
The different variants of our model are presented in tables \ref{tab:model_sizes} and \ref{tab:small_model_sizes}.

\renewcommand{\arraystretch}{1.05}
\begin{table*}[t]
    \centering
    \begin{tabular}{c|c|c|cccc|rr}
        \toprule
        \multicolumn{1}{c|}{\multirow{2}{*}{Model}} &
        \multicolumn{1}{c|}{\multirow{2}{*}{\# Convs}} &
        \multicolumn{1}{c|}{\multirow{2}{*}{Input size}} & \multicolumn{4}{c}{Transformer Backbone} & \multicolumn{2}{c}{Computational cost} \\
        &&& \# Layers & \# Heads & Ratio & Dim & Params & MACs \\
        \midrule
        \multirow{6}{*}{CCT-6/7} & \multirow{2}{*}{1} & 32 & \multirow{6}{*}{6} & \multirow{6}{*}{4} & \multirow{6}{*}{2} & \multirow{6}{*}{256} &  M &  G \\
        && 224 &&&&&  M &  G \\
        \cline{2-3}\cline{8-9}
        & \multirow{2}{*}{2} & 32 &&&&&  M &  G \\
        && 224 &&&&&  M &  G \\
        \cline{2-3}\cline{8-9}
        & \multirow{2}{*}{3} & 32 &&&&&  M &  G \\
        && 224 &&&&&  M &  G \\
        \midrule
        \multirow{6}{*}{CCT-7/7} & \multirow{2}{*}{1} & 32 & \multirow{6}{*}{7} & \multirow{6}{*}{4} & \multirow{6}{*}{2} & \multirow{6}{*}{256} &  M &  G \\
        && 224 &&&&&  M &  G \\
        \cline{2-3}\cline{8-9}
        & \multirow{2}{*}{2} & 32 &&&&&  M &  G \\
        && 224 &&&&&  M &  G \\
        \cline{2-3}\cline{8-9}
        & \multirow{2}{*}{3} & 32 &&&&&  M &  G \\
        && 224 &&&&&  M &  G \\
        \midrule
        \multirow{6}{*}{CCT-8/7} & \multirow{2}{*}{1} & 32 & \multirow{6}{*}{8} & \multirow{6}{*}{4} & \multirow{6}{*}{2} & \multirow{6}{*}{256} &  M &  G \\
        && 224 &&&&&  M &  G \\
        \cline{2-3}\cline{8-9}
        & \multirow{2}{*}{2} & 32 &&&&&  M &  G \\
        && 224 &&&&&  M &  G \\
        \cline{2-3}\cline{8-9}
        & \multirow{2}{*}{3} & 32 &&&&&  M &  G \\
        && 224 &&&&&  M &  G \\
        \midrule
        \multirow{6}{*}{CCT-10/7} & \multirow{2}{*}{1} & 32 & \multirow{6}{*}{10} & \multirow{6}{*}{8} & \multirow{6}{*}{3} & \multirow{6}{*}{512} &  M &  G \\
        && 224 &&&&&  M &  G \\
        \cline{2-3}\cline{8-9}
        & \multirow{2}{*}{2} & 32 &&&&&  M &  G \\
        && 224 &&&&&  M &  G \\
        \cline{2-3}\cline{8-9}
        & \multirow{2}{*}{3} & 32 &&&&&  M &  G \\
        && 224 &&&&&  M &  G \\
        \midrule
        \multirow{6}{*}{CCT-12/7} & \multirow{2}{*}{1} & 32 & \multirow{6}{*}{12} & \multirow{6}{*}{12} & \multirow{6}{*}{4} & \multirow{6}{*}{768} &  M &  G \\
        && 224 &&&&&  M &  G \\
        \cline{2-3}\cline{8-9}
        & \multirow{2}{*}{2} & 32 &&&&&  M &  G \\
        && 224 &&&&&  M &  G \\
        \cline{2-3}\cline{8-9}
        & \multirow{2}{*}{3} & 32 &&&&&  M &  G \\
        && 224 &&&&&  M &  G \\
        \bottomrule
    \end{tabular}
    \caption{Compact Convolutional Transformer variants and their computational costs.}
    \label{tab:model_sizes}
\end{table*}
 
\begin{table*}[t]
    \centering
    \begin{tabular}{c|c|c|cccc|rr}
        \toprule
        \multicolumn{1}{c|}{\multirow{2}{*}{Model}} &
        \multicolumn{1}{c|}{\multirow{2}{*}{\# Convs}} &
        \multicolumn{1}{c|}{\multirow{2}{*}{Input size}} & \multicolumn{4}{c}{Transformer Backbone} & \multicolumn{2}{c}{Computational cost} \\
        &&& \# Layers & \# Heads & Ratio & Dim & Params & MACs \\
        \midrule
        \multirow{2}{*}{CCT-2/3} & 1 & \multirow{2}{*}{32} & \multirow{2}{*}{2} & \multirow{2}{*}{2} & \multirow{2}{*}{1} & \multirow{2}{*}{128} &  K &  G \\
        & 2 &&&&&&  K &  G \\
        \midrule
        \multirow{2}{*}{CCT-4/3} & 1 & \multirow{2}{*}{32} & \multirow{2}{*}{4} & \multirow{2}{*}{2} & \multirow{2}{*}{1} & \multirow{2}{*}{128} &  K &  G \\
        & 2 &&&&&&  K &  G \\
        \midrule
        \multirow{2}{*}{CCT-6/3} & 1 & \multirow{2}{*}{32} & \multirow{2}{*}{6} & \multirow{2}{*}{4} & \multirow{2}{*}{2} & \multirow{2}{*}{256} &  M &  G \\
        & 2 &&&&&&  M &  G \\
        \midrule
        \multirow{2}{*}{CCT-8/3} & 1 & \multirow{2}{*}{32} & \multirow{2}{*}{7} & \multirow{2}{*}{4} & \multirow{2}{*}{2} & \multirow{2}{*}{256} &  M &  G \\
        & 2 &&&&&&  M &  G \\
        \bottomrule
    \end{tabular}
    \caption{Small CCT variants ( convolutions) and their computational costs.}
    \label{tab:small_model_sizes}
\end{table*}
 
As it can be noticed, with more convolutional blocks, the computational complexity (MACs) decreases, due to the smaller sequence length produced. However, the performance may drop as well, and the number of parameters slightly increases.

\end{document}
