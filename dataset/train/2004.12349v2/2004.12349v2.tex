The proposed framework has been evaluated on two challenging benchmarks for two tasks: (i) RGB-D object recognition (Sec. \ref{sec:exp.objectRecognition}) using Washington RGB-D object dataset \citep{Lai_ICRA_2011} and (ii) RGB-D scene recognition (Sec. \ref{sec:exp.sceneRecognition}) using SUN RGB-D scene dataset \citep{Song_CVPR_2015}. After introducing the datasets and setups, we first compare our results with state-of-the-art results for both benchmarks. Results of other methods are taken from the original papers. Then, we carry out extensive experiments (Sec. \ref{sec:exp.modelAblation}) on the challenging Washington RGB-D object dataset, which is a larger-scale RGB-D dataset comparing to other RGB-D benchmarks, to evaluate effects of various model parameters and setup properties in our framework.
\begin{figure*}[!ht]
	\begin{center}
		\includegraphics[width=\textwidth, keepaspectratio]{wrgbdIndividualResultsV2.pdf}
	\end{center}
	\caption{Per-category average accuracy performances of ResNet101-RNN on Washington RGB-D Object dataset.}
	\label{fig:wrgbdIndividualResults}
\end{figure*}
\subsection{Dataset and Setup} \label{sec:exp.datasets}
\subsubsection{Washington RGB-D Object Dataset}
Washington RGB-D object dataset includes a total of $41,877$ images for each modality under $51$ object categories and $300$ category instances. Categories are commonly used household objects such as cups, camera, keyboards, vegetables, fruits, etc. Each instance of a category has images taken from $30^\circ$, $45^\circ$ and $60^\circ$ elevation angles. The dataset provides $10$ train/test splits where in each split, one instance for each category is used for testing and the remaining instances are for training. Thus, for a single split run, a total of $51$ category instances (roughly $7,000$ images) are used at testing and the remaining $249$ instances (roughly $35,000$ images) are used at training phase. We evaluate the proposed work on the provided cropped images with the same setup in \cite{Lai_ICRA_2011} for the 10 splits and average accuracy results are reported for the comparison to the related works.
\subsubsection{SUN RGB-D Scene Dataset}
SUN RGB-D scene dataset is the largest real-world RGB-D scene understanding benchmark to the date and  contains RGB-D images of indoor scenes. Following the publicly available configuration of the dataset, we choose $19$ scene categories with a total of $4,845$ images for training and $4,659$ images for testing. We use the same train/test split of \cite{Song_CVPR_2015} to evaluate the proposed work for scene recognition.

\subsection{Object Recognition Performance} \label{sec:exp.objectRecognition}
Table \ref{table:wrgbdResults} shows average accuracy performance of our approach along with the state-of-the-art methods for object recognition on Washington RGB-D object benchmark. Our approach greatly improves the previous state-of-the-art results for both of RGB and depth modalities with a margin of $2.4\%$ and $1.3\%$, respectively. As for the combined RGB-D results, our approach surpasses all the other methods except that of \cite{Loghmani_RAL_2019}, which is slightly better than ours ($0.3\%$). As stated before (see Sec. \ref{sec:relatedwork}), their approach is based on a gated recurrent unit with a set of memory neurons and is powered by a multimodal fusion learning schema. On the other hand, in this paper, we focus on a simple yet effective multi-modal feature extraction framework with a soft voting SVM classification. These results emphasize the importance of deep features in a unified framework based on the incorporation of CNNs and random RNNs. What is interesting here is that even a simple model like AlexNet can yield quite successful results. Concretely, our previous work \citep{Caglayan_ECCVW_2018} with AlexNet architecture, called VGG\_f in MatConvNet toolbox, gives impressive results as the models used in this work.
\begin{table}[!h]
	\caption{Average accuracy comparison of our approach with the related methods on Washington RGB-D Object dataset (\%). \textcolor{red}{Red:} Best result, \textcolor{blue}{Blue:} Second best result, \textcolor{darkgreen}{Green:} Third best result.}
	\begin{center}
		\setlength{\tabcolsep}{0.9em} \def\arraystretch{1.1}
		\begin{adjustbox}{width=\columnwidth}
			\begin{tabular}{ lccc }
				\hline
				Method 											& RGB 							& Depth 						& RGB-D \\ \hline \hline
				Kernel SVM \citep{Lai_ICRA_2011}    				& 74.5 $\pm\hfil$ 3.1 			& 64.7 $\pm\hfil$ 2.2 	 		& 83.9 $\pm\hfil$ 3.5			\\ KDES \citep{Bo_IROS_2011}         				& 77.7 $\pm\hfil$ 1.9 			& 78.8 $\pm\hfil$ 2.7 			& 86.2 $\pm\hfil$ 2.1			\\ CNN-RNN \citep{Socher_NIPS_2012}    				& 80.8 $\pm\hfil$ 4.2 			& 78.9 $\pm\hfil$ 3.8			& 86.8 $\pm\hfil$ 3.3			\\ CaRFs \citep{Asif_ICRA_2015}         			& - 			                & - 			                & 88.1 $\pm\hfil$ 2.4			\\ MMDL \citep{Wang_2015_IEEE_ToM}         			& 74.6 $\pm\hfil$ 2.9			& 75.5 $\pm\hfil$ 2.7			& 86.9 $\pm\hfil$ 2.6			\\ Subset-RNN \citep{Bai_Neurocomp_2015}  			& 82.8 $\pm\hfil$ 3.4 			& 81.8 $\pm\hfil$ 2.6 	 		& 88.5 $\pm\hfil$ 3.1			\\ CNN Features \citep{Schwarz_ICRA_2015}  	        & 83.1 $\pm\hfil$ 2.0 			& -								& 89.4 $\pm\hfil$ 1.3			\\ CNN-SPM-RNN \citep{Cheng_CVIU_2015}        		& 85.2 $\pm\hfil$ 1.2		 	& 83.6 $\pm\hfil$ 2.3 			& 90.7 $\pm\hfil$ 1.1			\\ CFK \citep{Cheng_3DV_2015}  						& 86.8 $\pm\hfil$ 2.7 			& 85.8 $\pm\hfil$ 2.3	        & 91.2 $\pm\hfil$ 1.4			\\ Fus-CNN \citep{Eitel_IROS_2015}  	        	& 84.1 $\pm\hfil$ 2.7 			& 83.8 $\pm\hfil$ 2.7			& 91.3 $\pm\hfil$ 1.4			\\ AlexNet-RNN \citep{Bui_Access_2016}  			& 89.7 $\pm\hfil$ 1.7 			& -								& -								\\ Fusion 2D/3D CNNs \citep{Zia_ICCVW_2017}         & 89.0 $\pm\hfil$ 2.1 			& 78.4 $\pm\hfil$ 2.4			& 91.8 $\pm\hfil$ 0.9			\\ STEM-CaRFs \citep{Asif_ToR_2017}  			    & 88.8 $\pm\hfil$ 2.0 			& 80.8 $\pm\hfil$ 2.1			& 92.2 $\pm\hfil$ 1.3			\\
				MM-LRF-ELM \citep{Liu_Neurocomp_2018}        	& 84.3 $\pm\hfil$ 3.2		 	& 82.9 $\pm\hfil$ 2.5 			& 89.6 $\pm\hfil$ 2.5			\\ VGG\_f-RNN \citep{Caglayan_ECCVW_2018}     		& \bftab\textcolor{darkgreen}{89.9 $\pm\hfil$ 1.6} 	        & 84.0 $\pm\hfil$ 1.8			& 92.5 $\pm\hfil$ 1.2	        \\
				DECO \citep{Carlucci_RAS_2018}     		        & 89.5 $\pm\hfil$ 1.6 	        & 84.0 $\pm\hfil$ 2.3			& \bftab\textcolor{darkgreen}{93.6 $\pm\hfil$ 0.9}	        \\
				MDSI-CNN \citep{Asif_TPAMI_2018}  			    & \bftab\textcolor{darkgreen}{89.9 $\pm\hfil$ 1.8} 			& 84.9 $\pm\hfil$ 1.7			& 92.8 $\pm\hfil$ 1.2			\\
				HP-CNN \citep{Zaki_AuotRobots_2019}    		    & 87.6 $\pm\hfil$ 2.2 			& 85.0 $\pm\hfil$ 2.1			& 91.1 $\pm\hfil$ 1.4			\\ RCFusion \citep{Loghmani_RAL_2019}  			& 89.6 $\pm\hfil$ 2.2 			& \bftab\textcolor{darkgreen}{85.9 $\pm\hfil$ 2.7}			& \bftab\textcolor{red}{94.4 $\pm\hfil$ 1.4}			\\
				MMFLAN \citep{Qiao_2021_Neuroc}  				& 83.9 $\pm\hfil$ 2.2 			& 84.0 $\pm\hfil$ 2.6			& 93.1 $\pm\hfil$ 1.3			\\ \hline
				\bftab{This work - AlexNet-RNN}     			& 83.0 $\pm\hfil$ 1.9 	        & 84.1 $\pm\hfil$ 2.3			& 90.9 $\pm\hfil$ 1.3	            \\
				\bftab{This work - DenseNet121-RNN}     		& \bftab\textcolor{blue}{91.5 $\pm\hfil$ 1.1} 	        & \bftab\textcolor{blue}{86.9 $\pm\hfil$ 2.1}			& 93.5 $\pm\hfil$ 1.0	            \\
				\bftab{This work - ResNet101-RNN}     			& \bftab\textcolor{red}{92.3 $\pm\hfil$ 1.0} 	       & \bftab\textcolor{red}{87.2 $\pm\hfil$ 2.5}			& \bftab\textcolor{blue}{94.1 $\pm\hfil$ 1.0}	            \\
				\hline
			\end{tabular}
		\end{adjustbox}
		\label{table:wrgbdResults}
	\end{center}
\end{table}
 
We also present average accuracy performance of individual object categories on the 10 evaluation splits of Washinton RGB-D Object dataset using the best-performing structure, ResNet101-RNN. As shown in Fig. \ref{fig:wrgbdIndividualResults}, our approach is highly accurate in recognition of the most of the object categories. Categories with lower accuray results are \textit{mushroom}, \textit{peach}, and \textit{pitcher}. The common reason that leads to the lower performance in these categories seems to be due to their less number of instances. In particular, these categories have only $3$ instances, which is the minimum number for any category in the dataset. Considering the other categories with up to $14$ instances, this imbalance of the data may have biased the learning to favor of categories with more examples. Moreover, the accuracy of our combined RGB and depth based on weighted confidences of modalities reflects that the fusion of RGB and depth data in this way can provide strong discrimination capability for object categories.

\subsection{Scene Recognition Performance} \label{sec:exp.sceneRecognition}
To test the generalization ability of our approach, we also carry out comparative analysis of our best-performing model, namely ResNet101-RNN, on SUN RGB-D Scene \citep{Song_CVPR_2015} dataset for scene recognition as a more challenging task of scene understanding. To this end, we first apply ResNet101 pretrained model without finetuning, namely Fix ResNet101-RNN, for both of RGB and depth modalities. Then, we finetune the pretrained CNN model on SUN RGB-D Scene dataset using the same hyper-parameters of object recognition task (see Sec. \ref{sec.exp.ma.finetuning}). The results of these experiments together with the-state-of-the-art results on this dataset are reported in Table \ref{table:sunrgbdResults}. Our best system greatly improves the-state-of-the-art methods not only for RGB-D final result but also for individual data modalities. It is worth mentioning that we use the pretrained CNN model on object-centric dataset of ImageNet \citep{Deng_Imagenet_CVPR_2009}, which is less commonly used for scene recognition task than the pretrained models on scene-centric datasets such as Places \citep{Zhou_NIPS_2014}. Nevertheless, our approach produces superior results compared to existing state-of-the-art methods for RGB-D scene recognition task. Moreoever, it is interesting that our system even with fixed pretrained CNN model is already discriminative enough and achieves impressive accuracy performances. Contrary to our findings on Washington RGB-D Object dataset, finetuning provides much better results not only for depth domain but also for the RGB domain as well. This is what we expect as scene recognition is a cross-domain task for our approach that has the pretrained CNN model of the object-centric ImageNet as the backbone. Specifically, finetuning on depth data boosts the accuracy greatly by providing both domain and modality adaptation.
\begin{table}[!h]
	\caption{Accuracy comparison of our approach with the related methods on SUN RGB-D Scene dataset (\%). \textcolor{red}{Red:} Best result, \textcolor{blue}{Blue:} Second best result, \textcolor{darkgreen}{Green:} Third best result.}
	\begin{center}
		\setlength{\tabcolsep}{0.9em} \def\arraystretch{1.2}
		\begin{adjustbox}{width=\columnwidth}
			\begin{tabular}{ lccc }
				\hline
				Method 											& RGB 				& Depth 			& RGB-D \\ \hline \hline
				Places CNN-Lin SVM \citep{Zhou_NIPS_2014}    	& 35.6 				& 25.5 	 			& 37.2 		\\ Places CNN-RBF SVM \citep{Zhou_NIPS_2014}    	& 38.1 				& 27.7 	 			& 39.0 		\\ SS-CNN-R6 \citep{Liao_ICRA_2016}    			& 36.1 				& - 	 			& 41.3 		\\ DMFF \citep{Zhu_CVPR_2016}    					& 37.0  			& - 				& 41.5 		\\ Places CNN-RCNN \citep{Wang_CVPR_2016}         	& 40.4 	           	& 36.3 	        	& 48.1 		\\ MSMM \citep{Song_IJCAI_2017}         			& 41.5 	           	& 40.1 	        	& 52.3 		\\ RGB-D-CNN \citep{Song_AAAI_2017}         		& 42.7 	           	& 42.4 	        	& 52.4 		\\ MDSI-CNN \citep{Asif_TPAMI_2018}         		& 39.6 	           	& 35.2 	        	& 45.2 		\\ DF\textsuperscript{2}Net \citep{Li_AAAI_2018df} & - 	           	& - 	        	& 54.6 		\\ HP-CNN-T \citep{Zaki_AuotRobots_2019}         	& 38.8 	           	& 28.5 	        	& 42.2 		\\ LM-CNN \citep{2019_CogComp_Cai}         		& 44.3 	           	& 34.6 	        	& 48.7 		\\ RGB-D-OB \citep{Song_TIP_2019}         			& - 	           	& 42.4 	        	& 53.8 		\\ Cross-Modal Graph \citep{Yuan_2019_AAAI}        & 45.7 	           	& - 	        	& 55.1		\\
				RAGC \citep{Montoro_2019_ICCV} 					& - 	           	& - 	        	& 42.1 		\\ MAPNet \citep{2019_PR_Li} 						& - 	           	& - 	        	& 56.2 		\\ TRecgNet Aug \citep{Du_2019_CVPR}         		& 50.6 				& \bftab\textcolor{darkgreen}{47.9} 	& 56.7 		\\
				G-L-SOOR \citep{Song_TIP_2020}         			& 50.5 	           	& 44.1 	        	& 55.5		\\
				MSN \citep{2020_Neuroc_Xiong}         			& - 	           	& - 	        	& 56.2		\\
				CBCL \citep{Ayub_2020_BMVC}         			& 48.8 				& 37.3 	        	& \bftab\textcolor{darkgreen}{59.5} 		\\ 
				ASK \citep{2021_TIP_Xiong}  					& - 				& -	        		& 57.3 					\\
				2D-3D FusionNet \citep{2021_IF_Montoro}  		& \bftab\textcolor{blue}{56.4} 			& 44.1	        		& 58.6 					\\
				TRecgNet Aug - ResNet18 \citep{Du_2021_IJCV}  	& 53.8 				& \bftab\textcolor{blue}{49.3}	        	& 58.5 					\\
				TRecgNet Aug - ResNet101 \citep{Du_2021_IJCV}  	& \bftab\textcolor{darkgreen}{54.2} 			& \bftab\textcolor{blue}{49.3} 	        	& \bftab\textcolor{blue}{59.8} 					\\ \hline
				\bftab{This work - Fix ResNet101-RNN}           & 50.8 				& 38.6 				& 53.1 		\\ \bftab{This work - Finetuned ResNet101-RNN}  	& \bftab\textcolor{red}{58.5}  			& \bftab\textcolor{red}{50.1}  			& \bftab\textcolor{red}{60.7} 	\\ \hline
			\end{tabular}
		\end{adjustbox}
		\label{table:sunrgbdResults}
	\end{center}
\end{table} 
Fig. \ref{fig:sunrgbdConfusionMatrix} shows the confusion matrix of our approach with finetuning over the $19$ categories of SUN RGB-D Scene dataset for RGB-D. The matrix demonstrates the degree of confusion between pairs of scene categories and implies the similarity between scenes on this dataset. The largest misclassification errors happen to be between extremely similar scene categories such as \textit{computer room} - \textit{office}, \textit{conference room}-\textit{classroom}, \textit{discussion area}-\textit{rest space}, \textit{lecture theatre}-\textit{classroom}, \textit{study space}-\textit{classroom}, \textit{lab}-\textit{office}, etc. In addition to the inter-class similarity, other reasons for poor performance might be intra-class variations of the scenes and lack of getting enough representative knowledge transfer from the ImageNet models.
\begin{figure}[!ht]
	\begin{center}
		\includegraphics[width=\columnwidth, keepaspectratio]{sunrgbd_confusion_matrix.pdf}
	\end{center}
	\caption{RGB-D confusion matrix of ResNet101-RNN on SUN RGB-D Scene dataset (best viewed with magnification).}
	\label{fig:sunrgbdConfusionMatrix}
\end{figure}
To further analyse the performance of our system, we give top-3 and top-5 classification accuracy together with top-1 results as in Table \ref{table:top135sceneResults}. While the top-1 accuracy shows the percentage of test images that exactly matches with the predicted classes, the top-3 and top-5 indicates the percentage of test images that are among the top ranked 3 and 5 predictions, respectively. The top-3 and top-5 results demonstrate the effectiveness of our system more closely by overcoming ambiguity among scene categories greatly. Fig. \ref{fig:sunrgbdConfusedSamples} depicts some test examples of scene categories confused with each other frequently on SUN RGB-D Scene dataset. As shown in the figure, these scene categories have similar appearances that make them hard to distinguish even for a human expert without sufficient context knowledge in the evaluation. Nevertheless, our approach is able to identify scene category labels among the top-3 and top-5 predictions with high accuracy.
\begin{table}[!h]
	\caption{Scene recognition accuracy of top-1, top-3, and top-5 on SUN RGB-D Scene dataset (\%).}
	\begin{center}
		\setlength{\tabcolsep}{0.9em} \def\arraystretch{1.2}
		\begin{adjustbox}{width=0.65\columnwidth}
			\begin{tabular}{ lccc }
				\hline
				Accuracy			& RGB 				& Depth 			& RGB-D \\ \hline \hline
				top-1    			& 58.5 				& 50.1 	 			& 60.7 		\\ top-3    			& 81.0 				& 71.5 	 			& 83.5 		\\ top-5    			& 88.5 				& 80.9 	 			& 89.9 		\\ \hline
			\end{tabular}
		\end{adjustbox}
		\label{table:top135sceneResults}
	\end{center}
\end{table}
\begin{figure}[!t]
	\begin{center}
		\includegraphics[width=0.95\columnwidth, keepaspectratio]{confusedScenes.pdf}
	\end{center}
c	\caption{Top-5 RGB-D predictions of our system using sample test images of frequently confused scene categories on SUN RGB-D Scene dataset.}
	\label{fig:sunrgbdConfusedSamples}
\end{figure}
\subsection{Model Ablation} \label{sec:exp.modelAblation}
\begin{figure*}[!ht]
	\centering
	\subfloat{\includegraphics[width=\columnwidth, keepaspectratio]{randomnessRGB2.pdf}}\subfloat{\includegraphics[width=\columnwidth, keepaspectratio]{randomnessDepth2.pdf}}\caption{Effect of randomness on the accuracy results for each level (L1 to L7). Values indicate standard deviations.}
	\label{fig:randomness}
\end{figure*}
We have analyzed and validated the proposed framework with extensive experiments using a variety of architectural configurations on the popular benchmark of Washington RGB-D dataset, which is more than $4$ times larger than the SUN RGB-D scene dataset. In this section, the analysis and evaluations of the model ablative investigations are presented. Further experiments and analysis are given in the \textit{supplementary material}. The developmental experiments are carried out on two splits of Washington RGB-D Object dataset for both modalities in order to evaluate on more stable results. The average results are analyzed. However, in some experiments, more runs have been carried out, which are clearly stated in the related sections. In Sec. \ref{sec:exp.objectRecognition} and Sec. \ref{sec:exp.sceneRecognition}, the best performing models are compared with the state-of-the-art methods with the exact provided evaluation setups. We assess the proposed framework on a PC with AMD Ryzen 9 3900X 12-Core Processor, 3.8 GHz Base, 128 GB DDR4 RAM 2666 MHz, and NVIDIA GeForce GTX 1080 Ti graphics card with 11 GB memory.
\begin{figure*}[!b]
	\centering
	 \subfloat{\includegraphics[width=\columnwidth, keepaspectratio]{layerwisePerformancesRGB.pdf}}\subfloat{\includegraphics[width=\columnwidth, keepaspectratio]{layerwisePerformancesDepth.pdf}}l	\caption{Level-wise average accuracy performance of different baseline models on all the 10-splits of Washington RGB-D dataset.}
	\label{fig:levelwisePerformances}
\end{figure*}
\subsubsection{Empirical Evaluation of the Effect of Randomness}
\label{sec.exp.ma.randomness}
The use of random weights both in pooling and RNN structures leads to the question of how stable are the results. Thus, we experimentally investigate to see whether there is a decisive difference between different runs that generate and use new random weights. We run the pipeline with different random weights on two splits, 5 times for each. Fig. \ref{fig:randomness} reports average results with their standard deviations for each level. The figure clearly shows that randomness does not cause any instability in the model and produces similar results with very small deviations.

\subsubsection{Level-wise Performance of Different Models} \label{sec.exp.ma.levelPerformances}
Fig. \ref{fig:levelwisePerformances} shows level-wise average accuracy performances of all the baseline models for both of RGB and depth modalities on all the 10 evaluation splits. The graphs show a similar performance trend line with an upward at the beginning and a downward at the end. Although the levels at which optimum performance is obtained vary according to the model, what is common to all models in general is that instead of final level representations, intermediate level representations present the optimal results. These experiments also verify that while deep models transform attributes from general to specific through  the network eventually \citep{Razavian_CVPRW_2014, Zeiler_ECCV_2014}, intermediate layers present the optimal representations. This makes sense because while early layers response to low-level raw features such as corners and edges, late layers extract more object-specific features of the trained datasets. This is more clear on the depth plot in Fig. \ref{fig:levelwisePerformances}, where the dataset difference is obvious due to the domain difference. We should state that RNN encoding on features extracted from FC layers with less than $8192$ dimension might not be efficient since they are already compact enough. Therefore, encoding outputs of these layers to a larger feature space through RNNs might lead to redundancy in representations. This might be another reason why there is a drop in accuracy of these layers (e.g. see L7 in Fig. \ref{fig:levelwisePerformances}). In addition, depth plot contains more fluctuations and irregularities comparing to the RGB plot, since the pretrained models of the RGB ImageNet are used as fixed extractors without finetuning. As for the different baseline model comparison, ResNet-101 and DenseNet-121 models perform similarly in terms of accuracy and are better than others.
\begin{figure*}[!hb]
	\centering
	 \subfloat{\includegraphics[width=\columnwidth, keepaspectratio]{finetuningRGB.pdf}}\subfloat{\includegraphics[width=\columnwidth, keepaspectratio]{finetuningDepth.pdf}}f	\caption{Level-wise average accuracy performance of finetuned CNN models together with fixed models on all the 10-splits of Washington RGB-D dataset.}
	\label{fig:finetuning}
\end{figure*}
\subsubsection{Comparative Results of Random Weighted Pooling} \label{sec.exp.ma.poolingPerformances}
In our approach, we extend the idea of randomness into a pooling strategy to cope with the high dimensionality of CNN activations, which could not be only applied to map/window size but also can be used to reduce the number of maps. We particularly employ random pooling to confirm that randomness works greatly in overall RNN-Stage even in such a pooling strategy together with random RNNs. To this end, we investigate the comparative accuracy performances of random pooling together with average pooling and max pooling. We use the DenseNet-121 model, where pooling is used extensively on each level (except in level 4), and we conduct experiments using the same RNN weights for fair comparison. Fig. \ref{fig:poolingComparison} shows average accuracy results of two splits for each pooling on both RGB and depth data. As seen from the figure, random weighted pooling generally performs similar to average pooling, while its performance in average is better than max pooling. Moreover, it is seen that random pooling acquires better results especially in middle/late levels(L4-L7), which presents more stable and semantically meaningful representations comparing to the early levels. The results also show that the proposed random pooling and average pooling can be used interchangeably as their performances are similar.
\begin{figure}[!ht]
	\centering
	\includegraphics[width=\columnwidth, keepaspectratio]{poolingComparison.pdf}
p	\caption{Average accuracy performance of different pooling methods on RGB and depth data for the baseline model of DenseNet-121 on two splits of Washington RGB-D dataset.}
	\label{fig:poolingComparison}
\end{figure}

We further investigate the comparative accuracy performance of the proposed random pooling in our final ResNet-101 based pipeline. As it can be seen in Table~\ref{table:poolingComparison}, when the proposed pipeline is set to use random weighted pooling, it produces better or similar accuracy than max pooling and average pooling-based pipelines. This validates the power of randomness in a pooling strategy and the use of random pooling as an alternative way for down-sampling.
\begin{table}[!h]
	\caption{Average accuracy performance of different pooling methods in the best performing ResNet101-RNN pipeline on Washington RGB-D dataset (\%).}
	\begin{center}
		\setlength{\tabcolsep}{0.9em} \def\arraystretch{1.2}
		\begin{adjustbox}{width=0.75\columnwidth}
			\begin{tabular}{ lccc }
				\hline
				Accuracy			& RGB 								& Depth 							& RGB-D \\ \hline \hline
				Max   				& 91.1 $\pm\hfil$ 1.4 				& 87.1 $\pm\hfil$ 2.5 	 			& 93.8 $\pm\hfil$ 0.9 		\\ Average    			& 91.6 $\pm\hfil$ 1.6 				& 87.2 $\pm\hfil$ 2.5 	 			& 94.0 $\pm\hfil$ 1.0 		\\ Random   			& 92.3 $\pm\hfil$ 1.0 				& 87.2 $\pm\hfil$ 2.5 	 			& 94.1 $\pm\hfil$ 1.0 		\\ \hline
			\end{tabular}
		\end{adjustbox}
		\label{table:poolingComparison}
	\end{center}
\end{table}

\subsubsection{Contribution of Finetuning} \label{sec.exp.ma.finetuning}
We have not used any training or finetuning in our approach to feature extraction in the ablative experiments so far (except Table~\ref{table:poolingComparison}, where depth modality-based ResNet101 is finetuned). Although impressive results are obtained on RGB data, the same success is not achieved on depth data. The reason for this difference is that the baseline CNN models are pretrained models on RGB dataset of the ImageNet. Therefore, as the next step, we analyze the changes in accuracy performance of RGB and depth data modalities by finetuning the baseline CNN models in our approach. To this end, we first carry out a systematic inquiry to find optimal finetuning hyper-parameters on a predefined set of values using only one split of Washington RGB-D dataset as a validation set for AlexNet and DenseNet-121 models. Then, finetuning of the models are performed by stochastic gradient descent (SGD) with momentum. The hyper-parameters of momentum, learning rate, batch size, learning rate decay factor and decay step size, and number of epochs, respectively are used as following; $(0.9, 0.001, 32, 0.01, 10, 40)$ and $(0.9, 0.0001, 8, 0.1, 10, 40)$ are used for AlexNet on RGB and depth data, respectively, whereas $(0.95, 0.0001, 16, 0.1, 10, 40)$ and $(0.95, 0.001, 8, 0.1, 10, 40)$ are used for DenseNet-121. Apart from these two models, we also perform finetuning on the ResNet-101 model. We use the same finetuning hyperparameters of DenseNet-121 for ResNet-101, since they are in a similar architectural structure. Fig. \ref{fig:finetuning} shows average accuracy performance of finetuned CNN models together with fixed models on all the 10 evaluation splits of Washington RGB-D object dataset. The plot shows a clear upward in performance on depth data as expected. However, there is a loss of accuracy in general, when finetuning is performed on RGB data. Washington RGB-D object dataset contains a subset of the categories in ImageNet. Accordingly, pretrained models of ImageNet already satisfy highly correlated distribution on RGB data. Therefore, there is no need for finetuning on RGB data. In contrast, in order to ensure coherence and relevance, finetuning is required for depth data due to domain difference of the inputs with the pretrained models.

\subsubsection{Weighted Voting based RGB-D Fusion Performance} \label{sec.exp.ma.weightedFusion}
Finally, we provide RGB-D combined results for AlexNet, DenseNet-121, and ResNet-101 models as shown in Table \ref{table:rgbdFusions} based on the SVM confidences. The table reports average results for fusion of the best levels of RGB and depth, and the best trio levels (see the \textit{supplementary material}). We evaluate two types of soft voting, our proposed weighted vote and average vote. The proposed weighted vote increases accuracy comparing to the average vote for all the models both on the multi-modal fusion of the best single and best trio levels of RGB and depth streams. The results also confirm the strength of our multi-modal voting approach that combines RGB and depth modalities effectively. On the other hand, the reason why RGB-D fusion improves the individual RGB and depth results lies in the fact that these different data modalities support each other towards a more accurate representation by capturing different aspects of the data with a strong complementary approach. RGB data are rich in terms of texture and color information. Depth data have additional geometric information to depict object shapes. Moreover, depth sensors are more insensitive to changes in lighting conditions. Therefore, multi-modal data combination is useful not only for its integrative characteristic, but also for its complementarity when one modality data is lacking such as RGB data in dark environment or depth data on shiny surfaces.
\begin{table}[!h]
	\caption{Average accuracy performance of RGB-D (RGB + Depth) with different fusion combinations on Washington RGB-D dataset (\%).}  
	\label{table:rgbdFusions}    
	\centering
	\setlength{\tabcolsep}{0.9em} \def\arraystretch{1.2}
	\begin{adjustbox}{width=\columnwidth}
		\begin{tabular}{llccc}
			\hline
			&								       																				& AlexNet 						  & DenseNet-121 					& ResNet-101 				\\ \hline \hline
			Avg Vote 		& $\scalebox{.9}{RGB}_{\scalebox{.7}{LB1}} \kern 3.3em+ \scalebox{.9}{Depth}_{\scalebox{.7}{LB1}}$          		& 90.2 $\pm\hfil$ 1.3        	  & 92.9 $\pm\hfil$ 1.4           	& 92.7 $\pm\hfil$ 1.6         \\
			Weighted Vote 	& $\scalebox{.9}{RGB}_{\scalebox{.7}{LB1}} \kern 3.3em+ \scalebox{.9}{Depth}_{\scalebox{.7}{LB1}}$     			   	& 90.2 $\pm\hfil$ 1.2        	  & 93.5 $\pm\hfil$ 1.0          	& 93.8 $\pm\hfil$ 1.1          \\
			Avg Vote 		& $\scalebox{.9}{RGB}_{\scalebox{.7}{LB1+LB2+LB3}} \kern 0.2em+ \scalebox{.9}{Depth}_{\scalebox{.7}{LB1+LB2+LB3}}$  & 90.6 $\pm\hfil$ 1.6        	  & 92.6 $\pm\hfil$ 1.4          	& 93.0 $\pm\hfil$ 1.3           \\
			Weighted Vote 	& $\scalebox{.9}{RGB}_{\scalebox{.7}{LB1+LB2+LB3}} \kern 0.2em+ \scalebox{.9}{Depth}_{\scalebox{.7}{LB1+LB2+LB3}}$  & \textbf{90.9 $\pm\hfil$ 1.3}    & \textbf{93.5 $\pm\hfil$ 1.0}    & \textbf{94.1 $\pm\hfil$ 1.0} 	 \\ \hline
			\multicolumn{5}{l}{LB1: Best performing level \quad LB2: Second best performing level \quad LB3: Third best performing level}
		\end{tabular}
	\end{adjustbox}
\end{table}
 
\subsection{Discussion} \label{sec:exp.discussion}
Our framework presents an effective solution for deep feature extraction in an efficient way by integrating a pretrained CNN model with random weights based RNNs. Randomization throughout our RNN-Stage raises the question of whether the results are stable enough. The carefully implemented experiments in Sec. \ref{sec.exp.ma.randomness} are an empirical justification for the stability of random weights. On the other hand, our multi-level analysis shows that the optimum performance gain from a single level always comes from an intermediate level for all the models with/without finetuning for both of RGB and depth modalities. The only exception is in the use of finetuned DenseNet-121 model on depth data. This is an interesting finding, because one expects better representation capabilities of final layers, especially in the use of finetuned models. Yet, as expected, performance generally increases from the first level to the last level throughout the networks when the underlying CNN models are finetuned. Since Washington RGB-D Object \citep{Lai_ICRA_2011} dataset includes a subset of object categories in the ImageNet \citep{Deng_Imagenet_CVPR_2009}, finetuning does not improve accuracy success on RGB data. In contrast, accuracy gain is significant due to the need for domain adaptation in depth data. This also shows that using an appropriate technique to handle depth data as in our approach (see the \textit{supplementary material}), leads impressive performance improvement by knowledge transfer between modalities. 

In this study, although we have explored different techniques to fuse representations of multiple levels to further increase the classification success, a single optimum level may actually be sufficient enough for many tasks. In this way, especially for tasks where computational time is more critical, results can be obtained much faster without sacrificing accuracy success. Another point of interest is that the data imbalance in Washington RGB-D Object dataset results in poor performance for the individual categories with less instances and consequently leads to a drop in the overall success of the system. Hence, this imbalance might be overcome by applying data augmentation on the categories with less instances.

The success of our approach for RGB-D scene recognition confirms the generalization ability of the proposed framework. Unlike object recognition, when the underlying CNN models are finetuned, success in both RGB and depth modalities increases significantly in scene recognition task. This is due to the need for cross-domain task adaptation of object-centric based pretrained models. Therefore, similar findings in object recognition could be observed if scene-centric based pretrained models are employed for scene recognition (e. g. Places \citep{Zhou_NIPS_2014}). Moreover, such pretrained models could improve the results further within our framework. Another potential that could improve the success for scene recognition is embedding contextual knowledge by jointly employing attention mechanism such as \cite{Fukui_2019_CVPR} in our structure.

This work has been implemented as the extension of our previous work \citep{Caglayan_ECCVW_2018}. Therefore, we have not explored further multimodal architectures. However, instead of SVM, combining level-wise outputs through a multilayer perceptron (MLP) might be more convenient for RGB-D multimodal design. In particular, it would be interesting to use the soft voting approach proposed in this study with MLP. In the future, we plan to investigate such an approach for a better RGB-D multimodal tasks, such that success is focused on the ultimate RGB-D fusion rather than the individual accuracy success of the RGB and depth modalities.