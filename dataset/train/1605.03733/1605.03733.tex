\documentclass[10pt]{article}
\usepackage[T1]{fontenc}
\usepackage[ansinew]{inputenc}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{bbm}
\usepackage{mathtools}
\usepackage{color}
\usepackage{float}

\newcommand{\N}{\mathbbm{N}}
\newcommand{\Z}{\mathbbm{Z}}
\newcommand{\R}{\mathbbm{R}}
\newcommand{\devec}{\mathbf{R}}
\renewcommand{\P}{\mathbbm{P}}
\newcommand{\T}{\mathbf{T}}
\newcommand{\E}{\mathbf{E}}
\newcommand{\cov}{\mathrm{cov}}
\newcommand{\trace}{\mathrm{trace}}
\newcommand{\rmatrix}[1]{\begin{bmatrix}#1\end{bmatrix}}

\newcommand{\rsrsl}[1]{\textcolor{red}{#1}}

\newtheorem{example}{Example}
\newtheorem{corollary}{Corollary}
\newtheorem{problem}{Problem}
\newtheorem{remark}{Remark}
\newtheorem{proposition}{Proposition}
\newtheorem{theorem}{Theorem}



\begin{document}
\title{\LARGE \bf Kernel-based system identification\\ from noisy and
  incomplete input-output data}
\author{Riccardo S. Risuleo, Giulio Bottegal and H\r akan Hjalmarsson
  \thanks{R. S. Risuleo, G. Bottegal and H. Hjalmarsson are with the ACCESS
    Linnaeus Center, School of Electrical Engineering, KTH Royal Institute of
    Technology, Sweden ({\tt \small risuleo;bottegal;hjalmars@kth.se}).  This
    work was supported by the European Research Council under the advanced
    grant LEARN, contract 267381 and by the Swedish Research Council under
    contract 621-2009-4017.  }} \maketitle


\begin{abstract}
  In this contribution, we propose a kernel-based method for the identification
  of linear systems from noisy and incomplete input-output datasets. We model
  the impulse response of the system as a Gaussian process whose covariance
  matrix is given by the recently introduced stable spline kernel. We adopt an
  empirical Bayes approach to estimate the posterior distribution of the impulse
  response given the data.  The noiseless and missing data samples, together
  with the kernel hyperparameters, are estimated maximizing the joint marginal
  likelihood of the input and output measurements. To compute the
  marginal-likelihood maximizer, we build a solution scheme based on the
  Expectation-Maximization method. Simulations on a benchmark dataset show the
  effectiveness of the method.
\end{abstract}

\section{Introduction}
Common formulations of system identification problems postulate the perfect
knowledge of the input signal feeding the unknown system~\cite{ljung1999system}.
In many applications however, the input signal is available only in a noisy
version, giving raise to a setup usually referred to as an
\emph{errors-in-variables} (EIV) model~\cite{soederstroem2007errors}.  Static
EIV models have been subject of extensive studies in the statistical literature
since the beginning of the last century~\cite{frisch1934statistical}; later the
system identification community has become interested in dynamical EIV
models~\cite{anderson1985identification,fernando1985identification,soederstroem2007errors}.

Identification of EIV systems is a challenging task; even in the linear case,
standard least-squares yields biased estimates, due to the presence of noise in
the regressors. Therefore, lots of efforts have been devoted to the development
of ad-hoc methods for EIV systems. Bias eliminating least-squares (BELS) have
been introduced in~\cite{zheng1989unbiased} to correct the bias of standard
least-squares. Another method for the identification of EIV systems has been
obtained by generalizing the so-called Frisch scheme, originally developed for
static EIV models~\cite{frisch1934statistical}, to dynamic models.
Interestingly, the dynamic Frisch scheme provides a unique identified model
under mild conditions~\cite{beghelli1990frisch}. This is in contrast with the
static case, where in general many models are compatible with the observed data.
The accuracy of the Frisch scheme for dynamic system identification has been
extensively studied in the literature~\cite{soederstroem2002perspectives}; the
method has been recently extended to more general noise
setups~\cite{fan2010frisch,ning2015linear,zhang2015errors}. Other EIV
identification methods rely upon maximum-likelihood criteria. Both
time-domain~\cite{soederstroem1981identification,diversi2007maximum} and
frequency-domain~\cite{schoukens1997frequency} approaches have been developed in
the past; for a survey and a comparison of the maximum likelihood methods
see~\cite{soederstroem2010accuracy}.

In this paper, we consider a more general dynamic EIV setup. Specifically, we
assume  that some of the samples may be missing, for instance due to lossy
transmission channels or sensor malfunction. Therefore, our task is to jointly
identify the system and reconstruct the missing input-output values. Some
techniques to deal with this problem have been proposed in the past, both in
time and frequency
domains~\cite{isaksson1993Identification,pintelon2000frequency,wallin2014maximum,markovsky2015identification,zhang2013errors}.
Recently, regularization techniques based on nuclear norm have been proposed for
system identification with missing
data~\cite{liu2013nuclear,markovsky2013structured}.

The method described in this paper to deal with EIV models with missing data
relies on a regularized kernel-based approach. Interpreting kernel-based
regularization as a  Gaussian regression problem~\cite{rasmussen2006gaussian},
we model the unknown impulse response of the system as a zero-mean Gaussian
random vector. The covariance matrix is given by the recently introduced stable
spline kernel~\cite{pillonetto2010new,pillonetto2014kernel}, which penalizes
non-exponentially stable systems. According to the \emph{empirical Bayes} (EB)
paradigm~\cite{maritz1989empirical}, we obtain an impulse response estimator as
a function of the noiseless input and the kernel hyperparameters. These
quantities are estimated maximizing the joint \emph{marginal likelihood}
(marginal likelihood) of the noisy inputs and outputs. We devise an iterative
algorithm to solve the marginal likelihood maximization problem, based on the EM
method. We briefly address the problem of identifiability. We test the proposed
approach with numerical simulations.

In this paper we consider the case of both missing input and missing output
samples, as well as noisy data; as compared to~\cite{pillonetto2009bayesian}
where a kernel-based approach is adopted for the case of noiseless input and
missing output samples.

The paper is organized as follows. In Section~\ref{sec:formulation} we
formulate the problem the identification of EIV models with missing data. In
Section~\ref{sec:sysid} we show how to estimate the system impulse response. In
Section~\ref{sec:ml} we solve the marginal likelihood problem that yields the
missing samples and the kernel hyperparameters. In
Section~\ref{sec:identifiability} we discuss some pitfalls in the model. In
Section~\ref{sec:simulations} we validate our method on a benchmark dataset. In
Section~\ref{sec:conclusions} we discuss our results and conclude the paper.

\subsection{Notation}
We denote by ``'' a sequence of scalars  indexed by ;
``''
is the set of  with  ranging from  to . Given
,
``'' indicates the column vector of the stacked scalars and
``'' indicates the th element of said vector. If  is a column
vector, ``'' indicates the Toeplitz operator that associates
to the vector  the  matrix  such that

The symbol ``'' denotes the Kronecker delta and ``'' is the
standard Kronecker product between matrices. The symbol ``'' indicates
equality up to an additive constant.

\section{Problem formulation}\label{sec:formulation}
We consider the problem of identifying a dynamic system
from noisy samples of input and output. Figure~\ref{fig:block_scheme} shows a
schematic representation of the setup under study.
\begin{figure}[htb]
  \centering
  \includegraphics[width=0.5\columnwidth]{main-figure0.pdf}
  \caption{Block scheme of the system setup.}\label{fig:block_scheme}
\end{figure}
The system is strictly causal, asymptotically stable, and linear
time-invariant. The input-output relation can be represented as

where the  is the (unknown) impulse response
of the system. The objective is to reconstruct
the samples of the impulse response from  samples of
the input  and output . These samples
are measurements of the true system input  and output , corrupted by
sensor noises

The noise sequences  and  are assumed mutually
independent, Gaussian and white, with unknown variance  and
, respectively. The ratio  is assumed
known, in order to guarantee identifiability (see
e.g.~\cite{fernando1985identification}). We suppose that the system is at rest
prior to the collection of the measurements, that is , , for all
.

We assume also that some of the samples are not available; see the following
example.

\begin{example}
  We have run a system for  time instants collecting the following measurements:

\end{example}

Define the set of natural numbers  such that
, and  is an
available measurement. In a similar fashion define .
These sets indicate the  and  time instants at
which we have available sensor measurements of the input and output
respectively. We
define the available measurement vectors  and
 such that

Furthermore, define the operators  and  as the  respectively  matrices defined by

By construction, these matrices are \emph{right semi-orthogonal}:

they have full row rank and they represent the mappings betweev the complete
data and the available data:

where  and  are vectors of all the stacked values of all (available and
  not) measurements of input and output.
\setcounter{example}{0}
\begin{example}[continued]
  The times of available input measurements are:
  
  and the  matrix  is
  
\end{example}
\vspace{0.5em}

We can now formally define the problems of interest in this paper.
\begin{problem}\label{problem:1}[\textbf{System Identification}]
Given  ordered samples of the input process , collected at times
, and  ordered samples of the output process , collected at
times , estimate the first  samples of the system impulse response
.
\end{problem}
We also consider the problem of reconstructing the missing samples:
\begin{problem}\label{problem:2}[\textbf{Input smoothing}]
Given  ordered samples of the input process , collected at times
, and  ordered samples of the output process , collected at
times , estimate the sample , .
\end{problem}
\begin{problem}\label{problem:3}[\textbf{Output smoothing}]
Given  ordered samples of the input process , collected at times
, and  ordered samples of the output process , collected at
times , estimate the sample , .
\end{problem}

By adopting a kernel-based approach, we introduce a nonparametric model for
the impulse response that allows us to solve the three proposed problems
with a single algorithm based on a marginal likelihood approach. We will first
see how to solve Problem~\ref{problem:1}, using an EB approach.

\begin{remark}
  If  for , then  and , and  Problem~\ref{problem:1} corresponds to the standard dynamic
  EIV setup (see~\cite{soederstroem2007errors} for a survey).
\end{remark}

\section{Kernel-based linear system identification}\label{sec:sysid}
We first focus on the problem of identifying . For a given ,
Problem~\ref{problem:1} becomes a linear regression problem: collecting
 into the column vector , we can construct the 
Toeplitz matrix ; using this matrix we can write the
convolution~\eqref{eq:convolution} as the matrix product

and we can formulate the regression problem in the available output data:

In this equation  are the samples of the
noise that correspond to the available samples of the output. From the
semi-orthogonality of , we have that

Adopting a kernel-based approach~\cite{pillonetto2014kernel}, we model  as a
Gaussian random vector, with covariance matrix given by a kernel function
suitable for linear system identification. In particular, we use the
\emph{first-order stable-spline kernel}~\cite{pillonetto2010new}, so that

The quantity  is a scaling factor, while  is a
shaping parameter that regulates the exponential decay of the realizations from~\eqref{eq:prior}. These two parameters are usually referred to as
\emph{hyperparameters}.

By postulating~\eqref{eq:prior}  a Gaussian prior for the impulse response, we can
derive the joint distribution of the measurements  and  as

where

From~\eqref{eq:joint} we can calculate the posterior distribution of the unknown
impulse response parameters given the available data  (See, e.g.,~\cite[App. B.7]{soederstroem1981identification}):

where

With the posterior distribution, we find the (Bayesian) minimum variance
estimate of  as the posterior mean .
From~\eqref{eq:posterior_pars}, we see that the posterior mean depends on the quantities
,  and , as well as on the output noise covariance
. All these parameters are unknown and need to be estimated from the data. Using an
EB approach, we estimate the parameters by replacing them with their maximum
marginal likelihood
estimates ,  and  (and , but
this needs a special treatment: see Section~\ref{sec:identifiability}). In the
next section we focus on the problem of finding the maximizers
of the marginal likelihood. Solving the marginal likelihood problem is also the key to solve
Problem~\ref{problem:2} and Problem~\ref{problem:3}.

\section{Kernel-based input and output smoothing}\label{sec:ml}
\subsection*{Input smoothing and hyperparameter selection}
Consider the measurement model~\eqref{eq:measurements}. We can write it as a
regression in the smoothed input  by observing that

where . Considering also the unavailability of some
data, we obtain the linear regression model in the available measurements

where  is white noise of variance .
Under the Bayesian prior assumption~\eqref{eq:prior}, the available observation
vector and the impulse response parameter are jointly Gaussian, with a
log-likelihood given by

where

Since  is not available, we can interpret it as a \emph{latent variable},
which we estimate using the expectation maximization (EM) method.  The term
``EM method'' refers to a class of algorithms used to solve maximum likelihood
problems with latent variables.  In these methods, an iterative algorithm is
built by alternating between estimating the likelihood, and
updating the likelihood parameters using the estimated likelihood.

The estimated likelihood is created in the \emph{expectation step} by taking the conditional
expectation of the log-likelihood with respect to the posterior distribution of
the latent variables given the available data, for some estimate of the parameters:

where  are the parameters to be estimated, and
the expectation is taken with respect of the distribution~\eqref{eq:posterior},
with the vector  replaced with an estimate . By
construction this function
is such that

where  is the marginal likelihood of the available data.
In the subsequent \emph{maximization step}, the parameter update is chosen as
the maximum of , so that

and consequently the marginal likelihood, in the updated parameters, is increased as well.
By iterating the expectation and maximization steps, from any initialization of
the parameters, we obtain a sequence of estimates of the parameters that
converge to a local maximizer of the marginal likelihood of the available data (for a
complete look on the EM algorithm~\cite{mclachlan2007algorithm}).

In the case at hand, we can compute the expectation of the joint log-likelihood~\eqref{eq:complete_likelihood} in closed form:

All the expectations are taken with respect to the posterior
distribution~\eqref{eq:posterior}, with , , and  replaced by
their estimates ,  and . In these
expressions  and   are the posterior mean and covariance as
expressed in~\eqref{eq:posterior_pars}, with the unknown parameters replaced by
their estimates;  is the Toeplitz matrix of the
posterior mean and  is the posterior second moment of the matrix
, that is

The  matrix  is defined as

where  is the  upward shift operator


To find the updated parameter values, we maximize the conditional expectations
with respect to the parameters:


The cost function in~\eqref{eq:update_w} is quadratic in the decision variable,
so the maximum is available in closed form as

where .
The optimization~\eqref{eq:update_hypers} can be solved in closed form with
respect to :

With this, the update of  is given by

which can be solved with scalar optimization methods, or grid search. Once
we have , we also have the update for~:


Appealing to theory of the EM-method, we have the following result:
\begin{theorem}\label{thm:1}
  The sequences , , and 
  generated by the iterations~\eqref{eq:update_w} and~\eqref{eq:update_hypers}
  are such that:
  
  where  is the marginal likelihood of
  the data; and
  
  as ,  where  is a local extremum of .
\end{theorem}
\begin{proof}
  By construction, the iterations~\eqref{eq:update_w} and~\eqref{eq:update_hypers} are iterations in an EM algorithm. The E-step function~\eqref{eq:Q}, seen as the function of two variables  is continuous in
   and . So the sequence generated satisfy the conditions in Theorem~1 in~\cite{wu1983convergence} and~\eqref{eq:proof1} and~\eqref{eq:proof2} follow.
\end{proof}
Interestingly, except for pathological cases, the EM-method is guaranteed to
converge to a local maximum of the marginal likelihood (see~\cite[Ch. 3]{mclachlan2007algorithm} for details).
\begin{corollary}\label{cor:1}
  If the sequences , , and
  
  generated by the iterations~\eqref{eq:update_w} and~\eqref{eq:update_hypers}
  are such that:
  
  as , then they converge to a stationary point of .
\end{corollary}
\begin{proof}
  Follows directly from Theorem~6 in~\cite{wu1983convergence}.
\end{proof}
\begin{remark}
  Theorem~\ref{thm:1} gives a natural stopping criterion for the EM algorithm.
  When the increase in the likelihood between two iterates is below a certain
  threshold, (approximate) convergence to a maximimum is safely guaranteed.
  Corollary~\ref{cor:1} further guarantees the convergence of the parameters to
  a local maximizer when the change between iterations is infinitesimal.
\end{remark}
\subsection{Kernel-based output smoothing}\label{sec:output}
To solve Problem~\ref{problem:3} we first observe that, since the output noise
is white, the output smoothing problem is a simulation problem, and the
smoothed output signal is given by the convolution .
After solving Problem~\ref{problem:1} and Problem~\ref{problem:2}, we can find
an estimate of the smoothed output signal  by plugging in the
estimates  and  in the convolution, obtaining


\section{Some remarks on identifiability}\label{sec:identifiability}
It is well known (see, e.g.~\cite{bottegal2011identifiability},~\cite{soederstroem2002perspectives}) that, in general,
errors-in-variables problems are not identifiable. Different
models may explain the same observed data and therefore it is impossible to
assess the validity of a certain model from the data. In the case of Gaussian
noise, where only
second moments carry information about the distributions, any attempt to
identify the noise variances, the system, and the input samples is bound to
fail.  In our EM framework, this follows from the shape of the likelihood~\eqref{eq:complete_likelihood}: for instance leaving free both  and
,  we can choose  and 
can be made arbitrarily large by choosing a small enough . Various
additional assumptions can be posed to circumvent the non-identifiability
issue, see~\cite{soederstroem2007errors}. In our setup, if we know the ratio
 we can estimate the unknown variances by
adding the following equation to the iterations of the EM method: 

In the case of missing data we have other identifiability problems, in
addition to the ones inherited from errors-in-variables. The possibility of
multiple models explaining the available data is linked to aliasing, as the
missing data can be seen as data decimation~\cite{wallin2002multiple}. In order
to have a unique solution to the likelihood problem
 where  is the true
  impulse response, we need that the symmetric matrix
   is invertible. This is the case as long as the effect of every
missing input sample is visible at least once in the output:
\begin{proposition}\label{prop:identifiability}
  There is a unique solution to~\eqref{eq:minimize_w} if and
  only if for every missing input sample time , there
  is a  and a  such that 
  and .  \end{proposition} \begin{proof} Matrix~\eqref{eq:A} is
  invertible iff there is no  such that  and
  . The condition  means that  can be
  written as  where  are vectors in
  the canonical basis of  and  are scalars. The condition
   translates into , where  is defined in~\eqref{eq:shift_op}. This concludes the
  proof.  \end{proof}
g
\section{Simulations}\label{sec:simulations}
To evaluate the performance of the proposed method, we perform a set of Monte
Carlo (MC) simulations. In the MC simulations, we identify the impulse
responses of 500 systems from the dataset D1 described in~\cite{chen2012sparse}.
For each system in the dataset, we generate  input and output
samples; the input is Gaussian white noise with variance equal to 1. The output
measurements are affected by Gaussian white noise of variance equal 0.1, namely
10\% of the noiseless output variance. The variance of the noise affecting the
input varies with the experiment.

We use the iterative method presented in Section~\ref{sec:ml} to estimate the
first  samples of the impulse response. The noise variance is updated
iteratively with~\eqref{eq:noisecov}. The iterations are initialized at
, , . The noise variances
 and  are initialized, respectively, at the sample
variance
of the least squares residuals and at . The iterations
are stopped when the relative change of the parameter updates is below 1\%.

We evaluate the goodness of fit using the standard score

where  is a true value and  its estimate. We calculate
the median fit of the estimated impulse responses, inputs, and outputs over the
dataset.

We consider two different scenarios. In the first scenario, we corrupt the
dataset with increasing fractions of missing samples. In the second scenario, we
corrupt the dataset with input noises of increasing variance.
Table~\ref{tab:summary} gives a summary of the experimental conditions.

\begin{table}[b]
  \centering
  \begin{tabular}{ccccc}
    \toprule
    &  &  & missing input  & missing output\\
    \midrule
    A (Exp. 1) & 10\% & 10\% &   & \\
    A (Exp. 2) & 10\% & 10\% &  &  \\
    A (Exp. 3) & 10\% & 10\% &  & \\
    B & 10\% &  &  & \\
    \bottomrule
  \end{tabular}
  \caption{Experimental conditions in the simulation scenarios}\label{tab:summary}
\end{table}

\subsection*{Scenario A\@: Missing data}
The input noise is Gaussian white noise with variance 0.1 (10\% of the input
signal variance). Before performing
the identification, we randomly select and remove a fraction of the available
data: in Exp. 1, we remove from 0\% to 50\% of the input samples, in 10\%
increments; in Exp. 2, we remove from 0\% to 50\% of the output samples,
in 10\% increments; in Exp. 3, we remove equal fractions of input and output,
between 0\% and 25\%, in 5\% increments.  The results are plotted in
Figure~\ref{fig:scenario_a}. Interestingly, a large fraction of missing input
samples has severe effect on the performance, whereas a large fraction of
missing output samples has a milder effect on the identification performance. In
Exp. 1 and Exp. 2, the model has always resulted identifiable, whereas in Exp. 3
a number of systems were non-identifiable. The results are collected in
Table~\ref{tab:unsolve}.
\begin{figure}[H]
  \centering
  \includegraphics[width=\columnwidth]{main-figure1.pdf}
  \includegraphics[width=\columnwidth]{main-figure2.pdf}
  \includegraphics[width=\columnwidth]{main-figure3.pdf}
  \caption{Plot of the median fit of the impulse response (top), the smoothed
    input (middle) and the smoothed output (bottom) over 500 MC runs, for increasing
    fractions of missing samples; In Exp.1  we remove input samples, in
    Exp.2  we remove output samples, in Exp.3  we remove input and
    output samples.
}\label{fig:scenario_a}
\end{figure}

\begin{table}[b]
  \centering
  \begin{tabular}{ccccccc}
    \toprule
    Unsolvable problems&     0  &   3  &  5 &  16 &  27 &  40\\
    \midrule
    Total missing samples & 0\% & 10\% & 20\% & 30\% & 40\% & 50\%\\
    \bottomrule
  \end{tabular}
  \caption{Number of non-identifiable systems in Exp. 3 (out of 500)}\label{tab:unsolve}
\end{table}


\subsection*{Scenario B\@: Input noise}
The input noise is Gaussian white noise. We consider values of the input noise
variance between 0 (no noise) and 1 (same variance as the input), in increments
of 0.2. The results are plotted in Figure~\ref{fig:scenario_b}. In this
scenario, we compare the performance of the proposed method with a kernel-based
identification method that does not account for input noise. We estimate the
impulse response using the posterior mean  from~\eqref{eq:posterior_pars},
with ,  and  estimated trough marginal likelihood,
with all instances of  replaced by the noisy measurements .
\begin{figure}[htb]
  \centering
  \includegraphics[width=\columnwidth]{main-figure4.pdf}
  \caption{Plot of the median fit of the impulse response estimate over 500 MC
    runs, for increasing values of the input noise variance. We compare the
    proposed estimator (blue) with performance of an estimator that does not
  account for input noise (black).}\label{fig:scenario_b}
\end{figure}

\section{Conclusions}\label{sec:conclusions}
In this paper we have presented a nonparametric kernel-based method for the
identification of
the impulse response of a linear systems in the presence of noisy and missing input-output
data. The method relies upon a Gaussian regression framework,
where the impulse response of the system is modeled as a Gaussian process with
a suitable covariance matrix. Using an EB approach, we find the minimum
mean-squared estimate of the impulse response. This estimate depends on the
unknown noiseless input, as well as on the kernel hyperparameters and the noise
variances. These quantities are estimated from the marginal likelihood of the data, obtained
integrating out the impulse response. We have devised an iterative scheme that
solves the marginal likelihood maximization in simple updates, and we have discussed
the convergence properties of the algorithm. We have tested the method on a data
bank of linear systems, where we have analyzed the degradation in performance
for increasing amounts of missing data, and increasing noise variance on the input
measurements. We have briefly addressed the question of identifiability; and
simulations seem to validate our theoretical results, however, this aspect
needs further study.

\bibliographystyle{ieeetr}
\bibliography{main.bbl}

\end{document}
