Our experiments were designed to explore how the proposed model
compares to related approaches which are either not enhanced with
planning modules or non-incremental. We also investigated the sample
efficiency of these models and the quality of the predicted plans when
these are available. The majority of our results focus on automatic
evaluation metrics. We also follow previous work
\cite{wiseman-etal-2017-challenges,DBLP:journals/corr/abs-1809-00582,
  puduppully-etal-2019-data,Puduppully-2020} in eliciting judgments to
evaluate system output.

\subsection{Automatic Evaluation}


We evaluate model output using BLEU \cite{papineni-etal-2002-bleu}
with the gold summary as a reference. We also report model performance
against the Information Extraction (IE) metrics of
\citet{wiseman-etal-2017-challenges} which are defined based on the
output of an IE model which extracts entity (team and player names)
and value (numbers) pairs from the summary and predicts the type of
relation between them.



Let  be the gold summary and  be the model
output. \textsl{Relation Generation} (RG) measures the precision and
count of relations obtained from  that are found in the input
table. \textsl{Content Selection} (CS) measures the precision, recall,
and \mbox{F-measure} of relations extracted from  also found in
. And \textsl{Content Ordering} (CO) measures the complement
of the Damerau-Levenshtein distance between relations extracted from
 and . Higher values are better for RG Precision, CS
F-measure, CO, and BLEU. We reuse the IE model from
\citet{DBLP:journals/corr/abs-1809-00582} for \textsc{RotoWire},
\citet{Puduppully-2020} for MLB, and
\citet{hayashi-etal-2019-findings} for German \textsc{RotoWire}.  Our
computation of IE metrics for all systems includes duplicate records
\cite{Puduppully-2020}.


In addition to IE-based metrics, we report the number of errors made
by systems according to Number (incorrect number in digits, number
spelled in words, etc.), Name (incorrect names of teams, players, days
of week, etc.), and Word (errors in usage of words) following the
classification of \citet{thomson-reiter-2020-gold}. We detect such
errors automatically using the system of \citet{KasnerTextInContext}
which scored best against gold standard human annotations of the same
type \cite{thomson-reiter-2021-generation}.  We only report these
metrics for English \textsc{RotoWire}, since error annotations (for
automatic metric learning) are not available for other datasets.
Moreover, with regard to Word errors, we only report errors for
incorrect usage of the word \textsl{double-double}.\footnote{A
  double-double occurs when a player scores 10 points or more in two
  record types: points, rebounds, assists, steals, and blocked shots.}
We found such errors to be detected reliably in contrast to Word
errors as a whole for which the precision of the system of
\citet{KasnerTextInContext} is \textasciitilde 50\%.  Lower values are
better for the Number, Name, and double-double errors.  We note metrics
such as RG precision, Number, Name, and double-double errors \emph{directly}
compute the accuracy of the generation model. Metrics
such as CS, CO, and BLEU measure how similar model output is against a
reference summary. Thus, CS, CO and BLEU measure generation accuracy
\emph{indirectly} under the assumption that gold summaries are
accurate.

\begin{table}[t]
\footnotesize
\centering
\begin{tabular}{@{}l@{~}|@{~}c@{~~~}c@{~}|c@{~~~}c@{~~~}c@{~}|@{~}c@{~}|@{~}c@{}} 
\multicolumn{7}{c}{} \\ \thickhline
 \multirow{2}{*}{MLB} &\multicolumn{2}{c|}{RG} &\multicolumn{3}{c@{~}|@{~}}{CS} & CO & \multirow{2}{*}{BLEU}\\

 &\# & P\% & P\% & R\% & F\% & DLD\% & \\ \thickhline
Templ & 62.3 & 99.9 & 21.6 & 55.2 &  31.0 &  11.0 & 4.12 \\ \hline \hline 
EDCC & \textbf{32.5} & 91.3 & 27.8 & 40.6 & 33.0 &   17.1 & 9.68 \\
NCPCC& 19.6 & 81.3 & \textbf{44.5} & 44.1 & 44.3 &  21.9 & 9.68 \\
ENT&23.8 & 81.1 & 40.9 & 49.5 &44.8  &20.7 & 11.50 \\ 
Macro & \dotuline{30.8} & \dotuline{94.4} & 40.8 & \textbf{54.9} & \dotuline{46.8} & \dotuline{21.8} & \dotuline{12.62} \\
~~~Bin & 31.2 & 93.7 & 38.3 & 52.4 & 44.2 & 21.6 & 12.32 \\ \hline \hline 
SeqPlan & 28.9 & \textbf{95.9} & \dotuline{43.3} & \dotuline{53.5} & \textbf{47.8} & \textbf{22.7} & \textbf{14.29} \\
~~~w Uniform & 18.5 & 90.9 & 36.5 & 30.6 & 33.3 & 14.5 & 10.30 \\ 
~~~w Oracle & 27.6 & {95.9} & 42.5 & 50.4 & 46.1 & 22.0 & 13.13 \\
~~~2-Stage& 28.6 & {95.9} & 41.4 & 50.8 & 45.6 & 21.3 & 13.96 \\ \thickhline
\end{tabular}
\vspace*{-.2ex}
\caption{MLB results (test set); relation
  generation (RG) count (\#) and precision (P\%), content selection
  (CS) precision (P\%), recall (R\%), and F-measure (F\%), content
  ordering (CO) as complement of normalized Damerau-Levenshtein
  distance (DLD\%), and BLEU. \textbf{Highest} and \dotuline{second highest} generation models
  are highlighted.} 
\label{tbl:results-with-ie-test-mlb}
\end{table}


\paragraph{MLB Dataset}
Table~\ref{tbl:results-with-ie-test-mlb} summarizes our results on
MLB.  Our sequential planning model (SeqPlan) has the highest RG~P among
neural models and performs best in terms of CS~F, CO, and BLEU.  The
variant of Macro with length control (Bin) performs comparably or
worse than Macro.

To examine the importance of latent sequential planning, we also
present a variant of our model which uniformly samples a plan from the
pool~ instead of Equation~\eqref{eq:p_plan} (see row
w(ith) Uniform in Table~\ref{tbl:results-with-ie-test-mlb}). This
version obtains lower values compared to SeqPlan across all metrics
underscoring the importance of sequential planning.  We also present
two variants of SeqPlan (a)~one which makes use of oracle (instead of
predicted) plans during training to generate~; essentially, it
replaces  with  in Equation~\eqref{eq:generation} (row
w(ith) Oracle in Table~\ref{tbl:results-with-ie-test-mlb}) and (b)~a
two stage model which trains the planner (Equation~\eqref{eq:planner})
and generator (Equation~\eqref{eq:generation}) separately (row 2-stage
in Table~\ref{tbl:results-with-ie-test-mlb}); in this case, we use
greedy decoding to sample  from Equation~\eqref{eq:planner}
instead of Gumbel-Softmax and replace~ with~ in
Equation~\eqref{eq:generation}. Both variants are comparable to
SeqPlan in terms of RG~P but worse in terms of CS~F, CO, and BLEU.

Furthermore, we evaluate the accuracy of the inferred plans by
comparing them against oracle plans, using the CS and CO metrics
(computed over the entities and events in the plan) \footnote{To
  compute the accuracy of macro plans, entities and events from the
  model's plan need to be compared against entities and events in the
  oracle macro plan. \citet{Puduppully-2020} obtained the entities and
  events for the oracle macro plan by extracting these from reference
  summaries.  We noted that this includes coreferent or repeat
  mentions of entities and events within a paragraph.  We instead
  extract entities and events directly from the oracle macro plan.}.
Table~\ref{tbl:inf-plans-cs-co-rw} shows that SeqPlan achieves higher
CS~F and CO scores than Macro.  Again, this indicates planning is
beneficial, particularly when taking the table and the generated
summary into account.


\begin{table}[t]
\footnotesize
\centering
\begin{tabular}{@{}l@{~}|@{~}c@{~~~}c@{~}|c@{~~~}c@{~~~}c@{~}|@{~}c@{~}|@{~}c@{}} 
\multicolumn{7}{c}{} \\ \thickhline
\multirow{2}{*}{RW} &\multicolumn{2}{c|}{RG} &\multicolumn{3}{c@{~}|@{~}}{CS} & CO & \multirow{2}{*}{BLEU}\\ 
 &\# & P\% & P\% & R\% & F\% & DLD\% & \\ \thickhline

Templ & 54.3 & 99.9 &27.1 &{57.7} & 36.9 &13.1 &8.46  \\ \hline \hline
WS-2017 & 34.1 & 75.1 & 20.3 & 36.3  &26.1 & 12.4 & 14.19 \\
EDCC & 35.9 & 82.6 &19.8 & 33.8 & 24.9 & 12.0 & 14.99 \\ NCPCC &{40.8} & {87.6} & 28.0 & {51.1} & 36.2
 &15.8 & \textbf{16.50} \\ 
ENT&32.7 & \dotuline{91.7} & \textbf{34.7} & 48.5 & 40.5 & 16.6 & 16.12 \\
RBF-2020 & 44.9 & 89.5 & 23.9 & 47.0 & 31.7 & 14.3 & 17.16 \\
Macro & 42.1 & \textbf{97.6} & 34.1 & 57.8 &\textbf{42.9} & \textbf{17.7} & 15.46 \\ 
~~~Bin & \textbf{61.0} & 97.2 & 26.8 & \textbf{66.1} & 38.2 & 15.8 & \textbf{16.48} \\ \hline \hline
SeqPlan & \dotuline{46.7} & \textbf{97.6} & \dotuline{30.6} & \dotuline{57.4} & \dotuline{39.9} & \dotuline{16.7} & \dotuline{16.26} \\
~~~w Uniform & 22.0 & 80.2 & 18.2 & 19.6 & 18.9 & 6.0 & 8.61 \\
~~~w Oracle  & 50.4 & 97.2 & 29.0 & 59.1 & 38.9 & 16.8 & 16.32 \\
~~~2-stage & 53.4 & 97.5 & 28.5 & 61.3 & 38.9 & 16.1 & 16.61 \\
\thickhline
\multicolumn{7}{c}{} \\ \thickhline
\multirow{2}{*}{DE-RW} &\multicolumn{2}{c|}{RG} &\multicolumn{3}{c@{~}|@{~}}{CS} & CO & \multirow{2}{*}{BLEU}\\

 &\# & P\% & P\% & R\% & F\% & DLD\% & \\ \thickhline
Templ & 54.4 & 99.9 & 17.2 & 63.0 &  27.1 &  11.6 & 7.32 \\ \hline \hline
EDCC& \dotuline{24.8} & {59.3} & 6.7 & 18.8 & 9.9 & 6.8 & 5.09 \\
NCPCC& 17.7 & 52.5 & 11.3 & \dotuline{25.7} & 15.7 & 9.6 & \dotuline{7.29} \\
ENT& 17.4 & \dotuline{64.7} & \dotuline{13.3} & 24.0 & \dotuline{17.1} & \dotuline{9.8} & 6.52 \\
RBF-2020 & 0.2 & 4.0 & 1.1 & 0.4 & 0.6 & 0.3 &2.29 \\
Macro & \textbf{30.2} & 49.7 & 5.1 & 21.0 & 8.3 & 6.1 & 5.15 \\~~~Bin & 20.4 & 55.0 & 7.9 & 20.0 & 11.3 & 8.1 & 6.18 \\
\hline\hline     
SeqPlan & 13.8 & \textbf{91.8} & \textbf{38.0} & \textbf{38.4} & \textbf{38.2} & \textbf{21.2} & \textbf{8.65} \\
\thickhline

\end{tabular}
\vspace*{-.2ex}
\caption{Evaluation on
  \textsc{RotoWire} (RW) and German \textsc{RotoWire} (DE-RW) test sets; relation
  generation (RG) count (\#) and precision (P\%), content selection
  (CS) precision (P\%), recall (R\%), and F-measure (F\%), content
  ordering (CO) as complement of normalized Damerau-Levenshtein distance (DLD\%),
  and BLEU.  \textbf{Highest} and \dotuline{second highest} generation models
  are highlighted.}
\label{tbl:results-with-ie-test} 
\end{table}

\paragraph{English and German \textsc{RotoWire}}
Results on \textsc{RotoWire} are presented in
Table~\ref{tbl:results-with-ie-test} (top). In addition to Templ,
EDCC, NCPCC, and ENT, we compare with the models of
\citet{wiseman-etal-2017-challenges} (WS-2017) and
\citet{rebuffel2020hierarchical} (RBF-2020). WS-2017 is the best
performing model of \citet{wiseman-etal-2017-challenges}. Note that
EDCC is an improved re-implementation of WS-2017. RBF-2020
represents the current state-of-the-art on \textsc{RotoWire}, and
comprises of a Transformer encoder-decoder architecture
\cite{NIPS2017_7181} with hierarchical attention on entities and their
records. The models of \citet{saleh-etal-2019-naver},
\citet{iso-etal-2019-learning}, and \citet{gong-etal-2019-table} are
not comparable as they make use of information additional to the table
such as previous/next games or the author of the game summary. The
model of \citet{narayan-etal-2020-stepwise} is also not comparable as
it relies on a pretrained language model
\cite{rothe-etal-2020-leveraging} to generate the summary sentences.

Table~\ref{tbl:results-with-ie-test} (bottom) shows our results on
German \textsc{RotoWire}. We compare against NCPCC's entry in the
WNGT 2019 shared task\footnote{We thank Hiroaki Hayashi for providing
  us with the output of the NCPCC system.}
\cite{hayashi-etal-2019-findings}, and our implementation of Templ, EDCC, ENT, Macro
and RBF-2020.
\citet{saleh-etal-2019-naver} are not comparable as they pretrain on
32M parallel and 420M monolingual data. Likewise,
\citet{puduppully-etal-2019-university} make use of a jointly trained
multilingual model by combining \textsc{RotoWire} with German
\textsc{RotoWire}.


We find that SeqPlan achieves highest RG~P amongst neural models, and
performs on par with Macro (it obtains higher BLEU but lower CS~F and
CO scores).  The Bin variant of Macro performs better on BLEU but
worse on other metrics.  As in
Table~\ref{tbl:results-with-ie-test-mlb}, w~Uniform struggles across
metrics corroborating our hypothesis that latent sequential planning
improves generation performance.  The other two variants (w~Oracle and
2-Stage) are worse than SeqPlan in RG~P and CS~F, comparable in
CO, and slightly higher in terms of BLEU.

On German, our model is best across metrics achieving an RG~P
of~91.8\% which is higher by 42\% (absolute) compared to of Macro. In
fact, the RG~P of SeqPlan is superior to
\citet{saleh-etal-2019-naver} whose model is pretrained with
additional data and is considered state of the art
\cite{hayashi-etal-2019-findings}.  RG\# is lower mainly because of a
bug in the German IE which excludes number records. RG\# for NCPCC
and Macro is too high because the summaries contain a lot of
repetition.  The same record will repeat at least once with NCPCC
and three times with Macro, whereas only~7\% of the records are
repeated with SeqPlan. 

Table~\ref{tbl:inf-plans-cs-co-rw} evaluates the quality of the plans
inferred by our model on the \textsc{RotoWire} dataset. As can be
seen, SeqPlan is slightly worse than Macro in terms of CS~F and CO. We
believe this is because summaries in \textsc{RotoWire} are somewhat
formulaic, with a plan similar to Templ: an opening statement is
followed by a description of the top scoring players, and a conclusion
describing the next match. Such plans can be learnt well by Macro
without access to the summary. MLB texts show a lot more diversity in
terms of length, and the sequencing of entities and events. The
learning problem is also more challenging, supported by the fact that
the template system does not do very well in this domain (i.e., it is
worse in BLEU, CS~F, and CO compared to \textsc{RotoWire}).  In German
\textsc{RotoWire}, SeqPlan plans achieve higher CS~F and CO than
Macro.


\begin{table}[t]
\footnotesize
\centering
\begin{tabular}{ll|lcc|c} 
\multicolumn{5}{c}{} \\ \thickhline
 \multicolumn{2}{c}{}  &\multicolumn{3}{c}{CS} & CO \\ 
 \multicolumn{2}{c|}{\raisebox{.4ex}[0pt]{Datasets}} &  P\% & R\% & F\% & DLD\% \\ \thickhline
&Macro & 73.6 & 45.9 & 56.5 & 27.0 \\
\raisebox{-0.5ex}[0pt]{\begin{sideways}MLB~~\end{sideways}} &SeqPlan & 74.4 & 51.1 & 60.6 & 27.1 \\\hline\hline
&Macro & 81.5 & 62.7 & 70.9 & 36.3 \\  \raisebox{.1ex}[0pt]{\begin{sideways}RW\end{sideways}}&SeqPlan & 79.1
& 61.6 & 69.3& 35.5 \\\hline \hline
 & & & & & \\
&\raisebox{2ex}[0pt]{Macro} & \raisebox{2ex}[0pt]{86.8} & \raisebox{2ex}[0pt]{34.2} & \raisebox{2ex}[0pt]{49.0} & \raisebox{2ex}[0pt]{30.1} \\
\raisebox{.02ex}[0pt]{\begin{sideways}DE-RW\end{sideways}} &\raisebox{1.5ex}[0pt]{SeqPlan} & \raisebox{1.5ex}[0pt]{73.1} & \raisebox{1.5ex}[0pt]{60.8} & \raisebox{1.5ex}[0pt]{66.4} & \raisebox{1.5ex}[0pt]{31.0} \\
\thickhline 
\end{tabular}
\caption{Evaluation of macro planning stage (test set); 
content selection (CS) precision (P\%), recall (R\%), and F-measure
(F\%), content ordering (CO) as  complement of normalized
Damerau-Levenshtein distance (DLD\%).}
\label{tbl:inf-plans-cs-co-rw}
\end{table}




\begin{table}
\centering
\footnotesize
\begin{tabular}{ l|c|c|c} 
 \thickhline
 & Number & Name & double-double\\
 \thickhline
 Templ & \hspace*{1ex}0.08*&3.05* & 0.00*\\ \hline \hline
WS-2017 & 13.01*& 9.66* & 0.36*\\
 EDCC & \hspace*{1ex}8.11*& 8.29* & 0.31*\\ 
 NCPCC&\hspace*{1ex}7.89* & 7.76* & \hspace*{-1ex}0.14\\
 ENT & \hspace*{1ex}5.89* &7.24* & \hspace*{-1ex}0.15\\
RBF-2020 &\hspace*{1ex}6.20* &8.39*& 0.41*\\
Macro & 2.57 &4.60*  & \hspace*{-1ex}0.18\\
SeqPlan & 2.70& \hspace*{-1ex}6.56 & \hspace*{-1ex}0.20\\
 \thickhline
\end{tabular}
\caption{Number, Name, and double-double (Word) errors per
  example. Systems significantly different from   SeqPlan are marked
  with an asterisk~* (using a one-way ANOVA with
  posthoc Tukey HSD tests; \mbox{}).}
\label{tab:fact-eval}
\end{table}



Table~\ref{tab:fact-eval} reports complementary automatic metrics on
English \textsc{RotoWire} aiming to assess the factuality of generated
output.  We find that Templ has the least Number, Name, and
double-double errors. This is expected as it simply reproduces facts
from the table.  SeqPlan and Macro have similar Number errors, and
both are significantly better than other neural models. SeqPlan has
significantly more Name errors than Macro, and significantly fewer
than other neural models.  Inspection of Name errors revealed that
these are mostly due to incorrect information about next games. Such
information is not part of the input and models are prone to
hallucinate.  SeqPlan fares worse as it attempts to discuss next games
for both teams while Macro focuses on one team only. In terms of
double-double errors, SeqPlan is comparable to Macro, ENT and
NCPCC, and significantly better than WS-2017, EDCC, and
RBF-2020.


\begin{figure}[t]
\begin{tabular}{@{\hspace*{-.25cm}}c@{}c@{}}
 \includegraphics[width=0.26\textwidth]{figures/sample-efficiency-mlb} &
\includegraphics[width=0.26\textwidth]{figures/sample-efficiency}\\
\small{(a)} & \small{(b)}\\
\end{tabular}
\caption{Sample efficiency for (a)~MLB and (b)~\textsc{RotoWire}
  datasets. SeqPlan and Macro are trained on different portions (\%) of the
  training dataset and performance is measured with RG P\%.}
\label{fig:sample-efficiency-mlb}
\end{figure}


\subsection{Sample Efficiency}

We also evaluated whether SeqPlan is more sample efficient in
comparison to Macro, by examining how RG~P varies with (training)
data size. As shown in Figure~\ref{fig:sample-efficiency-mlb}, the
difference between SeqPlan and Macro is more pronounced when
relatively little data is available. For example, with 10\% of
training data, RG~P for SeqPlan on MLB is~85.7\% and 92.1\% on
\textsc{RotoWire}.  In contrast, Macro obtains~57.5\% on MLB and
47.1\% on \textsc{RotoWire}.  As more training data becomes available,
the difference in RG~P decreases.  The slope of increase in RG~P
for Macro is higher for \textsc{RotoWire} than MLB. We hypothesize
this is because MLB has longer summaries with more paragraphs, and is
thus more difficult for Macro to learn alignments between paragraph
plans and text paragraphs in the game summary.




