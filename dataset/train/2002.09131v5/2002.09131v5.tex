\section{Experimental Details}
\label{app:exp}

In this section, we provide the detailed setup of all experiments (datasets, model architectures, learning strategies and evaluation metrics) for both video prediction and early activity recognition.

\subsection{Preprocessing Module}
\label{app-sub:preprocessing}

In the main paper, we use a sliding window to group consecutive states in the proprocessing module (Section 3). In the Discussion (Section 5), we argued that other possible approaches are less effective in preserving spatio-temporal structure compared to our {\em sliding window approach}. Here, we discuss an alternative approach that was previously proposed for non-convolutional higher-order RNN~\citep{yu2017long}, which we name as {\em fixed window approach}. We will compare these two approaches in terms of computational complexity, ability to preserve temporal structure and predictive performance.

\textbf{Fixed window approach.} 
With fixed window approach,  previous steps  are first concatenated into a single tensor, which is then repeatedly mapped to  inputs  to the  module.

\textbf{Fixed Window (FW): \quad}
\tensorSup{\tilde{H}}{i} 
& = 
\tensorSup{P}{i} \ast \left[\tensorSup{H}{t-1}; \cdots; \tensorSup{H}{t-N} \right] \\
\textbf{Sliding Window (SW): \quad}
\tensorSup{\tilde{H}}{i} 
& =
\tensorSup{P}{i} \ast \left[\tensorSup{H}{t-i}; \cdots; \tensorSup{H}{t-i+N-M} \right]

For comparison, we list both equations for fixed window approach and sliding window approach. These two approaches are also illustrated in \autoref{fig:preprocessing}.

\textbf{Drawbacks of fixed window approach.}
{\bf(a)} The {fixed window approach} has a larger window size than the {sliding window approach}, thus requires more parameters in the preprocessing kernels and higher computational complexity.
{\bf(b)} More importantly, the fixed window approach does not preserve the chronological order of the preprocessed states; unlike sliding window approach, the index  for  in fixed window approach cannot reflect the time for the compressed states. Actually, all preprocessed states  are equivalent, which violates the property (2) in designing our convolutional tensor-train module (Section 3.1).
{\bf(c)} In \autoref{tab:eval-ablation-strategy}, we compare these two approaches on Moving-MNIST-2 under the same experimental setting, and we find that the sliding window approach performs slightly better than fixed window.
For all aforementioned reasons, we choose {sliding window approach} in our implementation of the preprocessing module.

\begin{figure*}[!htbp]
  \centering
    \begin{subfigure}[b]{0.48\textwidth}
  \centering 
      \includegraphics[trim={0cm 0cm 0cm 0cm},clip, height=5cm]{figs/sliding.pdf}
      \caption{Sliding window approach (final implementation)}
      \label{subfig:sliding-window}
  \end{subfigure}
  \begin{subfigure}[b]{0.48\textwidth}
  \centering
       \includegraphics[trim={0cm 0cm 0cm 0.0cm},clip,height=5cm]{figs/fixed.pdf}
      \caption{Fixed window approach (alternative)}
      \label{subfig:fixed-window}
  \end{subfigure}
  \caption{{\bf Variations of proprocessing modules.}}
   \label{fig:preprocessing}
\end{figure*}


\subsection{Model Architectures}
\label{app-sub:architecture}

\textbf{Video prediction.}
All experiments use a stack of 12-layers of {\ConvLSTM} or {\ConvTTLSTM} with 32 channels for the first and last 3 layers, and 48 channels for the 6 layers in the middle.
A convolutional layer is applied on top of all recurrent layers to compute the predicted frames, followed by an extra sigmoid layer for KTH action dataset.
Following~\citet{byeon2018contextvp}, two skip connections performing concatenation over channels are added between (3, 9) and (6, 12) layers.
An illustration of the network architecture is included in \autoref{subfig:conv-tt-lstm-vp}.
All convolutional kernels were initialized by Xavier's normalized initializer~\citep{glorot2010understanding} and initial hidden/cell states in {\ConvLSTM} or {\ConvTTLSTM} were initialized as zeros.

\textbf{Early activity recognition.}
Following~\citep{wang2018eidetic},
the network architecture consists of four modules: a 2D-CNN encoder, a video prediction network, a 2D-CNN decoder and a 3D-CNN classifier, as illustrated in \autoref{subfig:conv-tt-lstm-vc}. {\bf(1)} The 2D-CNN encoder has two -strided 2D-convolutional layers with  channels, which reduce the resolution from  to , and {\bf(2)} the 2D-CNN decoder contains two -strided transposed 2D-convolutional layers with  channels, which restore the resolution from  to . {\bf(3)} The video prediction network is miniature version of \autoref{subfig:conv-tt-lstm-vp}, where the number of layers in each block is reduced to . In the experiments, we evaluate three realizations of each layer: {\ConvLSTM}, {\ConvTTLSTM} or causal 3D-convolutional layer.
{\bf(4)} The 3D-CNN classifier takes the last  frames from the input, and predicts a label for the  categories. The classifier contains two -strided 3D-convolutional layers with  channels, each of which is followed by a 3D-pooling layer.
These layers reduce the resolution from  to , and the output feature is fed into a two-layers perceptron with  hidden units to compute the label.

\begin{figure*}[!htbp]
  \centering
  \begin{subfigure}{0.4\textwidth}
  \centering
      \includegraphics[trim={0cm 0cm 0cm 0.0cm},clip,height=5cm]{figs/prediction-architecture.pdf}
      \caption{{\bf Prediction model}}
      \label{subfig:conv-tt-lstm-vp}
  \end{subfigure}
  \begin{subfigure}{0.40\textwidth}
  \centering 
     \vspace{0.3cm}
      \includegraphics[trim={0cm 0cm 0cm 0cm},clip, height=4.5cm]{figs/recognition-architecture.pdf}
      \vspace{0.2cm}
      \caption{{\bf Recognition model}}
      \label{subfig:conv-tt-lstm-vc}
  \end{subfigure}
  \caption{{\bf Network architecture for video prediction and early activity recognition tasks.}}
  \label{fig:arch}
\end{figure*}

\subsection{Hyper-parameters selection.}
\label{app-sub:hyperparameters}

\autoref{tab:hyperparameter} summarizes our search values for different hyper-parameters for {\ConvTTLSTM}.
{\bf(1)} For filter size , we found models with larger filter size  consistently outperform the ones with .
{\bf(2)} For learning rate, we found that our models are unstable at a high learning rate such as , but learn poorly at a low learning rate . Consequently, we use gradient clipping with learning rate , with clipping value  for all experiments. {\bf(3)} While the performance typically increases as the order grows, the model suffers gradient instability in training with a high order, e.g.\ . Therefore, we choose the order  for all {\ConvTTLSTM} models. {\bf(4)(5)} For small ranks  and steps , the performance increases monotonically with  and . But the performance stays on plateau when we further increase them, therefore we settle down at  and  for all experiments.

\begin{table*}[!htbp]
  \centering
  \begin{tabular}{c c c c c}
    \toprule
     \footnotesize Filter size  &  \footnotesize Learning rate & \footnotesize Order of {\CTTD}  & \footnotesize Ranks of {\CTTD}  & \footnotesize Time steps  \\
    \midrule
    \{3, 5\} & \{, , \} & \{\} & \{\} & \{\} \\
    \bottomrule
  \end{tabular}
   \caption{Hyper-parameters search values for Conv-TT-LSTM experiments.}
  \label{tab:hyperparameter}
\end{table*}

Similarly, \autoref{tab:hyperparameter-compression} summarize the hyper-parameters search for tensor-train compression of ConvLSTM~\citep{ garipov2016ultimate}. {\bf(1)} Since the best {\ConvLSTM} baseline has filter size , we only consider  in the compression experiments. {\bf(2)} We observe that the compressed {\ConvLSTM} models consistently achieve better performance with learning rate 
. {\bf(3)(4)} The compressed {\ConvLSTM}s are robust to different order and ranks, and  wins by a small margin.

\begin{table*}[!htbp]
  \centering
  \begin{tabular}{c c c c}
    \toprule
     \footnotesize Filter size  &  \footnotesize Learning rate & \footnotesize Order of {\TTD}  & \footnotesize Ranks of {\TTD} \\
    \midrule
    5 & \{, \} & \{\} & \{\} \\
    \bottomrule
  \end{tabular}
   \caption{Hyper-parameters search values for Tensor-Train compression of {\ConvLSTM}.}
  \label{tab:hyperparameter-compression}
\end{table*}


\subsection{Datasets}
\label{app-sub:dataset}

\textbf{Moving-MNIST-2 dataset.}
The Moving-MNIST-2 dataset is generated by moving two digits of size  in MNIST dataset within a  black canvas. These digits are placed at a random initial location, and move with constant velocity in the canvas and bounce when they reach the boundary.
Following~\citet{wang2018predrnnpp}, we generate 10,000 videos for training, 3,000 for validation, and 5,000 for test with default parameters in the generator\tablefootnote{The Python code for Moving-MNIST-2 generator is publicly available online in \cite{mnist-source}.}.

Similarly, we summarize the search values 
\textbf{KTH action dataset.}
The KTH action dataset~\cite{laptev2004recognizing} contains videos of 25 individuals performing 6 types of actions on a simple background.
Our experimental setup follows~\citet{wang2018predrnnpp}, which uses persons 1-16 for training and 17-25 for testing, and each frame is resized to  pixels.
All our models are trained to predict 10 frames given 10 input frames. During training, we randomly select 20 contiguous frames from the training videos as a sample and group every 10,000 samples into one epoch to apply the learning strategy as explained at the beginning of this section.

\textbf{Something-Something V2 dataset.}
The Something-Something V2 dataset~\citep{goyal2017something} is a benchmark for activity recognition, which can be download online\footnote{\url{https://20bn.com/datasets/something-something}}. Following~\citet{wang2018eidetic}, we use the official subset with  categories that contains  training videos and  test videos. The video length ranges between  and  seconds with  frames per second (fps). We reserve  of the training videos for validation, and use the remaining  for optimizing the models.


\subsection{Evaluation Metrics}
\label{app-sub:metric}

We use two traditional metrics MSE (or PSNR) and SSIM~\citep{wang2004image}, and a recently proposed deep-learning based metric LPIPS~\citep{zhang2018unreasonable}, which measures the similarity between deep features.
Since MSE (or PSNR) is based on pixel-wise difference, it favors vague and blurry predictions, which is not a proper measurement of perceptual similarity.
While SSIM was originally proposed to address the problem, \citet{zhang2018unreasonable} shows that their proposed LPIPS metric aligns better to human perception. 


\subsection{Ablation Studies}
\label{app-sub:ablation}

Here, we show that our proposed {\ConvTTLSTM} consistently improves the performance of {\ConvLSTM}, regardless of the architecture, loss function and learning schedule used.
Specifically, we perform three ablation studies on our experimental setting, by \textbf{(1)} Reducing the number of layers from 12 layers to 4 layers (same as~\cite{xingjian2015convolutional} and~\cite{wang2018predrnnpp}); \textbf{(2)} Changing the loss function from  to  only; and \textbf{(3)} Disabling the scheduled sampling and use teacher forcing during training process.
We compare the performance of our proposed {\ConvTTLSTM} against the {\ConvLSTM} baseline in these ablated settings, \autoref{tab:eval-ablation-strategy}.
The results show that our proposed {\ConvTTLSTM} consistently outperforms {\ConvLSTM} in all settings, i.e.\ the {\ConvTTLSTM} model improves upon {\ConvLSTM} in a board range of setups, which is not limited to the certain setting used in our paper. These ablation studies further show that our setup is optimal for predictive learning in Moving-MNIST-2 dataset.

\begin{table*}[htbp]
\centering
\begingroup
\setlength{\tabcolsep}{5pt}
\renewcommand{\arraystretch}{1} \begin{tabular}{c |c| c c c c c c | c c c | c}
    \toprule
    \multicolumn{2}{c|}{\multirow{2}{*}{Model}} & \multicolumn{2}{c}{Layers} & \multicolumn{2}{c}{Sched.} & \multicolumn{2}{c|}{Loss} & \multicolumn{3}{c|}{(10 -> 30)} & \multirow{2}{*}{Params.} \\    
    \multicolumn{2}{c|}{} & 4 & 12 & TF & SS &  &  & MSE & SSIM & LPIPS & \\
    \midrule
    \midrule
    ConvLSTM & - & \multirow{2}{*}{\ding{51}} & \multirow{2}{*}{\ding{53}} & \multirow{2}{*}{\ding{53}} & \multirow{2}{*}{\ding{51}} & \multirow{2}{*}{\ding{53}} & \multirow{2}{*}{\ding{51}} & 37.19 & 0.791 & 184.2 & 11.48M \\
    Conv-TT-LSTM & FW & & & & & & & {\bf 31.46} & {\bf 0.819} & {\bf 112.5} &  \textbf{5.65M} \\
    \midrule
    ConvLSTM & - & \multirow{2}{*}{\ding{53}} & \multirow{2}{*}{\ding{51}} & \multirow{2}{*}{\ding{51}} & \multirow{2}{*}{\ding{53}} & \multirow{2}{*}{\ding{53}} & \multirow{2}{*}{\ding{51}} & 
    33.96 & 0.805 & 184.4 & 3.97M \\
    Conv-TT-LSTM & FW & & & & & & & 
    {\bf 30.27} & {\bf 0.827} & {\bf 118.2} & \textbf{2.65M}  \\
    \midrule
    ConvLSTM & - & \multirow{2}{*}{\ding{53}} & \multirow{2}{*}{\ding{51}} & \multirow{2}{*}{\ding{53}} & \multirow{2}{*}{\ding{51}} & \multirow{2}{*}{\ding{51}} & \multirow{2}{*}{\ding{53}} & 
    36.95 & 0.802 & 135.1 & 3.97M \\
    Conv-TT-LSTM & FW & & & & & & & 
    {\bf 34.84} & {\bf 0.807} & {\bf 128.4} &  \textbf{2.65M} \\
    \midrule
    \midrule
    ConvLSTM & - & \multirow{2}{*}{\ding{53}} & \multirow{2}{*}{\ding{51}} & \multirow{2}{*}{\ding{53}} & \multirow{2}{*}{\ding{51}} & \multirow{2}{*}{\ding{53}} & \multirow{2}{*}{\ding{51}} & 
    33.08 & 0.806 & 140.1 & 3.97M \\
    Conv-TT-LSTM & FW & & & & & & &
    {\bf 28.88} & {\bf 0.831} & {\bf 104.1} & \textbf{2.65M} \\
    \midrule
    Conv-TT-LSTM & SW & \ding{53} & \ding{51} & \ding{53} & \ding{51} & \ding{53} & \ding{51} & 25.81 & \textbf{0.840} & \textbf{90.38} & 2.69M \\
    \bottomrule
  \end{tabular}
  \endgroup
  \caption{Evaluation of {\ConvLSTM} and our {\ConvTTLSTM} under ablated settings. In this table, FW stands for {\em fixed window approach}, {SW} stands for {\em sliding window approach}; For learning scheduling, TF denotes {\em teaching forcing} and SS denotes {\em scheduled sampling}. The experiments show that \textbf{(1)} our {\ConvTTLSTM} is able to improve upon {\ConvLSTM} under all settings; \textbf{(2)} Our current learning approach is optimal in the search space; \textbf{(3)} The sliding window approach outperforms the fixed window one under the optimal experimental setting.}
  \label{tab:eval-ablation-strategy}
\end{table*}












%
