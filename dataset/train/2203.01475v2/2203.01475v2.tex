\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage[rebuttal]{cvpr}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{subcaption}



\usepackage[pagebackref,breaklinks,colorlinks,bookmarks=false]{hyperref}

\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}






\def\cvprPaperID{6790} \def\confName{CVPR}
\def\confYear{2022}

\begin{document}

\title{Rebuttal for "CycleMix: A Holistic Strategy for Medical Image Segmentation from\\Scribble Supervision"}  

\maketitle
\thispagestyle{empty}
\appendix

We appreciate comments from reviewers: novel idea (R1,R2,R3), clear and well written (R1,R2), 'inspire future research in this direction' (R1), 'effective and could generalize to other problems' (R2), 'experimental validation is exhaustive' (R3). Here are responses to specific questions.
\hfill
\\

\noindent{\bf\textcolor[RGB]{244,96,108}{R1}, It's unclear whether it extends to natural images.} Thanks. Natural image segmentation is a really meaningful application. Our model is designed for medical images, but could be generalized to natural images directly. We would like to verify it in the future work.
\hfill
\\

\noindent{\bf\bf\textcolor[RGB]{38,188,213}{R2}, Annotators would likely prefer a fast preprocessing approach whereby scribbles are used to generate full annotations, which are vetted or lightly retouched rather than used blindly as in this paper.} 
Thanks. Indeed fast preprocessing, such as interactive learning, is an interesting research direction. However, it is out of the scope of this paper. This research is aimed to train a segmentation network with limited scribble supervision, which is challenging [24,28]. The proposed method achieved comparable or even better performance than the fully-supervised methods.  Therefore, once trained, our model could generate accurate full annotations, and reduce the workload of the annotators.
\hfill
\\

\newline\noindent{\bf\textcolor[RGB]{251,178,23}{R3}, Why the mixture segmentation strategy works.}
Thanks. 
We were motivated by the assumption that a segmentation model should benefit from finer gradient flow via larger portion of annotated pixels.
Therefore, we performed mix augmentation to maximize scribbles of mixed image, which increased the amount of annotated pixels.
To provide regularization for unlabeled pixels, we applied cycle consistency to penalize inconsistent segmentation, which resulted in significant improvement of performance. 
We will further clarify in the revision.
\hfill
\\

\newline\noindent{\bf\textcolor[RGB]{251,178,23}{R3}, Why is the performance of the method by Baran Can worse?}
Thanks. As described in Sec 4.2, we cited the results of the method by Baran Can (TS-UNet) directly from [28], where it was observed that errors reinforced themselves in self-learning schemes, and inaccurate proposals in the relabeled training set led the retrained model to fit to errors.
The segmentation results of TS-UNet in [28] are listed in Table~\ref{tab:tab1}.
To eliminate misunderstandings, we will revise the comparison and clarify in the revision.
\begin{table}[!h]
  \centering
  \caption{Segmentation results of TS-UNet reported in [28].}
  \begin{tabular}{@{}lcccc@{}}
    \toprule
    Metric & LV & MYO & RV & Avg \\
    \midrule
    Dice & 0.479& 0.408&0.272& 0.386\\
    HD & 115.6&111.9&133.9& 120.5\\
    \bottomrule
  \end{tabular}
  \label{tab:tab1}
\end{table}
\hfill
\\
\newline\noindent{\bf\textcolor[RGB]{251,178,23}{R3}, How is the transportation matrix  obtained?  Is it specified by PuzzleMix?}
Yes. We obtain the transportation matrix  by maximizing the salience of mixed image in Sec 3.1.1. 
The optimization method is detailed in Puzzle mix [16], which adopts an alternating scheme, \textit{i.e.}, it first iterates over mask  and then updates  and .
We will further clarify this in the revision.
\hfill
\\

\newline\noindent{\bf\textcolor[RGB]{251,178,23}{R3}, The stability of the training? How does the loss converge?}
Thanks. 
The training process of our model is stable, and the losses converge quickly within 200 training epochs as \cref{fig:short} shows.
\cref{fig:short}\textcolor{red}{(a)} plots the curves of weighted losses at training stage for 250 epochs,
including the loss of unmixed samples (), the loss of mixed samples (), the global consistency loss (), and the local consistency loss ().
\cref{fig:short}\textcolor{red}{(b)} to \cref{fig:short}\textcolor{red}{(e)} display the individual curves of unweighted losses.
At the beginning, with the supervised loss  and  decreasing, the segmentation model was optimized, and , which penalizes disconnected local structures, also declined.
At this time the model should only learn the knowledge from the labeled pixels, and the segmentation of unlabeled pixels should be still inaccurate.
This led to the increase of global consistency loss .
After 50 epochs, while  kept been minimized,  started decreasing, indicating that the model had learned from the unlabeled pixels. 
\begin{figure}[t!]
  \centering
  \begin{subfigure}[b]{0.95\linewidth}
         \includegraphics[width=0.9\linewidth]{sec/figs/loss curve.png}
         \caption{The curves of weighted loss functions.}
    \label{fig:short-a}
  \end{subfigure}
  \begin{subfigure}[b]{0.23\linewidth}
         \includegraphics[width=\linewidth]{sec/figs/curve1.png}
         \caption{}
    \label{fig:short-b}
  \end{subfigure}
    \begin{subfigure}[b]{0.21\linewidth}
         \includegraphics[width=\linewidth]{sec/figs/curve2.png}
         \caption{}
    \label{fig:short-c}
  \end{subfigure}
    \begin{subfigure}[b]{0.21\linewidth}
         \includegraphics[width=\linewidth]{sec/figs/curve3.png}
         \caption{}
    \label{fig:short-d}
  \end{subfigure}
    \begin{subfigure}[b]{0.21\linewidth}
         \includegraphics[width=\linewidth]{sec/figs/curve4.png}
         \caption{}
    \label{fig:short-e}
  \end{subfigure}
  \caption{Loss curves of CycleMix}
  \label{fig:short}
\end{figure}



{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
