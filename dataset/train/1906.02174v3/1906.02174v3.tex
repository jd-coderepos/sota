\documentclass{article}
\usepackage[final, nonatbib]{neurips_2019}
\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage[caption=false,font=footnotesize]{subfig}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{epstopdf}
\usepackage{pifont}\usepackage{pxfonts}
\usepackage{amsmath,amssymb,bm}
\usepackage{footnote}
\usepackage{enumerate}
\usepackage{physics}
\usepackage{enumitem}
\usepackage[english]{babel}
\usepackage{comment}
\usepackage{setspace}
\usepackage{algorithm}
\usepackage{algpseudocode}
\newcommand{\INDSTATE}[1][1]{\STATE\hspace{#1\algorithmicindent}}
\usepackage{forloop}
\usepackage{chngpage}
\usepackage{color, colortbl}
\definecolor{darkgreen}{rgb}{0, 0.5, 0}
\definecolor{red}{rgb}{1, 0, 0}
\definecolor{purple}{rgb}{0.5, 0, 0.5}
\usepackage{booktabs, multirow}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator{\range}{range}
\DeclareMathOperator{\spn}{span}

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\X}{\mathbb{X}}
\newcommand\etal{\textit{et al.}}
\newcommand\ie{\textit{i.e.}}
\newcommand\eg{\textit{e.g.}}
\newcommand\st{\textit{s.t.}}
\newcommand\wrt{\textit{w.r.t.}}
\newcommand\etc{\textit{etc.}}
\newcommand\doubleE{\mathbb{E}}
\newcommand\doubleP{\mathbb{P}}
\newcommand\doubleR{\mathbb{R}}
\newcommand\scriptS{\mathcal{S}}
\newcommand\scriptO{\mathcal{O}}
\newcommand\scriptA{\mathcal{A}}
\newcommand\scriptX{\mathcal{X}}
\newcommand{\colvec}[2][1]{\scalebox{#1}{\renewcommand{\arraystretch}{1.45}}
}
\makeatletter
\newcommand*{\rom}[1]{\expandafter\@slowromancap\romannumeral #1@}
\makeatother

\newenvironment{theorem}[2][Theorem]{\begin{trivlist}
		\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{lemma}[2][Lemma]{\begin{trivlist}
		\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{exercise}[2][Exercise]{\begin{trivlist}
		\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{reflection}[2][Reflection]{\begin{trivlist}
		\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{proposition}[2][Proposition]{\begin{trivlist}
		\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{corollary}[2][Corollary]{\begin{trivlist}
		\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{problem}[2][Problem]{\begin{trivlist}
		\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{proof}{{\noindent\it Proof}\quad}{\hfill \par}

\def\coef_vec{
	\begin{bmatrix}
		\frac{g^{(0) } (0)}{0!} \6pt]
		\frac{g^{(2) } (0)}{2!}\6pt]
		\frac{g^{(\infty) } (0)}{\infty!}
\end{bmatrix}}

\makeatletter
\newsavebox{\@brx}
\newcommand{\llangle}[1][]{\savebox{\@brx}{}\mathopen{\copy\@brx\kern-0.5\wd\@brx\usebox{\@brx}}}
\newcommand{\rrangle}[1][]{\savebox{\@brx}{}\mathclose{\copy\@brx\kern-0.5\wd\@brx\usebox{\@brx}}}
\makeatother

\newtheorem{definition}{Definition}

\newcommand{\inprod}[2]{{\llangle #1 ,\;}{#2\rrangle}}

\title{Break the Ceiling: Stronger Multi-scale Deep Graph Convolutional Networks}








\author{
Sitao Luan, Mingde Zhao, Xiao-Wen Chang, Doina Precup\\
\{sitao.luan@mail, mingde.zhao@mail, chang@cs, dprecup@cs\}.mcgill.ca\\
McGill University; Mila; DeepMind\\
Equal Contribution
}






\begin{document}
\maketitle	

\begin{abstract}
Recently, neural network based approaches have achieved significant performance improvement for solving large, complex, graph-structured problems. However, the advantages of multi-scale information and deep architectures have not been sufficiently exploited. In this paper, we analyze how existing Graph Convolutional Networks (GCNs) have limited expressive power due to the constraint of the activation functions and their architectures. We generalize spectral graph convolution and deep GCN in block Krylov subspace forms and devise two architectures, both with the potential to be scaled deeper but each making use of the multi-scale information differently. On several node classification tasks, with or without validation set, the two proposed architectures achieve state-of-the-art performance.
\end{abstract}
	
\section{Introduction and Motivation}\label{sec:introduction}
Many real-world problems can be modeled as graphs \cite{hamilton2017inductive, kipf2016classification, liao2019lanczos, gilmer2017neural, monti2017geometric, defferrard2016fast}. Among the recent focus of applying machine learning algorithms on these problems, graph convolution in Graph Convolutional Networks (GCNs) stands out as one of the most powerful tools and the key operation, which was inspired by the success of Convolutional Neural Networks (CNNs) \cite{lecun1998gradient} in computer vision \cite{li2018adaptive}. In this paper, we focus on spectrum-free Graph Convolutional Networks (GCNs) \cite{bronstein2016geometric, shuman2012emerging}, which have obtained state-of-the-art performance on multiple transductive and inductive learning tasks \cite{defferrard2016fast, kipf2016classification, liao2019lanczos, chen2018fastgcn, chen2017stochastic}.
\par
One big challenge for the existing GCNs is the limited expressive power of their shallow learning mechanisms \cite{zhang2018graph, wu2019survey}. The difficulty of extending GCNs to richer architectures leads to several possible explanations and even some opinions that express the unnecessities of addressing such a problem:
\begin{enumerate}[leftmargin=12pt]
\item Graph convolution can be considered as a special form of Laplacian smoothing \cite{li2018deeper}. A network with multiple convolutional layers will suffer from an over-smoothing problem that makes the representation of the nodes indistinguishable even for the nodes that are far from each other \cite{zhang2018graph}. \item For many cases, it is not necessary for the label information to totally traverse the entire graph. Moreover, one can operate on the multi-scale coarsening of the input graph and obtain the same flow of information as GCNs with more layers \cite{bronstein2016geometric}.
\end{enumerate}
\par
Nevertheless, shallow learning mechanisms violate the compositionality principle of deep learning \cite{lecun2015deep, hinton2006fast} and restrict label propagation \cite{sun2019stage}. In this paper, we first give analyses of the lack of scalability of the existing GCN. Then we show that any graph convolution with a well-defined analytic spectral filter can be written as a product of a block Krylov matrix and a learnable parameter matrix in a special form. Based on the analyses, we propose two GCN architectures that leverage multi-scale information with differently and are scalable to deeper and richer structures, with the expectation of having stronger expressive powers and abilities to extract richer representations of graph-structured data. We also show that the equivalence of the two architectures can be achieved under certain conditions. For validation, we test the proposed architectures on multiple transductive tasks using their different instances. The results show that even the simplest instantiation of the proposed architectures yields state-of-the-art performance and the complex ones achieve surprisingly higher performance, both with or without the validation set.

\section{Preliminaries}
\label{sec:preliminaries}
We use bold font for vectors , block vectors  and matrix blocks  as in \cite{frommer2017block}. Suppose we have an undirected graph , where  is the node set with ,  is the edge set with , and  is a symmetric adjacency matrix. Let  denote the diagonal degree matrix, \ie{} . A diffusion process on  can be defined by a diffusion operator  \cite{coifman2006diffusion, coifman2006diffusionmaps} which is a symmetric positive semi-definite matrix, \eg{} graph Laplacian , normalized graph Laplacian  and affinity matrix , \etc{} We use  to denote a general diffusion operator in this paper. The eigendecomposition of  gives us , where  is a diagonal matrix whose diagonal elements are eigenvalues and the columns of  are the orthonormal eigenvectors and named graph Fourier basis. We also have a feature matrix (graph signals, can be regarded as a block vector)  defined on  and each node  has a feature vector , which is the  row of .

Graph convolution is defined in graph Fourier domain \st{} , where  and  is the Hadamard product \cite{defferrard2016fast}. Following from this definition, a graph signal  filtered by  can be written as

where  can be any function which is analytic inside a closed contour which encircles , \eg{} Chebyshev polynomial \cite{defferrard2016fast}. GCN generalizes this definition to signals with  input channels and  output channels and the network structure is

where  and . This is called spectrum-free method \cite{bronstein2016geometric} that requires no explicit computation of eigendecomposition \cite{zhang2018graph} and operations on the frequency domain. We will focus on the analysis of GCN in the following sections.





\section{Why GCN is not Scalable?}
Suppose we scale GCN to a deeper architecture in the same way as \cite{kipf2016classification, li2018deeper}, it becomes

For this, we have the following theorems.
\begin{theorem} 1 \label{thm1}
Suppose that  has  connected components and the diffusion operator  is defined as that in \eqref{eq0}. Let  be any block vector sampled from space  according to a continuous distribution and  be any set of parameter matrices, if  has no bipartite components, then in \eqref{eq1}, as ,  almost surely.
\end{theorem}	
\begin{proof}
See Appendix.
\end{proof}

\begin{theorem} 2
\label{thm2}
Suppose we randomly sample  under a continuous distribution and point-wise function , we have

\end{theorem}

\begin{proof}
See Appendix.
\end{proof}

Theorem 1 shows that if we simply increase the depth based on GCN architecture, the extracted features  will at most encode stationary information of graph structure and lose all the information in node features. In addition, from the proof we see that the point-wise ReLU transformation is a conspirator. Theorem 2 tells us that Tanh is better in keeping linear independence among column features. We design a numerical experiment on synthetic data (see Appendix) to test, under a 100-layer GCN architecture, how activation functions affect the rank of the output in each hidden layer during the feed-forward process. As Figure 1(a) shows, the rank of hidden features decreases rapidly with ReLU, while having little fluctuation under Tanh, and even the identity function performs better than ReLU (see Appendix for more comparisons). So we propose to replace ReLU by Tanh.\begin{figure*}[htbp]
\centering
\subfloat[Deep GCN]{
\captionsetup{justification = centering}
\includegraphics[width=0.32\textwidth]{{fig_GCN_compare}.pdf}}
\hfill
\subfloat[Snowball]{
\captionsetup{justification = centering}
\includegraphics[width=0.32\textwidth]{{fig_snowball_compare}.pdf}}
\hfill
\subfloat[Truncated Block Krylov]{
\captionsetup{justification = centering}
\includegraphics[width=0.32\textwidth]{{fig_truncated_krylov_compare}.pdf}}
\caption{Number of independent column features }
\label{activation_functions}
\end{figure*}
\section{Spectral Graph Convolution and Block Krylov Subspace Methods}
\subsection{Notation and Backgrounds}
Let  be a block vector subspace of  containing the identity matrix  that is closed under matrix multiplication and transposition. We define an inner product  in the block vector space  as follows \cite{frommer2017block},
\begin{definition}
A mapping  from  to  is called a block inner product onto  if it satisfies the following conditions for all  and :
\begin{enumerate}[leftmargin=12pt]
\item -linearity:   and 
\item symmetry: 
\item definiteness:  is positive definite if  has full rank, and  if and only if 
\end{enumerate}
\end{definition}


There are mainly three ways to define , we use the classical one:   and . We define a block vector subspace of , which will be used later.
\begin{definition}
Given a set of block vectors , the -span of  is defined as 
\end{definition}
Given the above definition, the order- block Krylov subspace with respect to , and   can be defined as . The corresponding block Krylov matrix is defined as .

\subsection{Spectral Graph Convolution in Block Krylov Subspace Form}
\label{conv_in_krylov}
In this section, we will show that any graph convolution with well-defined analytic spectral filter defined on  can be written as the product of a block Krylov matrix with a learnable parameter matrix in a specific form.

For any real analytic scalar function , its power series expansion around center 0 is

where  is the radius of convergence. We can define a filter by .
Let  denote the spectrum radius of  and suppose . The spectral filter  can be defined as

According to the definition of spectral graph convolution in \eqref{def}, graph signal  is filtered by  in the following way,

where  and . It is easy to see that   is a block Krylov matrix and Range()  Range().  We know that there exists a smallest  such that \cite{gutknecht2009block,frommer2017block}

\ie{} for any , .  depends on  and , so we will write it as  later, yet here we still use  for simplicity. From \eqref{krylov}, the convolution can be written as

where  are parameter matrix blocks and  under classical definition of inner product. Then
where . The essential  number of learnable parameters is . \subsection{Deep GCN in Block Krylov Subspace Form}
\label{deep_gcn_krylov}
Since the spectral graph convolution can be simplified as \eqref{eq5}\eqref{eq6}, we can build deep GCN in the following way.
	
Suppose we have a sequence of analytic spectral filters  and a sequence of point-wise nonlinear activation functions . Then,

Let us define  and . Then . From \eqref{eq6}\eqref{eq7}, we have an iterative relation that  where . It is easy to see that, when , \eqref{eq7} is fully connected network \cite{li2018deeper}; when , it is just GCN \cite{kipf2016classification}; when  is defined by Chebyshev polynomial \cite{hammond2011wavelets},  and under the global inner product, \eqref{eq7} is ChebNet \cite{defferrard2016fast}.

\subsection{Difficulties in Computation}
\label{difficulty}
In the last subsection, we gave a general form of deep GCN in block Krylov form. Following this idea, we can leverage the existing block Arnoldi (Lanczos) algorithm \cite{frommer2017block, frommer2017radau} to compute orthogonal basis of  and find . But there are some difficulties in practice:
\begin{enumerate}[leftmargin=12pt]
\item During the training phase,  changes every time that parameters are updated. This makes  become a variable and thus requires adaptive size for parameter matrices.
\item For classical inner product, the  factorization that is needed in block Arnoldi algorithm \cite{frommer2017block} is difficult to be put into backpropagation framework.
\end{enumerate}

Although direct implementation of block Krylov methods in GCN is hard, it inspires us that if we have a good way to stack multi-scale information in each hidden layer, the network will have the ability to be extended to deep architectures. We propose a way to alleviate the difficulties in section \ref{deep}.
\section{Deep GCN Architectures}
\label{deep}
Upon the analyses in the last section, we propose two architectures: \textit{snowball} and \textit{truncated Krylov}. These methods concatenate multi-scale feature information in hidden layers differently while both having the potential to be scaled to deeper architectures.
\subsection{Snowball}
\label{snowball_gcn}

In order to concatenate multi-scale features together and get a richer representation for each node, we design a densely connected graph network (Figure \ref{deep_gcn}(a)) as follows,

where  are learnable parameter matrices,  is the number of output channels in layer ;  are point-wise activation functions;  is a classifier of any kind; .  are extracted features.  can be a fully connected neural network or even an identity layer with . When ,  and when , , which means that we project  back onto graph Fourier basis which is necessary when graph structure encodes much information. Following this construction, we can stack all learned features as the input of the subsequent hidden layer, which is an efficient way to concatenate multi-scale information. The size of input will grow like a snowball and this construction is similar to DenseNet \cite{huang2017densely}, which is designed for regular grids (images). Thus, some advantages of DenseNet are naturally inherited, \eg{} alleviate the vanishing-gradient problem, encourage feature reuse, increase the variation of input for each hidden layer, reduce the number of parameters, strengthen feature propagation and improve model compactness.

\subsection{Truncated Krylov}

As stated in Section \ref{difficulty}, the fact that  is a variable makes GCN difficult to be merged into the block Krylov framework. But we can make a compromise and set  as a hyperparameter. Then we can get a truncated block Krylov network (Figure \ref{deep_gcn}(b)) as shown below,

where  are learnable parameter matrices,  and  are activation functions, and .

There are many works on the analysis of error bounds of doing truncation in block Krylov methods \cite{frommer2017block}. But the results need many assumptions either on , \eg{}  is a standard Gaussian matrix \cite{ wang2015improved}, or on  , \eg{} some conditions on the smallest and largest eigenvalues of   have to be satisfied \cite{musco2018stability}. Instead of doing truncation for a specific function or a fixed , we are dealing with variable  during training. So we cannot put any restriction on  and its relation to  to get a practical error bound.

\begin{figure*}[htbp]
\centering
\subfloat[Snowball]{
\captionsetup{justification = centering}
\includegraphics[width=0.49\textwidth]{{fig_snowball}.pdf}}
\hfill
\subfloat[Truncated Block Krylov]{
\captionsetup{justification = centering}
\includegraphics[width=0.49\textwidth]{{fig_truncated_krylov}.pdf}}
\caption{Deep GCN Architectures}
\label{deep_gcn}
\end{figure*}

Here we would like to mention \cite{liao2019lanczos}, which proposes to do low-rank approximation of  by the Lanczos algorithm. The problem with this technique is that information in  will be lost if  is actually not low-rank. If we increase the Lanczos step to keep more information, it will hurt the efficiency of the algorithm. Since most of the graphs we are dealing with have sparse connectivity structures, they are actually not low-rank, \eg{} Erd\H{o}s-R\'enyi graph  with  \cite{tran2013sparse} and Appendix \rom{4}.
Thus we did not do low-rank approximation in our computations.


\subsection{Equivalence of Linear Snowball GCN and Truncated Block Krylov Network}
\label{linear_snowball}


Under the snowball GCN architecture, the identity function outperforms ReLU as shown in Figure \ref{activation_functions} and it is easier to train than Tanh. In this part, we will show that a multi-layer linear snowball GCN with identity function as , identity layer as  and  is equivalent to a 1-layer block Krylov network with identity layer ,  and a special parameter matrix.

We write  as  and follow \eqref{snowball} we have

As in \eqref{snowball}, we have . Thus we can write
\iffalse

\fi
6pt]
 0 & \bm{I} & \cdots & 0 \6pt]
 0 & 0 & \cdots  & \bm{W_0} \\
\end{bmatrix}
\begin{bmatrix}
\bm{I} & 0 & \cdots & 0 \0pt]
 \vdots & \vdots & \ddots  & \vdots\6pt]
 0 & \bm{W_{n-1}^{n}} & \cdots & 0 \6pt]
 0 & 0 & \cdots &  \bm{W_{n-1}^1} \\
\end{bmatrix}
\label{eq2}
\underset{m \rightarrow \infty }{\text{lim}}\; (\frac{1}{\lambda_{max}} L )^m \bm{x} = [\bm{v_1},\dots, \bm{v_k}] \bm{\theta},
\doubleP(\text{rank}\left(\text{ReLU}([\bm{x}, \bm{y}])\right) \leq \text{rank}([\bm{x}, \bm{y}]) \;|\; \bm{x}, \bm{y} \in \mathbb{R}^N) = 1
&\doubleP\left(\text{rank}\left(\text{ReLU}([\bm{x}, \bm{y}])\right) \leq \text{rank}([\bm{x}, \bm{y}]) \;|\;\bm{x}, \bm{y} \in \mathbb{R}^N\right) = \frac{\doubleP\left(\text{rank}\left(\text{ReLU}([\bm{x}, \bm{y}])\right) \leq \text{rank}([\bm{x}, \bm{y}]) ,\bm{x}, \bm{y} \in \mathbb{R}^N\right)}{\doubleP\left(\bm{x}, \bm{y} \in \mathbb{R}^N\right)}\\
& = \doubleP\left(\text{rank}\left(\text{ReLU}([\bm{x}, \bm{y}])\right) \leq \text{rank}([\bm{x}, \bm{y}]) \;|\; \text{rank}([\bm{x}, \bm{y}]) = 1,\bm{x}, \bm{y} \in \mathbb{R}^N \right) \doubleP\left(\text{rank}([\bm{x}, \bm{y}]) = 1,\bm{x}, \bm{y} \in \mathbb{R}^N \right) +\\
&\doubleP\left(\text{rank}\left(\text{ReLU}([\bm{x}, \bm{y}])\right) \leq \text{rank}([\bm{x}, \bm{y}]) \;|\; \text{rank}([\bm{x}, \bm{y}]) = 2,\bm{x}, \bm{y} \in \mathbb{R}^N \right) \doubleP\left(\text{rank}([\bm{x}, \bm{y}]) = 2,\bm{x}, \bm{y} \in \mathbb{R}^N \right)\\
& = 0 \times \frac{1}{2} + 1 \times 1 = 1.
\doubleP(\bm{x_+} = d \bm{y_+}, \bm{x_-} \neq d \bm{y_-} \;|\; x,y \in \mathbb{R}^N, d \neq 0) = \frac{1}{2^N} \frac{1+N}{2^N}
    &\doubleP(\bm{x_+} = d \bm{y_+}, \bm{x_-} \neq d \bm{y_-}) \\
    & = P (\bm{x_+} = d \bm{y_+}, \bm{x_-} \neq d \bm{y_-} \;|\;  df(\bm{x_+}) \leq 1 ) \; P(df(\bm{x_+}) \leq 1) + P (\bm{x_+} = d \bm{y_+}, \bm{x_-} \neq d \bm{y_-} \;|\;  df(\bm{x_+}) > 1 ) \; P (df(\bm{x_+}) > 1)\\
    &=\frac{1}{2^N} \frac{1+N}{2^N} + 0 \cdot \frac{2^N -1 -N}{2^N} =\frac{1}{2^N} \frac{1+N}{2^N}
 \label{eq3}
\underset{n \rightarrow \infty }{\text{lim}} \; \text{rank}(\bm{Y'}) \leq \underset{n \rightarrow \infty }{\text{lim}}\; \text{rank}(L^{n+1} \bm{X}) = \text{rank} ([\bm{v_1},\dots, \bm{v_k}] [\bm{\theta_1},\dots, \bm{\theta_F}]) \leq \text{rank}([\bm{v_1},\dots, \bm{v_k}]) = k,
\doubleP(\text{rank}\left(\text{Tanh}([\bm{x},\bm{y}])\right) \geq \text{rank}([\bm{x},\bm{y}]) \;|\; \bm{x},\bm{y} \in \mathbb{R}^N) = 1 \frac{\text{Tanh}(x_1)}{\text{Tanh}(y_1)} = \frac{\text{Tanh}(x_2)}{\text{Tanh}(y_2)}, \text{ \st{} } \frac{x_1}{y_1} = \frac{x_2}{y_2} x_1 = \frac{1033977}{9530},\; x_2 = -\frac{929}{10} ,\; y_1 = -\frac{1113}{10}, \; y_2 = \frac{953}{10}\frac{\text{Tanh}(y_1)}{\text{Tanh}(y_2)} = \frac{x_1'}{x_2'} \doubleP(\text{rank}\left(\text{Tanh}([\bm{x},\bm{y}])\right) \geq \text{rank}([\bm{x},\bm{y}]) \;|\; \bm{x},\bm{y} \in \mathbb{R}^N) = 1  \frac{\text{Tanh}(x_1)}{\text{Tanh}(y_1)} = \frac{\text{Tanh}(x_2)}{\text{Tanh}(y_2)} = \cdots = \frac{\text{Tanh}(x_N)}{\text{Tanh}(y_N)}, \text{ \st{} } \frac{x_1}{y_1} = \frac{x_2}{y_2} \cdots = \frac{x_N}{y_N} 
The solution of any pair of equations covers an area of probability 0 in . Actually, this still holds when  have some 0 elements, \ie{} for any subset of the above equations, the area of the solution is still 0.

If  are linearly independent, suppose  is a vector in , and the space in  that can be transformed by point-wise  to the same line as  covers an area of probability 0. Therefore, Lemma 2 still holds in .
\end{proof}

\subsection*{Appendix \rom{2}: Numerical Experiments on Synthetic Data}
\label{appendix:2}
The goal of the experiments is to test which network structure with which kind of activation function has the potential to be extended to deep architecture. We measure this potential by the numerical rank of the output features in each hidden layer of the networks using synthetic data. The reason of choosing this measure can be explained by Theorem \ref{thm1}. We build the certain networks with depth 100 and the data is generated as follows.

We first randomly generate edges of an Erd\H{o}s-R\'enyi graph , \ie{} the existence of the edge between any pair of nodes is a Bernoulli random variable with .  Then, we construct the corresponding adjacency matrix  of the graph which is a  matrix. We generate a  feature matrix  and each of its element is drawn from . We normalize  and  as \cite{kipf2016classification} and abuse the notation  to denote the normalized matrices. We keep 3 blocks in each layer of truncated block Krylov network. The number of input channel in each layer depends on the network structures and the number of output channel is set to be 128 for all networks. Each element in every parameter matrix  is randomly sampled from  and the size is . With the synthetic , we simulate the feedforward process according to the network architecture and collect the numerical rank (at most 128) of the output in each of the 100 hidden layers. For each activation function under each network architecture, we repeat the experiments for 20 times and plot the mean results with standard deviation bars.
\subsection*{Appendix \rom{3}: Rank Comparison of Activation Functions and Networks}
\label{appendix:3}
\begin{figure*}[htbp]
\centering
\subfloat[GCN]{
\captionsetup{justification = centering}
\includegraphics[width=0.32\textwidth]{{fig_GCN_compare_all}.pdf}}
\hfill
\subfloat[Snowball]{
\captionsetup{justification = centering}
\includegraphics[width=0.32\textwidth]{{fig_snowball_compare_all}.pdf}}
\hfill
\subfloat[Truncated Block Krylov]{
\captionsetup{justification = centering}
\includegraphics[width=0.32\textwidth]{{fig_truncated_krylov_compare_all}.pdf}}
\caption{Column ranks of different activation functions with the same architecture}
\end{figure*}

\begin{figure*}[htbp]
\centering
\subfloat[ReLU]{
\captionsetup{justification = centering}
\includegraphics[width=0.32\textwidth]{{fig_relu_compare}.pdf}}
\hfill
\subfloat[Identity]{
\captionsetup{justification = centering}
\includegraphics[width=0.32\textwidth]{{fig_identity_compare}.pdf}}
\hfill
\subfloat[Tanh]{
\captionsetup{justification = centering}
\includegraphics[width=0.32\textwidth]{{fig_tanh_compare}.pdf}}
\caption{Column ranks of different architectures with the same activation function}
\end{figure*}

\subsection*{Appendix \rom{4}: Spectrum of the Datasets}
\label{appendix:4}
\label{spectrum}
\begin{figure*}[bhtp]
\centering
\subfloat[Cora]{
\captionsetup{justification = centering}
\includegraphics[width=0.32\textwidth]{{fig_spectrum_cora}.pdf}}
\hfill
\subfloat[Citeseer]{
\captionsetup{justification = centering}
\includegraphics[width=0.32\textwidth]{{fig_spectrum_citeseer}.pdf}}
\hfill
\subfloat[PubMed]{
\captionsetup{justification = centering}
\includegraphics[width=0.32\textwidth]{{fig_spectrum_pubmed}.pdf}}
\caption{Spectrum of the renormalized adjacency matrices for several datasets}
\label{spectrum_dataset}
\end{figure*}



\subsection*{Appendix \rom{5}: Experiment Settings and Hyperparameters}
\label{appendix:5}
The so-called public splits in \cite{liao2019lanczos} and the setting that randomly sample 20 instances for each class as labeled data in \cite{yang2016revisiting} is actually the same. Most of the results for the algorithms with validation are cited from \cite{liao2019lanczos}, where they are reproduced with validation. However, some of them actually do not use validation in original papers and can achieve better results. In the paper, We compare with their best results.

We use NVIDIA apex amp mixed-precision plugin for PyTorch to accelerate our experiments. Most of the results were obtained from NVIDIA V100 clusters on Beluga of Compute-Canada, with minor part of them obtained from NVIDIA K20, K80 clusters on Helios Compute-Canada. The hyperparameters are searched using Bayesian optimization.

A useful tip is the smaller your training set is, the larger dropout probability should be set and the larger early stopping you should have.

Table \ref{tab:hyperparameters_without_validation} and Table \ref{tab:hyperparameters_with_validation} shows the hyperparameters to achieve the performance in the experiments, for cases without and with validation, respectively. When conducting the hyperparameter search, we encounter memory problems: current GPUs cannot afford deeper and wider structures. But we do observe better performance with the increment of the network size. It is expected to achieve better performance with more advanced deep learning devices.

\begin{table}[htbp]
\setlength{\tabcolsep}{1.5pt}
  \centering
  \caption{Hyperparameters for Tests with Validation}
  \scriptsize
    \begin{tabular}{ccccc|cccccc}
    \toprule
    \toprule
    \multirow{2}[1]{*}{Architecture} & \multirow{2}[1]{*}{Dataset} & \multirow{2}[1]{*}{Percentage} & \multicolumn{2}{c|}{Accuracy} & \multicolumn{6}{c}{Corresponding Hyperparameters} \\
          &       &       & Ours  & SOTA  & lr    & weight\_decay & hidden & layers/n\_blocks & dropout & optimizer \\
    \midrule
    \multirow{11}[0]{*}{\textbf{linear Snowball}} & \multirow{4}[0]{*}{\textbf{Cora}} & 0.5\% & 69.99 & 60.8  & 1.0689E-03 & 1.4759E-02 & 128   & 6     & 0.66987 & RMSprop \\
          &       & 1\%   & 73.10 & 67.5  & 1.4795E-03 & 2.3764E-02 & 128   & 9     & 0.64394 & RMSprop \\
          &       & 3\%   & 80.96 & 77.7  & 2.6847E-03 & 5.1442E-03 & 64    & 9     & 0.23648 & RMSprop \\
          &       & 5.2\% (public) & 83.19 & 83.0  & 1.6577E-04 & 1.8606E-02 & 1024  & 3     & 0.65277 & RMSprop \\
          & \multirow{3}[0]{*}{\textbf{CiteSeer}} & 0.5\% & 59.41 & 53.8  & 4.9284E-04 & 6.9420E-03 & 512   & 11    & 0.90071 & RMSprop \\
          &       & 1\%   & 65.85 & 63.3  & 3.2628E-03 & 1.6374E-02 & 512   & 3     & 0.97331 & RMSprop \\
          &       & 3.6\% (public) & 73.54 & 72.5  & 2.8218E-03 & 1.9812E-02 & 5000  & 1     & 0.98327 & Adam \\
          & \multirow{4}[0]{*}{\textbf{Pubmed}} & 0.03\% & 68.12 & 61.0  & 2.1124E-03 & 4.4161E-02 & 128   & 7     & 0.78683 & RMSprop \\
          &       & 0.05\% & 70.04 & 68.8  & 4.9982E-03 & 2.6460E-02 & 128   & 4     & 0.86788 & RMSprop \\
          &       & 0.1\% & 73.83 & 73.4  & 1.2462E-03 & 4.9303E-02 & 128   & 6     & 0.3299 & RMSprop \\
          &       & 0.3\% (public) & 79.23 & 79.0  & 2.4044E-03 & 2.3157E-02 & 4000  & 1     & 0.98842 & Adam \\
    \midrule
    \multirow{11}[1]{*}{\textbf{Snowball}} & \multirow{4}[0]{*}{\textbf{Cora}} & 0.5\% & 72.96 & 60.8  & 2.3228E-04 & 2.1310E-02 & 950   & 7     & 0.88945 & RMSprop \\
          &       & 1\%   & 76.76 & 67.5  & 1.5483E-04 & 1.3963E-02 & 250   & 15    & 0.55385 & RMSprop \\
          &       & 3\%   & 80.72 & 77.7  & 1.6772E-03 & 1.0725E-02 & 64    & 14    & 0.80611 & RMSprop \\
          &       & 5.2\% (public) & 83.60 & 83.0  & 1.2994E-05 & 9.4469E-03 & 5000  & 3     & 0.025052 & RMSprop \\
          & \multirow{3}[0]{*}{\textbf{CiteSeer}} & 0.5\% & 62.05 & 53.8  & 2.0055E-03 & 3.1340E-02 & 512   & 5     & 0.88866 & RMSprop \\
          &       & 1\%   & 64.23 & 63.3  & 1.8759E-03 & 9.3636E-03 & 128   & 7     & 0.77334 & RMSprop \\
          &       & 3.6\% (public) & 72.61 & 72.5  & 2.5527E-03 & 6.2812E-03 & 256   & 1     & 0.56755 & RMSprop \\
          & \multirow{4}[1]{*}{\textbf{Pubmed}} & 0.03\% & 70.78 & 61.0  & 1.1029E-03 & 1.8661E-02 & 100   & 15    & 0.83381 & RMSprop \\
          &       & 0.05\% & 73.23 & 68.8  & 3.7159E-03 & 2.2088E-02 & 400   & 9     & 0.9158 & RMSprop \\
          &       & 0.1\% & 76.52 & 73.4  & 4.9106E-03 & 3.0777E-02 & 100   & 15    & 0.79133 & RMSprop \\
          &       & 0.3\% (public) & 79.54 & 79.0  & 4.9867E-03 & 3.5816E-03 & 3550  & 1     & 0.98968 & Adam \\
    \midrule
    \multirow{11}[2]{*}{\textbf{truncated Krylov}} & \multirow{4}[1]{*}{\textbf{Cora}} & 0.5\% & 73.89 & 60.8  & 1.6552E-04 & 4.4330E-02 & 4950  & 27    & 0.97726 & Adam \\
          &       & 1\%   & 77.38 & 67.5  & 2.8845E-04 & 4.8469E-02 & 4950  & 30    & 0.93928 & Adam \\
          &       & 3\%   & 82.23 & 77.7  & 8.6406E-04 & 4.0126E-03 & 2950  & 16    & 0.98759 & Adam \\
          &       & 5.2\% (public) & 83.51 & 83.0  & 1.0922E-03 & 3.5966E-02 & 1950  & 10    & 0.98403 & Adam \\
          & \multirow{3}[0]{*}{\textbf{CiteSeer}} & 0.5\% & 63.65 & 53.8  & 2.8208E-03 & 4.3395E-02 & 1150  & 30    & 0.92821 & Adam \\
          &       & 1\%   & 68.36 & 63.3  & 3.9898E-03 & 3.8525E-03 & 100   & 27    & 0.71951 & Adam \\
          &       & 3.6\% (public) & 73.89 & 72.5  & 1.8292E-03 & 4.2295E-02 & 600   & 11    & 0.98865 & Adam \\
          & \multirow{4}[1]{*}{\textbf{Pubmed}} & 0.03\% & 71.11 & 61.0  & 3.6759E-03 & 1.2628E-02 & 512   & 8     & 0.95902 & RMSprop \\
          &       & 0.05\% & 72.86 & 68.8  & 4.0135E-03 & 4.8831E-02 & 4250  & 5     & 0.95911 & Adam \\
          &       & 0.1\% & 75.68 & 73.4  & 4.7562E-03 & 3.7134E-02 & 950   & 7     & 0.96569 & Adam \\
          &       & 0.3\% (public) & 79.88 & 79.0  & 3.9673E-04 & 2.2931E-02 & 1900  & 4     & 0.000127 & Adam \\
    \bottomrule
    \bottomrule
    \end{tabular}\label{tab:hyperparameters_with_validation}\end{table} 
\begin{table}[htbp]
\setlength{\tabcolsep}{1.5pt}
  \centering
  \caption{Hyperparameters for Tests without Validation}
  \scriptsize
    \begin{tabular}{ccccc|cccccc}
    \toprule
    \toprule
    \multirow{2}[2]{*}{Architecture} & \multirow{2}[2]{*}{Dataset} & \multirow{2}[2]{*}{Percentage} & \multicolumn{2}{c|}{Accuracy} & \multicolumn{6}{c}{Correspondong Hyperparameters} \\
          &       &       & Ours  & SOTA  & lr    & weight\_decay & hidden & layers/n\_blocks & dropout & Optimizer \\
    \midrule
    \multirow{16}[2]{*}{\textbf{linear Snowball}} & \multirow{6}[1]{*}{\textbf{Cora}} & 0.5\% & 69.53 & 61.5  & 4.4438E-05 & 1.7409E-02 & 550   & 12    & 0.007753 & Adam \\
          &       & 1\%   & 74.12 & 69.9  & 1.0826E-03 & 3.3462E-03 & 1250  & 3     & 0.50426 & Adam \\
          &       & 2\%   & 79.43 & 75.9  & 2.4594E-06 & 9.6734E-03 & 1650  & 12    & 0.34073 & Adam \\
          &       & 3\%   & 80.41 & 78.5  & 2.8597E-05 & 3.4732E-02 & 900   & 15    & 0.039034 & Adam \\
          &       & 4\%   & 81.3  & 80.4  & 3.6830E-05 & 1.5664E-02 & 3750  & 4     & 0.93797 & Adam \\
          &       & 5\%   & 82.19 & 81.7  & 5.8323E-06 & 8.5940E-03 & 2850  & 5     & 0.14701 & Adam \\
          & \multirow{6}[0]{*}{\textbf{CiteSeer}} & 0.5\% & 56.76 & 56.1  & 4.5629E-03 & 2.0106E-03 & 300   & 3     & 0.038225 & Adam \\
          &       & 1\%   & 65.44 & 62.1  & 3.5530E-05 & 4.9935E-02 & 600   & 6     & 0.03556 & Adam \\
          &       & 2\%   & 68.78 & 68.6  & 6.1176E-06 & 3.0101E-02 & 1950  & 3     & 0.040484 & Adam \\
          &       & 3\%   & 71    & 70.3  & 2.1956E-05 & 4.3569E-02 & 3350  & 3     & 0.30207 & Adam \\
          &       & 4\%   & 72.23 & 70.8  & 9.1952E-05 & 4.6407E-02 & 3350  & 2     & 0.018231 & Adam \\
          &       & 5\%   & 72.21 & 71.3  & 3.7173E-03 & 1.9605E-03 & 2950  & 1     & 0.96958 & Adam \\
          & \multirow{4}[1]{*}{\textbf{Pubmed}} & 0.03\% & 64.133 & 62.2  & 1.0724E-03 & 8.1097E-03 & 64    & 4     & 0.8022 & RMSProp \\
          &       & 0.05\% & 69.48 & 68.3  & 1.5936E-03 & 3.0236E-03 & 6     & 10    & 0.73067 & RMSProp \\
          &       & 0.1\% & 72.93 & 72.7  & 4.9733E-03 & 1.3744E-03 & 128   & 3     & 0.91214 & RMSProp \\
          &       & 0.3\% & 79.33 & 79.2  & 1.7998E-03 & 9.6753E-04 & 512   & 1     & 0.97483 & RMSProp \\
    \midrule
    \multirow{16}[2]{*}{\textbf{Snowball}} & \multirow{6}[1]{*}{\textbf{Cora}} & 0.5\% & 67.15 & 61.5  & 9.8649E-04 & 1.0305E-02 & 1600  & 3     & 0.92785 & Adam \\
          &       & 1\%   & 73.47 & 69.9  & 1.4228E-04 & 1.3472E-02 & 100   & 13    & 0.68601 & Adam \\
          &       & 2\%   & 78.54 & 75.9  & 5.7111E-06 & 1.5544E-02 & 600   & 13    & 0.022622 & Adam \\
          &       & 3\%   & 79.97 & 78.5  & 4.0278E-05 & 2.7287E-02 & 4350  & 5     & 0.57173 & Adam \\
          &       & 4\%   & 81.49 & 80.4  & 1.4152E-05 & 2.3359E-02 & 2500  & 13    & 0.018578 & Adam \\
          &       & 5\%   & 81.82 & 81.7  & 1.2621E-03 & 1.5323E-02 & 3550  & 2     & 0.87352 & Adam \\
          & \multirow{6}[0]{*}{\textbf{CiteSeer}} & 0.5\% & 56.39 & 56.1  & 2.6983E-03 & 2.5370E-02 & 300   & 6     & 0.82964 & Adam \\
          &       & 1\%   & 65.04 & 62.1  & 1.6982E-03 & 1.5473E-02 & 2150  & 2     & 0.98611 & Adam \\
          &       & 2\%   & 69.48 & 68.6  & 9.7299E-05 & 4.9675E-02 & 2150  & 3     & 0.71216 & Adam \\
          &       & 3\%   & 71.09 & 70.3  & 1.7839E-04 & 3.0874E-02 & 2150  & 2     & 0.16549 & Adam \\
          &       & 4\%   & 72.32 & 70.8  & 5.6575E-05 & 3.5949E-02 & 4800  & 2     & 0.012576 & Adam \\
          &       & 5\%   & 72.8  & 71.3  & 2.8643E-04 & 1.6399E-02 & 2000  & 2     & 0.37308 & Adam \\
          & \multirow{4}[1]{*}{\textbf{Pubmed}} & 0.03\% & 62.94 & 62.2  & 1.2700E-03 & 1.4159E-03 & 128   & 4     & 0.76848 & RMSProp \\
          &       & 0.05\% & 68.31 & 68.3  & 1.1224E-03 & 9.9166E-05 & 256   & 3     & 0.85496 & RMSProp \\
          &       & 0.1\% & 73.29 & 72.7  & 6.0506E-04 & 1.0303E-03 & 256   & 2     & 0.97988 & RMSProp \\
          &       & 0.3\% & 79.63 & 79.2  & 1.1416E-03 & 6.1543E-04 & 128   & 1     & 0.989 & RMSProp \\
    \midrule
    \multirow{16}[2]{*}{\textbf{truncated Krylov}} & \multirow{6}[1]{*}{\textbf{Cora}} & 0.5\% & 72.96 & 61.5  & 3.3276E-03 & 1.0496E-04 & 128   & 18    & 0.76012 & RMSProp \\
          &       & 1\%   & 75.52 & 69.9  & 7.4797E-04 & 9.1736E-03 & 2048  & 20    & 0.98941 & RMSProp \\
          &       & 2\%   & 80.31 & 75.9  & 1.7894E-04 & 1.1079E-02 & 4096  & 16    & 0.97091 & RMSProp \\
          &       & 3\%   & 81.54 & 78.5  & 4.3837E-04 & 2.6958E-03 & 512   & 17    & 0.96643 & RMSProp \\
          &       & 4\%   & 82.47 & 80.4  & 3.6117E-03 & 4.1040E-04 & 64    & 25    & 0.021987 & RMSProp \\
          &       & 5\%   & 83.36 & 81.7  & 1.0294E-03 & 5.3882E-04 & 256   & 23    & 0.028392 & RMSProp \\
          & \multirow{6}[0]{*}{\textbf{CiteSeer}} & 0.5\% & 59.6  & 56.1  & 1.9790E-03 & 4.0283E-04 & 16    & 20    & 0.007761 & RMSProp \\
          &       & 1\%   & 65.95 & 62.1  & 7.8506E-04 & 8.2432E-03 & 64    & 24    & 0.28159 & RMSProp \\
          &       & 2\%   & 70.23 & 68.6  & 5.4517E-04 & 1.0818E-02 & 256   & 12    & 0.27027 & RMSProp \\
          &       & 3\%   & 71.81 & 70.3  & 1.4107E-04 & 5.0062E-03 & 1024  & 9     & 0.57823 & RMSProp \\
          &       & 4\%   & 72.36 & 70.8  & 4.8864E-06 & 1.8038E-02 & 4096  & 12    & 0.11164 & RMSProp \\
          &       & 5\%   & 72.24 & 71.3  & 2.1761E-03 & 1.1753E-02 & 5000  & 8     & 0.71473 & Adam \\
          & \multirow{4}[1]{*}{\textbf{Pubmed}} & 0.03\% & 69.07 & 62.2  & 6.8475E-04 & 2.8822E-02 & 4096  & 7     & 0.97245 & RMSProp \\
          &       & 0.05\% & 71.77 & 68.3  & 2.3342E+04 & 2.2189E-03 & 1024  & 8     & 0.93694 & RMSProp \\
          &       & 0.1\% & 76.07 & 72.7  & 4.2629E-04 & 4.1339E-03 & 2048  & 8     & 0.98914 & RMSProp \\
          &       & 0.3\% & 80.04 & 79.2  & 2.2602E-04 & 3.3626E-02 & 2000  & 7     & 0.070573 & Adam \\
    \bottomrule
    \bottomrule
    \end{tabular}\label{tab:hyperparameters_without_validation}\end{table} 
\end{document}
