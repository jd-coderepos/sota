\pdfoutput=1


\documentclass[11pt]{article}
\usepackage[dvipsnames]{xcolor}
\usepackage{graphicx}
\usepackage[]{acl}

\usepackage{times}
\usepackage{latexsym}

\usepackage[T1]{fontenc}


\usepackage[utf8]{inputenc}

\usepackage{microtype}






\usepackage{comment}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{siunitx}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage[export, demo]{adjustbox}  
\usepackage{multicol, makecell}
\usepackage{bbm}
\usepackage{tablefootnote}
\renewcommand\tabularxcolumn[1]{m{#1}}
\newcolumntype{C}{>{\centering\arraybackslash}X}
\DeclareMathOperator*{\argmax}{argmax}
\definecolor{light-gray}{gray}{0.95}
\newcommand{\code}[1]{\colorbox{light-gray}{\texttt{#1}}}

\newcommand{\howard}[1]{\textcolor{green}{#1}}
\newcommand{\eric}[1]{\textcolor{purple}{#1}}

\usepackage{pifont}\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}

\definecolor{olive}{RGB}{0,153,51}

\title{Improving hateful memes detection via learning hatefulness-aware embedding space through retrieval-guided contrastive learning}





\author{Jingbiao Mei, Jinghong Chen, Weizhe Lin, Bill Byrne, Marcus Tomalin \\
Department of Engineering\\
University of Cambridge\\
Cambridge, United Kingdom, CB2 1PZ \\
  \texttt{\{jm2245, jc2124, wl356, wjb31, mt126\}@cam.ac.uk} \\}
\begin{document}
\maketitle

\begin{abstract}
Hateful memes have emerged as a significant concern on the Internet. These memes, which are a combination of image and text, often convey messages vastly different from their individual meanings. Thus, detecting hateful memes requires the system to jointly understand the visual and textual modalities. 
However, our investigation reveals that the embedding space of existing CLIP-based systems lacks sensitivity to subtle differences in memes that are vital for correct hatefulness classification. To address this issue, we propose constructing a hatefulness-aware embedding space through retrieval-guided contrastive training. Specifically, we add an auxiliary loss that utilizes hard negative and pseudo-gold samples to train the embedding space. Our approach achieves state-of-the-art performance on the Hateful Memes Challenge(HMC) dataset with an AUROC of 86.7. Notably, our approach outperforms much larger fine-tuned Large Multimodal Models like Flamingo and LLaVA.
Finally, we demonstrate a retrieval-based hateful memes detection system, which is capable of making hatefulness classification based on data unseen in training from a database. This allows developers to update the hateful memes detection system by simply adding new data without retraining â€” a desirable feature for real services in the constantly-evolving landscape of hateful memes on the Internet.


\end{abstract}

\begin{figure}[htbp!] 
\centering    
\includegraphics[width=0.49\textwidth]{Figs/Multimodal_Confounders_6memes.png}
\caption{Illustrative (not in the dataset) examples from \citealt{KielaFBHMC2020}. Memes on the left are mean, the ones in the middle are benign image confounders, and those on the right are benign text confounders.}
\label{fig:conf_HMC}
\end{figure}

\section{Introduction}


The pervasive growth of social media platform has been accompanied by an alarming surge in hateful content. A recent study shows that between 2019 and 2021, posts containing hate speech related to race or ethnicity were published on the Internet at an average rate of once every 1.7 seconds~\cite{ditch_the_label_2021}. Hateful memes, which consist of an image accompanied by texts, are becoming a dominant form of online hate speech. If left unchecked, these contents can perpetuate stereotypes, incite discrimination, and even catalyze real-world violence. To handle the large volume of potential hateful content and prevent viral circulation, automatic detection of hateful memes with deep neural models has garnered significant interests in the research community~\cite{KielaFBHMC2020, TamilTroll2020, suryawanshi-etal-2020-MultiOFF, pramanickCovidMeme2021, LiuFigMemes2022, HossainMUTEMeme2022, PrakashTotalDefMeme2023, SahinARCHateSpeechEvent2023}.



Despite previous efforts, correctly detecting harmful memes remains difficult. Previous literature has identified a prominent challenge in classifying "confounder memes", in which subtle differences in either image or text may lead to completely different meanings~\citep{KielaFBHMC2020}.  As shown in Figure \ref{fig:conf_HMC}, the top left and top middle memes share the same caption. However, one of them is hateful and the other benign depending on the accompanying images. Confounder memes resemble real memes on the Internet, where the combined message of image and texts contribute to their hateful nature. Previous works attempted to tackle the challenge by leveraging outside knowledge to ground the reasoning \cite{RonHMC1st2020}, or building stronger multimodal fusions in the early stage ~\cite{PramanickMomenta2021} and intermediate stage ~\cite{KumarHateClip2022}.
However, it has been observed that even state-of-the-art models, such as HateCLIPper~\cite{KumarHateClip2022}, exhibit limited sensitivity to nuanced hateful memes (xx\% accuracy on confounder examples). 

We find that a key factor contributing to misclassification is that confounder memes are located in close proximity in the embedding space due to the high similarity of text/image content.  For instance,  the HateCLIPper embeddings of confounder memes in Figure \ref{fig:conf_HMC} have high cosine similarity  even though they have opposite meanings. This poses challenges for the classifier to distinguish harmful and benign memes, leading to suboptimal performance.

To address this issue, we propose ``\textbf{Retrieval-Guided Contrastive Learning}'' (RGCL) which aims to learn hatefulness-aware vision and language joint representations. Specifically, we align the embeddings of same-class examples that are semantically similar, and separate the embeddings of opposite-class examples. We dynamically retrieve these examples during training and train with a contrastive objective in addition to cross-entropy loss. Our system, RGCL, achieves higher performance than state-of-the-art large multimodal systems on the HatefulMemes dataset with 200 times fewer model parameters, and 16,000 times fewer trainable parameters. In addition, we demonstrate that the well-learned embedding space enables the use of K-nearest-neighbor majority voting classifier. We show that the encoder trained on HarmfulMemes can be applied to HatefulMemes without additional training while maintaining high AUC and accuracy using the KNN majority voting classifier, even outperforming the zero-shot performance of large multi-modal models. This allows efficient transfer and update of hateful memes detection systems to handle the fast-evolving landscape of hateful memes in real-life applications.
We summarize our contribution as follows:
\begin{enumerate}
    \item We propose Retrieval-Guided Contrastive Learning for hateful memes detection which learns a hatefulness-aware embedding space via an auxiliary contrastive objective with dynamically retrieved samples. Our system achieves state-of-the-art performance on HatefulMemes and Harmful Memes
\item We demonstrate that the retrieval-based KNN majority voting classifier on the learned embedding space outperforms the zero-shot performance of large multimodal models of much larger scales. This allows developers to easily update and extend hateful memes detection system without retraining. 
\end{enumerate}

\section{Related Work}
We categorise previous hateful meme detection systems into three types: Object Detection (OD)-based systems, CLIP encoder-based systems, and Large Multimodal Models (LMM).
Object Detection (OD)-based models such as VisualBERT \cite{VisualBert2019}, OSCAR \cite{li2020oscar}, and UNITER \cite{Uniter2019} have been employed for detecting hateful memes. However, these models are not end-to-end trainable, often utilizing off-the-shelf object detectors, which can lead to performance bottlenecks. Additionally, the use of Faster R-CNN \cite{fater_RCNN_2015} based object detectors~\cite{Anderson2017up-down, vinVL2021} results in high inference latency \cite{Kim_ViLT2021}.

Recently, systems based on CLIP (\cite{clip2021}) encoders have gained popularity for detecting hateful memes due to its simpler end-to-end architecture. MOMENTA \cite{PramanickMomenta2021} and PromptHate \cite{caoPromptHate2022} augment CLIP representations with additional features such as text attributes and image captions.
HateCLIPper \cite{KumarHateClip2022} explored different types of modality interaction for CLIP vision and language representations to address challenging hateful memes. However, it still misclassifies challenging confounder memes and performs worse than Large Multimodal Models (LMM).

Several LMM like Flamingo~\cite{Flamingo22}, InstructBLIP~\cite{DaiInstructBLIP2023}, and LENS~\cite{BerriosLens2023} have demonstrated their effectiveness on the Hateful Meme Challenge dataset. The fine-tuned Flamingo 80B achieved a State-of-the-art AUROC of 86.6, outperforming all the previous CLIP-based systems but requiring a resource-intensive fine-tuning process.  However, in this paper, we demonstrate that a much smaller CLIP-based model can achieve better performance than such LMM with our proposed retrieval-guided contrastive learning.

While contrastive learning is widely used in vision tasks \cite{Schroff_FaceNet_2015, Song_metriclearningLifetedFeatureEmbedding_2016, Harwood_SmartMiningDeepMetricLearning_2017, Suh_StochasticHardExampleMiningForDeepMetricLearning2019}, its application to multimodally pre-trained encoders for hateful memes has not been well-explored. \citet{LippeHMFramework2020} incorporated negative examples in contrastive learning. However, due to the low quality of randomly sampled negative examples, they observed a degradation in performance. In contrast, our paper demonstrates that by incorporating dynamically sampled positive and negative examples, the system is capable of learning a hatefulness-aware vision and language joint representation. 


\section{Methodology}
\subsection{Feature Extraction}
In each training example
,  is the image pixels of the meme;  is the caption overlaid on the meme;  is the label, where 0 for benign, 1 for hateful.

We leverage a Vision-Language (VL) encoder to extract image-text joint representations from the image and the overlaid caption: 

As shown in Figure~\ref{fig:Model_Architecture}, the VL encoder comprises a frozen CLIP encoder followed by a trainable multilayer perceptron (MLP). The frozen CLIP encoder encodes the text and image into embeddings that are then fused into a joint vision-language embedding before feeding into the MLP.
In this paper, we default to using HateCLIPper as our frozen CLIP encoder. For a detailed model architecture, readers are referred to the HateCLIPper's paper \cite{KumarHateClip2022}. In Sec.\ref{sec:abl_encoder}, we compare different choices of VL encoder to demonstrate that our approach is agnostic to the encoder.

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{Figs/RGCL_New_Crop.jpg}
    \caption{Model overview. (1) Using Vision-Language (VL) Encoder  to extract the joint vision-language representation for a training example . Additionally, the VL Encoder encodes the training memes into a retrieval database . (2) During training, pseudo-gold and hard negative examples are obtained using the Faiss nearest neighbour search. During inference,  nearest neighbours are obtained using the same querying process to perform the KNN-based inference. (3) During training, we optimise the joint loss function . (4) For inference, we demonstrate the result with conventional logistic classifier and retrieval-based KNN mahority voting.  }
    \label{fig:Model_Architecture}
\end{figure*}
\subsection{Retrieval Guided Contrastive Learning}
We propose Retrieval-Guided Contrastive Learning which aims to learn hatefulness-aware vision and language joint representations. Specifically, we align the embeddings of same-class examples that are semantically similar by incorporating retrieved pseudo-gold positive examples and separate the embeddings of confounding examples by incorporating retrieved hard negative examples. We dynamically retrieve these examples during training and train with the Retrieval-Guided Contrastive Loss in addition to cross-entropy loss.
\subsubsection{Pseudo-gold positive and hard negative examples}
Pseudo-gold positive examples facilitate the clustering of memes within the same class that exhibit similar semantic characteristics, thereby strengthening the model's ability to capture a wide range of semantic relationships.
In contrast, hard negative examples are samples in the training set that share similarities with the anchor meme in the embedding space but carry different labels. In essence, these represent memes that the current embedding space has failed to distinguish correctly, reflecting instances of misclassification or confounder cases in the dataset. Introducing training signals with hard negative examples can effectively enhance the embedding space.

To obtain these examples, we first encode the training set with our VL encoder. Due to the computational expense, we only update the database after every epoch. The encoded retrieval vector database is denoted as :

For a training sample , We obtain the hard negative and pseudo-gold positive example from the training set with Faiss nearest neighbour search \cite{johnson_Faiss2019billion} by computing the similarity scores between the embedding vector  and any target embedding vector . 


We denote the hard negative example's embedding vector as :

Similarly, for the pseudo-gold positive example's embedding vector :

we define the mask :



\subsubsection{In-batch negative examples}
To enhance training stability and encourage robust learning, we incorporate in-batch negative examples. In-batch negative examples introduce diverse gradient signals in the training, and the randomly selected in-batch negative memes are pushed apart in the embedding space.
For a training sample , the set of in-batch negative examples is defined as the examples in the same batch that have a different label  as label . We denote the embedding vectors for the in-batch negative examples as . There are a total of  in-batch negative examples correspond to the training sample .
\subsubsection{Training objective}

is the vector representation of the original, pseudo-gold positive, hard negative, and in-batch negative examples corresponding to a training example .
Our proposed Retrieval-Guided Contrastive Loss (RGCL) can be computed as:

To train the system, we optimise the joint loss function:





\subsection{Retrieval-based KNN majority voting}
To assess the expressiveness and discrimination capability of the trained joint embedding space, we extend our analysis beyond the conventional logistic regression employed in recent models like HateCLIPper. Additionally, we introduce a retrieval-based inference mode: For each test meme, we retrieve memes located in close proximity within the embedding space and utilize probability voting to predict whether it is hateful or not.
This majority voting strategy heavily relies on the discrimination capability of the trained joint embedding space.
Only when the trained embedding space successfully splits hateful and benign examples will majority voting achieve reasonable performance.
We conduct experiments in Sec.~\ref{sec:results_knn} to show that applying RGCL leads to much better performance with retrieval-based KNN inference. 
\section{Experiment}
\subsection{Dataset}
We evaluate the performance of the system on the HatefulMemes dataset \cite{KielaFBHMC2020} and the HarMeme dataset \cite{pramanickCovidMeme2021}. HatefulMemes dataset is released by the Hateful Memes Challenge competition \cite{KielaFBHMC2020}.
The HarMeme dataset consists of COVID-19-related memes collected from Twitter. These memes are labelled with three classes: \textit{very harmful, partially harmful}, and \textit{harmless}. Following previous works \cite{caoPromptHate2022, PramanickMomenta2021}, we combine the very harmful and partially harmful memes into hateful memes and regard harmless memes as benign memes. The dataset statistics are shown in Appendix~\ref{appendix:data_stats}. To make a fair comparison, we adopt the evaluation metrics commonly used in existing hateful meme classification studies \cite{KumarHateClip2022, caoPromptHate2022, KielaFBHMC2020}: Area Under the Receiver Operating Characteristic Curve (AUC) and Accuracy (Acc). We train the system on the training split, develop them on the development splits and report the final results on the test set. The experiment setup and hyperparameter settings are detailed in Appendix~\ref{appendix:exp_setup} and \ref{appendix:hyperparam}. 

\subsection{Experiment results} 
\subsubsection{Results with logistic regression}
Table~\ref{tab:results_HMC} presents the experimental results on the HatefulMemes dataset.
Our Retrieval-guided contrastive learning approach is compared to a range of baseline models, including fine-tuned large multimodal models, Object-detector (OD) based models, Large Multimodal Models (LMM) and CLIP-based systems. The original CLIP \cite{clip2021} performs comparably to the OD-based models such as ERNIE-Vil \cite{ErnieViL2020}, UNITER \cite{Uniter2019} and OSCAR \cite{li2020oscar}, exhibiting comparable AUC scores of around . Flamingo-80B \cite{Flamingo22} stands as the state-of-the-art model for HatefulMemes, with an AUC of \footnote{Flamingo only reports AUC score. Since Flamingo is not open sourced, we are unable to reproduce the accuracy. Thus, we reproduce LLaVA to understand performance on state-of-the-art open source LMM.}. 
Additionally, we transform HatefulMemes into instruction following data and fine-tuned on LLaVA \cite{LiuLLAVA2023} (Vicuna-13B \citealp{vicuna2023}). LLaVA achieves  accuracy and  AUC.  
PromptHate \cite{caoPromptHate2022} and HateCLIPper \cite{KumarHateClip2022}, built on top of CLIP \cite{clip2021}, outperform both the original CLIP and OD-based models.
HateCLIPper achieves an AUC of \footnote{HateCLIPper only reports AUC score, thus we reproduce the system with their released code and obtain the corresponding AUC and Acc scores.}, surpassing the original CLIP but falling short compared to Flamingo-80B. 
HateCLIPper, trained using our proposed RGCL, obtained an AUC of , outperforms the 200 times larger Flamingo-80B. Notably, our system's accuracy also improves over HateCLIPper by nearly , reaching an accuracy of .



Table~\ref{tab:results_Harmeme} shows the result on the HarMeme dataset. Being a less popular dataset, there are no baseline results available for large multimodal models, leading us to focus solely on comparing outcomes with CLIP-based systems for the HarMeme dataset. 
Our Retrieval-guided contrastive learning approach obtained a remarkable accuracy of , outperforming HateCLIPper with an accuracy of , and PromptHate with an accuracy of . Our system's state-of-the-art performance on the HarMeme dataset further emphasises Retrieval-guided contrastive learning's robustness and generalisation capacity to different types of hateful memes.

\begin{table}[htb]
\small
\caption{Results on the HatefulMemes dataset}
\label{tab:results_HMC}
\centering
\begin{tabularx}{0.48\textwidth}{Xll}
\toprule
 Model                     & \textbf{AUC} & \textbf{Acc}.  \\ 
\midrule
\multicolumn{3}{l}{\textit{~~~~~Object Detector based models}} \\
 \midrule
ERNIE-Vil  & 79.7 & 72.7\\
UNITER & 79.1 & 70.5 \\
OSCAR  & 78.7 & 73.4 \\
\midrule
\multicolumn{3}{l}{\textit{~~~~~Fine tuned Large Multimodal Models}}                             \\ \midrule
Flamingo-80B & 86.6 & - \\
LLaVA (Vicuna-13B) & 85.3 & 77.3  \\
\midrule
\multicolumn{3}{l}{\textit{~~~~~Systems based on CLIP}} \\
 \midrule
CLIP  & 79.8 & 72.0 \\
MOMENTA & 69.2 & 61.3\\
PromptHate &81.5 & 73.0 \\ 
HateCLIPper & 85.5 & 76.0    \\
HateCLIPper w/ RGCL \textit{(Ours)} & \textbf{86.7} & \textbf{78.8} \\
\textit{~~~~with Sparse retrieval}  & \textbf{86.7} & 78.1 \\
\bottomrule
\end{tabularx}
\end{table}
\begin{table}[htb]
\small
\caption{Results on the HarMeme dataset}
\label{tab:results_Harmeme}
\centering
\begin{tabularx}{0.48\textwidth}{Xll}
\toprule
 Model                     & \textbf{AUC}  & \textbf{Acc}. \\ 
\midrule
 \multicolumn{3}{l}{\textit{~~~~~Systems based on CLIP}} \\
 \midrule
CLIP & 82.6 & 76.7 \\
MOMENTA & 86.3 & 80.5\\
PromptHate & 90.9 & 84.5 \\ 
HateCLIPper & 89.7  & 84.8    \\
HateCLIPper w/ RGCL \textit{(Ours)} & \textbf{91.8} & \textbf{87.0} \\
\bottomrule
\end{tabularx}
\end{table}




\subsubsection{Results with retrieval-based KNN majority voting}
\label{sec:results_knn}
In this section, we present the results on the HatefulMemes dataset using KNN-based majority voting classifier. As shown in Table~\ref{tab:results_retrieval}, we compare our model with a range of state-of-the-art LMMs' zero-shot performance, including Flamingo \cite{Flamingo22}, Lens \cite{BerriosLens2023}, Instruct-BLIP \cite{Ouyang_InstructGPT_2022} and LLaVA \cite{LiuLLAVA2023}. These models encompass diverse language models, with Flan-T5 \cite{Chung_FLAN_2022} employing an encoder-decoder architecture and Vicuna \cite{vicuna2023} adopting a decoder-only configuration. Among these models, Lens with Flan-T5XXL 11B demonstrates the highest zero-shot performance, achieving an AUC of . 
Since these LMM only report the AUC score for the HatefulMemes dataset, we report the accuracy with the open-sourced LLaVA with Vicuna-13B which achieves an accuracy of 54.8. 

For the KNN-based classifier, we first evaluate our model's performance on the HatefulMemes dataset when trained on the HarMeme dataset. When using the HarMeme as the retrieval database, our system achieves an AUC of  surpasses the baseline HateCLIPper's AUC of  and the best LMM's zero-shot AUC score.
To demonstrate that our system can be updated by simply adding new data without retraining which is a desirable feature for real services in the constantly-evolving landscape of hateful memes on the Internet, we further experiment with the same model but with HatefulMemes dataset as the retrieval database. 
It is worth noting that after switching the retrieval database to HatefulMemes for the model trained on the HarMeme, the baseline HateCLIPper's performance degrades, suggesting its embedding space lacks robustness and generalising capability to different domains of hateful memes.
However, after enhancing the same model with RGCL training, the AUC score boosts to , outperforming the baseline by a large margin of . Similarly, we achieved an accuracy of , surpassing the baseline by . Both the AUC and accuracy score largely surpass the zero-shot LMM.


Turning to our fully supervised system, trained on the HatefulMemes dataset and evaluated on the same dataset, its accuracy of  remains slightly below the accuracy of the classification mode at . Nevertheless, this still outperforms HateCLIPper by a margin of  . The AUC is lower than the logistic regression baseline due to the KNN inference does not output raw logits. 
\begin{table}[]
\small
\caption{KNN Inference results compared to large multimodal models' zero-shot results on HatefulMemes}
\label{tab:results_retrieval}
\centering
\begin{tabularx}{0.495\textwidth}{Xll}
\toprule


 Model                & \textbf{AUC} & \textbf{Acc}. \\ 
\midrule
\multicolumn{3}{l}{\textit{~~~~~Zero shot based on Large Multimodal Models}}                             \\ \midrule
{Flamingo-80B} & 46.4 & - \\
{Lens \textit{(Flan-T5 11B)}}  & 59.4 & - \\
{InstructBLIP \textit{(Flan-T5 11B)}}  & 54.1 & - \\
{InstructBLIP \textit{(Vicuna 13B)}}  & 57.5 & -  \\ 
{LLaVA \textit{(Vicuna 13B)}}  & 57.9 & 54.8 \\
\midrule

\multicolumn{3}{l}{\textit{~~~~~(Zero-shot) Train and retrieve on HarMeme }}                             \\ \midrule
HateCLIPper &  54.8 & 52.2 \\
HateCLIPper w/ RGCL &  \textbf{59.8 \textit{(\textcolor{olive}{+5.0})}} & \textbf{57.1 \textit{(\textcolor{olive}{+4.9})}} \\
\midrule
\multicolumn{3}{l}{\textit{~~~~~(Zero-shot) Train on HarMeme, retrieve on HatefulMemes}}                             \\ \midrule
HateCLIPper &  53.6 & 50.9 \\
HateCLIPper w/ RGCL &   \textbf{66.6 \textit{(\textcolor{olive}{+13.0})}} & \textbf{59.1 \textit{(\textcolor{olive}{+8.2})}} \\


\midrule
\multicolumn{3}{l}{\textit{~~~~~Train and retrieve on HatefulMemes}}                             \\ \midrule


HateCLIPper &  78.5 & 73.6  \\
HateCLIPper w/ RGCL &  \textbf{83.1 \textit{(\textcolor{olive}{+4.6})}} & \textbf{78.0 \textit{(\textcolor{olive}{+4.4})}}  \\


\bottomrule
\end{tabularx}
\end{table}
\subsection{Ablation Study}
\subsubsection{Effects of incorporating hard negative and pseudo-gold positive examples}
\label{sec:abl_exp}
In Table~\ref{tab:ablation_examples}, we conduct a comparative analysis by examining the performance when specific examples are excluded during the training process. Notably, when the hard negative examples are excluded, leaving only in-batch negative samples, the overall quality of negative samples declines. Similarly, when we omit the positive samples, only in-batch positive examples are incorporated during the training. Upon observation, it becomes evident that the removal of either the hard negative or the pseudo-gold positive examples leads to a discernible degradation in performance. Specifically, there is a decrease of  and  in AUC when omitting hard negative and pseudo-gold positive examples, respectively.  Furthermore, the accuracy metric experiences a more substantial reduction, with drops of  and  for the respective cases. Notably, the combined exclusion of both the hard negative and pseudo-gold examples results in a marked decrease in performance. This discrepancy is apparent in the AUC score, which experiences a substantial drop when compared to our baseline. The AUC only matches with HateCLIPper's performance, as indicated in Table~\ref{tab:results_HMC}. Additionally, the accuracy of  slightly outperforms HateCLIPper's accuracy of .

Furthermore, we explored the performance implications of incorporating multiple examples for each scenario. The inclusion of two hard negative examples leads to substantial performance deterioration, with corresponding drops of  and  in AUC and accuracy. In a similar vein, training with two pseudo-gold positive examples yields a slight decline in performance, resulting in a  decrease in AUC and a  decrease in accuracy. This phenomenon aligns with recent findings in the literature, as \citet{dpr2020} reported that the incorporation of multiple hard negative examples does not necessarily enhance performance.
\begin{table}[htb]
\small
\caption{Ablation study on omitting Hard negative and/or Pseudo-Gold positive examples on the HatefulMemes}
\label{tab:ablation_examples}
\centering
\begin{tabularx}{0.48\textwidth}{Xll}
\toprule
 Model                     & \textbf{AUC} & \textbf{Acc}.  \\ 
\midrule
Baseline RGCL & \textbf{86.7} & \textbf{78.8}\\ \midrule
w/o Hard negative & 86.1 & 77.1 \\
w/o Pseudo-Gold positive & 86.0 & 77.3 \\
w/o Hard negative and Pseudo-gold positive & 85.5 &  76.8\\
\midrule
w/ 2 Hard negative & 85.9 & 77.3 \\
w/ 2 Pseudo-Gold positive & 86.6 & 78.5 \\
\bottomrule
\end{tabularx}
\end{table}
\subsubsection{Effects of different VL Encoder}
\label{sec:abl_encoder}
We ablate the performance of our system on various base VL encoders. As shown in Table~\ref{tab:ablation_encoder}, we first experiment with various encoders in the CLIP family, we experiment with the original CLIP \cite{clip2021}, OPENCLIP \cite{ilharco_gabriel_OPENCLIP2021,schuhmann2022laionbopenclip,cherti2023reproducibleopenclip}, and AltCLIP \cite{chen2022altclip}. 
our method boosts the performance of all these variants of CLIP by around  in both AUC and accuracy. Furthermore, to make sure our method is not overfit to the CLIP architecture, we carry out experiments with ALIGN \cite{Jia2021ALIGN}. ALIGN only open-sourced the base model which is less capable than the larger CLIP based models. Nevertheless, RGCL still manages to enhance the AUC score by a margin of  over the baseline ALIGN model, suggesting our approach is agnostic to the choice of VL encoders. 

\begin{table}[htb]
\small
\caption{Ablation study on various Vision-Language Encoder on the HatefulMemes dataset}
\label{tab:ablation_encoder}
\centering
\begin{tabularx}{0.45\textwidth}{Xll}
\toprule
 Model                     & \textbf{AUC} & \textbf{Acc}.  \\ 
\midrule
HateCLIPper & 85.5 &76.0 \\
HateCLIPper w/ RGCL & 86.7 \textbf{\textit{(\textcolor{olive}{+1.2})}} & 78.8 \textbf{\textit{(\textcolor{olive}{+2.8})}} \\
\midrule
CLIP & 79.8 & 72.0\\
CLIP w/ RGCL & 83.8 \textbf{\textit{(\textcolor{olive}{+4.0})}} & 75.8 \textbf{\textit{(\textcolor{olive}{+3.8})}}\\
\midrule
OpenCLIP  & 82.9 & 71.7 \\
OpenCLIP w/ RGCL & 84.1 \textbf{\textit{(\textcolor{olive}{+1.2})}} & 75.1 \textbf{\textit{(\textcolor{olive}{+3.4})}}\\
\midrule
AltCLIP & 83.4 & 74.1\\
AltCLIP w/ RGCL & 86.5 \textbf{\textit{(\textcolor{olive}{+3.1})}} & 76.8 \textbf{\textit{(\textcolor{olive}{+2.7})}}\\
\midrule
ALIGN &  73.2& 66.8  \\
ALIGN w/ RGCL & 77.6 \textbf{\textit{(\textcolor{olive}{+4.4})}} & 68.9 \textbf{\textit{(\textcolor{olive}{+2.1})}} \\
\bottomrule
\end{tabularx}
\end{table}


\subsubsection{Effects of Dense/Sparse Retrieval}
We use dense retrieval to obtain pseudo-gold positive and hard negative examples to avoid an additional pipeline of object detection. However, in previous literature like dense passage retrieval \cite{dpr2020}, sparse retrieval methods like BM-25 \cite{Robertson_BM25_2009} are used to obtain hard negative examples to avoid dynamically encoding the vector retrieval database. Here, we also ablate the performance of our system when incorporating sparse retrieval to obtain the retrieved examples. We use VinVL object detector \cite{vinVL2021} to obtain the region-of-interest object prediction and its corresponding attributes. 
We set a region-of-interest bounding box detection threshold of , a minimum of 10 bounding boxes, and a maximum of 100 bounding boxes, consistent with the default settings of VinVL. After obtaining the text-based image features, we concatenate these text with the overlaid caption from the meme to perform the sparse retrieval. As shown in Table~\ref{tab:results_HMC}'s last row, the sparse retrieval method achieves the same AUC of  with the dense retrieval, but with a lower accuracy of . 

\subsubsection{Loss function and similarity metrics}
Besides cosine similarity (Cos), inner product (IP) and Euclidean L2 distance are also commonly used as similarity measures. Since Euclidean distance (L2) is a distance metric, we take its negative to serve as a measure of similarity. We tested these alternatives and found cosine similarity performs slightly better as shown in Table~\ref{tab:ablation_loss_sim}. Additionally, another popular loss function for ranking is triplet loss which compares a positive example with a negative example for an anchor meme. Our results in Table~\ref{tab:ablation_loss_sim} suggest using triplet loss performs comparable to the default NLL loss.
\begin{table}[]
\small
\caption{Ablation study on the loss function and similarity metrics on the HatefulMemes dataset}
\label{tab:ablation_loss_sim}
\centering
\begin{tabularx}{0.35\textwidth}{XXll}
\toprule
Loss & Similarity           & \textbf{AUC} & \textbf{Acc}.  \\ 
\midrule
\multirow{3}{*}{NLL} & Cos & \textbf{86.7} & \textbf{78.8} \\
 & IP & 86.1 & 78.2 \\
 & L2 & 85.7 & 76.6  \\
\midrule
\multirow{3}{*}{Triplet} & Cos & 86.7 & 78.7 \\
 & IP & 86.1 & 78.2 \\
 & L2 &  85.7 & 76.8 \\

\bottomrule
\end{tabularx}
\end{table}






\subsection{Qualitative Analysis}
In this section, we demonstrate confounder examples from HatefulMemes in Table~\ref{tab:counfounder_visualisation}. 
In Table~\ref{tab:counfounder_visualisation} (a), both the image and text confounders appear benign and reside in the training set. Specifically, the image confounder presents a meme with the caption "This is the worst cancer I've ever seen," accompanied by an image of two individuals who could potentially be doctors discussing the disease. The text confounder, on the other hand, showcases a meme praising the flag of Israel with the caption "the flag flies high and proud." However, when the text and image of these two memes are combined, an extremely hateful and antisemitic meme emerges. This anchor meme, which is in the test set, draws a comparison between Israel and a type of disease.
HateCLIPper misclassifies this anchor meme as benign with a borderline probability of 0.454. This borderline probability indicates that HateCLIPper's modality fusion attempts to comprehend both modalities. The fusion recognises the hateful context arising from combining something negative like a disease with a flag. However, the model remains overfitted to the training set, primarily influenced by its benign image and text confounder memes presented during training. The high cosine similarity scores of the anchor meme with the confounder memes (0.702 and 0.733 respectively) further support the notion that these memes, differing in only one modality, are positioned closely in the embedding space due to the cross-entropy training criteria. The resulting highly similar joint vision-language embeddings contribute to misclassification and limited generalisation to the test set. In contrast, our system correctly and confidently predicts the anchor meme's hatefulness with a probability of 0.999. Additionally, our system demonstrates very low similarity scores between the anchor meme and the confounder memes (-0.751 and -0.571 respectively). This implies that the proposed Retrieval-guided contrastive learning effectively learns a hatefulness-aware embedding space, placing the meme within the embedding space with a comprehensive hateful understanding derived from both vision and language components. Here, we omit the similar analysis for Table~\ref{tab:counfounder_visualisation} (b).
\begin{table*}[hbtp]
\caption[Visualisation for the Confounder memes in the HatefulMemes dataset]{Visualisation for the Confounder memes in the HatefulMemes dataset: We present two trios of memes including anchor memes, image Confounders and text confounders, showcasing the impact of image and text alterations on hatefulness prediction. The labels are the ground truth annotation provided by the dataset.  We show the output hateful probability and predictions from two systems: HateCLIPper \cite{KumarHateClip2022} and our system. Further, we provide the cosine similarity score between the anchor meme and its corresponding confounder meme.}
\small
\label{tab:counfounder_visualisation}
\centering
(a)
\begin{tabularx}{\textwidth}{Xccc}
& Anchor Meme & Image Confounder & Text Confounder \\ 
\midrule
 Meme & \includegraphics[valign=c, width=0.235\textwidth]{Demo/Group3/45139.png} & \includegraphics[valign=c,width=0.235\textwidth]{Demo/Group3/92735.png} & \includegraphics[valign=c, width=0.235\textwidth]{Demo/Group3/47192.png} \\
\midrule
Labels & Hateful & Benign & Benign \\
\midrule
 \multicolumn{4}{l}{\textit{~~~~~HateCLIPper}} \\
 \midrule
Probability & 0.454 & 0.000 & 0.001 \\  
Prediction & \textcolor{red}{Benign \xmark}  & Benign & Benign \\  
Similarity with anchor & - & 0.702 & 0.733 \\
\midrule
 \multicolumn{4}{l}{\textit{~~~~~HateCLIPper w/ RGCL (Ours)}} \\
 \midrule
Probability & 0.999 & 0.000  & 0.000 \\
Prediction  & \textbf{\textcolor{green}{Hateful \cmark}} & Benign  & Benign \\
Similarity with anchor & - & \textbf{-0.751} & \textbf{-0.571} \\
\midrule
\end{tabularx}
\vspace{2pt}
(b)
\vspace{2pt}
\begin{tabularx}{\textwidth}{Xccc}
\midrule
 Meme & \includegraphics[valign=c, width=0.235\textwidth]{Demo/Group1/49023.png} & \includegraphics[valign=c, width=0.235\textwidth]{Demo/Group1/26930.png} & \includegraphics[valign=c, width=0.235\textwidth]{Demo/Group1/38154.png} \\
 \midrule
Labels & Hateful & Benign & Benign \\
\midrule
 \multicolumn{4}{l}{\textit{~~~~~HateCLIPper}} \\
 \midrule
Probability  & 0.038 & 0.000 & 0.001 \\  

Prediction & \textcolor{red}{Benign \xmark} & Benign & Benign \\  
Similarity with anchor & - & 0.898 & 0.913 \\
\midrule
 \multicolumn{4}{l}{\textit{~~~~~HateCLIPper w/ RGCL (Ours)}} \\
\midrule
Probability& 1.00 & 0.000  & 0.000 \\
Prediction & \textbf{\textcolor{green}{Hateful \cmark}} & Benign  & Benign \\
Similarity with anchor & - &\textbf{ -0.803} & \textbf{-0.769} \\
\bottomrule
\end{tabularx}
\end{table*}
\section{Conclusion}




In conclusion, we introduced Retrieval-Guided Contrastive Learning to enhance any VL encoders, addressing challenges in distinguishing confounding memes. Our approach, leveraging a novel auxiliary task loss with retrieved examples, significantly improved contextual understanding. Achieving an outstanding AUC score of  on HatefulMemes dataset, our system outperformed prior state-of-the-art models, including the 200 times larger Flamingo-80B. Additionally, our approach demonstrated state-of-the-art results on the HarMeme dataset, emphasizing its robust generalizability across diverse meme domains.


\section{Limitation}
Various works define hate speech differently, and they frequently use other terminology, such as online harassment, online aggression, cyberbullying, or harmful speech. United Nations Strategy and Plan of Action on Hate Speech stated that the definition of hateful could be controversial and disputed \cite{united_nations_2020}. 
Additionally, according to UK's Online Harms White Paper, harms could be insufficiently defined \cite{uk_parliament_2022}.
On the technical side, current state-of-the-art systems still perform far from satisfactory. For example, Table~\ref{tab:error_visualisation} shows a trio of memes from the HatefulMemes dataset, adopting a structure similar to Table~\ref{tab:counfounder_visualisation}. The Anchor meme portrays a person with an exaggeratedly elongated nose with a caption of "when your jewish friend smells a stash of coins in public". This meme carries implicit offensiveness towards the Jewish community. Our method correctly categorizes it as hateful, marking an improvement over the HateCLIPper model's performance.  In the case of the image confounder, the meme substitutes the image with one depicting a person discovering a dirty can in public, displaying a disgusted facial expression. The combination of text and image renders this meme benign. However, neither of the two systems successfully identifies this meme as hateful. This limitation might arise from the models' inability to comprehend facial expressions, which remains a constraint of our approach. Such challenges could potentially be addressed with a more robust vision encoder. 

\begin{table*}[hbt]
\caption[Visualisation for the error cases]{Visualisation for the error cases}
\small
\label{tab:error_visualisation}
\centering
\vspace{4pt}
\begin{tabularx}{\textwidth}{Xccc}
& Anchor Meme & Image Confounder & Text Confounder \\ 
\midrule
 Meme & \includegraphics[valign=c, width=0.22\textwidth]{Demo/Group2/19536.png} &  \includegraphics[valign=c, width=0.265\textwidth]{Demo/Group2/84150.png} & \includegraphics[valign=c,width=0.22\textwidth]{Demo/Group2/19075.png}  \\
 \midrule
Labels & Hateful & Benign & Benign \\
\midrule
 \multicolumn{4}{l}{\textit{~~~~~HateCLIPper}} \\
 \midrule
Probability & 0.355  & 0.870 & 0.000 \\  

Prediction & \textcolor{red}{Benign \xmark}  &  \textcolor{red}{Hateful \xmark} & Benign  \\  
Similarity with anchor & -  & 0.898  & 0.674\\
\midrule
 \multicolumn{4}{l}{\textit{~~~~~HateCLIPper w/ RGCL (Ours)}} \\
\midrule
Probability & 0.985   & 0.999 & 0.000 \\
Prediction & \textbf{\textcolor{green}{Hateful \cmark}}   & \textcolor{red}{Hateful \xmark} & Benign \\
Similarity with anchor & -  & 0.856 & \textbf{-0.548} \\
\bottomrule
\end{tabularx}
\end{table*}





\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}







\appendix
\label{sec:appendix}

\section{Experiment Setup}
\label{appendix:exp_setup}
A work station equipped with NVIDIA RTX 3090 and AMD 5900X was used for the experiment. \code{PyTorch 2.0.1}, \code{CUDA 11.8}, and \code{Python 3.10.12} were used for implementing the experiments. HuggingFace transformer library \cite{HuggingFace_trans_2019} was used for implementing the pretrained CLIP encoder. Faiss \cite{johnson_Faiss2019billion} vector similarity search library with version \code{faiss-gpu 1.7.2} was used to perform dense retrieval. Sparse retrieval was performed with \code{rank-bm25 0.2.2} \footnote{\href{https://github.com/dorianbrown/rank\_bm25}{https://github.com/dorianbrown/rank\_bm25}}. All the reported metrics were computed by \code{TorchMetrics 1.0.1} \footnote{\href{https://torchmetrics.readthedocs.io}{https://torchmetrics.readthedocs.io}. TorchMetrics version 1.0 and above has resolved the bug related to computing the micro F1 score.}.
Furthermore, for fine-tuning the LLaVA\cite{LiuLLAVA2023}, we utilises a system with 4 A100-80GB. 

\section{Hyperparameter}
\label{appendix:hyperparam}
The default hyperparameter for all the models are shown in Table~\ref{tab:hyperparameters}. The modelling hyperparameter is based on HateCLIPper's setting \cite{KumarHateClip2022} for a fair comparison. The hyperparameters associated with retrieval-guided contrastive learning are manually tuned with respect to the evaluation metric on the development set. With this configuration of hyperparameter, the number of trainable parameters is about 5 million and training takes around 30 minutes.
\begin{table}[h]
\small
\centering
\caption{Default hyperparameter values for the modelling and Retrieval-Guided Contrastive Learning (\textbf{RGCL})}
\label{tab:hyperparameters}
\begin{tabular}{lc}
\toprule
Modelling hyperparameter & Value \\
\midrule
\midrule
Image size & 336 \\
Pretrained CLIP model & ViT-L-Patch/14 \\
Projection dimension of MLP & 1024 \\
Number of layers in the MLP & 3 \\
Optimizer & AdamW \\
Maximum epochs & 30 \\
Batch size & 64 \\
Learning rate & 0.0001 \\
Weight decay & 0.0001 \\
Gradient clip value & 0.1 \\
\midrule 
RGCL hyperparameter  & Value \\
\midrule
\midrule
\# hard negative examples & 1 \\
\# pseudo-gold positive examples & 1 \\
Similarity metric & Cosine similarity \\
Loss function & NLL \\
Top-K for retrieval based inference & 10\\ 

\bottomrule
\end{tabular}
\end{table}
\begin{comment}
\section{Similarity metrics and loss functions}
\label{appendix:sim_loss}
In addition to the cosine similarity, we also compare the inner product and Euclidean distance (L2). Since Euclidean distance (L2) is a distance metric,  we take its negative to serve as a measure of similarity. Furthermore, we compare

    \begin{itemize}
    \item \textbf{Cosine similarity }
    
    \item \textbf{Inner product}
    
     and  are not mean normalised to have unit length, as normalisation would result in a reduction to \textbf{Cosine Similarity} 
    \item \textbf{Negative Squared Euclidean distance}
    
    \end{itemize}
\end{comment}




\begin{comment}
    \section{Positive examples }
TODO 
In the context of contrastive learning, positive examples are essential for ranking purposes, as positive examples from the same class should exhibit higher similarity scores compared to negative examples. Initially, we considered utilizing in-batch positive examples as positive demonstrations for an anchor meme. However, the randomness of the in-batch positive leads to a suboptimal performance as we will discuss in the next paragraph.

Ideally, in the embedding space, semantically similar examples should be near each other, while semantically dissimilar examples should be distant, even if they share the same label \cite{reverseEngSSL2023}. This implies that the system should generate distinct clusters for semantically similar examples with the same labels. For instance, memes in the Facebook Hateful Meme dataset might form clusters like "fun benign," "humorous benign," "sexist hate," "racist hate," and so forth. However, when training with in-batch positive examples, the randomness of sample selection tends to cluster all same-labelled examples together, regardless of their semantic similarities. Consequently, a single large cluster emerges, undermining the representation's ability to capture nuanced semantic relationships.


To address this limitation, we intend to group together memes within the same class that share semantic similarities, thereby enhancing the model's capacity to comprehend diverse semantic relationships. Inspired by the gold passages which are the ground truth passages in passage retrieval, we introduce the \textbf{Pseudo-gold positive} examples. To obtain these examples, we retrieve a positive sample that demonstrates a high similarity score with the anchor meme.
Incorporating pseudo-gold positive examples, similar examples with the same label are clustered together in the embedding space, while less-similar examples with the same labels do not coalesce excessively. This approach ensures a relatively sparse distribution of examples in the embedding space with multiple well-defined clusters. We find that this strategy outperforms the utilization of in-batch positive examples (i.e., random) or hard positive examples, which are more commonly adopted in contrastive learning methodologies.
\end{comment}






\section{Dataset statistics}
Table~\ref{tab:dataset_stats} shows the data split for the HatefulMeme and HarMeme dataset.
\label{appendix:data_stats}
\begin{table}[h]
\small
\centering
\caption{Statistical summary of HatefulMemes and HarMeme datasets}
\begin{tabular}{c|cc|cc}
\toprule
Datasets  & \multicolumn{2}{c}{Train}  & \multicolumn{2}{c}{Test}  \\
& \#Benign & \#Hate & \#Benign & \#Hate  \\
\midrule
HatefulMeme &   5450 & 3050 & 500 & 500              \\
\midrule
HarMeme  & 1949 & 1064 & 230 & 124             \\
\bottomrule
\end{tabular}\label{tab:dataset_stats}
\end{table}
\begin{comment}
    

\section{Sparse retrieval}
\label{appendix:sparse_retrieval}
Table~\ref{tab:ablation_loss_sim} presents a comparative analysis of the results achieved on the Hateful Memes dataset using sparse retrieval to obtain the hard negative and pseudo-gold positive examples. The evaluation includes the exploration of different values for the number of objects  detected per meme. When varying the number of objects  across different memes, we set a region-of-interest bounding box detection threshold of , a minimum of 10 bounding boxes, and a maximum of 100 bounding boxes, consistent with the default settings of the VinVL model \cite{vinVL2021}. The AUC score of using a variable number of objects in sparse retrieval is comparable with the dense retrieval baseline, however, the accuracy observes a degradation of . In the context of fixed object numbers, performance is compared for 72 and 50 bounding box predictions in sparse retrieval. Notably, the results are noticeably inferior compared to the dense retrieval baseline. Consequently, it is evident that the system's performance when using sparse retrieval is influenced by the number of bounding box predictions. Given the preference to maintain an end-to-end training procedure and avoid feature extraction, the decision to utilize dense retrieval for obtaining hard negative and pseudo-gold positive examples is reaffirmed.
\label{sec:abl_retrieval}
\begin{table}[htb]
\caption{Ablation study of Dense retrieval and Sparse retrieval on the HatefulMemes dataset}
\label{tab:ablation_sparse}
\centering
\begin{tabularx}{0.48\textwidth}{Xll}
\toprule
 Model             & \textbf{AUC} & \textbf{Acc}.  \\ 
\midrule
\multicolumn{3}{l}{\textit{~~~~~Training with Dense Retrieval}}                             \\ \midrule
Baseline & 86.7 & 78.8\\
\midrule
\multicolumn{3}{l}{\textit{~~~~~Training with Sparse Retrieval}}                             \\ \midrule
w/ Variable No. of objects & 86.7 & 78.1 \\
w/ 72 objects  & 86.1 & 77.1 \\
w/ 50 objects & 85.9 & 78.6  \\ 
\bottomrule
\end{tabularx}
\end{table}

\end{comment}

\end{document}
