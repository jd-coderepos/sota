\pdfoutput=1
\synctex=1
\def\confversion{1}



\documentclass{article} \usepackage{iclr,times}

\iclrfinalcopy 

\usepackage{lineno}
  \renewcommand{\linenumberfont}{\sffamily\tiny\color{gray!50}}
\usepackage{marginnote}
  \renewcommand*{\marginfont}{\color{red}\sffamily\scriptsize}
  \setlength{\marginparwidth}{1.1in}




\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{url}            \urlstyle{rm}
\usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{cleveref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bbm}
\usepackage{lipsum}
\usepackage{xspace}

\usepackage{array}
\usepackage{comment}
\usepackage{kotex}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{adjustbox}
\usepackage{caption}
\usepackage{subfigure}
\usepackage{wrapfig}

\usepackage{morefloats}
\usepackage{dblfloatfix}
\usepackage{placeins}
\usepackage{makecell}

\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{color}
\PassOptionsToPackage{usenames,dvipsnames}{xcolor}


\newcommand{\memo}[1]{{\color{cyan}{[#1]}}}
\newcommand{\wook}[1]{{\color{red}{\small\bf\sf [Jongwook: #1]}}}
\newcommand{\yijie}[1]{{\color{blue}{\small\bf\sf [Yijie: #1]}}}
\newcommand{\marcin}[1]{{\color{green!60!black}{\small\bf\sf [Marcin: #1]}}}
\newcommand{\neal}[1]{{\color{violet}{\small\bf\sf [Neal: #1]}}}
\newcommand{\alert}[1]{{\color{red}{#1}}}


\newcommand{\bm}[1]{\boldsymbol{#1}}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\softmax}{\operatornamewithlimits{softmax}}

\usepackage[colorinlistoftodos]{todonotes}
\newcommand{\jun}[1]{\todo[color=green!40,inline]{Jun: #1}}

\newcommand{\coex}{{CoEX}}
\newcommand{\coexRAM}{{CoEX+RAM}}
\newcommand{\ADM}{{ADM}}
\newcommand{\revise}[1]{{\color{blue}{#1}}}
\newcommand{\outdated}[1]{{\color{gray!50}{#1}}}

\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\onedot{.}
\def\eg{\emph{e.g}\onedot} \def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot} \def\Ie{\emph{I.e}\onedot}
\def\cf{\emph{c.f}\onedot} \def\Cf{\emph{C.f}\onedot}
\def\etc{\emph{etc}\onedot} \def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\etal{\emph{et al}\onedot}

\newcommand{\z}{\mathbf z}
\newcommand{\x}{\mathbf x}
\newcommand{\A}{\mathbf A}
\newcommand{\R}{\mathbb R}
\newcommand{\Z}{\mathbb Z}


\makeatletter
\newcommand*{\centerfloat}{\parindent \z@
  \leftskip \z@ \@plus 1fil \@minus \textwidth
  \rightskip\leftskip
  \parfillskip \z@skip}
\makeatother


\PassOptionsToPackage{sort}{natbib}
\usepackage{natbib}
\setcitestyle{authoryear,round,citesep={;},aysep={,},yysep={;}}
\usepackage[pagebackref=false,breaklinks=true,colorlinks,bookmarks=false,citecolor=blue]{hyperref}

\title{
Contingency-Aware Exploration in \\ Reinforcement Learning
}

\newcommand{\ProjectURL}{https://coex-rl.github.io/}





\author{Jongwook Choi\thanks{Equal contributions, listed in alphabetical order.}\,\, \hspace{1em}
    Yijie Guo          \hspace{1em}
    Marcin Moczulski          \hspace{1em} \\ \bf
    Junhyuk Oh          \hspace{1em}
    Neal Wu          \hspace{1em}
    Mohammad Norouzi          \hspace{1em}
    Honglak Lee      \hspace{1em}
    \
\widehat{a}_{t-1} = f_\text{inv}(s_{t-1}, s_{t}).

    e_t(i, j) =
        \mathrm{MLP}\Big(\big[
\phi^s_{t}(i, j) - \phi^s_{t-1}(i, j)  ;~  \phi^s_{t}(i, j)
\big]\Big)
    \in \mathbb{R}^{|\mathcal A|}.

\alpha_t = \mathrm{sparsemax}(\widetilde{\alpha}_t)
    \text{~~~~where~~~}
    \widetilde{\alpha}_t(i, j) = \mathrm{MLP}\big( \phi^s_t (i,j) \big),

    p(\widehat a_{t-1} \mid s_{t-1}, s_{t}) = \softmax \hspace*{-2pt}\left( {
            \textstyle \sum_{i,j} \alpha_{t}(i,j) \cdot e_t(i, j)
    } \right)\in \mathbb R^{|\mathcal A|}.

    \label{eq:loss_ADM}
    \mathcal L^\text{ADM} ~=~ \mathcal L_\text{action} + \textstyle\sum_{i,j} \mathcal L_\text{cell}^{i,j} + \lambda_\text{ent} \mathcal L_\text{ent} 5pt]
    \@title
           }\bottomtitlebar}
\makeatother
\fi


{\LARGE \textsc{Appendix}}\vspace{1em}

\appendix
\vspace*{-10pt}




\section{Summary of Training Algorithm}
\label{sec:summary_algorithm}

\begin{algorithm}
\begin{algorithmic}
\STATE Initialize parameter  for attentive dynamics model 
\STATE Initialize parameter  for actor-critic network
\STATE Initialize parameter  for context embedding projector if applicable (which is not trainable)
\STATE Initialize transition buffer 
\FOR{each iteration}
\STATE {\textit{ Collect on-policy transition samples, distributed over  parallel actors}}
	\FOR{each step }
		\STATE  Observe state
		\STATE 
        \STATE  Perform action  in the environment
\smallskip
        \STATE {\textit{ Compute the contingent region information}}
        \STATE  Compute the attention map of  using 
        \STATE  Compute the observation embedding cluster of  (Algorithm \ref{alg:clustering}) 
\smallskip
        \STATE {\textit{ Increment state visitation counter based on the representation}}
        \STATE  (, , )
\STATE 
		\STATE 
        \smallskip
        \STATE Store transition 
	\ENDFOR
	\STATE {\textit{ Perform actor-critic using on-policy samples in }}
    \STATE 
	\STATE {\textit{ Train the attentive dynamics model using on-policy samples in }}
    \STATE 
\STATE Clear transition buffer 
\ENDFOR
\end{algorithmic}
\caption{A2C+\coex{}}\label{alg:exp}
\end{algorithm}

The learning objective  is from Equation (\ref{eq:loss_ADM}).
The objective  of Advantage Actor-Critic (A2C) is as in \citep{Mnih:ICML2016:A3C,Dhariwal:2017:baselines}:

where  is the -step bootstrapped return and  is a weight for the standard entropy regularization loss term .
We omit the subscript as  when it is clear.


\clearpage





\section{Architecture and Hyperparameter Details}
\label{sec:hyperparam_details}

The architecture details of the attentive dynamics model (ADM), the policy network, and hyper-parameters are as follows.

\renewcommand{\arraystretch}{1.4}


\begin{table}[H]
\small
\centering
\caption{Network architecture and hyperparameters}
    \vspace*{-5pt}
\smallskip
\label{tbl:hyper-archi}
\begin{tabular}{l l l l}
\toprule
\multicolumn{2}{c}{Hyperparameters} & Value \\
\midrule
\multicolumn{2}{l}{Policy and Value Network Architecture} &
~Input: 84x84x1 \\
                                      & & - Conv(32-8x8-4) & /ReLU\\
                                      & & - Conv(64-4x4-2) & /ReLU\\
                                      & & - Conv(64-3x3-1) & /ReLU \\
                                      & & - FC(512)        & /ReLU\\
                                      & & - FC(), FC(1)\\
\midrule
\multicolumn{2}{l}{ADM Encoder Architecture} &
~Input: 160x160x3 \\
                         & & - Conv(8-4x4-2) & /LeakyReLU\\
                         & & - Conv(8-3x3-2) & /LeakyReLU\\
                         & & - Conv(16-3x3-2) & /LeakyReLU\\
                         & & - Conv(16-3x3-2) & /LeakyReLU\\
\multicolumn{2}{l}{ MLP Architecture for }
                         & FC(1296,256) &/ReLU \\
                                &&- FC(256,128)  &/ReLU \\
                                &&- FC(128,) \\
\multicolumn{2}{l}{MLP Architecture for }
                & FC(1296,64) &/ReLU \\
                                                 &&- FC(64,64) &/ReLU \\
                                                 &&- FC(64,1) \\
\multicolumn{2}{l}{ for Loss}
                & 0.001 \\
\midrule
A2C & Discount Factor   & 0.99 \\
    & Learning Rate (RMSProp) & 0.0007 \\
    & Number of Parallel Environments & 16 \\
    & Number of Roll-out Steps per Iteration & 5 \\
    & Entropy Regularization of Policy () & 0.01 \\
\midrule
PPO
    & Discount Factor   & 0.99 \\
    &  for GAE               & 0.95    \\
    & Learning rate (Adam)            & 0.00001 \\
    & Number of Parallel Environments & 128 \\
    & Rollout Length                  & 128    \\ 
    & Number of Minibatches           & 4      \\ 
    & Number of Optimization Epochs   & 4      \\ 
& Coefficient of Extrinsic and Intrinsic reward &       \\ 
    & Entropy Regularization of Policy ()           & 0.01   \\ 
\bottomrule
\end{tabular}
    \vspace*{-5pt}
\end{table}



\clearpage

\begin{table}[H]
\small
\centering
\caption{The list of hyperparameters used for A2C+\coex{} in each game.
    For the four games where there is no change of high-level visual context
    (\Freeway, \Frostbite, \Qbert and \Seaquest),
    we do not include  in the state representation , hence there is no .
The same values of  are used in PPO+\coex{}.
}
\label{tbl:hyper-a2c}
\begin{tabular}{l l l l l}
\toprule
Games   &  in A2C+\coex{}  &  in A2C+\coex{} & in A2C &  for clustering\\
\midrule
\Freeway & 10& 10 & 10 &-\\
\Frostbite & 10 & 10 & 10&- \\
\Hero &1 &0.1 & 1&0.7 \\
\MontezumaRevenge &10 &10 &10 &0.7\\
\PrivateEye &10 &10 &10 & 0.55\\
\Qbert &1 &0.5 &1 &-\\
\Seaquest &1 &0.5 &10 &-\\
\Venture &10 &10 & 10&0.7\\
\bottomrule
\end{tabular}
    \vspace*{-5pt}
\end{table}

\renewcommand{\arraystretch}{1.4}  




\clearpage

\section{Experiment with RAM Information}
\label{sec:a2c_exp_ram}
In order to understand the performance of exploration with perfect representation,
we extract the ground-truth location of the agent and the room number from RAM,
and then run count-based exploration with the perfect . Figure \ref{fig:a2c_exp_ram} shows the learning curves of the experiments;
we could see A2C+\coexRAM{} acts as an upper bound performance of our proposed method.
\begin{figure*}[bt] \begin{center}
\centerfloat
    \includegraphics[width=1.03\linewidth]{figures/a2c-allgames-ram.pdf}
    \vspace*{-5pt}
    \caption{
        Learning curves on several Atari games: A2C, A2C+\coex{}, and A2C+\coexRAM{}.
    }
    \label{fig:a2c_exp_ram}
    \vspace*{-5pt}
\end{center} \end{figure*}







\section{Observation Embedding Clustering}
\label{sec:context_embedding}
We describe the detail of a method to obtain the observation embedding.
Given an observation of shape , we flatten the observation and project it to an embedding of dimension .
We randomly initialize the parameter of the fully-connected layer for projection, and keep the values unchanged during the training to make the embedding stationary.

For the embedding of these observations, we cluster them based on a threshold value .
The value of  for each game with change of rooms is listed in Table \ref{tbl:hyper-a2c}.
If the distance between the current embedding and the center  of a cluster  is less than the threshold,
we assign this embedding to the cluster with the smallest distance and update its center with the mean value of all embeddings belonging to this cluster.
If the distance between the current embedding and the center of any cluster is larger than the threshold,
we create a new cluster and this embedding is assigned to this new cluster.


\begin{algorithm}
\begin{algorithmic}
\STATE Initialize parameter  for context embedding projector if applicable (which is not trainable)
\STATE Initialize threshold  for clustering
\STATE Initialize clusters set 
\FOR{each observation }
\STATE {\textit{ Get embedding of the observation from the random projection}}
\STATE 
\STATE {\textit{ Find a cluster to which the current embedding fits, if any}}
\STATE Find a cluster  with smallest , or  if there is no such
\IF{}
	    \STATE 
	\ELSE
	    \STATE {\textit{ if there's no existing cluster that  should be assigned to, create a new one}}
\STATE 
	\ENDIF
\ENDFOR
\end{algorithmic}
\caption{Observation Embedding Clustering}
\label{alg:clustering}
\end{algorithm}

In Figure \ref{fig:cluster_samples}, we also show the samples of observation in each cluster.
We could see observations from the same room are assigned to the same cluster and different clusters correspond to different rooms.
\begin{figure*}[bt] \begin{center}
\includegraphics[width=0.5555\linewidth]{figures/venture_cluster_samples.pdf}
    \vspace*{-0.3cm}
    \includegraphics[width=\linewidth]{figures/hero_cluster_samples.pdf}
    \vspace*{-0.3cm}
    \includegraphics[width=\linewidth]{figures/privateeye_cluster_samples.pdf}
    \vspace*{-0.3cm}
    \includegraphics[width=\linewidth]{figures/montezuma_cluster_samples.pdf}
    
    \vspace*{0.3cm}
    \caption{
        Sample of clustering results for \Venture, \Hero, \PrivateEye, and \MontezumaRevenge. Each column is one cluster, and we show 3 random samples assigned into this cluster.
    }
    \label{fig:cluster_samples}
\end{center} \end{figure*}

\clearpage

























 




\clearpage


\FloatBarrier

\section{Ablation Study on Attentive Dynamics Model}
\label{sec:ablation_adm}

We conduct a simple ablation study on the learning objectives of ADM, described in Equation (\ref{eq:loss_ADM}).
We evaluate the performance of ADM when trained on the same trajectory data under different combinations of loss terms,
simulating batches of on-policy transition data to be replayed.
The sample trajectory was obtained from an instance of A2C+CoEX+RAM and kept same across all the runs,
which allows a fair comparison between different variants.
We compare the following four methods:
\begin{itemize}[leftmargin=7mm]
\setlength{\itemsep}{0pt}\setlength{\parskip}{3pt}
\item ADM (action)             : train ADM using  only
\item ADM (action, cell)       : train ADM using  and 
\item ADM (action, ent)        : train ADM using  and 
\item ADM (action, cell, ent)  : train ADM using all losses ()
\end{itemize}


Figure~\ref{fig:a2c_exp_ablation_adm_fixed} shows the average distance
between the ground-truth location of the agent and the predicted one by ADM
during the early stages of training.
On \MontezumaRevenge, there is only little difference between the variants
although the full model worked slightly better on average.
On \Freeway, the effect of loss terms is more clear;
in the beginning
the agent tends to behave suboptimally by taking mostly single actions only
(UP out of three action choices --- UP, DOWN, and NO-OP),
hence very low entropy ,
which can confuse the ADM of telling
which part is actually controllable
as the action classifier would give correct answer regardless of attention.
We can observe additional loss terms help the model
quickly correct the attention to localize the controllable object among the uncontrollable clutters
with better stability.



\begin{figure*}[th]
\begin{center}
    \includegraphics[width=0.99\linewidth]{figures/ablation-adm-fixed.pdf}
    \vspace*{-0.2cm}
    \caption{
        Performance of ADM in terms of mean distance
        under different loss combinations in early stages,
        trained using the same online trajectory data.
        Plots were obtained by averaging runs over 5 random seeds.
    }
    \label{fig:a2c_exp_ablation_adm_fixed}
\end{center}
\end{figure*}


In another ablation study, we compare the end performance of the A2C+\coex{} agent
with the ADM jointly trained under different loss objectives
on these three games (\MontezumaRevenge, \Freeway and \Seaquest).
In our experiments,
the variant with full ADM worked best on \MontezumaRevenge and \Freeway.
The minimal training objective of ADM (\ie, )
also solely works reasonably well, but with the combination of other loss terms
we can attain a more stable performance.

\begin{figure*}[th]
\begin{center}
    \includegraphics[width=0.99\linewidth]{figures/ablation-adm-joint.pdf}
    \vspace*{-0.2cm}
    \caption{
        Learning curves of A2C+CoEX with ADM trained under different training objectives.
        The curve in solid line shows the mean episode over 40 recent episodes,
        averaged over 3 random seeds.
}
    \label{fig:a2c_exp_ablation_adm_joint}
\end{center}
\end{figure*}





\clearpage

\section{Ablation Study on the State Representation}
\label{sec:ablation_representation}

We present a result of additional ablation study on the state representation  used in count-based exploration.
The following baselines are considered:

\begin{itemize}[leftmargin=7mm]
\setlength{\itemsep}{0pt}\setlength{\parskip}{3pt}
\item A2C+\coex: Uses only the context embedding for exploration,
      \ie, .
\item A2C+\coex: Uses only the context embedding and the cumulative reward for exploration
      without contingent region information, \ie, .
\item A2C+\coex: Uses the contingent region information  as well as the context embedding ,
      however without the cumulative reward component, \ie, .
\end{itemize}

One can also consider another baseline similar to A2C+\coex
with ,
where the location information  is replaced with random coordinates uniformly sampled from the grid.
It ablates the learned contingent regions.
However, we found that it performs similarly to the presented A2C+CoEX() baseline.

The experimental results are summarized in Table \ref{tbl:a2c_additional_performance} and
Figure \ref{fig:a2c_exp_additional_1}. The variants without contingent regions
(\ie, A2C+\coex() and A2C+\coex()
performed significantly worse in most of the games than
A2C+\coex and
A2C+\coex 
giving little improvement over the A2C baseline.
Most notably, in the games with the hardest exploration such as \MontezumaRevenge and \Venture,
the performance is hardly better than the vanilla A2C or a random policy, achieving a score as low as zero.
The variants with contingent region information worked best and comparable to each other.
We observe that using the cumulative reward (total score) for exploration
gives a slight improvement on some environments.
These results support the effectiveness of the learned contingency-awareness information in count-based exploration.







\begin{figure*}[t]
\begin{center}
\includegraphics[width=\linewidth]{figures/ablation-cr-xyc-pred.pdf}
\vspace*{-0.5cm}
    \caption{
        Learning curves for the ablation study of state representation.
The exploration algorithm without the contingent region information (purple)
        performs significantly worse, yielding almost no improvement on hard-exploration games such as
        \MontezumaRevenge, \Venture, and \Frostbite.
        {The mean curve is obtained by averaging over 3 random seeds.}
        See Table \ref{tbl:a2c_additional_performance} for numbers.
    }
    \label{fig:a2c_exp_additional_1}
    \vspace*{-5pt}
\end{center}
\end{figure*}






\renewcommand{\r}{\color{red}}
\begin{table}[t]
    \tabcolsep=0.13cm
    \centering
\small
    \hspace*{-0.4cm}
    \begin{center}
    \begin{tabular}{@{}l|cccccccc@{}}
        \hline
    \iffalse................................\fi Method & \s Freeway & \s Frostbite & \s Hero  & \s Montezuma & \s PrivateEye & \s Qbert & \s Seaquest & \s Venture \\
        \hline
        A2C                                            &     7.2    & 1099         & 34352    & 12.5         & 574           & 19620    & 2401        & 0          \\
A2C+\coex{} ()                              &    10.7    & 1313         & 34269    & 14.7         &    2692       & 20942    & 1810        & 94         \\
        A2C+\coex{} ()                            & \B 34.0    & 941          & 34046    & 9.2          & \B 5458       & 21587    & 2056        & 77         \\
        A2C+\coex{} ()                          &    33.7    & \B 5066      &\B 36934  &    6558      &    5377       &    21130 &   1978      & \B 1374    \\
        A2C+\coex{} ()                        & \B 34.0    &    4260      &   36827  & \B 6635      &    5316       & \B 23962 &\B 5169      &    204     \\
        \hline
\end{tabular}
    \end{center}


    \vspace*{-5pt}
    \caption{
Summary of the results of the ablation study of the state representation.  We report the maximum mean score (averaged over 40 recent episodes) achieved over 100M environment steps,
        {averaged over 3 random seeds.}
    }
    \label{tbl:a2c_additional_performance}
    \vspace*{-5pt}
\end{table}







 

\end{document}
