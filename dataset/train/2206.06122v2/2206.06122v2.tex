\documentclass{article}
\PassOptionsToPackage{numbers, compress}{natbib}

\usepackage[preprint]{neurips_2022}

\usepackage{arydshln}
\usepackage{multirow}
\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         \usepackage{color}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{wrapfig}
\usepackage{enumitem}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{listings}
\usepackage[toc,title,page]{appendix}

\title{Singular Value Fine-tuning: Few-shot Segmentation requires Few-parameters Fine-tuning}

\author{
Yanpeng Sun\thanks{Equal Contribution.} , Qiang Chen, Xiangyu He, Jian Wang, Haocheng Feng \\ \textbf{Junyu Han, Errui Ding, Jian Cheng,  Zechao Li\thanks{Corresponding author.} , Jingdong Wang}\\
School of Computer Science and Engineering, Nanjing University of Science and Technology\\
Baidu VIS\\
NLPR, Institute of Automation, Chinese Academy of Sciences\\
}









\begin{document}


\maketitle

\begin{abstract}
\vspace{-.5em}


 



Freezing the pre-trained backbone has become a standard paradigm to avoid overfitting in few-shot segmentation. In this paper, we rethink the paradigm and explore a new regime: {\em fine-tuning a small part of parameters in the backbone}. We present a solution to overcome the overfitting problem, leading to better model generalization on learning novel classes. Our method decomposes backbone parameters into three successive matrices via the Singular Value Decomposition (SVD), then {\em only fine-tunes the singular values} and keeps others frozen. The above design allows the model to adjust feature representations on novel classes while maintaining semantic clues within the pre-trained backbone. We evaluate our {\em Singular Value Fine-tuning (SVF)} approach on various few-shot segmentation methods with different backbones. We achieve state-of-the-art results on both Pascal-5 and COCO-20 across 1-shot and 5-shot settings. Hopefully, this simple baseline will encourage researchers to rethink the role of backbone fine-tuning in few-shot settings. The source code and models will be available at \url{https://github.com/syp2ysy/SVF}.

 
 \vspace{-.9em} 
\end{abstract} \section{Introduction} \label{sec1}
\vspace{-.4em}
Benefiting from the large amounts of annotated data, deep learning has achieved noticeable improvements in the field of semantic segmentation~\cite{yuan2020object,li2021ctnet,TANG2022108792}. In contrast, their performances dramatically degrade when novel classes arrive or label data is insufficient. Thus, few-shot segmentation (FSS)~\cite{lu2021simpler,zhang2020sg} was proposed to address these challengs. In FSS, one needs to segment novel class objects in query images given only a few densely-annotated samples (\textit{i.e.}, support images and support masks).\footnote{In this paper, we consider two settings of few-shot segmentation: 1-shot and 5-shot, which contain only one and five support images and masks, respectively.} Due to the extremely limited data in FSS, over-fitting has become a critical problem that needs to be carefully handled. 

One feasible solution is to restrict the model's learning capacity so that it can not overfit the small dataset. Most recent works~\cite{yang2021mining, zhang2021few, shen2021partial,TangLPT20} follow this idea by freezing the pre-trained backbone. Then, different feature fusion methods and prototypes are introduced to enhance the generalization ability. Although this paradigm has achieved promising results, it is still suboptimal to directly adopt parameters pre-trained on image classification to image segmentation. The semantic clues contained in the pre-trained backbone can be irrelated to objects shown in support images, bringing unexpected obstacles to segmenting novel class objects in FSS.

In this paper, we rethink the paradigm of freezing the pre-trained backbone and show that fine-tuning {\em a small part of parameters in the backbone} is free from overfitting, leading to better model generalization in learning novel classes. Our method is illustrated in Figure~\ref{fig:structure}(b). First, to find such a small part of parameters for fine-tuning, we decompose pre-trained parameters into three successive matrices via the Singular Value Decomposition (SVD). Second, we then {\em fine-tune the singular value matrices} and keep others frozen. The above design, called {\em Singular Value Fine-tuning (SVF)}, follows two principles: (i) maintaining rich semantic clues in the pre-trained backbone and (ii) adjusting feature map representations when learning to segment novel classes.

\begin{figure*}
	\centering
	\includegraphics[width=\linewidth]{figure/structure.pdf}
	
	\caption{Previous paradigm \textit{vs.} SVF. (a) Previous paradigm introduces different segmentation heads based on the frozen pre-trained backbone. (b) SVF uses SVD to decompose the pre-trained parameters into three consecutive matrices, then only fine-tune the singular values and keep others frozen. Compared to the previous paradigm, SVF shows that fine-tuning a small part of parameters in the backbone is invulnerable to over-fitting, leading to better model generalization in learning novel classes.}
	\label{fig:structure}
	\vspace{-0.7cm}
\end{figure*}


We evaluate our SVF on two few-shot segmentation benchmarks, Pascal-5 and COCO-20. Extensive experiments show that SVF is invulnerable to overfitting and works well with various FSS methods using different backbones. It is significantly better than the freezing backbone counterpart, leading to new state-of-the-art results on both Pascal-5 and COCO-20. Moreover, we provide quantitative and qualitative analyses on how singular values change during fine-tuning. Results show that SVF helps models focus more on the objects to be segmented instead of the noisy background. Our experiments highlight that proper backbone fine-tuning consistently outperforms backbone freezing on several leading methods. We hope our simple method will encourage researchers to rethink the role of backbone fine-tuning for few-shot segmentation.

\vspace{-1.2em}
 \section{Related Work}
\vspace{-.9em}
\subsection{Few-shot Segmentation}
\vspace{-.5em}
The purpose of few-shot segmentation is to segment the unseen class in query image with a few densely-annotated samples. In this task, a semantically rich representation and a nice matching approach have a particularly large impact on the results. Therefore, mainstream methods~\cite{yang2021mining, xie2021scale,snell2017prototypical,dong2018few} focus on obtaining excellent prototypes from support images, and obtaining accurate segmentation results by improving the quality of prototype features. CANet~\cite{zhang2019canet}, PFENet~\cite{tian2020prior} and PANet~\cite{wang2019panet} filters class irrelevant information by global average pooling to obtain foreground and background prototypes. ASGNet~\cite{li2021adaptive} pointed out that increasing the number of prototypes can further improve the segmentation results. CyCTR~\cite{zhang2021few} believes that pixel-level features in support image are important for segmentation tasks, and proposes to use pixel-level prototypes to predict query images. On the other hand, some methods focus on designing better matching methods to improve segmentation performance. SG-One~\cite{zhang2020sg} uses cosine similarity to match prototype and query feature for segmentation results. CANet~\cite{zhang2019canet} proposes an additive alignment module to iteratively refine the network output. HSNet~\cite{hsnet} exploits neighborhood consensus to disambiguate semantics by analyzing patterns of local neighborhoods in matching tensors. In addition to the above work, BAM~\cite{lang2022learning} utilizes the segmentation results of the base class to guide the generation of unseen classes, and achieves SOTA results. However, the above methods are all based on backbone freeze, and freezing backbone not only reduces the representational ability of the model, but also does not fit distribution to data better. Unlike previous work, in this paper we focus on the prospect of fine-tuning backbone in FSS. Therefore, instead of proposing a new model, we adopt the classic PFENet~\cite{tian2020prior} and BAM~\cite{lang2022learning} as our baselines. Our SVF enables these methods to further improve segmentation results.
\vspace{-.6em}


\subsection{Backbone Fine-tuning}
\vspace{-.4em}
Fine-tuning backbone in downstream tasks has become a common approach in deep learning. The initial breakthroughs in vision tasks~\cite{he2017mask,huang2019ccnet,lin2017focal} were achieved by fine-tuning classification networks based ImageNet pre-trained weight, such as R-CNN~\cite{girshick2015fast} for detection and FCN~\cite{long2015fully} for segmentation. However, the direct application of fine-tuing the entire backbone in few-shot scenarios will lead to over-fitting of the model~\cite{shen2021partial}. Therefore, fine-tuning part parameters of the backbone in few-shot learning may avoid model over-fitting. P-Transfer~\cite{shen2021partial} utilizes NAS to search parameters of backbone that require fine-tune in few-shot classification tasks. However, this method is very complicated and cannot be directly applied to small sample segmentation. And some works~\cite{jia2022visual} borrow the idea of prompt~\cite{tsimpoukelli2021multimodal,shin2020autoprompt,wei2021pretrained} in NLP to fine-tuning part parameters in the visual transformer~\cite{dosovitskiy2020image}. The above methods are proposed in a transformer-based model, but most few-shot segmentation models use CNN-based backbones. Applying prompt-based methods to various few-shot segmentation methods may need further adjustments. Different from the above methods, our SVF borrows the commonly used SVD~\cite{andrews1976singular} in model compression and constructs~\cite{denton2014exploiting, zhuang2018discrimination} a novel part fine-tune method for few-shot segmentation task. In addition, some approaches~\cite{rusu2018meta,requeima2019fast,perez2018film} introduce highly constrained subset of parameters to fine-tuning. However, these methods are not applied on few-shot segmentation task.
\vspace{-.5em}

 


 \section{From Freezing Backbone to Singular Value Fine-tuning}
\vspace{-.4em}
\begin{figure}
	\centering
	\includegraphics[width=1.0\linewidth]{figure/pfenet_overfitting.pdf}
	\vspace{-0.6cm}
	\caption{The mIoU curve of PFENet~\cite{tian2020prior} with different fine-tune strategies on Pascal-5 Fold-0. (a) is the result of freezing backbone, (b) and (c) represent the result of fine-tuning the entire backbone and SVF, respectively. Compared with the direct fine-tuning (b), SVF not only avoids the overfitting problem, but also brings positive results.}
	\label{fig:overfitting}
	
	\vspace{-1.4em}
\end{figure}
In this section, we start with the preliminaries on the few-shot segmentation (FSS) setting. Then, we revisit the overfitting problem in FSS when fine-tuning the backbone in Section~\ref{sec3.1}. In Section~\ref{sec3.2}, we propose a novel {\em Singular Value Fine-tuning (SVF)} method for FSS instead of freezing the pre-trained backbone as proposed in previous methods. Section~\ref{sec3.3} provides a discussion on the differences between SVF and other fine-tuning methods.

\vspace{-1.0em}
\paragraph{FSS Setup.} Few-shot segmentation (FSS) aims to segment novel class objects given only a few densely-annotated samples. In this task, datasets are split into the training set () with base classes () and the testing set () with novel classes (), where . Following previous works, we adopt episode training. Each episode consists of  support images and one query image to construct a -shot segmentation task ( or  in this paper). Then, FSS methods are trained with episodes to segment novel class objects in the query image given the knowledge of  support images and support masks.
\vspace{-.7em}
\subsection{Revisiting Model Overfitting in FSS} \label{sec3.1}
\vspace{-.5em}
As presented in Section~\ref{sec1}, model overfitting is a critical problem in extremely limited data scenarios (1-shot and 5-shot), especially when the model has large amounts of learnable parameters. We validate this problem and design experiments with a typical FSS method, PFENet~\cite{tian2020prior}. Figure~\ref{fig:overfitting} shows that as model training moves on, fine-tuning backbone leads to better performance 
on the training dataset while it does not improve results on the validation set. It is a typical overfitting phenomenon. In contrast, freezing backbone can achieve steady improvements on the validation set during training.Therefore, existing methods in FSS turn to freezing the pre-trained backbone to avoid the overfitting problem. 

Although this strategy has achieved promising results, it is clear that directly adopting an ImageNet pre-trained backbone to image segmentation can be suboptimal. One need to extract the most related semantic clues within the backbone instead of involving too much noise coming from the irrelevant categories learned from upstream tasks.
In light of this, we rethink the paradigm of freezing the pre-trained backbone and try to find a new solution to the overfitting problem.
\vspace{-.5em}
\subsection{Singular Value Fine-tuning} \label{sec3.2}
\vspace{-.4em}
According to the analysis above, fine-tuning all parameters in the backbone can be unsatisfactory. One feasible solution is to restrict the backbone's learning capacity so that it is suitable for the few-shot circumstance. Instead of freezing the whole pre-trained backbone as in previous works, we consider exploring a new regime, which is {\em only fine-tuning a small part of the parameter in the backbone}.



\begin{wrapfigure}{r}{7.5cm}
\vspace{-0.7cm}
\centering
\begin{minipage}{1.0\linewidth}

  \begin{algorithm}[H]\small
\caption{Pseudocode of SVF in Python style}
\label{alg:SVF}
\definecolor{codeblue}{rgb}{0.25,0.5,0.5}
\definecolor{codekw}{rgb}{0.85, 0.18, 0.50}
\lstset{
  backgroundcolor=\color{white},
  basicstyle=\fontsize{7.5pt}{7.5pt}\ttfamily\selectfont,
  columns=fullflexible,
  breaklines=true,
  captionpos=b,
  commentstyle=\fontsize{7.5pt}{7.5pt}\color{codeblue},
  keywordstyle=\fontsize{7.5pt}{7.5pt}\color{codekw},
}
\begin{lstlisting}[language=python]
# Input: Conv2d with weight matrix W, Input feature X
# Output: Output feature Y

# Previous 3x3 Conv : 
# Y = F.Conv2d(W, X, kernel=(3,3))

# SVF : 
U, S, V = svd(W)  # decompose weights by SVD

U.requires_grad = False  # freeze Conv_U
V.requires_grad = False  # freeze Conv_V

Y = F.Conv2d(V, X, kernel=(3,3))  # a new 3x3 conv
Y = Y.mul(S)  # reconstruct a new affine layer
Y = F.Conv2d(U, Y, kernel=(1,1))  # a new 1x1 conv
\end{lstlisting}
\end{algorithm}
\end{minipage}
\vspace{-0.4cm}
\end{wrapfigure}

However, it is nontrivial to find such a small part of parameters to be fine-tuned in the backbone. Simply splitting the backbone's parameters into learnable and freezing ones results in negative results. Table~\ref{tab:layer_finetune} and Table~\ref{tab:conv_finetune} show the inferior performances no matter splitted by layers or convolution types. We attribute this to the adjustment of the backbone, making the model {\em biased towards base class objects} shown in the training set, yet leads to worse model generalization in segmenting novel classes. Figure~\ref{fig:overfitting_comparison} also provides evidences of the overfitting problem. Given that the backbone is pre-trained on a large-scale dataset with a classification task, it contains rich semantic clues but it is suboptimal to adopt for a segmentation task directly. We deliver two principles for finding a small part of fine-tuning parameters in the backbone: (i) maintaining rich semantic clues in the pre-trained backbone and (ii) adjusting feature map representations when learning to segment novel classes.

To fulfill the above goal, we resort to model compression methods in this paper. They are designed to approximate the original pre-trained model with fewer parameters, and also follow the above two principles. Among these methods, low-rank decomposition is a common technique to achieve model compression. It first splits the model weights into multiple subspaces and then compresses each subspace by shrinking its rank. We follow this direction and decompose the backbone parameters into subspaces via the Singular Value Decomposition (SVD). However, we do not shrink subspaces' ranks since our target is to find a small part of parameters to be fine-tuned instead of model compression.

In detail, for a convolution layer with  input channels,  output channels, and a kernel size of  in the pre-trained backbone, we first fold its weight tensor  into a matrix , then decompose the obtained matrix by applying SVD with full-rank in subspaces (rank ). Thus,

The obtained pair of matrices  and  construct two new convolution layers, and  is a diagonal matrix with singular values on the diagonal. Then back to convolutions, the results of the Equation~\ref{eq:1} corresponds to three successive layers: a  convolution layer, a scaling layer, and followed by a  convolution layer (Algorithm~\ref{alg:SVF} further provides a pseudo-code for SVF). We thus split each convolution layer in the pre-trained backbone into three functionalities: (i) decouple the semantic clues into a subspace with rank , (ii) re-weight the semantic clues with singular values for the given task, and (iii) project the re-weighted clues back to the original space. Based on the interpretation above, we propose to fine-tune the scaling layer, which is Singular Value Fine-tuning (SVF). SVF does not erase the semantic clues contained in the pre-trained backbone, but it re-weights the representations to help adjust the model for new segmentation tasks. As SVF restricts the learnable parameters to only singular values, which are extremely few () compared with the parameters in the whole backbone, SVF is less vulnerable to overfitting and shows better generalization capacity in learning novel classes (shown in Table~\ref{tab:layer_finetune}, Table~\ref{tab:conv_finetune}, Figure~\ref{fig:overfitting}, and Figure~\ref{fig:overfitting_comparison}).


\vspace{-.4em}
\subsection{Discussion on Fine-tuning Methods} \label{sec3.3}
\vspace{-.2em}
Fine-tuning pre-trained backbones is a promising way to achieve state-of-the-art results on downstream vision tasks. Many fine-tuning methods have been introduced to transfer pre-trained backbone's knowledge, such as full-model fine-tuning~\cite{he2017mask, zhang2020feature}, task-specific fine-tuning (freezing the backbone)\cite{he2020momentum,xie2021few}, residual adapter~\cite{houlsby2019parameter,bapna2019simple,wang2021k}, and bias tuning~\cite{zhang2020revisiting, guo2019spottune}. We compare our SVF with these methods. As presented in previous sections, methods like full-model fine-tuning may not be suitable with extremely limited data, and task-specific fine-tuning can not provide adjustment to the representations in the backbone for downstream tasks. Moreover, methods like residual adapter and bias tuning need prior knowledge to model structure or weights. SVF does not have the above limitations in the few-shot segmentation task.

Recently, a new fine-tuning method named Vision Prompt Tuning (VPT)~\cite{jia2022visual} has been proposed to fine-tune vision transformers. It introduces a small number of trainable parameters in the input space while keeping the backbone frozen. From this perspective, our SVF also introduces a small number of trainable parameters but in the singular value space. In SVF, the learned singular value diagonal matrix  can be formulated as a product of a frozen matrix  and a trainable matrix , which is . We give a detailed explanation of this perspective in the Appendix.
\vspace{-.8em}



 
\section{Experiments}
\vspace{-0.9em}
We conduct experiments on Pascal-5 \cite{shaban2017one} and COCO-20 \cite{nguyen2019feature} to discuss fine-tuning approach in FSS. In this section, we first introduce the used representative method and implementation details. Then we discuss the impact of different fine-tune methods on the FSS model, and finally verify the effectiveness and versatility of the proposed fine-tune method.

\begin{table}[]\scriptsize
\centering
\setlength\tabcolsep{4pt}
\renewcommand\arraystretch{1.3}
\caption{Performance on Pascal-5\cite{shaban2017one} in terms of mIoU for 1-shot and 5-shot segmentation. The best mean results are show in \textbf{bold}.  indicates that images from training set containing the novel class on test set were removed.}
\vspace{-0.6em}
\label{tab:final-voc}
\begin{tabular}{l|c|ccccc|ccccc}
\bottomrule
\multirow{2}{*}{Method} & \multirow{2}{*}{backbone} & \multicolumn{5}{c|}{1-shot}                                                                                                                    & \multicolumn{5}{c}{5-shot}                                                                                       \\ \cline{3-12}
                        &                           & Fold-0 & Fold-1 & Fold-2 & Fold-3 & Mean  & Fold-0               & Fold-1               & Fold-2               & Fold-3               & Mean                 \\ \hline
baseline &\multirow{6}{*}{VGG16}      & 57.48                      & 66.72                      & 62.66                      & 53.72                      & 60.15                      & 62.98                     & 70.57                     & 68.62                     & 59.60                     &  65.44                    \\
baseline+SVF & & 63.07 & 68.40 & 65.81 & 54.28 & \textbf{62.89 } & 68.52 & 72.15 & 69.08 & 63.59 & \textbf{68.34 }                    \\ \cdashline{1-1} \cdashline{3-12} 
PFENet~\cite{tian2020prior} & & 61.91  & 70.34  & 63.77  & 57.38  & 63.35 & 67.73 & 72.82 & 69.31 & 67.59 &  69.36 \\
PFENet+SVF & & 63.43  & 71.40  & 64.18  & 58.30  & \textbf{64.33} & 69.11                     & 73.67 & 69.13  & 67.30 & \textbf{69.80 }  \\ \cdashline{1-1} \cdashline{3-12} 
BAM~\cite{lang2022learning} & & 63.18  & 70.77  & 66.14  & 57.53  & 64.41 & 67.36  & 73.05 & 70.61  & 64.00 & 68.76                     \\
BAM+SVF & & 64.09  & 71.07  & 66.79     & 57.54 & \textbf{64.87 } & 67.75 & 74.11 & 70.99    & 63.57 & \textbf{69.11 } \\ \hline \hline
baseline  & \multirow{12}{*}{ResNet50} & 65.60  & 70.28 & 64.12  & 60.27  & 65.07 & 69.89 & 74.16 & 67.87 & 65.73 & 69.41  \\
baseline+SVF & & 67.42 & 71.57 & 67.99 & 61.57 & \textbf{67.14} & 70.37  & 75.06 & 71.08 & 69.16  &
\textbf{71.42}  \\
baseline                &                            & 66.36  & 69.22  & 57.64  & 58.73  & 62.99 & 70.75  & 72.92  & 58.86  & 65.56  & 67.02 \\
baseline + SVF          &                            & 66.88  & 70.84  & 62.33  & 60.63  & \textbf{65.17} & 71.49  & 74.04  & 59.38  & 67.43  & \textbf{68.09} \\ \cdashline{1-1} \cdashline{3-12} 
PFENet~\cite{tian2020prior} & & 66.61 & 72.55 & 65.33 & 60.91 & 66.35 & 70.93 & 75.32 & 69.60 &68.96  & 71.20  \\
PFENet+SVF & & 69.27 & 73.55 & 67.49 & 62.30 & \textbf{68.15 } & 71.82  & 74.92 & 70.97  & 69.58  & \textbf{71.82}  \\
PFENet& &67.06 &71.61 &55.21 & 59.46 & 63.34 & 72.11 &73.67 &61.61 & 67.50  &68.72\\
PFENet + SVF& &68.31 &71.99 &56.25 &61.82 &\textbf{64.59} &72.09 & 73.99 & 63.58 & 70.03 &\textbf{69.92}\\\cdashline{1-1} \cdashline{3-12} 
BAM~\cite{lang2022learning}&   & 68.97 & 73.59  & 67.55 & 61.13  & 67.81 & 70.59 & 75.05 & 70.79 & 67.20 & 70.91 \\
BAM+SVF  & & 69.38 & 74.51 & 68.80 & 63.09 & \textbf{68.95} & 72.05  & 76.17  & 71.97  & 68.91 & \textbf{72.28}  \\
BAM& &68.37 &72.05 &57.55 &60.38 &64.59 &70.72 &74.21 &63.58 &66.18 &68.67\\
BAM + SVF& &68.17 &72.86 &57.77 &62.04 &\textbf{65.21} &72.30& 74.43 &65.16 &69.43 &\textbf{70.33}\\\bottomrule 
\end{tabular}
\vspace{-0.7cm}
\end{table}


\vspace{-1.2em}
\subsection{Setting}
\vspace{-0.7em}
\textbf{Datasets.} Experiments are conducted on Pascal-5\cite{shaban2017one} and COCO-20  \cite{nguyen2019feature}. Following the previous work~\cite{hsnet, tian2020prior,shaban2017one}, we separate all classes in both datasets into 4 folds. For each fold, Pascal-5\cite{shaban2017one} has 15 classes used for training and 5 classes for test, COCO-20 \cite{nguyen2019feature} has 60 classes used for training and 20 classes for test. To verify the performance of the model, we randomly sample 1000 query-support pairs in each fold. Following the BAM \cite{lang2022learning}, we remove images from training set containing novel classes of test to prevent potential information leakage. We give a detailed explanation of this setting about train sets in the Appendix.

\textbf{Dataset Tricks:} The previously methods annotated novel classes in the training set as background during training step. It become a common paradigm in few-shot segmentation. However, based on BAM~\cite{lang2022learning}, we found a novel dataset trick to improve the performance of FSS models. It simply removes images from the training set that contain the novel classes. For fair comparison with BAM, we use this trick in our experiments. However, we know that previously methods does not use this trick. Therefore, we present the experimental results with and without dataset trick in Table~\ref{tab:final-voc}, and more detailed fair comparison results in Appendix. Here, we hope that researchers can make fair comparison under the same setting.

\begin{table}[] \scriptsize
\centering
\setlength\tabcolsep{4pt}
\renewcommand\arraystretch{1.3}
\caption{Performance on COCO-20 \cite{nguyen2019feature} in terms of mIoU for 1-shot and 5-shot segmentation. The best mean results are show in \textbf{bold}.  indicates that images from training set containing the novel class on test set were removed.}
\vspace{-1.0em}
\label{tab:final-coco}
\begin{tabular}{l|c|ccccc|ccccc}
\bottomrule
\multirow{2}{*}{Method} & \multirow{2}{*}{Backbone}  & \multicolumn{5}{c|}{1-shot}  & \multicolumn{5}{c}{5-shot} \\ \cline{3-12} 
    &  & Fold-0  & Fold-1 & Fold-2 & Fold-3  & Mean  & Fold-0 & Fold-1 & Fold-2 & Fold-3 & Mean  \\ \hline
baseline & \multirow{7}{*}{VGG-16} & 37.56 & 37.73 & 38.78 & 35.66 & 37.43 & 43.07 & 49.42 & 44.38 & 46.38 & 45.81      \\
baseline+SVF & & 39.32 & 39.64 & 38.63 & 35.45 &  \textbf{38.26} & 46.48 & 50.72 & 45.79 & 45.63 & \textbf{47.16}      \\ \cdashline{1-1} \cdashline{3-12}
PFENet~\cite{tian2020prior}  &   & 35.40  & 38.10 & 36.80 & 34.70  & 36.30  & 38.20  & 42.50  & 41.80  & 38.90  & 40.40 \\
PFENet~\cite{tian2020prior}  & & 41.03 & 44.22 & 43.74 & 38.90 & 41.97 &  48.66   & 48.26 & 45.49 & 51.02 & 48.36      \\
PFENet+SVF & & 42.68 & 44.90 & 42.60 & 38.79 & \textbf{42.24 } & 49.02 & 53.71 & 47.59       & 47.63 & \textbf{49.49}     \\\cdashline{1-1} \cdashline{3-12}
BAM~\cite{lang2022learning} & & 38.96 & 47.04 & 46.41 & 41.57 & 43.50 & 47.02  & 52.62  & 48.59  & 49.11  & \textbf{49.34} \\
BAM+SVF & & 40.21 & 46.62 & 46.23 & 41.97 & \textbf{43.76 } & 45.05 & 53.59 &48.35 &49.28 & 49.07      \\ \hline\hline
baseline &\multirow{6}{*}{ResNet-50}    & 38.91 & 46.07 & 42.67 & 39.71 & 41.84 & 50.35 & 56.78 & 49.61 & 50.96 & 51.93      \\
baseline+SVF & & 44.22   & 46.38 & 42.65 & 41.65 & \textbf{43.72} & 51.47       & 57.48       & 50.33       & 52.29       & \textbf{52.89}      \\ \cdashline{1-1} \cdashline{3-12} 
PFENet~\cite{tian2020prior} &  &   44.93 &  50.32 & 44.68 & 44.26 & 46.05 & 52.29 & 59.34 & 51.50 & 53.53 & 54.17      \\
PFENet+SVF & & 46.88 & 50.86 & 47.69 & 46.64 & \textbf{48.02} & 52.72       & 58.14       & 52.52       & 54.15       & \textbf{54.38}      \\ \cdashline{1-1} \cdashline{3-12} 
BAM~\cite{lang2022learning} & & 43.41 & 50.59 & 47.49 & 43.42 & 46.23 & 49.26  & 54.20  & 51.63  & 49.55  & 51.16 \\
BAM+SVF & & 46.87 & 53.80 & 48.43 & 44.78 & \textbf{48.47 } & 52.25        & 57.83       & 51.97        & 53.41       & \textbf{53.87}      \\ \bottomrule
\end{tabular}
\vspace{-1.9em}
\end{table}


\begin{minipage}{\textwidth}
\vspace{-1.0em}
\begin{minipage}{0.48\textwidth}\scriptsize
\makeatletter\def\@captype{table}\makeatother
\setlength\tabcolsep{4pt}
\renewcommand\arraystretch{1.3}
\centering
\caption{Performance on Pascal-5\cite{shaban2017one} in terms of FB-IoU for 1-shot and 5-shot segmentation.}
\vspace{+.4em}
\label{tab:fbiou}
\begin{tabular}{c|c|cc}
\hline
\multirow{2}{*}{Method} & \multirow{2}{*}{backbone}  & \multicolumn{2}{c}{FB-IoU (\%)}      \\ \cline{3-4} 
                        &                            & \multicolumn{1}{c|}{1-shot} & 5-shot \\ \hline
baseline                & \multirow{6}{*}{ResNet-50} & \multicolumn{1}{c|}{77.11}  & 80.56  \\
baseline+SVF            &                            & \multicolumn{1}{c|}{\textbf{78.86}}  & \textbf{82.66}  \\ \cdashline{1-1} \cdashline{3-4}
PFENet                 &                            & \multicolumn{1}{c|}{77.35}  & 82.30  \\
PFENet+SVF             &                            & \multicolumn{1}{c|}{\textbf{79.07}}  & \textbf{82.77}  \\ \cdashline{1-1} \cdashline{3-4}
BAM                     &                            & \multicolumn{1}{c|}{\textbf{81.10}}  & 82.18  \\
BAM+SVF                &                            & \multicolumn{1}{c|}{80.13}  & \textbf{83.17}  \\ \hline
\end{tabular}
\end{minipage}\hspace{2em}
\begin{minipage}{0.48\textwidth}
\centering
\setlength\tabcolsep{4pt}
\makeatletter\def\@captype{table}\makeatother
\caption{Ablation study of BN on Pascal-5 under 1-shot setting. \checkmark represents fine-tuning this feature space. The best mean results are show in \textbf{bold}.}
\vspace{+.4em}
\label{tab:BN}
\begin{tabular}{c|cl|c}
\hline
Method                    & BN     & S &  Mean  \\ \hline
& - & - &  65.07 \\ \cline{2-4}
\multirow{2}{*}{baseline} & \checkmark & & 63.12 \\
                          & \checkmark      & \checkmark     &  64.20 \\
                          &        & \checkmark     &  \textbf{67.14} \\ \hline
\end{tabular}
\end{minipage}

\end{minipage}
\vspace{+.3em}

\textbf{Methods.} To quickly verify the effectiveness of SVF, we propose a simple baseline method. Then, two representative methods are used to verify the generality of SVF. Next, we briefly introduce these three methods. Meanwhile, for fair comparison, we unify the three methods into the same framework.

\begin{itemize}[itemsep=2pt,topsep=0pt,parsep=0pt]
\item \textbf{Baseline}: We replace FEM module on PFENet~\cite{tian2020prior} with ASPP module to get the baseline method. The baseline only sets main loss, without adding auxiliary loss.
\item \textbf{PFENet ~\cite{tian2020prior}}: As a classic method in FSS, it proposed the Prior Guided Feature Enrichment Network, which has a huge impact on the subsequent FSS methods.
\item \textbf{BAM ~\cite{lang2022learning}}: As the state-of-the-art method in FSS, it represents the most cutting-edge results in FSS.
\end{itemize}






\textbf{Implementation details.} In our experiments, we use VGG-16~\cite{vgg} and ResNet-50~\cite{he2016deep} as the backbone network and initialize it with ImageNet~\cite{russakovsky2015imagenet} pre-trained weights. Due to the particularity of BAM, we use the initialization weights provided by author, and SVF does not finetuning base branch in BAM. We use SGD optimizer with cosine Learning rate decay~\cite{loshchilov2016sgdr}, the learning rate  and the random seed  when fine-tuning backbone. We keep original settings for the model without fine-tuning. All models are trained 200 epochs on Pascal-5 with batch size 8 and trained 50 epochs on COCO-20 with batch size 8. Image is resized to  on Pascal-5 and  on COCO-20. Following~\cite{liu2021few,xie2021few,yang2020prototype} , we adopt the mean intersection over union (mIoU) and foreground-background IoU (FB-IoU) as our evaluation metric. Since the middle-level features and 
high-level features are used in all FSS model, we set the SVF to fine-tuning the parameters of layers 2, 3, and 4 only. All model runs on four NVIDIA A100 GPUs.
\vspace{-.5em}

\subsection{Comparison with State-of-the-Art}
\vspace{-.5em}
In this section, the effectiveness of SVF is validated on three most representative methods. For fair comparison, we rerun all methods with unified framework. Then, we compare different methods with singular value fine-tuning and freeze backbone. The experimental results on Pascal-5 are in Table \ref{tab:final-voc}. It can be seen that the performance of different methods has been significantly improved after SVF. When the backbone is VGG-16, the SOTA method BAM improves miou by  and  in 1-shot \\

\begin{minipage}{\textwidth}
\vspace{-0.5cm}
\begin{minipage}{0.4\textwidth}\scriptsize
\makeatletter\def\@captype{table}\makeatother
\centering
\setlength\tabcolsep{4pt}
\renewcommand\arraystretch{1.3}
\caption{Comparative experiment with fine-tuning different layer of backbone on Pascal-5.}
\vspace{+.2em}
\label{tab:layer_finetune}
\begin{tabular}{c|c|c}
\bottomrule
Method                          & layer      &  Mean  \\ \hline
baseline                 & -          &  65.07  \\ \hline
+fully fine-tune                 & 1, 2, 3, 4 &  60.90 \\ \hline
\multirow{3}{*}{+ part fine-tune} & 2, 3, 4 & 61.15 \\
                                & 3, 4       & 61.08      \\
                                & 4          & 60.58      \\ \hline
+SVF                            & 2, 3, 4    & \textbf{67.14} \\ \bottomrule
\end{tabular}
\end{minipage}\hspace{2em}
\begin{minipage}{0.5\textwidth}\small
\makeatletter\def\@captype{table}\makeatother
\centering
\setlength\tabcolsep{4pt}
\renewcommand\arraystretch{1.3}
\caption{Comparative experiment with fine-tuning different convolutional layer of backbone on Pascal-5.}
\vspace{+.4em}
\label{tab:conv_finetune}
\begin{tabular}{c|c|cc|c}
\hline
Method                            & layer   & 3  3 & 1  1 &  Mean  \\ \hline
baseline                          & -       & -          & -          &  65.07  \\ \hline
\multirow{3}{*}{+part fine-tune} & 2, 3, 4 & \checkmark           & \checkmark           & 61.15 \\
                                  & 2, 3, 4 &  \checkmark          &            &  61.86      \\
                                  & 2, 3, 4 &            & \checkmark            &  61.62      \\ \hline
+SVF                              & 2, 3, 4 & -          & -          &  67.14 \\ \hline
\end{tabular}
\end{minipage}
\end{minipage}


and 5-shot respectively after singular value fine-tuning. However, when the backbone is ResNet-50, SVF improves BAM by  and  mIoU on 1-shot and 5-shot, respectively. It shows that SVF bring better performance in deeper backbone.  Meanwhile, Table~\ref{tab:final-coco} shows the effectiveness of SVF on more complex dataset COCO-20. Exspecially, SVF improves the performance of BAM by  and  mIoU on 1-shot and 5-shot. Furthermore, Table~\ref{tab:fbiou} shows the comparison results with FB-IoU on Pascal-5. Our SVF can also improve the performance of model. This experiment proves that SVF not only achieve state-of-the-art results, but also is a general method in FSS. In addition, the results (without ) in Table \ref{tab:final-voc} prove that the dataset trick can indeed improve the performance of FSS model. It also shows that whether or not the dataset tricks is used does not affect the effectiveness of SVF. And we give more comparative results in the Appendix.


\begin{figure*}
    \setlength{\abovecaptionskip}{-.6em}
	\centering
	\includegraphics[width=1.0\linewidth]{figure/compare_over.pdf}
	\caption{The mIoU curve of baseline with different finetune strategies on Pascal-5 Fold-0. (a) represent the results of baseline with freezing backbone, (b) represent directly fine-tuning layers 2, 3 and 4, (c) represent fine-tuning all  convolution layer in backbone, and (d) represent the proposed method SVF.}
	\label{fig:overfitting_comparison}
	\vspace{-0.6em}
\end{figure*}


\vspace{-.4em}
\subsection{Ablation Study}
\vspace{-.4em}
To verify the effectiveness of SVF, we conduct a series of ablation study in this section. We use the baseline method to conduct ablation study on Pascal-5 1-shot setting with ResNet-50 as the backbone network. Furthermore, we give the ablation study about hyperparameter in the Appendix.
 
\textbf{Batch Normalization (BN):} In Table~\ref{tab:BN}, we test the effect of BN on SVF. In the case of only fine-tuning the BN layer, the baseline will greatly reduce the performance. Next, we test SVF (fine-tune subspace ) on the baseline without fine-tuning BN. The results show that SVF achieves the best performance. Finally, we test the performance of the baseline method when fine-tuning subspace  and BN simultaneously. The results show that fine-tuning parameters of BN layer can cause performance of baseline to degrade. Therefore, we freeze the parameters of BN layer when using SVF.



\textbf{Traditional fine-tune methods:} In this part, we conduct experiments to verify the impact of traditional fine-tuning methods on the FSS model. Traditional fine-tuning methods can be divided into fully fine-tune and part fine-tune. The fully fine-tune method means to fine-tuning all the parameters in the backbone. The part fine-tune methods means to fine-tuning part parameters in the backbone, which includes layer-based and convolution-based fine-tune methods. In table~\ref{tab:layer_finetune}, we conduct quantitative experiments with fully and layer-based fine-tune on baseline method. The results show that fully fine-tune brings negative results to baseline method. Meanwhile, we find that the negative results of fully fine-tuning method are mitigated as the number of fine-tuning layers is reduced. However, these methods do not have a positive impact on the baseline. In table~\ref{tab:conv_finetune}, we conduct quantitative experiments on convolution-based fine-tune methods. For fair comparison, we only fine-tuning convolutions of 2, 3 and 4 layers. The results show that only fine-tuning  \\



\begin{minipage}{\textwidth}
\begin{minipage}{0.4\textwidth}\scriptsize
\centering
\makeatletter\def\@captype{table}\makeatother
\caption{Ablation study of SVF fine-tuning different subspace on Pascal-5.}
\vspace{+.4em}
\label{tab:svf_finetune}
\begin{tabular}{c|c|c|c|c}
\hline
Method                    & \textbf{} & \textbf{} & \textbf{} & Mean \\ \hline
\multirow{7}{*}{baseline} & \checkmark        &       &          &   61.09     \\ 
     &          & \checkmark     &          &  \textbf{67.14}     \\
    &          &       & \checkmark        &  60.88      \\ \cline{2-5} 
    & \checkmark        & \checkmark     &          &  61.57     \\
     &          & \checkmark     & \checkmark        & 60.42     \\
     & \checkmark        &      & \checkmark        &  60.02     \\  
& \checkmark        & \checkmark     & \checkmark        & 61.24     \\ \hline 
\end{tabular}
\end{minipage}\hspace{2em}
\begin{minipage}{0.55\textwidth}
\centering
\makeatletter\def\@captype{table}\makeatother
\caption{Ablation study of SVF fine-tuning different layer on Pascal-5. The best results are show in \textbf{bold}.}
\vspace{+.4em}
\label{tab:svf_layer}
\begin{tabular}{c|c|c}
\hline
Method & layer & Mean  \\ \hline
\multirow{4}{*}{baseline + SVF} & 4 & 66.21 \\
                & 3, 4 & \textbf{67.20} \\
                & 2, 3, 4 & \underline{67.14} \\  
                & 1, 2, 3, 4  & 67.12 \\ \hline 
\end{tabular}
\end{minipage}

\end{minipage}


\begin{minipage}{\textwidth}
\begin{minipage}{0.5\textwidth}\scriptsize
\centering
\makeatletter\def\@captype{table}\makeatother
\caption{Ablation study of different ways of changing semantic cues in weights on Pascal-5 1-shot.}
\vspace{+.2em}
\label{tab:ursrv}
\begin{tabular}{c|c|c|c}
\hline
Method & Expression of weight& Fine-tune param & Mean  \\ \hline
\multirow{7}{*}{baseline} & W &- &  65.07 \\
&               S'W &S'	&	63.52 \\
&               WS' &S'	&	64.62 \\
&               RS'RW &S'	&	43.36 \\
&               USV &S	&	\textbf{67.14} \\
 &  URSRV & - &  29.31 \\
&              URSRV & S	&	30.26 \\ \hline 
\end{tabular}
\end{minipage}
\hspace{1.5em}
\begin{minipage}{0.45\textwidth}
\centering
\makeatletter\def\@captype{table}\makeatother
\caption{Compare with parameter-efficient tuning methods on Pascal-5 1-shot.}
\vspace{+.3em}
\label{tab:adapter}
\begin{tabular}{c|c|c}
\hline
Method & fine-tune method & Mean  \\ \hline
\multirow{5}{*}{baseline} & freeze backbone &  65.07 \\
	& fully fine-tune& 60.90\\			
    & SVF &	67.14 \\
    & Adapter & 	20.71 \\  
    & bias tuning  &	62.93 \\ \hline 
\end{tabular}
\end{minipage}
\end{minipage}


convolution or  convolution can further improve the performance of layer-based fine-tune methods. It show that traditional fine-tune methods cannot bring positive results. However, SVF brings positive results to the baseline method. The success of SVF proves that traditional fine-tune method destroys the rich semantic clues in the pre-trained backbone. In Figure~\ref{fig:overfitting_comparison}, we compare the mIoU curves of the training and test sets in different fine-tune methods. The results show that part fine-tune method also produces the over-fitting problem. It proves that disrupting the rich semantic cues in pre-trained backbone will lead to model over-fitting, reducing model generalization. However, SVF solves the over-fitting problem without destroying semantic clues in pre-trained weight. And, it brings a new perspective for fine-tuning backbone.

\textbf{Fine-tuning which subspace:} To verify the influence of different sub-spaces on SVF, we conduct experiments on the subspace after SVD decomposition. The results are shown in Table~\ref{tab:svf_finetune}. We find that only fine-tuning  subspace brings positive results. Either fine-tuning the  or  subspace returns negative results. It shows that  and  contain rich semantic information in pre-trained weight after SVF. In other words, directly changing the feature distribution of the  or  subspace reduces the generalization ability of the model. To verify the above point, we test the performance of fine-tuning different subspace combinations. The results confirm that changing the distribution of  or  spaces brings negative results. The subspace  represents the weight distribution of different semantic cues. Therefore, fine-tuning the subspace  does not change the semantic cues of pre-trained weights. Meanwhile, adjusting the weights of different semantic cues enables model to better perform downstream tasks.

\textbf{Fine-tuning which layers:} Since the FSS model directly uses feature maps of layers 2, 3, and 4, we initially fine-tuning the subspace  of layers 2, 3 and 4. However, this setting is unreasonable. To verify which layer  have a greater impact on baseline, we conducted experiments on SVF under different layer combinations. The results are shown in Table~\ref{tab:svf_layer}. It can be seen that fine-tuning layers 3 and 4 achieves the best performance, while only fine-tuning the layer 4 achieves the lowest performance. It shows that semantic clues in layer3 are the most important for FSS. Next we discuss the reasons why SVF can achieve better performance by visualizing semantic cues in layer3.

\textbf{Compare with other parameter-efficient tuning methods:} Unlike SVF, the purpose of parameter-efficient tuning methods is to obtain performance similar with fully fine-tune by fine-tuning a small number of parameters. To verify the superiority of SVF over parameter-efficient tuning methods in FSS, we compare SVF with adapter~\cite{houlsby2019parameter} and bias tuning on Pascal-5 with the 1-shot setting. The details for adapter and bias tuning are given below:

\begin{itemize}[itemsep=2pt,topsep=0pt,parsep=0pt]
\item\textbf{Adapter:} Adapter is proposed in transformer-based models. When applying it into CNN-based backbone (ResNet), we make simple adjustments. We follow \cite{houlsby2019parameter} to build the adapter structures and add them after the stages in the ResNet.

\item\textbf{Bias Tuning:} In the ResNet backbone, the convolution layers do not contain bias term. The bias terms that can be used for tuning is the ones in BN layers. We fine-tune the bias terms in all BN layers in this method.
\end{itemize}

The experimental results are given in the table~\ref{tab:adapter}. It shows that SVF outperform Adapter and Bias Tuning by large margins. Moreover, we find that the introduction of Adapter will directly lead to over-fitting, while Bias Tuning reduces performance of the baseline model.
\begin{figure*}
	\centering
	\setlength{\abovecaptionskip}{0.cm}
	\includegraphics[width=1.0\linewidth]{figure/vis_layer3_conv3x3.pdf}
	\vspace{-.4em}
	\caption{The visualization of segmentation cues with the largest variation in singular values from the last  convolution in layer 3. (a) represents segmentation clues of subspace  with the largest singular value reduction, (b) represents segmentation clues of subspace  with the largest singular value growth.}
	\label{fig:vis_3x3}
	\vspace{-0.4cm}
\end{figure*}

\begin{figure*}
	\centering
	\setlength{\abovecaptionskip}{0.cm}
	\includegraphics[width=1.0\linewidth]{figure/vis_layer3_conv1x1.pdf}
	\vspace{-.4em}
	\caption{The visualization of segmentation cues with the largest variation in singular values from the last  convolution in layer 3. (a) represents segmentation clues of subspace  with the largest singular value reduction, (b) represents segmentation clues of subspace  with the largest singular value growth.}
	
	\label{fig:vis_1x1}
	\vspace{-0.6cm}
\end{figure*}
\vspace{-.9em}



\begin{wrapfigure}{r}{7.5cm}
    \vspace{-0.3cm}
	\centering
	\setlength{\abovecaptionskip}{-0.3cm}
	\includegraphics[width=1.0\linewidth]{figure/rank_distribution.pdf}
	\caption{Statistics chart about the changes of initial Top-30 largest singular values of the last  and  convolution layer in layer3 after fine-tuning.}
	\label{fig:scale_change}
	\vspace{-0.4cm}
\end{wrapfigure}



\vspace{-.4em}
\subsection{Discussion on Why SVF Works}
\vspace{-0.5em}
The larger singular value in subspace , the more important semantic cues in subspace  and . We first focus on the changes of singular values during fine-tuning based on the initial distribution of . In Figure~\ref{fig:scale_change}, we visualize the variation of Top-30 singular values in pre-trained weights. It can be seen that the singular values of either  or  convolution change dramatically after fine-tuning. Next, we visualize the semantic cues of subspace  with the largest variation in singular values. The results are shown in Figure~\ref{fig:vis_3x3} and Figure~\ref{fig:vis_1x1}. We only visualize the semantic cues where the singular value grows and decreases the most for a better view. Notice that semantic cues of decreasing singular values tend to focus on background regions. The semantic cues of increasing singular values always focus on foreground regions. The background-focused semantic cues in pre-trained backbone will damage the performance of FSS model. Since the original distribution of semantic cues in pre-trained backbone is not suitable for downstream tasks, SVF brings positive results to FSS model by increasing the weight of foreground cues and reducing the weight of background cues. It is also important to keep the semantic cues unchanged during fine-tuning. Overall, dynamically adjusting the weight of each cue without changing the semantic representation is the key to the success of SVF. 

To verify that changes in the singular value space do not affect the semantic information in pre-trained weight, we conduct an interesting experiment to intentionally changing semantic cues in weights. In Table~\ref{tab:ursrv}, We compare different approaches, including introducing a small number of training parameters , and introducing a random rotation matrix . It can be seen that changing the semantic cues in the weights negatively affects the FSS model (with or without fine-tuning a small number of parameters). Experimental results demonstrate that fine-tuning the singular value space is non-destructive (without destroy semantic cues). We give a detailed analysis about {\em why SVF work} in the Appendix.
\vspace{-0.9em}

\subsection{Broader Impact}
\vspace{-0.5em}
In this paper, we prove that freeze backbone is not the only paradigm in few-shot segmentation, fine-tune backbone is feasible. Meanwhile, we explore a new mechanism to redistribute the weights of different semantic cues without changing the semantic cues. As a new perspective of few-shot segmentation, it exposes the influence of pre-trained backbone on few-shot segmentation. Moreover, this mechanism not only works on few-shot, but also may be effective when fine-tune very large pre-trained models. This greatly reduces the cost of fine-tuning large models on downstream tasks.
\vspace{-0.8em}
\subsection{Limitations}
\vspace{-0.5em}
Although the above experiments demonstrate the power of SVF, it still has some limitations. For instance, SVF introduces a small number of learning parameters, but the occupancy rate of memory resources is high during training process. Using SVF in ResNet-50 will occupy 16G video memory per image in COCO-20 5-shot setting. Furthermore, SVF increase a small amount of training time compared with freeze backbone.


 \section{Conclusion}
\vspace{-.7em}
In this paper, we rethink the paradigm of freezing backbone in FSS and propose a new paradigm Singular Value Fine-tuning (SVF) for fine-tuning backbone. Firstly, SVF decompose pre-trained parameters into three subspaces by SVD, and then only fine-tune the singular value. Our SVF dynamically adjusts the weights of different semantic cues without changing the rich semantic cues in pre-trained backbone. We evaluate the effectiveness of SVF on two commonly used benchmarks, Pascal-5 and COCO-20. Extensive experiments prove that SVF as a new perspective to avoid overfitting and significantly improve the performance of various FSS methods. As a new paradigm of finetune, we will extend it to a variety of vision tasks in the future.

\textbf{Acknowledge} This work was partially supported by the National Natural Science Foundation of China (Grant No. U20B2064 and U21B2043).
 





{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib.bib}
}





























\clearpage
\appendix
\vspace{-1.0em}
\noindent
{\Large {\textbf{Appendix}}}

\section{More details}
\textbf{Training Strategy:} Different from the training strategy of previous methods, we set the learning rate to 0.015 and use an SGD optimizer with cosine learning rate decay when fine-tuning the backbone. Therefore, we compared the impact of different training strategies on benchmark datasets. As shown in Table~\ref{tab:strategy}, the new training strategy does not affect the performance of FSS models.
Therefore, {\em different training strategies are NOT the key to the success of SVF}.
\begin{table}[h]\scriptsize
\centering
\setlength\tabcolsep{4pt}
\renewcommand\arraystretch{1.3}
\caption{Compare with different training strategy on Pascal-5 training set in terms of mIoU for 1-shot segmentation.}
\label{tab:strategy}
\begin{tabular}{c|c|c|ccccc}
\hline
\multirow{2}{*}{Method} & \multirow{2}{*}{Backbone} & \multirow{2}{*}{Training Strategy} & \multicolumn{5}{c}{1-shot}                                    \\ \cline{4-8} 
                        &                           &                                    & Fold-0 & Fold-1 & Fold-2 & \multicolumn{1}{c|}{Fold-3} & Mean  \\ \hline
baseline                & \multirow{6}{*}{ResNet50} & original                           & 65.60  & 70.28  & 64.12  & \multicolumn{1}{c|}{60.27}  & 65.07 \\
baseline                &                           & ours                               & 64.95  & 69.75  & 65.91  & \multicolumn{1}{c|}{59.59}  & 65.05 \\ \cline{1-1} \cline{3-8} 
PFENet~\cite{tian2020prior}                  &                           & original                           & 66.61  & 72.55  & 65.33  & \multicolumn{1}{c|}{60.91}  & 66.35 \\
PFENet~\cite{tian2020prior}                  &                           & ours                               & 65.58  & 72.49  & 66.12  & \multicolumn{1}{c|}{60.30}  & 66.12 \\ \cline{1-1} \cline{3-8} 
BAM~\cite{lang2022learning}                     &                           & original                           & 68.97  & 73.59  & 67.55  & \multicolumn{1}{c|}{61.13}  & 67.81 \\
BAM~\cite{lang2022learning}                     &                           & ours                               & 68.43  & 73.66  & 67.98  & \multicolumn{1}{c|}{61.63}  & 67.93 \\ \hline
\end{tabular}
\end{table}
\begin{table}[h]\scriptsize
\centering
\setlength\tabcolsep{4pt}
\renewcommand\arraystretch{1.3}
\caption{Ablation study on the training trick.}
\label{tab:datsets}
\begin{tabular}{c|c|c|ccccc}
\hline
\multirow{2}{*}{Method} & \multirow{2}{*}{Backbone} & \multirow{2}{*}{Training Trick} & \multicolumn{5}{c}{1-shot}                                    \\ \cline{4-8} 
                        &                           &                          & Fold-0 & Fold-1 & Fold-2 & \multicolumn{1}{c|}{Fold-3} & Mean  \\ \hline
baseline                & \multirow{8}{*}{ResNet50} & w/o                        & 66.36  & 69.22  & 57.64  & \multicolumn{1}{c|}{58.73}  & 62.99 \\
baseline                &                           & w                        & 65.60  & 70.28  & 64.12  & \multicolumn{1}{c|}{60.27}  & 65.07 \\ \cline{1-1} \cline{3-8} 
PFENet~\cite{tian2020prior}                  &                           & w/o                        &67.06        & 71.61       & 55.21       &  \multicolumn{1}{c|}{59.46}       & 63.34      \\
PFENet~\cite{tian2020prior}                   &                           & w                        & 66.61  & 72.55  & 65.33  & \multicolumn{1}{c|}{60.91}  & 66.35  \\ \cline{1-1} \cline{3-8} 
CyCTR~\cite{zhang2021few}                   &                           & w/o                        & 67.80  & 72.80  & 58.00  & \multicolumn{1}{c|}{58.00}  & 64.20 \\
CyCTR~\cite{zhang2021few}                   &                           & w                        & 65.17  & 72.52  & 66.60  & \multicolumn{1}{c|}{60.9}   & 66.30 \\ \cline{1-1} \cline{3-8} 
BAM~\cite{lang2022learning}                     &                           & w/o                        & 68.37  & 72.05  & 57.55  & \multicolumn{1}{c|}{60.38}  & 64.59 \\
BAM~\cite{lang2022learning}                     &                           & w                        & 68.97  & 73.59  & 67.55  & \multicolumn{1}{c|}{61.13}  & 67.81 \\ \hline
\end{tabular}
\vspace{-2.0em}
\end{table}


\textbf{Training Tricks:} Following the same setting of BAM~\cite{lang2022learning}, we remove some images containing novel classes of the test set from the training set. This is a novel trick in FSS to further improve the performance. In Table~\ref{tab:datsets}, we compared the effect of this trick on FSS models. The results show that this trick brings 2.0 mIoU improvement over the original FSS model on average. Especially on Flod-2, the trend of improvement is very obvious. It proves that removing images with novel classes of the test set from the training set prevents potential information leakage.

\textbf{Test image of COCO-20:} We found that the number of test sets used in previous work was different when testing on COCO. For example, BAM~\cite{lang2022learning}, HSNet~\cite{min2021hypercorrelation} were tested with 1000 images, yet \emph{Yang}~\cite{yang2021mining} was tested with 4000 images, and CyCTR~\cite{zhang2021few} was tested with 5000 images. This is very detrimental to the development of the community. In Table~\ref{tab:coconum}, we compare the different number of test images on COCO-20 to observe changes in model performance. The experimental results show that as the number of test images increases, the performance of the baseline shows a downward trend. Therefore, we call on researchers to use the same training samples for a fair comparison. Meanwhile, SVF brings positive results in different numbers of test sets. It again shows the effectiveness of SVF. 



\vspace{-0.7em}
\section{Compare with other methods.}
\vspace{-0.7em}
To clear the doubts of dataset, we use the unprocessed training set to make a fair comparison with other SOTA methods, as show in Table\ref{tab:voc}. It can be seen that baseline with SVF achieves best performance on both Pascal-5 1-shot and 5-shot settings. The experimental results prove that the advantages of SVF will not disappear due to the introduction of the training trick. Meanwhile, the experimental results prove that finetuning backbone is not only feasible in FSS, but also brings positive results to FSS models.

\begin{table}[]\scriptsize
\centering
\renewcommand\arraystretch{1.1}
\caption{compare with parameter-efficient tuning methods on Pascal-5 1-shot.}
\label{tab:adapter}
\begin{tabular}{c|c|cccc|c}
\hline
Method & fine-tune method & Fold-0 & Fold-1 & Fold-2 & Fold-3 & Mean  \\ \hline
\multirow{4}{*}{baseline} & freeze backbone & 65.60 & 70.28 & 64.12 & 60.27 & 65.07 \\
				
    & SVF &67.42&	71.57&	67.99&	61.57&	67.14 \\
    & Adapter & 18.41&	20.21&	26.62&	17.62&	20.71 \\  
    & bias tuning  & 61.62&	70.10&	64.80&	55.19&	62.93 \\ \hline 
\end{tabular}
\end{table}

\begin{table}[]\small
\vspace{-0.7em}
\centering
\setlength\tabcolsep{4pt}
\renewcommand\arraystretch{1.3}
\caption{Compare with different test image on COCO-20 in terms of mIoU for 1-shot segmentation.}
\label{tab:coconum}
\begin{tabular}{c|c|c|ccccc}
\hline
\multirow{2}{*}{Method} & \multirow{2}{*}{backbone} & \multirow{2}{*}{test image} & \multicolumn{5}{c}{1-shot}                                                                                              \\ \cline{4-8} 
                        &                           &                             & Fold-0                    & Fold-1                    & Fold-2                    & \multicolumn{1}{c|}{Fold-3} & Mean  \\ \hline
baseline                & \multirow{6}{*}{ResNet50} & 1000                        & 38.91                     & 46.07                     & 42.67                     & \multicolumn{1}{c|}{39.71}  & 41.84 \\
baseline + SVF          &                           & 1000                        & 44.22                     & 46.38                     & 42.65                     & \multicolumn{1}{c|}{41.65}  & 43.72 \\ \cline{1-1} \cline{3-8} 
baseline                &                           & 4000                        & 37.19                     & 45.30                     & 42.90                     & \multicolumn{1}{c|}{38.49}  & 40.97 \\
baseline + SVF          &                           & 4000                        & 39.80                     & 46.99                     & 42.51                     & \multicolumn{1}{c|}{42.06}  & 42.84 \\ \cline{1-1} \cline{3-8} 
baseline                &                           & 5000                        & \multicolumn{1}{l}{36.59} & \multicolumn{1}{l}{45.17} & \multicolumn{1}{l}{43.34} & \multicolumn{1}{l|}{38.73}  & 40.96 \\
baseline + SVF          &                           & 5000                        & \multicolumn{1}{l}{39.49} & \multicolumn{1}{l}{46.95} & \multicolumn{1}{l}{42.09} & \multicolumn{1}{l|}{41.15}  & 42.42 \\ \hline
\end{tabular}
\end{table}
\begin{table}[h]\scriptsize
\centering
\setlength\tabcolsep{4pt}
\renewcommand\arraystretch{1.3}
\caption{Compare with SOTA on Pascal-5\cite{shaban2017one} in terms of mIoU for 1-shot and 5-shot segmentation.}
\label{tab:voc}
\begin{tabular}{c|c|ccccc|ccccc}
\hline
\multirow{2}{*}{Method} & \multirow{2}{*}{backbone}  & \multicolumn{5}{c|}{1-shot}               & \multicolumn{5}{c}{5-shot}                \\ \cline{3-12} 
                        &                            & Fold-0 & Fold-1 & Fold-2 & Fold-3 & Mean  & Fold-0 & Fold-1 & Fold-2 & Fold-3 & Mean  \\ \hline
PANet~\cite{wang2019panet}                  & \multirow{10}{*}{ResNet50} & 44.00  & 57.50  & 50.80  & 44.00  & 49.10 & 55.30  & 67.20  & 61.30  & 53.20  & 59.30 \\
CANet~\cite{zhang2019canet}                  &                            & 52.50  & 65.90  & 51.30  & 51.90  & 55.40 & 55.50  & 67.80  & 51.90  & 53.20  & 57.10 \\
PGNet~\cite{zhang2019pyramid}                  &                            & 56.00  & 66.90  & 50.60  & 50.40  & 56.00 & 57.70  & 68.70  & 52.90  & 54.60  & 58.50 \\
RPMM~\cite{yang2020prototype}                   &                            & 55.20  & 66.90  & 52.60  & 50.70  & 56.30 & 56.30  & 67.30  & 54.50  & 51.00  & 57.30 \\
PPNet~\cite{liu2020part}                  &                            & 47.80  & 58.80  & 53.80  & 45.60  & 51.50 & 58.40  & 67.80  & 64.90  & 56.70  & 62.00 \\
CWT~\cite{lu2021simpler}                     &                            & 56.30  & 62.00  & 59.90  & 47.20  & 56.40 & 61.30  & 68.50  & 68.50  & 56.60  & 63.70 \\
PFENet~\cite{tian2020prior}                  &                            & 61.70  & 69.50  & 55.40  & 56.30  & 60.80 & 63.10  & 70.70  & 55.80  & 57.90  & 61.90 \\
CyCTR~\cite{zhang2021few}                   &                            & 67.80  & 72.80  & 58.00  & 58.00  & 64.20 & 71.10  & 73.20  & 60.50  & 57.50  & 65.60 \\ \cline{1-1} \cline{3-12} 
baseline                &                            & 66.36  & 69.22  & 57.64  & 58.73  & 62.99 & 70.75  & 72.92  & 58.86  & 65.56  & 67.02 \\
baseline + SVF          &                            & 66.88  & 70.84  & 62.33  & 60.63  & \textbf{65.17} & 71.49  & 74.04  & 59.38  & 67.43  & \textbf{68.09} \\ \hline
\end{tabular}
\end{table}

\begin{figure*}
	\centering
	\setlength{\abovecaptionskip}{0.cm}
	\includegraphics[width=1.0\linewidth]{figure/append_over.pdf}
	\vspace{-.4em}
	\caption{The mIoU curve of baseline with different finetune strategies on Pascal-5 Fold-0.}
	\label{fig:app_over}
    \vspace{-0.5em}
\end{figure*}

\section{Detailed Ablation Study}
\vspace{-0.7em}
\textbf{Different finetune strategy:} In Figure~\ref{fig:app_over}, we visualize the mIoU curve of different fine-tuning strategies. It can be seen that both layer-based and convolution-based fine-tuning methods bring over-fitting problems. This result shows that traditional fine-tuning methods are not suitable for few-shot segmentation tasks. Directly fine-tuning the parameters of backbone in few-shot learning affects the robustness of FSS models. Therefore, we propose a novel fine-tuning strategy, namely SVF. It decompose pre-trained parameters into three successive matrices via the Singular Value Decomposition (SVD). Then, It only fine-tunes the singular value matrices during the training phase. The experimental results show that SVF can effectively avoid over-fitting while bringing positive results to FSS model.







\begin{table}[]\scriptsize
\centering
\renewcommand\arraystretch{1.3}
\caption{Ablation study of BN on Pascal-5 under 1-shot setting. \checkmark represents fine-tuning this feature space. The best mean results are show in \textbf{bold}.}
\label{tab:app_BN}
\begin{tabular}{c|cl|cccc|c}
\hline
Method                    & BN     & scale & Fold-0 & Fold-1 & Fold-2 & Fold-3 & Mean  \\ \hline
& & & 65.60  & 70.28 & 64.12  & 60.27  & 65.07 \\ \cline{2-8}
\multirow{3}{*}{baseline} & \checkmark      &       & 61.93  & 70.67  & 62.02  & 57.86  & 63.12 \\
                          & \checkmark      & \checkmark     & 63.46  & 70.66  & 64.93  & 57.75  & 64.20 \\
                          &        & \checkmark     & 67.42  & 71.57  & 67.99  & 61.57  & \textbf{67.14} \\ \hline
\end{tabular}
\end{table}

\textbf{Sigular value subspace:} In Figure~\ref{fig:layer3_3}, we visualize the changes of initial Top-30 largest singular values of all  convolutional in layer 3 after SVF. The experimental results show that the change of last 3x3 convolution is the most obvious, and the change of singular value gradually moderates as the network becomes shallower. To verify the above point, we visualize the singular value change map of all 3x3 convolutions of layer 2 in Figure~\ref{fig:layer2_3}. The variation of singular values in layer2 is more gradual. Furthermore we visualize the singular value changes from the  convolution of layer 3 and layer 2 in Figure~\ref{fig:layer3_1} and Figure~\ref{fig:layer2_1}. where the  convolution is the last  convolution of each block in ResNet. This result is the same trend as  convolution. It shown that the information concerned by deep convolutions in pre-train backbone is not conducive to few-shot segmentation tasks. SVF improves the expressiveness of FSS model by focusing on adjusting distribution of singular value subspace in the deep convolution. Meanwhile, It proves that semantic cues in deep convolutions have the greatest impact on few-shot segmentation. In addition, Figure~\ref{fig:all_change} shows the variation of all singular values. It can be easy seen that the change of singular values afterward tends to 0. Therefore, the change of top-30 singular values can describe the change of all singular values.

In Table~\ref{tab:app_BN}, Table~\ref{tab:app_layer_finetune}, Table~\ref{tab:app_conv_finetune}, Table~\ref{tab:app_svf_finetune} and Tbale~\ref{tab:app_svf_layer}, we give more detail ablation study results. It contains the results for each flod in different ablation study.

\begin{figure*}
\vspace{-0.7em}
	\centering
	\setlength{\abovecaptionskip}{0.cm}
	\includegraphics[width=0.8\linewidth]{figure/layer3-conv3.pdf}
	\vspace{-.4em}
	\caption{Statistics chart about the changes of initial Top-30 largest singular values of the  convolutional in layer3 after SVF. }
	\label{fig:layer3_3}
	
\end{figure*}

\begin{figure*}
	\centering
	\setlength{\abovecaptionskip}{0.cm}
	\includegraphics[width=1.0\linewidth]{figure/layer2-conv3.pdf}
	\vspace{-.4em}
	\caption{Statistics chart about the changes of initial Top-30 largest singular values of the  convolutional in layer2 after SVF. }
	\label{fig:layer2_3}
	
\end{figure*}

\begin{figure*}
	\centering
	\setlength{\abovecaptionskip}{0.cm}
	\includegraphics[width=0.8\linewidth]{figure/layer3-conv1.pdf}
	\vspace{-.4em}
	\caption{Statistics chart about the changes of initial Top-30 largest singular values of the  convolutional in layer3 after SVF. }
	\label{fig:layer3_1}
	
\end{figure*}

\begin{figure*}
	\centering
	\setlength{\abovecaptionskip}{0.cm}
	\includegraphics[width=1.0\linewidth]{figure/layer2-conv1.pdf}
	\vspace{-.4em}
	\caption{Statistics chart about the changes of initial Top-30 largest singular values of the  convolutional in layer2 after SVF. }
	\label{fig:layer2_1}
\end{figure*}

\begin{figure*}
\vspace{-0.7em}
	\centering
	\setlength{\abovecaptionskip}{0.cm}
	\includegraphics[width=0.8\linewidth]{figure/all_change_dis.pdf}
	\vspace{-.4em}
	\caption{Statistics chart about the changes of all singular values of the last  and  convolutional in layer3 after SVF.}
	\label{fig:all_change}
\end{figure*}

\begin{table}[]\scriptsize
\centering
\renewcommand\arraystretch{1.3}
\caption{Comparative experiment with fine-tuning different layer of backbone on Pascal-5.}
\label{tab:app_layer_finetune}
\begin{tabular}{c|c|cccc|c}
\bottomrule
Method                          & layer      & Fold-0 & Fold-1 & Fold-2 & Fold-3 & Mean  \\ \hline
baseline                 & -          & 65.60  & 70.28 & 64.12  & 60.27  & 65.07  \\ \hline
+fully fine-tune                 & 1, 2, 3, 4 & 57.97  & 70.51  & 61.33  & 53.80  & 60.90 \\ \hline
\multirow{3}{*}{+ part fine-tune} & 2, 3, 4    & 55.34  & 71.16  & 62.72  & 55.38  & 61.15 \\
                                & 3, 4       & 56.85       & 71.44       & 61.72       & 54.32       & 61.08      \\
                                & 4          & 56.19       & 70.63       & 59.98       & 55.50       & 60.58      \\ \hline
+SVF                            & 2, 3, 4    & \textbf{67.42}  & \textbf{71.57}  & \textbf{67.99}  & \textbf{61.57}  & \textbf{67.14} \\ \bottomrule
\end{tabular}

\end{table}

\begin{table}[]\scriptsize
\centering
\renewcommand\arraystretch{1.3}
\caption{Comparative experiment with fine-tuning different convolutional layer of backbone on Pascal-5.}
\label{tab:app_conv_finetune}
\begin{tabular}{c|c|cc|cccc|c}
\hline
Method                            & layer   & 3  3 & 1  1 & Fold-0 & Fold-1 & Fold-2 & Fold-3 & Mean  \\ \hline
baseline                          & -       & -          & -          & 65.60  & 70.28 & 64.12  & 60.27  & 65.07  \\ \hline
\multirow{3}{*}{+part fine-tune} & 2, 3, 4 & \checkmark           & \checkmark           & 55.34  & 71.16  & 62.72  & 55.38  & 61.15 \\
                                  & 2, 3, 4 &  \checkmark          &            & 59.57       & 69.96       & 61.74       & 56.16       & 61.86      \\
                                  & 2, 3, 4 &            & \checkmark            & 58.30       & 70.50       & 62.04       & 55.63       & 61.62      \\ \hline
+SVF                              & 2, 3, 4 & -          & -          & 67.42  & 71.57  & 67.99  & 61.57  & 67.14 \\ \hline
\end{tabular}
\end{table}

\begin{table}[]\scriptsize
\centering
\renewcommand\arraystretch{1.1}
\caption{Ablation study of SVF fine-tuning different subspace on Pascal-5.}
\label{tab:app_svf_finetune}
\begin{tabular}{c|c|c|c|cccc|c}
\hline
Method                    & \textbf{} & \textbf{} & \textbf{}& Fold-0 & Fold-1 & Fold-2 & Fold-3 & Mean \\ \hline
\multirow{7}{*}{baseline} & \checkmark        &       &          &  58.14      & 70.06       &  60.91      & 55.24       & 61.09     \\ 
     &          & \checkmark     &          & 67.42       & 71.57        & 67.99        & 61.57       & 67.14     \\
    &          &       & \checkmark        & 53.87       & 70.63       & 63.65       & 55.36        & 60.88      \\ \cline{2-9} 
    & \checkmark        & \checkmark     &          & 57.54  & 70.19  & 62.12  & 56.41  & 61.57     \\
     &          & \checkmark     & \checkmark        & 53.30  & 71.21  & 62.24  & 54.92  & 60.42     \\
     & \checkmark        &      & \checkmark        & 53.81        & 70.75       & 61.92       & 53.60       &  60.02     \\  
& \checkmark        & \checkmark     & \checkmark        & 56.64       & 70.47       & 63.48       & 54.36       & 61.24     \\ \hline 
\end{tabular}
\end{table}

\begin{figure*}[h]
\vspace{-0.9em}
	\centering
	\setlength{\abovecaptionskip}{0.cm}
	\includegraphics[width=0.6\linewidth]{figure/perspective.pdf}
	\caption{Different implementations of SVF.}
	\label{fig:perspective}
\end{figure*}

\begin{table}[]\scriptsize
\centering
\renewcommand\arraystretch{1.1}
\caption{Ablation study of SVF fine-tuning different layer on Pascal-5.}
\label{tab:app_svf_layer}
\begin{tabular}{c|c|cccc|c}
\hline
Method & layer & Fold-0 & Fold-1 & Fold-2 & Fold-3 & Mean  \\ \hline
\multirow{4}{*}{baseline + SVF} & 4 & 68.28 & 71.04 & 65.59 & 59.91 & 66.21 \\
                & 3, 4 & 67.21  & 71.88  & 68.12  & 61.57  & 67.20 \\
                & 2, 3, 4 & 67.42  & 71.57  & 67.99  & 61.57  & 67.14 \\  
                & 1, 2, 3, 4  & 67.06  & 71.69  & 67.77  & 61.94  & 67.12 \\ \hline 
\end{tabular}
\end{table}

\begin{table}[]\scriptsize
\centering
\renewcommand\arraystretch{1.1}
\caption{Comparing with only fine-tuning BN on Pascal-5.}
\label{tab:com_bn}
\begin{tabular}{c|c|c|cccc|c}
\hline
Method & Backbone& Fine-tuning Method & Fold-0 & Fold-1 & Fold-2 & Fold-3 & Mean  \\ \hline
\multirow{5}{*}{baseline} & \multirow{5}{*}{ResNet-50}  &Freeze Backbone& 65.60 & 70.28 & 64.12 & 60.27 & 65.07 \\
 & & Fine-tuning BN scale (weight) & 62.28  & 68.66  & 61.19  & 58.18  & 62.58 \\
 &               & Fine-tuning BN shift (bias)&	61.62&	70.10&	64.80&	55.19&	62.93 \\  
  &              & Fine-tuning BN (weight+bias)	&61.93	&70.67&	62.02&	57.86&	63.12 \\ 
  && SVF&	67.42&	71.57&	67.99&	61.57&	67.14\\
  \hline 
\end{tabular}
\end{table}

\section{Discussion}

\subsection{Discussion on other SVD}
In this section, we discuss the differences between other SVD-based methods~\cite{jia2017improving,sedghi2018singular} and SVF. Both SVB~\cite{jia2017improving} and \emph{Hanie}~\cite{sedghi2018singular} constrain the distribution of the singular values  where SVB~\cite{jia2017improving} forces the singular value around 1 and \emph{Hanie}~\cite{sedghi2018singular} clamps the large singular values into a constant, hence serving as a regularization term. We did not pose an extra constraint on , instead, encouraged the fully trainable singular values. As illustrated in SVB's Figure 1, the singular values of well-trained weights are widely spread around [0,2]. The strong regularization proposed in  SVB~\cite{jia2017improving} and \emph{Hanie}~\cite{sedghi2018singular} should damage the performance of pre-trained networks. Therefore, they turn to training from scratch, which is infeasible in the circumstance of few-shot segmentation. Our method coupled with pre-trained parameters can further exploit the capacity of the backbone, leading to superior results.


\subsection{Discussion on different implementation}

In this section, we provide a discussion on our SVF. The main idea of SVF is learning to change singular values in the backbone weights. It has different implementations. We show two possible ways to achieve SVF in Figure~\ref{fig:perspective}: (i) treat the single value matrix  as trainable parameters directly; (ii) freeze the original singular value matrix  and introduce another trainable singular value matrix  (we use exponential function {\em } to keep it positive and initialize it with zeros), where the final singular value matrix is a product of  (frozen) and  (trainable). In the second implementation, SVF keeps the backbone frozen (as all its weights are frozen) while introducing a small part of extra trainable parameters. It shares similarities with the recently proposed Visual Prompt Tuning (VPT)~\cite{jia2022visual}. The difference between VPT and SVF is that VPT introduces the trainable parameters in the input space while SVF introduces them in the singular value space. Although SVF and VPT freeze the original backbone, they can produce optimization on the feature maps of the backbone. This property enables SVF to perform better in few-shot segmentation (FSS) and is the essential difference from the properties in previous SSF methods with frozen backbone (they do not change the feature maps of the backbone).



\begin{table}[]\scriptsize
\centering
\renewcommand\arraystretch{1.1}
\caption{introduce a new small part of parameters S' to verify the importance of singular values on Pascal-5.}
\label{tab:about_s}
\begin{tabular}{c|c|c|c|cccc|c}
\hline
Method & Backbone& Expression of weight& Fine-tune param & Fold-0 & Fold-1 & Fold-2 & Fold-3 & Mean  \\ \hline
\multirow{4}{*}{baseline} & \multirow{4}{*}{ResNet-50}  &W &- & 65.60 & 70.28 & 64.12 & 60.27 & 65.07 \\
 & & S'W&S' & 60.96  & 71.99  & 62.54  & 58.58  & 63.52 \\
 &               & WS' &S'&	62.82&	71.69&	62.84&	61.13&	64.62 \\  
  &              & USV &S	&67.42	&71.57&	67.99&	61.57&	\textbf{67.14} \\ 
  \hline 
\end{tabular}
\end{table}

\begin{table}[]\scriptsize
\centering
\renewcommand\arraystretch{1.1}
\caption{Compare with different implementations of SVF on Pascal-5 1-shot.}
\label{tab:diff_s}
\begin{tabular}{c|c|c|c|cccc|c}
\hline
Method & Backbone& Expression of weight& Fine-tune param & Fold-0 & Fold-1 & Fold-2 & Fold-3 & Mean  \\ \hline
\multirow{4}{*}{baseline} & \multirow{4}{*}{ResNet-50}  &W &- & 65.60 & 70.28 & 64.12 & 60.27 & 65.07 \\
&              & USV &S	&67.42	&71.57&	67.99&	61.57&	\textbf{67.14} \\
&              & USS'V &S'	&67.16	&71.58&	68.59&	61.08&	67.10 \\
&              & USS'V &S + S'	&66.42	&71.73&	67.23&	61.12&	66.63 \\
   
  \hline 
\end{tabular}
\end{table}

\begin{table}[]\scriptsize
\centering
\renewcommand\arraystretch{1.1}
\caption{Compare with other SVD-based methods on Pascal-5 1-shot.}
\label{tab:rsrw}
\begin{tabular}{c|c|c|c|cccc|c}
\hline
Method & Backbone& Expression of weight& Fine-tune param & Fold-0 & Fold-1 & Fold-2 & Fold-3 & Mean  \\ \hline
\multirow{4}{*}{baseline} & \multirow{4}{*}{ResNet-50}  &W &- & 65.60 & 70.28 & 64.12 & 60.27 & 65.07 \\
&              & USV &S	&67.42	&71.57&	67.99&	61.57&	\textbf{67.14} \\
 & & S'W&S' & 60.96  & 71.99  & 62.54  & 58.58  & 63.52 \\
&              & RS'R'W & S'	&32.91	&51.93&	51.00&	37.60&	43.36 \\
   
  \hline 
\end{tabular}
\end{table}

\clearpage
\subsection{Discussion on success of SVF}
In this section, we discuss the truly responsible for the success of SVF from three question. First, Does fine-tune another small part of parameters in the backbone work? We conduct experiments on Pascal-5 with the 1-shot setting. We compare our SVF with methods that only fine-tune the parameters in the BN layers. The results in Table~\ref{tab:com_bn} show that only fine-tuning the parameters in BN layers does not bring over-fitting in few-shot segmentation methods, but they perform worse than the conventional paradigm (freezing backbone). While our SVF outperform other methods by large margins.

Second, Is it really necessary to fine-tune the singular values? What if we introduce a new small part of parameters S', which is not in the singular value space, and only fine-tune the S'? To answer this question, we conduction two experiments, where the weight becomes S'W or WS', and only fine-tune the introduced small part of parameters S'. The results in Table~\ref{tab:about_s} are consistence with Table~\ref{tab:com_bn}. Both of them can avoid over-fitting but show slightly worse performance than the freezing backbone baseline. The above experimental results suggest that fine-tuning a small part of parameters is a good way to avoid over-fitting when fine-tuning the backbone in few-shot segmentation. But it is non-trivial to find such a small part of parameters that can bring considerable improvements.

Third, What causes the differences between SVF and WS' or S'W?  In this question, we try to provide our understanding of what causes the superior performances of SVF over WS' and S'W. We conjecture that this may be related to the context that S or S' can access when fine-tuning the parameters. Assume that W has the shape of . S and S' are diagonal matrices. S has the shape of [Rank, Rank], and S' has the shape of  or . When optimizing the parameters, S' only has relations on dimension M or dimension N in a channel-wise manner, while S can connect all channels on both dimension M and dimension N, as S is in the singular value space. This differences can affect the received gradients when training S or S', which results in different performance. To give more evidences, we design more variants of SVF and provide their results in Table~\ref{tab:diff_s}.

Finaly, To verify whether SVF depends crucially on the singular value space, or simply on the number of effective updated parameters. we design a experiment: let R be a random rotation matrix, and set U=R' and V=RW, where W is the original weight matrix for the given layer. The formulation of the weight becomes RS'R'W. Note that S’ is initialized with an identity matrix as done in previous experiments. During the fine-tuning, we only train S’ while keep others frozen in the backbone. We provide the results in Table~\ref{tab:rsrw}. Random rotation formulation gives poor results. In fact, if we set R as an identity matrix (identity matrix is a rotation matrix), RS'R'W = S'W. As shown in the table, S'W is much better than random RS'R'W. It seems that the selection of the rotation matrix R is critical to the final segmentation performance. Meanwhile, If we consider RS’R’ (it is a diagonal matrix in the initialization stage) as a whole, RS'R is only related to one dimension of the weight W. Thus for the middle matrix S', it is also channel-aligned with respect to weight W.

In addition, if R is random initialized, we can not guarantee that RS'R' is a diagonal matrix when updating S' during training (we verify this phenomenon with the saved checkpoints when we finish the training). Note that the weight W is the one from the pre-trained backbone, which contains semantic clues or learned knowledge. The non-diagonal matrix RS'R' may bring unexpected transformation to the pre-trained weight W, leading to poor results.



 




\end{document}