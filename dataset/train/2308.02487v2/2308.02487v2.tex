\documentclass{article}








\PassOptionsToPackage{numbers, compress}{natbib}


\usepackage[final]{neurips_2023}





\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         


\usepackage{pgfplots} \usepackage{pgfplotstable} \pgfplotsset{compat=newest} \usetikzlibrary{plotmarks} \usetikzlibrary{colorbrewer} 

\usepackage{algorithm}
\usepackage[noend]{algorithmic}
\usepackage{amsmath, amssymb, caption, subcaption, multirow, overpic, textpos, grffile}
\usepackage{graphicx}


\usepackage{xspace}

\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}

\def\eg{\emph{e.g}\onedot}
\def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot}
\def\Ie{\emph{I.e}\onedot}
\def\cf{\emph{cf}\onedot}
\def\Cf{\emph{Cf}\onedot}
\def\etc{\emph{etc}\onedot}
\def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot}
\def\dof{d.o.f\onedot}
\def\etal{\emph{et al}\onedot}

\newcommand{\figref}[1]{Fig\onedot~\ref{#1}}
\newcommand{\equref}[1]{Eq\onedot~\eqref{#1}}
\newcommand{\secref}[1]{Sec\onedot~\ref{#1}}
\newcommand{\tabref}[1]{Tab\onedot~\ref{#1}}

\newlength\secmargin
\newlength\paramargin
\newlength\abovetabcapmargin
\newlength\belowtabcapmargin
\newlength\abovefigcapmargin
\newlength\belowfigcapmargin

\setlength{\secmargin}{-1.0mm}
\setlength{\paramargin}{0.0mm}
\setlength{\abovetabcapmargin}{-2pt}
\setlength{\belowtabcapmargin}{-5pt}
\setlength{\abovefigcapmargin}{-0pt}
\setlength{\belowfigcapmargin}{-13pt}

\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}


\usepackage{xcolor}
\usepackage{colortbl}

\newcommand{\modelname}{FC-CLIP\xspace}

\newcommand{\x}{{\times}}

\definecolor{baselinecolor}{gray}{.9}
\newcommand{\baseline}[1]{\cellcolor{baselinecolor}{#1}}
\definecolor{nacolor}{gray}{.8}
\newcommand{\nabaseline}[1]{\textcolor{nacolor}{#1}}

\newlength\savewidth\newcommand\shline{\noalign{\global\savewidth\arrayrulewidth
  \global\arrayrulewidth 1pt}\hline\noalign{\global\arrayrulewidth\savewidth}}
\newcommand{\tablestyle}[2]{\setlength{\tabcolsep}{#1}\renewcommand{\arraystretch}{#2}\centering\footnotesize}


\title{Convolutions Die Hard: Open-Vocabulary Segmentation with Single Frozen Convolutional CLIP}


\author{Qihang Yu\textsuperscript{1}, Ju He\textsuperscript{2}, Xueqing Deng\textsuperscript{1}, Xiaohui Shen\textsuperscript{1}, Liang-Chieh Chen\textsuperscript{1}\\
  \textsuperscript{1} ByteDance \qquad
   \textsuperscript{2} The Johns Hopkins University
}


\begin{document}


\maketitle

\begin{abstract}
Open-vocabulary segmentation is a challenging task requiring segmenting and recognizing objects from an open set of categories in diverse environments.
One way to address this challenge is to leverage multi-modal models, such as CLIP, to provide image and text features in a shared embedding space, which effectively bridges the gap between closed-vocabulary and open-vocabulary recognition.
Hence, existing methods often adopt a two-stage framework to tackle the problem, where the inputs first go through a mask generator and then through the CLIP model along with the predicted masks.
This process involves extracting features from raw images multiple times, which can be ineffective and inefficient.
By contrast, we propose to build everything into a single-stage framework using a \textit{shared \textbf{F}rozen \textbf{C}onvolutional \textbf{CLIP}} backbone, which not only significantly simplifies the current two-stage pipeline, but also remarkably yields a better accuracy-cost trade-off.
The resulting single-stage system, called \modelname, benefits from the following observations: the \textit{frozen} CLIP backbone maintains the ability of open-vocabulary classification and can also serve as a strong mask generator, and the \textit{convolutional} CLIP generalizes well to a larger input resolution than the one used during contrastive image-text pretraining.
Surprisingly, \modelname advances state-of-the-art results on various benchmarks, while running practically fast.
Specifically, when training on COCO panoptic data only and testing in a zero-shot manner, \modelname achieve 26.8 PQ, 16.8 AP, and 34.1 mIoU on ADE20K, 18.2 PQ, 27.9 mIoU on Mapillary Vistas, 44.0 PQ, 26.8 AP, 56.2 mIoU on Cityscapes, outperforming the prior art under the same setting by +4.2 PQ, +2.4 AP, +4.2 mIoU on ADE20K, +4.0 PQ on Mapillary Vistas and +20.1 PQ on Cityscapes, respectively.
Additionally, the training and testing time of \modelname is  and  
 significantly faster than the same prior art, while using  fewer total model parameters.
Meanwhile, \modelname also sets a new state-of-the-art performance across various open-vocabulary semantic segmentation datasets.
Code and models are available at \url{https://github.com/bytedance/fc-clip}.
\end{abstract} \section{Introduction}
\label{Introduction}

Panoptic segmentation~\cite{kirillov2018panoptic} is a complex computer vision task that aims to predict a set of non-overlapping masks, each with its corresponding class label.
It combines the tasks of semantic segmentation~\cite{he2004multiscale} and instance segmentation~\cite{hariharan2014simultaneous}, making it a challenging problem to solve.
Many methods~\cite{kirillov2019panoptic,xiong2019upsnet,cheng2019panoptic,wang2021max,li2021panoptic,yu2022cmt,cheng2021masked,yu2022k,liang2023clustseg} have been proposed to tackle this problem, and a significant progress has been made in terms of panoptic quality (PQ).
However, due to the high cost of annotating such a fine-grained dataset~\cite{lin2014microsoft,Cordts2016Cityscapes}, the number of semantic classes is typically limited to a few dozens or hundreds.
This restriction hinders the further application of existing approaches to real-world settings, where the number of possible semantic classes is unlimited.

To overcome the limitations of closed-vocabulary segmentation, open-vocabulary segmentation~\cite{li2022language,xu2021simple,ghiasi2022scaling,ding2022open} has been proposed. These approaches uses text embeddings of category names~\cite{zareian2021open}, represented in natural language, as label embeddings, instead of learning them from the training dataset.
By doing so, models can classify objects from a wider vocabulary, which improves their ability to handle a broader range of categories.
To ensure that meaningful embeddings are provided, a pretrained text encoder~\cite{devlin2019bert,raffel2020exploring,liu2019roberta,radford2021learning} is typically used. This encoder can effectively capture the semantic meaning of words and phrases, which is critical for open-vocabulary segmentation.

\begin{figure}[!t]
    \centering
    \vspace{-15ex}
    \includegraphics[width=1.0\linewidth]{figures/kmeans_vis.pdf}
    \vspace{-15ex}
    \caption{\textbf{-means visualization on top of frozen CLIP backbone features \wrt different input resolutions.} Both ViT-based and CNN-based CLIP produces semantic-meaningful features. However, when scaling up the input resolutions, we note that ViT-based CLIP features turn noisier, while CNN-based ones are smoother and generalize better. The smoother feature map is preferable for mask-pooling modules in our design.
    }
    \label{fig:kmeans_vis}
\end{figure}

Multi-modal models, such as CLIP~\cite{radford2021learning} and ALIGN~\cite{jia2021scaling}, have shown promise for open-vocabulary segmentation due to their ability to learn aligned image-text feature representations from large-scale Internet data~\cite{schuhmann2022laion}. 
SimBaseline~\cite{xu2021simple} and OVSeg~\cite{liang2022open} are two recent methods that use a two-stage framework to adapt CLIP for open-vocabulary segmentation.
In these methods, images are first processed by a heavy mask generator~\cite{he2017mask,cheng2021masked} to obtain mask proposals, and then each masked image crop is generated and fed into a frozen CLIP model for classification.
MaskCLIP~\cite{ding2022open} extends this approach to open-vocabulary panoptic segmentation, but additionally leverages mask proposals as attention masks in the CLIP backbone to efficiently avoid multiple forwarding processes for the masked crops.
More recently, ODISE~\cite{xu2023open} employs a stable diffusion UNet~\cite{ronneberger2015u,rombach2022high} as a frozen backbone for mask generator, which significantly boosts the state-of-the-art performance. However, despite these advances, they still rely on a two-stage framework, where the mask generator and CLIP classifier extract features from raw images separately, resulting in inefficiency and ineffectiveness.

A natural question thus arises as to \textit{whether it is possible to unify the mask generator and CLIP classifier into a single-stage framework for open-vocabulary segmentation}.
Sharing the feature extractor between them is a straightforward solution, but it poses two challenges.
First, fine-tuning CLIP backbone can disrupt the alignment between image and text features, resulting in a much worse performance on out-of-vocabulary categories.
Existing methods~\cite{xu2021simple,liang2022open,ding2022open,xu2023open} rely on another separate backbone for mask generator, increasing model size and computational costs.
Second, CLIP models are typically pretrained on relatively lower-resolution inputs, while dense prediction tasks require a much higher resolution for optimal performance.
This makes it difficult to directly apply CLIP-pretrained backbones to downstream dense prediction tasks, particularly ViT-based CLIP models~\cite{dosovitskiy2020image}, where careful treatments are required (\eg, side adapter~\cite{chen2023vision,xu2023side}, or cost aggregation~\cite{zhou2022extract,cho2023cat}).
Consequently, existing methods~\cite{ding2022open,xu2023open} perform mask segmentation and CLIP classification at different input scales, leading to sub-optimal performance.

To alleviate the two challenges, we propose to build both mask generator and CLIP classifier on top of a \textit{shared \textbf{F}rozen \textbf{C}onvolutional \textbf{CLIP}} backbone, resulting in a single-stage framework \modelname.
Its design is based on the following observations.
The \textit{frozen} CLIP backbone ensures that the pretrained image-text feature alignment is intact, allowing out-of-vocabulary classification.
It can also serve as a strong mask generator by appending a lightweight pixel decoder and mask decoder~\cite{cheng2021masked, yu2022k}.
The \textit{convolutional} CLIP, based on a Convolutional Neural Network (CNN)~\cite{lecun1998gradient}, empirically
shows a better generalization ability compared to ViT-based CLIP~\cite{dosovitskiy2020image}, when the input size scales up. 
This echoes the success of fully convolutional networks~\cite{long2014fully} in dense prediction tasks.
Both observations are critical for developing a single-stage framework, but they have been overlooked and undiscovered by existing two-stage pipelines~\cite{ding2022open,xu2023open}.
In \figref{fig:kmeans_vis}, we visualize the learned visual representation of ViT-based and CNN-based CLIP via -means clustering~\cite{lloyd1982least}.
As shown in the figure, the features learned by CNN-based CLIP are more robust across different input sizes.

Surprisingly, the adoption of a \textit{single frozen convolutional} CLIP as the shared feature extractor results in an extremely simple yet effective design.
Specifically, the single-stage \modelname consists of three modules built upon a shared frozen convolutional CLIP backbone: a class-agnostic mask generator, an in-vocabulary classifier, and an out-of-vocabulary classifier (see~\figref{fig:method_comp} for comparison between pipelines).
The proposed method not only enjoys a simple design, but also comes with a very low cost for both training and testing.
As a comparison, our model has only M frozen parameters and M trainable parameters, against the state-of-the-art work ODISE~\cite{xu2023open} that has M frozen and M trainable parameters.
Furthermore, our model training only takes  V100 GPU days, which is  faster compared to ODISE's  V100 GPU days.
During inference, our model also runs  faster. 
Although \modelname enjoys a simple design, it still outperforms previous methods across multiple datasets. Trained on COCO panoptic dataset only, \modelname surpasses prior state-of-the-art ODISE~\cite{xu2023open} significantly in a zero-shot manner. Specifically, \modelname achieves  PQ (),  PQ (), and  PQ () on ADE20K, Mapillary Vistas, and Cityscapes, respectively.

As panoptic segmentation unifies semantic and instance segmentation, \modelname naturally extends to open-vocabulary semantic and instance segmentation.
With the same model trained on COCO panoptic data only (\ie, no task-specific fine-tuning), \modelname achieves state-of-the-art performance on open-vocabulary instance and semantic segmentation.
Specifically, \modelname achieves  AP on ADE20K, surpassing the state-of-art ODISE~\cite{xu2023open} by .
\modelname also outperforms the state-of-art specialized open-vocabulary semantic segmentation model SAN~\cite{xu2023side} by  and  mIoU on the challenging ADE20K-847 (A-847) and PASCAL-Context-459 (PC-459) benchmarks, respectively.


In summary, through the lens of a careful re-design of existing two-stage open-vocabulary segmentation models, we establish a simple, strong, and fast baseline for the community.
The proposed \modelname adopts a single-stage framework by exploiting a shared frozen convolutional CLIP, which not only advances the state-of-the-art performances on multiple benchmarks, but also enjoys a practically fast training and inference speed.
We hope our study will inspire future research on efficient single-stage open-vocabulary segmentation models. \section{Related Work}
\label{RelatedWork}

\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{figures/open_vocab_comp.pdf}
    \caption{
    \textbf{Comparisons between open-vocabulary panoptic segmentation pipelines.}
    \textit{Left}: Existing methods~\cite{ding2022open,xu2023open} adopt a two-stage pipeline, where the first stage employs a high-resolution image to generate class-agnostic masks, and the second stage feeds both the low-resolution image and predicted masks to a frozen CLIP backbone for open-vocabulary recognition. This incurs heavy computation, as image features are extracted multiple times.
    \textit{Middle}: A na\"ive single-stage framework builds everything together and fine-tunes the CLIP backbone, breaking the pretrained alignment between images and texts.
    \textit{Right}: Our single-stage framework \modelname employs a shared frozen convolutional CLIP, where "frozen CLIP" maintains the open-vocabulary recognition and can serve as a strong mask generator, and "convolutional CLIP" generalizes well to large input sizes.
    Note that the predicted masks are used for CLIP recognition in all three schemes (not shown for simplicity).
    }
\label{fig:method_comp}
\end{figure}
Vision-language models target at encoding vision and language jointly in a fusion model. Early works~\cite{tan2019lxmert,chen2020uniter,zhang2021vinvl} extract visual representations by pretrained object detectors and fine-tune on downstream tasks with language supervision. Recently, with the breakthrough of large language models~\cite{devlin2019bert, brown2020language}, rapid progress has been made in this field. CLIP~\cite{radford2021learning} and ALIGN~\cite{jia2021scaling} demonstrate that pretraining dual-encoder models with contrastive objectives on large-scale noisy image-text pairs can learn representation with cross-modal alignment ability and show strong performance in zero-shot downstream tasks. The following works~\cite{yuan2021florence,alayrac2022flamingo,yu2022coca} further confirm these points and achieve impressive results in zero-shot transfer learning such as open-vocabulary image recognition.

Closed-vocabulary segmentation can be divided into three types according to the semantics of the grouping pixels, \ie semantic, instance and panoptic segmentation. Semantic segmentation interprets high-level category semantic concepts.
Prior works~\cite{deeplabv12015,ronneberger2015u,chen2018deeplabv2,chen2017deeplabv3,deeplabv3plus2018,fu2019dual,yuan2020object,xie2021segformer,zheng2021rethinking,gu2022multi} mainly treat this task as a per-pixel classification problem and build their models on top of the idea of FCN~\cite{long2014fully}.
Instance segmentation groups foreground pixels into different object instances.
Starting from Mask R-CNN~\cite{he2017mask}, prior works~\cite{kirillov2017instancecut,liu2018path,chen2018masklab,cai2018cascade,yolact,chen2019hybrid,tian2020conditional,wang2020solov2,qiao2021detectors} mainly address this task with mask classification, where a set of bounding boxes and binary masks are predicted.
Panoptic segmentation seeks for holistic scene understanding including both stuff and things. The pioneering work~\cite{kirillov2018panoptic} and prevalent ones~\cite{liu2019e2e,kirillov2019panoptic,xiong2019upsnet,cheng2019panoptic,li2020unifying,wang2020axial,chen2020scaling,qiao2021vip} decompose the problem into various proxy tasks and merge the results in the end.
Recently, following DETR~\cite{carion2020end}, most works~\cite{wang2021max,strudel2021segmenter,cheng2021per,cheng2021masked,li2021panoptic,yu2022cmt,yu2022k,jain2023oneformer,li2023mask,sun2023remax} present end-to-end solutions based on the idea of mask classification.
Standing on their shoulders, our proposed method builds on top of the pixel decoder and mask decoder of Mask2Former~\cite{cheng2021masked}
by additionally exploiting the open-vocabulary recognition ability from CLIP~\cite{radford2021learning}.

Open-vocabulary segmentation aims at segmenting arbitrary classes including those that can not be accessed during the training procedure.
Priors works \cite{li2022language,ghiasi2022scaling,xu2021simple,liang2022open,ding2022decoupling,xu2022groupvit,zhou2022extract,xu2023side,zou2022generalized,ma2022open,zhou2023zegclip,gu2023dataseg} perform open-vocabulary semantic segmentation through leveraging large pretrained vision-language models~\cite{radford2021learning,jia2021scaling,rombach2022high}.
Recently, MaskCLIP~\cite{ding2022open} presents a two-stage pipeline, which consists of a class-agnostic mask generator and a frozen CLIP~\cite{radford2021learning} encoder for cross-modal alignment, and thus expands the scope of the CLIP models into open-vocabulary panoptic segmentation.
ODISE~\cite{xu2023open} digs out the innate potential of pretrained text-image diffusion models~\cite{rombach2022high} in terms of the ability to present open concepts in the representation space for performing strong open-vocabulary panoptic segmentation.
FreeSeg~\cite{qin2023freeseg} encodes multi-granularity concepts into a compact textural abstraction, enabling generalizability to arbitrary text description.
Unlike those methods, we propose a single-stage framework by exploiting a single frozen convolutional CLIP backbone, resulting in a simpler, faster, and stronger model than existing works.

We also note that the pioneering work F-VLM~\cite{kuo2022f} builds an open-vocabulary detection framework on top of a frozen CLIP backbone. However, \modelname differs from it with a totally different observation and motivation. Specifically, our work was initially motivated by the state-of-art open-vocabulary segmentation model ODISE~\cite{xu2023open}, which found that the CLIP backbone extracts noisier features than diffusion models (Figure B. 1. in~\cite{xu2023open}), leading to inferior segmentation results (which justifies their adoption of diffusion models). Their observation motivated us to look deeply into the problem. Interestingly, our discoveries show that both ViT-based (used by ODISE~\cite{xu2023open}) and CNN-based CLIP can produce semantic-meaningful features. However, when scaling up the input resolutions, we discover that ViT-based CLIP features turn noisier, while CNN-based ones are smoother and generalize better across input sizes. F-VLM~\cite{kuo2022f} also empirically found that a frozen CLIP can provide meaningful features for object detection. However, they did not choose CNN-based CLIP on purpose and thus did not compare carefully between ViT-based and CNN-based CLIP backbones. On the other hand, in our paper, we have provided careful ablation studies on ViT-based and CNN-based CLIP, where we observe that even though both ViT-based and CNN-based CLIP initially have comparable performance at resolution , CNN-based CLIP shows better and more robust performance when input resolution scales up. \section{Method}
\label{Method}

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/arch_dog.pdf}
    \caption{
    \textbf{Overview of \modelname,} which contains three main components: mask generator, an in-vocabulary (in-vocab) classifier, and an out-of-vocabulary (out-vocab) classifier.
    All components build on top of a shared \textit{frozen covolutional} CLIP backbone.
    The pixel decoder and mask decoder follow the design of Mask2Former, and generate class-agnostic masks.
    The in-vocabulary classifier yields the class embeddings by mask-pooling over final pixel features from pixel decoder.
    During testing, \modelname additionally exploits the out-of-vocabulary classifier by mask-pooling over frozen CLIP backbone features, and the final class prediction is obtained by geometric ensembling both classifiers.    
    Note that the text embeddings are obtained by feeding category names into a CLIP text encoder, which are done beforehand and cached in memory, thus causing no additional costs.
    Also, the class-agnostic mask proposals are fed to the mask pooling modules (not shown for simplicity).
    }
\label{fig:model_arch}
\end{figure}

In this section, we first define the problem of open-vocabulary segmentation.
We then introduce the existing two-stage pipeline, followed by our proposed single-stage framework \modelname.


\noindent \textbf{Problem Definition}\quad
Open-vocabulary segmentation aims to segment the image  into 
a set of masks with associated semantic labels:



The  ground truth masks  contain the corresponding ground truth class label .
During training, a fixed set of class labels  is used, while during inference, another set of categories  is used.
In the open-vocabulary setting,  may contain novel categories unseen during training, \ie, .
We follow previous works~\cite{ding2022open, xu2023open} and assume the availability of the category names of  (represented in natural language) during testing.

\noindent \textbf{Two-Stage Open-Vocabulary Segmentation}\quad
Existing works~\cite{xu2021simple, liang2022open, ding2022open, xu2023open} adopt a two-stage pipeline for open-vocabulary segmentation.
The first stage contains a class-agnostic mask generator  with parameters  that generates a set of  mask proposals 
, given the input image :


In the second stage, a CLIP adapter  takes both image  and mask proposals  as inputs, where the latter input is used to guide the frozen CLIP model  ( denotes frozen).
The adapter performs mask classification through forwarding processes with either masked crops~\cite{xu2021simple, liang2022open} or masked attention~\cite{ding2022open, xu2023open}:

where  refers to the predicted class probabilities for the  predicted masks,  depending on training or testing phase, and  is the category size.

Although this framework has achieved impressive open-vocabulary segmentation performance, it has two limitations.
First, the image features are extracted \textit{twice}, once for mask generation and the other for mask classification.
The double feature extractions incur heavy computation, making it costly to scale up backbone parameters.
Second, the mask generator often requires high-resolution inputs (\eg, ), whereas the CLIP model is usually pretrained with lower-resolution images (\eg, ).
The two-stage pipeline thus needs to feed high-resolution images into the mask generator and low-resolution images into the CLIP classifier, making the model inefficient.

\noindent \textbf{Na\"ive Single-Stage Open-Vocabulary Segmentation}\quad
To avoid increasing the model size and computational cost of duplicate feature extractions, one may na\"ively formulate everything together into a single-stage framework , where both mask generator and mask classifier share the same CLIP-pretrained backbone  (not frozen) for extracting features from an input image :


However, we empirically discover that fine-tuning this na\"ive single-stage framework causes a misalignment between image and text features in the pretrained CLIP model, leading to sub-optimal performance, especially for novel unseen classes. It also increases the training costs by  to  GPU days.
Interestingly, our experiments also show that a frozen CLIP backbone can provide sufficient features for mask generation, while preserving the image-text aligned representation.
Nevertheless, we still face another challenge, where CLIP models are usually pretrained on low-resolution images (\eg, ), whereas segmentation models prefer higher-resolution inputs (\eg,  for COCO, or  for Cityscapes).
This discrepancy results in the significant performance degradation, when applying a frozen CLIP on large input images.
Digging into the details, we found that it is related to the popular ViT~\cite{dosovitskiy2020image} backbone used in CLIP that does not transfer well to different input sizes, which could be alleviated by extra careful designs (\eg, side adapter~\cite{chen2023vision,xu2023side}, or cost aggregation~\cite{zhou2022extract,cho2023cat}).
On the other hand, CNN-based CLIP models (such as ResNet~\cite{he2016deep} and ConvNeXt~\cite{liu2022convnet}) exhibit better generalization ability to different input sizes, due to their fully convolutional nature~\cite{long2014fully}.
Additionally, the CNN-based CLIP backbone, extracting multi-scale feature maps, can be used as a simple plug-in module into modern closed-vocabulary segmentation models~\cite{cheng2021masked,yu2022k}.
Motivated by the observations, we thus propose \modelname, a simple yet effective single-stage open-vocabulary segmentation framework built entirely on a \textit{single frozen convolutional} CLIP backbone :



\noindent \textbf{\modelname}\quad
The proposed \modelname leverages the semantic features of a frozen CNN-based CLIP backbone for both mask generation and CLIP classification.
Unlike previous works~\cite{xu2021simple,liang2022open,ding2022open,xu2023open}, which often train a separate mask generator and ignore the potential reuse of CLIP's semantic features, we incorporate the CNN-based CLIP backbone into the state-of-the-art segmentation method Mask2Former~\cite{cheng2021masked}.
We note that \modelname is a general meta-architecture that can build on top of several modern segmentation methods~\cite{cheng2021masked,yu2022k}.
Our approach offers several advantages.
By freezing and sharing the backbone features, our model is significantly more efficient during both training and testing (\ie, avoiding feature duplication).
The CNN-based CLIP backbone not only transfers well to different input resolutions (from its pretrained image size), but also generates multi-scale feature maps, seamlessly compatible with modern segmentation methods~\cite{cheng2021masked,yu2022k}.
At a high level, \modelname consists of three components: class-agnostic mask generator, in-vocabulary classifier, and out-of-vocabulary classifier. We detail each component below.

\noindent \textbf{Class-Agnostic Mask Generator}\quad
Following Mask2Former~\cite{cheng2021masked}, we use a pixel decoder enhanced with multi-scale deformable attention~\cite{zhu2020deformable} to improve the features extracted from the frozen CNN-based CLIP backbone.
The enhanced pixel features, together with a set of object queries~\cite{carion2020end, wang2021max}, are then passed through a series of mask decoders, where each consists of masked cross-attention~\cite{cheng2021masked}, self-attention~\cite{vaswani2017attention}, and a feed-forward network.
The resulting segmentation logits are obtained by performing a matrix multiplication between the object query and pixel features.
The predicted masks are matched with ground-truth masks in a one-to-one manner through Hungarian matching~\cite{Kuhn1955NAVAL} and are supervised accordingly.
Moreover, as the number of object queries is often greater than the number of labeled masks, only a subset of predicted masks are optimized through this matching process. We apply no penalty to the remaining unmatched proposals, which ensures that more mask proposals are obtained.

\noindent \textbf{In-Vocabulary Classifier}\quad
Once the mask proposals are predicted, they are classified with category text embedding in a contrastive manner, where the class embeddings for each mask and category text embeddings are projected into a common embedding space. That is, the predicted class probability by in-vocabulary classifier is defined as follows:  

where  is a learnable temperature parameter with initialization of  to control the sharpness of the distribution,  is cosine distance measurement,  is the class embeddings for -th predicted mask, which is obtained by mask pooling over the \textit{final pixel features from pixel decoder}, similar to~\cite{ghiasi2022scaling}.
 is the category name's text embeddings of class , which is obtained by feeding the category name to a CLIP-pretrained text encoder.
Note that these category text embeddings only need to be generated once.
They are then kept in memory to serve as text classifiers, and thus it incurs negligible additional cost during training. This forms our in-vocabulary classifier.

\noindent \textbf{Out-of-Vocabulary Classifier}\quad
During inference, however, we notice that using the in-vocabulary classifier alone fails to generalize to completely novel unseen classes, as the model is only trained on a finite set of categories and thus could not recognize diverse novel concepts.
To address this issue, we introduce an out-of-vocabulary classifier, which applies mask pooling to the \textit{frozen CLIP backbone features}, aiming to borrow the pretrained (intact) open-vocabulary recognition ability from CLIP.
Unlike the other two-stage methods~\cite{xu2021simple,liang2022open,ding2022open,xu2023open}, where one or multiple forward processes of CLIP are needed, the adopted out-of-vocabulary classifier introduces marginal additional costs, since the backbone features are already extracted (and only lightweight mask-pooling is performed).
The predicted class probability by out-of-vocabulary classifier  is then obtained in a manner similar to~\equref{equ:in_voc_classifier} by replacing  with the mask-pooled features over \textit{frozen CLIP backbone features}.
This classifier strictly maintains the original CLIP feature distribution, allowing us to better recognize brand new categories.
Note that the out-of-vocabulary classifier is only performed during testing.

\noindent \textbf{Combining In- and Out-of-Vocabulary Classifiers}\quad
Following prior works~\cite{gu2021open,ghiasi2022scaling,kuo2022f,xu2023open}, we employ geometric ensemble to fuse the classification scores between in-vocabulary and out-of-vocabulary classifiers.
That is, 

where  denotes the -th element of , and the underscripts  and  refer to in-vocabulary and out-of-vocabulary classifier, respectively.  balance the predictions between in- and out-of-vocabulary classifiers for seen and novel unseen categories. \section{Experimental Results}
\label{sec:experiments}
Herein, we provide implementation details of \modelname in~\secref{sec:implementation}.
After setting the stage, we introduce our main results, compared with state-of-the-art methods and ablations studies in~\secref{sec:main_results}.


\subsection{Implementation Details}
\label{sec:implementation}
\noindent \textbf{Architecture}\quad
We use ConvNeXt-Large CLIP~\cite{liu2022convnet,radford2021learning} backbones from OpenCLIP~\cite{ilharco_gabriel_2021_5143773}\footnote{\url{https://github.com/mlfoundations/open_clip}} pretrained on LAION-2B~\cite{schuhmann2022laion} dataset.
On top of the CLIP backbone, we build the mask generator, following Mask2Former~\cite{cheng2021masked}.
Nine mask decoders are employed to generate the class-agnostic masks by taking as inputs the enhanced pixel features and a set of object queries.
For in-vocabulary classification, following~\cite{ghiasi2022scaling}, the class embeddings are obtained by mask-pooling the pixel features from the pixel decoder's final output.
Afterwards, the classification logits (before softmax) is obtained by matrix multiplication between the predicted class embeddings and categories' text embeddings.

\noindent \textbf{Training Strategy}\quad
We follow~\cite{cheng2021masked} and adopt the same training recipe and losses without any special design. 
The training is optimized with AdamW~\cite{kingma2014adam,loshchilov2017decoupled} optimizer and weight decay . We use a crop size of .
We employ the learning rate  and a multi-step decay schedule.
The training batch size is , and the model is trained for  epochs on COCO panoptic training set~\cite{lin2014microsoft}.

\noindent \textbf{Inference Strategy}\quad
During inference, the shorted side of input images will be resized to  while ensuring longer side not exceeds . For Cityscapes and Mapillary Vistas, we increase the shorter side size to . We adopt mask-wise merging scheme~\cite{cheng2021masked} for the mask predictions.
The out-of-vocabulary classifier is only performed during inference by mask-pooling over the frozen CLIP backbone features.
The final classification results are then obtained by geometric ensembling in- and out-of-vocabulary classifiers~\cite{gu2021open,ghiasi2022scaling,kuo2022f,xu2023open}, as in~\equref{equ:geometric_ensemble}, where we default  and .
Following prior arts, we also adopt prompt engineering from~\cite{ghiasi2022scaling,xu2023open} and prompt templates from~\cite{gu2021open,liang2022open}.
If not specified, \modelname is only trained on COCO panoptic dataset~\cite{lin2014microsoft}.
Following prior works~\cite{ghiasi2022scaling,xu2023open}, we zero-shot evaluate the model on ADE20K~\cite{zhou2017scene}, Cityscapes~\cite{Cordts2016Cityscapes}, and Mapillary Vistas~\cite{neuhold2017mapillary} for open-vocabulary panoptic segmentation.
We also report open-vocabulary semantic segmentation results on those datasets along with PASCAL datasets~\cite{everingham2010pascal,mottaghi2014role}.
The panoptic segmentation results are evaluated with the panoptic quality (PQ)~\cite{kirillov2018panoptic}, Average Precision (AP), and mean intersection-over-union (mIoU), and semantic segmentation is evaluated with mIoU~\cite{everingham2010pascal}.
Note that all results are obtained with the same single checkpoint trained on COCO panoptic data only.

\begin{table*}[!t]
\small
\centering
\tablestyle{4pt}{1.05}
\caption{
    \label{tab:panoptic_similar}
    \textbf{Open-vocabulary panoptic segmentation performance on ADE20K.}
    The proposed \modelname demonstrates better performances than prior arts, while using much fewer frozen parameters. We provide more results in the supplementary material
}
\begin{tabular}{l|cc|ccc|ccc}
& \multicolumn{2}{c|}{} & \multicolumn{3}{c|}{zero-shot test dataset}      & \multicolumn{3}{c}{training dataset}                      \\

                            & \multicolumn{2}{c|}{params (M)} & \multicolumn{3}{c|}{ADE20K}                     & \multicolumn{3}{c}{COCO}                      \\
method                      & frozen  & trainable   & PQ            & AP   & mIoU     & PQ     & AP     & mIoU          \\
\shline
MaskCLIP~\cite{ding2022open} & 304  &     63      & 15.1  & 6.0             & 23.7      & -     & -  & -           \\
FreeSeg~\cite{qin2023freeseg} & -  &     -      & 16.3   & 6.5             & 24.6         & -  & -  & -           \\
ODISE~\cite{xu2023open}   & 1494    &     28      & 22.6    & 14.4      & 29.9  & 55.4 & 46.0   & 65.2 \\
ODISE~\cite{xu2023open} (caption)   &  1494        & 28      & 23.4 & 13.9  & 28.7 & 45.6 & 38.4  & 52.4  \\ 
\hline \hline
\modelname (ours) &  200  & 21    & 26.8 & 16.8 & 34.1  & 54.4 & 44.6  & 63.7

\end{tabular}
\end{table*}

\begin{table*}[!t]
\small
\centering
\tablestyle{4pt}{1.05}
\caption{
    \label{tab:panoptic_cross}
    \textbf{Open-vocabulary panoptic segmentation performance on street-view datasets}.
    The proposed \modelname demonstrates better transferability to street-view dataset
}
\begin{tabular}{l|cccc|ccccc}
& \multicolumn{9}{c}{zero-shot test dataset}   \\

                              & \multicolumn{4}{c|}{Mapillary Vistas} & \multicolumn{5}{c}{Cityscapes}            \\
method                     & PQ          & SQ & RQ  & mIoU  & PQ          & SQ  & RQ  & AP & mIoU            \\
\shline
ODISE~\cite{xu2023open}   & 14.2 & 61.0 & 17.2 & - & 23.9 & 75.3 & 29.0  & -   & -  \\ 
\hline \hline
\modelname (ours) & 18.2 & 57.7 & 22.9 & 27.9  & 44.0 & 75.4 & 53.6   & 26.8   & 56.2 
\end{tabular}
\end{table*}

\subsection{Results}
\label{sec:main_results}
We summarize the main results for open-vocabulary panoptic segmentation and semantic segmentation in~\tabref{tab:panoptic_similar},~\tabref{tab:panoptic_cross} and~\tabref{tab:semantic}, where we train \modelname on COCO \textit{train} set with panoptic annotation and evaluate it on various datasets in a zero-shot manner.

\noindent \textbf{Open-Vocabulary Panoptic Segmentation Evaluation on ADE20K}\quad
In~\tabref{tab:panoptic_similar}, we compare our \modelname with other state-of-the-art methods on ADE20K~\cite{zhou2017scene}, the main test-bed of zero-shot open-vocabulary panoptic segmentation.
As shown in the table, our method achieves significantly better performance compared to MaskCLIP~\cite{ding2022open}, with  PQ,  AP and  mIoU, even though we use fewer frozen (M) and trainable (M) parameters.
When compared to the concurrent methods FreeSeg~\cite{qin2023freeseg} and ODISE~\cite{xu2023open}, the advantage of \modelname persists.
\modelname is  PQ,  AP, and  mIoU better than FreeSeg without using COCO-Stuff annotations~\cite{caesar2018coco} (which contains more semantic classes than COCO-Panoptic).
Our PQ, AP, mIoU score are also , ,  higher than ODISE under the same training settings.
Compared to ODISE with caption~\cite{chen2015microsoft} for supervision, our model still outperforms it by  PQ, setting a new state-of-the-art record. Meanwhile, it is noticeable that our model has  () significantly fewer frozen (total) parameters compared to ODISE, which utilizes a strong large backbone from stable diffusion~\cite{rombach2022high} for feature extraction.


\begin{table*}[!t]
\tablestyle{2pt}{1.02}
\caption{
    \label{tab:semantic}
    \textbf{Open-vocabulary semantic segmentation performance.}
    The proposed \modelname also demonstrates state-of-the-art performances on open-vocabulary semantic segmentation
}
\begin{tabular}{l|c|cccccc}
                                    &    & \multicolumn{6}{c}{mIoU}                                                                      \\
method                            & training dataset     & A-847         & PC-459        & A-150         & PC-59         & PAS-21  & PAS-20        \\
\shline
SPNet~\cite{xian2019semantic}         & Pascal VOC~\cite{everingham2010pascal}       & -             & -             & -             & 24.3          & 18.3       & -        \\
ZS3Net~\cite{bucher2019zero}     & Pascal VOC~\cite{everingham2010pascal}           & -             & -             & -             & 19.4          & 38.3       & -       \\
LSeg~\cite{li2022language}         & Pascal VOC~\cite{everingham2010pascal}          & -             & -             & -             & -             & 47.4      & -    \\
\hline
GroupViT~\cite{xu2022groupvit}     & GCC~\cite{sharma2018conceptual}+YFCC~\cite{thomee2016yfcc100m}     & 4.3           & 4.9           & 10.6          & 25.9          & 50.7        & 52.3     \\
\hline
SimBaseline~\cite{xu2021simple}    & COCO Stuff~\cite{caesar2018coco}              & -             & -             & 15.3          & -             & 74.5       & -     \\
ZegFormer~\cite{ding2022decoupling} & COCO Stuff~\cite{caesar2018coco}                & -             & -             & 16.4          & -             & 73.3    & -       \\
LSeg+~\cite{li2022language,ghiasi2022scaling}        & COCO Stuff~\cite{caesar2018coco}              & 3.8           & 7.8           & 18.0          & 46.5          & -         & -    \\
OVSeg~\cite{liang2022open}      & COCO Stuff~\cite{caesar2018coco}           & 9.0           & 12.4           & 29.6          & 55.7          & -         & 94.5      \\
SAN~\cite{xu2023side}      & COCO Stuff~\cite{caesar2018coco}           & 13.7           & 17.1           & 33.3          & 60.2          & -        & 95.5        \\
\hline
OpenSeg~\cite{ghiasi2022scaling}      & COCO Panoptic + COCO Caption          & 6.3           & 9.0           & 21.1          & 42.1          & -         & -       \\
ODISE~\cite{xu2023open} (caption)        & COCO Panoptic + COCO Caption      & 11.0 & 13.8 & 28.7 & 55.3 & 82.7   & -  \\
\hline
MaskCLIP~\cite{ding2022open}       & COCO Panoptic              & 8.2           & 10.0          & 23.7          & 45.9          & -         & -     \\
ODISE~\cite{xu2023open}        & COCO Panoptic            & 11.1 & 14.5 & 29.9 & 57.3 & 84.6   & -  \\
\hline \hline
\modelname (ours)        & COCO Panoptic      & 14.8 & 18.2 & 34.1 & 58.4 & 81.8   & 95.4  \\
\end{tabular}

\end{table*}


\begin{table}[!t]
\tablestyle{6pt}{1.1}
\caption{
\textbf{FPS comparison.} All results are obtained with one V100 GPU, CUDA 11.6 and PyTorch 1.13, by taking the average runtime on the entire validation set, including post-processing time
}
\label{tab:fps_comp}

\begin{tabular}{l|ccc}
   method                    & ADE20K            & COCO            \\
   \shline
   ODISE~\cite{xu2023open} & 0.41 & 0.39   \\
   \hline \hline


   \modelname (ours)  & 2.71 () & 2.76 ()
\end{tabular}
\end{table}


\noindent \textbf{Open-Vocabulary Panoptic Segmentation Evaluation on Street-View Datasets}\quad
In~\tabref{tab:panoptic_cross}, we evaluate on Cityscapes and Mapillary Vistas, which focus on street driving scenes.
Compared to state-of-the-art method ODISE, \modelname achieves better performances on both datasets. Specifically, it outperforms ODISE by  PQ and  PQ on Mapillary Vistas and Cityscapes, respectively.
Notably, \modelname has a slightly lower SQ, which indicates our mask generator is actually weaker than the one in ODISE, which utilizes a much larger backbone.


\noindent \textbf{Open-Vocabulary Semantic Segmentation Evaluation}\quad
Although our model was trained on COCO panoptic data only, it also performs well on open-vocabulary semantic segmentation. In~\tabref{tab:semantic}, we report our model's performance on various benchmarks against other open-vocabulary segmentation models, where \modelname shows an overall superior performance. Specifically, with the same training annotations used, \modelname outperforms MaskCLIP by , , ,  mIoU across A-847, PC-459, A-150, and PC-59, respectively. Compared to methods with caption annotations, \modelname persists its advantages, where it outperforms ODISE (caption) by , , ,  mIoU across datasets A-847, PC-459, A-150, PC-59 respectively.
Against other open-vocabulary semantic segmentation methods, our model maintains its advantages across different datasets, despite being trained solely with panoptic annotations. Furthermore, it demonstrates comparable performance to state-of-the-art open-vocabulary semantic segmentation methods, which utilize the COCO-Stuff dataset as their training set. The COCO-Stuff dataset comprises 171 classes, 38 more classes than COCO-Panoptic, and offers highly desirable annotations for semantic segmentation tasks. It is worth mentioning that these methods build their approach on top of ViT-L (with extra designs~\cite{xu2023side}), resulting in a significantly larger model size compared to our deployed ConvNeXt-L (304M \vs 198M). Despite the disparity in model size, \modelname remains competitive in terms of performance. Specifially, \modelname outperforms state-of-the-art open-vocabulary semantic segmentation method SAN~\cite{xu2023side} by  and  mIoU on the challenging A-847 and PC-459 datasets. 

\noindent \textbf{Inference Speed}\quad
We provide a comparison of FPS (frames per second) in~\tabref{tab:fps_comp}. The proposed \modelname not only demonstrates superior performances, but also enjoys a significant fast inference time: \modelname runs  and  faster than ODISE evaluated on ADE20K and COCO datasets, respectively.


\noindent \textbf{Training on ADE20K and Evaluating on COCO}\quad
We further validate the effectiveness of \modelname by using a different training dataset.
Specifically, we follow~\cite{qin2023freeseg,xu2023open} to train our model on ADE20K dataset with panoptic annotation, and evaluate it on COCO panoptic dataset. As shown  in~\tabref{tab:train_with_ade},
\modelname outperforms FreeSeg~\cite{qin2023freeseg} by  PQ, and ODISE~\cite{xu2023open} by  PQ on COCO dataset.
Notably, our model actually has a lower SQ () compared to ODISE, which utilizes a much larger backbone and thus has a stronger mask generator.
Nevertheless, \modelname still outperforms ODISE significantly with a simple yet effective design.



\begin{table}[!t]
\tablestyle{6pt}{1.1}
\caption{
\textbf{Results of training on ADE20K panoptic and evaluating on COCO panoptic val set.} The proposed \modelname performs better than prior arts, even in the different setting (\ie, trained on ADE20K and zero-shot evaluated on COCO)
}
\label{tab:train_with_ade}
\begin{tabular}{l|ccc|ccc}

& \multicolumn{3}{c|}{zero-shot test dataset}                & \multicolumn{3}{c}{training dataset}          \\

                              & \multicolumn{3}{c|}{COCO}                & \multicolumn{3}{c}{ADE20K}          \\
   method                    & PQ            & SQ            & RQ            & PQ            & SQ            & RQ            \\
   \shline
   FreeSeg~\cite{qin2023freeseg} & 16.5 & 72.0 & 21.6 & - & - & - \\
   ODISE~\cite{xu2023open} & 25.0 & 79.4 & 30.4 & 31.4 & 77.9 & 36.9 \\
   \hline \hline
   \modelname (ours)  & 27.0 & 78.0 & 32.9 & 41.9 & 78.2 & 50.2 \\ 
\end{tabular}
\end{table}

\definecolor{myred}{RGB}{194,11,8}
\definecolor{mygreen}{RGB}{117,166,100}

\begin{figure}[!t]
    \centering
    \vspace{-12ex}
    \includegraphics[width=0.9\linewidth]{figures/chart3.pdf}
    \vspace{-15ex}
    \caption{
    \textbf{Trainable CLIP \vs Frozen CLIP, with per-class PQ analysis.} We show 10 common classes (labeled in \textcolor{mygreen}{green}) shared by COCO and ADE20K, and 10 novel classes (labeled in \textcolor{myred}{red}) that are only in ADE20K. The frozen CLIP demonstrates a much better recognition ability for novel classes, while performing similarly for the seen classes.
    }
    \label{fig:frozen_vs_trainable}
\end{figure}


\noindent \textbf{Fine-tuning CLIP Backbone Harms Performance on Novel Vocabularies}\quad
We validate the necessity of freezing CLIP backbone to ensure a better generalization to novel vocabularies. We compare the performance of trainable CLIP variant and frozen CLIP variant in~\figref{fig:frozen_vs_trainable}, where we use the same mask proposals to ensure a fair comparison. Specifically, we compare the performance on 10 seen classes, which are shared by both COCO and ADE20K (\eg, person, sky), and 10 unseen classes, which are only included in ADE20K dataset (\eg, arcade machine, dishwasher). As shown in the figure, tuning CLIP backbone leads to a worse performance on unseen concepts, which breaks the CLIP feature alignment and thus loses its recognition ability on a much wider vocabulary. \vspace{-1ex}
\section{Conclusion}
\label{Conclusions}
\vspace{-0.5ex}
In this work, we have presented \modelname, a simple yet effective single-stage framework for open-vocabulary segmentation.
\modelname shows great potential by building everything on top of a shared frozen convolutional CLIP backbone, which not only significantly reduces training and testing costs, but also establishes a strong baseline on multiple benchmarks.
Our study demonstrates how to better adapt a pretrained CLIP model for downstream dense prediction tasks, which we hope will shed the light on unleashing CLIP's potential for other various downstream tasks.

\vspace{-0.5ex}
\noindent \textbf{Limitations}\quad
\modelname presents a simple single-stage open-vocabulary segmentation framework with state-of-the-art performance. We note that there exist some interesting research topics to be explored in the near future, such as better unleashing CLIP's potential in both mask segmentation and classification, how to deal with conflict or overlapping vocabularies (\eg, cat \vs cat head), \etc.

\vspace{-0.5ex}
\noindent \textbf{Broader Impact}\quad
\modelname shows great potential for segmenting and naming every object in the scene, which could facilitate many applications including intelligent home assistants, robots, self-driving, \etc. Yet it relies on CLIP model pre-trained on the Internet data that may be biased, which calls for future research for calibration to avoid misuse. 
\clearpage
{\small
\bibliographystyle{plainnat}
\bibliography{egbib}
}


\clearpage


\noindent \textbf{Appendix}\quad
In the following supplementary materials, we present additional experimental results pertaining to the design of \modelname. Our supplementary analysis also includes comparisons against other methods that specifically address open-vocabulary semantic segmentation, ensemble methods, and hyperparameter tuning. Furthermore, we provide a quantitative comparison between ViT-based CLIP and CNN-based CLIP across varying input sizes, along with additional visualizations and comprehensive dataset details.


\section{Additional Experimental Results}
\noindent \textbf{Fine-tuning or Freezing CLIP Backbone in \modelname}\quad
In this study, we provide a comprehensive analysis of the impact of fine-tuning or freezing the CLIP backbone in our framework. We specifically focus on the PQ\textsuperscript{seen} and PQ\textsuperscript{unseen} metrics, which evaluate the performance for classes that overlap and do not overlap between the training and testing datasets, respectively.
To determine whether a class is seen or unseen, we adopt the prompt engineering technique described in~\cite{ghiasi2022scaling}, which provides synonyms or
subcategories of classes. Specifically, if any category name in test dataset overlaps with a category name in training dataset, we consider it as a seen class; otherwise unseen.
As discussed in the main paper, the proposed \modelname contains three components: a class-agnostic mask generator, an in-vocabulary classifier, and an out-of-vocabulary classifier.
We thus explore using frozen or trainable CLIP for each component, and summarize the results in~\tabref{tab:frozen_finetune_backbone}.
To ensure a fair comparison, all "trainable" modules utilize the same weights, resulting in identical mask proposals and in-vocabulary classification results.
Moreover, we note that the first row in~\tabref{tab:frozen_finetune_backbone} with trainable mask generator and in-vocabulary classifier, can be considered as an approximation to OpenSeg~\cite{ghiasi2022scaling} in our framework.
Our findings reveal that an in-vocabulary classifier built upon a trainable CLIP backbone achieves a higher PQ\textsuperscript{seen} score ( compared to ), but experiences a decrease in PQ\textsuperscript{unseen} ( compared to ) compared to a frozen out-of-vocabulary classifier.
Consequently, a model that incorporates a trainable CLIP backbone for all components yields a PQ of , which is  lower than our final model (last row) that relies on a single frozen CLIP backbone.
Using a trainable mask generator and in-vocabulary classifier, along with a frozen out-of-vocabulary classifier boosts the performance but requires maintaining one trainable and one frozen CLIP weights, resulting in  more backbone parameters.
In summary, our observations demonstrate that building the entire framework upon a frozen CLIP backbone is not only effective but also efficient, providing a better balance between PQ\textsuperscript{seen} and PQ\textsuperscript{unseen} metrics.


\begin{table}[h]
\tablestyle{4pt}{1.05}
\caption{
    \label{tab:frozen_finetune_backbone}
    \textbf{Effects of fine-tuning or freezing the CLIP backbone for each module in \modelname.}
    Building all three modules upon a single frozen CLIP backbone attains best performance.
    Note that our mask generator and in-vocabulary classifier use the same backbone following~\cite{cheng2021masked,ghiasi2022scaling,yu2022k}, and thus it is infeasible (denoted as N/A) for the setting in the 2nd last row. Our final setting is labeled in gray
}
\begin{tabular}{ccc|ccc}
mask generator & in-vocabulary classifier & out-of-vocabulary classifier & PQ & PQ\textsuperscript{seen} & PQ\textsuperscript{unseen} \\
\shline

trainable & trainable & - & 17.7 & 37.9 & 2.6 \\
trainable & - & frozen & 21.1 & 32.4 & 12.6 \\
trainable & trainable & trainable & 24.1 & 38.9 & 13.1 \\
trainable & trainable & frozen & 25.4 & 40.0 & 14.6 \\
\nabaseline{trainable} & \nabaseline{frozen} & \nabaseline{frozen} & \nabaseline{N/A} & \nabaseline{N/A} & \nabaseline{N/A} \\
\baseline{frozen} & \baseline{frozen} & \baseline{frozen} & \baseline{26.8} & \baseline{39.5} & \baseline{17.3} \\
\end{tabular}
\end{table}


\noindent \textbf{Evaluation with Grounding PQ and Grounding mIoU}\quad
It is worth emphasizing that despite the absence of grounding loss~\cite{gupta2020contrastive,zareian2021open,ghiasi2022scaling,xu2023open} during training, our model exhibits exceptional grounding segmentation capabilities. \tabref{tab:grounding} presents the grounding PQ and grounding mIoU scores of \modelname, following the evaluation methodology outlined in~\cite{ghiasi2022scaling}. In this evaluation, we exclusively employ ground-truth classes as text query inputs to assess the effectiveness of concept grounding.
Compared to OpenSeg~\cite{ghiasi2022scaling}, \modelname achieves a substantial performance improvement, with notable enhancements of , , , and  on A-847, PC-459, A-150, and PC-59, respectively. Even when compared to OpenSeg trained with the Localized Narrative dataset~\cite{pont2020connecting}, which enables training on a significantly larger vocabulary, \modelname still surpasses it with improvements of , ,  and  on A-847, PC-459, A-150 and PC-59, respectively, underscoring the grounding proficiency of \modelname.



\begin{table*}[!t]
\tablestyle{1pt}{1.05}
\caption{
    \label{tab:grounding}
    \textbf{Grounding segmentation performance.}
    The proposed \modelname also demonstrates state-of-the-art performances on grounding segmentation. MV: Mapillary Vistas
}
\begin{tabular}{l|ccc|cccccc}
                                    &  \multicolumn{3}{c}{grounding PQ}      & \multicolumn{6}{c}{grounding mIoU}                                                                      \\
method                            & ADE20K & Cityscapes & MV    & A-847         & PC-459        & A-150         & PC-59         & PAS-21    & PAS-20       \\
\shline
ALIGN~\cite{jia2021scaling,ghiasi2022scaling}      & - & - & -          & 17.8           & 21.8           & 25.7          & 34.2          & -      & -            \\
ALIGN w/ proposal~\cite{jia2021scaling,ghiasi2022scaling}      & - & - & -          & 17.3           & 19.7           & 25.3          & 32.0          & -       & -           \\
LSeg+~\cite{li2022language,ghiasi2022scaling}      & - & - & -           & 10.5           & 17.1           & 30.8          & 56.7          & -     & -             \\
OpenSeg~\cite{ghiasi2022scaling}      & - & - & -           & 21.8           & 32.1           & 41.0          & 57.2          & -       & -           \\
OpenSeg~\cite{ghiasi2022scaling} w/ L. Narr      & - & - & -           & 25.4           & 39.0           & 45.5          & 61.5         & -       & -           \\

\hline \hline

\modelname (ours)        &  38.4 & 48.1 & 21.5    & 33.4 & 41.2 & 54.1 & 74.9 & 88.7 & 98.5    \\
\end{tabular}


\end{table*}

\noindent \textbf{Ensemble In-Vocabulary and Out-of-Vocabulary Classifiers}\quad
In~\tabref{tab:ensemble_ablate}, we present experiments conducted to evaluate the impact of ensemble methods and ensemble parameters on the performance of the in-vocabulary and out-of-vocabulary classifiers. Specifically, we examine two ensemble methods: arithmetic and geometric. The arithmetic method involves a linear combination of the in-vocabulary classifier and the out-of-vocabulary classifier, while the geometric method is defined as shown in Equation~(7) of main paper.
It is worth noting that \modelname exhibits robustness to different ensemble methods, with both methods displaying a consistent trend within the explored hyper-parameter ranges. However, the geometric ensemble consistently outperforms the arithmetic ensemble by a slight margin. Additionally, we observe that preference is given to values of  and , which biases the model towards using the in-vocabulary classifier for seen classes and the out-of-vocabulary classifier for unseen classes.
We also explore extreme cases, including  and  (i.e., exclusively utilizing the in-vocabulary classifier for every class),  and  (i.e., exclusively utilizing the out-of-vocabulary classifier for every class),  and  (i.e., using the in-vocabulary classifier for seen classes and the out-of-vocabulary classifier for unseen classes), and  and  (i.e., using the out-of-vocabulary classifier for seen classes and the in-vocabulary classifier for unseen classes). The results align with our observations that it is preferable to bias towards the in-vocabulary classifier for seen classes and the out-of-vocabulary classifier for unseen classes.






\begin{table}[!t]
\tablestyle{6pt}{1.1}
\caption{
\textbf{Ensemble methods comparison with zero-shot evaluation (PQ) on ADE20K.}
Our method is robust to different ensemble methods (arithmetic and geometric).
The results show that it is preferable to bias towards using the in-vocabulary classifier for seen classes and the out-of-vocabulary classifier for unseen classes. Our final setting () is labeled in gray
}
\label{tab:ensemble_ablate}
\begin{tabular}{l|cc}
   method                    & arithmetic             & geometric            \\
   \shline
   () & 17.8 & 17.8   \\
   () & 21.9 & 21.9   \\
   () & 25.3 & 25.3   \\
   () & 17.5 & 17.5   \\
   () & 25.0 & 25.3   \\
   () & 25.6 & 26.4   \\
   () & 25.5 & 26.7   \\
   () & 25.4 & 26.6  \\
   () & 25.1 & 25.6   \\
   () & 25.6 & 26.4   \\
   \baseline{()} & \baseline{25.6} & \baseline{26.8}   \\
   () & 25.4 & 25.8   \\
   
\end{tabular}
\end{table}

\begin{table}[!t]
\tablestyle{2pt}{1.1}
\caption{
\textbf{Quantitative results of ViT-based CLIP and CNN-based CLIP when input size (denoted as "res") varies for panoptic segmentation on COCO and ADE20K.} All results are obtained by applying CLIP directly as a mask classifier with the same mask proposals from ODISE~\cite{xu2023open}
}
\label{tab:vit_vs_cnn_clip}
\begin{tabular}{l|ccccc|ccccc}
                              & \multicolumn{5}{c|}{COCO PQ @res}                & \multicolumn{5}{c}{ADE20K PQ @res}          \\
   CLIP backbone                    & 224            & 448            & 672   & 896 &  1120      & 224            & 448            & 672    & 896 & 1120        \\
   \shline
   ViT-L/14 & 19.3 & 22.5 & 20.6  & 18.5 & 14.9 & 11.9 & 13.7 & 12.6 & 11.6 & 9.1 \\ ConvNeXt-L  & 17.3 & 23.5 & 27.0 & 28.6 & 29.3 & 9.3 & 12.8 & 14.8 & 16.0 & 15.9
\end{tabular}
\end{table}

\begin{table}[!t]
\tablestyle{2pt}{1.3}
\caption{
\textbf{Quantitative results of ViT-based CLIP and CNN-based CLIP when input size (denoted as "res") varies for ImageNet-1k classification.}
}
\label{tab:vit_vs_cnn_clip_in1k}
\begin{tabular}{l|ccccccc}
                              & \multicolumn{7}{c}{Accuracy @res}                     \\
   CLIP backbone                    & 224            & 336            & 448   & 560 &  672      & 784            & 896    \\
   \shline
   ViT-L/14 & 75.3 & 74.3 & 71.3  & 67.5 & 63.1 & 58.5 & 53.9 \\
   ConvNeXt-L  & 75.1 & 77.1 & 76.8 & 74.2 & 69.8 & 65.6 & 58.4
\end{tabular}
\end{table}

\noindent \textbf{Quantitative ViT-based CLIP \vs CNN-based CLIP when Input Size Scales}\quad
Training our model solely with ViT-based CLIP, without any additional modifications~\cite{zhou2022extract,chen2023vision,xu2023side,cho2023cat}, is infeasible. Furthermore, applying ViT to large input sizes is computationally expensive.
Therefore, to evaluate the effects of using ViT- or CNN-based CLIP in our framework, we incorporate them into our out-of-vocabulary classifier, which is performed only during inference.
To ensure a fair comparison, we use the same mask proposals and disable the geometric ensemble scheme. We also perform experiment on the ImageNet~\cite{russakovsky2015imagenet} benchmark to ensure a comprehensive comaprison.
In~\tabref{tab:vit_vs_cnn_clip} and~\tabref{tab:vit_vs_cnn_clip_in1k}, we conduct an ablation study to analyze the impact of different input resolutions for CLIP models.
We consider both ViT-based (ViT-L/14) and CNN-based (ConvNeXt-L) CLIP models.
By employing them as zero-shot classifiers and varying the input resolutions, we observe that CNN-based CLIP demonstrates superior generalization ability as the input size scales up.
Specifically, we observe that the ViT-L/14 CLIP has a higher PQ and Accuracy at a lower resolution (\ie, input size 224), but suffers from a higher resolution, which leads existing two-stage methods~\cite{xu2021simple,liang2022open,ding2022open,xu2023side,xu2023open} to adopt different input resolutions for mask generator and classifier branches.
On the contrary, \modelname provides a simple solution by adopting a CNN-based CLIP that generalizes well to different input sizes.

\noindent \textbf{\modelname with Different Backbones and Different Segmentation Frameworks}\quad
Though we majorly report \modelname results with ConvNeXt-L~\cite{liu2022convnet,radford2021learning} backbone in Mask2Former~\cite{cheng2021masked} framework. We note that \modelname can be easily incorporated with different backbones and segmentation frameworks. Specifically, we experiment \modelname with different backbones (\eg, ResNet~\cite{he2016deep}) and different segmentation architecture (\eg, kMaX-DeepLab~\cite{yu2022k}). As shown in \tabref{tab:supp_panoptic}, \modelname demonstrates superior performance across different backbones and frameworks.

\begin{table*}[!t]
\small
\centering
\tablestyle{4pt}{1.05}
\caption{
    \label{tab:supp_panoptic}
    \textbf{Open-vocabulary segmentation performance with different backbones and segmentation frameworks.} All models are trained on COCO and tested on the other datasets in a zero-shot manner. MV: Mapillary Vistas. : kMaX-DeepLab with multi-scale deformable attention~\cite{zhu2020deformable}
}
\begin{tabular}{l|c|ccc|ccccc}
                           & \multicolumn{1}{c|}{} & \multicolumn{3}{c|}{panoptic datasets (PQ)}   & \multicolumn{4}{c}{semantic datasets (mIoU)}                                \\
method      & backbone                & ADE            & Cityscapes   & MV   & A-847 & PC-459 & PC-59 & PAS-21             \\
\shline
\modelname & R50~\cite{he2016deep,radford2021learning}   & 17.9 & 40.3 & 15.9  & 7.1 & 12.9 & 50.5 & 75.9 \\
\modelname & R101~\cite{he2016deep,radford2021learning}   & 19.1 & 40.9 & 16.7  & 7.7 & 12.3 & 48.9 & 77.6 \\
\modelname & R504~\cite{radford2021learning}    & 21.8 & 42.2 & 17.4  & 8.7 & 13.1 & 54.0 & 79.0 \\
\modelname & R5016~\cite{radford2021learning}    & 22.5 & 42.0 & 17.8  & 10.3 & 15.7 & 56.4 & 80.7 \\
\modelname  & R5064~\cite{radford2021learning}   & 22.8 & 42.7 & 18.2  & 10.8 & 16.2 & 55.7 & 80.3  \\
\hline 
\modelname w/ kMaX & ConvNeXt-L~\cite{liu2022convnet,ilharco_gabriel_2021_5143773}    & 24.5 & 43.0 & 17.0 & 11.4 & 15.0 & 57.4 & 84.7 \\
\modelname w/ kMaX & ConvNeXt-L~\cite{liu2022convnet,ilharco_gabriel_2021_5143773}  & 26.4 & 40.2 & 17.4 & 13.6  & 17.5 & 57.1  & 81.2 \\
\hline 
\modelname & ConvNeXt-L~\cite{liu2022convnet,ilharco_gabriel_2021_5143773}   & 26.8 & 44.0 & 18.2  & 14.8 & 18.2 & 58.4 & 81.8 \\

\end{tabular}
\end{table*}


\noindent \textbf{Visualization}\quad
We provide visualization on ADE20K \textit{val} set in~\figref{fig:ade20k_vis}.

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/vis.pdf}
    \vspace{-12ex}
    \caption{
    \textbf{Visualization examples of \modelname on ADE20K \textit{val} set.} \modelname is trained on COCO panoptic training set and zero-shot evaluated on ADE20K validation set.
    }
    \label{fig:ade20k_vis}
\end{figure}

\section{Datasets Information and Licenses}
The datasets we used for training and/or testing \modelname are described as follows.


\noindent\textbf{COCO:}\quad
We train \modelname on COCO data with panoptic annotation~\cite{lin2014microsoft}. We follow the 2017 splits which include  images for \textit{train} split and  images for \textit{val} split. If not specified, we train our model on the COCO \textit{train} split and report results on \textit{val} set of various datasets.

License: Creative Commons Attribution 4.0 License

URL: \url{https://cocodataset.org/#home}

\noindent\textbf{ADE20k:}\quad
ADE20k~\cite{zhou2017scene} covers a wide range of indoor and outdoor scenes, with  \textit{val} images. We evaluate \modelname on both the version with 847 classes (A-847) and the more widely-used version with 150 frequent categories (A-150).

License: Creative Commons BSD-3 License

URL: \url{https://groups.csail.mit.edu/vision/datasets/ADE20K/}

\noindent\textbf{Cityscapes:}\quad
Cityscapes~\cite{Cordts2016Cityscapes} focuses on semantic understanding of urban street scenes. We use the \textit{fine} data includes  images for validation set.

License: This dataset is made freely available to academic and non-academic entities for non-commercial purposes such as academic research, teaching, scientific publications, or personal experimentation.

URL: \url{https://www.cityscapes-dataset.com/}

\noindent\textbf{Mapillary Vistas:}\quad
Mapillary Vistas~\cite{neuhold2017mapillary} is a large-scale traffic-related dataset, including  images for validation purposes.

License: Creative Commons Attribution NonCommercial Share Alike (CC BY-NC-SA) license

URL: \url{https://www.mapillary.com/dataset/vistas}

\noindent\textbf{Pascal Context:} \quad
Pascal Context~\cite{mottaghi2014role} covers a wide variety of indoor and outdoor scenes and includes  \textit{val} images. We evaluate \modelname on both its full version (PC-459) with 459 classes and the more common version (PC-59) with 59 classes.

URL: \url{https://www.cs.stanford.edu/~roozbeh/pascal-context/}

\noindent\textbf{Pascal VOC:} \quad
Pascal VOC~\cite{everingham2010pascal} contains  \textit{val} images with 20 foreground classes and 1 background class. Due to the ambiguity in definition of ``background", we assign the background class to the pixels predicted as PC-59 categories that are not in Pascal VOC following~\cite{ghiasi2022scaling}, which leads to PAS-21. We also evaluate the model with background class excluded, which leads to PAS-20.

URL: \url{http://host.robots.ox.ac.uk/pascal/VOC/} 
\end{document}