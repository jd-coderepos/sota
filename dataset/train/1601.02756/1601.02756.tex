\documentclass{sig-alternate}
\let\proof\undefined
\let\endproof\undefined
\usepackage{amsthm,amssymb,amsmath}
\usepackage{graphics}
\usepackage{tikz}
\usepackage[plainpages=false,pdfpagelabels,colorlinks=true,citecolor=blue,hypertexnames=false]{hyperref}
\usepackage{color}

\def\eatspace#1{#1}
\def\step#1#2{\par\kern1pt\dimen44=#2em\advance\dimen44 1.67em\hangindent\dimen44\hangafter=1\noindent\rlap{\small#1}\kern\dimen44\relax\eatspace}

\newtheorem{theorem}{Theorem}
\newtheorem{prop}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{defi}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{fact}[theorem]{Fact}
\def\qed{\quad\rule{1ex}{1ex}}

\newcommand{\red}{\color{red}}
\newcommand{\blue}{\color{blue}}

\overfullrule=1ex

\let\set\mathbb
\def\lc{\operatorname{lc}}
\def\pa{\partial}
\def\d{\mathrm{d}}
\def\ord{\operatorname{ord}}
\def\ann{\operatorname{ann}}
\def\lcm{\operatorname{lcm}}



\boilerplate={}
\copyrightetc={}

\begin{document}

\title{Factorization of C-finite Sequences}

\numberofauthors{2}

\author{\alignauthor
 \leavevmode\mathstrut Manuel Kauers\thanks{partially supported by FWF grants F50-04 and Y464-N18.}\\medskipamount]
  \affaddr{\mathstrut Department of Mathematics}\\
  \affaddr{\mathstrut Rutgers University}\\
  \affaddr{\mathstrut New Brunswick, NJ, USA}\\
  \affaddr{\mathstrut zeilberg@math.rutgers.edu}
}

\maketitle
\begin{abstract}
  We discuss how to decide whether a given C-finite sequence
  can be written nontrivially as a product of two other
  C-finite sequences. 
\end{abstract}


\category{I.1.2}{Computing Methodologies}{Symbolic and Algebraic Manipulation}[Algorithms]


\terms{Algorithms}


\keywords{Factorization, Linear Recurrence, Computer Algebra}


\section{Introduction}

It is well known that when  and  are two
sequences that satisfy some linear recurrences with constant coefficients, then
the product sequence  also satisfies such a recurrence.
Sequences satisfying linear recurrences with constant coefficients are called
C-finite~\cite{zeilberger90,kauers10j,zeilberger13}, and the fact just refered to is one of several closure
properties that this class of sequences enjoys.  In this paper, we will consider
the inverse problem: given a C-finite sequence , can we
write it in a nontrivial way as the product of two other C-finite sequences?
This question is of interest in its own right, but it is also useful in some
applications in combinatorics.  For example, the celebrated solution by
Kasteleyn, and Temperley-Fisher, of the dimer problem~\cite{fisher61,kasteleyn61} as well as the even
more celebrated Onsager solution of the two-dimensional Ising model~\cite{onsager44}
can be (re)discovered using an algorithm for factorization of C-finite
sequences.

A C-finite sequence is uniquely determined by a recurrence and a choice
of sufficiently many initial values. The prototypical example of a C-finite
sequence is the Fibonacci sequence  defined by

Whether a C-finite sequence  admits a factorization depends
in general on both the recurrence as well as the initial values. For example,
the sequence , which satisfies the recurrence

can be factored as , while the sequence
, which satisfies the same recurrence, cannot be factored.

We shall consider a variant of the factorization problem that does not depend
on initial values but only on the recurrence equations. Linear recurrences
may be viewed as polynomials  acting on
sequences  via

For every fixed~, denote by  the set of all sequences
 with , i.e., the solution
space of the recurrence equation encoded by~.
This is a
vector space of dimension~. For any two operators  there
exists a unique monic polynomial  such that  is
vector space generated by all sequences 
with  and ,
i.e., . We write . 

Our problem shall be to decide, for a given monic polynomial ,
whether there exist  such that . In principle, it is
known how to do this. Singer~\cite{singer85a} gives a general algorithm for the
analogous problem for linear differential operators with rational function
coefficients, the problem is further discussed in~\cite{hessinger97}. Because of their
high cost, these algorithms are mainly of theoretical interest. For the special
case of differential operators of order 3 or~4 (still with rational function
coefficients), van Hoeij~\cite{hoeij07,hoeij02a} combines several observations to 
algorithms which handle these cases efficiently. For the recurrence case,
Cha~\cite{cha14} gives an algorithm for operators of order~3 with rational function
coefficients. An algorithm for the case of constant coefficients and arbitrary
order was recently sketched by the second author~\cite{zeilberger13}. This description
however only considers the ``generic case''. The present paper is a
continuation of this work in which we give a complete algorithm which also
handles ``degenerate'' cases. Our algorithm is efficient in the sense that it
does not require any Gr\"obner basis computation, but inefficient in the sense
that it requires a search that may take exponential time in the worst
case. 

\section{Preliminaries}

To fix notation, let us recall the basic facts about C-finite sequences.
Let  be an algebraically closed field. 

\begin{defi}
  \begin{enumerate}
  \item A sequence  is called \emph{C-finite,} if there exist
     with  such that for all  we have
    .
  \item In this case, the polynomial  is called a
    \emph{characteristic polynomial} for .
  \item For , the set  denotes the set of all C-finite sequences
    whose characteristic polynomial is~. It is called the \emph{solution space}
    of~.
  \end{enumerate}  
\end{defi}

\begin{theorem}\cite{stanley99,kauers10j}\label{thm:main}
  Let
  
  for pairwise distinct . Then 
  is the -vector space generated by the sequences
  
\end{theorem}

It is an immediate consequence of this theorem that for any two polynomials 
we have  and . The latter says in
particular that when  and  are C-finite, then so is
their sum . A similar result holds for the product: write
 and  and define

Then  is a characteristic polynomial for the product sequence .
Note that  for every .
Note also that  for every .

Our goal is to recover  and  from a given~.
The problem is thus to decide whether the roots of a given polynomial  are precisely
the pairwise products of the roots of two other polynomials  and~.
Besides the interpretation as a factorization of C-finite sequences, this problem can
also be viewed as factorization of algebraic numbers: given some algebraic number~,
specified by its minimal polynomial~, can we write  where 
are some other algebraic numbers with respective minimal polynomials  and~. 

Trivial decompositions are easy to find: For each  we obviously have .
Moreover, for every nonzero  we have , so we
can ``decompose''  into  and .
In order for a decomposition  to be interesting, we have to require that
both  and  have at least degree~2.

Even so, a factorization is in general not unique. Obviously, if  is a factorization,
then for any nonzero~ also .
Translated to sequences, this ambiguity corresponds to the facts that for every , both
 and  are C-finite, and that a sequence  is C-finite
iff for all  the sequence  is C-finite. 
But there is even more non-uniqueness: the polynomial

admits the two distinct factorizations

which cannot be obtained from one another by introducing factors  and
. Our goal will be to compute a finite list of factorizations
from which all others can be obtained by introducing factors
.

There is a naive but very expensive algorithm which does this job when  is
squarefree: For some choice  of degrees, make an ansatz
 and  with variables
.  Equate the coefficients of  with respect to  to zero and
solve the resulting system of algebraic equations for
.  After trying all possible degree
combinations  with , either a decomposition
has been found, or there is none.

\section{The Generic Case}

Typically, when  and  are square-free polynomials and 
are the roots of  and  are the roots of~, then the
products  for ,  will all be pairwise distinct.
In this case,  will have exactly  roots, and the factorization
problem consists in recovering  and 
from the (known) roots  of~.

As observed in~\cite{zeilberger13}, a necessary condition for  to admit a factorization
into two polynomials of respective degrees  and  is then that there is a
bijection  such that for
all  we have

and for all  we have

The explanation is simply that when a factorization exists, then the roots  of~
are precisely the products~, and if we define  so that it
maps each pair  to the corresponding root index~,
then the quotients

do not depend on  and the quotients

do not depend on~.

In fact, the existence of such a bijection  is also sufficient for the
existence of a factorization:
choose  arbitrarily and set  and 

and

Then we have  for all , and therefore 
for  and 
we have .
Note that  and  are squarefree, because if we have, say, 
for some , and then ,
and then , then .

\begin{example}
  \begin{enumerate}
  \item Consider , i.e., , , , .
    A possible choice for  is given by the table
    \begin{center}
      \begin{tabular}{c|cc}
         &  &  \\\hline
         &  &  \\
         &  & 
      \end{tabular}
    \end{center}
    (to be read like, e.g., ), because
    
    and
    
    Take  (for no particular reason), ,
    , 
    .
    Then
    
    as required.

    In this example, no other factorizations exist except for those that are
    obtained by replacing  and  by  and
     for some . This degree of freedom is
    reflected by the arbitrary choice of~.    
  \item The polynomial  cannot be written as
     for two quadratic polynomials  and~, because
    ,
    ,
    ,
    ,
    ,
    .
  \item Consider , i.e., , , , .
    We have seen that in this case there are two distinct factorizations.
    They correspond to the two bijections
     defined via
    \begin{center}
      \begin{tabular}{c|cccc}
               &  &  &  &  \\\hline
          &      &       &     &  \\
         &      &       &     & 
      \end{tabular}
    \end{center}
  \end{enumerate}
\end{example}

\section{Product Clashes}

Again let  be two square-free polynomials, and write
 for the roots of  and  for the
roots of~. Generically, the degree of  is equal to
. It cannot be larger than this, and it is smaller if and only
if there are two index pairs  with
. In this case, we say that  and  have a
product clash. Recall from equation~\eqref{eq:1} that  is formed as
the least common multiple of the factors , not as their product.

Product clashes appear naturally in the computation of .
For example, for  we have

because  is a clash. More generally, if  is a square-free
polynomial of degree~, then .

As an example that does not come from a product of the form ,
consider  and . Here we have
the clashes  and ,
so that  only has degree~4.

In order to include product clashes into the framework of the previous section,
we need to relax the requirement that  be injective. We still want it to
be surjective, because every root of  must be produced by the product 
of some root  of  and some root  of~. If the  and the
 are defined according to the formulas above, it can now happen that
 for some . We therefore adjust the definition
of  and  to , .
Then  and  are squarefree and for the set of roots of 
we obtain

as desired.

\begin{example}
  \begin{enumerate}
  \item To find the factorization ,
    set , , . Then
    a suitable choice for  is
    given by
    \begin{center}
      \begin{tabular}{c|cc}
         &  &  \\\hline
         &  &  \\
         &  &  
      \end{tabular}
    \end{center}
    because
    
    and
    
  \item
    Consider , i.e.,
    , , , . A possible
    choice for  is
    \begin{center}
      \begin{tabular}{c|ccc}
         &  &  &  \\\hline
           &  &  &  \\
           &  &  & 
      \end{tabular}
    \end{center}
    because
    
    and
    
  \end{enumerate}
\end{example}

\section{Searching for Assignments}\label{sec:search}

We now turn to the question how for a given  we can find a map  as required. Of course, since  is finite,
there are only finitely many possible choices for  and  such that
, and for each choice  there are only finitely many
functions . We can
simply try them all. But going through all these  many functions one
by one would take very long.

In order to improve the efficiency of the search, we can exploit the fact that
for most partial functions  it is easy to see that they cannot be extended
to a total function with the required properties. We can further reduce the
search space by taking into account that the order of the roots of the factors
is irrelevant, i.e., we can restrict the search to functions  with
 and
. Furthermore, because of
surjectivity, the root  must be reached, and we can choose to set
 without loss of generality. Next, discard all functions with
 for some  with  or with
 for some  with , because these
just signal some roots of a factor of  several times without providing any
additional information. So we can in fact enforce
 and .
Next,  is a solution iff
 with
 is a solution. We can therefore restrict the search to
functions where . 

The following algorithm takes these observations into account. It maintains an
assignment table  which encodes a function~ with

for all  and

for all . At every recursion level, the candidate under
consideration is extended to a function  with  for
some~. As soon as  is chosen, there is for each  at most one
choice  for the value of . The matrix 
stores these values  and marks the indices  for which no  exists with
. The result is a function  for some .  If this function is
surjective, we have found a solution.  Otherwise, we proceed recursively unless
we already have , because in this case any further extension could
only produce transposes of solutions that will be found at some other stage of
the search.

\medskip\noindent
INPUT: The roots  of some square-free polynomial .\\
OUTPUT: A list of functions  as required for solving the factorization problem.

\smallskip
\step 10 let  be a matrix with  for .
\step 20 call the procedure  as defined below.
\step 30 stop.

\smallskip
\step 40 procedure  
\step 51 for  do:
\step 62 set the th row of  to  and let  be the empty list
\step 72 for  do:
\step 83 if  and there exists  such that  and 
\step 94 set  and append  to 
\step {10}2 if  then:
\step {11}3 report the solution  with  for all .
\step {12}2 else if  then
\step {13}3 recursively call the procedure 

\medskip
In the interest of readability, we have refrained from some obvious optimizations.
For example, an actual implementation might perform some precomputation in order to
improve the search for  in Step~8.

It is not hard to implement the algorithm. A Mathematica implementation by the
authors is available on the website of this paper,
\url{http://www.math.rutgers.edu/~zeilberg/mamarim/mamarimhtml/Cfac.html}.  The
relevant function is \texttt{CFiniteFactor}.

\begin{example}
  Let  where
  , , , ,
  , .

  After initialisation, at the first level of the recursion, there are five choices
  for the first entry in the second row of~. Each of them uniquely determines the
  rest of the row, as follows (writing  for~):

  
  The second of these matrices corresponds to a solution
  
  which gives rise to the factorization
  
  while the other partial solutions cannot be continued to further solutions.   
\end{example}

\section{Multiple Roots}

Let us now drop the condition that  is square free. Write 
for the square free part of~. It is clear from equation~\eqref{eq:1} that
when  are such that , then , where  denote the square free parts of  and~,
respectively. It is therefore natural to first determine factorizations of the
square free part  of~ and in a second step obtain  and  from
 and  (if possible) by assigning appropriate multiplicities to
their roots. As the multiplicities in  or  cannot exceed those in~,
there are again just finitely many candidates and we could simply try them all.
And again, the search can be improved because many possibilities can be ruled
out easily. In fact, the freedom for the multiplicities is so limited that we
can compute them rather than search for them.

First consider the case when  and  were obtained from an injective
map , i.e., the case when there are no product clashes. In this case, each
root  of  corresponds to exactly one product 
of a root  of  and a root  of~. The
multiplicities  of  in  and  of  in~,
respectively, must be such that  equals the
multiplicity of  in~. This gives a linear system of equations.
Every solution of this system in the positive integers gives rise to a
factorization for~, and if there is no solution for the linear system of
any of the factorizations of the square-free part~, then  admits no factorization.

When there are product clashes, there are roots  of  which are obtained
in several distinct ways as products of roots of  and~, for instance
 for some
.  If  is the multiplicity of  in~, then the
requirement for the multiplicities
 of
 in  and~, respectively, is
that

We obtain a
system of such equations, one equation for reach root of~.  Such systems are
known as tropical linear systems, and algorithms are known for finding their
solutions in polynomial time~\cite{grigoriev13}.

\begin{example}
  \begin{enumerate}
  \item 
  Let .
  We have seen earlier that the square free part  of  admits
  two distinct factorizations
  
  Assigning multiplicities to the first, we get
  
  Comparing the exponents to those of~ gives the linear system
  
  which has no solution. For the second factorization, we get
  
  Comparing the exponents to those of~ gives the linear system
  
  whose unique solution in the positive integers is , , , ,
  thus
  
  \item
    Let .
    We have seen earlier that the square free part  of  admits the factorization
    
    Assigning multiplicities to the factors, we get
    
    Comparing the exponents to the exponents of the factors of~ gives a tropical linear system
    in the unknowns , which turns out to have two solutions. They
    correspond to the two factorizations
        
  \end{enumerate}  
\end{example}


\section{When we don't want to find the roots}

Sometimes our polynomials are with integer coefficients, and we prefer not to factorize them over the complex numbers. Of course, all
the roots are algebraic numbers, by definition, and computer-algebra systems know how to compute with them (without ``cheating'' and using
floating-point approximations), but it may be more convenient to find the tensor product (in the generic case: no product
clashes and no repeated roots) of  and ,
a certain polynomial  of degree~, as follows.
If the roots of  are  and  the roots of  are , then
the roots of  are, of course

Let  be the {\it power-sum symmetric functions}~\cite{macdonald95}, then of course

Now using {\it Newton's relations} (e.g. \cite{macdonald95}, Eq. I.(2.11') p.~23), one can go back and forth from
the elementary symmetric functions (essentially the coefficients of the polynomial up to sign) to the power-functions,
and {\it back,} enabling us easily to compute the tensor product without factorizing.

If you define the reverse of a polynomial~, to be , where  is the degree of~,
then  has, of course, the factor  but otherwise (generically) all distinct roots,
unless it has good reasons not to.
On the other hand, if  for some non-trivial polynomials  and  then   has repeated roots,
and the {\it repetition profile} can be easily predicted as above, or ``experimentally''. So using
this approach it is easy to {\it test} quickly whether  ``factorizes'', in the tensor-product sense.
However, to actually find the factors would take more effort.

This is implemented in the Maple package accompanying this article, linked to from
\url{http://www.math.rutgers.edu/~zeilberg/mamarim/mamarimhtml/Cfac.html}.
The tensor product operation is procedure {\tt Mul} and the testing procedure is {\tt TestFact}.

\section{Linear Combinations of Factorizations}

For almost all polynomials  there does not exist a factorization.
When no factorization exists, we may wonder whether  admits a decomposition of
a more general type. For example, we can ask whether there exist polynomials
 of degree at least two such that

Translated to the language of C-finite sequences, this means that we seek to write
a given C-finite sequence  as

for C-finite sequences , ,
, , none of which should satisfy a first-order
recurrence in order to make the problem nontrivial.

\def\im{\operatorname{im}}
It is not difficult to adapt the algorithm in Section~\ref{sec:search}
so that it can also discover such factorizations.
Suppose that  is squarefree. Then, instead of searching for 
a single surjective map

it suffices to find two functions

satisfying the same conditions previously requested for  but with surjectivity replaced by
. Once two such maps  have been found,
we can construct  by choosing  and  arbitrarily,
setting ,  and

for all  in question. Then , ,
, ,
are such that . 

In order to search for a pair , we can search for  very much like we searched
for  before, and for each partial solution encountered during the recursion, initiate a search
for another function  which is required to hit all the indices  not hit by
the partial solution~. Note that it is fine if some indices are hit by both  and~.
The suggested modification amounts to replacing lines 12 and~13 of the algorithm from Section~\ref{sec:search}
by the following:

\medskip
\step {12}2 else
\step {13}3 let .
\step {14}3 let  be an -matrix with  as first row. 
\step {15}3 call the procedure  defined below.
\step {16}3 for each function  it reports, report .
\step {17}3 if no  is found and  then
\step {18}4 recursively call 

\medskip
\step {19}0 procedure 
\step {20}1 [lines 5--9 literally as in the definition of ]
\step {21}2 if  then:
\step {22}3 [line 11 literally as in the definition of ]
\step {23}2 else if  then
\step {24}3 recursively call . 

\medskip

This settles the case of square free input. The extension to arbitrary
polynomials is like in the previous section. For every factorization of the
square free part we can assign variables for the multiplicities of all the roots
and compare the resulting multiplicities for  to those of~. This gives again a tropical linear system of equations
which can be solved with Grigoriev's algorithm~\cite{grigoriev13}.

\begin{example}
  The polynomial  cannot be written as
   for some . However, we have the representation
  
  for
  
  Note that the roots  and  of  are produced by both 
  and .  
\end{example}





\section{Examples}

Our main motivation for studying the factorization problem for C-finite sequences
are two interesting identities that can be interpreted as such factorizations. They
both originate from the transfer matrix method.

\def\-{{\tikz[x=1.5pt,y=1.5pt]\draw[very thin](0,0) rectangle (1,1) (1,0) rectangle (2,1);}}
\def\|{{\tikz[x=1.5pt,y=1.5pt]\draw[very thin](0,0) rectangle (1,1) (0,1) rectangle (1,2);}}

The first is a tiling problem studied in~\cite{kasteleyn61,fisher61}, and more recently in~\cite{zeilberger06a}.
Given a rectangle of size , the question is
in how many different ways we can fill it using tiles of size  or .
If  and  are even, it turns out that

is a bivariate polynomial in the variables  where the coefficient of a monomial  is exactly
the number of tilings of the  rectangle that uses exactly ~tiles of size 
and ~tiles of size . 
The transfer matrix method can be used to prove this result automatically for every fixed
 and arbitrary  (or vice versa). For every fixed choice of  (say), it delivers
a polynomial~ which encodes a recurrence for . For every fixed ,
the sequence

with  and  and  the Chebyshev polynomials
of the first and second kind, 
is C-finite with respect to~. An annihilating polynomial is

The formula for  can be proven for each particular choice of  and arbitrary  by checking
 and comparing the first  initial terms.
While the standard algorithms can confirm the correctness of some conjectured
factorization, the algorithm described in the present paper can
help discover the factorization in the first place, taking only  as input.
Fisher, Temperly~\cite{fisher61} or Kasteleyn~\cite{kasteleyn61} would probably
have found it useful back in the 1960s to apply the algorithm to 
and to detect the general pattern from the outputs.

The second identity has a similar nature. It describes the Ising model on an
 grid wrapped around a torus~\cite{onsager44,thompson72}. Starting
from a certain model in statistical physics that we do not want to explain here,
the transfer matrix method produces for every fixed  an annihilating
polynomial~ of degree~ for a certain C-finite sequence in~. The
asymptotic behaviour of this sequence for  is of interest. In view
of Theorem~\ref{thm:main}, it is goverend by the root of~ with the largest
absolute value. Onsager discovered that this largest root of~ is equal to

where  is some physical constant and  is defined as
\def\arccosh{\operatorname{arccosh}}

for  (compare eq.~(V.5.1) (p. 131) in~\cite{thompson72}). 

Let us translate these formulas to a more familiar form.
First note that because of periodicity and symmetry of the cosine,
we have  for . Hence each of the
 in the argument of  appears twice, except the
middle term , which only appears for odd~.
Set  and  for .
Then , and Onsager's expression for the
largest root of  simplifies to

For the second case we have used .
The equation for  says that  is a root of 

Set  when  is even and set 
when  is odd. Then Onsager's formula says that the largest root of  is equal
to the largest root of .

In fact, the polynomial  happens to be
exactly the irreducible factor of  corresponding to the largest root of~. Therefore,
our algorithm applied to this irreducible factor of  could have helped Onsager discover his formula.

\bibliographystyle{plain}
\bibliography{bib}

\end{document}
