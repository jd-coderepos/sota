
\subsection{Datasets}

\noindent \textbf{Panoptic Scene Graph Generation (PSG)~\cite{yang2022panoptic}}.
This is the first Panoptic Scene Graph generation dataset. It has a total of 48,749 labeled images including 2,177 test images and 46,572 training images.
The object categories comprise 80 thing classes and 53 stuff classes, which is the same as the COCO \cite{lin2014microsoft} and COCO-Stuff datasets \cite{caesar2018coco}.
The relation categories comprise 56 classes, including positional relations, common object-object relations, common actions, human actions, actions in the traffic scene, actions in the sports scene and interactions between backgrounds~\cite{yang2022panoptic}. 

\noindent \textbf{Visual Genome (VG)} \cite{krishna2017visual}.
VG is a widely used benchmark dataset for Scene Graph Generation.
Following previous work \cite{zellers2018neural, chen2019counterfactual}, we adopt the widely accepted split, VG-150, which contains 150 object categories and 50 relation categories.
The object categories cover a wide range of classes, such as \textit{animals}, \textit{vehicles} and \textit{household items}.
The relation categories include both spatial and semantic classes, such as \textit{on}, \textit{in} and \textit{wearing}.

\subsection{Tasks and Metrics}
Three subtasks have been proposed for the SGG and PSG tasks, which are \emph{Predicate Classification}, \emph{Scene Graph Classification} and \emph{Scene Graph Detection}~\cite{xu2017scene}.
We focus on Scene Graph Detection for both datasets, since it is the most comprehensive and addressed by \cite{yang2022panoptic}. This subtask
requires the model to first localize the objects and then predict the object classes and relations.
Note that Scene Graph Detection in PSG includes the detection on stuff classes, while in SGG it does not.

Following previous work \cite{tang2019learning, yu2020cogtree, yang2022panoptic}, we use Recall@K (R@K) and mean Recall@K (mR@K) as our metrics, where the former metric is dominated by high-frequency relations, while the latter assigns equal weight to all relation classes.

\subsection{Implementation details}
\label{sec:experiments:implementation}
In our experiments, we follow the training strategy of PSGTR \cite{yang2022panoptic}.
We use the AdamW optimizer \cite{loshchilov2017decoupled}, with a learning rate of  and weight decay of , except for the backbone, which is trained with a learning rate of . For initialization, we use Mask2Former \cite{cheng2022masked} pretrained on COCO \cite{lin2014microsoft} to initialize our backbone and pixel decoder.
Following Mask2Former \cite{cheng2022masked}, we use 100 triplet queries for the H-L and L-H decoders respectively.
Additionally, both the H-L and L-H decoders are initialized with Mask2Former's transformer decoder.
To ensure consistent comparison with PSGTR, we adopted the same data augmentation settings.
Our model is trained for 12 epochs with a step scheduler at epoch 10, taking approximately 18 hours to train on four A100 GPUs with a batch size of 1 for each GPU.

\subsection{Comparison to the state-of-the-art}
\label{sec:experiments:main_results}

\noindent \textbf{PSG}. 
Tab.~\ref{table:psg_main_results} reports the performance of our method compared to the state-of-the-art on the PSG dataset~\cite{yang2022panoptic}.
We separate the methods into two groups. 
The first are two-stage methods consisting of a separate segmentor and relation predictor, which are modified for the PSG task in \cite{yang2022panoptic}.
The second are one-stage end-to-end methods, which are able to simultaneously predict panoptic segmentation and relations.
Our method belongs to the second category.
For a fair comparison between the different methods, we use the same Resnet-50 \cite{he2016deep} backbone.
Our method shows superior performance compared to all previous methods.
Particularly, it outperforms the previous best-performing method PSGTR \cite{yang2022panoptic} by a large margin, \ie +6\% in R@100 and +11\% in mR@100.
Our model is able to converge within only 12 epochs of training, whereas PSGTR \cite{yang2022panoptic} is trained for 60 epochs.
We also evaluate our method using pre-trained transformer-based backbones, \ie Swin-B and Swin-L~\cite{liu2021swin}, with the latter being a bigger model.
Our results are consistently improved over all metrics due the powerful feature representation ability of the transformer.
We also conducted a visual comparison, as shown in Fig. \ref{fig:visual}.

\noindent \textbf{SGG}.
Here we study whether our approach that was developed for the PSG task can also be applied to the SGG task.
In Tab.~\ref{table:vg_main_results}, we conduct experiments on the VG-150 dataset.
Following IETrans~\cite{zhang2022fine}, we extend our \emph{unbiased} HiLo framework (Sec.~\ref{sec:framework}) without our PSG-specific baseline (Sec.~\ref{sec:baseline}) to four state-of-the-art \emph{biased} SGG methods, namely MOTIF \cite{zellers2018neural}, VCTree \cite{tang2019learning}, Transformer \cite{tang2020unbiased} and GPSNet \cite{lin2020gps}, using the implementation of \cite{zhang2022fine}.
Compared to the unbiased IETrans \cite{zhang2022fine} method, our method improves mean recall without sacrificing recall, demonstrating its effectiveness in enhancing the performance of low-frequency relations.
Compared to the biased baselines \cite{zellers2018neural, tang2019learning, tang2020unbiased, lin2020gps}, our method achieves a significantly larger mean recall, while still maintaining an acceptable recall.
This indicates that our method can improve the performance of low-frequency relations while also taking into account the performance of high-frequency relations.
It also shows that the HiLo framework is a general technique that yields systematic improvements in both the PSG and SGG tasks.

\begin{table}\small
    \centering
    \begin{tabular}{l|cc}
    \hline
        ~  & \multicolumn{2}{c}{Scene Graph Detection} \\
        \cline{2-3}
        Method & R/mR@50 & R/mR@100 \\
        \hline
        MOTIF \cite{zellers2018neural} & \textbf{31.0} / 6.7 & \textbf{35.1} / 7.7 \\
        \quad +IETrans \cite{zhang2022fine} & 26.4 / 12.4 & 30.6 / 14.9 \\
        \quad \textbf{+HiLo (ours)} & 26.2 / \textbf{14.7} & 30.3 / \textbf{17.7}  \\
        \hline
        VCTree \cite{tang2019learning} & \textbf{30.2} / 6.7 & \textbf{34.6} / 8.0 \\
        \quad +IETrans \cite{zhang2022fine} & 25.4 / 11.5 & 29.3 / 14.0 \\
        \quad \textbf{+HiLo (ours)} & 27.1 / \textbf{12.9} & 29.8 / \textbf{15.2} \\
        \hline
        Transformer \cite{tang2020unbiased} & \textbf{30.0} / 7.4 & \textbf{34.3} / 8.8 \\
        \quad +IETrans \cite{zhang2022fine} & 25.5 / 12.5 & 29.6 / 15.0 \\
        \quad \textbf{+HiLo (ours)} & 25.4 / \textbf{14.6} & 29.2 / \textbf{17.6} \\
        \hline
        GPSNet \cite{lin2020gps} & \textbf{30.3} / 5.9 & \textbf{35.0} / 7.1 \\
        \quad +IETrans \cite{zhang2022fine} & 25.9 / 14.6 & 28.1 / 16.5 \\
        \quad \textbf{+HiLo (ours)} & 25.6 / \textbf{15.8} & 27.9 / \textbf{18.0} \\
        \hline
    \end{tabular}
    \vspace{+1mm}
    \caption{Comparison between our HiLo framework and other methods on the VG-150 dataset. Similar to \cite{zhang2022fine}, we apply IETrans and our own method on top of four leading baselines.
    }
    \vspace{-5mm}
    \label{table:vg_main_results}
\end{table}

\begin{figure*}
\begin{center}
\includegraphics[width=\linewidth]{visual_results.pdf}
\end{center}
    \vspace{-5mm}
   \caption{Visualization of panoptic segmentations and the top 20 predicted triplets compared with ground truth.
   The upper left is the original image, the lower left is the ground truth and on the right are the predictions.
   The highlighted triplets represent the subject-object pairs with multiple relations, where the blue highlights represent the high frequency relations and the red highlights represents the low frequency relations.
   The visualization results show that our method can predict both high frequency and low frequency relations.
   }
\vspace{-4mm}
\label{fig:visual}
\end{figure*}

\subsection{Ablation Studies}
\label{sec:experiments:ablation}
Consistent with the paper, we use HiLo with a Resnet-50 backbone to perform ablation experiments on the PSG dataset.

\noindent \textbf{HiLo framework for different baselines}.
In this section we investigate whether our HiLo framework~(Sec.~\ref{sec:framework}) yields improvements for other baselines, rather than just the one presented in Sec.~\ref{sec:baseline}.
Tab.~\ref{table:different_baselines} shows the results for two baselines, with and without the HiLo framework.
We observe that our biased baseline outperforms the previous PSGTR~\cite{yang2022panoptic} method on all metrics.
Furthermore, by applying the HiLo framework, we can substantially improve the performance over both baselines.
It is worth mentioning that the HiLo framework improves recall and mean recall simultaneously, whereas other methods typically improve one metric at the cost of the other \cite{tang2020unbiased, yu2020cogtree}.

\begin{table}\small
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lc|ccc}
    \hline
        Baseline  & HiLo & R/mR@20 & R/mR@50 & R/mR@100 \\
        \hline
        HiLo baseline & \checkmark & \textbf{34.1} / \textbf{23.7} & \textbf{40.7} / \textbf{30.3} & \textbf{43.0} / \textbf{33.1} \\
        HiLo baseline & - & 32.6 / 20.9 & 38.0 / 27.4 & 38.9 / 28.4 \\
        \hline
        PSGTR~\cite{yang2022panoptic} & \checkmark & \textbf{30.1} / \textbf{20.2} & \textbf{36.6} / \textbf{23.9} & \textbf{38.3} / \textbf{24.5} \\
        PSGTR~\cite{yang2022panoptic} & - & 28.4 / 16.6 & 34.4 / 20.8 & 36.3 / 22.1 \\
        \hline
    \end{tabular}
    }
    \vspace{+1mm}
    \caption{Comparison of different baselines, with and without HiLo framework. Using the HiLo framework, we see significant improvements on both metrics.
    }
    \vspace{-2mm}
    \label{table:different_baselines}
\end{table}

\noindent \textbf{HiLo relation augmentation.}
We observe that out of 260,296 labeled triplets in the PSG dataset, only about 10\% of subject-object pairs have multiple relations, for which we can apply relation swapping~(Sec.~\ref{sec:framework:relation_generation}).
After applying our proposed relation augmentation technique~(Sec.~\ref{sec:framework:relation_generation}), this ratio significantly increases to 40\%.
Our experimental results in Tab.~\ref{table:proportion_of_swappable_triplets} demonstrate that only applying the HiLo framework on 10\% already gives an improvement over the baseline from Tab.~\ref{table:different_baselines}. As the number of swappable triplets increases due to augmentation, the model's performance is further enhanced, highlighting the potential of our method.


\begin{table}\small
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c|c|ccc}
    \hline
        Relation Aug. & Multiple relations & R/mR@50 & R/mR@100 \\
        \hline
        \checkmark & 40\%  & \textbf{40.7} / \textbf{30.3} & \textbf{43.0} / \textbf{33.1} \\
        - & 10\%  & 40.1 / 28.1 & 42.8 / 32.5 \\
        \hline
    \end{tabular}
    }
    \vspace{+1mm}
    \caption{Ablation study for HiLo relation augmentation. Relation augmentation affects the ratio of subject-object pairs with multiple relations. The larger this ratio, the more relations can be swapped, which leads to better results.
    }
    \vspace{-4mm}
    \label{table:proportion_of_swappable_triplets}
\end{table}



\noindent \textbf{HiLo prediction alignment}.
We conduct ablation experiments on the subject-object consistency loss and relation consistency loss~(Sec.~\ref{sec:framework:prediction_alignment}), which are used to align the predictions from the high and low frequency branches.
The results, as presented in Tab.~\ref{table:prediction_alignment}, demonstrate that using both losses yields the best performance.
It is worth mentioning that we have explored the margin in the relation consistency loss and found that setting the margin to zero leads to a small performance degradation.
This finding confirms that there is a partial semantic overlap between swapped relations, indicating that they are not entirely consistent.

To investigate the impact of relational index exchange (RIE) on the relation consistency loss, we conducted experiments to verify the effect of omitting RIE.
In the absence of RIE, we solely compute the consistency loss for relation categories that are not involved in relation swapping and exclude the swapping component from the calculation of the consistency loss.
The outcomes of this experiment are presented in Tab. \ref{table:rie}, and demonstrate a notable reduction in the mean recall and a decline in the model's performance for relations with relational semantic overlap when RIE is not utilized.

\begin{table}\small
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{ccc|ccc}
    \hline
        Object & Relation & Margin & R/mR@50 & R/mR@100 \\
        \hline
        \checkmark & \checkmark & 0.5 & \textbf{40.7} / \textbf{30.3} & \textbf{43.0} / \textbf{33.1} \\
        - & \checkmark & 0.5 & 40.6 / 29.7 & 42.8 / 32.8 \\
        - & \checkmark & 0.0 &  40.5 / 29.5 & 42.7 / 32.8 \\
        \checkmark & - & - &  40.4 / 29.0 & 42.8 / 32.2 \\
        - & - & - & 39.7 / 28.6 & 42.4 / 32.0 \\
        \hline
    \end{tabular}
    }
    \vspace{+1mm}
    \caption{Ablation study for different losses in HiLo prediction alignment. \emph{Object} refers to the subject-object consistency loss and \emph{relation} refers to the relation consistency loss. The margin parameter is defined in Eq.~\ref{eq:relation_consistency_loss}.
    }
    \vspace{-4mm}
    \label{table:prediction_alignment}
\end{table}

\begin{table}\small
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c|ccc}
    \hline
        Whether to use RIE & R/mR@20 & R/mR@50 & R/mR@100 \\
        \hline
        \checkmark & \textbf{34.1} / \textbf{23.7} & \textbf{40.7} / \textbf{30.3} & \textbf{43.0} / \textbf{33.1} \\
        - & 33.5 / 22.3 & 40.3 / 29.0 & 42.6 / 32.3 \\
        \hline
    \end{tabular}
    }
    \vspace{+1mm}
    \caption{Ablation study for relation consistency loss in HiLo prediction alignment.}
    \vspace{-6mm}
    \label{table:rie}
\end{table}

\noindent \textbf{HiLo inference fusion}.
We ablate the inference fusion~(Sec.~\ref{sec:framework:inference_fusion}) and evaluate the performance of each branch's output separately.
In Tab.~\ref{table:inference_fusion_1}, experimental results suggest that fusion can effectively leverage the uniqueness of the high and low frequency branch predictions to achieve comprehensive improvements on all metrics.

We also attempted to average the tensor generated by the two branches and obtain the PSG result through post-processing.
However, we found that this approach leads to a substantial drop in performance, as evident in Tab. \ref{table:inference_fusion_2}.
This can be attributed to the inconsistent prediction results of the two branches for the same query index.
These findings validate that the inference fusion method effectively merges the results from the two branches.
Furthermore, our experimental results demonstrate that the query associated with the identical index in two branches does not predict the same subject-object pair.
Thus, directly averaging the tensor produced by the two branches results in prediction ambiguity, ultimately leading to a substantial decline in performance.
This observation underscores the necessity of conducting triplet query correspondence when performing prediction alignment.
In particular, due to the inconsistent query prediction content for the corresponding index in the two branches, a one-to-one correspondence must be constructed based on the label assigned by each query to achieve prediction alignment.

\begin{table}\small
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{cc|ccc}
    \hline
        H-L Result & L-H Result & R/mR@50 & R/mR@100 \\
        \hline
        \checkmark & \checkmark & \textbf{40.7} / \textbf{30.3} & \textbf{43.0} / \textbf{33.1} \\
        \checkmark & - & 38.8 / 29.9 & 39.8 / 30.9 \\
        - & \checkmark & 38.5 / 26.5 & 39.5 / 27.8 \\
        \hline
    \end{tabular}
    }
    \vspace{+1mm}
    \caption{Ablation study for HiLo inference fusion.}
    \vspace{-4mm}
    \label{table:inference_fusion_1}
\end{table}


\begin{table}\small
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{l|ccc}
    \hline
        Fusion method & R/mR@20 & R/mR@50 & R/mR@100 \\
        \hline
        inference fusion & \textbf{34.1} / \textbf{23.7} & \textbf{40.7} / \textbf{30.3} & \textbf{43.0} / \textbf{33.1} \\
        average tensor & 19.6 / 13.1 & 23.1 / 15.7 & 23.9 / 16.3 \\
        \hline
    \end{tabular}
    }
    \vspace{+1mm}
    \caption{Ablation study for HiLo inference fusion.
    \textit{Inference fusion} refers to the method proposed in the paper to fuse the results of two branches.
    \textit{Average tensor} refers to the fusion method that directly averages the tensor output by the two branches.
}
    \vspace{-4mm}
    \label{table:inference_fusion_2}
\end{table}



\noindent \textbf{Masked relation attention}.
We investigate the impact of different mask input types for cross-attention on the HiLo baseline~(Sec.~\ref{sec:baseline}) performance.
Specifically, we compare two different attention focus regions, namely the subject-object region and the full image.
The results are shown in Tab.~\ref{table:masked_relation_attention}.
Focusing on the full image presents a more challenging optimization task for the model since no target region is specified.
Consequently, we observe a drop of 1.8\% in R@100 and 1.6\% in mR@100.
This shows that it is crucial to apply masked relation attention to the subject and object.

\begin{table}\small
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{l|ccc}
    \hline
        Attention focus & R/mR@20 & R/mR@50 & R/mR@100 \\
        \hline
        subject-object & \textbf{32.6} / \textbf{20.9} & \textbf{38.0} / \textbf{27.4} & \textbf{38.9} / \textbf{28.4} \\
        full image & 30.4 / 19.3 & 36.5 / 25.9 & 37.1 / 26.8 \\
        \hline
    \end{tabular}
    }
    \vspace{+1mm}
    \caption{Ablation study for masked relation attention.}
    \vspace{-10mm}
    \label{table:masked_relation_attention}
\end{table}

\subsection{Analysis}
\label{exp:analysis}
To further demonstrate the efficacy of our method, we conduct the following analysis on HiLo.

\noindent \textbf{Long-tail problem and relational semantic overlap.}
To verify whether the problems of long-tail and relational semantic overlap have been tackled, we conduct experiments shown in Tab. \ref{table:verify}.
For the \textit{long-tail problem}, we consider relations appearing less than 500 times (28 relations) in the PSG dataset (relations appear on average 4787 times) as rare relations, and report their mR@100.
There is a 14\% improvement for rare relations and an 11\% improvement for all relations.
This suggests that our method addresses the long-tail problem.

For the \textit{relational semantic overlap}, we select all test images that have this problem (927 images) and report their mR@100.
Our method shows a 15.3\% improvement over PSGTR on images with semantic overlap relations, exceeding the overall improvement across all test images.
This suggests that our approach addresses the problem of relational semantic overlap.
\begin{table}\small
    \centering
\resizebox{\linewidth}{!}{
    \begin{tabular}{l|cc|cc}
    \hline
        Method & All & Rare & All & Overlap \\ \hline
        PSGTR & 22.1 & 6.2 & 22.1 & 23.5 \\
        HiLo (ours) & 33.1 (\textcolor{darkgreen}{+11.0}) & 20.3 (\textcolor{darkgreen}{+14.1}) & 33.1 (\textcolor{darkgreen}{+11.0}) & 38.8 (\textcolor{darkgreen}{+15.3}) \\ \hline
    \end{tabular}
    }
    \vspace{+1mm}
    \caption{Verification of the improvements in the long-tail problem and relational semantic overlap.
    \textit{All} refers to testing on the whole test set.
    \textit{Rare} refers to testing only on rare relations.
    \textit{Overlap} refers to testing only on data with relation semantic overlap.}
    \vspace{-4mm}
    \label{table:verify}
\end{table}





\noindent \textbf{Convergence speed and time cost analysis}.
We evaluate the convergence of our model by assessing its performance on the validation set at various epochs, as illustrated in Fig.~\ref{figure:convergence_speed}.
Our analysis reveals that our proposed method outperforms prior methods \cite{yang2022panoptic} both in terms of final performance and convergence speed.
Specifically, PSGTR \cite{yang2022panoptic} achieves negligible performance in the initial 12 epochs, requiring 60 epochs to converge, as per the authors.
In contrast, our HiLo method achieves better results in just 12 epochs, indicating its superior convergence speed.

\begin{figure}
\begin{center}
\includegraphics[width=\columnwidth]{convergence_speed.pdf}
\end{center}
\vspace{-2mm}
\caption{Convergence speed analysis of different methods. 
Our method converges significantly faster than previous methods.}
\vspace{-6mm}
\label{figure:convergence_speed}
\end{figure}

For time cost, we primarily analyze relation augmentation and swapping.
1) \textit{Relation augmentation}, inspired by IETrans \cite{zhang2022fine}, involves training a baseline model and then using it to predict relation labels so as to augment the original relation labels. 
For the PSG dataset, following our experimental setup (see Sec. \ref{sec:experiments:implementation}), this takes 18 hours.
Afterwards, the training of our HiLo model requires 18 hours, which makes the whole process 36 hours.
In contrast, the PSGTR model training takes 48 hours.
Both our method and PSGTR utilize ResNet-50 as the backbone for fair comparison.
Our method's rapid convergence (see Sec. \ref{sec:experiments:ablation}) reduces training time compared to PSGTR, making the additional time cost for relation augmentation tolerable.
2) \textit{Relation swapping} is a quick operation on relation labels during training and does not significantly contribute to overall time consumption.

\noindent \textbf{Training and inference cost.}
Training and inference cost is shown in Tab. \ref{table:resource}.
Despite using two transformer-based decoders, our method requires less resource.
Given the same input sizes  and ResNet-50 as backbone, the resource usage is shown in Tab. \ref{table:resource}.
PSGTR \cite{yang2022panoptic} generates mask features for subject and object separately, while our method reduces this computation by only generating mask features once.
Our inference time is marginally higher due to more complex post-processing.
Using H-L and L-H data only changes the labels between two branches, not increasing resource use.
\begin{table}\small
    \centering
\resizebox{\linewidth}{!}{
    \begin{tabular}{l|cccc}
    \hline
        Method & GFlops & Param. (M) & Train Mem. (G) & Infer. Time (ms)\\ \hline
        PSGTR & 461.3 & 44.2 & 26.5 & 140 \\
        HiLo (ours) & 229.4 & 58.7 & 16.1 & 156 \\ \hline
    \end{tabular}
    }
    \vspace{+1mm}
    \caption{Training and inference cost.}
    \label{table:resource}
    \vspace{-6mm}
\end{table}
