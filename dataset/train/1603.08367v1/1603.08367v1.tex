\documentclass[twoside,11pt]{article}
\usepackage{jmlr2e}
\usepackage[latin1]{inputenc}
\usepackage[T1]{fontenc}

\jmlrheading{14}{2013}{1091--1143}{}{}{Markus Thom and G\"{u}nther Palm}

\ShortHeadings{Sparse Activity and Sparse Connectivity in Supervised Learning}{Thom and Palm}


\usepackage{subscript}
\usepackage[ruled, linesnumbered]{algorithm2e}
\usepackage{booktabs}\setlength{\abovetopsep}{1.5ex}
\usepackage{enumitem}
\setlist[enumerate]{label=(\alph*)}
\newenvironment{proofof}[1][Proof]{\par\noindent{\bf #1\ }}{\hfill\BlackBox\
  \sigma\colon\R^n\setminus\set{0}\to\intervalcc{0}{1}\text{,}\qquad x\mapsto\frac{\sqrt{n} - \nicefrac{\norm{x}_1}{\norm{x}_2}}{\sqrt{n}-1}\text{,}

  S^{(\lambda_1,\lambda_2)} := \Set{s\in\R^n | \norm{s}_1 = \lambda_1\text{ and }\norm{s}_2 = \lambda_2}\text{ and }
  S_{\geq 0}^{(\lambda_1,\lambda_2)} := S^{(\lambda_1,\lambda_2)} \cap \R_{\geq 0}^n\text{.}

  \proj_M(x) := \set{y\in M | \norm{y - x}_2 \leq \norm{z - x}_2\text{ for all }z\in M}

  S_{\geq 0}^{(\lambda_1,\lambda_2)} = \R_{\geq 0}^n \cap H \cap K =: D\text{.}

  \pi_{\geq 0}\colon\R^n\setminus R\to D\text{,}\qquad x\mapsto\proj_D(x)\text{.}

  \pi\colon\R^n\setminus R\to S^{(\lambda_1,\lambda_2)}\text{,}\qquad x\mapsto s\hada\pi_{\geq 0}\left(\abs{x}\right)\text{,}

  E_{\SOAE}\left(W,\ W_{\out},\ \theta_{\out}\right) := \left(1 - \alpha\right)\cdot s_R\left(\tilde{x},\ x\right) + \alpha\cdot s_C\left(y,\ t\right)

  A := \set{\text{SOAE-}}\text{, }\;
  B := \set{\text{SOAE--conn},\ \text{SOAE-},\ \text{SMLP-SCFC}}\text{,}\\
  \text{and }C := \set{\text{MLP-OBD},\ \text{MLP-random},\ \text{MLP-samples},\ \text{MLP-SCFC}}\text{.}

  \proj_{S^{(\lambda_1,\lambda_2)}}(x) = \nicefrac{\tilde{\lambda}_2}{\lambda_2}\cdot\proj_{S^{(\tilde{\lambda}_1,\tilde{\lambda}_2)}}(x)\text{ for all }x\in\R^n\text{.}

  \scp{s - q}{r}
  = \delta\norm{r - m}_2^2 + \nicefrac{1}{\delta}\cdot\big(\nicefrac{\lambda_1^2}{n} - \lambda_2^2 + \nicefrac{1}{2}\cdot\norm{q - s}_2^2\big)
  = \tfrac{1}{2\delta}\norm{q - s}_2^2\text{,}

  \norm{\tilde{r}}_2^2
  &= \norm{\alpha x}_2^2 + \norm{\nicefrac{1}{n}\cdot\left(\lambda_1 - \alpha e\transp x\right)e}_2^2 + 2\scp{\alpha x}{\nicefrac{1}{n}\cdot\left(\lambda_1 - \alpha e\transp x\right)e}\\
  &= \alpha^2\norm{x}_2^2 + \nicefrac{1}{n}\cdot\left(\lambda_1 - \alpha e\transp x\right)\left(\lambda_1 + \alpha e\transp x\right)
  = \alpha^2\norm{x}_2^2 + \nicefrac{1}{n}\cdot\left(\lambda_1^2 - \alpha^2 (e\transp x)^2\right)\text{,}

  \tilde{s}
  = m + \tilde{\delta}\left(\tilde{r} - m\right)
  = m + \nicefrac{\delta}{\alpha}\cdot\left(\alpha x + \nicefrac{\lambda_1}{n}\cdot e - \nicefrac{\alpha}{n}\cdot e\transp x e - \nicefrac{\lambda_1}{n}\cdot e\right)
  = m + \delta\left(x - \nicefrac{1}{n}\cdot e\transp xe\right) = s\text{,}

  s^{(k)} := s^{(k-1)} - s_{j_k}^{(k-1)}e_{j_k} + \tfrac{1}{n - k}s_{j_k}^{(k-1)}\left(e - \smallsum_{i=1}^k e_{j_i}\right)

  \scp{e - \smallsum_{\mu=1}^{i}e_{j_\mu}}{e - \smallsum_{\nu=1}^{k+1}e_{j_\nu}}
  &= \scp{e}{e} - \scp{e}{\smallsum_{\nu=1}^{k+1}e_{j_\nu}} -\scp{\smallsum_{\mu=1}^{i}e_{j_\mu}}{e} + \scp{\smallsum_{\mu=1}^{i}e_{j_\mu}}{\smallsum_{\nu=1}^{k+1}e_{j_\nu}}\\
  &= n - (k + 1) - i + i = n - k - 1\text{,}

  \scp{a_i}{a_{k+1}}
  &= \tfrac{1}{\left(n-i\right)\left(n-k-1\right)}\scp{e - \smallsum_{\mu=1}^{i}e_{j_\mu}}{e - \smallsum_{\nu=1}^{k+1}e_{j_\nu}}
     + \scp{e_{j_i}}{e_{j_{k+1}}}\\
  &\phantom{=} - \tfrac{1}{n-i}\scp{e - \smallsum_{\mu=1}^{i}e_{j_\mu}}{e_{j_{k+1}}}
     - \tfrac{1}{n-k-1}\scp{e_{j_i}}{e - \smallsum_{\nu=1}^{k+1}e_{j_\nu}}\\
  &= \tfrac{n-k-1}{\left(n-i\right)\left(n-k-1\right)} + 0 - \tfrac{1}{n-i} - \tfrac{0}{n-k-1}
  = 0\text{.}

  \scp{s^{(0)} - s^{(k)}}{s^{(k)} - s^{(k+1)}}
  = \scp{\smallsum_{i=1}^{k}s^{(i-1)}_{j_i}a_i}{s^{(k)}_{j_{k+1}}a_{k+1}}
  = \smallsum_{i=1}^{k}s^{(i-1)}_{j_i}s^{(k)}_{j_{k+1}}\scp{a_i}{a_{k+1}}
  = 0\text{.}

  s^{(k-1)}_{j_k}
  &= \bscp{e_{j_k}}{s^{(k-1)}}
   = \bscp{e_{j_k}}{s^{(0)}} + \smallsum_{i=1}^{k-1} s^{(i-1)}_{j_i}\scp{e_{j_k}}{a_i}
  = q_{j_k} + \smallsum_{i=1}^{k-1}\tfrac{1}{n-i} s^{(i-1)}_{j_i}\\
  &\equsing{IH} q_{j_k} + \smallsum_{i=1}^{k-1}\tfrac{1}{n-i}q_{j_i} + \smallsum_{i=1}^{k-1}\smallsum_{\mu=1}^{i-1}\tfrac{1}{n-i}\tfrac{1}{n-i+1}q_{j_\mu}
  = q_{j_k} + \smallsum_{i=1}^{k-1}q_{j_i}\left[\tfrac{1}{n-i} + \smallsum_{\mu=i+1}^{k-1}\tfrac{1}{n-\mu}\tfrac{1}{n-\mu+1}\right]\text{.}

  \bscp{s^{(0)} - s^{(k)}}{s^{(k)} - v}
  &= \bscp{s^{(0)} - s^{(k-1)} - s^{(k-1)}_{j_k}a_k}{s^{(k-1)} + s^{(k-1)}_{j_k}a_k - v}\\
  &= \bscp{s^{(0)} - s^{(k-1)}}{s^{(k-1)} - v} + s^{(k-1)}_{j_k}\bscp{a_k}{s^{(0)} - 2s^{(k-1)} - s^{(k-1)}_{j_k}a_k + v}\\
  &\equsing{IH} s^{(k-1)}_{j_k}\left(-\scp{a_k}{q} - s^{(k-1)}_{j_k}\scp{a_k}{a_k} + \scp{a_k}{v}\right)\\
  &= s^{(k-1)}_{j_k}\left(-\tfrac{\lambda_1}{n-k} + \tfrac{1}{n-k}\smallsum_{i=1}^{k}q_{j_i} +q_{j_k} - s^{(k-1)}_{j_k} \left(1 + \tfrac{1}{n-k}\right) + \tfrac{\lambda_1}{n-k}\right)\\
  &= s^{(k-1)}_{j_k}\left(q_{j_k}\left(1 + \tfrac{1}{n-k}\right) + \tfrac{1}{n-k}\smallsum_{i=1}^{k-1}q_{j_i} - s^{(k-1)}_{j_k} \left(1 + \tfrac{1}{n-k}\right)\right)
  = 0\text{,}

  s^{(k)}_i - s^{(k)}_j &=
  \phantom{-}s^{(k-1)}_i - s^{(k-1)}_{j_k}{\chi_{\set{j_k}}(i)} + \tfrac{1}{n-k}s^{(k-1)}_{j_k}{\chi_{\discint{j_1}{j_k}^C}(i)} \\
  &\phantom{=} -s^{(k-1)}_j + s^{(k-1)}_{j_k}\chi_{\set{j_k}}(j) - \tfrac{1}{n-k}s^{(k-1)}_{j_k}{\chi_{\discint{j_1}{j_k}^C}(j)}\\
  &= {s^{(k-1)}_i - s^{(k-1)}_j} + {s^{(k-1)}_{j_k}\left(\tfrac{1}{n-k} + \chi_{\set{j_k}}(j)\right)} \geq 0\text{,}

  \lambda_1
  &= \smallsum_{i=1}^ns^{(k-1)}_i
  = \smallsum_{i\in I}s^{(k-1)}_i + \smallsum_{i=1}^{k-1}s^{(k-1)}_{j_i} + s^{(k-1)}_{j_k} + \smallsum_{i=k+1}^hs^{(k-1)}_{j_i}\\
  &\geq \big((n-h) + 1 + (h - k)\big)s^{(k-1)}_{j_k} = \left(n - k + 1\right)s^{(k-1)}_{j_k}\text{,}

  \bscp{s^{(k-1)}}{a_k}
  = \tfrac{1}{n - k}\left(\lambda_1 - \smallsum_{i=1}^{k-1}s^{(k-1)}_{j_i} - s^{(k-1)}_{j_k}\right) - s^{(k-1)}_{j_k}
  = \tfrac{1}{n - k}\left(\lambda_1 - s^{(k-1)}_{j_k}\right) - s^{(k-1)}_{j_k}\text{,}

  \bnorm{s^{(k)}}_2^2 - \bnorm{s^{(k-1)}}_2^2
  &= \bnorm{s^{(k-1)}_{j_k}a_k}_2^2 + 2s^{(k-1)}_{j_k}\bscp{s^{(k-1)}}{a_k}\\
  &= s^{(k-1)}_{j_k}\left[s^{(k-1)}_{j_k}\left(1 + \tfrac{1}{n - k}\right) + \tfrac{2}{n-k}\left(\lambda_1 - s^{(k-1)}_{j_k}\right) - 2s^{(k-1)}_{j_k}\right]\\
  &= s^{(k-1)}_{j_k}\left[\tfrac{2\lambda_1}{n - k} - s^{(k-1)}_{j_k}\left(1 + \tfrac{1}{n - k}\right)\right]\text{,}

  d := \norm{q - v}_2^2 - \norm{P_\tau q - v}_2^2 = 2\left(q_j - q_i\right)\left(v_i - v_j\right)\text{.}

  \norm{q - v}_2^2 = \norm{q - s}_2^2 + \norm{s - v}_2^2 = \norm{q - s}_2^2 + \left(\norm{v - t}_2 + \norm{t - s}_2\right)^2
  > \norm{v - t}_2^2 \geq \norm{v - p}_2^2\text{.}

  \lambda_1
  = \smallsum_{j=1}^nr_j
  \geq \smallsum_{j\in J}r_j
  = \smallsum_{j\in J}\left(s_j - \hat{t}\right)
  = \smallsum_{j\in J}s_j - \abs{J}\cdot\hat{t}
  > \smallsum_{j\in J}s_j\text{,}

  0
  = \scp{e}{s - r}
  = \smallsum_{i\in I}\left(s_i - r_i\right) + \smallsum_{i\not\in I}\left(s_i - r_i\right)
  = \smallsum_{i\in I}\hat{t} + \smallsum_{i\not\in I}s_i
  = d\hat{t} + \lambda_1 - \smallsum_{i\in I}s_i\text{,}

  \scp{r}{s - r}
  = \smallsum_{i\in I}r_i\left(s_i - r_i\right)
  = \smallsum_{i\in I}\left(s_i - \hat{t}\right)\hat{t}
  = \hat{t}\left(\smallsum_{i\in I}s_i - d\hat{t}\right)
  = \lambda_1\hat{t}\text{.}

  \lambda_2^2
  &= \norm{s}_2^2
   = \smallsum_{i\in I}s_i^2 + \smallsum_{i\not\in I}s_i^2
   = \smallsum_{i\in I}\left(r_i + \hat{t}\right)^2 + \smallsum_{i\not\in I}s_i^2\\
  &> \smallsum_{i\in I}r_i^2 + d\hat{t}^2 + 2\hat{t}\smallsum_{i\in I}r_i
  = \norm{r}_2^2 + d\hat{t}^2 + 2\lambda_1\hat{t}
  \geq \norm{r}_2^2\text{,}

  \scp{p - r}{s - r} 
  &= \smallsum_{i\in I}p_i\left(s_i - r_i\right) + \smallsum_{i\not\in I}p_i\left(s_i - r_i\right) - \scp{r}{s - r}
  = \smallsum_{i\in I}p_i\hat{t} + \smallsum_{i\not\in I}p_is_i - \lambda_1\hat{t}\\
  &= \hat{t}\left(\lambda_1 - \smallsum_{i\not\in I}p_i\right) + \smallsum_{i\not\in I}p_is_i - \lambda_1\hat{t}
  = \smallsum_{i\not\in I}p_i\left(s_i - \hat{t}\right) \leq 0\text{.}

  \pi_{\geq 0}(x) = \left[\medcirc^{j=h}_1\big(\proj_{L_{I_j}}\circ\proj_C\big)\circ\proj_L\circ\proj_H\right](x)\text{,}

  A_i :=
  \delta_iE_N - \nicefrac{\delta_i}{d_i}\cdot J_{N\times N} -\alpha_is(i)s(i)\transp + \nicefrac{\alpha_i}{d_i}\cdot s(i)s(i)\transp J_{N\times N}\in\R^{N\times N}
  \text{ where }\alpha_i := \nicefrac{\delta_i}{\norm{r(i)}_2^2}\text{,}

  G := G_LG_C = \delta E_n - \nicefrac{\delta}{d}\cdot uu\transp - \delta\diag(v) - \alpha qq\transp + \nicefrac{\alpha}{d}\cdot qq\transp uu\transp + \alpha qq\transp\diag(v)\in\R^{n\times n}\text{.}

  G &= \delta \diag(E_d, \0) - \nicefrac{\delta}{d}\cdot\diag(J_{d\times d}, \0) - \alpha\diag(rr\transp, \0) + \nicefrac{\alpha}{d}\cdot\diag(rr\transp J_{d\times d}, \0)\\
  &= \diag\left(\delta E_d - \nicefrac{\delta}{d}\cdot J_{d\times d} - \alpha rr\transp + \nicefrac{\alpha}{d}\cdot rr\transp J_{d\times d},\ \0\right)\text{.}

  Az = \delta\left(z - \nicefrac{1}{d}\cdot\scp{z}{u} \cdot u\right) + \alpha\left(\nicefrac{1}{d}\cdot\scp{s}{u}\scp{z}{u}- \scp{s}{z}\right) s\text{,}

  \left(\tfrac{\partial s_R(\tilde{x}, x)}{\partial \tilde{x}}\right)\transp
  = \tfrac{1}{\sqrt{\lambda\mu}}\left(x - \tfrac{\scp{e}{x}}{d}e\right) - \tfrac{s_R(\tilde{x}, x)}{\lambda}\left(\tilde{x} - \tfrac{\scp{e}{\tilde{x}}}{d}e\right)\in\R^d\text{,}

where all entries of  are unity,  and .
\end{proposition}
\begin{proof}
One obtains  because  is the correlation coefficient.
The claim then follows with the quotient rule.
\end{proof}
The gradients of the similarity measure for classification capabilities are essentially equal to those of an ordinary two-layer neural network, and can be computed using the back-propagation algorithm \citep{Rumelhart1986}.
However, the pairing of the softmax transfer function with the cross-entropy error function provides a particularly simple structure of the gradient \citep{Dunne1997}.
For completeness, the gradients of the classification module of SOAE are summarized:
\begin{proposition}
\label{prop:SOAE-grad-classf}
If  is the cross-entropy error function,  is the softmax transfer function and the target vector for classification  is a one-of- code, then
,
 and
.
\end{proposition}
\begin{proof}
Basic matrix calculus \citep{Neudecker1969,Vetter1970} yields
,
 and
.
By requirement , where  denotes the element-wise quotient,  and .
Therefore  using , and the claim follows.
\end{proof}
As  is a convex combination of the reconstruction error and the classification error, its overall gradient follows immediately from Proposition~\ref{prop:SOAE-grad-reconst} and Proposition~\ref{prop:SOAE-grad-classf}.
Proposition~\ref{prop:corrcoeff_gradient}, the results from Appendix~\ref{sect:analytical_properties}, and the gradient of the  projection as described in Section~\ref{sect:projfunc_differentiability} can then be used to compute the explicit gradients for the procedure proposed in this paper.

\bibliography{the}
\end{document}
