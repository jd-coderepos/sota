\vspace{-2mm}
\Section{Experimental Results}
\label{Sec:Exp}
\vspace{-3mm}
We evaluate our proposed method on two challenging LPFA datasets, namely AFLW and AFW, both qualitatively and quantitatively, as well as the near-frontal face dataset of $300$W. 
Further, we conduct experiments on different CNN architectures to validate our visualization layer design. 

\Paragraph{Implementation details}
Our implementation is built upon the Caffe toolbox~\cite{jia2014caffe}. 
In all of the experiments, we use six visualization blocks ($N_v$) with two convolutional layers ($N_c$) and fully connected layers in each block (Fig.~\ref{fig:visualization_Block}). 
Details of the network structure are provided in Tab.~\ref{table:VBdetail}. 

Instead of using the sequentially pretrain strategy~\cite{yan2015hd}, we perform the joint end-to-end training from scratch. 
To better estimate the parameter update in each block and to increase the effectiveness of using visualization block, we set the weight of the loss function in the first visualization block to $1$, and linearly increase the weights by one for each block, i.e., the loss weight of the last block is $6$.
This strategy helps the CNN to pay more attention to the landmark loss used in later blocks. 
On the one hand, backpropagation of loss functions in the last blocks has more impact in the first block, and on the other hand the last block can adopt itself more quickly to the changes in the first block.  

In the training phase, we set the weight decay to $0.005$, the momentum to $0.99$, the initial learning rate to $1\mathrm{e}{-6}$. 
Besides, we decrease the learning rate to $5\mathrm{e}{-6}$ and $1\mathrm{e}{-7}$ after $20$ and $29$ epochs. 
In total, the training phase is continued for $33$ epochs for all experiments. 

\begin{table}[t!]\small
\caption{\small Number and size of convolutional filters in each visualization block. For all blocks, the two fully connected layers have the same length of $800$ and $236$.}\figvspace\vspace{-3mm}
\begin{center}
  \resizebox{\linewidth}{!}{
\begin{tabular}{ |c|c|c|c|c|c| } 
 \hline
Block \# & $1$ & $2$ & $3$ & $4$ & $5,6$\\\hline
Conv. & $12$ ($5$$\times$$5$) & $20$ ($3$$\times$$3$) & $28$ ($3$$\times$$3$) & $36$ ($3$$\times$$3$) & $40$ ($3$$\times$$3$)\\
layers & $16$ ($5$$\times$$5$) & $24$ ($3$$\times$$3$) & $32$ ($3$$\times$$3$) & $40$ ($3$$\times$$3$) & $40$ ($3$$\times$$3$)\\\hline



\end{tabular}
}
\end{center}
\label{table:VBdetail}\vspace{-10mm}
\end{table}
\vspace{-1mm}
\SubSection{Quantitative Evaluations on AFLW and AFW}
\vspace{-2mm}
The AFLW dataset~\cite{kostinger2011annotated} is a very challenging dataset with large-pose face images ($\pm90^{\circ}$ yaw). 
We use the subset of this dataset released by~\cite{jourabloo2016large}, which includes $3,901$ training images  and $1,299$ testing images. 
All face images in this subset are labeled with $34$ landmarks and a bounding box.
The AFW dataset~\cite{zhu2012face} contains $205$ images with $468$ faces. 
Each face image is labeled with at most $6$ landmarks with visibility labels, as well as a bounding box. 
AFW is used only for testing in our experiments. 
The bounding boxes in both datasets are used as initilization for our algorithm, as well as the baselines. 
We crop the face image inside the bounding box and normalize it to $114 \times 114$. 
Due to the memory constraint of GPUs, we have a pooling layer in the first visualization block after the first convolutional layer to decrease the size of feature maps to half, and the input to the subsequent visualization blocks is of $57 \times 57$.
To augment the training data, we generate $20$ different variations for each training image by adding noise to the location, width and height of the provided bounding boxes.


For quantitative evaluations, we use two conventional metrics. 
The first one is Mean Average Pixel Error (MAPE)~\cite{yu2013pose}, which is the average of the pixel errors for the visible landmarks. The other one is Normalized Mean Error (NME), i.e., the average of the normalized estimation error of visible landmarks.
The normalization factor is the square root of the face bounding box size~\cite{jourabloo2015pose}, instead of the eye-to-eye distance in the frontal-view face alignment.


We compare our method with several state-of-the-art methods in LPFA. 
For AFLW, we compare with LPFA~\cite{jourabloo2016large}, PIFA~\cite{jourabloo2015pose} and RCPR~\cite{burgos2013robust} with the NME metric. 
Tab.~\ref{table:AFLWRes} shows that the proposed method achieved a higher accuracy than the baseline methods. Also, CALE~\cite{bulat2016convolutional}, a heatmap-based $2$D face alignment method, reports NME of $2.96\%$ on the AFLW. We discuss about the advantage of the proposed method over heatmap-based methods in section~\ref{Sec:PriorWork}.
To demonstrate the capabilities of each visualization block, the NME computed using the estimated $\mathbf{P}$ after each block is shown in Tab.~\ref{table:AFLWStage}. 
If a higher alignment speed is desirable, it is possible to skip the last two visualization blocks with a reasonable NME.





On the AFW dataset, the comparisons are conducted with LPFA~\cite{jourabloo2016large}, PIFA~\cite{jourabloo2015pose}, CDM~\cite{yu2013pose} and TSPM~\cite{zhu2012face} with the MAPE metric. 
The evaluations are provided in Tab.~\ref{table:AFWRes}, which also shows the superiority of the proposed method.



\begin{table}[t!]\small
\caption{NME ($\%$) of four methods on AFLW dataset.}
\begin{center}
\begin{tabular}{ c|c|c|c } 
 \hline
 Proposed method & LPFA~\cite{jourabloo2016large} & PIFA & RCPR \\ 
 \hline
 $4.45$ & $4.72$ & $8.04$ & $6.26$ \\
 \hline  
\end{tabular}
\end{center}
\label{table:AFLWRes}\vspace{-7mm}
\end{table}





\begin{table}[t!]\small
\caption{NME ($\%$) of the proposed method at each visualization block on AFLW dataset. The initial NME is 25.8$\%$.}
\vspace{-3mm}
\begin{center}
  \resizebox{0.9\linewidth}{!}{
\begin{tabular}{ c|c|c|c|c|c|c } 
 \hline
 Block \# & $1$ & $2$ & $3$ & $4$ & $5$& $6$\\\hline
 NME & $9.26$ & $6.77$ & $5.51$ &$4.98$ &$4.60$ &$4.45$\\\hline
 
\end{tabular}
}
\end{center}
\label{table:AFLWStage}\vspace{-6mm}
\end{table}








\begin{table}[t!]\small
\caption{MAPE of five methods on AFW dataset.}
\begin{center}
\begin{tabular}{ c|c|c|c|c } 
 \hline
 Proposed method & LPFA~\cite{jourabloo2016large} & PIFA & CDM & TSPM \\ 
 \hline
 $6.27$ & $7.43$ & $8.61$ & $9.13$ & $11.09$ \\
 \hline  
\end{tabular}
\end{center}
\label{table:AFWRes}\vspace{-9mm}
\end{table}
Some examples of alignment results of the proposed method on AFLW and AFW datasets are shown in Fig.~\ref{figure:ResAFLWAFW1}. Three examples of visualization layer output at each visualization block are shown in Fig.~\ref{figure:ResVisEach1}. 

\vspace{-2mm}
\SubSection{Evaluation on 300W dataset}
\vspace{-2mm}
While our main goal is LPFA, we further evaluate on the most widely used near frontal $300$W dataset~\cite{sagonas2013300}. $300$W containes $3,148$ training and $689$ testing images, which are divide into common and challenging sets with $554$ and $135$ images, respectively. 
Tab.~\ref{table:300WRes} shows the NME (normalized by the interocular distance) of the proposed and state-of-the-art methods. The most related method to ours is $3$DDFA~\cite{zhu2015face}, which also estimates $\textbf{M}$ and $\textbf{p}$. 
Our method outperforms it on both common and challenging sets. Other near frontal alignment methods do not employ shape constraints e.g., $3$DMM which is an advantage for them. Because the span of the $3$D shape bases cannot cover all possible locations of landmarks. 
To comapre with the MDM~\cite{trigeorgis2016mnemonic}, we compute the failure rate with threshold of $0.08$. The failure rates of our method are $16.83\%$ ($6.80\%$ for MDM) and $8.99\%$ ($4.20\%$ for MDM) with $68$ and $51$ landmarks. 

\begin{table}[t!]\small
\caption{The NME of different methods on $300$W dataset.}
\begin{center}
\begin{tabular}{ c|c|c|c } 
 \hline
 Method & Common & Challenging & Full \\ 
 \hline
 ESR~\cite{cao2014face} & $5.28$ & $17.00$ & $7.58$ \\
 RCPR~\cite{burgos2013robust} & $6.18$ & $17.26$ & $8.35$ \\
 SDM~\cite{xiong2013supervised} & $5.57$ & $15.40$ & $7.50$ \\
 LBF~\cite{ren2014face} & $4.95$ & $11.98$ & $6.32$ \\
 CFSS~\cite{zhu2015face} & $4.73$ & $9.98$ & $5.76$ \\
 RCFA~\cite{wang2016recurrent} & $4.03$ & $9.85$ & $5.32$ \\
 RAR~\cite{xiao2016robust} & $4.12$ & $8.35$ & $4.94$ \\
 3DDFA~\cite{zhu2015face} & $6.15$ & $10.59$ & $7.01$ \\
 3DDFA+SDM & $5.53$ & $9.56$ & $6.31$ \\
 \hline  
 Proposed method & $5.43$ & $9.88$ & $6.30$ \\
\hline  
\end{tabular}
\end{center}
\label{table:300WRes}\vspace{-10mm}
\end{table}



\vspace{-2mm}
\SubSection{Analysis of the Visualization Layer}
\vspace{-2mm}
We perform four sets of experiments to study the properties of the visualization layer and network architectures. 


\Paragraph{Influence of visualization layers}
To analyze the influence of the visualization layer in the testing phase, we add $5\%$ noise to the fully connected layer parameters of each visualization block, and compute the alignment error on the AFLW test set. 
The NMEs are [$4.46$, $4.53$, $4.60$, $4.66$, $4.80$, $5.16$] when each block is modified seperately. 
This analysis shows that visualized image has more influence on the later blocks, since imprecise parameters of early blocks could be compensated by later blocks. 
To evaluate the influence of the visualization layer in the training phase, we train the network without any visualization layer. 
The final NME on AFLW is $7.18\%$ which shows the importance of visualization layers for guiding the network training.

\Paragraph{Advantage of deeper  features}
We train three CNN architectures (Fig.~\ref{fig:Arc}) on AFLW. 
The inputs of the visualization block in the first architecture are the input images $\textbf{I}$, feature maps $\textbf{F}$ and the visualization image $\textbf{V}$. The inputs of the second and the third architectures are $\{\textbf{F}, \textbf{V}\}$ and $\{\textbf{I},\textbf{V}\}$, respectively. 
The NME of each architecture is shown in Tab.~\ref{table:diffArch}.
While the first one performs the best, the substantial lower performance of the third one demonstrates the importance of deeper features learned across blocks.


At the first convolutional layer of each visualization block, we compute the average of the filter weights, across both the kernel size and number of maps.
The averages for three types of input features are shown in Fig.~\ref{fig:Arc_Analysis}.
As can be observed, from the first to the sixth block, the weights continue to decrease, making a more precise estimation of small-scale parameter updates. 
Considering the number of filters in Tab.~\ref{table:VBdetail}, the total impact of feature maps are higher than the other two inputs in all blocks. 
This again shows the importance of deeper features in guiding the network to estimate parameters. 
Furthermore, the average of the visualization filter is higher than that of the input image filter, which validates the stronger influence of the proposed visualization during training. 





\begin{figure}[t!]\small
\begin{center}
\begin{tabular}{cc}
(a) & \includegraphics[width=0.82\linewidth]{Arc2_im2.pdf} \\
(b) & \includegraphics[width=0.82\linewidth]{Arc_im2.pdf} \\
(c) & \includegraphics[width=0.82\linewidth]{Arc3_im2.pdf}
\end{tabular}
\end{center}
\figvspace
\caption{Architectures of three CNNs with different inputs.} 
\label{fig:Arc}\figvspace
\end{figure}


\begin{table}[t!]\small
\caption{The NME ($\%$) of three architectures with different inputs ($\textbf{I}$: Input image, $\textbf{V}$: Visualization, $\textbf{F}$: Feature maps).}\figvspace
\begin{center}
\begin{tabular}{ c|c|c } 
 \hline
 Architecture a & Architecture b & Architecture c \\ 
 $(\textbf{I},\textbf{F},\textbf{V})$ & $(\textbf{F}, \textbf{V})$ & $(\textbf{I},\textbf{V})$\\
 \hline
 $4.45$ & $4.48$ & $5.06$ \\
 \hline  
\end{tabular}
\end{center}
\label{table:diffArch}
\end{table}






\begin{figure}\tiny
\figvspace\vspace{-2mm}
\begin{center}
\begin{tabular}{@{}ccc@{}}
$\quad$ Input image filters & $\quad$ Visualization filters & $\qquad$ Feature maps filters\\
\includegraphics[width=0.3\linewidth]{I-c.pdf} &
\includegraphics[width=0.3\linewidth]{V-c.pdf} &
\includegraphics[width=0.3\linewidth]{F-c.pdf}



\end{tabular}
\end{center}
\figvspace
\caption{The average of filter weights for input image, visualization and feature maps in three architectures of Fig.~\ref{fig:Arc}. The $y$-axis and $x$-axis shows the average and the block index, respectively.} 
\label{fig:Arc_Analysis}\vspace{-7mm}

\end{figure}


\Paragraph{Advantage of using masks}
To show the advantage of using the mask in the visualization layer, we conduct an experiment with different masks. 
Specifically, we define another mask for comparison, which is shown in Fig.~\ref{fig:Mask2}. 
It has five positive areas, i.e., the eyes, nose tip and two lip corners. 
The values are normalized to zero-mean and unit standard deviation. 
Compared to the original mask in Fig.~\ref{fig:Mask}, this mask is more complicated and conveys more information about the informative facial areas to the network. 
Moreover, to show the necessity of using the mask, we also test using visualization layers without any mask. 
The NMEs of the trained networks with different masks are shown in Tab.~\ref{table:mask}. 
Comparing the first and third columns shows the advantage of using the mask in the network. The mask makes the pixel value of visualized images to be similar for faces with different poses and discriminate between the middle-area and contour-area of the face. By comparing the first and second columns, we can see that utilizing more complicated mask does not further improve the result, meaning the original mask provides sufficient information for its purpose. 

\begin{figure}[t!]\small
\begin{center}
\includegraphics[width=0.6\linewidth]{mask2-1.pdf} 
\end{center}
\figvspace
\caption{Mask $2$, a different designed mask with five positive areas on the eyes, top of the nose and sides of the lip.} 
\label{fig:Mask2} \vspace{-2mm}\end{figure}



\Paragraph{Different numbers of blocks and layers}
Given the total number of $12$ convolutional layers in our network, we can partition them to visualization blocks in various sizes.
To compare their performance, we train two additional CNNs, one with $4$ visualization blocks and each with $3$ convolutional layers; and the other with $3$ block and $4$ convolutional layers per block, where all three architectures have $12$ total convolutional layers.
The NME of these architectures are shown in Tab.~\ref{table:VBNum}. 
It shows the same conclusion as in~\cite{burgos2013robust} that the number of regressors is important for face alignment and we can potentially achieve a higher accuracy by increasing the number of visualization blocks. 

\begin{table}[t!]\small
\caption{NME ($\%$) of utilizing different masks.}
\begin{center}
\begin{tabular}{ c|c|c } 
 \hline
 Mask $1$ & Mask $2$ & No Mask \\ 
 \hline
 $4.45$ & $4.49$ & $5.31$ \\
 \hline  
\end{tabular}
\end{center}
\label{table:mask}\figvspace
\vspace{-3mm}
\end{table}

\begin{table}[t!]\small
\caption{NME ($\%$) of utilizing different numbers of visualization blocks ($N_v$) and convolutional layers ($N_c$).} \figvspace
\begin{center}
\begin{tabular}{ c|c|c } 
 \hline
 $N_v=6$ , $N_c=2$ &  $N_v=4$ , $N_c=3$ &  $N_v=3$ , $N_c=4$ \\ 
 \hline
 $4.45$ & $4.61$ & $4.83$ \\
 \hline  
\end{tabular}
\end{center}
\label{table:VBNum}\figvspace\vspace{-6mm}
\end{table}

\vspace{-2mm}
\SubSection{Time complexity} 
\vspace{-2mm}
Compared to the cascade of CNNs, one of the main advantages of end-to-end training a single CNN is the reduced training time. 
The training of the proposed method needs $33$ epochs and takes around $2.5$ days. 
The state of the art~\cite{jourabloo2016large}, that uses the same train and test sets as ours, trains six CNNs and each needs $70$ epochs. 
The total time of~\cite{jourabloo2016large} is around $7$ days. 
Similarly, the method in~\cite{zhu2015face} needs around $12$ days to train three CNNs each one with $20$ epochs, despite using different training data. 
Compared to~\cite{jourabloo2016large}, the proposed method reduces the training time by more than half.
The testing speed of proposed method is $4.3$ FPS on a Titan X GPU. It is much faster than the $0.6$ FPS speed of~\cite{jourabloo2016large} and is simalar to $4$ FPS speed of~\cite{xiao2016robust}. 







































































\begin{figure*}[h!]\small
\begin{center}
\includegraphics[width=0.95\linewidth]{All_Face.png} 
\end{center}

   \caption{Results of alignment on AFLW and AFW datasets, green landmarks show the estimated locations of visible landmarks and red landmarks show estimated locations of invisible landmarks. First row: provided bounding box by AFLW with initial locations of landmarks, Second: estimated $3$D dense shapes, Third: estimated landmarks, Fourth to sixth: estimated landmarks for AFLW, Seventh: estimated landmarks for AFW.}
   \label{figure:ResAFLWAFW1}
\end{figure*}



\begin{figure*}[h]\small
\begin{center}
\includegraphics[width=0.95\linewidth]{All_Face2.png} 
\end{center}

\caption{Three examples of outputs of visualization layer at each visualization block. The first row shows that the proposed method recovers the expression of the face gracefully, the third row shows the visualizations of a face with a more challenging pose.}
\label{figure:ResVisEach1}
\end{figure*}

\vspace{-2mm}