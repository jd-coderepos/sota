\documentclass[11pt,a4paper]{article}


\usepackage[hyperref]{acl2019}
\usepackage{times}
\usepackage{url}
\usepackage{amsfonts,amssymb}
\usepackage{amsmath}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{dsfont}
\usepackage{bm}
\usepackage{enumitem}\usepackage{amsthm}
\usepackage{array}
\usepackage{anyfontsize}


\aclfinalcopy \def\aclpaperid{962} 

\setlength\titlebox{5cm}


\newcommand\BibTeX{B{\sc ib}\TeX}
\newcommand\confname{ACL 2019}
\newcommand\conforg{SIGDAT}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\renewcommand{\arraystretch}{1.15}


\newtheorem{thm}{\protect\theoremname}
\newtheorem{lma}{\protect\lemmaname}

\providecommand{\theoremname}{Theorem}
\providecommand{\lemmaname}{Lemma}

\title{Relation Embedding with Dihedral Group in  Knowledge Graph}


\author{Canran Xu \thanks{\protect\phantom{\footnotesize 1}Equal contribution.} \\
  eBay Inc. \\
  \tt canxu@ebay.com
\And Ruijiang Li \footnotemark[1] \\
  eBay Inc. \\
  \tt ruijili@ebay.com
\\}


\date{}

\begin{document}
\maketitle
\begin{abstract}
Link prediction is critical for the application of incomplete knowledge graph (KG) in the downstream tasks. As a family of effective approaches for link predictions, embedding methods try to learn low-rank representations for both entities and relations such that the bilinear form defined therein is a well-behaved scoring function. Despite of their successful performances, existing bilinear forms overlook the modeling of relation compositions, resulting in lacks of interpretability for reasoning on KG. To fulfill this gap, we propose a new model called \textit{DihEdral}, named after dihedral symmetry group. This new model learns knowledge graph embeddings that can capture relation compositions by nature. Furthermore, our approach models the relation embeddings parametrized by discrete values, thereby decrease the solution space drastically. Our experiments show that \textit{DihEdral} is able to capture all desired properties such as (skew-) symmetry, inversion and (non-) Abelian composition, and outperforms existing bilinear form based approach and is comparable to or better than deep learning models such as ConvE \cite{convE}.
\end{abstract}

\section{Introduction}
Large-scale knowledge graph (KG) plays a critical role in the downstream tasks such as semantic search \cite{semantic_search}, dialogue management \cite{dialogue} and question answering \cite{kbqa}. In most cases, despite of its large scale, KG is not complete due to the difficulty to enumerate all facts in the real world. The capability of predicting the missing links based on existing dataset is one of the most important research topics for years.  A common representation of KG is a set of triples (\textit{head}, \textit{relation}, \textit{tail}), and the problem of link prediction can be viewed as predicting new triples from the existing set. A popular approach is KG embeddings, which maps both entities and relations in the KG to a vector space such that the scoring function of entities and relations for ground truth distinguishes from false facts \cite{ntn, transe, distmult}. Another family of approaches explicitly models the reasoning process on KG by synthesizing information from paths \cite{guu}. More recently, researchers are applying deep learning methods to KG embeddings so that non-linear interaction between entities and relations are enabled \cite{rgcn, convE}.

The standard task for link prediction is to answer queries (\textit{h}, \textit{r}, ?) or (? \textit{r}, \textit{t}). In this context, recent works on KG embedding focusing on bilinear form methods \cite{complex, hole, analogy, simple} are known to perform reasonably well. The success of this pack of models resides in the fact they are able to model relation (skew-) symmetries. Furthermore, when serving for downstream tasks such as learning first-order logic rule and reasoning over the KG, the learned relation representation is expected to discover relation composition by itself. One key property of relation composition is that in many cases it can be non-commutative. For example, exchanging the order between \verb|parent_of| and \verb|spouse_of| will result in completely different relation (\verb|parent_of| as opposed to \verb|parent_in_law_of|). We argue that, in order to learn relation composition within the link prediction task, this non-commutative property should be explicitly modeled. 

In this paper, we proposed \textit{DihEdral} to model the relation in KG with the representation of dihedral group. The elements in a dihedral group are constructed by rotation and reflection operations over a 2D symmetric polygon.
As the matrix representations of dihedral group can be symmetric or skew-symmetric, and the  multiplication of the group elements can be Abelian or non-Abelian, it is a good candidate to model the relations with all the corresponding properties desired.

To the best of our knowledge, this is the first attempt to employ finite non-Abelian group in KG embedding to account for relation compositions. Besides, another merit of using dihedral group is that even the parameters are quantized or even binarized, the performance in link prediction tasks can be improved over state-of-the-arts methods in bilinear form due to the implicit regularization imposed by quantization.





The rest of paper is organized as follows: in (\S\ref{sec:preliminary}) we present the mathematical framework of bilinear form modeling for link prediction task, followed by an introduction to group theory and dihedral group. In (\S\ref{sec: dihedral}) we formalize a novel model \textit{DihEdral} to represent relations with fully expressiveness. In (\S\ref{sec:training}, \S\ref{sec: experiment}) we develop two efficient ways to parametrize DihEdral and reveal that both approaches outperform existing bilinear form methods.
In (\S\ref{sec: case_studies}) we carried out extensive case studies to demonstrate the enhanced interpretability of relation embedding space by showing that the desired properties of (skew-) symmetry, inversion and relation composition are coherent with the relation embeddings learned from DihEdral.













%
 \section{Preliminaries}\label{sec:preliminary}



\subsection{Bilinear From for KB Link Prediction} \label{subsec:bilinear}

Let  and  be the set of entities and relations. A triple , where  are the head and tail entities, and  is a relation corresponding to an edge in the KG. 

In a bilinear form, the entities ,  are represented by vectors  where , and relation  is represented by a matrix . The score for the triple is defined as . A good representation of the entities and relations are learned such that the scores are high for positive triples and low for negative triples.






\subsection{Group and Dihedral Group}


Let  be two elements in a set , and  be a binary operation  between any two elements  in  . The set  forms a group when the following axioms are satisfied:
\begin{description}[leftmargin=0cm]
\item[Closure] For any two element ,  is also an element in .
\item[Associativity] For any , .
\item[Identity] There exists an identity element  in  such that, for every element  in , the equation  holds.
\item[Inverse] For each element , there is its inverse element  such that .
\end{description}
If the number of group elements is finite, the group is called a \textit{finite group}. If the group operation is commutative, i.e.  for all  and , the group is called \textit{Abelian}; otherwise the group is \textit{non-Abelian}. 



Moreover, if the group elements can be represented by a matrix, with group operations defined as matrix multiplications, the identity element is represented by the identity matrix and the inverse element is represented as matrix inverse. In the following, we will not distinguish between group element and its corresponding matrix representation when no confusion exists.

A dihedral group is a finite group that supports symmetric operations of a regular polygon in two dimensional space. Here the symmetric operations refer to the operator preserving the polygon. For a -side () polygon, the corresponding dihedral group is denoted as  that consists of  elements, within which there are  \emph{rotation} operators and  \emph{reflection} operators. A rotation operator  rotates the polygon anti-clockwise around the center by a degree of , and a reflection operator  mirrors the rotation  vertically.

\begin{figure}[ht]
\begin{centering}
\includegraphics[width=1\columnwidth]{sections/figures/ACL-crop.pdf}
\par\end{centering}
\caption{Elements in . Each subplot represents result after applying the corresponding operator to the square of `ACL' on the upper left corner (on top of ). The top row corresponds to the rotation operators and the bottom row corresponds to the reflection operators.} \label{figure: dihedral}
\end{figure}

The element in the dihedral group  can be represented as 2D orthogonal matrices\footnote{There are more than one 2D representations for the dihedral group , and we use the orthogonal representation throughout the paper. Check \citealt{group_rep} for details.}:

where . Correspondingly, the group operation of dihedral group can be represented as multiplication of the representation matrices. Note that when  is evenly divided by , rotation matrices  and  are skew-symmetric, and all the reflection matrices  and rotation matrices ,  are symmetric. The representation of  is shown in Figure \ref{figure: dihedral}. 

 \section{Relation Modeling with Dihedral Group and Expressiveness}\label{sec: dihedral}

\begin{table*}[ht]
\centering
\small
\begin{tabular}{l|ccccc}
\bottomrule
\multirow{2}{*}{} & \multirow{2}{*}{Component} & \multirow{2}{*}{Symmetric} & \multirow{2}{*}{Skew-Symmetric} &  \multicolumn{2}{c}{Composition} \\ \cline{5-6} & & & & Abelian & Non-Abelian \\ \hline
\-1em]
ComplEx & {} & {} & {} & \checkmark & NA \\
\-0.75em]
SimplE &  &  &  & \multicolumn{2}{c}{NA} \\
\ \label{eq: loss}
\min_{\Theta} \sum_{(h,r,t)\in\mathcal{T}^+\cup\mathcal{T}^-}-\log\sigma\left(y\phi(h,r,t)\right)
+ \lambda ||\Theta_{\mathcal{E}}||^2,
 \label{eq: gumbel}
    c^{(l)}_k = \frac{\exp\left[(s^{(l)}_k + q_k) / \tau\right]}{\sum_{k=1}^{2K} \exp\left[(s^{(l)}_k + q_k) / \tau\right]}

    \bm{R}^{(l)} = \left[
    \begin{array}{cc}
    \lambda & -\alpha \gamma \\
    \gamma & \alpha \lambda \\
    \end{array}\right],

\lambda	& =\begin{cases}
(x+y)/2 & K=4 \\
    y(3-x)/4 & K=6
\end{cases}, \\
\gamma & =\begin{cases}
(x-y)/2 & K=4 \\
    z(x+1)\sqrt{3} /4 & K=6
\end{cases}.

    \frac{\partial \mathrm{loss}}{\partial b_{\mathrm{real}}} = \frac{\partial \mathrm{loss}}{\partial b} \mathds{1},

-\sum_{l=1}^{L} \log\sigma\left(\bm{h}^{(l)\top}\bm{R}^{(l)}\bm{t}^{(l)} -\bm{h}^{*(l)\top}\bm{R}^{(l)}\bm{t}^{*(l)}\right).

where ,  and the corresponding negative triple .


For each composition , we compute the histogram of   . The result for relation compositions in FB15K-237 and FAMILY is shown in Figure \ref{fig: composition}, from which we could see good composition as matrix multiplication. We also reveal the non-Abelian property in FAMILY by exchanging the order of  and . \section{Related Works}


In this section we discuss the related works and their connections to our approach.

TransE \cite{transe} takes relations as a translating operator between head and tail entities. More complicated distance functions \cite{transh, transr, ptranse} are also proposed as extensions to TransE. TorusE \cite{torusE} proposed a novel distance function defined over a torus by transform the vector space by an Abelian group onto a -dimensional torus. ProjE \cite{proje} designs a neural network with a combination layer and a projection layer. R-GCN \cite{rgcn} employs convolution  over multiple entities to capture spectrum of the knowledge graph. ConvE \cite{convE} performs 2D convolution on the concatenation of entity and relation embeddings, thus by nature introduces non-linearity to enhance expressiveness. 

In RESCAL \cite{rescal} each relation is represented by a full-rank matrix. As a downside, there is a huge number of parameters in RESCAL making the model prone to overfitting.  A totally symmetric DistMult \cite{distmult} model simplifies RESCAL by representing each relation with a diagonal matrix. To parametrize skew-symmetric relations, ComplEx \cite{complex} extends DistMult by using complex-valued instead of real-valued vectors for entities and relations. The representation matrix of ComplEx supports both symmetric and skew-symmetric relations while being closed under matrix multiplication. HolE \cite{hole} models the skew-symmetry with circular correlation between entity embeddings, thus ensures shifts in covariance  between embeddings at different dimensions. It was recently showed that HolE is isomophic to ComplEx \cite{isomophic}. ANALOGY \cite{analogy} and SimplE \cite{simple} both reformulate the tensor decomposition approach in light of analogical and reversible relations.

Though embedding based approach achieves state-of-the-art performance on link prediction task, symbolic relation composition is not explicitly modeled. In contrast, the latter goal is currently popularized by directly modeling the reasoning paths \cite{composition_vectors, deeppath, minerva, multi_hop, longterm}. As paths are consistent with reasoning logic structure, non-Abelian composition is supported by nature. 


DihEdral is more expressive when compared to other bilinear form based embedding methods such as DistMult, ComplEX and ANALOGY. As the relation matrix is restricted to be orthogonal, DihEdral could bridge translation based and bilinear form based approaches as the training objective \textit{w.r.t.} the relation matrix is similar (cf Lemma \ref{th:equiv}). Besides, DihEdral is the first embedding method to incorporate non-Abelian relation compositions in terms of matrix multiplications (cf. Theorem \ref{th:group}).

%
 \section{Conclusion}



This paper proposed \textit{DihEdral} for KG relation embedding. By leveraging the desired properties of dihedral group,  relation (skew-) symmetry, inversion, and (non-) Abelian compositions are all supported. Our experimental results on benchmark KGs showed that DihEdral outperforms existing bilinear form models and even deep learning methods.  Finally, we demonstrated that the above g properties can be learned from DihEdral by extensive case studies, yielding a substantial increase in interpretability from existing models.  
\section*{Acknowledgments}
The authors would like to thank Vivian Tian, Hua Yang, Steven Li and Xiaoyuan Wu for their supports, and anonymous reviewers for their helpful comments.

\bibliography{dihedral}
\bibliographystyle{acl}
\end{document}