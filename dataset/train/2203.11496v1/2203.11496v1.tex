
In this section, we present the proposed method \Name~for LiDAR-camera 3D object detection. As shown in Fig.~\ref{fig:pipeline}, given {a} LiDAR BEV feature map and {an} image feature map from convolutional backbones,
{our transformer-based detection head first decodes object queries into initial bounding box predictions using the LiDAR information, and then performs LiDAR-camera fusion by attentively fusing object queries with useful image features.}
Below we will first provide the preliminary knowledge about a transformer architecture for detection and then present the detail of \Name.









\subsection{Preliminary: Transformer for {2D} Detection}
Transformer~\cite{Vaswani2017AttentionIA} has been widely used for 2D object detection~\cite{Zhu2021DeformableDD, Sun2020SparseRE, Gao2021FastCO, Yao2021EfficientDI} since DETR~\cite{carion2020endtoend} was proposed. 
DETR uses {a} CNN backbone to extract image features and {a} transformer 
architecture to convert a small set of learned embeddings~(called object queries) into a set of predictions. The follow{-}up works~\cite{Zhu2021DeformableDD, Sun2020SparseRE, Yao2021EfficientDI} further equip the object queries with positional information~\footnote{Slightly different concepts might be introduced, e.g., reference points in Deformable-DETR~\cite{Zhu2021DeformableDD} and proposal boxes in Sparse-RCNN~\cite{Sun2020SparseRE}.}. The final predictions of boxes are the relative offsets w.r.t. the query positions to reduce 
optimization difficulty. We refer readers to the original papers~\cite{carion2020endtoend, Zhu2021DeformableDD} for more details. 
{In our work,}
each object query contains {a} query position providing the localization of the object and a query feature
encoding instance information, such as the {box's} size, orientation, etc. 



















\subsection{Query Initialization}
\label{subsubsec: query_initialize}
\noindent\textbf{Input-dependent.} The query positions in {the seminal} works~\cite{carion2020endtoend, Zhu2021DeformableDD, Sun2020SparseRE} are randomly generated or learned as network parameters{, regardless of the input {data}.}
Such \prevqueryinit~
query positions will take extra stages~(decoder layers) for 
{their models~\cite{carion2020endtoend, Zhu2021DeformableDD}}
to learn the moving process towards the real object centers. Recently, it has been {observed} in 2D object detection~\cite{Yao2021EfficientDI} that with a better initialization of object queries, the gap between 1-layer structure and 6-layer structure could be bridged. 
{Inspired by this observation,}
we propose an \ourqueryinit~initialization strategy based on a center heatmap to achieve competitive performance using only one decoder layer. 

{Specifically}, given a  dimensional LiDAR BEV feature map , we first predict a class-specific heatmap , where  {describes} the size of {the} BEV feature map and  is the number of categories. Then we regard the heatmap as  object candidates and select the top- candidates {for all the categories} as our initial object queries. 
To avoid spatially too closed queries, following~\cite{Zhou2019ObjectsAP},
we select {the} local maximum elements {as our object queries},
whose values are greater than or equal to {their} 8-connected neighbors. Otherwise{,} a large number of queries are needed to cover the BEV plane. 
The positions and features of the selected candidates are used to initialize the query positions and query features. In this way, our initial object queries will locate at or close to the potential object centers, eliminating the need of multiple decoder layers~\cite{misra20213detr, Liu2021GroupFree3O, Wang2021ObjectD3} to refine the locations.






\noindent\textbf{Category-aware.}  
Unlike the{ir} 2D projections on the image plane, the objects on the BEV plane are all in absolute scale and has small scale variance 
among the same categories. To leverage such properties for better multi-class detection, we make the object queries category-aware by equipping each query with a category embedding. Specifically, using the category of each selected candidate~(e.g.  {belonging} to the -th category), we element-wisely sum the query feature with a category embedding produced by linearly projecting the one-hot category vector into a  vector. The category embedding brings benefits {in} two aspects: on {the} one hand, it serves as a useful side information when modelling the object-object relations in {the self-attention 
modules}
and the object-context relations in {the cross-attention modules.}
On the other hand, during prediction, it could deliver valuable prior knowledge of the object, making the network {focus} on intra-category variance and {thus} benefiting the property prediction. 
















\subsection{Transformer Decoder and FFN} 




\noindent The decoder layer follows the {design} of DETR~\cite{misra20213detr} 
and the detailed architecture is provided in the supplementary Sec.~\ref{supp:detailed_arch}.
The cross attention between object queries and the feature maps~(either from point clouds or images) aggregates relevant context onto the object candidates, while the self attention between object queries reasons pairwise relations between different object candidates. The query positions are embedded into -dimensional positional encoding with a Multilayer Perceptron~(MLP), {and element-wisely summed with the query features}. This enables the network to reason about both context and position jointly. 

The  object queries containing rich instance information are then independently decoded into boxes and class labels by a feed-forward network~(FFN). 
Following CenterPoint~\cite{Yin2020Centerbased3O}, our FFN predicts the center offset from the query position as , bounding box height as , size  as , yaw angle  as  and the velocity~(if available) as . We also predict a per-class probability  for  semantic classes. Each attribute is computed by a separate two-layer  convolution. 
 By decoding each object query into prediction in parallel, we get a set of predictions  as output, where  is the predicted bounding box for the -th query. 
Following \cite{misra20213detr}, we adopt the auxiliary decoding mechanism{,} which adds FFN and supervision after each decoder layer{. Hence}, we {can} have initial bounding box predictions from the first decoder layer. We leverage such initial predictions in the LiDAR-camera fusion module to constrain the cross attention, {as} explained in {the} next section. 

















\subsection{LiDAR-Camera Fusion}

\noindent\textbf{Image Feature Fetching.} Although impressive improvement has been brought by point-level fusion methods~\cite{Vora2020PointPaintingSF, Wang2021PointAugmentingCA}, their fusion quality is largely limited by the sparsity of LiDAR points. When an object only contains a small number of LiDAR points, it can fetch {only} the same number of image features, wasting the rich semantic information of high-resolution images. To mitigate this issue, we do not fetch the multiview image features based on the hard association between LiDAR points and image pixels. Instead, we retain all the image features  as our memory bank, and use the cross-attention mechanism in {the} transformer decoder to perform feature fusion 
in a sparse-to-dense and adaptive manner, as shown in Fig.~\ref{fig:pipeline}. 


\begin{figure}[t]
	\vspace{-0.5cm}
	\setlength{\abovecaptionskip}{0.0cm}
	\setlength{\belowcaptionskip}{-0.45cm}
	\center
    \includegraphics[width=8cm]{figures/cross_attention.pdf}
    \caption{The first row shows the input images and the predictions of object queries projected on the images, and the second row shows the cross-attention maps. Our fusion strategy is able to dynamically choose relevant image pixels and is not limited by the number of LiDAR {points}. The two images are picked from nuScenes and Waymo, respectively.}
    \label{fig:cross_attention}
\end{figure}

\noindent\textbf{SMCA for Image Feature Fusion.}
\label{subsubsec:img_fusion}
Multi-head attention is a popular mechanism to perform information exchange and build {a soft association} 
between two sets of inputs, and it has been widely used {for the feature matching task}~\cite{Sarlin2020SuperGlueLF, Sun2021LoFTRDL}. To mitigate the sensitivity towards sensor calibration and inferior image features brought by the hard-association strategy, we leverage the cross-attention mechanism to build {the soft association} between LiDAR and images, enabling the network to adaptively determine where and what information should be taken from the images. 

Specifically, we first {identify} the specific image {in which the object queries are located} 
using previous predictions as well as the calibration matrices, and then perform cross attention between {the} object queries and the corresponding image feature map.
However, as the LiDAR features and image features are from completely different domains, the object queries might attend to visual regions unrelated to the bounding box to be predicted, leading to a long training time for the network to accurately identify the proper regions on images. Inspired by \cite{Gao2021FastCO}, we design a spatially
modulated cross attention~(SMCA) module{,} which weighs the cross attention by a 2D circular Gaussian mask around the projected 2D center of each query. The 2D Gaussian {weight} mask  is generated in a similar way as CenterNet~\cite{Zhou2019ObjectsAP},

where  is the spatial indices of the weight mask ,  is the 2D center computed by projecting the query prediction 
onto the image plane,  is the radius of the minimum circumscribed circle
of the projected corners of the 3D bounding box, and  is the hyper-parameter to modulate the bandwidth of the Gaussian distribution. 
Then this weight map is element-wisely multiplied with the cross-attention map among all the {attention} heads.
In this way, each object query only attends to the related region around the projected 2D box, so that the network can learn where to select image features based on the input LiDAR features better and faster. The visualization of the attention map is shown in Fig.~\ref{fig:cross_attention}. The network typically tends to focus on the foreground pixels close to the object center and ignore the irrelevant pixels, providing valuable semantic information for {object classification and bounding box regression.}
After SMCA, we use another FFN to produce the final bound box predictions using the object queries containing both LiDAR and image information.









\subsection{Label Assignment and Losses} 
\noindent Following DETR~\cite{misra20213detr}, we find the bipartite matching between {the} predictions and ground truth objects through {the} Hungarian algorithm~\cite{Kuhn1955TheHM}, where the matching cost is defined by a weighted sum of classification, regression, and IoU cost{:}

where  is the binary cross entropy loss,  is the L1 loss between {the} {predicted BEV centers and the ground-truth centers~(both normalized in ),} 
and  is the IoU loss~\cite{Zhou2019IoULF} between {the} predicted boxes and ground-truth boxes.  are the coefficients of the individual cost terms. We provide sensitivity {analysis} of these terms in the supplementary Sec.~\ref{supp:label_assign}. Since the number of predictions is usually larger than that of GT boxes, the unmatched predictions are considered as negative samples.
Given all matched pairs, we compute {a} focal loss~\cite{Lin2017FocalLF} for the classification branch. The bounding box regression is supervised by {an} L1 loss for only positive pairs. For the heatmap prediction, we adopt a penalty-reduced focal loss following CenterPoint~\cite{Yin2020Centerbased3O}. The total loss is the weighted sum of losses for each component. We adopt the same label assignment strategy and loss formulation for both decoder layers.











\subsection{Image-Guided Query Initialization}
\label{subsubsec:img_query} 
{Since} our object queries are {currently} selected using only
LiDAR features, {it potentially leads to} sub-optimality in terms of the detection recall.
Empirically, our model already achieves high recall and shows superior performance over {the} baselines~(Sec.~\ref{sec:Exp}). Nevertheless, to further leverage the ability of high-resolution images 
{in detecting} {small} objects and make our algorithm more robust {against} sparse LiDAR point clouds, we propose an \secondmodule~strategy{,} which selects object queries leveraging both {the} LiDAR and camera information.
 
Specifically, we generate a LiDAR-camera BEV feature map  by projecting the image features  onto {the} BEV plane through cross attention with LiDAR BEV features .
Inspired by \cite{Roddick2020PredictingSM}, we use the multiview image features collapsed {a}long the height axis as {the key-value sequence of the attention mechanism,} 
as shown in Fig.~\ref{fig:img2bev}. The  {collapsing} operation is based on the observation that the relation between BEV locations and image columns can be established easily using camera geometry, and usually there is at most one object along each image column. Therefore, collapsing {along} the height axis {can} significantly reduce the computation without losing critical information. Although some fine-grained image features might be lost during this process, it already 
{meets} our need 
{as only a hint on potential object positions is required.} Afterward, similar {to} Sec. \ref{subsubsec: query_initialize}, we use  to predict the heatmap{, which is} averaged with the LiDAR-only heatmap  as the final heatmap . Using  to select and initialize the object queries, our model is able to detect objects that are difficult to detect in {LiDAR point clouds.}


Note that proposing a novel method to project the image features onto the BEV plane is beyond the scope of this paper{. We} believe that our method could benefit from more research progress~\cite{Roddick2019OrthographicFT, Roddick2020PredictingSM, Philion2020LiftSS} 
{in} this direction.

\begin{figure}[t]
	\vspace{-0.5cm}
	\setlength{\abovecaptionskip}{0.0cm}
	 \setlength{\belowcaptionskip}{-0.45cm}
	\center
    \includegraphics[width=8cm]{figures/img2bev.pdf}
    \caption{We first condense the image features along the vertical dimension, {and} then project the features onto {the} BEV plane using cross attention with the LiDAR BEV features. Each image is process{ed} by a separate multi-head attention layer{,} which captures the relation between image column and BEV locations.}
    \label{fig:img2bev}
\end{figure}


\xy{
}







