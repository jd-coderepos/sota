

\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage[pagenumbers]{cvpr} 

\usepackage[dvipsnames]{xcolor}
\newcommand{\red}[1]{{\color{red}#1}}
\newcommand{\todo}[1]{{\color{red}#1}}
\newcommand{\TODO}[1]{\textbf{\color{red}[TODO: #1]}}


 
\definecolor{cvprblue}{rgb}{0.21,0.49,0.74}
\usepackage[pagebackref,breaklinks,colorlinks,citecolor=cvprblue]{hyperref}
\usepackage{multicol, multirow, float}

\def\paperID{7054} \def\confName{CVPR}
\def\confYear{2024}

\title{Pretrain like Your Inference: Masked Tuning Improves Zero-Shot Composed Image Retrieval}

\author{Junyang Chen, Hanjiang Lai\\
School of Computer Science and Engineering, Sun Yat-Sen University\\
Guangzhou, Guangdong, China\\
{\tt\small chenjy855@mail2.sysu.edu.cn, laihanj3@mail.sysu.edu.cn}
}

\begin{document}
\maketitle

\begin{abstract}
    Zero-shot composed image retrieval (ZS-CIR), which aims to retrieve a target image based on textual modifications to a reference image without triplet labeling, has gained more and more attention. Current ZS-CIR research mainly relies on two unlabeled pre-trained models:  the vision-language model, e.g., CLIP, and the Pic2Word/textual inversion model. However, the pre-trained models and CIR tasks have substantial discrepancies, where the pre-trained models learn the similarities between vision and language but CIR aims to learn the modifications of the image guided by text. In this paper, we introduce a novel unlabeled and pre-trained masked tuning approach to reduce the gap between the pre-trained model and the downstream CIR task. We first reformulate the pre-trained vision-language contrastive learning as the CIR task, where we randomly mask input image patches to generate $\langle$masked image, text, image$\rangle$ triple from an image-text pair. Then, we propose a masked tuning, which uses the text and the masked image to learn the modifications of the original image. With such a simple design, it can learn to capture fine-grained text-guided modifications. Extensive experimental results demonstrate the significant superiority of our approach over the baseline models on three ZS-CIR datasets, including FashionIQ, CIRR, and CIRCO.
\end{abstract}

\section{Introduction}

Composed image retrieval (CIR) \cite{baldrati2022effective, liu2021image, lee2021cosmo} extends traditional image retrieval by incorporating natural language descriptions, which allows users to specify modifications to the query image using text, as shown in Figure~\ref{fig:intro} (a). This multi-modal input enables users to conduct fine-grained image retrieval, as the retrieved image must not only contain user-specified modifications but also remain consistent with the reference image~\cite{vo2019composing}. CIR has garnered significant attention due to its potential applications in various domains, including online retail and Internet search \cite{jandial2022sac, wu2021fashion}.

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{figs/Intro.jpg}
    \caption{(a) Workflow of composed image retrieval task, which is essentially different from the pre-trained VLM objective and textual inversion. (b) The pre-trained vision-language models~\cite{li2022blip, radford2021clip} are to align text and image features. (c) The recent ZS-CIR methods~\cite{Baldrati_2023_ICCV, saito2023pic2word} also introduced textual inversion into the pre-trained VLM, which mapped the reference image into the text domain, to further improve the performance.}
    \label{fig:intro}
\end{figure}

Current advanced work on CIR primarily relies on fine-tuning and fusion by leveraging the impressive capabilities of the pre-trained vision-language model (VLM)~\cite{baldrati2022effective, baldrati2022conditioned, liu2023bi}. As an illustration, Baldrati \textit{et al.} ~\cite{baldrati2022conditioned} employed a two-stage methodology, where the CLIP~\cite{radford2021clip} text encoder was firstly fine-tuned and then a fusion network was trained to combine the reference image and text features. Further, Liu \textit{et al.}~\cite{liu2023bi} took a more advanced step by employing a bidirectional training strategy to fine-tune the BLIP~\cite{li2022blip} text encoder and then trained the fusion network. Nevertheless, these methods require a large number of labeled data to support fully supervised training. The CIR dataset comprises a large number of triplets $\langle I_r, T, I_t\rangle$ that include reference images $I_r$, textual descriptions $T$, and target images $I_t$. Labeling these triples is a costly endeavor, which, in turn, restricts the scalability of existing fully supervised methods. Additionally, CIR models trained within a specific data domain may struggle to generalize to other domains. 

A new task, called zero-shot CIR (ZS-CIR)~\cite{Baldrati_2023_ICCV, saito2023pic2word}, is proposed to learn the retrieval models without the costly manually labeled data. The ZS-CIR approaches mainly build upon the powerful pre-trained vision-language models, e.g., CLIP~\cite{radford2021clip}, which were pre-trained on a large-scale dataset with image-text pairs. As illustrated in Figure~\ref{fig:intro} (b), the objective of the pre-trained models is to align features from image and text modalities. However, due to the gap between the CIR task and the pre-trained VLM, it may not had a good performance by simply applying the VLM to ZS-CIR. Hence, some approaches \cite{Baldrati_2023_ICCV, saito2023pic2word} had proposed an additional unlabeled trained model, e.g., textual inversion, to improve the performance. As depicted in Figure~\ref{fig:intro} (c), the textual inversion is first learned, where the reference image is translated into a pseudo-word vector, and it is concatenated with the relative text to obtain the query feature. Next, the VLM is utilized to retrieve target image features. 

However, the vision-language models and the textual inversion have substantial discrepancies with the CIR task. These pre-trained models learn the similarities or projections between the images and texts. For example, the image of white cloth should be similar to the word ``white cloth" (VLM), and textual inversion projects the white cloth to the text domain. While the CIR aims to learn the modification of the reference image guided by text. Given the image of white cloth and the text ``change it to black cloth", it is not to learn the semantic similarity or projection between the image and text, and CIR is to modify the image according to the text. 

In this paper, we propose a novel multi-modal self-supervised pre-training approach to bridge the gap between pre-trained vision-language models and downstream CIR tasks. The pivotal insight lies in adapting the pre-trained vision-language task more similar to the CIR task. To be more precise, we first make the pre-training data in the same form, i.e., changing the image-text pair $\langle T, I \rangle$  to the triplet form  $\langle I_m, T, I \rangle$. This triplet comprises a masked image, text, and the original image, where the masked image is obtained by randomly masking the original image. Then, we use the text $T$ to guide the masked image $I_m$ to obtain the fused feature, which is close to the feature of the original image $I$. Our idea here is very simple to let the pre-trained task approximate the CIR task. As an illustration, consider the scenario where ``A dog'' and ``bench'' are masked in the masked image $I_m$ accompanied by the text ``Two dogs sit on the bench''. In this context, the text signifies the necessary modifications to the masked image $I_m$ (another dog and bench should be added), while ensuring that the target image $I$ remains consistent with the masked image in unspecified attributes, like the color and breed of the unmasked dog. This aligns perfectly with the requirements of the CIR task semantically. 

While the method is straightforward, the gains are amazing, offering an intriguing avenue for further exploration of ZS-CIR. Experiments demonstrated significant enhancements in our approach when compared to baseline performance across three benchmark datasets: FashionIQ, CIRR, and CIRCO. Moreover, our proposed method outperforms previous state-of-the-art (SOTA) models and shows notable improvements, particularly on the FashionIQ dataset with Recall@10 and Recall@50, which show remarkable increases of up to 9.83\% and 10.71\%, respectively.

Our contributions can be summarized as follows:
\begin{itemize}
    \item We propose a novel pre-trained vision-language model that visually bridges the gap between pre-trained models and CIR tasks by simply employing masked tuning.
    \item Our proposed method enriches the CIR triplets by masking image-text pairs. This self-supervised pre-training technology ultimately reduces the retrieval model's reliance on costly triplet annotation data.
    \item Significant performance enhancements are observed in ZS-CIR across three benchmark datasets: FashionIQ, CIRR, and CIRCO. Our proposed method surpasses the baselines, establishing itself as the SOTA model.
\end{itemize}

\section{Related Work}
\subsection{Composed Image Retrieval}
Multi-modal compositional learning has gained widespread attention in diverse vision and language tasks, including visual question answering~\cite{li2021align}, image captioning~\cite{hu2022scaling}, and image generation~\cite{rombach2022high}. The composed image retrieval (CIR) pertains to multi-modal compositional learning, which is focused on retrieving a target image through the utilization of joint embedding features derived from compositions of a reference image and text~\cite{vo2019composing}. The field of CIR has seen significant exploration in the domain of fashion~\cite{wu2021fashion}, and more recently, it has been extended to real-life images~\cite{liu2021image}. 

Contemporary mainstream CIR research employs a supervised learning paradigm, wherein combinatorial multimodal information is acquired through training on CIR datasets. Cutting-edge CIR models~\cite{liu2021image, lee2021cosmo, baldrati2022effective, liu2023bi} hinge on a post-fusion strategy, i.e.,  fusion occurs subsequent to extracting visual and linguistic features using potent pre-trained encoders like CLIP and BLIP. Liu \textit{et al.}~\cite{liu2021image} demonstrated strong performance through the fine-tuning of CLIP text encoders for CIR tasks, along with the amalgamation of readily available image-text CLIP features by employing a combiner network. By implementing a bidirectional training scheme on the combiner, Liu \textit{et al.}~\cite{liu2023bi} attained performance enhancements for the BliP-based model. Another line of research shifts the focus away from network structure and instead delves into the intricacies of multi-granularity matching. Chen \textit{et al.}~\cite{chen2022composed} simulated multi-granularity queries by introducing normally distributed noise into the feature space. Furthermore, Chen and Lai~\cite{chen2023ranking} observed that current triple optimization methods tend to overlook semantic diversity and fail to model many-to-many mapping relationships with the necessary level of uncertainty. 

However, all of these methods necessitate tuning the model on the CIR datasets. Hence, recent advancements in CIR research have delved into the utilization of unlabelled pre-trained models to adapt to the downstream CIR task, notably in the form of zero-shot CIR (ZS-CIR). Pic2Word~\cite{saito2023pic2word} employed the pre-trained vision-language model in addition to unlabeled image datasets to train a textual inversion network. This network is designed to convert input images into linguistic tokens, enabling the flexible combination of image and text queries. SEARLE~\cite{Baldrati_2023_ICCV} employed image data and GPT-powered regularization to train a textual inversion network to generate a set of pseudo-word tokens. In contrast to these textual inversion-based approaches, our methodology adopt a novel pre-training paradigm. We choose to simply fine-tune the vision-language contrastive learning model, which can reduce the gap between the pre-training model and the CIR task. Furthermore, our approach eliminates the necessity for additional unlabeled image datasets typically required in the training of textual inversion models.

\subsection{Masked Modeling}
Masked language modeling~\cite{kenton2019bert, brown2020language} stands as a highly successful unsupervised representation learning approach in natural language processing (NLP). It involves the retention of a portion of the input sequence, and the pre-training model is trained to predict the missing content. This mask-based pre-training paradigm exhibits a robust ability to generalize to a variety of downstream NLP tasks, showcasing strong scalability~\cite{brown2020language}.
Inspired by the accomplishments in NLP, various image-specific masking techniques have been proposed. ViT~\cite{dosovitskiy2020vit} directly applied a standard Transformer~\cite{vaswani2017attention} to an image and predicted masked patches. In contrast, iGPT~\cite{chen2020generative} and BEiT~\cite{bao2021beit} respectively make predictions for masked pixels and discrete tokens. Notably, MAE~\cite{he2022masked} capitalized on the advantages of masking and exclusively inputs visible patches into ViT, resulting in improved training speed. 

Most recently, FLIP \cite{li2023scaling} has advanced masked modeling in vision-language models. FLIP is the most similar work. While FLIP \cite{li2023scaling}  emphasized the scaling of sparse computing implementation by masking, involving a trade-off between the benefits of processing more sample pairs and the potential degradation of sample-by-sample encoding. Although our proposed method also employed a masking model, our primary motivation focus lay in exploring how masking can serve as a bridge between pre-trained models and their application in downstream CIR tasks. Also, the inputs of FLIP were image-text pairs while our method took a triplet as input. The goals are totally different.


\section{Method}
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figs/overview.jpg}
    \caption{Overview of our masking cross-modal pre-training method. \textit{Left}: we randomly apply a high masking ratio to mask image patches, and let the pre-trained task approximate the CIR task. \textit{Right}: we leverage the pre-trained model  at inference time on ZS-CIR.}
    \label{fig:overview}
\end{figure*}

\subsection{Problem Definition}
In the settings of zero-shot composed image retrieval (ZS-CIR), we do not have any training data from the CIR dataset. Please note that not only the supervised triplet data but also unsupervised data is not available. 

ZS-CIR mainly builds upon the pre-trained vision-language models, e.g., CLIP. As indicated by the previous works~\cite{saito2023pic2word,Baldrati_2023_ICCV}, there is a gap between the pre-trained task and the CIR task. In this paper, we only use the image-text pairs $\langle T, I \rangle$, which are also used in the pre-trained vision-language model, to further finetune the vision-language model to reduce the gap.

\subsection{Masked Tuning}
As illustrated in Figure~\ref{fig:overview}, given the image-text pairs $\langle T, I \rangle$, our method utilizes a masking approach to generate an approximate triplet for CIR to adjust the pre-trained visual-language model. Specifically, we employ masking to deliberately omit image semantics and obtain the masked image. It can create a text-to-image modification query for input. The original, unmasked image serves as ``task supervision''. Our approach extends from image-text pairs to CIR triplets as a continuous pre-training strategy, which effectively reduces the gap between pre-trained models and CIR tasks. The key components of the proposed approach are described below.

Initially, we partition the image into non-overlapping blocks of a fixed size. The Vision Transformer (ViT) \cite{dosovitskiy2020vit} is utilized as the image encoder. Next, we proceed to randomly mask the blocks according to a uniform distribution and then utilize the remaining visible blocks as input to the ViT. We apply a high masking rate (50\% or 75\%) to obscure a majority of the blocks. This choice is deliberate and aimed at preventing the pre-trained model from depending on the extrapolation of semantic information from nearby visible blocks. Considering the inherent redundancy in image information \cite{he2022masked, li2023scaling}, the elevated masking rate compels the model to amalgamate the semantic information from the unmasked blocks with textual input to ``reconstruct'' the original unmasked image semantics.



The objective of the pre-training model is to minimize the cosine similarity between the combined query features and the target features, achieved through contrastive learning. Considering a multimodal dataset with image-text pairs, denoted as $S = \{(T_n, I_n)\}^N_{n=1}$, where $I$ and $T $ represent images and texts, respectively. By masking most of the patches in the image $I$, we obtain the masked image $I^m$. Consequently, we have a dataset $S^m = \{(I_n^m, T_n, I_n)\}^{N}_{n=1}$, where $I_n^m$ and $T_n$ serve as query inputs and $I_n$ is the target image. As shown in Figure \ref{fig:overview}, the masked image and text inputs of the query are separately encoded by the image encoder and text encoder, after which the combined feature $f^q$ is derived via simply averaging the two features. Simultaneously, the target image $I_n$ is fed through the image encoder, which employs shared weights, resulting in the target feature $f^t$. Subsequently, we train the image/text encoder to minimize the contrastive loss, utilizing other samples from the same batch as negative pairs to perform contrastive learning~\cite{chen2020simple}:
\begin{equation}
    \mathcal{L}_{CL}\left(f^q,f^t\right)=\frac1B\sum_{i=1}^B-\log\frac{\exp\left(\kappa\left(f^q_i,f^t_i\right)\right)}{\sum_{j=1}^B\exp\left(\kappa\left(f^q_i,f^t_j\right)\right)},
\end{equation} where $B$ is mini-batch size and $\kappa(\cdot)$ is cosine similarity function. It's noteworthy that, in accordance with the findings from research \cite{liu2023bi}, $f^q$ and $f^t$ undergo no additional projection or normalization during the pre-training phase. This contrastive loss compels the output of the combined feature to closely resemble the feature of the target image.

\subsection{Inference}
Thanks to the alignment between the motivation for pre-training and the requirements of the CIR task, we can effortlessly employ the pre-trained model to retrieve the target image, as depicted in Figure \ref{fig:overview}. During the inference stage, user provides the input $\langle I_r, T_r \rangle$ for the query. Subsequently, we obtain the combined feature $f_r$ by employing both the image encoder and text encoder as $f_r = (1-w)f_r^I + f_r^T$, where $w$ representing the mask rate and $f_r^I$ and $f_r^T$ denote the image and text features. We use the weighting method since we use the masked image as input in the pre-training model while we utilize the whole image as input in testing, which may lead to a distribution shift as discussed in FLIP~\cite{li2023scaling}. Hence, we propose the weighted method to alleviate this shift. Next, the features of the image dataset are also processed through the same image encoder. We locate the candidate image $I_t$ by comparing it with the combined feature. 


\textbf{Remark} Similar to the previous works~\cite{saito2023pic2word,Baldrati_2023_ICCV}, our method also needs an extra pre-training step. The previous works include two stages: 1) they firstly leveraged prior knowledge from the vision-language model, e.g., CLIP, using the large-scale datasets with image-text pairs, and 2) they then used another extra unlabeled image dataset to train an extra textural inversion model. Our method also includes two stages: 1) the pre-training vision-language model, and 2) we further finetune the VLM using the same dataset with image-text pairs. Therefore, we do not use more pre-training data than the existing methods.

\section{Experiments}

\subsection{Experimental Settings}
For a fair comparison, we follow \cite{Baldrati_2023_ICCV}, adopting the unlabeled test split of ImageNet1K \cite{russakovsky2015imagenet} as the pre-training dataset. This pre-training dataset comprises significantly less data compared to PALAVRA \cite{cohen2022my} and Pic2Word \cite{saito2023pic2word}, accounting for approximately 10\% and 3\% respectively. As ImageNet1K dataset exclusively comprises images, we employed the BLIP2-FlanT5-XL\footnote{\url{https://huggingface.co/Salesforce/blip2-flan-t5-xl}} vision-language model to generate corresponding captions for each image. The training process utilized the AdamW \cite{loshchilov2019decoupled} optimizer with a learning rate set at $10^{-6}$ and weight decay at $5 \times 10^{-5}$, following \cite{baldrati2022conditioned}. Contrastive learning was conducted with a batch size of 64. We employed the proposed method on three different pre-trained models CLIP ViT-B/32, CLIP ViT-L/14, and BLIP ViT-B/16. The mask ratios for CLIP and BLIP were set to 75\% and 50\%, respectively, according to ablation experiments. CLIP ViT-B/32 and BLIP ViT-B/16 were trained with one NVIDIA RTX3090, while CLIP ViT-L/14 was trained with four NVIDIA RTX3090. In the following table of results, we refer to ViT-B/32 and ViT-L/14 as B/32 and L/14, respectively. All proposed methods were implemented using the PyTorch \cite{NEURIPS2019_bdbca288} framework. To promote transparency and reproducibility, we will release the entire experimental codebase as open-source.


\subsection{Datasets and Metrics}
We conducted experiments to evaluate our method on three datasets: FashionIQ \cite{wu2021fashion}, CIRR \cite{liu2021image}, and CIRCO \cite{Baldrati_2023_ICCV}, adhering to established evaluation protocols \cite{baldrati2022effective}. For each dataset, we present the performance of the proposed method across three distinct backbone networks~\cite{radford2021clip, li2022blip}: CLIP ViT-B/32, CLIP ViT-L/14, and BLIP ViT-B/16.

\paragraph{FashionIQ.} FashionIQ \cite{wu2021fashion} is specialized in the domain of fashion and encompasses three distinct product categories: dresses, shirts, and toptee. In line with prior studies \cite{lee2021cosmo}, we adopt Recall@10 (R@10) and Recall@50 (R@50) as evaluation metrics, and calculate the average performance across these three subsets to gauge overall effectiveness. It is worth noting that all results pertaining to FashionIQ are derived from the validation set, as the test set remains undisclosed.

\paragraph{CIRR.} Compose Image Retrieval on Real-life Images (CIRR) \cite{liu2021image} dataset comprises over 36,000 pairs of crowd-sourced open-domain images juxtaposed with human-generated modified text. This dataset is curated to address the limitations found in existing datasets, notably the need for greater visual complexity and the prevalence of false negatives. In accordance with established evaluation protocols~\cite{liu2021image} in previous CIRR studies, we employed a combination of criteria, namely Recall@K (where K = 1, 5, 10, 50) and Recall$_{\text{Subset}}@K$ (where K = 1, 2, 3), as the evaluation metric. Notably, Recall$_{\text{Subset}}@K$ exclusively considers candidate pairs within the same subset for assessment.

\paragraph{CIRCO.} The Composed Image Retrieval on Common Objects in Context (CIRCO) \cite{Baldrati_2023_ICCV} dataset is introduced to address the challenge of unlabeled potential truth values for queries. CIRCO comprises a total of 1,020 queries, with each query possessing an average of 4.53 ground truth values. Notably, CIRCO surpasses the CIRR test set, which contains only 2,000 images of interference items, by utilizing the entirety of the 120,000 images from the COCO dataset \cite{lin2014microsoft} as the index set, thereby introducing a greater number of interference items. Benefiting from CIRCO having multiple ground truths, we followed the previous evaluation criterion~\cite{Baldrati_2023_ICCV} of mean Average Precision (mAP) as a more fine-grained metric. Specifically, we used mAP@K with $K= 5, 10, 25, 50$.

\begin{table*}[t]
\centering
  \resizebox{1\linewidth}{!}{ \begin{tabular}{clcccccccc} 
  \toprule
  \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{2}{c}{Shirt} & \multicolumn{2}{c}{Dress} &\multicolumn{2}{c}{Toptee} &\multicolumn{2}{c}{Average} \\
  \cmidrule(lr){3-4}
  \cmidrule(lr){5-6}
  \cmidrule(lr){7-8}
  \cmidrule(lr){9-10}
  \multicolumn{1}{c}{Backbone} & \multicolumn{1}{l}{Method} & R$@10$ & R$@50$ & R$@10$ & R$@50$ & R$@10$ & R$@50$ & R$@10$ & R$@50$ \\ 
  \midrule
  \multirow{8}{*}{CLIP B/32} & Image-only$^{\dagger}$ & 6.92 & 14.23 & 4.46 & 12.19 & 6.32 & 13.77 & 5.90 & 13.37 \\
  & Text-only$^{\dagger}$ & 19.87 & 34.99 & 15.42 & 35.05 & 20.81 & 40.49 & 18.70 & 36.84 \\ 
  & Captioning$^{\dagger}$~\cite{Baldrati_2023_ICCV} & 17.47 & 30.96 & 9.02 & 23.65 & 15.45 & 31.26 & 13.98 & 28.62 \\
  & PALAVRA$^{\dagger}$ \cite{cohen2022my} & 21.49 & 37.05 & 17.25 & 35.94 & 20.55 & 38.76 & 19.76 & 37.25 \\
  & SEARLE-OTI$^{\dagger}$~\cite{Baldrati_2023_ICCV} & \underline{25.37} & {41.32} & {17.85} & \underline{39.91} & {24.12} & {45.79} & {22.44} & {42.34} \\
  & SEARLE$^{\dagger}$~\cite{Baldrati_2023_ICCV} & {24.44} & \underline{41.61} & \underline{18.54} & {39.51} & \underline{25.70} & \underline{46.46} & \underline{22.89} & \underline{42.53}  \\ 
  & Baseline$^{\dagger}$ & 13.83 & 30.88 & 13.44 & 26.25 & 17.08 & 31.67 & 14.78 & 29.60 \\
  & Ours & \textbf{33.36} & \textbf{53.47} & \textbf{25.71} & \textbf{47.81} & \textbf{34.87} & \textbf{58.44} & \textbf{31.31} & \textbf{53.24} \\
  
  \midrule[.02em]
  \multirow{5}{*}{CLIP L/14} & Pic2Word$^{\S}$ \cite{saito2023pic2word} & 26.20 & 43.60 & 20.00 & 40.20 & 27.90 & 47.40 & 24.70 & 43.70 \\
  & {SEARLE-XL-OTI}$^{\dagger}$~\cite{Baldrati_2023_ICCV} & \underline{30.37} & \underline{47.49} & \underline{21.57} & \underline{44.47} & \underline{30.90} & \underline{51.76} & \underline{27.61} & \underline{47.90} \\
  & {SEARLE-XL}$^{\dagger}$~\cite{Baldrati_2023_ICCV} & {{26.89}} & {{45.58}} & {{20.48}} & {{43.13}} & {{29.32}} & {{49.97}} & {{25.56}} & {{46.23}} \\ 
  & Baseline$^{\S}$ & 21.00 & 34.50 & 16.30 & 33.60 & 22.20 & 39.00 & 19.80 & 35.70 \\
  & Ours & \textbf{38.63} & \textbf{58.51} & \textbf{28.11} & \textbf{51.12} & \textbf{39.42} & \textbf{62.68} & \textbf{35.39} & \textbf{57.44}\\


\bottomrule
  \end{tabular}}
  \caption{Quantitative evaluation results on FashionIQ validation set. The best and second-best scores are highlighted in bold and underlined, respectively. $^{\dagger}$ and $^\S$ indicates results are cited from \cite{Baldrati_2023_ICCV} and \cite{saito2023pic2word}, respectively.}
  \label{tab:fashioniq_val}
\end{table*}
\subsection{Results}
We furnish several zero-shot baseline models and previous competitive methodologies for a fair evaluation of our approach.

\begin{itemize}
    \item Baseline (Image + Text): This baseline method retrieves the target images by adding the image features extracted from the reference image with the features of the corresponding caption.
    \item Text-only: Similarity is assessed solely based on the CLIP text features derived from the corresponding caption.
    \item Image-only: This method retrieves the target images most akin to the reference images.
    \item Captioning~\cite{Baldrati_2023_ICCV}: The learnable soft prompt token is substituted with the caption generated from the reference image using a pre-trained captioning model for textual inversion training.
    \item PALAVRA~\cite{cohen2022my}: A two-stage approach predicated on textual inversion is employed, commencing with the utilization of a pre-trained mapping function, succeeded by an optimization procedure targeting the pseudo-word token.
    \item Pic2Word~\cite{saito2023pic2word}: This forward-only baseline leverages a pre-trained textual inversion network.
    \item SEARLE~\cite{Baldrati_2023_ICCV}: SEARLE coordinates the mapping of visual attributes from the reference image onto a generated word token within the CLIP token embedding space, which is integrated with the relative caption under the regularization of GPT~\cite{brown2020language}.
\end{itemize}



\subsubsection{Quantitative Results on FashionIQ}


Table \ref{tab:fashioniq_val} lists the performance of the proposed method alongside other approaches on the FashionIQ validation dataset. In general, our method demonstrates a substantial performance advantage over the baseline and previous state-of-the-art methods across all three backbone networks. Notably, on the CLIP ViT-B/32 backbone network, our method exhibits the most significant improvement in both average R@10 and R@50 metrics. To be precise, when compared to the previous state-of-the-art model SEARLE, our method showcases improvements of 8.42\% and 10.71\% in average R@10 and R@50, respectively. Considering the CLIP ViT-L/14 backbone network, our method demonstrates a significant boost over the SOTA model SEARLE-XL-OTI. And our method attains an average R@50 of 57.44\% using this backbone network, surpassing all other methods and claiming the top performance. 



\subsubsection{Quantitative Results on CIRR}
\begin{table*}[!ht]
 \centering
  \resizebox{1\linewidth}{!}{ \begin{tabular}{clccccccc} 
  \toprule
  \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{4}{c}{Recall$@K$} & \multicolumn{3}{c}{Recall$_{\text{Subset}}@K$} \\
  \cmidrule(lr){3-6}
  \cmidrule(lr){7-9}
  \multicolumn{1}{l}{Backbone} & \multicolumn{1}{l}{Method} & $K = 1$ & $K = 5$ & $K = 10$ & $K = 50$ & $K = 1$ & $K = 2$ & $K = 3$ \\ 
  \midrule
  \multirow{8}{*}{CLIP B/32} & Image-only$^{\dagger}$ & 6.89  & 22.99  & 33.68 & 59.23 & 21.04 & 41.04 & 60.31 \\ 
  & Text-only$^{\dagger}$ & 21.81 & 45.22 & 57.42 & 81.01 & \textbf{62.24} & \textbf{81.13} & \textbf{90.70} \\ 
  
  & Captioning$^{\dagger}$ & 12.46 & 35.04 & 47.71 & 77.35 & 42.94  & 65.49 & 80.36 \\
  & PALAVRA$^{\dagger}$ \cite{cohen2022my} & 16.62 & 43.49 & 58.51 & 83.95 & 41.61 & 65.30 & 80.94 \\
  & {SEARLE-OTI}$^{\dagger}$~\cite{Baldrati_2023_ICCV} & \textbf{24.27} & \underline{53.25} & \underline{66.10} & \underline{88.84} & 54.10 & 75.81 & 87.33 \\
  & {SEARLE}$^{\dagger}$~\cite{Baldrati_2023_ICCV} & \underline{24.00}  & \textbf{53.42} & \textbf{66.82} & \textbf{89.78} & \underline{54.89} & \underline{76.60} & \underline{88.19}  \\
  & Baseline$^{\dagger}$ & 11.71 & 35.06 & 48.94 & 77.49 & 32.77  & 56.89 & 74.96 \\
  & Ours & 18.80 & 46.07 & 60.75 & 86.41 & 44.29 & 68.10 & 	83.42 \\
  \midrule[.02em]
  
  \multirow{5}{*}{CLIP L/14} & Pic2Word$^\S$ \cite{saito2023pic2word} & 23.90 & 51.70 & 65.30 & 87.80 & -- & -- & -- \\
  & {SEARLE-XL-OTI}$^{\dagger}$~\cite{Baldrati_2023_ICCV} & {24.87} & {52.31} & {66.29} & {88.58} & {53.80} & {74.31} & {86.94}  \\
  & {SEARLE-XL}$^{\dagger}$~\cite{Baldrati_2023_ICCV} & \underline{{24.24}} & \underline{{52.48}} & \underline{{66.29}} & \textbf{{88.84}} & \underline{{53.76}} & \underline{{75.01}} & \underline{{88.19}} \\
  & Baseline$^\S$ & 12.40 & 36.20 & 49.10 & 78.20 &--&--&--\\
  & Ours & \textbf{25.52} & \textbf{54.58} & \textbf{67.59} & \underline{88.70} & \textbf{55.64} & \textbf{77.54} & \textbf{89.47} \\


\bottomrule
  \end{tabular}
  }
  \caption{Quantitative results on CIRR test set. Best and second-best scores are highlighted in bold and underlined, respectively. $^{\dagger}$ and $^\S$ indicates results from~\cite{Baldrati_2023_ICCV} and \cite{saito2023pic2word}, respectively. -- denotes results not reported in the original paper.}
  \label{tab:cirr_test}
\end{table*}

As shown in Table \ref{tab:cirr_test}, the results of our method are reported on the standard test set\footnote{\url{https://cirr.cecs.anu.edu.au/test_process/}}. It is worth noting a significant drawback of the CIRR dataset: the relative captions alone often suffice for image retrieval, rendering the information from reference images redundant. This observation has been previously discussed in studies \cite{saito2023pic2word, Baldrati_2023_ICCV}. In particular, the text-only baseline attains the highest performance on the Recall$_\text{Subset}$, surpassing both the Image-only and Image + Text methods across all metrics. This suggests that visual information not only provides little benefit but may even have a detrimental effect. This phenomenon is exacerbated when considering only a subset of reference and target images that exhibit high visual similarity.

Overall, our model demonstrates substantial improvements across all metrics compared to the baseline method, regardless of the backbone network. Notably, on the CLIP ViT-B/32 backbone network, SEARLE emerges as the top-performing method. Our approach surpasses the previous second-best method PALAVRA, securing a position of competitive performance. We attribute this performance gap to SEARLE's reliance on a textual inversion method that primarily uses text for image retrieval, aligning with the specific imperfection of the CIRR dataset. Consequently, when compared to our method, its performance aligns more closely or even surpasses that of the Text-only method. 

However, it's worth noting that, for SEARLE, employing the CLIP ViT-L/14 backbone network with more parameters does not result in a performance enhancement. Instead, it even leads to a slight degradation in performance. Consequently, the advantage gained by SEARLE-XL through the use of a larger-scale vision method on the CIRR dataset is marginal, suggesting that this method doesn't significantly benefit from model enlargement. In contrast, our method exhibits a substantial improvement when using the CLIP ViT-L/14 backbone network, surpassing CLIP ViT-B/32 across all metrics. This highlights our method's adroitness at harnessing the potent capabilities of larger models for visual information extraction, suggesting its potential for scalability to large-scale models. Furthermore, our method surpasses SEARLE-XL to achieve the SOTA performance. 



\subsubsection{Quantitative Results on CIRCO}

\begin{table}[!ht]
  \centering
  \Huge
  \resizebox{\linewidth}{!}{ \begin{tabular}{clcccc} 
  \toprule
  \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{4}{c}{mAP$@K$} \\
  \cmidrule(lr){3-6}
  \multicolumn{1}{l}{Backbone} & \multicolumn{1}{l}{Method} & $K=5$ & $K=10$ & $K=25$ & $K=50$ \\ \midrule
  \multirow{8}{*}{CLIP B/32} & Image-only$^{\dagger}$ & 1.34 & 1.60 & 2.12 & 2.41 \\
  & Text-only$^{\dagger}$ & 2.56 & 2.67 & 2.98 & 3.18 \\
  
  & Captioning$^{\dagger}$ & 5.48 & 5.77 & 6.44 & 6.85 \\
  & PALAVRA \cite{cohen2022my}$^{\dagger}$ & 4.61 & 5.32 & 6.33 & 6.80 \\
  & {SEARLE-OTI}~\cite{Baldrati_2023_ICCV}$^{\dagger}$ & {7.14} & {7.83} & {8.99} & {9.60} \\
   & {SEARLE}~\cite{Baldrati_2023_ICCV}$^{\dagger}$ & \textbf{9.35} & \textbf{9.94} & \textbf{11.13} & \textbf{11.84} \\ 
   & Baseline$^{\dagger}$ & 2.65 & 3.25 & 4.14 & 4.54 \\
   & Ours & \underline{8.14} & \underline{8.90} & \underline{10.12} & \underline{10.75}\\
   
   \midrule[.02em]
   
  \multirow{5}{*}{CLIP L/14} & Pic2Word$^{\dagger}$~\cite{saito2023pic2word} & 8.72 & 9.51& 10.64&11.29 \\
  & {SEARLE-XL-OTI}$^{\dagger}$~\cite{Baldrati_2023_ICCV} & {10.18} & {11.03} & {12.72} & {13.67} \\
  & {SEARLE-XL}$^{\dagger}$~\cite{Baldrati_2023_ICCV} & \textbf{11.68}  & \textbf{12.73} & \textbf{14.33} & \textbf{15.12} \\
  & Baseline & 4.02 & 4.93 &  6.13 & 6.78 \\
  & Ours & \underline{10.36} & \underline{11.63} & \underline{12.95} & \underline{13.67}\\




  \bottomrule
  \end{tabular}}
  \caption{Quantitative results on CIRCO test set considering  the multiple annotated ground truth. Best and second-best scores are highlighted in bold and underlined, respectively. $^{\dagger}$ indicates the results are cited from~\cite{Baldrati_2023_ICCV}.  Note that the comparison with SEARLE and SEARLE-XL is not fair, as they participate in the choice of ground truth images for CIRCO dataset, as mentioned in \cite{Baldrati_2023_ICCV}. Hence, without considering the SEARLE method, our proposed method is the state of the art.}
  \label{tab:circo_test}

\end{table}

Quantitative experimental results on the CIRCO test set\footnote{\url{https://circo.micc.unifi.it/evaluation}} are presented in Table \ref{tab:circo_test}. In general, the proposed method surpasses the previous state-of-the-art method SEARLE-OTI on CLIP backbone network. Additionally, our method exhibits a notable improvement over the baseline method across all backbone networks. It's important to point out that comparing our method with SEARLE-XL may not be entirely fair. This is because SEARLE-XL was employed for image retrieval from a selection of multiple ground truth images, as stated in \cite{Baldrati_2023_ICCV}. Furthermore, SEARLE-XL-OTI, while not directly involved in image selection, is used to distill a textual inversion network in SEARLE-XL. This may contribute to the reason why our method doesn't exhibit a significant improvement on CLIP ViT-L/14 when compared to CLIP ViT-B/32. Therefore, when relatively fairly compared to SEARLE-OTI, we can confidently consider our model as the SOTA method. 


\subsubsection{Quantitative Results on BLIP}
\begin{table}[!ht]
\centering
\Huge
  \resizebox{1\linewidth}{!}{ \begin{tabular}{lcccccccc} 
  \toprule
 \multicolumn{1}{c}{} & \multicolumn{2}{c}{Shirt} & \multicolumn{2}{c}{Dress} &\multicolumn{2}{c}{Toptee} &\multicolumn{2}{c}{Average} \\
  \cmidrule(lr){2-3}
  \cmidrule(lr){4-5}
  \cmidrule(lr){6-7}
  \cmidrule(lr){8-9}
   \multicolumn{1}{l}{Method} & R$@10$ & R$@50$ & R$@10$ & R$@50$ & R$@10$ & R$@50$ & R$@10$ & R$@50$ \\ 
  \midrule
     Baseline & 29.50 & 47.17 & 22.28 & 42.10 & 31.09 & 50.48 & 27.62 & 46.59 \\
       Ours & \textbf{38.09} & \textbf{57.79} & \textbf{28.62} & \textbf{50.78} & \textbf{40.92} & \textbf{62.68} & \textbf{35.88} & \textbf{57.08}\\
  \bottomrule
  \end{tabular}}
  \caption{Quantitative evaluation results on FashionIQ validation set. The best scores are highlighted in bold. }
  \label{tab:fashioniq_val_blip}
\end{table}

\begin{table}[!ht]
 \centering
 \Huge
  \resizebox{1\linewidth}{!}{ \begin{tabular}{lccccccc} 
  \toprule
  \multicolumn{1}{c}{} & \multicolumn{4}{c}{Recall$@K$} & \multicolumn{3}{c}{Recall$_{\text{Subset}}@K$} \\
  \cmidrule(lr){2-5}
  \cmidrule(lr){6-8}
   \multicolumn{1}{l}{Method} & $K = 1$ & $K = 5$ & $K = 10$ & $K = 50$ & $K = 1$ & $K = 2$ & $K = 3$ \\ 
  \midrule
  Baseline & 11.04 & 31.40 & 42.29 & 65.64 & 31.52 & 54.92 & 73.40 \\
  Ours & \textbf{27.23} & \textbf{58.87} & \textbf{71.40} & \textbf{91.25} & \textbf{55.13} & \textbf{77.42} & \textbf{89.11}\\
  \bottomrule
  \end{tabular}
  }
  \caption{Quantitative results on CIRR test set. The best and second-best scores are highlighted in bold and underlined, respectively.}
  \label{tab:cirr_test_blip}
\end{table}


\begin{figure*}
    \centering
    \includegraphics[width=0.8\linewidth]{figs/qualitative_results.jpg}
    \caption{Top-3 examples retrieved from CIRCO validation set. Ground truth retrievals are highlighted with red outline. We mainly compare the top-3 retrieved results of proposed method with the previous SOTA model SEARLE \cite{Baldrati_2023_ICCV}.}
    \label{fig:qualitative}
\end{figure*}


We establish a strong baseline model utilizing the BLIP ViT-B/16 backbone network. Notably, this baseline outperforms the previous SOTA model SEARLE on FashionIQ. Furthermore, the proposed method continues to exhibit significant performance enhancement when using this backbone, which outperforms all other methods by 35.88\% in terms of R@10. When evaluating the on CIRR, our method attains the highest performance in terms of the Recall metric. Finally, for completeness,  the performance of our method on CIRCO are 7.09\%, 8.03\%, 9.21\%, and 9.86\% when K is 5 , 10 , 25, and 50, respectively.


\subsubsection{Qualitative Visualization}
We show the top-3 retrieval results of the proposed method and the previous method SEARLE \cite{Baldrati_2023_ICCV} on CIRCO, as shown in Figure \ref{fig:qualitative}. Through qualitative visualization, we observe that SEARLE reduces combined image retrieval to text-to-image retrieval by textual inversion and ignores the information brought by the image itself. Specifically, at the top of Figure \ref{fig:qualitative}, SEARLE is unable to understand that the ``it" in ``next to it" should refer to the bear of the reference image. In essence, SEARLE fails to maintain the partial feature consistency of the reference image and the retrieved image. For example, at the bottom of Figure \ref{fig:qualitative}, the ``pizza" of the reference image is missing in the retrieved image. This indicates that the text does not specify the modification of this part of the features, but the text-to-image retrieval method tends to ignore the visual information of the reference image. In comparison, our model diligently captures and preserves more comprehensive visual information. This reflects that our model can perform combined image retrieval more accurately and provide a superior user experience.

\subsection{Ablation Study on Masking Ratio}

\begin{table}[!ht]
\centering
  \resizebox{1\linewidth}{!}{ \begin{tabular}{lccccc} 
  \toprule
  \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{4}{c}{Average} \\
  \cmidrule(lr){3-6}
  \multicolumn{1}{l}{Backbone} & \multicolumn{1}{c}{$w$} & R@1 & R@5 & R@10 & R@50 \\ 
  \midrule
  \multirow{3}{*}{CLIP B/32} 
& 0.25 & 7.49 & 17.67 & 24.07 & 42.36\\
  & 0.50 & \underline{9.63} & \underline{21.55} & \underline{29.27} & \underline{48.68} \\
  & 0.75 & \textbf{11.11} & \textbf{23.78} & \textbf{31.31} & \textbf{53.24}\\
  \midrule[.02em]
\multirow{3}{*}{BLIP B/16} 
& 0.25 & 10.33 & 21.20 & 27.92 & 49.03\\
  & 0.50 & \textbf{13.40} & \textbf{28.02} & \textbf{35.88} & \textbf{57.08} \\
  & 0.75 & \underline{13.28} & \underline{27.30} & \underline{35.23} & \underline{56.63}\\
  \bottomrule
  \end{tabular}}
  \caption{Ablation study of masking ratio $w$ to our method on the FashionIQ dataset.  The best scores are highlighted in bold.}
  \label{tab:mask ratio}
\end{table}

Table \ref{tab:mask ratio} presents the ablation experiments conducted on our method with varying mask ratios. For these experiments on CLIP and BLIP, we opted for three commonly used masking rates such as 25\%, 50\%, and 75\%. The experimental results demonstrate that higher masking ratios (50\% or 75\%) yield superior performance, aligning with the findings in \cite{li2023scaling, he2022masked}. To be specific, CLIP ViT-B/32 with a 75\% mask rate exhibits superior performance on the FashionIQ validation set. Conversely, when the mask rate is set at 50\%, BLIP ViT-B/16 outperforms. Furthermore, even though a 25\% mask rate proves to be less effective, it still surpasses the baseline methods in all cases.

\section{Conclusion}

In this study, we strived to address the challenging task of zero-shot composed image retrieval (ZS-CIR) and reduce the reliance of retrieval models on costly manually labeled CIR data. We introduced a novel cross-modal self-supervised pre-training method to intuitively narrow the gap between pre-trained models and ZS-CIR tasks. We employed a clever masking technique to generate a triplet that closely approximates the manually annotated CIR data. Furthermore, we utilized a high mask rate during training to encourage the model to compose both image and text semantics. Experimental results on three prominent CIR benchmark datasets (FashionIQ, CIRR, and CIRCO) demonstrated that our proposed method surpasses the baseline and previous competitive approaches, establishing a new state-of-the-art method.





{
    \small
    \bibliographystyle{ieeenat_fullname}
    \bibliography{main}
}


\end{document}
