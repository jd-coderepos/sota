\section{Experimental Results}
\label{sec.experiments}

We validate \textit{arch2vec} on three commonly used NAS search spaces. The details of the hyperparameters we used for searching in each search space are included in Appendix .


\textbf{NAS-Bench-101.} NAS-Bench-101 \cite{pmlr-v97-ying19a} is the first rigorous NAS dataset designed for benchmarking NAS methods. It targets the cell-based search space used in many popular NAS methods \cite{zoph2018learning,liu2018progressive,liu2018darts} and contains  unique neural architectures. Each architecture comes with pre-computed validation and test accuracies on CIFAR-10. The cell consists of 7 nodes and can take on any DAG structure from the input to the output with at most 9 edges, with the first node as input and the last node as output. The intermediate nodes can be either 11 convolution, 33 convolution or 33 max pooling. We split the dataset into 90\% training and 10\% held-out test sets for \textit{arch2vec} pre-training.

\textbf{NAS-Bench-201.} Different from NAS-Bench-101, the cell-based search space in NAS-Bench-201 \cite{dong2020nasbench201} is represented as a DAG with nodes representing sum of feature maps and edges associated with operation transforms. Each DAG is generated by 4 nodes and 5 associated operations: 11 convolution, 33 convolution, 33 average pooling, skip connection and zero, resulting in a total of  unique neural architectures. The training details for each architecture candidate are provided for three datasets: CIFAR-10, CIFAR-100 and ImageNet-16-120 \cite{tinyimagenet17}. We use the same data split as used in NAS-Bench-101.

\textbf{DARTS search space}. The DARTS search space \cite{liu2018darts} is a popular search
space for large-scale NAS experiments. The search space consists of two cells: a convolutional cell and a reduction cell, each with six nodes. For each cell, the first two nodes are the outputs from the previous two cells. The next four nodes contain two edges as input, creating a DAG. The network is then constructed by stacking the cells. Following \cite{liu2018progressive}, we use the same cell for both normal and reduction cell, allowing roughly  DAGs without considering graph isomorphism. We randomly sample 600,000 unique architectures in this search space following the mobile setting \cite{liu2018darts}. We use the same data split as used in NAS-Bench-101. 


For pre-training, we use a five-layer Graph Isomorphism Network (GIN) with hidden sizes of \{128, 128, 128, 128, 16\} as the encoder and a one-layer MLP with a hidden dimension of 16 as the decoder. The adjacency matrix is preprocessed as an undirected graph to allow bi-directional information flow. After forwarding the inputs to the model, the reconstruction error is minimized using Adam optimizer \cite{kingma:adam} with a learning rate of . We train the model with batch size 32 and the training loss is able to converge well after 8 epochs on NAS-Bench-101, and 10 epochs on NAS-Bench-201 and DARTS. After training, we extract the architecture embeddings from the encoder for the downstream architecture search.


In the following, we first evaluate the pre-training performance of \textit{arch2vec} (\S4.1) and then the neural architecture search performance based on its pre-trained representations (\S4.2).

\vspace{-1mm}
\subsection{Pre-training Performance} \label{sec:pretraining}
\vspace{-1mm}










\textbf{Observation (1):} We compare \textit{arch2vec} with two popular baselines GAE \cite{kipf2016variational} and VGAE \cite{kipf2016variational} using three metrics suggested by \cite{zhang2019d}: 1) Reconstruction Accuracy (reconstruction accuracy of the held-out test set), 2) Validity (how often a random sample from the prior distribution can generate a valid architecture), and 3) Uniqueness (unique architectures out of valid generations). As shown in Table \ref{table:pretraining}, \textit{arch2vec} outperforms both GAE and VGAE, and achieves the highest reconstruction accuracy, validity, and uniqueness across all the three search spaces. This is because encoding with GINs outperforms GCNs in reconstruction accuracy due to its better neighbor aggregation scheme; the KL term effectively regularizes the mapping from the discrete space to the continuous latent space, leading to better generative performance measured by validity and uniqueness. Given its superior performance, we stick to \textit{arch2vec} for the remainder of our evaluation.    





\vspace{-2mm}
\begin{table}[h]
\begin{adjustbox}{width=1.01\columnwidth,center}
\scriptsize{
\begin{tabular}{cccc|ccc|ccc}
\hline
\multirow{2}{*}{\textbf{Method}} & \multicolumn{3}{c|}{\textbf{NAS-Bench-101}}  & \multicolumn{3}{c|}{\textbf{NAS-Bench-201}} & \multicolumn{3}{c}{\textbf{DARTS}}  \\ \cline{2-10} 
& Accuracy & Validity & Uniqueness & Accuracy & Validity & Uniqueness &  Accuracy & Validity & Uniqueness \\ \hline
GAE \cite{kipf2016variational}  & 98.75 & 29.88 & 99.25 &  99.52 & 79.28 & 78.42 &  97.80 & 15.25 & 99.65 \\ \hline
VGAE \cite{kipf2016variational} & 97.45 & 41.18 & 99.34 &  98.32 & 79.30 & 88.42 &  96.80 & 25.25 & 99.27 \\ \hline
\textit{arch2vec (w.o. KL)} & \textbf{100} & 30.31 & 99.20 & \textbf{100} & 77.09 & 96.57 &  99.46 & 16.01 & 99.51 \\ \hline
\textit{arch2vec} & \textbf{100} & \textbf{44.97} & \textbf{99.69} & \textbf{100} & \textbf{79.41} & \textbf{98.72} &  \textbf{99.79} & \textbf{33.36} & \textbf{100} \\ \hline
\end{tabular}
}
\end{adjustbox}
\vspace{1mm}
\caption{Reconstruction accuracy, validity, and uniqueness of different GNNs.}
\vspace{-3mm}
\label{table:pretraining}
\end{table}

















\begin{wrapfigure}{htbp}{2.6in}
\begin{minipage}{2.6in}
\vspace{-7mm}
\resizebox{2.6in}{!}{\includegraphics{pred_compare_seed123_new.jpg}}
\vspace{-6mm}
\caption{{\footnotesize Predictive performance comparison between \textit{arch2vec} (left) and supervised architecture representation learning (right) on NAS-Bench-101.}}
\vspace{-3mm}
\label{fig:pred_compare_seed123}
\end{minipage}
\end{wrapfigure}
\textbf{Observation (2):} 
We compare \textit{arch2vec} with its supervised architecture representation learning counterpart on the predictive performance of the latent representations. This metric measures how well the latent representations can predict the performance of the corresponding architectures.
Being able to accurately predict the performance of the architectures based on the latent representations makes it easier to search for the high-performance points in the latent space. 
Specifically, we train a Gaussian Process model with 250 sampled architectures to predict the performance of the other architectures, and report the predictive performance across 10 different seeds. We use RMSE and the Pearson correlation coefficient (Pearson's r) to evaluate points with test accuracy higher than 0.8. 
Figure \ref{fig:pred_compare_seed123} compares the predictive performance between \textit{arch2vec} and its supervised counterpart on NAS-Bench-101. 
As shown, \textit{arch2vec} outperforms its supervised counterpart\footnote{The RMSE and Pearson's r are: 0.0380.025 / 0.530.09 for the supervised architecture representation learning, and 0.0180.001 / 0.670.02 for \textit{arch2vec}. A smaller RMSE and a larger Pearsonâ€™s r indicates a better predictive performance.}, indicating \textit{arch2vec} is able to better capture the local structure relationship of the input space and hence is more informative on guiding the downstream search process.












\textbf{Observation (3):} In Figure \ref{fig:l2_edit_correlation}, we plot the relationship between the L2 distance in the latent space and the edit distance of the corresponding DAGs between two architectures. As shown, for \textit{arch2vec}, the L2 distance grows monotonically with increasing edit distance. This result indicates that \textit{arch2vec} is able to preserve the closeness between two architectures measured by edit distance, which potentially benefits the effectiveness of the downstream search. In contrast, such closeness is not well captured by supervised architecture representation learning.






\textbf{Observation (4):} 
In Figure~\ref{fig:tsne}, we visualize the latent spaces of NAS-Bench-101 learned by \textit{arch2vec} (left) and its supervised counterpart (right) in the 2-dimensional space generated using t-SNE. We overlaid the original colorscale with red (>92\% accuracy) and black (<82\% accuracy) for highlighting purpose.
As shown, for \textit{arch2vec}, the architecture embeddings span the whole latent space, and architectures with similar accuracies are clustered together. 
Conducting architecture search on such smooth performance surface is much easier and is hence more efficient. 
In contrast, for the supervised counterpart, the embeddings are discontinuous in the latent space, and the transition of accuracy is non-smooth. This indicates that joint optimization guided by accuracy cannot injectively encode architecture structures. As a result, architecture does not have its unique embedding in the latent space, which makes the task of architecture search more challenging.

\begin{figure*}[t]
    \centering
    \begin{minipage}[ht]{0.45\textwidth}
    \vspace{3.5mm}
    \includegraphics[scale=0.56]{l2_edit_comparison_new.jpg}
    \caption{Comparing distribution of L2 distance between architecture pairs by edit distance on NAS-Bench-101, measured by 1,000 architectures sampled in a long random walk with 1 edit distance apart from consecutive samples. left: \textit{arch2vec}. right: supervised architecture representation learning.}
\label{fig:l2_edit_correlation}
    \end{minipage}
    \hspace{0.1cm}
    \begin{minipage}[ht]{0.5\textwidth}
    \centering
    \includegraphics[scale=0.59]{tsne_compare_new.jpg}
    \caption{Latent space 2D visualization \cite{vanDerMaaten2008} comparison between \textit{arch2vec} (left) and supervised architecture representation learning (right) on NAS-Bench-101. Color encodes test accuracy. We randomly sample  points and average the accuracy in each small area.}
    \label{fig:tsne}
    \end{minipage}
    \vspace{-4mm}
\end{figure*}

\vspace{2mm}
\textbf{Observation (5):} To provide a closer look at the learned latent space, Figure~\ref{fig:nn_visualization} visualizes the architecture cells decoded from the latent space of \textit{arch2vec} (upper) and supervised architecture representation learning (lower).
For \textit{arch2vec}, the adjacent architectures change smoothly and embrace similar connections and operations. This indicates that unsupervised architecture representation learning helps model a smoothly-changing structure surface. As we show in the next section, such smoothness greatly helps the downstream search since architectures with similar performance tend to locate near each other in the latent space instead of locating randomly. 
In contrast, the supervised counterpart does not group similar connections and operations well and has much higher edit distances between adjacent architectures. This biases the search direction since dependencies between architecture structures are not well captured. 



\vspace{-2mm}
\begin{figure*}[h]
\centering
\includegraphics[scale=0.79]{graph_visualization_compare_new.jpg}
\caption{Visualization of a sequence of architecture cells decoded from the learned latent space of \textit{arch2vec} (upper) and supervised architecture representation learning (lower) on NAS-Bench-101. The two sequences start from the same architecture. For both sequences, each architecture is the closest point of the previous one in the latent space excluding previously visited ones.  Edit distances between adjacent architectures of the upper sequence are 4, 6, 1, 5, 1, 1, 1, 5, 2, 3, 2, 4, 2, 5, 2, and the average is 2.9. Edit distances between adjacent architectures of the lower sequence are 8, 6, 7, 7, 9, 8, 11,  11, 6, 10, 10, 11, 10, 11, 9, and the average is 8.9.}
\vspace{-1mm}
\label{fig:nn_visualization}
\end{figure*}






\vspace{1mm}
\subsection{Neural Architecture Search (NAS) Performance}



\begin{comment}
\vspace{-5mm}
\makeatletter\def\@captype{figure}\makeatother
\begin{minipage}{0.38\textwidth}
\centering
\includegraphics[scale=0.36]{pred_compare_seed123_new.jpg}
\caption{Predictive performance comparison between \textit{arch2vec} (left) and supervised architecture representation learning (right) on NAS-Bench-101.}
\label{fig:predictive_performance}
\end{minipage}
\hspace{0.3mm}
\makeatletter\def\@captype{table}\makeatother
\begin{minipage}{0.6\textwidth}
\begin{adjustbox}{width=1\columnwidth,center}
\scriptsize{
\begin{tabular}{c|c|c|c|c} 
\hline
\textbf{NAS Methods} & \textbf{\#Queries} &  \textbf{Test Accuracy (\%)} & \textbf{Encoding} & \textbf{Search Method} \\ \hline
Random Search \cite{pmlr-v97-ying19a} & 1000 & 93.54 & Discrete & Random \\ 
RL \cite{pmlr-v97-ying19a}  & 1000 & 93.58 & Discrete &  REINFORCE \\ 
BO \cite{pmlr-v97-ying19a} & 1000 & 93.72 & Discrete & Bayesian Optimization \\
REA \cite{pmlr-v97-ying19a}  & 1000 & 93.72 & Discrete &  Evolution \\ \hline
NAO \cite{NAO} & 1000 & 93.74 & Supervised & Gradient Decent \\ 
BANANAS \cite{white2019bananas} & 500 & 94.08 & Supervised & Bayesian Optimization \\ 
RL (ours) & 400 & 93.74 & Supervised & REINFORCE \\
BO (ours) & 400 & 93.79 & Supervised & Bayesian Optimization \\ \hline
arch2vec-RL & \textbf{400} & \textbf{94.10} & Unsupervised & REINFORCE \\ 
arch2vec-BO & 400 & 94.05 & Unsupervised & Bayesian Optimization \\ \hline
\end{tabular}  
}
\end{adjustbox}
\caption{Performance comparison of different state-of-the-art NAS methods on NAS-Bench-101. The table shows the mean performance of 500 independent runs given the number of queried architectures.}
\label{table:nasbench101-query-comparison}
\end{minipage}
\vspace{2mm}
\end{comment}





\textbf{NAS results on NAS-Bench-101.}
For fair comparison, we reproduced the NAS methods which use the adjacency matrix-based encoding in \cite{pmlr-v97-ying19a}\footnote{\url{https://github.com/automl/nas_benchmarks}}, including Random Search (RS) \cite{bergstra12randsearch}, Regularized Evolution (RE) \cite{real2019regularized},  REINFORCE \cite{williams92rl} and BOHB \cite{falkner-icml-18}. 
For supervised architecture representation learning-based  methods, the hyperparameters are the same as \textit{arch2vec}, except that the architecture representation learning and search are jointly optimized. Figure \ref{fig:nasbench101-search} and Table \ref{table:nasbench101-query-comparison} summarize our results. 


\begin{figure*}[ht]
\centering
\includegraphics[scale=0.38]{nas101_search_results_new.png}
\caption{Comparison of NAS performance between discrete encoding, supervised architecture representation learning, and \textit{arch2vec} on NAS-Bench-101. The plot shows the mean test regret (left) and the empirical cumulative distribution of the final test regret (right) of 500 independent runs given  a wall-clock time budget of  seconds.}
\vspace{1mm}
\label{fig:nasbench101-search}
\end{figure*}

\begin{table}[t]
\begin{adjustbox}{width=0.9\columnwidth,center}
\scriptsize{
\begin{tabular}{c|c|c|c|c} 
\hline
\textbf{NAS Methods} & \textbf{\#Queries} &  \textbf{Test Accuracy (\%)} & \textbf{Encoding} & \textbf{Search Method} \\ \hline
Random Search \cite{pmlr-v97-ying19a} & 1000 & 93.54 & Discrete & Random \\ 
RL \cite{pmlr-v97-ying19a}  & 1000 & 93.58 & Discrete &  REINFORCE \\ 
BO \cite{pmlr-v97-ying19a} & 1000 & 93.72 & Discrete & Bayesian Optimization \\
RE \cite{pmlr-v97-ying19a}  & 1000 & 93.72 & Discrete &  Evolution \\ \hline
NAO \cite{NAO} & 1000 & 93.74 & Supervised & Gradient Decent \\ 
BANANAS \cite{white2019bananas} & 500 & 94.08 & Supervised & Bayesian Optimization \\ 
RL (ours) & 400 & 93.74 & Supervised & REINFORCE \\
BO (ours) & 400 & 93.79 & Supervised & Bayesian Optimization \\ \hline
\textit{arch2vec}-RL & \textbf{400} & \textbf{94.10} & Unsupervised & REINFORCE \\ 
\textit{arch2vec}-BO & 400 & 94.05 & Unsupervised & Bayesian Optimization \\ \hline
\end{tabular}  
}
\end{adjustbox}
\vspace{1mm}
\caption{Comparison of NAS performance between \textit{arch2vec} and SOTA methods on NAS-Bench-101. It reports the mean performance of 500 independent runs given the number of queried architectures.}
\vspace{-4mm}
\label{table:nasbench101-query-comparison}
\end{table}




As shown in Figure \ref{fig:nasbench101-search}, BOHB and RE are the two best-performing methods using the adjacency matrix-based encoding. However, they perform slightly worse than supervised architecture representation learning because the high-dimensional input may require more observations for the optimization. In contrast, supervised architecture representation learning focuses on low-dimensional continuous optimization and thus makes the search more efficient. As shown in Figure \ref{fig:nasbench101-search} (left), \textit{arch2vec} considerably outperforms its supervised counterpart and the adjacency matrix-based encoding after  wall clock seconds. Figure \ref{fig:nasbench101-search} (right) further shows that \textit{arch2vec} is able to robustly achieve the lowest final test regret after  seconds across 500 independent runs. 










Table \ref{table:nasbench101-query-comparison} shows the search performance comparison in terms of number of architecture queries. While RL-based search using discrete encoding suffers from the scalability issue, \textit{arch2vec} encodes architectures into a lower dimensional continuous space and is able to achieve competitive RL-based search performance with only a simple one-layer LSTM controller. For NAO \cite{NAO}, its performance is inferior to \textit{arch2vec} as it entangles structure reconstruction and accuracy prediction together, which inevitably biases the architecture representation learning.


\vspace{1mm}
\textbf{NAS results on NAS-Bench-201.}
For CIFAR-10, we follow the same implementation established in NAS-Bench-201 by searching based on the validation accuracy obtained after 12 training epochs with converged learning rate scheduling. The search budget is set to  seconds. The NAS experiments on CIFAR-100 and ImageNet-16-120 are conducted with a budget that corresponds to the same number of queries used in CIFAR-10. 
As listed in Table \ref{table:NasBench201}, searching with \textit{arch2vec} leads to better validation and test accuracy as well as reduced variability among different runs on all datasets.

\begin{table}[t]
\begin{adjustbox}{width=\columnwidth,center}
\scriptsize{
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{\textbf{NAS Methods}}  & \multicolumn{2}{c|}{\textbf{CIFAR-10}}                     & \multicolumn{2}{c|}{\textbf{CIFAR-100}}                  & \multicolumn{2}{c|}{\textbf{ImageNet-16-120}}              \\ \cline{2-7} & validation              & test                    & validation             & test                   & validation              & test                    \\ \hline
RE~\cite{real2019regularized}      & 91.080.43          & 93.840.43          & 73.020.46         & 72.860.55         & 45.780.56          & 45.630.64          \\
RS~\cite{bergstra12randsearch}  & 90.940.38          & 93.750.37          & 72.170.64         & 72.050.77         & 45.470.65          & 45.330.79          \\
REINFORCE~\cite{williams92rl}     & 91.030.33          & 93.820.31          & 72.350.63         & 72.130.79         & 45.580.62          & 45.300.86          \\
BOHB~\cite{falkner-icml-18}      & 90.820.53          & 93.610.52          & 72.590.82         & 72.370.90         & 45.440.70          & 45.260.83          \\ \hline
\textit{arch2vec}-RL    & 91.320.42          & 94.120.42          & 73.130.72         & 73.150.78         & 46.220.30 & 46.160.38          \\
\textit{arch2vec}-BO    & \textbf{91.410.22} & \textbf{94.180.24} & \textbf{73.350.32} & \textbf{73.370.30} & \textbf{46.340.18}          & \textbf{46.270.37} \\ \hline
\end{tabular}
}
\end{adjustbox}
\vspace{1mm}
\caption{The mean and standard deviation of the validation and test accuracy of different algorithms under three datasets in NAS-Bench-201. The results are calculated over 500 independent runs.} \vspace{-1mm}
\label{table:NasBench201}
\end{table}

\vspace{1mm}
\textbf{NAS results on DARTS search space.}
Similar to \cite{white2019bananas}, we set the budget to 100 queries in this search space. In each query, a sampled architecture is trained for 50 epochs and the average validation error of the last 5 epochs is computed. To ensure fair comparison with the same hyparameters setup, we re-trained the architectures from works that \emph{exactly}\footnote{\url{https://github.com/quark0/darts/blob/master/cnn/train.py}} use DARTS search space and report the final architecture. 
As shown in Table \ref{table:darts}, \textit{arch2vec} generally leads to competitive search performance among different cell-based NAS methods with comparable model parameters. The best performed cells and transfer learning results on ImageNet \cite{imagenet_cvpr09} are included in Appendix. 


\begin{table}[t] 
  \begin{adjustbox}{width=\columnwidth,center}
  \scriptsize{
  \begin{tabular}{|c|c|c|c|c|c|c|c|c|} 
  \hline
   &   \multicolumn{2}{c|}{\textbf{Test Error}} & \textbf{Params (M)} & \multicolumn{3}{c|}{\textbf{Search Cost}} &  \multicolumn{2}{c|}{}  \\ \hline
  \textbf{NAS Methods} &  Avg & Best &  & Stage 1 & Stage 2 & Total &  \textbf{Encoding} & \textbf{Search Method}           \\ \hline
  Random Search \cite{liu2018darts} & 3.290.15 & - & 3.2 & - & - & 4 & - & Random \\ 
  ENAS \cite{enas} & - & 2.89 & 4.6 & 0.5 & - & - & Supervised & REINFORCE \\ 
  ASHA \cite{Li2019RandomSA} & 3.030.13 & 2.85 & 2.2 & - & - & 9 & - & Random \\ 
  RS WS \cite{Li2019RandomSA} & 2.850.08 & 2.71 & 4.3 & 2.7 & 6 & 8.7 & - & Random \\  
  SNAS \cite{Xie2018SNAS} & 2.850.02 & - & 2.8 & 1.5 & - & - & Supervised & GD \\ 
  DARTS \cite{liu2018darts} & 2.760.09 & - & 3.3 & 4 & 1 & 5 & Supervised & GD \\ 
  BANANAS \cite{white2019bananas} & 2.64 & 2.57 & 3.6 & 100 (queries) & - & 11.8 & Supervised & BO \\ \hline 
  Random Search (ours) & 3.10.18 & 2.71 & 3.2 & - & - & 4 & - & Random  \\ 
  DARTS (ours) & 2.710.08 & 2.63 & 3.3 & 4 & 1.2 & 5.2 & Supervised & GD  \\ 
  BANANAS (ours) & 2.670.07 & 2.61 & 3.6 & 100 (queries) & 1.3 & 11.5 & Supervised & BO \\ \hline
  \textit{arch2vec}-RL & 2.650.05 & 2.60 & 3.3 & 100 (queries) & 1.2 & 9.5 &  Unsupervised & REINFORCE \\ 
  \textit{arch2vec}-BO & \textbf{2.560.05} & \textbf{2.48} & 3.6 & 100 (queries) & 1.3 & 10.5 & Unsupervised & BO \\ \hline  
  \end{tabular} 
  }
  \end{adjustbox}
  \vspace{0.5mm}
  \caption{Comparison with state-of-the-art cell-based NAS methods on DARTS search space using CIFAR-10. The test error is averaged over 5 seeds. Stage 1 shows the GPU days (or number of queries) for model search and Stage 2 shows the GPU days for model evaluation.}
  \vspace{-2mm}
  \label{table:darts}
  \end{table} 


  





