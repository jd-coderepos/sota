\section{Experiments}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.8\textwidth]{figs/MidFusionDistill}
	\caption{Model architecture of UMT for RGB (left) and depth (right) modalities.}
	\label{fig:mid}
\end{figure}

\label{Sec_4: experimental_results}
In this section, we test the effectiveness of UMT on two standard multi-modal tasks: audio-visual classification and RGB-Depth semantic segmentation. In ~\S\ref{sec:audio_visual}, we verify our method on audio-visual classification and report the performance. we also compare different solutions for modality failure. In \S\ref{sec:rgbd}, we demonstrate UMT in middle fusion task, \textit{i.e.}, semantic segmentation. In each sub-section, we first introduce the dataset, then describe the implementation details on how to apply our method to a specific task. We also provide further empirical demonstrations of modality failure through evaluating the performance of different encoders, as well as showing how UMT tackles this issue. Here, all the networks were built and trained using PyTorch~\cite{paszke2019pytorch} and all experiments were done by using one NVIDIA GeForce RTX 3090 GPU.

\subsection{Audio-visual Classification Experiment}\label{sec:audio_visual}


\paragraph{Dataset.} VGGSound dataset \cite{chen2020vggsound}, which contains over 200k video clips for 309 different sound classes, is used for evaluating our method. It is an audio-visual dataset \textit{in the wild} where each object that emits sound is also visible in the corresponding video clip, making it suitable for scene classification tasks. Please note that some clips in the dataset are no longer available on YouTube, and we actually use about 175k videos for training and 15k for testing, but the number of sound classes remains the same. 

\paragraph{Implementation details.} Two ResNet18~\cite{he2016deep} backbones are employed as our video and audio encoders respectively (whether 3D CNN is needed depends on the input), aiming at extracting visual and acoustic features simultaneously. Then we apply the late fusion, \textit{i.e.}, fusing the visual and acoustic features before the linear classifier, to incorporate video and audio information. We design a preprocessing paradigm to improve training efficiency as follows: (1) each video is interpolated to 256256 and saved as stacked images; (2) each audio is first converted to 16 kHz and 32-bit precision in the floating-point PCM format, then randomly cropped or tiled to a fixed duration of 10s. For video input, 32 frames are uniformly sampled from each clip before feeding to the video encoder. While for the audio input, a 1024-point discrete Fourier transform is performed using nnAudio \cite{cheuk2020nnaudio}, with 64 ms frame length and 32 ms frame-shift. And we only feed the magnitude spectrogram to the audio encoder. Please note that we do not use any kinds of data augmentation for both video and audio input. Besides, all the models are trained with a batch size of 24 and an initial learning rate of 1e-3 for 20 epochs, using the Adam optimizer~\cite{kingma2014adam}. We also apply a fixed learning rate scheduler, \textit{i.e.}, decay the learning rate by 0.1 for every 5 epochs and 10 epochs on UMT and baseline methods respectively. We use MSE loss as our distillation objective and we set the  parameter in Equation \eqref{eq:umt} to 50. Unless otherwise stated, all distillations were performed on feature-level.  





\paragraph{Baseline methods.} 
We compare UMT with other baseline methods, \textit{e.g.}, naive fusion, Gradient-Blending \cite{wang2020makes}, fine-tuning a multi-modal classifier from encoders pre-trained on uni-modal data, dropout \cite{srivastava2014dropout}, self distillation \cite{zhang2019your} and modality dropout. Besides, we also implement two other methods, \textit{i.e.}, video-only distillation and audio-only distillation, to further support our hypothesis. 

\begin{table}[t]
	\centering
	\renewcommand{\arraystretch}{1.1}
	\caption{Model performance comparison under UMT and ESANet on NYU-DepthV2 RGB-Depth semantic segmentation task.}
	\vspace{5pt}
	\begin{tabular}{c c c c}
		\toprule
		\multirow{2}*{\textbf{Initialization}} & \multicolumn{2}{c}{\textbf{Training Setting}}  & \multirow{2}*{\textbf{Improve}} \\ \cline{2-3} & ESANet~\cite{seichter2020efficient} & UMT\\
		\midrule
		From Scratch & 38.59 & 40.45 & \textbf{1.86} \\
		ImageNet Pre-train  & 48.48  & 49.14 & \textbf{0.66} \\
		\bottomrule
	\end{tabular}
	\label{tb:rgbd-result}
\end{table}


\paragraph{Results.}
From the results presented in Table~\ref{tb:av_result}, it is clear that UMT outperforms all the baseline methods by a large margin, suggesting that distillation could circumvent modality failure in joint training. In addition to showing that UMT can greatly improve performance, we also compare different methods to help us understand the fundamental issues of multi-modal learning:
\begin{itemize}
    \item Fine-tuning a multi-modal classifier over pre-trained uni-modal encoders can outperform naive fusion. This implies the modality failure problem is critical in fusion-based methods.
    \item UMT outperforms finetuning over pre-trained uni-modal encoders, showing that multi-modal learning can help the encoders to learn features that are unique to corresponding multi-modal tasks, since UMT differs from uni-modal pre-training by the multi-modal objective.
    \item We compare the results of vanilla Dropout \cite{srivastava2014dropout} with Modality Dropout. Our results show that modality-wise dropout is more effective in multi-modal learning. Also, self-distillation performing worse than UMT also implies we should pay more attention to modality-wise features.
    \item Distilling from only one modality can deteriorate the performance of other modalities. The results in Table~\ref{tb:modality_failure} and our analysis in Section~\ref{Sec_3: inspirations from theory} show that modality failure is caused by the learning process of strong modalities. Thus, only distilling from one modality is not able to avoid modality failure.
\end{itemize}


\paragraph{Encoder evaluation.}
We also do another experiment, \textit{i.e.}, fix the encoder from different training methods and fine-tune a classifier, which allows us to have a better understanding of why UMT outperforms baseline methods. As we can see in  Table~\ref{tb:modality_failure}, UMT achieves the best results on both audio and video modalities, manifesting that it could keep the representation power of each modality. It is worth noting that, while performing distillation on a single modality, the representation of the corresponding modality will be maintained, but that of the other modality will be significantly degraded. Hence, we further demonstrate that the inferior performance of naive fusion is because the stronger modal encoder would rapidly learn a powerful feature to fit the samples, while the weaker encoder cannot learn enough, which leads to modality failure.


\subsection{RGB-Depth Semantic Segmentation Experiment}
\label{sec:rgbd}
\paragraph{Dataset.} We evaluate our approach on the commonly used RGB-D multi-class indoor semantic segmentation dataset, namely NYUv2 dataset, which contains 1449 indoor RGB-Depth data totally and we use 40-class label setting. The number of training set and testing set is 795 and 654 respectively. 

\paragraph{UMT in semantic segmentation.} In contrast to the late fusion classification task, the RGB-Depth semantic segmentation belongs to middle fusion. Since features generated by each layer matter, we distill multi-scale depth feature maps using the MSE loss. For feature maps from the RGB encoder, however, since they are generated by fusing RGB and depth modalities, we cannot distill RGB feature maps directly like depth feature maps. To mitigate this effect, we curate predictors, namely 2 layers CNNs, aiming to facilitate the fused feature maps to predict the RGB feature maps trained by the RGB modality before distillation. The full schematic diagram is presented in Figure~\ref{fig:mid}.
\paragraph{Implementation details.} Our segmentation UMT is based on a state-of-the-art method, \textit{i.e.}, ESANet \cite{seichter2020efficient}. For RGB modality, the ResNet34 backbone, downsampling method, and contextual module are employed following \cite{orsic2019defense, zhao2017pyramid}. When it comes to the decoder, it receives skip connections from the encoder, which is akin to U-Net \cite{ronneberger2015u}. For Depth modality, we leverage another encoder that focuses on extracting geometric information and fuse it with RGB feature maps at the five different scales using the attention mechanism. We keep all hyper-parameters (\textit{e.g.}, learning rate, optimization schemes, dropout ratio, \textit{etc}.) the same as the official implementation\footnote{\url{https://github.com/TUI-NICR/ESANet}}.


\begin{table}[t]
	\centering
	\renewcommand{\arraystretch}{1.1}
	\caption{Ablation study on RGB-Depth semantic segmentation setting. “Initialization” indicates how weights are initialized for the network, “from Depth” represents end-to-end training with a depth-only segmentation network, and “from RGB+depth” refers to freezing the depth encoder from ESANet \cite{seichter2020efficient} then fine-tuning with a new decoder.}
	\vspace{5pt}
	\begin{tabular}{c c c c}
		\toprule
		\multirow{2}*{\textbf{Initialization}} & \multicolumn{2}{c}{\textbf{Training Setting}}  & \multirow{2}*{\textbf{Drop}} \\ \cline{2-3} & from Depth & from RGB+Depth\\
		\midrule
		From Scratch & 32.69 & 28.53 & \textbf{-4.16} \\
		ImageNet Pre-train  & 39.45  & 34.73 & \textbf{-4.72} \\
		\bottomrule
	\end{tabular}
	\label{tb:depth-ablation}
\end{table}


\paragraph{Results.} It is common to use ImageNet pre-trained parameters as the initialization weights to achieve higher performance \cite{orsic2019defense}. To this end, we train the network on both scenarios: (1) training from scratch on NYUv2; (2) pre-training on ImageNet followed by fine-tuning on NYUv2. As shown in Table~\ref{tb:rgbd-result}, UMT improves the mean Intersection over Union (mIoU) metric on both settings, demonstrating its effectiveness. Furthermore, the increment discrepancy on training from scratch is more apparent than that on fine-tuning, manifesting that each encoder in multi-modal architecture could learn better general-purpose representations \cite{huh2016makes} after pre-training.




\paragraph{Encoder evaluation.} To explore whether modality failure also exists in RGB-Depth segmentation, we evaluate the depth encoder from ESANet\cite{seichter2020efficient} by freezing its parameters and fine-tuning a new decoder. We compare it with uni-depth setting. Please note that only the depth encoder is applicable for this experiment since the RGB encoder has been fused with the depth information. As shown in Table~\ref{tb:depth-ablation}, it turns out the encoder from RGB-Depth yields worse performance on both types than uni-depth setting, which verifies our hypothesis.
