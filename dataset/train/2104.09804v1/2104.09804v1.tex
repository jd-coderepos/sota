\ifx\allfiles\undefined

\documentclass[letterpaper]{article}
\begin{document}
\else
\chapter{experiments}
\fi

\begin{table*}[t]
   \centering
   \footnotesize
\begin{tabular}{c|c|c|c|cccc|cccc|c}
       \hline
       &
       \multicolumn{1}{c|}{ \multirow{2}{*}{Method}} & \multicolumn{1}{c|}{ \multirow{2}{*}{Reference}}& \multicolumn{1}{c|}{ \multirow{2}{*}{Modality}} & \multicolumn{4}{|c|}{} & \multicolumn{4}{|c|}{} & \multicolumn{1}{|c}{\multirow{2}{*}{Time (ms)}} \\
       \cline{5-12}
       &
       \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{}& \multicolumn{1}{|c}{Easy} & \multicolumn{1}{c}{Mod} & \multicolumn{1}{c}{Hard} & \multicolumn{1}{c|}{mAP} &
       \multicolumn{1}{|c}{Easy} & \multicolumn{1}{c}{Mod} & \multicolumn{1}{c}{Hard} & \multicolumn{1}{c|}{mAP} & \multicolumn{1}{|c}{} \\
       \hline
       \hline
         \parbox[t]{2mm}{\multirow{14}{*}{\rotatebox[origin=c]{90}{Two-stage}}}
& MV3D~\cite{MV3D}     &CVPR 2017   & {LiDAR+RGB}   & 74.97  & 63.63  & 54.00 &64.2 &86.62 &78.93 &69.80 &78.45 & 360\\
         & F-PointNet~\cite{FPOINTNET} &CVPR 2018   & {LiDAR+RGB}   & 82.19  & 69.79  & 60.59 &70.86 &91.17 &84.67 &74.77 &83.54 & 170\\
         & AVOD~\cite{AVOD}            &IROS 2018   & {LiDAR+RGB}   & 83.07  & 71.76  & 65.73 &73.52 &89.75 &84.95 &78.32 &84.34 & 100\\
& PointRCNN~\cite{shi2019pointrcnn} &CVPR 2019   & {LiDAR}     & 86.96  & 75.64  & 70.70 &77.77 &92.13 &87.39 &82.72 & 87.41 & 100\\
         & F-ConvNet~\cite{wang2019frustum}  &IROS 2019   & {LiDAR+RGB}   & 87.36  & 76.39  & 66.69 &76.81 &91.51 &85.84 &76.11 &84.49 & 470*\\
         & 3D IoU Loss~\cite{zhou2019iou}    &3DV 2019   & {LiDAR}     & 86.16  & 76.50  & 71.39 &78.02 &91.36  &86.22  &81.20  &86.26 & 80*\\
& Fast PointRCNN~\cite{Chen2019fastpointrcnn} &ICCV 2019   & {LiDAR}     & 85.29  & 77.40  & 70.24 &77.64 &90.87 &87.84 &80.52 &86.41 & 65\\
         & UberATG-MMF~\cite{liang2019multi} &CVPR 2019   & {LiDAR+RGB}   & 88.40  & 77.43  & 70.22 &78.68 &93.67 &88.21 &81.99 &87.96 & 80\\
         & Part-~\cite{shi2020points}   &TPAMI 2020   & {LiDAR}     & 87.81  & 78.49  & 73.51 &79.94 &91.70  &87.79  &84.61  &88.03 & 80\\
& STD~\cite{yang2019std}            &ICCV 2019   & {LiDAR}     & 87.95  & 79.71  & 75.09 &80.92 &94.74 &89.19 &86.42 & 90.12 & 80\\
         & 3D-CVF~\cite{yoo20203d}           &ECCV 2020   & {LiDAR+RGB}   & 89.20  & 80.05  & 73.11 &80.79 &93.52  &89.56 &82.45  &88.51 & 75\\
         & CLOCs PVCas~\cite{pang2020clocs}  &IROS 2020   & {LiDAR+RGB}     & 88.94  & 80.67  &77.15 &82.25  &93.05  &89.80  &86.57  &89.81 &100*\\
         & PV-RCNN~\cite{shi2020pv}          &CVPR 2020   & {LiDAR}     & 90.25  & 81.43  & 76.82 & 82.83 &94.98  &90.65  &86.14  &90.59 & 80*\\
         & De-PV-RCNN~\cite{2020deformable} &ECCVW 2020   & {LiDAR}     & 88.25  & 81.46  & 76.96 &82.22 &92.42  &90.13  &85.93  &89.49 & 80*\\
      \hline
      \hline
         \parbox[t]{2mm}{\multirow{12}{*}{\rotatebox[origin=c]{90}{One-stage}}}
& VoxelNet~\cite{zhou2018voxelnet} &CVPR 2018   &{LiDAR}      & 77.82  & 64.17  & 57.51 &66.5 &87.95 &78.39 &71.29 &79.21 & 220\\
         & ContFuse~\cite{CONTFUSE}         &ECCV 2018   &{LiDAR+RGB}    & 83.68  & 68.78  & 61.67 &71.38 &94.07 &85.35 &75.88 & 85.1 & 60\\
         & SECOND~\cite{yan2018second}      &Sensors 2018   &{LiDAR}      & 83.34  & 72.55  & 65.82 &73.9 &89.39 &83.77 &78.59 &83.92 & 50\\
         & PointPillars~\cite{lang2019pointpillars}&CVPR 2019   &{LiDAR}      & 82.58  & 74.31  & 68.99 &75.29 &90.07 &86.56 &82.81 &86.48 & \bf 23.6\\
         & TANet~\cite{liu2020tanet}        &AAAI 2020   &{LiDAR}      & 84.39  & 75.94  & 68.82 &76.38 &91.58  &86.54  &81.19  &86.44 & 34.75\\
         & Associate-3Ddet~\cite{du2020associate}&CVPR 2020   &{LiDAR}      & 85.99  & 77.40  & 70.53 &77.97 &91.40  &88.09  &82.96  &87.48 & 60\\
         & HotSpotNet~\cite{chen2019object}  &ECCV 2020   & {LiDAR}      &87.60 &78.31 &73.34 &79.75 &94.06 &88.09 &83.24 & 88.46 & 40*\\
         & Point-GNN~\cite{shi2020point}    &CVPR 2020   &{LiDAR}      & 88.33  & 79.47  & 72.29 &80.03 &93.11  &89.17  &83.90  &88.73 & 643\\
         & 3DSSD~\cite{yang20203dssd}       &CVPR 2020   &{LiDAR}      & 88.36  & 79.57  & 74.55 &80.83 &92.66  &89.02  &85.86  &89.18 & 38\\
         & SA-SSD~\cite{he2020structure}    &CVPR 2020    &{LiDAR}      & 88.75  & 79.79  & 74.16 &80.90 &95.03 &91.03 &85.96 &90.67 & 40.1\\
         & CIA-SSD~\cite{zheng2020cia}    &AAAI 2021    &{LiDAR}      & 89.59   & 80.28   & 72.87  &80.91 &93.74 &89.84 &82.39  &88.66 & 30.76\\
         \cline{2-12}
         & \bf SE-SSD (ours) &- &{LiDAR} & \bf91.49  &\bf82.54  &\bf77.15 &\bf83.73 &\bf95.68 &\bf91.84 &\bf86.72 &\bf91.41 & 30.56\\
      \hline
   \end{tabular}
\vspace*{1mm}
   \caption{Comparison with the state-of-the-art methods on the KITTI \textit{test} set for car detection, with 3D and BEV average precisions of 40 sampling recall points evaluated on the KITTI server. Our SE-SSD attains the highest precisions for all difficulty levels with a very fast inference speed, outperforming all prior detectors. ``*'' means the runtime is cited from the submission on the KITTI website.
   }
   \vspace*{-3mm}
   \label{table1}
\end{table*}

\begin{table}[t]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{@{\hspace{1mm}}c@{\hspace{1mm}}|@{\hspace{1mm}}c@{\hspace{1mm}}c@{\hspace{1mm}}c@{\hspace{1mm}}|@{\hspace{1mm}}c@{\hspace{1mm}}c@{\hspace{1mm}}c@{\hspace{1mm}}|@{\hspace{1mm}}c@{\hspace{1mm}}}
    \hline
      \multirow{2}{*}{Method}&
        \multicolumn{3}{@{\hspace{1mm}}c@{\hspace{1mm}}|@{\hspace{1mm}}}{3D}
         &\multicolumn{3}{@{\hspace{1mm}}c@{\hspace{1mm}}|@{\hspace{1mm}}}{BEV}
         &\multicolumn{1}{@{\hspace{1mm}}c@{\hspace{1mm}}}{3D}\\
       \multicolumn{1}{@{\hspace{1mm}}c|@{\hspace{1mm}}}{} & Easy & Moderate & Hard & Easy & Moderate & Hard & Moderate\\ \hline \hline
        3DSSD~\cite{yang20203dssd}     & -        & -         & -       & -        & -           & -     &  79.45 \\
        SA-SSD~\cite{he2020structure}  & 92.23    & 84.30     & 81.36   & -        & -           & -     &  79.91 \\
        De-PV-RCNN~\cite{2020deformable}&-& 84.71     & -       & -        & -           & -     &  83.30 \\
        PV-RCNN~\cite{shi2020pv}       & 92.57    & 84.83     & 82.69   & 95.76    & 91.11       & 88.93 &  83.90 \\\hline
    \textbf{SE-SSE (ours)} &\textbf{93.19} &\textbf{86.12}&\textbf{83.31}&\textbf{96.59}&\textbf{92.28}&\textbf{89.72}&\textbf{85.71}\\ \hline
\end{tabular}
}
\vspace*{0mm}
\caption{Comparison with latest best two single- and two-stage detectors on KITTI \textit{val} split for car detection, in which ``R40'' and ``R11'' mean 40 and 11 sampling recall points for AP, respectively. }
\label{table3}
\vspace{-2.5mm}
\end{table}



\section{Experiments}
We evaluate our SE-SSD on the KITTI 3D and BEV object detection benchmark~\cite{geiger2013vision}.
This widely-used dataset contains 7,481 training samples and 7,518 test samples.
Following the common protocol, we further divide the training samples into a training set (3,712 samples) and a validation set (3,769 samples).
Our experiments are mainly conducted on the most commonly-used car category and evaluated by the average precision with an IoU threshold 0.7.
Also, the benchmark has three difficulty levels in the evaluation: easy, moderate, and hard, based on the object size, occlusion, and truncation levels, in which the moderate average precision is the official ranking metric for both 3D and BEV detection on the KITTI website.
We will {\em release our code on GitHub\/} upon the publication of this work.

Figure~\ref{detresults} shows 3D bounding boxes (2nd \& 5th rows) and BEV bounding boxes (3rd \& 6th rows) predicted by our SE-SSD model for six different inputs, demonstrating its high-quality prediction results.
Also, for a better visualization of the results, we project and overlay the 3D predictions onto the corresponding images (1st \& 4th rows).
Please refer to the supplemental material for more experimental results.

\subsection{Implementation Details}
\textbf{Data preprocessing}
We only use LiDAR point clouds as input and voxelize all points in ranges [0, 70.4], [-40, 40], and [-3, 1] meters into a grid of resolution [0.05, 0.05, 0.1] along , , and , respectively.
We empirically set hyperparameters =0.25, =0.05, and =0.1 (see Section~\ref{sec:3.2}).
Besides shape-aware data augmentation, we adopt three types of common data augmentation:
(i) mix-up~\cite{yan2018second}, which randomly samples ground-truth objects from other scenes and add them into the current scene;
(ii) local augmentation on points of individual ground-truth object,~\eg, random rotation and translation; and
(iii) global augmentation on the whole scene, including random rotation, translation, and flipping.
The former two are for preprocessing the inputs to both teacher and student SSDs.


\textbf{Training details}
We adopt the ADAM optimizer and cosine annealing learning rate~\cite{loshchilov2016sgdr} with a batch size of four for 60 epochs.
We follow~\cite{tarvainen2017mean} to ramp up  (Eq.~\eqref{totalloss}) from 0 to 1 in the first 15 epoches using a sigmoid-shaped function .
We set
 and  (Section~\ref{sec:3.3}) as 0.3 and 0.7, respectively,
 and  (Eq.~\eqref{totalloss}) as 2.0 and 0.2, respectively,
the EMA decay weight as 0.999, and
 (Eq.~\eqref{diou}) as 1.25.


\subsection{Comparison with State-of-the-Arts}
By submitting our prediction results to the KITTI server for evaluation, we obtain the 3D and BEV average precisions of our model on the KITTI test set and compare them with the state-of-the-art methods listed in Table~\ref{table1}.

As shown in the table, our model ranks the  place among all state-of-the-art methods for both 3D and BEV detections in all three difficulty levels.
Also, the inference speed of our model ranks the  place among all methods, about 2.6 times faster than the latest best two-stage detector Deformable PV-RCNN~\cite{2020deformable}.
In 3D detection, our one-stage model attains a significant improvement of 1.1 points on moderate AP compared with PV-RCNN~\cite{2020deformable} and Deformable PV-RCNN~\cite{shi2020pv}.
For single-stage detectors, our model also outperforms all prior works by a large margin, outperforming the previous one-stage detector SA-SSD~\cite{he2020structure} by an average of 2.8 points for all three difficulty levels and with shorter inference time (reduced by 25).
Our large improvement in APs comes mainly from a better model optimization by exploiting both soft and hard targets, and the high efficiency of our model is mainly due to the nature of our proposed methods,~\ie, we refine features in SSD without incurring extra computation in the inference.

In BEV detection, our model also leads the best single- and two-stage detectors by around 0.8 points on average.
Besides, we calculate the mean average precision (mAP) of three difficulty levels for comparison.
Our higher mAPs indicate that SE-SSD attains a more balanced performance compared with others, so our method is more promising to address various cases more consistently in practice.
Further, we compare our SE-SSD with latest best two single- and two-stage methods on KITTI \textit{val} split.
As shown in Table~\ref{table3}, our 3D and BEV moderate APs with 11 or 40 recall points both outperform these prior methods.




\subsection{Ablation Study}
Next, we present ablation studies to analyze the effectiveness of our proposed modules in SE-SSD on KITTI \textit{val} split.
Table~\ref{table4} summarizes the ablation results on our consistency loss (``Cons loss''), ODIoU loss (``ODIoU''), and shape-aware data augmentation (``SA-DA'').
For ODIoU loss, we replace it with the Smooth- loss in this ablation study, since we cannot simply remove it like Cons loss and SA-DA.
All reported APs are with 40 recall points.



\begin{table}[t]
\centering
\resizebox{0.98\columnwidth}{!}{
\begin{tabular}{ccc|ccc}
    \hline
      \multicolumn{1}{c}{Cons loss} &\multicolumn{1}{c}{ODIoU} &\multicolumn{1}{c|}{SA-DA} &\multicolumn{1}{c}{ \multirow{1}{*}{Easy}} &\multicolumn{1}{c}{ \multirow{1}{*}{Moderate}} &\multicolumn{1}{c}{ \multirow{1}{*}{Hard}}\\
      \hline\hline
         -        &   -        &   -        & 92.58    & 83.22       & 80.15    \\
         -        &   -        & \checkmark & 93.02    & 83.70       & 80.68   \\
         -        & \checkmark &   -        & 93.07    & 83.85       & 80.78   \\
       \checkmark &   -        &   -        & 93.13    & 84.15       & 81.17   \\
       \checkmark & \checkmark &   -        & 93.17    & 85.81       & 83.01   \\
       \checkmark & \checkmark & \checkmark & \textbf{93.19}  & \textbf{86.12}  & \textbf{83.31}\\ \hline
\end{tabular}
}
\vspace*{0mm}
 \caption{Ablation study on our designed modules.
 We report the 3D average precisions of 40 sampling recall points on KITTI \textit{val.}~split for car detection.
 Cons Loss and SA-DA denote our consistency loss and shape-aware data augmentation, respectively.
 }
\label{table4}
\vspace{-1.5mm}
\end{table}



\textbf{Effect of consistency loss}
As first and fourth rows in Table~\ref{table4} show, our consistency loss boosts the moderate AP by about 0.9 point.
This large improvement shows that using the more informative soft targets can contribute to a better model optimization.
For the slight increase in easy AP, we think that the predictions of the baseline on the easy subset are already very precise and thus are very close to the hard targets already.
Importantly, by combining hard labels with the ODIoU loss in the optimization, our SE-SSD further boosts the moderate and hard APs by about 2.6 points, as shown in the fifth row in Table~\ref{table4}.
This demonstrates the effectiveness of jointly optimizing the model by leveraging both hard and soft targets with our designed constraints.

Further, we analyze the effect of the consistency loss for bounding boxes (``reg'') and confidence  (``cls'') separately to show the effectiveness of the loss on both terms.
As Table~\ref{table5} shows, the gain in AP from the confidence term is larger, we argue that the confidence optimization may be more effective to alleviate the misalignment between the localization accuracy and classification confidence.
Also, we evaluate the box and confidence constraints~\cite{zhao2020sess} designed on the box-centers distance matching strategy and obtain a much lower AP (``dist''), we think that the underlying reason is related to their design, which is to address axis-aligned boxes and so is not suitable for our task.


\textbf{Effect of ODIoU loss}
As first and third rows in Table~\ref{table4} show, our ODIoU loss improves the moderate AP by about 0.6 points compared with the Smooth- loss.
This gain in AP is larger than the one with the SA-DA module, thus showing the effectiveness of the constraints on both the box-centers distance and orientation difference in the ODIoU loss.
Also, the gain in AP on the hard subset is larger than others, which is consistent with our expectation that even sparse points on the object surface could provide sufficient information to regress the box centers and orientations.


\textbf{Effect of shape-aware data augmentation}
In Table~\ref{table4}, the first two rows indicate that our shape-aware data augmentation (SA-DA) scheme brings an average improvement of about 0.5 points on the baseline model.
Based on the pre-trained SSD,  SA-DA further improves the moderate and hard APs of SE-SSD by about 0.3 points, as indicated in the last two rows in Table~\ref{table4}.
These gains in AP show the effectiveness of our SA-DA on boosting the performance by enhancing the object diversity and model generalizability.


\textbf{IoU-Based Matching Strategy}
Also, we compare different ways of filtering soft targets,~\ie, removing soft targets that
(i) overlap with each other using NMS (``nms filter''),
(ii) do not overlap with any ground truth (``gt filter''), and
(iii) do not overlap with student boxes for less than an IoU threshold (``stu filter'').
We can see from Table~\ref{table6} that our proposed ``stu filter'' attains the largest gain in AP, as it keeps the most related and informative soft targets for the student predictions, compared with other strategies.

\begin{table}[t]
\small
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{@{\hspace{1mm}}c@{\hspace{1.5mm}}|@{\hspace{1.5mm}}c@{\hspace{2mm}}c@{\hspace{2mm}}c@{\hspace{2mm}}c@{\hspace{2mm}}c@{\hspace{1mm}}}
    \hline
       Type         &baseline   & dist   & reg only      & cls only      & cls + reg                   \\ \hline
       Moderate AP  &83.22      & 80.38      & 83.65         & 83.83         & \textbf{84.15}                \\  \hline
\end{tabular}
}
\vspace*{0.25mm}
 \caption{Ablation study on our consistency loss, in which ``cls'' and ``reg'' mean our consistency loss on confidence and bounding boxes, respectively, and ``dist'' means the box and confidence constraints based on a box-centers distance matching strategy.}
\label{table5}
\vspace{-1.5mm}
\end{table}


\begin{table}[t]
\small
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{@{\hspace{1mm}}c|c@{\hspace{3mm}}c@{\hspace{3mm}}c@{\hspace{3mm}}c@{\hspace{1mm}}}
    \hline
       Type          &baseline  & nms filter      & gt filter         & stu filter             \\ \hline
       Moderate AP   &83.22     &  83.49          & 80.73             & \textbf{84.15}           \\  \hline
\end{tabular}
}
\vspace*{0.25mm}
 \caption{Ablation study on our IoU-based matching strategy, in which ``nms'', ``gt'', and ``stu'' mean that we filter soft targets with NMS, ground truths, and student predictions, respectively.}
\label{table6}
\vspace{-1.5mm}
\end{table}


\subsection{Runtime Analysis}
The overall inference time of SE-SSD is only 30.56ms, including
2.84ms for data preprocessing,
24.33ms for network forwarding, and
3.39ms for post-processing and producing the final predictions.
All evaluations were done on an Intel Xeon Silver CPU and a single TITAN Xp GPU.
Our method attains a faster inference speed compared with SA-SSD~\cite{he2020structure}, as our BEVConvNet structure is simpler and we further optimized our voxelization code.



\ifx\allfiles\undefined
\end{document}
