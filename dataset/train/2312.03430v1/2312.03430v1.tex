

\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage[pagenumbers]{cvpr} 

\usepackage[dvipsnames]{xcolor}
\newcommand{\red}[1]{{\color{red}#1}}
\newcommand{\todo}[1]{{\color{red}#1}}
\newcommand{\TODO}[1]{\textbf{\color{red}[TODO: #1]}}


\usepackage{bm}
\usepackage{multirow}
\usepackage{arydshln} 
\definecolor{cvprblue}{rgb}{0.21,0.49,0.74}
\usepackage[pagebackref,breaklinks,colorlinks,citecolor=cvprblue]{hyperref}

\def\paperID{*****} \def\confName{CVPR}
\def\confYear{2024}

\title{ShareCMP: Polarization-Aware RGB-P Semantic Segmentation} 
\author{
Zhuoyan Liu \quad Bo Wang\thanks{Corresponding author.} \quad Lizhi Wang \quad Chenyu Mao \quad Ye Li \\
Harbin Engineering University\\
{\tt\small \{liuzhuoyan,wb,wanglizhi111,maochenyu,liye\}@hrbeu.edu.cn}
} 
\begin{document}
\maketitle
\begin{abstract}
Multimodal semantic segmentation is developing rapidly, but the modality of RGB-\textbf{P}olarization remains underexplored. To delve into this problem, we construct a UPLight RGB-P segmentation benchmark with 12 typical underwater semantic classes which provides data support for Autonomous Underwater Vehicles (AUVs) to perform special perception tasks. In this work, we design the ShareCMP, an RGB-P semantic segmentation framework with a shared dual-branch architecture, which reduces the number of parameters by about 26-33\% compared to previous dual-branch models. It encompasses a Polarization Generate Attention (PGA) module designed to generate polarization modal images with richer polarization properties for the encoder. In addition, we introduce the Class Polarization-Aware Loss (CPALoss) to improve the learning and understanding of the encoder for polarization modal information and to optimize the PGA module. With extensive experiments on a total of three RGB-P benchmarks, our ShareCMP achieves state-of-the-art performance in mIoU with fewer parameters on the UPLight (92.45\%), ZJU (92.7\%), and MCubeS (50.99\%) datasets. The code is available at \url{https://github.com/LEFTeyex/ShareCMP}.
\end{abstract} \section{Introduction}
\label{sec:introduction}


\begin{figure}[t]
  \centering
   \includegraphics[width=1.0\linewidth]{figures/fig_abstract.pdf}
   \caption{Analysis of our ShareCMP, CMX~\cite{cmx} and CMNeXt~\cite{cmnext}. Our ShareCMP outperforms CMX and CMNeXt on the UPLight and MCubeS~\cite{mcubes} RGB-P datasets and reduces the number of parameters by about 26-33\%.}
   \label{fig:abstract}
\end{figure} 

Multimodal semantic segmentation provides the autonomous vehicle with scene understanding capability~\cite{30,51} using perceptual modalities such as RGB-\textbf{D}epth, -\textbf{T}hermal, -\textbf{E}vent, and -\textbf{L}iDAR, and is also developing rapidly in recent years. In underwater scenes, Autonomous Underwater Vehicles (AUVs) also need this kind of scene understanding capability to perceive comprehensive environmental information and make correct decisions. Inspired by the use of polarized light by the mantis shrimp to perceive the underwater environment~\cite{52,53}, we apply the modality of RGB-\textbf{P}olarization to the perception system of AUVs to improve the perception capability of AUVs in complex underwater environments.


Most of the current multimodal semantic segmentation methods~\cite{28,29,31,32} are designed for RGB-D and RGB-T, and the model designed based on RGB-P also refers to the mainstream methods using the Degree of Linear Polarization (DoLP) or the Angle of Linear Polarization (AoLP) calculated from polarized images as the polarization modal input~\cite{mcubes,2,zju,4,5}. However, the polarization properties of polarized images are not further explored. We believe that this fixed paradigm of DoLP and AoLP cannot fully represent the polarization image properties and has certain limitations. Meanwhile, the current mainstream multimodal semantic segmentation methods~\cite{cmx,cmnext,32} (RGB-X) adopt a dual-branch architecture to extract features for RGB and X modalities separately and perform attention interaction and fusion for these two modalities. Although these mainstream methods perform well on many multimodal benchmarks, they have two backbone branches and a large number of parameters that are not convenient for model deployment.


To delve into RGB-P multimodal semantic segmentation, we create an underwater RGB-P semantic segmentation benchmark that includes 0, 45, 90, and 135 degree polarized images with a total of 12 classes, which we call UPLight. In this paper, we investigate the performance of different polarization modalities in multimodal semantic segmentation and design a non-fixed paradigm Polarization Generate Attention (PGA) module using four-angle polarized images to generate better polarization modal images for the RGB-P multimodal semantic segmentation model. While based on the principle that the polarization angles of light reflected by different categories of materials are different, we propose a Class Polarization-Aware Loss (CPALoss) to assist the model in learning the polarization properties of different classes and improve the multimodal feature understanding and interaction capabilities of the RGB-P model. To reduce the number of parameters of the dual-branch model and reduce the resource consumption of model deployment, we design a shared dual-branch multimodal semantic segmentation architecture. Our method is called ShareCMP, which is inspired by CMX~\cite{cmx}.


With extensive experiments on UPLight and two additional public RGB-P datasets, we obtain performance evaluations of the ShareCMP model. ShareCMP achieves top mIoU of 92.45\% on UPLight, 92.4\% (MiT-B2~\cite{26})/92.7\% (MiT-B4) on ZJU~\cite{zju}, and 50.34\% (RGB-A)/50.55\% (RGB-D)/50.99\% (RGB-A-D) on MCubeS~\cite{mcubes} datasets. Our ShareCMP outperforms all previous RGB-P methods on these three datasets, achieving state-of-the-art performance with fewer parameters.


In conclusion, we deliver the following contributions:
\begin{itemize}
    \item We construct a new benchmark UPLight for underwater RGB-P semantic segmentation, which provides data support for AUVs to perform special perception tasks.
    \item We consider the shortcomings of the current dual-branch multimodal model and present a ShareCMP architecture with a shared dual-branch architecture to reduce the number of model parameters.
    \item We explore and compare different polarization modal representations and propose the PGA module to generate the representation with richer polarization properties.
    \item CPALoss is proposed, with Class Polarization-Aware Auxiliary Head (CPAAHead) to improve the learning and understanding of the encoder for polarization properties.
\end{itemize} \section{Related Work}
\label{sec:related_work}


\medskip
\noindent
\textbf{Semantic segmentation.}
Semantic segmentation can be seen as an extension of image classification from image level to pixel level. Since fully convolutional networks~\cite{6} introducing the end-to-end per-pixel classification paradigm, semantic segmentation has advanced significantly. The methods include capturing multi-scale features~\cite{7,8,9,10}, appending channel and self-attention blocks~\cite{11,12,13,14}, refining context priors~\cite{15,16,17,18}, and leveraging edge cues~\cite{19,20,21,22}. More recent methods prove the effectiveness of transformer-based architectures for semantic segmentation~\cite{23,24,25,26}. While these works achieve high performance, they still suffer under real-world conditions where RGB images do not provide sufficient textures such as low illumination and high dynamic areas.


\medskip
\noindent
\textbf{Multimodal semantic segmentation.}
At present, multimodal semantic segmentation has been widely studied, such as RGB-Depth~\cite{27,28,29}, RGB-Thermal~\cite{30,31,32}, RGB-Event~\cite{33}, RGB-LiDAR~\cite{34}, RGB-Polarization~\cite{zju,mcubes,2,4,5,40}, \etc. These methods adopt dual-branch or multi-branch architectures and use different attention modules between different branches for multi-scale cross-modal information interaction. Recently, unified multimodal segmentation CMX~\cite{cmx} and arbitrary-modal semantic segmentation CMNeXt~\cite{cmnext} have achieved better performance. However, the multiple branches of these methods have a large number of parameters, which makes model deployment difficult. Although the single-branch method based on data modal fusion~\cite{43} is convenient for model development, its performance is far inferior to the dual-branch model. Therefore, there is an urgent need to develop a multimodal semantic segmentation method that is easy to use and has excellent performance. Among the many data modalities, the polarization modality is quite different from other modalities. Its raw data are RGB images with different polarization angles, which is also a part of the RGB image because the RGB image consists of all polarization angle lights. We believe that the DoLP and AoLP~\cite{5,48} calculated from the raw data cannot fully represent the polarization image properties, which also limits the performance of the RGB-P model. To this end, we propose ShareCMP, a shared dual-branch RGB-P multimodal semantic segmentation framework. \section{Proposed Framework: ShareCMP}
\label{sec:method}


\subsection{ShareCMP Architecture}
\label{sec:arch}


\begin{figure*}[t]
  \centering
   \includegraphics[width=1.0\linewidth]{figures/fig_ShareCMP.pdf}
   \caption{ShareCMP framework. The ShareCMP encoder consists of modules with modal exclusive Overlap Patch Embeddings (OPEmbed) and other shared parameters in each stage. The Polarization Generate Attention (PGA) module generates a polarization modal image input with rich polarization properties based on channel attention and large receptive fields. The Class Polarization-Aware Auxiliary Head (CPAAHead) uses the  and  fused features in the ShareCMP encoder to construct Class Polarization-Aware Loss (CPALoss)  to improve the capability of the encoder to perceive optical polarization properties of different classes and optimize the PGA module.}
   \label{fig:ShareCMP}
\end{figure*} 

ShareCMP consists of a shared dual-branch and four-stage encoder, and its architecture is designed based on CMX~\cite{cmx} and Segfromer~\cite{26}. Both RGB and polarization modalities are encoded by a shared dual-branch encoder, and the four-stage architecture provides pyramidal fused features to the decoder. Meta-Transformer~\cite{metatransformer} shows that different modalities can be uniformly encoded using different data-to-sequence tokenization and a shared encoder. Inspired by this, we design a unified encoder with shared parameters for RGB and polarization modalities, as shown in \cref{fig:ShareCMP}. Each stage of the shared encoder consists of three modules: Overlap Patch Embeddings (OPEmbed), Efficient Self-Attention (ESAttn), and Mix-FFN (MFFN)~\cite{26}. OPEmbed generates feature patches with local continuous information. The feature patches are forwarded to the following ESAttn and MFFN modules. At each stage, the modal exclusive OPEmbed (ME OPEmbed) is constructed for the input  and , respectively, and using the shared-parameters ESAttn and MFFN introduces self-attention and leak location information~\cite{26} to obtain  and .

where  is feature patches of . After that, the Feature Rectification Module (FRM) and the Feature Fusion Module (FFM)~\cite{cmx} introduce multimodal cross-attention for  and  and fuse their feature information. After the encoder, the four-stage fused features  are forwarded to the MLP decoder~\cite{26} for the segmentation prediction. We also refer to the shared dual-branch encoder (ShareCMP encoder) as the pseudo-single-branch encoder. Although it still retains the dual-branch inference architecture, it reduces the number of multimodal encoder parameters by about 30\%.


\subsection{Polarization Generate Attention Module}
\label{sec:PGA}


To better represent the properties of polarization modal data, we develop the Polarization Generate Attention (PGA) module to generate polarization modal information with more polarization properties to better serve downstream tasks as semantic segmentation. As shown in \cref{fig:ShareCMP}, PGA uses Convolution (Conv) with  kernel size for four input images with 0, 45, 90, and 135 polarization angles  to extract features and increases the receptive field of features without reducing the resolution of the image features. Finally, the concatenated feature  is obtained, as in \cref{eq:fp}.

Previous multimodal fusion methods indicate channel information is crucial~\cite{46,47}. Inspired by this, after obtaining the concatenated feature , using Depth-Wise Convolution (DWConv), which consists of convolution with  kernel size and with  kernel size composed of grouped convolution with 2 dilation~\cite{43}, provides the following channel attention module with advanced features with a larger receptive field to enhance the generated channel attention features . The processing can be formalized as:

Finally, the concatenated feature  is computed by attention, residual connection, feed-forward network composed of DWConv and PReLU~\cite{45} activation, then the three-channel polarization modal image  is generated. It can be formalized as:

The activation function is PReLU in PGA to improve performance~\cite{43}. And PGA is a lightweight module with a simple architecture.


\subsection{Class Polarization-Aware Loss}
\label{sec:cpaloss}


To improve the learning and understanding of the encoder for polarization modal information, we design the Class Polarization-Aware Loss (CPALoss) and build the corresponding Class Polarization-Aware Auxiliary Head (CPAAHead) for CPALoss, as illustrated in \cref{fig:ShareCMP}. Based on the principle that the reflected light from different categories of materials produces different degrees of polarization and angles of polarization, CPALoss uses the estimates of AoLP and DoLP corresponding to different classes generated by CPAAHead and the ground truth AoLP and DoLP calculated from polarized images at four angles of 0, 45, 90, and 135 to calculate the loss . The loss is optimized to improve the capability of the encoder for perceiving the optical polarization properties of different classes while improving the quality of the polarization modal image generated by the PGA module. CPAAHead, which has a similar architecture to the MLP decoder~\cite{26}, uses the multi-scale multimodal fusion features generated in the encoder to estimate the AoLP and DoLP of different classes in the image, respectively. CPAAHead constructs two convolution layers with  kernel size for each scale feature. Upsampling operations are performed after the first convolution layer and the second convolution layer to unify the different scales of the features and to align the estimation and ground truth size of the AoLP or DoLP. The CPALoss can be written as:

where  represents the index of the  stage in the encoder,  represents the class in the dataset.  is a convolution with  followed by upsampling. ,  are the estimation and ground truth of AoLP/DoLP, respectively.  is the loss weight of CPALoss, which we set to 0.01 after experiments. We set CPALoss at 3 and 4 stages, more detailed analysis in \cref{sec:ablation_study}. \section{Experiments}
\label{sec:experiments}


\subsection{Polarization Modal Representations}
\label{sec:pmodal_rep}


Polarization is a basic property of light, expressing the direction of light vibration. The vibration of polarized light has a fixed direction or a regular change of direction and is classified as linearly polarized light, circularly polarized light, and elliptically polarized light. The polarization sensor is usually set four polarization plates with different angles (,,, on every four pixels, which collects linearly polarized images \{,,,\} with pixels aligned at these four angles at the same time. We investigate four polarization modal representations~\cite{zju,5}, the Angle of Linear Polarization (AoLP), the Degree of Linear Polarization (DoLP), the Sin Angle of Linear Polarization (SAoLP), and the Cos Angle of Linear Polarization (CAoLP), which the SAoLP and the CAoLP are designed based on the principles of trigonometric functions. These polarization modal representations are derived from stokes vectors  that describe the polarization state of the light. Precisely,  represents the total light intensity,  and  denote the ratio of  and  linear polarization over its perpendicular polarized portion, and  stands for the circular polarization power which is not involved in our work. The stokes vectors  can be calculated by the following \cref{eq:S}.

In the work, to represent the total light intensity more accurately, we use \cref{eq:S0} to calculate ~\cite{48,5}, and make  be the RGB input image of the RGB-P multimodal semantic segmentation model.

Then, AoLP, DoLP, SAoLP, and CAoLP are formally computed as follows:

In our experiments, we further studied the impact of AoLP, DoLP, SAoLP, and CAoLP on the performance of RGB-P multimodal semantic segmentation. We use AoLP, DoLP, SAoLP, and CAoLP as inputs to the polarization modality of the RGB-P model, respectively, to analyze the differences between them.


\subsection{Datasets and Implementation Details}
\label{sec:datasets_implementation}


\begin{figure*}
  \centering

  \begin{subfigure}{0.55\linewidth}
   \includegraphics[width=1.0\linewidth]{figures/fig_UPLight_vis.pdf}
    \caption{Collecting device and images of the UPLight.}
    \label{fig:UPLight_vis}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.43\linewidth}
   \includegraphics[width=1.0\linewidth]{figures/fig_UPLight_ana.pdf}
    \caption{Distribution of object quantity in 12 semantic classes.}
    \label{fig:UPLight_ana}
  \end{subfigure}

  \caption{UPLight RGB-P multimodal dataset.}
  \label{fig:UPLight}
\end{figure*}
 

\noindent
\textbf{UPLight dataset} is an underwater RGB-P semantic segmentation dataset that we create for AUV perception, which has 12 classes. We use a LUCID\_TRI050S-QC polarization sensor with a C-C-39N0-160-R12 liquid lens and customize a waterproof housing for underwater use, as in \cref{fig:UPLight_vis}. We mainly collect color polarized images of typical underwater objects arranged in the pool to provide data support for AUV to perform special perception tasks. Detailed dataset information is as shown in \cref{fig:UPLight}. The dataset is split into 1441/350 image pairs for training/validation at the size of . Each image pair includes four polarized images with different polarization angles (,,,. The input image is resized to .


\begin{table}[t]
\centering


\begin{tabular}{@{}cccccc@{}}
\toprule
\multirow{2}{*}{ShareCMP} & \multicolumn{4}{c}{Stage} & \multirow{2}{*}{mIoU} \\ \cmidrule(lr){2-5}
              & 1          & 2          & 3          & 4          &                \\ \midrule
w/ ME OPEmbed & \checkmark &            &            &            & 91.91          \\
w/ ME OPEmbed & \checkmark & \checkmark &            &            & 92.36          \\
w/ ME OPEmbed & \checkmark & \checkmark & \checkmark &            & 92.01          \\
w/ ME OPEmbed & \checkmark & \checkmark & \checkmark & \checkmark & \textbf{92.45} \\ \bottomrule
\end{tabular}

\caption{Ablation study of modal exclusive OPEmbed (ME OPEmbed) on different encoder stages.}
  \label{tab:OPEmbed}
\end{table} 

\medskip
\noindent
\textbf{ZJU dataset}~\cite{zju} is an RGB-P multimodal dataset for automated driving on complex campus street scenes, which has 8 classes. It has 344/50 image pairs for training/validation at the size of . Each image pair includes four polarized images with different polarization angles (,,,. The input image is resized to .


\medskip
\noindent
\textbf{MCubeS dataset}~\cite{mcubes} is an RGB-P-NIR multimodal dataset for material segmentation on different street scenes, which has 20 classes. It has 302/96/102 image pairs for training/validation/test at the size of . Each image pair includes RGB, AoLP, DoLP, and Near-Infrared (NIR) images. The input image is resized to .


\medskip
\noindent
\textbf{Implementation details.}
Our implementations are based on the MMSegmentation toolbox~\cite{mmseg}. Unless otherwise specified, we train our models on 2 A6000 GPUs with an initial learning rate (LR) of , which is scheduled by the poly strategy with power 1.0 over 200 epochs. The first 5 epochs are to warm up models with  the initial LR. The optimizer is AdamW~\cite{49} with a weight decay of 0.01, and the batch size is 4 per GPU. We use cross-entropy as the semantic segmentation loss function. The images are augmented by random resize with a ratio of 0.5-2.0, random horizontal flipping, random color jitter, and random cropping to  on all datasets. To conduct comparisons, the ImageNet-1K~\cite{50} pre-trained weight is used for ShareCMP on all datasets, but not for OPEmbed~\cite{26} modules in the polarization modal branch. We use mean Intersection over Union (mIoU) averaged across semantic classes as the primary evaluation metric to measure the segmentation performance.


\subsection{Ablation Study}
\label{sec:ablation_study}


\medskip
\noindent
\textbf{Ablation of ShareCMP encoder.}
The ablation of setting non-shared parameters modal exclusive OPEmbed~\cite{26} on different stages of ShareCMP encoder is conducted, as shown in \cref{tab:OPEmbed}. When the ShareCMP encoder sets modal exclusive OPEmbed in stage 1, its encoder architecture is similar to that of the Meta-Transformer unified encoder~\cite{metatransformer}, but it has poor performance. We believe that ShareCMP has multimodal attention interaction operations (by FRM and FFM~\cite{cmx}) after each stage. Simply setting a modal exclusive OPEmbed in stage 1 cannot meet the generalization performance requirements of the ShareCMP encoder. ShareCMP achieves the best performance when modal exclusive OPEmbed is set in all encoder stages.


\begin{table}[t]
\centering


\begin{tabular}{@{}cccccc@{}}
\toprule
\multirow{2}{*}{ShareCMP} & \multicolumn{4}{c}{Stage} & \multirow{2}{*}{mIoU} \\ \cmidrule(lr){2-5}
           & 1          & 2          & 3          & 4          &       \\ \midrule
w/ CPALoss & \checkmark & \checkmark & \checkmark & \checkmark & 91.79 \\
w/ CPALoss &            & \checkmark & \checkmark & \checkmark & 91.63 \\
w/ CPALoss &            &            & \checkmark & \checkmark & \textbf{92.45} \\
w/ CPALoss &            &            &            & \checkmark & 91.72 \\ \bottomrule
\end{tabular}

\caption{Ablation study of the CPALoss.}
  \label{tab:CPALoss}
\end{table} 

\medskip
\noindent
\textbf{Ablation of CPALoss.}
As shown in \cref{tab:CPALoss}, we ablate the combination of CPALoss used in different ShareCMP encoder stages and require that deep and high-level features must be optimized for CPALoss. When the CPALoss is optimized in stages 3 and 4, ShareCMP achieves the best performance.


\begin{table*}[t]
\centering
\resizebox{\linewidth}{!}{

\begin{tabular}{@{}lccccccccccccccc@{}}
\toprule
  Method &
  Modal &
  \rotatebox{90}{Unlabeled} &
  \rotatebox{90}{Mooringmine} &
  \rotatebox{90}{UUV} &
  \rotatebox{90}{Float} &
  \rotatebox{90}{Reflector} &
  \rotatebox{90}{Person} &
  \rotatebox{90}{Bottommine} &
  \rotatebox{90}{Cup} &
  \rotatebox{90}{Ironball} &
  \rotatebox{90}{Conch} &
  \rotatebox{90}{Fish} &
  \rotatebox{90}{Shell} &
  \rotatebox{90}{Starfish} &
  mIoU \\ \midrule
  
SegFormer-B2~\cite{26}  & RGB      & 99.49 & 92.77 & 94.47 & 89.12 & 97.96 & 95.00 & 97.44 & 89.66 & 82.92 & 84.12 & 89.31 & 83.25 & 69.25 & 89.60 \\ \midrule
MCubeSNet~\cite{mcubes} & RGB-AoLP & 89.72 & 83.32 & 84.78 & 85.20 & 88.79 & 85.63 & 88.94 & 81.76 & 75.47 & 83.37 & 80.77 & 81.40 & 65.18 & 82.64 \\
EAFNet~\cite{zju}       & RGB-AoLP & 94.61 & 88.21 & 89.67 & 90.09 & 93.68 & 90.52 & 93.83 & 86.65 & 80.36 & 88.26 & 85.66 & 86.29 & 70.07 & 87.53 \\
CMX (MiT-B2)~\cite{cmx} & RGB-AoLP & 99.53 & 93.08 & \textbf{94.90} & 92.27 & 98.07 & 95.27 & 97.88 & 88.67 & 84.83 & \textbf{93.60} & 90.40 & \textbf{92.62} & \textbf{76.53} & 92.13 \\ \midrule
MCubeSNet               & RGB-DoLP & 87.88 & 81.48 & 82.94 & 83.36 & 86.95 & 83.79 & 87.10 & 79.92 & 73.63 & 81.53 & 78.93 & 79.56 & 63.34 & 80.80 \\
EAFNet                  & RGB-DoLP & 94.42 & 88.02 & 89.48 & 89.90 & 93.49 & 90.33 & 93.64 & 86.46 & 80.17 & 88.07 & 85.47 & 86.10 & 69.88 & 87.34 \\
CMX (MiT-B2)            & RGB-DoLP & 99.53 & 92.88 & 94.79 & 90.48 & 98.19 & 95.22 & 97.51 & \textbf{92.00} & \textbf{85.48} & 91.26 & \textbf{90.95} & 92.51 & 76.17 & 92.07 \\ \midrule
ShareCMP (MiT-B2)       & RGB-\{,,,\} & 99.53 & \textbf{93.13} & 94.59 & \textbf{95.01} & \textbf{98.60} & \textbf{95.44} & \textbf{98.75} & 91.57 & 85.28 & 93.18 & 90.58 & 91.21 & 74.99 & \textbf{92.45} \\ \bottomrule
\end{tabular}

}
  \caption{Results on UPLight.}
  \label{tab:UPLight}
\end{table*} \begin{table}[t]
\centering


\begin{tabular}{@{}lcc@{}}
\toprule
Method      & Modal     & mIoU \\ \midrule
ShareCMP    & RGB-\{,,,\} & \textbf{92.45} \\ \midrule
w/o CPALoss & RGB-\{,,,\} & 91.09{\small (-1.36)} \\
w/o PGA     & RGB-AoLP  & 91.96{\small (-0.49)} \\
w/o PGA     & RGB-DoLP  & 92.03{\small (-0.42)} \\
w/o PGA     & RGB-SAoLP & 90.63{\small (-1.82)} \\
w/o PGA     & RGB-CAoLP & 91.15{\small (-1.30)} \\ \bottomrule
\end{tabular}

\caption{Ablation study of the ShareCMP framework.}
  \label{tab:ShareCMP}
\end{table} 

\medskip
\noindent
\textbf{Ablation of ShareCMP.}
In \cref{tab:ShareCMP}, we show the semantic segmentation performance of ShareCMP without CPALoss or PGA and investigate the effect of the four polarization modalities AoLP, DoLP, SAoLP, and CAoLP on the RGB-P semantic segmentation performance. When removing the CPALoss for polarization-aware learning, the performance decreases significantly by 1.36\%. Our designed SAoLP and CAoLP, which are new representation methods for the polarization modality, result in poor performance decreases of 1.82\% and 1.30\%, respectively. Since SAoLP and CAoLP are part of AoLP, their polarization modal representation is not as complete as AoLP, resulting in reduced performance. While AoLP and DoLP result in a similar slight decrease in performance, it indicates that AoLP and DoLP have similar effects on the ShareCMP and are also relatively effective polarization modal representations. Regardless of the fixed paradigm of the polarization modal representation, its performance is not as good as the representation generated by our PGA. The results confirm that our CPALoss and PGA are more advantageous for RGB-P semantic segmentation.


\subsection{Comparison against the State of the Art}
\label{sec:comparison_sota}


To verify the efficacy of our proposed ShareCMP framework, we perform extensive experiments on three RGB-P segmentation datasets. The results and comparisons against the state-of-the-art are shown in \cref{tab:UPLight,tab:ZJU,tab:MCubeS}.


\medskip
\noindent
\textbf{Results on UPLight.}
As shown in \cref{tab:UPLight}, we train models~\cite{zju,mcubes,cmx} that can be used for RGB-P semantic segmentation and compare them with ShareCMP on the UPLight dataset we built. Our ShareCMP achieves the best performance of 92.45\%. Compared to the RGB-only baseline with SegFormer-B2~\cite{26}, the IoU improvements on classes with polarization properties are clear, such as Float (+5.9\%), Bottom mine (+1.3\%), Ironball (+2.4\%), Conch (+9.1\%), Shell (+8.0\%), and Starfish (+5.7\%). This also shows that our RGB-P multimodal semantic segmentation model ShareCMP has important application significance in underwater aquaculture, fishing, ocean exploration, \etc. ShareCMP has only about 65\% of the number of parameters of CMX~\cite{cmx} (detailed parameter number analysis in \cref{sec:qv_analysis}), and has a 0.32\% performance improvement compared to CMX, which provides more advantages in the deployment of ShareCMP and its performance improvement on AUVs.


\begin{table*}[t]
\centering
\resizebox{\linewidth}{!}{

\begin{tabular}{@{}lcccccccccc@{}}
\toprule
Method                  & Modal    & Building & Glass & Car   & Road  & Tree  & Sky   & Pedestrian & Bicycle & mIoU \\ \midrule
SwiftNet~\cite{54}      & RGB      & 83.0     & 73.4  & 91.6  & 96.7  & 94.5  & 84.7  & 36.1       & 82.5    & 80.3 \\
SegFormer-B2~\cite{26}  & RGB      & 90.6     & 79.0  & 92.8  & 96.6  & 96.2  & 89.6  & 82.9       & 89.3    & 89.6 \\ \midrule
NLFNet~\cite{4}         & RGB-AoLP & 85.4     & 77.1  & 93.5  & 97.7  & 93.2  & 85.9  & 56.9       & 85.5    & 84.4 \\
EAFNet~\cite{zju}       & RGB-AoLP & 87.0     & 79.3  & 93.6  & 97.4  & 95.3  & 87.1  & 60.4       & 85.6    & 85.7 \\
EAFNet                  & RGB-DoLP & 86.4     & 76.9  & 93.0  & 97.1  & 95.5  & 86.1  & 62.1       & 86.0    & 85.4 \\
CMX (MiT-B2)~\cite{cmx} & RGB-AoLP & 91.5     & 87.3  & 95.8  & 98.2  & 96.6  & 89.3  & 85.6       & 91.9    & 92.0 \\
CMX (MiT-B2)            & RGB-DoLP & \textbf{91.8}     & 87.8  & 96.1  & 98.2  & 96.7  & 89.4  & \textbf{86.1}       & 91.8    & 92.2 \\
ShareCMP (MiT-B2)       & RGB-\{,,,\}    & 91.4     & \textbf{88.4}  & \textbf{96.2}  & 98.2  & 96.7  & \textbf{90.2}  & 85.7       & \textbf{92.2}    & \textbf{92.4} \\ \midrule
CMX (MiT-B4)            & RGB-AoLP & 91.6     & \textbf{88.8}  & 96.3  & 98.3  & 96.8  & 89.7  & 86.2       & \textbf{92.8}    & 92.6 \\
CMX (MiT-B4)            & RGB-DoLP & 91.6     & 88.6  & 96.3  & 98.3  & 96.7  & 89.5  & 86.4       & 92.2    & 92.5 \\
ShareCMP (MiT-B4)       & RGB-\{,,,\}    & \textbf{92.0}     & 88.6  & 96.3  & 98.3  & 96.8  & \textbf{90.0}  & \textbf{87.2}       & 92.5    & \textbf{92.7} \\ \bottomrule
\end{tabular}

}
  \caption{Results on ZJU~\cite{zju}. ShareCMP is trained for 500 epochs.}
  \label{tab:ZJU}
\end{table*} 

\medskip
\noindent
\textbf{Results on ZJU.}
In \cref{tab:ZJU}, we perform experiments to compare the ShareCMP method with the previous state-of-the-art methods~\cite{54,4,zju,cmx} on the ZJU~\cite{zju} dataset. Our ShareCMP outperforms the previous best RGB-P method by 0.2\% and 0.1\% based on MiT-B2~\cite{26} and MiT-B4, respectively. Compared to dual-branch multimodal semantic segmentation models~\cite{4,zju,cmx}, our ShareCMP beats them in segmentation performance with only about 65\% of their parameters.


\begin{table*}[t]
\centering
\resizebox{\linewidth}{!}{

\begin{tabular}{@{}lccccccccccccccccccccc@{}}
\toprule
  Method &
  Modal &
  \rotatebox{90}{Asphalt} &
  \rotatebox{90}{Concrete} &
  \rotatebox{90}{Metal} &
  \rotatebox{90}{Road Ma} &
  \rotatebox{90}{Fabric} &
  \rotatebox{90}{Glass} &
  \rotatebox{90}{Plaster} &
  \rotatebox{90}{Plastic} &
  \rotatebox{90}{Rubber} &
  \rotatebox{90}{Sand} &
  \rotatebox{90}{Gravel} &
  \rotatebox{90}{Ceramic} &
  \rotatebox{90}{Cobbles} &
  \rotatebox{90}{Brick} &
  \rotatebox{90}{Grass} &
  \rotatebox{90}{Wood} &
  \rotatebox{90}{Leaf} &
  \rotatebox{90}{Water} &
  \rotatebox{90}{Sky} &
  mIoU \\ \midrule

DRConv~\cite{55}              & RGB-A-D-N & -    & -    & -    & -    & -    & -    & -   & -    & -    & -    & -    & -    & -    & -    & -    & -    & -    & -    & -    & 34.63 \\
DDF~\cite{56}                 & RGB-A-D-N & -    & -    & -    & -    & -    & -    & -   & -    & -    & -    & -    & -    & -    & -    & -    & -    & -    & -    & -    & 36.16 \\
TransFuser~\cite{57}          & RGB-A-D-N & -    & -    & -    & -    & -    & -    & -   & -    & -    & -    & -    & -    & -    & -    & -    & -    & -    & -    & -    & 37.66 \\
MMTM~\cite{58}                & RGB-A-D-N & -    & -    & -    & -    & -    & -    & -   & -    & -    & -    & -    & -    & -    & -    & -    & -    & -    & -    & -    & 39.71 \\
FuseNet~\cite{59}             & RGB-A-D-N & -    & -    & -    & -    & -    & -    & -   & -    & -    & -    & -    & -    & -    & -    & -    & -    & -    & -    & -    & 40.58 \\
MCubeSNet~\cite{mcubes}       & RGB-A-D-N & 85.7 & 42.6 & 47.0 & 59.2 & 12.5 & 44.3 & 3.0 & 10.6 & 12.7 & 66.8 & 67.1 & 27.8 & 65.8 & 36.8 & 54.8 & 39.4 & 73.0 & 13.3 & 94.8 & \textbf{42.90} \\ \midrule
MCubeSNet                     & RGB-A     & 83.3 & 42.3 & 43.0 & 58.4 & 8.8  & 27.3 & 0.6 & 9.8  & 12.0 & 55.5 & 57.7 & 18.1 & 64.6 & 36.6 & 56.5 & 34.8 & 71.8 & 6.8  & 95.0 & 39.10 \\
CMNeXt (MiT-B2)~\cite{cmnext} & RGB-A     & -    & -    & -    & -    & -    & -    & -   & -    & -    & -    & -    & -    & -    & -    & -    & -    & -    & -    & -    & 48.42 \\
ShareCMP (MiT-B2)             & RGB-A     & \textbf{88.2} & \textbf{49.3} & \textbf{51.0} & \textbf{66.1} & \textbf{20.7} & \textbf{50.1} & \textbf{1.2} & \textbf{30.9} & \textbf{18.2} & \textbf{64.2} & \textbf{75.2} & \textbf{28.5} & \textbf{74.9} & \textbf{45.7} & \textbf{59.4} & \textbf{43.9} & \textbf{74.1} & \textbf{69.9} & \textbf{95.6} & \textbf{50.34} \\ \midrule
MCubeSNet                     & RGB-D     & 75.2 & 40.2 & 37.8 & 53.9 & 4.2  & 32.3 & \textbf{1.9} & 14.3 & 11.3 & 59.7 & 21.8 & 11.6 & 28.9 & 29.1 & 54.6 & 29.4 & 71.4 & 9.6  & 94.3 & 34.10 \\
ShareCMP (MiT-B2)             & RGB-D     & \textbf{88.5} & \textbf{50.4} & \textbf{52.6} & \textbf{65.9} & \textbf{23.2} & \textbf{49.8} & 0.0 & \textbf{36.7} & \textbf{18.8} & \textbf{64.6} & \textbf{75.4} & \textbf{30.4} & \textbf{72.2} & \textbf{45.3} & \textbf{57.3} & \textbf{43.9} & \textbf{74.5} & \textbf{66.1} & \textbf{95.7} & \textbf{50.55} \\ \midrule
MCubeSNet                     & RGB-A-D   & 83.0 & 42.6 & 45.5 & 59.8 & 17.0 & 44.2 & \textbf{1.2} & 18.6 & 4.8  & 54.8 & 51.5 & 26.4 & 67.6 & 41.9 & 57.0 & 39.4 & 74.0 & 15.5 & 95.3 & 42.00 \\
CMNeXt (MiT-B2)               & RGB-A-D   & -    & -    & -    & -    & -    & -    & -   & -    & -    & -    & -    & -    & -    & -    & -    & -    & -    & -    & -    & 49.48 \\
ShareCMP (MiT-B2)             & RGB-A-D   & \textbf{88.8} & \textbf{49.7} & \textbf{52.5} & \textbf{66.4} & \textbf{27.6} & \textbf{51.0} & 0.2 & \textbf{31.5} & \textbf{18.0} & \textbf{69.9} & \textbf{79.0} & \textbf{30.9} & \textbf{71.4} & \textbf{42.8} & \textbf{58.3} & \textbf{44.1} & \textbf{74.5} & \textbf{67.6} & \textbf{95.6} & \textbf{50.99} \\ \bottomrule
\end{tabular}

}
  \caption{Results on MCubeS~\cite{mcubes}. The human body class is omitted as its result is 0\%.}
  \label{tab:MCubeS}
\end{table*} 

\medskip
\noindent
\textbf{Results on MCubeS.}
In \cref{tab:MCubeS}, we benchmark 7 semantic segmentation methods on the MCubeS~\cite{mcubes} dataset, which contains AoLP, DoLP, and NIR modalities. Since the MCubeS dataset does not provide corresponding four-angle polarized images, our ShareCMP uses AoLP and DoLP as polarization modal inputs. On RGB-A and RGB-D modal inputs, our ShareCMP outperforms the previous best method by 1.92\% on CMNeXt~\cite{cmnext} and 16.45\% on MCubeSNet~\cite{mcubes}. ShareCMP achieves a state-of-the-art performance of 50.99\% when concatenating AoLP and DoLP modalities. In addition, our ShareCMP (RGB-A-D) even outperforms previous methods~\cite{55,56,57,58,59,mcubes} using RGB-A-D-N modalities, by 8.09\% compared to the best. The results indicate that our ShareCMP has great potential in RGB-P multimodal semantic segmentation.


\subsection{Quantitative and Visual Analysis}
\label{sec:qv_analysis}


\noindent
\textbf{Quantitative analysis of ShareCMP.}
In \cref{tab:PF}, our ShareCMP reduces the number of parameters by 22.12 M compared to CMX~\cite{cmx}, while the number of parameters of the ShareCMP encoder is only 65.7\% of the CMX encoder. Compared to CMNeXt~\cite{cmnext}, our ShareCMP also significantly reduces the number of parameters while increasing the computational complexity by only 0.77 G FLOPs. Compared to these state-of-the-art methods, our ShareCMP reduces a large number of parameters while still improving the segmentation performance, which also proves the efficacy of our proposed ShareCMP framework.
\begin{table}[t]
\centering


\begin{tabular}{@{}lcc@{}}
\toprule
Structure & \#Params(M) & FLOPs(G) \\ \midrule
CMX~\cite{cmx}       & 65.52                           & \textbf{52.03}         \\ \hdashline
\quad- Encoder       & 64.98                           & 47.07                  \\ \midrule
ShareCMP             & \textbf{43.40}{\small (-22.12)} & 66.19{\small (+14.16)} \\ \hdashline
\quad- Encoder       & \textbf{42.72}{\small (-22.26)} & 46.71                  \\
\quad- PGA           & 0.16                            & 14.53                  \\ \midrule
CMNeXt~\cite{cmnext} & 58.73                           & 65.42                  \\ \bottomrule
\end{tabular}

\caption{Comparison of the number of parameters (\#Params) and FLOPs which are counted in .}
  \label{tab:PF}
\end{table} \begin{figure*}[t]
  \centering
   \includegraphics[width=1.0\linewidth]{figures/fig_p_modal.pdf}
   \caption{Visualization of different polarization modal representations.}
   \label{fig:p_modal}
\end{figure*} \begin{figure}[t]
  \centering
   \includegraphics[width=1.0\linewidth]{figures/fig_seg_vis.pdf}
   \caption{Visualization of segmentation results. RGB-FP represents RGB and four polarized images with different polarization angles (RGB-\{,,,\}).}
   \label{fig:seg_vis}
\end{figure} \newline
\noindent
\textbf{Visualization of polarization modal representations.}
We show five polarization modal representations, as shown in \cref{fig:p_modal}. Compared with SAoLP and CAoLP, AoLP and DoLP cover the polarization information in SAoLP and CAoLP and show richer polarization features, which also verifies that the segmentation results of RGB-AoLP and RGB-DoLP are better than RGB-SAoLP and RGB-CAoLP in \cref{tab:ShareCMP}. While  has rich polarization properties, it also provides different color features for objects with different polarization properties, such as the glass reflection and the reflector of the motorcycle, the car shell at different angles in the second group of images, the glass and the sticker on the car glass on the right side, and different colors of classes in the third and fourth groups of images. The AoLP and SAoLP in the third group of images only have very few polarization features, while our  can still provide rich polarization features, indicating that our PGA module has good robustness in generating polarization features. In addition, compared to , the other four polarization modal representations are very coarse in some areas and have a lot of noise points, while  has smoother polarization features that eliminate a large number of noise points, especially for the underwater polarization modal representations in the UPLight dataset.
\newline
\noindent
\textbf{Visualization of segmentation results.}
In \cref{fig:seg_vis}, we show the semantic segmentation results of our ShareCMP against the CMX~\cite{cmx} with RGB-A and RGB-D. It can be seen that our ShareCMP achieves a more complete segmentation for persons in case of insufficient illumination on ZJU~\cite{zju} images. For blurred underwater images in UPLight, CMX does not segment the starfish in the image or segments the starfish and shells incompletely. Our ShareCMP is more robust in complex underwater scenes and accurately segments all objects. \section{Conclusion}
\label{sec:conclusion}


In this work, to revitalize multimodal pixel-wise semantic scene understanding for Autonomous Underwater Vehicles (AUVs), we investigate RGB-P semantic segmentation and propose ShareCMP, a shared dual-branch architecture, which reduces a large number of model parameters. We put forward the UPLight RGB-P multimodal dataset with 12 typical underwater semantic classes. We propose the Polarization Generate Attention (PGA) module for the stable generation of polarization modal images with rich polarization properties, and the Class Polarization-Aware Loss (CPALoss) to improve the learning and understanding of the encoder for polarization modal information. Our ShareCMP achieves state-of-the-art performance with fewer parameters on three RGB-P datasets.
\medskip
\newline
\noindent
\textbf{Acknowledgment.}
This research is funded by the National Natural Science Foundation of China, grant number 52371350, by the Natural Science Foundation of Hainan Province, grant number 2021JJLH0002, by the Foundation of National Key Laboratory, grant number JCKYS2022SXJQR-06, and 2021JCJQ-SYSJJ-LB06912.  {
    \small
    \bibliographystyle{ieeenat_fullname}
    \bibliography{sections/main}
}



\end{document}
