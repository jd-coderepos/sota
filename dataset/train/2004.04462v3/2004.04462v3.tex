\section{Experiments}
\label{sec:expe}

In this section, we evaluate our convolutional layer on shape classification, part segmentation and semantic segmentation, reaching the state of the art regarding task metrics while being efficient regarding computation time and memory usage.

\textit{Network architectures.}
In our experiments, we use a simple yet effective residual network for classification and semantic segmentation. We mimic the architecture of~\cite{thomas2019kpconv}, except that ours is designed for -NN convolution, i.e., we do not need to add phantom points and features to equalize the size of data tensor due to a variable number of points in radius search.
The network has an encoder-decoder structure. The encoder is composed of an alternation of residual blocks maintaining the resolution and residual blocks with down-sampling.
The decoder is a stack of fully-connected and nearest-neighbor upsampling layers.
The classification network is the encoder of the previously described network followed by a global average pooling.
For large scale semantic segmentation, we use either input modality dropout~\cite{thomas2019kpconv} or dual network fusion~\cite{boulch2020convpoint}, as indicated in tables.

\textit{Experimental setup.}
Our formulation (and code) allows a variable input size, but in order to use optimization with mini-batches, with train the networks with fixed input sizes.
As every operations of FKAConv are differentiable, all parameters are optimized via gradient descent (including the spatial gating parameters 's and 's).
Finally, we use a standard cross-entropy loss.

\subsection{Benchmark results}

\begin{table}[t]
\caption{Classification and part segmentation benchmarks.}
\label{tab:bench}
\vspace{-2mm}
\centering

    \begin{minipage}{0.6\linewidth}
        \centering
        (a) ModelNet40\\~\\
\tiny\scriptsize
        \begin{tabular}[b]{@{}l|c|@{\hspace{0.1cm}}c@{\hspace{0.2cm}}c@{}}
            \hline
            Methods & Num. & OA & AA\\
                    & points&\\
            \hline
            \textit{Mesh or voxels}\\
            Subvolume~\cite{qi2016volumetric}   &-& 89.2 & -\\
            MVCNN~\cite{su2015multi}            &-& 90.1 & -\\
            \hline
            \textit{Points} \\
            DGCNN~\cite{wang2018dynamdynamic}   &1024& 92.2 & \textbf{90.2} \\
            PointNet~\cite{qi2017pointnet}      &1024& 89.2 & 86.2 \\ 
            PointNet++~\cite{qi2017pointnet++}  &1024& 90.7 & - \\
            PointCNN~\cite{li2018pointcnn}      &1024& 92.2 & 88.1\\
            ConvPoint~\cite{boulch2020convpoint}&2048 & 92.5 & 89.6 \\
            KPConv~\cite{thomas2019kpconv}      &2048& \textbf{92.9} & - \\
            \hline
            \textit{Ours} & & \multicolumn{2}{l}{Averagestd. (best run)} \\
            FKAConv  & 1024  & 92.3 (92.5) & 89.6 (89.9)\\
                        & 2048  & 92.5 (92.5) & 89.5 (89.7)\\
            
        \end{tabular}
    \end{minipage}
    \hfill
    \begin{minipage}{0.39\linewidth}
        \centering
        (b) ShapeNet\\~\\
        \tiny\scriptsize
        
        \begin{tabular}{l|cc}
            \hline
            Method & mcIoU & mIoU \\
            \hline
            PointNet++~\cite{qi2017pointnet++}    & 81.9 & 85.1 \\
            SubSparseCN~\cite{graham20183d}              & 83.3 & 86.0 \\
            SPLATNet~\cite{su2018splatnet}        & 83.7 & 85.4 \\
            SpiderCNN~\cite{xu2018spidercnn}      & 81.7 & 85.3 \\
            SO-Net~\cite{li2018so}                & 81.0 & 84.9 \\
            PCNN~\cite{atzmon2018point}           & 81.8 & 85.1 \\
            KCNet~\cite{shen2018mining}           & 82.2 & 83.7 \\
            SpecGCN~\cite{wang2018local}          &    - & 85.4 \\
            RSNet~\cite{huang2018recurrent}       & 81.4 & 84.9 \\
            DGCNN~\cite{wang2018dynamdynamic}     & 82.3 & 85.1 \\
            SGPN~\cite{wang2018sgpn}              & 82.8 & 85.8 \\
            PointCNN~\cite{li2018pointcnn}         & 84.6 & 86.1 \\
            ConvPoint~\cite{boulch2020convpoint}  & 83.4 & 85.8 \\
            KPConv~\cite{thomas2019kpconv}        & \textbf{85.1} & \textbf{86.4} \\
            \hline
            FKAConv (Ours)   & 84.8 & 85.7\\
        \end{tabular}
        
    \end{minipage}
\end{table}

\textit{Shape classification.}
The classification task is evaluated on ModelNet40~\cite{wu20153d}.
As the spatial pooling process is stochastic, multiple predictions with the same point cloud might lead to different outcomes.
We aggregate 16 predictions for each point cloud and select the most predicted shape (we use a similar approach for part segmentation).
On the classification task (Table~\ref{tab:bench}(a)), we present average (and best) results over five runs.
For fair comparison, we train with 1024 (resp. 2048) points.
We rank first (resp. second) among the method trained with 1024 (resp. 2048) points.
We mainly observe that increasing the number points of reduces the standard deviation of the performances.

\textit{Part segmentation.}
On ShapeNet~\cite{yi2016scalable}, the network is trained with 2048 input points and 50 outputs (one for each part). The loss and scores are computed per object category (16 object categories with 2- to 6-part labels).
The results are presented in Table~\ref{tab:bench}(b).
We rank among the best methods: top-2 or top-5 depending on the metric used, i.e., mean class intersection over union (mcIoU) or instance average intersection over union (mIoU); we are only 0.3 point mcIoU and 0.7 point mIoU behind the best method.
It is interesting to notice that we are as good as or better than several methods for which the convolution falls into our formalism, such as ConvPoint~\cite{boulch2020convpoint} or SPLATNet~\cite{su2018splatnet}.


\textit{Semantic segmentation}
Three datasets are used for semantic segmentation corresponding to three different use cases.
S3DIS~\cite{armeni20163d} is an indoor dataset acquired with an RGBD camera.
The evaluation is done using a 6-fold cross validation.
NPM3D~\cite{roynard2018paris} is an outdoor dataset acquired in four sites using a lidar-equipped car.
Finally, Semantic8~\cite{hackel2017isprs} contains 30 lidar scenes acquired statically.
NPM3D and Semantic8 are datasets with hidden test labels. Scores in the tables are reported from the official evaluation servers.

We use 8192 input points but, as
subsampling the whole scene produces a significant loss of information,
we select instead points in vertical pillars with a square footprint of 2\,m for S3DIS, and 8\,m for NPM3D and Semantic8.
The center point of the pillar is selected randomly at training time and using a sliding window at test time.
If a point is seen several times, the prediction scores are summed and the most probable class is selected afterward.

\begin{table}[t!]
\caption{Semantic segmentation benchmarks.}
\label{tab:bench_seg}
\centering
    {
    \tiny
    \begin{tabular}{l|c|c|c@{\hspace{0.1cm}}c@{\hspace{0.1cm}}c@{\hspace{0.1cm}}c@{\hspace{0.1cm}}c@{\hspace{0.1cm}}c@{\hspace{0.1cm}}c@{\hspace{0.1cm}}c@{\hspace{0.1cm}}c@{\hspace{0.1cm}}c@{\hspace{0.1cm}}c@{\hspace{0.1cm}}c@{\hspace{0.1cm}}c}
        \multicolumn{16}{c}{\scriptsize (a) S3DIS}\\
        \hline
        Method & Search & IoU & ceil. & floor & wall & beam & col. & wind. & door & chair & table & book. & sofa & board & clut. \\
        \hline
        Pointnet~\cite{qi2017pointnet}          &Knn        & 47.6 & 88.0 & 88.7 & 69.3 & 42.4 & 23.1 & 47.5 & 51.6 & 42.0 & 54.1 & 38.2 &  9.6 & 29.4 & 35.2 \\
        RSNet~\cite{huang2018recurrent}         &-          & 56.5 & 92.5 & 92.8 & 78.6 & 32.8 & 34.4 & 51.6 & 68.1 & 60.1 & 59.7 & 50.2 & 16.4 & 44.9 & 52.0 \\
        PCCN~\cite{wang2018deep}                &-          & 58.3&  92.3 & 96.2 & 75.9 & 0.27 &  6.0 & \textbf{69.5} & 63.5 & 65.6 & 66.9 & 68.9 & 47.3 & 59.1 & 46.2 \\
        SPGraph~\cite{landrieu2018large}        & Super pt. & 62.1 & 89.9 & 95.1 & 76.4 & 62.8 & 47.1 & 55.3 & 68.4 & 73.5 & 69.2 & 63.2 & 45.9 &  8.7 & 52.9 \\ 
        PointCNN~\cite{li2018pointcnn}          & Knn       & 65.4 & 94.8 & 97.3 & 75.8 & 63.3 & 51.7 & 58.4 & 57.2 & 71.6 & 69.1 & 39.1 & 61.2 & 52.2 & 58.6 \\
        PointWeb~\cite{zhao2019pointweb}        & Knn       & 66.7 & 93.5 & 94.2 & 80.8 & 52.4 & 41.3 & 64.9 & 68.1 & 71.4 & 67.1 & 50.3 & 62.7 & \textbf{62.2} & 58.5 \\
        ShellNet~\cite{zhang2019shellnet}       & Knn       & 66.8 & 90.2 & 93.6 & 79.9 & 60.4 & 44.1 & 64.9 & 52.9 & 71.6 & 84.7 & 53.8 & 64.6 & 48.6 & 59.4 \\
        ConvPoint~\cite{boulch2020convpoint} & Knn       & 68.2 & \textbf{95.0} & 97.3 & 81.7 & 47.1 & 34.6 & 63.2 & 73.2 & 75.3 & \textbf{71.8} & 64.9 & 59.2 & 57.6 & 65.0 \\
        KPConv~\cite{thomas2019kpconv}          &Radius     & \textbf{70.6} & 93.6 & 92.4 & \textbf{83.1} & \textbf{63.9} & \textbf{54.3} & 66.1 & \textbf{76.6} & 57.8 & 64.0 & \textbf{69.3} & \textbf{74.9} & 61.3 & 60.3 \\
        \hline
        FKAConv (Ours \textit{RGB only})            & Knn       & 64.9 & 94.0 &	97.8 &	80.5 &	38.5 &	48.5 &	49.8 &	68.0 &	79.4 &	70.7 &	48.4 &	43.7 &    62.9 &	61.4 \\
        FKAConv (Ours \textit{RGB drop.})           & Knn       & 66.6 & 94.4	& 97.8	& 81.5	& 38.7	& 43.3	& 56.4	& 71.6	& 80.2	& 71.8	& 63.5	& 54.1	& 50.6	& 62.5 \\
        \hline
        FKAConv (Ours \textit{fusion})              & Knn       & 68.4 & 94.5 & \textbf{98.0} & 82.9 & 41.0 & 46.0 & 57.8 & 74.1 & \textbf{77.7} & 71.7 & 65.0 & 60.3 & 55.0 & \textbf{65.5} \\
        Rank                                    &           & 2    & 3    & 1    & 2    & 7    & 4    & 6    & 2    & 1    & 2    & 3    & 4    & 5    & 1 \\
      \end{tabular}
    }
    
    \vspace{0.2cm}
    {
    \tiny
    \begin{tabular}{@{}l|c|ccccccccc@{}}
    \multicolumn{11}{c}{\scriptsize (b) NPM3D}\\
        \hline
        Method & Av.IoU & Ground & Building & Pole & Bollard & Trash can & Barrier & Pedestrian & Car & Natural \\ \hline
RF MSSF~\cite{thomas2018semantic}	        & 56.3	& 99.3	& 88.6	& 47.8	& 67.3	& 2.3	& 27.1	& 20.6	& 74.8	& 78.8 \\
MS3 DVS~\cite{roynard2018classification}	        & 66.9	& 99.0	& 94.8	& 52.4	& 38.1	& 36.0	& 49.3	& 52.6	& 91.3	& 88.6 \\
        HDGCN~\cite{liang2019hierarchical}	        & 68.3	& 99.4	& 93.0	& 67.7	& 75.7	& 25.7	& 44.7	& 37.1	& 81.9	& 89.6 \\
ConvPoint~\cite{boulch2020convpoint}	    & 75.9	& 99.5	& 95.1	& 71.6	& 88.7	& 46.7	& 52.9	& 53.5	& 89.4	& 85.4 \\
KPConv~\cite{thomas2019kpconv}	        & 82.0	& 99.5	& 94.0	& 71.3	& 83.1	& \textbf{78.7}	& 47.7	& \textbf{78.2}	& 94.4	& 91.4 \\
        \hline
FKAConv	(ours \textit{fusion})      & \textbf{82.7}	& \textbf{99.6}	& \textbf{98.1}	& \textbf{77.2}	& \textbf{91.1}	& 64.7	& \textbf{66.5}	& 58.1	& \textbf{95.6}	& \textbf{93.9} \\
        Rank            & 1     & 1     & 1     & 1     & 1     & 2     & 1     & 2     & 1     & 1     \\
        \multicolumn{10}{l}{\textit{Note: We report here only the published methods at the time of writing.}}
    \end{tabular}
    }
    
    \vspace{0.2cm}
    {
    \tiny
    \begin{tabular}{l|c@{~}c|c@{~}c@{~}c@{~}c@{~}c@{~}c@{~}c@{~}c}
        \multicolumn{11}{c}{\scriptsize (c) Semantic3D}\\ \hline
        Method          & Av. & OA  & Man   & Nat.   & High  & Low   & Build. & Hard  & Art. & Cars\\
                        & IoU &     & made  &           & veg.  & veg.  &           & scape & \\
        \hline
        TML-PC~\cite{montoya2014mind}	        & 39.1	& 74.5	& 80.4	& 66.1	& 42.3	& 41.2	& 64.7	& 12.4	& 0.	& 5.8 \\
        TMLC-MS~\cite{hackel2016fast}           & 49.4	& 85.0	& 91.1	& 69.5	& 32.8	& 21.6	& 87.6	& 25.9	& 11.3	& 55.3 \\
        PointNet++~\cite{qi2017pointnet++}      & 63.1	& 85.7	& 81.9	& 78.1	& 64.3	& 51.7	& 75.9	& 36.4	& 43.7	& 72.6 \\
        EdgeConv~\cite{contreras2019edge}       & 64.4	& 89.6	& 91.1	& 69.5	& 65.0	& 56.0	& 89.7	& 30.0	& 43..8	& 69.7 \\
        SnapNet~\cite{boulch2017snapnet}        & 67.4	& 91.0	& 89.6	& 79.5	& 74.8	& 56.1	& 90.9	& 36.5	& 34.3	& 77.2 \\
        PointGCR~\cite{ma2020global}                         & 69.5	& 92.1	& 93.8	& 80.0	& 64.4	& 66.4	& 93.2	& 39.2	& 34.3	& 85.3 \\
        FPCR~\cite{truong2019fast}	                            & 72.0	& 90.6  & 86.4	& 70.3	& 69.5  & 68.0	& 96.9	& 43.4	& 52.3	& 89.5 \\
        SPGraph~\cite{landrieu2018large}        & 76.2	& 92.9	& 91.5	& 75.6	& \textbf{78.3}	& 71.7	& 94.4	& \textbf{56.8}	& 52.9	& 88.4 \\
        ConvPoint~\cite{boulch2020convpoint} & \textbf{76.5}	& 93.4	& 92.1	& 80.6	& 76.0	& \textbf{71.9}	& \textbf{95.6}	& 47.3	& \textbf{61.1}	& 87.7 \\
        \hline
        FKAConv* (ours fusion)                   & 74.6	& \textbf{94.1}	& \textbf{94.7}	& \textbf{85.2}	& 77.4	& 70.4	& 94.0	& 52.9	& 29.4	& \textbf{92.6} \\
        Rank                                & 3	    & 1	    & 1	    & 1	    & 2	    & 3	    & 5	    & 2	    & 9	    & 1 \\
        \multicolumn{10}{l}{\textit{Note: We report here only the published methods at the time of writing.}}\\
        \multicolumn{10}{l}{\textit{*In the official benchmark, the entry corresponding to our method is called LightConvPoint,}}\\
        \multicolumn{10}{l}{\textit{~~which refers to the framework used for our implementation.}}
        
    \end{tabular}
    }
\end{table}

\begin{figure}
    \centering
    \newcommand{\tlap}[1]{\vbox to 0pt{\vss\hbox{#1}}}
    
    \begin{tabular}{ccccc}
    \includegraphics[width=0.19\linewidth]{images/s3dis/s3dis_A5_office1_3_small_rgb.png}&
    \includegraphics[width=0.19\linewidth]{images/s3dis/s3dis_A5_office1_3_small_gt.png}&
    \includegraphics[width=0.19\linewidth]{images/s3dis/s3dis_A5_office1_3_small_pointweb.png}&
    \includegraphics[width=0.19\linewidth]{images/s3dis/s3dis_A5_office1_3_small_convpoint.png}&
    \includegraphics[width=0.19\linewidth]{images/s3dis/s3dis_A5_office1_3_small_fkaconv.png}\\
    \textit{\llap{In}put:\,3D\,\,w/RG\rlap{B}} & \textit{~Groung truth} & \textit{PointWeb}\cite{zhao2019pointweb} & \textit{ConvPoint}\cite{boulch2020convpoint} & \textit{\llap{F}KAConv (ours\rlap)}
    \end{tabular}
    
    \vspace{0.2cm}
    
    \begin{tabular}{cccc}
        \includegraphics[width=0.24\linewidth]{images/npm3d/ajaccio57_detail_intensity.png}&  
        \includegraphics[width=0.24\linewidth]{images/npm3d/ajaccio57_detail_elevation.png}&  
        \includegraphics[width=0.24\linewidth]{images/npm3d/ajaccio57_detail_convpoint.png}&  
        \includegraphics[width=0.24\linewidth]{images/npm3d/ajaccio57_detail_fkaconv.png} \\
        \textit{\llap{In}:\,3D\,w/laser\,intens\rlap{ity}} & \textit{Elevation view} & \textit{ConvPoint~\cite{boulch2020convpoint}} & \textit{FKAConv (ours)}
    \end{tabular}
    
    \vspace{0.2cm}
    
    \begin{tabular}{cccc}
        \includegraphics[width=0.24\linewidth]{images/semantic8/sg27_s10_detail_rgb.png} &  
        \includegraphics[width=0.24\linewidth]{images/semantic8/sg27_s10_detail_elevation.png} &
        \includegraphics[width=0.24\linewidth]{images/semantic8/sg27_s10_detail_convpoint.png} &
        \includegraphics[width=0.24\linewidth]{images/semantic8/sg27_s10_detail_fkaconv.png} \\
        \textit{Input: 3D with RGB} & \textit{Elevation view} & \textit{ConvPoint~\cite{boulch2020convpoint}} & \textit{FKAConv (ours)}
    \end{tabular}
    
    \caption{Visual results of semantic segmentation: S3DIS (1\tlap{\textsuperscript{st}} row), NPM3D (2\tlap{\textsuperscript{nd}} row) and Semantic3D (3\tlap{\textsuperscript{rd}} row). Ground truth of test data publicly unavailable for last two.}
    \label{fig:sem_visu}
\end{figure}


The results are presented in Fig.~\ref{fig:sem_visu} and Table~\ref{tab:bench_seg}.
We use S3DIS (Table~\ref{tab:bench_seg}(a)) to study the impact of the training strategy.
As underlined in~\cite{thomas2019kpconv,boulch2020convpoint}, direct learning with colored points yields a model relying too much on color information, at the expense of geometric information.
We train three models.
The first is the baseline model trained with color information, the second uses color dropout as in~\cite{thomas2019kpconv}, and the third is a dual model with a fusion module~\cite{boulch2020convpoint}.
We observe that fusion gives the best results.
In practice, the model trained with modality dropout tends to select one of the two modalities, either color or geometry, depending on what modality gives the best results.
On the contrary, the fusion technique uses two networks each trained with a different modality, resulting in a lot larger network, but ensuring that the information of both modalities is taken into account.

Our network is second on S3DIS, first on NPM3D and third on Semantic8.
On S3DIS, it is the best approach for 3 out of 13 categories and it performs well on the remaining ones.
We are only outperformed by KPConv, which is based on radius search.
On NPM3D, we reach an average intersection over union (av. IoU) of 82.7, which is 0.7 point above the second best method.
On Semantic8, we place third according to average IoU, and first on overall accuracy among the published and arXiv methods. We obtain the best scores in 3 out 8 categories (the top-3 for 6 categories out of 8). More interestingly, we exceed the scores of ConvPoint~\cite{boulch2020convpoint} on 5 categories.
The only downside is the very low score on the category of artefacts.
One possible explanation could be that the architecture used in this paper (the residual network) is not suitable to learn a reject class (the artefact class is mainly all the points that do not belong to the 7 other classes, i.e., pedestrians but also scanning outliers).
It is future work to train the ConvPoint network with our convolution layer to support this hypothesis.

    

\subsection{Support point sampling: discretization parameter.}
\label{subsubsec:expe_support}

The rule of thumb in Equation~\eqref{eq:vox_size} was derived in a simplistic case: a point cloud sampled from an axis aligned plane crossing a regular voxel grid.
In practice, planar surfaces are very common, particularly in semantic segmentation (walls, floors, etc.), but are not a good model for most of the object of the scenes (chairs, cars, vegetation, \emph{etc.}).
To validate Eq.\,\eqref{eq:vox_size}, we compute the optimal quantization parameter (i.e., the parameter with the largest value leading to the desired number of support points in a single quantization) computed using a dichotomic search on the parameter space and compare it to the derived expression. Figure~\ref{fig:voxsize} presents the results of the experiment.
For each point cloud, the optimal voxel size is represented by a semi-transparent disk (blue for ShapeNet, orange for S3DIS) and can be compared to the derived expression (red curve).
In our setting, a curve under the colored disks is not desired; it is an over-quantization.
We prefer a curve above these disks, possibly leading to extra iterations, but not affecting performance.
We observe that Equation~\eqref{eq:vox_size} provides a good estimate of the voxel size, especially for S3DIS which is a dataset containing a lot of planes.
For ShapeNet, we observe a higher variance, due to the great variability of shapes.
Because of numerous objects that cannot be modeled well by planes in ShapeNet, we slightly overestimate the voxel size, leading only to one spurious iteration, which only slightly slows down the operation.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\linewidth]{images/voxsize_estimation2.pdf}
    \caption{Empirical validation of voxel size estimation: ShapeNet (blue), S3DIS (orange). Each dot is the empirical optimal voxel size obtained by dichotomic search. The red line is the voxel size defined as the inverse square root of the number of support points.}
    \label{fig:voxsize}
\end{figure}



\subsection{Support point sampling: computation times.}

To assess our sampling approach, we run two experiments.
First, in Table~\ref{tab:timing_sampling}(a), we compare the sampling time as a function of the size of the input point cloud.
The number of support points is half the input point cloud size, and the number of neighbors is . The scores are averaged over 5000 random points clouds sampled in a cube.
We also report the ShapeNet scores to relate the performance and the computation times.
We compare our sampling strategy with farthest point sampling~\cite{qi2017pointnet++}, with iterative neighborhood rejection~\cite{boulch2020convpoint} and with a random baseline.
As farthest point sampling~\cite{qi2017pointnet++} is the reference of several state-of-the-art methods, we give the gain relatively to this method in percentage.
Our quantized sampling is almost as fast as random sampling and much more efficient than farthest point sampling.
In fact, our sampling has almost a linear complexity, compared to farthest point sampling, that has a quadratic complexity.

\begin{table}[t]
\caption{Computation time and memory consumption.}
\label{tab:timing_sampling}
\centering

\begin{minipage}{0.49\linewidth}
(a) Computation times for different\\
sampling strategies.
~\\~\\
{
\tiny
\begin{tabular}{l|r|r|r|c}
Method & \multicolumn{3}{c|}{Sampling time (ms)} & ShapeNet\\
       & 1k pts & 5k pts    &   10k pts & (mIoU)\\
\hline
Random     & 1.66 & 8.6 & 18.6 & 84.4\\
(baseline) & ({\color{ForestGreen}-60\%}) & ({\color{ForestGreen}-89\%}) &({\color{ForestGreen}-94\%}) \\
\hline
ConvPoint~\cite{boulch2020convpoint} & 2.60    & 25.4     & 88.2   & 84.6 \\
& ({\color{ForestGreen}-37\%}) & ({\color{ForestGreen}-68\%}) & ({\color{ForestGreen}-71\%}) \\
Farthest~\cite{qi2017pointnet++}        & 4.12    & 79.8    & 310.2   & 84.7\\
 &(-) &(-) &(-)\\
\hline
FKAConv sampling                            &  1.93   & 10.3   & 20.4 & 84.6\\
 &({\color{ForestGreen}-53\%}) & ({\color{ForestGreen}-87\%})   & ({\color{ForestGreen}-93\%})\\
\multicolumn{5}{l}{~~~~(time for  inputs points,  support points,}\\
\multicolumn{5}{l}{~~~~~ neighbors, averaged over  iterations).}
\end{tabular}
}

\end{minipage}
\hfill
\begin{minipage}{0.49\linewidth}

(b) Time and memory consumption for a segmentation network, with 8192 points.~\\~\\
{
\tiny
\begin{tabular}{l|rr|rr}Convolution & \multicolumn{2}{c|}{Training} & \multicolumn{2}{c}{Test}\\
Layer       & Time  & Memory& Time  & Memory\\
            & (ms)  & (GB)  & (ms)  & (GB)  \\
\hline
ConvPoint~\cite{boulch2020convpoint} & 85.7 & 10.1   & 65    & 2.9   \\
ConvPoint*                              & \textbf{12.2} & \textbf{4.3}    & 4.29  & 1.6  \\
PointCNN*~\cite{li2018pointcnn}         & 33.6 & 3.5    & 6.23  & 1.7  \\
PCCN*~\cite{wang2018deep}               & 31.1 & 4.9    & 10.2  & 2.3  \\
PCCN** (bs4)                            & 64.2 & 6.4    & 19.7  & 2.6  \\
\hline
FKAConv (Ours)                              & 19.1 & 5.6   & \textbf{4.9}  & \textbf{1.4}  \\
\multicolumn{5}{l}{*: reimplemented in our framework.}\\
\multicolumn{5}{l}{**: original formulation without separation trick,}\\
\multicolumn{5}{l}{~~~~~differs from code used for experiments in~\cite{wang2018deep}.}
\end{tabular}
}
\end{minipage}
\end{table}


\subsection{Inference time and memory consumption}

We present in Table~\ref{tab:timing_sampling}(b) the performance of our convolution layer and compare it to other convolutional layers.
All computation times and memory usage are given for the segmentation network architecture and for one point cloud. The measures were done with  points in each point cloud and a batch size of  (except for PCCN** for which the batch size is reduce to  to fit in the 11 GB GPU memory).
Computational times are given per point cloud in milliseconds, and memory usage is reported in gigabytes.

We observe that our computation times at inference are very similar to those of ConvPoint~\cite{boulch2020convpoint}, which is expected as it falls into our same general formulation.
The same would probably be observed for a -NN version of the KPConv~\cite{thomas2019kpconv}.
Then, we remark that PointCNN~\cite{li2018pointcnn} and PCCN~\cite{wang2018deep} are up to twice slower for inference.
PCCN uses the separable kernel trick to improve memory performance (cf.\ Section~\ref{sec:kernelsep}).
In this form, it is similar to  ( being the number of filters of the layer) parallel instances of our layer with one kernel element, i.e., it is equivalent to estimating a different  for each .
We also report in Table~\ref{tab:timing_sampling}(b) the performance for PCCN**, which is the the purely continuous convolution described in PCCN~\cite{wang2018deep}, but without the separable kernel trick.

\subsection{Filter visualization}

Our method FKAConv was derived from the discrete convolution on regular grids. The behavior of our 3D filters should thus be comparable to their 2D counterparts.
In Figure~\ref{fig:filter}, we present the outputs of early and deep filters for the classification network on ModelNet40.
For easier visualization, the features at coarse scales (high level / deep features) have been upsampled at the full point-cloud resolution.
We notice that early layers produce features based on surface orientation. This is consistent with the small receptive field of early layers, that yields fine-scale features.
On the contrary, deep layers produces shape-related features detecting objects parts, such as people heads or airplane bodies.

\begin{figure}[t]
\centering

\begin{tabular}{c@{}c@{}c@{}c@{~}|c@{}@{~}c@{}c@{}c@{}c}
    \multicolumn{4}{c}{Low-level features} & \multicolumn{5}{c}{High-level features}\\
    ~\\
    \includegraphics[width=0.10\linewidth]{images/filters/level_0/shape06_f05.png}&
    \includegraphics[width=0.10\linewidth]{images/filters/level_0/shape06_f07.png}&
    \includegraphics[width=0.10\linewidth]{images/filters/level_0/shape06_f51.png}&
    \includegraphics[width=0.10\linewidth]{images/filters/level_0/shape06_f61.png}&
     \includegraphics[width=0.10\linewidth]{images/filters/level_3/shape06_f48.png}&
     \includegraphics[width=0.10\linewidth]{images/filters/level_3/shape06_f58.png}&
     \includegraphics[width=0.10\linewidth]{images/filters/level_3/shape06_f69.png}&
     \includegraphics[width=0.10\linewidth]{images/filters/level_3/shape06_f71.png}&
     \includegraphics[width=0.10\linewidth]{images/filters/level_3/shape06_f80.png}\\
     \includegraphics[width=0.10\linewidth]{images/filters/level_0/shape10_f05.png}&
     \includegraphics[width=0.10\linewidth]{images/filters/level_0/shape10_f07.png}&
     \includegraphics[width=0.10\linewidth]{images/filters/level_0/shape10_f51.png}&
     \includegraphics[width=0.10\linewidth]{images/filters/level_0/shape10_f61.png}&
     \includegraphics[width=0.10\linewidth]{images/filters/level_3/shape10_f48.png}&
     \includegraphics[width=0.10\linewidth]{images/filters/level_3/shape10_f58.png}&
     \includegraphics[width=0.10\linewidth]{images/filters/level_3/shape10_f69.png}&
     \includegraphics[width=0.10\linewidth]{images/filters/level_3/shape10_f71.png}&
     \includegraphics[width=0.10\linewidth]{images/filters/level_3/shape10_f80.png}\\
     \includegraphics[width=0.10\linewidth]{images/filters/level_0/shape15_1_f05.png}&
     \includegraphics[width=0.10\linewidth]{images/filters/level_0/shape15_1_f07.png}&
     \includegraphics[width=0.10\linewidth]{images/filters/level_0/shape15_1_f51.png}&
     \includegraphics[width=0.10\linewidth]{images/filters/level_0/shape15_1_f61.png}&
     \includegraphics[width=0.10\linewidth]{images/filters/level_3/shape15_1_f48.png}&
     \includegraphics[width=0.10\linewidth]{images/filters/level_3/shape15_1_f58.png}&
     \includegraphics[width=0.10\linewidth]{images/filters/level_3/shape15_1_f69.png}&
     \includegraphics[width=0.10\linewidth]{images/filters/level_3/shape15_1_f71.png}&
     \includegraphics[width=0.10\linewidth]{images/filters/level_3/shape15_1_f80.png}
\end{tabular}

    \caption{FKAConv filter response for different input shapes on ModelNet40. Low-level features are extracted from the first layer (4 filters), and high-level features from the fourth layer (5 filters). The colormap represents the filter response for the shape, from blue (low response) to red (high response).}
    \label{fig:filter}
\end{figure}


