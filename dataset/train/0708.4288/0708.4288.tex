\emptythanks
\chapter{The Tree Inclusion Problem: In Linear Space and Faster}\label{chap:tree2}

\title{The Tree Inclusion Problem: In Linear Space and Faster}

\author{Philip Bille \\ IT University of Copenhagen \\ \texttt{beetle@itu.dk}
\and Inge Li G{\o}rtz\thanks{Part of this work was performed while the author was a PhD student at the IT University of Copenhagen.} \\ Technical University of Denmark \\ {\tt ilg@imm.dtu.dk}}

\date{}
\cleartooddpage

\maketitle


\begin{abstract}
Given two rooted, ordered, and labeled trees $P$ and $T$ the tree
inclusion problem is to determine if $P$ can be obtained from $T$
by deleting nodes in $T$. This problem has recently been
recognized as an important query primitive in XML databases.
Kilpel\"{a}inen and Mannila [\emph{SIAM J. Comput. 1995}] presented the
first polynomial time algorithm using quadratic time and space.
Since then several improved results have been obtained for special
cases when $P$ and $T$ have a small number of leaves or small
depth. However, in the worst case these algorithms still use
quadratic time and space. In this paper we present a new approach
to the problem which leads to an algorithm using linear space and subquadratic running time. Our algorithm improves all previous time and space bounds. Most importantly, the space is improved by a linear factor. This will likely make it possible to query larger XML databases and speed up the query time since more of the computation can be kept in main memory.
\end{abstract}


\section{Introduction}
Let $T$ be a rooted tree. We say that $T$ is \emph{labeled} if
each node is assigned a character from an alphabet
$\Sigma$ and we say that $T$ is \emph{ordered} if a left-to-right
order among siblings in $T$ is given. All trees in this paper are rooted, ordered, and labeled.
A tree $P$ is \emph{included} in $T$, denoted $P\sqsubseteq T$, if $P$ can be
obtained from $T$ by deleting nodes of $T$. Deleting a node $v$ in
$T$ means making the children of $v$ children of the parent of $v$
and then removing $v$. The children are inserted in the place of
$v$ in the left-to-right order among the siblings of $v$. The \emph{tree inclusion problem}
is to determine if $P$ can be included in $T$ and if so report all subtrees of $T$ that include $P$.
\begin{figure}[t]
\begin{center}
  \begin{psmatrix}[colsep=0.6cm,rowsep=0.4cm,labelsep=1pt, nodesep=1pt]
  &&&&&&&&[name=cat] catalog \\
&&[name=book] book &&&&&&[name=book2] book & [name=book3] & [name=book4]\\
& [name=author] author &&[name=chapter] chapter&&
  [name=author2] author && [name=chapter2] chapter && [name=chapter3] chapter \\
& [name=john] john && [name=xml] XML &&
  [name=name] name && [name=title] title  & [name=section] section & [name=title2] title \\
&&&&&  [name=john2] john && [name=DB] databases & [name=xml2] XML & [name=queries] queries \\
&& (a) &&&&&& (b) \\
\ncline{author}{book} \ncline{chapter}{book}
  \ncline{john}{author} \ncline{xml}{chapter}
\ncline{cat}{book2}\psset{linestyle=dashed,dash=2pt 4pt}\ncline{cat}{book3} \ncline{cat}{book4}
  \psset{linestyle=solid}
  \ncline{book2}{author2}\ncline{book2}{chapter2}\ncline{book2}{chapter3}
  \ncline{author2}{name}\ncline{chapter2}{title}\ncline{chapter2}{section}\ncline{chapter3}{title2}
  \ncline{name}{john2}\ncline{title}{DB}\ncline{section}{xml2}\ncline{title2}{queries}
  \end{psmatrix}

  \begin{psmatrix}[colsep=0.6cm,rowsep=0.4cm,labelsep=1pt, nodesep=1pt]
  &&&&&&&&[name=cat] catalog \\
&&[name=book] book &&&&&&[name=book2] book & [name=book3] & [name=book4]\\
& [name=author] author &&[name=chapter] chapter&&
  [name=author2] author && [name=chapter2] chapter && [name=chapter3] chapter \\
& [name=john] john && [name=xml] XML &&
  [name=name] name && [name=title] title  & [name=section] section & [name=title2] title \\
&&&&&  [name=john2] john && [name=DB] databases & [name=xml2] XML & [name=queries] queries \\
&&&&&&(c)&&&& \\
\ncline{author}{book} \ncline{chapter}{book}
  \ncline{john}{author} \ncline{xml}{chapter}
\ncline{cat}{book2}\psset{linestyle=dashed,dash=2pt 4pt} \ncline{cat}{book3} \ncline{cat}{book4}
  \psset{linestyle=solid}
  \ncline{book2}{author2}\ncline{book2}{chapter2}\ncline{book2}{chapter3}
  \ncline{author2}{name}\ncline{chapter2}{title}\ncline{chapter2}{section}\ncline{chapter3}{title2}
  \ncline{name}{john2}\ncline{title}{DB}\ncline{section}{xml2}\ncline{title2}{queries}

\psset{linestyle=dashed}
  \nccurve[angleA=5,angleB=175]{book}{book2}
  \nccurve[angleA=345,angleB=190]{author}{author2}
  \nccurve[angleA=320,angleB=190]{john}{john2}
  \nccurve[angleA=15,angleB=165]{chapter}{chapter2}
  \nccurve[angleA=350,angleB=170]{xml}{xml2}
  \end{psmatrix}
   \caption{Can the tree (a) be included in the tree (b)? It can and an embedding is given in (c).}
  \label{t2:inclusionexample}
  \end{center}
\end{figure}


Recently, the problem has been recognized as an important query
primitive for XML data and has received considerable attention,
see e.g., \cite{SM2002, YLH2003,YLH2004, ZADR03, SN2000, TRS2002}.
The key idea is that an XML document can be viewed as a tree and queries on the document correspond to a tree
inclusion problem. As an example consider Figure~\ref{t2:inclusionexample}. Suppose that we want to maintain
a catalog of books for a bookstore. A fragment of the tree,
denoted $D$, corresponding to the catalog is shown in (b). In addition to supporting
full-text queries, such as find all documents containing the word
"John", we can also utilize the tree structure of the catalog to
ask more specific queries, such as "find all books written by John
with a chapter that has something to do with XML". We can model
this query by constructing the tree, denoted $Q$, shown in (a) and
solve the tree inclusion problem: is $Q \sqsubseteq D$? The answer
is yes and a possible way to include $Q$ in $D$ is indicated by
the dashed lines in (c). If we delete all the nodes in $D$ not
touched by dashed lines the trees $Q$ and $D$ become isomorphic.
Such a mapping of the nodes from $Q$ to $D$ given by the dashed
lines is called an \emph{embedding} (formally defined in
Section~\ref{t2:sec:recursion}).

The tree inclusion problem was initially introduced by Knuth
\cite[exercise 2.3.2-22]{Knuth1969} who gave a sufficient
condition for testing inclusion. Motivated by applications in
structured databases \cite{KM93, MR90} Kilpel\"{a}inen and Mannila
\cite{KM1995} presented the first polynomial time algorithm using
$O(n_Pn_T)$ time and space, where $n_P$ and $n_T$ is the number of
nodes in $P$ and $T$, respectively. During the last decade
several improvements of the original algorithm of \cite{KM1995}
have been suggested \cite{Kilpelainen1992,AS2001, Richter1997a,
Chen1998}. The previously best known bound is due to Chen
\cite{Chen1998} who presented an algorithm using $O(l_Pn_T)$ time
and $O(l_P \cdot \min\{d_T, l_T\})$ space. Here, $l_S$ and $d_S$ denotes
the number of leaves and the maximum depth of a tree $S$,
respectively. This algorithm is based on an algorithm of
Kilpel\"{a}inen \cite{Kilpelainen1992}. Note that the time and
space is still $\Theta(n_Pn_T)$ for worst-case input trees.

In this paper we present three algorithms which combined improves all of the previously known time and space bounds. To avoid trivial cases we always assume that $1 \leq n_P \leq n_T$. We show the following theorem:
\begin{theorem}\label{t2:thm:main}
For trees $P$ and $T$ the tree inclusion problem can be solved in $O(n_T)$ space with the following running times:
\begin{equation*}
\min
\begin{cases}
      O(l_Pn_T), \\
      O(n_Pl_T\log \log n_T + n_T), \\
      O(\frac{n_Pn_T}{\log n_T} + n_{T}\log n_{T}).
\end{cases}
\end{equation*}
\end{theorem}
Hence, when either $P$ or $T$ has few leaves we obtain fast algorithms. When both trees have many leaves and $n_{P} = \Omega (\log^{2} n_{T})$, we instead improve the previous quadratic time bound by a logarithmic factor. Most importantly, the space used is linear. In the context of XML databases this will likely make it possible to query larger trees and speed up the query time since more of the computation can be kept in main memory.

\subsection{Techniques}
Most of the previous algorithms, including the best one
\cite{Chen1998}, are essentially based on a simple dynamic
programming approach from the original algorithm of \cite{KM1995}. The
main idea behind this algorithm is the following: Let $v$ be a node in $P$ with children $v_1, \ldots, v_i$ and
let $w$ be a node in $T$ with children $w_1, \ldots, w_j$. Consider the subtrees rooted at $v$ and $w$, denoted by $P(v)$ and $T(w)$. To decide if $P(v)$ can be included in $T(w)$ we try to find a sequence of numbers $1 \leq x_1 < x_2 < \cdots < x_i \leq j$ such that $P(v_k)$ can be included in $T(w_{x_k})$ for all $k$, $1 \leq k \leq i$. If we have already determined whether or not $P(v_s) \sqsubseteq T(w_t)$, for all $s$ and $t$, $1\leq s \leq i$, $1\leq t \leq j$, we can efficiently find such a sequence by scanning the children of $v$ from left to
right. Hence, applying this approach in a bottom-up fashion we can determine, if $P(v) \sqsubseteq T(w)$, for all pairs of nodes $v$ in $P$ and $w$ in $T$.

In this paper we take a different approach. The main
idea is to construct a data structure on $T$ supporting a small
number of procedures, called the \emph{set procedures}, on subsets
of nodes of $T$. We show that any such data structure implies an
algorithm for the tree inclusion problem. We consider various
implementations of this data structure which all use linear space.
The first simple implementation gives an algorithm with
$O(l_Pn_T)$ running time. As it turns out, the running time
depends on a well-studied problem known as the \emph{tree color
problem}. We show a direct connection between a data structure
for the tree color problem and the tree inclusion problem.
Plugging in a data structure of Dietz \cite{Die89} we obtain an
algorithm with $O(n_Pl_T\log \log n_T + n_T)$ running time.

Based on the simple algorithms above we show how to improve the
worst-case running time of the set procedures by a logarithmic
factor. The general idea used to achieve this is to divide $T$
into small trees called \emph{clusters} of logarithmic size which overlap with other clusters in at most $2$ nodes. Each cluster is represented by a constant number of nodes in a \emph{macro tree}. The nodes in the
macro tree are then connected according to the overlap of the cluster they represent. We show how to efficiently preprocess the clusters and the macro tree such that the set procedures use constant time for each cluster. Hence, the worst-case quadratic running time is improved by a logarithmic factor.

Throughout the paper we assume a unit-cost RAM model of computation with word size $\Theta(\log n_T)$ and a standard instruction set including bitwise boolean operations, shifts, addition, and multiplication. All space complexities refer to the number of words used by the algorithm.

\subsection{Related Work}
For some applications considering \emph{unordered} trees is more
natural. However, in \cite{MT1992,KM1995} this problem was proved
to be NP-complete. The tree inclusion problem is closely related
to the \emph{tree pattern matching problem} \cite{CO1982,
Kosaraju1989,DGM1990, CHI1999}. The goal is here to find an
injective mapping $f$ from the nodes of $P$ to the nodes of $T$
such that for every node $v$ in $P$ the $i$th child of $v$ is
mapped to the $i$th child of $f(v)$. The tree pattern matching
problem can be solved in $(n_P+n_T) \log^{O(1)} (n_P+n_T)$ time. Another similar problem is the \emph{subtree isomorphism} problem \cite{Chung1987, ST1999}, which is to determine if $T$ has
a subgraph isomorphic to $P$. The subtree isomorphism
problem can be solved efficiently for ordered and unordered trees.
The best algorithms for this problem use $O(\frac{n_P^{1.5}n_T}{\log
n_P} + n_T)$ time for unordered trees and $O(\frac{n_Pn_T}{\log n_P} + n_T)$ time for ordered
trees \cite{Chung1987, ST1999}. Both use $O(n_Pn_T)$ space. The
tree inclusion problem can be considered a special case of the
\emph{tree edit distance problem} \cite{Tai1979, ZS1989,
Klein1998, DMRW2006}. Here one wants to find the minimum sequence of insert,
delete, and relabel operations needed to transform $P$ into $T$.
Currently the best algorithm for this problem uses $O(n_T n_P^2 (1 + \log \frac{n_T}{n_P}))$ time~\cite{DMRW2006}. For more details and references see the survey \cite{Bille2005}.
\subsection{Outline}
In Section~\ref{t2:def} we give notation and definitions used
throughout the paper. In Section~\ref{t2:sec:recursion} a common
framework for our tree inclusion algorithms is given.
Section~\ref{t2:simple} present two simple algorithms and then, based on these result, we show how to get a faster algorithm in Section~\ref{t2:micromacro}.

\section{Notation and Definitions}\label{t2:def}
In this section we define the notation and definitions we will use
throughout the paper. For a graph $G$ we denote the set of nodes
and edges by $V(G)$ and $E(G)$, respectively. Let $T$ be a rooted
tree. The root of $T$ is denoted by $\roots(T)$. The \emph{size}
of $T$, denoted by $n_T$, is $|V(T)|$. The \emph{depth} of a node
$v\in V(T)$, $\depth(v)$, is the number of edges on the path from
$v$ to $\roots(T)$ and the depth of $T$, denoted $d_T$, is the
maximum depth of any node in $T$. The parent of $v$ is denoted $\parent(v)$ and the set of children of $v$ is denoted $\child(v)$. A node with no children is a leaf and otherwise an internal node. The set of leaves of $T$ is
denoted $L(T)$ and we define $l_T = |L(T)|$. We say that $T$ is \emph{labeled} if
each node $v$ is a assigned a character, denoted $\lab(v)$, from an alphabet
$\Sigma$ and we say that $T$ is \emph{ordered} if a left-to-right
order among siblings in $T$ is given. All trees in this paper are rooted, ordered, and labeled.

\paragraph{Ancestors and Descendants}
Let $T(v)$ denote the subtree of $T$ rooted at a node $v \in
V(T)$. If $w\in V(T(v))$ then $v$ is an ancestor of $w$, denoted
$v \preceq w$, and if $w\in V(T(v))\backslash \{v\}$ then $v$ is a
proper ancestor of $w$, denoted $v \prec w$. If $v$ is a (proper)
ancestor of $w$ then $w$ is a (proper) descendant of $v$. A node
$z$ is a common ancestor of $v$ and $w$ if it is an ancestor of
both $v$ and $w$. The nearest common ancestor of $v$ and $w$,
$\nca(v,w)$, is the common ancestor of $v$ and $w$ of greatest
depth. The \emph{first ancestor of $w$ labeled $\alpha$}, denoted
$\fl(w,\alpha)$, is the node $v$ such that $v \preceq w$, $\lab(v)
= \alpha$, and no node on the path between $v$ and $w$ is labeled
$\alpha$. If no such node exists then $\fl(w,\alpha) = \bot$,
where $\bot \not\in V(T)$ is a special \emph{null node}. 

\paragraph{Traversals and Orderings}
Let $T$ be a tree with root $v$ and let $v_1, \ldots ,v_k$ be the
children of $v$ from left-to-right. The \emph{preorder traversal}
of $T$ is obtained by visiting $v$ and then recursively visiting
$T(v_i)$, $1 \leq i \leq k$, in order. Similarly, the
\emph{postorder traversal} is obtained by first visiting $T(v_i)$,
$1 \leq i \leq k$, in order and then $v$. The \emph{preorder
number} and \emph{postorder number} of a node $w \in T(v)$,
denoted by $\pre(w)$ and $\post(w)$, is the number of nodes
preceding $w$ in the preorder and postorder traversal of $T$,
respectively. The nodes to the \emph{left} of $w$ in $T$ is the
set of nodes $u \in V(T)$ such that $\pre(u) < \pre(w)$ and
$\post(u) < \post(w)$. If $u$ is to the left of $w$, denoted by $u
\lhd w$, then $w$ is to the \emph{right} of $u$. If $u \lhd w$, $u
\preceq w$, or $w \prec u$ we write $u \unlhd w$. The null node
$\bot$ is not in the ordering, i.e., $\bot \ntriangleleft v$ for
all nodes $v$.


\paragraph{Minimum Ordered Pairs}
A set of nodes $X \subseteq V(T)$ is \emph{deep} if no node in $X$
is a proper ancestor of another node in $X$. For $k$ deep sets of
nodes $X_1, \ldots, X_k$ let $\Phi(X_1,\ldots,X_k) \subseteq (X_1
\times \cdots \times X_k)$, be the set of tuples such that $(x_1,
\ldots, x_k) \in \Phi(X_1,\ldots,X_k)$ iff $x_1 \lhd \cdots \lhd
x_k$. If $(x_1, \ldots, x_k) \in \Phi(X_1,\ldots,X_k) $ and there
is no $(x_1', \ldots, x_k') \in \Phi(X_1,\ldots,X_k) $, where
either $x_1 \lhd x_1' \lhd x_k' \unlhd x_k$ or $x_1 \unlhd x_1'
\lhd x_k' \lhd x_k$ then the pair $(x_1, x_k)$ is a \emph{minimum
ordered pair}. The set of minimum ordered pairs for $X_1, \ldots,
X_k$ is denoted by $\mop(X_1, \ldots, X_k)$.
Figure~\ref{t2:fig:mopexample} illustrates these concepts on a small
example.
\begin{figure}[t]
\begin{center}
\begin{psmatrix}[colsep=0.6cm,rowsep=0.3cm,labelsep=1pt]
&&&& \cnode{.1}{root}\rput(0,.3){}
  \\
& \cnode{.1}{l}\rput(0,.3){} &&&  \cnode{.1}{c}\rput(-.3,.3){}
  &&
  \cnode{.1}{r}\rput(0,.3){} \\
\cnode*[fillcolor=black]{.1}{ll}\rput(-.3,0){$v_1$}\rput(0,-.3){} &
  \psset{fillstyle=crosshatch}\cnode{.1}{lc}\rput(-.3,0){$v_2$}\rput(0,-.3){} &
  \cnode{.1}{lr}\rput(-.2,-.3){} &  \cnode{.1}{cl}\rput(0,-.3){}
  & &
  \cnode*[fillcolor=black]{.1}{cr}\rput(0,-.3){}\rput(-.3,0){$v_5$} & \psset{fillstyle=vlines}
  \cnode{.1}{rl}\rput(-.2,-.3){}\rput(-.3,0){$v_6$}
  &
  \psset{fillstyle=hlines}\cnode{.1}{rr}\rput(0,-.3){}\rput(-.3,0){$v_7$}&&&
     \\
&& \cnode{.1}{lrc}\rput(0,-.3){} & \cnode*[fillcolor=black]{.1}{cll}\rput(0,-.3){}\rput(-.3,0){$v_3$}
  & \psset{fillstyle=crosshatch}\cnode{.1}{clr}\rput(0,-.3){}\rput(-.3,0){$v_4$}&&
  \cnode{.1}{rlc}\rput(0,-.3){}\\
&&&&\rput(0,0){(a)} \\
\ncline{root}{l}\ncline{root}{c}\ncline{root}{r}
  \ncline{l}{ll}\ncline{l}{lc}\ncline{l}{lr}
  \ncline{c}{cr}\ncline{c}{cl}
  \ncline{r}{rl}\ncline{r}{rr}
  \ncline{lr}{lcr}\ncline{cl}{cll}\ncline{cl}{clr}
  \ncline{rl}{rlc}\ncline{lr}{lrc}
\cnode*[fillcolor=black]{.1}{1}\rput(.5,0){=$S_1$} &&
  \psset{fillstyle=crosshatch}\cnode{.1}{2}\rput(.5,0){=$S_2$} &&
  \psset{fillstyle=vlines}\cnode{.1}{3}\rput(.5,0){=$S_3$} &&
  \psset{fillstyle=hlines}\cnode{.1}{4}\rput(.5,0){=$S_4$} &&
  \end{psmatrix}
\begin{psmatrix}[colsep=0.6cm,rowsep=0.3cm,labelsep=1pt]
&&&& \cnode{.1}{root}\rput(0,.3){}
  \\
& \cnode{.1}{l}\rput(0,.3){} &&&  \cnode{.1}{c}\rput(-.3,.3){}
  &&
  \cnode{.1}{r}\rput(0,.3){} \\
\cnode*[fillcolor=black]{.1}{ll}\rput(-.3,0){$v_1$}\rput(0,-.3){} &
  \psset{fillstyle=crosshatch}\cnode{.1}{lc}\rput(-.3,0){$v_2$}\rput(0,-.3){} &
  \cnode{.1}{lr}\rput(-.2,-.3){} &  \psset{fillstyle=vlines}\cnode{.1}{cl}\rput(0,-.3){}\rput(-.3,0){$v_5$}
  &&
  \cnode{.1}{cr}\rput(0,-.3){} & \psset{fillstyle=vlines}
  \cnode{.1}{rl}\rput(-.2,-.3){}\rput(-.3,0){$v_8$}
  &
  \psset{fillstyle=hlines}\cnode{.1}{rr}\rput(0,-.3){}\rput(-.3,0){$v_9$}
     \\
&& \cnode*[fillcolor=black]{.1}{lrc}\rput(0,-.3){}\rput(-.3,0){$v_3$}
  & \psset{fillstyle=crosshatch}\cnode{.1}{cll}\rput(0,-.3){}\rput(-.3,0){$v_4$}
  & \cnode*[fillcolor=black]{.1}{clr}\rput(0,-.3){}\rput(-.3,0){$v_6$}&&
  \psset{fillstyle=hlines}\cnode{.1}{rlc}\rput(0,-.3){}\rput(-.3,0){$v_7$}\\
&&&&\rput(0,0){(b)} \\
\ncline{root}{l}\ncline{root}{c}\ncline{root}{r}
  \ncline{l}{ll}\ncline{l}{lc}\ncline{l}{lr}
  \ncline{c}{cr}\ncline{c}{cl}
  \ncline{r}{rl}\ncline{r}{rr}
  \ncline{lr}{lcr}\ncline{cl}{cll}\ncline{cl}{clr}
  \ncline{rl}{rlc}\ncline{lr}{lrc}
\end{psmatrix}
\caption{In (a) we have
      $\{(v_1,v_2,v_3,v_6,v_7),(v_1,v_2,v_5,v_6,v_7),
      (v_1,v_4,v_5,v_6,v_7),(v_3,v_4,v_5,v_6,v_7)\}=\Phi(S_1,S_2,S_1,S_3,S_4)$ and
      thus
      $\mop(S_1,S_2,S_1,S_3,S_4)=\{(v_3,v_7)\}$. In  (b) we have
      $\Phi(S_1,S_2,S_1,S_3,S_4)=\{(v_1,v_2,v_3,v_5,v_7),(v_1,v_2,v_6,v_8,v_9),
      (v_1,v_2,v_3,v_8,v_9),(v_1,v_2,v_3,v_5,v_9),(v_1,v_4,v_6,v_8,v_9),
      (v_3,v_4,v_6,v_8,v_9) \}$ and thus
      $\mop(S_1,S_2,S_1,S_3,S_4)=\{(v_1,v_7),(v_3,v_9)\}$.}
  \label{t2:fig:mopexample}
\end{center}
\end{figure}
For any set of pairs $Y$, let $\restrict{Y}{1}$ and
$\restrict{Y}{2}$ denote the \emph{projection} of $Y$ to the first
and second coordinate, that is, if $(y_1,y_2) \in Y$ then $y_1 \in
\restrict{Y}{1}$ and $y_2 \in \restrict{Y}{2}$. We say that $Y$ is
\emph{deep} if $\restrict{Y}{1}$ and $\restrict{Y}{2}$ are deep.
The following lemma shows that given deep sets $X_1, \ldots, X_k$
we can compute $\mop(X_1,\ldots,X_k)$ iteratively by first
computing $\mop(X_1,X_2)$ and then
$\mop(\restrict{\mop(X_1,X_2)}{2},X_3)$ and so on.

\begin{lemma}\label{t2:lem:nnsmaller2}
  For any deep sets of nodes $X_1, \ldots, X_k$ we have,
  $(x_1,x_k) \in \mop(X_1, \ldots, X_k)$ iff there exists a
  $x_{k-1}$  such that $(x_1,x_{k-1}) \in \mop(X_1, \ldots,
  X_{k-1})$ and $(x_{k-1},x_k) \in \mop(\restrict{\mop(X_1,
  \ldots, X_{k-1})}{2}, X_k)$.
\end{lemma}

\begin{proof}
  We start by showing that if $(x_1,x_k) \in \mop(X_1, \ldots,
  X_k)$ then there exists  a node $x_{k-1}$  such that $(x_1,x_{k-1}) \in
  \mop(X_1, \ldots, X_{k-1})$ and $(x_{k-1},x_k) \in
  \mop(\restrict{\mop(X_1, \ldots, X_{k-1})}{2}, X_k)$.

  First note that $(z_1,\ldots,z_k) \in
  \Phi(X_1,\ldots,X_k)$ implies $(z_1,\ldots,z_{k-1}) \in
  \Phi(X_1,\ldots,X_{k-1})$. Since $(x_1,x_k) \in
  \mop(X_1,\ldots,X_k)$ there must be a minimum $x_{k-1}$ such that
  the tuple $(x_1,\ldots,x_{k-1})$ is in $\Phi(X_1,\ldots,X_{k-1})$.
  We have $(x_1, x_{k-1}) \in \mop(X_1,\ldots,X_{k-1})$. We need to
  show $(x_{k-1},x_k) \in \mop(\restrict{\mop(X_1, \ldots, X_{k-1})}{2}, X_k)$.
  Since $(x_1,x_k) \in \mop(X_1, \ldots,
  X_k)$ there exists no $z \in X_k$ such that  $x_{k-1} \lhd z
  \lhd x_k$. Assume there exists a $x \in \restrict{\mop(X_1, \ldots,
  X_{k-1})}{2}$ such that  $x_{k-1} \lhd z
  \lhd x_k$. Since $(x,x_{k-1})\in \mop(X_1,\ldots,X_{k-1})$ this
  implies that there is a $z' \rhd x_1$ such that $(z',z) \in
  \mop(X_1,\ldots,X_{k-1})$. But this implies that the tuple
  $(z',\ldots,z,x_k)$ is in $\Phi(X_1,\ldots,X_k)$ contradicting that
  $(x_1,x_k) \in \mop(X_1,\ldots,X_k)$.

  We will now show that if there exists a $x_{k-1}$  such that $(x_1,x_{k-1}) \in
  \mop(X_1, \ldots, X_{k-1})$ and $(x_{k-1},x_k) \in
  \mop(\restrict{\mop(X_1, \ldots, X_{k-1})}{2}, X_k)$
  then the pair $(x_1,x_k) \in \mop(X_1, \ldots, X_k)$.
  Clearly, there exists a tuple $(x_1,\ldots,x_{k-1},x_k)\in \Phi(X_1,\ldots,X_k)$.
  Assume that there exists a tuple $(z_1,\ldots,z_k)\in
  \Phi(X_1,\ldots,X_k)$ such that $x_1 \lhd z_1 \lhd z_k \unlhd
  x_k$. Since $z_{k-1} \unlhd x_{k-1}$ this contradicts that
  $(x_1,x_{k-1}) \in \mop(X_1,\ldots,X_{k-1})$.
  Assume that there exists a tuple $(z_1,\ldots,z_k)\in
  \Phi(X_1,\ldots,X_k)$ such that $x_1 \unlhd z_1 \lhd z_k \lhd
  x_k$. Since $(x_1,x_{k-1})\in \mop(X_1,\ldots,X_{k-1})$ we have
  $x_{k-1} \unlhd z_{k-1}$ and thus $z_k \rhd x_{k-1}$ contradicting
  $(x_{k-1},x_k) \in \mop(\restrict{\mop(X_1, \ldots, X_{k-1})}{2}, X_k)$.
\qed \end{proof}

When we want to specify which tree we mean in the above relations we add a subscript. For instance, $v \prec_{T} w$ indicates that $v$ is an ancestor of $w$ \emph{in $T$}.

\section{Computing Deep Embeddings}\label{t2:sec:recursion}
In this section we present a general framework for answering tree
inclusion queries. As in \cite{KM1995} we solve the equivalent
\emph{tree embedding problem}. Let $P$ and $T$ be rooted labeled
trees. An \emph{embedding} of $P$ in $T$ is an injective function
$f : V(P) \rightarrow V(T)$ such that for all nodes $v,u \in
V(P)$,
\begin{itemize}
  \item[(i)] $\lab(v) = \lab(f(v))$. (label preservation condition)
  \item[(ii)] $v \prec u$  iff $f(v) \prec f(u)$. (ancestor condition)
  \item[(iii)] $v \lhd u$ iff $f(v) \lhd f(u)$. (order condition)
\end{itemize}
An example of an embedding is given in
Figure~\ref{t2:inclusionexample}(c).
\begin{lemma}[Kilpel\"{a}inen and Mannila~\cite{KM1995}]
For any trees $P$ and $T$, $P \sqsubseteq T$ iff there exists an
embedding of $P$ in $T$.
\end{lemma}
We say that the embedding $f$ is \emph{deep} if there is no
embedding $g$ such that $f(\roots(P)) \prec g(\roots(P))$. The
\emph{deep occurrences} of $P$ in $T$, denoted $\emb(P, T)$ is the
set of nodes,
\begin{equation*}
\emb(P,T) = \{f(\roots(P)) \mid \text{$f$ is a deep embedding of $P$ in $T$}\}.
\end{equation*}
By definition the set of ancestors of nodes in $\emb(P,T)$ is exactly the set of nodes $\{u \mid P \sqsubseteq T(u)\}$. Hence, to solve the tree inclusion problem it is sufficient to compute $\emb(P,T)$ and then, using additional $O(n_T)$ time, report all ancestors of this set. Note that the set $\emb(P,T)$ is deep.

In the following we show how to compute deep embeddings. The key
idea is to build a data structure for $T$ allowing a fast
implementation of the following procedures. For all $X \subseteq
V(T)$, $Y \subseteq V(T)\times V(T)$, and $\alpha \in \Sigma$
define:
\begin{relate}
    \item[$\Parent(X)$:] Return the set
    $\{\parent(x) \mid x \in X\}$.
    \item[$\Nca(Y)$:]
    Return the set $\{\nca(y_1,y_2) \mid (y_1,y_2) \in Y\}$.
    \item[$\Deep(X)$:] Return the set $\{x \in X\mid \text{there is no }
    z \in X \text{ such that } x \prec z\}$.
    \item[$\Mop(Y,X)$:]
    Return the set of pairs $R$ such that for any pair $(y_1,y_2)
    \in Y$, $(y_1,x) \in R$ iff $(y_2, x) \in
    \mop(\restrict{Y}{2}, X)$.
    \item[$\Fl(X, \alpha)$:]
    Return the set $\{\fl(x, \alpha) \mid x \in X\}$.
\end{relate}
Collectively we call these procedures the \emph{set procedures}.
The procedures $\Parent$, $\Nca$, and $\Fl$ are selfexplanatory.
$\Deep(X)$ returns the set of all nodes in $X$ that have no
descendants in $X$. Hence,  the returned set is always deep.
$\Mop$ is used to iteratively compute minimum ordered pairs. If we
want to specify that a procedure applies to a certain tree $T$ we
add the subscript $T$. With the set procedures we can compute deep
embeddings. The following procedure $\Emb(v)$, $v \in V(P)$,
recursively computes the set of deep occurrences of $P(v)$ in $T$.
Figure~\ref{t2:fig:embexample} illustrates how $\Emb$ works on a
small example.
\begin{relate}
\item[$\Emb(v)$:] Let $v_1, \ldots, v_k$ be the sequence of
children of $v$ ordered from left to right. There are three cases:
    \begin{enumerate}
    \item $k=0$ ($v$ is a leaf). Compute $R := \Deep(\Fl(L(T),\lab(v)))$.
    \item $k=1$. Recursively compute $R_1 := \Emb(v_1)$. 
    
    Compute $R := \Deep(\Fl(\Deep(\Parent(R_1)),\lab(v)))$.
    \item $k > 1$. Compute $R_1 := \Emb(v_1)$ and set $U_1 := \{(r,r) \mid r \in R_1\}$. 
    
    For $i := 2$ to $k$, compute $R_i := \Emb(v_i)$ and $U_i := \Mop(U_{i-1}, R_i)$.  
    
    Finally, compute $R := \Deep(\Fl(\Deep(\Nca(U_k)), \lab(v)))$.
    \end{enumerate}
If $R = \emptyset$ stop and report that there is no deep embedding
of $P(v)$ in $T$. Otherwise return $R$.
\end{relate}
\begin{figure}
\begin{center}
  \begin{psmatrix}[colsep=0.6cm,rowsep=0.3cm,labelsep=1pt]
  &&&\rput(0,0){$P$} &&&&&&&&&& \rput(0,0){$T$} \\
&&& \cnode{.1}{root1}\rput(.3,0){$a$}\rput(-.3,0){$1$} &&&&&&&&&&
  \cnode{.1}{root2}\rput(0,.3){$a$}\\
&& \cnode{.1}{l1}\rput(.3,0){$b$}\rput(-.3,0){$2$} &&  \cnode{.1}{r1}\rput(.3,0){$a$}\rput(-.3,0){$4$} &&&&&&
  \cnode{.1}{l2}\rput(0,.3){$b$} &&&  \cnode{.1}{c2}\rput(-.3,.3){$b$} &&&
  \cnode{.1}{r2}\rput(0,.3){$a$} \\
&& \cnode{.1}{ll1}\rput(.3,0){$a$}\rput(-.3,0){$3$} && &&&&&&
  \cnode{.1}{ll2}\rput(0,-.3){$a$} && \cnode{.1}{lc2}\rput(0,-.3){$a$} &&
  \cnode{.1}{rc2}\rput(-.3,-.3){$b$} & \cnode{.1}{lr2}\rput(0,-.3){$a$} &&
  \cnode{.1}{rr2}\rput(0,-.3){$b$} \\
&&&&&&&&&&&&&&\cnode{.1}{e2}\rput(0,-.3){$a$}\\
&&&\rput(0,0){(a)} &&&&&&&&&& \rput(0,0){(b)} \\
\ncline{root1}{l1}\ncline{root1}{r1}
  \ncline{l1}{ll1}
\ncline{root2}{l2}\ncline{root2}{c2}\ncline{root2}{r2}
  \ncline{l2}{ll2}\ncline{c2}{rc2}\ncline{c2}{lc2}
  \ncline{r2}{lr2}\ncline{r2}{rr2}
  \ncline{rc2}{e2}
  \end{psmatrix}
\begin{psmatrix}[colsep=0.6cm,rowsep=0.3cm,labelsep=1pt]
&&& \cnode{.1}{root1}\rput(0,.3){$a$} &&&&&&&&&&
  \cnode{.1}{root2}\rput(0,.3){$a$}\\
\cnode{.1}{l1}\rput(0,.3){$b$} &&&  \cnode{.1}{c1}\rput(-.3,.3){$b$} &&&
  \cnode{.1}{r1}\rput(0,.3){$a$} &&&&
  \cnode*[fillcolor=black]{.1}{l2}\rput(0,.3){$b$} &&&  \cnode{.1}{c2}\rput(-.3,.3){$b$} &&&
  \cnode{.1}{r2}\rput(0,.3){$a$} \\
\psset{fillstyle=crosshatch}\cnode{.1}{ll1}\rput(0,-.3){$a$} && \psset{fillstyle=crosshatch}\cnode{.1}{lc1}\rput(0,-.3){$a$} &&
  \cnode{.1}{rc1}\rput(-.3,-.3){$b$} & \psset{fillstyle=crosshatch}\cnode{.1}{lr1}\rput(0,-.3){$a$} &&
  \cnode{.1}{rr1}\rput(0,-.3){$b$} &&&
  \cnode{.1}{ll2}\rput(0,-.3){$a$} && \cnode{.1}{lc2}\rput(0,-.3){$a$} &&
  \cnode*[fillcolor=black]{.1}{rc2}\rput(-.3,-.3){$b$} & \cnode{.1}{lr2}\rput(0,-.3){$a$} &&
  \cnode{.1}{rr2}\rput(0,-.3){$b$} \\
&&&& \psset{fillstyle=crosshatch}\cnode{.1}{e1}\rput(0,-.3){$a$} &&&&&&&&&&
  \cnode{.1}{e2}\rput(0,-.3){$a$}\\
&&&\rput(0,0){(c)} &&&&&&&&&& \rput(0,0){(d)} \\
\ncline{root1}{l1}\ncline{root1}{c1}\ncline{root1}{r1}
  \ncline{l1}{ll1}\ncline{c1}{rc1}\ncline{c1}{lc1}
  \ncline{r1}{lr1}\ncline{r1}{rr1}
  \ncline{rc1}{e1}
\ncline{root2}{l2}\ncline{root2}{c2}\ncline{root2}{r2}
  \ncline{l2}{ll2}\ncline{c2}{rc2}\ncline{c2}{lc2}
  \ncline{r2}{lr2}\ncline{r2}{rr2}
  \ncline{rc2}{e2}
  \end{psmatrix}
\begin{psmatrix}[colsep=0.6cm,rowsep=0.3cm,labelsep=1pt]
&&& \cnode{.1}{root1}\rput(0,.3){$a$} &&&&&&&&&&
  \cnode*[fillcolor=black]{.1}{root2}\rput(0,.3){$a$}\\
\cnode*[fillcolor=black]{.1}{l1}\rput(0,.3){$b$} &&&  \cnode{.1}{c1}\rput(-.3,.3){$b$} &&&
  \cnode{.1}{r1}\rput(0,.3){$a$} &&&&
  \cnode{.1}{l2}\rput(0,.3){$b$} &&&  \cnode{.1}{c2}\rput(-.3,.3){$b$} &&&
  \cnode{.1}{r2}\rput(0,.3){$a$} \\


  \cnode{.1}{ll1}\rput(0,-.3){$a$} &&\psset{fillstyle=crosshatch}\cnode{.1}{lc1}\rput(0,-.3){$a$} &&
  \cnode*[fillcolor=black]{.1}{rc1}\rput(-.3,-.3){$b$} & \psset{fillstyle=crosshatch}\cnode{.1}{lr1}\rput(0,-.3){$a$} &&
  \cnode{.1}{rr1}\rput(0,-.3){$b$} &&&
  \cnode{.1}{ll2}\rput(0,-.3){$a$} && \cnode{.1}{lc2}\rput(0,-.3){$a$} &&
  \cnode{.1}{rc2}\rput(-.3,-.3){$b$} & \cnode{.1}{lr2}\rput(0,-.3){$a$} &&
  \cnode{.1}{rr2}\rput(0,-.3){$b$} \\
&&&& \cnode{.1}{e1}\rput(0,-.3){$a$} &&&&&&&&&&
  \cnode{.1}{e2}\rput(0,-.3){$a$}\\
&&&\rput(0,0){(e)} &&&&&&&&&& \rput(0,0){(f)} \\
\ncline{root1}{l1}\ncline{root1}{c1}\ncline{root1}{r1}
  \ncline{l1}{ll1}\ncline{c1}{rc1}\ncline{c1}{lc1}
  \ncline{r1}{lr1}\ncline{r1}{rr1}
  \ncline{rc1}{e1}
\ncline{root2}{l2}\ncline{root2}{c2}\ncline{root2}{r2}
  \ncline{l2}{ll2}\ncline{c2}{rc2}\ncline{c2}{lc2}
  \ncline{r2}{lr2}\ncline{r2}{rr2}
  \ncline{rc2}{e2}
  \end{psmatrix}
   \caption{Computing the deep occurrences of $P$ into $T$ depicted in (a) and (b)
   respectively. The nodes in $P$ are numbered $1$--$4$ for easy reference.
   (c) Case 1 of $\Emb$: The set $\Emb(3)$.
   Since $3$ and $4$ are leaves and $\lab(3) = \lab(4)$ we have $\Emb(3) = \Emb(4)$. (d)
   Case 2 of $\Emb$. The set $\Emb(2)$.
   Note that the middle child of the root of $T$ is not in the set since it is not a
   deep occurrence. (e) Case 3 of $\Emb$: The two minimal ordered pairs of
   (d) and (c). (f) The nearest common ancestors of both pairs in (e) give
   the root node of $T$ which is the only (deep) occurrence of $P$.}
  \label{t2:fig:embexample}
\end{center}
\end{figure}
\begin{lemma}
For trees $P$ and $T$ and node $v\in V(P)$, $\Emb(v)$ computes the set of
deep occurrences of $P(v)$ in $T$.
\end{lemma}
\begin{proof}
By induction on the size of the subtree $P(v)$. If $v$ is a leaf
we immediately have that $\emb(v,T) = \Deep(\Fl(L(T),
\lab(v)))$ and thus case 1 follows. Suppose that $v$ is an
internal node with $k\geq1$ children $v_1, \ldots, v_k$. We show
that $\emb(P(v),T) = \Emb(v)$. Consider cases 2 and 3 of the
algorithm.

If $k=1$ we have that $w\in \Emb(v)$ implies that $\lab(w) =
\lab(v)$ and there is a node $w_1 \in \Emb(v_1)$ such that
$\fl(\parent(w_1), \lab(v)) = w$, that is, no node on the path
between $w_1$ and $w$ is labeled $\lab(v)$.  By induction
$\Emb(v_1) = \emb(P(v_1),T)$ and therefore $w$ is the root of an
embedding of $P(v)$ in $T$. Since $\Emb(v)$ is the deep set of
all such nodes it follows that $w \in \emb(P(v),T)$. Conversely,
if $w \in \emb(P(v),T)$ then $\lab(w) = \lab(v)$, there is a node
$w_1 \in \emb(P(v_1),T)$ such that $w \prec w_1$, and no node on
the path between $w$ and $w_1$ is labeled $\lab(v)$, that is,
$\fl(w_1, \lab(v)) = w$. Hence, $w \in \Emb(v)$.

Before considering case 3 we first show that $U_j =
\mop(\Emb(v_1), \ldots, \Emb(v_j))$ by induction on $j$, $2
\leq j \leq k$. For $j=2$ it follows from the definition of
$\Mop$ that $U_2 = \mop(\Emb(v_1), \Emb(v_2))$. Hence,
assume that $j > 2$. We have $U_j = \Mop(U_{j-1}, \Emb(v_j)) =
\Mop(\mop(\Emb(v_1), \ldots, \Emb(v_{j-1})), R_j)$. By
definition of $\Mop$, $U_j$ is the set of pairs such that for
any pair  $(r_1, r_{j-1}) \in \mop(\Emb(v_1), \ldots,
\Emb(v_{j-1}))$, $(r_1, r_j) \in U_j$ iff $(r_{j-1}, r_j) \in
\mop(\restrict{\mop(\Emb(v_1), \ldots, \Emb(v_{j-1}))}{2},
R_j)$. By Lemma~\ref{t2:lem:nnsmaller2} it follows that $(r_1, r_j)
\in U_j$ iff $(r_1, r_j) \in \mop(\Emb(v_1), \ldots,
\Emb(v_j))$.

Next consider the case when $k>1$. If $w \in \Emb(v)$ we have
that $\lab(w) = \lab(v)$ and there are nodes $(w_1, w_k) \in
\mop(\emb(P(v_1),T), \ldots, \emb(P(v_k),T))$ such that
$w=\fl(\nca(w_1, w_k), \lab(v))$. Clearly, $w$ is the root of an
embedding of $P(v)$ in $T$. Assume for contradiction that $w$ is
not a deep embedding, that is, $w \prec u$ for some node $u \in
\emb(P(v),T)$. Since $w=\fl(\nca(w_1, w_k), \lab(v))$ there must
be nodes $u_1 \lhd \cdots \lhd u_k$, such that $u_i \in
\emb(P(v_i),T)$ and $u=\fl(\nca(u_1, u_k), \lab(v))$. However,
this contradicts the fact that $(w_1, w_k) \in
\mop(\emb(P(v_1),T), \ldots, \emb(P(v_k),T))$. If $w \in
\emb(P(v),T)$ a similar argument implies that $w\in \Emb(v)$.
\qed \end{proof}
The set $L(T)$ is deep and in all tree cases of $\Emb(V)$ the returned set is also deep. By induction it follows that the input to $\Parent$, $\Fl$, $\Nca$, and $\Mop$ is always deep. We will use this fact to our advantage in the following algorithms.

\section{A Simple Tree Inclusion Algorithm}\label{t2:simple}
In this section we a present a simple implementation of the set
procedures which leads to an efficient tree inclusion algorithm.
Subsequently, we modify one of the procedures to obtain a family
of tree inclusion algorithms where the complexities depend on the
solution to a well-studied problem known as the \emph{tree color
problem}.

\subsection{Preprocessing}\label{t2:sec:simplepreprocessing}
To compute deep embeddings we require a data structure
for $T$ which allows us, for any $v,w \in V(T)$, to compute
$\nca_T(v,w)$ and determine if $v \prec w$ or $v \lhd w$. In
linear time we can compute $\pre(v)$ and $\post(v)$ for all nodes
$v \in V(T)$, and with these it is straightforward to test the two
conditions. Furthermore,
\begin{lemma}[Harel and Tarjan~\cite{HT1984}]
For any tree $T$ there is a data structure using $O(n_T)$ space
and preprocessing time which supports nearest common ancestor
queries in $O(1)$ time.
\end{lemma}
Hence, our data structure uses linear preprocessing time and space (see also~\cite{BFC2000,AGKR2004} for more recent nearest common ancestor data structures). 

\subsection{Implementation of the Set Procedures}\label{t2:implementationsimple}
To answer tree inclusion queries we give an efficient
implementation of the set procedures. The idea is to represent
sets of nodes and sets of pairs of nodes in a left-to-right order
using linked lists. For this purpose we introduce some helpful
notation. Let $X = [x_1, \ldots, x_k]$ be a linked list of nodes.
The \emph{length} of $X$, denoted $|X|$, is the number of elements
in $X$ and the list with no elements is written $[]$. The $i$th
node of $X$, denoted $X[i]$, is $x_i$. Given any node $y$ the list
obtained by \emph{appending} $y$ to $X$, is the list $X \circ y =
[x_1, \ldots, x_k, y]$. If for all $i$, $1 \leq i \leq |X|-1$,
$X[i] \lhd X[i+1]$ then $X$ is \emph{ordered} and if $X[i] \unlhd
X[i+1]$ then $X$ is \emph{semiordered}. A list $Y = [(x_1, z_k),
\ldots, (x_k, z_k)]$ is a \emph{node pair list}. By analogy, we
define length, append, etc. for $Y$. For a pair $Y[i] = (x_i,
z_i)$ define $Y[i]_1 = x_i$ and $Y[i]_2 = z_i$. If the lists
$[Y[1]_1, \ldots, Y[k]_1]$ and $[Y[1]_2, \ldots, Y[k]_2]$ are both
ordered or semiordered then $Y$ is \emph{ordered} or
\emph{semiordered}, respectively.

The set procedures are implemented using node lists. All lists
used in the procedures are either ordered or semiordered. As noted
in Section~\ref{t2:sec:recursion} we may assume that the input to all
of the procedures, except $\Deep$, represent a deep set, that is,
the corresponding node list or node pair list is ordered. We
assume that the input list given to $\Deep$ is semiordered and the
output, of course, is ordered. Hence, the output of all the other
set procedures must be semiordered. In the following let $X$ be a
node list, $Y$ a node pair list, and $\alpha$ a character in
$\Sigma$. The detailed implementation of the set procedures is
given below. We show the correctness in
Section~\ref{t2:sec:simplecorrectness} and discuss the complexity in
Section~\ref{t2:sec:simplecomplexity}.
\begin{relate}[simple]
\item[$\Parent(X)$:] Return the list $[\parent(X[1]), \ldots,
\parent(X[\norm{X}])]$.

\item[$\Nca(Y)$:] Return the list $[\nca(Y[1]), \ldots, \nca(Y[\norm{Y}])]$.

\item[$\Deep(X)$:] Initially, set $x := X[1]$ and $R := []$.

For $i:=2$ to $|X|$ do:
\begin{itemize}
\item[] Compare $x$ and $X[i]$. There are three cases:
    \begin{enumerate}
    \item $x \lhd X[i]$. Set $R:= R \circ x$ and $x:= X[i]$.

    \item $x \prec X[i]$. Set $x := X[i]$.  

    \item $X[i] \prec x$. Do nothing.
    \end{enumerate}
\end{itemize}
Return $R \circ x$.
\end{relate}
The implementation of procedure \Deep\ takes advantage of the fact
that the input list is semiordered. In case 1 node $X[i]$ to the
right of our "potential output node" $x$. Since any node that is a
descendant of $x$ must be to the right of $X[i]$ it cannot not
appear later in the list $X$ than $X[i]$. We can thus safely add
$x$ to $R$ at this point. In case 2 node $x$ is an ancestor of
$X[i]$ and can thus not be in the output list. In case 3 node
$X[i]$ is an ancestor of $x$ and can thus not be in the output
list.
\begin{relate}[simple]
\item[$\Mop(Y,X)$:]
    Initially, set $R:=[]$.

    Find the smallest $j$ such that $Y[1]_2 \lhd X[j]$ and set
    $y:=Y[1]_1$, $x:= X[j]$, and $h:=j$. If no such $j$ exists stop.

For $i :=2$ to $\norm{Y}$ do:
    \begin{itemize}
    \item[] Set $h:=h+1$ until $Y[i]_2 \lhd X[h]$ or $h>\norm{X}$.


    If $h> \norm{X}$
    stop and return $R := R \circ (y,x)$.
    Otherwise, compare $X[h]$ and $x$.
    There are two cases:
    \begin{enumerate}
    \item If $x \lhd X[h]$ set
    $R:=R\circ (y,x)$, $y:=Y[i]_1$, and $x:=X[h]$.
    \item If $x=X[h]$
    set $y:=Y[i]_1$.
    \end{enumerate}
    \end{itemize}
    Return $R := R \circ (y,x)$.
\end{relate}
In procedure \Mop\ we have a "potential pair" $(y,x)$ where
$y=Y[i]_1$ for some $i$ and $Y[i]_2 \lhd x$. Let $j$ be the index
such that $y=Y[j]_1$. In case 1 we have $x \lhd X[h]$ and also
$Y[j]_2 \lhd Y[i]_2$ since the input lists are ordered (see
Figure~\ref{t2:fig:mopimplexample}(a)). Therefore, $(y,x)$ is
inserted into $R$. In case 2 we have $x=X[h]$, i.e., $Y[i]_2\lhd
x$, and as before $Y[j]_2 \lhd Y[i]_2$ (see
Figure~\ref{t2:fig:mopimplexample}(b)). Therefore $(y,x)$ cannot be
in the output, and we set $(Y[i]_1,x)$ to be the new potential
pair.
\begin{figure}[t]
\begin{center}
\begin{psmatrix}[colsep=0.7cm,rowsep=0.3cm,labelsep=2pt]
&&&& \cnode{.1}{root}\rput(0,.3){}
  \\
& \cnode{.1}{l}\rput(0,.3){} &&&  \cnode{.1}{c}\rput(-.3,.3){}
  &&
  \cnode{.1}{r}\rput(0,.3){} \\
\cnode{.1}{ll}&
\cnode*{.1}{lc}\rput(0,-.3){$Y[j]_2$} &
  \cnode{.1}{lr}\rput(-.2,-.3){} &  \cnode*{.1}{cl}\rput(.6,0){$Y[i]_2$}
  & &
  \cnode{.1}{cr}& \cnode{.1}{rl}
  &
  \psset{fillstyle=crosshatch}\cnode{.1}{rr}\rput(0,-.3){$X[h]$}&&&
     \\
&& \cnode{.1}{lrc}& \psset{fillstyle=crosshatch}\cnode{.1}{cll}\rput(0,-.3){$x$}
  & \cnode{.1}{clr}&&
  \cnode{.1}{rlc}\rput(0,-.3){}\\[.2cm]
&&&&\rput(0,0){(a)}
\ncline{root}{l}\ncline{root}{c}\ncline{root}{r}
  \ncline{l}{ll}\ncline{l}{lc}\ncline{l}{lr}
  \ncline{c}{cr}\ncline{c}{cl}
  \ncline{r}{rl}\ncline{r}{rr}
  \ncline{lr}{lcr}\ncline{cl}{cll}\ncline{cl}{clr}
  \ncline{rl}{rlc}\ncline{lr}{lrc}
\end{psmatrix}
\begin{psmatrix}[colsep=0.7cm,rowsep=0.3cm,labelsep=1pt]
&&&& \cnode{.1}{root}
  \\
& \cnode{.1}{l} &&&  \cnode{.1}{c}
  &&
  \cnode{.1}{r} \\
\cnode{.1}{ll} &
  \cnode*{.1}{lc}\rput(0,-.3){$Y[j]_2$} &
  \cnode{.1}{lr} &  \cnode{.1}{cl}
  &&
  \cnode{.1}{cr} &
  \cnode{.1}{rl}
  &
  \cnode{.1}{rr}
     \\
&& \cnode{.1}{lrc}
  & \cnode{.1}{cll}
  & \cnode*{.1}{clr}\rput(0,-.3){$Y[i]_2$}&&
  \psset{fillstyle=crosshatch}\cnode{.1}{rlc}\rput(.3,-.3){$x=X[h]$}\\[.2cm]
&&&&\rput(0,0){(b)}
\ncline{root}{l}\ncline{root}{c}\ncline{root}{r}
  \ncline{l}{ll}\ncline{l}{lc}\ncline{l}{lr}
  \ncline{c}{cr}\ncline{c}{cl}
  \ncline{r}{rl}\ncline{r}{rr}
  \ncline{lr}{lcr}\ncline{cl}{cll}\ncline{cl}{clr}
  \ncline{rl}{rlc}\ncline{lr}{lrc}
\end{psmatrix}
\caption{Case 1 and 2 from the implementation of \Mop.
     In (a) we have $Y[i]_2 \ntriangleleft x$.
     In  (b) we have $Y[j]_2 \lhd Y[i]_2 \lhd x = X[h]$.
      }
  \label{t2:fig:mopimplexample}
\end{center}
\end{figure}
\begin{relate}[simple]
\item[$\Fl(X,\alpha)$:]
Initially, set $Z:=X$, $R:=[]$, and
    $S:=[]$.

    Repeat until $Z:=[]$:
    \begin{itemize}
    \item[] For $i:=1$ to $\norm{Z}$ do: If
    $\lab(Z[i])=\alpha$ set $R:=\Insertt(Z[i],R)$. Otherwise set $S:=S
    \circ \parent(Z[i])$.

    \item[] Set $S:=\Deep(S)$, $W:=\Deep^*(S,R)$, and $S:=[]$.
    \end{itemize}
    Return $R$.
\end{relate}
The procedure $\Fl$ calls two auxiliary procedures: $\Insertt(x,R)$
that takes an ordered list $R$ and insert the node $x$ such that
the resulting list is ordered, and $\Deep^*(S,R)$ that takes two
ordered lists and returns the ordered list representing the set
$\Deep(S \cup R) \cap S$, i.e., $\Deep^*(S,R)=[s\in S|\nexists
z\in R:s\prec z]$. Below we describe in more detail how to
implement \Fl\ together with the auxiliary procedures.

We use one doubly linked list to represent all the lists $Z$, $S$,
and $R$. For each element in $Z$ we have pointers \Pred\ and
\Successor\ pointing to the predecessor and successor in the list,
respectively. We also have  at each element a pointer \Next\
pointing to the next element in $Z$. In the beginning
$\Next=\Successor$ for all elements, since all elements in the list are
in $Z$. When going through $Z$ in one iteration we simple follow
the \Next\ pointers. When \Fl\ calls $\Insertt(Z[i],R)$ we set
$\Next(\Pred(Z[i]))$ to $\Next(Z[i])$. That is, all nodes in the
list not in $Z$, i.e., nodes not having a \Next\ pointer pointing
to them, are in $R$. We do not explicitly maintain $S$. Instead we
just set save $\Parent(Z[i])$ at the position in the list instead
of $Z[i]$. Now $\Deep(S)$ can be performed following the \Next\
pointers and removing elements from the doubly linked list
accordingly to procedure \Deep. It remains to show how to
calculate $\Deep^*(S,R)$. This can be done by running through $S$
following the \Next\ pointers. At each node $s$ compare $\Pred(s)$
and $\Successor(s)$ with $s$. If one of them is a descendant of $s$
remove $s$ from the doubly linked list.

Using this linked list implementation $\Deep^*(S,R)$ takes time
$O(\norm{S})$, whereas using \Deep\ to calculate this would have
used time $O(\norm{S}+\norm{R})$.


\subsection{Correctness of the Set Procedures}\label{t2:sec:simplecorrectness}
Clearly, \Parent\ and \Nca\ are correct. The following lemmas show
that $\Deep$, $\Fl$, and $\Mop$ are also correctly implemented.
For notational convenience we write $x\in X$, for a list $X$, if
$x = X[i]$ for some $i$, $1 \leq i \leq \norm{X}$.
\begin{lemma}\label{t2:lem:deep}
    Procedure $\Deep(X)$ is correct.
\end{lemma}

\begin{proof} Let $y$ be an element in $X$.
    We will first prove that if there are no descendants of $y$ in $X$, i.e.,
    $X \cap V(T(y))=\emptyset$, then $y\in R$. Since
    $X \cap V(T(y))=\emptyset$ we must at some point during the procedure have
    $x=y$, and $x$ will not change before $x$ is added to $R$. If
    $y$ occurs several times in $X$ we will have $x=y$ each time
    we meet a copy of $y$ (except the first) and it follows from
    the implementation that $y$ will occur exactly once in $R$.

    We will now prove that if there are any descendants of $y$ in $V$, i.e.,
    $X \cap V(T(y))\neq \emptyset$, then $y \not \in
    R$. Let $z$ be the rightmost and deepest descendant of $y$ in $V$. There
    are two cases:
    \begin{enumerate}
    \item $y$ is before $z$ in $X$.
        Look at the time in the execution of the procedure when we
        look at $z$. There are two cases.
        \begin{enumerate}
        \item $x=y$. Since $y \prec z$ we set $x=z$ and proceed. It
            follows that $y \not \in R$.

        \item $x = x' \neq y$. Since any node to the left of $y$ also is to
            the left of $z$ and $X$ is an semiordered list we must have
            $x' \in V(T(y))$ and thus $y \not \in R$.
        \end{enumerate}

    \item $y$ is after $z$ in $X$. Since $z$ is the rightmost and
        deepest descendant of  $y$ and $V$ is semiordered we must have
        $x=z$ at the time in the procedure where we look at $y$. Therefore
        $y \not \in R$.
    \end{enumerate}
    If $y$ occurs several times in $X$, each copy will be taken
    care of by either case 1 or 2.
\qed \end{proof}

\begin{lemma}\label{t2:lem:nnm}
    Procedure $\Mop(Y,X)$ is correct.
\end{lemma}
\begin{proof}
    We want to show that for any $1\leq l < \norm{Y}$, $1 \leq k <
    \norm{X}$ the pair
    $(Y[l]_1,X[k])$ is in  $R$ if and only if $(Y[l]_2,X[k])
    \in \mop(\restrict{Y}{2},X)$.
    Since $\restrict{Y}{2}$ and $X$ are ordered lists we have
    \begin{equation*}(Y[l]_2,X[k]) \in \mop(X|_2,X) \quad \Leftrightarrow \quad
    X[k-1] \unlhd Y[l]_2 \lhd X[k] \unlhd Y[l+1]_2 \;,
    \end{equation*}
    for $k \geq 2$, and
    $$(Y[l]_2,X[1]) \in \mop(X|_2,X) \quad \Leftrightarrow \quad
    Y[l]_2 \lhd X[1] \unlhd Y[l+1]_2 \;,$$
    when $k=1$.

    It follows immediately from the implementation of the procedure, that
    if $Y[j]_2 \lhd X[t]$, $X[t-1] \unlhd Y[j]_2$,
    and $Y[j+1]_2 \unrhd
    X[t]$ then $(Y[j]_1, X[t]) \in R$.

    We will now show that $(Y[l]_1,X[k]) \in R \Rightarrow (Y[l]_2,X[k]) \in
    \mop(\restrict{Y}{2},X)$. That $(Y[l]_1,X[k]) \in R \Rightarrow
    X[k-1] \unlhd Y[l]_2 \lhd X[k]$ follows immediately from the
        implementation of the procedure by induction on $l$.
















It remains to show that $(Y[l]_1,X[k]) \in R
        \Rightarrow  X[k] \unlhd Y[l+1]_2$. Assume for the
        sake of contradiction that $Y[l+1]_2 \lhd X[k]$.
        Consider
        the iteration in the execution of the procedure
        when we look at $Y[l+1]_2$. We have
        $x=X[k]$ and thus set $y:=Y[l+1]_1$
        contradicting $(Y[l]_1,X[k]) \in R$.
\qed \end{proof}
To show that \Fl\ is correct we need the following proposition.
\begin{prop}\label{t2:prop:deeplist}
Let $X$ be an ordered list and let $x$ be an ancestor of $X[i]$
for some $i \in \{1,\ldots,k\}$. If $x$ is an ancestor of some
node in $X$ other than $X[i]$ then $x$ is an ancestor of $X[i-1]$
or $X[i+1]$.
\end{prop}
\begin{proof}
Assume for the sake of contradiction that $x \npreceq X[i-1]$, $x
\npreceq X[i+1]$, and $x \preceq z$, where $z \in X$ and $z \neq
X[i]$. Since $X$ is ordered either $z \lhd X[i-1]$ or $X[i+1] \lhd
z$. Assume $z \lhd X[i-1]$. Since $x \prec X[i]$, $x \npreceq
X[i-1]$, and $X[i-1]$ is to the left of $X[i]$, $X[i-1]$ is to the
left of $x$. Since $z \lhd X[i-1]$ and $X[i-1]\lhd x$ we have $z
\lhd x$ contradicting $x \prec z$. Assume $X[i+1] \lhd z$. Since
$x \prec X[i]$, $x \npreceq X[i+1]$, and $X[i+1]$ is to the right
of $X[i]$, $X[i+1]$ is to the right of $x$. Thus $x \lhd z$
contradicting $x \prec z$.
\qed \end{proof}
Proposition~\ref{t2:prop:deeplist} shows that the doubly linked list
implementation of $\Deep^*$ is correct. Clearly, \Insertt\ is
implemented correct by the doubly linked list representation,
since the nodes in the list remains in the same order throughout
the execution of the procedure.
\begin{lemma}\label{t2:lem:flcorrect}
    Procedure $\Fl(X,\alpha)$ is correct.
\end{lemma}
\begin{proof}
    Let $F=\{\fl(x,\alpha)\mid x \in X\}$.
    It follows immediately from the implementation of the
    procedure  that $\Fl(X,\alpha) \subseteq X$.
    It remains to show that $\Deep(F)\subseteq \Fl(X,\alpha)$.
    Let $x$ be a
    node in $\Deep(F))$, let $z \in X$ be the node such that
    $x=\fl(z,\alpha)$, and let $z=x_1,x_2,\ldots,x_k=x$
    be the nodes on the
    path from $z$ to $x$. In each iteration of the algorithm we have $x_i \in Z$
    for some $i$ unless $x \in R$.
\qed \end{proof}

\subsection{Complexity of the Set Procedures}\label{t2:sec:simplecomplexity}
For the running time of the node list implementation observe that,
given the data structure described in
Section~\ref{t2:sec:simplepreprocessing}, all set procedures, except
$\Fl$, perform a single pass over the input using constant time at
each step. Hence we have,
\begin{lemma}\label{t2:lem:auxprocedures}
For any tree $T$ there is a data structure using $O(n_T)$ space
and preprocessing which supports each of the procedures $\Parent$,
$\Deep$, $\Mop$, and $\Nca$ in linear time (in the size of their
input).
\end{lemma}
The running time of a single call to \Fl\ might take time
$O(n_T)$. Instead we will divide the calls to \Fl\ into groups and
analyze the total time used on such a group of calls. The
intuition behind the division is that for a path in $P$ the calls
made to \Fl\ by \Emb\ is done bottom up on disjoint lists of
nodes in $T$.


\begin{lemma}\label{t2:lem:fl}
For disjoint ordered node lists $X_1, \ldots, X_k$ and labels
$\alpha_1, \ldots, \alpha_k$, such that any node in $X_{i+1}$ is
an ancestor of some node in $\Deep(\Fl_T(X_i, \alpha_i))$, $2 \leq
i < k$, all of $\Fl_T(X_1, \alpha_1), \ldots , \Fl_T(X_k,
\alpha_k)$ can be computed in $O(n_T)$ time.
\end{lemma}
\begin{proof}
Let $Z$, $R$, and $S$ be as in the implementation of the
procedure. Since \Deep\ and $\Deep^*$ takes time $O(\norm{S})$, we
only need to show that the total length of the lists $S$---summed
over all the calls---is $O(n_T)$ to analyze the total time usage
of \Deep\ and $\Deep^*$. We note that in one iteration $\norm{S}
\leq \norm{Z}$. \Insertt\ takes constant time and it is thus enough
to show that any node in $T$ can be in $Z$ at most twice during
all calls to \Fl.


Consider a call to \Fl. Note that $Z$ is ordered at all times.
Except for the first iteration, a node can be in $Z$ only if one
of its children were in $Z$ in the last iteration. Thus in one
call to \Fl\ a node can be in $Z$ only once.

Look at a node $z$ the first time it appears in $Z$. Assume that
this is in the call $\Fl(X_i,\alpha_i)$. If $z \in X$ then $z$
cannot be in $Z$ in any later calls, since no node in $X_j$ where
$j>i$ can be a descendant of a node in $X_i$. If $z \not \in R$ in
this call then $z$ cannot be in $Z$ in any later calls. To see
this look at the time when $z$ removed from $Z$. Since the set $Z
\cup R$ is deep at all times no descendant of $z$ will appear in
$Z$ later in this call to \Fl, and no node in $R$ can be a
descendant of $z$. Since any node in $X_j$, $j>i$, is an ancestor
of some node in $\Deep(\Fl(X_i,\alpha_i))$ neither $z$ or any
descendant of $z$ can be in any $X_j$, $j>i$. Thus $z$ cannot
appear in $Z$ in any later calls to \Fl. Now if $z \in R$ then we
might have $z \in X_{i+1}$. In that case, $z$ will appear in $Z$
in the first iteration of the procedure call
$\Fl(X_{i+1},\alpha_i)$, but not in any later calls since the
lists are disjoint, and since no node in $X_j$ where $j>i+1$ can
be a descendant of a node in $X_{i+1}$. If $z \in R$ and $z \not
\in X_{i+1}$ then clearly $z$ cannot appear in $Z$ in any later
call. Thus a node in $T$ is in $Z$ at most twice during all the
calls.
\qed \end{proof}


\subsection{Complexity of the Tree Inclusion Algorithm}
Using the node list implementation of the set procedures we get:

\begin{theorem}\label{t2:thm:simple}
For trees $P$ and $T$ the tree inclusion problem can be solved in
$O(l_Pn_T)$ time and $O(n_T)$  space.
\end{theorem}
\begin{proof}
By Lemma~\ref{t2:lem:auxprocedures} we can preprocess $T$ in $O(n_T)$
time and space. Let $g(n)$ denote the time used by $\Fl$ on a
list of length $n$. Consider the time used by $\Emb(\roots(P))$.
We bound the contribution for each node $v\in V(P)$. From Lemma
\ref{t2:lem:auxprocedures} it follows that if $v$ is a leaf the cost
of $v$ is at most $O(g(l_T))$. Hence, by
Lemma~\ref{t2:lem:fl}, the total cost of all leaves is $O(l_Pg(l_T)) = O(l_P  n_T)$.
If $v$ has a single child $w$ the cost is $O(g(|\Emb(w)|))$. If
$v$ has more than one child the cost of $\Mop$, $\Nca$, and $\Deep$ is bounded by $\sum_{w \in \child(v)} O(|\Emb(w)|)$. Furthermore, since the length of the output of
$\Mop$ (and thus $\Nca$) is at most $z = \min_{w \in \child(v)}
|\Emb(w)|$ the cost of $\Fl$ is $O(g(z))$. Hence, the total cost
for internal nodes is,
\begin{equation}
\label{t2:eq:internal}  \sum_{v \in V(P)\backslash L(P)}
O\bigg(g(\min_{w \in \child(v)} |\Emb(w)|) + \sum_{w \in
\child(v)} |\Emb(w)|\bigg)\leq \sum_{v \in V(P)}
O(g(|\Emb(v)|)).
\end{equation}
Next we bound (\ref{t2:eq:internal}). For any $w \in \child(v)$ we have that $\Emb(w)$ and $\Emb(v)$ are disjoint ordered lists. Furthermore we have that any node in $\Emb(v)$ must be an ancestor of a node in $\Deep(\Fl(\Emb(w), \lab(v)))$. Hence, by Lemma~\ref{t2:lem:fl}, for any leaf to root path $\delta = v_1, \ldots, v_k$ in $P$, we have that $\sum_{u \in \delta} g(|\Emb(u)|) \leq O(n_T)$. Let
$\Delta$ denote the set of all root to leaf paths in $P$. It
follows that,
\begin{equation*}
\sum_{v \in V(T)} g(|\Emb(v)|) \leq \sum_{p \in \Delta} \sum_{u
\in p} g(|\Emb(u)|) \leq O(l_Pn_T).
\end{equation*}
Since this time dominates the time spent at the leaves the time
bound follows. Next consider the space used by
$\Emb(\roots(P))$. The preprocessing of
Section~\ref{t2:sec:simplepreprocessing} uses only $O(n_T)$ space.
Furthermore, by induction on the size of the subtree $P(v)$ it
follows immediately that at each step in the algorithm at most
$O(\max_{v\in V(P)}|\Emb(v)|)$ space is needed. Since
$\Emb(v)$ is a deep embedding, it follows that $|\Emb(v)| \leq
l_T$.
\qed \end{proof}

\subsection{An Alternative Algorithm}\label{t2:sec:alt}
In this section we present an alternative algorithm. Since the
time complexity of the algorithm in the previous section is
dominated by the time used by $\Fl$, we present an implementation
of this procedure which leads to a different complexity. Define a
\emph{firstlabel data structure} as a data structure supporting
queries of the form $\fl(v, \alpha)$, $v\in V(T)$, $\alpha \in
\Sigma$. Maintaining such a data structure is known as the
\emph{tree color problem}. This is a well-studied problem, see
e.g. \cite{Die89,MM1996, FM1996,AHR1998}. With such a data
structure available we can compute $\Fl$ as follows,
\begin{relate}
\item[$\Fl(X, \alpha)$:] Return the list $R := [\fl(X[1], \alpha),
\ldots, \fl(X[\norm{X}], \alpha)]$.
\end{relate}
\begin{theorem}\label{t2:thm:simple2}
Let $P$ and $T$ be trees. Given a firstlabel data structure using
$s(n_T)$ space, $p(n_T)$ preprocessing time, and $q(n_T)$ time for
queries, the tree inclusion problem can be solved in $O(p(n_T) +
n_Pl_T\cdot q(n_T))$ time and $O(s(n_T) + n_T)$ space.
\end{theorem}
\begin{proof}
Constructing the firstlabel data structures uses $O(s(n_T))$ and
$O(p(n_T))$ time. As in the proof of Theorem~\ref{t2:thm:simple} we
have that the total time used by $\Emb(\roots(P))$ is bounded by
$\sum_{v \in V(P)} g(|\Emb(v)|)$, where $g(n)$ is the time used
by $\Fl$ on a list of length $n$. Since $\Emb(v)$ is a deep
embedding and each $\fl$ takes $q(n_T)$ we have,
\begin{equation*}
\sum_{v \in V(P)} g(|\Emb(v)|) \leq \sum_{v \in V(P)} g(l_T) =
n_P l_T\cdot q(n_T).
\end{equation*}
\qed \end{proof}
Several firstlabel data structures are available, for instance, if we want to maintain linear space we have,
\begin{lemma}[Dietz~\cite{Die89}]\label{t2:lem:dietz}
For any tree $T$ there is a data structure using $O(n_T)$ space,
$O(n_T)$ expected preprocessing time which supports firstlabel
queries in $O(\log \log n_T)$ time.
\end{lemma}
The expectation in the preprocessing time is due to perfect hashing. Since our data structure does not need to support efficient updates we can remove the expectation by using the deterministic dictionary of Hagerup et. al. \cite{HMP2001}. This gives a worst-case preprocessing time of $O(n_T \log n_T)$, however, using a simple two-level approach this can be reduced to $O(n_T)$ (see e.g. \cite{Thorup2003}). Plugging in this data structure we obtain,
\begin{corollary}\label{t2:cor:simple}
For trees $P$ and $T$ the tree inclusion problem can be solved in
$O(n_Pl_T\log\log n_T + n_T)$ time and $O(n_T)$ space.
\end{corollary}

\section{A Faster Tree Inclusion Algorithm}\label{t2:micromacro}
In this section we present a new tree inclusion algorithm which
has a worst-case subquadratic running time. As discussed in the introduction the general idea is to divide $T$ into clusters of logarithmic size which we can efficiently preprocess and then use this to speedup the computation with a logarithmic factor.

\subsection{Clustering}
In this section we describe how to divide $T$ into clusters and
how the macro tree is created. For simplicity in the presentation
we assume that $T$ is a binary tree. If this is not the case it is
straightforward to construct a binary tree $B$, where $n_{B} \leq
2n_T$, and a mapping $g : V(T) \rightarrow V(B)$ such that for any
pair of nodes $v,w \in V(T)$, $\lab(v) = \lab(g(v))$, $v \prec w$
iff $g(v) \prec g(w)$, and $v \lhd w$ iff $g(v) \lhd g(w)$. If the
nodes in the set $U = V(B)\backslash \{g(v) \mid v \in V(T)\}$ is
assigned a special label $\beta \not\in \Sigma$ it follows that
for any tree $P$, $P \sqsubseteq T$ iff $P \sqsubseteq B$.


Let $C$ be a connected subgraph of $T$. A node in $V(C)$ incident
to a node in $V(T)\backslash V(C)$ is a \emph{boundary} node. The
boundary nodes of $C$ are denoted by $\delta C$. A \emph{cluster}
of $C$ is a connected subgraph of $C$ with at most two boundary
nodes. A set of clusters $CS$ is a \emph{cluster partition} of $T$
iff $V(T) = \cup_{C\in CS} V(C)$, $E(T) = \cup_{C\in CS} E(C)$,
and for any $C_1 ,C_2 \in CS$, $E(C_1) \cap E(C_2) = \emptyset$,
$|E(C_1)| \geq 1$, $\roots(T) \in \delta C$ if $\roots(T) \in
V(C)$. If $|\delta C| = 1$ we call $C$ a \emph{leaf cluster} and
otherwise an \emph{internal cluster}.


We use the following recursive procedure $\Cluster_T(v,s)$,
adopted from \cite{AR2002c}, which creates a cluster partition
$CS$ of the tree $T(v)$ with the property that $|CS| = O(s)$ and
$|V(C)| \leq \ceil{n_T/s}$. A similar cluster partitioning
achieving the same result follows from \cite{AHT2000, AHLT1997,
Frederickson1997}.
\begin{relate}
\item[$\Cluster_T(v,s)$:] For each child $u$ of $v$ there are two
cases:
\begin{enumerate}
  \item $|V(T(u))| + 1 \leq \ceil{n_T/s}$. Let the nodes $\{v\} \cup V(T(u))$ be a leaf cluster with boundary node $v$.
  \item $|V(T(u))| > \ceil{n_T/s}$. Pick a node $w \in V(T(u))$ of
  maximum depth such that $|V(T(u))| + 2 - |V(T(w))| \leq \ceil{n_T/s}$.
  Let the nodes $V(T(u)) \backslash V(T(w)) \cup \{v,w\}$ be an internal
  cluster with boundary nodes $v$ and $w$. Recursively, compute $\Cluster_T(w, s)$.
\end{enumerate}
\end{relate}

\begin{lemma}\label{t2:lem:clustering}
Given a tree $T$ with $n_T>1$ nodes, and a parameter $s$, where
$\ceil{n_T/s} \geq 2$, we can build a cluster partition $CS$ in
$O(n_T)$ time, such that $|CS| = O(s)$ and $|V(C)| \leq
\ceil{n_T/s}$ for any $C \in CS$.
\end{lemma}
\begin{proof}
The procedure $\Cluster_T(\roots(T), s)$ clearly creates a cluster
partition of $T$ and it is straightforward to implement in
$O(n_T)$ time. Consider the size of the clusters created. There
are two cases for $u$. In case $1$, $|V(T(u))| + 1 \leq
\ceil{n_T/s}$ and hence the cluster $C = \{v\} \cup V(T(u))$
has size $|V(C)| \leq \ceil{n_T/s}$. In case $2$, $|V(T(u))| + 2 -
|V(T(w))| \leq \ceil{n_T/s}$ and hence the cluster $C = V(T(u))
\backslash V(T(w)) \cup \{v,w\}$ has size $|V(C)| \leq
\ceil{n_T/s}$.

Next consider the size of the cluster partition.  Let $c=
\ceil{n_T/s}$. We say that a cluster $C$ is \emph{bad} if
$\norm{V(C)} \leq c/2$ and \emph{good} otherwise. We will show
that at least a constant fraction of the clusters in the cluster
partition are good. It is easy to verify that the cluster
partition created by procedure \Cluster\ has the following
properties:
\begin{itemize}
\item[(i)] Let $C$ be a bad internal cluster with boundary nodes
$v$ and $w$ ($v \prec
w$). Then $w$ has two children with at least $c/2$ descendants
each.
\item[(ii)] Let $C$ be a bad leaf cluster with boundary node
$v$. Then the boundary node $v$ is contained in  a good cluster.
\end{itemize}
By (ii) the number of bad leaf clusters is no larger than twice
the number of good internal clusters. By (i) each bad internal
cluster $C$ is sharing its lowest boundary node of $C$ with two
other clusters, and each of these two clusters are either internal
clusters or good leaf clusters. This together with (ii) shows that
number of bad clusters is at most a constant fraction of the total
number of clusters. Since a good cluster is of size more than
$c/2$, there can be at most $2s$ good clusters and thus
$\norm{CS}=O(s)$.
\qed \end{proof}

Let $C\in CS$ be an internal cluster $v, w \in \delta C$. The
\emph{spine path} of $C$ is the path between $v,w$
excluding $v$ and $w$. A node on the spine path is a \emph{spine
node}. A node to the left and right of $v$, $w$, or any node on
the spine path is a \emph{left node} and \emph{right node}, respectively.
If $C$ is a leaf cluster with $v \in \delta C$ then any proper
descendant of $v$ is a \emph{leaf node}.

\begin{figure}[t]
\begin{center}
\begin{psmatrix}[colsep=0.6cm,rowsep=0.3cm,labelsep=1pt]
&&& \psellipse(0,0)(.2,.2)\cnode*[fillcolor=black]{.1}{root1}\rput(0,.4){$v$} &&&&&&&&&
  \cnode*[fillcolor=black]{.1}{v}\rput(0,.4){$v$}\\
&& \cnode{.1}{a1} &  \cnode{.1}{a2} &&&&&& \\
& \cnode{.1}{b1} & \psellipse(-.3,0)(.6,1.2)\cnode{.1}{b2} & \cnode{.1}{b3}\psellipse(0,0)(.3,1.2) \psellipse(0,0)(2,1.45) & \psellipse(.3,-.3)(.6,.8)\cnode{.1}{b4}&&&&&&&& \cnode{.1}{s}\rput(.6,.3){$s(v,w)$} \\
& & \cnode{.1}{c1} & \cnode{.1}{c2} & \cnode{.1}{c3} & \cnode{.1}{c4} &&&&&
   \cnode{.1}{l}\rput(-.2,.4){$l(v,w)$} &&&& \cnode{.1}{r}\rput(.2,.4){$r(v,w)$}\\
& && \psellipse(0,0)(.2,.2)\cnode*[fillcolor=black]{.1}{d2}\rput(0,-.4){$w$} &&&&&&&&& \cnode*[fillcolor=black]{.1}{w}\rput(0,-.4){$w$}\\
&&& \rput(0,-.3){(a)} &&&&&&&&& \rput(0,-.3){(b)} \\\\
\ncline{root1}{a1}\ncline{root1}{a2}
  \ncline{a1}{b1}\ncline{a1}{b2} \ncline{a2}{b3}\ncline{a2}{b4}
  \ncline{b3}{c1}\ncline{b3}{c2} \ncline{b4}{c3}\ncline{b4}{c4}
  \ncline{c2}{d2}
\ncline{v}{s}\ncline{w}{s}\ncline{l}{s}\ncline{r}{s}
&&& \psellipse(0,0)(.2,.2)\cnode*[fillcolor=black]{.1}{root1}\rput(0,.4){$v$} &&&&&&&&&
  \cnode*[fillcolor=black]{.1}{v}\rput(0,.4){$v$}\\
&& \cnode{.1}{a1} &  \psellipse(0,-.3)(1,.8)\psellipse(0,-.3)(1.5,1.06)\cnode{.1}{a2} &&&&&& \\
&&\cnode{.1}{b2} & \cnode{.1}{b3} & \cnode{.1}{b4}&&&&&&&& \cnode{.1}{s}\rput(0,-.4){$l(v)$} \\
&&& \rput(0,-.4){(c)} &&&&&&&&& \rput(0,-.4){(d)} \\
\ncline{root1}{a1}\ncline{root1}{a2}
  \ncline{a1}{b2} \ncline{a2}{b3}\ncline{a2}{b4}
\ncline{v}{s}
  \end{psmatrix}
   \caption{The clustering and the macro tree. (a) An internal cluster. The black nodes are the
   boundary node and the internal ellipses correspond to the boundary nodes,
   the right and left nodes, and spine path. (b) The macro tree corresponding to
   the cluster in (a). (c) A leaf cluster. The internal ellipses are the boundary node
   and the leaf nodes. (d) The macro tree corresponding to the cluster in (c).}
  \label{t2:clusterexample}
\end{center}
\end{figure}


Let $CS$ be a cluster partition of $T$ as described in Lemma
\ref{t2:lem:clustering}. We define an ordered \emph{macro tree}
$M$. Our definition of $M$ may be viewed as an ''ordered''
version of the macro tree defined in \cite{AR2002c}. The node set $V(M)$ consists of the boundary nodes in $CS$. Additionally, for each internal cluster $C \in CS$, $v,w \in \delta C$, $v \prec w$, we have the nodes $s(v,w)$, $l(v,w)$ and $r(v,w)$ and edges $(v, s(v,w)), (s(v,w), w), (l(v,w),s(v,w))$, and $(r(v,w),s(v,w))$. The nodes are ordered such that $l(v,w) \lhd w \lhd r(v,w)$. For each leaf cluster $C$, $v \in \delta C$, we have the node $l(v)$ and edge $(l(v), v)$.  Since $\roots(T)$ is a boundary node $M$ is rooted at $\roots(T)$. Figure~\ref{t2:clusterexample} illustrates these definitions.

To each node $v \in V(T)$ we associate a unique macro node denoted
$c(v)$. Let $u \in V(C)$, where $C \in CS$.
\begin{equation*}
c(u) =
\begin{cases}
        u & \text{if $u$ is boundary node}, \\
        l(v) & \text{if $u$ is a leaf node and $v \in \delta C$}, \\
        s(v,w) & \text{if $u$ is a spine node, $v,w \in \delta C$, and $v\prec w$}, \\
    l(v,w) & \text{if $u$ is a left node, $v,w \in \delta C$, and $v\prec w$}, \\
    r(v,w) & \text{if $u$ is a right node, $v,w \in \delta C$, and $v\prec w$}.
\end{cases}
\end{equation*}

Conversely, for any macro node $i \in V(M)$ define the
\emph{micro forest}, denoted $C(i)$, as the induced
subgraph of $T$ of the set of nodes $\{v \mid v \in V(T), i =
c(v)\}$. We also assign a \emph{set} of labels to $i$ given by
$\lab(i) = \{\lab(v) \mid v \in V(C(i))\}$. If $i$ is spine node
or a boundary node the unique node in $V(C(i))$ of greatest depth
is denoted by $\first(i)$. Finally, for any set of nodes $\{i_1,
\ldots, i_k\} \subseteq V(M)$ we define $C(i_1, \ldots, i_k)$ as the induced subgraph of the set of nodes $V(C(i_1)) \cup \cdots \cup V(C(i_k))$.

The following propositions states useful properties of ancestors,
nearest common ancestor, and the left-to-right ordering in the
micro forests and in $T$. The propositions follows directly
from the definition of the clustering. See also
Figure~\ref{t2:fig:propositions}.
\begin{prop}[Ancestor relations]\label{t2:lem:ancestorlemma}
For any pair of nodes $v, w \in V(T)$, the following hold
\begin{itemize}
  \item[(i)] If $c(v) = c(w)$ then $v \prec_T w$ iff $v \prec_{C(c(v))} w$.
  \item[(ii)] If $c(v) \neq c(w)$, $c(v) \in \{s(v',w'), v'\}$, and $c(w)
  \in \{l(v', w'), r(v',w')\}$ then we have $v \prec_T w$ iff $v \prec_{C(c(v),s(v',w'), v')} w$.
  \item[(iii)] In all other cases, $w \prec_T v$ iff $c(w) \prec_{M} c(v)$.
\end{itemize}
\end{prop}
Case (i) says that if $v$ and $w$ belongs to the same macro node
then $v$ is an ancestor of $w$ iff $v$ is an ancestor of $w$ in
the micro forest for that macro node. Case (ii) says that if $v$
is a spine node or a top boundary node and
$w$ is a left or right node in the same cluster then
$v$ is an ancestor of $w$ iff $v$ is an ancestor of $w$ in the
micro tree induced by that cluster (Figure~\ref{t2:fig:propositions}(a)). Case (iii) says that in all
other cases $v$ is an ancestor of $w$ iff the macro node $v$
belongs to is an ancestor of the macro node $w$ belongs to in the
macro tree.
\begin{figure}[t]
\begin{center}
\begin{psmatrix}[colsep=0.8cm,rowsep=0.35cm,labelsep=1pt]
&&& \cnode{.1}{root1}\rput(-.3,0){$v'$}
  \\
&& \cnode{.1}{a1} &  \cnode*{.1}{a2}\rput(.2,0){$v$}
  \\
& \cnode{.1}{b1} & \psellipse(-.29,0)(.68,1.2) \cnode{.1}{b2} &
  \cnode{.1}{b3} \psellipse(0,.1)(.4,1.2) \psellipse[linestyle=dashed](-.63,.25)(1.28,1.8)
  &
\cnode{.1}{b4}\\
& & \cnode*{.1}{c1}\rput(-.3,0){$w$} & \cnode{.1}{c2} & \cnode{.1}{c3} & \cnode{.1}{c4} \\
& && \cnode{.1}{d2}\rput(0,-.4){$w'$}
 \\
&&&
  \rput(0,-.3){(a)}
  \\\\
\ncline{root1}{a1}\ncline{root1}{a2}
  \ncline{a1}{b1}\ncline{a1}{b2} \ncline{a2}{b3}\ncline{a2}{b4}
  \ncline{b3}{c1}\ncline{b3}{c2} \ncline{b4}{c3}\ncline{b4}{c4}
  \ncline{c2}{d2}
  \end{psmatrix}
  \begin{psmatrix}[colsep=0.8cm,rowsep=0.35cm,labelsep=1pt]
&&& 
  \cnode{.1}{root1}\rput(-.3,0){$v'$}
  \\
&& \cnode{.1}{a1} &  \cnode{.1}{a2}
  \\
& \cnode*{.1}{b1} \rput(.2,0){$v$}& \psellipse(-.29,0)(.68,1.2) \cnode{.1}{b2} &
  \cnode{.1}{b3}\psellipse[linestyle=dashed](-.63,.25)(1.28,1.8)
  &
\cnode{.1}{b4}\\
& & \cnode*{.1}{c1}\rput(-.3,0){$w$} & \cnode{.1}{c2} & \cnode{.1}{c3} & \cnode{.1}{c4} \\
& && \cnode{.1}{d2}\rput(0,-.4){$w'$}
 \\
&&&
  \rput(0,-.3){(b)}
  \\\\
\ncline{root1}{a1}\ncline{root1}{a2}
  \ncline{a1}{b1}\ncline{a1}{b2} \ncline{a2}{b3}\ncline{a2}{b4}
  \ncline{b3}{c1}\ncline{b3}{c2} \ncline{b4}{c3}\ncline{b4}{c4}
  \ncline{c2}{d2}
  \end{psmatrix}
\begin{psmatrix}[colsep=0.8cm,rowsep=0.35cm,labelsep=1pt]
&&&
  \cnode{.1}{root1}\rput(-.3,0){$v'$} 
  \\
&&\cnode*{.1}{a1}\rput(.3,0){$v$} &  \psellipse(0,-.35)(1.1,1)\psellipse[linestyle=dashed](0,-.35)(1.4,1.6)\cnode{.1}{a2} \\ 
&&\cnode{.1}{b2} & \cnode*{.1}{b3}\rput(.3,0){$w$} & \cnode{.1}{b4} &
  \\\\\\
&&& \rput(0,-.4){(c)}    \\\\
\ncline{root1}{a1}\ncline{root1}{a2}
  \ncline{a1}{b2} \ncline{a2}{b3}\ncline{a2}{b4}
  \end{psmatrix}
\begin{psmatrix}[colsep=0.8cm,rowsep=0.35cm,labelsep=1pt]
&&& 
  \cnode{.1}{root1}\rput(-.3,0){$v'$}
  \\
&& \cnode*{.1}{a1}\rput(-.3,0){$v$} &  \cnode{.1}{a2}
  \\
& \cnode{.1}{b1} & \psellipse(-.29,0)(.68,1.2) \cnode{.1}{b2} &
  \cnode*{.1}{b3}\rput(.2,.1){$w$} \psellipse(0,.1)(.4,1.2) \psellipse[linestyle=dashed](-.63,.25)(1.28,1.8)
  &
\cnode{.1}{b4}\\
& & \cnode{.1}{c1} & \cnode{.1}{c2} & \cnode{.1}{c3} & \cnode{.1}{c4} \\
& && \cnode{.1}{d2}\rput(0,-.4){$w'$}
 \\
&&&
  \rput(0,-.3){(d)}
  \\\\
\ncline{root1}{a1}\ncline{root1}{a2}
  \ncline{a1}{b1}\ncline{a1}{b2} \ncline{a2}{b3}\ncline{a2}{b4}
  \ncline{b3}{c1}\ncline{b3}{c2} \ncline{b4}{c3}\ncline{b4}{c4}
  \ncline{c2}{d2}
  \end{psmatrix}
\begin{psmatrix}[colsep=0.8cm,rowsep=0.35cm,labelsep=1pt]
&&& \cnode{.1}{root1}\rput(.3,0){$v'$}
  \\
&& \cnode{.1}{a1} &  \cnode{.1}{a2}
  \\
& \cnode{.1}{b1} & \cnode{.1}{b2} &
  \cnode{.1}{b3} \psellipse(0,.1)(.4,1.2) \psellipse[linestyle=dashed](.7,.25)(1.32,1.85)
  &
  \psellipse(.4,-.34)(.7,.8) \cnode*{.1}{b4}\rput(.3,0){$w$}
 \\
& & \cnode{.1}{c1} & \cnode*{.1}{c2}\rput(.15,.15){$v$} & \cnode{.1}{c3} & \cnode{.1}{c4} && \\
& && \cnode{.1}{d2}\rput(0,-.4){$w'$}
 \\
&&&
  \rput(0,-.3){(e)}
  \\
\ncline{root1}{a1}\ncline{root1}{a2}
  \ncline{a1}{b1}\ncline{a1}{b2} \ncline{a2}{b3}\ncline{a2}{b4}
  \ncline{b3}{c1}\ncline{b3}{c2} \ncline{b4}{c3}\ncline{b4}{c4}
  \ncline{c2}{d2}
  \end{psmatrix}
\begin{psmatrix}[colsep=0.8cm,rowsep=0.35cm,labelsep=1pt]
&&& \cnode{.1}{root1}\rput(.3,0){$v'$}
  \\
&& \cnode{.1}{a1} &  \cnode{.1}{a2}
  \\
& \cnode{.1}{b1} & \psellipse(-.29,0.1)(.68,1.2) \cnode{.1}{b2} &
  \cnode{.1}{b3} \psellipse[linestyle=dashed](0,0.2)(2.3,1.6)
  &
  \psellipse(.4,-.3)(.7,.8) \cnode*{.1}{b4}\rput(.3,0){$w$}
 \\
& & \cnode*{.1}{c1}\rput(-.3,0){$v$}& \cnode{.1}{c2} & \cnode{.1}{c3} & \cnode{.1}{c4} &&\\
& && \cnode{.1}{d2}\rput(0,-0.4){$w'$}
 \\
&&&
  \rput(0,-.3){(f)}
  \\
\ncline{root1}{a1}\ncline{root1}{a2}
  \ncline{a1}{b1}\ncline{a1}{b2} \ncline{a2}{b3}\ncline{a2}{b4}
  \ncline{b3}{c1}\ncline{b3}{c2} \ncline{b4}{c3}\ncline{b4}{c4}
  \ncline{c2}{d2}
  \end{psmatrix}
   \begin{psmatrix}[colsep=0.8cm,rowsep=0.35cm,labelsep=1pt]
&&& 
  \cnode{.1}{root1}\rput(-.3,0){$v'$}
  \\
&& \cnode*{.1}{a1}\rput(-.3,0){$v$} &  \cnode{.1}{a2}
  \\
& \cnode{.1}{b1} & \psellipse(-.29,0.05)(.68,1.1) \cnode{.1}{b2} &
  \cnode{.1}{b3} \psellipse[linestyle=dashed](-.63,.35)(1.28,1.6)
  &
\cnode{.1}{b4}\\
& & \cnode{.1}{c1} & \cnode{.1}{c2}\rput(-.3,0){$w'$} \\& & & \cnode*{.1}{d3}\rput(.3,0){$w$}
 \\[-.3cm]
  && \cnode{0}{d2} && \cnode{0}{d4}
  \\[-.5cm]
&&&
  \rput(0,-.3){(f)}
  \\
\ncline{root1}{a1}\ncline{root1}{a2}
  \ncline{a1}{b1}\ncline{a1}{b2} \ncline{a2}{b3}\ncline{a2}{b4}
  \ncline{b3}{c1}\ncline{b3}{c2} \ncline{c2}{d2}\ncline{c2}{d4}\ncline{d2}{d4}
  \end{psmatrix}
   \caption{Examples from the propositions. In all cases $v'$ and $w'$ are top and bottom
   boundary nodes of the cluster, respectively. (a)
   Proposition~\ref{t2:lem:ancestorlemma}(ii).
   Here $c(v)=s(v',w')$ and $c(w)=l(v',w')$ (solid ellipses). The dashed ellipse corresponds to
   $C(c(v),s(v',w'),v')$.
   (b) Proposition~\ref{t2:lem:orderlemma}(i) and~\ref{t2:lem:ncalemma}(ii). Here $c(v)=c(w)=l(v',w')$ (solid ellipse). 
   The dashed ellipse corresponds to
   $C(c(v),s(v',w'),v')$. 
   (c) Proposition~\ref{t2:lem:orderlemma}(ii) and~\ref{t2:lem:ncalemma}(i). Here $c(v)=c(w)=l(v')$ (solid ellipse). 
   The dashed ellipse corresponds to   $C(c(v),v')$. 
   (d)
    Proposition~\ref{t2:lem:orderlemma}(iii). Here $c(v)=l(v',w')$ and $c(w)=s(v',w')$ (solid ellipses). The dashed ellipse corresponds to
   $C(c(v),c(w),v')$.
   (e) Proposition~\ref{t2:lem:orderlemma}(iv). Here $c(v)=s(v',w')$ and $c(w)=r(v',w')$ (solid ellipses). The dashed ellipse corresponds to
   $C(c(v),c(w),v')$. (f) Proposition~\ref{t2:lem:ncalemma}(iv). Here $c(v)=r(v',w')$ and $c(w)=l(v',w')$ (solid ellipses). The dashed ellipse corresponds to $C(c(v),c(w),s(v',w'),v')$.  (g) Proposition~\ref{t2:lem:ncalemma}(v). Here $c(v)=r(v',w')$ (solid ellipse) and $w' \preceq_M c(w)$. The dashed ellipse corresponds to $C(c(v),s(v',w'),v',w'))$.}
  \label{t2:fig:propositions}
\end{center}
\end{figure}

\begin{prop}[Left-of relations]\label{t2:lem:orderlemma}
For any pair of nodes $v, w \in V(T)$, the following hold
\begin{itemize}
  \item[(i)] If $c(v) = c(w) \in \{r(v',w'),l(v',w')\}$ then $v \lhd w$
     iff $v \lhd_{C(c(v),v',s(v',w'))}
    w$.
  \item[(ii)] If $c(v) = c(w)=l(v')$ then $v \lhd w$ iff $v \lhd_{C(c(v),v')} w$.
  \item[(iii)] If $c(v) = l(v', w')$ and $c(w)=s(v',w')$  then
  $v \lhd w$ iff $v \lhd_{C(c(v), c(w), v')} w$.
  \item[(iv)] If  $c(v)=s(v',w')$  and $c(w) = r(v', w')$ then
  $v \lhd w$ iff $v \lhd_{C(c(v), c(w), v')} w$.
  \item[(v)] In all other cases, $v \lhd w$ iff $c(v) \lhd_{M} c(w)$.
\end{itemize}
\end{prop}
Case (i) says that if $v$ and $w$  are both either left or right nodes in the same cluster then $v$ is to the left of $w$ iff
$v$ is to the left of $w$ in the micro tree induced by their macro
node together with the spine and top boundary node of the cluster (Figure~\ref{t2:fig:propositions}(b)).
Case (ii) says that if $v$ and $w$ are both leaf nodes in the same cluster then $v$ is to the left of $w$ iff $v$ is to the
left of $w$ in the micro tree induced by that leaf cluster (Figure~\ref{t2:fig:propositions}(c)). Case (iii) says that if $v$ is a left node and $w$ is a spine node in the
same cluster then $v$ is to the left of $w$ iff $v$ is to the left
of $w$ in the micro tree induced by their two macro nodes and the
top boundary node of the cluster (Figure~\ref{t2:fig:propositions}(d)). Case (iv) says that if $v$ is a spine node and $w$  is a  right node in the same cluster then $v$ is to the left of $w$ iff $v$ is to the left
of $w$ in the micro tree induced by their two macro nodes  and the top boundary node of the cluster (Figure~\ref{t2:fig:propositions}(e)). In all other cases $v$ is to the
left of $w$ if the macro node $v$ belongs to is to the left of the
macro node of $w$ in the macro tree (Case (v)).

\begin{prop}[Nca relations]\label{t2:lem:ncalemma}
    For any pair of nodes $v, w \in V(T)$, the following hold
    \begin{itemize}
    \item[(i)] If $c(v) = c(w)=l(v')$
        then $\nca_T(v,w)=\nca_{C(c(v),v')}(v,w)$.
    \item[(ii)] If $c(v) = c(w)\in\{l(v',w'),r(v',w')\}$
        then $\nca_T(v,w)=\nca_{C(c(v), s(v',w'), v')}(v,w)$.
    \item[(iii)] If $c(v) = c(w)=s(v',w')$
        then $\nca_T(v,w)=\nca_{C(c(v))}(v,w)$.
    \item[(iv)] If $c(v) \neq c(w)$ and $c(v),c(w)\in\{l(v',w'),r(v',w'),s(v',w')\}$
        then \\ $\nca_T(v,w)=\nca_{C(c(v),c(w),s(v',w'),v')}(v,w)$.
    \item[(v)] If $c(v) \neq c(w)$,
        $c(v)\in\{l(v',w'),r(v',w'),s(v',w')\}$, and $w' \preceq_{M}
        c(w)$ then \\ $\nca_T(v,w)=\nca_{C(c(v),s(v',w'),v',w')}(v,w')$.
    \item[(vi)] If $c(v) \neq c(w)$,
        $c(w)\in\{l(v',w'),r(v',w'),s(v',w')\}$, and $w' \preceq_{M}
        c(v)$ then \\ $\nca_T(v,w)=\nca_{C(c(w'),s(v',w'),v',w')}(w,w')$.
    \item[(vii)] In all other cases, $\nca_T(v,w)=\nca_{M}(c(v),c(w))$.
    \end{itemize}
\end{prop}
Case (i) says that if $v$ and $w$ are leaf nodes in the same cluster
then the nearest common ancestor of $v$ and $w$ is the nearest
common ancestor of $v$ and $w$ in the micro tree induced by that
leaf cluster (Figure~\ref{t2:fig:propositions}(c)). Case (ii) says
that if $v$ and $w$ are both either left nodes
or right nodes then the nearest common ancestor of $v$ and $w$ is
the nearest common ancestor in the micro tree induced by their
macro node together with the spine and top boundary node of the
cluster (Figure~\ref{t2:fig:propositions}(b)). Case (iii) says that if $v$ and $w$ are both
spine nodes in the same cluster then the nearest common ancestor of $v$ and $w$ is the
nearest common ancestor of $v$ and $w$ in the micro tree induced
by their macro node. Case (iv) says that if $v$ and $w$ are in different macro nodes but are right, left, or spine nodes in the same cluster then the
nearest common ancestor of $v$ and $w$ is the nearest common
ancestor of $v$ and $w$ in the micro tree induced by that cluster
(we can omit the bottom boundary node) (Figure~\ref{t2:fig:propositions}(f)). Case (v) says that if $v$
is a left, right, or spine node, and the bottom boundary
node $w'$ of $v$'s cluster is an ancestor in the macro tree of the
macro node containing $w$, then the nearest common ancestor of $v$
and $w$ is the nearest common ancestor of $v$ and $w'$ in the
micro tree induced by the macro node of $v$, the spine node, and the top and bottom boundary nodes of $v$'s cluster (Figure~\ref{t2:fig:propositions}(g)). Case (vi) is the same as
case (v) with $v$ and $w$ interchanged. In all other cases the
nearest common ancestor of $v$ and $w$ is the nearest common
ancestor of their macro nodes in the macro tree (Case (vii)).

\subsection{Preprocessing}\label{t2:sec:preprocessing}
In this section we describe how to preprocess $T$. First build a
cluster partition $CS$ of the tree $T$ with clusters of size $s$,
to be fixed later, and the corresponding macro tree $M$ in
$O(n_T)$ time. The macro tree is preprocessed as in Section~\ref{t2:sec:simplepreprocessing}. However, since nodes in $M$ contain a set of labels, we now store a dictionary for $\lab(v)$ for each node $v \in V(M)$. Using the deterministic using the deterministic dictionary of Hagerup et. al. \cite{HMP2001} all these dictionaries can be constructed in $O(n_{T}\log n_{T})$ time and $O(n_{T})$ space.
Furthermore, we extend the definition of $\fl$ such that $\fl_{M}(v, \alpha)$ is the nearest ancestor $w$ of $v$ such that $\alpha \in \lab(w)$.

Next we show how to preprocess the micro forests. For any cluster $C\in CS$, deep sets $X, Y, Z\subseteq V(C)$, $i \in \mathbb{N}$, and $\alpha \in \Sigma$ define the following procedures on cluster $C$.


\begin{relate}[clusterprocedures]

\item[$\leftn_C(i,X)$:] Return the leftmost $i$ nodes in $X$.

\item[$\rn_C(i,X)$:] Return the rightmost $i$ nodes in $X$.

\item[$\leftof_C(X,Y)$:] Return all nodes of $X$ to the left of the leftmost node in $Y$.



\item[$\match_C(X,Y,Z)$,] where $X=\{m_1 \lhd \cdots \lhd m_k\}$,
$Y=\{v_1 \lhd \cdots \lhd v_k\}$, and $Z \subseteq Y$.
Return $R:=\{m_{j} \mid v_j \in Z \}$.


\item[$\mopc_{C}(X,Y)$] Return
the pair $(R_1,R_2)$. Where
$R_1=\restrict{\mop(M,N)}{1}$ and $R_2=\restrict{\mop(M,N)}{2}$.


\end{relate}
In addition to these procedures we also define the set procedures on clusters, that is, $\Parent_C$, $\Nca_C$, $\Deep_C$, and $\Fl_C$, as in Section~\ref{t2:sec:recursion}. Collectively, we will call these the \emph{cluster procedures}.  We represent the input and outputs set in the procedures as bit strings indexed by preorder numbers. Specifically, a subset $X$ in a cluster $C$ is given by a bit string $b_{1} \ldots b_{s}$, such that $b_{i} = 1$ iff the $i$th node in a preorder traversal of $C$ is in $X$. If $C$ contains fewer than $s$ nodes we leave the remaining values undefined.

The procedures $\leftn_C(i,X)$  then corresponds to setting
 all bits in $X$ larger than the $i$th set bit to zero. Similarly, $\rn_C(i,X)$ corresponds to setting all bits smaller than the $i$th largest set bit to zero. Similarly, the procedures $\leftof_C(X,Y)$, $\Rightof_C(X,Y)$, and $\match_C(X,Y,Z)$ only depends on the preorder of the nodes and thus only on the bit string not any other information about the cluster. We can thus ommit the subscript $C$ from these five procedures.

Next we show how to implement the cluster procedures efficiently. We precompute the value of all procedures, except $\Fl_{C}$, for all possible inputs and clusters. By definition, these procedures do not depend on any specific labeling of the nodes in the cluster. Hence, it suffices to precompute the value for all rooted, ordered trees with at most $s$ nodes. The total number of these is less than $2^{2s}$ (consider e.g. an encoding using balanced parenthesis). Furthermore, the number of possible input sets is at most $2^{s}$. Since at most $3$ sets are given as input to a cluster procedure, it follows that we can tabulate all solutions using less than $2^{3s}\cdot 2^{2s} = 2^{5s}$ bits of memory. Hence, choosing $s \leq 1/10\log n$ we use $O(2^{\frac{1}{2}\log n}) = O(\sqrt{n})$ bits. Using standard bit wise operations each solution is easily implemented in $O(s)$ time giving a total time of $O(\sqrt{n}\log n)$. 

Since the procedure $\Fl_{C}$ depends on the alphabet, which may be of size $n_{T}$, we cannot efficiently apply the same trick as above. Instead define for any cluster $C \in CS$, $X \subseteq V(C)$, and $\alpha \in \Sigma$:
\begin{relate}[simple]
\item[$\Ancestor_{C}(X)$:] Return the set $\{x \mid \text{$x$ is an ancestor of a node in $X$}\}$.
\item[$\Eq_C(\alpha)$:] Return the set $\{x \mid x \in V(C), \lab(x) = \alpha\}$.
\end{relate}
Clearly, $\Ancestor_{C}$ can be implemented as above. For $\Eq_{C}$ note that the
 total number of distinct labels in $C$ is at most $s$. Hence, $\Eq_{C}$ can be stored in a dictionary with at most $s$ entries each of which is a bit string of length $s$. Thus, (using again the result of \cite{HMP2001}) the total time to build all such dictionaries is $O(n_{T}\log n_{T})$.

By the definition of $\Fl_{C}$ we have that,
\begin{equation*}
\Fl_C(X,\alpha) = \Deep_C(\Ancestor_C(X) \cap \Eq_C(\alpha)).
\end{equation*}
Since intersection can be implemented using a binary \emph{and}-operation, $\Fl_{C}(X, \alpha)$ can be computed in constant time. Later, we will also need to compute union of bit strings and we note that this can be done using a binary \emph{or}-operation.

To implement the set procedures in the following section we often need to ``restrict'' the cluster procedures to work on a subtree of a cluster. Specifically, for any set of macro nodes $\{i_{1}, \ldots, i_{k}\}$ in the \emph{same} cluster $C$ (hence, $k \leq 5$), we will replace the subscript $C$ with $C(i_{1}, \ldots, i_{k})$. For instance, $\Parent_{C(s(v,w), l(v,w))}(X) = \{\parent(x) \mid x\in X \cap V(C(s(v,w), l(v,w))\} \cap V(C(s(v,w), l(v,w))$. To implement all restricted versions of the cluster procedures, we compute for each cluster $C \in CS$ a bit string representing the set of nodes in each micro forest. Clearly, this can be done in $O(n_{T})$ time. Since there are at most $5$ micro forests in each cluster it follows that we can compute any restricted version using an additional constant number of and-operations.

Note that the total preprocessing time and space is dominated by the construction of deterministic dictionaries which use $O(n_{T}\log n_{T})$ time and $O(n_T)$ space.




\subsection{Implementation of the Set Procedures}
Using the preprocessing from the previous section we show how to implement the set procedures in sublinear time. First we define a compact representation of node sets. Let $T$ be a tree with macro tree $M$. For simplicity, we identify nodes in $M$ with their preorder number. Let $S \subseteq V(T)$ be any subset of nodes of $T$.
A \emph{micro-macro node array} (abbreviated node array) $X$ representing $S$ is an array of size $n_{M}$. The $i$th entry, denoted $X[i]$, represents the subset of nodes in $C(i)$, that is, $X[i] =  V(C(i)) \cap S$. The set $X[i]$ is encoded using the same bit representation as in Section~\ref{t2:sec:preprocessing}. By our choice of parameter in the clustering the space used for this representation is $O(n_{T}/\log n_{T})$.

We now present the detailed implementation of the set procedures on node arrays. Let $X$ be a node array.
\begin{relate}
\item[$\Parent(X)$:] Initialize a node array $R$ of size $n_{M}$ and set $i:=1$.

Repeat until $i>n_M$:
\begin{itemize}
\item[] Set $i:=i+1$ until $X[i]\neq \emptyset$.

There are three cases depending on the type of $i$:
\begin{enumerate}
  \item $i\in \{l(v,w), r(v,w)\}$. Compute $N := \Parent_{C(i, s(v,w), v)}(X[i])$. For each $j \in \{i, s(v,w), v\}$, set $R[j] := R[j] \cup (N \cap V(C(j)))$.
  \item $i = l(v)$. Compute $N := \Parent_{C(i, v)}(X[i])$. For each $j \in \{i, v\}$, set $R[j] :=R[j] \cup (N \cap V(C(j)))$.
  \item $i \not\in \{l(v,w), r(v,w), l(v)\}$. Compute $N := \Parent_{C(i)}(X[i])$. If $N \neq \emptyset$ set $R[i]:=R[i] \cup N$. Otherwise, if $j := \parent_{M}(i) \neq \bot$ set
    $R[j] := R[j] \cup \{\first(j)\}$.
\end{enumerate}
Set $i:=i+1$.
\end{itemize}
Return $R$.
\end{relate}
To see the correctness of the implementation of procedure  $\Parent$
consider the three cases of the procedure. Case 1 handles
the fact that  left or right nodes may have a node on a spine or
boundary node as parent. Since no left or right nodes can have a
parent outside their cluster there is no need to compute parents
in the macro tree. Case 2 handles the fact that a leaf node may have the boundary node as parent. Since no leaf node can have a parent outside its cluster
there is no need to compute parents in the macro tree. Case 3
handles boundary and spine nodes. In this case there is either a parent within the micro forest or we can use
the macro tree to compute the parent of the root of the micro
tree.
Since the input to \Parent\ is deep we only need to do one of the two things. If the computation of parent in the micro tree returns a node $j$, this will either be a spine node or a boundary node. To take care of the case where $j$ is a spine node, we add the lowest node ($\first(j)$) in $j$ to the output. Procedure \Parent\ thus correctly computes parent for all kinds of macro nodes.

We now give the implementation of procedure \Nca.
The input to procedure \Nca\ is two node arrays $X$ and $Y$ representing two subsets $\mc{X}, \mc{Y} \subseteq V(T)$,  $\norm{\mc{X}}=\norm{\mc{Y}}=k$. The output is a node array $R$ representing the set $\{\nca(\mc{X}_i,\mc{Y}_i) \mid 1\leq i\leq k\}$, where $\mc{X}_i$ and $\mc{Y}_i$ is the $i$th element of  $\mc{X}$ and $\mc{Y}$, wrt.\ to their preorder number in the tree, respectively. We also assume that we have $\mc{X}_i \prec \mc{Y}_i$ for all $i$ (since $\Nca$ is always called on a set of minimum ordered pairs).
\begin{relate}
\item[$\Nca(X,Y)$:] Initialize a node array $R$ of size $n_{M}$, set $i:=1$ and $j:=1$.

    Repeat until $i>n_M$ or $j>n_M$:
    \begin{itemize}
    \item[] Until $X[i] \neq \emptyset$ set $i:=i+1$. Until  $Y[j] \neq \emptyset$ set $j:=j+1$.

    Compare $i$ and $j$.  There are two cases:
    \begin{enumerate}
    \item $i=j$. There are two subcases:
 	\begin{enumerate}
   	\item $i$ is a boundary node.
	
    	Set $R[i]:=X[i]$, $i:=i+1$ and $j:=j+1$.
       
            
        \item $i$ is not a boundary node. 
        
        	Compare the sizes of $X[i]$ and $Y[j]$. There are two cases:
             \begin{itemize}
             \item $\norm{X[i]} > \norm{Y[j]}$. Set 
$X_i:= \leftn (\norm{Y[j]},X[i])$,
             \item $\norm{X[i]} = \norm{Y[j]}$. Set $X_i=X[i]$.
             \end{itemize}
        Set
        \begin{equation*}
        S:=
            \begin{cases}
            C(i,v), &  \textrm{if } i=l(v),\\
            C(i, s(v,w), v), & \textrm{if } i\in\{l(v,w),r(v,w)\}, \\
            C(i), & \textrm{if } i=s(v,w).
            \end{cases}
            \end{equation*}
            Compute $N:=\Nca_S(X_i,Y_j)$.

        For each macronode $h$ in $S$ set 
        $R[h]:=R[h] \cup (N\cap V(C(h)))$.

        Set $X[i]:=X[i]\setminus X_i$ and $j:=j+1$.
	\end{enumerate}
    \item $i \neq j$. 
	    	Compare the sizes of $X[i]$ and $Y[j]$. There are three cases:
         	\begin{itemize}
		\item $\norm{X[i]} > \norm{Y[j]}$. 
			Set $X_i:=\leftn(\norm{Y[j]},X[i])$ 
and $Y_j:=Y[j]$, 
		
		\item $\norm{X[i]} < \norm{Y[j]}$. Set 
		$X_i:=X[i]$ and $Y_j :=\leftn(\norm{X[i]},Y[j])$,
		
		\item $\norm{X[i]} = \norm{Y[j]}$. Set 
		$X_i:=X[i]$ and $Y_j:=Y[j]$.
		\end{itemize}
	    Compute $h:=\Nca_M(i,j)$. There are two subcases:
            \begin{enumerate}
            \item $h$ is a boundary node. Set $R[h]:=1$.
            \item  $h$ is a spine node $s(v,w)$. There are three cases:
                \begin{enumerate}
                \item $i \in \{l(v,w),s(v,w)\}$ and $j\in \{s(v,w),r(v,w)\}$. 
                
                Compute $N:=\Nca_{C(i,j,h,v)}(X_i,Y_j)$.
                \item $i=l(v,w)$ and $w \preceq j$. 
                
                Compute $N:=\Nca_{C(i,h,v,w)}(\rn(1,X_i),w)$.
                \item $j=r(v,w)$ and $w \preceq i$. 
                
                Compute $N:=\Nca_{C(j,h,w,v)}(w,\leftn(1,Y_j))$.
                \end{enumerate}
                Set $R[h]:=R[h]\cup (N\cap V(C(h)))$ and 
                $R[v]:=R[v]\cup (N\cap V(C(v)))$.
            \end{enumerate}
            Set $X[i]:=X[i]\setminus X_i$ and $Y[j]:=Y[j]\setminus Y_j$.
    \end{enumerate}
    \end{itemize}
Return $R$.
\end{relate}
In procedure \Nca\ we first find the next non-empty entries in the node arrays $X[i]$and $Y[j]$. Then we have two cases depending on whether $i=j$ or not.
If $i=j$ (Case 1) we have two subcases. If $i$ is a boundary node (Case 1(a)) then $C(i)$ only consists of one node $v= X[i]=Y[j]$ and therefore $\nca(v,v)=v=X[i]$.
If $i$ is not a boundary node (Case 1(b)) we compare the sizes of the subsets represented by $X[i]$and $Y[i]$. If $\norm{X[i]} > \norm{Y[j]}$  we compute nearest common ancestors of the first/leftmost $\norm{Y[j]}$ nodes in $X[i]$and the nodes in $Y[j]$. Due to the assumption on the input ($\mc{X}_i \prec \mc{Y}_i$) we either have $\norm{X[i]} > \norm{Y[j]}$ or  $\norm{X[i]} = \norm{Y[j]}$. If $\norm{X[i]} > \norm{Y[j]}$ we must compute nearest common ancestors of the first/leftmost $\norm{Y[j]}$ nodes in $X[i]$and the nodes in $Y[j]$. If $\norm{X[i]} = \norm{Y[j]}$ we must compute nearest common ancestors of all nodes in $X[i]$and $Y[j]$. We now compute nearest common ancestors of the described nodes in a cluster $S$ depending on what kind of node $i$ is. If $i$ is a leaf node then the nearest common ancestors of the nodes in $X[i]$ and $Y[j]$ is either in $i$or in the boundary node (Proposition~\ref{t2:lem:ncalemma}(i)). If $i$ is a left or right node then the nearest common ancestors must be in $i$ on the spine or in the top boundary node (Proposition~\ref{t2:lem:ncalemma}(ii)). If $i$ is a spine node then the nearest common ancestors must be on the spine or in the top boundary node (Proposition~\ref{t2:lem:ncalemma}(iii)). We update the output node array, remove from  $X[i]$  the nodes we have just computed nearest common ancestors of, and increment $j$ since we have now computed nearest common ancestors for all nodes in $Y[j]$.

Now consider the case where $i \neq j$. First we
 compare the sizes of the subsets represented by $X[i]$and $Y[i]$. If $\norm{X[i]} > \norm{Y[j]}$  we should compute nearest common ancestors of the first/leftmost $\norm{Y[j]}$ nodes in $X[i]$and the nodes in $Y[j]$ as in Case 1(b). If $\norm{X[i]} < \norm{Y[j]}$ we must compute nearest common ancestors of the first/leftmost $\norm{X[i]}$ nodes in $Y[j]$and the nodes in $X[i]$. Otherwise $\norm{X[i]} = \norm{Y[j]}$ and we  compute nearest common ancestors of the all nodes  in $X[i]$ and $Y[j]$. We now compute the nearest common ancestor of $i$and $j$in the macro tree. This must either be a boundary node or a spine node due to the structure of the macro tree. If it is a boundary node then the nearest common ancestor of all nodes in $i$ and $j$ is this boundary node. If it is a spine node we have three different cases depending on the types of $i$and $j$. If $i$is a left or spine node and $j$is a spine or right node in the same cluster then we compute nearest common ancestors in that cluster (Proposition~\ref{t2:lem:ncalemma}(iv)). If $i$ is a left node and $j$ is a descendant of the bottom boundary node in $i$'s cluster then we compute the nearest common ancestor of the rightmost node in $X_i$ and $w$  in $i$'s cluster(Proposition~\ref{t2:lem:ncalemma}(v)). That we can restrict the computation to only the rightmost node in $X_i$ and $w$ is due to the fact that we always run \Deep\ on the output from \Nca\ before using it in any other computations. In the last case $j$ is a right node and $i$ is a descendant of the bottom boundary node of $j'$s cluster. Then we compute the nearest common ancestor of the leftmost node in $Y_j$ and $w$ (Proposition~\ref{t2:lem:ncalemma}(vi)) in $j$'s cluster. The argument for restricting the computation to the leftmost node of $Y_j$and $w$ is the same as in the previous case. Due to the assumption on the input ($\mc{X}_i \prec \mc{Y}_i$) the rest of the cases from Proposition~\ref{t2:lem:ncalemma}(iv)--(vi) cannot happen. Therefore, we have now argued that the procedure correctly takes care of all cases from Proposition~\ref{t2:lem:ncalemma}. Finally, we update the output node array and remove from $X[i]$ and $Y[j]$ the nodes we have just computed nearest common ancestors of.

The correctness of the procedure follows from the above and induction on the rank of the elements. 
 
\ignore{**************Case 1(b) handles the cases
(i), (ii), and (iii) from Proposition~\ref{t2:lem:ncalemma} and Case 1(a) handles part of case (vii). Case 2
handles the cases (iv), (v), (vi) and (vii) from
Proposition~\ref{t2:lem:ncalemma}. In all cases we make sure that the number of nodes in the microtrees/forrests that we work on are the same by comparing the sizes of $X[i]$ and $Y[j]$. In Case 1(a) $i=j$ is a boundary node and thus only contains one node. In Case 1(b) it is possible that there are more nodes in the set represented by $X[i]$ than in the one represented by $Y[j]$ (but not vice versa due to the assumptions on the input). In that case we extract the leftmost $\norm{Y[j]}$ nodes from $X[i]$ and use these for the computation. Before iterating we delete these nodes from $X[i]$. In Case 2 we have three cases since the set represented by $Y[j]$now can be larger than the set represented by $X[i]$. Again we extract the relevant nodes, compute $\nca$ on these and update $X[i]$and $Y[j]$before iterating. 
}

\begin{relate}
\item[$\Deep(X)$:] Initialize a node array $R$ of size $n_{M}$ and set $j := 1$.

Repeat until $i>n_M$:
\begin{itemize}
\item[] Set $i:=i+1$ until $X[i]\neq \emptyset$.

Compare $j$ and $i$. There are three cases:
\begin{enumerate}
  \item $j \lhd i$. Set \begin{equation*}
        S:=
            \begin{cases}
            C(j,v), &  \textrm{if } j=l(v),\\
            C(j, s(v,w), v), & \textrm{if } j\in\{l(v,w),r(v,w)\}, \\
            C(j), & \textrm{otherwise}.
            \end{cases}
            \end{equation*}
  Set $R[j] := \Deep_S (X[j])$ and $j := i$.
  \item $j \prec i$. If $i \in \{l(v,w), r(v,w)\}$ and $j=s(v,w)$ compute
  $N := \Deep_{C(i, s(v,w), v)}(X[i] \cup X[j])$, and set $R[j]:=R[j]\cap N$.


  Set $j:=i$.
\end{enumerate}
Set $i:=i+1$.
\end{itemize}
Set $R[j] := \Deep_S (X[j])$, where $S$is set as in Case 1.
  
Return $R$.
\end{relate}
The above $\Deep$ procedure resembles the previous $\Deep$
procedure implemented on the macro tree in the two first cases. The third case from the previous implementation can be omitted since the input list is now in preorder.
In case 1 node $i$ is to the
right of our "potential output node" $j$. Since any node $l$ that is a
descendant of $j$ must be to the left of $i$ ($l<i$) it cannot not
appear later in the list $X$ than $i$. We can thus safely add
$\Deep_S(X[j])$ to $R$ at this point. To ensure that the cluster we compute \Deep\ on is a tree we include the top boundary node if $j$is a leaf node and the top and spine node if $j$is a left or right node. In case 2 node $j$ is an ancestor of
$i$ and can therefore not be in the output list unless $j$ is a spine node and $i$is the corresponding left or right node. If this is the case we first compute \Deep\ of $X[j]$ in the cluster containing $i$and $j$ and add the result to the output before setting $i$ to be our new potential node.
After scanning the whole node array $X$ we add the last potential node $j$to the output after computing \Deep\ of it as in case 1.

That the procedure is correct follows by the proof of Lemma~\ref{t2:lem:deep} and the above.
\\\\
We now give the implementation of procedure \Mop.
Procedure $\Mop$ takes a pair of node arrays $(X,Y)$ and another node array $Z$ as input.  The pair $(X,Y)$ represents a set of minimum ordered pairs, where the first coordinates are in $X$ and the second coordinates are in $Y$.
To simplify the implementation of procedure $\Mop$ it calls two auxiliary procedures $\Mopsim$ and $\Match$ defined below. Procedure $\Mopsim$ computes $\mop$ of $Y$ and $Z$, and procedure $\Match$ takes care of finding the first-coordinates from $X$ corresponding to the first coordinates from the minimum ordered pairs from $M$.

\begin{relate}
\item[$\Mop((X,Y),Z)$] Compute $M:=\Mopsim(Y,Z)$. Compute $R:=\Match(X,Y,\restrict{M}{1})$. Return $(R,\restrict{M}{2})$.
\end{relate}
Procedure $\Mopsim$ takes two node arrays as input and computes $\mop$ of these.
\begin{relate}
\item[$\Mopsim(X,Y)$] Initialize two node arrays $R$ and $S$ of size $n_M$, set $i:=1$,
$j:=1$, $h:=1$,  $(r_1,r_2):=(0,\emptyset)$, $(s_1,s_2):=(0,\emptyset)$.
Repeat the following until $i>n_M$ or $j>n_M$:
\begin{itemize}
\item[] Set $i:=i+1$ until $X[i] \neq \emptyset$. There are three cases:
\begin{enumerate}
\item If $i=l(v,w)$ for some $v, w$ set $j:=j+1$ until $Y[j]\neq \emptyset$ and either $i \lhd j$, $i=j$, or $j=s(v,w)$.
\item If $i=s(v,w)$ for some $v, w$ set $j:=j+1$ until $Y[j]\neq \emptyset$ and either $i \lhd j$ or $j=r(v,w)$.
\item If $i \in \{r(v,w), l(v)\}$ for some $v, w$ set $j:=j+1$ until $Y[j]\neq \emptyset$ and either $i \lhd j$ or  $i=j$.
\item Otherwise ($i$ is a boundary node) set $j:=j+1$ until $Y[j]\neq \emptyset$ and $i \lhd j$.
\end{enumerate}
Compare $i$ and $j$. There are two cases:
    \begin{enumerate}
    \item $i\lhd j$: Compare $s_1$ and $j$. If $s_1 \lhd j$ set
        $R[r_1]:=R[r_1] \cup r_2$, $S[s_1]:=S[s_1] \cup s_2$, and
        $(s_1,s_2):= (j, \leftn_{C(j)}(1,Y[j]))$. 
        
        Set
        $(r_1,r_2):=(i, \rn_{C(i)}(1,X[i]))$ and $i=i+1$.
    \item Otherwise compute $(r,s):=\mopc_{C(i,j,v)}(X[i],Y[j])$, where $v$is the top boundary node in the cluster $i$ and $j$belongs to.
    
	If $r\neq \emptyset$ do:
            \begin{itemize}
            \item Compare $s_1$ and $j$. If  $s_1 \lhd j$ or if $s_1=j$
             and $\leftof_{C(i,j)}(X[i],s_2)=\emptyset$  then set
                $R[r_1]:=R[r_1] \cup r_2$, $S[s_1]:=S[s_1] \cup s_2$.

            \item  Set $(r_1,r_2):=(i,r)$ and $(s_1,s_2):=(j,s)$.

            \end{itemize}
		There are two subcases:
        \begin{enumerate}
        \item $i=j$ or $i=l(v,w)$ and $j=s(v,w)$. 
        Set  $X[i]:=\rn_{C(i)}(X[i])\setminus r_2$ and 
        $j:=j+1$.


        \item  $i=s(v,w)$ and $j=r(v,w)$. If $r_2=\emptyset$ set 
        $j:=j+1$ otherwise set $i:=i+1$.

        \end{enumerate}            
    \end{enumerate}
\end{itemize}
Set $R[r_1]:=R[r_1] \cup r_2$ and $S[s_1]:=S[s_1] \cup s_2$. Return $(R,S)$.
\end{relate}
Procedure $\Mopsim$ is somewhat similar to the previous implementation of the procedure $\Mop$ from Section~\ref{t2:implementationsimple}. We again have a "potential pair" $((r_1,r_2), (s_1,s_2))$ but we need more cases to take care of the different kinds of macro nodes. 


We first find the next non-empty macro node $i$. We then have 4 cases depending on which kind of node $i$ is. In Case 1 $i$ is a left node. Due to Proposition~\ref{t2:lem:orderlemma} we can have $\mop$ in $i$ (case (i)), in the spine (case (iii)), or in a node to the left of $i$ (case(v)). In Case 2 $i$ is a spine node. Due to Proposition~\ref{t2:lem:orderlemma} we can have $\mop$  in the right node (case (iv)) or in a node to the left of $i$ (case(v)). In Case 3 $i$ is a right node or a leaf node. Due to Proposition~\ref{t2:lem:orderlemma} we can have $\mop$ in $i$ (case (i) and (ii)) or in a node to the left of $i$ (case(v)). In the last case (Case 5) $i$ must be a boundary node and $\mop$must be in a node to the left of $i$.

We then compare $i$and $j$. The case were $i \lhd j$is similar to the previous implementation of the procedure. We compare $j$ with our potential pair. If $s_1 \lhd j$ then we can insert $r_2$ and $s_2$ into our output node arrays $R$ and $S$, respectively. We also set $s_1$ to $j$ and $s_2$ to the leftmost node in $Y[j]$. Then---both if $s_1 \lhd j$ or $s_1=j$---we set $r_1$ to $i$ and $r_2$ to the rightmost node in $X[i]$. We have thus updated $((r_1,r_2), (s_1,s_2))$ to be our new potential pair. That we only need the rightmost node in $X[i]$ and the leftmost node in $Y[j]$ follows directly from the definition of $\mop$.

Case 2 ($i \ntriangleleft j$) is more complicated. In this case we need to compute $\mop$ in the cluster $i$and $j$ belongs to. If this results in any minimum ordered pairs ($r \neq \emptyset$) we must update our potential pair.  As in the previous case we compare $s_1$ and $j$, but this time we must also add $r_1$ and $s_1$ to the output if $s_1 =j$ and no nodes in $X[i]$ are to the left of the leftmost node in $s_2$. To see this first note that since $r_1 \lhd i$ (the input is deep) we must have $r_1 \neq s_1$ and thus $s_2$ contains only one node $s'$. If $s'$ is to the  left of all nodes in $X[i]$ then no node in $X[i]$ can be in a minimum ordered pair with $s'$ and we can safely add our potential pair to the output. We then update our potential pair. Finally, we need to update $X[i]$, $i$, and $j$.  This update depends on which kind of macro nodes we have been working on. In Case (a) we either have $i=j$  or $i$is a left node and $j$is a spine node. In both cases we can have nodes in $X[i]$ that are to not to the left of any node in $Y[j]$. The rightmost of these nodes can be in a minimum ordered pair with a node from another macro node and we thus update $X[i]$ to contain this node only (if it exists).  Now all nodes in $Y[j]$must be to the left of all nodes in $X[i]$ in the next iteration and  thus we increment $j$. In Case (b) $i$is a spine node and $j$ is a right node. If $r_2= \emptyset$ then no node in $Y[j]$is to the right of the node in $X[i]$. Since the input arrays are deep, no node later in the array $X$ can be to the left of any node in $Y[j]$ and we therefore increment $j$. If $r_2 \neq \emptyset$ then the single node in $X[i]$is in the potential pair and we increment $i$. We do not increment $j$ as there could be nodes in $X[j]$to the left of the nodes in $Y[j]$.
When reaching the end of one of the arrays we add our potential pair to the output and return.

The correctness of the procedure follows from the proof of Lemma~\ref{t2:lem:nnm} and the above.
\\\\
Procedure \Match\ takes three node arrays $X$, $Y$, and $Y'$ representing deep sets $\mc{X}$, $\mc{Y}$, and $\mc{Y}'$, where 
$\norm{\mc{X}}=\norm{\mc{Y}}$, and $\mc{Y}' \subseteq \mc{Y}$. The output is a node array representing the set $\{\mc{X}_j \mid \mc{Y}_j \in \mc{Y'}\}$.
\begin{relate}
\item[$\Match(X,Y,Y')$]
Initialize a node array $R$  of size $n_M$, set $X_L:=\emptyset$, $Y_L:=\emptyset$, $Y_L':=\emptyset$, $x:=0$, $y:=0$, $i:=1$ and $j:=1$.

Repeat until $i>n_M$ or $j>n_M$:
    \begin{itemize}
    \item[] Until $X[i] \neq \emptyset$ set $i:=i+1$. Set $x:=\norm{X[i]}$.

    Until  $Y[j] \neq \emptyset$ set $j:=j+1$. Set $y:=\norm{Y[j]}$.


    Compare $Y[j]$ and $Y'[j]$. There are two cases:
        \begin{enumerate}
        \item $Y[j]=Y'[j]$. Compare $x$ and $y$. There are three cases:
            \begin{enumerate}
            \item $x=y$. Set $R[i]:=R[i] \cup X[i]$, $i:=i+1$, and $j:=j+1$.
            \item $x < y$. Set $R[i]:=R[i] \cup X[i]$, $
            Y[j]:=Y[j]\setminus \leftn(x,Y[j])$, $Y'[j]:=Y[j]$,  and $i:=i+1$.
            \item $x > y$. Set $X_L:=\leftn(y,X[i])$, $R[i]:=R[i] \cup X_L$, $X[i]:=X[i]\setminus X_L$, and $j:=j+1$.
            \end{enumerate}

        \item $Y[j]\neq Y'[j]$. Compare $x$ and $y$. There are three cases:
            \begin{enumerate}
            \item $x=y$. Set $R[i]:=R[i] \cup \match(X[i],Y[j],Y'[j])$, $i:=i+1$, and $j:=j+1$.
            \item $x < y$. Set $Y_L:=\leftn(x,Y[j])$, $Y'_L:=Y'[j] \cap Y_L$,
            
             $R[i]:=R[i] \cup \match(X[i],Y_L,Y'_L)$, $Y[j]:=Y[j]\setminus Y_L$, $Y'[j]:=Y'[j] \setminus Y'_L$,  and $i:=i+1$.

            \item $x > y$. Set $X_L:=\leftn(y,X[i])$, $R[i]:=R[i] \cup \match(X_L,Y[j],Y'[j])$, $X[i]:=X[i]\setminus X_L$, and $j:=j+1$.
            \end{enumerate}
        \end{enumerate}
    \end{itemize}
    Return $R$.
\end{relate}
Procedure $\Match$ proceeds as follows. First we find the next non-empty entries in the two node arrays $X[i]$ and $Y[j]$. We then compare $Y[j]$ and $Y'[j]$. 

If they are equal we keep all nodes in $X$ with the same rank as the nodes in $Y[j]$. We do this by splitting into three cases. If there are the same number of nodes $X[i]$ and $Y[j]$ we add all nodes in $X[i]$ to the output and increment $i$ and $j$. If there are more nodes in $Y[j]$than in $X[i]$ we add all nodes in $X[i]$ to the output and update $Y[j]$ to contain only the $y-x$ lefmost nodes in $Y[j]$. We then increment $i$ and iterate. If there are more nodes in $X[i]$than in $Y[j]$ we add the first $y$ nodes in $X[i]$to the output, increment $j$,  and update $X[i]$to contain only the nodes we did not add to the output. 

If $Y[j] \neq Y'[j]$ we call the cluster procedure \match. Again we split into three cases depending on the number of nodes in $X[i]$ and $Y[j]$. If they have the same number of nodes we can just call \match\ on $X[i]$, $Y[j]$, and $Y'[j]$ and increment $i$ and $j$. If $\norm{Y[j]} > \norm{X[i]}$ we call match with $X[i]$ the leftmost $\norm{X[i]}$ nodes of $Y[j]$ and with the part of $Y'[j]$ that are a subset of these leftmost $\norm{X[i]}$ nodes of $Y[j]$. We then update $Y[j]$ and $Y'[j]$ to contain only the nodes we did not use in the call to \match\ and increment $i$. If $\norm{Y[j]} < \norm{X[i]}$we call \match\ with the leftmost $\norm{Y[j]}$ nodes of $X[i]$, $Y[j]$, and $Y'[j]$. We then update $X[i]$ to contain only the nodes we did not use in the call to \match\ and increment $j$. 

It follows by induction on the rank of the elements that the procedure is correct.

\begin{relate}
\item[$\Fl(X, \alpha)$:] Initialize a node array $R$ of size $n_M$ and two node lists $L$ and $S$.

Repeat until $i>n_M$:
\begin{itemize}
\item[] Until $X[i] \neq \emptyset$ set $i:=i+1$. 

There are three cases depending on the type of $i$:
\begin{enumerate}
  \item $i \in \{l(v,w), r(v,w)\}$. Compute $N := \Fl_{C(i, s(v,w), v)}(X[i], \alpha)$.
  
  If $N \neq \emptyset$ for each $j \in \{i, s(v,w), v\}$ set $R[j] = R[j] \cup (N \cap V(C(j)))$. 
  
  Otherwise,
  set $L := L \circ \parent_{{M}}(v)$.
  \item  $i = l(v)$. Compute $N := \Fl_{C(i, v)}(X[i])$.
  
   If $N \neq \emptyset$ for each $j \in \{i, v\}$, set $R[j] :=R[j] \cup (N \cap V(C(j)))$. 
   
   Otherwise,
  set $L := L \circ \parent_{{M}}(v)$.
  \item $i \not\in \{l(v,w), r(v,w),l(v)\}$. Compute $N := \Fl_{C(i)}(X[i], \alpha)$.
  
  If $N \neq \emptyset$ set $R[i] := R[i] \cup N$. 
  
  Otherwise set $L := L \circ \parent_{{M}}(i)$.
\end{enumerate}
\end{itemize}
Subsequently, compute the list $S := \Fl_{M}(L, \alpha)$. For each node $i \in S$ set $R[i] := R[i] \cup \Fl_{C(S[i])}(\first(S[i]), \alpha))$. Return $R$.
\end{relate}
The $\Fl$ procedure is similar to $\Parent$. The cases 1, 2 and 3
compute $\Fl$ on a micro forest. If the result is within the micro
tree we add it to $R$ and otherwise we store the node in the
macro tree which contains the parent of the root of the micro forest in
a node list $L$. Since we always call \Deep\
on the output from $\Fl(X,\alpha)$ there is no need to
compute \Fl\ in the macro tree if $N$ is nonempty. We then compute $\Fl$ in the macro tree on the
list $L$, store the results in a list $S$, and use this to compute the final result.

Consider the cases of procedure \Fl.  In Case 1 $i$ is a left or right node. Due to Proposition~\ref{t2:lem:ancestorlemma} case (i) and (ii) $\fl$ of a node in $i$ can be in $i$ on the spine or in the top boundary node. If this is not the case it can be found  by a computation of $\Fl$ of the parent of the top boundary node of the $i$'s cluster in the macro tree (Proposition~\ref{t2:lem:ancestorlemma} case (iii)).
In Case 2 $i$is a leaf node. Then $\fl$ of a node in $i$ must either be in $i$, in the top boundary node, or can be found  by a computation of $\Fl$ of the parent of the top boundary node of the $i$'s cluster in the macro tree. If $i$ is a spine node or a boundary node   $\fl$ of a node in $i$ is either in $i$or can be found by a computation of $\Fl$ of the parent of $i$ in the macro tree. 
 
The correctness of the procedure follows from Proposition~\ref{t2:lem:ancestorlemma}, the
above, and the correctness of procedure $\Fl_{M}$.
































\ignore{******************* Omitted section correctness ******
\subsection{Correctness of the Set Procedures}
In this section we show the correctness of the mm-node set
implementation of the set procedures.

\begin{lemma}
Procedure $\Parent(X)$ is correct.
\end{lemma}
\ignore{\begin{proof}
Follows immediately by looking at all different kinds of macro
nodes, and by the comments below the implementation of the
procedure.
\qed \end{proof}
}
\begin{lemma}
Procedure $\Nca(X,Y)$ is correct.
\end{lemma}
\ignore{\begin{proof}
By induction on the rank of the elements. Let $x_l$ and $y_l$be the element of rank $l$in $X$ and $Y$, respectively.
Consider the base case $l=1$. Since $X[i]$ and $Y[j]$ are the first nonempty nodes in $X$ and $Y$ we have $x_1$ is the first node in  $X[i]$ and $y_1$ is the first node in $Y[j]$. Consider the two cases of the procedure. Case 1: If $i$is a boundary node then $C(i)$ only contains one node namely $x_1=y_1$ and thus the procedure computes $\nca(x_1,y_1)=x_1$. If $i$ is not a boundary node then we compare the sizes of the subsets represented by $X[i]$and $Y[i]$. If $\norm{X[i]} > \norm{Y[j]}$ then we compute $\nca$ of the first/leftmost $\norm{Y[j]}$ nodes in $X[i]$and the nodes in $Y[j]$. Since these two sets both include $x_1$ and $y_1$, respectively, as their leftmost element the procedure computes $\nca(x_1,y_1)$ as required. 
Otherwise we must have $\norm{X[i]} = \norm{Y[j]}$ due to the assumption that $x_i \prec y_i$, and again the procedure computes $\nca(x_1,y_1)$ as required. 
That $\nca(x_1,y_1)$ is computed correctly in Case 1(b) follows from Proposition~\ref{t2:lem:ncalemma} case (i), (ii), and (iii). Finally, we remove the nodes from $X[i]$ which we have just computed $\nca$for, and increment $j$ by one since we have now computed $\nca$ for all nodes in $Y[j]$. 

In Case 2 we first compare the sizes of the subsets represented by $X[i]$and $Y[i]$. If $\norm{X[i]} > \norm{Y[j]}$  we compute $\nca$ of the first/leftmost $\norm{Y[j]}$ nodes in $X[i]$and the nodes in $Y[j]$ as in Case 1(b). Again these two sets both include $x_1$ and $y_1$, respectively. If $\norm{X[i]} < \norm{Y[j]}$ we compute $\nca$ of the first/leftmost $\norm{X[i]}$ nodes in $Y[j]$and the nodes in $X[i]$. Again these two sets both include $x_1$ and $y_1$, respectively, as their leftmost element. Otherwise $\norm{X[i]} = \norm{Y[j]}$ and we use these two sets in the computation. We now compute $\nca_M(i,j)$.  It follows from Proposition~\ref{t2:lem:ncalemma} case (iv), (v), (vi), and (vii) that $\nca(x_1,y_1)$ is computed correctly. Finally, we update $X[i]$ and $Y[j]$ such that they only contains nodes for which we have not calculated $\nca$.

For the induction step assume that $\nca(x_{l-1},y_{l-1})$ have just been computed correctly. If $x_l$ and $y_l$ is were in the same macro nodes as $x_{l-1}$ and $y_{l-1}$, respectively, then it is easy to verify that $\nca(x_l,y_l)$ was also computed correctly at the same time. Now assume that at least one of $x_l$ and $y_l$ is in another macro node than its predecessor. Due to the update of $X[i]$ and $Y[j]$ in the last iteration we must have $x_{l-1}$ and $y_{l-1}$ as the first/leftmost node in $X[i]$ and $Y[j]$, respectively,  in this iteration. In now follows by the same arguments as in the base case that $\nca(x_l,y_l)$  is correctly computed by the procedure.
\qed \end{proof}}

\begin{lemma}
Procedure $\Deep(X)$ is correct.
\end{lemma}
\ignore{*********
\begin{proof}
Follows by the proof of Lemma~\ref{t2:lem:deep}, by looking at all different kinds of macro
nodes, and by the comments below the implementation of the
procedure.
\qed \end{proof}
***********}

\ignore{*************************************************
To prove the correctness of procedure \Mop\ we need the following
proposition.
\begin{prop}\label{t2:lem:nnlemma}
    Let $\mc{R}=[(r_i,M(r_i))\mid 1 \leq i \leq k]$ and
    $\mc{S}=[(s_i,M(s_i))\mid 1 \leq i \leq l]$ be deep, canonical lists.
    For any pair of nodes $r \in M(r_i)$, $s
    \in M(s_j)$ for some $i$ and $j$, then $(r,s)\in
    \mop_T(S(\mc{R}),S(\mc{S}))$ iff one of
    the following cases are true:
    \begin{enumerate}
    \item[(i)] $r_i=s_j$ and $(r,s) \in \mop_{I(r_i)}(M(r_i),M(s_j))$.

    \item[(ii)] $r_i = l(v,w)$, $s_j=s(v,w)$ and $(r,s)\in
    \mop_{I(r_i,s_j)}(M(r_i),M(s_j))$.

    \item[(iii)] $r_i = s(v,w)$, $s_j=r(v,w)$ and $(r,s)\in
    \mop_{I(r_i,s_j)}(M(r_i),M(s_j))$.

    \item[(iv)] $r_i = l(v,w)$, $s_j=r(v,w)$, $r_{i+1}\neq
    s(v,w)$, $s_{j-1}\neq s(v,w)$, $r=\rn(M(r_i))$, $s=\leftn(M(s_j))$, and $(r_i,s_j)\in
        \mop_{T^M}(\restrict{\mc{R}}{1},\restrict{\mc{S}}{1})$.

    \item[(v)] $r_i, s_j \in C \in CS$, $r_i \neq s_j$, either $r_i$
    or $s_j$ is the bottom boundary node $w$ of $C$,
    $r=\rn(M(r_i))$, $s=\leftn(M(s_j))$, and $(r_i,s_j)\in
        \mop_{T^M}(\restrict{\mc{R}}{1},\restrict{\mc{S}}{1})$.

    \item[(vi)] $r_i \in C_1 \in CS$, $ s_j \in C_2 \in CS$, $C_1 \neq C_2$,
        $r=\rn(M(r_i))$, $s=\leftn(M(s_j))$, and $(r_i,s_j)\in
        \mop_{T^M}(\restrict{\mc{R}}{1},\restrict{\mc{S}}{1})$.
    \end{enumerate}
\end{prop}
The proposition follows immediately, by considering all cases for
$r_i$ and $s_j$, i.e., $r_i=s_j$, $r_i$ and $s_j$ are in the same
cluster, and $r_i$ and $s_j$ are not in the same cluster. Using
Proposition~\ref{t2:lem:nnlemma} we get
***********************}

\begin{lemma}
Procedure $\Mop(X,Y,Z)$ is correct.
\end{lemma}

\ignore{**************************
\begin{proof}

Let $(x,M(x))=\mc{X}'[i]$ and $(z,M(z))=\mc{Z}[i]$. We call $r,t$
a corresponding pair in $(M(x),M(z))$ iff $r$ and $t$ are the
$i$th node in the left to right order of $M(x)$ and $M(z)$,
respectively. Let $$S:=\{(r,s)\mid r,t \textrm{ corresponding pair
in } (M(x),M(z)), \textrm{ and } (t,s) \in
\mop_T(S(\mc{Z}),S(\mc{Y}))\}.$$ We first show $(v_x,v_y) \in S
\Rightarrow (v_x,v_y)$ is a corresponding pair in
$(\mc{R}[i]_1,\mc{R}[i]_2)$. Let $(v_z,v_y)$ be the pair in
$\mop_T(S(\mc{Z}),S(\mc{Y}))$, where $v_z \in M(z_i)$ and $v_y \in
M(y_j)$, and look at each of the cases from
Proposition~\ref{t2:lem:nnlemma}.
\begin{itemize}
\item[-] Case (i), (ii), and (iii). This is case 2 in the
procedure. We have $v_x \in M$ and $v_y \in M_2$, which are both
added to $\mc{R}$.

\item[-] Case (iv), (v), and (vi). This is case 1 in the
procedure. Here we set $r:=(x,\rn_{I(x)}(M(x)))$ and
$s:=(y,\leftn_{I(y)}(M(y)))$, where such $v_x \in M(x)$ and $v_y
\in M(y)$. We need to show that $(r,s)$ is added to $\mc{R}$
before $r$ and $s$ are changed. If the next case is (i) again then
it follows from the fact that $(z_i,y_j) \in
\mop_{T^M}(\restrict{\mc{Z}}{1},\restrict{\mc{Y}}{1})$. If the
next case is $(ii)$ then we must have $s \lhd y$ or $s=y$ and
$\leftof_{I(z)}(M(z),M(y))=\true$ since $(z_i,y_j) \in
\mop_{T^M}(\restrict{\mc{Z}}{1},\restrict{\mc{Y}}{1})$.
\end{itemize}
We now show if $ (v_x,v_y)$ is a corresponding pair in
$(\mc{R}[i]_1,\mc{R}[i]_2)$ then $(v_x,v_y) \in S$. Look at the
two cases from the procedure. In case 1 we set
$r:=(x,\rn_{I(x)}(M(x)))$, $s:=(y,\leftn_{I(y)}(M(y)))$ because $z
\lhd y$. The pair $(r,s)$ is only added to $\mc{R}$ if there is no
other $z' \in \restrict{Z}{1}$, $z \lhd z'$ such that $z' \lhd y$,
or if  $z' = \lhd y$ and there are nodes in $M(y)$ to the left of
all nodes in $M(z')$. This corresponds to case (iv), (v), or (vi)
in Proposition~\ref{t2:lem:nnlemma}. In case 2 it is straightforward
to verify that it corresponds to one of the cases (i), (ii), or
(iii) in Proposition~\ref{t2:lem:nnlemma}.
\qed \end{proof}
************}

\begin{lemma}
Procedure $\Fl(X,\alpha)$ is correct.
\end{lemma}
\ignore{************
\begin{proof}
We only need to show that case 1, 2  and 3 correctly computes \Fl\ on
a micro forest. That the rest of the procedure is correct follows
from case (iii) in Proposition~\ref{t2:lem:ancestorlemma}, the
comments after the implementation, and the correctness of procedure $\Fl_{T^M}$.

That case 1, 2 and 3 are correct follows from
Proposition~\ref{t2:lem:ancestorlemma}. Since we always call \Deep\
on the output from $\Fl(X,\alpha)$ there is no need to
compute \Fl\ in the macro tree if $N$ is nonempty.
\qed \end{proof}
*************}
***** omitted section correctness **********}


\subsection{Complexity of the Tree Inclusion Algorithm}
To analyse the complexity of the node array implementation we first bound the running time of the above implementation of the set procedures. All procedures scan the input from left-to-right while gradually producing the output. In addition to this procedure $\Fl$ needs a call to a node list implementation of $\Fl$ on the macro tree. Given the data structure described in Section~\ref{t2:sec:preprocessing} it is easy to check that each step in the scan can be performed in $O(1)$ time giving a total of $O(n_{T}/\log n_T)$ time. Since the number of nodes in the macro tree is $O(n_{T}/\log n_T)$ the call to the node list implementation of $\Fl$ is easily done within the same time. Hence, we have the following lemma.
\begin{lemma}\label{t2:lem:auxmacro}
For any tree $T$ there is a data structure using $O(n_T)$ space
and $O(n_T\log n_{T})$ preprocessing time which supports all of the
set procedures in $O(n_T/\log n_T)$ time.
\end{lemma}
Next consider computing the deep occurrences of $P$ in $T$ using
the procedure $\Emb$ of Section~\ref{t2:sec:recursion} and
Lemma~\ref{t2:lem:auxmacro}. Since each node $v \in V(P)$ contributes
at most a constant number of calls to set procedures it follows
immediately that,
\begin{theorem}\label{t2:thm:complex}
For trees $P$ and $T$ the tree inclusion problem can be solved in
$O(n_Pn_T/\log n_T + n_{T}\log n_{T})$ time and $O(n_T)$ space.
\end{theorem}
Combining the results in Theorems~\ref{t2:thm:simple},
\ref{t2:thm:complex} and Corollary~\ref{t2:cor:simple} we
have the main result of Theorem~\ref{t2:thm:main}.



