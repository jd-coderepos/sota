\chapter{External Memory Soft Heap and Hard Heap, a Meldable Priority Queue}
\label{emsh:chapt}
\section{Introduction}
An external memory version of soft heap that we call ``External Memory Soft Heap''
(EMSH for short) is presented. 
It supports {\tt Insert}, {\tt Findmin}, {\tt Deletemin} and {\tt Meld} operations.
An EMSH may, as in its in-core version, and at its discretion, 
corrupt the keys of some elements in it, by revising them upwards.
But the EMSH guarantees that the number of corrupt elements in it is never more than
$\epsilon N$, where $N$ is the total number of items inserted in it, and $\epsilon$ is
a parameter of it called the error-rate.
The amortised I/O complexity of an {\tt Insert} is 
$O(\frac{1}{B} \log_{m}\frac{1}{\epsilon})$, where $m = \frac{M}{B}$.
{\tt Findmin}, {\tt Deletemin} and {\tt Meld} all have non-positive amortised I/O complexities.

When we choose an error rate $\epsilon<1/N$, EMSH stays devoid of corrupt nodes, 
and thus becomes a meldable priority queue that we call ``hard heap''.  
The amortised I/O complexity of an {\tt Insert}, in this case, is $O(\frac{1}{B} \log_{m}\frac{N}{B})$,
over a sequence of operations involving $N$ {\tt insert}s.
{\tt Findmin}, {\tt Deletemin} and {\tt Meld} all have non-positive amortised I/O complexities.
If the inserted keys are all unique, a {\tt Delete} (by key) operation can
	also be performed at an amortised I/O complexity of $O(\frac{1}{B} \log_{m}\frac{N}{B})$.
A balancing operation performed once in a while on a hard heap 
ensures that the number of I/Os performed by a sequence of $S$ operations on it is
$O(\frac{S}{B}+\frac{1}{B}\sum_{i = 1}^{S} \log_{m}\frac{N_i}{B})$, where
$N_i$ is the number of elements in the heap before the $i$th operation.

\subsection{Definitions}
A priority queue is a data structure used for maintaining a set $S$ of elements, 
	where each element has a key drawn from a linearly ordered set.
A priority queue typically supports the following operations:
\begin{enumerate}
\item {\tt Insert}$(S,x)$: Insert element $x$ into $S$.
\item {\tt Findmin}$(S)$: Return the element with the smallest key in $S$
\item {\tt Deletemin}$(S)$: Return the element with the smallest key in $S$ and remove it from $S$.
\item {\tt Delete}$(S,x)$: Delete element $x$ from $S$
\item {\tt Delete}$(S,k)$: Delete the element with key $k$ from $S$
\end{enumerate}
Algorithmic applications of priority queues abound \cite{AHU74,CLR90}.

Soft heap is an approximate meldable priority queue devised
	by Chazelle \cite{Ch00a}, and supports
	{\tt Insert}, {\tt Findmin}, {\tt Deletemin}, {\tt Delete}, and
	{\tt Meld} operations. 
A soft heap, may at its discretion, corrupt the keys of some elements in it, by revising them upwards.
A {\tt Findmin} returns the element with the smallest current key, which may or may not be corrupt. 
A soft heap guarantees that the number of corrupt elements in it is never more than
$\epsilon N$, where $N$ is the total number of items inserted in it, and $\epsilon$ is
a parameter of it called the error-rate.   
A {\tt Meld} operation merges two soft heaps into one new soft heap.

\subsection{Previous Results}
I/O efficient priority queues have been reported before \cite{Arge03,BK98,FJKT99,KS96,MZ03}.
An I/O efficient priority queue, called \emph{buffer tree}  was introduced by 
        Arge \cite{Arge03}.
A buffer tree is an $(a,b)$ tree, where $a = M/4$ and $b = M$, and each
        node contains a buffer of size $\Theta(M)$.
Buffer tree supports the {\tt Insert}, {\tt  Deletemin}, {\tt Delete} (by key), 
	and offline search operations.
The amortised complexity of each of these operations on buffer tree is
        $O(\frac{1}{B} \log_{m}\frac{N}{B})$
        I/Os, over a sequence of operations of length $N$.
External memory versions of heap are presented in \cite{KS96} and \cite{FJKT99}. 
The heap in \cite{KS96} is a $\sqrt{m}$-way tree, each node of which contains a buffer 
of size $\Theta(M)$; it supports {\tt Insert}, {\tt Deletemin}, and {\tt Delete} (by key) operations.
The amortised cost of each operation on this heap is
        $O(\frac{1}{B} \log_{m}\frac{N}{B})$
        I/Os, where $N$ is the total number of
        elements in the heap.
The heap in \cite{FJKT99} is an $m$-way tree and it does not contain a buffer at each node.
It supports {\tt Insert}, and {\tt Deletemin} operations.
For this heap, the total number of I/Os performed by a sequence of $S$
        operations is $O(\frac{S}{B}+\frac{1}{B}\sum_{i = 1}^{S} \log_{m}\frac{N_i}{B})$, where$N_i$ is the number
        of elements in the heap before the $i$th operation.

An external memory version of tournament tree that supports 
        the {\tt Deletemin}, {\tt Delete}, and {\tt Update}
        operations is also presented in \cite{KS96};
this is a complete binary tree.
An {\tt Update}$(x,k)$ operation changes the key of element $x$ to $k$
        if and only if $k$ is smaller than the present key of $x$.
The amortised cost of each operation on this data structure is $O(\frac{1}{B} \log_2 \frac{N}{B})$ I/Os \cite{KS96}.

The priority queue of \cite{BK98} maintains a hierarchy of sorted lists in secondary memory.
An integer priority is presented in \cite{MZ03}.
See \cite{BCF+00} for an experimental study on some of these priority queues.
Numerous applications of these data structure have also been reported:
graph problems, computational geometry problems and sorting to name a few
\cite{Arge03,FJKT99,KS96}.

See Table \ref{comparison:emsh} for a comparison of hard heap with the
        priority queues of \cite{Arge03,FJKT99,KS96}.

Soft heap is an approximate meldable priority queue devised
        by Chazelle \cite{Ch00a}, and supports
        {\tt Insert}, {\tt Findmin}, {\tt Deletemin}, {\tt Delete}, and
        {\tt Meld} operations. 
This data structure is used in computing minimum spanning
        trees \cite{Ch00b} in the fastest known in-core algorithm for the problem.
Soft heap has also been used for finding exact and approximate
    medians, and for approximate sorting \cite{Ch00a}.
An alternative simpler implementation of soft heap 
is given by Kaplan and Zwick \cite{KaZw09}.

\begin{table}
\begin{center}
            \begin{tabular}{|l|l|l|l|l|l|}
\cline{1-6}
               \hline
{\bf Properties}  &   {\bf buffer tree}     & {\bf heap} \cite{KS96}      & {\bf heap} \cite{FJKT99}    & {\bf tourn. tree}
   & {\bf EMSH} \\
\hline \hline
type of tree &  $(M/4,M)$   & $\sqrt{m}$-way & $m$-way & complete & set of
                                                                      $\sqrt{m}$- \\
             &  tree        & tree           & tree    & binary tree & way trees \\
\hline
size of a     & $\Theta(M)$              & $\Theta(\sqrt{m}B)$  & $\Theta(M)$        & $\Theta(M
)$    & $\Theta(\sqrt{m}B)$ \\
node          &                  &                &          &        &        \\
\hline
buffer of size &    yes          &   yes          & no       & yes    & no  \\
$M$ at each    &                 &                &          &        &   \\
node            &                 &                &          &        &   \\
\hline
extra space    & $\Theta(N)$     &  $\Theta(\sqrt{m}N)$ & 0   & $\Theta(N)$ & 0 \\
\hline
operations     & {\tt Insert} & {\tt Insert}    & {\tt Insert}    & {\tt Delete}    & {\tt Insert} \\
               & {\tt Delete} & {\tt Delete}    & {\tt Deletemin} & {\tt Deletemin} & {\tt Delete} \\
            & {\tt Deletemin} & {\tt Deletemin} & {\tt Findmin}   & {\tt Findmin}   & {\tt Deletemin} \\
            & {\tt Findmin}   & {\tt Findmin}   &                 & {\tt Update}& {\tt Findmin}, {\tt Meld} \\
\hline
\end{tabular}
\caption{Comparison with some known priority queues }
\label{comparison:emsh}
\end{center}
\end{table}
\subsection{Our Results}
In this chapter, we present an external memory version of soft heap that permits
batched operations. We call our data structure ``External Memory Soft Heap''
(EMSH for short). 
As far as we know, this is the first implementation of soft heap
on an external memory model. 
When we choose an error rate $\epsilon<1/N$, EMSH stays devoid of corrupt nodes, 
and thus becomes a meldable priority queue that we call ``hard heap''.  


EMSH is an adaptation of soft heap for this model.
It supports {\tt Insert}, {\tt Findmin}, {\tt Deletemin} and {\tt Meld} operations.
An EMSH may, as in its in-core version, and at its discretion, 
corrupt the keys of some elements in it, by revising them upwards.
But it guarantees that the number of corrupt elements in it is never more than
$\epsilon N$, where $N$ is the total number of items inserted in it, and $\epsilon$ is
a parameter of it called the error-rate.
The amortised I/O complexity of an {\tt Insert} is $O(\frac{1}{B} \log_{m}\frac{1}{\epsilon})$.
{\tt Findmin}, {\tt Deletemin} and {\tt Meld} all have non-positive amortised I/O complexities.

A hard heap (an EMSH with $\epsilon < 1/N$) does not have any corrupt element.
Therefore, it is an exact meldable priority queue.
The amortised I/O complexity of an {\tt Insert}, in this case, is $O(\frac{1}{B} \log_{m}\frac{N}{B})$.
{\tt Findmin}, {\tt Deletemin} and {\tt Meld} all have non-positive amortised I/O complexities.
If the inserted keys are all unique, a {\tt Delete} (by key) operation can
	also be performed at an amortised I/O complexity of $O(\frac{1}{B} \log_{m}\frac{N}{B})$.



\subsection{Organisation of This Chapter}

This chapter is organised as follows: Section~\ref{datastr:emsh} describes the data structure.
The correctness of the algorithm is proved in Section~\ref{correct:emsh}.
The amortised I/O analysis of the algorithm is presented in Section~\ref{analy:emsh}.
EMSH with $\epsilon<1/N$ is discussed in Section~\ref{hard:emsh}.
Some of its applications are shown in Section~\ref{appl:emsh}
\section{The Data Structure}
\label{datastr:emsh}
An EMSH consists of a set of trees on disk.
The nodes of the trees are classified as follows.  
A node without a child is a leaf. 
A node without a parent is a root.
A node is internal, if it is neither a leaf, nor a root. 

Every non-leaf in the tree has at most $\sqrt{m}$ children.
Nodes hold pointers to their children. 

Every node has a rank associated with it at the time of its creation. 
The rank of a node never changes.
All children of a node of rank $k$ are of rank $k-1$. 
The rank of a tree $T$ is the rank of $T$'s root. 
The rank of a heap $H$ is $\max\{\mbox{rank}(T) \ |\ T\in H\}$. 
An EMSH can have at most $\sqrt{m}-1$ trees of any particular rank.

Each element held in the data structure has a key drawn from a 
	linearly ordered set. 
We will treat an element and its key as indistinguishable.

Each instance of EMSH has an associated error-rate $\epsilon>0$. 
Define $r = \log_{\sqrt{m}}{1/\epsilon}$.
Nodes of the EMSH with a rank of at most $r$ are called {\em pnodes} (for ``pure nodes''), and 
	nodes with rank greater than $r$ are called {\em cnodes} (for ``corrupt nodes'').
Each {\em pnode} holds an array that contains elements in sorted order.
A tree is a ptree if its rank is at most $r$, and a ctree otherwise.

We say that a pnode $p$ satisfies the \emph{pnode invariant} (PNI), if 
\begin{quote}
	$p$ is a non-leaf and the array in $p$ contains at most 
	$B\sqrt{m}$ and at least $B\sqrt{m}/2$ elements, or \\
	$p$ is a leaf and the array in $p$ contains at most 
	$B\sqrt{m}$ elements.
\end{quote}
Note that a pnode that satisfies PNI may contain less than $B\sqrt{m}/2$ elements,
	if it is a leaf.

Every cnode has an associated doubly linked list of {\em listnodes}. 
A cnode holds pointers to the first and last listnodes of its list. 
The size of a list is the number of listnodes in it. 
Each listnode holds pointers to the next and previous
	listnodes of its list; the next (resp., previous)
        pointer of a listnode $l$ is null if $l$  is the last (resp., first)
        of its list.
Each listnode contains at most $B\sqrt{m}$, and unless it is the last of a list,
	at least $B\sqrt{m}/2$  elements. 
The last listnode of a list may contain less than $B\sqrt{m}/2$ elements.

Let $s_k$ be defined as follows:
\begin{equation*}
s_k =
 \begin{cases}
 0 & \text{if $k \leq r$} \\
 2 & \text{if $k = r+1$} \\
 \lceil \frac{3}{2} s_{k-1} \rceil & \text{if $k > r+1$}
 \end{cases}
\end{equation*}

We say that a cnode $c$ that is a non-leaf  
	satisfies the \emph{cnode invariant} (CNI), if 
	the list of $c$ has a size of at least $\lfloor s_k/2 \rfloor + 1$. 
A leaf cnode always satisfies CNI. 
	

\begin{table}
\begin{center}
            \begin{tabular}{|l|l|l|l|}
\cline{1-4} \hline
 {\bf Type of node} & {\bf Number of} & {\bf Number of elements in a} & {\bf Size of the list of a} \\
              & {\bf children} & {\bf pnode of this type} & {\bf cnode of this type} \\
              &          & {\bf if it satisfies PNI} & {\bf if it satisfies CNI} \\
\hline \hline
               leaf node & $0$ & $\leq B\sqrt{m}$ & $\geq 1$  \\
\hline
               root node & $\leq\sqrt{m}$ & $\geq B\sqrt{m}/2$; $\leq B\sqrt{m}$  & $\geq \lfloor s_k/2 + 1\rfloor$ \\
\hline
               internal node & $\leq\sqrt{m}$ & $\geq B\sqrt{m}/2$; $\leq B\sqrt{m}$ & $\geq \lfloor s_k/2 + 1\rfloor$  \\
\hline
            \end{tabular}
\caption{Types of nodes in the data structure, and the invariants on them}
\label{tab:data:structure:emsh}
\end{center}
\end{table}
 
Table \ref{tab:data:structure:emsh} summarizes the different types of nodes in an EMSH, 
the number of children each can have, and the PNI and CNI constraints on each.

Every cnode has a {\em ckey}.
For an element $e$ belonging to the list of a cnode $v$,
	the ckey of $e$ is the same as the ckey of $v$; 
	$e$ is called corrupt if its ckey
	is greater than its key.

An EMSH is said to satisfy the heap property if the following
	conditions are met:
For every cnode $v$ of rank greater than $r+1$, the ckey of $v$ is smaller than 
	the ckey of each of $v$'s children.
For every cnode $v$ of rank $r+1$, the ckey of $v$ is smaller than
	each key in each of $v$'s children.
For every pnode $v$, each key in $v$ is smaller than each key in each of $v$'s
	children.

For each rank $i$, we maintain a bucket $B_{i}$ for the roots of rank $i$. 
We store the following information in $B_i$:
\begin{enumerate}
\item the number of roots of rank $i$ in the EMSH; there are at most $\sqrt{m}-1$ such roots.
\item pointers to the roots of rank $i$ in the EMSH.
\item if $i>r$ and $k=\min\{\mbox{ckey}(y)\ |\ y \mbox{ is a root of rank } i\}$
	then a listnode of the list associated with the root of rank $i$, whose ckey value is $k$;
	this listnode will not be the last of the list, unless the list has only one listnode. 
\item if $i \leq r$ then the $n$ smallest of all elements in the roots of rank $i$, 
	for some $n\leq B\sqrt{m}/2$
\item a pointer suffixmin$[i]$
\end{enumerate}

We define the minkey of a tree as follows:
for a ptree $T$, the minkey of $T$ is defined as the 
	smallest key in the root of $T$;
for a ctree $T$, the minkey of $T$ is the ckey of the root of $T$.
The minkey of a bucket $B_{i}$ is the smallest of the minkeys of the trees of rank $i$ in the EMSH;
	$B_{i}$ holds pointers to the roots of these trees.  
The suffixmin pointer of $B_{i}$ points to the bucket with the smallest minkey
	among $\{B_{x}\ |\ x\geq i\}$.  

For each bucket, we keep the items in 1, 2 and 5 above, and at most a block of the elements 
	(3 or 4 above) in the main memory.
When all elements of the block are deleted by {\tt Deletemin}s, the next 
	block is brought in.
The amount of main memory needed for a bucket is, thus, $O(B+\sqrt{m})$. 
As we shall show later, the maximum rank in the data structure, and so the number
	of buckets is $O(\log_{\sqrt{m}} (N/B))$. Therefore, if 
	$N = O(B m^{M/2(B + \sqrt{m})})$ 
	the main memory suffices for all the buckets.
(See Subsection~\ref{memory:requirement}). 

We do not keep duplicates of elements. 
All elements and listnodes that are taken into the buckets would be 
	physically removed from the respective roots.
But these elements and listnodes would still be thought of as belonging to their
	original positions.
For example, the above definition of minkeys assumes this.

A bucket $B_i$ becomes {\em empty}, irrespective of the value of $i$,
when all the elements in it have been deleted. 

	



\subsection{The Operations}

In this section we discuss the {\tt Insert}, {\tt Deletemin}, {\tt Findmin}, {\tt Meld}, {\tt Sift} and {\tt Fill-Up} 
    operations on EMSH. The first four are the basic operations. 
The last two are auxiliary.
The {\tt Sift} operation is invoked only on non-leaf nodes that fail to satisfy the 
	pnode-invariant (PNI) or cnode-invariant (CNI), whichever is relevant.
When the invocation returns, the node will satisfy the invariant.
Note that PNI applies to pnodes and CNI applies to cnodes. 
{\tt Fill-Up} is invoked by the other operations on a bucket when they find it empty.

\subsubsection{{\tt Insert}}
For each heap, a buffer of size $B\sqrt{m}$ is maintained in the main memory. 
If an element $e$ is to be inserted into heap $H$, store it in the buffer of $H$. 
The buffer stores its elements in sorted order of key values. 
If the buffer is full (that is, $e$ is the $B\sqrt{m}$-th  element of the buffer), 
	create a new node $x$ of rank $0$, and copy all elements in the buffer into it.
The buffer is now empty.
Create a tree $T$ of rank $0$ with $x$ as its only node. 
Clearly, $x$ is a root as well as a leaf. 
Construct a new heap $H'$ with $T$ as its sole tree. 
Create a bucket $B_0$ for $H'$, set the number of trees in it to $1$, and include a pointer to $T$ in it.
Invoke {\tt Meld} on $H$ and $H'$.

\subsubsection{{\tt Deletemin}}
A {\tt Deletemin} operation is to delete and return an element with the smallest key in the EMSH.
The pointer suffixmin$[0]$ points to the bucket with the smallest minkey.
A {\tt Deletemin} proceeds as in Figure~\ref{fig:deletemin}.

\begin{figure}
\vspace{0.2in}
\hrule
\begin{tabbing}
aaaa \= aaaa \= aaaa \= aaaa \= aaaa \= aaaa \kill \\
Let $B_i$ be the bucket pointed by suffixmin$[0]$; \\
let $e$ be the smallest element in the insert buffer; \\
if the key of $e$ is smaller than the minkey of $B_i$ then \\
	\> delete $e$ from the insert buffer, and return e; \\
\\
if $i\leq r$ then \\
	\> the element with the smallest key in the EMSH is in bucket $B_{i}$; \\
	\> let it be $e$; delete $e$ from $B_{i}$; \\
	\> if $B_{i}$ is not empty, then \\ 
	\>	\> update its minkey value; \\

else \\
	\> let $x$ be the root of the tree $T$ that lends its minkey to $B_{i}$; the ckey of $x$ is smaller \\
	\> than all keys in the pnodes and all ckeys; $B_{i}$ holds elements from a listnode $l$ of $x$;\\
	\> let $e$ be an element from $l$;  delete $e$ from $l$; \\
\\
if $B_{i}$ is empty then \\
	\> fill it up with an invocation to {\tt Fill-Up}(), and update $B_i$'s minkey value; \\
\\
update the suffixmin pointers of buckets $B_{i}, \ldots, B_{0}$; \\ 
return $e$; 
\end{tabbing}
\vspace{0.1in}
\hrule
\vspace{0.2in}
\caption{Deletemin}
\label{fig:deletemin}
\end{figure}

Note that if after a {\tt Deletemin}, a root fails to satisfy the relevant invariant,
then a {\tt Sift} is not called immediately. We wait till the next {\tt Fill-Up}.
(While deletions happen in buckets, they are counted against the root from which
the deleted elements were taken. Therefore, a deletion can cause the corresponding root
to fail the relevant invariant.)
 
Recall that we keep at most a block of $B[i]$'s elements in the main memory.
When all elements of the block are deleted by {\tt Deletemin}s, the next block is
	brought in.

\subsubsection{{\tt Findmin}}
A {\tt Findmin} return the same element that a {\tt Deletemin} would.
But the element is not deleted from the EMSH. 
Therefore, a {\tt Findmin} does not need to perform any of the updations
that a {\tt Deletemin} has to perform on the data structure.

As it is an in-core operation, a {\tt Findmin} does not incur any I/O. 

\subsubsection{{\tt Meld}}

In the {\tt Meld} operation, two heaps $H_1$ and $H_2$ are to be merged
        into a new heap $H$.
It is assumed that the buckets of the two heaps remain in the main memory.

Combine the input buffers of $H_1$ and $H_2$.
If the total number of elements exceeds $B\sqrt{m}$, then
create a new node $x$ of rank $0$, move $B\sqrt{m}$ elements from the buffer into it
leaving the rest behind, create a tree $T$ of rank $0$ with $x$ as its only node, 
create a bucket $B_0'$, set the number of trees in it to $1$, and include a pointer to $T$ in it.

Let $B_{1,i}$ (resp., $B_{2,i}$) be the $i$-th bucket of $H_1$ (resp., $H_2$).
Let max denote the largest rank in the two heaps $H_1$ and $H_2$. The {\tt Meld} is analogous to
the summation of two $\sqrt{m}$-radix numbers of $\max$ digits. At position $i$,
buckets $B_{1,i}$ and $B_{2,i}$ are the ``digits''; there could also be
a ``carry-in'' bucket $B'_{i}$. The ``summing'' at position $i$ produces a new
$B_{1,i}$ and a ``carry-out'' $B'_{i+1}$. 
$B_0'$ will function as the ``carry in'' for position $0$.

The {\tt Meld} proceeds as in Figure~\ref{fig:meld}.

\begin{figure}
\vspace{0.2in}
\hrule
\begin{tabbing}
aaaa \= aaaa \= aaaa \= aaaa \= aaaa \= aaaa \kill \\
for $i=0$ to max$+1$ \\
begin \\
\> if only one of $B_{1,i}$, $B_{2,i}$ and $B'_{i}$ exists then \\
\>\>that bucket becomes $B_{1,i}$; {\tt Fill-Up} that bucket, if necessary; \\
\>\>there is no carry-out; \\
\> else \\
\>\>if $i\leq r$ \\
\>\>\>if $B_{1,i}$ (resp., $B_{2,i}$) contains elements then \\
\>\>\>\> send the elements of $B_{1,i}$ (resp., $B_{2,i}$) back to the \\
\>\>\>\>roots from which they were taken; \\ 
\>\>\>for each root $x$ pointed by $B_{1,i}$ or $B_{2,i}$ \\ 
\>\>\>\>if $x$ does not satisfy PNI, invoke {\tt Sift}$(x)$; \\ 
\>\>else \\ 
\>\>\>if $B_{1,i}$ (resp., $B_{2,i}$) and the last listnode $l_1$ (resp., $l_2$) \\
\>\>\>of the root $x_1$ (resp., $x_2$) with the smallest ckey in $B_{1,i}$ (resp., $B_{2,i}$) \\
\>\>\>have sizes $<B\sqrt{m}/2$ each, then \\ 
\>\>\>\>merge the elements in $B_{1,i}$ (resp., $B_{2,i}$) into $l_1$ (resp., $l_2$); \\ 
\>\>\>else \\ 
\>\>\>\>store the elements in $B_{1,i}$ (resp., $B_{2,i}$) in a new listnode $l$\\ 
\>\>\>\>and insert $l$ into the list of $x_1$ (resp., $x_2$) so that \\
\>\>\>\>all but the last listnode will have $\geq B\sqrt{m}/2$ elements; \\ 
\>\>\>\>if $x_1$ (resp., $x_2$) does not satisfy CNI, then {\tt Sift} it; \\ 
\>if the total number of root-pointers in $B_{1,i}$, $B_{2,i}$ and $B'_{i}$ is $<\sqrt{m}$, then  \\
\>\>move all root-pointers to $B_{1,i}$; {\tt Fill-Up} $B_{1,i}$; \\
\>\>delete $B_{2,i}$ and $B'_{i}$; There is no carry-out; \\
\end{tabbing}
\hfill {CONTINUED}
\label{fig:meld1}
\end{figure}

\begin{figure}
\begin{tabbing}
aaaa \= aaaa \= aaaa \= aaaa \= aaaa \= aaaa \kill \\
\>else \\
\>\>create a tree-node $x$ of rank $i+1$; \\
\>\>pool the root-pointers in $B_{1,i}$, $B_{2,i}$ and $B'_{i}$; \\
\>\>take $\sqrt{m}$ of those roots and make them children of $x$; {\tt Sift}$(x)$; \\
\>\>create a carry-out bucket $B'_{i+1}$; \\ 
\>\>place in it a pointer to $x$; this is to be the only root-pointer of $B'_{i+1}$; \\ 
\>\>move the remaining root-pointers into $B_{1,i}$; {\tt Fill-Up} $B_{1,i}$; \\
\>\>delete $B_{2,i}$ and $B'_{i}$; \\
end; \\
update the suffixmin pointers; \\
\end{tabbing}
\vspace{0.1in}
\hrule
\vspace{0.2in}
\caption{Meld}
\label{fig:meld}
\end{figure}

\subsubsection{{\tt Sift}}

The {\tt Sift} operation is invoked only on non-leaf nodes that fail to satisfy 
	PNI or CNI, whichever is relevant.
When the invocation returns, the node will satisfy the invariant.
We shall use in the below a procedure called
	extract that is to be invoked only on cnodes of rank $r+1$, 
	and pnodes, and is defined in Figure~\ref{fig:extract}.

\begin{figure}[ht]
\vspace{0.2in}
\hrule
\begin{tabbing}
aaaa \= aaaa \= aaaa \= aaaa \= aaaa \= aaaa \kill \\
extract($x$) \\
begin \\
\> let $N_x$ be the total number of elements in all the children of $x$ \\
\> put together; extract the smallest $\min\{B\sqrt{m}/2,N_x\}$ of those \\
\> elements and store them in $x$; \\
end
\end{tabbing}
\vspace{0.1in}
\hrule
\vspace{0.2in}
\caption{Extract}
\label{fig:extract}
\end{figure}

Suppose {\tt Sift} is invoked on a node $x$. 
This invocation could be recursive, or from {\tt Meld} or {\tt Fill-Up}.
{\tt Meld} and {\tt Fill-Up} invoke {\tt Sift} only on roots.
Recursive invocations of {\tt Sift} proceed top-down; thus, any recursive
	invocation of {\tt Sift} on $x$ must be from the parent of $x$. 
Also, as can be seen from the below, 
	as soon as a non-root fails its relevant invariant (PNI or CNI),
	{\tt Sift} is invoked on it.
Therefore, at the beginning of a {\tt Sift} on $x$, each child of $x$ must
	satisfy PNI or CNI, as is relevant.   


{\bf If $x$ is a pnode} (and thus, PNI is the invariant violated), then
	$x$ contains less than $B\sqrt{m}/2$ elements.
Each child of $x$ satisfies PNI, and therefore has, unless it is a leaf,  
	at least $B\sqrt{m}/2$ elements.  
Invoke extract$(x)$.
This can be done in $O(\sqrt{m})$ I/Os by performing a $\sqrt{m}$-way merge of $x$'s children's arrays. 
For each non-leaf child $y$ of $x$ that now violates PNI, recursively invoke {\tt Sift}$(y)$.
Now the size of $x$ is in the range $[B\sqrt{m}/2, B\sqrt{m}]$, unless 
all of $x$'s children are empty leaves. 

{\bf If $x$ is a cnode of rank $r+1$}, then CNI is the invariant violated.
The children of $x$ are of rank $r$, and are thus pnodes.
There are two possibilities: (A)~This {\tt Sift} was invoked from a {\tt Fill-Up} or {\tt Meld}, and thus
	$x$ has one listnode $l$ left in it. (B)~This {\tt Sift} was invoked recursively, and thus
	$x$ has no listnode left in it.
In either case, to begin with, invoke extract$(x)$, and invoke {\tt Sift}$(y)$ 
	for each non-leaf child $y$ of $x$ that now violates PNI.
The number of elements gathered in $x$ is $B\sqrt{m}/2$, 
	unless all of $x$'s children are empty leaves. 
 
Suppose case (A) holds. 
Create a new listnode $l'$, and store in $l'$ the elements just extracted into $x$.
If $l'$ has a size of $B\sqrt{m}/2$, insert $l'$ at the front of $x$'s list;
	else if $l$ and $l'$ together have at most $B\sqrt{m}/2$ elements, then
	merge $l'$ into $l$;
	else, append $l'$ at the end of the list, and transfer enough elements
	from $l'$ to $l$ so that $l$ has a size of $B\sqrt{m}/2$.

If case (B) holds, then if $x$ has nonempty children, once again, extract$(x)$, and invoke {\tt Sift}$(y)$ 
	for each non-leaf child $y$ of $x$ that now violates PNI.
The total number of elements gathered in $x$ now is $B\sqrt{m}$, 
	unless all of $x$'s children are empty leaves. 
If the number of elements gathered is at most $B\sqrt{m}/2$,
	then create a listnode, store the elements in it, and make it the sole member
	of $x$'s list;
otherwise, create two listnodes, insert them in the list of $x$, store $B\sqrt{m}/2$ elements in the first, 
	and the rest in the second.
 
In both the cases, update the ckey of $x$ so that it will be the largest of all keys now present in $x$'s list.
	
{\bf If $x$ is a cnode of rank greater than $r+1$}, then
	while the size of $x$ is less than $s_{k}$, and not all children of $x$ hold
	empty lists, do the following repeatedly: 
        (i)~pick the child $y$ of $x$ with the smallest ckey, (ii)~remove the last listnode 
	of $x$ and merge it with the last listnode $y$, if they together have at most
	$B\sqrt{m}$ elements, (iii)~merge the resultant list of $y$ to the resultant list of $x$ such that all but the last listnode will have al least $B \sqrt{m}/2$ elements,
	(iv)~set the ckey of $x$ to the ckey of $y$, and (v)~invoke {\tt Sift}$(y)$ recursively.
If merging is not required, then the concatenation merely updates $O(1)$ pointers.
Merging, when it is needed, incurs $O(\sqrt{m})$ I/Os.

The {\tt Sift} operation removes all leaves it renders empty.
An internal node becomes a leaf, when all its children are removed.

\subsubsection{{\tt Fill-Up}}
The {\tt Fill-Up} operation is invoked by {\tt Deletemin} and {\tt Meld} 
	on a bucket $B_{i}$ when those operations find $B_i$ empty.
$B_i$ is filled up using the procedure given in Figure~\ref{fig:fillup}.

\begin{figure}
\vspace{0.2in}
\hrule
\begin{tabbing}
aaaa \= aaaa \= aaaa \= aaaa \= aaaa \= aaaa \kill \\
if $i\leq r$ then \\
	\> for each root $x$ in $B_i$ that does not satisfy PNI \\
	\> 	\> {\tt Sift}$(x)$; \\
	\> Let $N_i$ be the total number of elements in all the roots of $B_i$ put together; \\
	\> extract the smallest $\min\{B\sqrt{m}/2,N_i\}$ of those and store them in $B_i$; \\
else \\
	\> for each root $x$ in $B_i$ that does not satisfy CNI \\
	\>	\> {\tt Sift}$(x)$; \\
	\> pick the root $y$ with the smallest ckey in $B_i$; \\
        \> copy the contents of one $l$ of $y$'s listnodes (not the last one) into $B_i$; \\
        \> remove $l$ from the list of $y$.
\end{tabbing}
\vspace{0.1in}
\hrule
\vspace{0.2in}
\caption{Fill-Up}
\label{fig:fillup}
\end{figure}

A bucket remembers, for each element $e$ in it, the root from which $e$ was extracted. 
This is useful when the {\tt Meld} operation sends the elements in the bucket back to their
respective nodes. 

Even if a {\tt Fill-Up} moves all elements of a root $x$ without children into the bucket,
	$x$ is retained	until all its elements are deleted from the bucket. 
(A minor point: For $i\leq r$, if the roots in the bucket all have sent up all their
	elements into the bucket, are without children, and have at most
	$B\sqrt{m}/2$ elements together, then all of them except one can be deleted at the time
	of the {\tt Fill-Up}.)

\subsubsection{The Memory Requirement}
\label{memory:requirement}
The following lemma establishes the largest rank that can be present in a 
	heap.
\begin{lemma}
\label{lem:nodesofarank}
There are at most $\frac{N}{B\sqrt{m}^{k+1}}$ tree-nodes of rank $k$ when $N$ elements have been 
inserted into it.
\end{lemma}
{\bf Proof:}
We prove this by induction on $k$.
The basis is provided by the rank-$0$ nodes.
A node of rank $0$ is created when $B\sqrt{m}$ new elements have been accumulated in 
	the main memory buffer. 
Since the total number of elements inserted in the heap is $N$, 
	the total number of nodes of rank $0$ is at most $N/B\sqrt{m}$. 
Inductively hypothesise that the lemma is true for tree-nodes of
	rank at most $(k-1)$. 
Since a node of rank $k$ is generated when
	$\sqrt{m}$ root nodes of rank $k-1$ are combined, the number of 
	nodes of rank $k$ is at most
	$\frac{N}{B\sqrt{m}^{k}\sqrt{m}}=\frac{N}{B\sqrt{m}^{k+1}}$.
\hfill ${\Box}$

Therefore, if there is at least one node of rank $k$ in the heap, then 
$\frac{N}{B\sqrt{m}^{k+1}} \geq 1$, and so $k\leq \log_{\sqrt{m}}\frac{N}{B}$.
Thus, the rank of the EMSH is at most $\log_{\sqrt{m}} \frac{N}{B}$.
Note that there can be at most $\sqrt{m}-1$ trees of the same rank.

The main memory space required for a bucket is $O(B+\sqrt{m})$.
So, the total space required for all the buckets is
        $O((B+\sqrt{m}) \log_{\sqrt{m}} \frac{N}{B})$.
We can store all buckets in main memory, if we assume that
        $(B+\sqrt{m}) \log_{\sqrt{m}} \frac{N}{B} = O(M)$.
This assumption is valid for all values of $N=O(Bm^{M/2(B+\sqrt{m})})$.
Assume the modest values for $M$ and $B$ given in \cite{MR99}: say, a block is of size $1$ KB, and the main
	memory is of size $1$ MB, and can contain $B=50$ and $M=50000$ records respectively.
Then, if $N<10^{900}$, which is practically always, the buckets will all fit in the main memory.

\section{A Proof of Correctness}
\label{correct:emsh}
If the heap order property is satisfied at every node in the EMSH before an 
	invocation of {\tt Sift}$(x)$, then it will be satisfied after the
	invocation returns too.
This can be shown as follows.

If $x$ is a pnode, then the invocation causes a series of {\tt Extract}s, each
	of which moves up into a node a set of smallest elements in its children;
	none of them can cause a violation of the heap order property.

If $x$ is a cnode of rank $r+1$, then a set of smallest elements at $x$'s children
	move up into $x$ and become corrupt. 
All these elements have key values greater than $k'$, the ckey of $x$ prior to the {\tt Sift}.
The new ckey of $x$ is set to the largest key $k$ among the elements moving in. 
Thus, $k$ is smaller than each key in each of $x$'s
	children after the invocation; and $k>k'$. 

If $x$ is a cnode of rank greater than $r+1$, then a set of corrupt elements move
	into $x$ from $y$, the child of $x$ with the smallest ckey, and the ckey
	of $x$ is set to the ckey of $y$.
Inductively assume that the ckey of $y$ is increased by the recursive {\tt Sift} on $y$.  
Therefore, at the end of the {\tt Sift} on $x$, the ckey of $x$ is smaller than the
	ckey of each of $x$'s children.

In every other operation of the EMSH, all data movements between nodes 
	are achieved through {\tt Sift}s.
Thus, they too cannot violate the heap order property.

When we note that a {\tt Fill-Up} on a bucket $B_i$ moves into it
	a set elements with smallest keys or the smallest ckey from
	its roots, and that 
	the suffixmin pointer of $B_{0}$ points to the bucket with the smallest minkey
	among $\{B_{x}\ |\ x\geq 0\}$, we have the following Lemma. 

\begin{lemma}
If there is no cnode in the EMSH, then the element returned by {\tt Deletemin}
will be the one with the smallest key in the EMSH.
If there are cnodes, and if the returned element is corrupt (respectively, not corrupt), 
then its ckey (respectively, key) will be the smallest
of all keys in the pnodes and ckeys of the EMSH.
\end{lemma}

For all $k>r$, and for every nonleaf $x$ of rank $k$ that satisfies CNI, 
the size of the list in $x$ is at least $\lfloor s_k/2 \rfloor + 1$.
For a root $x$ of rank $k>r$, when the size of its list falls below $\lfloor s_k /2
	\rfloor + 1$, {\tt Sift}$(x)$ is not invoked until at least the next invocation
	of {\tt Fill-Up}, {\tt Meld} or {\tt Deletemin}.


The following lemma gives an upperbound on the size of the list.

\begin{lemma}
\label{lem:3sk}
For all $k>r$, and for every node $x$ of rank $k$, 
the size of the list in $x$ is at most $3 s_k$.
\end{lemma}
{\bf proof:}
We prove this by an induction on $k$.
Note that between one {\tt Sift} and another on a node $x$, the list of $x$ can
lose elements, but never gain.  

A {\tt Sift} on a node of rank $r+1$ causes it to have a list of size at most two;
$2\leq 3s_{r+1}=6$; this forms the basis. 

Let $x$ be a node of rank $ > r + 1$. Hypothesise that the upperbound
holds for all nodes of smaller ranks. When {\tt Sift}$(x)$ is called,
repeatedly, a child of $x$ gives $x$ a list $L'$ that is then added to the list $L$ of $x$,
until the size of $L$ becomes at least $s_{k}$ or $x$ becomes a leaf.
The size of each $L'$ is, by the hypothesis, at most $3 s_{k-1} \leq
2\lceil\frac{3}{2}s_{k-1}\rceil =2s_{k}$.




The size of $L$ is at most $s_k - 1$ before the last iteration.
Therefore, its size afterwards can be at most $3 s_k -1 < 3 s_k$. 
\hfill $\Box$


\begin{lemma}
\label{lem:skbounds}
For all values of $k > r$,
\begin{equation*}
\left(\frac{3}{2}\right)^{k - r -1} \leq s_k \leq 2 \left(\frac{3}{2}\right)^{k - r} - 1
\end{equation*}
\end{lemma}

{\bf Proof:}
A simple induction proves the lowerbound. 
Basis: $s_{r+1}=2\geq\left(\frac{3}{2}\right)^{0}=1$.
Step: For all $k>r+1$, $s_{k}=\lceil \frac{3}{2} s_{k-1} \rceil\geq\frac{3}{2} s_{k-1}\geq\left(\frac{3}{2}\right)^{k - r -1}$.

Similarly, a simple induction shows that, 
for all values of $k \geq r+4$, $s_k \leq 2 \left(\frac{3}{2}\right)^{k - r} - 2$.
Basis: $s_{r+4}=8\leq 2\left(\frac{3}{2}\right)^{4} - 2=8.125$.
Step: $s_{k}=\lceil \frac{3}{2} s_{k-1} \rceil\leq\frac{3}{2} s_{k-1}+1\leq
	\frac{3}{2}\left[2\left(\frac{3}{2}\right)^{k - r -1}-2\right]+1=2 \left(\frac{3}{2}\right)^{k - r} - 2$.
Note that
$s_{r+1}=2=2\left(\frac{3}{2}\right)-1$,
$s_{r+2}=3<2\left(\frac{3}{2}\right)^{2}-1$, and
$s_{r+3}=5<2\left(\frac{3}{2}\right)^{3}-1$,
Therefore, for all values of $k > r$, $s_k \leq 2 \left(\frac{3}{2}\right)^{k - r} - 1$.
 \hfill $\Box$



\begin{lemma}
If $m>110$, at any time there are at most $\epsilon N$ corrupt elements in the EMSH,
where $N$ is the total number of insertions performed.
\end{lemma}
{\bf Proof:}
All corrupt elements are stored in nodes of rank greater than $r$.
The size of the list of a node of rank $k>r$ is
    at most $3s_k$ by Lemma~\ref{lem:3sk}.
Each listnode contains at most $B\sqrt{m}$ corrupt elements.
Thus, the total number of corrupt elements at a node of rank $k>r$
	is at most $3s_kB\sqrt{m}$.
Suppose $m>110$. Then $\sqrt{m}>10.5$.

As $r = log_{\sqrt{m}}\frac{1}{\epsilon}$, 
by Lemma~\ref{lem:skbounds} and Lemma~\ref{lem:nodesofarank},
the total number of
corrupt elements are at most
\begin{equation*}
\begin{split}
\sum_{k>r}(3s_kB\sqrt{m})\frac{N}{B(\sqrt{m})^{k+1}} & =
\frac{N}{(\sqrt{m})^r}  \sum_{k>r}\frac{3  s_k}{(\sqrt{m})^{k-r}} \\
 & \leq \frac{N}{(\sqrt{m})^r} \sum_{k>r}\frac{3  (2  (3/2)^{k-r} - 1)}{(\sqrt{m})^{k-r}} \\
 & \leq \frac{N}{(\sqrt{m})^r} \sum_{k>r}\frac{6 
 (3/2)^{k-r}}{(\sqrt{m})^{k-r}} \\
 & \leq \frac{N}{(\sqrt{m})^r}  \frac{9}{(\sqrt{m}- 1.5)}\\
 & < \frac{N}{(\sqrt{m})^r} = \epsilon N
\end{split}
\end{equation*}
\hfill $\Box$


\section{An Amortised I/O Analysis}
\label{analy:emsh}

Suppose charges of colours green, red and yellow remain distributed over the
data structure as follows: (i)~each root carries $4$  green charges,
(iii)~each bucket carries $2\sqrt{m}$ green charges,
(ii)~each element carries $1/B$ red charges,
(iv)~each nonleaf node carries one yellow charge, and
(v)~each leaf carries $\sqrt{m}+1$ yellow charges.

In addition to these, each element also carries a number of blue charges.
The amount of blue charges that an element carries can vary with its position
in the data structure.

The amortised cost of each operation is its actual cost plus
the total increase in all types charges caused by it.
Now we analyse each operation for its amortised cost.

{\tt Insert}:
Inserts actually cost $\Theta(\sqrt{m})$ I/Os when the insert buffer in the main memory runs full,
which happens at intervals of $\Omega(B\sqrt{m})$ inserts.
Note that some {\tt Deletemin}s return elements in the insert buffer.
If an {\tt Insert} causes the buffer to become full, 
then it creates a new node $x$, a tree $T$ with $x$ as its only node, and
a heap $H'$ with $T$ as its only tree. The $B\sqrt{m}$ elements in the buffer
are copied into $x$. Moreover, a bucket is created for $H'$.
New charges are created and placed on all new entities.
Thus, this {\tt Insert} creates $4+2\sqrt{m}$ green charges, $\sqrt{m}$ red charges,
and $\sqrt{m}+1$ yellow charges. Suppose it also places $\Theta(r/B)$ blue charges
on each element of $x$. That is a total of 
$\Theta(r\sqrt{m})$ blue charges on $x$. 
That is, the total increase in the charges of the system is
$\Theta(r\sqrt{m})$. 
It follows that the amortised cost of a single {\tt Insert} is
$O(r/B)$. 

{\tt Meld}:
The buckets are processed for positions $0$ to max$+1$ in that order, where
max is the largest rank in the two heaps melded. The process is
analogous to the addition of two $\sqrt{m}$-radix numbers.
At the $i$-th position, at most three buckets are to be handled:
$B_{1,i}$, $B_{2,i}$ and the ``carry-in'' $B'_{i}$. 
Assume inductively that each bucket holds
$2\sqrt{m}$ green charges. 

If only one of the three buckets is present at
position $i$, then there is no I/O to perform, no charge is released, and, therefore,
the amortised cost at position $i$ is zero. 

If at least two of the three are present,
then the actual cost of the operations at position $i$ is $O(\sqrt{m})$.
As only one bucket will be left at position $i$, at least one bucket is
deleted, and so at least $2\sqrt{m}$ green charges are freed. Suppose, $\sqrt{m}$ of this 
pays for the work done. If there is no ``carry-out'' $B'_{i+1}$ to be formed,
then the amortised cost at position $i$ is negative.

If $B'_{i+1}$ is to be formed, then
place $\sqrt{m}$ of the remaining green charges
on it. When $B'_{i+1}$ is formed, $\sqrt{m}$ roots hook
up to a new node; these roots cease to be roots, and so together give 
up $4\sqrt{m}$ green charges; four of that will be placed on the new root; 
$4\sqrt{m}-4$ remain; $4\sqrt{m}-4\geq \sqrt{m}$, as $m\geq 2$. 
So we have an extra of $\sqrt{m}$ green charges to put on the carry-out
which, with that addition, holds $2\sqrt{m}$ green charges. No charge of other colours is freed.

The amortised cost is non-positive at each position $i$. 
So the total amortised cost of {\tt Meld} is also non-positive.


{\tt Deletemin}:
A typical {\tt Deletemin} is serviced from the main memory, and does
not cause an I/O. Occasionally, however, an invocation
to {\tt Fill-Up} becomes necessary. The actual I/O cost
of such an invocation is $O(\sqrt{m})$. A {\tt Fill-Up} is triggered
in a bucket when $\Theta(B\sqrt{m})$ elements are deleted from it.
At most a block of the bucket's elements are kept in the main memory.
Thus, a block will have to be fetched into the main memory once in every
$B$ {\tt Deletemin}s. 
The red charges of deleted items can pay for the cost of the all these I/Os.  
The amortised cost of a {\tt Deletemin} is, therefore, at most zero.

{\tt Findmin}: As no I/O is performed, and no charge is released,
the amortised cost is zero.

{\tt Fill-Up}:
This operation is invoked only from {\tt Meld} or {\tt Deletemin}.
The costs have been accounted for in those.

{\tt Sift}:
Consider a {\tt Sift} on a node $x$ of rank $\leq r+1$.
This performs one or two Extracts. The actual cost of the Extracts is $O(\sqrt{m})$.
If the number of extracted elements is $\Theta(B\sqrt{m})$,
	then each extracted element can contribute $1/B$ blue charges 
	to pay off the actual cost.
If the number of extracted elements is $o(B\sqrt{m})$, then
$x$ has become a leaf after the {\tt Sift}.
Therefore, all of $x$'s children were leaves before the {\tt Sift}. 
One of them can pay for the {\tt Sift} with its $\sqrt{m}$ yellow charges;
	at least one remained at the time of the {\tt Sift}.
Node $x$ that has just become a leaf, has lost the $\sqrt{m}$ children 
	it once had. 
If one yellow charge from each child has been preserved in $x$, then
	$x$ now holds $\sqrt{m}+1$ yellow charges, enough for a leaf.

Consider a {\tt Sift} on a node $x$ of rank $i > r+1$.
A number of iterations are performed, each of which costs $O(\sqrt{m})$ I/Os.
In each iteration, a number of elements move from a node of rank 
$i-1$ (namely, the child $y$ of $x$ with the smallest ckey) to a node of rank $i$
(namely, $x$). If the number of elements moved is $\Omega(B\sqrt{m})$, then
the cost of the iteration can be charged to the elements moved. 
Suppose each element moved contributes $\frac{1}{s_{i-1}B}$ blue charges.
Since the list of $y$ has at least $\lfloor s_{i-1}/2 + 1\rfloor$ listnodes,
in which all but the last have at least $B\sqrt{m}/2$ elements, the total number of
blue charges contributed is at least 
$\frac{1}{s_{i-1}B}\lfloor \frac{s_{i-1}}{2}\rfloor \frac{B\sqrt{m}}{2} =\Theta{(\sqrt{m})}$. 
Thus, the cost of the
	iteration is paid off.

If the number of elements moved is $o(B\sqrt{m})$, then $y$ was a leaf prior
to the {\tt Sift}, and so can pay for the {\tt Sift} with its $\sqrt{m}$ yellow charges.
If $x$ becomes a leaf at the end of the {\tt Sift}, it will have $\sqrt{m}+1$ yellow charges 
on it, as one yellow charge from each deleted child is preserved in $x$.

An element sheds $1/B$ blue charges for each level it climbs up, for the first
$r+1$ levels. After that when it moves up from level $i-1$ to $i$, it
sheds $\frac{1}{s_{i-1}B}$ blue charges. Therefore, with 
\[\frac{r+1}{B}+\sum_{i>r+1}\frac{1}{s_{i-1}B}=\frac{r+1}{B}+\sum_{i>r}\frac{1}{B}
 \left(\frac{2}{3}\right)^{i - r -1}=\Theta{\left(\frac{r}{B}\right)}\]
blue charges initially placed on the element, it can pay for its travel upwards. 

Thus, we have the following lemma.
\begin{lemma}
In EMSH, the amortised complexity of an {\tt Insert} is $O(\frac{1}{B}\log_{m} \frac{1}{\epsilon})$. {\tt Findmin}, {\tt Deletemin} and {\tt Meld} all have non-positive amortised complexity.
\end{lemma}

\section{Hard heap: A Meldable Priority Queue}
\label{hard:emsh}
When an EMSH has error-rate $\epsilon=1/(N+1)$, no element in it can be corrupt.
In this case, EMSH becomes an exact priority queue, which we call hard heap.
In it every node is a pnode, and every tree is a ptree. 
{\tt Deletemin}s always report the exact minimum in the hard heap. 
The height of each tree is $O(\log_{m}\frac{N}{B})$, as before.
But, since all nodes are pnodes, the amortised cost of an insertion is
$O(\frac{1}{B}\log_{m}\frac{N}{B})$ I/Os. The amortised costs of all 
other operations remain unchanged.

The absence of corrupt nodes will also permit us to implement a {\tt Delete}
operation: To delete the element with key value $k$, insert a ``Delete'' record
with key value $k$. Eventually, when $k$ is the smallest key value in the hard heap,
a {\tt Deletemin} will cause
the element with key $k$ and the ``Delete'' record to come up together. Then the
two can annihilate each other. The amortised cost of a {\tt Delete} is
$O(\frac{1}{B}\log_{m}\frac{N}{B})$ I/Os, the same as that of an {\tt Insert}.
 
None of the known external memory priority queues (EMPQs) \cite{Arge03,FJKT99,KS96},
support a {\tt meld} operation. However, in all of them, two queues could be {\tt meld}ed by 
inserting elements of the smaller queue into the larger queue one by one.
This is expensive if the two queues have approximately the same size.
The cost of this is not factored into the amortised complexities of those
EMPQs. 

The actual cost of a {\tt meld} of two hard heap's with $N$ elements each is
$O(\sqrt{m}\log_{m}\frac{N}{B})$ I/Os; the amortised cost of the {\tt meld} is subzero.
But this is the case only if the buckets of
both the heaps are in the main memory. Going by our earlier analysis in Section~\ref{memory:requirement},
if $N=O(Bm^{M/2k(B+\sqrt{m})})$ then $k$ heaps of size $N$ each can keep their buckets in the main memory.

The buckets of the heaps to be melded could be kept in the secondary memory, 
and brought into the main memory, and written back either side of the meld. 
The cost of this can be accounted
by an appropriate scaling of the amortised complexities.
However, operations other than {\tt meld} can be performed only if the buckets are 
in the main memory.
 
In hard heap, unlike in the other EMPQs, 
elements move only in the upward direction. 
That makes hard heap easier to implement.
Hard heap and the external memory heap of \cite{FJKT99} do not require any extra space
other than is necessary for the elements.
The other EMPQs \cite{Arge03,KS96} use extra space (See Table~\ref{comparison:emsh}).
  
The buffer tree \cite{Arge03} is a B+ tree, and therefore uses a balancing procedure.
However, because of delayed deletions, its height may depend on the number of pending
deletions, as well as the number of elements left in it.
The external memory heap of \cite{FJKT99} is a balanced heap, and therefore,
incurs a balancing cost. But, in it the number of
I/Os performed by a sequence of $S$ operations is
$O(\frac{S}{B}+\frac{1}{B}\sum_{i = 1}^{S} \log_{m}\frac{N_i}{B})$, where $N_i$ is the number
of elements remaining in the heap before the $i$-th operation; this is helpful
when the {\tt insert}s and {\tt deletemin}s are intermixed so that the number
of elements remaining in the data structure at any time is small.  

In comparison, hard heap is not a balanced tree data structure. It does not use a costly
balancing procedure like the heaps of \cite{FJKT99,KS96}.
However, for a sequence of $N$ operations, 
	the amortised cost of each operation is $O(\frac{1}{B} \log_m \frac{N}{B})$ I/Os.

We can make the amortised cost depend on $N_i$, the number
of elements remaining in the hard heap before the $i$-th operation, at the cost
of adding a balancing procedure.
In a sequence of operations, whenever $N_I$, the number of {\tt insert}s performed, and $N_D$, 
	the number of {\tt deletemin}s performed, satisfy $N_I - N_D < N_I / \sqrt{m}$,
	and the height of the hard heap is $\log_{\sqrt{m}} \frac{N_I}{B}$, 
	delete the remaining $N_I - N_D$ elements from the hard heap,  insert them back in,
	and set the counts $N_I$ and $N_D$ to $N_I-N_D$ and zero respectively;
	we can think of this as the end of an {\em epoch} and the beginning of the next in
	the life of the hard heap. 
The sequence of operations, thus, is a concatenation of several epochs.
Perform an amortised analysis of each epoch independently. 
The cost of reinsertions can be charged to the elements actually deleted in the previous
	epoch, thereby multiplying the amortised cost
	by a factor of $O(1+\frac{1}{\sqrt{m}})$.
It is easy to see that now the number of
I/Os performed by a sequence of $S$ operations is
$O(\frac{S}{B}+\frac{1}{B}\sum_{i = 1}^{S} \log_{m}\frac{N_i}{B})$.


\subsection{Heap Sort}
We now discuss an implementation of Heap Sort using hard heap, and
count the number of comparisons performed.
To sort, insert the $N$ input elements into an initially empty hard heap, and 
then perform $N$ {\tt deletemin}s.

When a node of rank $0$ is created, $O(B\sqrt{m} \log_2 (B\sqrt{m}))$ comparisons are performed;
that is $O(\log_2 (B\sqrt{m}))$ comparisons per element involved.
When an elements moves from a node to its parent, it participates in a $\sqrt{m}$-way merge;
	a $\sqrt{m}$-way merge that outputs $k$ elements requires to perform only
	$O(k\log_2 \sqrt{m})$ comparisons; that is $O(\log_2 \sqrt{m})$ comparisons per element involved.
Since the number of levels in the hard heap is at most $\log_{\sqrt{m}} N/B\sqrt{m}$, the total
	number of comparisons performed by one element is $\log_2 N$.
Each {\tt deletemin} operation can cause at most $\log_{\sqrt{m}} (N/\sqrt{mB})$ comparisons
	among the suffixmin pointers.
Thus, the total number of comparisons is $O(N \log_2 N)$. 
\section{Applications of EMSH}
\label{appl:emsh} 
The external memory soft heap data structure is useful for finding exact and 
	approximate medians, and for approximate sorting \cite{Ch00a}.
Each of these computations take $O(N/B)$ I/Os:
\begin{enumerate}
\item To compute the median in a set of $N$ numbers, insert the numbers
        in an EMSH with error rate $\epsilon$.
Next, perform $\epsilon N$ {\tt Deletemin}s. The largest number $e$ deleted
        has a rank between $\epsilon N$ and $2\epsilon N$.
Partition the set using $e$ as the pivot in $O(N/B)$ I/Os.
We can now recurse with a partition of size at most $\max\{\epsilon, (1-2\epsilon)\}N$.
The median can be found in $O(N/B)$ I/Os.
This is an alternative to the algorithm in \cite{Si02} which also requires $O(N/B)$ I/Os.
\item To approximately sort $N$ items, insert them into an EMSH with error rate $\epsilon$, and
        perform $N$ {\tt Deletemin}s consecutively. Each element can form an inversion with
        at most $\epsilon N$ of the items remaining in the EMSH at the time of its deletion.
        The output sequence, therefore, has at most $\epsilon N^2$ inversions.
We can also use EMSH to near sort $N$ numbers in $O(N/B)$ I/Os such that
        the rank of each number in the output sequence differs from
        its true rank by at most $\epsilon N$; the in-core algorithm given in \cite{Ch00a} suffices.
\end{enumerate}

