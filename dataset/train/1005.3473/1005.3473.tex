\chapter{External Memory Soft Heap and Hard Heap, a Meldable Priority Queue}
\label{emsh:chapt}
\section{Introduction}
An external memory version of soft heap that we call ``External Memory Soft Heap''
(EMSH for short) is presented. 
It supports {\tt Insert}, {\tt Findmin}, {\tt Deletemin} and {\tt Meld} operations.
An EMSH may, as in its in-core version, and at its discretion, 
corrupt the keys of some elements in it, by revising them upwards.
But the EMSH guarantees that the number of corrupt elements in it is never more than
, where  is the total number of items inserted in it, and  is
a parameter of it called the error-rate.
The amortised I/O complexity of an {\tt Insert} is 
, where .
{\tt Findmin}, {\tt Deletemin} and {\tt Meld} all have non-positive amortised I/O complexities.

When we choose an error rate , EMSH stays devoid of corrupt nodes, 
and thus becomes a meldable priority queue that we call ``hard heap''.  
The amortised I/O complexity of an {\tt Insert}, in this case, is ,
over a sequence of operations involving  {\tt insert}s.
{\tt Findmin}, {\tt Deletemin} and {\tt Meld} all have non-positive amortised I/O complexities.
If the inserted keys are all unique, a {\tt Delete} (by key) operation can
	also be performed at an amortised I/O complexity of .
A balancing operation performed once in a while on a hard heap 
ensures that the number of I/Os performed by a sequence of  operations on it is
, where
 is the number of elements in the heap before the th operation.

\subsection{Definitions}
A priority queue is a data structure used for maintaining a set  of elements, 
	where each element has a key drawn from a linearly ordered set.
A priority queue typically supports the following operations:
\begin{enumerate}
\item {\tt Insert}: Insert element  into .
\item {\tt Findmin}: Return the element with the smallest key in 
\item {\tt Deletemin}: Return the element with the smallest key in  and remove it from .
\item {\tt Delete}: Delete element  from 
\item {\tt Delete}: Delete the element with key  from 
\end{enumerate}
Algorithmic applications of priority queues abound \cite{AHU74,CLR90}.

Soft heap is an approximate meldable priority queue devised
	by Chazelle \cite{Ch00a}, and supports
	{\tt Insert}, {\tt Findmin}, {\tt Deletemin}, {\tt Delete}, and
	{\tt Meld} operations. 
A soft heap, may at its discretion, corrupt the keys of some elements in it, by revising them upwards.
A {\tt Findmin} returns the element with the smallest current key, which may or may not be corrupt. 
A soft heap guarantees that the number of corrupt elements in it is never more than
, where  is the total number of items inserted in it, and  is
a parameter of it called the error-rate.   
A {\tt Meld} operation merges two soft heaps into one new soft heap.

\subsection{Previous Results}
I/O efficient priority queues have been reported before \cite{Arge03,BK98,FJKT99,KS96,MZ03}.
An I/O efficient priority queue, called \emph{buffer tree}  was introduced by 
        Arge \cite{Arge03}.
A buffer tree is an  tree, where  and , and each
        node contains a buffer of size .
Buffer tree supports the {\tt Insert}, {\tt  Deletemin}, {\tt Delete} (by key), 
	and offline search operations.
The amortised complexity of each of these operations on buffer tree is
        
        I/Os, over a sequence of operations of length .
External memory versions of heap are presented in \cite{KS96} and \cite{FJKT99}. 
The heap in \cite{KS96} is a -way tree, each node of which contains a buffer 
of size ; it supports {\tt Insert}, {\tt Deletemin}, and {\tt Delete} (by key) operations.
The amortised cost of each operation on this heap is
        
        I/Os, where  is the total number of
        elements in the heap.
The heap in \cite{FJKT99} is an -way tree and it does not contain a buffer at each node.
It supports {\tt Insert}, and {\tt Deletemin} operations.
For this heap, the total number of I/Os performed by a sequence of 
        operations is , where is the number
        of elements in the heap before the th operation.

An external memory version of tournament tree that supports 
        the {\tt Deletemin}, {\tt Delete}, and {\tt Update}
        operations is also presented in \cite{KS96};
this is a complete binary tree.
An {\tt Update} operation changes the key of element  to 
        if and only if  is smaller than the present key of .
The amortised cost of each operation on this data structure is  I/Os \cite{KS96}.

The priority queue of \cite{BK98} maintains a hierarchy of sorted lists in secondary memory.
An integer priority is presented in \cite{MZ03}.
See \cite{BCF+00} for an experimental study on some of these priority queues.
Numerous applications of these data structure have also been reported:
graph problems, computational geometry problems and sorting to name a few
\cite{Arge03,FJKT99,KS96}.

See Table \ref{comparison:emsh} for a comparison of hard heap with the
        priority queues of \cite{Arge03,FJKT99,KS96}.

Soft heap is an approximate meldable priority queue devised
        by Chazelle \cite{Ch00a}, and supports
        {\tt Insert}, {\tt Findmin}, {\tt Deletemin}, {\tt Delete}, and
        {\tt Meld} operations. 
This data structure is used in computing minimum spanning
        trees \cite{Ch00b} in the fastest known in-core algorithm for the problem.
Soft heap has also been used for finding exact and approximate
    medians, and for approximate sorting \cite{Ch00a}.
An alternative simpler implementation of soft heap 
is given by Kaplan and Zwick \cite{KaZw09}.

\begin{table}
\begin{center}
            \begin{tabular}{|l|l|l|l|l|l|}
\cline{1-6}
               \hline
{\bf Properties}  &   {\bf buffer tree}     & {\bf heap} \cite{KS96}      & {\bf heap} \cite{FJKT99}    & {\bf tourn. tree}
   & {\bf EMSH} \\
\hline \hline
type of tree &     & -way & -way & complete & set of
                                                                      - \\
             &  tree        & tree           & tree    & binary tree & way trees \\
\hline
size of a     &               &   &         &     &  \\
node          &                  &                &          &        &        \\
\hline
buffer of size &    yes          &   yes          & no       & yes    & no  \\
 at each    &                 &                &          &        &   \\
node            &                 &                &          &        &   \\
\hline
extra space    &      &   & 0   &  & 0 \\
\hline
operations     & {\tt Insert} & {\tt Insert}    & {\tt Insert}    & {\tt Delete}    & {\tt Insert} \\
               & {\tt Delete} & {\tt Delete}    & {\tt Deletemin} & {\tt Deletemin} & {\tt Delete} \\
            & {\tt Deletemin} & {\tt Deletemin} & {\tt Findmin}   & {\tt Findmin}   & {\tt Deletemin} \\
            & {\tt Findmin}   & {\tt Findmin}   &                 & {\tt Update}& {\tt Findmin}, {\tt Meld} \\
\hline
\end{tabular}
\caption{Comparison with some known priority queues }
\label{comparison:emsh}
\end{center}
\end{table}
\subsection{Our Results}
In this chapter, we present an external memory version of soft heap that permits
batched operations. We call our data structure ``External Memory Soft Heap''
(EMSH for short). 
As far as we know, this is the first implementation of soft heap
on an external memory model. 
When we choose an error rate , EMSH stays devoid of corrupt nodes, 
and thus becomes a meldable priority queue that we call ``hard heap''.  


EMSH is an adaptation of soft heap for this model.
It supports {\tt Insert}, {\tt Findmin}, {\tt Deletemin} and {\tt Meld} operations.
An EMSH may, as in its in-core version, and at its discretion, 
corrupt the keys of some elements in it, by revising them upwards.
But it guarantees that the number of corrupt elements in it is never more than
, where  is the total number of items inserted in it, and  is
a parameter of it called the error-rate.
The amortised I/O complexity of an {\tt Insert} is .
{\tt Findmin}, {\tt Deletemin} and {\tt Meld} all have non-positive amortised I/O complexities.

A hard heap (an EMSH with ) does not have any corrupt element.
Therefore, it is an exact meldable priority queue.
The amortised I/O complexity of an {\tt Insert}, in this case, is .
{\tt Findmin}, {\tt Deletemin} and {\tt Meld} all have non-positive amortised I/O complexities.
If the inserted keys are all unique, a {\tt Delete} (by key) operation can
	also be performed at an amortised I/O complexity of .



\subsection{Organisation of This Chapter}

This chapter is organised as follows: Section~\ref{datastr:emsh} describes the data structure.
The correctness of the algorithm is proved in Section~\ref{correct:emsh}.
The amortised I/O analysis of the algorithm is presented in Section~\ref{analy:emsh}.
EMSH with  is discussed in Section~\ref{hard:emsh}.
Some of its applications are shown in Section~\ref{appl:emsh}
\section{The Data Structure}
\label{datastr:emsh}
An EMSH consists of a set of trees on disk.
The nodes of the trees are classified as follows.  
A node without a child is a leaf. 
A node without a parent is a root.
A node is internal, if it is neither a leaf, nor a root. 

Every non-leaf in the tree has at most  children.
Nodes hold pointers to their children. 

Every node has a rank associated with it at the time of its creation. 
The rank of a node never changes.
All children of a node of rank  are of rank . 
The rank of a tree  is the rank of 's root. 
The rank of a heap  is . 
An EMSH can have at most  trees of any particular rank.

Each element held in the data structure has a key drawn from a 
	linearly ordered set. 
We will treat an element and its key as indistinguishable.

Each instance of EMSH has an associated error-rate . 
Define .
Nodes of the EMSH with a rank of at most  are called {\em pnodes} (for ``pure nodes''), and 
	nodes with rank greater than  are called {\em cnodes} (for ``corrupt nodes'').
Each {\em pnode} holds an array that contains elements in sorted order.
A tree is a ptree if its rank is at most , and a ctree otherwise.

We say that a pnode  satisfies the \emph{pnode invariant} (PNI), if 
\begin{quote}
	 is a non-leaf and the array in  contains at most 
	 and at least  elements, or \\
	 is a leaf and the array in  contains at most 
	 elements.
\end{quote}
Note that a pnode that satisfies PNI may contain less than  elements,
	if it is a leaf.

Every cnode has an associated doubly linked list of {\em listnodes}. 
A cnode holds pointers to the first and last listnodes of its list. 
The size of a list is the number of listnodes in it. 
Each listnode holds pointers to the next and previous
	listnodes of its list; the next (resp., previous)
        pointer of a listnode  is null if   is the last (resp., first)
        of its list.
Each listnode contains at most , and unless it is the last of a list,
	at least   elements. 
The last listnode of a list may contain less than  elements.

Let  be defined as follows:


We say that a cnode  that is a non-leaf  
	satisfies the \emph{cnode invariant} (CNI), if 
	the list of  has a size of at least . 
A leaf cnode always satisfies CNI. 
	

\begin{table}
\begin{center}
            \begin{tabular}{|l|l|l|l|}
\cline{1-4} \hline
 {\bf Type of node} & {\bf Number of} & {\bf Number of elements in a} & {\bf Size of the list of a} \\
              & {\bf children} & {\bf pnode of this type} & {\bf cnode of this type} \\
              &          & {\bf if it satisfies PNI} & {\bf if it satisfies CNI} \\
\hline \hline
               leaf node &  &  &   \\
\hline
               root node &  & ;   &  \\
\hline
               internal node &  & ;  &   \\
\hline
            \end{tabular}
\caption{Types of nodes in the data structure, and the invariants on them}
\label{tab:data:structure:emsh}
\end{center}
\end{table}
 
Table \ref{tab:data:structure:emsh} summarizes the different types of nodes in an EMSH, 
the number of children each can have, and the PNI and CNI constraints on each.

Every cnode has a {\em ckey}.
For an element  belonging to the list of a cnode ,
	the ckey of  is the same as the ckey of ; 
	 is called corrupt if its ckey
	is greater than its key.

An EMSH is said to satisfy the heap property if the following
	conditions are met:
For every cnode  of rank greater than , the ckey of  is smaller than 
	the ckey of each of 's children.
For every cnode  of rank , the ckey of  is smaller than
	each key in each of 's children.
For every pnode , each key in  is smaller than each key in each of 's
	children.

For each rank , we maintain a bucket  for the roots of rank . 
We store the following information in :
\begin{enumerate}
\item the number of roots of rank  in the EMSH; there are at most  such roots.
\item pointers to the roots of rank  in the EMSH.
\item if  and 
	then a listnode of the list associated with the root of rank , whose ckey value is ;
	this listnode will not be the last of the list, unless the list has only one listnode. 
\item if  then the  smallest of all elements in the roots of rank , 
	for some 
\item a pointer suffixmin
\end{enumerate}

We define the minkey of a tree as follows:
for a ptree , the minkey of  is defined as the 
	smallest key in the root of ;
for a ctree , the minkey of  is the ckey of the root of .
The minkey of a bucket  is the smallest of the minkeys of the trees of rank  in the EMSH;
	 holds pointers to the roots of these trees.  
The suffixmin pointer of  points to the bucket with the smallest minkey
	among .  

For each bucket, we keep the items in 1, 2 and 5 above, and at most a block of the elements 
	(3 or 4 above) in the main memory.
When all elements of the block are deleted by {\tt Deletemin}s, the next 
	block is brought in.
The amount of main memory needed for a bucket is, thus, . 
As we shall show later, the maximum rank in the data structure, and so the number
	of buckets is . Therefore, if 
	 
	the main memory suffices for all the buckets.
(See Subsection~\ref{memory:requirement}). 

We do not keep duplicates of elements. 
All elements and listnodes that are taken into the buckets would be 
	physically removed from the respective roots.
But these elements and listnodes would still be thought of as belonging to their
	original positions.
For example, the above definition of minkeys assumes this.

A bucket  becomes {\em empty}, irrespective of the value of ,
when all the elements in it have been deleted. 

	



\subsection{The Operations}

In this section we discuss the {\tt Insert}, {\tt Deletemin}, {\tt Findmin}, {\tt Meld}, {\tt Sift} and {\tt Fill-Up} 
    operations on EMSH. The first four are the basic operations. 
The last two are auxiliary.
The {\tt Sift} operation is invoked only on non-leaf nodes that fail to satisfy the 
	pnode-invariant (PNI) or cnode-invariant (CNI), whichever is relevant.
When the invocation returns, the node will satisfy the invariant.
Note that PNI applies to pnodes and CNI applies to cnodes. 
{\tt Fill-Up} is invoked by the other operations on a bucket when they find it empty.

\subsubsection{{\tt Insert}}
For each heap, a buffer of size  is maintained in the main memory. 
If an element  is to be inserted into heap , store it in the buffer of . 
The buffer stores its elements in sorted order of key values. 
If the buffer is full (that is,  is the -th  element of the buffer), 
	create a new node  of rank , and copy all elements in the buffer into it.
The buffer is now empty.
Create a tree  of rank  with  as its only node. 
Clearly,  is a root as well as a leaf. 
Construct a new heap  with  as its sole tree. 
Create a bucket  for , set the number of trees in it to , and include a pointer to  in it.
Invoke {\tt Meld} on  and .

\subsubsection{{\tt Deletemin}}
A {\tt Deletemin} operation is to delete and return an element with the smallest key in the EMSH.
The pointer suffixmin points to the bucket with the smallest minkey.
A {\tt Deletemin} proceeds as in Figure~\ref{fig:deletemin}.

\begin{figure}
\vspace{0.2in}
\hrule
\begin{tabbing}
aaaa \= aaaa \= aaaa \= aaaa \= aaaa \= aaaa \kill \\
Let  be the bucket pointed by suffixmin; \\
let  be the smallest element in the insert buffer; \\
if the key of  is smaller than the minkey of  then \\
	\> delete  from the insert buffer, and return e; \\
\\
if  then \\
	\> the element with the smallest key in the EMSH is in bucket ; \\
	\> let it be ; delete  from ; \\
	\> if  is not empty, then \\ 
	\>	\> update its minkey value; \\

else \\
	\> let  be the root of the tree  that lends its minkey to ; the ckey of  is smaller \\
	\> than all keys in the pnodes and all ckeys;  holds elements from a listnode  of ;\\
	\> let  be an element from ;  delete  from ; \\
\\
if  is empty then \\
	\> fill it up with an invocation to {\tt Fill-Up}(), and update 's minkey value; \\
\\
update the suffixmin pointers of buckets ; \\ 
return ; 
\end{tabbing}
\vspace{0.1in}
\hrule
\vspace{0.2in}
\caption{Deletemin}
\label{fig:deletemin}
\end{figure}

Note that if after a {\tt Deletemin}, a root fails to satisfy the relevant invariant,
then a {\tt Sift} is not called immediately. We wait till the next {\tt Fill-Up}.
(While deletions happen in buckets, they are counted against the root from which
the deleted elements were taken. Therefore, a deletion can cause the corresponding root
to fail the relevant invariant.)
 
Recall that we keep at most a block of 's elements in the main memory.
When all elements of the block are deleted by {\tt Deletemin}s, the next block is
	brought in.

\subsubsection{{\tt Findmin}}
A {\tt Findmin} return the same element that a {\tt Deletemin} would.
But the element is not deleted from the EMSH. 
Therefore, a {\tt Findmin} does not need to perform any of the updations
that a {\tt Deletemin} has to perform on the data structure.

As it is an in-core operation, a {\tt Findmin} does not incur any I/O. 

\subsubsection{{\tt Meld}}

In the {\tt Meld} operation, two heaps  and  are to be merged
        into a new heap .
It is assumed that the buckets of the two heaps remain in the main memory.

Combine the input buffers of  and .
If the total number of elements exceeds , then
create a new node  of rank , move  elements from the buffer into it
leaving the rest behind, create a tree  of rank  with  as its only node, 
create a bucket , set the number of trees in it to , and include a pointer to  in it.

Let  (resp., ) be the -th bucket of  (resp., ).
Let max denote the largest rank in the two heaps  and . The {\tt Meld} is analogous to
the summation of two -radix numbers of  digits. At position ,
buckets  and  are the ``digits''; there could also be
a ``carry-in'' bucket . The ``summing'' at position  produces a new
 and a ``carry-out'' . 
 will function as the ``carry in'' for position .

The {\tt Meld} proceeds as in Figure~\ref{fig:meld}.

\begin{figure}
\vspace{0.2in}
\hrule
\begin{tabbing}
aaaa \= aaaa \= aaaa \= aaaa \= aaaa \= aaaa \kill \\
for  to max \\
begin \\
\> if only one of ,  and  exists then \\
\>\>that bucket becomes ; {\tt Fill-Up} that bucket, if necessary; \\
\>\>there is no carry-out; \\
\> else \\
\>\>if  \\
\>\>\>if  (resp., ) contains elements then \\
\>\>\>\> send the elements of  (resp., ) back to the \\
\>\>\>\>roots from which they were taken; \\ 
\>\>\>for each root  pointed by  or  \\ 
\>\>\>\>if  does not satisfy PNI, invoke {\tt Sift}; \\ 
\>\>else \\ 
\>\>\>if  (resp., ) and the last listnode  (resp., ) \\
\>\>\>of the root  (resp., ) with the smallest ckey in  (resp., ) \\
\>\>\>have sizes  each, then \\ 
\>\>\>\>merge the elements in  (resp., ) into  (resp., ); \\ 
\>\>\>else \\ 
\>\>\>\>store the elements in  (resp., ) in a new listnode \\ 
\>\>\>\>and insert  into the list of  (resp., ) so that \\
\>\>\>\>all but the last listnode will have  elements; \\ 
\>\>\>\>if  (resp., ) does not satisfy CNI, then {\tt Sift} it; \\ 
\>if the total number of root-pointers in ,  and  is , then  \\
\>\>move all root-pointers to ; {\tt Fill-Up} ; \\
\>\>delete  and ; There is no carry-out; \\
\end{tabbing}
\hfill {CONTINUED}
\label{fig:meld1}
\end{figure}

\begin{figure}
\begin{tabbing}
aaaa \= aaaa \= aaaa \= aaaa \= aaaa \= aaaa \kill \\
\>else \\
\>\>create a tree-node  of rank ; \\
\>\>pool the root-pointers in ,  and ; \\
\>\>take  of those roots and make them children of ; {\tt Sift}; \\
\>\>create a carry-out bucket ; \\ 
\>\>place in it a pointer to ; this is to be the only root-pointer of ; \\ 
\>\>move the remaining root-pointers into ; {\tt Fill-Up} ; \\
\>\>delete  and ; \\
end; \\
update the suffixmin pointers; \\
\end{tabbing}
\vspace{0.1in}
\hrule
\vspace{0.2in}
\caption{Meld}
\label{fig:meld}
\end{figure}

\subsubsection{{\tt Sift}}

The {\tt Sift} operation is invoked only on non-leaf nodes that fail to satisfy 
	PNI or CNI, whichever is relevant.
When the invocation returns, the node will satisfy the invariant.
We shall use in the below a procedure called
	extract that is to be invoked only on cnodes of rank , 
	and pnodes, and is defined in Figure~\ref{fig:extract}.

\begin{figure}[ht]
\vspace{0.2in}
\hrule
\begin{tabbing}
aaaa \= aaaa \= aaaa \= aaaa \= aaaa \= aaaa \kill \\
extract() \\
begin \\
\> let  be the total number of elements in all the children of  \\
\> put together; extract the smallest  of those \\
\> elements and store them in ; \\
end
\end{tabbing}
\vspace{0.1in}
\hrule
\vspace{0.2in}
\caption{Extract}
\label{fig:extract}
\end{figure}

Suppose {\tt Sift} is invoked on a node . 
This invocation could be recursive, or from {\tt Meld} or {\tt Fill-Up}.
{\tt Meld} and {\tt Fill-Up} invoke {\tt Sift} only on roots.
Recursive invocations of {\tt Sift} proceed top-down; thus, any recursive
	invocation of {\tt Sift} on  must be from the parent of . 
Also, as can be seen from the below, 
	as soon as a non-root fails its relevant invariant (PNI or CNI),
	{\tt Sift} is invoked on it.
Therefore, at the beginning of a {\tt Sift} on , each child of  must
	satisfy PNI or CNI, as is relevant.   


{\bf If  is a pnode} (and thus, PNI is the invariant violated), then
	 contains less than  elements.
Each child of  satisfies PNI, and therefore has, unless it is a leaf,  
	at least  elements.  
Invoke extract.
This can be done in  I/Os by performing a -way merge of 's children's arrays. 
For each non-leaf child  of  that now violates PNI, recursively invoke {\tt Sift}.
Now the size of  is in the range , unless 
all of 's children are empty leaves. 

{\bf If  is a cnode of rank }, then CNI is the invariant violated.
The children of  are of rank , and are thus pnodes.
There are two possibilities: (A)~This {\tt Sift} was invoked from a {\tt Fill-Up} or {\tt Meld}, and thus
	 has one listnode  left in it. (B)~This {\tt Sift} was invoked recursively, and thus
	 has no listnode left in it.
In either case, to begin with, invoke extract, and invoke {\tt Sift} 
	for each non-leaf child  of  that now violates PNI.
The number of elements gathered in  is , 
	unless all of 's children are empty leaves. 
 
Suppose case (A) holds. 
Create a new listnode , and store in  the elements just extracted into .
If  has a size of , insert  at the front of 's list;
	else if  and  together have at most  elements, then
	merge  into ;
	else, append  at the end of the list, and transfer enough elements
	from  to  so that  has a size of .

If case (B) holds, then if  has nonempty children, once again, extract, and invoke {\tt Sift} 
	for each non-leaf child  of  that now violates PNI.
The total number of elements gathered in  now is , 
	unless all of 's children are empty leaves. 
If the number of elements gathered is at most ,
	then create a listnode, store the elements in it, and make it the sole member
	of 's list;
otherwise, create two listnodes, insert them in the list of , store  elements in the first, 
	and the rest in the second.
 
In both the cases, update the ckey of  so that it will be the largest of all keys now present in 's list.
	
{\bf If  is a cnode of rank greater than }, then
	while the size of  is less than , and not all children of  hold
	empty lists, do the following repeatedly: 
        (i)~pick the child  of  with the smallest ckey, (ii)~remove the last listnode 
	of  and merge it with the last listnode , if they together have at most
	 elements, (iii)~merge the resultant list of  to the resultant list of  such that all but the last listnode will have al least  elements,
	(iv)~set the ckey of  to the ckey of , and (v)~invoke {\tt Sift} recursively.
If merging is not required, then the concatenation merely updates  pointers.
Merging, when it is needed, incurs  I/Os.

The {\tt Sift} operation removes all leaves it renders empty.
An internal node becomes a leaf, when all its children are removed.

\subsubsection{{\tt Fill-Up}}
The {\tt Fill-Up} operation is invoked by {\tt Deletemin} and {\tt Meld} 
	on a bucket  when those operations find  empty.
 is filled up using the procedure given in Figure~\ref{fig:fillup}.

\begin{figure}
\vspace{0.2in}
\hrule
\begin{tabbing}
aaaa \= aaaa \= aaaa \= aaaa \= aaaa \= aaaa \kill \\
if  then \\
	\> for each root  in  that does not satisfy PNI \\
	\> 	\> {\tt Sift}; \\
	\> Let  be the total number of elements in all the roots of  put together; \\
	\> extract the smallest  of those and store them in ; \\
else \\
	\> for each root  in  that does not satisfy CNI \\
	\>	\> {\tt Sift}; \\
	\> pick the root  with the smallest ckey in ; \\
        \> copy the contents of one  of 's listnodes (not the last one) into ; \\
        \> remove  from the list of .
\end{tabbing}
\vspace{0.1in}
\hrule
\vspace{0.2in}
\caption{Fill-Up}
\label{fig:fillup}
\end{figure}

A bucket remembers, for each element  in it, the root from which  was extracted. 
This is useful when the {\tt Meld} operation sends the elements in the bucket back to their
respective nodes. 

Even if a {\tt Fill-Up} moves all elements of a root  without children into the bucket,
	 is retained	until all its elements are deleted from the bucket. 
(A minor point: For , if the roots in the bucket all have sent up all their
	elements into the bucket, are without children, and have at most
	 elements together, then all of them except one can be deleted at the time
	of the {\tt Fill-Up}.)

\subsubsection{The Memory Requirement}
\label{memory:requirement}
The following lemma establishes the largest rank that can be present in a 
	heap.
\begin{lemma}
\label{lem:nodesofarank}
There are at most  tree-nodes of rank  when  elements have been 
inserted into it.
\end{lemma}
{\bf Proof:}
We prove this by induction on .
The basis is provided by the rank- nodes.
A node of rank  is created when  new elements have been accumulated in 
	the main memory buffer. 
Since the total number of elements inserted in the heap is , 
	the total number of nodes of rank  is at most . 
Inductively hypothesise that the lemma is true for tree-nodes of
	rank at most . 
Since a node of rank  is generated when
	 root nodes of rank  are combined, the number of 
	nodes of rank  is at most
	.
\hfill 

Therefore, if there is at least one node of rank  in the heap, then 
, and so .
Thus, the rank of the EMSH is at most .
Note that there can be at most  trees of the same rank.

The main memory space required for a bucket is .
So, the total space required for all the buckets is
        .
We can store all buckets in main memory, if we assume that
        .
This assumption is valid for all values of .
Assume the modest values for  and  given in \cite{MR99}: say, a block is of size  KB, and the main
	memory is of size  MB, and can contain  and  records respectively.
Then, if , which is practically always, the buckets will all fit in the main memory.

\section{A Proof of Correctness}
\label{correct:emsh}
If the heap order property is satisfied at every node in the EMSH before an 
	invocation of {\tt Sift}, then it will be satisfied after the
	invocation returns too.
This can be shown as follows.

If  is a pnode, then the invocation causes a series of {\tt Extract}s, each
	of which moves up into a node a set of smallest elements in its children;
	none of them can cause a violation of the heap order property.

If  is a cnode of rank , then a set of smallest elements at 's children
	move up into  and become corrupt. 
All these elements have key values greater than , the ckey of  prior to the {\tt Sift}.
The new ckey of  is set to the largest key  among the elements moving in. 
Thus,  is smaller than each key in each of 's
	children after the invocation; and . 

If  is a cnode of rank greater than , then a set of corrupt elements move
	into  from , the child of  with the smallest ckey, and the ckey
	of  is set to the ckey of .
Inductively assume that the ckey of  is increased by the recursive {\tt Sift} on .  
Therefore, at the end of the {\tt Sift} on , the ckey of  is smaller than the
	ckey of each of 's children.

In every other operation of the EMSH, all data movements between nodes 
	are achieved through {\tt Sift}s.
Thus, they too cannot violate the heap order property.

When we note that a {\tt Fill-Up} on a bucket  moves into it
	a set elements with smallest keys or the smallest ckey from
	its roots, and that 
	the suffixmin pointer of  points to the bucket with the smallest minkey
	among , we have the following Lemma. 

\begin{lemma}
If there is no cnode in the EMSH, then the element returned by {\tt Deletemin}
will be the one with the smallest key in the EMSH.
If there are cnodes, and if the returned element is corrupt (respectively, not corrupt), 
then its ckey (respectively, key) will be the smallest
of all keys in the pnodes and ckeys of the EMSH.
\end{lemma}

For all , and for every nonleaf  of rank  that satisfies CNI, 
the size of the list in  is at least .
For a root  of rank , when the size of its list falls below , {\tt Sift} is not invoked until at least the next invocation
	of {\tt Fill-Up}, {\tt Meld} or {\tt Deletemin}.


The following lemma gives an upperbound on the size of the list.

\begin{lemma}
\label{lem:3sk}
For all , and for every node  of rank , 
the size of the list in  is at most .
\end{lemma}
{\bf proof:}
We prove this by an induction on .
Note that between one {\tt Sift} and another on a node , the list of  can
lose elements, but never gain.  

A {\tt Sift} on a node of rank  causes it to have a list of size at most two;
; this forms the basis. 

Let  be a node of rank . Hypothesise that the upperbound
holds for all nodes of smaller ranks. When {\tt Sift} is called,
repeatedly, a child of  gives  a list  that is then added to the list  of ,
until the size of  becomes at least  or  becomes a leaf.
The size of each  is, by the hypothesis, at most .




The size of  is at most  before the last iteration.
Therefore, its size afterwards can be at most . 
\hfill 


\begin{lemma}
\label{lem:skbounds}
For all values of ,

\end{lemma}

{\bf Proof:}
A simple induction proves the lowerbound. 
Basis: .
Step: For all , .

Similarly, a simple induction shows that, 
for all values of , .
Basis: .
Step: .
Note that
,
, and
,
Therefore, for all values of , .
 \hfill 



\begin{lemma}
If , at any time there are at most  corrupt elements in the EMSH,
where  is the total number of insertions performed.
\end{lemma}
{\bf Proof:}
All corrupt elements are stored in nodes of rank greater than .
The size of the list of a node of rank  is
    at most  by Lemma~\ref{lem:3sk}.
Each listnode contains at most  corrupt elements.
Thus, the total number of corrupt elements at a node of rank 
	is at most .
Suppose . Then .

As , 
by Lemma~\ref{lem:skbounds} and Lemma~\ref{lem:nodesofarank},
the total number of
corrupt elements are at most

\hfill 


\section{An Amortised I/O Analysis}
\label{analy:emsh}

Suppose charges of colours green, red and yellow remain distributed over the
data structure as follows: (i)~each root carries   green charges,
(iii)~each bucket carries  green charges,
(ii)~each element carries  red charges,
(iv)~each nonleaf node carries one yellow charge, and
(v)~each leaf carries  yellow charges.

In addition to these, each element also carries a number of blue charges.
The amount of blue charges that an element carries can vary with its position
in the data structure.

The amortised cost of each operation is its actual cost plus
the total increase in all types charges caused by it.
Now we analyse each operation for its amortised cost.

{\tt Insert}:
Inserts actually cost  I/Os when the insert buffer in the main memory runs full,
which happens at intervals of  inserts.
Note that some {\tt Deletemin}s return elements in the insert buffer.
If an {\tt Insert} causes the buffer to become full, 
then it creates a new node , a tree  with  as its only node, and
a heap  with  as its only tree. The  elements in the buffer
are copied into . Moreover, a bucket is created for .
New charges are created and placed on all new entities.
Thus, this {\tt Insert} creates  green charges,  red charges,
and  yellow charges. Suppose it also places  blue charges
on each element of . That is a total of 
 blue charges on . 
That is, the total increase in the charges of the system is
. 
It follows that the amortised cost of a single {\tt Insert} is
. 

{\tt Meld}:
The buckets are processed for positions  to max in that order, where
max is the largest rank in the two heaps melded. The process is
analogous to the addition of two -radix numbers.
At the -th position, at most three buckets are to be handled:
,  and the ``carry-in'' . 
Assume inductively that each bucket holds
 green charges. 

If only one of the three buckets is present at
position , then there is no I/O to perform, no charge is released, and, therefore,
the amortised cost at position  is zero. 

If at least two of the three are present,
then the actual cost of the operations at position  is .
As only one bucket will be left at position , at least one bucket is
deleted, and so at least  green charges are freed. Suppose,  of this 
pays for the work done. If there is no ``carry-out''  to be formed,
then the amortised cost at position  is negative.

If  is to be formed, then
place  of the remaining green charges
on it. When  is formed,  roots hook
up to a new node; these roots cease to be roots, and so together give 
up  green charges; four of that will be placed on the new root; 
 remain; , as . 
So we have an extra of  green charges to put on the carry-out
which, with that addition, holds  green charges. No charge of other colours is freed.

The amortised cost is non-positive at each position . 
So the total amortised cost of {\tt Meld} is also non-positive.


{\tt Deletemin}:
A typical {\tt Deletemin} is serviced from the main memory, and does
not cause an I/O. Occasionally, however, an invocation
to {\tt Fill-Up} becomes necessary. The actual I/O cost
of such an invocation is . A {\tt Fill-Up} is triggered
in a bucket when  elements are deleted from it.
At most a block of the bucket's elements are kept in the main memory.
Thus, a block will have to be fetched into the main memory once in every
 {\tt Deletemin}s. 
The red charges of deleted items can pay for the cost of the all these I/Os.  
The amortised cost of a {\tt Deletemin} is, therefore, at most zero.

{\tt Findmin}: As no I/O is performed, and no charge is released,
the amortised cost is zero.

{\tt Fill-Up}:
This operation is invoked only from {\tt Meld} or {\tt Deletemin}.
The costs have been accounted for in those.

{\tt Sift}:
Consider a {\tt Sift} on a node  of rank .
This performs one or two Extracts. The actual cost of the Extracts is .
If the number of extracted elements is ,
	then each extracted element can contribute  blue charges 
	to pay off the actual cost.
If the number of extracted elements is , then
 has become a leaf after the {\tt Sift}.
Therefore, all of 's children were leaves before the {\tt Sift}. 
One of them can pay for the {\tt Sift} with its  yellow charges;
	at least one remained at the time of the {\tt Sift}.
Node  that has just become a leaf, has lost the  children 
	it once had. 
If one yellow charge from each child has been preserved in , then
	 now holds  yellow charges, enough for a leaf.

Consider a {\tt Sift} on a node  of rank .
A number of iterations are performed, each of which costs  I/Os.
In each iteration, a number of elements move from a node of rank 
 (namely, the child  of  with the smallest ckey) to a node of rank 
(namely, ). If the number of elements moved is , then
the cost of the iteration can be charged to the elements moved. 
Suppose each element moved contributes  blue charges.
Since the list of  has at least  listnodes,
in which all but the last have at least  elements, the total number of
blue charges contributed is at least 
. 
Thus, the cost of the
	iteration is paid off.

If the number of elements moved is , then  was a leaf prior
to the {\tt Sift}, and so can pay for the {\tt Sift} with its  yellow charges.
If  becomes a leaf at the end of the {\tt Sift}, it will have  yellow charges 
on it, as one yellow charge from each deleted child is preserved in .

An element sheds  blue charges for each level it climbs up, for the first
 levels. After that when it moves up from level  to , it
sheds  blue charges. Therefore, with 

blue charges initially placed on the element, it can pay for its travel upwards. 

Thus, we have the following lemma.
\begin{lemma}
In EMSH, the amortised complexity of an {\tt Insert} is . {\tt Findmin}, {\tt Deletemin} and {\tt Meld} all have non-positive amortised complexity.
\end{lemma}

\section{Hard heap: A Meldable Priority Queue}
\label{hard:emsh}
When an EMSH has error-rate , no element in it can be corrupt.
In this case, EMSH becomes an exact priority queue, which we call hard heap.
In it every node is a pnode, and every tree is a ptree. 
{\tt Deletemin}s always report the exact minimum in the hard heap. 
The height of each tree is , as before.
But, since all nodes are pnodes, the amortised cost of an insertion is
 I/Os. The amortised costs of all 
other operations remain unchanged.

The absence of corrupt nodes will also permit us to implement a {\tt Delete}
operation: To delete the element with key value , insert a ``Delete'' record
with key value . Eventually, when  is the smallest key value in the hard heap,
a {\tt Deletemin} will cause
the element with key  and the ``Delete'' record to come up together. Then the
two can annihilate each other. The amortised cost of a {\tt Delete} is
 I/Os, the same as that of an {\tt Insert}.
 
None of the known external memory priority queues (EMPQs) \cite{Arge03,FJKT99,KS96},
support a {\tt meld} operation. However, in all of them, two queues could be {\tt meld}ed by 
inserting elements of the smaller queue into the larger queue one by one.
This is expensive if the two queues have approximately the same size.
The cost of this is not factored into the amortised complexities of those
EMPQs. 

The actual cost of a {\tt meld} of two hard heap's with  elements each is
 I/Os; the amortised cost of the {\tt meld} is subzero.
But this is the case only if the buckets of
both the heaps are in the main memory. Going by our earlier analysis in Section~\ref{memory:requirement},
if  then  heaps of size  each can keep their buckets in the main memory.

The buckets of the heaps to be melded could be kept in the secondary memory, 
and brought into the main memory, and written back either side of the meld. 
The cost of this can be accounted
by an appropriate scaling of the amortised complexities.
However, operations other than {\tt meld} can be performed only if the buckets are 
in the main memory.
 
In hard heap, unlike in the other EMPQs, 
elements move only in the upward direction. 
That makes hard heap easier to implement.
Hard heap and the external memory heap of \cite{FJKT99} do not require any extra space
other than is necessary for the elements.
The other EMPQs \cite{Arge03,KS96} use extra space (See Table~\ref{comparison:emsh}).
  
The buffer tree \cite{Arge03} is a B+ tree, and therefore uses a balancing procedure.
However, because of delayed deletions, its height may depend on the number of pending
deletions, as well as the number of elements left in it.
The external memory heap of \cite{FJKT99} is a balanced heap, and therefore,
incurs a balancing cost. But, in it the number of
I/Os performed by a sequence of  operations is
, where  is the number
of elements remaining in the heap before the -th operation; this is helpful
when the {\tt insert}s and {\tt deletemin}s are intermixed so that the number
of elements remaining in the data structure at any time is small.  

In comparison, hard heap is not a balanced tree data structure. It does not use a costly
balancing procedure like the heaps of \cite{FJKT99,KS96}.
However, for a sequence of  operations, 
	the amortised cost of each operation is  I/Os.

We can make the amortised cost depend on , the number
of elements remaining in the hard heap before the -th operation, at the cost
of adding a balancing procedure.
In a sequence of operations, whenever , the number of {\tt insert}s performed, and , 
	the number of {\tt deletemin}s performed, satisfy ,
	and the height of the hard heap is , 
	delete the remaining  elements from the hard heap,  insert them back in,
	and set the counts  and  to  and zero respectively;
	we can think of this as the end of an {\em epoch} and the beginning of the next in
	the life of the hard heap. 
The sequence of operations, thus, is a concatenation of several epochs.
Perform an amortised analysis of each epoch independently. 
The cost of reinsertions can be charged to the elements actually deleted in the previous
	epoch, thereby multiplying the amortised cost
	by a factor of .
It is easy to see that now the number of
I/Os performed by a sequence of  operations is
.


\subsection{Heap Sort}
We now discuss an implementation of Heap Sort using hard heap, and
count the number of comparisons performed.
To sort, insert the  input elements into an initially empty hard heap, and 
then perform  {\tt deletemin}s.

When a node of rank  is created,  comparisons are performed;
that is  comparisons per element involved.
When an elements moves from a node to its parent, it participates in a -way merge;
	a -way merge that outputs  elements requires to perform only
	 comparisons; that is  comparisons per element involved.
Since the number of levels in the hard heap is at most , the total
	number of comparisons performed by one element is .
Each {\tt deletemin} operation can cause at most  comparisons
	among the suffixmin pointers.
Thus, the total number of comparisons is . 
\section{Applications of EMSH}
\label{appl:emsh} 
The external memory soft heap data structure is useful for finding exact and 
	approximate medians, and for approximate sorting \cite{Ch00a}.
Each of these computations take  I/Os:
\begin{enumerate}
\item To compute the median in a set of  numbers, insert the numbers
        in an EMSH with error rate .
Next, perform  {\tt Deletemin}s. The largest number  deleted
        has a rank between  and .
Partition the set using  as the pivot in  I/Os.
We can now recurse with a partition of size at most .
The median can be found in  I/Os.
This is an alternative to the algorithm in \cite{Si02} which also requires  I/Os.
\item To approximately sort  items, insert them into an EMSH with error rate , and
        perform  {\tt Deletemin}s consecutively. Each element can form an inversion with
        at most  of the items remaining in the EMSH at the time of its deletion.
        The output sequence, therefore, has at most  inversions.
We can also use EMSH to near sort  numbers in  I/Os such that
        the rank of each number in the output sequence differs from
        its true rank by at most ; the in-core algorithm given in \cite{Ch00a} suffices.
\end{enumerate}

