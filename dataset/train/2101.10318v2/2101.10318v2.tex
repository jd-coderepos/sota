\vspace{-2em}
\section{Experiments}
In this section, we present interpolation and classification experiments using a range of models and three real-world data sets (Physionet Challenge 2012, MIMIC-III, and a Human Activity dataset). Additional illustrative results on synthetic data can be found in Appendix \ref{sec:synthetic_data}. 

\textbf{Datasets:} The PhysioNet Challenge 2012 dataset \citep{physionet} consists of multivariate time series data with $37$ variables extracted from intensive care unit (ICU) records. Each record contains sparse and irregularly spaced measurements from the first $48$ hours after admission to ICU. We follow the procedures of \citet{Rubanova2019} and round the observation times to the nearest minute. This leads to $2880$ possible measurement times per time series. The data set includes $4000$ labeled instances and $4000$ unlabeled instances.  We use all $8000$ instances for interpolation experiments and the $4000$ labeled instances for classification experiments. We focus on predicting in-hospital mortality. $13.8\%$ of examples are in the positive class.

The MIMIC-III data set \citep{johnson2016mimic} is a multivariate time series dataset consisting of sparse and irregularly sampled physiological signals collected at Beth Israel Deaconess Medical Center from 2001 to 2012. Following the procedures  of \citet{shukla2019}, we extract $53,211$ records each containing $12$ physiological variables. We focus on predicting in-hospital mortality using the first $48$ hours of data. $8.1\%$ of the instances have positive labels.


The human activity dataset consists of 3D positions of the waist, chest and ankles collected from five individuals performing various activities including walking, sitting, lying, standing, etc. We follow the data preprocessing steps of \citet{Rubanova2019} and construct a dataset of $6,554$ sequences with $12$ channels and $50$ time points.  We focus on classifying each time point in the sequence into one of eleven types of activities. 





\textbf{Experimental Protocols:} 
We conduct interpolation experiments using the 8000 data cases in the PhysioNet data set. We randomly divide the data set into a training set containing 80\% of the instances, and a test set containing the remaining 20\% of instances. We use 20\% of the training data for validation.
In the interpolation task, we condition on a subset of available points and predict values for rest of the time points. We perform interpolation experiments with a varying percentage of observed points ranging from $50\%$ to $90\%$ of the available points. At test time, the values of observed points are conditioned on and each model is used to infer the values at rest of the available time points in the test instance. We repeat each experiment five times using different random seeds to initialize the model parameters. We assess performance using mean squared error (MSE).



We use the labeled data in all three data sets to conduct classification experiments. The PhysioNet and MIMIC III problems are whole time series classification problems. Note that for the human activity dataset, we classify each time point in the time series. We treat this as a smoothing problem and condition on all available observations when producing the classification at each time-point (similar to labeling in a CRF). We use bidirectional RNNs as the RNN-based baselines for the human activity dataset. We randomly divide each data set into a training set containing 80\% of the time series,  and a test set containing the remaining 20\% of instances. We use 20\% of the training set for validation. We repeat each experiment five times using different random seeds to initialize the model parameters. Due to class imbalance in the Physionet and MIMIC-III data sets, we assess classification performance using area under the ROC curve (the AUC score). For the Human Activity dataset, we evaluate models using accuracy. 

For both interpolation and prediction, we select hyper-parameters on the held-out validation set using grid search, and then apply the best trained model to the test set. The hyper-parameter ranges searched for each model/dataset/task are fully described in Appendix \ref{sec:hyperparamters}.





\begin{table}[t]
\centering
\scriptsize
\caption{Interpolation performance versus percent observed time points on PhysioNet}
\label{table:phy_interp}
\begin{tabular}[h]{l c c c c c}
     \toprule
     {\bf Model} & \multicolumn{5}{c}{{\bf Mean Squared Error} $(\times 10^{-3})$} \\
     \midrule
     RNN-VAE &  $13.418 \pm 0.008$ & $ 12.594 \pm 0.004$ & $11.887 \pm 0.005$ & $11.133 \pm 0.007$ & $ 11.470 \pm 0.006$\\
     L-ODE-RNN & $8.132 \pm 0.020$ & $ 8.140 \pm 0.018$ & $8.171 \pm 0.030$ & $8.143 \pm 0.025$ & $ 8.402 \pm 0.022$ \\
     L-ODE-ODE &  $6.721 \pm 0.109$ & $ 6.816 \pm 0.045$ & $6.798 \pm 0.143$ & $6.850 \pm 0.066$ & $ 7.142 \pm 0.066$\\
     mTAND-Full & ${\bf 4.139} \pm {\bf 0.029}$ & ${\bf 4.018} \pm {\bf 0.048}$ & ${\bf 4.157} \pm {\bf 0.053}$ &  ${\bf 4.410} \pm {\bf 0.149}$ & ${\bf 4.798} \pm {\bf 0.036}$ \\
     \midrule
     Observed \%  & $50\%$ & $60\%$ & $70\%$ & $80\%$ & $90\%$ \\ 
    \bottomrule
\end{tabular}
\end{table}

\textbf{Models:} The model we focus on is the encoder-decoder architecture based on the discretized multi-time attention module (\textbf{mTAND-Full}). 
In the classification experiments, the hidden state at the last observed point is passed to a two-layer binary classification module for all models. For each data set, the structure of this classifier is the same for all models. For the proposed model, the sequence of latent states is first passed through a GRU and then the final hidden state is passed through the same classification module. For the classification task only, we consider an ablation of the full model that uses the proposed mTAND encoder, which consists of our mTAND module followed by a GRU to extract a final hidden state, which is then passed to the classification module (\textbf{mTAND-Enc}). 
We compare to several deep learning models that expand on recurrent networks to accommodate irregular sampling. We also compare to several encoder-decoder approaches. The full list of model variants is briefly described below. We use a Gated Recurrent Unit (GRU) \citep{gru} module as the recurrent network throughout. Architecture details can be found in Appendix \ref{sec:arch}. 
\begin{itemize}[leftmargin=*]
     \item \textbf{RNN-Impute:} Missing observations replaced with weighted average of last observed measurement within that time series and global mean of the variable across training examples \citep{che2016recurrent}.
     \item \textbf{RNN-$\Delta_t$:} Input is concatenated with masking variable and time interval $\Delta_t$ indicating how long the particular variable is missing.
     \item \textbf{RNN-Decay:} RNN with exponential decay on hidden states \citep{mozer2017,che2016recurrent}.
     \item \textbf{GRU-D:} combining hidden state decay with input decay \citep{che2016recurrent}.
     \item \textbf{Phased-LSTM:} Captures time irregularity by a time gate that regulates access to the hidden and cell state of the LSTM \citep{phased2016} with forward filling to handle partially observed vectors. 
     \item \textbf{IP-Nets:} Interpolation prediction networks, which use several semi-parametric RBF interpolation layers, followed by a GRU \citep{shukla2019}.
     \item \textbf{SeFT:} Uses a set function based approach where all the observations are modeled individually before pooling them together using an attention based approach \citep{seft}.
     \item \textbf{RNN-VAE:} A VAE-based model where the encoder and decoder are standard RNN models. 
     \item \textbf{ODE-RNN:} Uses neural ODEs to model hidden state dynamics and an RNN to update the hidden state in presence of a new observation \citep{Rubanova2019}.
    \item \textbf{L-ODE-RNN:}  Latent ODE where the encoder is an RNN and decoder is a neural ODE \citep{neural_ode2018}. 
    \item \textbf{L-ODE-ODE:} Latent ODE where the encoder is an ODE-RNN and decoder is a neural ODE \citep{Rubanova2019}.
\end{itemize}

\textbf{Physionet Experiments:}
Table \ref{table:phy_interp} compares the performance of all methods on the interpolation task where we observe $50\% - 90\%$ of the values in the test instances.
As we can see, the proposed method (mTAND-Full) consistently and substantially outperforms all of the previous approaches across all of the settings of observed time points. We note that in this experiment, different columns correspond to different setting (for example, in the case of $70\%$, we condition on $70\%$ of data and predict the rest of the data; i.e., $30\%$) and, hence the results across columns are not comparable. 


Table \ref{table:classif} compares predictive performance on the PhysioNet mortality prediction task. The full Multi-Time Attention network model (mTAND-Full) and the classifier based only on the Multi-Time Attention network encoder (mTAND-Enc) achieve significantly improved performance relative to the current state-of-the-art methods (ODE-RNN and L-ODE-ODE) and other baseline methods. 

We also report the time per epoch in minutes for all the methods. We note that the ODE-based models require substantially more run time than other methods due to the required use of an ODE solver \citep{neural_ode2018, Rubanova2019}. These methods also require taking the union of all observation time points in a batch, which further slows down the training process. As we can see, the proposed full Multi-Time Attention network (mTAND-Full) is over 85 times faster than ODE-RNN and over 100 times faster than L-ODE-ODE, the best-performing ODE-based models.









\begin{table}[t]
\centering
\footnotesize
    \caption{Classification Performance on PhysioNet, MIMIC-III and Human Activity dataset}
    \label{table:classif}
     \begin{tabular}[h]{l c c c r}
        \toprule
        \multirow{2}{*}{\bf Model} & \multicolumn{2}{c}{\bf AUC Score} & {\bf Accuracy} & \multirow{2}{*}{\bf time}\\
        \cmidrule{2-4}
        & {\bf PhysioNet} & {\bf MIMIC-III} & {\bf Human Activity} & {\bf per epoch}\\
        \midrule
        RNN-Impute	    & $	0.764	\pm	0.016	$ & $0.8249 \pm 0.0010$ & $	0.859	\pm	0.004	$ & $ 0.5$\\
        RNN-$\Delta_t$	& $	0.787	\pm	0.014	$ & $0.8364 \pm 0.0011$ & $	0.857	\pm	0.002	$ & $ 0.5$\\
        RNN-Decay	    & $	0.807	\pm	0.003	$ & $0.8392 \pm 0.0012$ & $	0.860	\pm	0.005	$ & $ 0.7$\\
        RNN GRU-D	    & $	0.818	\pm	0.008	$ & $0.8270 \pm 0.0010$ & $	0.862	\pm	0.005	$ & $ 0.7$\\
        Phased-LSTM     & $ 0.836   \pm 0.003   $ & $0.8429 \pm 0.0035$ & $ 0.855   \pm 0.005   $ & $ 0.3$\\    
        IP-Nets	        & $	0.819	\pm	0.006	$ & $0.8390 \pm 0.0011$ & $ 0.869   \pm 0.007   $ & $ 1.3$\\
        SeFT	        & $	0.795	\pm	0.015	$ & $0.8485 \pm 0.0022$ & $ 0.815   \pm 0.002$ & $ 0.5 $\\
        RNN-VAE	        & $	0.515	\pm	0.040	$ & $0.5175 \pm 0.0312$ & $	0.343	\pm	0.040	$ & $ 2.0$\\
        ODE-RNN	        & $	0.833	\pm	0.009	$ & $\bf{0.8561}\pm \bf{0.0051}	$ & $	0.885	\pm	0.008	$ & $ 16.5$\\
        L-ODE-RNN 	    & $	0.781	\pm	0.018	$ & $0.7734	\pm	0.0030$ & $	0.838	\pm	0.004	$ & $ 6.7$\\
        L-ODE-ODE	    & $	0.829	\pm	0.004	$ & $\bf{0.8559}\pm	\bf{0.0041}	$ & $	0.870	\pm	0.028	$ & $ 22.0$\\
        \midrule 
        {mTAND-Enc}	& $	{0.854	\pm	0.001}	$ & $0.8419 \pm 0.0017$ & $	\bf{0.907 \pm 0.002}$ & $ \bf{0.1}$\\
        {mTAND-Full}& $	\bf{0.858	\pm	0.004}	$ & $\bf{0.8544} \pm \bf{0.0024}$ & $   \bf{0.910 \pm 0.002}$ & $ 0.2$\\
        \bottomrule
    \end{tabular}
\end{table}



\textbf{MIMIC-III Experiments:}
Table \ref{table:classif} compares the  predictive performance of the models on the mortality prediction task on MIMIC-III.  The Multi-Time Attention network-based encoder-decoder framework (mTAND-Full) achieves better performance than the recent IP-Net and SeFT model as well as all of the RNN baseline models. While 
ODE-RNN and L-ODE-ODE both have slightly better mean AUC than mTAND-Full, the differences are not statistically significant. Further, as shown on the PhysioNet classification problem, mTAND-Full is more than an order of magnitude faster than the ODE-based methods.

\textbf{Human Activity Experiments:}
Table \ref{table:classif} shows that the mTAND-based classifiers achieve significantly better performance than the baseline models on this prediction task, followed by ODE-based models and IP-Nets. 

\textbf{Additional Experiments:} In Appendix \ref{sec:synthetic_data}, we demonstrate the effectiveness of learning  temporally distributed latent representations with mTANs on a synthetic dataset. We show that mTANs are able to capture local structure in the time series better than latent ODE-based methods that encode to a single time point. This property of mTANs helps to improve the interpolation performance in terms of mean squared error.

We also perform ablation experiments to show the performance gain achieved by learning similarity kernels and time embeddings in Appendix \ref{sec:ablation}. In particular, we show that learning the time embedding improves classification performance compared to using fixed positional encodings. We also demonstrate the effectiveness of learning the similarity kernel by comparing to an approach that uses fixed RBF kernels. Appendix \ref{sec:ablation} shows that  learning the similarity kernel using the mTAND module performs as well as or better than using a fixed RBF kernel.
