\documentclass[runningheads]{llncs}

\let\vec\relax
\DeclareMathAccent{\vec}{\mathord}{letters}{"7E}

\usepackage{graphicx}
\usepackage{comment}
\usepackage{amsmath,amssymb} 
\usepackage{color}
\usepackage{lipsum}
\usepackage{multirow}
\usepackage{pifont}
\usepackage[shortlabels]{enumitem}

\newcommand{\nicepar}[1]{\paragraph{\normalfont\textbf{#1}}}

\newcommand{\luca}[1]{\textcolor{red}{LUCA:#1}}
\newcommand{\angelo}[1]{\textcolor{blue}{ANGELO:#1}}

\newcommand{\VKD}{\textbf{VKD}}

\renewcommand{\vec}[1]{\bm{#1}}

\newcommand{\mars}{MARS}
\newcommand{\duke}{Duke}
\newcommand{\dukefull}{Duke-Video-ReID}
\newcommand{\veri}{VeRi-776}
\newcommand{\atrw}{ATRW}

\newcommand{\argmin}{\operatornamewithlimits{argmin}}

\newcommand{\norm}[1]{\left\lVert#1\right\rVert} 
\newcommand{\loss}[1]{\mathcal{L}_\textnormal{#1}}
\newcommand{\lossnnz}[1]{\mathcal{L}_{\textnormal{#1} \neq 0}}
\newcommand{\nnfn}{f_\theta}

\newcommand{\resnet}[1]{ResNet-#1}
\newcommand{\tea}[1]{\resnet{#1}}
\newcommand{\stf}[1]{ResVKD-#1}

\newcommand{\teadense}{DenseNet-121}
\newcommand{\stfdense}{DenseVKD-121}

\newcommand{\teamobile}{MobileNet-V2}
\newcommand{\stfmobile}{MobileVKD-V2}
\newcommand{\R}{\mathbb{R}}

\newcommand{\topk}[1]{top$_{#1}$}


\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}

\newcommand{\attnpsup}[1]{\includegraphics[width=0.08\columnwidth]{img/attn_grid_suppl/#1.png}}
\newcommand{\attnp}[1]{\includegraphics[width=0.08\columnwidth]{img/attn_grid/#1.png}}
\newcommand{\attnv}[1]{\includegraphics[width=0.1\columnwidth]{img/attn_grid/#1.png}}
\newcommand{\attnt}[1]{\includegraphics[width=0.2\columnwidth]{img/attn_grid/#1.png}}


\newcommand{\trackpersonimg}[1]{\includegraphics[width=0.11\columnwidth]{img/dist_matrix_person/#1.png}}
\newcommand{\trackvehicleimg}[1]{\includegraphics[width=0.156\columnwidth]{img/dist_matrix_vehicle/#1.png}}

\newcommand{\reid}{Re-ID}
\newcommand{\reidlong}{Re-Identification}

\newcommand{\quotationmarks}[1]{``#1''}
 \begin{document}

\pagestyle{headings}
\mainmatter
\def\ECCVSubNumber{996}  

\title{Robust Re-Identification by Multiple Views Knowledge Distillation - Supplementary Material} 

\titlerunning{Supplementary Material}

\author{Angelo Porrello \orcidID{0000-0002-9022-8484},
Luca Bergamini \orcidID{0000-0003-1221-8640},
Simone Calderara \orcidID{0000-0001-9056-1538}}
\authorrunning{A. Porrello, L. Bergamini, S. Calderara}
\institute{AImageLab, University of Modena and Reggio Emilia\\
\email{\{angelo.porrello, luca.bergamini24, simone.calderara\}@unimore.it}
}
\maketitle

\begin{table}[b]
    \centering
    \caption{Analysis on camera bias -- in terms of viewpoint classification accuracy -- for different methods. We indicate with \quotationmarks{ResTKD-50} a student restricted to time information solely.}
    \label{tab:biastimevsmv}
    \setlength{\tabcolsep}{5pt}
    \begin{tabular}{l|c|c}
    \hline
    \multirow{2}{*}{} & \multicolumn{1}{c|}{\mars{}} & \multicolumn{1}{c}{\duke{}}\\
    \hline
    Prior Class. & 0.19 & 0.14 \\
    \hline\hline
    \tea{50} (teacher) & \textbf{0.74} & \textbf{0.76}\\
    ResTKD-50 (time-based distillation) & 0.69 & \textbf{0.76}\\
    \stf{50} (viewpoints-based distillation) & 0.49 & 0.69 \\
    \hline
    \end{tabular}
\end{table}
\nicepar{Distilling viewpoints \textit{vs} time: impact on camera bias.} As discussed in the main paper (Introduction and Sec.~4.4), limiting the teacher-student transfer to the temporal axis does not explicitly encourage invariance and robustness to different viewpoints. To further prove such a claim, we again measure the camera bias lying in high-level features, in the same manner as described in Sec.~4.4 of the main paper. This time, though, we focus on a student accessing fewer frames from the same tracklet, thus being educated to capture time information solely. Table~\ref{tab:biastimevsmv} compares this strategy (third row) with our proposal (fourth row), which instead forces the transfer at viewpoint level. As expected: \textit{i)} time-based distillation performs similarly to the teacher, confirming its poor ability to confer robustness to shifts in background appearance; \textit{ii)} as advocated by our work, a student shows a lower camera bias when trained on different viewpoints instead of using temporal information only.
\nicepar{Student explanation - other examples.} In Sec.~4.4 of the main paper, we investigate which regions the student focuses on, showing that it pays higher attention to foreground details when compared to its teacher. We observe that this happens systematically, especially when dealing with person Re-ID. Figure~\ref{fig:attention_suppl} reports additional comparisons between the explanations provided by the teacher and its student on \dukefull{}~\cite{wu2018duke1}.
\begin{figure}[t]
\centering
\resizebox{0.95\textwidth}{!}{\begin{tabular}{p{0.05\textwidth}llllllllll}
\rotatebox{90}{\quad \quad \quad input} & \attnpsup{duke_13} &  \attnpsup{duke_3} & \attnpsup{duke_4} &
\attnpsup{duke_5} &  \attnpsup{duke_6} & \attnpsup{duke_7} &
\attnpsup{duke_9} & \attnpsup{duke_11} & \attnpsup{duke_1} & \attnpsup{duke_18} \\\hline \noalign{\vskip 1mm}
\rotatebox{90}{\quad \quad teacher} & \attnpsup{duke_tea_13} &  \attnpsup{duke_tea_3} & \attnpsup{duke_tea_4} &
\attnpsup{duke_tea_5} &  \attnpsup{duke_tea_6} & \attnpsup{duke_tea_7} &
\attnpsup{duke_tea_9} & \attnpsup{duke_tea_11} & \attnpsup{duke_tea_1} & \attnpsup{duke_tea_18} \\\hline \noalign{\vskip 1mm}
\rotatebox{90}{\quad \quad \textbf{student}} & \attnpsup{duke_std_13} &  \attnpsup{duke_std_3} & \attnpsup{duke_std_4} &
\attnpsup{duke_std_5} &  \attnpsup{duke_std_6} & \attnpsup{duke_std_7} &
\attnpsup{duke_std_9} & \attnpsup{duke_std_11} & \attnpsup{duke_std_1} & \attnpsup{duke_std_18} \\
\end{tabular}
}
\caption{Model explanation (\dukefull{}) on \tea{50} (teacher) and \stf{50} (student).}
\label{fig:attention_suppl}
\end{figure}
 \nicepar{Errors Analysis} We provide here some visual examples of the errors of our method and try to investigate their nature. With reference to the Video-To-Video setting on \mars{}~\cite{zheng2016mars}, our model (\stf{50}) misidentifies 223 out of 1980 top-1 matchings. From an analysis computed on top of these 223 cases, we identify four different categories of errors. We also asked two external researchers to annotate the errors according to these four classes as follows: 

\begin{enumerate}[a)]
    \item \textbf{True errors}: the network associates the query to a wrong identity from the gallery set (Figure~\ref{fig:errors}a). This often happens when similar clothes and appearances between the two identities fool the network. Out of 223, 103 (\textbf{46.2\%}) were identified as true errors;
    \item \textbf{Wrong ID Annotations}: the ground truth indicates that the network associates the query to a wrong identity from the gallery set. However -- for a limited set of queries -- this does not hold true when visually inspecting the gallery identity. This is due to annotation errors, probably caused by a drift in the tracker (Figure~\ref{fig:errors}b). Out of 223, 29 (\textbf{13.0\%}) were identified as true errors; 
    \item \textbf{Couples of People}: some crops depict more than one subject (\textit{e.g.} two) but only one can be associated with the tracklet id (Figure~\ref{fig:errors}c). Out of 223, 37 (\textbf{16.6\%}) were identified as errors involving frames with more than one person;
    \item \textbf{Misleading Distractors}: cases in which the subject has been correctly identified, but the gallery tracklet was erroneously indicated as a distractor. Again, because this set has not been manually checked, some distractors are valid as they depict people (Figure~\ref{fig:errors}d). Out of 223, 54 (\textbf{24.2\%}) were identified as misleading distractors;
\end{enumerate}
\begin{figure}[t]
    \begin{minipage}[t][][b]{.65\linewidth}
    \centering
    \includegraphics[width=.99\linewidth]{img/errors/true_errors.pdf}
    \\\small{(a) True Errors}.
    \end{minipage} 
     \begin{minipage}[t][][b]{.34\linewidth}
    \centering
    \includegraphics[width=.99\linewidth]{img/errors/annot_errors.pdf}
    \\\small{(b) Wrong ID Annotations}.
    \end{minipage}
    \vskip.5\baselineskip \begin{minipage}[t][][b]{.59\linewidth}
    \centering
    \includegraphics[width=.99\linewidth]{img/errors/couple_errors.pdf} 
    \\\small{(c) Couples of People}.
    \end{minipage} 
     \begin{minipage}[t][][b]{.40\linewidth}
    \centering
    \includegraphics[width=.99\linewidth]{img/errors/distr_errors.pdf} \\\small{(d) Misleading Distractors}.
    \end{minipage}
    \caption{Different categories of errors on \mars{}. While almost half of them can be attributed to our method misidentifying between similar appearances (a), the other half are due to the automatic annotation process. In particular, wrong annotation caused by tracking drift (b), more than one identity in the same tracklet (c) and misleading distractors (d).}
\label{fig:errors}
\hfill
\end{figure} \noindent It is worth noting that the presence of the last three types of errors places a limit on the maximum score a method can obtain.
\bibliographystyle{splncs04}
\bibliography{main}
\end{document}