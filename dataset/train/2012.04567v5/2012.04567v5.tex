\documentclass{article}




\PassOptionsToPackage{sort, numbers, compress}{natbib}




\usepackage[final]{neurips_2021}



\usepackage[dvipsnames]{xcolor}
\definecolor{darkb}{rgb}{0.2,0.2,0.8}
\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage[hidelinks,colorlinks = true,
            linkcolor = darkb,
            urlcolor  = darkb,
            citecolor = darkb,
            anchorcolor = darkb]{hyperref}
\usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         

\title{Bayesian Image Reconstruction using Deep Generative Models}



\author{Razvan V.~Marinescu\\MIT CSAIL\\
  \texttt{razvan@csail.mit.edu} \\
  \And
  Daniel Moyer \\
  MIT CSAIL \\
  \texttt{dmoyer@csail.mit.edu} \\
  \AND
  Polina Golland \\
  MIT CSAIL \\
  \texttt{polina@csail.mit.edu} \\
}




\usepackage{filecontents}


\begin{filecontents*}[overwrite]{bibliography.bib}
@inproceedings{menon2020pulse,
  title={{PULSE}: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models},
  author={Menon, Sachit and Damian, Alexandru and Hu, Shijia and Ravi, Nikhil and Rudin, Cynthia},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={2437--2445},
  year={2020}
}
@inproceedings{wang2018esrgan,
  title={Esrgan: Enhanced super-resolution generative adversarial networks},
  author={Wang, Xintao and Yu, Ke and Wu, Shixiang and Gu, Jinjin and Liu, Yihao and Dong, Chao and Qiao, Yu and Change Loy, Chen},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={0--0},
  year={2018}
}
@inproceedings{li2019feedback,
  title={Feedback network for image super-resolution},
  author={Li, Zhen and Yang, Jinglei and Liu, Zheng and Yang, Xiaomin and Jeon, Gwanggil and Wu, Wei},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={3867--3876},
  year={2019}
}
@book{peters2017elements,
  title={Elements of causal inference},
  author={Peters, Jonas and Janzing, Dominik and Sch{\"o}lkopf, Bernhard},
  year={2017},
  publisher={The MIT Press}
}
@inproceedings{karras2019style,
  title={A style-based generator architecture for generative adversarial networks},
  author={Karras, Tero and Laine, Samuli and Aila, Timo},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4401--4410},
  year={2019}
}
@inproceedings{karras2020analyzing,
  title={Analyzing and improving the image quality of stylegan},
  author={Karras, Tero and Laine, Samuli and Aittala, Miika and Hellsten, Janne and Lehtinen, Jaakko and Aila, Timo},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={8110--8119},
  year={2020}
}
@article{karras2020training,
  title={Training generative adversarial networks with limited data},
  author={Karras, Tero and Aittala, Miika and Hellsten, Janne and Laine, Samuli and Lehtinen, Jaakko and Aila, Timo},
  journal={arXiv preprint arXiv:2006.06676},
  year={2020}
}
@article{johnson2016mimic,
  title={{MIMIC-III}, a freely accessible critical care database},
  author={Johnson, Alistair EW and Pollard, Tom J and Shen, Lu and Li-Wei, H Lehman and Feng, Mengling and Ghassemi, Mohammad and Moody, Benjamin and Szolovits, Peter and Celi, Leo Anthony and Mark, Roger G},
  journal={Scientific data},
  volume={3},
  number={1},
  pages={1--9},
  year={2016},
  publisher={Nature Publishing Group}
}
@article{brock2018large,
  title={Large scale gan training for high fidelity natural image synthesis},
  author={Brock, Andrew and Donahue, Jeff and Simonyan, Karen},
  journal={arXiv preprint arXiv:1809.11096},
  year={2018}
}
@inproceedings{van2017neural,
  title={Neural discrete representation learning},
  author={Van Den Oord, Aaron and Vinyals, Oriol and others},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6306--6315},
  year={2017}
}
@inproceedings{van2016conditional,
  title={Conditional image generation with pixelcnn decoders},
  author={Van den Oord, Aaron and Kalchbrenner, Nal and Espeholt, Lasse and Vinyals, Oriol and Graves, Alex and others},
  booktitle={Advances in neural information processing systems},
  pages={4790--4798},
  year={2016}
}
@article{oord2016pixel,
  title={Pixel recurrent neural networks},
  author={Oord, Aaron van den and Kalchbrenner, Nal and Kavukcuoglu, Koray},
  journal={arXiv preprint arXiv:1601.06759},
  year={2016}
}
@article{higgins2016beta,
  title={beta-{VAE}: {Learning} basic visual concepts with a constrained variational framework},
  author={Higgins, Irina and Matthey, Loic and Pal, Arka and Burgess, Christopher and Glorot, Xavier and Botvinick, Matthew and Mohamed, Shakir and Lerchner, Alexander},
  year={2016}
}
@inproceedings{kingma2018glow,
  title={Glow: Generative flow with invertible 1x1 convolutions},
  author={Kingma, Durk P and Dhariwal, Prafulla},
  booktitle={Advances in neural information processing systems},
  pages={10215--10224},
  year={2018}
}
@article{dinh2016density,
  title={Density estimation using real nvp},
  author={Dinh, Laurent and Sohl-Dickstein, Jascha and Bengio, Samy},
  journal={arXiv preprint arXiv:1605.08803},
  year={2016}
}
@inproceedings{chen2018neural,
  title={Neural ordinary differential equations},
  author={Chen, Ricky TQ and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K},
  booktitle={Advances in neural information processing systems},
  pages={6571--6583},
  year={2018}
}
@article{leeb2020structural,
  title={Structural autoencoders improve representations for generation and transfer},
  author={Leeb, Felix and Annadani, Yashas and Bauer, Stefan and Sch{\"o}lkopf, Bernhard},
  journal={arXiv preprint arXiv:2006.07796},
  year={2020}
}
@article{adler2018deep,
  title={Deep bayesian inversion},
  author={Adler, Jonas and {\"O}ktem, Ozan},
  journal={arXiv preprint arXiv:1811.05910},
  year={2018}
}
@article{dashti2013bayesian,
  title={The Bayesian approach to inverse problems},
  author={Dashti, Masoumeh and Stuart, Andrew M},
  journal={arXiv preprint arXiv:1302.6989},
  year={2013}
}
@article{bora2018ambientgan,
  title={AmbientGAN: Generative models from lossy measurements.},
  author={Bora, Ashish and Price, Eric and Dimakis, Alexandros G},
  journal={ICLR},
  volume={2},
  number={5},
  pages={3},
  year={2018}
}
@article{bora2017compressed,
  title={Compressed sensing using generative models},
  author={Bora, Ashish and Jalal, Ajil and Price, Eric and Dimakis, Alexandros G},
  journal={arXiv preprint arXiv:1703.03208},
  year={2017}
}
@inproceedings{ledig2017photo,
  title={Photo-realistic single image super-resolution using a generative adversarial network},
  author={Ledig, Christian and Theis, Lucas and Husz{\'a}r, Ferenc and Caballero, Jose and Cunningham, Andrew and Acosta, Alejandro and Aitken, Andrew and Tejani, Alykhan and Totz, Johannes and Wang, Zehan and others},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4681--4690},
  year={2017}
}
@inproceedings{pathak2016context,
  title={Context encoders: Feature learning by inpainting},
  author={Pathak, Deepak and Krahenbuhl, Philipp and Donahue, Jeff and Darrell, Trevor and Efros, Alexei A},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2536--2544},
  year={2016}
}
@inproceedings{tikhonov1963solution,
  title={On the solution of ill-posed problems and the method of regularization},
  author={Tikhonov, Andrei Nikolaevich},
  booktitle={Doklady Akademii Nauk},
  volume={151},
  number={3},
  pages={501--504},
  year={1963},
  organization={Russian Academy of Sciences}
}
@inproceedings{figueiredo2005bound,
  title={A bound optimization approach to wavelet-based image deconvolution},
  author={Figueiredo, M{\'a}rio AT and Nowak, Robert D},
  booktitle={IEEE International Conference on Image Processing 2005},
  volume={2},
  pages={II--782},
  year={2005},
  organization={IEEE}
}
@inproceedings{mairal2009online,
  title={Online dictionary learning for sparse coding},
  author={Mairal, Julien and Bach, Francis and Ponce, Jean and Sapiro, Guillermo},
  booktitle={Proceedings of the 26th annual international conference on machine learning},
  pages={689--696},
  year={2009}
}
@article{aharon2006k,
  title={{K-SVD}: An algorithm for designing overcomplete dictionaries for sparse representation},
  author={Aharon, Michal and Elad, Michael and Bruckstein, Alfred},
  journal={IEEE Transactions on signal processing},
  volume={54},
  number={11},
  pages={4311--4322},
  year={2006},
  publisher={IEEE}
}
@article{zhu2018image,
  title={Image reconstruction by domain-transform manifold learning},
  author={Zhu, Bo and Liu, Jeremiah Z and Cauley, Stephen F and Rosen, Bruce R and Rosen, Matthew S},
  journal={Nature},
  volume={555},
  number={7697},
  pages={487--492},
  year={2018},
  publisher={Nature Publishing Group}
}
@inproceedings{yu2019free,
  title={Free-form image inpainting with gated convolution},
  author={Yu, Jiahui and Lin, Zhe and Yang, Jimei and Shen, Xiaohui and Lu, Xin and Huang, Thomas S},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={4471--4480},
  year={2019}
}
@inproceedings{yu2018generative,
  title={Generative image inpainting with contextual attention},
  author={Yu, Jiahui and Lin, Zhe and Yang, Jimei and Shen, Xiaohui and Lu, Xin and Huang, Thomas S},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={5505--5514},
  year={2018}
}
@article{yang2017dagan,
  title={{DAGAN}: Deep de-aliasing generative adversarial networks for fast compressed sensing {MRI} reconstruction},
  author={Yang, Guang and Yu, Simiao and Dong, Hao and Slabaugh, Greg and Dragotti, Pier Luigi and Ye, Xujiong and Liu, Fangde and Arridge, Simon and Keegan, Jennifer and Guo, Yike and others},
  journal={IEEE transactions on medical imaging},
  volume={37},
  number={6},
  pages={1310--1321},
  year={2017},
  publisher={IEEE}
}
@article{jack2008alzheimer,
  title={The {Alzheimer}'s disease neuroimaging initiative ({ADNI}): {MRI} methods},
  author={Jack Jr, Clifford R and Bernstein, Matt A and Fox, Nick C and Thompson, Paul and Alexander, Gene and Harvey, Danielle and Borowski, Bret and Britson, Paula J and L. Whitwell, Jennifer and Ward, Chadwick and others},
  journal={Journal of Magnetic Resonance Imaging: An Official Journal of the International Society for Magnetic Resonance in Medicine},
  volume={27},
  number={4},
  pages={685--691},
  year={2008},
  publisher={Wiley Online Library}
}
@article{marcus2010open,
  title={Open access series of imaging studies: longitudinal {MRI} data in nondemented and demented older adults},
  author={Marcus, Daniel S and Fotenos, Anthony F and Csernansky, John G and Morris, John C and Buckner, Randy L},
  journal={Journal of cognitive neuroscience},
  volume={22},
  number={12},
  pages={2677--2684},
  year={2010},
  publisher={MIT Press}
}
@article{heinsfeld2018identification,
  title={Identification of autism spectrum disorder using deep learning and the {ABIDE} dataset},
  author={Heinsfeld, Anibal S{\'o}lon and Franco, Alexandre Rosa and Craddock, R Cameron and Buchweitz, Augusto and Meneguzzi, Felipe},
  journal={NeuroImage: Clinical},
  volume={17},
  pages={16--23},
  year={2018},
  publisher={Elsevier}
}
@article{marek2018parkinson,
  title={The {Parkinson}'s progression markers initiative {(PPMI)}--establishing a {PD} biomarker cohort},
  author={Marek, Kenneth and Chowdhury, Sohini and Siderowf, Andrew and Lasch, Shirley and Coffey, Christopher S and Caspell-Garcia, Chelsea and Simuni, Tanya and Jennings, Danna and Tanner, Caroline M and Trojanowski, John Q and others},
  journal={Annals of clinical and translational neurology},
  volume={5},
  number={12},
  pages={1460--1477},
  year={2018},
  publisher={Wiley Online Library}
}
@article{ellis2009australian,
  title={The Australian Imaging, Biomarkers and Lifestyle {(AIBL)} study of aging: methodology and baseline characteristics of 1112 individuals recruited for a longitudinal study of {Alzheimer}'s disease},
  author={Ellis, Kathryn A and Bush, Ashley I and Darby, David and De Fazio, Daniela and Foster, Jonathan and Hudson, Peter and Lautenschlager, Nicola T and Lenzo, Nat and Martins, Ralph N and Maruff, Paul and others},
  journal={International psychogeriatrics},
  volume={21},
  number={4},
  pages={672--687},
  year={2009},
  publisher={Cambridge University Press}
}
@inproceedings{abdal2019image2stylegan,
  title={Image2stylegan: How to embed images into the stylegan latent space?},
  author={Abdal, Rameen and Qin, Yipeng and Wonka, Peter},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={4432--4441},
  year={2019}
}
@inproceedings{abdal2020image2stylegan,
  title={Image2StyleGAN++: How to Edit the Embedded Images?},
  author={Abdal, Rameen and Qin, Yipeng and Wonka, Peter},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={8296--8305},
  year={2020}
}
@inproceedings{zhang2018unreasonable,
  title={The unreasonable effectiveness of deep features as a perceptual metric},
  author={Zhang, Richard and Isola, Phillip and Efros, Alexei A and Shechtman, Eli and Wang, Oliver},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={586--595},
  year={2018}
}
@article{tudosiu2020neuromorphologicaly,
  title={Neuromorphologicaly-preserving Volumetric data encoding using {VQ-VAE}},
  author={Tudosiu, Petru-Daniel and Varsavsky, Thomas and Shaw, Richard and Graham, Mark and Nachev, Parashkev and Ourselin, Sebastien and Sudre, Carole H and Cardoso, M Jorge},
  journal={arXiv preprint arXiv:2002.05692},
  year={2020}
}
@article{richardson2020encoding,
  title={Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation},
  author={Richardson, Elad and Alaluf, Yuval and Patashnik, Or and Nitzan, Yotam and Azar, Yaniv and Shapiro, Stav and Cohen-Or, Daniel},
  journal={arXiv preprint arXiv:2008.00951},
  year={2020}
}
@inproceedings{dalca2018anatomical,
  title={Anatomical priors in convolutional networks for unsupervised biomedical segmentation},
  author={Dalca, Adrian V and Guttag, John and Sabuncu, Mert R},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={9290--9299},
  year={2018}
}
@article{kingma2013auto,
  title={Auto-encoding variational bayes},
  author={Kingma, Diederik P and Welling, Max},
  journal={arXiv preprint arXiv:1312.6114},
  year={2013}
}
@inproceedings{mantiuk2012comparison,
  title={Comparison of four subjective methods for image quality assessment},
  author={Mantiuk, Rafa{\l} K and Tomaszewska, Anna and Mantiuk, Rados{\l}aw},
  booktitle={Computer graphics forum},
  volume={31},
  number={8},
  pages={2478--2491},
  year={2012},
  organization={Wiley Online Library}
}
@article{simonyan2014very,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}
@inproceedings{cvitkovic2019minimal,
  title={Minimal achievable sufficient statistic learning},
  author={Cvitkovic, Milan and Koliander, G{\"u}nther},
  booktitle={International Conference on Machine Learning},
  pages={1465--1474},
  year={2019},
  organization={PMLR}
}
@book{krantz2008geometric,
  title={Geometric integration theory},
  author={Krantz, Steven G and Parks, Harold R},
  year={2008},
  publisher={Springer Science \& Business Media}
}
@inproceedings{roth2005fields,
  title={Fields of experts: A framework for learning image priors},
  author={Roth, Stefan and Black, Michael J},
  booktitle={2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)},
  volume={2},
  pages={860--867},
  year={2005},
  organization={IEEE}
}
@article{brainard1997bayesian,
  title={Bayesian color constancy},
  author={Brainard, David H and Freeman, William T},
  journal={JOSA A},
  volume={14},
  number={7},
  pages={1393--1411},
  year={1997},
  publisher={Optical Society of America}
}
@inproceedings{levin2003learning,
  title={Learning How to Inpaint from Global Image Statistics.},
  author={Levin, Anat and Zomet, Assaf and Weiss, Yair}
}
@article{freeman1994generic,
  title={The generic viewpoint assumption in a framework for visual perception},
  author={Freeman, William T},
  journal={Nature},
  volume={368},
  number={6471},
  pages={542--545},
  year={1994},
  publisher={Nature Publishing Group}
}
@inproceedings{russell2003exploiting,
  title={Exploiting the sparse derivative prior for super-resolution and image demosaicing},
  author={Russell, Marshall F Tappen Bryan C and Freeman, William T}
}
@article{tipping2003bayesian,
  title={Bayesian image super-resolution},
  author={Tipping, Michael E and Bishop, Christopher M and others},
  journal={Advances in neural information processing systems},
  pages={1303--1310},
  year={2003},
  publisher={Citeseer}
}
@inproceedings{rezende2014stochastic,
  title={Stochastic backpropagation and approximate inference in deep generative models},
  author={Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
  booktitle={International conference on machine learning},
  pages={1278--1286},
  year={2014},
  organization={PMLR}
}
@inproceedings{blundell2015weight,
  title={Weight uncertainty in neural network},
  author={Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
  booktitle={International Conference on Machine Learning},
  pages={1613--1622},
  year={2015},
  organization={PMLR}
}
@inproceedings{khan2018fast,
  title={Fast and scalable bayesian deep learning by weight-perturbation in adam},
  author={Khan, Mohammad and Nielsen, Didrik and Tangkaratt, Voot and Lin, Wu and Gal, Yarin and Srivastava, Akash},
  booktitle={International Conference on Machine Learning},
  pages={2611--2620},
  year={2018},
  organization={PMLR}
}
@inproceedings{welling2011bayesian,
  title={Bayesian learning via stochastic gradient Langevin dynamics},
  author={Welling, Max and Teh, Yee W},
  booktitle={Proceedings of the 28th international conference on machine learning (ICML-11)},
  pages={681--688},
  year={2011},
  organization={Citeseer}
}
@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}
@inproceedings{ulyanov2018deep,
  title={Deep image prior},
  author={Ulyanov, Dmitry and Vedaldi, Andrea and Lempitsky, Victor},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={9446--9454},
  year={2018}
}
@inproceedings{pan2020exploiting,
  title={Exploiting deep generative prior for versatile image restoration and manipulation},
  author={Pan, Xingang and Zhan, Xiaohang and Dai, Bo and Lin, Dahua and Loy, Chen Change and Luo, Ping},
  booktitle={European Conference on Computer Vision},
  pages={262--277},
  year={2020},
  organization={Springer}
}
@inproceedings{mildenhall2020nerf,
  title={Nerf: Representing scenes as neural radiance fields for view synthesis},
  author={Mildenhall, Ben and Srinivasan, Pratul P and Tancik, Matthew and Barron, Jonathan T and Ramamoorthi, Ravi and Ng, Ren},
  booktitle={European Conference on Computer Vision},
  pages={405--421},
  year={2020},
  organization={Springer}
}
@article{anirudh2020mimicgan,
  title={Mimicgan: Robust projection onto image manifolds with corruption mimicking},
  author={Anirudh, Rushil and Thiagarajan, Jayaraman J and Kailkhura, Bhavya and Bremer, Peer-Timo},
  journal={International Journal of Computer Vision},
  pages={1--19},
  year={2020},
  publisher={Springer}
}
@inproceedings{asim2020invertible,
  title={Invertible generative models for inverse problems: mitigating representation error and dataset bias},
  author={Asim, Muhammad and Daniels, Max and Leong, Oscar and Ahmed, Ali and Hand, Paul},
  booktitle={International Conference on Machine Learning},
  pages={399--409},
  year={2020},
  organization={PMLR}
}
@article{hand2018phase,
  title={Phase retrieval under a generative prior},
  author={Hand, Paul and Leong, Oscar and Voroninski, Vladislav},
  journal={arXiv preprint arXiv:1807.04261},
  year={2018}
}
@inproceedings{whang2021composing,
  title={Composing Normalizing Flows for Inverse Problems},
  author={Whang, Jay and Lindgren, Erik and Dimakis, Alex},
  booktitle={International Conference on Machine Learning},
  pages={11158--11169},
  year={2021},
  organization={PMLR}
}
@article{kelkar2021prior,
  title={Prior Image-Constrained Reconstruction using Style-Based Generative Models},
  author={Kelkar, Varun A and Anastasio, Mark A},
  journal={arXiv preprint arXiv:2102.12525},
  year={2021}
}
@article{jalal2021robust,
  title={Robust Compressed Sensing MRI with Deep Generative Priors},
  author={Jalal, Ajil and Arvinte, Marius and Daras, Giannis and Price, Eric and Dimakis, Alexandros G and Tamir, Jonathan I},
  journal={arXiv preprint arXiv:2108.01368},
  year={2021}
}
@inproceedings{lugmayr2020srflow,
  title={Srflow: Learning the super-resolution space with normalizing flow},
  author={Lugmayr, Andreas and Danelljan, Martin and Van Gool, Luc and Timofte, Radu},
  booktitle={European Conference on Computer Vision},
  pages={715--732},
  year={2020},
  organization={Springer}
}
@article{prakash2021removing,
  title={Removing Pixel Noises and Spatial Artifacts with Generative Diversity Denoising Methods},
  author={Prakash, Mangal and Delbracio, Mauricio and Milanfar, Peyman and Jug, Florian},
  journal={arXiv preprint arXiv:2104.01374},
  year={2021}
}
@article{ohayon2021high,
  title={High perceptual quality image denoising with a posterior sampling cgan},
  author={Ohayon, Guy and Adrai, Theo and Vaksman, Gregory and Elad, Michael and Milanfar, Peyman},
  journal={arXiv preprint arXiv:2103.04192},
  year={2021}
}
@article{jalal2021instance,
  title={Instance-Optimal Compressed Sensing via Posterior Sampling},
  author={Jalal, Ajil and Karmalkar, Sushrut and Dimakis, Alexandros G and Price, Eric},
  journal={arXiv preprint arXiv:2106.11438},
  year={2021}
}
@article{geman1984stochastic,
  title={Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images},
  author={Geman, Stuart and Geman, Donald},
  journal={IEEE Transactions on pattern analysis and machine intelligence},
  number={6},
  pages={721--741},
  year={1984},
  publisher={IEEE}
}
\end{filecontents*}


\usepackage{tabu} \usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{colortbl} 
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{latexsym,amstext,amsfonts,graphicx,setspace,bm}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{enumerate} 
\usepackage{paralist}
\usepackage{float}
\usepackage{subcaption}
\usepackage{array}
\usepackage{comment}
\usepackage{mathtools}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[thinc]{esdiff}


\usepackage{mathtools}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[thinc]{esdiff}
\usepackage{anyfontsize}
\usepackage{placeins}

\usepackage{enumitem}


\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}

\DeclarePairedDelimiterX{\infdivx}[2]{(}{)}{#1\;\delimsize\|\;#2}
\newcommand{\infdiv}{D\infdivx}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}

\usepackage{tikz}
\usetikzlibrary{bayesnet}
\usetikzlibrary{arrows, positioning}
\usetikzlibrary{fit,positioning}
\usetikzlibrary{calc}

\tikzset{>=latex}

\usepackage{pgfplots}

\usetikzlibrary{intersections, pgfplots.fillbetween}


\newcommand{\BlueComment}[1]{\color{blue}{#1}\color{black}}
\newcommand{\NewText}[1]{\color{red}{#1}\color{black}~}
\newcommand{\NewForRevision}[1]{\textbf{#1}~}

\newcommand{\CapOpt}[1]{#1}

\newcommand{\ToBeRemoved}[1]{\color{green}{#1}\color{black}~}
\newcommand{\CiteNeeded}{\color{blue}{Citation Needed}\color{black}}
\newcommand{\CiteLater}[1]{\color{blue}{#1}\color{black}}

\definecolor{nice_blue}{RGB}{65, 105, 225}
\definecolor{nice_red}{RGB}{168, 34, 34}
\definecolor{IGCBlue}{HTML}{16197A}
\definecolor{dark_green}{RGB}{20, 110, 10}

\definecolor{graph_blue}{RGB}{144, 195, 212}
\definecolor{graph_purple}{RGB}{195, 144, 212}
\definecolor{graph_green}{RGB}{161, 212, 144}
\definecolor{graph_orred}{RGB}{212, 161, 144}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\diag}{diag}
\DeclareMathOperator*{\tr}{tr}
\DeclareMathOperator*{\sign}{sign}
\newcommand*\dif{\mathop{}\!\mathrm{d}}
\DeclareMathOperator*{\E}{\mathbb{E}}
\newcommand\symeq{\stackrel{\mathclap{\tiny\mbox{sym}}}{=}}
\DeclareMathOperator*{\A}{\mathbb{A}}
\DeclareMathOperator*{\B}{\mathbb{B}}

\definecolor{dark_green}{RGB}{20, 110, 10}
\definecolor{dark_blue}{RGB}{20, 10, 120}




\definecolor{LightGray}{rgb}{0.85,0.85,0.85}



\newcommand{\centered}[1]{\begin{tabular}{l} #1 \end{tabular}}

\newcommand{\fld}{results}
\newcommand{\valid}{validation}

\newcommand{\fs}{00607} \newcommand{\fii}{00599} \newcommand{\fk}{00600} 

\newcommand{\xs}{00608} \newcommand{\xii}{00602} \newcommand{\xk}{00603} 

\newcommand{\ms}{00604} \newcommand{\mii}{00605} \newcommand{\mk}{00606} 

\newcommand{\imgnrF}{0038-step5000}
\newcommand{\imgnrX}{0001-step5000}
\newcommand{\imgnrB}{0002-step5000}



\newcommand{\nrTF}{0014} \newcommand{\nrTX}{0001}
\newcommand{\nrTB}{0003}




\newcommand{\inc}[1]{\raisebox{-.4\height}{\includegraphics[height=\w]{#1}}}

\newcommand{\w}{2.5cm}

\newcommand{\mc}[1]{\mathbb{#1}}

\newcommand{\srTFL}[1]{\fld/00607-recon-real-imagesffhq_test-super-resolution/image#1-target} \newcommand{\srTFO}[1]{\fld/00607-recon-real-imagesffhq_test-super-resolution/image#1-clean-step5000} \newcommand{\inTFM}[1]{\fld/00700-recon-real-imagesffhq_test-inpaint-eval/image#1-target} \newcommand{\inTFO}[1]{\fld/00700-recon-real-imagesffhq_test-inpaint-eval/inpaint/image#1} 


\newcommand{\srTXL}[1]{\fld/00608-recon-real-imagesxray_frontal_test-super-resolution/image#1-target} \newcommand{\srTXO}[1]{\fld/00608-recon-real-imagesxray_frontal_test-super-resolution/image#1-clean-step5000} \newcommand{\inTXM}[1]{\fld/00699-recon-real-imagesxray_frontal_test-inpaint-eval/image#1-target} \newcommand{\inTXO}[1]{\fld/00699-recon-real-imagesxray_frontal_test-inpaint-eval/inpaint/image#1} 

\newcommand{\srTBL}[1]{\fld/00626-recon-real-imagesbrains_test_mono-super-resolution/image#1-target} \newcommand{\srTBO}[1]{\fld/00626-recon-real-imagesbrains_test_mono-super-resolution/image#1-clean-step5000} \newcommand{\inTBO}[1]{\fld/00707-recon-real-imagesbrains_test_mono-inpaint-eval/inpaint/image#1} \newcommand{\inTBM}[1]{\fld/00707-recon-real-imagesbrains_test_mono-inpaint-eval/image#1-target} 



\newcommand\myfigure{\begin{center}
\setlength{\tabcolsep}{0pt}
\renewcommand{\arraystretch}{1.5}\begin{tabular}{>{\centering }m{1.7cm}cccccc}\arrayrulecolor{LightGray}
 & Input & Output & Input & Output & Input & Output \\

super-resolution & 
\includegraphics[height=\w,align=c]{\srTFL{\nrTF}}&\includegraphics[height=\w,align=c]{\srTFO{\nrTF}}&\includegraphics[height=\w,align=c]{\srTXL{\nrTX}}&\includegraphics[height=\w,align=c]{\srTXO{\nrTX}}&\includegraphics[height=\w,align=c]{\srTBL{\nrTB}}&\includegraphics[height=\w,align=c]{\srTBO{\nrTB}} \\ 

in-painting  & 
\includegraphics[height=\w,align=c]{\inTFM{\nrTF}}&\includegraphics[height=\w,align=c]{\inTFO{\nrTF}}&\includegraphics[height=\w,align=c]{\inTXM{\nrTX}}&\includegraphics[height=\w,align=c]{\inTXO{\nrTX}}&\includegraphics[height=\w,align=c]{\inTBM{\nrTB}}&\includegraphics[height=\w,align=c]{\inTBO{\nrTB}} \\ 

 
\end{tabular}
\end{center}
\captionof{figure}{Given a generator pre-trained on an image set (human faces, x-rays, brain MRIs), we perform various reconstructions such as super-resolution and in-painting. Our method does not require any additional \emph{corruption-specific} training, as it couples a dataset-specific generator , pre-trained on clean images, with a forward corruption process , such as downsampling or cropping. We demonstrate our results based on the state-of-the-art unconditional generator StyleGAN2 \cite{karras2020analyzing}.}
\label{fig:teaser}
}



\newcommand{\ci}[1]{\circ{#1}}

\newcommand{\modifcolor}{black}

\newcommand{\tc}[1]{\textcolor{\modifcolor}{#1}}
\newcommand{\tcc}[2]{\textcolor{#1}{#2}}


\newcommand{\pulse}{PULSE \cite{menon2020pulse}}
\newcommand{\esrgan}{ESRGAN \cite{wang2018esrgan}}
\newcommand{\srfbn}{SRFBN \cite{li2019feedback}}
\newcommand{\patchgan}{SN-PatchGAN \cite{yu2019free}} 
\newcommand{\imagestylegan}{Image2StyleGAN \cite{abdal2019image2stylegan}}
\newcommand{\imagestyleganplusplus}{Image2StyleGAN++ \cite{abdal2020image2stylegan}}
\newcommand{\brgm}{\textbf{BRGM}}
\newcommand{\dip}{Deep Image Prior \cite{ulyanov2018deep}} 

\newcommand{\wplus}{ }
\newcommand{\loss}{\mathcal{L}}


\begin{document}

\maketitle

\begin{abstract}
Machine learning models are commonly trained end-to-end and in a supervised setting, using paired (input, output) data. Examples include recent super-resolution methods that train on pairs of (low-resolution, high-resolution) images. However, these end-to-end approaches require re-training every time there is a distribution shift in the inputs (e.g., night images vs daylight) or relevant latent variables (e.g., camera blur or hand motion). In this work, we leverage state-of-the-art (SOTA) generative models (here StyleGAN2) for building powerful image priors, which enable application of Bayes' theorem for many downstream reconstruction tasks. Our method, \emph{Bayesian Reconstruction through Generative Models} (BRGM), uses a single pre-trained generator model to solve different image restoration tasks, i.e., super-resolution and in-painting, by combining it with different forward corruption models. We keep the weights of the generator model fixed, and reconstruct the image by estimating the Bayesian maximum a-posteriori (MAP) estimate over the input latent vector that generated the reconstructed image. We further use Variational Inference to approximate the posterior distribution over the latent vectors, from which we sample multiple solutions. We demonstrate BRGM on three large and diverse datasets: (i) 60,000 images from the Flick Faces High Quality dataset \cite{karras2019style} (ii) 240,000 chest X-rays from MIMIC III \cite{johnson2016mimic} and (iii) a combined collection of 5 brain MRI datasets with 7,329 scans \cite{dalca2018anatomical}. Across all three datasets and without any dataset-specific hyperparameter tuning, our simple approach yields performance competitive with current task-specific state-of-the-art methods on super-resolution and in-painting, while being more generalisable and without requiring any training. Our source code and pre-trained models are available online: \url{https://razvanmarinescu.github.io/brgm/}
\end{abstract}




\section{Introduction}

While end-to-end supervised learning is currently the most popular paradigm in the research community, it suffers from several problems. First, distribution shifts in the inputs often require re-training, as well as the effort of collecting an updated dataset. In some settings, such shifts can occur often (hospital scanners are upgraded) and even continuously (population is slowly aging due to improved healthcare).  Secondly, current state-of-the-art machine learning (ML) models often require prohibitive computational resources, which are only available in a select number of companies and research centers. Therefore, the ability to leverage \emph{pre-trained} models for solving downstream prediction or reconstruction tasks becomes crucial. 





\newcommand{\diagfld}{images/diagram}
\newcommand{\fstikz}[1]{\footnotesize{#1}}

\newcommand{\figscl}{0.8}

\begin{figure*}
 \centering
     \begin{subfigure}[t]{0.35\textwidth}
         \centering
\begin{tikzpicture}[scale=\figscl, every node/.style={scale=\figscl}]

\def\rightimgX{4.5}

\node (cxr_img) at (1, 3.5) {\includegraphics[width=2cm]{\diagfld/blur.png}};
\node (cxr_text) at (1,4.75) {\footnotesize{Low Res.}};

\node (blur_img) at (\rightimgX, 3.5) {\includegraphics[width=2cm]{\diagfld/original.png}};
\node (blur_text) at (\rightimgX,4.75) {\footnotesize{High Res.}};

\node (cxr_img) at (1, 1) {\includegraphics[width=2cm]{\diagfld/crop.png}};
\node (blur_img) at (\rightimgX, 1) {\includegraphics[width=2cm]{\diagfld/original.png}};

\node (left_img_top_out) at (2,3.5) {};
\node (right_img_top_in) at (\rightimgX - 1, 3.5) {};

\draw[->, thick]
(left_img_top_out)
to
(right_img_top_in) ;


\node (left_img_top_out_2) at (2,1) {};
\node (right_img_top_in_2) at (\rightimgX - 1, 1) {};

\draw[->, thick]
(left_img_top_out_2)
to
(right_img_top_in_2) ;


\node (left_img_top_in) at (2,2.5) {};
\node (right_img_top_out) at (\rightimgX - 1, 2.5) {};

\node (rng_text) at (2.75,3.75) {};
\node (rng_text) at (2.75,3.25) {\fstikz{Learned}};
\node (rng_text) at (2.75,3) {\fstikz{Inverse}};
\node (rng_text) at (2.75,2.75) {\fstikz{Corruption}};

\node (rng_text) at (2.75,1.25) {};

\end{tikzpicture}
         \caption{Task-specific methods}
         \label{diagram-prev}
     \end{subfigure}
     \begin{subfigure}[t]{0.6\textwidth}
         \centering
\begin{tikzpicture}[scale=\figscl, every node/.style={scale=\figscl}]

\def\rightimgX{4.5}

\filldraw[fill=gray!50!white, draw=black, label={Latent}] (-2,2.5) rectangle (-1.5,4.5);
\node (rng_text) at (-1.75,4.75) {\footnotesize{Latent}};
\node (rng_text) at (-1.75,3.5) {};

\node (cxr_img) at (1, 3.5) {\includegraphics[width=2cm]{\diagfld/original.png}};
\node (cxr_text) at (1,4.75) {\footnotesize{High Res.}};

\node (blur_img) at (\rightimgX, 3.5) {\includegraphics[width=2cm]{\diagfld/blur.png}};
\node (blur_text) at (\rightimgX,4.75) {\footnotesize{Low Res.}};

\node (blur_img) at (\rightimgX+2.3, 3.5) {\includegraphics[width=2cm]{\diagfld/blur.png}};
\node (blur_text) at (\rightimgX+2.3,4.75) {\footnotesize{Input}};

\draw[decoration={brace,mirror,raise=5pt},decorate]
  (4.5,2.5) -- node[below=6pt] {loss} (6.8,2.5);


\node (rng_top_out) at (-1.5,3.5) {};
\node (center_img_left_in) at (0, 3.5) {};

\draw[->, thick]
(rng_top_out)
to
(center_img_left_in) ;

\node (left_img_top_out) at (2,3.5) {};
\node (right_img_top_in) at (\rightimgX - 1, 3.5) {};

\draw[->, thick]
(left_img_top_out)
to
(right_img_top_in) ;

\node (left_img_top_in) at (2,2.5) {};
\node (right_img_top_out) at (\rightimgX - 1, 2.5) {};




\node (left_img_bot_out) at (0,2.5) {};
\node (rng_bot_out) at (-1.5, 2.5) {};
\draw[->, dashed, red, thick]
(rng_bot_out)
to [in=-120, out=-60]
(left_img_bot_out);

\node (rng_text) at (-0.75,3.75) {};
\node (rng_text) at (-0.75,3.25) {\fstikz{Image}};
\node (rng_text) at (-0.75,3) {\fstikz{Generation}};

\node (rng_text) at (2.75,3.75) {};
\node (rng_text) at (2.75,3.25) {\fstikz{Known}};
\node (rng_text) at (2.75,3) {\fstikz{Corruption}};

\node (rng_text) at (-0.75,1.75) {\fstikz{Learned a-priori}};
\node (rng_text) at (-0.75,1.5) {\fstikz{given High Res Data}};


\node (rng_text) at (-0.5,0.75) {\fstikz{Can be redone}};
\node (rng_text) at (-0.5,0.5) {\fstikz{using the same }};
\node (rng_text) at (-0.5,0.25) {\fstikz{for multiple }};






\node (left_img_center_out) at (1,2.5) {};
\node (right_bot_out_extra) at (3.9 - 1, 1) {};
\draw[->, thick]
(left_img_center_out)
to [in=90, out=-90]
(1,1) 
to [out=0, in=180]
(right_bot_out_extra);

\node (right_bot_out_extra_2) at (3.9 - 1, 0.5) {};
\draw[->, thick]
(left_img_center_out)
to [in=90, out=-90]
(1,0.5) 
to [out=0, in=180]
(right_bot_out_extra_2);


\node (rng_text) at (1.75,1.25) {};
\node (rng_text) at (1.75,0.75) {};
\node (rng_text) at (1.75,0.25) {};


\node (blur_img) at (3.5+0.5, 0.5) {\includegraphics[width=1cm]{\diagfld/compressive_sensing.png}};
\node (blur_img) at (3.5, 1.0) {\includegraphics[width=1cm]{\diagfld/crop.png}};

\end{tikzpicture}

         \caption{Generative approach}
         \label{diagram-ours}
\end{subfigure}
\caption{(a) Classical deep-learning methods for image reconstruction learn to invert specific corruption models such as downsampling with a specific kernel or in-painting with rectangular masks. (b) We use a generative approach that can handle arbitrary corruption processes, such as downsampling or in-painting with an arbitrary mask, by optimizing for it on-the-fly at inference time. Given a latent vector , we use generator  to generate clean images , followed by a corruption model  to generate a corrupted image . Given an input image , we find the latent  that \emph{generated the input image} using the Bayesian MAP estimate , and we use Variational Inference to sample from the posterior . This can be repeated for other corruption processes (, ) such as masking, motion, to-grayscale, as well as for other parametrisations of the process (e.g., super-resolution with different kernels or factors).}
\label{diagram}
\end{figure*}


Deep generative models have recently obtained state-of-the-art results in simulating high-quality images from a variety of computer vision datasets. Generative Adversarial Networks (GANs) such as StyleGAN2 \cite{karras2020analyzing} and StyleGAN-ADA \cite{karras2020training} have been demonstrated for unconditional image generation, while BigGAN has shown impressive performance in class-conditional image generation \cite{brock2018large}. Similarly, Variational Autoencoder-based methods such as VQ-VAE \cite{van2017neural} and -VAE \cite{higgins2016beta} have also been competitive in several image generation tasks. Other lines of research in deep generative models are auto-regressive models such as PixelCNN \cite{van2016conditional} and PixelRNN \cite{oord2016pixel}, as well as invertible flow models such as NeuralODE \cite{chen2018neural}, Glow \cite{kingma2018glow} and RealNVP \cite{dinh2016density}. While these models generate high-quality images that are similar to the training distribution, they are not directly applicable for solving more complex tasks such as image reconstruction. 

A particularly important application domain for generative models are inverse problems, which aim to reconstruct an image that has undergone a \emph{corruption process} such as blurring. Previous work has focused on regularizing the inversion process using smoothness \cite{tikhonov1963solution} or sparsity \cite{figueiredo2005bound,mairal2009online,aharon2006k} priors. However, such priors often result in blurry images, and do not enable hallucination of features, which is essential for such ill-posed problems. More recent deep-learning approaches \cite{wang2018esrgan,li2019feedback,yu2018generative,yu2019free} address this challenge using training data made of pairs of (low-resolution, high-resolution) images. However, one fundamental limitation is that they compute the pixelwise or perceptual loss in the high-resolution/in-painted space, which leads to the so-called \emph{averaging effect} \cite{ledig2017photo}: since multiple high-resolution images map to the same low-resolution image , the loss function minimizes the \emph{average} of all such solutions, resulting in a blurry image. Some methods \cite{ledig2017photo,pathak2016context} address this through adversarial losses, which force the model to output a solution that lies on the image manifold. However, even with adversarial losses, it is not clear which solution image is retrieved, and how to sample multiple solutions from the posterior distribution.

To overcome the averaging of all possible solutions to ill-posed problems, one can build methods that estimate by design \emph{all potential solutions}, or a distribution of solutions, for which a Bayesian framework is a natural choice. Bayesian solutions for image reconstruction problems include: Markov Random Field (MRF) priors for denoising and in-painting \cite{roth2005fields}, generative models of photosensor responses from surfaces and illuminants \cite{brainard1997bayesian}, MRF models that leverage global statistics for in-painting \cite{levin2003learning}, Bayesian quantification of the distribution of scene parameters and light direction for inference of shape, surface properties and motion \cite{freeman1994generic}, and sparse derivative priors for super-resolution and image demosaicing \cite{russell2003exploiting}. The seminal work of \cite{geman1984stochastic} formulated the image reconstruction task as the Bayesian optimization over an energy function in a lattice-like physical system. 


Starting with \cite{bora2017compressed}, several works proposed solving inverse problems using \emph{deep generative priors} \cite{pan2020exploiting,asim2020invertible,hand2018phase,kelkar2021prior}, which constrain the solution to belong to the learned image manifold of a deep generative model. While all these methods focus on deriving a single point estimate, some recent methods further derive a \emph{distribution} of potential solutions \cite{jalal2021robust,jalal2021instance} through Langevin dynamics. However, Langevin dynamics has slow mixing, and additionally, their loss function is \emph{only loosely} based on a Bayesian MAP estimate. A fully-Bayesian model allows optimizing  \emph{distributional losses} instead of single point-estimate losses, and allows the derivation of more complex uncertainty measures, such as -level confidence intervals. While \cite{lugmayr2020srflow,whang2021composing} solve the inverse problem in a Bayesian setting, they only demonstrate it for invertible flow models. It is thus still unclear how to use models other than flows, such as GANs or VAEs, to perform this reconstruction in a Bayesian framework, and how to use GANs to capture \emph{a distribution} of solutions for a given corrupted image.

In this work, we propose a Bayesian method to perform reconstruction through deep generative priors built using state-of-the-art GAN models. Given a pre-trained generator  (here StyleGAN2), a known corruption model , and a corrupted image  to be restored, we minimize at inference time the Bayesian MAP estimate , where  is latent vector given as input to . We further adapt previous Variational Inference (VI) methods \cite{kingma2013auto,blundell2015weight} to approximate the posterior  with a Gaussian distribution, thus enabling sampling of multiple solutions. Our key theoretical contributions are (i) the formulation of image reconstruction in a principled Bayesian framework using deep generative priors and (ii) the adaptation of previous deep-learning based VI methods to sample multiple reconstructions, while on the application side, we demonstrate our method on three datasets, including two challenging medical datasets, and show its competitive performance against four state-of-the-art methods.





\subsection{Related work}
 
\textbf{Deep Generative Prior (DGP) methods:} A related work to ours is \pulse, which uses pre-trained StyleGAN models for face super-resolution. Other DGP methods \cite{bora2017compressed,pan2020exploiting,asim2020invertible,hand2018phase,kelkar2021prior} 
 are close to our work, especially those that derive a distribution of potential solutions either through (i) direct sampling from conditionalized flow models \cite{lugmayr2020srflow}, (ii) Langevin dynamics \cite{jalal2021robust,jalal2021instance} or (iii) Variational Inference (VI) \cite{whang2021composing}, each having several advantages and disadvantages: (i) conditionalized flow models generate independent samples from the exact posterior, but use more restricted transformations due to the need to have computable Jacobian determinants, (ii) Langevin dynamics also sample the exact posterior, but generate highly correlated samples and can get stuck in distributional modes while (iii) VI methods can generate a large number of independent samples from an approximate posterior, but require fitting the variational parameters. Our sampling method that uses VI is closest to \cite{whang2021composing}, but we demonstrate it for GAN models such as StyleGAN2 instead of normalizing flows as in \cite{whang2021composing}.

\textbf{Encoder methods:} Other approaches attempt to invert generative models by estimating encoders that map the input images directly into the generator's latent space \cite{richardson2020encoding} in an end-to-end framework. Encoder methods are complimentary to our work, as they can be used to obtain a fast initial estimate of the latent , followed by a slightly longer optimisation process such as ours that can give more accurate reconstructions.


\textbf{Other inverse problems methods:} Approaches similar to ours have also been discussed in inverse problems research. Deep Bayesian Inversion \cite{adler2018deep} performs image reconstruction using a supervised learning approximation. AmbientGAN \cite{bora2018ambientgan} builds a GAN model of clean images given noisy observations only, for a specified corruption model. Deep Image Prior (DIP) \cite{ulyanov2018deep} has shown that the structure of deep convolutional networks captures texture-level statistics that can be used for zero-shot image reconstruction. MimicGAN \cite{anirudh2020mimicgan} has shown how to optimise the parameters of the unknown corruption model. Image2StyleGAN \cite{abdal2019image2stylegan} finds a projection of a given clean image in the latent space of StyleGAN, while the more updated Image2StyleGAN++ \cite{abdal2020image2stylegan} uses the estimated latent projections to demonstrate image manipulations, as well as in-painting. Recent neural radiance fields (NeRF) \cite{mildenhall2020nerf} achieved state-of-the-art results for synthesizing novel views. 







\section{Method}



An overview of our method is shown in Fig. \ref{diagram}. We assume a given generator  can model the distribution of clean images in a given dataset (e.g., human faces), then use a pre-defined forward model  that corrupts the clean image. Given a corrupted input image , we reconstruct it as , where  is the Bayesian MAP estimate over the latent vector  of . The graphical model is given in Supp. Fig. \ref{modeldiagram}.




Given an input corrupted image , we aim to reconstruct the clean image . In practice, there could be a distribution  of such clean images given a particular input image , which is estimated using Bayes' theorem as . The prior term  describes the manifold of clean images, restricting the possible reconstructions  to \emph{realistic} images. In our context, the likelihood term  describes the corruption process , which takes a clean image and produces a corrupted image. 




\newcommand{\ei}[1]{\fld/00653-recon-real-imagesffhq_test-super-resolution-orig/image#1-target} \newcommand{\eo}[1]{\fld/00653-recon-real-imagesffhq_test-super-resolution-orig/image#1-clean-step5000.jpg} \newcommand{\en}[1]{\fld/00654-recon-real-imagesffhq_test-super-resolution-nonoise/image#1-clean-step5000.jpg} \newcommand{\ew}[1]{\fld/00655-recon-real-imagesffhq_test-super-resolution-wplus/image#1-clean-step5000.jpg} \newcommand{\el}[1]{\fld/00656-recon-real-imagesffhq_test-super-resolution-l2/image#1-clean-step5000.jpg} \newcommand{\ep}[1]{\fld/00657-recon-real-imagesffhq_test-super-resolution-priorw/image#1-clean-step5000.jpg} \newcommand{\ec}[1]{\fld/00658-recon-real-imagesffhq_test-super-resolution/image#1-clean-step5000.jpg} \newcommand{\eh}[1]{\fld/00653-recon-real-imagesffhq_test-super-resolution-orig/image#1-true.jpg} 


\newcommand{\eii}[1]{\fld/00700-recon-real-imagesffhq_test-inpaint-eval/image#1-target} \newcommand{\eio}[1]{\fld/00705-recon-real-imagesffhq_test-inpaint-eval-orig/image#1-clean-step5000.jpg} \newcommand{\ein}[1]{\fld/00704-recon-real-imagesffhq_test-inpaint-eval-nonoise/image#1-clean-step5000.jpg} \newcommand{\eiw}[1]{\fld/00703-recon-real-imagesffhq_test-inpaint-eval-wplus/image#1-clean-step5000.jpg} \newcommand{\eil}[1]{\fld/00702-recon-real-imagesffhq_test-inpaint-eval-l2/image#1-clean-step5000.jpg} \newcommand{\eip}[1]{\fld/00701-recon-real-imagesffhq_test-inpaint-eval-priorw/image#1-clean-step5000.jpg} \newcommand{\eic}[1]{\fld/00700-recon-real-imagesffhq_test-inpaint-eval/image#1-clean-step5000.jpg} \newcommand{\eih}[1]{\fld/00700-recon-real-imagesffhq_test-inpaint-eval/image#1-true.jpg} 


\newcommand{\nrE}{0014}
\newcommand{\nrEtwo}{0005}
\newcommand{\nrEthree}{0012}
\newcommand{\nrEfour}{0014}

\newcommand{\nrEI}{0000}



\renewcommand{\w}{1.75cm}

\newcommand{\fnt}{\fontsize{7}{9}\selectfont}

\fboxsep=-0.3pt

\begin{figure*}
\centering
\setlength{\tabcolsep}{0pt}
\begin{tabu}{cccccccc}
\rowfont{\fnt} Input & StyleGAN2 inv. & + no noise & + \wplus optim. & + pixelwise  & + prior  & + colinear & True\\
 \inc{\ei{\nrE}} &  \inc{\eo{\nrE}} & \inc{\en{\nrE}} & \inc{\ew{\nrE}} & \inc{\el{\nrE}} & \inc{\ep{\nrE}} & \inc{\ec{\nrE}} & \inc{\eh{\nrE}}\\
\inc{\eii{\nrEI}} &  \inc{\eio{\nrEI}} & \inc{\ein{\nrEI}} & \inc{\eiw{\nrEI}} & \inc{\eil{\nrEI}} & \inc{\eip{\nrEI}} & \inc{\eic{\nrEI}} & \inc{\eih{\nrEI}}\\
\rowfont{\fnt} &  (a) & (b) & (c) & (d) & (e) & (f) & \\
\end{tabu}
\caption{Reconstructions as the loss function evolves from the original StyleGAN2 inversion to our proposed method. Top row shows super resolution, while bottom row shows in-painting. We start from (a) the original StyleGAN2 inversion, and (b) remove noise optimisation, (c) extend optimisation to full \wplus space, (d) add pixelwise  term, (e) add prior on  latent variables and (f) add colinear loss term for . }
\label{evolution}
\end{figure*}
 
\subsection{The image prior term}  
\label{prior}
The prior model  has been trained \emph{a-priori}, before the corruption task is known, hence satisfying the \emph{principle of independent mechanisms} from causal modelling \cite{peters2017elements}. In our experiments, , where  is the latent vector of StyleGAN2 (18 vectors for each resolution level),  is the deterministic function given by the StyleGAN2 synthesis network, and  is the output resolution of StyleGAN2, in our case 1024x1024 (FFHQ, X-Rays) or 256x256 (brains). Our framework is not specific to StyleGAN2: other generator models such as invertible flows \cite{dinh2016density,kingma2018glow} or VAEs \cite{kingma2013auto} can be used, as long as one can flow gradients through the model. 


We use the change of variables to express the probability density function over clean images:

While the traditional change of variables formula assumes that the function  is invertible, it can be extended to non-invertible\footnote{The supp. section of \cite{cvitkovic2019minimal} presents an excellent introduction to the generalized change of variable theorem.} mappings\cite{cvitkovic2019minimal,krantz2008geometric}. In addition, we assume that the Jacobian determinant  is constant for all , which is a reasonable assumption in StyleGAN2 due to its path length regularization (see Eq. 4 in \cite{karras2020analyzing}).

We now seek to instantiate . Since the latent space of StyleGAN2 consists of many vectors , where  (one for each layer), we need to set meaningful priors for them. While StyleGAN2 assumed that all vectors  are equal, we slightly relax that assumption but set two priors: (i) a cosine similarity prior similar to PULSE \cite{menon2020pulse} that ensures every pair  and  are roughly colinear, and (ii) another prior  that ensures the  vectors lie in the same region as the vectors used during training. We use the following distribution for :

where  is the angle between vectors  and , and  is the \emph{von Mises distribution} with mean zero and scale parameter  which ensures that vectors  are aligned. This distribution is analogous to a Gaussian distribution over angles in . We compute  and  as the mean and standard deviation of 10,000 latent variables passed through the mapping network, like the original StyleGAN2 inversion \cite{karras2020analyzing}.

\subsection{The image likelihood term}
\label{likelihood}

We instantiate the likelihood term  with a potentially probabilistic forward corruption process , parameterized\footnote{Since in our experiments  is fixed, we drop the notation of  in subsequent derivations.} by . We study two types of corruption processes  as follows:
\begin{itemize}
\item Super-resolution:  is defined as the forward operator that performs downsampling parameterized by a given kernel . For a high-resolution image , this produces a low-resolution (corrupted) image , where  denotes convolution and  denotes downsampling operator by a factor . The parameters are 
\item In-painting with arbitrary mask:  is implemented as an operator that performs pixelwise multiplication with a mask . For a given clean image  and a 2D binary mask , it produces a cropped-out (corrupted) image , where  is the Hadamard product. The parameters of this corruption process are  where , where  and  are the height and width of the image.
\end{itemize}


The likelihood model becomes:

 where  is the Jacobian matrix of  evaluated at , and is again assumed constant. For the noise model in , we consider two types of noise distributions: pixelwise independent Gaussian noise, as well as ``perceptual noise'', i.e. independent Gaussian noise in the perceptual VGG embedding space. This yields the following model\footnote{\tc{Model is equivalent to }}:

where  is the VGG network,  and  are diagonal covariance matrices,  and  are the resolutions of the corrupted images  as well as perceptual embeddings . Images , ,  and  are flattened to 1D vectors, while covariance matrices  and  are of dimensions  and .
 







\newcommand{\srFL}[1]{\fld/00607-recon-real-imagesffhq_test-super-resolution/image#1-target} \newcommand{\srFB}[1]{\fld/00607-recon-real-imagesffhq_test-super-resolution/bicubic-x4/image#1-target} \newcommand{\srFE}[1]{\valid/ESRGAN/results/ffhq_LLR/image#1-target_rlt} \newcommand{\srFS}[1]{\valid/SRFBN_CVPR19/results/SR/ffhq_LLR/SRFBN/x4/image#1-target} \newcommand{\srFP}[1]{\valid/pulse/runs/ffhq_LLR/image#1-target} \newcommand{\srFO}[1]{\fld/00607-recon-real-imagesffhq_test-super-resolution/clean_64/image#1-clean-step5000} \newcommand{\srFOF}[1]{\fld/00607-recon-real-imagesffhq_test-super-resolution/image#1-clean-step5000} \newcommand{\srFH}[1]{\fld/00607-recon-real-imagesffhq_test-super-resolution/image#1-true} 

\newcommand{\srFLtwo}[1]{\fld/00620-recon-real-imagesffhq_test-super-resolution/image#1-target} \newcommand{\srFBtwo}[1]{\fld/00620-recon-real-imagesffhq_test-super-resolution/bicubic-x4/image#1-target} \newcommand{\srFEtwo}[1]{\valid/ESRGAN/results/ffhq_LLR32x32/image#1-target_rlt} \newcommand{\srFStwo}[1]{\valid/SRFBN_CVPR19/results/SR/ffhq_LLR32x32/SRFBN/x4/image#1-target} \newcommand{\srFPtwo}[1]{\valid/pulse/runs/ffhq_LLR32x32/image#1-target} \newcommand{\srFOtwo}[1]{\fld/00620-recon-real-imagesffhq_test-super-resolution/clean_128/image#1-clean-step5000} \newcommand{\srFOFtwo}[1]{\fld/00620-recon-real-imagesffhq_test-super-resolution/image#1-clean-step5000} \newcommand{\srFHtwo}[1]{\fld/00620-recon-real-imagesffhq_test-super-resolution/image#1-true} 

\newcommand{\srFLthree}[1]{\fld/00624-recon-real-imagesffhq_test-super-resolution/image#1-target} \newcommand{\srFBthree}[1]{\fld/00624-recon-real-imagesffhq_test-super-resolution/bicubic-x4/image#1-target} \newcommand{\srFEthree}[1]{\valid/ESRGAN/results/ffhq_LLR64x64/image#1-target_rlt} \newcommand{\srFSthree}[1]{\valid/SRFBN_CVPR19/results/SR/ffhq_LLR64x64/SRFBN/x4/image#1-target} \newcommand{\srFPthree}[1]{\valid/pulse/runs/ffhq_LLR64x64/image#1-target} \newcommand{\srFOthree}[1]{\fld/00624-recon-real-imagesffhq_test-super-resolution/clean_256/image#1-clean-step5000} \newcommand{\srFOFthree}[1]{\fld/00624-recon-real-imagesffhq_test-super-resolution/image#1-clean-step5000} \newcommand{\srFHthree}[1]{\fld/00624-recon-real-imagesffhq_test-super-resolution/image#1-true} 

\newcommand{\srFLfour}[1]{\fld/00598-recon-real-imagesffhq_test-super-resolution/image#1-target} \newcommand{\srFBfour}[1]{\fld/00598-recon-real-imagesffhq_test-super-resolution/bicubic-x4/image#1-target} \newcommand{\srFEfour}[1]{\valid/ESRGAN/results/ffhq_LR/image#1-target_rlt} \newcommand{\srFSfour}[1]{\valid/SRFBN_CVPR19/results/SR/ffhq_LR/SRFBN/x4/image#1-target} \newcommand{\srFPfour}[1]{\valid/pulse/runs/ffhq_LR/512x512/image#1-target} \newcommand{\srFOfour}[1]{\fld/00598-recon-real-imagesffhq_test-super-resolution/clean_512/image#1-clean-step5000} \newcommand{\srFOFfour}[1]{\fld/00598-recon-real-imagesffhq_test-super-resolution/image#1-clean-step5000} \newcommand{\srFHfour}[1]{\fld/00598-recon-real-imagesffhq_test-super-resolution/image#1-true} 


\newcommand{\nrF}{0011} \newcommand{\nrFtwo}{0001}
\newcommand{\nrFthree}{0002}
\newcommand{\nrFfour}{0003}

\renewcommand{\w}{1.66cm}

\renewcommand{\fnt}{\fontsize{8}{10}\selectfont}


\begin{figure*}
\begin{center}
\setlength{\tabcolsep}{0pt}
\begin{tabu}{ccccccccc}\arrayrulecolor{LightGray}
\rowfont{\fnt} & Low-Res & Bicubic & \esrgan & \srfbn & \pulse  & \brgm      & \brgm      & True \\
\rowfont{\fnt} &    & (x4)    & (x4)   & (x4)  & 1024x1024 & (x4)      & 1024x1024 & 1024x1024 \\
\fnt 16x16 &
\inc{\srFL{\nrF}}&
\inc{\srFB{\nrF}}&
\inc{\srFE{\nrF}}&
\inc{\srFS{\nrF}}&
\inc{\srFP{\nrF}}&
\inc{\srFO{\nrF}}&
\inc{\srFOF{\nrF}}&
\inc{\srFH{\nrF}} \\

\fnt 32x32 &
\inc{\srFLtwo{\nrF}}&
\inc{\srFBtwo{\nrF}}&
\inc{\srFEtwo{\nrF}}&
\inc{\srFStwo{\nrF}}&
\inc{\srFPtwo{\nrF}}&
\inc{\srFOtwo{\nrF}}&
\inc{\srFOFtwo{\nrF}}&
\inc{\srFHtwo{\nrF}} \\

\fnt 64x64 &
\inc{\srFLthree{\nrF}}&
\inc{\srFBthree{\nrF}}&
\inc{\srFEthree{\nrF}}&
\inc{\srFSthree{\nrF}}&
\inc{\srFPthree{\nrF}}&
\inc{\srFOthree{\nrF}}&
\inc{\srFOFthree{\nrF}}&
\inc{\srFHthree{\nrF}} \\






\end{tabu}
\caption{Qualitative evaluation on FFHQ at different input resolutions. Left column shows low resolution inputs, while right column shows true high-quality images. ESRGAN and SRFBN show clear distortion and blurriness, while PULSE does not recover the true image due to strong priors. BRGM shows significant improvements, especially at low resolutions.}
\label{ffhq-super-resolution}
\end{center}
\end{figure*}
 
\subsection{Image restoration as Bayesian MAP estimate}
\label{bayesianmap}
The restoration of the optimal clean image  given a noisy input image  can be performed through the Bayesian maximum a-posteriori (MAP) estimate:




We now instantiate the prior  and the likelihood  with formulas from Eq. \ref{priordef} and Eq. \ref{likelihoodDef}, and recast the problem as an optimisation over : . This can be simplified to the following loss function (see Supplementary section \ref{supderiv} for full derivation):

which can be succinctly written as a weighted sum of four loss terms: , where  is the prior loss over ,  is the colinearity loss on ,  is the pixelwise loss on the corrupted images, and  is the perceptual loss, ,  and . Given the Bayesian MAP solution , the clean image is returned as 

\subsection{Sampling multiple reconstructions using Variational Inference}
\label{samplingmethod}

To sample multiple image reconstructions from the posterior distribution , we use Variational Inference. We use an approach similar to the Variational Auto-encoder \cite{kingma2013auto,rezende2014stochastic}, where for each data-point we estimate a Gaussian distribution of latent vectors, with the main difference that we do not use an encoder network, but instead optimise the mean and covariance directly. Thus, our approach is also similar to Bayes-by-Backprop (BBB) \cite{blundell2015weight}, but we estimate a Gaussian distribution over the \emph{latent vector}, instead of the network weights as in their case.  

Variational inference (Hinton and Van Camp 1993, Graves 2011) aims to find a parametric approximation , where  are parameters to be learned, to the true posterior  over the latent inputs  to the generator network. We seek to minimize:

Using the same approach as in Bayes-by-Backprop \cite{blundell2015weight}, we approximate the expected value over  using Monte Carlo samples  taken from :

We parameterize  as a Gaussian distribution, although in practice we can choose any parametric form for  (e.g. mixture of Gaussians) due to the Monte Carlo approximation. We sample the Gaussian by first sampling unit Gaussian noise , and then shifting it by the variational mean  and variational standard deviation . To ensure  is always positive, we re-parameterize it as . The variational posterior parameters are . The prior and likelihood models,  and  are as defined in Eq. \ref{priordef} and Eq. \ref{likelihoodDef}.

While the role of the entropy term  is to regularize the variance of  and ensure there is no mode-collapse, we found it useful to also add a prior over the variational parameter , to give the samples more variability. We therefore optimise the following:
 
where  is an inverse gamma distribution on the variational parameter , with concentration  and rate . This prior, although optional, encourages larger standard deviations, which ensure that as much of the posterior as possible is covered. Note that, even with this prior, we don't optimise  directly, rather we optimise . We compute the gradients using the same approach as in Bayes-by-Backprop \cite{blundell2015weight}.

To sample the posterior , we also tried Stochastic Gradient Langevin Dynamics (SGLD) \cite{welling2011bayesian} and Variational Adam \cite{khan2018fast}, which is equivalent to Variational Online Gauss-Newton (VOGN) \cite{khan2018fast} in our case when the batch size is 1. However, we could not get these methods to work in our setup: SGLD was adding noise of too-high magnitude and the optimisation quickly diverged, while Variational Adam produced little variability between the samples.



\renewcommand{\w}{1.87cm}

\newcommand{\srXL}[1]{\fld/00608-recon-real-imagesxray_frontal_test-super-resolution/image#1-target} \newcommand{\srXB}[1]{\fld/00608-recon-real-imagesxray_frontal_test-super-resolution/bicubic-x4/image#1-target} \newcommand{\srXE}[1]{\valid/ESRGAN/results/xray_LLR/image#1-target_rlt} \newcommand{\srXS}[1]{\valid/SRFBN_CVPR19/results/SR/xray_LLR/SRFBN/x4/image#1-target} \newcommand{\srXO}[1]{\fld/00608-recon-real-imagesxray_frontal_test-super-resolution/clean_64/image#1-clean-step5000} \newcommand{\srXOF}[1]{\fld/00608-recon-real-imagesxray_frontal_test-super-resolution/image#1-clean-step5000} \newcommand{\srXH}[1]{\fld/00608-recon-real-imagesxray_frontal_test-super-resolution/image#1-true} 

\newcommand{\srXLtwo}[1]{\fld/00622-recon-real-imagesxray_frontal_test-super-resolution/image#1-target} \newcommand{\srXBtwo}[1]{\fld/00622-recon-real-imagesxray_frontal_test-super-resolution/bicubic-x4/image#1-target} \newcommand{\srXEtwo}[1]{\valid/ESRGAN/results/xray_LLR32x32/image#1-target_rlt} \newcommand{\srXStwo}[1]{\valid/SRFBN_CVPR19/results/SR/xray_LLR32x32/SRFBN/x4/image#1-target} \newcommand{\srXOtwo}[1]{\fld/00622-recon-real-imagesxray_frontal_test-super-resolution/clean_128/image#1-clean-step5000} \newcommand{\srXOFtwo}[1]{\fld/00622-recon-real-imagesxray_frontal_test-super-resolution/image#1-clean-step5000} \newcommand{\srXHtwo}[1]{\fld/00622-recon-real-imagesxray_frontal_test-super-resolution/image#1-true} 

\newcommand{\srXLthree}[1]{\fld/00625-recon-real-imagesxray_frontal_test-super-resolution/image#1-target} \newcommand{\srXBthree}[1]{\fld/00625-recon-real-imagesxray_frontal_test-super-resolution/bicubic-x4/image#1-target} \newcommand{\srXEthree}[1]{\valid/ESRGAN/results/xray_LLR64x64/image#1-target_rlt} \newcommand{\srXSthree}[1]{\valid/SRFBN_CVPR19/results/SR/xray_LLR64x64/SRFBN/x4/image#1-target} \newcommand{\srXOthree}[1]{\fld/00625-recon-real-imagesxray_frontal_test-super-resolution/clean_256/image#1-clean-step5000} \newcommand{\srXOFthree}[1]{\fld/00625-recon-real-imagesxray_frontal_test-super-resolution/image#1-clean-step5000} \newcommand{\srXHthree}[1]{\fld/00625-recon-real-imagesxray_frontal_test-super-resolution/image#1-true} 

\newcommand{\srXLfour}[1]{\fld/00601-recon-real-imagesxray_frontal_test-super-resolution/image#1-target} \newcommand{\srXBfour}[1]{\fld/00601-recon-real-imagesxray_frontal_test-super-resolution/bicubic-x4/image#1-target} \newcommand{\srXEfour}[1]{\valid/ESRGAN/results/xray_LR/image#1-target_rlt} \newcommand{\srXSfour}[1]{\valid/SRFBN_CVPR19/results/SR/xray_LR/SRFBN/x4/image#1-target} \newcommand{\srXOfour}[1]{\fld/00601-recon-real-imagesxray_frontal_test-super-resolution/clean_512/image#1-clean-step5000} \newcommand{\srXOFfour}[1]{\fld/00601-recon-real-imagesxray_frontal_test-super-resolution/image#1-clean-step5000} \newcommand{\srXHfour}[1]{\fld/00601-recon-real-imagesxray_frontal_test-super-resolution/image#1-true} 


\newcommand{\srBL}[1]{\fld/00626-recon-real-imagesbrains_test_mono-super-resolution/image#1-target} \newcommand{\srBB}[1]{\fld/00626-recon-real-imagesbrains_test_mono-super-resolution/bicubic-x4/image#1-target} \newcommand{\srBE}[1]{\valid/ESRGAN/results/brains_LLR16x16/image#1-target_rlt} \newcommand{\srBS}[1]{\valid/SRFBN_CVPR19/results/SR/brains_LLR16x16/SRFBN/x4/image#1-target} \newcommand{\srBO}[1]{\fld/00626-recon-real-imagesbrains_test_mono-super-resolution/clean_64/image#1-clean-step5000} \newcommand{\srBOF}[1]{\fld/00626-recon-real-imagesbrains_test_mono-super-resolution/image#1-clean-step5000} \newcommand{\srBH}[1]{\fld/00626-recon-real-imagesbrains_test_mono-super-resolution/image#1-true} 

\newcommand{\srBLtwo}[1]{\fld/00604-recon-real-imagesbrains_test_mono-super-resolution/image#1-target} \newcommand{\srBBtwo}[1]{\fld/00604-recon-real-imagesbrains_test_mono-super-resolution/bicubic-x4/image#1-target} \newcommand{\srBEtwo}[1]{\valid/ESRGAN/results/brains_LR/image#1-target_rlt} \newcommand{\srBStwo}[1]{\valid/SRFBN_CVPR19/results/SR/brains_LR/SRFBN/x4/image#1-target} \newcommand{\srBOtwo}[1]{\fld/00604-recon-real-imagesbrains_test_mono-super-resolution/clean_128/image#1-clean-step5000} \newcommand{\srBOFtwo}[1]{\fld/00604-recon-real-imagesbrains_test_mono-super-resolution/image#1-clean-step5000} \newcommand{\srBHtwo}[1]{\fld/00604-recon-real-imagesbrains_test_mono-super-resolution/image#1-true} 




\newcommand{\nrX}{0000}
\newcommand{\nrXtwo}{0000}

\newcommand{\nrB}{0001}
\newcommand{\nrBtwo}{0001}

\renewcommand{\fnt}{\fontsize{8}{10}\selectfont}

\begin{figure*}
\begin{center}
\setlength{\tabcolsep}{0pt}
\renewcommand{\arraystretch}{0}\resizebox{0.8\columnwidth}{!}{\begin{tabu}{cccccccc}\arrayrulecolor{LightGray}
\rowfont{\fnt} & Low-Res. & Bicubic & \esrgan & \srfbn & \brgm & \brgm & True \\
\rowfont{\fnt} &    &  (x4)   &  (x4)  &  (x4) & (x4) & (full-res.) & \\   

\fnt 16x16 &
\inc{\srXL{\nrX}}&
\inc{\srXB{\nrX}}&
\inc{\srXE{\nrX}}&
\inc{\srXS{\nrX}}&
\inc{\srXO{\nrX}}&
\inc{\srXOF{\nrX}}&
\inc{\srXH{\nrX}} \\

\fnt 32x32 &
\inc{\srXLtwo{\nrX}}&
\inc{\srXBtwo{\nrX}}&
\inc{\srXEtwo{\nrX}}&
\inc{\srXStwo{\nrX}}&
\inc{\srXOtwo{\nrX}}&
\inc{\srXOFtwo{\nrX}}&
\inc{\srXHtwo{\nrX}} \\





\fnt 16x16 &
\inc{\srBL{\nrB}}&
\inc{\srBB{\nrB}}&
\inc{\srBE{\nrB}}&
\inc{\srBS{\nrB}}&
\inc{\srBO{\nrB}}&
\inc{\srBOF{\nrB}}&
\inc{\srBH{\nrB}} \\

\fnt 32x32 &
\inc{\srBLtwo{\nrBtwo}}&
\inc{\srBBtwo{\nrBtwo}}&
\inc{\srBEtwo{\nrBtwo}}&
\inc{\srBStwo{\nrBtwo}}&
\inc{\srBOtwo{\nrBtwo}}&
\inc{\srBOFtwo{\nrBtwo}}&
\inc{\srBHtwo{\nrBtwo}} \\

\end{tabu}
}
\caption{Qualitative evaluation on medical datasets at different resolutions. The left column shows input images, while the right column shows the true high-quality images. BRGM shows improved quality of reconstructions across all resolution levels and datasets. We used the exact same setup as in FFHQ in Fig. \ref{ffhq-super-resolution}, without any dataset-specific parameter tuning.}
\label{medical-super-resolution}
\end{center}
\end{figure*}
 
\subsection{Model Optimisation}
\label{modeloptim}


We optimise the loss in Eq. \ref{fullloss} using Adam \cite{kingma2014adam} with learning rate of 0.001, while fixing ,  and ,  and  a-priori. On our datasets, we found the following values to give good results: , , ,  and . In Fig \ref{evolution}, we show image super-resolution and in-painting starting from the original StyleGAN2 inversion, and gradually modify the loss function and optimisation until we arrive at our proposed solution. The original StyleGAN2 inversion results in line artifacts for super-resolution, while for in-painting it cannot reconstruct well. After removing the optimisation of noise layers from the original StyleGAN2 inversion\cite{karras2020analyzing} and switching to the extended latent space \wplus, where each resolution-specific vector , ... ,  is independent, the image quality improves for super-resolution, while for in-painting the existing image is recovered well, but the reconstructed part gets even worse. More improvements are observed by adding the pixelwise  loss, mostly because the perceptual loss only operates at 256x256 resolution. Adding the prior on  and the cosine loss produces smoother reconstructions with less artifacts, especially for in-painting. 


 
\subsection{Model training and evaluation}
\label{trainingmain}

We train our model on data from three datasets: (i) 70,000 images from FFHQ \cite{karras2019style} at  resolution, 240,000 frontal-view chest X-ray image from MIMIC III \cite{johnson2016mimic} at  resolution, as well as 7,329 middle coronal 2D slices from a collection of 5 brain datasets: ADNI \cite{jack2008alzheimer}, OASIS \cite{marcus2010open}, PPMI \cite{marek2018parkinson}, AIBL \cite{ellis2009australian} and ABIDE \cite{heinsfeld2018identification}. We obtained ethical approval for all data used. All brain images were pre-registered rigidly. For all experiments, we trained the generator, in our case StyleGAN2, on 90\% of the data, and left the remaining 10\% for testing. We did not use the pre-trained StyleGAN2 on FFHQ as it was trained on the full FFHQ. Training was performed on 4 Titan-Xp GPUs using StyleGAN2 config-e, and was performed for 20,000,000 images shown to the discriminator (20,000 kimg), which took almost 2 weeks on our hardware. For a description of the generator training on all three datasets, see Supp. Section \ref{training}. For a description of the inference times of BRGM MAP and VI, see Supp. Section \ref{times}.

We compare our method with the state-of-the-art methods in super-resolution (\esrgan\ and \srfbn) as well as in-painting (\patchgan). We additionally compared with \pulse\ due to its closeness to our method, as well as the use of the state-of-the-art StyleGAN model. For these methods, we downloaded the pre-trained models. We could not compare with NeRF \cite{mildenhall2020nerf} as it requires multiple views of the same object. Since DIP \cite{ulyanov2018deep} uses statistics in the input image only and cannot handle large masks or large super-resolution factors, we did not include it in the performance evaluation, although we show results with DIP in the supplementary material. For \pulse, we only tested it on FFHQ, as for the medical datasets it required re-training of the StyleGAN2 generator in their own PyTorch implementation, that was different from the official StyleGAN2 implementation. We release our code with the CC-BY license.

\section{Results}




We applied BRGM and the other models on super-resolution at different resolution levels (Fig. \ref{ffhq-super-resolution} and Fig. \ref{medical-super-resolution}). On all three datasets, our method performs considerably better than other models, in particular at lower input resolutions: ESRGAN yields jittery artifacts, SRFBN gives smoothed-out results, while PULSE generates very high-resolution images that don't match the true image, likely due to the hard projection of their optimized latent to , the unit sphere in -dimensions, as opposed to a soft prior term such as  in our case. Moreover, as opposed to ESRGAN and SRFBN, both our model as well as PULSE can perform more than x4 super-resolution, going up to 1024x1024. Without changing any hyper-parameters, we observe similar trends on the other two medical datasets. 




\newcommand{\inFH}[1]{\fld/00636-recon-real-imagesffhq_test-inpaint-arbitrary/image#1-true} \newcommand{\inFM}[1]{masks/image#1-merged} \newcommand{\inFP}[1]{\valid/inpaint-sn-patchgan/output/ffhq/image#1} \newcommand{\inFD}[1]{validation/dip/output/ffhq_arbitrary/image#1_6000} \newcommand{\inFO}[1]{\fld/00636-recon-real-imagesffhq_test-inpaint-arbitrary/inpaint/image#1} 

\newcommand{\inFEH}[1]{\fld/00700-recon-real-imagesffhq_test-inpaint-eval/image#1-true} \newcommand{\inFEM}[1]{\fld/00700-recon-real-imagesffhq_test-inpaint-eval/image#1-target} \newcommand{\inFEP}[1]{\valid/inpaint-sn-patchgan/output/ffhq_eval/image#1} \newcommand{\inFED}[1]{validation/dip/output/ffhq/image#1_6000} \newcommand{\inFEO}[1]{\fld/00700-recon-real-imagesffhq_test-inpaint-eval/inpaint/image#1} 



\newcommand{\inXH}[1]{\fld/00699-recon-real-imagesxray_frontal_test-inpaint-eval/image#1-true} \newcommand{\inXM}[1]{\fld/00699-recon-real-imagesxray_frontal_test-inpaint-eval/image#1-target} \newcommand{\inXP}[1]{\valid/inpaint-sn-patchgan/output/xray_eval/image#1} \newcommand{\inXD}[1]{validation/dip/output/xray/image#1_6000} \newcommand{\inXO}[1]{\fld/00699-recon-real-imagesxray_frontal_test-inpaint-eval/inpaint/image#1} 


\newcommand{\inBH}[1]{\fld/00707-recon-real-imagesbrains_test_mono-inpaint-eval/image#1-true} \newcommand{\inBM}[1]{\fld/00707-recon-real-imagesbrains_test_mono-inpaint-eval/image#1-target} \newcommand{\inBP}[1]{\valid/inpaint-sn-patchgan/output/brains_eval/image#1} \newcommand{\inBD}[1]{validation/dip/output/brains/image#1_6000} \newcommand{\inBO}[1]{\fld/00707-recon-real-imagesbrains_test_mono-inpaint-eval/inpaint/image#1} 


\newcommand{\nrI}{0002} \newcommand{\nrItwo}{0014} \newcommand{\nrIthree}{0000} \newcommand{\nrIfour}{0001} \newcommand{\nrIfive}{0008} \newcommand{\nrIsix}{0009} 

\newcommand{\nrIM}{0000} \newcommand{\nrIMtwo}{0001} \newcommand{\nrIMthree}{0003} \newcommand{\nrIMfour}{0004} 

\newcommand{\nrIMB}{0001} \newcommand{\nrIMBtwo}{0002} \newcommand{\nrIMBthree}{0003} \newcommand{\nrIMBfour}{0004} \newcommand{\nrIMBfive}{0003} 

\renewcommand{\w}{2.05cm}

\begin{figure}
\begin{center}
\setlength{\tabcolsep}{0pt}
\renewcommand{\arraystretch}{0.5}\resizebox{\columnwidth}{!}{\begin{tabu}{cccc||cccc}\arrayrulecolor{white}
\rowfont{\footnotesize} Original & Masked & SN-PatchGAN & \brgm & Original & Masked & SN-PatchGAN & \brgm\\
\rowfont{\footnotesize} & & \cite{yu2019free} & & & & \cite{yu2019free} & \\

\inc{\inFH{\nrI}}&
\inc{\inFM{\nrI}}&
\inc{\inFP{\nrI}}&
\inc{\inFO{\nrI}}& 
\inc{\inFEH{\nrItwo}}&
\inc{\inFEM{\nrItwo}}&
\inc{\inFEP{\nrItwo}}&
\inc{\inFEO{\nrItwo}}\\




\end{tabu}
}
\end{center}
\caption{Comparison between our method and \patchgan\ on in-painting. SN-PatchGAN fails on large masks, while our method can still recover the high-level structure.}
\label{inpainting}
\end{figure}


Fig. \ref{inpainting} illustrates our method's performance on in-painting with arbitrary as well as rectangular masks, as compared to to the leading in-painting model \patchgan. Our method produces considerably better results than SN-PatchGAN. In particular, SN-PatchGAN lacks high-level semantics in the reconstruction, and cannot handle large masks. For example, in the first figure, when the mother is cropped out, SN-PatchGAN is unable to reconstruct the ear. Our method on the other hand is able to reconstruct the ear and the jawline. One reason for the lower performance of SN-PatchGAN could be that it was trained on CelebA, which has lower variation than FFHQ. In Supplementary Figs. \ref{ffhq-inpainting}, \ref{xray-inpainting} and \ref{brain-inpainting}, we show further in-painting examples with our method as well as \patchgan, on all three datasets, and for different types of arbitrary masks.



\newcommand{\flds}{results/samFFHQ}

\newcommand{\stepth}{step1100}





\newcommand{\is}{15489}
\newcommand{\step}{step460}

\newcommand{\fldst}{results/samFFHQinp}
\newcommand{\ist}{22706}
\newcommand{\stept}{step290}



\begin{figure*}
\renewcommand{\w}{1.98cm}
\centering
\setlength{\tabcolsep}{0pt}
\renewcommand{\arraystretch}{1.5}\begin{tabular}{cc||ccccc}\arrayrulecolor{white}
Input & True & Est. Mean & Sample 1 & Sample 2 & Sample 3 & Sample 4\\
\inc{\flds/\is_target.jpg} & \inc{\flds/\is_true.jpg} & \inc{\flds/\is_clean_\step.jpg} & \inc{\flds/\is_sample1_\step.jpg} & \inc{\flds/\is_sample2_\step.jpg} & \inc{\flds/\is_sample3_\step.jpg} & \inc{\flds/\is_sample4_\step.jpg}\\
 





\inc{\fldst/\ist_target.jpg} & \inc{\fldst/\ist_true.jpg} & \inc{\fldst/\ist_merged_\stept.jpg} & \inc{\fldst/\ist_mergedsample1_\stept.jpg} & \inc{\fldst/\ist_mergedsample2_\stept.jpg} & \inc{\fldst/\ist_mergedsample3_\stept.jpg} & \inc{\fldst/\ist_mergedsample4_\stept.jpg}\\

 \end{tabular}
 \caption{\tc{Sampling using Variational Inference. Given the input image (left column), we show the estimated mean image  (third column), alongside samples around the mean .}}
 \label{figsampling}
\end{figure*}





In Fig. \ref{figsampling}, we show samples from the variational posterior , for both super-resolution and in-painting. For super-resolution, we show an extreme downsampling example (x256) going from 1024x1024 to 4x4, in order to clearly see the potential variability in the reconstructions. The Variational Inference method gives samples of reasonably high variability and fidelity, although in harder cases (Supp. Fig. \ref{samplingsupp} and \ref{samplinginpsupp}) it overfits the posterior.

In supplementary section \ref{ablation}, we show in an ablation study over the hyper-parameters ,  and  that our method is not sensitive to the choice of these parameters, as there are multiple levels of magnitudes giving good results. In addition, in supplementary section \ref{downstream}, we show that BRGM-reconstructed images can be used for downstream processing, through an example of edema severity prediction on the Chest X-Ray images.


\subsection{Quantitative evaluation}

Table \ref{eval} (left) reports performance metrics of super-resolution on 100 unseen images at different resolution levels. At low  input resolutions, our method outperforms all other super-resolution methods consistently on all three datasets. However, at resolutions of  and higher, \srfbn\ achieves the lowest LPIPS\cite{zhang2018unreasonable} and root mean squared error (RMSE), albeit the qualitative results from this method showed that the reconstructions are overly smooth and lack detail. The performance degradation of our model is likely because the StyleGAN2 generator  cannot easily generate these unseen images at high resolutions, although this is expected to change in the near future given the fast-paced improvements in such generator models. However, compared to those methods, our method is more generalisable as it is not specific to a particular type of corruption, and can increase the resolution by a factor higher than 4x. In Supplementary Table \ref{eval-super-resolution-supplementary}, we additionally provide PSNR, SSIM and MAE scores, which show a similar behavior to LPIPS and RMSE.

For quantitative evaluation on in-painting, we generated 7 masks similar to the setup of \cite{abdal2020image2stylegan}, and applied them in cyclical order to 100 unseen images from the test sets of each dataset. In Table \ref{eval} (top-right), we show that our method consistently outperforms \patchgan\ with respect to all performance measures. 

To account for human perceptual quality, we performed a forced-choice pairwise comparison test, which has been shown to be most sensitive and simple for users to perform \cite{mantiuk2012comparison}. Twenty raters were each shown 100 test pairs of the true image and the four reconstructed images by each algorithm, and raters were asked to choose the best reconstruction (see supplementary section \ref{supp-eval-humans} for more information on the design). We opted for this paired test instead of the mean opinion score (MOS) because it also accounts for fidelity of the reconstruction to the true image. This is important in our setup, because a method such as PULSE can reconstruct high-resolution faces that are nonetheless of a different person (see Fig. \ref{ffhq-super-resolution}). In Table \ref{eval} (bottom-right), the results confirm that out method is the best at low  resolution and second-best at  resolution, with lower performance at  resolution. 



\newcommand{\tb}[1]{\textbf{#1}}
\newcommand{\lpi}{\multicolumn{1}{c}{\small LPIPS}}
\newcommand{\rms}{\multicolumn{1}{c}{\small RMSE}}



\begin{table}[!t]
\setlength{\tabcolsep}{2.0pt}
\begin{subtable}[t]{.5\linewidth}
\centering
\fnt Super-resolution (LPIPS / RMSE)\\
\resizebox{\columnwidth}{!}{\renewcommand{\arraystretch}{1.37}
\begin{tabular}{l|cccc}
\toprule
Dataset &       \brgm  &    \pulse    &   \esrgan    &   \srfbn  \\


\midrule
   FFHQ  &  \tb{0.24}/25.66 &  0.29/27.14 &  0.35/29.32 &  0.33/\tb{22.07} \\
   FFHQ  &  0.30/18.93 &  0.48/42.97 &  0.29/23.02 &  \tb{0.23}/\tb{12.73} \\
   FFHQ  &  0.36/16.07 &  0.53/41.31 &  0.26/18.37 &   \tb{0.23}/\tb{9.40} \\
X-ray  &  \tb{0.18}/\tb{11.61} &           - &  0.32/14.67 &  0.37/12.28 \\
   X-ray  &  0.23/10.47 &           - &  0.32/12.56 &   \tb{0.21}/\tb{6.84} \\
   X-ray  &  0.31/10.58 &           - &   0.30/8.67 &   \tb{0.22}/\tb{5.32} \\
Brains  &  \tb{0.12}/\tb{12.42} &           - &  0.34/22.81 &  0.33/12.57 \\
 Brains  &  \tb{0.17}/11.08 &           - &  0.31/14.16 &   0.18/\tb{6.80} \\


\bottomrule
\end{tabular}
}
\end{subtable}
\begin{subtable}[t]{.5\linewidth}
\centering
\fnt In-painting\\
\resizebox{\columnwidth}{!}{\begin{tabular}{l|cccc|cccc}
\toprule
 &  \multicolumn{4}{c|}{\brgm}    & \multicolumn{4}{c}{\patchgan} \\
Dataset & LPIPS & RMSE & PSNR & SSIM & LPIPS & RMSE & PSNR & SSIM\\
\midrule
   FFHQ &  \tb{0.19} & \tb{24.28} & \tb{21.33} & \tb{0.84} &  0.24 & 30.75 & 19.67 & 0.82 \\
  X-ray &  \tb{0.13} & \tb{13.55} & \tb{27.47} & \tb{0.91} &  0.20 & 27.80 & 22.02 & 0.86 \\
 Brains & \tb{0.09} & \tb{8.65} & \tb{30.94} & \tb{0.88} &  0.22 & 24.74 & 21.47 & 0.75 \\
\bottomrule
\end{tabular}}
\vspace{0.5em}

\fnt Human evaluation (proportion of votes for best image)\\
\resizebox{\columnwidth}{!}{\begin{tabular}{l|cccc}
\toprule
 Dataset &       \brgm  &    \pulse    &   \esrgan    &   \srfbn  \\

\midrule
  FFHQ  &  \tb{42\%} &   32\% &    11\% &   15\% \\
  FFHQ  &  39\% &   2\% &    12\% &   \tb{47\%} \\
  FFHQ  &  14\% &   8\% &    32\% &   \tb{45\%} \\
\bottomrule
\end{tabular}
}
\end{subtable}
\caption{(left) Evaluation on (x4) super-resolution at different input resolution levels. Reported are LPIPS/RMSE scores. (top-right) Evaluation of BRGM and SN-PatchGAN on in-painting. (bottom-right) Human evaluation showing the proportion of votes for the best super-resolution re-construction in the forced-choice pairwise comparison test. Bold numbers show best performance.}
\label{eval}
\end{table}


\subsection{Method limitations and potential negative societal impact}
\label{limitations}

In Supp. Fig. \ref{failure}, we show failure cases on the super-resolution task. The reason for the failures is likely due to the limited generalisation abilities of the StyleGAN2 generator to such unseen images. We particularly note that, as opposed to the simple inversion of Image2StyleGAN \cite{abdal2019image2stylegan}, which relies on latent variables at high resolution to recover the fine details, we cannot optimize these high-resolution latent variables, thus having to rely on the proper ability of StyleGAN2 to extrapolate from lower-level latent variables.  Another limitation of our method is the inconsistency between the downsampled input image and the given input image, which we exemplify in Supp. Figs \ref{imgdiff-ffhq} and \ref{imgdiff-medical}. We attribute this again to the limited generalisation of the generator to these unseen images. The same inconsistency also applies to in-painting, as shown in Fig. \ref{evolution}.

By leveraging models pre-trained on FFHQ, our methodology can be potentially biased towards images from people that are over-represented in the dataset. On the medical datasets, we also have biases in disease labels. For example, the MIMIC dataset contains both healthy and pneumonia lung images, but many other lung conditions are not covered, while for the brain dataset it contains healthy brains as well as Alzheimer's and Parkinson's, but does not cover rarer brain diseases. Before deployment in the real-world, further work is required to make the method robust to diverse inputs, in order to avoid negative impact on the users. 

\section{Conclusion}

We proposed a simple Bayesian framework for performing different reconstruction tasks using deep generative models such as StyleGAN2. We estimate the optimal reconstruction as the Bayesian MAP estimate, and use Variational Inference to sample from an approximate posterior of all possible solutions. We demonstrated our method on two reconstruction tasks, and on three distinct datasets, including two challenging medical datasets, obtaining competitive results in comparison with state-of-the-art models. Future work can focus on jointly optimizing the parameters of the corruption models, as well as extending to more complex corruption models.





{
\small

\bibliographystyle{unsrtnat}
\bibliography{bibliography}


}








\pagebreak

\appendix

\begin{center}
\Large{Supplementary Material} 
\end{center}



\section{Derivation of loss function for the Bayesian MAP estimate}
\label{supderiv}


We assume  is the StyleGAN2 latent vector,  is the corrupted input image,  is the StyleGAN2 generator network function,  is the corruption function, and  is a function describing the perceptual network. ,  and  are the resolutions of the clean image , corrupted image  and of the perceptual embedding . The full Bayesian posterior  of our model is proportional to:



where ,  are means and standard deviations of the prior on ,  is the von Mises distribution\footnote{The von-Mises distribution is the analogous of the Gaussian distribution over angles .  is analogous to , where } with mean zero and scale parameter , and  and  are identity matrices scaled by variance terms. 

The Bayesian MAP estimate is the vector  that maximizes Eq. \ref{fulldist}, and provides the most likely vector  that could have generated input image :




Since logarithm is a strictly increasing function that won't change the output of the  operator, we take the logarithm to simplify Eq. \ref{bmap} to:



We expand the probability density functions of each distribution to get:




where , ,  and  are constants with respect to , so we can ignore them. 

We remove the constants, multiply by (-2), which requires switching to the  operator, to get:



This is equivalent to Eq. \ref{fullloss}, which finishes our proof. 


\begin{figure}
 \centering
 \begin{tikzpicture}[scale=0.9, every node/.style={scale=0.9}]
\node[latent, label={}]                               (Ilr) {}; \node[obs, right=of Ilr, xshift=0.2cm]                               (I) {}; \node[latent, left=of Ilr, xshift=-0.1cm, label={}] (Ihr) {}; \node[latent, left=of Ihr, xshift=-0.2cm] (w) {}; 

\node[obs, above=of w, xshift=-1cm] (mu) {}; 
\node[obs, above=of w, xshift=0cm] (sigma) {}; 
\node[obs, above=of w, xshift=1cm] (kappa) {};  

\node[latent, left=of w, xshift=-0.7cm] (muv) {}; \node[latent, above=of muv, yshift=-0.7cm] (sigmav) {}; \node[latent, left=of sigmav, xshift=-0.0cm] (rhov) {}; 


\node[obs, above=of I, xshift=-0.8cm] (sigmapix) {\small{}}; 
\node[obs, above=of I, xshift=0.8cm] (sigmaperc) {\small{}};
\path (w) edge [->, >={triangle 45}]   (Ihr)  ;\path (w) -- node[above] {G}  (Ihr)  ;

\edge {Ihr} {Ilr} ; \path (Ihr) -- node[above] {f}  (Ilr)  ;\edge {Ilr} {I} ; 

\edge {muv} {w};
\edge {sigmav} {w};
\edge {rhov} {sigmav};



\edge {mu} {w} ; \edge {sigma} {w} ; \edge {kappa} {w} ; 

\edge {sigmapix} {I} ; \edge {sigmaperc} {I} ; 



\draw[red,thick,line width=0.15mm] (-9.5,-0.6)  rectangle (-6,1.7);
\node[above] at (-7.7,1.70) {\textcolor{red}{variational parameters}};

 \end{tikzpicture}
\caption{\tc{Graphical model of our method. In gray shade are known observations or parameters: the input corrupted image , the parameters ,  and  defining the prior on latent vector , and , , the parameters defining the noise model over . In the red box are the variational parameters ,  and  defining an approximated Gaussian posterior over  (section \ref{samplingmethod}). Unknown latent variables (in white), to be estimated, are , the latent vectors of StyleGAN, , the clean image, and , the corrupted image simulated through the pipeline. Transformation  is modelled by the StyleGAN2 generator, while   by a known corruption model (e.g. downsampling with a known kernel).} }
\label{modeldiagram}
\end{figure}




\section{Training StyleGAN2}
\label{training}

In Fig. \ref{generated}, we show uncurated images generated by the cross-validated StyleGAN2 trained on our medical datasets, along with a few real examples. For the high-resolution X-rays, we notice that the image quality is very good, although some artifacts are still present: some text tags are not properly generated, some bones and rib contours are wiggly, and the shoulder bones show less contrast. For the brain dataset, we do not notice any clear artifacts, although we did not assess distributional preservation of regional volumes as in \cite{tudosiu2020neuromorphologicaly}. For the cross-validated FFHQ model, we obtained an FID of 4.01, around 0.7 points higher than the best result of 3.31 reported for config-e \cite{karras2020analyzing}.


\FloatBarrier





\newcommand{\genF}{\fld/00610-generate-images-ffhq/}
\newcommand{\genX}{\fld/00618-generate-images-xray/}
\newcommand{\genB}{\fld/00619-generate-images-brains/}

\newcommand{\realX}{images}


\newcommand{\realB}{images}


\renewcommand{\w}{2.7cm}

\begin{figure*}
\centering
\setlength{\tabcolsep}{0pt}
\renewcommand{\arraystretch}{1.5}\begin{tabular}{c||cccc}\arrayrulecolor{white}
Real & \multicolumn{4}{c}{Generated (FID: 9.2)}\\
\inc{\realX/00068d26-8d583659-af7de1da-fc6c0476-d94aada1}&\inc{\genX/seed0001}&\inc{\genX/seed0002}&\inc{\genX/seed0003}&\inc{\genX/seed0004}\\
\inc{\realX/0007f9a9-e098be9b-81942dab-995b3d31-4b19f648}&\inc{\genX/seed0005}&\inc{\genX/seed0006}&\inc{\genX/seed0007}&\inc{\genX/seed0008}\\

Real & \multicolumn{4}{c}{Generated (FID: 7.3)} \\

\inc{\realB/COBRE_0040073_mri_talairach_norm}&\inc{\genB/seed0001}&\inc{\genB/seed0002}&\inc{\genB/seed0003}&\inc{\genB/seed0004} \\
\inc{\realB/OASIS_OAS1_0043_MR1_mri_talairach_norm}&\inc{\genB/seed0005}&\inc{\genB/seed0006}&\inc{\genB/seed0007}&\inc{\genB/seed0008} \\

\end{tabular}
\caption{Uncurated images generated by our StyleGAN2 generator trained on the chest X-ray dataset (MIMIC III) (top) and the brain dataset (bottom). Left images are random examples of real images from the actual datasets, while the right-side images are generated. The image quality is relatively good, albeit some anatomical artifacts are still observed, such as incomplete labels, wiggly bones or discontinuous wires.} \label{generated}
\end{figure*}


\section{Inference times of our method}
\label{times}

The inference time of our method is as follows: for MAP inference, it takes between 32-34 seconds for 500 iterations on a 1024x1024 image, while for fitting the variational posterior parameters (, ) it takes approximately 2.5 minutes for 500 iterations. Once the variational posterior is fit, the model can generate any arbitrary number of samples instantaneously.


\renewcommand{\w}{2.9cm}


\begin{figure*}
\begin{center}
\setlength{\tabcolsep}{0pt}
\renewcommand{\arraystretch}{0.5}\resizebox{\columnwidth}{!}{\begin{tabu}{ccccc}\rowfont{\footnotesize} Original & Masked & \patchgan & \dip & \brgm \\

\inc{\inFH{\nrIthree}}&
\inc{\inFM{\nrIthree}}&
\inc{\inFP{\nrIthree}}&
\inc{\inFD{\nrIthree}}&
\inc{\inFO{\nrIthree}}\\

\inc{\inFH{\nrIfour}}&
\inc{\inFM{\nrIfour}}&
\inc{\inFP{\nrIfour}}&
\inc{\inFD{\nrIfour}}&
\inc{\inFO{\nrIfour}}\\

\inc{\inFEH{\nrIfive}}&
\inc{\inFEM{\nrIfive}}&
\inc{\inFEP{\nrIfive}}&
\inc{\inFED{\nrIfive}}&
\inc{\inFEO{\nrIfive}}\\

\inc{\inFEH{\nrIsix}}&
\inc{\inFEM{\nrIsix}}&
\inc{\inFEP{\nrIsix}}&
\inc{\inFED{\nrIsix}}&
\inc{\inFEO{\nrIsix}}\\

\end{tabu}
}
\caption{Uncurated in-painting examples by BRGM on the FFHQ dataset, compared against \patchgan\ and \dip.}\label{ffhq-inpainting}
\end{center}
\end{figure*}


\begin{figure*}
\begin{center}
\setlength{\tabcolsep}{0pt}
\renewcommand{\arraystretch}{0.5}\resizebox{\columnwidth}{!}{\begin{tabu}{ccccc}\rowfont{\footnotesize} Original & Masked & \patchgan & \dip & \brgm \\

\inc{\inXH{\nrIM}}&
\inc{\inXM{\nrIM}}&
\inc{\inXP{\nrIM}}&
\inc{\inXD{\nrIM}}&
\inc{\inXO{\nrIM}}\\

\inc{\inXH{\nrIMtwo}}&
\inc{\inXM{\nrIMtwo}}&
\inc{\inXP{\nrIMtwo}}&
\inc{\inXD{\nrIMtwo}}&
\inc{\inXO{\nrIMtwo}}\\

\inc{\inXH{\nrIMthree}}&
\inc{\inXM{\nrIMthree}}&
\inc{\inXP{\nrIMthree}}&
\inc{\inXD{\nrIMthree}}&
\inc{\inXO{\nrIMthree}}\\

\inc{\inXH{\nrIMfour}}&
\inc{\inXM{\nrIMfour}}&
\inc{\inXP{\nrIMfour}}&
\inc{\inXD{\nrIMfour}}&
\inc{\inXO{\nrIMfour}}\\

\end{tabu}
}
\caption{Uncurated in-painting examples by BRGM on the Chest X-ray dataset, compared against \patchgan\ and \dip.}\label{xray-inpainting}
\end{center}
\end{figure*}


\begin{figure*}
\begin{center}
\setlength{\tabcolsep}{0pt}
\renewcommand{\arraystretch}{0.5}\resizebox{\columnwidth}{!}{\begin{tabu}{ccccc}\rowfont{\footnotesize} Original & Masked & \patchgan & \dip & \brgm \\

\inc{\inBH{\nrIMB}}&
\inc{\inBM{\nrIMB}}&
\inc{\inBP{\nrIMB}}&
\inc{\inBD{\nrIMB}}&
\inc{\inBO{\nrIMB}}\\

\inc{\inBH{\nrIMBtwo}}&
\inc{\inBM{\nrIMBtwo}}&
\inc{\inBP{\nrIMBtwo}}&
\inc{\inBD{\nrIMBtwo}}&
\inc{\inBO{\nrIMBtwo}}\\

\inc{\inBH{\nrIMBthree}}&
\inc{\inBM{\nrIMBthree}}&
\inc{\inBP{\nrIMBthree}}&
\inc{\inBD{\nrIMBthree}}&
\inc{\inBO{\nrIMBthree}}\\

\inc{\inBH{\nrIMBfour}}&
\inc{\inBM{\nrIMBfour}}&
\inc{\inBP{\nrIMBfour}}&
\inc{\inBD{\nrIMBfour}}&
\inc{\inBO{\nrIMBfour}}\\

\end{tabu}
}
\caption{Uncurated in-painting examples by BRGM on the brain dataset, compared against \patchgan\ and \dip.}\label{brain-inpainting}
\end{center}
\end{figure*}






\newcommand{\mcc}[1]{\multicolumn{1}{c}{#1}}

\begin{table*}
\centering
\setlength{\tabcolsep}{2pt}
\resizebox{0.9\columnwidth}{!}{\begin{tabular}{l|ccc|ccc|ccc|ccc}
\toprule
Dataset &       \multicolumn{3}{c}{\brgm}  &    \multicolumn{3}{c}{\pulse}    &   \multicolumn{3}{c}{\esrgan}    &   \multicolumn{3}{c}{\srfbn}  \\
& PSNR & SSIM & MAE  & PSNR & SSIM & MAE & PSNR & SSIM & MAE & PSNR & SSIM & MAE\\
\midrule
FFHQ  &  20.13 & 0.74 & 17.46 &  19.51 & 0.68 & 19.20 &  18.91 & 0.69 & 20.01 &  21.43 & 0.76 & 15.01 \\
   FFHQ  &  22.74 & 0.74 & 12.52 &  15.37 & 0.35 & 33.13 &  21.10 & 0.72 & 14.53 &   26.28 & 0.89 & 7.46 \\
   FFHQ  &  24.16 & 0.70 & 10.63 &  15.74 & 0.37 & 31.54 &  23.14 & 0.72 & 10.94 &   28.96 & 0.90 & 5.21 \\
X-ray  &   27.14 & 0.91 & 7.45 &             - & - & - &  25.17 & 0.87 & 10.14 &   26.88 & 0.92 & 7.63 \\
   X-ray  &   27.84 & 0.84 & 6.77 &             - & - & - &   26.44 & 0.81 & 8.36 &   31.80 & 0.95 & 3.71 \\
   X-ray  &   27.62 & 0.79 & 6.63 &             - & - & - &   29.47 & 0.87 & 5.33 &   33.91 & 0.95 & 2.47 \\
Brains  &   26.33 & 0.84 & 7.29 &             - & - & - &  21.06 & 0.60 & 14.27 &   26.21 & 0.77 & 8.62 \\
 Brains  &   27.30 & 0.81 & 6.54 &             - & - & - &   25.23 & 0.78 & 8.35 &   31.60 & 0.93 & 3.86 \\




\bottomrule
\end{tabular}
}
\caption{Additional performance metrics (PSNR, SSIM and MAE) for the super-resolution evaluation.}
\label{eval-super-resolution-supplementary}
\end{table*}




\begin{table*}
\centering
\setlength{\tabcolsep}{3pt}
\resizebox{\columnwidth}{!}{\begin{tabular}{l|cc|cc|rr|rr}
\toprule
Dataset &       \multicolumn{2}{c}{\brgm}  &    \multicolumn{2}{c}{\pulse}    &   \multicolumn{2}{c}{\esrgan}    &   \multicolumn{2}{c}{\srfbn}  \\


 & \lpi & \rms  & \lpi & \rms & \lpi & \rms & \lpi & \rms\\
\midrule
  FFHQ  & 0.24  0.07 & 25.66  6.13 & 0.29  0.07 & 27.14  4.08 & 0.35  0.07 & 29.32  6.52 & 0.33  0.05 & 22.07  4.47 \\
  FFHQ  & 0.30  0.07 & 18.93  3.95 & 0.48  0.08 & 42.97  3.78 & 0.29  0.05 & 23.02  5.48 & 0.23  0.04 & 12.73  3.08 \\
  FFHQ  & 0.36  0.06 & 16.07  3.21 & 0.53  0.07 & 41.31  3.57 & 0.26  0.05 & 18.37  5.06 &  0.23  0.04 & 9.40  2.48 \\
 FFHQ  & 0.34  0.05 & 15.84  3.23 & 0.57  0.06 & 34.89  2.21 & 0.15  0.05 & 15.84  4.83 &  0.09  0.02 & 7.55  2.30 \\
 X-ray  & 0.18  0.05 & 11.61  3.22 &                              - & - & 0.32  0.07 & 14.67  4.48 & 0.37  0.04 & 12.28  4.39 \\
 X-ray  & 0.23  0.05 & 10.47  2.04 &                              - & - & 0.32  0.05 & 12.56  3.34 &  0.21  0.03 & 6.84  2.01 \\
 X-ray  & 0.31  0.04 & 10.58  1.81 &                              - & - &  0.30  0.03 & 8.67  1.86 &  0.22  0.02 & 5.32  1.44 \\
X-ray  & 0.27  0.03 & 10.53  1.91 &                              - & - &  0.20  0.02 & 7.19  1.34 &  0.07  0.01 & 4.33  1.30 \\
Brains  & 0.12  0.03 & 12.42  1.71 &                              - & - & 0.34  0.04 & 22.81  3.26 & 0.33  0.03 & 12.57  1.51 \\
Brains  & 0.17  0.03 & 11.08  1.29 &                              - & - & 0.31  0.03 & 14.16  2.36 &  0.18  0.03 & 6.80  1.14 \\

\bottomrule
\end{tabular}
}
\caption{Performance metrics for super-resolution as in Table \ref{eval} (left), but additionally including the standard deviation of scores across the 100 test images.}
\label{eval-super-resolution-supplementary-lpips-std}
\end{table*}


\FloatBarrier

\section{Additional Evaluation Results}
\label{supp-eval-humans}


To evaluate human perceptual quality, we performed a forced-choice pairwise comparison test as shown in Fig. \ref{human-study}. Each rater is shown a true, high-quality image on the left, and four potential reconstructions they have to choose from.  For each input resolution level (, , \dots), we ran the human evaluation on 20 raters using 100 pairs of 5 images each (total of 500 images per experiment shown to each rater). We launched all human evaluations on Amazon Mechanical Turk. We paid \9. We obtained IRB approval for this study from our institution.


\begin{figure*}
\begin{center}
\includegraphics[width=0.8\textwidth]{images/human_study}
\caption{Setup of our human study, using a forced-choice pairwise comparison design. Each rater is shown a true, high-quality image on the left, and four potential reconstructions (A-D) by different algorithms. They have to select which reconstruction best resembled the HQ image. }
\label{human-study}
\end{center}
\end{figure*}







\renewcommand{\w}{1.8cm}



\renewcommand{\is}{52116}
\renewcommand{\step}{step700}

\begin{figure*}
\setlength{\tabcolsep}{0pt}
\renewcommand{\arraystretch}{1.5}\begin{tabular}{c|c||c||ccccc}\arrayrulecolor{white}
& True & Est. Mean & Sample 1 & Sample 2 & Sample 3 & Sample 4 & Sample 5\\
HR & \inc{\flds/\is_true.jpg} & \inc{\flds/\is_clean_\step.jpg} & \inc{\flds/\is_sample1_\step.jpg} & \inc{\flds/\is_sample2_\step.jpg} & \inc{\flds/\is_sample3_\step.jpg} & \inc{\flds/\is_sample4_\step.jpg} & \inc{\flds/\is_sample5_\step.jpg}\\
 
LR & \inc{\flds/\is_target.jpg} & \inc{\flds/\is_corrupted_\step.jpg} & \inc{\flds/\is_corrsample1_\step.jpg} & \inc{\flds/\is_corrsample2_\step.jpg} & \inc{\flds/\is_corrsample3_\step.jpg} & \inc{\flds/\is_corrsample4_\step.jpg} & \inc{\flds/\is_corrsample5_\step.jpg}\\
 \end{tabular}

\renewcommand{\is}{68319}
\renewcommand{\step}{step490}

\begin{tabular}{c|c||c||ccccc}\arrayrulecolor{white}
HR & \inc{\flds/\is_true.jpg} & \inc{\flds/\is_clean_\step.jpg} & \inc{\flds/\is_sample1_\step.jpg} & \inc{\flds/\is_sample2_\step.jpg} & \inc{\flds/\is_sample3_\step.jpg} & \inc{\flds/\is_sample4_\step.jpg} & \inc{\flds/\is_sample5_\step.jpg}\\
 
LR & \inc{\flds/\is_target.jpg} & \inc{\flds/\is_corrupted_\step.jpg} & \inc{\flds/\is_corrsample1_\step.jpg} & \inc{\flds/\is_corrsample2_\step.jpg} & \inc{\flds/\is_corrsample3_\step.jpg} & \inc{\flds/\is_corrsample4_\step.jpg} & \inc{\flds/\is_corrsample5_\step.jpg}\\
 \end{tabular}
 
\renewcommand{\flds}{results/samXRAY}
\renewcommand{\is}{img00000664}
\renewcommand{\step}{step400}

\begin{tabular}{c|c||c||ccccc}\arrayrulecolor{white}
HR & \inc{\flds/\is_true.jpg} & \inc{\flds/\is_clean_\step.jpg} & \inc{\flds/\is_sample1_\step.jpg} & \inc{\flds/\is_sample2_\step.jpg} & \inc{\flds/\is_sample3_\step.jpg} & \inc{\flds/\is_sample4_\step.jpg} & \inc{\flds/\is_sample5_\step.jpg}\\
 
LR & \inc{\flds/\is_target.jpg} & \inc{\flds/\is_corrupted_\step.jpg} & \inc{\flds/\is_corrsample1_\step.jpg} & \inc{\flds/\is_corrsample2_\step.jpg} & \inc{\flds/\is_corrsample3_\step.jpg} & \inc{\flds/\is_corrsample4_\step.jpg} & \inc{\flds/\is_corrsample5_\step.jpg}\\
 \end{tabular}


\renewcommand{\flds}{results/samBrains}
\renewcommand{\is}{img00000034}
\renewcommand{\step}{step400}


\begin{tabular}{c|c||c||ccccc}\arrayrulecolor{white}
HR & \inc{\flds/\is_true.jpg} & \inc{\flds/\is_clean_\step.jpg} & \inc{\flds/\is_sample1_\step.jpg} & \inc{\flds/\is_sample2_\step.jpg} & \inc{\flds/\is_sample3_\step.jpg} & \inc{\flds/\is_sample4_\step.jpg} & \inc{\flds/\is_sample5_\step.jpg}\\
 
LR & \inc{\flds/\is_target.jpg} & \inc{\flds/\is_corrupted_\step.jpg} & \inc{\flds/\is_corrsample1_\step.jpg} & \inc{\flds/\is_corrsample2_\step.jpg} & \inc{\flds/\is_corrsample3_\step.jpg} & \inc{\flds/\is_corrsample4_\step.jpg} & \inc{\flds/\is_corrsample5_\step.jpg}\\

 \end{tabular}
\caption{Sampling of multiple reconstructions using Variational Inference on super-resolution tasks with varying factors. From left, we show the true image, the estimated variational mean, alongside five random samples around that mean. For each high-resolution (HR) image, we show the corresponding low-resolution (LR) image below. While for some of the images, the reconstructions don't match the true image, the downsampled low-resolution images do match with the true image. We chose such extreme super-resolution in order to obtain a wide posterior distribution.}
\label{samplingsupp}
\end{figure*}








\renewcommand{\flds}{results/samFFHQinp}
\renewcommand{\is}{01014}
\renewcommand{\step}{step270}

\begin{figure*}
\setlength{\tabcolsep}{0pt}
\renewcommand{\arraystretch}{1.5}\begin{tabular}{p{1.1cm}|c||c||ccccc}\arrayrulecolor{white}
& True & Est. Mean & Sample 1 & Sample 2 & Sample 3 & Sample 4 & Sample 5\\
Clean & \inc{\flds/\is_true.jpg} & \inc{\flds/\is_clean_\step.jpg} & \inc{\flds/\is_sample1_\step.jpg} & \inc{\flds/\is_sample2_\step.jpg} & \inc{\flds/\is_sample3_\step.jpg} & \inc{\flds/\is_sample4_\step.jpg} & \inc{\flds/\is_sample5_\step.jpg}\\
 
Mask/ Merged & \inc{\flds/\is_target.jpg} & \inc{\flds/\is_merged_\step.jpg} & \inc{\flds/\is_mergedsample1_\step.jpg} & \inc{\flds/\is_mergedsample2_\step.jpg} & \inc{\flds/\is_mergedsample3_\step.jpg} & \inc{\flds/\is_mergedsample4_\step.jpg} & \inc{\flds/\is_mergedsample5_\step.jpg}\\
 \end{tabular}

 \renewcommand{\is}{03456}
\renewcommand{\step}{step290}
 
 \begin{tabular}{p{1.1cm}|c||c||ccccc}\arrayrulecolor{white}
Clean & \inc{\flds/\is_true.jpg} & \inc{\flds/\is_clean_\step.jpg} & \inc{\flds/\is_sample1_\step.jpg} & \inc{\flds/\is_sample2_\step.jpg} & \inc{\flds/\is_sample3_\step.jpg} & \inc{\flds/\is_sample4_\step.jpg} & \inc{\flds/\is_sample5_\step.jpg}\\
 
Mask/ Merged & \inc{\flds/\is_target.jpg} & \inc{\flds/\is_merged_\step.jpg} & \inc{\flds/\is_mergedsample1_\step.jpg} & \inc{\flds/\is_mergedsample2_\step.jpg} & \inc{\flds/\is_mergedsample3_\step.jpg} & \inc{\flds/\is_mergedsample4_\step.jpg} & \inc{\flds/\is_mergedsample5_\step.jpg}\\
 \end{tabular}

 
\renewcommand{\flds}{results/samXRAYinp}
\renewcommand{\is}{img00000664}
\renewcommand{\step}{step440}

\begin{tabular}{p{1.1cm}|c||c||ccccc}\arrayrulecolor{white}
Clean & \inc{\flds/\is_true.jpg} & \inc{\flds/\is_clean_\step.jpg} & \inc{\flds/\is_sample1_\step.jpg} & \inc{\flds/\is_sample2_\step.jpg} & \inc{\flds/\is_sample3_\step.jpg} & \inc{\flds/\is_sample4_\step.jpg} & \inc{\flds/\is_sample5_\step.jpg}\\
 

Mask/ Merged & \inc{\flds/\is_target.jpg} & \inc{\flds/\is_merged_\step.jpg} & \inc{\flds/\is_mergedsample1_\step.jpg} & \inc{\flds/\is_mergedsample2_\step.jpg} & \inc{\flds/\is_mergedsample3_\step.jpg} & \inc{\flds/\is_mergedsample4_\step.jpg} & \inc{\flds/\is_mergedsample5_\step.jpg}\\
 \end{tabular}



\renewcommand{\flds}{results/samBrainsInp}
\renewcommand{\is}{img00000034} \renewcommand{\step}{step490}

\begin{tabular}{p{1.1cm}|c||c||ccccc}\arrayrulecolor{white}
Clean & \inc{\flds/\is_true.jpg} & \inc{\flds/\is_clean_\step.jpg} & \inc{\flds/\is_sample1_\step.jpg} & \inc{\flds/\is_sample2_\step.jpg} & \inc{\flds/\is_sample3_\step.jpg} & \inc{\flds/\is_sample4_\step.jpg} & \inc{\flds/\is_sample5_\step.jpg}\\
 

Mask/ Merged & \inc{\flds/\is_target.jpg} & \inc{\flds/\is_merged_\step.jpg} & \inc{\flds/\is_mergedsample1_\step.jpg} & \inc{\flds/\is_mergedsample2_\step.jpg} & \inc{\flds/\is_mergedsample3_\step.jpg} & \inc{\flds/\is_mergedsample4_\step.jpg} & \inc{\flds/\is_mergedsample5_\step.jpg}\\

 \end{tabular}
 \caption{Sampling of multiple reconstructions using Variational Inference on in-painting tasks. From left, we show the true image, the estimated variational mean, alongside five random samples around that mean. For the mean and for each sample, we show both the clean image, as well as the true image with the in-painted area from the sample.}
\label{samplinginpsupp}
\end{figure*}


\section{Sensitivity to hyper-parameters}
\label{ablation}

To understand how sensitive our model is to the hyper-parameters ,  and , we performed in Table \ref{tab:ablation} an ablation analysis and computed the perceptual distance (LPIPS) and root mean squared error (RMSE) between our reconstructions and the true images. We performed this ablation on 64x super-resolution on 5 FFHQ test images, where we varied only one parameter at a time, keeping the others fixed to the following values: ,  and . The results show that our method is not overly-sensitive to the choice of the lambda hyper-parameters, as there are multiple values over which results are satisfactory: , , and .

\begin{table}
\centering
\renewcommand{\arraystretch}{1.5}\begin{tabular}{c|ccccc}
\hline
 &  &  &  &  & \\
LPIPS/RMSE & 0.65/49.48 & 0.57/36.33 & 0.59/36.73 & 0.58/35.87 & 0.58/36.77\\
\hline
 &  &  &  &  & \\
LPIPS/RMSE & 0.60/36.42 & 0.59/35.19 & 0.57/34.67 & 0.56/33.56 & 0.60/38.86\\
\hline
 &  &  &  &  & \\
LPIPS/RMSE & 0.56/31.57 & 0.57/34.38 & 0.58/35.35 & 0.64/44.83 & 0.68/63.95\\
\hline
\end{tabular}
\vspace{0.25em}
\caption{Results of ablation study over hyper-parameters ,  and . Reported are perceptual distance (LPIPS) and pixelwise root mean squared errors (RMSE) between the true images and the reconstructed images.}
\label{tab:ablation}
\end{table}


\section{Downstream processing of BRGM-restored images}
\label{downstream}


In order to show that the BRGM-reconstructed images are useful for downstream tasks, we ran a ResNet-16 that predicts lung-edema severity on BRGM-restored Chest X-Ray images. Edema severity was predicted as either (1) no edema, (2) mild -- vascular congestion, (3) moderate -- interstitial edema and (4) severe -- alveolar edema. The confusion matrix is shown in Table \ref{tab:downstream} (top table). We ran the model on 100 normal images (denoted as True), as well as the same 100 images but with the left-half masked and then reconstructed by our BRGM method (denoted as Reconstructed). We obtained a precision of 0.58 and a recall of 0.58. Most (73/100) of the severity scores are either preserved in the reconstructed images (diagonal), or change to an adjacent score (just above/below the diagonal). We note that, in general, the classification between adjacent severity levels (e.g. No edema vs Mild, Mild vs Moderate, etc ...) is a very hard problem, where the ResNet itself has an  score of 45.03\% (random guessing would give 25\% on a balanced dataset).

We also note that these results depend on how much of the image is reconstructed. If we re-run the analysis with a smaller mask (only a quadrant masked and reconstructed instead of half of the image -- see Table \ref{tab:downstream} bottom), we obtain significantly better results in the confusion matrix, and a precision of 0.75 and recall of 0.75. In particular, we note that the true severe cases improved their diagnosis.

\begin{table}
\centering
\renewcommand{\arraystretch}{1.5}Mask 50\% of image (left half of image)\\
\begin{tabular}{c|ccccc}
True Reconstructed & No edema	& Mild & Moderate & Severe\\
\hline
No edema & 53 & 2 & 11 & 1 \\ 
Mild & 5 & 0 & 1 & 0 \\
Moderate & 9 & 3 & 5 & 0 \\
Severe & 3 & 3 & 4 & 0 \\
\end{tabular}
\vspace{2.5em}

Mask 25\% of image (top-right quadrant only)\\
\begin{tabular}{c|ccccc}
True Reconstructed & No edema	& Mild & Moderate & Severe\\
\hline
No edema & 66 & 2 & 8 & 1\\
Mild & 3 & 1 & 3 & 0\\
Moderate & 5 & 0 & 4 & 1\\
Severe & 0 & 1 & 1 & 4\\
\end{tabular}
\caption{Results of downstream processing on BRGM-reconstructed images vs true images. (Top table) We show the confusion matrix of a ResNet-16 at edema prediction on BRGM-inpainted images vs True images. For the inpainted images, the mask covered 50\% of the pixels (left-half). (Bottom table) As above, but only 25\% of image is masked and then reconstructed. }
\label{tab:downstream}
\end{table}

\section{Method Inconsistency}
\label{inconsistency}

One caveat of our method is that it can create reconstructions that are inconsistent with the input data. We highlight this in Figs. \ref{imgdiff-ffhq} and \ref{imgdiff-medical}. This is because our method relies on the ability of a pre-trained generator to generate any potential realistic image as input. In addition to that, in Eq. (\ref{fullloss}), our method optimizes the pixelwise and perceptual loss terms between the input image and the downsampled reconstruction. As we show in Figs. \ref{imgdiff-ffhq} and \ref{imgdiff-medical}, while there are little differences between the input and the reconstruction at low 16x16 resolutions, at higher 128x128 resolutions, these differences become larger and more noticeable. Another aspect that contributes to this issue is the extra prior term , which is however required to ensure better reconstructions (see Fig \ref{evolution}). Nevertheless, we believe that the inconsistency is fundamentally caused by limitations of the generator , that will be solved in the near future with better generator models that offer improved generalisability to unseen images.



\newcommand{\diffFT}[1]{\fld/00607-recon-real-imagesffhq_test-super-resolution/image#1-target} \newcommand{\diffFO}[1]{\fld/00607-recon-real-imagesffhq_test-super-resolution/image#1-corrupted-step5000} \newcommand{\diffFD}[1]{\fld/00607-recon-real-imagesffhq_test-super-resolution/image#1-diff} 

\newcommand{\diffXT}[1]{\fld/00608-recon-real-imagesxray_frontal_test-super-resolution/image#1-target}
\newcommand{\diffXO}[1]{\fld/00608-recon-real-imagesxray_frontal_test-super-resolution/image#1-corrupted-step5000}
\newcommand{\diffXD}[1]{\fld/00608-recon-real-imagesxray_frontal_test-super-resolution/image#1-diff}


\newcommand{\diffBT}[1]{\fld/00626-recon-real-imagesbrains_test_mono-super-resolution/image#1-target}
\newcommand{\diffBO}[1]{\fld/00626-recon-real-imagesbrains_test_mono-super-resolution/image#1-corrupted-step5000}
\newcommand{\diffBD}[1]{\fld/00626-recon-real-imagesbrains_test_mono-super-resolution/image#1-diff}


\newcommand{\diffFTtwo}[1]{\fld/00620-recon-real-imagesffhq_test-super-resolution/image#1-target} \newcommand{\diffFOtwo}[1]{\fld/00620-recon-real-imagesffhq_test-super-resolution/image#1-corrupted-step5000} \newcommand{\diffFDtwo}[1]{\fld/00620-recon-real-imagesffhq_test-super-resolution/image#1-diff} 

\newcommand{\diffXTtwo}[1]{\fld/00622-recon-real-imagesxray_frontal_test-super-resolution/image#1-target}
\newcommand{\diffXOtwo}[1]{\fld/00622-recon-real-imagesxray_frontal_test-super-resolution/image#1-corrupted-step5000}
\newcommand{\diffXDtwo}[1]{\fld/00622-recon-real-imagesxray_frontal_test-super-resolution/image#1-diff}


\newcommand{\diffBTtwo}[1]{\fld/00604-recon-real-imagesbrains_test_mono-super-resolution/image#1-target}
\newcommand{\diffBOtwo}[1]{\fld/00604-recon-real-imagesbrains_test_mono-super-resolution/image#1-corrupted-step5000}
\newcommand{\diffBDtwo}[1]{\fld/00604-recon-real-imagesbrains_test_mono-super-resolution/image#1-diff}

\newcommand{\diffFTthree}[1]{\fld/00624-recon-real-imagesffhq_test-super-resolution/image#1-target} \newcommand{\diffFOthree}[1]{\fld/00624-recon-real-imagesffhq_test-super-resolution/image#1-corrupted-step5000} \newcommand{\diffFDthree}[1]{\fld/00624-recon-real-imagesffhq_test-super-resolution/image#1-diff} 

\newcommand{\diffXTthree}[1]{\fld/00625-recon-real-imagesxray_frontal_test-super-resolution/image#1-target}
\newcommand{\diffXOthree}[1]{\fld/00625-recon-real-imagesxray_frontal_test-super-resolution/image#1-corrupted-step5000}
\newcommand{\diffXDthree}[1]{\fld/00625-recon-real-imagesxray_frontal_test-super-resolution/image#1-diff}



\newcommand{\diffFTfour}[1]{\fld/00598-recon-real-imagesffhq_test-super-resolution/image#1-target} \newcommand{\diffFOfour}[1]{\fld/00598-recon-real-imagesffhq_test-super-resolution/image#1-corrupted-step5000} \newcommand{\diffFDfour}[1]{\fld/00598-recon-real-imagesffhq_test-super-resolution/image#1-diff} 

\newcommand{\diffXTfour}[1]{\fld/00601-recon-real-imagesxray_frontal_test-super-resolution/image#1-target}
\newcommand{\diffXOfour}[1]{\fld/00601-recon-real-imagesxray_frontal_test-super-resolution/image#1-corrupted-step5000}
\newcommand{\diffXDfour}[1]{\fld/00601-recon-real-imagesxray_frontal_test-super-resolution/image#1-diff}


\newcommand{\nrD}{0016}

\newcommand{\nrDtwo}{0017}
\newcommand{\nrDthree}{0000}
\newcommand{\nrDfour}{0000}


\renewcommand{\w}{3.2cm}

\begin{figure*}
\centering
\setlength{\tabcolsep}{0pt}
\renewcommand{\arraystretch}{1.5}\resizebox{\columnwidth}{!}{\begin{tabular}{lccc||ccc}\arrayrulecolor{white}
& Input & Downsampled Recon. & Difference & Input & Downsampled Recon. & Difference\\

   & \inc{\diffFT{\nrD}} & \inc{\diffFO{\nrD}} & \inc{\diffFD{\nrD}} & \inc{\diffFT{\nrDtwo}} & \inc{\diffFO{\nrDtwo}} & \inc{\diffFD{\nrDtwo}}\\
   & \inc{\diffFTtwo{\nrD}} & \inc{\diffFOtwo{\nrD}} & \inc{\diffFDtwo{\nrD}} & \inc{\diffFTtwo{\nrDtwo}} & \inc{\diffFOtwo{\nrDtwo}} & \inc{\diffFDtwo{\nrDtwo}}\\ 
   & \inc{\diffFTthree{\nrD}} & \inc{\diffFOthree{\nrD}} & \inc{\diffFDthree{\nrD}} & \inc{\diffFTthree{\nrDtwo}} & \inc{\diffFOthree{\nrDtwo}} & \inc{\diffFDthree{\nrDtwo}}\\
 & \inc{\diffFTfour{\nrD}} & \inc{\diffFOfour{\nrD}} & \inc{\diffFDfour{\nrD}} & \inc{\diffFTfour{\nrDtwo}} & \inc{\diffFOfour{\nrDtwo}} & \inc{\diffFDfour{\nrDtwo}}\\
\end{tabular}
}
\caption{Inconsistency of our method on FFHQ, across different resolution levels, using uncurated example pictures. Left columns shows input images, and the middle columns show downsampled reconstructions (i.e. ) where images were 4x super-resolved, then downsampled by 4x to match again the input. Right columns show difference between input and the downsampled reconstructions. For higher resolution inputs (128x128), the method cannot accurately reconstruct the input image, likely because the generator has limited generalisability to such unseen faces from FFHQ (our method was trained not on the entire FFHQ, but on a training subset). The difference maps, representing x3 scaled mean absolute errors, show that certain regions in particular are not well reconstructed, such as the hair of the girl on the right. }
\label{imgdiff-ffhq}
\end{figure*}


\begin{figure*}
\centering
\setlength{\tabcolsep}{0pt}
\renewcommand{\arraystretch}{1.5}\resizebox{\columnwidth}{!}{\begin{tabular}{lccc||ccc}\arrayrulecolor{white}
& Input & Downsampled Recon. & Difference & Input & Downsampled Recon. & Difference\\

16x16   & \inc{\diffXT{\nrD}} & \inc{\diffXO{\nrD}} & \inc{\diffXD{\nrD}} & \inc{\diffXT{\nrDtwo}} & \inc{\diffXO{\nrDtwo}} & \inc{\diffXD{\nrDtwo}}\\
32x32   & \inc{\diffXTtwo{\nrD}} & \inc{\diffXOtwo{\nrD}} & \inc{\diffXDtwo{\nrD}}& \inc{\diffXTtwo{\nrDtwo}} & \inc{\diffXOtwo{\nrDtwo}} & \inc{\diffXDtwo{\nrDtwo}}\\
64x64   & \inc{\diffXTthree{\nrD}} & \inc{\diffXOthree{\nrD}} & \inc{\diffXDthree{\nrD}} & \inc{\diffXTthree{\nrDtwo}} & \inc{\diffXOthree{\nrDtwo}} & \inc{\diffXDthree{\nrDtwo}}\\
128x128 & \inc{\diffXTfour{\nrD}} & \inc{\diffXOfour{\nrD}} & \inc{\diffXDfour{\nrD}} & \inc{\diffXTfour{\nrDtwo}} & \inc{\diffXOfour{\nrDtwo}} & \inc{\diffXDfour{\nrDtwo}}\\

16x16   & \inc{\diffBT{\nrD}} & \inc{\diffBO{\nrD}} & \inc{\diffBD{\nrD}} & \inc{\diffBT{\nrDtwo}} & \inc{\diffBO{\nrDtwo}} & \inc{\diffBD{\nrDtwo}}\\
32x32   & \inc{\diffBTtwo{\nrD}} & \inc{\diffBOtwo{\nrD}} & \inc{\diffBDtwo{\nrD}} & \inc{\diffBTtwo{\nrDtwo}} & \inc{\diffBOtwo{\nrDtwo}} & \inc{\diffBDtwo{\nrDtwo}}\\

\end{tabular}
}
\caption{Inconsistency of our method on the medical datasets, using uncurated examples. Same setup as in Fig. \ref{imgdiff-ffhq}. }
\label{imgdiff-medical}
\end{figure*}


\renewcommand{\fld}{results}

\newcommand{\nrFA}{0096}
\newcommand{\nrFAtwo}{0095}
\newcommand{\nrFAth}{0090}
\newcommand{\nrFAfo}{0093}
\newcommand{\nrFAfi}{0080}
\newcommand{\nrFAsi}{0082}

\newcommand{\fai}[1]{\fld/00620-recon-real-imagesffhq_test-super-resolution/image#1-target}
\newcommand{\fao}[1]{\fld/00620-recon-real-imagesffhq_test-super-resolution/image#1-clean-step5000}
\newcommand{\fat}[1]{\fld/00620-recon-real-imagesffhq_test-super-resolution/image#1-true}

\newcommand{\diffFTmain}[1]{\fld/00598-recon-real-imagesffhq_test-super-resolution/image#1-target} \newcommand{\diffFOmain}[1]{\fld/00598-recon-real-imagesffhq_test-super-resolution/image#1-corrupted-step5000} \newcommand{\diffFDmain}[1]{\fld/00598-recon-real-imagesffhq_test-super-resolution/image#1-diff} 

\newcommand{\nrDmain}{0017}

\renewcommand{\w}{2.4cm}


\begin{figure}
\centering
\setlength{\tabcolsep}{0pt}
\begin{tabular}{ccc||ccc}\arrayrulecolor{white}
Input & Reconstruction & True & Input & Reconstruction & True\\
 \inc{\fai{\nrFA}} & \inc{\fao{\nrFA}} & \inc{\fat{\nrFA}} &\inc{\fai{\nrFAtwo}} & \inc{\fao{\nrFAtwo}} & \inc{\fat{\nrFAtwo}}\\
 
 \inc{\fai{\nrFAth}} & \inc{\fao{\nrFAth}} & \inc{\fat{\nrFAth}} & \inc{\fai{\nrFAfo}} & \inc{\fao{\nrFAfo}} & \inc{\fat{\nrFAfo}}\\
 
 \inc{\fai{\nrFAfi}} & \inc{\fao{\nrFAfi}} & \inc{\fat{\nrFAfi}} & \inc{\fai{\nrFAsi}} & \inc{\fao{\nrFAsi}} & \inc{\fat{\nrFAsi}}\\

\end{tabular}
\caption{Failure cases of our method.}
\label{failure}
\end{figure}







\end{document}
