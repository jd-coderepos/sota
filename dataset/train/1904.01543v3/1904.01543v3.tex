\documentclass{article}

\PassOptionsToPackage{numbers, compress}{natbib}

\usepackage[final]{neurips_2020}
\newcommand{\xhdr}[1]{{\noindent\bfseries #1}}

\usepackage[utf8]{inputenc} 
\usepackage[T1]{fontenc}    
\usepackage{hyperref}      
\usepackage{url}           
\usepackage{booktabs}      
\usepackage{amsfonts}      
\usepackage{nicefrac}     
\usepackage{adjustbox}
\usepackage{comment}

\usepackage{floatrow}
\newfloatcommand{capbtabbox}{table}[][\FBwidth]

\usepackage{graphicx}

\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{url}
\usepackage{todonotes}
\usepackage{multirow}
\usepackage{tikz}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{thmtools}		
\usepackage{mleftright}
\usepackage{stmaryrd}
\usepackage{nicefrac}

\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{remark}[theorem]{Remark}

\usepackage{thm-restate}
\usepackage[mathic=true]{mathtools}
\usepackage{fixmath}
\usepackage{siunitx}
\usepackage{color}

\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}

\usepackage{enumitem}
\setlist[enumerate]{noitemsep, topsep=0.5\topsep}
\setlist[description]{noitemsep, topsep=0.5\topsep}
\setlist[itemize]{noitemsep, topsep=0.5\topsep}

\usepackage{setspace}
\usepackage{ellipsis}
\usepackage{xspace}

\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\algdef{SE}{ParForAllLoop}{EndParForAllLoop}[1]{\textbf{parallel for} \textbf{do}}{\textbf{end}}

\usepackage{ifthen}
\newcommand{\CC}[1][]{}

\newcommand{\win}[1]{\textbf{#1}}
\newcommand{\sd}[1]{\scriptsize{#1}}

\makeatletter
\def\thmt@refnamewithcomma #1#2#3,#4,#5\@nil{\@xa\def\csname\thmt@envname #1utorefname\endcsname{#3}\ifcsname #2refname\endcsname
	\csname #2refname\expandafter\endcsname\expandafter{\thmt@envname}{#3}{#4}\fi
}
\makeatother
\usepackage[capitalise,noabbrev]{cleveref}   
\newcommand{\new}[1]{\emph{#1}}

\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}

\newcommand{\cA}{\ensuremath{{\mathcal A}}\xspace}
\newcommand{\cB}{\ensuremath{{\mathcal B}}\xspace}
\newcommand{\cC}{\ensuremath{{\mathcal C}}\xspace}
\newcommand{\cD}{\ensuremath{{\mathcal D}}\xspace}
\newcommand{\cF}{\ensuremath{{\mathcal F}}\xspace}
\newcommand{\cH}{\ensuremath{{\mathcal H}}\xspace}
\newcommand{\cN}{\ensuremath{{\mathcal N}}\xspace}
\newcommand{\cO}{\ensuremath{{\mathcal O}}\xspace}
\newcommand{\cP}{\ensuremath{{\mathcal P}}\xspace}
\newcommand{\cR}{\ensuremath{{\mathcal R}}\xspace}
\newcommand{\cS}{\ensuremath{{\mathcal S}}\xspace}
\newcommand{\cU}{\ensuremath{{\mathcal U}}\xspace}
\newcommand{\cV}{\ensuremath{{\mathcal V}}\xspace}
\newcommand{\cPn}{\ensuremath{{\mathcal P}_n}\xspace}

\newcommand{\fA}{\ensuremath{\mathfrak{A}}\xspace}
\newcommand{\fB}{\ensuremath{\mathfrak{B}}\xspace}
\newcommand{\fC}{\ensuremath{\mathfrak{C}}\xspace}

\newcommand{\fa}{\ensuremath{\mathfrak{a}}\xspace}
\newcommand{\fb}{\ensuremath{\mathfrak{b}}\xspace}
\newcommand{\fc}{\ensuremath{\mathfrak{c}}\xspace}
\newcommand{\fd}{\ensuremath{\mathfrak{d}}\xspace}

\newcommand{\bA}{\ensuremath{{\bf A}}\xspace}
\newcommand{\bB}{\ensuremath{{\bf B}}\xspace}
\newcommand{\bK}{\ensuremath{{\bf K}}\xspace}
\newcommand{\bE}{\ensuremath{{\bf E}}\xspace}
\newcommand{\bN}{\ensuremath{{\bf N}}\xspace}
\newcommand{\bG}{\ensuremath{{\bf G}}\xspace}

\newcommand{\ba}{\ensuremath{{\bf a}}\xspace}
\newcommand{\bb}{\ensuremath{{\bf b}}\xspace}
\newcommand{\bc}{\ensuremath{{\bf c}}\xspace}

\newcommand{\bbE}{\ensuremath{\mathbb{E}}}
\newcommand{\bbR}{\ensuremath{\mathbb{R}}}
\newcommand{\bbQ}{\ensuremath{\mathbb{Q}}}
\newcommand{\bbP}{\ensuremath{\mathbb{P}}}
\newcommand{\bbZ}{\ensuremath{\mathbb{Z}}}
\newcommand{\bbN}{\ensuremath{\mathbb{N}}}
\newcommand{\bbNn}{\ensuremath{\mathbb{N}_0}}

\newcommand{\bbRnp}{\ensuremath{\bbR_{\geq 0}}}
\newcommand{\bbQnp}{\ensuremath{\bbQ_{\geq}}}
\newcommand{\bbZnp}{\ensuremath{\bbZ_{\geq}}}

\newcommand{\bbRsp}{\ensuremath{\bbR_>}}
\newcommand{\bbQsp}{\ensuremath{\bbQ_>}}
\newcommand{\bbZsp}{\ensuremath{\bbZ_>}}

\newcommand{\cp}{\textsf{P}\xspace}
\newcommand{\cnp}{\textsf{NP}\xspace}

\newcommand{\RR}{\mathbb{R}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\GG}{\mathbb{G}}
\newcommand{\GN}{\mathbb{G}_n}

\newcommand{\rb}{\right\}\xspace}
\newcommand{\lb}{\left\{\xspace}
\newcommand{\lbr}{\left(\xspace} 
\newcommand{\rbr}{\right)\xspace}
\newcommand{\ndelta}{\ensuremath{\overline{\delta}}}

\newcommand{\oms}{\{\!\!\{}
\newcommand{\cms}{\}\!\!\}}


\newcommand{\trans}{^T}
\renewcommand{\vec}[1]{\mathbf{#1}}

\newcommand{\wl}{-\textsf{WL}\xspace}
\newcommand{\kwl}{-\textsf{WL}\xspace}
\newcommand{\fkwl}{-\textsf{FWL}\xspace}
\newcommand{\deltakwl}{--\textsf{WL}\xspace}
\newcommand{\kwlm}{k\textrm{-}\textsf{WL}\xspace}
\newcommand{\fkwlm}{k\textrm{-}\textsf{FWL}\xspace}
\newcommand{\deltakwlm}{\delta\textrm{-}k\textrm{-}\textsf{WL}\xspace}
\newcommand{\localkwl}{--\textsf{LWL}\xspace}
\newcommand{\pluskwl}{--\textsf{LWL}\xspace}
\newcommand{\localkwlm}{\delta\textrm{-}k\textrm{-}LWL\xspace}
\newcommand{\pluskwlm}{\delta\textrm{-}k\textrm{-}\textsf{LWL}^+\xspace}
\newcommand{\deltakwln}{--\textsf{GNN}\xspace}
\newcommand{\localkwln}{--\textsf{LGNN}\xspace}
\newcommand{\kwln}{-\textsf{WL-GNN}\xspace}
\newcommand{\kgnn}{\textrm{-}\textsf{GNN}\xspace}
\newcommand{\kign}{\textrm{-}\textsf{IGN}\xspace}
\newcommand{\gnn}{\textsf{GNN}\xspace}
\newcommand{\mpnn}{\textsf{MPNN}\xspace}
\newcommand{\shp}{\textsf{SP}\xspace}
\newcommand{\gr}{\textsf{GR}\xspace}
\newcommand{\wloa}{\textsf{WLOA}\xspace}
\newcommand{\gin}{\textsf{GIN}\xspace}
\newcommand{\gine}{\textsf{GINE}\xspace}
\newcommand{\gineps}{\textsf{GIN-}\xspace}
\newcommand{\gineeps}{\textsf{GINE-}\xspace}
\newcommand{\UNR}{\textsf{UNR}\,}
\newcommand{\locstab}{\ell_\infty^\delta}
\newcommand{\localwl}{--LWL\xspace}
\newtheorem{claim}[theorem]{Claim}
\newcommand{\deltaunr}{\delta\text{-}\UNR\xspace}
\newcommand{\localunr}{\textsf{L}\text{-}\UNR\xspace}
\newcommand{\plusunr}{\textsf{L}^{\!+}\text{-}\UNR\xspace}

\newcommand{\highlight}[1]{\par\noindent
	\colorbox{gray!30}{\parbox{\dimexpr\linewidth-2\fboxsep\relax}{#1
		}}}

\usetikzlibrary{hobby,arrows,decorations.pathmorphing,backgrounds,positioning,fit,petri,calc}
\usepackage[framemethod=tikz]{mdframed}

\definecolor{mycolor}{rgb}{0.122, 0.435, 0.698}

\newmdenv[innerlinewidth=0.5pt, roundcorner=4pt,linecolor=mycolor,innerleftmargin=6pt,
innerrightmargin=6pt,innertopmargin=6pt,innerbottommargin=6pt]{mybox}


\title{Weisfeiler and Leman go sparse: Towards scalable higher-order graph embeddings}

\author{Christopher Morris\thanks{CERC in Data Science for Real-Time Decision-Making, Polytechnique Montr{\'e}al} \And Gaurav Rattan\thanks{Department of Computer Science, RWTH Aachen University} \And Petra Mutzel\thanks{Department of Computer Science, University of Bonn}
}

\includeonly{appendix}
\begin{document}

\maketitle

\begin{abstract}
Graph kernels based on the -dimensional Weisfeiler-Leman algorithm and corresponding neural architectures recently emerged as powerful tools for (supervised) learning with graphs. However, due to the purely local nature of the algorithms, they might miss essential patterns in the given data and can only handle binary relations. The -dimensional Weisfeiler-Leman algorithm addresses this by considering -tuples, defined over the set of vertices, and defines a suitable notion of adjacency between these vertex tuples. Hence, it accounts for the higher-order interactions between vertices. However, it does not scale and may suffer from overfitting when used in a machine learning setting. Hence, it remains an important open problem to design WL-based graph learning methods that are simultaneously expressive, scalable, and non-overfitting. Here, we propose \new{local} variants and corresponding neural architectures, which consider a subset of the original neighborhood, making them more scalable, and less prone to overfitting. The expressive power of (one of) our algorithms is strictly higher than the original algorithm, in terms of ability to distinguish non-isomorphic graphs. Our experimental study confirms that the local algorithms, both kernel and neural architectures, lead to vastly reduced computation times, and prevent overfitting. The kernel version establishes a new state-of-the-art for graph classification on a wide range of benchmark datasets, while the neural version shows promising performance on large-scale molecular regression tasks.
\end{abstract}

\section{Introduction}
Graph-structured data is ubiquitous across application domains ranging from chemo- and bioinformatics~\cite{Barabasi2004,Sto+2020} to image~\cite{Sim+2017} and social network analysis~\cite{Eas+2010}. To develop successful machine learning models in these domains, we need techniques that can exploit the rich information inherent in the graph structure, as well as the feature information contained within nodes and edges. In recent years, numerous approaches have been proposed for machine learning with graphs---most notably, approaches based on \new{graph kernels}~\cite{Kri+2019} or using \new{graph neural networks} (GNNs)~\cite{Cha+2020,Gil+2017,Gro+2020}. Here, graph kernels based on the \new{-dimensional Weisfeiler-Leman algorithm} (\wl)~\cite{Gro2017,Wei+1968}, and corresponding GNNs~\cite{Mor+2019,Xu+2018b} have recently advanced the state-of-the-art in supervised node and graph learning. Since the \wl operates via simple neighborhood aggregation, the purely local nature of these approaches can miss important patterns in the given data. Moreover, they are only applicable to binary structures, and therefore cannot deal with general -ary structures, e.g., hypergraphs~\cite{Zho+2006} or subgraphs, in a straight-forward way. A provably more powerful algorithm (for graph isomorphism testing) is the \emph{-dimensional Weisfeiler-Leman algorithm} (\kwl)~\cite{Cai+1992,Gro2017,Mar+2019}. The algorithm can capture more global, higher-order patterns by iteratively computing a coloring (or discrete labeling) for -tuples, instead of single vertices, based on an appropriately defined notion of adjacency between two -tuples. However, it fixes the cardinality of this neighborhood to , where  denotes the number of vertices of a given graph. Hence, the running time of each iteration does not take the \emph{sparsity} of a given graph into account. Further, new neural architectures~\cite{Mar+2019,Mar+2019b} that possess the same power as the \kwl in terms of separating non-isomorphic graphs suffer from the same drawbacks, i.e., they have to resort to dense matrix multiplications. Moreover, when used in a machine learning setting with real-world graphs, the \kwl may capture the isomorphism type, which is the complete structural information inherent in a graph, after only a couple of iterations, which may lead to overfitting, see~\cite{Mor+2017}, and the experimental section of the present work.

\xhdr{Present work} To address this, we propose a \new{local} version of the \kwl, the \new{local --dimensional Weisfeiler-Leman algorithm} (\localkwl), which considers a subset of the original neighborhood in each iteration. The cardinality of the \new{local neighborhood} depends on the sparsity of the graph, i.e., the degrees of the vertices of a given -tuple. We theoretically analyze the strength of a variant of our local algorithm and prove that it is strictly more powerful in distinguishing non-isomorphic graphs compared to the \kwl. Moreover, we devise a hierarchy of pairs of non-isomorphic graphs that a variant of the \localkwl can separate while the \kwl cannot. On the neural side, we devise a higher-order graph neural network architecture, the \localkwln, and show that it has the same expressive power as the \localkwl. Moreover, we connect it to recent advancements in learning theory for GNNs~\cite{Gar+2020}, which show that the \localkwl architecture has better generalization abilities compared to dense architectures based on the \kwl. See~\cref{overview} for an overview of the proposed algorithms.

Experimentally, we apply the discrete algorithms (or kernels) and the (local) neural architectures to supervised graph learning, and verify that both are several orders of magnitude faster than the global, discrete algorithms or dense, neural architectures, and prevent overfitting. The discrete algorithms establish a new state-of-the-art for graph classification on a wide range of small- and medium-scale classical datasets. The neural version shows promising performance on large-scale molecular regression tasks.

\xhdr{Related work}
In the following, we review related work from graph kernels and GNNs. We refer to~\cref{exprel} for an in-depth discussion of related work, as well as a discussion of theoretical results for the \kwl.

Historically, kernel methods---which implicitly or explicitly map graphs to elements of a Hilbert space---have been the dominant approach for supervised learning on graphs. Important early work in this area includes kernels based on random-walks~\cite{Gae+2003,Kas+2003,Kri+2017b}, shortest paths~\cite{Bor+2005}, and kernels based on the \wl~\cite{She+2011}. \citeauthor{Mor+2017}~\cite{Mor+2017} devised a local, set-based variant of the \kwl. However, the approach is (provably) weaker than the tuple-based algorithm, and they do not prove convergence to the original algorithm. For a thorough survey of graph kernels, see~\cite{Kri+2019}. Recently, graph neural networks (GNNs)~\cite{Gil+2017,Sca+2009} emerged as an alternative to graph kernels. Notable instances of this architecture include, e.g.,~\cite{Duv+2015,Ham+2017,Vel+2018}, and the spectral approaches proposed in, e.g.,~\cite{Bru+2014,Def+2015,Kip+2017,Mon+2017}---all of which descend from early work in~\cite{Kir+1995,Mer+2005,Spe+1997,Sca+2009}. A survey of recent advancements in GNN techniques can be found, e.g., in~\cite{Cha+2020,Wu+2019,Zho+2018}. Recently, connections to Weisfeiler-Leman type algorithms have been shown~\cite{Bar+2020,Che+2019,Gee+2020a,Gee+2020b,Mae+2019,Mar+2019,Mor+2019,Xu+2018b}. Specifically, the authors of~\cite{Mor+2019,Xu+2018b} showed that the expressive power of any possible GNN architecture is limited by the \wl in terms of distinguishing non-isomorphic graphs. \citeauthor{Mor+2019}~\cite{Mor+2019} introduced \emph{-dimensional GNNs} (\kgnn) which rely on a message-passing scheme between subgraphs of cardinality . Similar to~\cite{Mor+2017}, the paper employed a local, set-based (neural) variant of the \kwl, which is (provably) weaker than the variant considered here. Later, this was refined in~\cite{Mar+2019} by introducing \emph{-order invariant graph networks} (\kign), based on \citeauthor{Mar+2019b}~\cite{Mar+2019b}, which are equivalent to the folklore variant of the \kwl~\cite{Gee+2020b,Gro2017} in terms of distinguishing non-isomorphic graphs. However, \kign may not scale since they rely on dense linear algebra routines. \citeauthor{Che+2019}~\cite{Che+2019} connect the theory of universal approximation of permutation-invariant functions and the graph isomorphism viewpoint and introduce a variation of the -\textsf{WL}, which is more powerful than the former. Our comprehensive treatment of higher-order, sparse, (graph) neural networks for arbitrary  subsumes all of the algorithms and neural architectures mentioned above.


\begin{figure}[t]
	\begin{center}
		\tikzset{
			treenode/.style = {shape=rectangle, rounded corners,
				draw, align=center,
				minimum width=50pt,
			}
		}
		\trimbox{0pt 20pt 0pt 10pt}{
		\begin{tikzpicture}[scale=1.4,font=\footnotesize,  >=stealth', thick,sibling distance=15mm, level distance=30pt,minimum size=18pt,sibling distance=50pt]
		\node(a) [treenode,fill=green!40] {} 
		child { node(b) [treenode,fill=red!20] {}
			child { node(d) [treenode,fill=red!20] {}
				edge from parent node [left] {\large } [->] }
			edge from parent node [left,->,label={[shift={(-0.3,-0.5)}]\large }]{} [<->] }
		child { node(c) [treenode,fill=green!40] {-}
		edge from parent node [left,->,label={[shift={(1.0,-0.4)}]\large }]{} [->] };
		
		\node(e) at (-3.0,0) [treenode,fill=green!40] {};
		\node(f) at (-3.0,-1.05) [treenode,fill=red!60] {};
		\node(g) at (-3.0,-2.1) [treenode,fill=red!60] {};
		\node(h) at (.88,-2.11) [treenode,fill=red!20,grow=east] {(-};
		\node(i) at (3.0,-1.05) [treenode,fill=green!40,grow=east] {-};
		{[shift={(1.0,0.3)}]Label}]
		
		\draw[<->] (i) -- (c) node[midway,label={[shift={(0.0,-.4)}]\large }]{};
		\draw[<->] (a) -- (e) node[midway,label={[shift={(0.0,-.4)}]\large }]{};
		\draw[<->] (b) -- (f) node[midway,above]{\large };
		\draw[<->] (d) -- (g) node[midway,above]{\large };
		\draw[<-] (d) -- (h) node[midway,label={[shift={(0.0,-.5)}]\large }]{};
		
		\draw[->, thick] (-4.2,-2.54) -- (-4.2,0.18) node [midway,fill=white] {\rotatebox{90}{\textbf{Power}}};		
		
		\begin{pgfonlayer}{background}
		
		\node[fill=gray!10, inner sep=4.0mm,label=below:,fit=(b) (d) (g)] {};
		\node[fill=gray!10, inner sep=4.0mm,label=below:,fit=(d) (h)] {};

		\end{pgfonlayer}
		\end{tikzpicture}}
	\end{center}
	\caption{Overview of the power of proposed algorithms and neural architectures. The green and dark red nodes represent algorithms proposed in the present work. The grey region groups dense algorithms and neural architectures. \\
		---Follows directly from the proof of~\cref{equal}.  (, ): algorithm  is  more powerful (strictly more powerful, equally powerful) than , 	---Follows by definition, strictness open.}\label{overview}
\end{figure}

\section{Preliminaries}

We briefly describe the Weisfeiler-Leman algorithm and, along the way, introduce our notation, see~\cref{prelim} for expanded preliminaries. As usual, let  for , and let  denote a multiset. We also assume elementary definitions from graph theory (such as graphs, directed graphs, vertices, edges, neighbors,
trees, and so on). The vertex and the edge set of a graph  are denoted by  and  respectively. The \new{neighborhood} of  in  is denoted by . Moreover, its complement . We say that two graphs  and  are \new{isomorphic} () if there exists an adjacency preserving bijection , i.e.,  is in  if and only if 
 is in , call  an \emph{isomorphism} from  to . If the graphs have vertex/edges labels, the isomorphism is additionally required to match these labels. A \new{rooted tree} is a tree with a designated vertex called \new{root} in which the edges are directed in such a way that they point away from the root. Let  be a vertex in a directed tree then we call its out-neighbors \new{children} with parent . Given a -tuple of vertices , let  denote the subgraph induced on the set , where, the vertex  is labeled with , for  in . 


\xhdr{Vertex refinement algorithms}\label{vr}
For a fixed positive integer , let  denote the set of -tuples of vertices of . A \new{coloring} of  is a mapping , i.e., we assign a number (or color) to every tuple in . The \new{initial coloring}  of  is specified by the isomorphism types of the tuples, i.e., two tuples  and  in  get a common color iff the mapping  induces an isomorphism between the labeled subgraphs  and . A \new{color class} corresponding to a color  is the set of all tuples colored , i.e., the set . For  in , let  be the -tuple obtained by replacing the  component of  with the vertex . That is, . If  for some  in , call  a -\new{neighbor} of  (and vice-versa). The \emph{neighborhood} of  is then defined as the set of all tuples  such that  for some  in  and  in . The \emph{refinement} of a coloring , denoted by , is a coloring  defined as follows. 
For each  in , collect the colors of the -neighbors of  as a multiset .
Then, for a tuple , define , where  is the -tuple . For consistency, the strings  thus obtained are lexicographically sorted and renamed as integers. Observe that the new color  of  is solely dictated by the color histogram of its neighborhood and the previous color of . In general, a different mapping  could be used, depending on the neighborhood information that we would like to aggregate. 

\xhdr{The -dim. Weisfeiler-Leman} For , the \kwl computes a coloring  of a given graph , as follows.\footnote{We define the -\textsf{WL} in the next subsection.} To begin with, the initial coloring  is computed. Then, starting with , successive refinements  are computed until convergence. That is,

where 

The successive refinement steps are also called \new{rounds} or \new{iterations}. Since the disjoint union of the color classes form a partition of , there must exist a finite  such that . In the end, the \kwl outputs  as the \emph{stable coloring} . The \kwl \new{distinguishes} two graphs  and  if, upon running the \kwl on their disjoint union , there exists a color  in  in the stable coloring such that the corresponding color class  satisfies
, i.e., there exist an unequal number of -colored tuples in  and . Hence, two graphs distinguished by the \kwl must be non-isomorphic. See \cref{wl_app} for its relation to the folklore \kwl.

\xhdr{The --dim. Weisfeiler-Leman} Let  be a -{neighbor} of . Call  a \new{local} -neighbor of  if  is adjacent to the replaced vertex . Otherwise, call  a \new{global} -neighbor of . For tuples  and  in , let the function  evaluate to  or , depending on whether  is a local or a global neighbor, respectively, of . The --dimensional \new{Weisfeiler-Leman algorithm}, denoted by \deltakwl, is a variant of the classic \kwl which \emph{differentiates} between the local and the global neighbors during neighborhood aggregation \cite{Mal2014}. Formally, the \deltakwl algorithm refines a coloring  (obtained after  rounds) via the aggregation function 

instead of the \kwl aggregation specified by~\cref{mi}. We define the -\textsf{WL} to be the -1-\textsf{WL}, which is commonly known as color refinement or naive vertex classification.

\xhdr{Comparison of -\textsf{WL} variants} Let  and  denote two vertex refinement algorithms, we write  if  distinguishes between all non-isomorphic pairs  does, and  if both directions hold. The corresponding strict relation is denoted by . The following result shows that the \deltakwl is strictly more powerful than the \kwl for  (see~\cref{proofrel1} for the proof).  
\begin{proposition}\label{rel1} 
For , the following holds: 	

\end{proposition}
\section{Local --dimensional Weisfeiler-Leman algorithm}\label{lwl}

In this section, we define the new \new{local --dimensional Weisfeiler-Leman algorithm} (\localkwl). This variant of the \deltakwl considers only local neighbors during the neighborhood aggregation process, and discards any information about the global neighbors. Formally, the \localkwl algorithm refines a coloring  (obtained after  rounds) via the aggregation function, 
		
instead of~\cref{mid}, hence considering only the local -neighbors of the tuple  in each iteration. The indicator function  used in~\cref{mid} is trivially equal to  here, and is thus omitted. The coloring function for the \localkwl is then defined by
 

We also define the \pluskwl, a minor variation of the \localkwl. Later, we will show that the \pluskwl is equivalent in power to the \deltakwl (\cref{loco}). 
Formally, the \pluskwl algorithm refines a coloring  (obtained after  rounds) via the aggregation function, 

instead of \localkwl aggregation defined in~\cref{eqnmidd}. 
Here, the function

where  denotes that  is -neighbor of , for  in . Essentially,  counts the number of -neighbors (local or global) of  which have the same color as  under the coloring  (i.e., after  rounds). For a fixed ,
the function  is uniform over the set , where  is a color class obtained after  iterations of the \pluskwl and  denotes the set of -neighbors of . 
Note that after the stable partition has been reached  will not change anymore. Intuitively, this variant captures local and to some extent global information, while still taking the sparsity of the underlying graph into account. Moreover, observe that each iteration of the \pluskwl has the same asymptotic running time as an iteration of the \localkwl, and that the information of the  function is already implicitly contained in~\cref{mid}.

The following theorem shows that the local variant \pluskwl is at least as powerful as the \deltakwl when restricted to the class of connected graphs. The possibly slower convergence leads to advantages in a machine learning setting, see~\cref{neural,exp}, and also~\cref{prac_app} for a discussion of practicality, running times, and remaining challenges.
\begin{theorem}\label{loco}
	For the class of connected graphs, the following holds for all :
	
\end{theorem}	
Along with~\cref{rel1}, this establishes the superiority of the \pluskwl over the \kwl. 
\begin{corollary}\label{cloco} For the class of connected graphs, the following holds for all :	
\end{corollary}
In fact, the proof of \cref{rel1} shows that the infinite family of graphs  witnessing the strictness condition can even be distinguished by the \localkwl,
for each corresponding . We note here that the restriction to connected graphs can easily be circumvented by adding a specially marked vertex, which is connected to every other vertex in the graph.

\xhdr{Kernels based on vertex refinement algorithms}
After running the \localkwl (and the other vertex refinements algorithms), the concatenation of the histogram of colors in each iteration can be used as a feature vector in a kernel computation. 
Specifically, in the histogram for every color  in  there is an entry containing the number of nodes or -tuples that are colored with .

\xhdr{Local converges to global: proof of~\cref{loco}} The main technique behind the proof is to construct tree-representations of the colors assigned by the \kwl (or its variants). Given a graph , a tuple , and an integer , the \emph{unrolling tree} of the graph  \new{at  of depth } is a rooted directed tree  (with vertex and edge labels) which encodes the color assigned by \kwl to the tuple  after  rounds, see~\cref{locodes} for a formal definition and~\cref{rolling_app} for an illustration. The usefulness of these tree representations is established by the following lemma. Formally, let  and  be two -vertex-tuples in .
\begin{lemma}\label{encwl}
	The colors of  and  after  rounds of \kwl are identical if and only if the unrolling tree  is isomorphic to the unrolling tree .  
\end{lemma}
For different \kwl variants, the construction of these unrollings are slightly different, since an unrolling tree needs to faithfully represent the corresponding aggregation process for generating new colors. For the variants \deltakwl, \localkwl, and \pluskwl, we define respective unrolling trees , , and  along with analogous lemmas, as above, stating their correctness/usefulness. Finally, we show that for \emph{connected graphs}, the  unrolling trees (of sufficiently large depth) at two tuples  and  are identical only if the respective \pluskwl unrolling trees (of sufficiently larger depth) are identical, as shown in the following lemma. 
\begin{lemma}\label{pluseqtodelta}
	Let  be a connected graph, and let  and  in . If the stable colorings of  and  under \pluskwl are identical, 
	then the stable colorings of  and  under \deltakwl are also identical. 
\end{lemma}
Hence, the local algorithm \pluskwl is at least as powerful as the global \deltakwl, for connected graphs, i.e., . 
The exact details and parameters of this proof can be found in the Appendix.

\section{Higher-order neural architectures}\label{neural}
Although the discrete kernels defined in the previous section are quite powerful, they are limited due to their fixed feature construction scheme, hence suffering from poor adaption to the learning task at hand and the inability to handle continuous node and edge labels in a meaningful way. Moreover, they often result in high-dimensional embeddings forcing one to resort to non-scalable, kernelized optimization procedures. 
This motivates our definition of a new neural architecture, called \new{local --}\textsf{GNN} (\localkwln).
Given a labeled graph , let each tuple  in  be annotated with an initial feature  determined by its isomorphism type. 
In each layer ,  we compute a new feature  as 

in   for a tuple , where  and  are learnable parameter matrices from .\footnote{For clarity of presentation we omit biases.}. Moreover,  and the permutation-invariant  can be arbitrary (permutation-invariant) differentiable functions, responsible for merging and aggregating the relevant feature information, respectively. Initially, we set  to a one-hot encoding of the (labeled) isomorphism type of . Note that we can naturally handle discrete node and edge labels as well as directed graphs, see~\cref{contfeat} on how to deal with continuous information. The following result demonstrates the expressive power of the \deltakwln, in terms of distinguishing non-isomorphic graphs. 
\begin{theorem}\label{equal}
	Let  be a labeled graph. Then for all \mbox{} there exists a sequence of weights  such that 
	
	Hence, for all graphs, the following holds for all :
	
\end{theorem}     
Moreover, the \deltakwln inherits the main strength of the \localkwl, i.e., it can be implemented using sparse matrix multiplication. Note that it is not possible to come up with an architecture, i.e., instantiations of  and  , such that it becomes more powerful than the \localkwl, see~\cite{Mor+2019}. However, all results from the previous section can be lifted to the neural setting. That is, one can derive neural architectures based on the \pluskwl, \deltakwl, and \kwl, called \localkwln\hspace{-3pt}, \deltakwln, and \kwln, respectively, and prove results analogous to~\cref{equal}.

\xhdr{Incorporating continous information}\label{contfeat}
Since many real-world graphs, e.g., molecules, have continuous features (real-valued vectors) attached to vertices and edges, using a one-hot encoding of the (labeled) isomorphism type is not a sensible choice. Let  be a function such that each vertex  is annotated with a feature  in , and let  be a -tuple of vertices. Then we can compute an inital feature

for the tuple . Here,  is an arbitrary differentiable, parameterized function, e.g., a multi-layer perceptron or a standard GNN aggregation function, that computes a joint representation of the  node features . Moreover, it is also straightforward to incorporate the labeled isomorphism type and continuous edge label information. We further explore this in the experimental section.


\xhdr{Generalization abilities of the  neural architecture}
\citeauthor{Gar+2020}~\cite{Gar+2020}, studied the generalization abilities of a standard GNN architecture for binary classification using a margin loss. Under mild conditions, they bounded the empirical Rademacher complexity as

where  is the maximum degree of the employed graphs,  is the number of components of the node features,  is the number of layers, and  is a parameter of the loss function. It is straightforward to transfer the above bound to the higher-order (local) layer from above. Hence, this shows that local, sparsity-aware, higher-order variants, e.g., \localkwln, exhibit a smaller generalization error compared to dense, global variants like the \kwln.


\section{Practicality, barriers ahead, and possible road maps}\label{prac_app}

As~\cref{loco} shows, the \pluskwl and its corresponding neural architecture, the \localkwln, have the same power in distinguishing non-isomorphic graphs as \deltakwl. Although for dense graphs, the local algorithms will have the same running time, for sparse graphs, the running time for each iteration can be upper-bounded by , where  denotes the maximum or average degree of the graph. Hence, the local algorithm takes the sparsity of the underlying graph into account, resulting in improved computation times compared to the non-local \deltakwl and the \kwl (for the same number of iterations). These observations also translate into practice, see~\cref{exp}. The same arguments can be used in favor of the \localkwl and \localkwln, which lead to even sparser algorithms. 

\xhdr{Obstacles} The biggest obstacle in applying the algorithms to truly large graphs is the fact that the algorithm considers all possible -tuples leading to a lower bound on the running time of . Lifting the results to the folklore \kwl, e.g.,~\cite{Mar+2019}, only ``shaves off one dimension''. Moreover, applying higher-order algorithms for large  might lead to overfitting issues, see also~\cref{exp}. 

\xhdr{Possible solutions} Recent sampling-based approaches for graph kernels or GNNs, see, e.g.,~\cite{Che+2018,Che+2018a,Ham+2017,Hua+2018,Mor+2017} address the dependence on , while appropriate pooling methods along the lines of~\cref{encode} address the overfitting issue. Finally, new directions from the theory community, e.g.,~\cite{Gro+2020a} paint further directions, which might result in more scalable algorithms.


\section{Experimental evaluation}\label{exp}

Our intention here is to investigate the benefits of the local, sparse algorithms, both kernel and neural architectures, compared to the global, dense algorithms, and standard kernel and GNN baselines.
More precisely, we address the following questions:\\
\xhdr{Q1} Do the local algorithms, both kernel and neural architectures, lead to improved classification and regression scores on real-world benchmark datasets compared to global, dense algorithms and standard baselines? \\
\xhdr{Q2}  Does the \pluskwl lead to improved classification accuracies compared to the  \localkwl? Does it lead to higher computation times?\\
\xhdr{Q3}  Do the local algorithms prevent overfitting to the training set?\\
\xhdr{Q4} How much do the local algorithms speed up the computation time compared to the non-local algorithms or dense neural architectures?

The source code of all methods and evaluation procedures is available at \url{https://www.github.com/chrsmrrs/sparsewl}. 

\xhdr{Datasets} To evaluate kernels, we use the following, well-known, small-scale datasets: \textsc{Enzymes}~\cite{Sch+2004,Bor+2005}, \textsc{IMDB-Binary}, \textsc{IMDB-Multi}~\cite{Yan+2015a}, \textsc{NCI1}, \textsc{NCI109}~\cite{Wal+2008}, \textsc{PTC\_FM}~\cite{Hel+2001}, \textsc{Proteins}~\cite{Dob+2003,Bor+2005}, and \textsc{Reddit-Binary}~\cite{Yan+2015a}. To show that our kernels also scale to larger datasets, we additionally used the mid-scale datasets: \textsc{Yeast}, \textsc{YeastH}, \textsc{UACC257}, \textsc{UACC257H}, \textsc{OVCAR-8}, \textsc{OVCAR-8H}~\cite{Yan+2008}. For the neural architectures, we used the large-scale molecular regression datasets \textsc{Zinc}~\cite{Dwi+2020,Jin+2018a} and \textsc{Alchemy}~\cite{Che+2020}.
To further compare to the (hierarchical) \kgnn~\cite{Mor+2019} and \kign~\cite{Mar+2019}, and show the benefits of our architecture in presence of continuous features, we used the \textsc{QM9}~\cite{Ram+2014,Wu+2018} regression dataset.\footnote{We opted for comparing on the \textsc{QM9} dataset to ensure a fair comparison concerning hyperparameter selection.}  All datasets can be obtained from~\url{http://www.graphlearning.io}~\cite{Mor+2020}. See~\cref{datasets} for further details.

\xhdr{Kernels} We implemented the \localkwl, \pluskwl, \deltakwl, and  \kwl kernel for  in . We compare our kernels to the Weisfeiler-Leman subtree kernel (\wl)~\cite{She+2011}, the Weisfeiler-Leman Optimal Assignment kernel (\wloa)~\cite{Kri+2016}, the graphlet kernel (\gr)~\cite{She+2009}, and the shortest-path kernel~\cite{Bor+2005} (\shp). All kernels were (re-)implemented in \CC[11]. For the graphlet kernel, we counted (labeled) connected subgraphs of size three. We followed the evaluation guidelines outlined in~\cite{Mor+2020}. We also provide precomputed Gram matrices for easier reproducability.

\xhdr{Neural architectures} We used the \gin and \gineps architecture~\cite{Xu+2018b} as neural baselines. For data with (continuous) edge features, we used a -layer MLP to map them to the same number of components as the node features and combined them using summation (\gine and \gineeps). For the evaluation of the neural architectures of~\cref{neural}, \localkwln, \deltakwln, and \kwln, we implemented them using \textsc{PyTorch Geometric}~\cite{Fey+2019}, using a  Python-wrapped \CC[11] preprocessing routine to compute the computational graphs for the higher-order GNNs.\footnote{We opted for not  implementing the \localkwln as it would involve precomputing .} We used the \gineps layer to express  and  of~\cref{gnngeneral}. 

See~\cref{protocol} for a detailed description of all evaluation protocols and hyperparameter selection routines. 
\begin{table}[t]\centering			
	\caption{Classification accuracies in percent and standard deviations,  \textsc{OOT}--- Computation did not finish within one day, \textsc{OOM}--- Out of memory.}
	\label{t2}	
	\resizebox{1.0\textwidth}{!}{ 	\renewcommand{\arraystretch}{1.05}
		\begin{tabular}{@{}c <{\enspace}@{}lcccccccc@{}}	\toprule
			& \multirow{3}{*}{\vspace*{4pt}\textbf{Method}}&\multicolumn{8}{c}{\textbf{Dataset}}\\\cmidrule{3-10}
			& & {\textsc{Enzymes}}         &  {\textsc{IMDB-Binary}}      & {\textsc{IMDB-Multi}}           & {\textsc{NCI1}}       & {\textsc{NCI109}}           & 
			{\textsc{PTC\_FM}}         & {\textsc{Proteins}}         &
			{\textsc{Reddit-Binary}  } \\	\toprule
			\multirow{4}{*}{\rotatebox{90}{\hspace*{-3pt}Baseline}} & \gr     &  29.7   \scriptsize	  & 58.9	   \scriptsize 	  &    39.0   \scriptsize	 & 66.1  \scriptsize	     & 66.3    \scriptsize	      &    61.3      \scriptsize	   &    71.2      \scriptsize	     &  60.0    \scriptsize	     \\ 
			& \shp  & 40.7  \scriptsize	   &  58.5   \scriptsize	  &  39.4   \scriptsize	     & 74.0   \scriptsize	     & 73.0  \scriptsize	     & 61.3  \scriptsize	 & 75.6   \scriptsize	 & 84.6 \scriptsize   	 \\ 
			& \textsf{-WL}         & 50.7   \scriptsize       & 72.5   \scriptsize	   &  50.0      \scriptsize 	     & 84.2  \scriptsize	  & 84.3   \scriptsize	  & \bf{62.6} \scriptsize	   &  72.6 \scriptsize	      & 72.8   \scriptsize	      \\ 						
			& \textsf{WLOA}            &    56.8    \scriptsize       &   72.7   \scriptsize	   &   50.1       \scriptsize	     &   84.9      \scriptsize	  &  85.2    \scriptsize	  &  61.8   \scriptsize	   &     73.2       \scriptsize	      &    88.1     \scriptsize	      \\ 		
			\cmidrule{2-10}		
			\multirow{2}{*}{\rotatebox{90}{\hspace*{-3pt}Neural}} & \textsf{Gin-} & 38.8  \scriptsize  &    72.7  \scriptsize  & 49.9 \scriptsize  & 78.5 \scriptsize  & 76.7  \scriptsize  & 58.2   \scriptsize  &  71.3 \scriptsize  & 89.8  \scriptsize 
			\\ 
			& \textsf{Gin-} & 39.4 \scriptsize  &   72.9        \scriptsize	  & 49.6    \scriptsize & 78.6  \scriptsize  & 77.0  \scriptsize  &  57.7   \scriptsize  &  71.1   \scriptsize  & 90.3  \scriptsize  \\ 
			\cmidrule{2-10}	
			\multirow{4}{*}{\rotatebox{90}{Global}} 	&
			\textsf{-WL}        &   36.7   \scriptsize   &  68.2  \scriptsize  &  48.1  \scriptsize    &       67.1 \scriptsize   &  67.5  \scriptsize   & 62.3 \scriptsize   &  75.0   \scriptsize   & \textsc{Oom} \\
			&  \textsf{-WL}        	&    42.3 \scriptsize & 67.8 \scriptsize  & 47.0   \scriptsize  &\textsc{Oot} &\textsc{Oot} & 61.5    \scriptsize  &\textsc{Oom} & \textsc{Oom} \\            
			\cmidrule{2-10}			
			& \textsf{--WL}  	& 37.5  \scriptsize    &  68.1   \scriptsize &   47.9     \scriptsize   &      67.0    \scriptsize   &  67.2   \scriptsize  &   61.9  \scriptsize  & 75.0 \scriptsize   & \textsc{OOM} \\  
			& \textsf{--WL}          		& 43.0  \scriptsize & 67.5 \scriptsize  &  47.3 \scriptsize  &\textsc{Oot} &\textsc{Oot} &   61.2  \scriptsize  &\textsc{Oom} & \textsc{Oom} \\                                                   
			\cmidrule{2-10}		
			\multirow{4}{*}{\rotatebox{90}{Local}}    		
			& \textsf{--LWL}         	& 56.6    \scriptsize  &  73.3 \scriptsize   & 50.2 \scriptsize  & 84.7 \scriptsize  &   84.2   \scriptsize   & 60.3   \scriptsize  &   75.1  \scriptsize  & 89.7 \scriptsize          \\      		
			& \textsf{--LWL}         	&  52.9  \scriptsize  &  75.7 \scriptsize  &  62.5   \scriptsize  &   \bf{91.4}   \scriptsize   &  \bf{89.3}        \scriptsize  &  \bf{62.6}  \scriptsize  &   \textbf{79.3}    \scriptsize &   \bf{91.1}    \scriptsize  \\     
			& \textsf{--LWL} 	&  \bf{57.6}   \scriptsize  & 72.8 \scriptsize  &   49.3     \scriptsize   &  83.4   \scriptsize  &  82.4   \scriptsize   &    61.3    \scriptsize  & \textsc{Oom} & \textsc{Oom} \\ 	
			& \textsf{--LWL}         	& 56.8   \scriptsize  &  \bf{76.2}     \scriptsize  & \bf{64.2}       \scriptsize   &  82.7   \scriptsize  0.5 &  81.9   \scriptsize  & 61.3 \scriptsize  & \textsc{Oom}& \textsc{Oom} \\    
			\bottomrule
	\end{tabular}}
\end{table}	
	
\begin{table}
	\begin{center}
		\begin{subfigure}[c,top]{0.48\textwidth}	\centering
			\label{t2n}	
\caption{\label{t3_short} Training versus test accuracy of local and global kernels for a subset of the datasets.
}
\resizebox{0.95\textwidth}{!}{ 	\renewcommand{\arraystretch}{1.05}
\begin{tabular}{@{}c <{\enspace}@{}lccc@{}}	\toprule
	& \multirow{3}{*}{\vspace*{4pt}\textbf{Set}}&\multicolumn{3}{c}{\textbf{Dataset}}\\\cmidrule{3-5}
	& & {\textsc{Enzymes}}         &  {\textsc{IMDB-Binary}}      & {\textsc{IMDB-Multi}}  \\	\toprule
	\multirow{2}{*}{\rotatebox{25}{\textsf{-2-WL}}}    		
	& Train & 91.2  & 83.8  &57.6  \\ 
	
	& Test & 37.5   &  68.1 &   47.9    \\              
	\cmidrule{2-5}
	\multirow{2}{*}{\rotatebox{25}{\textsf{--LWL}}}    		
	& Train  & 98.8  &  83.5 & 59.9   \\ 
	& Test  &  56.6  &  73.3  & 50.2   \\   
	\cmidrule{2-5}
	\multirow{2}{*}{\rotatebox{25}{\textsf{--LWL}}}    		
	& Train  & 99.5  & 95.1 &  86.5   \\ 
	& Test   	&  52.9 &  75.7 &  62.5   \\       
	\bottomrule
\end{tabular}
}
		\end{subfigure}\hspace{10pt}
		\begin{subfigure}[c,top]{0.48\textwidth}	\centering
							\caption{Mean MAE (mean std. MAE, logMAE) on large-scale (multi-target) molecular regression tasks.\label{neural_short_tt}}
						\resizebox{.85\textwidth}{!}{\renewcommand{\arraystretch}{1.05}
				\begin{tabular}{@{}c <{\enspace}@{}lcc@{}}	\toprule
					& \multirow{3}{*}{\vspace*{4pt}\textbf{Method}}&\multicolumn{2}{c}{\textbf{Dataset}}\\\cmidrule{3-4}
					&    &  {\textsc{Zinc (Full)}}     & {\textsc{alchemy (Full)}}       \\	\toprule
					\multirow{4}{*}{\rotatebox{90}{Baseline\hspace{3pt}}}
					& \gineeps  & 0.084                                                                                                                                                                                                                                     
					\scriptsize                                                                                                                                                                                                                                   & 0.103 {\scriptsize } -2.956 {\scriptsize } \\	
					\cmidrule{2-4}
					&  \textsf{-WL-GNN}    & 0.133 \scriptsize     & {0.093                                                                                                                                                                                                                                       
						\scriptsize }  {-3.394                                                                                                                                                                                                                                    
						\scriptsize }                                                                                                                                \\
					&  --\textsf{GNN}      &\textbf{0.042} {\scriptsize }   & \textbf{0.080}                                                                           {\scriptsize } -3.516 {\scriptsize } \\			
					\cmidrule{2-4}	
					\multirow{2}{*}{\rotatebox{90}{}}   & --\textsf{LGNN} &  0.045  \scriptsize  & 0.083 {\scriptsize } -3.476  {\scriptsize } \\
					\bottomrule
			\end{tabular}}\vspace{14pt}
		\end{subfigure}
	\end{center}
		\caption{Additional results for kernel and neural approaches.}\label{add_tt}
\end{table}

\xhdr{Results and discussion} In the following we answer questions \textbf {Q1} to \textbf{Q4}.

\xhdr{A1} \textit{Kernels} See~\cref{t2}. The local algorithm, for   and , severely improves the classification accuracy compared to the \kwl and the \deltakwl. For example, on the \textsc{Enzymes} dataset the --\textsf{LWL} achieves an improvement of almost \%, and the --\textsf{LWL} achieves the best accuracies over all employed kernels, improving over the -\textsf{WL} and the --\textsf{WL} by more than \%. This observation holds over all datasets. Our algorithms also perform better than neural baselines. See~\cref{t2l} in the appendix for additional results on the mid-scale datasets. However, it has to be noted that increasing  does not always result in increased accuracies. For example, on all datasets (excluding \textsc{Enzymes}), the performance of the --\textsf{LWL} is better or on par with the --\textsf{LWL}. Hence, with increasing  the local algorithm is more prone to overfitting.\\
\textit{Neural architectures} See~\cref{neural_short_tt,plot}. On the \textsc{Zinc} and \textsc{Alchemy} datasets, the --\textsf{LGNN} is on par or slightly worse than the --\textsf{GNN}. Hence, this is in contrast to the kernel variant. We assume that this is due to the  --\textsf{GNN} being more flexible than its kernel variant in weighing the importance of global and local neighbors. This is further highlighted by the worse performance of the -\textsf{WL-GNN}, which even performs worse than the (-dimensional) \gineeps on the \textsc{Zinc} dataset. On the \textsc{QM9} dataset, see~\cref{t2nn_short}, the --\textsf{LGNN} performs better than the higher-order methods from~\cite{Mar+2019,Mor+2019} while being on par with the \mpnn architecture. We note here that the \mpnn was specifically tuned to the \textsc{Qm9} dataset, which is not the case for the --\textsf{LGNN} (and the other higher-order architectures).\\
\xhdr{A2} See~\cref{t2}. The --\textsf{LWL} improves over the --\textsf{LWL} on all datasets excluding \textsc{Enzymes}. For example, on \textsc{IMDB-Multi}, \textsc{NCI1}, \textsc{NCI109}, and \textsc{Proteins} the algorithm achieves an improvement over of , respectively, achieving a new state-of-the-art. The computation times are only increased slightly, see~\cref{t1_app} in the appendix. Similar results can be observed on the mid-scale datasets, see~\cref{t2l,t1l_app} in the appendix.\\
\xhdr{A3} \textit{Kernels} As~\cref{t3_short} (\cref{t3} for all datasets) shows the --\textsf{WL} reaches slightly higher training accuracies over all datasets compared to the --\textsf{LWL}, while the testing accuracies are much lower (excluding \text{PTC\_FM} and \textsc{Proteins}). This indicates that the  --\textsf{WL} overfits on the training set. The higher test accuracies of the local algorithm are likely due to the smaller neighborhood, which promotes that the number of colors grow slower compared to the global algorithm. The \pluskwl inherits the strengths of both algorithms, i.e., achieving the overall best training accuracies while achieving state-of-the-art testing accuracies.\\
\textit{Neural architectures} See~\cref{plot}. In contrast to the kernel variants, the -\textsf{WL} and the --\textsf{WL}, the corresponding neural architectures, the -\textsf{WL-GNN} and the --\textsf{GNN}, seem less prone to overfitting. However, especially on the \textsc{Alchemy} dataset, the --\textsf{LGNN} overfits less. \\
\textbf{A4}
\textit{Kernels} See~\cref{t2lee} (\cref{t1_app,t1l_app} for all datasets). The local algorithm severely speeds up the computation time compared to the \deltakwl and the \kwl for  and . For example, on the \textsc{Enzymes} dataset the --\textsf{LWL} is over ten times faster than the --\textsf{WL}.  The improvement of the computation times can be observed across all datasets.  For some datasets, the -\textsf{WL} and the --\textsf{WL} did not finish within the given time limit or went out of memory. For example, on four out of eight datasets, the -3-\textsf{WL} is out of time or out of memory. In contrast, for the corresponding local algorithm, this happens only two out of eight times. Hence, the local algorithm is more suitable for practical applications. \\
\textit{Neural architectures} See~\cref{t2len}. The local algorithm severely speeds up the computation time of training and testing. Especially, on the \textsc{Zinc} dataset, which has larger graphs compared to the \textsc{Alchemy} dataset, the --\textsf{LGNN} achieves a computation time that is more  than two times lower compared to the --\textsf{GNN} and the -\textsf{WL-GNN}.
\begin{figure}[t]
	\begin{center}
		\begin{subfigure}[c]{0.25\textwidth}\centering
			\vspace{2pt}
			\resizebox{.95\textwidth}{!}{\renewcommand{\arraystretch}{1.05}
				\begin{tabular}{@{}c <{\enspace}@{}lc@{}}	\toprule
					&\textbf{Method}&  \textsc{QM9} \\	\toprule
					\multirow{6}{*}{\rotatebox{90}{Baseline}}
					& \gineeps   &  0.081 {\scriptsize } \\	
					& \mpnn & 0.034 {\scriptsize }   \\	
					& \textsf{--GNN} &  0.068   {\scriptsize }  \\   
					& \textsf{--GNN} &  0.088 {\scriptsize }  \\
					& \textsf{---GNN} & 0.062  {\scriptsize } \\	
					& \textsf{-IGN} &  0.046 {\scriptsize }  \\	
					\midrule
					& \textsf{--LGNN} & \textbf{0.029} {\scriptsize } \\	
					\bottomrule
			\end{tabular}}	\vspace{6pt}
					\caption{Mean std.~MAE compared to~\cite{Gil+2017,Mar+2019,Mor+2019}.}
			\label{t2nn_short}
		\end{subfigure}
		\begin{subfigure}[c]{0.32\textwidth}
			\centering
			\includegraphics[scale=0.28]{zinc}
					\caption{\textsc{Zinc}}
			\label{plot_zinc}	
		\end{subfigure}
		\begin{subfigure}[c]{0.3\textwidth}
			\centering
			\includegraphics[scale=0.28]{alchem}
						\caption{\textsc{Alchemy}}
			\label{plot_alchem}	
		\end{subfigure}
	\end{center}
	\caption{Additional results for neural architectures.}	\label{plot}
\end{figure}
\begin{table}
	\begin{center}
		\begin{subfigure}[c,top]{0.48\textwidth}	\centering
			\caption{\label{t2lee}Speed up ratios of local kernel computations for a subset of the datasets.}
			\resizebox{0.95\textwidth}{!}{\renewcommand{\arraystretch}{1.05}
				\begin{tabular}{@{}c<{\enspace}@{}lccc@{}}
					\toprule
					& \multirow{3}{*}{\vspace*{4pt}\textbf{Method}} &       \multicolumn{3}{c}{\textbf{Dataset}}       \\
					\cmidrule{3-5}               &                                               & {\textsc{Enzymes}} & {\textsc{IMDB-Binary}} & {\textsc{NCI1}} \\ \toprule
					\multirow{2}{*}{\rotatebox{90}{Global\hspace{-1pt}}} &  \textsf{-WL}                                 &        10.4        &   3.6         & 14.3     \\
					& \textsf{--WL}                           &    10.1      &  3.6        &  14.5          \\
					\cmidrule{2-5}      
					& \textsf{--LWL}                     &          1.2          &           1.2& 1.3            \\
					& \textsf{--LWL}                      &          1.0          &           1.0         &  1.0 \\ \bottomrule
			\end{tabular}}
		\end{subfigure}\hspace{10pt}
		\begin{subfigure}[c,top]{0.48\textwidth}	\centering
			\caption{\label{t2len}Average speed up ratios over all epochs (training and testing).}
			\resizebox{.65\textwidth}{!}{\renewcommand{\arraystretch}{1.05}
				\begin{tabular}{@{}c<{\enspace}@{}lcc@{}}
					\toprule
					& \multirow{3}{*}{\vspace*{4pt}\textbf{Method}} &       \multicolumn{2}{c}{\textbf{Dataset}}       \\
					\cmidrule{3-4}               &                                               & {\textsc{Zinc}} & {\textsc{alchemy}} \\ \toprule
					\multirow{2}{*}{\rotatebox{90}{Dense\hspace{-1pt}}} 
					& \textsf{-WL-GNN}                           &          2.2          &           1.1            \\
					& \textsf{--GNN}                     &          2.5          &           1.7            \\
					\cmidrule{2-4}      
					& \gineeps                                 &          0.2          &           0.4            \\
					& \textsf{--LGNN}                      &          1.0          &           1.0            \\ \bottomrule
			\end{tabular}}
			\vspace{0pt}
		\end{subfigure}
	\end{center}
	\caption{Speed up ratios of local over global algorithms.}\label{add}
\end{table}
\section{Conclusion}
We introduced local variants of the -dimensional Weisfeiler-Leman algorithm. We showed that one variant and its corresponding neural architecture are strictly more powerful than the \kwl while taking the underlying graph's sparsity into account. To demonstrate the practical utility of our findings, we applied them to graph classification and regression. We verified that our local, sparse algorithms lead to vastly reduced computation times compared to their global, dense counterparts while establishing new state-of-the-art results on a wide range of benchmark datasets. \emph{We believe that our local, higher-order kernels and GNN architectures should become a standard approach in the regime of supervised learning with small graphs, e.g., molecular learning.} 

Future work includes a more fine-grained analysis of the proposed algorithm, e.g., moving away from the restrictive graph isomorphism objective and deriving a deeper understanding of the neural architecture's capabilities when optimized with stochastic gradient descent.

 

\section*{Broader impact}
We view our work mainly as a methodological contribution. It studies the limits of current (supervised) graph embeddings methods, commonly used in chemoinformatics~\cite{Sto+2020}, bioinformatics~\cite{Barabasi2004}, or network science~\cite{Eas+2010}. Currently, methods used in practice, such as GNNs or extended-connectivity fingerprints~\cite{Rogers2010} have severe limitations and might miss crucial patterns in today's complex, interconnected data. We investigate how to scale up graph embeddings that can deal with higher-order interactions of vertices (or atom of molecules, users in social networks, variables in optimization, \dots) to larger graphs or networks.  Hence, our method paves the way for more resource-efficient and expressive graph embeddings.

We envision that our (methodological) contributions enable the design of more expressive and scalable graph embeddings in fields such as quantum chemistry, drug-drug interaction prediction, in-silicio, data-driven drug design/generation, and network analysis for social good. However, progress in graph embeddings might also trigger further advancements in hostile social network analysis, e.g., extracting more fine-grained user interactions for social tracking.

\xhdr{Example impact} We are actively cooperating with chemists on drug design to evaluate further our approach to new databases for small molecules. Here, the development of new databases is quite tedious, and graph embeddings can provide hints to the wet lab researcher where to start their search. However, still, humans need to do much of the intuition-driven, manual wet lab work.  Hence, we do not believe that our methods will result in job losses in the life sciences in the foreseeable future.

\begin{ack}
We thank Matthias Fey for answering our questions with regard to \textsc{PyTorch Geometric} and Xavier Bresson for providing the \textsc{Zinc} dataset.
This work is funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany's Excellence Strategy -- EXC-2047/1 -- 390685813
and under DFG Research Grants Program--RA 3242/1-1--411032549. 
\end{ack}


\bibliographystyle{plainnat} 
\begin{thebibliography}{125}
	\providecommand{\natexlab}[1]{#1}
	\providecommand{\url}[1]{\texttt{#1}}
	\expandafter\ifx\csname urlstyle\endcsname\relax
	\providecommand{\doi}[1]{doi: #1}\else
	\providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi
	
	\bibitem[Abu{-}El{-}Haija et~al.(2019)Abu{-}El{-}Haija, Perozzi, Kapoor,
	Alipourfard, Lerman, Harutyunyan, Steeg, and Galstyan]{Hai+2019}
	S.~Abu{-}El{-}Haija, B.~Perozzi, A.~Kapoor, N.~Alipourfard, K.~Lerman,
	H.~Harutyunyan, G.~Ver Steeg, and A.~Galstyan.
	\newblock Mixhop: Higher-order graph convolutional architectures via sparsified
	neighborhood mixing.
	\newblock In \emph{International Conference on Machine Learning}, pages 21--29,
	2019.
	
	\bibitem[Anderson et~al.(2019)Anderson, Hy, and Kondor]{And+2019}
	B.~M. Anderson, T.{-}S. Hy, and R.~Kondor.
	\newblock Cormorant: Covariant molecular neural networks.
	\newblock In \emph{Advances in Neural Information Processing Systems 32}, pages
	14510--14519, 2019.
	
	\bibitem[Arora et~al.(2019)Arora, Du, Hu, Li, Salakhutdinov, and
	Wang]{Aro+2019}
	S.~Arora, S.~S. Du, W.~Hu, Z.~Li, R.~Salakhutdinov, and R.~Wang.
	\newblock On exact computation with an infinitely wide neural net.
	\newblock In \emph{Advances in Neural Information Processing Systems}, pages
	8139--8148, 2019.
	
	\bibitem[Arvind et~al.(2015)Arvind, K{\"{o}}bler, Rattan, and
	Verbitsky]{Arv+2015}
	V.~Arvind, J.~K{\"{o}}bler, G.~Rattan, and O.~Verbitsky.
	\newblock On the power of color refinement.
	\newblock In \emph{International Symposium on Fundamentals of Computation
		Theory}, pages 339--350, 2015.
	
	\bibitem[Arvind et~al.(2019)Arvind, Fuhlbr{\"{u}}ck, K{\"{o}}bler, and
	Verbitsky]{Arv+2019}
	V.~Arvind, F.~Fuhlbr{\"{u}}ck, J.~K{\"{o}}bler, and O.~Verbitsky.
	\newblock On {Weisfeiler-Leman} invariance: Subgraph counts and related graph
	properties.
	\newblock In \emph{International Symposium on Fundamentals of Computation
		Theory}, pages 111--125, 2019.
	
	\bibitem[Atserias and Maneva(2013)]{Ast+2013}
	A.~Atserias and E.~N. Maneva.
	\newblock Sherali-adams relaxations and indistinguishability in counting
	logics.
	\newblock \emph{{SIAM} Journal on Computing}, 42\penalty0 (1):\penalty0
	112--137, 2013.
	
	\bibitem[Atserias et~al.(2019)Atserias, Mancinska, Roberson, S{\'{a}}mal,
	Severini, and Varvitsiotis]{Ats+2019}
	A.~Atserias, L~Mancinska, D.~E. Roberson, R.~S{\'{a}}mal, S.~Severini, and
	A.~Varvitsiotis.
	\newblock Quantum and non-signalling graph isomorphisms.
	\newblock \emph{Journal of Combinatorial Theory, Series B}, 136:\penalty0
	289--328, 2019.
	
	\bibitem[Babai(2016)]{Bab+2016}
	L.~Babai.
	\newblock Graph isomorphism in quasipolynomial time.
	\newblock In \emph{{ACM} {SIGACT} Symposium on Theory of Computing}, pages
	684--697, 2016.
	
	\bibitem[Bai et~al.(2019)Bai, Zhang, and Torr]{Bai+2019}
	S.~Bai, F.~Zhang, and P.~H.~S. Torr.
	\newblock Hypergraph convolution and hypergraph attention.
	\newblock \emph{CoRR}, abs/1901.08150, 2019.
	
	\bibitem[Barabasi and Oltvai(2004)]{Barabasi2004}
	A.-L. Barabasi and Z.~N. Oltvai.
	\newblock Network biology: Understanding the cells functional organization.
	\newblock \emph{Nature Reviews Genetics}, 5\penalty0 (2):\penalty0 101--113,
	2004.
	
	\bibitem[Barcel{\'{o}} et~al.(2020)Barcel{\'{o}}, Kostylev, Monet, P{\'{e}}rez,
	Reutter, and Silva]{Bar+2020}
	P.~Barcel{\'{o}}, E.~V. Kostylev, M.~Monet, J.~P{\'{e}}rez, J.~L. Reutter, and
	J.~Pablo Silva.
	\newblock The logical expressiveness of graph neural networks.
	\newblock In \emph{International Conference on Learning Representations}, 2020.
	
	\bibitem[Berkholz et~al.(2013)Berkholz, Bonsma, and Grohe]{Ber+2013}
	C.~Berkholz, P.~S. Bonsma, and M.~Grohe.
	\newblock Tight lower and upper bounds for the complexity of canonical colour
	refinement.
	\newblock In \emph{Annual European Symposium on Algorithms}, pages 145--156.
	Springer, 2013.
	
	\bibitem[Borgwardt and Kriegel(2005)]{Bor+2005}
	K.~M. Borgwardt and H.-P. Kriegel.
	\newblock Shortest-path kernels on graphs.
	\newblock In \emph{IEEE International Conference on Data Mining}, pages 74--81,
	2005.
	
	\bibitem[Bruna et~al.(2014)Bruna, Zaremba, Szlam, and LeCun]{Bru+2014}
	J.~Bruna, W.~Zaremba, A.~Szlam, and Y.~LeCun.
	\newblock Spectral networks and deep locally connected networks on graphs.
	\newblock In \emph{International Conference on Learning Representation}, 2014.
	
	\bibitem[Cai et~al.(1992)Cai, F{\"{u}}rer, and Immerman]{Cai+1992}
	J.~Cai, M.~F{\"{u}}rer, and N.~Immerman.
	\newblock An optimal lower bound on the number of variables for graph
	identifications.
	\newblock \emph{Combinatorica}, 12\penalty0 (4):\penalty0 389--410, 1992.
	
	\bibitem[Cangea et~al.(2018)Cangea, Velickovic, Jovanovic, Kipf, and
	Li{\`{o}}]{Can+2018}
	C.~Cangea, P.~Velickovic, N.~Jovanovic, T.~Kipf, and P.~Li{\`{o}}.
	\newblock Towards sparse hierarchical graph classifiers.
	\newblock \emph{CoRR}, abs/1811.01287, 2018.
	
	\bibitem[Cao et~al.(2015)Cao, Lu, and Xu]{Cao+2015}
	S.~Cao, W.~Lu, and Q.~Xu.
	\newblock {GraRep:} {L}earning graph representations with global structural
	information.
	\newblock In \emph{{ACM} International Conference on Information and Knowledge
		Management}, pages 891--900, 2015.
	
	\bibitem[Chami et~al.(2019)Chami, Ying, R{\'{e}}, and Leskovec]{Cha+2019}
	I.~Chami, Z.~Ying, C.~R{\'{e}}, and J.~Leskovec.
	\newblock Hyperbolic graph convolutional neural networks.
	\newblock In \emph{Advances in Neural Information Processing Systems}, pages
	4869--4880, 2019.
	
	\bibitem[Chami et~al.(2020)Chami, Abu{-}El{-}Haija, Perozzi, R{\'{e}}, and
	Murphy]{Cha+2020}
	I.~Chami, S.~Abu{-}El{-}Haija, B.~Perozzi, C.~R{\'{e}}, and K.~Murphy.
	\newblock Machine learning on graphs: {A} model and comprehensive taxonomy.
	\newblock \emph{CoRR}, abs/2005.03675, 2020.
	
	\bibitem[Chang and Lin(2011)]{Cha+11}
	C.-C. Chang and C.-J. Lin.
	\newblock {{LIBSVM}}: {A} library for support vector machines.
	\newblock \emph{ACM Transactions on Intelligent Systems and Technology},
	2:\penalty0 27:1--27:27, 2011.
	
	\bibitem[Chen et~al.(2019{\natexlab{a}})Chen, Chen, Hsieh, Lee, Liao, Liao,
	Liu, Qiu, Sun, Tang, Zemel, and Zhang]{Che+2020}
	G.~Chen, P.~Chen, C.{-}Y. Hsieh, C.{-}K. Lee, B.~Liao, R.~Liao, W.~Liu, J.~Qiu,
	Q.~Sun, J.~Tang, R.~S. Zemel, and S.~Zhang.
	\newblock Alchemy: {A} quantum chemistry dataset for benchmarking {AI} models.
	\newblock \emph{CoRR}, abs/1906.09427, 2019{\natexlab{a}}.
	
	\bibitem[Chen et~al.(2018{\natexlab{a}})Chen, Ma, and Xiao]{Che+2018}
	J.~Chen, T.~Ma, and C.~Xiao.
	\newblock Fastgcn: Fast learning with graph convolutional networks via
	importance sampling.
	\newblock In \emph{International Conference on Learning Representation},
	2018{\natexlab{a}}.
	
	\bibitem[Chen et~al.(2018{\natexlab{b}})Chen, Zhu, and Song]{Che+2018a}
	J.~Chen, J.~Zhu, and L.~Song.
	\newblock Stochastic training of graph convolutional networks with variance
	reduction.
	\newblock In \emph{International Conference on Machine Learning}, pages
	941--949, 2018{\natexlab{b}}.
	
	\bibitem[Chen et~al.(2019{\natexlab{b}})Chen, Villar, Chen, and
	Bruna]{Che+2019}
	Z.~Chen, S.~Villar, L.~Chen, and J.~Bruna.
	\newblock On the equivalence between graph isomorphism testing and function
	approximation with {GNN}s.
	\newblock In \emph{Advances in Neural Information Processing Systems}, pages
	15868--15876, 2019{\natexlab{b}}.
	
	\bibitem[Chen et~al.(2020)Chen, Chen, Villar, and Bruna]{Che+2020a}
	Z.~Chen, L.~Chen, S.~Villar, and J.~Bruna.
	\newblock Can graph neural networks count substructures?
	\newblock \emph{CoRR}, abs/2002.04025, 2020.
	
	\bibitem[Corso et~al.(2020)Corso, Cavalleri, Beaini, Li{\`{o}}, and
	Velickovic]{Cor+2020}
	G.~Corso, L.~Cavalleri, D.~Beaini, P.~Li{\`{o}}, and P.~Velickovic.
	\newblock Principal neighbourhood aggregation for graph nets.
	\newblock \emph{CoRR}, abs/2004.05718, 2020.
	
	\bibitem[D. and J.(2010)]{Eas+2010}
	Easley D. and Kleinberg J.
	\newblock \emph{Networks, Crowds, and Markets: Reasoning About a Highly
		Connected World}.
	\newblock Cambridge University Press, 2010.
	
	\bibitem[Dasoulas et~al.(2020)Dasoulas, Santos, Scaman, and Virmaux]{Das+2020}
	G.~Dasoulas, L.~Dos Santos, K.~Scaman, and A.~Virmaux.
	\newblock Coloring graph neural networks for node disambiguation.
	\newblock In \emph{International Joint Conference on Artificial Intelligence},
	pages 2126--2132, 2020.
	
	\bibitem[Defferrard et~al.(2016)Defferrard, X., and Vandergheynst]{Def+2015}
	M.~Defferrard, Bresson X., and P.~Vandergheynst.
	\newblock Convolutional neural networks on graphs with fast localized spectral
	filtering.
	\newblock In \emph{Advances in Neural Information Processing Systems}, pages
	3844--3852, 2016.
	
	\bibitem[Dell et~al.(2018)Dell, Grohe, and Rattan]{Del+2018}
	H.~Dell, M.~Grohe, and G.~Rattan.
	\newblock Lov{\'{a}}sz meets {W}eisfeiler and {L}eman.
	\newblock In \emph{International Colloquium on Automata, Languages, and
		Programming}, pages 40:1--40:14, 2018.
	
	\bibitem[Dobson and Doig(2003)]{Dob+2003}
	P.~D Dobson and A.~J. Doig.
	\newblock Distinguishing enzyme structures from non-enzymes without alignments.
	\newblock \emph{Journal of Molecular Biology}, 330\penalty0 (4):\penalty0 771
	-- 783, 2003.
	
	\bibitem[Du et~al.(2019)Du, Hou, Salakhutdinov, Poczos, Wang, and Xu]{Du+2019}
	S.~S. Du, K.~Hou, R.~R. Salakhutdinov, B.~Poczos, R.~Wang, and K.~Xu.
	\newblock {Graph Neural Tangent Kernel:} {F}using graph neural networks with
	graph kernels.
	\newblock In \emph{Advances in Neural Information Processing Systems}, pages
	5723--5733, 2019.
	
	\bibitem[Duvenaud et~al.(2015)Duvenaud, Maclaurin, Iparraguirre, Bombarell,
	Hirzel, Aspuru-Guzik, and Adams]{Duv+2015}
	D.~K. Duvenaud, D.~Maclaurin, J.~Iparraguirre, R.~Bombarell, T.~Hirzel,
	A.~Aspuru-Guzik, and R.~P. Adams.
	\newblock Convolutional networks on graphs for learning molecular fingerprints.
	\newblock In \emph{Advances in Neural Information Processing Systems}, pages
	2224--2232, 2015.
	
	\bibitem[Dwivedi et~al.(2020)Dwivedi, Joshi, Laurent, Bengio, and
	Bresson]{Dwi+2020}
	V.~P. Dwivedi, C.~K. Joshi, T.~Laurent, Y.~Bengio, and X.~Bresson.
	\newblock Benchmarking graph neural networks.
	\newblock \emph{CoRR}, abs/2003.00982, 2020.
	
	\bibitem[Fan et~al.(2008)Fan, Chang, Hsieh, Wang, and Lin]{Fan+2008}
	R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin.
	\newblock {LIBLINEAR}: {A} library for large linear classification.
	\newblock \emph{Journal of Machine Learning Research}, 9:\penalty0 1871--1874,
	2008.
	
	\bibitem[Fey and Lenssen(2019)]{Fey+2019}
	M.~Fey and J.~E. Lenssen.
	\newblock Fast graph representation learning with {PyTorch Geometric}.
	\newblock \emph{CoRR}, abs/1903.02428, 2019.
	
	\bibitem[Fey et~al.(2018)Fey, Lenssen, Weichert, and M{\"u}ller]{Fey+2018}
	M.~Fey, J.~E. Lenssen, F.~Weichert, and H.~M{\"u}ller.
	\newblock {SplineCNN}: Fast geometric deep learning with continuous {B}-spline
	kernels.
	\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition},
	pages 869--877, 2018.
	
	\bibitem[Flam{-}Shepherd et~al.(2020)Flam{-}Shepherd, Wu, Friederich, and
	Aspuru{-}Guzik]{Fla+2020}
	D.~Flam{-}Shepherd, T.~Wu, P.~Friederich, and A.~Aspuru{-}Guzik.
	\newblock Neural message passing on high order paths.
	\newblock \emph{CoRR}, abs/2002.10413, 2020.
	
	\bibitem[F{\"{u}}rer(2017)]{Fue+2017}
	Martin F{\"{u}}rer.
	\newblock On the combinatorial power of the {Weisfeiler-Lehman} algorithm.
	\newblock In \emph{International Conference on Algorithms and Complexity},
	pages 260--271, 2017.
	
	\bibitem[Gao and Ji(2019)]{Gao+2019}
	H.~Gao and S.~Ji.
	\newblock {Graph U-Nets}.
	\newblock In \emph{International Conference on Machine Learning}, pages
	2083--2092, 2019.
	
	\bibitem[Garg et~al.(2020)Garg, Jegelka, and Jaakkola]{Gar+2020}
	V.~K. Garg, S.~Jegelka, and T.~S. Jaakkola.
	\newblock Generalization and representational limits of graph neural networks.
	\newblock \emph{CoRR}, abs/2002.06157, 2020.
	
	\bibitem[G\"{a}rtner et~al.(2003)G\"{a}rtner, Flach, and Wrobel]{Gae+2003}
	T.~G\"{a}rtner, P.~Flach, and S.~Wrobel.
	\newblock On graph kernels: Hardness results and efficient alternatives.
	\newblock In \emph{Learning Theory and Kernel Machines}, pages 129--143. 2003.
	
	\bibitem[Geerts et~al.(2020)Geerts, Mazowiecki, and P{\'{e}}rez]{Gee+2020a}
	F.~Geerts, F.~Mazowiecki, and G.~A. P{\'{e}}rez.
	\newblock Let's agree to degree: Comparing graph convolutional networks in the
	message-passing framework.
	\newblock \emph{CoRR}, abs/2004.02593, 2020.
	
	\bibitem[Geerts(2020)]{Gee+2020b}
	Floris Geerts.
	\newblock The expressive power of kth-order invariant graph networks.
	\newblock \emph{CoRR}, abs/2007.12035, 2020.
	
	\bibitem[Gilmer et~al.(2017)Gilmer, Schoenholz, Riley, Vinyals, and
	Dahl]{Gil+2017}
	J.~Gilmer, S.~S. Schoenholz, P.~F. Riley, O.~Vinyals, and G.~E. Dahl.
	\newblock Neural message passing for quantum chemistry.
	\newblock In \emph{International Conference on Machine Learning}, 2017.
	
	\bibitem[Grohe(2017)]{Gro2017}
	M.~Grohe.
	\newblock \emph{Descriptive Complexity, Canonisation, and Definable Graph
		Structure Theory}.
	\newblock Lecture Notes in Logic. Cambridge University Press, 2017.
	
	\bibitem[Grohe(2020)]{Gro+2020}
	M.~Grohe.
	\newblock {Word2vec, Node2vec, Graph2vec, X2vec:} {T}owards a theory of vector
	embeddings of structured data.
	\newblock \emph{CoRR}, abs/2003.12590, 2020.
	
	\bibitem[Grohe and Otto(2015)]{GroheO15}
	M.~Grohe and M.~Otto.
	\newblock Pebble games and linear equations.
	\newblock \emph{Journal of Symbolic Logic}, 80\penalty0 (3):\penalty0 797--844,
	2015.
	
	\bibitem[Grohe et~al.(2014)Grohe, Kersting, Mladenov, and Selman]{Gro+2014}
	M.~Grohe, K.~Kersting, M.~Mladenov, and E.~Selman.
	\newblock Dimension reduction via colour refinement.
	\newblock In \emph{European Symposium on Algorithms}, pages 505--516, 2014.
	
	\bibitem[Grohe et~al.(2020)Grohe, Schweitzer, and D]{Gro+2020a}
	M.~Grohe, P.~Schweitzer, and Wiebking D.
	\newblock Deep {Weisfeiler Leman}.
	\newblock \emph{CoRR}, abs/2003.10935, 2020.
	
	\bibitem[Hamilton et~al.(2017)Hamilton, Ying, and Leskovec]{Ham+2017}
	W.~L. Hamilton, R.~Ying, and J.~Leskovec.
	\newblock Inductive representation learning on large graphs.
	\newblock In \emph{Advances in Neural Information Processing Systems}, pages
	1025--1035, 2017.
	
	\bibitem[Heimann et~al.(2019)Heimann, Safavi, and Koutra]{Hei+2019}
	M.~Heimann, T.~Safavi, and D.~Koutra.
	\newblock Distribution of node embeddings as multiresolution features for
	graphs.
	\newblock In \emph{{IEEE} International Conference on Data Mining}, pages
	289--298, 2019.
	
	\bibitem[Helma et~al.(2001)Helma, King, Kramer, and Srinivasan]{Hel+2001}
	C.~Helma, R.~D. King, S.~Kramer, and A.~Srinivasan.
	\newblock {The Predictive Toxicology Challenge 20002001 }.
	\newblock \emph{Bioinformatics}, 17\penalty0 (1):\penalty0 107--108, 01 2001.
	
	\bibitem[Huang et~al.(2018)Huang, Zhang, Rong, and Huang]{Hua+2018}
	W.~Huang, T.~Zhang, Y.~Rong, and J.~Huang.
	\newblock Adaptive sampling towards fast graph representation learning.
	\newblock In \emph{Advances in Neural Information Processing Systems}, pages
	4563--4572, 2018.
	
	\bibitem[Immerman and Lander(1990)]{Imm+1990}
	N.~Immerman and E.~Lander.
	\newblock \emph{Describing Graphs: {A} First-Order Approach to Graph
		Canonization}, pages 59--81.
	\newblock Springer, 1990.
	
	\bibitem[Jacot et~al.(2018)Jacot, Hongler, and Gabriel]{Jac+2020}
	A.~Jacot, C.~Hongler, and F.~Gabriel.
	\newblock {Neural Tangent kernel:} convergence and generalization in neural
	networks.
	\newblock In \emph{Advances in Neural Information Processing Systems}, pages
	8580--8589, 2018.
	
	\bibitem[Jin et~al.(2018)Jin, Barzilay, and Jaakkola]{Jin+2018a}
	W.~Jin, R.~Barzilay, and T.~S. Jaakkola.
	\newblock Junction tree variational autoencoder for molecular graph generation.
	\newblock In \emph{International Conference on Machine Learning}, pages
	2328--2337, 2018.
	
	\bibitem[Jin et~al.(2019)Jin, Song, and Shi]{Jin+2020}
	Y.~Jin, G.~Song, and C.~Shi.
	\newblock {GraLSP:} {G}raph neural networks with local structural patterns.
	\newblock \emph{CoRR}, abs/1911.07675, 2019.
	
	\bibitem[Johansson and Dubhashi(2015)]{Joh+2015}
	F.~D. Johansson and D.~Dubhashi.
	\newblock Learning with similarity functions on graphs using matchings of
	geometric embeddings.
	\newblock In \emph{ACM SIGKDD International Conference on Knowledge Discovery
		and Data Mining}, pages 467--476, 2015.
	
	\bibitem[Kashima et~al.(2003)Kashima, Tsuda, and Inokuchi]{Kas+2003}
	H.~Kashima, K.~Tsuda, and A.~Inokuchi.
	\newblock Marginalized kernels between labeled graphs.
	\newblock In \emph{International Conference on Machine Learning}, pages
	321--328, 2003.
	
	\bibitem[Kiefer and McKay(2020)]{Kie+2020}
	S.~Kiefer and B.~D. McKay.
	\newblock The iteration number of colour refinement.
	\newblock \emph{CoRR}, abs/2005.10182, 2020.
	
	\bibitem[Kiefer and Schweitzer(2016)]{Kie+2016}
	S.~Kiefer and P.~Schweitzer.
	\newblock Upper bounds on the quantifier depth for graph differentiation in
	first order logic.
	\newblock In \emph{{ACM/IEEE} Symposium on Logic in Computer Science}, pages
	287--296, 2016.
	
	\bibitem[Kiefer et~al.(2015)Kiefer, Schweitzer, and Selman]{Kie+2015}
	S.~Kiefer, P.~Schweitzer, and E.~Selman.
	\newblock Graphs identified by logics with counting.
	\newblock In \emph{International Symposium on Mathematical Foundations of
		Computer Science}, pages 319--330, 2015.
	
	\bibitem[Kipf and Welling(2017)]{Kip+2017}
	T.~N. Kipf and M.~Welling.
	\newblock Semi-supervised classification with graph convolutional networks.
	\newblock In \emph{International Conference on Learning Representation}, 2017.
	
	\bibitem[Kireev(1995)]{Kir+1995}
	D.~B. Kireev.
	\newblock Chemnet: A novel neural network based method for graph/property
	mapping.
	\newblock \emph{Journal of Chemical Information and Computer Sciences},
	35\penalty0 (2):\penalty0 175--180, 1995.
	
	\bibitem[Klicpera et~al.(2020)Klicpera, Gro{\ss}, and
	G{\"{u}}nnemann]{Kli+2020}
	J.~Klicpera, J.~Gro{\ss}, and S.~G{\"{u}}nnemann.
	\newblock Directional message passing for molecular graphs.
	\newblock In \emph{International Conference on Learning Representations}, 2020.
	
	\bibitem[Kondor and Pan(2016)]{Kon+2016}
	R.~Kondor and H.~Pan.
	\newblock The multiscale {L}aplacian graph kernel.
	\newblock In \emph{Advances in Neural Information Processing Systems}, pages
	2982--2990, 2016.
	
	\bibitem[Kriege et~al.(2016)Kriege, Giscard, and Wilson]{Kri+2016}
	N.~M. Kriege, P.-L. Giscard, and R.~C. Wilson.
	\newblock On valid optimal assignment kernels and applications to graph
	classification.
	\newblock In \emph{Advances in Neural Information Processing Systems}, pages
	1615--1623, 2016.
	
	\bibitem[Kriege et~al.(2018)Kriege, Morris, Rey, and Sohler]{Kri+2018}
	N.~M. Kriege, C.~Morris, A.~Rey, and C.~Sohler.
	\newblock A property testing framework for the theoretical expressivity of
	graph kernels.
	\newblock In \emph{International Joint Conference on Artificial Intelligence},
	pages 2348--2354, 2018.
	
	\bibitem[Kriege et~al.(2019)Kriege, Neumann, Morris, Kersting, and
	Mutzel]{Kri+2017b}
	N~M. Kriege, M.~Neumann, C.~Morris, K.~Kersting, and P.~Mutzel.
	\newblock A unifying view of explicit and implicit feature maps of graph
	kernels.
	\newblock \emph{Data Minining and Knowledge Discovery}, 33\penalty0
	(6):\penalty0 1505--1547, 2019.
	
	\bibitem[Kriege et~al.(2020)Kriege, Johansson, and Morris]{Kri+2019}
	N.~M. Kriege, F.~D. Johansson, and C.~Morris.
	\newblock A survey on graph kernels.
	\newblock \emph{Applied Network Science}, 5\penalty0 (1):\penalty0 6, 2020.
	
	\bibitem[Lee et~al.(2019)Lee, Rossi, Kong, Kim, Koh, and Rao]{Lee+2019}
	J.~B. Lee, R.~A. Rossi, X.~Kong, S.~Kim, E.~Koh, and A.~Rao.
	\newblock Graph convolutional networks with motif-based attention.
	\newblock In \emph{28th {ACM} International Conference on Information}, pages
	499--508, 2019.
	
	\bibitem[Lichter et~al.(2019)Lichter, Ponomarenko, and Schweitzer]{Lic+2019}
	M.~Lichter, I.~Ponomarenko, and P.~Schweitzer.
	\newblock Walk refinement, walk logic, and the iteration number of the
	{Weisfeiler-Leman} algorithm.
	\newblock In \emph{34th Annual {ACM/IEEE} Symposium on Logic in Computer
		Science}, pages 1--13, 2019.
	
	\bibitem[Loukas(2020)]{Loukas20}
	Andreas Loukas.
	\newblock What graph neural networks cannot learn: depth vs width.
	\newblock In \emph{International Conference on Learning Representations}, 2020.
	
	\bibitem[Maehara and NT(2019)]{Mae+2019}
	T.~Maehara and H.~NT.
	\newblock A simple proof of the universality of invariant/equivariant graph
	neural networks.
	\newblock \emph{CoRR}, abs/1910.03802, 2019.
	
	\bibitem[Malkin(2014)]{Mal2014}
	P.~N. Malkin.
	\newblock Sheraliadams relaxations of graph isomorphism polytopes.
	\newblock \emph{Discrete Optimization}, 12:\penalty0 73 -- 97, 2014.
	
	\bibitem[Maron et~al.(2019{\natexlab{a}})Maron, Ben{-}Hamu, Serviansky, and
	Lipman]{Mar+2019}
	H.~Maron, H.~Ben{-}Hamu, H.~Serviansky, and Y.~Lipman.
	\newblock Provably powerful graph networks.
	\newblock In \emph{Advances in Neural Information Processing Systems}, pages
	2153--2164, 2019{\natexlab{a}}.
	
	\bibitem[Maron et~al.(2019{\natexlab{b}})Maron, Ben{-}Hamu, Shamir, and
	Lipman]{Mar+2019b}
	H.~Maron, H.~Ben{-}Hamu, N.~Shamir, and Y.~Lipman.
	\newblock Invariant and equivariant graph networks.
	\newblock In \emph{International Conference on Learning Representations},
	2019{\natexlab{b}}.
	
	\bibitem[Meng et~al.(2018)Meng, Mouli, Ribeiro, and Neville]{Cha+2018}
	C.~Meng, S.~C. Mouli, B.~Ribeiro, and J.~Neville.
	\newblock Subgraph pattern neural networks for high-order graph evolution
	prediction.
	\newblock In \emph{{AAAI} Conference on Artificial Intelligence}, pages
	3778--3787, 2018.
	
	\bibitem[Merkwirth and Lengauer(2005)]{Mer+2005}
	C.~Merkwirth and T.~Lengauer.
	\newblock Automatic generation of complementary descriptors with molecular
	graph networks.
	\newblock \emph{Journal of Chemical Information and Modeling}, 45\penalty0
	(5):\penalty0 1159--1168, 2005.
	
	\bibitem[Monti et~al.(2017)Monti, Boscaini, Masci, Rodol{\`{a}}, Svoboda, and
	Bronstein]{Mon+2017}
	F.~Monti, D.~Boscaini, J.~Masci, E.~Rodol{\`{a}}, J.~Svoboda, and M.~M.
	Bronstein.
	\newblock Geometric deep learning on graphs and manifolds using mixture model
	{CNNs}.
	\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition},
	pages 5425--5434, 2017.
	
	\bibitem[Morris et~al.(2017)Morris, Kersting, and Mutzel]{Mor+2017}
	C.~Morris, K.~Kersting, and P.~Mutzel.
	\newblock Glocalized {W}eisfeiler-{L}ehman kernels: Global-local feature maps
	of graphs.
	\newblock In \emph{IEEE International Conference on Data Mining}, pages
	327--336. IEEE, 2017.
	
	\bibitem[Morris et~al.(2019)Morris, Ritzert, Fey, Hamilton, Lenssen, Rattan,
	and Grohe]{Mor+2019}
	C.~Morris, M.~Ritzert, M.~Fey, W.~L. Hamilton, Jan~Eric Lenssen, G.~Rattan, and
	M.~Grohe.
	\newblock {Weisfeiler and Leman} go neural: {H}igher-order graph neural
	networks.
	\newblock In \emph{AAAI Conference on Artificial Intelligence}, pages
	4602--4609, 2019.
	
	\bibitem[Morris et~al.(2020)Morris, Kriege, Bause, Kersting, Mutzel, and
	Neumann]{Mor+2020}
	C.~Morris, N.~M. Kriege, F.~Bause, K.~Kersting, P.~Mutzel, and M.~Neumann.
	\newblock {TUDataset:} {A} collection of benchmark datasets for learning with
	graphs.
	\newblock \emph{CoRR}, abs/2007.08663, 2020.
	
	\bibitem[Murphy et~al.(2019{\natexlab{a}})Murphy, Srinivasan, Rao, and
	Ribeiro]{Mur+2019a}
	R.~L. Murphy, B.~Srinivasan, V.~A. Rao, and B.~Ribeiro.
	\newblock Janossy pooling: Learning deep permutation-invariant functions for
	variable-size inputs.
	\newblock In \emph{International Conference on Learning Representations},
	2019{\natexlab{a}}.
	
	\bibitem[Murphy et~al.(2019{\natexlab{b}})Murphy, Srinivasan, Rao, and
	Ribeiro]{Mur+2019}
	Ryan~L. Murphy, B.~Srinivasan, V.~A. Rao, and B.~Ribeiro.
	\newblock Relational pooling for graph representations.
	\newblock In \emph{International Conference on Machine Learning}, pages
	4663--4673, 2019{\natexlab{b}}.
	
	\bibitem[Niepert et~al.(2016)Niepert, Ahmed, and Kutzkov]{Nie+2016}
	M.~Niepert, M.~Ahmed, and K.~Kutzkov.
	\newblock Learning convolutional neural networks for graphs.
	\newblock In \emph{International Conference on Machine Learning}, pages
	2014--2023, 2016.
	
	\bibitem[Nikolentzos et~al.(2017)Nikolentzos, Meladianos, and
	Vazirgiannis]{Nik+2017}
	G.~Nikolentzos, P.~Meladianos, and M.~Vazirgiannis.
	\newblock Matching node embeddings for graph similarity.
	\newblock In \emph{AAAI Conference on Artificial Intelligence}, pages
	2429--2435, 2017.
	
	\bibitem[Nikolentzos et~al.(2018)Nikolentzos, Meladianos, Limnios, and
	Vazirgiannis]{Nik+2018}
	G.~Nikolentzos, P.~Meladianos, S.~Limnios, and M.~Vazirgiannis.
	\newblock A degeneracy framework for graph similarity.
	\newblock In \emph{International Joint Conference on Artificial Intelligence},
	pages 2595--2601, 2018.
	
	\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
	Killeen, Lin, Gimelshein, Antiga, Desmaison, K{\"{o}}pf, Yang, DeVito,
	Raison, Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala]{Pas+2019}
	A.~Paszke, S.~Gross, F.~Massa, A.~Lerer, J.~Bradbury, G.~Chanan, T.~Killeen,
	Z.~Lin, N.~Gimelshein, L.~Antiga, A.~Desmaison, A.~K{\"{o}}pf, E.~Yang,
	Z.~DeVito, M.~Raison, A.~Tejani, S.~Chilamkurthy, B.~Steiner, L.~Fang,
	J.~Bai, and S.~Chintala.
	\newblock {PyTorch: A}n imperative style, high-performance deep learning
	library.
	\newblock In \emph{Advances in Neural Information Processing Systems}, pages
	8024--8035, 2019.
	
	\bibitem[Ramakrishnan et~al.(2014)Ramakrishnan, Dral, Rupp, and von
	Lilienfeld]{Ram+2014}
	R.~Ramakrishnan, O.~Dral, P., M.~Rupp, and O.~A. von Lilienfeld.
	\newblock Quantum chemistry structures and properties of 134 kilo molecules.
	\newblock \emph{Scientific Data}, 1, 2014.
	
	\bibitem[Rieck et~al.(2019)Rieck, Bock, and Borgwardt]{Rie+2019}
	B.~Rieck, C.~Bock, and K.~M. Borgwardt.
	\newblock A persistent {Weisfeiler-Lehman} procedure for graph classification.
	\newblock In \emph{International Conference on Machine Learning}, pages
	5448--5458, 2019.
	
	\bibitem[Rogers and Hahn(2010)]{Rogers2010}
	D.~Rogers and M.~Hahn.
	\newblock Extended-connectivity fingerprints.
	\newblock \emph{Journal of Chemical Information and Modeling}, 50\penalty0
	(5):\penalty0 742--754, May 2010.
	
	\bibitem[Rong et~al.(2020)Rong, Huang, Xu, and Huang]{Ron+2020}
	Y.~Rong, W.~Huang, T.~Xu, and J.~Huang.
	\newblock {DropEdge:} {Towards} deep graph convolutional networks on node
	classification.
	\newblock In \emph{International Conference on Learning Representations}, 2020.
	
	\bibitem[Sato et~al.(2019)Sato, Yamada, and Kashima]{Sat+2019}
	R.~Sato, M.~Yamada, and H.~Kashima.
	\newblock Approximation ratios of graph neural networks for combinatorial
	problems.
	\newblock In \emph{Neural Information Processing Systems}, pages 4083--4092,
	2019.
	
	\bibitem[Sato et~al.(2020)Sato, Yamada, and Kashima]{Sat+2020}
	R.~Sato, M.~Yamada, and H.~Kashima.
	\newblock Random features strengthen graph neural networks.
	\newblock \emph{CoRR}, abs/2002.03155, 2020.
	
	\bibitem[Scarselli et~al.(2009)Scarselli, Gori, Tsoi, Hagenbuchner, and
	Monfardini]{Sca+2009}
	F.~Scarselli, M.~Gori, A.~C. Tsoi, M.~Hagenbuchner, and G.~Monfardini.
	\newblock The graph neural network model.
	\newblock \emph{IEEE Transactions on Neural Networks}, 20\penalty0
	(1):\penalty0 61--80, 2009.
	
	\bibitem[Schomburg et~al.(2004)Schomburg, Chang, Ebeling, Gremse, Heldt, Huhn,
	and Schomburg]{Sch+2004}
	I.~Schomburg, A.~Chang, C.~Ebeling, M.~Gremse, C.~Heldt, G.~Huhn, and
	D.~Schomburg.
	\newblock {BRENDA}, the enzyme database: updates and major new developments.
	\newblock \emph{Nucleic acids research}, 32\penalty0 (Database issue):\penalty0
	D4313, January 2004.
	
	\bibitem[Shervashidze et~al.(2009)Shervashidze, Vishwanathan, Petri, Mehlhorn,
	and Borgwardt]{She+2009}
	N.~Shervashidze, S.~V.~N. Vishwanathan, T.~H. Petri, K.~Mehlhorn, and K.~M.
	Borgwardt.
	\newblock Efficient graphlet kernels for large graph comparison.
	\newblock In \emph{International Conference on Artificial Intelligence and
		Statistics}, pages 488--495, 2009.
	
	\bibitem[Shervashidze et~al.(2011)Shervashidze, Schweitzer, van Leeuwen,
	Mehlhorn, and Borgwardt]{She+2011}
	N.~Shervashidze, P.~Schweitzer, E.~J. van Leeuwen, K.~Mehlhorn, and K.~M.
	Borgwardt.
	\newblock Weisfeiler-{L}ehman graph kernels.
	\newblock \emph{Journal of Machine Learning Research}, 12:\penalty0 2539--2561,
	2011.
	
	\bibitem[Simonovsky and Komodakis(2017)]{Sim+2017}
	M.~Simonovsky and N.~Komodakis.
	\newblock Dynamic edge-conditioned filters in convolutional neural networks on
	graphs.
	\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition},
	pages 29--38, 2017.
	
	\bibitem[Sperduti and Starita(1997)]{Spe+1997}
	A.~Sperduti and A.~Starita.
	\newblock Supervised neural networks for the classification of structures.
	\newblock \emph{IEEE Transactions on Neural Networks}, 8\penalty0 (2):\penalty0
	714--35, 1997.
	
	\bibitem[Stokes et~al.(2020)Stokes, Yang, Swanson, Jin, Cubillos-Ruiz, Donghia,
	MacNair, French, Carfrae, Bloom-Ackerman, Tran, Chiappino-Pepe, Badran,
	Andrews, Chory, Church, Brown, Jaakkola, Barzilay, and Collins]{Sto+2020}
	J.~Stokes, K.~Yang, K.~Swanson, W.~Jin, A.~Cubillos-Ruiz, N.~Donghia,
	C.~MacNair, S.~French, L.~Carfrae, Z.~Bloom-Ackerman, V.~Tran,
	A.~Chiappino-Pepe, A.~Badran, I.~Andrews, E.~Chory, G.~Church, E.~Brown,
	T.~Jaakkola, R.~Barzilay, and J.~Collins.
	\newblock A deep learning approach to antibiotic discovery.
	\newblock \emph{Cell}, 180:\penalty0 688--702.e13, 02 2020.
	
	\bibitem[Togninalli et~al.(2019)Togninalli, Ghisu, Llinares{-}L{\'{o}}pez,
	Rieck, and Borgwardt]{Tog+2019}
	M.~Togninalli, E.~Ghisu, F.~Llinares{-}L{\'{o}}pez, B.~Rieck, and K.~M.
	Borgwardt.
	\newblock Wasserstein {Weisfeiler-Lehman} graph kernels.
	\newblock In \emph{Advances in Neural Information Processing Systems}, pages
	6436--6446, 2019.
	
	\bibitem[Velickovic et~al.(2018)Velickovic, Cucurull, Casanova, Romero,
	Li{\`{o}}, and Bengio]{Vel+2018}
	P.~Velickovic, G.~Cucurull, A.~Casanova, A.~Romero, P.~Li{\`{o}}, and
	Y.~Bengio.
	\newblock Graph attention networks.
	\newblock In \emph{International Conference on Learning Representations}, 2018.
	
	\bibitem[Verma and Zhang(2017)]{Ver+2017}
	S.~Verma and Z.{-}L. Zhang.
	\newblock Hunt for the unique, stable, sparse and fast feature learning on
	graphs.
	\newblock In \emph{Advances in Neural Information Processing Systems}, pages
	88--98, 2017.
	
	\bibitem[Verma and Zhang(2019)]{Ver+2019}
	S.~Verma and Z.{-}L. Zhang.
	\newblock Stability and generalization of graph convolutional neural networks.
	\newblock In \emph{{ACM} {SIGKDD} International Conference on Knowledge
		Discovery {\&} Data Mining}, pages 1539--1548, 2019.
	
	\bibitem[Vinyals et~al.(2016)Vinyals, Bengio, and Kudlur]{Vin+2016}
	O.~Vinyals, S.~Bengio, and M.~Kudlur.
	\newblock Order matters: Sequence to sequence for sets.
	\newblock In \emph{International Conference on Learning Representations}, 2016.
	
	\bibitem[Wale et~al.(2008)Wale, Watson, and Karypis]{Wal+2008}
	N.~Wale, I.~A. Watson, and G.~Karypis.
	\newblock Comparison of descriptor spaces for chemical compound retrieval and
	classification.
	\newblock \emph{Knowledge and Information Systems}, 14\penalty0 (3):\penalty0
	347--375, 2008.
	
	\bibitem[Weisfeiler(1976)]{Wei+1976}
	B.~Weisfeiler.
	\newblock \emph{On Construction and Identification of Graphs}.
	\newblock Lecture Notes in Mathematics, Vol. 558. Springer, 1976.
	
	\bibitem[Weisfeiler and Leman.(1968)]{Wei+1968}
	B.~Weisfeiler and A.~Leman.
	\newblock The reduction of a graph to canonical form and the algebra which
	appears therein.
	\newblock \emph{Nauchno-Technicheskaya Informatsia}, 2\penalty0 (9):\penalty0
	12--16, 1968.
	\newblock English translation by G. Ryabov is available at
	\url{https://www.iti.zcu.cz/wl2018/pdf/wl_paper_translation.pdf}.
	
	\bibitem[Wu et~al.(2018)Wu, Ramsundar, Feinberg, Gomes, Geniesse, Pappu,
	Leswing, and Pande]{Wu+2018}
	Z.~Wu, B.~Ramsundar, E.~N. Feinberg, J.~Gomes, C.~Geniesse, A.~S. Pappu,
	K.~Leswing, and V.~Pande.
	\newblock {MoleculeNet:} {A} benchmark for molecular machine learning.
	\newblock \emph{Chemical Science}, 9:\penalty0 513--530, 2018.
	
	\bibitem[Wu et~al.(2019)Wu, Pan, Chen, Long, Zhang, and Yu]{Wu+2019}
	Z.~Wu, S.~Pan, F.~Chen, G.~Long, C.~Zhang, and P.~S. Yu.
	\newblock A comprehensive survey on graph neural networks.
	\newblock \emph{CoRR}, abs/1901.00596, 2019.
	
	\bibitem[Xu et~al.(2018)Xu, Li, Tian, Sonobe, Kawarabayashi, and
	Jegelka]{Xu+2018}
	K.~Xu, C.~Li, Y.~Tian, T.~Sonobe, K.~Kawarabayashi, and S.~Jegelka.
	\newblock Representation learning on graphs with jumping knowledge networks.
	\newblock In \emph{International Conference on Machine Learning}, pages
	5453--5462, 2018.
	
	\bibitem[Xu et~al.(2019)Xu, Hu, Leskovec, and Jegelka]{Xu+2018b}
	K.~Xu, W.~Hu, J.~Leskovec, and S.~Jegelka.
	\newblock How powerful are graph neural networks?
	\newblock In \emph{International Conference on Learning Representations}, 2019.
	
	\bibitem[Yadati et~al.(2019)Yadati, Nimishakavi, Yadav, Nitin, Louis, and
	Talukdar]{Yad+2019}
	N.~Yadati, M.~Nimishakavi, P.~Yadav, V.~Nitin, A.~Louis, and P.~P. Talukdar.
	\newblock {HyperGCN:} {A} new method for training graph convolutional networks
	on hypergraphs.
	\newblock In \emph{Advances in Neural Information Processing Systems}, pages
	1509--1520, 2019.
	
	\bibitem[Yan et~al.(2008)Yan, Cheng, Han, and Yu]{Yan+2008}
	X.~Yan, H.~Cheng, J.~Han, and P.~S. Yu.
	\newblock Mining significant graph patterns by leap search.
	\newblock In \emph{{ACM} {SIGMOD} International Conference on Management of
		Data}, pages 433--444, 2008.
	
	\bibitem[Yanardag and Vishwanathan(2015{\natexlab{a}})]{Yan+2015}
	P.~Yanardag and S.~V.~N. Vishwanathan.
	\newblock A structural smoothing framework for robust graph comparison.
	\newblock In \emph{Advances in Neural Information Processing Systems}, pages
	2125--2133, 2015{\natexlab{a}}.
	
	\bibitem[Yanardag and Vishwanathan(2015{\natexlab{b}})]{Yan+2015a}
	P.~Yanardag and S.~V.~N. Vishwanathan.
	\newblock Deep graph kernels.
	\newblock In \emph{ACM SIGKDD International Conference on Knowledge Discovery
		and Data}, pages 1365--1374. ACM, 2015{\natexlab{b}}.
	
	\bibitem[Ying et~al.(2018)Ying, You, Morris, Ren, Hamilton, and
	Leskovec]{Yin+2018}
	R.~Ying, J.~You, C.~Morris, X.~Ren, W.~L. Hamilton, and J.~Leskovec.
	\newblock Hierarchical graph representation learning with differentiable
	pooling.
	\newblock In \emph{Advances in Neural Information Processing Systems}, pages
	4800--4810, 2018.
	
	\bibitem[You et~al.(2019)You, Ying, and Leskovec]{You+2019}
	J.~You, R.~Ying, and J.~Leskovec.
	\newblock Position-aware graph neural networks.
	\newblock In \emph{International Conference on Machine Learning}, pages
	7134--7143, 2019.
	
	\bibitem[Zhang et~al.(2018)Zhang, Cui, Neumann, and Yixin]{Zha+2018}
	M.~Zhang, Z.~Cui, M.~Neumann, and C.~Yixin.
	\newblock An end-to-end deep learning architecture for graph classification.
	\newblock In \emph{AAAI Conference on Artificial Intelligence}, pages
	4428--4435, 2018.
	
	\bibitem[Zhang et~al.(2020)Zhang, Zou, and Ma]{Zha+2019}
	R.~Zhang, Y.~Zou, and J.~Ma.
	\newblock {Hyper-SAGNN:} {A} self-attention based graph neural network for
	hypergraphs.
	\newblock In \emph{International Conference on Learning Representations}, 2020.
	
	\bibitem[Zhou et~al.(2006)Zhou, Huang, and Sch{\"{o}}lkopf]{Zho+2006}
	D.~Zhou, J.~Huang, and B.~Sch{\"{o}}lkopf.
	\newblock Learning with hypergraphs: Clustering, classification, and embedding.
	\newblock In \emph{Advances in Neural Information Processing Systems}, pages
	1601--1608, 2006.
	
	\bibitem[Zhou et~al.(2018)Zhou, Cui, Zhang, Yang, Liu, Wang, Li, and
	Sun]{Zho+2018}
	J.~Zhou, G.~Cui, Z.~Zhang, C.~Yang, Z.~Liu, L.~Wang, C.~Li, and M.~Sun.
	\newblock Graph neural networks: A review of methods and applications.
	\newblock \emph{CoRR}, abs/1812.08434, 2018.
	
\end{thebibliography}


\newpage
\appendix
\section*{Appendix}

\section{Related work (Expanded)}\label{exprel}

In the following, we review related work from graph kernels, GNNs, and theory.

\xhdr{Graph kernels}
Historically, kernel methods---which implicitly or explicitly map graphs to elements of a Hilbert space---have been the dominant approach for supervised learning on graphs. Important early work in this area includes random-walk based kernels~\cite{Gae+2003,Kas+2003,Kri+2017b} and kernels based on shortest paths~\cite{Bor+2005}. More recently, graph kernels' developments have emphasized scalability, focusing on techniques that bypass expensive Gram matrix computations by using explicit feature maps, see, e.g.,~\cite{She+2011}. \citeauthor{Mor+2017}~\cite{Mor+2017} devised a local, set-based variant of the \kwl. However, the approach is (provably) weaker than the tuple-based algorithm, and they do not prove convergence to the original algorithm. \citeauthor{Yan+2015} successfully employed Graphlet~\cite{She+2009}, and Weisfeiler-Leman kernels within frameworks for smoothed~\cite{Yan+2015} and deep graph kernels~\cite{Yan+2015a}. Other recent works focus on assignment-based~\cite{Joh+2015,Kri+2016,Nik+2017}, spectral~\cite{Kon+2016,Ver+2017}, graph decomposition ~\cite{Nik+2018}, randomized binning approaches~\cite{Hei+2019}, and the extension of kernels based on the \wl~\cite{Rie+2019,Tog+2019}. For a theoretical investigation of graph kernels, see~\cite{Kri+2018}, for a thorough survey of graph kernels, see~\cite{Kri+2019}. 

\xhdr{GNNs}
Recently, graph neural networks (GNNs)~\cite{Gil+2017,Sca+2009} emerged as an alternative to graph kernels. Notable instances of this architecture include, e.g.,~\cite{Duv+2015,Fey+2018,Ham+2017,Vel+2018}, and the spectral approaches proposed in, e.g.,~\cite{Bru+2014,Def+2015,Kip+2017,Mon+2017}---all of which descend from early work in~\cite{Kir+1995,Mer+2005,Spe+1997,Sca+2009}. Recent extensions and improvements to the GNN framework include approaches to incorporate different local structures (around subgraphs), e.g.,~\cite{Hai+2019,Fla+2020,Jin+2020,Nie+2016,Xu+2018}, novel techniques for pooling node representations in order perform graph classification, e.g.,~\cite{Can+2018,Gao+2019,Yin+2018,Zha+2018}, incorporating distance information~\cite{You+2019}, and non-euclidian geometry approaches~\cite{Cha+2019}. Moreover, recently empirical studies on neighborhood aggregation functions for continuous vertex features~\cite{Cor+2020}, edge-based GNNs leveraging physical knowledge~\cite{And+2019,Kli+2020}, and sparsification methods~\cite{Ron+2020} emerged. \citeauthor{Loukas20}~\cite{Loukas20} and \citeauthor{Sat+2019} studied the limits of GNNs when applied to combinatorial problems. A survey of recent advancements in GNN techniques can be found, e.g., in~\cite{Cha+2020,Wu+2019,Zho+2018}. \citeauthor{Gar+2020}~\cite{Gar+2020} and~\citeauthor{Ver+2019}~\cite{Ver+2019} studied the generalization abilities of GNNs, and~\cite{Du+2019} related wide GNNs to a variant of the neural tangent kernel~\cite{Aro+2019,Jac+2020}. \citeauthor{Mur+2019}~\cite{Mur+2019a,Mur+2019} and~\citeauthor{Sat+2020}~\cite{Sat+2020} extended the expressivity of GNNs by considering all possible permutations of a graphs adjacency matrix, or adding random node features, respectivley. The connection between random colorings and universality was investigated in~\cite{Das+2020}.

Recently, connections to Weisfeiler-Leman type algorithms have been shown~\cite{Bar+2020,Che+2019,Gee+2020a,Gee+2020b,Mae+2019,Mar+2019,Mor+2019,Xu+2018b}. Specifically,~\cite{Mor+2019,Xu+2018b} showed that the expressive power of any possible GNN architecture is limited by the \wl in terms of distinguishing non-isomorphic graphs. \citeauthor{Mor+2019}~\cite{Mor+2019} also introduced \emph{-dimensional GNNs} (\kgnn) which rely on a message-passing scheme between subgraphs of cardinality . Similar to~\cite{Mor+2017}, the paper employed a local, set-based (neural) variant of the \kwl, which is (provably) weaker than the variant considered here. Later, this was refined in~\cite{Mar+2019} by introducing \emph{-order invariant graph networks} (\kign), based on \citeauthor{Mar+2019b}~\cite{Mar+2019b}, and references therein, which are equivalent to the folklore variant of the \kwl~\cite{Gro2017} in terms of distinguishing non-isomorphic graphs. However, \kign may not scale since they rely on dense linear algebra routines. \citeauthor{Che+2019}~\cite{Che+2019} connect the theory of universal approximation of permutation-invariant functions and the graph isomorphism viewpoint and introduce a variation of the -\textsf{WL}, which is more powerful than the former. Our comprehensive treatment of higher-order, sparse, neural networks for arbitrary  subsumes all of the algorithms and neural architectures mentioned above.

Finally, there exists a new line of work focusing on extending GNNs to hypergraphs, see, e.g.,~\cite{Bai+2019,Yad+2019,Zha+2019}, and a line of work in the data mining community incorporating global or higher-order information into graph or node embeddings, see, e.g.,~\cite{Cao+2015,Lee+2019,Cha+2018}.



\xhdr{Theory}
The Weisfeiler-Leman algorithm constitutes one of the earliest approaches to isomorphism testing~\cite{Wei+1976,Wei+1968}, having been heavily investigated by the theory community over the last few decades~\cite{Gro+2014}. Moreover, the fundamental nature of the \kwl is evident from a variety of connections to other fields such as logic, optimization, counting complexity, and quantum computing. The power and limitations of -WL can be neatly characterized in terms of logic and descriptive complexity~\cite{Imm+1990}, Sherali-Adams relaxations of the natural integer linear program for the graph isomorphism problem~\cite{Ast+2013,GroheO15,Mal2014}, homomorphism counts~\cite{Del+2018}, and quantum isomorphism games~\cite{Ats+2019}. In their seminal paper~\cite{Imm+1990},~\citeauthor{Cai+1992} showed that for each  there exists a pair of non-isomorphic graphs of size  each that cannot be distinguished by the \kwl. \citeauthor{Gro+2014}~\cite{Gro+2014} gives a thorough overview of these results. For , the power of the algorithm has been completely characterized~\cite{Arv+2015,Kie+2015}.  Moreover, upper bounds on the running time for ~\cite{Ber+2013,Kie+2020}, and the number of iterations for the folklore ~\cite{Kie+2016,Lic+2019} have been shown. For  and ,~\citeauthor{Arv+2019}~\cite{Arv+2019} studied the abilities of the (folklore) \kwl to detect and count fixed subgraphs, extending the work of~\citeauthor{Fue+2017}~\cite{Fue+2017}. The former was refined in~\cite{Che+2020a}. The algorithm (for logarithmic ) plays a prominent role in the recent result of Babai~\cite{Bab+2016} improving the best-known running time for the graph isomorphism problem. Recently,~\citeauthor{Gro+2020a}~\cite{Gro+2020a} introduced the framework of Deep Weisfeiler Leman algorithms, which allow the design of a more powerful graph isomorphism test than Weisfeiler-Leman type algorithms. Finally, the emerging connections between the Weisfeiler-Leman paradigm and graph learning are described in a recent survey of \citeauthor{Gro+2020}~\cite{Gro+2020}. 


\section{Preliminaries (Expanded)}\label{prelim}

We briefly describe the Weisfeiler-Leman algorithm and, along the way, introduce our notation. We also state a variant of the algorithm, introduced in~\cite{Mal2014}. As usual, let  for , and let  denote a multiset. 

\xhdr{Graphs} A \new{graph}  is a pair  with a \emph{finite} set of
\new{vertices}  and a set of \new{edges} . We denote the set of vertices and the set
of edges of  by  and , respectively. For ease of
notation, we denote the edge  in  by  or
. In the case of \emph{directed graphs} . A \new{labeled graph}  is a triple
 with a label function ,
where  is some finite alphabet. Then  is a
\new{label} of  for  in . 
The \new{neighborhood} 
of  in  is denoted by . 
Moreover, its complement . 
Let  then  is the \new{subgraph induced} by  with
. A \new{tree} is a connected graph without
cycles. A \new{rooted tree} is a tree with a designated vertex called \new{root} in which the edges are directed in such a way that they point away from the root. 
Let  be a vertex in a directed tree then we call its out-neighbors \new{children} with parent . 

We say that two graphs  and 
are \new{isomorphic} if there exists an edge preserving bijection
, i.e.,  is in  if and only if
 is in . If  and  are isomorphic,
we write  and call  an \new{isomorphism} between
 and . Moreover, we call the equivalence classes induced by
 \emph{isomorphism types}, and denote the isomorphism type of  by
. In the case of labeled graphs, we additionally require that
 for  in  and  for  in . 
Let  be a \emph{tuple} in  for , then  is the subgraph induced by the components of , where the vertices are labeled with integers from  corresponding to indices of . 

\xhdr{Kernels} A \emph{kernel} on a non-empty set  is a positive semidefinite function 
.
Equivalently, a function  is a kernel if there is a \emph{feature map} 
 to a Hilbert space  with inner product 
, such that 
 for all  and  in .
Let  be the set of all graphs, then a (positive semidefinite) function  is called a \emph{graph kernel}.

\section{Vertex refinement algorithms (Expanded)}\label{vr}

Let  be a fixed positive integer. As usual, let  denote the set of -tuples of vertices of . 

A \new{coloring} of  is a mapping , i.e., we assign a number (color) to every tuple in . The \new{initial coloring}  of  is specified by the isomorphism types of the tuples, i.e., two tuples  and  in  get a common color iff the mapping  induces an isomorphism between the labeled subgraphs  and . A \new{color class} corresponding to a color  is the set of all tuples colored ,
i.e., the set . 

The \new{neighborhood} of a vertex tuple  in  is defined as follows. For  in , let  be the -tuple obtained by replacing the 
 component of  with the vertex . That is, . If  for some  in , call  a -\new{neighbor} of . The neighborhood of  is thus defined as the set of all tuples  such that  for some  in  and  in . 

The \emph{refinement} of a coloring , denoted by , is a coloring  defined as follows. 
For each  in , collect the colors of the -neighbors of  as a multiset .
Then, for a tuple , define

where  is the -tuple . For consistency, the strings  thus obtained are lexicographically sorted and renamed as integers. Observe that the new color  of  is solely dictated by the color histogram of its neighborhood. In general, a different mapping  could be used, depending on the neighborhood information that we would like to aggregate. We will refer to a mapping  as an \new{aggregation map}. 

\xhdr{-dimensional Weisfeiler-Leman}\label{wl_app} For , the \kwl computes a coloring  of a given graph , as follows.\footnote{We define the -\textsf{WL} in the next subsection.} To begin with, the initial coloring  is computed. Then, starting with , successive refinements  are computed until convergence. That is,

where 

The successive refinement steps are also called \new{rounds} or \new{iterations}. Since the disjoint union of the color classes form a partition of , there must exist a finite  such that . In the end, the \kwl outputs  as the \emph{stable coloring} . 

The \kwl \new{distinguishes} two graphs  and  if, upon running the \kwl on their disjoint union , there exists a color  in  in the stable coloring such that the corresponding color class  satisfies

i.e., there exist an unequal number of -colored tuples in  and . Hence, two graphs distinguished by the \kwl must be non-isomorphic. 

In fact, there exist several variants of the above defined \kwl. These variants result from the application of different aggregation maps . For example, setting  to be 

yields a well-studied variant of the \kwl (see, e.g., \cite{Cai+1992}), commonly known as ``folklore'' \kwl in machine learning literature. It holds that the -WL using~\cref{app:mi} is as powerful as the folklore -WL~\cite{GroheO15}.

\subsection{-Weisfeiler-Leman algorithm} 

Let  be a -{neighbor} of . Call  a \new{local} -neighbor of  if  is adjacent to the replaced vertex . Otherwise, call  a \new{global} -neighbor of . \cref{kwlill} illustrates this definition for a 3-tuple .
For tuples  and  in , the function

indicates whether  is a local or global neighbor of . 

The \new{--dimensional Weisfeiler-Leman algorithm}, denoted by \deltakwl, is a variant of the classic \kwl which \emph{differentiates} between the local and the global neighbors during neighborhood aggregation \cite{Mal2014}. Formally, the \deltakwl algorithm refines a coloring  (obtained after  rounds) via the aggregation map 

instead of the \kwl aggregation specified by~\cref{app:mi}. We define the -\textsf{WL} to be the -1-\textsf{WL}, which is commonly known as color refinement or naive vertex classification.
\begin{figure}[t]
	\begin{center}
		\begin{subfigure}[c]{0.28\textwidth}
			\centering
			\includegraphics[scale=1]{g.pdf}
			\caption{Underlying graph , with tuple }
		\end{subfigure}\hspace{10pt}
		\begin{subfigure}[c]{0.28\textwidth}
			\centering
			\includegraphics[scale=1]{g1.pdf}
			\caption{ is a local -neighbor of   }
		\end{subfigure}\hspace{10pt}
		\begin{subfigure}[c]{0.28\textwidth}
			\centering
			\includegraphics[scale=1]{g2.pdf}
			\caption{ is a global -neighbor of }
		\end{subfigure}
	\end{center}
	\caption{Illustration of the local and global neighborhood of the -tuple .}\label{kwlill}
\end{figure}


\xhdr{Comparing -\textsf{WL} variants} Given that there exist several variants ofthe \kwl, corresponding to different aggregation maps , it is natural to ask whether they are equivalent in power, vis-a-vis distinguishing non-isomorphic graphs. Let  and  denote two vertex refinement algorithms, we write
 if  distinguishes between all non-isomorphic pairs  does, and  if both directions hold. The corresponding strict relation is denoted by . 

The following result relates the power of the \kwl and \deltakwl. Since for a graph ,   implies   for all  and  in  and , it immediately follows that . 
For , these two algorithms are equivalent by definition. For , this relation can be shown to be strict, see the next section.


\begin{proposition}[restated,~\cref{rel1} in the main text]\label{app:rel1} For all graphs and , the following holds: 	
\end{proposition}

\subsubsection{Proof of \cref{rel1}}\label{proofrel1}

It suffices to show an infinite family of graphs , , such that (a) \kwl does not distinguish  and , although (b) 
\deltakwl distinguishes  and . 

We proceed to the construction of this family. The graph family is based on the classic construction of \cite{Cai+1992}, commonly referred to as Cai-Furer-Immermman (CFI) graphs. 

\xhdr{Construction.} Let  denote the complete graph on  vertices (there are no loops in ). The vertices of  are numbered from  to . Let  denote the set of edges incident to  in : clearly,  for all . Define the graph  as follows:
\begin{enumerate}
	\item For the vertex set , we add   
	\begin{enumerate}
		\item[(a)]  for each  and for each \emph{even} subset  of , 
		\item[(b)] two vertices  for each edge .  
	\end{enumerate} 
	\item For the edge set , we add 
	\begin{enumerate}
		\item[(a)] an edge  for each , 
		\item[(b)] an edge between  and  if  and ,  
		\item[(c)] an edge between  and  if  and ,  
	\end{enumerate} 
\end{enumerate} 
Define a companion graph , in a similar manner to , with the following exception: in Step 1(a), for the vertex , we choose all \emph{odd} subsets of . Counting vertices, we find that . This finishes the construction of graphs  and . We set  and . 

A set  of vertices is said to form a \emph{distance-two-clique} if the distance between any two vertices in  is exactly two. 
\begin{lemma}
	The following holds for graphs  and  defined above. 
	\begin{itemize}
		\item There exists a distance-two-clique of size  inside .
		\item There does not exist a distance-two-clique of size  inside .
	\end{itemize}
	Hence,  and  are non-isomorphic. 
\end{lemma}
\begin{proof}
	In the graph , consider the vertex subset  of size . That is, from each ``cloud'' of vertices of the form  for a fixed , we pick the vertex corresponding to the trivial even subset, the empty set denoted by . Observe that any two vertices in  are at distance two from each other. This holds because for any ,
	 is adjacent to  which is adjacent to  (e.g. see Figure 1). Hence, the vertices in  form a distance-two-clique of size . 
	
	On the other hand, for the graph , suppose there exists a distance-two-clique, say  in ,
	where each . If we compute the parity-sum of the parities of , we end up with  since there is exactly one odd subset in this collection, viz. . On the other hand, we can also compute this parity-sum in an edge-by-edge manner: for each edge , since  and  are at distance two, either both  and  contain the edge  or neither of them contains : hence, the parity-sum contribution of  and  to the term corresponding to  is zero. Since the contribution of each edge to the total parity-sum is , the total parity-sum must be zero. This is a contradiction, and hence, there does not exist a distance-two-clique in . 
\end{proof}  

Next, we show that the local algorithm \localkwl can distinguish  and . Since  \localkwl, the above lemma implies the strictness condition . 

\begin{lemma}
	\localkwl distinguishes  and . 
\end{lemma} 
\begin{proof}
	The proof idea is to show that \localkwl algorithm is powerful enough to detect distance-two-cliques of size , which ensures the distinguishability of  and . Indeed, consider the -tuple  =  in . We claim that there is no tuple  in  such that the unrolling of  is isomorphic to the unrolling of . Indeed, for the sake of contradiction, assume that there does exist  in  such that the unrolling of  is isomorphic to the unrolling of . Comparing isomorphism types, we know that the tuple  must be of the form . 
	
	Consider the depth-two unrolling of : from the root vertex , we can go down via two local-edges labeled , to hit the tuple . If we consider the depth-two unrolling of , the isomorphism type of  implies that the vertices  and  must be at distance-two in the graph . Repeating this argument, we obtain that  form a distance-two-clique in  of size . Our goal is to produce a distance-two-clique in  of size , for the sake of contradiction. 
	
	For that, consider the depth-four unrolling of : from the root vertex , we can go down via two local-edges labeled  to hit the tuple . For each , we can further go down from  via two local edges labeled  to reach a tuple whose  and  entry is . Similarly, for the unrolling of , there exists a subset  and a corresponding tuple , such that for each , we can further go down from  via two local edges labeled  to reach a tuple whose  and  entry is . Comparing the isomorphism types of all these tuples, we deduce that  must be at distance two from each of  for . This implies that the vertex set  is a distance-two-clique of size  in , which is impossible. Hence, there does not exist any -tuple  in  such that the unrolling of  and the unrolling of  are isomorphic. Hence, the \localkwl distinguishes  and . 
\end{proof}

Finally, we note that CFI graphs are standard tools from graph isomorphism theory, and are often used to analyze the power and limitations of WL-type algorithms. It follows from results of \cite{Cai+1992} that for every , \kwl fails to distinguish the graphs  and  of our constructed family. This finishes the proof of the proposition. 

\section{Local --dimensional Weisfeiler-Leman algorithm (Expanded)}\label{lwl}

In this section, we define the new \new{local --dimensional Weisfeiler-Leman algorithm} (\localkwl). This variant of \deltakwl considers only local neighbors during the neighborhood aggregation process, and discards any information about the global neighbors. Formally, the \localkwl algorithm refines a coloring  (obtained after  rounds) via the aggregation map, 
		
instead of~\cref{app:mid}. That is, the algorithm only considers the local -neighbors of the vertex  in each iteration. 
Therefore, the indicator function  used in~\cref{app:mid} is trivially equal to  here, and is hence omitted. 
The coloring function for the \localkwl is defined by
 
We also define \pluskwl, a minor variation of \localkwl. Later, we will show that \pluskwl is equivalent in power to --WL (\cref{app:loco}). 
Formally, the \pluskwl algorithm refines a coloring  (obtained after  rounds) via the aggregation function, 

instead of \localkwl aggregation defined in~\cref{app:eqnmidd}. 
Here, the function

where  denotes that  is -neighbor of , for  in . Essentially,  counts the number of -neighbors (local or global) of  which have the same color as  under the coloring  (i.e., after  rounds). For a fixed ,
the function  is uniform over the set , where  is a color class obtained after  iterations of the \pluskwl and  denotes the set of -neighbors of . Note that after the stable partition has been reached  will not change anymore. Observe that each iteration of the \pluskwl has the same asymptotic running time as an iteration of the \localkwl.

The following theorem shows that the local variant \pluskwl is at least as powerful as the \deltakwl when restricted to the class of connected graphs. In other words, given two \emph{connected} graphs  and , if these graphs are distinguished by \deltakwl, then they must also be distinguished by the \pluskwl. On the other hand, it is important to note that, in general, the \pluskwl might need a larger number of iterations to distinguish two graphs, as compared to \deltakwl. However, this leads to advantages in a machine learning setting, see~\cref{exp}.
\begin{theorem}[restated,~\cref{loco} in the main text]\label{app:loco}
	For the class of connected graphs, the following holds for all :
	
\end{theorem}	
Along with~\cref{rel1}, we obtain the following corollary relating the power of \kwl and \pluskwl. 
\begin{corollary}[restated,~\cref{cloco} in the main text] For the class of connected graphs, the following holds for all :	
\end{corollary}
In fact, the proof of \cref{rel1} shows that the infinite family of graphs  witnessing the strictness condition can even be distinguished by \localkwl,
for each corresponding . We note here that the restriction to connected graphs can easily be circumvented by adding a specially marked vertex, which is connected to every other vertex in the graph.

\subsection{Kernels based on vertex refinement algorithms}

The idea for a kernel based on the \localkwl (and the other vertex refinements algorithms) is to compute it for  iterations resulting in a coloring function  for each iteration . Now, after each iteration, we compute a \new{feature vector}  in  for each graph . Each component  counts the number of occurrences of -tuples labeled by  in . The overall feature vector  is defined as the concatenation of the feature vectors of all  iterations, i.e., . The corresponding kernel for  iterations then is computed as  , where  denotes the standard inner product.

\subsection{Local converges to global: Proof of~\cref{loco}}\label{locodes}

The main technique behind the proof is to encode the colors assigned by the \kwl (or its variants) as rooted directed trees, called \emph{unrolling trees}. 
The exact construction of the unrolling tree depends on the aggregation map  used by the \kwl variant under consideration. 
We illustrate this construction for the \kwl. For other variants such as the \deltakwl, \localkwl, and \pluskwl, we will specify analogous constructions. 

\xhdr{Unrollings (``Rolling in the deep'')} Given a graph , a tuple  in , and an integer , 
the \emph{unrolling}  is a rooted, directed tree with vertex and edge labels, defined recursively as follows. 
\begin{itemize}
	\item[-] For ,  is defined to be a single vertex, labeled with the isomorphism type .  
	This lone vertex is also the root vertex.
	
	\item[-] For ,  is defined as follows. 
	First, introduce a root vertex , labeled with the isomorphism type .
	Next, for each  and for each -neighbor  of , 
	append the rooted subtree  below the root . 
	Moreover, the directed edge  from  to the root of  is labeled  iff  is a -neighbor of . 
	
\end{itemize}

\begin{figure}
	
	\centering
	\begin{tikzpicture}[scale=0.8]
	\tikzset{
		vertex/.style = {circle, fill = purple,opacity=0.3, minimum size = 50pt}, empty/.style = {},
		mini/.style = {circle,fill, scale = 0.5},
		minie/.style = {-, thick},
		pre/.style = {<-, shorten <= 1pt, >=stealth', thick}
	}
	\node[vertex] (root) at (0,0) [label = above : ] {};
	\begin{scope}[yshift = -0.25cm, scale = 0.7]
	\node[mini] (v1) at (-0.5,0) [label = left: \small 1] {};
	\node[mini] (v2) at (+0.5,0) [label = right: \small 2] {} edge[minie] (v1);
	\node[mini] (v3) at (0,0.85) [label = above: \small 3] {} edge[minie] (v1) edge[minie] (v2);
	\end{scope}
	
	\node[vertex] (w2) at (1.5,-2.5) [label = below : ] {} edge[pre] node[auto,swap] {2} (root);
	\begin{scope}[xshift = 1.5cm, yshift = -2.50cm, scale = 0.7]
	\node[mini] (v1) at (-0.5,0) [label = above: \small 1, label = below: \small 2] {};
	\node[mini] (v2) at (+0.5,0) [label = right: \small 3] {} edge[minie] (v1);
	\end{scope}
	
	\node[vertex] (w3) at (-1.5,-2.5) [label = below : ] {} edge[pre] node[auto] {2} (root);
	\begin{scope}[xshift = -1.5cm, yshift = -2.75cm, scale = 0.7]
	\node[mini] (v1) at (-0.5,0) [label = left: \small 1] {};
	\node[mini] (v2) at (+0.5,0) [label = right: \small 2] {} edge[minie] (v1);
	\node[mini] (v3) at (0,0.85) [label = above: \small 3] {} edge[minie] (v1) edge[minie] (v2);
	\end{scope}
	
	\node[vertex] (w1) at (4.5,-2.5)[label = below : ] {} edge[pre] node[auto,swap] {2} (root);
	\begin{scope}[xshift = 4.5cm, yshift = -2.5cm, scale = 0.7]
	\node[mini] (v1) at (-0.5,0) [label = left: \small 1] {};
	\node[mini] (v2) at (+0.5,0) [label = above: \small 2, label = below: \small 3] {} edge[minie] (v1);
	\end{scope}
	
	\node[vertex] (w4) at (-4.5,-2.5) [label = below : ] {} edge[pre] node[auto] {2} (root);
	\begin{scope}[xshift = -4.5cm, yshift = -2.75cm, scale = 0.7]
	\node[mini] (v1) at (-0.5,0) [label = left: \small 1] {};
	\node[mini] (v2) at (+0.5,0) [label = right: \small 2] {};
	\node[mini] (v3) at (0,0.85) [label = above: \small 3] {} edge[minie] (v1) edge[minie] (v2);
	\end{scope}
	
	\node[empty] (left) at (-7.5, -1.5) [label = below: ] {} edge[pre,dotted] node[auto] {-nbrs} (root); 
	\node[empty] (right) at (7.5, -1.5) [label = below: ] {} edge[pre,dotted] node[auto,swap] { -nbrs} (root);
	
	\end{tikzpicture}
	\caption{Unrolling at the tuple  of depth one.}	\label{rolling_app}
\end{figure}

We refer to  as the unrolling of the graph  \new{at  of depth }. \cref{rolling_app} partially illustrates the recursive construction of unrolling trees: it describes the unrolling tree for the graph in \cref{kwlill} at the tuple , of depth . Each node  in the unrolling tree is associated with some -tuple , indicated alongside the node in the figure. We call  the tuple corresponding to the node .

Analogously, we can define unrolling trees , , and  for the \kwl-variants \deltakwl, \localkwl, and \pluskwl respectively. The minor differences lie in the recursive step above, since the unrolling construction needs to faithfully represent the aggregation process. 

\begin{itemize}
	\item[-] For , we additionally label the directed edge  with  or  instead of just , depending on whether the neighborhood is local or global. 
	\item[-] For , we consider only the subtrees  for local -neighbors . 
	\item[-] For , we again consider only the subtrees  for local -neighbors . However,
	the directed edge  to this subtree is also labeled with the  counter value .
\end{itemize}


\xhdr{Encoding colors as trees} The following Lemma shows that the computation of the \kwl can be faithfully encoded by the unrolling trees. 
Formally, let  and  be two -vertex-tuples in .

\begin{lemma}\label{encwl}
	The colors of  and  after  rounds of \kwl are identical 
	if and only if the unrolling tree  is isomorphic to the unrolling tree .  
\end{lemma}

\begin{proof}
	By induction on . For the base case , observe that the initial colors of  and  are equal to the 
	respective isomorphism types  and . On the other hand, the vertex labels for the single-vertex graphs  and  are also the respective isomorphism types  and . Hence, the statement holds for . 
	
	For the inductive case, we proceed with the forward direction. Suppose that \kwl assigns the same color to  and  after  rounds. For each  in , the -neighbors of  form a partition  corresponding to their colors after  rounds of \kwl. Similarly, the -neighbors of  form a partition  corresponding to their colors after  rounds of \kwl, where for  in ,  and  have the same size and correspond to the same color. By inductive hypothesis, the corresponding depth  unrollings  and  are isomorphic, for every  in  and  in . Since we have a bijective correspondence between the depth  unrollings of the -neighbors of  and , respectively, there exists an isomorphism between  and . Moreover, this isomorphism preserves vertex labels (corresponding to isomorphism types) and edges labels (corresponding to -neighbors). 
	
	For the backward direction, suppose that  is isomorphic to . Then, we have a bijective correspondence between the depth  unrollings of the -neighbors of  and of , respectively. For each  in , the -neighbors of  form a partition  corresponding to their unrolling trees after  rounds of -WL. Similarly, the -neighbors of  form a partition  corresponding to their unrolling trees after  rounds of -WL, where for  in , , and  have the same size and correspond to the same isomorphism type of the unrolling tree. By induction hypothesis, the -neighborhoods of  and  have an identical color profile after  rounds. Finally, since the depth  trees  and  are trivially isomorphic, the tuples  and  have the same color after  rounds. Therefore, \kwl must assign the same color to  and  after  rounds.  
\end{proof}

Using identical arguments, we can state the analogue of~\cref{encwl} for the algorithms \deltakwl, \localkwl, \pluskwl, and their corresponding unrolling constructions ,  and . The proof is identical and is hence omitted.

\begin{lemma}\label{enc}
	The following statements hold. 
	\begin{enumerate}
		\item The colors of  and  after  rounds of \deltakwl are identical 
		if and only if the unrolling tree  is isomorphic to the unrolling tree .   
		\item The colors of  and  after  rounds of \localkwl are identical 
		if and only if the unrolling tree  is isomorphic to the unrolling tree .   
		\item The colors of  and  after  rounds of \pluskwl are identical 
		if and only if the unrolling tree  is isomorphic to the unrolling tree .   
	\end{enumerate}
\end{lemma}

\xhdr{Equivalence} The following Lemma establishes that the local algorithm \pluskwl is at least as powerful as the global \deltakwl, for connected graphs, i.e., . 

\begin{lemma}\label{pluseqtodelta}
	Let  be a connected graph, and let  and  in . If the stable colorings of  and  under the \pluskwl are identical, 
	then the stable colorings of  and  under \deltakwl are also identical. 
\end{lemma}

\begin{figure}
	\centering
	
	
	\begin{tikzpicture}[scale=0.75]
	\tikzset{
		empty/.style = {},
		vertex/.style = {circle, draw, fill=white, scale = 0.8},
		node/.style = {circle, draw = red!20, fill = red!20},
		pre/.style = {<-, shorten <= 1pt, >=stealth', semithick},
		predot/.style = {<-, shorten <= 1pt, >=stealth', dotted, semithick},
		post/.style = {->, shorten >= 1pt, >=stealth', semithick},
		undir/.style = {-, shorten <= 1pt, >=stealth', dotted, thick},
		prepost/.style = {<->, shorten <= 1pt, >=stealth', dotted, thick}
	}
	\fill[black!10!white] (0,0) -- (-3,-8) -- (3,-8) -- (0,0);
	\node[vertex] (s) [label = left : ] {}; 
	
	\fill[black!20!white] (0.3,-3) -- (0.3-1.9,-8) -- (0.3+1.9,-8) -- (0.3,-3);
	\node[vertex] (w) at (0.3,-3) [label = left:] {}; 
	
	\fill[black!40!white] (0.5,-4.5) -- (0.5-1,-8) -- (0.5+1,-8) -- (0.5,-4.5);
	\node[vertex] (x) at (0.5,-4.5) [label = left:] {} edge [pre,thick] node[midway,right] {} (w); 
	
	\draw [->,decorate,decoration={snake,amplitude=1.2mm,segment length=3mm,post length=1mm}]
	(s) -- (w) node [below,align=center,midway] (lab) { \quad};
	
	\fill[black!10!white] (9+0,0) -- (9-3,-8) -- (9+3,-8) -- (9+0,0);
	\node[vertex] (t) at (9,0) [label = right:] {}; 
	
	\fill[black!20!white] (+0.3+9,-3) -- (0.3-1.9+9,-8) -- (0.3+1.9+9,-8) -- (0.3+9,-3);
	\node[vertex] (z) at (+0.3+9,-3) [label = right: ] {}; 
	
	\fill[black!40!white] (+9,-4.5) -- (-1+9,-8) -- (+1+9,-8) -- (+9,-4.5);
	\node[vertex] (y) at (+9,-4.5) [label = right: ] {} edge [pre,thick] node[midway,left] {} (z); 
	
	\draw [->,decorate,decoration={snake,amplitude=1.2mm,segment length=3mm,post length=1mm}]
	(t) -- (z) node [below,align=center,midway] (lab2) { \quad};

	\draw [post,dashed] (s) to node[midway, label={[shift={(0.2,0)}] }] {} (t) ;
	\draw [post,dashed] (w) to node[midway, label= ] {} (z) ;
	\draw [post,dashed] (x) to node[midway, label={[shift={(0.1,0)}] }] {} (y) ;
	
	\draw [prepost] (1.5,-4.5) to node {\quad \quad \quad  } (1.5,-8);
	\end{tikzpicture}
	\caption{Unrollings  and  of sufficiently large depth.}\label{rollrollmap}
\end{figure}

\begin{proof}  
	Let  denote the number of rounds needed to attain the stable coloring under \pluskwl. Consider unrollings  and  of sufficiently large depth . Since  and  have the same stable coloring under \pluskwl, the trees  and  are isomorphic (by \cref{enc}). Let  be an isomorphism from  to . 
	
	We prove the following equivalent statement. If  and  are isomorphic, then for all , .
	The proof is by induction on . The base case  follows trivially by comparing the isomorphism types of  and . 
	
	For the inductive case, let . Let  be the set of -neighbors of . Similarily, let  be the set of -neighbors of . Our goal is to construct, for every , a corresponding bijection  between  and  satisfying the following conditions.
	
	\begin{enumerate}
		\item For all  in ,  is a local -neighbor of  if and only if  is a local -neighbor of . 
		\item For all  in , , i.e.,  and  are identically colored after  rounds of \deltakwl.
	\end{enumerate} 
	
	From the definition of  trees, the existence of such  immediately implies the desired claim . First, we show the following claim. 
	
	\begin{claim}
		Let  be a color class in the stable coloring of  under \pluskwl. 
		Let . Then, .
	\end{claim}
	
	\begin{proof}
		Either , in which case we are done. Otherwise, assume without loss of generality that . Let  in . Since  is connected, we can start from the root  of , go down along -labeled edges, and reach a vertex  such that  corresponds to the tuple . Let  be the parent of , and let  be the tuple corresponding to . Note that  is a local -neighbor of . Moreover, the depth of  is at most . Hence, the height of the subtree of  rooted at  is at least .  
		
		Consider the tuple  corresponding to the vertex  in . Observe that the path from the root  of  to the vertex  consists of -labeled edges. Therefore,  is -neighbor of , and hence  in . The stable colorings of  and  under \pluskwl are identical, because the subtrees rooted at  and  are of depth more than . Let  denote the common color class of  and , in the stable coloring of  under \pluskwl.
		
		Since  is a local neighbor of , the agreement of the  function values ensures that the number of -neighbors (local or global) of  in  is equal to the number of -neighbors (local or global) of  in . Finally, the set of -neighbors of  is equal to the set of -neighbors of , which is . Similarily, the set of -neighbors of  is equal to the set of -neighbors of , which is . 
		Hence, . 
	\end{proof} 
	
	Moreover, for each , the number of local -neighbors of  in  is equal to the number of local -neighbors of  in . Otherwise, we could perform one more round of \pluskwl and derive different colors for  and , a contradiction. 
	
	Hence, we can devise the required bijection  as follows. We pick an arbitrary bijection  between the set of local -neighbors of  inside  and the set of local -neighbors of  inside . We also pick an arbitrary bijection  between the set of global -neighbors of  inside  and the set of global -neighbors of  inside . Clearly,  satisfies the first stipulated condition. By induction hypothesis, the second condition is also satisifed. Hence, we can obtain a desired bijection  satisfying the two stipulated conditions. Since we obtain the desired bijections , this finishes the proof of the lemma. 
\end{proof}

Finally, since for a graph ,   implies   for all  and  in  and , it holds that . Together with~\cref{pluseqtodelta} above, this finishes the proof of~\cref{app:loco}.

\section{Details on experiments and additional results}\label{app:exp}

Here we give details on the experimental study of~\cref{exp}.

\subsection{Datasets, graph kernels, and neural architectures}\label{datasets}

\begin{table}[h]
	\begin{center}
		\caption{Dataset statistics and properties, ---Continuous vertex labels following~\cite{Gil+2017}, the last three components encode 3D coordinates.}
		\resizebox{1.0\textwidth}{!}{ 	\renewcommand{\arraystretch}{1.05}
			\begin{tabular}{@{}lcccccc@{}}\toprule
				\multirow{3}{*}{\vspace*{4pt}\textbf{Dataset}}&\multicolumn{6}{c}{\textbf{Properties}}\\
				\cmidrule{2-7}
				& Number of  graphs & Number of classes/targets &  Number of vertices &  Number of edges & Vertex labels & Edge labels \\ \midrule
				       & 600               & 6                 & 32.6                             & 62.1                          & \cmark  & \xmark           \\
				   & 1\,000              & 2                 & 19.8                             & 96.5                          & \xmark   & \xmark          \\
				         & 1\,500               & 3                & 13.0                             & 65.9                          & \xmark    & \xmark         \\
				          & 4\,110              & 2                 & 29.9                             & 32.3                          & \cmark   & \xmark          \\
				        & 4\,127              & 2                 & 29.7                             & 32.1                          & \cmark    & \xmark         \\
				       & 349               & 2                 & 14.1                             & 14.5                          & \cmark    & \xmark         \\
				      & 1\,113              & 2                 & 39.1                             & 72.8                          & \cmark    & \xmark         \\
				 & 2\,000              & 2                 & 429.6                            & 497.8                         & \xmark     & \xmark        \\ \midrule
				
				       &   79\,601&	2&	21.5&	22.8                & \cmark  & \cmark           \\
				       &  79\,601&	2&	39.4&	40.7                  & \cmark  & \cmark           \\
				       &      39\,988&	2&	26.1&	28.1                      & \cmark  & \cmark           \\
				       &  39\,988&	2&	46.7&	48.7      & \cmark  & \cmark           \\
				       &      	40\,516&	2&	26.1&	28.1             & \cmark  & \cmark           \\
				       &        40\,516	&2&	46.7&	48.7              & \cmark  & \cmark           \\
				\midrule        
				       &  249\,456 &12	&23.1 &	24.9     & \cmark  & \cmark           \\
				       & 202\,579 &12	& 10.1 &	10.4 	     & \cmark  & \cmark           \\
				       &129\,433  &12	& 18.0 &	18.6     & \cmark (13+3D)  & \cmark (4)          \\
				\bottomrule
		\end{tabular}}
		\label{ds}
	\end{center}
\end{table}

In the following, we give an overview of employed datasets, (baselines) kernels, and (baseline) neural architectures. 



\begin{description}
	\item[Datasets] To evaluate kernels, we use the following, well-known, small-scale \textsc{Enzymes}~\cite{Sch+2004,Bor+2005}, \textsc{IMDB-Binary}, \textsc{IMDB-Multi}~\cite{Yan+2015a}, \textsc{NCI1}, \textsc{NCI109}~\cite{Wal+2008}, \textsc{PTC\_FM}~\cite{Hel+2001}\footnote{\url{https://www.predictive-toxicology.org/ptc/}}, \textsc{Proteins}~\cite{Dob+2003,Bor+2005}, and \textsc{Reddit-Binary}~\cite{Yan+2015a} datasets. To show that our kernels also scale to larger datasets, we additionally used the mid-scale \textsc{Yeast}, \textsc{YeastH}, \textsc{UACC257}, \textsc{UACC257H}, \textsc{OVCAR-8}, \textsc{OVCAR-8H}~\cite{Yan+2008}\footnote{\url{https://sites.cs.ucsb.edu/~xyan/dataset.htm}} datasets. For the neural architectures we used the large-scale molecular regression datasets \textsc{Zinc}~\cite{Dwi+2020,Jin+2018a} and \textsc{Alchemy}~\cite{Che+2020}.
	We opted for not using the 3D-coordinates of the \textsc{Alchemy} dataset to solely show the benefits of the (sparse) higher-order structures concerning graph structure and discrete labels.
	To further compare to the (hierarchical) \kgnn~\cite{Mor+2019} and \kign~\cite{Mar+2019}, and show the benefits of our architecture in presence of continuous features, we used the \textsc{QM9}~\cite{Ram+2014,Wu+2018} regression dataset.\footnote{We opted for comparing on the \textsc{QM9} dataset to ensure a fair comparison concerning hyperparameter selection.}
	To study data efficiency, we also used smaller subsets of the \textsc{Zinc} and \textsc{Alchemy} dataset. That is, for the \textsc{Zinc 10k} (\textsc{Zink 50k}) dataset, following~\cite{Dwi+2020}, we sampled 10\,000 (50\,000) graphs from the training, and 1\,000 (5\,000) from the training and validation split, respectively. For \textsc{Zinc 10k}, we used the same splits as provided by~\cite{Dwi+2020}. For the \textsc{Alchemy 10k} (\textsc{Alchemy 50k}) dataset, as there is no fixed split available for the full dataset\footnote{Note that the full dataset is different from the contest dataset, e.g., it does not provide normalized targets, see \url{https://alchemy.tencent.com/}.}, we sampled the (disjoint) training, validation, and test splits uniformly and at random from the full dataset. See~\cref{ds} for dataset statistics and properties.\footnote{All datasets can be obtained from~\url{http://www.graphlearning.io}.} 
	
	\item[Kernels] We implemented the \localkwl, \pluskwl, \deltakwl, and  \kwl kernel for  in . We compare our kernels to the Weisfeiler-Leman subtree kernel (\wl)~\cite{She+2011}, the Weisfeiler-Leman Optimal Assignment kernel (\wloa)~\cite{Kri+2016}, the graphlet kernel~\cite{She+2009} (\gr), and the shortest-path kernel~\cite{Bor+2005} (\shp). All kernels were (re-)implemented in \CC[11]. For the graphlet kernel we counted (labeled) connected subgraphs of size three. 
	
	\item[Neural architectures] We used the \gin and \gineps architecture~\cite{Xu+2018b} as neural baselines. For data with (continuous) edge features, we used a -layer MLP to map them to the same number of components as the node features and combined them using summation (\gine and \gineeps). For the evaluation of the neural architectures of~\cref{neural}, \localkwln, \deltakwln, \kwln, we implemented them using \textsc{PyTorch Geometric}~\cite{Fey+2019}, using a  Python-wrapped \CC[11] preprocessing routine to compute the computational graphs for the higher-order GNNs. We used the \gineps layer to express  and  of~\cref{gnngeneral}. Finally, we used the \textsc{PyTorch}~\cite{Pas+2019} implementations of the -\textsf{IGN}~\cite{Mar+2019}, and --\gnn, --\gnn, ---\gnn~\cite{Mor+2019} made available by the respective authors. 
	
	For the \textsc{QM9} dataset, we additionally used the \mpnn architecture as a baseline, closely following the setup of~\cite{Gil+2017}. For the \gineeps and the \mpnn architecture, following~\citeauthor{Gil+2017}~\cite{Gil+2017}, we used a complete graph, computed pairwise  distances based on the 3D-coordinates, and concatenated them to the edge features. We note here that our intent is not the beat state-of-the-art, physical knowledge-incorporating architectures, e.g., \textsf{DimeNet}~\cite{Kli+2020} or \textsf{Cormorant}~\cite{And+2019}, but to solely show the benefits of the (local) higher-order architectures compared to the corresponding (-dimensional) GNN. For the --\textsf{GNN}, to implement~\cref{encode}, for each -tuple we concatenated the (two) node and edge features, computed pairwise  distances based on the 3D-coordinates, and a one-hot encoding of the (labeled) isomorphism type. Finally, we used a -layer MLP to learn a joint, initial vectorial representation.
\end{description}
The source code of all methods and evaluation procedures is available at \url{https://www.github.com/chrsmrrs/sparsewl}.

\begin{table}[t]\centering		
	\caption{Classification accuracies in percent and standard deviations on medium-scale datasets.}
	\label{t2l}	
	\resizebox{0.75\textwidth}{!}{ 	\renewcommand{\arraystretch}{1.05}
		\begin{tabular}{@{}c <{\enspace}@{}lcccccc@{}}	\toprule
			
			& \multirow{3}{*}{\vspace*{4pt}\textbf{Method}}&\multicolumn{6}{c}{\textbf{Dataset}}\\\cmidrule{3-8}
			& & {\textsc{Yeast}}         &  {\textsc{YeastH}}      & {\textsc{UACC257}}           & {\textsc{UACC257H}}       & {\textsc{OVCAR-8}}           & {\textsc{OVCAR-8H}}      \\	\toprule
			& \textsf{-WL}            &  88.8 \scriptsize        & 88.8 \scriptsize  & 96.8 \scriptsize  & 96.9  \scriptsize  & 96.1 \scriptsize  & 96.2 \scriptsize  \\
			\cmidrule{2-8}	
			\multirow{2}{*}{\rotatebox{90}{Neural}}   & \textsf{GINE}  & 88.3  \scriptsize  & 88.3 \scriptsize   & 95.9 \scriptsize   &  95.9  \scriptsize   & 94.9   \scriptsize   & 94.9 \scriptsize  \\
			& \textsf{GINE-}  &  88.3 \scriptsize  & 88.3  \scriptsize    & 95.9   \scriptsize   & 95.9 \scriptsize   & 94.9
			  \scriptsize  & 94.9 \scriptsize  \\
			\cmidrule{2-8}	
			\multirow{2}{*}{\rotatebox{90}{Local}}   & \textsf{--LWL}  & 89.2 \scriptsize  & 88.9 \scriptsize  &  97.0 \scriptsize  & 96.9 \scriptsize  & 96.4 \scriptsize  & 96.3 \scriptsize  \\
			& \textsf{--LWL}  & \textbf{95.0} \scriptsize  & \textbf{95.7}  \scriptsize   & \textbf{97.4} \scriptsize  & \textbf{98.1} \scriptsize  & \textbf{ 97.4} 
			\scriptsize  &\textbf{97.7}  \scriptsize  \\
			\bottomrule
	\end{tabular}}
\end{table}		


\begin{table}[t]\centering	\renewcommand{\arraystretch}{1.1}
	\caption{Training versus test accuracy of local and global kernels.}
	\label{t3}
	\resizebox{1.0\textwidth}{!}{ 	\renewcommand{\arraystretch}{1.05}
		\begin{tabular}{@{}c <{\enspace}@{}lcccccccc@{}}	\toprule
			
			& \multirow{3}{*}{\vspace*{4pt}\textbf{Set}}&\multicolumn{8}{c}{\textbf{Dataset}}\\\cmidrule{3-10}
			& & {\textsc{Enzymes}}         &  {\textsc{IMDB-Binary}}      & {\textsc{IMDB-Multi}}           & {\textsc{NCI1}}       & {\textsc{NCI109}}           & 
			{\textsc{PTC\_FM}}         & {\textsc{Proteins}}         &
			{\textsc{Reddit-Binary}  } 
			\\	\toprule
			\multirow{2}{*}{\rotatebox{25}{\textsf{-2-WL}}}    		
			& Train & 91.2  & 83.8  &57.6 & 91.5   & 92.4  &  74.1 & 85.4        & --  \\ 
			
			& Test & 37.5   &  68.1 &   47.9    &      67.0   &  67.2  &   61.9  & 75.0 & \textsc{--} \\              
			\cmidrule{2-10}
			\multirow{2}{*}{\rotatebox{25}{\textsf{--LWL}}}    		
			& Train  & 98.8  &  83.5 & 59.9  & 98.6 & 99.1 & 84.0  &  84.5 &  92.0  \\ 
			& Test  &  56.6  &  73.3  & 50.2 & 84.7 &   84.2  & 60.3  &   75.1 & 89.7  \\   
			\cmidrule{2-10}
			\multirow{2}{*}{\rotatebox{25}{\textsf{--LWL}}}    		
			& Train  & 99.5  & 95.1 &  86.5  & 95.8 &94.4  & 96.1  & 90.9 &  96.2  \\ 
			& Test   	&  52.9 &  75.7 &  62.5  &   91.4   &  89.3     &62.6  &   79.3 &   91.1    \\       
			\bottomrule
	\end{tabular}}
\end{table}




\begin{table}[t]\centering	
	\caption{Mean MAE (mean std. MAE, logMAE) on large-scale (multi-target) molecular regression tasks.}	\label{t2n_app}	
	\resizebox{.95\textwidth}{!}{ 	\renewcommand{\arraystretch}{1.05}
		\begin{tabular}{@{}c <{\enspace}@{}lcccccc@{}}	\toprule
			
			& \multirow{3}{*}{\vspace*{4pt}\textbf{Method}}&\multicolumn{6}{c}{\textbf{Dataset}}\\\cmidrule{3-8}
			&  & {\textsc{Zinc} (10k)}         &  {\textsc{Zinc} (50k)}   &  {\textsc{Zinc (Full)}}    & {\textsc{alchemy (10k)}}     & {\textsc{alchemy (50k)}}   & {\textsc{alchemy (Full)}}       \\	\toprule
			\multirow{4}{*}{\rotatebox{90}{Baseline}}
			& \gineeps  & \textbf{0.278} \scriptsize     & 0.145 \scriptsize  & 0.084                                                                                                                                                                                                                                     
			\scriptsize   & 0.185 {\scriptsize } -1.864  \scriptsize  	& 0.127
			{\scriptsize } -2.415 {\scriptsize }
			& 0.103 {\scriptsize } -2.956 {\scriptsize } \\	
			\cmidrule{2-8}
			&  \textsf{-WL-GNN}      &  0.399  \scriptsize 
			& 0.357  \scriptsize   & 0.133 \scriptsize    &  0.149  {\scriptsize }  -2.609  {\scriptsize }
			& 0.105   {\scriptsize }  -3.139  \scriptsize   & {0.093                                                                                                                                                                                                                                       
				\scriptsize }  {-3.394                                                                                                                                                                                                                                    
				\scriptsize }                                                                                                                                \\
			&  --\textsf{GNN}      & 0.374 \scriptsize 
			& 0.150 \scriptsize  & \textbf{0.042} {\scriptsize }   & \textbf{0.118}  {\scriptsize } -2.679 {\scriptsize }  & \textbf{0.085} {\scriptsize } -3.239 {\scriptsize } &\textbf{0.080}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  {\scriptsize } -3.516 {\scriptsize } \\
			
			
			\cmidrule{2-8}	
			\multirow{2}{*}{\rotatebox{90}{}}   & --\textsf{LGNN} &  0.306 \scriptsize 
			& \textbf{0.100} \scriptsize   & 0.045  \scriptsize  & 0.122 {\scriptsize } -2.573 {\scriptsize }  & 0.090 {\scriptsize } -3.176 {\scriptsize } & 0.083 {\scriptsize } -3.476  {\scriptsize } \\
			\bottomrule
o	\end{tabular}}
\end{table}



\begin{table}[t]\centering	\renewcommand{\arraystretch}{1.05}
	\caption{Overall computation times for the whole datasets in seconds  (Number of iterations for \textsf{-WL}, \textsf{-WL}, \textsf{-WL}, \textsf{--WL}, \textsf{WLOA}, \textsf{--WL}, \textsf{--LWL}, and \textsf{--LWL}: 5), \textsc{Oot}--- Computation did not finish within one day (24h), \textsc{Oom}--- Out of memory.}
	\label{t1_app}
	\resizebox{1.0\textwidth}{!}{ 	\renewcommand{\arraystretch}{1.1}
		\begin{tabular}{@{}c <{\enspace}@{}lcccccccc@{}}	\toprule
			& \multirow{3}{*}{\vspace*{4pt}\textbf{Graph Kernel}}&\multicolumn{8}{c}{\textbf{Dataset}}\\\cmidrule{3-10}
			& & {\textsc{Enzymes}}         &  {\textsc{IMDB-Binary}}      & {\textsc{IMDB-Multi}}           & {\textsc{NCI1}}       & {\textsc{NCI109}}           & 
			{\textsc{PTC\_FM}}         & {\textsc{Proteins}}         &
			{\textsc{Reddit-Binary}  } 
			\\	\toprule
			\multirow{4}{*}{\rotatebox{90}{\hspace*{-3pt}Baseline}} & \gr      &<1 & <1&<1 &1 &1 &<1 &<1 & 2 \\ 
			& \shp & <1 & <1 & <1 & 2 & 2 & <1 & <1 & 1\,035 \\
			& \textsf{-WL}          & <1 & <1 & <1 & 2 & 2 & <1 & <1 & 2 \\
			& \textsf{WLOA}          & <1 & <1 & <1 & 14 & 14 & <1 & 1 & 15 \\
			\cmidrule{2-10}	
			\multirow{4}{*}{\rotatebox{90}{Global}} 	&
			\textsf{-WL}        & 302 & 89 & 44 & 1\,422 & 1\,445& 11 &  14\,755 & \textsc{Oom} \\
			& \textsf{-WL}        	& 74\,712 & 18\,180 & 5\,346 & \textsc{Oot}&\textsc{Oot} & 5\,346&\textsc{Oom} &\textsc{Oom} \\
			\cmidrule{2-10}
			
			& \textsf{--WL}  	& 294& 89& 44 & 1\,469& 1\,459& 11 &14\,620 & \textsc{Oom}  \\
			
			& \textsf{--WL}          		&64\,486 & 17\,464 & 5\,321 & \textsc{Oot}&\textsc{Oot} & 1119 &\textsc{Oom} &\textsc{Oom} \\                                                   
			
			\cmidrule{2-10}		
			\multirow{4}{*}{\rotatebox{90}{Local}}    		
			
			& \textsf{--LWL}         	&29 & 25& 20 & 101 & 102 & 1 & 240 & 59\,378 \\      
			& \textsf{--LWL}         	& 35 & 31 & 24 & 132& 132 & 1 & 285 &84\,044\\       
			
			& \textsf{--LWL} 	&4\,453 &3\,496 &2\,127 & 18\,035 & 17\,848 & 98 &\textsc{Oom} &\textsc{Oom} \\ 
			& \textsf{--LWL}         	&4\,973 & 3\,748 & 2\,275 &20\,644 & 20\,410& 105 & \textsc{Oom}  &\textsc{Oom} \\    
			\bottomrule
	\end{tabular}}
\end{table}



\begin{table}[t]\centering	\renewcommand{\arraystretch}{1.1}
	\caption{Overall computation times for the whole datasets in seconds on medium-scale datasets (Number of iterations for \textsf{-WL}, \textsf{--LWL}, and \textsf{--LWL}: 2).}
	\label{t1l_app}
	\resizebox{.75\textwidth}{!}{ 	\renewcommand{\arraystretch}{1.05}
		\begin{tabular}{@{}c <{\enspace}@{}lcccccc@{}}	\toprule
			
			& \multirow{2}{*}{\vspace*{4pt}\textbf{Graph Kernel}}&\multicolumn{6}{c}{\textbf{Dataset}}\\\cmidrule{3-8}
			& & {\textsc{Yeast}}         &  {\textsc{YeastH}}      & {\textsc{UACC257}}           & {\textsc{UACC257H}}       & {\textsc{OVCAR-8}}           & {\textsc{OVCAR-8H}}      \\	\toprule
			
			& \textsf{-WL}            & 11   & 19  & 6 & 10& 6 & 10 \\
			
			\cmidrule{2-8}	
			\multirow{2}{*}{\rotatebox{90}{Local}}   & \textsf{--LWL}  &1\,499  & 5\,934  & 1\,024 & 3\,875	 & 1\,033  & 4\,029 \\
			& \textsf{--LWL}         	& 2\,627 &7\,563 & 1\,299 & 4\,676& 1\,344 & 4\,895 \\    
			\bottomrule
	\end{tabular}}
\end{table}




\subsection{Experimental protocol and model configuration}\label{protocol}

In the following, we describe the experimental protocol and hyperparameter setup.
\begin{description}
	\item[Kernels] For the smaller datasets (first third of~\cref{ds}), for each kernel, we computed the (cosine) normalized gram matrix. We computed the classification accuracies using the -SVM implementation of \textsc{LibSVM}~\cite{Cha+11}, using 10-fold cross-validation. 
	We repeated each 10-fold cross-validation ten times with different random folds, and report average accuracies and standard deviations. For the larger datasets (second third of~\cref{ds}), we computed explicit feature vectors for each graph and used the linear -SVM implementation of \textsc{LibLinear}~\cite{Fan+2008}, again using 10-fold cross-validation (repeated ten times). 
	Following the evaluation method proposed in~\cite{Mor+2020}, in the both cases, the -parameter was selected from   using a validation set sampled uniformly at random from the training fold (using 10\% of the training fold). Similarly, the number of iterations of the \wl, \wloa, \localkwl, \pluskwl, and \kwl were selected from  using the validation set. Moreover, for the \pluskwl, we only added the additional label function  on the last iteration to prevent overfitting.
	We report computation times for the \wl, \wloa, \localkwl, \pluskwl, and \kwl with five refinement steps. All kernel experiments were conducted on a workstation with an \text{Intel Xeon E5-2690v4} with 2.60\si GHz and 384\si GB of RAM running \text{Ubuntu 16.04.6 LTS} using a single core. Moreover, we used the GNU \CC Compiler 5.5.0 with the flag \texttt{--O2}. 
	\item[Neural architectures] For comparing to kernel approaches, see \cref{t2,t2l}, we used 10-fold cross-validation, and again used the approach outlined in~\cite{Mor+2020}. The number of components of the (hidden) node features in  and the number of layers in  of the \gin (\gine) and \gineps (\gineeps) layer were again selected using a validation set sampled uniformly at random from the training fold (using 10\% of the training fold). We used mean pooling to pool the learned node embeddings to a graph embedding and used a -layer MLP for the final classification, using a dropout layer with  after the first layer
	of the MLP. We repeated each 10-fold cross-validation ten times with different random folds, and report the average accuracies and standard deviations. Due to the different training methods, we do not provide computation times for the GNN baselines. 
	
	For the larger molecular regression tasks, \textsc{Zinc} and \textsc{Alchemy}, see~\cref{t2n_app}, we closely followed the hyperparameters found in~\cite{Dwi+2020} and~\cite{Che+2020}, respectively, for the \gineeps layers. That is, for \textsc{Zinc}, we used four \gineeps layers with a hidden dimension of 256 followed by batch norm and a -layer MLP for the joint regression of the twelve targets, after applying mean pooling. For \textsc{Alchemy} and \textsc{QM9}, we used six layers with 64 (hidden) node features and a set2seq layer~\cite{Vin+2016} for graph-level pooling, followed by a -layer MLP for the joint regression of the twelve targets. We used exactly the same hyperparameters for the (local) --\textsf{LGNN}, and the dense variants --\textsf{GNN} and -\textsf{WL-GNN}. 
	
	For \textsc{Zinc}, we used the given train, validation split, test split, and report the MAE over the test set. For the \textsc{Alchemy} and \textsc{Qm9} datasets, we uniformly and at random sampled 80\% of the graphs for training, and 10\% for validation and testing, respectively. Moreover, following~\cite{Che+2020,Gil+2017}, we normalized the targets of the training split to zero mean and unit variance. We used a single model to predict all targets. Following~\cite[Appendix C]{Kli+2020}, we report mean standardized MAE and mean standardized logMAE. We repeated each experiment five times (with different random splits in case of \textsc{Alchemy} and \textsc{Qm9}) and report average scores and standard deviations. 
	
	To compare training and testing times between the --\textsf{LGNN}, the dense variants the --\textsf{GNN} and -\textsf{WL-GNN}, and the (1-dimensional) \gineeps layer, we trained all four models on \textsc{Zinc (10k)} and \textsc{Alchemy (10k)} to convergence, divided by the number of epochs, and calculated the ratio with regard to the average epoch computation time of the --\textsf{LGNN} (average computation time of dense or baseline layer divided by average computation time of the --\textsf{LGNN}). All neural experiments were conducted on a workstation with four Nvidia Tesla V100 GPU cards with 32GB of GPU memory running Oracle Linux Server 7.7.
\end{description} 







 \end{document}
