

\documentclass[nohyperref]{article}

\usepackage{microtype}
\usepackage{caption}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} 

\usepackage{hyperref}


\newcommand{\theHalgorithm}{\arabic{algorithm}}



\usepackage[accepted]{icml2023}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage[capitalize,noabbrev]{cleveref}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage[textsize=tiny]{todonotes}

\icmltitlerunning{Magnification Invariant Medical Image Analysis}

\begin{document}

\twocolumn[
\icmltitle{Magnification Invariant Medical Image Analysis: A Comparison of Convolutional Networks, Vision Transformers, and Token Mixers}





\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Pranav Jeevan}{yyy}
\icmlauthor{Nikhil Cherian Kurian}{yyy}
\icmlauthor{Amit Sethi}{yyy}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Department of Electrical Engineering, Indian Institute of Technology Bombay, Mumbai, India}


\icmlcorrespondingauthor{Pranav Jeevan}{194070025@iitb.ac.in}


\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]





\printAffiliationsAndNotice{}  

\begin{abstract}
 Convolution Neural Networks (CNNs) are widely used in medical image analysis, but their performance degrade when the  magnification of testing images differ from the training images. The inability of CNNs to generalize across magnification scales can result in sub-optimal performance on external datasets. This study aims to evaluate the robustness of various deep learning architectures in the analysis of breast cancer histopathological images with varying magnification scales at training and testing stages. Here we explore and compare the performance of multiple deep learning architectures, including CNN-based ResNet and MobileNet, self-attention-based Vision Transformers and Swin Transformers, and token-mixing models, such as FNet, ConvMixer, MLP-Mixer, and WaveMix. The experiments are conducted using the BreakHis \cite{spanhol2015dataset} dataset, which contains breast cancer histopathological images at varying magnification levels. We show that performance of WaveMix is invariant to the magnification of training and testing data and can provide stable and good classification accuracy. These evaluations are critical in identifying deep learning architectures that can robustly handle changes in magnification scale, ensuring that scale changes across anatomical structures do not disturb the inference results. 
\end{abstract}

\section{Introduction}
\label{submission}
Computer aided medical image analysis has become a critical component in the diagnosis and treatment of various diseases \cite{chakraborty2023overview,duncan2000medical}. Deep learning models, such as Convolution neural networks (CNNs), have shown exceptional performance in analyzing medical images, including magnetic resonance imaging (MRI), computed tomography (CT), and histology images \cite{chan2020deep}. However, the performance of these models can be affected by several factors, including variations in image quality, lighting conditions, and magnification scales. In particular, changes in magnification scales between training and testing datasets can significantly impact the accuracy and robustness of deep learning models in medical image analysis \cite{gupta2017breast}.







CNNs are cited to be the most commonly used deep learning architecture for medical image analysis \cite{li2014medical}. However CNN, can struggle when it comes to handling medical images with anatomical features at varying magnification scales. In general, training a CNN on images at a specific magnification scale may result in good performance on that scale, but this performance may not generalize well to other magnification scales \cite{alkassar2021going}. This is a significant limitation when analysing medical imaging modalities like histology images where slight to moderate changes in magnification variability is common. The inability of CNN to generalize across magnification scales leads to sub-optimal inference performance on external datasets \cite{gupta2017breast}. Though, augmenting input images with perturbations in scales can slightly improve performance of CNNs, it is also important to explore or develop more robust deep learning architectures that can generate features that are inherently invariant to the changes in scale of input images. Such architectures should be designed to capture the important features in the images, regardless of the shift in the magnification scale, in order to provide robust performance for medical image analysis in a clinical settings. 

In this study, we evaluate the robustness of multiple popular deep learning architectures including CNN based architectures such as ResNet \cite{he2016deep} and MobileNet \cite{Howard2017-rm}, Self-attention based architectures such as Vision Transformers (VIT) \cite{dosovitskiy2021an} and Swin Transformers \cite{liu2021swin}, and token mixing models such as Fourier-Net (FNet) \cite{lee2021fnet}, ConvMixer \cite{trockman2022patches}, Multi-Layer Perceptron-Mixer (MLP-Mixer) \cite{tolstikhin2021mlp}, and WaveMix \cite{https://doi.org/10.48550/arxiv.2205.14375}. Our aim is to compare the performance of these deep learning models when the magnification of the test data differs from the training data. The BreakHis \cite{spanhol2015dataset} dataset , which includes breast cancer histopathological images at varying magnification levels, is utilized for our experiments. The empirical performance differences between the deep learning models will be used to determine the most robust architecture for histopathological image analysis.












\section{Experiments}
\subsection{Dataset}
We utilize the BreakHis \cite{spanhol2015dataset} dataset, which is a well-known public dataset in the field of digital breast histopathology for our experiments. It has been widely used in the development and evaluation of computer-aided diagnosis (CAD) systems for breast cancer diagnosis. It provides a challenging benchmark for the development of CAD systems due to the inherent large variations in tissue appearances.


The dataset consist of  7,909 microscopy images of breast tissue biopsy specimens from 82 patients diagnosed with either benign or malignant breast tumors. The images are collected from four different institutions and are of four different magnifications scales - 40X, 100X, 200X and 400X.

In addition to the malignancy information of each image, the dataset is further annotated with information like the patient's age, the sub-type of malignancy and the type of biopsy. The dataset is slightly imbalanced in terms of the distribution of benign and malignant cases and the distribution of different magnifications. In the dataset there are 5,429 malignant cases whereas benign cases are only about 2,480. 

As the BreakHis \cite{spanhol2015dataset} dataset contains multiple images at different magnification levels, the dataset serves as a challenging and representative testbed for evaluating the robustness of deep learning architectures across the different magnification levels or scales. These evaluations will be carried out by training some of the recently reported deep learning architecture on one magnification level of the BreakHis \cite{spanhol2015dataset} dataset and testing these trained models across multiple held-out magnification levels. Observing the  average test accuracy   on the different  magnification levels can hence reveal the robustness of deep learning architectures to varying image magnification at inference..  





\subsection{Models}

\begin{figure*}[ht]
\vskip 0.2in
\begin{center}
\includegraphics[scale=0.75]{tokenmixer.drawio.pdf}
\caption{Architectures of various token-mixers along with the general MetaFormer block. The token-mixing operation in different models is performed by different operations, such as spacial MLP, depth-wise convolution, self-attention, Fourier and Wavelet transforms}
\label{fig:token}
\end{center}
\vskip -0.2in
\end{figure*}

For CNN based models, we compared performance using ResNet-18, ResNet-34 and ResNet-50 from the ResNet family \cite{he2016deep}, and MobileNetV3-small-0.50, MobileNetV3-small-0.75 and MobileNetV3-small-100 from MobileNet family of models. We used ViT-Tiny, ViT-Small and ViT-Base (all using patch size of 16, see \cite{dosovitskiy2021an}) along with Swin-Tiny and Swin-Base (all using patch size of 4 and window size of 7, see \cite{liu2021swin}) for the experiments.

\subsubsection{Token-Mixers}
Token-mixers are the family of models which uses an architecture similar to MetaFormer \cite{yu2022metaformer} as its fundamental block as shown in ~\Cref{fig:token}. Transformer models can be considered as token-mixing model which uses self-attention for token-mixing. Other token-mixers use Fourier transforms (FNet) \cite{lee2021fnet}, Wavelet transforms (WaveMix) \cite{https://doi.org/10.48550/arxiv.2205.14375}, spatial-MLP (MLP-Mixer) \cite{tolstikhin2021mlp} or depth-wise convolutions (ConvMixer) \cite{trockman2022patches} for token-mixing. Token-mixing models have been shown to be more efficient in terms of parameters and computation compared to attention-based transformers \cite{yu2022metaformer}.

FNet \cite{lee2021fnet} was actually designed for natural language processing (NLP) tasks and was designed to handle 1D inputs sequences. We have used the 2D-FNet, i.e., a modified FNet that used a 2D Fourier transform for spacial token-mixing instead of a 1D Fourier transform used in FNet. The 2-D FNet can process images in the 2D form without the need to unroll it into sequence of patches or pixels as done in transformer and FNet. We experimented by varying the embedding dimension and number of layers to get the best model.

WaveMix \cite{https://doi.org/10.48550/arxiv.2205.14375} uses 2D-Discrete Wavelet transform (2D-DWT) for token-mixing. We experimented by varying the embedding dimension, number of layers and number of levels of 2D-DWT used in WaveMix to get the model which gives highest validation accuracy in the dataset. 

ConvMixer \cite{trockman2022patches} uses depth-wise convolution for spacial token-mixing and point-wise convolutions for channel toke-mixing. We used ConvMixer-1536/20, ConvMixer-768/32, and ConvMixer-1024/20 available in Timm model library for our experiments. 

MLP-Mixer \cite{tolstikhin2021mlp} uses spacial MLP and channel MLP to mix tokens. We used MLP-Mixer-Small (patch size of 16) and MLP-Mixer-Base (patch size of 16) in our experiments. 






\subsection{Implementation details}

The dataset was divided into train, validation and test sets in the ratio 7:1:2 for each of the magnification. Due to limited computational resources, the maximum number of training epochs was set to 300. All experiments were done with a single 80 GB Nvidia A100 GPU. \emph{No pre-trained weights was used for any of the models}. We used the ResNet, MobileNet, Vision transformer, Swin transformer, ConvMixer and MLP-Mixer available in Timm (PyTorch Image Models) library~\cite{rw2019timm}\footnote{available at \url{http://github.com/rwightman/pytorch-image-models/}} Since WaveMix and FNet  was unavailable in Timm library, it was implemented from original paper. The Timm training script~\cite{rw2019timm} with default hyper-parameter values was used to train all the models. Cross-entropy loss was used for training. We used automatic mixed precision in PyTorch during training to optimize speed and memory consumption. 

The images were resized to  for the experiments. Transformer-based models and MLP-Mixer required the images to be resized to certain specific sizes like  or . We trained models of varying sizes belonging to the same architecture on the training set and evaluated it on validation set to find the model size that gives the best performance on the Breakhis \cite{spanhol2015dataset} dataset. The model size with highest average validation performance over all magnifications was used for evaluation using test set.

The maximum batch-size was set to 128. For larger models, we reduced the batch-size so that it can fit in the GPU. Top-1 accuracy on the test set of the best of three runs with random initialization is reported as a generalization metric based on prevailing protocols~\cite{hassani2021escaping}. 

\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\includegraphics[scale=0.68]{graphbreak.pdf}
\caption{Average of all test accuracy reported for various training magnifications for each of the models compared}
\label{fig:test}
\end{center}
\vskip -0.2in
\end{figure}

\section{Results and Discussions}



\
\begin{table*}[]
\centering
\caption{Results of Inter-magnification classification performance of all CNN, transformers and token-mixers on Breakhis \cite{spanhol2015dataset} dataset. Accuracy on test set is reported.}
\vspace{0.5 cm}
\centering

\includegraphics[scale=0.70]{mod_expt1.pdf}
\label{tab:result}
\end{table*}

\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\includegraphics[scale=0.83]{graphhis.pdf}
\caption{Average of test accuracy when training and testing was done on same magnification for each model is compared}
\label{fig:diag}
\end{center}
\vskip -0.2in
\end{figure}

The Inter-magnification classification performance of all the best performing model variants of CNN, transformer and token-mixer models are shown in ~\Cref{tab:result}. We can see that WaveMix performs better than all the other models in maintaining high performance across different testing magnifications. Only ConvMixer, another token-mixer could perform better than WaveMix in one magnification (). We also observe that the accuracy of WaveMix  is the most stable, never falling bellow 87\%. Other models that perform well, such as, ConvMixer  and ResNet-34, suffers from unstable performance with accuracy falling to 81\% and 78\% respectively. The better performance of WaveMix is due to the ability of 2D wavelet transform to efficiently mix token information and the subsequent use of deconvolution layers which also aids in rapid expansion of receptive field after each wavelet block.

We also see from~\Cref{fig:test} that WaveMix performs the best amoung all models when we take the overall average of all the average testing accuracy over all magnifications. We observe that the performance of token-mixers (green) like MLP-Mixer and FNet is comparable to that of transformer based models (red). CNN-based models (blue) perform better than transformer models. 

\Cref{fig:diag} show the average of test accuracy when training and testing was done on same magnifications. We observe that ConvMixer performs better than WaveMix when train and test magnifications are same. Even ResNet-34 is performing almost on par with WaveMix and ConvMixer. This shows that even though other models perform well when magnification of training and test data are same, they cannot translate that performance when magnification of training and testing set differs from each other. WaveMix is mostly invariant to this change of magnification between train and test data and is able to provide consistent performance compared to other CNN, transformer and token-mixing models.


FNet consumed largest GPU RAM (4-8 more) compared to other architectures. CNN-based models perform much better than transformer model-based models in Breakhis classification. There is a significant drop in performance when the transformer-based models are trained on 40 magnification and tested. Similar drop in accuracy for 40 magnification training was observed for MLP-Mixer.





\section{Conclusions}
In conclusion, our study evaluated the robustness of various deep learning models for histopathological image analysis under different testing magnifications. We compared ResNet, MobileNet, Vision Transformers, Swin Transformers, Fourier-Net, ConvMixer, MLP-Mixer, and WaveMix using the BreakHis \cite{spanhol2015dataset} dataset. Our experiments demonstrated that the WaveMix architecture, which intrinsically incorporates multi-resolution features, is the most robust model to changes in inference magnification. We observed a stable accuracy of at least 87\% across all test scenarios. These findings highlight the importance of implementing a robust architecture, such as WaveMix, not only for histopathological image analysis but also for medical image analysis in general. This would help to ensure that anatomical features of diverse scales do not influence the accuracy of deep learning-based systems, thereby improving the reliability of diagnostic inference in clinical practice.
\nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2023}







\end{document}
