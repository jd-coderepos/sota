

\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage{cvpr}              

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{capt-of}
\usepackage{cuted}
\usepackage{xfrac}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\usepackage[accsupp]{axessibility}



\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


\def\cvprPaperID{1064} \def\confName{CVPR}
\def\confYear{2022}

\newcommand{\wy}[1]{{\color{gray}{\bf\sf #1}}}

\begin{document}
	
\title{MAT: Mask-Aware Transformer for Large Hole Image Inpainting}
	
\author{Wenbo Li\textsuperscript{1} \quad Zhe Lin\textsuperscript{2} \quad Kun Zhou\textsuperscript{3} \quad Lu Qi\textsuperscript{1} \quad Yi Wang\textsuperscript{4}\thanks{Corresponding author} \quad Jiaya Jia\textsuperscript{1} \\
		The Chinese University of Hong Kong \quad Adobe Inc. \\
		The Chinese University of Hong Kong (Shenzhen) \quad Shanghai AI Laboratory \\
		{\tt\small \{wenboli,luqi,leojia\}@cse.cuhk.edu.hk} \\
		{\tt\small zlin@adobe.com \quad kunzhou@link.cuhk.edu.cn \quad wangyi@pjlab.org.cn}
		\vspace{-0.4in}
	}
	\maketitle
	
	
	\begin{strip}\centering
		\includegraphics[width=1.0\linewidth]{teasing}
		\captionof{figure}{The proposed MAT supports photo-realistic and pluralistic large hole image inpainting. The first example is a real-world high-resolution image and the other two examples () are from Places~\cite{zhou2017places} and FFHQ~\cite{karras2019style} datasets. \label{fig:teasing}} \end{strip}
	
\begin{abstract}




		Recent studies have shown the importance of modeling long-range interactions in the inpainting problem. To achieve this goal, existing approaches exploit either standalone attention techniques or transformers, but usually under a low resolution in consideration of computational cost. In this paper, we present a novel transformer-based model for large hole inpainting, which unifies the merits of transformers and convolutions to efficiently process high-resolution images. We carefully design each component of our framework to guarantee the high fidelity and diversity of recovered images. Specifically, we customize an inpainting-oriented transformer block, where the attention module aggregates non-local information only from partial valid tokens, indicated by a dynamic mask. Extensive experiments demonstrate the state-of-the-art performance of the new model on multiple benchmark datasets. Code is released at \url{https://github.com/fenglinglwb/MAT}.
		
	\end{abstract}
	
\vspace{-0.1in}
	\section{Introduction}
	\label{sec:intro}
	
	Image completion (a.k.a. inpainting) is a fundamental problem in computer vision, which aims to fill missing regions with plausible contents. It has many applications including image editing~\cite{jo2019sc}, image re-targeting~\cite{cho2017weakly}, photo restoration~\cite{wan2020bringing,wan2020old} and object removal~\cite{barnes2009patchmatch}.
	
	In inpainting, modeling the contextual information is crucial, especially for large masks. Creating reasonable structures and textures for the missing areas demands contextual understanding, using distant information according to non-local priors~\cite{buades2005non,mairal2009non,berman2016non,wang2018non} in images. Previous works employ stacked convolutions to reach large receptive fields and model long-range relationships, which works well on aligned (\eg, faces, bodies) and texture-heavy (\eg, forests, water) data. When processing images with complicated structures (\ie, the first example in the  row in Figure~\ref{fig:teasing}), it is difficult for fully convolutional neural networks (CNNs) to characterize the semantic correspondences between distant areas. This is mainly due to the inherent properties of CNNs, the slow growth of the effective receptive field and the inevitable dominance of nearby pixels. To explicitly model long-range dependencies in inpainting,~\cite{yu2018generative,xie2019image,yi2020contextual} propose to employ attention modules in the CNN-based generator. However, limited by the quadratic computational complexity, the attention module is merely applied to relatively small-scale feature maps with a few times, where long-range modeling is not fully exploited.
	
	In contrast to applying attention modules to CNNs, transformer~\cite{vaswani2017attention} is a natural architecture to handle non-local modeling, where attention is a basic component in every block. Recent advances~\cite{wan2021high,zheng2021tfill,yu2021diverse} adopt transformer structures to address the inpainting problem. Nonetheless, affected by the complexity issue, existing works only employ transformers to infer low-resolution predictions (\eg\ ) for subsequent processing, hence the produced image structure is coarse, compromising the final image quality, especially on large-scale masks.
	
	In this paper, we develop a new inpainting transformer, capable of generating high-resolution completed results for large mask inpainting. Due to the lack of useful information in some regions (this is common when the given mask rules out most pixels), we find the commonly utilized transformer block (LNMSALNFFN) exhibits inferior performance in adversarial training. In this regard, we customize the vanilla Transformer block to increase optimization stability and also improve performance, by removing the conventional layer normalization~\cite{ba2016layer} and replacing the residual learning with fusion learning using feature concatenation. We analyze why these modifications are crucial for learning and empirically demonstrate they are non-trivial. Also, to handle possible heavy interactions between all tokens extracted from the high-resolution input, we propose a new variant of multi-head self-attention (MSA), named multi-head contextual attention (MCA). It computes non-local relations only using partial valid tokens. The selection of adopted tokens is indicated by a dynamic mask, which is initialized by the input mask and updated with spatial constraints and long-range interactions, improving the efficiency at no cost of effectiveness. Additionally, we incorporate a novel style manipulation module into the proposed framework, inherently supporting pluralistic generation. As shown in Fig.~\ref{fig:teasing}, our method successfully fills large holes with visually realistic and exceptionally diverse contents. Our contributions are summarized as:
	
	\begin{itemize}
		\item We develop a novel inpainting framework MAT. It is the first transformer-based inpainting system capable of directly processing high-resolution images.
		\item We meticulously design components of MAT. The proposed multi-head contextual attention conducts long-range dependency modeling efficiently by exploiting valid tokens, indicated by a dynamic mask. We also propose a modified transformer block to make training large masks more stable. Moreover, we design a novel style manipulation module to improve diversity.
		\item MAT sets new state of the arts on multiple benchmark datasets including Places~\cite{zhou2017places} and CelebA-HQ~\cite{karras2018progressive}. It also enables pluralistic completion.
	\end{itemize}
	


\section{Related Work}
	\label{sec:rela}
	
	Image completion has been a longstanding problem in computer vision. Early diffusion-based methods~\cite{bertalmio2000image, ballester2001filling} propagate neighboring undamaged information to the holes. Within an internal or external searching space, patch-based or exemplar-based approaches~\cite{hays2007scene,sun2005image,criminisi2004region,le2011examplar,criminisi2003object,ding2018image,lee2016laplacian} borrow patches with similar appearance based on human-defined distance metrics to complete missing regions. PatchMatch~\cite{barnes2009patchmatch} proposes a multi-scale patch searching strategy to accelerate the inpainting process. Moreover, partial differential equation~\cite{grossauer2004combined,bertalmio2006strong} and global or local image statistics~\cite{levin2003learning,ghorai2019multiple,fadili2009inpainting} are vastly studied in the literature. Though traditional methods can often obtain visually realistic results, the lack of high-level understanding hinders them from generating semantically reasonable contents.
	
	In the few years, deep learning has achieved great success on the image completion. Pathak \etal~\cite{pathak2016context} bring the adversarial training~\cite{goodfellow2014generative} to inpainting and utilize an encoder-decoder-based architecture to fill holes. Afterwards, numerous variants~\cite{yan2018shift,zeng2019learning,liu2020rethinking,wang2018image} of the U-Net structure~\cite{ronneberger2015u} have been developed for image completion. Besides, more sophisticated networks or learning strategies are proposed to generate high-quality images, including global and local discrimination~\cite{iizuka2017globally}, contextual attention~\cite{yu2018generative,liu2019coherent,xie2019image,yi2020contextual}, partial~\cite{liu2018image} and gated~\cite{yu2019free} convolution, \etc. Multi-stage generation has also received a great amount of attention, where intermediate clues like object edges~\cite{nazeri2019edgeconnect}, foreground contours~\cite{xiong2019foreground}, structures~\cite{ren2019structureflow} and semantic segmentation maps~\cite{song2018spg} are extensively exploited. To allow for high-resolution image inpainting, a few attempts have been made to develop progressively generation systems, such as~\cite{zhang2018semantic,guo2019progressive,li2020recurrent,zeng2020high,oh2019onion}.
	
	\begin{figure*}[t]
		\begin{center}
			\includegraphics[width=0.98\linewidth]{framework}
		\end{center}
		\vspace{-0.1in}
		\caption{The proposed mask-aware transformer (MAT) for pluralistic inpainting, which consists of a convolutional head, a transformer body and a convolutional tail for reconstruction together with a Conv-U-Net for refinement. The mask updating strategy is described in Sec.~\ref{sec:mca}.} \label{fig:framework}
		\vspace{-0.15in}
	\end{figure*}
	
	Recently, researchers switch their focus to more challenging settings, among which the most representative problems are pluralistic generation and large hole filling. For the former, Zheng \etal~\cite{zheng2019pluralistic} propose a probabilistically principled framework with two parallel paths, capable of producing multiple plausible solutions. UCTGAN~\cite{zhao2020uctgan} projects the instance image space and masked image space into a common low-dimensional manifold space via optimizing the KL-divergence to allow diverse generations of missing contents. Later on, ~\cite{wan2021high} and~\cite{yu2021diverse} take advantage of bidirectional attention or auto-regressive transformers to accomplish this goal. Although these methods improve the diversity, their completion and inference performances are limited due to the variational training and raster-scan-order-based generation. On the other hand, some works~\cite{ma2019region,zhao2020large,suvorov2021resolution,zheng2021tfill} are proposed to solve the large hole inpainting problem. For example, CoModGAN~\cite{zhao2020large} leverages the modulation techniques~\cite{chen2018self,karras2019style,karras2020analyzing} to handle large-scale missing regions. In this work, we develop a novel framework to simultaneously achieve high-quality pluralistic generation and large hole filling, bringing the best of long-range context interaction and unconditional generation to the image completion task.
	
\section{Method}
	\label{sec:method}
	
	Given a masked image, formulated as , image completion aims to hallucinate visually appealing and semantically appropriate contents for missing areas. In this work, we present a mask-aware transformer (MAT) for large mask inpainting, supporting conditional long-range interactions. 
	Besides, in light of the ill-posed nature of image completion problem, \ie, there could be numerous plausible solutions to fill the large holes, our approach is designed to support pluralistic generation.
	
	\subsection{Overall Architecture}
	\label{sec:arc}
	As shown in Fig.~\ref{fig:framework}, our proposed MAT architecture consists of a convolutional head, a transformer body, a convolutional tail and a style manipulation module, bringing the merits of transformers and convolutions into full play. Specifically, a convolutional head is used to extract tokens, then the main body with five stages of transformer blocks at varying resolutions (with different numbers of tokens) models long-range interactions via the proposed multi-head contextual attention (MCA). For the output tokens from the body, a convolution-based reconstruction module is adopted to upsample the spatial resolution to the input size. Moreover, we adopt another Conv-U-Net to refine high-frequency details, leaning upon the local texture refinement capability and efficiency of CNNs. At last, we design a style manipulation module, enabling the model to deliver diverse predictions by modulating the weights of convolutions. All components in our method are detailed below.
	








	\subsection{Convolutional Head}
	
	The convolutional head takes in the incompleted image  and the given mask , and produces  sized feature maps used for tokens. It contains four convolutional layers, one for changing the input dimension and others for downsampling the resolution.
	
	We utilize a convolutional head mainly for two reasons. First, the incorporation of local inductive priors in early visual processing remains vital for better representation~\cite{raghu2021vision} and optimizability~\cite{xiao2021early}. On the other hand, it is designed for fast downsampling to reduce computational complexity and memory cost. Also, we empirically find this design is better than the linear projection head used in ViT~\cite{dosovitskiy2020image}, as validated in the supplementary material. 
	
	\subsection{Transformer Body}
	The transformer body processes tokens by building long-range correspondences. It contains five stages of the proposed adjusted transformer blocks, with an efficient attention mechanism guided by an additional mask.
	
	\subsubsection{Adjusted Transformer Block}
	
	We propose a new transformer block variant to handle the optimization of masks with large holes. In detail, we remove the layer normalization (LN)~\cite{ba2016layer} and employ fusion learning (using feature concatenation) instead of residual learning. As shown in Fig.~\ref{fig:tran}, we concatenate the input and output of attention and use a fully connected (FC) layer:
	
	where  is the output of the MLP module of the -th block in the -th stage. After several transformer blocks, as illustrated in Fig.~\ref{fig:tran}, we adopt a convolution layer with a global residual connection. Note that we abandon the positional embedding in the transformer block since~\cite{xie2021segformer,wu2021cvt} have shown that  convolutions are sufficient to provide positional information for transformers. Thus, the flowing only depends on the feature similarity, which promotes long-range interactions.
	
	\begin{figure}[t]
		\begin{center}
			\includegraphics[width=0.95\linewidth]{transformer}
		\end{center}
		\vspace{-0.1in}
		\caption{Structure of a single transformer stage. ``TB'' refers to an adjusted transformer block and ``MCA'' represents the proposed multi-head contextual attention. The valid tokens are denoted as  and invalid tokens are . The blue arrow indicates the output of attention is computed as the weighted sum of valid tokens (indicated by blue arrows) while ignoring invalid tokens.}
		\label{fig:tran}
		\vspace{-0.1in}
	\end{figure}
	
	\vspace{0.08in}
	\noindent{\textbf{Analysis.}}
	The general architecture of transformer~\cite{vaswani2017attention} contains two sub-modules, a multi-head self-attention (MSA) module and an MLP module. Layer normalization is applied before every module and a residual connection~\cite{he2016deep} after every module. Whereas, we observe unstable optimization using the general block when handling large-scale masks, sometimes incurring gradient exploding. We attribute this training issue to the large ratio of invalid tokens (their values are nearly zero). In this circumstance, layer normalization may magnify useless tokens overwhelmingly, leading to unstable training. Besides, residual learning generally encourages the model to learn high-frequency contents. However, considering most tokens are invalid at the beginning, it is difficult to directly learn high-frequency details without proper low-frequency basis in GAN training, which makes the optimization harder. Replacing such residual learning with concatenation leads to obviously superior results, as verified in Sec.~\ref{sec:abl}.

	
	\subsubsection{Multi-Head Contextual Attention}
	\label{sec:mca}
	
	To handle a large number of tokens (up to 4096 tokens for  images) and low fidelity in the given tokens (at most 90 tokens are useless), our attention module exploits shifted windows~\cite{liu2021Swin} and a dynamical mask, capable of conducting non-local interactions using a few feasible tokens. The output is computed as the weighted sum of valid tokens, as shown in Fig.~\ref{fig:tran}, which is formulated as
	
	where  are the query, key, value matrices and  is the scaling factor. The mask  is expressed as:
	
	where  is a large positive integer (100 in our experiments). In this case, the aggregation weights of invalid tokens are nearly 0. After each attention, we shift the positions of  sized windows by  pixels, enabling cross-window connections.
	
	\vspace{0.08in}
	\noindent{\textbf{Mask Updating Strategy.}}
	The mask () points out whether a token is valid or invalid, which is initialized by the input mask and automatically updated during propagation. The updating follows a rule that all tokens in a window are updated to be valid after attention as long as there is at least one valid token before. If all tokens in a window are invalid, they remain invalid after attention. As shown in Fig.~\ref{fig:update}, going through an attention from (a) to (b), all tokens in the top left window become valid, while tokens in other windows are still invalid. After several times of window shift and attention, the mask is updated to be fully valid.
	
	\vspace{0.08in}
	\noindent{\textbf{Analysis.}}
	For images dominated by missing regions, the default attention strategy not only fails to borrow visible information to inpaint the holes, but also undermines the effective valid pixels. To reduce color discrepancy or blurriness, we propose to only involve valid tokens (selected by a dynamic mask) for computing relations. The effectiveness of our design is manifested in Sec~\ref{sec:abl}.
	
	\begin{figure}[t]
		\begin{center}
			\includegraphics[width=0.85\linewidth]{update}
		\end{center}
		\vspace{-0.2in}
		\caption{Toy example of mask updating. The feature map is initially partitioned into  windows (in orange). ``U'' means a mask updating after attention and ``S'' indicates the window shift.  }
		\label{fig:update}
		\vspace{-0.1in}
	\end{figure}
	
	
	\subsection{Style Manipulation Module} 
	Inspired by~\cite{chen2018self,karras2019style,karras2020analyzing}, we design a style manipulation module to endow our framework with pluralistic generation. It manipulates the output by changing the weight normalization of convolution layers in the reconstruction procedure with an additional noise input.
	To enhance the representation ability of noise inputs, we enforce the image-conditional style  to learn from both the image feature  and the noise-unconditional style , formulated as
	
	where  is a random binary mask, on which values are set to 1 with a probability of  and to 0 with ,  and  are mapping functions. As shown in Fig.~\ref{fig:framework}, the style representation is obtained by fusing both style representations:
	\vspace{-0.1in}
	
	where  is a mapping function.
	Then the weights  of convolutions are baked as
	
	where  denotes the input channels, output channels and spatial footprint of the convolution, respectively, and  is a small constant. The modulation of different style representations leads to pluralistic outputs. Also, we incorporate the noise injection~\cite{karras2019style} into our framework to further enhance the diversity of generation.
	
	\subsection{Loss Functions}
	\label{sec:lf}
	To improve the quality and diversity of the generation, we adopt the non-saturating adversarial loss~\cite{goodfellow2014generative} for both two stages to optimize our framework, regardless of the pixel-wise MAE or MSE loss that usually leads to averaged blurry results. We also use the  regularization~\cite{mescheder2018training,ross2018improving}, written as . Besides, we adopt the perceptual loss~\cite{johnson2016perceptual} with an empirically low coefficient since we notice it enables easier optimization.
	
	\vspace{0.08in}
	\noindent{\textbf{Adversarial Loss.}}
	We calculate the adversarial loss as
	
	where  and  are the real and generated images. We apply adversarial loss to both two-stage generations in Fig.~\ref{fig:framework}.
	
	\vspace{0.08in}
	\noindent{\textbf{Perceptual Loss.}}
	The perceptual loss is formulated as
	
	where  denotes the layer activation of a pre-trained VGG-19~\cite{Simonyan15} network. We only consider the high-level features of  and , allowing for variations of inpainted results, with scaling coefficients  as  and .
	
	\vspace{0.08in}
	\noindent{\textbf{Overall Loss.}}
	The overall loss of the generator is
	
	where  and .
	
\section{Experiments}
	\label{sec:exp}
	
	\subsection{Datasets and Metrics}
	We conduct experiments on the Places365-Standard~\cite{zhou2017places} and the CelebA-HQ~\cite{karras2018progressive} datasets at  resolution. Specifically, on the Places dataset, we use the 1.8 million and 36.5 thousand images from train and validation sets to train and evaluate our models, respectively. Images are randomly cropped or padded to  size during training while centrally cropped or padded for evaluation. For CelebA-HQ, train and validation splits are organized with 24,183 and 2,993 images. Though trained on  images, we show our model generalizes well to a larger resolution in the supplementary material.
		
	\begin{figure*}[t]
		\begin{center}
			\includegraphics[width=1.0\linewidth]{ablation}
		\end{center}
		\vspace{-0.2in}
		\caption{Visual examples for ablation study. Model A is our full model, while model B, C, D refer to models replacing transformers with convolutions, using the conventional transformer block and multi-head attention, respectively.}
		\label{fig:ablation}
		\vspace{-0.1in}
	\end{figure*}
	
	In terms of the large hole setting, following~\cite{zhao2020large}, we opt for perceptual metrics including FID~\cite{heusel2017gans}, P-IDS~\cite{zhao2020large} and U-IDS~\cite{zhang2018unreasonable} for evaluation. We suggest that it is inappropriate to use the pixel-wise L1 distance, PSNR and SSIM~\cite{wang2004image}, since preliminary works~\cite{ledig2017photo,sajjadi2017enhancenet} have shown that these metrics correlate weakly with human perception regarding image quality, especially for the ill-posed large-scale image completion problem. Though LPIPS~\cite{zhang2018unreasonable} is calculated in the deep feature space, the pixel-wise evaluation still greatly punishes diverse inpainting systems for large holes. Thus we only use it for reference in the supplementary material. 



	\subsection{Implementation Details}
	
	In our framework, we set the numbers of convolution channels and FC dimensions to 180 for the head, body, and reconstruction modules. The block numbers and window sizes of 5-level transformer groups are   and , respectively. The last Conv-U-Net firstly downsamples the resolution to  and then upsamples to the original size, where the numbers of convolution layers and channels at different scales are borrowed from~\cite{karras2020analyzing}. The mapping network consists of 8 FC layers and the style manipulation module is implemented with convolutions followed with an AvgPool layer. Different from~\cite{wan2021high,zheng2021tfill,yu2021diverse}, our transformer architecture is \textit{without} pre-training.
	
	All experiments are carried out on 8 NVidia V100 GPUs. Following~\cite{zhao2020large}, we train our models for 50M images on Places365-Standard and 25M images on CelebA-HQ. The batch size is 32. We adopt an Adam optimizer with  and  and set the learning rate to . The free-form mask is described in the supplementary file. 

	
	\subsection{Ablation Study}
	\label{sec:abl}
	In this section, we tease apart which components of our framework contribute most to the final performance. To enable a quick exploration, we only use 100K training images in Places~\cite{zhou2017places} () at  resolution and train the models for 5M samples ( of the full setting). We adopt the first 10K validation images for evaluation. The quantitative comparison is shown in Table~\ref{tab:ablation}.
	


	\vspace{0.05in}
	\noindent{\textbf{Conv-Transformer Architecture.}} We explore whether the long-range context relations modeled by transformers are useful for filling large holes. Replacing the transformer blocks with convolution blocks (model ``B'' in Table~\ref{tab:ablation}), we find an obvious performance drop on all metrics, especially on P-IDS and U-IDS, indicating that the inpainted images lose some fidelity. Moreover, we show some visual examples in Fig.~\ref{fig:ablation}. Compared to the fully convolutional network, our MAT takes advantage of distant context to reconstruct the structure of net and texture of dinosaur skeleton well, showing the effectiveness of long-range interactions.
	
	\begin{table}[t]
		\renewcommand\arraystretch{1.1}
		\begin{center}
			\resizebox{\linewidth}{!}{
				\begin{tabular}{c | l | c | c| c}
					\hline
					Type & Model & FID & P-IDS () & U-IDS() \\
					\hline
					A & Full Model & \textbf{5.97} & \textbf{13.17} & \textbf{29.23}\\
					\hline
					B & - Tran. & 6.21 & 11.30 & 27.39 \\	
					C & - Adjusted Tran. Block & 6.36 & 12.30 & 28.05 \\
					D & - MCA & 6.08 & 13.13 & 29.19 \\
					E & - Style Mani. Module & 6.10 & 11.88 & 27.94 \\
					\hline
					F & - High-Res. Gen. & 6.32 & 12.57 & 28.21 \\
					\hline
				\end{tabular}	
			}
		\end{center}
		\vspace{-0.1in}
		\caption{Ablation study of the framework components. ``A'' represents our full model. ``B'' replaces transformers with convolutions. ``C'' replaces our adjusted transformer block with the original design~\cite{vaswani2017attention}. ``D'' means using the conventional attention strategy.  ``E'' removes the noise style manipulation. ``F'' limits the output size of first-stage generation to . }
		\label{tab:ablation}
		\vspace{-0.2in}
	\end{table}







	
	\begin{figure}[t]
		\begin{center}
			\includegraphics[width=1.0\linewidth]{style}
		\end{center}
		\vspace{-0.1in}
		\caption{Visual examples with different style representations.} \label{fig:interpolation}
		\vspace{-0.1in}
	\end{figure}
	
	\vspace{0.05in}
	\noindent{\textbf{Adjusted Transformer Block.}} In our framework, we develop a novel transformer block since the conventional design easily leads to unstable optimization, in which case we need to lower the learning rate of transformer body. As illustrated in Table~\ref{tab:ablation}, our design (model ``A'') obtains superior performance, 0.39 improvement on FID, than model ``C'' with the original transformer block. As illustrated in Fig.~\ref{fig:ablation}, we notice our design produces more visually appealing results, supporting high-quality image completion. Especially for the first example, even though the missing area is extremely large, our method can still recover a semantically consistent and visually realistic indoor scene.
	
	
	\vspace{0.05in}
	\noindent{\textbf{Multi-Head Contextual Attention.}} To quickly fill the missing regions with realistic contents, we propose a multi-head contextual attention (MCA). To make a deeper understanding, we build a model without partial aggregation from valid tokens. The quantitative results are shown as model ``D'' in Table~\ref{tab:ablation}. It is noted that FID drops by 0.1 yet other metrics do not change too much. We suggest the proposed contextual attention is helpful for maintaining color consistency and reducing blurriness. As illustrated in Fig.~\ref{fig:ablation}, the model without MCA generates contents with incorrect colors for the first example, while producing blurry artifacts for the second example. Both the quantitative and qualitative results validate the power of our MCA.
	
	\vspace{0.05in}
	\noindent{\textbf{Style Manipulation Module.}} To deal with large masks, apart from the conditional long-range interaction, we also introduce unconditional generation. To quantify the unconditional generative capability of our framework, we strip the noise style manipulation. From the results of model ``E'' in Table~\ref{tab:ablation}, we find a large gap on P-IDS and U-IDS, showing the modulation of stochastic noise styles further improves the naturalness of completed images.
	
	\vspace{0.05in}
	\noindent{\textbf{High Resolution in Reconstruction.}} Due to quadratically increased computational complexity, existing works~\cite{wan2021high,zheng2021tfill,yu2021diverse} adopt transformers to synthesize low-resolution results, typically , for subsequent processing. By contrast, our MAT architecture takes advantage of its computational efficiency to enable high-resolution outputs in the reconstruction stage. As illustrated in Table~\ref{tab:ablation}, our full model ``A'' achieves significant improvement over model ``F'', demonstrating the importance of high-resolution prediction.
	






	\subsection{Comparison with State of the Arts}
	We compare the proposed MAT with a number of state-of-the-art approaches. For a fair comparison, we use publicly available models to test on the same masks. As illustrated in Table~\ref{tab:sota}, MAT achieves state-of-the-art performance on both CelebA-HQ and Places. Especially, even if we only use a subset Places365-Standard (1.8M images) to train our model, much fewer than CoModGAN~\cite{zhao2020large} (8M images) and Big LaMa~\cite{suvorov2021resolution} (4.5M images), MAT still yields promising results. Besides, our method is much more parameter-efficient than the second-best CoModGAN and transformer-based ICT~\cite{wan2021high}. As illustrated in Fig~\ref{fig:qualitative}, compared to other methods, the proposed MAT restores more photo-realistic images with fewer artifacts. For example, our method successfully recovers visually pleasing flowers and regular building structures. 

	




\subsection{Pluralistic Generation}
	\label{sec:pluralistic}
	
The inherent diversity of our framework mainly sources from the style manipulation. As shown in Fig.~\ref{fig:interpolation}, style variants lead to different completions. From the first example in Fig.~\ref{fig:interpolation}, we observe a change from a pursed smile to a toothy laugh. And the second example shows different face contours and appearances. As for the final one, we find different window and roof structures.
	






	\begin{figure}[t]
		\begin{center}
			\includegraphics[width=1.0\linewidth]{failure}
		\end{center}
		\vspace{-0.1in}
		\caption{Failure cases of our method (MAT). }
		\label{fig:failure}
		\vspace{-0.1in}
	\end{figure}
	
	\begin{table*}[t]
		\renewcommand\arraystretch{1.2}
		\setlength\tabcolsep{2pt}
		\begin{center}
			\resizebox{\linewidth}{!}{
				\begin{tabular}{c | c | c c c | c c c | c c c | c c c }
					\hline
					\multirow{3}{*}{Method} & \multirow{3}{*}{\tabincell{c}{\#Param. \\ }} & \multicolumn{6}{|c}{Places ()} & \multicolumn{6}{|c}{CelebA-HQ ()} \\
					\cline{3-14}
					~ & ~ & \multicolumn{3}{|c|}{Small Mask} & \multicolumn{3}{|c|}{Large Mask} & \multicolumn{3}{|c|}{Small Mask} & \multicolumn{3}{|c}{Large Mask} \\
					\cline{3-14}
					~ & ~ & FID & P-IDS() & U-IDS() & FID& P-IDS() & U-IDS() & FID & P-IDS() & U-IDS() & FID & P-IDS() & U-IDS() \\
					\hline
					\textbf{MAT (Ours)} & \multirow{2}{*}{62} & \textcolor{red}{0.78} & \textcolor{red}{31.72} & \textcolor{red}{43.71} & \textcolor{red}{1.96} & \textcolor{red}{23.42} & \textcolor{red}{38.34} & \multirow{2}{*}{\textcolor{red}{2.86}} & \multirow{2}{*}{\textcolor{red}{21.15}} & \multirow{2}{*}{\textcolor{red}{32.56}} & \multirow{2}{*}{\textcolor{red}{4.86}} & \multirow{2}{*}{\textcolor{red}{13.83}} & \multirow{2}{*}{\textcolor{red}{25.33}} \\
					\textbf{MAT (Ours)} & ~ & 1.07 & \textcolor{blue}{27.42} & \textcolor{blue}{41.93} & \textcolor{blue}{2.90} & 19.03 & 35.36 & ~ & ~ & ~ & ~ & ~ & ~ \\
					\hline
					CoModGAN~\cite{zhao2020large} & 109 & 1.10 & 26.95 & 41.88 & 2.92 & \textcolor{blue}{19.64} & \textcolor{blue}{35.78} & \textcolor{blue}{3.26} & \textcolor{blue}{19.65} & \textcolor{blue}{31.41} & \textcolor{blue}{5.65} & \textcolor{blue}{11.23} & \textcolor{blue}{22.54} \\
					LaMa~\cite{suvorov2021resolution} & 51/27 & \textcolor{blue}{0.99} & 22.79 & 40.58 & 2.97 & 13.09 & 32.29 & 4.05 & 9.72 & 21.57 & 8.15 & 2.07 & 7.58 \\
ICT~\cite{wan2021high} & 150 & - & - & - & - & - & - & 6.28 & 2.24 & 9.99 & 12.84 & 0.13 & 0.58 \\
					MADF~\cite{zhu2021image} & 85 & 2.24 & 14.85 & 35.03 & 7.53 & 6.00 & 23.78 & 3.39 & 12.06 & 24.61 & 6.83 & 3.41 & 11.26 \\
					AOT GAN~\cite{zeng2021aggregated} & 15 & 3.19 & 8.07 & 30.94 & 10.64 & 3.07 & 19.92 & 4.65 & 7.92 & 20.45 & 10.82 & 1.94 & 6.97 \\	
					HFill~\cite{yi2020contextual} & 3 & 7.94 & 3.98 & 23.60 & 28.92 & 1.24 & 11.24 & - & - & - & - & - & - \\
					DeepFill v2~\cite{yu2019free} & 4 & 3.02 & 9.17 & 32.56 & 9.27 & 4.01 & 21.32 & 10.11 & 3.11 & 9.52 & 24.42 & 0.17 & 0.42\\
					EdgeConnect~\cite{nazeri2019edgeconnect} & 22 & 4.03 & 5.88 & 27.56 & 12.66 & 1.93 & 15.87 & 10.58 & 4.14 & 12.45 & 39.99 & 0.10 & 0.22 \\
\hline  
				\end{tabular}
			}
		\end{center}
		\vspace{-0.15in}
\caption{Quantitative comparison on Places~\cite{zhou2017places} and CelebA-HQ~\cite{karras2018progressive}. ``'': Our Mat, CoModGAN~\cite{zhao2020large} and LaMa~\cite{suvorov2021resolution} use 8M, 8M and 4.5M training images on Places, respectively, while our other model (without ``'') is only trained on a subset (1.8M images). The LaMa models on Places and CelebA are different in size. The results of LPIPS and  CelebA are provided in the supplementary. The \textcolor{red}{best} and \textcolor{blue}{second best} results are in red and blue.} \label{tab:sota}
	\end{table*}



	\begin{figure*}[t]
		\begin{center}
			\includegraphics[width=1.0\linewidth]{sota}
		\end{center}
		\vspace{-0.15in}
		\caption{Qualitative comparison () with state-of-the-art methods. Our results are more visually realistic, containing more details.}
		\label{fig:qualitative}
		\vspace{-0.05in}
	\end{figure*}
	


	\subsection{Limitations and Failure Cases}
	Trained without semantic annotations, MAT usually struggles when processing objects with a variety of shapes, e.g., running animals. As shown in Fig.~\ref{fig:failure}, our method fails to recover the cat and car due to the lack of semantic context understanding. Also, limited by the downsampling and pre-defined window sizes in attention, we need to pad or resize an image to make its size a multiple of 512. 

	
	\section{Conclusion}
	
	We have presented a mask-aware transformer (MAT) for pluralistic large hole image inpainting. Taking advantage of the proposed adjusted transformer architecture and partial attention mechanism, the proposed MAT achieves state-of-the-art performance on multiple benchmarks. Also, we design a style modulation module to improve the diversity of generation. Extensive qualitative comparisons have demonstrated the superiority of our framework in terms of image quality and diversity. 

	{\small
		\bibliographystyle{ieee_fullname}
		\bibliography{egbib}
	}
	
	\clearpage
	
	\renewcommand\thesection{\Alph{section}}
	\renewcommand\thesubsection{\thesection.\arabic{subsection}}
	\renewcommand\thefigure{\Alph{section}.\arabic{figure}}
	\renewcommand\thetable{\Alph{section}.\arabic{table}} 
	
	\setcounter{section}{0}
	\setcounter{figure}{0}
	\setcounter{table}{0}
	
	\twocolumn[
	\begin{@twocolumnfalse}
		\begin{center}
			\noindent{\Large{\textbf{MAT: Mask-Aware Transformer for Large Hole Image Inpainting \\ 
						\vspace{0.1in}
						(Supplementary Material)}}}
		\end{center}
		\vspace{0.3in}
	\end{@twocolumnfalse}
	]
	

	\section{Network Architecture}
	\label{sec:net}
	As illustrated in Sec.~\textcolor{red}{3.1}, the proposed MAT is a two-stage framework, where the first stage consists of a convolutional head, a transformer body and a convolutional reconstruction tail while the second stage is a Conv-U-Net. And the discriminator follows the design of CoModGAN~\cite{zhao2020large}.
	
	Given an  input, the head first applies a convolution to change the number of channels from 4 (image 3 + mask 1) to 180 and then adopts three strided convolutions (stride = 2) to downsample the feature size to . The feature is transformed to tokens as input to the transformer body. The body is composed of five stages of transformer blocks, where the block numbers are  and the corresponding feature sizes are . The downsampling and upsampling are realized by convolutions. The detailed structure of a transformer block is shown in Sec.~\textcolor{red}{3.3}. Then the output tokens from the body are converted to a 2D feature, passed to the reconstruction tail. The convolutional tail upsamples the feature size from  to  and generates a completed image, during which style modulation is applied to all layers to enable pluralistic generation.
	
	The second-stage Conv-U-Net takes in the coarse prediction and the input mask for subsequent high-fidelity detail rendering. It first downsamples the feature size to  and then upsamples the size back to . Shortcut connections are adopted at each resolution. The number of convolution channels in the encoder starts from 64 and is doubled after each downsampling, with a maximum of 512, while the decoder uses a symmetrical setting. Besides, all decoding layers are modulated by the image-conditional and noise-unconditional style representations.
	
	\begin{figure}[t]
		\begin{center}
			\includegraphics[width=1.0\linewidth]{mask}
		\end{center}
		\vspace{-0.1in}
		\caption{Examples of free-form masks (). Visible and invisible pixels are in white and black colors.}
		\label{fig:mask}
	\end{figure}
	
	\begin{figure}[t]
		\begin{center}
			\includegraphics[width=1.0\linewidth]{mask_statistics}
		\end{center}
		\vspace{-0.15in}
		\caption{Small and large mask () statistics on the Places Val set~\cite{zhou2017places}. The are totally 36500 masks.}
		\label{fig:statistics}
		\vspace{-0.2in}
	\end{figure}
	
	\section{Free-Form Mask Sampling and Statistics}
	\label{sec:mask}
	
	Referring to DeepFill v2~\cite{yu2019free}, we sample rectangles and brush strokes with random sizes, shapes and locations to generate free-form masks. During training, we use a large mask sampling strategy. The number of up to full-size or half-size rectangles is uniformly sampled within  or . The number of strokes is randomly sampled within [0, 9], with a random brush width within  and vertex number within . During testing, apart from the large mask setup, we also introduce a small mask sampling strategy, where the number of up to full-size or half-size rectangles is within  or  and the number of strokes is within [0, 4], while other settings remain unchanged. Note that our model is trained on large masks and is evaluated on both small and large mask settings. As shown in Fig.~\ref{fig:statistics}, we present the mask statistics on the Places Val set~\cite{zhou2017places} that is used for evaluation. It is observed that large masks are very aggressive and diverse.
	
	\section{Tokenization}
	\label{sec:token}
	
	As described in Sec.~\ref{sec:net}, we adopt a stack of convolutions (the convolutional head) to extract tokens for the transformer body, which is specially tailored to the inpainting problem. Compared to the linear projection of ViT~\cite{dosovitskiy2020image}, our design owns two merits. First, stacked convolutions can gradually fill the holes, producing more effective tokens. Second, the multi-scale downsampled features can be passed to the decoder through shortcut connections, improving the optimization. As illustrated in Table~\ref{tab:lp} and Fig.~\ref{fig:token}, stacked convolutions obtain obviously superior results. The model using linear projection is more likely to generate unpleasing artifacts and fail to borrow surrounding textures to fill the holes, while our MAT successfully recovers high-fidelity contents. Both the quantitative and qualitative results demonstrate the effectiveness of our MAT.
	
	\begin{table}[t]
		\renewcommand\arraystretch{1.1}
		\small
		\begin{center}
\begin{tabular}{ l | c | c| c}
				\hline
				Model & FID & P-IDS () & U-IDS() \\
				\hline
				Stacked Conv. (Ours) & \textbf{5.97} & \textbf{13.17} & \textbf{29.23}\\
				Linear Projection & 10.54 & 5.77 & 20.86 \\
				\hline
			\end{tabular}	
\end{center}
		\vspace{-0.15in}
		\caption{Quantitative comparison between linear projection and stacked convolutions for token extraction. We use the same training setting as the ablation study (Sec.~\textcolor{red}{4.3}).}
		\label{tab:lp}
	\end{table}
	
	\begin{figure}[t]
		\begin{center}
			\includegraphics[width=1.0\linewidth]{token}
		\end{center}
		\vspace{-0.15in}
		\caption{Qualitative comparison between linear projection and stacked convolutions (ours) for tokenization.}
		\label{fig:token}
		\vspace{-0.1in}
	\end{figure}
	


	\section{Model Configuration}
	Following the same experimental setting as ablation study, we explore several model variants in terms of feature width, block number and window size of the transformer body, leaving Conv-U-Net unchanged. The results are shown in the Table~\ref{tab:config}. The performance is positively correlated to the model capacity and attention range. 

	\begin{table}[t]
		\small
		\centering
		\resizebox{\linewidth}{!}{
			\begin{tabular}{c | c | c | c | c }
				\hline
				Model & Feature Dim. & Block Num. & Window Size & FID \\
				\hline
				Ours & 180 &  &  & \textbf{5.97} \\
				\hline
				V1 & 90 &  &  & 6.28 \\
				V2 & 180 &  &  & 6.18 \\
				V3 & 180 &  &  & 6.09 \\
				\hline
			\end{tabular}
		}
\caption{Ablation study on model configuration.}
		\label{tab:config}
	\end{table}
	
	\section{CelebA-HQ 256  256 Results}
	
	We provide the quantitative results on  CelebA-HQ~\cite{karras2018progressive}. As illustrated in Table~\ref{tab:celeba}, our MAT yields significant improvements on FID~\cite{heusel2017gans}, P-IDS~\cite{zhao2020large} and U-IDS~\cite{zhang2018unreasonable} metrics over other methods.
	
	\section{LPIPS Results}
	As discussed in Sec.~\textcolor{red}{4.1}, LPIPS~\cite{zhang2018unreasonable} is not an appropriate measure for large mask inpainting, especially for pluralistic generation systems, since there could be numerous plausible solutions to fill the holes. Therefore, we provide the LPIPS results only for reference. As shown in Table~\ref{tab:lpips}, our method achieves superior or comparable performance on the CelebA-HQ~\cite{karras2018progressive} and Places~\cite{zhou2017places} datasets. \textit{Note that we only use 22.5\% of full data to train our Places model}.
	
	\begin{table}[t]
		\renewcommand\arraystretch{1.2}
		\setlength\tabcolsep{3pt}
		\begin{center}
			\resizebox{\linewidth}{!}{
				\begin{tabular}{c | c c c | c c c}
					\hline
					\multirow{2}{*}{Method} & \multicolumn{3}{|c}{Small Mask} & \multicolumn{3}{|c}{Large Mask} \\
					\cline{2-7}
					~ & FID & P-IDS & U-IDS & FID & P-IDS & U-IDS \\
\hline
					MAT (Ours) & \textbf{2.94} & \textbf{20.88} & \textbf{32.01} & \textbf{5.16} & \textbf{13.90} & \textbf{25.13} \\
					\hline
					LaMa~\cite{suvorov2021resolution} & 3.98 & 8.82 & 22.57  & 8.75 & 2.34 & 8.77 \\
					ICT~\cite{wan2021high} & 5.24 & 4.51 & 17.39 & 10.92 & 0.90 & 5.23 \\
					MADF~\cite{zhu2021image} & 10.43 & 6.25 & 14.62 & 23.59 & 0.50 & 1.44 \\
					AOT GAN~\cite{zeng2021aggregated} & 9.64 & 5.61 & 14.62 & 22.91 & 0.47 & 1.65 \\
					DeepFill v2~\cite{yu2019free} & 5.69 & 6.62 & 16.82 & 13.23 & 0.84 & 2.62 \\
					EdgeConnect~\cite{nazeri2019edgeconnect} & 5.24 & 5.61 & 15.65 & 12.16 & 0.84 & 2.31 \\
					\hline  
				\end{tabular}
			}
		\end{center}
		\vspace{-0.1in}
		\caption{Quantitative results on CelebA-HQ at  size. The results of P-IDS and U-IDS are shown in percentage ().}
		\label{tab:celeba}
	\end{table}
	
	\begin{table}[t]
		\renewcommand\arraystretch{1.1}
\begin{center}
			\resizebox{\linewidth}{!}{
				\begin{tabular}{c | c | c c | c c }
					\hline
					\multirow{2}{*}{Method} & \#Param. & \multicolumn{2}{|c}{CelebA-HQ} & \multicolumn{2}{|c}{Places} \\
					\cline{3-6}
					~ &  & Small & Large & Small & Large \\
\hline
					MAT (Ours) & 60 & \textbf{0.065} & \textbf{0.125} & 0.099 & 0.189 \\
					\hline
					CoModGAN~\cite{zhao2020large} & 109 & 0.073 & 0.140 & 0.101 & 0.192 \\
					LaMa~\cite{suvorov2021resolution} & 27/51 & 0.075 & 0.143 & \textbf{0.086} & \textbf{0.166} \\
					ICT~\cite{wan2021high} & 150 & 0.105 & 0.195 & - & - \\
					MADF~\cite{zhu2021image} & 85 & 0.068 & 0.130 & 0.095 & 0.181\\
					AOT GAN~\cite{zeng2021aggregated} & 15 & 0.074 & 0.145 & 0.101 & 0.195 \\
					HFill~\cite{yi2020contextual} & 3 & - & - & 0.148 & 0.284 \\
					DeepFill v2~\cite{yu2019free} & 4 & 0.117 & 0.221 & 0.113 & 0.213 \\
					EdgeConnect~\cite{nazeri2019edgeconnect} & 22 & 0.101 & 0.208 & 0.114 & 0.275 \\
					\hline  
				\end{tabular}
			}
		\end{center}
		\vspace{-0.1in}
		\caption{LPIPS~\cite{zhang2018unreasonable} comparison on  CelebA-HQ~\cite{karras2018progressive} and Places~\cite{zhou2017places} datasets. ``'': CoModGAN~\cite{zhao2020large} and LaMa~\cite{suvorov2021resolution} use 8M and 4.5M Places images to train their models, while our model is only trained on Places365-Standard (1.8M images). The LaMa models on CelebA-HQ and Places are different in size. }
		\label{tab:lpips}
	\end{table}
	
	\section{Generalization to A Higher Resolution}
	Though trained on  images, our model generalizes well to larger resolutions. For example, we transfer our model and Big LaMa~\cite{suvorov2021resolution} trained at  resolution to . Compared to Big LaMa (FID 6.31, PIDS 4.98), our model (FID 5.83, P-IDS 9.51) obtains superior results on Places under the large mask setting. We suggest that maintaining a resolution consistency during training and testing yields better visual quality.
	
	\section{Diversity-Fidelity Tradeoff}
	To evaluate the fidelity and diversity, apart from FID (depending on both diversity and fidelity), we also follow~\cite{kynkaanniemi2019improved,dhariwal2021diffusion} to use Improved Precision and Recall to separately measure sample fidelity (precision) and diversity (recall). As shown in Table~\ref{tab:tradeoff}, our method obtains better FID, higher recall yet slightly lower precision compared to CoModGAN on Places. It is noted that we use much less training data.
	
	\begin{table}[t]
		\small
		\centering
		\resizebox{\linewidth}{!}{
			\begin{tabular}{c | c | c | c | c}
				\hline
				Method & Training Data & FID & Precision & Recall \\
				\hline
				\textbf{MAT (Ours)} & \textbf{1.8M} & \textbf{2.90} & 0.925 & \textbf{0.951} \\
				CoModGAN & 8M & 2.92 & \textbf{0.929} & 0.942 \\
				\hline
			\end{tabular}
		}
		\caption{Precision and Recall results of our MAT and CoModGAN on Places.}
		\label{tab:tradeoff}
\end{table}
	




	\section{Additional Qualitative Results}
	\label{sec:visual}
	
	We present more visual comparisons on the Places~\cite{zhou2017places} dataset between our MAT and other state-of-the-art methods. As shown in Fig~\ref{fig:sota1} and Fig~\ref{fig:sota2}, our method generates more photo-realistic results with few artifacts, manifesting the effectiveness of MAT. Due to potential copyright issues with CelebA-HQ~\cite{karras2018progressive}, we do not provide visual comparisons on this dataset. If necessary, you can process CelebA-HQ images with the provided code and model, or contact the authors. 
	
	\section{Licenses of Face Images}
	\label{sec:license}
	All face images used in the paper and supplementary material are from the FFHQ~\cite{karras2019style} dataset. Here we provide the detailed information on source and license.
	\begin{itemize}
		\item Face image in Fig.1 of main paper, source: \url{https://www.flickr.com/photos/v63/5876049365/}, license: CC BY-NC 2.0 (\url{https://creativecommons.org/licenses/by-nc/2.0/}).
		
		\item Face image in Fig.2 of main paper, source: \url{https://www.flickr.com/photos/tbisaacs/4089001580/}, license: CC BY 2.0 (\url{https://creativecommons.org/licenses/by/2.0/}).
		
		\item The first face image in Fig.6 of main paper, source: \url{https://www.flickr.com/photos/southlanarkshirecouncil/8341157963/},  license: CC BY-NC 2.0 (\url{https://creativecommons.org/licenses/by-nc/2.0/}).
		
		\item The second face image in Fig.6 of main paper, source: \url{https://www.flickr.com/photos/afge/34804627253/}, license: CC BY 2.0 (\url{https://creativecommons.org/licenses/by/2.0/}).
	\end{itemize}
	
	
	\begin{figure*}[t]
		\begin{center}
			\includegraphics[width=1.0\linewidth]{sota1}
		\end{center}
		\caption{Qualitative comparison () with state-of-the-art methods on the Places dataset. Zoom in for a better view.}
		\label{fig:sota1}
	\end{figure*}
	
	\begin{figure*}[t]
		\begin{center}
			\includegraphics[width=1.0\linewidth]{sota2}
		\end{center}
		\caption{Qualitative comparison () with state-of-the-art methods on the Places dataset. Zoom in for a better view.}
		\label{fig:sota2}
	\end{figure*}
	
\end{document}
