\documentclass[11pt, letterpaper]{article}
\usepackage{fullpage}



\usepackage[T1]{fontenc}


\usepackage[colorlinks=true,pdfpagemode=none,linkcolor=blue,citecolor=blue]{hyperref}
\usepackage{amsmath,amsfonts,amsthm}
\usepackage[figure,boxed,lined]{algorithm2e}
\usepackage{color}
\usepackage{multirow}
\usepackage{enumerate}
\usepackage{graphicx}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{warning}[theorem]{Warning}
\newtheorem{invariant}[theorem]{Invariant}

\newcommand{\cdelta}{C_{\delta}}
\newcommand{\cendecrease}{C_{P}}
\newcommand{\crestrict}{C_{R}}
\newcommand{\cenergy}{C_{E}}
\newcommand{\ceta}{C_{\eta}}
\newcommand{\cbounded}{C_{\ovkappa}}

\newcommand{\cheavy}{C_{H}}
\newcommand{\fheavy}{F_{H}}
\newcommand{\cfreeze}{C_{F}}
\newcommand{\ckstar}{C_{K}}

\newcommand{\cinter}{C_{I}}
\newcommand{\cdiver}{C_{B}}
\newcommand{\clight}{C_{L}}
\newcommand{\cincrease}{C_{S}}
\newcommand{\cauxiliary}{C_{A}}
\newcommand{\fauxiliary}{F_{A}}


\newcommand{\todo}[1]{}
\newcommand{\bbR}{\mathbb{R}}
\newcommand{\onev}{\mathbf{1}}

\newcommand{\ceil}[1]{\lceil #1 \rceil}
\newcommand{\floor}[1]{\lfloor #1 \rfloor}

\newcommand{\chist}{\chi_{s,t}}
\newcommand{\bchist}{\boldsymbol{\mathit{\chi_{s,t}}}}

\newcommand{\norm}[2]{\|#1\|_{#2}}
\newcommand{\onorm}[1]{|#1|_{1}}
\newcommand{\tnorm}[1]{\|#1\|_{2}}
\newcommand{\fnorm}[1]{\|#1\|_{4}}
\newcommand{\inorm}[1]{\|#1\|_{\infty}}

\newcommand{\tO}[1]{\widetilde{O}(#1)}
\newcommand{\tOm}[1]{\widetilde{\Omega}(#1)}
\newcommand{\tT}[1]{\widetilde{\Theta}(#1)}

\newcommand{\hA}{\widehat{A}}
\newcommand{\oG}{\bar{G}}
\newcommand{\cG}{{G}}
\renewcommand{\oe}{\bar{e}}
\newcommand{\ob}{\bar{b}}
\newcommand{\tG}{\widetilde{G}}
\newcommand{\hG}{\widehat{G}}
\newcommand{\oV}{\bar{V}}
\newcommand{\cV}{{V}}
\newcommand{\tV}{\widetilde{V}}
\newcommand{\hV}{\widehat{V}}
\newcommand{\oE}{\bar{E}}
\newcommand{\cE}{{E}}
\newcommand{\tE}{\widetilde{E}}
\newcommand{\hE}{\widehat{E}} 
\newcommand{\oF}{\bar{F}}
\newcommand{\tF}{\widetilde{F}}
\newcommand{\hF}{\widehat{F}}
\newcommand{\hm}{\widehat{m}}
\newcommand{\tm}{\widetilde{m}}
\newcommand{\cm}{{m}}
\newcommand{\hn}{\hat{}}
\newcommand{\cn}{{n}}
\newcommand{\om}{\bar{m}}
\newcommand{\on}{\bar{n}}
\newcommand{\oq}{\bar{q}}
\newcommand{\op}{\bar{p}}
\newcommand{\ox}{\bar{x}}
\newcommand{\tS}{\tilde{S}}
\newcommand{\oS}{\bar{S}}
\newcommand{\hS}{\hat{S}}
\newcommand{\oT}{\bar{T}}
\newcommand{\hT}{\widehat{T}}


\newcommand{\pinv}[1]{{#1}^{\dagger}}
\newcommand{\Reff}[2]{R_{\mathrm{eff}}^{#1}(#2)}
\newcommand{\Ceff}[2]{C_{\mathrm{eff}}^{#1}(#2)}



\newcommand{\energy}[2]{\mathcal{E}_{#1}(#2)}
\newcommand{\dist}[2]{\mathrm{dist}(#1,#2)}
\newcommand{\enE}[3]{E_{#2}^{#1}(#3)}
\newcommand{\Cset}[2]{S_{#1}(#2)}

\newcommand{\eps}{\varepsilon}


\newcommand{\bvec}[1]{{\mbox{\boldmath }}}

\newcommand{\tf}{\tilde{f}}
\newcommand{\hf}{\hat{f}}
\newcommand{\of}{\bar{f}}
\newcommand{\ohf}{\widehat{\bar{f}}}
\newcommand{\os}{\bar{s}}
\newcommand{\tr}{\tilde{r}}
\newcommand{\hr}{\hat{r}}
\newcommand{\oor}{\bar{r}}
\newcommand{\ot}{\bar{t}}
\newcommand{\ov}{\bar{v}}
\newcommand{\onu}{\bar{\nu}}
\newcommand{\hmu}{\hat{\mu}}
\newcommand{\ohmu}{\hat{\bar{\mu}}}
\newcommand{\tmu}{\tilde{\mu}}
\newcommand{\omu}{\bar{\mu}}
\newcommand{\ol}{\bar{l}}
\newcommand{\hl}{\hat{l}}
\newcommand{\hrho}{\hat{\rho}}
\newcommand{\hgamma}{\hat{\gamma}}
\newcommand{\hsigma}{\hat{\sigma}}
\newcommand{\htheta}{\hat{\theta}}
\newcommand{\otheta}{\bar{\theta}}
\newcommand{\okappa}{\bar{\kappa}}
\newcommand{\hkappa}{\hat{\kappa}}
\newcommand{\tkappa}{\tilde{\kappa}}
\newcommand{\halpha}{\widehat{\alpha}}
\newcommand{\hlambda}{\hat{\lambda}}



\newcommand{\vphi}{\boldsymbol{\mathit{\phi}}}
\newcommand{\ovphi}{\boldsymbol{\mathit{\bar{\phi}}}}
\newcommand{\vrho}{\boldsymbol{\mathit{\rho}}}
\newcommand{\vmu}{\boldsymbol{\mathit{\mu}}}
\newcommand{\vnu}{\boldsymbol{\mathit{\nu}}}
\newcommand{\ovnu}{\boldsymbol{\bar{\mathit{\nu}}}}
\newcommand{\hvmu}{\boldsymbol{\mathit{\hat{\mu}}}}
\newcommand{\ohvmu}{\boldsymbol{\mathit{\hat{\bar{\mu}}}}}
\newcommand{\tvmu}{\boldsymbol{\mathit{\tilde{\mu}}}}
\newcommand{\vsigma}{\boldsymbol{\mathit{\sigma}}}
\newcommand{\ovsigma}{\boldsymbol{\mathit{\bar{\sigma}}}}
\newcommand{\tvsigma}{\boldsymbol{\mathit{\tilde{\sigma}}}}
\newcommand{\hvsigma}{\boldsymbol{\mathit{\hat{\sigma}}}}
\newcommand{\tsigma}{\tilde{\sigma}}
\newcommand{\vkappa}{\boldsymbol{\mathit{{\kappa}}}}
\newcommand{\ovkappa}{\boldsymbol{\mathit{\bar{\kappa}}}}
\newcommand{\oPhi}{\bar{\Phi}}
\newcommand{\hphi}{\widehat{\phi}}
\newcommand{\ophi}{\bar{\phi}}
\newcommand{\tphi}{\widetilde{\phi}}
\newcommand{\hvphi}{\boldsymbol{\widehat{\phi}}}
\newcommand{\tvphi}{\boldsymbol{\tilde{\phi}}}
\newcommand{\hvkappa}{\boldsymbol{\hat{\kappa}}}
\newcommand{\tvkappa}{\boldsymbol{\tilde{\kappa}}}
\newcommand{\vlambda}{\boldsymbol{\lambda}}
\newcommand{\hvlambda}{\boldsymbol{\widehat{\lambda}}}


\renewcommand{\aa}{\boldsymbol{\mathit{a}}}
\newcommand{\bb}{\boldsymbol{\mathit{b}}}
\newcommand{\obb}{\boldsymbol{\mathit{\bar{b}}}}
\newcommand{\cc}{\boldsymbol{\mathit{c}}}
\newcommand{\dd}{\boldsymbol{\mathit{d}}}
\newcommand{\ee}{\boldsymbol{\mathit{e}}}
\newcommand{\ff}{\boldsymbol{\mathit{f}}}
\newcommand{\tff}{\boldsymbol{\mathit{\tilde{f}}}}
\newcommand{\off}{\boldsymbol{\mathit{\bar{f}}}}
\newcommand{\ohff}{\boldsymbol{\mathit{\widehat{\bar{f}}}}}
\newcommand{\hff}{\boldsymbol{\mathit{\hat{f}}}}
\renewcommand{\gg}{\boldsymbol{\mathit{g}}}
\newcommand{\hh}{\boldsymbol{\mathit{h}}}
\newcommand{\ii}{\boldsymbol{\mathit{i}}}
\newcommand{\jj}{\boldsymbol{\mathit{j}}}
\newcommand{\kk}{\boldsymbol{\mathit{k}}}
\renewcommand{\ll}{\boldsymbol{\mathit{l}}}
\newcommand{\hll}{\boldsymbol{\mathit{\hat{l}}}}
\newcommand{\oll}{\boldsymbol{\mathit{\bar{l}}}}
\newcommand{\mm}{\boldsymbol{\mathit{m}}}
\newcommand{\nn}{\boldsymbol{\mathit{n}}}
\newcommand{\oo}{\boldsymbol{\mathit{o}}}
\newcommand{\pp}{\boldsymbol{\mathit{p}}}
\newcommand{\qq}{\boldsymbol{\mathit{q}}}
\newcommand{\rr}{\boldsymbol{\mathit{r}}}
\newcommand{\hrr}{\boldsymbol{\mathit{\hat{r}}}}
\newcommand{\orr}{\boldsymbol{\mathit{\bar{r}}}}
\newcommand{\trr}{\boldsymbol{\mathit{\tilde{r}}}}
\renewcommand{\ss}{\boldsymbol{\mathit{s}}}
\newcommand{\oss}{\boldsymbol{\bar{\mathit{s}}}}
\renewcommand{\tt}{\boldsymbol{\mathit{t}}}
\newcommand{\uu}{\boldsymbol{\mathit{u}}}
\newcommand{\tuu}{\boldsymbol{\mathit{\tilde{u}}}}
\newcommand{\vv}{\boldsymbol{\mathit{v}}}
\newcommand{\ww}{\boldsymbol{\mathit{w}}}
\newcommand{\xx}{\boldsymbol{\mathit{x}}}
\newcommand{\oxx}{\boldsymbol{\bar{\mathit{x}}}}
\newcommand{\yy}{\boldsymbol{\mathit{y}}}
\newcommand{\zz}{\boldsymbol{\mathit{z}}}


\renewcommand{\AA}{\boldsymbol{\mathit{A}}}
\newcommand{\BB}{\boldsymbol{\mathit{B}}}
\newcommand{\CC}{\boldsymbol{\mathit{C}}}
\newcommand{\DD}{\boldsymbol{\mathit{D}}}
\newcommand{\EE}{\boldsymbol{\mathit{E}}}
\newcommand{\FF}{\boldsymbol{\mathit{F}}}
\newcommand{\GG}{\boldsymbol{\mathit{G}}}
\newcommand{\HH}{\boldsymbol{\mathit{H}}}
\newcommand{\II}{\boldsymbol{\mathit{I}}}
\newcommand{\JJ}{\boldsymbol{\mathit{J}}}
\newcommand{\KK}{\boldsymbol{\mathit{K}}}
\newcommand{\LL}{\boldsymbol{\mathit{L}}}
\newcommand{\MM}{\boldsymbol{\mathit{M}}}
\newcommand{\NN}{\boldsymbol{\mathit{N}}}
\newcommand{\OO}{\boldsymbol{\mathit{O}}}
\newcommand{\PP}{\boldsymbol{\mathit{P}}}
\newcommand{\QQ}{\boldsymbol{\mathit{Q}}}
\newcommand{\RR}{\boldsymbol{\mathit{R}}}
\renewcommand{\SS}{\boldsymbol{\mathit{S}}}
\newcommand{\TT}{\boldsymbol{\mathit{T}}}
\newcommand{\UU}{\boldsymbol{\mathit{U}}}
\newcommand{\VV}{\boldsymbol{\mathit{V}}}
\newcommand{\WW}{\boldsymbol{\mathit{W}}}
\newcommand{\XX}{\boldsymbol{\mathit{X}}}
\newcommand{\YY}{\boldsymbol{\mathit{Y}}}
\newcommand{\ZZ}{\boldsymbol{\mathit{Z}}}














 


 
\begin{document}


\clubpenalty=10000
\widowpenalty = 10000



\title{Navigating Central Path with Electrical Flows: from Flows to Matchings, and Back \\{\bf (Preliminary draft)}}

\author{Aleksander M\k{a}dry\thanks{Part of this work was done when the author was with Microsoft Research New England.}\\
       {EPFL}\\
       { aleksander.madry@epfl.ch}}
\date{}

\maketitle
\begin{abstract}

We present an -time\footnote{We recall that  denotes , for some constant .} algorithm for the maximum - flow and the minimum - cut problems in directed graphs with unit capacities. This is the first improvement over the sparse-graph case of the long-standing  running time bound due to Even and Tarjan \cite{EvenT75} and Karzanov \cite{Karzanov73}. By well-known reductions, this also establishes an -time algorithm for the maximum-cardinality bipartite matching problem. That, in turn, gives an improvement over the celebrated  running time bound of Hopcroft and Karp \cite{HopcroftK73} and Karzanov \cite{Karzanov73} whenever the input graph is sufficiently sparse. 

At a very high level, our results stem from acquiring a deeper understanding of interior-point methods -- a powerful tool in convex optimization -- in the context of flow problems, as well as, utilizing certain interplay between maximum flows and bipartite matchings.

The core of our approach comprises a primal-dual algorithm for {(near-)}perfect bipartite -matching problem. This algorithm is inspired by path-following interior-point methods and employs electrical flow computations to gradually improve the quality of maintained solution by advancing it toward (near-)optimality along so-called central path. To analyze this process, we establish a formal connection that ties its convergence rate to the structure of corresponding electrical flows. Then, we exploit that connection to obtain a convergence guarantee for our algorithm that improves upon the well-known barrier of  iterations corresponding to the generic worst-case performance bounds for interior-point-method-based algorithms. This improvement is based on refining certain insights into behavior of electrical flows that stem from the work of Christiano et al. \cite{ChristianoKMST11} and combining them with a new technique for preconditioning primal-dual solutions.

The final ingredient of our approach is a simple reduction of the maximum - flow problem to the bipartite -matching problem. This reduction is then composed with the recent sub-linear-time algorithm for finding perfect matchings in regular graphs of Goel et al. \cite{GoelKK10}, to derive an efficient procedure for rounding fractional - flows and bipartite matchings. 

 

\end{abstract}

\thispagestyle{empty}
\newpage
\setcounter{page}{1}







\section{Introduction}


  The maximum - flow problem and its dual, the minimum - cut problem,
  are two of the most fundamental and extensively studied graph problems in combinatorial optimization ~\cite{Schrijver03,AhujaMO93}. They have a wide range of applications (see~\cite{AhujaMOR95}), are often used as subroutines in other algorithms (see, e.g., \cite{AroraHK05,Sherman09}), and a number of other important problems  -- e.g., bipartite matching problem \cite{CormenLRS09} -- can be reduced to them.  Furthermore, these two problems were often a testbed for development of fundamental algorithmic tools and concepts. Most prominently, the Max-Flow Min-Cut theorem \cite{EliasFS56,FordF56} constitutes the prototypical primal-dual relation.
  
  Several decades of extensive work resulted in a number of developments on these problems (see Goldberg and Rao \cite{GoldbergR98} for an overview) and  many of their generalizations and special cases. Still, despite all this effort, the basic problem of computing maximum - flow and minimum - cut in general graphs has resisted progress for a long time. In particular, the current best running time bound of  (with  denoting the largest integer arc capacity) was established over 15 years ago in a breakthrough paper by Goldberg and Rao \cite{GoldbergR98} and this bound, in turn, matches the  bound for unit-capacity graphs that Even and Tarjan \cite{EvenT75} -- and, independently, Karzanov \cite{Karzanov73} -- put forth over 35 years ago. 
  
  Recently, however, important progress was made in the context of undirected graphs. Christiano et al. \cite{ChristianoKMST11} developed an algorithm that allows one to compute a -approximation to the undirected maximum - flow (and the minimum - cut) problem in  time. Their result relies on devising a new approach to the problem that combines electrical flow computations with multiplicative weights update method (see \cite{AroraHK05}). Later, Lee et al. \cite{LeeRS13} presented a quite different -- but still electrical-flow-based -- algorithm that employs purely gradient-descent-type view to obtain an -time -approximation for the case of unit capacities. Finally, very recently, this line of work was culminated by Sherman \cite{Sherman13} and Kelner et al. \cite{KelnerLOS13} who independently showed how to integrate non-Euclidean gradient-descent methods with fast poly-logarithmic-approximation algorithms for cut problems of M\k{a}dry \cite{Madry10b} to get an -time -approximation to the undirected maximum flow problem. 
  
Finally, we note that, in parallel to the above work that is focused on designing weakly-polynomial algorithms for the maximum - flow and minimum - cut problems, there is also a considerable interest in obtaining running time bounds that are strongly-polynomial, i.e., that do not depend on the values of arc capacities. The current best such bound is  and it follows by combining the algorithms of King et al. \cite{KingRT94} and Orlin \cite{Orlin13}. 
  
  
  
  
  
  
  
  
  \paragraph{Bipartite Matching Problem.}
  
  Another problem that we will be interested in is the (maximum-cardinality) bipartite matching problem -- a fundamental assignment problem with numerous applications (see, e.g., \cite{AhujaMO93,LovaszP86}) and long history. Already in 1931, K\"onig \cite{Konig31} and Egerv\'ary \cite{Egervary31} provided first constructive characterization of maximum matchings in bipartite graphs. This characterization can be turned into a polynomial-time algorithm. Then, in 1973, Hopcroft and Karp \cite{HopcroftK73} -- and, independently, Karzanov \cite{Karzanov73} -- devised the celebrated -time algorithm. Till date, this bound is the best one known in the regime of relatively sparse graphs. It can be improved, however, when the input graph is dense, i.e., when  is close to . In this case, one can combine the algebraic approach of Rabin and Vazirani \cite{RabinV89} -- that itself builds on the work of Tutte \cite{Tutte47} and Lov\'asz \cite{Lovasz79} -- with matrix-inversion techniques of Bunch and Hopcroft \cite{BunchH74} to get an algorithm that runs in  time (see \cite{Mucha05}), where  is the exponent of matrix multiplication \cite{CoppersmithW90, Vassilevska12}. Also, later on, Alt et al. \cite{AltBMP91}, as well as, Feder and Motwani \cite{FederM95} developed combinatorial algorithms that offer a slight improvement -- by a factor of, roughly,  -- over the  bound of Hopcroft and Karp whenever the graph is sufficiently dense. 
  
  Finally, it is worth mentioning that there was also a lot of developments on the (maximum-cardinality) matching problem in general, i.e., not necessarily bipartite, graphs. Starting with the pioneering work of Edmonds \cite{Edmonds65}, these developments led to bounds that essentially match the running time guarantees that were previously known only for bipartite case. More specifically, the running time bound of  for the general-graph case was obtained by Micali and Vazirani \cite{MicaliV80,Vazirani94} (see also \cite{GabowT91} and \cite{GoldbergK04}). While, building on the algebraic characterization of the problem due to Rabin and Vazirani \cite{RabinV89}, Mucha and Sankowski \cite{MuchaS04} and then Harvey \cite{Harvey09} gave -time algorithms for general graphs.
  
\subsection{Our Contribution}

In this paper, we develop a new algorithm for solving maximum - flow and minimum - cut problems in directed graphs. More precisely, we prove the following theorem.
  
\begin{theorem}\label{thm:main}
Let  be a directed graph with  arcs and unit capacities. For any two vertices  and , one can compute an integral maximum - flow and minimum - cut of  in  time.
\end{theorem}

This improves over the long-standing  running time bound due to Even and Tarjan \cite{EvenT75} and, in particular, finally breaks the  running time barrier for sparse directed graphs.  


Furthermore, by applying a well-known reduction (see \cite{CormenLRS09}), our new algorithm gives the first improvement on the sparse-graph case of the seminal -time algorithms of Hopcroft-Karp \cite{HopcroftK73} and Karzanov \cite{Karzanov73} for the maximum-cardinality bipartite matching problem. 

\begin{theorem}\label{thm:main_matchings}
Let  be an undirected bipartite graph with  edges, one can solve the maximum-cardinality bipartite matching problem in  in  time. 
\end{theorem}

\noindent This, again, breaks the 40-years-old running time barrier of  for this problem in sparse graphs. 

Additionally, we design a simple reduction of the maximum - flow problem to perfect bipartite -matching problem (see Theorem \ref{thm:flow_to_matchings}). (This reduction can be seen as an adaptation of the reduction of the maximum vertex-disjoint --path problem to the bipartite matching problem due to Hoffman \cite{Hoffman60} -- cf. Section 16.7c in \cite{Schrijver03}.\footnote{We thank Lap Chi Lau \cite{Lau13} for pointing out this similarity.}) As the reduction in the other direction is well-known already, this establishes an algorithmic equivalence of these two problems. We also show (see Theorem \ref{thm:rounding_matchings} and Corollary \ref{col:rounding_flows}) how this reduction, together with the sub-linear-time algorithm for perfect matching problem in regular bipartite graphs of Goel et al. \cite{GoelKK10}, leads to an efficient, nearly-linear time, rounding procedure for - flows.\footnote{Recently, it came to our attention that a very similar rounding result was independently obtained by Khanna et al. \cite{KhannaKL13}.}

Finally, our main technical contribution is a primal-dual algorithm for {(near-)}perfect bipartite -matching problem (see Theorem \ref{thm:interior_point_matchings}). This iterative algorithm draws on ideas underlying interior-point methods and the electrical flow framework of Christiano et al. \cite{ChristianoKMST11}. It employs electrical flow computations to gradually improve the quality of maintained solution by advancing it toward (near-)optimality along so-called central path. 

We develop a way of analyzing this algorithm's rate of convergence by relating it to the structure of the corresponding electrical flows (see Theorem \ref{thm:main_interior_point}). This understanding enables us to devise a way of perturbing (see Section \ref{sec:heavy}) and preconditioning (see Section \ref{sec:preconditioning}) our intermediate solutions to ensure a convergence in only  iterations and thus improve over the well-known barrier of  iterations that all the previous interior-point-methods-based algorithms suffer from. (To the best of our knowledge, this is the first time that this barrier was broken for a natural optimization problem.) 

We also note that most of this understanding of convergence behavior of interior-point methods can be carried over to general LP setting. Therefore, we are hopeful that our techniques can be extended and will eventually lead to breaking the  iterations barrier for general interior-point methods.

\subsection{Our Approach}

The core of our approach comprises two components. One of them is combinatorial in nature and exploits an intimate connection between the maximum - flow problem and  bipartite matching problem. The other one is more linear-algebraic and relies on interplay of interior-point methods and electrical flows. 

\paragraph{Maximum flows and bipartite matchings.} The combinatorial component shows that not only one can reduce bipartite matching problem to the maximum - flow problem, but also that a reduction in the other direction exists. Namely, one can reduce, in a simple and purely combinatorial way, the maximum - flow problem to a certain variant of bipartite matching problem (see Theorem \ref{thm:flow_to_matchings}). Once this reduction is established, it allows us to shift our attention to the matching problem. 

Also, as a byproduct, this reduction -- together with the algorithm of Goel et al. \cite{GoelKK10} -- yields a fast procedure for rounding fractional maximum flows (see Corollary \ref{col:rounding_flows}). This enables us to focus on obtaining solutions that are only nearly-optimal, instead of being optimal.

\paragraph{Bipartite Matchings and Electrical Flows.} The other component is based on using the interior-point method framework in conjunction with nearly-linear time electrical flow computations, to develop a faster algorithm for the bipartite matching problem. 

The point of start here is a realization that the recent approaches to approximating undirected maximum flow \cite{ChristianoKMST11,LeeRS13,Sherman13,KelnerLOS13}, despite achieving impressive progress, have fundamental limitations that make them unlikely to yield improvements for the exact undirected or (approximate) directed setting.\footnote{Note that it is known -- see, e.g., \cite{Madry11} -- that computing exact maximum - flow in undirected graphs is algorithmically equivalent to computing the exact or approximate maximum - flow in directed graph.}  Very roughly speaking, these limitations stem from the fact that, at their core, all these algorithms employ some version of gradient-descent method that relies on purely primal arguments, while almost completely neglecting the dual aspect of the problem. It is well-understood, however, that getting a running time guarantee that depends logarithmically, instead of polynomially, on  -- and such dependence is a prerequisite to making progress in directed setting -- one needs to also embrace  the dual side of the problem and take full advantage of it.  

\paragraph{Interior-point methods and fast algorithms.} The above realization motivates us to consider a more sophisticated approach, one that is inherently primal-dual and achieves logarithmic dependence on : interior-point methods. These methods constitute a powerful optimization paradigm that is a cornerstone of convex optimization (see, e.g., \cite{BoydV04,Wright97,Ye97}) and already led to development of polynomial-time exact algorithms for a variety of problems. Unfortunately, despite all its advantages and successes in tackling hard optimization tasks, this paradigm has certain shortcomings in the context of designing fast algorithms. The main reason for that is the fact that each iteration of interior-point method requires solving of a linear system, a task for which the current fastest general-purpose algorithm runs in  time \cite{AhoHU74,CoppersmithW90,Vassilevska12}. So, this bound becomes a bottleneck if one was aiming for, say, even sub-quadratic-time algorithm.   

Fortunately, it turns out that there is a way to circumvent this issue. Namely, even though the above bound is the best one known in general, one can get a better running time when dealing with some specific problem. This is achieved by exploiting the special structure of the corresponding linear systems. A prominent (and most important from our point of view) example here is the family of flow problems. Daitch and Spielman \cite{DaitchS08} showed that in the context of flow problems one can use the power of fast (approximate) Laplacian system solvers \cite{SpielmanTeng04,KoutisMP10,KoutisMP11,KelnerOSZ13} to solve the corresponding linear systems in nearly-linear time. This enabled \cite{DaitchS08} to develop a host of -time algorithms for a number of important generalizations of the maximum flow problem for which there was no such algorithms before. 

Unfortunately, this bound of  time turns out to also be  a barrier if one wants to obtain even faster algorithms. The new difficulty here is that the best worst-case bound on the number of iterations needed for an interior-point method to converge to near-optimal solution is . Although it is widely believed that this bound is far from optimal, it seems that our theoretical understanding of interior-point method convergence is still insufficient to make any progress on this front. In fact,  improving this state of affairs is a major and long-standing challenge in mathematical programing. 

\paragraph{Beyond the  barrier.}  Our approach to circumventing this  barrier and obtaining the desired -time algorithm for the bipartite -matching problem consists of two stages.

First one -- presented in Section \ref{sec:simple} -- corresponds to setting up a primal-dual framework for solving the near-perfect -matching problem. This framework is directly inspired by the principles underlying path-following interior-point methods and, in some sense, is equivalent to them. In it, we start with some initial sub-optimal solution (that is encoded as a minimum-cost flow problem instance) and gradually improve its quality up to near-optimality. These gradual improvements are guided by certain electrical flow computations -- the flows are used to update the primal solution and the corresponding voltages update the dual one -- and our solution ends up following a special trajectory in the feasible space: so-called central path. 

We analyze the performance of this optimization process by establishing a formal connection that ties the size of each improvement step to a certain characteristic of the corresponding electrical flow. Very roughly speaking, this size (and thus the resulting rate of convergence) is directly related to how much the electrical flow we compute resembles the current primal solution (which is also a flow). Once this connection is established, a simple energy-based argument immediately recovers the generic  iterations bound known for interior-point methods. So, as each electrical flow computation can be performed in  time, this gives an overall -time algorithm.  

Finally, to improve upon the above  iterations bound and deliver the desired -time procedure, in Section \ref{sec:improved}, we devise two techniques: perturbation of arcs -- that can be seen as a refinement of the edge removal technique of  Christiano et al. \cite{ChristianoKMST11}; and solution preconditioning -- a way of adding auxiliary arcs to the solution to improve its conductance properties. We show that by a careful composition of these techniques, one is able to ensure that the guiding electrical flows align better with the primal solution -- thus allowing taking larger progress steps and guaranteeing faster convergence -- while keeping the unwanted impact of these modifications on the quality of final solution minimal. The analysis of this process constitutes the technical core of our result and is based on understanding of the interplay between the interior-point method and both the primal and dual structure of electrical flows.

We believe that this approach of understanding interior-point methods through the lens of electrical flows is a promising direction and our result is just a first step towards realizing its full potential. 




\subsection{Organization}

We begin the technical part of the paper in Section \ref{sec:preliminaries} where we present some preliminaries on maximum flow problem, electrical flows, and bipartite (-)matching problem, as well as, introduce some theorems we will need in the sequel. In Section \ref{sec:outline}, we provide a general outline of our results and the structure of our proof. 

In Section \ref{sec:reduction}, we describe the reduction of maximum - flow problem to the bipartite -matching problem. Next, in Sections \ref{sec:simple} and \ref{sec:improved}, we explain how our path-following algorithms and electrical flows can be used to get an improved algorithm for the bipartite -matching problem, with Section \ref{sec:proof_main_interior_point} presenting the analysis of our path-following primitive. Finally, we conclude in Section \ref{sec:rounding} by showing how to round fractional -matchings to integral ones.  \section{Preliminaries}\label{sec:preliminaries}

In this section, we introduce some basic notation and definitions we will need later. 

\subsection{\texorpdfstring{-Flows}{Sigma-Flows} and the Maximum - Flow Problem}



Throughout this paper, we denote by  a directed graph with vertex set , arc set  (we allow parallel arcs), and (non-negative) integer capacities , for each arc . We usually define  to be the number of arcs of the graph in question and  to be the number of its vertices. Each arc  of  is an ordered pair , where  is its {\em tail} and  is its {\em head}.

The basic notion of this paper is the notion of a {\em -flow} in , where , with , is the {\em demand vector}. By a -flow in  we understand any vector  that assigns values to arcs  and satisfies the {\em flow conservation constraints}:
 
Here,  (resp. ) is the set of arcs of  that are leaving (resp. entering) vertex . Intuitively, these constraints enforce that the net balance of the total in-flow into vertex  and  the total out-flow out of that vertex is equal to , for every . 

Furthermore, we say that a -flow  is {\em feasible} in  iff  obeys the {\em non-negativity and capacity constraints}:
 

One type of -flows that will be of special interest to us are - flows, where  (the {\em source}) and  (the {\em sink}) are two distinguish vertices of . Formally, a -flow  is an {\em - flow} iff its demand vector  is equal to  for some  -- we call  the {\em value} of  -- and the demand vector  that has  (resp. ) at the coordinate corresponding to  (resp. ) and zeros everywhere else. 

Now, the {\em maximum - flow problem} corresponds to a task of finding for a given graph , a source , and a sink , a feasible - flow  in  of maximum value . We call such a flow  that maximizes  {\em the maximum - flow of } and denote its value by .

Sometimes, we will be also interested in (uncapacitated) {\em minimum-cost -flow} problem (with non-negative costs). In this problem, we have a directed graph  with infinite capacities on arcs (i.e., , for all ) and certain {\em (non-negative) length} (or {\em cost})  assigned to each arc . Our goal is to find a feasible -flow  in  whose {\em cost } is minimal. (Note that as we have infinite capacities here, the feasibility constraint \eqref{eq:capacity_constraints} just requires that  for all arcs .)

Finally, one more problem that will be relevant in this context is the {\em minimum - cut} problem. In this problem, we are given a directed graph  with integer capacities, as well as, a source  and sink , and our task is to find an - cut  in   minimizes the {\em capacity } among all - cuts. Here, a cut  is an {\em - cut} iff  and , and   is the set of all arcs  with  and . It is well-known \cite{EliasFS56,FordF56} that the minimum - cut problem is the dual of the maximum - problem and, in particular, that the capacity of the minimum - cut is equal to the value of the maximum - flow, as well as, that given a maximum - flow one can easily obtain the corresponding minimum - cut. 

\subsection{Undirected Graphs}

Although the focus of our results is on directed graphs, it will be crucial for us to consider undirected graphs too. To this end, we view an undirected graph  as a directed one in which the ordered pair  does not denote an arc anymore, but an (undirected) {\em edge}  and the order just specifies an {\em orientation} of that edge from  to . (Even though we use the same notation for these two different types of graphs, we will always make sure that it is clear from the context whether we deal with directed graph that has arcs, or with undirected graph that has edges.) From this perspective, the definitions of -flow  that we introduced above for directed graphs transfer over to undirected setting almost immediately. The only (but very crucial) difference  is that in undirected graphs a feasible flow can have some of s being negative - this corresponds to the flow flowing in the direction that is opposite to the edge orientation. As a result, the feasibility condition \eqref{eq:capacity_constraints} becomes 
 
 Also, the set  (resp. ) denotes now the set of incident edges that are oriented towards (resp. away) from , and  is just the set of all edges incident to , regardless of their orientation. 

Finally, given a directed graph , by its {\em projection}  we understand an undirected graph that arises from treating each arc of  as an edge with the corresponding orientation. Note that if  had two arcs  and  then  will have two parallel edges  and  that have opposite orientation and, possibly, different capacities.   


\subsection{Electrical Flows and Potentials}

A notion that will play a fundamental role in this paper is the notion of electrical flows. Here, we just briefly review some of the key properties that we will need later. For an in-depth treatment we refer the reader to \cite{Bollobas98}. 

Consider an undirected graph  and some vector of resistances  that assigns to each edge  its {\em resistance} . For a given -flow  in , let us define its {\em energy} (with respect to )  to be

where  is an  diagonal matrix with , for each edge . 



For a given undirected graph , a demand vector , and a vector of resistances , we define an {\em electrical -flow} in  (that is {\em determined} by resistances ) to be the -flow that minimizes the energy  among all -flows in . As energy is a strictly convex function, one can easily see that such a flow is unique. Also, we emphasize that we do \emph{not} require here that this flow is feasible with respect to capacities of  (cf. \eqref{eq:undir_capacity_constraints}). Furthermore, whenever we consider electrical flows in the context of a directed graph , we will mean an electrical flow -- as defined above -- in the (undirected) projection  of .  


One of very useful properties of electrical flows is that it can be characterized in terms of vertex potentials inducing it. Namely, one can show that a -flow  in  is an electrical -flow determined by resistances  iff there exist {\em vertex potentials}  (that we collect into a vector ) such that, for any edge  in  that is oriented from  to ,

In other words, a -flow  is an electrical -flow iff it is {\em induced} via \eqref{eq:potential_flow_def} by some vertex potential . (Note that orientation of edges matters in this definition.)

Using vertex potentials, we are able to express the energy  (see \eqref{eq:def_energy_flow}) of an electrical -flow  in terms of the potentials  inducing it as


One of the consequences of this characterization of electrical flows via vertex potentials is that one can view the energy of an electrical -flow as being a result of optimization not over all the -flows but rather over certain set of vertex potentials. Namely, we have the following lemma that, for completeness, we prove in the Appendix \ref{app:effective_conductance}. 

\begin{lemma}\label{lem:effective_conductance}
For any graph , any vector of resistances , and any demand vector , 

where  is the electrical -flow determined by  in . Furthermore, if  are the vertex potentials corresponding to  then the minimum is attained by taking  to be equal to .
\end{lemma}

Note that the above lemma provides a convenient way of lowerbounding the energy of an electrical -flow. One just needs to expose any vertex potentials  such that  and this will immediately constitute an energy lowerbound. 
Also, another basic but useful property of electrical -flows is captured by the following fact.

\begin{fact}[Rayleigh Monotonicity]
\label{fa:rayleigh_monotonicity}
For any graph , demand vector  and any two vectors of resistances  and  such that , for all , we have that if  (resp. ) is the electrical -flow determined by  (resp. ) then

\end{fact}



\subsection{Laplacian Solvers}

A very important algorithmic property of electrical flows is that one can compute very good approximations of them in nearly-linear time. Below, we briefly describe the tools enabling that.

To this end, let us recall that electrical -flow is the (unique) -flow induced by vertex potentials via \eqref{eq:potential_flow_def}. So, finding such a flow boils down to computing the corresponding vertex potentials . It turns out that computing these potentials can be cast as a task of solving certain type of linear system called {\em Laplacian} systems. To see that, let us define the \emph{edge-vertex incidence matrix}  being an  matrix with rows indexed by vertices
  and columns indexed by edges such that


Now, we can compactly express the flow conservation constraints \eqref{eq:conservation_constraints} of a -flow  (that we view as a vector in ) as


On the other hand, if  are some vertex potentials, the corresponding flow  induced by  via \eqref{eq:potential_flow_def} (with respect to resistances ) can be written as

where again  is a diagonal  matrix with , for each edge . 

Putting the two above equations together, we get that the vertex potentials  that induce the electrical -flow determined by resistances  are given by a  solution to the following linear system

where  is the (weighted) \emph{Laplacian } of  (with respect to the resistances ). One can easily check that  is an  matrix indexed by vertices of  with entries given by 



One can see that the Laplacian  is not invertible, but -- as long as, the underlying graph is connected -- it's null-space is one-dimensional and spanned by all-ones vector. As we require our demand vectors  to have its entries sum up to zero (otherwise, no -flow can exist), this means that they are always orthogonal to that null-space. Therefore, the linear system \eqref{eq:elec_flow_lin_system} has always a solution  and one of these solutions\footnote{Note that the linear system \eqref{eq:elec_flow_lin_system} will have many solutions, but each two of them are equivalent up to a translation. So, as the formula \eqref{eq:potential_flow_def} is translation-invariant, each of these solutions will yield the same unique electrical -flow.} is given by

where  is the Moore-Penrose pseudo-inverse of .

Now, from the algorithmic point of view, the crucial property of the Laplacian  is that it is symmetric and {\em diagonally dominant}, i.e., for any , . This enables us to use fast approximate solvers for symmetric and diagonally dominant linear systems to compute an approximate electrical -flow. Namely, building on the work of Spielman and Teng \cite{SpielmanT03,SpielmanTeng04}, Koutis et al. \cite{KoutisMP10,KoutisMP11} designed an SDD linear system solver that implies the following theorem. (See also recent work of Kelner et al. \cite{KelnerOSZ13} that presents an even simpler nearly-linear-time Laplacian solver.)


\begin{theorem}\label{thm:vanilla_SDD_solver}
For any , any graph  with  vertices and  edges, any demand vector , and any resistances , one can compute in  time vertex potentials  such that
, where  is the Laplacian of ,  are potentials inducing the electrical -flow determined by resistances , and .
\end{theorem}

To understand the type of approximation offered by the above theorem, observe that  is just the energy of the flow induced by vertex potentials . Therefore,  is the energy of the electrical flow  that ``corrects'' the vertex demands of the electrical -flow induced by potentials , to the ones that are dictated by . So, in other words, the above theorem tells us that we can quickly find an electrical -flow  in  such that  is a slightly perturbed version of  and  can be corrected to the electrical -flow  that we are seeking, by adding to it some electrical flow  whose energy is at most  fraction of the energy of the flow . (Note that electrical flows are linear, so we indeed have that .) As we will see, this kind of approximation is completely sufficient for our purposes. 

\subsection{Bipartite \texorpdfstring{-Matchings}{b-Matchings}}


A fundamental graph problem that constitutes both an application of our results, as well as, one of the tools we use to establish them, is the {\em (maximum-cardinality) bipartite -matching problem}. In this problem, we are given an undirected bipartite graph  with  -- where  and  are the two sets of bipartition -- as well as, a \emph{demand vector}  that assigns to every vertex  an integral and positive demand . Our goal is to find a maximum cardinality \emph{multiset}  of the edges of  that forms a {\em -matching}. That is, we want to find a multi-set  of edges of  that is of maximum cardinality subject to a constraint that, for each vertex , the number of edges of  that are incident to  is at most . (When  for every vertex , we will simply call such  a \emph{matching}.)

 We say that a -matching  is \emph{perfect} iff every vertex in  has exactly  edges incident to it in . Note that a perfect -matching - if it exists in  - has to necessarily be of maximum cardinality. Also, if a graph has a perfect -matching then it must be that . Now, by the {\em perfect bipartite -matching problem} we mean a task in which we need to either find the perfect -matching in  or conclude that it does not exist. 
 
 Finally, by a {\em fractional} solution to a -matching problem, we understand an -dimensional vector  that allocates non-negative value of  to each edge  and is such that for every vertex  of , the sum  of (fractional) incident edges in  is at most . Also, we define the {\em size} of a fractional -matching   to be .

An interesting class of graphs that is guaranteed to always have a perfect matching are bipartite graphs that are \emph{-regular}, i.e., that have the degree of each vertex equal to . A remarkable algorithm of Goel et al. \cite{GoelKK10} shows that one can find a perfect matching in such graphs in time that is proportional only to number of its vertices and not edges. (Note that a -regular bipartite graph has exactly  edges and thus this number can be much higher than  when  is large.) In particular, they prove the following theorem that we will use later. 

\begin{theorem}[see Theorem 4 in \cite{GoelKK10}]\label{thm:regular_bipartite_matchings}
Given an  doubly-stochastic matrix  with  non-zero entries, one can find a perfect matching in the support of  in  expected time with  preprocessing time.
\end{theorem}





 \section{From Flows to Matchings, and Back}\label{sec:outline}


As we already mentioned, our results stem from exploiting the interplay between the maximum - flow and bipartite -matching problem, as well as, from understanding the performance of interior-point methods -- when applied to these two problems -- via the structure of corresponding electrical flows. To highlight these elements, we decompose the proof of our main theorem (Theorem \ref{thm:main}) into three natural parts.

\subsubsection*{Reducing Maximum Flow to -Matching}

First, we focus on analyzing the relationship between the maximum - flow and the (maximum-cardinality) bipartite -matching problem. It is well-known that the latter  can be reduced to the former in a simple way. As it turns out, however, one can also go the other way -- there is a simple, combinatorial reduction from the maximum flow problem to the task of finding a perfect bipartite -matching.\footnote{One can view this as one possible explanation of why the techniques used in the context of bipartite matchings and maximum flows are so similar.} 

Before making this precise, let us introduce one definition. Consider a -matching problem instance corresponding to a bipartite graph  with  and  () being two sides of the bipartition. For any edge , let us define the {\em thickness}  of that edge to be . (So,  is an upper bound on the value of  in any feasible -matching .) We say that a -matching instance is {\em balanced} iff 


Now, in Section \ref{sec:reduction}, we establish the following result.

\begin{theorem}\label{thm:flow_to_matchings}
If one can solve a balanced instance of a perfect bipartite -matching problem in a (bipartite) graph with   vertices and  edges in  time, then one can solve the maximum - flow problem in a graph  with  arcs and capacity vector  in  time. 
\end{theorem}

This connection between maximum flows and bipartite matchings is useful in two ways. Firstly, it enables us to reduce the main problem we want to solve -- the maximum - flow problem with unit capacities -- to a seemingly simpler one: the perfect bipartite -matching problem. Secondly, the fact that this reduction works also for fractional instances provides us with an ability to lift our -matching rounding procedure that we develop later (see Theorem \ref{thm:rounding_matchings}) to the maximum flow setting (see Corollary \ref{col:rounding_flows}). 


\subsubsection*{The Algorithm for Near-Perfect -Matching Problem}

Once the above reduction is established, we can proceed to designing an improved algorithm for the perfect bipartite -matching problem. This algorithm consists of two parts. 

The first one -- constituting the technical core of our paper -- is related to the (fractional) near-perfect bipartite -matching problem, a certain relaxation of the perfect bipartite -matching problem. To describe this task formally, let us call a -matching   {\em near-perfect} if its size  is at least , i.e., it is within  additive factor of the size of a perfect -matching. Now, given a bipartite graph  and demand vector , the {\em near-perfect -matching problem} is a task of either finding a {\em near-perfect} -matching in  or concluding that no {\em perfect} -matching exists in that graph.

Our goal is to design an algorithm that solves this near-perfect -matching problem in  time. To this end, 
in Sections \ref{sec:simple} and \ref{sec:improved} we prove the following theorem. 

\begin{theorem}\label{thm:interior_point_matchings}
Let  with  be an undirected bipartite graph with  vertices and  edges and let  be a demand vector that corresponds to a balanced -matching instance with . In  time, one can either find a fractional near-perfect -matching  or conclude that no perfect -matching exists in . 
\end{theorem}

(Observe that whenever we have an instance of maximum - flow problem that has  arcs and unit capacities,  is exactly . So, if we apply the reduction from Theorem \ref{thm:flow_to_matchings} to that instance then the resulting -matching problem instance will be balanced, have  edges, as well as, . Therefore, we will be able to apply the above Theorem \ref{thm:interior_point_matchings} to it.)




At a very high level, our algorithm for the near-perfect -matching problem is inspired by the way the existing interior-point method path-following algorithms  (see, e.g., \cite{Ye97,Wright97,BoydV04}) can be used to solve it. Basically, our algorithm is an iterative method that starts with some initial, far-from-optimal solution and then gradually improves this maintained solution to near-optimality (pushing it along so-called central path)  using appropriate electrical flows as a guidance. We then show how to tie the convergence rate of this process to the structure of the guiding electrical flows. At that point, one can use a simple energy-bounding argument to establish a generic convergence bound that yields an (unsatisfactory) -time algorithm.

To improve upon this bound and deliver the desired -time algorithm, we show how one can appropriately ``shape'' these guiding electrical flows to make their guidance more effective and thus guarantee faster convergence. Very roughly speaking, it turns out there is a way of changing the maintained solution to make it essentially the same from the point of view of our -matching instance, while dramatically improving the quality of corresponding electrical flows that guide it. 

Our way of executing this idea is based on a careful composition of two techniques. One of them  corresponds to perturbing, in a certain way, the arcs that are most significantly distorting the structure of electrical flow -- this technique can be viewed as a refinement of edge removal technique of Christiano et al. \cite{ChristianoKMST11}. The other technique corresponds to preconditioning the whole solution by adding additional, auxiliary, arcs to it. These arcs are chosen so to significantly improve the conductance properties of the solution (when viewed as a graph with resistances) while not leading to too significant deformation of the final obtained solution. 


\subsubsection*{Rounding Near-Perfect -Matchings}

Finally, our final step on our way towards solving the perfect -matching problem (and thus the maximum - flow problem) is related to turning the approximate and fractional answer returned by the algorithm from Theorem \ref{thm:interior_point_matchings} into an exact and integral one. To this end, note that if that algorithm returned a near-perfect -matching that was integral, there would be a standard way to either turn it into a perfect -matching or conclude that no such perfect -matching exists. Namely, one could just use repeated augmenting path computations. It is well-known that given an integral -matching, one can perform,  in  time, an augmenting path computation that either results in increasing the size of our -matching by one, or concludes that no further augmentation is possible (and thus no perfect -matching exists). So, as our initial near-perfect -matching has size at least , after at most  iterations, i.e., in time , we would get the desired answer. 

Unfortunately, the above approach can fail completely once our near-perfect -matching is fractional. This is so, as in this case we do not have any meaningful lowerbound on the progress on the size of the -matching brought by the augmenting path computation. 

Therefore, to deal with this issue, we develop the last ingredient of our algorithm: a nearly-linear time procedure that allows one to round fractional -matchings.  More precisely, in Section \ref{sec:rounding}, building on the work of Goel et al. \cite{GoelKK10} (see Theorem \ref{thm:regular_bipartite_matchings}), we establish the following theorem.

\begin{theorem}\label{thm:rounding_matchings}
Let  be an undirected bipartite graph with  edges and let  be a demand vector, if  is a fractional -matching in  of size  then one can find in  time an integral -matching in  of size .
\end{theorem}

Clearly, if we apply the above rounding method to the fractional near-perfect matching  computed by the algorithm from Theorem \ref{thm:interior_point_matchings}, it will give us an integral -matching  whose size is still at least . So, the augmenting path-based approach we outlined above will let us obtain the desired integral and exact answer to the perfect -matching problem within the desired time bound. 

In the light of all the above, we see that combining all the above pieces indeed yields an -time algorithm for the perfect bipartite -matching problem in graphs with . Now, using the reduction from Theorem \ref{thm:flow_to_matchings}, this gives us the analogous algorithm for the maximum - flow problem in unit-capacity graphs and that, in turn, results in an algorithm for the bipartite matching problem. So, both Theorem \ref{thm:main} and Theorem \ref{thm:main_matchings} hold.

\subsubsection*{Rounding - Flows}

Finally, we mention the other byproduct of our techniques -- the fast rounding procedure for flows. Namely, using the reduction described in Theorem \ref{thm:flow_to_matchings} and the rounding from Theorem \ref{thm:rounding_matchings} we can obtain a fast rounding procedure not only for fractional -matchings but also for fractional - flows. Specifically, the proof of the following corollary appears in Appendix \ref{app:col_rounding_flows}. 

\begin{corollary}\label{col:rounding_flows}
Let  be a directed graph with capacities and let  be some feasible fractional - flow in  of value . In  time, we can obtain out of  an integral - flow  of value  that is feasible in . 
\end{corollary} 

\noindent Again, we note that a very similar rounding result was independently obtained by Khanna et al. \cite{KhannaKL13}.  \section{From Maximum Flows to Perfect Matchings}\label{sec:reduction}

In this section, we show how to reduce the maximum - flow problem in a directed capacitated graph  to solving  balanced instances of the perfect bipartite -matching problem, i.e., we prove Theorem \ref{thm:flow_to_matchings}. We note that our reduction can be seen as an adaptation of the reduction of the maximum vertex-disjoint - path problem to the bipartite matching problem due to Hoffman \cite{Hoffman60} -- cf. Section 16.7c in \cite{Schrijver03}.

To this end, let  with  vertices and  arcs, as well as, the source  and sink  be our input instance of the maximum - flow problem. Without loss of generality, we can assume that there is no arcs entering  and no arcs leaving , as these arcs do not affect the maximum - flow. Also, let  be the value of the maximum - flow in .  

\subsection{The Reduction}



\begin{figure}[ht]
\centering
\vspace{8pt}
\includegraphics[width=\textwidth]{figures/reduction}
\vspace{8pt}
\caption{{\bf a)} An example directed - flow instance . Numbers next to arcs denote their capacities.  {\bf b)} The -matching instance corresponding to the example from a) in case of . Here, numbers next to vertices denote their demands. }
\label{fig:reduction_example}
\end{figure}


We show that for any integral value of , we can setup, in  time, a balanced bipartite -matching problem instance, for some demands  and bipartite graph , such that: (1) there will be a perfect -matching in  if there is a feasible - flow of value  in ; and (2) given a perfect -matching in  one can recover in  time an - flow of value  that is feasible in . Observe that once such a reduction is designed, Theorem \ref{thm:flow_to_matchings} will follow by noticing that  and applying a simple binary search strategy to find the value of  and extract the corresponding maximum --flow. 

Given the input graph , source , sink  and the value of , the construction of our desired balanced bipartite -matching instance  is as follows. First, for each arc , we create two vertices  and  and an edge  between them, as well as, we set the demand  and  of these vertices to . Next, for every vertex  of  other than  and , we add a vertex  to  and a vertex  to . Also, we create an edge , as well as, an edge  (resp. ) for every arc  that is incoming to (resp. outgoing of)  in . We set the demands  (resp. ) to be equal to  (resp. ). Finally, we create a vertex  (resp. ) and add an edge  (resp.  for each arc  that is leaving  (resp. incoming to ) in . We put the demand  (resp. ) to be  (resp. ). (Note that we can assume here that both these quantities are non-negative as both  and  are obvious upperbounds on the value of .) 

An example - flow instance and the corresponding instance of the bipartite -matching can be found in Figure \ref{fig:reduction_example}.

To see that this instance is balanced, note that every edge  of  that is incident to some vertex  or  has its thickness  equal to . So, the contribution of these edges to the total thickness  of edges of  is at most . On the other hand, the only edges that are not incident to some  or  are the ones of the form . However, the total contribution of these edges to the total thickness is at most 

as needed.


Now, the proof of correctness of this reduction appears in Appendix \ref{app:reduction}.  
\section{Basic \texorpdfstring{-Time}{\~O(m\textasciicircum(3/2))-Time} Algorithm for Bipartite -Matching Problem}
\label{sec:simple}



Over the next two sections, we prove Theorem \ref{thm:interior_point_matchings}. That is, we present an algorithm for the {near-perfect} bipartite -matching problem in the setting where the input instance is balanced (see \eqref{eq:def_balanced}) and  is . In what follows we assume, for convenience, that  is at most  and that the graph  is sparse, i.e., .\footnote{It is easy to see that these assumptions are made without loss of generality. Whenever  is , one can ensure that  and  by adding an appropriate -- but still  -- number of dummy copies of complete bipartite  graph with uniform demands. Adding each such dummy isolated copy brings the ratio of  and , as well as, of  to  down towards , while never leading to violation of the balance condition \eqref{eq:def_balanced} and preserving the -matching structure of the original input graph.} 




In this section, we show a basic algorithm that runs in  time. Later, in Section \ref{sec:improved}, we refine this algorithm to obtain the desired running time of .

For the sake of clarity, in our description and analysis we assume that the nearly-linear time Laplacian system solver (see Theorem \ref{thm:vanilla_SDD_solver}) always returns an exact solution, i.e., all the electrical -flows we compute are exact. We discuss how to handle the approximate nature of the solver's output in Appendix \ref{app:inexact_elec_flow_disc}. 

\subsubsection*{From \texorpdfstring{-Matching}{b-Matching} to Min-Cost \texorpdfstring{-flow}{Sigma-flow}}
Let us fix our instance of the bipartite -matching problem in bipartite graph  with . We will solve our -matching instance by reducing it to a task of finding a minimum-cost -flow in a certain related directed graph  with  being a length vector. 

\begin{figure}[ht]
\centering
\vspace{8pt}
\includegraphics[width=\textwidth]{figures/min_cost_flow}
\vspace{8pt}
\caption{{\bf a)} An example instance of bipartite -matching problem. Numbers next to vertices represent their demands.  {\bf b)} The minimum-cost -flow problem instance corresponding to the example from a). All arcs have cost  equal to  and the numbers next to vertices denote their demands in . There are two parallel copies of the arc  and three parallel copies of the arc . Also, each dashed arc represents two arcs that have the same endpoints but opposite orientation. }
\label{fig:min_cost_example}
\end{figure}



The reduction is performed as follows (see Figure \ref{fig:min_cost_example} for an example). The vertex set  of the graph  consist of a special vertex , as well as, vertices  (resp. ), for every vertex  (resp. ) of the graph . Next, for every edge  in , we add to   copies of an arc , where we recall that  is the thickness of . Finally, for each vertex  (resp. ) of , we add to  arcs  and  (resp.  and ). We set the lengths  of all arcs  to . 

To gain some intuition on this reduction, note that if a perfect -matching indeed exists in  then the flow that encodes it in  is fully supported on the arcs  and does not send more than one unit of flow on any of these arcs. So, the purpose of including the extra vertex  and the arcs incident to it is to support (and appropriately penalize) the initial and intermediate solutions as they approach optimality.  

Also, observe that this new graph has  vertices and, due to our -matching instance being balanced, we have that the total number  of arcs is at most

So, bounding our running time in terms of  provides a bound in terms of the number of edges  of our original -matching instance that is asymptotically the same. 

Now, consider a demand vector  that has surplus of  at each vertex , a deficit of  at each vertex  and a zero demand at vertex . (Note that such a demand vector will be valid, i.e., , as we can assume that  -- otherwise it would be impossible to have a perfect -matching in .) We claim that any near-optimal -flow gives us a solution to our near-perfect -matching instance. (Recall from Section \ref{sec:outline} that a -matching is near-perfect if its size is at least . Although, in the lemma below it suffices that we have a slack of only  instead of .)

\begin{lemma}\label{lem:mincost_to_matchings}
Given any feasible -flow  in  whose cost  is within additive  of the optimum, in  time, we can either compute a (fractional) near-perfect -matching  in  or conclude that no perfect -matching exists in . 
\end{lemma} 

\begin{proof}
First, observe that if there exists a perfect -matching  in  then a flow  that just puts, for each  of ,  units of flow on each (of ) copies of the arc  in , is a feasible -flow with cost . (Recall that in the minimum-cost problem we assume that arc capacities are infinite, thus feasibility condition \eqref{eq:capacity_constraints} boils down to non-negativity of all s.) So, we can assume that our -flow  has its cost  at most . (Otherwise, we know that there is no perfect -matching in .)

Now, given any feasible -flow in , we can decompose it into a collection of flow-paths and flow-cycles, where each of these flow-paths transports some amount of flow from some vertex  to some vertex . By our construction of the graph , each such flow-path has to have a length at least . On the other hand, if this flow-path is indeed of length exactly  then it has to correspond to a single arc  that reflects the existence of edge  in . As a result, our feasible -flow  in  has to have its cost  to be at least  and, furthermore,  is an upper bound on the total amount of flow in  that is not transported over the direct one-arc flow paths (and thus passes through the vertex ).  

So, as we argued that the cost of  has to be at most , there is only at most  units of flow in  that passes through the vertex . Now, to extract the desired (fractional) near-perfect -matching , we just take , for each edge  in . Clearly, the size of such fractional matching is at least , which is well above our lowerbound of  for a near-perfect matching. Also, our construction works in  time, as desired. 
\end{proof}

\subsubsection*{Slack Variables}

In the light of the above, our goal now is to compute the near-optimal solution to our minimum-cost -flow problem instance in the graph . Our approach to this task is inspired by so-called path-following interior-point methods \cite{Ye97,Wright97,BoydV04}. At a very high level, we will start with certain initial solution that is far from being optimal, and then we will gradually improve -- in an iterative manner -- its quality until close-to-optimal solution is obtained. This gradual improvement will be performed in a very specific way. It will always try to push the current solution further down so-called central path. 

Before we can define the central path, let us first mention that, in general, there are two natural ways of tracking the progress of a current solution towards optimality. One of them is purely primal and relies on just maintaining a feasible solution  and comparing its cost against some estimate of the cost of the optimal solution. The second one -- and the one that we will actually use here -- is based on primal-dual paradigm. Namely, in addition to maintaining a feasible primal solution , we will also keep a dual feasible solution . This dual solution provides an embedding of all the vertices in  into a line, i.e.,  just assigns a real number  to each vertex  of . Its feasibility condition is that for any arc  of  it should be the case that its {\em slack variable}  is always non-negative, i.e., that the length of the arc  in this embedding is never larger than its length according to the length vector . 

Before we proceed further, we note that the dual solution  is uniquely determined -- up to a translation -- by the vector  (given the length vector ). So, for notational convenience, from now on, we will describe the dual solutions in terms of the vector  instead of .

\subsubsection*{Duality Gap}

It is not hard to see that any feasible dual solution  provides a lower-bound on the cost of the optimal solution (after all, this is just a consequence of weak duality). In particular, one has that for any pair  of feasible primal and dual solutions, the so-called {\em duality gap}, i.e., the difference between the upper bound on the value of optimal solution that is provided by the primal solution  and the lower bound provided by the dual solution  is exactly 

where , for each arc , and  is all-ones vector (of dimension ). 

This means that one can obtain a close-to-optimal solution by devising a procedure that (quickly) converges to a pair of primal and dual solutions  whose duality gap  is small (in our case, at most ). 



\subsubsection*{-Centered Solutions and the Central Path}

To describe in more detail the convergence process we will employ, let us associate with each arc  a {\em measure} . One can view  as a certain notion of importance of a given arc. (The motivation behind introducing this notion will be clear later.) We will always make sure that the measures of arcs are not smaller than  and also that their total sum is never too large. That is, we will make sure to maintain the following invariant.

\begin{invariant}
\label{inv:measure_upperbound}
We have that  and for each arc , .
\end{invariant}

We want to note that when discussing the preservation of the above invariant we will only focus on ensuring that the upperbound is not violated. The fact that  for all arcs  will be automatically enforced as we will make sure that the initial measure of all the arcs is always at least  and our algorithm will never decrease any measures -- they only might increase. 


\paragraph{-centered solutions.} Now, let us define, for each arc ,  to be the normalized value of  and let 

be the weighted average value of  with weights given by the measures .

We will call a solution  (where  represents the associated measures) {\em -centered}, for some , if 

where, for a given vector ,
 
i.e.,  is the -norm of the vector  reweighed by the measures . 

Note that in a -centered solution  we have all  equal to . More generally, a simple but very useful observation is that
\begin{fact}
\label{fa:central_vs_max_min}
For any -centered solution  we have that

for each arc . 
\end{fact}

\paragraph{ as a measure of progress.} The quantity  will be important to us for one more reason. It will constitute our measure of progress on the quality of our maintained solution. To see why it indeed can serve this role, recall that by Invariant \ref{inv:measure_upperbound} we have that 

So, if our goal is to obtain a solution whose duality gap is at most  we just need to make sure that the corresponding value of  is at most . 

The main reason why we choose to measure our progress in terms of  instead of the actual duality gap  is that in our algorithm we will sometime end up increasing measures of arcs. Such increases lead to an increase of the duality gap, so measuring our progress in terms of  would require dealing with such local non-monotonicity of this quantity. Continently, once we focus on keeping track of   (and ensure that Invariant \ref{inv:measure_upperbound} is never violated), these issues will be avoided.


\paragraph{The central path.} Finally, after introducing the above definitions, we can define the central path to be the set of all the -centered solutions.\footnote{Strictly speaking, in the literature, the central path corresponds to -centered solutions with the measures of all arcs being one.} One can show that this set constitutes an actual path in feasible space that spans all the -centered solutions and passes arbitrarily close to (but never reaches) an optimal solution to our minimum cost flow problem. This explains the name of ``path-following'' interior-point methods, as they start with some initial -centered solution and gradually advance along the central path to get increasingly more optimal -centered solution for some small fixed .


 \subsubsection*{Traversing the Central Path with Electrical Flows}

Motivated by this path-following approach, our algorithm for computing near-optimal solution to the minimum-cost -flow problem will start with some -centered solution  that has fairly large value of  (and thus is far from being optimal). Then, we will devise a sequence of solutions , where  is the step index, that have increasingly smaller value of  (and thus, indirectly, the duality gap) while making sure that they always are -centered for some small constant . This way, our algorithm will eventually converge to the desired close-to-optimal solution.

To implement this approach, we start with the following lemma that shows we can get the initial -centered solution  -- its proof appears in Appendix \ref{app:initial_solution}. 
\begin{lemma}
\label{lem:initial_solution}
There exists an explicit -centered primal-dual feasible solution  with  and . 
\end{lemma}



Note that the bound on the total measure of the arcs ensures that the Invariant \ref{inv:measure_upperbound} is preserved. Furthermore, there is a slack of at least  remaining between  and the upperbound of  from Invariant \ref{inv:measure_upperbound}. It will be used to accommodate future measure increases in our improved algorithm (see Section \ref{sec:improved}).  

We now proceed to explaining how given some -centered solution , we can modify it to obtain a -centered solution  that has a smaller value of . 

\paragraph{The associated flow .} For a given solution  let us call it {\em -feasible}, for some demand vector , if it is dual feasible (i.e., ) and if  is a feasible -flow. (So, a -feasible solution is a solution that is primal-dual feasible for our minimum-cost -flow problem.) Next, given a -feasible solution , let us define an {\em associated electrical flow } to be the electrical -flow in (the undirected projection of)  determined by resistances  that are given as

for arc . (Whenever we use this definition, it will be always the case that all s are positive and thus the resistances  are well-defined.)

 \paragraph{Making an improvement step.} The central object in our procedure for taking an improvement step will be the electrical flow  that is associated with the solution . The fundamental property of this flow is that it allows us to simultaneously update our solution  {both} in the primal (flow) space -- via the flow  itself -- and in the dual (line embedding) space -- via the vertex potentials  that induced  (see \eqref{eq:potential_flow_def}). (In Section \ref{sec:proof_main_interior_point}, we provide a detailed description of the whole improvement step.)

 As we will see, such a guided update not only decreases the duality gap of our solution, but also perfectly maintains its centering when only first-order terms (i.e., terms linear in the updates) are accounted for. Unfortunately, the second-order terms (i.e., the ones depending on the product of primal and dual updates) can disturb the centering. So, to be able to control this deficiency, we need to ensure that the step size  that governs the ``aggressiveness'' of the improvement step is sufficiently small. 
 
Of course, on the other hand, it is important for us to have this step be as large as possible. After all, the extent of our duality gap improvement -- and thus overall convergence rate of our algorithm -- is directly proportional to this size. So, it is crucial for us to develop a good grasp on how the size of that step relates to the properties of the flow . 

To this end, let us define, for some -- not necessarily feasible -- flow  and a positive vector ,  to be the vector of congestions inflicted in  by  with respect to capacities given by .  That is, 

for each arc  in . 

Now, in Section \ref{sec:proof_main_interior_point}, we present a precise implementation and analysis of our update step. (This implementation can be viewed as a direct analogue of the update steps of path-following interior-point methods.) The result of this analysis is presented in the following theorem, which, in particular, ties the congestion vector  inflicted by the electrical flow  with respect to the primal solution , to an upperbound on the size  of the improvement step. 

\begin{theorem}
\label{thm:main_interior_point}
Let  be a solution that is -centered and -feasible, and let  be the associated electrical flow. We can compute in  time a -centered and -feasible solution  with , as long as, 

Furthermore, we have that the measures do not change, i.e., , and if for each arc , we define  and  to make  (resp. ) reflect the relative change (scaled by ) of resistances  (resp. flows ) then  and

for some vector  with .
\end{theorem} 

So, we see that the allowed size  of the improvement steps is proportional to how much the guiding flow  resembles the current primal solution . Thus, for example, if there is some arc  that flows much larger flow in  than in , i.e., an arc  with large value of , this arc will be severely penalized by the -norm measuring the quality of the resemblance. 

Also, it is worth pointing out that it is very important that the above bound is based on  instead, say  norm. In fact, one can show (see Lemma \ref{lem:energy_bound}) that in case of our problem the  norm of congestion vector is always . So, using  norm would not lead to any improvement over the  iteration bound. 


\subsection{Bounding the Running Time}


At this point, we want to present a fairly elementary proof of  lowerbound on our allowed improvement step size . Note that once we achieve that then, by Lemma \ref{lem:initial_solution} and Theorem \ref{thm:main_interior_point}, we will have that the value of our measure of progress  after  steps is at most

So, by setting , we recover the  iterations convergence bound of interior-point methods. This leads to a simple -time procedure that produces a solution with duality gap at most 

where we used Invariant \ref{inv:measure_upperbound} (see \eqref{eq:duality_gap_bound}). This, in turn, by Lemma \ref{lem:mincost_to_matchings} provides us with a solution to our instance of near-perfect -matching problem. 

Therefore, to conclude the analysis of the simple -time algorithm for the near-perfect -matching problem, it remains to establish the claimed lowerbound on . 

\paragraph{Congestion and energy.} By Theorem \ref{thm:main_interior_point}, performing such lowerbounding of  boils down to upperbounding . To understand how the latter can be done, one should observe the following simple but crucial fact. (This fact follows from Fact \ref{fa:central_vs_max_min} and definition of the resistances  \eqref{eq:hf_resistances}.)


\begin{fact}
\label{fa:rho_vs_rt}
For any -centered solution  and any flow  in  we have that

and, similarly,

for any arc  in . 
\end{fact}

Observe that the above inequalities state that -- up to a  factor -- the square of the congestion  incurred by an arc  in the flow  is upperbounded by

which corresponds to normalized (by )  contribution of the arc  to the energy  of the flow  with respect to resistances .

This simple connection between the congestion of an arc in  and its contribution to the energy of that flow that is provided by Fact \ref{fa:rho_vs_rt} will be fundamental to the rest of our discussion. In particular, it gives us an intuition on why we even expect the guiding electrical flows  to inflict small congestion with respect to  and thus allow us to take a larger step size. This intuition is based on an observation that the main goal of electrical flows is to minimize energy. So, by choosing the resistances appropriately, we in some sense align this goal with our goal of making as large step sizes as possible. Roughly speaking, we are employing here the  norm minimization offered by electrical flows to achieve the desired -minimization corresponding to larger step sizes. 

Now, an immediate consequence of the above connection is an elementary way of upperbounding : we just bound the -energy of the guiding electrical flow  that is associated with our solution  and exploit the generic relationship between  and  norms.  

To implement this approach, let us start with the following lemma that gives us a bound on the -energy of the electrical flow . 

\begin{lemma}
\label{lem:hfft_energy_bound}
For any -feasible solution  and associated electrical flow , we have that

\end{lemma}

\begin{proof}
The fact that  follows directly from the definition of  and the fact that electrical -flow minimizes energy among all the -flows (which includes ). 

Now, by definition \eqref{eq:hf_resistances}  of  and that of  \eqref{eq:def_hmu} we have that

where the last line follows by Invariant \ref{inv:measure_upperbound}.
\end{proof}

Once we establish this upperbound on -energy, we simply use it to upperbound the -energy of the congestion vector. Specifically, by applying Cauchy-Schwarz inequality and the fact that , for any vector , we get that

where we also used the fact that . 

Now, to bound the  norm (instead of  norm) of the congestion vector, we just note that by Fact \ref{fa:rho_vs_rt} and Lemma \ref{lem:hfft_energy_bound}
 

Therefore, we can conclude with the following lowerbound on . 

\begin{fact}
\label{fa:basic_delta_lowerbound}
For any , .
\end{fact}

It is worth pointing out that, as we discussed before, the fact that we settled here for an -norm-based (instead of an -norm-based) dependence of  on the congestion vector , this  lowerbound is the best possible to achieve with this approach. Therefore, to have any hope of obtaining an improvement that goes beyond this bound (as we will do in the next section), we crucially require to be working with -norm-based (instead of only -norm-based) arguments. 
  \section{An Improved Algorithm for Bipartite -Matching Problem}
\label{sec:improved}

After setting up our primal-dual framework and presenting the -time algorithm in the previous section, we can now proceed to developing our improved algorithm with the running time of , for .

Given our analysis and discussion in the previous section, a tempting approach to obtaining such an improved bound could be trying to simply tighten our analysis performed there (e.g., by taking advantage of -norm-based instead of only -norm-based arguments) and thus improve the worst-case lowerbound on  that we established (cf. Fact \ref{fa:basic_delta_lowerbound}). 

It turns out, however, that getting an improved bound is not merely a matter of performing a better analysis. In the worst-case, our  bound is actually tight. After all, if there is an arc that incurs  congestion in the associated electrical flow, the resulting -norm of the congestion vector will be . So, even though the connection between congestion and energy we established before (see Fact \ref{fa:rho_vs_rt} and Lemma \ref{lem:hfft_energy_bound}) tells us that there cannot be too many such arcs (as each one of them would need to contribute a very significant, , fraction of the total energy of the electrical flow), having just one such arc is already enough to prevent us from taking larger than  improvement step. 

Therefore, as we cannot rule out that such worst-case situation arises in each iteration of our algorithm\footnote{One would suspect, however, that such situations are indeed rare. This might be one explanation of why in practice interior-point methods are able to take most of its step sizes to be very large and thus converge much faster than indicated by the worst-case bound of .}, getting our desired improvement requires developing a strategy that explicitly ensures that this is not the case (or, at least, not too often). 

At a high level, our general approach to accomplishing this goal is based on ``massaging'' the solution that we maintain. That is, we devise and carefully combine two methods of altering our solutions. These methods, on one hand, significantly improve the behavior of the associated electrical flow while, on other hand, only slightly perturb the characteristics of that solution that are vital to recovering the desired near-perfect -matching at the end.

The first of these two methods is related to edge removal technique of Christiano et al. \cite{ChristianoKMST11}. Their technique is based on repeated removal from the graph of the edges that suffer too much congestion. As \cite{ChristianoKMST11} showed (via a simple energy-based argument), when such edge removal is applied to electrical flows that guide multiplicative-weight-update-based optimization routine, one obtains a significantly faster convergence to approximately optimal solution.

Unfortunately, as our primal-dual framework has much more delicate nature than the multiplicative-weight-update method, such removal of ``bottlenecking'' arcs would be too drastic and, in particular, could destroy the structure of our dual solution. Therefore, we apply a more careful approach. 

Instead of removing arcs, we only perturb them by moderately increasing their lengths (and thus their slack variables). (Note that, by \eqref{eq:hf_resistances}, increasing arc's slack variable increases its resistance.) Furthermore, to avoid significant distortion of the dual solution, we do not apply this perturbation to all ``bottlenecking'' arcs, but only the ones that are ``heavy'' in the primal solution (see Definition \ref{def:heavy} below). 

We then use a certain refinement of the original energy-based argument of Christiano et al. \cite{ChristianoKMST11} (that needs, in particular, to deal with the fact that -- in contrast to the multiplicative-weight-update-based framework of \cite{ChristianoKMST11} -- in our framework the arc's resistances can change in a completely non-monotonic fashion) to show that the behavior of our guiding electrical flows on such ``heavy'' arcs is indeed improved. 

Now, our second method -- that is somewhat complementary to the first one and aims at accommodating the ``light'' arcs -- is based on an appropriate preconditioning of our solution by augmenting it with auxiliary arcs. The purpose of adding these arcs is to improve the connectivity (and thus electrical conductance) of the underlying solution (when treated as a graph with resistances) while changing the structure of our original solution in only minimal way (that can be fixed later). We then show via a certain dual-based argument that existence of these auxiliary arcs ensures that ``light'' arcs are never the bottlenecking ones (and thus do need to be dealt with anymore). 

We proceed now to detailed description and analysis of our improved algorithm.

\subsubsection*{The Sets  and -Smoothness}


We start by specifying the behavior of associated electrical flows that is ``good'' from our perspective. To this end, for a flow  in , a solution , and integer , let us define  to be the set of all the arcs  such that

i.e., the collection of all the arcs whose congestion in the flow  (with respect to capacities given by ) is between  and .


Now, we introduce a definition that will be fundamental to the rest of our discussion.
\begin{definition}
\label{def:smoothness}
For some , a flow , and solution  (that will be always clear from the context), we say that  is {\em -smooth on some of arcs } iff, for any integer , we have that

where . 
Furthermore, we simply say that  is {\em -smooth} if , i.e.,  contains all the arcs. 
\end{definition} 
Clearly, the -smoothness constraints the distribution of the arcs that suffer high congestion in . In particular, it implies that there is no arcs whose congestion  is larger than . 

Observe that the tight worst-case example for the lowerbound on  (cf. Fact \ref{fa:basic_delta_lowerbound}) corresponds to situation where the electrical flow  associated with the maintained -centered solution solution  makes some arcs highly-congested, i.e., makes them suffer congestion of . However, the above definition of -smoothness, forbids existence of such arcs. Therefore, the hope is that once our electrical flows  are -smooth, a better lowerbound on  (and thus faster convergence) is possible. As the following lemma shows, this hope is indeed well-founded.

\begin{lemma}
\label{lem:better_delta_lowerbound}
Let  be a -feasible and -centered solution and let  be the associated electrical flow that is -smooth, for some . We have that

for some sufficiently large constant .
\end{lemma}

\begin{proof}
By Theorem \ref{thm:main_interior_point}, in order to lowerbound  we need to upperbound the quantity
 

To this end, note that 

where we used the -smoothness of  (cf. Definition \ref{def:smoothness}) and the fact that  whenever . So, the lemma follows once  is chosen to be an appropriately large constant.
\end{proof}


In the light of the above lemma, if we somehow knew that all -- or, at least, most of -- the flows  that we compute are indeed -smooth for some  , we would immediately get the desired faster algorithm. Unfortunately, as we already discussed, it seems to be hard to argue that this is what happens in the worst-case.  Therefore, to address this problem we develop a perturbation approach that we carefully apply to our maintained solutions to ensure that most of the flows  is indeed -smooth for some small enough value of . 



\subsubsection*{-Stretching}

One of the main operations that we will use to implement our perturbations is called -stretching. To describe it, consider a solution  that is -centered and a parameter . We define an {\em -stretching} of an arc  to be an operation that returns a solution  obtained from  by, first, increasing the length  of the arc  (and thus the value of ) by  and, then, increasing the measure  of  by a factor of , where

The remaining part of the solution remains the same.

The property of  -stretching that is key from our point of view, is that after applying it to some arc  its resistance  increases by a factor of exactly . Furthermore, our choice of value of  is justified by the lemma below -- its proof appears in Appendix \ref{app:choosing_beta}.

\begin{lemma}
\label{lem:choosing_beta}
If  was a -centered solution with  then so will be  and . Furthermore, .
\end{lemma}

So, we see that applying -stretching with this setting of  does not perturb our measure of progress , even though the duality gap  changes due to corresponding increase in measure. (Again, this is one reason why we chose  to measure our progress.)

On the other hand, besides the increase in measure, another undesirable byproduct of -stretching is the increase of arc's length. To mitigate the effect of this process on the validity of our final solution, we will ensure that the following invariant is maintained throughout the algorithm.

\begin{invariant}
\label{inv:length_upper}
The overall increase of arcs' length due to -stretching is at most  and no individual arc has its total increase of length larger than .
\end{invariant} 

\noindent Maintaining this invariant will be important for two reasons. One is captured by the following simple lemma, whose proof appears in Appendix \ref{app:bound_on_s}.

\begin{lemma}
\label{lem:bound_on_s}
If Invariant \ref{inv:length_upper} is preserved then for any -feasible solution , we have that  is at most , for any arc .
\end{lemma}

The other, and even more important one, is that as long as this invariant is preserved the final close-to-optimal solution  to our perturbed problem still allows us to recover the desired near-perfect -matching in the (original) graph  (or conclude that no perfect -matching exists). More precisely, in Appendix \ref{app:perturbed_solution} we prove the following lemma.

\begin{lemma}
\label{lem:perturbed_solution}
Provided Invariant \ref{inv:length_upper} holds, given any feasible -flow  in  whose cost is within additive  of the optimum, we can recover in  time a (fractional) near-perfect -matching in , or conclude that no perfect -matching exists in . 
\end{lemma}

\subsubsection*{Heavy Arcs}

As we already mentioned, an important role in our improved algorithm is played by a classification of arcs into two classes, ``heavy'' and ``light'', depending on their current flow in the primal solution. We make this classification precise below.  

\begin{definition}\label{def:heavy}
Given a -centered solution  with , we call an arc  {\em heavy} if , where

for some sufficiently large constant  that we will fix later (see Lemma \ref{lem:separated_sets}). We say that an arc is {\em light} if it is not heavy.
\end{definition}

The motivation for the above classification stems from a desire to control the increase in arc's length due to an application of -stretching.  Namely, observe that if we -stretch an heavy arc  then the increase in this arc's length is by at most 
 
where we used Fact \ref{fa:central_vs_max_min}. So, as long as we apply -stretching operations only to heavy arcs -- which essentially will be the case in our algorithm -- we can guarantee that the resulting change in arc length is relatively small. This will be important to ensuring that Invariant \ref{inv:length_upper} is never violated. 


Having introduced the above concepts, we are ready to proceed to presenting our improved algorithm. In this presentation, we fix for the rest of this section , where 

 and  is a sufficiently large constant to be fixed later. 

We  describe our algorithm in two stages. First, in Section \ref{sec:heavy}, we present a variant of the algorithm that works under an ad-hoc assumption that all the electrical flows  that we compute are always -smooth on the set of light arcs. (So, we need to deal there only with its possible non--smoothness on the set of heavy arcs.) Then, in Section \ref{sec:preconditioning}, we show how to apply a preconditioning technique to obtain an augmented version of our graph such that when we run our algorithm it is indeed true that the above ad-hoc assumption holds. 
 \subsection{Perturbing Heavy Arcs}\label{sec:heavy}


In this section, we work under an ad-hoc assumption that the electrical flows  that are associated with the maintained solutions  are always -smooth -- with  -- on the set of arcs that are light with respect to that solution. We present an -time algorithm for this setting. 

\subsubsection*{-Improvement Phase}

The core of this algorithm is an implementation of a primitive we call a {\em -improvement phase}. This primitive, given a -feasible and -centered solution , returns in  time a -feasible and -centered solution  such that 

where  and  is the constant from Lemma \ref{lem:better_delta_lowerbound}. 

Observe that once we obtain an implementation of such a -improvement phase, we can get the desired improved algorithm as follows. We start with a -feasible and -centered solution  as in Lemma \ref{lem:initial_solution}. Next, we apply  iterations of -improvement phase to it, with


Note that after doing this, we know that if   is the final -feasible -centered solution we compute then
 

So, as long as we can show that  satisfies Invariants \ref{inv:measure_upperbound} and \ref{inv:length_upper}, we can use Lemma \ref{lem:perturbed_solution} to recover the desired near-perfect -matching in  or conclude that no perfect -matching exists in . 
Also, the overall running time of this algorithm will indeed be , as desired. 

\subsubsection*{Implementation of -Improvement Phase via Stretch-boosts}

In the light of the above discussion, we just need to focus on implementing the -improvement phase, as well as, ensuring that running it for  iterations will not violate Invariants \ref{inv:measure_upperbound} and \ref{inv:length_upper}. 



\IncMargin{1em}
\begin{algorithm}
\DontPrintSemicolon
\SetAlFnt{\small \sf}
\SetKwComment{tcc}{ }{ }


\AlFnt
\SetKw{Return}{return}
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{A -feasible and -centered solution }
\Output{A -feasible and -centered solution  with }
\BlankLine
{Initialize }\;
\While{\AlFnt{}}{
Compute the electrical -flow  associated with \;
\eIf{\AlFnt{ is -smooth on heavy arcs}}{
Apply interior-point method step from Theorem \ref{thm:main_interior_point} to   with  \;
Save the resulting solution as \tcc*[r]{progress step}
}{
Let  be such that \;
\lForEach{\AlFnt{arc  in }}{
apply -stretching to \tcc*[r]{stretch-boost}
}
Save the resulting solution as 
}
\;
}
Output  as the solution \;
\caption{Implementation of -improvement phase via stretch-boosts}\label{fig:improve_phase_nonlight}
\end{algorithm}\DecMargin{1em}

Our implementation -- presented in Figure \ref{fig:improve_phase_nonlight} -- is an iterative procedure. We maintain a -feasible -centered solution  -- initially,  is equal to . Next, as long as  we repeat the following iterative step. 

We first check if the electrical flow  associated with  is -smooth on the set of heavy arcs. 

If it is indeed the case then one can easily see that such  needs to be -smooth (on the set of all the arcs). (This uses our ad-hoc assumption that all  we compute are always -smooth on the set of light arcs.) So, in this situation, we can just apply an interior-point method step -- as described in Theorem \ref{thm:main_interior_point} -- to  with setting . (Note that by Lemma \ref{lem:better_delta_lowerbound} this setting of  is valid.)  For future reference, we call this step a {\em progress step}. After executing it, we set the resulting -feasible and -centered solution  as our current solution and proceed to next iterative step. 

Otherwise, that is, if  is not -smooth on the set of heavy arcs, then -- by Definition \ref{def:smoothness} -- there is an  such that 

where we used the fact that all measures of arcs are always at least one and  denotes the set of heavy arcs with respect to the solution . 

To cope with this situation, we perform -stretching of all the arcs in . (Note that, by Lemma \ref{lem:choosing_beta}, this operation does not change the value of  and our solution remains -feasible and -centered.) We call this operation {\em stretch-boosting} and  will be referred to as the {\em index} of this stretch-boosting. After performing stretch-boosting, we proceed to the next iterative step. 

This finishes the description of our implementation. 

\subsubsection*{Analysis}

To analyze the above procedure, let us note that due to our stopping condition, once this procedure terminates the resulting solution  satisfies our requirements. Also, there will be at most  progress steps executed. This is so, as -stretching does not affect the value of  and, by Theorem \ref{thm:main_interior_point}, each progress step decreases  by a factor of at least . Thus, as each of these steps runs in  time, the resulting total time of progress steps is , as desired.

Therefore, we can just focus on bounding the number of stretch-boost operations executed, as well as, on showing that calling our implementation of -improvement phase  times does not violate Invariants \ref{inv:measure_upperbound} and \ref{inv:length_upper}. 

We start with the former task. In this bounding of the number of stretch-boost operations, we assume that Invariant \ref{inv:measure_upperbound} holds. We will justify this assumption later when proving that our two desired invariants are indeed preserved by our algorithm. 

To do the bounding, we consider the energy  of the electrical -flow  (determined by resistances  given by \eqref{eq:hf_resistances}) that is associated with our current solution . We treat this quantity as a potential function and show the following facts:


\begin{enumerate}[(a)]\addtolength{\itemsep}{-.4\baselineskip}
\item  is always at least  and at most , for some sufficiently large constant  -- see Lemma \ref{lem:energy_bound};\label{cond:stretch_tot_energy}
\item  increases by a factor of at least , for some constant , whenever a stretch-boosting step is applied -- see Lemma \ref{lem:stretch_boosting_energy_increase};\label{cond:stretch_boosting_energy}
\item  decreases by a factor of at most  each time a progress step is executed, where  is some sufficiently large constant -- see Lemma \ref{lem:int_step_energy_decrease}.\label{cond:stretch_progress_energy}
\end{enumerate}

Note that once the above statements are established, it must be the case that there is at most  stretch-boost operation overall. To see that, assume this was not the case, i.e., that there was more than  stretch-boosts. Then, by the above statements and the fact that there is at most  progress steps we would have that

which would violate the upperbound on energy  established by statement \eqref{cond:stretch_tot_energy}.

 So, it must be then indeed the case that there is at most  stretch-boosts, which gives the desired  total running time bound. 

In the light of the above, we can turn our attention to proving statements \eqref{cond:stretch_tot_energy}-\eqref{cond:stretch_progress_energy}. We start with statement \eqref{cond:stretch_tot_energy}. This statement essentially follow from Lemma \ref{lem:hfft_energy_bound} and some simple energy-lowerbounding argument. The prove of the following lemma appears in Appendix \ref{app:energy_bound}. 


\begin{lemma}
\label{lem:energy_bound}
Let  be a -feasible and -centered solution. Provided that Invariant \ref{inv:measure_upperbound} holds, we have that

where  is the electrical -flow associated with the solution  and  is a sufficiently large constant. 
\end{lemma}


Next, we proceed to analyzing the effect of stretch-boosting on the energy . Intuitively, by the connection between congestion and energy hinted  by Fact \ref{fa:rho_vs_rt}, we know that the arcs with large congestion have to have unusually high contribution to the energy . So, as -stretching effectively doubles the resistances of such arcs, it is not surprising that it ends up significantly increasing that energy. We make this formal -- and thus establish statement \eqref{cond:stretch_boosting_energy} -- in the lemma below. Its proof appears in Appendix \ref{app:stretch_boosting_energy_increase}.


\begin{lemma}
\label{lem:stretch_boosting_energy_increase}
Each stretch-boost increases  by a factor of at least 

for some constant .
\end{lemma}

To complete our analysis, it remains to show that our potential  does not decrease too much during the progress steps. In other words, we prove statement \eqref{cond:stretch_progress_energy}. 

Note that the difficulty here stems from the fact that, in principle, the resistances of arcs can change pretty arbitrarily during a progress step. They can either increase or decrease and even by a constant multiplicative factor, thus possibly leading to severe and very non-monotone energy fluctuations. 

The key reason that enables us to control that energy change after all, is that we perform the progress step only if the flow  is -smooth. This is helpful in two ways. Firstly, because we can use it together with the connection between the change of the resistance of an arc and its congestion that we established in Theorem \ref{thm:main_interior_point}, to show that there is not too many arcs that significantly change their resistance (see Lemma \ref{lem:bounding_smooth} below). Secondly (and even more importantly), our connection between congestion and energy, allows us to conclude that -smoothness implies that there is no small (measure-wise) set of arcs that contributes unusually high portion of the energy. So, even if some small set of arcs changes its resistances significantly, it is not able to influence the overall energy by too much (see Lemma \ref{lem:int_step_energy_decrease}). (In a sense, this intuition is one of the main motivations for introducing the notion of -smoothness.) We, again, formalize this intuition below. 

First, for a given vector  and an integer , let us define  to be the set of all arcs such that

Now, we say that  is {\em -restricted}, for some measure  and  if for any , 

Now, the lemma below bounds the change of resistances during any of our progress steps. 

\begin{lemma}
\label{lem:bounding_smooth}
Let  be a -smooth electrical flow associated with . Let  be the solution obtained by applying an interior-point method step -- as in Theorem \ref{thm:main_interior_point} -- to  with . Then the vectors  and  are all -restricted (with respect to ) for some constant .
\end{lemma}


\begin{proof}
We will prove that both the vector  and the vector  are -restricted. It is easy to see that then the bound from Theorem \ref{thm:main_interior_point} will imply that  and  are -restricted too. So, choosing large enough constant  will prove the lemma.

To this end, observe  is -restricted as  by Theorem \ref{thm:main_interior_point}. On the other hand, note that if

 for some  and arc , then  for some 
 
But by -smoothness of , this means that the total measure of such arcs is at most

which establishes that  is indeed -restricted. The lemma follows.
\end{proof}

Using the above observation, we can now finish establishing property \eqref{cond:stretch_progress_energy} by proving the following lemma whose appears in Appendix \ref{app:int_step_energy_decrease}.
\begin{lemma}
\label{lem:int_step_energy_decrease}
Let  be a -smooth electrical flow associated with the -centered solution . Let  be the solution obtained by applying an interior-point methods step -- as in Theorem \ref{thm:main_interior_point} -- to  with . Then, 

where  is the electrical flow associated with  and  is a sufficiently large constant. 
\end{lemma}




\subsubsection*{Preservation of Invariants \ref{inv:measure_upperbound} and \ref{inv:length_upper}}


Now, as we completed the analysis of the running time of our -improvement phase implementation, we establish the remaining claim, i.e., we prove that executing the above procedure  times does not lead to violation of Invariants \ref{inv:measure_upperbound} and \ref{inv:length_upper}. 

\paragraph{Bounding measure increase.}

To this end, let us first focus on bounding the measure increases. By Lemma \ref{lem:choosing_beta}, we know that whenever we -stretch an arc , its measure increases by at most . So, to bound the total measure increase it suffices to bound the total measure of arcs that are affected by -stretches across all the stretch-boost operations. (Here, if the same arc becomes -stretched multiple times, in different stretch-boosts, we account for its measure multiple times.) 

In order to do that, note that by Lemma \ref{lem:stretch_boosting_energy_increase}, if  is the measure of the set of arcs that are -stretched in -th stretch-boost, we have that the total increase of energy resulting from that is at least

Also, by Lemma \ref{lem:energy_bound}, we know that we have to have that in any single stretch-boost, the energy cannot increase by more than  factor. So, we have that

and thus , for each . 

As a result, we can lowerbound the total increase of energy due to stretch-boosts by

where . 

Finally, by Lemma \ref{lem:energy_bound} and Lemma \ref{lem:int_step_energy_decrease}, as well as, the fact that we have at most  progress steps, we know that the overall (multiplicative) increase of energy resulting from all the stretch-boosts can be at most

Therefore, as  is , we have that the total measure increase  is at most
 
As a result, after executing at most  -improvement phases, the overall increase of measure can be bounded by

Now, given that by Lemma \ref{lem:initial_solution}, we start with our measure being at most  and thus have a slack of at least  measure left before Invariant \ref{inv:measure_upperbound} becomes violated, this overall increase will indeed not lead to violation of this invariant.

\paragraph{Bounding arc length increase.} To show that Invariant \ref{inv:length_upper} is preserved as well, let us first note that the only way for length of arcs to increase is due to -stretching occurring during stretch-boosts. Furthermore, we only -stretch an arc if it is heavy. So, if a given (heavy) arc  gets -stretched at some step  then its length increases by at most
 
On the other hand, by Lemma \ref{lem:choosing_beta}, the increase of measure of such arc is at least . So, as , the increase of measure of an arc is within a factor of  of increase of the length. So, as we just proved that the total measure increase is at most  (cf. \eqref{eq:total_measure}), the desired bound of  on the total length increase follows. 

Finally, as each -stretch increases the measure by a factor of at least  and -- as we discussed above -- we never -stretch anymore an arc whose measure is bigger than , no single arc will get -stretched more than  times. As a result, no single arc has its length increased by more that  that is much smaller than . Therefore, the Invariant \ref{inv:length_upper} is also preserved. This concludes our analysis. 
 

\subsection{Preconditioning the Graph \texorpdfstring{}{G}}\label{sec:preconditioning}


Our analysis from the previous section was crucially relying on the assumption that all the flows  are always -smooth on the set of light arcs. Unfortunately, this assumption is not always valid. 

To cope with this problem, we develop a modification of our algorithm that ensures that this -smoothness assumption holds after all. Roughly speaking, we achieve that by an appropriate preconditioning our solution at the beginning of each -improvement phase. This preconditioning is based on augmenting the graph  with additional, auxiliary arcs and correspondingly extending our solution on them. These arcs are very light (i.e., have small value  of flow flowing through them in augmented solution), while providing good connectivity (and thus relatively low effective resistance) between different vertices of the augmented graph. 

The underlying intuition here is that the over-congestion of a light arc  is caused by amounts of flow that are at most  (cf. Definition \eqref{def:heavy}) and thus are relatively small compared to the whole duality gap. So, by deploying these very light auxiliary arcs we encourage the electrical flow  to reroute such over-congesting flow from  and send it along auxiliary arcs. On the other hand. as the small value of this rerouted flow is small, the perturbation of our desired (non-augmented) solution introduced by these rerouting is relatively minor. Thus, we are able to deal with it relatively easily at the end of the whole -improvement phase, while still ending up making overall progress on the quality of our solution. 

\subsubsection*{Augmenting the Graph and the Solution}

The exact implementation of our preconditioning is based on modifying the execution of -improvement phase that was presented in the previous section in the following way. Let  be the  -centered and -feasible solution at the beginning of some -improvement phase. 

We start with augmenting the graph  by adding to it a new vertex , as well as,  copies of an arc  and  copies of an arc , for each vertex  of  other than , where

is the total measure (with respect to ) of all the arcs adjacent to  in . We will call these newly added arcs {\em auxiliary} and denote the augmented graph as . 

Next, we extend the solution  to that augmented graph  by assigning ,  and  to each auxiliary arc , where

with  being some sufficiently large constant to be fixed later, and the lengths of the auxiliary arcs being chosen so that the extended solution is still dual feasible. (As we will soon see, the actual lengths of auxiliary arcs are irrelevant.) Note that after this extension, the solution  remains -centered, -feasible and the value of  is unchanged. Also, observe that by Invariant \ref{inv:measure_upperbound}, the number  of arcs of the augmented graph  is still only . So, relating various quantities -- in particular, the running times of our procedures -- to either  or  results in only a constant-factor discrepancy (that we will ignore in what follows).

Now, after the above preprocessing, we run the -improvement phase implementation, as described in the previous section, on the extended solution in the augmented graph . (In Section \ref{sec:smoothness}, we will prove that the assumption that underlies the analysis from the previous section, i.e., that all the flows  are -smooth on light arcs, is indeed valid.) The only further modification here is that after each progress step we -stretch each auxiliary arc  with  (cf. Theorem \ref{thm:main_interior_point}). We will call this stretch operation {\em freezing}. (Note that as -stretching only increases the resistances of arcs, this modification is compatible with the energy-based potential argument we employed in the previous section.) This freezing ensures that the flows on auxiliary arcs do not change to significantly in our solution and thus the impact of preconditioning provided by auxiliary arcs on the quality of the final solution is minimized. We make this more precise in the following lemma whose proof appears in Appendix \ref{app:auxiliary_flow_growth}.

\begin{lemma}
\label{lem:auxiliary_flow_growth}
During the whole -improvement phase, we have that for each auxiliary arc , , for some constant . Also, the total increase of measure of auxiliary arcs in that phase is at most .
\end{lemma}



Finally, once the execution of the above -improvement phase finishes, we end up with a -centered and -feasible solution  such that , as desired. However, this solution corresponds to the augmented graph  instead of to the original graph . 

To deal with this deficiency, we first simply discard all the auxiliary arcs and correspondingly truncate the solution  to non-auxiliary arcs. Unfortunately, doing that might, in particular, render that solution not -feasible. So, to alleviate this problem, in Section \ref{sec:fixing} below, we describe a fixing procedure that, given such a truncated solution, produces the intended solution  that corresponds to the original graph , is -centered, -feasible and

(Note that in our algorithm we are executing only  -improvement phases overall. So, this additional  factor above is inconsequential.) 

As we will see, a byproduct of this fixing procedure is an increase in the measure of (non-auxiliary) arcs. However, we will show that this increase is bounded by . Thus, taking  to be sufficiently large ensures that the resulting measure increases do not lead to violation of Invariant \ref{inv:measure_upperbound}. (Note that the auxiliary arcs are always discarded at the end, so from the point of view of Invariant \ref{inv:measure_upperbound}, it suffices that by Lemma \ref{lem:auxiliary_flow_growth} the measure of these arcs is always .)

In the light of the above discussion, all that remains is to describe and analyze the fixing procedure and to show that one can indeed assume that all the electrical flows  computed during such -improvement phase are -smooth on the set of light arcs. 


\subsubsection*{Fixing Procedure}\label{sec:fixing}
We start by describing and analyzing the fixing procedure that we employ at the end of each -improvement phase. Recall that in this procedure we are given as input a -centered and -feasible solution  in the augmented graph  such that
. Our goal is to obtain 
a -centered -feasible solution  in the original graph  that satisfies .

We do this in two steps. First, we simply truncate the solution  to the original graph  by discarding all the auxiliary arcs and flow on them. Let us denote the resulting solution as . It is not hard to see that this solution is still -centered. In the following lemma -- whose proof appears in Appendix \ref{app:fixing_duality_increase} -- we argue that also the value of  has not increased by much. 

\begin{lemma}
\label{lem:fixing_duality_increase}
.
\end{lemma}

At this point, we know that the solution  is -centered and  is as small as needed. Unfortunately, this solution can still be not -feasible. 

Therefore, in the second step of our procedure, we address this last shortcoming. Our approach here requires introducing a certain simple operation. For a given some solution , as well as, some  and an arc , let us define {\em -widening} of  (in ) as an operation in which we increase the value of  by a factor of  and increase  by a factor of , where  is given via \eqref{eq:def_of_beta}.

We can view the -widening operation as a counterpart of the -stretching operation. In fact, one can see that due to symmetric nature of  and  and our choice of , Lemma \ref{lem:choosing_beta} also holds for -widening operation. (Note that in the proof of Lemma \ref{lem:choosing_beta} the roles of  and  are completely interchangeable.)

Now, our way of obtaining the desired solution  is very simple. Let us denote by  the actual demand vector of  and let  be the vector of demand differences. We start with  and for each vertex  of  other than , we do the following. If  (resp. ), we apply -widening to the arc  (resp. ) with . 

We take  to be the resulting solution. It is easy to see that this solution is -feasible now. Also, by Lemma \ref{lem:choosing_beta}, we know that this solution remains -centered and that , as needed.

So, we just need to establish the claimed bound of  on total measure increase resulting from this procedure. To this end, note that by Lemma \ref{lem:choosing_beta} this increase is at most

where we used the Fact \ref{fa:central_vs_max_min} and Invariant \ref{inv:length_upper}, as well as, we applied Lemma \ref{lem:bound_on_s} to conclude that each  is . 

Thus, in the light of the above, it only remains to bound . 

\begin{lemma}
\label{lem:fixing_value_flow}

\end{lemma}

\begin{proof}
One can see that we can bound  by bounding the total (additive) change of the flow  on all auxiliary arcs during the whole execution of -improvement procedure. Furthermore, as the flow  changes only during progress steps, and there is at most  of them, it suffices to prove that in each progress step this change is at most . 

Now, by Theorem \ref{thm:main_interior_point} and Lemma \ref{lem:auxiliary_flow_growth}, this (additive) change at step  can be bounded as

where  is the set of auxiliary arcs. By Cauchy-Schwarz inequality, we get that 
 
where we used \eqref{eq:worst_case_rho} and the fact that by Lemma \ref{lem:auxiliary_flow_growth}  is . Similarly, we obtain that

where we used that fact that . 

Plugging the above to bounds back into \eqref{eq:fixing_inter_flow_bound} and recalling that we always set , we obtain that


where we utilized \eqref{eq:def_of_fauxliary}, as well as, the fact that, due to our stopping condition for -improvement phase, we can always assume that  is . The lemma follows.
\end{proof}

Clearly, by setting  to be a sufficiently large constant, we can ensure that the total measure increase due to fixing procedure will not lead to violation of Invariant \ref{inv:measure_upperbound}.

\subsubsection*{-smoothness on Light Arcs}\label{sec:smoothness}

As the final step of our analysis, we prove now that in the course of our algorithm -- after the modifications described above -- all the electrical flows  that we compute are indeed -smooth on the set of light arcs. That is, the assumption underlying the analysis performed in Section \ref{sec:heavy} is indeed justified.

To this end, let us fix some -feasible and -centered solution  in our augmented graph  and let  be the associated electrical -flow. For convenience, we drop from now on all the references to  in our notation. 

Our proof will take advantage of the dual nature of electrical flows. In particular, it will be instrumental for us to consider the vertex potentials  that induce the electrical flow  via \eqref{eq:potential_flow_def}. The crucial property of these potentials is that they provide an embedding of all the vertices of  into a line. To make it precise, for a given arc , let us denote by  (resp. ): the value of  (resp. ), if ; and the value of  (resp. ), otherwise. In other words,  (resp. ) is the coordinate of the left-most (resp. right-most) endpoint of  in this line embedding. 

Observe that by \eqref{eq:potential_flow_def} and definition of resistances  (cf. \eqref{eq:hf_resistances}), we have that for a given arc , the distance  between the embeddings of its endpoints is 

and thus by Fact \ref{fa:central_vs_max_min}

Furthermore, for two subsets  of vertices of , let us define the distance  between these sets to be

Also, let us call two such subsets  and , {\em -separated}, for some  and integer , if  and , where  and  is defined in \eqref{eq:def_of_a}.


Now, assume for the sake of contradiction that  is not -smooth on the set of light arcs, i.e., there exists an  such that

where  denotes  with the set  defined by \eqref{eq:def_of_C} and  denotes the set of heavy arcs. Our main goal is to show that in this case there exist two subsets  of vertices that are -separated with 

where  is the constant from Definition \ref{def:heavy},  is given by \eqref{eq:def_of_fauxliary}, and   is a sufficiently large constant that does not depend on  and will be set later.

To motivate this goal, we prove the following lemma.


\begin{lemma}
\label{lem:separated_sets}
If there exist  that are -separated then 

provided  is chosen to be large enough.
\end{lemma}

Observe that the conclusion of this lemma violates the bound from Lemma \ref{lem:energy_bound}. Thus, the resulting contradiction would allows us to conclude that  indeed needs to be -smooth on the set of light arcs, as we wanted to prove. 

\begin{proof}
Note that as , it must be the case that either  or . (Recall that  is the special vertex of  that is adjacent to all the auxiliary arcs.) Let us assume -- without loss of generality -- that the first case holds.


Now, as , we know that, in particular, . This, in turn, means that at least  of auxiliary arcs  must have . Furthermore, by Lemma \ref{lem:auxiliary_flow_growth}, we know that all but  of these arcs have measure . So, as  is , by ensuring that the constant  in the definition of  (\eqref{eq:def_eta}) is big enough, we can conclude that the set  of auxiliary arcs with  and  has size of at least .

So, by \eqref{eq:delta_embedding_estimation} and Lemma \ref{lem:auxiliary_flow_growth}, we have that, for any such arc  in ,

where we used the fact that  and , for all  in .

Now, the above inequality enables us to lowerbound the energy  of the flow  using sole contribution of arcs in . We get that 
where we used the definition of  \eqref{eq:hf_resistances} and Fact \ref{fa:central_vs_max_min}.

So, once  is chosen to be large enough constant -- which we can always ensure to be the case -- the lemma follows. (Note that at this point the constant  is fixed already and we will make sure that when we later set the constant , it does not depend on the value of .) 
\end{proof}



\subsubsection*{Finding the -separated Sets}

In the light of the above, it remains to establish how condition \eqref{eq:embedd_local_non_smooth} implies the existence of such -separated sets  and .  To this end, for a given , let us define  (resp. ) to be the set of vertices  with  (resp. ). Also, let  denote the set of arcs  of  such that . 

Now, let  be the smallest  such that . If  then taking  and  will clearly constitute the -separated sets we are looking for. 

So, we can focus on the case that . Let us then take . Note that, as  is smaller than the number of all auxiliary arcs, we need to have . Next, let us take . Clearly, . Therefore, once we show that ,  and  will constitute the desired -separated sets.

We proceed now to showing that indeed . Let us define  (resp. ) to be the total flow of  (resp. ) flowing through the arcs in . We will be interested in two quantities

where  is an interval . (Observe that if the interval  was not excluded,  would be equal to the energy  of the flow .)

\paragraph{Lowerbounding } First, we want to lowerbound . To this end, we note that by \eqref{eq:delta_embedding_estimation}, for any  (recall that  and thus, in particular, is contain only light arcs), we have that

where we also used \eqref{eq:def_of_C}, \eqref{eq:def_delt_star_k_star}, and Definition \ref{def:heavy}.

As the interval  has length , this means that for any arc , the interval  has length of at least 


This, in turn, implies that even if we account  for contributions of the arcs from  only, we have that

where we used \eqref{eq:def_of_C}, \eqref{eq:delta_embedding_estimation}, and \eqref{eq:embedd_local_non_smooth}. 

\paragraph{Upperbounding } Now, we want to upperbound the value of . To do that, let us define  to be the set of arcs that have at least one endpoint outside of the interval . Note that by our way of setting up the auxiliary arcs and the fact that by Lemma \ref{lem:auxiliary_flow_growth} and \eqref{eq:measure_increase_phase}, the total increase of measure of arcs during the -improvement phase is , we have that
 
So, if we are able to show that  and ensure again that the constant  in definition of  \eqref{eq:def_eta} is large enough, we will prove that , as desired. 


To establish such lowerbound on , we use \eqref{eq:delta_embedding_estimation} and observe that

where we noted that the only arcs that can contribute to  are all in the set . 
Therefore, by Fact \ref{fa:central_vs_max_min} and Cauchy-Schwarz inequality, we have that

So, putting the above two bounds together, we get that

where we also used \eqref{eq:worst_case_rho} and Lemma \ref{lem:hfft_energy_bound}.

At this point, our last needed observation is captured by the following lemma.
\begin{lemma}
\label{lem:Astar_vs_Ahat}
For any , we have that .
\end{lemma} 

Notice that once the above lemma is established, we have that 

and, as a result, we can put \eqref{eq:precond_hA_lowerbound} and \eqref{eq:upperbound_on_A_star} together to obtain

once the constant  in the definition \eqref{eq:def_delt_star_k_star} of  is taken to be large enough. (Note that the term  in the definition of  \eqref{eq:def_delt_star_k_star} is bounded by a constant that is independent of . So, indeed  does not depend on , as we wanted to ensure.)

Therefore, by \eqref{eq:precond_os_vs_ustar}, the above bounds shows that indeed , as needed. 

At this point, we just need to perform the remaining proof of the lemma and the analysis of our improved algorithm will be concluded.

\begin{proof}
The simple, but fundamental, observation we need to make here is that the flow  -- being an electrical -flow induced by vertex potentials  via relationship \eqref{eq:potential_flow_def} -- is always flowing in one direction, i.e., from left to right, with respect to the line embedding given by .  This, together with the fact that  is a -flow, implies that


On the other hand,  is also a feasible -flow, which means that the net inflow into  of  has to be at least . This gives us that

as we wanted to establish.
\end{proof}

 \section{Electrical Flows and the Central Path}\label{sec:proof_main_interior_point}

In this section, we describe how we can use electrical flows to advance our solution along the central path. In other words, we describe and analyze the implementation of the improvement step and thus prove Theorem \ref{thm:main_interior_point}. This implementation is directly inspired by -- and, in fact, can be seen as a reinterpretation of -- the improvement steps used in path-following method. 

Recall that in the improvement step, we are given a -centered -feasible solution  and our goal is to compute, in  time, a -centered -feasible solution  with 


We perform this improvement in two main steps. The first one -- the descent step -- uses the electrical flow  associated with   and the corresponding vertex potentials  that induce it, to perform a primal and dual update that results in a new, intermediate, solution . This intermediate solution is -feasible and has  as desired, but it might be not -centered anymore. To fix that, in the second -- centering -- step, we compute the desired solution  out of  by using another electrical flow computation that again provides a primal and dual update.

 We describe and analyze both of these steps below. Note that as each of these two steps requires only one computation of electrical flow, it can be easily implemented to run in  time, as needed.

\paragraph{Descent Step}

Let  be the electrical -flow associated with the solution  and let  be the vertex potentials that induce . Consider a new primal-dual solution  given by

for each arc  in , where  satisfies conditions of the theorem and we also used the definition \eqref{eq:hf_resistances} of the resistances that determine , as well as, the relationship \eqref{eq:potential_flow_def} between electrical flow and the vertex potentials that induce it. 


Observe that as  is a convex combination of two -flows -- the flows  and  -- it also is an -flow.  Furthermore, as all , we have  and thus, for each arc ,

and similarly

So,  is -feasible, as desired.

Let us now analyze the value of . To this end, observe that, for any arc ,

So, by definition \eqref{eq:def_hmu} and the fact that  for all , we see that 

and this inequality would be an equality if the second-order terms (i.e., terms quadratic in ) were ignored. (Also, if these terms were not present, the centrality of the solution would be preserved too.)

Finally, let us focus on analyzing the centrality of . To this end,  note that by definition \eqref{eq:def_centrality} and by \eqref{eq:duality_change} above we have

In the above derivation, the first inequality follows as the  is always minimized by taking . We also used the fact that  is -centered, Fact \ref{fa:central_vs_max_min} and the upperbound on .

Therefore, we see that the price of making progress on the duality gap is that the centrality of our solution could deteriorate by a factor of at most three. 

\paragraph{Centering Step}

To alleviate this possible increase of centrality, we apply a second step that restores the centrality back within the desired bounds while not increasing the duality gap (so to not to counter the progress on the duality gap that we just achieved).

To this end, consider a flow  in  defined as

for every arc  of . Note that the flow  might (and actually will) not be feasible in , as some of  can be negative.

Now, consider a flow  given by

for each arc . Observe that  is feasible in  (i.e., , for all ) and 

for each arc . That is,  is -centered with .

So, this solution would be a perfect candidate for  except that the flow  does not need to be a -flow and thus this solution might not be -feasible. 

To fix that -- and obtain our desired solution  -- let  be the demand vector of the flow , and consider an electrical -flow  that corresponds to resistances

for each arc  and let  be the corresponding vertex potentials.

Let us define  to be

for each arc . 



Clearly, now  is a -flow, as desired. Let us analyze its centrality. To this end, let us fix some arc , and notice that

where we used \eqref{eq:perfect_centering_of_intermediate}. So, we see in particular that 

as needed. 

Now, by our derivation above, we have that


To bound the resulting expression, let us note that by Cauchy-Schwarz inequality and the fact that measures are always at least  we have 


Now, the key insight here is that by \eqref{eq:tfft_resistances}, 

So, by bounding the energy of the electrical flow  we will be able to bound the centrality of our solution . 
To bound this energy, we will first bound the energy  of the flow  and use the fact that both  and  are -flows and thus, by definition,  is minimizing the energy among all the -flows. 

Observe that by definition \eqref{eq:def_of_offstar} of the flow , the fact that  is -centered -- cf. \eqref{eq:centrality_increase} -- and Fact \ref{fa:central_vs_max_min}, we have that

where we also used the definition \eqref{eq:offprime_def} of the flow .


In the light of the above discussion, we can conclude that 

as . So, indeed  is -centered. 

Now, to prove that  is also -feasible, we just need to show that for any arc , 

To this end, note that by \eqref{eq:perfect_centering_of_intermediate} and \eqref{eq:energy_estimate_final} we have


Thus, indeed, we can conclude that we obtained a -centered -feasible solution  with , as desired. 

This concludes the proof of the first part of the Theorem \ref{thm:main_interior_point}. The proof of the second part appears in Appendix \ref{app:interior}. 

 \section{Rounding Fractional Bipartite -Matchings}\label{sec:rounding}

In this section, we show how given a fractional -matching  in some bipartite graph  with  edges, one can find in  time an integral -matching  in  whose size is at least . In other words, we prove Theorem \ref{thm:rounding_matchings}.

\subsubsection*{Rounding Perfect Matchings}
Let us first consider the case when  is just a fractional perfect matching, i.e.,  for all vertices and the size  of  is , i.e., the fractional degree of each vertex in  is . We claim that in this case we can just use Theorem \ref{thm:regular_bipartite_matchings} to obtain an integral perfect matching in  time. 

To see why this is the case, consider a  matrix  in which rows and columns are indexed by vertices from  and , respectively, and the entries are given by  if the edge  exists in ; and , otherwise. Observe that if  is perfect and all  are equal to  then we need to have . Thus,  is a square matrix. Furthermore,  needs to be also doubly-stochastic, as for any row indexed by vertex  (resp. column indexed by vertex ), the sum  (resp. ) of the entries in this row (resp. column) is equal to  (resp. ). So, invoking Theorem \ref{thm:regular_bipartite_matchings}, we can obtain in  time an integral matching  in the support of  that is also the support of the edge set  of our graph . 

\subsubsection*{Rounding Non-Perfect Matchings}

Now, to recover the desired integral matching in the case when  is not necessarily perfect (but still all  are equal to ), our first step is to extend  to a perfect matching  in a certain augmented graph  that is created from  by adding some dummy edges and vertices to it.


More precisely, let  (resp. ) be the total deficits of vertices in  (resp. in ), i.e., 

 Note that the size  of  has to be exactly . We add to the vertex set ,  (resp. to the vertex set , ) dummy vertices  (resp. ). Next, we extend the fractional matching  to  by going over each non-dummy vertex  (resp. ) and fractionally matching it to the dummy vertices  (resp. ), so to ensure that its fractional degree becomes  and the fractional degree of dummy vertices never exceeds one. It is not hard to see that by employing a simple greedy approach we can achieve this goal in  time and, furthermore, ensure that: (1) each non-dummy vertex is matched to at most two dummy vertices in ; (2) at the end, there are at most two dummy vertices, say,  and , (one on each side of the bipartition) that are yet not fully matched in . To alleviate the latter problem, we just match these two dummy vertices to each other (one can check that their deficits have to be equal) and take the set of edges  of our augmented graph  to be the support of the matching . (Note that by property (1), the size of this support will be still .)

Clearly,  is a perfect matching in , so we can use the -time procedure we described above to get an integral perfect matching  in that graph. Once we do that, we take our desired integral matching  in  to be  after we removed from it all the edges of  that are not in , i.e., all the edges that are incident to dummy vertices. Obviously,  is a feasible matching in  and it is integral. To see that its size is at least , note that, as there is at most  dummy vertices in , there could be at most that many edges incident to these vertices in . But, as  is perfect, its size is equal to 

where we used the fact that . Thus, indeed after removing at most  edges from , the resulting integral matching  will have its size  to be at least , as desired.



\subsubsection*{Rounding -Matchings}

In the light of the above, it remains to show how to deal with the case when in the demand vector  there are some  that are bigger than  (and thus some of the entries of  could be bigger than , as well). To this end, let us observe first that if there is an edge  with , we can just subtract  copies of this edge from our matching right away, while decreasing the demands  and  of 's endpoints accordingly, i.e., by . (Note that by feasibility of , .) So, one can see that if  is the fractional matching  after we made such transformation and  are the corresponding demands, then once we compute an integral -matching  of size at least  from , we can just add back these subtracted  copies of edge  to  to obtain the desired integral -matching  of size at least . 

Therefore, we can assume from now on that in our -marching  all s are smaller than one (but still we can have some demands  to be bigger than one). To round such fractional -matchings, for each vertex  that has its demand  bigger than , we split it into  vertices  -- each with demand one. Next, for every edge  that was previously incident to , we connect it to the new vertices and distribute its fractional weight  in  among these new vertices. Again, by applying a simple greedy approach we can ensure that each edge is connected to at most two among the vertices  and none of these vertices has its fractional degree bigger than . (Note that this means, in particular, that once we apply such splitting to all vertices with  then the support of the corresponding ``split'' fractional matching is at most by a factor of four larger than the support of .) Clearly, at this point, we are again in situation where we just need to round a fractional bipartite matching (with all demands being at most ). Thus, we can use our rounding procedure we described above and recover the integral matching we are seeking. This finishes the proof of Theorem \ref{thm:rounding_matchings}.
 \vspace{10pt}

\noindent{\bf Acknowledgments.} We are grateful to Andrew Goldberg, Jonathan Kelner, Lap Chi Lau, Gary Miller, Richard Peng, Seth Pettie, Daniel Spielman, and Shang-Hua Teng for a number of helpful discussions on this topic. We also thank Monika Henziger, Satish Rao, and Jens Vygen for useful feedback on the manuscript.  \bibliographystyle{my}
\bibliography{../main}
\appendix
\section{Proof of Lemma \ref{lem:effective_conductance}}\label{app:effective_conductance}
Let  be the value of right-hand side of the equality we need to establish, and - for notational convenience - let us denote the energy  as . So, our goal is to show that  and that taking  attains the minimum .

We start by noting that, for any vertex potentials , we have

where we used the fact that  is a -flow (cf. \eqref{eq:conservation_constraints}).

Note that by the above calculations and the definition of  we have 

where we used \eqref{eq:potential_flow_def} and the definition of energy \eqref{eq:def_energy_flow}. Therefore, we see that  as by \eqref{eq:def_energy_potentials}



Now, let  be the potential such that  and let  with , for each , be the corresponding flow induced via \ref{eq:potential_flow_def}. (Note that in principle   does not need to be a -flow).

From \eqref{eq:conductance_proof_identity} we get that 

where we again used \eqref{eq:potential_flow_def} and the fact that  by definition.

We claim that the energy  of  (and thus the value of ) is at least . To this end, let us note that

As we have seen in \eqref{eq:effective_conductance_1}, , thus  and we can write

as  for any . 

So,  too and thus . Also, by \eqref{eq:effective_conductance_2} and \eqref{eq:effective_conductance_3} we see that  indeed attains the minimum, as desired.

\section{Proof of Corollary \ref{col:rounding_flows}}\label{app:col_rounding_flows}

Let  be a fractional feasible - flow of value  in  and let us consider first the case when  is integral. Recall that the reduction presented in Section \ref{sec:reduction} allows one to obtain in  time an instance of bipartite -matching problem -- corresponding to some bipartite graph  -- that has a property that if there exists a feasible - flow of value  in  then  has a perfect -matching. Now, the crucial observation is that the proof of that property presented in Section \ref{sec:reduction} is fully constructive and, in particular, provides an -time algorithm that produces such a perfect -matching in  out of a feasible - flow in  of value . Furthermore, this construction also works for fractional flows, it just produces a perfect -matching that is fractional. 

In the light of the above, we can simply apply this transformation to our flow  and get a fractional perfect -matching  in . Next, we can use the rounding procedure from Theorem \ref{thm:rounding_matchings} to obtain in  time a perfect -matching  in  that is integral. (Note that since  is always integral, so is the size of any perfect -matching.) This, in turn, allows us to utilize another property of the graph  that was established in Section \ref{sec:reduction}. Namely, that out of any integral perfect -matching in , one can extract -- in  time -- an integral and feasible - flow  in  of value . Clearly, by combining all of the above steps, we get our desired integral - flow. 

Finally, to deal with the case when  is not integral, we just add an arc  to , set its capacity to , and put a flow of  on it. Obviously, now we have a feasible - flow of value  in such modified graph  and  is integral. Therefore, we can use our approach we described above to get an integral and feasible - flow  in this graph and  will have a value of . Note that  can have non-zero flow on the arc  that we added, but as this arc has capacity of , there can be exactly one unit of flow on this arc. So, if we simply remove it from , we will get an integral and feasible - flow in the original graph  and the value of  will be , as desired. This concludes the proof of the corollary. \section{Appendix to Section \ref{sec:reduction}}\label{app:reduction}

\subsection{Correctness Analysis}

It is easy to verify that the produced -matching instance is indeed bipartite (we have edges only between different sides of bipartition  and ), has exactly  vertices,  edges, and . So, we just need to establish the claimed connection to existence of feasible - flows in the graph . 


\subsubsection*{From Flow  to Perfect -matching }

To this end, assume that there exists a feasible - flow  in  of value . To see that a perfect -matching in  exists, consider a -matching  that, for each arc  in , takes exactly  edges  and  edges  and . Then, for every vertex  of  other than  and ,  takes  copies of the edge . 

To see that  is indeed a perfect -matching, observe that due to feasibility of  (cf. \eqref{eq:capacity_constraints}),   for each arc , and thus . Also, by the construction of , all vertices  and  have exactly  edges adjacent to them in . So, they are fully matched. To see that all vertices  and  are fully matched too, consider some . Indeed, by definition of , we have exactly  (resp. ) edges adjacent to  (resp. ), where we used the fact that , as  obeys flow conservation constraints \eqref{eq:conservation_constraints}. Finally, in the case of vertex  (resp. ) we have that their degree in  is exactly  (resp. ), due to the value  of the flow  being exactly . So, indeed such  is perfect, as claimed.

\subsubsection*{From Perfect -matching  to Flow }

Now, to see that given a perfect -matching  in  we can quickly, i.e., in  time, recover an - flow of value  that is feasible in , consider a flow  given by  for each arc  in . That is, the flow  on an arc  is equal to the number of times a copy of an edge  appears in . Note that as the demands  and  of the endpoints of each edge  are equal to ,  is feasible in .

Finally, to prove that  also preserves flow conservation constraints (cf. \eqref{eq:conservation_constraints}), note that as  is perfect, it has to be that for any vertex  and  (resp. )  (resp. ). So, if we do not take into account the edges , each vertex  (resp. ) has exactly  (resp. ) edges adjacent to it in . This means, in particular, that in case of  (resp. ) we need to have that  (resp. ) and thus  (resp. ), i.e., the value of  is . Furthermore, for any vertex  other than  and , as  is perfect,  w need to have that  (resp. ). Therefore, , i.e.,  obeys all flow conservation constraints. So, indeed  is a feasible - flow of value  in , as desired.

Lastly, it is worth pointing out that even though in the above proof we assume that both the - flow  and the -matching  are integral, the proof goes through unchanged in the case when  and  are fractional. We just will have that if  is fractional then so will be the corresponding -matching  and vice versa. 



 \section{Appendix to Section \ref{sec:simple}}\label{app:basic_algorithm}

\subsection{Proof of Lemma \ref{lem:initial_solution}}\label{app:initial_solution}


Let us take  to be the all-ones vector  (this corresponds to  assigning zero value to all vertices). Next, let the flow  and measures  be defined as follows. 

For each arc of the form  in , we give it a measure of one in  and a flow of one unit is sent through it in . Now, for each vertex  (resp. ) in , let  (resp. ). If  (resp. ) then we put a flow of one and measure of one on the arc  (resp. ) and a flow and measure of  (resp. )  on the arc  (resp. ). On the other hand, if  (resp. ) then we put a flow and measure of  (resp. ) on the arc  (resp. ) and a flow and measure of one on the arc  (resp. ).

One can verify that the resulting flow  is indeed a -flow (again, one needs to use here the fact that , as otherwise there is no perfect -matching in ) and thus the solution is primal-dual feasible. 

Also, the total measure  of all the arcs is at most . Finally, we have that  for all arcs  and thus the solution is indeed -centered and , as desired.

 \section{Appendix to Section \ref{sec:improved}}\label{app:improved_algorithm}




\subsection{Proof of Lemma \ref{lem:choosing_beta}}\label{app:choosing_beta}


That  follows directly from Fact \ref{fa:central_vs_max_min}.

Let us show now that . To this end, let us define  so as 


By definition \eqref{eq:def_hmu}, we have that


Now, to bound the centrality of , we need to bound the value of . However, as  and the two solution coincide on all the arcs except , it suffices to analyze the change in contribution of arc  to the centrality of the solution. Namely, we just need to show that 


To this end, observe the right side of the above inequality is just


So, we need to show that 



But this is true as 

where we used that fact that  since, by Fact \ref{fa:central_vs_max_min}, . 

\subsection{Proof of Lemma \ref{lem:bound_on_s}}\label{app:bound_on_s}


Note that by construction of the graph  and by Invariant \ref{inv:length_upper}, we have that for any two vertices ,  in  there is a directed path from  to , as well as, a one from  and , with each one of them consisting of at most two arcs and having length at most . As  is -feasible then it is, in particular, dual feasible. So, this implies that if  is the embedding of the vertices of  into a line corresponding to the slack variables , then  is at most  as well. Thus, we can conclude that for any arc  in , we have that  is at most , as desired.


\subsection{Proof of Lemma \ref{lem:perturbed_solution}}\label{app:perturbed_solution}


Note first that all the arcs in the original version of  have length  and -stretching can only increase these lengths. This means that it is still true -- as in the proof of Lemma \ref{lem:mincost_to_matchings} -- that  is an upper bound on the total flow between the vertices  and  that is not flowing over the direct arcs  reflecting the original edges of . 

Furthermore, as by Invariant \ref{inv:length_upper} the total increase in the length of the arcs of  is , the cost  of the flow that encodes the perfect -matching in  (cf. the proof of Lemma \ref{lem:mincost_to_matchings}), can increase to at most . (We use here the fact that  never flows more than one unit of flow through any of the arcs.) So, we can assume that the cost  of the flow  we have is also , as otherwise we could conclude that no perfect -matching exists in . 

However, then it must be the case that the total flow in  that does not correspond to taking the direct arcs is at most . Thus, the fractional -matching obtained by taking only the flow that uses these direct flow-paths will still result in a near-perfect -matching in . 


\subsection{Proof of Lemma \ref{lem:energy_bound}}\label{app:energy_bound}


Note first that the upperbound follows directly from Lemma \ref{lem:hfft_energy_bound} as long as we ensure that  (which indeed will be the case). 

To establish the lowerbound, let us note first that without loss of generality we can assume that in our -matching instance ,  and thus, as our graph  is sparse, . (Otherwise, we just exchange the roles of  and  in what follows below.)  

Let us define a vector of resistances  given by

where  is the set of all arcs incident to  in . In other words,  corresponds to setting to zero resistances (i.e., collapsing) of all the arcs that are not adjacent to some vertex in ; and making the resistances of arcs that are adjacent to such  equal to their original resistances in . This means, in particular, that , for each arc , and thus, by Rayleigh Monotonicity principle (cf. Fact \ref{fa:rayleigh_monotonicity}), we know that if  is the electrical -flow determined by resistances  then


Therefore, we can just focus on lowerbounding . To this end, note that after collapsing all the arcs that were not adjacent to some vertex in , we can think of  as a graph that consists only of vertices from  and a single vertex  that represents the remaining collapsed vertices. Furthermore, as there is no arcs in  between different vertices , all the arcs in this collapsed graph are of the form  or  for some .

As a result,  is equal to

where  is the effective resistance between vertex  and  with respect to resistances  and the last equality follows as  is a -flow and all arcs are connecting to . 

Now, to lowerbound , for some , note that by definition of  \eqref{eq:hf_resistances}, the fact that  for all , and Fact \ref{fa:central_vs_max_min}, we have that

where  and we used the well-known formula for effective resistance of a circuit that consists solely of parallel arcs. 

So, all the above considerations allow us to observe that

where  and we used the fact that , as well as, that for any -dimensional vector , . 

Thus, it remains to provide an upperbound on . To this end, let us decompose the flow  into flow-paths (whose endpoints are vertices  and ) and flow-cycles. Clearly, the total contribution of the flow-paths to  can be at most , since our -matching instance is balanced. On the other hand, as length of any flow-cycle is at least two and each flow-cycle contributes its whole volume to the duality gap  (as flow-cycles do not exist in optimal solution), the total contribution of flow-cycle to  is at most . Thus, by \eqref{eq:duality_gap_bound} and Invariant \ref{inv:measure_upperbound}, this contribution is at most . 

Therefore, we can conclude that  and since  is  we have

where  is an appropriately chosen constant.

\subsection{Proof of Lemma \ref{lem:stretch_boosting_energy_increase}}\label{app:stretch_boosting_energy_increase}

Let us denote by , , and , respectively, the resistances , measures , and electrical -flow  before stretch-boost and by , , and  the corresponding objects after stretch-boost. Also, let as define , where  is the index of the stretch-boost. In this notation, we want to show that  

Clearly, the last inequality follows as  for all arcs . To see that the second inequality holds, observe that by \eqref{eq:boosting_condition} we have that

and thus

 Therefore, once the first inequality in \eqref{eq:bound_increase2} is established, our lemma will follow by choosing .


So, let us proceed to establishing that inequality. By Lemma \ref{lem:effective_conductance}, we know that if  is the vector of vertex potentials corresponding to the flow  and resistances  then 

where , for each , and . 

Now, consider an arc . By \eqref{eq:potential_flow_def}, the definition of the sets  \eqref{eq:def_of_C}, and Fact \ref{fa:rho_vs_rt}, we have that

where we also used Lemma \ref{lem:hfft_energy_bound}. In other words, the contribution of arc  to the sum in \eqref{eq:contr_hfft} constitutes at least -fraction of this sum.

Next, observe that, by definition of -stretching, we need to have that the resistance doubles, i.e., , for all the arcs , and remains the same for other arcs, i.e., , for .  
This means that



But, by Lemma \ref{lem:effective_conductance}, we know that the above estimation provides an upper bound on the value of , i.e., we have

where we used the fact that , by definition of . Multiplying both sides by  and noticing that  for any , gives us the desired inequality in \eqref{eq:bound_increase2}. 

\subsection{Proof of Lemma \ref{lem:int_step_energy_decrease}}\label{app:int_step_energy_decrease}

Let us denote the solution  by , the associated electrical flow  by , and let  be the resistances  corresponding to this solution (cf. \eqref{eq:hf_resistances}). Also, let , , and , denote these respective object after the interior-point method step is applied. 

In this notation, our goal is to show that


 
To perform such lowerbounding of the energy decrease, we proceed similarly as we did in the proof of Lemma \ref{lem:stretch_boosting_energy_increase}. Namely, by Lemma \ref{lem:effective_conductance}, we know that 

where  and . We want to show that if we keep the same vertex potentials  and change the resistances to  then still the corresponding sum -- as in the equation above -- will not increase by too much (and thus provide a good upperbound on ). 

More specifically, recall that by Theorem \ref{thm:main_interior_point}, for any arc ,

and that . 

So, by Lemma \ref{lem:effective_conductance}, we have that


where we use the fact that  when  and that by definition of , .  

Furthermore, by \eqref{eq:potential_flow_def} and definition of , we have

So, we again just need to show that 

and the lemma will follow.

To establish this last claim, note that for any arc ,  is just the fraction of energy of the flow  (with respect to resistances ) that is contributed by the arc . So, by Fact \ref{fa:rho_vs_rt} and Lemma \ref{lem:energy_bound}, we have that 

whenever  (cf. \eqref{eq:def_of_C}), for some integer . 

As a result, we can conclude that

where  denotes  (cf. \eqref{eq:def_t_lambda}), \eqref{eq:ener_decr_bounding_re}, and the fact that 

Now, by -smoothness of  (cf. Definition \ref{def:smoothness}), we get that 

for each  and . Also, the fact that by Lemma \ref{lem:bounding_smooth}  is -restricted implies that, for any fixed , 

Therefore, we can see that for any , we have

for some . Here, we used the fact that by \eqref{eq:internal_smoothness_lambda}, , if  and that the expression we are bounding will be maximized if the set  contains as many high-energy arcs as possible. (Note that due to the constraint  and the bound \eqref{eq:internal_smoothness_lambda},  can then only contain all the arcs in sets  for all  up to .) So, we can conclude that



To finish our overall bound, we just need to note that by our above derivation, as well as, the fact that  if  (as ),

as desired, once  is chosen to be large enough.

\subsection{Proof of Lemma \ref{lem:auxiliary_flow_growth}}\label{app:auxiliary_flow_growth}

We start by bounding the increase of measure due to freezing. Let us fix some progress step  and some auxiliary arc  that is in  for some , where  denotes , as defined in \eqref{eq:def_t_lambda}. (Note that only arcs in  with  can be frozen at step .) 

By Lemma \ref{lem:choosing_beta} and definition of , the increase of measure resulting from -stretching  is at most

However, by Lemma \ref{lem:bounding_smooth}, we know that the vector  is -restricted. Therefore, the total contribution to measure increase of all the frozen arcs in  is at most

where we used the fact that . 

So, as there is at most  different sets  that contribute in each progress step, and there is at most  progress steps, the overall increase of measure due to freezing is at most , as required. 

Note that once we establish below that all auxiliary have always  that is within a factor of  of , the fact that  is much smaller than  will imply that all auxiliary arcs are always light and thus never get stretch-boosted. So, the measure of auxiliary arc can increase only due to freezing and we have already bounded this increase above.

Now, to prove the first part of the lemma, let us fix some auxiliary arc . Initially,  is equal to . So, one just need to argue that the total multiplicative change of  during the course of the -improvement phase execution is bounded by a constant. 

To this end, note that the flows on arcs change only during the progress steps. So, by Theorem \ref{thm:main_interior_point}, if we fix some auxiliary arc , its overall flow changes by a factor of at most


Therefore, the total change of flow of  during progress steps that have not resulted in freezing it, can bounded by

that is constant, as desired. 

So, now we just need to focus on bounding the change of flow on  resulting from the remaining progress steps, i.e., the ones in which it was frozen. To this end, recall that whenever  in some step  then freezing -stretches . By Lemma \ref{lem:choosing_beta}, the resulting increase of measure of  is at least by a factor of 
 
while the change of the flow is by a factor of at most


Therefore, as the former factor is significantly larger than the latter one,  (by Theorem \ref{thm:main_interior_point}), and as from discussion above we know that once the measure of an arc becomes larger than  it will never be frozen again, the constant bound of the maximum multiplicative change of the flow of an auxiliary arc follows. This concludes the proof of the lemma.

\subsection{Proof of Lemma \ref{lem:fixing_duality_increase}}\label{app:fixing_duality_increase}


By definition of  (cf. \eqref{eq:def_hmu}), we have that

where , for each arc .

On the other hand, we have that

where  is the set of non-auxiliary arcs of . 

Now, observe that by definition of -centrality (cf. \eqref{eq:def_centrality}) we have

So, by applying Cauchy-Schwarz inequality we get that

where we use the fact that . 

By putting the above inequality and \eqref{eq:fixing_lem_lem} together, the lemma follows.


 

\subsection{Handling Approximate Nature of Electrical Flow Computations}\label{app:inexact_elec_flow_disc}



Here, we discuss how one can adjust our algorithm developed in Sections \ref{sec:simple}--\ref{sec:proof_main_interior_point} to nearly-linear time electrical flow computations that are only approximate -- as in Theorem \ref{thm:vanilla_SDD_solver} -- instead of being exact. 

To this end, let us first recall that we are using electrical flow computations in two places of our algorithm. One is our improvement step described in Section \ref{sec:proof_main_interior_point}. There, to make the descent step, we compute the electrical flow  associated with our solution and then, to make the centering step, we compute the electrical flow . The other place where we use electrical flow computations is to check the -smoothness condition (cf. Definition \ref{def:smoothness}), that is to check which arcs are in the sets , for . (Note that it is sufficient for us to know this classification only approximately, say up to a constant factor.)

Observe that in all these three cases, we end up computing some electrical -flows that are determined by some resistances  defined as  (cf. \eqref{eq:hf_resistances}), for each arc , and where  is some -centered and -feasible solution  with  and both  and  being . Furthermore, we always have that , all variables  are bounded by a constant (see Lemma \ref{lem:bound_on_s}), and the duality gap is . (All the definitions that are relevant here can be found in Section \ref{sec:simple} and at the beginning of Section \ref{sec:improved}.)

This implies that, for any arc ,  is always polynomially bounded in . This is so since, given our polynomially-bounded demands, any flow of value  would need to consist mostly of flow-cycles, and such flow-cycles would contribute to duality gap (as they cannot exist in optimal solution), which is always . 

This, in turn, together with -centrality (see Fact \ref{fa:central_vs_max_min}) and Invariant \ref{inv:measure_upperbound}, allows us to conclude that all the resistances 

are within a polynomial in  factor of each other. 

It is known (see, e.g., Theorem 2.3 in \cite{ChristianoKMST11}) that once all the resistances are within polynomial of each other, one can afford very good (and fast) approximation to all the major characteristics of the electrical flows (including good approximation to the flow on each of the edges). In particular, one is able to easily perform (approximate) classification of arcs into sets , for . (Note that we want to classify here only arcs that contribute significant portion of the total energy anyway.)  Also, looking at our analysis of our improvement step in Section \ref{sec:proof_main_interior_point}, one can see that the most fundamental requirement there is that the flows  and  that we compute are indeed electrical flow, i.e., there are voltages that induce them via \eqref{eq:potential_flow_def}. After all, this is what ensures that our first-order updates to the centrality are canceling out. The fact that these flow might not have the exact demands we requested is of lesser importance. The only effect of the latter will be that our improvement steps will end up perturbing the -feasibility of our maintained solution. However, given that we have polynomially bounded resistance ratio and logarithmic dependence on error, we can always make these perturbation very small and just fix them at the end of each -improvement steps via the fixing procedure that we already employ to fix the effects of the preconditioning -- see Section \ref{sec:preconditioning}. 

In the light of the above, we can conclude that indeed, having approximate, instead of exact, electrical flow computations is acceptable for our algorithm, at least as long the dependence of the running time on the error is only logarithmic (which is the case here). 
 \section{Appendix to Section \ref{sec:proof_main_interior_point}}\label{app:interior}


To prove the second part of the theorem, note first that indeed . Next, we can check that the cumulative changes of the vectors  and  are equal to

for each arc  in . 

As a result, by \eqref{eq:hf_resistances}, we have that for each arc ,


Recall that from the discussion above we already know that, for each arc ,   (cf. \eqref{eq:interior_bound_2}),  (cf. Fact \ref{fa:central_vs_max_min}), and  (cf. \eqref{eq:interior_bound_1}). So, as , we have that 

and by performing a simple Taylor expansion approximation we can obtain that 

for each arc , where . 

Clearly, this means, in particular, that , as desired. So, we just need to show that  too. To this end, observe that

as desired, where we used a combination of Fact \ref{fa:central_vs_max_min}, the fact that  is -centered, as well as, equations \eqref{eq:perfect_centering_of_intermediate} and  \eqref{eq:energy_estimate_final}.
This concludes the proof of the theorem. 
\end{document}
