\section{Experiments}

In this section, we compare our {\ourmodelshort} with a wide range of recent competitors on YouTube-VOS, DAVIS 2017 and SegTrack v2 datasets. 
YouTube-VOS has  and  videos for training and validation, respectively. Among the  object categories in the validation set,  are seen in the training set while the other  are unseen. 
DAVIS 2017 has  and  video sequences for training and validation respectively and the average video length is around .
The average number of instances per sequence are  and  for training and validation, respectively.
For the SegTrack v2 dataset, there are  low resolution videos ( frames in total) with  generic foreground objects.
All of our experiments are conducted on NVIDIA Titan XP GPUs.

\begin{table}[t]
\resizebox{\linewidth}{!}{
\begin{tabular}{@{}l|c|cccc|c|c@{}}
\hline
\toprule
& \texttt{OL} &  &  &  &   &  & \texttt{FPS} \\ 
\midrule
\midrule
OSMN~\cite{Yang2018EfficientVO}  & \xmark  & 60.0 & 40.6 & 60.1 & 44.0 & 51.2 & 8.0 \\
SiamMask~\cite{wang2019fast} & \xmark  & 60.2 & 45.1 & 58.2 & 47.7& 52.8 & \textbf{55} \\
RGMP~\cite{wug2018fast} & \xmark  & 59.5&-&45.2&-&53.8& 7  \\
OnAVOS~\cite{voigtlaender2017online} & \cmark & 60.1 & 46.6 & 62.7 & 51.4 & 55.2 & - \\
RVOS~\cite{Ventura_2019_CVPR} & \xmark&63.6&45.5&67.2&51.0&56.8 &24  \\ 
S2S~\cite{Xu2018YouTubeVOSSV} & \xmark  & 66.7&48.2&65.5&50.3&57.7&6  \\
OSVOS~\cite{caelles2017one} & \cmark & 59.8 & 54.2 & 60.5 & 60.7 & 58.8 & - \\
S2S~\cite{Xu2018YouTubeVOSSV} & \cmark  & \textbf{71.0}&\textbf{55.5} &\textbf{70.0}&\textbf{61.2}&\textbf{64.4}& - \\ DMM-Net & \cmark  & 59.2 & 47.6 & 62.6 & 53.9 & 55.8&  -  \\
DMM-Net+ & \xmark  & 58.3 & 41.6 & 60.7 & 46.3 & 51.7 &  12  \\
DMM-Net+ & \cmark  & 60.3 & 50.6 &	63.5 &	57.4  & 58.0&  -  \\
\bottomrule
\end{tabular}
}
\caption{Results on YouTube-VOS (validation set) and frame-per-second (FPS) during inference for methods withour online learning. `' and `' denote the seen and unseen categories. `OL': online learning. `+' means we use ResNet-101 as feature extractor instead of ResNet-50}
\vspace{-5mm}
\label{tab:youtubevos}
\end{table}

\begin{table}[t]
\centering
{\begin{tabular}{@{}c|c|c|c @{}}
\hline
\toprule
Methods &  &  &  \\ \midrule
\midrule
MaskRNN~\cite{hu2017maskrnn} &  45.5 & - & - \\ OSMN~\cite{Yang2018EfficientVO} &  52.5 & 57.1 & 54.8 \\FAVOS~\cite{cheng2018fast} &  54.6 & 61.8 & 58.2  \\VideoMatch~\cite{hu2018videomatch}  & 56.5 & - & - \\MSK~\cite{perazzi2017learning}  & 63.3 & 67.2 & 65.3 \\RGMP~\cite{wug2018fast} & 64.8 & 68.6 & 66.7\\FEELVOS~\cite{feelvos2019} & 65.9  & 72.3  & 69.1 \\DyeNet~\cite{li2018video} & 67.3 & 71.0 & 69.1 \\\ourmodelshort & \textbf{68.1} & \textbf{73.3} & \textbf{70.7}\\\bottomrule
\end{tabular}
}
\caption{Results without online learning on the validation set of DAVIS 2017 dataset. FEELVOS also reports another better performed model which is trained on YouTube-VOS~\cite{xu2018youtube}. `-' means no public results available.}
\vspace{-0.2cm}
\label{table:DAVIS}
\end{table}




\subsection{Implementation Details}~\label{sect:implement}

We first introduce implementation details of our model.

\vspace{-2mm}
\paragraph{Mask Proposal Generation}
In the stage of mask proposal generation, we use Mask R-CNN with ResNeXt-101-FPN as the backbone which is pretrained on COCO dataset~\cite{lin2014microsoft}.
Score threshold of the ROI head is set to .
We resize the input image such that its short-side is no larger than . 
We first train the class-agnostic binary mask proposal network on COCO.
Following the strategy used in~\cite{wang2019fast}, we then finetune the proposal network on the combination of COCO and YouTube-VOS with learning rate , batch size  and number of training iteration .

\paragraph{Differentiable Mask Matching}
For the feature extractor , we use a COCO-pretrained Mask R-CNN with a ResNet-50-FPN backbone. 
We also try a ResNet-101 backbone of which the weights are initialized from the released model of RVOS~\cite{Ventura_2019_CVPR}.
We denote this model as DMM-Net+. 
Note that it is possible to share the backbone between the proposal network and the feature extractor of matching such that the overall model is more compact. 
However, sharing backbone leads to worse results in our experiments which may suggest that generating good proposals and learning good feature for matching require different representations.
After we obtain the proposals for each frame, we perform ROI pooling for each proposal to extract multi-scale feature from the backbone and then average the feature spatially to obtain a single feature vector. Similar to the feature fed to the refinement layer, we obtain the proposals feature from the last layer of conv2-5 block in the backbone. 
Input image of the feature extractor is resized such that the short-side is no larger than  and  for DAVIS 2017 and SegTrack v2, respectively. 
For YouTube-VOS, we resize image to  in order to have a fair comparison with S2S~\cite{Xu2018YouTubeVOSSV} and RVOS~\cite{Ventura_2019_CVPR}.
The score weight  used to compute the matching cost in Eq.~(\ref{eq:cost_matrix}) is set to  for DAVIS 2017 and YouTube-VOS, and  for SegTrack v2.
For the mask matching, we set ,  and learning rate . Ablation study on the hyperparameters of the matching is left in the supplementary material. 
After the matching assignment matrix  is obtained, we also found it helpful to remove the non-confident matching by applying a differentiable masking  .

\begin{table}[t]
\centering
{\begin{tabular}{@{}c|c|c|c@{}}
\hline
\toprule
\multirow{2}{*}{Methods} & Online & \multirow{2}{*}{mIoU} & \multirow{2}{*}{mIoU} \\
 & Learning & & \\ 
\midrule
\midrule
OSVOS~\cite{caelles2017one} & \cmark &61.9 & 65.4 \\
OFL~\cite{tsai2016video} & \cmark & 67.5 & - \\
MSK~\cite{perazzi2017learning} & \cmark & 67.4 &70.3 \\
RGMP~\cite{wug2018fast} & & - &71.1 \\
MaskRNN~\cite{hu2017maskrnn} &  & 72.1 & - \\
LucidTracker~\cite{khoreva2017lucid} & \cmark & - & 77.6 \\
DyeNet~\cite{li2018video} &  &- &78.3 \\
DyeNet~\cite{li2018video} & \cmark & - & \textbf{78.7} \\
\ourmodelshort &  & \textbf{76.8} & 76.7 \\
\bottomrule
\end{tabular}
}
\caption{Results on the SegTrack v2 dataset. mIoU is averaged over all frames whereas mIoU is averaged over all instances.}
\vspace{-0.5cm}
\label{table:segtrack}
\end{table}



\paragraph{Refinement Network}\label{sec:refine}
A light version of refinement network containing only four ConvLSTM layers is used for experiments on YouTube-VOS to reduce the computational cost. 
The weights are randomly initialized and trained with the matching layer in an end-to-end manner. 
Input images are resized to be  for both training and inference. 
The refinement network outputs our final predicted masks. 

A heavier version of refinement network is used for model trained on DAVIS 2017. We follow the U-Net style architecture as RGMP~\cite{wug2018fast} and initialize the weights from their released model. The refinement network is also trained together with the matching layer. 

\paragraph{Fine-tune}\label{para:ft}
Since our {\ourmodelshort} is end-to-end differentiable, we fine-tune it on the training sets of YouTube-VOS and DAVIS 2017.
We use the Adam~\cite{kingma2014adam} optimizer with the learning rate  for the weights initialized from pre-trained model and  for the weights from random initialization. 
We set batch-size to  and train on YouTube-VOS dataset for  epochs in total. 
Data augmentation such as random affine transformation is applied during training. 
On DAVIS 2017, we use the same optimizer with learning rate  and batch size . We fine-tune the model for  epochs in total.


\paragraph{Online Learning}
For online learning, we train both the proposal generator and the DMM-Net with the refinement module on the first frame of the validation set. We use the same learning rate and batch size as  Section~\ref{para:ft} except that the number of epochs is  for online learning. 

\paragraph{Evaluation}
For YouTube-VOS and DAVIS 2017, we follow~\cite{pont2017} and use the region (), boundary () and their average () score as the metrics.
For SegTrack v2, there are two types of mean IoU adopted in the existing literature. 
Specifically, one can compute the IoU averaging over all instances per frame and then average over all frames as in~\cite{hu2017maskrnn}, denoted as mIoU.
One can also compute the IoU per instance, average over the frames where the instance has appeared, and then average over all instances as in~\cite{li2018video}, denoted as mIoU. 
We report both metrics on this dataset.

\subsection{Main Results}



\begin{figure*}[t]
	\centering
	\begin{tabular}{@{\hspace{0mm}}c@{\hspace{0 mm}}c@{\hspace{0 mm}}c@{\hspace{0mm}}c@{\hspace{0mm}}c}
		\includegraphics[width=0.195\linewidth]{imgs/vis/ytb/e8fa21eb13/00000.png}&
		\includegraphics[width=0.195\linewidth]{imgs/vis/ytb/e8fa21eb13/00040.png}&
		\includegraphics[width=0.195\linewidth]{imgs/vis/ytb/e8fa21eb13/00085.png}&
		\includegraphics[width=0.195\linewidth]{imgs/vis/ytb/e8fa21eb13/00130.png}&
		\includegraphics[width=0.195\linewidth]{imgs/vis/ytb/e8fa21eb13/00175.png}\-1.0mm]
\includegraphics[width=0.195\linewidth]{imgs/vis/davis/dogs/DMM-00000.jpg}&
		\includegraphics[width=0.195\linewidth]{imgs/vis/davis/dogs/DMM-00016.jpg}&
		\includegraphics[width=0.195\linewidth]{imgs/vis/davis/dogs/DMM-00033.jpg}&
		\includegraphics[width=0.195\linewidth]{imgs/vis/davis/dogs/DMM-00051.jpg}&
		\includegraphics[width=0.195\linewidth]{imgs/vis/davis/dogs/DMM-00065.jpg}\-1.0mm]
\includegraphics[width=0.195\linewidth]{imgs/vis/segtrack/drift/ours-00000.jpg}&
		\includegraphics[width=0.195\linewidth]{imgs/vis/segtrack/drift/ours-00018.jpg}&
		\includegraphics[width=0.195\linewidth]{imgs/vis/segtrack/drift/ours-00036.jpg}&
		\includegraphics[width=0.195\linewidth]{imgs/vis/segtrack/drift/ours-00054.jpg}&
		\includegraphics[width=0.195\linewidth]{imgs/vis/segtrack/drift/ours-00073.jpg}\0.7mm]
	\end{tabular}
	\caption{Visualization of our results on YouTube-VOS, DAVIS 2017 and SegTrack v2 at different time steps (percentage \wrt the whole video length). The first , the middle  and the last  rows correspond to the YouTube-VOS, DAVIS 2017 and SegTrack v2 datasets respectively.} 
	\vspace{-0.5cm}
	\label{fig:vis}
\end{figure*} 
\paragraph{YouTube-VOS}
We fine-tune both our DMM-Net and proposal net on YouTube-VOS. We first split the official training set into train-train, train-val and train-test splits. Our train-val split consists of 200 videos while the train-train split consists of 3000 videos. Our training is performed on the train-train split, and the best model is selected based on the performance on the train-val split. The final performance on the official validation set is reported in Table~\ref{tab:youtubevos}. 
Compared to the state-of-the-art method S2S, our model achieves competitive segmentation metrics with double speed.
In general, we obtain a good trade off between the performance and running time. 
Moreover, we found that using a stronger backbone, \ie, DMM-Net+, can further boost the performance.

\paragraph{DAVIS 2017}
We compare with a wide range of recent competitors on the validation set of DAVIS 2017. For experiments on DAVIS, we only fine-tune our DMM-Net on the training set of DAVIS 2017 and use the proposal generator pretrained on COCO. 
The models without online learning are listed in Table~\ref{table:DAVIS}. 
From the table, it is clear that without online learning, our method achieves the state-of-the-art performance.
In Fig.~\ref{fig:vis}, we show the qualitative results of our {\ourmodelshort} at different time steps (uniformly sampled percentage \wrt the whole video length) of each video sequence.
From the figure, we can see that our model consistently keeps a very good segmentation quality as time goes on.
We also show a visual comparison at the last frame of the \textit{soapbox} sequence with other strong competitors in Fig.~\ref{fig:vis_comp}.
It is clear that our model does a better job in segmenting the details of the persons and the soapbox.
However, some failure cases still exist. 
For example, in the  column and th row of Fig.~\ref{fig:vis}, the segmentation of the goldfish in the bottom-right corner is unsatisfying.

\paragraph{SegTrack v2}
We test our {\ourmodelshort} model (fine-tuned on DAVIS 2017 training set) directly on the full SegTrack v2 dataset.
We do not perform any fine-tuning on this dataset. Moreover, for simplicity, we again do not adopt any online learning such that we could fully test the generalization ability of our model.
The quantitative results are listed in Table~\ref{table:segtrack}.
From the table, we can see that without any fine-tuning and online learning, our {\ourmodelshort} achieves comparable performance to the state-of-the-art methods.
We show some visual examples in the bottom two rows of Fig.~\ref{fig:vis}.
We can see that our model again has a consistently high segmentation quality across different time steps.


\subsection{Ablation Study}\label{sec:ablation}
In this section, we conduct thorough ablation study to justify the design choice and hyperparameters of our model.
\newcommand{\colwidthA}{0.3cm}
\newcommand{\colwidthB}{0.7cm}
\begin{table}[t]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{c|cc|cc|cc}
\hline \toprule
\multicolumn{1}{c|}{Matching} & \multicolumn{2}{c|}{DMM-Net} & \multicolumn{2}{c|}{Prop. Net} & \multicolumn{2}{c}{Train-val} \.3em]    \cline{2-7} 
 & \texttt{Ft.} & \texttt{Unroll} & \texttt{Arch.} & \texttt{+ytb} &  &  \\ 
\midrule \midrule  
Greedy &      - & - & X101&  & 57.1 & 68.1 \\
Hungarian &      - & - & X101&  & 57.3 & 68.4 \\
Ours & \xmark & - & X101&  & 57.3 & 68.3 \\
Ours& \cmark & 2 & R50 &  & 58.5  & 71.4 \\
Ours& \cmark & 2 & X101&     & 59.0 & 71.7 \\
Ours& \cmark & 3 & X101&  & 58.2  & 71.4 \\
Ours& \cmark & 2 & X101& \cmark & \textbf{60.2}  & \textbf{73.0} \\
\bottomrule
\end{tabular}
}
\caption{Ablation study evaluated on our train-val split of YouTube-VOS. Prop. Net: mask proposal network. `+ytb': using YouTube-VOS train-train split during the training of proposal net or not. `Ft.': fine-tuning, `Arch.': architecture for the proposal net, `R50': ResNet-50, `X101': ResNetXt-101.}
\vspace{-0.2cm}
\label{table:DAVIS_ablation_1}
\end{table}







\paragraph{Greedy vs. Hungarian vs. Our Matching Layer}
We first test the matching layer against the optimal matching using the Hungarian method and the popular greedy approximation during inference.
For a fair comparison, we use the same set of mask proposals, the same feature extractor network pretrained on COCO dataset.
We show the mean of  and  scores on YouTube-VOS dataset in Table~\ref{table:DAVIS_ablation_1}. All the ablation results are obtained by training on our train-train split and evaluating on train-val split. 
From the table, we can see that our matching layer has similar performance compared to the optimal matching and is superior to the greedy one during inference.







\paragraph{End-to-End Fine-tuning}
We now study the effect of fine-tuning the whole model on the train-train split of YouTube-VOS.
As shown in Table~\ref{table:DAVIS_ablation_1}, the performance is improved significantly, which verifies the benefits of the end-to-end training and the differentiability of our matching layer.

\paragraph{Mask Proposal Network}
We also try different backbones for the mask proposal network and fine-tune the network on YouTube-VOS dataset. As shown in Table~\ref{table:DAVIS_ablation_1}, the performance gain is pretty high by fine-tuning the proposal net on large scale video dataset such as YouTube-VOS.


\paragraph{Number of Unrolled Steps} 
At last, we investigate the number of unrolled steps of the refinement network during training.
As shown in Table~\ref{table:DAVIS_ablation_1}, unrolling more than  step seems not helpful and increases the memory cost significantly.
Note that during test we unroll from the beginning to the end of the video sequence. 















