\begin{table*}[h]
\small
\begin{minipage}{\textwidth}
 \begin{minipage}[t]{0.6\textwidth}
  \centering
     \makeatletter\def\@captype{table}\makeatother
       \setlength{\tabcolsep}{0.75 mm}{
        \begin{tabular}{c |ccccc| ccccc|c}
            \toprule
            \multirow{2}{*}{Method}&
            \multicolumn{5}{c|}{Recall}&\multicolumn{5}{c|}{Precision} & \multirow{2}{*}{F1} \\
             &0.3&0.5&0.7&0.9 &avg &0.3&0.5&0.7&0.9&avg&  \\
            \midrule
            MFT~\cite{Xiong2018ECCV}  & 46.18 & 29.76 & 15.54 & 5.77 & 24.31 & 86.34 & 68.79 & 38.30 & 12.19 & 51.41 & 33.01 \\
            SDVC~\cite{Mun2019stream}  & \textbf{93.41} & \textbf{76.40} & 42.40 & 10.10 & {55.58} & 96.71 & 77.73 & \textbf{44.84} & 10.99 & 57.57 & 56.56 \\
\textbf{PDVC\_light} & 88.78 & 71.74 & \textbf{45.70} & \textbf{17.45} & \textbf{55.92}& 96.83 & 78.01 & 41.05 & \textbf{14.69} & 57.65 & \textbf{56.77}\\
            \textbf{PDVC} &  89.47 & 71.91 & {44.63} & {15.67} & 55.42 & \textbf{97.16} & \textbf{78.09} & 42.68 & 14.40 & \textbf{58.07} & {56.71} \\
            \bottomrule
        \end{tabular}}
        \caption{Event localization on the ActivityNet Captions validation set.}
        \label{table:EventLoc}
\end{minipage}
\begin{minipage}[t]{0.08\textwidth}
~
\end{minipage}
\begin{minipage}[t]{0.30\textwidth}
   \centering
        \makeatletter\def\@captype{table}\makeatother
        \setlength{\tabcolsep}{0.8 mm}{
        \begin{tabular}{lcccc}
            \toprule
            \multirow{2}{*}{Method}  & \multicolumn{4}{c}{{Predicted proposals}} \\
            & \multirow{1}{*}{B4} & \multirow{1}{*}{M}  & \multirow{1}{*}{C} & \multirow{1}{*}{SODA\_c} \\
            \midrule
            MT~\cite{zhou2018end} &0.30 & 3.18  & 6.10 & - \\
            ECHR~\cite{wang2020event} & - & 3.82 & - & - \\
\textbf{PDVC\_light} & \textbf{{0.89}}  & {4.56}  & \textbf{{23.07}} & 4.34 \\
            \textbf{PDVC} & 0.80 & \textbf{4.74} & 22.71  & \textbf{4.42} \\
            \bottomrule
        \end{tabular}}
        \caption{Dense captioning on YouCook2.}
        \label{table:SotaYC}
   \end{minipage}
\end{minipage}
\end{table*}



\begin{table*}[h]
\small
\begin{minipage}{\textwidth}
 \begin{minipage}[t]{0.61\textwidth}
  \centering
     \makeatletter\def\@captype{table}\makeatother
     \setlength{\tabcolsep}{0.9 mm}{
\begin{tabular}{l|c|ccc|cccc}
    \toprule
    \multirow{2}{*}{Method} & \multirow{2}{*}{Features} &\multicolumn{3}{c|}{{Ground-truth proposals}} & \multicolumn{4}{c}{{Predicted proposals}} \\
    &  & {~~~B4~~~} & {~~~M~~~} & {~~~C~~~} & {B4} & {~M~}  & {~C~} & SODA\_c \\
    \midrule
    DCE~\cite{krishna2017dense} & C3D & 1.60 & 8.88 & 25.12 & 0.17 & 5.69 & 12.43& - \\
    TDA-CG~\cite{wang2018bidirectional}{} & C3D & - & {9.69} & - & 1.31 & 5.86 & 7.99 & - \\
    DVC~\cite{li2018jointly} & C3D& 1.62 & 10.33 & 25.24 & 0.73 & 6.93 & 12.61 & - \\
    SDVC~\cite{Mun2019stream} &C3D& - & - & - & - & 6.92 & - & -\\
    Efficient~\cite{Suin2020efficient} & C3D & - & - & - & 1.35 & 6.21 & 13.82 & -  \\
    ECHR~\cite{wang2020event} & C3D& 1.96 & \textbf{10.58} & 39.73 & 1.29 & 7.19 & 14.71 & 3.22 \\
    \textbf{PDVC\_light} & C3D& 2.61 & 10.48 & \textbf{47.83} & 1.51 & 7.11 & \textbf{26.21} & 5.17 \\ 
    \textbf{PDVC} & C3D& \textbf{2.64} & 10.54 & 47.26 & \textbf{1.65} & \textbf{7.50} & {25.87} & \textbf{5.26} \\
    \midrule
    MT~\cite{zhou2018end}{} & TSN & 2.71 & 11.16 & 47.71 & 1.15 & 4.98 & 9.25 & - \\
    \textbf{PDVC} & TSN & \textbf{3.07} & \textbf{11.27} & \textbf{52.53} & \textbf{1.78} & \textbf{7.96} & \textbf{28.96} & \textbf{5.44} \\
    \midrule
    MDVC~\cite{Iashin2020MDVC}{} & I3D+VGGish & 1.98 & 11.07 & 42.67 & 1.01 & 6.86 & 7.77 & - \\
    BMT~\cite{Iashin2020better}{} & I3D+VGGish & 1.99 & 10.90 & 41.85 & 1.88 & 7.43 & 11.94& - \\
    \textbf{PDVC}{} & I3D+VGGish & \textbf{3.12} & \textbf{11.26} & \textbf{53.65} & \textbf{1.96} & \textbf{8.08} & \textbf{28.59} & \textbf{5.42} \\
    \bottomrule
\end{tabular}}
\caption{Dense captioning on the ActivityNet Captions validation set. B4/M/C is short for BLEU4/METEOR/CIDEr.  indicates results re-evaluated by the same evaluation toolkit.  means results with part of the dataset (9\% videos missing). }
\label{table:SotaANET}
\end{minipage}
\begin{minipage}[t]{0.04\textwidth}
~
\end{minipage}
\begin{minipage}[t]{0.34\textwidth}
  \centering
        \makeatletter\def\@captype{table}\makeatother
        \setlength{\tabcolsep}{0.7 mm}{
\begin{tabular}{lcccc}
    \toprule
\multirow{2}{*}{Method}  & \multirow{2}{*}{Features} & \multirow{2}{*}{B4} & \multirow{2}{*}{M}  & \multirow{2}{*}{C} \\
    \\
    \midrule
    \multicolumn{5}{l}{\textbf{Ground-truth proposals}} \\
    HSE~\cite{Zhang2018ECCV} & V &9.84 & 13.78 & 18.78  \\
    MART~\cite{lei2020mart} & V+F & 10.33 & 15.68 & 23.42 \\
    VTrans~\cite{zhou2018end} & V+F & 9.75 & 15.64 & 22.16 \\
    Trans-XL~\cite{Dai2019trans} & V+F & 10.39 & 15.09 & 21.67 \\
    GVD~\cite{Zhou2019ground} & V+F+O & 11.04 & 15.71 & 21.95 \\
    GVDsup~\cite{Zhou2019ground}& V+F+O & 11.30 & 16.41 & 22.94 \\ 
    AdvInf~\cite{park2019adver} & V+F+O & 10.04 & \textbf{16.60} & 20.97 \\
    \textbf{PDVC} & V+F & \textbf{11.80} & 15.93  & \textbf{27.27} \\
    \midrule
    \multicolumn{5}{l}{\textbf{Predicted proposals}} \\
    MFT~\cite{Xiong2018ECCV} & V+F &\textbf{10.29} & 14.73  & 19.12 \\
    \textbf{PDVC} & V+F & {10.24} & \textbf{15.80}  & \textbf{20.45} \\
    \bottomrule
\end{tabular}}
\caption{Paragraph captioning on the ActivityNet Captions \textit{ae-val} set~\cite{Zhou2019ground}. V/F/O refers to visual/flow/object features.}
\label{table:SotaParaCap}
\end{minipage}
\end{minipage}
\vspace{-1em}
\end{table*}


\section{Experiments}

\subsection{Experimental Settings}

\paragraph{Datasets.}
We use two large-scale benchmark datasets, ActivityNet Captions~\cite{krishna2017dense}, and YouCook2~\cite{zhou2018towards} to evaluate the effectiveness of the proposed PDVC. ActivityNet Captions contains 20k long untrimmed videos of various human activities. On average, each video lasts 120s and is annotated with 3.65 temporally-localized sentences. We follow the standard split with 10009/4925/5044 videos for training, validation, and test. YouCook2 has 2000 untrimmed videos of cooking procedures with an average duration of 320s. Each video has 7.7 annotated segments with associated sentences. We use the official split with 1333/457/210 videos for training, validation, and test. 

\vspace{0.5em}
\noindent{\textbf{Evaluation metrics.}}
We evaluate our method in three aspects:  1) For localization performance, we use the average precision, average recall across IOU at \{0.3, 0.5, 0.7, 0.9\}  and their harmonic mean, F1 score. 2)  For dense captioning performance, we follow the \href{https://github.com/ranjaykrishna/densevid_eval/tree/deba7d7e83012b218a4df888f6c971e21cfeea33}{official evaluation tool} provided by ActivityNet Challenge 2018, which calculates the average precision (measured by BLEU4~\cite{papineni2002bleu}, METEOR~\cite{lavie2005meteor}, and CIDEr~\cite{vedantam2015cider}) of the matched pairs between generated captions and the ground truth across IOU thresholds of \{0.3, 0.5, 0.7, 0.9\}. However, the official scorer does not consider the storytelling quality, \ie, how well the generated captions can cover the video's whole story. We further adopt \href{https://github.com/fujiso/SODA/tree/22671b3570e088217139bcb1e4de7a3499c30294}{SODA\_c}~\cite{fujitasoda} for an overall evaluation. 3) For paragraph captioning performance, we form a paragraph by sorting generated captions according to their starting time and report the \href{https://github.com/jayleicn/recurrent-transformer/tree/3c31d2444c178ccbc78998d2ae1d4910b02b95ae/densevid_eval}{paragraph-level captioning performance}.
Note that ActivityNet Captions has two sets of annotations for the validation set. For SODA\_c, we evaluate it by two sets independently and report their average score.

\vspace{0.5em}
\noindent{\textbf{Implementation details.}}
For ActivityNet Captions, we use a C3D~\cite{tran2015learning} pre-trained on Sports1M~\cite{karpathy2014large} to extract frame-level features. To fairly compare with state-of-the-art methods, we also test our model based on TSN~\cite{wang2018temporal} features provided by~\cite{zhou2018end}, and I3D+VGGish features provided by~\cite{Iashin2020better}. For YouCook2, we use the same TSN features as in~\cite{zhou2018end}. 

We use a two-layer deformable transformer with multi-scale (4 levels) deformable attention. The deformable transformer uses a hidden size of 512 in MSDAtt layers and 2048 in feed-forward layers. The number of event queries is 10/100 for ActivityNet Captions/YouCook2. We implement a lightweight PDVC (termed PDVC\_light) with the vanilla LSTM captioner and the standard PDVC with the LSTM-DSA captioner. The LSTM hidden dimension in captioning heads is 512. For the event counter, we choose the maximum count as 10/20 for ActivityNet Captions/YouCook2. In Eqn.~\ref{rank}, the length modulation factor  is set to 2, and the trade-off ratio  is set to 0.3/1.0 for PDVC\_light/PDVC. The cost ratios in the bipartite matching are :=2:1 and the loss ratios are :::=2:1:1:1. We use Adam~\cite{kingma2015adam} optimizer with an initial learning rate of 5e-5 and the mini-batch size of 1 video.

\subsection{Comparison with State-of-the-art Methods}

\vspace{0.5em}
\noindent{\textbf{Localization performance.}} The comparison of event localization quality is shown in Table~\ref{table:EventLoc}. SDVC and MFT generate event proposals by a sophisticated ``localize-select-describe" workflow. 
In contrast, PDVC removes the hand-crafted designs in the traditional proposal modules and directly outputs the proposals in a parallel manner, which is more efficient to deal with long sequences than recurrent counterparts. We surpass MFT by a large margin and achieve similar (slightly better) performance to SDVC, which shows the effectiveness of parallel set prediction in our method. Besides, the choice of the captioning head can slightly influence the balance of precision and recall. 

\vspace{0.5em}
\noindent{\textbf{Dense captioning performance.}}
In Table~\ref{table:SotaANET}, we list the performance of state-of-the-art models with cross-entropy training\footnote{{A few methods~\cite{Mun2019stream, wang2020event} incorporates Reinforcement Learning (RL)~\cite{Rennie2017self} after the cross-entropy training to further boost the performance. Note that we do not compare with these methods since RL training requires a more complex captioning network (\eg, Hierarchical RNN~\cite{yu2016video}) and extra-long training time, which is opposite to the design philosophy of PDVC. Moreover, RL training tends to produce longer sentences with repeated phrase~\cite{wang2019describe}, lowering the coherence and readability of generated captions.}}
on ActivityNet Captions. With ground-truth proposals, PDVC achieves considerable improvement over the state-of-the-art on BLEU4 and CIDEr, which shows a deformable transformer plus an LSTM captioner can give good caption quality. With predicted proposals, PDVC with C3D features achieves the best performance on BLEU4/METEOR/CIDEr/SODA\_c, giving a 22.22\%/4.31\%/75.87\%/63.35\% relative improvement over state-of-the-art scores. We find that PDVC with ground-truth proposals does not show much superiority over ECHR on METEOR but surpasses ECHR with predicted proposals, indicating the generated proposals of PDVC are much better.  Even with a lightweight LSTM as a captioner, PDVC\_light can surpass most two-stage approaches on BLEU4/CIDEr/SODA\_c. The reason mainly comes from the parallel decoding of the captioning head and localization head, which helps to generate proposals with high descriptiveness and discriminative internal representation. 


Table~\ref{table:SotaYC} shows the dense captioning performance on the YouCook2 validation set. Our method achieve state-of-the-art performance with a considerable performance gain over other methods on all metrics.

\vspace{0.5em}
\noindent{\textbf{Paragraph captioning performance.}}
Table~\ref{table:SotaParaCap} shows the comparison between PDVC and state-of-the-art paragraph captioning methods. With ground-truth proposals, PDVC with a deformable transformer plus an attention-based LSTM can surpass several transformer-based captioning models, like MART, VTrans, and Trans-XL, indicating the strong representation ability of deformable attention in the encoder-decoder and the LSTM-DSA. It is promising for PDVC to get a further performance boost by incorporating a transformer captioner.  We leave this for future work.

Even with predicted proposals, we observe PDVC has a comparable performance with previous methods with ground-truth proposals, indicating that query features contain rich information covering main segments in videos. While most previous paragraph captioning methods require ground-truth annotation at testing, our model reduces the captioning module's dependence on accurate proposals by parallel decoding, raising the possibility of generating good paragraphs without human annotations of the test video.

\vspace{0.5em}
\noindent{\textbf{Efficiency.}} We compare the inference time of PDVC against two-stage methods~TDA-CG~\cite{wang2018bidirectional}, MT~\cite{zhou2018end} under the same hardware environment in Table~\ref{table:eff}. Our methods are more efficient since: 1) Only a few event proposals with their captions are predicted in parallel; 2) We do not require a dense-to-sparse proposal selection like NMS; 3) MSDAtt is an efficient operator due to the sparse sampling. 


\subsection{Interaction between Localization \& Captioning}
\label{sec: interaction}

In this part, we go deeper into the mutual effect between two subtasks. It is straightforward that localization can aid captioning since the localization supervision guides the query features to specific ground-truth regions, which contains rich semantics matching the target captions. Therefore, we focus on how captioning task affects proposals' quality, which is less explored in the previous literature.

\vspace{0.5em}
\noindent{\textbf{Captioning supervision helps generate proposals with descriptiveness.}} To better study the quality of proposals generated by PDVC,  
we use the same pre-trained event captioning model~\cite{yao2015describing} to evaluate the descriptiveness of generated proposals of different models. 
We also reimplemented two mainstream proposal generation modules SST and SST+ESGN for comparison. 
Both SST and SST+ESGN are trained with localization loss only, while PDVC is trained with both localization and captioning loss. As shown in Table~\ref{table:LossType}, PDVC achieves a slightly lower F1 score but the best descriptiveness score among the four models.


We match each generated proposal with one ground-truth segment with the highest overlap. Fig.~\ref{fig:Interact} demonstrates the statistics of matching results. Surprisingly, incorporating caption supervision yields a considerable boost in caption quality of high-precision proposals (\ie, IOU0.9). The reason may be that the captioning head is trained based on event query features corresponding to accurate proposals, so PDVC learns to enhance the descriptiveness of the high-precision proposals. The last subfigure shows the IOU distribution of matched pairs. Most proposals produced by SST are not very accurate (mainly with 0.5{\rm IOU}0.8). When further incorporating ESGN for adaptively proposal selection, the majority of proposals are with 0.6{\rm IOU}0.9. Ours and Ours\_loc\_only achieve a similar IOU distribution to SST+ESGN, but do not introduce any hand-crafted components like anchor generation and NMS.

\begin{table}[]
\setlength{\tabcolsep}{1.3mm}{
\small
\begin{tabular*}{8.3cm}{l c c c c c}
    \toprule
    Method &  BAF-CG~[24] & MT~[31]  & \textbf{PDVC\_light} & \textbf{PDVC}\\
    \midrule
Time(secs) & 2.39 & 2.05 & \textbf{0.09} & \textbf{0.16}  \\
    \bottomrule
\end{tabular*}}
\caption{Inference speed. We report average inference time (secs/video) of 100 sampled videos with a single Tesla V100 GPU.}
\vspace{-0.5 em}
\label{table:eff}
\end{table}

\begin{table}[]
\fontsize{8.5pt}{1em} \selectfont
    \centering
        \setlength{\tabcolsep}{0.6 mm}{
        \begin{tabular}{l c ccc cccc}
            \toprule
{Method} &{Loss} &{\#p} & {Rec.} &{Pre.} & {F1} & {B4} & {M}  &{C} \\
            \midrule
            {SST~\cite{buch2017sst}} & loc. & 3.00 & 42.00 & 60.99 &49.74 & 0.98 & 6.70 & 17.34  \\
            {SST+ESGN~\cite{Mun2019stream}} &loc.& 2.79 & 53.80 & 61.37 & 57.33 & 1.09 & 6.80 & 19.67 \\
            {Ours\_loc\_only} &loc. & 3.26 & {56.35} & 58.69 & {57.49} & 0.98 & 6.71 & 19.36 \\
            \textbf{Ours (PDVC)} & loc.+cap. & 3.03 & 55.42 & 58.07 & 56.71 & \textbf{1.24} & \textbf{7.03} & \textbf{21.91}\\
            \bottomrule
        \end{tabular}}
        \caption{Proposal quality with different loss types. Rec./Pre./F1 measures the localization performance, while B4/M/C measures dense captioning performance. \#p is the number of proposals.}
        \label{table:LossType}
        \vspace{-0.5em}
\end{table}

\begin{figure}[t]
    \centering
    \includegraphics[width= 0.99 \columnwidth]{figs/Exper_Interaction_loc_cap_v2.pdf}
\caption{Caption quality vs. IOU.  We omit the pairs with IOU (less than 2\% of all pairs).}
\label{fig:Interact} 
\end{figure}

\begin{figure*}
    \centering
    \includegraphics[width=1. \textwidth]{figs/exper_weak2.pdf}
    \caption{The distribution of predicted proposals without localization supervision. We plot the predicted proposals of 200 randomly sampled videos in the YouCook2 validation set. Horizontal and vertical axes represent the re-scaled center position and re-scaled length of proposals, respectively. Each sub-figure contains 30 clusters with different colors corresponds to 30 input event queries. R and P refer to recall and precision of the 30 generated proposals, respectively.}
    \label{fig:weak}
\end{figure*}




\begin{table*}[h]
\small
    \begin{subtable}[h]{0.38\textwidth}
        \centering
        \setlength{\tabcolsep}{0.65 mm}{
        \begin{tabular}{c c | c c c | c c}
                \toprule
                \multicolumn{2}{c|}{Transformer} & \multicolumn{3}{c|}{Captioning head} & \multirow{2}{*}{M} & \multirow{2}{*}{SODA\_c}  \\
                Vanilla & Deformable & LSTM & SA & DSA &  &  \\
                \midrule
                 & & & & & 6.10 & 3.06 \\
                & &  && &  7.11  & 5.17 \\
                &  & &  & & 6.15 & 3.40 \\
                &  & & &  & \textbf{7.50}  & \textbf{5.26}\\
                \bottomrule
            \end{tabular}}
            \caption{Ablating the deformable operations}
            \label{table:abl}
    \end{subtable}
{~}
    \begin{subtable}[h]{0.35\textwidth}
        \centering
        \setlength{\tabcolsep}{0.8 mm}{
        \begin{tabular}{cccccc}
            \toprule
            \#q & counter & Rec. & Pre. & {M}  & SODA\_c\\
            \midrule
5 &  & 57.46 & 57.10 & 6.96 & 5.02 \\
            10 &  &55.92 & 57.65 & 7.11 & 5.17 \\
            30 & & 53.35 & 59.08 & 7.18 & 4.90 \\
            100 & &51.88 &59.27 & 7.33 & 4.59 \\
            10 &  & 77.67 & 44.88 & 6.62 & 4.30 \\
\bottomrule
        \end{tabular}}
\caption{Varying query number \& event counter}
        \label{table:query}
    \end{subtable}
{~}
    \begin{subtable}[h]{0.2\textwidth}
        \centering
        \setlength{\tabcolsep}{0.8 mm}{
        \begin{tabular}{c c c c c}
                \toprule
\multirow{1}{*}{} & \multirow{1}{*}{Rec.} & \multirow{1}{*}{Pre.} & \multirow{1}{*}{M} & \multirow{1}{*}{SODA\_c}\\
\midrule
0.0 & 48.67 & 47.35 & 5.78 & 5.23  \\
                0.5 & 50.63 & 50.08 & 6.59 & 5.25 \\
                1.0 & 51.52 & 53.31 & 7.35 & 5.02 \\ {2.0} & 55.92 & 57.65 & 7.11 & {5.17} \\
                3.0  & 55.72 & 57.87 & 6.90 & 5.19 \\
                \bottomrule
            \end{tabular}}
\caption{Varying }
            \label{table:gamma}
    \end{subtable}
     \caption{Ablation studies on the ActivityNet Captions validation set. Subfigure (b) and (c) are based on PDVC\_light.}
     \vspace{-1em}
     \label{tab:temps}
\end{table*}

Generally speaking, descriptiveness is positively correlated to the precision of proposals with an ideal captioner. However, the performance of existing captioners is still far from satisfactory, which means they generate wrong or boring captions for some proposals. To reduce improper captions of the final results, it is essential to generate not only location-accurate but caption-aware proposals. Our model provides an effective solution to explore the mutual benefits between localization and captioning by parallel decoding.

\vspace{0.5em}
\noindent{\textbf{Captioning supervision helps learn location-aware features.}} Another advantage of parallel decoding is that we can directly remove the localization head to study the behavior of captioning head. We train an event proposal generation module based on merely captioning supervision, by making some modifications to the original PDVC to stabilize training, such as fixing the sampling offsets in the decoder and using the captioning cost in bipartite matching. More details can be found in the supplementary material. After iterative refinement in the decoder, we directly regard the reference points corresponding to event queries in the last decoder layer as event proposals. Fig.~\ref{fig:weak} shows the position distribution of predicted proposals on YouCook2. We also report quantitative results such as recall and precision.

As the training epoch increases, proposals' center tends to spread uniformly, and the proposals' length tends to focus on a relatively small value. Though a noticeable gap exists between the distributions of predicted proposals and ground-truth proposals, we see that the predicted proposals are gradually approaching the ground truth during training. The recall/precision at epoch 13 is 30.32/14.24, which is better than that at initialization (24.14/13.56). Based on the above findings, we argue that our method can implicitly capture the location-aware features from caption supervision, helping the optimization of the event localization.

\subsection{Ablation Studies}

\noindent{\textbf{Deformable components.}} As shown in Table~\ref{table:abl}, when removing deformable operations from deformable transformer or LSTM-DSA, the performance degrades considerably. We conclude that: 1) Adding locality into transformer helps to extract temporally-sensitive features for localization-aware tasks; 2) Focusing on a small segment around the proposals rather than the whole video helps the optimization of the event captioning.

\vspace{0.5em}
\noindent{\textbf{Query number \& event counter.}} As shown in Table~\ref{table:query}, only a few queries are sufficient for good performance. Too many queries lead to high precision and  METEOR, but low Recall and SODA\_c. We choose an appropriate query number for striking a balance of recall and precision. The final event number also controls the balance of precision and recall. The event counter can predict a reasonable number of event instances, making the generated captions reveal a whole story in the video.


\vspace{0.5em}
\noindent{\textbf{Length modulation.}} Table~\ref{table:gamma} shows that modulating the caption length~() obtains a better trade-off between METEOR\ \& SODA\_c and Precision \& Recall than averaging () or summing () the word scores. 

