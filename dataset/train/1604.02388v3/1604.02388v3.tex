

We evaluate our approach on the 4-class \cite{Silberman:ECCV12}, 13-class \cite{couprie2013indoor}, and 40-class \cite{gupta2013perceptual} tasks of the \textit{NYU--Depth--V2} (NYUDv2) dataset \cite{Silberman:ECCV12}, and 33-class task of the SUN3D dataset \cite{SUN3D}.

The NYUDv2 dataset contains 518 RGBD videos, which have more than 400,000 images.
Among them, there are 1449 densely labeled frames, which are split into 795 training images and 654 testing images.
{We follow the experimental settings of \cite{deng2015semantic} to test on 65 labeled frames.}
We compare our models of different settings to previous state-of-the-art multi-view methods as well as single-view methods, which are summarized in Table \ref{table:table_competing}.
We report the results on the labeled frames, using the same evaluation protocol and metrics as \cite{long2015fully},
pixel accuracy (\textit{Pixel Acc.}), mean accuracy (\textit{Mean Acc.}), region intersection over union (\textit{Mean IoU}),
and frequency weighted intersection over union (\textit{f.w. IoU}).


\begin{table}[h!]
\scriptsize
  \begin{center}
    \caption{Configurations of competing methods}
    \label{table:table_competing}
    \begin{tabular}{lcc}
      \toprule
       & RGB  & RGBD \\
      \cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}
      Single-View & \cite{david2015multiscale,alex2015bayesiansegnet}  & \cite{region_end2end2016eccv,chen2014semantic,chen2016deeplab,deng2015semantic,raghudeep2015spCNN,gupta2014learning,long2015fully,Unsupervised_RGBD_segmentation,specificfeature2016eccv,crfasrnn_iccv2015}   \\
      Multi-View  & / & \cite{couprie2013indoor,hermans2014dense,stuckler2015dense,SemanticFusion}  \\
      \bottomrule
    \end{tabular}
  \end{center}
\end{table}

      
\begin{table*}[t!]
\scriptsize
  \begin{center}
    \caption{Performance of the 40-class semantic segmentation task on NYUDv2. We compare our method to various state-of-the-art methods:
            \cite{long2015fully,gupta2014learning,alex2015bayesiansegnet,david2015multiscale} are also based on convolutional networks,
            \cite{chen2014semantic, crfasrnn_iccv2015, chen2016deeplab} are the models based on convolutional networks and CRF,
            and \cite{raghudeep2015spCNN, region_end2end2016eccv, deng2015semantic} are region labeling methods, and thus related to ours.
            We mark the best performance in all methods with \textbf{BOLD} font, and the second best one is written with \underline{UNDERLINE}.}
    \label{table:table_state_of_the_art}
    \begin{tabular}{lccccccccccccccc}
      \toprule
       Methods & \rotatebox{90}{wall}  & \rotatebox{90}{floor} & \rotatebox{90}{cabinet} & \rotatebox{90}{bed}  & \rotatebox{90}{chair} & \rotatebox{90}{sofa} & \rotatebox{90}{table}  & \rotatebox{90}{door} & \rotatebox{90}{window} & \rotatebox{90}{bookshelf}  & \rotatebox{90}{picture} & \rotatebox{90}{counter} & \rotatebox{90}{blinds}  & \rotatebox{90}{desk} & \rotatebox{90}{shelves} \\
      \cmidrule(lr){1-1}\cmidrule(lr){2-16}
      Mutex Constraints \cite{deng2015semantic}      & 65.6  & 79.2  & 51.9  & 66.7  & 41.0  & 55.7  & 36.5  & 20.3  & 33.2  & 32.6  & 44.6  & 53.6  & {49.1}  & 10.8  & \underline{9.1}  \\
      RGBD R-CNN  \cite{gupta2014learning}      & 68.0  & 81.3  & 44.9  & 65.0  & 47.9  & 47.9  & 29.9  & 20.3  & 32.6  & 18.1  & 40.3  & 51.3  & 42.0  & 11.3  & 3.5  \\
      Bayesian SegNet  \cite{alex2015bayesiansegnet}      & -  & -  & -  & -  & -  & -  & -  & -  & -  & -  & -  & -  & -  & -  & -  \\
      Multi-Scale CNN  \cite{david2015multiscale}      & -  & -  & -  & -  & -  & -  & -  & -  & -  & -  & -  & -  & -  & -  & -  \\
      CRF-RNN \cite{crfasrnn_iccv2015}      &  70.3  &  81.5  & 49.6  &  64.6  &  51.4  & 50.6  & 35.9   & 24.6   &  38.1  & 36.0  & 48.8   & 52.6   & 47.6   &  13.2  & 7.6   \\
      DeepLab \cite{chen2014semantic}      & 67.9  & 83.0  & 53.1  & 66.8  & 57.8  & 57.8  & \underline{43.4}  & 19.4  & \underline{45.5}  & 41.5  & 49.3  & \underline{58.3}  & 47.8  & 15.5  & 7.3  \\
      DeepLab-LFOV \cite{chen2016deeplab}      & 70.2  & \underline{85.2}  & \underline{55.3}  & 68.9  & \textbf{60.5}  & \underline{59.8}  & \textbf{44.5}  & 25.4  & \textbf{47.8}  & \textbf{42.6}  & 47.9  & 57.7  & \textbf{52.4}  & \textbf{20.7}  & \underline{9.1}  \\
      BI (1000)  \cite{raghudeep2015spCNN}      &  62.8  &  66.8  &  44.2  &  47.7  &  35.8  &  35.9  &  10.9  & 18.3   &  21.5  &  35.9  & 41.5   & 30.9   & 47.4   & 12.8 & 8.5  \\
      BI (3000) \cite{raghudeep2015spCNN}      &  61.7  &  68.1  &  45.2  &  50.6  &  38.9  &  40.3  &  26.2  & 20.9   &  36.0  &  34.4  & 40.8   & 31.6   & 48.3   & 9.3 & 7.9  \\
      E2S2  \cite{region_end2end2016eccv}  &  56.9  & 67.8   & 50.0  & 59.5  &  43.8  &   44.3  &  31.3  & 24.6   &  37.9  &  32.7  &  46.1 & 45.0  & \underline{51.8}   &  \underline{15.8}  &  \underline{9.1} \\
      FCN \cite{long2015fully}  & 69.9  & 79.4  & 50.3  & 66.0  & 47.5  & 53.2  & 32.8  & 22.1  & 39.0  & 36.1  & 50.5  & 54.2  & 45.8  & 11.9  & 8.6  \\
      \cmidrule(lr){1-1}\cmidrule(lr){2-16}
      Ours (\textit{superpixel}) & 70.9  & 83.4  & 52.6  & 68.5  & 54.1  & 56.0  & 40.4  & 25.5  & 38.4  & 40.9  & 51.5  & 54.8  & 47.3  & 11.3  & 7.5  \\
      Ours (\textit{superpixel+}) & \underline{72.4}  & {84.3}  & {52.0}  & \underline{71.5}  & {54.3}  & {58.8}  & {37.9}  & \underline{28.2}  & {41.9}  & {38.5}  & \underline{52.3}  & {58.2}  & {49.7}  & {14.3}  & 8.1  \\
      Ours (\textit{full model}) & \textbf{72.7}  & \textbf{85.7}  & \textbf{55.4}  & \textbf{73.6}  & \underline{58.5}  & \textbf{60.1}  & {42.7}  & \textbf{30.2}  & {42.1}  & \underline{41.9}  & \textbf{52.9}  & \textbf{59.7}  & 46.7  & {13.5}  & \textbf{9.4}  \\
     \hline
    \end{tabular}
    
    \begin{tabular}{lccccccccccccccc}
      Methods  & \rotatebox{90}{curtain}  & \rotatebox{90}{dresser} & \rotatebox{90}{pillow} & \rotatebox{90}{mirror}  & \rotatebox{90}{floormat} & \rotatebox{90}{clothes} & \rotatebox{90}{ceiling}  & \rotatebox{90}{books} & \rotatebox{90}{fridge} & \rotatebox{90}{tv}  & \rotatebox{90}{paper} & \rotatebox{90}{towel} & \rotatebox{90}{showercurtain}  & \rotatebox{90}{box} & \rotatebox{90}{whiteboard} \\
      \cmidrule(lr){1-1}\cmidrule(lr){2-16}
      Mutex Constraints \cite{deng2015semantic}      & \textbf{47.6}  & 27.6  & \textbf{42.5}  & {30.2}  & \underline{32.7}  & 12.6  & {56.7}  & 8.9  & 21.6  & 19.2  & \textbf{28.0}  & 28.6  & {22.9}  & 1.6  & 1.0  \\
      RGBD R-CNN \cite{gupta2014learning}      & 29.1  & 34.8  & 34.4  & 16.4  & 28.0  & 4.7  & \underline{60.5}  & 6.4  & 14.5  & 31.0  & 14.3  & 16.3  & 4.2  & 2.1  & 14.2  \\
      Bayesian SegNet \cite{alex2015bayesiansegnet}      & -  & -  & -  & -  & -  & -  & -  & -  & -  & -  & -  & -  & -  & -  & -  \\
      Multi-Scale CNN \cite{david2015multiscale}      & -  & -  & -  & -  & -  & -  & -  & -  & -  & -  & -  & -  & -  & -  & -  \\
      CRF-RNN \cite{crfasrnn_iccv2015}      &  34.8  &  33.2  & 34.7  & 20.8   &  24.0  & 18.7  &  \textbf{60.9}  & \underline{29.5}  & 31.2   & 41.1   & 18.2   & 25.6   &  \underline{23.0}  &  7.4  & 13.9   \\
      DeepLab \cite{chen2014semantic}      & 32.9  & 34.3  & 40.2  & 23.7  & 15.0  & \underline{20.2}  & 55.1  & 22.1  & 30.6  & {49.4}  & \underline{21.8}  & 32.1  & 6.4  & 5.8  & 14.8  \\ 
      DeepLab-LFOV \cite{chen2016deeplab}      & 36.0  & {36.9}  & 41.4  & \underline{32.5}  & 16.0  & 17.8  & 58.4  & 20.5  & \textbf{45.1}  & 48.0  & 21.0  & \textbf{41.5}  & 9.4  & \textbf{8.0}  & 14.3  \\ 
      BI (1000) \cite{raghudeep2015spCNN}      &  29.3  & 20.3  & 21.7   & 13.0   &  18.2  &  14.1  &  44.7  & 10.9   &  21.5  &  30.4  &  18.8  &  22.3  &  17.7  & 5.5 & 12.4   \\
      BI (3000) \cite{raghudeep2015spCNN}      &  30.8  & 22.9  & 19.5   & 13.9   &  16.1  &  13.7  &  42.5  & 21.3   &  16.6  &  30.9  &  14.9  &  23.3  &  17.8  & 3.3 & 9.9   \\
      E2S2 \cite{region_end2end2016eccv}     & 38.0 &  34.8  & 31.5  &  31.7  &  25.3  &   14.2 & 39.7 & 26.7   & 27.1   &  35.2  &  17.8  &  21.0 &  19.9  & 7.4   & \textbf{36.9}    \\
      FCN  \cite{long2015fully}  & 32.5  & 31.0  & 37.5  & 22.4  & 13.6  & 18.3  & {59.1}  & 27.3  & 27.0  & 41.9  & 15.9  & 26.1  & 14.1 & {6.5}  & 12.9  \\
      \cmidrule(lr){1-1}\cmidrule(lr){2-16}
      Ours (\textit{superpixel}) & 34.5  & \underline{41.6}  & 37.7  & 20.1  & 15.9  & 20.1  & 56.8  & 28.8  & 23.8  & \underline{51.8} & 19.1  & 26.6  & \textbf{29.3} & {6.8}  & {4.7}  \\
      Ours (\textit{superpixel+}) & \underline{42.9}  & {35.9}  & 40.8  & 27.7  & 31.9  & {19.3}  & 55.6  & {28.2}  & {38.3}  & {46.9}  & 17.6  & {31.2}  & 11.0 & {6.5}  & {28.2}  \\
      Ours (\textit{full model}) & 40.7  & \textbf{44.1}  & \underline{42.0}  & \textbf{34.5}  & \textbf{35.6}  & \textbf{22.2}  & 55.9  & \textbf{29.8}  & \underline{41.7}  & \textbf{52.5}  & {21.1}  & \underline{34.4}  & {15.5} & \underline{7.8}  & \underline{29.2}  \\
      \hline
    \end{tabular}

    \begin{tabular}{lcccccccccccccc}
       Methods & \rotatebox{90}{person}  & \rotatebox{90}{nightstand} & \rotatebox{90}{toilet} & \rotatebox{90}{sink}  & \rotatebox{90}{lamp} & \rotatebox{90}{bathtub} & \rotatebox{90}{bag}  & \rotatebox{90}{other struct} & \rotatebox{90}{other furni} & \rotatebox{90}{other props}  & \rotatebox{90}{Pixel Acc.} & \rotatebox{90}{Mean Acc.} & \rotatebox{90}{Mean IoU} & \rotatebox{90}{f.w. IoU}  \\
      \cmidrule(lr){1-1}\cmidrule(lr){2-11}\cmidrule(lr){12-15}
      Mutex Constraints \cite{deng2015semantic}      & 9.6 & 30.6  & 48.4  & 41.8  & 28.1  & 27.6  & 0  & 9.8  & 7.6  & 24.5  & 63.8  & - & 31.5  & 48.5  \\
      RGBD R-CNN \cite{gupta2014learning}     & 0.2  & 27.2  & 55.1  & 37.5  & 34.8  & {38.2}  & 0.2  & 7.1  & 6.1  & 23.1  & 60.3  & - & 28.6  & 47.0 \\
      Bayesian SegNet \cite{alex2015bayesiansegnet}      & -  & -  & -  & -  & -  & -  & -  & -  & -  & -  & 68.0  & 45.8 & 32.4  & - \\
      Multi-Scale CNN \cite{david2015multiscale}      & -  & -  & -  & -  & -  & -  & -  & -  & -  & -  & 65.6  & 45.1 & 34.1  & 51.4  \\
      CRF-RNN \cite{crfasrnn_iccv2015}      & 57.9 &  31.4  & 57.2  &  45.4  & 36.9  &  39.1  & 4.9  & 14.6   &  9.5  &  29.5  &  66.3  & 48.9  & 35.4   & 51.0   \\
      DeepLab \cite{chen2014semantic}      & 55.3 & 37.7  & 57.9  & \underline{47.7}  & \underline{40.0}  & \underline{44.7}  & 6.6  & 18.0  & \underline{12.9}  & \textbf{33.8}  & 68.7  & 46.9 & 36.8  & 52.5  \\
      DeepLab-LFOV \cite{chen2016deeplab}      & \textbf{67.0} & \underline{41.8}  & \textbf{69.7}  & 46.8  & \textbf{40.1}  & \textbf{45.1}  & 2.1  & \textbf{20.7}  & 12.4  & \underline{33.5}
      & \textbf{70.3}  & 49.6 & \underline{39.4}  & \underline{54.7}  \\
      BI (1000) \cite{raghudeep2015spCNN}        &  45.9  &  15.8 & 56.5   & 32.2  & 24.7   & 17.1  &  0.1  &  12.2  & 6.7 & 21.9   & 57.7   & 37.8  & 27.1   & 41.9   \\
      BI (3000) \cite{raghudeep2015spCNN}        &  44.7  &  15.8 & 53.8   & 32.1  & 22.8   & 19.0  &  0.1  &  12.3  & 5.3 & 23.2   & 58.9   & 39.3  & 27.7   & 43.0   \\
      E2S2 \cite{region_end2end2016eccv}      & 35.0 & 17.6 &  31.8  & 36.3  &  14.8  &  26.0 &  \textbf{9.9}  & 14.5 &   {9.3}  & 20.9   & 58.1   & \underline{52.9}  & 31.0   & 44.2   \\
      FCN  \cite{long2015fully}  & 57.6  & 30.1  & 61.3  & 44.8  & 32.1  & {39.2}  & 4.8  & 15.2  & 7.7  & {30.0}  & 65.4  & 46.1 & 34.0  & 49.5  \\
      \cmidrule(lr){1-1}\cmidrule(lr){2-11}\cmidrule(lr){12-15}
      Ours (\textit{superpixel}) & 66.1  & {37.4}  & 56.1  & 46.3  & 34.5  & 26.7  & 5.8  & 12.7  & 12.3  & 30.6  & {68.5}  & {48.7} & {36.0}  & {52.9} \\
      Ours (\textit{superpixel+}) & \underline{66.7}  & {34.1}  & \underline{62.8}  & \textbf{47.8}  & {35.1}  & 26.4  & \underline{8.8}  & \underline{19.3}  & {10.9}  & {29.2}  & {68.4}  & {52.1} & {38.1}  & {54.0} \\
      Ours (\textit{full model}) & {60.7}  & \textbf{42.2}  & {62.7}  & {47.4}  & {38.6}  & {28.5}  & {7.3}  & {18.8}  & \textbf{15.1}  & {31.4}  & \underline{70.1}  & \textbf{53.8} & \textbf{40.1}  & \textbf{55.7} \\
     \bottomrule
    \end{tabular}
  \end{center}
\end{table*}

\subsection{Results on NYUDv2 40-class task}
Table \ref{table:table_state_of_the_art} evaluates performance of our method on NYUDv2 40-class task
and compares to state-of-the-art methods and related approaches \cite{long2015fully,deng2015semantic,gupta2014learning,alex2015bayesiansegnet,david2015multiscale,crfasrnn_iccv2015,chen2014semantic,chen2016deeplab,raghudeep2015spCNN,region_end2end2016eccv}
\footnote{For \cite{long2015fully,deng2015semantic, gupta2014learning, alex2015bayesiansegnet, david2015multiscale}, we copy the performance from their paper. For \cite{crfasrnn_iccv2015,chen2014semantic,chen2016deeplab,raghudeep2015spCNN,region_end2end2016eccv}, we run the code provided by the authors with RGB+HHA images.
Specifically, for \cite{raghudeep2015spCNN}, we also increase the maximum number of superpixels from 1000 to 3000. The original coarse version and the fine version are abbreviated as BI(1000) and BI(3000).
}.
We include 3 versions of our approach:

\paragraph{Our \textit{superpixel} model} is trained on single frames without additional unlabeled data, and tested using a single target frame. It improves the baseline FCN on all four metrics by at least 2 percentage points (\textit{pp}),
and it achieves in particular better performance than recently proposed methods based on superpixels and CNN\cite{raghudeep2015spCNN,region_end2end2016eccv}.
\paragraph{Our \textit{superpixel+} model} leverages additional unlabeled data in the training while it only uses the target frame for test. It obtains 3.4\textit{pp}, 2.1\textit{pp}, 1.1\textit{pp} improvements over the \textit{superpixel} model on \textit{Mean Acc.}, \textit{Mean IoU} and \textit{f.w. IoU},
leading to more favorable performance than many state-of-the-art methods \cite{deng2015semantic,gupta2014learning,alex2015bayesiansegnet,david2015multiscale,crfasrnn_iccv2015,chen2014semantic,raghudeep2015spCNN,region_end2end2016eccv}.
This highlights the benefits of leveraging unlabeled data.
\paragraph{Our \textit{full model}} leverages additional unlabeled data both in the training and test. It achieves a consistent improvement over the \textit{superpixel+} model and outperforms all competitors in \textit{Mean Acc.}, \textit{Mean IoU} and \textit{f.w. IoU} by $0.9pp, 0.7pp, 1.0pp$ respectively.
Particularly strong improvements are observed on challenging object classes such as dresser(+$7.2pp$), door(+$4.8pp$), bed(+$4.7pp$) and TV(+$3.1pp$).

Figure \ref{fig:baseline} demonstrates that our method is able to produce smooth predictions with accurate boundaries.
We present the most related methods, which either apply CRF \cite{crfasrnn_iccv2015, chen2016deeplab} or incorporate superpixels \cite{raghudeep2015spCNN, region_end2end2016eccv}, in the columns 3 to 6 of this figure.
According to the qualitative comparison to these approaches, we can see the benefit of our method. It captures small objects like chair legs, as well as large areas like floormat and door.
In addition, we also present FCN and the \textit{superpixel} model at the 7-th and 8-th column of Figure~\ref{fig:baseline}. The FCN is boosted by introducing superpixels but not as precise as our \textit{full model} using unlabeled data.

\vspace{-0.3cm}
\paragraph{Average vs. max spatio-temporal data-driven pooling.}
Our data-driven pooling aggregates the local information from multiple observations within a segment and across multiple views.
Average pooling and max pooling are canonical choices used in many deep neural network architectures.
Here we test average pooling and max pooling both in the spatial and temporal pooling layer, and show the results in Table~\ref{table:table_avg_max}.
All the models are trained with multiple frames, and tested on multiple frames.
Average pooling turns out to perform best for spatial and temporal pooling. This result confirms our design choice.

\begin{table}[t!]
\scriptsize
  \begin{center}
    \caption{Comparison of average and max spatio-temporal data-driven pooling.}
    \label{table:table_avg_max}
    \begin{tabular}{cccccc}
      \toprule
      Spatial/Temporal & Pixel Acc.  & Mean Acc. & Mean IoU & f.w. IoU \\
      \cmidrule(lr){1-1}\cmidrule(lr){2-5}
      {\sc Avg} / {\sc Avg} & \textbf{70.1} & \textbf{53.8} & \textbf{40.1} & \textbf{55.7}  \\
      {\sc Avg} / {\sc Max}  & 69.4 & 51.0 & 38.0  & 54.4 \\
      {\sc Max} / {\sc Avg} & 66.4 & 45.4 & 33.8 & 49.6 \\
      {\sc Max} / {\sc Max} & 64.9 & 44.5 & 32.1 & 47.9 \\
      \bottomrule
    \end{tabular}
  \end{center}
\end{table}

\vspace{-0.2cm}
\paragraph{Region vs. pixel correspondences.}
We compare our \textit{full model}, which is built on the region correspondences, to the model with pixel correspondences. It only uses the per-pixel correspondences by optical flow and applies average pooling to fuse the information from multiple view. The visualization results of this baseline are presented in column 9 of Figure~\ref{fig:baseline}. 
Obtaining accurate pixel correspondences is challenging because the optical flow is not perfect and the error can accumulate over time.
Consequently, the model with pixel correspondences only improves slightly over the FCN baseline, as it is also reflected in the numbers in Table~\ref{table:table_baseline}.
Establishing region correspondences with the proposed rejection strategy described in section \ref{subsection:corr} seems indeed to be favorable over pixel correspondences.
Our \textit{full model} shows a significant improvement over the pixel-correspondence baseline and FCN in all 4 measures. 


\vspace{-0.3cm}
\paragraph{Analysis of multi-view prediction.}
In our multi-view model, we subsample frames from a whole video for computational considerations.
There is a trade-off between close-by and distant frames to be made.
If we select frames far away from the target frames, they can provide more diverse views of an object, while matching is more challenging and potentially less accurate than for close-by frames.
Hence, we analyze the influence of the distance of selected frames to target frames, and report the \textit{Mean Acc.} and \textit{Mean IoU} in Figure \ref{fig:statistics_max_k}.
In results, providing wider views is helpful, as the performance is improved with the increase of max distance.
And selecting the data in the future, which is another way to provide wider views, also contributes to the improvements of performance.

\begin{figure}[!t]
\begin{center}
   \includegraphics[trim=0.1cm 0cm 3.5cm 2.5cm, clip=true,width=0.48\linewidth]{fig_meanacc_abs_k.pdf}
   \includegraphics[trim=0.1cm 0cm 3.5cm 2.5cm, clip=true,width=0.48\linewidth]{fig_meaniou_abs_k.pdf}
   \end{center}
   \caption{The performance of multi-view prediction with varying maximum distance.
   Green lines show the results of using future and past views.
   Blue lines show the results of only using past views. }
\label{fig:statistics_max_k}
\end{figure}

\begin{table}[t!]
\scriptsize
  \begin{center}
    \caption{Comparison results with baselines on NYUDv2 40-class task}
    \label{table:table_baseline}
    \begin{tabular}{lcccc}
      \toprule
      Methods & Pixel Acc.  & Mean Acc. & Mean IoU & f.w. IoU \\
      \cmidrule(lr){1-1}\cmidrule(lr){2-5}
      FCN \cite{long2015fully} & 65.4  & 46.1 & 34.0  & 49.5  \\
Pixel Correspondence  & 66.2 & 45.9 & 34.6  & 50.2 \\
      Superpixel Correspondence & \textbf{70.1} & \textbf{53.8} & \textbf{40.1} & \textbf{55.7} \\
      \bottomrule
    \end{tabular}
  \end{center}
\end{table}

\begin{figure*}[!t]
\begin{center}
   \includegraphics[width=0.78\linewidth]{fig_experiment_sun3d.pdf}
   \end{center}
   \scriptsize
   \hspace{2.4cm} Image
   \hspace{1.1cm}GT
   \hspace{0.95cm}CRF-RNN\hspace{0.4cm}DeepLab-LFOV\hspace{0.48cm}BI(3000)\hspace{1.01cm}E2S2\hspace{1.16cm}FCN\hspace{0.77cm}Our full model
   \caption{Qualititive results of the SUN3D dataset.
   For each example, the images are arranged from top to bottom, from left to right as color image, groundtruth, CRF-RNN \cite{crfasrnn_iccv2015}, DeepLab-LFOV \cite{chen2016deeplab}, BI \cite{raghudeep2015spCNN}, E2S2 \cite{region_end2end2016eccv}, FCN \cite{long2015fully} and ours.}
\label{fig:sun3d}
\end{figure*}

\subsection{Results on NYUDv2 4-class and 13-class tasks}
\vspace{-0.1cm}
To show the effectiveness of our multi-view semantic segmentation approach, we compare our method to previous state-of-the-art multi-view semantic segmentation methods \cite{couprie2013indoor,hermans2014dense,stuckler2015dense,SemanticFusion} on the 4-class and 13-class tasks of NYUDv2 as shown in Table \ref{table:table_state_of_the_art_multiview}.
Besides, we also present previous state-of-the-art single-view methods  \cite{david2015multiscale, specificfeature2016eccv,Unsupervised_RGBD_segmentation}.
We observe that our \textit{superpixel+} model already outperforms all the multi-view competitors, and the proposed temporal pooling scheme further boosts \textit{Pixel Acc.} and \textit{Mean Acc.} by more than $1pp$ and then outperforms the state-of-the-art \cite{david2015multiscale}.
In particular, the recent proposed method by McCormac \emph{et al.} \cite{SemanticFusion} is also built on CNN, however, their performance on 13-class task is about $5pp$ worse than ours.

\begin{table}[t!]
\scriptsize
  \begin{center}
    \caption{Performance of the 4-class (left) and 13-class (right) semantic segmentation tasks on NYUDv2. }
    \label{table:table_state_of_the_art_multiview}
    \begin{tabular}{lcccc}
      \toprule
       Methods & {Pixel Acc.} & {Mean Acc.} & {Pixel Acc.} & {Mean Acc.} \\
      \cmidrule(lr){1-1}\cmidrule(lr){2-3}\cmidrule(lr){4-5}
      Couprie \emph{et al.} \cite{couprie2013indoor}  & 64.5  & 63.5  &  52.4  & 36.2 \\
      Hermans \emph{et al.} \cite{hermans2014dense}     & 69.0  & 68.1 & 54.2 & 48.0 \\
      St{\"u}ckler \emph{et al.} \cite{stuckler2015dense}     & 70.6  & 66.8  & - & - \\
      McCormac \emph{et al.} \cite{SemanticFusion}      & -  & -   & 69.9 & 63.6 \\
      \cmidrule(lr){1-1}\cmidrule(lr){2-3}\cmidrule(lr){4-5}
      Wang \emph{et al.} \cite{Unsupervised_RGBD_segmentation}      & -  & 65.3   & - & 42.2 \\
      Wang \emph{et al.} \cite{specificfeature2016eccv}      & -  & 74.7   & - & 52.7 \\
      Eigen \emph{et al.} \cite{david2015multiscale}      & \underline{83.2}  & \underline{82.0}   & \underline{75.4} & 66.9 \\
      \cmidrule(lr){1-1}\cmidrule(lr){2-3}\cmidrule(lr){4-5}
      Ours (\textit{superpixel+}) & {82.7}  & {81.3}   & 74.8 & \underline{67.0} \\
      Ours (\textit{full model}) & \textbf{83.6}  & \textbf{82.5}   & \textbf{75.8} & \textbf{68.4} \\
      \bottomrule
    \end{tabular}
  \end{center}
\end{table}

\vspace{-0.1cm}
\subsection{Results on SUN3D 33-class task}
\vspace{-0.15cm}
Table \ref{table:sun3d} shows the results of our method and baselines on the SUN3D dataset.
We follow the experimental settings of \cite{deng2015semantic} to test all the methods \cite{deng2015semantic,crfasrnn_iccv2015,chen2014semantic,chen2016deeplab,raghudeep2015spCNN,region_end2end2016eccv,long2015fully} on all 65 labeled frames in SUN3D,
which are trained with the NYUDv2 40-class annotations.
After computing the 40-class prediction, we map 7 unseen semantic classes into 33 classes.
Specifically, \textit{floormat} is merged to \textit{floor}, \textit{dresser} is merged to  \textit{other furni} and five other classes are merged to \textit{other  props}.
Among all the methods, we achieve the best \textit{Mean IoU} score that our \textit{superpixel+} and \textit{full model} are $1.2pp$ and $4.7pp$ better than \cite{deng2015semantic} and \cite{chen2016deeplab} .
For \textit{Pixel Acc.}, our method is comparable to the previous state of the art \cite{deng2015semantic}.
In addition, we observe that our \textit{superpixel+} model boosts the baseline FCN by $3.7pp$, $2.3pp$, $3.3pp$, $3.9pp$ on the four metrics, and applying multi-view information further improves $3.0pp$, $0.4pp$, $3.5pp$, $3.7pp$, respectively.
Besides, we achieve much better performance than DeepLab-LFOV, which is comparable to our model on the NYUDv2 40-class task.
This illustrates  the generalization capability of our model, even without finetuning on the new  domain or dataset.

\begin{table}[t!]
\scriptsize
  \begin{center}
    \caption{Performance of the 33-class semantic segmentation task on SUN3D. All 65 images are used as the test set. }
    \label{table:sun3d}
    \begin{tabular}{lcccc}
      \toprule
      Methods & Pixel Acc. & Mean Acc. & Mean IoU & f.w. IoU \\
      \cmidrule(lr){1-1}\cmidrule(lr){2-5}
Mutex Constraints \cite{deng2015semantic}     & \textbf{65.7}  & - & 28.2 & \underline{51.0} \\
      CRF-RNN \cite{crfasrnn_iccv2015}     & 59.8  & -  & 25.5 & 43.3 \\
      DeepLab \cite{chen2014semantic}      & 60.9  & 30.7   & 24.0 & 44.1 \\
      DeepLab-LFOV \cite{chen2016deeplab}    & 62.3  & 35.3   & 28.2 & 46.2 \\
      BI (1000) \cite{raghudeep2015spCNN}   & 53.8 & 31.1 & 20.8 & 37.1 \\
      BI (3000) \cite{raghudeep2015spCNN}   & 53.9 & 31.6 & 21.1 & 37.4 \\
      E2S2 \cite{region_end2end2016eccv}     & 56.7  & \textbf{47.7}  & 27.2  & 43.3  \\
      FCN \cite{long2015fully}     & 58.8  & 38.5 & 26.1 & 43.9 \\
      \cmidrule(lr){1-1}\cmidrule(lr){2-5}
      Ours (\textit{superpixel+}) & {62.5}  & 40.8   & \underline{29.4} & {47.8} \\
      Ours (\textit{full model}) & \underline{65.5}  & \underline{41.2}   & \textbf{32.9} & \textbf{51.5} \\
      \bottomrule
    \end{tabular}
  \end{center}
\end{table}

