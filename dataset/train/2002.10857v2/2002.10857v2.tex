 \documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bbm}
\usepackage{multirow}
\usepackage{bm}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{subcaption}



\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}
\cvprfinalcopy 

\def\cvprPaperID{925} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ifcvprfinal\pagestyle{empty}\fi
\begin{document}


\title{Circle Loss: A Unified Perspective of Pair Similarity Optimization}
\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}
\author{{\fontsize{10.5}{12.6}\selectfont Yifan Sun\thanks{Equal contribution.}, Changmao Cheng\samethanks, Yuhan Zhang\samethanks, Chi Zhang, Liang Zheng, Zhongdao Wang, Yichen Wei\thanks{Corresponding author.}}\\
{\fontsize{10.5}{12.6}\selectfont {MEGVII Technology}
{Beihang University }
{Australian National University }
{Tsinghua University}}\\
{\texttt{\small{\{peter, chengchangmao, zhangchi, weiyichen\}@megvii.com}} \hspace{0.5cm}}\\
}
\maketitle
\thispagestyle{empty}

\begin{abstract}
This paper provides a pair similarity optimization viewpoint on deep feature learning, aiming to maximize the within-class similarity  and minimize the between-class similarity . We find a majority of loss functions, including the triplet loss and the softmax cross-entropy loss, embed  and  into similarity pairs and seek to reduce . Such an optimization manner is inflexible, because the penalty strength on every single similarity score is restricted to be equal. Our intuition is that if a similarity score deviates far from the optimum, it should be emphasized. 
To this end, we simply re-weight each similarity to highlight the less-optimized similarity scores. It results in a Circle loss, which is named due to its circular decision boundary. The Circle loss has a unified formula for two elemental deep feature learning paradigms, \emph {i.e.}, learning with class-level labels and pair-wise labels. Analytically, we show that the Circle loss offers a more flexible optimization approach towards a more definite convergence target, compared with the loss functions optimizing . Experimentally, we demonstrate the superiority of the Circle loss on a variety of deep feature learning tasks. On face recognition, person re-identification, as well as several fine-grained image retrieval datasets, the achieved performance is on par with the state of the art.
 
\end{abstract}



\section{Introduction}\label{sec:intro}
This paper holds a similarity optimization view towards two elemental deep feature learning paradigms, \emph{i.e.}, learning from data with class-level labels and from data with pair-wise labels. The former employs a classification loss function (\emph{e.g.}, softmax cross-entropy loss~\cite{sun2014deep,Liu2016LargeMarginSL,wen2016discriminative}) to optimize the similarity between samples and weight vectors. The latter leverages a metric loss function (\emph{e.g.}, triplet loss~\cite{hoffer2015deep,schroff2015facenet}) to optimize the similarity between samples. 
In our interpretation, there is no intrinsic difference between these two learning approaches. They both seek to minimize between-class similarity , as well as to maximize within-class similarity .

From this viewpoint, we find that many popular loss functions (\emph{e.g.}, triplet loss~\cite{hoffer2015deep,schroff2015facenet}, softmax cross-entropy loss and its variants~\cite{sun2014deep,Liu2016LargeMarginSL,wen2016discriminative,wang2018additive,Wang_2018_CVPR,deng2019arcface}) share a similar optimization pattern. They all embed  and  into similarity pairs and seek to reduce . In , increasing  is equivalent to reducing . We argue that this symmetric optimization manner is prone to the following two problems.

\begin{figure}[t!]
	\centering
	\includegraphics[width=0.95\linewidth]{figures/fig_intro.pdf}
	\caption{Comparison between the popular optimization manner of reducing  and the proposed optimization manner of reducing . (a) Reducing  is prone to inflexible optimization (,  and  all have equal gradients with respect to  and ), as well as ambiguous convergence status (both  and  on the decision boundary are acceptable). (b) With , the Circle loss dynamically adjusts its gradients on  and , and thus benefits from a flexible optimization process. For , it emphasizes on increasing ; for , it emphasizes on reducing . Moreover, it favors a specified point  on the circular decision boundary for convergence, setting up a definite convergence target.}
	\vspace{-4mm}
	\label{fig:intro}
\end{figure}


 \textbf{Lack of flexibility for optimization.}  The penalty strength on  and  is restricted to be equal. Given the specified loss functions, the gradients with respect to  and  are of same amplitudes (as detailed in Section~\ref{sec:revisit}). In some corner cases, \emph{e.g.},  is small and  already approaches 0 (``'' in Fig.~\ref{fig:intro} (a)), it keeps on penalizing  with a large gradient. It is inefficient and irrational. 

 \textbf{Ambiguous convergence status.}
Optimizing  usually leads to a decision boundary of  ( is the margin). This decision boundary allows ambiguity (\emph{e.g.}, ``'' and ``'' in Fig.~\ref{fig:intro} (a)) for convergence. For example,  has  and  has . They both obtain the margin . However, comparing them against each other, we find the gap between  and  is only . Consequently, the ambiguous convergence compromises the separability of the feature space. 


With these insights, we reach an intuition that different similarity scores should have different penalty strength. If a similarity score deviates far from the optimum, it should receive a strong penalty. Otherwise, if a similarity score already approaches the optimum, it should be optimized mildly. To this end, we first generalize  into , where  and  are independent weighting factors, allowing  and  to learn at different paces. 
We then implement  and  as linear functions \wrt  and  respectively, to make the learning pace adaptive to the optimization status: The farther a similarity score deviates from the optimum, the larger the weighting factor will be. Such optimization results in the decision boundary , yielding a circle shape in the  space, so we name the proposed loss function \emph{Circle loss}.

Being simple, Circle loss intrinsically reshapes the characteristics of the deep feature learning from the following three aspects:

\textbf{First, a unified loss function}. From the unified similarity pair optimization perspective, we propose a unified loss function for two elemental learning paradigms, \emph{learning with class-level labels and with pair-wise labels.}

\textbf{Second, flexible optimization}. During training, the gradient back-propagated to  () will be amplified by  (). Those less-optimized similarity scores will have larger weighting factors and consequentially get larger gradients. As shown in Fig.~\ref{fig:intro} (b), the optimization on ,  and  are different to each other.



\textbf{Third, definite convergence status.} On the circular decision boundary, Circle loss favors a specified convergence status (``'' in Fig.~\ref{fig:intro} (b)), as to be demonstrated in Section~\ref{sec:method_character}. Correspondingly, it sets up a definite optimization target and benefits the separability. 





The main contributions of this paper are summarized as follows:
\begin {itemize}
\item We propose Circle loss, a simple loss function for deep feature learning. By re-weighting each similarity score under supervision, Circle loss benefits the deep feature learning with flexible optimization and definite convergence target. 
\item We present Circle loss with compatibility to both class-level labels and pair-wise labels. Circle loss degenerates to triplet loss or softmax cross-entropy loss with slight modifications. \item We conduct extensive experiments on a variety of deep feature learning tasks, \emph{e.g.} face recognition, person re-identification, car image retrieval and so on. On all these tasks, we demonstrate the superiority of Circle loss with performance on par with the state of the art. 
\end {itemize}



































\section{A Unified Perspective}\label{sec:revisit}
Deep feature learning aims to maximize the within-class similarity , as well as to minimize the between-class similarity . Under the cosine similarity metric, for example, we expect  and .

To this end, \textbf{learning with class-level labels} and \textbf{learning with pair-wise labels} are two elemental  paradigms. They are conventionally considered separately and significantly differ from each other \emph{w.r.t} to the loss functions. Given class-level labels, the first one basically learns to classify each training sample to its target class with a classification loss, \textit{e.g.} L2-Softmax~\cite{ranjan2017l2}, Large-margin Softmax~\cite{liu2017sphereface}, Angular Softmax~\cite{Liu2016LargeMarginSL}, NormFace~\cite{wang2017normface}, AM-Softmax~\cite{wang2018additive}, CosFace~\cite{Wang_2018_CVPR}, ArcFace~\cite{deng2019arcface}. These methods are also known as proxy-based learning, as they optimize the similarity between samples and a set of proxies representing each class. 
In contrast, given pair-wise labels, the second one directly learns pair-wise similarity (\emph{i.e.}, the similarity between samples) in the feature space and thus requires no proxies, \emph{e.g.}, constrastive loss~\cite{hadsell2006dimensionality,chopra2005learning}, triplet loss~\cite{hoffer2015deep, schroff2015facenet}, Lifted-Structure loss~\cite{oh2016deep}, N-pair loss~\cite{Sohn2016ImprovedDM}, Histogram loss~\cite{Ustinova2016LearningDE}, Angular loss~\cite{Wang2017DeepML}, Margin based loss~\cite{wu2017sampling},  Multi-Similarity loss~\cite{wang2019multi} and so on. 

This paper views both learning approaches from a unified perspective, with no preference for either proxy-based or pair-wise similarity. 
Given a single sample  in the feature space, let us assume that there are  within-class similarity scores and  between-class similarity scores associated with . We denote these similarity scores as  and , respectively. 

To minimize each  as well as to maximize , , we propose a unified loss function by:
 
in which  is a scale factor and  is a margin for better similarity separation. 

Eq.~\ref{eq:proto} is intuitive. It iterates through every similarity pair to reduce . 
We note that it degenerates to triplet loss or classification loss, through slight modifications.




\begin{figure*}[t!]
	\centering
	\includegraphics[width=1\linewidth]{figures/gradients.pdf}
	\caption{The gradients of the loss functions. (a) Triplet loss. (b) AM-Softmax loss. (c) The proposed Circle loss. Both triplet loss and AM-Softmax loss present the lack of flexibility for optimization. The gradients with respect to  (left) and  (right) are restricted to equal and undergo a sudden decrease upon convergence (the similarity pair B). For example, at , the within-class similarity score  already approaches , and still incurs a large gradient. Moreover, the decision boundaries are parallel to , which allows ambiguous convergence. In contrast, the proposed Circle loss assigns different gradients to the similarity scores, depending on their distances to the optimum. For  (both  and  are large), Circle loss lays emphasis on optimizing . For B, since  significantly decreases, Circle loss reduces its gradient and thus enforces a moderated penalty. Circle loss has a circular decision boundary, and promotes accurate convergence status.}
	\vspace{-4mm}
	\label{fig:gradient}
\end{figure*}

\textbf{Given class-level labels}, we calculate the similarity scores between  and weight vectors  ( is the number of training classes) in the classification layer. Specifically, we get  between-class similarity scores by:  ( is the -th non-target weight vector). Additionally, we get a single within-class similarity score (with the superscript omitted) . 
With these prerequisite, Eq.~\ref{eq:proto} degenerates to AM-Softmax~\cite{wang2018additive,Wang_2018_CVPR}, an important variant of Softmax loss (\emph{i.e.}, softmax cross-entropy loss):

 

Moreover, with , Eq.~\ref{eq:degenerate_softmax} further degenerates to Normface~\cite{wang2017normface}.
By replacing the cosine similarity with the inner product and setting , it finally degenerates to Softmax loss.

\textbf{Given pair-wise labels}, we calculate the similarity scores between  and the other features in the mini-batch. Specifically,
 ( is the -th sample in the negative sample set ) and  ( is the -th sample in the positive sample set ). Correspondingly, . Eq.~\ref{eq:proto} degenerates to triplet loss with hard mining~\cite{schroff2015facenet, hermans2017defense}:

 

Specifically, we note that in Eq.~\ref{eq:degenerate_triplet}, the ``'' operation is utilized by Lifted-Structure loss~\cite{oh2016deep}, N-pair loss~\cite{Sohn2016ImprovedDM}, Multi-Similarity loss~\cite{wang2019multi} and \emph{etc.}, to conduct ``soft'' hard mining among samples. Enlarging  gradually reinforces the mining intensity and when , it results in the canonical hard mining in~\cite{schroff2015facenet, hermans2017defense}.

\textbf{Gradient analysis.} 
Eq.~\ref{eq:degenerate_softmax} and Eq.~\ref{eq:degenerate_triplet} show triplet loss, Softmax loss and its several variants can be interpreted as specific cases of Eq.~\ref{eq:proto}. In another word, they all optimize . Under the toy scenario where there are only a single  and , we visualize the gradients of triplet loss and AM-Softmax loss in Fig.~\ref{fig:gradient} (a) and (b), from which we draw the following observations:
\begin{itemize}
\item First, before the loss reaches its decision boundary (upon which the gradients vanish), the gradients with respect to both  and  are the same to each other. The status  has , indicating good within-class compactness. However,  still receives a large gradient with respect to .
It leads to a lack of flexibility during optimization. 

\item Second, the gradients stay (roughly) constant before convergence and undergo a sudden decrease upon convergence. The status  lies closer to the decision boundary and is better optimized, compared with . However, the loss functions (both triplet loss and AM-Softmax loss) enforce an approximately equal penalty on  and . It is another evidence of inflexibility. 

\item Third, the decision boundaries (the white dashed lines) are parallel to . Any two points (\emph{e.g.},  and  in Fig.~\ref{fig:intro}) on this boundary have an equal similarity gap of , and are thus of equal difficulties to achieve. In another word, loss functions minimizing  lay no preference on  or  for convergence, and are prone to ambiguous convergence. Experimental evidence of this problem is to be accessed in Section~\ref{sec:exp_mechanism}.
\end{itemize}

These problems originate from the optimization manner of minimizing , in which reducing  is equivalent to increasing . In the following Section~\ref{sec:circle_loss}, we will transfer such an optimization manner into a more general one to facilitate higher flexibility. 



\section{A New Loss Function} \label{sec:circle_loss}
\subsection{Self-paced Weighting}
We consider to enhance the optimization flexibility by allowing each similarity score to learn at its own pace, depending on its current optimization status. We first neglect the margin item  in Eq.~\ref{eq:proto} and transfer the unified loss function into the proposed Circle loss by:
 
in which  and  are non-negative weighting factors.

Eq.~\ref{eq:circle} is derived from Eq.~\ref{eq:proto} by generalizing  into .
During training, the gradient with respect to  is to be multiplied with  () when back-propagated to  (). 
When a similarity score deviates far from its optimum (\emph{i.e.},  for  and  for ), it should get a large weighting factor so as to get effective update with large gradient.
To this end, we define  and  in a self-paced manner:


in which  is the ``cut-off at zero'' operation to ensure  and  are non-negative.

\textbf{Discussions.} Re-scaling the cosine similarity under supervision is a common practice in modern classification losses~\cite{ranjan2017l2,wang2017normface,wang2018additive,Wang_2018_CVPR,Zhang2018HeatedUpSE,Zhang2019AdaCosAS}. Conventionally, all the similarity scores share an equal scale factor . The equal re-scaling is natural when we consider the softmax value in a classification loss function as the probability of a sample belonging to a certain class. In contrast, Circle loss multiplies each similarity score with an independent weighting factor before re-scaling. It thus gets rid of the constraint of equal re-scaling and allows more flexible optimization. Besides the benefits of better optimization, another significance of such a re-weighting (or re-scaling) strategy is involved with the underlying interpretation. Circle loss abandons the interpretation of classifying a sample to its target class with a large probability. Instead, it holds a similarity pair optimization perspective, which is compatible with two learning paradigms.

\subsection{Within-class and Between-class Margins}\label{sec:method_margin}

In loss functions optimizing , adding a margin  reinforces the optimization~\cite{liu2017sphereface,Liu2016LargeMarginSL,wang2018additive,Wang_2018_CVPR}. Since  and  are in symmetric positions, a positive margin on  is equivalent to  a negative margin on . It thus only requires a single margin . In Circle loss,  and  are in asymmetric positions. Naturally, it requires respective margins for  and , which is formulated by:

\begin{scriptsize}
 
\end{scriptsize}
in which  and  are the between-class and within-class margins, respectively. 

Basically, Circle loss in Eq.~\ref{eq:margin_circle} expects  and . We further analyze the settings of  and  by deriving the decision boundary. For simplicity, we consider the case of binary classification, in which the decision boundary is achieved at . 
Combined with Eq.~\ref{eq:scale}, the decision boundary is given by:

in which .

Eq.~\ref{eq:boundary} shows that the decision boundary is the arc of a circle, as shown in Fig.~\ref{fig:intro} (b). The center of the circle is at , and its radius equals . 



There are five hyper-parameters for Circle loss, \emph{i.e.}, ,  in Eq.~\ref{eq:scale} and , ,  in Eq.~\ref{eq:margin_circle}. We reduce the hyper-parameters by setting , , , and . 
Consequently, the decision boundary in Eq.~\ref{eq:boundary} is reduced to:



With the decision boundary defined in Eq.~\ref{eq:simple_boundary}, we have another intuitive interpretation of Circle loss. It aims to optimize  and . The parameter  controls the radius of the decision boundary and can be viewed as a relaxation factor. In another word, Circle loss expects  and .

Hence there are only two hyper-parameters, \emph{i.e.}, the scale factor  and the relaxation margin .
We will experimentally analyze the impacts of  and  in Section~\ref{sec:exp_param}.




\subsection{The Advantages of Circle Loss}\label{sec:method_character}
{The gradients} of Circle loss with respect to  and  are derived as follows:
\begin{small}

\end{small}
and
\begin{small}

\end{small}
in both of which



Under the toy scenario of binary classification (or only a single  and ), we visualize the gradients under different settings of  in Fig.~\ref{fig:gradient} (c), from which we draw the following three observations:

\emph{Balanced optimization on  and .} We recall that the loss functions minimizing  always have equal gradients on  and  and is inflexible. In contrast, Circle loss presents dynamic penalty strength. Among a specified similarity pair , if  is better optimized in comparison to  (\emph{e.g.},  in Fig.~\ref{fig:gradient} (c)), Circle loss assigns a larger gradient to  (and vice versa), so as to decrease  with higher superiority. The experimental evidence of balanced optimization is to be accessed in Section~\ref{sec:exp_mechanism}.

\emph{Gradually-attenuated gradients.} At the start of training, the similarity scores deviate far from the optimum and gain large gradients (\emph{e.g.}, ``'' in Fig.~\ref{fig:gradient} (c)). As the training gradually approaches the convergence, the gradients on the similarity scores correspondingly decays (\emph{e.g.}, ``'' in Fig.~\ref{fig:gradient} (c)), elaborating mild optimization. Experimental result in Section~\ref{sec:exp_param} shows that the learning effect is robust to various settings of  (in Eq.~\ref{eq:margin_circle}), which we attribute to the automatically-attenuated gradients. 

\emph{A (more) definite convergence target.}
Circle loss has a circular decision boundary and favors  rather than  (Fig.~\ref{fig:intro}) for convergence. It is because  has the smallest gap between  and , compared with all the other points on the decision boundary. In another word,  has a larger gap between  and  and is inherently more difficult to maintain. In contrast, losses that minimize  have a homogeneous decision boundary, that is, every point on the decision boundary is of the same difficulty to reach. 
Experimentally, we observe that Circle loss leads to a more concentrated similarity distribution after convergence, as to be detailed in Section \ref{sec:exp_mechanism} and Fig.~\ref{fig:scatter}.




\section{Experiments}

We comprehensively evaluate the effectiveness of Circle loss under two elemental learning approaches, \emph{i.e.}, learning with class-level labels and learning with pair-wise labels. For the former approach, we evaluate our method on face recognition (Section~\ref{sec:exp_face}) and person re-identification (Section~\ref{sec:exp_reid}) tasks. For the latter approach, we use the fine-grained image retrieval datasets (Section~\ref{sec:exp_finegrain}), which are relatively small and encourage learning with pair-wise labels. We show that Circle loss is competent under both settings. 
Section~\ref{sec:exp_param} analyzes the impact of the two hyper-parameters, \emph{i.e.}, the scale factor  in Eq.~\ref{eq:margin_circle} and the relaxation factor  in Eq.~\ref{eq:simple_boundary}. We show that Circle loss is robust under reasonable settings. Finally, Section~\ref{sec:exp_mechanism} experimentally confirms the characteristics of Circle loss.

\subsection{Settings}

\textbf{Face recognition.}\quad We use the popular dataset MS-Celeb-1M~\cite{guo2016ms} for training. The native MS-Celeb-1M data is noisy and has a long-tailed data distribution. We clean the dirty samples and exclude few tail identities ( images per identity). It results in  images and  identities. For evaluation, we adopt MegaFace Challenge 1 (MF1)~\cite{kemelmacher2016megaface}, IJB-C~\cite{maze2018iarpa}, LFW~\cite{LFWTech}, YTF~\cite{wolf2011face} and CFP-FP~\cite{cfp-paper} datasets and the official evaluation protocols are used. We also polish the probe set and 1M distractors on MF1 for more reliable evaluation, following~\cite{deng2019arcface}. For data pre-processing, we resize the aligned face images to  and linearly normalize the pixel values of RGB images to ~\cite{wen2016discriminative,liu2017sphereface,Wang_2018_CVPR}. We only augment the training samples by random horizontal flip. We choose the popular residual networks~\cite{he2016deep} as our backbones.
All the models are trained with 182k iterations. The learning rate is started with 0.1 and reduced by 10 at 50\%, 70\% and 90\% of total iterations respectively. The default hyper-parameters of our method are  and  if not specified. For all the model inference, we extract the 512-D feature embeddings and use cosine distance as the metric.

\textbf{Person re-identification.}\quad 
Person re-identification (re-ID) aims to spot the appearance of the same person in different observations. We evaluate our method on two popular datasets, \emph{i.e.}, Market-1501~\cite{Zheng_2015_ICCVmarket} and MSMT17~\cite{Wei_2018_CVPRMSMT17}. Market-1501 contains 1,501 identities, 12,936 training images and 19,732 gallery images captured with 6 cameras. MSMT17 contains 4,101 identities, 126,411 images captured with 15 cameras and presents a long-tailed sample distribution. We adopt two network structures, \emph{i.e.} a global feature learning model backboned on ResNet50 and a part-feature model named MGN~\cite{Wang_2018MGN}. We use MGN with consideration of its competitive performance and relatively concise structure. The original MGN uses a Sofmax loss on each part feature branch for training. Our implementation concatenates all the part features into a single feature vector for simplicity. For Circle loss, we set  and .




\textbf{Fine-grained image retrieval.}\quad We use three datasets for evaluation on fine-grained image retrieval, \textit{i.e.} CUB-200-2011~\cite{WahCUB_200_2011}, Cars196~\cite{krause20133d} and Stanford Online Products~\cite{oh2016deep}. 
CARS-196 contains  images which belong to  class of cars. The first  classes are used for training and the last  classes are used for testing. CUB-200-2010 has  different class of birds. We use the first  class with  images for training and the last  class with  images for testing. SOP is a large dataset that consists of  images belonging to  classes of online products. The training set contains  class includes  images and the rest  class includes  images are for testing.
The experimental setup follows~\cite{oh2016deep}. We use BN-Inception~\cite{ioffe2015batch} as the backbone to learn 512-D embeddings. We adopt P-K sampling trategy~\cite{hermans2017defense} to construct mini-batch with  and . 
For Circle loss, we set  and .

\begin{table}[t]
    \small
    \centering
    \caption{Face identification and verification results on MFC1 dataset. ``Rank 1'' denotes rank-1 identification accuracy. ``Veri.'' denotes verification TAR (True Accepted Rate) at 1e-6 FAR (False Accepted Rate) with  distractors. ``R34'' and ``R100'' denote using ResNet34 and ResNet100 backbones, respectively.}
    \label{tab:mf1}
    \begin{tabularx}{\linewidth}{Xcccc}
    \toprule
     \multirow{2}{*}{Loss function} & \multicolumn{2}{c}{Rank 1 (\%)} &
     \multicolumn{2}{c}{Veri. (\%)}\\
    \cmidrule(l{2pt}r{2pt}){2-3} \cmidrule(l{2pt}r{2pt}){4-5}
     & R34 & R100 &R34 & R100  \\
     \midrule
     
    Softmax &92.36 &95.04& 92.72 &95.16 \\
    NormFace~\cite{wang2017normface} &92.62  &95.27 &92.91 &95.37 \\
    AM-Softmax~\cite{wang2018additive,Wang_2018_CVPR} &97.54 &98.31 & 97.64 &98.55 \\
    ArcFace~\cite{deng2019arcface} &97.68 & 98.36 &97.70 &98.58 \\
    CircleLoss (ours) &\textbf{97.81} &\textbf{98.50}&\textbf{98.12} &\textbf{98.73} \\
    \bottomrule
    \end{tabularx}
\end{table}
\begin{table}[t]
    \small
    \centering
    \caption{Face verification accuracy (\%) on LFW, YTF and CFP-FP with ResNet34 backbone.}
    \label{tab:face-verif}
    \begin{tabularx}{\linewidth}{lccc}
    \toprule
     Loss function & LFW~\cite{LFWTech}& YTF~\cite{wolf2011face} & CFP-FP~\cite{cfp-paper} \\
     \midrule
    Softmax &99.18 & 96.19& 95.01\\
    NormFace~\cite{wang2017normface} & 99.25 & 96.03 & 95.34\\
    AM-Softmax~\cite{wang2018additive,Wang_2018_CVPR} & 99.63 & 96.31 & 95.78 \\
    ArcFace~\cite{deng2019arcface} & 99.68 & 96.34 & 95.84\\
    CircleLoss(ours) & \textbf{99.73} & \textbf{96.38} & \textbf{96.02} \\
    \bottomrule
    \end{tabularx}
\end{table}

\subsection{Face Recognition}\label{sec:exp_face}






For face recognition task, we compare Circle loss against several popular classification loss functions, \emph{i.e.}, vanilla Softmax, NormFace~\cite{wang2017normface}, AM-Softmax~\cite{wang2018additive} (or CosFace~\cite{Wang_2018_CVPR}), ArcFace~\cite{deng2019arcface}. Following the original papers~\cite{wang2018additive, deng2019arcface}, we set  for AM-Softmax and  for ArcFace. 

\begin{table}[t]
    \small
    \centering
    \caption{Comparison of TARs on the IJB-C 1:1 verification task.}
    \label{tab:ijb-c}
    \begin{tabularx}{\linewidth}{Xccc}
    \toprule
     \multirow{2}{*}{Loss function} & \multicolumn{3}{c}{TAR@FAR (\%)} \\
     \cmidrule{2-4}
     & 1e-3 & 1e-4 & 1e-5 \\
     \midrule
    ResNet34, AM-Softmax~\cite{wang2018additive,Wang_2018_CVPR} & 95.87 & 92.14 & 81.86\\
    ResNet34, ArcFace~\cite{deng2019arcface} 	&95.94 & 92.28 & 84.23\\
    ResNet34, CircleLoss(ours) & \textbf{96.04} & \textbf{93.44} & \textbf{86.78} \\
    \hline
    ResNet100, AM-Softmax~\cite{wang2018additive,Wang_2018_CVPR} & 95.93 & 93.19 & 88.87\\
    ResNet100, ArcFace~\cite{deng2019arcface} 	&96.01 & 93.25 & 89.10\\
    ResNet100, CircleLoss(ours) & \textbf{96.29} & \textbf{93.95} & \textbf{89.60} \\
    \bottomrule
    \end{tabularx}
\end{table}



We report the identification and verification results on MegaFace Challenge 1 dataset (MFC1) in Table~\ref{tab:mf1}. Circle loss marginally outperforms the counterparts under different backbones. For example, with ResNet34 as the backbone, Circle loss surpasses the most competitive one (ArcFace) by +0.13\% at rank-1 accuracy. With ResNet100 as the backbone, while ArcFace achieves a high rank-1 accuracy of 98.36\%, Circle loss still outperforms it by +0.14\%. The same observations also hold for the verification metric.

Table~\ref{tab:face-verif} summarizes face verification results on LFW~\cite{LFWTech}, YTF~\cite{wolf2011face} and CFP-FP~\cite{cfp-paper}.
We note that performance on these datasets is already near saturation. Specifically, ArcFace is higher than AM-Softmax by +0.05\%, +0.03\%, +0.07\% on three datasets, respectively. Circle loss remains the best one, surpassing ArcFace by +0.05\%, +0.06\% and +0.18\%, respectively.

We further compare Circle loss with AM-Softmax and ArcFace on IJB-C 1:1 verification task in Table~\ref{tab:ijb-c}. Under both ResNet34 and ResNet100 backbones, Circle loss presents considerable superiority. For example, with ResNet34, Circle loss significantly surpasses ArcFace by +1.16\% and +2.55\% on ``TAR@FAR=1e-4'' and ``TAR@FAR=1e-5'', respectively.



\begin{table}[t]
    \small
    \centering
    \caption{Evaluation of Circle loss on re-ID task. We report R-1 accuracy (\%) and mAP (\%). }
    \label{tab:person-reid}
    \begin{tabular}{lcccc}
    \toprule
     \multirow{2}{*}{Method} & \multicolumn{2}{c}{Market-1501} & \multicolumn{2}{c}{MSMT17}\\
     \cmidrule{2-5}
     & R-1 & mAP & R-1& mAP \\
\midrule
PCB~\cite{Sun_2018_ECCVPCB} (Softmax)&93.8&81.6&68.2&40.4\\
     MGN~\cite{Wang_2018MGN} (Softmax+Triplet) &95.7&86.9&-&-\\
     JDGL~\cite{Zheng_2019_CVPRJDGL} &94.8&86.0&\textbf{77.2}&\textbf{52.3}\\

    ResNet50 + AM-Softmax &92.4&83.8&75.6&49.3\\
    ResNet50 + CircleLoss(ours) &94.2& 84.9 &76.3&50.2\\
    MGN + AM-Softmax &95.3&86.6&76.5&51.8\\
    MGN + CircleLoss(ours) &\textbf{96.1}&\textbf{87.4}&{76.9}&{52.1}\\
    \bottomrule
    \end{tabular}
\end{table}





\begin{table*}[t]
    \small
    \centering
\caption{Comparison of R@K(\%) on three fine-grained image retrieval datasets. Superscript denotes embedding size.}
    \label{tab:cub-cars}
    \begin{tabularx}{\textwidth}{Xcccccccccccccc}
    \toprule
     \multirow{2}{*}{Loss function} & \multicolumn{4}{c}{CUB-200-2011~\cite{WahCUB_200_2011}} && \multicolumn{4}{c}{Cars196~\cite{krause20133d}} && \multicolumn{4}{c}{Stanford Online Products~\cite{oh2016deep}}\\
     \cmidrule{2-5} \cmidrule{7-10} \cmidrule{12-15}
  & R@1 & R@2 & R@4 & R@8 &  & R@1 & R@2 & R@4 & R@8 &  & R@1 &R@10& R@ & R@\\
     \midrule
     
    LiftedStruct~\cite{oh2016deep}      &43.6 &56.6&68.6&79.6  &&53.0&65.7&76.0&84.3  &&62.5&80.8&91.9&97.4 \\
HDC~\cite{Song_2017_CVPRHDC}              &53.6 &65.7&77.0&85.6  &&73.7&83.2&89.5&93.8  &&69.5&84.4&92.8&97.7\\
HTL~\cite{Ge_2018_ECCVHTL}               &57.1 &68.8&78.7&86.5  &&81.4&88.0&92.7&95.7  &&74.8&88.3&94.8&98.4\\
    ABIER~\cite{ABIER}   &57.5 &71.5&79.8&87.4  &&82.0&89.0&93.2&96.1  &&74.2&86.9&94.0&97.8\\
    ABE~\cite{Kim_2018_ECCVABE}    &60.6 &71.5&79.8&87.4  &&\textbf{85.2}&\textbf{90.5}&94.0&96.1  &&76.3&88.4&94.8&98.2\\ 
Multi-Simi~\cite{wang2019multi} & 65.7 & 77.0 & \textbf{86.3} & 91.2 &&   {84.1} & {90.4} & 94.0 & 96.5 && 78.2 & 90.5 & 96.0 & \textbf{98.7}\\
    CircleLoss                 & \textbf{66.7} & \textbf{77.4} & 86.2 & \textbf{91.2} &&   83.4 & 89.8 & \textbf{94.1} & \textbf{96.5} && \textbf{78.3} & \textbf{90.5} & \textbf{96.1} & 98.6 \\
    
    
    \bottomrule
    \end{tabularx}
\end{table*}


\subsection{Person Re-identification}\label{sec:exp_reid}

We evaluate Circle loss on re-ID task in Table~\ref{tab:person-reid}. MGN~\cite{Wang_2018MGN} is one of the state-of-the-art methods and is featured for learning multi-granularity part-level features. Originally, it uses both Softmax loss and triplet loss to facilitate joint optimization. Our implementation of ``MGN (ResNet50) + AM-Softmax'' and ``MGN (ResNet50)+ Circle loss'' only use a single loss function for simplicity. 

We make three observations from Table~\ref{tab:person-reid}. First, we find that Circle loss can achieve competitive re-ID accuracy against state of the art. We note that ``JDGL'' is slightly higher than ``MGN + Circle loss'' on MSMT17~\cite{Wei_2018_CVPRMSMT17}. JDGL~\cite{Zheng_2019_CVPRJDGL} uses a generative model to augment the training data, and significantly improves re-ID over the long-tailed dataset. Second, comparing Circle loss with AM-Softmax, we observe the superiority of Circle loss, which is consistent with the experimental results on the face recognition task. Third, comparing ``ResNet50 + Circle loss'' against ``MGN + Circle loss'', we find that part-level features bring incremental improvement to Circle loss. It implies that Circle loss is compatible with the part-model specially designed for re-ID. 



\subsection{Fine-grained Image Retrieval}\label{sec:exp_finegrain}
\vspace{0.5em}
We evaluate the compatibility of Circle loss to pair-wise labeled data on three fine-grained image retrieval datasets, \emph{i.e.}, CUB-200-2011, Cars196, and Standford Online Products. On these datasets, majority methods~\cite{oh2016deep,Song_2017_CVPRHDC,Ge_2018_ECCVHTL,ABIER,Kim_2018_ECCVABE,wang2019multi} adopt the encouraged setting of learning with pair-wise labels. We compare Circle loss against these state-of-the-art methods in Table~\ref{tab:cub-cars}. We observe that Circle loss achieves competitive performance, on all of the three datasets. Among the competing methods, LiftedStruct~\cite{oh2016deep} and Multi-Simi~\cite{wang2019multi} are specially designed with elaborate hard mining strategies for learning with pair-wise labels. HDC~\cite{Song_2017_CVPRHDC}, ABIER~\cite{ABIER} and ABE~\cite{Kim_2018_ECCVABE} benefit from model ensemble. In contrast, the proposed Circle loss achieves performance on par with the state of the art, without any bells and whistles. 

\begin{figure}[t]
\centering
  \includegraphics[width=\linewidth]{figures/params.pdf}  
  \caption{Impact of two hyper-parameters. In (a), Circle loss presents high robustness on various settings of scale factor . In (b), Circle loss surpasses the best performance of both AM-Softmax and ArcFace within a large range of relaxation factor . }
\label{fig:params}
\end{figure}


\subsection{Impact of the Hyper-parameters}\label{sec:exp_param}
We analyze the impact of two hyper-parameters, \emph{i.e.}, the scale factor  in Eq.~\ref{eq:margin_circle} and the relaxation factor  in Eq.~\ref{eq:simple_boundary} on face recognition tasks. 



\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/train_logit}
    \caption{The change of  and  values during training. We linearly lengthen the curves within the first 2 iterations to highlight the initial training process (in the \textcolor{green}{green} zone). During the early training stage, Circle loss rapidly increases , because  deviates far from the optimum at the initialization and thus attracts higher optimization priority.}
    \label{fig:logit-train}
\end{figure}


\begin{figure*}[ht]
\centering
  \includegraphics[width=0.9\linewidth]{figures/distribution.pdf}
\caption{Visualization of the similarity distribution after convergence. The \textcolor{blue}{blue} dots mark the similarity pairs crossing the decision boundary during the whole training process. The \textcolor{green}{green} dots mark the similarity pairs after convergence. (a) AM-Softmax seeks to minimize . During training, the similarity pairs cross the decision boundary through a wide passage. After convergence, the similarity pairs scatter in a relatively large region in the  space. In (b) and (c), Circle loss has a circular decision boundary. The similarity pairs cross the decision boundary through a narrow passage and gather into a relatively concentrated region. }
\label{fig:scatter}
\end{figure*}

\textbf{The scale factor } determines the largest scale of each similarity score. The concept of the scale factor is critical in a lot of variants of Softmax loss. We experimentally evaluate its impact on Circle loss and make a comparison with several other loss functions involving scale factors. We vary  from  to  for both AM-Softmax and Circle loss. For ArcFace, we only set  to 32, 64 and 128, as it becomes unstable with larger  in our implementation. The results are visualized in Fig.~\ref{fig:params}. Compared with AM-Softmax and ArcFace, Circle loss exhibits high robustness on . The main reason for the robustness of Circle loss on  is the automatic attenuation of gradients. As the similarity scores approach the optimum during training, the weighting factors gradually decrease. Consequentially, the gradients automatically decay, leading to a moderated optimization.

\textbf{The relaxation factor } determines the radius of the circular decision boundary. We vary  from  to  (with  as the interval) and visualize the results in Fig.~\ref{fig:params} (b). It is observed that under all the settings from  to , Circle loss surpasses the best performance of Arcface, as well as AM-Softmax, presenting a considerable degree of robustness. 


\subsection{Investigation of the Characteristics}\label{sec:exp_mechanism}

\textbf{Analysis of the optimization process.}\quad
To intuitively understand the learning process, we show the change of  and  during the whole training process in Fig.~\ref{fig:logit-train}, from which we draw two observations:

First, at the initialization, all the  and  scores are small. It is because randomized features are prone to be far away from each other in the high dimensional feature space~\cite{Zhang2019AdaCosAS,helanqing_dissection}. Correspondingly,  get significantly larger weights (compared with ), and the optimization on  dominates the training, incurring a fast increase in similarity values in Fig.~\ref{fig:logit-train}. This phenomenon evidences that Circle loss maintains a flexible and balanced optimization.
 
Second, at the end of the training, Circle loss achieves both better within-class compactness and between-class discrepancy (on the training set), compared with AM-Softmax. Because Circle loss achieves higher performance on the testing set, we believe that it indicates better optimization.



\textbf{Analysis of the convergence.}\quad
We analyze the convergence status of Circle loss in Fig.~\ref{fig:scatter}.
We investigate two issues: how the similarity pairs consisted of  and  cross the decision boundary during training and how they are distributed in the  space after convergence. The results are shown in Fig.~\ref{fig:scatter}. In Fig.~\ref{fig:scatter} (a), AM-Softmax loss adopts the optimal setting of . In Fig.~\ref{fig:scatter} (b), Circle loss adopts a compromised setting of . The decision boundaries of (a) and (b) are tangent to each other, allowing an intuitive comparison. In Fig.~\ref{fig:scatter} (c), Circle loss adopts its optimal setting of . Comparing Fig.~\ref{fig:scatter} (b) and (c) against Fig.~\ref{fig:scatter} (a), we find that Circle loss presents a relatively narrower passage on the decision boundary, as well as a more concentrated distribution for convergence (especially when ). It indicates that Circle loss facilitates more consistent convergence for all the similarity pairs, compared with AM-Softmax loss. 
This phenomenon confirms that Circle loss has a more definite convergence target, which promotes the separability in the feature space.

\section{Conclusion}
This paper provides two insights into the optimization process for deep feature learning. First, a majority of loss functions, including the triplet loss and popular classification losses, conduct optimization by embedding the between-class and within-class similarity into similarity pairs. Second, within a similarity pair under supervision, each similarity score favors different penalty strength, depending on its distance to the optimum. These insights result in Circle loss, which allows the similarity scores to learn at different paces. The Circle loss benefits deep feature learning with high flexibility in optimization and a more definite convergence target. It has a unified formula for two elemental learning approaches, \emph{i.e.}, learning with class-level labels and learning with pair-wise labels. On a variety of deep feature learning tasks, \emph{e.g.}, face recognition, person re-identification, and fine-grained image retrieval, the Circle loss achieves performance on par with the state of the art. 



{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}


 


\vspace{0.5em}

\end{document}
