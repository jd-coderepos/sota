\documentclass{article}







\usepackage[nonatbib,final]{neurips_2021}


\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{hyperref}

\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\usepackage{xspace}
\usepackage{booktabs}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{cleveref}
\usepackage[fleqn]{nccmath}
\Crefformat{section}{Section~#2#1#3}
\Crefformat{proposition}{Proposition~#2#1#3}
\Crefformat{equation}{Eq.\,#2#1#3}

\usepackage{bm}
\usepackage{xspace}
\usepackage{enumitem}
\usepackage[dvipsnames]{xcolor}
\usepackage{subcaption}
\usepackage{caption}
\usepackage{wrapfig}
\usepackage{stackengine}
\usepackage{multirow}
\usepackage{array}
\usepackage{pbox}
\usepackage{dcolumn}
\usepackage[final]{pdfpages}

\usepackage{dsfont}
\usepackage{pifont}
\usepackage{marvosym}
\usepackage{makecell}
\usepackage{cellspace}
\usepackage[super]{nth}

\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}\newcolumntype{Y}{>{\centering\arraybackslash}X}

\newlength\savewidth\newcommand\shline{\noalign{\global\savewidth\arrayrulewidth
  \global\arrayrulewidth 1pt}\hline\noalign{\global\arrayrulewidth\savewidth}}
\newcommand{\tablestyle}[2]{\setlength{\tabcolsep}{#1}\renewcommand{\arraystretch}{#2}\centering\footnotesize}
\makeatletter\renewcommand\paragraph{\@startsection{paragraph}{4}{\z@}
  {.5em \@plus1ex \@minus.2ex}{-.5em}{\normalfont\normalsize\bfseries}}\makeatother
\definecolor{pearDark}{HTML}{2980B9}
\definecolor{pearDarker}{HTML}{1D2DEC}


\def\x{}
	






\newcommand{\HRFormerblock}[6]{
\multirow{3}{*}{

}
}


\newcommand{\convblock}[4]{
\multirow{3}{*}{

}
}


\newcommand{\calA}{\mathcal{A}}
\newcommand{\calB}{\mathcal{B}}
\newcommand{\calC}{\mathcal{C}}
\newcommand{\calD}{\mathcal{D}}
\newcommand{\calE}{\mathcal{E}}
\newcommand{\calF}{\mathcal{F}}
\newcommand{\calG}{\mathcal{G}}
\newcommand{\calH}{\mathcal{H}}
\newcommand{\calI}{\mathcal{I}}
\newcommand{\calJ}{\mathcal{J}}
\newcommand{\calK}{\mathcal{K}}
\newcommand{\calL}{\mathcal{L}}
\newcommand{\calM}{\mathcal{M}}
\newcommand{\calN}{\mathcal{N}}
\newcommand{\calO}{\mathcal{O}}
\newcommand{\calP}{\mathcal{P}}
\newcommand{\calQ}{\mathcal{Q}}
\newcommand{\calR}{\mathcal{R}}
\newcommand{\calS}{\mathcal{S}}
\newcommand{\calT}{\mathcal{T}}
\newcommand{\calU}{\mathcal{U}}
\newcommand{\calV}{\mathcal{V}}
\newcommand{\calW}{\mathcal{W}}
\newcommand{\calX}{\mathcal{X}}
\newcommand{\calY}{\mathcal{Y}}
\newcommand{\calZ}{\mathcal{Z}}

\newcommand{\mathA}{\mathbb{A}}
\newcommand{\mathB}{\mathbb{B}}
\newcommand{\mathC}{\mathbb{C}}
\newcommand{\mathD}{\mathbb{D}}
\newcommand{\mathE}{\mathbb{E}}
\newcommand{\mathF}{\mathbb{F}}
\newcommand{\mathG}{\mathbb{G}}
\newcommand{\mathH}{\mathbb{H}}
\newcommand{\mathI}{\mathbb{I}}
\newcommand{\mathJ}{\mathbb{J}}
\newcommand{\mathK}{\mathbb{K}}
\newcommand{\mathL}{\mathbb{L}}
\newcommand{\mathM}{\mathbb{M}}
\newcommand{\mathN}{\mathbb{N}}
\newcommand{\mathO}{\mathbb{O}}
\newcommand{\mathP}{\mathbb{P}}
\newcommand{\mathQ}{\mathbb{Q}}
\newcommand{\mathR}{\mathbb{R}}
\newcommand{\mathS}{\mathbb{S}}
\newcommand{\mathT}{\mathbb{T}}
\newcommand{\mathU}{\mathbb{U}}
\newcommand{\mathV}{\mathbb{V}}
\newcommand{\mathW}{\mathbb{W}}
\newcommand{\mathX}{\mathbb{X}}
\newcommand{\mathY}{\mathbb{Y}}
\newcommand{\mathZ}{\mathbb{Z}}

\newcommand{\vecA}{\mathbf{A}}
\newcommand{\vecB}{\mathbf{B}}
\newcommand{\vecC}{\mathbf{C}}
\newcommand{\vecD}{\mathbf{D}}
\newcommand{\vecE}{\mathbf{E}}
\newcommand{\vecF}{\mathbf{F}}
\newcommand{\vecG}{\mathbf{G}}
\newcommand{\vecH}{\mathbf{H}}
\newcommand{\vecI}{\mathbf{I}}
\newcommand{\vecJ}{\mathbf{J}}
\newcommand{\vecK}{\mathbf{K}}
\newcommand{\vecL}{\mathbf{L}}
\newcommand{\vecM}{\mathbf{M}}
\newcommand{\vecN}{\mathbf{N}}
\newcommand{\vecO}{\mathbf{O}}
\newcommand{\vecP}{\mathbf{P}}
\newcommand{\vecQ}{\mathbf{Q}}
\newcommand{\vecR}{\mathbf{R}}
\newcommand{\vecS}{\mathbf{S}}
\newcommand{\vecT}{\mathbf{T}}
\newcommand{\vecU}{\mathbf{U}}
\newcommand{\vecV}{\mathbf{V}}
\newcommand{\vecW}{\mathbf{W}}
\newcommand{\vecX}{\mathbf{X}}
\newcommand{\vecY}{\mathbf{Y}}
\newcommand{\vecZ}{\mathbf{Z}}
\newcommand{\vecAlpha}{\boldsymbol{\alpha}}
\newcommand{\vecBeta}{\boldsymbol{\beta}}
\newcommand{\vecGamma}{\boldsymbol{\gamma}}
\newcommand{\vecLambda}{\boldsymbol{\lambda}}
\newcommand{\vecDelta}{\boldsymbol{\Delta}}
\newcommand{\vecdelta}{\boldsymbol{\delta}}
\newcommand{\vecphi}{\boldsymbol{\phi}}
\newcommand{\vectheta}{\boldsymbol{\theta}}

\newcommand{\veca}{\mathbf{a}}
\newcommand{\vecb}{\mathbf{b}}
\newcommand{\vecc}{\mathbf{c}}
\newcommand{\vecd}{\mathbf{d}}
\newcommand{\vece}{\mathbf{e}}
\newcommand{\vecf}{\mathbf{f}}
\newcommand{\vecg}{\mathbf{g}}
\newcommand{\vech}{\mathbf{h}}
\newcommand{\veci}{\mathbf{i}}
\newcommand{\vecj}{\mathbf{j}}
\newcommand{\veck}{\mathbf{k}}
\newcommand{\vecl}{\mathbf{l}}
\newcommand{\vecm}{\mathbf{m}}
\newcommand{\vecn}{\mathbf{n}}
\newcommand{\veco}{\mathbf{o}}
\newcommand{\vecp}{\mathbf{p}}
\newcommand{\vecq}{\mathbf{q}}
\newcommand{\vecr}{\mathbf{r}}
\newcommand{\vecs}{\mathbf{s}}
\newcommand{\vect}{\mathbf{t}}
\newcommand{\vecu}{\mathbf{u}}
\newcommand{\vecv}{\mathbf{v}}
\newcommand{\vecw}{\mathbf{w}}
\newcommand{\vecx}{\mathbf{x}}
\newcommand{\vecy}{\mathbf{y}}
\newcommand{\vecz}{\mathbf{z}}
\newcommand{\vecxi}{\bm{\xi}}



\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\conv}{conv}
\DeclareMathOperator{\cov}{Cov}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\length}{len}

\newcommand{\removed}[1]{}
\newcommand{\defeq}{\stackrel{\text{def}}{=}}
\newcommand{\one}[1]{\left[\!\left[ #1 \right]\!\right]}
\newcommand{\argmin}[1]{\underset{#1}{\mathrm{argmin}} \:}
\newcommand{\argmax}[1]{\underset{#1}{\mathrm{argmax}} \:}
\newcommand{\inner}[2]{\left\langle #1, #2 \right\rangle}
\newcommand{\hE}[1]{{\mathbb{\hat{E}}\left[{#1}\right]}}
\newcommand{\Ep}[2]{{\mathbb{E}_{#1}\left[{#2}\right]}}
\newcommand{\relu}[1]{\left[#1\right]_+}
\newcommand{\norm}[1]{\left\lVert{#1}\right\rVert}
\newcommand{\abs}[1]{{\left\lvert{#1}\right\rvert}}
\newcommand{\margin}{\text{margin}}

\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\pr}[2]{{\mathbb{P}_{#1}\left[{#2}\right]}}

\newcommand{\net}{\text{net}}
\newcommand{\eps}{\epsilon}
\newcommand{\tW}{\widetilde{W}}
\newcommand{\In}{\text{in}}
\newcommand{\Out}{\text{out}}
\newcommand{\Win}{\vecW_{\text{in}}}
\newcommand{\Wout}{\vecW_{\text{out}}}
\newcommand{\Wrec}{\vecW_{\text{rec}}}
\newcommand{\err}{Err}
\newcommand{\error}{\epsilon}
\newcommand{\hatell}{\hat{\ell}}

\newcommand{\vin}{v_{\textrm{in}}}
\newcommand{\vout}{v_{\textrm{out}}}
\newcommand{\layer}{\text{layer}}
\newcommand{\gnorm}[1]{\norm{#1}}
\newcommand{\pathr}{\phi}
\newcommand{\DAG}{\text{DAG}}
\newcommand{\convexnn}{\nu}
\newcommand{\nout}{n_\text{out}}
\newcommand{\nin}{n_\text{in}}
\newcommand{\netdepth}{d}
\newcommand{\netwidth}{H}
\newcommand{\nparam}{n_{\text{param}}}


\newcommand{\mc}[2]{\mathbb{C}^{#1\times #2}}
\newcommand{\mr}[2]{\mathbb{R}^{#1\times #2}}
\newcommand{\mrt}[3]{\mathbb{R}^{#1\times #2\times #3}}
\newcommand{\mrN}{\mr{I_1}{I_2\times \dots I_N}}
\newcommand{\rank}{\mathrm{rank}}
\newcommand{\nf}[2]{\nicefrac{#1}{#2}}
\newcommand{\vct}[1]{\bm{#1}}
\newcommand{\mat}[1]{\bm{#1}}
\newcommand{\ten}[1]{\mathcal{#1}}
\newcommand{\frob}[1]{\|#1\|_2}
\newcommand{\F}[1]{\|#1\|^2_F}
\newcommand{\p}[2]{\textrm{Pr}[ #1\in #2] }
\newcommand{\pe}[2]{\mathop{\textrm{Pr}}\limits_{#1}\left[#2\right] }
\newcommand{\Var}[1]{\textrm{Var}\left[#1\right]}
\newcommand{\m}[1]{\mathcal{#1}}
\newcommand{\ba}{\bm{a}}
\newcommand{\bv}{\bm{v}}
\newcommand{\bx}{\bm{x}}
\newcommand{\bw}{\bm{w}}
\newcommand{\be}{\bm{e}}
\newcommand{\bbf}{\bm{f}}
\newcommand{\bp}{\bm{p}}
\newcommand{\bn}{\bm{n}}
\newcommand{\bb}{\bm{b}}
\newcommand{\bxi}{\bm{\xi}}
\newcommand{\balpha}{\bm{\alpha}}
\newcommand{\bbeta}{\bm{\beta}}
\newcommand{\beps}{\bm{\eps}}
\newcommand{\mX}{\mathcal{X}}
\newcommand{\mY}{\mathcal{Y}}
\newcommand{\mF}{\mathcal{F}}
\newcommand{\mH}{\mathcal{H}}
\newcommand{\RE}[3]{\E_{#1}{\sup_{#2}#3}}
\newcommand{\reals}{\mathbb{R}}
\newcommand{\rc}[1]{R\left(#1\right)}
\newcommand{\ceil}[1]{\left\lceil#1\right\rceil}
\newcommand{\inp}[2]{\left\langle #1,#2\right\rangle}
\newcommand{\TODO}[1]{{\color{red} #1}}
\newcommand{\BL}{\text{BERT}_{\text{LARGE}}}
\newcommand{\BB}{\text{BERT}_{\text{BASE}}}
\newcommand{\newT}{\text{CMT}}
\newcommand{\CHANGES}[1]{{#1}}

\newcommand{\etal}{\textit{{et al}.}}
\newcommand{\eqnsm}[2]{}
\newcommand{\TP}{\mathit{TP}}
\newcommand{\FN}{\mathit{FN}}
\newcommand{\FP}{\mathit{FP}}
\newcommand{\IoU}{\text{IoU}}
\newcommand{\BN}{\mathbb N}
\newcommand{\SL}{{\cal L}}
\newcommand{\things}{\tss{Th}\xspace}
\newcommand{\stuff}{\tss{St}\xspace}






\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\mathbf{0}}}
\def\vone{{\mathbf{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\mathbf{a}}}
\def\vb{{\mathbf{b}}}
\def\vc{{\mathbf{c}}}
\def\vd{{\mathbf{d}}}
\def\ve{{\mathbf{e}}}
\def\vf{{\mathbf{f}}}
\def\vg{{\mathbf{g}}}
\def\vh{{\mathbf{h}}}
\def\vi{{\mathbf{i}}}
\def\vj{{\mathbf{j}}}
\def\vk{{\mathbf{k}}}
\def\vl{{\mathbf{l}}}
\def\vm{{\mathbf{m}}}
\def\vn{{\mathbf{n}}}
\def\vo{{\mathbf{o}}}
\def\vp{{\mathbf{p}}}
\def\vq{{\mathbf{q}}}
\def\vr{{\mathbf{r}}}
\def\vs{{\mathbf{s}}}
\def\vt{{\mathbf{t}}}
\def\vu{{\mathbf{u}}}
\def\vv{{\mathbf{v}}}
\def\vw{{\mathbf{w}}}
\def\vx{{\mathbf{x}}}
\def\vy{{\mathbf{y}}}
\def\vz{{\mathbf{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\mathbf{A}}}
\def\mB{{\mathbf{B}}}
\def\mC{{\mathbf{C}}}
\def\mD{{\mathbf{D}}}
\def\mE{{\mathbf{E}}}
\def\mF{{\mathbf{F}}}
\def\mG{{\mathbf{G}}}
\def\mH{{\mathbf{H}}}
\def\mI{{\mathbf{I}}}
\def\mJ{{\mathbf{J}}}
\def\mK{{\mathbf{K}}}
\def\mL{{\mathbf{L}}}
\def\mM{{\mathbf{M}}}
\def\mN{{\mathbf{N}}}
\def\mO{{\mathbf{O}}}
\def\mP{{\mathbf{P}}}
\def\mQ{{\mathbf{Q}}}
\def\mR{{\mathbf{R}}}
\def\mS{{\mathbf{S}}}
\def\mT{{\mathbf{T}}}
\def\mU{{\mathbf{U}}}
\def\mV{{\mathbf{V}}}
\def\mW{{\mathbf{W}}}
\def\mX{{\mathbf{X}}}
\def\mY{{\mathbf{Y}}}
\def\mZ{{\mathbf{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 


\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
\def \deitbasedisup {DeiT-B\alambic} \def \alambic {\includegraphics[width=0.02\linewidth]{figs/alembic-crop.pdf}\xspace}


\newcommand{\caution}[1]{{\color{red}#1}}

\title{HRFormer: High-Resolution Transformer for Dense Prediction}

\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}
\author{
Yuhui Yuan\textsuperscript{124}
\enskip \enskip 
Rao Fu\textsuperscript{1}
\enskip \enskip
Lang Huang\textsuperscript{3}
\enskip \enskip 
\textbf{Weihong Lin\textsuperscript{4}}
\enskip \enskip 
\textbf{Chao Zhang\textsuperscript{3}}
\enskip \enskip 
\textbf{Xilin Chen\textsuperscript{12}}
\enskip \enskip \\
\textbf{Jingdong Wang\textsuperscript{5}\thanks{Corresponding author.}}
\\
\textsuperscript{1}University of Chinese Academy of Sciences
\enskip\enskip\enskip
\textsuperscript{2}Institute of Computing Technology, CAS \\
\enskip\enskip\enskip
\textsuperscript{3}Peking University
\enskip\enskip\enskip
\textsuperscript{4}Microsoft Research Asia
\enskip\enskip\enskip
\textsuperscript{5}Baidu
}

\begin{document}

\maketitle

\begin{abstract}
  We present a High-Resolution Transformer (HRFormer) that learns high-resolution representations for dense prediction tasks,
  in contrast to the original
  Vision Transformer that produces low-resolution representations and has high memory and computational cost.
  We take advantage of the multi-resolution parallel design introduced in high-resolution convolutional networks (HRNet~\cite{WangSCJDZLMTWLX19}), along with local-window self-attention that performs self-attention over small non-overlapping image windows~\cite{huang2019interlaced}, for improving the memory and computation efficiency.
  In addition, we introduce a convolution into the FFN to exchange information across the disconnected image windows.
  We demonstrate the effectiveness of the High-Resolution Transformer on both human pose estimation and semantic segmentation tasks,
  e.g., HRFormer outperforms Swin transformer~\cite{liu2021swin} by  AP on COCO pose estimation with  fewer parameters and  fewer FLOPs.
  Code is available at: {\footnotesize\color{blue}{\url{https://github.com/HRNet/HRFormer}}}.
\end{abstract}


\section{Introduction}
Vision Transformer (ViT)~\cite{dosovitskiy2020image} shows promising performance on ImageNet classification tasks.
Many follow-up works boost the classification accuracy through knowledge distillation~\cite{touvron2020deit}, adopting deeper architecture~\cite{touvron2021going}, directly introducing convolution operations~\cite{graham2021levit,wu2021cvt}, redesigning input image tokens~\cite{yuan2021tokens}, and etc.
Besides, some studies attempt to extend the transformer to address
broader vision tasks such as object detection~\cite{carion2020end}, semantic segmentation~\cite{SETR,Ranftl2021}, pose estimation~\cite{yang2020transpose,li2021pose}, video understanding~\cite{zhang2021multiscale,bertasius2021spacetime,neimark2021video}, and so on. This work focuses on the transformer for dense prediction tasks, including pose estimation and semantic segmentation.


Vision Transformer splits an image into a sequence of image patches of size , and extracts the feature representation of each image patch. Thus, the output representations of Vision Transformer lose the fine-grained spatial details that are essential for accurate dense predictions.
The Vision Transformer only outputs a single-scale feature representation,
and thus lacks the capability to
handle multi-scale variation.
To mitigate the loss of feature granularity and model the multi-scale variation,
we present High-Resolution Transformer (HRFormer) that contains
richer spatial information and constructs multi-resolution representations
for dense predictions.


The High-Resolution Transformer is built
by following the multi-resolution parallel design
that is adopted in HRNet~\cite{WangSCJDZLMTWLX19}.
First, HRFormer adopts convolution in both the stem and the first stage
as several concurrent studies~\cite{dai2021coatnet,xiao2021early}
also suggest that convolution performs better in the early stages.
Second, HRFormer maintains a high-resolution stream
through the entire process with parallel medium- and low-resolution streams
helping boost high-resolution representations.
With feature maps of different resolutions,
thus HRFormer is capable to model the multi-scale variation.
Third, HRFormer mixes the short-range and long-range attention
via exchanging multi-resolution feature information
with the multi-scale fusion module.


At each resolution, the local-window self-attention mechanism is adopted to reduce the memory and computation complexity.
We partition the representation maps into
a set of non-overlapping small image windows
and perform self-attention in each image window separately.
This reduces the memory and computation complexity
from quadratic to linear
with respect to spatial size.
We further introduce a  depth-wise  convolution
into the feed-forward network (FFN) that follows the local-window self-attention,
to exchange information
between the image windows
which are disconnected in the local-window self-attention process.
This helps to expand the receptive field
and is essential for dense prediction tasks.
Figure~\ref{fig:HRFormer_block} shows the details of an HRFormer block.


We conduct experiments on image classification, pose estimation, and semantic segmentation tasks, and achieve competitive performance on various benchmarks.
For example,
HRFormer-B gains  top- accuracy on ImageNet classification
over DeiT-B~\cite{touvron2020deit} with  fewer parameters and  fewer FLOPs.
HRFormer-B gains  AP over HRNet-W~\cite{XLW19} on COCO  set with with  fewer parameters and  fewer FLOPs.
HRFormer-B + OCR gains  and  mIoU over HRNet-W + OCR~\cite{yuan2019object}
with  fewer parameters and slightly more FLOPs
on PASCAL-Context \texttt{test} and COCO-Stuff \texttt{test}, respectively.


\begin{figure}[t]
  \small
  \centering
  \includegraphics[width=1.\linewidth]{figs/transunit.pdf}
  \caption{\small \textbf{Illustrating the HRFormer block.}
    The HRFormer block is composed of
    (a) local-window self-attentionm and (b) feed-forward network (FFN) with depth-wise convolution.
    The local-window self-attention scheme is inspired
    by the interlaced sparse self-attention~\cite{yuan2018ocnet,huang2019interlaced}.
  }
  \label{fig:HRFormer_block}
\end{figure}


\section{Related work}


\paragraph{Vision Transformers.}
With the success of Vision Transformer (ViT)~\cite{dosovitskiy2020image} and the data-efficient image transformer (DeiT)~\cite{touvron2020deit},
various techniques are proposed to improve the ImageNet classification
accuracy of Vision Transformer~\cite{dascoli2021convit,touvron2021going,wu2021cvt,graham2021levit,yuan2021tokens,han2021transformer,chen2021crossvit,liu2021swin,jiang2021token,srinivas2021bottleneck}.
Among the very recent advancements,
the community has verified several effective improvements such as multi-scale feature hierarchies and incorporating convolutions.

For example,
the concurrent works MViT~\cite{fan2021multiscale}, PVT~\cite{wang2021pyramid}, and Swin~\cite{liu2021swin} introduce the multi-scale feature hierarchies into transformer following the spatial configuration of a typical convolutional architecture such as ResNet-.
Different from them, our HRFormer incorporates the multi-scale feature hierarchies through
exploiting the multi-resolution parallel design inspired by HRNet.
CvT~\cite{wu2021cvt}, CeiT~\cite{yuan2021incorporating}, and LocalViT~\cite{li2021localvit}
propose to enhance the locality of transformer via inserting depth-wise  convolutions into either the self-attention or the FFN.
The purpose of the inserted convolution within our HRFormer is different,
apart from enhancing the locality,
it also ensures information exchange
across the non-overlapping windows.

Several previous studies~\cite{ramachandran2019stand,hu2019local} have proposed
similar local self-attention schemes for image classification.
They construct the overlapped local windows following the strided convolution,
resulting in heavy computation cost.
Similar to~\cite{huang2019interlaced,vaswani2021scaling,liu2021swin},
we propose to apply the local-window self-attention scheme
to divide the input feature map into non-overlapping windows.
Then we apply the self-attention within each window independently so as to improve
the efficiency significantly.

There are several concurrently-developed works~\cite{SETR,Ranftl2021} use the
Vision Transformer to address the dense predict tasks such as semantic segmentation.
They have shown that increasing the spatial resolution of the representations
output by the Vision Transformer is important for semantic segmentation.
Our HRFormer provides a different path to address the low-resolution problem
of the Vision Transformer via exploiting the multi-resolution parallel transformer
scheme.


\paragraph{High-Resolution CNN for Dense Prediction.}
The high-resolution convolutional schemes have achieved great success on
both pose estimation and semantic segmentation tasks.
In the development of high-resolution convolutional neural networks, the community has developed three main paths including: (i) applying dilated convolutions to remove some down-sample layers~\cite{chen2017deeplab,yu2015multi},
(ii) recovering high-resolution representations from low-resolution representations with  decoders~\cite{RonebergerFB15,BadrinarayananK17,newell2016stacked,NewellYD16},
and (iii) maintaining high-resolution representations throughout the network~\cite{WangSCJDZLMTWLX19,FourureEFMT017,SaxenaV16,ZhouHZ15,wang2020deep,zhang2020distribution,Huang2020CVPR}.
Our HRFormer belongs to the third path,
and retains the advantages of both
vision transformer and HRNet~\cite{WangSCJDZLMTWLX19}.


\begin{figure}[t]
  \small
  \centering
  \includegraphics[width=1.0\linewidth]{figs/HRFormer.pdf}
  \caption{\small \textbf{Illustrating the High-Resolution Transformer architecture.}
    The multi-resolution parallel transformer modules are marked with light blue color areas.
    Each module consists of multiple successive multi-resolution parallel transformer blocks. The first stage is constructed with convolution block and the remained three stages are constructed with transformer block.
  }
  \label{fig:HRFormer_arch}
\end{figure}



\section{High-Resolution Transformer}

\vspace{.1cm}
\noindent\textbf{Multi-resolution parallel transformer.}
We follow the HRNet~\cite{WangSCJDZLMTWLX19} design
and start from a high-resolution convolution stem as the first stage, gradually adding high-to-low resolution streams one by one as new stages. The multi-resolution streams are connected in parallel. The main body
consists of a sequence of stages.
In each stage, the feature representation of each resolution stream is updated
with multiple transformer blocks independently and the information
across resolutions is exchanged repeatedly with the convolutional
multi-scale fusion modules.

Figure~\ref{fig:HRFormer_arch} illustrates the overall HRFormer architecture.
The design of convolutional multi-scale fusion modules exactly follows HRNet.
We illustrate the details of the transformer block in the following discussion
and more details are presented in Figure~\ref{fig:HRFormer_block}.


\vspace{.1cm}
\noindent\textbf{Local-window self-attention.}
We divide the feature maps  into
a set of non-overlapping small windows:
,
where each window is of size .
We perform multi-head self-attention (MHSA) within each window independently.
The formulation of multi-head self-attention on the -th window is given as:

\noindent where ,
,
,
and 
for .
 represents the number of heads,
 represents the number of channels,
 represents the input resolutions, and
 represents the output representation of MHSA.
We also apply the relative position embedding scheme
introduced in the T model~\cite{2020t5} to incorporate the relative position
information into the local-window self-attention.


\renewcommand{\arraystretch}{1.1}
\begin{table*}[!htbp]
  \setlength{\tabcolsep}{0.5pt}
  \caption{
    \footnotesize
    \textbf{The architecture configuration of HRFormer.}
    {\textcolor{black}{LSA}}: local-window self-attention, {\textcolor{black}{FFN-DW}}: feed-forward network with a  depth-wise convolution,
    : the number of modules,
    : the number of blocks,
    : the size of windows,
    : the number of heads,
    : the MLP expansion ratios.
  }
  \label{tab:HRFormer_arch}
  \centering
  \scriptsize
  {
    \begin{tabular}{l|c|c|c|c}
      \shline
      {Res.}                                                             & {Stage }                               & Stage                                                            & Stage                                                            & Stage                                                            \\
      \shline
      \multirow{3}{*}{}
                                                                         & \convblock{}{}{}{} &
      \HRFormerblock{}{}{}{}{}{}   &
      \HRFormerblock{}{}{}{}{}{} &
      \HRFormerblock{}{}{}{}{}{}                                                                                                                                                                                                                                                                 \\
                                                                         &                                           &                                                                     &                                                                     &                                                                     \\
                                                                         &                                           &                                                                     &                                                                     &                                                                     \\
      \hline
      \multirow{3}{*}{}
                                                                         &                                           & \HRFormerblock{}{}{}{}{}{} & \HRFormerblock{}{}{}{}{}{} & \HRFormerblock{}{}{}{}{}{} \\
                                                                         &                                           &                                                                     &                                                                     &                                                                     \\
                                                                         &                                           &                                                                     &                                                                     &                                                                     \\
      \hline
      \multirow{3}{*}{}
                                                                         &                                           &                                                                     & \HRFormerblock{}{}{}{}{}{}   & \HRFormerblock{}{}{}{}{}{}   \\
                                                                         &                                           &                                                                     &                                                                     &                                                                     \\
                                                                         &                                           &                                                                     &                                                                     &                                                                     \\
      \hline
      \multirow{3}{*}{}
                                                                         &                                           &                                                                     &                                                                     & \HRFormerblock{}{}{}{}{}{} \\
                                                                         &                                           &                                                                     &                                                                     &                                                                     \\
                                                                         &                                           &                                                                     &                                                                     &                                                                     \\
      \shline
    \end{tabular}
  }
\end{table*}


\renewcommand{\arraystretch}{1.1}
\begin{table}[h]
  \footnotesize
  \centering
  \caption{\footnotesize\textbf{HRFormer instances.}
    HRFormer-T, HRFormer-S, and HRFormer-B represents tiny, small, and base HRFormer model, respectively.
  }
  \label{tab:HRFormer_arch_variants}
  \setlength{\tabcolsep}{8pt}
  \begin{tabular}{l|c|c|c|c}
    \shline
    Model      & \thead{\#modules                                                                                          \\ } & \thead{\#blocks\\} & \thead{\#channels \\ } & \thead{\#heads\\} \\
    \shline
    HRFormer-T &  &  &    &   \\
    HRFormer-S &  &  &   &   \\
    HRFormer-B &  &  &  &  \\
    \shline
  \end{tabular}
\end{table}


With \rm{MHSA} aggregates information within each window, we merge them to compute
the output :
\begin{ceqn}
  
\end{ceqn}
The left part of Figure~\ref{fig:HRFormer_block} illustrates how local-window self-attention updates the D input representations,
where the multi-head self-attention operates within each window independently.


\vspace{.1cm}
\noindent\textbf{FFN with depth-wise convolution.}
Local-window self-attention performs self-attention
over the non-overlapping windows separately.
There is no information exchange across the windows.
To handle this issue,
we add a  depth-wise convolution in
between the two point-wise MLPs
that form the FFN in Vision transformer:
.
The right part of Figure~\ref{fig:HRFormer_block} shows an example
of how FFN with  depth-wise convolution updates the D input representations.


\vspace{.1cm}
\noindent\textbf{Representation head designs.}
As shown in Figure~\ref{fig:HRFormer_arch},
the output of HRFormer consists of four feature maps of different resolutions.
We illustrate the details of the representation head designs for
different tasks as following:
(i) ImageNet classification,
we send the four-resolution feature maps into a bottleneck and the output channels
are changed to , , , and  respectively.
Then, we apply the strided convolutions to fuse them and output a
feature map of the lowest resolution with  channels.
Last, we apply a global average pooling operation followed by the final classifier.
(ii) pose estimation,
we only apply the regression head over the highest resolution feature map.
(iii) semantic segmentation,
we apply the semantic segmentation head over the concatenated representations,
which are computed by first upsampling all the low-resolution representations to the highest resolution and then concatenate them together.



\begin{wrapfigure}{l}{0.5\textwidth}
  \small
  \centering
  \includegraphics[width=0.6\linewidth]{figs/dw33connect.pdf}\\
  \caption{\small {Illustrating that FFN with  depth-wise convolution connects the non-overlapping windows.}}
  \label{fig:HRFormer_analysis}
\end{wrapfigure}

\vspace{.1cm}
\noindent\textbf{Instantiation.}
We illustrate the overall architecture configuration of HRFormer
in Table~\ref{tab:HRFormer_arch}.
We use  and  to represent the
number of modules and the number of blocks of
\{state, stage, stage, stage\}, respectively.
We use ,  and  to represent
the number of channels, the number of heads and the MLP expansion ratios in transformer block associated with different resolutions.
We keep the first stage unchanged following the original HRNet
and use the bottleneck as the basic building block.
We apply the transformer blocks in the other stages and each
transformer block consists of a local-window self-attention followed
by an FFN with  depth-wise convolution.
We have not included the convolutional multi-scale fusion modules in Table~\ref{tab:HRFormer_arch}
for simplicity.
In our implementation,
we set the size of the windows on four resolution streams
as  by default.
Table~\ref{tab:HRFormer_arch_variants} illustrates the configuration details of three different HRFormer instances with increasing complexities,
where the MLP expansion ratios  are set as  for all models and are not shown.




\vspace{.1cm}
\noindent\textbf{Analysis.}
The benefits of  depth-wise convolution are twofold:
one is enhancing the locality and the other one is enabling the interactions
across windows.
We illustrate how the FFN with depth-wise convolution is capable to expand the
interactions beyond the non-overlapping local windows and model the relations between them in Figure~\ref{fig:HRFormer_analysis}.
Therefore, based on the combination of the local-window self-attention
and the FFN with  depth-wise convolution, we can build the HRFormer block
that improves the memory and computation efficiency significantly.



\section{Experiments}

\subsection{Human Pose Estimation}
\noindent\textbf{Training setting.}
We study the performance of HRFormer
on the COCO~\cite{lin2014microsoft} human pose estimation benchmark,
which contains more than
K images and K person instances labeled with  keypoints.
We train our model on COCO \texttt{train}  dataset, including K images
and K person instances.
We evaluate our approach on the \texttt{val}  set and \texttt{test-dev} ,
containing K images and K images, respectively.

We follow most of the default training and evaluation settings
of \texttt{mmpose}~\cite{mmpose2020}\footnote{https://github.com/open-mmlab/mmpose, Apache License 2.0},
and change the optimizer from Adam to AdamW.
For the training batch size, we choose  for HRFormer-T and HRFormer-S
and  for HRFormer-B due to limited GPU memory.
Each HRFormer experiment on COCO pose estimation task takes  G-V GPUs.


\noindent\textbf{Results.}
Table~\ref{tab:coco_pose_val} reports the comparisons on COCO \texttt{val} set.
We compare HRFormer to the representative convolutional method such as HRNet~\cite{XLW19}
and several recent transformer methods, including PRTR~\cite{li2021pose}, TransPose-H-A~\cite{yang2020transpose},
and TokenPose-L/D~\cite{li2021tokenpose}.
HRFormer-B gains 
with  fewer parameters and  fewer FLOPs when compared to HRNet-W
with an input size of .
Therefore,
our HRFormer-B already achieves  w/o using any advanced techniques
such as UDP~\cite{Huang2020CVPR} and DARK\cite{zhang2020distribution}. We believe that our HRFormer-B could achieve better results by exploiting either UDP or DARK scheme.
We also report the comparisons on COCO \texttt{test-dev} set in Table~\ref{tab:coco_pose_test}.
Our HRFormer-B outperforms HRNet-W by around 
with fewer parameters and FLOPs.
Figure~\ref{fig:coco_pose_example} shows some example results
of human pose estimation on COCO \texttt{val} set.


\renewcommand{\arraystretch}{1.1}
\begin{table}[h]
  \footnotesize
  \setlength{\tabcolsep}{5pt}
  \renewcommand{\arraystretch}{1.3}
  \centering
  \caption{\textbf{Comparison on the COCO pose estimation \texttt{val} set.}
    The number of parameters and FLOPs
    for the pose estimation network are measured w/o considering
    neither human detection nor keypoint grouping.
    All results are based on ImageNet pretraining.
     means the numbers are not provided in the original paper.
  }
  \label{tab:coco_pose_val}
  \footnotesize
  {
    \begin{tabular}{l|ccc|cccccc}
      \shline
      Method                                    & input size               & \#param.                 & FLOPs                   &
                             &  &  &  &  &                                              \\\shline
      HRNet-W~\cite{XLW19}                  &           & M                  & G                  &                 &             &  &  &  &  \\
      HRNet-W~\cite{XLW19}                  &           & M                  & G                 &                 &             &  &  &  &  \\
      HRNet-W~\cite{XLW19}                  &           & M                  & G                 &                 &             &  &  &  &  \\
      HRNet-W~\cite{XLW19}                  &           & M                  & G                 &                 &             &  &  &  &  \\
      PRTR~\cite{li2021pose}                    &           & M                  & G                 &                 &             &  &  &  &  \\
      TransPose-H-A~\cite{yang2020transpose} &           & M                  & G                 &                 &                &     &     &     &  \\
      TokenPose-L/D~\cite{li2021tokenpose}  &           & M                  & G                 &                 &             &  &  &  &  \\\hline
      HRFormer-T                                &           & M                   & G                  &                 &             &    &    &  &  \\
      HRFormer-T                                &           & M                   & G                  &                 &             &  &  &  &  \\
      HRFormer-S                                &           & M                   & G                  &                 &             &  &  &  &  \\
      HRFormer-S                                &           & M                   & G                  &                 &             &  &  &  &  \\
      HRFormer-B                                &           & M                  & G                 &                 &             &  &  &  &  \\
      HRFormer-B                                &           & M                  & G                 &                 &             &  &  &  &  \\
      \shline
    \end{tabular}
  }
\end{table}


\renewcommand{\arraystretch}{1.1}
\begin{table}[h]
  \footnotesize
  \centering
  \setlength{\tabcolsep}{5pt}
  \renewcommand{\arraystretch}{1.3}
  \caption{\textbf{Comparison on the COCO pose estimation \texttt{test-dev} set.}
    The number of parameters and FLOPs
    for the pose estimation network are measured w/o considering
    neither human detection nor keypoint grouping.
    All results are based on ImageNet pretraining.
  }
  \label{tab:coco_pose_test}
  \footnotesize
  {
    \begin{tabular}{l|ccc|cccccc}
      \shline
      Method                                    & input size               & \#param.                 & FLOPs                   &
                             &  &  &  &  &                                              \\\shline
      HRNet-W~\cite{XLW19}                  &           & M                  & G                 &                 &             &  &  &  &  \\
      HRNet-W~\cite{XLW19}                  &           & M                  & G                 &                 &             &  &  &  &  \\
      PRTR~\cite{li2021pose}                    &           & M                  & G                 &                 &             &  &  &  &  \\
      TransPose-H-A~\cite{yang2020transpose} &           & M                  & G                 &                 &             &  &  &  &     \\
      TokenPose-L/D~\cite{li2021tokenpose}  &           & M                  & G                 &                 &             &  &  &  &  \\\hline
      HRFormer-S                                &           & M                   & G                  &                 &             &  &  &  &  \\
      HRFormer-B                                &           & M                  & G                 &                 &             &  &  &  &  \\
      \shline
    \end{tabular}
  }
\end{table}


\begin{figure*}[t]
  \centering
  \includegraphics[height = 0.171\textwidth]{figs/poseexampleresults/hrvitw78score_902_id_53626_000000053626.png}
  \includegraphics[height = 0.171\textwidth]{figs/poseexampleresults/hrvitw78score_773_id_38829_000000038829.png}
  \includegraphics[height = 0.171\textwidth]{figs/poseexampleresults/hrvitw78score_818_id_51314_000000051314.png}
  \includegraphics[height = 0.171\textwidth]{figs/poseexampleresults/hrvitw78score_727_id_18491_000000018491.png}
  \vspace{-0mm}
  \caption{{Example results of HRFormer-B on COCO pose estimation \texttt{val}: containing occlusion, multiple persons, viewpoint and appearance change.}}
  \label{fig:coco_pose_example}
  \vspace{-0mm}
\end{figure*}


\renewcommand{\arraystretch}{1.1}
\begin{table}[h]
  \centering
  \footnotesize
  \setlength{\tabcolsep}{1pt}
  \renewcommand{\arraystretch}{1.3}
  \caption{\textbf{Comparison with the recent SOTA on semantic segmentation tasks.}
    We report the mIoUs on Cityscapes \texttt{val}, PASCAL-Context \texttt{test}, COCO-Stuff \texttt{test}, and ADEK \texttt{val}.
    The number of parameters and FLOPs are measured
    on the image size of ,
    and the output label map size of .
    All results are evaluated with multi-scale testing.
    : the results are obtained with extra pre-training on ADEK.
  }
  {
    \begin{tabular}{l|cc|cccc}
      \shline
      Method                                          & \#params. & FLOPs      & {Cityscapes} & {PASCAL-Context} & {COCO-Stuff} & ADEK \\
      \shline
      \multicolumn{6}{l}{\emph{Transformer backbone}}                                                                                      \\\hline
      SETR-PUP~\cite{SETR}                            & M  & G  &        &            &           &    \\
      SETR-MLA~\cite{SETR}                            & M  & G  &           &            &           &    \\
      Swin-S + UperNet~\cite{liu2021swin}             & M  & G &           &               &           &    \\
      Swin-B + UperNet~\cite{liu2021swin}             & M & G &           &               &           &    \\
      PVT-Large + Semantic FPN~\cite{wang2021pyramid} & M   & G       &           &               &           &    \\
      \hline
      \multicolumn{6}{l}{\emph{CNN backbone}}                                                                                              \\\hline
      Deeplabv3~\cite{chen2017rethinking}             & M   & G  &        &            &           &       \\
      PSPNet~\cite{zhao2017pyramid}                   & M   & G  &        &            &        &       \\
      HRNet-W + OCR~\cite{yuan2019object}         & M   & G   &           &            &        &    \\ \hline
      \multicolumn{6}{l}{\emph{CNN+Transformer backbone}}                                                                                  \\\hline
      DPT-Hybrid~\cite{Ranftl2021}                    & M  & G  &           &    &           &    \\
      HRFormer-B + OCR                                & M   & G  &      &          &        &    \\
      HRFormer-B + OCR + SegFix~\cite{yuan2020segfix} & M   & G  &      &               &           &       \\
      \shline
    \end{tabular}
  }
  \label{tab:seg_result}
\end{table}


\begin{figure*}[t]
  \centering
  \includegraphics[height = 0.12\textwidth]{figs/segexampleresults/munster_000132_000019_leftImg8bit.png}
  \includegraphics[height = 0.12\textwidth]{figs/segexampleresults/COCO_train2014_000000264526.jpg}
  \includegraphics[height = 0.12\textwidth]{figs/segexampleresults/COCO_train2014_000000292312.jpg}
  \includegraphics[height = 0.12\textwidth]{figs/segexampleresults/2008_000052.jpg}
  \includegraphics[height = 0.12\textwidth]{figs/segexampleresults/2008_001640.jpg}
  \caption{Example results of HRFormer-B + OCR on
      {Cityscapes \texttt{val} (left one), COCO-Stuff \texttt{test} (middle two), and PASCAL-Context \texttt{test} (right two).}}
  \label{fig:seg_result_example}
  \vspace{-.2cm}
\end{figure*}


\subsection{Semantic Segmentation}

\vspace{1mm}
\noindent\textbf{Cityscapes}.
The Cityscapes dataset~\cite{cordts2016cityscapes} is for urban scene understanding.
There are a total of
 classes
and only  classes are used for parsing evaluation.
The dataset contains K high-quality pixel-level finely annotated images and K coarsely annotated images. The finely annotated K images are divided into   images,   images and   images.
We set the initial learning rate as , weight decay as , crop size as , batch size as , and training iterations as K by default.
Each HRFormer + OCR experiment on Cityscapes takes  G-V GPUs.

Table~\ref{tab:seg_result} reports the results on Cityscapes \texttt{val}.
We choose to use HRFormer + OCR as our semantic segmentation architecture.
We compare our method with several well-known Vision Transformer based methods~\cite{SETR,Ranftl2021} and CNN based methods~\cite{chen2017deeplab,zhao2017pyramid,yuan2019object}.
Specifically,
SETR-PUP and SETR-MLA use the ViT-Large~\cite{dosovitskiy2020image} as the backbone.
DPT-Hybrid uses the ViT-Hybrid~\cite{dosovitskiy2020image} that consists of a ResNet- followed by  transformer layers.
Both ViT-Large and ViT-Hybrid are initialized with the weights pre-trained on ImageNet-K,
where both of them achieve around  top accuracy on ImageNet.
DeepLabv3~\cite{chen2017deeplab} and PSPNet~\cite{zhao2017pyramid} are based on dilated ResNet- with output stride .
According to the fourth column of Table~\ref{tab:seg_result},
HRFormer + OCR achieves competitive performance overall.
For example,
HRFormer-B + OCR achieves comparable performance with SETR-PUP while saving  parameters
and  FLOPs.

\vspace{1mm}
\noindent\textbf{PASCAL-Context}.
The PASCAL-Context dataset~\cite{mottaghi2014role} is a challenging
scene parsing dataset that contains  semantic
classes and  background class.
The \texttt{train} set and \texttt{test} set consist of
 and  images respectively.
We set the initial learning rate as , weight decay as , crop size as , batch size as , and training iterations as K by default.
We report the comparisons on
the fifth column of Table~\ref{tab:seg_result}.
Accordingly,
HRFormer-B + OCR gains ,  over HRNet-W + OCR, SETR-MLA with fewer parameters
and FLOPs, respectively.
Notably, DPT-Hybrid achieves the best performance through extra pre-training the models
on ADEK in advance.
Each HRFormer + OCR experiment on PASCAL-Context takes  G-V GPUs.


\vspace{1mm}
\noindent\textbf{COCO-Stuff}.
The COCO-Stuff dataset~\cite{caesar2018coco} is a challenging
scene parsing dataset that contains  semantic
classes.
The \texttt{train} set and \texttt{test} set consist of
K and K images respectively.
We set the initial learning rate as , weight decay as , crop size as , batch size as , and training iterations as K by default.
We report the comparisons on the last column of Table~\ref{tab:seg_result}
and HRFormer-B + OCR outperforms the previous best-performing HRNet-W + OCR by nearly .
Each HRFormer + OCR experiment on COCO-Stuff takes  G-V GPUs.
Figure~\ref{fig:seg_result_example} shows some example results on Cityscapes,
PASCAL-Context, and COCO-Stuff.



\subsection{ImageNet Classification}
\noindent\textbf{Training setting.}
We conduct the comparisons on ImageNet-K,
which consists of M \texttt{train} images and K \texttt{val} images
with  classes.
We train all models with batch size  for  epochs with AdamW~\cite{loshchilov2017decoupled} optimizer,
cosine decay learning rate schedule, weight decay as ,
and a bag of augmentation policies, including rand augmentation~\cite{cubuk2019randaugment},
mixup~\cite{zhang2017mixup}, cutmix~\cite{yun2019cutmix}, and so on.
HRFormer-T and HRFormer-S require G-V GPUs and HRFormer-B requires G-V GPUs.

\noindent\textbf{Results.}
We compare HRFormer to some representative CNN methods and vision transformer methods in Table~\ref{tab:imagenet_results}, where all methods are trained on ImageNet-K only.
The results of ViT-Large with larger dataset such as ImageNet-K not included for fairness.
According to Table~\ref{tab:imagenet_results}, HRFormer achieves competitive performance.
For example, HRFormer-B gains  over DeiT-B
while saving nearly  parameters and  FLOPs.



\begin{table}[h]
  \footnotesize
  \centering
  \setlength{\tabcolsep}{19 pt}
  \renewcommand{\arraystretch}{1.2}
  \caption{\small\textbf{Comparisons on ImageNet-K \texttt{val}.}
  }
  \label{tab:imagenet_results}
  \begin{tabular}{l|ccc|c}
    \shline
    Method                                        & image size       & \#param. & FLOPs    & {Top-1 acc.} \\
    \shline
    ResNet-~\cite{he2016deep}                 &  & M    & G   &        \\
    ResNet-~\cite{he2016deep}                 &  & M    & G   &        \\
    ResNet-~\cite{he2016deep}                &  & M    & G   &        \\
    \hline
    HRNet-W~\cite{WangSCJDZLMTWLX19}          &  & M  & G   &        \\
    HRNet-W~\cite{WangSCJDZLMTWLX19}          &  & M  & G   &        \\
    HRNet-W~\cite{WangSCJDZLMTWLX19}          &  & M  & G  &        \\ \hline
    RegNetY-G~\cite{radosavovic2020designing}  &  & M    & G   &        \\
    RegNetY-G~\cite{radosavovic2020designing}  &  & M    & G   &        \\
    RegNetY-G~\cite{radosavovic2020designing} &  & M    & G  &        \\
    \hline
    ViT-B/~\cite{dosovitskiy2020image}        &  & M    & G  &        \\
    ViT-L/~\cite{dosovitskiy2020image}        &  & M   & G &        \\
    \hline
    DeiT-T~\cite{touvron2020deit}                 &  & M     & G   &        \\
    DeiT-S~\cite{touvron2020deit}                 &  & M    & G   &        \\
    DeiT-B~\cite{touvron2020deit}                 &  & M    & G  &        \\
    {\deitbasedisup}~\cite{touvron2020deit}       &  & M    & G  &        \\ \hline
    Conformer-T~\cite{peng2021conformer}             &  & M  & G   &        \\
    Conformer-S~\cite{peng2021conformer}             &  & M  & G  &        \\
    Conformer-B~\cite{peng2021conformer}             &  & M  & G  &        \\ \hline
    PVT-T~\cite{wang2021pyramid}                  &  & M  & G   &        \\
    PVT-S~\cite{wang2021pyramid}                  &  & M  & G   &        \\
    PVT-M~\cite{wang2021pyramid}                  &  & M  & G   &        \\
    PVT-L~\cite{wang2021pyramid}                  &  & M  & G   &        \\ \hline
    Swin-T~\cite{liu2021swin}                     &  & M    & G   &        \\
    Swin-S~\cite{liu2021swin}                     &  & M    & G   &        \\
    Swin-B~\cite{liu2021swin}                     &  & M    & G  &      \\
    Swin-B~\cite{liu2021swin}                     &  & M    & G    &   \\ \hline
    HRFormer-T                                    &  & M   & G   &        \\
    HRFormer-S                                    &  & M  & G   &        \\
    HRFormer-B                                    &  & M  & G  &        \\
    \shline
  \end{tabular}
\end{table}


\subsection{Ablation Experiments}



\begin{table}[h]
  \footnotesize
  \centering
  \setlength{\tabcolsep}{10pt}
  \renewcommand{\arraystretch}{1.3}
  \caption{\label{tab:comp_HRFormer}\textbf{Study
      of the 33 depth-wise convolution in FFN.}
    We report the top1 acc., mIoU, and AP on ImageNet \texttt{val}, PASCAL-Context \texttt{test}, and COCO pose estimation \texttt{val}, respectively.
    Results on PASCAL-Context are evaluated with single-scale testing.
    The number of parameters and FLOPs are measured on ImageNet.
  }
  {
    \begin{tabular}{l|cc|ccc}
      \shline
      {Method}                    & \#param. & FLOPs   & {ImageNet} & {PASCAL-Context} & {COCO}  \\
      \shline
      FFN w/o 33 DW-Conv. & M   & G &     &           &  \\
      FFN w/ 3 3 DW-Conv. & M   & G &     &           &  \\
      \shline
    \end{tabular}
  }
\end{table}



\renewcommand{\arraystretch}{1.2}
\begin{table}[h]
  \footnotesize
  \begin{center}
    \setlength{\tabcolsep}{9pt}
    \centering
    \caption{\label{tab:comp_swin}Influence of shifted window scheme \& 33 depth-wise convolution within FFN based on Swin-T.}
    \begin{tabular}{l|c|c|c|c}
      \shline
      Method     & 3 3 depth-wise convolution in FFN & \#param. & FLOPs  & ImageNet top1 acc. \\
      \shline
      Swin-T     & \xmark                                    & M  & G &              \\
      Swin-T     & \cmark                                    & M  & G &              \\ \hline
      IntraWin-T & \xmark                                    & M  & G &              \\
      IntraWin-T & \cmark                                    & M  & G &              \\
      \shline
    \end{tabular}
  \end{center}
\end{table}



\renewcommand{\arraystretch}{1.2}
\begin{table}[h]
  \footnotesize
  \begin{center}
    \setlength{\tabcolsep}{6pt}
    \centering
    \caption{\label{tab:comp_small_hrnet}Shifted window scheme v.s.
      3 3 depth-wise convolution within FFN based on HRFormer-T.}
    \resizebox{1\linewidth}{!}
    {
      \begin{tabular}{c|c|c|c|ccc}
        \shline
        \thead{shifted                                               \\window scheme} & \thead{33 depth-wise\\convolution within FFN} & \#param. & FLOPs & \thead{ImageNet \\top1 acc.} & \thead{PASCAL-Context\\mIoU} & \thead{COCO\\AP}  \\
        \shline
        \xmark & \cmark & M & G &  &  &  \\
        \cmark & \xmark & M & G &  &  &  \\
        \shline
      \end{tabular}
    }
  \end{center}
\end{table}


\begin{table}[h]
  \footnotesize
  \setlength{\tabcolsep}{24 pt}
  \renewcommand{\arraystretch}{1.2}
  \centering
  \caption{\textbf{Comparisons to ViT \& DeiT on COCO pose estimation \texttt{val}.}
     marks the methods pretrained on ImageNet-K.
  }
  \label{tab:comp_vit_deit}
  {
    \begin{tabular}{l|ccc|c}
      \shline
      Method            & image size     & \#param. & FLOPs   & {COCO} \\
      \shline
      ViT-Large &  & M & G &  \\
      {\deitbasedisup}  &  & M  & G &  \\
      Swin-B    &  & M  & G &  \\
      \hline
      HRFormer-B        &  & M  & G &  \\
      \shline
    \end{tabular}
  }
\end{table}


\begin{table}[h]
  \footnotesize
  \centering
  \setlength{\tabcolsep}{15 pt}
  \renewcommand{\arraystretch}{1.3}
  \caption{\textbf{Comparisons to HRNet.}
    We report the top1 acc., mIoU, and AP on ImageNet \texttt{val}, PASCAL-Context \texttt{test}, and COCO pose estimation \texttt{val}, respectively.
    Results on PASCAL-Context are based on single-scale testing.
    The number of parameters and FLOPs are measured on ImageNet.
  }
  \label{tab:HRFormer_vs_hrnet}
  {
    \begin{tabular}{l|cc|ccc}
      \shline
      Method     & \#param. & FLOPs   & {ImageNet} & {PASCAL-Context} & {COCO} \\
      \shline
      HRNet-T    & M  & G  &      &            &  \\
      HRFormer-T & M   & G  &      &            &  \\  \hline
      HRNet-S    & M  & G  &      &            &  \\
      HRFormer-S & M  & G  &      &            &  \\ \hline
      HRNet-B    & M  & G &      &            &  \\
      HRFormer-B & M  & G &      &            &  \\
      \shline
    \end{tabular}
  }
\end{table}


\paragraph{Influence of  depth-wise convolution within FFN}
We study the influence of the  depth-wise convolution within FFN
based on HRFormer-T in Table~\ref{tab:comp_HRFormer}.
We observe that applying  depth-wise convolution in FFN significantly
improves the performance on multiple tasks, including ImageNet classification, PASCAL-Context segmentation,
and COCO pose estimation.
For example,
HRFormer-T + FFN w/ 3 3 depth-wise convolution
outperforms HRFormer-T + FFN w/o 3 3 depth-wise convolution
by ,  and 
on ImageNet, PASCAL-Context and COCO, respectively.


\paragraph{Influence of shifted window scheme \& 33 depth-wise convolution within FFN based on Swin-T.}
We compare our method with the shifted windows scheme of Swin transformer~\cite{liu2021swin} in Table~\ref{tab:comp_swin}.
For fair comparisons, we construct a Intra-Window transformer architecture following the same architecture configurations of Swin-T~\cite{liu2021swin} except that we do not apply shifted windows scheme.
We see that applying 33 depth-wise convolution within FFN improves both Swin-T
and Intrawin-T.
Surprisingly, when equipped with 3 3 depth-wise convolution within FFN, Intrawin-T even outperforms Swin-T.


\paragraph{Shifted window scheme v.s. 33 depth-wise convolution within FFN based on HRFormer-T.}
In Table~\ref{tab:comp_small_hrnet},
we compare the  depth-wise convolution within FFN scheme
to the shifted window scheme based on HRFormer-T.
According to the results, we see that applying 33 depth-wise convolution within FFN
significantly outperforms applying shifted window scheme across all different tasks.


\paragraph{Comparison to ViT, DeiT  \& Swin on pose estimation.}
We report the COCO pose estimation results based on the two well-known transformer models, including ViT-Large~\cite{dosovitskiy2020image}, {\deitbasedisup}~\cite{touvron2020deit}
and Swin-B~\cite{liu2021swin} in Table~\ref{tab:comp_vit_deit}.
Notably, both ViT-Large and Swin-B are pre-trained on ImageNetK in advance and then finetuned on ImageNetK and achieve  and  top-1 accuracy respectively.
  {\deitbasedisup} is trained on ImageNetK for  epochs and achieves  top-1 accuracy.
We apply deconvolution modules
to upsample the output representations of the encoder
following the SimpleBaseline~\cite{xiao2018simple} for three methods.
The number of parameters and FLOPs are listed on the fourth and fifth columns of Table~\ref{tab:comp_vit_deit}.
According to the results in Table~\ref{tab:comp_vit_deit},
we see that our HRFormer-B achieves better performance than all three methods with fewer parameters and FLOPs.


\paragraph{Comparison to HRNet.}
We compare our HRFormer to the convolutional HRNet with almost the same architecture configurations
via replacing all the transformer blocks with the conventional basic block consisting
of two  convolutions.
Table~\ref{tab:HRFormer_vs_hrnet} shows the comparison results
on ImageNet, PASCAL-Context, and COCO.
We observe that
HRFormer significantly outperforms HRNet under various configurations
with much less model and computation complexity.
For example, HRFormer-T outperforms HRNet-T by , , and 
on three tasks while requiring only around  parameters and FLOPs, respectively.
In summary, HRFormer achieves better performance via
exploiting the benefits of transformers such as content-dependent dynamic interactions.


\begin{figure}[t]
  \centering
  \includegraphics[height = 0.24\textwidth]{figs/supp/pose/hrvitw78score_878_id_23359_000000023359.png}
  \includegraphics[height = 0.24\textwidth]{figs/supp/pose/hrvitw78score_732_id_872_000000000872.png}
  \includegraphics[height = 0.24\textwidth]{figs/supp/pose/hrvitw78score_824_id_9772_000000009772.png}
  \includegraphics[height = 0.24\textwidth]{figs/supp/pose/hrvitw78score_724_id_3934_000000003934.png}
  \\
  \vspace{1mm}
  \includegraphics[height = 0.235\textwidth]{figs/supp/pose/hrvitw78score_784_id_1000_000000001000.png}
  \includegraphics[height = 0.235\textwidth]{figs/supp/pose/hrvitw78score_699_id_52996_000000052996.png}
  \includegraphics[height = 0.235\textwidth]{figs/supp/pose/hrvitw78score_878_id_41990_000000041990.png}
  \vspace{-0mm}
  \caption{{Visualization of the pose estimation results based on HRFormer-B on COCO  \texttt{val}.}}
  \label{fig:more_pose_example}
  \vspace{-0mm}
\end{figure}

\begin{figure}[hb]
  \centering
  \includegraphics[height = 0.248\textwidth]{figs/supp/seg/frankfurt_000000_012868_leftImg8bit.png}
  \includegraphics[height = 0.248\textwidth]{figs/supp/seg/frankfurt_000001_000538_leftImg8bit.png}
  \includegraphics[height = 0.245\textwidth]{figs/supp/seg/2008_002835.jpg}
  \includegraphics[height = 0.245\textwidth]{figs/supp/seg/2008_000780.jpg}
  \includegraphics[height = 0.245\textwidth]{figs/supp/seg/COCO_train2014_000000014546.jpg}
  \includegraphics[height = 0.247\textwidth]{figs/supp/seg/2008_000009.jpg}
  \includegraphics[height = 0.247\textwidth]{figs/supp/seg/2008_000516.jpg}
  \includegraphics[height = 0.247\textwidth]{figs/supp/seg/2008_002752.jpg}\\
  \caption{{Visualization of the semantic segmentation results based on HRFormer-B + OCR on Cityscapes \texttt{val}, PASCAL-Context \texttt{test}, and COCO-Stuff \texttt{test}.}}
  \label{fig:more_seg_example}
  \vspace{-0mm}
\end{figure}

\section{Conclusion}
In this work, we present the High-Resolution Transformer (HRFormer),
a simple yet effective transformer architecture,
for dense prediction tasks, including pose estimation and semantic segmentation.
The key insight is to integrate the HRFormer block,
which combines local-window self-attention
and FFN with depth-wise convolution to improve the memory and computation efficiency,
with the multi-resolution parallel design of the convolutional HRNet.
Besides, HRFormer also benefits from adopting convolution in the
early stages and mixing short-range and long-range attention
with multi-scale fusion scheme.
We empirically verify the effectiveness of our HRFormer
on both pose estimation and semantic segmentation tasks.


\section{Appendix}

\paragraph{More Visualization Results.}
We present additional visualizations of the example results of our method
on both pose estimation and semantic segmentation tasks.

Figure~\ref{fig:more_pose_example} shows
more pose estimation results of HRFormer-B on COCO \texttt{val}.
Figure~\ref{fig:more_seg_example}
shows more semantic segmentation results on Cityscapes \texttt{val}, PASCAL-Context \texttt{test} and COCO-Stuff \texttt{test}.

\paragraph{Ablation of window sizes.}
We report the results with different window sizes at different resolutions on semantic segmentation tasks and we will add more results if necessary. We use  to represent the window sizes associated with feature maps with different resolutions with stride , , , . We choose larger window sizes for higher resolution branches, thus, we have . According to these results, we can see that applying larger windows improves the performance, and applying different window sizes at different resolutions makes no big difference.


\begin{table}[!htbp]
  \footnotesize
  \centering
  \setlength{\tabcolsep}{14pt}
  \renewcommand{\arraystretch}{1.2}
  \caption{\label{tab:window_size}\textbf{Influence of the size of windows  in HRFormer-B on PASCAL Context.}}
  {
    \begin{tabular}{l|c|cc|c}
      \shline
      Method & {} & \#param. & FLOPs   & ss result (ms result) \\
      \shline
      \multirow{8}{*}{HRFormer-B + OCR}
             &               & M  & G &           \\
             &               & M  & G &           \\
             &           & M  & G &           \\
             &           & M  & G &           \\
             &           & M  & G &           \\
             &            & M  & G &           \\
             &            & M  & G &           \\
             &           & M  & G &           \\
      \shline
    \end{tabular}
  }
\end{table}

\bibliography{main}
\bibliographystyle{plain}

\end{document}