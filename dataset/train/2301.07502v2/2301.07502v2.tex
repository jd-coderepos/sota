

\begin{figure*}[ht]\centering
\subfloat[][]{
    \includegraphics[width=0.46\textwidth]{exp_coefficients_a}
}\qquad
\subfloat[][]{
    \includegraphics[width=0.46\textwidth]{exp_coefficients_b}
}
\caption{Plot of the accuracy on the Tobacco3482 dataset for twelve combinations of the multimodal coefficients. Base and first side component use MobileNetV2 (a) and ResNet50 (b) architectures. Each trend corresponds to a different configuration of the side-tuned network.}\label{fig:exp_coefficients}\end{figure*}



We performed an analysis of the multimodal side-tuning architecture to assess the quality of our methodology and to better understand how it contributed to the classification accuracy.
In the following, we first introduce the datasets used in our experiments, then we detail the training procedure, and finally we provide a comparison of the performance with respect to the state-of-the-art.
We also give a brief analysis of the inference process running time.

\subsection{Datasets}
The Tobacco3482 dataset~\cite{KUMAR2014119} comprises  greyscale scans of documents divided unevenly in  categories, e.g.\ resume, email, letter, memo.
The documents distribution among the classes spans from  for the resume category to  for memo.
It is a small subset of the Truth Tobacco Industry Documents and collects many hybrid content documents.
The textual version of this dataset, namely \textit{QS-OCR-Small}~\cite{audebert} reflects the same structure of the original image dataset. 
In our setting, we random sampled three subsets to be used for train, validation and test, fixing their cardinality to , , and  respectively, as in~\cite{7333910,audebert}.

The Ryerson Vision Lab Complex Document Information Processing (RVL-CDIP) dataset~\cite{7333910} contains  images divided into  categories from the Truth Tobacco Industry Documents, e.g.\ scientific publication, scientific report, handwritten. 
Its textual counterpart, \textit{QS-OCR-Large}, was developed in the same work that released \textit{QS-OCR-Small}~\cite{audebert}.
Differently from Tobacco3482, RVL-CDIP comes with pre-built subsets for train, validation and test that have respectively dimension of , , and .


\begin{table}[t]
    \caption{Baseline models and multimodal overall accuracy for Tobacco3482 using MobileNetV2 (visual features) and 1D CNN (textual features) architectures. Best result in bold.}\label{tab:exp_baseline}
    \centering
    \begin{tabular}{llc}
        \toprule
        \bfseries Model & \bfseries \#Params & \bfseries OA \\ 
        \midrule
        \textit{Text} & M &  \\
        \textit{Image} ({\scriptsize fine-tuning}) & M &  \\    
        \textit{Image} ({\scriptsize side-tuning}) & M &  \\
        \midrule
        \textit{Multimodal} ({\scriptsize side-tuning}) & M &  \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Training details}\label{subsec:exp_details}
All the models are implemented using the \textit{PyTorch} framework, version , and trained using an \textit{NVIDIA Titan XP} GPU\@. 
The hyper-parameters are selected from the experiments performed on the Tobacco3482 dataset.

We set the maximum number of epochs to  and the batch size to  documents for Tobacco3482 experiments while we chose to train, validate, and test with batches of  for RVL-CDIP for  epochs. 
We used the cross-entropy loss function in all the experiments.

We performed all tests using the \textit{Stochastic Gradient Descent} (SGD) optimizer with a momentum of  and an initial learning rate of , subject to a scheduled update at each iteration that follows the scheme proposed in~\cite{8270080}:


\subsection{Ablation Study}\label{subsec:exp_ablation}
Table~\ref{tab:exp_baseline} reports the \textit{Overall Accuracy} (OA) obtained on the Tobacco3482 dataset.
As shown in the table, the multimodal network outperforms the other combinations, proving that we are able to combine efficiently the features space and benefit from side-tuning.
The side-tuning architecture used in Table~\ref{tab:exp_baseline} uses the MobileNetV2 as base model and side component for visual features. 
The model is pre-trained on ImageNet, coefficients are  for image-only side-tuning, while , , and  for the multimodal version.

The second analysis explores the behavior of multimodal side-tuning with respect to different coefficients for the linear combination.
Indeed, each  plays a central role in the balancing of the learning process for side-tuning.
To assess their impact in our setting, we train several models following twelve different alpha configurations.
We select values ranging from  to  to always be able to exploit each component of the framework without excessively lowering the weights of the other networks.
We also consider two architectures for the image input (MobileNetV2 and ResNet50) and for both we tested two different network configurations.
The first inputs directly the combination of the base model and side models to the classification layer, while the second considers an additional FC layer before the classification one.
We test two different dimensions for the latter.
In Figure~\ref{fig:exp_coefficients}, we analyze the behaviors of this set of experiments.

The coefficients are ordered so that the linear combination in the merging layer gives incrementally more importance to the model component exploiting textual features ().
The accuracy increases with the progressive shift from the model that favors visual features, with  or  greater than , to a more text-centered classifier.
Small changes in the coefficients affect the training for all the architectures.
Nevertheless, those models with the additional fully connected layers, both in MobileNetV2 and in ResNet50 show the best trends. 
In particular, the best accuracy is reached by model with a dense layer of dimension 1024 for MobileNetV2 with the configuration ,  , and   and the one with a dense layer of dimension 512 for ResNet50 with the configuration ,  , and  .


In Table~\ref{tab:exp_architectures} we present the best results for MobileNetV2 and ResNet50 architectures on the Tobacco3482 dataset.
First, we tested the two architecture in the side-tuning framework using one side component and the same alpha configuration ( and  ).
Next, we take advantage of the second side component --- the text classifier --- to perform the multimodal side tuning.
Although very similar, both the experiments present better results when the MobileNetV2 architecture is selected.

In Table~\ref{tab:exp_overall_acc_rvl} we report the experiments for RVL-CDIP dataset, presenting a different trend with respect to Tobacco3482 experiments, in fact, ResNet50 has the best accuracy.
This is due to the fact that an architecture with a larger number of parameters (ResNet50) can benefit from a bigger dataset (RVL-CDIP) while suffering from small inter-class variability.
\begin{table}[t]
    \caption{Overall accuracy on the Tobacco3482 image dataset using two different off-the-shelf architectures for both base and side model in the side tuning framework. Best result in bold.}\label{tab:exp_architectures}
    \centering
    \begin{tabular}{llcc}
        \toprule
        \bfseries Model (base architecture) & \bfseries \#Params & \bfseries OA \\ 
        \midrule
        \textit{Image (ResNet50)} & M &  \\
        \textit{Image (MobileNetV2)} & M &  \\    
        \midrule
        \textit{Multimodal (ResNet50)} & M &  \\
        \textit{Multimodal (MobileNetV2)} & M &  \\    
        \bottomrule
    \end{tabular}
\end{table}
\begin{table}[t]
    \caption{Overall accuracy on the RVL-CDIP dataset compared with the results from previous works. Modalities of the data source are  image (\texttt{I}), text (\texttt{T}), or both (\texttt{I+T}). The selected alpha configuration  for the multimodal side-tuning is , , and  for MobileNetV2 and , , and  for ResNet50. Best result in bold.}\label{tab:exp_overall_acc_rvl}
    \centering
    \begin{tabular}{lccl}
        \toprule
        \bfseries Model & \bfseries \#Params & \bfseries Modality & \bfseries OA \\
        \midrule
        \textit{CNNs}~\cite{7333910} & M & \texttt{I} &  \\
        \textit{Audebert}~\cite{audebert} & M & \texttt{I+T} &  \\
        \textit{AlexNet + SPP}~\cite{8270002} & M & \texttt{I} & \\
        \textit{VGG16}~\cite{8270080} & M & \texttt{I} &  \\
        \textit{VGG16 + ULMFit}~\cite{8977998} & M & \texttt{I+T} &  \\
        \midrule
        \textit{Text} & M & \texttt{T} &  \\
        \textit{Multimodal (MobileNetV2)} & M & \texttt{I+T} &  \\    
        \textit{Multimodal (ResNet50)} & M & \texttt{I+T} &  \\
       \bottomrule
    \end{tabular}
\end{table}
\begin{table*}[ht]
    \caption{Overall and per-class accuracy on the Tobacco3482 dataset compared with the results from~\cite{audebert}. The selected alpha configuration  for the multimodal side-tuning is , , and  for MobileNetV2 and , , and  for ResNet50. Best results in bold.}\label{tab:exp_tobacco_class}
    \centering
    \begin{tabular}{lccccccccccc}
        \toprule
        \bfseries Model & \bfseries OA & \bfseries Adve & \bfseries Email & \bfseries Form & \bfseries Letter & \bfseries Memo & \bfseries News & \bfseries Note & \bfseries Report & \bfseries Resume & \bfseries Scientific\\
        \midrule
        \textit{Audebert} &  &  &  &  &  &  &  &  &  &  &  \\
        \midrule
Text &  &  &  &  &  &  &  &  &  &  &  \\
        \textit{Multimodal (ResNet50)} &  &  &  &  &  &  &  &  &  &  &  \\
        \textit{Multimodal (MobileNetV2)} &  &  &  &  &  &  &  &  &  &  &  \\
        \bottomrule
    \end{tabular}
\end{table*}

\subsection{Comparison with the state-of-the-art}
We proved the effectiveness of multimodal side-tuning compared to the fine-tuning on images and training from scratch on textual features.

In Table~\ref{tab:exp_overall_acc_rvl}, we compare five different state-of-the-art solutions with the proposed multimodal approach in terms of  overall accuracy on the RVL-CDIP dataset.
All the experiments have been carried out considering only the best configurations of alpha for both MobileNetV2 and ResNet50.

The works considered are the CNN implementation of~\cite{7333910}, the multimodal solution from~\cite{audebert}, the VGG16 network in~\cite{8270080}, the AlexNet implementation of~\cite{8270002}, and the VGG16  UMLFit of~\cite{8977998}. 
As it is possible to observe, performance on the RVL-CDIP dataset highlights that the proposed solution slightly improves the classification performance with respect to the methods proposed in~\cite{7333910,audebert,8270080,8270002} but obtains slighter lower results compare to~\cite{8977998}.
This is related to the difference in the networks complexity between our solution and the method from~\cite{8977998}, which has M parameters.

Finally, among these solutions, we select the one from~\cite{audebert}, the most similar approach to what we propose, and compare the per-class accuracy on the Tobacco3482 dataset.
In fact, the authors in~\cite{audebert} strived to use lightweight architecture as in our case but concatenated the output of the networks used for the images and the text before the classification. 
This also give us the chance to provide insights on the performance for the classes of interest in the Tobacco3482 dataset.
Table~\ref{tab:exp_tobacco_class} shows that the gain of our model is consistent over all the classes except for the scientific class. 
When compared with the solution proposed in~\cite{audebert}, the side-tuning model improves the overall accuracy of .

\subsection{Processing time}\label{subsec:exp_time}
We now provide a discussion about execution time of our algorithm to analyze the performance of the document classification system.
Although some document analysis could be conducted offline, critical applications require low latency in order to be performed as close to real time as possible.
We then averaged the timings of the multimodal MobileNetV2 version over five classification runs.
The full inference process of our model on a single document is carried on a Intel Xeon Silver 4208 CPU takes ms. 
Of those, ms () are spent for Tesseract OCR image processing and text extraction\footnote{Average timing for Tesseract has been computed using four threads as in~\cite{audebert}.}, ms () for evaluation of the base model, ms () for the side model exploiting image features, and ms () are spent in the inference of the side component fed with textual features.
The timings for the image (ms) and text load (ms) from disk occupy the remaining time ().
On NVIDIA Titan Xp GPU, the side-tuning model runs in ms --- whit Tesseract OCR occupying  of the time. The base model is evaluated in ms, ms for the image side model, and ms for the text model.

Compared to models with more complex architectures, the proposed system is able to be used in real-time applications with latencies around the second. If the selected components were to be replaced with heavier models, this would lead to an inevitable performance impoverishment.
