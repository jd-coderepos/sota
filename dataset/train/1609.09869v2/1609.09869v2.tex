\pdfoutput=1
\def\year{2017}\relax
\documentclass[letterpaper]{article}
\usepackage{aaai17}
\usepackage[table,x11names]{xcolor}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{times}
\usepackage{graphicx}
\usepackage{sidecap}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{subcaption}
\usepackage{cancel}
\usepackage{amsmath}
\usepackage{dsfont}
\usepackage{amsthm}
\usepackage{tikz-qtree}
\usepackage{listings}
\usepackage{bbm}
\usepackage{mathtools}
\usepackage{breqn}
\usepackage{todonotes}
\usepackage{rotating}
\usepackage{mathtools}
\usepackage{ amssymb }
\usepackage{multirow}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\newcommand{\Hc}{\bar{H}}
\newcommand{\etal}{et al.} 
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\brangle{\langle}{\rangle}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newcommand{\mathtext}[1]{\small{\textit{(#1)}}}
\newcommand{\Exp}[2]{\mathop{\mathbb{E}}_{#2}\left[#1\right]}
\newcommand{\vsigma}{\vec{\sigma}}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\KL}{KL}
\newcommand{\pt}[2]{p(Z_{#1}|Z_{#2})}
\newcommand{\pth}{p_\theta}
\newcommand{\qph}{q_\phi}
\newcommand{\Id}{\mathds{1}}
\newcommand{\dt}{\Delta}
\newcommand{\distt}[1]{\mathcal{N}(Z_{#1}+U_{#1}|\Delta \mathds{1})}
\newcommand{\cattwo}[2]{[#1\;\;#2]}
\newcommand{\catthr}[3]{[#1\;\;#2\;\;#3]}
\DeclareMathOperator{\Tr}{Tr}
\newcommand{\Prec}[1]{{\Sigma}_{#1}^{-1}}
\newcommand{\normtwo}[1]{\|#1\|_2}
\newcommand{\diff}{\textrm{\textit{diff}}}
\newcommand{\Normal}{\mathcal{N}}
\newcommand{\Ind}[1]{\mathbb{I}\left[#1\right]}
\newcommand{\lgsigsq}{\log\vec{\sigma}^2}
\newcommand{\deriv}[2]{\frac{\textit{\mathbf{d}}#1}{\textit{\mathbf{d}}#2}}
\newcommand{\vone}{\vec{\mathbf{1}}}
\newcommand{\vecx}{\vec{x}}
\newcommand{\vecu}{\vec{u}}
\newcommand{\vecz}{\vec{z}}
\newcommand{\bigCI}{\mathrel{\text{\scalebox{1.07}{}}}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\meanfxn}{\textit{G}_{\alpha}}
\newcommand{\covfxn}{\textit{S}_{\beta}}
\newcommand{\emisfxn}{\textit{F}_{\kappa}}
\newcommand{\lthph}{\mathcal{L}(\vecx;(\theta, \phi))}
\newcommand{\qIndep}{\textbf{q-INDEP}}
\newcommand{\qLR}{\textbf{q-LR}}
\newcommand{\qRNN}{\textbf{q-RNN}}
\newcommand{\qBRNN}{\textbf{q-BRNN}}
\usepackage{tikz}
\usetikzlibrary{bayesnet}
\usetikzlibrary{calc}
\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem*{cor}{Corollary}
\newcommand{\MLP}{\text{MLP}}
\newcommand{\ReLU}{\text{ReLU}}
\newcommand{\Tanh}{\text{tanh}}
\newcommand{\Sigmoid}{\text{sigmoid}}
\newcommand{\Softplus}{\text{softplus}}
\newcommand{\citep}{\cite}
\newcommand{\citet}{\cite}
\usepackage{xspace}
\newcommand{\DKF}{DMM\xspace}
\newcommand{\DMM}{DMM\xspace}
\newcommand{\DKS}{\textbf{DKS}\xspace}
 \frenchspacing
\pdfinfo{
/Title (Structured Inference Networks for Nonlinear State Space Models)
/Author (Rahul G. Krishnan, Uri Shalit, David Sontag)
}
\setcounter{secnumdepth}{2}  
\title{Title}
\author{Rahul G. Krishnan, Uri Shalit, David Sontag\\
	Courant Institute of Mathematical Sciences, 
	New York University\\
	\{rahul, shalit, dsontag\}@cs.nyu.edu\\
}
\begin{document}
\title{Structured Inference Networks for Nonlinear State Space Models} 
\maketitle
\begin{abstract}
Gaussian state space models have been used for decades as generative models of sequential data. They admit an intuitive probabilistic interpretation, have a simple functional form, and enjoy widespread adoption. We introduce a unified algorithm to efficiently learn a broad class of linear and non-linear state space models, including variants where the emission and transition distributions are modeled by deep neural networks. Our learning algorithm simultaneously learns a compiled inference network and the generative model, leveraging a structured variational approximation parameterized by recurrent neural networks to mimic the posterior distribution. We apply the learning algorithm to both synthetic and real-world datasets, demonstrating its scalability and versatility. We find that using the structured approximation to the posterior results in models with significantly higher held-out likelihood.
 \end{abstract}

\section{Introduction}

Models of sequence data such as hidden Markov models (HMMs) and
recurrent neural networks (RNNs)
are widely used in machine translation, 
speech recognition, 
and computational biology.
Linear and non-linear
Gaussian state space models (GSSMs, Fig.~\ref{fig:dkf}) are used in applications including
robotic planning and missile
tracking. 
However, despite huge progress over the last decade, efficient learning of
non-linear models from
complex high dimensional time-series remains a major challenge.
Our paper proposes a unified learning algorithm for a broad class of
GSSMs, and we introduce an inference procedure that scales easily to high dimensional data, 
compiling approximate (and where feasible, exact)
inference into the parameters of a neural network.  

In engineering and control, the parametric form of the GSSM model is often known, with typically a few
specific parameters that need to be fit to data. The most commonly used approaches for these types of learning and inference problems are often computationally demanding, e.g. dual extended Kalman filter \cite{Wan_NIPS96}, expectation maximization \cite{Briegel99fisherscoring,roweis2000algorithm}
or particle filters \cite{schon2011system}. Our compiled inference algorithm can easily deal with high-dimensions both in the observed and the latent spaces, without compromising the quality of inference and learning.

When the parametric form of the model is unknown, 
we propose learning {\em deep Markov models} (\DKF), a class of generative models 
where classic linear emission and
transition distributions 
are replaced with complex multi-layer perceptrons (MLPs).
These are GSSMs that retain the 
Markovian structure of HMMs, but
leverage the representational power of deep neural 
networks to model complex high dimensional data. 
If one augments a \DKF model such as the one presented in Fig.~\ref{fig:dkf} with edges from the observations 
to the latent states of the following time step , then the \DKF 
 can be seen to be similar to, though more restrictive than, stochastic RNNs \cite{bayer2014learning} and variational RNNs
\cite{chung2015recurrent}.

Our learning algorithm performs stochastic gradient ascent on
a variational lower bound of the likelihood. 
Instead of introducing
variational parameters for each data point, we {\em compile} the
inference procedure at the same time as learning the generative model. 
This idea was originally used in the wake-sleep algorithm for
unsupervised learning \cite{hinton1995wake}, and has since led to 
state-of-the-art results for unsupervised learning of deep
generative models \cite{kingma2013auto,mnih2014neural,rezende2014stochastic}. 

Specifically, we introduce a new family of {\em structured
 inference networks}, parameterized by recurrent neural networks, and
evaluate their effectiveness in three scenarios: 
(1) when the generative model is known and fixed, 
(2) in parameter estimation when the functional form of the model is known 
and (3) for learning deep Markov models.
By looking at the structure of the true posterior, we show both
theoretically and empirically that inference for a latent state should
be performed using information \emph{from its future}, as opposed to
recent work which performed inference using only information from the
past \cite{chung2015recurrent,gan2015deep,gregor2015draw}, and that a
structured variational approximation outperforms mean-field based approximations.
Our approach may easily be 
adapted to learning more general generative models, for example models
with edges from observations to latent states.  

Finally, we learn a \DMM on a polyphonic music dataset and on a dataset of
electronic health records (a complex high dimensional setting with missing data). We use the model learned on health records to ask queries such as ``what would have happened to patients had they not received treatment'', and show that our model correctly identifies the way certain medications affect a patient's health.

\begin{figure}[t]
\centering
	\centering
	\begin{tikzpicture}[scale=0.75, transform shape,blackdot/.style={thin, draw=black, align=center, scale = 0.3,fill=black}]
	\node [latent] (z1) {};
	\node [latent, right= of z1] (z2) {};
	\node [const, right=of z2] (dotsz) {\ldots};
	\node [obs, below= of z1] (x1) {};
	\node [obs, below= of z2] (x2) {};
	\edge {z1} {z2};
	\edge {z2} {dotsz};
	\edge {z1} {x1};
	\edge {z2} {x2};
	\end{tikzpicture}\qquad
	\begin{tikzpicture}[scale=0.75, transform shape,blackdot/.style={thin, draw=black, align=center, scale = 0.3,fill=black}]
	\node [latent] (z1) {};
	\node [latent, right= of z1] (z2) {};
	\node [const, right=of z2] (dotsz) {\ldots};
	\node [obs, below= of z1] (x1) {};
	\node [obs, below= of z2] (x2) {};
	\edge {z1} {z2};
	\edge {z2} {dotsz};
	\draw[->]  (z1) -- node[blackdot] {d} (z2);
	\draw[->]  (z1) -- node[blackdot] {d} (x1);
	\draw[->]  (z2) -- node[blackdot] {d} (x2);
	\draw[->]  (z2) -- node[blackdot] {d} (dotsz);
	\end{tikzpicture}\quad
	\begin{tikzpicture}[scale=0.75, transform shape,blackdot/.style={thin, draw=black, align=center, scale = 0.3, fill=black}]
	\node [det] (h1) {};
	\node [det, right= of h1] (h2) {};
	\node [const, right=of h2] (dotsz) {\ldots};
	\node [obs, below= of h1] (x1) {};
	\node [obs, below= of h2] (x2) {};
	\edge {h1} {z2};
	\edge {h2} {dotsz};
	\edge {x2} {dotsz};
	\edge {h1} {x1};
	\edge {x1} {h2};
	\edge {h2} {x2};
	\end{tikzpicture}
	\caption{\small \textbf{Generative Models of Sequential Data: } (\textbf{Top Left}) Hidden Markov Model (HMM), (\textbf{Top Right}) Deep Markov Model (DMM)
	{ {}} denotes the neural networks used in DMMs
        for the emission and transition functions. 
	(\textbf{Bottom}) Recurrent Neural Network (RNN), {{}} denotes a deterministic intermediate representation. 
	Code for learning
        DMMs and reproducing our results 
	may be found at: \texttt{\small github.com/clinicalml/structuredinference} \label{fig:dkf}}
\end{figure}
 





\textbf{Related Work:} Learning GSSMs with MLPs for the transition distribution was considered by \cite{raiko2009variational}. 
They approximate the posterior with non-linear dynamic 
factor analysis \cite{valpola2002unsupervised}, which scales quadratically with 
the observed dimension and is impractical for large-scale learning.

Recent work has considered variational learning of time-series
data using structured inference or recognition networks.
\citeauthor{archer2015black} propose using a
Gaussian approximation to the posterior distribution with a block-tridiagonal inverse covariance. 
\citeauthor{johnson2016structured} use a conditional random field as the inference network for time-series models. 
Concurrent to our own work, \citeauthor{fraccaro2016sequential} also learn sequential generative models 
using structured inference networks parameterized by recurrent neural networks.

\citeauthor{bayer2014learning} and \citeauthor{fabius2014variational} create a stochastic variant of RNNs 
by making the hidden state of the RNN at every time step be a function
of independently sampled latent variables. \citeauthor{chung2015recurrent}
apply a similar model to speech data, sharing parameters between the
RNNs for the generative model and the inference network.
\citeauthor{gan2015deep} learn a 
model with discrete random variables, using a structured inference network that only considers information from the 
past, similar to \citeauthor{chung2015recurrent} and \citeauthor{gregor2015draw}'s models. 
In contrast to these works, we use information from the future within a
structured inference network, which we show to be preferable both
theoretically and practically. Additionally, we systematically
evaluate the impact of the different variational approximations on
learning. 

\citeauthor{watter2015embed} construct a first-order Markov model using inference networks. However, their learning
algorithm is based on data tuples over consecutive time steps. This makes the strong assumption
that the posterior distribution can be recovered based on observations
at the current and next time-step.
As we show, for generative models like the one in Fig. \ref{fig:dkf}, the 
posterior distribution at any time step is a function of \emph{all}
future (and past) observations. 
 \section{Background}\label{sec:back}

\textbf{Gaussian State Space Models:}
We consider both inference and learning in a class of latent variable models given by:
We denote by  a vector valued latent variable and by  a vector valued observation. 
A sequence of such latent variables and observations is denoted  respectively.

We assume that the distribution of the latent states is a multivariate
Gaussian with a mean and covariance which are differentiable functions of the previous latent state and  (the time elapsed
of time between  and ). 
The multivariate observations  are distributed according to a distribution  
(e.g., independent Bernoullis if the data is binary) whose parameters are a function of the corresponding latent state .
Collectively, we denote by  the parameters of the generative model. 

Eq. \ref{eqn:gen_model} subsumes a large family of linear 
and non-linear Gaussian state space models. 
For example, by setting , where ,  and  are matrices,
we obtain linear state space models. 
The functional forms
and initial parameters for  may be pre-specified.

\textbf{Variational Learning: }
Using recent advances in variational inference 
we optimize a variational lower bound on the data log-likelihood. 
The key technical innovation is the introduction of an \emph{inference network} or \emph{recognition network} \cite{hinton1995wake,kingma2013auto,mnih2014neural,rezende2014stochastic}, a neural network which approximates the intractable posterior. This is a parametric conditional distribution that is optimized to perform inference. 
Throughout this paper we will use  to denote the parameters of the generative model, and  to denote the parameters of the inference network.

For the remainder of this section, we consider learning in a Bayesian network whose joint distribution factorizes as:
. 
The posterior distribution  is typically intractable. Using the well-known 
variational principle, we posit an approximate posterior distribution  to
obtain the following lower bound on the marginal likelihood:

where the inequality is by Jensen's inequality. 
\citeauthor{kingma2013auto,rezende2014stochastic} use a neural net (with parameters ) to parameterize .
The challenge in the resulting optimization problem is that the lower bound in Eq. \ref{eqn:varlowbnd} includes an expectation w.r.t. , which implicitly depends on the network parameters . 
When using a Gaussian variational approximation 
, 
where  are parametric functions of the
observation , this difficulty is overcome by using \emph{stochastic backpropagation}:
a simple transformation allows 
one to obtain unbiased Monte Carlo estimates of the gradients of  with respect to .
The  term in Eq. \ref{eqn:varlowbnd} can be estimated similarly since it is also an expectation. 
When the prior  is Normally distributed, the  and its gradients 
may be obtained analytically.
 \section{A Factorized Variational Lower Bound \label{sec:learnmodel}}

We leverage stochastic backpropagation to learn generative models given by 
Eq.~\ref{eqn:gen_model}, corresponding to the graphical model in Fig. \ref{fig:dkf}.
Our insight is that for the purpose of inference, we can use the Markov properties of
the generative model to guide us in deriving a structured
approximation to the posterior. Specifically, the posterior factorizes as:

To see this, use the independence statements implied by the graphical
model in Fig. \ref{fig:dkf}
 to note that , the true posterior, factorizes as:
	
Now, we notice that , yielding the desired result.
The significance of Eq. \ref{thm:p_fact} is that it yields 
insight into the structure of the exact posterior for the class of models
laid out in Fig. \ref{fig:dkf}. 

We directly mimic the structure of the posterior with the following factorization of the variational approximation:

where  and  are functions parameterized by neural nets.
Although  has the option to condition on all information across time, Eq. \ref{thm:p_fact}
suggests that in fact it suffices to condition on information from the future
and the previous latent state. The previous latent state serves as a summary statistic for information from the past.

\textit{Exact Inference: } We can match the factorization of the true posterior using the inference network
but using a Gaussian variational approximation for the approximate posterior over each latent variable (as we do) limits the expressivity
of the inferential model, except for the case of linear dynamical
systems where the posterior distribution is Normally distributed.
However, one could augment our proposed inference network with recent innovations that improve the variational
approximation to allow for multi-modality \cite{rezende2015variational,tran2016variational}. Such modifications could yield black-box 
methods for exact inference in time-series models, which we leave for future work. 


{\bf Deriving a Variational Lower Bound:} 
For a generative model (with parameters ) and an inference network (with parameters ), we are interested in 
. For ease of exposition, we instantiate the derivation of the variational bound for a single data point 
though we learn  from a corpus. 

The lower bound in Eq.~\ref{eqn:varlowbnd} has an analytic form of the  term only for the simplest of transition models  between  and  (Eq. \ref{eqn:gen_model}). 
One could estimate the gradient of the KL term by sampling from the variational model, 
but that results in high variance estimates and gradients. We use a different factorization of the KL term (obtained by using the prior distribution over latent variables), leading to the variational lower bound we use as our objective function:


The key point is the resulting objective function has more stable analytic gradients.
Without the factorization of the KL divergence in Eq. \ref{eqn:bound_likelihood}, 
we would have to estimate  via
Monte-Carlo sampling, since it has no analytic form. In contrast, in Eq. \ref{eqn:bound_likelihood} 
the individual KL terms \emph{do} have analytic forms.
A detailed derivation of the bound and the factorization of the KL divergence is detailed in the supplemental material.

\textbf{Learning with Gradient Descent:} 
The objective in Eq.~\ref{eqn:bound_likelihood} is differentiable
in the parameters of the model ().
If the generative model  is fixed, we perform gradient ascent of Eq. \ref{eqn:bound_likelihood}
in . Otherwise, we perform gradient ascent in both  and .
We use stochastic backpropagation \cite{kingma2013auto,rezende2014stochastic} 
for estimating the gradient w.r.t. . 
Note that the expectations are only taken with respect to the variables , which are the sufficient statistics of the Markov model. 
For the KL terms in Eq. \ref{eqn:bound_likelihood}, we use the fact that the prior  and the
variational approximation to the posterior
 are both Normally distributed, and
hence their KL divergence may be estimated analytically.

\begin{algorithm}[h]
\caption{\small \textbf{Learning a \DMM with stochastic gradient descent: } 
	We use a single sample from the recognition network during learning to evaluate expectations in the bound. 
We aggregate gradients across mini-batches.}
\begin{algorithmic} \label{alg1}
	\STATE \textbf{Inputs}: Dataset 
\STATE \qquad\quad\; Inference Model: 
\STATE \qquad\quad\; Generative Model: 
\WHILE{}
\STATE 1. Sample datapoint: 
\STATE 2. Estimate posterior parameters (Evaluate )
\STATE 3. Sample 
\STATE 4. Estimate conditional likelihood:  \& KL
\STATE 5. Evaluate  
\STATE 6. Estimate MC approx. to  
\STATE 7. Estimate MC approx. to  
\STATE (Use stochastic backpropagation to move gradients with respect to  inside expectation)
\STATE 8. Update  using ADAM \citep{kingma2014adam} 
\ENDWHILE 
\end{algorithmic}
\end{algorithm}

Algorithm \ref{alg1} depicts an overview of the learning algorithm.
We outline the algorithm for a mini-batch of size one, 
but in practice gradients are averaged across stochastically sampled mini-batches of the training set.
We take a gradient step in  and
, typically with an adaptive learning rate such as
\citet{kingma2014adam}. 
 \section{Structured Inference Networks\label{sec:opt_q}}


We now detail how 
we construct the variational approximation , and specifically 
how we model the mean and diagonal covariance functions  and  using recurrent neural networks (RNNs).
Since our implementation only models the diagonal of the covariance matrix (the vector valued variances), we denote this as  rather
than .
This parameterization cannot in general be expected 
to be equal to , but in many cases is a reasonable approximation. 
We use RNNs due to their ability to scale well to large datasets. 

Table \ref{tab:recognition_models} details
the different choices for inference networks that we evaluate.
The Deep Kalman Smoother \textbf{\DKS} corresponds exactly to the functional form suggested by Eq. \ref{thm:p_fact}, and is our proposed variational approximation. The \DKS smoothes 
information from the past () and future () to form the approximate posterior distribution. 

We also evaluate other possibilities for the variational models (inference
networks) : two are mean-field models (denoted \textbf{MF}) and
two are structured models (denoted \textbf{ST}). They are
distinguished by whether they use information from the past (denoted
\textbf{L}, for left), the future (denoted \textbf{R}, for right), or
both (denoted \textbf{LR}). See Fig. \ref{fig:var_approx_all} for an
illustration of two of these methods.
Each conditions on a different subset of the observations
to summarize information in the input sequence . 
\DKS corresponds to \textbf{ST-R}. 

\begin{table}[t]
\centering 
\caption{\small \textbf{Inference Networks: } BRNN refers to a Bidirectional RNN and comb.fxn is shorthand for combiner function.}
\resizebox{\linewidth}{!}{
\begin{tabular}{ccc}
\toprule 
Inference Network & Variational Approximation for  & Implemented With \\
\midrule 
\textbf{MF-LR} &  & BRNN\\
\textbf{MF-L} &  &  RNN\\
\textbf{ST-L} &  &  RNN \& comb.fxn\\
\textbf{\DKS} &  &  RNN \& comb.fxn \\
\textbf{ST-LR} &  & BRNN \& comb.fxn \\
\bottomrule 
\end{tabular}\label{tab:recognition_models}
}
\end{table}


\begin{figure}[t]
  \centering 
	\begin{tikzpicture}[scale=.7, transform shape]
	\tikzstyle{recnet}=[rectangle,fill=blue!25,draw=black,minimum size=17pt,inner sep=2pt]
	\node [obs] (x1) {};
	\node [obs, right= of x1, xshift=5pt] (x2) {};
	\node [obs, right= of x2, xshift=5pt] (xT) {};
		
	\node [det, above= of x1] (h1R) {};
	\node [det, above= of x2] (h2R) {};
	\node [det, above= of xT] (hTR) {};
	\node [const, left= of h1R](hRlabel){Forward RNN};

	\node [det, above= of h1R] (h1L) {};
	\node [det, above= of h2R] (h2L) {};
	\node [det, above= of hTR] (hTL) {};
	\node [const, left= of h1L, xshift=5pt](hRlabel){Backward RNN};


	\node [recnet, above= of h1L] (z1) {};
	\node [recnet, above= of h2L] (z2) {};
	\node [recnet, above= of hTL] (zT) {};
	\node [const, left=of z1, xshift=25pt](comb){\textcolor{red}{Combiner function}};
	
	\node [const, above= of z1, yshift=-20pt, xshift=-33pt, red](label1){(a)};
	\node [const, above= of z2, yshift=-20pt, xshift=-33pt, red](label2){(a)};
	\node [const, above= of zT, yshift=-20pt, xshift=-33pt, red](label3){(a)};
	

	\node [latent, above= of z1] (zhat1) {};
	\node [latent, above= of z2] (zhat2) {};
	\node [latent, above= of zT] (zhatT) {};
   	\node [latent, left= of zhat1, xshift=5pt](zhat0) {};
   		
	\edge [blue]{h1R} {h2R};
	\edge [blue]{h2R} {hTR};
	
	\edge [blue]{h2L} {h1L};	
	\edge [blue]{hTL}{h2L};	
	
	\edge [blue]{x1}{h1R};
	\edge [blue]{x2} {h2R};
	\edge [blue]{xT} {hTR};
	\edge [blue, bend left]{x1} {h1L};
	\edge [blue, bend left]{x2} {h2L};
	\edge [blue, bend left]{xT} {hTL};
	\edge [red, bend left]{h1R} {z1};
	\edge [red, bend left]{h2R} {z2};
	\edge [red, bend left]{hTR} {zT};
	\edge [red]{h1L} {z1};
	\edge [red]{h2L} {z2};
	\edge [red]{hTL} {zT};
	\edge [dashed]{z1}{zhat1};
	\edge [dashed]{z2}{zhat2};
    \edge [dashed]{zT}{zhatT};
    \edge [red, shorten <=0pt]{zhat0}{z1};
    \edge [red]{zhat1}{z2};
    \edge [red]{zhat2}{zT};
	\end{tikzpicture} 
	\caption{\label{fig:var_approx_all}\small \textbf{Structured Inference Networks: } \textbf{MF-LR} and
          \textbf{ST-LR} variational approximations for a sequence of
          length , using a bi-directional recurrent neural net
          (BRNN). The BRNN takes as input the sequence , and through a series of non-linearities denoted by
          the \textcolor{blue}{blue} arrows it forms a sequence of
          hidden states summarizing information from the left and right 
          ( and )
          respectively. Then through a further sequence of
          non-linearities which we call the ``combiner function'' (marked (a) above), and
          denoted by the \textcolor{red}{red} arrows, it outputs two
          vectors  and , parameterizing the mean and
          diagonal covariance of  of Eq.~\ref{eqn:q_fact}. Samples  are drawn from , as indicated by the black dashed arrows.
	For the structured variational models \textbf{ST-LR}, the samples  are fed into the computation of  and , as indicated by the red arrows with the label (a). The mean-field model does \emph{not} have these arrows, and therefore computes . 
	We use .
	The inference network for \DKS (ST-R) is structured like that of ST-LR except without the RNN from the past. 
} 
\end{figure}


The hidden states of the RNN parameterize the variational
distribution, which go through what we call the ``combiner function''.
We obtain the mean  and diagonal covariance  for the
approximate posterior at each time-step in a manner akin to Gaussian belief
propagation. Specifically, we interpret the hidden states of the
forward and backward RNNs as parameterizing the mean and variance of
two Gaussian-distributed ``messages'' summarizing the observations from the
past and the future, respectively. We then multiply these two
Gaussians, performing a variance-weighted average of the means. 
All operations should be understood to be performed
element-wise on the corresponding vectors.
 are the hidden states of the RNNs that run from the past and the future
respectively (see Fig. \ref{fig:var_approx_all}). 

{\bf Combiner Function for Mean Field Approximations:}
For the \textbf{MF-LR} inference network, the mean  and
diagonal variances  of the variational distribution
 are predicted using the output of the RNN (not
conditioned on ) as follows, where :



{\bf Combiner Function for Structured Approximations:} 
The combiner functions for the structured approximations are
implemented as:

The combiner function uses the  non-linearity from  to
approximate the transition function (alternatively, one could share
parameters with the generative model), and here we use a
simple weighting between the components.


\textbf{Relationship to Related Work: }
\citeauthor{archer2015black,gao2016linear} use  where . 
The key difference from our approach is that this parameterization (in particular, conditioning the posterior means only on ) does not account for the information
from the future relevant to the approximate posterior distribution for . 

\citeauthor{johnson2016structured} 
interleave predicting the local variational parameters of the graphical model (using an inference network) with steps of message passing inference.
A key difference between our approach and theirs is that we rely on the structured inference network to predict the optimal local variational parameters
directly. In contrast, in \citeauthor{johnson2016structured}, any suboptimalities 
in the initial local variational parameters may be overcome by the subsequent steps of optimization 
albeit at additional computational cost. 

\citeauthor{chung2015recurrent} propose the Variational RNN (VRNN) in
which Gaussian noise is introduced at each time-step of a RNN.
\citeauthor{chung2015recurrent} use an inference network that shares
parameters with the generative model and only uses information from
the past. If one views the noise variables and the hidden state of the
RNN at time-step  together as , then a factorization similar
to Eq. \ref{eqn:bound_likelihood} can be shown to hold, although the
KL term would no longer have an analytic form since  would not be Normally distributed. Nonetheless, our same
structured inference networks (i.e. using an RNN to summarize
observations from the future) could be used to improve the tightness
of the variational lower bound, and our empirical results suggest that
it would result in better learned models.
 \section{Deep Markov Models}
Following \cite{Raiko2006}, we apply the ideas of deep learning to non-linear continuous state space models.
When the transition and emission function have an unknown functional form, we 
parameterize  from Eq. \ref{eqn:gen_model}
with deep neural networks. See Fig. \ref{fig:dkf} (right) for an illustration of the graphical model.  

{\bf Emission Function:} 
We parameterize the emission function  using a two-layer MLP (multi-layer perceptron),
,
where NL denotes non-linearities such as ReLU, sigmoid, or
tanh units applied element-wise to the input vector. 
For modeling binary data, 
parameterizes the mean probabilities of independent Bernoullis. 

{\bf Gated Transition Function:} 
We parameterize the transition function from  to  using
a gated transition function inspired by Gated
Recurrent Units \citep{chung2014empirical}, instead of an MLP.
Gated recurrent units (GRUs) are a neural architecture that parameterizes the recurrence equation in the 
RNN with gating units to control the flow of information from one hidden state to the next, conditioned on the observation. 
Unlike GRUs, in the \DKF, the transition function is not conditional on any of the observations. All the information
must be encoded in the completely stochastic latent state. To achieve this goal, we create 
a Gated Transition Function. 
We would like the model to have the flexibility to choose
a linear transition for some dimensions while having
a non-linear transitions for the others. We 
adopt the following parameterization, where  denotes 
the identity function and  denotes element-wise multiplication:


Note that the mean and covariance functions both share the use of .
In our experiments, we initialize  to be the identity function and  to .
The parameters of the emission and transition function 
form the set . 
 \section{Evaluation}
Our models and learning algorithm are implemented in Theano \cite{theano}. 
We use Adam \cite{kingma2014adam} with a learning rate of  to train the \DKF. 
Our code is available at \texttt{\small github.com/clinicalml/structuredinference}.

\textbf{Datasets: } We evaluate on three datasets. 

\textit{Synthetic: } We consider simple linear and non-linear GSSMs. To train
the inference networks we use  datapoints of length . We consider
both one and two dimensional systems for inference and parameter estimation. 
We compare our results using the training value of the variational bound  (Eq. \ref{eqn:bound_likelihood}) and the
,
where  correspond to the true underlying 's that generated the data. 

\textit{Polyphonic Music: }
We train DMMs on polyphonic music data \cite{boulanger2012modeling}. 
An instance in the sequence comprises 
an 88-dimensional binary vector corresponding to the notes of a piano. 
We learn for  epochs and report results based on early stopping using the validation set. 
We report held-out negative log-likelihood (NLL) in the format ``a (b) \{c\}''.
 is an importance sampling based estimate of the
NLL (details in supplementary material);
 where  is the length
of sequence . This is an upper bound on the NLL, which facilitates comparison
to RNNs;
TSBN \cite{gan2015deep} (in their code) report 
. We compute this 
to facilitate comparison with their work.
As in \citep{howtotrain}, we found 
annealing the KL divergence in the variational bound () from  to  over
 parameter updates got better results. 

\textit{Electronic Health Records (EHRs): } 
The dataset comprises  diabetic patients using data from a major health insurance provider. 
The observations of interest are: A1c level (hemoglobin A1c, a protein
for which a high level indicates that the patient is diabetic) and glucose (blood sugar).
We bin glucose into quantiles and A1c into clinically meaningful bins. The observations also include age, gender and ICD-9 
diagnosis codes for co-morbidities of diabetes such as congestive heart failure, chronic kidney disease and obesity. 
There are  binary observations for a patient at every time-step. 
We group each patient's data (over  years) into three month intervals, yielding a sequence of length . 

\subsection{Synthetic Data}
\textbf{Compiling Exact Inference: }
We seek to understand whether inference networks can accurately 
compile exact posterior inference into the network parameters 
for linear GSSMs when exact inference is feasible.
For this experiment we optimize Eq. \ref{eqn:bound_likelihood} over
, while  is fixed to a synthetic distribution given by a
one-dimensional GSSM. 
We compare results obtained by the various approximations we propose 
to those obtained by an implementation of Kalman smoothing \citep{kfimplementation} which performs \emph{exact inference}. 
Fig. \ref{fig:synthetic_linear} (top and middle) depicts our results. 
The proposed \textbf{\DKS} (i.e., {\textbf{ST-R}) and \textbf{ST-LR}
outperform the mean-field based variational method \textbf{MF-L} that
only looks at information from the past. \textbf{MF-LR}, however, is
often able to catch up when it comes to RMSE,
highlighting the role that information from the 
future plays when performing posterior 
inference, as is evident in the posterior factorization in Eq. \ref{thm:p_fact}. 
Both \textbf{\DKS} and \textbf{ST-LR} converge to the RMSE of the
exact Smoothed KF, and moreover their lower bound on the likelihood becomes tight.



\begin{figure}[t!]
	\begin{center}
		\includegraphics[width=0.45\textwidth,keepaspectratio]{./images/synthetic/synthetic9-train-bound.pdf}
		\includegraphics[width=0.45\textwidth,keepaspectratio]{./images/synthetic/synthetic9-vis.pdf}
	\end{center}
\caption{\label{fig:synthetic_linear}
\small \textbf{Synthetic Evaluation: } ({\bf Top \& Middle}) Compiled inference for a \emph{fixed} linear
  GSSM: , .
The training set comprised  one-dimensional 
observations of sequence length .
{\bf (Top left)} RMSE with respect to true 
  that generated the data. 
{\bf (Top right)} Variational bound during
  training. The results on held-out data are very similar (see supplementary material).
{\bf (Bottom)} Visualizing inference in two sequences (denoted (1) and (2)); Left panels show the Latent Space of variables , right panels show the Observations . Observations are generated by the application of the 
emission function to the posterior shown in Latent Space.
Shading denotes standard deviations.
}
\end{figure}

\begin{figure}[t!]
	\centering
	\includegraphics[width=0.45\textwidth,keepaspectratio]{./images/param-estimation-synthetic11.pdf}
\caption{\label{fig:synthetic_param_est}
\small 
\textbf{Parameter Estimation: }
Learning parameters  in a two-dimensional non-linear GSSM. 

 where  denotes a vector,  denotes concatenation and superscript denotes
indexing.
}
\end{figure}


\textbf{Approximate Inference and Parameter Estimation: }
Here, we experiment with applying the inference networks to synthetic non-linear generative models
as well as using \DKS for learning a subset of parameters within a fixed generative model.
On synthetic non-linear datasets (see supplemental material)
we find, similarly, that the structured variational approximations 
are capable of matching the performance of inference using a smoothed Unscented Kalman Filter \cite{wan2000unscented} on held-out data. 
Finally, Fig. \ref{fig:synthetic_param_est} illustrates a toy
instance where we successfully perform 
parameter estimation in a synthetic, two-dimensional, non-linear GSSM.

\subsection{Polyphonic Music}
\textbf{Mean-Field vs Structured Inference Networks: }
Table \ref{tab:polyphonic_structural} shows the
results of learning a \DMM on the polyphonic music dataset using 
\textbf{MF-LR}, \textbf{ST-L}, \textbf{\DKS} and \textbf{ST-LR}.
\textbf{ST-L} is a structured variational approximation that only considers
information from the past and, up to implementation details, is comparable
to the one used in \cite{gregor2015draw}.
Comparing the negative log-likelihoods of the learned models, we see
that the looseness in the variational bound (which we first observed in the synthetic
setting in Fig. \ref{fig:synthetic_linear} top right) significantly
affects the ability to learn. \textbf{ST-LR} and \textbf{\DKS}
substantially outperform \textbf{MF-LR} and \textbf{ST-L}.
This adds credence to 
the idea that by taking into consideration the factorization of the posterior, one can perform
better inference and, consequently, learning, in real-world, high
dimensional settings. Note that the \textbf{\DKS} network has half the parameters
of the \textbf{ST-LR} and \textbf{MF-LR} networks.


\textbf{A Generalization of the \DMM: }
To display the efficacy of our inference algorithm to model variants beyond
first-order Markov Models, we further augment the \DKF with edges from  to 
and from  to .  
We refer to the resulting generative model as \DKF-Augmented (Aug.).
Augmenting the \DKF
with additional edges 
realizes a richer
class of generative models.

We show that \textbf{\DKS} can be used \emph{as is} for inference on 
a more complex generative model than \DMM, while making gains in held-out likelihood. All following experiments use {\bf \DKS} for posterior inference.

The baselines we compare to in Table \ref{tab:polyphonic} also have more complex generative models than the \DKF. 
STORN has edges from  to  given by the recurrence update
and TSBN has edges from  to  as well as from  to .  
HMSBN shares
the same structural properties as the \DKF, but
is learned using a simpler inference network. 

In Table~\ref{tab:polyphonic}, 
as we increase the complexity of the generative model, we obtain better results across all datasets. 

The \DKF outperforms both RNNs and HMSBN everywhere, outperforms STORN on JSB, Nottingham 
and outperform TSBN on all datasets except Piano. 
Compared to LV-RNN (that optimizes the inclusive KL-divergence), 
\DKF-Aug obtains better results on all datasets except JSB. This showcases our flexible, structured inference network's ability to learn 
powerful generative models that compare favourably to other 
state of the art models. We provide audio
files for samples from the learned \DMM models in the code repository. 

\begin{table}[t]
	\centering
	\caption{\small \textbf{Comparing Inference Networks: }  Test negative log-likelihood on polyphonic music of different inference networks trained
	on a \DKF with a fixed structure (lower is better). The numbers
      inside parentheses are the variational bound.}
\resizebox{\linewidth}{!}{
\begin{tabular}{ccccc}
    \toprule
    ~Inference Network & JSB  & Nottingham & Piano & Musedata\\
    \midrule 
{\bf \DKS} (i.e., {\bf ST-R})&  \begin{tabular}{c} 6.605 (7.033)  \end{tabular} 
    & \begin{tabular}{c} 3.136 (3.327) \end{tabular}
    & \begin{tabular}{c} 8.471 (8.584)\end{tabular}
    & \begin{tabular}{c} 7.280 (7.136)\end{tabular}\\
    \midrule
    {\bf ST-L}&  \begin{tabular}{c} 7.020 (7.519)  \end{tabular} 
    & \begin{tabular}{c} 3.446 (3.657) \end{tabular}
    & \begin{tabular}{c} 9.375 (9.498)\end{tabular}
    & \begin{tabular}{c} 8.301 (8.495)\end{tabular}\\
    \midrule
{\bf ST-LR }&  \begin{tabular}{c} 6.632 (7.078)  \end{tabular} 
    & \begin{tabular}{c}  3.251 (3.449) \end{tabular}
    & \begin{tabular}{c}  8.406 (8.529)\end{tabular}
    & \begin{tabular}{c}  7.127 (7.268)\end{tabular}\\
    \midrule
{\bf MF-LR}&  \begin{tabular}{c} 6.701  (7.101) \end{tabular} 
    & \begin{tabular}{c} 3.273 (3.441)  \end{tabular}
    & \begin{tabular}{c} 9.188 (9.297) \\ \end{tabular}
    & \begin{tabular}{c} 8.760 (8.877) \end{tabular}\\
    \bottomrule
    \end{tabular}\label{tab:polyphonic_structural}
   }
\end{table}
 
\begin{table}[t]
	\centering
	\caption{
		\label{tab:polyphonic}
		\small{\textbf{Evaluation against Baselines: } Test negative log-likelihood (lower is better) on Polyphonic Music Generation dataset.
			\textbf{Table Legend}: RNN \cite{boulanger2012modeling}, 
			LV-RNN \cite{gu2015neural}, STORN \cite{bayer2014learning},
			TSBN, HMSBN \cite{gan2015deep}.}}
\resizebox{\linewidth}{!}{
\begin{tabular}{ccccc}
    \toprule
    ~Methods       & JSB  & Nottingham & Piano & Musedata\\
    \midrule
    \midrule
\DKF &  \begin{tabular}{c} 6.388 \2.964)\\ \{2.954\} \end{tabular}
    & \begin{tabular}{c}  7.835 \6.989)\\ \{6.203\} \end{tabular}\\
    \midrule
\DKF-Aug. &  \begin{tabular}{c} 6.288 \2.856)\\ \{2.872\} \end{tabular}
    & \begin{tabular}{c} 7.591\6.476)\\ \{5.766\} \end{tabular}\\
    \midrule
    HMSBN &  \begin{tabular}{c}(8.0473)\\ \{7.9970\}\end{tabular} 
    & \begin{tabular}{c} (5.2354)\\ \{5.1231\} \end{tabular}
    & \begin{tabular}{c} (9.563)\\ \{9.786\}  \end{tabular}
    & \begin{tabular}{c} (9.741)\\ \{8.9012\}  \end{tabular}\\
    \midrule
    STORN& 6.91 & 2.85 & 7.13 & 6.16\\
    \midrule
    RNN  & 8.71 & 4.46 & 8.37 & 8.13\\ 
    \midrule
    TSBN & \{7.48\} & \{3.67\} & \{7.98\} & \{6.81\} \\
    \midrule
    LV-RNN & 3.99 & 2.72 & 7.61 & 6.89\\
\bottomrule
    \end{tabular}
}
\end{table}

\subsection{EHR Patient Data}
Learning models from large observational
health datasets is a promising approach to advancing
precision medicine and could be used, for example, to understand which
medications work best, for whom. In this section, 
we show how a \DMM may be used for precisely such an application.
Working with EHR data poses some technical challenges: EHR data are noisy, high dimensional and difficult to characterize easily.
Patient data is rarely contiguous over large parts of the dataset and is often missing (not at random). We learn a \DMM on the data 
showing how to handle the aforementioned technical challenges and use it for model based counterfactual prediction. 

\textbf{Graphical Model: }
Fig. \ref{fig:dmm_action} represents the generative model we use when . 
The model captures the idea of an underlying time-evolving latent state for a patient () that
is solely responsible for the diagnosis codes and lab values () we observe. In addition, the patient state
is modulated by drugs () prescribed by the doctor. We may assume that the drugs prescribed at any point in time
depend on the patient's entire medical history
though in practice, the dotted edges in the Bayesian network never need to be modeled since  and  are always assumed to be observed. A natural line of follow up work would be to consider learning when  is missing or latent.  

We make use of time-varying (binary) drug prescription  for each patient
by augmenting the \DMM with an additional edge every time step. 
Specifically, the \DMM's transition function is now 
 (cf. Eq. \ref{eqn:gen_model}).
In our data, each  is an indicator vector of eight anti-diabetic drugs including Metformin and Insulin, where Metformin is the most commonly prescribed first-line anti-diabetic drug. 


\begin{figure}[h]
\centering
        \begin{tikzpicture}[scale=0.7, transform shape]
        \node[latent] (z1) {};
        \node[obs, above= of z1] (u1) {};
        \node[obs,below= of z1] (x1) {};
        \node[latent,right=of z1] (z2) {};
        \node[obs, above=of z2] (u2) {};
        \node[obs, below=of z2] (x2) {};
        \node[latent,right=of z2] (z3) {};
        \node[obs, above=of z3] (u3) {};
        \node[obs, below=of z3] (x3) {};
        \node[latent,right=of z3] (z4) {};
        \node[obs, below=of z4] (x4) {};
	\edge{z1}{z2};
	\edge{z2}{z3};
	\edge{z3}{z4};
	\edge{z1}{x1};
	\edge{z2}{x2};
	\edge{z3}{x3};
	\edge{z4}{x4};
	\edge{u1}{z2};
	\edge{u2}{z3};
	\edge{u3}{z4};
	\edge[dashed] {x1}{u2};
	\edge[dashed] {x1}{u3};
	\edge[dashed] {x2}{u3};
    \end{tikzpicture}
    \caption{\small \textbf{\DMM for Medical Data: } 
    The \DMM (from Fig. \ref{fig:dkf}) is augmented with external actions  representing medications presented to the patient.  is 
the latent state of the patient.  are the observations that we model.
Since both  and  are always assumed observed, the conditional distribution  
may be ignored during learning.}
\label{fig:dmm_action}
\end{figure}

\textbf{Emission \& Transition Function:}The choice of emission and transition function to use for 
such data is not well understood. In Fig. \ref{fig:cfac} (right), we experiment with variants of 
DMMs and find that using MLPs (rather than linear functions) in the emission and transition function yield the best generative models in terms of held-out likelihood. 
In these experiments, the hidden dimension was set as  for the emission and transition functions.
We used an RNN size of  and a latent dimension of size . We use the \DKS as our inference network for learning.  

\textbf{Learning with Missing Data: }
In the EHR dataset, a subset of the observations (such as A1C and Glucose
values which are commonly used to assess blood-sugar levels for diabetics) is frequently missing in the data. We marginalize them 
out during learning, which is straightforward within the probabilistic semantics of our Bayesian network. 
The sub-network of the original graph we are concerned with is the emission function since missingness affects our ability 
to evaluate  (the first term in Eq. \ref{eqn:bound_likelihood}). 
The missing random variables are leaves in the Bayesian sub-network (comprised of the emission function).  
Consider a simple example of two modeling two observations
at time , namely . The log-likelihood of the data () conditioned 
on the latent variable  decomposes as 
since the random variables are conditionally independent given their parent. 
If  is missing and marginalized out while  is observed, then
our log-likelihood is:  (since ) 
i.e we effectively ignore the missing observations when estimating the log-likelihood of the data. 

\begin{figure}[t!]
	\centering
	\includegraphics[width=\linewidth]{./images/cfac_final.pdf}
	\caption{\small {\bf (Left Two Plots) } 
	Estimating Counterfactuals with \DKF:  The x-axis denotes the number of -month intervals after prescription of Metformin. 
		The y-axis denotes the proportion of patients (out of a test set size of ) who, after their first prescription of Metformin, experienced
	a high level of A1C. In each tuple of bar plots at every time step, the left aligned bar plots (green) represent the population that received diabetes
medication while the right aligned bar plots (red) represent the population that did not receive diabetes medication.
{\bf (Rightmost Plot) } Upper bound on negative-log likelihood for different DMMs trained on the medical data. (T) denotes ``transition'', (E) denotes ``emission'', (L) denotes ``linear'' and (NL) denotes ``non-linear''. 
}
	\label{fig:cfac}
\end{figure}
\textbf{The Effect of Anti-Diabetic Medications: } 
Since our cohort comprises diabetic patients, we ask a counterfactual question: what \emph{would 
have happened} to a patient had anti-diabetic drugs not been prescribed? Specifically we are interested in the patient's blood-sugar level as measured by the widely-used A1C blood-test.
We perform inference using held-out patient data leading up to the time  of first prescription of Metformin.
From the posterior mean, we perform ancestral sampling tracking two latent trajectories: 
(1) the factual: where we sample new latent states conditioned 
on the medication  the patient had actually received and 
(2) the counterfactual: where we sample conditioned on 
not receiving any drugs for all remaining timesteps (i.e  set to the zero-vector). 
We reconstruct the patient observations , threshold
the predicted values of A1C levels into high and low and visualize the average number of high A1C levels 
we observe among the synthetic patients in both scenarios. 
This is an example of performing do-calculus \cite{pearl2009causality} in order to estimate model-based counterfactual effects.

The results are shown in Fig. \ref{fig:cfac}. We see the model learns that, on average, patients who were prescribed anti-diabetic medication
had more controlled levels of A1C than patients who did not receive any medication.  
Despite being an aggregate effect, this is interesting because it is a phenomenon that coincides with our intuition
but was confirmed by the model in an entirely unsupervised manner. Note that in our dataset, most diabetic patients are indeed prescribed anti-diabetic medications, making the counterfactual prediction harder. The ability of this model to answer such
queries opens up possibilities into building personalized neural models of healthcare. 
Samples from the learned generative model
and implementation details may be found in the supplement. 

 \section{Discussion}

We introduce a general algorithm for scalable learning in a rich family of latent variable models for time-series data. The underlying methodological principle we propose is to build the inference network to mimic the posterior distribution (under the generative model).
The space complexity of our learning algorithm depends neither on the sequence
length  nor on the training set size , offering massive
savings compared to classical variational inference methods.

Here we propose and evaluate building variational inference networks to mimic
the structure of the true posterior distribution.  
Other structured variational approximations 
are also possible. For example, one could instead use an RNN 
from the past, conditioned on a summary statistic of the future,
during learning and inference.

Since we use RNNs only in the inference network, 
it should be possible to continue to increase their capacity and 
condition on different modalities that might be relevant to approximate 
posterior inference without worry of overfitting the data. 
Furthermore, this confers us the ability to easily model in the presence of missing data 
since the semantics of the \DMM render it easy to marginalize out unobserved data.
In contrast, in a (stochastic) RNN (bottom in Fig. \ref{fig:dkf}) it is much more difficult
to marginalize out unobserved data due to the dependence of the intermediate hidden states on the previous
input. Indeed this allowed us to develop a principled application of the learning algorithm to
modeling longitudinal patient data in EHR data and 
inferring treatment effect.  
 \section*{Acknowledgements}
The Tesla K40s used for this research were donated by the NVIDIA 
Corporation. The authors gratefully acknowledge support by the DARPA Probabilistic Programming for Advancing
Machine Learning (PPAML) Program under AFRL prime contract
no. FA8750-14-C-0005, ONR \#N00014-13-1-0646, a NSF CAREER award
\#1350965, and Independence Blue Cross. We thank David Albers, Kyunghyun Cho, Yacine Jernite, Eduardo Sontag and anonymous reviewers
for their valuable feedback and comments. 
 
\bibliographystyle{aaai}
\bibliography{refs}

\clearpage
\appendix
\centerline{\large \bf Appendix}
\section{Lower Bound on the Likelihood of data\label{appsec:lb_likelihood}}

We can derive the bound on the likelihood  as follows:

In the following we omit the dependence of  on , and omit the subscript .
We can show that the  divergence between the approximation to the posterior and the prior simplifies as:


For evaluating the marginal likelihood on the test set, we can use the following Monte-Carlo estimate:

This may be derived in a manner akin to the one depicted in Appendix E \citep{rezende2014stochastic} or Appendix D \citep{kingma2013auto}.

The log likelihood on the test set is computed using: 

Eq. \ref{eqn:test_ll_logsum} may be computed in a numerically stable manner using the log-sum-exp trick.  
 \section{KL divergence between Prior and Posterior\label{appsec:kldiv}}

Maximum likelihood learning requires us to compute:


The KL divergence between two multivariate Gaussians ,  with respective means and covariances  can be written as:

The choice of  and  is suggestive. using Eq. \ref{eqn:KLdiv_factorized_app} \& \ref{eqn:KLdiv_multivar}, 
we can derive a closed form for the KL divergence between  and .
 are the outputs of the variational model. Our functional form for  is based on our generative and can
be summarized as: 


Here,  is assumed to be a learned diagonal matrix and  a scalar parameter.  

\textbf{Term (a)}
For , we have:


For , we have:


\textbf{Term (b)}
For , we have:


For , we have: 


\textbf{Term (c)}
For , we have:


For , we have:


Rewriting Eq. \ref{eqn:KLdiv_factorized_app} using Eqns. \ref{eqn:logdet_1}, \ref{eqn:logdet_t}, \ref{eqn:trace_1}, \ref{eqn:trace_t}, \ref{eqn:quad_form_1}, \ref{eqn:quad_form_t}, we get:



 
\section{Polyphonic Music Generation}

In the models we trained, the hidden dimension
was set to be  for the emission distribution and  in the transition function.
We typically used RNN sizes from one of  and 
a latent dimension of size . 

\textbf{Samples: } Fig. \ref{fig:poly_samples} depicts mean probabilities of samples
from the \DMM trained on JSB Chorales
\citep{boulanger2012modeling}. 
MP3 songs corresponding to two different samples
from the best \DKF model in the main paper learned on each of the four polyphonic data
sets may be found in the code repository.

\textbf{Experiments with NADE: }
We also experimented with Neural Autoregressive Density Estimators (NADE) \citep{larochelle2011neural} 
in the emission distribution for \DKF-Aug and denote it \DKF-Aug-NADE. 
In Table \ref{tab:polyphonic_nade}, we see that \DKF-Aug-NADE performs comparably to the state of the art RNN-NADE on 
JSB, Nottingham and Piano.  

\begin{table}[h]
	\centering
	\caption{
		\label{tab:polyphonic_nade}
		\small{\textbf{Experiments with NADE Emission: } Test negative log-likelihood (lower is better) on Polyphonic Music Generation dataset.
			\textbf{Table Legend}: RNN-NADE \citep{boulanger2012modeling}}} 
\resizebox{.45\textwidth}{!}{
\begin{tabular}{ccccc}
    \toprule
    ~Methods       & JSB  & Nottingham & Piano & Musedata\\
    \midrule
    \DKF-Aug.-NADE  &  \begin{tabular}{c} 5.118 \2.347)\\ \{2.364\} \end{tabular}
    & \begin{tabular}{c} 7.048 \7.099)\\ \{7.361\} \end{tabular}
    & \begin{tabular}{c} 6.049 \\ (6.115)\\ \{5.247\} \end{tabular}\\
    \midrule
    RNN-NADE & 5.19 & 2.31 & 7.05 & 5.60\\ 
    \bottomrule
    \end{tabular}
    }
\end{table}



\begin{figure}[h]
	\centering
\begin{subfigure}[b]{0.23\textwidth}
	\includegraphics[width=\textwidth,keepaspectratio]{./images/polyphonic/samples-6.pdf}
	\caption{Sample 1}
	\label{fig:jsb-s1}
\end{subfigure}~
\begin{subfigure}[b]{0.23\textwidth}
	\includegraphics[width=\textwidth,keepaspectratio]{./images/polyphonic/samples-4.pdf}
	\caption{Sample 2}
	\label{fig:jsb-s2}
\end{subfigure}
\caption{Two samples from the \DKF trained on JSB Chorales}
\label{fig:poly_samples}
\end{figure} 

\newpage
\section{Experimental Results on Synthetic Data}

\textbf{Experimental Setup: } We used an RNN size of  in the inference networks used for the synthetic experiments. 

\textbf{Linear SSMs :} Fig. \ref{fig:synthetic9-valid} (N=500, T=25) depicts the performance of inference networks using the same setup as in the main paper, only now using held out data to evaluate the RMSE and the upper bound. 
We find that the results echo those in the training set, and that on unseen data points, 
the inference networks, particularly the structured ones, are capable of generalizing compiled inference. 
\begin{figure}[h]
	\includegraphics[width=0.4\textwidth,keepaspectratio]{./images/synthetic/synthetic9-valid-bound.pdf}
	\caption{\textbf{Inference in a Linear SSM on Held-out Data: } Performance of inference networks on held-out data using a generative model with Linear Emission and Linear Transition (same setup as main paper)}
	\label{fig:synthetic9-valid}
\end{figure}

\begin{figure}[h]
	\centering
\begin{subfigure}[b]{0.45\textwidth}
	\includegraphics[width=\textwidth,keepaspectratio]{./images/synthetic/synthetic10-train-bound.pdf}
	\caption{Performance on training data}
	\label{fig:synthetic10-train}
\end{subfigure}
\begin{subfigure}[b]{0.45\textwidth}
	\includegraphics[width=\textwidth,keepaspectratio]{./images/synthetic/synthetic10-valid-bound.pdf}
	\caption{Performance on held-out data}
	\label{fig:synthetic10-valid}
\end{subfigure}
\caption{\textbf{Inference in a Non-linear SSM: } Performance of inference networks trained with data from a Linear Emission and Non-linear Transition SSM}
\label{fig:synth-non-linear}
\end{figure} 

\textbf{Non-linear SSMs :} Fig. \ref{fig:synth-non-linear} considers learning inference networks on a synthetic 
non-linear dynamical system (). 
We find once again that inference networks that match the posterior realize
faster convergence and better training (and validation) accuracy. 
 
\begin{figure}[h!]
	\includegraphics[width=0.45\textwidth,keepaspectratio]{./images/synthetic/synthetic10-vis.pdf}
	\caption{\textbf{Inference on Non-linear Synthetic Data: } Visualizing inference on training data. Generative Models: (a) Linear Emission and Non-linear Transition 
	 denotes the latent variable that generated the observation.  denotes the true data. We compare against the results obtained by a smoothed Unscented Kalman Filter (UKF) \citep{wan2000unscented}. 
	The column denoted ``Observations" denotes the result of applying the
emission function of the respective generative model on the posterior estimates shown in the column ``Latent Space".
The shaded areas surrounding each curve  denotes  for each plot.
}
\label{fig:synth-reconstructions}
\end{figure} 

\textbf{Visualizing Inference: } In Fig. \ref{fig:synth-reconstructions} 
we visualize the posterior
estimates obtained by the inference network. 
We run posterior inference on the training set  times and take the empirical
expectation of the posterior means and covariances of each method. 
We compare posterior estimates with those obtained by a smoothed 
Unscented Kalman Filter (UKF) \cite{wan2000unscented}. 
 \section{Generative Models of Medical Data}

In this section, we detail some implementation details and visualize samples from the generative model trained on patient data. 

\textbf{Marginalizing out Missing Data: } We describe the method we use 
to implement the marginalization operation. The main paper notes that 
marginalizing out observations in the \DMM corresponds to ignoring absent observations during learning. 
We track indicators denoting whether A1C values and Glucose values were observed in the data. These are 
used as markers of missingness. During batch learning, at every time-step , we obtain a matrix  of size batch-size  48, where  is the dimensionality of the observations,
comprising the log-likelihoods of every dimension for patients in the batch. We multiply this with a matrix of .  has the same dimensions as 
and has a  if the patient's A1C value was observed and a  otherwise. For dimensions that are never missing,  is always . 

\textbf{Sampling a Patient: }
We visualize samples from the \DMM trained on medical data in Fig. \ref{fig:pat_samples}
The model captures correlations 
within timesteps as well as variations in A1C level and Glucose level across timesteps. 
It also captures rare occurrences of comorbidities found amongst diabetic patients. 

\onecolumn
\begin{figure}[h]
	\includegraphics[width=0.8\linewidth]{./images/Patient3.pdf}
	\includegraphics[width=0.8\linewidth]{./images/Patient4.pdf}
	\caption{\small
		\textbf{Generated Samples} Samples of a patient from the model, including the most important observations. The x-axis denotes time and the y-axis denotes the observations. The intensity of the color denotes its value between zero and one}
	\label{fig:pat_samples}
\end{figure}
\twocolumn
 



\end{document}
