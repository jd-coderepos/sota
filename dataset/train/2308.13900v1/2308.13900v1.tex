\pdfoutput=1
\documentclass{article}


\PassOptionsToPackage{round}{natbib}
\usepackage[preprint]{neurips_2023}











\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    


\usepackage{cellspace}
\setlength\cellspacetoplimit{1.5ex}
\setlength\cellspacebottomlimit{1.5ex}

\usepackage{amsfonts}
\usepackage{physics} 
\usepackage[protrusion=true,tracking=true,kerning=true,expansion,final]{microtype}   \usepackage{booktabs} \usepackage{subcaption}
\usepackage{nicefrac} \usepackage{cancel}

\usepackage{xspace}
\usepackage{upref}
\usepackage{textcomp}
\usepackage[T1]{fontenc}


\usepackage{array}
\usepackage{amssymb}
\usepackage{gensymb}
\usepackage{bbding}
\usepackage{mathtools}
\usepackage{centernot} \usepackage{bigints}
\usepackage{dsfont} \usepackage{esint} \usepackage{multirow}
\usepackage{enumitem}
\usepackage{graphbox} \usepackage{xcolor}
\usepackage{dsfont}
\usepackage{bbm}

\usepackage[ruled,linesnumbered]{algorithm2e}



\usepackage{varioref} \usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}


\usepackage[capitalise]{cleveref} 
\crefname{appsec}{appendix}{appendices}
\Crefname{appsec}{Appendix}{Appendices}

\usepackage{url}


\newcommand{\yrcite}[1]{\citeyearpar{#1}}
\renewcommand{\cite}[1]{\citep{#1}}
\definecolor{mydarkblue}{rgb}{0,0.08,0.45}
\definecolor{urlcolor}{rgb}{0,.145,.698}
\definecolor{linkcolor}{rgb}{.71,0.21,0.01}
\hypersetup{ pdftitle={},  pdfauthor={},
	pdfsubject={}, pdfkeywords={},
	pdfborder=0 0 0,   
    pdfpagemode=UseOutlines,
	breaklinks=true, colorlinks=true,
	bookmarksnumbered=true,
	urlcolor=urlcolor,
	linkcolor=linkcolor,
	citecolor=mydarkblue,
	filecolor=mydarkblue,
	pdfview=FitH}

\renewcommand*{\backref}[1]{} \renewcommand*{\backrefalt}[4]{\ifcase #1 \or
	(cited on p. #2)\else
	(cited on pp. #2)\fi
}
\makeatletter
\renewcommand{\@biblabel}[1]{#1.}
\makeatother

\crefname{algorithm}{Algorithm}{Algorithms}
\usepackage{listings}

\hyphenpenalty=1200

\tolerance=1000
\hbadness=2000
\vbadness=\maxdimen
\sloppy 

\renewcommand{\topfraction}{0.95}
\renewcommand{\textfraction}{.05}
\renewcommand{\bottomfraction}{.95}
\renewcommand{\dbltopfraction}{.95}
\renewcommand{\floatpagefraction}{0.98}
\renewcommand{\dblfloatpagefraction}{0.95}
\setcounter{dbltopnumber}{2}  
\setcounter{topnumber}{4}  
\setcounter{bottomnumber}{4} 
\setcounter{totalnumber}{8}

\usepackage{bm}
\newcommand{\eb}[1]{{\scriptsize\,\,#1}}


\newcommand\cb[1]{\textcolor{blue}{(\textbf{Chaim}: #1)}} \newcommand\orl[1]{\textcolor{purple}{(\textbf{OrL}: #1)}}
\newcommand\ez[1]{\textcolor{brown}{(\textbf{Evgenii}: #1)}} \newcommand\mk[1]{\textcolor{teal}{(\textbf{Moshe}: #1)}} \newcommand\sk[1]{\textcolor{red}{(\textbf{Shai}: #1)}} \newcommand{\methodname}{S4MC}
\title{Semi-Supervised Semantic Segmentation via Marginal Contextual Information}
\newcommand\changes[1]{\textcolor{red}{#1}}





 
\author{Moshe Kimhi, Shai Kimhi, Evgenii Zheltonozhskii, Or Litany, and  Chaim Baskin \\
{ Technion -- Israel Institute of Technology, NVIDIA}
\\
{\texttt{\{\href{mailto:moshekimhi@campus.technion.ac.il}{moshekimhi},\href{mailto:shai.kimhi@campus.technion.ac.il}{shai.kimhi},\href{mailto:evgeniizh@campus.technion.ac.il}{evgeniizh}\}@campus.technion.ac.il}}\\ 
{\texttt{\{\href{mailto:orlitany@technion.ac.il}{orlitany},\href{mailto:chaimbaskin@technion.ac.il}{chaimbaskin}\}@technion.ac.il}}\
}



\begin{document}


\maketitle


\begin{abstract}
We present a novel confidence refinement scheme that enhances pseudo-labels in semi-supervised semantic segmentation. Unlike current leading methods, which filter pixels with low-confidence predictions in isolation, our approach leverages the spatial correlation of labels in segmentation maps by grouping neighboring pixels and considering their pseudo-labels collectively. With this contextual information, our method, named \methodname{}, increases the amount of unlabeled data used during training while maintaining the quality of the pseudo-labels, all with negligible computational overhead. Through extensive experiments on standard benchmarks, we demonstrate that \methodname{} outperforms existing state-of-the-art semi-supervised learning approaches, offering a promising solution for reducing the cost of acquiring dense annotations. For example, \methodname{} achieves a  1.29 mIoU improvement over the prior state-of-the-art method on PASCAL VOC 12 with 366 annotated images. The code to reproduce our experiments is available at \url{https://s4mcontext.github.io/}.
\end{abstract}


\section{Introduction}\label{sec:intro}

Supervised learning has been the driving force behind advancements in modern computer vision, including classification~\cite{krizhevsky2012alexnet,dai2021coatnet}, object detection~\cite{girshick2015rcnn,zong2022detrs}, and segmentation~\cite{zagoruyko2016multipath,deeplab,li2022mask,kirillov2023segment}. 
However, it requires extensive amounts of labeled data, which can be costly and time-consuming to obtain. In many practical scenarios, there is no shortage of available data, but only a fraction can be labeled due to resource constraints. This challenge has led to the development of semi-supervised learning~\citep[SSL;][]{rasmus2015semisupervised,berthelot2019mixmatch,sohn2020fixmatch,yang2022ccssl}, a methodology that leverages both labeled and unlabeled data for model training.

This paper focuses on applying SSL to semantic segmentation, which has applications in various areas such as perception for autonomous vehicles~\cite{bartolomei2020semanticpalnning}, mapping~\cite{etten2018spacenet} and agriculture~\cite{milioto2018precisionagriculture}. SSL is particularly appealing for segmentation tasks, as manual labeling can be prohibitively expensive.

A widely adopted approach for SSL is pseudo-labeling~\cite{lee2013pseudolabel,Arazo2020PseudoLabelingAC}. This technique dynamically assigns supervision targets to unlabeled data during training based on the model's predictions.  To generate a meaningful training signal, it is essential to adapt the predictions before integrating them into the learning process. Several techniques have been proposed, such as using a teacher network to generate supervision to a student network~\cite{hinton2015distilling}. The teacher network can be made more powerful during training by applying a moving average to the student network's weights~\cite{tarvainen2017meanteacher}. Additionally, the teacher may undergo weaker augmentations than the student~\cite{berthelot2019mixmatch}, simplifying the teacher's task.

\begin{figure}
	\centering	\includegraphics[width=0.8\linewidth]{figures/teaser.jpg} 
 \vspace{-.1cm}
	\caption{
 \textbf{Confidence refinement.} \textbf{Left:} pseudo-labels generated by the teacher network without refinement. \textbf{Middle:} pseudo-labels obtained from the same model after refinement with marginal contextual information. \textbf{Right} \textbf{Top:} predicted probabilities of the top two classes of the pixel highlighted by the red square before, and \textbf{Bottom:}  after refinement. \methodname{} allows additional correct pseudo labels to propagate.}
 \vspace{-0.5cm}
	\label{fig:teaser}
\end{figure}

However, pseudo-labeling is intrinsically susceptible to confirmation bias, which tends to reinforce the model predictions instead of improving the student model. Mitigating confirmation bias becomes particularly important when dealing with erroneous predictions made by the teacher network.

Confidence-based filtering is a popular technique to address this issue ~\cite{sohn2020fixmatch}. This approach assigns pseudo-labels only when the model's confidence surpasses a specified threshold, thereby reducing the number of incorrect pseudo-labels. Though simple, this strategy was proven effective and inspired multiple improvements in  semi-supervised classification~\cite{zhang2021flexmatch,ups}, segmentation~\cite{wang2022semi}, and object detection in images~\cite{sohn2020simple,liu2021unbiased} and 3D scenes~\cite{zhao2020sess,wang20203dioumatch}. 
However, the strict filtering of the supervision signal leads to extended training periods and, potentially, to overfitting when the labeled instances used are insufficient to represent the entire sample distribution. Lowering the threshold would allow for higher training volumes at the cost of reduced quality, further hindering the performance~\cite{sohn2020fixmatch}.

In response to these challenges, we introduce a novel confidence refinement scheme for the teacher network predictions in segmentation tasks designed to increase the availability of pseudo-labels without sacrificing their accuracy. Drawing on the observation that labels in segmentation maps exhibit strong spatial correlation, we propose to group neighboring pixels and collectively consider their pseudo-labels.  When considering pixels in spatial groups, we asses the event-union probability, which is the probability that at least one pixel belongs to a given class. We assign a pseudo-label if this probability is sufficiently larger than the event-union probability of any other class. By taking context into account, our approach \emph{Semi-Supervised Semantic Segmentation via Marginal Contextual Information} (\methodname{}), enables a relaxed filtering criterion which increases the number of unlabeled pixels utilized for learning while maintaining high-quality labeling, as demonstrated in \cref{fig:teaser}. 


We evaluated \methodname{} on multiple semi-supervised segmentation benchmarks. \methodname{} achieves significant improvements in performance over previous state-of-the-art methods. In particular, we observed an increase of \textbf{+1.29 mIoU} on PASCAL VOC 12~\cite{voc} using 366 annotated images and an increase of \textbf{+1.01 mIoU} on Cityscapes~\cite{cityscapes} using only 186 annotated images. These findings highlight the effectiveness of \methodname{} in producing high-quality segmentation results with minimal labeled data. \section{Related Work}

\subsection{Semi-Supervised Learning}
Pseudo-labeling \cite{lee2013pseudolabel} is a popular and effective technique in SSL, where labels are assigned to unlabeled data based on model predictions. To make the most of these labels during training, it is essential to refine them~\cite{laine2016temporalensembling, berthelot2019mixmatch,berthelot2019remixmatch,xie2020uda}. One way to achieve this is through consistency regularization \cite{laine2016temporalensembling,tarvainen2017meanteacher, miyato2018vat}, which ensures consistent predictions between different views of the unlabeled data. Alternatively, a teacher model can be used to obtain pseudo-labels, which are then used to train a student model. To ensure that the pseudo-labels are helpful, the temperature of the prediction \citep[soft pseudo-labels;][]{berthelot2019mixmatch} can be increased, or the label can be assigned to samples with high confidence \citep[hard pseudo-labels;][]{xie2020uda,sohn2020fixmatch, zhang2021flexmatch}. In this work, we follow the hard pseudo-label assignment approach and improve upon previous methods by proposing a confidence refinement scheme.

\subsection{Semi-Supervised Semantic Segmentation}
In semantic segmentation, most SSL methods rely on consistency regularization and developing augmentation strategies compatible with segmentation tasks \cite{french2019semi,gct,cps,pc2seg,xu2022semisupervised}. Given the uneven distribution of labels typically encountered in segmentation maps, techniques such as adaptive sampling, augmentation, and loss re-weighting are commonly employed \cite{ael}. Feature perturbations (FP) on unlabeled data \cite{cct, pseudoseg, psmt, unimatch} are also used to enhance consistency and the virtual adversarial training \cite{psmt}. Curriculum learning strategies that incrementally increase the proportion of data used over time are beneficial in exploiting more unlabeled data \cite{st++, wang2022semi}. A recent approach introduced by \citet{wang2022semi} used \textit{unreliable} predictions by employing contrastive loss with the least confident classes predicted by the model. Unimatch \cite{unimatch} combined SSL~\citep{sohn2020fixmatch} with several self-supervision signals, i.e., two strong augmentations and one more with FP, obtained good results without complex losses or class-level heuristics.
 However, most existing works primarily focus on individual pixel label predictions. In contrast, we delve into the contextual information offered by spatial predictions on unlabeled data.

\subsection{Contextual Information}\label{sec:context}
Contextual information encompasses environmental cues that assist in interpreting and extracting meaningful insights from visual perception \cite{Toussaint1978TheUO,Elliman1990ARO}. Incorporating spatial context explicitly has been proven beneficial in segmentation tasks, for example, by encouraging smoothness like in the Conditional Random Fields  method~\cite{deeplab} and attention mechanisms \cite{vaswani2017attention,dosovitskiy2021image,wang2020eca}. Combating dependence on context has shown to be helpful by~\citet{nekrasov2021mix3d}. This work uses the context from neighboring pixel predictions to enhance pseudo-label propagation.  


 \section{Method}
This section describes the proposed method using the teacher--student paradigm. Adjustments from teacher--student consistency to weak--strong image-level consistency are described in \cref{apdx:weak_strong_consistency}.
\subsection{Overview}
\begin{figure*}
	\centering
	\includegraphics[width=\textwidth]{figures/method_new.pdf} 
	\caption{\textbf{Left:} \methodname{} employs a teacher-student paradigm for semi-supervised segmentation. Labeled images are used to supervise the student network directly; both teacher and student networks process unlabeled images. Predictions from the teacher network are refined and used to evaluate the margin value, which is then thresholded to produce pseudo-labels that guide the student network. The threshold, denoted as , is dynamically adjusted based on the teacher network's predictions. \textbf{Right:} Our confidence refinement module exploits neighboring pixels to adjust per-class predictions, as detailed in \cref{sec:pseudo-labels}. The class distribution of the pixel marked by the yellow circle on the left is changed. Before refinement, the margin surpasses the threshold and erroneously assigns the blue class (dog) as a pseudo-label. However, after refinement, the margin significantly reduces, thereby preventing the propagation of this error.}
 \vspace{-0.5cm}
	\label{fig:method}
\end{figure*}
In semi-supervised semantic segmentation, we are given a labeled training set of images , and an unlabeled set  sampled from the same distribution, i.e., .  Here,  are 2D tensors of shape , assigning a semantic label to each pixel of . 
We aim to train a neural network  to predict the semantic segmentation of unseen images sampled from .

We follow a teacher--student approach 
\cite{tarvainen2017meanteacher} and train two networks  and  that share the same architecture but update their parameters separately. The student network  is trained using supervision from the labeled samples and pseudo-labels created by the teacher's predictions for unlabeled ones. The teacher model  is updated as an exponential moving average (EMA) of the student weights.
 and  denote the predictions of the student and teacher models for the  sample, respectively. At each training step, a batch of  and  images is sampled from  and , respectively.
The optimization objective can be written as the following loss:

where  and  are the losses over the labeled and unlabeled data correspondingly,   is a hyperparameter controlling their relative weight, and  is the pseudo-label for the -th unlabeled image. 
Not every pixel of  has a corresponding label or pseudo-label, and  and  denote the number of pixels with label and assigned pseudo-label in the image batch, respectively.


\subsubsection{Pseudo-label Propagation} 
For a given image , we denote by  the pixel in the -th row and -th column.
We adopt a thresholding-based criterion inspired by FixMatch~\cite{sohn2020fixmatch}. By establishing a score, denoted as , which is based on the class distribution predicted by the teacher network, we assign a pseudo-label to a pixel if its score exceeds a threshold :

where  is the pixel probability of class . A commonly used score is given by  .
However, we found that using a pixel-wise margin, similar to scores proposed by \citet{Scheffer2001ActiveHM} and \citet{Shin2021AllYN}, produces more stable results. This approach calculates the margin as the difference between the highest and the second-highest values of the probability vector:

where  denotes the vector's second highest value.

\subsubsection{Dynamic Partition Adjustment (DPA)}\label{sec:DPA} Following \citet{wang2022semi}, we use a decaying threshold . DPA replaces the fixed threshold with a quantile-based threshold that decreases with time. At each iteration, we set  as the -th quantile of  over all pixels of all images in the batch. We use linearly decreasing :

As the model predictions improve with each iteration, gradually lowering the threshold increases the number of propagated pseudo-labels without compromising their quality.

\subsection{Marginal Contextual Information}\label{sec:s4n}
Utilizing contextual information (\cref{sec:context}), we look at surrounding predictions (predictions on  neighboring pixels) to refine the semantic map at each pixel. We introduce the concept of ``Marginal Contextual Information,'' which involves integrating additional information to enhance predictions across all classes. At the same time, reliability-based pseudo-label methods focus on the dominant class only \cite{sohn2020fixmatch, wang2023freematch}.
\cref{sec:pseudo-labels} describes our confidence refinement, followed by our thresholding strategy and a description of \methodname{} methodology. 

\subsubsection{Confidence Margin Refinement}
\label{sec:pseudo-labels}

We refine the predicted pseudo-label of each pixel by considering the predictions of its neighboring pixels. Given a pixel  with a corresponding per-class prediction , we examine neighboring pixels  within an  pixel neighborhood surrounding it. We then calculate the probability that at least one of the two pixels belongs to class :

where  denote the joint probability of both  and  belonging to the same class . 

While the model does not predict joint probabilities, it is reasonable to assume a non-negative correlation between the probabilities of neighboring pixels. This is largely due to the nature of segmentation maps, which are typically piecewise constant. Consequently, any information regarding the model's prediction of neighboring pixels belonging to a specific class should not lead to a reduction in the posterior probability of the given pixel also falling into that class. The joint probability can thus be bounded from below by assuming independence: . By substituting this into \cref{eqn:union}, we obtain an upper bound for the event union probability:

This formulation allows us to filter out confidence margins that do not exceed the threshold.

For each class , we select the neighbor with the maximal information gain using \cref{eqn:union2}:

Computing the event union over all classes employs neighboring predictions to amplify differences in ambiguous cases.
Similarly, this prediction refinement prevents the creation of over-confident predictions not supported by additional spatial evidence and helps reduce confirmation bias. The refinement is visualized in \cref{fig:teaser}. In our experiments, we used a neighborhood size of .
To determine whether the incorporation of contextual information could be enhanced with larger neighborhoods, we conducted an ablation study focusing on the neighborhood size and the neighbor selection criterion, as detailed in \cref{tab:abalation_neigbhors}. For larger neighborhoods, we decrease the probability contribution of the neighboring pixels with a distance-dependent factor:
 
where  is a spatial weighting function.
Empirically, contextual information refinement affects mainly the most probable one or two classes.
This aligns well with our choice to use the margin confidence (\ref{eqn:kappa_margin}).

Considering more than two events (more than one neighbor), one can use the formulation for three or four event-union. In practice, we calculate it iteratively, starting with two event-union defined by \cref{eq:refined}, using it as , finding the next desired event using \cref{eq:choosing_neigbhor} with the remaining neighbors, and repeating the process.


\begin{figure*}
	\centering	\includegraphics[width=.9\textwidth]{figures/temporary_imgs/image2.png}
    \includegraphics[width=.9\textwidth]{figures/temporary_imgs/image1.png}
	\caption{\textbf{Qualitative results of S4MC.} The outputs of two trained models and the annotated ground truth. The segmentation map predicted by \methodname{} (\textit{Ours}) compared to the segmentation map using no refinement module (\textit{Baseline}) and  to the ground truth. \textit{Heat map} represents the uncertainty of the model as , showing a more confident prediction over certain areas, yielding to a smother segmentation maps (compared in the red boxes).}
 \label{fig:neigbors}
\end{figure*}

\subsubsection{Threshold Setting }\label{sec:th_set}
Setting a high threshold can prevent confirmation bias from the teacher model's ``beliefs'' transferring to the student model. However, this comes at the expense of learning from fewer examples, potentially resulting in a less comprehensive model. 
For determining the DPA threshold, we use the teacher predictions pre-refinement , but we filter values based on . Consequently, more pixels pass the threshold that remains unaffected. We set , i.e., 60\% of raw predictions pass the threshold at , as this value demonstrated superior performance in our experiments. An ablation study for  is provided in \cref{tab:abalation_alpha}. 



\subsection{Putting it All Together}

We perform semi-supervised learning for semantic segmentation by pseudo-labeling pixels using their neighbors' contextual information. 
Labeled images are only fed into the student model, producing the supervised loss (\cref{eq:suploss}). Unlabeled images are fed into the student and teacher models. We sort the margin based  (\cref{eqn:kappa_margin}) values of teacher predictions and set  as described in \cref{sec:th_set}.
The per-class teacher predictions are refined using the \textit{weighted union event} relaxation, as defined in \cref{eq:refined}. Pixels with  higher margin values than   are assigned with pseudo-labels as described in \cref{eqn:pseudolabel}, producing the unsupervised loss (\cref{eq:unsloss}). The entire pipeline is visualized in \cref{fig:method}.

The impact of \methodname{} is demonstrated in \cref{fig:quant_qual}, which compares the fraction of pixels that pass the threshold with and without refinement. Our method uses more unlabeled data during most of the training process (a), while the refinement ensures high-quality pseudo-labels (b). We further analyze the quality improvement by studying true positive (TP) and false positive (FP) rates, as shown in \cref{fig:tpfp} in the Appendix.  Qualitative results are presented in \cref{fig:neigbors}, where one can see both the confidence heatmap and the pseudo-labels with and without the impact of \methodname{}. \section{Experiments}\label{sec:experiments}

This section presents our experimental results. The setup for the different datasets and partition protocols is detailed in \cref{sec:setup}. \cref{sec:compare} compares our method against existing approaches and \cref{sec:ablation} provides the ablation study. Further implementation details are given in \cref{appendix:implementation}.


\begin{figure*}
    \centering
    \begin{subfigure}{0.49\linewidth}
    \includegraphics[width=\textwidth]{figures/pass.pdf} 
        \caption{\textbf{Data fraction that passes the threshold}. The baseline model has a fixed percentage, as it is based on DPA. Our method increases the number of pixels assigned  pseudo-label, mostly in the early stage of the training when the model is under-confident.}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\linewidth}
 	\includegraphics[width=\textwidth]{figures/quality.pdf}
        \caption{\textbf{Fraction of correct pseudo-labels} the assigned pseudo-labels with the correct class divided by the total assigned pseudo-label. \methodname{} produces more quality pseudo-labels during the training process, most notably at the early stages.}
    \end{subfigure}
	\caption{Pseudo-label quantity and quality on PASCAL VOC 2012 \cite{voc} with 366 labeled images using our margin (\ref{eqn:kappa_margin}) confidence function.
 }
	\label{fig:quant_qual}
\end{figure*}

\begin{table*}
\centering
\caption{
Comparison between our method and prior art on the PASCAL VOC 2012 \texttt{val} on different partition protocols. the caption describes the share of the training set used as labeled data and, in parentheses, the actual number of labeled images. Larger improvement can be observed for partitions of extremely low annotated data, where other methods suffer from starvation due to poor teacher generalization. * denotes reproduced results,  denotes the use of UniMatch \cite{Yang2022RevisitingWC} without the use of feature perturbation.}
\label{tab:classic}
\resizebox{\linewidth}{!}{
\begin{tabular}{lccccc}
\toprule
\textbf{Method} & 
\textbf{1/16 (92)} & \textbf{1/8 (183)} & \textbf{1/4 (366)} & \textbf{1/2 (732)} & \textbf{Full (1464)} \\
\midrule


CutMix-Seg \cite{french2019semi} &
52.16 & 63.47 & 69.46 & 73.73 & 76.54 \\
ReCo \cite{reco} &
64.80 & 72.0 & 73.10 & 74.70 & - \\
ST++ \cite{st++} &
65.2 & 71.0 & 74.6 & 77.3 & 79.1 \\
UPL  \cite{wang2022semi} & 
67.98 & 69.15 & 73.66 & 76.16 & 79.49 \\
PS-MT \cite{psmt} &
65.8 & 69.6 & 76.6 & 78.4 & 80.0 \\
PCR \cite{xu2022semisupervised} &
70.06 & 74.71 & 77.16 & 78.49 & \underline{80.65} \\
FixMatch* \cite{Yang2022RevisitingWC} &
68.07 & 73.72 &76.38 & 77.97 & 79.97 \\
UniMatch* \cite{Yang2022RevisitingWC} &
\underline{73.75} & \underline{75.05} & \underline{77.7} & \underline{79.9} & 80.43 \\
\midrule
CutMix-Seg + \methodname{}  &
 70.96  & 
 71.69  &
 75.41  & 
 77.73 &
 80.58  \\ FixMatch + \methodname{} &
73.13  & 
74.72  & 
77.27  & 
79.07  & 
79.6 \\ 
 + \methodname{}  &
\textbf{74.72} & 
\textbf{75.21} & 
\textbf{79.09} & 
\textbf{80.12} & 
\textbf{81.56} \\ 




\bottomrule
\end{tabular}
}
\end{table*}

\subsection{Setup}\label{sec:setup}
\paragraph{Datasets}
In our experiments, we use PASCAL VOC 2012 \cite{voc} and Cityscapes \cite{cityscapes} datasets.

The PASCAL VOC dataset comprises 20 object classes (plus background). The dataset includes 2,913 annotated images, divided into a training set of 1,464 images and a validation set of 1,449 images. In addition, the dataset includes 9,118 coarsely annotated training images \cite{sbd}, in which only a subset of the pixels are labeled. Following previous research, we conducted two sets of experiments. The ''classic`` setup uses only the original training set \cite{wang2022semi,pseudoseg}, while the ''coarse`` setup uses all available data \cite{wang2022semi,cps,ael}.

The Cityscapes \cite{cityscapes} dataset includes urban scenes from 50 different cities with 30 classes, of which only 19 are typically used for evaluation \cite{deeplab,deeplabv3p}. Similarly to PASCAL, in addition to 2,975 training and 500 validation images, the dataset includes 19,998 coarsely annotated images, which we do not use in our experiment. 

\paragraph{Implementation details}\label{sec:model}
We implement \methodname{} on top of two framework variants: teacher-student \cite{tarvainen2017meanteacher,french2019semi} and consistency optimization~\cite{sohn2020fixmatch, unimatch}. Both use  DeepLabv3+ \cite{deeplabv3p} with a Imagenet-pre-trained \cite{ILSVRC15} ResNet-101 \cite{resnet}. For the teacher--student setup, the teacher parameters  are updated via an exponential moving average (EMA) of the student parameters \citep{tarvainen2017meanteacher}:

where  defines how close the teacher is to the student and  denotes the training iteration.  We used . 
For the consistency paradigm, the teaching branch uses weak augmentations and the student branch uses strong ones. 
Additional details are provided in \cref{appendix:implementation}.


\begin{table}\centering
\caption{
Comparison between our method and prior art on the 'coarse' PASCAL VOC 2012 \texttt{val} dataset under different partition protocols, using additional unlabeled data from \cite{sbd}.
We included the number of labeled images in parentheses for each partition ratio. As in \cref{tab:classic}, larger improvements are observed for partitions with less annotated data. * denotes reproduced results,  denotes the use of UniMatch \cite{Yang2022RevisitingWC} without the use of feature perturbation.}
\label{tab:blender}
\begin{tabular}{lcccc}
\toprule
\textbf{Method} & 
\textbf{1/16 (662)} & \textbf{1/8 (1323)} & \textbf{1/4 (2646)} & \textbf{1/2 (5291)} \\
\midrule
CutMix-Seg \cite{french2019semi}& 
71.66 & 75.51 & 77.33 & 78.21  \\
AEL \cite{ael} & 
77.20 & 77.57 & 78.06 & 80.29\\
PS-MT \cite{psmt} &
75.5 & 78.2 & 78.7 & -\\
UPL \cite{wang2022semi} &
77.21 & 79.01 & 79.3 & 80.50\\
PCR \cite{xu2022semisupervised} &
\textbf{78.6} & \textbf{80.71} & \textbf{80.78} & \underline{80.91} \\
FixMatch* \cite{Yang2022RevisitingWC} &
74.35 & 76.33 & 76.87 & 77.46 \\

UniMatch* \cite{Yang2022RevisitingWC} &
76.6 & 77.0 & 77.32 & 77.9 \\


\midrule
\methodname{} + CutMix-Seg (Ours) &
\underline{78.49} &
\underline{79.67}  &
\underline{79.85}  & 
\textbf{81.11} \\

FixMatch + \methodname{} &
75.19  & 
76.56 &
77.11 &
78.07 \\ 

 + \methodname{} &
76.95  &  77.54  & 77.62  & 78.08 \\ 
\bottomrule
\end{tabular}
\end{table}


\begin{table}\centering
\caption{
Comparison between our method and prior art on the Cityscapes \texttt{val} dataset under different partition protocols. 
Labeled and unlabeled images are selected from the Cityscapes \texttt{training} dataset. For each partition protocol, the caption gives the share of the training set used as labeled data, in parentheses, the  number of labeled images.
* denotes reproduced results,  denotes the use of UniMatch \cite{Yang2022RevisitingWC} without the use of feature perturbation.}
\label{tab:city}
\begin{tabular}{lcccc}
\toprule
\textbf{Method} & 
\textbf{1/16 (186)} & \textbf{1/8 (372)} & \textbf{1/4 (744)} & \textbf{1/2 (1488)} \\
\midrule
CutMix-Seg \cite{french2019semi} & 
69.03 & 72.06 & 74.20 & 78.15 \\
AEL \cite{ael}  &
74.45 & 75.55 & 77.48 & 79.01 \\
UPL \cite{wang2022semi}  &
70.30 & 74.37  & 76.47  & 79.05  \\
PS-MT \cite{psmt}  &
- & 76.89  & 77.6  & 79.09 \\
PCR \cite{xu2022semisupervised} &
73.41 & 76.31 & 78.4 & 79.11 \\
FixMatch* \cite{Yang2022RevisitingWC} &
74.17 & 76.2 & 77.14 & 78.43  \\
UniMatch* \cite{Yang2022RevisitingWC} &
\underline{75.99} & 77.55 & 78.54 & 79.22  \\
\midrule
CutMix-Seg + \methodname{} &
75.03 & 
77.02 & 
78.78 & 
78.86 \\

FixMatch + \methodname{}  &
75.2 & 
\underline{77.61} & 
\underline{79.04}  & 
\underline{79.74}  \\

 + \methodname{} &
\textbf{77.0} & 
\textbf{77.78} & 
\textbf{79.52}  & 
\textbf{79.76}  \\
\bottomrule
\end{tabular}
\end{table}


\paragraph{Evaluation}
We compare \methodname{} with state-of-the-art methods and baselines under the common partition protocols -- using , , , and  of the training set as labeled data. For the 'classic' setting of the PASCAL experiment, we additionally compare using all the finely annotated images. 
We follow standard protocols and use mean Intersection over Union (mIoU) as our evaluation metric.
We use the data split published by \citet{wang2022semi} when available to ensure a fair comparison.
We use PASCAL VOC 2012 \texttt{val} with  partition for the ablation studies.



\subsection{Results}\label{sec:compare}

\paragraph{PASCAL VOC 2012.}
\cref{tab:classic} compares our method with state-of-the-art baselines on the PASCAL VOC 2012 dataset. While \cref{tab:blender} shows the comparison results on the PASCAL VOC 2012 dataset with additional coarsely annotated data from SBD \cite{sbd}.
In both setups, \methodname{} outperforms all the compared methods in standard partition protocols, both when using labels only for the original PASCAL VOC 12 dataset and when using SBD annotations as well. Qualitative results are shown in \cref{fig:neigbors}. 
As can be seen our refinement procedure aids in both adding falsely filtered pseudo-labels as well as removing erroneous ones. 

\paragraph{Cityscapes.} \cref{tab:city} presents the comparison results on the Cityscapes \texttt{val} dataset. 
\cref{tab:city} compares our method with other state-of-the-art methods on the Cityscapes \cite{cityscapes} dataset under various partition protocols.
\methodname{} outperforms the compared methods in most partitions, except for the  setting, and combined with the FixMatch scheme, \methodname{} outperforms compared approaches across all partitions.

\paragraph{Contextual information at inference.}
Given that our margin refinement scheme operates through prediction adjustments, we explored whether it could be employed at inference time to enhance performance. The results reveal a negligible improvement in the DeepLab-V3-plus model, from an 85.7 mIOU to 85.71. This underlines that the performance advantage of \methodname{} primarily derives from the adjusted margin, as the most confident class is rarely swapped. A heatmap of the prediction over several samples is presented in \cref{fig:neigbors} and \cref{fig:images_app}.

\subsection{Ablation Study}\label{sec:ablation}
We ablate different components of our method using the CutMix-Seg framework variant and evaluated using the Pascal VOC 12 dataset with a partition protocol of 1/4 labeled images.

\paragraph{Neighborhood size and neighbor selection criterion.} 
Our prediction refinement scheme employs event-union probability with neighboring pixels, which depends on the chosen neighbor to pair with the current pixel. To assess this, we tested varying neighborhood sizes () and criteria for selecting the neighboring pixel: (a) random, (b) maximal class probability, (c) minimal class probability, and (d) two neighbors, as described in \cref{sec:pseudo-labels}. We also compare with  neighborhood, which corresponds to not using \methodname{} at all. As shown in \cref{tab:abalation_neigbhors}, a small  neighborhood with one neighboring pixel of the highest class probability proved most efficient in our experiments.

\begin{table}
\caption{The effect of neighborhood size and neighbor selection criterion.}
\begin{subtable}[t]{0.53\textwidth}\centering
\caption{Neighborhood choice.}
\setlength{\tabcolsep}{10pt}
\label{tab:abalation_neigbhors}
\resizebox{\linewidth}{!}{
\begin{tabular}{lcccc}
\toprule
\multirow{2}{*}{\textbf{Selection criterion}} &\multicolumn{4}{c}{\textbf{Neighborhood size}   } \\ \cmidrule{2-5}
&  &   &   &     \\
\midrule
Random neighbor  & \multirow{ 4}{*}{69.46} &  73.25 & 71.1  & 70.41  \\
Max neighbor     &  &  \textbf{75.41} & 75.18 & 74.89  \\
Min neighbor     &  &  74.54 & 74.11 & 70.28  \\
Two max neighbors&  &  74.14 & 75.15 & 74.36  \\
\bottomrule
\end{tabular}
}
\end{subtable}
\hfill
\begin{subtable}[t]{0.43\textwidth}\centering
\caption{
 in \cref{eq:alpha}, which controls the initial proportion of confidence pixels 
}
\setlength{\tabcolsep}{10pt}
\label{tab:abalation_alpha}
\resizebox{\linewidth}{!}{
\begin{tabular}{ccccc}
\toprule
20\% &  30\% & 40\% & 50\% & 60\%   \\
\midrule
74.45 &73.85 &\textbf{75.41}  & 74.56 & 74.31 \\
\bottomrule
\end{tabular}
}
\end{subtable}
\vspace{-0.5cm}
\end{table}


We also examine the contribution of the proposed pseudo-label refinement (PLR) as well as DPA.
Results in \cref{tab:ablation_components} show that the PLR helps improve the mask mIoU by 1.09\%, while DPA alone harms the performance. This indicates that PLR helps semi-supervised learning mainly because it enforces more spatial dependence on the pseudo-labels.


\begin{table}
\centering
\caption{Ablation study on the different components of \methodname{} on top of FixMatch. \textbf{PLR} is the pseudo-label refinement module and \textbf{DPA} is dynamic partition adjustment.}
\setlength{\tabcolsep}{10pt}
\label{tab:ablation_components}
\begin{tabular}{ccc}
\toprule
\textbf{PLR} & \textbf{DPA} & \textbf{1/4}    \\
\midrule
  &  &   76.8 \\
\checkmark &  & 76.2   \\
 & \checkmark & 77.89   \\
  \checkmark & \checkmark & \textbf{78.07}   \\
\bottomrule
\end{tabular}
\end{table}


\paragraph{Threshold parameter tuning}
As outlined in \cref{sec:DPA}, we utilize a dynamic threshold that depends on an initial value, . In \cref{tab:abalation_alpha}, we examine the effect of different initial quantiles to establish this threshold. A smaller  would propagate too many errors, leading to significant confirmation bias. In contrast, a larger  would mask most of the data, resulting in insufficient label propagation, rendering the semi-supervised learning process lengthy and inefficient. We found that an  of 40\% yields the best performance. 


 \section{Conclusion}\label{sec:conclusion}
In this paper, we introduce \methodname{}, a novel approach for incorporating spatial contextual information in semi-supervised segmentation. This strategy refines confidence levels and enables us to leverage more unlabeled data. 
\methodname{} outperforms existing approaches and achieves state-of-the-art results on multiple popular benchmarks under various data partition protocols, such as  Cityscapes and Pascal VOC 12.
Despite its effectiveness in lowering the annotation requirement, there are several limitations to using \methodname{}. First, its reliance on event-union relaxation is applicable only in cases involving spatial coherency. As a result, using our framework for other dense prediction tasks would require an examination of this relaxation's applicability. Furthermore, our method uses a fixed-shape neighborhood without considering the object's structure. It would be interesting to investigate the use of segmented regions to define new neighborhoods; this is a future direction we plan to explore.  

\begin{ack}
We sincerely thank Amlan Kar for his invaluable feedback and deeply impactful discussions during the entire research process. Evgenii Zheltonozhskii is supported by the Adams Fellowships Program of the Israel Academy of Sciences and Humanities. Or Litany is a Taub fellow and is supported by the Azrieli Foundation Early Career Faculty Fellowship.
\end{ack}


\clearpage
{\small
\bibliographystyle{plainnat}
\bibliography{twosample}
}

\clearpage
\appendix{}


\renewcommand\thefigure{\thesection.\arabic{figure}} 
\renewcommand\thetable{\thesection.\arabic{table}} 
\renewcommand\theequation{\thesection.\arabic{equation}} 


\numberwithin{equation}{section}
\numberwithin{figure}{section}
\numberwithin{table}{section}


\setcounter{figure}{0}  
\setcounter{table}{0}
\setcounter{equation}{0}

\section{Limitations and Potential Negative Social Impacts}
\paragraph{Limitations.}
The constraint imposed by the spatial coherence assumption also restricts the applicability of this work to dense prediction tasks. Improving pseudo-labels' quality for overarching tasks such as classification might necessitate reliance on data distribution and the exploitation of inter-sample relationships. We are currently exploring this avenue of research.

\paragraph{Societal impact.}
Similar to most semi-supervised models, we utilize a small subset of annotated data, which can potentially introduce biases from the data into the model. Further, our PLR module assumes spatial coherence. While that holds for natural images, it may yield adverse effects in other domains, such as medical imaging. It is important to consider these potential impacts before choosing to use our proposed method.








\section{Pseudo-labels quality analysis}
\label{sec:quality_analysis}

The quality improvement and the quantity increase of pseudo-labels are shown in \cref{fig:quant_qual}.
Further analysis of the quality improvement of our method is demonstrated in \cref{fig:tpfp} by separating the \textit{true positive} and \textit{false positive}.

Within the initial phase of the learning process, the enhancement in the quality of pseudo-labels can be primarily attributed to the advancement in true positive labels. In our method, the refinement not only facilitates the inclusion of a larger number of pixels surpassing the threshold but also ensures that a significant majority of these pixels are of high quality.

As the learning process progresses, most improvements are obtained from a decrease in false positives pseudo-labels. This analysis shows that our method effectively minimizes the occurrence of incorrect pseudo-labeled, particularly when the threshold is set to a lower value. In other words, our method reduces confirmation bias from decaying the threshold as the learning process progresses.

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{figures/TPFP.pdf} 
	\caption{\textbf{Quality of pseudo-labels}, on PASCAL VOC 2012 \cite{voc} over training iterations. \cref{fig:quant_qual} separated to \textit{True positive} and \textit{False positive} analysis. \textit{True positive} are the bigger part of improvement at the early stage of the training process, while reduction of \textit{false positive} is the main contribution late in the training process}
	\label{fig:tpfp}
\end{figure}

\section{weak--strong consistency}
\label{apdx:weak_strong_consistency}
To adjust the method to image level consistency framework \cite{sohn2020fixmatch,zhang2021flexmatch,wang2023freematch}, we need to re-define the supervision branch. Recall that within the teacher-student framework, we denote  and  as the predictions made by the student and teacher models for input , where the teacher serves as the source for generating confidence-based pseudo-labels. In the context of image-level consistency, both branches differ by augmented versions ,  and  share identical weights . Here,  and  represent the weak and strong augmented renditions of the input , respectively. 
Following the aforementioned framework, it is the branch associated with weak augmentation that generates the pseudo-labels.


\section{Ablation studies}






\subsection{Confidence function alternatives}
\label{sec:Confidence}
In this paper, we introduce a confidence function to determine pseudo-label propagation.
We introduced  and mentioned other alternatives have been examined.

Here, we define several options for the confidence function.

The simplest option is to look at the probability of the dominant class,

which is commonly used to generate pseudo-labels.

The second alternative is negative entropy, defined as

Note that this is indeed a confidence function since high entropy corresponds to high uncertainty, and low entropy corresponds to high confidence.

The third option is for us to define the margin function \cite{Scheffer2001ActiveHM,Shin2021AllYN} as the difference between the first and second maximal values of the probability vector and also described in the main paper:

where  denotes the vector's second maximum value.
All alternatives are compared in \cref{tab:abalation_kappa}.

\begin{table}
\centering
\caption{Ablation study on the confidence function , over Pascal VOC 12 with partition protocols}
\setlength{\tabcolsep}{10pt}
\label{tab:abalation_kappa}
\begin{tabular}{cccc}
\toprule
\textbf{Function}    & \textbf{1/4 (366)} & \textbf{1/2 (732)} & \textbf{Full (1464)}    \\
\midrule
      & 74.29 & 76.16 & 79.49   \\
   &  75.18 & 77.55& 79.89   \\
 &  75.41 & 77.73 & 80.58   \\
\bottomrule
\end{tabular}
\end{table}

\cref{tab:abalation_kappa} studies the impact of different confidence functions on pseudo-label refinement.
We found that using a margin to describe confidence is a suitable way when there is a contradiction in smooth regions.




\subsection{Decomposition and analysis of Unimatch}\label{appendix:decompose_unimatch}
Unimatch \cite{unimatch} investigating the consistency and suggest to use FixMatch \cite{sohn2020fixmatch} and a strong baseline for semi-supervised semantic segmentation. Moreover, they provide analysis that shows that combining 3 students for each supervision signal, one feature level augmentation, feature perturbation, denoted by FP, and two strong augmentations, denoted by S1 and S2.
Fusing Unimatch and our method did not provide significant improvements, and we examined the contribution of different components of Unimatch.   
We measure the pixel-agreement as described in \cref{eq:refined} and showed that the feature perturbation branch has the same effect on pixel-agreement as \methodname{}.
\cref{fig:agreement} present the distribution of agreement using FixMatch (S1), DusPerb (S1,S2), Unimatch (S1, S2, FP) and S4MC (S1, S2).


\begin{figure*}
    \centering
    \begin{subfigure}{0.49\linewidth}
    \includegraphics[width=\textwidth]{figures/agreement.pdf} 
        \caption{The spatial agreement as we define in in \ref{eq:refined} compared between different variations of Unimatch and \methodname{}, on PASCAL VOC 12 dataset.}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\linewidth}
 	\includegraphics[width=\textwidth]{figures/time_agree.pdf} 
        \caption{The spatial agreement, compared between different variations of Unimatch \cite{unimatch} and S4MC, on PASCAL over time.}
    \end{subfigure}
\label{fig:agreement}
\end{figure*}

\section{Computational cost}

Let us denote the image size by  and the number of classes by C.

First, the predicted map of dimension  is stacked with the padded-shifted versions, creating a tensor of shape [n,H,W,C]. K top neighbors are picked via top-k operation and calculate the union event as presented in \cref{eq:refined}.
(The pseudo label refinement pytorch-like pseudo-code can be obtained in \cref{alg:unimatch} for  and  max neighbors.)


The overall space (memory) complexity of the calculation is , which is negligible considering all parameters and gradients of the model. Time complexity adds 3 tensor operations (stack, topk, and multiplication) over the  tensor, where the multiplication operates k times, which means . This is again negligible for any reasonable number of classes compared to tens of convolutional layers with hundreds of channels.


To verify that, we conducted a training time analysis comparing FixMatch and FixMatch + \methodname{} over PASCAL with 366 labeled examples, using distributed training with 8 Nvidia RTX 3090 GPUs.
FixMatch average epoch takes 28:15 minutes, and FixMatch + S4MC average epoch takes 28:18 minutes, an increase of about 0.2\% in runtime.





\begin{algorithm}[t]
\caption{\small{Pseudocode: Pseudo label refinement of S4MC, PyTorch-like style.}}
\label{alg:unimatch}
\definecolor{codeblue}{rgb}{0.25,0.5,0.5}
\lstset{
  backgroundcolor=\color{white},
  basicstyle=\fontsize{9pt}{9pt}\ttfamily\selectfont,
  columns=fullflexible,
  breaklines=true,
  captionpos=b,
  commentstyle=\fontsize{9pt}{9pt}\color{codeblue},
  keywordstyle=\fontsize{9pt}{9pt}
}
\begin{lstlisting}[language=python]
# X: predict prob of unlabeled data B x C x H x W 
# k: number of neigbors

#create neighborhood tensor 
neigborhood=[]
X = X.unsqueeze(1)
X = torch.nn.functional.pad(X, (1, 1, 1, 1, 0, 0, 0, 0))
for i,j in [(None,-2),(1,-1),(2,None)]:
    for k,l in [(None,-2),(1,-1),(2,None)]:
        if i==k and i==1:
            continue
        neighborhood.append(X[:,:,i:j, k:l])
neighborhood = torch.stack(neighborhood)

#pick k neighbors for union event 
ktop_neighbors,neigbor_idx=torch.topk(neighborhood, k=k,axis=0)
for nbr in ktop_neighbors:
    beta = torch.exp((-1/2) * neigbor_idx)
    X = X + beta*nbr - (X*nbr*beta)

\end{lstlisting}
\end{algorithm}



\section{Bounding the joint probability}\label{sec:joint_bound}
In this paper, we had the union event estimation with the independence assumption, defined as


In addition to the independence approximation, it is possible to estimate the unconditional expectation of two neighboring pixels belonging to the same class based on labeled data:


To avoid overestimating that could lead to overconfidence, we set  


That upper bound of joint probability ensures that the independence assumption does not underestimate the joint probability, preventing overestimating the union event probability.
Using \cref{eqn:joint_max} increase the mIOU by \textbf{0.22} on average, compared to non use of \methodname{} refinement, using 366 annotated images from PASCAL VOC 12 
Using only \cref{eqn:p2_joint} reduced the mIOU by \textbf{-14.11} compared to non-use of \methodname{} refinement and actually harmed the model capabilities to produce quality pseudo-labels.



\section{Implementation Details}\label{appendix:implementation}
All experiments were  conducted for 80 training epochs with the simple stochastic gradient descent (SGD) optimizer with a momentum of 0.9 and learning rate policy of .

For the student--teacher paradigm, we apply resize, crop, horizontal flip, GaussianBlur, and with probability of , we also apply Cutmix \cite{cutmix} on the unlabeled data.

For the consistency paradigm (\cite{sohn2020fixmatch,unimatch}) we apply resize, crop, and horizontal flip for weak and strong augmentations as well as  ColorJitter, RandomGrayscale, and Cutmix for strong augmentations.



For PASCAL VOC 2012  and the decoder only , the weight decay is set to  and all images are cropped to  and .

For Cityscapes, all parameters use , and the weight decay is set to . The learning rate decay parameter is set to
. Due to memory constraints, all images are cropped to  and .
All experiments are conducted on a machine with  Nvidia RTX A5000 GPUs.



\section{More visual results}\label{sec:visual_resutls}
We present in \cref{fig:images_app} an extension of \cref{fig:neigbors}, showing more instances from the unlabeled data and the corresponding pseudo-labeled with the baseline model and \methodname{}.

Our method can achieve more accurate predictions during the inference phase without any refinements. This results in the generation of more seamless and continuous predictions, which depict the spatial configuration of objects more accurately.

\begin{figure*}\label{fig:images_app}\centering
    \caption{\textbf{Example of refined pseudo-labels}, the structure is as in \cref{fig:neigbors}, and the numbers under the predictions show the pixel-wise accuracy of the prediction map.}
    \label{fig:my_label}
    \includegraphics[width=.95\textwidth]{figures/appendix_visualizations/total114.png}
\includegraphics[width=.95\textwidth]{figures/appendix_visualizations/total48.png}
\includegraphics[width=.95\textwidth]{figures/appendix_visualizations/total52.png}
\includegraphics[width=.95\textwidth]{figures/appendix_visualizations/total59.png}
\includegraphics[width=.95\textwidth]{figures/appendix_visualizations/total64.png}
\includegraphics[width=.95\textwidth]{figures/appendix_visualizations/total93.png}
\includegraphics[width=.95\textwidth]{figures/appendix_visualizations/total82.png}
\end{figure*}

\iffalse
\section{Additional experiments}
FixMatch has recently gained popularity as a framework for semi-supervised semantic segmentation \cite{Rabadan2022DenseFA}
We additionally compared our method with them, denoted by  a re-implementation that achieves better results than reported in the paper. All setups for different datasets and partition protocols are reported in \cref{tab:pascal_with_fix,tab:blender_fix,tab:city_fix}, similar to all experiments in the main paper. 


\begin{table*}[t]
\centering
\caption{
Comparison between our method and prior art on the PASCAL VOC 2012 \texttt{val} on different partition protocols. the caption describes the share of the training set used as labeled data and, in parentheses, the actual number of labeled images. Larger improvement can be observed for partitions of extremely low annotated data, where other methods suffer from starvation due to poor teacher generalization.}
\label{tab:pascal_with_fix}
\begin{tabular}{lccccc}
\toprule
\textbf{Method} & 
\textbf{1/16 (92)} & \textbf{1/8 (183)} & \textbf{1/4 (366)} & \textbf{1/2 (732)} & \textbf{Full (1464)} \\
\midrule
Supervised Only & 45.77 & 54.92 & 65.88 & 71.69 &72.50 \\
\midrule
CutMix-Seg \cite{french2019semi} & 52.16 & 63.47 & 69.46 & 73.73 & 76.54 \\
PseudoSeg \cite{pseudoseg} & 57.60 & 65.50 & 69.14 & 72.41 & 73.23 \\
PCSeg \cite{pc2seg} & 57.00 & 66.28 & 69.78 & 73.05 & 74.15 \\
CPS \cite{cps} & 64.10 & 67.40 & 71.70 & 75.90 & - \\
ReCo \cite{reco} & 64.80 & 72.0 & 73.10 & 74.70 & - \\
ST++ \cite{st++} & 65.2 & 71.0 & 74.6 & 77.3 & 79.1 \\
UPL  \cite{wang2022semi} & 67.98 & 69.15 & 73.66 & 76.16 & 79.49 \\
PS-MT \cite{psmt} & 65.8 & 69.6 & \underline{76.6} & 78.4 & 80.0 \\
FixMatch* \cite{Rabadan2022DenseFA} & 65.93 & \underline{72.72} & 75 & \underline{77.8} & 78.35 \\



\midrule
\methodname{} + CutMix-Seg (Ours) &
 \underline{70.96} & 71.69 & 75.41 & 77.73 & \underline{80.58} \\ 

\methodname{} + FixMatch (Ours) &
\textbf{74.32} & \textbf{75.62} & \textbf{77.84} & \textbf{79.72} & \textbf{81.51} \\ 

\bottomrule
\end{tabular}
\end{table*}
\begin{table}\centering
\caption{
Comparison between our method and prior art on the 'coarse' PASCAL VOC 2012 \texttt{val} dataset under different partition protocols, using additional unlabeled data from \cite{sbd}.
For each partition ratio, we included the number of labeled images in parentheses. As in \cref{tab:classic}, larger improvements are observed for partitions with less annotated data.
}
\label{tab:blender_fix}
\begin{tabular}{lcccc}
\toprule
\textbf{Method} & 
\textbf{1/16 (662)} & \textbf{1/8 (1323)} & \textbf{1/4 (2646)} & \textbf{1/2 (5291)} \\
\midrule
Supervised Only & 
67.87 & 71.55 & 75.80 & 77.13 \\
\midrule
CutMix-Seg \cite{french2019semi}& 
71.66 & 75.51 & 77.33 & 78.21  \\
CCT \cite{cct} & 
71.86 & 73.68 & 76.51 & 77.40 \\
GCT \cite{gct} &
70.90 & 73.29 & 76.66 & 77.98 \\
CPS \cite{cps} & 
74.48 & 76.44 & 77.68 & 78.64\\
AEL \cite{ael} & 
77.20 & 77.57 & 78.06 & 80.29\\
PS-MT \cite{psmt} &
75.5 & 78.2 & 78.7 & -\\
UPL \cite{wang2022semi} &
77.21 & 79.01 & 79.3 & 80.50\\
FixMatch* \cite{Rabadan2022DenseFA}  &
76.5 &  77.19  & 78.07  & 78.13 \\
\midrule
\methodname{} + CutMix-Seg (Ours) &
\underline{78.49}  &
\underline{79.67}  &
\underline{79.85}  & 
\underline{81.11} \\

\methodname{} + FixMatch (Ours) &
\textbf{80.77} & \textbf{81.9} & \textbf{82.3} & \textbf{83.3} \\ 
\bottomrule
\end{tabular}
\end{table}



\begin{table}\centering
\caption{
Comparison between our method and prior art on the Cityscapes \texttt{val} dataset under different partition protocols. 
Labeled and unlabeled images are selected from the Cityscapes \texttt{training} dataset. 
For each partition protocol, the caption gives the share of the training set used as labeled data, in parentheses, the  number of labeled images.
}
\label{tab:city_fix}
\begin{tabular}{lcccc}
\toprule
\textbf{Method} & 
\textbf{1/16 (186)} & \textbf{1/8 (372)} & \textbf{1/4 (744)} & \textbf{1/2 (1488)} \\
\midrule
Supervised Only & 
62.96 & 69.81 & 74.08 & 77.46 \\
\midrule
CutMix-Seg \cite{french2019semi} & 
69.03 & 72.06 & 74.20 & 78.15 \\
CCT \cite{cct} & 
69.32 & 74.12 & 75.99 & 78.10 \\
GCT \cite{gct} &
66.75 & 72.66 & 76.11 & 78.34 \\
CPS \cite{cps} &
69.78 & 74.31 & 74.58 & 76.81 \\
AEL \cite{ael}  &
74.45 & 75.55 & 77.48 & 79.01 \\
UPL \cite{wang2022semi}  &
70.30 & 74.37  & 76.47  & 79.05  \\
PS-MT \cite{psmt}  &
- & 76.89  & 77.6  & \underline{79.09} \\
FixMatch* \cite{Rabadan2022DenseFA}  &
72.6 &  76.15  & 76.93  & 78.22 \\





\midrule
\methodname{} + CutMix-Seg (Ours) &
\underline{75.03}  & 
\underline{77.02}   & 
\underline{78.78}  & 
78.86  \\
\methodname{} + FixMatch (Ours) &
\textbf{76.3}  & 
\textbf{78.25}   & 
\textbf{78.95}  & 
\textbf{79.13}  \\
\bottomrule
\end{tabular}
\end{table}
\fi
 

\end{document}