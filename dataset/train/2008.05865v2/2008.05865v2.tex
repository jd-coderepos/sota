\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{epstopdf}
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false,urlcolor=red,citecolor=cyan]{hyperref}
\usepackage{booktabs}
\usepackage{stfloats}
\usepackage{float}
\usepackage{multirow}
\usepackage{bbding}
\usepackage{bm}
\usepackage{amsfonts}

\newcommand{\hao}[1]{\textcolor{blue}{#1}}
\usepackage{caption}
\captionsetup{skip=0pt}

\usepackage{enumitem}
\setitemize{noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt}
\usepackage{flushend}


\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy 

\def\iccvPaperID{7709} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}




\begin{document}

\title{Deep Fusion Generative Adversarial Networks for Text-to-Image Synthesis}

\author{Ming Tao \ Hao Tang \ Songsong Wu \ Nicu Sebe \  Xiaoyuan Jing \ Fei Wu \ Bingkun Bao \\
Nanjing University of Posts and Telecommunications\quad
	University of Trento \\
	Guangdong University of Petrochemical Technology \quad
	Wuhan University
}


\maketitle


\begin{abstract}
Synthesizing high-quality realistic images from text descriptions is a challenging task. 
Almost all existing text-to-image Generative Adversarial Networks employ stacked architecture as the backbone.
They utilize cross-modal attention mechanisms to fuse text and image features, and introduce extra networks to ensure text-image semantic consistency.
In this work, we propose a much simpler, but more effective text-to-image model than previous works.
Corresponding to the above three limitations, we propose: 
1) a novel one-stage text-to-image backbone which is able to synthesize high-quality images directly by one pair of generator and discriminator,
2) a novel fusion module called deep text-image fusion block which deepens the text-image fusion process in generator,
3) a novel target-aware discriminator composed of matching-aware gradient penalty and one-way output which promotes the generator to synthesize more realistic and text-image semantic consistent images without introducing extra networks.
Compared with existing text-to-image models, our proposed method (i.e., DF-GAN) is simpler but more efficient to synthesize realistic and text-matching images and achieves better performance.
Extensive experiments on both Caltech-UCSD Birds 200 and COCO datasets demonstrate the superiority of the proposed model in comparison to state-of-the-art models. The source code and trained models are available at~\url{https://github.com/tobran/DF-GAN}.
\end{abstract}

\section{Introduction}
The last few years have witnessed the great success of Generative Adversarial Networks (GANs) \cite{goodfellow2014generative} for a variety of applications. 
Among them, text-to-image synthesis is one of the important applications of GANs. It aims to generate realistic and text-consistent images from given natural language descriptions. Due to its practical applications, text-to-image synthesis has become an active research area recently~\cite{yuan2019ckd, hong2018inferring, li2020exploring, gou2020segattngan, cheng2020rifegan, ramesh2021zero}.

\begin{figure}[t] \small
  \centering
  \includegraphics[width=\linewidth]{c_01.png}
  \caption{(a) Existing text-to-image models stack multiple generators and discriminators to generate high-resolution images. (b) Our proposed DF-GAN generates high-quality images directly and fuses the text and image features deeply by our deep text-image fusion blocks.}
  \label{fig1}
  \vspace{-0.4cm}
\end{figure}

Most recently proposed text-to-image synthesis models are based on Stacked GANs \cite{zhang2017stackgan, zhang2018stackgan} to generate high-resolution images, which employ concatenating and cross-modal attention to fuse text and image features~\cite{reed2016generative, zhang2017stackgan, zhang2018stackgan, xu2018attngan, zhu2019dm} and then introduce DAMSM loss \cite{xu2018attngan}, cycle consistency \cite{qiao2019mirrorgan}, and Siamese network \cite{yin2019semantics} to ensure the text-image semantic consistency by extra networks.

Although impressive results have been presented by previous work \cite{li2019controllable, qiao2019learn, qiao2019mirrorgan, zhu2019dm, yin2019semantics, li2019object,gou2020segattngan}, there still remain three problems. 
First, the entanglements between generators in stacked architecture makes the final refined images look like a simple combination of fuzzy shape and some details (see Figure~\ref{fig1}).
Second, introducing extra network to ensure text-image semantic consistency increase the training complexity and bring a large additional computational cost.
Third, simply concatenating text and image features can not make full use of text information, and cross-modal attention can only applied two times on  and  image features due to large computational cost. It makes the generator cannot fuse text and image features effectively.

To address these three issues, we propose a novel text-to-image generation method named as Deep Fusion Generative Adversarial Network (DF-GAN). 
For the first issue, we replace the stacked backbone with a one-stage backbone composed of hinge loss~\cite{zhang2019self} and residual networks~\cite{he2016deep}.
Based on this one-stage text-to-image backbone, our model can synthesize high-resolution images directly with better object shapes and realistic fine-grained details.

For the second issue, we propose a novel target-aware discriminator composed of Matching-Aware Gradient Penalty (MA-GP) and one-way output to ensure text-image semantic consistency. 
MA-GP is a regularization strategy on the discriminator. By pushing the real and matching input pairs to the place where the gradients of the discriminator loss function are zero, the generator will be promoted to synthesize more realistic and text-image semantic consistent images.
Simultaneously, MA-GP can stabilize the training process and ensure the text-image semantic consistency.
Moreover, we point out that the previous two-ways output is harmful to text-image semantic consistency and slows the convergence process of the generator under MA-GP.
To solve this problem, we replace it with a one-way output. 
One-way output can further promote the effectiveness of MA-GP and accelerate the convergence process of the generator.
Compared with previous methods, our model can synthesize images with higher quality and better text-image semantic consistence without introducing extra networks.

For the third issue, we propose a Deep text-image Fusion Block (DFBlock) to effectively fuse the text information into image features. 
The DFBlock consists of several Affine Transformations.
The Affine Transformation is a lightweight module which only manipulates the visual feature maps through channel-wise scaling and shifting operation.
Stacking multiple DFBlocks at all image scales deepens the text-image fusion process which promotes the generator to fuse text and image features effectively.
In addition, we concatenate the noise vector on the sentence vector and apply truncation operation on the noise, it converts a single, discrete sentence vector into a beam of sentence vectors to provide the generator with a stable and continuous text latent space.
We call this process as skip-z with truncation.

Overall, our contributions are summarized as follows:
\begin{itemize}[leftmargin=*]
\item We propose a novel one-stage text-to-image backbone which can synthesize high-resolution images with higher image quality.
\item We propose a novel target-aware discriminator composed of Matching-Aware Gradient Penalty (MA-GP) and one-way output, it significantly improves the image quality and text-image semantic consistency, and accelerate the convergence of generator without introducing extra networks. 
\item We propose a novel Deep text-image Fusion Block (DFBlock) which fuses text and image features more effectively and deeply.
\item We propose a novel method called the skip-z with truncation to provide the generator with a stable and better text latent space for synthesizing higher-quality images.
\item Extensive qualitative and quantitative experiments on two challenging datasets demonstrate that the proposed DF-GAN outperforms exiting state-of-the-art text-to-image models.
\end{itemize}

\section{Related Work}

Generative Adversarial Networks (GANs) \cite{goodfellow2014generative} are an attractive framework that can be used to mimic complex real-world distributions by solving a min-max optimization problem between generator and discriminator~\cite{zhang2019self,karras2019style,karras2020analyzing,tang2020xinggan}.
For instance, Reed \emph{et al.} first apply the conditional GAN to generate plausible images from text descriptions \cite{reed2016generative, reed2016learning}. 
StackGAN \cite{zhang2017stackgan, zhang2018stackgan} generates high-resolution images by stacking multiple generators and discriminators and provides the text information to the generator by naively concatenating text vectors to the input noises and intermediate features. 
Next, AttnGAN \cite{xu2018attngan} introduces the cross-modal attention mechanism to help the generator synthesize images with more details. 
MirrorGAN \cite{qiao2019mirrorgan} regenerates text descriptions from generated images for text-image semantic consistency \cite{zhu2017unpaired}. 
SD-GAN \cite{yin2019semantics} employs the Siamese structure \cite{varior2016gated,varior2016siamese} to distill the semantic commons from texts for image generation consistency. 
Obj-GAN \cite{li2019object} generates complex scenes from pre-generated semantic layouts and text descriptions and employs a Fast R-CNN \cite{girshick2015fast} to compute an object-wise loss.
Since Obj-GAN introduces bounding box and class label information besides text description, we did not compare it within our experiment. 
Finally, DM-GAN \cite{zhu2019dm} introduces the Memory Network \cite{gulcehre2018dynamic,weston2014memory} to cope with the first aforementioned problem of stacked architecture. But memory network only alleviates the dependence on initial images, it does not solve the problem completely. 
And adding Memory Network on stacked architecture makes the network require a considerable computational cost. 
Recently, there are some new text-to-image methods~\cite{ramesh2021zero,lin2021m6} based on large transformer model that autoregressively models the text and image tokens as a single stream of data.

Our proposed method is much different from previous methods.
First, our DF-GAN generates high-resolution images directly by a one-stage text-to-image backbone.
Second, our model adopts a target-aware discriminator to ensure image quality and text-image semantic consistency without introducing extra network.
Third, our model fuses text and image features more deeply and efficiently through a sequence of DFBlocks.
Finally, the skip-z with truncation method is employed to provide the generator with a stable and continuous text latent space.
Compared with previous models, our DF-GAN is simpler and more effective to synthesize realistic and text-matching images.

\begin{figure*}[t]
  \centering
  \includegraphics[width=\linewidth]{c_02.jpg}
  \caption{The architecture of the proposed DF-GAN for text-to-image synthesis. DF-GAN generates high-resolution images directly by one pair of generator and discriminator and fuses the text information and visual feature maps through multiple Deep text-image Fusion Blocks (DFBlock) in UPBlocks.  Armed with Matching-Aware Gradient Penalty (MA-GP) and one-way output, our model can synthesize more realistic and text-matching images.}
  \label{fig2}
  \vspace{-0.4cm}
\end{figure*}

\section{Deep Fusion GANs}

In this paper, we propose a novel cross-modal Generative Adversarial Network named as Deep Fusion GAN (DF-GAN) for text-to-image generation (see Figure~\ref{fig2}). 
We start to present DF-GAN in this section. First, we introduce the overall structure of DF-GAN and also describe the components of the generator and discriminator. 
Then, we illustrate the proposed Matching-Aware Gradient Penalty (MA-GP), one-way output, Deep text-image Fusion Block (DFBlock), and the skip-z with truncation in detail. 

\subsection{Model Overview}
The goal of our proposed DF-GAN is to generate realistic and text-image semantic consistent images from given text descriptions. 
The entire network is composed of a generator, a discriminator, and pre-trained text encoder~\cite{xu2018attngan}. 
In the following, we describe the structure of the generator and discriminator, respectively.

The generator has two inputs, a sentence vector that is encoded by text encoder and a noise vector sampled from the Gaussian distribution to ensure the diversity of generated images. 
The noise vector is first fed into a fully connected layer and the output is reshaped to . We then apply a series of UPBlocks to upsample the image features. The UPBlock is composed of an upsample layer, a residual block, and DFBlocks (see Figure~\ref{fig5}(d)) to fuse the text and image features during the image generation process. 
Finally, a convolution layer converts image features into images.

The discriminator is composed of several DownBlocks and convolution layers. 
First, the discriminator converts images into feature maps and the output is downsampled by a series of DownBlocks. 
Then the sentence vector will be replicated and concatenated on the image feature. 
An adversarial loss will be predicted to evaluate the visual realism and semantic consistency of inputs. 
By distinguishing generated images from real samples, the discriminator promotes the generator to synthesize images with higher quality and text-image semantic consistency.


\subsection{One-Stage Text-to-Image Backbone}
Previous text-to-image GANs are based on stacked architecture \cite{zhang2017stackgan,zhang2018stackgan} to generate high-resolution images. 
However, the entanglements between different generators makes the generated image look like a simple combination of coarse shape synthesized by  and , and some details synthesized by  (see Figure~\ref{fig1}). It makes the final generated images unrealistic.

Inspired by recent models proposed for unconditional image generation \cite{lim2017geometric,zhang2019self}, we propose a one-stage text-to-image backbone which can synthesize high-resolution images directly by one pair of generator and discriminator. 
We employ the hinge loss \cite{lim2017geometric} to stabilize the training process. 
Since there is only one pair of generator and discriminator in our backbone, the model only attends to the quality of the final generated images. 
Without the entanglements between generated images of different scales in stacked architecture, our one-stage text-to-image backbone can control the object shape and fine-grained visual details in final generated images directly through a deeper generator network with a sequence of residual networks.
The formulation of our one-stage method is as follows:


where  is the noise vector sampled from Gaussian distribution;  is the sentence vector; , ,  denote the synthetic data distribution, real data distribution, and mismatching data distribution, respectively.



\subsection{Target-Aware Discriminator}

In this work, we also propose a novel Target-Aware Discriminator which is composed of Matching-Aware Gradient Penalty (MA-GP) and one-way output.
The Target-Aware Discriminator promotes the generator to synthesize more realistic and text-image semantic-consistent images. 




\subsubsection{Matching-Aware Gradient Penalty}

Taking text-to-image synthesis as an example in Figure~\ref{fig3}, there are four kinds of data pairs in two-dimensional data space: synthetic images with matching text, synthetic images with mismatched text, real images with matching text, real images with mismatched text. 
To generate text-matching and realistic images from given text descriptions, the discriminator should put real and matching data points to the minimum point of the discriminator loss function surface and put other inputs at high points.
Moreover, the discriminator should ensure a smooth vicinity of real and matching data points to help the generator converge to the minimum point more easily.
Therefore, we propose the Matching-Aware Gradient Penalty (MA-GP) which applies the gradient penalty on the target data point: real images with the matching sentences. 
The whole formulation of our model with MA-GP is as follows:


where  and  are two hyper-parameters to balance the effectiveness of gradient penalty.

Compared with previous methods for ensuring the image quality and semantic consistency, our proposed MA-GP does not introduce extra networks to compute the text-image semantic similarity.
We consider that the discriminator itself is a sufficiently strong network. 
The addition of extra networks is not necessary. 
But it is very important to construct a proper discriminator loss function surface to meet our expectations. 
In text-to-image generation task, we want the generator to synthesize realistic and text-image semantic consistent images.
So we propose the MA-GP which can ensure that the real and text-matching data points are at the minimum points of the discriminator loss function surface, and the vicinity of real and text-matching data points is smooth.
Armed with MA-GP, our model is able to synthesize more realistic images with better text-image semantic consistency.

\begin{figure}[t] \small
  \centering
  \includegraphics[width=\linewidth]{c_03.jpg}
  \caption{A diagram for Matching-Aware Gradient Penalty (MA-GP). The data point (real, match) marked by a circle should be applied MA-GP.}
  \label{fig3}
  \vspace{-0.4cm}
\end{figure}

\begin{figure}[t] \small
  \centering
  \includegraphics[width=\linewidth]{c_04.jpg}
  \caption{Comparison between two-ways output and our proposed one-way output. The two-ways output predicts conditional loss and unconditional loss and sums them up as the final adversarial loss. Our one-way output predicts the whole adversarial loss directly.}
  \label{fig4}
    \vspace{-0.4cm}
\end{figure}


\subsubsection{One-Way Output}

In previous text-to-image GANs \cite{zhang2017stackgan,zhang2018stackgan,xu2018attngan}, the discriminator first extracts the image feature through a series of downsampling operations. 
Then the image feature will be used in two ways. 
As shown in Figure~\ref{fig4}, one way determines whether the image is real or fake, another way concatenates the image feature and sentence vector to evaluate text-image semantic consistency. 
So there are two kinds of loss computed, the unconditional loss and the conditional loss. 

We found that the two-ways output weakens the effectiveness of MA-GP and slows the convergence of the generator. 
As depicted in Figure~\ref{fig3}, the discriminator should put wrong data at high points and right data at minimum points. 
The proposed MA-GP ensures a smooth loss surface to help the generator converge to the right data (minimum points) through gradient descending. 
But the two-ways output decomposes the adversarial loss into conditional loss and unconditional loss. 
The conditional loss gives a gradient  pointing to the real and matching inputs after backpropagation. 
But the unconditional loss gives a gradient  only pointing to the real images. 
As shown in Figure~\ref{fig3}, the final gradient  does not point to the real and matching data points directly. 
This will make the convergence of the generator deviate from the target data point (real and match). 

Therefore, we propose the one-way output for text-to-image synthesis. 
As shown in Figure~\ref{fig4}(b), our discriminator concatenates the image feature and sentence vector, then outputs one adversarial loss through two convolution layers. 
The one-way output only gives one gradient  pointing to the target data points (real and match) directly, it gives a more clear convergence goal to the generator. 
Armed with MA-GP and one-way output, the discriminator will guide the generator to synthesize more realistic images with better text-image semantic consistency more directly.
Our experiments and ablation studies also demonstrate that the one-way output can further promote the effectiveness of MA-GP and accelerate the convergence process of the generator. 


\subsection{Efficient Text-Image Fusion}

To fuse text and image features efficiently, we propose a novel Deep text-image Fusion Block (DFBlock) which fuses text and visual information more effectively and deeply during the generation process. 
Furthermore, we propose the method called the skip-z with truncation to provides the generator with a stable and better text latent space to synthesize higher-quality images.

\subsubsection{Deep Text-Image Fusion Block}
In this work, we propose a novel Deep text-image Fusion Block (DFBlock) (see Figure~\ref{fig5}) which fuses text and visual information more effectively and deeply. 

Figure~\ref{fig5}(a) shows a typical upsample Residual Block in generator~\cite{mescheder2018training, brock2018large}, there are two Fusion Blocks stacked in Residual Block. 
There are many implementations of Fusion Block, the Fusion Block with Conditional Batch Normalization (CBN) is implemented as Figure~\ref{fig5}(b)~\cite{yin2019semantics}. 
Batch Normalization (BN) normalizes the feature maps, and the Affine Transformation manipulates the output by scaling and shifting parameters predicted from conditions.
We consider that BN in Fusion Block2 will reduce the effectiveness of Affine Transformation in Fusion Block1. 
Since BN transforms the feature maps into a normal distribution, it can be regarded as a unconditional reverse operation of Affine Transformation and reduces the distance between each feature map in a batch, it is not beneficial for the conditional generation process.
So we extract the Affine Transformation from CBN, and employ Affine Transformations to manipulate visual feature maps conditioned on natural language descriptions.
We adopt two one-hidden-layer MLPs to predict the language-conditioned channel-wise scaling parameters  and shifting parameters  from sentence vector , respectively:


\begin{figure*}[t] \small
  \centering
  \includegraphics[width=\linewidth]{c_05.jpg}
  \caption{We redesign the architecture of the Fusion Block and compare DFBlock with AFFBlock and CBNBlock. (a) A typical UPBlock in the generator network. The UPBlock upsamples the image features and fuses text and image features by two Fusion Blocks. (b) The CBNBlock is a Fusion Block which employs the Conditional Batch Normalization to fuse text and image features. (c) AFFBlock is a simplified version of CBNBlock which removes the Batch Normalization layer. (d) The DFBlock is an enhanced version of AFFBlock, it deepens the text-image fusion process by stacking multiple Affine Transformations.}
  \label{fig5}\
  \vspace{-0.4cm}
\end{figure*}

If the input is feature map , the output of MLP will be a vector of size . 
We first conduct the channel-wise scaling operation on  with the scaling parameter , then we apply the channel-wise shifting operation on  with the shifting parameter . 
This process can be formally expressed as follows:


where  is Affine Transformation;  is the  channel of visual feature maps;  is the sentence vector;  and  are scaling parameter and shifting parameter for the  channel of visual feature maps. 
Through channel-wise scaling and shifting, the generator can capture the semantic information in text description and synthesize realistic images matching with given text descriptions. 
We denote this module which fuses text and image features through one Affine Transformation as AFFBlock (see Figure~\ref{fig5}(c)).

Furthermore, after decomposing the effectiveness of Affine Transformation from CBN, we can fuse image features and conditions in a more free way. 
So we propose the Deep text-image Fusion Block (DFBlock) which stacks multiple Affine Transformations and ReLU layers in Fusion Block. 
As shown in Figure~\ref{fig5}(d), there are two Affine Transformations and ReLU layers stacked sequentially in one Fusion Block. 
The DFBlock deepens the depth of the text-image fusion process. For neural networks, a deeper network always means a stronger ability. 
We consider that deepening the fusion process can bring two main benefits for text-to-image generation: 
First, it gives the generator more chances to fuse text and image features, so that the text information can be fully exploited. 
Second, deepening the fusion process makes the fusion network have more nonlinearities, which is beneficial to generate semantic consistent images from different text descriptions.


We conduct extensive experiments to compare the effectiveness between CBNBlock (Figure~\ref{fig5}(b)) \cite{miyato2018cgans}, AFFBlock (Figure~\ref{fig5}(c)), and DFBlock (Figure~\ref{fig5}(d)) in the ablation studies. 
Experimental results show that our DFBlock is more effective to fuse the text and image features, which proves its superiority and efficiency.

\subsubsection{Skip-Z with Truncation}

In text-to-image synthesis works, the input text description will be encoded into a sentence vector by a fixed pretrained text encoder.
Thus, each description in the training set only corresponds to a fixed sentence vector, it makes the text latent space discontinuous and leads to overfitting on training set.
To deal with this problem, we concatenate the noise vector on the sentence vector, it converts a single, discrete sentence vector into a beam of sentence vectors (see Figure~\ref{fig3}).
We call this noise concatenation as skip-z.
Skip-z brings variety to sentence vector, but the variety is not entirely beneficial. 
Since the noise is sampled from the Gaussian distribution, when the model is used to generate images from text descriptions and noises, uncommon noises will cause negative effects on the synthesized image quality.
So we introduce Truncation Trick~\cite{brock2018large,karras2019style} to truncate the noise vector through resampling the values with magnitude above a chosen threshold.
The skip-z with Truncation provides the generator with a stable and better text latent space which ensures the synthesized image quality.

\section{Experiments}
We first introduce the datasets, training details, and evaluation metrics used in our experiments, then evaluate DF-GAN and its variants quantitatively and qualitatively. 

\noindent{\bf Datasets.} We follow previous work~\cite{zhang2017stackgan,zhang2018stackgan,xu2018attngan,zhu2019dm,yin2019semantics,qiao2019mirrorgan} and evaluate the proposed model on two challenging datasets, i.e., CUB bird \cite{wah2011caltech} and COCO \cite{lin2014microsoft}.
The CUB dataset contains 11788 images belonging to 200 bird species. Each bird image has ten language descriptions. 
The COCO dataset contains 80k images for training and 40k images for testing. Each image in this dataset has five language descriptions. 

\noindent{\bf Training Details.} We optimize our network using Adam \cite{kingma2014adam} with  and . The learning rate is set to  for the generator and  for the discriminator according to Two Timescale Update Rule (TTUR) \cite{heusel2017gans}. 

\noindent{\bf Evaluation Details.} Following previous works \cite{xu2018attngan, zhu2019dm}, we choose the Inception Score (IS) \cite{salimans2016improved} and Fr\'echet Inception Distance (FID) \cite{heusel2017gans} to evaluate the performance of our network. 
Specifically, IS computes the Kullback-Leibler (KL) divergence between conditional distribution and marginal distribution. 
Higher IS means higher quality of the generated images and each image clearly belongs to a specific class. 
FID \cite{heusel2017gans} is another assessment which computes the Fr\'echet distance between the distribution of the synthetic images and real-world images in the feature space of a pre-trained Inception v3 network. 
Contrary to IS, more realistic images have a lower FID.
To compute both IS and FID, each model generates 30,000 images ( resolution) from text descriptions randomly selected from the test dataset. 

It should be pointed out that, as observed in Obj-GAN paper \cite{li2019object,ramesh2021zero}, we also found that IS on the COCO dataset completely fails in evaluating the synthesized image quality of text-to-image models. Thus, we do not compare IS on the COCO dataset. While  FID is more robust and aligns human qualitative evaluation on the COCO dataset. 

\begin{figure*}[t] \small
  \centering
  \includegraphics[width=\linewidth]{c_06.jpg}
  \caption{Examples of images synthesized by AttnGAN \cite{xu2018attngan}, DM-GAN \cite{zhu2019dm}, and our proposed DF-GAN conditioned on text descriptions from the test set of COCO and CUB datasets.}
  \label{fig6}
  \vspace{-0.4cm}
\end{figure*}

\subsection{Quantitative Evaluation}
We compare the proposed method with several state-of-the-art methods including AttnGAN \cite{xu2018attngan}, MirrorGAN \cite{qiao2019mirrorgan}, SD-GAN \cite{yin2019semantics}, and DM-GAN \cite{zhu2019dm}, which have achieved the remarkable success of text-to-image synthesis by using stacked structures. 
As shown in Table~\ref{table1}, our proposed DF-GAN achieves the highest Inception Scores (IS) and lowest Fr\'echet Inception Distance (FID) compared with other leading models. 

Specifically, compared with AttnGAN \cite{xu2018attngan} which employs cross-modal attention to fuse text and image features, our DF-GAN improves the IS metric from 4.36 to 5.10 and decreases the FID metric from 23.98 to 14.81 on the CUB dataset, and decreases FID from 35.49 to 21.42 on the COCO dataset.
Compared with MirrorGAN \cite{qiao2019mirrorgan} and SD-GAN \cite{yin2019semantics} which employ cycle consistency and Siamese network to ensure text-image semantic consistency, our DF-GAN improves IS from 4.56 and 4.67 to 5.10 respectively on the CUB dataset.
Compared with DM-GAN \cite{zhu2019dm} which introduces Memory Network to refine fuzzy image contents, our model also improves IS from 4.75 to 5.10 and decrease  FID from 16.09 to 14.81 on CUB, and decrease FID from 32.64 to 21.42 on COCO.
The quantitative comparisons of IS show that our proposed DF-GAN is able to synthesize higher-quality images from text descriptions. 


\begin{table}[t] \small
\centering
\caption{The results of IS and FID compared with the state-of-the-art methods on the test set of CUB and COCO.}
\begin{tabular}{r|c|c|c}
\toprule
Method & CUB-IS & CUB-FID        & COCO-FID       \\ \midrule
AttnGAN \cite{xu2018attngan} & 4.36        & 23.98      & 35.49          \\
MirrorGAN \cite{qiao2019mirrorgan} &4.56   & 18.34      & 34.71       \\
SD-GAN \cite{yin2019semantics} &4.67       & -          & -       \\
DM-GAN  \cite{zhu2019dm} & 4.75            & 16.09      & 32.64                   \\
DF-GAN (Ours)   & \textbf{5.10}   & \textbf{14.81}       & \textbf{21.42} \\ \bottomrule
\end{tabular}
\label{table1}
\vspace{-0.4cm}
\end{table}

\subsection{Qualitative Evaluation}
We also compare the visualization results synthesized by AttnGAN \cite{xu2018attngan}, DM-GAN \cite{zhu2019dm}, and the proposed DF-GAN. 
We first compare the quality of the synthesized images and text-image semantic consistency on the CUB dataset, then compare the results on a more challenging COCO dataset.

It can be seen that images synthesized by AttnGAN \cite{xu2018attngan} and DM-GAN \cite{zhu2019dm} in Figure~\ref{fig6} look like a simple combination of fuzzy shape and some visual details (1, 3, 5, 7, and 8 columns). 
The reason is the employment of their stacked architecture and cross-modal spatial attention.
Although DM-GAN \cite{zhu2019dm} introduces Memory Network to alleviate this problem, the problem is not completely solved. 
As shown in the 5, 7, and 8 columns, the birds synthesized by AttnGAN \cite{xu2018attngan} and DM-GAN \cite{zhu2019dm} contain wrong shapes. 
Since DF-GAN employs a novel one-stage text-to-image backbone and a sequence of Deep text-image Fusion Blocks (DFBlock), the images synthesized by our DF-GAN have better object shapes and realistic fine-grained details (e.g., 1, 3, 7 and 8 columns). Besides, the posture of the bird in our DF-GAN result is also more natural (e.g., 7 and 8 columns). 

Comparing the text-image semantic consistency with other models, we find that our DF-GAN is also able to capture more fine-grained details in text descriptions.
For example, as the result shown in 1, 2, 6 columns in Figure~\ref{fig6}, other models cannot synthesize the ``holding ski poles'', ``train track'', and ``a black stripe by its eyes'' described in the text well, but the proposed DF-GAN can synthesize them more correctly. 


\subsection{Ablation Study}

In this section, we conduct ablation studies on the testing set of the CUB dataset to verify the effectiveness of each component in the proposed DF-GAN. 
The components include One-Stage text-to-image Backbone (OS-B), Matching-Aware Gradient Penalty (MA-GP), One-Way Output (OW-O), Deep text-image Fusion Block (DFBlock), and Skip-z with Truncation (SZ-T). 
Our baseline is a stacked text-to-image GAN which employs two-ways output. 
In baseline, the sentence vector is naively concatenated to the input noise and intermediate feature maps.
We first evaluate the effectiveness of OS-B, MA-GP, and OW-O. 
We conduct user study to evaluate the text-image semantic consistency (SC), and we asked 10 users to score the 100 synthesized images with text descriptions. 
The scores range from 1 (worst) to 5 (best).
The results on the CUB dataset are shown in Table~\ref{table3}.

\begin{table}[t] \small
\centering
\caption{The performance of different components of our model on the test set of CUB.}
\begin{tabular}{l|c|c|c}\toprule
Architecture        &IS    &FID       &SC  \\ \midrule
Baseline            & 3.96           & 51.34                & -\\
OS-B                & 4.11           & 43.45                & 1.46\\
OS-B w/ MA-GP    & 4.46           & 32.52                & 3.55\\
OS-B w/ MA-GP w/ OW-O & \textbf{4.57}  & \textbf{23.16} & \textbf{4.61}\\ \bottomrule
\end{tabular}
\label{table3}
\vspace{-0.4cm}
\end{table}

Compared with baseline, our proposed One-Stage text-to-image Backbone (OS-B) improves IS from 3.96 to 4.11, and decreases FID from 43.45 to 32.52. 
The results prove that our one-stage backbone is more effective than stacked architecture.
Armed with MA-GP, the model further improves IS to 4.46, SC to 3.55, and decreases FID to 32.52 significantly. 
It demonstrates that the proposed MA-GP can promote the generator to synthesize more realistic and text-image semantic consistent images.
The proposed OW-O also improves IS from 4.46 to 4.57, SC from 3.55 to 4.61, and decreases FID from 32.52 to 23.16. 
It also demonstrates that the one-way output is more effective than a two-ways output in the text-to-image generation task. 

To prove the superiority of DFBlock, we compare the DFBlock with CBNBlock, AFFBlock which employs conditional Batch Normalization \cite{de2017modulating, miyato2018cgans, brock2018large}, and one Affine Transformation layer to fuse text and image features, respectively. 
Their differences are shown in Figure~\ref{fig5}. 
There results are listed in Table~\ref{table4}.
MA-GP GAN is the model that employs One-Stage text-to-image Backbone, Matching-Aware Gradient Penalty, and One-Way Output.

\begin{table}[t] \small
\centering
\caption{The performance of MA-GP GAN with different modules on the test set of CUB.}
\begin{tabular}{l|c|c}\toprule
Architecture                 &IS    &FID  \\\midrule
MA-GP GAN w/ Concat           & 4.57           & 23.16\\ 
MA-GP GAN w/ CBNBLK           & 4.71           & 21.77\\
MA-GP GAN w/ AFFBLK           & 4.75           & 20.71\\
MA-GP GAN w/ DFBLK            & 4.88           & 18.84\\
MA-GP GAN w/ DFBLK w/ SZ       & 4.86           & 19.21\\
MA-GP GAN w/ DFBLK w/ T        & 4.81           & 19.97 \\
MA-GP GAN w/ DFBLK w/ SZ-T (DF-GAN)      & \textbf{5.10}  & \textbf{14.81} \\ \bottomrule
\end{tabular}
\label{table4}
\vspace{-0.4cm}
\end{table}

From the results in Table~\ref{table4}, we find that compared with other fusion methods, concatenating cannot efficiently fuse text and image features.
The comparison between CBNBlock and AFFBlock proves that Batch Normalization is not essential in Fusion Block, and removing normalization even slightly improves the results. 
The comparison between DFBlock and AFFBlock demonstrates the effectiveness of deepening the text-image fusion process. 
The comparison result proves the effectiveness of our proposed DFBlock. 
Moreover, we compare the model with Skip-z (SZ), Truncation (T), Skip-z with Truncation (SZ-T) respectively.
The MA-GP GAN with DFBLK and SZ-T (DF-GAN) achieves the best results compared with other models, it also demonstrates that the proposed SZ-T can promote the generator to synthesize higher-quality images.


\section{Conclusions}
In this paper, we propose a novel Deep Fusion Generative Adversarial Networks (DF-GAN) for text-to-image generation tasks which is able to directly synthesize more realistic and text-image semantic consistent images without stacking architecture and extra networks.
Moreover, we propose a novel target-aware discriminator composed of Matching-Aware Gradient Penalty (MA-GP) and one-way output, it significantly improves the image quality and text-image semantic consistency, and accelerate the convergence of generator.
Besides, we propose a novel Deep text-image Fusion Block (DFBlock) which fuses text and image features more effectively and deeply.
Furthermore, we present a novel method called the skip-z with truncation to provide the generator with a stable and better text latent space for synthesizing higher-quality images.
Extensive experiment results show that our proposed DF-GAN significantly outperforms state-of-the-art models on the CUB dataset and more challenging COCO dataset. 


\clearpage
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
