\pdfoutput=1


\documentclass[11pt]{article}

\usepackage[]{naacl2021}

\usepackage{times}
\usepackage{latexsym}
\usepackage{booktabs}
\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amssymb}\usepackage{pifont}\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}\usepackage{xcolor}
\usepackage{todonotes}
\usepackage{float}

\usepackage[absolute,overlay]{textpos}

\usepackage[T1]{fontenc}


\usepackage[utf8]{inputenc}

\usepackage{microtype}
\usepackage{xspace}

\newcommand{\custo}[1]{\textsc{\normalsize #1}}
\newcommand{\beir}{\custo{beir}\xspace}




\title{\includegraphics[width=0.8cm]{emoji/beer-clicking-google.png} BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models}


\author{Nandan Thakur, Nils Reimers, Andreas R\"uckl\'e, Abhishek Srivastava, Iryna Gurevych \\
  Ubiquitous Knowledge Processing Lab (UKP-TUDA) \\
  Department of Computer Science, Technische Universit{\"a}t Darmstadt \\
  \href{https://www.informatik.tu-darmstadt.de/ukp/ukp_home/index.en.jsp}{\texttt{www.ukp.tu-darmstadt.de}}}


\begin{document}
 \maketitle

\begin{abstract}

Neural IR models have often been studied in homogeneous and narrow settings, which has considerably limited insights into their generalization capabilities. To address this, and to allow researchers to more broadly establish the effectiveness of their models, we introduce \textbf{\beir} (\emph{\textbf{Be}nchmarking} \textbf{\emph{IR}}), a \emph{heterogeneous benchmark} for information retrieval. We leverage a careful selection of 17 datasets for evaluation spanning diverse retrieval tasks including open-domain datasets as well as narrow expert domains. We study the effectiveness of nine state-of-the-art retrieval models in a \emph{zero-shot} evaluation setup on \beir, finding that performing well consistently across all datasets is challenging.


Our results show \emph{BM25} is a robust baseline and \emph{Reranking}-based models overall achieve the best zero-shot performances, however, at high computational costs. In contrast, \emph{Dense}-retrieval models are computationally more efficient but often underperform other approaches, highlighting the considerable room for improvement in their generalization capabilities. In this work, we extensively analyze different retrieval models and provide several suggestions that we believe may be useful for future work. \beir datasets and code are available at \url{https://github.com/UKPLab/beir}.


\end{abstract}



\begin{figure*}[t]
    \centering
    \begin{center}
        \includegraphics[trim=0 137 50 40,clip,width=\textwidth]{figures/flowchart.pdf}
        \captionof{figure}{An overview of the diverse tasks and datasets present in \beir.}
        \label{fig:beir-diagram}
    \end{center}
    \vspace*{-\baselineskip}
\end{figure*}

\section{Introduction}

Many real-world NLP problems rely on a practical and efficient retrieval component as a first step to find relevant information. Examples are open-domain question-answering \cite{chen-etal-2017-reading}, claim-verification \cite{thorne-etal-2018-fever}, and duplicate question detection \cite{zhang2015multi}. Traditionally, retrieval has been dominated by lexical approaches like TF-IDF or BM25 \cite{bm25}. However, these approaches suffer from what is known as lexical gap \cite{berger2000bridging} and only retrieve documents that contain the keywords also present within the query. Further, queries and documents are treated in a bag-of-words manner which does not take word ordering into consideration.

Recently, deep learning and in particular pre-trained Transformer models like BERT \cite{devlin2018bert} have became popular in the information retrieval space \cite{lin2020pretrained}. They overcome the lexical gap by mapping queries and documents to a dense vector \cite{10.1145/2983323.2983769, lee-etal-2019-latent, karpukhin-etal-2020-dense, guu2020realm, gao2020complementing, liang2020embeddingbased, ma2021zeroshot}. The relevant documents for a given query are then retrieved using (approximate) nearest neighbor search~\cite{JDH17}.

Another widely used approach involves re-ranking documents from the output of a first-stage retrieval system \cite{nogueira2019multistage, nogueira-etal-2020-document, nogueira2020passage, 10.1145/3397271.3401075}. While dense retrieval approaches try to overcome the (potential) lexical gap, re-ranking approaches aim to create a better comparison of the retrieved documents. Different approaches can also be combined together \cite{ding2020rocketqa, gao2020complementing, luan2021sparse}.

Previous approaches were commonly trained on rather large datasets like the Natural Questions (NQ) dataset \cite{47761} containing around 133k training examples or the MSMARCO dataset \cite{nguyen2016ms} with more than 500k training examples. Existing approaches have been shown to perform well when evaluated in-domain or for similar tasks \cite{nogueira2020passage, karpukhin-etal-2020-dense, ding2020rocketqa}. However, large training corpora are not available for most tasks and domains. As creating a large training corpus can be expensive, it is not feasible to create such for most tasks and domains. Hence, in most scenarios, we apply retrieval models in a \textbf{zero-shot} setup, i.e.\ pre-trained models are applied out-of-the-box across new tasks and domains. In this crucial setting, it remains unclear whether and how well these retrieval models transfer or generalize to new tasks or domains. 

In this work, we establish a new heterogeneous benchmark for Information Retrieval called \textbf{\beir} (\emph{\textbf{Be}nchmarking} \textbf{\emph{IR}}) consisting of a broad range of domains and tasks. Through \beir, we systematically study the zero-shot generalization capabilities of multiple neural retrieval approaches. 
Existing benchmarks \cite{guo2020multireqa, petroni2020kilt} have issues of a comparatively narrow evaluation focusing either only on a single task or on a certain domain. \beir overcomes these shortcomings by covering 9 diverse retrieval tasks with 17 datasets including a diverse set of domains, query and document types.

We use \beir to evaluate nine diverse state-of-the-art retrieval approaches. From our analysis, we find that no approach consistently outperforms all others. Further, we notice that the in-domain performance of a model does not correlate with its generalization capabilities: models fine-tuned with the same training data might generalize differently in \beir.

In \beir, we observe a trade-off between the zero-shot performances and the computational cost: computationally expensive re-ranking models overall perform the best. More computationally efficient dense models can, depending on the task and domain, substantially underperform traditional lexical models like BM25. In fact, BM25 is the third best performing approach on our benchmark, beaten only by neural re-ranking approaches.

With \beir, we take an important step towards broadly establishing and improving the zero-shot capabilities of neural retrieval systems. Our benchmark attempts to help the community better to understand when a retrieval system could work or fail for a certain domain or task. Further, our benchmark attempts to help researchers develop a \textit{universal} neural retriever that performs well across diverse tasks and domains. 

We publicly release \beir and an integration of various retrieval models in a well-documented, easy to use and extensible open-source implementation. The implementation is designed to allow easy integration of new tasks, datasets and retrieval models. 

\section{Background and Related Work}
To our knowledge, \beir is the first zero-shot information retrieval benchmark. Existing benchmarks do not evaluate retrieval in a zero-shot setting in depth, they either focus over a single task, small corpora or on a certain domain. This setting hinders for investigation of model generalization across diverse set of domains and task types. 

\subsection{Existing Benchmarks}

MultiReQA \cite{guo2020multireqa} consists of eight Question-Answering (QA) datasets and evaluates sentence-level answer retrieval given a question.MultiReQA only tests a single task (question to answer-sentence retrieval) and five out of eight datasets are on retrieval from Wikipedia. Further, MultiReQA evaluates retrieval over rather small corpora: six out of eight tasks have less than 100k candidate sentences, which benefits dense retrieval over lexical as shown previously in \cite{reimers2020curse}. 
KILT \cite{petroni2020kilt} consists of five knowledge-intensive tasks including a total of eleven datasets. The tasks involve retrieval, but it is not the primary task. Further, KILT retrieves documents only from Wikipedia. 



\subsection{Neural Retrieval}
Information retrieval is the process of searching and returning relevant documents for a query from a collection. In our paper, we focus on text retrieval and use \textit{document} as a cover term for text of any length in the given collection and \textit{query} for the user input, which can be of any length as well. 

Traditionally, lexical approaches like TF-IDF and BM25 \cite{bm25} have dominated textual information retrieval. Recently, there is a strong interest in using neural networks to improve or replace these lexical approaches. In this section, we highlight a few neural-based approaches and we refer the reader to \newcite{lin2020pretrained} for a detailed survey on recent developments in neural retrieval.


\textbf{Retriever-based} \quad Lexical approaches suffer from the lexical gap \cite{berger2000bridging}. To overcome this, earlier techniques proposed to improve lexical retrieval systems with neural networks. Doc2query \cite{nogueira2019document} utilized a deep encoder-decoder architecture and generated possible queries for a given document. These queries were appended to the document. DeepCT \cite{10.1145/3397271.3401204} utilized BERT to learn relevant terms in a document and repeats those terms at the end of the document, to achieve a higher term-frequency weight. More recently, dense retrieval approaches have been proposed to map queries and documents in a shared, dense vector space \cite{gillick2018endtoend}. Here, a dual-encoder neural architecture based on pre-trained Transformers showed strong performance for various tasks  \cite{guo2020multireqa, karpukhin-etal-2020-dense, liang2020embeddingbased, ma2021zeroshot}. This dense approach was recently extended by hybrid lexical-dense approaches which aims to combine the strengths of both approaches \cite{gao2020complementing, seo-etal-2019-real, luan2021sparse}. In contrast to the dense approaches, SPARTA \cite{zhao2020sparta} learns sparse representations by combining contextualized and non-contextualized embeddings.


\textbf{Reranking-based} \quad Neural reranking approaches use the output of a first-stage retrieval system, often BM25, and re-ranks the documents based on the estimated relevancy between the query and the document. Significant improvement in performance was achieved with the cross-attention mechanism of BERT \cite{nogueira2020passage}. A disadvantage of reranking with cross-attention is the high computational overhead  \cite{reimers-gurevych-2019-sentence}. In contrast, ColBERT \cite{10.1145/3397271.3401075} computes contextualized embeddings independently for queries and documents and uses an efficient maximum-similarity operation for the relevance estimation.



\section{BEIR Benchmark} \label{sec_beir_benchmark}

\begin{table*}[t!]
    \small
    \ra{1.2}
    \resizebox{\textwidth}{!}{\begin{tabular}{ l l l c c c c c c c l l }
        \toprule
         \multicolumn{1}{l}{\textbf{Split} ()} &
         \multicolumn{4}{c}{} &
         \multicolumn{1}{c}{\textbf{Train}}    &
         \multicolumn{1}{c}{\textbf{Dev}}    &
         \multicolumn{3}{c}{\textbf{Test}}   &
         \multicolumn{2}{c}{\textbf{(Train + Dev + Test)}} \\
         \cmidrule(lr){6-6}
         \cmidrule(lr){7-7}
         \cmidrule(lr){8-10}
         \cmidrule(lr){11-12}
           \textbf{Task ()} &\textbf{Domain ()} & \textbf{Dataset ()} & \textbf{Title} & \textbf{Relevancy} & \textbf{\#Pairs} & \textbf{\#Query} & \textbf{\#Query} & \textbf{\#Corpus} & \textbf{Avg. Docs~/~Q } & \textbf{Avg.~Q Len} & \textbf{Avg.~Doc Len} \\
         \midrule
    Passage-Retrieval    & Misc. & MSMARCO           & \xmark & Binary  & 532,761 &   ----  &   6,980   &   8,841,823      & 1.1 & 5.96  & 55.98  \\ \midrule[0.05pt] \midrule[0.05pt]
    Bio-Medical          & Bio-Medical & (1) TREC-COVID    & \cmark & 3-level &   ----    &   ----  & 50     & 171,332   & 493.5& 10.60 & 160.77 \\
    Information          & Bio-Medical & (2) NFCorpus      & \cmark & 3-level & 110,575 &  324  & 323    & 3,633     & 38.2 & 3.30  & 232.26 \\
    Retrieval (IR)       & Bio-Medical & (3) BioASQ        & \cmark & Binary  & 32,916 & ---- & 500    & 14,914,602& 4.7  & 8.05  & 202.61 \\ \midrule
    Question             & Wikipedia  & (4) NQ            & \cmark & Binary  & 132,803  &   ----  & 3,452 & 2,681,468 & 1.2  & 9.16  & 78.88  \\
    Answering       & Wikipedia  & (5) HotpotQA      & \cmark & Binary  & 170,000 & 5,447 & 7,405  & 5,233,329 & 2.0  & 17.61 & 46.30  \\
     (QA)           &Finance& (6) FiQA-2018     & \xmark & Binary  & 14,166  &  500  & 648    & 57,638    & 2.6  & 10.77 & 132.32 \\ \midrule
    Tweet-Retrieval      &Twitter& (7) Signal-1M (RT)     & \xmark & 3-level &   ----    &   ----  & 97     & 2,866,316 & 19.6 & 9.30  & 13.93  \\ \midrule
    News-Retrieval      &News& (8) TREC-NEWS     & \cmark & 5-level &   ----    &   ----  & 57     & 594,977 & 19.6 & 11.14  & 634.79  \\ \midrule
    Argument       & Misc. & (9) ArguAna      & \cmark & Binary  &   ----    &   ----  & 1,406  & 8,674     & 1.0  & 192.98& 166.80 \\
    Retrieval   & Misc. & (10) T\'ouche-2020 & \cmark & 6-level &   ----    &   ----  & 49     & 382,545   & 49.2 & 6.55  & 292.37 \\ \midrule
    Duplicate-Question   &StackEx.& (11) CQADupStack  & \cmark & Binary  &   ----    &   ----  & 13,145 & 457,199   & 1.4  & 8.59  & 129.09 \\
    Retrieval            & Quora & (12) Quora        & \xmark & Binary  &   ----    & 5,000 & 10,000 & 522,931   & 1.6  & 9.53  & 11.44  \\ \midrule
    Entity-Retrieval     & Wikipedia  & (13) DBPedia      & \cmark & 3-level &   ----    &   67  & 400    & 4,635,922 & 38.2 & 5.39  & 49.68  \\ \midrule
    Citation-Prediction  & Scientific& (14) SCIDOCS      & \cmark & Binary  &   ----    &   ----  & 1,000  & 25,657    & 4.9  & 9.38  & 176.19 \\ \midrule
                         & Wikipedia  & (15) FEVER        & \cmark & Binary  & 140,085 & 6,666 & 6,666  & 5,416,568 & 1.2  & 8.13  & 84.76  \\ 
    Fact Checking        & Wikipedia  & (16) Climate-FEVER & \cmark & Binary  &   ----    &   ----  & 1,535  & 5,416,593 & 3.0  & 20.13 & 84.76  \\
                         & Scientific & (17) SciFact      & \cmark & Binary  &   920      &   ----  &  300   & 5,183     & 1.1  & 12.37 & 213.63  \\
    \bottomrule
    \end{tabular}}
    \caption{Statistics of all the tasks, domains and datasets included in \textbf{\beir}. Few datasets contain documents without titles. Relevancy column indicates the relation between the query and document: binary (relevant, irrelevant) or further graded into sub-levels. Avg.~Docs/Query column indicates the average relevant documents per question.}
    \label{tab:dataset_stats}
\end{table*}

The selection of diverse datasets is an integral part of a benchmark. Including a diverse set of challenging datasets is essential to infer broadly the applicable findings from the observed benchmark results. Furthermore, the benchmark should cover challenging tasks to support future work in that area \cite{gehrmann2021gem}.

To collect the tasks and datasets with the desired properties, a selection methodology is crucial. For \beir, the methodology is motivated by the following three factors:

\paragraph{(1) Diversity in Tasks} Information retrieval is a broad task and the lengths of queries and indexed documents can differ extremely between tasks. Sometimes, queries are short, like a keyword, while in other cases, they can be long like a complete news article. Similarly, indexed documents can sometimes be long, and for other tasks, short like a tweet. In \beir, we aim to include vastly different tasks with different properties for queries and documents. 

\paragraph{(2) Diversity in Domains} Information retrieval systems are applied in all types of textual-based domains. From broad and generic domains, like Wikipedia, to highly specialized ones such as scientific publications of a specialised field. Hence, we include tasks or datasets which provide a representation of real-world problems and are diverse ranging from generic to specialized domains.

\paragraph{(3) Challenging Datasets} 
The \textit{difficulty} of a dataset has to be sufficient. If a task is easily solved by any algorithm, it will not be useful to differentiate the various models used for evaluation. We evaluate several datasets based on existing model performances and literature. We select tasks which we believe are challenging and are not yet fully solved with existing approaches and where model performances could be further improved. 

\subsection{Evaluation Tasks and Datasets} 

Following the selection criteria, we include seventeen English\footnote{We find it also necessary to benchmark multilingual and cross-lingual tasks in \beir, which we keep for future work.} and evaluation datasets that span over nine heterogeneous tasks. As the majority of the evaluated approaches have been trained on the MSMARCO dataset, we also report performances on this dataset, but don't include the outcome in our zero-shot comparison. 

\autoref{fig:beir-diagram} shows each task, dataset and domain together with the type of query and document. \autoref{tab:dataset_stats} shows detailed statistics of the tasks and datasets in \beir. A majority of the retrieval tasks are binary, i.e.\ a search result can be relevant or irrelevant, and a few contain fine-grained relevancy judgements.
Some datasets have few (< 2) annotated relevant documents for a question, while others contain multiple relevant documents. Not all datasets include a training or a development split denoting the practical importance for zero-shot retrieval techniques and model performances.


In \autoref{sec:datasets}, we motivate each task and dataset included within the benchmark. We would refer the reader to this section for a detailed overview on the tasks and datasets in the benchmark. We additionally provide downloadable links (\autoref{tab:dataset_links}) and examples (\autoref{tab:examples}) for all datasets.

\subsection{Dataset and Diversity Analysis}
As shown in \autoref{tab:dataset_stats}, the datasets present in \beir are from various domains including Wikipedia, scientific publications, Twitter, News, online user communities, and more. To measure the domain similarity between the datasets, we compute a pairwise weighted Jaccard similarity \cite{ioffe2010improved} on unigram word overlap between the document collections. For understanding how the similarity score was calculated, refer to \autoref{sec:weighted_jaccard_similarity}. The pairwise results can be found in \autoref{fig:heatmap} in the appendix. We use these pairwise similarity scores and cluster the datasets using the force-directed placement algorithm. The result is shown in \autoref{fig:diversity-graph}. Nodes close in this graph have a high word overlap, while nodes far away in the graph have a low overlap. From \autoref{fig:heatmap}, we observe a rather low weighted Jaccard word overlap between the different domains, indicating that \beir is a challenging benchmark and to perform well on it, retrieval approaches must generalize well to diverse text types.

\begin{figure}[htb!]
\centering
\begin{center}
    \includegraphics[trim=0 0 0 0,clip,width=0.35\textwidth]{figures/diversity-graph.png}
    \captionof{figure}{A 2D-visualization of the pairwise weighted jaccard similarity scores (\autoref{fig:heatmap}) using the force-directed placement algorithm with NetworkX \cite{SciPyProceedings_11}. We color datasets differently for different domains.}
    \label{fig:diversity-graph}
\end{center}
\vspace*{-\baselineskip}
\end{figure}


\section{Experimental Setup}
We now describe our experimental setup which we use to benchmark different state-of-the-art retrieval architectures and models. 

\subsection{Retrieval Models}
In our work, we include a broad spectrum of retrieval models which we believe cover diverse settings and architectures. We focus on selecting models based on its performance, contemporaneity and popularity. 
We try to include publicly available pre-trained checkpoints from a practical point of view, which could lead to different model configurations. 
Unless specified otherwise, we concatenate the title and passage together with a single white-space across the models. 
We group the models based on their retrieval architecture: we select one lexical, one sparse, six dense and two re-ranking models, leading to a total of nine different retrieval approaches. Besides the included ones, in future, models can be easily incorporated and compared within the benchmark. All model checkpoints can be found in \autoref{tab:model_links}.

\subsubsection{Lexical Model}

\paragraph{\textbf{BM25}} \cite{bm25} is a commonly-used  bag-of-words retrieval function based on token-matching between two high-dimensional sparse vectors with TF-IDF token weights. Due to its strong and efficient performance, it is the de-facto industrial standard. We use the implementation available on Elasticsearch\footnote{\href{https://www.elastic.co}{https://www.elastic.co}} with the default settings. We index the title (if available) and passage of each document as separate text fields for retrieval.


\subsubsection{Sparse Model}

\paragraph{\textbf{SPARTA}} \cite{zhao2020sparta} computes the similarity between the non-contextualized word piece embeddings from BERT and the contextualized word piece embeddings for a given document. Due to the finite size of 30k non-contextualized embeddings, the similarity scores can be pre-computed for a given document, which results in a 30k dimensional sparse vector. As the original implementation is not publicly available, we re-implement the approach. We fine-tune a DistilBERT \cite{sanh2020distilbert} model on MSMARCO \cite{nguyen2016ms} and use sparse-vectors with 2,000 non-zero entries. 


\begin{table*}[t!]
    \small
    \ra{1.05}
    \resizebox{\textwidth}{!}{\begin{tabular}{l | c | c | c c c c c | c c}
        \toprule
        \multicolumn{1}{l}{\textbf{Model ()}} &
        \multicolumn{1}{c}{Lexical}   &
        \multicolumn{1}{c}{Sparse}   &
        \multicolumn{5}{c}{Dense / Neural} &
        \multicolumn{2}{c}{Reranking} \\ 
        \cmidrule(lr){1-1}
        \cmidrule(lr){2-2}
        \cmidrule(lr){3-3}
        \cmidrule(lr){4-8}
        \cmidrule(lr){9-10}
        \multicolumn{1}{l}{\textbf{Dataset ()}} &
        \multicolumn{1}{c}{\textbf{BM25}} &
        \multicolumn{1}{c}{\textbf{SPARTA}} &
        \multicolumn{1}{c}{\textbf{USE-QA}} &
        \multicolumn{1}{c}{\textbf{DPR}} &
        \multicolumn{1}{c}{\textbf{ANCE}} &
        \multicolumn{1}{c}{\textbf{SBERT}} &
        \multicolumn{1}{c}{\textbf{GenQ}} &
        \multicolumn{1}{c}{\textbf{BM25+CE}} &
        \multicolumn{1}{c}{\textbf{ColBERT}} \\
        \midrule

   MSMARCO & 0.218 & 0.351 & 0.259 & 0.177 & 0.388 & \underline{0.389} & \underline{0.389} & 0.384 & \textbf{0.425} \\  \midrule \midrule
   
   
   TREC-COVID    & 0.616 & 0.538 & 0.528 & 0.332 & 0.654 & 0.482 & 0.554 & \underline{0.667} & \textbf{0.677} \\
   BioASQ        & \textbf{0.514} & 0.351 & 0.093 & 0.127 & 0.306 & 0.295 & 0.351 & \underline{0.489} & 0.474 \\
   NFCorpus      & 0.297 & 0.301 & 0.252 & 0.189 & 0.237 & 0.257 & 0.293 & \underline{0.303} & \textbf{0.305} \\ \midrule
   NQ            & 0.310 & 0.398 & 0.180 & 0.474 & 0.446 & 0.450 & 0.360 & \underline{0.516} & \textbf{0.524} \\ 
   HotpotQA      & \underline{0.601} & 0.492 & 0.258 & 0.391 & 0.456 & 0.513 & 0.497 & \textbf{0.701} & 0.593 \\ 
   FiQA-2018     & 0.239 & 0.198 & 0.264 & 0.112 & 0.295 & 0.258 & 0.284 & \textbf{0.326} & \underline{0.317} \\ \midrule
   Signal-1M (RT)& \textbf{0.388} & 0.252 & 0.241 & 0.155 & 0.249 & 0.261 & 0.257 & \underline{0.308} & 0.274 \\ \midrule
   TREC-NEWS     & 0.371 & 0.258 & 0.340 & 0.161 & 0.382 & 0.367 & 0.369 & \textbf{0.430} & \underline{0.393} \\ \midrule
   ArguAna       & \underline{0.441} & 0.279 & 0.048 & 0.175 & 0.415 & 0.429 & \textbf{0.517} & 0.313 & 0.233 \\  
   T\'ouche-2020 & \textbf{0.605} & 0.231 & 0.252 & 0.127 & 0.284 & 0.249 & 0.226 & \underline{0.378} & 0.275 \\ \midrule 
   CQADupStack   & 0.316 & 0.257 & 0.236 & 0.153 & 0.296 & 0.306 & 0.339 & \underline{0.348} & \textbf{0.350} \\
   Quora         & 0.742 & 0.630 & 0.767 & 0.248 & 0.852 & \textbf{0.855} & 0.853 & 0.778 & \underline{0.854} \\ \midrule
   DBPedia       & 0.288 & 0.314 & 0.238 & 0.263 & 0.281 & 0.339 & 0.320 & \underline{0.380} & \textbf{0.392} \\ \midrule
   SCIDOCS       & \textbf{0.156} & 0.126 & 0.104 & 0.077 & 0.122 & 0.133 & 0.148 & \underline{0.154} & 0.145 \\ \midrule
   FEVER         & 0.648 & 0.596 & 0.546 & 0.562 & 0.669 & 0.670 & 0.641 & \textbf{0.793} & \underline{0.771} \\ 
   Climate-FEVER & 0.179 & 0.082 & 0.113 & 0.148 & 0.198 & 0.205 & \underline{0.220} & \textbf{0.246} & 0.184 \\ 
   SciFact       & \underline{0.620} & 0.582 & 0.312 & 0.318 & 0.507 & 0.531 & 0.592 & 0.524 & \textbf{0.671} \\ \midrule
   AVERAGE       & 0.419 & 0.346 & 0.280 & 0.233 & 0.391 & 0.388 & 0.401 & \textbf{0.447} & \underline{0.437} \\ 
        \bottomrule
    \end{tabular}}
    \caption{In-domain and Zero-shot retrieval performances on \beir datasets. All scores denote \textbf{nDCG@10}. The best retrieval performance on a given dataset is marked in \textbf{bold}, and the second best performance is \underline{underlined}. Corresponding Recall@100 performances can be found in \autoref{tab:results-recall}.  indicates the in-domain performances.}
    \label{tab:results}
\end{table*}

\subsubsection{Dense Models}

\paragraph{USE-QA} \cite{yang2020multilingual} is a two-tower bi-encoder encoding the query and document separately. The model has been fine-tuned on multiple online question-answering forums and the SNLI \cite{bowman-etal-2015-large} corpus. The documents are encoded as a sentence-level input with additional context information and are retrieved using cosine-similarity. We utilize the English USE-QA model\footnote{\href{https://tfhub.dev/google/universal-sentence-encoder-qa/3}{https://tfhub.dev/google/universal-sentence-encoder-qa/3}}. We substitute the title as the input; passage as the surrounding context for the original response encoder. For datasets without document titles, we substitute the passage in both the fields. 

\paragraph{DPR} \cite{karpukhin-etal-2020-dense} is a two-tower bi-encoder. The model uses \texttt{[CLS]} token pooling and retrieves similar documents using dot-product. 
We experimented and found the Multi model\footnote{\href{https://huggingface.co/transformers/model_doc/dpr.html}{https://huggingface.co/transformers/model\_doc/dpr.html}} to perform better compared to the NQ model fine-tuned only on Natural Questions \cite{47761}. The Multi-DPR model was fine-tuned on four QA datasets: \cite[NQ;][]{47761}, \cite[TriviaQA;][]{joshi-etal-2017-triviaqa}, \cite[WebQuestions;][]{berant-etal-2013-semantic} and \cite[CuratedTREC;][]{baudivs2015modeling}. The model requires a title for retrieval and concatenates the title and passage together with a \texttt{[SEP]} token. However, we keep the title field empty for evaluation if unavailable for a dataset.

\paragraph{ANCE} \cite{xiong2020approximate} is a siamese bi-encoder encoding the query and document together in a common embedding space. The model uses \texttt{[CLS]} token pooling and retrieves similar documents using dot-product. ANCE constructed negatives from an Approximate Nearest Neighbor (ANN) index of the corpus, which in parallel was updated to select negative training instances during fine-tuning of the model. We use the open-sourced RoBERTa \cite{liu2019roberta} model\footnote{\href{https://github.com/microsoft/ANCE}{https://github.com/microsoft/ANCE}} fine-tuned on MSMARCO \cite{nguyen2016ms} for 600K steps.

\paragraph{SBERT}\label{sec:sbert_model} \cite{reimers-gurevych-2019-sentence} is a siamese bi-encoder using mean pooling for encoding and cosine-similarity for retrieval. We fine-tune a DistilBERT \cite{sanh2020distilbert} model\footnote{\href{https://www.sbert.net/docs/pretrained_models.html}{https://www.sbert.net/docs/pretrained\_models.html}} on MSMARCO \cite{nguyen2016ms}. During fine-tuning, we include in-batch negatives \cite{gillick2018endtoend} together with hard-negatives as provided by MSMARCO. Then, for all train queries we retrieve the top-20 most similar documents from a collection of 3 million MSMARCO passages. We use an ELECTRA \cite{Clark2020ELECTRA} cross-encoder model fine-tuned on MSMARCO (\autoref{sec:electra_model}) to filter out false positives and only keep (query, passage) pairs with a relevancy score below 0.1. We use these pairs as additional hard negatives and fine-tune the model for 10 more epochs.

\paragraph{GenQ} Similar to past work \cite{liang2020embeddingbased,ma2021zeroshot}, we propose an unsupervised domain-adaption approach for dense retrieval models using synthetic queries. First, we fine-tune a T5-base \cite{JMLR:v21:20-074} model to generate queries given a passage. We use the MSMARCO dataset and train for 2 epochs. Then, for a target corpus we generate 5 queries for each document using a combination of top-k and nucleus-sampling (top-k: 25; top-p: 0.95). Due to resource constraints, we cap the maximum number of target documents in each dataset to 100K. We found the T5 model to perform better compared to BART \cite{lewis-etal-2020-bart} and our decoding setting better as compared to beam-search. For retrieval, we continue to fine-tune the SBERT model (\autoref{sec:sbert_model}) on the synthetic queries and document pairs. Note, this creates an independent model for each task.

\subsubsection{Reranking Models}
\paragraph{BM25 + CE}\label{sec:electra_model}  We retrieve the top-100 relevant documents using BM25 and rerank them using a ELECTRA cross-encoder model \cite{Clark2020ELECTRA}. The model has been fine-tuned on MSMARCO \cite{nguyen2016ms} using the approach described by \cite{nogueira2020passage}. 


\paragraph{ColBERT} \cite{10.1145/3397271.3401075} independently encodes the query and the document into a bag of token embeddings. The model first top-k retrieves candidates through approximate nearest neighbor search using faiss\footnote{\href{https://github.com/facebookresearch/faiss}{https://github.com/facebookresearch/faiss}} \cite{JDH17} and then reranks those by computing the sum of maximum cosine similarity between the query and document token embeddings. We use the provided implementation\footnote{\href{https://github.com/stanford-futuredata/ColBERT}{https://github.com/stanford-futuredata/ColBERT}} and retrieve the top-100 relevant documents using faiss. For reranking, we use a \textit{bert-base-uncased} model fine-tuned on the MSMARCO dataset for 300K steps.


\subsection{Evaluation Metric}

Depending upon the nature and requirements of real-world applications, retrieval tasks can be either be precision or recall focused. To obtain comparable results across models and datasets in \beir, we argue that it is important to leverage a single evaluation metric that can be computed comparably across all tasks. Decision support metrics such as Precision and Recall which are both rank unaware are not suitable. Binary rank-aware metrics such as MRR (Mean Reciprocal Rate) and MAP (Mean Average Precision) fail to evaluate tasks with graded relevance judgements. We find that \textbf{Normalised Cumulative Discount Gain} (nDCG@k) provides a good balance suitable for both tasks involving binary and graded relevance judgements.\footnote{Note that we additionally provide other metrics like Precision, Recall and MAP (Mean Average Precision) in \beir.} We refer the reader to \newcite{wang2013theoretical} for understanding the theoretical advantages of the metric. For our experiments, we utilize the Python interface of the official TREC evaluation tool\footnote{\href{https://github.com/cvangysel/pytrec\_eval}{https://github.com/cvangysel/pytrec\_eval}} \cite{VanGysel2018pytreceval} and compute nDCG@10 for all datasets.


\begin{figure}[t!]
\centering
\begin{center}
    \includegraphics[trim=60 15 25 17,clip,width=0.48\textwidth]{figures/zero-shot-results.pdf}
    \captionof{figure}{Comparison of zero-shot neural retrieval performances with BM25. Only Reranking-based models, i.e., BM25+CE and ColBERT outperform BM25 on more than half of the evaluation datasets in \beir.}
    \label{fig:results}
\end{center}
\vspace{-5mm}
\end{figure}

\section{Zero-Shot Evaluation Results}

In \autoref{tab:results}, we report the nDCG@10 results for the retrieval approaches on a single in-domain dataset (MSMARCO) and 17 zero-shot datasets. \autoref{fig:results} shows on how many datasets the neural approaches outperform (green) and underperform (red) our BM25 baseline in the zero-shot evaluation setup. Overall, we observe that in-domain performance is not a good indicator of the generalization capability as discussed below:


\textbf{Lexical Model} \quad Although BM25 underperforms neural approaches in the in-domain evaluation setting, the zero-shot results indicate that it is a strong baseline for generalization. It even achieves the best performance in 4 out of 17 tasks.


\textbf{Sparse Model}\label{sparse_models} \quad SPARTA outperforms BM25 by a large margin on the in-domain dataset, but performs poorly on \beir datasets. It beats BM25 only for 3 out of 17 datasets with strong performance drops for other datasets.


\textbf{Dense Models} \quad Dense retrieval approaches perform well only for certain datasets, while for others, there are strong performance drops compared to BM25. DPR and USE-QA perform poorly on nearly all the datasets. In contrast, SBERT and ANCE perform well for 6 and 7 datasets respectively. As we will show in \autoref{sec:domain-shift}, these approaches fine-tuned on the MSMARCO dataset perform well if the domain is similar to the training dataset. Interestingly, GenQ, which further fine-tunes SBERT on synthetically generated query data is able to capture vital domain information. It outperforms SBERT on specialised domains such as scientific, finance or StackExchange domains. However, it underperforms SBERT on tasks over broad domains such as Wikipedia. 
We intuitively believe this is due to catastrophic forgetting during fine-tuning of GenQ with synthetic queries.

\textbf{Reranking Models} \quad The experiments show strong results for both the re-ranking strategies: BM25 + CE and ColBERT. Both techniques achieve the best performances. They both outperform BM25 in 11 out of 17 datasets. This indicates re-ranking methods involving a cross-encoder generalize well to unseen datasets and domains, which is in line with the findings of other recent work~\citep{akkalyoncu-yilmaz-etal-2019-cross,rueckle-etal-2020-multicqa,thakur2020augmented}. 


\begin{table}[t!]
    \small
    \ra{1.2}
    \resizebox{0.48\textwidth}{!}{\begin{tabular}{c l | c | c  c  c}
        \toprule
\multicolumn{3}{c}{\textbf{DBPedia (1 Million)}} &
        \multicolumn{2}{|c}{\textbf{Retrieval Latency}} &
        \multicolumn{1}{c}{\textbf{Index}} \\
        \cmidrule(lr){1-3}
        \cmidrule(lr){4-5}
        \cmidrule(lr){6-6}
        \multicolumn{1}{c}{\textbf{Rank}} &
        \multicolumn{1}{l}{\textbf{Model}} &
        \multicolumn{1}{|c|}{\textbf{Dim.}} &
        \multicolumn{1}{c}{\textbf{GPU}} & 
        \multicolumn{1}{c}{\textbf{CPU}} &
        \multicolumn{1}{c}{\textbf{Size}} \\ \midrule
        (1) & BM25+CE      & 768 & 550ms& 7100ms & 0.4GB \\
        (2) & ColBERT      & 128 & 350ms& -- & 20GB \\
        (3) & BM25         & --  &   -- & 20ms& 0.4GB \\
        (4) & GenQ         & 768 & 14ms & 125ms& 3GB \\
        (5) & ANCE         & 768 & 20ms & 275ms& 3GB \\
        (6) & SBERT        & 768 & 14ms & 125ms& 3GB \\
        (7) & SPARTA       & 2000&  --  & 20ms & 12GB \\
        (8) & USE-QA       & 512 & 35ms & 75ms & 2GB \\
        (9) & DPR          & 768 & 19ms & 230ms & 3GB \\
        \bottomrule
    \end{tabular}}
    \caption{Estimated retrieval latency for a single query and index sizes for 1 million documents in DBPedia. Ranked from best to worst zero-shot performance. Lower the latency and size is better.}
    \label{tab:efficiency}
    \vspace*{-\baselineskip}
\end{table}


\section{Retrieval Latency and Index Sizes}

In this section, we compare different retrievers from a practical viewpoint. Models need to potentially compare a single query against millions of documents, hence, a high computational speed for retrieving results in real-time is desired. Pre-computation and storing documents in indexes is vital at inference or during deployment on production systems. For our comparison, we randomly sample 1M documents from DBPedia \cite{Hasibi:2017:DVT}. We evaluate retrieval for dense models using exact-search with an exception for ColBERT where we followed the original implementation \cite{10.1145/3397271.3401075}. For both reranking approaches we evaluate performances for top-100 hits. CPU performances were measured with an 8 core Intel Xeon Platinum 8168 CPU~@~2.70GHz. GPU performances were measured using a single Nvidia Tesla V100, CUDA 11.0 and cuDNN. Results are shown in \autoref{tab:efficiency}.

\paragraph{Retrieval Latency} The good generalization capabilities of the both reranking approaches come at the cost of high computational overhead, being slowest at inference. High latency of 0.4 and 0.6 seconds are neither desirable nor practical for many applications. In contrast, dense retrieval models are about 20-30x faster compared to the reranking models, enabling real-time responses. On CPU, the simpler model architectures such as SPARTA and BM25 dominate in terms of speed. Note, we did not specifically speed-optimize the implementations and used them as provided. With further optimizations, latency can potentially be reduced for all approaches, however the observed behaviour of the approaches will generally be the same.

\paragraph{Index Sizes} Besides speed, the size of the index is a critical factor as well since it is usually stored in memory. Out of all the models, BM25 requires the smallest memory to store the index. SPARTA has the second largest index sizes, as it stores a 2000 dimensional sparse vector for each document, and ColBERT has the largest index size, as it stores a 128 dimensional dense vector for each document token. Index size is especially relevant for larger document collections: ColBERT stores an index of 900GB size for BioASQ with around 15M documents, while BM25 uses 18GB to store its index. 

\section{Analysis}\label{sec:analysis}
This section attempts to analyse a few retrieval techniques in depth and uncover interesting insights observed in our zero-shot experiments.

\paragraph{Dataset Annotation Biases}\label{sec:dataset_biases} The biggest performance drops in both dense and re-ranking techniques compared to BM25 are found in BioASQ, Signal-1M (RT) and T\'ouche-2020 datasets. One possible reason lies in the construction of the datasets itself. In BioASQ\footnote{Please see details at \url{http://participants-area.bioasq.org/general_information/Task8b/}}, document candidates are retrieved for annotation via term-matching with boosting tags \cite{tsatsaronis2015overview}. Further, the annotation depth is shallow (approximately 5 relevant articles per query), whereas the total number of documents is around 15M, which favours lexical term-matching systems like BM25 \cite{liang2020embeddingbased}. Creation of Signal-1M (RT), involved retrieving tweets for a query using 8 different retrieval systems for annotation. 7 out of these 8 techniques relied upon term-matching signals and we suspect this causes a bias towards lexical annotated documents \cite{Signal1MRelatedTweetsRetrieval2018}. With T\'ouche-2020, we believe one of the reasons lies in the skewed length distribution of relevant documents as shown in \autoref{fig:touche-task}. All neural approaches have limitations with document lengths as they have limit of 512 word pieces. On the other hand BM25 greatly profits from longer documents as more keywords occur in them thereby increasing the chances of lexical overlap. 

\begin{figure}[htb!]
\centering
\begin{center}
    \includegraphics[trim=7 7 10 10,clip,width=0.48\textwidth]{figures/touche-doc-lengths.pdf}
    \captionof{figure}{Annotated relevant document lengths (in words) for T\'ouche-2020 \cite{stein:2020v}. Majority of the highly relevant documents are greater than 300-350 words providing a better score with BM25.}
    \label{fig:touche-task}
    \vspace*{-\baselineskip}
\end{center}
\end{figure}

\paragraph{Domain Shift}\label{sec:domain-shift} 
The MSMARCO \cite{nguyen2016ms} dataset is used for fine-tuning six out of the nine retrieval models. Similar to \newcite{shah-etal-2018-adversarial} setup, we calculate the proportion of n-grams shared between the source (MSMARCO) and the target datasets present in \beir. We calculate the overlap using a Weighted Jaccard Similarity score \cite{ioffe2010improved}. From \autoref{tab:domain-shift}, we observe that \emph{dense} models are unable to generalize on target datasets with a small weighted Jaccard similarity score with MSMARCO, thereby under-performing \emph{BM25} on these datasets. To analyse the correlation, we calculate the Spearman's rank correlation () between the difference in performances of the dense and lexical-based models ranked accordingly from high to low unigram overlap.\footnote{We drop \textit{BioASQ}, \textit{Signal-1M (RT)} and \textit{T\`ouche-2020} datasets in the calculation of Spearman's rank correlation due to annotation biases as discussed in \autoref{sec:dataset_biases}.} We achieve a correlation score () of 0.77 for SBERT and 0.71 for ANCE denoting a high correlation between domain overlap and the performance of dense retrievers.


\begin{table}[t!]
    \ra{1.1}
    \small
    \resizebox{.48\textwidth}{!}{\begin{tabular}{l  c  c  c  c  c}
    \toprule
     \multicolumn{1}{l}{\textbf{Dataset}} &
     \multicolumn{2}{c}{\textbf{Avg. Jaccard Sim (\%)}} &
     \multicolumn{3}{c}{\textbf{nDCG@10}} \\
     \cmidrule(lr){1-1}
     \cmidrule(lr){2-3}
     \cmidrule(lr){4-6}
    \textbf{Target ()} &  \textbf{Unigrams} & \textbf{Bigrams} & \textbf{BM25} & \textbf{ANCE} & \textbf{SBERT} \\ 
    \midrule
NQ            & 47.27 & 20.49 & 0.310 & \colorbox{green!20}{0.446} & \colorbox{green!20}{0.450} \\
Quora         & 39.75 & 8.69  & 0.742 & \colorbox{green!20}{0.852} & \colorbox{green!20}{0.855} \\
TREC-NEWS     & 38.90 & 14.77 & 0.371 & \colorbox{green!20}{0.382} & \colorbox{red!30}{0.367} \\
T\'ouche-2020 & 37.02 & 14.05 & 0.605 & \colorbox{red!30}{0.284}   & \colorbox{red!30}{0.249} \\
FiQA-2018     & 35.95 & 9.78  & 0.239 & \colorbox{green!20}{0.295} & \colorbox{green!20}{0.258} \\
FEVER         & 34.79 & 14.99 & 0.648 & \colorbox{green!20}{0.669} & \colorbox{green!20}{0.670} \\
Climate-FEVER & 34.79 & 14.99 & 0.179 & \colorbox{green!20}{0.198} & \colorbox{green!20}{0.205} \\
ArguAna       & 32.92 & 4.78  & 0.441 & \colorbox{red!30}{0.415}   & \colorbox{red!30}{0.429} \\ 
Signal-1M (RT)& 32.15 & 8.64  & 0.388 & \colorbox{red!30}{0.249}   & \colorbox{red!30}{0.261} \\
HotpotQA      & 30.87 & 12.55 & 0.601 & \colorbox{red!30}{0.456}   & \colorbox{red!30}{0.513} \\
CQADupStack   & 30.64 & 10.16 & 0.316 & \colorbox{red!30}{0.296}   & \colorbox{red!30}{0.306} \\
DBPedia       & 30.16 & 11.88 & 0.288 & \colorbox{red!30}{0.281}   & \colorbox{green!20}{0.339} \\
SCIDOCS       & 27.92 & 5.31 & 0.156  & \colorbox{red!30}{0.122}   & \colorbox{red!30}{0.133} \\
BioASQ        & 27.10 & 10.03 & 0.514 & \colorbox{red!30}{0.306}   & \colorbox{red!30}{0.295} \\
TREC-COVID    & 26.80 & 7.36 & 0.616  & \colorbox{green!20}{0.654} & \colorbox{red!30}{0.482} \\
NFCorpus      & 23.45 & 3.09 & 0.297  & \colorbox{red!30}{0.237}   & \colorbox{red!30}{0.257} \\
SciFact       & 22.16 & 2.92 & 0.620  & \colorbox{red!30}{0.507}   & \colorbox{red!30}{0.531} \\ \bottomrule
     \end{tabular}
    }
    \centering
    \caption{Domain shift analysis with MSMARCO \cite{nguyen2016ms}. Datasets are sorted from high to low unigram weighted Jaccard similarity \cite{ioffe2010improved}. ANCE and SBERT outperform BM25 (green) more often when MSMARCO and the target dataset share a high domain overlap.} 
    \label{tab:domain-shift}
    \vspace*{-\baselineskip}
\end{table}

\paragraph{Dot Product vs.\ Cosine Similarity} Dense models require a similarity function to retrieve relevant documents for a given query within an embedding space. This similarity function is also used during training dense models with the InfoNCE \cite{oord2019representation} loss:

using  in-batch negatives for each query  and a scaling factor . where  denotes the relevant (positive) document for query . Commonly used similarity functions () are cosine-similarity (used by USE-QA and SBERT) and dot-product (used by ANCE and DPR). 

As far as we know, the differences between the two similarity functions were not systematically studied before. The performances of SBERT and ANCE with in-domain evaluation on MSMARCO are nearly identical. However, in the zero-shot evaluation setup, the two models can perform vastly different: on TREC-COVID, ANCE outperforms SBERT by 17.2 points, whereas SBERT outperforms ANCE by 9.7 points on HotpotQA.

We re-ran the training of SBERT (\autoref{sec:sbert_model}) and only changed the similarity function from cosine-similarity to dot-product. As shown in \autoref{tab:trec-covid} and \autoref{tab:dot-vs-cosine} in the appendix, we observe significant performance differences for some datasets. For TREC-COVID, the model trained with dot-product achieves the biggest improvement with 15.3 points, while for a majority of other datasets, it performs worse than the cosine-similarity model.



We observe that these (nearly) identical models retrieve documents with vastly different lengths as shown in the violin plots in \autoref{tab:trec-covid} and \autoref{fig:violin-plots-all} in the appendix. For all datasets, we find the cosine-similarity model to prefer shorter documents over longer ones. This is especially severe for TREC-COVID: a large fraction of the scientific papers (approx.~42k out of 171k) consist only of publication titles without an abstract. The cosine-similarity model prefers retrieving these documents. In contrast, the dot-product model primarily retrieves longer documents, i.e., publications with an abstract. Cosine-similarity uses vectors of unit length, thereby having no notion of the encoded text length. In contrast, for dot-product, longer documents can result in vectors with higher magnitudes which can yield higher similarity scores for a query.

As we previously saw in \autoref{fig:touche-task}, relevance scores are not uniformly distributed over document lengths: for some datasets, longer documents are annotated with higher relevancy scores, while in others, shorter documents are. This can be either due to the annotation process, e.g., the candidate selection method prefers short or long documents, or due to the task itself, where shorter or longer documents could be more relevant to the user information need. Hence, it can be more advantageous to train a model with either cosine-similarity or dot-product depending upon the nature and needs of the specific task.


\begin{table}[htb!]
    \small
    \includegraphics[trim=5 5 5 20,clip,width=0.48\textwidth]{figures/violin-plots/dot-vs-cosine-main-paper.pdf}
    \resizebox{0.48\textwidth}{!}{\begin{tabular}{c c c c c c}

        \multicolumn{2}{c}{\textbf{TREC-COVID}} &
        \multicolumn{2}{c}{\textbf{Signal-1M (RT)}} &
        \multicolumn{2}{c}{\textbf{FEVER}} \\
        \cmidrule(lr){1-2}
        \cmidrule(lr){3-4}
        \cmidrule(lr){5-6}
        \textbf{Cosine-Sim.} & \textbf{Dot-Prod.} & \textbf{Cosine-Sim.} & \textbf{Dot-Prod.} & \textbf{Cosine-Sim.} & \textbf{Dot-Prod.} \\ \midrule
        
        0.482 & \textbf{0.635} & \textbf{0.261} & 0.243 & 0.670 & \textbf{0.685} \\ \midrule
    \end{tabular}}
    \caption{Violin plots \cite{citeulike:4075875} of document lengths for the top-10 retrieved hits and nDCG@10 scores using SBERT (\autoref{sec:sbert_model}) model trained with either cosine similarity (blue, top) or dot product (orange, bottom).}
    \label{tab:trec-covid}
    \vspace*{-\baselineskip}
\end{table}

\section{Conclusion}

In this work, we presented \beir: a heterogeneous benchmark for information retrieval. In contrast to the existing benchmarks, we provided a broader selection of target tasks ranging from narrow expert domains to open domain datasets. We included 9 different retrieval tasks spanning 17 diverse datasets. \beir marks the first large scale benchmark for the evaluation of zero-shot transfer capabilities of retrieval models to a wide range of settings.

We broadly studied the effectiveness of nine different retrieval models on \beir. We show that \emph{BM25} is a strong baseline for zero-shot evaluation. We demonstrated, that in-domain performance cannot predict how well an approach will generalize in a zero-shot setup. Overall, we demonstrate \emph{Reranking}-based models generalize well across all the datasets, at the cost of slower retrieval times. \emph{Dense} models outperform \emph{BM25} when a high domain overlap is present between the training dataset and the evaluated target dataset. \emph{Dense} models, similar to BM25, are fast in retrieval and produce light-weight indexes.


\section{Limitations of BEIR}

Even though we cover a wide range of tasks and domains in \beir, no benchmark is perfect and has its limitations. Making those explicit is a critical point in understanding the results on the benchmark and, for future work, to improve up-on the benchmark. 

\paragraph{Multilingual Tasks} Currently, our benchmark is focused on English tasks.  We would like to extend \beir to include multi- and cross-lingual tasks and models.

\paragraph{Long Document Retrieval} Most of our tasks have average document lengths up-to a few hundred words roughly equivalent to a few paragraphs. Including tasks that require the retrieval of longer documents would be highly relevant. However, as transformer-based approaches often have a length limit of 512 word pieces, a fundamental different setup would be required to compare approaches.

\paragraph{Multi-factor Search} Until now, we focused on pure textual search in \beir. In many real-world applications, further signals are used to estimate the relevancy of documents, such as PageRank \cite{ilprints422}, recency \cite{10.1145/1772690.1772725}, authority score \cite{10.1145/324133.324140} or user-interactions such as click-through rates \cite{10.1145/1458082.1458092}. The integration of such signals in the tested approaches is often not straight-forward and is an interesting direction for research.

\paragraph{Multi-field Retrieval} Retrieval can often be performed over multiple fields. For example, for scientific publication we have the title, the abstract, the document body, the authors list, and the journal name. So far we focused only on datasets that have one or two fields.

\paragraph{Personalised Search} Document relevancy can vary with a user personal interest or preference \cite{ghorab2013personalised}. We currently rank relevant documents from a single global annotated relevant or not-relevant label. In future, we would like to incorporate personal preferences within the benchmark.

\section{Future Work}

In this work, we presented \beir together with results and analysis for nine different retrieval methods. However, \beir offers plenty more opportunities for future work. Some of the most interesting are: Understanding why approaches generalize differently across tasks and how to improve the zero-shot performance for approaches? Further, it would be interesting to be able to predict when a certain pre-trained model performs well and when it does not? As we have shown, re-ranking approaches generally perform better than other approaches, but come at a higher computational cost: Understanding how to reduce the computational overhead while maintaining the good generalization capability would be of high practical value.

\section*{Acknowledgement}
This work has been supported by the German Federal Ministry of Education and Research (BMBF) under the promotional reference 03VP02540 (ArgumenText), by the German Research Foundation through the German-Israeli Project Cooperation (DIP, grant DA 1600/1-1 and
grant GU 798/17-1) and has been funded by the German Federal Ministry of Education and Research and the Hessian Ministry of Higher Education, Research, Science and the Arts within their joint support of the National Research Center for Applied Cybersecurity ATHENE. We would like to thank Tilman Beck, Kexin Wang and Jorge Cardona for their feedback.



\bibliography{custom}
\bibliographystyle{acl_natbib}



\appendix

\section{Appendices}

In this Appendix, we mention the following sections in detail: Training and in-domain evaluation (\ref{sec:training_dataset}), descriptions of all zero-shot tasks and datasets in \beir (\ref{sec:datasets}). We further explain how we calculate weighted jaccard similarity (\ref{sec:weighted_jaccard_similarity}) and capped recall at k (\ref{sec:capped_recall_score}) metrics. 


\section{Training and In-domain Evaluation}\label{sec:training_dataset}
We use the MSMARCO Passage Ranking dataset \cite{nguyen2016ms}, which contains 8.8M Passages and an official training set of 532,761 query-passage pairs for fine-tuning for a majority of retrievers. The dataset contains queries from Bing search logs with one text passage from various web sources annotated as relevant. We find the dataset useful for training, in terms of covering a wide variety of topics and providing the highest number of training pairs. It has been extensively explored and used for fine-tuning dense retrievers in recent works \cite{nogueira2020passage,gao2020complementing,ding2020rocketqa}. We use the official MSMARCO development set for our in-domain evaluation
which has been widely used in prior research \cite{nogueira2020passage, gao2020complementing,ding2020rocketqa}. It has 6,980 queries. Most of the queries have only 1 document judged relevant; the labels are binary.


\section{Zero-shot Evaluation Tasks}\label{sec:datasets}

Following the selection criteria mentioned in \autoref{sec_beir_benchmark}, we include 17 evaluation datasets that span over 9 heterogeneous tasks. Each dataset contains a document corpus denoted by  and test queries for evaluation denoted by . We additionally provide website-links in \autoref{tab:dataset_links} and intuitive examples in \autoref{tab:examples}. We now summarise each task and describe each dataset included for evaluation in \beir:


\subsection{Bio-Medical Information Retrieval}
Bio-medical information retrieval is the task of searching relevant scientific documents such as research papers or blogs for a given scientific query in the biomedical domain \cite{jiang2007empirical}. We consider a scientific query as \textit{input} and retrieve bio-medical documents as \textit{output}.

\paragraph{TREC-COVID} \cite{10.1145/3451964.3451965} is an ad-hoc search challenge based on the CORD-19 dataset containing scientific articles related to the COVID-19 pandemic \cite{wang2020cord19}. We include the July 16, 2020 version of CORD-19\footnote{\href{https://www.semanticscholar.org/cord19}{https://www.semanticscholar.org/cord19}} as corpus  and use the final cumulative judgements of the original task as queries .

\paragraph{NFCorpus} \cite{boteva2016} contains natural language queries harvested from NutritionFacts\footnote{\href{https://nutritionfacts.org}{https://nutritionfacts.org}} (NF). We use the original splits provided alongside all content sources from NF (videos, blogs, and Q\&A posts) as queries  and annotated medical documents from PubMed\footnote{\href{https://pubmed.ncbi.nlm.nih.gov}{https://pubmed.ncbi.nlm.nih.gov}} as corpus .

\paragraph{BioASQ}\cite{tsatsaronis2015overview} Task 8b is a biomedical semantic QA challenge. We use the original train and test splits provided in Task 8b as queries  and collect around 15M articles from PubMed provided in Task 8a as our corpus . 

\subsection{Question Answering}

Retrieval in open domain question answering \cite{chen-etal-2017-reading} is the task of retrieving the correct answer for a question, without a predefined location for the answer. In open-domain tasks, model must retrieve over an entire knowledge source (such as Wikipedia). We consider the question as \textit{input} and the passage containing the answer as \textit{output}.

\paragraph{Natural Questions} \cite{47761} contains Google search queries and documents with paragraphs and answer spans within Wikipedia articles. We did not use the NQ version from ReQA \cite{ahmad-etal-2019-reqa} as it focused on queries having a short answer. As a result, we parsed the HTML of the original NQ dataset and include more complex development queries that often require a longer passage as answer compared to ReQA. We filtered out queries without an answer, or having a table as an answer, or with conflicting Wikipedia pages. We retain 2,681,468 passages as our corpus  and 3452 test queries  from the original dataset.

\paragraph{HotpotQA} \cite{yang-etal-2018-hotpotqa} contains questions which require reasoning over multiple paragraphs to find the correct answer. We include the full-wiki task setting: utilizing processed Wikipedia passages as corpus . We held out randomly sampled 5447 queries from training as development split. We use the original task's development set as our test queries .

 \paragraph{FiQA-2018} \cite{10.1145/3184558.3192301} Task 2 consists of opinion-based QA. We include financial data by crawling StackExchange posts under the Investment topic from 2009-2017 as our corpus . We randomly sample out 500 and 648 as queries  from the training split as development and test splits respectively.

\subsection{Tweet Retrieval}
Twitter is a popular micro-blogging website on which people post real-time messages (i.e.\ tweets) about their opinions on a variety of topics and discuss current issues. We consider a news headline as \textit{input} and retrieve relevant tweets as \textit{output}.

\paragraph{Signal-1M Related Tweets} \cite{Signal1MRelatedTweetsRetrieval2018} includes 97 news articles from the Signal-1M dataset \cite{Signal1M2016} as queries . We construct our twitter corpus  by manually scraping tweets from given tweet-ids using Tweepy\footnote{\href{https://www.tweepy.org}{https://www.tweepy.org}}.

\subsection{News Retrieval}

\paragraph{TREC-NEWS} \cite{soboroff2019trec} 2019 track involves background linking: Given a news headline, we retrieve relevant news articles that provide important context or background information. We include the shared task data as our test queries  and the TREC Washington Post\footnote{\href{https://ir.nist.gov/wapo/}{https://ir.nist.gov/wapo/}} as our corpus . We convert the original exponential gain relevant judgements to linear labels for simplicity. 

\subsection{Argument Retrieval}
Argument retrieval is the task of ranking argumentative texts in a collection of focused arguments (\textit{output}) in order of their relevance to a textual query (\textit{input}) on different topics.

\paragraph{ArguAna Counterargs Corpus} \cite{wachsmuth:2018a} involves the retrieval of the best counterargument to an argument. It contains pairs of argument and counterargument scraped from the online debate portal\footnote{\href{https://idebate.org)}{https://idebate.org}} as corpus . We consider the original arguments present in the original tasks test split as our queries .

\paragraph{Touch\`e-2020} \cite{stein:2020v} Task 1 is a conversational argument retrieval task. We use the conclusion as title and premise for arguments present in args.me\footnote{\href{https://zenodo.org/record/4139439/}{https://zenodo.org/record/4139439/}} \cite{stein:2017r} as corpus . We include the shared task data as our test queries . The original relevance judgements (qrels) file also included negative judgements (-2), but for simplicity we substitute them as zero.  

\subsection{Duplicate Question Retrieval}

Duplicate question retrieval is the task of identifying duplicate questions asked in community question answering (cQA) forums. A given query is the \textit{input} and the duplicate questions are the \textit{output}.

\paragraph{CQADupStack} \cite{hoogeveen2015cqadupstack} is a benchmark dataset for community question-answering (cQA) research. The corpus  comprises of queries from 12 different StackExchange subforums. We utilize the predefined test split for our queries . We evaluate each subforum separately and report the overall mean score.

\paragraph{Quora} Duplicate Questions dataset identifies whether two questions are duplicates. Quora originally released containing 404,290 question pairs. We add transitive closures to the original dataset. Further, we split it into train, dev, and test sets with a ratio of about 85\%, 5\% and 10\% of the original pairs. We remove all overlaps between the splits and ensure that a question in one split of the dataset does not appear in any other split to mitigate the transductive classification problem \cite{10.1007/978-3-642-15880-3_42}. We finally achieve 522,931 unique queries as our corpus  and 5,000 dev and 10,000 test queries  respectively.

\subsection{Entity Retrieval}
Entity retrieval involves retrieving unique Wikipedia pages to entities mentioned in the query. This is crucial for tasks involving Entity Linking (EL). The entity-bearing query is the \textit{input} and the entity abstract and title are retrieved as \textit{output}.

\paragraph{DBPedia-Entity-v2} \cite{Hasibi:2017:DVT} is an established entity retrieval dataset. It contains a set of heterogeneous entity-bearing queries  and retrieves entities from the english part of DBpedia corpus  from October 2015. We randomly sample out 67 queries from the test split as development.

\subsection{Citation Prediction}
Citations are a key signal of relatedness between scientific papers \cite{cohan-etal-2020-specter}. In this task, the model attempts to retrieve cited papers (\textit{output}) for a given query paper title (\textit{input}).

\paragraph{SCIDOCS} \cite{cohan-etal-2020-specter} contains a corpus  of 30K held-out pool of scientific papers. We consider the original direct-citations task as our retrieval task. It includes 1k papers as queries  and 5 relevant papers and 25 (randomly selected) uncited papers for each query. 

\subsection{Fact Checking}
Fact checking verifies a claim against a big collection of evidence \cite{thorne-etal-2018-fever}. The task requires knowledge about the claim and reasoning over multiple documents. We consider the claim as \textit{input} and the relevant document passage verifying the claim as \textit{output}.

\paragraph{FEVER} \cite{thorne-etal-2018-fever} The Fact Extraction and VERification dataset is collected to facilitate the automatic fact checking. We utilize the original paper splits as queries  and retrieve evidences from the pre-processed Wikipedia Pages (June 2017 dump) as our corpus .

\paragraph{Climate-FEVER} \cite{diggelmann2020climatefever} is a dataset for verification of real-world climate claims. We include the claims as queries  and retrieve evidences from the same FEVER Wiki corpus .

\paragraph{SciFact} \cite{wadden-etal-2020-fact} retrieves abstracts from the research literature containing evidence for a given scientific claim. We use the original dev split from the task containing 300 queries as our test queries , and include documents from the original scientific corpus .


\section{Weighted Jaccard Similarity}\label{sec:weighted_jaccard_similarity}

The weighted Jaccard similarity  \cite{ioffe2010improved} is intuitively calculated as the unique word overlap for all words present in both the datasets. More formally, the normalized frequency for an unique word  in a dataset is calculated as the frequency of word  divided over the sum of frequencies of all words in the dataset. 

 is the normalized frequency of word  in the source dataset  and  for the target dataset  respectively. The weighted Jaccard similarity between  and  is defined as:

 

where the sum is over all unique words  present in datasets  and .

\section{Capped Recall@k Score}\label{sec:capped_recall_score}

Recall at  is calculated as the fraction of the relevant documents that are successfully retrieved within the top  extracted documents. More formally, the  score is calculated as:



where  is the set of queries,  is the set of relevant documents for the th query, and  is a scored list of documents provided by the model, from which top  are extracted. 

However measuring recall can be counterintuitive, if a high number of relevant documents () are present within a dataset. For example, consider a hypothetical dataset with 500 relevant documents for a query. Retrieving all relevant documents would produce a maximum  score = 0.2, which is quite low and unintuitive. To avoid this we cap the recall score () at k for datasets if the number of relevant documents for a query greater than k. It is defined as: 

 

where the only difference lies within the denominator where we compute the minimum of k and , instead of  present in the original recall.

\begin{figure*}[b]
    \centering
    \begin{center}
        \includegraphics[trim=5 5 5 80,clip,width=0.9\textwidth]{figures/heatmap.pdf}
        \captionof{figure}{Heatmap showing pairwise weighted jaccard similarity \cite{ioffe2010improved} on unigram word overlap between the document collections for all datasets in \textbf{\beir}..}
        \label{fig:heatmap}
    \end{center}
    \vspace*{-\baselineskip}
\end{figure*}

\begin{table*}[b]
    \small
    \ra{1.2}
    \resizebox{\textwidth}{!}{\begin{tabular}{ l | l }
        \toprule
        \multicolumn{1}{l|}{\textbf{Dataset}} &
        \multicolumn{1}{|c}{\textbf{Website (Link)}} \\
        \midrule
        MSMARCO & \url{https://microsoft.github.io/msmarco/} \\
        TREC-COVID & \url{https://ir.nist.gov/covidSubmit/index.html} \\
        NFCorpus & \url{https://www.cl.uni-heidelberg.de/statnlpgroup/nfcorpus/} \\
        BioASQ & \url{http://bioasq.org} \\
        NQ & \url{https://ai.google.com/research/NaturalQuestions} \\
        HotpotQA & \url{https://hotpotqa.github.io} \\
        FiQA-2018 & \url{https://sites.google.com/view/fiqa/} \\
        Signal-1M (RT) & \url{https://research.signal-ai.com/datasets/signal1m-tweetir.html} \\
        TREC-NEWS & \url{https://trec.nist.gov/data/news2019.html} \\
        ArguAna & \url{http://argumentation.bplaced.net/arguana/data} \\
        Touch\`e-2020 & \url{https://webis.de/events/touche-20/shared-task-1.html} \\
        CQADupStack & \url{http://nlp.cis.unimelb.edu.au/resources/cqadupstack/} \\
        Quora & \url{https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs} \\
        DBPedia-Entity & \url{https://github.com/iai-group/DBpedia-Entity/} \\
        SCIDOCS & \url{https://allenai.org/data/scidocs} \\
        FEVER & \url{http://fever.ai} \\
        Climate-FEVER & \url{http://climatefever.ai} \\
        SciFact & \url{https://github.com/allenai/scifact} \\
        \bottomrule
    \end{tabular}}
    \caption{Original dataset website (link) for all datasets present in \textbf{\beir}.}
    \label{tab:dataset_links}
    \vspace*{-\baselineskip}
\end{table*}

\begin{table*}[b]
    \small
    \ra{1.4}
    \resizebox{\textwidth}{!}{\begin{tabular}{ l | l }
        \toprule
        \multicolumn{1}{l|}{\textbf{Model}} &
        \multicolumn{1}{c}{\textbf{Model Checkpoints (URL)}} \\
        \midrule
        USE-QA & \url{https://tfhub.dev/google/universal-sentence-encoder-qa/3} \\
        SBERT (Cos-Sim.) & \url{https://huggingface.co/sentence-transformers/msmarco-distilbert-base-v3} \\
        SBERT (Dot-Prod.) & \url{https://huggingface.co/sentence-transformers/msmarco-distilbert-base-dot-prod-v3} \\
        ELECTRA (CE) & \url{https://huggingface.co/sentence-transformers/ce-ms-marco-electra-base} \\
        SPARTA & \url{https://huggingface.co/BeIR/sparta-msmarco-distilbert-base-v1} \\
        ANCE & \url{https://public.ukp.informatik.tu-darmstadt.de/reimers/sentence-transformers/v0.2/msmarco-roberta-base-ance-fristp.zip} \\
        DPR (Query) &  \url{https://public.ukp.informatik.tu-darmstadt.de/reimers/sentence-transformers/v0.2/facebook-dpr-question_encoder-multiset-base.zip} \\
        DPR (Context) & \url{https://public.ukp.informatik.tu-darmstadt.de/reimers/sentence-transformers/v0.2/facebook-dpr-ctx_encoder-multiset-base.zip} \\
        ColBERT & \url{https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/models/ColBERT/msmarco.psg.l2.zip} \\
        
        \bottomrule
    \end{tabular}}
    \caption{Neural model and checkpoints used for evaluation in \textbf{\beir}.}
    \label{tab:model_links}
\end{table*}



\begin{table*}[t!]
    \small
    \ra{1.5}
    \resizebox{\textwidth}{!}{\begin{tabular}{l | l | l }
        \toprule
        \multicolumn{1}{l|}{\textbf{Dataset}}    &
        \multicolumn{1}{c}{\textbf{Query}}   &
        \multicolumn{1}{|c}{\textbf{Relevant-Document}} \\
        \midrule
   MSMARCO & \multicolumn{1}{p{6cm}|}{what fruit is native to australia} & \multicolumn{1}{p{13cm}}{\textit{<Paragraph>} Passiflora herbertiana. A rare passion fruit native to Australia. Fruits are green-skinned, white fleshed, with an unknown edible rating. Some sources list the fruit as edible, sweet and tasty, while others list the fruits as being bitter and inedible. assiflora herbertiana. A rare passion fruit native to Australia...} \\ \midrule
   TREC-COVID    & \multicolumn{1}{p{6cm}|}{what is the origin of COVID-19} & \multicolumn{1}{p{13cm}}{\textit{<Title>} Origin of Novel Coronavirus (COVID-19): A Computational Biology Study using Artificial Intelligence \textit{<Paragraph>} Origin of the COVID-19 virus has been intensely debated in the community...} \\ \midrule
   BioASQ        & \multicolumn{1}{p{6cm}|}{What is the effect of HMGB2 loss on CTCF clustering} & \multicolumn{1}{p{13cm}}{\textit{<Title>} HMGB2 Loss upon Senescence Entry Disrupts Genomic Organization and Induces CTCF Clustering across Cell Types. \textit{<Paragraph>} Processes like cellular senescence are characterized by complex events giving rise to heterogeneous cell populations. However, the early molecular events driving this cascade remain elusive....}\\ \midrule
   NFCorpus      & \multicolumn{1}{p{6cm}|}{Titanium Dioxide \& Inflammatory Bowel Disease} & \multicolumn{1}{p{13cm}}{\textit{<Title>} Titanium Dioxide Nanoparticles in Food and Personal Care Products \textit{<Paragraph>} Titanium dioxide is a common additive in many food, personal care, and other consumer products used by people, which after use can enter the sewage system, and subsequently enter the environment as treated effluent discharged to surface waters or biosolids applied to agricultural land, or incinerated wastes...} \\ \midrule
   NQ            & \multicolumn{1}{p{6cm}|}{when did they stop cigarette advertising on television?} & \multicolumn{1}{p{13cm}}{\textit{<Title>} Tobacco advertising \textit{<Paragraph>} The first calls to restrict advertising came in 1962 from the Royal College of Physicians, who highlighted the health problems and recommended stricter laws...}\\ \midrule
   HotpotQA      & \multicolumn{1}{p{6cm}|}{Stockely Webster has paintings hanging in what home (that serves as the residence for the Mayor of New York)?}& \multicolumn{1}{p{13cm}}{\textit{<Title>} Stokely Webster \textit{<Paragraph>} Stokely Webster (1912  2001) was best known as an American impressionist painter who studied in Paris. His paintings can be found in the permanent collections of many museums, including the Metropolitan Museum of Art in New York, the National Museum...} \\ \midrule
    FiQA-2018    & \multicolumn{1}{p{6cm}|}{What is the PEG ratio? How is the PEG ratio calculated? How is the PEG ratio useful for stock investing?} & \multicolumn{1}{p{13cm}}{\textit{<Paragraph>} PEG is Price/Earnings to Growth.  It is calculated as Price/Earnings/Annual EPS Growth.  It represents how good a stock is to buy, factoring in growth of earnings, which P/E does not.  Obviously when PEG is lower, a stock is more undervalued, which means that it is a better buy, and more likely...} \\ \midrule
   Signal-1M (RT) & \multicolumn{1}{p{6cm}|}{Genvoya, a Gentler Anti-HIV Cocktail, Okayed by EU Regulators} & \multicolumn{1}{p{13cm}}{\textit{<Paragraph>} All people with \#HIV should get anti-retroviral drugs: @WHO, by @kkelland  via @Reuters\_Health \#AIDS \#TasP}\\ \midrule
   TREC-NEWS     & \multicolumn{1}{p{6cm}|}{Websites where children are prostituted are immune from prosecution. But why?} & \multicolumn{1}{p{13cm}}{\textit{<Title>} Senate launches bill to remove immunity for websites hosting illegal content, spurred by Backpage.com \textit{<Paragraph>}  The legislation, along with a similar bill in the House, sets the stage for a battle between Congress and some of the Internets most powerful players, including Google and various free-speech advocates, who believe that Congress shouldnt regulate Web content or try to force websites to police themselves more rigorously...}\\ \midrule
   ArguAna       & \multicolumn{1}{p{6cm}|}{Sexist advertising is subjective so would be too difficult to codify.  Effective advertising appeals to the social, cultural, and personal values of consumers. Through the connection of values to products, services and ideas, advertising is able to accomplish its goal of adoption... } & \multicolumn{1}{p{13cm}}{\textit{<Title>} media modern culture television gender house would ban sexist advertising \textit{<Paragraph>} Although there is a claim that sexist advertising is to difficult to codify, such codes have and are being developed to guide the advertising industry. These standards speak to advertising which demeans the status of women, objectifies them, and plays upon stereotypes about women which harm women and society in general. Earlier the Council of Europe was mentioned, Denmark, Norway and Australia as specific examples of codes or standards for evaluating sexist advertising which have been developed.} \\ \midrule
   T\'ouche-2020 & \multicolumn{1}{p{6cm}|}{Should the government allow illegal immigrants to become citizens?} & \multicolumn{1}{p{13cm}}{\textit{<Title>} America should support blanket amnesty for illegal immigrants. \textit{<Paragraph>} Undocumented workers do not receive full Social Security benefits because they are not United States citizens " nor should they be until they seek citizenship legally. Illegal immigrants are legally obligated to pay taxes...} \\ \midrule 
   CQADupStack   & \multicolumn{1}{p{6cm}|}{Command to display first few and last few lines of a file} & \multicolumn{1}{p{13cm}}{\textit{<Title>} Combing head and tail in a single call via pipe \textit{<Paragraph>} On a regular basis, I am piping the output of some program to either `head` or `tail`. Now, suppose that I want to see the first AND last 10 lines of piped output, such that I could do something like ./lotsofoutput | headtail...} \\ \midrule
   Quora         & \multicolumn{1}{p{6cm}|}{How long does it take to methamphetamine out of your blood?} & \multicolumn{1}{p{13cm}}{\textit{<Paragraph>} How long does it take the body to get rid of methamphetamine?} \\ \midrule
   DBPedia       & \multicolumn{1}{p{6cm}|}{Paul Auster novels} & \multicolumn{1}{p{13cm}}{\textit{<Title>} The New York Trilogy \textit{<Paragraph>} The New York Trilogy is a series of novels by Paul Auster.  Originally published sequentially as City of Glass (1985), Ghosts (1986) and The Locked Room (1986), it has since been collected into a single volume.} \\ \midrule
   SCIDOCS       & \multicolumn{1}{p{6cm}|}{CFD Analysis of Convective Heat Transfer Coefficient on External Surfaces of Buildings} & \multicolumn{1}{p{13cm}}{ \textit{<Title>} Application of CFD in building performance simulation for the outdoor environment: an overview \textit{<Paragraph>} This paper provides an overview of the application of CFD in building performance simulation for the outdoor environment, focused on four topics...} \\ \midrule
   FEVER         & \multicolumn{1}{p{6cm}|}{DodgeBall: A True Underdog Story is an American movie from 2004} & \multicolumn{1}{p{13cm}}{\textit{<Title>} DodgeBall: A True Underdog Story \textit{<Paragraph>} DodgeBall: A True Underdog Story is a 2004 American sports comedy film written and directed by Rawson Marshall Thurber and starring Vince Vaughn and Ben Stiller. The film follows friends who enter a dodgeball tournament... } \\ \midrule
   Climate-FEVER & \multicolumn{1}{p{6cm}|}{Sea level rise is now increasing faster than predicted due to unexpectedly rapid ice melting.} & \multicolumn{1}{p{13cm}}{\textit{<Title>} Sea level rise \textit{<Paragraph>} A sea level rise is an increase in the volume of water in the world 's oceans, resulting in an increase in global mean sea level. The rise is usually attributed to global climate change by thermal expansion of the water in the oceans and by melting of Ice sheets and glaciers...}\\ \bottomrule
    \end{tabular}}
    \caption{Examples of queries and relevant documents for all datasets included in \textbf{\beir}. (\textit{<Title>}) and (\textit{<Paragraph>}) are used to distinguish the title separately from the paragraph within a document in the table above. These tokens were not passed to the respective models.}
    \label{tab:examples}
\end{table*}

\begin{table*}[t!]
    \small
    \ra{1.3}
    \resizebox{\textwidth}{!}{\begin{tabular}{l | c | c | c c c c c | c c}
        \toprule
        \multicolumn{1}{l}{\textbf{Model ()}} &
        \multicolumn{1}{c}{Lexical}   &
        \multicolumn{1}{c}{Sparse}   &
        \multicolumn{5}{c}{Dense / Neural} &
        \multicolumn{2}{c}{Reranking} \\ 
        \cmidrule(lr){1-1}
        \cmidrule(lr){2-2}
        \cmidrule(lr){3-3}
        \cmidrule(lr){4-8}
        \cmidrule(lr){9-10}
        \multicolumn{1}{l}{\textbf{Dataset ()}} &
        \multicolumn{1}{c}{\textbf{BM25}} &
        \multicolumn{1}{c}{\textbf{SPARTA}} &
        \multicolumn{1}{c}{\textbf{USE-QA}} &
        \multicolumn{1}{c}{\textbf{DPR}} &
        \multicolumn{1}{c}{\textbf{ANCE}} &
        \multicolumn{1}{c}{\textbf{SBERT}} &
        \multicolumn{1}{c}{\textbf{GenQ}} &
        \multicolumn{1}{c}{\textbf{BM25+CE}} &
        \multicolumn{1}{c}{\textbf{ColBERT}} \\
        \midrule
MSMARCO       & 0.621 & 0.793 & 0.720 & 0.552 & \underline{0.852} & 0.847 & 0.847 & 0.621 & \textbf{0.865} \\ \midrule
   TREC-COVID    & 0.447 & 0.409 & 0.339 & 0.212 & \underline{0.457} & 0.344 & 0.442 & 0.447 & \textbf{0.464} \\
   BioASQ        & \textbf{0.716} & 0.351 & 0.145 & 0.256 & 0.463 & 0.466 & 0.577 & \textbf{0.716} & \underline{0.645} \\
   NFCorpus      & 0.196 & 0.243 & \textbf{0.273} & 0.208 & 0.232 & 0.235 & \underline{0.272} & 0.196 & 0.254 \\ \midrule
   NQ            & 0.753 & 0.787 & 0.605 & \underline{0.880} & 0.836 & 0.858 & 0.845 & 0.753 & \textbf{0.912} \\ 
   HotpotQA      & \textbf{0.757} & 0.651 & 0.398 & 0.591 & 0.578 & 0.637 & 0.624 & \textbf{0.757} & \underline{0.748} \\ 
   FiQA-2018     & 0.505 & 0.446 & 0.601 & 0.342 & 0.581 & 0.540 & \textbf{0.609} & 0.505 & \underline{0.603} \\ \midrule
   Signal-1M (RT)& \textbf{0.376} & 0.270 & 0.266 & 0.162 & 0.239 & 0.263 & 0.271 & \textbf{0.376} & \underline{0.283} \\ \midrule
   TREC-NEWS     & \textbf{0.401} & 0.262 & 0.381 & 0.215 & \underline{0.398} & 0.367 & 0.395 & \textbf{0.401} & 0.367 \\ \midrule
   ArguAna       & 0.931 & 0.893 & 0.454 & 0.751 & 0.937 & \underline{0.945} & \textbf{0.979} & 0.931 & 0.914 \\  
   T\'ouche-2020 & \textbf{0.463} & 0.257 & 0.257 & 0.171 & 0.307 & 0.300 & 0.273 & \textbf{0.463} & \underline{0.309} \\ \midrule 
   CQADupStack   & 0.588 & 0.521 & 0.546 & 0.403 & 0.579 & 0.596 & \textbf{0.672} & 0.588 & \underline{0.624} \\
   Quora         & 0.948 & 0.896 & 0.978 & 0.470 & \underline{0.987} & \textbf{0.989} & \textbf{0.989} & 0.948 & \textbf{0.989} \\ \midrule
   DBPedia       & 0.384 & \underline{0.411} & 0.281 & 0.349 & 0.319 & 0.403 & 0.396 & 0.384 & \textbf{0.461} \\ \midrule
   SCIDOCS       & \textbf{0.346} & 0.297 & 0.255 & 0.219 & 0.269 & 0.296 & 0.334 & \textbf{0.346} & \underline{0.344} \\ \midrule
   FEVER         & 0.908 & 0.843 & 0.850 & 0.840 & 0.900 & 0.914 & \underline{0.920} & 0.908 & \textbf{0.934} \\ 
   Climate-FEVER & 0.383 & 0.227 & 0.346 & 0.390 & 0.445 & \underline{0.448} & \textbf{0.517} & 0.383 & 0.444 \\ 
   SciFact       & 0.838 & 0.863 & 0.631 & 0.727 & 0.816 & 0.849 & \textbf{0.892} & 0.838 & \underline{0.878} \\ \midrule
   AVERAGE       & 0.587 & 0.523 & 0.463  & 0.430 & 0.566 & 0.572 & \underline{0.603} & 0.587 & \textbf{0.613} \\ 
        \bottomrule
    \end{tabular}}
    \caption{In-domain and zero-shot retrieval performance on \beir datasets. Scores denote \textbf{Recall@100}. The best retrieval performance on a given dataset is marked in \textbf{bold}, and the second best performance is \underline{underlined}.  indicates in-domain retrieval performance.  indicates the capped recall score: R\_cap@100 (\autoref{sec:capped_recall_score}). }
    \label{tab:results-recall}
\end{table*}



\begin{figure*}[t]
    \centering
    \begin{center}
        \includegraphics[trim=0 290 0 0,clip,width=\textwidth]{figures/violin-plots/dot-vs-cosine-appendix.pdf}
        \includegraphics[trim=0 0 0 0,clip,width=\textwidth]{figures/violin-plots/dot-vs-cosine-appendix-last-two.pdf}
        \captionof{figure}{Violin plots \cite{citeulike:4075875} of document lengths for top-10 retrieved hits for SBERT (\autoref{sec:sbert_model}) trained with either cosine-similarity or dot-product. The x-axis shows the retrieved document length,  capped at a max length of 512 words. The dashed lines within the plot denotes the corresponding (25\%, 50\% and 75\%) quantiles. The plot shows the cosine-similarity (top, blue) model across all datasets retrieves on average shorter length documents compared to the dot product model (bottom, orange).}
        \label{fig:violin-plots-all}
    \end{center}
    \vspace{-2mm}
\end{figure*}


\begin{table*}[htb!]
    \centering
    \fbox{\small
    \resizebox{0.9\textwidth}{!}{\begin{tabular}{c c c c c c c c}
        \multicolumn{2}{c}{\textbf{MSMARCO}} &
        \multicolumn{2}{c}{\textbf{TREC-COVID}} &
        \multicolumn{2}{c}{\textbf{BioASQ}} &
        \multicolumn{2}{c}{\textbf{NFCorpus}} \\
        \cmidrule(lr){1-2}
        \cmidrule(lr){3-4}
        \cmidrule(lr){5-6}
        \cmidrule(lr){7-8}
        Cosine-Sim. & Dot-Prod. & Cosine-Sim. & Dot-Prod. & Cosine-Sim. & Dot-Prod. & Cosine-Sim. & Dot-Prod. \\ 
        \cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}\cmidrule(lr){8-8}
        0.389 & 0.389 & 0.482 & \textbf{0.635} & \textbf{0.295} & 0.280 & 0.257 & \textbf{0.267} \\ \\
        
        \multicolumn{2}{c}{\textbf{NQ}} &
        \multicolumn{2}{c}{\textbf{HotpotQA}} &
        \multicolumn{2}{c}{\textbf{FiQA-2018}} &
        \multicolumn{2}{c}{\textbf{Signal-1M (RT)}} \\
        \cmidrule(lr){1-2}
        \cmidrule(lr){3-4}
        \cmidrule(lr){5-6}
        \cmidrule(lr){7-8}
        Cosine-Sim. & Dot-Prod. & Cosine-Sim. & Dot-Prod. & Cosine-Sim. & Dot-Prod. & Cosine-Sim. & Dot-Prod. \\ 
        \cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}\cmidrule(lr){8-8}
        \textbf{0.450} & 0.442 & \textbf{0.513} & 0.477 & \textbf{0.258} & 0.255 & \textbf{0.261} & 0.243 \\ \\
        
        \multicolumn{2}{c}{\textbf{TREC-NEWS}} &
        \multicolumn{2}{c}{\textbf{ArguAna}} &
        \multicolumn{2}{c}{\textbf{T\'ouche-2020}} &
        \multicolumn{2}{c}{\textbf{Quora}} \\
        \cmidrule(lr){1-2}
        \cmidrule(lr){3-4}
        \cmidrule(lr){5-6}
        \cmidrule(lr){7-8}
        Cosine-Sim. & Dot-Prod. & Cosine-Sim. & Dot-Prod. & Cosine-Sim. & Dot-Prod. & Cosine-Sim. & Dot-Prod. \\ 
        \cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}\cmidrule(lr){8-8}
        0.389 & 0.389 & 0.482 & \textbf{0.635} & \textbf{0.295} & 0.280 & \textbf{0.855} & 0.833 \\ \\
        
        \multicolumn{2}{c}{\textbf{CQADupStack}} &
        \multicolumn{2}{c}{\textbf{DBPedia}} &
        \multicolumn{2}{c}{\textbf{SCIDOCS}} &
        \multicolumn{2}{c}{\textbf{FEVER}} \\
        \cmidrule(lr){1-2}
        \cmidrule(lr){3-4}
        \cmidrule(lr){5-6}
        \cmidrule(lr){7-8}
        Cosine-Sim. & Dot-Prod. & Cosine-Sim. & Dot-Prod. & Cosine-Sim. & Dot-Prod. & Cosine-Sim. & Dot-Prod. \\ 
        \cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}\cmidrule(lr){8-8}
        \textbf{0.306} & 0.283 & \textbf{0.339} & 0.315 & \textbf{0.133} & 0.113 & 0.670 & \textbf{0.685} \\ \\
        
        \multicolumn{2}{c}{} &
        \multicolumn{2}{c}{\textbf{Climate-FEVER}} &
        \multicolumn{2}{c}{\textbf{SciFact}} &
        \multicolumn{2}{c}{} \\

        \cmidrule(lr){3-4}
        \cmidrule(lr){5-6}
         &  & Cosine-Sim. & Dot-Prod. & Cosine-Sim. & Dot-Prod. & & \\ 
        \cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}
        &  & \textbf{0.205} & 0.169 &  \textbf{0.531} & 0.511 & & \\
        
    \end{tabular}}}
    \caption{\textbf{nDCG@10} scores for SBERT (\autoref{sec:sbert_model}) trained with either cosine-similarity (Cosine-Sim.) or dot-product (Dot-Prod.). For few datasets, we find the dot-product model to outperform cosine-sim. model by a large margin. For a majority of datasets, we find the cosine-sim.~model to marginally outperform the dot-product model.}
    \label{tab:dot-vs-cosine}
    \vspace*{-\baselineskip}
\end{table*}

\end{document}
