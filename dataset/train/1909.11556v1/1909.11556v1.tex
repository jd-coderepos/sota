\section{Experimental Setup}

We apply our method to a variety of sequence modeling tasks: neural machine translation, language modeling, summarization, long form question answering, and various natural language understanding tasks.
Our models are implemented in PyTorch using \texttt{fairseq-py} \citep{ott2019fairseq}. Additional implementation and training details with hyperparameter settings are in the Appendix.

\paragraph{Neural Machine Translation.}

We experiment on the WMT English-German machine translation benchmark using the Transformer Big architecture. We use the dataset of~M en-de sentence pairs from~WMT16 \citep{vaswani2017attention} for training, newstest2013 for validation, and newstest2014 for test. We optimize the dropout value within the range  on the validation set and set the LayerDrop rate  to .
For generation, we average the last~ checkpoints, set the length penalty to~, and beam size to~, following the settings suggested in \cite{wu2018pay}, and measure case-sensitive tokenized BLEU. We apply compound splitting, as used in \cite{vaswani2017attention}.

\paragraph{Language Modeling.}

We experiment on the Wikitext-103 language modeling benchmark~\citep{merity2016wikitext} which contains M tokens and a large vocabulary size of K.
We adopt the~ layer Transformer used in~\citet{baevski2018adaptive}. We set the LayerDrop rate  to  and tune the standard dropout parameter in  on the validation set.
We report test set perplexity~(PPL).

\begin{table}[t]
    \centering
    \begin{tabular}{lccc}
      \toprule
        Model & Enc Layers & Dec Layers & BLEU  \\
        \midrule
Transformer \citep{vaswani2017attention} & 6 & 6 & 28.4 \\
        Transformer \citep{ott2018scaling} & 6 & 6 & 29.3 \\
        DynamicConv \citep{wu2018pay} & 7 & 6 & 29.7 \\
        \midrule
        Transformer \citep{ott2018scaling} + LayerDrop & 6 & 6 & 29.6 \\
        Transformer \citep{ott2018scaling} + LayerDrop & 12 & 6 & \textbf{30.2}\\
        \bottomrule
    \end{tabular}
        \caption{\textbf{Results on WMT en-de Machine Translation} (newstest2014 test set)}
        \label{tab:nmt_ende}
\end{table}

\begin{table}[t]
\centering
\begin{tabular}{lccc}
\toprule
Model & Layers & Params & PPL \\
\midrule
Adaptive Inputs \citep{baevski2018adaptive} & 16 & 247M & 18.7 \\
Transformer XL Large \citep{dai2019transformer} & 18 & 257M & 18.3 \\
\midrule
Adaptive Inputs + LayerDrop & 16 & 247M & 18.3 \\
Adaptive Inputs + LayerDrop & 40 & 423M & \bf 17.7 \\
\bottomrule
\end{tabular}
\caption{
\textbf{Results on Wikitext-103} language modeling benchmark (test set).
}
\label{tab:wiki103_regularization}
\end{table}

\begin{table}[th]
\centering
\begin{tabular}{lccccc}
\toprule
Model & Enc & Dec & ROUGE-1 & ROUGE-2 & ROUGE-L \\
\midrule
\textit{Abstractive Summarization} & & & & & \\
Transformer \citep{edunov2019pre} & 6 & 6 & 40.1 & 17.6 & 36.8 \\
Transformer + LayerDrop & 6 & 6 & 40.5 & 17.9 & 37.1 \\
Transformer + LayerDrop & 6 & 8 & \bf 41.1 & \bf 18.1 & \bf 37.5 \\
\midrule
\textit{Long Form Question Answering} & & & & & \\
Transformer Multitask~\citep{fan2019eli5} & 6 & 6 & 28.9 & 5.4 & 23.1 \\
Transformer Multitask + LayerDrop & 6 & 6 & \bf 29.4 & \bf 5.5 & \bf 23.4 \\
\bottomrule
\end{tabular}
\caption{
\textbf{Results for CNN-Dailymail Summarization} and \textbf{ELI5 QA} (test set).}
\label{tab:tasks_regularization}
\end{table}


\paragraph{Summarization.}

We adopt the Transformer base architecture and training schedule from \cite{edunov2019pre} and experiment on the CNN-Dailymail multi-sentence summarization benchmark.
The training data contains over K full-text news articles paired with multi-sentence summaries~\citep{hermann15, see2017acl}.
We tune a generation length in the range~ and use~-gram blocking. We set the LayerDrop rate  to~.
We evaluate using~ROUGE~\citep{lin04rouge}.

\paragraph{Long Form Question Answering.}

We consider the Long Form Question Answering Dataset ELI5 of~\citet{fan2019eli5}, which consists of 272K question answer pairs from the subreddit \textit{Explain Like I'm Five} along with extracted supporting documents from web search.
We follow the Transformer Big architecture and training procedure of~\citet{fan2019eli5}. We generate long answers using beam search with beam size~ and apply~-gram blocking \citep{fan17controllable}.
We evaluate with~ROUGE.

\paragraph{Sentence representation Pre-training.}

We train base and large~BERT~\citep{devlin2018bert} models following the open-source implementation of~\citet{liu2019roberta}. We use two datasets: Bookscorpus + Wiki from \cite{liu2019roberta} and the larger combination of Bookscorpus + OpenWebText + CC-News + Stories \citep{liu2019roberta}. We evaluate the pretrained models on various natural language understanding tasks.
Specifically, we evaluate accuracy on~MRPC~\citep{dolan2005automatically},~QNLI~\citep{rajpurkar2016squad}, MNLI \citep{williams2018broad}, and SST2 \citep{socher2013recursive}.

\section{Results}


\begin{table}
\centering
\begin{tabular}{lllccccc}
\toprule
Data & Layers & Model  &  MNLI-m & MRPC & QNLI & SST2 \\
\midrule
Books + Wiki & 24 & RoBERTa             &  89.0    & 90.2 & 93.9     & 95.3 \\
 & 24 & RoBERTa + LayerDrop & \bf 89.2 & 90.2 & \bf 94.2 & \bf 95.4 \\
\midrule
+ more data & 24 & RoBERTa             & 90.2 & 90.9 & 94.7 & 96.4 \\
 & 24 & RoBERTa + LayerDrop &  90.1 & \bf 91.0 & 94.7 &  96.8 \\
 & 48\footnotemark & RoBERTa + LayerDrop & \bf 90.4 & 90.9 &  \bf 94.8 &  \bf 96.9\\
\bottomrule
\end{tabular}
\caption{
\textbf{Results on Various NLU Tasks} for RoBERTa Large trained for 500K updates (dev set).}
\label{tab:bert_regularization}
\end{table}

\begin{figure}[t]
    \centering
    \includegraphics{layerdrop_pruning.pdf}
    \caption{\textbf{Performance as a function of Pruning} on various generation tasks (test set), compared to training smaller models from scratch and pruning a Transformer baseline trained without LayerDrop. Pruning networks with LayerDrop performs strongly compared to these alternatives.
}
    \label{fig:ablation_1}
\end{figure}

\begin{figure}[t]
\begin{minipage}[t]{0.67\linewidth}
    \vspace{0pt}
    \centering
    \includegraphics{layerdrop_bert.pdf}
    \label{fig:ablation_pruning}
\end{minipage}
\hfill
\begin{minipage}[t]{0.32\linewidth}
    \vspace{17pt}
    \centering
    \footnotesize
    \begin{tabular}{lrr}
        \toprule
         & MNLI & SST2 \\
        \midrule
        \multicolumn{2}{l}{\textit{6 Layers} (50\% Pruned)} \\
        RoBERTa & 82.3 & 92.1\\
        + LayerDrop & 82.9 & 92.5 \\
        + more data & \bf 84.1 & \bf 93.2 \\
        \midrule
        \multicolumn{2}{l}{\textit{3 Layers} (75\% Pruned)} \\
        RoBERTa & 78.1 & 90.3 \\
        + LayerDrop & 78.6 & 90.5 \\
        + more data & \bf 82.2 & \bf 92.0\\
        \bottomrule
    \end{tabular}
    \label{tbl:test}
\end{minipage}
    \caption{(left) \textbf{Performance as a function of Pruning} on MNLI and SST2 compared to BERT and RoBERTa trained from scratch and DistilBERT. Pruning one network trained with LayerDrop (blue) outperforms alternatives that require a new network for each point. (right) \textbf{Performance when Training on More Data} shows even stronger results on MNLI and SST2 for pruned models.}
\end{figure}


\subsection{LayerDrop as a Regularizer}

\paragraph{Language Modeling.} In Table~\ref{tab:wiki103_regularization}, we show the impact of LayerDrop on the performance of a Transformer network trained in the setting of Adaptive Inputs~\citep{baevski2018adaptive}.
Adding LayerDrop to a~ layer Transformer improves the performance by  perplexity, matching the state-of-the-art results of Transformer-XL.
Our  layer Transformer with LayerDrop further improves the state of the art by  points.
Very deep Transformers are typically hard to train because of instability and memory usage, and they are prone to overfitting on a small dataset like Wikitext-103.
LayerDrop regularizes the network, reduces the memory usage, and increases training stability as fewer layers are active at each forward pass.
These results confirm that this type of approach can be used to efficiently train very deep networks, as shown in~\citet{huang2016deep} for convolutional networks.

\paragraph{Sequence to sequence modeling.} Similarly, as shown in Table~\ref{tab:nmt_ende} and Table~\ref{tab:tasks_regularization}, applying LayerDrop to Transformers on text generation tasks such as neural machine translation, summarization, and long form question answering also boosts performance for all tasks.
In these experiments, we take the Transformer architectures that are state-the-art and train them with LayerDrop.
In neural machine translation on newstest2014, our~ encoder layer Transformer model with LayerDrop further improves the state of the art, reaching~ BLEU. In comparison, a standard Transformer trained without LayerDrop diverges with 12 encoder layers. This is a known problem, and techniques such as improved initialization could be used to maintain stability \citep{junczys2019microsoft,zhang2019fixup}, but are out of the scope of this work. Similar results are seen in summarization.

\paragraph{Bi-Directional Pre-training.}
\footnotetext{The 48 layer model was trained for 300K updates.}
In a second set of experiments, we look at the impact of LayerDrop on pre-training for sentence representation models and subsequent finetuning on multiple natural language understanding tasks. We compare our models to a variant of BERT for sentence representations, called RoBERTa \citep{liu2019roberta}, and analyze the results of finetuning for data adaptation on MNLI, MRPC, QNLI, and SST2.
We apply LayerDrop during both pre-training and finetuning.

We compare the performance of the large architecture on the BooksCorpus+Wiki dataset used in~BERT. We analyze the performance of training on the additional data used in RoBERTa as well as pre-training for even longer. Comparing fixed model size and training data,  LayerDrop can improve the performance of RoBERTa on several tasks. LayerDrop can further be used to both enable and stabilize the training~\citep{huang2016deep} of models double the size for even stronger performance.


\subsection{Pruning Transformer Layers to On-Demand Depth with LayerDrop}


\paragraph{Pruning Generation Tasks.} In Figure~\ref{fig:ablation_1}, we investigate the impact of the number of pruned decoder layers on the performance of a Transformer for language modeling, neural machine translation, and summarization.
We compare three different settings: standard Transformer models trained without LayerDrop but subsequently pruned, standard Transformer models trained from scratch to each desired depth, and lastly our approach: pruning layers of a Transformer trained with LayerDrop. Our model is trained once with the maximum number of layers and then pruned to the desired depth, without any finetuning in the shallower configuration.
Our approach outperforms small models trained from scratch, showing that LayerDrop leads to more accurate small models at a whole range of depths. Further, training with LayerDrop does not incur the computational cost of retraining a new model for each desired depth.
For completeness, dropping layers of a deep Transformer trained without LayerDrop performs poorly as it was not trained to be robust to missing layers.

\paragraph{Pruning BERT-like Models.} In Table~\ref{tab:bert_reduction} (left), we compare pruning Transformers trained with LayerDrop to different approaches used to create smaller, shallower models.
We compare to BERT base and RoBERTa base trained from scratch with  and  layers as well as recent work on distillation, called DistilBERT~\citep{distilbert}. We analyze both BERT and RoBERTa models as the vocabulary is not the same due to differences in subword tokenization, which affects performance.

DistilBERT occasionally performs worse than BERT of the same size trained from scratch, which confirms the findings of~\citet{liu2018rethinking} about the performance of pruned models compared to training small models from scratch.
Our approach, however, obtains results better than BERT and RoBERTa trained from scratch.
Further, our method does not need any post-processing: we simply prune every other layer of our RoBERTa model that has been pre-trained with LayerDrop and finetune the small models on each of the downstream tasks, following standard procedure. When training with additional data, shown in Table~\ref{tab:bert_reduction} (right), even stronger performance can be achieved.

\section{Ablation Studies}

\begin{figure}[t]
    \centering
\includegraphics{layerdrop_type.pdf} \hspace{-1.2em}
    \includegraphics{layerdrop_what_drop.pdf}
    \caption{(left) \textbf{Impact of Various Structured Dropouts} on Wikitext-103 Valid. Dropping Layers is straightforward and has strong performance. (right) \textbf{Comparison of Pruning Strategies} on Wikitext-103 Valid. Marginal gains can be achieved, but dropping every other layer is hard to beat.
}
    \label{fig:ablation_3}
\end{figure}


\paragraph{Comparison of Structured Dropout}

Figure~\ref{fig:ablation_3} (left) contrasts various forms of structured dropout: dropping attention heads, FFN matrices, and entire Transformer layers.
Dropping \textit{heads} alone is worse than dropping entire sub-layers or layers.
It also offers no advantage in terms of running time as attention heads are computed in parallel for computational efficiency.
We observe no large differences between dropping sub-layers and layers, possibly because we are working with relatively shallow networks.
In theory, dropping sub-layers should perform better and we expect this to be the case with very deep Transformers.
We experiment with overlapping structured groups, such as \textit{heads + layers} and \textit{heads + sub-layers} and find that the beneficial effect can be advantageously combined. We focus on layers for simplicity, as dropping more  structures introduces more parameters to tune.

\paragraph{Comparison of Various Pruning Strategies.}
Figure~\ref{fig:ablation_3} (right) contrasts various approaches to sub-selecting model layers at inference time.
The predominant method used in this paper, the straightforward strategy of selecting every other layer, is tough to beat.
We find only marginal improvement can be gained by searching over the validation set for the best set of 8 layers to use and by learning which layers to drop. In contrast, dropping chunks of consecutive layers is harmful. Namely, removing the first half or last half of a model is particularly harmful, as the model does not have the ability to process the input or project to the full vocabulary to predict the subsequent word.

\begin{figure}[t]
    \centering
    \begin{minipage}[t]{.55\linewidth}
        \includegraphics{layerdrop_importance_layers.pdf}
        \caption{\textbf{Relative Importance of Specific Layers.} (Wikitext-103 Valid) The full network is pruned into various 8 layer sub-network configurations, and the average perplexity pruning layer  is displayed above.}
    \label{fig:layer_importance}
    \end{minipage}
\hfill
    \centering
    \begin{minipage}[t]{.41\linewidth}
        \includegraphics{layerdrop_train_test.pdf}
        \caption{\textbf{Effect of Train LayerDrop} on Inference-time Pruning. (Wikitext-103 Valid) Training with larger LayerDrop is beneficial for significant pruning.}
    \label{fig:lm_layerdrop_train}
    \end{minipage}
\end{figure}


\paragraph{Choosing which Layers to Prune.}

Not all layers are equally important.  In an experiment on Wikitext-103, we pruned selections of 8 layers at random. Figure~\ref{fig:layer_importance} displays the perplexity when that layer is removed, averaging results from 20 pruned model per layer. The input and output layers of a network are the most important, as they process the input and project to the output vocabulary.

\paragraph{Relationship between LayerDrop at Training Time and Pruning at Inference Time.}

Figure~\ref{fig:lm_layerdrop_train} displays the relationship between the training time LayerDrop and the performance of a pruned network at test time. If significant depth reduction is desired, training with larger LayerDrop is beneficial --- this equalizes the train and test time settings. An analysis for BERT is in the Appendix.
