
\documentclass{article} \usepackage{iclr2020_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 \usepackage{caption}
\usepackage[pdftex]{graphicx}
\usepackage{enumitem} 
\usepackage{url}
\usepackage{multirow}
\usepackage{xcolor,colortbl}
\usepackage{amstext,amsmath,amssymb,amsthm}
\usepackage{booktabs}


\title{Geom-GCN: Geometric Graph Convolutional Networks}


\iclrfinalcopy 

\author{Hongbin Pei\thanks{
This work is conducted partially during his visit at University of Illinois at Urbana-Champaign.}\\
\texttt{peihb15@mails.jlu.edu.cn} \\
\And Bingzhe Wei\\
\texttt{bwei6@illinois.edu} \\
\And Kevin Chen-Chuan Chang \\
\texttt{kcchang@illinois.edu} \\
\And Yu Lei \\
\texttt{csylei@comp.polyu.edu.hk} \\
\And Bo Yang\thanks{Corresponding author.}\\
\texttt{ybo@jlu.edu.cn} \\
\AND
\vspace{-5mm}
~\\ 
College of Computer Science and Technology, Jilin University, China \\
Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign, USA\\
Department of Computer Science, University of Illinois at Urbana-Champaign, USA \\
Department of Computing, Hong Kong Polytechnic University, Hong Kong\\
Key Laboratory of Symbolic Computation and Knowledge Engineering of Ministry of Education, China
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}


\maketitle

\begin{abstract}
Message-passing neural networks (MPNNs) have been successfully applied to representation learning on graphs in a variety of real-world applications.
However, two fundamental weaknesses of MPNNs' aggregators limit their ability to represent graph-structured data: losing the structural information of nodes in neighborhoods and lacking the ability to capture long-range dependencies in disassortative graphs.
Few studies have noticed the weaknesses from different perspectives.
From the observations on classical neural network and network geometry, we propose a novel geometric aggregation scheme for graph neural networks to overcome the two weaknesses. 
The behind basic idea is the aggregation on a graph can benefit from a continuous space underlying the graph.
The proposed aggregation scheme is permutation-invariant and consists of three modules, node embedding, structural neighborhood, and bi-level aggregation.
We also present an implementation of the scheme in graph convolutional networks, termed Geom-GCN, to perform transductive learning on graphs.
Experimental results show the proposed Geom-GCN achieved state-of-the-art performance on a wide range of open datasets of graphs.
\end{abstract}

\section{Introduction}
Message-passing neural networks (MPNNs), such as GNN \citep{scarselli2008graph}, ChebNet \citep{defferrard2016convolutional}, GG-NN \citep{DBLP:journals/corr/LiTBZ15}, GCN \citep{DBLP:conf/iclr/KipfW17}, are powerful for learning on graphs with various applications ranging from brain networks to online social network \citep{DBLP:conf/icml/GilmerSRVD17,DBLP:conf/kdd/WangXLLCDWS19}.
In a layer of MPNNs, each node sends its feature representation, a ``message'', to the nodes in its neighborhood; 
and then updates its feature representation by aggregating all ``messages'' received from the neighborhood.
The neighborhood is often defined as the set of adjacent nodes in graph.
By adopting permutation-invariant aggregation functions (e.g., summation, maximum, and mean), MPNNs are able to learn representations which are invariant to isomorphic graphs, i.e., graphs that are topologically identical.




Although existing MPNNs have been successfully applied in a wide variety of scenarios, two fundamental weaknesses of MPNNs' aggregators limit their ability to represent graph-structured data.
Firstly, \emph{the aggregators lose the structural information of nodes in neighborhoods}.
Permutation invariance is an essential requirement for any graph learning method. 
To meet it, existing MPNNs adopt permutation-invariant aggregation functions which treat all ``messages'' from neighborhood as a set.
For instance, GCN simply sums the normalized ``messages'' from all one-hop neighbors \citep{DBLP:conf/iclr/KipfW17}.
Such aggregation loses the structural information of nodes in neighborhood because it does not distinguish the ``messages'' from different nodes. 
Therefore, after such aggregation, we cannot know which node contributes what to the final aggregated output.



Without modeling such structural information, as shown in \citep{DBLP:conf/iclr/KondorSPAT18} and \citep{xu2018how}, the existing MPNNs cannot discriminate between certain non-isomorphic graphs.
In those cases, MPNNs may map non-isomorphic graphs to the same feature representations, which is obviously not desirable for graph representation learning.
Unlike MPNNs, classical convolutional neural networks (CNNs) avoid this problem by using aggregators (i.e., convolutional filters) with a structural receiving filed defined on grids, i.e., a Euclidean space, and are hence able to distinguish each input unit.
As shown by our experiments, such structural information often contains clues regarding topology patterns in graph (e.g., hierarchy), and should be extracted and used to learn more discriminating representations for graph-structured data.

Secondly, \emph{the aggregators lack the ability to capture long-range dependencies in disassortative graphs.}
In MPNNs, the neighborhood is defined as the set of all neighbors one hop away (e.g., GCN), or all neighbors up to  hops away (e.g., ChebNet).
In other words, only messages from nearby nodes are aggregated.
The MPNNs with such aggregation are inclined to learn similar representations for proximal nodes in a graph.
This implies that they are probably desirable methods for assortative graphs (e.g., citation networks \citep{DBLP:conf/iclr/KipfW17} and community networks \citep{chen2018supervised}) where node homophily holds (i.e., similar nodes are more likely to be proximal, and vice versa), but may be inappropriate to the disassortative graphs \citep{newman2002assortative} where node homophily does not hold. 
For example, \cite{DBLP:conf/kdd/RibeiroSF17} shows disassortative graphs where nodes of the same class exhibit high structural similarity but are far apart from each other.
In such cases, the representation ability of MPNNs may be limited significantly, since they cannot capture the important features from distant but informative nodes.

A straightforward strategy to address this limitation is to use a multi-layered architecture so as to receive ``messages'' from distant nodes. 
For instance, due to the localized nature of convolutional filters in classical CNNs, a single convolutional layer is similarly limited in its representational ability.
CNNs typically use multiple layers connected in a hierarchical manner to learn complex and global representations. 
However, unlike CNNs, it is difficult for multi-layer MPNNs to learn good representations for disassortative graphs because of two reasons. 
On one hand, relevant messages from distant nodes are mixed indistinguishably with a large number of irrelevant messages from proximal nodes in multi-layer MPNNs, which implies that the relevant information will be ``washed out''  and cannot be extracted effectively. 
On the other hand, the representations of different nodes would become very similar in multi-layer MPNNs, and every node's representation actually carries the information about the entire graph \citep{DBLP:conf/icml/XuLTSKJ18}.


In this paper, we overcome the aforementioned weaknesses of graph neural networks starting from two basic observations: 
i) Classical neural networks effectively address the similar limitations thanks to the stationarity, locality, and compositionality in a continuous space \citep{bronstein2017geometric}; 
ii) The notion of network geometry bridges the gap between continuous space and graph \citep{hoff2002latent,muscoloni2017machine}.
Network geometry aims to understand networks by revealing the latent continuous space underlying them, which assumes that nodes are sampled discretely from a latent continuous space and edges are established according to their distance. 
In the latent space, complicated topology patterns in graphs can be preserved and presented as intuitive geometry, such as subgraph \citep{DBLP:journals/corr/NarayananCCLS16}, community \citep{ni2019community}, and hierarchy \citep{DBLP:conf/nips/NickelK17,DBLP:conf/icml/NickelK18}. 
Inspired by those two observations, we raise an enlightening question about the aggregation scheme in graph neural network.
\begin{itemize}[leftmargin=15pt]
	\item Can the aggregation on a graph benefit from a continuous latent space, such as using geometry in the space to build structural neighborhoods and capture long-range dependencies in the graph?
\end{itemize}

To answer the above question, we propose a novel aggregation scheme for graph neural networks, termed the \emph{geometric aggregation scheme}. 
In the scheme, we map a graph to a continuous latent space via node embedding, and then use the geometric relationships defined in the latent space to build structural neighborhoods for aggregation.
Also, we design a bi-level aggregator operating on the structural neighborhoods to update the feature representations of nodes in graph neural networks, which are able to guarantee permutation invariance for graph-structured data.
Compared with existing MPNNs, the scheme extracts more structural information of the graph and can aggregate feature representations from distant nodes via mapping them to neighborhoods defined in the latent space.

We then present an implementation of the geometric aggregation scheme in graph convolutional networks, which we call \emph{Geom-GCN}, to perform transductive learning, node classification, on graphs.
We design particular geometric relationships to build the structural neighborhood in Euclidean and hyperbolic embedding space respectively.
We choose different embedding methods to map the graph to a suitable latent space for different applications, where suitable topology patterns of graph are preserved. 
Finally, we empirically validate and analyze Geom-GCN on a wide range of open datasets of graphs, and Geom-GCN achieved the state-of-the-art results.

In summary, the contribution of this paper is three-fold:
i) We propose a novel geometric aggregation scheme for graph neural network, which operates in both graph and latent space, to overcome the aforementioned two weaknesses;
ii) We present an implementation of the scheme, Geom-GCN, for transductive learning in graph;
iii) We validate and analyze Geom-GCN via extensive comparisons with state-of-the-art methods on several challenging benchmarks.

\vspace{-2mm}
\section{Geometric aggregation scheme}
\vspace{-1mm}
In this section, we start by presenting the geometric aggregation scheme, and then outline its advantages and limitations compared to existing works.
As shown in Fig.~\ref{fig:illus}, the aggregation scheme consists of three modules, node embedding (panel A1 and A2), structural neighborhood (panel B1 and B2), and bi-level aggregation (panel C). We will elaborate on them in the following.

\begin{figure}[!tbh]
\begin{center}
\includegraphics[width=1  \textwidth]{illustration.jpg}
\end{center} 
\caption{An illustration of the geometric aggregation scheme. \textbf{A1-A2} The original graph is mapped to a latent continuous space.
\textbf{B1-B2} The structural neighborhood. 
All adjacent nodes lie in a small region around a center node in B1 for visualization. 
In B2, the neighborhood in the graph contains all adjacent nodes in graph;
the neighborhood in the latent space contains the nodes within the dashed circle whose radius is . 
The relational operator  is illustrated by a colorful  grid where each unit is corresponding to a geometric relationship to the red target node. 
\textbf{C} Bi-level aggregation on the structural neighborhood. 
Dashed and solid arrows denote the low-level and high-level aggregation, respectively. 
Blue and green arrows denote the aggregation on the neighborhood in the graph and the latent space, respectively.}
\label{fig:illus}
\vspace{-2mm}
\end{figure}

\textbf{A. Node embedding}. 
This is a fundamental module which maps the nodes in a graph to a latent continuous space.
Let  be a graph, where each node  has a feature vector  and each edge  connects two nodes. 
Let   be a mapping function from a node in graph to a representation vector.
Here,  can also be considered as the position of node  in a latent continuous space, and  is the number of dimensions of the space. 
During the mapping, the structure and properties of graph are preserved and presented as the geometry in the latent space.
For instance, hierarchical pattern in graph is presented as the distance to the original in embedding hyperbolic space \citep{DBLP:conf/nips/NickelK17}. 
One can employ various embedding methods to infer the latent space \citep{cai2018comprehensive,DBLP:conf/icdm/WangC0XDSZ18}.


\textbf{B. Structural neighborhood}. 
Based on the graph and the latent space, we then build a structural neighborhood, ,  for the next aggregation.
The structural neighborhood consists of a set of neighborhood , and a relational operator on neighborhoods .

The neighborhood in the graph, , is the set of adjacent nodes of .
The neighborhood in the latent space, , is the set of nodes from which the distance to  is less than a pre-given parameter . The distance function  depends on the particular metric in the space.
Compared with ,  may contain nodes which are far from  in the graph, but have a certain similarity with , and hence are mapped together with  in the latent space though preserving the similarity.
By aggregating on such neighborhood , the long-range dependencies in disassortative graphs can be captured.

The relational operator  is a function defined in the latent space.
It inputs an ordered position pair  of nodes  and , and outputs a discrete variable  which indicates the geometric relationship from  to  in the latent space. For ,

where  is the set of the geometric relationships.
According to the particular latent space and application,  can be specified as an arbitrary geometric relationship of interest.
A requirement on  is that it should guarantee that each ordered position pair has only one geometric relationship.
For example,  is illustrated in Fig.~\ref{fig:illus}B by a colorful 3  3 grid in a 2-dimensional Euclidean space, in which each unit is corresponding to a geometric relationship to node .

\textbf{C. Bi-level aggregation}.
With the structural neighborhood , we propose a novel bi-level aggregation scheme for graph neural network to update the hidden features of nodes.
The bi-level aggregation consists of two aggregation functions and operates in a neural network layer.
It can extract effectively structural information of nodes in neighborhoods as well as guarantee permutation invariance for graph.
Let  be the hidden features of node  at the -th layer, and  be the node features.
The -th layer updates  for every  by the following.


In the low-level, the hidden features of nodes that are in the same neighborhood  and have the same geometric relationship  are aggregated to a virtual node via the aggregation function .
The features of the virtual node are , and the virtual node is indexed by  which is corresponding to the combination of a neighborhood  and a relationship . 
It is required to adopt a permutation-invariant function for , such as an -norm (the choice of  results in average, energy, or max pooling).
The low level aggregation is illustrated by dashed arrows in Fig.~\ref{fig:illus}C.


In the high-level, the features of virtual nodes are further aggregated by function .
The inputs of function  contain both the features of virtual nodes  and the identity of virtual nodes .
That is,  can be a function that take an ordered object as input, e.g., concatenation, to distinguish the features of different virtual nodes, thereby extracting the structural information in the neighborhoods explicitly.
The output of high-level aggregation is a vector .
Then new hidden features of , , are given  by a non-linear transform, wherein  is a learnable weight matrix on the -th layer shared by all nodes, and  is a non-linear activation function, e.g., a ReLU.

Permutation invariance is an essential requirement for aggregators in graph neural networks. 
Thus, we then prove that the proposed bi-level aggregation, Eq.~\ref{qu:bi-level}, is able to guarantee invariance for any permutation of nodes.
We firstly give a definition for permutation-invariant mapping of graph.

\textbf{Definition 1.} \emph{Let a bijective function  be a permutation for nodes, which renames  as . 
Let  and  be the node and edge set after a permutation , respectively.
A mapping of graph, , is permutation-invariant if, given any permutation , we have .}

\textbf{Lemma 1.} \emph{For a composite function , if  is permutation-invariant, the entire composite function  is permutation-invariant.}
\begin{proof}

Let  be an isomorphic graph of  after a permutation , as defined in Definition 1.
If  is permutation-invariant, we have . 
Therefore, the entire composite function  is permutation-invariant because .
\end{proof}
\vspace{-2.5mm}

\textbf{Theorem 1.} \emph{Given a graph  and its structural neighborhood , the bi-level aggregation, Eq.~\ref{qu:bi-level}, is a permutation-invariant mapping of graph.}
\vspace{-3mm}
\begin{proof}
The bi-level aggregation, Eq.~\ref{qu:bi-level}, is a composite function, where the low-level aggregation is the input of the high-level aggregation.
Thus, Eq.~\ref{qu:bi-level} is permutation-invariant if the low-level aggregation is permutation-invariant according to Lemma 1. 

We then prove that the low-level aggregation is permutation-invariant.
The low-level aggregation consists of   sub-aggregations, each of which is corresponding to the nodes in a neighborhood  and with a relationship  to .
Firstly, the input of each sub-aggregations is permutation-invariant because both  and  are determined by the given structural neighborhood , which is constant for any permutation. 
Secondly, Eq.~\ref{qu:bi-level} adopts a permutation-invariant aggregation function  for the sub-aggregations.
Thus the low-level aggregation is permutation-invariant.
\vspace{-2mm}
\end{proof}


\subsection{Comparisons to related work}
\vspace{-2mm}
We now discuss how the proposed geometric aggregation scheme overcomes the two aforementioned weaknesses, i.e., how it effectively models the structural information and captures the long-range dependencies, in comparison to some closely related works.  

To overcome the first weakness of MPNNs, i.e.,  losing the structural information of nodes in neighborhoods, the proposed scheme explicitly models the structural information by exploiting the geometric relationship between nodes in  latent space and then extracting the information effectively by using the bi-level aggregations. 
In contrast, several existing works attempt to learn some implicit structure-like information to distinguish different neighbors when aggregating features. 
For example, GAT \citep{DBLP:journals/corr/abs-1710-10903}, LGCL \citep{gao2018large} and GG-NN \citep{DBLP:journals/corr/LiTBZ15} learn weights on ``messages'' from different neighbors by using attention mechanisms and node and/or edge attributes. 
CCN \citep{DBLP:conf/iclr/KondorSPAT18} utilizes a covariance architecture to learn structure-aware representations.
The major difference between these works and ours is that we offer an explicit and interpretable way to model the structural information of nodes in neighborhood, with the assistance of the geometry in a latent space.
We note that our work is orthogonal with existing methods and thus can be readily incorporated to further improve their performance.
In particular, we exploit geometric relationships from the aspect of \emph{graph topology}, while other methods focus on that of \emph{feature representation}-- the two aspects are complementary. 

For the second weakness of MPNNs, i.e., lacking the ability to capture long-range dependencies, the proposed scheme models the long-range dependencies in disassortative graphs in two different ways. 
First of all, the distant (but similar) nodes in the graph can be mapped into a latent-space-based neighborhood of the target node, and then their useful feature representations can be used for aggregations. 
This way depends on an appropriate embedding method, which is able to preserve the similarities between the distant nodes and the target node. 
On the other hand, the structural information enables the method to distinguish different nodes in a graph-based neighborhood (as mentioned above). 
The informative nodes may have some special geometric relationships to the target node (e.g., a particular angle or distance), whose relevant features hence will be passed to the target node with much higher weights, compared to the uninformative nodes. 
As a result, the long-range dependencies are captured indirectly through the whole message propagation process in all graph-based neighborhoods. 
In literature, a recent method JK-Nets \citep{DBLP:conf/icml/XuLTSKJ18} captures the long-range dependencies by skipping connections during feature aggregations.

\vspace{-2mm}
\subsubsection{Case study on distinguishing non-isomorphic graphs}
\vspace{-2mm}
In literature, \cite{DBLP:conf/iclr/KondorSPAT18} and \cite{xu2018how} construct several non-isomorphic example graphs that cannot be distinguished by the aggregators (e.g., mean and maximum) in existing MPNNs. 
We present a case study to illustrate how to distinguish the non-isomorphic example graphs once the structural neighborhood is applied.
We take two non-isomorphic graphs in \citep{xu2018how} as an example, where each node has the same feature  and after any mapping  remains the same across all nodes, as shown in Fig. \ref{fig:example} (left).
Then the aggregator, e.g., mean or maximum, over  remains , and hence the final representations of the nodes are the same. 
That is, mean and maximum aggregators fail to distinguish the two different graphs.

\begin{figure}[!t]
\begin{center}
\includegraphics[width=0.85  \textwidth]{GIN_example_graphs.png}
\end{center} 
\caption{An illustration to distinguish non-isomorphic graphs by proposed structural neighborhood.}
\label{fig:example}
\vspace{-4mm}
\end{figure}

In contrast, the two graphs become distinguishable once we apply a structural neighborhood in aggregation.
With the structural neighborhood, the nodes have different geometric relationships to the center node  in the structural neighborhood, as shown in Fig. \ref{fig:example} (right).
Taking aggregation for  as an example, we can adopt different mapping function  to the neighbors with different geometric relationship  to .
Then, the aggregator in two graph have different inputs, ,  in the left graph and , ,  in the right graph.
Finally, the aggregator (mean or maximum) will output different representations for the node  in the two graphs,  thereby distinguishing the topological difference between the two graphs.

\vspace{-2.5mm}
\section{Geom-GCN: An implementation of the scheme}
\vspace{-2.5mm}
In this section, we present Geom-GCN, a specific implementation of the geometric aggregation scheme in graph convolutional networks, to perform transductive learning in graphs.
To implement the general aggregation scheme, one needs to specify its three modules: node embedding, structural neighborhood, and bi-level aggregation function.

Node embedding is the fundamental. 
As shown in our experiments, a common embedding method which only preserves the connection and distance pattern in a graph can already benefit the aggregation. 
For particular applications, one can specify embedding methods to create suitable latent spaces where particular topology patterns (e.g., hierarchy) are preserved.
We employ three embedding methods, Isomap \citep{tenenbaum2000global}, Poincare embedding \citep{DBLP:conf/nips/NickelK17}, and struc2vec \citep{DBLP:conf/kdd/RibeiroSF17}, which result in three Geom-GCN variants: Geom-GCN-I, Geom-GCN-P, and Geom-GCN-S.
Isomap is a widely used isometry embedding method, by which distance patterns (lengths of shortest paths) are preserved explicitly in the latent space.
Poincare embedding and struc2vec can create particular latent spaces that preserve hierarchies and local structures in a graph, respectively.
We use an embedding space of dimension 2 for ease of explanation. 

The structural neighborhood  of node  includes its neighborhoods in both the graph and latent space.
The neighborhood-in-graph  consists of the set of 's adjacent nodes in the graph, and the neighborhood-in-latent-space  those nodes whose distances to  are less than a parameter  in the latent space.
We determine  by increasing  from zero until the average cardinality of  equals to that of , -- i.e., when the average neighborhood sizes in the graph and latent spaces are the same.
We use Euclidean distance in the Euclidean space.
In the hyperbolic space, we approximate the geodesic distance between two nodes via their Euclidean distance in the local tangent plane.

Here we simply implement the geometric operator  as four relationships of the relative positions between two nodes in a 2-D Euclidean or hyperbolic space. 
Particularly, the relationship set  \{upper left, upper right, lower left, lower right\}, and a  is given by Table \ref{tab:operator}.
Note that, we adopt the rectangular coordinate system in the Euclidean space and angular coordinate in the hyperbolic space.
By this way, the relationship ``upper" indicates the node nearer to the origin and thus lie in a higher level in a hierarchical graph.
One can design a more sophisticated operator , such as borrowing the structure of descriptors in manifold geometry \citep{kokkinos2012intrinsic,monti2017geometric}, thereby preserving more and richer structural information in neighborhood.

\begin{table}[h]
  \vspace{-2mm}
  \begin{center}
  \caption{The relationship operator}
  \vspace{-1mm}
  \label{tab:operator}
    \begin{tabular}{c|c|c}  
       &  & \\
      \hline
       & upper left &  upper right\\
       &  lower left &  lower right\\
    \end{tabular}
  \end{center}
\end{table}


Finally, to implement the bi-level aggregation, we adopt the same summation of normalized hidden features as GCN \citep{DBLP:conf/iclr/KipfW17} as the aggregation function  in the low-level aggregation,

where  is the degree of node  in graph, and  is a Kronecker delta function that only allows the nodes with relationship  to  to be included.
The features of all virtual nodes  are further aggregated in the high-level aggregation. The aggregation function  is a concatenation  for all layers except the final layer, which uses mean for its aggregation function.
Then, the overall bi-level aggregation of Geom-GCN is given by

where we use ReLU as the non-linear activation function  and  is the weight matrix to estimate by backpropagation.

\vspace{-1mm}
\section{Experiments}
\vspace{-1mm}

We validate Geom-GCN by comparing Geom-GCN's performance with the performance of Graph Convolutional Networks (GCN) (\cite{DBLP:conf/iclr/KipfW17}) and Graph Attention Networks (GAT) (\cite{DBLP:journals/corr/abs-1710-10903}).
Two state-of-the-art graph neural networks, on transductive node-label  classification tasks on a wide variety of open graph datasets.

\vspace{-1mm}
\subsection{Datasets}
\vspace{-1mm}
We utilize nine open graph datasets to validate the proposed Geom-GCN.
An overview summary of characteristics of the datasets is given in Table \ref{tab:stat}.

\begin{table*}[!hbtp]
\centering
\caption{Datasets statistics}
\vspace{-1mm}
\label{tab:stat}
\begin{tabular}{lcccccccccc}
\toprule
  \multicolumn{1}{l}{\textbf{Dataset}}               &Cora                          & Cite.                    & Pubm.                   & Cham.           & Squi.           & Actor          & Corn.           & Texa.                    & Wisc.           \\
\midrule
\# Nodes                         & 2708                   & 3327                   & 19717           & 2277           & 5201          & 7600           & 183           & 183              & 251        \\
\# Edges                         & 5429                    & 4732                   & 44338           & 36101           & 217073          & 33544           & 295           & 309                & 499     \\
\# Features                         & 1433                    & 3703                   & 500           & 2325           & 2089          & 931           & 1703           & 1703               & 1703      \\
\# Classes                         & 7                    & 6                   & 3           & 5           & 5          &5           & 5           & 5                   & 5      \\
\bottomrule
\end{tabular}
\vspace{-1mm}
\end{table*}

\emph{Citation networks}. Cora, Citeseer, and Pubmed are standard citation network benchmark datasets \citep{sen2008collective,namata2012query}.
In these networks, nodes represent papers, and edges denote citations of one paper by another. 
Node features are the bag-of-words representation of papers, and node label is the academic topic of a paper.

\emph{WebKB}. WebKB is a webpage dataset collected from computer science departments of various universities by Carnegie Mellon University.
We use the three subdatasets of it, Cornell, Texas, and Wisconsin, where nodes represent web pages, and edges are hyperlinks between them.
Node features are the bag-of-words representation of web pages.
The web pages are manually classified into the five categories, student, project, course, staff, and faculty.

\emph{Actor co-occurrence network}.
This dataset is the actor-only induced subgraph of the film-director-actor-writer network \citep{tang2009social}. 
Each nodes correspond to an actor, and the edge between two nodes denotes co-occurrence on the same Wikipedia page.
Node features correspond to some keywords in the Wikipedia pages.
We classify the nodes into five categories in term of words of actor's Wikipedia.

\emph{Wikipedia network}.
Chameleon and squirrel are two page-page networks on specific topics in Wikipedia \citep{rozemberczki2019multi}.
In those datasets, nodes represent web pages and edges are mutual links between pages. 
And node features correspond to several informative nouns in the Wikipedia pages.
We classify the nodes into five categories in term of the number of the average monthly traffic of the web page.

\subsection{Experimental setup}
\vspace{-2mm}
As mentioned in Section 3, we construct three Geom-GCN variants by using three embedding methods, Isomap (Geom-GCN-I), Poincare   (Geom-GCN-P), and struc2vec  (Geom-GCN-S).
We specify the dimension of embedding space as two, and use the relationship operator  defined in Table \ref{tab:operator}, and apply mean and concatenation as the low- and high- level aggregation function, respectively.

With the structural neighborhood, we perform a hyper-parameter search for all models on validation set.
For fairness, the size of search space for each method is the same. 
The searching hyper-parameters include number of hidden unit, initial learning rate, weight decay, and dropout. 
We fix the number of layer to 2 and use Adam optimizer \citep{kingma2014adam} for all models. 
We use ReLU as the activation function for Geom-GCN and GCN, and ELU for GAT.

The final hyper-parameter setting is dropout of , initial learning rate of , patience of 100 epochs, weight decay of - (WebKB datasets) or - (the other all datasets).
In GCN, the number of hidden unit is  (Cora),  (Citeseer),  (Pubmed),  (WebKB),  (Wikipedia), and  (Actor). 
In Geom-GCN,  the number of hidden unit is  times as many as the number in GCN since Geom-GCN has   virtual nodes.
For each attention head in GAT, the number of hidden unit is  (Citation networks),  (WebKB),  (Wikipedia), and  (Actor). 
GAT has  attention heads in layer one and  (Pubmed) or  (the all other datasets) attention heads in layer two.

For all graph datasets, we randomly split nodes of each class into , , and  for training, validation and testing.
With the hyper-parameter setting, we report the average performance of all models on the test sets over  random splits.

\vspace{-2mm}
\subsection{Results and analysis}
\vspace{-2mm}
Results are summarized in Table \ref{tab:res}. 
The reported numbers denote the mean classification accuracy in percent.
In general, Geom-GCN achieves state-of-the-art performance.
The best performing method is highlighted.
From the results, Isomap embedding (Geom-GCN-I) which only preserves the connection and distance pattern in graph can already benefit the aggregation.
We can also specify an embedding method to create a suitable latent space for a particular application (e.g., disassortative graph or hierarchical graph), by doing which a significant performance improvement is achieved (e.g., Geom-GCN-P).

\begin{table*}[!hbtp]
    \vspace{-3mm}
    \centering
    \caption{Mean Classification Accuracy (Percent)}
    \vspace{-1.5mm}
    \label{tab:res}
    \begin{tabular}{lcccccccccc}
    \toprule
        \multicolumn{1}{l}{\textbf{Dataset}}  &Cora  &Cite.  &Pubm.  &Cham.  &Squi.  &Actor  &Corn.  &Texa.  &Wisc. \\
        \midrule
        GCN  &85.77  &73.68  &88.13  &28.18  &23.96  &26.86  &52.70  &52.16  &45.88 \\
        GAT  &\textbf{86.37} &74.32  &87.62  &42.93  &30.03  &28.45  &54.32  &58.38  &49.41 \\
        \midrule
        Geom-GCN-I  &85.19  &\textbf{77.99}  &\textbf{90.05}  &60.31  &33.32  &29.09  &56.76  &57.58  &58.24 \\
        Geom-GCN-P  &84.93  &75.14  &88.09  &\textbf{60.90}  &\textbf{38.14}  &\textbf{31.63}  &\textbf{60.81}  &\textbf{67.57}  &\textbf{64.12} \\
        Geom-GCN-S  &85.27  &74.71  &84.75  &59.96  &36.24  &30.30  &55.68  &59.73  &56.67 \\
\bottomrule
\end{tabular}
\end{table*}

\vspace{-2mm}
\subsubsection{Ablation study on contributions from two neighborhoods}
\vspace{-1mm}
The proposed Geom-GCN aggregates ``message'' from two neighborhoods which are defined in graph and latent space respectively. 
In this section, we present an ablation study to evaluate the contribution from each neighborhood though constructing new Geom-GCN variants with only one neighborhood. 
For the variants with only neighborhood in graph, we use ``g'' as a suffix of their name (e.g., Geom-GCN-I-g), and use suffix ``s'' to denote the variants with only neighborhood in latent space (e.g., Geom-GCN-I-s).
Here we set GCN as a baseline so that the contribution can be measured via the performance improvement comparing with GCN.
The results are summarized in Table \ref{tab:contribution}, where positive improvement is denoted by an up arrow  and negative improvement by a down arrow . The best performing method is highlighted.

We also design an index denoted by  to measure the homophily in a graph,

A large  value implies that the homophily, in term of node label, is strong in a graph, i.e., similar nodes tend to connect together.
From Table \ref{tab:contribution}, one can see that assortative graphs (e.g., citation networks) have a much larger  than disassortative graphs (e.g., WebKB networks).

Table \ref{tab:contribution} exhibits three interesting patterns: 
i) Neighborhoods in graph and latent space both benefit the aggregation in most cases;  
ii) Neighborhoods in latent space have larger contributions in disassortative graphs (with a small ) than assortative ones, which implies relevant information from disconnected nodes is captured effectively by the neighborhoods in latent space;
iii) To our surprise, several variants with only one neighborhood (in Table \ref{tab:contribution}) achieve better performances than the variants with two neighborhoods (in Tabel \ref{tab:res}). 
We think the reason is that Geom-GCN with two neighborhoods aggregate more irrelevant ``messages'' than Geom-GCN with only one neighborhood, and the 
irrelevant ``messages'' adversely affect the performance. 
Thus, we believe an attention mechanism can alleviate this issue-- which we will study as future work. 

\vspace{-0.5mm}
\begin{table*}[!hbtp]
    \centering
    \small
    \caption{Mean Classification Accuracy (Percent)}
    \vspace{-1.5mm}
    \label{tab:contribution}
    \begin{tabular}{lccccccccc}
        \toprule
        \multicolumn{1}{l}{\textbf{Dataset}}  &Cora  &Cite.  &Pumb.  & Cham.  &Squi.  &Actor  & Corn.  &Texa.  &Wisc.\\
          &0.83  &0.71  &0.79  &0.25  &0.22  &0.24  & 0.11  &0.06  &0.16 \\
        \midrule
        \multirow{2}*{Geom-GCN-I-g}  &86.26  &\textbf{80.64}  &\textbf{90.72}  &\textbf{68.00}  &\textbf{46.01}  &31.96  &65.40  &72.51  &68.23 \\
        &0.48  &6.96  &2.59  &39.82  &22.05  &4.04  &12.70  &21.35  &22.35  \\
        \midrule
        \multirow{2}*{Geom-GCN-I-s}  &77.34  &72.22  &85.02  &61.64  &37.98  &30.59  & 62.16  &60.54  &64.90 \\
        &8.34  &1.46  &3.11  &33.46  &14.02  &2.67  &9.46  &8.38  &19.01 \\
        \midrule
        \multirow{2}*{Geom-GCN-P-g}  &86.30  &75.45  &88.40  &63.07  &38.41  &31.55  &64.05  &73.05  &69.41 \\
        &0.52  &1.76  &0.27  &34.89  &14.45  &3.63  &11.35  &21.89  &23.53 \\
        \midrule
        \multirow{2}*{Geom-GCN-P-s}  &73.14  &71.65  &86.95  &43.20  &30.47  &\textbf{34.59}  &\textbf{75.40}  &\textbf{73.51}  &\textbf{80.39} \\
        &12.63  &2.04  &1.18  &15.02  &6.51  &6.67  &22.70  &21.35  &34.51 \\
        \midrule
        \multirow{2}*{Geom-GCN-S-g}  &\textbf{87.00}  &75.73  &88.44  &67.04  &44.92  &31.27  &67.02  &71.62  &69.41 \\
        &1.23  &2.04  &0.31  &38.86  &20.96  &3.35  &14.32  &19.46 &23.52\\
        \midrule
        \multirow{2}*{Geom-GCN-S-s}  &66.92  &66.03  &79.41  &49.21  &31.27  &30.32  &62.43  &63.24  &64.51 \\
        &18.85  &7.65  &8.72  &21.03  &7.31  &2.40  &9.73  &11.08  &18.63 \\
        \bottomrule
    \end{tabular}
    \vspace{-1mm}
\end{table*}

\subsubsection{Analysis of embedding space combination}
The structural neighborhood in Geom-GCN is very flexible, where one can combine arbitrary embedding space.
To study which combination of embedding spaces is desirable, we construct new Geom-GCN variants by adopting neighborhoods built by different embedding space. 
For the variants adopted Isomap and poincare embedding space to build neighborhood in graph and in latent space respectively,  we use Geom-GCN-IP to denote it. 
The naming rule is the same for other combinations. 
The performances of all variants are summarized in Table \ref{tab:experiment_two}.
One can observe that several combinations achieve better performance than Geom-GCN with neighborhoods built by only one embedding space (in Table \ref{tab:res}); and there are also many combinations that have bad performance. 
Thus, we think it's significant future work to design an end-to-end framework that can automatically determine the right embedding spaces for Geom-GCN.

\begin{table*}[!hbtp]
    \vspace{-1mm}
	\centering
	\caption{Mean Classification Accuracy (Percent)}
	\vspace{-1mm}
	\label{tab:experiment_two}
	\begin{tabular}{lccccccccc}
		\toprule
		\multicolumn{1}{l}{\textbf{Dataset}} &Cora &Cite. &Pubm. &Cham. &Squi. &Actor &Corn. &Texa. &Wisc. \\
		\midrule
		Geom-GCN-IP &85.13 &\textbf{79.41} &\textbf{90.49} &65.77 &\textbf{45.49} &\textbf{31.94} &\textbf{60.00} &66.49 &62.75 \\
		Geom-GCN-PI &85.09 &75.08 &85.64  &59.19 &32.65  &29.16 &58.11 &58.11 &58.63 \\
  		Geom-GCN-IS &84.51 &77.83 &88.66 &58.40 &35.29 &29.41 &54.32 &57.57 &57.65 \\
  		Geom-GCN-SI &85.31 &75.50 &85.52 &62.13 &32.57 &28.97 &57.30 &60.00 &55.10 \\
		Geom-GCN-PS &\textbf{85.65} &74.84 &84.96 &56.34 &28.27 &29.53 &58.11 &62.43 &60.59 \\
		Geom-GCN-SP &85.43 &75.71 &88.00 &\textbf{65.81} &44.53 &31.16 &58.38 &\textbf{67.84} &\textbf{65.10} \\
		\bottomrule
	\end{tabular}
	\vspace{-1mm}
\end{table*}


\subsubsection{Analysis of time complexity}
\vspace{-1mm}
Time complexity is very important for graph neural networks because real-world graphs are always very large. In this subsection, we  firstly present the theoretical time complexity of Geom-GCN and then compare the real running time of GCN, GAT, and Geom-GCN.

To update the representations of one node, the time complexity of Geom-GCN is  where  is the size of input representations,  is the number of hidden unit in non-linear transform for each virtual node (i.e., ), and  is the number of virtual nodes.
Geom-GCN has  times complexity than GCN whose time complexity is .

We also compare the real running time (500 epochs) of GCN, GAT, and Geom-GCN on all datasets with the hyper-parameters described in Section 4.2. 
Results are shown in Fig. \ref{fig:running_time_tsn} (a). 
One can see that GCN is the fastest, and GAT and Geom-GCN are on the same level.
An important future work is to develop accelerating technology so as to solve the scalability of Geom-GCN.


\begin{figure*}[!hbt]
\vspace{-1mm}
\centering
\begin{tabular}{cc}
\includegraphics[width=0.47  \textwidth]{running_time.png} &
\includegraphics[width=0.47  \textwidth]{tsn_embedding.jpg} \\
(a) & (b) 
\end{tabular}
\vspace{-2mm}
\caption{
(a) Running time comparison. GCN, GAT, and Geom-GCN both run 500 epochs, and  axis is the log seconds. 
GCN is the fastest, and GAT and Geom-GCN are on the same level.
(b) A visualization for the feature representations of Cora obtained from Geom-GCN-P in a 2-D space. 
Node colors denote node labels. 
There are two obvious patterns, nodes with the same label exhibit a spatial clustering and all nodes distribute radially. 
The radial pattern indicates graph's hierarchy learned by Poincare embedding.}
\vspace{-3.5 mm}
\label{fig:running_time_tsn}
\end{figure*}

\vspace{-1mm}
\subsubsection{Visualization}
\vspace{-1mm}
To study what patterns are learned in the feature representations of node by Geom-GCN, we visualize the feature representations extracted by the last layer of Geom-GCN-P on Cora dataset by mapping it into a 2-D space though t-SNE \citep{maaten2008visualizing}, as shown in Fig.~\ref{fig:running_time_tsn} (b).
In the figure, the nodes with the same label exhibit spatial clustering, which could shows the discriminative power of Geom-GCN. 
That all nodes distribute radially in the figure indicates the proposed model learn graph's hierarchy by Poincare embedding.

\vspace{-1mm}
\subsection{Conclusion and future work}
\vspace{-1mm}
We tackle the two major weaknesses of existing message-passing neural networks over graphs-- losses of discriminative structures and long-range dependencies. 
As our key insight, we bridge a discrete graph to a continuous geometric space via graph embedding.
That is, we exploit the principle of convolution: \emph{spatial aggregation over a meaningful space}-- and our approach thus extracts or ``recovers" the lost information (discriminative structures and long-range dependencies) in an embedding space from a graph.
We proposed a general geometric aggregation scheme and instantiated it with several specific Geom-GCN implementations, and our experiments validated clear advantages over the state-of-the-art.
As future work, we will explore techniques for choosing a right embedding method-- depending not only on input graphs but also on target applications, such as epidemic dynamic prediction on social contact network \citep{DBLP:journals/pami/YangPCLX17,pei2018group}. 

\vspace{-1mm}
\subsubsection*{Acknowledgments}
\vspace{-1mm}
We thank the reviewers for their valuable feedback. 
This work was supported in part by National Natural Science Foundation of China under grant 61876069, 61572226 and 61902145,
National Science Foundation IIS 16-19302 and IIS 16-33755,
Jilin Province Key Scientific and Technological Research and Development project under grants 20180201067GX and 20180201044GX, 
University science and technology research plan project of Jilin Province under grants JJKH20190156KJ, 
Zhejiang University ZJU Research 083650, Futurewei Technologies HF2017060011 and 094013, UIUC OVCR CCIL Planning Grant 434S34, UIUC CSBS Small Grant 434C8U, Advanced Digital Sciences Center Faculty Grant, 
and China Scholarships Council under scholarship 201806170202.
Any opinions, findings, and conclusions or recommendations expressed in this publication are those of the author(s) and do not necessarily reflect the views of the funding agencies.


\bibliography{iclr2020_conference}
\bibliographystyle{iclr2020_conference}

\end{document}
