



\documentclass[11pt]{article}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{arabtex}
\usepackage{utf8}
\usepackage{multirow}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{svg}
\usepackage[T1]{fontenc}





\usepackage{siunitx}
\usepackage{authblk}
\usepackage{coling2020}
\usepackage{hyperref}

\newcommand\nnfootnote[1]{\begin{NoHyper}
  \renewcommand\thefootnote{}\footnote{#1}\addtocounter{footnote}{-1}\end{NoHyper}
}

\DeclarePairedDelimiter\abs{\lvert}{\rvert}\DeclarePairedDelimiter\norm{\lVert}{\rVert}

\hypersetup{
    colorlinks=true,
    linkcolor=red,
    citecolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
}



\begin{document}
\title{Deep Diacritization: Efficient Hierarchical Recurrence for Improved Arabic Diacritization}


\author[1]{Badr AlKhamissi}
\author[1,2]{Muhammad N. ElNokrashy}
\author[2]{Mohamed Gabr}

\affil[ ]{
    {\tt
    \href{mailto:badr@khamissi.com}{badr@khamissi.com},
    \href{mailto:muhammad.nael@gmail.com}{muhammad.nael@gmail.com},
    \href{mailto:mohamed.gabr@hotmail.com}{mohamed.gabr@hotmail.com}}
}

\affil[1]{The American University in Cairo (AUC)}

\affil[2]{Microsoft Egypt Development Center (EGDC)
}

\renewcommand\Affilfont{\fontsize{10}{10}\itshape}



\setcode{utf8}
\maketitle

\begin{abstract}
    We propose a novel architecture for labelling character sequences that achieves state-of-the-art results on the Tashkeela Arabic diacritization benchmark. The core is a two-level recurrence hierarchy that operates on the word and character levels separately---enabling faster training and inference than comparable traditional models. A cross-level attention module further connects the two, and opens the door for network interpretability. The task module is a softmax classifier that enumerates valid combinations of diacritics. This architecture can be extended with a recurrent decoder that optionally accepts priors from partially diacritized text, which improves results. We employ extra tricks such as sentence dropout and majority voting to further boost the final result. Our best model achieves a WER of 5.34\%, outperforming the previous state-of-the-art with a 30.56\% relative error reduction.
\end{abstract}

\section{Introduction}
\label{intro}

\blfootnote{
\hspace{-0.65cm}  This work is licensed under a Creative Commons 
    Attribution 4.0 International Licence.
    Licence details:
    \url{http://creativecommons.org/licenses/by/4.0/}.
}
\blfootnote{\hspace{-0.65cm}
    Affiliation emails:
    \fontsize{8.5}{9}
    {\tt \{\href{mailto:balkhamissi@aucegypt.edu}{balkhamissi}, \href{mailto:m.n.elnokrashy@aucegypt.edu}{m.n.elnokrashy}\}@aucegypt.edu};
    {\tt \{\href{mailto:muelnokr@microsoft.com}{muelnokr},
      \href{mailto:mogabr@microsoft.com}{mogabr}\}@microsoft.com}
}
\blfootnote{\hspace{-0.25cm}
    *\,This work is not sponsored by the affiliated institutions of the authors.}
\blfootnote{\hspace{-0.25cm}
    **\,This work was accepted at the Fifth Arabic Natural Language Processing Workshop (WANLP 2020).}

The Arabic script (and similarly Hebrew, Aramaic, Pahlavi...) is an impure abjad. These writing systems represent short consonants and long vowels using full letter graphemes, but generally omit short vowels and consonant length from writing. This leaves the task of inferring the missing phonemes to the reader by using context from neighbouring words and knowledge of the language structure to determine the correct pronunciation and disambiguate the meaning of the text. Those sounds are represented by diacritical marks---small graphemes that appear usually above or below a basic letter in the abjad. Table \ref{diac-table} shows the diacritics considered in this work. Diacritics are usually utilized in specific domains where it is important to explicitly clear up ambiguities or where inferring the correct forms might be difficult for non-experts, such as religious texts, some literary works such as poetry, and language teaching books as novice readers have yet to build up the intuition for reading undiacritized text.

We focus in this work on diacritization of Arabic texts. However, our proposed architecture has no explicitly language-dependent components and should be adaptable for other character sequence labelling tasks. Although it is the first language of several million people, and is spoken in some of the fastest growing markets \cite{tinsley_board}, the Arabic language, like many others, lacks attention from the NLP community compared to established test bed languages such as English or Chinese, which both enjoy higher momentum and an abundance of established resources and techniques. The automatic restoration of diacritics to Arabic text is arguably one of the most important NLP tasks for the Arabic language. Besides direct applications like facilitating learning, diacritics are used to enhance language modeling, acoustic modeling for speech recognition, morphological analysis, machine translation, and text-to-speech systems (which need to restore the lost phonemes to render words properly) \cite{zitouni09,azmi_2013}.

To illustrate this further, Table \ref{elm-table} shows the Arabic word \textit{Elm}\footnote{This paper uses Buckwalter transliteration \label{foot:buckwalter}} in different diacritized forms with their corresponding English translations, showcasing the importance of diacritics in resolving ambiguity. Note that the MADA \cite{MADA} morphological analyzer produces at least 13 different forms for this undiacritized word \cite{belinkov-glass-2015-arabic}.

\begin{table}[ht]
\centering
\begin{tabular}{|r|l|l|}
\hline 
\bf Arabic (diacritized) & \bf Transliteration & \bf English Translation \\ \hline
\<عَلِمَ>   &   \textit{Ealima}          & He knew \\
\<عُلِمَ>   &   \textit{Eulima}          & It was known \\
\<عَلَّمَ>   &   \textit{Ealama}    & He taught \\
\<عِلْمُ>   &   \textit{Eilomu}          & Knowledge \\
\<عَلَمُ>   &   \textit{Ealamu}          & Flag \\
\hline
\end{tabular}
\caption{Subset of possible diacritized forms for \textit{Elm} adapted from \protect\cite{belinkov-glass-2015-arabic} }
\label{elm-table}
\end{table}

Table \ref{diac-table} shows the different diacritics commonly used in Arabic texts along with their phonemic symbols. They fit roughly into four kinds. (1) \d{H}arakāt are diacritics for short vowels; we have three: fat\d{h}ah, kasrah, dammah. The symbols for those vowels have another form (usually a visual doubling) used at the end of a word to form a (2) tanwīn, or nunation, which is a VC sound of the \d{h}arakah's vowel followed by the consonant ``n'' (). (3) The shaddah is the gemination symbol used to indicate consonant doubling. It can be combined with one of the \d{h}arakāt or tanwīn on the same character. Finally, (4) the sukūn is used to indicate that the current consonant is not followed by a vowel and instead forms a cluster with the next consonant. Diacritics which appear at the end of a word are referred to as case-endings (CE); most of which are specified by the syntactic role of the word. They are harder to infer than the core-word diacritics (CW) that specify lexical selection and appear on the rest of the word \cite{mubarak19-highly}.

\begin{table}[ht]
\begin{center}
\begin{tabular}{|r|l|l|l|l|}
\hline 
\bf Symbol & \bf Name & \bf Type & \bf Transliteration & \bf IPA phoneme \\ \hline
\<هُ> & dammah       & \d{h}arakāt   & \it u & /u/ \\
\<هَ> & fathah       & \d{h}arakāt   & \it a & /a/ \\
\<هِ> & kasrah       & \d{h}arakāt   & \it i & /i/ \\
\<هٌ> & dammatain    & tanwīn        & \it N & /un/ \\
\<هً> & fathatain    & tanwīn        & \it F & /an/ \\
\<هٍ> & kasratain    & tanwīn        & \it K & /in/ \\
\<هّ> & shaddah      & shaddah       &  &  /h:/ Gemination. \\
\<هْ> & sukūn        & sukūn         & \it o & No vowel. \\
\hline
\end{tabular}
\end{center}
\caption{Primary Arabic diacritics on letter \<ه>}
\label{diac-table}
\end{table}

The paper is structured as follows: First we cover some of the approaches used in related works on restoring Arabic diacritics. Then we introduce our system and support it by comparing experimental results on an adapted version of the Tashkeela corpus \cite{tashkeela17} proposed by \cite{fadel19} as a standard benchmark for Arabic diacritization systems. Each design decision will then be motivated by an ablation study. We analyze the learned attention model then discuss existing limitations in an error analysis. Finally, we offer directions for future work.

\section{Related Work}

The literature surrounding the automatic diacritization of Arabic text provides methods in two categories: classical rule-based solutions, and statistical modeling-based methods. Early approaches have worked on constructing a large set of language specific rules to restore the lost diacritics \cite{MADA,zitouni06,pasha14-madamira,darwish17}. Researchers have then shifted to rely more on learning-based methods that do not require extra expert systems such as morphological analyzers and part-of-speech taggers. \cite{belinkov-glass-2015-arabic} have shown that recurrent neural networks are suitable candidate models for learning the task entirely from data and can be easily extended to other languages and dialects without the use of manually engineered features. Other methods such as hidden Markov models (HMMs) \cite{elshafei06}, conditional random fields (CRFs) \cite{darwish17}, maximum-entropy models \cite{zitouni06} and finite-state transducers \cite{shieber05} have similarly been employed. However, more recent works have started to use deep (neural-based) architectures such as sequence-to-sequence transformers and recurrent cell-based models inspired by work in Neural Machine Translation \cite{mubarak19-highly}. Solutions combining both rule-based and deep learning methods appear in recently published work \cite{hamza20}. \cite{joint2020} have shown that the diacritization task benefits from jointly modelling lexicalized and non-lexicalized morphological features instead of targeting only the diacritization task.

\section{Approach}

\subsection{Datasets}
We report on the cleaned version of the Tashkeela corpus \cite{fadel19}---a high quality, publicly available dataset. It is split into train (\si{2,\!449 k} tokens), dev (\si{119 k} tokens), and test (\si{125 k} tokens) sets.

\subsection{Architecture}
\label{sub:arch}

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\linewidth]{figures/diagram-v18h.png}
    \caption{D3 Architecture}
    \label{fig:arch}
\end{figure}


In this work, we propose two models: The Two-Level Diacritizer (D2) and the Two-Level Diactritizer with Decoder (D3). D3 extends D2 by allowing partially diacritized text to be taken as input---a much needed feature for neural diacritizers \cite{fadel19}. D2 outperforms fully character-based models in both task and runtime performance measures.
\subsubsection{Two-Level (Hierarchical) Model}
\label{arch:hie}
Restoring diacritics can be seen as a character sequence labeling task. Each label depends on word- and character-level context. This structure motivates the hierarchy in our two-level encoder architecture---the first encoder sees the sequence of words (where words are atoms) and provides word-level context for the second, character-level encoder. The character-level encoder is evaluated independently for each word in the sentence, enabling much faster training and inference compared to character-level recurrent cell-based models of similar structure to previous works \cite{belinkov-glass-2015-arabic,mubarak19-highly,joint2020,fadel19-neural}. Let  be the maximum number of words allowed in a sentence and  the maximum number of characters allowed in a word. Then the overall character sequence length that a model in prior works would see is in the order of . In contrast, our approach operates on a maximum sequence length of  at the word level and  at the character level. Because character-level recurrence is independent of characters outside the current word, the serial bottleneck complexity goes from  down to , assuming adequate parallelization across word units. See Table \ref{speed-table} for a speed comparison.

Let  denote a sequence of words.  are the corresponding fastText features pretrained on CommonCrawl Arabic data \cite{fastText17}. Let  denote the sequence of characters for the word at index  in sentence . Each character is assigned a 32-dimensional learned embedding . Let  denote the feature vector from the word-level encoder which maps each word in  to a contextual word representation. Let  denote the character-level recurrence that
outputs a contextual encoding of the character relative to its parent word and sentence. See Figure \ref{fig:arch} for an overview. Formally, the contextual embedding  of  is

Where  is the attention view. Both  and  use Bidirectional LSTM (Bi-LSTM) layers \cite{graves05} trained with backpropagation through time. We note that any similar sequence modelling architecture would be applicable \cite{gru14,attention} but leave that to future work.

\subsubsection{Cross-level Attention Module}
\label{arch:attn}
This module attends over the word-level encodings  based on the character encodings  of each character in a word. In other words, it uses the initial contextualization of the characters () to attend to all words in the sentence (except the current) to refine the character's representation. We use the attention formulation from \cite{attention}. For each character , we calculate 



where


 where in eq. \eqref{eq:Attention}, , , , and  are independent linear layers. We tried to remove  but faced lower performance.


\subsubsection{Decoder}
Used in D3, this component is a forward-only LSTM that takes as input a concatenation of the basic contextual character embedding  and a one-hot representation of the output of the previous character from the classifier module. Formally: . The  signal passed to the decoder also encodes the Beginning-of-Word in addition to the previous-character diacritics. This allows the model to accept partially diacritized sentences such that the ground truth diacritic is injected in place of  during inference. This feature is important as many Arabic texts contain sparse diacritics that act as hints to assist readers. Having this clean signal yields improvements as shown in Figure \ref{fig:pd}.

\subsubsection{Task Objective}
The final classifier optimizes a Softmax objective over an enumeration of all valid diacritic combinations. Combined, we have 3 \d{h}arakāt in 4 variants (with tanwīn, shadda, tanwīn and shadda, and neither), the sukūn, and the plain shadda. Thus 15 classes including the None (no diacritic) class.

\subsection{Experimental Setup}

\subsubsection{Parameters, Hyper-parameters, and Regularization}

\paragraph{Optimization}
We use the Adam optimizer \cite{adam14} with an initial learning rate of . The model is left to converge until the validation loss does not improve for  consecutive epochs where each epoch enumerates a randomly shuffled version of the training segments exactly once. The learning rate is reduced by half when the validation loss does not improve for one epoch. We train with a mini-batch size of  segments.

\paragraph{Encoders and Decoder}
\label{exp:enc-dec}
The word and character level encoders are \emph{each} a 2-layer stacked Bi-LSTM with  and  hidden units, respectively. We apply feature-level dropout \cite{dropout} with probability  to the input of the character level encoder. The decoder in D3 is a one-layer forward-only LSTM with  hidden units. All recurrent cells use a vertical and recurrent dropout of  each. The recurrent dropout used is untied between time-steps, in contrast to \cite{rec-dropout}.

\paragraph{Context Window and Voting}
\label{exp:voting}
Similar to \cite{mubarak19-highly}, we use a sliding context window of size  on each sentence. A given sentence is split into several overlapping segments each of which is given separately to the model during training. This works well as the local context is often sufficient for correct inference. During inference, the same sequence of characters may appear in different contexts (different segments from one sentence) and potentially lead to different diacritized forms. To choose the final diacritic, we use a popularity voting mechanism and, in the case of a tie, choose one of the outputs at random. The values chosen for ,  and the \textit{stride} are:  with  for training and validation (a small  was observed to stabilize training and improve results);  with  for evaluation/testing; and  for both training and evaluation.

\paragraph{Sentence Dropout}
\label{exp:sdo}
We randomly dropout 20\% of the words given to the word-level encoder during training. The positions of the dropped out words are preserved, and their embedding vectors  are replaced with zeros. This was observed to lead to better generalization in some cases.

\subsubsection{D3 Training}
\label{d3-training}
This model is not trained from scratch, but uses the weights of the encoders and character embeddings learned from D2. Those weights are kept frozen and only the decoder and classifier are trained.

\paragraph{Ramp-up of Teacher-forcing Signal}
\label{exp:gt-rampup}
We pass the ground truth of  of the previous-character diacritics as input to the decoder at the current time-step. This value is ramped up from  (all characters receive previous diacritics as zeros; i.e. no signal) to  (all characters receive ground truth of previous diacritic as signal). This is done over a period of  epochs in increments of . Then the model is left to converge using the same stopping criteria as in D2. We found this to be the best approach as otherwise the model overfits early on the teacher forcing signal given from the previous time-step. 

\subsubsection{Source Code}
The code is made open source and is available on GitHub\footnote{\url{https://github.com/bkhmsi/deep-diacritization}}. We also provide an accompanying web application to demo the proposed models which can be found at this web address\footnote{\url{https://deep-diacritization.herokuapp.com/}}.
The system uses PyTorch for implementing the neural training and inference components \cite{pytorch19}.
The LSTM cell implementation used is due to \cite{pt-rnn}\footnote{\url{https://github.com/munael/pt-rnn}}.

\subsection{Results}

\begin{table}[h]
\begin{tabular}{|l|c|c|c|c|}
\hline
\multicolumn{1}{|c|}{\multirow{2}{*}{DER/WER}} & \multicolumn{2}{c|}{Including `no diacritic'} & \multicolumn{2}{c|}{Excluding `no diacritic'} \\ \cline{2-5} 
\multicolumn{1}{|c|}{} & \multicolumn{1}{l|}{w/ case ending} & \multicolumn{1}{l|}{w/o case ending} & \multicolumn{1}{l|}{w/ case ending} & \multicolumn{1}{l|}{w/o case ending} \\ \hline
\cite{shakkala}         & 3.73\% / 11.19\%          & 2.88\% / 6.53\%         & 4.36\% / 10.89\%           & 3.33\% / 6.37\%            \\ \hline
\cite{fadel19-neural}   & 2.60\% / 7.69\%           & 2.11\% / 4.57\%         & 3.00\% / 7.39\%            & 2.42\% / 4.44\%            \\ \hline
\cite{hamza20}          & 3.39\% / 9.94\%           & 2.61\% / 5.83\%         & 3.34\% / 7.98\%            & 2.43\% / 3.98\%            \\ \hline
D2 (Ours)               & 1.85\% / 5.53\%           & 1.49\% / 3.27\%         & 2.11\% / 5.26\%            & 1.71\% / 3.15\%            \\ \hline
D3 (Ours) (@0\% hints)  & \textbf{1.83\%} / \textbf{5.34\%}  & \textbf{1.48\%} / \textbf{3.11\%}  & \textbf{2.09}\%  / \textbf{5.08\%}     & \textbf{1.69\%} / \textbf{3.00\%}   \\ \hline
\end{tabular}
\caption{Results on the Tashkeela benchmark}
\label{tashkeela-results}
\end{table}

We use the script provided by \cite{fadel19} to evaluate our results. To be consistent with prior work, we report our results in terms of both word error-rate (WER) and diacritic error-rate (DER), with and without case-endings, as well as including and excluding characters with no diacritics. Table \ref{tashkeela-results} shows our results on the Tashkeela benchmark in comparison with the more recent works. We outperform state-of-the-art by 30.56\% relative ( absolute) error reduction on ``WER with case-ending''.

\begin{table}[h]
\begin{center}
\begin{tabular}{|l|r|r|r|r|c|}
\hline
\bf Method         & \bf \#Params & \bf T/epoch & \bf Convergence & \bf Inference    & \bf Full DER/WER   \\ \hline
\bf D2 -- \{Attn\} &     13.369M  & \bf 28 mins & 17 epochs       & \bf 34,996   wps & \bf 1.94\% / 5.80\%  \\
\bf Flat           & \bf 13.304M  & 121 mins    & \bf 13 epochs   &      2,466   wps &     2.20\% / 6.39\%  \\
\hline
\end{tabular}
\end{center}
\caption{Speed Comparison\protect\footnotemark}
\label{speed-table}
\end{table}
\footnotetext{All models use the same custom LSTM implementation and are run on a single Nvidia GeForce RTX 2080 Ti.}
Table \ref{speed-table} compares our plain 2-level hierarchy design (without Attention) with a ``Flat" model in task and runtime performance. The Flat model comprises a 4-layer stacked Bi-LSTM with similar implementation details as described in \ref{exp:enc-dec} for D2, including Sentence Dropout and the Voting mechanism. The Flat model sees each sentence as one sequence of characters.
\paragraph{Partially Diacritized Text}
Figure \ref{fig:pd} shows the results of DER including `no diacritic' with and without case ending when the model is supplied with partially diacritized text as input. For each character in the sentence, with some probability, we may replace the predicted output of the previous time-step with the ground truth as input to the decoder in the current step. The reported output of the previous time-step is masked to force the provided hint to be the model's ``prediction'' even if the inferred were different (see Figure \ref{fig:pd_der_masked})---in contrast to Figure \ref{fig:pd_der_raw} where the final predictions are the model's unmodified outputs. The results are averaged across five runs with different seeds (i.e. injecting the ground truth signal at different characters in the sentence). Error bars represent standard deviation. Many Arabic texts already come with some hints that can improve model performance. Here we show how a neural model could be trained to leverage that.

\begin{figure}[h]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/pd_der_raw.png}
  \caption{DER vs Percentage of Injected Hints (actual output)}
  \label{fig:pd_der_raw}
\end{subfigure}\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/pd_der_forced.png}
  \caption{DER vs Percentage of Injected Hints (hints covering output)}
  \label{fig:pd_der_masked}
\end{subfigure}
\caption{Error Rate of Partially Diacritized Text}
\label{fig:pd}
\end{figure}


\section{Discussion}

\subsection{Ablation Study}

We conduct an ablation study to measure the effect of components proposed for the final model. We train and evaluate the D2 model previously detailed but with the component(s) specified removed. Table \ref{tab:ablation} shows the results after removing the sentence dropout and cross-level attention module.

\begin{table}[h]
\centering
\begin{tabular}{|l|r|r|r|r|}
\hline
\multicolumn{1}{|c|}{\multirow{2}{*}{DER/WER}} & \multicolumn{2}{c|}{Including `no-diacritic'}                              & \multicolumn{2}{c|}{Excluding `no-diacritic'}                              \\ \cline{2-5} 
\multicolumn{1}{|c|}{}                         & \multicolumn{1}{l|}{w/ case ending} & \multicolumn{1}{l|}{w/o case ending} & \multicolumn{1}{l|}{w/ case ending} & \multicolumn{1}{l|}{w/o case ending} \\ \hline
\cite{fadel19-neural}                          & 2.60\% / 7.69\%                     & 2.11\% / 4.57\%                      & 3.00\% / 7.39\%                     & 2.42\% / 4.44\%                      \\ \hline
D3 (@0\% hints)                                & \textbf{1.83\%} / \textbf{5.34\%}  & \textbf{1.48\%} / \textbf{3.11\%}  & \textbf{2.09}\%  / \textbf{5.08\%}     & \textbf{1.69\%} / \textbf{3.00\%}    \\ \hline
D2                                             & 1.85\% / 5.53\%                     & 1.49\% / 3.27\%                      & 2.11\% / 5.26\%                     & 1.71\% / 3.15\%                      \\ \hline
D2  \{Attention \ref{arch:attn}\}           & 1.94\% / 5.80\%                     & 1.58\% / 3.44\%                      & 2.23\% / 5.52\%                     & 1.80\% / 3.31\%                      \\ \hline
D2  \{SDO \ref{exp:sdo}\}                   & 1.91\% / 5.71\%                     & 1.54\% / 3.36\%                      & 2.18\% / 5.43\%                     & 1.75\% / 3.23\%                      \\ \hline
D2  \{Attn, SDO\}                           & 1.93\% / 5.78\%                     & 1.57\% / 3.45\%                      & 2.21\% / 5.49\%                     & 1.79\% / 3.32\%                      \\ \hline
\end{tabular}
\caption{Ablation Study}
\label{tab:ablation}
\end{table}

\subsection{Attention Analysis}

The cross-level attention module allows us to gauge the contribution of each word to each output diacritic. Here we examine some examples to see whether the model was able to learn such Arabic grammar rules as a human expert would use when annotating case endings. The examples presented in this section reflect patterns we have found repeated during our analysis.

\begin{figure}[!h]
\centering
\begin{subfigure}[t]{.49\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{attn_figures/ahmed_borto2an_we_tofa7.png}
  \caption{Ex. of case agreement by \d{h}arf-atf}
  \label{fig:ano_1}
\end{subfigure}\begin{subfigure}[t]{.49\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{attn_figures/attn_1.png}
  \caption{Ex. of CE of ism-majrūr depending on \d{h}arf-jarr}
  \label{fig:ano_2}
\end{subfigure}
\caption{Attention visualization of words correctly attending to grammatical parents.}
\label{fig:attn_ano_1}
\end{figure}

The pattern in Figure \ref{fig:ano_1} is related to the \d{h}arf-atf\footnote{\<حرف عطف>} rule (generally prepositions), which states that the word coming after it gets the same case as the main word of the phrase it is related to---the grammatical parent. We see indeed that the word coming after the (``w'') \d{h}arf-atf attends the most on the word that comes before it. This is similar to what an expert would do; look at the main word in the phrase preceding the ``w'' in order to determine the case and case-ending of what follows.

\begin{figure}[h]
\centering

\begin{subfigure}[t]{.49\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{attn_figures/attn_2.png}
  \caption{\emph{Self} attends on a local word with similar \emph{effect} on \emph{Self}}
  \label{fig:ano_3}
\end{subfigure} \begin{subfigure}[t]{.49\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{attn_figures/baynama_tomares_al_remaya.png}
  \caption{\emph{Self} attends on a local word with similar \emph{role} to \emph{Self}}
  \label{fig:ano_4}
\end{subfigure}
\caption{Attention visualization of confusion of grammatical parents.}
\label{fig:attn_ano_2}
\end{figure}

Figure \ref{fig:ano_2} shows another prevalent example where the word in question attends the most on the \d{h}arf-jarr\footnote{\<حرف جر>} preceding it. However, in other cases where the same rule appears twice in a segment, we found that the model may choose to attend equally or more on the components of the first occurrence rather than the occurrence the current word is actually affected by---the grammatical parent. Figure \ref{fig:ano_3} shows one example of this where the word (``zmylth'') attends equally to two words that would affect it the same (``<lY'' and ``mE''), but only the second should be affecting it. In Figure \ref{fig:ano_4} we see that the second maf'ool-bih\footnote{\<مفعول به>} (roughly an object of a verb) (``AlrmAyp'') attends heavily on the first occurrence of a mf'ool-bih in the segment (``Altjdyf''), rather than the verb that should be affecting it (``tmArs''). This behavior of attending on a previous word with a similar role suggests that the attention mechanism is aware of grammatical rules; it is able to group words with the same role together.

Generally, we found that not all sentences yield interpretable attention weights. We leave the task of comprehensively studying the extent of agreement of the learned weights with Arabic grammatical rules to future work.

\subsection{Error Analysis}

\begin{figure}[h]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.95\linewidth]{figures/case_ending_final3.png}
  \caption{Confusion Matrix for Case Endings (Errors Only)}
  \label{fig:conf_ce}
\end{subfigure}\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.95\linewidth]{figures/core_words_final_3.png}
  \caption{Confusion Matrix for Core Word Diacritics (Errors Only)}
  \label{fig:conf_core}
\end{subfigure}
\caption{Error Confusion Matrix for CE and Core Diacritics}
\label{fig:conf}
\end{figure}

Figures \ref{fig:conf_ce} and \ref{fig:conf_core} show the the confusion matrices (for visualization clarity we show errors only) of case ending and core words diacritics respectively. Many confusions are between the dammah and kasrah, and between dammah and fat\d{h}ah, and the confusion goes both ways. We analyze the errors between kasrah and dammah for case endings and try to correlate them with grammar rules. 

The first error example is related to the start of a new sentence in Arabic grammar. The word “wa” composed of one letter can either mark a conjunction (e.g. in an enumeration), or mark the start of a new sentence, based on context. An example with such confusion is in the following sentence, with the ground truth: ``wa yarudu Ealayohi >ana faAqida AlTahuwrayoni wa naHowahu layosa lahu SalaApN
<laA <*aA DaAqa Alowaqotu''
\footnote{\<وَيَرُدُّ عَلَيْهِ أَنَّ فَاقِدَ الطَّهُورَيْنِ وَنَحْوَهُ لَيْسَ لَهُ صَلَاةٌ إلَّا إذَا ضَاقَ الْوَقْتُ>}
. We see one confusion example where ``wa naHow\textbf{a}h'' was predicted as ``wa naHow\textbf{i}h''. Grammatically, the ``wa'' relates the next word to a ``sentence starter'' word (``faAqida'') in a case that would make \textbf{a} the correct diacritic. Instead, we observe it follows the word immediately before the ``wa'' (``AlTahuwr\textbf{ayoni}''), which is indeed in a grammatical case that would make \textbf{i} the correct diacritic for ``naHow\{\textbf{a}/i\}h'', were it the correct grammatical parent of this ``wa''.

The second example is related to the use of punctuation marks that signal an abrupt start of a new sentence or an end of one with unique context that may not be easily learnt. In the following sentence with the ground truth: ``kaqaA\}ilK : AloHar\textbf{u} >awo Alobarod\textbf{u} Al\\sim\sim0.240.24$ as frequent as the most frequent word).

\section{Conclusion}
In this work, we presented a novel architecture that outperforms previously published results on the Tashkeela Arabic diacritization benchmark. Future work may include:
\begin{itemize}
    \setlength\itemsep{-0.3em}
    \item Replacing the word- and character- level Bi-LSTM encoders with transformer-based encoders.
    \item Using byte-pair-encoding (BPE) \cite{bpe-16} to better handle suffixes and prefixes as Arabic is a moderately fusional language.
    \item Investigating more efficient use of injected hints to improve performance.
    \item Training/Evaluating this design/model on Modern Standard Arabic and dialectical benchmarks.
    \item Cleaning the testset of Tashkeela to remove any inconsistencies as described in the error analysis.
    \item Finally, achieving more interpretable attention weights through multi-task training, training on larger datasets, or otherwise.
\end{itemize}

\section*{Acknowledgements}
We offer special thanks to \textit{Khaled Essam}, as well as \textit{Mohamed Afify} and \textit{Ahmed Tawfik} of \textit{Microsoft EGDC}, for many helpful discussions, suggestions and comments on the paper.

\bibliographystyle{acl}
\bibliography{references}

\end{document}
