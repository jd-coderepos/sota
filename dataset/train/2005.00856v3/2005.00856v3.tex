

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2020}
\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}

\usepackage{microtype}

\aclfinalcopy 



\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage[inline]{enumitem}
\usepackage{fancyhdr}
\usepackage{placeins}

\usepackage{caption}
\usepackage{subcaption}
\usepackage[symbol]{footmisc}
\usepackage{listings} \usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsthm,amsmath,amssymb}

\usepackage{mathrsfs}


\usepackage[normalem]{ulem}
\usepackage{cancel}

\usepackage{balance}  

\usepackage{microtype}
\DisableLigatures[f]{encoding=*, family =*}
\usepackage{comment}
\usepackage{mathtools}
\usepackage{bbm}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{pifont}
\usepackage{threeparttable}  
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{bm}
\usepackage{tablefootnote}

\newcommand{\cmark}{\text{\ding{51}}}
\newcommand{\xmark}{\text{\ding{55}}}




\newcommand{\E}{{\mathbb{E}}}
\newcommand{\R}{{\mathbb{R}}}
\newcommand{\todo}[1]{\textcolor{red}{TODO: #1}}

\newcommand{\SegVec}[1]{\mathbf{#1}}
\newcommand{\Outperform}[1]{\mathbf{#1}}
\newcommand{\TableSize}{\footnotesize} \renewcommand{\thefootnote}{\fnsymbol{footnote}}

\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}

\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{SEEK: Segmented Embedding of Knowledge Graphs}

\author{Wentao  Xu, Shun Zheng, Liang He, Bin Shao, Jian Yin\thanks{\ \ Corresponding author.}, and Tie-Yan Liu\\
   School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China;\\
  Guangdong Key Laboratory of Big Data Analysis and Processing, Guangzhou, China\\
   Microsoft Research Asia, Beijing, China \\
  \texttt{\{xuwt6@mail2,issjyin@mail\}.sysu.edu.cn} \\
  \texttt{\{Shun.Zheng,Liang.He,binshao,tyliu\}@microsoft.com} \\
}

\date{}

\begin{document}
\maketitle

\begin{abstract}
In recent years, knowledge graph embedding becomes a pretty hot research topic of artificial intelligence and plays increasingly vital roles in various downstream applications, such as recommendation and question answering.
However, existing methods for knowledge graph embedding can not make a proper trade-off between the model complexity and the model expressiveness, which makes them still far from satisfactory.
To mitigate this problem, we propose a lightweight modeling framework that can achieve highly competitive relational expressiveness without increasing the model complexity.
Our framework focuses on the design of scoring functions and highlights two critical characteristics:
1) facilitating sufficient feature interactions; 
2) preserving both symmetry and antisymmetry properties of relations.
It is noteworthy that owing to the general and elegant design of scoring functions, our framework can incorporate many famous existing methods as special cases.
Moreover, extensive experiments on public benchmarks demonstrate the efficiency and effectiveness of our framework.
Source codes and data can be found at \url{https://github.com/Wentao-Xu/SEEK}.
\end{abstract}
 \section{Introduction}
\label{sec:intro}

Learning embeddings for a knowledge graph (KG) is a vital task in artificial intelligence (AI) and can benefit many downstream applications, such as personalized recommendation~\cite{zhang2016collaborative,wang2018dkn} and question answering~\cite{huang2019knowledge}.
In general, a KG stores a large collection of entities and inter-entity relations in a triple format, , where  denotes the \underline{h}ead entity,  represents the \underline{t}ail entity, and  corresponds to the relationship between  and . 
The goal of knowledge graph embedding (KGE) is to project massive interconnected triples into a low-dimensional space and preserve the initial semantic information at the same time.

Although recent years witnessed tremendous research efforts on the KGE problem, existing research did not make a proper trade-off between the model complexity (the number of parameters) and the model expressiveness (the performance in capturing semantic information).
To illustrate this issue, we categorize existing research into two categories.

The first category of methods prefers the simple model but suffers from poor expressiveness.
Some early KGE methods, such as TransE~\cite{bordes2013translating} and DistMult~\cite{yang2015embedding}, fell into this category.
It is easy to apply these methods to large-scale real-world KGs, but their performance in capturing semantic information (such as link prediction) is far from satisfactory.

In contrast, the second category pursues the excellent expressiveness but introduces much more model parameters and tensor computations.
Typical examples include TransH~\cite{wang2014knowledge}, TransR~\cite{lin2015learning}, TransD~\cite{ji2015knowledge}, Single DistMult~\cite{kadlec2017knowledge}, ConvE~\cite{dettmers2018conve} and InteractE~\cite{vashishth2019interacte}.
However, as pointed out by~\citet{dettmers2018conve}, the high model complexity often leads to poor scalability, which is prohibitive in practice because real-world KGs usually contain massive triples.

To address these drawbacks of existing methods, in this paper, we propose a light-weight framework for KGE that achieves highly competitive expressiveness without the sacrifice in the model complexity.
Next, we introduce our framework from three aspects: 
1) facilitating sufficient feature interactions, 
2) preserving various necessary relation properties, 
3) designing both efficient and effective scoring functions.

First, to pursue high expressiveness with the reasonable model complexity, we need to facilitate more sufficient feature interactions given the same number of parameters. 
Specifically, we divide the embedding dimension into multiple segments and encourage the interactions among different segments.
In this way, we can obtain highly expressive representations without increasing model parameters.
Accordingly, we name our framework as \underline{Se}gmented \underline{E}mbedding for \underline{K}Gs (SEEK). 

Second, it is crucial to preserve different relation properties, especially the symmetry and the antisymmetry.
We note that some previous research did not preserve the symmetry or the antisymmetry and thus obtained inferior performance~\cite{bordes2013translating,lin2015learning,yang2015embedding}.
Similar to the recent advanced models~\cite{trouillon2016complex,kazemi2018simple,sun2018rotate,DBLP:conf/acl/XuL19}, we also pay close attention to the modeling support of both symmetric and antisymmetric relationships.

Third, after an exhaustive review of the literature, we find that one critical difference between various KGE methods lies in the design of scoring functions.
Therefore, we dive deeply into designing powerful scoring functions for a triple .
Specifically, we combine the above two aspects (facilitating feature interactions and preserving various relation properties) and develop four kinds of scoring functions progressively.
Based on these scoring functions, we can specify many existing KGE methods, including DistMult~\cite{yang2015embedding}, HoIE~\cite{nickel2016holographic}, and ComplEx~\cite{trouillon2016complex}, as special cases of SEEK.
Hence, as a general framework, SEEK can help readers to understand better the pros and cons of existing research as well as the relationship between them.
Moreover, extensive experiments demonstrate that SEEK can achieve either state-of-the-art or highly competitive performance on a variety of benchmarks for KGE compared with existing methods.

In summary, this paper makes the following contributions.
\begin{itemize}[label={-},leftmargin=1.5em]
	\item We propose a light-weight framework (SEEK) for KGE that achieves highly competitive expressiveness without the sacrifice in the model complexity.
	\item As a unique framework that focuses on designing scoring functions for KGE, SEEK combines two critical characteristics: facilitating sufficient feature interactions and preserving fundamental relation properties.
	\item As a general framework, SEEK can incorporate many previous methods as special cases, which can help readers to understand and compare existing research.
	\item Extensive experiments demonstrate the effectiveness and efficiency of SEEK. Moreover, sensitivity experiments about the number of segments also verify the robustness of SEEK. 
\end{itemize}


 \section{Related Work}
\label{sec:related}

\begin{table*}[!h]
	\centering
	\TableSize
	\begin{tabular}{ l |c| c | c | c c}
		\toprule
		\multirow{2}{*}{\textbf{Methods}} & \multirow{2}{*}{\textbf{Scoring Function}} &  \multirow{2}{*}{\textbf{Performance}} & \multirow{2}{*}{\textbf{\# Parameters}} &\multicolumn{2}{c}{\textbf{Properties}} \\
		\cmidrule{5-6}
		& & & & \textbf{Sym}  & \textbf{Antisym}\\
		\midrule
		TransE~\cite{bordes2013translating} &  & Low & Small & \xmark & \cmark\\
		DistMult~\cite{yang2015embedding} &  & Low  & Small & \cmark & \xmark\\
ComplEx~\cite{trouillon2016complex} &  & Low   & Small  & \cmark & \cmark\\
		Single DistMult~\cite{kadlec2017knowledge} & & High  & Large & \cmark & \xmark\\
		ConvE~\cite{dettmers2018conve} & & High  & Large & \xmark &  \cmark\\
		\midrule
		SEEK &   & High  & Small & \cmark & \cmark\\
		\bottomrule
	\end{tabular}
	\caption{Comparison between our SEEK framework and some representative knowledge graph embedding methods in the aspects of the scoring function, performance, the number of parameters, and the ability to preserve the symmetry and antisymmetry properties of relations.}	
	\label{tab:compare}
\end{table*}

We can categorize most of the existing work into two categories according to the model complexity and the model expressiveness.

The first category of methods is the simple but lack of expressiveness, which can easily scale to large knowledge graphs. This kind of methods includes TransE~\cite{bordes2013translating} and DistMult~\cite{yang2015embedding}. TransE uses relation  as a translation from a head entity  to a tail entity  for calculating their embedding vectors of ; DistMult utilizes the multi-linear dot product as the scoring function.

The second kind of work introduces more parameters to improve the expressiveness of the simple methods. TransH~\cite{wang2014knowledge}, TransR~\cite{lin2015learning}, TransD~\cite{ji2015knowledge}, and ITransF~\cite{xie2017interpretable} are the extensions of TransE, which introduce other parameters to map the entities and relations to different semantic spaces. The Single DistMult~\cite{kadlec2017knowledge} increases the embedding size of the DistMult to obtain more expressive features. Besides, ProjE~\cite{shi2017proje}, ConvE~\cite{dettmers2018conve} and InteractE~\cite{vashishth2019interacte} leverage neural networks to capture more feature interactions between embeddings and thus improves the expressiveness. However, these neural network-based methods would also lead to more parameters since there are many parameters in the neural network. Although the second kind of methods has a better performance compared with simple methods, they are difficult to apply to real-world KGs due to the high model complexity (a large number of parameters).

Compared with the two types of methods above, our SEEK can achieve high expressiveness without increasing the number of model parameters. Table~\ref{tab:compare} shows the comparison between our framework and some representative KGE methods in different aspects. 

Besides, preserving the symmetry and antisymmetry properties of relations is vital for KGE models. Many recent methods devote to preserving these relation properties to improve the expressiveness of embeddings~\cite{trouillon2016complex,nickel2016holographic,guo2018:RUGE, boyang2018:aer,kazemi2018simple,sun2018rotate,DBLP:conf/acl/XuL19}.  Motivated by these methods, we also pay attention to preserving symmetry and antisymmetry properties of relations when we design our scoring functions.
 \section{SEEK}
\label{sec:complex}



Briefly speaking, we build SEEK by designing scoring functions, which is one of the most critical components of various existing KGE methods, as discussed in the related work.
During the procedure of designing scoring functions, we progressively introduce two characteristics that hugely contribute to the model expressiveness:
1) facilitating sufficient feature interactions;
2) supporting both symmetric and antisymmetric relations.
In this way, SEEK enables the excellent model expressiveness given a light-weight model with the same number of parameters as some simple KGE counterparts, such as TransE~\cite{bordes2013translating} and DistMult~\cite{yang2015embedding}.

\subsection{Scoring Functions}
In this section, we illustrate our four scoring functions progressively.

\subsubsection{: Multi-linear Dot Product}
First, we start with the scoring function  developed by~\citet{yang2015embedding}, which computes a multi-linear dot product of three vectors:

where  are low-dimensional representations of the relation , the head entity , and the tail entity , respectively, and  correspond to the -th dimension of , respectively.

We note that the function  is the building block of much previous research~\cite{trouillon2016complex,kadlec2017knowledge,kazemi2018simple}.
Different from these existing research, we focus on designing more advanced scoring functions with better expressiveness.




\subsubsection{: Multi-linear Dot Product Among Segments}

Next, we introduce fine-grained feature interactions to improve the model expressiveness further.
To be specific, we develop the scoring function  that conducts the multi-linear dot product among different segments of the entity/relation embeddings.
First, we uniformly divide the -dimensional embedding of the head , the relation , and the tail  into  segments, and the dimension of each segment is .
For example, we can write the embedding of relation  as:

where  is the -th segment of the embedding .

Then, we define the scoring function  as follows:

Compared with the scoring function , where the interactions only happen among the same positions of  embeddings, the scoring function  can exploit more feature interactions among different segments of embeddings.

\begin{figure}[!h]
	\centering
	\includegraphics[width=1.0\columnwidth]{figure/f3-Example}
	\caption{Scoring function  with .}
	\label{fig:f3_example}
\end{figure}
\subsubsection{: Modeling both Symmetric and Antisymmetric Relations}
Although the scoring function  can facilitate fine-grained feature interactions, it can only preserve the symmetry property of relations and can not support the modeling of antisymmetric relations.
For example, given a symmetric relation , we have , but for an antisymmetric relation , the value of  is also equal to , which is unreasonable because  is a false triple.

To preserve the antisymmetry property of relations, we divide the segments of relation embedding  into odd and even parts. Then we define a variable  to enable the even parts of segments to capture the symmetry property of relations and the odd parts to capture the antisymmetry property. We define the scoring function after adding  as:

where

In the scoring function ,  indicates the sign of each dot product term . 
Figure~\ref{fig:f3_example} depicts an example of the function  with .
When  is the even part of  (the index  is even),  is positive, and the summation  of  equals to the corresponding one  of .
Therefore, the function  can model symmetric relations via the even segments of .
When  is the odd part of  (the index  is odd),  can be either negative or positive depending on whether .
Then, the summation of odd parts of  is differ from that of .
Accordingly,  can support antisymmetric relations with the odd segments of . 

\begin{figure}[!h]
	\centering
	\includegraphics[width=1.0\columnwidth]{figure/SEG-Example}
	\caption{Scoring function  with .}
	\label{fig:embedding_example}
\end{figure}

The scoring function  can support both symmetric and antisymmetric relations inherently because of the design of segmented embeddings.
Moreover, the optimization of relation embeddings is entirely data-driven, and thus we focus on providing the proper mechanism to capture common relation properties.

\subsubsection{: Reducing Computing Overheads}
However, though capturing various relation properties, the function  suffers from huge computation overheads.
The time complexity of function  is  because there are  dot product terms  in total.
Therefore, the scoring function  needs  times of dot product to compute the score of a triple .
Recall that the dimension of each segment is , so each multi-linear dot product requires  times of multiplication.
As a conclusion, the time complexity of the function  is , which can be calculated by .
To reduce the computation overheads of the function , we introduce another variable  for the index of tail entity .
Accordingly, we define the scoring function  as follows.

where

The scoring function  reduces the number of dot product terms to , so its time complexity is  (calculated by ).
Moreover, the scoring function  can also preserve symmetry property in the even parts of  and preserve antisymmetry property in the odd parts of . 

Figure~\ref{fig:embedding_example} shows the example of the scoring function  with . The dot product terms in Figure~\ref{fig:embedding_example} can be categorized into four groups according to the segment indexes of .  
In the groups of  and , which are the even parts of , the segment 's index  is same as the segment 's index , and  is always positive. Thus, the summation  of the even parts of   is equal to the corresponding one  of . 
In the groups of  and , which are the odd parts of , the segment indexes of  are , where  and  are the indexes of  and , respectively. When , the variable  will change from positive to negative. So the summation of the odd parts of  and  will not be the same. Besides, it is apparent that the number of feature interactions on ,  and  are increasing  times since each segment has  interactions with other segments.

In summary, the scoring function  of our SEEK framework has the following characteristics:
\begin{itemize}[label={-}]

	
	\item \textit{Tunable Computation.} The scoring function exactly involves each segment of , , and   times. Thus the number of feature interactions and the computation cost are fully tunable with a single hyperparameter .
	
	\item \textit{Symmetry and Antisymmetry Preservation.}  The even parts of  can preserve the symmetry property of relations, and the odd parts of  can preserve the antisymmetry property.
	
	\item \textit{Dimension Isolation.}  The dimensions within the same segment are isolated from each other, which will prevent the embeddings from excessive correlations.\end{itemize}

\begin{comment}
\begin{figure*}[!h]
\centering
\includegraphics[width=2.0\columnwidth]{figure/SEG-Example-8-2}
\caption{Scoring Function with }
\label{fig:embedding_example_8}
\end{figure*}
\end{comment}

\subsection{Discussions}
\paragraph{Complexity analysis}
As described before, the number of dot product terms in scoring function  is , and each term requires  times of multiplication. So the time complexity of our SEEK framework is  (calculated by ), where  is a small constant such as 4 or 8. For the space complexity, the dimension of entity and relation embeddings is , and there are no other parameters in our SEEK framework. Thus, the space complexity of SEEK is . The low time and space complexity of our framework demonstrate that our SEEK framework has high scalability, which is vital for large-scale real-world knowledge graphs.

\paragraph{Connection with existing methods}
Our SEEK framework is a generalized framework of some existing methods, such as DistMult~\cite{yang2015embedding}, ComplEx~\cite{trouillon2016complex}, and HolE~\cite{nickel2016holographic}. In the following, we will prove that these methods are special cases of our framework when we set  and , respectively.
\begin{proposition}
	SEEK () is equivalent to DistMult.
\end{proposition}
\begin{proof}
	The proof is trivial. Given , we have  and  in scoring function  and , , and . Thus the function  can be written as , which is the same scoring function of DistMult.
\end{proof}
\begin{proposition}
	SEEK () is equivalent to the ComplEx and HolE.
\end{proposition}
\begin{proof}
	Given , function  can be written as:
	
	then we expand the right part of the equation:
	
	If we consider  as the real part of , and  as the imaginary part,
	then  is exactly the scoring function of ComplEx framework. Since~\cite{hayashi2017equivalence} has already discussed the equivalence of ComplEx and HolE, the SEEK () is also equivalent to the HolE framework.
\end{proof}

\subsection{Training}
SEEK takes the negative log-likelihood loss function with  regularization as its objective function to optimize the parameters of entities and relations:

where  is a sigmoid function defined as , and  represents the parameters in the embeddings of entities and relations in knowledge graphs;  is the triple set containing the true triples in the knowledge graphs and the false triples generated by negative sampling. In the negative sampling, we generate a false triple  or  by replacing the head or tail entity of a true triple with a random entity.  is the label of , which is  for the true triples and  for the false triples.  is the  regularization parameter.


The gradients of Equation~\ref{eq:objective-function} are then given by:

where  represents the objective function of SEEK, and  is the parameters in the segments. Specifically, the partial derivatives of function  for the -th segment of  and the -th segment of  are:


where  is the entry-wise product of two vectors, e.g.  
results in the -th dimension of  is .
The derivative of scoring function  for  is different from those of the above two:

where  has value  if  holds, otherwise it is .
 \section{Experimental Evaluation}

\label{sec:exp}
In this section, we present thorough empirical studies to evaluate and analyze our proposed SEEK framework. 
We first introduce the experimental setting. Then we evaluate our SEEK framework on the task of link prediction. Then, we study the influence of the number of segments  to the SEEK framework, and present the case studies to demonstrate why our SEEK framework has high effectiveness.

\subsection{Experimental Setting}
\paragraph*{Datasets}
In our experiments, we firstly use a \textit{de facto} benchmark dataset: FB15K. FB15K is a subset of the Freebase dataset~\cite{bollacker2008freebase}, and we used the same training, validation and test set provided by \cite{bordes2013translating}. We also use another two new datasets proposed in recent years: DB100K~\cite{boyang2018:aer} and YAGO37~\cite{guo2018:RUGE}. DB100K was built from the mapping-based objects of core DBpedia~\cite{bizer2009dbpedia}; YAGO37 was extracted from the core facts of YAGO3~\cite{mahdisoltani2013yago3}. Table~\ref{tab:datasets} lists the statistics of the three datasets.

\begin{table}[!h]
	\centering
	\TableSize
	\begin{tabular}{ p{3.4em} | p{2.9em}  p{1.9em}  p{3em}  p{2.5em}  p{2.5em} }
		\toprule
		\textbf{Dataset} & \textbf{\#Ent} & \textbf{\#Rel} & \textbf{\#Train} & \textbf{\#Valid} & \textbf{\#Test} \\
		\midrule
		FB15K &  &  &  &  &   \\
		DB100K &  &   &   &   &   \\
		YAGO37 &  &  &  &  &  \\
		\bottomrule
	\end{tabular}
	\caption{Statistics of datasets.}
	\label{tab:datasets}
\end{table}

\paragraph*{Compared Methods}
There are many knowledge graph embedding methods developed in recent years. We categorize the compared methods as the following groups:
\begin{itemize}[label={-},leftmargin=1.5em]
	\item Some simple knowledge graph embedding methods that have low time and space complexity, like TransE~\cite{bordes2013translating}, DistMult~\cite{yang2015embedding}, HolE~\cite{nickel2016holographic}, ComplEx~\cite{trouillon2016complex}, and Analogy~\cite{liu2017analogical}. Specifically, TransE is a translation based method, and others are the multi-linear dot product-based framework.
	
	\item Some methods that achieve state-of-the-art performance on DB100K and YAGO37, which include RUGE~\cite{guo2018:RUGE} and ComplEx-NNE+AER~\cite{boyang2018:aer}.
	
	\item Some latest methods that achieve current state-of-the-art performance on FB15K, including Single DistMult~\cite{kadlec2017knowledge}, ConvE~\cite{dettmers2018conve}, SimplE~\cite{kazemi2018simple}, RotatE~\cite{sun2018rotate}, and DihEdral~\cite{DBLP:conf/acl/XuL19}. 
	
	\item We evaluate the scoring function  to apply an ablation study for our approach. Then we can observe the respective effect of facilitating sufficient feature interactions and preserving the relation properties. Since the scoring function  can only preserve the symmetric property, we refer to it as Sym-SEEK.
\end{itemize}
Since our framework does not use additional information like text~\cite{toutanova2015observed}, relational path~\cite{ebisu2019graph}, or external memory~\cite{shen2017modeling}, we do not compare the methods with additional information. Moreover, we only compare our method with single models, and the Ensemble DistMult~\cite{kadlec2017knowledge} is a simple ensemble of multiple different methods, so we do not compare with it.

\begin{table*}[!h]
	\centering
	\begin{threeparttable}
		
		\TableSize
		\begin{tabular}{ l |c  c  c  c | c  c  c  c}
			\toprule
			\multirow{3}{*}{\textbf{Methods}} & \multicolumn{4}{c|}{\textbf{DB100K}} &  \multicolumn{4}{c}{\textbf{YAGO37}} \\
			\cmidrule{2-9}
			& \multirow{2}{*}{\textbf{MRR}}  &  \multicolumn{3}{c|}{\textbf{Hits@N}} & \multirow{2}{*}{\textbf{MRR}}  &  \multicolumn{3}{c}{\textbf{Hits@N}} \\ 
			\cmidrule{3-5}	\cmidrule{7-9}
			&  & \textbf{1} & \textbf{3} & \textbf{10} & & \textbf{1} & \textbf{3} & \textbf{10}  \\ 
			\midrule
			TransE~\cite{bordes2013translating}&  &  &  &  &  &  &  &    \\
			DistMult~\cite{yang2015embedding}&  &  &  &  &  &  &  &  \\
			HolE~\cite{nickel2016holographic} &  &  &  &  &   &  &  &   \\
			ComplEx~\cite{trouillon2016complex}&  &  &  &   &  &  &  &  \\		
			Analogy~\cite{liu2017analogical} &  &  &  &  &  &  &  &  \\
			\midrule
			RUGE~\cite{guo2018:RUGE}&  &  &  &  & &  &  &  \\
			ComplEx-NNE+AER~\cite{boyang2018:aer} &  &  &  &  &  &  &  &  \\
			\midrule
			\textbf{Sym-SEEK}\tnote{*} &  &   &  &  & &   &  &  \\
			\textbf{SEEK}\tnote{*} &  &   &  &  & &   &  &  \\
			\bottomrule
		\end{tabular}
		\begin{tablenotes}[para,flushleft]
			\item[*] Statistically significant improvements by independent -test with .
		\end{tablenotes}
		
	\end{threeparttable}
	\caption{Results of link prediction on DB100K and YAGO37.}
	\label{tab:kbc-performance-yago37-db100k}
\end{table*}

\paragraph*{Experimental Details}

We use the asynchronous stochastic gradient descent (SGD) with the learning rate adapted by AdaGrad~\cite{duchi2011adaptive} to optimize our framework. The loss function of our SEEK framework is given by Equation \ref{eq:objective-function}. We conducted a grid search to find hypeparameters which maximize the results on validation set, by tuning number of segments , the dimension of embeddings ,  regularization parameter  and the number of negative samples per true triple . The optimal combinations of hyperparameters are , , ,  on FB15K; , , ,  on DB100K; and , , ,  on YAGO37. We set the initial learning rate  to 0.1 and the number of epochs to 100 for all datasets. 

\subsection{Link Prediction}

We study the performance of our method on the task of link prediction, which is a prevalent task to evaluate the performance of knowledge graph embeddings. We used the same data preparation process as \cite{bordes2013translating}. Specifically, we replace the head/tail entity of a true triple in the test set with other entities in the dataset and name these derived triples as \textit{corrupted triples}. The goal of the link prediction task is to score the original true triples higher than the corrupted ones. We rank the triples by the results of the scoring function.

\begin{table}[!h]
	\centering
	\begin{threeparttable}
		
		\TableSize
		\begin{tabular}{ l | c  c  c  c}
			\toprule
			\multirow{3}{*}{\textbf{Methods }}  &  \multicolumn{4}{c}{\textbf{FB15K}} \\
			\cmidrule{2-5}	
			& \multirow{2}{*}{\textbf{MRR}}  &  \multicolumn{3}{c}{\textbf{Hits@N}} \\ 
			\cmidrule{3-5}	
			& & \textbf{1} & \textbf{3} & \textbf{10}  \\ 
			\midrule
			TransE&  &  &  &  \\
			DistMult&  &  &  &  \\
			HolE &  &  &  &  \\
			ComplEx&  &  &  &  \\		
			Analogy&  &  &  &  \\
			\midrule
			RUGE&  &  &  &  \\
			ComplEx-NNE+AER &  &  &  & \\
			\midrule
			Single DistMult &  & && \\	
			ConvE&  &  &  &  \\
SimplE&  &  &  &   \\
RotatE&  &  &  &  \\ 
			DihEdral &  &  &  &  \\
			\midrule
			\textbf{Sym-SEEK}\tnote{*} &  &   &  &  \\
			\textbf{SEEK}\tnote{*} &  &   &  &  \\
			\bottomrule
		\end{tabular}
		
		\begin{tablenotes}[para,flushleft]
			\item[*] Statistically significant improvements by independent -test with .
		\end{tablenotes}
		
	\end{threeparttable}
	\caption{Results of link prediction on FB15K.}	
	\label{tab:kbc-performance-fb15k}
\end{table}

We use the MRR and Hit@N metrics to evaluate the ranking results:
\begin{enumerate*}[label={\alph*)}]
	\item MRR: the mean reciprocal rank of original triples;
	\item Hits@N: the percentage rate of original triples ranked at the top  in prediction.
\end{enumerate*}
For both metrics, we remove some of the corrupted triples that exist in datasets from the ranking results, which is also called \emph{filtered} setting in~\cite{bordes2013translating}. We use Hits@1, Hits@3, and Hits@10 for the metrics of Hits@N.

Table~\ref{tab:kbc-performance-yago37-db100k} summarizes the results of link prediction on DB100K and YAGO37, and Table~\ref{tab:kbc-performance-fb15k} shows the results on FB15K. Note, the results of compared methods on DB100K and YAGO37 are taken from~\cite{boyang2018:aer,guo2018:RUGE}; the results on FB15K are taken from~\cite{kadlec2017knowledge,boyang2018:aer,kazemi2018simple,sun2018rotate,DBLP:conf/acl/XuL19}.


\begin{figure}[!h]
	\centering
	\includegraphics[width=0.96\columnwidth]{figure/Effect_of_Segment_Count}
	\caption{The influence of the number of segments  to the MRR and the running time of link prediction on FB15K.}
	\label{fig:Seg_count}
\end{figure}

On the DB100K, SEEK outperforms the compared methods in all metrics, and the Sym-SEEK also can achieve a good performance. On the YAGO37, the SEEK and Sym-SEEK have a similar result and outperform other previous methods. The results on YAGO37 show that exploiting more feature interactions can significantly improve the performance of the embeddings on YAGO37 while preserving the semantic properties have a slight improvement. On FB15K, SEEK achieves the best performance on MRR, Hit@1 and Hit@3. 
Although SEEK is worse than the Single DistMult on the metrics of Hit@10, the Single DistMult is just a higher dimensional version of DistMult. The Single DistMult uses 512-dimensional embeddings, which is larger than the 400-dimensional embeddings of the SEEK framework. 
On the whole, our method's improvements on these datasets demonstrate that our method has a higher expressiveness.



\subsection{Influence of the Number of Segments }
In the SEEK framework, a larger number of segments  implies more feature interactions and higher computational cost. To empirically study the influence of the number of segments  to the performance and computation time of SEEK, we let  vary in  and fix all the other hyperparameters, then we observe the MRR and time costs for the link prediction task on the test set of FB15K. 

\begin{figure}[!h]
	\centering
	\includegraphics[width=1.0\columnwidth]{figure/casestudy_triples}
	\caption{The correct probabilities of four triples in DB100K and their reverse triples. The probabilities ,  and  are corresponding to the scoring functions ,  and , respectively.}
	\label{fig:case_study}
\end{figure}
Figure~\ref{fig:Seg_count} shows the MRR and time costs of different segment counts  on FB15K. As we can see, changing  affects the performance of knowledge graph embeddings significantly. When  varies from  to , the performance is increased steadily. However, when  becomes even larger, no consistent and dramatic improvements observed on the FB15K dataset. This phenomenon suggests that excessive feature interactions cannot further improve performance. Therefore,  is a sensitive hyperparameter that needs to be tuned for the best performance given a dataset. Figure~\ref{fig:Seg_count} also illustrates that the running time of SEEK is linear in , and it verifies that the time complexity of SEEK is .


\subsection{Case Studies}
We employ case studies to explain why our framework has a high expressiveness.
Specifically, we utilize the scoring functions ,  and  to train the embeddings of DB100K, respectively. Then we use the corresponding scoring functions to score the triples in the test set and their reverse triples, and we feed the scores to the sigmoid function to get the correct probabilities ,  and  of each triple.
Figure~\ref{fig:case_study} shows the correct probabilities of some triples. In these triples, two triples have symmetric relations, and the other two have antisymmetric relations. On the triples with symmetric relations, the original triples in the test set and their reverse triples are true triples, and the scoring functions , ,  can result in high probabilities on original and reverse triples. On the triples with antisymmetric relations, the reverse triples are false. Since the values of  or  are equal to  or , the scoring functions  and  result in high probabilities on the reverse triples. But the scoring function , which can model both symmetric and antisymmetric relations, results in low probabilities on the reverse triples. Meanwhile, we can also find that function  have higher probabilities than function  on the true triples. This phenomenon further explains that facilitating sufficient feature interactions can improve the expressiveness of embeddings.

 \section{Conclusion and Future Work}
\label{sec:conclusion}
In this paper, we propose a lightweight KGE framework (SEEK) that can improve the expressiveness of embeddings without increasing the model complexity. To this end, our framework focuses on designing scoring functions and highlights two critical characteristics: 1) facilitating sufficient feature interactions and 2) preserving various relation properties.
Besides, as a general framework, SEEK can incorporate many existing models, such as DistMult, ComplEx, and HolE, as special cases.
Our extensive experiments on widely used public benchmarks demonstrate the efficiency, the effectiveness, and the robustness of SEEK.
In the future, we plan to extend the key insights of segmenting features and facilitating interactions to other representation learning problems. \section*{Acknowledgments}

This work is supported by the National Natural Science Foundation of China (U1711262, U1611264, U1711261, U1811261, U1811264, U1911203), National Key R\&D Program of China (2018YFB1004404), Guangdong Basic and Applied Basic Research Foundation (2019B1515130001),  Key R\&D Program of Guangdong Province (2018B010107005). 

\balance
\bibliography{acl2020}
\bibliographystyle{acl_natbib}

\end{document}
