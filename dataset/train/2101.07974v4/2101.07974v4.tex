
\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage[pagenumbers]{cvpr} 

\usepackage{framed,multirow}

\usepackage{amssymb}
\usepackage{latexsym}
\usepackage{subcaption}
\usepackage{enumitem}
\usepackage{float}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{pifont}
\usepackage{xcolor}
\usepackage{bbm}
\usepackage{xpatch}
\usepackage[percent]{overpic}
\usepackage{soul}
\usepackage{nicefrac}
\usepackage{bbold}
\usepackage{mathtools}
\usepackage{amssymb,amsmath,amsthm,enumitem}
\xapptocmd\normalsize{\abovedisplayskip=12pt plus 3pt minus 9pt
 \abovedisplayshortskip=0pt plus 3pt
 \belowdisplayskip=12pt plus 3pt minus 9pt
 \belowdisplayshortskip=7pt plus 3pt minus 4pt
}{}{}

\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}

\usepackage[capitalize]{cleveref}
\usepackage[accsupp]{axessibility}

\newcommand{\grayedout}[1]{{\color{gray}  #1}}
\definecolor{britishracinggreen}{rgb}{0.0, 0.26, 0.15}
\definecolor{cadmiumgreen}{rgb}{0.0, 0.42, 0.24}
\definecolor{darkspringgreen}{rgb}{0.09, 0.45, 0.27}
\definecolor{ferngreen}{rgb}{0.31, 0.47, 0.26}
\definecolor{forestgreen}{rgb}{0.0, 0.27, 0.13}
\definecolor{htmlcssgreen}{rgb}{0.0, 0.5, 0.0}
\newcommand{\increase}[1]{\footnotesize \textcolor{htmlcssgreen}{#1}\normalsize}
\newcommand{\bestresult}[1]{\textbf{#1}}
\newcommand{\secondbest}[1]{\underline{#1}}
\newcommand{\supdagger}{\textsuperscript{\textdagger}}
\newcommand{\supstar}{\textsuperscript{\textasteriskcentered}}

\newcommand{\xmark}{\ding{55}}
\newcommand{\cmark}{\ding{51}}



\definecolor{newcolor}{rgb}{.8,.349,.1}
\definecolor{c0}{rgb}{0.12156862745098039,0.4666666666666667,0.7058823529411765}
\definecolor{c1}{rgb}{1.0,0.4980392156862745,0.054901960784313725}
\definecolor{c2}{rgb}{0.17254901960784313,0.6274509803921569,0.17254901960784313}
\definecolor{c3}{rgb}{0.8392156862745098,0.15294117647058825,0.1568627450980392}
\definecolor{c4}{rgb}{0.5803921568627451,0.403921568627451,0.7411764705882353}
\definecolor{c5}{rgb}{0.5490196078431373,0.33725490196078434,0.29411764705882354}
\definecolor{c6}{rgb}{0.8901960784313725,0.4666666666666667,0.7607843137254902}
\definecolor{c7}{rgb}{0.4980392156862745,0.4980392156862745,0.4980392156862745}
\definecolor{c8}{rgb}{0.7372549019607844,0.7411764705882353,0.13333333333333333}
\definecolor{c9}{rgb}{0.09019607843137255,0.7450980392156863,0.8117647058823529}
\definecolor{FieldHockeyPenalty}{rgb}{0.12156862745098039,0.4666666666666667,0.7058823529411765}
\definecolor{GolfSwing}{rgb}{1.0,0.4980392156862745,0.054901960784313725}
\definecolor{SoccerJuggling}{rgb}{0.17254901960784313,0.6274509803921569,0.17254901960784313}
\definecolor{SoccerPenalty}{rgb}{0.8392156862745098,0.15294117647058825,0.1568627450980392}
\definecolor{PlayingGuitar}{rgb}{0.5803921568627451,0.403921568627451,0.7411764705882353}
\definecolor{PlayingPiano}{rgb}{0.5490196078431373,0.33725490196078434,0.29411764705882354}
\definecolor{PlayingSitar}{rgb}{0.8901960784313725,0.4666666666666667,0.7607843137254902}
\definecolor{ApplyEyeMakeup}{rgb}{0.4980392156862745,0.4980392156862745,0.4980392156862745}
\definecolor{ApplyLipstick}{rgb}{0.7372549019607844,0.7411764705882353,0.13333333333333333}
\definecolor{BlowDryHair}{rgb}{0.09019607843137255,0.7450980392156863,0.8117647058823529}






\begin{document}

\title{\textbf{TCLR}: Temporal Contrastive Learning for Video Representation}

\author{Ishan Dave, Rohit Gupta, Mamshad Nayeem Rizve, Mubarak Shah\\
Center for Research in Computer Vision, University of Central Florida, Orlando, USA\\
\tt\small \{ishandave, rohitg, nayeemrizve\}@knights.ucf.edu, shah@crcv.ucf.edu
}

\maketitle

\begin{abstract}
Contrastive learning has nearly closed the gap between supervised and self-supervised learning of image representations, and has also been explored for videos. However, prior work on contrastive learning for video data has not explored the effect of explicitly encouraging the features to be distinct across the temporal dimension. We develop a new temporal contrastive learning framework consisting of two novel losses to improve upon existing contrastive self-supervised video representation learning methods. The local-local temporal contrastive loss adds the task of discriminating between non-overlapping clips from the same video, whereas the global-local temporal contrastive aims to discriminate between timesteps of the feature map of an input clip in order to increase the temporal diversity of the learned features. Our proposed temporal contrastive learning framework achieves significant improvement over the state-of-the-art results in various downstream video understanding tasks such as action recognition, limited-label action classification, and nearest-neighbor video retrieval on multiple video datasets and backbones. We also demonstrate significant improvement in fine-grained action classification for visually similar classes. With the commonly used 3D ResNet-18 architecture with UCF101 pretraining, we achieve 82.4\% (+5.1\% increase over the previous best) top-1 accuracy on UCF101 and 52.9\% (+5.4\% increase) on HMDB51 action classification, and 56.2\% (+11.7\% increase) Top-1 Recall on UCF101 nearest neighbor video retrieval. Code released at \url{https://github.com/DAVEISHAN/TCLR}.
\end{abstract}


\section{Introduction}
\vspace{-1mm}



Large-scale labeled datasets such as Kinetics~\cite{kinetics}, LSHVU~\cite{diba2020large} etc have been crucial for recent advances in video understanding tasks. Since training a video encoder using existing supervised learning approaches is label-inefficient~\cite{kenshohara_mega_scale}, annotated video data is required at a large scale. This costs enormous human effort and time, much more so than annotating images. At the same time, a tremendous amount of unlabeled video data is easily available on the internet. Research in self-supervised video representation learning can unlock the corpus of readily available unlabeled video data and unshackle progress in video understanding.

Recently, Contrastive Self-supervised Learning (CSL) based methods~\cite{simclr, moco, swav} have demonstrated the ability to learn powerful {\em image representations} in a self-supervised manner, and have narrowed down the performance gap between {\em unsupervised} and {\em supervised} representation learning on various image understanding downstream tasks.




\begin{figure}
    \centering
    \subfloat[Running and Jumping: distinct stages of \texttt{LongJump} action]{{\includegraphics[clip, trim=0cm 0.6cm 0cm 0cm,width=0.99\linewidth]{./long_diverse_action_1.pdf}{\label{fig:longjump}} }} \\
    \subfloat[NN-Retrieval]{{\includegraphics[width=0.49\linewidth]{recall_fig1_final.pdf}{\label{fig:nnr}} }}\hspace*{-0.4em}
    \subfloat[Linear Classification]{{\includegraphics[width=0.49\linewidth]{./acc_fig1_final.pdf}{\label{fig:lineval}} }}
    \caption{Videos from standard action recognition datasets often have distinct temporal stages. For example, in figure \textbf{(a)} we can see the two distinct stages (Running and Jumping) of the \texttt{LongJump} action. Typically predictions across multiple short clips are aggregated, as a single short clip may not capture both stages of the action. We show the comparison of vanilla instance discrimination based contrastive (IC) self-supervision and our proposed TCLR method on \textbf{(b)} Nearest neighbour retrieval and \textbf{(c)} Linear classification tasks. We find that IC trained models do not benefit much from using multiple clips during evaluation. This is a result of IC imposing within instance temporal invariance. This motivates our proposed TCLR pre-training, which explicitly encourages learning distinct features across time.}
    \vspace{-4mm}
    \label{fig:intro}
    
    
\end{figure}




\begin{figure*}
\begin{center}
  \includegraphics[width=\textwidth]{./Fig1_TCLR.pdf}
\end{center}
\vspace{-5mm}
\caption{The proposed temporal contrastive learning framework (TCLR) for learning temporally distinct video representations consists of three different losses.}

  \vspace{-5mm}

  
\label{fig:main}
\end{figure*}




A simple yet effective extension of CSL to the video domain can be obtained by using the InfoNCE instance discrimination objective, where the model learns to distinguish clips of a given video from the clips of other videos in the dataset (see Figure~\ref{fig:main}a). Unlike images, videos have both time-invariant and the temporally varying properties. For example, in a \texttt{LongJump} video from UCF101 (See Figure~\ref{fig:intro}), running and jumping represent two very different stages of the action. Usually, video understanding models utilize temporally varying features by aggregating along the temporal dimension to obtain a video level prediction. While the  significant success can be achieved on many video understanding tasks by only modelling the temporally invariant properties, it maybe possible that the temporally varying properties can also play an important role in further improvements on these tasks. Whether video representations should be invariant or distinct along the temporal dimension is an open question in the literature.  Instance contrastive pre-training, however, encourages the model to learn similar features to represent temporally distant clips from the video, i.e. it enforces temporal invariance on the features. While instance level contrastive learning lies on one end of the spectrum, some recent works have tried to relax the invariance constraint through various means, such as, using a weighted temporal sampling strategy to avoid enforcing invariance between temporally distant clips~\cite{cvrl}, cross-modal mining of positive samples from across video instances~\cite{cotraining} or adding additional pretext tasks that require learning temporal features~\cite{pace_pred, iclr21submitted_st_invariant, seco, taco}. 














We take a different approach by explicitly encouraging the learning of temporally distinct video representations. The challenge with video classification is modelling variable length videos with a fixed number of parameters. 3D CNNs tackle this challenge by temporal aggregation of features across two levels: averaging across distinct fixed length temporal segments of a video (clips) and also temporal pooling across the feature map of each clip. Based on this observation, we propose two different temporal contrastive losses in order to learn temporally distinct features across the video: one which acts across clips of the same video, and another which acts across the timesteps of the feature map of the same clip. Combined with the vanilla instance contrastive loss, these novel losses result in an increase in the temporal diversity of the learned features, and better accuracy on downstream tasks. 


Our first proposed loss is the {\em local-local temporal contrastive loss} (Figure~\ref{fig:main}b), which ensures that temporally non-overlapping clips from the same video are mapped to distinct representations. This loss treats randomly augmented versions of the same clip as positive pairs to be brought together, and other non-overlapping clips from the same video as negative matches to be pushed away. While the local-local loss ensures that distinct clips have distinct representations, in order to encourage temporal variation {\em within each clip}, we introduce a second temporal contrastive loss, the {\em global-local temporal contrastive loss} (Figure~\ref{fig:main}c). This loss constrains the \textit{timesteps} of the feature map of a long \textit{``global"} video clip to match the representations of the temporally aligned shorter \textit{``local"} video clips. 




Our complete framework is called \textit{Temporal Contrastive Learning of video Representations} (henceforth referred to as \textit{TCLR}). TCLR retains the ability of representations to successfully discriminate between video instances due to its instance contrastive loss. In addition, TCLR attempts to capture the {\em within-instance} temporal variation. Through extensive experiments on various downstream video understanding tasks, we demonstrate that both of our proposed Temporal Contrastive losses contribute to the learning of powerful video representations, and provide significant improvements.























\noindent{The original contributions of this work can be summarized as below:}
\setlist{nolistsep}
\begin{itemize}
    \item TCLR is the first contrastive learning framework to explicitly enforce \textit{within} instance temporal feature variation for video understanding tasks. 
    \item Novel {\em local-local} and {\em global-local temporal contrastive losses}, which when combined with the standard instance contrastive loss significantly outperform the state-of-the-art on various downstream video understanding tasks like action recognition, nearest neighbor video retrieval and action classification with limited labeled data, while using 3 different 3D CNN architectures and 2 datasets (UCF101 \& HMDB51).  
    \item We propose the use of the challenging Diving48 fine-grained action classification task for evaluating the quality of learned video representations. 
    
\end{itemize}

\section{Related Work}
 Recent approaches for self-supervised video representation learning can be categorized into two major groups based on the self-supervised learning objective: (1) Pretext task based methods, and (2) Contrastive Learning based methods.



\noindent{\bf Pretext task based approaches:}  Various pretext tasks have been devised for self-supervised video representation learning based on learning the correct temporal order of the data: verifying correct frame order~\cite{misra2016shuffle}, identifying the correctly ordered tuple from a set of shuffled orderings~\cite{fernando2017self, suzuki2018learning}, sorting frame order~\cite{lee2017unsupervised}, and predicting clip order~\cite{vcop}. Some methods extend existing pretext tasks from the image domain to video domain, for example, solving spatio-temporal jigsaw puzzles~\cite{videojigsaw, aaai19, csj} and identifying the rotation of transformed video clips~\cite{3drotnet}. Many recent works rely on predicting video properties like playback rate of the video~\cite{cho2020self,prp,pace_pred,iclr21submitted_st_invariant}, temporal transformation that has been applied from a given set~\cite{simon,jenni2021time}, speediness of moving objects~\cite{speedNet}, and motion and appearance statistics of the video~\cite{statistics_cvpr19, statistics2}. 




\noindent {\bf Contrastive Self-supervised Learning (CSL) based approaches:}  Following the success of contrastive learning approaches of self-supervised image representation learning such as SimCLR~\cite{simclr} and MoCo~\cite{moco}, there have been many extensions of contrastive learning to the video domain. For instance, various video CSL methods~\cite{pace_pred,videomoco, taco, cvrl, iic, seco,idt, iclr21submitted_st_invariant,vtc} leverage \textit{Instance level Discrimination} objectives, and build their method upon them, where clips from the same video are treated as positives and clips from the different videos as negatives. CVRL~\cite{cvrl} studies the importance of temporal augmentation and develops a temporal sampler to avoid enforcing excessive temporal invariance in learning video representation. VideoMoCo~\cite{videomoco} improves image-based MoCo framework for video representation by encouraging temporal robustness of the encoder and modeling temporal decay of the keys. VTHCL~\cite{vtc} employs SlowFast architecture~\cite{slowfast} and uses contrastive loss with the slow and fast pathway representations as the positive pair. VIE~\cite{vie} is proposed as a deep neural embedding-based method to learn video representation in an unsupervised manner, by combining both static image representation from 2D CNN and dynamic motion representation from 3D CNN. Generative contrastive learning-based approaches such as predicting the the dense representation of the next video block~\cite{dpc, memdpc}, or Contrastive Predictive Coding (CPC)~\cite{cpc} for videos~\cite{wacv20} have also been studied in the literature. 

AMDIM~\cite{amdim} is another CSL approach for image representation learning, where a local view (spatial slice of the feature map taken from an intermediate layer) and a global view (full feature map) of differently augmented versions of the same image are considered as a positive pair, and global views of other images form negative pair of the contrastive loss. The method is adapted for the video domain~\cite{vdim,dvim} by generating local views from the spatio-temporal features. Unlike this class of methods, which try to maximize agreement across features from different levels of the encoder, our Global-Local loss tries to learn distinct features across temporal slices of the feature map instead. 

Some recent works combine pretext tasks along with contrastive learning in a multi-task setting to learn temporally varying features in the video representation. For example, using video clips with different playback rates as positive pairs for contrastive loss along with predicting the playback rate~\cite{pace_pred}, or temporal transforms~\cite{taco}. Other works propose frame-based contrastive learning, along with existing pretext tasks of frame rotation prediction~\cite{tce} and frame-tuple order verification~\cite{seco}. Unlike these works, TCLR takes a different approach by adding explicit temporal contrastive losses that encourage temporal diversity in the learned features, instead of utilizing a pretext task for this purpose. 

Some works which try to capture intra-video variance using optical flow, but are nevertheless interesting to compare with. IIC~\cite{iic} uses intra-instance negatives, but it relies on frame repeating and shuffling to generate these ``hard" negatives, and does not focus on learning distinct features across the temporal axis. DSM~\cite{scene-motion} tries to decouple scene and motion features by an intra-instance triplet loss, which uses negatives generated by optical flow scaling and spatial warping. Some recent works use extra supervisory signals in addition to the RGB video data to learn video representation in a self-supervised manner. However, these methods either require additional cross-modal data (e.g. text narration~\cite{miech2020end}, audio~\cite{afouras2020self}) or expensive and time-consuming computation of hand-crafted visual priors (e.g. optical flow~\cite{iic,cbt,aot,local_motion_cues,cotraining} or dense trajectories~\cite{idt}). In this work we focus only on learning from RGB data without using any auxiliary data from any extra modality or additionally computed visual priors. 

\section{Method}
\vspace{-2mm}

The key idea in our proposed framework is to learn two levels of contrastive discrimination: instance discrimination using the {\em instance contrastive loss} and within-instance temporal level discrimination using our novel temporal contrastive losses. The two different temporal contrastive losses which are applied within the same video instance: {\em Local-Local Loss} and {\em Global-Local loss}. Each of these losses is explained in the following sections.

\subsection{Instance Contrastive Loss}

We leverage the idea of \textit{instance discrimination} using InfoNCE~\cite{nce} based contrastive loss for learning video representations. In the video domain, in addition to leveraging image-based spatial augmentations, temporal augmentations can also be applied to generate different transformed versions of a particular instance. For a video instance, we extract various  clips (starting from different timestamps and/or having different frame sampling rates). We consider a randomly sampled mini-batch of size  from different video instances, and from each instance we extract a pair of clips from random timesteps resulting in a total of 2N clips. The extracted clips are augmented using standard stochastic appearance and geometric transformations.\footnote{More details about the augmentations are available in Section \ref{para:dataset} and Section C of the supplementary material.} Each of the transformed clips is then passed through a 3D-CNN based video encoder which is followed by a non-linear projection head (multi-layer perceptron) to project the encoded features on the representation space. Hence, for each video-instance  we get two clip representations . The instance contrastive loss is defined as follows:




\noindent where,  is used to compute the similarity between  and  vectors with an adjustable parameter temperature, .  is an indicator function which equals 1 iff .
\subsection{Temporal Contrastive Losses}





For self-supervised training using the instance contrastive loss of Equation~\ref{eq:IC}, the model is presented with multiple clips cropped from random spatio-temporal locations within a single video as positive matches. This encourages the model to become invariant to the inherent variation present within an instance. In order to enable contrastive learning to represent within instance temporal variation, we introduce two novel temporal contrastive losses: {\em local-local temporal contrastive loss} and {\em global-local temporal contrastive loss}. 








\subsubsection{Local-Local Temporal Contrastive Loss}
\label{sec:ccl}


\begin{figure}[t]
\centering
    \includegraphics[width=0.8\linewidth]{./ll_cviu_re1-cropped.pdf}
    \vspace{-2mm}
  \caption{\textbf{Local-Local Temporal Contrastive Loss} is applied to representations of non-overlapping clips extracted from same video instance . For the clip starting at timestep , two randomly transformed versions are generated and their representations  and  serve as the positive pair for the loss, whereas the other non-overlapping clips along with the anchor, , form the negative pairs.  serves as anchor, further details in Section~\ref{sec:ccl}. , , ...,  are random set of augmentation sampled from universal set .}
\label{fig:tcw}
\end{figure}

For this loss, we treat non-overlapping clips sampled from different temporal segments of the same video instance as negative pairs, and randomly transformed versions of the same clip as a positive pair.   

The local-local loss is defined by Equation~\ref{eq:ccont} and illustrated in Figure~\ref{fig:tcw}. A given video instance  is divided into  non-overlapping clips. For the anchor clip starting at timestep , its representation , and the representation of its transformed version form the positive pair (, for this loss; whereas the other  clips from the same video instance (and their transformed versions) form the negative pairs. Hence, for every positive pair, the local-local contrastive loss has  negative pairs as defined in the following loss: 




The key difference between the Instance contrastive loss (Equation~\ref{eq:IC}) and the proposed Local-Local Temporal contrastive loss (Equation~\ref{eq:ccont}) is that for the local-local loss the negatives come from the same video instance but from a different temporal segment (clips), whereas in Equation~\ref{eq:IC}, the negative pairs come from different video instances.

\subsubsection{Global-Local Temporal Contrastive Loss}
\label{sec:pcl}


\begin{figure}[h]
\centering
  \includegraphics[width=0.95\linewidth]{./gl_cviu_re1-cropped.pdf}
 \vspace{-3mm}
 \caption{\textbf{Global-Local Temporal Contrastive Loss} A global clip (Clip E) is extracted from a video instance and divided into 4 equal length local clips (Clips A through D). The global clip is temporally downsampled to have the same number of frames as each local clip. The local representations  through  from the global clip are obtained from the penultimate layer of the 3D-CNN (prior to temporal pooling). Global representations of the local clips,  through  are obtained from the CNN (after temporal pooling layer). This loss aims to maximize the similarity between the local representation of the global clip and the global representations of the corresponding local clip. Further details in Section~\ref{sec:pcl}.}
\label{fig:tcp}
\end{figure}



The feature map in the higher layers of 3D CNNs are capable of representing temporal variation in the input clip, which is temporally pooled before being used for classification, or projected in the representation space in the case of contrastive learning. The objective of our proposed global-local temporal contrastive loss is to explicitly encourage the model to learn feature maps that represent the temporal locality of the input clip across temporal dimension of the feature map. 


 







This loss is illustrated in Figure~\ref{fig:tcp}. The notion of \textit{local} and \textit{global} is used at two different levels: at the input clip level and the feature level. Clip-E is a global clip and Clip A-D are local clips contained within Clip-A. Features are referred to as \textit{global} after the final pooling operation and a temporal slice of the feature map before the pooling operation is referred to as a \textit{local} feature. In Figure~\ref{fig:tcp},  is the local feature of the global Clip-A and  is the global feature of the local Clip-B. 



For a video instance , divided into  clips, the local clip  can either be represented by a global (pooled) representation  or a local representation  of the corresponding timestep in the feature map of the global clip. This loss has two sets of reciprocal terms, with  and  serving as the anchor for each term. The negative pairs are supplied by matching the anchors with representations corresponding to other non-overlapping local clips. Note that similar to our local-local temporal contrastive loss we do not use negatives from other video instances for calculating this loss. The loss is defined by the following equations: 







\section{Experiments}
\noindent {\bf Datasets and Implementation:}\label{para:dataset} We use three action recognition datasets: UCF101~\cite{ucf101}, Kinetics400~\cite{kinetics}, and HMDB51~\cite{hmdb} for our experiments. We use the three most commonly used networks from the literature: 3D-ResNet-18 (R3D-18)~\cite{kenshohara}, R(2+1)D-18~\cite{r2plus1d}, and C3D~\cite{c3d} for our experiments. For non-linear projection head, we use a multi-layer perceptron with 1-hidden layer following experimental setting of \cite{simclr}. We utilize 4 local clips per global clip for the global-local temporal contrastive loss.  For all reported results, we utilize commonly used random augmentations including appearance-based transforms such as grayscale, channel dropping, and color jittering and geometry-based transforms like random scaling, random cropping, random cut-out and random horizontal-flip. Our results can be further improved by using more complex augmentations like Gaussian blurring, shearing and rotation, however these are not used in the results reported in this paper. We provide results with more complex augmentation in Section D of the supplementary material. For self-supervised pretraining we use UCF101 training set (split-1) or Kinetics400 training set, without using any class labels. For all self-supervised pretraining, supervised finetuning and other downstream tasks, we use clips of 16 frames with a resolution of . More implementation details can be found in Section C of the supplementary material.





 




















\subsection{Evaluating Self-supervised Representations}



 We evaluate the learned video representation using different downstream video understanding tasks: i) action recognition and ii) nearest neighbor video retrieval on UCF101 and HMDB51 datasets, and iii) limited label training on UCF101, following protocols from prior works \cite{memdpc}. We also evaluate our learned representations on the challenging Diving-48 fine-grained action recognition task~\cite{diving}; to the best of our knowledge TCLR is the first work that reports result on this challenging task. Our method is also employed in Knights~\cite{dave2021knights} to get first place in ICCV-21 Action recognition challenge~\cite{lengyel2022vipriors}.     
 
 


\noindent {\bf Action Recognition on UCF101 and HMDB51: }



\begin{table*}
\begin{center}
\small
\vspace{-5em}
\begin{tabular}{ccccccc}
\hline

\hline

\hline\-3mm]
\\
 \hline

\hline

\hline
 \multicolumn{7}{c}{\textbf{Backbone: R3D-18}}\\
 \hline
ST-Puzzle~\cite{aaai19} & AAAI-19 &  &  &  &  & \\
STS~\cite{statistics2} & TPAMI-21 &  &  &  &  & \\
DPC~\cite{dpc} & ICCVw-19 &  & & & & \\
VCOP~\cite{vcop} & CVPR-20 &  & &  & & \\
Pace Pred~\cite{pace_pred} & ECCV-20 &  & & & & \\
VCP~\cite{vcp} & AAAI-20 &  & &  & & \\
PRP~\cite{prp} & CVPR-20 &  & &  & &     \\
Var. PSP~\cite{cho2020self} & Access-21 &  & & & & \\
MemDPC~\cite{memdpc} & ECCV-20 &  & & & & \\
TCP~\cite{wacv20} & WACV-21 &  & &  & & \\
VIE~\cite{vie} & CVPR-20 &  &  &  &  &   \\
UnsupIDT~\cite{idt} & ECCVw-20 &  &  &  &    &   \\
CSJ~\cite{csj} & - &  &   &  &  &   \\

BFP~\cite{bfp} & WACV-21 &  &  &  &  & \\
IIC (RGB)~\cite{iic} & ACMMM-20 &  &  &  &  & \\
CVRL (Reproduced)~\cite{cvrl} & CVPR-21 &  &    &   & &    \\
SSTL~\cite{iclr21submitted_st_invariant} & - &  &  &  &  & \textcolor{blue}{} \\
VTHCL~\cite{vtc} & - &  &  &  &  &  \\
VideoMoCo~\cite{videomoco} & CVPR-21 &  &  &  &  & \\
RSPNet~\cite{rspnet} & AAAI-21 &  &  &  &  & \\
Temp Trans~\cite{simon}& ECCV-20 &  & \textcolor{blue}{}  & \textcolor{blue}{}  &  &  \\
TaCo~\cite{taco} & - &  &  &  &  & \\
MFO~\cite{iccv21qian} & ICCV-21 &  &  &  &  & \\

\hline
\textbf{TCLR} & &  & \textcolor{red}{}  & \textcolor{red}{}  & \textcolor{red}{}  & \textcolor{red}{}  \\\hline
\textbf{TCLR (Best Ablation)} & &  & \textcolor{red}{}  & \textcolor{red}{}  & \textcolor{red}{}  & \textcolor{red}{}  \\
\hline
 \multicolumn{7}{c}{\textbf{Backbone: R(2+1)D-18}}\\
 \hline
VCP~\cite{vcp} & AAAI-20 &  &   &   &  &   \\
PRP~\cite{prp} & CVPR-20 &  &   &   &  &   \\
VCOP~\cite{vcop} & CVPR-20 &  &   &   &      &       \\
Pace Pred~\cite{pace_pred} & ECCV-20 &  &   &   &   &    \\
STS~\cite{statistics2} & TPAMI-21 &  &   &   &      &   \\
VideoMoCo~\cite{videomoco} & CVPR-21 &  &  &  &  & \\
VideoDIM~\cite{vdim} & - &  &  &  &  &   \\
RSPNet~\cite{rspnet} & AAAI-21 &  &  &  &  & \\
Temp Trans~\cite{simon} & ECCV-20 &  & \textcolor{blue}{}  & \textcolor{blue}{}  &  &   \\
TaCo~\cite{taco} & - &  &  &  &  & \\
\hline
{\bf TCLR }&  &  & \textcolor{red}{}     & \textcolor{red}{} & \textcolor{red}{} & \textcolor{red}{}      \\\hline

\hline
 \multicolumn{7}{c}{\textbf{Backbone: C3D}}\\
 \hline
MA Stats-1~\cite{statistics_cvpr19} & CVPR-19 &  &   &   &  &   \\
Temp Trans~\cite{simon} & ECCV-20 &  &   &   &  &   \\
PRP~\cite{prp} & CVPR-20 & &   &   &     &      \\
VCP~\cite{vcp} & AAAI-20 & &   &   &     &      \\
VCOP~\cite{vcop} & CVPR-20 &  &   &   &     &      \\
Pace Pred~\cite{pace_pred} & ECCV-20 & &   &   &   &    \\
STS~\cite{statistics2} & TPAMI-21 & &   &   &   &  \\
Var. PSP~\cite{cho2020self} & Access-21 &  & \textcolor{blue}{} &  &   &      \\
DSM~\cite{scene-motion} & AAAI-21 &  &  &  &  & \\
\hline
{\bf TCLR} & &  & \textcolor{red}{}     & \textcolor{red}{}  & &    \\\hline

\hline
\multicolumn{6}{c}{\textbf{Other Configurations}}\\
\hline
\grayedout{CVRL (R3D-50)~\cite{cvrl}} & \grayedout{CVPR-21} & \grayedout{} &      &   & \grayedout{} & \grayedout{}   \\
\grayedout{RSPNet (S3D-G)~\cite{rspnet}} & \grayedout{AAAI-21} & \grayedout{} &      &   & \grayedout{} & \grayedout{}   \\
\grayedout{CoCLR\textsuperscript{\textdagger} (S3D-23)~\cite{cotraining}} & \grayedout{NeurIPS-20} & \grayedout{} & \grayedout{} & \grayedout{} & \grayedout{} & \grayedout{}\\
\grayedout{SpeedNet (S3D-G)~\cite{speedNet}} & \grayedout{CVPR-20} & \grayedout{} &  &  & \grayedout{} & \grayedout{}\\
\grayedout{SimCLR (R50)~\cite{Feichtenhofer_2021_CVPR}} & \grayedout{CVPR-21} & \grayedout{} & \grayedout{} & \grayedout{} & \grayedout{} & \grayedout{}\\
\grayedout{SeCO (R50+TSN)~\cite{seco}} & \grayedout{AAAI-21} & \grayedout{} &  &  & \grayedout{} & \grayedout{}\\
\hline
\end{tabular}
\end{center}
\caption{Finetuning Results (average of 3 splits) for action classification on UCF101 and HMDB51. Self supervised pretraining  was done on UCF101 (left) and Kinetics (right). \textsuperscript{\textdagger} indicates models that utilize optical flow.  indicates Kinetics-600 self-supervised pretraining.  indicates ImageNet+Kinetics pre-training. \textcolor{red}{Best} and \textcolor{blue}{second best} results are highlighted.}
\label{tab:finetuning-results}
\end{table*}

\begin{table*}
\footnotesize

\centering
 \begin{tabular}{cccccccccc}
\hline

\hline

\hline\-3mm]
\multicolumn{10}{c}{\textbf{Backbone: R3D-18}} \\
\hline
VCOP~\cite{vcop} & CVPR-20 &   &   &   &      &   &   &   &      \\
VCP~\cite{vcp}   & AAAI-20 &   &   &   &      &   &   &   &      \\
Pace Pred~\cite{pace_pred} & ECCV-20 &   &   &   &      &   &   &   &      \\
Var. PSP~\cite{cho2020self}               & Access-21 &   &   &   &      &   &   &   &      \\
Temp Trans~\cite{simon} & ECCV-20 &   &   &   &      &   &   &   &      \\
STS\supstar~\cite{statistics2}              & TPAMI-21 &   &   &   &      &   &   &   &      \\
SSTL\supstar~\cite{iclr21submitted_st_invariant}  & - & \textcolor{blue}{}  &   &   &      & \textcolor{blue}{}  &   &   &      \\
CSJ\supstar~\cite{csj}                                        & - &      &      &      &         &      &      &      &         \\
MemDPC~\cite{memdpc} & ECCV-20 &   &   &   &      &   &   &   &      \\
RSPNet~\cite{rspnet}                                     & AAAI-21 &   &   &   &    &   &   &   &    \\
MFO~\cite{iccv21qian}                                     & ICCV-21 &   &   &   &    &   &   &   &    \\
\hline
\textbf{TCLR}                                        & - & \textcolor{red}{} & \textcolor{red}{} & \textcolor{red}{} & \textcolor{red}{}  & \textcolor{red}{} & \textcolor{red}{} & \textcolor{red}{} & \textcolor{red}{}  \\
\hline 

\hline
\multicolumn{10}{c}{\textbf{Backbone: C3D}} \\
\hline 
VCOP~\cite{vcop}                                     & CVPR-20 &   &   &   &    &   &   &   &    \\
VCP~\cite{vcp}                                       & AAAI-20 &   &   &   &    &   &   &   &    \\
Pace Pred~\cite{pace_pred}                           & ECCV-20 &   &   &   &    &   &   &   &    \\
DSM~\cite{pace_pred}                                   & AAAI-21 &   &   &   &      &   &   &   &      \\
STS\supstar~\cite{statistics2}      & TPAMI-21 &   &   &   &    & \textcolor{blue}{} & \textcolor{blue}{}  &   &    \\
RSPNet~\cite{rspnet}                                     & AAAI-21 &   &   &   &    &   &   &   &    \\
\hline
\textbf{TCLR}                                        & - & \textcolor{red}{} & \textcolor{red}{} & \textcolor{red}{} & \textcolor{red}{}  & \textcolor{red}{} & \textcolor{red}{} & \textcolor{red}{} &  \textcolor{red}{}  \\

\hline

\hline
\multicolumn{10}{c}{\textbf{Backbone: R(2+1)D-18}} \\
\hline 
VCOP~\cite{vcop}                                     & CVPR-20 &   &   &   &    &   &   &   &    \\
VCP~\cite{vcp}                                       & AAAI-20 &   &   &   &    &   &   &   &    \\
Pace Pred~\cite{pace_pred}                           & ECCV-20 &   &   &   &    &   &   &   &    \\
STS\supstar~\cite{statistics2}      & TPAMI-21 & \textcolor{blue}{}  & \textcolor{blue}{}  & \textcolor{blue}{}  & \textcolor{blue}{}   & \textcolor{blue}{}  & \textcolor{blue}{}  & \textcolor{blue}{}  & \textcolor{blue}{}   \\
\hline
\textbf{TCLR}               & - & \textcolor{red}{} & \textcolor{red}{}  & \textcolor{red}{}  & \textcolor{red}{}   & \textcolor{red}{} & \textcolor{red}{}  & \textcolor{red}{}  & \textcolor{red}{}   \\
\hline

\hline

\hline\-3mm]
\textbf{Pre-Training}  & \textbf{Accuracy}  \\
\hline

\hline

\hline\-3mm]

\end{tabular}
\vspace{-2mm}
\caption{\label{tab:diving-results} Diving48 fine-grained action classification results.}
\vspace{-4mm}
\end{table}

 Results are summarized in Table~\ref{tab:diving-results}. TCLR pretraining on Diving48 without extra data outperforms random initialization and MiniKinetics~\cite{minikinetics} supervised pretraining. The within-instance temporal discrimination losses in TCLR help it outperform the instance contrastive loss. This is due to TCLR learning features to represent fine-grained differences between parts of diving video instances. 













\vspace{-2mm}
\subsection{Ablation Study}
\vspace{-4mm}
\begin{table}[H]
\small
\centering
\begingroup
\setlength{\tabcolsep}{3pt} \renewcommand{\arraystretch}{1.0} \resizebox{\columnwidth}{!}{
\begin{tabular}{@{\hskip0pt}ccccccc@{\hskip0pt}}
\hline

\hline 

\hline\-3mm]
\multicolumn{3}{c}{Random Init.} &  &  &  &  \\
\xmark & \cmark & \xmark &  &  &  &  \\
\xmark & \xmark & \cmark &  &  &  &  \\
\xmark & \cmark & \cmark &  &  &  &  \\
\hline
\cmark & \xmark & \xmark &  &  &  &  \\
\cmark & \cmark & \xmark &  \increase{~~+8\%} & \increase{~~+6\%} & \increase{+11\%} & \increase{+10\%} \\
\cmark & \xmark & \cmark &  \increase{+10\%}  & \increase{~~+5\%} & \increase{+10\%} & \increase{~~+7\%} \\
\cmark & \cmark & \cmark &  \increase{+15\%}  & \increase{+11\%}  & \increase{+14\%} & \increase{+15\%} \\
 \hline

\hline 

\hline\\label{eq:all_losses}
  \mathcal{L} = \sum_{i=1}^{N_B} \mathcal{L}_{IC}^{i} + \mathcal{L}_{LL}^{i} + \mathcal{L}_{GL}^{i},
-3mm]
\textbf{Method}         & \textbf{Recall@1}   & \textbf{Recall@5}   & \textbf{Recall@10}  & \textbf{Recall@20}   \\ 
\hline

\hline

\hline\-3mm]
\end{tabular}
\endgroup
\caption{Nearest Neighbor Video Retrieval on UCF101 using self-supervised R3D-18 models pretrained on UCF101 videos.}

\label{table:nnr_ucf_abl}
\end{table}

\begin{table}[h]
\centering
\begingroup
\setlength{\tabcolsep}{3pt} \renewcommand{\arraystretch}{1.0} \begin{tabular}{lcccc} 
\hline

\hline

\hline\-3mm]


\textbf{IC}         &14.38& 35.62& 48.37& 61.57  \\
\textbf{IC+LL}      & 19.07& 42.42& 54.97& 69.35 \\
\textbf{IC+GL}      & 18.43& 41.70& 53.59& 67.19  \\
\textbf{TCLR }      & 22.75& 45.36& 57.84& 73.07  \\ 
\hline

\hline

\hline\-3mm]
       \textbf{Method} & \textbf{1\%}  & \textbf{10\%}   & \textbf{20\%}  & \textbf{50\%}   \\ 
\hline

\hline

\hline\-3mm]
\end{tabular}
\caption{Top-1 Accuracy after limited label finetuning of self-supervised R3D-18 models pretrained on UCF101 (split-1) videos.}
\label{table:limited_label_abl}
\end{table}




\subsection{Augmentation Types}

    We perform an ablation study to measure the impact of computationally expensive transformations (specifically random Gaussian Blurring, Shearing and Rotation). We observe (Results in Table~\ref{table:augablation}) that adding these transforms only has a small effect on the performance of the model on downstream tasks, and removing them can reduce training time by as much as 30\%. In these augmentation ablations, we use Gaussian blur with 30\% probability, kernel size  and standard deviation selected randomly from  interval. We apply shear and rotation with 30\% probability, with rotation angles selected randomly from . Even though it's possible to obtain slightly better results by using blur and other complex augmentations we do not use them for our main reported results.
    
\begin{table}[h]
\centering
\begin{tabular}{llc} 
\hline

\hline

\hline\-3mm]
\textbf{Basic}  & \textbf{Basic} & \textbf{69.91}                                        \\
+ Blur & Basic & 70.7 \textcolor{c2}{(+0.8)}                                 \\ 
Basic  & + Shear \& Rotate & 70.2 \textcolor{c2}{(+0.3)}  \\
\hline

\hline

\hline\-3mm]
\textbf{Embedding size} & \begin{tabular}[c]{@{}l@{}}\textbf{Linear Evaluation}\\~(\textbf{Top-1 Accuracy})\end{tabular}  \\ 
\hline

\hline

\hline\-3mm]
\end{tabular}
\caption{Effect of embedding size.}
\label{table:embedabl}
\end{table}

\subsection{Number of Timesteps}
A key hyperparameter for the Temporal Constrastive losses is the number of Timesteps  that a given video instance is sliced into. Apart from the default setting of  we also study  and find that it degrades performance. We do not use higher values of  since that causes a significant increase in amount of GPU memory and computation required. Results are available in Table~\ref{table:timestepabl}.



\begin{table}[h]
\centering
\begin{tabular}{cc} 
\hline

\hline

\hline\-3mm]
\textbf{4} & \textbf{69.91} \\
2 & 66.60 \textcolor{c3}{(-3.31)} \\ 
\hline

\hline

\hline\-3mm]
\begin{tabular}[c]{@{}c@{}}\textbf{\textbf{}}\\~\textbf{Temperature}\end{tabular} &\begin{tabular}[c]{@{}c@{}}\textbf{\textbf{, }}\\~\textbf{Temperature}\end{tabular}  & \begin{tabular}[c]{@{}c@{}}\textbf{Linear Evaluation}\\~(\textbf{Top-1 Accuracy})\end{tabular}  \\ 
\hline

\hline

\hline\-3mm]
\end{tabular}
\caption{Effect of temperatures used in contrastive losses.}
\label{table:tempabl}
\end{table}
\section{Baseline Verification}
\label{sec:bl_verification}
\noindent {\bf Instance Contrastive Baseline:} 
Our instance contrastive baseline using similar model architectures and input clip resolution (, 16 frames) achieves 71.3\% .  We use the same input size, architecture and data augmentations for our instance contrastive baseline and TCLR model. 

To ensure our instance contrastive baseline is correctly trained we also provide results from prior works in Table~\ref{tab:ic-baselines} 



\begin{figure*}[h]
\begin{center}
 \includegraphics[width=0.9\linewidth]{supp_figs/combined2_chart_ucf101pret.pdf}
\end{center}
\vspace{-4mm}
   \caption{\textbf{Effect of UCF101 pre-training duration} on downstream UCF101 and HMDB51 action recognition. \textit{TCLR outperforms the Vanilla IC baseline with 4-8 times longer pre-training}. (U)  UCF101 finetuning  and (H)  HMDB51 finetuning}
\label{fig:combined}
\end{figure*}

\begin{table}[h]
\centering
\begin{tabular}{lccc} 
 \hline

\hline

\hline\1pt]
 \hline

\hline

\hline\-3mm]
\end{tabular}
\caption{\textbf{Comparison of TCLR vs Vanilla IC with longer pre-training on Kinetics400} as reported in prior work. \\  email from authors}
\label{table:kinetics}
\end{table}

\begin{table}[h]
\small
\centering
\begin{tabular}{llcc}
\hline

\hline

\hline\-3mm]

\textbf{Our }        & R3D18       & UCF101           & 71.3/ 38.3                         \\ 
\cite{pace_pred}                    & R(2+1)D18     & UCF101           & 67.3/ 28.6                         \\ 
\cite{idt}                          & R3D18       & K400      & 70.0/ 39.9                         \\ 
\cite{iclr21submitted_st_invariant} & R3D18       & K400      & 69.5/        \\ 
\cite{taco}                         & R3D18 & K400      & 73.0/ 40.6                         \\\hline

\hline

\hline\-3mm]

& \multicolumn{3}{c}{\textbf{UCF101}} & \multicolumn{2}{|c}{\textbf{HMDB51}} \\
\cline{2-6}
        & \textbf{Linear Eval}          & \textbf{Finetuning}  & \textbf{NN-Retrieval}              & \textbf{Finetuning} & \textbf{NN-Retrieval}  \\ 
\hline

\hline

\hline\-3mm]
\end{tabular}
\vspace{-3mm}
\caption{Effect of using multiple clips on different evaluation results. \textbf{1-clip/ 10-clip} performance on different downstream tasks.}
\vspace{0.3in}
\label{tab:summtask_clips}
\end{table*}



\section{Additional Comparison} 
\label{sec:additional_comparison}

Results from prior work which were excluded from the main paper are presented in Table~\ref{table:results}. The Table is divided into 3 sections. The first section includes results from papers which utilize visual modality but only provide results on specialized architectures or larger input sizes which are not widely used and hence cannot be compared fairly with other methods.  The second section includes works which utilize multi-modal data, beyond the visual domain, such as text and audio. The third section shows our results for temporal contrastive learning from the main paper to allow for comparison with these works, such a comparison however is not fair for reasons explained earlier. We also perform linear classification of Kinetics-400 using our self-supervised pre-trained R2+1D model, which gives 21.8\% top-1 accuracy. 




\begin{table*}[h!]
\centering
\begin{tabular}{lclccccc} 
\hline

\hline

\hline\-3mm]\multicolumn{8}{l}{\textbf{Specialized Architectures}} \\
\hline
DVIM~\cite{dvim}          &         & R2D-18     & V & -            & UCF101 & 62.1  & 28.2     \\
DVIM~\cite{dvim}          &         & R2D-18     & V & -            & K400 & 64.0  & 29.7   \\
TCE~\cite{tce}                 &         & R2D50& V          & 23.0         & K400 & 71.2  & 36.6   \\ 
O3N~\cite{fernando2017self}      &         & AlexNet& V           & 61.0         & UCF101 & 60.3  & 32.5      \\ 
Shuffle-Learn~\cite{misra2016shuffle}              &         & AlexNet& V           & 61.0         & UCF101 & 50.2  & 18.1       \\ 
B\"uchler et al.~\cite{buchler2018improving} &         & AlexNet& V  & -            & UCF101 & 58.6  & 25.0        \\ 
Video Jigsaw~\cite{videojigsaw}             &        & CaffeNet& V          & 61.0         & UCF101 & 46.4  & -     \\
Video Jigsaw~\cite{videojigsaw}             &        & CaffeNet& V          & 61.0         & K400  & 55.4  & 27.0   \\
CBT~\cite{xdc}                  &        & S3D & V               & 20.0         & K600  & 79.5  & 44.6   \\ 


\hline
\multicolumn{8}{l}{\textbf{Multi-Modal Methods}} \\
\hline
AVTS~\cite{gdt} &   & I3D & V+A & - & K400 & 83.7 & 53.0 \\
AVTS~\cite{gdt} &   & MC3 & V+A & - & AudioSet & 89.0 & 61.6 \\
MIL-NCE~\cite{milnce} &  & S3D & V+T & - & HowTo100M & 91.3 & 61.0 \\
GDT~\cite{gdt} &   & R(2+1)D & V+A & - & K400 & 89.3 & 60.0 \\
GDT~\cite{gdt} &   & R(2+1)D & V+A & - & IG65M & 95.2 & 72.8 \\
XDC~\cite{gdt} &   & R(2+1)D & V+A & - & K400 & 84.2 & 47.1 \\
\hline
\multicolumn{8}{l}{\textbf{TCLR}} \\
\hline
TCLR (Ours)   &        & R3D-18& V      & 13.5         & UCF101 & 82.4  & 52.9       \\ 
TCLR (Ours)   &        & R3D-18& V      & 13.5         & K400 & 84.1     & 53.6      \\ 
TCLR (Ours)   &        & R(2+1)D& V           & 14.4         & UCF101 & 82.8     & 53.6         \\ 
TCLR (Ours)   &        & R(2+1)D& V           & 14.4         & K400 &  84.3     & 54.2      \\ 
TCLR (Ours)   &        & C3D& V               & 27.7         & UCF101 & 76.1     & 48.6     \\
\hline

\hline

\hline\-3mm]
\end{tabular}

\caption{Additional Finetuning Results (average of 3 splits) for action classification on UCF101 and HMDB51 from prior work that were excluded from the main paper, along with our results for comparison. Please note that these results are not strictly comparable since the prior work uses specialized architectures and larger input sizes, which has significant effect on performance and require excessive computational resources. \textbf{V} denotes Video(RGB) modality, \textbf{A} denotes audio, and \textbf{T} denotes text modality.  modified architecture}
\label{table:results}
\end{table*}













\section{Feature Slice Representation Similarity}
\label{sec:feature_slice_similarity}

The goal of this visualization (see Figure~\ref{fig:timecontrast} is to verify if the model learns temporal diversity in the feature map as intended by the TCLR framework. With a long global clip (16 frames with a skip rate of 4, hence covering 64 frames), representations for 4 different timesteps of the video feature map were obtained using the video encoder and projection head. Cosine similarity was computed between timesteps for each video and then averaged across the dataset. As can be observed,  has the maximum impact on increasing within clip feature diversity, while  also induces more diversity than the IC baseline. 

\begin{figure}[h]
\vspace{-2mm}
\centering
    \begin{subfigure}{0.25\textwidth}
    \centering

        \includegraphics[width=\textwidth]{supp_figs/SCRATCH.pdf}
        \caption{Random Initialization}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.25\textwidth}
        \includegraphics[width=\textwidth]{supp_figs/IC.pdf}
        \caption{}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.25\textwidth}
        \centering
        \includegraphics[width=\textwidth]{supp_figs/LL.pdf}
        \caption{ + }
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.25\textwidth}
       \centering
       \includegraphics[width=\textwidth]{supp_figs/GL.pdf}
        \caption{ + }
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.25\textwidth}
        \centering
        \includegraphics[width=\textwidth]{supp_figs/TCLR.pdf}
        \caption{TCLR}
    \end{subfigure}
    
    \vspace{-3mm}
    \caption{Cosine similarity between timesteps of the feature map learned using different losses. It can be observed that temporal distinctiveness learned features increases with the addition of temporal contrastive losses.}    \label{fig:timecontrast}
    \vspace{0.5in}
\end{figure}



\section{Qualitative: Nearest Neighbour Retrieval}
\label{sec:qualitative}
Qualitative results for nearest neighbour retrieval task on UCF101 are presented in Figure~\ref{fig:retrieval_qual}. These results are obtained using self-supervised pretrained models, without any supervised training with labels.
\begin{figure*}[t]
\begin{center}
  \includegraphics[width=\linewidth]{suppfigs2/retrieval_11novV2-cropped.pdf}
\end{center}
\vspace{-6mm}
  \caption{\textbf{Qualitative Results:} Nearest Neighbour Video Retrieval results on UCF101. For each query, the upper row show videos retrieved by our method and the lower row shows results from standard instance contrastive loss.}
\label{fig:retrieval_qual}
\vspace{0.2in}
\end{figure*}



\begin{figure*}[t]
\vspace{1.5in}
\centering
\begin{center}
    \includegraphics[clip, trim=2cm 1.2cm 0.5cm 1cm,width=0.33\textwidth]{supp_figs/scratch_val_pcainit_p50.pdf}
    \includegraphics[clip, trim=2cm 1.2cm 0.5cm 1cm,width=0.33\textwidth]{supp_figs/mix_IC_val_tsne_10_pcainit_p50.pdf}
    \includegraphics[clip, trim=2cm 1.2cm 0.5cm 1cm,width=0.33\textwidth]{supp_figs/mix_Full_val_tsne_10_pcainit_p50.pdf}
\end{center}
    \caption{t-SNE visualization of Randomly Initialized (left) features; features learned using Instance Contrastive loss (center);  and TCLR (right),  for 10  randomly chosen action classes from UCF101 Test set:  \textcolor{FieldHockeyPenalty}{FieldHockeyPenalty}, \textcolor{GolfSwing}{GolfSwing}, \textcolor{SoccerJuggling}{SoccerJuggling}, \textcolor{SoccerPenalty}{SoccerPenalty}, \textcolor{PlayingGuitar}{PlayingGuitar}, \textcolor{PlayingPiano}{PlayingPiano}, \textcolor{PlayingSitar}{PlayingSitar}, \textcolor{ApplyEyeMakeup}{ApplyEyeMakeup}, \textcolor{ApplyLipstick}{ApplyLipstick} and \textcolor{BlowDryHair}{BlowDryHair}.  TCLR results in more coherent clusters compared to the instance contrastive lossand is able to discriminate between similar classes with fine-grained differences such as \textcolor{ApplyEyeMakeup}{ApplyEyeMakeup}, \textcolor{ApplyLipstick}{ApplyLipstick} and \textcolor{BlowDryHair}{BlowDryHair}.}
    \label{fig:tsnecompare}
    \vspace{0.8in}
\end{figure*}


\section{Visualization of Learned Representations}
\label{sec:tsne}

\vspace{-0.1in}




We use t-SNE \cite{tsne} to visualize and study the representation learned by the model during self-supervised pre-training (without any supervised finetuning).  We compare our method to the standard instance contrastive pretraining and randomly initialized features for a selected set of classes in Figure~\ref{fig:tsnecompare} for better visibility. In Figure~\ref{fig:tsne} we compare TCLR to standard instance contrastive training for different sets of classes are selected at random to get a broad look at the overall representation space. The training set (split-1) of UCF101 without labels is used for pre-training and the test set is used for visualization. In order to ensure reproducible t-SNE results, we use PCA initialization and set perplexity to 50. As can be seen by comparing the representations, we can see that TCLR results in well separated clusters compared to the standard instance contrastive loss.



\begin{figure*}[h!]
\vspace{-2mm}
\centering
\begin{subfigure}[t]{0.49\textwidth}
    \includegraphics[clip, trim=2cm 1.2cm 0.5cm 1cm, width=0.49\textwidth]{suppfigs2/tsne/tsne0_IC_val_pcainit_p50.pdf}
    \includegraphics[clip, trim=2cm 1.2cm 0.5cm 1cm,width=0.49\textwidth]{suppfigs2/tsne/tsne0_TCLR_val_pcainit_p50.pdf}
    \caption{\textcolor{c0}{MilitaryParade}, \textcolor{c1}{TrampolineJumping}, \textcolor{c2}{PlayingSitar}, \textcolor{c3}{PlayingViolin}, \textcolor{c4}{FieldHockeyPenalty}, \textcolor{c5}{HighJump}, \textcolor{c6}{StillRings}, \textcolor{c7}{BlowDryHair}, \textcolor{c8}{ApplyEyeMakeup}}
\end{subfigure}
\hfill \begin{subfigure}[t]{0.49\textwidth}
    \includegraphics[clip, trim=2cm 1.2cm 0.5cm 1cm, width=0.49\textwidth]{suppfigs2/tsne/tsne11_IC_val_pcainit_p50.pdf}
    \includegraphics[clip, trim=2cm 1.2cm 0.5cm 1cm,width=0.49\textwidth]{suppfigs2/tsne/tsne11_TCLR_val_pcainit_p50.pdf}
    \caption{\textcolor{c0}{PlayingSitar}, \textcolor{c1}{PlayingPiano}, \textcolor{c2}{GolfSwing}, \textcolor{c3}{HighJump}, \textcolor{c4}{ShavingBeard}, \textcolor{c5}{ApplyLipstick}, \textcolor{c6}{PoleVault}, \textcolor{c7}{BasketballDunk}, \textcolor{c8}{Typing}, \textcolor{c9}{MixingBatter}}
\end{subfigure}
\begin{subfigure}[t]{0.49\textwidth}
    \includegraphics[clip, trim=2cm 1.2cm 0.5cm 1cm, width=0.49\textwidth]{suppfigs2/tsne/tsne23_IC_val_pcainit_p50.pdf}
    \includegraphics[clip, trim=2cm 1.2cm 0.5cm 1cm,width=0.49\textwidth]{suppfigs2/tsne/tsne23_TCLR_val_pcainit_p50.pdf}
    \caption{\textcolor{c0}{FieldHockeyPenalty}, \textcolor{c1}{HighJump}, \textcolor{c2}{Knitting}, \textcolor{c3}{Typing}, \textcolor{c4}{Basketball}, \textcolor{c5}{UnevenBars}, \textcolor{c6}{ApplyLipstick}, \textcolor{c7}{BlowDryHair}, \textcolor{c8}{FrontCrawl}, \textcolor{c9}{BreastStroke}}
\end{subfigure}
\hfill \begin{subfigure}[t]{0.49\textwidth}
    \includegraphics[clip, trim=2cm 1.2cm 0.5cm 1cm, width=0.49\textwidth]{suppfigs2/tsne/tsne32_IC_val_pcainit_p50.pdf}
    \includegraphics[clip, trim=2cm 1.2cm 0.5cm 1cm,width=0.49\textwidth]{suppfigs2/tsne/tsne32_TCLR_val_pcainit_p50.pdf}
    \caption{\textcolor{c0}{Knitting}, \textcolor{c1}{Typing}, \textcolor{c2}{Diving}, \textcolor{c3}{BreastStroke}, \textcolor{c4}{HammerThrow}, \textcolor{c5}{Nunchucks}, \textcolor{c6}{Haircut}, \textcolor{c7}{ShavingBeard}, \textcolor{c8}{Biking}, \textcolor{c9}{TrampolineJumping}}
\end{subfigure}
\begin{subfigure}[t]{0.49\textwidth}
    \includegraphics[clip, trim=2cm 1.2cm 0.5cm 1cm, width=0.49\textwidth]{suppfigs2/tsne/tsne41_IC_val_pcainit_p50.pdf}
    \includegraphics[clip, trim=2cm 1.2cm 0.5cm 1cm,width=0.49\textwidth]{suppfigs2/tsne/tsne41_TCLR_val_pcainit_p50.pdf}
    \caption{\textcolor{c0}{PullUps}, \textcolor{c1}{PushUps}, \textcolor{c2}{WritingOnBoard}, \textcolor{c3}{WalkingWithADog}, \textcolor{c4}{HighJump}, \textcolor{c5}{VolleyballSpiking}, \textcolor{c6}{BasketballDunk}, \textcolor{c7}{StillRings}, \textcolor{c8}{BrushingTeeth}, \textcolor{c9}{ShavingBeard}}
\end{subfigure}
\hfill \begin{subfigure}[t]{0.49\textwidth}
    \includegraphics[clip, trim=2cm 1.2cm 0.5cm 1cm, width=0.49\textwidth]{suppfigs2/tsne/tsne43_IC_val_pcainit_p50.pdf}
    \includegraphics[clip, trim=2cm 1.2cm 0.5cm 1cm,width=0.49\textwidth]{suppfigs2/tsne/tsne43_TCLR_val_pcainit_p50.pdf}
    \caption{\textcolor{c0}{BasketballDunk}, \textcolor{c1}{ParallelBars}, \textcolor{c2}{PlayingViolin}, \textcolor{c3}{PlayingSitar}, \textcolor{c4}{Knitting}, \textcolor{c5}{Typing}, \textcolor{c6}{WritingOnBoard}, \textcolor{c7}{BlowingCandles}, \textcolor{c8}{JumpingJack}, \textcolor{c9}{HandstandWalking}}
\end{subfigure}
\begin{subfigure}[t]{0.49\textwidth}
    \includegraphics[clip, trim=2cm 1.2cm 0.5cm 1cm, width=0.49\textwidth]{suppfigs2/tsne/tsne44_IC_val_pcainit_p50.pdf}
    \includegraphics[clip, trim=2cm 1.2cm 0.5cm 1cm,width=0.49\textwidth]{suppfigs2/tsne/tsne44_TCLR_val_pcainit_p50.pdf}
    \caption{\textcolor{c0}{WallPushups}, \textcolor{c1}{Lunges}, \textcolor{c2}{PlayingFlute}, \textcolor{c3}{PlayingViolin}, \textcolor{c4}{ParallelBars}, \textcolor{c5}{PommelHorse}, \textcolor{c6}{Diving}, \textcolor{c7}{BreastStroke}, \textcolor{c8}{Shotput}, \textcolor{c9}{CricketBowling}}
\end{subfigure}
\hfill \begin{subfigure}[t]{0.49\textwidth}
    \includegraphics[clip, trim=2cm 1.2cm 0.5cm 1cm, width=0.49\textwidth]{suppfigs2/tsne/tsne46_IC_val_pcainit_p50.pdf}
    \includegraphics[clip, trim=2cm 1.2cm 0.5cm 1cm,width=0.49\textwidth]{suppfigs2/tsne/tsne46_TCLR_val_pcainit_p50.pdf}
    
    \caption{\textcolor{c0}{Skijet}, \textcolor{c1}{BreastStroke}, \textcolor{c2}{Basketball}, \textcolor{c3}{PommelHorse}, \textcolor{c4}{PlayingSitar}, \textcolor{c5}{PlayingDaf}, \textcolor{c6}{FrisbeeCatch}, \textcolor{c7}{GolfSwing}, \textcolor{c8}{WallPushups}, \textcolor{c9}{HandstandWalking}}
\end{subfigure}
\begin{subfigure}[t]{0.49\textwidth}
    \includegraphics[clip, trim=2cm 1.2cm 0.5cm 1cm, width=0.49\textwidth]{suppfigs2/tsne/tsne48_IC_val_pcainit_p50.pdf}
    \includegraphics[clip, trim=2cm 1.2cm 0.5cm 1cm,width=0.49\textwidth]{suppfigs2/tsne/tsne48_TCLR_val_pcainit_p50.pdf}
    \caption{\textcolor{c0}{HandstandPushups}, \textcolor{c1}{WallPushups}, \textcolor{c2}{Typing}, \textcolor{c3}{Knitting}, \textcolor{c4}{Archery}, \textcolor{c5}{Shotput}, \textcolor{c6}{PlayingDhol}, \textcolor{c7}{PlayingFlute}, \textcolor{c8}{WalkingWithADog}, \textcolor{c9}{BlowingCandles}}
\end{subfigure}
\hfill \begin{subfigure}[t]{0.49\textwidth}
    \includegraphics[clip, trim=2cm 1.2cm 0.5cm 1cm, width=0.49\textwidth]{suppfigs2/tsne/tsne49_IC_val_pcainit_p50.pdf}
    \includegraphics[clip, trim=2cm 1.2cm 0.5cm 1cm,width=0.49\textwidth]{suppfigs2/tsne/tsne49_TCLR_val_pcainit_p50.pdf}
    \caption{\textcolor{c0}{Swing}, \textcolor{c1}{BodyWeightSquats}, \textcolor{c2}{CuttingInKitchen}, \textcolor{c3}{MixingBatter}, \textcolor{c4}{Shotput}, \textcolor{c5}{Archery}, \textcolor{c6}{BabyCrawling}, \textcolor{c7}{WritingOnBoard}, \textcolor{c8}{PlayingDhol}, \textcolor{c9}{PlayingSitar}}
\end{subfigure}
\vspace{-3mm}
\caption{t-SNE visualization of representations learned using \textbf{Instance Contrastive loss (left)} and  \textbf{TCLR (right)} of  randomly chosen action classes from UCF101 Test set. In case of TCLR, we observe that the class boundaries are more compact and discriminative.}
\label{fig:tsne}
\vspace{-3mm}
\end{figure*}










		
 
		
		
		
		
	



























\section{Model Attention}
\label{sec:attention}

In order to further examine the effect of TCLR pre-training on model performance, we use the method of Zagoruyko and  Komodakis~\cite{attentiontransfer} to generate model attention for our method (from the fourth convolutional block of the R3D model) and compare it with the baseline supervised model. The qualitative samples are presented in Figures~\ref{fig:qual1}, \ref{fig:qual2}, \ref{fig:qual3}, and \ref{fig:qual4}. We observe that the TCLR pre-trained model has significantly better focus on relevant portions of the video.



\begin{figure*}[h!]
    \centering
\begin{overpic}[width=0.49\textwidth]{suppfigs2/attention/52_comparison_BoxingSpeedBag_BoxingSpeedBag_BoxingSpeedBag_.png}
 \put (1,71.5) {\colorbox{white}{\textcolor{blue}{\scriptsize BoxingSpeedBag}}}
 \put (1,46.75) {\colorbox{white}{\textcolor{c2}{\scriptsize BoxingSpeedBag}}}
 \put (1,22) {\colorbox{white}{\textcolor{c2}{\scriptsize BoxingSpeedBag}}}
\end{overpic}
\hfill \begin{overpic}[width=0.49\textwidth]{suppfigs2/attention/1276_comparison_Fencing_Fencing_Fencing_.png}
 \put (1,71.5) {\colorbox{white}{\textcolor{blue}{\scriptsize Fencing}}}
 \put (1,46.75) {\colorbox{white}{\textcolor{c2}{\scriptsize Fencing}}}
 \put (1,22) {\colorbox{white}{\textcolor{c2}{\scriptsize Fencing}}}
\end{overpic}
\hfill \begin{overpic}[width=0.49\textwidth]{suppfigs2/attention/1290_comparison_Skiing_Kayaking_Skiing_.png}
 \put (1,71.5) {\colorbox{white}{\textcolor{blue}{\scriptsize Skiing}}}
 \put (1,46.75) {\colorbox{white}{\textcolor{c3}{\scriptsize Kayaking}}}
 \put (1,22) {\colorbox{white}{\textcolor{c2}{\scriptsize Skiing}}}
\end{overpic}
\hfill \begin{overpic}[width=0.49\textwidth]{suppfigs2/attention/1454_comparison_ApplyLipstick_ApplyLipstick_ApplyLipstick_.png}
 \put (1,71.5) {\colorbox{white}{\textcolor{blue}{\scriptsize ApplyLipstick}}}
 \put (1,46.75) {\colorbox{white}{\textcolor{c2}{\scriptsize ApplyLipstick}}}
 \put (1,22) {\colorbox{white}{\textcolor{c2}{\scriptsize ApplyLipstick}}}
\end{overpic}
\hfill \begin{overpic}[width=0.49\textwidth]{suppfigs2/attention/1622_comparison_Rowing_Skijet_Rowing_.png}
 \put (1,71.5) {\colorbox{white}{\textcolor{blue}{\scriptsize Rowing}}}
 \put (1,46.75) {\colorbox{white}{\textcolor{c3}{\scriptsize Skijet}}}
 \put (1,22) {\colorbox{white}{\textcolor{c2}{\scriptsize Rowing}}}
\end{overpic}
\hfill \begin{overpic}[width=0.49\textwidth]{suppfigs2/attention/1982_comparison_PizzaTossing_PizzaTossing_PizzaTossing_.png}
 \put (1,71.5) {\colorbox{white}{\textcolor{blue}{\scriptsize PizzaTossing}}}
 \put (1,46.75) {\colorbox{white}{\textcolor{c2}{\scriptsize PizzaTossing}}}
 \put (1,22) {\colorbox{white}{\textcolor{c2}{\scriptsize PizzaTossing}}}
\end{overpic}
\caption{\textbf{Model attention:}  Ground-Truth Label on Video frames \textbf{(Top Row)}, Attention for baseline fully supervised model \textbf{(Middle Row)} and Attention for TCLR pre-trained model\textbf{(Bottom Row)}. We notice that for the TCLR pre-trained model the attention is more focused on action-centric regions.}
    \label{fig:qual1}
\end{figure*}



\begin{figure*}[h!]
    \centering
\begin{overpic}[width=0.49\textwidth]{suppfigs2/attention/2052_comparison_Typing_Typing_Typing_.png}
 \put (1,71.5) {\colorbox{white}{\textcolor{blue}{\scriptsize Typing}}}
 \put (1,46.75) {\colorbox{white}{\textcolor{c2}{\scriptsize Typing}}}
 \put (1,22) {\colorbox{white}{\textcolor{c2}{\scriptsize Typing}}}
\end{overpic}
\hfill \begin{overpic}[width=0.49\textwidth]{suppfigs2/attention/2125_comparison_VolleyballSpiking_BasketballDunk_VolleyballSpiking_.png}
 \put (1,71.5) {\colorbox{white}{\textcolor{blue}{\scriptsize VolleyballSpiking}}}
 \put (1,46.75) {\colorbox{white}{\textcolor{c3}{\scriptsize BasketballDunk}}}
 \put (1,22) {\colorbox{white}{\textcolor{c2}{\scriptsize VolleyballSpiking}}}
\end{overpic}
\hfill \begin{overpic}[width=0.49\textwidth]{suppfigs2/attention/2130_comparison_VolleyballSpiking_Basketball_VolleyballSpiking_.png}
 \put (1,71.5) {\colorbox{white}{\textcolor{blue}{\scriptsize VolleyballSpiking}}}
 \put (1,46.75) {\colorbox{white}{\textcolor{c3}{\scriptsize Basketball}}}
 \put (1,22) {\colorbox{white}{\textcolor{c2}{\scriptsize VolleyballSpiking}}}
\end{overpic}
\hfill \begin{overpic}[width=0.49\textwidth]{suppfigs2/attention/2305_comparison_PlayingGuitar_PlayingGuitar_PlayingGuitar_.png}
 \put (1,71.5) {\colorbox{white}{\textcolor{blue}{\scriptsize PlayingGuitar}}}
 \put (1,46.75) {\colorbox{white}{\textcolor{c2}{\scriptsize PlayingGuitar}}}
 \put (1,22) {\colorbox{white}{\textcolor{c2}{\scriptsize PlayingGuitar}}}
\end{overpic}
\hfill \begin{overpic}[width=0.49\textwidth]{suppfigs2/attention/2439_comparison_SoccerJuggling_Shotput_SoccerJuggling_.png}
 \put (1,71.5) {\colorbox{white}{\textcolor{blue}{\scriptsize SoccerJuggling}}}
 \put (1,46.75) {\colorbox{white}{\textcolor{c3}{\scriptsize Shotput}}}
 \put (1,22) {\colorbox{white}{\textcolor{c2}{\scriptsize SoccerJuggling}}}
\end{overpic}
\hfill \begin{overpic}[width=0.49\textwidth]{suppfigs2/attention/2530_comparison_JugglingBalls_JugglingBalls_JugglingBalls_.png}
 \put (1,71.5) {\colorbox{white}{\textcolor{blue}{\scriptsize JugglingBalls}}}
 \put (1,46.75) {\colorbox{white}{\textcolor{c2}{\scriptsize JugglingBalls}}}
 \put (1,22) {\colorbox{white}{\textcolor{c2}{\scriptsize JugglingBalls}}}
\end{overpic}
\caption{\textbf{Model attention (continued):}  Ground-Truth Label on Video frames \textbf{(Top Row)}, Attention for baseline fully supervised model \textbf{(Middle Row)} and Attention for TCLR pre-trained model\textbf{(Bottom Row)}. We notice that for the TCLR pre-trained model the attention is more focused on action-centric regions. }
    \label{fig:qual2}
\end{figure*}


\begin{figure*}[h!]
    \centering
\begin{overpic}[width=0.49\textwidth]{suppfigs2/attention/2662_comparison_PlayingSitar_CuttingInKitchen_PlayingSitar_.png}
 \put (1,71.5) {\colorbox{white}{\textcolor{blue}{\scriptsize PlayingSitar}}}
 \put (1,46.75) {\colorbox{white}{\textcolor{c3}{\scriptsize CuttingInKitchen}}}
 \put (1,22) {\colorbox{white}{\textcolor{c2}{\scriptsize PlayingSitar}}}
\end{overpic}
\hfill \begin{overpic}[width=0.49\textwidth]{suppfigs2/attention/2920_comparison_FrontCrawl_FrontCrawl_FrontCrawl_.png}
 \put (1,71.5) {\colorbox{white}{\textcolor{blue}{\scriptsize FrontCrawl}}}
 \put (1,46.75) {\colorbox{white}{\textcolor{c2}{\scriptsize FrontCrawl}}}
 \put (1,22) {\colorbox{white}{\textcolor{c2}{\scriptsize FrontCrawl}}}
\end{overpic}
\hfill \begin{overpic}[width=0.49\textwidth]{suppfigs2/attention/3024_comparison_HeadMassage_PlayingTabla_HeadMassage_.png}
 \put (1,71.5) {\colorbox{white}{\textcolor{blue}{\scriptsize HeadMassage}}}
 \put (1,46.75) {\colorbox{white}{\textcolor{c3}{\scriptsize PlayingTabla}}}
 \put (1,22) {\colorbox{white}{\textcolor{c2}{\scriptsize HeadMassage}}}
\end{overpic}
\hfill \begin{overpic}[width=0.49\textwidth]{suppfigs2/attention/3089_comparison_Lunges_PizzaTossing_Lunges_.png}
 \put (1,71.5) {\colorbox{white}{\textcolor{blue}{\scriptsize Lunges}}}
 \put (1,46.75) {\colorbox{white}{\textcolor{c3}{\scriptsize PizzaTossing}}}
 \put (1,22) {\colorbox{white}{\textcolor{c2}{\scriptsize Lunges}}}
\end{overpic}
\hfill \begin{overpic}[width=0.49\textwidth]{suppfigs2/attention/3158_comparison_Mixing_CuttingInKitchen_Mixing_.png}
 \put (1,71.5) {\colorbox{white}{\textcolor{blue}{\scriptsize Mixing}}}
 \put (1,46.75) {\colorbox{white}{\textcolor{c3}{\scriptsize CuttingInKitchen}}}
 \put (1,22) {\colorbox{white}{\textcolor{c2}{\scriptsize Mixing}}}
\end{overpic}
\hfill \begin{overpic}[width=0.49\textwidth]{suppfigs2/attention/3308_comparison_Rafting_Rafting_Rafting_.png}
 \put (1,71.5) {\colorbox{white}{\textcolor{blue}{\scriptsize Rafting}}}
 \put (1,46.75) {\colorbox{white}{\textcolor{c2}{\scriptsize Rafting}}}
 \put (1,22) {\colorbox{white}{\textcolor{c2}{\scriptsize Rafting}}}
\end{overpic}
\caption{\textbf{Model attention (continued):}  Ground-Truth Label on Video frames \textbf{(Top Row)}, Attention for baseline fully supervised model \textbf{(Middle Row)} and Attention for TCLR pre-trained model\textbf{(Bottom Row)}.We notice that for the TCLR pre-trained model the attention is more focused on action-centric regions. }
    \label{fig:qual3}
\end{figure*}

\begin{figure*}[h!]
    \centering
\begin{overpic}[width=0.49\textwidth]{suppfigs2/attention/3365_comparison_SumoWrestling_SumoWrestling_SumoWrestling_.png}
 \put (1,71.5) {\colorbox{white}{\textcolor{blue}{\scriptsize SumoWrestling}}}
 \put (1,46.75) {\colorbox{white}{\textcolor{c2}{\scriptsize SumoWrestling}}}
 \put (1,22) {\colorbox{white}{\textcolor{c2}{\scriptsize SumoWrestling}}}
\end{overpic}
\hfill \begin{overpic}[width=0.49\textwidth]{suppfigs2/attention/3413_comparison_TableTennisShot_TableTennisShot_TableTennisShot_.png}
 \put (1,71.5) {\colorbox{white}{\textcolor{blue}{\scriptsize TableTennisShot}}}
 \put (1,46.75) {\colorbox{white}{\textcolor{c2}{\scriptsize TableTennisShot}}}
 \put (1,22) {\colorbox{white}{\textcolor{c2}{\scriptsize TableTennisShot}}}
\end{overpic}
\hfill \begin{overpic}[width=0.49\textwidth]{suppfigs2/attention/3747_comparison_ShavingBeard_ShavingBeard_ShavingBeard_.png}
 \put (1,71.5) {\colorbox{white}{\textcolor{blue}{\scriptsize ShavingBeard}}}
 \put (1,46.75) {\colorbox{white}{\textcolor{c2}{\scriptsize ShavingBeard}}}
 \put (1,22) {\colorbox{white}{\textcolor{c2}{\scriptsize ShavingBeard}}}
\end{overpic}
\hfill \begin{overpic}[width=0.49\textwidth]{suppfigs2/attention/379_comparison_PlayingCello_PlayingCello_PlayingCello_.png}
 \put (1,71.5) {\colorbox{white}{\textcolor{blue}{\scriptsize PlayingCello}}}
 \put (1,46.75) {\colorbox{white}{\textcolor{c2}{\scriptsize PlayingCello}}}
 \put (1,22) {\colorbox{white}{\textcolor{c2}{\scriptsize PlayingCello}}}
\end{overpic}
\hfill \begin{overpic}[width=0.49\textwidth]{suppfigs2/attention/506_comparison_CuttingInKitchen_BabyCrawling_CuttingInKitchen_.png}
 \put (1,71.5) {\colorbox{white}{\textcolor{blue}{\scriptsize CuttingInKitchen}}}
 \put (1,46.75) {\colorbox{white}{\textcolor{c3}{\scriptsize BabyCrawling}}}
 \put (1,22) {\colorbox{white}{\textcolor{c2}{\scriptsize CuttingInKitchen}}}
\end{overpic}
\hfill \begin{overpic}[width=0.49\textwidth]{suppfigs2/attention/1434_comparison_BaseballPitch_GolfSwing_BaseballPitch_.png}
 \put (1,71.5) {\colorbox{white}{\textcolor{blue}{\scriptsize BaseballPitch}}}
 \put (1,46.75) {\colorbox{white}{\textcolor{c3}{\scriptsize GolfSwing}}}
 \put (1,22) {\colorbox{white}{\textcolor{c2}{\scriptsize BaseballPitch}}}
\end{overpic}
\caption{\textbf{Model attention (continued):}  Ground-Truth Label on Video frames \textbf{(Top Row)}, Attention for baseline fully supervised model \textbf{(Middle Row)} and Attention for TCLR pre-trained model\textbf{(Bottom Row)}. We notice that for the TCLR pre-trained model the attention is more focused on action-centric regions. }
    \label{fig:qual4}
\end{figure*}

\clearpage
\clearpage

\section{Detailed comparison with recent prior work}
\label{sec:detailed_comparison}

We contrast our method with 6 recent works in the area: CVRL, TaCo, CoCLR, IIC,  Video DeepInfoMax and SeCo.

\subsection{CVRL~\cite{cvrl}}

\begin{figure}[h]
\begin{center}
 \includegraphics[width=1.0\linewidth]{supp_figs/cvrl.png}
\end{center}
\vspace{-5mm}
  \caption{CVRL Framework from Qian et al.~\cite{cvrl}}
\label{fig:cvrl}
\end{figure}

CVRL introduces the Temporal Interval Sampler, the key idea behind it being to not take positives from uniformly random timestamps from the same instance, rather take positives which close to each other temporally. CVRL improves upon standard instance contrastive learning by trying to avoid learning excessive temporal invariance while TCLR explicitly learns within-instance temporal distinctiveness by taking negatives from the same instance, and hence is an orthogonal approach to the problem of excessive temporal invariance. 

\subsection{TaCo~\cite{taco}}

\begin{figure}[h]
\begin{center}
 \includegraphics[width=1.0\linewidth]{supp_figs/framework_taco.pdf}
\end{center}
\vspace{-5mm}
   \caption{TaCo Framework from Bai et al.~\cite{taco}}
\label{fig:taco}
\end{figure}

TaCo combines instance contrastive learning with multiple temporal pretext tasks (rotation, shuffled, reverse, or speed) in a multi-task setting in order to learn temporally varying features. The different tasks have their own projection and task heads, but share a common backbone. The TaCo approach to learning temporally distinct features is a differnt approach from TCLR, where we utilize temporal contrastive losses instead of any pretext tasks.


\subsection{CoCLR~\cite{cotraining}}

\begin{figure}[h]
\begin{center}
 \includegraphics[width=1.0\linewidth]{supp_figs/coclr.pdf}
\end{center}
\vspace{-5mm}
   \caption{CoCLR Framework from Han et al.~\cite{cotraining}}
\label{fig:coclr}
\end{figure}
CoCLR introduces a multi-stage pretraining process that improves upon instance contrastive learning by mining positive pairs across instances in a cross-modal fashion, i.e. positive pairs mined from RGB modality are used for optical flow, and vice versa. 

\subsection{IIC~\cite{iic}}

\begin{figure}[h]
\begin{center}
 \includegraphics[width=1.0\linewidth]{supp_figs/iic-framework.pdf}
\end{center}
\vspace{-5mm}
   \caption{IIC Framework from Tao et al.~\cite{iic}}
\label{fig:iic}
\end{figure}

IIC framework builds on the contrastive loss by utilizing positive pairs from different modalities (such as optical flow) and creating a new class of negatives by shuffling and repeating the anchor. Unlike TCLR's  loss, IIC does not use temporally distinct clips as negative pairs.





\subsection{VideoDeepInfoMax~\cite{vdim}}

\begin{figure}[h]
\begin{center}
 \includegraphics[width=1.0\linewidth]{supp_figs/vdim.png}
\end{center}
\vspace{-5mm}
   \caption{Video DIM Framework from Devon Hjelm and Bachman~\cite{cotraining}}
\label{fig:vdim}
\end{figure}

VideoDIM learns through \textit{instance discrimination} using both local and global views of the features. \textit{Positive} pairs are formed from local and global features of the \textit{same} instance, whereas \textit{negative} pairs are formed from local and global features of \textit{different} instances.The local features are obtained from lower level convolutional layers, whereas the global feature comes from the final layer. Like other forms of instance discrimination based learning VideoDIM does not achieve temporal distinctiveness, and rather enforces invariance. Whereas,  loss in TCLR promotes distinctiveness in the local features along the temporal dimension.

\subsection{Comparison with SeCo~\cite{seco}}

SeCo combines multiple pretext tasks and its results cannot be compared with VideoSSL methods like TCLR which {\em start from scratch}; SeCo uses a 2D CNN initialized with \textit{ImageNet MoCov2 pre-trained} weights. Moreover, most of the gain of SeCo over ImageNet trained initialization comes from instance contrastive loss and adding intra-frame loss in SeCo only results in gain of 1-2\% across downstream tasks (Table 1 of~\cite{seco}). Whereas with TCLR, gains across downstream tasks upon adding the Local-Local loss are significant (6-11\%, see Table 4 of the main paper). SeCo doesn't include any temporal component (like 3D-Conv or RNN) in the backbone during the SSL phase; it uses 2D CNN and as a result their intra-frame contrastive loss only learns to discriminate \textit{visual appearance} instead of \textit{spatio-temporal} features.









\end{document}
