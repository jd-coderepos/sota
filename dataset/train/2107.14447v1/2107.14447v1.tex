\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{authblk}

\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage[ruled,linesnumbered]{algorithm2e}
\newtheorem{theorem}{theorem}  \newtheorem{definition}{definition}
\makeatletter
\newcommand{\printfnsymbol}[1]{\textsuperscript{\@fnsymbol{#1}}}
\usepackage[breaklinks=true,bookmarks=false]{hyperref}
\usepackage{url}
\iccvfinalcopy 

\def\iccvPaperID{****} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ificcvfinal\pagestyle{empty}\fi

\begin{document}

\title{T-SVDNet: Exploring High-Order Prototypical Correlations for Multi-Source Domain Adaptation}
\author{Ruihuang Li\thanks{Work partly done during an internship at Noah’s Ark Lab}, Xu Jia\thanks{Corresponding Author}, Jianzhong He, Shuaijun Chen, Qinghua Hu\printfnsymbol{2}   \\ \vspace{-0.8em}
	Tianjin University, Dalian University of Technology, Huawei Technologies, Noah’s Ark Lab, Huawei Technologies \\ 
	{\tt\small \{liruihuang, huqinghua\}@tju.edu.cn}, {\tt\small xjia@dlut.edu.cn}, {\tt\small chensj1110@163.com}, {\tt\small jianzhong.he@huawei.com} }




\maketitle
\ificcvfinal\thispagestyle{empty}\fi

\begin{abstract}
	Most existing domain adaptation methods focus on adaptation from only one source domain, however, in practice there are a number of relevant sources that could be leveraged to help improve performance on target domain. We propose a novel approach named T-SVDNet to address the task of Multi-source Domain Adaptation (MDA), which is featured by incorporating Tensor Singular Value Decomposition (T-SVD) into a neural network's training pipeline. Overall, high-order correlations among multiple domains and categories are fully explored so as to better bridge the domain gap. Specifically, we impose Tensor-Low-Rank (TLR) constraint on a tensor obtained by stacking up a group of prototypical similarity matrices, aiming at capturing consistent data structure across different domains. Furthermore, to avoid negative transfer brought by noisy source data, we propose a novel uncertainty-aware weighting strategy to adaptively assign weights to different source domains and samples based on the result of uncertainty estimation. Extensive experiments conducted on public benchmarks demonstrate the superiority of our model in addressing the task of MDA compared to state-of-the-art methods. Code is available at {\small  \url{https://github.com/lslrh/T-SVDNet}}.
\end{abstract}
\section{Introduction}
Deep learning methods have shown superior performance with huge amounts of training data as rocket fuel. However, directly transferring knowledge learned on a certain visual domain to other domains with different distributions would degrade the performance significantly due to the existence of domain shift \cite{yosinski2014transferable}. To handle this problem, the prominent approaches such as transfer learning and unsupervised domain adaptation (UDA) endeavor to extract domain-invariant features. Discrepancy-based methods reduce the domain gap by minimizing the discrepancy between source and target distributions, such as Maximum Mean Discrepancy (MMD) \cite{long2015learning}, correlation alignment \cite{sun2017correlation}, and contrastive domain discrepancy \cite{kang2019contrastive}. Adversarial methods attempt to align source and target domains through adversarial training \cite{saito2018maximum,tzeng2017adversarial} or GAN-based loss \cite{hoffman2018cycada,zhu2017unpaired}. These methods only focus on domain adaptation with only single source. However, in many practical application scenarios, there are a number of relevant sources collected in different ways available, which could be used to help improve performance on target domain. \par
Naively combining various sources into one is not an effective way to fully exploit abundant information within multiple sources, and might even perform worse than single-source methods, because domain gap among multiple sources causes confusion in the learning process \cite{zhao2020}. Some Multi-Source Domain Adaptation (MDA) approaches~\cite{xu2018deep,li2018extracting,zhao2020multi,peng2019moment,guo2018multi} focus on aligning multiple source domains and a target domain by projecting them into a domain-invariant feature space. This is done by either explicitly minimizing the discrepancy of different domains \cite{guo2018multi,hoffman2018algorithms,peng2019moment} or learning an adversarial discriminator to align distributions of different domains \cite{xu2018deep,zhao2018adversarial,li2018extracting}. However, eliminating distribution discrepancy of data has the risk of sacrificing discrimination ability. Moreover, these methods only achieve pair-wise matching, neglecting underlying high-order relations among all domains. Another widely used way in MDA is distribution-weighted combining rule~\cite{hoffman2018algorithms,zhao2018adversarial, li2018extracting}, which takes a weighted combination of pre-trained source classifiers as the classifier for target domain. In spite of reasonable performance on MDA task, they do not take into consideration intra-domain weightings among different training samples, so that underlying noisy source data may hurt the performance of learning in the target, which is referred to as ``negative transfer"~\cite{pan2009survey}.\par
To address the aforementioned limitations, we propose a novel method named T-SVDNet which incorporates tensor singular value decomposition into a neural network's training pipeline. In MDA tasks, although there is large domain gap between different domains, data belonging to the same category do share essential semantic information across domains. Therefore, we assume that data from different domains should follow a certain kind of category-wise structure. Based on this assumption, we explore high-order relationships among multiple domains and categories in order to enforce the alignment of source and target at the prototypical correlation level. Specifically, we impose Tensor-Low-Rank (TLR) constraint on a tensor which is obtained by stacking up a set of prototypical similarity matrices, so that the relationships between categories are enforced to be consistent across domains by pursuing the lowest-rank structure of tensor. Furthermore, to avoid negative transfer \cite{pan2009survey} caused by noisy training data, we propose a novel uncertainty-aware weighting strategy to guide the adaptation process. It could dynamically assign weights to different domains and training samples based on the result of uncertainty estimation. To train the whole framework with both classification loss and low-rank regularizer, we adopt an alternative optimization strategy, that is, optimizing network parameters with the low-rank tensor fixed and optimizing the low-rank tensor with network parameters unchanged. We conduct extensive evaluations on several public benchmark datasets, where a significant improvement over existing MDA methods has been achieved. Overall, the main contributions of this paper can be summarized as follows:\\
 We propose the T-SVDNet to explore high-order relationships among multiple domains and categories from the perspective of tensor, which facilitates both domain-invariance and category-discriminability.\\
 We devise a novel uncertainty-aware weighting strategy to balance different source domains and samples, so that clean data are fully exploited while negative transfer led by noisy data is avoided. \\
 We propose an alternative optimization method to train the deep model along with low-rank regularizer. Extensive evaluations on benchmark datasets demonstrate the superiority of our method. 
\begin{figure*}
	\centering
	\includegraphics[scale=0.28]{overview3.pdf}\\
	\caption{The framework of T-SVDNet. Given  labeled source domains  and an unlabeled target domain , we first extract features for input images and compute prototype  for each category and each domain in an online fashion. Furthermore, the relations between pairwise prototypes are modeled by a group of similarity matrices . Then we stack these prototypical similarity matrices into a 3-order tensor  on which tensor-low-rank constraint is imposed in order to explore high-order relationships among different domains. Finally, together with low-rank regularizer, the model is effectively trained in an alternative optimization strategy. }
	\label{fig1}
	\vspace{-1em}
\end{figure*}




\section{Related Work}
\label{sec2}
\textbf{Single-source Domain Adaptation (SDA).} SDA aims to generalize a model learned from a labeled source domain to a related unlabeled domain with different data distribution. Existing SDA methods usually incorporate two terms: one term is task loss like cross-entropy loss which helps learn a model on the labeled source; the other adaptation term aims to align the distributions of source and target domains. These SDA methods can be roughly categorized into three groups according to the alignment strategies: (1) discrepancy-based methods aim to minimize the discrepancy which is explicitly measured on corresponding layers, including Maximum Mean Discrepancy (MMD) \cite{long2015learning}, correlation alignment \cite{sun2017correlation}, and contrastive domain discrepancy \cite{kang2019contrastive}; (2) Some adversarial-based methods align different data distributions by confusing a well-trained domain discriminator \cite{tzeng2017adversarial,tsai2018learning}. In addition, adversarial generative methods aggregate domains at pixel level by generating adapted fake data \cite{zhu2017unpaired}; (3) Reconstruction-based methods propose to reconstruct the target domain from latent representation by using the source task model \cite{ghifary2016deep}. \par
\textbf{Multi-source Domain Adaptation (MDA).} In practical applications, data may be collected from multiple related domains \cite{bhatt2016multi,sun2015survey}, which involve more abundant information but also bring the difficulty in handling the domain shift. Thus MDA methods become more and more popular. The earlier MDA methods mainly focus on weighted combination of source classifiers \cite{hoffman2018algorithms,li2018extracting,lee2019sliced,saito2018maximum} based on the assumption that target distribution can be approximated by the mixture of source distributions \cite{blitzer2007learning,ben-david2010a}. Hoffman \etal \cite{hoffman2018algorithms} cast distribution combination as a DC-programming and derived a tighter domain generalization bound. Besides classification losses, various domain assignment constraints are devised to reduce the domain gap. In addition to minimizing domain discrepancy between the target and each source domain, Li \etal \cite{li2018extracting} also took into consideration the relationships between pairwise source domains and proposed a tighter bound on the discrepancy among multiple sources. Many explicit measures of discrepancy have been used in MDA methods, such as MMD \cite{guo2018multi},  distance \cite{rakshit2019unsupervised}, and moment distance \cite{peng2019moment}. Some approaches also focus on prototype-based alignment between different domains \cite{pan2019transferrable,xie2018learning,wang2020learning}. As for the adversarial MDA methods which aim to confuse the discriminator so that domain-invariant features are extracted, the optimized objective can be -divergence \cite{zhao2018adversarial}, Wasserstein distance \cite{zhao2020multi,li2018extracting}. \par
\textbf{Uncertainty Estimation.} 
Quantifying and measuring uncertainty is of great theoretical and practical significance~\cite{faber2005on,kiureghian2009aleatory}. In Bayesian modeling, there are two main categories of uncertainty~\cite{kendall2017what}: epistemic uncertainty and aleatoric uncertainty. The former is often referred to as model uncertainty, which captures uncertainty in the model parameters, while the latter accounts for noise inherent from the observations. There have been many methods proposed to estimate uncertainty in deep learning~\cite{blundell2015weight,gal2016dropout,cipolla2018multi}. Resorting to these techniques, the robustness and interpretability of many computer vision tasks are improved, such as object detection \cite{choi2019gaussian,kraus2019uncertainty} and face recognition \cite{chang2020data}.        
\section{Method}
In the MDA setting, there are  labeled source domains  and an unlabeled target domain . Each source domain  contains  observations , where  is the desired label, while in the target domain , the label  is not available. Most existing MDA models can be formulated as the following mapping function:

where  is trained on both labeled samples  in the source domain and unlabeled samples  in the target domain. \par
In this section, we propose the T-SVDNet which fully explores high-order relationships among all domains by exploiting the tensor obtained by stacking up a set of prototypical similarity matrices (see Fig.~\ref{fig1}). In addition, we propose a novel uncertainty-aware weighting strategy to achieve both inter- and intra-domain weightings so that negative transfer is reduced (see Fig.~\ref{fig2}). This section is organized as follows: we first construct prototypical similarity matrix in Sec.~\ref{sec3.1}. Then we propose the tensor-low-rank constraint and uncertainty-aware weighting strategy in Sec.~\ref{sec3.2} and Sec.~\ref{sec3.3}, respectively. Finally, we formulate the total objective function in Sec.~\ref{sec3.4} and propose a novel alternative optimization method in Sec.~\ref{sec3.5}.
\subsection{Prototypical similarity matrix}
\label{sec3.1}
In the proposed T-SVDNet, we first map input image into latent space through a feature extractor denoted by , then we update the centroid of each category (prototype) based on the feature embeddings of a mini-batch \cite{pan2019transferrable,xie2018learning,wang2020learning}. For domain , the prototype of the -th category denoted by  is computed by: 
 
where  is the set of training samples belonging to the -th category in domain , \ie, . It is noteworthy that for unlabeled target domain, we first assign pseudo labels  to samples with high classification confidence. Specifically, we first map each image in target domain  into a classification probability vector , then we set a threshold  for selecting confident predictions as pseudo labels , \ie,

\par
In order to reduce the randomness in sampling of each mini-batch and stabilize the training process, the category prototypes are updated according to exponential moving average (EMA) method:

where  is the exponential decay rate and  denotes current iteration. \par
Then we employ Gaussian kernel to model inter-class relationships and construct a series of prototypical similarity matrices : 
  
where  and  are a pair of category centroids from domain , and  is the deviation parameter which is set as  in experiments.\par 

\subsection{Tensor-low-rank constraint via T-SVD}
\label{sec3.2}
Unlike conventional methods only considering pairwise matching, we achieve high-order alignment of all domains at the prototypical correlation level. Specifically, we stack prototypical similarity matrices into a 3-order tensor  along the third dimension, where  and  denote the number of classes and domains, respectively. Then we impose the Tensor-Low-Rank (TLR) constraint on the assembled tensor in order to explore high-order correlations among domains and enforce the relationships between categories to be consistent across domains. Here we first give definitions of T-SVD and tensor rank as follows:\par
\begin{definition}
	(T-SVD) Given tensor , the tensor singular value decomposition of  is defined as a finite sum of outer product of matrices \cite{martin2013an}:
	
	where  and  are orthogonal tensors with size  and , respectively.  is a tensor with the size , each frontal slice of which is a diagonal matrix.  denotes tensor product (T-product). 
\end{definition}\par
T-SVD also can be computed more efficiently in the Fourier domain. Specifically, it can be replaced by conducting fast Fourier transformation (FFT) along the third dimension of  to get , and performing matrix SVDs on each frontal slice of :

where  means matrix product. We use  to denote the -th frontal slice of , \ie, . The result of T-SVD is finally obtained by taking the inverse FFT on  along the third dimension (see Alg. \ref{alg1}).

\begin{algorithm}[!t]
	\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
	\caption{T-SVD}\label{alg1}
	\Input{\;}
	\Output{, ,  \;}
	\;
	\For{}{\;}
	 \;
\end{algorithm}

\begin{definition}
	(Tensor~rank)~\cite{kilmer2013third,lu2019tensor} The rank of  is a vector  with the -th element equal to the rank of the -th frontal slice of . 
\end{definition}

However, we need an adequate convex relaxation to  norm of tensor rank in optimization process. To this end we formulate it as tensor nuclear norm, which is defined as the sum of the singular values of all frontal slices :

\textbf{Tensor rotation.} In view of each frontal slice  of tensor  only contains information from single domain, we rotate it horizontally (or vertically) to obtain  (see Fig. \ref{fig1}). In this way, each frontal slice  will involve information from different domains. In the end, imposing tensor-low-rank constraint on the rotated tensor  benefits the exploration of high-order relationships among different domains. The inter-category data structure is enforced to be consistent across domains by pursuing the lowest rank of each frontal slice  in Fourier domain.
\begin{figure}
	\centering
	\includegraphics[scale=0.6]{framework.pdf}\\
	\caption{Uncertainty-aware weighting strategy. Each sample is modeled as a Gaussian distribution parameterized by mean  and variance . The classification loss is weighted by estimated data uncertainty .}
	\label{fig2}
\vspace{-1em}
\end{figure}
\subsection{Uncertainty-aware weighting strategy}
\label{sec3.3}
Instead of equally treating each source domain and sample, we propose a novel uncertainty-aware weighting strategy to adaptively balance different sources and alleviate negative transfer led by noisy data. Considering that data uncertainty could capture the noise inherent in the data, \ie, it reflects the reliability of output \cite{kendall2017what,chang2020data}, we could weigh different sources and samples based on the result of uncertainty estimation. As shown in Fig.~\ref{fig2},  and  serve on a classifier and uncertainty predictor, respectively. The output of network is modeled as a Gaussian distribution parameterized by mean  and variance . Specifically, the mean is acted by original feature vector, while the variance quantifies uncertainty of training samples. For regression tasks, the Gaussian likelihood is defined as:

with  and . For classification task, we often squash the model output through a softmax function and obtain a scaled classification likelihood:

This can be interpreted as a Boltzmann distribution (Gibbs distribution) and  works as temperature for re-scaling input. The log likelihood of output is: 
{\small }where  denotes the -th element of vector . Then the total classification loss is defined as:
{\small }where  denotes classification cross entropy loss with  not scaled.  and  denote the number of domains and samples, respectively.  prevents  from getting too large. Noisy data with large uncertainty would be assigned less weights, \ie, . The derivation process of Eq.~\ref{eq13} is provided in supplementary material.\\

\subsection{Objective function}
\label{sec3.4}
The overall objective function of the proposed model is as follows:

where  denotes two operations: tensor rotation and tensor nuclear norm, and  represents the operation of stacking up all the domain-specific prototypical similarity matrices into a tensor.  denotes neural network parameters.
The first term is classification loss and the second term imposes TLR constraint on the stacked tensor, aiming at achieving high-order alignment of domains. \par
{\tiny \begin{algorithm}[!t]
	\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
	\caption{Optimization of T-SVDNet}\label{alg2}
	\Input{Training data  \;}
	\Output{Model parameters  of T-SVDNet \;}
	\For{ to }
	{	 \textbf{Updating} \\
		{ }\\
		 \\
		 \textbf{Updating} \\
		 \\
		\;
		\For{}{\;
				\;}
		 \\
		 \;
		
	}
\end{algorithm}}
\subsection{Optimization of T-SVDNet}
\label{sec3.5}
The optimization of T-SVDNet is presented in Alg.~\ref{alg2}. In order to make the problem tractable, we introduce an auxiliary variable and alternatively update it along with the network parameters till convergence.\par
\textbf{Auxiliary variable.} To optimize the objective function in Eq.~\ref{eq10}, we first introduce an auxiliary tensor  to replace , which converts the original optimization problem into the following one:  

where  is a penalty parameter. It starts from a small initial positive scalar , and gradually increases to the maximum truncated value  with iterations, \ie, it is updated by , where  represents the rate of increase which is set as  in all experiments. The reason why we update  in such a incremental fashion is that randomly initialized  may lead to the wrong direction of gradient descent at the beginning of training process.  \par 
\textbf{Update of network parameters .} The parameters of feature extractor , classifier , and uncertainty predictor  are updated through gradient descent with auxiliary tensor  fixed. \par
\textbf{Update of auxiliary variable .} When network parameters are fixed, we optimize the subproblem associated with  as follows:
  
We solve this problem in Fourier domain with basic procedure similar to Alg.~\ref{alg1}. We first transform tensor  to Fourier domain , and perform matrix SVD on each -th frontal slice  and obtain the , , . Then, each frontal slice  of auxiliary variable can be updated by \textit{shrinkage operation} \cite{cai2010a,kolda2009tensor} on  in the Fourier domain defined as follows:  
\begin{table*}[h]
	\begin{center}
		\scalebox{0.7}{
			\begin{tabular}{lccccccc}
				\toprule[1.5pt]
				\multicolumn{1}{c}{Standards}   & \multicolumn{1}{c}{Methods}           & \multicolumn{1}{c}{ mm}         & \multicolumn{1}{c}{ mt}         & \multicolumn{1}{c}{ up}         & \multicolumn{1}{c}{ sv}         & \multicolumn{1}{c}{ syn}        & \multicolumn{1}{c}{Avg}        \\ \toprule[1.5pt]
				\multicolumn{1}{c}{\multirow{5}{*}{Single Best}} & Source-Only       &            &            &            &            &            &            \\
				\multicolumn{1}{c}{}                             & DAN               &  &  &  &  &  &  \\
				\multicolumn{1}{c}{}                             & DANN              &  &  &  &  &  &  \\
				\multicolumn{1}{c}{}                             & CORAL             &  &  &  &  &  &  \\
				\multicolumn{1}{c}{}                             & ADDA              &  &  &  &  &  &  \\ \hline
				\multirow{6}{*}{\begin{tabular}[c]{@{}c@{}}Source \\ Combination\end{tabular}}                  & Source-Only       &  &  &  &  &  &       \\
				& DAN               &  &  &  &  &  &       \\
				& DANN              &  &  &  &  &  &       \\
				& ADDA              &  &  &  &  &  &       \\
				& MCD               &  &  &  &  &  &       \\
				& JAN               &  &  &  &  &  &      \\ \hline 
				\multicolumn{1}{c}{\multirow{6}{*}{Multi-Source}}                    & MDAN              &  &  &  &  &  &       \\
				& MDDA              & {\color{green}} & {\color{green}} &  &  & {\color{green}} & {\color{green}}     \\
				& DCTN              &  &  &  &  &  &       \\
				& MSDA             &  &  & {\color{green}} & {\color{green}} &  &       \\
& T-SVDNet & {\color{blue}}      & {\color{blue}}      & {\color{blue}}      & {\color{blue}}      & {\color{blue}}      & {\color{blue}}      \\
				& T-SVDNet          & {\color{blue}}     & {\color{blue}}      & {\color{blue}}            & {\color{blue}}      & {\color{blue}}      & {\color{blue}}           \\
				\bottomrule[1.5pt]
			\end{tabular}
		}	
	\end{center}
	\caption{{\small{Classification results on Digits-Five. The top value is highlighted in {\color{blue}{blue}} bold font and the second best in {\color{green} green} bold font.}}}
	\label{tab1}
\end{table*}

where {\small{}} is singular value shrinkage operator. {\small{}} is a diagonal matrix with the -th diagonal element to be {\small}. Finally, updated  is obtained by inverse fast Fourier transform from .

\section{Experiments}
In this section, we perform extensive evaluations on several benchmark datasets with state-of-the-art methods. 
\subsection{Datasets}
\textbf{Digits-Five}~\cite{hull1994database} contains 5 different domains including MNIST (mt), MNIST-M (mm), SVHN (sv), USPS (up), and Synthetic Digits (syn). Each domain consists of 10 numerals from `0' to `9'. Previous methods only use a subset of samples in each domain, \ie, 25000 training data and 9000 testing data. But we find that there will be further performance gain if all data are employed for training. For a fair comparison, we report the results on both settings (T-SVDNet and T-SVDNet in Tab.~\ref{tab1}).\par
\textbf{PACS}~\cite{li2017deeper} is a small-scale multi-domain dataset containing 9991 images from 4 domains: photo~(P), art-painting~(A), cartoon~(C), sketch~(S) whose styles are different. These domains share the same seven categories.\par
\textbf{DomainNet}~\cite{peng2019moment} is a large-scale dataset for Multi-Source Domain Adaptation. Due to the great number of categories and samples (345 categories, around 0.6 million images) and large domain shift. DomainNet is by far the most difficult dataset which contains 6 different domains: clipart (clp), infograph (inf), painting (pnt), quickdraw (qdr), real (rel), and sketch (skt).
\subsection{Compared methods}
For all experiments, we compare our method with state-of-the-art single-source and multi-source domain adaptation algorithms. Specifically, two strategies are adopted to train the single-source model: Single Best and Source Combination. The former reports the best result among all domains, while the latter simply combines all source domains together. Overall, these compared methods can be roughly categorized into two main groups: (1) adversarial-based methods include Domain Adversarial Neural Network (DANN) \cite{ganin2016domain}, Adversarial Discriminative Domain Adaptation (ADDA) \cite{tzeng2017adversarial}, Maximum Classifier Discrepancy (MCD) \cite{saito2018maximum}, Deep Cocktail Network (DCTN) \cite{xu2018deep}, Adversarial Multiple Source Domain Adaptation (MDAN) \cite{zhao2018adversarial} and Multi-Source Distilling Domain Adaptation (MDDA) \cite{zhao2020multi}; (2) another typical strategy is discrepancy minimization, the representative methods involve Deep Adaptation Network (DAN) \cite{long2015learning}, Joint Adaptation Network (JAN) \cite{long2017deep}, Residual Transfer Network (RTN) \cite{long2016unsupervised}, Correlation Alignment (CORAL) \cite{sun2017correlation}, and Moment Matching for Multi-Source Domain Adaptation (MSDA) \cite{peng2019moment}. Source-Only directly transfers the model trained in source domain to target domain. For a fair comparison, we use the same model architecture and data pre-processing routines as compared methods in all experiments. More implementation details are provided in supplementary materials.
\begin{table}[]
	\begin{center}
		\scalebox{0.7}{
			\begin{tabular}{cccccc}
				\toprule[1.5pt]
				\multicolumn{1}{c}{Methods} & \multicolumn{1}{l}{ A} & \multicolumn{1}{c}{ C} & \multicolumn{1}{c}{ S} & \multicolumn{1}{c}{ P} & \multicolumn{1}{c}{Avg} \\ \toprule[1.5pt]
				Source-Only  				& 					&
				&					&					&	\\
				MDAN                        &                   &                  &                   &                    &  \\
				MDDA                        & {\color{green} }                 &                   & {\color{green}}                  &   &                  {\color{green}}\\
				DCTN                        & {}                  & {\color{green}}                  &                   & {\color{green}}        &            \\
				MSDA                       &                   &                   &                   &                     & \\
				T-SVDNet                    & {\color{blue}}                  & {\color{blue}}                  & {\color{blue}}                  & {\color{blue}}                  &
				{\color{blue}} \\\bottomrule[1.5pt]
\vspace{-1em}		\end{tabular}}
	\end{center}
	\caption{{\small{Classification results on PACS. The top value is highlighted in {\color{blue}{blue}} bold font and the second best in {\color{green} green} bold font.}}}
	\label{tab2}
\end{table}
\begin{table*}[]
	\begin{center}
		\scalebox{0.7}{
			\begin{tabular}{ccccccccc}
				\toprule[1.5pt]
				Standards                       & \multicolumn{1}{c}{Methods} & clp      & inf      & pnt      & qdr      & rel      & skt      & Avg  \\ \toprule[1.5pt]
				\multirow{7}{*}{\begin{tabular}[c]{@{}c@{}}Single- \\ Best\end{tabular}}    & Source-Only                 &  &   &  &  &  &      &  \\
				& DAN                         &  &  &  & {\color{green}} &  &  &  \\
				& RTN                         &  &  &  &  &  &  &  \\
				& JAN                         &  &   &  &  &  &  &  \\
				& ADDA                        &  &  &  &  &  &  &  \\
				& DANN                        &  &  &  &  &  &  &  \\
				& MCD                         &  &  &  &   &  &  &  \\ \hline
				\multirow{6}{*}{\begin{tabular}[c]{@{}c@{}}Source \\ Combination\end{tabular}} & Source-Only                 &  &  &  &  &  &  &  \\
				& DAN                         &  &  &  &  &  &  &  \\
				& RTN                         &  &  &  &  &  &  &  \\
				& ADDA                        &  &  &  &  &  &  &  \\
				& JAN                         &  &  &  &  &  &  &  \\
				& MCD                         &  &  &  &   &  &  &  \\ \hline
				\multirow{6}{*}{\begin{tabular}[c]{@{}c@{}}Multi- \\ Source\end{tabular}}   & MDAN                        &  &  &  &   &  &  &  \\
				& MDDA                        & {\color{green}} &  & {\color{green}} &  &  &  & {\color{green}} \\
				& DCTN                        &  &  &  &   &  &  &  \\
				& MSDA                       &  & {\color{blue}} &  &   & {\color{green}} & {\color{green}} &  \\
& T-SVDNet                    & {\color{blue}} & {\color{green}} & {\color{blue}} & {\color{blue}} & {\color{blue}} & {\color{blue}} & {\color{blue}} \\ \bottomrule[1.5pt]
		\end{tabular}}
	\end{center}
	\caption{\small{Classification results on DomainNet. The top value is highlighted in {\color{blue}{blue}} bold font and the second best in {\color{green}green} bold font.}}
	\label{tab3}
\end{table*}

\begin{table*}[!htbp]
	\centering                                                                      \scalebox{0.7}{
		\begin{tabular}{lccccccc}
			\toprule[1.5pt]
			\multicolumn{1}{c}{Methods}            & \multicolumn{1}{c}{ mm}         & \multicolumn{1}{c}{ mt}         & \multicolumn{1}{c}{ up}         & \multicolumn{1}{c}{ sv}         & \multicolumn{1}{c}{ syn}        & \multicolumn{1}{c}{Avg}    & \multicolumn{1}{c}{Gain}    \\ \toprule[1.5pt]
			\multicolumn{1}{c}{Source-Only}       &            &            &            &            &            &         &     \\
			\multicolumn{1}{c}{T-SVDNet~(+E)}       &  &  &  &  &  &  & \\
			\multicolumn{1}{c}{T-SVDNet~(+E+T)}                       &  &  &  &  &  &  &  \\
			\multicolumn{1}{c}{T-SVDNet~(+E+T+U)}                       &  {}     & {}      & {}            & {}      & {}      & {}  &  
\\ \bottomrule[1.5pt]
	\end{tabular}}
	\caption{{\small{Ablation study on key components of model on Digits-Five.}}} 
	\label{tab4}
	\vspace{-10pt}
\end{table*}
\subsection{Experimental results}
The results on \textbf{Digits-Five} are shown in Tab.~\ref{tab1}. Overall, our method tops the list in all domains and achieves 93.37\% average accuracy, around 5.25\% higher than the second best method MDDA. In particular, a performance improvement about 11.42\% and 5.21\% over MDDA is achieved on `' and `' tasks, respectively. The performance will be further boosted to 93.94\% if all training data is used, outperforming other algorithms by a large margin.  \par
The results on \textbf{PACS} are shown in Tab.~\ref{tab2}. Our method T-SVDNet achieves the best performance on all domains and gets 91.25\% average accuracy, outperforming the second best method MDDA by 5.14\%. Especially on `' task, our method achieves a 7.93\% performance gain over MDDA. \par
The experimental results on \textbf{DomainNet} are reported in Tab.~\ref{tab3}. Overall, T-SVDNet achieves the best performance on five out of six tasks. It obtains average accuracy of 47.0\% on six domains and ranks the first in the list, with 3.8\% performance improvement over MDDA, which is mainly attributed to the thorough exploration of high-order relations between different domains and categories. It is noteworthy that the performances of many MDA methods drop obviously compared to Single Best on `' task due to negative transfer, while our method still attains better performance because of uncertainty-aware weighting strategy. Negative transfer is avoided by filtering out noisy source samples near decision boundaries for training, while clean data with low noise intensity are fully exploited. \par






\begin{figure}
	\centering
	\includegraphics[scale=0.21]{TNN.jpg}\\
	\caption{Tensor nuclear norm and classification accuracy curves on `` mm'' task. }
	\label{fig6}
	\vspace{-1em}
\end{figure}
\section{Analysis}
\textbf{Ablation study.} We further validate the effects of some key components in our framework. Tab.~\ref{tab4} shows the results of controlled experiments on Digits-Five dataset. As a reference, we report the performance of Source-Only that directly transfers the model trained on source domains to target domain. For convenience, `+E', `+T', `+U' denote entropy minimization constraint on target domain, tensor-low-rank constraint, and uncertainty-aware weighting, respectively. In general, we have the following observations according to Tab.~\ref{tab4}: (1) Entropy minimization boosts performance obviously due to the exploitation of target domain; (2) It is remarkable that Tensor-Low-Rank constraint significantly improves the performance by 14.91\% on ` mm' task. This is attributed to the high-order alignment between different domains and the extraction of domain-invariant features; (3) Uncertainty-aware weighting strategy further improves the performance by 1.26\% on average, which suggests that our model is able to learn more transferable features across domains.\par

\textbf{The effect of TLR constraint.} We compute tensor nuclear norm (TNN) which is usually used as an approximate measure of tensor rank. As shown in Fig.~\ref{fig6}, we compare the TNN curves w/ and w/o TLR constraint. We find that TNN w/ TLR drops significantly during the first several epochs and stabilizes at around 46, while the baseline w/o TLR drops slowly and becomes stable earlier. This demonstrates that our proposed TLR constraint is effective and brings large performance improvement. 
\begin{figure}
	\centering
	\includegraphics[scale=0.15]{TSNE.pdf}\\
	\caption{The t-SNE visualizations of feature embeddings on `C' task on PACS. The top row represents category information (each color denotes a class). The bottom row represents domain information (red: source domain; purple: target domain).}
	\label{fig3}
	\vspace{-1em}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[scale=0.31]{similarity1.pdf}\\
	\caption{Visualizations of similarity matrices on `mm' task on Digits-Five. The top row denotes the model w/o TLR constraint, and the bottom row is T-SVDNet. Blue and green represent source and target domain, respectively.}
	\label{fig4}
	\vspace{-1em}
\end{figure}
\textbf{Feature visualization.} 
To demonstrate the transfer ability of our model, we visualize the feature embeddings of different models on `' task on PACS. As shown in Fig.~\ref{fig3}~(a), the target features learned by Source-Only almost mismatch with source domain and different classes in target domain are entirely mixed up. Compared to M3SDA and Source-Only, our method produces clusters with clearer boundaries, which suggests that T-SVDNet possesses better transfer ability on target and is able to eliminate domain discrepancy without sacrificing discrimination ability.\par
\textbf{Visualizations of similarity matrices.} 
To further validate the effect of TLR constraint, we visualize the prototypical similarity matrices of three domains on Digits-Five dataset in Fig. \ref{fig4}. Compared to the baseline without TLR constraint (the top row), our method (the bottom row) could capture clearer category-wise data structure. Specifically, matrices in the bottom row contain less domain-specific noise, because we search for a lowest-rank structure of tensor and enforce prototypical correlations to be consistent across domains. Especially on the target domain (MNIST-M), the noise is reduced by a large margin compared to Source-Only. These results indicate the effectiveness of TLR constraint on aligning source and target domains.\par
\textbf{Uncertainty estimation.} We conduct qualitative and quantitative experiments to demonstrate the ability of model to measure noise intensity (data uncertainty).\par
\textit{(1)~Inter-domain weighting.}  The uncertainty distributions of different domains on `mm' task are shown in Fig.~\ref{fig5}~(a). Overall, the estimated uncertainty is highly correlated with domain quality. \eg, the uncertainty distribution of high-quality domain MNIST (blue curve) is more concentrated than low-quality domain SVHN (green curve). This validates that our model is able to measure the quality of domain, and guide the combination of data distributions.\par
\textit{(2)~Intra-domain weighting.} To demonstrate the ability of our model to capture noise inherent in data, we add different proportions of Gaussian noise to images and plot the estimated uncertainty distributions in Fig.~\ref{fig5}~(b). Specifically, we add noise sampled from Gaussian distribution  to original images, \ie, , where  denotes noise and  controls the intensity of noise. According to Fig.~\ref{fig5}~(b), when noise intensity is small (), the curves of noisy and clean samples () are highly overlapped. However, with the increase of noise intensity (), the uncertainty distributions get more dispersed. This demonstrates that our model could accurately evaluate intra-domain data quality, so that noisy samples would be assigned less weights and negative transfer would be avoided.
\begin{figure}
	\centering
	\includegraphics[scale=0.3]{uncertainty.pdf}\\
	\caption{(a)~Uncertainty distributions of different domains on Digits-Five. (b)~Uncertainty distribution of single domain varies with the increasing noise intensity  on MNIST.}
	\label{fig5}
	\vspace{-1em}
\end{figure}
\section{Conclusion}
In this paper, we propose the T-SVDNet for multi-source domain adaptation, which is featured by incorporating tensor singular value decomposition into neural network training process. Category-wise relations are modeled by prototypical similarity matrix, aiming at capturing complex data structure. Furthermore, high-order relations between different domains are fully explored by imposing tensor-low-rank constraint on the tensor stacked by domain-specific similarity matrices. In addition, a novel uncertainty-aware weighting strategy is proposed to combine data distributions of different domains, which reduces negative transfer led by noisy data. We adopt alternative optimization algorithm to train T-SVDNet efficiently. Extensive experiments on three public benchmark datasets demonstrate the favorable performance against state-of-the-art methods. 

{\small
	\bibliographystyle{ieee_fullname}
	\bibliography{egbib}
}
\section{Supplementary}
\subsection{Relevant Definitions}
{\definition (Tensor product) The tensor product  of  and , \ie, , is a tensor of size , each -th tube of which denoted by  with  and  is given by:}

where  denotes the circular convolution between two vectors. Tensor product in the original domain can be replaced by matrix multiplication of frontal slices in the Fourier domains as follows:

{\definition (Tensor transpose) For , the transpose of  denoted by  can be obtained by transposing each frontal slice of  and reversing the order of the transposed slices along the third dimension.}
{\definition (Identity tensor) The identity tensor  is a tensor whose first frontal slice is the  identity matrix and all other frontal slices are zeros.}
{\definition (Orthogonal tensor) A tensor  is orthogonal if 

where  is the tensor product.}
\begin{table*}[t]
	\begin{tabular}{clccccccccc}
		\hline
		\multicolumn{2}{c}{dataset}                    & domains            & classes              & input size               & backbone                    & batch size          & learning rate              &                 &                    & feature dimension     \\ \hline
		\multicolumn{2}{c}{Digits-Five}                & 5                  & 10                   & 32*32                    & 3Conv-2FC                   & 128                 & 5e-4                       & 1000                  & 1                    & 2048                  \\
		\multicolumn{2}{c}{\multirow{2}{*}{PACS}}      & \multirow{2}{*}{4} & \multirow{2}{*}{7}   & \multirow{2}{*}{224*224} & \multirow{2}{*}{ResNet-18}  & \multirow{2}{*}{16} & E:3e-5                     & \multirow{2}{*}{1000} & \multirow{2}{*}{0.1} & \multirow{2}{*}{512}  \\
		\multicolumn{2}{c}{}                           &                    &                      &                          &                             &                     & C:1e-3                     &                       &                      &                       \\
		\multicolumn{2}{c}{\multirow{2}{*}{DomainNet}} & \multirow{2}{*}{6} & \multirow{2}{*}{345} & \multirow{2}{*}{224*224} & \multirow{2}{*}{ResNet-101} & \multirow{2}{*}{16} & E:5e-5                     & \multirow{2}{*}{100}  & \multirow{2}{*}{1}   & \multirow{2}{*}{2048} \\
		\multicolumn{2}{c}{}                           &                    &                      &                          &                             &                     & {C:5e-4} &                       &                      &                       \\ \hline	
	\end{tabular}
	\caption{The experimental setups on different datasets. E and C denote feature extractor and classifier, respectively.}
	\label{tab5}
\end{table*}
{\definition (f-diagonal tensor) The f-diagonal tensor is a tensor each frontal slice of which is diagonal matrix. The tensor product of two f-diagonal tensors with the same size  is also a tensor with the same size, each -th  diagonal tube of which is:

}
\subsection{Derivation of Eq.~\ref{eq13}}
First, the cross entropy loss  is denoted as:

then we obtain the following derivation process pf Eq.~\ref{eq13}:

here we introduce a simplifying assumption in the last transition:
{\small }
which becomes equality when . This assumption simplifies the optimization objective, while the performance is improved empirically. 

\subsection{Implementation Details}
Overall, for fair comparisons, we use the same model architecture and data pre-processing routines as compared methods in all experiments. Specifically, we present the detailed parameter settings on three datasets in Tab.~\ref{tab5}. Parameters  and  are set as  and  in all experiments.


\end{document}
