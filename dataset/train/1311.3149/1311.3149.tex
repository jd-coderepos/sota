\documentclass{article}
\usepackage{epsfig, subfigure}
\usepackage{color}
\usepackage{amssymb,amsmath}
\usepackage{graphicx}
\usepackage{a4wide}

\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{observation}{Observation}
\newtheorem{fact}{Fact}

\def\Pr{\mathop{\rm Pr}}
\def\area{\mathop{\rm area}}
\def\bd{\mathop{\rm bd}}
\def\diam{\mathop{\rm diam}}
\def\peri{\mathop{\rm peri}}
\def\components{\mathop{\rm components}}

\newenvironment{proof}{\noindent\emph{Proof}.\hspace{1ex}}{\hfill\unitlength=0.18ex\begin{picture}(12,12)
    \put(1,1){\framebox(9,9){}}
    \put(1,4){\framebox(6,6){}}
  \end{picture}\linebreak
}

\pagestyle{plain}
\sloppy

\begin{document}

\title{
Local Event Boundary Detection with Unreliable Sensors:
Analysis of the Majority Vote Scheme\thanks{Work by C.-S. Shin was supported by National Research Foundation of Korea(NRF) grant funded by the Korea government(MEST) (No. 2011-0002827).}}


\author{
	Peter~Brass\thanks{Dept. of Computer Science, City College, New York, USA \tt{peter@cs.ccny.cuny.edu}} \and 
	Hyeon-Suk Na\thanks{School of Computing, Soongsil University, Seoul, Korea. \tt{hsnaa@ssu.ac.kr}} 
	\and Chan-Su Shin\thanks{Dept. of Electrical Information Engineering, Hankuk University of Foreign Studies, Korea. \tt{cssin@hufs.ac.kr}}
}
\date{}
\maketitle

\begin{abstract}
In this paper we study the identification of an event region  within a
larger region , in which the sensors are distributed by a Poisson process
of density  to detect this event region, i.e., its boundary. The model of sensor is a 0-1
sensor that decides whether it lies in  or not, and which might be
incorrect with probability . It also collects information on the
0-1 values of the neighbors within some distance  and revises its
decision by the majority vote of these neighbors. In the most general setting, we analyze this simple majority vote scheme
and derive some upper and lower bounds on the expected number of misclassified
sensors. These bounds depend on several sensing parameters of , , and some geometric parameters of the event region . By making some assumptions on the shape of , we prove a significantly improved upper bound on the expected number of misclassified sensors; especially for convex regions with sufficiently round boundary, and we find that the majority vote scheme performs well in the simulation rather than its theoretical upper bound.
\end{abstract}




\section{Introduction}
Suppose we have distributed many sensors in a region, each of which detects
if it rains at that point. We want to obtain a summary: in which sub-region
is it raining? Just listing all the positions at which a raindrop has been
detected is not a helpful answer, first because a long list of positions
is not the answer a user would want on the question ``Where does it rain?,''
but also because each individual answer is subject to random
errors; the detected drop of water could have come from an air-conditioner,
or from children splashing in the water, or numerous other random events,
and in the same way, even if it is raining, the sensor might coincidentally
not catch any drop. We expect a useful answer to the question ``Where does it rain?'' to be some region with a simple structure.

\par
The abstract model underlying this question is as follows: we have a
region , in which there is a set  of sensors. There is an unknown
region  in which the event happens. Possible events would
be rain, forest fires, or occurrences of invasive species. We want to detect the event region , most importantly, its boundary, where it is far from the boundary of  and has a `nice' topology, not being highly irregular, random or fractal. The sensors  are 0-1 sensors who decide whether  or , making an error with probability  in this measurement, called the measurement error.

\subsection{Simple Majority Voting Scheme}
Our aim is to reduce the error rate in detecting the boundary of 
by allowing each sensor to compare its result with those of its neighbors. To reduce the error rate by local communication, we assume that each sensor knows the values measured by all neighboring sensors within distance . The most
straightforward method to use the neighbors' sensing information is
to follow the majority.

This majority vote scheme is as follows: if the sensor has  neighbors and knows its own and those  other measurements, then in its revised decision it just follows the majority of the measurements of its  neighbors, with itself as tie-breaking if necessary.
This scheme was already proposed by Chintalapudi and Govindan~\cite{CG03}
and further in~\cite{KI04}. It does not use the position information of
the neighboring sensors. It was stated in~\cite{KI04} that this scheme
gives a good correction of measurement errors for sensor error  up
to . However, we think this observation needs further qualification,
since the situation really depends on the size of voting neighborhood
and several geometric parameters of the boundary of .

\begin{figure}
\centering
\includegraphics[width=11cm]{fig1.eps}
\caption{ sensors are distributed uniformly in a unit square , and the gray-colored region is the event region . Among  sensors,  sensors (marked by black boxes in left figure) made wrong initial measurements, but  sensors,  of misclassified ones, revised correctly (marked by gray discs in the right figure), and the other  sensors (marked by black boxes) remained still in their wrong classification, but  sensors (marked by black discs) turned from the correct classification to the wrong classification.}
\label{fig:intro}
\end{figure}

\par
Figure~\ref{fig:intro} shows a simulation on the majority vote scheme
with  sensors distributed uniformly in a unit square . The
event region  is a square of side length  with rounded corners of the curvature radius . The measurement error probability  and the neighborhood radius . The error correction rate by this scheme is about .
From this simulation, one can observe that the total error of event
detection is significantly reduced by the majority vote scheme. In fact,
the error depends on several sensing parameters such as the measurement
error  and the neighborhood radius , and some geometric parameters
of the event region  such as the convexity, the perimeter and the
boundary curvature.

In this paper, we analyze the majority scheme
further, and make explicit and precise the dependency on these parameters. To analyze this, we first need an assumption on the distribution of sensors in the region of interest . We adapt the most important model; the sensors are randomly distributed by a Poisson process of density , which is independent from the measurement error  that the sensors make. To our best knowledge, this gives the first bounds on the expected number of incorrectly classified sensors in the majority vote scheme with all these parameters, , , , and the shape of .

\par
It should be noticed that our sensors are point sensors; there is
no sensing range, but a yes/no decision about the situation at the sensor.
Many other papers have dealt with continuous-valued sensors, but then the dependence of a sensor's decision on the neighboring values is much less clear. Also, for many practical applications a yes/no decision is ultimately the
desired answer: `does it rain?', `is there a forest fire?', etc.
The distance  within which we compare the sensor values is
not related to the communication distance of the network nodes;
it is a choice made depending on our a-priori knowledge on the
size and shape of , that is, choosing the right radius  is an important aspect.
Finally, our majority vote scheme does not need absolute positions
of the sensors, but it is not efficient in detecting thin and long event regions; to identify such event regions we would need sensors with
known positions with the help of GPS units and by more complicated decision algorithms.

\subsection{Previous Work}
Local event boundary detection problem has been studied in several previous papers.

Chintalapudi and Govindan~\cite{CG03} were the first to analyze the local
event boundary detection problem. They proposed three different types of
algorithms; among them a simple neighborhood counting scheme that does
not use the position information of the neighboring sensors, and a scheme
that finds the optimum line separating in the neighborhood the event-sensors
from the no-event-sensors.
They found by simulation that the separating-line scheme performs best,
but provided no analysis of that scheme, and assumed for the other schemes
that the event boundary is a straight line.
\par
Krishnamachari and Iyengar~\cite{KI04} discussed a model with a similar
counting scheme, not using the neighbor's position information; in their
simulation, however, the sensors are always distributed in a square grid.
Wu et al.~\cite {WCDXLD07} discussed continuous-valued sensors, looking
for threshold events, and proposed several methods
based on comparing a sensor's value with the median of a set of neighbor's
values to identify faulty or boundary sensors. Similar was the discussion
by Jin and Nittel~\cite{JN06}, who used the mean instead of the median.
Ding and Cheng~\cite{DC09} fit a mixture of multivariate gaussian
distributions to the observed sensor values and decided on the base
of that fitted model which sensors are boundary sensors.

\par
Wang et al.~\cite{WGM06} considered a model that can be interpreted as
only no-event sensors being available, e.g., because the event like the fire in the forest destroyed
all sensors in its region; they reconstruct a boundary of the regione
containing sensors, based on neighbor connectivity information without
the neighbor positions. Nowak and Mitra~\cite {NM03} use a non-local
communication model based on a hierarchical partitioning scheme to
identify event boundaries.
\par
A different line of related works contains the fusion of different
information sources for the same event; e.g., combining
the output of multiple classifiers in a pattern-recognition problem.
This has been studied in \cite{LaS97,KiH*98,AlK99,CC01,Ku02,Na03},
but that problem abstracts from the geometric structure which is the
core of our considerations.
\par
Yet another related line of works contains the opinion formation
in social networks: instead of spatially related nodes, we have
persons with friends, and a person might change his opinion
to conform to the majority opinion of his group. This has been
studied both as social dynamics and as abstract process on graphs,
see, e.g., \cite{Ag*88,MuP01,Pe02,Kr02,MuP04,Zo10}.
Majority vote among neighbors has also been studied as
model for some spin systems in physics, see e.g.,
\cite{Ol92,Li*05,Ya*08,Cu*10}.




\subsection{Our Results}

For a set  in the plane, we denote its area, perimeter, boundary and number of components, by ,  and , respectively.

\par
We assume that the set  of sensors is generated by a Poisson process of density  on our region of interest . Within , an event happens in the region . Each sensor  makes a 0-1 event detection (or measurement), whether  or , which might be incorrect with probability . The sensor errors are independent from each other, and from the Poisson process placing the sensors. Each sensor knows the measurement results of all other sensors within radius  and revises his own measurement based on that information.

\par
The comparison with neighboring sensors gives information only if the sensor has neighboring sensors. The expected number of neighbors in this model is  and thus the probability that a sensor has no neighbors is , which should be much smaller than the measurement error . The expected number of sensors in  is , so without correction by neighborhood comparison, the expected number of incorrect sensors, i.e., misclassified sensors, is . Theorem~\ref{thm:general_outZ} and Theorem~\ref{thm:general_inZ} show that the expected number of misclassified sensors is improved significantly by the majority vote scheme.

\par
Let  be the set of points within distance  to the boundary of . Any sensor in this dubious region  has potentially neighbors inside and outside , in other words, we possibly have both of correct 0- and 1-answers within the same neighborhood, which would lead such sensors to make the wrong decision after the majority vote. Thus the analysis on the expected number of misclassified sensors in  is a key in the majority vote scheme.

In Section~\ref{sec:analysis}, we analyze the majority vote scheme in a most general setting and derive the following bounds on the expected number of misclassified sensors in  and on the expected number of misclassified sensors in .
\begin{theorem}\label{thm:general_outZ}
For , the expected number of sensors in  that are misclassified by the simple majority rule in the neighborhood of radius  is at most

and at least

\end{theorem}

\begin{theorem}\label{thm:general_inZ}
For , the expected number of sensors in  that are misclassified by the simple majority rule in the neighborhood of radius  is at most

There exists some event region  such that the expected number of misclassified sensors in  is at least .
\end{theorem}

The ratio of the upper bound to the lower bound in Theorem~\ref{thm:general_outZ} grows linearly with the expected number of neighbors . However, since  for any , they both decrease exponentially with the expected number of neighbors , so the expected number of misclassified sensors outside  decreases exponentially with the expected number of neighbors . Theorem~\ref{thm:general_inZ} tells us that the expected number of sensors in  grows with the parameters  and , and the perimeter of . Thus a region  with long boundary or with many components would be the worst in the majority vote scheme. Moreover such worst examples exist. As a result, Theorems~\ref{thm:general_outZ} and~\ref{thm:general_inZ} illustrate the trade-off between the error outside , which decreases exponentially with  and , and the error inside , which increases with  and .

\par
Sensors very near to the boundary of  can be unavoidably misclassified according to Theorem~\ref{thm:general_inZ}. If  is thin so that , then there are no sensors sufficiently deep inside  whose neighbors are mainly inside , so the region will not be recognized by the majority vote scheme. We thus need to make some (seemingly strong) assumptions on the shape of  such that  is guaranteed. In this paper, we consider  as a convex event region with a bounded curvature, i.e., with sufficiently rounded boundary. For such , in Section~\ref{sec:convex}, we prove a significantly improved upper bound on the expected number of misclassified sensors in , which is a main result in this paper.

\begin{theorem}\label{thm:convex}
Let . If the event region  is convex and the radius of curvature at each point on the boundary is at least , then the expected number of sensors in  that are misclassified by the simple majority rule in the neighborhood of radius  is less than

\end{theorem}

Finally, in Section~\ref{sec:experiment}, we perform some simulation for convex and round event regions and check the effect of the various parameters in the majority vote scheme such as , , , and the perimeter of , and present a refinement method to improve the performance particularly for the tricky cases, i.e., for small  and large .


\section{Analysis for General Event Regions}~\label{sec:analysis}

In this section we analyze the simple majority rule and prove Theorem~\ref{thm:general_outZ} and Theorem~\ref{thm:general_inZ}. Throughout the paper, we will use the following lemma.


\begin{lemma}\label{lem:Bn}
For , the probability  of at least  successes among  independent Bernoulli trials of success probability  is

\end{lemma}
\begin{proof}
The upper bound can be easily derived by the Chernoff inequality proven in~\cite{HR90}. For the lower bound, let . Then . By simple arithmetic calculation, we can show that the probability of at least   successes among  independent Bernoulli trials of success probability  is at least

The last inequality is given in~\cite{MV08}.
Applying  to the last term of the above inequality, we get the lower bound we wanted as follows:

\end{proof}


\subsection{Proof of Theorem~\ref{thm:general_outZ}}

Recall that  is the set of points of  within distance  to , the boundary of . The expected number of the misclassified sensors in  by the majority vote is  times the probability of a sensor  in  being  misclassified by the majority rule in the neighborhood of radius .

Suppose that  has  neighbors. Since , the  neighbors of  lie all inside  or all outside . The probability of  being misclassified is the one that at least half of measurements of the neighbors should be erroneous. When  is odd, at least  errors among  measurements must happen. But when  is even, the measurement of  can be served as a tie breaker, thus at least  errors must happen among  measurements including a measurement of .

Let  be the probability that at least  successes among  trials with success probability . For odd , it holds from binomial distribution that  because  for . The probability of  being misclassified is simplified as follows:


The first probability is  by the definition of the Poisson process. The second probability  is at most  by Lemma~\ref{lem:Bn}. Thus we get the upper bound of the probability of  being misclassified as follows:

Multiplying this with  gives the upper bound of the theorem.

\par
For the lower bound in Theorem~\ref{thm:general_outZ} have a similar inequality as in the upper bound as follows:

where  for .

The second probability is at least  by Lemma~\ref{lem:Bn}. Thus we get the following lower bound of the probability of  being misclassified, which proves the lower bound of the theorem.



\subsection{Proof of Theorem~\ref{thm:general_inZ}}
For the upper bound of the theorem, we simply assume that any sensor in  always makes the wrong decision. The expected number of sensors in  is , and we have the geometric bound  for any general set . Thus the expected number of misclassified sensors in  by the majority vote rule is at most .

\begin{figure}
\centering
    \includegraphics[width=8cm]{thin-lowerbound.eps}
\caption{A thin and long rectangle .}
\label{fig:thin-lowerbound}
\end{figure}

\par
We now explain that this bound is asymptotically the best we can obtain for the expected number of misclassified sensors in  for any  with . Indeed, if  is a thin and long rectangle of height  and of width  as shown in Figure~\ref{fig:thin-lowerbound}, then all sensors in  will be in , i.e., . The perimeter of  is . Consider any sensor  in  at distance at least  from the both vertical edges of . Let  be a disk of radius  around . Since the height of  is ,  consists of three parts as in Figure~\ref{fig:thin-lowerbound}; two circle segments of  and the middle part, , between the circle segments. The expected numbers of sensors in  and  whose initial measurement is ``not in X'' are  and , respectively. Thus  has at least  neighbors in  whose initial measurement is ``not in ''. We can prove that this is at least half of the number of sensors in , i.e.,  for any  by simple calculation. This results in making a wrong decision of  by the majority vote scheme. The expected number of such misclassified sensors in  is , which is at least .

\begin{figure}
\centering
    \includegraphics[width=10cm]{lowerbound.eps}
\caption{A non-convex region , satisfying that the radius of curvature is at least  everywhere on the boundary.}
\label{fig:lowerbound}
\end{figure}

\par
We can also find such a worst example even when  is not convex but has a round boundary satisfying some curvature constraint: the radius of curvature is at least  everywhere on the boundary. Figure~\ref{fig:lowerbound} illustrates an event region  of the curvature radius , but not convex. All sensors in roughly  thin rectangular strips have a majority of neighbors outside , thus they will make wrong decisions. Assume that  is a multiple of , so . Total area of thin strips is at least . The boundary of  consists of circular arcs at its both ends and linear segments of thin strips. Since the length of any circular arc is at most , the total length of circular arcs is at most . Then  since . Thus the expected number of misclassified sensors in  of this non-convex region  with bounded curvature  is at least . We now complete the proof of Theorem~\ref{thm:general_inZ}.



\section{Analysis for Convex Event Regions with Round Boundary}~\label{sec:convex}

We now prove our main result, Theorem~\ref{thm:convex} that if  is a convex region with a round boundary of the curvature radius , then the expected number of misclassified sensors in  significantly decreases. As a result, the convexity and the curvature constraint both are crucial for a better bound.

Let  be a sensor in  whose nearest point to  is  in distance  for some constant . Let  be the disc of radius  around . Let  be the fraction of  on the same side of  as , i.e.,  if ,  if . Then we can prove the following:
\begin{lemma}\label{lem:good0}
If  and , then the probability of  being misclassified is at most

\end{lemma}
\begin{proof}
Assume first that  lies in . Then . The sensors are located in  according to a Poisson distribution, but if we assume there are  sensors in ,  which happens with the probability , then the conditional distribution of these  sensors over  is uniform. So each of these  sensors independently falls with probability  in , and  in ,
and there, with probability , reports incorrectly, and with probability , reports correctly.
Thus, we have  independent Bernoulli experiments, which report being in  with probability  and being outside  with probability . Since  is in , the probability of an incorrect classification is the probability of a majority vote for being outside , that is, more than half of  Bernoulli experiments being successful with success probability ; if , by the Chernoff bound (Lemma~\ref{lem:Bn}), this is at most

For  and , the necessary condition for the Chernoff inequality, i.e., , is satisfied, and thus the probability of  being misclassified, without the condition on , is at most

Set

For fixed ,  is a monotone
increasing function in  with  and , and satisfies that . Thus we get the probability of  in   being misclassified is at most 

\par
For sensors  lying in , letting , we get the same upper bound as (\ref{eq:insideX}) for the probability of  being misclassified by more than half of neighbours reporting being inside . Therefore we get the upper bound of the lemma for the probability of  being misclassified, no matter whether  is inside or outside .
\end{proof}

\par
This lemma provides us a good bound on the expected number of misclassified sensors, but only for those satisfying its necessary condition, . Since  is convex, all sensors in  satisfy the condition, but some sensors in  may not satisfy the condition. Indeed, for sensors , we can get a simple lower bound on  as follows: the sensor  is assumed to be on the inner parallel curve at distance  from  for some . Let  be the point on  from  in distance . Consider another disk  of radius  with its center in  such that it is tangent to  at . Since the radius of curvature is at least  everywhere on , by Blaschke's rolling ball theorem~\cite[pp. 114-116]{BL}, the interior of  is completely contained in , thus we have , where

Since  is a monotone increasing function with , we can conclude that for sensors in  within distance   to the boundary of , it is not necessarily true that , i.e., . Therefore it is unavoidable to split the sensors into ``good" and ``bad" groups and to analyze them in different ways.





\begin{figure}
\centering
    \includegraphics[width=12cm]{convex-good-bad.eps}
\caption{Classification of sensors . (a)  is good since . (b)  is bad since .}
\label{fig:bad}
\end{figure}


\par
For easier analysis, we use the following criteria for the split. Consider the disk  of radius  around  and the boundary curve  as illustrated in Figure~\ref{fig:bad}. By Blaschke's rolling ball theorem, the curve  enters  once at some point , leaves  once at some point  and it never enters  afterwards. Let  be the diametrically opposite point of  in . Then there are two possibilities when  is in :  lies in the same side of  as , or in the opposite side of  to . For the two cases when  lies in , see Figure~\ref{fig:bad}. We call  \emph{good} if  lies in the side of  as , and otherwise \emph{bad}. By the convexity of , it is clear that all sensors in  are good.


\subsection{Upper Bound on the Misclassified Good Sensors}

For good sensors in , we get the following bound on the probability of being misclassified.
\begin{lemma}\label{lem:good1}
Let  be a good sensor in  whose distance to the boundary of  is . Then the probability of  being misclassified is at most

\end{lemma}
\begin{proof}
As in Figure~\ref{fig:bad}(a), we first consider the case that . Let  be a disk of radius  around  on the inner parallel curve at distance  from , and let  be the closest point on  from . Using the curvature constraint and the fact that , the half of  bounded by line  is contained in . In addition, on the other side of , the triangle  of height , where  is the point on  directly above , is also contained in . Thus . This gives us a lower bound on , where the area of the portion of  lying inside  is expressed as , that is, . Then 
Similarly, for good sensors  lying in , the portion of  lying outside  satisfies that .
Plugging the lower bound  into  of Lemma~\ref{lem:good0}, we get the result.
\end{proof}

\par
We integrate this probability over all inner and outer parallel curves and get an upper bound on the expected number of misclassified good sensors in :

For the upper bound on the integrals, we used that , where  is an error function appeared in integrating the Gaussian function with  for any . 



\subsection{Upper Bound on the Misclassified Bad Sensors}
Now we derive a bound on the expected number of misclassified bad sensors in . Note that all bad sensors of  appear only in , and see Figure~\ref{fig:bad}(b) for illustration. For bad points(sensors), we do not know any upper bound but  on the probability of being misclassified. However, we can get an upper bound on the total length of disjoint bad curve segments, which consist of bad points only, on , an inner parallel curve at distance  from  for some .
For this we use a covering argument and the fact that the total direction change of a simple closed convex curve is .

\begin{lemma}\label{lem:length_bad}
The total length of bad curve segments on the inner parallel curve  at distance  to the boundary of  is at most

\end{lemma}
\begin{proof}
As in the proof for the good sensors, we define , , and  for a bad sensor  on the inner curve . See Figure~\ref{fig:bad}(b). Then the disk  around  of radius  touches  at  and is completely contained in . Let  be the angle by which the direction of  changes in counterclockwise direction between entering and leaving . Using the facts that , , and  over , we have that

Since the total direction change of  traversing a simple curve once around is at most , we cannot have more than  such curve segments on  whose interiors are disjoint, each with a direction change of at least .

\begin{figure}
\centering
    \includegraphics[width=11cm]{pick.eps}
\caption{Left half disks around each picked bad points.}
\label{fig:pick}
\end{figure}

We now pick bad points on  at distance of at least  along  as traversing it in counterclockwise direction as follows. If the points on  are all bad, then its length becomes the perimeter of , i.e., at most . Otherwise, there must be at least one good point on . We traverse  from the good point in counterclockwise direction. We will meet the first bad point, then we pick this bad point and call it . We next pick the bad point  on  at distance of at least  from  along the curve. Continuing this picking process, we can pick  bad points  where the distance from  to  might be less than . As in Figure~\ref{fig:pick}(a), we denote by  a right half of the disk of radius  around each picked bad point  with respect to the traversing direction. Then it is clear that the union of such right half disks covers all the bad points on  because any bad point  between  and \footnote{The addition on the indices is a modular addition with .} is contained in .

Without loss of generality, we assume that  is odd. Let us now consider the intersections of  with the right half disks  around every even picked bad points . These intersections  for even  result in curve segments (or arc intervals) of  whose the left endpoint is . We claim that these segments except from the first and last ones are disjoint;  can overlap with . As in Figure~\ref{fig:pick}, we consider two consecutive intersections,  and  for even . It suffices to show that the right endpoint of  lies in the left of the left endpoint of  on the curve. The arc length between  and  is at least  by picking rule, and the length of  is at most  by curvature constraint. Thus the distance between the right endpoint of  and the left endpoint of  is at least , so the claim is proved.

The number of disjoint bad curve segments on  is already proved to be no more than , so the sum of their length (excluding the length of ) is at most . For the last curve segment , we simply add its length , which gives the length of  for . Considering the curve segments generated by every odd picked bad points, the sum of their length is at most . Note here that the first odd segment does not overlap with the last odd one. Thus the total length of bad curve segments on  is at most . Furthermore, the total length should be no more than the length of , , which completes the lemma.
\end{proof}

Integrating this over all inner parallel curves, we get an upper bound on the expected number of all misclassified bad sensors in  as follows:



\par
Now we put both  (\ref{eq:good}) and (\ref{eq:bad}) together to obtain the upper bound on the expected number of misclassified points in , completing the proof of Theorem~\ref{thm:convex}:



\section{Simulation Results}\label{sec:experiment}

We perform some simulations to see the performance of the majority vote scheme and the dependency of several parameters such as the error probability  of sensors, the neighboring radius , and the geometric parameters of a convex event region . We simulate the majority vote scheme each with , and  sensors distributed by Poisson process in a unit square . We consider two event regions  and  as shown in Figure~\ref{fig:experiment};  is a square of side length  with rounded corners of the curvature radius , and  is a longer and thinner rectangle of dimension  with the same type of corners. Note here that they have the same area, but  has a longer perimeter than ;  and . We test the majority vote scheme for  different radii  by incrementing  from  to  and for  different error probabilities  by incrementing  from  to .

\begin{figure}
\centering
\epsfig{file=square.eps, width = 4.5cm}\quad\quad\quad\quad
\epsfig{file=long.eps, width = 4.5cm }
\caption{Event regions  and , having the same area but different perimeter.}
\label{fig:experiment}
\end{figure}

\subsection{Results}

We first check the correction rate of the major vote scheme, which is the ratio of the number of sensors revised correctly after the major vote scheme with the number of the initial errors. Figure~\ref{fig:sim-ratio} shows such correction rates for various values of ,  and , respectively. If  is small or  is small, then each sensor has too few neighbors to correct its error by the majority vote scheme, and what is worse is that the initial right decision can be changed even when the sensor is far from the boundary of . However, as  grows or  increases, much more misclassified sensors are correctly revised by neighborhood comparisons; more than  for  or . The correction rate for  becomes the highest around ; almost  for  and  for . But, if  is too small or too large, then the rate goes below , still above .


\par
Figure~\ref{fig:sim-ub-final} shows the comparison on the difference between the number of final errors in the simulation and the upper bound on the number of the final errors proved in this paper for  and . The upper bound is given as the sum of two upper bounds, each in  of Theorem~\ref{thm:general_outZ} and in  of Theorem~\ref{thm:convex}, which is at most

The number of final errors in the simulation is clearly much less than the theoretical upper bound, which tells us the upper bound could be improved further.

\par
We next test how much the sensors in  are likely to be misclassified out of the total final errors in . Figure~\ref{fig:sim-beta} shows that as  grows (or  increases), the misclassified sensors occur mostly in  since the errors in  decreases exponentially with the expected number of neighbors  by the bound~(\ref{eq:ub}), but the errors in  increases linearly with . However, the high probability  causes many initial errors everywhere in , which makes the voting effect less powerful for the sensors in , thus the ratio decreases as  increases.

We can also see the effect of the perimeter. Since  and  have the same area, the average number of sensors falling into them is almost equal, but  has a  longer perimeter than . A longer perimeter is a bad factor to misclassify more sensors outside the event region. According to Theorem~\ref{thm:convex}, we can expect there would be a linear relation between the number of misclassified sensors and the perimeter of . We can find such relation in Figure~\ref{fig:sim-ratio} to Figure~\ref{fig:sim-beta}.

\par
We now count the misclassified sensors  and  separately. We expected the misclassified sensors in  are more likely to occur than the ones in  since the sensors in  are all good, i.e., , where  is the disk of radius  around a sensor in . We have checked such situation actually happens as in Figure~\ref{fig:sim-gamma}.

\par
Finally, we conclude from the simulation results that the best radius  that produces the least misclassified sensors is  for ,  for ,  for , and  for .

\subsection{Further Refinements}

A sensor with a small neighborhood radius , say less than  in our simulation, has few neighbors to correct its error by the majority vote scheme, so the correction rate is not satisfactory as we already checked in Figure~\ref{fig:sim-ratio}. A refinement method to achieve the high correction rate for small  is to apply the majority vote scheme more than once, i.e., multiple vote rounds. Suppose that an initial error of a sensor  is not corrected during the first vote round. After the first vote, several erroneous neighbors of  would be revised correctly, thus  is more likely to correct its error at the second vote round. After  rounds, the measurement of the sensors at distance  from  can affect the decision of . This, on the other hand, tells that more rounds are not always helpful, particularly for the sensor  near  in the sense that  can receive the information from many sensors on the opposite side of , which can lead  to make the wrong change. Thus we need to choose  carefully. For a fixed distance from , as  gets smaller,  becomes larger, so  should be set in proportional to . For large error probability , we can expect the similar effect as for small , but its impact seems a bit weaker than that of . We set  and . In our simulation,  is at most .

\par
In our simulation, instead of simply repeating a majority vote scheme  times, we take an indirect but efficient implementation as follows: With each sensor , we associate a real value . Initially,  if the initial measurement of  is that  and  if . At each round,  is updated to be the average value of  for the neighbors  of . After the first round, if , then it means  has the majority of its neighbors inside , otherwise outside , which is the exactly same judgement as the single-round vote scheme. The score values are propagated gradually like the pattern in the well-known Gau{\ss}-Seidel iterative method, thus after  rounds, each sensor  receives the score values of the sensors within distance  from . Each sensor  decides its final measurement by the sign of , that is, decides that  if  and  if . If , then it follows the measurement of  made at the previous round.

\par
Figure~\ref{fig:sim-s-m} compares the correction rates for  and  between single-round and multiple-round vote schemes. The multiple-round scheme performs much better for large ;  and  improvements each for  and . If we focus only on small radius  between  and , then the correction rates are improved more as in Figure~\ref{fig:sim-s-m}. As a result, the multiple-round vote scheme is a reasonable refinement to improve the correction rate for small  and large .

The simulation is done in the desktop computer with Intel Core i7-2600 CPU, 3.40GHz. Regardless of the values of , , and , every major vote scheme runs in  seconds, which is the multiple-round case with  rounds where , , and , but the average execution time over all , , and  is  seconds.

\begin{figure}
\centering
\mbox{\epsfig{file=2-2.eps, width=10cm}}
\mbox{\epsfig{file=2-1.eps, width=10cm}}
\mbox{\epsfig{file=2-3.eps, width=10cm}}
\caption{The correction rate by majority vote scheme for  and . (a) For  (averaged by  and ). (b) For  (averaged by  and ). (c) For  (averaged by  and ).}
\label{fig:sim-ratio}
\end{figure}

\begin{figure}
\centering
\mbox{\epsfig{file=6-2.eps, width=10cm}}
\mbox{\epsfig{file=6-1.eps, width=10cm}}
\mbox{\epsfig{file=6-3.eps, width=10cm}}
\caption{Comparisons between the number of final errors in simulations and the upper bound on the misclassified errors proved in Theorem~\ref{thm:general_outZ} and Theorem~\ref{thm:convex}.}
\label{fig:sim-ub-final}
\end{figure}

\begin{figure}
\centering
\mbox{\epsfig{file=3-2.eps, width=10cm}}
\mbox{\epsfig{file=3-1.eps, width=10cm}}
\mbox{\epsfig{file=3-3.eps, width=10cm}}
\caption{Ratio of misclassified sensors in  with the total errors in .}
\label{fig:sim-beta}
\end{figure}

\begin{figure}
\centering
\mbox{\epsfig{file=3-5.eps, width=10cm}}
\mbox{\epsfig{file=3-4.eps, width=10cm}}
\mbox{\epsfig{file=3-6.eps, width=10cm}}
\caption{Ratio of errors in  with the errors in .}
\label{fig:sim-gamma}
\end{figure}

\begin{figure}
\centering
\mbox{\epsfig{file=4-2.eps, width=10cm}}
\mbox{\epsfig{file=4-1.eps, width=10cm}}
\mbox{\epsfig{file=4-3.eps, width=10cm}}
\caption{Comparison of single-round and multiple-round vote schemes.}
\label{fig:sim-s-m}
\end{figure}

\section{Conclusion}
In this paper we analyzed the simple majority rule and make explicit and precise the dependency on the error probability  of sensors, the radius  of the voting neighborhood, and the geometric parameters of event regions. To our best knowledge, this is the first to give bounds on the expected number of incorrectly classified sensors, with all such parameters in majority vote scheme. We also provided some empirical evidence indicating the dependency on such parameters.

The structure of our error bounds are the following:
\begin{itemize}
\item There is some background error,  which happens even when there is no event at all, i.e., . This error depends on the total size of the area of interest , but decreases exponentially fast with the expected number of neighbors of each sensor, i.e., .

\item There is a term that depends on the perimeter of , which means that sensors very near to the boundary of  can be unavoidably misclassified. In fact,  sensors can be misclassified in a simplest thin rectangle of height , which gives the lower bound for the term.

\item There are terms that depend on the expected number of neighbors of a point, the number of components of , or the logarithm of the perimeter of  specially for a convex region with bounded curvature.
\end{itemize}

\par
The assumption on the boundary curvature might look strong. We need it for two related things; for the existence of inner parallel curves of  at distance up to , and for the property that any sensor neighborhood extends across  only on one side. The first might be a technical restriction which can somehow be circumvented, but the second is crucial for the voting algorithm: if the set  is thin, then there are no sensor positions sufficiently deep inside  that the majority of their neighbors will also be inside , so the set will not be recognized by the majority rule.



\begin{thebibliography}{100}
\bibitem{Ag*88} {Z.~Agur, A.S.~Frankel, and S.T.~Klein}.
{The Number of Fixed Points of the Majority Rule}.
{\em Discrete Mathematics}, {\bf 70}: 295--302, 1988.


\bibitem{AlK99} {F.~Alkoot and J.~Kittler}.
{Experimental Evaluation of Expert Fusion Strategies}.
{\em Pattern Recognition Letters}, {\bf 20}: 1361--1369, 1999.

\bibitem{BL} {W. Blaschke}.
{\em Kreis und Kugel}.
{Zweite Auflage, Walter de Gruyter AG, Berlin}, 1956.

\bibitem{CC01} {D.~Chen and X.~Cheng}.
{An Asymptotic Analysis of Some Expert Fusion Methods}.
{\em Pattern Recognition Letters}, {\bf 22}: 901--904, 2001.

\bibitem{CG03} {K. K.~Chintalapudi and R.~Govindan}. {Localized Edge Detection
in Sensor Fields}.  {\em Ad Hoc Networks}, {\bf 1}: 273--291, 2003.

\bibitem{Cu*10} {F.~Cun-Fang, G.~Jian-Yue, W.~Zhi-Xi, and W.~Ying-Hai}.
{Effects of Average Degree of Network on an Order-Disorder Transition in
Opinion Dynamics}. {\em Chinese Physics B}, {\bf 19}: 060203, 2010.

\bibitem{DC09} {M.~Ding and X.~Cheng}. {Robust Event Boundary Detection
in Sensor Networks--A Mixture Model Approach}. {\em Proc. IEEE INFOCOM 2009},
2991--2995.

\bibitem{HR90} {T.~Hagerup and C.~ R\"ub}.
{A Guided Tour of Chernoff Bounds}. {\em Information Processing Letters},
{\bf 33}(6): 305--308, 1990.

\bibitem{JN06}{G.~Jin and S.~Nittel}.
{NED: An Efficient Noise-Tolerant Event and Event  Boundary Detection
Algorithm in Wireless Sensor Networks}.
{\em  Proc. 7th Int. IEEE Conference on Mobile Data Management}, 153, 2006.

\bibitem{KiH*98} {J.~Kittler, M.~Hatef, R.~Duin, and J. Matas}.
{On Combining Classifiers}.{\em IEEE Transactions on
Pattern Analysis and Machine Intelligence}, {\bf 20}: 226--239, 1998.

\bibitem{Kr02} {R.~Kr\'alovi\v{c}}. {On Majority Voting Games in Trees}.
{\em Proc. SOFSEM '01}, LNCS {\bf 2234}: 282--291, 2001.

\bibitem{KI04}{B.~Krishnamachari and S.~Iyengar}.
{Distributed Bayesian Algorithms for Fault-Tolerant Event Region Detection
in Wireless Sensor Networks}. {\em IEEE Transactions on Computers},
{\bf 53}(3): 241--250, 2004.


\bibitem{Ku02} {L.I.~Kuncheva}.
{A Theoretical Analysis of Six Classifier Fusion Strategies}.
{\em IEEE Transactions on Pattern Analysis and Machine Intelligence},
{\bf 24}: 281--286, 2002.

\bibitem{LaS97} {L.~Lam and C.Y.~Suen}. {Application of Majority Voting
to Pattern Recognition: an Analysis of its Behavior and Performance}.
{\em IEEE Transactions on Systems, Man, and Cybernetics}, {\bf 27}: 553-568, 1997.

\bibitem{Li*05} {F.W.S.~Lima, U.L.~Fulco, and R.N.~Costa~Filho}.
{Majority-Vote Model on a Random Lattice}. {\em  Physical Review E},
{\bf 71}: 036105, 2005.

\bibitem{MV08} {J.~Matousek and J.~Vondrak}.
{Lecture Notes: The Probabilistic Method}. pp. 1--71, 2008.

\bibitem{MuP01} {N.A.Mustafa, A.~Peke\v{c}}. {Majority Consensus and
the Local Majority Rule}.  {\em Proc. ICALP '01},
LNCS {\bf 2076}: 530--542, 2001.

\bibitem{MuP04} {N.A.Mustafa, A.~Peke\v{c}}.
{Listen to your Neighbors: How (not) to Reach a Consensus}.
{\em SIAM Journal of Discrete Mathematics}, {\bf 17}: 634-660, 2004.

\bibitem{Na03} {A.M.~Narasimhamurthy}.
{A Framework for the Analysis of Majority Voting}.
{\em Proc. SCIA 2003} (J. Bigun, T. Gustavsson, Eds.),
LNCS {\bf 2749}: 268--274, 2003.

\bibitem{NM03} {R.~Nowak and U.~Mitra}. {Boundary Estimation in
Sensor Networks: Theory and Methods}. {\em Proc. 2nd Int. Workshop on Information Processing in Sensor Networks}, LNCS {\bf 2634}: 80--95, 2003.

\bibitem{Ol92} {M.J.~de~Oliveira}. {Isotropic Majority-Vote Model on a
Square Lattice}. {\em Journal of Statistical Physics}, {\bf 66}: 273--281, 1992.

\bibitem{Pe02} {D.~Peleg}. {Local Majorities, Coalitions and Monopolies
in Graphs: A Review}. {\em Theoretical Computer Science}, {\bf 282}: 231--257, 2002.

\bibitem{WGM06} {Y.~Wang, J.~Gao, and J.S.B.~Mitchell}.
{Boundary Recognition in Sensor Networks by Topological Methods}.
{\em Proc. 12th International Conference on
Mobile Computing and Networking}: 122--133, 2006.

\bibitem{WCDXLD07} {W.~Wu, X.~Cheng, M.~Ding, K.~Xing, F.~Liu, and P.~Deng}.
{Localized Outlying and Boundary Data Detection in Sensor Networks}.
{\em IEEE Transactions on Knowledge and Data Engineering}, {\bf 19}(8): 1145--1157, 2007.

\bibitem{Ya*08} {J.-S.~Yang, I.-M.~Kim, and  W.~Kwak}.
{Existence of an Upper Critical Dimension in the Majority Voter Model}.
{\em Physical Review E}, {\bf 77}: 051122, 2008.

\bibitem{Zo10} {K.J.S.~Zollman}. {Social Structure and the Effects of
Conformity}. {\em Synthese}, {\bf 172}: 317--340, 2010.

\end{thebibliography}


\end{document}
