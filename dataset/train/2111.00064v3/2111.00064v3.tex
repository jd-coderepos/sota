
\documentclass{article} \usepackage{iclr2022_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}
\usepackage{amsmath,amsfonts}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{bm}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage[normalem]{ulem}
\useunder{\uline}{\ul}{}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage[center]{subfigure}
\usepackage{xcolor}

\newcommand{\Eli}[1]{{\textcolor{blue}{[Eli: #1]}}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{proposition}[theorem]{Proposition}

\title{Node Feature Extraction by Self-Supervised Multi-scale Neighborhood Prediction}



\author{Eli Chien\thanks{This work was done during Eli Chien’s internship at Amazon, USA.} \\
University of Illinois Urbana-Champaign, USA\\
\texttt{ichien3@illinois.edu} \\
\And
Wei-Cheng Chang \\
Amazon, USA \\
\texttt{chanweic@amazon.com} \\
\AND
Cho-Jui Hsieh \\
University of California, Los Angeles, USA\\
\texttt{chohsieh@cs.ucla.edu} \\
\And
Hsiang-Fu Yu,  Jiong Zhang \\
Amazon, USA \\
\texttt{\{hsiangfu,jiongz\}@amazon.com} \\
\AND
Olgica Milenkovic\\
University of Illinois Urbana-Champaign, USA\\
\texttt{milenkov@illinois.edu}\\
\And
Inderjit S. Dhillon\\
Amazon, USA \\
\texttt{isd@amazon.com} \\
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}


\maketitle

\begin{abstract}
Learning on graphs has attracted significant attention in the learning community due to numerous real-world applications. In particular, graph neural networks (GNNs), which take \emph{numerical} node features and graph structure as inputs, have been shown to achieve state-of-the-art performance on various graph-related learning tasks. Recent works exploring the correlation between numerical node features and graph structure via self-supervised learning have paved the way for further performance improvements of GNNs. However, methods used for extracting numerical node features from \emph{raw data} are still \emph{graph-agnostic} within standard GNN pipelines. This practice is sub-optimal as it prevents one from fully utilizing potential correlations between graph topology and node attributes. To mitigate this issue, we propose a new self-supervised learning framework, Graph Information Aided Node feature exTraction (GIANT). GIANT makes use of the eXtreme Multi-label Classification (XMC) formalism, which is crucial for fine-tuning the language model based on graph information, and scales to large datasets. We also provide a theoretical analysis that justifies the use of XMC over link prediction and motivates integrating XR-Transformers, a powerful method for solving XMC problems, into the GIANT framework. We demonstrate the superior performance of GIANT over the standard GNN pipeline on Open Graph Benchmark datasets: For example, we improve the accuracy of the top-ranked method GAMLP from  to , SGC from  to  and MLP from  to  on the ogbn-papers100M dataset by leveraging GIANT. Our implementation is public available\footnote{\url{https://github.com/amzn/pecos/tree/mainline/examples/giant-xrt}}. 
\end{abstract}

\vspace{-0.5cm}
\section{Introduction}
The ubiquity of graph-structured data and its importance in solving various real-world problems such as node and graph classification have made graph-centered machine learning an important research area~\citep{lu2011link,shervashidze2011weisfeiler,zhu2005semi}. Graph neural networks (GNNs) offer state-of-the-art performance on many graph learning tasks and have by now become a standard methodology in the field~\citep{kipf2017semi,hamilton2017inductive,velickovic2018graph,chien2020adaptive}. In most such studies, GNNs take graphs with \emph{numerical node attributes} as inputs and train them with task-specific labels. 

Recent research has shown that self-supervised learning (SSL) leads to performance improvements in many applications, including graph learning, natural language processing and computer vision. Several SSL approaches have also been successfully used with GNNs~\citep{Hu*2020Strategies,you2018graphrnn,you2020graph,hu2020gpt,velickovic2019deep,kipf2016variational,deng2020graphzoom}. The common idea behind these works is to explore the correlated information provided by the \emph{numerical} node features and graph topology, which can lead to improved node representations and GNN initialization. However, one critical yet neglected issue in the current graph learning literature is how to actually obtain \emph{the numerical node features from raw data} such as text, images and audio signals. As an example, when dealing with raw text features, the standard approach is to apply \emph{graph-agnostic} methods such as bag-of-words, word2vec~\citep{mikolov2013distributed} or pre-trained BERT~\citep{devlin2018bert} (As a further example, raw texts of product descriptions are used to construct node features via the bag-of-words model for benchmarking GNNs on the ogbn-products dataset~\citep{hu2020open,chiang2019cluster}). The pre-trained BERT language model, as well as convolutional neural networks (CNNs)~\citep{goyal2019scaling,kolesnikov2019revisiting}, produce numerical features that can significantly improve the performance of various downstream learners~\citep{devlin2018bert}. Still, none of these works leverage graph information for actual self-supervision. Clearly, using graph-agnostic methods to extract numerical features is sub-optimal, as correlations between the graph topology and raw features are ignored. 

Motivated by the recent success of SSL approaches for GNNs, we propose GIANT, an SSL framework that resolves the aforementioned issue of graph-agnostic feature extraction in the standard GNN learning pipeline. Our framework takes raw node attributes and generates numerical node features with graph-structured self-supervision. To integrate the graph topology information into language models such as BERT, we also propose a novel SSL task termed \emph{neighborhood prediction}, which works for both homophilous and heterophilous graphs, and establish connections between neighborhood prediction and the eXtreme Multi-label Classification (XMC) problem~\citep{shen2020extreme,yu2020pecos,chang2020taming}. Roughly speaking, the neighborhood of each node can be encoded using binary multi-labels (indicating whether a node is a neighbor or not) and the BERT model is fine-tuned by successively improving the predicted neighborhoods. This approach allows us to not only leverage the advanced solvers for the XMC problem and address the issue of graph-agnostic feature extraction, but also to perform a theoretical study of the XMC problem and determine its importance in the context of graph-guided SSL.  


\begin{figure}[t]
    \centering
    \includegraphics[trim={1.7cm 4.2cm 4.7cm 6.2cm},clip,width=0.69\linewidth]{Overall_Pipelines.pdf}
    \includegraphics[trim={0cm 4cm 16cm 0cm},clip,width=0.3\linewidth]{XMC_illustration.pdf}
    \vspace{-0.5cm}
    \caption{Left: Illustration of the standard GNN pipeline and our GIANT framework. Note that only the language model (i.e., the Transformers) in GIANT can use the correlation between graph topology and raw text. Nevertheless, GIANT can work with other types of input data formats, such as images and audio; the study of these models is deferred to future work. Right: Illustration of the connection between our neighborhood prediction and the XMC problem. We use graph information to self-supervise fine-tuning of the text encoder  (i.e., the Transformer) in our neighborhood prediction problem. The resulting fine-tuned text encoder is then used to generate numerical node features  for use in downstream tasks (see also Figure~\ref{fig:XRT} and the description in Section~\ref{sec:method}).}
    \label{fig:pipelines}
    \vspace{-0.3in}
\end{figure}

Throughout the work, we focus on raw texts as these are the most common data used for large-scale graph benchmarking. Examples include titles/abstracts in citation networks and product descriptions in co-purchase networks. To solve our proposed self-supervised XMC task, we adopt the state-of-the-art XR-Transformer method~\citep{jiong2021fast}. By using the encoder from the XR-Transformer pre-trained with GIANT, we obtain informative numerical node features which consistently boost the performance of GNNs on downstream tasks. 


Notably, GIANT significantly improves state-of-the-art methods for node classification tasks described on the Open Graph Benchmark (OGB)~\citep{hu2020open} leaderboard on three large-scale graph datasets, with absolute improvements in accuracy roughly  for the first-ranked methods,  for standard GNNs and  for multilayer perceptron (MLP). GIANT coupled with XR-Transformer is also highly scalable and can be combined with other downstream learning methods.

Our contributions may be summarized as follows.

\textbf{1.} We identify the issue of graph-agnostic feature extraction in standard GNN pipelines and propose a new GIANT self-supervised framework as a solution to the problem.

\textbf{2.} We introduce a new approach to extract numerical features by graph information based on the idea of \emph{neighborhood prediction.} The gist of the approach is to use neighborhood prediction within a language model such as BERT to guide the process of fine-tuning the features. Unlike link-prediction, neighborhood prediction resolves problems associated with heterophilic graphs.

\textbf{3.} We establish pertinent connections between neighborhood prediction and the XMC problem by noting that neighborhoods of individual nodes can be encoded by binary vectors which may be interpreted as multi-labels. This allows for performing neighborhood prediction via XR-Transformers, especially designed to solve XMC problems at scale.

\textbf{4.} We demonstrate through extensive experiments that GIANT consistently improves the performance of tested GNNs on downstream tasks by large margins. We also report new state-of-the-art results on the OGB leaderboard, including absolute improvements in accuracy roughly  compared to the top-ranked method,  for standard GNNs and  for multilayer perceptron (MLP). More precisely, we improve the accuracy of the top-ranked method GAMLP~\citep{zhang2021graph} from  to , SGC~\citep{wu2019simplifying} from  to  and MLP from  to  on the ogbn-papers100M dataset. 

\textbf{5.} We present a new theoretical analysis that verifies the benefits of key components in XR-Transformers on our neighborhood prediction task. This analysis also further improves our understanding of XR-Transformers and the XMC problem.

Due to the space limitation, all proofs are deferred to the Appendix.

\vspace{-0.3cm}
\section{Background and related work}
\textbf{General notation. }Throughout the paper, we use bold capital letters such as  to denote matrices. We use  for the -th row of the matrix and  for its entry in row  and column . We reserve bold lowercase letters such as  for vectors. The symbol  denotes the identity matrix while  denotes the all-ones vector. We use  in the standard manner.


\textbf{SSL in GNNs.} 
SSL is a topic of substantial interest due to its potential for improving the performance of GNNs on various tasks. Exploiting the correlation between node features and the graph structure is known to lead to better node representations or GNN initialization~\citep{Hu*2020Strategies,you2018graphrnn,you2020graph,hu2020gpt}. Several methods have been proposed for improving node representations, including (variational) graph autoencoders~\citep{kipf2016variational}, Deep Graph Infomax~\citep{velickovic2019deep} and GraphZoom~\citep{deng2020graphzoom}. For more information, the interested reader is referred to a survey of SSL GNNs~\citep{xie2021self}. While these methods can be used as SSL modules in GNNs (Figure~\ref{fig:pipelines}), it is clear that they do not solve the described graph agnostics issue in the standard GNN pipeline. Furthermore, as the above described SSL GNNs modules and other pre-processing and post-processing methods for GNNs such as C\&S~\citep{huang2021combining} and FLAG~\citep{kong2020flag} in general improve graph learners, it is worth pointing out that they can be naturally be integrated into the GIANT framework. This topic is left as a future work.  

\textbf{The XMC problem, PECOS and XR-Transformer.} 
The XMC problem can be succinctly formulated as follows: We are given a training set  where  is the th input text instance and  is the target multi-label from an extremely large collection of labels. The goal is to learn a function , where  captures the relevance between the input text  and the label . The XMC problem is of importance in many real-world applications~\citep{jiang2021lightxml,ye2020pretrained}: For example, in E-commerce dynamic search advertising, XMC arises when trying to find a ``good'' mapping from items to bid queries on the market~\citep{prabhu2018parabel,prabhu2014fastxml}. In open-domain question answering, XMC problems arise when trying to map questions to ``evidence'' passages containing the answers~\citep{chang2020pretraining,lee2019latent}. Many methods for the XMC problem leverage hierarchical clustering approaches for labels~\citep{prabhu2018parabel,you2019attentionxml}. This organizational structure allows one to handle potentially enormous numbers of labels, such as used by PECOS~\citep{yu2020pecos}. The key is to take advantage of the correlations among labels within the hierarchical clustering. 
In our approach, we observe that the multi-labels correspond to neighborhoods of nodes in the given graph. Neighborhoods have to be predicted using the textual information in order to best match the a priori given graph topology. We use the state-of-the-art XR-Transformer~\citep{jiong2021fast} method for solving the XMC problem to achieve this goal. The high-level idea is to first cluster the output labels, and then learn the instance-to-cluster ``matchers'' (please refer to Figure~\ref{fig:XRT}). Note that many other methods have used PECOS (including XR-Transformers) for solving large-scale real-world learning problems~\citep{etter2021accelerating,liu2021label,chang2020taming,baharav2021enabling,chang2021extreme,yadav2021session,sen2021top}, but not in the context of self-supervised numerical feature extraction as done in our work. 

\textbf{GNNs with raw text data. }
It is conceptually possible to jointly train BERT and GNNs in an end-to-end fashion, which could potentially resolve the issue of being graph agnostic in the standard pipeline. However, the excessive model complexity of BERT makes such a combination practically prohibitive due to GPU memory limitations. Furthermore, it is nontrivial to train this combination of methods with arbitrary mini-batch sizes~\citep{chiang2019cluster,graphsaint-iclr20}. In contrast, the XR-Transformer architecture naturally supports mini-batch training and scales well~\citep{jiang2021lightxml}. Hence, our GIANT method uses XR-Transformers instead of combinations of BERT and GNNs. To the best of our knowledge, we are aware of only one prior work that uses raw text inputs for node classification problem~\citep{zhang2020graph}, but it still follows the standard pipeline described in Figure~\ref{fig:pipelines}. Some other works apply GNNs on texts and for document classification, where the actual graphs are constructed based on the raw text. This is clearly not the focus of this work~\citep{yao2019graph,huang2019text,zhang2020text,liu2020tensor}.

\vspace{-0.3cm}
\section{Methods}\label{sec:method}
Our goal is to resolve the issue of graph-agnostic numerical feature extraction for standard GNN learning pipelines. Although our interest lies in raw text data, as already pointed out, the proposed methodology can be easily extended to account for other types of raw data and corresponding feature extraction methods. 


To this end, consider a large-scale graph  with node set  and adjacency matrix . Each node  is associated with some raw text, which we denote by . The language model is treated as an encoder  that maps the raw text  to numerical node feature . Key to our SSL approach is the task of \emph{neighborhood prediction}, which aims to determine the neighborhood  from . The neighborhood vector  can be viewed as a target multi-label  for node , where we have  labels. Hence, neighborhood prediction represents an instance of the XMC problem, which we solve by leveraging XR-Transformers. The trained encoder in an XR-Transformer generates informative numerical node features, which can then be used further in downstream tasks, the SSL GNNs module and for GNN pre-training. 

\begin{figure}
\centering
  \includegraphics[trim={0 10cm 2cm 1cm},clip,width=0.57\linewidth]{PECOS_1_new.pdf}
  \includegraphics[trim={0 9cm 9cm 1cm},clip,width=0.42\linewidth]{PECOS_2_new.pdf}
 \vspace{-0.5cm}
\caption{Illustration of the use of XR-Transformers. Step 1: Perform semantic hierarchical clustering of target labels (neighborhoods) to build a tree. Step 2: At each intermediate (internal node) level of the tree, fine-tune the Transformers for the XMC sub-problem that maps raw text of nodes to label clusters. Note that the results of higher levels are used to guide the Transformers at lower levels and hence improve their performance. The resulting Transformers are used as encoders that generate numerical node features from raw texts. Note that we can change the encoder (e.g., Transformer) to address other raw data formats such as images or audio signals.}\label{fig:XRT}
\vspace{-0.1in}
\end{figure}

\textbf{Detailed description regarding the use of XR-Transformers for Neighborhood Prediction.}
The most straightforward instance of the XMC problem is the one-versus-all (OVA) model, which can be formalized as  where  are weight vectors and  is the encoder that maps  to a -dimensional feature vector. OVA can be a deterministic model such as bag-of-words, the Term Frequency-Inverse Document Frequency (TFIDF) model or some other model with learnable parameters, such as XLNet~\citep{yang2019xlnet} and RoBERTa~\citep{liu2019roberta}. We choose to work with pre-trained BERT~\citep{devlin2018bert}. Also, one can change  according to the type of input data format (i.e., CNNs for images). Despite their simple formulation, it is known~\cite{chang2020taming} that fine-tuning transformer models directly on large output spaces can be prohibitively complex. For neighborhood prediction, , and the graphs encountered may have millions of nodes. Hence, we need a more scalable approach to training Transformers. As part of an XR-Transformer, one builds a hierarchical label clustering tree based on the label features ;  is based on Positive Instance Feature Aggregation (PIFA):

Note that for neighborhood prediction, the above expression represents exactly one step of a graph convolution with node features , followed by a norm normalization; here,  denotes some text vectorizer such as bag-of-words or TFIDF. In the next step, XR-Transformer uses balanced -means to recursively partition label sets and generate the hierarchical label cluster tree in a top-down fashion. This step corresponds to Step 1 in Figure~\ref{fig:XRT}. Note that at each intermediate level, it learns a matcher to find the most relevant clusters, as illustrated in Step 2 of Figure~\ref{fig:XRT}. By leveraging the label hierarchy defined by the cluster tree, the XR-Transformer can train the model on multi-resolution objectives. Multi-resolution learning has been used in many different contexts, including computer vision~\citep{lai2017deep,karras2017progressive,karras2019style,pedersoli2015coarse}, meta-learning~\citep{liu2019self}, but has only recently been applied to the XMC problem as part of PECOS and XR-Transformers. For neighborhood prediction, multi-resolution amounts to generating a hierarchy of coarse-to-fine views of neighborhoods. The only line of work in self-supervised graph learning that somewhat resembles this approach is GraphZoom~\citep{deng2020graphzoom}, in so far that it applies SSL on coarsened graphs. Nevertheless, the way in which we perform coarsening is substantially different; furthermore, GraphZoom still falls into the standard GNN pipeline category depicted in Figure~\ref{fig:pipelines}.

\vspace{-0.2cm}
\section{Theoretical analysis}
We also provide theoretical evidence in support of using each component of our proposed learning framework. First, we show that self-supervised neighborhood prediction is better suited to the task at hand than standard link prediction. More specifically, we show that the standard design criteria in self-supervised link prediction tasks are biased towards graph homophily assumptions~\citep{mcpherson2001birds,klicpera2018predict}. In contrast, our self-supervised neighborhood prediction model works for both homophilic and heterophilic graphs. This universality property is crucial for the robustness of graph learning methods, especially in relationship to GNNs~\citep{chien2020adaptive}. Second, we demonstrate the benefits of using PIFA embeddings and clustering in XR-Transformers for graph-guided numerical feature extraction. 
Our analysis is based on the contextual stochastic block model (cSBM)~\citep{deshpande2018contextual}, which was also used in~\cite{chien2020adaptive} for testing the GPR-GNN framework and in~\cite{baranwal2021graph} for establishing the utility of graph convolutions for node classification.

\textbf{Link versus neighborhood prediction. }
One standard SSL task on graphs is link prediction, which aims to find an entry in the adjacency matrix according to

Here, the function Similarity is a measure of similarity of two vectors,  and . The most frequently used choice for the function is the inner product of two input vectors followed by a sigmoid function. However, this type of design implicitly relies on the homophily assumption: \emph{Nodes with similar node representations are more likely to have links.} It has been shown in~\cite{pei2020geom,chien2020adaptive,zhu2020beyond,lim2021new} that there are real-world graph datasets that violate the homophily assumption and on which many GNN architectures fail. 
\begin{wrapfigure}{r}{0.3\textwidth}
    \centering
    \vspace{-\intextsep}
    \includegraphics[width=0.99\linewidth]{Counter_example.PNG}
    \vspace{-0.8cm}
  \caption{A counter-example for standard link prediction methodology.}
  \label{fig:counter}
  \vspace{-\intextsep}
\end{wrapfigure}
A simple example that shows how SSL link prediction may fail is presented in Figure~\ref{fig:counter}. Nodes of the same color share the same features (these are for simplicity represented as numerical values). Clearly, no matter what encoder  we have, the similarity of node features for nodes of the same color is the highest. However, there is no edge between nodes of the same color, hence the standard methodology of link prediction based on homophily assumption fails to work for this simple heterophilous graph. In order to fix this issue, we use a different modeling assumption, stated below.
\begin{assumption}\label{conj:1}
Nodes with similar node features have similar ``structural roles'' in the graph. In our study, we equate ``structure'' with the 1-hop neighborhood of a node (i.e., the row of the adjacency matrix indexed by the underlying node).
\end{assumption}
The above assumption is in alignment with our XMC problem assumptions, where nodes with a small perturbation in their raw text should be mapped to a similar multi-label. Our assumption is more general then the standard homophily assumption; it is also clear that there exists a perfect mapping from node features to their neighborhoods for the example in Figure~\ref{fig:counter}. Hence, neighborhood prediction appears to be a more suitable SSL approach than SSL link prediction for graph-guided feature extraction.

\textbf{Analysis of key components in XR-Transformers.}
In the original XR-Transformer work~\citep{jiong2021fast}, the authors argued that one needs to perform clustering of the multi-label space in order to resolve scarce training instances in XMC. They also empirically showed that directly fine-tuning language models on extremely large output spaces is prohibitive. Furthermore, they empirically established that constructing clusters based on PIFA embedding with TFIDF features gives the best performance. However, no theoretical evidence was given in support of this approach to solving the XMC problem. 
We next leverage recent advances in graph learning to analytically characterize the benefits of using XR-Transformers.

\begin{wrapfigure}{r}{0.33\textwidth}
    \centering
    \vspace{-\intextsep}
    \includegraphics[width=0.99\linewidth]{cSBM.PNG}
    \vspace{-0.8cm}
  \caption{Illustration of a cSBM: Node features are independent Gaussian random vectors while edges are modeled as independent Bernoulli random variables.}
  \label{fig:cSBM}
  \vspace{-\intextsep}
\end{wrapfigure}
\textit{Description of the cSBM. }Using our Assumption~\ref{conj:1}, we analyze the case where the graph and node features are generated according to a cSBM~\citep{deshpande2018contextual} (see Figure~\ref{fig:cSBM}). For simplicity, we use the most straightforward two-cluster cSBM. Let  be the labels of nodes in a graph. We denote the size of class  by . We also assume that the classes are balanced, i.e., . The node features  are independent -dimensional Gaussian random vectors, such that  if  and  if . The adjacency matrix of the cSBM is denoted by , and is clearly symmetric. All edges are drawn according to independent Bernoulli random variables, so that  if  and  if . Our analysis is inspired by~\cite{baranwal2021graph} and~\cite{li2019optimizing}, albeit their definitions of graph convolutions and random walks differ from those in PIFA. For our subsequent analysis, we also make use of the following standard assumption and define the notion of \emph{effect size}.
\begin{assumption}\label{ass:1}
	. . . .
\end{assumption}
Note that~\cite{baranwal2021graph} also poses constraints on  and . In contrast, we do not require  to hold ~\citep{baranwal2021graph,li2019optimizing} so that we can address graph structures that are either homophilic or heterophilic. Due to the difference between PIFA and standard graph convolution, we require  to be larger compared to the corresponding values used in~\cite{baranwal2021graph}.

\begin{definition}\label{def:eff_size}
	For cSBM, the effect size of the two centroids of the node features  of the two different classes is defined as
	
\end{definition}
In the standard definition of effect size, the mean difference is divided by the standard deviation of a class, as the latter is assumed to be the same for both classes. We use the sum of both standard deviations to prevent any ambiguity in our definition. Note that for the case of isotropic Gaussian distributions, the larger the effect size the larger the separation of the two classes.

\textit{Theoretical results. }We are now ready to state our main theoretical result, which asserts that the effect size of centroids for PIFA embeddings  is asymptotically larger than that obtained from the original node features. Our Theorem~\ref{thm:main} provides strong evidence that using PIFA in XR-Transformers offers improved clustering results and consequently, better feature quality.
\begin{theorem}\label{thm:main}
	For the cSBM and under Assumption~\ref{ass:1}, the effect size of the two centroids of the node features  of the two different classes is . Moreover, the effect size of the two centroids of the PIFA embedding  of the two different classes, conditioned on an event of probability at least  for some constant , is .
\end{theorem}
We see that although two nodes  from the same class have the same neighborhood vectors in expectation, , their Hamming distance can be large in practice. This finding is formally characterized in Proposition~\ref{prop:Bino}.
\begin{proposition}\label{prop:Bino}
	For the cSBM and under Assumption~\ref{ass:1}, the Hamming distance between  and  with  is  with probability at least  for some .
\end{proposition}
Hence, directly using neighborhood vectors for self-supervision is not advisable. Our result also agrees with findings from the XMC literature~\citep{chang2020taming}. It is also intuitively clear that averaging neighborhood vectors from the same class can reduce the variance, which is approximately performed by clustering based on node representations (in our case, via a PIFA embedding). This result establishes the importance of clustering within the XR-Transformer approach and for the SSL neighborhood prediction task.

\vspace{-0.3cm}
\section{Experiments}\label{sec:exp}

\begin{table}[t!]
\caption{Basic statistics of the OGB benchmark datasets~\citep{hu2020open}.}
\vspace{0.1cm}
\label{tab:data-stats}
\centering
\resizebox{\columnwidth}{!}{\begin{tabular}{@{}lrrccc@{}}
    \toprule
    & \textbf{\#Nodes} & \textbf{\#Edges} & \textbf{Avg. Node Degree} & \textbf{Split ratio (\%)} & \textbf{Metric} \\
    \midrule
ogbn-arxiv      &     169,343 &     1,166,243 & 13.7 & 54/18/28 & Accuracy        \\
ogbn-products   &   2,449,029 &    61,859,140 & 50.5 &   8/2/90 & Accuracy        \\
ogbn-papers100M & 111,059,956 & 1,615,685,872 & 29.1 & 78/8/14 & Accuracy        \\
    \bottomrule
\end{tabular}}
\vspace{-0.15in}
\end{table}

\paragraph{Evaluation Datasets.}
We consider node classification as our downstream task and evaluate GIANT on three large-scale OGB datasets~\citep{hu2020open} with available raw text: ogbn-arxiv, ogbn-products, and ogbn-papers100M. The parameters of these datasets are given in Table~\ref{tab:data-stats} and detailed descriptions are available in the Appendix~\ref{apx:exp-data}. Following the OGB benchmarking protocol, we report the average test accuracy and the corresponding standard deviation by repeating 3 runs of each downstream GNN model.

\paragraph{Evaluation Protocol.}
We refer to our actual implementation as GIANT-XRT since the multi-scale neighborhood prediction task in the proposed GIANT framework is solved by an XR-Transformer. In the pre-training stage, GIANT-XRT learns a raw text encoder by optimizing the self-supervised neighborhood prediction objective, and generates a fine-tuned node embedding for later stages.
For the node classification downstream tasks, we input the node embeddings from GIANT-XRT into several different GNN models. One is the multi-layer perceptron (MLP), which does not use graph information. Two other methods are GraphSAGE~\citep{hamilton2017inductive}, which we applied to ogbn-arxiv, and GraphSAINT~\citep{graphsaint-iclr20}, which we used for ogbn-products as it allows for mini-batch training. Due to scalability issues, we used Simple Graph Convolution (SGC)~\citep{wu2019simplifying} for ogbn-papers100M. We also tested the state-of-the-art GNN for each dataset.

At the time we conducted the main experiments (07/01/2021), the top-ranked model for ogbn-arxiv was RevGAT\footnote{\url{https://github.com/lightaime/deep_gcns_torch/tree/master/examples/ogb_eff/ogbn_arxiv_dgl}}~\citep{li2021training} and the top-ranked model for ogbn-products was SAGN\footnote{\url{https://github.com/skepsun/SAGN_with_SLE}}~\citep{sun2021scalable}. When we conducted the experiment on ogbn-papers100M (09/10/2021), the top-ranked model for ogbn-papers100M was GAMLP\footnote{\url{https://github.com/PKU-DAIR/GAMLP}}~\citep{zhang2021graph} (Since then, the highest reported accuracy was improved by  for ogbn-arxiv and  for ogbn-products; both of these improvements fall short compared to those offered by GIANT). For all evaluations, we use publicly available implementations of the GNNs. For RevGAT, we report the performance of the model with and without self knowledge distillation; the former setting is henceforth referred to as +SelfKD. For SAGN, we report results with the self-label-enhanced (SLE) feature, and denote them by SAGN+SLE. For GAMLP, we report results with and without Reliable Label Utilization (RLU); the former is denoted as GAMLP+RLU.

\paragraph{SSL GNN Competing Methods.}
We compare GIANT-XRT to methods that rely on graph-agnostic feature inputs and use node embeddings generated by various SSL GNNs modules. The graph-agnostic features are either default features available from the OGB datasets (denoted by OGB-feat) or obtained from plain BERT embeddings (without fine-tuning) generated from raw text (denoted by BERT). For OGB-feat combined with downstream GNN methods, we report the results from the OGB leaderboard (and denote them by ).
For the SSL GNNs modules, we test three frequently-used methods:
(Variantional) Graph AutoEncoders~\citep{kipf2016variational} (denoted by (V)GAE); Deep Graph Infomax~\citep{velickovic2019deep} (denoted by DGI); and GraphZoom~\citep{deng2020graphzoom} (denoted by GZ).
The hyper-parameters of SSL GNNs modules are given in the Appendix~\ref{apx:hparams-ssl-baseline}.
For all reported results, we use ,  and  (c.f. Figure~\ref{fig:pipelines}) to denote which framework the method belongs to. Note that  refers to our approach. The implementation details and hyper-parameters of GIANT-XRT can be founded in the Appendix~\ref{apx:hparams-giant-xrt}.

\begin{table}[t!]
\caption{Results for the obgn-arxiv and ogbn-products datasets. Mean accuracy ()  one standard deviation. Boldfaced numbers indicate the best performances of downstream models, while underlined numbers indicate the best performance of models with a standard GNN pipeline for downstream models using  and . Methods under  (GIANT framework) are part of the ablation study.
}
\vspace{0.1cm}
\centering
\setlength{\tabcolsep}{2.5pt}
\label{tab:ogbn-arxiv}
\scriptsize
\begin{tabular}{@{}cccccc|ccc@{}}
\toprule
\multicolumn{1}{l}{} &
  Dataset &
  \multicolumn{4}{c|}{ogbn-arxiv} &
  \multicolumn{3}{c}{ogbn-products} \\ \midrule
\multicolumn{1}{l}{} &
   &
  MLP &
  GraphSAGE &
  RevGAT &
  RevGAT+SelfKD &
  MLP &
  GraphSAINT &
  SAGN+SLE \\ \midrule
\multirow{2}{*}{} &
  OGB-feat &
  55.50 ± 0.23 &
  {\ul 71.49 ± 0.27 } &
  {\ul 74.02 ± 0.18} &
  {\ul 74.26 ± 0.17} &
  {\ul 61.06 ± 0.08} &
  79.08 ± 0.24 &
  {\ul 84.28 ± 0.14} \\
 &
  BERT &
  {\ul 62.91 ± 0.60} &
  70.97 ± 0.33 &
  73.59 ± 0.10 &
  73.55 ± 0.41 &
  60.90 ± 1.09 &
  {\ul 79.55 ± 0.85} &
  83.11 ± 0.18 \\ \midrule
\multirow{8}{*}{} &
  OGB-feat+GZ &
  {\ul 70.95 ± 0.38} &
  71.41 ± 0.09 &
  72.42 ± 0.16 &
  72.50 ± 0.08 &
  74.19 ± 0.55 &
  78.38 ± 0.21 &
  79.78 ± 0.11 \\
 &
  BERT+GZ &
  70.46 ± 0.21 &
  71.24 ± 0.19 &
  72.33 ± 0.06 &
  72.30 ± 0.20 &
  OOM &
  OOM &
  OOM \\
 &
  OGB-feat+DGI &
  56.02 ± 0.16 &
  71.72 ± 0.26 &
  73.48 ± 0.14 &
  73.90 ± 0.26 &
  70.54 ± 0.13 &
  79.26 ± 0.16 &
  81.59 ± 0.14 \\
 &
  BERT+DGI &
  59.42 ± 0.38 &
  72.15 ± 0.06 &
  73.24 ± 0.25 &
  73.60 ± 0.21 &
  73.62 ± 0.23 &
  81.29 ± 0.41 &
  82.90 ± 0.21 \\
 &
  OGB-feat+GAE &
  56.47 ± 0.08 &
  72.00 ± 0.27 &
  73.70 ± 0.28 &
  74.06 ± 0.10 &
  74.81 ± 0.22 &
  78.23 ± 0.10 &
  82.85 ± 0.11 \\
 &
  BERT+GAE &
  62.11 ± 0.32 &
  72.72 ± 0.17 &
  {\ul 74.26 ± 0.20} &
  {\ul 74.48 ± 0.15} &
  78.42 ± 0.14 &
  82.74 ± 0.16 &
  {\ul 84.42 ± 0.04} \\
 &
  OGB-feat+VGAE &
  56.70 ± 0.20 &
  72.04 ± 0.29 &
  73.59 ± 0.17 &
  73.95 ± 0.09 &
  74.66 ± 0.10 &
  78.65 ± 0.20 &
  83.06 ± 0.06 \\
 &
  BERT+VGAE &
  62.48 ± 0.14 &
  {\ul 72.92 ± 0.02} &
  74.21 ± 0.01 &
  74.44 ± 0.09 &
  {\ul 78.81 ± 0.25} &
  {\ul 82.80 ± 0.11} &
  84.40 ± 0.09 \\ \midrule
\multicolumn{1}{l}{\multirow{5}{*}{}} &
  BERT+LP &
  67.33 ± 0.54 &
  66.61 ± 2.86 &
  75.50 ± 0.11 &
  75.75 ± 0.04 &
  73.83 ± 0.06 &
  81.66 ± 0.08 &
  82.33 ± 0.16 \\ \cmidrule(l){2-9} 
\multicolumn{1}{l}{} &
  NO TFIDF+ NO PIFA &
  69.33 ± 0.19 &
  73.41 ± 0.34 &
  74.95 ± 0.07 &
  75.16 ± 0.06 &
  74.16 ± 0.22 &
  80.70 ± 0.51 &
  81.63 ± 0.28 \\
\multicolumn{1}{l}{} &
  NO TFIDF+PIFA &
  72.74 ± 0.17 &
  74.43 ± 0.20 &
  75.88 ± 0.05 &
  76.06 ± 0.02 &
  78.91 ± 0.28 &
  81.54 ± 0.14 &
  82.22 ± 0.15 \\
\multicolumn{1}{l}{} &
  TFIDF+NO PIFA &
  71.74 ± 0.15 &
  74.09 ± 0.33 &
  75.56 ± 0.09 &
  75.85 ± 0.05 &
  79.37 ± 0.15 &
  83.83 ± 0.14 &
  85.01 ± 0.10 \\
\multicolumn{1}{l}{} &
  GIANT-XRT &
  \textbf{73.08 ± 0.06} &
  \textbf{74.59 ± 0.28} &
  \textbf{75.96 ± 0.09} &
  \textbf{76.12 ± 0.16} &
  \textbf{79.82 ± 0.07} &
  \textbf{84.40 ± 0.17} &
  \textbf{85.47 ± 0.29} \\
  \bottomrule
  \vspace{-2em}
\end{tabular}


\vspace{-0.2cm}
\end{table}


\subsection{Main Results}

The results for the ogbn-arxiv and ogbn-products datasets are listed in Table~\ref{tab:ogbn-arxiv}. Our GIANT-XRT approach gives the best results for both datasets and all downstream models. It improves the accuracy of the top-ranked OGB leaderboard models by a large margin:   on ogbn-arxiv and  on ogbn-products. Using graph-agnostic BERT embeddings does not necessarily lead to good results (see the first two rows in Table~\ref{tab:ogbn-arxiv}). This shows that the improvement of our method is not merely due to the use of a more powerful language model, and establishes the need for self-supervision governed by graph information.
Another observation is that among possible combinations involving a standard GNN pipeline with a SSL module, BERT+(V)GAE offers the best performance. This can be attributed to exploiting the correlation between \emph{numerical} node features and the graph structure, albeit in a two-stage approach within the standard GNN pipeline. The most important finding is that using node features generated by GIANT-XRT leads to consistent and significant improvements in the accuracy of all tested methods, when compared to the standard GNN pipeline: In particular, on ogbn-arxiv, the improvement equals  for MLP and  for GraphSAGE; on ogbn-products, the improvement equals  for MLP and  for GraphSAINT. Figure~\ref{fig:improvement} in the Appendix~\ref{app:improvement} further illustrate the gain obtained by our GIANT-XRT over SOTA methods on OGB leaderboard.

Another important observation is that GIANT-XRT is highly scalable, which can be clearly observed on the example of the ogbn-papers100M dataset, for which the results are shown in Table~\ref{tab:ogbn-papers100M}. In particular, GIANT-XRT improves the accuracy of the top-ranked model, GAMLP-RLU, by a margin of . Furthermore, GIANT-XRT again consistently improves all tested downstream methods on the ogbn-papers100M dataset. As a final remark, we surprisingly find that combining MLP with GIANT-XRT greatly improves the performance of the former learner on all datasets. It becomes just slightly worse then GIANT-XRT+GNNs and can even outperform the GraphSAGE and GraphSAINT methods with default OGB features on ogbn-arxiv and ogbn-products datasets. This is yet another positive property of GIANT, since MLPs are low-complexity and more easily implementable than other GNNs.



\begin{table}[t]
\setlength{\tabcolsep}{2.75pt}
\caption{Results for the obgn-papers100M dataset. Mean accuracy ()  one standard deviation. Boldfaced values indicate the best performance amongst the tested downstream models.}
\vspace{0.1cm}
\label{tab:ogbn-papers100M}
\centering
\small
\begin{tabular}{@{}ccccccc@{}}
    \toprule
    & ogbn-papers100M & MLP & SGC & GAMLP & GAMLP+RLU \\
    \midrule
    \multicolumn{1}{c}{\multirow{2}{*}{}} & OGB-feat
        & 47.24 ± 0.31 & 63.29 ± 0.19 & 67.71 ± 0.20 & 68.25 ± 0.19 \\
    \multicolumn{1}{c}{} & BERT
        & 47.24 ± 0.39 & 61.69 ± 0.29 & 66.25 ± 0.05 & 67.15 ± 0.07 \\
    \midrule
     & GIANT-XRT
        & \textbf{61.10 ± 0.19} & \textbf{66.10 ± 0.13} & \textbf{69.16 ± 0.08} & \textbf{69.67 ± 0.04} \\
    \bottomrule
\end{tabular}
\vspace{-0.2cm}
\end{table}

\subsection{Ablation study}
We also conduct an ablation study of the GIANT framework to determine the relevance of each module involved. The first step is to consider alternatives to the proposed multi-scale neighborhood prediction task: In this case, we fine-tune BERT with a SSL link prediction approach, which we for simplicity refer to as BERT+LP.
In addition, we examine how the PIFA embedding affects the performance of GIANT-XRT and how more informative node features (TFIDF) can improve the clustering steps. First, recall that in GIANT-XRT, we use TFIDF features from raw text to construct PIFA embeddings. We subsequently use the term ``NO TFIDF'' to indicate that we replaced the TFIDF feature matrix by an identity matrix, which contain no raw text information. The term ``TFIDF+NO PIFA'' is used to refer to the setting where only raw text information (node attributes) is used to perform hierarchical clustering. Similarly, ``NO TFIDF+PIFA'' indicates that we  only use normalized neighborhood vectors (graph structure) to construct the hierarchical clustering. If both node attributes and graph structure are ignore, the result is a random clustering. Nevertheless, we keep the same sizes of clusters at each level in the hierarchical clustering.

The results of the ablation study are listed under rows indexed by  in Table~\ref{tab:ogbn-arxiv} for ogbn-arxiv and ogbn-products datasets. They once again confirm that GIANT-XRT consistently outperforms other tested methods. For BERT+LP, we find that it has better performance on ogbn-arixv compared to that of the standard GNN pipeline but a worse performance on ogbn-products. This shows that using link prediction to fine-tune BERT in a self-supervised manner is not robust in general, and further strengthens the case for using neighborhood instead of link prediction. With respect to the ablation study of GIANT-XRT, we see that NO TFIDF+NO PIFA indeed gives the worst results. Using node attributes (TFIDF features) or graph information (PIFA) to construct the hierarchical clustering in GIANT-XRT leads to performance improvements that can be seen from Table~\ref{tab:ogbn-arxiv}. Nevertheless, combining both as done in GIANT-XRT gives the best results. Moreover, one can observe that using PIFA always gives better results compared to the case when PIFA is not used. It aligns with our theoretical analysis, which shows that PIFA embeddings lead to better hierarchical clusterings.







\subsubsection*{Acknowledgement}
The authors thank the support from Amazon and the Amazon Post Internship Fellowship. Cho-Jui Hsieh is partially supported by NSF under IIS-2008173 and IIS-2048280. This work was funded in part by the NSF grant 1956384.

\section{Ethics Statement}
We are not aware of any potential ethical issues regarding our work.
\section{Reproducibility Statement}
We provide our code in the supplementary material along with an easy-to-follow description and package dependency for reproducibility. Our experimental setting is stated in Section~\ref{sec:exp} and details pertaining to hyperparameters and computational environment are described in the Appendix. All tested methods are integrated in our code:
\url{https://github.com/amzn/pecos/tree/mainline/examples/giant-xrt}.


\bibliography{Ref}
\bibliographystyle{iclr2022_conference}


\clearpage
\appendix
\section*{Appendix}
\section{Conclusions}
We introduced a novel self-supervised learning framework for graph-guided numerical node feature extraction from raw data, and evaluated it within multiple GNN pipelines. Our method, termed GIANT, for the first time successfully resolved the issue of graph-agnostic numerical feature extraction. We also described a new SSL task, neighborhood prediction, established a connection between the task and the XMC problem, and solved it using XR-Transformers. We also examined the theoretical properties of GIANT in order to evaluate the advantages of neighborhood prediction over standard link prediction, and to assess the benefits of using XR-Transformers. Our extensive numerical experiments, which showed that GIANT consistently improves state-of-the-art GNN models, were supplemented with an ablation study that aligns with our theoretical analysis.

\section{Proof of Theorem~\ref{thm:main}}


Throughout the proof, we use  to denote the mean of node feature from class  and  for class . We choose to keep this notation to demonstrate that our setting on mean can be easily generalized. The choice of the high probability events will be clear in the proof.

The proof for the effect size of centroid for node feature  is quite straightforward from the Definition~\ref{def:eff_size}. By directly plugging in the mean and standard deviation, we have

The last equality is due to our assumption that both  are both constants.

To prove the effect size of centroid for PIFA embedding , we need to first introduce some standard concentration results for sum of Bernoulli and Gaussian random variables.

\begin{lemma}	[Hoeffiding's inequality~\citep{hoeffding1994probability}]\label{lma:Hoeffding}
	Let , where  are i.i.d. Bernoulli random variable with parameter . Then for any , we have
	
\end{lemma}

\begin{lemma}	[Concentration of sum of Gaussian]\label{lma:gauss_concentration}
	Let , where  are i.i.d. Gaussian random variable with zero mean and standard deviation . Then for some constant , we have
	
\end{lemma}

Now we are ready to prove our claim. Recall that the definition of PIFA embedding  is as follows:

We first focus on analyzing the vector . We denote  and . Without loss of generality, we assume . The conditional expectation of it is as follows:

Next, by leveraging Lemma~\ref{lma:Hoeffding}, we have

By choosing  for some constant , we know that with probability at least , . Finally, by our Assumption~\ref{ass:1}, we know that . Thus, we arrive to the conclusion that with probability at least , .

Following the same analysis, we can prove that with probability at least , . The only difference is that we are dealing with  random variables in this case as there's no self-loop. Nevertheless,  so the result is the same. Note that we need to apply union bound over  error events ( and  cases for  and  respectively). Together we know that the error probability is upper bounded by  for some new constant . Hence, we characterize the mean of  on a high probability event .

Next we need to analyze its norm. By direct analysis and condition on , we have

where  are i.i.d. Gaussian random variables with zero mean,  standard deviation and  stands for equal in distribution. Then by Lemma~\ref{lma:gauss_concentration} we know that with probability at least  for some constant 

This is because condition on , we are summing over  Gaussian random variables. Recall that condition on our high probability event , . Thus, we know that for some , with probability at least  we have .  Again, recall that both , thus together we have

Again, we need to apply union bound over  error events, which result in the error probability  since  and  in our Assumption~\ref{ass:1}. We denote the corresponding high probability event to be . Note that the same analysis can be applied to the case , where the result for the norm is the same and the result for  would be just swapping  and . Combine all the current result, we know that with probability at least  for some , the PIFA embedding  equals to the following

Hence, the centroid distance would be


Now we turn to the standard deviation part. Specifically, we will characterize the following quantity (again, recall that we assume  w.l.o.g.).

Recall that the latter part is the centroid for nodes with label . Hence, by characterize this quantity we can understand the deviation of PIFA embedding around its centroid. From the analysis above, we know that given , we have

For the terms  and , we already derive their concentration results above. Plug in those results, the first term becomes


The second term becomes

where the last equality is from our Assumption~\ref{ass:1} that  are constants. Together we show that the deviation of nodes from their centroid is of scale . The similar result holds for the case . Together we have shown that the standard deviation of  is  on the high probability event . Hence, the effect size for the PIFA embedding is  with probability at least  for some constant , which implies that PIFA embedding gives a better clustered node representation. Thus, it is preferable to use PIFA embedding and we complete the proof.
\section{Proof of Proposition~\ref{prop:Bino}}
Note that under the setting of cSBM and the Assumption~\ref{ass:1}, the Hamming distance of  for  is a Poisson-Binomial random variable. More precisely, note that

where they are all independent. Hence, we have

where  denotes the Hamming distance of  and  stands for the Binomial random variable with  trials and the success probability is . By leveraging the Lemma~\ref{lma:Hoeffding}, we know that for a random variable , we have

 Note that the function  is monotonic increasing for  and has maximum at . Combine with Assumption~\ref{ass:1} we know that . Hence, by choosing  for some constant , with probability at least  we have
 
Finally, by noting the fact that with probability  we have . Hence, by showing  is of order  with probability at least  implies that the Hamming distance of  is of order  with at least the same probability. Together we complete the proof.
 
\section{Proof of Lemma~\ref{lma:gauss_concentration}}
By Chernoff bound, we have

By the i.i.d assumption, we have

Note that the moment generating function (MGF) of a zero-mean,  standard deviation Gaussian random variable is . Hence we have

By minimizing the upper bound with respect to , we can choose . Plug in this choice of  we have

Finally, by choosing  for some , applying the same bound for the other side and the union bound, we complete the proof.


\section{Experimental detail}

\subsection{Datasets}
\label{apx:exp-data}



In this work, we choose node classification as our downstream task to focus. We conduct experiments on three large-scale datasets, ogbn-arxiv, ogbn-products and ogbn-papers100M as these are the only three datasets with raw text available in OGB. Ogbn-arxiv and ogbn-papers100M~\citep{wang2020microsoft,hu2020open} are citation networks where each node represents a paper. The corresponding input raw text consists of titles and abstracts and the node labels are the primary categories of the papers. Ogbn-products~\citep{chiang2019cluster,hu2020open} is an Amazon co-purchase network where each node represents a product. The corresponding input raw text consists of titles and descriptions of products. The node labels are categories of products. 

\subsection{Hyper-parameters of SSL GNN Modules}
\label{apx:hparams-ssl-baseline}

The implementation of (V)GAE and DGI are taken from Pytorch Geometric Library~\citep{fey2019fast}. Note that due to the GPU memory constraint, we adopt GraphSAINT~\citep{graphsaint-iclr20} to (V)GAE and DGI for ogbn-products. GraphZoom is directly taken from the official repository\footnote{\url{https://github.com/cornell-zhang/GraphZoom}}. For all downstream GNNs in the experiment, we average the results over three independent runs. The only exception is OGB-feat + downstream GNNs, where we directly take the results from OGB leaderboard. Note that we also try to repeat the experiment of OGB-feat + downstream GNNs, where the accuracy is similar to the one reported on the leaderboard. To prevent confusion we decide to take the results from OGB leaderboard for comparison. For the BERT model used throughout the paper, we use ``bert-base-uncased'' downloaded from HuggingFace~\footnote{\url{https://huggingface.co/bert-base-uncased}}. For the methods used in the SSL GNNs module, we try our best to follow the default setting. We slightly optimize some hyperparameters (such as learning rate, max epochs...etc) to ensure the training process converge. To ensure the fair comparison, we fix the output dimension for all SSL GNNs as  which is the same as bert-base-uncased and XR-Transformer.

\subsection{Hyper-parameters of GIANT-XRT and BERT+LP}
\label{apx:hparams-giant-xrt}

\paragraph{Pre-training of GIANT-XRT.}
In Table~\ref{tb:xrt-hparams}, we outline the pre-training hyper-parameter of GIANT-XRT for all three OGB benchmark datasets.
We mostly follow the convention of XR-Transformer~\citep{jiong2021fast} to set the hyper-parameters.
For ogbn-arxiv and ogbn-products datasets, we use the full graph adjacency matrix as the XMC instance-to-label matrix , where  is number of nodes in the graph.
For ogbn-papers100M, we sub-sample ~M (out of 111M) most important nodes based on page rank scores of the bipartite graph~\citep{he2016birank}.
The resulting XMC instance-to-label matrix  has M of rows, M of columns, and ~B of edges.
The PIFA node embedding for hierarchical clustering is constructed by aggregating its neighborhood nodes' TF-IDF features.
Specifically, the PIFA node embedding is a 4.2M high-dimensional sparse vector, consisting of 1M of word unigram, 3M of word bigram, and 200K of character triagram. 
Finally, for ogbn-arxiv and ogbn-products, we use + negative sampling to pre-train XR-Transformer where the model-aware negatives (MAN) are selected from top 20 model's predictions.
Because of the extreme scale of ogbn-papers100M, we consider  only to avoid excessive CPU memory consumption on the GPU machine.


\begin{table*}[h]
    \centering
    \caption{Hyper-parameters of GIANT-XRT.
         defines the structures of the hierarchical label trees.
         is the maximum learning rate in pre-training.
         is the number of optimization steps for each layer of HLT, respectively.
         is total number of batch size when using  Nvidia V100 GPUs.
         is the negative sampling strategy in XR-Transformer.
         is the th layer of  where we take the Transformer encoder to generate node embeddings as input for downstream GNN models.}
    \label{tb:xrt-hparams}
	\resizebox{1.0\textwidth}{!}{
    \begin{tabular}{l|rrrrrrr}
        \toprule
        Dataset         &                      &         &  &  &         &  \\
        \midrule
        ogbn-arxiv      &   32-128-512-2048         &  &   2,000    & 256 & + & 4   \\
        ogbn-products   &  128-512-2048-8192-32768  &  &  10,000    & 256 & + & 2   \\
        ogbn-papers100M &  128-2048-32768           &  & 100,000    & 512 &        & 3   \\
    \bottomrule
    \end{tabular}
    }
\end{table*}

\paragraph{Pre-training on BERT+LP.}
To verify the effectiveness of multi-scale neighborhood prediction loss, we consider learning a Siamese BERT encoder with the alternative Link Prediction loss for pre-training, hence the name BERT+LP.
We implement BERT+LP baseline with the triplet loss~\citep{balntas2016learning} as we empirically observed it has better performance than other loss functions for the link prediction task. 
We sample one positive pair and one negative pair for each node as training samples for each epoch, and train the model until the loss is converged.


\subsection{Hyper-parameters of downstream methods}
For the downstream models, we optimize the learning rate within  for all models. For MLP, GraphSAGE and GraphSAINT, we optimize the number of layers within . For RevGAT, we keep the hyperparameter choice the same as default. For SAGN, we also optimize the number of layers within . For GAMLP, we directly adopt the setting from the official implementation. Note that all hyperparameter tuning applies for all pre-trained node features (,  and ). 

\subsection{Computational Environment}
All experiments are conducted on the AWS p3dn.24xlarge instance, consisting of
96 Intel Xeon CPUs with 768 GB of RAM and 8 Nvidia V100 GPUs with 32 GB of memory each.

\subsection{Illustration on the improvement of GIANT-XRT}\label{app:improvement}
See Figure~\ref{fig:improvement}.
\begin{figure}[h]
\centering
  \includegraphics[trim={0 0cm 0cm 0cm},clip,width=0.9\linewidth]{ogbn-arxiv.png}
  \includegraphics[trim={0 0cm 0cm 0cm},clip,width=0.9\linewidth]{ogbn-papers100M.png}
  \includegraphics[trim={0 0cm 0cm 0cm},clip,width=0.9\linewidth]{ogbn-products.png}
\caption{To further demonstrate that our GIANT-XRT indeed achieves a significant improvement over state-of-the-art methods, we plot the performance of top 8 models on OGB leaderboard (As of Nov. 11th, 2021). Note that our results on ogbn-products is better than those reported in Table~\ref{tab:ogbn-arxiv}, since we adopt the latest choice of hyperparameters provided in SAGN GitHub repository.}\label{fig:improvement}
\vspace{-0.1in}
\end{figure}






\end{document}
