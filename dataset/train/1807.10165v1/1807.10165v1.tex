\documentclass[runningheads]{llncs}

\usepackage{color}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[labelformat=simple]{subfig}
\usepackage[ruled,lined,linesnumbered]{algorithm2e}
\usepackage{algorithmic}
\usepackage{blindtext}
\usepackage{enumitem}
\usepackage{multirow}
\usepackage[labelformat=simple]{subfig}
\usepackage{threeparttable/threeparttable}
\usepackage{bm}
\usepackage[normalem]{ulem}
\usepackage{hyperref}
\usepackage{amssymb}\usepackage{pifont}\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}

\newcommand{\etal}{\mbox{\emph{et al.}}}
\newcommand{\ie}{\mbox{\emph{i.e.,\ }}}
\newcommand{\bd}[1]{\textbf{#1}}


\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\newcommand{\tpm}[1]{\resizebox{4mm}{!}{\Large\raisebox{1mm}{}}}

\usepackage{array}
\usepackage{hyperref}

\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}

\newenvironment{jlred}{\color{red}}{}
\newenvironment{jlblue}{\color{blue}}{}
\newenvironment{jlorange}{\color{orange}}{}
\newenvironment{jlgreen}{\color{green}}{}
\newenvironment{jlgrey}{\color{grey}}{}

\begin{document}
\title{NablaNet: A Nested Neural Network Architecture\\for Medical Image Segmentation}
\title{-Net: A Nested Ensemble of Neural Networks for Medical Image Segmentation}
\title{-Net: A Upside-down Triangulated Neural Network Architecture for Medical Image Segmentation}
\title{UNet++: A Nested U-Net Architecture\\for Medical Image Segmentation}
\titlerunning{UNet++: A Nested U-Net Architecture}
\author{
    Zongwei Zhou \and
    Md Mahfuzur Rahman Siddiquee \and \\
    Nima Tajbakhsh \and
    Jianming Liang
}
\authorrunning{Z. Zhou, \etal}


\institute{
    Arizona State University\\
    \email{\{zongweiz,mrahmans,ntajbakh,jianming.liang\}@asu.edu}
}
\maketitle              

\begin{abstract}
  In this paper, we present UNet++, a new, more powerful architecture for medical image segmentation. Our architecture is essentially a deeply-supervised encoder-decoder network where the encoder and decoder sub-networks are connected through a series of nested, dense  skip pathways. The re-designed skip pathways aim at reducing the semantic gap between the feature maps of the encoder and decoder sub-networks. We argue that the optimizer would deal with an easier learning task when the feature maps from the decoder and encoder networks are semantically similar. We have evaluated UNet++ in comparison with U-Net and wide U-Net architectures across multiple medical image segmentation tasks: nodule segmentation in the low-dose CT scans of chest, nuclei segmentation in the microscopy images, liver segmentation in abdominal CT scans, and polyp segmentation in colonoscopy videos. Our experiments demonstrate that UNet++ with deep supervision achieves an average IoU gain of 3.9 and 3.4 points over U-Net and wide U-Net, respectively.



  \iffalse
  The state-of-the-art models for medical image segmentation are variants of U-Net and fully convolutional networks (FCN). They share a key component: skip connections, which not only migrate context spatial information from layers in the contracting path to layers in the expanding path, but also help overcome the vanishing gradient problem. However, we have observed information gaps in the gradient flow through the skip connections. To address this issue, we introduce a new method for smooth transition of features through long skip connections, making the following three key contributions: (1) We improve long skip connections by introducing intermediate layers to allow the features to adapt progressively through skip connections and mitigate information gaps in long skip connections. (2) We show generalized solution on how any segmentation network with skip connections like U-Net, FCN, and Mask-RCNN can be improved by progressive feature adaption through long skip connections. (3) Our method improves both semantic and instance segmentation. For our experiments, we took different variants of U-Net, FCNs, Mask-RCNN and replaced the long skip connections with several intermediate convolutional layers. Throughout the paper, these resultant new architectures are known as Nested Neural Network (Nest-Net). We have demonstrated that replacing long skip connections between contracting and expanding path with progressive feature adaptation significantly outperforms the original U-Net, FCNs, Mask-RCNN and their variants in several applications across diseases and modalities. This performance is attributed to several advantages of our approach. With a modest increase in parameters, our approach (1) smooths multiple resolution feature propagation, (2) encourages feature map reuse, (3) reduces information gap through long skip connections, (4) requires less data augmentation, (5) yields more precise segmentation, and (6) accelerate convergence speed by utilizing transfer learning in base network.
  \fi



\end{abstract}

\section{Introduction}
\label{sec:introduction}











The state-of-the-art models for image segmentation are variants of the encoder-decoder architecture like U-Net~\cite{ronneberger2015u} and fully convolutional network (FCN)~\cite{long2015fully}. These encoder-decoder networks used for segmentation share a key similarity: skip connections, which combine deep, semantic, coarse-grained feature maps from the decoder sub-network with shallow, low-level, fine-grained feature maps from the encoder sub-network. The skip connections have proved effective in recovering fine-grained details of the target objects; generating segmentation masks with fine details even on complex background. Skip connections is also fundamental to the success of instance-level segmentation models such as Mask-RCNN, which enables the segmentation of occluded objects. Arguably, image segmentation in natural images has reached a satisfactory level of performance, but do these models meet the strict segmentation requirements of medical images?


Segmenting lesions or abnormalities in medical images demands a higher level of accuracy than what is desired in natural images. While a precise segmentation mask may not be critical in natural images, even marginal segmentation errors in medical images can lead to poor user experience in clinical settings. For instance, the subtle spiculation patterns around a nodule may indicate nodule malignancy; and therefore, their exclusion from the segmentation masks would lower the credibility of the model from the clinical perspective. Furthermore, inaccurate segmentation may also lead to a major change in the subsequent computer-generated diagnosis. For example, an erroneous measurement of nodule growth in longitudinal studies can result in the assignment of an incorrect Lung-RADS category to a screening patient. \iffalse staging of cancer or imprecise measurement of lung volume during the inhalation and exhalation may lead to the misdiagnosis of chronic obstructive pulmonary diseases.\fi It is therefore desired to devise more effective image segmentation architectures that can effectively recover the fine details of the target objects in medical images.



To address the need for more accurate segmentation in medical images, we present UNet++, a new segmentation architecture based on nested and dense skip connections.
The underlying hypothesis behind our architecture is that the model can more effectively capture fine-grained details of the foreground objects when high-resolution feature maps from the encoder network are gradually enriched prior to fusion with the corresponding semantically rich feature maps from the decoder network. We argue that the network would deal with an easier learning task when the feature maps from the decoder and encoder networks are semantically similar. This is in contrast to the plain skip connections commonly used in U-Net, which directly fast-forward high-resolution feature maps from the encoder to the decoder network, resulting in the fusion of semantically dissimilar feature maps. According to our experiments, the suggested architecture is effective, yielding significant performance gain over U-Net and wide U-Net.







\section{Related Work}
\label{sec:related_works}

\iffalse
Because of the widespread interest as evidenced by an IEEE TMI special issue~\cite{greenspan2016guest} and two recent books~\cite{ZhouGreenspanShen2016,lu2017deep}, the literature in deep learning for medical image analysis~\cite{badrinarayanan2017segnet,deeplabv3plus2018,jegou2017one,zhou2017fine} is expanding very rapidly; therefore, due to space, we focus on the most relevant works only.
\fi

Long~\etal~\cite{long2015fully} first introduced fully convolutional networks (FCN), while U-Net was introduced by Ronneberger~\etal~\cite{ronneberger2015u}. They both share a key idea: skip connections.  In FCN, up-sampled feature maps are summed with feature maps skipped from the encoder, while U-Net concatenates them and add convolutions and non-linearities between each up-sampling step. The skip connections have shown to help recover the full spatial resolution at the network output, making fully convolutional methods suitable for semantic segmentation. Inspired by DenseNet architecture~\cite{huang2017densely}, Li~\etal~\cite{li2017h} proposed H-denseunet for liver and liver tumor segmentation. In the same spirit, Drozdzal\etal~\cite{drozdzal2016importance} systematically investigated the importance of skip connections, and introduced short skip connections within the encoder. Despite the minor differences between the above architectures, they all tend to fuse semantically dissimilar feature maps from the encoder and decoder sub-networks, which, according to our experiments, can degrade segmentation performance.



\iffalse However, none of the existing architectures have nested intermediate layers as we have proposed in this paper, which is critical to bridge the information gaps in the gradient flow through the skip connections, leading to outstanding performance in comparison with the U-Net, FCN, and its variants. \fi


The other two recent related works are GridNet~\cite{fourure2017residual} and Mask-RCNN~\cite{he2017mask}. GridNet is an encoder-decoder architecture wherein the feature maps are wired in a grid fashion, generalizing several classical segmentation architectures. GridNet, however, lacks up-sampling layers between skip connections; and thus, it does not represent UNet++. Mask-RCNN is perhaps the most important meta framework for object detection, classification and segmentation. We would like to note that UNet++ can be readily deployed as the backbone architecture in Mask-RCNN by simply replacing the plain skip connections with the suggested nested dense skip pathways. Due to limited space, we were not able to include results of Mask RCNN with  UNet++ as the backbone architecture; however, the interested readers can refer to the supplementary material for further details.


\iffalse
The other three recent related works are GridNet~\cite{fourure2017residual}, DLA~\cite{yu2017deep} and Mask-RCNN~\cite{he2017mask}. GridNet is an encoder-decoder architecture wherein the feature maps are wired in a grid fashion, generalizing several classical segmentation architectures. GridNet, however, lacks up-sampling layers between skip connections; and thus, it does not represent UNet++. Mask-RCNN is perhaps the most important meta framework for object detection, classification and segmentation. We would like to note that UNet++ can be readily deployed as the backbone architecture in Mask-RCNN by simply replacing the plain skip connections with the suggested nested dense skip pathways. Due to limited space, we were not able to include results of Mask RCNN with  UNet++ as the backbone architecture; however, the interested readers can refer to the supplementary material for further details.
\fi






\iffalse
We have reported preliminary results (unpublished) of our architecture for medical image segmentation, but it was limited to semantic segmentation using only U-Net. This paper is a significant extension, making four \bd{contributions}: (1) introducing intermediate layers into skip connections between contracting and expanding paths of general encoder-decoder network; (2) generalizing our method in several different backbone architecture and outperforming all backbone architectures; (3) Using transfer learning in backbone architectures to make training to converge faster; (4) improving both semantic and instance segmentation architecture.
\fi

\section{Proposed Network Architecture: UNet++}
\label{sec:methods}

\begin{figure}[t]
\begin{center}
\includegraphics[width=1.0\linewidth]{fig_nabla_net.png}
\end{center}
\caption{(a) UNet++ consists of an encoder and decoder that are connected through a series of nested dense convolutional blocks. The main idea behind UNet++ is to bridge the semantic gap between the feature maps of the encoder and decoder prior to fusion. For example, the semantic gap between (X,X) is bridged using a dense convolution block with three convolution layers. In the graphical abstract, black indicates the original U-Net, green and blue show dense convolution blocks on the skip pathways, and red indicates deep supervision. Red, green, and blue components distinguish  UNet++ from U-Net. (b) Detailed analysis of the first skip pathway of UNet++. (c) UNet++ can be pruned at inference time, if trained with deep supervision. }
\label{fig:network_architecture}
\end{figure}


\figurename~\ref{fig:network_architecture}a shows a high-level overview of the suggested architecture. As seen, UNet++ starts with an encoder sub-network or backbone followed by a decoder sub-network. What distinguishes UNet++ from U-Net (the black components in \figurename~\ref{fig:network_architecture}a) is the re-designed skip pathways (shown in green and blue) that connect the two sub-networks and the use of deep supervision (shown red).

\subsection{Re-designed skip pathways}
Re-designed skip pathways transform the connectivity of the encoder and decoder sub-networks. In U-Net, the feature maps of the encoder are directly received in the decoder; however, in  UNet++, they undergo a dense convolution block whose number of convolution layers depends on the pyramid level. For example, the skip pathway between nodes X and X consists of a dense convolution block with three convolution layers where each convolution layer is preceded by a concatenation layer that fuses the output from the previous convolution layer of the same dense block with the corresponding up-sampled output of the lower dense block. Essentially, the dense convolution block brings the semantic level of the encoder feature maps closer to that of the feature maps awaiting in the decoder. The hypothesis is that the optimizer would face an easier optimization problem when the received encoder feature maps and the corresponding decoder feature maps are semantically similar.


Formally, we formulate the skip pathway as follows: let  denote the output of node X where  indexes the down-sampling layer along the encoder and  indexes the convolution layer of the dense block along the skip pathway. The stack of feature maps represented by  is computed as



\iffalse

\fi




\noindent where function  is a convolution operation followed by an activation function,  denotes an up-sampling layer, and   denotes the concatenation layer. Basically, nodes at level   receive only one input from the previous layer of the encoder; nodes at level  receive two inputs, both from the encoder sub-network but at two consecutive levels; and nodes at level  receive  inputs, of which  inputs are the outputs of the previous  nodes in the same skip pathway and the last input is the up-sampled output from the lower skip pathway. The reason that all prior feature maps accumulate and arrive at the current node is because we make use of a dense convolution block along each skip pathway. \figurename~\ref{fig:network_architecture}b further clarifies Eq.~\ref{eq_unet} by showing how the feature maps travel through the top skip pathway of UNet++.





\iffalse
\begin{figure}[!ht]
\begin{center}
\includegraphics[width=0.9\linewidth]{./pics/nablanet_detailed.png}
\end{center}
\caption{}
\label{fig:network_architecture_detailed}
\end{figure}
\fi






\subsection{Deep supervision}
We propose to use deep supervision~\cite{lee2015deeply} in UNet++, enabling the model to operate in two modes: 1) accurate mode wherein the outputs from all segmentation branches are averaged; 2) fast mode wherein the final segmentation map is selected from only one of the segmentation branches, the choice of which determines the extent of model pruning and speed gain.  \figurename~\ref{fig:network_architecture}c shows how the choice of segmentation branch in fast mode results in architectures of varying complexity.


Owing to the nested skip pathways, UNet++ generates full resolution feature maps at multiple semantic levels, , which are amenable to deep supervision. We have added a combination of binary cross-entropy and dice coefficient as the loss function to each of the above four semantic levels, which is described as:



\iffalse
\noindent where  denotes the flatten predicted probabilities of  image,  indicates the flatten labels of  image, and  indicates the batch size.
\fi
\noindent where  and  denote the flatten predicted probabilities and the flatten ground truths of  image respectively, and  indicates the batch size.



In summary, as depicted in \figurename~\ref{fig:network_architecture}a, UNet++ differs from the original U-Net in three ways: 1) having convolution layers on skip pathways (shown in green), which bridges the semantic gap between encoder and decoder feature maps; 2) having dense skip connections on skip pathways (shown in blue), which improves gradient flow; and 3) having deep supervision (shown in red), which as will be shown in Section~\ref{sec:experiments} enables model pruning and improves or in the worst case achieves comparable performance to using only one loss layer.







\section{Experiments}
\label{sec:experiments}

\noindent{\bf{Datasets:}} As shown in Table~\ref{tab:dataset}, we use four medical imaging datasets for model evaluation, covering lesions/organs from different medical imaging modalities. For further details about datasets and the corresponding data pre-processing, we refer the readers to the supplementary material.

\iffalse
\noindent{\bf{Datasets:}} We used four datasets for performance evaluation, covering lesions: 1) \texttt{Nuclei} dataset\footnote{Data Science Bowl 2018: \href{https://www.kaggle.com/c/data-science-bowl-2018}{kaggle.com/c/data-science-bowl-2018/}} is provided by the Data Science Bowl 2018 segmentation challenge. The dataset is a collection of large amount of segmented nuclei images from different modalities. The ground truth provides separated masks for each nuclei. 2)  \texttt{Lung tumor} segmentation dataset\footnote{LIDC-IDRI: \href{https://wiki.cancerimagingarchive.net/display/Public/LIDC-IDRI}{wiki.cancerimagingarchive.net/display/Public/LIDC-IDRI/}} is provided by The Lung Image Database Consortium image collection (LIDC-IDRI)\iffalse~\cite{armato2011lung}\fi. The dataset consists of 1018 cases collected by seven academic centers and eight medical imaging companies. We exclude six cases which have issues on continuous ground truth mask slices for the tumors. Each case is a full 3D CT volume scan and the nodule has been marked as a volumetric binary mask. 3) Our colon \texttt{polyp} database contains 25 short colonoscopy videos with polyp from 25 different patients, and they are separated into the training set (15 videos), validation set (4 videos) and testing set (6 videos) at the patient level. 4) \texttt{Liver} segmentation dataset\footnote{LiTS - Liver Tumor Segmentation Challenge: \href{https://competitions.codalab.org/competitions/17094}{competitions.codalab.org/}} is provided by MICCAI 2018 LiTS Challenge. The original patient CT scan is 3D volume, we select the slices that liver area is greater 10 pixels of the whole slice (512512 pixels) based on the ground truth to make sure liver area is at least distinguishable in the 2D slice. In patient level, the dataset are divided into the training (100 patients), validation (15 patients), and testing (15 patients).
\fi



\begin{table}[t]
\centering
\caption{The image segmentation datasets used in our experiments.}
\label{tab:dataset} \begin{tabular}{P{0.16\linewidth}P{0.16\linewidth}P{0.18\linewidth}P{0.18\linewidth}P{0.28\linewidth}}
\hline
Dataset & Images & Input Size & Modality & Provider \\
\hline
cell nuclei & 670 & 9696 & microscopy & \tiny{\href{https://www.kaggle.com/c/data-science-bowl-2018}{Data Science Bowl 2018}} \\
colon polyp & 7,379 & 224224 & RGB video & \tiny{ASU-Mayo}~\cite{tajbakhsh2016convolutional,Zhou_2017_CVPR} \\
liver & 331  & 512512 & CT & \tiny{\href{https://competitions.codalab.org/competitions/17094}{MICCAI 2018 LiTS Challenge}} \\
lung nodule & 1,012 & 646464 & CT & \tiny{\href{https://wiki.cancerimagingarchive.net/display/Public/LIDC-IDRI}{LIDC-IDRI}}~\cite{armato2011lung}\\
\hline
\end{tabular}
\end{table}



\vspace{4pt}
\noindent{\bf{Baseline models:}} For comparison, we used the original U-Net and a customized wide U-Net architecture. We chose U-Net because it is a common performance baseline for image segmentation. We also designed a wide U-Net with similar number of parameters as our suggested architecture. This was to ensure that the performance gain yielded by our architecture is not simply due to increased number of parameters. Table~\ref{tab:wide-unet} details the U-Net and wide U-Net architecture.



\begin{table}[t]
\centering
\caption{Number of convolutional kernels in U-Net and wide U-Net.}
\label{tab:wide-unet} \begin{tabular}{P{0.25\linewidth}P{0.14\linewidth}P{0.14\linewidth}P{0.14\linewidth}P{0.14\linewidth}P{0.14\linewidth}}
\hline
encoder / decoder & X/X & X/X & X/X & X/X & X/X \\
\hline
U-Net & 32 & 64 & 128 & 256 & 512 \\
wide U-Net & 35 & 70 & 140 & 280 & 560 \\
\hline
\end{tabular}
\end{table}



\vspace{4pt}
\noindent{\bf{Implementation details:}} \iffalse All models were implemented using Keras library\footnote{Keras: \href{https://keras.io/}{https://keras.io/}}.\fi We monitored the Dice coefficient and Intersection over Union (IoU), and used {\em early-stop} mechanism on the validation set. We also used Adam optimizer with a learning rate of 3e-4. Architecture details for U-Net and wide U-Net are shown in Table~\ref{tab:wide-unet}. UNet++ is constructed from the original U-Net architecture.  All convolutional layers along a skip pathway (X) use  kernels of size 33 (or 333 for 3D lung nodule segmentation) where . To enable deep supervision, a 11 convolutional layer followed by a sigmoid activation function was appended to each of the target nodes:   . As a result, UNet++ generates four segmentation maps given an input image, which will be further averaged to generate the final segmentation map. More details can be founded at \href{https://github.com/MrGiovanni/Nested-UNet}{github.com/Nested-UNet}.



\begin{figure}[t]
\begin{center}
\includegraphics[width=0.96\linewidth]{fig-prediction_comparison.png}
\end{center}
\caption{Qualitative comparison between U-Net, wide U-Net, and UNet++, showing segmentation results for polyp, liver, and cell nuclei datasets (2D-only for a distinct visualization). }
\label{fig:predict_visualization}
\end{figure}





\vspace{4pt}
\noindent{\bf{Results:}}
Table~\ref{tab:main_results} compares U-Net, wide U-Net, and UNet++ in terms of the number parameters and segmentation accuracy for the tasks of lung nodule segmentation, colon polyp segmentation, liver segmentation, and cell nuclei segmentation. As seen, wide U-Net consistently outperforms U-Net except for liver segmentation where the two architectures perform comparably. This improvement is attributed to the larger number of parameters in wide U-Net. UNet++ without deep supervision achieves a significant performance gain over both U-Net and wide U-Net, yielding  average improvement of 2.8 and 3.3 points in IoU. UNet++ with deep supervision exhibits average improvement of 0.6 points over UNet++ without deep supervision. Specifically,  the use of deep supervision leads to marked improvement for liver and lung nodule segmentation, but such improvement vanishes for cell nuclei and colon polyp segmentation. This is because polyps and liver appear at varying scales in video frames and CT slices; and thus, a multi-scale approach using all segmentation branches (deep supervision) is essential for accurate segmentation. \figurename~\ref{fig:predict_visualization} shows a qualitative comparison between the results of U-Net, wide U-Net, and UNet++.




\vspace{4pt}
\noindent{\bf{Model pruning:}} Fig.~\ref{fig:prune_network} shows  segmentation performance of UNet++ after applying different levels of pruning. We use UNet++ L to denote UNet++ pruned at level  (see \figurename~\ref{fig:network_architecture}c for further details). As seen, UNet++ L achieves on average 32.2\% reduction in inference time while degrading IoU by only 0.6 points.  More aggressive pruning further reduces the inference time but at the cost of significant accuracy degradation.


\begin{table}[t]
\begin{center}
\begin{threeparttable}
\caption{Segmentation results (IoU: ) for U-Net, wide U-Net and our suggested architecture UNet++ with and without deep supervision (DS).}
\label{tab:main_results}
    \begin{tabular}{P{0.23\linewidth}P{0.12\linewidth}P{0.15\linewidth}P{0.15\linewidth}P{0.15\linewidth}P{0.15\linewidth}}
    \hline
    \multirow{2}*{Architecture} & \multirow{2}*{Params} & \multicolumn{4}{c}{Dataset} \\
    \cline{3-6}
     & & cell nuclei & colon polyp & liver & lung nodule \\
    \hline
    U-Net~\cite{ronneberger2015u}   & 7.76M & 90.77 & 30.08 & 76.62 &71.47 \\
    Wide U-Net & 9.13M  & 90.92 & 30.14 & 76.58 & 73.38\\
    UNet++ w/o DS & 9.04M &   \textbf{92.63}&	\textbf{33.45}&	79.70&	76.44\\
    UNet++ w/ DS & 9.04M&	92.52&	32.12&	\textbf{82.90}& \textbf{77.21}\\
    \hline
    \end{tabular}
\end{threeparttable}
\end{center}
\end{table}



\iffalse
\begin{table}
\centering
    \begin{tabular}{P{0.20\textwidth}P{0.15\textwidth}P{0.15\textwidth}P{0.15\textwidth}P{0.15\textwidth}P{0.15\textwidth}}
    \hline
    \hline
          Architecture& \#parameters & &datasets & &\\
          \cline{3-6}
          & & cell nuclei & colon polyp & liver \hspace{6pt} & lung nodule\\
         \hline
         U-Net~\cite{} &7.76M	& 90.77	& 30.08	& 76.62	&71.47 \\
          Wide U-Net & 9.13M&	90.92& 	30.14	 &76.58	 &73.38\\
          UNet++ w/o DS & 9.04M&	92.63&	33.45&	79.7&	76.44\\
          UNet++ w/ DS & 9.04M&	92.52&	32.12&	82.9& 77.21\\

          \hline
          \hline

    \end{tabular}
    \caption{DS: deep supervision}
    \label{tab:versions}
\end{table}
\fi

\iffalse
\begin{table}[t]
\begin{center}
\begin{threeparttable}
\caption{Complexity, speed, and accuracy of UNet++ after pruning. The number of parameters is shown in the second row for both 2D and 3D variants of UNet++. The inference time is the time taken to process {\bf 10k} test images using one NVIDIA TITAN X (Pascal) with 12 GB memory.}
\label{tab:prune_network}
    \begin{tabular}{P{0.2\textwidth}P{0.1\textwidth}P{0.16\textwidth}P{0.16\textwidth}P{0.16\textwidth}P{0.16\textwidth}}
       \hline
     & & UNet++-L & UNet++-L & UNet++-L & UNet++-L \\
     \cline{3-6}
     \multicolumn{2}{c}{{\scriptsize \#Params (2D / 3D)}} & {\scriptsize 0.1M / 0.3M} & {\scriptsize 0.5M / 1.5M} & {\scriptsize 2.2M / 6.4M} & {\scriptsize 9.0M / 26.2M} \\
    \hline
    \multirow{2}*{cell nuclei} & {\scriptsize IoU} & 91.47 & 92.22 & 92.16 & 92.52 \\
     & {\scriptsize Time} & 15.4s & 22.6s & 36.2s & 54.5s \\
    \hline
    \multirow{2}*{colon polyp} & {\scriptsize IoU} & 25.37 & 27.64 & 30.15 & 32.12 \\
     & {\scriptsize Time} & 15.4s & 22.6s & 36.2s & 54.5s \\
    \hline
    \multirow{2}*{liver} & {\scriptsize IoU} & 76.23 & 81.87 & 82.52 & 82.65 \\
     & {\scriptsize Time} & 15.4s & 22.6s & 36.2s & 54.5s \\
    \hline
    \multirow{2}*{lung nodule} & {\scriptsize IoU}  & 72.05 & 76.92 & 76.89 & 77.21 \\
     & {\scriptsize Time} & 227s & 354s & 635s & 885s \\
    \hline
    \end{tabular}
\end{threeparttable}
\end{center}
\end{table}
\fi

\begin{figure}[t]
\begin{center}
\includegraphics[width=1.0\linewidth]{fig_inference_time.png}
\end{center}
\caption{Complexity, speed, and accuracy of UNet++ after pruning on (a) cell nuclei, (b) colon polyp, (c) liver, and (d) lung nodule segmentation tasks respectively. The inference time is the time taken to process {\bf 10k} test images using one NVIDIA TITAN X (Pascal) with 12 GB memory.}
\label{fig:prune_network}
\end{figure}





\section{Conclusion}
\label{sec:conclusion}

To address the need for more accurate medical image segmentation, we proposed UNet++. The suggested architecture takes advantage of re-designed skip pathways and deep supervision. The re-designed skip pathways aim at reducing the semantic gap between the feature maps of the encoder and decoder sub-networks, resulting in a possibly simpler optimization problem for the optimizer to solve. Deep supervision also enables more accurate segmentation particularly for lesions that  appear at multiple scales such as polyps in colonoscopy videos. We evaluated UNet++ using four medical imaging datasets covering lung nodule segmentation, colon polyp segmentation, cell nuclei segmentation, and liver segmentation. Our experiments demonstrated that UNet++ with deep supervision achieved an average IoU gain of 3.9 and 3.4 points over U-Net and wide U-Net, respectively.



\subsubsection*{Acknowledgments}

This research has been supported partially by NIH under Award Number R01HL128785, by ASU and Mayo Clinic through a Seed Grant and an Innovation Grant. The content is solely the responsibility of the authors and does not necessarily represent the official views of NIH.

{\small
\bibliographystyle{ieee}
\bibliography{DLMIA,PAMI_JL}
}

\iffalse



\newpage

\appendix

\section*{Appendices}

\section{Component ablation studies}
\label{sec:component_study}

\texttt{Cell} and \texttt{EM} dataset are selected for the network ablation studies (see Table~\ref{tab:architecture_analysis}). To understand the U-Net performance, we first run four independent U-Net with depth 1, 2, 3, 4, observing that the optimized networks depth depends on certain segmentation task. We have done similar study on FCN architectures too (see Table~\ref{tab:architecture_analysis}) but the configurations were changed only in expanding/decoder path as originally FCN does not have same amount of down-sampling and up-sampling. In Nested version of FCN, we took FCN-2s as it performed better in our experiments (see Table~\ref{tab:architecture_analysis}) and introduced intermediate convolutional layers inside skip connections. On the other-hand, nested version of U-Net can be seen as merging of U-Net of all different depth as well. These results in rewarding the network with more full-resolution feature maps (see Fig.~\ref{fig:concatenation}).

\begin{table}[h]
\begin{center}
\begin{threeparttable}
\caption{{\bf Component analysis} on the \texttt{ISBI2012 EM segmentation challenge} testing dataset evaluated by local testing data (IoU: , Dice: ). We break standard meta segmentation architectures FCN and U-Net into several components and explore their performance with the same setup. The original FCN paper~\cite{long14} proposed up-sampling with three different configuration, \ie FCN-32s, FCN-16s, and FCN-8s. We extend the up-sampling further resulting in FCN-4s and FCN-2s (see details in Sec.~\ref{sec:component_study}) to understand the effect of number of up-sampling. Our observation is that gradual up-sampling with smaller filter size makes more precise segmentation than sudden up-sampling with larger filter size. We have studied U-Net as well in terms of different depth, which means different number of down-sampling are applied in the contracting path and therefore corresponds to different size of receptive fields of the input images. The suggested Nest-Net crafted with these observations outperforms all the basic architectures.}
\label{tab:architecture_analysis}
    \begin{tabular}{P{0.2\linewidth}P{0.15\linewidth}P{0.12\linewidth}|P{0.2\linewidth}P{0.15\linewidth}P{0.12\linewidth}}
    \hline
    Architecture & IoU & Dice & Architecture & IoU & Dice \\
    \hline
    FCN-32s & 45.03\tpm{2.07} & 83.30\tpm{7.09} & UNet-1d & 86.84\tpm{6.70} & 93.13\tpm{3.73}  \\
    FCN-16s & 59.65\tpm{2.73} & 86.71\tpm{5.79} & UNet-2d & 87.60\tpm{5.71} & 93.57\tpm{3.16}  \\
    FCN-8s~\cite{long2015fully} & 71.89\tpm{3.32} & 90.93\tpm{4.11} & UNet-3d & 88.17\tpm{5.30} & 93.86\tpm{2.93}  \\
    FCN-4s & 74.04\tpm{3.43} & 92.28\tpm{4.09} & UNet-4d~\cite{ronneberger2015u} & 88.27\tpm{5.80} & 93.95\tpm{3.11}  \\
    FCN-2s & 74.29\tpm{3.41} & 92.43\tpm{3.92} \\
    \bd{Nest-Net} & \bd{74.43}\tpm{3.42} & \bd{92.80}\tpm{3.85} & \bd{Nest-Net} & \bd{88.88}\tpm{5.80} & \bd{94.26}\tpm{3.16} \\
    \hline
    \end{tabular}
\end{threeparttable}
\end{center}
\end{table}


\section{Semantic segmentation performance}
\label{sec:semantic_results}

Semantic segmentation is to understanding an image at pixel level, \ie the segmentation model is supposed to fitting to assign each pixel in the image an object class, delineate the boundaries of each object. Unlike instance segmentation, the pixel-wise predictions don't required to identify each instance belonging to the same class.

We report performance of our Nest-Net in Table~\ref{tab:fcn_family_performance} comparing with popularized semantic segmentation architectures FCN and U-Net, embedded with various backbone feature extractors, \ie VGG-16, ResNet-101 on \texttt{Data Science Bowl 2018} dataset. Our method gives an impressive improvement respect to base architectures measured by pixel Intersection over Union (IoU), Dice Coefficient as well as public leaderboard score. Note that the leaderboard score is computed by mean average precision at different intersection over union (IoU) thresholds. In particular, the public leaderboard score improves from 0.271 to 0.302, from 0.273 to 0.329, from 0.244 to 0.327 using  U-Net, VGG-16 and ResNet-101 respectively. Further, in \texttt{EM dataset}, Nest-Net outperforms FCN with VGG-16 backbone from 74.43 to 71.89, from 92.80 to 90.93 in terms of IoU and Dice in the testing dataset respectively. In \texttt{cell dataset}, Nest-Net yields 91.295.44 in terms of testing mean IoU, while U-Net only achieves 88.897.26. Nearly 2.7 performance improvement thanks to the advantages that Nest-Net offers.

\section{Instance segmentation results}

Instance segmentation is more challenging than semantic segmentation. In instance segmentation task, on top of semantic segmentation, it is important to distinguish the segmented objects. Initially, region with convolution networks (RCNN) were proposed for object detection. These networks usually propose multiple regions from an image and label those regions with presence of a certain objects. In Mask R-CNN, the feature extractor of previous RCNN networks was used as decoder and an encoder network (FCN) was connected for segmentation masks. We have applied our method in Mask-RCNN and improved the skip connections of FCN that improved overall segmentation masks. For comparison of resultant Nest-Net (network after we apply our method on Mask-RCNN) and original Mask-RCNN, we used \texttt{Data Science Bowl 2018} dataset, where ground truth of all nuclei instances in every image are available as separated segmentation mask. The comparison results are reported in Table~\ref{tab:fcn_family_performance}. We observe that the nested network embedding to the segmentation branch in the Mask RCNN significantly improves the accuracy of segmenting an object, that is a boost performance from 93.28 to 95.10, from 87.91 to 91.36, from 0.401 to 0.414 in terms of IoU, Dice and Leaderboard score on the testing dataset respectively. When comparing to the performance of semantic segmentation models, Mask RCNN outperform in this particular segmentation task a lot thank to the various outstanding mechanisms designed inside such as segment proposals, RoIAlign and object-wised labeling for each individual nuclei provided by \texttt{Data Science Bowl 2018}.





\begin{table}[t]
\begin{center}
\begin{threeparttable}
\caption{{\bf Semantic and instance segmentation.} We report comparative study of Nest-Net with state-of-the-art semantic and instance segmentation meta architectures using the \texttt{2018 Data Science Bowl} testing dataset evaluated by public LB (score) and local testing data (IoU: , Dice: ). To analyze local testing performance, we sampled the whole labeled images into training (60), validation (20) and testing (20) dataset. Nest-Net surpasses all of them with good margin. For experiments, several backbones are used with transfer learning, \ie VGG-16, ResNet-101, and are transformed into Nest-Net. In addition, Mask-RCNN (ResNet-101-FCN) meta framework is modified on the mask branch inside for comparison. U-Net and nested U-Net (second row) are considered as benchmark performance. }
\label{tab:fcn_family_performance}
    \begin{tabular}{P{0.15\linewidth}P{0.28\linewidth}P{0.18\linewidth}P{0.18\linewidth}P{0.18\linewidth}}
    \hline
    backbone & architecture & IoU & Dice & Score \\
    \hline
    \multirow{2}*{} & U-Net~\cite{ronneberger2015u} & 90.77\tpm{1.62} & 78.54\tpm{7.39} & 0.271 \\
     & Nest-Net & 92.90\tpm{1.37} & 87.72\tpm{6.94} & 0.302 \\
    \hline
    \multirow{2}*{VGG-16} & FCN-8s~\cite{long2015fully} & 91.28\tpm{1.49} & 88.19\tpm{16.88} & 0.273 \\
     & \bd{Nest-Net} & \bd{93.27}\tpm{1.26} & \bd{90.49}\tpm{15.30} & \bd{0.329} \\
    \hline
    \multirow{2}*{ResNet-101} & FCN-8s~\cite{quan2016fusionnet} & 91.03\tpm{1.75} & 75.73\tpm{28.78} & 0.244 \\
     & \bd{Nest-Net} & \bd{92.55}\tpm{1.17} & \bd{89.74}\tpm{15.67} & \bd{0.327} \\
    \hline
    \multirow{2}*{Mask RCNN} & FCN~\cite{he2017mask} & 93.28\tpm{1.44} & 87.91\tpm{11.44} & 0.401 \\
     & \bd{Nest-Net} & \bd{95.10}\tpm{1.04} & \bd{91.36}\tpm{9.50} & \bd{0.414} \\
    \hline
    \end{tabular}
\end{threeparttable}
\end{center}
\end{table}


\begin{figure}[t]
\begin{center}
\includegraphics[width=0.85\linewidth]{fig-feature-maps.png}
\end{center}
\caption{\bd{Information flow.} Examples of cell segmentation intermediate feature maps and predictions of U-Net and Nest-Net. The feature maps are computed by averaging all the maps from convolution component X. By smoothing the information flow and gradient flow along the network, detailed area around object boundary can be classified accurately. In \texttt{cell dataset}, Nest-Net yields 91.295.44 in terms of testing mean IoU (), while U-Net only achieves 88.897.26. Nearly 2.7 performance improvement thanks to the advantages that Nest-Net offers.}
\label{fig:feature_maps}
\end{figure}


\section{Intermediate feature-maps visualization and analysis}
\label{sec:feature_map}


We observe that the feature maps from different depths of contracting path are appearing largely different in encoder-decoder architectures, as shown in Fig.~\ref{fig:concatenation} map A and map B. Map A is the feature map that the input image goes through only one convolutional block, while map B is the feature map that the input image goes through a sequence of convolutional blocks including several down-sampling and up-sampling operations. Therefore, in spite of their sharing the same resolution, the information is in fact asymmetry. Directly summing up or concatenating first and last feature maps as proposed in FCN and U-Net may lead the network to confusion. We explore a more reasonable way for matching size feature skip connections. Iteratively concatenating intermediate level of feature-maps not only reduces the information lost from up-sampling and down-sampling operations, but also gives reuse the matching level feature information. As a result, Nest-Net has ability to smooth the information flow comparing to FCN-like or U-Net-like architecture, which merges map A and map B together directly. Fig.~\ref{fig:concatenation} showing the depth =1 condition, we believe that this phenomenon exist in other depth condition, and of course, the larger depth is, the more asymmetry between feature maps appear.

\iffalse
X-X represent the feature-maps of each expanding path of networks. We average all the channels and visualize the intermediate layers intuitively. Dot arrows denote single skip connection in U-Net and Nest-Net. Comparing to U-Net feature-maps, i.e., X and X, we claim that simply concatenate feature-maps between the one with rich row input information and the one with abstract feature information together will cause network confusion and result in gradient vanish. Based on the feature maps changing from X to X and segmentation performance, we observe that nesting different level feature-maps gradually can help learn more accurate segmentation and grasp more information.
\fi

\begin{figure}[t]
\begin{center}
\includegraphics[width=0.95\linewidth]{fig-concatenation.png}
\end{center}
\caption{\bd{Feature maps concatenation.} Visualization of intermediate feature maps concatenation and flow along the top components of Nest-Net (visualizing depth  feature maps). The standard U-Net concatenates feature map A and B together following up with a successive convolution layer. However, when network becoming deeper, the information gap between A and B will be enlarged and ultimately influence the gradient flow between. To address this problem, we introduce intermediate layers and skip connecting them accordingly. Top row denotes the feature maps from convolution component; bottom row denotes the feature maps from up-sampling layer.}
\label{fig:concatenation}
\end{figure}


\fi

\end{document}
