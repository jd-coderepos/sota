
\documentclass{article} \usepackage{iclr2022_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}
\usepackage[pdftex]{graphicx}



\title{Attention-Free Keyword Spotting}



\author{Mashrur M. Morshed \& Ahmad Omar Ahsan\thanks{Equal contribution.} \\
Islamic University of Technology \\ 
Dhaka, Bangladesh \\
\texttt{\{mashrurmahmud,ahmadomar\}@iut-dhaka.edu}}




\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}


\maketitle

\begin{abstract}
Till now, attention-based models have been used with great success in the keyword spotting problem domain. However, in light of recent advances in deep learning, the question arises whether self-attention is truly irreplaceable for recognizing speech keywords. We thus explore the usage of gated MLPs---previously shown to be alternatives to transformers in vision tasks---for the keyword spotting task. We provide a family of highly efficient MLP-based models for keyword spotting, with less than 0.5 million parameters. We show that our approach achieves competitive performance on Google Speech Commands V2-12 and V2-35 benchmarks with much fewer parameters than self-attention-based methods.

\end{abstract}

\section{Introduction}





Transformers \citep{vaswani2017attention} have shown remarkable success in Computer Vision tasks with the advent of the Vision Transformer (ViT) \citep{dosovitskiy2020image}. They have lately been studied in the field of keyword spotting (KWS). Several works \citep{berg21_interspeech,gong2021ast} have obtained exceptional results with ViT-like approaches on KWS.

 Recent research \citep{tolstikhin2021mlp,liu2021pay,melas2021you,touvron2021resmlp} shows that a core component of Transformers, self-attention, may not be necessary for achieving good performance in vision and language tasks. This finding necessitates a study on whether MLPs can be an alternative to self-attention, which has been a main focus of several state-of-the-art methods for the KWS problem. Our contributions can be summarized as follows:

\begin{enumerate}

\item We introduce the Keyword-MLP (KW-MLP), a memory-efficient, attention-free alternative to the Keyword Transformer (KWT) \citep{berg21_interspeech}. It achieves 97.63\% and 97.56\% accuracy on the Google Speech Commands V2-12 and V2-35 benchmarks \citep{warden2018speech} respectively---showing comparable performance to the KWT, while having much fewer parameters.

\item We distill smaller and shallower versions of KW-MLP, with the smallest having only 0.213 million parameters, and accuracies of 97.12\% and 97.17\% on Google Speech Commands V2-12 and V2-35 benchmarks respectively.

\end{enumerate}


\section{Related Work}

\subsection{Keyword Spotting}
Keyword spotting (KWS) deals with identifying some pre-specified speech keywords from an audio stream. As it is commonly used in always-on edge applications, KWS research often focuses on both accuracy and efficiency. While research in keyword spotting goes back to the 1960s \citep{teacher1967experimental}, most of the recent and relevant works have been focused on the Google Speech Commands dataset \citep{warden2018speech}, which has inspired numerous works and has rapidly grown to be the standard benchmark in this field. The dataset contains 1 second long audio clips, each containing an utterance of a word. Notably, there are two versions, V1 and V2, consisting of 30 and 35 keywords respectively. There is also a 12 keyword task for either version, where it is required to identify 10 keywords and two additional classes, `silence' and `unknown' (containing instances of the unused keywords).

Initial approaches to keyword spotting on the speech commands consisted of convolutional models \citep{warden2018speech}. \citet{majumdar2020matchboxnet}, \citet{mordido2021compressing} and \citet{zhang2017end} proposed lightweight CNN models with depth-wise separable convolutions. \citet{de2018neural} proposed using a convolutional recurrent model with attention, introducing the usage of attention in the KWS task. \citet{rybakov2020streaming} proposed a multi-headed, self-attention-based RNN (MHAtt-RNN). \citet{vygon2021learning} proposed an efficient representation learning method with triplet loss for KWS. While the state of the art in KWS at that time was the method of \citet{rybakov2020streaming}, it was empirically seen that triplet loss performed poorly with RNN-based models. The authors later obtained excellent results with ResNet \citep{tang2018deep} variants.

Recently, \citet{berg21_interspeech} and \citet{gong2021ast} proposed the Keyword Transformer (KWT) and Audio Spectrogram Transformer (AST) respectively. Both approaches are inspired by the success of the Vision Transformer (ViT) \citep{dosovitskiy2020image}, and show that patch-based transformer models with self-attention can obtain state of the art or comparable results on the keyword spotting task. A key difference between these two transformer approaches is that AST uses ImageNet \citep{imagenet} and Audioset \citep{audioset} pre-training. Furthermore, the best performing KWT models are trained with attention-based distillation method \citep{touvron2021training} using a teacher MHAtt-RNN \citep{rybakov2020streaming}.

\subsection{MLP-based Vision}
The Vision Transformer \citep{dosovitskiy2020image} has thus far shown the remarkable capability of Transformers on image and vision tasks. However, several recent works have questioned the necessity of self-attention in ViT. \citet{melas2021you} directly raises the question on the necessity of attention, and shows that the effectiveness of the Vision Transformer may be more related to the idea of the patch embedding rather than self-attention. \citet{tolstikhin2021mlp} proposed the MLP-Mixer, which performs token mixing and channel mixing on image patches/tokens, and shows competitive performance on the ImageNet benchmark. \citet{touvron2021resmlp} showed similarly good ImageNet results with ResMLP, a residual network with patch-wise and channel-wise linear layers. \citet{liu2021pay} proposed the gMLP, consisting of very simple channel projections and spatial projections with multiplicative gating---showing remarkable performance without any apparent use of self-attention.

\section{Keyword-MLP}
\label{sec:proposed-method}

Inputs to KW-MLP consist of mel-frequency cepstrum coefficients (MFCC). Let an arbitrary input MFCC be denoted as , where  and  are the frequency bins and time-steps respectively. We divide  into patches of shape , getting a total of  patches. Each patch is effectively a vector of mel-frequencies for a particular time-step. 

The  patches are flattened, giving us . We then map  to a higher dimension , with a linear projection matrix , getting the frequency domain patch embeddings .



\begin{figure}[ht]
\centering
    \includegraphics[width=0.6\linewidth]{GMLP_2.png}
\caption{The Keyword-MLP architecture, consisting of L blocks (equation \ref{eqn:gmlp}). Note that we move the LayerNorm to the end, before the skip-connection, different from \citet{liu2021pay} where norm is applied at the beginning. ( represents a residual skip-connection, while  represents element-wise product.)}

\label{fig:kw-mlp}
\end{figure}

The obtained  is passed through  consecutive, identical gated-MLP (gMLP) blocks \citep{liu2021pay}. On a high level, we can summarize the gMLP blocks used in KW-MLP as a pair of projections across the embedding dimension separated by a projection across the temporal dimension. The block can also be formulated with the following set of equations (omitting bias and normalization for the sake of conciseness):




First, we use the matrix  to linearly project  from the embedding dimension  to the projection dimension  (essentially a matmul operation).  represents the GELU activation function.  represents the the Temporal Gating Unit (TGU) shown in Figure \ref{fig:kw-mlp}. The input to TGU, , is first split into  and , the residual and the gate respectively. We use the matrix  to performs the linear projection across the temporal axis. This is followed by the linear gating---an element-wise multiplication with the residual . While the temporal projection operation can be implemented as passing  transposed through a \texttt{Dense(T, T)} layer, in practice, it can be implemented more efficiently by instead passing  through a \texttt{Conv1D(T, T, 1)} layer.  is projected back to the embedding dimension  with the matrix  and then added with the skip-connected input .

The original gMLP paper \citep{liu2021pay} applies LayerNorm \textit{before} the initial channel projection (analogous to embedding projections for images). We however find that applying norm \textit{after} the second embedding projection results in a notably faster and more optimal convergence. \citet{berg21_interspeech} also observe a similar phenomenon in their work.

The overall system is shown in Figure \ref{fig:kw-mlp}. In KW-MLP, we primarily use  (12 consecutive gMLP blocks), embedding dim , and projection dim . We also explore a group of smaller KW-MLP models with shallower depth, i.e. . The input MFCCs to the model are of shape , where 40 is the number of frequency bins, and 98 is the number of timesteps. All settings are also shown in Table \ref{tbl:settings}.

It is to be noted that the largest KW-MLP model has only 0.424M parameters, which is smaller than the smallest KWT variant (KWT-1 with 0.607M params) and much smaller than AST models (87M params). However, from Table \ref{tbl:accuracy} we can see that KW-MLP shows competitive accuracy with these models, particularly on the Speech Commands V2-35 benchmark.

\subsection{Knowledge Distillation}
In order to boost the accuracies of shallower KW-MLP variants, we use knowledge distillation (KD) \citep{hinton2015distilling}, using the KW-MLP model with  as the teacher model. We use an annealed KD approach \citep{jafari2021annealing} where the temperature parameter decreases every step following a cosine-annealing rule till it reaches 1. Other KD parameters, such as alpha, are shown in Table \ref{tbl:settings}. We also do not use label smoothing when training with KD, as soft targets are obtained from teacher predictions.

\begin{table}[ht]
	\centering
	\caption{Comparison of Model Parameters and Accuracy on Google Speech Commands V2-12 and V2-35 benchmarks \citep{warden2018speech}}
	\label{tbl:accuracy}
	\begin{tabular}	{l  l l l l }
		\hline
		Method    & Extra Knowledge & V2-12 & V2-35 & \# Params (M) \\
		\hline
		Att-RNN \quad [\citenum{de2018neural}]   & &  96.9 & 93.9  &  0.202\\
		Res-15 \quad [\citenum{vygon2021learning}]  & & 98.0 & 96.4 & 0.237\\
		MHAtt-RNN \quad [\citenum{rybakov2020streaming}]  & & 98.0 & 97.27 & 0.743\\
		\hline
		AST-S \quad [\citenum{gong2021ast}] & Pre. ImageNet  & & \textbf{98.11} & 87\\
		AST-P  \quad [\citenum{gong2021ast}] &  Pre. ImageNet  \& Audioset & & 97.88 & 87\\
		\hline
		KWT-3  \quad [\citenum{berg21_interspeech}] & KD with MHAtt-RNN & \textbf{98.56} & 97.69 & 5.361 \\
		KWT-2   \quad [\citenum{berg21_interspeech}] & KD with MHAtt-RNN & 98.43 & 97.74 & 2.394 \\
		KWT-1  \quad [\citenum{berg21_interspeech}]  & KD with MHAtt-RNN & 98.08 & 96.95 & 0.607 \\
		\hline
		KWT-3  \quad [\citenum{berg21_interspeech}] & & 98.54 & 97.51 & 5.361 \\
		KWT-2  \quad [\citenum{berg21_interspeech}]  & & 98.21 & 97.53 & 2.394 \\
		KWT-1  \quad [\citenum{berg21_interspeech}] & & 97.72 & 96.85 & 0.607 \\
		\hline
		KW-MLP  & & \textbf{97.63} & \textbf{97.56} & 0.424 \\
		KW-MLP   & &  97.38 & 97.35 & 0.353 \\
		KW-MLP   & & 97.28 & 97.26 & 0.283 \\
		KW-MLP   & & 97.03 & 97.07 & 0.213 \\
		\hline
		KW-MLP   & KD with KW-MLP & 97.30 &  97.49     & 0.353 \\
		KW-MLP   & KD with KW-MLP & 97.24  &  97.45     & 0.283 \\
		KW-MLP  & KD with KW-MLP & 97.12 & 97.17 & 0.213 \\
	\end{tabular}
\end{table}






\section{Experimental Details}
We follow similar hyperparameters to \citet{rybakov2020streaming,berg21_interspeech}, with minor changes; all our hyperparameters and settings are shown in Table \ref{tbl:settings}. For training, we use a smaller batch-size of 256, and train for 140 epochs. No other augmentation apart from Spectral Augmentation \citep{park2019specaugment} is applied (to enable fast training). As an additional regularization method, each gMLP block has a survival probability of 0.9 (alternatively, a 0.1 probability to drop each block). We run experiments on Google Speech Commands V2-12 and V2-35 benchmarks, following the standard protocol described in \citet{warden2018speech}.

As seen from Table \ref{tbl:accuracy}, the largest KW-MLP model has only 424K parameters, which is much fewer than the KWT models, while having comparable accuracy. Furthermore, since we do not apply expensive run-time augmentations like resampling, time-shifting, adding background noise, mixup, etc. (used by \citep{rybakov2020streaming,berg21_interspeech,gong2021ast}), it is possible to train KW-MLP models in a very short time on free cloud compute such as the NVIDIA Tesla K80 or Tesla P100 provided by Google Colab and Kaggle. 

As a trade-off for fast training, a limitation of the KW-MLP experiments is that the effect of various augmentation methods have not been explored. This is more apparent in the V2-12 task, which contains much fewer training examples () than the V2-35 task (). The KW-MLP model does not generalize as well here, as compared to V2-35.

\begin{table}[ht]
	\centering
	\caption{Overview of Hyper-Parameters and Settings}
	\label{tbl:settings}
	\begin{tabular} {c c c c c c c c}
		\cline{1-2}  \cline{4-5} \cline{7-8}
		\multicolumn{2}{c}{Training} & & \multicolumn{2}{c}{Augmentation} & & \multicolumn{2}{c}{Model} \\
		\cline{1-2}  \cline{4-5} \cline{7-8}
		Epochs & 140 &  & \# Time Masks & 2 & &  \# Blocks,  & 12 \\
		Batch Size & 256 & & Time Mask Width & [0, 25] & &  Input Shape &  \\
		Optimizer & AdamW & & \# Freq Masks & 2 & &  Patch Size &  \\
		Learning Rate & 0.001 & & Freq Mask Width & [0, 7] & & Dim,  & 64 \\
		Warmup Epochs & 10 & & & & &  Dim Proj. & 256 \\
		Scheduling & Cosine &  & & & & \# Classes & 35 \\
		
		\cline{1-2}  \cline{4-5}  \cline{7-8}
		\multicolumn{2}{c}{Regularization} & & \multicolumn{2}{c}{Audio Processing} & & \multicolumn{2}{c}{KD}\\
		\cline{1-2}  \cline{4-5} \cline{7-8}
		Label Smoothing & 0.1 & & Sampling Rate & 16000 & &  & 0.9\\
		Weight Decay & 0.1 &  &  Window Length & 30 ms & & Init Temp & 5.0 \\
		Block Survival Prob. & 0.9 & &  Hop Length & 10 ms \\
		& & &  n\_mfcc & 40  \\

	\end{tabular}
\end{table}

\section{Temporal Projection Matrices}

We additionally visualize the weights of the temporal gating unit (the temporal projection matrix  in equation \ref{eqn:gmlp}). Interestingly, we can observe that our model learns weights which seem similar to diagonal, identity, or toeplitz matrices. This suggests that KW-MLP may partially learn a form of shift-invariance, which is necessary for the keyword spotting task. For instance, in a 1 second audio clip, a keyword can occur at different temporal positions; so the model needs to be invariant to temporal shift.


\begin{figure}[h]
\centering
    \includegraphics[width=0.6\linewidth]{grid.png}
\caption{Visualization of the temporal projection matrices, , for each of the  gMLP blocks of KW-MLP. The matrices are arranged in row major order.}
\label{fig:temp-matrix}
\end{figure}

\section{Conclusion}
The Keyword-MLP has shown itself to be an efficient solution to the keyword spotting task, and an alternative to self-attention-based methods. We hope that we provide an additional avenue of future research in audio and speech domains, particularly when resource-efficiency is concerned.



\bibliography{iclr2022_conference}
\bibliographystyle{iclr2022_conference}



\end{document}
