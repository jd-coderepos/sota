\clearpage 
\appendix










All our code is available as part of the Tensor2Tensor library and it includes instructions on
how to run our experiments: \url{https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/rl}.

\section{Ablations}\label{sec:ablations}

To evaluate the design of our method, we independently varied a number of the design decisions: the choice of the model, the  parameter and the length of PPO rollouts. The results for  experimental configurations are summarized in the Table \ref{tab:count_best_models}.

\begin{table}
\caption{\textbf{Summary of SimPLe ablations.} For each game, a configuration was assigned a score being the mean over  experiments. The best and median scores were calculated per game. The table reports the number of games a given configuration achieved the best score or at least the median score, respectively.}
\begin{center}
  \begin{tabular}{lrr}\label{tab:ab}
model  &  best &  at least median \\
\midrule
deterministic       &      &                 \\
det. recurrent  &      &                \\
SD         &      &                \\
SD      &      &                \\
default     &      &                \\
SD  steps    &      &                \\
SD  steps     &      &                \\
\bottomrule
\end{tabular} 
\end{center}
\label{tab:count_best_models}
\end{table}

\paragraph{Models.} To assess the model choice, we evaluated the following models: deterministic, deterministic recurrent, and stochastic discrete (see Section \ref{sec:architectures}). Based on Table~\ref{tab:count_best_models} it can be seen that our proposed stochastic discrete model performs best. Figures \ref{fig:ablations_stochasticity} and \ref{fig:ablations_recurrence} show the role of stochasticity and recurrence.  

\paragraph{Steps.} See Figure \ref{fig:ablations_steps}. As described in Section \ref{sec:policy_training} every  steps we reinitialize the simulated environment with ground-truth data. By default we use , in some experiments 
we set  or . It is clear from the table above and Figure~\ref{fig:ablations_steps} that  is a bit worse than either  or , likely due to compounding model errors,
but this effect is much smaller than the effect of model architecture.

\paragraph{Gamma.} See Figure \ref{fig:ablations_gamma}. We used the discount factor  unless specified otherwise.  We see that  is slightly better than other values, and we hypothesize that it is due to better tolerance to model imperfections. But overall, all three values of  seem to perform comparably at the same number of steps.

\paragraph{Model-based iterations.}
The iterative process of training the model, training the policy, and collecting data is crucial for non-trivial tasks where simple random data collection is insufficient. In the game-by-game analysis, we quantified the number of games where the best results were obtained in later iterations of training. In some games, good policies could be learned very early. While this might have been due simply to the high variability of training, it does suggest the possibility that much faster training -- in many fewer than 100k steps -- could be obtained in future work with more directed exploration policies. We leave this question to future work.

In Figure \ref{fig:Cdf} we present the cumulative distribution plot for the (first) point during learning when the maximum score for the run was achieved in the main training loop of Algorithm~\ref{alg:basic_loop}. 

On Figure \ref{fig:ablations_epochs} we show results for experiments in which the number samples was fixed to be 100K but the number of training loop varied. We conclude that  is beneficial for training.


\paragraph{Long model training} Our best results were obtained with much 5 times longer training of the world models, see Figure~\ref{fig:ablations_longer} for comparison with shorter training. Due to our resources constraints other ablations were made with the short model training setting.

\paragraph{Random starts.} Using short rollouts is crucial to mitigate the compounding errors under the model. To ensure exploration SimPLe starts rollouts from randomly selected states taken from the real data buffer . In Figure \ref{fig:Cdf} we present a comparison with an experiment without random starts and rollouts of length  on \seaquest. These data strongly indicate that ablating random starts substantially deteriorate results.  



\newcommand{\mywidthapp}{3.2in}
\begin{figure}
\vspace{-1.8cm}
\begin{tabular}{@{\hskip -3em}cc}

\subfloat[Effect of stochasticity.]{\includegraphics[width = \mywidthapp]{figures/graph_Effect_of_stochasticity.pdf} \label{fig:ablations_stochasticity}} &

\subfloat[Effect of recurrent architecture.]{\includegraphics[width = \mywidthapp]{figures/graph_Effect_of_a_recurrent_architecture.pdf} \label{fig:ablations_recurrence}}  \\

\subfloat[Effect of adjusting of number of epochs.]{\includegraphics[width = \mywidthapp]{figures/graph_Effect_of_adjusting_number_of_main_loop_iterations.pdf} \label{fig:ablations_epochs}} &

\subfloat[Effect of adjusting of number of steps.]{\includegraphics[width = \mywidthapp]{figures/graph_Effect_of_adjusting_number_of_steps.pdf} \label{fig:ablations_steps}}
\end{tabular}
\caption{Ablations part 1. The graphs are in the same format as Figure~\ref{fig:compare_dopamine_ppo}: each bar illustrates the number of interactions with environment required by Rainbow to achieve the same score as a particular variant of SimPLe. The red line indicates the K interactions threshold which is used by SimPLe. \label{fig:ablations1}}
\end{figure}

\begin{figure}
\vspace{-1.8cm}
\begin{tabular}{@{\hskip -3em}cc}
\subfloat[Effect of extended model training.]{\includegraphics[width = \mywidthapp]{figures/graph_Effect_of_extended_model_training.pdf} \label{fig:ablations_longer}} &


\subfloat[Effect of adjusting  in PPO training]{\includegraphics[width = \mywidthapp]{figures/graph_Effect_of_adjusting_Gamma.pdf} \label{fig:ablations_gamma}}

\end{tabular}
\caption{Ablations part 2. The graphs are in the same format as Figure~\ref{fig:compare_dopamine_ppo}: each bar illustrates the number of interactions with environment required by Rainbow to achieve the same score as a particular variant of SimPLe. The red line indicates the K interactions threshold which is used by SimPLe. \label{fig:ablations2}}
\end{figure}










\section{Qualitative Analysis}\label{qualitative_analysis}
This section provides a qualitative analysis and case studies of individual games. We emphasize that we did not adjust the method nor hyperparameters individually for each game, but we provide specific qualitative analysis to better understand the predictions from the model.\footnote{We strongly encourage the reader to watch accompanying videos \url{https://goo.gl/itykP8}} 

\begin{figure}
\centering
\includegraphics[width=0.49\columnwidth]{figures/cdf_max_attained}
\includegraphics[width=0.49\columnwidth]{figures/random_starts_ablations}
\caption{(left) CDF of the number of iterations to acquire maximum score. The vertical axis represents the fraction of all games. (right) Comparison of random starts vs no random starts on \seaquest\, (for better readability we clip game rewards to ). The vertical axis shows a mean reward and the horizontal axis the number of iterations of Algorithm \ref{alg:basic_loop}. }
\label{fig:Cdf}
\end{figure}

\paragraph{Solved games.} 
The primary goal of our paper was to use model-based methods to achieve good performance within a modest budget of k interactions. For two games, \pong\, and \freeway, our method, SimPLe, was able to achieve the maximum score.

\paragraph{Exploration.}  \freeway\, is a particularly interesting game. Though simple, it presents a substantial exploration challenge. The chicken, controlled by the agents, is quite slow to ascend when exploring randomly as it constantly gets bumped down by the cars (see the left video \url{https://goo.gl/YHbKZ6}). This makes it very unlikely to fully cross the road and obtain a non-zero reward. Nevertheless, SimPLe is able to capture such rare events, internalize them into the predictive model and then successfully learn a successful policy.

However, this good performance did not happen on every run. We conjecture the following scenario in failing cases. If at early stages the entropy of the policy decayed too rapidly the collected experience stayed limited leading to a poor world model, which was not powerful enough to support exploration (e.g. the chicken disappears when moving to high). In one of our experiments, we observed that the final policy was that the chicken moved up only to the second lane and stayed waiting to be hit by the car and so on so forth. 

\paragraph{Pixel-perfect games.} In some cases (for \pong, \freeway, \breakout) our models were able to predict the future perfectly, down to every pixel. This property holds for rather short time intervals, we observed episodes lasting up to 50 time-steps. Extending it to long sequences would be a very exciting research direction. See videos \url{https://goo.gl/uyfNnW}.

\paragraph{Benign errors.} Despite the aforementioned positive examples, accurate models are difficult to acquire for some games, especially at early stages of learning. However, model-based RL should be tolerant to modest model errors. Interestingly, in some cases our models differed from the original games in a way that was harmless or only mildly harmful for policy training.

For example, in \bowling\, and \pong, the ball sometimes splits into two. While nonphysical, seemingly these errors did not distort much the objective of the game, see Figure \ref{fig:pong} and also \url{https://goo.gl/JPi7rB}.

\begin{figure}[htbp]
\makebox[\columnwidth]{\includegraphics[width=0.25\columnwidth]{figures/pong_ball_frame1.png}\hfill    
\includegraphics[width=0.25\columnwidth]{figures/pong_ball_frame2.png}\hfill    
\includegraphics[width=0.25\columnwidth]{figures/pong_ball_frame3.png}\hfill    
\includegraphics[width=0.25\columnwidth]{figures/pong_ball_frame4.png}}\hfill  

\caption{Frames from the \pong\ environment. }
\label{fig:pong}
\end{figure}


In \kungfumaster\, our model's predictions deviate from the real game by spawning a different number of opponents, see Figure \ref{fig:boxing}. In \crazyclimber\, we observed the bird appearing earlier in the game. These cases are probably to be attributed to the stochasticity in the model. Though not aligned with the true environment, the predicted behaviors are plausible, and the resulting policy can still play the original game.

\begin{figure}[htbp]
\makebox[\columnwidth]{\includegraphics[width=0.50\columnwidth]{figures/kungfu1.png}\hfill    
\includegraphics[width=0.50\columnwidth]{figures/kungfu2.png}}\caption{Frames from the \kungfumaster\ environment (left) and its model (right). }
\label{fig:boxing}
\label{fig:kungfu}
\end{figure}

\paragraph{Failures on hard games.}
On some of the games, our models simply failed to produce useful predictions. We believe that listing such errors may be helpful in designing better training protocols and building better models. The most common failure was due to the presence of very small but highly relevant objects. For example, in \atlantis\, and \battlezone\, bullets are so small that they tend to disappear. Interestingly, \battlezone\, has pseudo-3D graphics, which may have added to the difficulty. See videos \url{https://goo.gl/uiccKU}.

Another interesting example comes from \privateeye\, in which the agent traverses different scenes, teleporting from one to the other. We found that our model generally struggled to capture such large global changes.

\onecolumn
\newpage

\section{Architecture details}\label{sec:architecture-details}
The world model is a crucial ingredient of our algorithm. Therefore the neural-network architecture of the model plays a crucial role. The high-level overview of the architecture is given in Section~\ref{sec:architectures} and Figure~\ref{fig:full_discrete}. We stress that the model is general, not Atari specific, and we believe it could handle other visual prediction tasks. The whole model has around M parameters and the inference/backpropagation time is approx. s/s respectively, where inference is on batch size  and backpropagation on batch size , running on NVIDIA Tesla P100.
This gives us around ms per frame from our simulator, in comparison one step of the ALE simulator takes approximately ms.


Below we give more details of the architecture. First, the frame prediction network:

\begin{tabular}{ l c c }
Layer & Number of outputs & Other details \\ \hline
Input frame dense & 96 & - \\
Downscale convolution 1 & 192 & kernel 4x4, stride 2x2 \\
Downscale convolution 2 & 384 & kernel 4x4, stride 2x2 \\
Downscale convolution 3 & 768 & kernel 4x4, stride 2x2 \\
Downscale convolution 4 & 768 & kernel 4x4, stride 2x2 \\
Downscale convolution 5 & 768 & kernel 4x4, stride 2x2 \\
Downscale convolution 6 & 768 & kernel 4x4, stride 2x2 \\
Action embedding & 768 & - \\
Latent predictor embedding & 128 & - \\
Latent predictor LSTM & 128 & - \\
Latent predictor output dense & 256 & - \\
Reward predictor hidden & 128 & - \\
Reward predictor output dense & 3 & - \\
Middle convolution 1 & 768 & kernel 3x3, stride 1x1 \\
Middle convolution 2 & 768 & kernel 3x3, stride 1x1 \\
Upscale transposed convolution 1 & 768 & kernel 4x4, stride 2x2 \\
Upscale transposed convolution 2 & 768 & kernel 4x4, stride 2x2 \\
Upscale transposed convolution 3 & 768 & kernel 4x4, stride 2x2 \\
Upscale transposed convolution 4 & 384 & kernel 4x4, stride 2x2 \\
Upscale transposed convolution 5 & 192 & kernel 4x4, stride 2x2 \\
Upscale transposed convolution 6 & 96 & kernel 4x4, stride 2x2 \\
Output frame dense & 768 & - \\
\end{tabular}

The latent inference network, used just during training:

\begin{tabular}{ l c c }
Layer & Number of outputs & Other details \\ \hline
Downscale convolution 1 & 128 & kernel 8x8, stride 4x4 \\
Downscale convolution 2 & 512 & kernel 8x8, stride 4x4 \\
\end{tabular}

All activation functions are ReLU, except for the layers marked as "output", which have softmax activations, and LSTM internal layers. In the frame prediction network, the downscale layers are connected to the corresponding upscale layers with residual connections. All convolution and transposed convolution layers are preceded by dropout 0.15 and followed by layer normalization. The latent predictor outputs 128 bits sequentially, in chunks of 8.

\newpage

\section{Numerical results}\label{numerical_results}
Below we present numerical results of our experiments. We tested SimPLe on  configurations (see description in Section \ref{sec:ablations}). For each configuration we run  experiments. For the evaluation of the -th experiments we used the policy given by , where   is the final learnt policy in the experiment and  is the temperature parameter. We found empirically that  worked best in most cases. A tentative explanation is that polices with temperatures smaller than  are less stochastic and thus more stable. However, going down to  proved to be detrimental in many cases as, possibly, it makes policies more prone to imperfections of models. 

In Table \ref{tab:meanStdDev} we present the mean and standard deviation of the  experiments. We observed that the median behaves rather similarly, which is reported it in Table \ref{tab:minmax}. In this table we also show maximal scores over  runs. Interestingly, in many cases they turned out to be much higher. This, we hope, indicates that our methods has a further potential of reaching these higher scores. 

Human scores are "Avg. Human" from Table 3 in \cite{Pohlenetal2018}.

\newenvironment{changemargin}[2]{\begin{list}{}{\setlength{\topsep}{0pt}\setlength{\leftmargin}{#1}\setlength{\rightmargin}{#2}\setlength{\listparindent}{\parindent}\setlength{\itemindent}{\parindent}\setlength{\parsep}{\parskip}}\item[]}{\end{list}}


\begin{landscape}
\begin{changemargin}{0cm}{0cm}
\begin{center}

\topskip0pt
\vspace*{\fill}
\setlength{\tabcolsep}{4pt}
\begin{table}[!htbp]
\scriptsize
\caption{Models comparison. Mean scores and standard deviations over five training runs. Right most columns presents score for random agent and human.}
\begin{tabular}{lrlrlrlrlrlrlrlrlcc}

Game &          \multicolumn{2}{c}{Ours, deterministic}  &     \multicolumn{2}{c}{Ours, det. recurrent}   & \multicolumn{2}{c}{Ours, SD long} &     \multicolumn{2}{c}{Ours, SD} &     \multicolumn{2}{c}{Ours, SD }   &     \multicolumn{2}{c}{Ours, SD } &          \multicolumn{2}{c}{Ours, SD 100 steps}	&     \multicolumn{2}{c}{Ours, SD 25 steps} &		random &		human\\

\midrule
Alien          &    378.3 &    (85.5) &    321.7 &     (50.7) &\textbf{    616.9 }&    (252.2) &    405.2 &    (130.8) &    413.0 &     (89.7) &    590.2 &     (57.8) &    435.6 &     (78.9) &    534.8 &    (166.2) &    184.8 &   7128.0 \\
Amidar         &     62.4 &    (15.2) &     86.7 &     (18.8) &     74.3 &     (28.3) &\textbf{     88.0 }&     (23.8) &     50.3 &     (11.7) &     78.3 &     (18.8) &     37.7 &     (15.1) &     82.2 &     (43.0) &     11.8 &   1720.0 \\
Assault        &    361.4 &   (166.6) &    490.5 &    (143.6) &    527.2 &    (112.3) &    369.3 &    (107.8) &    406.7 &    (118.7) &    549.0 &    (127.9) &    311.7 &     (88.2) &\textbf{    664.5 }&    (298.2) &    233.7 &    742.0 \\
Asterix        &    668.0 &   (294.1) &\textbf{   1853.0 }&    (391.8) &   1128.3 &    (211.8) &   1089.5 &    (335.3) &    855.0 &    (176.4) &    921.6 &    (114.2) &    777.0 &    (200.4) &   1340.6 &    (627.5) &    248.8 &   8503.0 \\
Asteroids      &    743.7 &    (92.2) &    821.7 &    (115.6) &    793.6 &    (182.2) &    731.0 &    (165.3) &    882.0 &     (24.7) &\textbf{    886.8 }&     (45.2) &    821.9 &     (93.8) &    644.5 &    (110.6) &    649.0 &  47389.0 \\
Atlantis       &  14623.4 &  (2122.5) &  12584.4 &   (5823.6) &\textbf{  20992.5 }&  (11062.0) &  14481.6 &   (2436.9) &  18444.1 &   (4616.0) &  14055.6 &   (6226.1) &  14139.7 &   (2500.9) &  11641.2 &   (3385.0) &  16492.0 &  29028.0 \\
BankHeist      &     13.8 &     (2.5) &     15.1 &      (2.2) &\textbf{     34.2 }&     (29.2) &      8.2 &      (4.4) &     11.9 &      (2.5) &     12.0 &      (1.4) &     13.1 &      (3.2) &     12.7 &      (4.7) &     15.0 &    753.0 \\
BattleZone     &   3306.2 &   (794.1) &   4665.6 &   (2799.4) &   4031.2 &   (1156.1) &\textbf{   5184.4 }&   (1347.5) &   2781.2 &    (661.7) &   4000.0 &    (788.9) &   4068.8 &   (2912.1) &   3746.9 &   (1426.8) &   2895.0 &  37188.0 \\
BeamRider      &    463.8 &    (29.2) &    358.9 &     (87.4) &\textbf{    621.6 }&     (79.8) &    422.7 &    (103.6) &    456.2 &    (160.8) &    415.4 &    (103.4) &    456.0 &     (60.9) &    386.6 &    (264.4) &    372.1 &  16926.0 \\
Bowling        &     25.3 &    (10.4) &     22.3 &     (17.0) &     30.0 &      (5.8) &\textbf{     34.4 }&     (16.3) &     27.7 &      (5.2) &     23.9 &      (3.3) &     29.3 &      (7.5) &     33.2 &     (15.5) &     24.2 &    161.0 \\
Boxing         &     -9.3 &    (10.9) &     -3.1 &     (14.1) &      7.8 &     (10.1) &      9.1 &      (8.8) &\textbf{     11.6 }&     (12.6) &      5.1 &     (10.0) &     -2.1 &      (5.0) &      1.6 &     (14.7) &      0.3 &     12.0 \\
Breakout       &      6.1 &     (2.8) &     10.2 &      (5.1) &\textbf{     16.4 }&      (6.2) &     12.7 &      (3.8) &      7.3 &      (2.4) &      8.8 &      (5.1) &     11.4 &      (3.7) &      7.8 &      (4.1) &      0.9 &     30.0 \\
ChopperCommand &    906.9 &   (210.2) &    709.1 &    (174.1) &    979.4 &    (172.7) &\textbf{   1246.9 }&    (392.0) &    725.6 &    (204.2) &    946.6 &     (49.9) &    729.1 &    (185.1) &   1047.2 &    (221.6) &    671.0 &   7388.0 \\
CrazyClimber   &  19380.0 &  (6138.8) &  54700.3 &  (14480.5) &\textbf{  62583.6 }&  (16856.8) &  39827.8 &  (22582.6) &  49840.9 &  (11920.9) &  34353.1 &  (33547.2) &  48651.2 &  (14903.5) &  25612.2 &  (14037.5) &   7339.5 &  35829.0 \\
DemonAttack    &    191.9 &    (86.3) &    120.3 &     (38.3) &\textbf{    208.1 }&     (56.8) &    169.5 &     (41.8) &    187.5 &     (68.6) &    194.9 &     (89.6) &    170.1 &     (42.4) &    202.2 &    (134.0) &    140.0 &   1971.0 \\
FishingDerby   &    -94.5 &     (3.0) &    -96.9 &      (1.7) &    -90.7 &      (5.3) &    -91.5 &      (2.8) &    -91.0 &      (4.1) &    -92.6 &      (3.2) &\textbf{    -90.0 }&      (2.7) &    -94.5 &      (2.5) &    -93.6 &    -39.0 \\
Freeway        &      5.9 &    (13.1) &     23.7 &     (13.5) &     16.7 &     (15.7) &     20.3 &     (18.5) &     18.9 &     (17.2) &\textbf{     27.7 }&     (13.3) &     19.1 &     (16.7) &     27.3 &      (5.8) &      0.0 &     30.0 \\
Frostbite      &    196.4 &     (4.4) &    219.6 &     (21.4) &    236.9 &     (31.5) &\textbf{    254.7 }&      (4.9) &    234.6 &     (26.8) &    239.2 &     (19.1) &    226.8 &     (16.9) &    252.1 &     (54.4) &     74.0 &      - \\
Gopher         &    510.2 &   (158.4) &    225.2 &    (105.7) &    596.8 &    (183.5) &    771.0 &    (160.2) &\textbf{    845.6 }&    (230.3) &    612.6 &    (273.9) &    698.4 &    (213.9) &    509.7 &    (273.4) &    245.9 &   2412.0 \\
Gravitar       &\textbf{    237.0 }&    (73.1) &    213.8 &     (57.4) &    173.4 &     (54.7) &    198.3 &     (39.9) &    219.4 &      (7.8) &    213.0 &     (37.3) &    188.9 &     (27.6) &    116.4 &     (84.0) &    227.2 &   3351.0 \\
Hero           &    621.5 &  (1281.3) &    558.3 &   (1143.3) &   2656.6 &    (483.1) &   1295.1 &   (1600.1) &   2853.9 &    (539.5) &\textbf{   3503.5 }&    (892.9) &   3052.7 &    (169.3) &   1484.8 &   (1671.7) &    224.6 &  30826.0 \\
IceHockey      &    -12.6 &     (2.1) &    -14.0 &      (1.8) &    -11.6 &      (2.5) &\textbf{    -10.5 }&      (2.2) &    -12.2 &      (2.9) &    -11.9 &      (1.2) &    -13.5 &      (3.0) &    -13.9 &      (3.9) &     -9.7 &      1.0 \\
Jamesbond      &     68.8 &    (37.2) &    100.5 &     (69.8) &    100.5 &     (36.8) &    125.3 &    (112.5) &     28.9 &     (12.7) &     50.5 &     (21.3) &     68.9 &     (42.7) &\textbf{    163.4 }&     (81.8) &     29.2 &    303.0 \\
Kangaroo       &\textbf{    481.9 }&   (313.2) &    191.9 &    (301.0) &     51.2 &     (17.8) &    323.1 &    (359.8) &    148.1 &    (121.5) &     37.5 &      (8.0) &    301.2 &    (593.4) &    340.0 &    (470.4) &     42.0 &   3035.0 \\
Krull          &    834.9 &   (166.3) &   1778.5 &    (906.9) &   2204.8 &    (776.5) &\textbf{   4539.9 }&   (2470.4) &   2396.5 &    (962.0) &   2620.9 &    (856.2) &   3559.0 &   (1896.7) &   3320.6 &   (2410.1) &   1543.3 &   2666.0 \\
KungFuMaster   &  10340.9 &  (8835.7) &   4086.6 &   (3384.5) &  14862.5 &   (4031.6) &\textbf{  17257.2 }&   (5502.6) &  12587.8 &   (6810.0) &  16926.6 &   (6598.3) &  17121.2 &   (7211.6) &  15541.2 &   (5086.1) &    616.5 &  22736.0 \\
MsPacman       &    560.6 &   (172.2) &   1098.1 &    (450.9) &\textbf{   1480.0 }&    (288.2) &    762.8 &    (331.5) &   1197.1 &    (544.6) &   1273.3 &     (59.5) &    921.0 &    (306.0) &    805.8 &    (261.1) &    235.2 &   6952.0 \\
NameThisGame   &   1512.1 &   (408.3) &   2007.9 &    (367.0) &\textbf{   2420.7 }&    (289.4) &   1990.4 &    (284.7) &   2058.1 &    (103.7) &   2114.8 &    (387.4) &   2067.2 &    (304.8) &   1805.3 &    (453.4) &   2136.8 &   8049.0 \\
Pong           &    -17.4 &     (5.2) &    -11.6 &     (15.9) &\textbf{     12.8 }&     (17.2) &      5.2 &      (9.7) &     -2.9 &      (7.3) &     -2.5 &     (15.4) &    -13.9 &      (7.7) &     -1.0 &     (14.9) &    -20.4 &     15.0 \\
PrivateEye     &     16.4 &    (46.7) &     50.8 &     (43.2) &     35.0 &     (60.2) &     58.3 &     (45.4) &     54.4 &     (49.0) &     67.8 &     (26.4) &     88.3 &     (19.0) &\textbf{   1334.3 }&   (1794.5) &     26.6 &  69571.0 \\
Qbert          &    480.4 &   (158.8) &    603.7 &    (150.3) &\textbf{   1288.8 }&   (1677.9) &    559.8 &    (183.8) &    899.3 &    (474.3) &   1120.2 &    (697.1) &    534.4 &    (162.5) &    603.4 &    (138.2) &    166.1 &  13455.0 \\
Riverraid      &   1285.6 &   (604.6) &   1740.7 &    (458.1) &   1957.8 &    (758.1) &   1587.0 &    (818.0) &   1977.4 &    (332.7) &\textbf{   2115.1 }&    (106.2) &   1318.7 &    (540.4) &   1426.0 &    (374.0) &   1451.0 &  17118.0 \\
RoadRunner     &   5724.4 &  (3093.1) &   1228.8 &   (1025.9) &   5640.6 &   (3936.6) &   5169.4 &   (3939.0) &   1586.2 &   (1574.1) &\textbf{   8414.1 }&   (4542.8) &    722.2 &    (627.2) &   4366.2 &   (3867.8) &      0.0 &   7845.0 \\
Seaquest       &    419.5 &   (236.2) &    289.6 &    (110.4) &\textbf{    683.3 }&    (171.2) &    370.9 &    (128.2) &    364.6 &    (138.6) &    337.8 &     (79.0) &    247.8 &     (72.4) &    350.0 &    (136.8) &     61.1 &  42055.0 \\
UpNDown        &   1329.3 &   (495.3) &    926.7 &    (335.7) &\textbf{   3350.3 }&   (3540.0) &   2152.6 &   (1192.4) &   1291.2 &    (324.6) &   1250.6 &    (493.0) &   1828.4 &    (688.3) &   2136.5 &   (2095.0) &    488.4 &  11693.0 \\
YarsRevenge    &   3014.9 &   (397.4) &   3291.4 &   (1097.3) &\textbf{   5664.3 }&   (1870.5) &   2980.2 &    (778.6) &   2934.2 &    (459.2) &   3366.6 &    (493.0) &   2673.7 &    (216.8) &   4666.1 &   (1889.4) &   3121.2 &  54577.0 \\
\end{tabular}
\label{tab:meanStdDev}
\end{table}
\vspace*{\fill}
\end{center}
\end{changemargin}
\end{landscape}







\begin{landscape}
\begin{changemargin}{0cm}{0cm}
\begin{center}
\topskip0pt
\vspace*{\fill}
\setlength{\tabcolsep}{5pt}

\begin{table}[!htbp]
\scriptsize
\caption{Comparison of our method (SimPLe) with model-free benchmarks - PPO and Rainbow, trained with 100 thousands/500 thousands/1 million steps. (1 step equals 4 frames)}
\begin{tabular}{lrlrlrlrlrlrlrlcc}

Game &          \multicolumn{2}{c}{SimPLe}  &     \multicolumn{2}{c}{PPO\_100k}   &     \multicolumn{2}{c}{PPO\_500k} &     \multicolumn{2}{c}{PPO\_1m}   &     \multicolumn{2}{c}{Rainbow\_100k} &          \multicolumn{2}{c}{Rainbow\_500k}	&     \multicolumn{2}{c}{Rainbow\_1m} &		random &		human\\


\midrule
Alien          &    616.9 &    (252.2) &    291.0 &    (40.3) &      269.0 &      (203.4) &    362.0 &    (102.0) &    290.6 &   (14.8) &    828.6 &     (54.2) &     945.0 &     (85.0) &    184.8 &   7128.0 \\
Amidar         &     74.3 &     (28.3) &     56.5 &    (20.8) &       93.2 &       (36.7) &    123.8 &     (19.7) &     20.8 &    (2.3) &    194.0 &     (34.9) &     275.8 &     (66.7) &     11.8 &   1720.0 \\
Assault        &    527.2 &    (112.3) &    424.2 &    (55.8) &      552.3 &      (110.4) &   1134.4 &    (798.8) &    300.3 &   (14.6) &   1041.5 &     (92.1) &    1581.8 &    (207.8) &    233.7 &    742.0 \\
Asterix        &   1128.3 &    (211.8) &    385.0 &   (104.4) &     1085.0 &      (354.8) &   2185.0 &    (931.6) &    285.7 &    (9.3) &   1702.7 &    (162.8) &    2151.6 &    (202.6) &    248.8 &   8503.0 \\
Asteroids      &    793.6 &    (182.2) &   1134.0 &   (326.9) &     1053.0 &      (433.3) &   1251.0 &    (377.9) &    912.3 &   (62.7) &    895.9 &     (82.0) &    1071.5 &     (91.7) &    649.0 &  47389.0 \\
Atlantis       &  20992.5 &  (11062.0) &  34316.7 &  (5703.8) &  4836416.7 &  (6218247.3) &      - &      (-) &  17881.8 &  (617.6) &  79541.0 &  (25393.4) &  848800.0 &  (37533.1) &  16492.0 &  29028.0 \\
BankHeist      &     34.2 &     (29.2) &     16.0 &    (12.4) &      641.0 &      (352.8) &    856.0 &    (376.7) &     34.5 &    (2.0) &    727.3 &    (198.3) &    1053.3 &     (22.9) &     15.0 &    753.0 \\
BattleZone     &   4031.2 &   (1156.1) &   5300.0 &  (3655.1) &    14400.0 &     (6476.1) &  19000.0 &   (4571.7) &   3363.5 &  (523.8) &  19507.1 &   (3193.3) &   22391.4 &   (7708.9) &   2895.0 &  37188.0 \\
BeamRider      &    621.6 &     (79.8) &    563.6 &   (189.4) &      497.6 &      (103.5) &    684.0 &    (168.8) &    365.6 &   (29.8) &   5890.0 &    (525.6) &    6945.3 &   (1390.8) &    372.1 &  16926.0 \\
Bowling        &     30.0 &      (5.8) &     17.7 &    (11.2) &       28.5 &        (3.4) &     35.8 &      (6.2) &     24.7 &    (0.8) &     31.0 &      (1.9) &      30.6 &      (6.2) &     24.2 &    161.0 \\
Boxing         &      7.8 &     (10.1) &     -3.9 &     (6.4) &        3.5 &        (3.5) &     19.6 &     (20.9) &      0.9 &    (1.7) &     58.2 &     (16.5) &      80.3 &      (5.6) &      0.3 &     12.0 \\
Breakout       &     16.4 &      (6.2) &      5.9 &     (3.3) &       66.1 &      (114.3) &    128.0 &    (153.3) &      3.3 &    (0.1) &     26.7 &      (2.4) &      38.7 &      (3.4) &      0.9 &     30.0 \\
ChopperCommand &    979.4 &    (172.7) &    730.0 &   (199.0) &      860.0 &      (285.3) &    970.0 &    (201.5) &    776.6 &   (59.0) &   1765.2 &    (280.7) &    2474.0 &    (504.5) &    671.0 &   7388.0 \\
CrazyClimber   &  62583.6 &  (16856.8) &  18400.0 &  (5275.1) &    33420.0 &     (3628.3) &  58000.0 &  (16994.6) &  12558.3 &  (674.6) &  75655.1 &   (9439.6) &   97088.1 &   (9975.4) &   7339.5 &  35829.0 \\
DemonAttack    &    208.1 &     (56.8) &    192.5 &    (83.1) &      216.5 &       (96.2) &    241.0 &    (135.0) &    431.6 &   (79.5) &   3642.1 &    (478.2) &    5478.6 &    (297.9) &    140.0 &   1971.0 \\
FishingDerby   &    -90.7 &      (5.3) &    -95.6 &     (4.3) &      -87.2 &        (5.3) &    -88.8 &      (4.0) &    -91.1 &    (2.1) &    -66.7 &      (6.0) &     -23.2 &     (22.3) &    -93.6 &    -39.0 \\
Freeway        &     16.7 &     (15.7) &      8.0 &     (9.8) &       14.0 &       (11.5) &     20.8 &     (11.1) &      0.1 &    (0.1) &     12.6 &     (15.4) &      13.0 &     (15.9) &      0.0 &     30.0 \\
Frostbite      &    236.9 &     (31.5) &    174.0 &    (40.7) &      214.0 &       (10.2) &    229.0 &     (20.6) &    140.1 &    (2.7) &   1386.1 &    (321.7) &    2972.3 &    (284.9) &     74.0 &      - \\
Gopher         &    596.8 &    (183.5) &    246.0 &   (103.3) &      560.0 &      (118.8) &    696.0 &    (279.3) &    748.3 &  (105.4) &   1640.5 &    (105.6) &    1905.0 &    (211.1) &    245.9 &   2412.0 \\
Gravitar       &    173.4 &     (54.7) &    235.0 &   (197.2) &      235.0 &      (134.7) &    325.0 &     (85.1) &    231.4 &   (50.7) &    214.9 &     (27.6) &     260.0 &     (22.7) &    227.2 &   3351.0 \\
Hero           &   2656.6 &    (483.1) &    569.0 &  (1100.9) &     1824.0 &     (1461.2) &   3719.0 &   (1306.0) &   2676.3 &   (93.7) &  10664.3 &   (1060.5) &   13295.5 &    (261.2) &    224.6 &  30826.0 \\
IceHockey      &    -11.6 &      (2.5) &    -10.0 &     (2.1) &       -6.6 &        (1.6) &     -5.3 &      (1.7) &     -9.5 &    (0.8) &     -9.7 &      (0.8) &      -6.5 &      (0.5) &     -9.7 &      1.0 \\
Jamesbond      &    100.5 &     (36.8) &     65.0 &    (46.4) &      255.0 &      (101.7) &    310.0 &    (129.0) &     61.7 &    (8.8) &    429.7 &     (27.9) &     692.6 &    (316.2) &     29.2 &    303.0 \\
Kangaroo       &     51.2 &     (17.8) &    140.0 &   (102.0) &      340.0 &      (407.9) &    840.0 &    (806.5) &     38.7 &    (9.3) &    970.9 &    (501.9) &    4084.6 &   (1954.1) &     42.0 &   3035.0 \\
Krull          &   2204.8 &    (776.5) &   3750.4 &  (3071.9) &     3056.1 &     (1155.5) &   5061.8 &   (1333.4) &   2978.8 &  (148.4) &   4139.4 &    (336.2) &    4971.1 &    (360.3) &   1543.3 &   2666.0 \\
KungFuMaster   &  14862.5 &   (4031.6) &   4820.0 &   (983.2) &    17370.0 &    (10707.6) &  13780.0 &   (3971.6) &   1019.4 &  (149.6) &  19346.1 &   (3274.4) &   21258.6 &   (3210.2) &    616.5 &  22736.0 \\
MsPacman       &   1480.0 &    (288.2) &    496.0 &   (379.8) &      306.0 &       (70.2) &    594.0 &    (247.9) &    364.3 &   (20.4) &   1558.0 &    (248.9) &    1881.4 &    (112.0) &    235.2 &   6952.0 \\
NameThisGame   &   2420.7 &    (289.4) &   2225.0 &   (423.7) &     2106.0 &      (898.8) &   2311.0 &    (547.6) &   2368.2 &  (318.3) &   4886.5 &    (583.1) &    4454.2 &    (338.3) &   2136.8 &   8049.0 \\
Pong           &     12.8 &     (17.2) &    -20.5 &     (0.6) &       -8.6 &       (14.9) &     14.7 &      (5.1) &    -19.5 &    (0.2) &     19.9 &      (0.4) &      20.6 &      (0.2) &    -20.4 &     15.0 \\
PrivateEye     &     35.0 &     (60.2) &     10.0 &    (20.0) &       20.0 &       (40.0) &     20.0 &     (40.0) &     42.1 &   (53.8) &     -6.2 &     (89.8) &    2336.7 &   (4732.6) &     26.6 &  69571.0 \\
Qbert          &   1288.8 &   (1677.9) &    362.5 &   (117.8) &      757.5 &       (78.9) &   2675.0 &   (1701.1) &    235.6 &   (12.9) &   4241.7 &    (193.1) &    8885.2 &   (1690.9) &    166.1 &  13455.0 \\
Riverraid      &   1957.8 &    (758.1) &   1398.0 &   (513.8) &     2865.0 &      (327.1) &   2887.0 &    (807.0) &   1904.2 &   (44.2) &   5068.6 &    (292.6) &    7018.9 &    (334.2) &   1451.0 &  17118.0 \\
RoadRunner     &   5640.6 &   (3936.6) &   1430.0 &   (760.0) &     5750.0 &     (5259.9) &   8930.0 &   (4304.0) &    524.1 &  (147.5) &  18415.4 &   (5280.0) &   31379.7 &   (3225.8) &      0.0 &   7845.0 \\
Seaquest       &    683.3 &    (171.2) &    370.0 &   (103.3) &      692.0 &       (48.3) &    882.0 &    (122.7) &    206.3 &   (17.1) &   1558.7 &    (221.2) &    3279.9 &    (683.9) &     61.1 &  42055.0 \\
UpNDown        &   3350.3 &   (3540.0) &   2874.0 &  (1105.8) &    12126.0 &     (1389.5) &  13777.0 &   (6766.3) &   1346.3 &   (95.1) &   6120.7 &    (356.8) &    8010.9 &    (907.0) &    488.4 &  11693.0 \\
YarsRevenge    &   5664.3 &   (1870.5) &   5182.0 &  (1209.3) &     8064.8 &     (2859.8) &   9495.0 &   (2638.3) &   3649.0 &  (168.6) &   7005.7 &    (394.2) &    8225.1 &    (957.9) &   3121.2 &  54577.0 \\


\end{tabular}
\label{tab:ppo_rainbow_comparison}
\end{table}
\vspace*{\fill}
\end{center}
\end{changemargin}
\end{landscape}



\begin{landscape}
\begin{changemargin}{0cm}{0cm}
\begin{center}
\topskip0pt
\vspace*{\fill}
\setlength{\tabcolsep}{5pt}
\begin{table}[!htbp]
\scriptsize
\caption{Models comparison. Scores of median (left) and best (right) models out of five training runs. Right most columns presents score for random agent and human.}
\begin{tabular}{lrlrlrlrlrlrlrlrlcc}

Game &          \multicolumn{2}{c}{Ours, deterministic}  &     \multicolumn{2}{c}{Ours, det. recurrent}   &     \multicolumn{2}{c}{Ours, SD  long}   &     \multicolumn{2}{c}{Ours, SD} &     \multicolumn{2}{c}{Ours, SD }   &     \multicolumn{2}{c}{Ours, SD } &          \multicolumn{2}{c}{SD 100 steps}	&     \multicolumn{2}{c}{Ours, SD 25 steps} &		random &		human\\
\midrule
Alien          &    354.4 &    516.6 &    299.2 &    381.1 &    515.9 &   1030.5 &    409.2 &    586.9 &    411.9 &    530.5 &    567.3 &    682.7 &    399.5 &    522.3 &    525.5 &    792.8 &    184.8 &   7128.0 \\
Amidar         &     58.0 &     84.8 &     82.7 &    118.4 &     80.2 &    102.7 &     85.1 &    114.0 &     55.1 &     58.9 &     84.3 &    101.4 &     45.2 &     47.5 &     93.1 &    137.7 &     11.8 &   1720.0 \\
Assault        &    334.4 &    560.1 &    566.6 &    627.2 &    509.1 &    671.1 &    355.7 &    527.9 &    369.1 &    614.4 &    508.4 &    722.5 &    322.9 &    391.1 &    701.4 &   1060.3 &    233.7 &    742.0 \\
Asterix        &    529.7 &   1087.5 &   1798.4 &   2282.0 &   1065.6 &   1485.2 &   1158.6 &   1393.8 &    805.5 &   1159.4 &    923.4 &   1034.4 &    813.3 &   1000.0 &   1128.1 &   2313.3 &    248.8 &   8503.0 \\
Asteroids      &    727.3 &    854.7 &    827.7 &    919.8 &    899.7 &    955.6 &    671.2 &    962.0 &    885.5 &    909.1 &    886.1 &    949.5 &    813.8 &    962.2 &    657.5 &    752.7 &    649.0 &  47389.0 \\
Atlantis       &  15587.5 &  16545.3 &  15939.1 &  17778.1 &  13695.3 &  34890.6 &  13645.3 &  18396.9 &  19367.2 &  23046.9 &  12981.2 &  23579.7 &  15020.3 &  16790.6 &  12196.9 &  15728.1 &  16492.0 &  29028.0 \\
BankHeist      &     14.4 &     16.2 &     14.7 &     18.8 &     31.9 &     77.5 &      8.9 &     13.9 &     12.3 &     14.5 &     12.3 &     13.1 &     12.8 &     17.2 &     14.1 &     17.0 &     15.0 &    753.0 \\
BattleZone     &   3312.5 &   4140.6 &   4515.6 &   9312.5 &   3484.4 &   5359.4 &   5390.6 &   7093.8 &   2937.5 &   3343.8 &   4421.9 &   4703.1 &   3500.0 &   8906.2 &   3859.4 &   5734.4 &   2895.0 &  37188.0 \\
BeamRider      &    453.1 &    515.5 &    351.4 &    470.2 &    580.2 &    728.8 &    433.9 &    512.6 &    393.5 &    682.8 &    446.6 &    519.2 &    447.1 &    544.6 &    385.7 &    741.9 &    372.1 &  16926.0 \\
Bowling        &     27.0 &     36.2 &     28.4 &     43.7 &     28.0 &     39.6 &     24.9 &     55.0 &     27.7 &     34.9 &     22.6 &     28.6 &     28.4 &     39.9 &     37.0 &     54.7 &     24.2 &    161.0 \\
Boxing         &     -7.1 &      0.2 &      3.5 &      5.0 &      9.4 &     21.0 &      8.3 &     21.5 &      6.4 &     31.5 &      2.5 &     15.0 &     -0.7 &      2.2 &     -0.9 &     20.8 &      0.3 &     12.0 \\
Breakout       &      5.5 &      9.8 &     12.5 &     13.9 &     16.0 &     22.8 &     11.0 &     19.5 &      7.4 &     10.4 &     10.2 &     14.1 &     10.5 &     16.7 &      6.9 &     13.0 &      0.9 &     30.0 \\
ChopperCommand &    942.2 &   1167.2 &    748.4 &    957.8 &    909.4 &   1279.7 &   1139.1 &   1909.4 &    682.8 &   1045.3 &    954.7 &   1010.9 &    751.6 &    989.1 &   1031.2 &   1329.7 &    671.0 &   7388.0 \\
CrazyClimber   &  20754.7 &  23831.2 &  49854.7 &  80156.2 &  55795.3 &  87593.8 &  41396.9 &  67250.0 &  56875.0 &  58979.7 &  19448.4 &  84070.3 &  53406.2 &  64196.9 &  19345.3 &  43179.7 &   7339.5 &  35829.0 \\
DemonAttack    &    219.2 &    263.0 &    135.8 &    148.4 &    191.2 &    288.9 &    182.4 &    223.9 &    160.3 &    293.8 &    204.1 &    312.8 &    164.4 &    222.6 &    187.5 &    424.8 &    140.0 &   1971.0 \\
FishingDerby   &    -94.3 &    -90.2 &    -97.3 &    -94.2 &    -91.8 &    -84.3 &    -91.6 &    -88.6 &    -90.0 &    -85.7 &    -92.0 &    -88.8 &    -90.6 &    -85.4 &    -95.0 &    -90.7 &    -93.6 &    -39.0 \\
Freeway        &      0.0 &     29.3 &     29.3 &     32.2 &     21.5 &     32.0 &     33.5 &     34.0 &     31.1 &     32.0 &     33.5 &     33.8 &     30.0 &     32.3 &     29.9 &     33.5 &      0.0 &     30.0 \\
Frostbite      &    194.5 &    203.9 &    213.4 &    256.2 &    248.8 &    266.9 &    253.1 &    262.8 &    246.7 &    261.7 &    250.0 &    255.9 &    215.8 &    247.7 &    249.4 &    337.5 &     74.0 &      - \\
Gopher         &    514.7 &    740.6 &    270.3 &    320.9 &    525.3 &    845.6 &    856.9 &    934.4 &    874.1 &   1167.2 &    604.1 &   1001.6 &    726.9 &    891.6 &    526.2 &    845.0 &    245.9 &   2412.0 \\
Gravitar       &    232.8 &    310.2 &    219.5 &    300.0 &    156.2 &    233.6 &    202.3 &    252.3 &    223.4 &    225.8 &    228.1 &    243.8 &    193.8 &    218.0 &     93.0 &    240.6 &    227.2 &   3351.0 \\
Hero           &     71.5 &   2913.0 &     75.0 &   2601.5 &   2935.0 &   3061.6 &    237.5 &   3133.8 &   3135.0 &   3147.5 &   3066.2 &   5092.0 &   3067.3 &   3256.9 &   1487.2 &   2964.8 &    224.6 &  30826.0 \\
IceHockey      &    -12.4 &     -9.9 &    -14.8 &    -11.8 &    -12.3 &     -7.2 &    -10.0 &     -7.7 &    -11.8 &     -8.5 &    -11.6 &    -10.7 &    -12.9 &    -10.0 &    -12.2 &    -11.0 &     -9.7 &      1.0 \\
Jamesbond      &     64.8 &    128.9 &     64.8 &    219.5 &    110.9 &    141.4 &     87.5 &    323.4 &     25.0 &     46.9 &     58.6 &     69.5 &     61.7 &    139.1 &    139.8 &    261.7 &     29.2 &    303.0 \\
Kangaroo       &    500.0 &    828.1 &     68.8 &    728.1 &     62.5 &     65.6 &    215.6 &    909.4 &    103.1 &    334.4 &     34.4 &     50.0 &     43.8 &   1362.5 &     56.2 &   1128.1 &     42.0 &   3035.0 \\
Krull          &    852.2 &   1014.3 &   1783.6 &   2943.6 &   1933.7 &   3317.5 &   4264.3 &   7163.2 &   1874.8 &   3554.5 &   2254.0 &   3827.1 &   3142.8 &   6315.2 &   3198.2 &   6833.4 &   1543.3 &   2666.0 \\
KungFuMaster   &   7575.0 &  20450.0 &   4848.4 &   8065.6 &  14318.8 &  21054.7 &  17448.4 &  21943.8 &  12964.1 &  21956.2 &  20195.3 &  23690.6 &  19718.8 &  25375.0 &  18025.0 &  20365.6 &    616.5 &  22736.0 \\
MsPacman       &    557.3 &    818.0 &   1178.8 &   1685.9 &   1525.0 &   1903.4 &    751.2 &   1146.1 &   1410.5 &   1538.9 &   1277.3 &   1354.5 &    866.2 &   1401.9 &    777.2 &   1227.8 &    235.2 &   6952.0 \\
NameThisGame   &   1468.1 &   1992.7 &   1826.7 &   2614.5 &   2460.0 &   2782.8 &   1919.8 &   2377.7 &   2087.3 &   2155.2 &   1994.8 &   2570.3 &   2153.4 &   2471.9 &   1964.2 &   2314.8 &   2136.8 &   8049.0 \\
Pong           &    -19.6 &     -8.5 &    -17.3 &     16.7 &     20.7 &     21.0 &      1.4 &     21.0 &     -2.0 &      6.6 &      3.8 &     14.2 &    -17.9 &     -2.0 &    -10.1 &     21.0 &    -20.4 &     15.0 \\
PrivateEye     &      0.0 &     98.9 &     75.0 &     82.8 &      0.0 &    100.0 &     76.6 &    100.0 &     75.0 &     96.9 &     60.9 &    100.0 &     96.9 &     99.3 &    100.0 &   4038.7 &     26.6 &  69571.0 \\
Qbert          &    476.6 &    702.7 &    555.9 &    869.9 &    656.2 &   4259.0 &    508.6 &    802.7 &    802.3 &   1721.9 &    974.6 &   2322.3 &    475.0 &    812.5 &    668.8 &    747.3 &    166.1 &  13455.0 \\
Riverraid      &   1416.1 &   1929.4 &   1784.4 &   2274.5 &   2360.0 &   2659.8 &   1799.4 &   2158.4 &   2053.8 &   2307.5 &   2143.6 &   2221.2 &   1387.8 &   1759.8 &   1345.5 &   1923.4 &   1451.0 &  17118.0 \\
RoadRunner     &   5901.6 &   8484.4 &    781.2 &   2857.8 &   5906.2 &  11176.6 &   2804.7 &  10676.6 &   1620.3 &   4104.7 &   7032.8 &  14978.1 &    857.8 &   1342.2 &   2717.2 &   8560.9 &      0.0 &   7845.0 \\
Seaquest       &    414.4 &    768.1 &    236.9 &    470.6 &    711.6 &    854.1 &    386.9 &    497.2 &    330.9 &    551.2 &    332.8 &    460.9 &    274.1 &    317.2 &    366.9 &    527.2 &     61.1 &  42055.0 \\
UpNDown        &   1195.9 &   2071.1 &   1007.5 &   1315.2 &   1616.1 &   8614.5 &   2389.5 &   3798.3 &   1433.3 &   1622.0 &   1248.6 &   1999.4 &   1670.3 &   2728.0 &   1825.2 &   5193.1 &    488.4 &  11693.0 \\
YarsRevenge    &   3047.0 &   3380.5 &   3416.3 &   4230.8 &   6580.2 &   7547.4 &   2435.5 &   3914.1 &   2955.9 &   3314.5 &   3434.8 &   3896.3 &   2745.3 &   2848.1 &   4276.3 &   6673.1 &   3121.2 &  54577.0 \\
\end{tabular}
\label{tab:minmax}
\end{table}
\vspace*{\fill}
\end{center}
\end{changemargin}
\end{landscape}


\section{Baselines optimization}\label{sec:baselines_optimizations}

To assess the performance of SimPle we compare it with model-free algorithms. To make this comparison more reliable we tuned Rainbow in the low data regime. To this end we run an hyperparameter search over the following parameters from \url{https://github.com/google/dopamine/blob/master/dopamine/agents/rainbow/rainbow_agent.py}:

\begin{itemize}
    \item \texttt{update\_horizon} in \{1, 3\}, best parameter = 3
    \item \texttt{min\_replay\_history} in \{500, 5000, 20000\}, best parameter = 20000
    \item \texttt{update\_period} in \{1, 4\}, best parameter = 4
    \item \texttt{target\_update\_period} \{50, 100, 1000, 4000\}, best parameter = 8000
    \item \texttt{replay\_scheme} in \{\texttt{uniform}, \texttt{prioritized}\}, best parameter = \texttt{prioritized}
\end{itemize}
Each set of hyperparameters was used to train  Rainbow agents on the game of \texttt{Pong} until  million of interactions with the environment.
Their average performance was used to pick the best hyperparameter set.

For PPO we used the standard set of hyperparameters from \url{https://github.com/openai/baselines}.

\section{Results at different numbers of interactions} \label{appdx:num_samples}

\newcommand{\mywidthappd}{3.4in}

\begin{figure}
\vspace{-1.6cm}
\begin{tabular}{@{\hskip -5.5em}cc}
\subfloat[Fraction at K clipped to .]{\includegraphics[width = \mywidthappd]{figures/nfraction_of_rainbow_at_100K.png}} &

\subfloat[Fraction at K]{\includegraphics[width = \mywidthappd]{figures/nfraction_of_rainbow_at_200K.png}}  \\

\subfloat[Fraction at K.]{\includegraphics[width = \mywidthappd]{figures/nfraction_of_rainbow_at_500K.png}} &

\subfloat[Fraction at M.]{\includegraphics[width = \mywidthappd]{figures/nfraction_of_rainbow_at_1M.png}}
\end{tabular}
\caption{Fractions of the rainbow scores at given number of samples. These were calculate with the formula ; if denominator is smaller than 0, both nominator and denominator are increased by 1.}
\end{figure}
\newpage

\begin{figure}
\vspace{-1.6cm}
\begin{tabular}{@{\hskip -5.5em}cc}

\subfloat[Fraction at K clipped to .]{\includegraphics[width = \mywidthappd]{figures/nfraction_of_ppo_at_100K.png}} &

\subfloat[Fraction at K]{\includegraphics[width = \mywidthappd]{figures/nfraction_of_ppo_at_200K.png}}  \\

\subfloat[Fraction at K.]{\includegraphics[width = \mywidthappd]{figures/nfraction_of_ppo_at_500K.png}} &

\subfloat[Fraction at M.]{\includegraphics[width = \mywidthappd]{figures/nfraction_of_ppo_at_1M.png}}
\end{tabular}
\caption{Fractions of the ppo scores at given number of samples. These were calculate with the formula ; if denominator is smaller than 0, both nominator and denominator are increased by 1.}
\end{figure}

\begin{figure}
\vspace{-1.7cm}
\begin{tabular}{@{\hskip -5.5em}cc}

\subfloat[SimPLe compared to Rainbow at K.]{\includegraphics[width = \mywidthappd]{figures/difference_from_rainbow_at_100K.png}} &

\subfloat[SimPLe compared to Rainbow at K]{\includegraphics[width = \mywidthappd]{figures/difference_from_rainbow_at_200K.png}}  \\

\subfloat[SimPLe compared to PPO at K.]{\includegraphics[width = \mywidthappd]{figures/difference_from_ppo_at_100K.png}} &

\subfloat[SimPLe compared to PPO at K.]{\includegraphics[width = \mywidthappd]{figures/difference_from_ppo_at_200K.png}}
\end{tabular}
\caption{Comparison of scores from Simple against Rainbow and PPO at different numbers of interactions.
The following formula is used: .
Points are normalized by average human score in order to be presentable in one graph.}
\end{figure}



