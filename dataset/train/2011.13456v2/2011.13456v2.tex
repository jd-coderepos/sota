
\documentclass{article} \usepackage{iclr2021_conference,times}
\iclrfinalcopy

\usepackage{xcolor}
\usepackage{hyperref}       \usepackage{url}
\hypersetup{
  colorlinks,
  linkcolor={red!50!black},
  citecolor={blue!50!black},
  urlcolor={blue!80!black}
  }
  \definecolor{orange}{HTML}{ff7f0e}
  \definecolor{blue}{HTML}{1f77b4}

\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{algorithmicx}

\usepackage{microtype}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{sidecap}
\usepackage{caption}
\usepackage{booktabs} \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{comment}
\usepackage{enumitem}
\usepackage{wrapfig,tikz}
\usepackage{amsmath,longtable,fancyhdr}
\usepackage{bigstrut, tabularx, multirow, makecell, diagbox}
\usepackage[export]{adjustbox}
\usepackage{graphicx,float}
\usepackage{amssymb,amsthm, mathtools}
\usepackage{stackrel}
\usepackage{color}
\usepackage{colortbl}
\usepackage{theoremref}
\usepackage{cases}
\usepackage{stmaryrd}
\usepackage{mathabx}
\usepackage{dsfont}
\usepackage[capitalise]{cleveref}
\newcommand{\abs}[1]{\lvert#1\rvert}
\newcommand*{\tran}{^{\mkern-1.5mu\mathsf{T}}}
\newcommand{\mbf}[1]{\mathbf{#1}}
\newcommand{\bs}[1]{\boldsymbol{#1}}
\newcommand{\mbb}[1]{\mathbb{#1}}
\newcolumntype{C}[1]{>{\centering\arraybackslash}m{#1}}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\newcommand{\ud}{\mathrm{d}}
\newcommand{\up}{\mathrm}
\def\dbar{\mathrm{\mathchar'26\mkern-12mu d}}
\newcommand{\wave}{\sim}
\newcommand{\mcal}{\mathcal}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newtheorem{problem}{Problem}
\usepackage{mdframed}
\newtheorem{recipe}{Technique}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{remark}{Remark}
\newtheorem{observation}{Observation}
\newtheorem{claim}{Claim}
\newtheorem{proposition}{Proposition}
\newtheorem{assumption}{Assumption}
\newenvironment{oneshot}[1]{\@begintheorem{#1}{\unskip}}{\@endtheorem}
\newtheorem{innercustomthm}{Theorem}
\newtheorem{definition}{Definition}
\newenvironment{customthm}[1]
{\renewcommand\theinnercustomthm{#1}\innercustomthm}
{\endinnercustomthm}
\newtheorem{innercustomlem}{Lemma}
\newenvironment{customlem}[1]
{\renewcommand\theinnercustomlem{#1}\innercustomlem}
{\endinnercustomlem}
\newtheorem{innercustomhyp}{Hypothesis}
\newenvironment{customhyp}[1]
{\renewcommand\theinnercustomhyp{#1}\innercustomhyp}
{\endinnercustomhyp}
\newtheorem{innercustomprop}{Proposition}
\newenvironment{customprop}[1]
{\renewcommand\theinnercustomprop{#1}\innercustomprop}
{\endinnercustomprop}

\newcommand{\ys}[1]{{\color{magenta} [YS: {#1}]}}

\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\definecolor{Gray}{gray}{0.85}
\definecolor{LightCyan}{rgb}{0.88,1,1}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argsup}{arg\,sup}
\DeclareMathOperator*{\arginf}{arg\,inf}

\makeatletter
\usepackage{xspace} 
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}

\newcommand{\eqnref}[1]{Eq\onedot~\eqref{#1}}

\newcommand{\bfx}{\mathbf{x}}
\newcommand{\bfA}{\mathbf{A}}
\newcommand{\bfB}{\mathbf{B}}
\newcommand{\bfW}{\mathbf{W}}
\newcommand{\bfw}{\mathbf{w}}
\newcommand{\bfV}{\mathbf{V}}
\newcommand{\bfM}{\mathbf{M}}
\newcommand{\bfa}{\mathbf{a}}
\newcommand{\bfb}{\mathbf{b}}
\newcommand{\bfv}{\mathbf{v}}
\newcommand{\bfz}{\mathbf{z}}
\newcommand{\bfI}{\mathbf{I}}
\newcommand{\bft}{\mathbf{t}}
\newcommand{\bfu}{\mathbf{u}}
\newcommand{\bfr}{\mathbf{r}}
\newcommand{\bff}{\mathbf{f}}
\newcommand{\bfm}{\mathbf{m}}
\newcommand{\bfsigma}{\mbf{\Sigma}}
\newcommand{\bfF}{\mathbf{F}}
\newcommand{\bfL}{\mathbf{L}}
\newcommand{\bfzero}{\mathbf{0}}
\newcommand{\bfe}{{\bs{\epsilon}}}
\newcommand{\bftheta}{{\boldsymbol{\theta}}}
\newcommand{\bfalpha}{{\boldsymbol{\alpha}}}
\newcommand{\bfphi}{{\boldsymbol{\phi}}}
\newcommand{\bfy}{\mathbf{y}}
\newcommand{\bfs}{\mathbf{s}}
\newcommand{\bfh}{\mathbf{h}}
\newcommand{\bfg}{\mathbf{g}}
\newcommand{\bfG}{\mathbf{G}}
\newcommand{\bfxd}{\mathbf{x}_{\text{data}}}
\newcommand{\pd}{p_{\mathrm{data}}}
\newcommand{\sm}{\sigma_{\text{min}}}
\newcommand{\sM}{\sigma_{\text{max}}}
\newcommand{\bbeta}{\bar{\beta}}
\newcommand{\bomega}{{\bar{\Omega}}}
\newcommand{\bbetam}{\bbeta_{\text{min}}}
\newcommand{\bbetaM}{\bbeta_{\text{max}}}
\newcommand{\betam}{\beta_{\text{min}}}
\newcommand{\betaM}{\beta_{\text{max}}}
\newcommand{\bfmu}{{\boldsymbol \mu}}
\definecolor{blue1}{RGB}{0,128,255}
\definecolor{blue3}{RGB}{0,0,128}
\definecolor{darkpastelgreen}{rgb}{0.01, 0.75, 0.24}
\definecolor{cerulean}{rgb}{0.0, 0.48, 0.65}

\def\eg{\emph{e.g}\onedot}
\def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot}
\def\Ie{\emph{I.e}\onedot}
\def\cf{\emph{cf}\onedot}
\def\Cf{\emph{Cf}\onedot}
\def\etc{\emph{etc}\onedot}
\def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot}
\def\dof{d.o.f\onedot}
\def\aka{a.k.a\onedot}
\def\iid{i.i.d\onedot}
\def\etal{\emph{et al}\onedot}

\definecolor{darkgreen}{rgb}{0,0.6,0}
\newcommand{\js}[1]{\textcolor{darkgreen}{{[jascha: #1]}}}
\newcommand{\se}[1]{\textcolor{magenta}{{[SE: #1]}}}
\newcommand{\todo}[1]{\textcolor{red}{{[TODO: #1]}}}
\newcommand{\abhi}[1]{\textcolor{brown}{{[AK: #1]}}}

\title{Score-Based Generative Modeling through Stochastic Differential Equations}


\author{Yang Song\thanks{Work partially done during an internship at Google Brain.}\\
Stanford University\\
\texttt{yangsong@cs.stanford.edu}\\
\And 
Jascha Sohl-Dickstein\\
Google Brain\\
\texttt{jaschasd@google.com}\\
\And
Diederik P. Kingma \\
Google Brain\\
\texttt{durk@google.com}\\
\AND
Abhishek Kumar \\
Google Brain\\
\texttt{abhishk@google.com}\\
\And 
Stefano Ermon \\
Stanford University\\
\texttt{ermon@cs.stanford.edu}\\
\And
Ben Poole\\
Google Brain\\
\texttt{pooleb@google.com}
}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}


\maketitle

\begin{abstract}
Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. 
Crucially, the reverse-time SDE depends only on the time-dependent gradient field (\aka, score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, 
as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of $1024\times 1024$ images for the first time from a score-based generative model.
\end{abstract} \section{Introduction}



Two successful classes of probabilistic generative models involve sequentially corrupting training data with slowly increasing noise, and then learning to reverse this corruption in order to form a generative model of the data. 
\emph{Score matching with Langevin dynamics} (SMLD)~\citep{song2019generative} estimates the \emph{score} (\ie, the gradient of the log probability density with respect to data) at each noise scale, and then uses Langevin dynamics to sample from a sequence of decreasing noise scales during generation. 
\emph{Denoising diffusion probabilistic modeling} (DDPM)~\citep{sohl2015deep,ho2020denoising} trains a sequence of probabilistic models to reverse each step of the noise corruption, 
using knowledge of the functional form of the reverse distributions to make training tractable. 
For continuous state spaces, the DDPM training objective implicitly computes 
scores at each noise scale. 
We therefore refer to these two model classes together as \emph{score-based generative models}. 

Score-based generative models, and related techniques \citep{bordes2017learning,goyal2017variational, du2019implicit}, have proven effective at generation of images~\citep{song2019generative,song2020improved,ho2020denoising}, audio~\citep{chen2020wavegrad,kong2020diffwave}, graphs~\citep{niu20a}, and shapes~\citep{ShapeGF}. To enable new sampling methods and further extend the capabilities of score-based generative models, we propose a unified framework that generalizes previous approaches through the lens of stochastic differential equations (SDEs).



\begin{SCfigure}
    \centering
    \caption{{\bf Solving a reverse-time SDE yields a score-based generative model.} Transforming data to a simple noise distribution can be accomplished with a continuous-time SDE. This SDE can be reversed if we know the score of the distribution at each intermediate time step, $\nabla_\bfx \log p_t(\bfx)$. }
    \includegraphics[width=0.65\linewidth, trim=105 0 110 0, clip]{figures/diffusion_schematic.pdf}
    \label{fig:teaser}
\end{SCfigure}

Specifically, instead of perturbing data with a finite number of noise distributions, we consider a continuum of distributions that evolve over time according to a diffusion process. This process progressively diffuses a data point into random noise, and is given by a prescribed SDE that does not depend on the data and has no trainable parameters. 
By reversing this process, we can smoothly mold random noise into data for sample generation. Crucially, this reverse process satisfies a reverse-time SDE~\citep{Anderson1982-ny}, which can be derived from the forward SDE given the score of the marginal probability densities as a function of time. We can therefore approximate the reverse-time SDE by training a time-dependent neural network to estimate the scores, and then produce samples using numerical SDE solvers. Our key idea is summarized in \cref{fig:teaser}.

Our proposed framework has several theoretical and practical contributions: 

\textbf{Flexible sampling and likelihood computation:}~ We can employ any general-purpose SDE solver to integrate the reverse-time SDE for sampling. In addition, we propose two special methods not viable for general SDEs: (i) Predictor-Corrector (PC) samplers that combine numerical SDE solvers with score-based MCMC approaches, such as Langevin MCMC~\citep{parisi1981correlation} and HMC~\citep{neal2011mcmc}; and (ii) deterministic samplers based on the probability flow ordinary differential equation (ODE). The former \emph{unifies and improves} over existing sampling methods for score-based models. The latter allows for \emph{fast adaptive sampling} via black-box ODE solvers, \emph{flexible data manipulation} via latent codes, a \emph{uniquely identifiable encoding}, and notably, \emph{exact likelihood computation}.

\textbf{Controllable generation:}~ We can modulate the generation process by conditioning on information not available during training, because the conditional reverse-time SDE can be efficiently estimated from \emph{unconditional} scores. This enables applications such as class-conditional generation, image inpainting, colorization and other inverse problems, all achievable using a single unconditional score-based model without re-training.


\textbf{Unified framework:}~ Our framework provides a unified way to explore and tune various SDEs for improving score-based generative models.
The methods of SMLD and DDPM can be amalgamated into our framework as discretizations of two separate SDEs. 
Although DDPM~\citep{ho2020denoising} was recently reported to achieve higher sample quality than SMLD~\citep{song2019generative,song2020improved}, we show that with better architectures and new sampling algorithms allowed by our framework, the latter can catch up---it achieves new state-of-the-art Inception score (9.89) and FID score (2.20) on CIFAR-10, as well as high-fidelity generation of $1024\times 1024$ images for the first time from a score-based model. In addition, we propose a new SDE under our framework that achieves a likelihood value of 2.99 bits/dim on uniformly dequantized CIFAR-10 images, setting a new record on this task.

 \section{Background}
\subsection{Denoising score matching with Langevin dynamics (SMLD)}\label{sec:ncsn}
Let $p_{\sigma}(\tilde{\bfx} \mid \bfx) \coloneqq \mcal{N}(\tilde{\bfx}; \bfx, \sigma^2 \bfI)$ be a perturbation kernel, and $p_{\sigma}(\tilde{\bfx}) \coloneqq \int \pd(\bfx) p_{\sigma}(\tilde{\bfx} \mid \bfx) \ud \bfx$, where $\pd(\bfx)$ denotes the data distribution. Consider a sequence of positive noise scales $\sm = \sigma_1 < \sigma_2 < \cdots < \sigma_{N} = \sM$. Typically, $\sm$ is small enough such that $p_{\sm}(\bfx) \approx \pd(\bfx)$, and $\sM$ is large enough such that $p_{\sM}(\bfx) \approx \mcal{N}(\bfx; \bfzero, \sM^2 \bfI)$. \citet{song2019generative} propose to train a Noise Conditional Score Network (NCSN), denoted by $\bfs_\bftheta(\bfx, \sigma)$, with a weighted sum of denoising score matching~\citep{vincent2011connection} objectives:
\begin{align}
\bftheta^* &= \argmin_\bftheta
   \sum_{i=1}^{N} \sigma_i^2  \mbb{E}_{\pd(\bfx)}\mbb{E}_{p_{\sigma_i}(\tilde{\bfx} \mid \bfx)}\big[\norm{ \bfs_\bftheta(\tilde{\bfx}, \sigma_i) - \nabla_{\tilde{\bfx}} \log p_{\sigma_i}(\tilde{\bfx} \mid \bfx)}_2^2 \big]. \label{eqn:ncsn_obj}
\end{align}
Given sufficient data and model capacity, the optimal score-based model $\bfs_{\bftheta^*}(\bfx, \sigma)$ matches $\nabla_\bfx \log p_{\sigma}(\bfx)$ almost everywhere for $\sigma \in \{\sigma_i\}_{i=1}^{N}$. For sampling, \citet{song2019generative} run $M$ steps of Langevin MCMC to get a sample for each $p_{\sigma_i}(\bfx)$ sequentially:
\begin{align}
    \bfx_i^m = \bfx_i^{m-1} + \epsilon_i \bfs_{\bftheta^*}(\bfx_i^{m-1}, \sigma_{i}) + \sqrt{2\epsilon_i} \bfz_{i}^m,\quad m=1,2,\cdots, M,\label{eqn:langevin}
\end{align}
where $\epsilon_i > 0$ is the step size, and $\bfz_i^m$ is standard normal. The above is repeated for $i=N, N-1, \cdots, 1$ in turn with $\bfx_{N}^0 \sim \mcal{N}(\bfx \mid \bfzero, \sM^2 \bfI)$ and $\bfx_i^0 = \bfx_{i+1}^{M}$ when $i < N$. As $M \to \infty$ and $\epsilon_i \to 0$ for all $i$, $\bfx_1^M$ becomes an exact sample from $p_{\sm}(\bfx) \approx \pd(\bfx)$ under some regularity conditions.

\subsection{Denoising diffusion probabilistic models (DDPM)}\label{sec:ddpm}
\citet{sohl2015deep,ho2020denoising} consider a sequence of positive noise scales $0 < \beta_1, \beta_2, \cdots, \beta_N < 1$. For each training data point $\bfx_0 \sim \pd(\bfx)$, a discrete Markov chain $\{ \bfx_0, \bfx_1, \cdots, \bfx_N \}$ is constructed such that $p(\bfx_{i} \mid \bfx_{i-1}) = \mcal{N}(\bfx_{i} ; \sqrt{1-\beta_i} \bfx_{i-1}, \beta_i \bfI)$, and therefore $p_{\alpha_i}(\bfx_i \mid \bfx_0) = \mcal{N}(\bfx_i ;\sqrt{\alpha_i} \bfx_0, (1-\alpha_i)\bfI)$, where $\alpha_i \coloneqq \prod_{j=1}^i (1-\beta_j)$. Similar to SMLD, we can denote the perturbed data distribution as $p_{\alpha_i}(\tilde{\bfx}) \coloneqq \int p_\text{data}(\bfx) p_{\alpha_i}(\tilde{\bfx} \mid \bfx) \ud \bfx$. The noise scales are prescribed such that $\bfx_N$ is approximately distributed according to $\mcal{N}(\bfzero, \bfI)$. A variational Markov chain in the reverse direction is parameterized with $p_\bftheta(\bfx_{i-1} | \bfx_{i}) = \mcal{N}(\bfx_{i-1}; \frac{1}{\sqrt{1-\beta_i}} (\bfx_i + \beta_i \bfs_\bftheta(\bfx_i, i)), \beta_i \bfI)$, and trained with a re-weighted variant of the evidence lower bound (ELBO):
\begin{align}
\bftheta^* &= \argmin_\bftheta
    \sum_{i=1}^{N} (1-\alpha_i) \mbb{E}_{\pd(\bfx)}\mbb{E}_{p_{\alpha_i}(\tilde{\bfx} \mid \bfx)}[\norm{ \bfs_\bftheta(\tilde{\bfx}, i) - \nabla_{\tilde{\bfx}} \log p_{\alpha_i}(\tilde{\bfx} \mid \bfx)}_2^2]. \label{eqn:ddpm_obj}
\end{align}
After solving \cref{eqn:ddpm_obj} to get the optimal model $\bfs_{\bftheta^*}(\bfx, i)$, samples can be generated by starting from $\bfx_N \sim \mcal{N}(\bfzero, \bfI)$ and following the estimated reverse Markov chain as below
\begin{align}
    \bfx_{i-1} = \frac{1}{\sqrt{1-\beta_i}} (\bfx_i + \beta_i \bfs_{\bftheta^*}(\bfx_i, i)) + \sqrt{\beta_i}\bfz_i, \quad i=N, N-1, \cdots, 1. \label{eqn:ddpm_sampling}
\end{align}
We call this method \emph{ancestral sampling}, since it amounts to performing ancestral sampling from the graphical model $\prod_{i=1}^N p_\bftheta(\bfx_{i-1} \mid \bfx_i)$. The objective \cref{eqn:ddpm_obj} described here is $L_{\text{simple}}$ in \citet{ho2020denoising}, written in a form to expose more similarity to \cref{eqn:ncsn_obj}. 
Like \cref{eqn:ncsn_obj}, \cref{eqn:ddpm_obj} is also a weighted sum of denoising score matching objectives, which implies that the optimal model, $\bfs_{\bftheta^*}(\tilde{\bfx}, i)$, matches the score of the perturbed data distribution, $\nabla_\bfx \log p_{\alpha_i}(\bfx)$. Notably, the weights of the $i$-th summand in \cref{eqn:ncsn_obj} and \cref{eqn:ddpm_obj}, namely $\sigma_i^2$ and $(1-\alpha_i)$, are related to corresponding perturbation kernels in the same functional form: $\sigma_i^2 \propto 1/\mbb{E}[\norm{\nabla_\bfx \log p_{\sigma_i}(\tilde{\bfx} \mid \bfx)}_2^2]$ and $(1-\alpha_i) \propto 1/\mbb{E}[\norm{\nabla_\bfx \log p_{\alpha_i}(\tilde{\bfx} \mid \bfx)}_2^2]$. 

 \section{Score-based generative modeling with SDEs}
Perturbing data with multiple noise scales is key to the success of previous methods. We propose to generalize this idea further to an infinite number of noise scales, such that perturbed data distributions evolve according to an SDE as the noise intensifies. An overview of our framework is given in \cref{fig:toy}.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth, trim=16 0 12 14, clip]{figures/teaser.pdf}
    \caption{\textbf{Overview of score-based generative modeling through SDEs}. We can map data to a noise distribution (the prior) with an SDE (\cref{sec:sde_perturb}), and reverse this SDE for generative modeling (\cref{sec:sde_reverse}). We can also reverse the associated probability flow ODE (\cref{sec:flow}), which yields a deterministic process that samples from the same distribution as the SDE. Both the reverse-time SDE and probability flow ODE can be obtained by estimating the score $\nabla_\bfx \log p_t(\bfx)$ (\cref{sec:training}).}
    \label{fig:toy}
\end{figure}

\subsection{Perturbing data with SDEs}\label{sec:sde_perturb}
Our goal is to construct a diffusion process $\{\bfx(t)\}_{t=0}^T$ indexed by a continuous time variable $t\in [0,T]$, such that $\bfx(0) \sim p_{0}$, for which we have a dataset of \iid samples, and $\bfx(T) \sim p_T$, for which we have a tractable form to generate samples efficiently. In other words, $p_0$ is the data distribution and $p_T$ is the prior distribution. This diffusion process can be modeled as the solution to an It\^{o} SDE:
\begin{align}
    \ud \bfx = \bff(\bfx, t) \ud t + {g}(t) \ud \bfw, \label{eqn:forward_sde}
\end{align}
where $\bfw$ is the standard Wiener process (\aka, Brownian motion), $\bff(\cdot, t): \mbb{R}^d \to \mbb{R}^d$ is a vector-valued function called the \emph{drift} coefficient of $\bfx(t)$, and ${g}(\cdot): \mbb{R} \to \mbb{R}$ is a scalar function known as the \emph{diffusion} coefficient of $\bfx(t)$. For ease of presentation we assume the diffusion coefficient is a scalar (instead of a $d \times d$ matrix) and does not depend on $\bfx$, but our theory can be generalized to hold in those cases (see \cref{app:general_sde}). The SDE has a unique strong solution as long as the coefficients are globally Lipschitz in both state and time~\citep{oksendal2003stochastic}. We hereafter denote by $p_t(\bfx)$ the probability density of $\bfx(t)$, and use $p_{st}(\bfx(t) \mid \bfx(s))$ to denote the transition kernel from $\bfx(s)$ to $\bfx(t)$, where $0 \leq s < t \leq T$.

Typically, $p_T$ is an unstructured prior distribution that contains no information of $p_0$, such as a Gaussian distribution with fixed mean and variance. There are various ways of designing the SDE in \cref{eqn:forward_sde} such that it diffuses the data distribution into a fixed prior distribution. We provide several examples later in \cref{sec:sde_examples} that are derived from continuous generalizations of SMLD and DDPM.

\subsection{Generating samples by reversing the SDE}\label{sec:sde_reverse}
By starting from samples of $\bfx(T) \sim p_T$ and reversing the process, we can obtain samples $\bfx(0)\sim p_0$. A remarkable result from~\citet{Anderson1982-ny} states that the reverse of a diffusion process is also a diffusion process, running backwards in time and given by the reverse-time SDE:
\begin{align}
    \ud \bfx = [\bff(\bfx, t) - g(t)^2  \nabla_{\bfx}  \log p_t(\bfx)] \ud t + g(t) \ud \bar{\bfw},\label{eqn:backward_sde}
\end{align}
where $\bar{\bfw}$ is a standard Wiener process
when time flows backwards from $T$ to $0$, and $\ud t$ is an infinitesimal negative timestep. Once the score of each marginal distribution, $\nabla_\bfx \log p_t(\bfx)$, is known for all $t$, we can derive the reverse diffusion process from \cref{eqn:backward_sde} and simulate it to sample from $p_0$.


\subsection{Estimating scores for the SDE}\label{sec:training}
The score of a distribution can be estimated by training a score-based model on samples with score matching~\citep{hyvarinen2005estimation,song2019sliced}. To estimate $\nabla_\bfx \log p_t(\bfx)$, we can train a time-dependent score-based model $\bfs_\bftheta(\bfx, t)$ via a continuous generalization to \cref{eqn:ncsn_obj,eqn:ddpm_obj}:
\begin{align}
   \bftheta^* = \argmin_\bftheta 
   \mbb{E}_{t}\Big\{\lambda(t) \mbb{E}_{\bfx(0)}\mbb{E}_{\bfx(t) \mid \bfx(0) }
   \big[\norm{\bfs_\bftheta(\bfx(t), t) - \nabla_{\bfx(t)}\log p_{0t}(\bfx(t) \mid \bfx(0))}_2^2 \big]\Big\}. \label{eqn:training}
\end{align}
Here $\lambda: [0, T] \to \mbb{R}_{>0}$ is a positive weighting function, $t$ is uniformly sampled over $[0, T]$, $\bfx(0) \sim p_0(\bfx)$ and $\bfx(t) \sim p_{0t}(\bfx(t) \mid \bfx(0))$. With sufficient data and model capacity, score matching ensures that the optimal solution to \cref{eqn:training}, denoted by $\bfs_{\bftheta^\ast}(\bfx, t)$, equals $\nabla_\bfx \log p_t(\bfx)$ for almost all $\bfx$ and $t$. As in SMLD and DDPM, we can typically choose $\lambda \propto 1/{\mbb{E}\big[\norm{\nabla_{\bfx(t)}\log p_{0t}(\bfx(t) \mid \bfx(0))}_2^2\big]}$. Note that \cref{eqn:training} uses denoising score matching, but other score matching objectives, such as sliced score matching~\citep{song2019sliced} and finite-difference score matching~\citep{pang2020efficient} are also applicable here. 

We typically need to know the transition kernel $p_{0t}(\bfx(t) \mid \bfx(0))$ to efficiently solve \cref{eqn:training}. When $\bff(\cdot, t)$ is affine, the transition kernel is always a Gaussian distribution, where the mean and variance are often known in closed-forms and can be obtained with standard techniques (see Section 5.5 in \citet{sarkka2019applied}). For more general SDEs, we may solve Kolmogorov's forward equation~\citep{oksendal2003stochastic} to obtain $p_{0t}(\bfx(t) \mid \bfx(0))$. Alternatively, we can simulate the SDE to sample from $p_{0t}(\bfx(t) \mid \bfx(0))$ and replace denoising score matching in \cref{eqn:training} with sliced score matching for model training, which bypasses the computation of $\nabla_{\bfx(t)} \log p_{0t}(\bfx(t) \mid \bfx(0))$ (see \cref{app:general_sde}).




\subsection{Examples: VE, VP SDEs and beyond}\label{sec:sde_examples}
The noise perturbations used in SMLD and DDPM can be regarded as discretizations of two different SDEs. Below we provide a brief discussion and relegate more details to \cref{app:sde_derive}.

When using a total of $N$ noise scales, each perturbation kernel $p_{\sigma_i}(\bfx \mid \bfx_0)$ of SMLD corresponds to the distribution of $\bfx_i$ in the following Markov chain:
\begin{align}
\bfx_i = \bfx_{i-1} + \sqrt{\sigma_{i}^2 - \sigma_{i-1}^2} \bfz_{i-1}, \quad i=1,\cdots,N \label{eqn:ncsn},
\end{align}
where $\bfz_{i-1} \sim \mcal{N}(\bfzero, \bfI)$, and we have introduced $\sigma_0 = 0$ to simplify the notation.
In the limit of $N \to \infty$, $\{\sigma_i\}_{i=1}^N$ becomes a function $\sigma(t)$, $\bfz_i$ becomes $\bfz(t)$, and the Markov chain $\{\bfx_i\}_{i=1}^N$ becomes a continuous stochastic process $\{\bfx(t)\}_{t=0}^1$, where we have used a continuous time variable $t \in [0, 1]$ for indexing, rather than an integer $i$. The process $\{\bfx(t)\}_{t=0}^1$ is given by the following SDE
\begin{align}
\ud \bfx = \sqrt{\frac{\ud \left[ \sigma^2(t) \right]}{\ud t}}\ud \bfw. \label{eqn:ncsn_sde}
\end{align}
Likewise for the perturbation kernels $\{p_{\alpha_i}(\bfx \mid \bfx_0)\}_{i=1}^N$ of DDPM, the discrete Markov chain is
\begin{align}
    \bfx_i = \sqrt{1-\beta_{i}} \bfx_{i-1} + \sqrt{\beta_{i}} \bfz_{i-1}, \quad i=1,\cdots,N. \label{eqn:ddpm}
\end{align}
As $N\to \infty$, \cref{eqn:ddpm} converges to the following SDE,
\begin{align}
\ud \bfx = -\frac{1}{2}\beta(t) \bfx~ \ud t + \sqrt{\beta(t)} ~\ud \bfw. \label{eqn:ddpm_sde}
\end{align}

Therefore, the noise perturbations used in SMLD and DDPM correspond to discretizations of SDEs \cref{eqn:ncsn_sde,eqn:ddpm_sde}. Interestingly, the SDE of \cref{eqn:ncsn_sde} always gives a process with exploding variance when $t \to \infty$, whilst the SDE of \cref{eqn:ddpm_sde} yields a process with a fixed variance of one when the initial distribution has unit variance (proof in \cref{app:sde_derive}). Due to this difference, we hereafter refer to \cref{eqn:ncsn_sde} as the Variance Exploding (VE) SDE, and \cref{eqn:ddpm_sde} the Variance Preserving (VP) SDE. 

Inspired by the VP SDE, we propose a new type of SDEs which perform particularly well on likelihoods (see \cref{sec:flow}), given by
\begin{align}
    \ud \bfx = -\frac{1}{2}\beta(t) \bfx~ \ud t + \sqrt{\beta(t)(1 - e^{-2\int_0^t \beta(s)\ud s})} \ud \bfw.\label{eqn:subvp_sde}
\end{align}
When using the same $\beta(t)$ and starting from the same initial distribution, the variance of the stochastic process induced by \cref{eqn:subvp_sde} is always bounded by the VP SDE at every intermediate time step (proof in \cref{app:sde_derive}). For this reason, we name \cref{eqn:subvp_sde} the sub-VP SDE.

Since VE, VP and sub-VP SDEs all have affine drift coefficients, their perturbation kernels $p_{0t}(\bfx(t) \mid \bfx(0))$ are all Gaussian and can be computed in closed-forms, as discussed in \cref{sec:training}. This makes training with \cref{eqn:training} particularly efficient. 







 \section{Solving the reverse SDE}
After training a time-dependent score-based model $\bfs_\bftheta$, we can use it to construct the reverse-time SDE and then simulate it with numerical approaches to generate samples from $p_0$. 

\subsection{General-purpose numerical SDE solvers}

Numerical solvers provide approximate trajectories from SDEs. Many general-purpose numerical methods exist for solving SDEs, such as Euler-Maruyama and stochastic Runge-Kutta methods~\citep{kloeden2013numerical}, which correspond to different discretizations of the stochastic dynamics. We can apply any of them to the reverse-time SDE for sample generation. 

Ancestral sampling, the sampling method of DDPM (\cref{eqn:ddpm_sampling}), actually corresponds to one special discretization of the reverse-time VP SDE (\cref{eqn:ddpm_sde}) (see \cref{app:reverse_diffusion}). Deriving the ancestral sampling rules for new SDEs, however, can be non-trivial. To remedy this, we propose \emph{reverse diffusion samplers} (details in \cref{app:reverse_diffusion}), which discretize the reverse-time SDE in the same way as the forward one, and thus can be readily derived given the forward discretization.
As shown in \cref{tab:compare_samplers}, reverse diffusion samplers perform slightly better than ancestral sampling for both SMLD and DDPM models on CIFAR-10 (DDPM-type ancestral sampling is also applicable to SMLD models, see \cref{app:ancestral}.)


\subsection{Predictor-corrector samplers}
Unlike generic SDEs, we have additional information that can be used to improve solutions. Since we have a score-based model $\bfs_{\bftheta^*}(\bfx, t) \approx \nabla_\bfx \log p_t(\bfx)$, we can employ score-based MCMC approaches, such as Langevin MCMC~\citep{parisi1981correlation,grenander1994representations} or HMC~\citep{neal2011mcmc} to sample from $p_t$ directly, and correct the solution of a numerical SDE solver. 

Specifically, at each time step, the numerical SDE solver first gives an estimate of the sample at the next time step, playing the role of a ``predictor''. Then, the score-based MCMC approach corrects the 
marginal distribution of the estimated sample, playing the role of a ``corrector''. The idea is analogous to Predictor-Corrector methods, a family of numerical continuation techniques for solving systems of equations~\citep{allgower2012numerical}, and we similarly name our hybrid sampling algorithms \emph{Predictor-Corrector} (PC) samplers. Please find pseudo-code and a complete description in \cref{app:pc}. PC samplers generalize the original sampling methods of SMLD and DDPM: the former uses an identity function as the predictor and annealed Langevin dynamics as the corrector, while the latter uses ancestral sampling as the predictor and identity as the corrector. 

\begin{table}
    \definecolor{h}{gray}{0.9}
	\caption{Comparing different reverse-time SDE solvers on CIFAR-10. Shaded regions are obtained with the same computation (number of score function evaluations). Mean and standard deviation are reported over five sampling runs. ``P1000'' or ``P2000'': predictor-only samplers using 1000 or 2000 steps. ``C2000'': corrector-only samplers using 2000 steps. ``PC1000'': Predictor-Corrector (PC) samplers using 1000 predictor and 1000 corrector steps.}\label{tab:compare_samplers}
	\centering
	\begin{adjustbox}{max width=\linewidth}
		\begin{tabular}{c|c|c|c|c|c|c|c|c}
			\Xhline{3\arrayrulewidth} \bigstrut
			  & \multicolumn{4}{c|}{Variance Exploding SDE (SMLD)} & \multicolumn{4}{c}{Variance Preserving SDE (DDPM)}\\
			 \Xhline{1\arrayrulewidth}\bigstrut
			\diagbox[height=1cm, width=3cm]{Predictor}{FID$\downarrow$}{Sampler} & P1000 & \cellcolor{h}P2000 & \cellcolor{h}C2000 & \cellcolor{h}PC1000 & P1000 & \cellcolor{h}P2000 & \cellcolor{h}C2000 & \cellcolor{h}PC1000  \\
			\Xhline{1\arrayrulewidth}\bigstrut
            ancestral sampling & 4.98\scalebox{0.7}{ $\pm$ .06}	& \cellcolor{h}4.88\scalebox{0.7}{ $\pm$ .06} &\cellcolor{h} & \cellcolor{h}\textbf{3.62\scalebox{0.7}{ $\pm$ .03}} & 3.24\scalebox{0.7}{ $\pm$ .02}	& \cellcolor{h}3.24\scalebox{0.7}{ $\pm$ .02} &\cellcolor{h} & \cellcolor{h}\textbf{3.21\scalebox{0.7}{ $\pm$ .02}}\\
        	reverse diffusion & 4.79\scalebox{0.7}{ $\pm$ .07} & \cellcolor{h}4.74\scalebox{0.7}{ $\pm$ .08} & \cellcolor{h} & \cellcolor{h}\textbf{3.60\scalebox{0.7}{ $\pm$ .02}} & 3.21\scalebox{0.7}{ $\pm$ .02} & \cellcolor{h}3.19\scalebox{0.7}{ $\pm$ .02} & \cellcolor{h} &\cellcolor{h}\textbf{3.18\scalebox{0.7}{ $\pm$ .01}}\\
            probability flow &	15.41\scalebox{0.7}{ $\pm$ .15} &\cellcolor{h}10.54\scalebox{0.7}{ $\pm$ .08}&\cellcolor{h} \multirow{-3}{*}{20.43\scalebox{0.7}{ $\pm$ .07}} & \cellcolor{h}\textbf{3.51\scalebox{0.7}{ $\pm$ .04}} & 3.59\scalebox{0.7}{ $\pm$ .04} & \cellcolor{h}3.23\scalebox{0.7}{ $\pm$ .03} & \cellcolor{h}\multirow{-3}{*}{19.06\scalebox{0.7}{ $\pm$ .06}} & \cellcolor{h}\textbf{3.06\scalebox{0.7}{ $\pm$ .03}}\\
			\Xhline{3\arrayrulewidth}
		\end{tabular}
	\end{adjustbox}
\end{table}



We test PC samplers on SMLD and DDPM models (see \cref{alg:pc_smld,alg:pc_ddpm} in \cref{app:pc}) trained with original discrete objectives given by \cref{eqn:ncsn_obj,eqn:ddpm_obj}. This exhibits the compatibility of PC samplers to score-based models trained with a fixed number of noise scales. We summarize the performance of different samplers in \cref{tab:compare_samplers}, where probability flow is a predictor to be discussed in \cref{sec:flow}. Detailed experimental settings and additional results are given in \cref{app:pc}. We observe that our reverse diffusion sampler always outperform ancestral sampling, and corrector-only methods (C2000) perform worse than other competitors (P2000, PC1000) with the same computation (In fact, we need way more corrector steps per noise scale, and thus more computation, to match the performance of other samplers.) For all predictors, adding one corrector step for each predictor step (PC1000) doubles computation but always improves sample quality (against P1000). Moreover, it is typically better than doubling the number of predictor steps without adding a corrector (P2000), where we have to interpolate between noise scales in an ad hoc manner (detailed in \cref{app:pc}) for SMLD/DDPM models. In \cref{fig:computation} (\cref{app:pc}), we additionally provide qualitative comparison for models trained with the continuous objective \cref{eqn:training} on $256\times 256$ LSUN images and the VE SDE, where PC samplers clearly surpass predictor-only samplers under comparable computation, when using a proper number of corrector steps.








\subsection{Probability flow and connection to neural ODEs}\label{sec:flow}
Score-based models enable another numerical method for solving the reverse-time SDE. For all diffusion processes, there exists a corresponding \emph{deterministic process} whose trajectories share the same marginal probability densities $\{p_t(\bfx)\}_{t=0}^T$ as the SDE. This deterministic process satisfies an ODE~(more details in \cref{app:prob_flow_derive}):
\begin{align}
    \ud \bfx = \Big[\bff(\bfx, t) - \frac{1}{2} g(t)^2\nabla_\bfx \log p_t(\bfx)\Big] \ud t, \label{eqn:flow}
\end{align}
which can be determined from the SDE once scores are known. We name the ODE in \cref{eqn:flow} the \emph{probability flow ODE}.
When the score function is approximated by the time-dependent score-based model, which is typically a neural network, this is an example of a neural ODE~\citep{chen2018neural}. 



\textbf{Exact likelihood computation}~ Leveraging the connection to neural ODEs, we can compute the density defined by \cref{eqn:flow}  via the instantaneous change of variables formula \citep{chen2018neural}. This allows us to compute the \emph{exact likelihood on any input data} (details in \cref{app:prob_flow_likelihood}). As an example, we report negative log-likelihoods (NLLs) measured in bits/dim on the CIFAR-10 dataset in \cref{tab:bpd}. We compute log-likelihoods on uniformly dequantized data, and only compare to models evaluated in the same way (omitting models evaluated with variational dequantization~\citep{ho2019flow++} or discrete data), except for DDPM ($L$/$L_\text{simple}$) whose ELBO values (annotated with *) are reported on discrete data. Main results: (i) For the same DDPM model in \citet{ho2020denoising}, we obtain better bits/dim than ELBO, since our likelihoods are exact; (ii) Using the same architecture, we trained another DDPM model with the continuous objective in \cref{eqn:training} (\ie, DDPM cont.), which further improves the likelihood; (iii) With sub-VP SDEs, we always get higher likelihoods compared to VP SDEs; (iv) With improved architecture (\ie, DDPM++ cont., details in \cref{sec:arch}) and the sub-VP SDE, we can set a new record bits/dim of 2.99 on uniformly dequantized CIFAR-10 even \emph{without maximum likelihood training}.

\begin{table}
\begin{minipage}[t]{0.48\textwidth}
\vspace{-0.3cm}
\begin{table}[H]
\centering
\caption{NLLs and FIDs (ODE) on CIFAR-10. }\label{tab:bpd}\vspace{-1em}
\footnotesize
 \setlength\tabcolsep{3.5pt}
\begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{l c c}
    \toprule
    Model & NLL Test $\downarrow$ & FID $\downarrow$\\
    \midrule
    RealNVP~\citep{dinh2016density} & 3.49 & -\\
    iResNet~\citep{behrmann2019invertible} & 3.45 & -\\
    Glow~\citep{kingma2018glow} & 3.35 & - \\
    MintNet~\citep{song2019mintnet} & 3.32 & - \\
    Residual Flow~\citep{chen2019residual} & 3.28 & 46.37\\
    FFJORD~\citep{grathwohl2018ffjord} & 3.40 & -\\
    Flow++~\citep{ho2019flow++} & 3.29 & -\\
    DDPM ($L$)~\citep{ho2020denoising} & $\leq$ 3.70\textsuperscript{*} & 13.51\\
    DDPM ($L_{\text{simple}}$)~\citep{ho2020denoising} & $\leq$ 3.75\textsuperscript{*} & 3.17\\
    \midrule
    DDPM & 3.28 & 3.37\\\relax
    DDPM cont. (VP) & 3.21 & 3.69\\\relax
    DDPM cont. (sub-VP) & 3.05 & 3.56\\\relax
    DDPM++ cont. (VP) & 3.16 & 3.93\\\relax
    DDPM++ cont. (sub-VP) & 3.02 & 3.16\\ \relax
    DDPM++ cont. (deep, VP) & 3.13 & 3.08\\ \relax
    DDPM++ cont. (deep, sub-VP) & \textbf{2.99} & \textbf{2.92} \\
    \bottomrule
    \end{tabular}
\end{adjustbox}
\end{table}
\end{minipage}\hfil
\begin{minipage}[t]{0.48\textwidth}
\vspace{-0.3cm}
\begin{table}[H]
\centering
\caption{CIFAR-10 sample quality.}\label{tab:fid}\vspace{-1em}
\footnotesize
 \setlength\tabcolsep{3.5pt}
\begin{adjustbox}{max width=\textwidth}
  \begin{tabular}{lcc}
    \toprule
    Model & FID$\downarrow$ & IS$\uparrow$  \\
        \midrule
      {\bf Conditional} & & \\
      \midrule
      BigGAN~\citep{brock2018large} & 14.73 & 9.22 \\
      StyleGAN2-ADA~\citep{karras2020training} & \textbf{2.42} & \textbf{10.14} \\
      \midrule
      {\bf Unconditional} & & \\
      \midrule
      StyleGAN2-ADA~\citep{karras2020training} & 2.92 & 9.83 \\
      NCSN~\citep{song2019generative} & 25.32 & 8.87 $\pm$ .12 \\
      NCSNv2~\citep{song2020improved} & 10.87 & 8.40 $\pm$ .07\\
      DDPM~\citep{ho2020denoising} & 3.17 & 9.46 $\pm$ .11\\
      \midrule
      DDPM++ & 2.78 & 9.64\\
      DDPM++ cont. (VP) & 2.55 & 9.58 \\
      DDPM++ cont. (sub-VP) & 2.61 & 9.56 \\
      DDPM++ cont. (deep, VP) & 2.41 & 9.68\\
      DDPM++ cont. (deep, sub-VP) & 2.41 & 9.57\\
      NCSN++ & 2.45 & 9.73 \\
      NCSN++ cont. (VE) & 2.38 & 9.83\\
      NCSN++ cont. (deep, VE) & \textbf{2.20} & \textbf{9.89}\\
    \bottomrule 
    \end{tabular}
\end{adjustbox}
\end{table}
\end{minipage}
\end{table}

\textbf{Manipulating latent representations}~ By integrating \cref{eqn:flow}, we can encode any datapoint $\bfx(0)$ into a latent space $\bfx(T)$. Decoding can be achieved by integrating a corresponding ODE for the reverse-time SDE. As is done with other invertible models such as neural ODEs and normalizing flows~\citep{dinh2016density,kingma2018glow}, we can manipulate this latent representation for image editing, such as interpolation, and temperature scaling (see \cref{fig:prob_flow} and \cref{app:flow}). 

\textbf{Uniquely identifiable encoding}~ Unlike most current invertible models, 
our encoding is {\em uniquely identifiable}, meaning that with sufficient training data, model capacity, and optimization accuracy, the encoding for an input is uniquely determined by the data distribution \citep{roeder2020linear}. This is because our forward SDE, \cref{eqn:forward_sde}, has no trainable parameters, and its associated probability flow ODE, \cref{eqn:flow}, provides the same trajectories given perfectly estimated scores. We provide additional empirical verification on this property in \cref{app:identifiability}.

\textbf{Efficient sampling}~ As with neural ODEs, we can sample $\bfx(0) \sim p_0$ by solving \cref{eqn:flow} from different final conditions $\bfx(T)\sim p_T$. Using a fixed discretization strategy we can generate competitive samples, especially when used in conjuction with correctors (\cref{tab:compare_samplers},  ``probability flow sampler'', details in \cref{app:prob_flow_sampling}). Using a black-box ODE solver~\citep{dormand1980family} not only produces high quality samples (\cref{tab:bpd}, details in \cref{app:flow}), but also allows us to explicitly trade-off accuracy for efficiency. With a larger error tolerance, the number of function evaluations can be reduced by over $90\%$ without affecting the visual quality of samples (\cref{fig:prob_flow}).
\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/ode_tight}
    \vspace{-5mm}
    \caption{{\bf Probability flow ODE enables fast sampling} with adaptive stepsizes as the numerical precision is varied (\textit{left}), and reduces the number of score function evaluations (NFE) without harming quality (\textit{middle}). The invertible mapping from latents to images allows for interpolations (\textit{right}).}
    \label{fig:prob_flow}
\end{figure}



\subsection{Architecture improvements}\label{sec:arch}



We explore several new architecture designs for score-based models using both VE and VP SDEs (details in \cref{app:arch_search}), where we train models with the same discrete objectives as in SMLD/DDPM. We directly transfer the architectures for VP SDEs to sub-VP SDEs due to their similarity. Our optimal architecture for the VE SDE, named NCSN++, achieves an FID of 2.45 on CIFAR-10 with PC samplers, while our optimal architecture for the VP SDE, called DDPM++, achieves 2.78.

By switching to the continuous training objective in \cref{eqn:training}, and increasing the network depth, we can further improve sample quality for all models. The resulting architectures are denoted as NCSN++ cont. and DDPM++ cont. in \cref{tab:fid} for VE and VP/sub-VP SDEs respectively. Results reported in \cref{tab:fid} are for the checkpoint with the smallest FID over the course of training, where samples are generated with PC samplers. In contrast, FID scores and NLL values in \cref{tab:bpd} are reported for the last training checkpoint, and samples are obtained with black-box ODE solvers. As shown in \cref{tab:fid}, VE SDEs typically provide better sample quality than VP/sub-VP SDEs, but we also empirically observe that their likelihoods are worse than VP/sub-VP SDE counterparts. This indicates that practitioners likely need to experiment with different SDEs for varying domains and architectures.

Our best model for sample quality, NCSN++ cont. (deep, VE), doubles the network depth and sets new records for both inception score and FID on unconditional generation for CIFAR-10. Surprisingly, we can achieve better FID than the previous best conditional generative model without requiring labeled data. With all improvements together, we also obtain the first set of high-fidelity samples on CelebA-HQ $1024\times 1024$ from score-based models (see \cref{app:hq}). Our best model for likelihoods, DDPM++ cont. (deep, sub-VP), similarly doubles the network depth and achieves a log-likelihood of 2.99 bits/dim with the continuous objective in \cref{eqn:training}. To our best knowledge, this is the highest likelihood on uniformly dequantized CIFAR-10.

 \section{Controllable generation}
The continuous structure of our framework allows us to not only produce data samples from $p_0$, but also from $p_0(\bfx(0) \mid \bfy)$ if $p_t(\bfy \mid \bfx(t))$ is known. Given a forward SDE as in \cref{eqn:forward_sde}, we can sample from $p_t(\bfx(t) \mid \bfy)$ by starting from $p_T(\bfx(T) \mid \bfy)$ and solving a conditional reverse-time SDE:
\begin{align}
\ud \bfx = \{\bff(\bfx, t) - g(t)^2[ \nabla_\bfx  \log p_t(\bfx)+  \nabla_{\bfx}  \log p_t(\bfy \mid \bfx)]\} \ud t + g(t) \ud \bar{\bfw}.\label{eqn:cond_sde}
\end{align}
In general, we can use \cref{eqn:cond_sde} to solve a large family of \emph{inverse problems} with score-based generative models, once given an estimate of the gradient of the forward process, $\nabla_\bfx \log p_t(\bfy \mid \bfx(t) )$. In some cases, it is possible to train a separate model to learn the forward process $\log p_t(\bfy \mid \bfx(t))$ and compute its gradient. Otherwise, we may estimate the gradient with heuristics and domain knowledge. In \cref{app:inverse_prob}, we provide a broadly applicable method for obtaining such an estimate without the need of training auxiliary models.

We consider three applications of controllable generation with this approach: class-conditional generation, image imputation and colorization. When $\bfy$ represents class labels, we can train a time-dependent classifier $p_t(\bfy \mid \bfx(t))$ for class-conditional sampling. Since the forward SDE is tractable, we can easily create training data $(\bfx(t), \bfy)$ for the time-dependent classifier
by first sampling $(\bfx(0), \bfy)$ from a dataset, and then sampling $\bfx(t) \sim p_{0t}(\bfx(t) \mid \bfx(0))$. Afterwards, we may employ a mixture of cross-entropy losses over different time steps, like \cref{eqn:training}, to train the time-dependent classifier $p_t(\bfy \mid \bfx(t))$. We provide class-conditional CIFAR-10 samples in \cref{fig:cond_gen} (left), and relegate more details and results to \cref{app:cond_gen}.

Imputation is a special case of conditional sampling.
Suppose we have an incomplete data point $\bfy$ where only some subset, $\Omega(\bfy)$ is known. Imputation amounts to sampling from $p(\bfx(0) \mid \Omega(\bfy))$, which we can accomplish using an unconditional model (see \cref{app:imputation}). Colorization is a special case of imputation, except that the known data dimensions are coupled. We can decouple these data dimensions with an orthogonal linear transformation, and perform imputation in the transformed space (details in \cref{app:colorization}). \cref{fig:cond_gen} (right) shows results for inpainting and colorization achieved with unconditional time-dependent score-based models.


\begin{figure}
    \centering
    \includegraphics[width=0.327\linewidth, trim=5 0 7 0, clip, valign=c]{figures/cond_gen_line.png}
    \includegraphics[width=0.66\linewidth, valign=c]{figures/inpaint_colorization.jpg}
    \caption{\textit{Left}: Class-conditional samples on $32\times 32$ CIFAR-10. Top four rows are automobiles and bottom four rows are horses. \textit{Right:} Inpainting (top two rows) and colorization (bottom two rows) results on $256\times 256$ LSUN. First column is the original image, second column is the masked/gray-scale image, remaining columns are sampled image completions or colorizations.}
    \label{fig:cond_gen}
\end{figure}
 \section{Conclusion}
We presented a framework for score-based generative modeling based on SDEs. Our work enables a better understanding of existing approaches,  new sampling algorithms, exact likelihood computation, uniquely identifiable encoding, latent code manipulation, and brings new conditional generation abilities to the family of score-based generative models.

While our proposed sampling approaches improve results and enable more efficient sampling, they remain slower at sampling than GANs~\citep{goodfellow2014generative} on the same datasets. Identifying ways of combining the stable learning of score-based generative models with the fast sampling of implicit models like GANs remains an important research direction. Additionally, the breadth of samplers one can use when given access to score functions introduces a number of hyper-parameters. Future work would benefit from improved methods to automatically select and tune these hyper-parameters, as well as more extensive investigation on the merits and limitations of various samplers.

\subsubsection*{Acknowledgements}
We would like to thank Nanxin Chen, Ruiqi Gao, Jonathan Ho, Kevin Murphy, Tim Salimans and Han Zhang for their insightful discussions during the course of this project. This research was partially supported by NSF (\#1651565, \#1522054, \#1733686),
ONR (N00014\-19\-1\-2145), AFOSR (FA9550\-19\-1\-0024), and TensorFlow Research Cloud. Yang Song was partially supported by the Apple PhD Fellowship in AI/ML. 
\bibliography{ncsn3}
\bibliographystyle{iclr2021_conference}
\appendix
\newpage
\section*{Appendix}
We include several appendices with additional details, derivations, and results.
Our framework allows general SDEs with matrix-valued diffusion coefficients that depend on the state, for which we provide a detailed discussion in \cref{app:general_sde}. We give a full derivation of VE, VP and sub-VP SDEs in \cref{app:sde_derive}, and discuss how to use them from a practitioner's perspective in \cref{app:wild_sde}. We elaborate on the probability flow formulation of our framework in \cref{app:prob_flow_ode}, including a derivation of the probability flow ODE (\cref{app:prob_flow_derive}), exact likelihood computation (\cref{app:prob_flow_likelihood}), probability flow sampling with a fixed discretization strategy (\cref{app:prob_flow_sampling}), sampling with black-box ODE solvers (\cref{app:flow}), and experimental verification on uniquely identifiable encoding (\cref{app:identifiability}). We give a full description of the reverse diffusion sampler in \cref{app:reverse_diffusion}, the DDPM-type ancestral sampler for SMLD models in \cref{app:ancestral}, and Predictor-Corrector samplers in \cref{app:pc}. We explain our model architectures and detailed experimental settings in \cref{app:arch_search}, with $1024\times 1024$ CelebA-HQ samples therein. Finally, we detail on the algorithms for controllable generation in \cref{app:cond_gen}, and include extended results for class-conditional generation (\cref{app:class_cond_sampling}), image inpainting (\cref{app:imputation}), colorization (\cref{app:colorization}), and a strategy for solving general inverse problems (\cref{app:inverse_prob}).

\section{The framework for more general SDEs}\label{app:general_sde}
In the main text, we introduced our framework based on a simplified SDE~\cref{eqn:forward_sde} where the diffusion coefficient is independent of $\bfx(t)$. It turns out that our framework can be extended to hold for more general diffusion coefficients. We can consider SDEs in the following form:
\begin{align}
    \ud \bfx = \bff(\bfx, t) \ud t + \mbf{G}(\bfx, t) \ud \bfw, \label{eqn:forward_sde_2}
\end{align}
where $\bff(\cdot, t): \mbb{R}^d \to \mbb{R}^d$ and $\mbf{G}(\cdot, t): \mbb{R}^d \to \mbb{R}^{d\times d}$. We follow the It\^{o} interpretation of SDEs throughout this paper.

According to \citep{Anderson1982-ny}, the reverse-time SDE is given by (\cf, \cref{eqn:backward_sde})
\begin{align}
    \ud \bfx = \{\bff(\bfx, t) - \nabla \cdot [\bfG(\bfx, t) \bfG(\bfx,t)\tran] - \bfG(\bfx,t)\bfG(\bfx,t)\tran  \nabla_\bfx  \log p_t(\bfx)\} \ud t + \mbf{G}(\bfx, t) \ud \bar{\bfw},\label{eqn:backward_sde_2}
\end{align}
where we define $\nabla\cdot \mbf{F}(\bfx) := (\nabla \cdot \bff^1(\bfx), \nabla\cdot \bff^2(\bfx), \cdots, \nabla\cdot \bff^d(\bfx))\tran$ for a matrix-valued function $\mbf{F}(\bfx) := (\bff^1(\bfx), \bff^2(\bfx), \cdots, \bff^d(\bfx))\tran$ throughout the paper. 

The probability flow ODE corresponding to \cref{eqn:forward_sde_2} has the following form (\cf, \cref{eqn:flow}, see a detailed derivation in \cref{app:prob_flow_derive}):
\begin{align}
    \ud \bfx = \bigg\{\bff(\bfx, t) - \frac{1}{2} \nabla \cdot [\bfG(\bfx, t)\bfG(\bfx, t)\tran] - \frac{1}{2} \bfG(\bfx, t)\bfG(\bfx, t)\tran \nabla_\bfx \log p_t(\bfx)\bigg\} \ud t. \label{eqn:flow_2}
\end{align}
Finally for conditional generation with the general SDE \cref{eqn:forward_sde_2}, we can solve the conditional reverse-time SDE below (\cf, \cref{eqn:cond_sde}, details in \cref{app:cond_gen}):
\begin{multline}
\ud \bfx = \{\bff(\bfx, t) - \nabla \cdot [\bfG(\bfx, t) \bfG(\bfx,t)\tran] - \bfG(\bfx,t)\bfG(\bfx,t)\tran  \nabla_\bfx  \log p_t(\bfx)\\ - \bfG(\bfx,t)\bfG(\bfx,t)\tran  \nabla_{\bfx}  \log p_t(\bfy \mid \bfx)\} \ud t + \mbf{G}(\bfx, t) \ud \bar{\bfw}.\label{eqn:cond_sde_2}
\end{multline}

When the drift and diffusion coefficient of an SDE are not affine, it can be difficult to compute the transition kernel $p_{0t}(\bfx(t) \mid \bfx(0))$ in closed form. This hinders the training of score-based models, because \cref{eqn:training} requires knowing $\nabla_{\bfx(t)}\log p_{0t}(\bfx(t) \mid \bfx(0))$. To overcome this difficulty, we can replace denoising score matching in \cref{eqn:training} with other efficient variants of score matching that do not require computing $\nabla_{\bfx(t)}\log p_{0t}(\bfx(t) \mid \bfx(0))$. For example, when using sliced score matching~\citep{song2019sliced}, our training objective \cref{eqn:training} becomes
\begin{align}
   \bftheta^* = \argmin_\bftheta 
   \mbb{E}_{t}\bigg\{\lambda(t) \mbb{E}_{\bfx(0)}\mbb{E}_{\bfx(t) }\mbb{E}_{\bfv \sim p_\bfv}
   \bigg[\frac{1}{2}\norm{\bfs_\bftheta(\bfx(t), t)}_2^2 + \bfv\tran \bfs_\bftheta(\bfx(t), t) \bfv \bigg]\bigg\}, \label{eqn:training_ssm}
\end{align}
where $\lambda: [0,T] \to \mbb{R}^+$ is a positive weighting function, $t \sim \mcal{U}(0, T)$, $\mbb{E}[\bfv] = \bfzero$, and $\operatorname{Cov}[\bfv] = \bfI$. We can always simulate the SDE to sample from $p_{0t}(\bfx(t) \mid \bfx(0))$, and solve \cref{eqn:training_ssm} to train the time-dependent score-based model $\bfs_\bftheta(\bfx, t)$.




\section{VE, VP and sub-VP SDEs}\label{app:sde_derive}
Below we provide detailed derivations to show that the noise perturbations of SMLD and DDPM are discretizations of the Variance Exploding (VE) and Variance Preserving (VP) SDEs respectively. We additionally introduce sub-VP SDEs, a modification to VP SDEs that often achieves better performance in both sample quality and likelihoods.

First, when using a total of $N$ noise scales, each perturbation kernel $p_{\sigma_i}(\bfx \mid \bfx_0)$ of SMLD can be derived from the following Markov chain:
\begin{align}
\bfx_i = \bfx_{i-1} + \sqrt{\sigma_{i}^2 - \sigma_{i-1}^2} \bfz_{i-1}, \quad i=1,\cdots,N \label{eqn:ncsn},
\end{align}
where $\bfz_{i-1} \sim \mcal{N}(\bfzero, \bfI)$, $\bfx_0 \sim p_\text{data}$, and we have introduced $\sigma_0 = 0$ to simplify the notation.
In the limit of $N \to \infty$, the Markov chain $\{\bfx_i\}_{i=1}^N$ becomes a continuous stochastic process $\{\bfx(t)\}_{t=0}^1$, $\{\sigma_i\}_{i=1}^N$ becomes a function $\sigma(t)$, and $\bfz_i$ becomes $\bfz(t)$, where we have used a continuous time variable $t \in [0, 1]$ for indexing, rather than an integer $i \in \{1, 2, \cdots, N\}$. 
Let $\bfx\left(\frac{i}{N}\right) = \bfx_i$, $\sigma\left(\frac{i}{N}\right) = \sigma_i$, and $\bfz\left(\frac{i}{N}\right) = \bfz_i$ for $i=1,2,\cdots, N$. We can rewrite \cref{eqn:ncsn} as follows with $\Delta t = \frac{1}{N}$ and $t \in \big\{0, \frac{1}{N}, \cdots, \frac{N-1}{N}\big\}$:
\begin{align*}
    \bfx(t + \Delta t) = \bfx(t) + \sqrt{\sigma^2(t + \Delta t)- \sigma^2(t)}~\bfz(t) \approx \bfx(t) + \sqrt{\frac{\ud \left[ \sigma^2(t) \right] }{\ud t}\Delta t} ~\bfz(t),
\end{align*}
where the approximate equality holds when $\Delta t \ll 1$. 
In the limit of $\Delta t \rightarrow 0$, this converges to
\begin{align}
\ud \bfx = \sqrt{\frac{\ud \left[ \sigma^2(t) \right]}{\ud t}}\ud \bfw, \label{eqn:ncsn_sde_2}
\end{align}
which is the VE SDE. 

For the perturbation kernels $\{p_{\alpha_i}(\bfx \mid \bfx_0)\}_{i=1}^N$ used in DDPM, the discrete Markov chain is
\begin{align}
    \bfx_i = \sqrt{1-\beta_{i}} \bfx_{i-1} + \sqrt{\beta_{i}} \bfz_{i-1}, \quad i=1,\cdots,N, \label{eqn:ddpm_markov}
\end{align}
where $\bfz_{i-1} \sim \mcal{N}(\bfzero, \bfI)$. To obtain the limit of this Markov chain when $N\to\infty$, we define an auxiliary set of noise scales $\{\bbeta_i = N \beta_i\}_{i=1}^N$, and re-write \cref{eqn:ddpm_markov} as below
\begin{align}
    \bfx_i = \sqrt{1-\frac{\bbeta_{i}}{N}} \bfx_{i-1} + \sqrt{\frac{\bbeta_{i}}{N}} \bfz_{i-1}, \quad i=1,\cdots,N. \label{eqn:ddpm_markov2}
\end{align}
In the limit of $N\to \infty$, $\{\bbeta_i\}_{i=1}^N$ becomes a function $\beta(t)$ indexed by $t \in [0, 1]$. Let $\beta\left(\frac{i}{N}\right) = \bbeta_i$, $\bfx(\frac{i}{N}) = \bfx_i$, $\bfz(\frac{i}{N}) = \bfz_i$. We can rewrite the Markov chain \cref{eqn:ddpm_markov2} as the following with $\Delta t = \frac{1}{N}$ and $t \in \{0, 1, \cdots, \frac{N-1}{N}\}$:
\begin{align}
\bfx(t + \Delta t) &= \sqrt{1 - \beta(t + \Delta t)\Delta t} ~\bfx(t) + \sqrt{\beta(t + \Delta t)\Delta t}~ \bfz(t) \notag\\
&\approx \bfx(t) -\frac{1}{2}\beta(t + \Delta t)\Delta t~\bfx(t) + \sqrt{\beta(t+\Delta t)\Delta t}~\bfz(t) \notag\\
&\approx \bfx(t) -\frac{1}{2}\beta(t)\Delta t~\bfx(t) + \sqrt{\beta(t)\Delta t}~ \bfz(t), \label{eqn:ddpm_discrete}
\end{align}
where the approximate equality holds when $\Delta t \ll 1$.
Therefore, in the limit of $\Delta t \to 0$, \cref{eqn:ddpm_discrete} converges to the following VP SDE:
\begin{align}
\ud \bfx = -\frac{1}{2}\beta(t) \bfx~ \ud t + \sqrt{\beta(t)} ~\ud \bfw.
\end{align}

So far, we have demonstrated that the noise perturbations used in SMLD and DDPM correspond to discretizations of VE and VP SDEs respectively. The VE SDE always yields a process with exploding variance when $t \to \infty$. In contrast, the VP SDE yields a process with bounded variance. In addition, the process has a constant unit variance for all $t \in [0, \infty)$ when $p(\bfx(0))$ has a unit variance. Since the VP SDE has affine drift and diffusion coefficients, we can use Eq.~(5.51) in \citet{sarkka2019applied} to obtain an ODE that governs the evolution of variance
\begin{align*}
    \frac{\ud \bfsigma_\text{VP}(t)}{\ud t} = \beta(t) (\bfI - \bfsigma_\text{VP}(t)),
\end{align*}
where $\bfsigma_\text{VP}(t) \coloneqq \operatorname{Cov}[\bfx(t)]$ for $\{\bfx(t)\}_{t=0}^1$ obeying a VP SDE. Solving this ODE, we obtain
\begin{align}
    \bfsigma_\text{VP}(t) = \bfI + e^{\int_0^t - \beta(s) \ud s}(\bfsigma_\text{VP}(0) - \bfI),
\end{align}
from which it is clear that the variance $\bfsigma_\text{VP}(t)$ is always bounded given $\bfsigma_\text{VP}(0)$. Moreover, $\bfsigma_\text{VP}(t) \equiv \bfI$ if $\bfsigma_\text{VP}(0) = \bfI$. Due to this difference, we name \cref{eqn:ncsn_sde} as the \emph{Variance Exploding (VE) SDE}, and \cref{eqn:ddpm_sde} the \emph{Variance Preserving (VP) SDE}.

Inspired by the VP SDE, we propose a new SDE called the \emph{sub-VP SDE}, namely
\begin{align}
    \ud \bfx = -\frac{1}{2}\beta(t) \bfx~ \ud t + \sqrt{\beta(t)(1 - e^{-2\int_0^t \beta(s)\ud s})} \ud \bfw.\label{eqn:subVP}
\end{align}
Following standard derivations, it is straightforward to show that $\mbb{E}[\bfx(t)]$ is the same for both VP and sub-VP SDEs; the variance function of sub-VP SDEs is different, given by
\begin{align}
    \bfsigma_\text{sub-VP}(t) = \bfI + e^{-2 \int_{0}^t \beta(s)\ud s} \bfI + e^{-\int_0^t \beta(s) \ud s}(\bfsigma_\text{sub-VP}(0) - 2\bfI),
\end{align}
where $\bfsigma_\text{sub-VP}(t) \coloneqq \operatorname{Cov}[\bfx(t)]$ for a process $\{\bfx(t)\}_{t=0}^1$ obtained by solving \cref{eqn:subVP}. In addition, we observe that (i) $\bfsigma_{\text{sub-VP}}(t) \preccurlyeq \bfsigma_{\text{VP}}(t)$ for all $t \geq 0$ with $\bfsigma_\text{sub-VP}(0) = \bfsigma_\text{VP}(0)$ and shared $\beta(s)$; and (ii) $\lim_{t\to \infty} \bfsigma_{\text{sub-VP}}(t) = \lim_{t \to \infty}\bfsigma_{\text{VP}}(t) = \bfI$ if $\lim_{t\to \infty}\int_0^t \beta(s) \ud s = \infty$. The former is why we name \cref{eqn:subVP} the sub-VP SDE---its variance is always upper bounded by the corresponding VP SDE. The latter justifies the use of sub-VP SDEs for score-based generative modeling, since they can perturb any data distribution to standard Gaussian under suitable conditions, just like VP SDEs.


VE, VP and sub-VP SDEs all have affine drift coefficients. Therefore, their perturbation kernels $p_{0t}(\bfx(t) \mid \bfx(0))$ are all Gaussian and can be computed with Eqs.~(5.50) and (5.51) in \citet{sarkka2019applied}:
\begin{align}
    p_{0t}(\bfx(t) \mid \bfx(0)) = \begin{cases} 
    \mcal{N}\big(\bfx(t) ; \bfx(0), [\sigma^2(t) - \sigma^2(0)] \bfI\big), &~~~ \text{(VE SDE)}\\
    \mcal{N}\big(\bfx(t) ; \bfx(0)e^{-\frac{1}{2}\int_0^t \beta(s) \ud s}, \bfI - \bfI e^{-\int_0^t \beta(s) \ud s}\big) &~~~ \text{(VP SDE)}\\
    \mcal{N}\big(\bfx(t) ; \bfx(0)e^{-\frac{1}{2}\int_0^t \beta(s) \ud s}, [1 - e^{-\int_0^t \beta(s) \ud s}]^2 \bfI\big) &~~~ \text{(sub-VP SDE)}
    \end{cases}.\label{eqn:perturb_kernel}
\end{align}
As a result, all SDEs introduced here can be efficiently trained with the objective in \cref{eqn:training}.

\section{SDEs in the wild} \label{app:wild_sde}
Below we discuss concrete instantiations of VE and VP SDEs whose discretizations yield SMLD and DDPM models, and the specific sub-VP SDE used in our experiments. In SMLD, the noise scales $\{\sigma_i\}_{i=1}^N$ is typically a geometric sequence where $\sigma_\text{min}$ is fixed to $0.01$ and $\sigma_\text{max}$ is chosen according to Technique 1 in \citet{song2020improved}. Usually, SMLD models normalize image inputs to the range $[0, 1]$. Since $\{\sigma_i\}_{i=1}^N$ is a geometric sequence, we have $\sigma(\frac{i}{N}) = \sigma_i = \sm \left(\frac{\sM}{\sm}\right)^{\frac{i-1}{N-1}}$ for $i=1,2,\cdots, N$. In the limit of $N \to \infty$, we have $\sigma(t) = \sm \left(\frac{\sM}{\sm}\right)^{t}$ for $t \in (0, 1]$. The corresponding VE SDE is
\begin{align}
\ud \bfx = \sm \bigg( \frac{\sM}{\sm} \bigg)^t \sqrt{2 \log \frac{\sM}{\sm}}\ud \bfw, \quad t \in (0, 1],
\end{align}
and the perturbation kernel can be derived via \cref{eqn:perturb_kernel}:
\begin{align}
    p_{0t}(\bfx(t) \mid \bfx(0)) = \mcal{N}\left(\bfx(t) ; \bfx(0), \sm^2 \Big( \frac{\sM}{\sm}\Big)^{2t} \bfI\right), \quad t \in (0, 1].
\end{align}
There is one subtlety when $t=0$: by definition, $\sigma(0) = \sigma_0 = 0$ (following the convention in \cref{eqn:ncsn}), but $\sigma(0^+) \coloneqq \lim_{t \to 0^+} \sigma(t) = \sigma_\text{min} \neq 0$. In other words, $\sigma(t)$ for SMLD is not differentiable since $\sigma(0) \neq \sigma(0^+)$, causing the VE SDE in \cref{eqn:ncsn_sde_2} undefined for $t=0$. In practice, we bypass this issue by always solving the SDE and its associated probability flow ODE in the range $t \in [\epsilon, 1]$ for some small constant $\epsilon > 0$, and we use $\epsilon = 10^{-5}$ in our VE SDE experiments. 

For DDPM models, $\{\beta_i\}_{i=1}^N$ is typically an arithmetic sequence where $\beta_i = \frac{\bbetam}{N} + \frac{i-1}{N(N-1)}(\bbetaM -\bbetam)$ for $i=1,2,\cdots, N$. Therefore, $\beta(t) = \bbetam + t(\bbetaM - \bbetam)$ for $t\in[0,1]$ in the limit of $N \to \infty$. This corresponds to the following instantiation of the VP SDE:
\begin{gather}
\ud \bfx = -\frac{1}{2}(\bbetam + t(\bbetaM - \bbetam)) \bfx \ud t + \sqrt{\bbetam + t(\bbetaM - \bbetam)} \ud \bfw, \quad t\in[0,1],
\end{gather}
where $\bfx(0) \sim p_\text{data}(\bfx)$. In our experiments, we let $\bbetam = 0.1$ and $\bbetaM = 20$ to match the settings in \citet{ho2020denoising}. The perturbation kernel is given by
\begin{multline}
    p_{0t}(\bfx(t) \mid \bfx(0)) \\
    = \mcal{N}\left(\bfx(t) ; e^{-\frac{1}{4}t^2(\bbetaM-\bbetam) - \frac{1}{2}t\bbetam}\bfx(0), \bfI - \bfI e^{-\frac{1}{2}t^2(\bbetaM -\bbetam) - t\bbetam} \right), \quad t\in[0,1].
\end{multline}
For DDPM, there is no discontinuity issue with the corresponding VP SDE; yet, there are numerical instability issues for training and sampling at $t=0$, due to the vanishing variance of $\bfx(t)$ as $t \to 0$. Therefore, same as the VE SDE, we restrict computation to $t\in[\epsilon, 1]$ for a small $\epsilon > 0$. For sampling, we choose $\epsilon=10^{-3}$ so that the variance of $\bfx(\epsilon)$ in VP SDE matches the variance of $\bfx_1$ in DDPM; for training and likelihood computation, we adopt $\epsilon=10^{-5}$ which empirically gives better results.

\begin{figure}
    \centering
    \begin{subfigure}{0.333\linewidth}
        \includegraphics[width=\linewidth]{figures/NCSN_var.pdf}\caption{SMLD}
    \end{subfigure}
    \begin{subfigure}{0.32\linewidth}
        \includegraphics[width=\linewidth]{figures/ddpm_mean.pdf}\caption{DDPM (mean)}
    \end{subfigure}
    \begin{subfigure}{0.32\linewidth}
        \includegraphics[width=\linewidth]{figures/ddpm_var.pdf}
        \caption{DDPM (variance)}
    \end{subfigure}
    \caption{Discrete-time perturbation kernels and our continuous generalizations match each other almost exactly. (a) compares the variance of perturbation kernels for SMLD and VE SDE; (b) compares the scaling factors of means of perturbation kernels for DDPM and VP SDE; and (c) compares the variance of perturbation kernels for DDPM and VP SDE.}
    \label{fig:discretization}
\end{figure}
As a sanity check for our SDE generalizations to SMLD and DDPM, we compare the perturbation kernels of SDEs and original discrete Markov chains in \cref{fig:discretization}. The SMLD and DDPM models both use $N=1000$ noise scales. For SMLD, we only need to compare the variances of perturbation kernels since means are the same by definition. For DDPM, we compare the scaling factors of means and the variances. As demonstrated in \cref{fig:discretization}, the discrete perturbation kernels of original SMLD and DDPM models align well with perturbation kernels derived from VE and VP SDEs.

For sub-VP SDEs, we use exactly the same $\beta(t)$ as VP SDEs. This leads to the following perturbation kernel
\begin{multline}
    p_{0t}(\bfx(t) \mid \bfx(0)) \\
    = \mcal{N}\left(\bfx(t) ; e^{-\frac{1}{4}t^2(\bbetaM-\bbetam) - \frac{1}{2}t\bbetam}\bfx(0), [1 - e^{-\frac{1}{2}t^2(\bbetaM -\bbetam) - t\bbetam}]^2 \bfI \right), \quad t\in[0,1].
\end{multline}
We also restrict numerical computation to the same interval of $[\epsilon, 1]$ as VP SDEs.

Empirically, we observe that smaller $\epsilon$ generally yields better likelihood values for all SDEs. For sampling, it is important to use an appropriate $\epsilon$ for better Inception scores and FIDs, although samples across different $\epsilon$ look visually the same to human eyes.

\section{Probability flow ODE}\label{app:prob_flow_ode}
\subsection{Derivation}\label{app:prob_flow_derive}
The idea of probability flow ODE is inspired by \citet{maoutsa2020interacting}, and one can find the derivation of a simplified case therein. Below we provide a derivation for the fully general ODE in \cref{eqn:flow_2}. We consider the SDE in \cref{eqn:forward_sde_2}, which possesses the following form:
\begin{align*}
    \ud \bfx = \bff(\bfx, t) \ud t + \mbf{G}(\bfx, t) \ud \bfw,
\end{align*}
where $\bff(\cdot, t): \mbb{R}^d \to \mbb{R}^d$ and $\mbf{G}(\cdot, t): \mbb{R}^d \to \mbb{R}^{d\times d}$. The marginal probability density $p_t(\mbf{x}(t))$ evolves according to Kolmogorov's forward equation (Fokker-Planck equation)~\citep{oksendal2003stochastic}
\begin{align}
    \frac{\partial p_t(\bfx)}{\partial t} = -\sum_{i=1}^d \frac{\partial}{\partial x_i}[f_i(\bfx, t) p_t(\bfx)] + \frac{1}{2}\sum_{i=1}^d\sum_{j=1}^d \frac{\partial^2}{\partial x_i \partial x_j}\Big[\sum_{k=1}^d G_{ik}(\bfx, t) G_{jk}(\bfx, t) p_t(\bfx)\Big] \label{eqn:fokker_planck}.
\end{align}
We can easily rewrite \cref{eqn:fokker_planck} to obtain
\begin{align}
    \frac{\partial p_t(\bfx)}{\partial t} &= -\sum_{i=1}^d \frac{\partial}{\partial x_i}[f_i(\bfx, t) p_t(\bfx)] + \frac{1}{2}\sum_{i=1}^d\sum_{j=1}^d \frac{\partial^2}{\partial x_i \partial x_j}\Big[\sum_{k=1}^d G_{ik}(\bfx, t) G_{jk}(\bfx, t) p_t(\bfx)\Big]\notag \\
    &= -\sum_{i=1}^d \frac{\partial}{\partial x_i}[f_i(\bfx, t) p_t(\bfx)] + \frac{1}{2}\sum_{i=1}^d \frac{\partial}{\partial x_i} \Big[ \sum_{j=1}^d \frac{\partial}{\partial x_j}\Big[\sum_{k=1}^d G_{ik}(\bfx, t) G_{jk}(\bfx, t) p_t(\bfx)\Big]\Big]. \label{eqn:ode_part_1}
\end{align}
Note that
\begin{align*}
    &\sum_{j=1}^d \frac{\partial}{\partial x_j}\Big[\sum_{k=1}^d G_{ik}(\bfx, t) G_{jk}(\bfx, t) p_t(\bfx)\Big]\\
    =& \sum_{j=1}^d \frac{\partial}{\partial x_j} \Big[ \sum_{k=1}^d G_{ik}(\bfx, t) G_{jk}(\bfx, t) \Big] p_t(\bfx) + \sum_{j=1}^d \sum_{k=1}^d G_{ik}(\bfx, t)G_{jk}(\bfx, t) p_t(\bfx) \frac{\partial}{\partial x_j} \log p_t(\bfx)\\
    =& p_t(\bfx) \nabla \cdot [\bfG(\bfx, t)\bfG(\bfx, t)\tran] + p_t(\bfx) \bfG(\bfx, t)\bfG(\bfx, t)\tran \nabla_\bfx \log p_t(\bfx),
\end{align*}
based on which we can continue the rewriting of \cref{eqn:ode_part_1} to obtain
\begin{align}
    \frac{\partial p_t(\bfx)}{\partial t} &= -\sum_{i=1}^d \frac{\partial}{\partial x_i}[f_i(\bfx, t) p_t(\bfx)] + \frac{1}{2}\sum_{i=1}^d \frac{\partial}{\partial x_i} \Big[ \sum_{j=1}^d \frac{\partial}{\partial x_j}\Big[\sum_{k=1}^d G_{ik}(\bfx, t) G_{jk}(\bfx, t) p_t(\bfx)\Big]\Big]\notag \\
     &= -\sum_{i=1}^d \frac{\partial}{\partial x_i}[f_i(\bfx, t) p_t(\bfx)] \notag \\
     &\quad + \frac{1}{2}\sum_{i=1}^d \frac{\partial}{\partial x_i} \Big[ p_t(\bfx) \nabla \cdot [\bfG(\bfx, t)\bfG(\bfx, t)\tran] + p_t(\bfx) \bfG(\bfx, t)\bfG(\bfx, t)\tran \nabla_\bfx \log p_t(\bfx) \Big]\notag \\
     &= -\sum_{i=1}^d \frac{\partial}{\partial x_i}\Big\{ f_i(\bfx, t)p_t(\bfx) \notag \\
     &\quad - \frac{1}{2} \Big[ \nabla \cdot [\bfG(\bfx, t)\bfG(\bfx, t)\tran] +  \bfG(\bfx, t)\bfG(\bfx, t)\tran \nabla_\bfx \log p_t(\bfx) \Big] p_t(\bfx) \Big\}\notag \\
     &= -\sum_{i=1}^d \frac{\partial}{\partial x_i} [\tilde{f}_i(\bfx, t)p_t(\bfx)],\label{eqn:ode_part_2}
\end{align}
where we define
\begin{align*}
    \tilde{\bff}(\bfx, t) \coloneqq \bff(\bfx, t) - \frac{1}{2} \nabla \cdot [\bfG(\bfx, t)\bfG(\bfx, t)\tran] - \frac{1}{2} \bfG(\bfx, t)\bfG(\bfx, t)\tran \nabla_\bfx \log p_t(\bfx).
\end{align*}
Inspecting \cref{eqn:ode_part_2}, we observe that it equals Kolmogorov's forward equation of the following SDE with $\tilde{\bfG}(\bfx, t) \coloneqq \bfzero$ (Kolmogorov's forward equation in this case is also known as the Liouville equation.)
\begin{align*}
    \ud \bfx = \tilde{\bff}(\bfx, t) \ud t + \tilde{\bfG}(\bfx, t) \ud \bfw,
\end{align*}
which is essentially an ODE:
\begin{align*}
    \ud \bfx = \tilde{\bff}(\bfx, t) \ud t,
\end{align*}
same as the probability flow ODE given by \cref{eqn:flow_2}. Therefore, we have shown that the probability flow ODE \cref{eqn:flow_2} induces the same marginal probability density $p_t(\bfx)$ as the SDE in \cref{eqn:forward_sde_2}.

\subsection{Likelihood computation}\label{app:prob_flow_likelihood}
The probability flow ODE in \cref{eqn:flow_2} has the following form when we replace the score $\nabla_\bfx \log p_t(\bfx)$ with the time-dependent score-based model $\bfs_\bftheta(\bfx, t)$:
\begin{align}
      \ud \bfx = \underbrace{\bigg\{\bff(\bfx, t) - \frac{1}{2} \nabla \cdot [\bfG(\bfx, t)\bfG(\bfx, t)\tran] - \frac{1}{2} \bfG(\bfx, t)\bfG(\bfx, t)\tran \bfs_\bftheta(\bfx, t)\bigg\}}_{=: \tilde{\bff}_\bftheta(\bfx, t)} \ud t. \label{eqn:flow_3}
\end{align}
With the instantaneous change of variables formula~\citep{chen2018neural}, we can compute the log-likelihood of $p_0(\bfx)$ using
\begin{align}
    \log p_0(\bfx(0)) = \log p_T(\bfx(T)) + \int_0^T \nabla \cdot \tilde{\bff}_\bftheta(\bfx(t), t) \ud t, \label{eqn:likelihood}
\end{align}
where the random variable $\bfx(t)$ as a function of $t$ can be obtained by solving the probability flow ODE in \cref{eqn:flow_3}. In many cases computing $\nabla \cdot \tilde{\bff}_\bftheta(\bfx, t)$ is expensive, so we follow \citet{grathwohl2018ffjord} to estimate it with the Skilling-Hutchinson trace estimator~\citep{skilling1989eigenvalues,hutchinson1990stochastic}. In particular, we have
\begin{align}
    \nabla \cdot \tilde{\bff}_\bftheta(\bfx, t) = \mbb{E}_{p(\bfe)}[\bfe\tran \nabla \tilde{\bff}_\bftheta(\bfx, t)\bfe], \label{eqn:hutchinson}
\end{align}
where $\nabla \tilde{\bff}_\bftheta$ denotes the Jacobian of $\tilde{\bff}_\bftheta(\cdot, t)$, and the random variable $\bfe$ satisfies $\mbb{E}_{p(\bfe)}[\bfe] = \bfzero$ and $\operatorname{Cov}_{p(\bfe)}[\bfe] = \bfI$. The vector-Jacobian product $\bfe\tran \nabla\tilde{\bff}_\bftheta(\bfx, t)$ can be efficiently computed using reverse-mode automatic differentiation, at approximately the same cost as evaluating $\tilde{\bff}_\bftheta(\bfx, t)$. As a result, we can sample $\bfe \sim p(\bfe)$ and then compute an efficient unbiased estimate to $\nabla \cdot \tilde{\bff}_\bftheta(\bfx, t)$ using $\bfe\tran \nabla \tilde{\bff}_\bftheta(\bfx, t) \bfe$. Since this estimator is unbiased, we can attain an arbitrarily small error by averaging over a sufficient number of runs. Therefore, by applying the Skilling-Hutchinson estimator \cref{eqn:hutchinson} to \cref{eqn:likelihood}, we can compute the log-likelihood to any accuracy.

In our experiments, we use the RK45 ODE solver~\citep{dormand1980family} provided by \verb|scipy.integrate.solve_ivp| in all cases. The bits/dim values in \cref{tab:bpd} are computed with \verb|atol=1e-5| and \verb|rtol=1e-5|, same as \citet{grathwohl2018ffjord}. To give the likelihood results of our models in \cref{tab:bpd}, we average the bits/dim obtained on the test dataset over five different runs with $\epsilon=10^{-5}$ (see definition of $\epsilon$ in \cref{app:wild_sde}).

\subsection{Probability flow sampling}\label{app:prob_flow_sampling}
Suppose we have a forward SDE
\begin{align*}
    \ud \bfx = \bff(\bfx, t) \ud t + \bfG(t) \ud \bfw,
\end{align*}
and one of its discretization
\begin{align}
    \bfx_{i+1} = \bfx_i + \bff_i(\bfx_i) + \bfG_i \bfz_i, \quad i = 0, 1, \cdots, N-1, \label{eqn:discrete_forward_2}
\end{align}
where $\bfz_i \sim \mcal{N}(\bfzero, \bfI)$. We assume the discretization schedule of time is fixed beforehand, and thus we absorb the dependency on $\Delta t$ into the notations of $\bff_i$ and $\bfG_i$. Using \cref{eqn:flow_2}, we can obtain the following probability flow ODE:
\begin{align}
    \ud \bfx = \left\{ \bff(\bfx, t) - \frac{1}{2}\bfG(t)\bfG(t)\tran \nabla_\bfx \log p_t(\bfx) \right\} \ud t.
\end{align}
We may employ any numerical method to integrate the probability flow ODE backwards in time for sample generation. In particular, we propose a discretization in a similar functional form to \cref{eqn:discrete_forward_2}:
\begin{align*}
    \bfx_i = \bfx_{i+1} - \bff_{i+1}(\bfx_{i+1}) + \frac{1}{2}\bfG_{i+1}\bfG_{i+1}\tran \bfs_{\bftheta^*}(\bfx_{i+1}, i+1),\quad  i =0,1,\cdots, N-1,
\end{align*}
where the score-based model $\bfs_{\bftheta^*}(\bfx_i, i)$ is conditioned on the iteration number $i$. This is a deterministic iteration rule. Unlike reverse diffusion samplers or ancestral sampling, there is no additional randomness once the initial sample $\bfx_N$ is obtained from the prior distribution. When applied to SMLD models, we can get the following iteration rule for probability flow sampling:
\begin{align}
    \bfx_i = \bfx_{i+1} + \frac{1}{2} (\sigma_{i+1}^2 - \sigma_i^2) \bfs_{{\bftheta^*}}(\bfx_{i+1}, \sigma_{i+1}), \quad i=0,1,\cdots, N-1.
\end{align}
Similarly, for DDPM models, we have
\begin{align}
    \bfx_i = (2 - \sqrt{1 -\beta_{i+1}})\bfx_{i+1} + \frac{1}{2} \beta_{i+1} \bfs_{\bftheta^*}(\bfx_{i+1}, i+1), \quad i=0,1,\cdots, N-1.
\end{align}

\subsection{Sampling with black-box ODE solvers}\label{app:flow}
For producing figures in \cref{fig:prob_flow}, we use a DDPM model trained on $256\times 256$ CelebA-HQ with the same settings in \citet{ho2020denoising}. All FID scores of our models in \cref{tab:bpd} are computed on samples from the RK45 ODE solver implemented in  \verb|scipy.integrate.solve_ivp| with \verb|atol=1e-5| and \verb|rtol=1e-5|. We use $\epsilon = 10^{-5}$ for VE SDEs and $\epsilon = 10^{-3}$ for VP SDEs (see also \cref{app:wild_sde}). 

Aside from the interpolation results in \cref{fig:prob_flow}, we demonstrate more examples of latent space manipulation in \cref{fig:celeba256}, including interpolation and temperature scaling. The model tested here is a DDPM model trained with the same settings in \citet{ho2020denoising}.
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/celeba_interp.png}
    \includegraphics[width=\linewidth]{figures/celeba_temp.png}
    \caption{Samples from the probability flow ODE for VP SDE on $256\times 256$ CelebA-HQ. Top: spherical interpolations between random samples. Bottom: temperature rescaling (reducing norm of embedding).}
    \label{fig:celeba256}
\end{figure}

Although solvers for the probability flow ODE allow fast sampling, their samples typically have higher (worse) FID scores than those from SDE solvers if no corrector is used. We have this empirical observation for both the discretization strategy in \cref{app:prob_flow_sampling}, and black-box ODE solvers introduced above. Moreover, the performance of probability flow ODE samplers depends on the choice of the SDE---their sample quality for VE SDEs is much worse than VP SDEs especially for high-dimensional data.

\subsection{Uniquely identifiable encoding}\label{app:identifiability}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/identifiability.pdf}
    \caption{Comparing the first 100 dimensions of the latent code obtained for a random CIFAR-10 image. ``Model A'' and ``Model B'' are separately trained with different architectures.}
    \label{fig:identifiability}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{figures/identifiability_boxplot.pdf}
    \includegraphics[width=0.41\linewidth]{figures/identifiability_cor.pdf}
    \caption{\emph{Left}: The dimension-wise difference between encodings obtained by Model A and B. As a baseline, we also report the difference between shuffled representations of these two models.
    \emph{Right}: The dimension-wise correlation coefficients of encodings obtained by Model A and Model B.}
    \label{fig:identifiability_boxplot}
\end{figure}

As a sanity check, we train two models (denoted as ``Model A'' and ``Model B'') with different architectures using the VE SDE on CIFAR-10. Here Model A is an NCSN++ model with 4 layers per resolution trained using the continuous objective in \cref{eqn:training}, and Model B is all the same except that it uses 8 layers per resolution. Model definitions are in \cref{app:arch_search}. 

We report the latent codes obtained by Model A and Model B for a random CIFAR-10 image in \cref{fig:identifiability}. In \cref{fig:identifiability_boxplot}, we show the dimension-wise differences and correlation coefficients between latent encodings on a total of 16 CIFAR-10 images. Our results demonstrate that for the same inputs, Model A and Model B provide encodings that are close in every dimension, despite having different model architectures and training runs.


\section{Reverse diffusion sampling}\label{app:reverse_diffusion}
Given a forward SDE 
\begin{align*}
    \ud \bfx = \bff(\bfx, t) \ud t + \bfG(t) \ud \bfw,
\end{align*}
and suppose the following iteration rule is a discretization of it:
\begin{align}
    \bfx_{i+1} = \bfx_i + \bff_i(\bfx_i) + \bfG_i \bfz_i, \quad i = 0, 1, \cdots, N-1 \label{eqn:discrete_forward}
\end{align}
where $\bfz_i \sim \mcal{N}(\bfzero, \bfI)$. Here we assume the discretization schedule of time is fixed beforehand, and thus we can absorb it into the notations of $\bff_i$ and $\bfG_i$.

Based on \cref{eqn:discrete_forward}, we propose to discretize the reverse-time SDE
\begin{align*}
\ud \bfx = [\bff(\bfx, t) - \bfG(t)\bfG(t)\tran  \nabla_\bfx  \log p_t(\bfx)] \ud t + \mbf{G}(t) \ud \bar{\bfw},
\end{align*}
with a similar functional form, which gives the following iteration rule for $i \in \{0, 1, \cdots, N-1\}$:
\begin{align}
    \bfx_{i} = \bfx_{i+1} -\bff_{i+1}(\bfx_{i+1}) + \bfG_{i+1}\bfG_{i+1}\tran \bfs_{\bftheta^*}(\bfx_{i+1}, i+1) + \bfG_{i+1}\bfz_{i+1}, \label{eqn:discrete_backward}
\end{align}
where our trained score-based model $\bfs_{\bftheta^*}(\bfx_i, i)$ is conditioned on iteration number $i$.

When applying \cref{eqn:discrete_backward} to \cref{eqn:ncsn,eqn:ddpm}, we obtain a new set of numerical solvers for the reverse-time VE and VP SDEs, resulting in sampling algorithms as shown in the ``predictor'' part of \cref{alg:pc_smld,alg:pc_ddpm}. We name these sampling methods (that are based on the discretization strategy in \cref{eqn:discrete_backward}) \emph{reverse diffusion samplers}. 

As expected, the ancestral sampling of DDPM~\citep{ho2020denoising} (\cref{eqn:ddpm_sampling}) matches its reverse diffusion counterpart when $\beta_i \to 0$ for all $i$ (which happens when $\Delta t \to 0$ since $\beta_i = \bbeta_i \Delta t$, see \cref{app:sde_derive}), because
\begin{align*}
    \bfx_i = &\frac{1}{\sqrt{1-\beta_{i+1}}}(\bfx_{i+1} + \beta_{i+1} \bfs_{\bftheta^*}(\bfx_{i+1}, i+1)) + \sqrt{\beta_{i+1}} \bfz_{i+1}\\
    =&  \bigg( 1 + \frac{1}{2}\beta_{i+1} + o(\beta_{i+1}) \bigg)(\bfx_{i+1} + \beta_{i+1} \bfs_{\bftheta^*}(\bfx_{i+1}, i+1)) + \sqrt{\beta_{i+1}} \bfz_{i+1}\\
    \approx & \bigg( 1 + \frac{1}{2}\beta_{i+1} \bigg)(\bfx_{i+1} + \beta_{i+1} \bfs_{\bftheta^*}(\bfx_{i+1}, i+1)) + \sqrt{\beta_{i+1}} \bfz_{i+1}\\
    =&\bigg(1 + \frac{1}{2}\beta_{i+1} \bigg) \bfx_{i+1} + \beta_{i+1}\bfs_{\bftheta^*}(\bfx_{i+1}, i+1) + \frac{1}{2}\beta_{i+1}^2 \bfs_{\bftheta^*}(\bfx_{i+1}, i+1) + \sqrt{\beta_{i+1}} \bfz_{i+1}\\
    \approx& \bigg(1 + \frac{1}{2}\beta_{i+1} \bigg) \bfx_{i+1} + \beta_{i+1}\bfs_{\bftheta^*}(\bfx_{i+1}, i+1) + \sqrt{\beta_{i+1}} \bfz_{i+1}\\
    =& \bigg[2 - \bigg(1 - \frac{1}{2}\beta_{i+1}\bigg) \bigg] \bfx_{i+1} + \beta_{i+1}\bfs_{\bftheta^*}(\bfx_{i+1}, i+1) + \sqrt{\beta_{i+1}} \bfz_{i+1}\\
    \approx& \bigg[2 - \bigg(1 - \frac{1}{2}\beta_{i+1}\bigg) + o(\beta_{i+1}) \bigg] \bfx_{i+1} + \beta_{i+1}\bfs_{\bftheta^*}(\bfx_{i+1}, i+1) + \sqrt{\beta_{i+1}} \bfz_{i+1}\\
    =& (2 - \sqrt{1-\beta_{i+1}})\bfx_{i+1} + \beta_{i+1}\bfs_{\bftheta^*}(\bfx_{i+1}, i+1)+ \sqrt{\beta_{i+1}} \bfz_{i+1}.
\end{align*}
Therefore, the original ancestral sampler of \cref{eqn:ddpm_sampling} is essentially a different discretization to the same reverse-time SDE. This unifies the sampling method in \citet{ho2020denoising} as a numerical solver to the reverse-time VP SDE in our continuous framework. 


\section{Ancestral sampling for SMLD models}\label{app:ancestral}
The ancestral sampling method for DDPM models can also be adapted to SMLD models. Consider a sequence of noise scales $\sigma_1 < \sigma_2 < \cdots < \sigma_N$ as in SMLD. By perturbing a data point $\bfx_0$ with these noise scales sequentially, we obtain a Markov chain $\bfx_0 \to \bfx_1 \to \cdots \to \bfx_N$, where
\begin{align*}
    p(\bfx_i \mid \bfx_{i-1}) = \mcal{N}(\bfx_i; \bfx_{i-1}, (\sigma_i^2 - \sigma_{i-1}^2)\bfI), \quad i=1,2,\cdots,N.
\end{align*}
Here we assume $\sigma_0 = 0$ to simplify notations. Following \citet{ho2020denoising}, we can compute
\begin{align*}
    q(\bfx_{i-1} \mid \bfx_i, \bfx_0) = \mcal{N}\left(\bfx_{i-1}; \frac{\sigma_{i-1}^2}{\sigma_{i}^2}\bfx_i + \Big( 1 - \frac{\sigma_{i-1}^2}{\sigma_i^2} \Big)\bfx_0, \frac{\sigma_{i-1}^2 (\sigma_i^2 - \sigma_{i-1}^2)}{\sigma_i^2} \bfI  \right).
\end{align*}
If we parameterize the reverse transition kernel as $p_\bftheta(\bfx_{i-1} \mid \bfx_i) = \mcal{N}(\bfx_{i-1}; \bfmu_\bftheta(\bfx_i, i), \tau^2_i \bfI)$, then
\begin{align*}
    L_{t-1} &= \mbb{E}_q[D_{\text{KL}}(q(\bfx_{i-1} \mid \bfx_i, \bfx_0))~\|~p_\bftheta(\bfx_{i-1} \mid \bfx_i)]\\
    &= \mbb{E}_q\left[\frac{1}{2\tau^2_i} \norm{\frac{\sigma_{i-1}^2}{\sigma_{i}^2}\bfx_i + \Big( 1 - \frac{\sigma_{i-1}^2}{\sigma_i^2} \Big)\bfx_0 - \bfmu_\bftheta(\bfx_i, i)}_2^2 \right] + C\\
    &=\mbb{E}_{\bfx_0, \bfz}\left[\frac{1}{2\tau^2_i} \norm{\bfx_i(\bfx_0, \bfz) - \frac{\sigma_i^2 - \sigma_{i-1}^2}{\sigma_i} \bfz - \bfmu_\bftheta(\bfx_i(\bfx_0, \bfz), i)}_2^2 \right] + C,
\end{align*}
where $L_{t-1}$ is one representative term in the ELBO objective (see Eq.~(8) in \citet{ho2020denoising}), $C$ is a constant that does not depend on $\bftheta$, $\bfz \sim \mcal{N}(\bfzero, \bfI)$, and $\bfx_i(\bfx_0, \bfz) = \bfx_0 + \sigma_i \bfz$. We can therefore parameterize $\bfmu_\bftheta(\bfx_i, i)$ via
\begin{align*}
    \bfmu_\bftheta(\bfx_i, i) = \bfx_i + (\sigma_i^2 - \sigma_{i-1}^2) \bfs_\bftheta(\bfx_i, i),
\end{align*}
where $\bfs_\bftheta(\bfx_i, i)$ is to estimate $\bfz / \sigma_i$. As in \citet{ho2020denoising}, we let $\tau_i = \sqrt{\frac{\sigma_{i-1}^2 (\sigma_i^2 - \sigma_{i-1}^2)}{\sigma_i^2}}$. Through ancestral sampling on $\prod_{i=1}^N p_\bftheta(\bfx_{i-1} \mid \bfx_i)$, we obtain the following iteration rule
\begin{align}
    \bfx_{i-1} = \bfx_i + (\sigma_i^2 - \sigma_{i-1}^2) \bfs_{\bftheta^*}(\bfx_i, i) + \sqrt{\frac{\sigma_{i-1}^2 (\sigma_i^2 - \sigma_{i-1}^2)}{\sigma_i^2}} \bfz_i, i=1,2,\cdots, N, \label{eqn:ncsn_ancestral}
\end{align}
where $\bfx_N \sim \mcal{N}(\bfzero, \sigma_N^2 \bfI)$, $\bftheta^*$ denotes the optimal parameter of $\bfs_\bftheta$, and $\bfz_i \sim \mcal{N}(\bfzero, \bfI)$. We call \cref{eqn:ncsn_ancestral} the ancestral sampling method for SMLD models.


\section{Predictor-Corrector samplers}\label{app:pc}

\begin{algorithm}[!t]
   \caption{Predictor-Corrector (PC) sampling}
   \label{alg:pc}
    \begin{algorithmic}[1]
        \Require    
            \Statex $N$: Number of discretization steps for the reverse-time SDE
            \Statex $M$: Number of corrector steps
       \State{Initialize $\bfx_N \sim p_T(\bfx)$}
       \For{$i=N-1$ {\bfseries to} $0$}
        \State $\bfx_i \gets \operatorname{Predictor}(\bfx_{i+1})$
         \For{$j=1$ {\bfseries to} $M$}
            \State $\bfx_i \gets \operatorname{Corrector}(\bfx_i)$
         \EndFor
       \EndFor
       \State {\bfseries return} $\bfx_0$
    \end{algorithmic}
    \end{algorithm}

\paragraph{Predictor-Corrector (PC) sampling} 
The predictor can be any numerical solver for the reverse-time SDE with a fixed discretization strategy. The corrector can be any score-based MCMC approach. In PC sampling, we alternate between the predictor and corrector, as described in \cref{alg:pc}. For example, when using the reverse diffusion SDE solver (\cref{app:reverse_diffusion}) as the predictor, and annealed Langevin dynamics~\citep{song2019generative} as the corrector, we have \cref{alg:pc_smld,alg:pc_ddpm} for VE and VP SDEs respectively, where $\{\epsilon_i\}_{i=0}^{N-1}$ are step sizes for Langevin dynamics as specified below.

\algnewcommand{\LineComment}[1]{\Statex \(\triangleright\) #1}
\algrenewcommand\algorithmicindent{0.7em}\begin{figure}
\begin{minipage}{.49\textwidth}
    \vspace{-0.5cm}
    \begin{algorithm}[H]
            \small
           \caption{PC sampling (VE SDE)}
           \label{alg:pc_smld}
            \begin{algorithmic}[1]
               \State $\bfx_N \sim \mcal{N}(\bfzero, \sM^2 \bfI)$
               \For{$i=N-1$ {\bfseries to} $0$}
                \begin{tikzpicture}[remember picture,overlay]
                    \node[xshift=3.5cm,yshift=-1.32cm] at (0,0){\includegraphics[width=2.05\textwidth]{figures/pc_boxes2.png}};
                \end{tikzpicture}
                 \State{$\bfx_i' \gets \bfx_{i+1} + (\sigma_{i+1}^2 - \sigma_i^2) \bfs_{\bftheta^*}(\bfx_{i+1}, \sigma_{i+1})$}
                 \State{$\bfz \sim \mcal{N}(\bfzero, \bfI)$}
                 \State{$\bfx_i \gets \bfx_i' + \sqrt{\sigma_{i+1}^2 - \sigma_i^2} \bfz$}
                 \For{$j=1$ {\bfseries to} $M$}
                    \State{$\bfz \sim \mcal{N}(\bfzero, \bfI)$}
                    \State{$\bfx_{i} \gets \bfx_{i} + \epsilon_i \bfs_{\bftheta^*}(\bfx_{i}, \sigma_{i}) + \sqrt{2\epsilon_i}\bfz$}
                 \EndFor
               \EndFor
               \State {\bfseries return} $\bfx_0$
            \end{algorithmic}
    \end{algorithm}
\end{minipage}
\begin{minipage}{.49\textwidth}
    \vspace{-0.5cm}
    \begin{algorithm}[H]
          \small
           \caption{PC sampling (VP SDE)}
           \label{alg:pc_ddpm}
            \begin{algorithmic}[1]
               \State $\bfx_N \sim \mcal{N}(\bfzero, \bfI)$
               \For{$i=N-1$ {\bfseries to} $0$}
               \vspace{0.35em}
                 \State{\resizebox{\linewidth}{!}{$\bfx_i' \gets (2 - \sqrt{1-\beta_{i+1}})\bfx_{i+1} + \beta_{i+1} \bfs_{\bftheta^*}(\bfx_{i+1}, i+1)$}}\vspace{0.37em}
                 \State{$\bfz \sim \mcal{N}(\bfzero, \bfI)$}\vspace{0.17em}
                 \State{$\bfx_i \gets \bfx_i' + \sqrt{\beta_{i+1}} \bfz$}
                 \For{$j=1$ {\bfseries to} $M$}
                    \State{$\bfz \sim \mcal{N}(\bfzero, \bfI)$}
                    \State{$\bfx_{i} \gets \bfx_{i} + \epsilon_i \bfs_{\bftheta^*}(\bfx_{i}, i) + \sqrt{2\epsilon_i}\bfz$}
                 \EndFor
               \EndFor
               \State {\bfseries return} $\bfx_0$
            \end{algorithmic}
    \end{algorithm}
\end{minipage}
\end{figure}

\paragraph{The corrector algorithms} We take the schedule of annealed Langevin dynamics in \citet{song2019generative}, but re-frame it with slight modifications in order to get better interpretability and empirical performance. We provide the corrector algorithms in \cref{alg:corrector_ve,alg:corrector_vp} respectively, where we call $r$ the ``signal-to-noise'' ratio. We determine the step size $\epsilon$ using the norm of the Gaussian noise $\norm{\bfz}_2$, norm of the score-based model $\norm{\bfs_{\bftheta^*}}_2$ and the signal-to-noise ratio $r$. When sampling a large batch of samples together, we replace the norm $\norm{\cdot}_2$ with the average norm across the mini-batch. When the batch size is small, we suggest replacing $\norm{\bfz}_2$ with $\sqrt{d}$, where $d$ is the dimensionality of $\bfz$. 



\begin{minipage}[t]{0.49\linewidth}
\begin{algorithm}[H]
	\caption{Corrector algorithm (VE SDE).}
	\label{alg:corrector_ve}
	\begin{algorithmic}[1]
	    \Require{$\{\sigma_i\}_{i=1}^N, r, N, M$.}
	    \State{$\bfx_N^0 \sim \mcal{N}(\bfzero, \sM^2 \bfI)$}
	    \For{$i \gets N$ to $1$}
            \For{$j \gets 1$ to $M$}
                \State{$\bfz \sim \mcal{N}(\bfzero, \bfI)$}
                \State{$\bfg \gets \bfs_{\bftheta^*}(\bfx_i^{j-1},\sigma_i)$}
                \State{$\epsilon \gets 2 (r \norm{\bfz}_2 / \norm{\bfg}_2)^2$}
                \State{$\bfx_i^{j} \gets \bfx_i^{j-1} + \epsilon~\bfg  + \sqrt{2 \epsilon}~ \bfz$}
            \EndFor
            \State{$\bfx_{i-1}^0 \gets \bfx_{i}^M$}
        \EndFor
        \item[]
        \Return{$\bfx_{0}^0$}
	\end{algorithmic}
\end{algorithm}
\end{minipage}
\begin{minipage}[t]{0.49\linewidth}
\begin{algorithm}[H]
	\caption{Corrector algorithm (VP SDE).}
	\label{alg:corrector_vp}
	\begin{algorithmic}[1]
	    \Require{$\{\beta_i\}_{i=1}^N, \{\alpha_i\}_{i=1}^N, r, N, M$.}
	    \State{$\bfx_N^0 \sim \mcal{N}(\bfzero, \bfI)$}
	    \For{$i \gets N$ to $1$}
            \For{$j \gets 1$ to $M$}
                \State{$\bfz \sim \mcal{N}(\bfzero, \bfI)$}
                \State{$\bfg \gets \bfs_{\bftheta^*}(\bfx_i^{j-1}, i)$}
                \State{$\epsilon \gets 2 \alpha_i (r \norm{\bfz}_2 / \norm{\bfg}_2)^2$}
                \State{$\bfx_i^{j} \gets \bfx_i^{j-1} + \epsilon~\bfg  + \sqrt{2 \epsilon}~ \bfz$}
            \EndFor
            \State{$\bfx_{i-1}^0 \gets \bfx_{i}^M$}
        \EndFor
        \item[]
        \Return{$\bfx_{0}^0$}
	\end{algorithmic}
\end{algorithm}
\end{minipage}


\paragraph{Denoising} For both SMLD and DDPM models, the generated samples typically contain small noise that is hard to detect by humans. As noted by \citet{jolicoeur2020adversarial}, FIDs can be significantly worse without removing this noise. This unfortunate sensitivity to noise is also part of the reason why NCSN models trained with SMLD has been performing worse than DDPM models in terms of FID, because the former does not use a denoising step at the end of sampling, while the latter does. In all experiments of this paper we ensure there is a single denoising step at the end of sampling, using Tweedie's formula~\citep{efron2011tweedie}.


\begin{figure}[h]
    \centering
    \includegraphics[width=0.45\linewidth]{figures/computation_bedroom_compressed.pdf}
    \includegraphics[width=0.451\linewidth]{figures/computation_church_compressed.pdf}
    \caption{PC sampling for LSUN bedroom and church. The vertical axis corresponds to the total computation, and the horizontal axis represents the amount of computation allocated to the corrector. Samples are the best when computation is split between the predictor and corrector.}
    \label{fig:computation}
\end{figure}




\paragraph{Training} 
We use the same architecture in \cite{ho2020denoising} for our score-based models. For the VE SDE, we train a model with the original SMLD objective in \cref{eqn:ncsn_obj}; similarly for the VP SDE, we use the original DDPM objective in \cref{eqn:ddpm_obj}. We apply a total number of 1000 noise scales for training both models. For results in \cref{fig:computation}, we train an NCSN++ model (definition in \cref{app:arch_search}) on $256\times 256$ LSUN bedroom and church\_outdoor~\citep{yu2015lsun} datasets with the VE SDE and our continuous objective \cref{eqn:training}. The batch size is fixed to 128 on CIFAR-10 and 64 on LSUN.

\paragraph{Ad-hoc interpolation methods for noise scales} 
Models in this experiment are all trained with 1000 noise scales. To get results for P2000 (predictor-only sampler using 2000 steps) which requires 2000 noise scales, we need to interpolate between 1000 noise scales at test time. The specific architecture of the noise-conditional score-based model in \citet{ho2020denoising} uses sinusoidal positional embeddings for conditioning on integer time steps. This allows us to interpolate between noise scales at test time in an ad-hoc way (while it is hard to do so for other architectures like the one in \citet{song2019generative}). Specifically, for SMLD models, we keep $\sm$ and $\sM$ fixed and double the number of time steps. For DDPM models, we halve $\betam$ and $\betaM$ before doubling the number of time steps. Suppose $\{\bfs_\bftheta(\bfx, i)\}_{i=0}^{N-1}$ is a score-based model trained on $N$ time steps, and let $\{\bfs_\bftheta'(\bfx, i)\}_{i=0}^{2N-1}$ denote the corresponding interpolated score-based model at $2N$ time steps. We test two different interpolation strategies for time steps: linear interpolation where $\bfs_\bftheta'(\bfx, i) = \bfs_\bftheta(\bfx, i / 2)$ and rounding interpolation where $\bfs_\bftheta'(\bfx, i) = \bfs_\bftheta(\bfx, \lfloor i/2 \rfloor)$. We provide results with linear interpolation in \cref{tab:compare_samplers}, and give results of rounding interpolation in \cref{tab:compare_samplers_diff_interpolate}. We observe that different interpolation methods result in performance differences but maintain the general trend of predictor-corrector methods performing on par or better than predictor-only or corrector-only samplers.

\paragraph{Hyper-parameters of the samplers}
For Predictor-Corrector and corrector-only samplers on CIFAR-10, we search for the best signal-to-noise ratio ($r$) over a grid that increments at 0.01. We report the best $r$ in \cref{tab:compare_samplers_snr}. For LSUN bedroom/church\_outdoor, we fix $r$ to 0.075. Unless otherwise noted, we use one corrector step per noise scale for all PC samplers. We use two corrector steps per noise scale for corrector-only samplers on CIFAR-10. For sample generation, the batch size is 1024 on CIFAR-10 and 8 on LSUN bedroom/church\_outdoor. 

\begin{table}
    \definecolor{h}{gray}{0.9}
	\caption{Comparing different samplers on CIFAR-10, where ``P2000'' uses the rounding interpolation between noise scales. Shaded regions are obtained with the same computation (number of score function evaluations). Mean and standard deviation are reported over five sampling runs.}\label{tab:compare_samplers_diff_interpolate}
	\centering
	\begin{adjustbox}{max width=\linewidth}
		\begin{tabular}{c|c|c|c|c|c|c|c|c}
			\Xhline{3\arrayrulewidth} \bigstrut
			  & \multicolumn{4}{c|}{Variance Exploding SDE (SMLD)} & \multicolumn{4}{c}{Variance Preserving SDE (DDPM)}\\
			 \Xhline{1\arrayrulewidth}\bigstrut
			\diagbox[height=1cm, width=3cm]{Predictor}{FID$\downarrow$}{Sampler} & P1000 & \cellcolor{h}P2000 & \cellcolor{h}C2000 & \cellcolor{h}PC1000 & P1000 & \cellcolor{h}P2000 & \cellcolor{h}C2000 & \cellcolor{h}PC1000  \\
			\Xhline{1\arrayrulewidth}\bigstrut
            ancestral sampling & 4.98\scalebox{0.7}{ $\pm$ .06}	& \cellcolor{h}4.92\scalebox{0.7}{ $\pm$ .02} &\cellcolor{h} & \cellcolor{h}\textbf{3.62\scalebox{0.7}{ $\pm$ .03}} & 3.24\scalebox{0.7}{ $\pm$ .02}	& \cellcolor{h}\textbf{3.11\scalebox{0.7}{ $\pm$ .03}} &\cellcolor{h} & \cellcolor{h}3.21\scalebox{0.7}{ $\pm$ .02}\\
        	reverse diffusion & 4.79\scalebox{0.7}{ $\pm$ .07} & \cellcolor{h}4.72\scalebox{0.7}{ $\pm$ .07} & \cellcolor{h} & \cellcolor{h}\textbf{3.60\scalebox{0.7}{ $\pm$ .02}} & 3.21\scalebox{0.7}{ $\pm$ .02} & \cellcolor{h}\textbf{3.10\scalebox{0.7}{ $\pm$ .03}} & \cellcolor{h} &\cellcolor{h}3.18\scalebox{0.7}{ $\pm$ .01}\\
            probability flow &	15.41\scalebox{0.7}{ $\pm$ .15} &\cellcolor{h}12.87\scalebox{0.7}{ $\pm$ .09}&\cellcolor{h} \multirow{-3}{*}{20.43\scalebox{0.7}{ $\pm$ .07}} & \cellcolor{h}\textbf{3.51\scalebox{0.7}{ $\pm$ .04}} & 3.59\scalebox{0.7}{ $\pm$ .04} & \cellcolor{h}3.25\scalebox{0.7}{ $\pm$ .04} & \cellcolor{h}\multirow{-3}{*}{19.06\scalebox{0.7}{ $\pm$ .06}} & \cellcolor{h}\textbf{3.06\scalebox{0.7}{ $\pm$ .03}}\\
			\Xhline{3\arrayrulewidth}
		\end{tabular}
	\end{adjustbox}
\end{table}

\begin{table}
    \definecolor{h}{gray}{0.9}
	\caption{Optimal signal-to-noise ratios of different samplers. ``P1000'' or ``P2000'': predictor-only samplers using 1000 or 2000 steps. ``C2000'': corrector-only samplers using 2000 steps. ``PC1000'': PC samplers using 1000 predictor and 1000 corrector steps.}\label{tab:compare_samplers_snr}
	\centering
	\begin{adjustbox}{max width=\linewidth}
		\begin{tabular}{c|c|c|c|c|c|c|c|c}
			\Xhline{3\arrayrulewidth} \bigstrut
			  & \multicolumn{4}{c|}{VE SDE (SMLD)} & \multicolumn{4}{c}{VP SDE (DDPM)}\\
			 \Xhline{1\arrayrulewidth}\bigstrut
			\diagbox[height=1cm, width=3cm]{Predictor}{$r$}{Sampler} & P1000 & P2000 & C2000 & PC1000 & P1000 & P2000 & C2000 & PC1000  \\
			\Xhline{1\arrayrulewidth}\bigstrut
            ancestral sampling & -	& - & & 0.17 & -	& - & & 0.01\\
        	reverse diffusion & - & - &  & 0.16 & - & - &  & 0.01\\
            probability flow &	- &-& \multirow{-3}{*}{0.22} & 0.17 & - & - & \multirow{-3}{*}{0.27} & 0.04\\
			\Xhline{3\arrayrulewidth}
		\end{tabular}
	\end{adjustbox}
\end{table}

\section{Architecture improvements}\label{app:arch_search}
We explored several architecture designs to improve score-based models for both VE and VP SDEs. Our endeavor gives rise to new state-of-the-art sample quality on CIFAR-10, new state-of-the-art likelihood on uniformly dequantized CIFAR-10, and enables the first high-fidelity image samples of resolution $1024\times 1024$ from score-based generative models. Code and checkpoints are open-sourced at \href{https://github.com/yang-song/score\_sde}{https://github.com/yang-song/score\_sde}.

\subsection{Settings for architecture exploration} Unless otherwise noted, all models are trained for 1.3M iterations, and we save one checkpoint per 50k iterations. For VE SDEs, we consider two datasets: $32\times 32$ CIFAR-10~\citep{krizhevsky2009learning} and $64\times 64$ CelebA~\citep{liu2015faceattributes}, pre-processed following \citet{song2020improved}. We compare different configurations based on their FID scores averaged over checkpoints after 0.5M iterations. For VP SDEs, we only consider the CIFAR-10 dataset to save computation, and compare models based on the average FID scores over checkpoints obtained between 0.25M and 0.5M iterations, because FIDs turn to increase after 0.5M iterations for VP SDEs.

All FIDs are computed on 50k samples with \href{https://github.com/tensorflow/gan}{\texttt{tensorflow\_gan}}. For sampling, we use the PC sampler discretized at 1000 time steps. We choose reverse diffusion (see \cref{app:reverse_diffusion}) as the predictor. We use one corrector step per update of the predictor for VE SDEs with a signal-to-noise ratio of 0.16, but save the corrector step for VP SDEs since correctors there only give slightly better results but require double computation. We follow \citet{ho2020denoising} for optimization, including the learning rate, gradient clipping, and learning rate warm-up schedules. Unless otherwise noted, models are trained with the original discrete SMLD and DDPM objectives in~\cref{eqn:ncsn_obj,eqn:ddpm_obj} and use a batch size of 128. The optimal architectures found under these settings are subsequently transferred to continuous objectives and deeper models. We also directly transfer the best architecture for VP SDEs to sub-VP SDEs, given the similarity of these two SDEs.
\begin{figure}[H]
    \centering
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/fir.pdf}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/skip_rescale.pdf}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/resblock_type.pdf}
    \end{subfigure}\\
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/num_res_blocks.pdf}
    \end{subfigure}
    \begin{subfigure}{0.58\textwidth}
        \includegraphics[width=\textwidth]{figures/prog_arch.pdf}
    \end{subfigure}
    \caption{The effects of different architecture components for score-based models trained with VE perturbations.}
    \label{fig:arch_search}
\end{figure}
Our architecture is mostly based on \citet{ho2020denoising}. We additionally introduce the following components to maximize the potential improvement of score-based models.
\begin{enumerate}
    \item Upsampling and downsampling images with anti-aliasing based on Finite Impulse Response (FIR)~\citep{zhang2019shiftinvar}. We follow the same implementation and hyper-parameters in StyleGAN-2~\citep{Karras2019stylegan2}.
    \item Rescaling all skip connections by $\nicefrac{1}{\sqrt{2}}$. This has been demonstrated effective in several best-in-class GAN models, including ProgressiveGAN~\citep{karras2018progressive}, StyleGAN~\citep{karras2019style} and StyleGAN-2~\citep{Karras2019stylegan2}.
    \item Replacing the original residual blocks in DDPM with residual blocks from BigGAN~\citep{brock2018large}.
    \item Increasing the number of residual blocks per resolution from $2$ to $4$.
    \item Incorporating progressive growing architectures. We consider two progressive architectures for input: ``input skip'' and ``residual'', and two progressive architectures for output: ``output skip'' and ``residual''. These progressive architectures are defined and implemented according to StyleGAN-2.
\end{enumerate}

We also tested equalized learning rates, a trick used in very successful models like ProgressiveGAN~\citep{karras2018progressive} and StyleGAN~\citep{karras2019style}. However, we found it harmful at an early stage of our experiments, and therefore decided not to explore more on it.

The exponential moving average (EMA) rate has a significant impact on performance. For models trained with VE perturbations, we notice that 0.999 works better than 0.9999, whereas for models trained with VP perturbations it is the opposite. We therefore use an EMA rate of 0.999 and 0.9999 for VE and VP models respectively.


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/cifar_samples.jpg}
    \caption{Unconditional CIFAR-10 samples from NCSN++ cont. (deep, VE).}
    \label{fig:cifar_samples}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/combine.jpg}
    \caption{Samples on $1024\times 1024$ CelebA-HQ from a modified NCSN++ model trained with the VE SDE.}
    \label{fig:celebahq}
\end{figure}

\subsection{Results on CIFAR-10}\label{app:rescifar10} 

All architecture components introduced above can improve the performance of score-based models trained with VE SDEs, as shown in \cref{fig:arch_search}. The box plots demonstrate the importance of each component when other components can vary freely. On both CIFAR-10 and CelebA, the additional components that we explored always improve the performance on average for VE SDEs. For progressive growing, it is not clear which combination of configurations consistently performs the best, but the results are typically better than when no progressive growing architecture is used. Our best score-based model for VE SDEs 1) uses FIR upsampling/downsampling, 2) rescales skip connections, 3) employs BigGAN-type residual blocks, 4) uses 4 residual blocks per resolution instead of 2, and 5) uses ``residual'' for input and no progressive growing architecture for output. We name this model ``NCSN++'', following the naming convention of previous SMLD models~\citep{song2019generative,song2020improved}. 

We followed a similar procedure to examine these architecture components for VP SDEs, except that we skipped experiments on CelebA due to limited computing resources. The NCSN++ architecture worked decently well for VP SDEs, ranked 4th place over all 144 possible configurations. The top configuration, however, has a slightly different structure, which uses no FIR upsampling/downsampling and no progressive growing architecture compared to NCSN++. We name this model ``DDPM++'', following the naming convention of~\citet{ho2020denoising}.


The basic NCSN++ model with 4 residual blocks per resolution achieves an FID of 2.45 on CIFAR-10, whereas the basic DDPM++ model achieves an FID of 2.78. Here in order to match the convention used in \citet{karras2018progressive,song2019generative} and \citet{ho2020denoising}, we report the lowest FID value over the course of training, rather than the average FID value over checkpoints after 0.5M iterations (used for comparing different models of VE SDEs) or between 0.25M and 0.5M iterations (used for comparing VP SDE models) in our architecture exploration. 

Switching from discrete training objectives to continuous ones in \cref{eqn:training} further improves the FID values for all SDEs. To condition the NCSN++ model on continuous time variables, we change positional embeddings, the layers in \citet{ho2020denoising} for conditioning on discrete time steps, to random Fourier feature embeddings~\citep{tancik2020fourfeat}. The scale parameter of these random Fourier feature embeddings is fixed to 16. We also reduce the number of training iterations to 0.95M to suppress overfitting. These changes improve the FID on CIFAR-10 from 2.45 to 2.38 for NCSN++ trained with the VE SDE, resulting in a model called ``NCSN++ cont.''. In addition, we can further improve the FID from 2.38 to 2.20 by doubling the number of residual blocks per resolution for NCSN++ cont., resulting in the model denoted as ``NCSN++ cont. (deep)''. All quantitative results are summarized in \cref{tab:fid}, and we provide random samples from our best model in \cref{fig:cifar_samples}. 

Similarly, we can also condition the DDPM++ model on continuous time steps, resulting in a model ``DDPM++ cont.''. When trained with the VP SDE, it improves the FID of 2.78 from DDPM++ to 2.55. When trained with the sub-VP SDE, it achieves an FID of 2.61. To get better performance, we used the Euler-Maruyama solver as the predictor for continuously-trained models, instead of the ancestral sampling predictor or the reverse diffusion predictor. This is because the discretization strategy of the original DDPM method does not match the variance of the continuous process well when $t\to 0$, which significantly hurts FID scores. As shown in \cref{tab:bpd}, the likelihood values are 3.21 and 3.05 bits/dim for VP and sub-VP SDEs respectively. Doubling the depth, and trainin with 0.95M iterations, we can improve both FID and bits/dim for both VP and sub-VP SDEs, leading to a model ``DDPM++ cont. (deep)''. Its FID score is 2.41, same for both VP and sub-VP SDEs. When trained with the sub-VP SDE, it can achieve a likelihood of 2.99 bits/dim. Here all likelihood values are reported for the last checkpoint during training.



\subsection{High resolution images} 
\label{app:hq}

Encouraged by the success of NCSN++ on CIFAR-10, we proceed to test it on $1024\times 1024$ CelebA-HQ~\citep{karras2018progressive}, a task that was previously only achievable by some GAN models and VQ-VAE-2~\citep{razavi2019generating}. We used a batch size of 8, increased the EMA rate to 0.9999, and trained a model similar to NCSN++ with the continuous objective (\cref{eqn:training}) for around 2.4M iterations (please find the detailed architecture in our code release.) We use the PC sampler discretized at 2000 steps with the reverse diffusion predictor, one Langevin step per predictor update and a signal-to-noise ratio of 0.15. The scale parameter for the random Fourier feature embeddings is fixed to 16. We use the ``input skip'' progressive architecture for the input, and ``output skip'' progressive architecture for the output. We provide samples in \cref{fig:celebahq}. Although these samples are not perfect (\eg, there are visible flaws on facial symmetry), we believe these results are encouraging and can demonstrate the scalability of our approach. Future work on more effective architectures are likely to significantly advance the performance of score-based generative models on this task.


\section{Controllable generation}\label{app:cond_gen}
Consider a forward SDE with the following general form
\begin{align*}
     \ud \bfx = \bff(\bfx, t) \ud t + \mbf{G}(\bfx, t) \ud \bfw,
\end{align*}
and suppose the initial state distribution is $p_0(\bfx(0) \mid \bfy)$. The density at time $t$ is $p_t(\bfx(t) \mid \bfy)$ when conditioned on $\bfy$. Therefore, using \citet{Anderson1982-ny}, the reverse-time SDE is given by
\begin{align}
\resizebox{0.92\linewidth}{!}{$\displaystyle \ud \bfx = \{\bff(\bfx, t) - \nabla \cdot [\bfG(\bfx, t) \bfG(\bfx,t)\tran] - \bfG(\bfx,t)\bfG(\bfx,t)\tran  \nabla_\bfx  \log p_t(\bfx \mid \bfy)\} \ud t + \mbf{G}(\bfx, t) \ud \bar{\bfw}$} \label{eqn:cond_sde_2}.
\end{align}
Since $p_t(\bfx(t) \mid \bfy) \propto p_t(\bfx(t)) p(\bfy \mid \bfx(t))$, the score $\nabla_\bfx \log p_t(\bfx(t) \mid \bfy)$ can be computed easily by
\begin{align}
    \nabla_{\bfx} \log p_t(\bfx(t) \mid \bfy) = \nabla_{\bfx} \log p_t(\bfx(t)) + \nabla_{\bfx} \log p(\bfy \mid \bfx(t)).
\end{align}
This subsumes the conditional reverse-time SDE in \cref{eqn:cond_sde} as a special case. All sampling methods we have discussed so far can be applied to the conditional reverse-time SDE for sample generation.

\subsection{Class-conditional sampling}\label{app:class_cond_sampling}
When $\bfy$ represents class labels, we can train a time-dependent classifier $p_t(\bfy \mid \bfx(t))$ for class-conditional sampling. Since the forward SDE is tractable, we can easily create a pair of training data $(\bfx(t), \bfy)$ by first sampling $(\bfx(0), \bfy)$ from a dataset and then obtaining $\bfx(t) \sim p_{0t}(\bfx(t) \mid \bfx(0))$. Afterwards, we may employ a mixture of cross-entropy losses over different time steps, like \cref{eqn:training}, to train the time-dependent classifier $p_t(\bfy \mid \bfx(t))$.

To test this idea, we trained a Wide ResNet~\citep{zagoruyko2016wide} (\verb|Wide-ResNet-28-10|) on CIFAR-10 with VE perturbations. The classifier is conditioned on $\log \sigma_i$ using random Fourier features~\citep{tancik2020fourfeat}, and the training objective is a simple sum of cross-entropy losses sampled at different scales. We provide a plot to show the accuracy of this classifier over noise scales in \cref{fig:cond_sample_extend}. The score-based model is an unconditional NCSN++ (4 blocks/resolution) in \cref{tab:fid}, and we generate samples using the PC algorithm with 2000 discretization steps. The class-conditional samples are provided in \cref{fig:cond_gen}, and an extended set of conditional samples is given in \cref{fig:cond_sample_extend}.
\begin{figure}
    \centering
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/conditional_shallow_0.png}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/conditional_shallow_1.png}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/conditional_shallow_2.png}
    \end{subfigure}\\
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/conditional_shallow_3.png}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/conditional_shallow_4.png}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/conditional_shallow_5.png}
    \end{subfigure}\\
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/conditional_shallow_6.png}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/conditional_shallow_7.png}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/conditional_shallow_8.png}
    \end{subfigure}\\
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/conditional_shallow_9.png}
    \end{subfigure}
    \begin{subfigure}{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/accuracy_curve.pdf}
    \end{subfigure}
    \caption{Class-conditional image generation by solving the conditional reverse-time SDE with PC. The curve shows the accuracy of our noise-conditional classifier over different noise scales.}
    \label{fig:cond_sample_extend}
\end{figure}

\subsection{Imputation} \label{app:imputation}
Imputation is a special case of conditional sampling. Denote by $\Omega(\bfx)$ and $\bomega(\bfx)$ the known and unknown dimensions of $\bfx$ respectively, and let $\bff_{\bomega}(\cdot, t)$ and $\bfG_{\bomega}(\cdot, t)$ denote $\bff(\cdot, t)$ and $\bfG(\cdot, t)$ restricted to the unknown dimensions. For VE/VP SDEs, the drift coefficient $\bff(\cdot, t)$ is element-wise, and the diffusion coefficient $\bfG(\cdot, t)$ is diagonal. When $\bff(\cdot, t)$ is element-wise, $\bff_{\bomega}(\cdot, t)$ denotes the same element-wise function applied only to the unknown dimensions. When $\bfG(\cdot, t)$ is diagonal, $\bfG_{\bomega}(\cdot, t)$ denotes the sub-matrix restricted to unknown dimensions. 

For imputation, our goal is to sample from $p(\bomega(\bfx(0)) \mid \Omega(\bfx(0)) = \bfy)$. Define a new diffusion process $\bfz(t) = \bomega(\bfx(t))$, and note that the SDE for $\bfz(t)$ can be written as
\begin{align*}
\ud \bfz = \bff_\bomega(\bfz, t) \ud t + \bfG_\bomega(\bfz, t) \ud \bfw.
\end{align*}
The reverse-time SDE, conditioned on $\Omega(\bfx(0)) = \bfy$, is given by
\begin{multline*}
\ud\bfz = \big\{\bff_\bomega(\bfz, t) - \nabla \cdot [\bfG_{\bomega}(\bfz, t) \bfG_{\bomega}(\bfz,t)\tran] \\- \bfG_\bomega(\bfz, t)\bfG_\bomega(\bfz, t)\tran \nabla_\bfz \log p_t(\bfz \mid \Omega(\bfz(0)) = \bfy)\big\}\ud t + \bfG_\bomega(\bfz, t)\ud\bar{\bfw}.
\end{multline*}
Although $p_t(\bfz(t) \mid \Omega(\bfx(0)) = \bfy)$ is in general intractable, it can be approximated. Let $A$ denote the event $\Omega(\bfx(0)) = \bfy $. We have
\begin{align*}
p_t(\bfz(t) \mid \Omega(\bfx(0)) = \bfy) = p_t(\bfz(t) \mid A) &= \int p_t(\bfz(t) \mid \Omega(\bfx(t)), A) p_t(\Omega(\bfx(t)) \mid A) \ud \Omega(\bfx(t)) \\
&= \mathbb{E}_{p_t(\Omega(\bfx(t)) \mid A)}[p_t(\bfz(t) \mid \Omega(\bfx(t)), A)]\\
&\approx \mathbb{E}_{p_t(\Omega(\bfx(t)) \mid A)}[p_t(\bfz(t) \mid \Omega(\bfx(t)))]\\
&\approx p_t(\bfz(t) \mid \hat{\Omega}(\bfx(t))),
\end{align*}
where $\hat{\Omega}(\bfx(t))$ is a random sample from $p_t(\Omega(\bfx(t)) \mid A)$, which is typically a tractable distribution. Therefore,
\begin{align*}
\nabla_\bfz \log p_t(\bfz(t) \mid \Omega(\bfx(0))=\bfy) &\approx \nabla_\bfz \log p_t(\bfz(t) \mid \hat{\Omega}(\bfx(t))) \\
&= \nabla_\bfz \log p_t([\bfz(t); \hat{\Omega}(\bfx(t))]),
\end{align*}
where $[\bfz(t); \hat{\Omega}(\bfx(t))]$ denotes a vector $\bfu(t)$ such that $\Omega(\bfu(t)) = \hat{\Omega}(\bfx(t))$ and $\bar{\Omega}(\bfu(t)) = \bfz(t)$, and the identity holds because $\nabla_\bfz \log p_t([\bfz(t); \hat{\Omega}(\bfx(t))]) = \nabla_\bfz \log p_t(\bfz(t) \mid \hat{\Omega}(\bfx(t))) + \nabla_\bfz \log p(\hat{\Omega}(\bfx(t))) = \nabla_\bfz \log p_t(\bfz(t) \mid \hat{\Omega}(\bfx(t)))$.

We provided an extended set of inpainting results in \cref{fig:bedroom_inpainting,fig:church_inpainting}.

\subsection{Colorization}\label{app:colorization}
Colorization is a special case of imputation, except that the known data dimensions are coupled. We can decouple these data dimensions by using an orthogonal linear transformation to map the gray-scale image to a separate channel in a different space, and then perform imputation to complete the other channels before transforming everything back to the original image space. The orthogonal matrix we used to decouple color channels is
\begin{align*}
    \begin{pmatrix}
        0.577 &  -0.816 &  0\\
        0.577 &  0.408 &  0.707\\
        0.577 &  0.408 & -0.707\\
    \end{pmatrix}.
\end{align*}
Because the transformations are all orthogonal matrices, the standard Wiener process $\bfw(t)$ will still be a standard Wiener process in the transformed space, allowing us to build an SDE and use the same imputation method in \cref{app:imputation}. We provide an extended set of colorization results in \cref{fig:bedroom_colorization,fig:church_colorization}.

\subsection{Solving general inverse problems}\label{app:inverse_prob}
Suppose we have two random variables $\bfx$ and $\bfy$, and we know the forward process of generating $\bfy$ from $\bfx$, given by $p(\bfy \mid \bfx)$. The inverse problem is to obtain $\bfx$ from $\bfy$, that is, generating samples from $p(\bfx \mid \bfy)$. In principle, we can estimate the prior distribution $p(\bfx)$ and obtain $p(\bfx \mid \bfy)$ using Bayes' rule: $p(\bfx \mid \bfy) = p(\bfx) p(\bfy \mid \bfx) / p(\bfy)$. In practice, however, both estimating the prior and performing Bayesian inference are non-trivial.

Leveraging \cref{eqn:cond_sde_2}, score-based generative models provide one way to solve the inverse problem. Suppose we have a diffusion process $\{\bfx(t)\}_{t=0}^T$ generated by perturbing $\bfx$ with an SDE, and a time-dependent score-based model $\bfs_{\bftheta*}(\bfx(t), t)$ trained to approximate $\nabla_\bfx \log p_t(\bfx(t))$. Once we have an estimate of $\nabla_\bfx \log p_t(\bfx(t) \mid \bfy)$, we can simulate the reverse-time SDE in \cref{eqn:cond_sde_2} to sample from $p_0(\bfx(0) \mid \bfy) = p(\bfx \mid \bfy)$. To obtain this estimate, we first observe that
\begin{align*}
    \nabla_\bfx \log p_t(\bfx(t) \mid \bfy) = \nabla_\bfx \log \int p_t(\bfx(t) \mid \bfy(t), \bfy) p(\bfy(t) \mid \bfy) \ud \bfy(t),
\end{align*}
where $\bfy(t)$ is defined via $\bfx(t)$ and the forward process $p(\bfy(t) \mid \bfx(t))$. Now assume two conditions:
\begin{itemize}
    \item $p(\bfy(t) \mid \bfy)$ is tractable. We can often derive this distribution from the interaction between the forward process and the SDE, like in the case of image imputation and colorization.
    \item $p_t(\bfx(t) \mid \bfy(t), \bfy) \approx p_t(\bfx(t) \mid \bfy(t))$. For small $t$, $\bfy(t)$ is almost the same as $\bfy$ so the approximation holds. For large $t$, $\bfy$ becomes further away from $\bfx(t)$ in the Markov chain, and thus have smaller impact on $\bfx(t)$. Moreover, the approximation error for large $t$ matter less for the final sample, since it is used early in the sampling process.
\end{itemize}
Given these two assumptions, we have
\begin{align}
    \nabla_\bfx \log p_t(\bfx(t) \mid \bfy) &\approx \nabla_\bfx \log \int p_t(\bfx(t) \mid \bfy(t)) p(\bfy(t) \mid \bfy) \ud \bfy \notag\\
    &\approx \nabla_\bfx \log p_t(\bfx(t) \mid \hat{\bfy}(t)) \notag\\
    &= \nabla_\bfx \log p_t(\bfx(t)) + \nabla_\bfx \log p_t(\hat{\bfy}(t) \mid \bfx(t)) \notag\\
    &\approx \bfs_{\bftheta^*}(\bfx(t), t) + \nabla_\bfx \log p_t(\hat{\bfy}(t) \mid \bfx(t)), \label{eqn:cond_approx}
\end{align}
where $\hat{\bfy}(t)$ is a sample from $p(\bfy(t) \mid \bfy)$. Now we can plug \cref{eqn:cond_approx} into \cref{eqn:cond_sde_2} and solve the resulting reverse-time SDE to generate samples from $p(\bfx \mid \bfy)$.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/bedroom_inpainting.jpg}
    \caption{Extended inpainting results for $256\times 256$ bedroom images.}
    \label{fig:bedroom_inpainting}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/church_inpainting.jpg}
    \caption{Extended inpainting results for $256\times 256$ church images.}
    \label{fig:church_inpainting}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/bedroom_colorization.jpg}
    \caption{Extended colorization results for $256\times 256$ bedroom images.}
    \label{fig:bedroom_colorization}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/church_colorization.jpg}
    \caption{Extended colorization results for $256\times 256$ church images.}
    \label{fig:church_colorization}
\end{figure}
 
\end{document}
