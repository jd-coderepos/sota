
\documentclass{article} \usepackage{iclr2021_conference,times}
\iclrfinalcopy

\usepackage{xcolor}
\usepackage{hyperref}       \usepackage{url}
\hypersetup{
  colorlinks,
  linkcolor={red!50!black},
  citecolor={blue!50!black},
  urlcolor={blue!80!black}
  }
  \definecolor{orange}{HTML}{ff7f0e}
  \definecolor{blue}{HTML}{1f77b4}

\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{algorithmicx}

\usepackage{microtype}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{sidecap}
\usepackage{caption}
\usepackage{booktabs} \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{comment}
\usepackage{enumitem}
\usepackage{wrapfig,tikz}
\usepackage{amsmath,longtable,fancyhdr}
\usepackage{bigstrut, tabularx, multirow, makecell, diagbox}
\usepackage[export]{adjustbox}
\usepackage{graphicx,float}
\usepackage{amssymb,amsthm, mathtools}
\usepackage{stackrel}
\usepackage{color}
\usepackage{colortbl}
\usepackage{theoremref}
\usepackage{cases}
\usepackage{stmaryrd}
\usepackage{mathabx}
\usepackage{dsfont}
\usepackage[capitalise]{cleveref}
\newcommand{\abs}[1]{\lvert#1\rvert}
\newcommand*{\tran}{^{\mkern-1.5mu\mathsf{T}}}
\newcommand{\mbf}[1]{\mathbf{#1}}
\newcommand{\bs}[1]{\boldsymbol{#1}}
\newcommand{\mbb}[1]{\mathbb{#1}}
\newcolumntype{C}[1]{>{\centering\arraybackslash}m{#1}}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\newcommand{\ud}{\mathrm{d}}
\newcommand{\up}{\mathrm}
\def\dbar{\mathrm{\mathchar'26\mkern-12mu d}}
\newcommand{\wave}{\sim}
\newcommand{\mcal}{\mathcal}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newtheorem{problem}{Problem}
\usepackage{mdframed}
\newtheorem{recipe}{Technique}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{remark}{Remark}
\newtheorem{observation}{Observation}
\newtheorem{claim}{Claim}
\newtheorem{proposition}{Proposition}
\newtheorem{assumption}{Assumption}
\newenvironment{oneshot}[1]{\@begintheorem{#1}{\unskip}}{\@endtheorem}
\newtheorem{innercustomthm}{Theorem}
\newtheorem{definition}{Definition}
\newenvironment{customthm}[1]
{\renewcommand\theinnercustomthm{#1}\innercustomthm}
{\endinnercustomthm}
\newtheorem{innercustomlem}{Lemma}
\newenvironment{customlem}[1]
{\renewcommand\theinnercustomlem{#1}\innercustomlem}
{\endinnercustomlem}
\newtheorem{innercustomhyp}{Hypothesis}
\newenvironment{customhyp}[1]
{\renewcommand\theinnercustomhyp{#1}\innercustomhyp}
{\endinnercustomhyp}
\newtheorem{innercustomprop}{Proposition}
\newenvironment{customprop}[1]
{\renewcommand\theinnercustomprop{#1}\innercustomprop}
{\endinnercustomprop}

\newcommand{\ys}[1]{{\color{magenta} [YS: {#1}]}}

\newcommand{\be}{}
\definecolor{Gray}{gray}{0.85}
\definecolor{LightCyan}{rgb}{0.88,1,1}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argsup}{arg\,sup}
\DeclareMathOperator*{\arginf}{arg\,inf}

\makeatletter
\usepackage{xspace} 
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}

\newcommand{\eqnref}[1]{Eq\onedot~\eqref{#1}}

\newcommand{\bfx}{\mathbf{x}}
\newcommand{\bfA}{\mathbf{A}}
\newcommand{\bfB}{\mathbf{B}}
\newcommand{\bfW}{\mathbf{W}}
\newcommand{\bfw}{\mathbf{w}}
\newcommand{\bfV}{\mathbf{V}}
\newcommand{\bfM}{\mathbf{M}}
\newcommand{\bfa}{\mathbf{a}}
\newcommand{\bfb}{\mathbf{b}}
\newcommand{\bfv}{\mathbf{v}}
\newcommand{\bfz}{\mathbf{z}}
\newcommand{\bfI}{\mathbf{I}}
\newcommand{\bft}{\mathbf{t}}
\newcommand{\bfu}{\mathbf{u}}
\newcommand{\bfr}{\mathbf{r}}
\newcommand{\bff}{\mathbf{f}}
\newcommand{\bfm}{\mathbf{m}}
\newcommand{\bfsigma}{\mbf{\Sigma}}
\newcommand{\bfF}{\mathbf{F}}
\newcommand{\bfL}{\mathbf{L}}
\newcommand{\bfzero}{\mathbf{0}}
\newcommand{\bfe}{{\bs{\epsilon}}}
\newcommand{\bftheta}{{\boldsymbol{\theta}}}
\newcommand{\bfalpha}{{\boldsymbol{\alpha}}}
\newcommand{\bfphi}{{\boldsymbol{\phi}}}
\newcommand{\bfy}{\mathbf{y}}
\newcommand{\bfs}{\mathbf{s}}
\newcommand{\bfh}{\mathbf{h}}
\newcommand{\bfg}{\mathbf{g}}
\newcommand{\bfG}{\mathbf{G}}
\newcommand{\bfxd}{\mathbf{x}_{\text{data}}}
\newcommand{\pd}{p_{\mathrm{data}}}
\newcommand{\sm}{\sigma_{\text{min}}}
\newcommand{\sM}{\sigma_{\text{max}}}
\newcommand{\bbeta}{\bar{\beta}}
\newcommand{\bomega}{{\bar{\Omega}}}
\newcommand{\bbetam}{\bbeta_{\text{min}}}
\newcommand{\bbetaM}{\bbeta_{\text{max}}}
\newcommand{\betam}{\beta_{\text{min}}}
\newcommand{\betaM}{\beta_{\text{max}}}
\newcommand{\bfmu}{{\boldsymbol \mu}}
\definecolor{blue1}{RGB}{0,128,255}
\definecolor{blue3}{RGB}{0,0,128}
\definecolor{darkpastelgreen}{rgb}{0.01, 0.75, 0.24}
\definecolor{cerulean}{rgb}{0.0, 0.48, 0.65}

\def\eg{\emph{e.g}\onedot}
\def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot}
\def\Ie{\emph{I.e}\onedot}
\def\cf{\emph{cf}\onedot}
\def\Cf{\emph{Cf}\onedot}
\def\etc{\emph{etc}\onedot}
\def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot}
\def\dof{d.o.f\onedot}
\def\aka{a.k.a\onedot}
\def\iid{i.i.d\onedot}
\def\etal{\emph{et al}\onedot}

\definecolor{darkgreen}{rgb}{0,0.6,0}
\newcommand{\js}[1]{\textcolor{darkgreen}{{[jascha: #1]}}}
\newcommand{\se}[1]{\textcolor{magenta}{{[SE: #1]}}}
\newcommand{\todo}[1]{\textcolor{red}{{[TODO: #1]}}}
\newcommand{\abhi}[1]{\textcolor{brown}{{[AK: #1]}}}

\title{Score-Based Generative Modeling through Stochastic Differential Equations}


\author{Yang Song\thanks{Work partially done during an internship at Google Brain.}\\
Stanford University\\
\texttt{yangsong@cs.stanford.edu}\\
\And 
Jascha Sohl-Dickstein\\
Google Brain\\
\texttt{jaschasd@google.com}\\
\And
Diederik P. Kingma \\
Google Brain\\
\texttt{durk@google.com}\\
\AND
Abhishek Kumar \\
Google Brain\\
\texttt{abhishk@google.com}\\
\And 
Stefano Ermon \\
Stanford University\\
\texttt{ermon@cs.stanford.edu}\\
\And
Ben Poole\\
Google Brain\\
\texttt{pooleb@google.com}
}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}


\maketitle

\begin{abstract}
Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. 
Crucially, the reverse-time SDE depends only on the time-dependent gradient field (\aka, score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, 
as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of  images for the first time from a score-based generative model.
\end{abstract} \section{Introduction}



Two successful classes of probabilistic generative models involve sequentially corrupting training data with slowly increasing noise, and then learning to reverse this corruption in order to form a generative model of the data. 
\emph{Score matching with Langevin dynamics} (SMLD)~\citep{song2019generative} estimates the \emph{score} (\ie, the gradient of the log probability density with respect to data) at each noise scale, and then uses Langevin dynamics to sample from a sequence of decreasing noise scales during generation. 
\emph{Denoising diffusion probabilistic modeling} (DDPM)~\citep{sohl2015deep,ho2020denoising} trains a sequence of probabilistic models to reverse each step of the noise corruption, 
using knowledge of the functional form of the reverse distributions to make training tractable. 
For continuous state spaces, the DDPM training objective implicitly computes 
scores at each noise scale. 
We therefore refer to these two model classes together as \emph{score-based generative models}. 

Score-based generative models, and related techniques \citep{bordes2017learning,goyal2017variational, du2019implicit}, have proven effective at generation of images~\citep{song2019generative,song2020improved,ho2020denoising}, audio~\citep{chen2020wavegrad,kong2020diffwave}, graphs~\citep{niu20a}, and shapes~\citep{ShapeGF}. To enable new sampling methods and further extend the capabilities of score-based generative models, we propose a unified framework that generalizes previous approaches through the lens of stochastic differential equations (SDEs).



\begin{SCfigure}
    \centering
    \caption{{\bf Solving a reverse-time SDE yields a score-based generative model.} Transforming data to a simple noise distribution can be accomplished with a continuous-time SDE. This SDE can be reversed if we know the score of the distribution at each intermediate time step, . }
    \includegraphics[width=0.65\linewidth, trim=105 0 110 0, clip]{figures/diffusion_schematic.pdf}
    \label{fig:teaser}
\end{SCfigure}

Specifically, instead of perturbing data with a finite number of noise distributions, we consider a continuum of distributions that evolve over time according to a diffusion process. This process progressively diffuses a data point into random noise, and is given by a prescribed SDE that does not depend on the data and has no trainable parameters. 
By reversing this process, we can smoothly mold random noise into data for sample generation. Crucially, this reverse process satisfies a reverse-time SDE~\citep{Anderson1982-ny}, which can be derived from the forward SDE given the score of the marginal probability densities as a function of time. We can therefore approximate the reverse-time SDE by training a time-dependent neural network to estimate the scores, and then produce samples using numerical SDE solvers. Our key idea is summarized in \cref{fig:teaser}.

Our proposed framework has several theoretical and practical contributions: 

\textbf{Flexible sampling and likelihood computation:}~ We can employ any general-purpose SDE solver to integrate the reverse-time SDE for sampling. In addition, we propose two special methods not viable for general SDEs: (i) Predictor-Corrector (PC) samplers that combine numerical SDE solvers with score-based MCMC approaches, such as Langevin MCMC~\citep{parisi1981correlation} and HMC~\citep{neal2011mcmc}; and (ii) deterministic samplers based on the probability flow ordinary differential equation (ODE). The former \emph{unifies and improves} over existing sampling methods for score-based models. The latter allows for \emph{fast adaptive sampling} via black-box ODE solvers, \emph{flexible data manipulation} via latent codes, a \emph{uniquely identifiable encoding}, and notably, \emph{exact likelihood computation}.

\textbf{Controllable generation:}~ We can modulate the generation process by conditioning on information not available during training, because the conditional reverse-time SDE can be efficiently estimated from \emph{unconditional} scores. This enables applications such as class-conditional generation, image inpainting, colorization and other inverse problems, all achievable using a single unconditional score-based model without re-training.


\textbf{Unified framework:}~ Our framework provides a unified way to explore and tune various SDEs for improving score-based generative models.
The methods of SMLD and DDPM can be amalgamated into our framework as discretizations of two separate SDEs. 
Although DDPM~\citep{ho2020denoising} was recently reported to achieve higher sample quality than SMLD~\citep{song2019generative,song2020improved}, we show that with better architectures and new sampling algorithms allowed by our framework, the latter can catch up---it achieves new state-of-the-art Inception score (9.89) and FID score (2.20) on CIFAR-10, as well as high-fidelity generation of  images for the first time from a score-based model. In addition, we propose a new SDE under our framework that achieves a likelihood value of 2.99 bits/dim on uniformly dequantized CIFAR-10 images, setting a new record on this task.

 \section{Background}
\subsection{Denoising score matching with Langevin dynamics (SMLD)}\label{sec:ncsn}
Let  be a perturbation kernel, and , where  denotes the data distribution. Consider a sequence of positive noise scales . Typically,  is small enough such that , and  is large enough such that . \citet{song2019generative} propose to train a Noise Conditional Score Network (NCSN), denoted by , with a weighted sum of denoising score matching~\citep{vincent2011connection} objectives:

Given sufficient data and model capacity, the optimal score-based model  matches  almost everywhere for . For sampling, \citet{song2019generative} run  steps of Langevin MCMC to get a sample for each  sequentially:

where  is the step size, and  is standard normal. The above is repeated for  in turn with  and  when . As  and  for all ,  becomes an exact sample from  under some regularity conditions.

\subsection{Denoising diffusion probabilistic models (DDPM)}\label{sec:ddpm}
\citet{sohl2015deep,ho2020denoising} consider a sequence of positive noise scales . For each training data point , a discrete Markov chain  is constructed such that , and therefore , where . Similar to SMLD, we can denote the perturbed data distribution as . The noise scales are prescribed such that  is approximately distributed according to . A variational Markov chain in the reverse direction is parameterized with , and trained with a re-weighted variant of the evidence lower bound (ELBO):

After solving \cref{eqn:ddpm_obj} to get the optimal model , samples can be generated by starting from  and following the estimated reverse Markov chain as below

We call this method \emph{ancestral sampling}, since it amounts to performing ancestral sampling from the graphical model . The objective \cref{eqn:ddpm_obj} described here is  in \citet{ho2020denoising}, written in a form to expose more similarity to \cref{eqn:ncsn_obj}. 
Like \cref{eqn:ncsn_obj}, \cref{eqn:ddpm_obj} is also a weighted sum of denoising score matching objectives, which implies that the optimal model, , matches the score of the perturbed data distribution, . Notably, the weights of the -th summand in \cref{eqn:ncsn_obj} and \cref{eqn:ddpm_obj}, namely  and , are related to corresponding perturbation kernels in the same functional form:  and . 

 \section{Score-based generative modeling with SDEs}
Perturbing data with multiple noise scales is key to the success of previous methods. We propose to generalize this idea further to an infinite number of noise scales, such that perturbed data distributions evolve according to an SDE as the noise intensifies. An overview of our framework is given in \cref{fig:toy}.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth, trim=16 0 12 14, clip]{figures/teaser.pdf}
    \caption{\textbf{Overview of score-based generative modeling through SDEs}. We can map data to a noise distribution (the prior) with an SDE (\cref{sec:sde_perturb}), and reverse this SDE for generative modeling (\cref{sec:sde_reverse}). We can also reverse the associated probability flow ODE (\cref{sec:flow}), which yields a deterministic process that samples from the same distribution as the SDE. Both the reverse-time SDE and probability flow ODE can be obtained by estimating the score  (\cref{sec:training}).}
    \label{fig:toy}
\end{figure}

\subsection{Perturbing data with SDEs}\label{sec:sde_perturb}
Our goal is to construct a diffusion process  indexed by a continuous time variable , such that , for which we have a dataset of \iid samples, and , for which we have a tractable form to generate samples efficiently. In other words,  is the data distribution and  is the prior distribution. This diffusion process can be modeled as the solution to an It\^{o} SDE:

where  is the standard Wiener process (\aka, Brownian motion),  is a vector-valued function called the \emph{drift} coefficient of , and  is a scalar function known as the \emph{diffusion} coefficient of . For ease of presentation we assume the diffusion coefficient is a scalar (instead of a  matrix) and does not depend on , but our theory can be generalized to hold in those cases (see \cref{app:general_sde}). The SDE has a unique strong solution as long as the coefficients are globally Lipschitz in both state and time~\citep{oksendal2003stochastic}. We hereafter denote by  the probability density of , and use  to denote the transition kernel from  to , where .

Typically,  is an unstructured prior distribution that contains no information of , such as a Gaussian distribution with fixed mean and variance. There are various ways of designing the SDE in \cref{eqn:forward_sde} such that it diffuses the data distribution into a fixed prior distribution. We provide several examples later in \cref{sec:sde_examples} that are derived from continuous generalizations of SMLD and DDPM.

\subsection{Generating samples by reversing the SDE}\label{sec:sde_reverse}
By starting from samples of  and reversing the process, we can obtain samples . A remarkable result from~\citet{Anderson1982-ny} states that the reverse of a diffusion process is also a diffusion process, running backwards in time and given by the reverse-time SDE:

where  is a standard Wiener process
when time flows backwards from  to , and  is an infinitesimal negative timestep. Once the score of each marginal distribution, , is known for all , we can derive the reverse diffusion process from \cref{eqn:backward_sde} and simulate it to sample from .


\subsection{Estimating scores for the SDE}\label{sec:training}
The score of a distribution can be estimated by training a score-based model on samples with score matching~\citep{hyvarinen2005estimation,song2019sliced}. To estimate , we can train a time-dependent score-based model  via a continuous generalization to \cref{eqn:ncsn_obj,eqn:ddpm_obj}:

Here  is a positive weighting function,  is uniformly sampled over ,  and . With sufficient data and model capacity, score matching ensures that the optimal solution to \cref{eqn:training}, denoted by , equals  for almost all  and . As in SMLD and DDPM, we can typically choose . Note that \cref{eqn:training} uses denoising score matching, but other score matching objectives, such as sliced score matching~\citep{song2019sliced} and finite-difference score matching~\citep{pang2020efficient} are also applicable here. 

We typically need to know the transition kernel  to efficiently solve \cref{eqn:training}. When  is affine, the transition kernel is always a Gaussian distribution, where the mean and variance are often known in closed-forms and can be obtained with standard techniques (see Section 5.5 in \citet{sarkka2019applied}). For more general SDEs, we may solve Kolmogorov's forward equation~\citep{oksendal2003stochastic} to obtain . Alternatively, we can simulate the SDE to sample from  and replace denoising score matching in \cref{eqn:training} with sliced score matching for model training, which bypasses the computation of  (see \cref{app:general_sde}).




\subsection{Examples: VE, VP SDEs and beyond}\label{sec:sde_examples}
The noise perturbations used in SMLD and DDPM can be regarded as discretizations of two different SDEs. Below we provide a brief discussion and relegate more details to \cref{app:sde_derive}.

When using a total of  noise scales, each perturbation kernel  of SMLD corresponds to the distribution of  in the following Markov chain:

where , and we have introduced  to simplify the notation.
In the limit of ,  becomes a function ,  becomes , and the Markov chain  becomes a continuous stochastic process , where we have used a continuous time variable  for indexing, rather than an integer . The process  is given by the following SDE

Likewise for the perturbation kernels  of DDPM, the discrete Markov chain is

As , \cref{eqn:ddpm} converges to the following SDE,


Therefore, the noise perturbations used in SMLD and DDPM correspond to discretizations of SDEs \cref{eqn:ncsn_sde,eqn:ddpm_sde}. Interestingly, the SDE of \cref{eqn:ncsn_sde} always gives a process with exploding variance when , whilst the SDE of \cref{eqn:ddpm_sde} yields a process with a fixed variance of one when the initial distribution has unit variance (proof in \cref{app:sde_derive}). Due to this difference, we hereafter refer to \cref{eqn:ncsn_sde} as the Variance Exploding (VE) SDE, and \cref{eqn:ddpm_sde} the Variance Preserving (VP) SDE. 

Inspired by the VP SDE, we propose a new type of SDEs which perform particularly well on likelihoods (see \cref{sec:flow}), given by

When using the same  and starting from the same initial distribution, the variance of the stochastic process induced by \cref{eqn:subvp_sde} is always bounded by the VP SDE at every intermediate time step (proof in \cref{app:sde_derive}). For this reason, we name \cref{eqn:subvp_sde} the sub-VP SDE.

Since VE, VP and sub-VP SDEs all have affine drift coefficients, their perturbation kernels  are all Gaussian and can be computed in closed-forms, as discussed in \cref{sec:training}. This makes training with \cref{eqn:training} particularly efficient. 







 \section{Solving the reverse SDE}
After training a time-dependent score-based model , we can use it to construct the reverse-time SDE and then simulate it with numerical approaches to generate samples from . 

\subsection{General-purpose numerical SDE solvers}

Numerical solvers provide approximate trajectories from SDEs. Many general-purpose numerical methods exist for solving SDEs, such as Euler-Maruyama and stochastic Runge-Kutta methods~\citep{kloeden2013numerical}, which correspond to different discretizations of the stochastic dynamics. We can apply any of them to the reverse-time SDE for sample generation. 

Ancestral sampling, the sampling method of DDPM (\cref{eqn:ddpm_sampling}), actually corresponds to one special discretization of the reverse-time VP SDE (\cref{eqn:ddpm_sde}) (see \cref{app:reverse_diffusion}). Deriving the ancestral sampling rules for new SDEs, however, can be non-trivial. To remedy this, we propose \emph{reverse diffusion samplers} (details in \cref{app:reverse_diffusion}), which discretize the reverse-time SDE in the same way as the forward one, and thus can be readily derived given the forward discretization.
As shown in \cref{tab:compare_samplers}, reverse diffusion samplers perform slightly better than ancestral sampling for both SMLD and DDPM models on CIFAR-10 (DDPM-type ancestral sampling is also applicable to SMLD models, see \cref{app:ancestral}.)


\subsection{Predictor-corrector samplers}
Unlike generic SDEs, we have additional information that can be used to improve solutions. Since we have a score-based model , we can employ score-based MCMC approaches, such as Langevin MCMC~\citep{parisi1981correlation,grenander1994representations} or HMC~\citep{neal2011mcmc} to sample from  directly, and correct the solution of a numerical SDE solver. 

Specifically, at each time step, the numerical SDE solver first gives an estimate of the sample at the next time step, playing the role of a ``predictor''. Then, the score-based MCMC approach corrects the 
marginal distribution of the estimated sample, playing the role of a ``corrector''. The idea is analogous to Predictor-Corrector methods, a family of numerical continuation techniques for solving systems of equations~\citep{allgower2012numerical}, and we similarly name our hybrid sampling algorithms \emph{Predictor-Corrector} (PC) samplers. Please find pseudo-code and a complete description in \cref{app:pc}. PC samplers generalize the original sampling methods of SMLD and DDPM: the former uses an identity function as the predictor and annealed Langevin dynamics as the corrector, while the latter uses ancestral sampling as the predictor and identity as the corrector. 

\begin{table}
    \definecolor{h}{gray}{0.9}
	\caption{Comparing different reverse-time SDE solvers on CIFAR-10. Shaded regions are obtained with the same computation (number of score function evaluations). Mean and standard deviation are reported over five sampling runs. ``P1000'' or ``P2000'': predictor-only samplers using 1000 or 2000 steps. ``C2000'': corrector-only samplers using 2000 steps. ``PC1000'': Predictor-Corrector (PC) samplers using 1000 predictor and 1000 corrector steps.}\label{tab:compare_samplers}
	\centering
	\begin{adjustbox}{max width=\linewidth}
		\begin{tabular}{c|c|c|c|c|c|c|c|c}
			\Xhline{3\arrayrulewidth} \bigstrut
			  & \multicolumn{4}{c|}{Variance Exploding SDE (SMLD)} & \multicolumn{4}{c}{Variance Preserving SDE (DDPM)}\\
			 \Xhline{1\arrayrulewidth}\bigstrut
			\diagbox[height=1cm, width=3cm]{Predictor}{FID}{Sampler} & P1000 & \cellcolor{h}P2000 & \cellcolor{h}C2000 & \cellcolor{h}PC1000 & P1000 & \cellcolor{h}P2000 & \cellcolor{h}C2000 & \cellcolor{h}PC1000  \\
			\Xhline{1\arrayrulewidth}\bigstrut
            ancestral sampling & 4.98\scalebox{0.7}{  .06}	& \cellcolor{h}4.88\scalebox{0.7}{  .06} &\cellcolor{h} & \cellcolor{h}\textbf{3.62\scalebox{0.7}{  .03}} & 3.24\scalebox{0.7}{  .02}	& \cellcolor{h}3.24\scalebox{0.7}{  .02} &\cellcolor{h} & \cellcolor{h}\textbf{3.21\scalebox{0.7}{  .02}}\\
        	reverse diffusion & 4.79\scalebox{0.7}{  .07} & \cellcolor{h}4.74\scalebox{0.7}{  .08} & \cellcolor{h} & \cellcolor{h}\textbf{3.60\scalebox{0.7}{  .02}} & 3.21\scalebox{0.7}{  .02} & \cellcolor{h}3.19\scalebox{0.7}{  .02} & \cellcolor{h} &\cellcolor{h}\textbf{3.18\scalebox{0.7}{  .01}}\\
            probability flow &	15.41\scalebox{0.7}{  .15} &\cellcolor{h}10.54\scalebox{0.7}{  .08}&\cellcolor{h} \multirow{-3}{*}{20.43\scalebox{0.7}{  .07}} & \cellcolor{h}\textbf{3.51\scalebox{0.7}{  .04}} & 3.59\scalebox{0.7}{  .04} & \cellcolor{h}3.23\scalebox{0.7}{  .03} & \cellcolor{h}\multirow{-3}{*}{19.06\scalebox{0.7}{  .06}} & \cellcolor{h}\textbf{3.06\scalebox{0.7}{  .03}}\\
			\Xhline{3\arrayrulewidth}
		\end{tabular}
	\end{adjustbox}
\end{table}



We test PC samplers on SMLD and DDPM models (see \cref{alg:pc_smld,alg:pc_ddpm} in \cref{app:pc}) trained with original discrete objectives given by \cref{eqn:ncsn_obj,eqn:ddpm_obj}. This exhibits the compatibility of PC samplers to score-based models trained with a fixed number of noise scales. We summarize the performance of different samplers in \cref{tab:compare_samplers}, where probability flow is a predictor to be discussed in \cref{sec:flow}. Detailed experimental settings and additional results are given in \cref{app:pc}. We observe that our reverse diffusion sampler always outperform ancestral sampling, and corrector-only methods (C2000) perform worse than other competitors (P2000, PC1000) with the same computation (In fact, we need way more corrector steps per noise scale, and thus more computation, to match the performance of other samplers.) For all predictors, adding one corrector step for each predictor step (PC1000) doubles computation but always improves sample quality (against P1000). Moreover, it is typically better than doubling the number of predictor steps without adding a corrector (P2000), where we have to interpolate between noise scales in an ad hoc manner (detailed in \cref{app:pc}) for SMLD/DDPM models. In \cref{fig:computation} (\cref{app:pc}), we additionally provide qualitative comparison for models trained with the continuous objective \cref{eqn:training} on  LSUN images and the VE SDE, where PC samplers clearly surpass predictor-only samplers under comparable computation, when using a proper number of corrector steps.








\subsection{Probability flow and connection to neural ODEs}\label{sec:flow}
Score-based models enable another numerical method for solving the reverse-time SDE. For all diffusion processes, there exists a corresponding \emph{deterministic process} whose trajectories share the same marginal probability densities  as the SDE. This deterministic process satisfies an ODE~(more details in \cref{app:prob_flow_derive}):

which can be determined from the SDE once scores are known. We name the ODE in \cref{eqn:flow} the \emph{probability flow ODE}.
When the score function is approximated by the time-dependent score-based model, which is typically a neural network, this is an example of a neural ODE~\citep{chen2018neural}. 



\textbf{Exact likelihood computation}~ Leveraging the connection to neural ODEs, we can compute the density defined by \cref{eqn:flow}  via the instantaneous change of variables formula \citep{chen2018neural}. This allows us to compute the \emph{exact likelihood on any input data} (details in \cref{app:prob_flow_likelihood}). As an example, we report negative log-likelihoods (NLLs) measured in bits/dim on the CIFAR-10 dataset in \cref{tab:bpd}. We compute log-likelihoods on uniformly dequantized data, and only compare to models evaluated in the same way (omitting models evaluated with variational dequantization~\citep{ho2019flow++} or discrete data), except for DDPM (/) whose ELBO values (annotated with *) are reported on discrete data. Main results: (i) For the same DDPM model in \citet{ho2020denoising}, we obtain better bits/dim than ELBO, since our likelihoods are exact; (ii) Using the same architecture, we trained another DDPM model with the continuous objective in \cref{eqn:training} (\ie, DDPM cont.), which further improves the likelihood; (iii) With sub-VP SDEs, we always get higher likelihoods compared to VP SDEs; (iv) With improved architecture (\ie, DDPM++ cont., details in \cref{sec:arch}) and the sub-VP SDE, we can set a new record bits/dim of 2.99 on uniformly dequantized CIFAR-10 even \emph{without maximum likelihood training}.

\begin{table}
\begin{minipage}[t]{0.48\textwidth}
\vspace{-0.3cm}
\begin{table}[H]
\centering
\caption{NLLs and FIDs (ODE) on CIFAR-10. }\label{tab:bpd}\vspace{-1em}
\footnotesize
 \setlength\tabcolsep{3.5pt}
\begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{l c c}
    \toprule
    Model & NLL Test  & FID \\
    \midrule
    RealNVP~\citep{dinh2016density} & 3.49 & -\\
    iResNet~\citep{behrmann2019invertible} & 3.45 & -\\
    Glow~\citep{kingma2018glow} & 3.35 & - \\
    MintNet~\citep{song2019mintnet} & 3.32 & - \\
    Residual Flow~\citep{chen2019residual} & 3.28 & 46.37\\
    FFJORD~\citep{grathwohl2018ffjord} & 3.40 & -\\
    Flow++~\citep{ho2019flow++} & 3.29 & -\\
    DDPM ()~\citep{ho2020denoising} &  3.70\textsuperscript{*} & 13.51\\
    DDPM ()~\citep{ho2020denoising} &  3.75\textsuperscript{*} & 3.17\\
    \midrule
    DDPM & 3.28 & 3.37\\\relax
    DDPM cont. (VP) & 3.21 & 3.69\\\relax
    DDPM cont. (sub-VP) & 3.05 & 3.56\\\relax
    DDPM++ cont. (VP) & 3.16 & 3.93\\\relax
    DDPM++ cont. (sub-VP) & 3.02 & 3.16\\ \relax
    DDPM++ cont. (deep, VP) & 3.13 & 3.08\\ \relax
    DDPM++ cont. (deep, sub-VP) & \textbf{2.99} & \textbf{2.92} \\
    \bottomrule
    \end{tabular}
\end{adjustbox}
\end{table}
\end{minipage}\hfil
\begin{minipage}[t]{0.48\textwidth}
\vspace{-0.3cm}
\begin{table}[H]
\centering
\caption{CIFAR-10 sample quality.}\label{tab:fid}\vspace{-1em}
\footnotesize
 \setlength\tabcolsep{3.5pt}
\begin{adjustbox}{max width=\textwidth}
  \begin{tabular}{lcc}
    \toprule
    Model & FID & IS  \\
        \midrule
      {\bf Conditional} & & \\
      \midrule
      BigGAN~\citep{brock2018large} & 14.73 & 9.22 \\
      StyleGAN2-ADA~\citep{karras2020training} & \textbf{2.42} & \textbf{10.14} \\
      \midrule
      {\bf Unconditional} & & \\
      \midrule
      StyleGAN2-ADA~\citep{karras2020training} & 2.92 & 9.83 \\
      NCSN~\citep{song2019generative} & 25.32 & 8.87  .12 \\
      NCSNv2~\citep{song2020improved} & 10.87 & 8.40  .07\\
      DDPM~\citep{ho2020denoising} & 3.17 & 9.46  .11\\
      \midrule
      DDPM++ & 2.78 & 9.64\\
      DDPM++ cont. (VP) & 2.55 & 9.58 \\
      DDPM++ cont. (sub-VP) & 2.61 & 9.56 \\
      DDPM++ cont. (deep, VP) & 2.41 & 9.68\\
      DDPM++ cont. (deep, sub-VP) & 2.41 & 9.57\\
      NCSN++ & 2.45 & 9.73 \\
      NCSN++ cont. (VE) & 2.38 & 9.83\\
      NCSN++ cont. (deep, VE) & \textbf{2.20} & \textbf{9.89}\\
    \bottomrule 
    \end{tabular}
\end{adjustbox}
\end{table}
\end{minipage}
\end{table}

\textbf{Manipulating latent representations}~ By integrating \cref{eqn:flow}, we can encode any datapoint  into a latent space . Decoding can be achieved by integrating a corresponding ODE for the reverse-time SDE. As is done with other invertible models such as neural ODEs and normalizing flows~\citep{dinh2016density,kingma2018glow}, we can manipulate this latent representation for image editing, such as interpolation, and temperature scaling (see \cref{fig:prob_flow} and \cref{app:flow}). 

\textbf{Uniquely identifiable encoding}~ Unlike most current invertible models, 
our encoding is {\em uniquely identifiable}, meaning that with sufficient training data, model capacity, and optimization accuracy, the encoding for an input is uniquely determined by the data distribution \citep{roeder2020linear}. This is because our forward SDE, \cref{eqn:forward_sde}, has no trainable parameters, and its associated probability flow ODE, \cref{eqn:flow}, provides the same trajectories given perfectly estimated scores. We provide additional empirical verification on this property in \cref{app:identifiability}.

\textbf{Efficient sampling}~ As with neural ODEs, we can sample  by solving \cref{eqn:flow} from different final conditions . Using a fixed discretization strategy we can generate competitive samples, especially when used in conjuction with correctors (\cref{tab:compare_samplers},  ``probability flow sampler'', details in \cref{app:prob_flow_sampling}). Using a black-box ODE solver~\citep{dormand1980family} not only produces high quality samples (\cref{tab:bpd}, details in \cref{app:flow}), but also allows us to explicitly trade-off accuracy for efficiency. With a larger error tolerance, the number of function evaluations can be reduced by over  without affecting the visual quality of samples (\cref{fig:prob_flow}).
\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/ode_tight}
    \vspace{-5mm}
    \caption{{\bf Probability flow ODE enables fast sampling} with adaptive stepsizes as the numerical precision is varied (\textit{left}), and reduces the number of score function evaluations (NFE) without harming quality (\textit{middle}). The invertible mapping from latents to images allows for interpolations (\textit{right}).}
    \label{fig:prob_flow}
\end{figure}



\subsection{Architecture improvements}\label{sec:arch}



We explore several new architecture designs for score-based models using both VE and VP SDEs (details in \cref{app:arch_search}), where we train models with the same discrete objectives as in SMLD/DDPM. We directly transfer the architectures for VP SDEs to sub-VP SDEs due to their similarity. Our optimal architecture for the VE SDE, named NCSN++, achieves an FID of 2.45 on CIFAR-10 with PC samplers, while our optimal architecture for the VP SDE, called DDPM++, achieves 2.78.

By switching to the continuous training objective in \cref{eqn:training}, and increasing the network depth, we can further improve sample quality for all models. The resulting architectures are denoted as NCSN++ cont. and DDPM++ cont. in \cref{tab:fid} for VE and VP/sub-VP SDEs respectively. Results reported in \cref{tab:fid} are for the checkpoint with the smallest FID over the course of training, where samples are generated with PC samplers. In contrast, FID scores and NLL values in \cref{tab:bpd} are reported for the last training checkpoint, and samples are obtained with black-box ODE solvers. As shown in \cref{tab:fid}, VE SDEs typically provide better sample quality than VP/sub-VP SDEs, but we also empirically observe that their likelihoods are worse than VP/sub-VP SDE counterparts. This indicates that practitioners likely need to experiment with different SDEs for varying domains and architectures.

Our best model for sample quality, NCSN++ cont. (deep, VE), doubles the network depth and sets new records for both inception score and FID on unconditional generation for CIFAR-10. Surprisingly, we can achieve better FID than the previous best conditional generative model without requiring labeled data. With all improvements together, we also obtain the first set of high-fidelity samples on CelebA-HQ  from score-based models (see \cref{app:hq}). Our best model for likelihoods, DDPM++ cont. (deep, sub-VP), similarly doubles the network depth and achieves a log-likelihood of 2.99 bits/dim with the continuous objective in \cref{eqn:training}. To our best knowledge, this is the highest likelihood on uniformly dequantized CIFAR-10.

 \section{Controllable generation}
The continuous structure of our framework allows us to not only produce data samples from , but also from  if  is known. Given a forward SDE as in \cref{eqn:forward_sde}, we can sample from  by starting from  and solving a conditional reverse-time SDE:

In general, we can use \cref{eqn:cond_sde} to solve a large family of \emph{inverse problems} with score-based generative models, once given an estimate of the gradient of the forward process, . In some cases, it is possible to train a separate model to learn the forward process  and compute its gradient. Otherwise, we may estimate the gradient with heuristics and domain knowledge. In \cref{app:inverse_prob}, we provide a broadly applicable method for obtaining such an estimate without the need of training auxiliary models.

We consider three applications of controllable generation with this approach: class-conditional generation, image imputation and colorization. When  represents class labels, we can train a time-dependent classifier  for class-conditional sampling. Since the forward SDE is tractable, we can easily create training data  for the time-dependent classifier
by first sampling  from a dataset, and then sampling . Afterwards, we may employ a mixture of cross-entropy losses over different time steps, like \cref{eqn:training}, to train the time-dependent classifier . We provide class-conditional CIFAR-10 samples in \cref{fig:cond_gen} (left), and relegate more details and results to \cref{app:cond_gen}.

Imputation is a special case of conditional sampling.
Suppose we have an incomplete data point  where only some subset,  is known. Imputation amounts to sampling from , which we can accomplish using an unconditional model (see \cref{app:imputation}). Colorization is a special case of imputation, except that the known data dimensions are coupled. We can decouple these data dimensions with an orthogonal linear transformation, and perform imputation in the transformed space (details in \cref{app:colorization}). \cref{fig:cond_gen} (right) shows results for inpainting and colorization achieved with unconditional time-dependent score-based models.


\begin{figure}
    \centering
    \includegraphics[width=0.327\linewidth, trim=5 0 7 0, clip, valign=c]{figures/cond_gen_line.png}
    \includegraphics[width=0.66\linewidth, valign=c]{figures/inpaint_colorization.jpg}
    \caption{\textit{Left}: Class-conditional samples on  CIFAR-10. Top four rows are automobiles and bottom four rows are horses. \textit{Right:} Inpainting (top two rows) and colorization (bottom two rows) results on  LSUN. First column is the original image, second column is the masked/gray-scale image, remaining columns are sampled image completions or colorizations.}
    \label{fig:cond_gen}
\end{figure}
 \section{Conclusion}
We presented a framework for score-based generative modeling based on SDEs. Our work enables a better understanding of existing approaches,  new sampling algorithms, exact likelihood computation, uniquely identifiable encoding, latent code manipulation, and brings new conditional generation abilities to the family of score-based generative models.

While our proposed sampling approaches improve results and enable more efficient sampling, they remain slower at sampling than GANs~\citep{goodfellow2014generative} on the same datasets. Identifying ways of combining the stable learning of score-based generative models with the fast sampling of implicit models like GANs remains an important research direction. Additionally, the breadth of samplers one can use when given access to score functions introduces a number of hyper-parameters. Future work would benefit from improved methods to automatically select and tune these hyper-parameters, as well as more extensive investigation on the merits and limitations of various samplers.

\subsubsection*{Acknowledgements}
We would like to thank Nanxin Chen, Ruiqi Gao, Jonathan Ho, Kevin Murphy, Tim Salimans and Han Zhang for their insightful discussions during the course of this project. This research was partially supported by NSF (\#1651565, \#1522054, \#1733686),
ONR (N00014\-19\-1\-2145), AFOSR (FA9550\-19\-1\-0024), and TensorFlow Research Cloud. Yang Song was partially supported by the Apple PhD Fellowship in AI/ML. 
\bibliography{ncsn3}
\bibliographystyle{iclr2021_conference}
\appendix
\newpage
\section*{Appendix}
We include several appendices with additional details, derivations, and results.
Our framework allows general SDEs with matrix-valued diffusion coefficients that depend on the state, for which we provide a detailed discussion in \cref{app:general_sde}. We give a full derivation of VE, VP and sub-VP SDEs in \cref{app:sde_derive}, and discuss how to use them from a practitioner's perspective in \cref{app:wild_sde}. We elaborate on the probability flow formulation of our framework in \cref{app:prob_flow_ode}, including a derivation of the probability flow ODE (\cref{app:prob_flow_derive}), exact likelihood computation (\cref{app:prob_flow_likelihood}), probability flow sampling with a fixed discretization strategy (\cref{app:prob_flow_sampling}), sampling with black-box ODE solvers (\cref{app:flow}), and experimental verification on uniquely identifiable encoding (\cref{app:identifiability}). We give a full description of the reverse diffusion sampler in \cref{app:reverse_diffusion}, the DDPM-type ancestral sampler for SMLD models in \cref{app:ancestral}, and Predictor-Corrector samplers in \cref{app:pc}. We explain our model architectures and detailed experimental settings in \cref{app:arch_search}, with  CelebA-HQ samples therein. Finally, we detail on the algorithms for controllable generation in \cref{app:cond_gen}, and include extended results for class-conditional generation (\cref{app:class_cond_sampling}), image inpainting (\cref{app:imputation}), colorization (\cref{app:colorization}), and a strategy for solving general inverse problems (\cref{app:inverse_prob}).

\section{The framework for more general SDEs}\label{app:general_sde}
In the main text, we introduced our framework based on a simplified SDE~\cref{eqn:forward_sde} where the diffusion coefficient is independent of . It turns out that our framework can be extended to hold for more general diffusion coefficients. We can consider SDEs in the following form:

where  and . We follow the It\^{o} interpretation of SDEs throughout this paper.

According to \citep{Anderson1982-ny}, the reverse-time SDE is given by (\cf, \cref{eqn:backward_sde})

where we define  for a matrix-valued function  throughout the paper. 

The probability flow ODE corresponding to \cref{eqn:forward_sde_2} has the following form (\cf, \cref{eqn:flow}, see a detailed derivation in \cref{app:prob_flow_derive}):

Finally for conditional generation with the general SDE \cref{eqn:forward_sde_2}, we can solve the conditional reverse-time SDE below (\cf, \cref{eqn:cond_sde}, details in \cref{app:cond_gen}):


When the drift and diffusion coefficient of an SDE are not affine, it can be difficult to compute the transition kernel  in closed form. This hinders the training of score-based models, because \cref{eqn:training} requires knowing . To overcome this difficulty, we can replace denoising score matching in \cref{eqn:training} with other efficient variants of score matching that do not require computing . For example, when using sliced score matching~\citep{song2019sliced}, our training objective \cref{eqn:training} becomes

where  is a positive weighting function, , , and . We can always simulate the SDE to sample from , and solve \cref{eqn:training_ssm} to train the time-dependent score-based model .




\section{VE, VP and sub-VP SDEs}\label{app:sde_derive}
Below we provide detailed derivations to show that the noise perturbations of SMLD and DDPM are discretizations of the Variance Exploding (VE) and Variance Preserving (VP) SDEs respectively. We additionally introduce sub-VP SDEs, a modification to VP SDEs that often achieves better performance in both sample quality and likelihoods.

First, when using a total of  noise scales, each perturbation kernel  of SMLD can be derived from the following Markov chain:

where , , and we have introduced  to simplify the notation.
In the limit of , the Markov chain  becomes a continuous stochastic process ,  becomes a function , and  becomes , where we have used a continuous time variable  for indexing, rather than an integer . 
Let , , and  for . We can rewrite \cref{eqn:ncsn} as follows with  and :

where the approximate equality holds when . 
In the limit of , this converges to

which is the VE SDE. 

For the perturbation kernels  used in DDPM, the discrete Markov chain is

where . To obtain the limit of this Markov chain when , we define an auxiliary set of noise scales , and re-write \cref{eqn:ddpm_markov} as below

In the limit of ,  becomes a function  indexed by . Let , , . We can rewrite the Markov chain \cref{eqn:ddpm_markov2} as the following with  and :

where the approximate equality holds when .
Therefore, in the limit of , \cref{eqn:ddpm_discrete} converges to the following VP SDE:


So far, we have demonstrated that the noise perturbations used in SMLD and DDPM correspond to discretizations of VE and VP SDEs respectively. The VE SDE always yields a process with exploding variance when . In contrast, the VP SDE yields a process with bounded variance. In addition, the process has a constant unit variance for all  when  has a unit variance. Since the VP SDE has affine drift and diffusion coefficients, we can use Eq.~(5.51) in \citet{sarkka2019applied} to obtain an ODE that governs the evolution of variance

where  for  obeying a VP SDE. Solving this ODE, we obtain

from which it is clear that the variance  is always bounded given . Moreover,  if . Due to this difference, we name \cref{eqn:ncsn_sde} as the \emph{Variance Exploding (VE) SDE}, and \cref{eqn:ddpm_sde} the \emph{Variance Preserving (VP) SDE}.

Inspired by the VP SDE, we propose a new SDE called the \emph{sub-VP SDE}, namely

Following standard derivations, it is straightforward to show that  is the same for both VP and sub-VP SDEs; the variance function of sub-VP SDEs is different, given by

where  for a process  obtained by solving \cref{eqn:subVP}. In addition, we observe that (i)  for all  with  and shared ; and (ii)  if . The former is why we name \cref{eqn:subVP} the sub-VP SDE---its variance is always upper bounded by the corresponding VP SDE. The latter justifies the use of sub-VP SDEs for score-based generative modeling, since they can perturb any data distribution to standard Gaussian under suitable conditions, just like VP SDEs.


VE, VP and sub-VP SDEs all have affine drift coefficients. Therefore, their perturbation kernels  are all Gaussian and can be computed with Eqs.~(5.50) and (5.51) in \citet{sarkka2019applied}:

As a result, all SDEs introduced here can be efficiently trained with the objective in \cref{eqn:training}.

\section{SDEs in the wild} \label{app:wild_sde}
Below we discuss concrete instantiations of VE and VP SDEs whose discretizations yield SMLD and DDPM models, and the specific sub-VP SDE used in our experiments. In SMLD, the noise scales  is typically a geometric sequence where  is fixed to  and  is chosen according to Technique 1 in \citet{song2020improved}. Usually, SMLD models normalize image inputs to the range . Since  is a geometric sequence, we have  for . In the limit of , we have  for . The corresponding VE SDE is

and the perturbation kernel can be derived via \cref{eqn:perturb_kernel}:

There is one subtlety when : by definition,  (following the convention in \cref{eqn:ncsn}), but . In other words,  for SMLD is not differentiable since , causing the VE SDE in \cref{eqn:ncsn_sde_2} undefined for . In practice, we bypass this issue by always solving the SDE and its associated probability flow ODE in the range  for some small constant , and we use  in our VE SDE experiments. 

For DDPM models,  is typically an arithmetic sequence where  for . Therefore,  for  in the limit of . This corresponds to the following instantiation of the VP SDE:

where . In our experiments, we let  and  to match the settings in \citet{ho2020denoising}. The perturbation kernel is given by

For DDPM, there is no discontinuity issue with the corresponding VP SDE; yet, there are numerical instability issues for training and sampling at , due to the vanishing variance of  as . Therefore, same as the VE SDE, we restrict computation to  for a small . For sampling, we choose  so that the variance of  in VP SDE matches the variance of  in DDPM; for training and likelihood computation, we adopt  which empirically gives better results.

\begin{figure}
    \centering
    \begin{subfigure}{0.333\linewidth}
        \includegraphics[width=\linewidth]{figures/NCSN_var.pdf}\caption{SMLD}
    \end{subfigure}
    \begin{subfigure}{0.32\linewidth}
        \includegraphics[width=\linewidth]{figures/ddpm_mean.pdf}\caption{DDPM (mean)}
    \end{subfigure}
    \begin{subfigure}{0.32\linewidth}
        \includegraphics[width=\linewidth]{figures/ddpm_var.pdf}
        \caption{DDPM (variance)}
    \end{subfigure}
    \caption{Discrete-time perturbation kernels and our continuous generalizations match each other almost exactly. (a) compares the variance of perturbation kernels for SMLD and VE SDE; (b) compares the scaling factors of means of perturbation kernels for DDPM and VP SDE; and (c) compares the variance of perturbation kernels for DDPM and VP SDE.}
    \label{fig:discretization}
\end{figure}
As a sanity check for our SDE generalizations to SMLD and DDPM, we compare the perturbation kernels of SDEs and original discrete Markov chains in \cref{fig:discretization}. The SMLD and DDPM models both use  noise scales. For SMLD, we only need to compare the variances of perturbation kernels since means are the same by definition. For DDPM, we compare the scaling factors of means and the variances. As demonstrated in \cref{fig:discretization}, the discrete perturbation kernels of original SMLD and DDPM models align well with perturbation kernels derived from VE and VP SDEs.

For sub-VP SDEs, we use exactly the same  as VP SDEs. This leads to the following perturbation kernel

We also restrict numerical computation to the same interval of  as VP SDEs.

Empirically, we observe that smaller  generally yields better likelihood values for all SDEs. For sampling, it is important to use an appropriate  for better Inception scores and FIDs, although samples across different  look visually the same to human eyes.

\section{Probability flow ODE}\label{app:prob_flow_ode}
\subsection{Derivation}\label{app:prob_flow_derive}
The idea of probability flow ODE is inspired by \citet{maoutsa2020interacting}, and one can find the derivation of a simplified case therein. Below we provide a derivation for the fully general ODE in \cref{eqn:flow_2}. We consider the SDE in \cref{eqn:forward_sde_2}, which possesses the following form:

where  and . The marginal probability density  evolves according to Kolmogorov's forward equation (Fokker-Planck equation)~\citep{oksendal2003stochastic}

We can easily rewrite \cref{eqn:fokker_planck} to obtain

Note that

based on which we can continue the rewriting of \cref{eqn:ode_part_1} to obtain

where we define

Inspecting \cref{eqn:ode_part_2}, we observe that it equals Kolmogorov's forward equation of the following SDE with  (Kolmogorov's forward equation in this case is also known as the Liouville equation.)

which is essentially an ODE:

same as the probability flow ODE given by \cref{eqn:flow_2}. Therefore, we have shown that the probability flow ODE \cref{eqn:flow_2} induces the same marginal probability density  as the SDE in \cref{eqn:forward_sde_2}.

\subsection{Likelihood computation}\label{app:prob_flow_likelihood}
The probability flow ODE in \cref{eqn:flow_2} has the following form when we replace the score  with the time-dependent score-based model :

With the instantaneous change of variables formula~\citep{chen2018neural}, we can compute the log-likelihood of  using

where the random variable  as a function of  can be obtained by solving the probability flow ODE in \cref{eqn:flow_3}. In many cases computing  is expensive, so we follow \citet{grathwohl2018ffjord} to estimate it with the Skilling-Hutchinson trace estimator~\citep{skilling1989eigenvalues,hutchinson1990stochastic}. In particular, we have

where  denotes the Jacobian of , and the random variable  satisfies  and . The vector-Jacobian product  can be efficiently computed using reverse-mode automatic differentiation, at approximately the same cost as evaluating . As a result, we can sample  and then compute an efficient unbiased estimate to  using . Since this estimator is unbiased, we can attain an arbitrarily small error by averaging over a sufficient number of runs. Therefore, by applying the Skilling-Hutchinson estimator \cref{eqn:hutchinson} to \cref{eqn:likelihood}, we can compute the log-likelihood to any accuracy.

In our experiments, we use the RK45 ODE solver~\citep{dormand1980family} provided by \verb|scipy.integrate.solve_ivp| in all cases. The bits/dim values in \cref{tab:bpd} are computed with \verb|atol=1e-5| and \verb|rtol=1e-5|, same as \citet{grathwohl2018ffjord}. To give the likelihood results of our models in \cref{tab:bpd}, we average the bits/dim obtained on the test dataset over five different runs with  (see definition of  in \cref{app:wild_sde}).

\subsection{Probability flow sampling}\label{app:prob_flow_sampling}
Suppose we have a forward SDE

and one of its discretization

where . We assume the discretization schedule of time is fixed beforehand, and thus we absorb the dependency on  into the notations of  and . Using \cref{eqn:flow_2}, we can obtain the following probability flow ODE:

We may employ any numerical method to integrate the probability flow ODE backwards in time for sample generation. In particular, we propose a discretization in a similar functional form to \cref{eqn:discrete_forward_2}:

where the score-based model  is conditioned on the iteration number . This is a deterministic iteration rule. Unlike reverse diffusion samplers or ancestral sampling, there is no additional randomness once the initial sample  is obtained from the prior distribution. When applied to SMLD models, we can get the following iteration rule for probability flow sampling:

Similarly, for DDPM models, we have


\subsection{Sampling with black-box ODE solvers}\label{app:flow}
For producing figures in \cref{fig:prob_flow}, we use a DDPM model trained on  CelebA-HQ with the same settings in \citet{ho2020denoising}. All FID scores of our models in \cref{tab:bpd} are computed on samples from the RK45 ODE solver implemented in  \verb|scipy.integrate.solve_ivp| with \verb|atol=1e-5| and \verb|rtol=1e-5|. We use  for VE SDEs and  for VP SDEs (see also \cref{app:wild_sde}). 

Aside from the interpolation results in \cref{fig:prob_flow}, we demonstrate more examples of latent space manipulation in \cref{fig:celeba256}, including interpolation and temperature scaling. The model tested here is a DDPM model trained with the same settings in \citet{ho2020denoising}.
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/celeba_interp.png}
    \includegraphics[width=\linewidth]{figures/celeba_temp.png}
    \caption{Samples from the probability flow ODE for VP SDE on  CelebA-HQ. Top: spherical interpolations between random samples. Bottom: temperature rescaling (reducing norm of embedding).}
    \label{fig:celeba256}
\end{figure}

Although solvers for the probability flow ODE allow fast sampling, their samples typically have higher (worse) FID scores than those from SDE solvers if no corrector is used. We have this empirical observation for both the discretization strategy in \cref{app:prob_flow_sampling}, and black-box ODE solvers introduced above. Moreover, the performance of probability flow ODE samplers depends on the choice of the SDE---their sample quality for VE SDEs is much worse than VP SDEs especially for high-dimensional data.

\subsection{Uniquely identifiable encoding}\label{app:identifiability}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/identifiability.pdf}
    \caption{Comparing the first 100 dimensions of the latent code obtained for a random CIFAR-10 image. ``Model A'' and ``Model B'' are separately trained with different architectures.}
    \label{fig:identifiability}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{figures/identifiability_boxplot.pdf}
    \includegraphics[width=0.41\linewidth]{figures/identifiability_cor.pdf}
    \caption{\emph{Left}: The dimension-wise difference between encodings obtained by Model A and B. As a baseline, we also report the difference between shuffled representations of these two models.
    \emph{Right}: The dimension-wise correlation coefficients of encodings obtained by Model A and Model B.}
    \label{fig:identifiability_boxplot}
\end{figure}

As a sanity check, we train two models (denoted as ``Model A'' and ``Model B'') with different architectures using the VE SDE on CIFAR-10. Here Model A is an NCSN++ model with 4 layers per resolution trained using the continuous objective in \cref{eqn:training}, and Model B is all the same except that it uses 8 layers per resolution. Model definitions are in \cref{app:arch_search}. 

We report the latent codes obtained by Model A and Model B for a random CIFAR-10 image in \cref{fig:identifiability}. In \cref{fig:identifiability_boxplot}, we show the dimension-wise differences and correlation coefficients between latent encodings on a total of 16 CIFAR-10 images. Our results demonstrate that for the same inputs, Model A and Model B provide encodings that are close in every dimension, despite having different model architectures and training runs.


\section{Reverse diffusion sampling}\label{app:reverse_diffusion}
Given a forward SDE 

and suppose the following iteration rule is a discretization of it:

where . Here we assume the discretization schedule of time is fixed beforehand, and thus we can absorb it into the notations of  and .

Based on \cref{eqn:discrete_forward}, we propose to discretize the reverse-time SDE

with a similar functional form, which gives the following iteration rule for :

where our trained score-based model  is conditioned on iteration number .

When applying \cref{eqn:discrete_backward} to \cref{eqn:ncsn,eqn:ddpm}, we obtain a new set of numerical solvers for the reverse-time VE and VP SDEs, resulting in sampling algorithms as shown in the ``predictor'' part of \cref{alg:pc_smld,alg:pc_ddpm}. We name these sampling methods (that are based on the discretization strategy in \cref{eqn:discrete_backward}) \emph{reverse diffusion samplers}. 

As expected, the ancestral sampling of DDPM~\citep{ho2020denoising} (\cref{eqn:ddpm_sampling}) matches its reverse diffusion counterpart when  for all  (which happens when  since , see \cref{app:sde_derive}), because

Therefore, the original ancestral sampler of \cref{eqn:ddpm_sampling} is essentially a different discretization to the same reverse-time SDE. This unifies the sampling method in \citet{ho2020denoising} as a numerical solver to the reverse-time VP SDE in our continuous framework. 


\section{Ancestral sampling for SMLD models}\label{app:ancestral}
The ancestral sampling method for DDPM models can also be adapted to SMLD models. Consider a sequence of noise scales  as in SMLD. By perturbing a data point  with these noise scales sequentially, we obtain a Markov chain , where

Here we assume  to simplify notations. Following \citet{ho2020denoising}, we can compute

If we parameterize the reverse transition kernel as , then

where  is one representative term in the ELBO objective (see Eq.~(8) in \citet{ho2020denoising}),  is a constant that does not depend on , , and . We can therefore parameterize  via

where  is to estimate . As in \citet{ho2020denoising}, we let . Through ancestral sampling on , we obtain the following iteration rule

where ,  denotes the optimal parameter of , and . We call \cref{eqn:ncsn_ancestral} the ancestral sampling method for SMLD models.


\section{Predictor-Corrector samplers}\label{app:pc}

\begin{algorithm}[!t]
   \caption{Predictor-Corrector (PC) sampling}
   \label{alg:pc}
    \begin{algorithmic}[1]
        \Require    
            \Statex : Number of discretization steps for the reverse-time SDE
            \Statex : Number of corrector steps
       \State{Initialize }
       \For{ {\bfseries to} }
        \State 
         \For{ {\bfseries to} }
            \State 
         \EndFor
       \EndFor
       \State {\bfseries return} 
    \end{algorithmic}
    \end{algorithm}

\paragraph{Predictor-Corrector (PC) sampling} 
The predictor can be any numerical solver for the reverse-time SDE with a fixed discretization strategy. The corrector can be any score-based MCMC approach. In PC sampling, we alternate between the predictor and corrector, as described in \cref{alg:pc}. For example, when using the reverse diffusion SDE solver (\cref{app:reverse_diffusion}) as the predictor, and annealed Langevin dynamics~\citep{song2019generative} as the corrector, we have \cref{alg:pc_smld,alg:pc_ddpm} for VE and VP SDEs respectively, where  are step sizes for Langevin dynamics as specified below.

\algnewcommand{\LineComment}[1]{\Statex  #1}
\algrenewcommand\algorithmicindent{0.7em}\begin{figure}
\begin{minipage}{.49\textwidth}
    \vspace{-0.5cm}
    \begin{algorithm}[H]
            \small
           \caption{PC sampling (VE SDE)}
           \label{alg:pc_smld}
            \begin{algorithmic}[1]
               \State 
               \For{ {\bfseries to} }
                \begin{tikzpicture}[remember picture,overlay]
                    \node[xshift=3.5cm,yshift=-1.32cm] at (0,0){\includegraphics[width=2.05\textwidth]{figures/pc_boxes2.png}};
                \end{tikzpicture}
                 \State{}
                 \State{}
                 \State{}
                 \For{ {\bfseries to} }
                    \State{}
                    \State{}
                 \EndFor
               \EndFor
               \State {\bfseries return} 
            \end{algorithmic}
    \end{algorithm}
\end{minipage}
\begin{minipage}{.49\textwidth}
    \vspace{-0.5cm}
    \begin{algorithm}[H]
          \small
           \caption{PC sampling (VP SDE)}
           \label{alg:pc_ddpm}
            \begin{algorithmic}[1]
               \State 
               \For{ {\bfseries to} }
               \vspace{0.35em}
                 \State{\resizebox{\linewidth}{!}{}}\vspace{0.37em}
                 \State{}\vspace{0.17em}
                 \State{}
                 \For{ {\bfseries to} }
                    \State{}
                    \State{}
                 \EndFor
               \EndFor
               \State {\bfseries return} 
            \end{algorithmic}
    \end{algorithm}
\end{minipage}
\end{figure}

\paragraph{The corrector algorithms} We take the schedule of annealed Langevin dynamics in \citet{song2019generative}, but re-frame it with slight modifications in order to get better interpretability and empirical performance. We provide the corrector algorithms in \cref{alg:corrector_ve,alg:corrector_vp} respectively, where we call  the ``signal-to-noise'' ratio. We determine the step size  using the norm of the Gaussian noise , norm of the score-based model  and the signal-to-noise ratio . When sampling a large batch of samples together, we replace the norm  with the average norm across the mini-batch. When the batch size is small, we suggest replacing  with , where  is the dimensionality of . 



\begin{minipage}[t]{0.49\linewidth}
\begin{algorithm}[H]
	\caption{Corrector algorithm (VE SDE).}
	\label{alg:corrector_ve}
	\begin{algorithmic}[1]
	    \Require{.}
	    \State{}
	    \For{ to }
            \For{ to }
                \State{}
                \State{}
                \State{}
                \State{}
            \EndFor
            \State{}
        \EndFor
        \item[]
        \Return{}
	\end{algorithmic}
\end{algorithm}
\end{minipage}
\begin{minipage}[t]{0.49\linewidth}
\begin{algorithm}[H]
	\caption{Corrector algorithm (VP SDE).}
	\label{alg:corrector_vp}
	\begin{algorithmic}[1]
	    \Require{.}
	    \State{}
	    \For{ to }
            \For{ to }
                \State{}
                \State{}
                \State{}
                \State{}
            \EndFor
            \State{}
        \EndFor
        \item[]
        \Return{}
	\end{algorithmic}
\end{algorithm}
\end{minipage}


\paragraph{Denoising} For both SMLD and DDPM models, the generated samples typically contain small noise that is hard to detect by humans. As noted by \citet{jolicoeur2020adversarial}, FIDs can be significantly worse without removing this noise. This unfortunate sensitivity to noise is also part of the reason why NCSN models trained with SMLD has been performing worse than DDPM models in terms of FID, because the former does not use a denoising step at the end of sampling, while the latter does. In all experiments of this paper we ensure there is a single denoising step at the end of sampling, using Tweedie's formula~\citep{efron2011tweedie}.


\begin{figure}[h]
    \centering
    \includegraphics[width=0.45\linewidth]{figures/computation_bedroom_compressed.pdf}
    \includegraphics[width=0.451\linewidth]{figures/computation_church_compressed.pdf}
    \caption{PC sampling for LSUN bedroom and church. The vertical axis corresponds to the total computation, and the horizontal axis represents the amount of computation allocated to the corrector. Samples are the best when computation is split between the predictor and corrector.}
    \label{fig:computation}
\end{figure}




\paragraph{Training} 
We use the same architecture in \cite{ho2020denoising} for our score-based models. For the VE SDE, we train a model with the original SMLD objective in \cref{eqn:ncsn_obj}; similarly for the VP SDE, we use the original DDPM objective in \cref{eqn:ddpm_obj}. We apply a total number of 1000 noise scales for training both models. For results in \cref{fig:computation}, we train an NCSN++ model (definition in \cref{app:arch_search}) on  LSUN bedroom and church\_outdoor~\citep{yu2015lsun} datasets with the VE SDE and our continuous objective \cref{eqn:training}. The batch size is fixed to 128 on CIFAR-10 and 64 on LSUN.

\paragraph{Ad-hoc interpolation methods for noise scales} 
Models in this experiment are all trained with 1000 noise scales. To get results for P2000 (predictor-only sampler using 2000 steps) which requires 2000 noise scales, we need to interpolate between 1000 noise scales at test time. The specific architecture of the noise-conditional score-based model in \citet{ho2020denoising} uses sinusoidal positional embeddings for conditioning on integer time steps. This allows us to interpolate between noise scales at test time in an ad-hoc way (while it is hard to do so for other architectures like the one in \citet{song2019generative}). Specifically, for SMLD models, we keep  and  fixed and double the number of time steps. For DDPM models, we halve  and  before doubling the number of time steps. Suppose  is a score-based model trained on  time steps, and let  denote the corresponding interpolated score-based model at  time steps. We test two different interpolation strategies for time steps: linear interpolation where  and rounding interpolation where . We provide results with linear interpolation in \cref{tab:compare_samplers}, and give results of rounding interpolation in \cref{tab:compare_samplers_diff_interpolate}. We observe that different interpolation methods result in performance differences but maintain the general trend of predictor-corrector methods performing on par or better than predictor-only or corrector-only samplers.

\paragraph{Hyper-parameters of the samplers}
For Predictor-Corrector and corrector-only samplers on CIFAR-10, we search for the best signal-to-noise ratio () over a grid that increments at 0.01. We report the best  in \cref{tab:compare_samplers_snr}. For LSUN bedroom/church\_outdoor, we fix  to 0.075. Unless otherwise noted, we use one corrector step per noise scale for all PC samplers. We use two corrector steps per noise scale for corrector-only samplers on CIFAR-10. For sample generation, the batch size is 1024 on CIFAR-10 and 8 on LSUN bedroom/church\_outdoor. 

\begin{table}
    \definecolor{h}{gray}{0.9}
	\caption{Comparing different samplers on CIFAR-10, where ``P2000'' uses the rounding interpolation between noise scales. Shaded regions are obtained with the same computation (number of score function evaluations). Mean and standard deviation are reported over five sampling runs.}\label{tab:compare_samplers_diff_interpolate}
	\centering
	\begin{adjustbox}{max width=\linewidth}
		\begin{tabular}{c|c|c|c|c|c|c|c|c}
			\Xhline{3\arrayrulewidth} \bigstrut
			  & \multicolumn{4}{c|}{Variance Exploding SDE (SMLD)} & \multicolumn{4}{c}{Variance Preserving SDE (DDPM)}\\
			 \Xhline{1\arrayrulewidth}\bigstrut
			\diagbox[height=1cm, width=3cm]{Predictor}{FID}{Sampler} & P1000 & \cellcolor{h}P2000 & \cellcolor{h}C2000 & \cellcolor{h}PC1000 & P1000 & \cellcolor{h}P2000 & \cellcolor{h}C2000 & \cellcolor{h}PC1000  \\
			\Xhline{1\arrayrulewidth}\bigstrut
            ancestral sampling & 4.98\scalebox{0.7}{  .06}	& \cellcolor{h}4.92\scalebox{0.7}{  .02} &\cellcolor{h} & \cellcolor{h}\textbf{3.62\scalebox{0.7}{  .03}} & 3.24\scalebox{0.7}{  .02}	& \cellcolor{h}\textbf{3.11\scalebox{0.7}{  .03}} &\cellcolor{h} & \cellcolor{h}3.21\scalebox{0.7}{  .02}\\
        	reverse diffusion & 4.79\scalebox{0.7}{  .07} & \cellcolor{h}4.72\scalebox{0.7}{  .07} & \cellcolor{h} & \cellcolor{h}\textbf{3.60\scalebox{0.7}{  .02}} & 3.21\scalebox{0.7}{  .02} & \cellcolor{h}\textbf{3.10\scalebox{0.7}{  .03}} & \cellcolor{h} &\cellcolor{h}3.18\scalebox{0.7}{  .01}\\
            probability flow &	15.41\scalebox{0.7}{  .15} &\cellcolor{h}12.87\scalebox{0.7}{  .09}&\cellcolor{h} \multirow{-3}{*}{20.43\scalebox{0.7}{  .07}} & \cellcolor{h}\textbf{3.51\scalebox{0.7}{  .04}} & 3.59\scalebox{0.7}{  .04} & \cellcolor{h}3.25\scalebox{0.7}{  .04} & \cellcolor{h}\multirow{-3}{*}{19.06\scalebox{0.7}{  .06}} & \cellcolor{h}\textbf{3.06\scalebox{0.7}{  .03}}\\
			\Xhline{3\arrayrulewidth}
		\end{tabular}
	\end{adjustbox}
\end{table}

\begin{table}
    \definecolor{h}{gray}{0.9}
	\caption{Optimal signal-to-noise ratios of different samplers. ``P1000'' or ``P2000'': predictor-only samplers using 1000 or 2000 steps. ``C2000'': corrector-only samplers using 2000 steps. ``PC1000'': PC samplers using 1000 predictor and 1000 corrector steps.}\label{tab:compare_samplers_snr}
	\centering
	\begin{adjustbox}{max width=\linewidth}
		\begin{tabular}{c|c|c|c|c|c|c|c|c}
			\Xhline{3\arrayrulewidth} \bigstrut
			  & \multicolumn{4}{c|}{VE SDE (SMLD)} & \multicolumn{4}{c}{VP SDE (DDPM)}\\
			 \Xhline{1\arrayrulewidth}\bigstrut
			\diagbox[height=1cm, width=3cm]{Predictor}{}{Sampler} & P1000 & P2000 & C2000 & PC1000 & P1000 & P2000 & C2000 & PC1000  \\
			\Xhline{1\arrayrulewidth}\bigstrut
            ancestral sampling & -	& - & & 0.17 & -	& - & & 0.01\\
        	reverse diffusion & - & - &  & 0.16 & - & - &  & 0.01\\
            probability flow &	- &-& \multirow{-3}{*}{0.22} & 0.17 & - & - & \multirow{-3}{*}{0.27} & 0.04\\
			\Xhline{3\arrayrulewidth}
		\end{tabular}
	\end{adjustbox}
\end{table}

\section{Architecture improvements}\label{app:arch_search}
We explored several architecture designs to improve score-based models for both VE and VP SDEs. Our endeavor gives rise to new state-of-the-art sample quality on CIFAR-10, new state-of-the-art likelihood on uniformly dequantized CIFAR-10, and enables the first high-fidelity image samples of resolution  from score-based generative models. Code and checkpoints are open-sourced at \href{https://github.com/yang-song/score\_sde}{https://github.com/yang-song/score\_sde}.

\subsection{Settings for architecture exploration} Unless otherwise noted, all models are trained for 1.3M iterations, and we save one checkpoint per 50k iterations. For VE SDEs, we consider two datasets:  CIFAR-10~\citep{krizhevsky2009learning} and  CelebA~\citep{liu2015faceattributes}, pre-processed following \citet{song2020improved}. We compare different configurations based on their FID scores averaged over checkpoints after 0.5M iterations. For VP SDEs, we only consider the CIFAR-10 dataset to save computation, and compare models based on the average FID scores over checkpoints obtained between 0.25M and 0.5M iterations, because FIDs turn to increase after 0.5M iterations for VP SDEs.

All FIDs are computed on 50k samples with \href{https://github.com/tensorflow/gan}{\texttt{tensorflow\_gan}}. For sampling, we use the PC sampler discretized at 1000 time steps. We choose reverse diffusion (see \cref{app:reverse_diffusion}) as the predictor. We use one corrector step per update of the predictor for VE SDEs with a signal-to-noise ratio of 0.16, but save the corrector step for VP SDEs since correctors there only give slightly better results but require double computation. We follow \citet{ho2020denoising} for optimization, including the learning rate, gradient clipping, and learning rate warm-up schedules. Unless otherwise noted, models are trained with the original discrete SMLD and DDPM objectives in~\cref{eqn:ncsn_obj,eqn:ddpm_obj} and use a batch size of 128. The optimal architectures found under these settings are subsequently transferred to continuous objectives and deeper models. We also directly transfer the best architecture for VP SDEs to sub-VP SDEs, given the similarity of these two SDEs.
\begin{figure}[H]
    \centering
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/fir.pdf}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/skip_rescale.pdf}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/resblock_type.pdf}
    \end{subfigure}\\
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/num_res_blocks.pdf}
    \end{subfigure}
    \begin{subfigure}{0.58\textwidth}
        \includegraphics[width=\textwidth]{figures/prog_arch.pdf}
    \end{subfigure}
    \caption{The effects of different architecture components for score-based models trained with VE perturbations.}
    \label{fig:arch_search}
\end{figure}
Our architecture is mostly based on \citet{ho2020denoising}. We additionally introduce the following components to maximize the potential improvement of score-based models.
\begin{enumerate}
    \item Upsampling and downsampling images with anti-aliasing based on Finite Impulse Response (FIR)~\citep{zhang2019shiftinvar}. We follow the same implementation and hyper-parameters in StyleGAN-2~\citep{Karras2019stylegan2}.
    \item Rescaling all skip connections by . This has been demonstrated effective in several best-in-class GAN models, including ProgressiveGAN~\citep{karras2018progressive}, StyleGAN~\citep{karras2019style} and StyleGAN-2~\citep{Karras2019stylegan2}.
    \item Replacing the original residual blocks in DDPM with residual blocks from BigGAN~\citep{brock2018large}.
    \item Increasing the number of residual blocks per resolution from  to .
    \item Incorporating progressive growing architectures. We consider two progressive architectures for input: ``input skip'' and ``residual'', and two progressive architectures for output: ``output skip'' and ``residual''. These progressive architectures are defined and implemented according to StyleGAN-2.
\end{enumerate}

We also tested equalized learning rates, a trick used in very successful models like ProgressiveGAN~\citep{karras2018progressive} and StyleGAN~\citep{karras2019style}. However, we found it harmful at an early stage of our experiments, and therefore decided not to explore more on it.

The exponential moving average (EMA) rate has a significant impact on performance. For models trained with VE perturbations, we notice that 0.999 works better than 0.9999, whereas for models trained with VP perturbations it is the opposite. We therefore use an EMA rate of 0.999 and 0.9999 for VE and VP models respectively.


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/cifar_samples.jpg}
    \caption{Unconditional CIFAR-10 samples from NCSN++ cont. (deep, VE).}
    \label{fig:cifar_samples}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/combine.jpg}
    \caption{Samples on  CelebA-HQ from a modified NCSN++ model trained with the VE SDE.}
    \label{fig:celebahq}
\end{figure}

\subsection{Results on CIFAR-10}\label{app:rescifar10} 

All architecture components introduced above can improve the performance of score-based models trained with VE SDEs, as shown in \cref{fig:arch_search}. The box plots demonstrate the importance of each component when other components can vary freely. On both CIFAR-10 and CelebA, the additional components that we explored always improve the performance on average for VE SDEs. For progressive growing, it is not clear which combination of configurations consistently performs the best, but the results are typically better than when no progressive growing architecture is used. Our best score-based model for VE SDEs 1) uses FIR upsampling/downsampling, 2) rescales skip connections, 3) employs BigGAN-type residual blocks, 4) uses 4 residual blocks per resolution instead of 2, and 5) uses ``residual'' for input and no progressive growing architecture for output. We name this model ``NCSN++'', following the naming convention of previous SMLD models~\citep{song2019generative,song2020improved}. 

We followed a similar procedure to examine these architecture components for VP SDEs, except that we skipped experiments on CelebA due to limited computing resources. The NCSN++ architecture worked decently well for VP SDEs, ranked 4th place over all 144 possible configurations. The top configuration, however, has a slightly different structure, which uses no FIR upsampling/downsampling and no progressive growing architecture compared to NCSN++. We name this model ``DDPM++'', following the naming convention of~\citet{ho2020denoising}.


The basic NCSN++ model with 4 residual blocks per resolution achieves an FID of 2.45 on CIFAR-10, whereas the basic DDPM++ model achieves an FID of 2.78. Here in order to match the convention used in \citet{karras2018progressive,song2019generative} and \citet{ho2020denoising}, we report the lowest FID value over the course of training, rather than the average FID value over checkpoints after 0.5M iterations (used for comparing different models of VE SDEs) or between 0.25M and 0.5M iterations (used for comparing VP SDE models) in our architecture exploration. 

Switching from discrete training objectives to continuous ones in \cref{eqn:training} further improves the FID values for all SDEs. To condition the NCSN++ model on continuous time variables, we change positional embeddings, the layers in \citet{ho2020denoising} for conditioning on discrete time steps, to random Fourier feature embeddings~\citep{tancik2020fourfeat}. The scale parameter of these random Fourier feature embeddings is fixed to 16. We also reduce the number of training iterations to 0.95M to suppress overfitting. These changes improve the FID on CIFAR-10 from 2.45 to 2.38 for NCSN++ trained with the VE SDE, resulting in a model called ``NCSN++ cont.''. In addition, we can further improve the FID from 2.38 to 2.20 by doubling the number of residual blocks per resolution for NCSN++ cont., resulting in the model denoted as ``NCSN++ cont. (deep)''. All quantitative results are summarized in \cref{tab:fid}, and we provide random samples from our best model in \cref{fig:cifar_samples}. 

Similarly, we can also condition the DDPM++ model on continuous time steps, resulting in a model ``DDPM++ cont.''. When trained with the VP SDE, it improves the FID of 2.78 from DDPM++ to 2.55. When trained with the sub-VP SDE, it achieves an FID of 2.61. To get better performance, we used the Euler-Maruyama solver as the predictor for continuously-trained models, instead of the ancestral sampling predictor or the reverse diffusion predictor. This is because the discretization strategy of the original DDPM method does not match the variance of the continuous process well when , which significantly hurts FID scores. As shown in \cref{tab:bpd}, the likelihood values are 3.21 and 3.05 bits/dim for VP and sub-VP SDEs respectively. Doubling the depth, and trainin with 0.95M iterations, we can improve both FID and bits/dim for both VP and sub-VP SDEs, leading to a model ``DDPM++ cont. (deep)''. Its FID score is 2.41, same for both VP and sub-VP SDEs. When trained with the sub-VP SDE, it can achieve a likelihood of 2.99 bits/dim. Here all likelihood values are reported for the last checkpoint during training.



\subsection{High resolution images} 
\label{app:hq}

Encouraged by the success of NCSN++ on CIFAR-10, we proceed to test it on  CelebA-HQ~\citep{karras2018progressive}, a task that was previously only achievable by some GAN models and VQ-VAE-2~\citep{razavi2019generating}. We used a batch size of 8, increased the EMA rate to 0.9999, and trained a model similar to NCSN++ with the continuous objective (\cref{eqn:training}) for around 2.4M iterations (please find the detailed architecture in our code release.) We use the PC sampler discretized at 2000 steps with the reverse diffusion predictor, one Langevin step per predictor update and a signal-to-noise ratio of 0.15. The scale parameter for the random Fourier feature embeddings is fixed to 16. We use the ``input skip'' progressive architecture for the input, and ``output skip'' progressive architecture for the output. We provide samples in \cref{fig:celebahq}. Although these samples are not perfect (\eg, there are visible flaws on facial symmetry), we believe these results are encouraging and can demonstrate the scalability of our approach. Future work on more effective architectures are likely to significantly advance the performance of score-based generative models on this task.


\section{Controllable generation}\label{app:cond_gen}
Consider a forward SDE with the following general form

and suppose the initial state distribution is . The density at time  is  when conditioned on . Therefore, using \citet{Anderson1982-ny}, the reverse-time SDE is given by

Since , the score  can be computed easily by

This subsumes the conditional reverse-time SDE in \cref{eqn:cond_sde} as a special case. All sampling methods we have discussed so far can be applied to the conditional reverse-time SDE for sample generation.

\subsection{Class-conditional sampling}\label{app:class_cond_sampling}
When  represents class labels, we can train a time-dependent classifier  for class-conditional sampling. Since the forward SDE is tractable, we can easily create a pair of training data  by first sampling  from a dataset and then obtaining . Afterwards, we may employ a mixture of cross-entropy losses over different time steps, like \cref{eqn:training}, to train the time-dependent classifier .

To test this idea, we trained a Wide ResNet~\citep{zagoruyko2016wide} (\verb|Wide-ResNet-28-10|) on CIFAR-10 with VE perturbations. The classifier is conditioned on  using random Fourier features~\citep{tancik2020fourfeat}, and the training objective is a simple sum of cross-entropy losses sampled at different scales. We provide a plot to show the accuracy of this classifier over noise scales in \cref{fig:cond_sample_extend}. The score-based model is an unconditional NCSN++ (4 blocks/resolution) in \cref{tab:fid}, and we generate samples using the PC algorithm with 2000 discretization steps. The class-conditional samples are provided in \cref{fig:cond_gen}, and an extended set of conditional samples is given in \cref{fig:cond_sample_extend}.
\begin{figure}
    \centering
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/conditional_shallow_0.png}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/conditional_shallow_1.png}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/conditional_shallow_2.png}
    \end{subfigure}\\
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/conditional_shallow_3.png}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/conditional_shallow_4.png}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/conditional_shallow_5.png}
    \end{subfigure}\\
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/conditional_shallow_6.png}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/conditional_shallow_7.png}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/conditional_shallow_8.png}
    \end{subfigure}\\
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/conditional_shallow_9.png}
    \end{subfigure}
    \begin{subfigure}{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/accuracy_curve.pdf}
    \end{subfigure}
    \caption{Class-conditional image generation by solving the conditional reverse-time SDE with PC. The curve shows the accuracy of our noise-conditional classifier over different noise scales.}
    \label{fig:cond_sample_extend}
\end{figure}

\subsection{Imputation} \label{app:imputation}
Imputation is a special case of conditional sampling. Denote by  and  the known and unknown dimensions of  respectively, and let  and  denote  and  restricted to the unknown dimensions. For VE/VP SDEs, the drift coefficient  is element-wise, and the diffusion coefficient  is diagonal. When  is element-wise,  denotes the same element-wise function applied only to the unknown dimensions. When  is diagonal,  denotes the sub-matrix restricted to unknown dimensions. 

For imputation, our goal is to sample from . Define a new diffusion process , and note that the SDE for  can be written as

The reverse-time SDE, conditioned on , is given by

Although  is in general intractable, it can be approximated. Let  denote the event . We have

where  is a random sample from , which is typically a tractable distribution. Therefore,

where  denotes a vector  such that  and , and the identity holds because .

We provided an extended set of inpainting results in \cref{fig:bedroom_inpainting,fig:church_inpainting}.

\subsection{Colorization}\label{app:colorization}
Colorization is a special case of imputation, except that the known data dimensions are coupled. We can decouple these data dimensions by using an orthogonal linear transformation to map the gray-scale image to a separate channel in a different space, and then perform imputation to complete the other channels before transforming everything back to the original image space. The orthogonal matrix we used to decouple color channels is

Because the transformations are all orthogonal matrices, the standard Wiener process  will still be a standard Wiener process in the transformed space, allowing us to build an SDE and use the same imputation method in \cref{app:imputation}. We provide an extended set of colorization results in \cref{fig:bedroom_colorization,fig:church_colorization}.

\subsection{Solving general inverse problems}\label{app:inverse_prob}
Suppose we have two random variables  and , and we know the forward process of generating  from , given by . The inverse problem is to obtain  from , that is, generating samples from . In principle, we can estimate the prior distribution  and obtain  using Bayes' rule: . In practice, however, both estimating the prior and performing Bayesian inference are non-trivial.

Leveraging \cref{eqn:cond_sde_2}, score-based generative models provide one way to solve the inverse problem. Suppose we have a diffusion process  generated by perturbing  with an SDE, and a time-dependent score-based model  trained to approximate . Once we have an estimate of , we can simulate the reverse-time SDE in \cref{eqn:cond_sde_2} to sample from . To obtain this estimate, we first observe that

where  is defined via  and the forward process . Now assume two conditions:
\begin{itemize}
    \item  is tractable. We can often derive this distribution from the interaction between the forward process and the SDE, like in the case of image imputation and colorization.
    \item . For small ,  is almost the same as  so the approximation holds. For large ,  becomes further away from  in the Markov chain, and thus have smaller impact on . Moreover, the approximation error for large  matter less for the final sample, since it is used early in the sampling process.
\end{itemize}
Given these two assumptions, we have

where  is a sample from . Now we can plug \cref{eqn:cond_approx} into \cref{eqn:cond_sde_2} and solve the resulting reverse-time SDE to generate samples from .

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/bedroom_inpainting.jpg}
    \caption{Extended inpainting results for  bedroom images.}
    \label{fig:bedroom_inpainting}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/church_inpainting.jpg}
    \caption{Extended inpainting results for  church images.}
    \label{fig:church_inpainting}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/bedroom_colorization.jpg}
    \caption{Extended colorization results for  bedroom images.}
    \label{fig:bedroom_colorization}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/church_colorization.jpg}
    \caption{Extended colorization results for  church images.}
    \label{fig:church_colorization}
\end{figure}
 
\end{document}
