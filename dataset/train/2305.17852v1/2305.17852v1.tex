\section{Experiments}
\label{sec:experiments}
The section evaluates the accuracy and the latency of the HMNet on three tasks: semantic segmentation, object detection, and monocular depth estimation.

Below, Sec.~\ref{sec:basic_setups} explains the basic settings of the experiments, and Sec.~\ref{sec:semantic_segmentation} to Sec.~\ref{sec:depth_estimation} show the results on each task.
Finally, Sec.~\ref{sec:ablation_study} performs the ablation study to verify the effectiveness of each component of the proposed model.
See supplementary materials for more detailed setups.

\figResutSemsegObjdet

\subsection{Basic setups}
\label{sec:basic_setups}

\noindent
\textbf{HMNet variants.}
We build four variants of HMNet, HMNet-B1/L1/B3/L3.
HMNet-B1 is the simplest form with only one latent memory. HMNet-B3 is its multi-level version with three latent levels. HMNet-L1/L3 is a high-capacity version of HMNet-B1/B3 with increased latent dimensions. Specifically, let $D$ be the dimension of each memory and $H$ be the number of the head for cross attention in the ``write'' operations; the configurations are as follows:
\begin{itemize}
\setlength{\parskip}{-1mm}
\setlength{\itemsep}{-3mm}
\item HMNet-B1: $D=128, H=4$ \\
\item HMNet-L1: $D=256, H=8$ \\
\item HMNet-B3: $D=(128,256,256), H=(4,8,8)$ \\
\item HMNet-L3: $D=(256,256,256), H=(8,8,8)$ \\
\end{itemize}
\vspace{-0.5cm}
For the memories $\{\bm{z}_1, \bm{z}_2, \bm{z}_3\}$, we set the global stride of features $s$ as $\{4,8,16\}$
and the operation cycle as $\{1,3,9\}$ time-steps, where the time-step size is set as $5$ms.

\vspace{0.1cm}
\noindent
\textbf{Baselines.}
We built baselines by 1) Using Time Surface to convert raw events into frame-based representations; 2) Taking a popular backbone network (ResNet, ConvNeXt, or Swin Transformer); and 3) Adding a task head to output predictions.
We also built their recurrent version by inserting ConvGRU \cite{Ballas2016} between stages.

\vspace{0.1cm}
\noindent
\textbf{Event-image fusion.}
We extended HMNet-B3/L3 for event-image fusion as explained in Sec.~\ref{subsec:sensor_fusion}.
For the image encoder, the layers until the stage3 of ResNet-18 \cite{He2016} are used.
For comparison, we also extend the baselines by concatenating the most recent image frame with its inputs.


\vspace{0.1cm}
\noindent
\textbf{Training and inference.}
For training, we used Adam \cite{Kingma2015} and AdamW \cite{Loshchilov2019} as an optimizer, depending on the task. We set the batch size as 16 and the initial learning rate as 2.0e-4 with a cosine learning rate decay.
We used random resize, crop, and horizontal flip for data augmentation.
Unless otherwise mentioned, we test all the methods using Tesla V100 GPU and 20-core Intel(R) Xeon(R) Gold 6148 CPU @ 2.4GHz.

\vspace{0.1cm}
\noindent
\textbf{Justification for hyperparameter tuning.}
We manually determined the hyperparameters for all the experiments.
To ensure that they are not overly tuned, we compared the manual tuning result to automatic tuning with Hyperopt \cite{Bergstra2013} on the object detection task, where we observed similar accuracy (44.7 and 44.4 mAP for manual and automatic tuning).



\subsection{Semantic segmentation}
\label{sec:semantic_segmentation}

\noindent
\textbf{Setups.}
The experiments are conducted on DSEC-Semantic dataset \cite{Sun2022}.
The dataset contains street-scene event data with a resolution of $640 \times 480$ pixels.
It also contains RGB frames recorded at 20Hz. Note that the cameras have different viewpoints; thus, the RGB frames are not perfectly aligned with the event data.
The pseudo pixel-wise annotations are available at 20Hz.
We used the task head of UPerNet \cite{Xiao2018} and trained our models for 60k iterations.

\tbSemsegFusion

\figVisAllTasks

\vspace{0.1cm}
\noindent
\textbf{Results.}
Fig.~\ref{fig:result_semseg} shows the results.
HMNet-B3/L3 outperforms the state-of-the-art ESS model \cite{Sun2022} on both accuracy and latency. Note that ESS utilizes additional training data on the RGB domain, while our methods are trained only on the event data.
HMNet-L3 also outperforms the baselines with strong backbones.
The effectiveness of HMNet-L3 is shown in a qualitative sample in Fig.~\ref{fig:vis_all_tasks} (top row).
The comparison between HMNet-B1/L1 and HMNet-B3/L3 shows the performance gain obtained by stacking multiple memories (+3.8\%/+3.2\%).
The latency of HMNet-B3/L3 can be further reduced by parallel computation when multi-GPU is available (depicted by ``mGPU'' in the figure).

\vspace{0.1cm}
\noindent
\textbf{Event-image fusion.}
We trained the fusion models using events and frames from the left RGB camera. Table~\ref{tab:semseg_rgb_fusion} compares the results in the case with and without RGB fusion.
The baselines perform worse with RGB fusion due to the misalignment between events and RGB frames, whereas HMNet shows improved accuracy.
Even with the misaligned inputs, cross-attention can robustly associate features, enabling the model to leverage both modalities.
We also test the models using the images from the right RGB camera at inference time.
To our surprise, the HMNet models keep the accuracy high, showing their robustness to the different sensor setups.

\subsection{Object detection}
\label{sec:object_detection}

\noindent
\textbf{Setups.}
The experiments are conducted on GEN1 dataset \cite{Perot2020}.
The dataset includes about 40 hours of event data with a resolution of $304\times 240$ pixels. The bounding box annotations are available at 1Hz to 4Hz, depending on the video sequences. The labels are defined for two classes: pedestrian and car.
We built a lite-weight detection head based on YOLOX \cite{Ge2021}. Specifically, we adopt FPN \cite{Lin2017} with additional bottom-up feature fusion instead of PAFPN \cite{Ge2021}.

\vspace{0.1cm}
\noindent
\textbf{Results.}
Fig.~\ref{fig:result_objdet} compares the HMNet with the previous methods and baselines. The proposed method outperforms the state-of-the-art methods (ASTMNet \cite{Li2022} and AED \cite{Liu2022}) in both accuracy and latency.
Primarily, HMNet-B1/L1 performs remarkably well though they have only a single-level memory. For instance, HMNet-B1 performs competitively to AED \cite{Liu2022} while reducing the latency by 57\%, showing the effectiveness of the proposed ESCA. A qualitative sample in Fig.~\ref{fig:vis_all_tasks} (second row) shows the effectiveness of HMNet for detecting fast-moving objects.

Although the GRU-CSPDarknet-53 baseline achieved the best accuracy, the model has higher latency than ours (more than double compared to HMNet-L1).
More importantly, the recurrent baselines require a long accumulation time (\eg $50$ms) for constructing an event frame, which limits the frame rate of these methods.

\subsection{Monocular depth estimation}
\label{sec:depth_estimation}

\noindent
\textbf{Setups.}
Following Gehrig \etal \cite{Gehrig2021}, we pretrained our models on the synthetic Eventscape dataset \cite{Gehrig2021}. We then fine-tuned and evaluated the models on real MVSEC dataset \cite{Zhu2018MVSEC}. MVSEC dataset consists of street-scene event data and gray-scale images recorded by a DAVIS event camera with a resolution of $346\times 260$ pixels. Since the DAVIS camera is coaxial, events and gray-scale images are aligned initially.
The ground truth depth maps are recorded at 20Hz using a LiDAR sensor.
Note that they are not synchronized with the gray-scale images.
We used {\it outdoor day2} for training and {\it outdoor day1} and {\it outdoor night1} for evaluation.

\tbDepthEstimation

\vspace{0.1cm}
\noindent
\textbf{Results.}
Table~\ref{tab:depth_estimation} shows the results on MVSEC dataset.
On the daytime sequence, HMNet outperforms the previous methods while reducing latency (\eg 44\% reduction comparing HMNet-B3 and RAMNet \cite{Gehrig2021}).
Although the ResNet-50 baseline performs the best on night-time sequence, HMNet-B3 achieves competitive performance with much lower latency (65\% reduction).
On both sequences, HMNet-B3/L3 is more accurate than HMNet-B1/L1, showing the effectiveness of the multi-level memory architecture.
The qualitative results in Fig.~\ref{fig:vis_all_tasks} (bottom two rows) show that HMNet can accurately estimate the cars in the lanes.

\vspace{0.1cm}
\noindent
\textbf{Event-image fusion.}
Table~\ref{tab:depth_estimation} reports the results for event-image fusion.
The baselines show degraded accuracy with the image fusion, which is due to the temporal mismatch between the images and the ground truth.
The models need to fuse up-to-date events with an old image frame at inference timing, which is difficult to handle by naive concatenation.
On the other hand, the HMNet has dedicated architecture to handle such temporal variation and hence enjoys performance gain with the image fusion on the daytime sequence.
Meanwhile, the HMNet models perform worse with image fusion on the night-time sequence due to the image appearance gap between daytime and night-time.

\tbAblationNoiseDWCycle
\figVisEvGate
\figAblationDeltaT

\subsection{Ablation study}
\label{sec:ablation_study}
We conducted ablation studies on several key elements of HMNet using DSEC-Semantic and GEN1 datasets.

\vspace{0.1cm}
\noindent
\textbf{Effect of event gate and down-write.}
Table~\ref{tab:ablation_noise_dw_cycle} left investigates the effect of the event gate used in ESCA and the down-write operation of HMNet.
The results show the effectiveness of the components.
Applying both of them improves the accuracy by +1.6\% and +1.1\% for each dataset.
To further analyze the effect of the event gate, we visualized the attention weights of each memory cell at $\bm{z}_1$.
Specifically, Fig.~\ref{fig:vis_evgate} visualizes the accumulated attention weights written into each cell during $10$ time steps (\ie 50ms).
Without the event gate, the latent memory receives much noisy information. In contrast, with the event gate, the latent memory selectively picks up essential information about the scene (\eg the car in the green rectangle).

\vspace{0.1cm}
\noindent
\textbf{Sensitivity analysis on time step size.}
In practice, the time step size needs to be adjustable depending on the available hardware at inference time.
Hence, we performed the sensitivity analysis on the time step size.
We trained HMNet-B3/L3 using a time step size of 5ms and evaluated them with various step sizes
(3ms to 15ms).
Fig.~\ref{fig:ablation_delta_t} shows the analysis.
The models perform well until the time step size reaches 10ms
even showing better accuracy around 6ms-8ms. After that, the accuracy degrades linearly.

\vspace{0.1cm}
\noindent
\textbf{Sensitivity analysis on cycle length.}
Table~\ref{tab:ablation_noise_dw_cycle} right shows the accuracy of HMNet-B3 when the cycle length of $\bm{z}_3$ is changed between 3 to 12.
The cycle length of 9 performs better than the shorter ones, showing the importance of long-term information propagation by $\bm{z}_3$.
Though, too long cycle length impairs the accuracy on GEN1 dataset.

