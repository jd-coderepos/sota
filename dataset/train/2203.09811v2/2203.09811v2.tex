

\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}              

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage[ruled]{algorithm2e}
\usepackage[accsupp]{axessibility}  \hyphenpenalty=5000
\tolerance=1000


\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}

\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


\begin{document}



\title{Stacked Hybrid-Attention and Group Collaborative Learning\\
for Unbiased Scene Graph Generation}

\author{Xingning Dong\textsuperscript{\rm 1,\rm 2}, \quad Tian Gan\textsuperscript{\rm 1}{\footnotemark[2]}, \quad Xuemeng Song\textsuperscript{\rm 1}, \quad Jianlong Wu\textsuperscript{\rm 1}, \quad Yuan Cheng\textsuperscript{\rm 2}{\footnotemark[2]}, \quad Liqiang Nie\textsuperscript{\rm 1}\
	\mathbf{x}_i = {Enc}^{obj}([\mathbf{v}_i, FC(\mathbf{s}_i)], Emb( {l}_i) ),

	{l}^{\prime}_i = {\rm argmax}({\rm Softmax}(Dec^{obj}(\mathbf{x}_i))),

	\mathbf{x}^{\prime}_i = {Enc}^{rel}([\mathbf{v}_i, \mathbf{x}_i], Emb( {l}^{\prime}_i)),

	{p}_{i\rightarrow j} = {\rm argmax}({\rm Softmax}({Dec}^{rel}(\mathbf{x}^{\prime}_i , \mathbf{x}^{\prime}_j , \mathbf{u}_{ij})) ,

    \left \{
    \begin{aligned}
	\mathbf{X}^{(l)} = SA(\mathbf{X}^{(l-1)}) + CA(\mathbf{X}^{(l-1)}, \mathbf{Y}^{(l-1)}),
	\\
	\mathbf{Y}^{(l)} = SA(\mathbf{Y}^{(l-1)}) + CA(\mathbf{Y}^{(l-1)}, \mathbf{X}^{(l-1)}),
	\end{aligned}
	\right.

    \phi_{i}^{k} = 
	\left\{
	\begin{aligned}
		& \frac{Med({\mathcal{P}}_{k}^{\prime})}{ Count({p}_{i}) },\quad \mathbf{if}\;\; Med({\mathcal{P}}_{k}^{\prime})< Count({p}_{i}),
		\\
		&{ \quad\;\; 1.0 \;\;\;\quad } ,  \quad \mathbf{if}\;\; Med({\mathcal{P}}_{k}^{\prime}) \geq Count({p}_{i}).
	\end{aligned}
	\right.

    \mathbf{w}_{ij}^{k} = {\rm Softmax}(FC([\mathbf{x}^{\prime}_i , \mathbf{x}^{\prime}_j]) \otimes \mathbf{u}_{ij}),

	\mathcal{L}_{PCO}=\sum_{k=1}^{K} \frac{1}{|\mathcal{D}_k|}\sum_{{(o_i,o_j)} \in \mathcal{D}_k}  \mathcal{L}_{CE} ({y}_{ij}, \mathbf{w}_{ij}^{k}),

	\mathcal{L}_{CKD}=\frac{1}{| \mathcal{Q} |} \sum_{{(m, n)} \in \mathcal{Q}}   \frac{1}{|\mathcal{D}_n|}\sum_{{(o_i,o_j)} \in \mathcal{D}_n}  \mathcal{L}_{KL} (\mathbf{w}_{ij}^{m}, \widehat{\mathbf{w}}_{ij}^{{n}}),

	\mathcal{L}_{KL} (\mathbf{w}_{m}, \widehat{\mathbf{w}}_{n}) = - \sum_{l=1}^{L} \mathbf{w}_{m}^{l} \log \widehat{\mathbf{w}}_{n}^{l}.

	\mathcal{L}_{GCL} = \mathcal{L}_{PCO} + \alpha\mathcal{L}_{CKD},

where  is the pre-defined hyper-parameters to weigh the total loss . By employing these two types of constraint, we effectively mitigate the overwhelming punishments to the tail classes and compensate for the under-fitting on the head ones, which benefits in establishing a reasonable trade-off during the predicate predictions.




\begin{table*}[t]
	\small
	\vspace{-0.4cm}
	\begin{tabular}{p{2.9cm}|p{1.1cm}<{\centering}p{1.1cm}<{\centering}p{1.1cm}<{\centering}|p{1.1cm}<{\centering}p{1.1cm}<{\centering}p{1.1cm}<{\centering}|p{1.1cm}<{\centering}p{1.1cm}<{\centering}p{1.1cm}<{\centering}}
		\hline
		\multicolumn{1}{c|}{\multirow{2}{*}{Model}} & \multicolumn{3}{c|}{PredCls}& \multicolumn{3}{c|}{SGCls}& \multicolumn{3}{c}{SGDet}\\ \cline{2-10} 
		\multicolumn{1}{c|}{}& \multicolumn{1}{c}{mR@20} & \multicolumn{1}{c}{mR@50} & \multicolumn{1}{c|}{mR@100} & \multicolumn{1}{c}{mR@20} & \multicolumn{1}{c}{mR@50} & \multicolumn{1}{c|}{mR@100} & \multicolumn{1}{c}{mR@20} & \multicolumn{1}{c}{mR@50} & \multicolumn{1}{c}{mR@100} \\ \hline
		
		IMP+ & -  &9.8  &10.5 &-  &5.8  &6.0 &-  & 3.8 &4.8 \\
		KERN & -  &17.7  &19.2 &-  &9.4  &10.0 &-  & 6.4 &7.3 \\
		GPS-Net & 17.4  &21.3  &22.8 &10.0  &11.8  &12.6 &6.9  & 8.7 &9.8 \\
		PCPL & -  &35.2  &37.8 &-  &18.6  &19.6 &-  & 9.5 &11.7 \\
		VTransE+ &13.6  &17.1  &18.6 &6.6  &8.2  &8.7 &5.1  & 6.8 &8.0 \\
	    SG-CogTree & 22.9  &28.4  &31.0 &13.0  &15.7  &16.7 &7.9  & 11.1 &12.7 \\
		BGNN & -  &30.4  &32.9 &-  &14.3  &16.5 &-  & 10.7 &12.6 \\
		
		 \hline
		
		Motifs &11.7  &14.8  &16.1 &6.7  &8.3 &8.8 &5.0  &6.8  & 7.9\\ 
		Motifs + Reweight &14.3  &17.3  &18.6 &9.5  &11.2 &11.7 &6.7  &9.2  & 10.9\\
		Motifs + TDE &18.5  &25.5  &29.1 &9.8  &13.1 &14.9 &5.8  &8.2  & 9.8\\
		Motifs + CogTree &20.9  &26.4  &29.0 &12.1  &14.9 &16.1 &7.9  &10.4  & 11.8\\
		Motifs + DLFE &22.1  &26.9  &28.8 &12.8  &15.2 &15.9 &8.6  &11.7  & 13.8\\
		Motifs + EBM &14.2  &18.0  &28.8 &8.2  &10.2 &11.0 &5.7  &7.7  & 9.3\\
		\textbf{Motifs + GCL} &\textbf{30.5}  &\textbf{36.1}  &\textbf{38.2} &\textbf{18.0}  &\textbf{20.8}  &\textbf{21.8}  &\textbf{12.9}  &\textbf{16.8}  &\textbf{19.3} \\
		\hline
		
		VCTree &13.1 &16.7 &18.1 & 9.6  & 11.8  &12.5 &5.4  &7.4 &8.7 \\
		VCTree + Reweight&16.3 &19.4 &20.4 & 10.6  & 12.5  &13.1 &6.6  &8.7 &10.1 \\
		VCTree + TDE&18.4 &25.4 &28.7 & 8.9  & 12.2  &14.0 &6.9  &9.3 &11.1 \\
		VCTree + CogTree&22.0 &27.6 &29.7 & 15.4  & 18.8  &19.9 &7.8  &10.4 &12.1 \\
		VCTree + DLFE&20.8 &25.3 &27.1 & 15.8  & 18.9  &20.0 &8.6  &11.8 &13.8 \\
		VCTree + EBM&14.2 &18.2 &19.7 & 10.4  & 12.5  &13.5 &5.7  &7.7 &9.1 \\
		\textbf{VCTree + GCL}  &\textbf{31.4}   &\textbf{37.1}  &\textbf{39.1}  &\textbf{19.5}  &\textbf{22.5}  &\textbf{23.5}  &\textbf{11.9}  &\textbf{15.2} &\textbf{17.5} \\
		 \hline 
		 
		SHA &14.4  &18.8  &20.5 &8.7  &10.9 &11.6 &5.7  &7.8  & 9.1\\
		
		\textbf{SHA + GCL (ours)} &\textbf{35.6}  &\textbf{41.6}  &\textbf{44.0}  &\textbf{19.6}  &\textbf{23.0}  &\textbf{24.3}  &\textbf{14.2}  &\textbf{17.9}  &\textbf{20.9} \\
		 \hline
		
	\end{tabular}
\vspace{0.02cm}
\caption{Performance comparison of different methods on PredCls, SGCls, and SGDet tasks of VG150 with respect to mR@20/50/100 (\%). The superscript  denotes that the method employs Faster R-CNN with VGG-16 as the object detector, while the subscript  denotes that the method is model-agnostic and targets to address the biased relationship predictions in SGG.}
\vspace{-0.4cm}
\label{result_VG}
\end{table*}

\begin{table}[t]
	\small
	\vspace{0.0cm}
	\begin{tabular}{p{2.3cm}|p{1.7cm}<{\centering}|p{1.4cm}<{\centering}|p{1.4cm}<{\centering}}
		\hline
		\multicolumn{1}{c|}{\multirow{2}{*}{Model}} & \multicolumn{1}{c|}{PredCls}& \multicolumn{1}{c|}{SGCls}&\multicolumn{1}{c}{SGDet}
		\\ 
		\cline{2-4} 
		\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{mR 50/100} & \multicolumn{1}{c|}{mR 50/100} & \multicolumn{1}{c}{mR 50/100} \\ \hline
		
		VTransE &14.0 / 15.0  & 8.1 / 8.7 & 5.8 / 6.6  \\ 
		\textbf{VTransE + GCL} &\textbf{30.4} / \textbf{32.3} &\textbf{16.6} / \textbf{17.4}  &\textbf{14.7} / \textbf{16.4}  \\
		\hline
		
		Motifs &16.4 / 17.1  &8.2 / 8.6  &6.4 / 7.7 \\ 
		\textbf{Motifs + GCL} &\textbf{36.7} / \textbf{38.1} &\textbf{17.3} / \textbf{18.1}  &\textbf{16.8} / \textbf{18.8}  \\
		\hline
		
		VCTree &16.6 / 17.4 & 7.9 / 8.3  & 6.5 / 7.4  \\
		\textbf{VCTree + GCL} &\textbf{35.4} / \textbf{36.7} &\textbf{17.3} / \textbf{18.0} &\textbf{15.6} / \textbf{17.8}\\
		 \hline
		 
		SHA &19.5 / 21.1  & 8.5 / 9.0  & 6.6 / 7.8    \\
		\textbf{SHA + GCL} &\textbf{41.0} / \textbf{42.7} &\textbf{20.6} / \textbf{21.3}  &\textbf{17.8} / \textbf{20.1}  \\
		 \hline
		
	\end{tabular}
\vspace{0.02cm}
\caption{Performance comparison of different methods on three tasks of GQA200 with respect to mR@50/100 (\%).}
\vspace{-0.4cm}
\label{result_GQA}
\end{table}


\section{Experiments}

\subsection{Experimental Settings}
\noindent\textbf{Dataset.}
We present the experimental results on two datasets: Visual Genome (VG)~\cite{krishna2016visual} and GQA~\cite{hudson2019gqa}. VG is the most widely-used benchmark for SGG, which is composed of more than 108K images and 2.3M relation instances. Following the prior approaches \cite{xu2017scene, chen2019knowledge, lin2020gps, yan2020pcpl, zhang2017visual, li2021bipartite, zellers2018neural, tang2019learning, tang2020unbiased, yu2020cogtree, chiou2021recovering, suhail2021energy}, we adopt the most widely-used VG150 split, which contains the most frequent 150 object classes and 50 predicate classes. GQA is another vision-and-language benchmark with more than 3.8M relation annotations. In order to achieve a representative split like VG150, we manually clean up a substantial fraction of annotations that have poor-quality or ambiguous meanings, and then select Top-200 object classes as well as Top-100 predicate classes by their frequency, thus establishing the GQA200 split. For both VG150 and GQA200, we use 70 of the images for training and the remaining 30 for testing. We also follow~\cite{zellers2018neural} to sample a 5K validation set from the training set for parameter tuning.

\noindent\textbf{Tasks.}
To comprehensively evaluate the performance, we follow three conventional tasks: 1) Predicate Classification (\textbf{PredCls}) predicts the relationships of all the pairwise objects by employing the given ground-truth bounding boxes and classes; 2) Scene Graph Classification (\textbf{SGCls}) predicts the objects classes and their pairwise relationships by employing the given ground-truth object bounding boxes; and 3) Scene Graph Detection (\textbf{SGDet}) detects all the objects in an image, and predicts their bounding boxes, classes and pairwise relationships.

\noindent\textbf{Evaluation Metrics.} Following \cite{yan2020pcpl, lin2020gps, li2021bipartite, tang2020unbiased, yu2020cogtree, chiou2021recovering, suhail2021energy}, we use mean Recall@K
(mR@K)\cite{tang2019learning,chen2019knowledge}, which computes the average Recall@K (R@K) for each predicate class, to evaluate the unbiased SGG. As R@K is easily dominated by the head classes due to the extremely unbiased dataset, mR@K could give a fair performance appraisal for both head and tail classes, which is widely used as an unbiased evaluation metric.

\noindent\textbf{Implementation Details.} We adopt a pre-trained Faster R-CNN \cite{Ren2017Faster} with ResNeXt-101-FPN\cite{xie2017aggregated} provided by \cite{tang2020unbiased} as the object detector. We employ Glove\cite{pennington2014glove} to obtain the semantic embedding. The object encoder and the relation encoder contain four and two SHA layers, respectively. We set the division threshold , and employ the Top-Down strategy (each classifier is forced to learn the prediction behavior from all its predecessors, see Figure \ref{parameters} for more details) to construct the pairwise knowledge matching set . The hyper-parameter  which balances the optimization objective is set to be 1.0. We optimize the proposed network by the Adam optimizer with a momentum of 0.9. For all three tasks, the total training stage lasts for 60,000 steps with a batch size of 8. The initial learning rate is 0.001, and we adopt the same warm-up and decayed strategy as \cite{tang2020unbiased}. One RTX2080 Ti is used to conduct all the experiments.

\subsection{Compared Methods}
We want to declare that our proposed method is not only powerful in generating unbiased scene graphs, but also applicable for a variety of SGG approaches. For the former, we compare it with state-of-the-art approaches, including re-produced IMP+\cite{xu2017scene}, KERN\cite{chen2019knowledge}, GPS-Net\cite{lin2020gps}, PCPL\cite{yan2020pcpl}, re-produced VTransE+\cite{zhang2017visual} and BGNN\cite{li2021bipartite}. For the latter, we adopt two typical baselines, namely Motifs\cite{zellers2018neural} and VCTree\cite{tang2019learning}, to give a fair comparison with other model-agnostic approaches, such as Reweighting\cite{chiou2021recovering},  TDE\cite{tang2020unbiased}, CogTree\cite{yu2020cogtree}, DLFE\cite{chiou2021recovering} and EBM\cite{suhail2021energy}. 

Table \ref{result_VG} and Table \ref{result_GQA} present the performance of different approaches conducted on VG150 and GQA200, respectively. We have several observations as follows: 1) Our proposed SHA+GCL significantly outperforms all the baselines on all three tasks. To the best of our knowledge, our work is the first to breakthrough the 40\% precision in both mR@50 and mR@100 on PredCls, and we also achieve the best performance on SGCls and SGDet. 2) Motifs+GCL and VCTree+GCL nearly double the performance in mean Recall on all three tasks compared with Motifs and VCTree. It demonstrates that the proposed GCL is model-agnostic and can largely enhance the unbiased relationship predictions. 3) Compared with Motifs+GCL and VCTree+GCL, we witness an obvious performance gain in SHA+GCL. It indicates that the proposed SHA module could facilitate both the intra-modal refinement and the inter-modal interaction, thus leading to more accurate predictions. In conclusion, SHA+GCL effectively addresses two aforementioned concerns in SGG, \ie, insufficient modality fusion and biased relationship predictions.

\subsection{Ablation Study}
As aforementioned, we propose the Stacked Hybrid Attention (SHA) network to improve the object encoder and the relation encoder, and propose the Group Collaborative Learning (GCL) strategy, which employs the Parallel Classifier Optimization (PCO) as the ``weak constraint'' and Collaborative Knowledge Distillation (CKD) as the ``strong constraint'', to guide the training of the decoder. In order to prove the effectiveness of the above components, we test various ablation models on VG150 as follows: 

\vspace{-0.2cm}
\begin{itemize}
\setlength{\itemsep}{0pt}
\setlength{\parsep}{0pt}
\setlength{\parskip}{0pt}
	\item w/o-GCL: To evaluate the effectiveness of GCL, we let the relation decoder be a one-layer classifier, where a regular Cross-Entropy loss is performed.
	
	\item w/o PCO\&CKD: To evaluate the effectiveness of PCO in GCL, we remove the PCO loss and CKD loss, and only employ the Median Re-Sampling strategy and a regular Cross-Entropy loss in the optimization step.
	
	\item w/o CKD: To evaluate the effectiveness of CKD in GCL, we remove the CKD loss but retain all the classifiers to compute the PCO loss.
	
	\item w/o CA or w/o SA: To evaluate the effectiveness of SHA, we remove either the Cross-Attention (CA) unit or the Self-Attention (SA) unit in every SHA layer.

\end{itemize}
\vspace{-0.2cm}

\begin{table}[t]
	\small
	\vspace{0.4cm}
	\begin{tabular}{p{2.3cm}|p{1.4cm}<{\centering}|p{1.4cm}<{\centering}|p{1.4cm}<{\centering}}
		\hline
		\multicolumn{1}{c|}{\multirow{2}{*}{Model}} & \multicolumn{1}{c|}{PredCls}& \multicolumn{1}{c|}{SGCls}&\multicolumn{1}{c}{SGDet}
		\\ 
		\cline{2-4} 
		\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{mR 50/100} & \multicolumn{1}{c|}{mR 50/100} & \multicolumn{1}{c}{mR 50/100} \\ \hline
		
		w/o - GCL &18.8 / 20.5  &10.9 / 11.6  &7.8 / 9.1  \\ 
		w/o - PCO\&CKD & 35.2 / 37.4   & 20.1 / 21.2  & 14.6 / 16.9   \\
		w/o - CKD &39.3 / 41.7  &22.0 / 23.2  &16.5 / 19.0   \\
		w/o - CA &39.8 / 42.5   &22.6 / 23.6  &16.8 / 19.3   \\
		w/o - SA &39.2 / 41.5   &22.6 / 23.7  &17.5 / 20.1   \\
		
		\hline
		
		SHA + GCL &\textbf{41.6} / \textbf{44.0} &\textbf{23.0} / \textbf{24.3}  &\textbf{17.9} / \textbf{20.9}  \\
		\hline
	\end{tabular}
\vspace{0.02cm}
\caption{Ablation study of the proposed method on VG150.}
\vspace{-0.2cm}
\label{result_AB}
\end{table}

\begin{figure}
	\centering
	\begin{subfigure}{1\linewidth}
		\includegraphics[width=1.0\textwidth]{./fig/ab1.pdf}
		\caption{\scriptsize{R@100 of all the predicate classes of w/o-GCL and SHA+GCL on VG150.}}
		\label{norVSgist}
	\end{subfigure}
 	\begin{subfigure}{1\linewidth}
		\includegraphics[width=1.0\textwidth]{./fig/ab2.pdf}
		\caption{\scriptsize{R@100 of all the predicate classes of w/o-CKD and SHA+GCL on VG150.}}
		\label{gclVSgist}
	\end{subfigure}
	\vspace{-0.2cm}
	\caption{R@100 of 50 predicate classes on PredCls on VG150.}
	\vspace{-0.4cm}
	\label{ablation}
\end{figure}

Table \ref{result_AB} presents the results of all the ablation models. We have several observations as follows: 1) Compared with w/o-GCL, SHA+GCL nearly doubles the performance. Moreover, in Figure \ref{norVSgist}, we compare w/o-GCL and SHA+GCL with respect to R@100 of all the predicate classes. As can be observed, SHA+GCL obviously improves the performance on most of the predicate classes, only with an acceptable decay on the head classes in Group 1 and Group 2, showing a powerful capability in generating unbiased scene graphs. 2) Compared with w/o-PCO\&CKD, w/o-CKD evidently improves the prediction performance, demonstrating that the ``weak constraint'', namely gathering gradients from all the classifiers, would facilitate the convergence of the final classifier. 3) Compared with w/o-CKD, we witness an obvious performance gain in SHA+GCL. Moreover, we compare w/o-CKD and SHA+GCL on the detailed precision towards every predicate class on VG150. As shown in Figure \ref{gclVSgist}, CKD effectively prevents the model from sacrificing much on the head classes, as well as achieves a comparable performance towards the tail predictions. It demonstrates that the ``strong constraint'', namely a knowledge transfer paradigm, could effectively compensate for the under-fitting on the head classes by preserving the discriminating capability learned previously, and thus benefits in achieving a reasonable trade-off. 4) From the last three rows in Table \ref{result_AB}, we witness an obvious performance decay when removing either the CA unit or the SA unit. It verifies that combining both attentions would effectively alleviate the insufficient modality fusion, thus leading to more accurate predictions.

\subsection{Parameter Analysis}



As aforementioned, the threshold  and the organization strategy would influence the performance of GCL. As Figure \ref{parameters} illustrates, for the former, we set 3, 4, and 5, and obtain 6, 5, and 4 group divisions, respectively. For the latter, we provide two alternatives, namely Adjacent and Top-Down strategy, whose difference is whether each classifier could learn the knowledge from its nearest predecessor (Adjacent) or from all the predecessors (Top-Down).

\begin{table}
	\small
	\vspace{0.4cm}
	\begin{tabular}{p{0.4cm}<{\centering}|p{1.4cm}<{\centering}|p{1.4cm}<{\centering}|p{1.4cm}<{\centering}|p{1.4cm}<{\centering}}
		\hline
		\multicolumn{2}{c|}{Model} & \multicolumn{1}{c|}{PredCls}& \multicolumn{1}{c|}{SGCls}&\multicolumn{1}{c}{SGDet}
		\\ 
		
		\hline
		\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{Strategy} & \multicolumn{1}{c|}{mR 50/100} & \multicolumn{1}{c|}{mR 50/100} & \multicolumn{1}{c}{mR 50/100} \\ \hline
		3 & Adjacent &40.0 / 42.4 & 22.5 / 23.4  & 16.8 / 19.2    \\
		4 & Adjacent &41.0 / 43.5 &23.0 / 23.9  &17.3 / 19.7   \\
		5 & Adjacent &39.4 / 41.7   & 21.8 / 23.0   & 16.7 / 19.1    \\\hline
		3 & Top-down &40.9 / 43.2   &22.9 / 23.8  &17.0 / 19.9   \\
		4 & Top-down &\textbf{41.6} / \textbf{44.0} &\textbf{23.0} / \textbf{24.3}  &\textbf{17.9} / \textbf{20.9}  \\
		5 & Top-down &39.7 / 42.0   &23.1 / 23.8  &16.9 / 19.6   \\
		 \hline
	\end{tabular}
\vspace{0.02cm}
\caption{Parameter analysis towards the threshold  and the pairwise knowledge matching strategies of GCL on VG150.}
\vspace{-0.2cm}
\label{result_PA}
\end{table}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.49\textwidth]{./fig/parameters_final.pdf}
	\vspace{-0.5cm}
	\caption{Illustration of three configurations of the balanced group divisions according to the threshold  (top), and two alternatives of the pairwise knowledge matching strategy (down).}
	\vspace{-0.4cm}
	\label{parameters}
\end{figure}


Table \ref{result_PA} presents the performance comparisons, where  and the Top-Down strategy is the best combination. 






\section{Conclusion}



In this work, we declare two concerns that restrict the practical applications of SGG, namely insufficient modality fusion and biased relationship predictions. To address such deficiency, we propose the Stacked Hybrid-Attention network and the Group Collaborative Learning strategy. In this way, we establish a new state-of-the-art in the unbiased metric and provide a model-agnostic debiasing method. In the future, we plan to explore more robust group dividing methods and devise more knowledge distillation strategies.

\setlength{\parskip}{8pt}
\noindent\textbf{Acknowledgments.} This work is supported by the National Natural Science Foundation of China, No.: 62176137, No.:U1936203, and No.: 62006140; the Shandong Provincial Natural Science and Foundation, No.: ZR2020QF106; Beijing Academy of Artificial Intelligence(BAAI); Ant Group.
\setlength{\parskip}{0pt}

\newpage
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\newpage

\setcounter{section}{0}

\setlength{\parskip}{0pt}
\section{Introduction}

In this supplementary material, we present more analyses, experiments, and visualization results, as well as discuss the limitations and future work of our method.

\section{Parameter Statistics}
We compare the total number of parameters between three baseline methods (\ie, Motifs, VCTree, and SHA) and their enhanced versions that are equipped with our model-agnostic GCL in Table \ref{para_counts}. As can be observed, compared with the original methods which possess a massive number of total trainable parameters (about 200M), GCL only additionally introduces a limited number of parameters (about 2M), which could hardly influence the overall training procedure.

\section{Detailed Performance}
We present the complete results of our experiments employing the regular Recall@K\cite{lu2016visual}, the unbiased Mean Recall@K\cite{tang2019learning,chen2019knowledge}, and their mean\cite{lin2020gps} on all three tasks (\ie, PredCls, SGCls, and SGDet) on VG150\cite{krishna2016visual} and GQA200\cite{hudson2019gqa} dataset in Table \ref{result_all}, where . Note that all the methods are implemented with a pre-trained Faster R-CNN\cite{Ren2017Faster} with ResNeXt-101-FPN\cite{xie2017aggregated} provided by \cite{tang2020unbiased} as the object detector, thus we could give a fair comparison to prove the superiority of our method.

From Table \ref{result_all}, we observe that 1) our proposed SHA+GCL achieves the best performance on all three tasks towards the unbiased metric mR@K in both two datasets. In VG150, we breakthrough the 40\% precision in both mR@50 and mR@100 on PredCls, and 20\% precision in mR@100 on both SGCls and SGDet, thus establishing a new state-of-the-art in the unbiased metric. 2) Our improvement towards the relation decoder, namely GCL strategy, is model-agnostic and could largely enhance the unbiased SGG. In both VG150 or GQA200, the method equipped with GCL nearly doubles the performance compared with the original one, showing the outstanding capability in generating unbiased scene graphs.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.49\textwidth]{./fig/future_final.pdf}
	\vspace{-0.4cm}
	\caption{The group-incremental configuration (left) may not be the only alternative to fulfill the ``conquer'' step in GCL. For example, the group-split configuration (right) is another promising strategy. Therefore, we aim to explore more robust group dividing methods and classifier configuration strategies in the future.}
	\vspace{-0.4cm}
	\label{future_work}
\end{figure}

\section{Visualization Results}
To get an intuitive perception of the superior performance in generating unbiased scene graphs of our proposed GCL, we visualize several PredCls examples generated from the biased SHA and the unbiased SHA+GCL. As shown in Figure \ref{visualization}, the model employing the proposed GCL strategy prefers to providing more informative and specific relationship predictions (\eg, lying on and riding) rather than common and trivial ones (\eg, on and has), \eg, ``person1-riding-elephant'' in the top-right example and ``train-pulling-car'' in the bottom-left example. Moreover, the model equipped with our model-agnostic GCL could also capture potential reasonable relationships, such as ``person1-watching-person2'' in the top-right example and ``sidewalk-beside-train'' in the bottom-left example. In a nutshell, the proposed GCL could enhance the unbiased relationship predictions, thus achieving more informative scene graphs to support various down-stream tasks.

\section{Limitations and Future Work}

In this section, we would like to discuss the limitations of our method, based on which we provide several potential directions to further improve our SHA+GCL.

\subsection{More Configurations Could be Further Explored}
As aforementioned, we follow the intuition of ``divide-conquer-cooperate'' to address the biased relationship predictions. In the second step, namely ``conquer'', we borrow the idea from class-incremental learning \cite{hu2020learning} and employ the group-incremental configuration. Actually, we employ this configuration mainly due to its simplicity and efficiency, as we could directly leverage the final classifier that covers all the candidate classes to obtain the predictions in the evaluation stage. However, we should argue that it is not the only alternative to fulfill the ``conquer'' step. Therefore, in the future, we aim to explore more robust group dividing methods as well as classifier configuration strategies to promote the unbiased SGG, \eg, the group-split configuration in Figure \ref{future_work}.

\subsection{``Strong Constraint'' Could be Further Enhanced}
As aforementioned, in the ``cooperate'' step, we use the collaborative knowledge distillation to establish an effective knowledge transfer mechanism, where a regular Kullback-Leibler Divergence loss is employed. However, since various novel methods have been proposed in the knowledge distillation area, we could further enhance our GCL by devising more efficient strategies, thus strengthening the ``Strong Constraint'' and promoting the unbiased SGG.

\begin{table*}[t]
	\small
	\begin{tabular}{p{2.0cm}<{\centering}p{1.1cm}<{\centering}p{1.1cm}<{\centering}|p{2.0cm}<{\centering}p{1.1cm}<{\centering}p{1.1cm}<{\centering}|p{2.0cm}<{\centering}|p{1.1cm}<{\centering}p{1.1cm}}
		\hline
		\multicolumn{1}{c}{Model}& \multicolumn{1}{c}{Fixed} & \multicolumn{1}{c|}{Trainable} & \multicolumn{1}{c}{Model} & \multicolumn{1}{c}{Fixed} & \multicolumn{1}{c|}{Trainable} & \multicolumn{1}{c}{Model} & \multicolumn{1}{c}{Fixed} & \multicolumn{1}{c}{Trainable}\\ \hline
		
		Motifs  & 158.7M & 208.5M & VCTree & 158.7M & 199.8M & SHA & 158.7M & 228.8M \\ 
		Motifs + GCL  & 158.7M & 210.5M & VCTree + GCL & 158.7M & 201.8M & SHA + GCL & 158.7M & 230.9M \\  \hline
		
	\end{tabular}
	\caption{Comparison of different methods on the number of parameters.       ``Fixed'' counts the number of parameters that belong to the pre-trained object detector, and ``Trainable'' counts the number of parameters that can be updated during the training procedure.}
	\label{para_counts}
	\vspace{0.1cm}
\end{table*}

\begin{table*}[t]
	\small
	\begin{tabular}{p{3.2cm}|p{1.4cm}<{\centering}p{1.4cm}<{\centering}|p{1.4cm}<{\centering}p{1.4cm}<{\centering}|p{1.4cm}<{\centering}p{1.4cm}<{\centering}|p{0.5cm}<{\centering}p{0.5cm}}
		\hline
\multicolumn{9}{c}{\multirow{2}{*}{Evaluation on Visual Genome Dataset}}\\
		\multicolumn{9}{c}{}\\
		\hline
		\multicolumn{1}{c|}{\multirow{2}{*}{Model}} & \multicolumn{2}{c|}{PredCls}& \multicolumn{2}{c|}{SGCls}& \multicolumn{2}{c|}{SGDet}& \multicolumn{2}{c}{MEAN}\\ \cline{2-9} 
		\multicolumn{1}{c|}{}& \multicolumn{1}{c}{R@50/100} & \multicolumn{1}{c|}{mR@50/100} & \multicolumn{1}{c}{R@50/100} & \multicolumn{1}{c|}{mR@50/100} & \multicolumn{1}{c}{R@50/100} & \multicolumn{1}{c|}{mR@50/100} & \multicolumn{1}{c}{R-M} & \multicolumn{1}{c}{mR-M}\\ \hline
		
		{IMP}  \cite{suhail2021energy}  & 61.1 / 63.1 & 11.0 / 11.8 & 37.4 / 38.3 & 6.4 / 6.7 & 23.6 / 28.7 & 3.3 / 4.1 & 42.0 & 7.2 \\ 
		{GPS-Net} \cite{li2021bipartite}  & 65.2 / 67.1 & 15.2 / 16.6 & 37.8 / 39.2 & 8.5 / 9.1 & 31.1 / 35.9 & 6.7 / 8.6 & 46.1 & 10.8 \\ 
		SG-CogTree \cite{yu2020cogtree} & 38.4 / 39.7 & 28.4 / 31.0 & 22.9 / 23.4 & 15.7 / 16.7 & 19.5 / 21.7 & 11.1 / 12.7 & 27.6 & 19.3\\ 
		BGNN \cite{li2021bipartite} & 59.2 / 61.3 & 30.4 / 32.9 & 37.4 / 38.5 & 14.3 / 16.5 & 31.0 / 35.8 & 10.7 / 12.6 & 43.9 & 19.6 \\ \hline
		VTransE \cite{tang2020unbiased} & \underline{\textbf{65.7}} / \underline{\textbf{67.6}} & 14.7 / 15.8 & \underline{38.6} / \underline{39.4} & 8.2 / 8.7 & \underline{29.7} / \underline{34.3} & 5.0 / 6.0 & \underline{45.9} & 9.7 \\
		VTransE + TDE \cite{tang2020unbiased} & 48.5 / 43.1 & 24.6 / 28.0 & 25.7 / 28.5 & 12.9 / 14.8 & 18.7 / 22.6 & 8.6 / 10.5 & 31.2 & 16.6 \\
		\textbf{VTransE + GCL} & 35.4 / 37.3 & \underline{34.2} / \underline{36.3} & 25.8 / 26.9 & \underline{20.5} / \underline{21.2} & 14.6 / 17.1 & \underline{13.6} / \underline{15.5} & 26.2 & \underline{23.5} \\\hline
		
		Motifs \cite{tang2020unbiased} &  \underline{65.2} / 67.0 & 14.8 / 16.1 & 38.9 / 39.8 & 8.3 / 8.8 & \underline{\textbf{32.8}} / \underline{\textbf{37.2}} & 6.8 / 7.9 & \underline{46.8} & 10.4 \\
		Motifs + Reweight \cite{chiou2021recovering} &  54.7 / 56.5 & 17.3 / 18.6 & 29.5 / 31.5 & 11.2 / 11.7 & 24.4 / 29.3 & 9.2 / 10.9 & 37.7 & 13.2 \\
		Motifs + TDE \cite{tang2020unbiased} & 46.2 / 51.4 & 25.5 / 29.1 & 27.7 / 29.9 & 13.1 / 14.9 & 16.9 / 20.3 & 8.2 / 9.8 & 32.1 & 16.8 \\
		Motifs + {PCPL} \cite{chiou2021recovering}  & 54.7 / 56.5 & 24.3 / 26.1 & 35.3 / 36.1 & 12.0 / 12.7 & 27.8 / 31.7 & 10.7 / 12.6 & 40.4 & 16.4 \\
		Motifs + CogTree \cite{yu2020cogtree} & 35.6 / 36.8 & 26.4 / 29.0 & 21.6 / 22.2 & 14.9 / 16.1 & 20.0 / 22.1 & 10.4 / 11.8 & 26.4 & 18.1 \\
		Motifs + DLFE \cite{chiou2021recovering} & 52.5 / 54.2 & 26.9 / 28.8 & 32.3 / 33.1 & 15.2 / 15.9 & 25.4 / 29.4 & 11.7 / 13.8 & 37.8 & 18.7 \\
		Motifs + EMB \cite{suhail2021energy} & 65.2 / \underline{67.3} & 18.0 / 19.5 & \underline{39.2} / \underline{40.0} & 10.2 / 11.0 & 31.7 / 36.3 & 7.7 / 9.3 & 46.6 & 12.6 \\
		\textbf{Motifs + GCL} & 42.7 / 44.4 & \underline{36.1} / \underline{38.2} & 26.1 / 27.1 & \underline{20.8} / \underline{21.8} & 18.4 / 22.0 & \underline{16.8} / \underline{19.3} & 30.1 & \underline{25.5} \\\hline
		
		VCTree \cite{tang2020unbiased} & \underline{65.4} / \underline{67.2} & 16.7 / 18.2 & \underline{\textbf{46.7}} / \underline{\textbf{47.6}} & 11.8 / 12.5 & \underline{31.9} / \underline{36.2} & 7.4 / 8.7 & \underline{\textbf{49.2}} & 12.6 \\
		VCTree + Reweight \cite{chiou2021recovering} & 60.7 / 62.6 & 19.4 / 20.4 & 42.3 / 43.5 & 12.5 / 13.1 & 27.8 / 32.0 & 8.7 / 10.1 & 44.8 & 14.0 \\
		VCTree + TDE \cite{tang2020unbiased} & 47.2 / 51.6 & 25.4 / 28.7 & 25.4 / 27.9 & 12.2 / 14.0 & 19.4 / 23.2 & 9.3 / 11.1 & 32.5 & 16.8 \\
		VCTree + {PCPL} \cite{chiou2021recovering}  & 56.9 / 58.7 & 22.8 / 24.5 & 40.6 / 41.7 & 15.2 / 16.1 & 26.6 / 30.3 & 10.8 / 12.6 & 42.5 & 17.0 \\
		VCTree + CogTree \cite{yu2020cogtree} &  44.0 / 45.4 & 27.6 / 29.7 & 30.9 / 31.7 & 18.8 / 19.9 & 18.2 / 20.4 & 10.4 / 12.1 & 31.8 & 19.8 \\ 
		VCTree + DLFE \cite{chiou2021recovering} & 51.8 / 53.5 & 25.3 / 27.1 & 33.5 / 34.6 & 18.9 / 20.0 & 22.7 / 26.3 & 11.8 / 13.8 & 37.1 & 19.5 \\
		VCTree + EMB \cite{suhail2021energy} & 64.0 / 65.8 & 18.2 / 19.7 & 44.7 / 45.8 & 12.5 / 13.5 & 31.4 / 35.9 & 7.7 / 9.1 & 47.9 & 13.5 \\ 
		\textbf{VCTree + GCL} & 40.7 / 42.7 & \underline{37.1} / \underline{39.1} & 27.7 / 28.7 & \underline{22.5} / \underline{23.5} & 17.4 / 20.7 & \underline{15.2} / \underline{17.5} & 29.6 & \underline{25.8} \\\hline
		
		SHA & 64.3 / 66.4 & 18.8 / 20.5 & 38.0 / 39.0 & 10.9 / 11.6 & 30.6 / 34.9 & 7.8 / 9.1 & 45.5 & 13.1 \\
		\textbf{SHA + GCL} & 35.1 / 37.2  & \textbf{41.6} / \textbf{44.1}  & 22.8 / 23.9 & \textbf{23.0} / \textbf{24.3} & 14.9 / 18.2 & \textbf{17.9} / \textbf{20.9} & 25.4 & \textbf{28.6} \\\hline
		
		
	    
		 
		\multicolumn{9}{c}{\multirow{2}{*}{Evaluation on GQA Dataset}}\\
		\multicolumn{9}{c}{}\\
		 
		\hline
		\multicolumn{1}{c|}{\multirow{2}{*}{Model}} & \multicolumn{2}{c|}{PredCls}& \multicolumn{2}{c|}{SGCls}& \multicolumn{2}{c|}{SGDet}& \multicolumn{2}{c}{MEAN}\\ \cline{2-9} 
		\multicolumn{1}{c|}{}& \multicolumn{1}{c}{R@50/100} & \multicolumn{1}{c|}{mR@50/100} & \multicolumn{1}{c}{R@50/100} & \multicolumn{1}{c|}{mR@50/100} & \multicolumn{1}{c}{R@50/100} & \multicolumn{1}{c|}{mR@50/100} & \multicolumn{1}{c}{R-M} & \multicolumn{1}{c}{mR-M}\\ \hline
		
		VTransE & 55.7 / 57.9 & 14.0 / 15.0 & 33.4 / 34.2 & 8.1 / 8.7 & 27.2 / 30.7 & 5.8 / 6.6 & 39.9 & 9.6 \\ 
		\textbf{VTransE + GCL} & 35.5 / 37.4 & 30.4 / 32.3 & 22.9 / 23.6 & 16.6 / 17.4 & 15.3 / 18.0 & 14.7 / 16.4 & 25.4 & 21.4 \\
		\hline
		
		Motifs & \textbf{65.3} / \textbf{66.8} & 16.4 / 17.1 & \textbf{34.2} / \textbf{34.9} & 8.2 / 8.6 & \textbf{28.9} / \textbf{33.1} & 6.4 / 7.7 & \textbf{43.9} & 10.9 \\ 
		\textbf{Motifs + GCL} & 44.5 / 46.2 & 36.7 / 38.1 & 23.2 / 24.0 & 17.3 / 18.1 & 18.5 / 21.8 & 16.8 / 18.8 & 29.7 & 24.2 \\
		\hline
		
		VCTree & 63.8 / 65.7 & 16.6 / 17.4 & 34.1 / 34.8 & 7.9 / 8.3 & 28.3 / 31.9 & 6.5 / 7.4 & 43.1 & 10.5 \\
		\textbf{VCTree + GCL} & 44.8 / 46.6 & 35.4 / 36.7 & 23.7 / 24.5 & 17.3 / 18.0 & 17.6 / 20.7 & 15.6 / 17.8 & 29.6 & 23.6 \\
		 \hline
		 
		SHA & 63.3 / 65.2 & 19.5 / 21.1 & 32.7 / 33.6 & 8.5 / 9.0 & 25.5 / 29.1 & 6.6 / 7.8 & 41.6 & 12.1 \\
		\textbf{SHA + GCL} & 42.7 / 44.5 & \textbf{41.0} / \textbf{42.7} & 21.4 / 22.2 & \textbf{20.6} / \textbf{21.3} & 14.8 / 17.9 & \textbf{17.8} / \textbf{20.1} & 27.3 & \textbf{27.3} \\
		 \hline
		
		
	\end{tabular}
	\caption{Detailed performance comparison of different methods on PredCls, SGCls, and SGDet tasks of both VG150 and GQA200 with respect to R@50/100 (\%), mR@50/100 (\%), and their mean (\%). R-M and mR-M denote the mean on all three tasks over R@50/100 and mR@50/100, respectively. The optimal results from the same baseline (\ie, VTransE, Motifs and VCTree) in VG150 are underlined. The global optimal results over all the methods in VG150 and GQA200 are in bold. The superscript  denotes that the method is reproduced. Note that all the methods are implemented on the same object detector, \ie, a pre-trained Faster R-CNN with ResNeXt-101-FPN.}
	\label{result_all}
\end{table*}

\begin{figure*}[t]
	\begin{center}
		\includegraphics[width=1.0\textwidth]{./fig/visualization_final.pdf}
	\end{center}
	\vspace{-0.4cm}
	\caption{Qualitative comparisons between SHA and SHA+GCL with regard to R@20 on PredCls setting. Green edges represent the ground truth relationships that are correctly predicted, red edges represent the ground truth relationships that are failed to be detected, and purple edges represent the reasonable relationships which are predicted by the model but are not annotated in the ground truth.}
	\vspace{-0.4cm}
	\label{visualization}
\end{figure*}

\end{document}
