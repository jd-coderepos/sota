









\documentclass[10pt, conference]{IEEEtran}
\newcommand{\matr}[1]{\mathbf{#1}} 


\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak 



\usepackage{graphicx}

\usepackage{amsmath}
\usepackage{amsfonts}     

\usepackage{amsthm}
\usepackage{nicefrac,placeins,flushend} 
 

\usepackage{amssymb}

\usepackage{hhline}
\usepackage{array}

\usepackage{multirow}
\usepackage{tabularx}

\usepackage{diagbox}

\usepackage{xcolor}
\usepackage{adjustbox} 
\usepackage{booktabs}     
\usepackage{subfig}
\usepackage{float}



















\ifCLASSINFOpdf
\else
\fi













































\DeclareMathOperator*{\argmaxA}{arg\,max} \DeclareMathOperator*{\argminA}{arg\,min} \hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}








\title{EPE-NAS: Efficient Performance Estimation Without Training for Neural Architecture Search} 



\newcommand\blfootnote[1]{\begingroup
  \renewcommand\thefootnote{}\footnote{#1}\addtocounter{footnote}{-1}\endgroup
}

\author{\IEEEauthorblockN{ Vasco Lopes\IEEEauthorrefmark{1}, Saeid Alirezazadeh \IEEEauthorrefmark{2,3},   Lu{\'{\i}}s A. Alexandre\IEEEauthorrefmark{1}}
\IEEEauthorblockA{\IEEEauthorrefmark{1}NOVA LINCS, Universidade da Beira Interior}
\IEEEauthorblockA{\IEEEauthorrefmark{2}C4-Cloud Computing Competence Center, Universidade da Beira Interior}
\{vasco.lopes, luis.alexandre\}@ubi.pt \qquad \{saeid.alirezazadeh\}@gmail.com
}




\newcommand{\ver}[1]{{\color{red}!!! #1 !!!}}  

\maketitle



\begin{abstract}
Neural Architecture Search (NAS) has shown excellent results in designing architectures for computer vision problems. NAS alleviates the need for human-defined settings by automating architecture design and engineering. However, NAS methods tend to be slow, as they require large amounts of GPU computation. This bottleneck is mainly due to the performance estimation strategy, which requires the evaluation of the generated architectures, mainly by training them, to update the sampler method. In this paper, we propose EPE-NAS, an efficient performance estimation strategy, that mitigates the problem of evaluating networks, by scoring untrained networks and creating a correlation with their trained performance. We perform this process by looking at intra and inter-class correlations of an untrained network. We show that EPE-NAS can produce a robust correlation and that by incorporating it into a simple random sampling strategy, we are able to search for competitive networks, without requiring any training, in a matter of seconds using a single GPU. Moreover, EPE-NAS is agnostic to the search method, since it focuses on the evaluation of untrained networks, making it easy to integrate into almost any NAS method.
\end{abstract}






\IEEEpeerreviewmaketitle








\section{Introduction}
In the past years, deep learning algorithms have been extensively researched, and efficiently applied to various tasks with excellent results \cite{deng2014deep, goodfellow2016deep}, especially those related to computer vision \cite{voulodimos2018deep}. The great success in computer vision tasks is mainly attributed to the advent of Convolutional Neural Networks (CNNs) \cite{khan2020survey}, given their robust feature extraction capability and transferability between different problems. Different CNNs architectures have gradually been proposed, incrementally showing that CNNs can be improved, by revising the architecture itself, adding additional components such as residual connections, reducing the number of parameters, the size or inference time \cite{DBLP:conf/nips/KrizhevskySH12, DBLP:journals/corr/SimonyanZ14a, DBLP:conf/cvpr/SzegedyLJSRAEVR15, DBLP:conf/cvpr/HeZRS16, DBLP:conf/cvpr/HuangLMW17, DBLP:conf/icml/TanL19}. However, designing efficient architectures is extremely time-consuming. It requires expert knowledge and trial and error. Deep neural networks can have many design choices, such as layers, their combination and sequence, parameters associated with the layers, architecture, and the training procedure as well as optimization rules. Therefore, an automated way to conduct neural architectures' design came as a natural process \cite{hutter2019automated}.


Neural Architecture Search (NAS) aims to automate architecture engineering and design, by autonomously designing high performance architectures for a given problem \cite{elsken2019neural}. NAS methods for computer vision problems have been successfully applied to various tasks, such as image classification, semantic segmentation, object detection, and others \cite{elsken2019neural, DBLP:journals/corr/abs-1905-01392}. Since the incipience proposal \cite{DBLP:conf/iclr/ZophL17}, NAS methods broadly focused on designing architectures using a similar flow. A controller, using a specified search strategy, being the most common Reinforcement Learning or Evolutionary Strategies, samples an architecture  from the space of possible architectures , which is defined by the search space, that comprises the possible operations (e.g., convolution, pooling) and the architecture type. The generated architecture is evaluated, and the result is given as a reward to the controller to update its parameters. This process is repeated thousands of times, whereby the controller learns to sample better architectures over time. A visualization of this process can be seen in Fig. \ref{fig:nasdiagram}.

\begin{figure}[!t]
    \centering
    \includegraphics[width=1\columnwidth]{images/NAS_diagram.pdf}
    \caption{General Neural Architecture Search flow. A controller generates an architecture  from the search space of possible operations and architectures, , which is then evaluated, and its performance is used as reward to update the controller. Our method acts in the performance estimation block, by obtaining estimates without training. \label{fig:nasdiagram}}
\end{figure}

Although NAS methods have shown excellent results, the computational cost of most methods is extremely high, which in some cases can be in the order of months of GPU computation \cite{DBLP:conf/iclr/ZophL17, liu2018progressive, Zoph_2018}. This is mainly associated with the performance estimation strategy, which evaluates the generated architectures based on regular training, either from scratch until convergence or partial training \cite{runge2018learning, real2019aging}. Recent approaches, attempt to smooth the training process, by sharing parameters \cite{pham2018efficient}, applying mutations to already trained networks \cite{DBLP:conf/iclr/ElskenMH19}, or by using one-shot NAS, where the controller generates architectures and corresponding weights \cite{DBLP:conf/iclr/LiuSY19, DBLP:conf/iclr/CaiZH19, DBLP:conf/iclr/ZelaESMBH20}. However, some NAS proposals have shown to be overfitting the search space and not allowing exploration due to the introducing of design bias \cite{Yang2020NAS}.


To mitigate the aforementioned problems, in this paper, we propose EPE-NAS, a performance estimation strategy that scores generated networks at initialization stage, without requiring any training. By evaluating how the gradients of the network behave with respect to the input, it is possible to score \textbf{untrained} networks, eliminating the need to train generated architectures to update parameters. The proposed method is extremely fast, allowing the analysis of thousands of networks in seconds. We show that this method can be used to guide the search over the search space due to its fast inference of a network trained accuracy from its untrained state. The proposed method can be easily integrated into almost any NAS method, by entirely replacing the performance estimation strategy, or complementing it, by creating a multi estimation strategy. We show this by incorporating the proposed method into a random search strategy, achieving competitive results in seconds. The code for the proposed method is also available\footnote{Code publicly available on GitHub: www.github.com/VascoLopes/EPENAS}.

The main contributions of this paper can be summarized as follows:
\begin{itemize}
    \item We propose a novel performance estimation strategy that can evaluate the trained performance of an  untrained network, which can be easily integrated into almost any NAS method.
    \item We analyze the impact of the proposed method when coupled with random search, showing that it can achieve competitive results in a few seconds.
    \item We compare the proposed method with different NAS methods, as well as with a surrogate performance estimator in NAS-Bench-201.
    \item We show that the proposed method allows the search space to be quickly analyzed without the need to train networks, allowing bad candidates to be weed out, by analyzing the relationship between the score and the network performance when trained.
\end{itemize}

The remainder of this work is organized as follows. Section~\ref{related_work}, contextualizes the related work. Section~\ref{proposed_method}, describes the proposed method in detail. In Section~\ref{experiments}, we present the experiments performed, the datasets and benchmark used, the results and discussion. Finally, in Section~\ref{conclusions}, a conclusion is drawn.









\section{Related Work}
\label{related_work}
Generally, NAS methods attempt to automatically design optimal CNNs using a sample-evaluate-update scheme, where a controller generates an architecture and is updated using the generated architecture performance. The problem with this is that evaluating the generated architectures is very costly. Zoph and Le \cite{DBLP:conf/iclr/ZophL17} initially formulated NAS as a reinforcement learning problem, where a controller was trained over-time to sample more efficient architectures. The problem was that this method required more than 60 years of GPU computation, as it trained all generated architectures to convergence. As follow-up work, the authors tackle this problem by performing a cell-based search in a search space with 13 operations \cite{Zoph_2018}. By focusing on designing two types of cells: normal cells (perform convolutional operations) and reduction cells (reduce input size), the authors could reduce the GPU computation to 2000 days. More, they found that cell-based architectures searched in CIFAR-10 can be transferred to ImageNet by stacking more cells. In this method, more than 20000 networks were trained and evaluated.

Similar to \cite{DBLP:conf/iclr/ZophL17}, in \cite{DBLP:conf/iclr/BakerGNR17} the authors present MetaQNN, a method based on reinforcement learning and Q-learning, where the learning agent was trained to sequentially sample CNN layers. Using a similar reinforcement learning approach, BlockQNN \cite{zhong2018practical} focuses on sampling blocks of operations used to form entire networks. However, BlockQNN still required 96 GPU days of computation. ENAS \cite{pham2018efficient}, used a controller, trained with policy gradient, to discover architectures by searching for an optimal subgraph within a large computational graph. By constructing a sizeable computational graph, where each subgraph represents a network, ENAS forced all generated architectures to share their parameters. In this way, efficient search was enabled in less than one GPU computational day. The authors of \cite{DBLP:conf/iclr/LiuSY19} proposed DARTS, a gradient-based method, that by performing continuous relaxation of the search space to be continuous, it optimized architectures using gradient descent. The authors propose a bilevel gradient optimization, which jointly learns the architecture and the weights in a few GPU days. This paper served as foundation for many other one-shot methods. In \cite{Zela2020Understanding}, the authors improve DARTS generated architecture performances by introducing regularization mechanisms. Also using a differentiable approach, GDAS \cite{dong2019searching} is a method that makes the search procedure differentiable, so that sub-graphs can be sampled from the directed acyclic graph representing the search space, which can be trained end-to-end to sample efficient networks. GDAS' controller is optimized based on the validation loss of the trained sampled architecture. SETN \cite{DBLP:conf/iccv/Dong019a}, also uses a differentiable approach, but uses an evaluator trained to indicate the probability of each architecture to have a low validation loss, which allows selective sampling of networks. REA \cite{DBLP:conf/aaai/RealAHL19}, instead of reinforcement learning or gradient-based methods, focuses on using evolutionary tournament selection algorithms with an age property that favors younger architectures.


To mitigate the bottleneck of the performance estimation strategy, surrogate methods have also been proposed to extrapolate the learning curve with a partial train \cite{domhan2015speeding,DBLP:conf/iclr/BakerGRN18}, or by learning a HyperNet that generates weights based on the architecture \cite{DBLP:conf/iclr/BrockLRW18}. In \cite{DBLP:conf/icml/FalknerKH18}, the authors propose BOBH, that focuses on hyperparameter optimization, which includes architecture design, by combining Bayesian optimization and bandit-based methods. However, it still required 33 GPU days to design an optimal network in CIFAR-10.


Our work differentiates from the aforementioned, being closer to NAS-WOT \cite{mellor2020neural}, as our focus is to evaluate a generated network, without requiring any training, neither for the performance estimation strategy, nor for the generated networks. Thus creating a score that correlates an \textbf{untrained} network to its performance once trained, in efficient time.





\section{Proposed Method}
\label{proposed_method}
In this paper, we propose EPE-NAS, a novel performance estimation strategy, whose goal is to estimate the performance of generated networks without requiring any training, neither for the generated networks nor for the performance estimator. To do this, we score \textbf{untrained} networks as an indicator of their accuracy when trained.


We base our approach on the idea proposed in \cite{mellor2020neural}, which states that different networks can be compared by evaluating their behavior using local linear operators at different data points. The local linear operators are obtained by multiplying the linear maps at each layer interspersed with the binary rectification units. To do this, one can define a linear map, , which maps the input , through the network, , where  represents an image that belongs to a batch , and  is the input dimension. Then, the linear map can be computed using:




In order to evaluate how a network behaves with different data points, we calculate the Jacobian matrix  for different data points, , of the batch , :


The Jacobian Matrix  contains information about the network output with respect to the input for several data points. We then can evaluate how points belonging to the same class correlate with each other, where the goal is to see if an \textbf{untrained} network is capable of modeling complex functions. Explicitly, a flexible network should simultaneously be able to distinguish local linear operators for each data point, but also have similar results for similar data points, which in a supervised approach means that the data points belong to the same class. The perfect scenario would be to have an \textbf{untrained} network with low correlation between different data points, where data points of the same category are closer to each other, which means that the network would easily learn to distinguish the two data points during training. To evaluate this behavior, we evaluate the correlation of  values with respect to their class, by computing a covariance matrix for each class present in : , where  is the matrix with elements: 


\noindent where  represents the class, , and  is the number of classes present in the batch. Then, it is possible to calculate the correlation matrix per class, , for each covariance matrix : , where  represents the  element of the matrices.

Each individual correlation matrix allows the analysis of how the \textbf{untrained} network behaves for each class, which may be an indication of the ability of the local linear operators to perceive differences between classes.

To allow comparison between the different individual correlation matrices, as they may have different sizes due to the number of data points per class, they are individually evaluated:



\noindent where  is a small-constant with the value of , and  is the number of classes in batch . To avoid confusion with absolute value operation, we denote  as the number of elements of the set X.
The normalization based on the size of the correlation matrix is due to the fact that for a constant batch size, as the number of classes increases, the size of the individual correlation matrices becomes smaller, a correlation matrix with a larger size would obtain a larger value.

Then, a network is scored based on the individual evaluations of the correlation matrices by:



\noindent where E is the vector containing all the correlation matrices' scores. Depending on the number of classes present in the batch, the final score is either a sum of the individual correlation matrices' scores or a normalized pair-wise difference. Normalization serves to mitigate the class difference when evaluating networks in datasets with a high number of classes and noise.







\hfill
\section{Experiments}
\label{experiments}
We evaluate the effectiveness of EPE-NAS on three datasets: CIFAR-10, CIFAR-100 and ImageNet16-120 from NAS-Bench-201, using a batch size of 256. As the proposed method is a performance estimation strategy that does not require any training, we also evaluate EPE-NAS by combining it with a random search strategy \cite{li2020random}, where a candidate network from the search space is randomly proposed and scored using the proposed performance estimation method, instead of training the network. This evaluation is done for different sample sizes of  networks.

The setup for all the experiments conducted was a desktop computer, with a single 1080Ti GPU and 32GB of ram. In the following sections, we comment on NAS-Bench-201, and individually detail the experiments, results and provide a discussion.




\begin{table*}[htb]
	\caption{Comparison of several search methods evaluated using the NAS-Bench-201 benchmark. Performance shown in accuracy with meanstd, on CIFAR-10, CIFAR-100 and ImageNet-16-120. Methods are divided into 4 blocks, depending on their approach: weight sharing, non-weight sharing, training-free approaches (with a direct comparison between the proposed method and NAS-WOT), and a baseline using an SVM as a surrogate estimator. 
		Search times are the mean time required to search for cells in CIFAR-10, using a single 1080Ti GPU. Search time includes the time taken to train networks as part of the process where applicable. The performances of the training-free approaches are given for different sample size~\texttt{N}. For each sample size, we also report the optimal network. Table adapted from \cite{mellor2020neural}, with reported results for non-weight and weight sharing methods from~\cite{Dong2020NAS-Bench-201}.}
	\vspace{2mm}
	\label{table:benchmarking}
	\footnotesize
	
	\setlength{\tabcolsep}{3pt}
	\setlength{\arrayrulewidth}{2pt}
	
	\renewcommand{\arraystretch}{1.3}
	\resizebox{\textwidth}{!}{\begin{tabular}{@{}lr@{\hskip 0.15in}llcllcll@{}} \hline 
			\multirow{2}{*}{Method} & \multirow{2}{*}{\shortstack{Search \\Time (s)}}  & \multicolumn{2}{c}{CIFAR-10} & \phantom{ab} & \multicolumn{2}{c}{CIFAR-100} & \phantom{ab} & \multicolumn{2}{c}{ImageNet-16-120} \\
			\cmidrule{3-4} \cmidrule{6-7} \cmidrule{9-10}
			& & \multicolumn{1}{c}{validation} & \multicolumn{1}{c}{test} && \multicolumn{1}{c}{validation} & \multicolumn{1}{c}{test} && \multicolumn{1}{c}{validation} & \multicolumn{1}{c}{test} \\
			\midrule
			\multicolumn{10}{c}{\textbf{Non-weight sharing}}\\
			REA       &  12000 & 91.190.31 & 93.920.30 && 71.811.12 & 71.840.99 && 45.150.89 & 45.541.03 \\
			RS        &  12000 & 90.930.36 & 93.700.36 && 70.931.09 & 71.041.07 && 44.451.10 & 44.571.25 \\
			REINFORCE &  12000 & 91.090.37 & 93.850.37 && 71.611.12 & 71.711.09 && 45.051.02 & 45.241.18 \\
			BOHB      &  12000 & 90.820.53 & 93.610.52 && 70.741.29 & 70.851.28 && 44.261.36 & 44.421.49 \\
			\midrule \midrule
			
			\multicolumn{10}{c}{\textbf{Weight sharing}}\\
			RSPS        & 7587  & 84.161.69 & 87.661.69 && 59.004.60 & 58.334.34 && 31.563.28 & 31.143.88 \\
			DARTS-V1    & 10890 & 39.770.00 & 54.300.00 && 15.030.00 & 15.610.00 && 16.430.00 & 16.320.00 \\
			DARTS-V2    & 29902 & 39.770.00 & 54.300.00 && 15.030.00 & 15.610.00 && 16.430.00 & 16.320.00 \\
			GDAS        & 28926 & 90.000.21 & 93.510.13 && 71.140.27 & 70.610.26 && 41.701.26 & 41.840.90 \\
			SETN        & 31010 & 82.255.17 & 86.194.63 && 56.867.59 & 56.877.77 && 32.543.63 & 31.904.07 \\
			ENAS        & 13315 & 39.770.00 & 54.300.00 && 15.030.00 & 15.610.00 && 16.430.00 & 16.320.00 \\
			\midrule
			\midrule
			\multicolumn{10}{c}{\textbf{Training-free}}\\
			NAS-WOT (\texttt{N=10}){}  & 3.1 & 89.560.56 & 92.470.04 && 69.361.55 & 69.201.05 && 42.081.61 & \textbf{42.20}\textbf{1.37} \\
			\textbf{Ours} (\texttt{N=10})  & \textbf{2.3}   & 89.900.21 & \textbf{92.63}\textbf{0.32}  && 69.782.44  & \textbf{70.10}\textbf{1.71} && 41.733.60       & 41.924.25 \0.1cm]
			
			NAS-WOT (\texttt{N=500}){} & 126.8 & 88.730.81 & 91.711.37 && 67.621.61 & 67.542.23 && 39.373.01 & 39.843.68 \\ 
			\textbf{Ours} (\texttt{N=500}) & \textbf{105.8} & 88.171.35 & \textbf{92.27}\textbf{1.75} && 69.230.62 & \textbf{69.33}\textbf{0.66} && 41.933.19       & \textbf{42.05}\textbf{3.09} \\ [0.1cm]
			
			NAS-WOT (\texttt{N=1000}){}& 252.6 & 89.600.90 & 91.202.04 && 68.570.41 & 68.950.72 && 38.011.66 & 38.081.58 \\
			\textbf{Ours} (\texttt{N=1000})& \textbf{206.2} & 87.870.85 & \textbf{91.31}\textbf{1.69} && 69.440.83 & \textbf{69.58}\textbf{0.83} && 41.862.33       & \textbf{41.84}\textbf{2.06} \\
			
\midrule
Optimal (\texttt{N}=10)  & N/A & 90.000.95 & 93.410.45  && 70.111.70  & 70.111.70   && 44.671.87       & 44.671.87 \\
			Optimal (\texttt{N}=100) & N/A & 91.120.11 & 94.120.21  && 72.730.78  & 72.730.78   && 46.310.47       & 46.310.47 \\
			Optimal (\texttt{N}=500) & N/A & 91.150.12 & 94.130.22  && 72.830.64  & 72.830.64   && 46.060.66       & 46.060.66 \\
			Optimal (\texttt{N}=1000)& N/A & 91.240.21 & 94.190.15  && 72.920.53  & 72.920.53   && 46.570.59       & 46.570.59 \\ 
			\midrule
			\midrule
			\multicolumn{10}{c}{\textbf{Surrogate Estimator with Training}}\\
			SVM (\texttt{N}=10)   & 359426.3  & 89.741.10 & 92.800.97  && 65.216.48  & 65.466.37   && 37.508.56       & 37.318.66 \\
			SVM (\texttt{N}=100)  & 359449.4 & 87.032.33 & 92.681.47  && 62.825.75  & 63.255.70   && 41.573.55       & 41.733.55 \\
			SVM (\texttt{N}=500)  & 359547.7& 87.372.63 & 93.050.71  && 66.834.34  & 67.364.28   && 41.841.38       & 41.491.39 \\
			SVM (\texttt{N}=1000) & 359666.2& 87.063.14 & 91.242.28  && 68.400.48  & 69.020.84   && 41.321.31       & 41.191.29 \\ 
			\midrule
		\end{tabular}
	}
	~Results obtained by running the author's publicly available code 3 times with the same settings as the proposed method.\\
	~Includes the time required to train, which was done using information of the performance of 100 fully trained networks, which collectively required 4.16 training days to train.
\end{table*}

\FloatBarrier


\subsection{NAS-Bench-201}
NAS methods tend to be hard to reproduce, compare with other methods, and evaluate their real performance on common search spaces \cite{lindauer2020best}. Increases in search spaces size, result in increasing the number of possible networks that can be generated, which using performance estimation strategies that require some type of training makes exhaustively evaluating the performance of NAS methods extremely hard, ultimately resulting in evaluations using subsets of the whole search space (thousands of networks in a search space that can ultimately be unbounded). It is crucial that methods smooth the reproducibility process by adopting common training procedures and settings \cite{lindauer2020best}.

Recently, NAS benchmarks have been proposed, where the goal is to have a controlled setting, where information about the training and final performance of possible networks under the proposed search space is provided, allowing rapid prototyping and comparison between different NAS methods using the same search space, training procedures and hyper-parameters \cite{DBLP:conf/icml/YingKCR0H19, DBLP:conf/iclr/ZelaSH20, DBLP:journals/corr/abs-2008-09777,DBLP:conf/iclr/Dong020}.

In this work, we used NAS-Bench-201 \cite{DBLP:conf/iclr/Dong020} to evaluate the proposed method. NAS-Bench-201 provides information about trained networks in three different datasets: CIFAR-10, CIFAR-100 and ImageNet16-120, with fixed splits, and also provide results of several NAS methods under its constraints, allowing direct comparison. In this benchmark, the goal is to design cell-based architectures, where each cell is comprised of 6 edges and 4 nodes. All nodes receive an input edge from all the preceding nodes. The edges represent the possible operations, which are selected from a pool of 5 operations: (1) zeroize, which zeros the information, (2) skip connection, (3)  convolution, (4)  convolution, and (5)  average pooling layer. The number of possible operations and edges means that there are  possible cells. The final networks are comprised of a fixed macro skeleton, where a cell is a replicated block in the network, meaning that there are as many networks as possible cells, as the only change in the macro skeleton is the cell to be replicated.



\subsection{Results and Discussion}
First, we evaluate the effectiveness of the proposed method, by randomly sampling 1000 networks from each dataset of NAS-Bench-201 and score them to see the correlation between the score and the networks' performance when trained. This evaluation can be seen in Fig. \ref{fig:plotscoredatasets}, where the first row refers to CIFAR-10, the second to CIFAR-100 and the third to ImageNet16-120. On the left, it is possible to see a strong correlation between the score given by EPE-NAS and the network's accuracy once trained. This validates the proposed method by showing that scoring untrained networks highly is indicative of a higher performance than the networks scored lower. From this, it is also possible to see that by defining thresholds, e.g.,  in CIFAR-10, we can efficiently weed out bad candidates, which is of utmost importance to methods based on evolution, where this method can be used to select the best networks from a pool of generated networks that will serve the purpose of generating the next iteration of the evolution. More, this information can also serve to guide the search of large search spaces, by directly indicating which network configurations are better. This is important because searching for networks in large search spaces, possibly unbounded, is extremely difficult and prone to converge to \textit{local minimas}, mainly due to lack of information about the search space which ultimately leads search methods to converge fast to the best networks initially sampled.





Then, by combining EPE-NAS with a random search strategy, we can compare the effectiveness of a simple search strategy coupled with the proposed performance estimation strategy against other NAS methods. To perform this experiment, a network is randomly proposed, and instead of training it, we evaluate its performance by scoring the network. This setup requires no training, and we can perform this for different sample sizes (, where  represents the number of networks evaluated). Table \ref{table:benchmarking} shows the results for EPE-NAS with random search, and compares it with several methods. Methods that perform the search without weight sharing are shown in the first block, whereas weight sharing methods are shown in the second block. In the third block, we present the results for the proposed method and directly compare it with NAS-WOT \cite{mellor2020neural}, while also showing the optimal network in each setting where our method and NAS-WOT were evaluated. Finally, in the last block, we show a baseline method based on a Support Vector Machine (SVM). The SVM approach is indicative of a possible surrogate model that is trained with information of the 100 trained networks performances. The SVM input was created by computing a single correlation matrix of the batch and then calculating the eigenvalues of the matrix. Denote that for training the 100 networks, 4.16 days of GPU computation were required. After finalizing the SVM training, there is no need to further train any network, as the SVM infers the performance based on untrained networks' correlation matrix.

From this table, it is possible to see that our proposed method requires orders of magnitude less time to search for efficient networks, while both non-weight sharing and weight sharing incur in a large search time cost. Our method also achieves better results throughout all datasets than weight sharing methods, except for GDAS on CIFAR-10 and CIFAR-100. However, our method is more than  faster. The non-weight sharing methods outperform our method (random search coupled with the proposed performance estimation strategy), but our method is still on pair with them, being capable of achieving competitive results in all datasets. As for the direct comparison with NAS-WOT, in Table \ref{table:benchmarking} it is also shown that the proposed method outperforms NAS-WOT both in terms of inference, being faster in all settings, and in terms of accuracy, being capable of selecting high performant networks in CIFAR-10 and CIFAR-100 in the settings where the sample size is 10 and 100, and CIFAR-10, CIFAR-100 and ImageNet-16-120 for higher sample sizes (500 and 1000). It is important to note that for NAS-WOT, as sample size increases, it increasingly suffers from noise, increasing the gap between the chosen network accuracy and the optimal result and decreasing the performance compared to smaller sample sizes. The opposite happens with our method. As the sample size increases, our method is capable of selecting high performant networks without losing precision, which is of extreme importance, as it is improbable that optimal networks are present in small sample sizes. More focused on ImageNet-16-120, which is a dataset with more noise, due to the image sizes () and the high number of classes, our method can select networks that attain excellent test accuracies, when compared with no weight sharing methods and NAS-WOT.


As can be seen in the second column of Table \ref{table:benchmarking}, the execution time of our method is a great advantage, as it is capable of evaluating 1000 networks in 206 seconds. To further evaluate the gains in terms of execution time compared to NAS-WOT, we explored how both methods behave in scoring a network, with a batch size of increasingly different image sizes, which can be seen in Fig. \ref{fig:plottimes}. This evaluation shows that the proposed method consistently outperforms NAS-WOT, and that it is capable of evaluating images with sizes  in approximately 5 seconds, meaning that the proposed method can also serve as an improvement for current NAS methods that solely search for networks in CIFAR-10, due to the reduced image size, and then transfer the best networks to ImageNet settings. Thus, NAS methods that were incapable of searching using larger datasets due to time complexity, can use EPE-NAS to search networks in larger datasets directly.


\begin{figure}[t]
    \centering
    \includegraphics[width=1\columnwidth]{images/plot_times.pdf}
    \caption{Comparison of the time, in seconds, required to score 1 network using our proposed method (in blue) against NAS-WOT, for different image sizes (x-axis). The image size represents the image's width and height, as the images evaluated are square and with 3 channels (RGB). \label{fig:plottimes}}
\end{figure}


The reason why the proposed method is capable of outperforming NAS-WOT in terms of time is directly linked with the time complexity of creating a correlation matrix, which is highly dependant on the number of data points and features. By evaluating individual correlation matrices, one per class, we reduce each correlation matrix's size, allowing for faster computations.

Considering the mean time required to evaluate 1000 networks by our method (Table \ref{table:benchmarking}), EPE-NAS also allows exhaustive exploration of a search space, as the proposed method is capable of evaluating over 1 million architectures in just 2 days of GPU computing, under these settings. Therefore, this could be used to evaluate a search space's behaviour, giving information to the search method on how to start and proceed, which is a significant benefit when considering large, possibly unbounded, search spaces where information about their shape is limited.


An important property of the proposed method is that it can easily be incorporated in almost any NAS method either as the sole method that evaluates networks or as a complementary method to perform mixed training, where the reward to update the controller parameters (Fig. \ref{fig:nasdiagram}) is a combination of complementary evaluations (e.g., EPE-NAS score combined with the inference/latency of the network in a mobile setting \cite{tan2019mnasnet, wu2019fbnet}). More, EPE-NAS is agnostic to the search method, as it focuses on the evaluation of networks, being the perfect addition to search methods that rely on information about generated networks or to guide the search, allowing the analysis of thousands of networks in seconds.









\section{Conclusions}
\label{conclusions}
In this paper, we propose EPE-NAS, a performance estimation strategy that scores \textbf{untrained} networks with a high correlation to their trained performance. By leveraging information about the gradients of the output of a network with regards to its input, our method can accurately infer if the generated network is good in less than one second, being capable of evaluating thousands of networks in a matter of seconds. More, in this work, we have shown that using a simple random search coupled with the proposed estimation strategy, it is possible to sample high performant networks, in seconds, that can outperform many current NAS methods.

Our proposal can also contribute to allow NAS methods to search large search spaces, by providing an efficient way of extracting information about generated networks without requiring any training, and large databases, as our method is still very fast even in the presence of large image sizes. Furthermore, the proposed method is agnostic to the search strategy, allowing it to be integrated into almost any NAS method.




\section*{Acknowledgments}
This work was supported by `FCT - Fundação para a Ciência e Tecnologia' through the research grant `2020.04588.BD', partially supported by NOVA LINCS under grant `UID/EEA/50008/2019' and and partially supported by operation Centro-01-0145-FEDER-000019 - C4 - Centro de Competencias em Cloud Computing, cofinanced by the European Regional Development Fund (ERDF) through the Programa Operacional Regional do Centro (Centro 2020), in the scope of the Sistema de Apoio à Investigação Cientifíca e Tecnologica - Programas Integrados de IC\&DT.


\begin{figure}[H]
\vspace{-12pt}
  \centering
  \raisebox{-0.5\height}{\subfloat{\includegraphics[width=0.49\columnwidth, keepaspectratio]{images/scores_plot/cifar10_3_cropped.pdf}\label{fig:score_cifar10}}}
  \raisebox{-0.5\height}{\subfloat{\includegraphics[width=0.49\columnwidth, keepaspectratio]{images/cells/cell_cifar10.pdf}\label{fig:cell_cifar10}}}\\
  \vspace{-0.8em}
  \raisebox{-0.5\height}{\subfloat{\includegraphics[width=0.49\columnwidth]{images/scores_plot/cifar100_3_cropped.pdf}\label{fig:score_cifar100}}}
  \raisebox{-0.5\height}{\subfloat{\includegraphics[width=0.49\columnwidth]{images/cells/cell_cifar100.pdf}\label{fig:cell_cifar100}}}\\ 
  \vspace{-0.75em}
  \raisebox{-0.5\height}{\subfloat{\includegraphics[width=0.49\columnwidth]{images/scores_plot/imagenet_3_cropped.pdf}\label{fig:score_in16120}}}
  \raisebox{-0.5\height}{\subfloat{\includegraphics[width=0.49\columnwidth]{images/cells/cell_imagenet.pdf}\label{fig:cell_in16120}}}\\

  \caption{Plots of scoring 1000 random untrained networks using the proposed method against the final accuracy when the networks are trained, using the 3 different datasets. On the right, the cell with the highest score, in each setting, is shown.\label{fig:plotscoredatasets}}

\end{figure}



\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
