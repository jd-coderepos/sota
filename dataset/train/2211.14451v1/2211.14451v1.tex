\documentclass{bmvc2k}


\usepackage[T1]{fontenc}
\usepackage{epstopdf}


\usepackage{varwidth}
\usepackage{array}
\newcolumntype{x}[1]{>{\centering\arraybackslash\hspace{0pt}}p{#1}}







\title{GLAMI-1M:\\A Multilingual Image-Text Fashion Dataset}
\addauthor{Václav Košař}{vaclav.kosar@glami.cz}{1}
\addauthor{Antonín Hoskovec}{antonin.hoskovec@glami.cz}{1,3}
\addauthor{Milan Šulc}{milan.sulc@rossum.ai}{2}
\addauthor{Radek Bartyzal}{radek.bartyzal@glami.cz}{1}

\addinstitution{
 GLAMI.cz\\
 Křižíkova 148/34, 186 00 Prague 8,\\
 Czech Republic
}
\addinstitution{
Rossum.ai\\ Křižíkova 148/34, 186 00 Prague 8,\\
Czech Republic
}
\addinstitution{
FNSPE, Czech Technical University in Prague,\\
Břehová 7, 119 15, Prague 1,\\
Czech Republic
}

\runninghead{Košař, Hoskovec, Šulc, Bartyzal}{GLAMI-1M}

\begin{document}
\newcolumntype{M}{>{\begin{varwidth}{3.5cm}}l<{\end{varwidth}}}











\newgeometry{twoside,headsep=3mm,papersize={410pt,620pt},inner=15mm,outer=6mm,top=3mm,includehead,bottom=1mm,heightrounded}



\maketitle              \begin{abstract}
We introduce GLAMI-1M: the largest multilingual image-text classification dataset and benchmark. The dataset contains images of fashion products with item descriptions, each in 1 of 13 languages. Categorization into 191 classes has high-quality annotations: all 100k images in the test set and 75\% of the 1M training set were human-labeled. The paper presents baselines for image-text classification showing that the~dataset presents a challenging fine-grained classification problem: The best scoring EmbraceNet model using both visual and textual features achieves 69.7\% accuracy. Experiments with a modified Imagen model show the dataset is also suitable for image generation conditioned on text.
The dataset, source code and model checkpoints are published at: \url{https://github.com/glami/glami-1m}. 
\end{abstract}




\begin{figure}[h!]
    \centering
    \vspace*{-0.6cm}
    \setlength{\tabcolsep}{1pt}
\renewcommand{\arraystretch}{0.8}
\small
\begin{tabular}{x{3cm} x{3cm} x{3cm} x{3cm}}
         \includegraphics[height=3.0cm]{examples/289757831.jpg}
 &  \includegraphics[height=3.0cm]{examples/148592644.jpg}
 & \includegraphics[height=3.0cm]{examples/287967797.jpg}
 & \includegraphics[height=3.0cm]{examples/319076698.jpg} \\
 ANKA KEMER Kadın Heybe Çantalı Kemer 16x14 cm
 & Pánská kotníková obuv Mustang 4107-605-820 modrá
 & Ženski kopalni plašč DKaren Basic & Pilgrim Auskarai 'THANKFUL' sidabrinė \\
 (Turkey) & (Czechia) & (Slovenia) & (Lithuania) \\
 \textit{`womens-belts`} & \textit{`mens-boots`} & \textit{`womens-bathrobes`} & \textit{`womens-earrings`} \\

\end{tabular}
\caption{Examples from GLAMI-1M with their \textit{image}, \textit{name}, \textit{country} and \textit{class}. Available information not displayed: \textit{description}, \textit{label source} and \textit{item-ID}.}
\label{fig:exampleFront}
\end{figure}
\vspace*{-.5cm}

\restoregeometry



\section{Introduction}
Public datasets are a cornerstone of machine learning research: Cross-evaluation of different methods is possible thanks to public benchmarks with pre-defined training and test data splits. 
Pushing the envelope in machine learning often relies on considerable amount of training samples. For example, while the existence of Convolutional Neural Networks dates back to the 1980s \cite{fukushima1980neocognitron,lecun1989backpropagation}, the deep learning era in computer vision started with the success \cite{krizhevsky2012imagenet} on the ILSVRC 2012 challenge dataset \cite{ILSVRC15} commonly addressed as ImageNet. At the time of writing this paper, the best results reported\footnote{\url{https://paperswithcode.com/sota/image-classification-on-imagenet}} on ImageNet were achieved by an image-text model CoCa \cite{coca}, pre-trained on proprietary large-scale datasets JFT-3B \cite{jft3b} and ALIGN \cite{align} to produce joint image-text representation. Similarly, CMA-CLIP \cite{Liu2021CMACLIPCA} incorporated CLIP \cite{clip}, an ALIGN model \cite{align} predecessor, trained on proprietary WebImageText to achieve state-of-the-art image-text classification results on Fashion-Gen \cite{fashiongen}. These results suggest that image-text models have a great potential to aid image-based classification. 

Owing to the success of multilingual models \cite{bert,xml} and multimodal models \cite{clip,align}, datasets combining both multilingual and multimodal features are increasingly relevant to machine learning research (see \autoref{tab:multilingual}). 
However, public large scale image-text classification datasets \cite{recipe1m+,fashiongen,upmcFood101,Xie2019VisualEA} are still of rather limited size and language diversity (see \autoref{tab:imageTextClassification}). Note that, Recipe1M+ is not human annotated, rather its categories are extracted from recipe titles using statistical methods. In particular within the fashion domain, to the best of our knowledge, there is no large diverse multilingual text and image dataset (see \autoref{tab:fashion}) and machine translation cannot replace human produced text (yet).

In this paper, we introduce GLAMI-1M: the largest multilingual image-text classification dataset and benchmark. The dataset contains images of fashion products with item descriptions from an e-commerce platform. GLAMI-1M is a collection of 1.11M records representing a fashion product with an image, a name and description in one of 13~languages and a category within the GLAMI fashion search engine\footnote{at the point of extraction in 2022.}. 
Categorization into 191 classes has high-quality annotations: all 100k images in the test set and 75\% of the 1M training set were human-labeled. 

Organizing products from public listings into categories is an important problem in e-commerce platforms. 
Data from online production systems pose several challenges: dealing with imbalanced long-tailed class distributions \cite{shopify}, prior shift \cite{priorShift,vsipka2022hitchhiker}, noisy labels in case of rule-based annotations \cite{Sun2014ChimeraLC,distillingFromNoise} (as opposed to human labels), multimodal inputs \cite{shopify,foodi}, multilingual text \cite{shopify,foodi}, and utilizing available metadata \cite{df20}.





Datasets for related tasks and domains are reviewed in \autoref{sec:relatedWork}. The GLAMI-1M dataset and benchmark is introduced in \autoref{sec:GLAMI-1M}, including detailed analysis of its content and description of its creation. Baseline methods for image-text classification and text-conditional image generation are introduced in \autoref{sec:experiments}.  Additional details about the dataset and the experiments, and baselines for machine translation are provided in the supplementary material.

\section{Related Work}
\label{sec:relatedWork}

Large-scale image and multilingual text datasets are listed in \autoref{tab:multilingual}.
GLAMI-1M is the largest multilingual dataset for image-text classification.
Larger image-text datasets LAION-5B \cite{LAION5B}, WIT \cite{wit}, FooDI-ML \cite{foodi} are used for image-text retrieval, and miss standardized class labels.
Note that in \autoref{tab:multilingual}, we do not list translations of MS-COCO \cite{coco} such as  \cite{japaneseCoco,italianCoco,germanCoco,dutchCoco,chineseCoco,vietnameseCoco} as they are distributed in bilingual form, which does not pass the table's minimum of 3 languages. 

\begin{table}[htb]
\small
\centering
\caption{\label{tab:multilingual} Publicly available multilingual image-text datasets. Datasets with <3 languages and with <10k images or texts are omitted. The column task gives the most relevant task.}
\vspace{1mm}
    \setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1}
\begin{tabular}{|l|l|l|l|l|m{2.9cm}|}
\hline
Dataset                          & Images        & Texts         & Langs     & Domain           & Task \\
\hline
LAION-5B \cite{LAION5B}          & 5.85B          & 5.85B         & 100+     & Web images        & image-text retr. \\
YFCC100M \cite{YFCC100M}         & 100M           & 100M          & 172      & Web images        & image-text retr. \\
WIT \cite{wit}                   & 11.5M          & 37.6M         & 108      & Wiki images       & image-text retr. \\
FooDI-ML \cite{foodi}            & 1.5M           & 9.5M          & 33       & Food, groceries   & text-image retr. \\
GLAMI-1M  & 968k          & 1.01M         & 13       & Fashion           & classification   \\
MultiSub (I4) \cite{multisub}    & 45k            & 180k         & 4        & subtitles, nouns  & fill-in-the-blank \\
Multi30k \cite{WMT18,multi30kGe,multi30kFr,multi30kCz}  & 30k    & 4 x 30k  & 4       & General           & machine translation \\
\hline
\end{tabular}
\end{table}


\begin{table}
\centering
\small
\caption{\label{tab:imageTextClassification} Publicly available image-text classification datasets.
Datasets with <30k images or texts are omitted.
}
\vspace{1mm}
\begin{tabular}{|l|l|l|l|l|l|l|}
\hline
Dataset                          & Images        & Texts         & Langs      & Domain           & Class. task    & Classes \\
\hline
Recipe1M+ \cite{recipe1m+}      & 13M           & 1M             & 1          & Recipes          & single-label    & 1047  \\
GLAMI-1M                        & 968k          & 1.01M          & 13         & Fashion          & single-label   & 191   \\
FashionGen \cite{fashiongen}     &325k           & 78k           & 1          & Fashion           & single-label  & 121 \\
UPMC Food-101 \cite{upmcFood101} &100k           & 100k          & 1          & Food              & single-label  & 101 \\
SNLI-VE \cite{Xie2019VisualEA}   & 30k            & 565k         & 1          & General           & single-label  & 3 \\
\hline
\end{tabular}
\vspace*{-5mm}
\end{table}


Large fashion datasets with image and text features are summarized in \autoref{tab:fashion}.
To the best of our knowledge, GLAMI-1M is the largest image-text dataset in terms of items and the most diverse dataset in terms of languages.
GLAMI-1M also offers the highest number of categories (191) for classification.
The only other multilingual fashion image-text dataset, Fashion-MMT \cite{fashionMMT}, is bilingual and ten times smaller in the number of items. \\




Other Fashion datasets without text annotations include:
DeepFashion2 \cite{deepfashion2} contains 800k diverse photos with clothing segmentation metadata.
Clothing-1M \cite{clothing1m} contains 1M product images with majority noisy class (14) labels.
MVC \cite{mvc} dataset of 161k items for view-invariant clothing retrieval, classification (23), colors (13), attribute prediction.
ModaNet \cite{modanet} is a 55k image segmentation dataset.
Fashionpedia \cite{fashionpedia} is a 45k image dataset with fine-grained apparel attribute (294) prediction, segmentation (27 categories, 19 parts), and an ontology.
StreetStyle \cite{streetStyle} is a 45k image dataset with various attributes including category (7).
DeepFashion3D \cite{deepfashion3d} is a 2k image to 3D reconstruction dataset with annotations including 10 categories.
Colorful-Fashion \cite{Liu2014FashionPW} is 2k image dataset for segmentation into 23 categories, 13 colors.

\begin{table}[h!]
\centering
\small
    \setlength{\tabcolsep}{2pt}
\renewcommand{\arraystretch}{1.2}
\caption{\label{tab:fashion} Overview of publicly available fashion product datasets with image and text features. GLAMI-1M is the biggest, most fine-grained, and uniquely multilingual fashion dataset.}
\vspace{1mm}
\begin{tabular}{|m{3.3cm}|c|c|m{6.4cm}|c|}
\hline
Dataset & Items & Imgs & Features & Langs\\
\hline
GLAMI-1M             & 1.11M & 968k &  image, name, description, class (191) & 13 \\
FACAD \cite{facad}             & 130k & 993K  & image, description, class (78) & 1 \\
Fashion-MMT \cite{fashionMMT}  & 110k & 853k & image, description with noisy translations, class (78), attributes & 2  \\
Fashion550k \cite{fashion550k} & 550k & 408k & image (in-the-wild), user comments, garment class, attributes, other metadata & 1  \\
Neti-look \cite{netilook}      & 350k  & 355k &  image (in-the-wild), comments & 1 \\
FashionGen \cite{fashiongen}   & 78k & 325k  & image, description, class (121) & 1 \\
Amazon Fashion Products 2020 \cite{amazonFashionProducts2020} & 132k & 132k+  & multiple images, name, other & 1 \\
Fashion IQ \cite{fashionIQ}    & 50k  & 50k & image, description, attributes, relative caption & 1 \\
Fashion Product Images \cite{fashionProductImages} & 44k & 44k & image, name, description, class, other & 1 \\
\hline
\end{tabular}
\end{table}


\begin{table}[htb]
\centering
\small
\setlength{\tabcolsep}{3pt}
\renewcommand{\arraystretch}{1.5}
\caption{\label{tab:columns} GLAMI-1M column descriptions, and unique value count in training and test sets.}
\vspace{1mm}
\begin{tabular}{ | m{1.9cm} | m{8.1cm} | r | r| }
\hline
          Name &                                                                                                                                                                                                                                                                           Description &  \# Train. &  \# Test \\
\hline
       item\_id &    Item integer identifier (Not unique). &   992528 &  116004 \\
      image\_id &       Image integer identifier. Products with duplicate images exists across different geos. &   882846 &   85577 \\
           geo &        Country code in lower case. It is a strong indicator of language used in the text. &       13 &      13 \\
          name &          Product name text. Often contains product's brand. &   752092 &  99783 \\
   description &        Product description text. It describes the product and advertises the product. &   656067 &   90313 \\
      category &        Integer category id label. &      191 &     191 \\
 category\_name &       Human readable category name label. &      191 &     191 \\
  label\_source &  Source of the class labels indicating label quality:\newline \textit{admin}, \textit{quality-check}, \textit{custom-tag}: human labels\newline \textit{combined-tag}, \textit{NaN}: machine labels -- simple rule based systems&        5 &       3 \\
\hline
\end{tabular}
\end{table}

\begin{figure}
\centering
\includegraphics[width=\textwidth,trim={0 2mm 0 0},clip]{figures/train-test-distribution-per-category-eps-converted-to.pdf}
\vspace*{-6mm}
\caption{Distribution of samples per category.
The distribution is mostly exponential, but steeper along the edges, so we regard this as a long tailed distribution.
} \label{fig:category}
\end{figure}


\section{Dataset Description}
\label{sec:GLAMI-1M}

We introduce GLAMI-1M: a 13-lingual image-text classification dataset of 1.10M items representing a product and its leaf category within GLAMI production catalog category tree.
Each item represents a product listing with:
 image, texts (name and description) in one of the 13 languages, category label and its label source.
Examples from the dataset are in \autoref{fig:exampleFront} and in the supplementary material.


\begin{table}[thb]
\centering
\caption{The 10 most and 10 least represented from the 191 total training set categories.}
\vspace{1mm}
\label{tab:top10cats}
        \setlength{\tabcolsep}{2.2pt}
\renewcommand{\arraystretch}{1.2}

\begin{tabular}{|l|r|r|}
\hline
                      Category name &  \# Train. &  \# Test \\
\hline
        mens-t-shirts-and-tank-tops &    75724 &    7497 \\
 womens-tops-tank-tops-and-t-shirts &    50000 &    6187 \\
                      mens-sneakers &    32385 &    3668 \\
                    womens-sneakers &    31137 &    2417 \\
                            dresses &    29350 &    3084 \\
                      baby-clothing &    27896 &    3631 \\
          womens-blouses-and-shirts &    25292 &    3017 \\
                       womens-pants &    24998 &    1305 \\
                            bikinis &    24712 &    5286 \\
                  womens-flip-flops &    23219 &    2612 \\
\hline
\end{tabular}
\begin{tabular}{|l|r|r|}
\hline
      Category name &  \# Train. &  \# Test \\
\hline
    mens-bath-robes &      211 &      26 \\
 mens-handkerchiefs &      200 &      11 \\
    mens-shoe-laces &      187 &       3 \\
     mens-umbrellas &      179 &      10 \\
    mens-suspenders &      171 &      19 \\
           broaches &      155 &      17 \\
        mens-chains &      122 &      16 \\
  mens-rubber-boots &       99 &      24 \\
      mens-earrings &       88 &      12 \\
     boys-tank-tops &       81 &      14 \\
\hline
\end{tabular}
\end{table}


\begin{figure}[thb]
\includegraphics[width=\textwidth]{figures/train-test-distribution-per-geo-eps-converted-to.pdf}
\caption{
Sample distribution per country (geo), which roughly approximates the language distribution.
} \label{fig:geo}
\end{figure}

Items for the dataset were selected from the GLAMI catalogue in two phases: first, we sampled items with higher-quality human annotations (i.e. based on the label source). 100k of these items were randomly sampled for the test set.
Then items with labels from less reliable rule-based (heuristic) labeling systems were sampled proportionally to the catalog category distribution, in order to get a training set of 1M items. Zero overlap between the training and test set images and texts was checked via MD5 hashes and cosine similarity threshold of CLIP embeddings \cite{clip,mclip}. See the source code and the supplementary material for details. Text was preprocessed by removing backslashes, braces, brackets, semicolons, angle brackets, and replacing line ends, carriage returns and forward slashes with a space.

\autoref{tab:columns} describes the dataset's data columns with the numbers of unique values. The training set may contain several records describing the same item (i.e. records with the same \textit{item\_id}) -- e.g. because unisex items appear in both men's and women's category variants.
The test set contains only consistent human-label annotations without such duplicate records (with same \textit{item\_id}). However, up to tens of items still have the same \textit{image\_id}, since the same products are sometimes sold by multiple e-shops within the same or different country. In these cases the items have a different \textit{item\_id}. The classes are fine-grained: 15 categories of women shoes and total 191 categories in contrast to FashionGen's 121. The class distribution is long tailed, as shown in \autoref{fig:category}. The 10 most and 10 least frequent training set categories can be found in \autoref{tab:top10cats}. \autoref{fig:geo} shows a train-test distribution shift in number of samples per country. The distribution of product name and description lengths is illustrated in  \autoref{fig:nameLength}.  For the distribution of label source, please see the supplementary material.  

The dataset is primarily shared in a compact 10GB archive with 228x298px images in JPEG format. Larger 800x800px resolution variants are available in a separate archive. 








\begin{figure}[thb]
\centering
\includegraphics[width=\textwidth]{figures/train-test-distribution-of-name-length-eps-converted-to.pdf}\\
\includegraphics[width=\textwidth,trim={0 2.5mm 0 0},clip]{figures/train-test-distribution-of-description-length-eps-converted-to.pdf}
\vspace*{-2mm}
\caption{Distribution of \textit{name} (top) and \textit{description} (bottom) length in characters in the training and test set. For \textit{name} the median is 38 characters, 
for \textit{description} the median is 150. Note that the last bin for description contains all the samples longer than 990 characters (up to 4000).}
\label{fig:nameLength}
\end{figure}





Together with the dataset, we set up a \textbf{public benchmark}\footnote{\label{note:leaderboard}accessible from \href{https://github.com/glami/glami-1m}{the repository}.} for multilingual image-text  classification. The benchmark's primary score is the test set accuracy. The benchmark allows using pre-trained models and additional training data, if explicitly stated in the method description. Initial results for baseline classification methods are provided in \autoref{sec:experiments_classification}.

Additionally, we provide baseline generative models for text-conditioned image generation, as described in \autoref{sec:experiments_imagen}, and baseline models and results for machine translation in the supplementary material.



\section{Experiments}
\label{sec:experiments}



\subsection{Multimodal Classification}
\label{sec:experiments_classification}
Classification is one the fundamental tasks of supervised learning \cite{sen2020supervised}. Multimodal classification models process inputs of several different modalities. In our benchmark the inputs come from three \emph{modalities}: textual (title + description), visual (image) and categorical (label source). The label source could be used as a meta information for training methods like sample weighting \cite{metaWeightNetLA} or label correction \cite{metaLabelCorrection}, however these experiments are beyond the scope of this paper. For baseline we have chosen EmbraceNet \cite{embracenet}, a robust model essentially capable of taking encoded inputs from any modality and automatically combining them into a single model. In all experiments, the model was trained for two epochs (early stopping) with the Adam optimizer and the internal EmbraceNet dimension set to 512. 

For the encoding of various modalities we have relied on well tested, publicly available, pre-trained models. We have encoded the textual inputs with the \emph{small} variant of the mT5 model \cite{xue-etal-2021-mt5}, which has been pretrained on a superset of the languages in our dataset. We encoded with maximum length of 32 tokens, which resulted in  dimensional embeddings of the concatenated title + description. For the image inputs we have used a pretrained \emph{ResNeXt-50 32x4d} model \cite{resnext}, which after the last max pooling layer gives  dimensional embeddings. We finetuned ResNext, but froze mT5.

To better understand the quality of the input features, we have trained several versions of EmbraceNet by dropping one or multiple modalities from the input and by training on human-labels only or including the noisy labels too, see \autoref{tab:embracenets}. The best top-1 accuracy of 0.697 was achieved with the combination of both text and image and by including the noisy labels, while separately the image features outperformed the textual inputs. We note that we did not tune the probabilities of the fusion process in EmbraceNet \cite{embracenet}. The probability of the docking layers of each modality being included was thus  for the bi-modal version. To see how EmbraceNet trained on images compares to the ResNeXt-50 32x4d model, we used a pre-trained ResNext and finetuned it on our dataset. Since in this case EmbraceNet essentially replaces the last fully connected layer in ResNext with several layers, thus increasing the number of parameters, the image-only version of EmbraceNet outperformed the original architecture. The presence of the noisy labels only has a small impact on the performance of EmbraceNet.

 A weak zero-shot CLIP baseline is available in the supplementary material.

\begin{table}
\centering
\caption{Top-k accuracies of EmbraceNet with various input modalities, trained either on all labels (\textit{all}) or human-labeled samples only (\textit{hum.}).}
\vspace{1mm}
\renewcommand{\arraystretch}{1.1}
\label{tab:embracenets}
\begin{tabular}{lcccc}
\hline
Included modality/model & Top-1 (all) &   Top-5 (all)  & Top-1 (hum.) &   Top-5 (hum.) \\
\hline
Text + Image & \textbf{0.697} & 0.940 & 0.694 & 0.932  \\
Image & 0.685 & \textbf{ 0.948} & 0.679 & 0.943  \\
Text &  0.593 & 0.840 &  0.613 & 0.849  \\
Finetuned ResNeXt-50 32x4d & 0.631 &  0.935 & 0.642 & 0.933 \\
\hline
\end{tabular}
\end{table}

\subsection{Text-Conditional Image Generation}
\label{sec:experiments_imagen}
The area of image generation conditioned on text has recently attracted much attention \cite{imagen,dall-e-2,rombach2022high}. Our dataset can be used for this task. We have trained a "small" version of the Imagen-like model \cite{imagen} on a single NVIDIA T4 GPU over 72 hours on 884k images and 992k texts. This underscores the position of our dataset in the matter of its size. It lies on the border between extremely large datasets, allowing to push the envelope of the state-of-the-art in machine learning,  and datasets compact enough to train a model on a single GPU in days. 

\begin{figure}[thb]
    \centering
    \caption{Cherry-picked images generated by the Imagen-like model with the corresponding country codes, 500 time steps of diffusion. Large images are the generated ones, the two smaller are the closest images from train set based on \emph{ResNeXt-50 32x4d} embeddings.}
    \label{tab:imagen}
    \vspace{1mm}
    \setlength{\tabcolsep}{1pt}
\begin{tabular}{cccccccccc}
\hline
 bg & bg & bg & bg & cz & cz & es & es & hr & hu \\
 \includegraphics[width=1.2cm]{imagen/bg-4753423_1.jpg}                                                               & \includegraphics[width=1.2cm]{imagen/bg-6978183_1.jpg}                                                               & \includegraphics[width=1.2cm]{imagen/bg-7122401_1.jpg}                                                               & \includegraphics[width=1.2cm]{imagen/bg-8786911_1.jpg}                                                               & \includegraphics[width=1.2cm]{imagen/cz-21773945_1.jpg}                             
 &
 \includegraphics[width=1.2cm]{imagen/cz-27986555_1.jpg}                                                              & \includegraphics[width=1.2cm]{imagen/es-4581291_1.jpg}                                                               & \includegraphics[width=1.2cm]{imagen/es-6155059_1.jpg}                                                               & \includegraphics[width=1.2cm]{imagen/hr-4960006_1.jpg}                                                               & \includegraphics[width=1.2cm]{imagen/hu-10466634_1.jpg}                              
 \\
 \includegraphics[width=0.6cm]{imagen/309827679.jpg}\includegraphics[width=0.6cm]{imagen/239675786.jpg} & \includegraphics[width=0.6cm]{imagen/276075753.jpg}\includegraphics[width=0.6cm]{imagen/306106196.jpg} & \includegraphics[width=0.6cm]{imagen/332368403.jpg}\includegraphics[width=0.6cm]{imagen/334321293.jpg} & \includegraphics[width=0.6cm]{imagen/305169770.jpg}\includegraphics[width=0.6cm]{imagen/311538108.jpg} & \includegraphics[width=0.6cm]{imagen/330796935.jpg}\includegraphics[width=0.6cm]{imagen/290113695.jpg} &
 \includegraphics[width=0.6cm]{imagen/232063226.jpg}\includegraphics[width=0.6cm]{imagen/184085010.jpg} & \includegraphics[width=0.6cm]{imagen/308834796.jpg}\includegraphics[width=0.6cm]{imagen/324602271.jpg} & \includegraphics[width=0.6cm]{imagen/308193851.jpg}\includegraphics[width=0.6cm]{imagen/310999429.jpg} & \includegraphics[width=0.6cm]{imagen/332128604.jpg}\includegraphics[width=0.6cm]{imagen/338907062.jpg} & \includegraphics[width=0.6cm]{imagen/234665554.jpg}\includegraphics[width=0.6cm]{imagen/206750757.jpg} \\
 lt & lt & lt & lt & ro & si & sk & sk & sk & tr                                                                                                                   \\
 \includegraphics[width=1.2cm]{imagen/lt-1537716_1.jpg}                                                               & \includegraphics[width=1.2cm]{imagen/lt-2023139_1.jpg}                                                               & \includegraphics[width=1.2cm]{imagen/lt-3322603_1.jpg}                                                               & \includegraphics[width=1.2cm]{imagen/lt-3425725_1.jpg}                                                               & \includegraphics[width=1.2cm]{imagen/ro-12758029_1.jpg}                                                              &
 \includegraphics[width=1.2cm]{imagen/si-3567349_1.jpg}                                                               & \includegraphics[width=1.2cm]{imagen/sk-1496513_1.jpg}                                                               & \includegraphics[width=1.2cm]{imagen/sk-25801301_1.jpg}                                                              & \includegraphics[width=1.2cm]{imagen/sk-9034777_1.jpg}                                                               & \includegraphics[width=1.2cm]{imagen/tr-39352245_1.jpg}                                                              \\
 \includegraphics[width=0.6cm]{imagen/312893194.jpg}\includegraphics[width=0.6cm]{imagen/302185461.jpg} & \includegraphics[width=0.6cm]{imagen/287592351.jpg}\includegraphics[width=0.6cm]{imagen/290023991.jpg} & \includegraphics[width=0.6cm]{imagen/299574824.jpg}\includegraphics[width=0.6cm]{imagen/325371443.jpg} & \includegraphics[width=0.6cm]{imagen/262718817.jpg}\includegraphics[width=0.6cm]{imagen/325442081.jpg} & \includegraphics[width=0.6cm]{imagen/307687989.jpg}\includegraphics[width=0.6cm]{imagen/149230223.jpg} &
 \includegraphics[width=0.6cm]{imagen/322490149.jpg}\includegraphics[width=0.6cm]{imagen/307223876.jpg} & \includegraphics[width=0.6cm]{imagen/256024683.jpg}\includegraphics[width=0.6cm]{imagen/179931788.jpg} & \includegraphics[width=0.6cm]{imagen/336888003.jpg}\includegraphics[width=0.6cm]{imagen/320582276.jpg} & \includegraphics[width=0.6cm]{imagen/267282137.jpg}\includegraphics[width=0.6cm]{imagen/278284984.jpg} & \includegraphics[width=0.6cm]{imagen/268692319.jpg}\includegraphics[width=0.6cm]{imagen/276082450.jpg} \\
\hline
\end{tabular}

\end{figure}


\begin{figure}[thb]
    \centering
    \caption{Random samples of images generated by the Imagen-like model with the corresponding country codes, 500 time steps of diffusion.}
    \label{tab:imagen-random}
    \vspace{1mm}
    \setlength{\tabcolsep}{1pt}
\begin{tabular}{cccccccccc}
\hline

sk &     tr &     sk &     bg &     cz &     si &     cz &     tr &     bg &     ee \\

     \includegraphics[width=1.2cm]{imagen/sk-11083578_1.jpg} & \includegraphics[width=1.2cm]{imagen/tr-46808459_1.jpg} & \includegraphics[width=1.2cm]{imagen/sk-31726057_1.jpg} & \includegraphics[width=1.2cm]{imagen/bg-9691306_1.jpg}  & \includegraphics[width=1.2cm]{imagen/cz-13409246_1.jpg} &
     \includegraphics[width=1.2cm]{imagen/si-1848589_1.jpg}  & \includegraphics[width=1.2cm]{imagen/cz-38327254_1.jpg} & \includegraphics[width=1.2cm]{imagen/tr-38678044_1.jpg} & \includegraphics[width=1.2cm]{imagen/bg-7845933_1.jpg}  & \includegraphics[width=1.2cm]{imagen/ee-324286_1.jpg}   \\
     
sk &     es &     sk &     sk &     sk &     sk &     cz &     hu &     si &     bg \\
     \includegraphics[width=1.2cm]{imagen/sk-1342970_1.jpg}  & \includegraphics[width=1.2cm]{imagen/es-6155059_1.jpg}  & \includegraphics[width=1.2cm]{imagen/sk-16409481_1.jpg} & \includegraphics[width=1.2cm]{imagen/sk-25565766_1.jpg} & \includegraphics[width=1.2cm]{imagen/sk-12039784_1.jpg} &
     \includegraphics[width=1.2cm]{imagen/sk-9441027_1.jpg}  & \includegraphics[width=1.2cm]{imagen/cz-49288966_1.jpg} & \includegraphics[width=1.2cm]{imagen/hu-12497318_1.jpg} & \includegraphics[width=1.2cm]{imagen/si-222501_1.jpg}   & \includegraphics[width=1.2cm]{imagen/bg-9390009_1.jpg}  \\

\hline
\end{tabular}

\end{figure}

\begin{figure}
    \centering
    \caption{Images generated by the Imagen-like model for the input "sneakers" translated into all 13 languages, 500 time steps of diffusion.}
    \vspace{1mm}
    \label{tab:imagen_snakers}
    \scriptsize
        \setlength{\tabcolsep}{1pt}
\renewcommand{\arraystretch}{1}
    \begin{tabular}{lllllll}
    \hline
     bg: \raisebox{-0.07cm}{\includegraphics[height=0.2cm]{labels/bg.png}}                                   & cz: tenisky                                    & ee: tossud                                     & es: las zapatillas                             & gr: \raisebox{-0.07cm}{\includegraphics[height=0.2cm]{labels/gr.png}}
     & hr: tenisice                                   & hu: tornacipő\\
     \includegraphics[width=1.8cm]{imagen/bg_1.jpg} & \includegraphics[width=1.8cm]{imagen/cz_1.jpg} & \includegraphics[width=1.8cm]{imagen/ee_1.jpg} & \includegraphics[width=1.8cm]{imagen/es_1.jpg} & \includegraphics[width=1.8cm]{imagen/gr_1.jpg} & 
     \includegraphics[width=1.8cm]{imagen/hr_1.jpg} & \includegraphics[width=1.8cm]{imagen/hu_1.jpg} 
     \\
    lt: sportbačiai                                & lv: kedas                                      & ro: adidași & si: superge                                    & sk: tenisky                                    & tr: spor ayakkabı                              &                                   \\
     \includegraphics[width=1.8cm]{imagen/lt_1.jpg} & \includegraphics[width=1.8cm]{imagen/lv_1.jpg} & \includegraphics[width=1.8cm]{imagen/ro_1.jpg} &
     \includegraphics[width=1.8cm]{imagen/si_1.jpg} & \includegraphics[width=1.8cm]{imagen/sk_1.jpg} & \includegraphics[width=1.8cm]{imagen/tr_1.jpg} &                                           \\
    \hline
    \end{tabular}
\vspace{-4mm}
\end{figure}









We have trained a small cascading Denoising Diffusion Probabilistic Model \cite{denoisingDP} conditioned on text embeddings with two Unet models inside, each with the internal dimension of 128. We used the same text embeddings as in \autoref{sec:experiments_classification}. The generation happens in two steps, we first generate a 64x64 image from which we upscale to 128x128 pixels. We make this model publicly available, including its weights. See the \href{https://github.com/glami/glami-1m}{the source code} for more details.

We report a sample of visual results: \autoref{tab:imagen} shows images sampled on the texts from the test set. Another interesting property of the generator is the novelty of the generated images. We have looked up the two closest images using visual embeddings in the training set for each of the generated images and we were unable to find identical looking images in the train set. Let us underscore that not only the generated images are not pixel perfect replicas of the train samples, but they are quite far from the training samples even by human standards - not just L2 distance caused by imperceptible noise. We have cherry-picked the samples in this table to show that the model learned to draw product images that appear almost realistic. For an unbiased sample of images generated from the test set, see more examples in the \autoref{tab:imagen-random}.
About one third of the images generated by the model appears realistic based on a sample of about 1000 images checked by hand. Furthermore, we have experimented with the text-conditioning and generated images for various phrases translated into all of the languages in the dataset. In about one third of the texts the conditioning failed, in about a third it reflected the correct piece of clothing, but the style was wrong. In other words, high-level category such as "shoes" was correct, but a low-level one such as "sneakers" was often wrong. In about the last third of cases it worked to obtain a realistic sample, see for example \autoref{tab:imagen_snakers}. 









\section{Conclusion}

The paper introduced GLAMI-1M: the largest publicly available multilingual image-text classification dataset and the largest image-text dataset in the fashion domain. 
Thanks to its characteristics, the dataset has the potential to accelerate research in several areas of machine learning, including multilingual image-text classification, text-conditional image generation and multilingual machine translation. For example, it can be used as an alternative to Recipe1M+ \cite{recipe1m+} adding the aspect of multilinguality, or as a larger alternative to FashionGen \cite{fashiongen} and other datasets in the fashion domain.


Experiments on multimodal image classification in \autoref{sec:experiments_classification} show the dataset presents a challenging problem. Together with the dataset, we introduce a benchmark with baseline results and pre-trained models available, and we invite everyone to evaluate their models in the public leaderboard. 

Additional experiments on text-conditional image generation and multi-language machine translation are described in \autoref{sec:experiments_imagen}  and in the supplementary material respectively. The experiments illustrate the dataset's usefulness for other tasks than classification. Pre-trained models and code for the tasks are also shared with the paper.


Other relevant problems left for future work include long-tail learning, adaptation to prior shift, learning from a combination of trusted (human) and noisy (rule-based) annotations.


\newpage
\begin{thebibliography}{63}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Aggarwal(2019, March)]{fashionProductImages}
Param Aggarwal.
\newblock Fashion product images dataset, version 1, 2019, March.
\newblock URL
  \url{https://www.kaggle.com/datasets/paramaggarwal/fashion-product-images-dataset}.

\bibitem[Antonio et~al.(2019)Antonio, Croce, and Basili]{italianCoco}
Scaiella Antonio, Danilo Croce, and Roberto Basili.
\newblock Large scale datasets for image and video captioning in italian.
\newblock \emph{Italian Journal of Computational Linguistics}, 2019.

\bibitem[Barrault et~al.(2018)Barrault, Bougares, Specia, Lala, Elliott, and
  Frank]{multi30kCz}
Lo{\"\i}c Barrault, Fethi Bougares, Lucia Specia, Chiraag Lala, Desmond
  Elliott, and Stella Frank.
\newblock Findings of the third shared task on multimodal machine translation.
\newblock In \emph{Proceedings of the Third Conference on Machine Translation:
  Shared Task Papers}, pages 304--323, 2018.

\bibitem[Carlsson et~al.(2022)Carlsson, Eisen, Rekathati, and Sahlgren]{mclip}
Fredrik Carlsson, Philipp Eisen, Faton Rekathati, and Magnus Sahlgren.
\newblock Cross-lingual and multilingual clip.
\newblock In \emph{Proceedings of the Language Resources and Evaluation
  Conference}, pages 6848--6854, Marseille, France, June 2022. European
  Language Resources Association.
\newblock URL \url{https://aclanthology.org/2022.lrec-1.739}.

\bibitem[Choi and Lee(2019)]{embracenet}
Jun-Ho Choi and Jong-Seok Lee.
\newblock Embracenet: A robust deep learning architecture for multimodal
  classification.
\newblock \emph{Information Fusion}, 51:\penalty0 259--270, 2019.
\newblock URL \url{https://arxiv.org/pdf/1904.09078.pdf}.

\bibitem[Conneau et~al.(2020)Conneau, Khandelwal, Goyal, Chaudhary, Wenzek,
  Guzm{\'a}n, Grave, Ott, Zettlemoyer, and Stoyanov]{xml}
Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume
  Wenzek, Francisco Guzm{\'a}n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and
  Veselin Stoyanov.
\newblock Unsupervised cross-lingual representation learning at scale.
\newblock In \emph{ACL}, 2020.
\newblock URL \url{https://arxiv.org/pdf/1911.02116.pdf}.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{NAACL}, 2019.
\newblock URL \url{https://aclanthology.org/N19-1423.pdf}.

\bibitem[{Elliott} et~al.(2016){Elliott}, {Frank}, {Sima'an}, and
  {Specia}]{multi30kGe}
D.~{Elliott}, S.~{Frank}, K.~{Sima'an}, and L.~{Specia}.
\newblock Multi30k: Multilingual english-german image descriptions.
\newblock \emph{Association for Computational Linguistics}, pages 70--74, 2016.

\bibitem[Elliott et~al.(2017)Elliott, Frank, Barrault, Bougares, and
  Specia]{multi30kFr}
Desmond Elliott, Stella Frank, Lo\"{i}c Barrault, Fethi Bougares, and Lucia
  Specia.
\newblock {Findings of the Second Shared Task on Multimodal Machine Translation
  and Multilingual Image Description}.
\newblock In \emph{Proceedings of the Second Conference on Machine
  Translation}, Copenhagen, Denmark, September 2017.

\bibitem[Fu et~al.(2022)Fu, Xu, Liu, Liu, Xie, Wang, Liu, Sun, and
  Wang]{Liu2021CMACLIPCA}
Jinmiao Fu, Shaoyuan Xu, Huidong Liu, Yang Liu, Ning Xie, Chien-Chih Wang, Jia
  Liu, Yi~Sun, and Bryan Wang.
\newblock Cma-clip: Cross-modality attention clip for text-image
  classification.
\newblock In \emph{IEEE ICIP 2022}, 2022.
\newblock URL
  \url{https://www.amazon.science/publications/cma-clip-cross-modality-attention-clip-for-text-image-classification}.

\bibitem[FUKUSHIMA(1980)]{fukushima1980neocognitron}
K~FUKUSHIMA.
\newblock Neocognitron: a self-organizing neural network model for a mechanism
  of pattern recognition unaffected by shift in position.
\newblock \emph{Biological Cybernetics}, 36\penalty0 (4):\penalty0 193--202,
  1980.

\bibitem[Ge et~al.(2019)Ge, Zhang, Wu, Wang, Tang, and Luo]{deepfashion2}
Yuying Ge, Ruimao Zhang, Lingyun Wu, Xiaogang Wang, Xiaoou Tang, and Ping Luo.
\newblock A versatile benchmark for detection, pose estimation, segmentation
  and re-identification of clothing images.
\newblock \emph{CVPR}, 2019.

\bibitem[Guo et~al.(2021)Guo, Wu, Gao, Rennie, and Feris]{fashionIQ}
Xiaoxiao Guo, Hui Wu, Yupeng Gao, Steven~J. Rennie, and Rog{\'e}rio~Schmidt
  Feris.
\newblock Fashion iq: A new dataset towards retrieving images by natural
  language feedback.
\newblock \emph{2021 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pages 11302--11312, 2021.

\bibitem[Heming et~al.(2020)Heming, Yu, Hang, Weikai, Dong, Zhangye, Shuguang,
  and Xiaoguang]{deepfashion3d}
Zhu Heming, Cao Yu, Jin Hang, Chen Weikai, Du~Dong, Wang Zhangye, Cui Shuguang,
  and Han Xiaoguang.
\newblock Deep fashion3d: A dataset and benchmark for 3d garment reconstruction
  from single images.
\newblock In \emph{Computer Vision -- ECCV 2020}, pages 512--530. Springer
  International Publishing, 2020.
\newblock ISBN 978-3-030-58452-8.
\newblock URL
  \url{https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460494.pdf}.

\bibitem[Hitschler et~al.(2016)Hitschler, Schamoni, and Riezler]{germanCoco}
Julian Hitschler, Shigehiko Schamoni, and Stefan Riezler.
\newblock Multimodal pivots for image caption translation.
\newblock \emph{ArXiv}, abs/1601.03916, 2016.
\newblock URL \url{https://aclanthology.org/P16-1227.pdf}.

\bibitem[Ho et~al.(2020)Ho, Jain, and Abbeel]{denoisingDP}
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.F. Balcan, and H.~Lin,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~33,
  pages 6840--6851. Curran Associates, Inc., 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf}.

\bibitem[Inoue et~al.(2017)Inoue, Simo-Serra, Yamasaki, and
  Ishikawa]{fashion550k}
Naoto Inoue, Edgar Simo-Serra, Toshihiko Yamasaki, and Hiroshi Ishikawa.
\newblock {Multi-Label Fashion Image Classification with Minimal Human
  Supervision}.
\newblock In \emph{Proceedings of the International Conference on Computer
  Vision Workshops (ICCVW)}, 2017.

\bibitem[Jia et~al.(2021)Jia, Yang, Xia, Chen, Parekh, Pham, Le, Sung, Li, and
  Duerig]{align}
Chao Jia, Yinfei Yang, Ye~Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc~V.
  Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig.
\newblock Scaling up visual and vision-language representation learning with
  noisy text supervision.
\newblock In \emph{ICML}, 2021.

\bibitem[Jia et~al.(2020)Jia, Shi, Sirotenko, Cui, Cardie, Hariharan, Adam, and
  Belongie]{fashionpedia}
Menglin Jia, Mengyun Shi, Mikhail Sirotenko, Yin Cui, Claire Cardie, Bharath
  Hariharan, Hartwig Adam, and Serge Belongie.
\newblock Fashionpedia: Ontology, segmentation, and an attribute localization
  dataset.
\newblock In \emph{European Conference on Computer Vision (ECCV)}, 2020.

\bibitem[Kaneko et~al.(2019)Kaneko, Ushiku, and Harada]{clothing1m}
Takuhiro Kaneko, Y.~Ushiku, and Tatsuya Harada.
\newblock Label-noise robust generative adversarial networks.
\newblock \emph{2019 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pages 2462--2471, 2019.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{krizhevsky2012imagenet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock \emph{Advances in neural information processing systems}, 25, 2012.

\bibitem[Lam et~al.(2020)Lam, Le, Nguyen, and Nguyen]{vietnameseCoco}
Quan~Hoang Lam, Quang Duy~– Le, Kiet~Van Nguyen, and Ngan Luu-Thuy Nguyen.
\newblock Uit-viic: A dataset for the first evaluation on vietnamese image
  captioning.
\newblock In \emph{ICCCI}, 2020.

\bibitem[LeCun et~al.(1989)LeCun, Boser, Denker, Henderson, Howard, Hubbard,
  and Jackel]{lecun1989backpropagation}
Yann LeCun, Bernhard Boser, John~S Denker, Donnie Henderson, Richard~E Howard,
  Wayne Hubbard, and Lawrence~D Jackel.
\newblock Backpropagation applied to handwritten zip code recognition.
\newblock \emph{Neural computation}, 1\penalty0 (4):\penalty0 541--551, 1989.

\bibitem[Li et~al.(2019)Li, Xu, Wang, Lan, Jia, Yang, and Xu]{chineseCoco}
Xirong Li, Chaoxi Xu, Xiaoxu Wang, Weiyu Lan, Zhengxiong Jia, Gang Yang, and
  Jieping Xu.
\newblock Coco-cn for cross-lingual image tagging, captioning, and retrieval.
\newblock \emph{IEEE Transactions on Multimedia}, 21:\penalty0 2347--2360,
  2019.

\bibitem[Lin et~al.(2014)Lin, Maire, Belongie, Hays, Perona, Ramanan,
  Doll{\'a}r, and Zitnick]{coco}
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
  Ramanan, Piotr Doll{\'a}r, and C.~Lawrence Zitnick.
\newblock Microsoft coco: Common objects in context.
\newblock In David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars,
  editors, \emph{Computer Vision -- ECCV 2014}, pages 740--755, Cham, 2014.
  Springer International Publishing.
\newblock ISBN 978-3-319-10602-1.
\newblock URL \url{https://arxiv.org/abs/1405.0312}.

\bibitem[Lin et~al.(2018)Lin, Chen, Chiang, and Hsu]{netilook}
Wen~Hua Lin, Kuan-Ting Chen, HungYueh Chiang, and Winston~H. Hsu.
\newblock Netizen-style commenting on fashion photos: Dataset and diversity
  measures.
\newblock \emph{Companion Proceedings of the The Web Conference 2018}, 2018.

\bibitem[Liu et~al.(2016)Liu, Chen, and Chen]{mvc}
Kuan-Hsien Liu, Ting-Yen Chen, and Chu-Song Chen.
\newblock Mvc: A dataset for view-invariant clothing retrieval and attribute
  prediction.
\newblock \emph{Proceedings of the 2016 ACM on International Conference on
  Multimedia Retrieval}, 2016.

\bibitem[Liu et~al.(2014)Liu, Feng, Domokos, Xu, Huang, Hu, and
  Yan]{Liu2014FashionPW}
Si~Liu, Jiashi Feng, Csaba Domokos, Hui Xu, Junshi Huang, Zhenzhen Hu, and
  Shuicheng Yan.
\newblock Fashion parsing with weak color-category labels.
\newblock \emph{IEEE Transactions on Multimedia}, 16:\penalty0 253--265, 2014.

\bibitem[Marin et~al.(2019)Marin, Biswas, Ofli, Hynes, Salvador, Aytar, Weber,
  and Torralba]{recipe1m+}
Javier Marin, Aritro Biswas, Ferda Ofli, Nicholas Hynes, Amaia Salvador, Yusuf
  Aytar, Ingmar Weber, and Antonio Torralba.
\newblock Recipe1m+: A dataset for learning cross-modal embeddings for cooking
  recipes and food images.
\newblock \emph{{IEEE} Trans. Pattern Anal. Mach. Intell.}, 2019.
\newblock URL \url{http://pic2recipe.csail.mit.edu/}.

\bibitem[Matzen et~al.(2017)Matzen, Bala, and Snavely]{streetStyle}
Kevin Matzen, Kavita Bala, and Noah Snavely.
\newblock Streetstyle: Exploring world-wide clothing styles from millions of
  photos.
\newblock \emph{ArXiv}, abs/1706.01869, 2017.
\newblock URL \url{https://arxiv.org/abs/1706.01869}.

\bibitem[Olóndriz et~al.(2021)Olóndriz, Puigdevall, and Palau]{foodi}
David~Amat Olóndriz, Ponç~Palau Puigdevall, and Adrià~Salvador Palau.
\newblock Foodi-ml: a large multi-language dataset of food, drinks and
  groceries images and descriptions, 2021.
\newblock URL \url{https://arxiv.org/abs/2110.02035}.

\bibitem[Picek et~al.(2022)Picek, Šulc, Matas, Jeppesen, Heilmann-Clausen,
  L{\ae}ss{\o}e, and Fr{\o}slev]{df20}
Lukáš Picek, Milan Šulc, Jiří Matas, Thomas~S. Jeppesen, Jacob
  Heilmann-Clausen, Thomas L{\ae}ss{\o}e, and Tobias Fr{\o}slev.
\newblock Danish fungi 2020 - not just another image recognition dataset.
\newblock In \emph{Proceedings of the IEEE/CVF Winter Conference on
  Applications of Computer Vision (WACV)}, pages 1525--1535, January 2022.

\bibitem[PromptCloud(2021, March)]{amazonFashionProducts2020}
PromptCloud.
\newblock Amazon fashion products 2020, version 1, 2021, March.
\newblock URL
  \url{https://www.kaggle.com/datasets/promptcloud/amazon-fashion-products-2020}.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,
  Sastry, Askell, Mishkin, Clark, Krueger, and Sutskever]{clip}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
  Gretchen Krueger, and Ilya Sutskever.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In \emph{ICML}, 2021.
\newblock URL \url{https://arxiv.org/pdf/2103.00020.pdf}.

\bibitem[Raghavan(2021)]{shopify}
Kshetrajna Raghavan.
\newblock Using rich image and text data to categorize products at scale, 2021.
\newblock URL
  \url{https://shopify.engineering/using-rich-image-text-data-categorize-products}.

\bibitem[Ramesh et~al.(2022)Ramesh, Dhariwal, Nichol, Chu, and Chen]{dall-e-2}
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.
\newblock Hierarchical text-conditional image generation with clip latents,
  2022.
\newblock URL \url{https://arxiv.org/abs/2204.06125}.

\bibitem[Rombach et~al.(2022)Rombach, Blattmann, Lorenz, Esser, and
  Ommer]{rombach2022high}
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj{\"o}rn
  Ommer.
\newblock High-resolution image synthesis with latent diffusion models.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 10684--10695, 2022.
\newblock URL \url{https://arxiv.org/pdf/2112.10752.pdf}.

\bibitem[Rostamzadeh et~al.(2018)Rostamzadeh, Hosseini, Boquet, Stokowiec,
  Zhang, Jauvin, and Pal]{fashiongen}
Negar Rostamzadeh, Seyedarian Hosseini, Thomas Boquet, Wojciech Stokowiec, Ying
  Zhang, Christian Jauvin, and Chris Pal.
\newblock Fashion-gen: The generative fashion dataset and challenge, 2018.
\newblock URL \url{https://arxiv.org/abs/1806.08317}.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, Bernstein, Berg, and Fei-Fei]{ILSVRC15}
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
  Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein,
  Alexander~C. Berg, and Li~Fei-Fei.
\newblock {ImageNet Large Scale Visual Recognition Challenge}.
\newblock \emph{International Journal of Computer Vision (IJCV)}, 115\penalty0
  (3):\penalty0 211--252, 2015.
\newblock \doi{10.1007/s11263-015-0816-y}.

\bibitem[Saharia et~al.(2022)Saharia, Chan, Saxena, Li, Whang, Denton,
  Ghasemipour, Ayan, Mahdavi, Lopes, Salimans, Ho, Fleet, and Norouzi]{imagen}
Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily~L.
  Denton, Seyed Kamyar~Seyed Ghasemipour, Burcu~Karagol Ayan, Seyedeh~Sara
  Mahdavi, Raphael~Gontijo Lopes, Tim Salimans, Jonathan Ho, David Fleet, and
  Mohammad Norouzi.
\newblock Photorealistic text-to-image diffusion models with deep language
  understanding.
\newblock \emph{arXiv}, 2022.
\newblock URL \url{https://arxiv.org/abs/2205.11487}.

\bibitem[Schuhmann et~al.(2022)Schuhmann, Vencu, Beaumont, Coombes, Gordon,
  Katta, Kaczmarczyk, and Jitsev]{LAION5B}
Christoph Schuhmann, Richard Vencu, Romain Beaumont, Theo Coombes, Cade Gordon,
  Aarush Katta, Robert Kaczmarczyk, and Jenia Jitsev.
\newblock Laion-5b: An open large-scale dataset for training next generation
  image-text models, 2022.
\newblock URL \url{https://openreview.net/pdf?id=M3Y74vmsMcY}.

\bibitem[Sen et~al.(2020)Sen, Hajra, and Ghosh]{sen2020supervised}
Pratap~Chandra Sen, Mahimarnab Hajra, and Mitadru Ghosh.
\newblock Supervised classification algorithms in machine learning: A survey
  and review.
\newblock In \emph{Emerging technology in modelling and graphics}, pages
  99--111. Springer, 2020.

\bibitem[Shu et~al.(2019)Shu, Xie, Yi, Zhao, Zhou, Xu, and
  Meng]{metaWeightNetLA}
Jun Shu, Qi~Xie, Lixuan Yi, Qian Zhao, Sanping Zhou, Zongben Xu, and Deyu Meng.
\newblock Meta-weight-net: Learning an explicit mapping for sample weighting.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[{\v{S}}ipka et~al.(2022){\v{S}}ipka, {\v{S}}ulc, and
  Matas]{vsipka2022hitchhiker}
Tom{\'a}{\v{s}} {\v{S}}ipka, Milan {\v{S}}ulc, and Ji{\v{r}}{\'\i} Matas.
\newblock The hitchhiker's guide to prior-shift adaptation.
\newblock In \emph{Proceedings of the IEEE/CVF Winter Conference on
  Applications of Computer Vision (WACV)}, pages 1516--1524, 2022.
\newblock URL \url{https://arxiv.org/pdf/2106.11695.pdf}.

\bibitem[Song et~al.(2021)Song, Chen, Jin, Luo, Xie, and Huang]{fashionMMT}
Yuqing Song, Shizhe Chen, Qin Jin, Wei Luo, Jun Xie, and Fei Huang.
\newblock Product-oriented machine translation with cross-modal cross-lingual
  pre-training.
\newblock In \emph{Proceedings of the 29th {ACM} International Conference on
  Multimedia}, 2021.

\bibitem[Specia et~al.(2018)Specia, Frank, Barrault, Bougares, and
  Elliott]{WMT18}
Lucia Specia, Stella Frank, Loïc Barrault, Fethi Bougares, and Desmond
  Elliott.
\newblock Wmt18: Multimodal machine translation on multi30k, 2018.
\newblock URL \url{https://www.statmt.org/wmt18/multimodal-task.html}.

\bibitem[Srinivasan et~al.(2021)Srinivasan, Raman, Chen, Bendersky, and
  Najork]{wit}
Krishna Srinivasan, Karthik Raman, Jiecao Chen, Michael Bendersky, and Marc
  Najork.
\newblock Wit: Wikipedia-based image text dataset for multimodal multilingual
  machine learning.
\newblock \emph{Proceedings of the 44th International ACM SIGIR Conference on
  Research and Development in Information Retrieval}, 2021.
\newblock URL \url{http://arxiv.org/pdf/2103.01913}.

\bibitem[{\v{S}}ulc and Matas(2019)]{priorShift}
Milan {\v{S}}ulc and Ji{\v{r}}{\'\i} Matas.
\newblock Improving cnn classifiers by estimating test-time priors.
\newblock \emph{2019 IEEE/CVF International Conference on Computer Vision
  Workshop (ICCVW)}, pages 3220--3226, 2019.
\newblock URL \url{https://arxiv.org/pdf/1805.08235.pdf}.

\bibitem[Sun et~al.(2014)Sun, Rampalli, Yang, and Doan]{Sun2014ChimeraLC}
Chong Sun, Narasimhan Rampalli, Frank Yang, and AnHai Doan.
\newblock Chimera: Large-scale classification using machine learning, rules,
  and crowdsourcing.
\newblock \emph{Proc. VLDB Endow.}, 7:\penalty0 1529--1540, 2014.
\newblock URL \url{https://pages.cs.wisc.edu/~anhai/papers/chimera-vldb14.pdf}.

\bibitem[Thomee et~al.(2016)Thomee, Shamma, Friedland, Elizalde, Ni, Poland,
  Borth, and Li]{YFCC100M}
Bart Thomee, David~A. Shamma, Gerald Friedland, Benjamin Elizalde, Karl~S. Ni,
  Douglas~N. Poland, Damian Borth, and Li-Jia Li.
\newblock Yfcc100m: the new data in multimedia research.
\newblock \emph{Commun. ACM}, 59:\penalty0 64--73, 2016.
\newblock URL \url{https://arxiv.org/abs/1503.01817}.

\bibitem[van Miltenburg et~al.(2018)van Miltenburg, K{\'a}d{\'a}r, Koolen, and
  Krahmer]{dutchCoco}
Emiel van Miltenburg, {\'A}kos K{\'a}d{\'a}r, Ruud Koolen, and Emiel~J.
  Krahmer.
\newblock Didec: The dutch image description and eye-tracking corpus.
\newblock In \emph{COLING}, 2018.
\newblock URL \url{https://aclanthology.org/C18-1310/}.

\bibitem[Wang et~al.(2022)Wang, Figueiredo, and Specia]{multisub}
Josiah Wang, Josiel Figueiredo, and Lucia Specia.
\newblock {M}ulti{S}ubs: A large-scale multimodal and multilingual dataset.
\newblock In \emph{Proceedings of the Thirteenth Language Resources and
  Evaluation Conference}, pages 6776--6785, Marseille, France, June 2022.
  European Language Resources Association.
\newblock URL \url{https://aclanthology.org/2022.lrec-1.730}.

\bibitem[Wang et~al.(2015)Wang, Kumar, Thome, Cord, and Precioso]{upmcFood101}
Xin Wang, Devinder Kumar, Nicolas Thome, Matthieu Cord, and Fr{\'e}d{\'e}ric
  Precioso.
\newblock Recipe recognition with large multimodal food dataset.
\newblock \emph{2015 IEEE International Conference on Multimedia \& Expo
  Workshops (ICMEW)}, pages 1--6, 2015.
\newblock URL
  \url{https://hal.archives-ouvertes.fr/hal-01196959/file/CEA_ICME2015.pdf}.

\bibitem[Xie et~al.(2019)Xie, Lai, Doran, and Kadav]{Xie2019VisualEA}
Ning Xie, Farley Lai, Derek Doran, and Asim Kadav.
\newblock Visual entailment: A novel task for fine-grained image understanding.
\newblock \emph{ArXiv}, abs/1901.06706, 2019.
\newblock URL \url{https://arxiv.org/abs/1901.06706}.

\bibitem[Xie et~al.(2017)Xie, Girshick, Doll{\'a}r, Tu, and He]{resnext}
Saining Xie, Ross~B. Girshick, Piotr Doll{\'a}r, Zhuowen Tu, and Kaiming He.
\newblock Aggregated residual transformations for deep neural networks.
\newblock \emph{2017 IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, pages 5987--5995, 2017.
\newblock URL
  \url{https://openaccess.thecvf.com/content_cvpr_2017/papers/Xie_Aggregated_Residual_Transformations_CVPR_2017_paper.pdf}.

\bibitem[Xue et~al.(2021)Xue, Constant, Roberts, Kale, Al-Rfou, Siddhant,
  Barua, and Raffel]{xue-etal-2021-mt5}
Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya
  Siddhant, Aditya Barua, and Colin Raffel.
\newblock m{T}5: A massively multilingual pre-trained text-to-text transformer.
\newblock In \emph{Proceedings of the 2021 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pages 483--498, Online, June 2021. Association for
  Computational Linguistics.
\newblock \doi{10.18653/v1/2021.naacl-main.41}.
\newblock URL \url{https://aclanthology.org/2021.naacl-main.41}.

\bibitem[Yang et~al.(2020)Yang, Zhang, Jin, Liu, Wu, Tan, Xie, Wang, and
  Wang]{facad}
Xuewen Yang, Heming Zhang, Di~Jin, Yingru Liu, Chi-Hao Wu, Jianchao Tan,
  Dongliang Xie, Jue Wang, and Xin Wang.
\newblock Fashion captioning: Towards generating accurate descriptions with
  semantic rewards.
\newblock In \emph{ECCV}, 2020.

\bibitem[Yoshikawa et~al.(2017)Yoshikawa, Shigeto, and Takeuchi]{japaneseCoco}
Yuya Yoshikawa, Yutaro Shigeto, and Akikazu Takeuchi.
\newblock {STAIR} captions: Constructing a large-scale {J}apanese image caption
  dataset.
\newblock In \emph{Proceedings of the 55th Annual Meeting of the Association
  for Computational Linguistics (Volume 2: Short Papers)}, pages 417--421,
  Vancouver, Canada, July 2017. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/P17-2066}.
\newblock URL \url{https://aclanthology.org/P17-2066}.

\bibitem[Yu et~al.(2022)Yu, Wang, Vasudevan, Yeung, Seyedhosseini, and
  Wu]{coca}
Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and
  Yonghui Wu.
\newblock Coca: Contrastive captioners are image-text foundation models.
\newblock \emph{Transactions on Machine Learning Research}, 2022.
\newblock \doi{10.48550/ARXIV.2205.01917}.
\newblock URL \url{https://openreview.net/forum?id=Ee277P3AYC}.

\bibitem[Zhai et~al.(2022)Zhai, Kolesnikov, Houlsby, and Beyer]{jft3b}
Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer.
\newblock Scaling vision transformers.
\newblock \emph{2022 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pages 1204--1213, 2022.
\newblock URL
  \url{https://openaccess.thecvf.com/content/CVPR2022/papers/Zhai_Scaling_Vision_Transformers_CVPR_2022_paper.pdf}.

\bibitem[Zhang et~al.(2020)Zhang, Zhang, Arik, Lee, and
  Pfister]{distillingFromNoise}
Zizhao Zhang, Han Zhang, Sercan~{\"O}. Arik, Honglak Lee, and Tomas Pfister.
\newblock Distilling effective supervision from severe label noise.
\newblock \emph{2020 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pages 9291--9300, 2020.
\newblock URL \url{https://arxiv.org/pdf/1910.00701.pdf}.

\bibitem[Zheng et~al.(2021)Zheng, Awadallah, and Dumais]{metaLabelCorrection}
Guoqing Zheng, Ahmed~Hassan Awadallah, and Susan~T. Dumais.
\newblock Meta label correction for noisy label learning.
\newblock In \emph{AAAI}, 2021.

\bibitem[Zheng et~al.(2018)Zheng, Yang, Kiapour, and Piramuthu]{modanet}
Shuai Zheng, F.~Yang, Mohammad~Hadi Kiapour, and Robinson Piramuthu.
\newblock Modanet: A large-scale street fashion dataset with polygon
  annotations.
\newblock \emph{Proceedings of the 26th ACM international conference on
  Multimedia}, 2018.

\end{thebibliography}
 

\end{document}
