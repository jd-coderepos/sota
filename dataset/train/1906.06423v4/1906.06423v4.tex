\appendix 





\onecolumn
\begin{minipage}{\textwidth}
\begin{center}
\Huge
Supplementary material for \\
``Fixing the train-test resolution discrepancy''
\end{center}
\end{minipage}


\vspace*{1cm}


In this supplementary material we report details and results that did not fit in the main paper.
This includes the estimation of the parametric distribution of activations in Section~\ref{sec:parametricfit}, a small study on border/round-off effects of the image size for a convolutional neural net in Section~\ref{sec:roundoff} and more exhaustive result tables in Section~\ref{sec:resulttables}. Section~\ref{sec:challenges} further demonstrates the interest of our approach through our participation to two competitive challenges in fine-grained recognition.




\section{Fitting the activations}
\label{sec:parametricfit}

\subsection{Parametric Fr\'echet model after average-pooling}
In this section we derive a parametric model that fits the distribution of activations on output of the spatial pooling layer. 

The output the the last convolutional layer can be well approximated with a Gaussian distribution. 
Then the batch-norm centers the Gaussian and reduces its variance to unit, and the ReLU replaces the negative part with 0. 
Thus the ReLU outputs an equal mixture of a cropped unit Gaussian and a Dirac of value 0.

The average pooling sums  to  of those distributions together.
Assuming independence of the inputs, it can be seen as a sum of  cropped Gaussians, where  follows a discrete binomial distribution. 
Unfortunately, we found this composition of distributions is not tractable in close form. 

Instead, we observed experimentally that the output distribution is close to an extreme value distribution. 
This is due to the fact that only the positive part of the Gaussians contributes to the output values.
In an extreme value distribution that is the sum of several (arbitrary independent) distributions, the same happens: only the highest parts of those distributions contribute. 

Thus, we model the statistics of activations as a Fr\'echet (a.k.a. inverse Weibull) distribution. 
This is a 2-parameter distribution whose CDF has the form: 

With~~a~positive~constant,  
We observed that the parameter  can be kept constant at  to fit the distributions.



\begin{figure*}[b]
\begin{minipage}{0.24\linewidth}
    \includegraphics[width=\linewidth]{figs/Frechet_distribution/CDF_subplots/whitout_title/CDF_of_non_zero_value_after_average_pooling_Frechet_fig1_res64.pdf}  \\
    Resolution: 64
\end{minipage}
\hfill
\begin{minipage}{0.24\linewidth}
\includegraphics[width=\linewidth]{figs/Frechet_distribution/CDF_subplots/whitout_title/CDF_of_non_zero_value_after_average_pooling_Frechet_fig2_res128.pdf} \\
Resolution: 128
\end{minipage}
\hfill
\begin{minipage}{0.24\linewidth}
\includegraphics[width=\linewidth]{figs/Frechet_distribution/CDF_subplots/whitout_title/CDF_of_non_zero_value_after_average_pooling_Frechet_fig3_res224.pdf} \\
Resolution: 224
\end{minipage}
\hfill
\begin{minipage}{0.24\linewidth}
\includegraphics[width=\linewidth]{figs/Frechet_distribution/CDF_subplots/whitout_title/CDF_of_non_zero_value_after_average_pooling_Frechet_fig4_res448.pdf} \\
 Resolution: 448
 \end{minipage}   
\caption{\label{fig:pdfresolutionfrechet}
    Fitting of the CDF of activations with a Fr\'echet distribution.
}
\end{figure*}

Figure~\ref{fig:pdfresolutionfrechet} shows how the Fr\'echet model fits the empirical CDF of the distribution. 
The parameters were estimated using least-squares minimization, excluding the zeros, that can be considered outliers.
The fit is so exact that the difference between the curves is barely visible.

To correct the discrepancy in distributions at training and test times, we compute the parameters  of the distribution observed on training images time for . 
Then we increase  to the target resolution and measure the parameters  again. 
Thus, the transformation is just an affine scaling, still ignoring zeros.

When running the transformed neural net on the Imagenet evaluation, we obtain accuracies: 
\begin{center}
\begin{tabular}{|l|rrrrrr|}
\hline 
 & 64 & 128 & 224 & 256 & 288 & 448 \\
    accuracy & 29.4 & 65.4 & 77 &78 & 78.4 & 76.5 \\
\hline
\end{tabular}
\end{center}
Hence, the accuracy does not improve with respect to the baseline. 
This can be explained by several factors: 
the scalar distribution model, however good it fits to the observations, is insufficient to account for the individual distributions of the activation values; 
just fitting the distribution may not be enough to account for the changes in behavior of the convolutional trunk.




\subsection{Gaussian model before the last ReLU activation}

Following the same idea as what we did previously we looked at the distribution of activations by channel  before the last ReLU according to the resolution.

We have seen that the distributions are different from one resolution to another. 
With higher resolutions,  the mean tends to be closer to 0 and the variance tends to become smaller.
By acting on the distributions before the ReLU, it is also possible to affect the sparsity of values after spatial-pooling, which was not possible with the previous analysis based on Frechet's law.
We aim at matching the distribution before the last ReLU with the distribution of training data at lower resolution. 
We compare the effect of this transformation before/after fine tuning with the learnt batch-norm approach. 
The results are summarized in Table~\ref{tab:adaptlaw}.

We can see that adapting the resolution by changing the distributions is effective especially in the case of small resolutions. 
Nevertheless, the adaptation obtained by fine-tuning the the batch norm improves performs better in general.

\begin{table*}
\caption{\label{tab:adaptlaw}
    Matching  distribution before the last Relu  application to  ResNet-50: 
    Resulting top-1 accuracy \% on ImageNet validation set
  }  
  \centering {\small
  \begin{tabular}{cccccc@{\mysp}c@{\mysp}c@{\mysp}c@{\mysp}c}
    \toprule
    
    \multicolumn{1}{c}{Model} &\multicolumn{1}{c}{Train}  & \multicolumn{1}{c}{Adapted }& \multicolumn{2}{c}{Fine-tuning}& \multicolumn{5}{c}{Test resolution} \\
     \cmidrule(lr){4-5} \cmidrule(lr){6-10} 
    used &resolution  & Distribution & Classifier & Batch-norm & 64 & 224 & 288 & 352 & 384\\
    \cmidrule(lr){1-1} \cmidrule(lr){2-2} \cmidrule(lr){3-5} \cmidrule(lr){6-10} 
    ResNet-50 & 224 & \_    & \_ &  \_     &  29.4  & 77.0 & 78.4 & 78.1 & 77.7  \\
    ResNet-50 & 224 & \checkmark    & \_ &  \_     &  29.8  & 77.0 & 77.7 & 77.3 & 76.8 \\
    ResNet-50 & 224 & \_    & \checkmark  &  \_     &  40.6  & 77.1 & 78.6 &78.9 & 78.9 \\
    ResNet-50 & 224 & \_    & \checkmark  &  \checkmark   &  41.7  & 77.1 & 78.5 & 78.9 & 79.0  \\
    ResNet-50 & 224 & \checkmark    & \checkmark  &  \_   &  41.8  & 77.1 & 78.5 & 78.8 & 78.9 \\


    \bottomrule
\end{tabular}}
\end{table*}


\section{Border and round-off effects}
\label{sec:roundoff}

Due to the complex discrete nature of convolutional layers, the accuracy is not a monotonous function of the input resolution. 
There is a strong dependency on the kernel sizes and strides used in the first convolutional layers. 
Some resolutions will not match with these parameters so we will have a part of the images margin that will not be taken into account by the convolutional layers. 

In Figure~\ref{fig:onepixels}, we show the variation in accuracy when the resolution of the crop is increased by steps of 1~pixel.  
Of course, it is possible to do padding but it will never be equivalent to having a resolution image adapted to the kernel and stride size.

Although the global trend is increasing, there is a lot of jitter that comes from those border effects. 
There is a large drop just after resolution 256.
We observe the drops at each multiple of 32, they correspond to a changes in the top-level activation map's resolution.
Therefore we decided to use only sizes that are multiples of 32 in the experiments. 

\begin{figure}[t]
\begin{center}
\includegraphics[width=0.5\columnwidth]{figs/pixels_increment_by_one/Best_accuracy_resolution_zoom_224.pdf}~\end{center}
\caption{\label{fig:onepixels}
    Evolution of the top-1 accuracy of the ResNet-50 trained with resolution 224 according to the testing resolution (no finetuning).
    This can be considered a zoom of figure~\ref{fig:accuracyresolution} with 1-pixel increments.
}
\end{figure}





\section{Result tables}
\label{sec:resulttables}

Due to the lack of space, we report only the most important results in the main paper. 
In this section, we report the full result tables for several experiments.

Table~\ref{tab:baselines} report the numerical results corresponding to Figure~\ref{fig:accuracyresolution} in the main text.
Table~\ref{tab:ablationstudytable} reports the full ablation study results (see Section~\ref{sec:ablation}). 
\label{sec:supptimings}
Table~\ref{tab:time} reports the runtime measurements that Section~\ref{sec:timings} refers to.
Table~\ref{tab:dataaugmentationcomparison} reports a comparaison between test DA and test DA2 that Section~\ref{sec:experiments} refers to. 
 
\begin{table*}


{\small ~\hfill \begin{tabular}{lrcccc}
    \toprule
    test  train     & 64 \ \ & 128 & 160 & 224 & 384 \\
    \midrule
    64 & 63.2 & 48.3 & 40.1 & 29.4 & 12.6 \\
    128 & \textbf{68.2} & 73.3 &71.2 & 65.4 & 48.0 \\
    224 & 55.3 & \textbf{75.7} &\textbf{ 77.3}  & 77.0 & 70.5 \\
    288 & 42.4 & 73.8 & 76.6 & \textbf{78.4} & 75.2 \\
    384 & 23.8 & 69.6 & 73.8 & 77.7 & 78.2 \\
    448 & 13.0 & 65.8 &71.5 & 76.6 & \textbf{78.8} \\
    480 & 9.7 & 63.9 & 70.2 & 75.9 & 78.7 \\
    \bottomrule
  \end{tabular}\hfill
  \begin{tabular}{l|llll}
    \toprule
    test  train     & 64 & 128 & 224 & 384 \\
    \midrule
    64 & 63.5  & 53.7  & 41.7 & 27.5 \\
    128 & \textbf{71.3} & 73.4 & 67.7 & 55.7 \\
    224 & 66.9 & \textbf{77.1}  & 77.1 & 71.9 \\
    288 & 62.4 & 76.6 & 78.6 & 75.7 \\
    384 & 55.0 & 74.8 & \textbf{79.0} & 78.2 \\
    448 & 49.7 & 73.0 & 78.4 & 78.8 \\
    480 & 46.6 & 72.2 & 78.1 & \textbf{79.0} \\
    \bottomrule
\end{tabular} \hfill ~}
\smallskip
    \caption{
    \label{tab:baselines}
	Top-1 validation accuracy for different combinations of training and testing resolution. 
	Left: with the standard training procedure, (no finetuning, no adaptation of the ResNet-50).
	Right: with our data-driven adaptation strategy and test-time augmentations.
}
\end{table*}

\iffalse
\begin{table}
\centering
  \begin{tabular}{l|llll}
    \toprule
    test  train     & 64 & 128 & 224 & 384 \\
    \midrule
    64 & 63.5 & 53.7 & 41.7 & 27.5 \\
    128 & \textbf{71.3} & 73.4 & 67.7 & 55.7 \\
    224 & 66.9 & \textbf{77.1}  & 77.1 & 71.9 \\
    288 & 62.4 & 76.6 & 78.6 & 75.7 \\
    384 & 55.0 & 74.8 & \textbf{79.0} & 78.2 \\
    448 & 49.7 & 73.0 & 78.4 & 78.8 \\
    480 & 46.6 & 72.2 & 78.1 & \textbf{79.0} \\
    \bottomrule
\end{tabular}
\caption{\label{tab:finetunebatchnormbaselines}
	Top-1 validation accuracy for different combinations of training and testing resolution. 
	This is with the fine-tuning of the last batch norm and the classification layer  of the ResNet-50 network with test-time augmentations.
}
\end{table}
\fi







\begin{table*}
\centering
  {\small
  \begin{tabular}{c|ccc|cccccc}
    \toprule
    \multicolumn{1}{c}{Train} & \multicolumn{3}{|c}{Fine-tuning}& \multicolumn{6}{|c}{Test resolution (top-1 accuracy)} \\
    \cmidrule(lr){2-4} \cmidrule(lr){5-10} 
    resolution & Classifier & Batch-norm &  Data aug. & 64 & 128 &224 & 288 & 384 & 448\\
    \midrule
        & --         & -- &  n/a     &  48.3  & 73.3 & \textbf{75.7} & 73.8 & 69.6 & 65.8 \\
& \checkmark & -- &  train DA &  52.8 & 73.3 & \textbf{77.1} & 76.3& 73.2 & 71.7 \\
    128 & \checkmark & -- &  test DA&  53.3 & 73.4 & \textbf{77.1} & 76.4& 74.4 & 72.3 \\
& \checkmark & \checkmark &  train DA &  53.0 & 73.3 & \textbf{77.1} & 76.5 & 74.4 & 71.9 \\
        & \checkmark & \checkmark  &  test DA &  53.7 & 73.4 & \textbf{77.1} & 76.6 & 74.8 & 73.0 \\
    \midrule
         & --         & -- &  n/a     & 29.4 & 65.4 & 77.0 & \textbf{78.4} & 77.7 & 76.6 \\
& \checkmark & -- &  train DA &  39.9 &67.5 & 77.0 & 78.6 & \textbf{78.9} & 78.0 \\
    224 & \checkmark & -- &   test DA &  40.6 & 67.3 & 77.1 & 78.6 & \textbf{78.9} & 77.9 \\
& \checkmark & \checkmark &  train DA  &  40.4  & 67.5 & 77.0 & 78.6 & \textbf{78.9} & 78.0 \\
        & \checkmark & \checkmark &   test DA &  41.7 & 67.7 & 77.1 & 78.6 & \textbf{79.0} & 78.4 \\
    \bottomrule
\end{tabular}}
\smallskip
\caption{\label{tab:ablationstudytable}
    Ablation study: 
    Accuracy when enabling or disabling some components of the training method. 
    Train DA: training-time data augmentation during fine-tuning, test DA: test-time one. 
}
\end{table*}


\newcommand{\std}[1]{\footnotesize{}}

\begin{table*}
\centering
\small
\begin{tabular}{ll|rrrr|rc}
  \toprule    
  \multicolumn{2}{c}{Resolution} & \multicolumn{2}{c}{Train time per batch (ms)} & \multicolumn{2}{c}{Resolution fine-tuning (ms)} & \multicolumn{2}{c}{Performance} \\
  \cmidrule(lr){1-2} \cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} 
  train & test & backward        & forward        & backward      & forward        & Total time (h)       & accuracy  \\
  \midrule
  128 & 128 & 29.0 \std{4.0}  & 12.8 \std{2.8} & \_            & \_             & 111.8 \hspace{1.5em} & 73.3 \\
  160 & 160 & 30.2 \std{3.2}  & 14.5 \std{3.4} & \_            & \_             & 119.7 \hspace{1.5em} & 75.1 \\
  224 & 224 & 35.0 \std{2.0}  & 15.2 \std{3.2} & \_            & \_             & 133.9 \hspace{1.5em} & 77.0 \\
  384 & 384 & 112.4 \std{6.2} & 18.2 \std{3.9} & \_            & \_             & 348.5 \hspace{1.5em} & 78.2 \\
  \midrule
  160 & 224 & 30.2 \std{3.2}  & 14.5 \std{3.4} & \_            & \_             & 119.7 \hspace{1.5em} & 77.3 \\
  224 & 288 & 35.0 \std{2.0}  & 15.2 \std{3.2} & \_            & \_             & 133.9 \hspace{1.5em} & 78.4 \\
    \midrule
  128 & 224 & 29.0 \std{4.0}  & 12.8 \std{2.8} & 4.4 \std{0.9} & 14.4 \std{2.5} & 124.1 \hspace{1.5em} & 77.1 \\
  160 & 224 & 30.2 \std{3.2}  & 14.5 \std{3.4} & 4.4 \std{0.9} & 14.4 \std{2.5} & 131.9 \hspace{1.5em} & 77.6  \\
  224 & 384 & 35.0 \std{2.0}  & 15.2 \std{3.2} & 8.2 \std{1.3} & 18.0 \std{2.7} & 151.5 \hspace{1.5em} & 79.0\\
  \bottomrule
\end{tabular}
\smallskip
\caption{\label{tab:time}
Execution time for the training. 
Training and fine-tuning times are reported for a batch of size 32 for training and 64 for fine-tuning, on one GPU.  
Fine-tuning uses less memory than training therefore we can use larger batch size.
The total time is the total time spent on both, with 120 epochs for training and 60 epochs of fine-tuning on ImageNet. 
Our approach corresponds to fine-tuning of the batch-norm and the classification layer.
}
\end{table*}




\begin{table*}
\centering
\small
\begin{tabular}{lcccc}
    \toprule 
    Models & Train  & Test & Top-1 test DA (\%) & Top-1 test DA2 (\%) \\
    \midrule
    ResNext-101 32x48d  & 224 & 288 & 86.0 & 86.1 \\
    ResNext-101 32x48d & 224 & 320 & 86.3 & 86.4 \\
    \midrule
    ResNet-50  & 224 & 320 & 79.0 & 79.1 \\
    \midrule
    ResNet-50 CutMix & 224 & 384 & 79.7 & 79.8  \\
    \bottomrule
\end{tabular}
\smallskip
\caption{\label{tab:dataaugmentationcomparison}
Comparisons of performance between data-augmentation test DA and test DA2 in the case of fine-tuning batch-norm and classifier.
}
\end{table*}


\section{Impact of Random Resized Crop}
\label{sec:RandomResizedCropEffect}
In this section we measure the impact of the RandomResizedCrop illustrated in the section~\ref{sec:experiments}.
To do this we did the same experiment as in section~\ref{sec:experiments} but we replaced the RandomResizedCrop with a Resize followed by a random crop with a fixed size.
The figure~\ref{fig:norandomresizedcrop} and table~\ref{tab:norandomresizedcrop} shows our results. 
We can see that the effect observed in the section~\ref{sec:experiments} is mainly due to the Random Resized Crop as we suggested with our analysis of the section~\ref{sec:analysis}.

\begin{table}
  \centering
  \begin{tabular}{l|llll}
    \toprule
    test  train     & 64 & 128 & 224 & 384 \\
    \midrule
    64 & 60.0 & 48.7 & 28.1 & 11.5 \\
    96 & \textbf{61.6} & 65.0 & 50.9 & 29.8 \\
    128 & 54.2 & 70.8 & 63.5 & 46.0 \\
    160 & 42.4 & \textbf{72.4} & 69.7 & 57.0 \\
    224 & 21.7 & 69.8 & 74.6 & 68.8 \\
    256 & 15.3 & 66.4 & \textbf{75.2} & 72.1 \\
    384 & 4.3 & 44.8 & 71.7 & 76.7 \\
    440 & 2.3 & 33.6 & 67.1 & \textbf{77.0} \\
    \bottomrule
\end{tabular}
\caption{\label{tab:norandomresizedcrop}
	Top-1 validation accuracy for different combinations of training and testing resolution. 
ResNet-50 train with resize and random crop with a fixed size instead of random resized crop.
}
\end{table}

\begin{figure}[t]
\begin{center}
\includegraphics[width=0.5\columnwidth]{figs/No_Random_Resized_Crop/without_random_resized_crop_test_higher.pdf}~\end{center}
\caption{\label{fig:norandomresizedcrop}
Top-1 accuracy of the ResNet-50 according to the test time resolution. ResNet-50 train with resize and random crop with a fixed size instead of random resized crop.
}
\end{figure}

\section{Fine-Grained Visual Categorization contests: iNaturalist \& Herbarium}
\label{sec:challenges}

In this section we summarize the results we obtained with our method during the CVPR 2019 iNaturalist~\cite{Horn2017INaturalist} and Herbarium~\cite{Kiat2019Herbarium} competitions\footnote{
\url{https://www.kaggle.com/c/herbarium-2019-fgvc6} \newline
\url{https://www.kaggle.com/c/inaturalist-2019-fgvc6}}.
We used the approach lined out in Subsection~\ref{sec:transferlearning}, except that we adapted the preprocessing to each dataset and added a few ``tricks'' useful in competitions (ensembling, multiple crops). 

\subsection{Challenges}

The iNaturalist Challenge 2019 dataset contains images of 1010 animal and plant species, with a training set of 268,243 images and a test set of 35,351 images. 
The main difficulty is that the species are very similar within the six main families (Birds, Reptiles, Plants, Insects, Fungi and Amphibians) contained in the dataset. 
There is also a very high variability within the classes as the appearance of males, females and juveniles is often very different.
What also complicates the classification is the size of the area of interest which is very variable from one image to another, sometimes the images are close-ups on the subject, sometimes we can hardly distinguish it. 
As a preprocessing, all images have been resized to have a maximum dimension of 800 pixels.

The Herbarium contest requires to identify melastome species from 683 herbarium specimenina.
The training set contain 34,225 images and the test set contain 9,565 images. 
The main difficulty is that the specimina are very similar and not always intact. 
In this challenge the particularity is that there is no variability in the background: each specimen is photographed on a white sheet of paper. All images have been also resized to have a maximum dimension of 800 pixels.

\subsection{Ensemble of classifiers}

In both cases we used 4 different CNNs to do the classification and we averaged their results, which are themselves from 10 crops of the image. 
We chose 4 quite different in their architectures in order to obtain orthogonal classification results.
We tried to include the ResNet-50, but it was significantly worse than the other models, even when using an ensemble of models, probably due to its limited capacity. 

We used two fine-tuning stages: (1) to adapt to the new dataset in 120 epochs and (2) to adapt to a higher resolution in a few epochs. 
We chose the initial training resolution with grid-search, within the computational constraints. 
We did not skew the sampling to balance the classes. 
The rationale for this is that the performance measure is top-1 accuracy, so the penalty to misclassify infrequent classes is low.

\subsection{Results}

Table~\ref{tab:challenge} summarizes the parameters of our submission and the results. 
We report our top-performing approach, 3 and 1 points behind the winners of the competition. 
Note that we just used our method off-the-shelf and therefore used much fewer evaluations on the public part of the test set (5 for iNaturalist and 8 for Herbarium). 
The number of CNNs that at are combined in our ensemble is also smaller that two best performing ones.
In addition, for iNaturalist we did not train on data from the 2018 version of the contest.
In summary, our participation was a run with minimal if no tweaking, where we obtain excellent results (5th out of more than 200 on iNaturalist), thanks to the test-time resolution adaptation exposed in this article.






\begin{table*}
\centering
  \small
  \begin{tabular}{c|c|ccc|c}
    \toprule
    \multicolumn{1}{c}{INaturalist}& \multicolumn{1}{|c}{Train} & \multicolumn{3}{|c}{Fine-tuning} & \multicolumn{1}{|c}{Test}\\
    \cmidrule(lr){3-5} 
    Model used & resolution & Layer 4 & Classifier & Batch-norm &  resolution  \\
    \midrule
  SE-ResNext-101-32x4d & 448 &  -- & \checkmark & \checkmark &  704 \\
  SENet-154 & 448 &  \checkmark  & \checkmark & \checkmark &  672 \\
  Inception-ResNet-V2 & 491 &  -- & \checkmark & \checkmark &  681 \\
  ResNet-152-MPN-COV \cite{Li2017MPN} & 448 &  -- & -- & -- &  448 \\
   \midrule
   \multicolumn{1}{c}{}& \multicolumn{1}{c}{final score : } &\multicolumn{1}{c}{86.577 \%}& \multicolumn{1}{c}{Rank : 5 / 214} &\multicolumn{1}{c}{}& \multicolumn{1}{c}{}\\
   \toprule

    \multicolumn{1}{c}{Herbarium}& \multicolumn{1}{|c}{Train} & \multicolumn{3}{|c}{Fine-tuning} & \multicolumn{1}{|c}{Test}\\
    \cmidrule(lr){3-5} 
    Model used & resolution & Layer 4 & Classifier & Batch-norm &  resolution  \\
    \midrule
    SENet-154 & 448 &  --  & \checkmark & \checkmark &  707 \\
    ResNet-50 & 384 &  --  & \checkmark & \checkmark &  640 \\
    \midrule
   \multicolumn{1}{c}{}& \multicolumn{1}{c}{final score : } &\multicolumn{1}{c}{88.845 \%}& \multicolumn{1}{c}{Rank : 4 / 22} &\multicolumn{1}{c}{}& \multicolumn{1}{c}{}\\
    \bottomrule
\end{tabular}
\smallskip
\caption{\label{tab:challenge}
Our best ensemble results for the Herbarium and INaturalist competitions.
}
\end{table*}